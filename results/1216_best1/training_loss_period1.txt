iteration 0, loss = 0.6272590160369873
iteration 1, loss = 0.5897620916366577
iteration 2, loss = 0.5529040098190308
iteration 3, loss = 0.4910535216331482
iteration 4, loss = 0.5617032051086426
iteration 5, loss = 0.5732005834579468
iteration 6, loss = 0.5283353328704834
iteration 7, loss = 0.4621138572692871
iteration 8, loss = 0.3298867344856262
iteration 9, loss = 0.289588987827301
iteration 10, loss = 0.40880072116851807
iteration 11, loss = 0.40869247913360596
iteration 12, loss = 0.38947200775146484
iteration 13, loss = 0.396971195936203
iteration 14, loss = 0.42698991298675537
iteration 15, loss = 0.3552766740322113
iteration 16, loss = 0.4462343156337738
iteration 17, loss = 0.3443077504634857
iteration 18, loss = 0.3965473175048828
iteration 19, loss = 0.42416977882385254
iteration 20, loss = 0.35470104217529297
iteration 21, loss = 0.37456727027893066
iteration 22, loss = 0.3231499493122101
iteration 23, loss = 0.3958927094936371
iteration 24, loss = 0.308644562959671
iteration 25, loss = 0.4051043391227722
iteration 26, loss = 0.3391815423965454
iteration 27, loss = 0.2817215919494629
iteration 28, loss = 0.38784754276275635
iteration 29, loss = 0.39515435695648193
iteration 30, loss = 0.32845160365104675
iteration 31, loss = 0.3835117518901825
iteration 32, loss = 0.271788090467453
iteration 33, loss = 0.3155583441257477
iteration 34, loss = 0.33168134093284607
iteration 35, loss = 0.2702394127845764
iteration 36, loss = 0.32801753282546997
iteration 37, loss = 0.34405067563056946
iteration 38, loss = 0.25354209542274475
iteration 39, loss = 0.3569318950176239
iteration 40, loss = 0.3239786922931671
iteration 41, loss = 0.3100341558456421
iteration 42, loss = 0.35488593578338623
iteration 43, loss = 0.3194490671157837
iteration 44, loss = 0.2700628638267517
iteration 45, loss = 0.37531930208206177
iteration 46, loss = 0.3298455774784088
iteration 47, loss = 0.31764018535614014
iteration 48, loss = 0.2572069466114044
iteration 49, loss = 0.260322242975235
iteration 50, loss = 0.33868908882141113
iteration 51, loss = 0.43777158856391907
iteration 52, loss = 0.24517861008644104
iteration 53, loss = 0.28220298886299133
iteration 54, loss = 0.2505216598510742
iteration 55, loss = 0.3130851984024048
iteration 56, loss = 0.40049028396606445
iteration 57, loss = 0.23268085718154907
iteration 58, loss = 0.34631508588790894
iteration 59, loss = 0.3794330954551697
iteration 60, loss = 0.2403145432472229
iteration 61, loss = 0.2328028380870819
iteration 62, loss = 0.29671183228492737
iteration 63, loss = 0.32379579544067383
iteration 64, loss = 0.34143131971359253
iteration 65, loss = 0.3416425585746765
iteration 66, loss = 0.21417608857154846
iteration 67, loss = 0.3546615242958069
iteration 68, loss = 0.3326548635959625
iteration 69, loss = 0.2707458436489105
iteration 70, loss = 0.2584497928619385
iteration 71, loss = 0.17894674837589264
iteration 72, loss = 0.30311644077301025
iteration 73, loss = 0.218495711684227
iteration 74, loss = 0.27697622776031494
iteration 75, loss = 0.3471721112728119
iteration 76, loss = 0.25966373085975647
iteration 77, loss = 0.2946995496749878
iteration 78, loss = 0.29817119240760803
iteration 79, loss = 0.17758849263191223
iteration 80, loss = 0.2728171944618225
iteration 81, loss = 0.17081958055496216
iteration 82, loss = 0.17053665220737457
iteration 83, loss = 0.2564072012901306
iteration 84, loss = 0.27870407700538635
iteration 85, loss = 0.19918537139892578
iteration 86, loss = 0.19286325573921204
iteration 87, loss = 0.22231557965278625
iteration 88, loss = 0.23459167778491974
iteration 89, loss = 0.19208554923534393
iteration 90, loss = 0.18906421959400177
iteration 91, loss = 0.2296701967716217
iteration 92, loss = 0.14661917090415955
iteration 93, loss = 0.21935561299324036
iteration 94, loss = 0.16061918437480927
iteration 95, loss = 0.2486850917339325
iteration 96, loss = 0.17229865491390228
iteration 97, loss = 0.2242799997329712
iteration 98, loss = 0.1970110386610031
iteration 99, loss = 0.3146666884422302
iteration 100, loss = 0.31436482071876526
iteration 101, loss = 0.22842437028884888
iteration 102, loss = 0.12242662906646729
iteration 103, loss = 0.21645492315292358
iteration 104, loss = 0.10356733947992325
iteration 105, loss = 0.21515953540802002
iteration 106, loss = 0.16979607939720154
iteration 107, loss = 0.19186478853225708
iteration 108, loss = 0.2724335491657257
iteration 109, loss = 0.1421324610710144
iteration 110, loss = 0.3246411085128784
iteration 111, loss = 0.13757066428661346
iteration 112, loss = 0.1254083216190338
iteration 113, loss = 0.13198640942573547
iteration 114, loss = 0.13746435940265656
iteration 115, loss = 0.1783844232559204
iteration 116, loss = 0.13585418462753296
iteration 117, loss = 0.23212304711341858
iteration 118, loss = 0.19975458085536957
iteration 119, loss = 0.1599864810705185
iteration 120, loss = 0.19827121496200562
iteration 121, loss = 0.1059798076748848
iteration 122, loss = 0.18024469912052155
iteration 123, loss = 0.1719338744878769
iteration 124, loss = 0.10007781535387039
iteration 125, loss = 0.08920888602733612
iteration 126, loss = 0.132865309715271
iteration 127, loss = 0.1806267946958542
iteration 128, loss = 0.17840585112571716
iteration 129, loss = 0.12900501489639282
iteration 130, loss = 0.14062252640724182
iteration 131, loss = 0.20338024199008942
iteration 132, loss = 0.19726084172725677
iteration 133, loss = 0.1683761179447174
iteration 134, loss = 0.13169927895069122
iteration 135, loss = 0.09555395692586899
iteration 136, loss = 0.09373444318771362
iteration 137, loss = 0.1946965456008911
iteration 138, loss = 0.23109877109527588
iteration 139, loss = 0.07150135189294815
iteration 140, loss = 0.159401074051857
iteration 141, loss = 0.12376321852207184
iteration 142, loss = 0.16652601957321167
iteration 143, loss = 0.1999063938856125
iteration 144, loss = 0.14465247094631195
iteration 145, loss = 0.09199044108390808
iteration 146, loss = 0.12110769003629684
iteration 147, loss = 0.08623567968606949
iteration 148, loss = 0.2109067738056183
iteration 149, loss = 0.19399325549602509
iteration 0, loss = 0.09125307947397232
iteration 1, loss = 0.23464909195899963
iteration 2, loss = 0.21423184871673584
iteration 3, loss = 0.19304490089416504
iteration 4, loss = 0.15244778990745544
iteration 5, loss = 0.1544291228055954
iteration 6, loss = 0.18498462438583374
iteration 7, loss = 0.14284053444862366
iteration 8, loss = 0.1666378229856491
iteration 9, loss = 0.13349740207195282
iteration 10, loss = 0.15204860270023346
iteration 11, loss = 0.22965800762176514
iteration 12, loss = 0.07769758254289627
iteration 13, loss = 0.12896129488945007
iteration 14, loss = 0.1397746205329895
iteration 15, loss = 0.09651131182909012
iteration 16, loss = 0.1875976026058197
iteration 17, loss = 0.12990590929985046
iteration 18, loss = 0.21618333458900452
iteration 19, loss = 0.0721728727221489
iteration 20, loss = 0.27863937616348267
iteration 21, loss = 0.14966988563537598
iteration 22, loss = 0.2037704885005951
iteration 23, loss = 0.20701086521148682
iteration 24, loss = 0.0434662401676178
iteration 25, loss = 0.2609994411468506
iteration 26, loss = 0.1490320861339569
iteration 27, loss = 0.06895638257265091
iteration 28, loss = 0.06455577164888382
iteration 29, loss = 0.04933351278305054
iteration 30, loss = 0.10889840871095657
iteration 31, loss = 0.1270417869091034
iteration 32, loss = 0.12167978286743164
iteration 33, loss = 0.057743608951568604
iteration 34, loss = 0.1667228639125824
iteration 35, loss = 0.15042170882225037
iteration 36, loss = 0.09487806260585785
iteration 37, loss = 0.2606506049633026
iteration 38, loss = 0.11065544933080673
iteration 39, loss = 0.11104314774274826
iteration 40, loss = 0.16074462234973907
iteration 41, loss = 0.08459386229515076
iteration 42, loss = 0.09270644932985306
iteration 43, loss = 0.09689189493656158
iteration 44, loss = 0.16244491934776306
iteration 45, loss = 0.09742727130651474
iteration 46, loss = 0.061809975653886795
iteration 47, loss = 0.1487722247838974
iteration 48, loss = 0.11066986620426178
iteration 49, loss = 0.049339599907398224
iteration 50, loss = 0.13934524357318878
iteration 51, loss = 0.16310878098011017
iteration 52, loss = 0.08384345471858978
iteration 53, loss = 0.142556294798851
iteration 54, loss = 0.15936937928199768
iteration 55, loss = 0.09272821992635727
iteration 56, loss = 0.07059237360954285
iteration 57, loss = 0.05237021669745445
iteration 58, loss = 0.1991998851299286
iteration 59, loss = 0.03919745236635208
iteration 60, loss = 0.0957472175359726
iteration 61, loss = 0.07244553416967392
iteration 62, loss = 0.14219939708709717
iteration 63, loss = 0.09645110368728638
iteration 64, loss = 0.11467929184436798
iteration 65, loss = 0.05478590726852417
iteration 66, loss = 0.07081888616085052
iteration 67, loss = 0.09873402863740921
iteration 68, loss = 0.09125014394521713
iteration 69, loss = 0.09750954061746597
iteration 70, loss = 0.14017167687416077
iteration 71, loss = 0.2330569326877594
iteration 72, loss = 0.1391288936138153
iteration 73, loss = 0.19497555494308472
iteration 74, loss = 0.08080528676509857
iteration 75, loss = 0.12784433364868164
iteration 76, loss = 0.15765884518623352
iteration 77, loss = 0.20025907456874847
iteration 78, loss = 0.12786591053009033
iteration 79, loss = 0.10109236091375351
iteration 80, loss = 0.06496276706457138
iteration 81, loss = 0.13065126538276672
iteration 82, loss = 0.1420482099056244
iteration 83, loss = 0.09276384115219116
iteration 84, loss = 0.10032641142606735
iteration 85, loss = 0.18103739619255066
iteration 86, loss = 0.06203603371977806
iteration 87, loss = 0.12349102646112442
iteration 88, loss = 0.04247219115495682
iteration 89, loss = 0.12217320501804352
iteration 90, loss = 0.06608205288648605
iteration 91, loss = 0.06504108011722565
iteration 92, loss = 0.07635873556137085
iteration 93, loss = 0.17473752796649933
iteration 94, loss = 0.1188403069972992
iteration 95, loss = 0.09382594376802444
iteration 96, loss = 0.09045728296041489
iteration 97, loss = 0.1187802255153656
iteration 98, loss = 0.08858280628919601
iteration 99, loss = 0.13685724139213562
iteration 100, loss = 0.1081806942820549
iteration 101, loss = 0.14217764139175415
iteration 102, loss = 0.13695919513702393
iteration 103, loss = 0.10700754821300507
iteration 104, loss = 0.12027399241924286
iteration 105, loss = 0.07410964369773865
iteration 106, loss = 0.08812210708856583
iteration 107, loss = 0.13570824265480042
iteration 108, loss = 0.08570053428411484
iteration 109, loss = 0.13770142197608948
iteration 110, loss = 0.0246005617082119
iteration 111, loss = 0.05411228537559509
iteration 112, loss = 0.15682236850261688
iteration 113, loss = 0.07642226666212082
iteration 114, loss = 0.060844842344522476
iteration 115, loss = 0.0691659152507782
iteration 116, loss = 0.12416099011898041
iteration 117, loss = 0.10527344048023224
iteration 118, loss = 0.1308169662952423
iteration 119, loss = 0.12246449291706085
iteration 120, loss = 0.12732987105846405
iteration 121, loss = 0.17546924948692322
iteration 122, loss = 0.17088541388511658
iteration 123, loss = 0.1547650396823883
iteration 124, loss = 0.12195738404989243
iteration 125, loss = 0.10021663457155228
iteration 126, loss = 0.11138571798801422
iteration 127, loss = 0.08359117060899734
iteration 128, loss = 0.02781427837908268
iteration 129, loss = 0.05831294134259224
iteration 130, loss = 0.11309470981359482
iteration 131, loss = 0.09014615416526794
iteration 132, loss = 0.0635402575135231
iteration 133, loss = 0.13697445392608643
iteration 134, loss = 0.06278225034475327
iteration 135, loss = 0.06635993719100952
iteration 136, loss = 0.19829867780208588
iteration 137, loss = 0.05835747718811035
iteration 138, loss = 0.0535769984126091
iteration 139, loss = 0.08520238846540451
iteration 140, loss = 0.0314728282392025
iteration 141, loss = 0.22119775414466858
iteration 142, loss = 0.04389970004558563
iteration 143, loss = 0.16627268493175507
iteration 144, loss = 0.036780279129743576
iteration 145, loss = 0.03802938014268875
iteration 146, loss = 0.05669517070055008
iteration 147, loss = 0.016391189768910408
iteration 148, loss = 0.08547631651163101
iteration 149, loss = 0.11816519498825073
iteration 0, loss = 0.0271386057138443
iteration 1, loss = 0.04385371878743172
iteration 2, loss = 0.05561460927128792
iteration 3, loss = 0.03648556023836136
iteration 4, loss = 0.05953172966837883
iteration 5, loss = 0.19870233535766602
iteration 6, loss = 0.09092552959918976
iteration 7, loss = 0.16306230425834656
iteration 8, loss = 0.09734830260276794
iteration 9, loss = 0.16495434939861298
iteration 10, loss = 0.035807620733976364
iteration 11, loss = 0.17186078429222107
iteration 12, loss = 0.10856908559799194
iteration 13, loss = 0.09705755114555359
iteration 14, loss = 0.039848778396844864
iteration 15, loss = 0.04678576812148094
iteration 16, loss = 0.05227624997496605
iteration 17, loss = 0.11889636516571045
iteration 18, loss = 0.04617052525281906
iteration 19, loss = 0.0864713191986084
iteration 20, loss = 0.07450505346059799
iteration 21, loss = 0.0503092035651207
iteration 22, loss = 0.06819303333759308
iteration 23, loss = 0.02982719987630844
iteration 24, loss = 0.16177105903625488
iteration 25, loss = 0.12466344982385635
iteration 26, loss = 0.09266927093267441
iteration 27, loss = 0.041368380188941956
iteration 28, loss = 0.12929990887641907
iteration 29, loss = 0.028198761865496635
iteration 30, loss = 0.03662753850221634
iteration 31, loss = 0.07159269601106644
iteration 32, loss = 0.13203421235084534
iteration 33, loss = 0.12072537839412689
iteration 34, loss = 0.08983644098043442
iteration 35, loss = 0.12984591722488403
iteration 36, loss = 0.14089535176753998
iteration 37, loss = 0.03271776810288429
iteration 38, loss = 0.062003541737794876
iteration 39, loss = 0.1038280576467514
iteration 40, loss = 0.055022284388542175
iteration 41, loss = 0.10226821154356003
iteration 42, loss = 0.09641590714454651
iteration 43, loss = 0.040798429399728775
iteration 44, loss = 0.09113960713148117
iteration 45, loss = 0.1821175515651703
iteration 46, loss = 0.05224090814590454
iteration 47, loss = 0.09717293083667755
iteration 48, loss = 0.24987027049064636
iteration 49, loss = 0.0431440994143486
iteration 50, loss = 0.1082233339548111
iteration 51, loss = 0.12276482582092285
iteration 52, loss = 0.022976934909820557
iteration 53, loss = 0.0480843223631382
iteration 54, loss = 0.09408850967884064
iteration 55, loss = 0.03029993176460266
iteration 56, loss = 0.181441530585289
iteration 57, loss = 0.10800320655107498
iteration 58, loss = 0.0763828381896019
iteration 59, loss = 0.02605106122791767
iteration 60, loss = 0.02896391600370407
iteration 61, loss = 0.05952956900000572
iteration 62, loss = 0.038241058588027954
iteration 63, loss = 0.12662632763385773
iteration 64, loss = 0.1053960993885994
iteration 65, loss = 0.054536230862140656
iteration 66, loss = 0.07396278530359268
iteration 67, loss = 0.1861192286014557
iteration 68, loss = 0.06990957260131836
iteration 69, loss = 0.09354215860366821
iteration 70, loss = 0.0996776670217514
iteration 71, loss = 0.22205837070941925
iteration 72, loss = 0.0550432913005352
iteration 73, loss = 0.11853627860546112
iteration 74, loss = 0.10536988079547882
iteration 75, loss = 0.03655627369880676
iteration 76, loss = 0.018572788685560226
iteration 77, loss = 0.19192850589752197
iteration 78, loss = 0.08505862951278687
iteration 79, loss = 0.08381927013397217
iteration 80, loss = 0.021498732268810272
iteration 81, loss = 0.10197395086288452
iteration 82, loss = 0.028450416401028633
iteration 83, loss = 0.09829973429441452
iteration 84, loss = 0.0688435509800911
iteration 85, loss = 0.05831797793507576
iteration 86, loss = 0.1803533434867859
iteration 87, loss = 0.13959982991218567
iteration 88, loss = 0.16823357343673706
iteration 89, loss = 0.0339842364192009
iteration 90, loss = 0.022214122116565704
iteration 91, loss = 0.11385969817638397
iteration 92, loss = 0.045909229665994644
iteration 93, loss = 0.037421390414237976
iteration 94, loss = 0.02925664745271206
iteration 95, loss = 0.12139427661895752
iteration 96, loss = 0.049331314861774445
iteration 97, loss = 0.1165851354598999
iteration 98, loss = 0.1671503782272339
iteration 99, loss = 0.055621445178985596
iteration 100, loss = 0.03389047086238861
iteration 101, loss = 0.02661692351102829
iteration 102, loss = 0.03612295538187027
iteration 103, loss = 0.24158087372779846
iteration 104, loss = 0.030061418190598488
iteration 105, loss = 0.0976567417383194
iteration 106, loss = 0.119670569896698
iteration 107, loss = 0.06545334309339523
iteration 108, loss = 0.03478981554508209
iteration 109, loss = 0.07438100129365921
iteration 110, loss = 0.021806925535202026
iteration 111, loss = 0.13113734126091003
iteration 112, loss = 0.08291766047477722
iteration 113, loss = 0.037803374230861664
iteration 114, loss = 0.058022432029247284
iteration 115, loss = 0.1612992137670517
iteration 116, loss = 0.16447283327579498
iteration 117, loss = 0.04203064367175102
iteration 118, loss = 0.04799598827958107
iteration 119, loss = 0.028026357293128967
iteration 120, loss = 0.04792807996273041
iteration 121, loss = 0.049642741680145264
iteration 122, loss = 0.09499266743659973
iteration 123, loss = 0.030991805717349052
iteration 124, loss = 0.022900009527802467
iteration 125, loss = 0.07281608134508133
iteration 126, loss = 0.014481566846370697
iteration 127, loss = 0.14377135038375854
iteration 128, loss = 0.013908779248595238
iteration 129, loss = 0.061387404799461365
iteration 130, loss = 0.09736505150794983
iteration 131, loss = 0.12171884626150131
iteration 132, loss = 0.18733352422714233
iteration 133, loss = 0.09488198906183243
iteration 134, loss = 0.03026265650987625
iteration 135, loss = 0.03558195009827614
iteration 136, loss = 0.02723745070397854
iteration 137, loss = 0.09679029881954193
iteration 138, loss = 0.17321689426898956
iteration 139, loss = 0.05612727627158165
iteration 140, loss = 0.040435366332530975
iteration 141, loss = 0.010237367823719978
iteration 142, loss = 0.1346079558134079
iteration 143, loss = 0.2163083851337433
iteration 144, loss = 0.06765981018543243
iteration 145, loss = 0.08017989993095398
iteration 146, loss = 0.033550191670656204
iteration 147, loss = 0.0946325734257698
iteration 148, loss = 0.02680324949324131
iteration 149, loss = 0.10892167687416077
iteration 0, loss = 0.0931592732667923
iteration 1, loss = 0.04368622601032257
iteration 2, loss = 0.07918849587440491
iteration 3, loss = 0.01440748292952776
iteration 4, loss = 0.19739222526550293
iteration 5, loss = 0.09073764085769653
iteration 6, loss = 0.06233508884906769
iteration 7, loss = 0.0791231244802475
iteration 8, loss = 0.02030697651207447
iteration 9, loss = 0.03132236748933792
iteration 10, loss = 0.032861098647117615
iteration 11, loss = 0.1941370666027069
iteration 12, loss = 0.03931419178843498
iteration 13, loss = 0.08548466116189957
iteration 14, loss = 0.1028081476688385
iteration 15, loss = 0.18713423609733582
iteration 16, loss = 0.08939628303050995
iteration 17, loss = 0.03518383204936981
iteration 18, loss = 0.04171106591820717
iteration 19, loss = 0.018822507932782173
iteration 20, loss = 0.06442353874444962
iteration 21, loss = 0.026095297187566757
iteration 22, loss = 0.0801863968372345
iteration 23, loss = 0.1296176314353943
iteration 24, loss = 0.08766401559114456
iteration 25, loss = 0.08798157423734665
iteration 26, loss = 0.11445634067058563
iteration 27, loss = 0.023408517241477966
iteration 28, loss = 0.09797690808773041
iteration 29, loss = 0.05658898875117302
iteration 30, loss = 0.022663619369268417
iteration 31, loss = 0.16044174134731293
iteration 32, loss = 0.07207348197698593
iteration 33, loss = 0.011098607443273067
iteration 34, loss = 0.04394097626209259
iteration 35, loss = 0.06798935681581497
iteration 36, loss = 0.14832036197185516
iteration 37, loss = 0.058545831590890884
iteration 38, loss = 0.013492045924067497
iteration 39, loss = 0.12495104223489761
iteration 40, loss = 0.07919688522815704
iteration 41, loss = 0.06694129854440689
iteration 42, loss = 0.02057689242064953
iteration 43, loss = 0.12140510231256485
iteration 44, loss = 0.05079188570380211
iteration 45, loss = 0.017747582867741585
iteration 46, loss = 0.023034535348415375
iteration 47, loss = 0.0916728675365448
iteration 48, loss = 0.021527880802750587
iteration 49, loss = 0.08231639117002487
iteration 50, loss = 0.0573200061917305
iteration 51, loss = 0.028449617326259613
iteration 52, loss = 0.04131614416837692
iteration 53, loss = 0.011028869077563286
iteration 54, loss = 0.025735003873705864
iteration 55, loss = 0.0883084088563919
iteration 56, loss = 0.10357923805713654
iteration 57, loss = 0.04187510907649994
iteration 58, loss = 0.05175667256116867
iteration 59, loss = 0.11462017893791199
iteration 60, loss = 0.016413692384958267
iteration 61, loss = 0.09489257633686066
iteration 62, loss = 0.018988560885190964
iteration 63, loss = 0.10735325515270233
iteration 64, loss = 0.02256160043179989
iteration 65, loss = 0.08692097663879395
iteration 66, loss = 0.02036968059837818
iteration 67, loss = 0.18420515954494476
iteration 68, loss = 0.12053565680980682
iteration 69, loss = 0.07705088704824448
iteration 70, loss = 0.10264847427606583
iteration 71, loss = 0.09321649372577667
iteration 72, loss = 0.039128080010414124
iteration 73, loss = 0.007944915443658829
iteration 74, loss = 0.029799768701195717
iteration 75, loss = 0.10367108881473541
iteration 76, loss = 0.09479241073131561
iteration 77, loss = 0.00772967841476202
iteration 78, loss = 0.12446784228086472
iteration 79, loss = 0.22391794621944427
iteration 80, loss = 0.08022011816501617
iteration 81, loss = 0.016866426914930344
iteration 82, loss = 0.024288613349199295
iteration 83, loss = 0.04388165473937988
iteration 84, loss = 0.030503517016768456
iteration 85, loss = 0.07618414610624313
iteration 86, loss = 0.19767016172409058
iteration 87, loss = 0.11745426058769226
iteration 88, loss = 0.05545920133590698
iteration 89, loss = 0.08577380329370499
iteration 90, loss = 0.1644560843706131
iteration 91, loss = 0.011096948757767677
iteration 92, loss = 0.023249903693795204
iteration 93, loss = 0.03646068274974823
iteration 94, loss = 0.10636410862207413
iteration 95, loss = 0.03784199431538582
iteration 96, loss = 0.11464306712150574
iteration 97, loss = 0.09071115404367447
iteration 98, loss = 0.08361458778381348
iteration 99, loss = 0.030750244855880737
iteration 100, loss = 0.08496791124343872
iteration 101, loss = 0.027099084109067917
iteration 102, loss = 0.09180361777544022
iteration 103, loss = 0.011411648243665695
iteration 104, loss = 0.0950954481959343
iteration 105, loss = 0.11758444458246231
iteration 106, loss = 0.0926133394241333
iteration 107, loss = 0.0205125343054533
iteration 108, loss = 0.03897378221154213
iteration 109, loss = 0.02699587680399418
iteration 110, loss = 0.0871533751487732
iteration 111, loss = 0.04765094816684723
iteration 112, loss = 0.03777209296822548
iteration 113, loss = 0.0816507339477539
iteration 114, loss = 0.03565831482410431
iteration 115, loss = 0.030757049098610878
iteration 116, loss = 0.01460161991417408
iteration 117, loss = 0.028422463685274124
iteration 118, loss = 0.02799099124968052
iteration 119, loss = 0.024730850011110306
iteration 120, loss = 0.10149520635604858
iteration 121, loss = 0.009721897542476654
iteration 122, loss = 0.09449005872011185
iteration 123, loss = 0.03929610922932625
iteration 124, loss = 0.14251866936683655
iteration 125, loss = 0.028655683621764183
iteration 126, loss = 0.1626279056072235
iteration 127, loss = 0.042739156633615494
iteration 128, loss = 0.15226390957832336
iteration 129, loss = 0.0472370982170105
iteration 130, loss = 0.01181165874004364
iteration 131, loss = 0.09578593820333481
iteration 132, loss = 0.0209447480738163
iteration 133, loss = 0.014900211244821548
iteration 134, loss = 0.02934715524315834
iteration 135, loss = 0.03538445755839348
iteration 136, loss = 0.03777854144573212
iteration 137, loss = 0.04330285266041756
iteration 138, loss = 0.0873311311006546
iteration 139, loss = 0.0071575213223695755
iteration 140, loss = 0.16047559678554535
iteration 141, loss = 0.10243683308362961
iteration 142, loss = 0.005016871728003025
iteration 143, loss = 0.17124707996845245
iteration 144, loss = 0.06125316768884659
iteration 145, loss = 0.1261620819568634
iteration 146, loss = 0.20072965323925018
iteration 147, loss = 0.07576058059930801
iteration 148, loss = 0.09956799447536469
iteration 149, loss = 0.03591836243867874
iteration 0, loss = 0.07834155857563019
iteration 1, loss = 0.15075340867042542
iteration 2, loss = 0.20214976370334625
iteration 3, loss = 0.11036407947540283
iteration 4, loss = 0.16651776432991028
iteration 5, loss = 0.07937420159578323
iteration 6, loss = 0.04831387847661972
iteration 7, loss = 0.08625585585832596
iteration 8, loss = 0.026091638952493668
iteration 9, loss = 0.13546454906463623
iteration 10, loss = 0.10078588128089905
iteration 11, loss = 0.1713186353445053
iteration 12, loss = 0.12472034990787506
iteration 13, loss = 0.016343887895345688
iteration 14, loss = 0.00885738991200924
iteration 15, loss = 0.028914766386151314
iteration 16, loss = 0.022884642705321312
iteration 17, loss = 0.008016340434551239
iteration 18, loss = 0.2257215976715088
iteration 19, loss = 0.04217996820807457
iteration 20, loss = 0.07550925761461258
iteration 21, loss = 0.04653619974851608
iteration 22, loss = 0.08130691200494766
iteration 23, loss = 0.0269574336707592
iteration 24, loss = 0.0685751736164093
iteration 25, loss = 0.03786566108465195
iteration 26, loss = 0.08697324246168137
iteration 27, loss = 0.04671843349933624
iteration 28, loss = 0.08647607266902924
iteration 29, loss = 0.14025463163852692
iteration 30, loss = 0.02229391410946846
iteration 31, loss = 0.08973914384841919
iteration 32, loss = 0.1301766037940979
iteration 33, loss = 0.038369227200746536
iteration 34, loss = 0.07497125118970871
iteration 35, loss = 0.027979694306850433
iteration 36, loss = 0.04372906684875488
iteration 37, loss = 0.09712811559438705
iteration 38, loss = 0.026963375508785248
iteration 39, loss = 0.08955612033605576
iteration 40, loss = 0.03976131230592728
iteration 41, loss = 0.024577990174293518
iteration 42, loss = 0.08918476104736328
iteration 43, loss = 0.06994613260030746
iteration 44, loss = 0.01563814841210842
iteration 45, loss = 0.010455316863954067
iteration 46, loss = 0.08105994015932083
iteration 47, loss = 0.030816370621323586
iteration 48, loss = 0.01974472776055336
iteration 49, loss = 0.009975004009902477
iteration 50, loss = 0.020044950768351555
iteration 51, loss = 0.02410219796001911
iteration 52, loss = 0.031075241044163704
iteration 53, loss = 0.14224159717559814
iteration 54, loss = 0.04435175657272339
iteration 55, loss = 0.10328057408332825
iteration 56, loss = 0.07987485826015472
iteration 57, loss = 0.07193847000598907
iteration 58, loss = 0.07406249642372131
iteration 59, loss = 0.09006328135728836
iteration 60, loss = 0.01810341700911522
iteration 61, loss = 0.022931134328246117
iteration 62, loss = 0.011787219904363155
iteration 63, loss = 0.09894633293151855
iteration 64, loss = 0.06661418080329895
iteration 65, loss = 0.09663274884223938
iteration 66, loss = 0.04293295741081238
iteration 67, loss = 0.08375047892332077
iteration 68, loss = 0.011923110112547874
iteration 69, loss = 0.018926987424492836
iteration 70, loss = 0.01527206227183342
iteration 71, loss = 0.046740490943193436
iteration 72, loss = 0.07820415496826172
iteration 73, loss = 0.02797495760023594
iteration 74, loss = 0.02812862955033779
iteration 75, loss = 0.04166262969374657
iteration 76, loss = 0.024995887652039528
iteration 77, loss = 0.11690922826528549
iteration 78, loss = 0.1602250188589096
iteration 79, loss = 0.02384641394019127
iteration 80, loss = 0.02818405255675316
iteration 81, loss = 0.02720087580382824
iteration 82, loss = 0.1056840643286705
iteration 83, loss = 0.08636756241321564
iteration 84, loss = 0.04805900529026985
iteration 85, loss = 0.08633841574192047
iteration 86, loss = 0.1143324077129364
iteration 87, loss = 0.0791986882686615
iteration 88, loss = 0.006103454157710075
iteration 89, loss = 0.004193148110061884
iteration 90, loss = 0.07969580590724945
iteration 91, loss = 0.01724529266357422
iteration 92, loss = 0.01944265514612198
iteration 93, loss = 0.043658215552568436
iteration 94, loss = 0.08430173993110657
iteration 95, loss = 0.08695250749588013
iteration 96, loss = 0.15750408172607422
iteration 97, loss = 0.1037968099117279
iteration 98, loss = 0.021948128938674927
iteration 99, loss = 0.01952851004898548
iteration 100, loss = 0.08099763840436935
iteration 101, loss = 0.012553168460726738
iteration 102, loss = 0.03416583687067032
iteration 103, loss = 0.11231498420238495
iteration 104, loss = 0.021452508866786957
iteration 105, loss = 0.014817995019257069
iteration 106, loss = 0.05098472163081169
iteration 107, loss = 0.08582327514886856
iteration 108, loss = 0.029410092160105705
iteration 109, loss = 0.08047012239694595
iteration 110, loss = 0.021189792081713676
iteration 111, loss = 0.009946692734956741
iteration 112, loss = 0.013535752892494202
iteration 113, loss = 0.026497915387153625
iteration 114, loss = 0.06066402792930603
iteration 115, loss = 0.08088089525699615
iteration 116, loss = 0.01932581514120102
iteration 117, loss = 0.08705219626426697
iteration 118, loss = 0.08369389176368713
iteration 119, loss = 0.01266716793179512
iteration 120, loss = 0.22573503851890564
iteration 121, loss = 0.1582479476928711
iteration 122, loss = 0.0238141268491745
iteration 123, loss = 0.026249339804053307
iteration 124, loss = 0.07593762129545212
iteration 125, loss = 0.0852881669998169
iteration 126, loss = 0.010275816544890404
iteration 127, loss = 0.07587520778179169
iteration 128, loss = 0.008172882720828056
iteration 129, loss = 0.07979211211204529
iteration 130, loss = 0.014196937903761864
iteration 131, loss = 0.08035977929830551
iteration 132, loss = 0.023897793143987656
iteration 133, loss = 0.17575499415397644
iteration 134, loss = 0.07997306436300278
iteration 135, loss = 0.009451315738260746
iteration 136, loss = 0.07566754519939423
iteration 137, loss = 0.007995840162038803
iteration 138, loss = 0.01140985544770956
iteration 139, loss = 0.12070532888174057
iteration 140, loss = 0.020308969542384148
iteration 141, loss = 0.01738625392317772
iteration 142, loss = 0.016206279397010803
iteration 143, loss = 0.01395098865032196
iteration 144, loss = 0.07648864388465881
iteration 145, loss = 0.024895405396819115
iteration 146, loss = 0.033654022961854935
iteration 147, loss = 0.0022743886802345514
iteration 148, loss = 0.021394869312644005
iteration 149, loss = 0.07826563715934753

iteration 0, loss = 0.7308540344238281
iteration 1, loss = 0.7512425184249878
iteration 2, loss = 0.7486284971237183
iteration 3, loss = 0.7413527965545654
iteration 4, loss = 0.7509751319885254
iteration 5, loss = 0.7375754117965698
iteration 6, loss = 0.7374933958053589
iteration 7, loss = 0.7424183487892151
iteration 8, loss = 0.7458568811416626
iteration 9, loss = 0.7295849323272705
iteration 10, loss = 0.7331210970878601
iteration 11, loss = 0.7219051122665405
iteration 12, loss = 0.7273527383804321
iteration 13, loss = 0.7116822004318237
iteration 14, loss = 0.7430958151817322
iteration 15, loss = 0.719741702079773
iteration 16, loss = 0.7135987281799316
iteration 17, loss = 0.7058945298194885
iteration 18, loss = 0.7205269932746887
iteration 19, loss = 0.7092748880386353
iteration 20, loss = 0.7050409317016602
iteration 21, loss = 0.7142096161842346
iteration 22, loss = 0.7036843299865723
iteration 23, loss = 0.699847936630249
iteration 24, loss = 0.7010436058044434
iteration 25, loss = 0.7058807015419006
iteration 26, loss = 0.6886265277862549
iteration 27, loss = 0.6954346895217896
iteration 28, loss = 0.6980351209640503
iteration 29, loss = 0.6954046487808228
iteration 30, loss = 0.6754717826843262
iteration 31, loss = 0.6805087924003601
iteration 32, loss = 0.6723563075065613
iteration 33, loss = 0.6844074130058289
iteration 34, loss = 0.660401463508606
iteration 35, loss = 0.6894937753677368
iteration 36, loss = 0.6716628670692444
iteration 37, loss = 0.6850088834762573
iteration 38, loss = 0.6725059747695923
iteration 39, loss = 0.6649800539016724
iteration 40, loss = 0.6760684251785278
iteration 41, loss = 0.675310492515564
iteration 42, loss = 0.6588643789291382
iteration 43, loss = 0.6580533981323242
iteration 44, loss = 0.6428035497665405
iteration 45, loss = 0.6373931765556335
iteration 46, loss = 0.6536622047424316
iteration 47, loss = 0.6591119766235352
iteration 48, loss = 0.659630298614502
iteration 49, loss = 0.6605575084686279
iteration 50, loss = 0.6342846155166626
iteration 51, loss = 0.6406558156013489
iteration 52, loss = 0.622525691986084
iteration 53, loss = 0.6296262145042419
iteration 54, loss = 0.6443418860435486
iteration 55, loss = 0.6339558362960815
iteration 56, loss = 0.6447333097457886
iteration 57, loss = 0.6407731175422668
iteration 58, loss = 0.6298786401748657
iteration 59, loss = 0.6285154223442078
iteration 60, loss = 0.6383035182952881
iteration 61, loss = 0.6217846870422363
iteration 62, loss = 0.6141732931137085
iteration 63, loss = 0.6238356828689575
iteration 64, loss = 0.6297110319137573
iteration 65, loss = 0.6240478754043579
iteration 66, loss = 0.6309413909912109
iteration 67, loss = 0.6264888048171997
iteration 68, loss = 0.6097897291183472
iteration 69, loss = 0.6040458679199219
iteration 70, loss = 0.6084147691726685
iteration 71, loss = 0.6173871755599976
iteration 72, loss = 0.6103876829147339
iteration 73, loss = 0.6119413375854492
iteration 74, loss = 0.6139286756515503
iteration 75, loss = 0.5805398225784302
iteration 76, loss = 0.5977145433425903
iteration 77, loss = 0.6220064759254456
iteration 78, loss = 0.5898451805114746
iteration 79, loss = 0.6115409135818481
iteration 80, loss = 0.5986640453338623
iteration 81, loss = 0.5900356769561768
iteration 82, loss = 0.5874421000480652
iteration 83, loss = 0.607563316822052
iteration 84, loss = 0.6118279695510864
iteration 85, loss = 0.5876685976982117
iteration 86, loss = 0.5857332944869995
iteration 87, loss = 0.5911804437637329
iteration 88, loss = 0.5828412175178528
iteration 89, loss = 0.5776939392089844
iteration 90, loss = 0.5826200842857361
iteration 91, loss = 0.5771270990371704
iteration 92, loss = 0.5729655623435974
iteration 93, loss = 0.5812758803367615
iteration 94, loss = 0.5879833698272705
iteration 95, loss = 0.5778467655181885
iteration 96, loss = 0.5754238367080688
iteration 97, loss = 0.5906597375869751
iteration 98, loss = 0.5675803422927856
iteration 99, loss = 0.5837109684944153
iteration 100, loss = 0.5541179776191711
iteration 101, loss = 0.5669471025466919
iteration 102, loss = 0.5509405732154846
iteration 103, loss = 0.5741910934448242
iteration 104, loss = 0.5619900226593018
iteration 105, loss = 0.5620177388191223
iteration 106, loss = 0.5559593439102173
iteration 107, loss = 0.5483461022377014
iteration 108, loss = 0.5504822731018066
iteration 109, loss = 0.5530701875686646
iteration 110, loss = 0.5711266398429871
iteration 111, loss = 0.5351086854934692
iteration 112, loss = 0.5570718050003052
iteration 113, loss = 0.5377755165100098
iteration 114, loss = 0.5663458108901978
iteration 115, loss = 0.5510955452919006
iteration 116, loss = 0.5443174839019775
iteration 117, loss = 0.546151876449585
iteration 118, loss = 0.5447293519973755
iteration 119, loss = 0.5439064502716064
iteration 120, loss = 0.5472843050956726
iteration 121, loss = 0.5501706004142761
iteration 122, loss = 0.5636957883834839
iteration 123, loss = 0.5378217101097107
iteration 124, loss = 0.5393873453140259
iteration 125, loss = 0.5329641699790955
iteration 126, loss = 0.5415688157081604
iteration 127, loss = 0.541289746761322
iteration 128, loss = 0.5222891569137573
iteration 129, loss = 0.5707886219024658
iteration 130, loss = 0.5543550848960876
iteration 131, loss = 0.5176513195037842
iteration 132, loss = 0.5190133452415466
iteration 133, loss = 0.5178108811378479
iteration 134, loss = 0.5310909152030945
iteration 135, loss = 0.5243621468544006
iteration 136, loss = 0.5138615965843201
iteration 137, loss = 0.5080901384353638
iteration 138, loss = 0.5250650644302368
iteration 139, loss = 0.5130575895309448
iteration 140, loss = 0.5460978746414185
iteration 141, loss = 0.5116184949874878
iteration 142, loss = 0.5322499871253967
iteration 143, loss = 0.5149153470993042
iteration 144, loss = 0.5428687334060669
iteration 145, loss = 0.5268510580062866
iteration 146, loss = 0.5156823396682739
iteration 147, loss = 0.48925742506980896
iteration 148, loss = 0.5044243335723877
iteration 149, loss = 0.5261719822883606
iteration 150, loss = 0.5179962515830994
iteration 151, loss = 0.49483153223991394
iteration 152, loss = 0.5160523056983948
iteration 153, loss = 0.5092853307723999
iteration 154, loss = 0.5152561664581299
iteration 155, loss = 0.5096573829650879
iteration 156, loss = 0.49674826860427856
iteration 157, loss = 0.5074541568756104
iteration 158, loss = 0.514946699142456
iteration 159, loss = 0.5215504169464111
iteration 160, loss = 0.5002537965774536
iteration 161, loss = 0.4982876777648926
iteration 162, loss = 0.4912295341491699
iteration 163, loss = 0.5005223155021667
iteration 164, loss = 0.49396729469299316
iteration 165, loss = 0.5090306997299194
iteration 166, loss = 0.5006677508354187
iteration 167, loss = 0.5017821788787842
iteration 168, loss = 0.4886016845703125
iteration 169, loss = 0.49245864152908325
iteration 170, loss = 0.5075960159301758
iteration 171, loss = 0.5085744261741638
iteration 172, loss = 0.4903046488761902
iteration 173, loss = 0.5067267417907715
iteration 174, loss = 0.4671761393547058
iteration 175, loss = 0.48469215631484985
iteration 176, loss = 0.48846420645713806
iteration 177, loss = 0.4968295097351074
iteration 178, loss = 0.4991442561149597
iteration 179, loss = 0.5025176405906677
iteration 180, loss = 0.4887431859970093
iteration 181, loss = 0.49443840980529785
iteration 182, loss = 0.49225419759750366
iteration 183, loss = 0.466225802898407
iteration 184, loss = 0.48678290843963623
iteration 185, loss = 0.48591387271881104
iteration 186, loss = 0.4798696041107178
iteration 187, loss = 0.477710485458374
iteration 188, loss = 0.496507465839386
iteration 189, loss = 0.4809882640838623
iteration 190, loss = 0.4963197112083435
iteration 191, loss = 0.45979613065719604
iteration 192, loss = 0.4780052900314331
iteration 193, loss = 0.5014057755470276
iteration 194, loss = 0.5008413791656494
iteration 195, loss = 0.48853129148483276
iteration 196, loss = 0.4586006700992584
iteration 197, loss = 0.4728555679321289
iteration 198, loss = 0.5142086744308472
iteration 199, loss = 0.47767287492752075
iteration 200, loss = 0.4781304597854614
iteration 201, loss = 0.4484539330005646
iteration 202, loss = 0.4691988229751587
iteration 203, loss = 0.46015825867652893
iteration 204, loss = 0.46002310514450073
iteration 205, loss = 0.47706833481788635
iteration 206, loss = 0.46438106894493103
iteration 207, loss = 0.4678257703781128
iteration 208, loss = 0.4658357501029968
iteration 209, loss = 0.46167582273483276
iteration 210, loss = 0.4575565457344055
iteration 211, loss = 0.46921825408935547
iteration 212, loss = 0.45860737562179565
iteration 213, loss = 0.4554904103279114
iteration 214, loss = 0.46625518798828125
iteration 215, loss = 0.45598381757736206
iteration 216, loss = 0.46917200088500977
iteration 217, loss = 0.45200470089912415
iteration 218, loss = 0.44605931639671326
iteration 219, loss = 0.46190863847732544
iteration 220, loss = 0.4502977132797241
iteration 221, loss = 0.45995092391967773
iteration 222, loss = 0.45545780658721924
iteration 223, loss = 0.4251578450202942
iteration 224, loss = 0.4473772346973419
iteration 225, loss = 0.43630197644233704
iteration 226, loss = 0.42803674936294556
iteration 227, loss = 0.45092952251434326
iteration 228, loss = 0.4417854845523834
iteration 229, loss = 0.4555726647377014
iteration 230, loss = 0.44369611144065857
iteration 231, loss = 0.4504513144493103
iteration 232, loss = 0.4142319858074188
iteration 233, loss = 0.4739398658275604
iteration 234, loss = 0.4434617757797241
iteration 235, loss = 0.43490904569625854
iteration 236, loss = 0.42315900325775146
iteration 237, loss = 0.44019806385040283
iteration 238, loss = 0.42885151505470276
iteration 239, loss = 0.44608062505722046
iteration 240, loss = 0.4318998157978058
iteration 241, loss = 0.47704482078552246
iteration 242, loss = 0.4179680645465851
iteration 243, loss = 0.43710899353027344
iteration 244, loss = 0.4326891303062439
iteration 245, loss = 0.4455938935279846
iteration 246, loss = 0.4081430733203888
iteration 247, loss = 0.42473867535591125
iteration 248, loss = 0.45531830191612244
iteration 249, loss = 0.4209609031677246
iteration 250, loss = 0.4269917607307434
iteration 251, loss = 0.4311385750770569
iteration 252, loss = 0.4163813889026642
iteration 253, loss = 0.41518890857696533
iteration 254, loss = 0.4247962236404419
iteration 255, loss = 0.4493442475795746
iteration 256, loss = 0.40816542506217957
iteration 257, loss = 0.4310177266597748
iteration 258, loss = 0.43243980407714844
iteration 259, loss = 0.41062846779823303
iteration 260, loss = 0.42938464879989624
iteration 261, loss = 0.42994844913482666
iteration 262, loss = 0.4318680763244629
iteration 263, loss = 0.40448954701423645
iteration 264, loss = 0.4185173809528351
iteration 265, loss = 0.4254204034805298
iteration 266, loss = 0.43677449226379395
iteration 267, loss = 0.42568233609199524
iteration 268, loss = 0.4281212091445923
iteration 269, loss = 0.40707695484161377
iteration 270, loss = 0.41598910093307495
iteration 271, loss = 0.4154342710971832
iteration 272, loss = 0.4154491126537323
iteration 273, loss = 0.3970866799354553
iteration 274, loss = 0.4033980369567871
iteration 275, loss = 0.41509854793548584
iteration 276, loss = 0.4122212529182434
iteration 277, loss = 0.4238446354866028
iteration 278, loss = 0.39408472180366516
iteration 279, loss = 0.4008530378341675
iteration 280, loss = 0.42115116119384766
iteration 281, loss = 0.4116986393928528
iteration 282, loss = 0.3987443447113037
iteration 283, loss = 0.3935939073562622
iteration 284, loss = 0.40327221155166626
iteration 285, loss = 0.3961758315563202
iteration 286, loss = 0.41184815764427185
iteration 287, loss = 0.397612065076828
iteration 288, loss = 0.4114324152469635
iteration 289, loss = 0.41451552510261536
iteration 290, loss = 0.3879866600036621
iteration 291, loss = 0.39363399147987366
iteration 292, loss = 0.39088279008865356
iteration 293, loss = 0.3780975639820099
iteration 294, loss = 0.39766737818717957
iteration 295, loss = 0.38049522042274475
iteration 296, loss = 0.3835706114768982
iteration 297, loss = 0.40949636697769165
iteration 298, loss = 0.3884502053260803
iteration 299, loss = 0.3931368291378021
iteration 0, loss = 0.39057138562202454
iteration 1, loss = 0.38622575998306274
iteration 2, loss = 0.3842225670814514
iteration 3, loss = 0.37647098302841187
iteration 4, loss = 0.3644671142101288
iteration 5, loss = 0.3897078037261963
iteration 6, loss = 0.40467771887779236
iteration 7, loss = 0.3697214424610138
iteration 8, loss = 0.36831414699554443
iteration 9, loss = 0.37728965282440186
iteration 10, loss = 0.3710539638996124
iteration 11, loss = 0.3957427740097046
iteration 12, loss = 0.39288732409477234
iteration 13, loss = 0.37420910596847534
iteration 14, loss = 0.3732799291610718
iteration 15, loss = 0.3846045732498169
iteration 16, loss = 0.38070738315582275
iteration 17, loss = 0.36465615034103394
iteration 18, loss = 0.38912680745124817
iteration 19, loss = 0.3546326160430908
iteration 20, loss = 0.33959493041038513
iteration 21, loss = 0.3788622319698334
iteration 22, loss = 0.33650505542755127
iteration 23, loss = 0.3731962740421295
iteration 24, loss = 0.36408573389053345
iteration 25, loss = 0.37005001306533813
iteration 26, loss = 0.3544403612613678
iteration 27, loss = 0.3595730662345886
iteration 28, loss = 0.3690280318260193
iteration 29, loss = 0.35756319761276245
iteration 30, loss = 0.37709271907806396
iteration 31, loss = 0.3509633243083954
iteration 32, loss = 0.3746900260448456
iteration 33, loss = 0.3334081768989563
iteration 34, loss = 0.3557514548301697
iteration 35, loss = 0.36353984475135803
iteration 36, loss = 0.3632732629776001
iteration 37, loss = 0.3753114938735962
iteration 38, loss = 0.3538793921470642
iteration 39, loss = 0.3598524332046509
iteration 40, loss = 0.360750287771225
iteration 41, loss = 0.36370849609375
iteration 42, loss = 0.36019816994667053
iteration 43, loss = 0.36096638441085815
iteration 44, loss = 0.35017186403274536
iteration 45, loss = 0.3588654696941376
iteration 46, loss = 0.365028440952301
iteration 47, loss = 0.3397759795188904
iteration 48, loss = 0.3382997512817383
iteration 49, loss = 0.3319090008735657
iteration 50, loss = 0.3599992096424103
iteration 51, loss = 0.32526952028274536
iteration 52, loss = 0.35802358388900757
iteration 53, loss = 0.32295897603034973
iteration 54, loss = 0.3333035409450531
iteration 55, loss = 0.3261058032512665
iteration 56, loss = 0.33404144644737244
iteration 57, loss = 0.33251550793647766
iteration 58, loss = 0.3296930193901062
iteration 59, loss = 0.3493176996707916
iteration 60, loss = 0.3196743428707123
iteration 61, loss = 0.30220648646354675
iteration 62, loss = 0.34516459703445435
iteration 63, loss = 0.3593500256538391
iteration 64, loss = 0.3243096172809601
iteration 65, loss = 0.32910069823265076
iteration 66, loss = 0.3095567524433136
iteration 67, loss = 0.31761860847473145
iteration 68, loss = 0.34871307015419006
iteration 69, loss = 0.3549043536186218
iteration 70, loss = 0.33793511986732483
iteration 71, loss = 0.35323265194892883
iteration 72, loss = 0.3499639630317688
iteration 73, loss = 0.31106895208358765
iteration 74, loss = 0.34160301089286804
iteration 75, loss = 0.27691125869750977
iteration 76, loss = 0.3265567719936371
iteration 77, loss = 0.30132532119750977
iteration 78, loss = 0.34184738993644714
iteration 79, loss = 0.3839946389198303
iteration 80, loss = 0.3096570670604706
iteration 81, loss = 0.28708896040916443
iteration 82, loss = 0.32224494218826294
iteration 83, loss = 0.2890562415122986
iteration 84, loss = 0.3282797634601593
iteration 85, loss = 0.29834866523742676
iteration 86, loss = 0.3136172890663147
iteration 87, loss = 0.28545576333999634
iteration 88, loss = 0.3289392590522766
iteration 89, loss = 0.29020702838897705
iteration 90, loss = 0.329532265663147
iteration 91, loss = 0.29837292432785034
iteration 92, loss = 0.29536888003349304
iteration 93, loss = 0.34388217329978943
iteration 94, loss = 0.2954579293727875
iteration 95, loss = 0.2970445454120636
iteration 96, loss = 0.3218071162700653
iteration 97, loss = 0.2934081554412842
iteration 98, loss = 0.3224226236343384
iteration 99, loss = 0.2896862328052521
iteration 100, loss = 0.29731082916259766
iteration 101, loss = 0.295300155878067
iteration 102, loss = 0.30884429812431335
iteration 103, loss = 0.2855980396270752
iteration 104, loss = 0.32944628596305847
iteration 105, loss = 0.306380033493042
iteration 106, loss = 0.27936407923698425
iteration 107, loss = 0.32340434193611145
iteration 108, loss = 0.28286370635032654
iteration 109, loss = 0.29192522168159485
iteration 110, loss = 0.3111741244792938
iteration 111, loss = 0.2639172077178955
iteration 112, loss = 0.29930272698402405
iteration 113, loss = 0.30072763562202454
iteration 114, loss = 0.28471001982688904
iteration 115, loss = 0.30766481161117554
iteration 116, loss = 0.29420000314712524
iteration 117, loss = 0.291130006313324
iteration 118, loss = 0.31010541319847107
iteration 119, loss = 0.27802199125289917
iteration 120, loss = 0.28321781754493713
iteration 121, loss = 0.28365033864974976
iteration 122, loss = 0.279267281293869
iteration 123, loss = 0.2648512125015259
iteration 124, loss = 0.26796087622642517
iteration 125, loss = 0.2832545340061188
iteration 126, loss = 0.24814987182617188
iteration 127, loss = 0.25860342383384705
iteration 128, loss = 0.2897442579269409
iteration 129, loss = 0.27337315678596497
iteration 130, loss = 0.2838248908519745
iteration 131, loss = 0.25343239307403564
iteration 132, loss = 0.28727442026138306
iteration 133, loss = 0.24872098863124847
iteration 134, loss = 0.2626882791519165
iteration 135, loss = 0.28390392661094666
iteration 136, loss = 0.293739378452301
iteration 137, loss = 0.24618194997310638
iteration 138, loss = 0.2680363655090332
iteration 139, loss = 0.2698560357093811
iteration 140, loss = 0.2840760350227356
iteration 141, loss = 0.2693485617637634
iteration 142, loss = 0.2754223048686981
iteration 143, loss = 0.27487388253211975
iteration 144, loss = 0.25359487533569336
iteration 145, loss = 0.2972029149532318
iteration 146, loss = 0.27390265464782715
iteration 147, loss = 0.24695506691932678
iteration 148, loss = 0.2450970858335495
iteration 149, loss = 0.2613578736782074
iteration 150, loss = 0.278106689453125
iteration 151, loss = 0.28638187050819397
iteration 152, loss = 0.25583648681640625
iteration 153, loss = 0.26483362913131714
iteration 154, loss = 0.3010347783565521
iteration 155, loss = 0.2935824990272522
iteration 156, loss = 0.23220659792423248
iteration 157, loss = 0.23582608997821808
iteration 158, loss = 0.22853682935237885
iteration 159, loss = 0.26593464612960815
iteration 160, loss = 0.2825547754764557
iteration 161, loss = 0.2801913619041443
iteration 162, loss = 0.2614269256591797
iteration 163, loss = 0.2498679757118225
iteration 164, loss = 0.24072425067424774
iteration 165, loss = 0.25414618849754333
iteration 166, loss = 0.23825284838676453
iteration 167, loss = 0.22883819043636322
iteration 168, loss = 0.23518848419189453
iteration 169, loss = 0.2412523478269577
iteration 170, loss = 0.24564611911773682
iteration 171, loss = 0.25859779119491577
iteration 172, loss = 0.23465333878993988
iteration 173, loss = 0.2522965967655182
iteration 174, loss = 0.2350674271583557
iteration 175, loss = 0.22949491441249847
iteration 176, loss = 0.23432637751102448
iteration 177, loss = 0.2707832157611847
iteration 178, loss = 0.25968530774116516
iteration 179, loss = 0.24945437908172607
iteration 180, loss = 0.2646995484828949
iteration 181, loss = 0.2578587830066681
iteration 182, loss = 0.2240733951330185
iteration 183, loss = 0.2560178339481354
iteration 184, loss = 0.2304334044456482
iteration 185, loss = 0.2153891623020172
iteration 186, loss = 0.27898240089416504
iteration 187, loss = 0.26882556080818176
iteration 188, loss = 0.22043436765670776
iteration 189, loss = 0.21735559403896332
iteration 190, loss = 0.23958711326122284
iteration 191, loss = 0.24006985127925873
iteration 192, loss = 0.24343039095401764
iteration 193, loss = 0.20543651282787323
iteration 194, loss = 0.2450738251209259
iteration 195, loss = 0.2590843737125397
iteration 196, loss = 0.21549661457538605
iteration 197, loss = 0.2680029571056366
iteration 198, loss = 0.21597212553024292
iteration 199, loss = 0.21884825825691223
iteration 200, loss = 0.23409318923950195
iteration 201, loss = 0.2167622447013855
iteration 202, loss = 0.2217034250497818
iteration 203, loss = 0.21104714274406433
iteration 204, loss = 0.2353155016899109
iteration 205, loss = 0.22884821891784668
iteration 206, loss = 0.23031656444072723
iteration 207, loss = 0.23055322468280792
iteration 208, loss = 0.26529771089553833
iteration 209, loss = 0.20131637156009674
iteration 210, loss = 0.2191547006368637
iteration 211, loss = 0.19766557216644287
iteration 212, loss = 0.22808583080768585
iteration 213, loss = 0.199052631855011
iteration 214, loss = 0.22147636115550995
iteration 215, loss = 0.22024352848529816
iteration 216, loss = 0.20755526423454285
iteration 217, loss = 0.1994941085577011
iteration 218, loss = 0.2245090901851654
iteration 219, loss = 0.20990967750549316
iteration 220, loss = 0.2152971625328064
iteration 221, loss = 0.21367596089839935
iteration 222, loss = 0.2043853998184204
iteration 223, loss = 0.24252676963806152
iteration 224, loss = 0.2104998379945755
iteration 225, loss = 0.23170848190784454
iteration 226, loss = 0.1992427557706833
iteration 227, loss = 0.22735175490379333
iteration 228, loss = 0.20071116089820862
iteration 229, loss = 0.1916905641555786
iteration 230, loss = 0.2092808485031128
iteration 231, loss = 0.18973766267299652
iteration 232, loss = 0.1934596598148346
iteration 233, loss = 0.2177853286266327
iteration 234, loss = 0.21071787178516388
iteration 235, loss = 0.19661767780780792
iteration 236, loss = 0.20133182406425476
iteration 237, loss = 0.19689200818538666
iteration 238, loss = 0.2040691077709198
iteration 239, loss = 0.19511130452156067
iteration 240, loss = 0.2292887419462204
iteration 241, loss = 0.20227819681167603
iteration 242, loss = 0.19323143362998962
iteration 243, loss = 0.19779084622859955
iteration 244, loss = 0.1694023609161377
iteration 245, loss = 0.17138957977294922
iteration 246, loss = 0.22988778352737427
iteration 247, loss = 0.19706882536411285
iteration 248, loss = 0.1922260820865631
iteration 249, loss = 0.23228812217712402
iteration 250, loss = 0.1640387773513794
iteration 251, loss = 0.17279547452926636
iteration 252, loss = 0.18450713157653809
iteration 253, loss = 0.18987305462360382
iteration 254, loss = 0.19995172321796417
iteration 255, loss = 0.195175439119339
iteration 256, loss = 0.18752241134643555
iteration 257, loss = 0.19372764229774475
iteration 258, loss = 0.18077237904071808
iteration 259, loss = 0.1868656724691391
iteration 260, loss = 0.17344683408737183
iteration 261, loss = 0.19985230267047882
iteration 262, loss = 0.20251266658306122
iteration 263, loss = 0.15883901715278625
iteration 264, loss = 0.17118674516677856
iteration 265, loss = 0.18226505815982819
iteration 266, loss = 0.16413898766040802
iteration 267, loss = 0.19365806877613068
iteration 268, loss = 0.1659819334745407
iteration 269, loss = 0.19970431923866272
iteration 270, loss = 0.18988379836082458
iteration 271, loss = 0.15781806409358978
iteration 272, loss = 0.16986753046512604
iteration 273, loss = 0.16498202085494995
iteration 274, loss = 0.19619624316692352
iteration 275, loss = 0.2343648374080658
iteration 276, loss = 0.17657333612442017
iteration 277, loss = 0.20657292008399963
iteration 278, loss = 0.1926400065422058
iteration 279, loss = 0.15912576019763947
iteration 280, loss = 0.1805015504360199
iteration 281, loss = 0.19374501705169678
iteration 282, loss = 0.17535273730754852
iteration 283, loss = 0.1631564199924469
iteration 284, loss = 0.1934187412261963
iteration 285, loss = 0.16396236419677734
iteration 286, loss = 0.18039871752262115
iteration 287, loss = 0.17187915742397308
iteration 288, loss = 0.174397811293602
iteration 289, loss = 0.17457064986228943
iteration 290, loss = 0.16304230690002441
iteration 291, loss = 0.18056751787662506
iteration 292, loss = 0.17275771498680115
iteration 293, loss = 0.1649864763021469
iteration 294, loss = 0.16190311312675476
iteration 295, loss = 0.18166600167751312
iteration 296, loss = 0.18847431242465973
iteration 297, loss = 0.17968103289604187
iteration 298, loss = 0.16320867836475372
iteration 299, loss = 0.152754545211792
iteration 0, loss = 0.16967478394508362
iteration 1, loss = 0.1963299959897995
iteration 2, loss = 0.13690797984600067
iteration 3, loss = 0.17483732104301453
iteration 4, loss = 0.16862241923809052
iteration 5, loss = 0.16099905967712402
iteration 6, loss = 0.1536126732826233
iteration 7, loss = 0.15259775519371033
iteration 8, loss = 0.17518509924411774
iteration 9, loss = 0.17044317722320557
iteration 10, loss = 0.1788041740655899
iteration 11, loss = 0.15674175322055817
iteration 12, loss = 0.1357353776693344
iteration 13, loss = 0.17106084525585175
iteration 14, loss = 0.16249777376651764
iteration 15, loss = 0.1604490429162979
iteration 16, loss = 0.16562257707118988
iteration 17, loss = 0.1692093312740326
iteration 18, loss = 0.15737581253051758
iteration 19, loss = 0.16233676671981812
iteration 20, loss = 0.14879067242145538
iteration 21, loss = 0.15384653210639954
iteration 22, loss = 0.15858495235443115
iteration 23, loss = 0.15609970688819885
iteration 24, loss = 0.1478985995054245
iteration 25, loss = 0.156473308801651
iteration 26, loss = 0.1415528655052185
iteration 27, loss = 0.1445479691028595
iteration 28, loss = 0.14810968935489655
iteration 29, loss = 0.1421131193637848
iteration 30, loss = 0.16804379224777222
iteration 31, loss = 0.19363248348236084
iteration 32, loss = 0.13478392362594604
iteration 33, loss = 0.14784416556358337
iteration 34, loss = 0.1283729523420334
iteration 35, loss = 0.1536729782819748
iteration 36, loss = 0.12124311923980713
iteration 37, loss = 0.17465358972549438
iteration 38, loss = 0.153596892952919
iteration 39, loss = 0.1414402723312378
iteration 40, loss = 0.13977453112602234
iteration 41, loss = 0.14216411113739014
iteration 42, loss = 0.1847655475139618
iteration 43, loss = 0.12624746561050415
iteration 44, loss = 0.14553354680538177
iteration 45, loss = 0.15006721019744873
iteration 46, loss = 0.1542491614818573
iteration 47, loss = 0.13311097025871277
iteration 48, loss = 0.1371331810951233
iteration 49, loss = 0.1278344690799713
iteration 50, loss = 0.12766133248806
iteration 51, loss = 0.150029256939888
iteration 52, loss = 0.125865176320076
iteration 53, loss = 0.1525750309228897
iteration 54, loss = 0.12016937136650085
iteration 55, loss = 0.1286194622516632
iteration 56, loss = 0.135761097073555
iteration 57, loss = 0.1515447497367859
iteration 58, loss = 0.13607195019721985
iteration 59, loss = 0.1338142603635788
iteration 60, loss = 0.11999697983264923
iteration 61, loss = 0.13404527306556702
iteration 62, loss = 0.12801477313041687
iteration 63, loss = 0.1272648423910141
iteration 64, loss = 0.16442683339118958
iteration 65, loss = 0.14589087665081024
iteration 66, loss = 0.15199781954288483
iteration 67, loss = 0.12054667621850967
iteration 68, loss = 0.13604971766471863
iteration 69, loss = 0.12615324556827545
iteration 70, loss = 0.1351829618215561
iteration 71, loss = 0.1384391188621521
iteration 72, loss = 0.13787011802196503
iteration 73, loss = 0.1352059245109558
iteration 74, loss = 0.14143170416355133
iteration 75, loss = 0.13233083486557007
iteration 76, loss = 0.12187912315130234
iteration 77, loss = 0.12873362004756927
iteration 78, loss = 0.13157619535923004
iteration 79, loss = 0.1514308899641037
iteration 80, loss = 0.12171769142150879
iteration 81, loss = 0.12891390919685364
iteration 82, loss = 0.12629172205924988
iteration 83, loss = 0.1446564495563507
iteration 84, loss = 0.15867705643177032
iteration 85, loss = 0.14678138494491577
iteration 86, loss = 0.16437476873397827
iteration 87, loss = 0.1307920664548874
iteration 88, loss = 0.1207035556435585
iteration 89, loss = 0.10840509831905365
iteration 90, loss = 0.11984585225582123
iteration 91, loss = 0.11533936858177185
iteration 92, loss = 0.11428405344486237
iteration 93, loss = 0.12144354730844498
iteration 94, loss = 0.12684080004692078
iteration 95, loss = 0.12329848110675812
iteration 96, loss = 0.13155552744865417
iteration 97, loss = 0.11768659204244614
iteration 98, loss = 0.10911564528942108
iteration 99, loss = 0.11547086387872696
iteration 100, loss = 0.1354859322309494
iteration 101, loss = 0.128902867436409
iteration 102, loss = 0.12062262743711472
iteration 103, loss = 0.11574477702379227
iteration 104, loss = 0.14538294076919556
iteration 105, loss = 0.11274316161870956
iteration 106, loss = 0.13781686127185822
iteration 107, loss = 0.10383167117834091
iteration 108, loss = 0.11394575983285904
iteration 109, loss = 0.11456083506345749
iteration 110, loss = 0.122487373650074
iteration 111, loss = 0.14032095670700073
iteration 112, loss = 0.1282390058040619
iteration 113, loss = 0.0971226692199707
iteration 114, loss = 0.11037879437208176
iteration 115, loss = 0.1263735443353653
iteration 116, loss = 0.11442814767360687
iteration 117, loss = 0.10459008812904358
iteration 118, loss = 0.11775392293930054
iteration 119, loss = 0.11235912144184113
iteration 120, loss = 0.10379952192306519
iteration 121, loss = 0.11943204700946808
iteration 122, loss = 0.10916464030742645
iteration 123, loss = 0.11655953526496887
iteration 124, loss = 0.10309875011444092
iteration 125, loss = 0.11073526740074158
iteration 126, loss = 0.09890906512737274
iteration 127, loss = 0.10708051174879074
iteration 128, loss = 0.11475282162427902
iteration 129, loss = 0.1064402163028717
iteration 130, loss = 0.12916402518749237
iteration 131, loss = 0.1461113691329956
iteration 132, loss = 0.10017170011997223
iteration 133, loss = 0.11271536350250244
iteration 134, loss = 0.08766865730285645
iteration 135, loss = 0.12306775897741318
iteration 136, loss = 0.11900997906923294
iteration 137, loss = 0.10210572928190231
iteration 138, loss = 0.1182292103767395
iteration 139, loss = 0.10408394038677216
iteration 140, loss = 0.1163339614868164
iteration 141, loss = 0.10382907092571259
iteration 142, loss = 0.12098349630832672
iteration 143, loss = 0.09246975928544998
iteration 144, loss = 0.09843560308218002
iteration 145, loss = 0.11118944734334946
iteration 146, loss = 0.09836779534816742
iteration 147, loss = 0.09549888968467712
iteration 148, loss = 0.10147955268621445
iteration 149, loss = 0.08413520455360413
iteration 150, loss = 0.10094063729047775
iteration 151, loss = 0.14022117853164673
iteration 152, loss = 0.10481187701225281
iteration 153, loss = 0.09211915731430054
iteration 154, loss = 0.10584095120429993
iteration 155, loss = 0.10300691425800323
iteration 156, loss = 0.10829498618841171
iteration 157, loss = 0.08708899468183517
iteration 158, loss = 0.09911827743053436
iteration 159, loss = 0.098105788230896
iteration 160, loss = 0.10612417012453079
iteration 161, loss = 0.11323752999305725
iteration 162, loss = 0.08790273219347
iteration 163, loss = 0.1096668466925621
iteration 164, loss = 0.09092738479375839
iteration 165, loss = 0.10099955648183823
iteration 166, loss = 0.11558035761117935
iteration 167, loss = 0.11767470836639404
iteration 168, loss = 0.0984373614192009
iteration 169, loss = 0.08507984131574631
iteration 170, loss = 0.10368683189153671
iteration 171, loss = 0.09535684436559677
iteration 172, loss = 0.09648139774799347
iteration 173, loss = 0.11021407693624496
iteration 174, loss = 0.08933230489492416
iteration 175, loss = 0.1119372770190239
iteration 176, loss = 0.09281954169273376
iteration 177, loss = 0.08588182926177979
iteration 178, loss = 0.08982519060373306
iteration 179, loss = 0.09307486563920975
iteration 180, loss = 0.09559648483991623
iteration 181, loss = 0.10014990717172623
iteration 182, loss = 0.08947399258613586
iteration 183, loss = 0.09473998844623566
iteration 184, loss = 0.09514112025499344
iteration 185, loss = 0.1088067963719368
iteration 186, loss = 0.09408831596374512
iteration 187, loss = 0.08416839689016342
iteration 188, loss = 0.08036334067583084
iteration 189, loss = 0.10205281525850296
iteration 190, loss = 0.0954519510269165
iteration 191, loss = 0.08439953625202179
iteration 192, loss = 0.09317364543676376
iteration 193, loss = 0.08818833529949188
iteration 194, loss = 0.10554526001214981
iteration 195, loss = 0.10161181539297104
iteration 196, loss = 0.10103049874305725
iteration 197, loss = 0.09229018539190292
iteration 198, loss = 0.14805401861667633
iteration 199, loss = 0.0838397964835167
iteration 200, loss = 0.0853504166007042
iteration 201, loss = 0.09097077697515488
iteration 202, loss = 0.09148131310939789
iteration 203, loss = 0.08713650703430176
iteration 204, loss = 0.09933086484670639
iteration 205, loss = 0.08803246170282364
iteration 206, loss = 0.09034428000450134
iteration 207, loss = 0.08628331869840622
iteration 208, loss = 0.08302441984415054
iteration 209, loss = 0.09425292909145355
iteration 210, loss = 0.1364581286907196
iteration 211, loss = 0.09085560590028763
iteration 212, loss = 0.08658500015735626
iteration 213, loss = 0.08838851749897003
iteration 214, loss = 0.0781368836760521
iteration 215, loss = 0.07937385886907578
iteration 216, loss = 0.0878693014383316
iteration 217, loss = 0.08633100986480713
iteration 218, loss = 0.07348482310771942
iteration 219, loss = 0.07320507615804672
iteration 220, loss = 0.11191312223672867
iteration 221, loss = 0.0739770382642746
iteration 222, loss = 0.07945361733436584
iteration 223, loss = 0.08670254051685333
iteration 224, loss = 0.07814650237560272
iteration 225, loss = 0.07604318857192993
iteration 226, loss = 0.07755839079618454
iteration 227, loss = 0.07860907912254333
iteration 228, loss = 0.0757656842470169
iteration 229, loss = 0.0724741667509079
iteration 230, loss = 0.07731875777244568
iteration 231, loss = 0.09107377380132675
iteration 232, loss = 0.06928837299346924
iteration 233, loss = 0.09298025071620941
iteration 234, loss = 0.12607355415821075
iteration 235, loss = 0.08098138123750687
iteration 236, loss = 0.07956376671791077
iteration 237, loss = 0.09581945836544037
iteration 238, loss = 0.08550968021154404
iteration 239, loss = 0.0772826299071312
iteration 240, loss = 0.06536289304494858
iteration 241, loss = 0.09057999402284622
iteration 242, loss = 0.06979553401470184
iteration 243, loss = 0.08595355600118637
iteration 244, loss = 0.08302940428256989
iteration 245, loss = 0.08722352236509323
iteration 246, loss = 0.08043481409549713
iteration 247, loss = 0.09652132540941238
iteration 248, loss = 0.08863499015569687
iteration 249, loss = 0.10439412295818329
iteration 250, loss = 0.07102504372596741
iteration 251, loss = 0.08196649700403214
iteration 252, loss = 0.07415258139371872
iteration 253, loss = 0.07376158982515335
iteration 254, loss = 0.08589943498373032
iteration 255, loss = 0.07315219193696976
iteration 256, loss = 0.07341589778661728
iteration 257, loss = 0.07276320457458496
iteration 258, loss = 0.07581301033496857
iteration 259, loss = 0.072738416492939
iteration 260, loss = 0.08654334396123886
iteration 261, loss = 0.06095653027296066
iteration 262, loss = 0.0713881328701973
iteration 263, loss = 0.09642518311738968
iteration 264, loss = 0.07546262443065643
iteration 265, loss = 0.0643635243177414
iteration 266, loss = 0.08223148435354233
iteration 267, loss = 0.06365694105625153
iteration 268, loss = 0.06234915927052498
iteration 269, loss = 0.07960812002420425
iteration 270, loss = 0.07180530577898026
iteration 271, loss = 0.08006639033555984
iteration 272, loss = 0.06141485646367073
iteration 273, loss = 0.06345278769731522
iteration 274, loss = 0.07586653530597687
iteration 275, loss = 0.06962590664625168
iteration 276, loss = 0.06621906906366348
iteration 277, loss = 0.06438849866390228
iteration 278, loss = 0.059581220149993896
iteration 279, loss = 0.05855854973196983
iteration 280, loss = 0.059544190764427185
iteration 281, loss = 0.08104260265827179
iteration 282, loss = 0.06396351754665375
iteration 283, loss = 0.06516195088624954
iteration 284, loss = 0.07089173048734665
iteration 285, loss = 0.070933997631073
iteration 286, loss = 0.07673271000385284
iteration 287, loss = 0.07514972984790802
iteration 288, loss = 0.06735513359308243
iteration 289, loss = 0.0909833163022995
iteration 290, loss = 0.08842004835605621
iteration 291, loss = 0.086736761033535
iteration 292, loss = 0.1035127267241478
iteration 293, loss = 0.06212785467505455
iteration 294, loss = 0.0641750767827034
iteration 295, loss = 0.07010042667388916
iteration 296, loss = 0.060393061488866806
iteration 297, loss = 0.10045750439167023
iteration 298, loss = 0.060771044343709946
iteration 299, loss = 0.06620859354734421
iteration 0, loss = 0.06859113276004791
iteration 1, loss = 0.06614244729280472
iteration 2, loss = 0.05901392921805382
iteration 3, loss = 0.07141586393117905
iteration 4, loss = 0.06669948250055313
iteration 5, loss = 0.09206458181142807
iteration 6, loss = 0.06541457772254944
iteration 7, loss = 0.05730463191866875
iteration 8, loss = 0.05582166463136673
iteration 9, loss = 0.07941801100969315
iteration 10, loss = 0.06894262135028839
iteration 11, loss = 0.06155776232481003
iteration 12, loss = 0.05827324092388153
iteration 13, loss = 0.056822724640369415
iteration 14, loss = 0.06808154284954071
iteration 15, loss = 0.06586901843547821
iteration 16, loss = 0.10237634181976318
iteration 17, loss = 0.06590045988559723
iteration 18, loss = 0.07079028338193893
iteration 19, loss = 0.0928381085395813
iteration 20, loss = 0.07382869720458984
iteration 21, loss = 0.08089683949947357
iteration 22, loss = 0.06936324387788773
iteration 23, loss = 0.06349194794893265
iteration 24, loss = 0.0727299302816391
iteration 25, loss = 0.08756443113088608
iteration 26, loss = 0.07575958222150803
iteration 27, loss = 0.06521663069725037
iteration 28, loss = 0.056053128093481064
iteration 29, loss = 0.09317938983440399
iteration 30, loss = 0.055801644921302795
iteration 31, loss = 0.05792873352766037
iteration 32, loss = 0.05367258936166763
iteration 33, loss = 0.05574708804488182
iteration 34, loss = 0.0653434693813324
iteration 35, loss = 0.05831090733408928
iteration 36, loss = 0.052772168070077896
iteration 37, loss = 0.08270557224750519
iteration 38, loss = 0.05183998495340347
iteration 39, loss = 0.05237549915909767
iteration 40, loss = 0.09600900113582611
iteration 41, loss = 0.06383693218231201
iteration 42, loss = 0.0699927881360054
iteration 43, loss = 0.079654261469841
iteration 44, loss = 0.08536114543676376
iteration 45, loss = 0.06121376156806946
iteration 46, loss = 0.06766115874052048
iteration 47, loss = 0.05560579523444176
iteration 48, loss = 0.05305615812540054
iteration 49, loss = 0.05006899684667587
iteration 50, loss = 0.05930619686841965
iteration 51, loss = 0.06780911237001419
iteration 52, loss = 0.06673271209001541
iteration 53, loss = 0.061685800552368164
iteration 54, loss = 0.05456269532442093
iteration 55, loss = 0.055026981979608536
iteration 56, loss = 0.05492493510246277
iteration 57, loss = 0.054587945342063904
iteration 58, loss = 0.058534905314445496
iteration 59, loss = 0.050122927874326706
iteration 60, loss = 0.06079133227467537
iteration 61, loss = 0.05482355132699013
iteration 62, loss = 0.06172813102602959
iteration 63, loss = 0.06162441521883011
iteration 64, loss = 0.05377625301480293
iteration 65, loss = 0.050256527960300446
iteration 66, loss = 0.05492645502090454
iteration 67, loss = 0.04765857011079788
iteration 68, loss = 0.05830567330121994
iteration 69, loss = 0.06683418154716492
iteration 70, loss = 0.05188993737101555
iteration 71, loss = 0.08113036304712296
iteration 72, loss = 0.05359512194991112
iteration 73, loss = 0.050779782235622406
iteration 74, loss = 0.05579099431633949
iteration 75, loss = 0.060549721121788025
iteration 76, loss = 0.07573678344488144
iteration 77, loss = 0.06458861380815506
iteration 78, loss = 0.056463707238435745
iteration 79, loss = 0.05736222490668297
iteration 80, loss = 0.049489788711071014
iteration 81, loss = 0.057559482753276825
iteration 82, loss = 0.07004383206367493
iteration 83, loss = 0.06426431238651276
iteration 84, loss = 0.048810239881277084
iteration 85, loss = 0.06085158884525299
iteration 86, loss = 0.05044696852564812
iteration 87, loss = 0.050769805908203125
iteration 88, loss = 0.049220018088817596
iteration 89, loss = 0.05656706169247627
iteration 90, loss = 0.05175285413861275
iteration 91, loss = 0.051942527294158936
iteration 92, loss = 0.05183961242437363
iteration 93, loss = 0.04506121575832367
iteration 94, loss = 0.04843404144048691
iteration 95, loss = 0.0639304667711258
iteration 96, loss = 0.05334446579217911
iteration 97, loss = 0.047616831958293915
iteration 98, loss = 0.05520961433649063
iteration 99, loss = 0.04861847311258316
iteration 100, loss = 0.052604854106903076
iteration 101, loss = 0.04622805491089821
iteration 102, loss = 0.04899635165929794
iteration 103, loss = 0.05843113362789154
iteration 104, loss = 0.04614770784974098
iteration 105, loss = 0.07591380178928375
iteration 106, loss = 0.08133640885353088
iteration 107, loss = 0.04236356541514397
iteration 108, loss = 0.04752921685576439
iteration 109, loss = 0.04743565618991852
iteration 110, loss = 0.04918552562594414
iteration 111, loss = 0.052501317113637924
iteration 112, loss = 0.05317085236310959
iteration 113, loss = 0.07485310733318329
iteration 114, loss = 0.05384191498160362
iteration 115, loss = 0.052103158086538315
iteration 116, loss = 0.04843669757246971
iteration 117, loss = 0.043781399726867676
iteration 118, loss = 0.048879217356443405
iteration 119, loss = 0.06225087493658066
iteration 120, loss = 0.0480223149061203
iteration 121, loss = 0.04457128793001175
iteration 122, loss = 0.04597686603665352
iteration 123, loss = 0.053687792271375656
iteration 124, loss = 0.04229401424527168
iteration 125, loss = 0.05713164806365967
iteration 126, loss = 0.04918675497174263
iteration 127, loss = 0.041215185075998306
iteration 128, loss = 0.04363732784986496
iteration 129, loss = 0.04684437811374664
iteration 130, loss = 0.04188079386949539
iteration 131, loss = 0.0488341823220253
iteration 132, loss = 0.045577604323625565
iteration 133, loss = 0.05643479526042938
iteration 134, loss = 0.04655061289668083
iteration 135, loss = 0.06687697768211365
iteration 136, loss = 0.036848973482847214
iteration 137, loss = 0.06829742342233658
iteration 138, loss = 0.05038333311676979
iteration 139, loss = 0.04276642948389053
iteration 140, loss = 0.04881835728883743
iteration 141, loss = 0.04330284893512726
iteration 142, loss = 0.04753823205828667
iteration 143, loss = 0.05202900990843773
iteration 144, loss = 0.04280031844973564
iteration 145, loss = 0.042431917041540146
iteration 146, loss = 0.05141657963395119
iteration 147, loss = 0.040087148547172546
iteration 148, loss = 0.04684901237487793
iteration 149, loss = 0.040090687572956085
iteration 150, loss = 0.047492485493421555
iteration 151, loss = 0.04715923219919205
iteration 152, loss = 0.04032142832875252
iteration 153, loss = 0.05387283116579056
iteration 154, loss = 0.04658076912164688
iteration 155, loss = 0.04834776371717453
iteration 156, loss = 0.043338604271411896
iteration 157, loss = 0.05091063678264618
iteration 158, loss = 0.04558023437857628
iteration 159, loss = 0.038847751915454865
iteration 160, loss = 0.05474930629134178
iteration 161, loss = 0.04275289177894592
iteration 162, loss = 0.04852886497974396
iteration 163, loss = 0.04014258086681366
iteration 164, loss = 0.042304232716560364
iteration 165, loss = 0.04040548950433731
iteration 166, loss = 0.0367143377661705
iteration 167, loss = 0.049611128866672516
iteration 168, loss = 0.06091136112809181
iteration 169, loss = 0.04386827349662781
iteration 170, loss = 0.05366355925798416
iteration 171, loss = 0.04862423986196518
iteration 172, loss = 0.044075191020965576
iteration 173, loss = 0.044924188405275345
iteration 174, loss = 0.03904161602258682
iteration 175, loss = 0.046528175473213196
iteration 176, loss = 0.05927326902747154
iteration 177, loss = 0.04573649913072586
iteration 178, loss = 0.04641902074217796
iteration 179, loss = 0.04598165675997734
iteration 180, loss = 0.037434790283441544
iteration 181, loss = 0.04644356295466423
iteration 182, loss = 0.06155899912118912
iteration 183, loss = 0.0378996916115284
iteration 184, loss = 0.04266960918903351
iteration 185, loss = 0.06457624584436417
iteration 186, loss = 0.04800202697515488
iteration 187, loss = 0.04474903270602226
iteration 188, loss = 0.0354553647339344
iteration 189, loss = 0.03547153249382973
iteration 190, loss = 0.05108961835503578
iteration 191, loss = 0.041751760989427567
iteration 192, loss = 0.04443188011646271
iteration 193, loss = 0.03647833317518234
iteration 194, loss = 0.033833131194114685
iteration 195, loss = 0.03862530365586281
iteration 196, loss = 0.043442532420158386
iteration 197, loss = 0.043477751314640045
iteration 198, loss = 0.06197338551282883
iteration 199, loss = 0.04417959228157997
iteration 200, loss = 0.03557353466749191
iteration 201, loss = 0.04120690003037453
iteration 202, loss = 0.04050680249929428
iteration 203, loss = 0.035483263432979584
iteration 204, loss = 0.03837753087282181
iteration 205, loss = 0.05618121102452278
iteration 206, loss = 0.0358787439763546
iteration 207, loss = 0.03455613553524017
iteration 208, loss = 0.043016210198402405
iteration 209, loss = 0.04913610965013504
iteration 210, loss = 0.03704984113574028
iteration 211, loss = 0.03927221894264221
iteration 212, loss = 0.039145328104496
iteration 213, loss = 0.03745722025632858
iteration 214, loss = 0.03719550743699074
iteration 215, loss = 0.036663301289081573
iteration 216, loss = 0.03983922302722931
iteration 217, loss = 0.030113082379102707
iteration 218, loss = 0.04403415694832802
iteration 219, loss = 0.06490278244018555
iteration 220, loss = 0.03996972367167473
iteration 221, loss = 0.06275532394647598
iteration 222, loss = 0.030078714713454247
iteration 223, loss = 0.04400547966361046
iteration 224, loss = 0.03748030960559845
iteration 225, loss = 0.03972725570201874
iteration 226, loss = 0.0371665395796299
iteration 227, loss = 0.03075418993830681
iteration 228, loss = 0.03878733515739441
iteration 229, loss = 0.037555012851953506
iteration 230, loss = 0.04849807918071747
iteration 231, loss = 0.04686335474252701
iteration 232, loss = 0.04071924090385437
iteration 233, loss = 0.036780573427677155
iteration 234, loss = 0.062363121658563614
iteration 235, loss = 0.04122389853000641
iteration 236, loss = 0.039142534136772156
iteration 237, loss = 0.037072282284498215
iteration 238, loss = 0.03490961343050003
iteration 239, loss = 0.03422484174370766
iteration 240, loss = 0.03153248876333237
iteration 241, loss = 0.03467947989702225
iteration 242, loss = 0.04073819890618324
iteration 243, loss = 0.03354300558567047
iteration 244, loss = 0.03838634863495827
iteration 245, loss = 0.03684563934803009
iteration 246, loss = 0.03517798334360123
iteration 247, loss = 0.03799513354897499
iteration 248, loss = 0.03320244327187538
iteration 249, loss = 0.03116518072783947
iteration 250, loss = 0.028974147513508797
iteration 251, loss = 0.03980156034231186
iteration 252, loss = 0.04331463575363159
iteration 253, loss = 0.03539000824093819
iteration 254, loss = 0.041546642780303955
iteration 255, loss = 0.03380824625492096
iteration 256, loss = 0.03377964720129967
iteration 257, loss = 0.03876972571015358
iteration 258, loss = 0.04189128801226616
iteration 259, loss = 0.05305297672748566
iteration 260, loss = 0.03611015900969505
iteration 261, loss = 0.04212995618581772
iteration 262, loss = 0.03483336791396141
iteration 263, loss = 0.031090619042515755
iteration 264, loss = 0.03453456610441208
iteration 265, loss = 0.03864362835884094
iteration 266, loss = 0.038047872483730316
iteration 267, loss = 0.03936263173818588
iteration 268, loss = 0.03454229608178139
iteration 269, loss = 0.0307321660220623
iteration 270, loss = 0.030031435191631317
iteration 271, loss = 0.030027735978364944
iteration 272, loss = 0.052608147263526917
iteration 273, loss = 0.035842541605234146
iteration 274, loss = 0.03545135632157326
iteration 275, loss = 0.033589597791433334
iteration 276, loss = 0.038776516914367676
iteration 277, loss = 0.03108695149421692
iteration 278, loss = 0.05516446754336357
iteration 279, loss = 0.05203396454453468
iteration 280, loss = 0.03424975275993347
iteration 281, loss = 0.029201971367001534
iteration 282, loss = 0.0325196348130703
iteration 283, loss = 0.035266272723674774
iteration 284, loss = 0.031417232006788254
iteration 285, loss = 0.030347609892487526
iteration 286, loss = 0.03324761241674423
iteration 287, loss = 0.041984282433986664
iteration 288, loss = 0.026948735117912292
iteration 289, loss = 0.034416139125823975
iteration 290, loss = 0.02991807460784912
iteration 291, loss = 0.03312470763921738
iteration 292, loss = 0.038119085133075714
iteration 293, loss = 0.03254334628582001
iteration 294, loss = 0.0334041565656662
iteration 295, loss = 0.0395360067486763
iteration 296, loss = 0.03337111696600914
iteration 297, loss = 0.03669848293066025
iteration 298, loss = 0.03300044313073158
iteration 299, loss = 0.05719148367643356
iteration 0, loss = 0.03189995512366295
iteration 1, loss = 0.03212432935833931
iteration 2, loss = 0.02670661173760891
iteration 3, loss = 0.03327979892492294
iteration 4, loss = 0.05100206658244133
iteration 5, loss = 0.031211884692311287
iteration 6, loss = 0.036699745804071426
iteration 7, loss = 0.033280692994594574
iteration 8, loss = 0.030485844239592552
iteration 9, loss = 0.0324850007891655
iteration 10, loss = 0.02866196259856224
iteration 11, loss = 0.026191575452685356
iteration 12, loss = 0.035406190901994705
iteration 13, loss = 0.029813988134264946
iteration 14, loss = 0.03175006061792374
iteration 15, loss = 0.027295252308249474
iteration 16, loss = 0.0505325086414814
iteration 17, loss = 0.03560269623994827
iteration 18, loss = 0.03567725047469139
iteration 19, loss = 0.06398967653512955
iteration 20, loss = 0.04759633541107178
iteration 21, loss = 0.03533334657549858
iteration 22, loss = 0.04294243082404137
iteration 23, loss = 0.030857212841510773
iteration 24, loss = 0.04907485470175743
iteration 25, loss = 0.029329143464565277
iteration 26, loss = 0.03616970032453537
iteration 27, loss = 0.03377194330096245
iteration 28, loss = 0.03337501734495163
iteration 29, loss = 0.02746846340596676
iteration 30, loss = 0.025584276765584946
iteration 31, loss = 0.03647706285119057
iteration 32, loss = 0.038136910647153854
iteration 33, loss = 0.02354375645518303
iteration 34, loss = 0.027410490438342094
iteration 35, loss = 0.03221490979194641
iteration 36, loss = 0.033268243074417114
iteration 37, loss = 0.03769867494702339
iteration 38, loss = 0.030799731612205505
iteration 39, loss = 0.0474344901740551
iteration 40, loss = 0.026676760986447334
iteration 41, loss = 0.030497238039970398
iteration 42, loss = 0.0298909991979599
iteration 43, loss = 0.02736527845263481
iteration 44, loss = 0.026101982221007347
iteration 45, loss = 0.026813622564077377
iteration 46, loss = 0.029647041112184525
iteration 47, loss = 0.025108536705374718
iteration 48, loss = 0.05251294746994972
iteration 49, loss = 0.028107069432735443
iteration 50, loss = 0.025996636599302292
iteration 51, loss = 0.026089105755090714
iteration 52, loss = 0.02430865727365017
iteration 53, loss = 0.02361251786351204
iteration 54, loss = 0.032112739980220795
iteration 55, loss = 0.03308100998401642
iteration 56, loss = 0.028990915045142174
iteration 57, loss = 0.04318904131650925
iteration 58, loss = 0.03182244300842285
iteration 59, loss = 0.027263544499874115
iteration 60, loss = 0.03461611270904541
iteration 61, loss = 0.028240086510777473
iteration 62, loss = 0.03641490638256073
iteration 63, loss = 0.02692299894988537
iteration 64, loss = 0.029533300548791885
iteration 65, loss = 0.028422217816114426
iteration 66, loss = 0.031971342861652374
iteration 67, loss = 0.027599342167377472
iteration 68, loss = 0.03314600884914398
iteration 69, loss = 0.026449978351593018
iteration 70, loss = 0.03333216905593872
iteration 71, loss = 0.02733597531914711
iteration 72, loss = 0.02817314676940441
iteration 73, loss = 0.0257572028785944
iteration 74, loss = 0.044922154396772385
iteration 75, loss = 0.02677195332944393
iteration 76, loss = 0.02576576918363571
iteration 77, loss = 0.0263537485152483
iteration 78, loss = 0.021989762783050537
iteration 79, loss = 0.027951648458838463
iteration 80, loss = 0.03458559885621071
iteration 81, loss = 0.02943430095911026
iteration 82, loss = 0.04040094465017319
iteration 83, loss = 0.028040770441293716
iteration 84, loss = 0.026884794235229492
iteration 85, loss = 0.030589276924729347
iteration 86, loss = 0.033254969865083694
iteration 87, loss = 0.03391856327652931
iteration 88, loss = 0.023207228630781174
iteration 89, loss = 0.03115081414580345
iteration 90, loss = 0.027227867394685745
iteration 91, loss = 0.028990522027015686
iteration 92, loss = 0.029039176180958748
iteration 93, loss = 0.0412323921918869
iteration 94, loss = 0.023579267784953117
iteration 95, loss = 0.033684127032756805
iteration 96, loss = 0.04238085076212883
iteration 97, loss = 0.024282965809106827
iteration 98, loss = 0.02867349050939083
iteration 99, loss = 0.038168564438819885
iteration 100, loss = 0.0258506890386343
iteration 101, loss = 0.034956950694322586
iteration 102, loss = 0.02673044055700302
iteration 103, loss = 0.02297203615307808
iteration 104, loss = 0.023472623899579048
iteration 105, loss = 0.02728278562426567
iteration 106, loss = 0.021111637353897095
iteration 107, loss = 0.037975188344717026
iteration 108, loss = 0.023280637338757515
iteration 109, loss = 0.030197715386748314
iteration 110, loss = 0.02188943140208721
iteration 111, loss = 0.0246744304895401
iteration 112, loss = 0.03487315773963928
iteration 113, loss = 0.028312716633081436
iteration 114, loss = 0.02434813603758812
iteration 115, loss = 0.029133880510926247
iteration 116, loss = 0.02519483119249344
iteration 117, loss = 0.05460221320390701
iteration 118, loss = 0.0281800776720047
iteration 119, loss = 0.02599194459617138
iteration 120, loss = 0.02178747020661831
iteration 121, loss = 0.02395305223762989
iteration 122, loss = 0.023040885105729103
iteration 123, loss = 0.023078463971614838
iteration 124, loss = 0.02794591523706913
iteration 125, loss = 0.021626653149724007
iteration 126, loss = 0.024102257564663887
iteration 127, loss = 0.023190060630440712
iteration 128, loss = 0.022243322804570198
iteration 129, loss = 0.023310497403144836
iteration 130, loss = 0.028123289346694946
iteration 131, loss = 0.03358348831534386
iteration 132, loss = 0.03072448819875717
iteration 133, loss = 0.031574420630931854
iteration 134, loss = 0.030169948935508728
iteration 135, loss = 0.026772141456604004
iteration 136, loss = 0.05658121779561043
iteration 137, loss = 0.025642864406108856
iteration 138, loss = 0.023516960442066193
iteration 139, loss = 0.029019363224506378
iteration 140, loss = 0.029176749289035797
iteration 141, loss = 0.04429168999195099
iteration 142, loss = 0.030179748311638832
iteration 143, loss = 0.024227986112236977
iteration 144, loss = 0.02385874278843403
iteration 145, loss = 0.02784847654402256
iteration 146, loss = 0.02956545539200306
iteration 147, loss = 0.028193220496177673
iteration 148, loss = 0.026029733940958977
iteration 149, loss = 0.027072865515947342
iteration 150, loss = 0.020266789942979813
iteration 151, loss = 0.03162575513124466
iteration 152, loss = 0.02332226000726223
iteration 153, loss = 0.02718229405581951
iteration 154, loss = 0.029982466250658035
iteration 155, loss = 0.02426869608461857
iteration 156, loss = 0.022976182401180267
iteration 157, loss = 0.021699197590351105
iteration 158, loss = 0.03932685777544975
iteration 159, loss = 0.02248639054596424
iteration 160, loss = 0.022928640246391296
iteration 161, loss = 0.022791534662246704
iteration 162, loss = 0.020335841923952103
iteration 163, loss = 0.03434743732213974
iteration 164, loss = 0.021760113537311554
iteration 165, loss = 0.026860956102609634
iteration 166, loss = 0.022085219621658325
iteration 167, loss = 0.021714292466640472
iteration 168, loss = 0.02215060591697693
iteration 169, loss = 0.028802528977394104
iteration 170, loss = 0.022013727575540543
iteration 171, loss = 0.027995429933071136
iteration 172, loss = 0.01992618478834629
iteration 173, loss = 0.024502648040652275
iteration 174, loss = 0.021515490487217903
iteration 175, loss = 0.02240222878754139
iteration 176, loss = 0.01998172700405121
iteration 177, loss = 0.02023816853761673
iteration 178, loss = 0.025110822170972824
iteration 179, loss = 0.02399379573762417
iteration 180, loss = 0.026945685967803
iteration 181, loss = 0.018186647444963455
iteration 182, loss = 0.02770291268825531
iteration 183, loss = 0.026400873437523842
iteration 184, loss = 0.020266452804207802
iteration 185, loss = 0.019525444135069847
iteration 186, loss = 0.02400754950940609
iteration 187, loss = 0.022597938776016235
iteration 188, loss = 0.01998094469308853
iteration 189, loss = 0.018814394250512123
iteration 190, loss = 0.03320326283574104
iteration 191, loss = 0.021555211395025253
iteration 192, loss = 0.018974224105477333
iteration 193, loss = 0.026239821687340736
iteration 194, loss = 0.02437191642820835
iteration 195, loss = 0.028527261689305305
iteration 196, loss = 0.029027944430708885
iteration 197, loss = 0.023382509127259254
iteration 198, loss = 0.022337626665830612
iteration 199, loss = 0.022157486528158188
iteration 200, loss = 0.020869340747594833
iteration 201, loss = 0.02260548248887062
iteration 202, loss = 0.036785759031772614
iteration 203, loss = 0.022583026438951492
iteration 204, loss = 0.03423967957496643
iteration 205, loss = 0.023010794073343277
iteration 206, loss = 0.025491198524832726
iteration 207, loss = 0.023248976096510887
iteration 208, loss = 0.02518913708627224
iteration 209, loss = 0.027428533881902695
iteration 210, loss = 0.026911025866866112
iteration 211, loss = 0.02245439402759075
iteration 212, loss = 0.03225673362612724
iteration 213, loss = 0.02608107402920723
iteration 214, loss = 0.023705080151557922
iteration 215, loss = 0.02065090648829937
iteration 216, loss = 0.017568696290254593
iteration 217, loss = 0.021360240876674652
iteration 218, loss = 0.02026326395571232
iteration 219, loss = 0.017564840614795685
iteration 220, loss = 0.023318011313676834
iteration 221, loss = 0.0166814886033535
iteration 222, loss = 0.019260726869106293
iteration 223, loss = 0.019334085285663605
iteration 224, loss = 0.024737924337387085
iteration 225, loss = 0.034478552639484406
iteration 226, loss = 0.018356159329414368
iteration 227, loss = 0.02457272633910179
iteration 228, loss = 0.026437152177095413
iteration 229, loss = 0.025785299018025398
iteration 230, loss = 0.01935136318206787
iteration 231, loss = 0.020057640969753265
iteration 232, loss = 0.017881397157907486
iteration 233, loss = 0.01979473978281021
iteration 234, loss = 0.02527296170592308
iteration 235, loss = 0.019663548097014427
iteration 236, loss = 0.031184546649456024
iteration 237, loss = 0.03471120446920395
iteration 238, loss = 0.026019752025604248
iteration 239, loss = 0.01580699346959591
iteration 240, loss = 0.03557238355278969
iteration 241, loss = 0.03294171765446663
iteration 242, loss = 0.019998032599687576
iteration 243, loss = 0.018797602504491806
iteration 244, loss = 0.017860054969787598
iteration 245, loss = 0.023403003811836243
iteration 246, loss = 0.020805059000849724
iteration 247, loss = 0.020999491214752197
iteration 248, loss = 0.01825985684990883
iteration 249, loss = 0.02221761830151081
iteration 250, loss = 0.016731753945350647
iteration 251, loss = 0.038394033908843994
iteration 252, loss = 0.018249616026878357
iteration 253, loss = 0.021401865407824516
iteration 254, loss = 0.02213309332728386
iteration 255, loss = 0.017775608226656914
iteration 256, loss = 0.01971418783068657
iteration 257, loss = 0.02152707800269127
iteration 258, loss = 0.022977599874138832
iteration 259, loss = 0.01974848471581936
iteration 260, loss = 0.01874386891722679
iteration 261, loss = 0.023741398006677628
iteration 262, loss = 0.016578059643507004
iteration 263, loss = 0.0184613149613142
iteration 264, loss = 0.0172272901982069
iteration 265, loss = 0.020856948569417
iteration 266, loss = 0.017944972962141037
iteration 267, loss = 0.02207324281334877
iteration 268, loss = 0.019479066133499146
iteration 269, loss = 0.015560208819806576
iteration 270, loss = 0.031058188527822495
iteration 271, loss = 0.01779761351644993
iteration 272, loss = 0.021176546812057495
iteration 273, loss = 0.020188381895422935
iteration 274, loss = 0.022529589012265205
iteration 275, loss = 0.01971207745373249
iteration 276, loss = 0.01869114115834236
iteration 277, loss = 0.018114455044269562
iteration 278, loss = 0.020188312977552414
iteration 279, loss = 0.020937519147992134
iteration 280, loss = 0.01855603978037834
iteration 281, loss = 0.019811788573861122
iteration 282, loss = 0.01782337576150894
iteration 283, loss = 0.019005289301276207
iteration 284, loss = 0.021198753267526627
iteration 285, loss = 0.01937289722263813
iteration 286, loss = 0.016305796802043915
iteration 287, loss = 0.02510942704975605
iteration 288, loss = 0.019002340734004974
iteration 289, loss = 0.017027344554662704
iteration 290, loss = 0.01988656260073185
iteration 291, loss = 0.024305004626512527
iteration 292, loss = 0.02775150164961815
iteration 293, loss = 0.019443390890955925
iteration 294, loss = 0.01770036853849888
iteration 295, loss = 0.01687181182205677
iteration 296, loss = 0.018002808094024658
iteration 297, loss = 0.022615846246480942
iteration 298, loss = 0.019999900832772255
iteration 299, loss = 0.01986181177198887
iteration 0, loss = 0.02502606250345707
iteration 1, loss = 0.014630166813731194
iteration 2, loss = 0.0198231004178524
iteration 3, loss = 0.028658347204327583
iteration 4, loss = 0.016570426523685455
iteration 5, loss = 0.023170042783021927
iteration 6, loss = 0.022589068859815598
iteration 7, loss = 0.017275474965572357
iteration 8, loss = 0.02069683186709881
iteration 9, loss = 0.030306275933980942
iteration 10, loss = 0.01811925694346428
iteration 11, loss = 0.02153180167078972
iteration 12, loss = 0.01810259185731411
iteration 13, loss = 0.018624641001224518
iteration 14, loss = 0.01735863648355007
iteration 15, loss = 0.01997208595275879
iteration 16, loss = 0.02226988971233368
iteration 17, loss = 0.016672227531671524
iteration 18, loss = 0.023061754181981087
iteration 19, loss = 0.015576424077153206
iteration 20, loss = 0.015262439846992493
iteration 21, loss = 0.0196642205119133
iteration 22, loss = 0.016927963122725487
iteration 23, loss = 0.02174091897904873
iteration 24, loss = 0.023337936028838158
iteration 25, loss = 0.019558411091566086
iteration 26, loss = 0.01856570690870285
iteration 27, loss = 0.015720805153250694
iteration 28, loss = 0.015735823661088943
iteration 29, loss = 0.017672959715127945
iteration 30, loss = 0.014994083903729916
iteration 31, loss = 0.01867801696062088
iteration 32, loss = 0.016981074586510658
iteration 33, loss = 0.019093763083219528
iteration 34, loss = 0.020374435931444168
iteration 35, loss = 0.015287474729120731
iteration 36, loss = 0.016754910349845886
iteration 37, loss = 0.02746470645070076
iteration 38, loss = 0.014817586168646812
iteration 39, loss = 0.020358456298708916
iteration 40, loss = 0.017049403861165047
iteration 41, loss = 0.016345124691724777
iteration 42, loss = 0.017803244292736053
iteration 43, loss = 0.018344230949878693
iteration 44, loss = 0.02169029787182808
iteration 45, loss = 0.01650470867753029
iteration 46, loss = 0.017254739999771118
iteration 47, loss = 0.017557669430971146
iteration 48, loss = 0.025127416476607323
iteration 49, loss = 0.013954788446426392
iteration 50, loss = 0.030712751671671867
iteration 51, loss = 0.018902653828263283
iteration 52, loss = 0.015084045939147472
iteration 53, loss = 0.017100278288125992
iteration 54, loss = 0.014427445828914642
iteration 55, loss = 0.015324884094297886
iteration 56, loss = 0.01584053412079811
iteration 57, loss = 0.016761435195803642
iteration 58, loss = 0.015191984362900257
iteration 59, loss = 0.02691231667995453
iteration 60, loss = 0.015230197459459305
iteration 61, loss = 0.014973118901252747
iteration 62, loss = 0.02189755253493786
iteration 63, loss = 0.013595398515462875
iteration 64, loss = 0.019015934318304062
iteration 65, loss = 0.016354793682694435
iteration 66, loss = 0.014556745998561382
iteration 67, loss = 0.01491980254650116
iteration 68, loss = 0.014225717633962631
iteration 69, loss = 0.01586325280368328
iteration 70, loss = 0.014870393089950085
iteration 71, loss = 0.014695290476083755
iteration 72, loss = 0.015119451098144054
iteration 73, loss = 0.014642467722296715
iteration 74, loss = 0.025622498244047165
iteration 75, loss = 0.012065001763403416
iteration 76, loss = 0.021891161799430847
iteration 77, loss = 0.016875172033905983
iteration 78, loss = 0.014193914830684662
iteration 79, loss = 0.015727758407592773
iteration 80, loss = 0.015067012049257755
iteration 81, loss = 0.015948176383972168
iteration 82, loss = 0.019375501200556755
iteration 83, loss = 0.01566106639802456
iteration 84, loss = 0.016353541985154152
iteration 85, loss = 0.01420492585748434
iteration 86, loss = 0.018216032534837723
iteration 87, loss = 0.016904741525650024
iteration 88, loss = 0.018221460282802582
iteration 89, loss = 0.01601642370223999
iteration 90, loss = 0.012946528382599354
iteration 91, loss = 0.020335139706730843
iteration 92, loss = 0.013692894950509071
iteration 93, loss = 0.015646781772375107
iteration 94, loss = 0.01644175313413143
iteration 95, loss = 0.014879674650728703
iteration 96, loss = 0.026840949431061745
iteration 97, loss = 0.014208685606718063
iteration 98, loss = 0.014967579394578934
iteration 99, loss = 0.016939310356974602
iteration 100, loss = 0.01607210747897625
iteration 101, loss = 0.014204091392457485
iteration 102, loss = 0.017113402485847473
iteration 103, loss = 0.012266290374100208
iteration 104, loss = 0.029173843562602997
iteration 105, loss = 0.016477428376674652
iteration 106, loss = 0.01805911399424076
iteration 107, loss = 0.01912637986242771
iteration 108, loss = 0.013898609206080437
iteration 109, loss = 0.01655922830104828
iteration 110, loss = 0.013264371082186699
iteration 111, loss = 0.016849689185619354
iteration 112, loss = 0.015558773651719093
iteration 113, loss = 0.017896512523293495
iteration 114, loss = 0.01396345254033804
iteration 115, loss = 0.01563006080687046
iteration 116, loss = 0.01983806863427162
iteration 117, loss = 0.015599105507135391
iteration 118, loss = 0.016949545592069626
iteration 119, loss = 0.013544871471822262
iteration 120, loss = 0.01502959243953228
iteration 121, loss = 0.016714759171009064
iteration 122, loss = 0.027792155742645264
iteration 123, loss = 0.018319185823202133
iteration 124, loss = 0.01395336538553238
iteration 125, loss = 0.022851211950182915
iteration 126, loss = 0.01520058885216713
iteration 127, loss = 0.021036438643932343
iteration 128, loss = 0.02095331810414791
iteration 129, loss = 0.012925134971737862
iteration 130, loss = 0.014186619780957699
iteration 131, loss = 0.025300903245806694
iteration 132, loss = 0.014493071474134922
iteration 133, loss = 0.01563160866498947
iteration 134, loss = 0.018286533653736115
iteration 135, loss = 0.02437014691531658
iteration 136, loss = 0.031814176589250565
iteration 137, loss = 0.016749564558267593
iteration 138, loss = 0.013886082917451859
iteration 139, loss = 0.011910680681467056
iteration 140, loss = 0.014498254284262657
iteration 141, loss = 0.014041800983250141
iteration 142, loss = 0.020398806780576706
iteration 143, loss = 0.012701866216957569
iteration 144, loss = 0.014419433660805225
iteration 145, loss = 0.012390870600938797
iteration 146, loss = 0.020755412057042122
iteration 147, loss = 0.019888874143362045
iteration 148, loss = 0.014331025071442127
iteration 149, loss = 0.022152084857225418
iteration 150, loss = 0.0165115874260664
iteration 151, loss = 0.019985772669315338
iteration 152, loss = 0.017379246652126312
iteration 153, loss = 0.018176216632127762
iteration 154, loss = 0.017435478046536446
iteration 155, loss = 0.014246148988604546
iteration 156, loss = 0.013016272336244583
iteration 157, loss = 0.015669582411646843
iteration 158, loss = 0.01585938036441803
iteration 159, loss = 0.012986693531274796
iteration 160, loss = 0.02230573259294033
iteration 161, loss = 0.013552333228290081
iteration 162, loss = 0.0196798425167799
iteration 163, loss = 0.015751931816339493
iteration 164, loss = 0.018287263810634613
iteration 165, loss = 0.015306826680898666
iteration 166, loss = 0.014487095177173615
iteration 167, loss = 0.01190333440899849
iteration 168, loss = 0.018703985959291458
iteration 169, loss = 0.01560278795659542
iteration 170, loss = 0.021542547270655632
iteration 171, loss = 0.022226553410291672
iteration 172, loss = 0.014826210215687752
iteration 173, loss = 0.01221240870654583
iteration 174, loss = 0.018199045211076736
iteration 175, loss = 0.011744528077542782
iteration 176, loss = 0.012966254726052284
iteration 177, loss = 0.015559466555714607
iteration 178, loss = 0.016821369528770447
iteration 179, loss = 0.013663357123732567
iteration 180, loss = 0.014083600603044033
iteration 181, loss = 0.020046409219503403
iteration 182, loss = 0.022749193012714386
iteration 183, loss = 0.015168631449341774
iteration 184, loss = 0.01980675756931305
iteration 185, loss = 0.01645958237349987
iteration 186, loss = 0.011781684122979641
iteration 187, loss = 0.017721643671393394
iteration 188, loss = 0.013947476632893085
iteration 189, loss = 0.016330137848854065
iteration 190, loss = 0.015117348171770573
iteration 191, loss = 0.013819805346429348
iteration 192, loss = 0.012445488013327122
iteration 193, loss = 0.019283004105091095
iteration 194, loss = 0.015433523803949356
iteration 195, loss = 0.023113904520869255
iteration 196, loss = 0.013331257738173008
iteration 197, loss = 0.010791613720357418
iteration 198, loss = 0.014896557666361332
iteration 199, loss = 0.012306691147387028
iteration 200, loss = 0.018008515238761902
iteration 201, loss = 0.011508682742714882
iteration 202, loss = 0.012110707350075245
iteration 203, loss = 0.014738177880644798
iteration 204, loss = 0.014541168697178364
iteration 205, loss = 0.025774212554097176
iteration 206, loss = 0.012840256094932556
iteration 207, loss = 0.014547708444297314
iteration 208, loss = 0.01192076038569212
iteration 209, loss = 0.022915929555892944
iteration 210, loss = 0.019683007150888443
iteration 211, loss = 0.015007994137704372
iteration 212, loss = 0.01027226448059082
iteration 213, loss = 0.010603489354252815
iteration 214, loss = 0.013676670379936695
iteration 215, loss = 0.009122420102357864
iteration 216, loss = 0.01586136221885681
iteration 217, loss = 0.026211010292172432
iteration 218, loss = 0.012481455691158772
iteration 219, loss = 0.019004981964826584
iteration 220, loss = 0.012555374763906002
iteration 221, loss = 0.013379581272602081
iteration 222, loss = 0.010996542870998383
iteration 223, loss = 0.013447126373648643
iteration 224, loss = 0.03430675342679024
iteration 225, loss = 0.012676293030381203
iteration 226, loss = 0.012925472110509872
iteration 227, loss = 0.029115907847881317
iteration 228, loss = 0.016291039064526558
iteration 229, loss = 0.011531692929565907
iteration 230, loss = 0.011156832799315453
iteration 231, loss = 0.011917399242520332
iteration 232, loss = 0.014988943934440613
iteration 233, loss = 0.01099072489887476
iteration 234, loss = 0.013064746744930744
iteration 235, loss = 0.027149785310029984
iteration 236, loss = 0.015261013060808182
iteration 237, loss = 0.01801251247525215
iteration 238, loss = 0.01122165285050869
iteration 239, loss = 0.022970348596572876
iteration 240, loss = 0.016271304339170456
iteration 241, loss = 0.026328548789024353
iteration 242, loss = 0.013603124767541885
iteration 243, loss = 0.025671202689409256
iteration 244, loss = 0.02897159941494465
iteration 245, loss = 0.013246845453977585
iteration 246, loss = 0.01646721363067627
iteration 247, loss = 0.013331837020814419
iteration 248, loss = 0.01876307837665081
iteration 249, loss = 0.013789993710815907
iteration 250, loss = 0.012363891117274761
iteration 251, loss = 0.011128484271466732
iteration 252, loss = 0.02338409051299095
iteration 253, loss = 0.015452518127858639
iteration 254, loss = 0.010148845613002777
iteration 255, loss = 0.012287142686545849
iteration 256, loss = 0.013217229396104813
iteration 257, loss = 0.015175888314843178
iteration 258, loss = 0.011292537674307823
iteration 259, loss = 0.016252540051937103
iteration 260, loss = 0.011454845778644085
iteration 261, loss = 0.013214319944381714
iteration 262, loss = 0.013160694390535355
iteration 263, loss = 0.01314968429505825
iteration 264, loss = 0.010625630617141724
iteration 265, loss = 0.021517110988497734
iteration 266, loss = 0.012686531990766525
iteration 267, loss = 0.010228496044874191
iteration 268, loss = 0.009877962991595268
iteration 269, loss = 0.017337394878268242
iteration 270, loss = 0.01035478338599205
iteration 271, loss = 0.023014001548290253
iteration 272, loss = 0.01046756748110056
iteration 273, loss = 0.012471849098801613
iteration 274, loss = 0.012165805324912071
iteration 275, loss = 0.016512857750058174
iteration 276, loss = 0.010816413909196854
iteration 277, loss = 0.01202489621937275
iteration 278, loss = 0.012917086482048035
iteration 279, loss = 0.02012546919286251
iteration 280, loss = 0.014885695651173592
iteration 281, loss = 0.016552312299609184
iteration 282, loss = 0.013562418520450592
iteration 283, loss = 0.01080329343676567
iteration 284, loss = 0.03019551932811737
iteration 285, loss = 0.014596868306398392
iteration 286, loss = 0.013483330607414246
iteration 287, loss = 0.011848634108901024
iteration 288, loss = 0.013600623235106468
iteration 289, loss = 0.013462943956255913
iteration 290, loss = 0.013849353417754173
iteration 291, loss = 0.014210163615643978
iteration 292, loss = 0.014328627847135067
iteration 293, loss = 0.011335100047290325
iteration 294, loss = 0.02119601145386696
iteration 295, loss = 0.012283219955861568
iteration 296, loss = 0.01102065946906805
iteration 297, loss = 0.011134610511362553
iteration 298, loss = 0.01108528207987547
iteration 299, loss = 0.015273270197212696
iteration 0, loss = 0.01036037690937519
iteration 1, loss = 0.013031164184212685
iteration 2, loss = 0.02183324657380581
iteration 3, loss = 0.016494261100888252
iteration 4, loss = 0.015412332490086555
iteration 5, loss = 0.018221156671643257
iteration 6, loss = 0.013957624323666096
iteration 7, loss = 0.016488371416926384
iteration 8, loss = 0.011040196754038334
iteration 9, loss = 0.013659178279340267
iteration 10, loss = 0.011054483242332935
iteration 11, loss = 0.012212354689836502
iteration 12, loss = 0.01055154763162136
iteration 13, loss = 0.01036823634058237
iteration 14, loss = 0.0117868110537529
iteration 15, loss = 0.015290478244423866
iteration 16, loss = 0.018524235114455223
iteration 17, loss = 0.01522053498774767
iteration 18, loss = 0.020411400124430656
iteration 19, loss = 0.01230609230697155
iteration 20, loss = 0.013403140008449554
iteration 21, loss = 0.013006064109504223
iteration 22, loss = 0.016440952196717262
iteration 23, loss = 0.015607403591275215
iteration 24, loss = 0.009834792464971542
iteration 25, loss = 0.014100836589932442
iteration 26, loss = 0.013164058327674866
iteration 27, loss = 0.015602157451212406
iteration 28, loss = 0.01136651448905468
iteration 29, loss = 0.009319527074694633
iteration 30, loss = 0.01519769337028265
iteration 31, loss = 0.010460995137691498
iteration 32, loss = 0.01035260409116745
iteration 33, loss = 0.010230092331767082
iteration 34, loss = 0.017702044919133186
iteration 35, loss = 0.011682744137942791
iteration 36, loss = 0.013533707708120346
iteration 37, loss = 0.021783150732517242
iteration 38, loss = 0.010442402213811874
iteration 39, loss = 0.01409146562218666
iteration 40, loss = 0.01296783983707428
iteration 41, loss = 0.01583961211144924
iteration 42, loss = 0.016216713935136795
iteration 43, loss = 0.01070213783532381
iteration 44, loss = 0.016236616298556328
iteration 45, loss = 0.01210794597864151
iteration 46, loss = 0.009715305641293526
iteration 47, loss = 0.012259434908628464
iteration 48, loss = 0.012300945818424225
iteration 49, loss = 0.013057218864560127
iteration 50, loss = 0.012269837781786919
iteration 51, loss = 0.013686721213161945
iteration 52, loss = 0.010657773353159428
iteration 53, loss = 0.009375402703881264
iteration 54, loss = 0.012502225115895271
iteration 55, loss = 0.01606637053191662
iteration 56, loss = 0.013293972238898277
iteration 57, loss = 0.00906872283667326
iteration 58, loss = 0.015021709725260735
iteration 59, loss = 0.01938439905643463
iteration 60, loss = 0.011522027663886547
iteration 61, loss = 0.010599343106150627
iteration 62, loss = 0.009110301733016968
iteration 63, loss = 0.014058872126042843
iteration 64, loss = 0.023566819727420807
iteration 65, loss = 0.010249635204672813
iteration 66, loss = 0.010615793988108635
iteration 67, loss = 0.016485078260302544
iteration 68, loss = 0.00892915390431881
iteration 69, loss = 0.008995340205729008
iteration 70, loss = 0.01357358694076538
iteration 71, loss = 0.010170111432671547
iteration 72, loss = 0.009236544370651245
iteration 73, loss = 0.011729241348803043
iteration 74, loss = 0.010764400474727154
iteration 75, loss = 0.015201195143163204
iteration 76, loss = 0.009022342972457409
iteration 77, loss = 0.010954252444207668
iteration 78, loss = 0.01158569660037756
iteration 79, loss = 0.009980432689189911
iteration 80, loss = 0.010477167554199696
iteration 81, loss = 0.0132091473788023
iteration 82, loss = 0.010614143684506416
iteration 83, loss = 0.008524670265614986
iteration 84, loss = 0.010764691978693008
iteration 85, loss = 0.010846907272934914
iteration 86, loss = 0.011054951697587967
iteration 87, loss = 0.009340016171336174
iteration 88, loss = 0.008540015667676926
iteration 89, loss = 0.010341979563236237
iteration 90, loss = 0.011057781986892223
iteration 91, loss = 0.007621861062943935
iteration 92, loss = 0.020450664684176445
iteration 93, loss = 0.00995188020169735
iteration 94, loss = 0.01105869933962822
iteration 95, loss = 0.01775912009179592
iteration 96, loss = 0.01153977494686842
iteration 97, loss = 0.012012188322842121
iteration 98, loss = 0.011082326993346214
iteration 99, loss = 0.008119669742882252
iteration 100, loss = 0.014637540094554424
iteration 101, loss = 0.010362502187490463
iteration 102, loss = 0.012217083014547825
iteration 103, loss = 0.016878513619303703
iteration 104, loss = 0.012023949064314365
iteration 105, loss = 0.011755350045859814
iteration 106, loss = 0.01702401600778103
iteration 107, loss = 0.01161501370370388
iteration 108, loss = 0.012695963494479656
iteration 109, loss = 0.018436500802636147
iteration 110, loss = 0.010763892903923988
iteration 111, loss = 0.014618512243032455
iteration 112, loss = 0.011518113315105438
iteration 113, loss = 0.01316732820123434
iteration 114, loss = 0.0130457179620862
iteration 115, loss = 0.010279098525643349
iteration 116, loss = 0.015548898838460445
iteration 117, loss = 0.013220344670116901
iteration 118, loss = 0.010068915784358978
iteration 119, loss = 0.013532301411032677
iteration 120, loss = 0.019408851861953735
iteration 121, loss = 0.01215561293065548
iteration 122, loss = 0.016306757926940918
iteration 123, loss = 0.010649039410054684
iteration 124, loss = 0.009440753608942032
iteration 125, loss = 0.007823461666703224
iteration 126, loss = 0.009814677760004997
iteration 127, loss = 0.008976167067885399
iteration 128, loss = 0.008939528837800026
iteration 129, loss = 0.01548496913164854
iteration 130, loss = 0.0098796421661973
iteration 131, loss = 0.010584330186247826
iteration 132, loss = 0.011301363818347454
iteration 133, loss = 0.010877718217670918
iteration 134, loss = 0.008265347220003605
iteration 135, loss = 0.010157492943108082
iteration 136, loss = 0.009726256132125854
iteration 137, loss = 0.011757833883166313
iteration 138, loss = 0.011143055744469166
iteration 139, loss = 0.00969025120139122
iteration 140, loss = 0.017554279416799545
iteration 141, loss = 0.00990128144621849
iteration 142, loss = 0.008904730901122093
iteration 143, loss = 0.009960688650608063
iteration 144, loss = 0.010291585698723793
iteration 145, loss = 0.010826374404132366
iteration 146, loss = 0.008555511943995953
iteration 147, loss = 0.014824524521827698
iteration 148, loss = 0.009799344465136528
iteration 149, loss = 0.009072464890778065
iteration 150, loss = 0.011581094935536385
iteration 151, loss = 0.010801476426422596
iteration 152, loss = 0.012490781024098396
iteration 153, loss = 0.00891089253127575
iteration 154, loss = 0.009001491591334343
iteration 155, loss = 0.009031755849719048
iteration 156, loss = 0.009298386983573437
iteration 157, loss = 0.013250340707600117
iteration 158, loss = 0.010165783576667309
iteration 159, loss = 0.008550179190933704
iteration 160, loss = 0.009150546044111252
iteration 161, loss = 0.018799202516674995
iteration 162, loss = 0.008722303435206413
iteration 163, loss = 0.0164631437510252
iteration 164, loss = 0.00962916575372219
iteration 165, loss = 0.010501382872462273
iteration 166, loss = 0.014420234598219395
iteration 167, loss = 0.009207840077579021
iteration 168, loss = 0.010636376217007637
iteration 169, loss = 0.00853000022470951
iteration 170, loss = 0.008347546681761742
iteration 171, loss = 0.011238702572882175
iteration 172, loss = 0.013333856128156185
iteration 173, loss = 0.008063106797635555
iteration 174, loss = 0.0120698818936944
iteration 175, loss = 0.018555300310254097
iteration 176, loss = 0.013060461729764938
iteration 177, loss = 0.01653049886226654
iteration 178, loss = 0.007911693304777145
iteration 179, loss = 0.012997516430914402
iteration 180, loss = 0.008399239741265774
iteration 181, loss = 0.009911762550473213
iteration 182, loss = 0.012812850065529346
iteration 183, loss = 0.00946365762501955
iteration 184, loss = 0.01287651713937521
iteration 185, loss = 0.012645012699067593
iteration 186, loss = 0.008646683767437935
iteration 187, loss = 0.02100382000207901
iteration 188, loss = 0.010637085884809494
iteration 189, loss = 0.008017064072191715
iteration 190, loss = 0.010424342006444931
iteration 191, loss = 0.009744497947394848
iteration 192, loss = 0.00928476545959711
iteration 193, loss = 0.010597949847579002
iteration 194, loss = 0.009587557055056095
iteration 195, loss = 0.01058533787727356
iteration 196, loss = 0.01010054163634777
iteration 197, loss = 0.007740424480289221
iteration 198, loss = 0.011304396204650402
iteration 199, loss = 0.00873579178005457
iteration 200, loss = 0.010807323269546032
iteration 201, loss = 0.011393306776881218
iteration 202, loss = 0.013491949066519737
iteration 203, loss = 0.008110451512038708
iteration 204, loss = 0.008552810177206993
iteration 205, loss = 0.011758760549128056
iteration 206, loss = 0.00911803636699915
iteration 207, loss = 0.009823212400078773
iteration 208, loss = 0.009682665579020977
iteration 209, loss = 0.008544337004423141
iteration 210, loss = 0.009890839457511902
iteration 211, loss = 0.008905094116926193
iteration 212, loss = 0.012430686503648758
iteration 213, loss = 0.007027199491858482
iteration 214, loss = 0.007586856838315725
iteration 215, loss = 0.011264028958976269
iteration 216, loss = 0.007248342968523502
iteration 217, loss = 0.008487285114824772
iteration 218, loss = 0.00867923442274332
iteration 219, loss = 0.012413963675498962
iteration 220, loss = 0.009342980571091175
iteration 221, loss = 0.007288931868970394
iteration 222, loss = 0.009470444172620773
iteration 223, loss = 0.010495364665985107
iteration 224, loss = 0.007988953031599522
iteration 225, loss = 0.0072245365008711815
iteration 226, loss = 0.016148583963513374
iteration 227, loss = 0.010726826265454292
iteration 228, loss = 0.009611880406737328
iteration 229, loss = 0.009711649268865585
iteration 230, loss = 0.009197259321808815
iteration 231, loss = 0.010137966834008694
iteration 232, loss = 0.01009210292249918
iteration 233, loss = 0.010794240981340408
iteration 234, loss = 0.014383864589035511
iteration 235, loss = 0.007903212681412697
iteration 236, loss = 0.008361768908798695
iteration 237, loss = 0.01015799306333065
iteration 238, loss = 0.017840197309851646
iteration 239, loss = 0.00790390744805336
iteration 240, loss = 0.012649721466004848
iteration 241, loss = 0.009825417771935463
iteration 242, loss = 0.0165870301425457
iteration 243, loss = 0.009236854501068592
iteration 244, loss = 0.007463553454726934
iteration 245, loss = 0.015777410939335823
iteration 246, loss = 0.015764791518449783
iteration 247, loss = 0.012507779523730278
iteration 248, loss = 0.012204894796013832
iteration 249, loss = 0.0101516954600811
iteration 250, loss = 0.010285177268087864
iteration 251, loss = 0.010756382718682289
iteration 252, loss = 0.010228119790554047
iteration 253, loss = 0.007182271219789982
iteration 254, loss = 0.012110122479498386
iteration 255, loss = 0.006196773145347834
iteration 256, loss = 0.008024141192436218
iteration 257, loss = 0.011070860549807549
iteration 258, loss = 0.010031738318502903
iteration 259, loss = 0.015334594994783401
iteration 260, loss = 0.007693244144320488
iteration 261, loss = 0.008669225499033928
iteration 262, loss = 0.013767285272479057
iteration 263, loss = 0.009141248650848866
iteration 264, loss = 0.01059774775058031
iteration 265, loss = 0.009278100915253162
iteration 266, loss = 0.011203574016690254
iteration 267, loss = 0.011633271351456642
iteration 268, loss = 0.009110189974308014
iteration 269, loss = 0.00990377739071846
iteration 270, loss = 0.008898560889065266
iteration 271, loss = 0.010435443371534348
iteration 272, loss = 0.007052197121083736
iteration 273, loss = 0.007005918771028519
iteration 274, loss = 0.007234531454741955
iteration 275, loss = 0.007410260848701
iteration 276, loss = 0.011967504397034645
iteration 277, loss = 0.007054091896861792
iteration 278, loss = 0.01110655628144741
iteration 279, loss = 0.01680045761168003
iteration 280, loss = 0.009927840903401375
iteration 281, loss = 0.009650376625359058
iteration 282, loss = 0.008866379037499428
iteration 283, loss = 0.008771860972046852
iteration 284, loss = 0.011560975573956966
iteration 285, loss = 0.009180007502436638
iteration 286, loss = 0.013392876833677292
iteration 287, loss = 0.006400494836270809
iteration 288, loss = 0.01197515893727541
iteration 289, loss = 0.00855808425694704
iteration 290, loss = 0.007947899401187897
iteration 291, loss = 0.00996690895408392
iteration 292, loss = 0.00883939303457737
iteration 293, loss = 0.007833896204829216
iteration 294, loss = 0.007778405677527189
iteration 295, loss = 0.0077603659592568874
iteration 296, loss = 0.010712685994803905
iteration 297, loss = 0.007161139510571957
iteration 298, loss = 0.012773691676557064
iteration 299, loss = 0.0081904586404562
iteration 0, loss = 0.010710337199270725
iteration 1, loss = 0.007594656199216843
iteration 2, loss = 0.017848223447799683
iteration 3, loss = 0.008833701722323895
iteration 4, loss = 0.008362519554793835
iteration 5, loss = 0.009343776851892471
iteration 6, loss = 0.009698417037725449
iteration 7, loss = 0.009884070605039597
iteration 8, loss = 0.006977051496505737
iteration 9, loss = 0.010013695806264877
iteration 10, loss = 0.007600259035825729
iteration 11, loss = 0.008879107423126698
iteration 12, loss = 0.013345187529921532
iteration 13, loss = 0.010644197463989258
iteration 14, loss = 0.00962800718843937
iteration 15, loss = 0.01175772026181221
iteration 16, loss = 0.009633456356823444
iteration 17, loss = 0.00796595774590969
iteration 18, loss = 0.008619860745966434
iteration 19, loss = 0.012638624757528305
iteration 20, loss = 0.011031233705580235
iteration 21, loss = 0.008679887279868126
iteration 22, loss = 0.01643424667418003
iteration 23, loss = 0.007602948695421219
iteration 24, loss = 0.006572115700691938
iteration 25, loss = 0.008008510805666447
iteration 26, loss = 0.008604057133197784
iteration 27, loss = 0.009344315156340599
iteration 28, loss = 0.008865457028150558
iteration 29, loss = 0.015078019350767136
iteration 30, loss = 0.00825568474829197
iteration 31, loss = 0.010060586035251617
iteration 32, loss = 0.009704302065074444
iteration 33, loss = 0.007658200804144144
iteration 34, loss = 0.006879746913909912
iteration 35, loss = 0.005617184564471245
iteration 36, loss = 0.007744277361780405
iteration 37, loss = 0.009789610281586647
iteration 38, loss = 0.007697416935116053
iteration 39, loss = 0.016508784145116806
iteration 40, loss = 0.00844268687069416
iteration 41, loss = 0.006637339014559984
iteration 42, loss = 0.009520425461232662
iteration 43, loss = 0.00830504298210144
iteration 44, loss = 0.006219625938683748
iteration 45, loss = 0.009414075873792171
iteration 46, loss = 0.007654436863958836
iteration 47, loss = 0.008150488138198853
iteration 48, loss = 0.011341768316924572
iteration 49, loss = 0.007462294772267342
iteration 50, loss = 0.009249961003661156
iteration 51, loss = 0.00907487329095602
iteration 52, loss = 0.006329882889986038
iteration 53, loss = 0.010762046091258526
iteration 54, loss = 0.017028989270329475
iteration 55, loss = 0.009541841223835945
iteration 56, loss = 0.0076297977939248085
iteration 57, loss = 0.006951172836124897
iteration 58, loss = 0.009338094852864742
iteration 59, loss = 0.006855180021375418
iteration 60, loss = 0.010859325528144836
iteration 61, loss = 0.010291140526533127
iteration 62, loss = 0.006865563802421093
iteration 63, loss = 0.006555079482495785
iteration 64, loss = 0.009542969986796379
iteration 65, loss = 0.00846636388450861
iteration 66, loss = 0.00649756845086813
iteration 67, loss = 0.00905341561883688
iteration 68, loss = 0.005863771308213472
iteration 69, loss = 0.006650829687714577
iteration 70, loss = 0.00893406756222248
iteration 71, loss = 0.01080368272960186
iteration 72, loss = 0.009663133881986141
iteration 73, loss = 0.007690296974033117
iteration 74, loss = 0.005537860561162233
iteration 75, loss = 0.008181875571608543
iteration 76, loss = 0.009170656092464924
iteration 77, loss = 0.015589390881359577
iteration 78, loss = 0.007919883355498314
iteration 79, loss = 0.0063431416638195515
iteration 80, loss = 0.012996611185371876
iteration 81, loss = 0.008637748658657074
iteration 82, loss = 0.010204154998064041
iteration 83, loss = 0.01268855296075344
iteration 84, loss = 0.00886639766395092
iteration 85, loss = 0.006725807674229145
iteration 86, loss = 0.008457982912659645
iteration 87, loss = 0.008611354045569897
iteration 88, loss = 0.008032668381929398
iteration 89, loss = 0.007544009480625391
iteration 90, loss = 0.009180746972560883
iteration 91, loss = 0.007794641889631748
iteration 92, loss = 0.014323679730296135
iteration 93, loss = 0.007586709223687649
iteration 94, loss = 0.010050948709249496
iteration 95, loss = 0.005883004982024431
iteration 96, loss = 0.008128565736114979
iteration 97, loss = 0.008497955277562141
iteration 98, loss = 0.010732263326644897
iteration 99, loss = 0.008159302175045013
iteration 100, loss = 0.007510604802519083
iteration 101, loss = 0.007606911938637495
iteration 102, loss = 0.00869554840028286
iteration 103, loss = 0.007849253714084625
iteration 104, loss = 0.0159528199583292
iteration 105, loss = 0.013524787500500679
iteration 106, loss = 0.0063718329183757305
iteration 107, loss = 0.00948239304125309
iteration 108, loss = 0.007244587410241365
iteration 109, loss = 0.007464371155947447
iteration 110, loss = 0.017454132437705994
iteration 111, loss = 0.006408315151929855
iteration 112, loss = 0.007945386692881584
iteration 113, loss = 0.006578968837857246
iteration 114, loss = 0.0070087360218167305
iteration 115, loss = 0.008493447676301003
iteration 116, loss = 0.011834194883704185
iteration 117, loss = 0.008621961809694767
iteration 118, loss = 0.009365230798721313
iteration 119, loss = 0.009359524585306644
iteration 120, loss = 0.00940778199583292
iteration 121, loss = 0.008276501670479774
iteration 122, loss = 0.010899397544562817
iteration 123, loss = 0.007180741056799889
iteration 124, loss = 0.014165396802127361
iteration 125, loss = 0.008002078160643578
iteration 126, loss = 0.007515102159231901
iteration 127, loss = 0.011139487847685814
iteration 128, loss = 0.009505940601229668
iteration 129, loss = 0.016017774119973183
iteration 130, loss = 0.008529093116521835
iteration 131, loss = 0.006934237666428089
iteration 132, loss = 0.007680371403694153
iteration 133, loss = 0.007348212413489819
iteration 134, loss = 0.008480045013129711
iteration 135, loss = 0.006442449986934662
iteration 136, loss = 0.00676558306440711
iteration 137, loss = 0.014058837667107582
iteration 138, loss = 0.00702957296743989
iteration 139, loss = 0.007474032696336508
iteration 140, loss = 0.012166205793619156
iteration 141, loss = 0.007022941019386053
iteration 142, loss = 0.005715135484933853
iteration 143, loss = 0.009451154619455338
iteration 144, loss = 0.014232689514756203
iteration 145, loss = 0.009871824644505978
iteration 146, loss = 0.006467366125434637
iteration 147, loss = 0.007021635305136442
iteration 148, loss = 0.01103564165532589
iteration 149, loss = 0.008317979983985424
iteration 150, loss = 0.00753052020445466
iteration 151, loss = 0.008143123239278793
iteration 152, loss = 0.010686913505196571
iteration 153, loss = 0.007800799794495106
iteration 154, loss = 0.005522479303181171
iteration 155, loss = 0.00816342979669571
iteration 156, loss = 0.00782045815140009
iteration 157, loss = 0.006941285450011492
iteration 158, loss = 0.005554341711103916
iteration 159, loss = 0.008798438124358654
iteration 160, loss = 0.009029081091284752
iteration 161, loss = 0.011404097080230713
iteration 162, loss = 0.010501725599169731
iteration 163, loss = 0.006287523079663515
iteration 164, loss = 0.006814284715801477
iteration 165, loss = 0.00520691042765975
iteration 166, loss = 0.006261711940169334
iteration 167, loss = 0.013586888089776039
iteration 168, loss = 0.006226219702512026
iteration 169, loss = 0.009614244103431702
iteration 170, loss = 0.005147676914930344
iteration 171, loss = 0.0074926018714904785
iteration 172, loss = 0.006885589100420475
iteration 173, loss = 0.006662641651928425
iteration 174, loss = 0.006756917107850313
iteration 175, loss = 0.006440292112529278
iteration 176, loss = 0.008505106903612614
iteration 177, loss = 0.006458308082073927
iteration 178, loss = 0.014638083986938
iteration 179, loss = 0.006556786596775055
iteration 180, loss = 0.006501651369035244
iteration 181, loss = 0.006370548624545336
iteration 182, loss = 0.0061132642440497875
iteration 183, loss = 0.00647183321416378
iteration 184, loss = 0.0062681990675628185
iteration 185, loss = 0.006876263301819563
iteration 186, loss = 0.009366537444293499
iteration 187, loss = 0.00882673915475607
iteration 188, loss = 0.0075426227413117886
iteration 189, loss = 0.0070360079407691956
iteration 190, loss = 0.006731774192303419
iteration 191, loss = 0.007485558278858662
iteration 192, loss = 0.010465072467923164
iteration 193, loss = 0.013174273073673248
iteration 194, loss = 0.007581045385450125
iteration 195, loss = 0.0062505630776286125
iteration 196, loss = 0.0055457064881920815
iteration 197, loss = 0.005825543776154518
iteration 198, loss = 0.00570160336792469
iteration 199, loss = 0.008230693638324738
iteration 200, loss = 0.006716753821820021
iteration 201, loss = 0.015211474150419235
iteration 202, loss = 0.00781831331551075
iteration 203, loss = 0.007449353113770485
iteration 204, loss = 0.011122465133666992
iteration 205, loss = 0.006286232732236385
iteration 206, loss = 0.007503942586481571
iteration 207, loss = 0.007577018346637487
iteration 208, loss = 0.007022179663181305
iteration 209, loss = 0.007872477173805237
iteration 210, loss = 0.006605096161365509
iteration 211, loss = 0.010323435999453068
iteration 212, loss = 0.006386507768183947
iteration 213, loss = 0.009151212871074677
iteration 214, loss = 0.007126522250473499
iteration 215, loss = 0.0069118388928473
iteration 216, loss = 0.005544022656977177
iteration 217, loss = 0.008408983238041401
iteration 218, loss = 0.005916023161262274
iteration 219, loss = 0.0073478007689118385
iteration 220, loss = 0.00689788069576025
iteration 221, loss = 0.006051948294043541
iteration 222, loss = 0.006945224944502115
iteration 223, loss = 0.007214539684355259
iteration 224, loss = 0.0069319577887654305
iteration 225, loss = 0.005828514229506254
iteration 226, loss = 0.006441375706344843
iteration 227, loss = 0.008241257630288601
iteration 228, loss = 0.0053522358648478985
iteration 229, loss = 0.008572641760110855
iteration 230, loss = 0.008686982095241547
iteration 231, loss = 0.00780557980760932
iteration 232, loss = 0.00842648558318615
iteration 233, loss = 0.004943946376442909
iteration 234, loss = 0.009163030423223972
iteration 235, loss = 0.006439681630581617
iteration 236, loss = 0.007072863169014454
iteration 237, loss = 0.010461734607815742
iteration 238, loss = 0.008837258443236351
iteration 239, loss = 0.015905972570180893
iteration 240, loss = 0.006084347143769264
iteration 241, loss = 0.008477822877466679
iteration 242, loss = 0.007980424910783768
iteration 243, loss = 0.011332426220178604
iteration 244, loss = 0.007733779028058052
iteration 245, loss = 0.006487746257334948
iteration 246, loss = 0.007041022647172213
iteration 247, loss = 0.00562148867174983
iteration 248, loss = 0.006385789718478918
iteration 249, loss = 0.00956458505243063
iteration 250, loss = 0.005746918264776468
iteration 251, loss = 0.007730779703706503
iteration 252, loss = 0.013613184913992882
iteration 253, loss = 0.007268467452377081
iteration 254, loss = 0.007734397891908884
iteration 255, loss = 0.0061685736291110516
iteration 256, loss = 0.0049501401372253895
iteration 257, loss = 0.005754933226853609
iteration 258, loss = 0.006646213587373495
iteration 259, loss = 0.00643268134444952
iteration 260, loss = 0.009097122587263584
iteration 261, loss = 0.014972632750868797
iteration 262, loss = 0.006972876377403736
iteration 263, loss = 0.009360296651721
iteration 264, loss = 0.005070195533335209
iteration 265, loss = 0.005325024016201496
iteration 266, loss = 0.008045638911426067
iteration 267, loss = 0.007861444726586342
iteration 268, loss = 0.007629846688359976
iteration 269, loss = 0.005704001989215612
iteration 270, loss = 0.005623247940093279
iteration 271, loss = 0.005584110505878925
iteration 272, loss = 0.006545630749315023
iteration 273, loss = 0.007711461745202541
iteration 274, loss = 0.0070475023239851
iteration 275, loss = 0.0068855006247758865
iteration 276, loss = 0.0059629520401358604
iteration 277, loss = 0.007901370525360107
iteration 278, loss = 0.006243507377803326
iteration 279, loss = 0.005538068246096373
iteration 280, loss = 0.016235776245594025
iteration 281, loss = 0.005743901710957289
iteration 282, loss = 0.005208917427808046
iteration 283, loss = 0.0051378849893808365
iteration 284, loss = 0.006816765293478966
iteration 285, loss = 0.00698880385607481
iteration 286, loss = 0.006367872469127178
iteration 287, loss = 0.0074672154150903225
iteration 288, loss = 0.005607654340565205
iteration 289, loss = 0.012584351003170013
iteration 290, loss = 0.008002704940736294
iteration 291, loss = 0.005408161319792271
iteration 292, loss = 0.007474827580153942
iteration 293, loss = 0.0060236286371946335
iteration 294, loss = 0.006647711619734764
iteration 295, loss = 0.008126686327159405
iteration 296, loss = 0.007691777776926756
iteration 297, loss = 0.007697588298469782
iteration 298, loss = 0.0106879323720932
iteration 299, loss = 0.006636767648160458
iteration 0, loss = 0.006598334759473801
iteration 1, loss = 0.0075564817525446415
iteration 2, loss = 0.006587102077901363
iteration 3, loss = 0.004821576178073883
iteration 4, loss = 0.005109637975692749
iteration 5, loss = 0.0062185898423194885
iteration 6, loss = 0.00834158156067133
iteration 7, loss = 0.00691959960386157
iteration 8, loss = 0.006462789140641689
iteration 9, loss = 0.0077529591508209705
iteration 10, loss = 0.0064795976504683495
iteration 11, loss = 0.009739293716847897
iteration 12, loss = 0.005810285918414593
iteration 13, loss = 0.005617727059870958
iteration 14, loss = 0.007093797903507948
iteration 15, loss = 0.0052700587548315525
iteration 16, loss = 0.00854564644396305
iteration 17, loss = 0.0059959362260997295
iteration 18, loss = 0.005756323225796223
iteration 19, loss = 0.016681522130966187
iteration 20, loss = 0.006711964495480061
iteration 21, loss = 0.01236796285957098
iteration 22, loss = 0.008462443947792053
iteration 23, loss = 0.0067301420494914055
iteration 24, loss = 0.005586967337876558
iteration 25, loss = 0.005025149323046207
iteration 26, loss = 0.008773069828748703
iteration 27, loss = 0.005345606245100498
iteration 28, loss = 0.007754861377179623
iteration 29, loss = 0.004887348040938377
iteration 30, loss = 0.0058240401558578014
iteration 31, loss = 0.007909350097179413
iteration 32, loss = 0.005678961519151926
iteration 33, loss = 0.005941505078226328
iteration 34, loss = 0.010706961154937744
iteration 35, loss = 0.0061946650967001915
iteration 36, loss = 0.010950508527457714
iteration 37, loss = 0.007934534922242165
iteration 38, loss = 0.006121477577835321
iteration 39, loss = 0.00696756923571229
iteration 40, loss = 0.005385139957070351
iteration 41, loss = 0.010009760968387127
iteration 42, loss = 0.005861825775355101
iteration 43, loss = 0.012360887601971626
iteration 44, loss = 0.006703934632241726
iteration 45, loss = 0.005192013923078775
iteration 46, loss = 0.006216078530997038
iteration 47, loss = 0.005280924495309591
iteration 48, loss = 0.006104093510657549
iteration 49, loss = 0.006513422355055809
iteration 50, loss = 0.006647513713687658
iteration 51, loss = 0.004950281698256731
iteration 52, loss = 0.0070548150688409805
iteration 53, loss = 0.007920335046947002
iteration 54, loss = 0.004445485305041075
iteration 55, loss = 0.007337798830121756
iteration 56, loss = 0.006558560766279697
iteration 57, loss = 0.006580154411494732
iteration 58, loss = 0.0055695814080536366
iteration 59, loss = 0.005856378935277462
iteration 60, loss = 0.006052821408957243
iteration 61, loss = 0.006780159194022417
iteration 62, loss = 0.00545713072642684
iteration 63, loss = 0.006693888921290636
iteration 64, loss = 0.005308572202920914
iteration 65, loss = 0.0052224271930754185
iteration 66, loss = 0.012053797021508217
iteration 67, loss = 0.007669760379940271
iteration 68, loss = 0.00907069444656372
iteration 69, loss = 0.0071928915567696095
iteration 70, loss = 0.005527334753423929
iteration 71, loss = 0.005529586225748062
iteration 72, loss = 0.006468157283961773
iteration 73, loss = 0.007548210676759481
iteration 74, loss = 0.00820603221654892
iteration 75, loss = 0.004805712029337883
iteration 76, loss = 0.00456493953242898
iteration 77, loss = 0.00831679068505764
iteration 78, loss = 0.006587928161025047
iteration 79, loss = 0.00780463544651866
iteration 80, loss = 0.006306379102170467
iteration 81, loss = 0.005117759574204683
iteration 82, loss = 0.0058836666867136955
iteration 83, loss = 0.005535535980015993
iteration 84, loss = 0.004160174168646336
iteration 85, loss = 0.005160963628441095
iteration 86, loss = 0.005355245433747768
iteration 87, loss = 0.005327480845153332
iteration 88, loss = 0.00871244166046381
iteration 89, loss = 0.004596880171447992
iteration 90, loss = 0.012600226327776909
iteration 91, loss = 0.005969795398414135
iteration 92, loss = 0.00649463152512908
iteration 93, loss = 0.005934153217822313
iteration 94, loss = 0.006208796985447407
iteration 95, loss = 0.0058773416094481945
iteration 96, loss = 0.010974048636853695
iteration 97, loss = 0.00782275851815939
iteration 98, loss = 0.005023040343075991
iteration 99, loss = 0.005570574663579464
iteration 100, loss = 0.006885077338665724
iteration 101, loss = 0.006937500089406967
iteration 102, loss = 0.010134991258382797
iteration 103, loss = 0.005763500928878784
iteration 104, loss = 0.006859017536044121
iteration 105, loss = 0.005467489827424288
iteration 106, loss = 0.006480644457042217
iteration 107, loss = 0.006423738785088062
iteration 108, loss = 0.004687350243330002
iteration 109, loss = 0.004665477201342583
iteration 110, loss = 0.004887435585260391
iteration 111, loss = 0.014014698565006256
iteration 112, loss = 0.004456510301679373
iteration 113, loss = 0.006127443630248308
iteration 114, loss = 0.006149844732135534
iteration 115, loss = 0.004358403384685516
iteration 116, loss = 0.004796317312866449
iteration 117, loss = 0.007714071311056614
iteration 118, loss = 0.0064910766668617725
iteration 119, loss = 0.0038020280189812183
iteration 120, loss = 0.011144489049911499
iteration 121, loss = 0.008970982395112514
iteration 122, loss = 0.005976890679448843
iteration 123, loss = 0.006513881962746382
iteration 124, loss = 0.005096659064292908
iteration 125, loss = 0.00577880721539259
iteration 126, loss = 0.0075047994032502174
iteration 127, loss = 0.0077749332413077354
iteration 128, loss = 0.0069204834289848804
iteration 129, loss = 0.00815088115632534
iteration 130, loss = 0.010483387857675552
iteration 131, loss = 0.0061281281523406506
iteration 132, loss = 0.010411650873720646
iteration 133, loss = 0.005565647501498461
iteration 134, loss = 0.006472534965723753
iteration 135, loss = 0.006170911714434624
iteration 136, loss = 0.009572884999215603
iteration 137, loss = 0.008562306873500347
iteration 138, loss = 0.005348637234419584
iteration 139, loss = 0.011299102567136288
iteration 140, loss = 0.006909627467393875
iteration 141, loss = 0.007264201529324055
iteration 142, loss = 0.005404796916991472
iteration 143, loss = 0.004683788865804672
iteration 144, loss = 0.005894696339964867
iteration 145, loss = 0.010262458585202694
iteration 146, loss = 0.005611344240605831
iteration 147, loss = 0.0037132236175239086
iteration 148, loss = 0.007208751980215311
iteration 149, loss = 0.007174068130552769
iteration 150, loss = 0.005617944989353418
iteration 151, loss = 0.005722600035369396
iteration 152, loss = 0.004450567997992039
iteration 153, loss = 0.005649850703775883
iteration 154, loss = 0.004575585015118122
iteration 155, loss = 0.005182716064155102
iteration 156, loss = 0.005603068042546511
iteration 157, loss = 0.003771236166357994
iteration 158, loss = 0.007048087660223246
iteration 159, loss = 0.00557113578543067
iteration 160, loss = 0.006382484920322895
iteration 161, loss = 0.005930897314101458
iteration 162, loss = 0.0050536515191197395
iteration 163, loss = 0.008910748176276684
iteration 164, loss = 0.004823505878448486
iteration 165, loss = 0.006367384921759367
iteration 166, loss = 0.004723332356661558
iteration 167, loss = 0.010035050101578236
iteration 168, loss = 0.008052942343056202
iteration 169, loss = 0.0046813273802399635
iteration 170, loss = 0.006186765152961016
iteration 171, loss = 0.005650945473462343
iteration 172, loss = 0.007514507044106722
iteration 173, loss = 0.004654710181057453
iteration 174, loss = 0.00936286523938179
iteration 175, loss = 0.014584180898964405
iteration 176, loss = 0.004537834785878658
iteration 177, loss = 0.005791229195892811
iteration 178, loss = 0.00983178336173296
iteration 179, loss = 0.006683957297354937
iteration 180, loss = 0.006260985042899847
iteration 181, loss = 0.004073164891451597
iteration 182, loss = 0.004732816945761442
iteration 183, loss = 0.0049963160417973995
iteration 184, loss = 0.0069706859067082405
iteration 185, loss = 0.010930852964520454
iteration 186, loss = 0.007229200564324856
iteration 187, loss = 0.004712942522019148
iteration 188, loss = 0.007341742515563965
iteration 189, loss = 0.008357029408216476
iteration 190, loss = 0.007460698019713163
iteration 191, loss = 0.005322469864040613
iteration 192, loss = 0.007253310643136501
iteration 193, loss = 0.005320832133293152
iteration 194, loss = 0.004996424075216055
iteration 195, loss = 0.004232035484164953
iteration 196, loss = 0.0051777781918644905
iteration 197, loss = 0.010464263148605824
iteration 198, loss = 0.00648613553494215
iteration 199, loss = 0.007950969971716404
iteration 200, loss = 0.008418499492108822
iteration 201, loss = 0.007652252446860075
iteration 202, loss = 0.00857651699334383
iteration 203, loss = 0.006087538320571184
iteration 204, loss = 0.00805705040693283
iteration 205, loss = 0.0062745497561991215
iteration 206, loss = 0.005337997805327177
iteration 207, loss = 0.006670322734862566
iteration 208, loss = 0.00535934790968895
iteration 209, loss = 0.00558770727366209
iteration 210, loss = 0.004954622592777014
iteration 211, loss = 0.004040842410176992
iteration 212, loss = 0.005162307992577553
iteration 213, loss = 0.00698089087381959
iteration 214, loss = 0.006129168439656496
iteration 215, loss = 0.00462694326415658
iteration 216, loss = 0.005620250012725592
iteration 217, loss = 0.005105882417410612
iteration 218, loss = 0.005221664905548096
iteration 219, loss = 0.005265682935714722
iteration 220, loss = 0.004765661433339119
iteration 221, loss = 0.004328299779444933
iteration 222, loss = 0.0049502127803862095
iteration 223, loss = 0.004929347429424524
iteration 224, loss = 0.01812647469341755
iteration 225, loss = 0.004759432747960091
iteration 226, loss = 0.003926371689885855
iteration 227, loss = 0.008100795559585094
iteration 228, loss = 0.004523963667452335
iteration 229, loss = 0.0053445883095264435
iteration 230, loss = 0.008640885353088379
iteration 231, loss = 0.0056322477757930756
iteration 232, loss = 0.006193523295223713
iteration 233, loss = 0.010175809264183044
iteration 234, loss = 0.00584480119869113
iteration 235, loss = 0.004357629921287298
iteration 236, loss = 0.01017826795578003
iteration 237, loss = 0.005486003123223782
iteration 238, loss = 0.004147964529693127
iteration 239, loss = 0.004131021443754435
iteration 240, loss = 0.007258615456521511
iteration 241, loss = 0.006515014450997114
iteration 242, loss = 0.006760144606232643
iteration 243, loss = 0.004995655734091997
iteration 244, loss = 0.0066502708941698074
iteration 245, loss = 0.006046287715435028
iteration 246, loss = 0.004283900838345289
iteration 247, loss = 0.005769434850662947
iteration 248, loss = 0.004758331459015608
iteration 249, loss = 0.0041716331616044044
iteration 250, loss = 0.004969140514731407
iteration 251, loss = 0.009317195042967796
iteration 252, loss = 0.005274797324091196
iteration 253, loss = 0.005944864824414253
iteration 254, loss = 0.01129923202097416
iteration 255, loss = 0.007052519358694553
iteration 256, loss = 0.005130366887897253
iteration 257, loss = 0.004501522984355688
iteration 258, loss = 0.00392012856900692
iteration 259, loss = 0.004134027287364006
iteration 260, loss = 0.004303365014493465
iteration 261, loss = 0.005398865323513746
iteration 262, loss = 0.005238586571067572
iteration 263, loss = 0.005577777978032827
iteration 264, loss = 0.0043333834037184715
iteration 265, loss = 0.006191459018737078
iteration 266, loss = 0.0052956161089241505
iteration 267, loss = 0.006232365034520626
iteration 268, loss = 0.007729469332844019
iteration 269, loss = 0.005009673535823822
iteration 270, loss = 0.0039192368276417255
iteration 271, loss = 0.003534916788339615
iteration 272, loss = 0.0048906500451266766
iteration 273, loss = 0.005901333875954151
iteration 274, loss = 0.00963291060179472
iteration 275, loss = 0.007002928759902716
iteration 276, loss = 0.004953407682478428
iteration 277, loss = 0.006633711978793144
iteration 278, loss = 0.005533634684979916
iteration 279, loss = 0.005516346078366041
iteration 280, loss = 0.007341377902776003
iteration 281, loss = 0.010222451761364937
iteration 282, loss = 0.009951761923730373
iteration 283, loss = 0.00566839799284935
iteration 284, loss = 0.004912274423986673
iteration 285, loss = 0.006436775904148817
iteration 286, loss = 0.0073263030499219894
iteration 287, loss = 0.004417125601321459
iteration 288, loss = 0.004607211798429489
iteration 289, loss = 0.005349722225219011
iteration 290, loss = 0.005602284334599972
iteration 291, loss = 0.005060683004558086
iteration 292, loss = 0.004818265791982412
iteration 293, loss = 0.004251740872859955
iteration 294, loss = 0.0037723856512457132
iteration 295, loss = 0.0044396501034498215
iteration 296, loss = 0.004056714009493589
iteration 297, loss = 0.004673912189900875
iteration 298, loss = 0.004775502253323793
iteration 299, loss = 0.005252963397651911
iteration 0, loss = 0.004758632741868496
iteration 1, loss = 0.0046090479008853436
iteration 2, loss = 0.009204464964568615
iteration 3, loss = 0.00798023957759142
iteration 4, loss = 0.006468822713941336
iteration 5, loss = 0.005325625650584698
iteration 6, loss = 0.0032268131617456675
iteration 7, loss = 0.004883705638349056
iteration 8, loss = 0.007739834953099489
iteration 9, loss = 0.006537619512528181
iteration 10, loss = 0.005052873864769936
iteration 11, loss = 0.005654714070260525
iteration 12, loss = 0.004956620745360851
iteration 13, loss = 0.004976370837539434
iteration 14, loss = 0.007090784143656492
iteration 15, loss = 0.004292362369596958
iteration 16, loss = 0.00558463716879487
iteration 17, loss = 0.005692622158676386
iteration 18, loss = 0.0034646792337298393
iteration 19, loss = 0.00549301877617836
iteration 20, loss = 0.007071929983794689
iteration 21, loss = 0.006623897701501846
iteration 22, loss = 0.010867973789572716
iteration 23, loss = 0.00455027399584651
iteration 24, loss = 0.004437791183590889
iteration 25, loss = 0.0031302243005484343
iteration 26, loss = 0.012235765345394611
iteration 27, loss = 0.004381072241812944
iteration 28, loss = 0.004956587217748165
iteration 29, loss = 0.005829731933772564
iteration 30, loss = 0.006144266575574875
iteration 31, loss = 0.006344374269247055
iteration 32, loss = 0.00543098384514451
iteration 33, loss = 0.0037296824157238007
iteration 34, loss = 0.007431165315210819
iteration 35, loss = 0.004741291049867868
iteration 36, loss = 0.005168481729924679
iteration 37, loss = 0.005484061315655708
iteration 38, loss = 0.0032980020623654127
iteration 39, loss = 0.005969749763607979
iteration 40, loss = 0.005589708685874939
iteration 41, loss = 0.0059258779510855675
iteration 42, loss = 0.00506063224747777
iteration 43, loss = 0.005230327136814594
iteration 44, loss = 0.004954804666340351
iteration 45, loss = 0.004613399505615234
iteration 46, loss = 0.009125140495598316
iteration 47, loss = 0.0073848566971719265
iteration 48, loss = 0.006147992331534624
iteration 49, loss = 0.0075051928870379925
iteration 50, loss = 0.004183691926300526
iteration 51, loss = 0.0067606475204229355
iteration 52, loss = 0.007409469690173864
iteration 53, loss = 0.008682779967784882
iteration 54, loss = 0.004567028023302555
iteration 55, loss = 0.005298551172018051
iteration 56, loss = 0.006185182835906744
iteration 57, loss = 0.004186639096587896
iteration 58, loss = 0.0044337790459394455
iteration 59, loss = 0.0060266777873039246
iteration 60, loss = 0.004946641158312559
iteration 61, loss = 0.0037702322006225586
iteration 62, loss = 0.004407865926623344
iteration 63, loss = 0.0041321818716824055
iteration 64, loss = 0.006547732278704643
iteration 65, loss = 0.009362364187836647
iteration 66, loss = 0.008263535797595978
iteration 67, loss = 0.014662032946944237
iteration 68, loss = 0.00350686046294868
iteration 69, loss = 0.004531470127403736
iteration 70, loss = 0.004516005981713533
iteration 71, loss = 0.004148016218096018
iteration 72, loss = 0.004855955019593239
iteration 73, loss = 0.004447935614734888
iteration 74, loss = 0.004300625529140234
iteration 75, loss = 0.005884181708097458
iteration 76, loss = 0.0037612728774547577
iteration 77, loss = 0.0047526126727461815
iteration 78, loss = 0.007486572023481131
iteration 79, loss = 0.0038902375381439924
iteration 80, loss = 0.005121016409248114
iteration 81, loss = 0.006931487005203962
iteration 82, loss = 0.003417776431888342
iteration 83, loss = 0.005190394818782806
iteration 84, loss = 0.0035703747998923063
iteration 85, loss = 0.006655491888523102
iteration 86, loss = 0.004725856706500053
iteration 87, loss = 0.0038981193210929632
iteration 88, loss = 0.005466076545417309
iteration 89, loss = 0.007590366527438164
iteration 90, loss = 0.00427552405744791
iteration 91, loss = 0.004472179338335991
iteration 92, loss = 0.0037687502335757017
iteration 93, loss = 0.0047334907576441765
iteration 94, loss = 0.0031454304698854685
iteration 95, loss = 0.0032064623665064573
iteration 96, loss = 0.0039892904460430145
iteration 97, loss = 0.005408431403338909
iteration 98, loss = 0.0047258613631129265
iteration 99, loss = 0.005279660690575838
iteration 100, loss = 0.00663303816691041
iteration 101, loss = 0.011393801309168339
iteration 102, loss = 0.0060131303034722805
iteration 103, loss = 0.0045072464272379875
iteration 104, loss = 0.004709222819656134
iteration 105, loss = 0.005906286649405956
iteration 106, loss = 0.011906947009265423
iteration 107, loss = 0.004878072999417782
iteration 108, loss = 0.00509597547352314
iteration 109, loss = 0.004631220828741789
iteration 110, loss = 0.005958518944680691
iteration 111, loss = 0.004118071869015694
iteration 112, loss = 0.00395200215280056
iteration 113, loss = 0.003794696182012558
iteration 114, loss = 0.004933202639222145
iteration 115, loss = 0.003688310505822301
iteration 116, loss = 0.004980731755495071
iteration 117, loss = 0.004779322538524866
iteration 118, loss = 0.004909789655357599
iteration 119, loss = 0.011581918224692345
iteration 120, loss = 0.005976925604045391
iteration 121, loss = 0.004610975738614798
iteration 122, loss = 0.003635078901425004
iteration 123, loss = 0.006102454382926226
iteration 124, loss = 0.004740579053759575
iteration 125, loss = 0.005677842069417238
iteration 126, loss = 0.009109466336667538
iteration 127, loss = 0.007810242939740419
iteration 128, loss = 0.003484591841697693
iteration 129, loss = 0.0045410143211483955
iteration 130, loss = 0.0039540743455290794
iteration 131, loss = 0.004539978224784136
iteration 132, loss = 0.003794681979343295
iteration 133, loss = 0.002969359513372183
iteration 134, loss = 0.004633459262549877
iteration 135, loss = 0.006470239255577326
iteration 136, loss = 0.004157111048698425
iteration 137, loss = 0.004412336740642786
iteration 138, loss = 0.005316507071256638
iteration 139, loss = 0.00567478546872735
iteration 140, loss = 0.007216700818389654
iteration 141, loss = 0.006418527103960514
iteration 142, loss = 0.0036378733348101377
iteration 143, loss = 0.007930973544716835
iteration 144, loss = 0.00606051180511713
iteration 145, loss = 0.003213337389752269
iteration 146, loss = 0.00451030395925045
iteration 147, loss = 0.008466186001896858
iteration 148, loss = 0.00427958182990551
iteration 149, loss = 0.005528256297111511
iteration 150, loss = 0.004427634179592133
iteration 151, loss = 0.005721086636185646
iteration 152, loss = 0.0040828450582921505
iteration 153, loss = 0.004247007891535759
iteration 154, loss = 0.0049377381801605225
iteration 155, loss = 0.005418803542852402
iteration 156, loss = 0.003699570195749402
iteration 157, loss = 0.00609336793422699
iteration 158, loss = 0.0042203268967568874
iteration 159, loss = 0.003863430581986904
iteration 160, loss = 0.005117643624544144
iteration 161, loss = 0.00857622642070055
iteration 162, loss = 0.00677148625254631
iteration 163, loss = 0.00992795079946518
iteration 164, loss = 0.008668485097587109
iteration 165, loss = 0.009747873060405254
iteration 166, loss = 0.0036595729179680347
iteration 167, loss = 0.0037261336110532284
iteration 168, loss = 0.004858226515352726
iteration 169, loss = 0.0042053889483213425
iteration 170, loss = 0.003233018796890974
iteration 171, loss = 0.004917399026453495
iteration 172, loss = 0.006109497044235468
iteration 173, loss = 0.003574902890250087
iteration 174, loss = 0.004003249574452639
iteration 175, loss = 0.004405391402542591
iteration 176, loss = 0.004987843334674835
iteration 177, loss = 0.004973996430635452
iteration 178, loss = 0.004087352659553289
iteration 179, loss = 0.006291559897363186
iteration 180, loss = 0.004914622753858566
iteration 181, loss = 0.004353530704975128
iteration 182, loss = 0.01013216096907854
iteration 183, loss = 0.004030593205243349
iteration 184, loss = 0.004121299367398024
iteration 185, loss = 0.0034928740933537483
iteration 186, loss = 0.0036836587823927402
iteration 187, loss = 0.004558326676487923
iteration 188, loss = 0.009556559845805168
iteration 189, loss = 0.0028464924544095993
iteration 190, loss = 0.0033573200926184654
iteration 191, loss = 0.004009499214589596
iteration 192, loss = 0.00900364015251398
iteration 193, loss = 0.005151547957211733
iteration 194, loss = 0.004698632284998894
iteration 195, loss = 0.004576047882437706
iteration 196, loss = 0.003598134033381939
iteration 197, loss = 0.006707335822284222
iteration 198, loss = 0.004972547292709351
iteration 199, loss = 0.004103153012692928
iteration 200, loss = 0.006614763755351305
iteration 201, loss = 0.004906290210783482
iteration 202, loss = 0.003658347763121128
iteration 203, loss = 0.003704886417835951
iteration 204, loss = 0.0040410542860627174
iteration 205, loss = 0.0037732399068772793
iteration 206, loss = 0.003963505383580923
iteration 207, loss = 0.003293776884675026
iteration 208, loss = 0.004003703594207764
iteration 209, loss = 0.004524076357483864
iteration 210, loss = 0.005865250248461962
iteration 211, loss = 0.004052567295730114
iteration 212, loss = 0.0032608890905976295
iteration 213, loss = 0.004614640958607197
iteration 214, loss = 0.0132340332493186
iteration 215, loss = 0.005029016174376011
iteration 216, loss = 0.0038353251293301582
iteration 217, loss = 0.007531518582254648
iteration 218, loss = 0.010509510524570942
iteration 219, loss = 0.0038512444589287043
iteration 220, loss = 0.006915924604982138
iteration 221, loss = 0.0049313753843307495
iteration 222, loss = 0.003445266978815198
iteration 223, loss = 0.003670382546260953
iteration 224, loss = 0.00629917997866869
iteration 225, loss = 0.006054035387933254
iteration 226, loss = 0.003692399011924863
iteration 227, loss = 0.006315320730209351
iteration 228, loss = 0.00465979240834713
iteration 229, loss = 0.0035921544767916203
iteration 230, loss = 0.004086310043931007
iteration 231, loss = 0.003488621674478054
iteration 232, loss = 0.0038567164447158575
iteration 233, loss = 0.0029055981431156397
iteration 234, loss = 0.006228448357433081
iteration 235, loss = 0.004440535791218281
iteration 236, loss = 0.004125901497900486
iteration 237, loss = 0.0035440302453935146
iteration 238, loss = 0.003438330255448818
iteration 239, loss = 0.004325147718191147
iteration 240, loss = 0.005295907147228718
iteration 241, loss = 0.0060188123025000095
iteration 242, loss = 0.0037841012235730886
iteration 243, loss = 0.004175418056547642
iteration 244, loss = 0.004998552612960339
iteration 245, loss = 0.006398046389222145
iteration 246, loss = 0.004888187162578106
iteration 247, loss = 0.004203610587865114
iteration 248, loss = 0.0037450306117534637
iteration 249, loss = 0.004402033984661102
iteration 250, loss = 0.003956229891628027
iteration 251, loss = 0.0032024129759520292
iteration 252, loss = 0.00548201147466898
iteration 253, loss = 0.002980566816404462
iteration 254, loss = 0.004692224785685539
iteration 255, loss = 0.006604071240872145
iteration 256, loss = 0.0029564446303993464
iteration 257, loss = 0.003450343618169427
iteration 258, loss = 0.003566865576431155
iteration 259, loss = 0.005160507746040821
iteration 260, loss = 0.004634033888578415
iteration 261, loss = 0.0058500193990767
iteration 262, loss = 0.0032882262021303177
iteration 263, loss = 0.006091775372624397
iteration 264, loss = 0.006252097897231579
iteration 265, loss = 0.004544880706816912
iteration 266, loss = 0.0034607669804245234
iteration 267, loss = 0.008280714973807335
iteration 268, loss = 0.004298738669604063
iteration 269, loss = 0.00445642601698637
iteration 270, loss = 0.004595805890858173
iteration 271, loss = 0.0031853271648287773
iteration 272, loss = 0.0046190135180950165
iteration 273, loss = 0.008934849873185158
iteration 274, loss = 0.005355227738618851
iteration 275, loss = 0.004029377363622189
iteration 276, loss = 0.00459884200245142
iteration 277, loss = 0.004042844288051128
iteration 278, loss = 0.005623471457511187
iteration 279, loss = 0.003826930420473218
iteration 280, loss = 0.0034974366426467896
iteration 281, loss = 0.004652621224522591
iteration 282, loss = 0.0029388058464974165
iteration 283, loss = 0.004275157582014799
iteration 284, loss = 0.0037055525463074446
iteration 285, loss = 0.00391083350405097
iteration 286, loss = 0.0026681290473788977
iteration 287, loss = 0.003383523551747203
iteration 288, loss = 0.002795435255393386
iteration 289, loss = 0.004043716937303543
iteration 290, loss = 0.003437473438680172
iteration 291, loss = 0.0038596084341406822
iteration 292, loss = 0.0054954104125499725
iteration 293, loss = 0.005289955995976925
iteration 294, loss = 0.0034136506728827953
iteration 295, loss = 0.0035015232861042023
iteration 296, loss = 0.007202801760286093
iteration 297, loss = 0.005072839558124542
iteration 298, loss = 0.005400818772614002
iteration 299, loss = 0.0031529944390058517
iteration 0, loss = 0.004562877118587494
iteration 1, loss = 0.004431584849953651
iteration 2, loss = 0.003349045757204294
iteration 3, loss = 0.0045645833015441895
iteration 4, loss = 0.008541117422282696
iteration 5, loss = 0.004468776285648346
iteration 6, loss = 0.0061232056468725204
iteration 7, loss = 0.005099839996546507
iteration 8, loss = 0.004501769319176674
iteration 9, loss = 0.004780192393809557
iteration 10, loss = 0.003282120916992426
iteration 11, loss = 0.0034270768519490957
iteration 12, loss = 0.0030953488312661648
iteration 13, loss = 0.004752654582262039
iteration 14, loss = 0.003141645109280944
iteration 15, loss = 0.003998419735580683
iteration 16, loss = 0.003429785370826721
iteration 17, loss = 0.009313971735537052
iteration 18, loss = 0.004318782594054937
iteration 19, loss = 0.005090039223432541
iteration 20, loss = 0.004541277419775724
iteration 21, loss = 0.004327748902142048
iteration 22, loss = 0.007061255630105734
iteration 23, loss = 0.0033171833492815495
iteration 24, loss = 0.0036074472591280937
iteration 25, loss = 0.005224492866545916
iteration 26, loss = 0.003913318272680044
iteration 27, loss = 0.0039734006859362125
iteration 28, loss = 0.003298399271443486
iteration 29, loss = 0.0056458511389791965
iteration 30, loss = 0.003934117499738932
iteration 31, loss = 0.003969073295593262
iteration 32, loss = 0.0035797697491943836
iteration 33, loss = 0.008559430949389935
iteration 34, loss = 0.004364282824099064
iteration 35, loss = 0.003234084229916334
iteration 36, loss = 0.008598849177360535
iteration 37, loss = 0.004149862565100193
iteration 38, loss = 0.009547965601086617
iteration 39, loss = 0.006098794750869274
iteration 40, loss = 0.004897783510386944
iteration 41, loss = 0.0027153289411216974
iteration 42, loss = 0.002992040943354368
iteration 43, loss = 0.0032250976655632257
iteration 44, loss = 0.004828689154237509
iteration 45, loss = 0.004138148855417967
iteration 46, loss = 0.0043508997187018394
iteration 47, loss = 0.0031491501722484827
iteration 48, loss = 0.007284213788807392
iteration 49, loss = 0.0036077199038118124
iteration 50, loss = 0.007193760015070438
iteration 51, loss = 0.0036919498816132545
iteration 52, loss = 0.004582914523780346
iteration 53, loss = 0.004870974458754063
iteration 54, loss = 0.0042125205509364605
iteration 55, loss = 0.0034652594476938248
iteration 56, loss = 0.009319365955889225
iteration 57, loss = 0.0035042709205299616
iteration 58, loss = 0.00711216451600194
iteration 59, loss = 0.003534953109920025
iteration 60, loss = 0.003575892187654972
iteration 61, loss = 0.003962291870266199
iteration 62, loss = 0.003304681507870555
iteration 63, loss = 0.0034863415639847517
iteration 64, loss = 0.0053212703205645084
iteration 65, loss = 0.004468970466405153
iteration 66, loss = 0.00798927154392004
iteration 67, loss = 0.005691342055797577
iteration 68, loss = 0.006607780698686838
iteration 69, loss = 0.003260057419538498
iteration 70, loss = 0.004323356784880161
iteration 71, loss = 0.005225916393101215
iteration 72, loss = 0.003265512641519308
iteration 73, loss = 0.0032885465770959854
iteration 74, loss = 0.008919847197830677
iteration 75, loss = 0.005054271314293146
iteration 76, loss = 0.003353916807100177
iteration 77, loss = 0.0040519582107663155
iteration 78, loss = 0.0038878335617482662
iteration 79, loss = 0.004757692106068134
iteration 80, loss = 0.006512149702757597
iteration 81, loss = 0.009384100325405598
iteration 82, loss = 0.004008240066468716
iteration 83, loss = 0.004158647730946541
iteration 84, loss = 0.0030283345840871334
iteration 85, loss = 0.0036380323581397533
iteration 86, loss = 0.0031676695216447115
iteration 87, loss = 0.0054267775267362595
iteration 88, loss = 0.003893742337822914
iteration 89, loss = 0.005378671456128359
iteration 90, loss = 0.004174365662038326
iteration 91, loss = 0.0029938286170363426
iteration 92, loss = 0.004255735781043768
iteration 93, loss = 0.0032887908164411783
iteration 94, loss = 0.005568847060203552
iteration 95, loss = 0.003804141189903021
iteration 96, loss = 0.0037206606939435005
iteration 97, loss = 0.004504546523094177
iteration 98, loss = 0.005349805112928152
iteration 99, loss = 0.007351555861532688
iteration 100, loss = 0.00288179749622941
iteration 101, loss = 0.00447007454931736
iteration 102, loss = 0.003787000197917223
iteration 103, loss = 0.0031397631391882896
iteration 104, loss = 0.0028533670119941235
iteration 105, loss = 0.004019515588879585
iteration 106, loss = 0.004029010888189077
iteration 107, loss = 0.0029126231092959642
iteration 108, loss = 0.0038544891867786646
iteration 109, loss = 0.003490972565487027
iteration 110, loss = 0.0032849181443452835
iteration 111, loss = 0.0030351250898092985
iteration 112, loss = 0.0035357503220438957
iteration 113, loss = 0.004931792616844177
iteration 114, loss = 0.003437745152041316
iteration 115, loss = 0.0034102555364370346
iteration 116, loss = 0.004213293548673391
iteration 117, loss = 0.006239170208573341
iteration 118, loss = 0.0035803550854325294
iteration 119, loss = 0.002750933635979891
iteration 120, loss = 0.0034902961924672127
iteration 121, loss = 0.0038413465954363346
iteration 122, loss = 0.0035563847050070763
iteration 123, loss = 0.005693781189620495
iteration 124, loss = 0.006154065486043692
iteration 125, loss = 0.0033136075362563133
iteration 126, loss = 0.004543466027826071
iteration 127, loss = 0.0035560731776058674
iteration 128, loss = 0.003161507425829768
iteration 129, loss = 0.004382847808301449
iteration 130, loss = 0.006053529679775238
iteration 131, loss = 0.0056590246967971325
iteration 132, loss = 0.003928652964532375
iteration 133, loss = 0.0037656163331121206
iteration 134, loss = 0.003614193294197321
iteration 135, loss = 0.003023350378498435
iteration 136, loss = 0.0038466928526759148
iteration 137, loss = 0.0043703592382371426
iteration 138, loss = 0.004860303830355406
iteration 139, loss = 0.004083903506398201
iteration 140, loss = 0.004999426659196615
iteration 141, loss = 0.0051718116737902164
iteration 142, loss = 0.0037698077503591776
iteration 143, loss = 0.002875695703551173
iteration 144, loss = 0.00608849199488759
iteration 145, loss = 0.004359669517725706
iteration 146, loss = 0.004141515586525202
iteration 147, loss = 0.003518375102430582
iteration 148, loss = 0.005056288558989763
iteration 149, loss = 0.003297040006145835
iteration 150, loss = 0.0030378461815416813
iteration 151, loss = 0.003829214023426175
iteration 152, loss = 0.005717516876757145
iteration 153, loss = 0.0036725890822708607
iteration 154, loss = 0.003588263178244233
iteration 155, loss = 0.0032235635444521904
iteration 156, loss = 0.003999481908977032
iteration 157, loss = 0.003710689954459667
iteration 158, loss = 0.0035109964665025473
iteration 159, loss = 0.0034651337191462517
iteration 160, loss = 0.005140853580087423
iteration 161, loss = 0.0038845946546643972
iteration 162, loss = 0.005472414195537567
iteration 163, loss = 0.0029848439153283834
iteration 164, loss = 0.0052548847161233425
iteration 165, loss = 0.00295412540435791
iteration 166, loss = 0.004093628842383623
iteration 167, loss = 0.0027398415841162205
iteration 168, loss = 0.00463849538937211
iteration 169, loss = 0.004415858071297407
iteration 170, loss = 0.010022993199527264
iteration 171, loss = 0.0027278114575892687
iteration 172, loss = 0.006628073286265135
iteration 173, loss = 0.003534170566126704
iteration 174, loss = 0.0060711996629834175
iteration 175, loss = 0.0034308000467717648
iteration 176, loss = 0.005155827850103378
iteration 177, loss = 0.004570369608700275
iteration 178, loss = 0.0034549757838249207
iteration 179, loss = 0.003010690677911043
iteration 180, loss = 0.004204277414828539
iteration 181, loss = 0.002907101297751069
iteration 182, loss = 0.0030511075165122747
iteration 183, loss = 0.006472547072917223
iteration 184, loss = 0.004634345415979624
iteration 185, loss = 0.002395053394138813
iteration 186, loss = 0.005293761845678091
iteration 187, loss = 0.003230521222576499
iteration 188, loss = 0.004282345063984394
iteration 189, loss = 0.004754239693284035
iteration 190, loss = 0.003190391231328249
iteration 191, loss = 0.0033903056755661964
iteration 192, loss = 0.005561736412346363
iteration 193, loss = 0.0036990230437368155
iteration 194, loss = 0.0038221341092139482
iteration 195, loss = 0.003154034959152341
iteration 196, loss = 0.0035792312119156122
iteration 197, loss = 0.0036381545942276716
iteration 198, loss = 0.005546425934880972
iteration 199, loss = 0.003762072417885065
iteration 200, loss = 0.0036234657745808363
iteration 201, loss = 0.007399097550660372
iteration 202, loss = 0.00837811641395092
iteration 203, loss = 0.0037113986909389496
iteration 204, loss = 0.003467663424089551
iteration 205, loss = 0.006375467870384455
iteration 206, loss = 0.006005085539072752
iteration 207, loss = 0.006450391374528408
iteration 208, loss = 0.00294451299123466
iteration 209, loss = 0.003622886259108782
iteration 210, loss = 0.003802347229793668
iteration 211, loss = 0.003708834294229746
iteration 212, loss = 0.003086075186729431
iteration 213, loss = 0.0041494532488286495
iteration 214, loss = 0.003539731726050377
iteration 215, loss = 0.004150352440774441
iteration 216, loss = 0.004510384052991867
iteration 217, loss = 0.003603602759540081
iteration 218, loss = 0.0049382164143025875
iteration 219, loss = 0.0023903297260403633
iteration 220, loss = 0.004651191178709269
iteration 221, loss = 0.0028919747564941645
iteration 222, loss = 0.00312814861536026
iteration 223, loss = 0.004018377047032118
iteration 224, loss = 0.0036466424353420734
iteration 225, loss = 0.0042419820092618465
iteration 226, loss = 0.002826056908816099
iteration 227, loss = 0.003395767416805029
iteration 228, loss = 0.003917340654879808
iteration 229, loss = 0.0024592773988842964
iteration 230, loss = 0.0036755080800503492
iteration 231, loss = 0.003201201790943742
iteration 232, loss = 0.007593598682433367
iteration 233, loss = 0.004605066031217575
iteration 234, loss = 0.0025444745551794767
iteration 235, loss = 0.003299488453194499
iteration 236, loss = 0.002940097823739052
iteration 237, loss = 0.007832946255803108
iteration 238, loss = 0.003371776547282934
iteration 239, loss = 0.0037100131157785654
iteration 240, loss = 0.008249395526945591
iteration 241, loss = 0.003462466411292553
iteration 242, loss = 0.0037741458509117365
iteration 243, loss = 0.0034857867285609245
iteration 244, loss = 0.003175067249685526
iteration 245, loss = 0.0028845802880823612
iteration 246, loss = 0.0036100572906434536
iteration 247, loss = 0.0025853817351162434
iteration 248, loss = 0.0029582150746136904
iteration 249, loss = 0.00491170072928071
iteration 250, loss = 0.00445012841373682
iteration 251, loss = 0.002227224875241518
iteration 252, loss = 0.004757534712553024
iteration 253, loss = 0.004878875333815813
iteration 254, loss = 0.0032982933335006237
iteration 255, loss = 0.007488601375371218
iteration 256, loss = 0.0045275697484612465
iteration 257, loss = 0.0025828261859714985
iteration 258, loss = 0.002850073855370283
iteration 259, loss = 0.002975806826725602
iteration 260, loss = 0.004564790520817041
iteration 261, loss = 0.0036973897367715836
iteration 262, loss = 0.0026774255093187094
iteration 263, loss = 0.0030275462195277214
iteration 264, loss = 0.0024964050389826298
iteration 265, loss = 0.003156999358907342
iteration 266, loss = 0.0030480248387902975
iteration 267, loss = 0.003037297399714589
iteration 268, loss = 0.0027828861493617296
iteration 269, loss = 0.007746306248009205
iteration 270, loss = 0.003057722467929125
iteration 271, loss = 0.0026378952898085117
iteration 272, loss = 0.007429065182805061
iteration 273, loss = 0.003997706342488527
iteration 274, loss = 0.004354310221970081
iteration 275, loss = 0.004688819404691458
iteration 276, loss = 0.003238353645429015
iteration 277, loss = 0.0033733700402081013
iteration 278, loss = 0.003206065855920315
iteration 279, loss = 0.0033380554523319006
iteration 280, loss = 0.005361135117709637
iteration 281, loss = 0.0033115511760115623
iteration 282, loss = 0.002389083616435528
iteration 283, loss = 0.003424903145059943
iteration 284, loss = 0.004704771563410759
iteration 285, loss = 0.00395662896335125
iteration 286, loss = 0.0036693974398076534
iteration 287, loss = 0.0026726904325187206
iteration 288, loss = 0.00409771827980876
iteration 289, loss = 0.0025677348021417856
iteration 290, loss = 0.0022742964792996645
iteration 291, loss = 0.004637440666556358
iteration 292, loss = 0.003218704368919134
iteration 293, loss = 0.002123627346009016
iteration 294, loss = 0.0075647239573299885
iteration 295, loss = 0.006170713808387518
iteration 296, loss = 0.003258034586906433
iteration 297, loss = 0.005541086662560701
iteration 298, loss = 0.003762046806514263
iteration 299, loss = 0.003307512030005455
iteration 0, loss = 0.0032366616651415825
iteration 1, loss = 0.0038911416195333004
iteration 2, loss = 0.004274869803339243
iteration 3, loss = 0.004259306471794844
iteration 4, loss = 0.004551999270915985
iteration 5, loss = 0.0037667560391128063
iteration 6, loss = 0.0036355929914861917
iteration 7, loss = 0.0039059496484696865
iteration 8, loss = 0.003905130550265312
iteration 9, loss = 0.0032232748344540596
iteration 10, loss = 0.004663405939936638
iteration 11, loss = 0.0019470847910270095
iteration 12, loss = 0.0041394783183932304
iteration 13, loss = 0.0025953936856240034
iteration 14, loss = 0.00953754037618637
iteration 15, loss = 0.0026035760529339314
iteration 16, loss = 0.0036137436982244253
iteration 17, loss = 0.003011920489370823
iteration 18, loss = 0.00376755278557539
iteration 19, loss = 0.004077267833054066
iteration 20, loss = 0.006518709473311901
iteration 21, loss = 0.002763675758615136
iteration 22, loss = 0.0031648664735257626
iteration 23, loss = 0.002301635919138789
iteration 24, loss = 0.003167988732457161
iteration 25, loss = 0.00273394794203341
iteration 26, loss = 0.004203071817755699
iteration 27, loss = 0.0025419373996555805
iteration 28, loss = 0.00409885635599494
iteration 29, loss = 0.002391659654676914
iteration 30, loss = 0.0068060471676290035
iteration 31, loss = 0.0038402872160077095
iteration 32, loss = 0.003304603509604931
iteration 33, loss = 0.003267675871029496
iteration 34, loss = 0.0027206395752727985
iteration 35, loss = 0.0030202174093574286
iteration 36, loss = 0.002870934084057808
iteration 37, loss = 0.0031643249094486237
iteration 38, loss = 0.004935632459819317
iteration 39, loss = 0.0031386699993163347
iteration 40, loss = 0.0026065525598824024
iteration 41, loss = 0.0032704032491892576
iteration 42, loss = 0.004710379522293806
iteration 43, loss = 0.002892357762902975
iteration 44, loss = 0.004284457303583622
iteration 45, loss = 0.0035627686884254217
iteration 46, loss = 0.0023917527869343758
iteration 47, loss = 0.0028277544770389795
iteration 48, loss = 0.0031421370804309845
iteration 49, loss = 0.004108780529350042
iteration 50, loss = 0.0035413315054029226
iteration 51, loss = 0.0028716374654322863
iteration 52, loss = 0.0027342133689671755
iteration 53, loss = 0.0034420678857713938
iteration 54, loss = 0.004623884800821543
iteration 55, loss = 0.004588043782860041
iteration 56, loss = 0.007930658757686615
iteration 57, loss = 0.003444715403020382
iteration 58, loss = 0.0024304273538291454
iteration 59, loss = 0.006954326294362545
iteration 60, loss = 0.00273615843616426
iteration 61, loss = 0.0031410232186317444
iteration 62, loss = 0.0021293035242706537
iteration 63, loss = 0.0044308193027973175
iteration 64, loss = 0.0032018148340284824
iteration 65, loss = 0.003211966482922435
iteration 66, loss = 0.002420628210529685
iteration 67, loss = 0.003976225852966309
iteration 68, loss = 0.004084988497197628
iteration 69, loss = 0.003367055207490921
iteration 70, loss = 0.0037894807755947113
iteration 71, loss = 0.003969051409512758
iteration 72, loss = 0.0033365364652127028
iteration 73, loss = 0.0032782345078885555
iteration 74, loss = 0.004353788681328297
iteration 75, loss = 0.0029072128236293793
iteration 76, loss = 0.0031082811765372753
iteration 77, loss = 0.005939831025898457
iteration 78, loss = 0.003618439892306924
iteration 79, loss = 0.004281144123524427
iteration 80, loss = 0.004037813283503056
iteration 81, loss = 0.003748352639377117
iteration 82, loss = 0.003642033552750945
iteration 83, loss = 0.002570489654317498
iteration 84, loss = 0.0025549079291522503
iteration 85, loss = 0.00257915910333395
iteration 86, loss = 0.002832856960594654
iteration 87, loss = 0.00319476006552577
iteration 88, loss = 0.0031922287307679653
iteration 89, loss = 0.0033198390156030655
iteration 90, loss = 0.002764236181974411
iteration 91, loss = 0.0026452152524143457
iteration 92, loss = 0.002784929471090436
iteration 93, loss = 0.006495529320091009
iteration 94, loss = 0.003957008011639118
iteration 95, loss = 0.007883355021476746
iteration 96, loss = 0.0028297980315983295
iteration 97, loss = 0.0041006263345479965
iteration 98, loss = 0.0035861206706613302
iteration 99, loss = 0.0035248787607997656
iteration 100, loss = 0.003551710629835725
iteration 101, loss = 0.004848341457545757
iteration 102, loss = 0.003037834307178855
iteration 103, loss = 0.0032985336147248745
iteration 104, loss = 0.004423108883202076
iteration 105, loss = 0.003209321293979883
iteration 106, loss = 0.0027741091325879097
iteration 107, loss = 0.004607365000993013
iteration 108, loss = 0.0024939782451838255
iteration 109, loss = 0.005793605465441942
iteration 110, loss = 0.0037208576686680317
iteration 111, loss = 0.0025359138380736113
iteration 112, loss = 0.002286492381244898
iteration 113, loss = 0.0042086755856871605
iteration 114, loss = 0.002305554458871484
iteration 115, loss = 0.004582085181027651
iteration 116, loss = 0.002456074580550194
iteration 117, loss = 0.0069534508511424065
iteration 118, loss = 0.00237056496553123
iteration 119, loss = 0.003051717998459935
iteration 120, loss = 0.003671135986223817
iteration 121, loss = 0.004681985825300217
iteration 122, loss = 0.0024251779541373253
iteration 123, loss = 0.0028351787477731705
iteration 124, loss = 0.003433187957853079
iteration 125, loss = 0.002449413761496544
iteration 126, loss = 0.0033483384177088737
iteration 127, loss = 0.002500922419130802
iteration 128, loss = 0.007150194142013788
iteration 129, loss = 0.0034932007547467947
iteration 130, loss = 0.0029652847442775965
iteration 131, loss = 0.00247719488106668
iteration 132, loss = 0.004104958847165108
iteration 133, loss = 0.0073829516768455505
iteration 134, loss = 0.0032541058026254177
iteration 135, loss = 0.002948038512840867
iteration 136, loss = 0.0037393090315163136
iteration 137, loss = 0.003822810249403119
iteration 138, loss = 0.0024437960237264633
iteration 139, loss = 0.00268068490549922
iteration 140, loss = 0.0032888869754970074
iteration 141, loss = 0.003162343753501773
iteration 142, loss = 0.0032703804317861795
iteration 143, loss = 0.0036607899237424135
iteration 144, loss = 0.0035318410955369473
iteration 145, loss = 0.004179790150374174
iteration 146, loss = 0.0031418688595294952
iteration 147, loss = 0.0025179695803672075
iteration 148, loss = 0.003238660516217351
iteration 149, loss = 0.003880971111357212
iteration 150, loss = 0.0037660219240933657
iteration 151, loss = 0.0036850443575531244
iteration 152, loss = 0.003263806691393256
iteration 153, loss = 0.0031915209256112576
iteration 154, loss = 0.0034875182900577784
iteration 155, loss = 0.002739626681432128
iteration 156, loss = 0.002544039161875844
iteration 157, loss = 0.0030580898746848106
iteration 158, loss = 0.004867450334131718
iteration 159, loss = 0.0038312303368002176
iteration 160, loss = 0.00497391214594245
iteration 161, loss = 0.0023662224411964417
iteration 162, loss = 0.0030578901059925556
iteration 163, loss = 0.004420291632413864
iteration 164, loss = 0.004266834352165461
iteration 165, loss = 0.005081287119537592
iteration 166, loss = 0.0026492541655898094
iteration 167, loss = 0.0040364633314311504
iteration 168, loss = 0.0049498314037919044
iteration 169, loss = 0.0021008020266890526
iteration 170, loss = 0.002373133320361376
iteration 171, loss = 0.0033316570334136486
iteration 172, loss = 0.003514578565955162
iteration 173, loss = 0.004853362217545509
iteration 174, loss = 0.0024307705461978912
iteration 175, loss = 0.0025278343819081783
iteration 176, loss = 0.00397108681499958
iteration 177, loss = 0.0033187945373356342
iteration 178, loss = 0.003679038258269429
iteration 179, loss = 0.0061667729169130325
iteration 180, loss = 0.0035619703121483326
iteration 181, loss = 0.0030702974181622267
iteration 182, loss = 0.0024349228478968143
iteration 183, loss = 0.0033724731765687466
iteration 184, loss = 0.004481439478695393
iteration 185, loss = 0.002136565512046218
iteration 186, loss = 0.0036174498964101076
iteration 187, loss = 0.006625669077038765
iteration 188, loss = 0.003404895542189479
iteration 189, loss = 0.005384494084864855
iteration 190, loss = 0.0029792108107358217
iteration 191, loss = 0.0034402969758957624
iteration 192, loss = 0.0031565832905471325
iteration 193, loss = 0.004125463776290417
iteration 194, loss = 0.006730382796376944
iteration 195, loss = 0.004300564061850309
iteration 196, loss = 0.004314378369599581
iteration 197, loss = 0.0019005106296390295
iteration 198, loss = 0.003208538517355919
iteration 199, loss = 0.006148491054773331
iteration 200, loss = 0.003879471914842725
iteration 201, loss = 0.0033354414626955986
iteration 202, loss = 0.004737065639346838
iteration 203, loss = 0.004226223099976778
iteration 204, loss = 0.00314702489413321
iteration 205, loss = 0.0036523034796118736
iteration 206, loss = 0.003041128860786557
iteration 207, loss = 0.002850959310308099
iteration 208, loss = 0.0029560993425548077
iteration 209, loss = 0.0040956237353384495
iteration 210, loss = 0.0031852105166763067
iteration 211, loss = 0.003180482890456915
iteration 212, loss = 0.00350356288254261
iteration 213, loss = 0.0023040748201310635
iteration 214, loss = 0.0024537034332752228
iteration 215, loss = 0.0030104818288236856
iteration 216, loss = 0.0036963324528187513
iteration 217, loss = 0.0031829525250941515
iteration 218, loss = 0.0024879607371985912
iteration 219, loss = 0.004671868402510881
iteration 220, loss = 0.0043882341124117374
iteration 221, loss = 0.003888574428856373
iteration 222, loss = 0.003824331797659397
iteration 223, loss = 0.0047866166569292545
iteration 224, loss = 0.0030683912336826324
iteration 225, loss = 0.002126258797943592
iteration 226, loss = 0.003236900782212615
iteration 227, loss = 0.007047327235341072
iteration 228, loss = 0.0036850145552307367
iteration 229, loss = 0.004396022297441959
iteration 230, loss = 0.0022804050240665674
iteration 231, loss = 0.0030594714917242527
iteration 232, loss = 0.002935569966211915
iteration 233, loss = 0.002904907800257206
iteration 234, loss = 0.004651808179914951
iteration 235, loss = 0.004615769721567631
iteration 236, loss = 0.004701768979430199
iteration 237, loss = 0.0036195905413478613
iteration 238, loss = 0.0027203683275729418
iteration 239, loss = 0.0021068891510367393
iteration 240, loss = 0.002569083124399185
iteration 241, loss = 0.0022187908180058002
iteration 242, loss = 0.003274577436968684
iteration 243, loss = 0.006019324995577335
iteration 244, loss = 0.009295355528593063
iteration 245, loss = 0.0026117167435586452
iteration 246, loss = 0.0022288120817393064
iteration 247, loss = 0.003748734947293997
iteration 248, loss = 0.004278403706848621
iteration 249, loss = 0.002201538998633623
iteration 250, loss = 0.003428845200687647
iteration 251, loss = 0.0030025315936654806
iteration 252, loss = 0.0029178750701248646
iteration 253, loss = 0.0029070039745420218
iteration 254, loss = 0.002316227648407221
iteration 255, loss = 0.006628720555454493
iteration 256, loss = 0.0026127935852855444
iteration 257, loss = 0.004804892931133509
iteration 258, loss = 0.0028005323838442564
iteration 259, loss = 0.0034896454308182
iteration 260, loss = 0.002704599406570196
iteration 261, loss = 0.001928684301674366
iteration 262, loss = 0.00619285786524415
iteration 263, loss = 0.002841657493263483
iteration 264, loss = 0.0026563953142613173
iteration 265, loss = 0.0028393601533025503
iteration 266, loss = 0.003030202817171812
iteration 267, loss = 0.003048080950975418
iteration 268, loss = 0.005956179928034544
iteration 269, loss = 0.0024384644348174334
iteration 270, loss = 0.003076540306210518
iteration 271, loss = 0.009121977724134922
iteration 272, loss = 0.0020541190169751644
iteration 273, loss = 0.005372488871216774
iteration 274, loss = 0.0019074527081102133
iteration 275, loss = 0.004726090934127569
iteration 276, loss = 0.002271745353937149
iteration 277, loss = 0.002954751718789339
iteration 278, loss = 0.002546851057559252
iteration 279, loss = 0.0031457538716495037
iteration 280, loss = 0.0026643380988389254
iteration 281, loss = 0.0032885977998375893
iteration 282, loss = 0.0023831287398934364
iteration 283, loss = 0.004682610277086496
iteration 284, loss = 0.002524001756682992
iteration 285, loss = 0.0033812117762863636
iteration 286, loss = 0.0031207208521664143
iteration 287, loss = 0.0024437466636300087
iteration 288, loss = 0.003298502415418625
iteration 289, loss = 0.0028060157783329487
iteration 290, loss = 0.002671186812222004
iteration 291, loss = 0.0018510394729673862
iteration 292, loss = 0.0036236813757568598
iteration 293, loss = 0.0026076631620526314
iteration 294, loss = 0.002687512431293726
iteration 295, loss = 0.0023375304881483316
iteration 296, loss = 0.0031196149066090584
iteration 297, loss = 0.00460942042991519
iteration 298, loss = 0.006493809167295694
iteration 299, loss = 0.002529717981815338
iteration 0, loss = 0.0022552746813744307
iteration 1, loss = 0.00351076596416533
iteration 2, loss = 0.006512211170047522
iteration 3, loss = 0.0021035224199295044
iteration 4, loss = 0.004993974696844816
iteration 5, loss = 0.002719656564295292
iteration 6, loss = 0.003309770720079541
iteration 7, loss = 0.0026449039578437805
iteration 8, loss = 0.002659895922988653
iteration 9, loss = 0.003393627470359206
iteration 10, loss = 0.004643530584871769
iteration 11, loss = 0.0030231275595724583
iteration 12, loss = 0.002299321349710226
iteration 13, loss = 0.002544952789321542
iteration 14, loss = 0.0027912589721381664
iteration 15, loss = 0.003053019754588604
iteration 16, loss = 0.0034310834016650915
iteration 17, loss = 0.0024680979549884796
iteration 18, loss = 0.003392652375623584
iteration 19, loss = 0.002424833597615361
iteration 20, loss = 0.002700692741200328
iteration 21, loss = 0.0027607225347310305
iteration 22, loss = 0.0021244571544229984
iteration 23, loss = 0.002472860738635063
iteration 24, loss = 0.0036965757608413696
iteration 25, loss = 0.0028499215841293335
iteration 26, loss = 0.003908796235918999
iteration 27, loss = 0.002485267585143447
iteration 28, loss = 0.0027226004749536514
iteration 29, loss = 0.0026305417995899916
iteration 30, loss = 0.002505345270037651
iteration 31, loss = 0.0022781025618314743
iteration 32, loss = 0.0027324066031724215
iteration 33, loss = 0.0033033655490726233
iteration 34, loss = 0.0032507029827684164
iteration 35, loss = 0.0019897182937711477
iteration 36, loss = 0.003252656664699316
iteration 37, loss = 0.0024489115457981825
iteration 38, loss = 0.005261777900159359
iteration 39, loss = 0.004602108616381884
iteration 40, loss = 0.0021146228536963463
iteration 41, loss = 0.00578408082947135
iteration 42, loss = 0.0023505142889916897
iteration 43, loss = 0.002402333775535226
iteration 44, loss = 0.005653852596879005
iteration 45, loss = 0.003907176200300455
iteration 46, loss = 0.006028317846357822
iteration 47, loss = 0.002818323904648423
iteration 48, loss = 0.005844721104949713
iteration 49, loss = 0.003335462650284171
iteration 50, loss = 0.0026524625718593597
iteration 51, loss = 0.0020861816592514515
iteration 52, loss = 0.0024132595863193274
iteration 53, loss = 0.0027545690536499023
iteration 54, loss = 0.002731363521888852
iteration 55, loss = 0.002379530342295766
iteration 56, loss = 0.0028129322454333305
iteration 57, loss = 0.004480417817831039
iteration 58, loss = 0.0053834933787584305
iteration 59, loss = 0.0028833348769694567
iteration 60, loss = 0.0027016783133149147
iteration 61, loss = 0.003238047007471323
iteration 62, loss = 0.007225510198622942
iteration 63, loss = 0.0028380192816257477
iteration 64, loss = 0.00426869373768568
iteration 65, loss = 0.006202638149261475
iteration 66, loss = 0.003033692017197609
iteration 67, loss = 0.002282058820128441
iteration 68, loss = 0.001991615630686283
iteration 69, loss = 0.0021299519576132298
iteration 70, loss = 0.0024746062699705362
iteration 71, loss = 0.005323084536939859
iteration 72, loss = 0.0026341252960264683
iteration 73, loss = 0.0031850417144596577
iteration 74, loss = 0.004404814913868904
iteration 75, loss = 0.0032232259400188923
iteration 76, loss = 0.006021355744451284
iteration 77, loss = 0.0024602175690233707
iteration 78, loss = 0.003301230724900961
iteration 79, loss = 0.0021665231324732304
iteration 80, loss = 0.0029265733901411295
iteration 81, loss = 0.003423744346946478
iteration 82, loss = 0.0019476470770314336
iteration 83, loss = 0.0028362043667584658
iteration 84, loss = 0.0029633601661771536
iteration 85, loss = 0.0025287268217653036
iteration 86, loss = 0.0030543857719749212
iteration 87, loss = 0.004322080872952938
iteration 88, loss = 0.0038953081239014864
iteration 89, loss = 0.0033570118248462677
iteration 90, loss = 0.001960135530680418
iteration 91, loss = 0.003370133228600025
iteration 92, loss = 0.003010787069797516
iteration 93, loss = 0.002279769629240036
iteration 94, loss = 0.0026667925994843245
iteration 95, loss = 0.0022907492239028215
iteration 96, loss = 0.002085141371935606
iteration 97, loss = 0.001936301589012146
iteration 98, loss = 0.005767519120126963
iteration 99, loss = 0.0033319287467747927
iteration 100, loss = 0.002423189813271165
iteration 101, loss = 0.006571865174919367
iteration 102, loss = 0.002790330443531275
iteration 103, loss = 0.0033009203616529703
iteration 104, loss = 0.0029236001428216696
iteration 105, loss = 0.0027269134297966957
iteration 106, loss = 0.0037290670443326235
iteration 107, loss = 0.004109219182282686
iteration 108, loss = 0.002553725615143776
iteration 109, loss = 0.0027132725808769464
iteration 110, loss = 0.0031284899450838566
iteration 111, loss = 0.003934417385607958
iteration 112, loss = 0.0033416443038731813
iteration 113, loss = 0.0025140135549008846
iteration 114, loss = 0.0025010022800415754
iteration 115, loss = 0.0040749600157141685
iteration 116, loss = 0.0022756026592105627
iteration 117, loss = 0.0031950557604432106
iteration 118, loss = 0.0023262742906808853
iteration 119, loss = 0.0026938330847769976
iteration 120, loss = 0.0026980245020240545
iteration 121, loss = 0.0028199125081300735
iteration 122, loss = 0.002104588784277439
iteration 123, loss = 0.002660330617800355
iteration 124, loss = 0.002648895140737295
iteration 125, loss = 0.0026492129545658827
iteration 126, loss = 0.002225649543106556
iteration 127, loss = 0.004001445136964321
iteration 128, loss = 0.002471384359523654
iteration 129, loss = 0.0022441730834543705
iteration 130, loss = 0.008261604234576225
iteration 131, loss = 0.0022476264275610447
iteration 132, loss = 0.0036661738995462656
iteration 133, loss = 0.0027431417256593704
iteration 134, loss = 0.003956791013479233
iteration 135, loss = 0.002540478017181158
iteration 136, loss = 0.002688513370230794
iteration 137, loss = 0.005745194852352142
iteration 138, loss = 0.003170666517689824
iteration 139, loss = 0.002451864769682288
iteration 140, loss = 0.004123201593756676
iteration 141, loss = 0.002587307943031192
iteration 142, loss = 0.0037389006465673447
iteration 143, loss = 0.00192178413271904
iteration 144, loss = 0.00476063322275877
iteration 145, loss = 0.00225701741874218
iteration 146, loss = 0.0018996498547494411
iteration 147, loss = 0.002941583748906851
iteration 148, loss = 0.0027988834772258997
iteration 149, loss = 0.002238224260509014
iteration 150, loss = 0.004099220968782902
iteration 151, loss = 0.0027471589855849743
iteration 152, loss = 0.002300465479493141
iteration 153, loss = 0.002077870536595583
iteration 154, loss = 0.003380377544090152
iteration 155, loss = 0.0022133055608719587
iteration 156, loss = 0.0019654971547424793
iteration 157, loss = 0.002555439481511712
iteration 158, loss = 0.0030683143995702267
iteration 159, loss = 0.0021202305797487497
iteration 160, loss = 0.004190938081592321
iteration 161, loss = 0.002729005878791213
iteration 162, loss = 0.004817177075892687
iteration 163, loss = 0.003431395860388875
iteration 164, loss = 0.0027325700502842665
iteration 165, loss = 0.002832156838849187
iteration 166, loss = 0.003106105839833617
iteration 167, loss = 0.0030073202215135098
iteration 168, loss = 0.0022874961141496897
iteration 169, loss = 0.0019670675974339247
iteration 170, loss = 0.003110232762992382
iteration 171, loss = 0.002566465875133872
iteration 172, loss = 0.0024136584252119064
iteration 173, loss = 0.0033432510681450367
iteration 174, loss = 0.004096766468137503
iteration 175, loss = 0.0017321646446362138
iteration 176, loss = 0.002464854856953025
iteration 177, loss = 0.0017789819976314902
iteration 178, loss = 0.002227985765784979
iteration 179, loss = 0.005672155413776636
iteration 180, loss = 0.003154264995828271
iteration 181, loss = 0.0037673672195523977
iteration 182, loss = 0.004381966311484575
iteration 183, loss = 0.003442700719460845
iteration 184, loss = 0.005844220519065857
iteration 185, loss = 0.003356727072969079
iteration 186, loss = 0.0037707246374338865
iteration 187, loss = 0.0024923845194280148
iteration 188, loss = 0.003325951984152198
iteration 189, loss = 0.001980044413357973
iteration 190, loss = 0.0017277933657169342
iteration 191, loss = 0.002224111929535866
iteration 192, loss = 0.0028423022013157606
iteration 193, loss = 0.00228210655041039
iteration 194, loss = 0.0028918890748173
iteration 195, loss = 0.0016824660124257207
iteration 196, loss = 0.002772420644760132
iteration 197, loss = 0.0017216757405549288
iteration 198, loss = 0.002908947877585888
iteration 199, loss = 0.0024570119567215443
iteration 200, loss = 0.002761712297797203
iteration 201, loss = 0.0045447987504303455
iteration 202, loss = 0.002716619987040758
iteration 203, loss = 0.0020671645179390907
iteration 204, loss = 0.0026077046059072018
iteration 205, loss = 0.0017849202267825603
iteration 206, loss = 0.003981991671025753
iteration 207, loss = 0.0036954230163246393
iteration 208, loss = 0.001744491746649146
iteration 209, loss = 0.003148461692035198
iteration 210, loss = 0.0032348153181374073
iteration 211, loss = 0.0025596413761377335
iteration 212, loss = 0.0031759855337440968
iteration 213, loss = 0.0034740252885967493
iteration 214, loss = 0.006891446653753519
iteration 215, loss = 0.0021589677780866623
iteration 216, loss = 0.0024151792749762535
iteration 217, loss = 0.001995338359847665
iteration 218, loss = 0.002739788731560111
iteration 219, loss = 0.0024864086881279945
iteration 220, loss = 0.0026360522024333477
iteration 221, loss = 0.0029596583917737007
iteration 222, loss = 0.002032478339970112
iteration 223, loss = 0.002178083173930645
iteration 224, loss = 0.0026385230012238026
iteration 225, loss = 0.0031129219569265842
iteration 226, loss = 0.0024889595806598663
iteration 227, loss = 0.0024642415810376406
iteration 228, loss = 0.0016274824738502502
iteration 229, loss = 0.0022835053969174623
iteration 230, loss = 0.004468943458050489
iteration 231, loss = 0.005722855217754841
iteration 232, loss = 0.002750878920778632
iteration 233, loss = 0.00208451971411705
iteration 234, loss = 0.002683390397578478
iteration 235, loss = 0.002913211937993765
iteration 236, loss = 0.001987256808206439
iteration 237, loss = 0.0018139593303203583
iteration 238, loss = 0.0022037639282643795
iteration 239, loss = 0.004464860539883375
iteration 240, loss = 0.003713604062795639
iteration 241, loss = 0.008118938654661179
iteration 242, loss = 0.00616479804739356
iteration 243, loss = 0.002303185872733593
iteration 244, loss = 0.00255070673301816
iteration 245, loss = 0.003896174253895879
iteration 246, loss = 0.002647377084940672
iteration 247, loss = 0.0021773751359432936
iteration 248, loss = 0.0038450646679848433
iteration 249, loss = 0.0026835519820451736
iteration 250, loss = 0.001974080689251423
iteration 251, loss = 0.0019109123386442661
iteration 252, loss = 0.0020351901184767485
iteration 253, loss = 0.002049796748906374
iteration 254, loss = 0.0022109998390078545
iteration 255, loss = 0.004041111562401056
iteration 256, loss = 0.0023855241015553474
iteration 257, loss = 0.0030073230154812336
iteration 258, loss = 0.004475992172956467
iteration 259, loss = 0.002230235608294606
iteration 260, loss = 0.004288870375603437
iteration 261, loss = 0.002180475043132901
iteration 262, loss = 0.0023832395672798157
iteration 263, loss = 0.0022616456262767315
iteration 264, loss = 0.0024440507404506207
iteration 265, loss = 0.0022527521941810846
iteration 266, loss = 0.0026624768506735563
iteration 267, loss = 0.0026481400709599257
iteration 268, loss = 0.00233447621576488
iteration 269, loss = 0.0028071203269064426
iteration 270, loss = 0.006139985751360655
iteration 271, loss = 0.003080824390053749
iteration 272, loss = 0.001991651952266693
iteration 273, loss = 0.00393869960680604
iteration 274, loss = 0.0028452828992158175
iteration 275, loss = 0.0032823700457811356
iteration 276, loss = 0.0025173891335725784
iteration 277, loss = 0.0034268044400960207
iteration 278, loss = 0.0026124240830540657
iteration 279, loss = 0.003050556406378746
iteration 280, loss = 0.0032315459102392197
iteration 281, loss = 0.002228477504104376
iteration 282, loss = 0.0037957781460136175
iteration 283, loss = 0.001749604125507176
iteration 284, loss = 0.002368427347391844
iteration 285, loss = 0.0024764062836766243
iteration 286, loss = 0.004799298010766506
iteration 287, loss = 0.00390057940967381
iteration 288, loss = 0.0020159303676337004
iteration 289, loss = 0.004844021517783403
iteration 290, loss = 0.0030199617613106966
iteration 291, loss = 0.0027094907127320766
iteration 292, loss = 0.0031731566414237022
iteration 293, loss = 0.002774985274299979
iteration 294, loss = 0.0039006497245281935
iteration 295, loss = 0.0027146441861987114
iteration 296, loss = 0.0026424075476825237
iteration 297, loss = 0.0026536108925938606
iteration 298, loss = 0.0030086992774158716
iteration 299, loss = 0.002822370734065771
iteration 0, loss = 0.002705655060708523
iteration 1, loss = 0.0018248683772981167
iteration 2, loss = 0.001763010397553444
iteration 3, loss = 0.002175672445446253
iteration 4, loss = 0.0023939376696944237
iteration 5, loss = 0.006781729403883219
iteration 6, loss = 0.002109369495883584
iteration 7, loss = 0.0024755746126174927
iteration 8, loss = 0.0028208736330270767
iteration 9, loss = 0.0024275858886539936
iteration 10, loss = 0.003469770308583975
iteration 11, loss = 0.001650744816288352
iteration 12, loss = 0.0017197042470797896
iteration 13, loss = 0.0030660766642540693
iteration 14, loss = 0.002723386976867914
iteration 15, loss = 0.0022236837539821863
iteration 16, loss = 0.0025543393567204475
iteration 17, loss = 0.0021137353032827377
iteration 18, loss = 0.001709883683361113
iteration 19, loss = 0.003137743566185236
iteration 20, loss = 0.0025803400203585625
iteration 21, loss = 0.002780501265078783
iteration 22, loss = 0.002900049788877368
iteration 23, loss = 0.002771178260445595
iteration 24, loss = 0.002052201423794031
iteration 25, loss = 0.002609626855701208
iteration 26, loss = 0.002247155411168933
iteration 27, loss = 0.002426357939839363
iteration 28, loss = 0.0025499167386442423
iteration 29, loss = 0.0021191842388361692
iteration 30, loss = 0.001978538231924176
iteration 31, loss = 0.0024189648684114218
iteration 32, loss = 0.002780510811135173
iteration 33, loss = 0.0019067334942519665
iteration 34, loss = 0.0020521979313343763
iteration 35, loss = 0.002610032679513097
iteration 36, loss = 0.002628421178087592
iteration 37, loss = 0.005909031257033348
iteration 38, loss = 0.0032854173332452774
iteration 39, loss = 0.007508362177759409
iteration 40, loss = 0.003192368894815445
iteration 41, loss = 0.0025411704555153847
iteration 42, loss = 0.001514902920462191
iteration 43, loss = 0.0028996223118156195
iteration 44, loss = 0.002231057733297348
iteration 45, loss = 0.0026557007804512978
iteration 46, loss = 0.002161158947274089
iteration 47, loss = 0.0037868754006922245
iteration 48, loss = 0.002031860873103142
iteration 49, loss = 0.0033754913602024317
iteration 50, loss = 0.0016771000809967518
iteration 51, loss = 0.0020032620523124933
iteration 52, loss = 0.0022730701602995396
iteration 53, loss = 0.0031666348222643137
iteration 54, loss = 0.003571205073967576
iteration 55, loss = 0.002777562476694584
iteration 56, loss = 0.00575448339805007
iteration 57, loss = 0.0020257621072232723
iteration 58, loss = 0.0015817855019122362
iteration 59, loss = 0.0017003326211124659
iteration 60, loss = 0.002315287943929434
iteration 61, loss = 0.0022740610875189304
iteration 62, loss = 0.002354113385081291
iteration 63, loss = 0.0018591223051771522
iteration 64, loss = 0.0017867162823677063
iteration 65, loss = 0.0019097828771919012
iteration 66, loss = 0.0027624033391475677
iteration 67, loss = 0.00383006245829165
iteration 68, loss = 0.002287037204951048
iteration 69, loss = 0.0017115856753662229
iteration 70, loss = 0.002440978307276964
iteration 71, loss = 0.005864142905920744
iteration 72, loss = 0.0033123898319900036
iteration 73, loss = 0.00303838774561882
iteration 74, loss = 0.0035436158068478107
iteration 75, loss = 0.002370241330936551
iteration 76, loss = 0.0016527555417269468
iteration 77, loss = 0.0019385741325095296
iteration 78, loss = 0.004096764139831066
iteration 79, loss = 0.0025716174859553576
iteration 80, loss = 0.0018018862465396523
iteration 81, loss = 0.0020490637980401516
iteration 82, loss = 0.004436872433871031
iteration 83, loss = 0.003644892480224371
iteration 84, loss = 0.0019958915654569864
iteration 85, loss = 0.001900803530588746
iteration 86, loss = 0.002076168777421117
iteration 87, loss = 0.0018264198442921042
iteration 88, loss = 0.0031044266652315855
iteration 89, loss = 0.003959023859351873
iteration 90, loss = 0.005353351589292288
iteration 91, loss = 0.005951124243438244
iteration 92, loss = 0.005603122524917126
iteration 93, loss = 0.0021220126654952765
iteration 94, loss = 0.0016658230451866984
iteration 95, loss = 0.002655481221154332
iteration 96, loss = 0.002688416512683034
iteration 97, loss = 0.002647806191816926
iteration 98, loss = 0.0027638296596705914
iteration 99, loss = 0.0032373566646128893
iteration 100, loss = 0.005052171181887388
iteration 101, loss = 0.0019116485491394997
iteration 102, loss = 0.0029420119244605303
iteration 103, loss = 0.0062386225908994675
iteration 104, loss = 0.0023688196670264006
iteration 105, loss = 0.0022824606858193874
iteration 106, loss = 0.0021845526061952114
iteration 107, loss = 0.002192482817918062
iteration 108, loss = 0.0016843847697600722
iteration 109, loss = 0.002242987509816885
iteration 110, loss = 0.004126621875911951
iteration 111, loss = 0.0021918441634625196
iteration 112, loss = 0.003216730896383524
iteration 113, loss = 0.0029342658817768097
iteration 114, loss = 0.0022087174002081156
iteration 115, loss = 0.003428248455747962
iteration 116, loss = 0.0027529022190719843
iteration 117, loss = 0.0031138428021222353
iteration 118, loss = 0.002389737404882908
iteration 119, loss = 0.003086550859734416
iteration 120, loss = 0.0027632920537143946
iteration 121, loss = 0.0027186917141079903
iteration 122, loss = 0.00242426386103034
iteration 123, loss = 0.0031176090706139803
iteration 124, loss = 0.0021180070471018553
iteration 125, loss = 0.005046398378908634
iteration 126, loss = 0.003033851273357868
iteration 127, loss = 0.004419279750436544
iteration 128, loss = 0.0059343320317566395
iteration 129, loss = 0.003039028961211443
iteration 130, loss = 0.0024038234259933233
iteration 131, loss = 0.002435167320072651
iteration 132, loss = 0.003343540243804455
iteration 133, loss = 0.003916184417903423
iteration 134, loss = 0.0016550509026274085
iteration 135, loss = 0.0022837736178189516
iteration 136, loss = 0.0016838597366586328
iteration 137, loss = 0.0023580039851367474
iteration 138, loss = 0.002525854855775833
iteration 139, loss = 0.002866497728973627
iteration 140, loss = 0.0019441322656348348
iteration 141, loss = 0.0024814223870635033
iteration 142, loss = 0.0019622724503278732
iteration 143, loss = 0.0030136278364807367
iteration 144, loss = 0.0036035003140568733
iteration 145, loss = 0.00278105353936553
iteration 146, loss = 0.002369872760027647
iteration 147, loss = 0.003285601269453764
iteration 148, loss = 0.0016586543060839176
iteration 149, loss = 0.001441400614567101
iteration 150, loss = 0.0027545711491256952
iteration 151, loss = 0.0021369338501244783
iteration 152, loss = 0.0030131519306451082
iteration 153, loss = 0.0028487425297498703
iteration 154, loss = 0.0014843088574707508
iteration 155, loss = 0.0015012524090707302
iteration 156, loss = 0.0027719701174646616
iteration 157, loss = 0.002428668551146984
iteration 158, loss = 0.0019989414140582085
iteration 159, loss = 0.0022037439048290253
iteration 160, loss = 0.0026526383589953184
iteration 161, loss = 0.00224871258251369
iteration 162, loss = 0.002581522334367037
iteration 163, loss = 0.0039052811916917562
iteration 164, loss = 0.005073626060038805
iteration 165, loss = 0.0019482341594994068
iteration 166, loss = 0.0015847699251025915
iteration 167, loss = 0.0018899363931268454
iteration 168, loss = 0.001908100675791502
iteration 169, loss = 0.0024547860957682133
iteration 170, loss = 0.0020135114900767803
iteration 171, loss = 0.0023685460910201073
iteration 172, loss = 0.0017037760699167848
iteration 173, loss = 0.0037441339809447527
iteration 174, loss = 0.0028488619718700647
iteration 175, loss = 0.0034037528093904257
iteration 176, loss = 0.0019059684127569199
iteration 177, loss = 0.0023584295995533466
iteration 178, loss = 0.0021004469599574804
iteration 179, loss = 0.0025993247982114553
iteration 180, loss = 0.00405146274715662
iteration 181, loss = 0.0017833916936069727
iteration 182, loss = 0.0020296317525207996
iteration 183, loss = 0.0020286794751882553
iteration 184, loss = 0.003372767474502325
iteration 185, loss = 0.0035794114228338003
iteration 186, loss = 0.0031903099734336138
iteration 187, loss = 0.0028045133221894503
iteration 188, loss = 0.0019781766459345818
iteration 189, loss = 0.0015438847476616502
iteration 190, loss = 0.0019124399404972792
iteration 191, loss = 0.002711651613935828
iteration 192, loss = 0.0020215476397424936
iteration 193, loss = 0.0024675524327903986
iteration 194, loss = 0.0017727988306432962
iteration 195, loss = 0.002632895251736045
iteration 196, loss = 0.00185679632704705
iteration 197, loss = 0.0037644903641194105
iteration 198, loss = 0.0026777407620102167
iteration 199, loss = 0.002374546602368355
iteration 200, loss = 0.0021244394592940807
iteration 201, loss = 0.0038827715907245874
iteration 202, loss = 0.0014768900582566857
iteration 203, loss = 0.0026886581908911467
iteration 204, loss = 0.002382888924330473
iteration 205, loss = 0.001629926380701363
iteration 206, loss = 0.0018313971813768148
iteration 207, loss = 0.0017508325399830937
iteration 208, loss = 0.0019547182600945234
iteration 209, loss = 0.0022722394205629826
iteration 210, loss = 0.0019717123359441757
iteration 211, loss = 0.0020807937253266573
iteration 212, loss = 0.002253576647490263
iteration 213, loss = 0.006936414632946253
iteration 214, loss = 0.002282972214743495
iteration 215, loss = 0.0019106378313153982
iteration 216, loss = 0.0019728150218725204
iteration 217, loss = 0.0018357414519414306
iteration 218, loss = 0.0015727391000837088
iteration 219, loss = 0.0038641600403934717
iteration 220, loss = 0.0027439878322184086
iteration 221, loss = 0.004900417756289244
iteration 222, loss = 0.00244410103186965
iteration 223, loss = 0.002814504085108638
iteration 224, loss = 0.0021284683607518673
iteration 225, loss = 0.0021353380288928747
iteration 226, loss = 0.00259801116771996
iteration 227, loss = 0.001767362467944622
iteration 228, loss = 0.0020642115268856287
iteration 229, loss = 0.0012957964790984988
iteration 230, loss = 0.0024352397304028273
iteration 231, loss = 0.002498957794159651
iteration 232, loss = 0.0023154213558882475
iteration 233, loss = 0.0022351741790771484
iteration 234, loss = 0.002111398382112384
iteration 235, loss = 0.0019348330097272992
iteration 236, loss = 0.0028063624631613493
iteration 237, loss = 0.00379289616830647
iteration 238, loss = 0.0020498388912528753
iteration 239, loss = 0.00176569155883044
iteration 240, loss = 0.0022287084721028805
iteration 241, loss = 0.0014808133710175753
iteration 242, loss = 0.002266988158226013
iteration 243, loss = 0.002213424537330866
iteration 244, loss = 0.004545340780168772
iteration 245, loss = 0.001748427632264793
iteration 246, loss = 0.004198139999061823
iteration 247, loss = 0.0027435093652457
iteration 248, loss = 0.00331956148147583
iteration 249, loss = 0.003398732515051961
iteration 250, loss = 0.0017162583535537124
iteration 251, loss = 0.002182617085054517
iteration 252, loss = 0.0022644170094281435
iteration 253, loss = 0.0038147051818668842
iteration 254, loss = 0.0019202664261683822
iteration 255, loss = 0.0020880596712231636
iteration 256, loss = 0.003453674726188183
iteration 257, loss = 0.004247906617820263
iteration 258, loss = 0.0031379861757159233
iteration 259, loss = 0.002192093525081873
iteration 260, loss = 0.001922078663483262
iteration 261, loss = 0.006346522364765406
iteration 262, loss = 0.002287081675603986
iteration 263, loss = 0.001838037627749145
iteration 264, loss = 0.003065811935812235
iteration 265, loss = 0.0021621265914291143
iteration 266, loss = 0.003061882918700576
iteration 267, loss = 0.004775234032422304
iteration 268, loss = 0.0018149691168218851
iteration 269, loss = 0.0026901671662926674
iteration 270, loss = 0.002338077174499631
iteration 271, loss = 0.0016894186846911907
iteration 272, loss = 0.00270279822871089
iteration 273, loss = 0.002374205505475402
iteration 274, loss = 0.005479415412992239
iteration 275, loss = 0.002180794719606638
iteration 276, loss = 0.0024296967312693596
iteration 277, loss = 0.0019936831668019295
iteration 278, loss = 0.002889488125219941
iteration 279, loss = 0.0030603944323956966
iteration 280, loss = 0.0023576279636472464
iteration 281, loss = 0.0016538259806111455
iteration 282, loss = 0.004725633189082146
iteration 283, loss = 0.002095048548653722
iteration 284, loss = 0.002428078791126609
iteration 285, loss = 0.0035637561231851578
iteration 286, loss = 0.0026318433228880167
iteration 287, loss = 0.0017715182621032
iteration 288, loss = 0.003287860192358494
iteration 289, loss = 0.0030025364831089973
iteration 290, loss = 0.0018335952190682292
iteration 291, loss = 0.0025340761058032513
iteration 292, loss = 0.002377050230279565
iteration 293, loss = 0.001657247543334961
iteration 294, loss = 0.0036070027854293585
iteration 295, loss = 0.00296153430826962
iteration 296, loss = 0.0020925309509038925
iteration 297, loss = 0.0034025677014142275
iteration 298, loss = 0.0026249410584568977
iteration 299, loss = 0.0020239558070898056
iteration 0, loss = 0.0020778696052730083
iteration 1, loss = 0.0031988106202334166
iteration 2, loss = 0.0022081679198890924
iteration 3, loss = 0.0026171973440796137
iteration 4, loss = 0.00240451586432755
iteration 5, loss = 0.002261892193928361
iteration 6, loss = 0.002799315843731165
iteration 7, loss = 0.002029125113040209
iteration 8, loss = 0.0017361455829814076
iteration 9, loss = 0.0017051202012225986
iteration 10, loss = 0.002716458635404706
iteration 11, loss = 0.0024870913475751877
iteration 12, loss = 0.0022804290056228638
iteration 13, loss = 0.0017445425037294626
iteration 14, loss = 0.0035830424167215824
iteration 15, loss = 0.003200766397640109
iteration 16, loss = 0.005675299093127251
iteration 17, loss = 0.0018188607646152377
iteration 18, loss = 0.0032673475798219442
iteration 19, loss = 0.003694613231346011
iteration 20, loss = 0.0017700160387903452
iteration 21, loss = 0.0021079762373119593
iteration 22, loss = 0.0020637433044612408
iteration 23, loss = 0.0026085420977324247
iteration 24, loss = 0.0022855675779283047
iteration 25, loss = 0.00165658222977072
iteration 26, loss = 0.0016450535040348768
iteration 27, loss = 0.0019948205444961786
iteration 28, loss = 0.001984629314392805
iteration 29, loss = 0.002086594933643937
iteration 30, loss = 0.002448074286803603
iteration 31, loss = 0.002387168351560831
iteration 32, loss = 0.0021550508681684732
iteration 33, loss = 0.002293009776622057
iteration 34, loss = 0.002740804571658373
iteration 35, loss = 0.005446088034659624
iteration 36, loss = 0.002990987151861191
iteration 37, loss = 0.003268199972808361
iteration 38, loss = 0.001437945058569312
iteration 39, loss = 0.0015628503169864416
iteration 40, loss = 0.0017142074648290873
iteration 41, loss = 0.00217590294778347
iteration 42, loss = 0.0014853671891614795
iteration 43, loss = 0.002107408130541444
iteration 44, loss = 0.002535356907173991
iteration 45, loss = 0.001407281612046063
iteration 46, loss = 0.0021117322612553835
iteration 47, loss = 0.0032781062182039022
iteration 48, loss = 0.0024189113173633814
iteration 49, loss = 0.0024241481442004442
iteration 50, loss = 0.0021817637607455254
iteration 51, loss = 0.002085160929709673
iteration 52, loss = 0.0026303622871637344
iteration 53, loss = 0.0021736635826528072
iteration 54, loss = 0.001649342942982912
iteration 55, loss = 0.0022164685651659966
iteration 56, loss = 0.0025046677328646183
iteration 57, loss = 0.0021393669303506613
iteration 58, loss = 0.003258815035223961
iteration 59, loss = 0.002101865131407976
iteration 60, loss = 0.0028599423822015524
iteration 61, loss = 0.004966487176716328
iteration 62, loss = 0.0022110675927251577
iteration 63, loss = 0.0019952156580984592
iteration 64, loss = 0.0019609469454735518
iteration 65, loss = 0.0021622776985168457
iteration 66, loss = 0.0021792370826005936
iteration 67, loss = 0.001561945304274559
iteration 68, loss = 0.001873284811154008
iteration 69, loss = 0.005391909275203943
iteration 70, loss = 0.002037146594375372
iteration 71, loss = 0.0017928086454048753
iteration 72, loss = 0.002162579447031021
iteration 73, loss = 0.0031504640355706215
iteration 74, loss = 0.0028394756373018026
iteration 75, loss = 0.00255953730084002
iteration 76, loss = 0.0027748371940106153
iteration 77, loss = 0.0017628601053729653
iteration 78, loss = 0.0032428819686174393
iteration 79, loss = 0.0023049372248351574
iteration 80, loss = 0.0015679813222959638
iteration 81, loss = 0.002619926817715168
iteration 82, loss = 0.0015729705337435007
iteration 83, loss = 0.0017006315756589174
iteration 84, loss = 0.001990292686969042
iteration 85, loss = 0.002552255755290389
iteration 86, loss = 0.0023426380939781666
iteration 87, loss = 0.002311825519427657
iteration 88, loss = 0.0026176213286817074
iteration 89, loss = 0.0017924589337781072
iteration 90, loss = 0.004054664168506861
iteration 91, loss = 0.002164161764085293
iteration 92, loss = 0.003285705577582121
iteration 93, loss = 0.0025019787717610598
iteration 94, loss = 0.0037801226135343313
iteration 95, loss = 0.0020531881600618362
iteration 96, loss = 0.0025659543462097645
iteration 97, loss = 0.0018412246135994792
iteration 98, loss = 0.002283060224726796
iteration 99, loss = 0.0018633895087987185
iteration 100, loss = 0.002431885339319706
iteration 101, loss = 0.0018431739881634712
iteration 102, loss = 0.0025042968336492777
iteration 103, loss = 0.0015547898365184665
iteration 104, loss = 0.00303473393432796
iteration 105, loss = 0.0016739423153921962
iteration 106, loss = 0.0021258860360831022
iteration 107, loss = 0.005758157931268215
iteration 108, loss = 0.002045462606474757
iteration 109, loss = 0.0025969159323722124
iteration 110, loss = 0.0019911108538508415
iteration 111, loss = 0.003137280000373721
iteration 112, loss = 0.0014928114833310246
iteration 113, loss = 0.0020205643959343433
iteration 114, loss = 0.0017433961620554328
iteration 115, loss = 0.0035520996898412704
iteration 116, loss = 0.0016846895450726151
iteration 117, loss = 0.001814320683479309
iteration 118, loss = 0.003693333361297846
iteration 119, loss = 0.0015324283158406615
iteration 120, loss = 0.001579896779730916
iteration 121, loss = 0.0023536880034953356
iteration 122, loss = 0.0026409400161355734
iteration 123, loss = 0.0030875338707119226
iteration 124, loss = 0.0013985681580379605
iteration 125, loss = 0.003008659929037094
iteration 126, loss = 0.002877855906262994
iteration 127, loss = 0.0018898978596553206
iteration 128, loss = 0.0016235330840572715
iteration 129, loss = 0.0028956022579222918
iteration 130, loss = 0.002907796762883663
iteration 131, loss = 0.0015966041246429086
iteration 132, loss = 0.002564499154686928
iteration 133, loss = 0.0022545026149600744
iteration 134, loss = 0.002373943105340004
iteration 135, loss = 0.0016255286755040288
iteration 136, loss = 0.0018019748385995626
iteration 137, loss = 0.0028086360543966293
iteration 138, loss = 0.002295498503372073
iteration 139, loss = 0.0019519752822816372
iteration 140, loss = 0.0036494769155979156
iteration 141, loss = 0.0016180385136976838
iteration 142, loss = 0.001566203311085701
iteration 143, loss = 0.0028258231468498707
iteration 144, loss = 0.001546634128317237
iteration 145, loss = 0.0017285366775467992
iteration 146, loss = 0.0015914083924144506
iteration 147, loss = 0.0020946136210113764
iteration 148, loss = 0.0020173098891973495
iteration 149, loss = 0.001811882248148322
iteration 150, loss = 0.0035142344422638416
iteration 151, loss = 0.002796606160700321
iteration 152, loss = 0.004714051261544228
iteration 153, loss = 0.003176105907186866
iteration 154, loss = 0.0019777812995016575
iteration 155, loss = 0.002269998425617814
iteration 156, loss = 0.0018962608883157372
iteration 157, loss = 0.00290262745693326
iteration 158, loss = 0.001876302994787693
iteration 159, loss = 0.0030558661092072725
iteration 160, loss = 0.00227790093049407
iteration 161, loss = 0.001853750552982092
iteration 162, loss = 0.0018027342157438397
iteration 163, loss = 0.005827386397868395
iteration 164, loss = 0.0015720123192295432
iteration 165, loss = 0.003098899032920599
iteration 166, loss = 0.0016849092207849026
iteration 167, loss = 0.0029943962581455708
iteration 168, loss = 0.00214678468182683
iteration 169, loss = 0.0015491837402805686
iteration 170, loss = 0.0014514431823045015
iteration 171, loss = 0.001341389142908156
iteration 172, loss = 0.0045232512056827545
iteration 173, loss = 0.003080368274822831
iteration 174, loss = 0.002317459788173437
iteration 175, loss = 0.0020471615716814995
iteration 176, loss = 0.0023019020445644855
iteration 177, loss = 0.004420715384185314
iteration 178, loss = 0.002451636828482151
iteration 179, loss = 0.0018386304145678878
iteration 180, loss = 0.003087260527536273
iteration 181, loss = 0.0018233979353681207
iteration 182, loss = 0.0018452440854161978
iteration 183, loss = 0.003130057593807578
iteration 184, loss = 0.0030181664042174816
iteration 185, loss = 0.0020948518067598343
iteration 186, loss = 0.001530351466499269
iteration 187, loss = 0.0021529076620936394
iteration 188, loss = 0.0017118490068241954
iteration 189, loss = 0.0018262388184666634
iteration 190, loss = 0.0034032820258289576
iteration 191, loss = 0.0018475502729415894
iteration 192, loss = 0.0013980099465698004
iteration 193, loss = 0.0014055055798962712
iteration 194, loss = 0.0013319728896021843
iteration 195, loss = 0.0013740173308178782
iteration 196, loss = 0.002427173312753439
iteration 197, loss = 0.001348298043012619
iteration 198, loss = 0.0023345197550952435
iteration 199, loss = 0.0019769726786762476
iteration 200, loss = 0.0015003435546532273
iteration 201, loss = 0.0023578128311783075
iteration 202, loss = 0.001727920025587082
iteration 203, loss = 0.0013513516169041395
iteration 204, loss = 0.002397762844339013
iteration 205, loss = 0.0021760021336376667
iteration 206, loss = 0.0010782763129100204
iteration 207, loss = 0.0024888073094189167
iteration 208, loss = 0.0017911788308992982
iteration 209, loss = 0.0022548562847077847
iteration 210, loss = 0.0026820185594260693
iteration 211, loss = 0.00210418994538486
iteration 212, loss = 0.0012717577628791332
iteration 213, loss = 0.0021672111470252275
iteration 214, loss = 0.003287401981651783
iteration 215, loss = 0.001567952218465507
iteration 216, loss = 0.005388814490288496
iteration 217, loss = 0.0021256341133266687
iteration 218, loss = 0.005764648783951998
iteration 219, loss = 0.0014144864398986101
iteration 220, loss = 0.004305026028305292
iteration 221, loss = 0.0026003969833254814
iteration 222, loss = 0.0028868112713098526
iteration 223, loss = 0.0016970187425613403
iteration 224, loss = 0.0019212013576179743
iteration 225, loss = 0.0025053834542632103
iteration 226, loss = 0.001745807472616434
iteration 227, loss = 0.003068762132897973
iteration 228, loss = 0.0017651735106483102
iteration 229, loss = 0.0021652276627719402
iteration 230, loss = 0.0015517513966187835
iteration 231, loss = 0.0014168398920446634
iteration 232, loss = 0.001604981254786253
iteration 233, loss = 0.002676727483049035
iteration 234, loss = 0.0013294498203322291
iteration 235, loss = 0.0022206392604857683
iteration 236, loss = 0.0018150035757571459
iteration 237, loss = 0.001845771330408752
iteration 238, loss = 0.002160897245630622
iteration 239, loss = 0.001962633104994893
iteration 240, loss = 0.005094111431390047
iteration 241, loss = 0.0024198542814701796
iteration 242, loss = 0.0026961583644151688
iteration 243, loss = 0.002179529285058379
iteration 244, loss = 0.0018851457862183452
iteration 245, loss = 0.00315386475995183
iteration 246, loss = 0.001726113143377006
iteration 247, loss = 0.00157487066462636
iteration 248, loss = 0.0017814597813412547
iteration 249, loss = 0.005147314630448818
iteration 250, loss = 0.0020262051839381456
iteration 251, loss = 0.004823913332074881
iteration 252, loss = 0.0031738998368382454
iteration 253, loss = 0.0026859340723603964
iteration 254, loss = 0.0019099683267995715
iteration 255, loss = 0.0023490681778639555
iteration 256, loss = 0.0034607499837875366
iteration 257, loss = 0.002974146045744419
iteration 258, loss = 0.0021867030300199986
iteration 259, loss = 0.0018004958983510733
iteration 260, loss = 0.0017087170854210854
iteration 261, loss = 0.0033945408649742603
iteration 262, loss = 0.0023068629670888186
iteration 263, loss = 0.001650789869017899
iteration 264, loss = 0.002043926389887929
iteration 265, loss = 0.0027756241615861654
iteration 266, loss = 0.002901882166042924
iteration 267, loss = 0.002218978013843298
iteration 268, loss = 0.0018671766156330705
iteration 269, loss = 0.0015977700240910053
iteration 270, loss = 0.001958499662578106
iteration 271, loss = 0.0016083202790468931
iteration 272, loss = 0.0019352748058736324
iteration 273, loss = 0.0017098186071962118
iteration 274, loss = 0.002773835090920329
iteration 275, loss = 0.0013840895844623446
iteration 276, loss = 0.0042219990864396095
iteration 277, loss = 0.001222586608491838
iteration 278, loss = 0.0015531594399362803
iteration 279, loss = 0.003168661380186677
iteration 280, loss = 0.0033448534086346626
iteration 281, loss = 0.0013360263546928763
iteration 282, loss = 0.001960077555850148
iteration 283, loss = 0.002358954632654786
iteration 284, loss = 0.0025851232931017876
iteration 285, loss = 0.0014848329592496157
iteration 286, loss = 0.0027008820325136185
iteration 287, loss = 0.004452061373740435
iteration 288, loss = 0.0024073785170912743
iteration 289, loss = 0.0012665664544329047
iteration 290, loss = 0.0023189648054540157
iteration 291, loss = 0.0021679275669157505
iteration 292, loss = 0.002134141279384494
iteration 293, loss = 0.003231448121368885
iteration 294, loss = 0.0022086494136601686
iteration 295, loss = 0.0015643618535250425
iteration 296, loss = 0.0012674018507823348
iteration 297, loss = 0.0020059626549482346
iteration 298, loss = 0.002817475702613592
iteration 299, loss = 0.001612353720702231
iteration 0, loss = 0.002315073972567916
iteration 1, loss = 0.0018328198930248618
iteration 2, loss = 0.0017743776552379131
iteration 3, loss = 0.0023453368339687586
iteration 4, loss = 0.003403519047424197
iteration 5, loss = 0.0032831865828484297
iteration 6, loss = 0.0038968324661254883
iteration 7, loss = 0.0017463227268308401
iteration 8, loss = 0.0021757904905825853
iteration 9, loss = 0.0018530824454501271
iteration 10, loss = 0.0029466571286320686
iteration 11, loss = 0.002071319380775094
iteration 12, loss = 0.0013853994896635413
iteration 13, loss = 0.002283190842717886
iteration 14, loss = 0.0033393558114767075
iteration 15, loss = 0.0013727631885558367
iteration 16, loss = 0.0020722663030028343
iteration 17, loss = 0.0024199127219617367
iteration 18, loss = 0.0017156905960291624
iteration 19, loss = 0.0017328745452687144
iteration 20, loss = 0.0010425979271531105
iteration 21, loss = 0.0014858345966786146
iteration 22, loss = 0.0017773173749446869
iteration 23, loss = 0.0026837335899472237
iteration 24, loss = 0.0016130153089761734
iteration 25, loss = 0.0014890240272507071
iteration 26, loss = 0.0016658110544085503
iteration 27, loss = 0.0025297629181295633
iteration 28, loss = 0.0016804873012006283
iteration 29, loss = 0.0016662271227687597
iteration 30, loss = 0.0025759900454431772
iteration 31, loss = 0.0027843837160617113
iteration 32, loss = 0.002203586744144559
iteration 33, loss = 0.005124817136675119
iteration 34, loss = 0.0011917536612600088
iteration 35, loss = 0.0017079085810109973
iteration 36, loss = 0.0033427386078983545
iteration 37, loss = 0.002045620698481798
iteration 38, loss = 0.004894303157925606
iteration 39, loss = 0.0013995375484228134
iteration 40, loss = 0.002107110805809498
iteration 41, loss = 0.0022018542513251305
iteration 42, loss = 0.001854851609095931
iteration 43, loss = 0.0014844448305666447
iteration 44, loss = 0.0013957624323666096
iteration 45, loss = 0.002048791153356433
iteration 46, loss = 0.0019088004482910037
iteration 47, loss = 0.006564745679497719
iteration 48, loss = 0.0033841892145574093
iteration 49, loss = 0.0017983994912356138
iteration 50, loss = 0.0014470190508291125
iteration 51, loss = 0.0014694661367684603
iteration 52, loss = 0.0016333481762558222
iteration 53, loss = 0.0015380200929939747
iteration 54, loss = 0.0013266033492982388
iteration 55, loss = 0.004865274764597416
iteration 56, loss = 0.002111806534230709
iteration 57, loss = 0.0021348465234041214
iteration 58, loss = 0.0022175624035298824
iteration 59, loss = 0.0012268424034118652
iteration 60, loss = 0.0017994651570916176
iteration 61, loss = 0.0017560679698362947
iteration 62, loss = 0.0018598169554024935
iteration 63, loss = 0.002125241095200181
iteration 64, loss = 0.0021081012673676014
iteration 65, loss = 0.001774592325091362
iteration 66, loss = 0.001987389288842678
iteration 67, loss = 0.0012784653808921576
iteration 68, loss = 0.004613786935806274
iteration 69, loss = 0.0015666717663407326
iteration 70, loss = 0.0011634156107902527
iteration 71, loss = 0.00438148295506835
iteration 72, loss = 0.0023470078594982624
iteration 73, loss = 0.001670038909651339
iteration 74, loss = 0.002706591971218586
iteration 75, loss = 0.0017791439313441515
iteration 76, loss = 0.00205411110073328
iteration 77, loss = 0.004400602076202631
iteration 78, loss = 0.002031015232205391
iteration 79, loss = 0.001408752053976059
iteration 80, loss = 0.0018648516852408648
iteration 81, loss = 0.0018511012895032763
iteration 82, loss = 0.0029663746245205402
iteration 83, loss = 0.002069411100819707
iteration 84, loss = 0.001388956792652607
iteration 85, loss = 0.001265259925276041
iteration 86, loss = 0.001766121364198625
iteration 87, loss = 0.001360411522909999
iteration 88, loss = 0.002107025124132633
iteration 89, loss = 0.0016360594891011715
iteration 90, loss = 0.001920835580676794
iteration 91, loss = 0.0009672206360846758
iteration 92, loss = 0.0017566867172718048
iteration 93, loss = 0.0022128650452941656
iteration 94, loss = 0.0026090512983500957
iteration 95, loss = 0.0021071434020996094
iteration 96, loss = 0.0014858364593237638
iteration 97, loss = 0.002130453009158373
iteration 98, loss = 0.0016882591880857944
iteration 99, loss = 0.00406030984595418
iteration 100, loss = 0.0017867237329483032
iteration 101, loss = 0.002069479553028941
iteration 102, loss = 0.002150782849639654
iteration 103, loss = 0.003203329164534807
iteration 104, loss = 0.0014043661067262292
iteration 105, loss = 0.0018804932478815317
iteration 106, loss = 0.001754786353558302
iteration 107, loss = 0.0012883227318525314
iteration 108, loss = 0.0020373633597046137
iteration 109, loss = 0.0019026795634999871
iteration 110, loss = 0.0019628340378403664
iteration 111, loss = 0.002347624395042658
iteration 112, loss = 0.0018167481757700443
iteration 113, loss = 0.00422918563708663
iteration 114, loss = 0.0014219871954992414
iteration 115, loss = 0.0018881631549447775
iteration 116, loss = 0.0012503823963925242
iteration 117, loss = 0.002731418004259467
iteration 118, loss = 0.0013590594753623009
iteration 119, loss = 0.0037788443733006716
iteration 120, loss = 0.001556894276291132
iteration 121, loss = 0.002379977609962225
iteration 122, loss = 0.0016998141072690487
iteration 123, loss = 0.002038688398897648
iteration 124, loss = 0.0018606733065098524
iteration 125, loss = 0.0019255110528320074
iteration 126, loss = 0.0017659175209701061
iteration 127, loss = 0.001985289389267564
iteration 128, loss = 0.002417915966361761
iteration 129, loss = 0.002201111987233162
iteration 130, loss = 0.003295350354164839
iteration 131, loss = 0.0011096043745055795
iteration 132, loss = 0.002067386172711849
iteration 133, loss = 0.001808470580726862
iteration 134, loss = 0.0012285448610782623
iteration 135, loss = 0.001788845518603921
iteration 136, loss = 0.0027291157748550177
iteration 137, loss = 0.0043894569389522076
iteration 138, loss = 0.002024704124778509
iteration 139, loss = 0.0014147768961265683
iteration 140, loss = 0.0016111497534438968
iteration 141, loss = 0.002644590800628066
iteration 142, loss = 0.00201298575848341
iteration 143, loss = 0.006354578770697117
iteration 144, loss = 0.0033088955096900463
iteration 145, loss = 0.0019339765422046185
iteration 146, loss = 0.0024642052594572306
iteration 147, loss = 0.001424375339411199
iteration 148, loss = 0.0045098005793988705
iteration 149, loss = 0.002908474998548627
iteration 150, loss = 0.002039629500359297
iteration 151, loss = 0.004013906233012676
iteration 152, loss = 0.00175359973218292
iteration 153, loss = 0.0035835877060890198
iteration 154, loss = 0.003490958595648408
iteration 155, loss = 0.0024304655380547047
iteration 156, loss = 0.0030217128805816174
iteration 157, loss = 0.0018150322139263153
iteration 158, loss = 0.0017678108997642994
iteration 159, loss = 0.0014489730820059776
iteration 160, loss = 0.0011509222676977515
iteration 161, loss = 0.0017063252162188292
iteration 162, loss = 0.0026042498648166656
iteration 163, loss = 0.001975852996110916
iteration 164, loss = 0.0023694843985140324
iteration 165, loss = 0.0026173170190304518
iteration 166, loss = 0.0014454495394602418
iteration 167, loss = 0.0013410410610958934
iteration 168, loss = 0.0027083572931587696
iteration 169, loss = 0.002141662174835801
iteration 170, loss = 0.002085525542497635
iteration 171, loss = 0.0016644434072077274
iteration 172, loss = 0.0019775074906647205
iteration 173, loss = 0.0030720869544893503
iteration 174, loss = 0.004231743980199099
iteration 175, loss = 0.0021932737436145544
iteration 176, loss = 0.004030751995742321
iteration 177, loss = 0.004205002915114164
iteration 178, loss = 0.0011748416582122445
iteration 179, loss = 0.0023460551165044308
iteration 180, loss = 0.0012590008554980159
iteration 181, loss = 0.0017002786044031382
iteration 182, loss = 0.0017604113090783358
iteration 183, loss = 0.001515080570243299
iteration 184, loss = 0.002065069740638137
iteration 185, loss = 0.001499712816439569
iteration 186, loss = 0.0026246458292007446
iteration 187, loss = 0.0012004042509943247
iteration 188, loss = 0.0014655914856120944
iteration 189, loss = 0.0015852893702685833
iteration 190, loss = 0.0015097636496648192
iteration 191, loss = 0.003321452299132943
iteration 192, loss = 0.0011199270375072956
iteration 193, loss = 0.002433950314298272
iteration 194, loss = 0.002045189030468464
iteration 195, loss = 0.0024757678620517254
iteration 196, loss = 0.0018903427990153432
iteration 197, loss = 0.0016841588076204062
iteration 198, loss = 0.0019532775040715933
iteration 199, loss = 0.0021109238732606173
iteration 200, loss = 0.001423948910087347
iteration 201, loss = 0.0018239652272313833
iteration 202, loss = 0.000931508606299758
iteration 203, loss = 0.002598088700324297
iteration 204, loss = 0.002270120196044445
iteration 205, loss = 0.0014377485495060682
iteration 206, loss = 0.0015361795667558908
iteration 207, loss = 0.001839544391259551
iteration 208, loss = 0.0033590570092201233
iteration 209, loss = 0.0031816852279007435
iteration 210, loss = 0.001823794562369585
iteration 211, loss = 0.0029525025747716427
iteration 212, loss = 0.001327655976638198
iteration 213, loss = 0.0033291911240667105
iteration 214, loss = 0.003005180275067687
iteration 215, loss = 0.002053217962384224
iteration 216, loss = 0.001581295276992023
iteration 217, loss = 0.004002485889941454
iteration 218, loss = 0.001426076516509056
iteration 219, loss = 0.0014447071589529514
iteration 220, loss = 0.0016625897260382771
iteration 221, loss = 0.0012909742072224617
iteration 222, loss = 0.0016399126034229994
iteration 223, loss = 0.0015240038046613336
iteration 224, loss = 0.0014072395861148834
iteration 225, loss = 0.00171962333843112
iteration 226, loss = 0.0015022014267742634
iteration 227, loss = 0.0017617522971704602
iteration 228, loss = 0.0013573968317359686
iteration 229, loss = 0.001656419481150806
iteration 230, loss = 0.0016228919848799706
iteration 231, loss = 0.0020530936308205128
iteration 232, loss = 0.0013633935013785958
iteration 233, loss = 0.0024750332813709974
iteration 234, loss = 0.0016312471125274897
iteration 235, loss = 0.0030494360253214836
iteration 236, loss = 0.002414759946987033
iteration 237, loss = 0.0018388757016509771
iteration 238, loss = 0.0013586769346147776
iteration 239, loss = 0.00266597094014287
iteration 240, loss = 0.0037230902817100286
iteration 241, loss = 0.001532050664536655
iteration 242, loss = 0.0016840710304677486
iteration 243, loss = 0.00257882266305387
iteration 244, loss = 0.0012723472900688648
iteration 245, loss = 0.0014016189379617572
iteration 246, loss = 0.001830244087614119
iteration 247, loss = 0.0016790721565485
iteration 248, loss = 0.0017142541473731399
iteration 249, loss = 0.0014502573758363724
iteration 250, loss = 0.001318327966146171
iteration 251, loss = 0.0014212881214916706
iteration 252, loss = 0.0013786483323201537
iteration 253, loss = 0.002040768740698695
iteration 254, loss = 0.004850044380873442
iteration 255, loss = 0.0032702742610126734
iteration 256, loss = 0.0019248437602072954
iteration 257, loss = 0.0011974029475823045
iteration 258, loss = 0.0023594580125063658
iteration 259, loss = 0.0014407315757125616
iteration 260, loss = 0.0015523906331509352
iteration 261, loss = 0.0020705005154013634
iteration 262, loss = 0.0013704757438972592
iteration 263, loss = 0.0015566025394946337
iteration 264, loss = 0.002009777585044503
iteration 265, loss = 0.0015328647568821907
iteration 266, loss = 0.00177043117582798
iteration 267, loss = 0.003250943962484598
iteration 268, loss = 0.002085002837702632
iteration 269, loss = 0.0014817336341366172
iteration 270, loss = 0.0012698669452220201
iteration 271, loss = 0.0011951932683587074
iteration 272, loss = 0.002347764326259494
iteration 273, loss = 0.0030404936987906694
iteration 274, loss = 0.002305953297764063
iteration 275, loss = 0.00117583351675421
iteration 276, loss = 0.0018184843938797712
iteration 277, loss = 0.0021736875642091036
iteration 278, loss = 0.003250730223953724
iteration 279, loss = 0.0016186136053875089
iteration 280, loss = 0.0012569710379466414
iteration 281, loss = 0.0012412770884111524
iteration 282, loss = 0.0028029021341353655
iteration 283, loss = 0.0018755922792479396
iteration 284, loss = 0.0019593248143792152
iteration 285, loss = 0.001467821653932333
iteration 286, loss = 0.0015771291218698025
iteration 287, loss = 0.0014476205687969923
iteration 288, loss = 0.0020433643367141485
iteration 289, loss = 0.0015783112030476332
iteration 290, loss = 0.0022592090535908937
iteration 291, loss = 0.0012484537437558174
iteration 292, loss = 0.0017440755618736148
iteration 293, loss = 0.0013268089387565851
iteration 294, loss = 0.0014876974746584892
iteration 295, loss = 0.0019416785798966885
iteration 296, loss = 0.0015711999731138349
iteration 297, loss = 0.0013303181622177362
iteration 298, loss = 0.0015624857041984797
iteration 299, loss = 0.002257541287690401
iteration 0, loss = 0.0011433132458478212
iteration 1, loss = 0.0016929763369262218
iteration 2, loss = 0.0011170674115419388
iteration 3, loss = 0.004375683609396219
iteration 4, loss = 0.0012903222814202309
iteration 5, loss = 0.0023403975646942854
iteration 6, loss = 0.0016749114729464054
iteration 7, loss = 0.0012673562159761786
iteration 8, loss = 0.0015736273489892483
iteration 9, loss = 0.0014208811335265636
iteration 10, loss = 0.0014117207610979676
iteration 11, loss = 0.0014656573766842484
iteration 12, loss = 0.0014767695683985949
iteration 13, loss = 0.0030975386034697294
iteration 14, loss = 0.00131853180937469
iteration 15, loss = 0.0018024785676971078
iteration 16, loss = 0.0023784483782947063
iteration 17, loss = 0.0014524502912536263
iteration 18, loss = 0.0015503766480833292
iteration 19, loss = 0.0016699288971722126
iteration 20, loss = 0.005439563654363155
iteration 21, loss = 0.001331619918346405
iteration 22, loss = 0.004634968936443329
iteration 23, loss = 0.0013901774073019624
iteration 24, loss = 0.002244055038318038
iteration 25, loss = 0.0014355297898873687
iteration 26, loss = 0.0027556743007153273
iteration 27, loss = 0.0015856651589274406
iteration 28, loss = 0.0018897607224062085
iteration 29, loss = 0.0016963921952992678
iteration 30, loss = 0.001678613480180502
iteration 31, loss = 0.0011960180709138513
iteration 32, loss = 0.0020886254496872425
iteration 33, loss = 0.0013954564929008484
iteration 34, loss = 0.0019724909216165543
iteration 35, loss = 0.00244333129376173
iteration 36, loss = 0.0024039375130087137
iteration 37, loss = 0.0014576228568330407
iteration 38, loss = 0.0013716166140511632
iteration 39, loss = 0.001175284618511796
iteration 40, loss = 0.0018850222695618868
iteration 41, loss = 0.0026971912011504173
iteration 42, loss = 0.0028041768819093704
iteration 43, loss = 0.0012464170577004552
iteration 44, loss = 0.00192385190166533
iteration 45, loss = 0.0017115630907937884
iteration 46, loss = 0.0016847890801727772
iteration 47, loss = 0.0018115518614649773
iteration 48, loss = 0.004497982095927
iteration 49, loss = 0.0020299709867686033
iteration 50, loss = 0.00163922063075006
iteration 51, loss = 0.002508581383153796
iteration 52, loss = 0.001170713221654296
iteration 53, loss = 0.0017539962427690625
iteration 54, loss = 0.0021424400620162487
iteration 55, loss = 0.003179040504619479
iteration 56, loss = 0.001189699280075729
iteration 57, loss = 0.0017308165552094579
iteration 58, loss = 0.004112278576940298
iteration 59, loss = 0.0016291973879560828
iteration 60, loss = 0.0017630800139158964
iteration 61, loss = 0.0019955511670559645
iteration 62, loss = 0.0011695708381012082
iteration 63, loss = 0.003219243139028549
iteration 64, loss = 0.0013747350312769413
iteration 65, loss = 0.0041603995487093925
iteration 66, loss = 0.0019062928622588515
iteration 67, loss = 0.0020490156020969152
iteration 68, loss = 0.001248726504854858
iteration 69, loss = 0.0022017904557287693
iteration 70, loss = 0.0011820687213912606
iteration 71, loss = 0.0009735068306326866
iteration 72, loss = 0.0011496408842504025
iteration 73, loss = 0.0020454057957977057
iteration 74, loss = 0.002395533723756671
iteration 75, loss = 0.002406085841357708
iteration 76, loss = 0.0014572037616744637
iteration 77, loss = 0.0014680176973342896
iteration 78, loss = 0.0013454128056764603
iteration 79, loss = 0.0026186967734247446
iteration 80, loss = 0.0021488836500793695
iteration 81, loss = 0.001855323207564652
iteration 82, loss = 0.002181573072448373
iteration 83, loss = 0.002635709708556533
iteration 84, loss = 0.00396315660327673
iteration 85, loss = 0.0012150559341534972
iteration 86, loss = 0.0014682693872600794
iteration 87, loss = 0.004117096774280071
iteration 88, loss = 0.001581339631229639
iteration 89, loss = 0.0024647756945341825
iteration 90, loss = 0.0017313184216618538
iteration 91, loss = 0.0013814825797453523
iteration 92, loss = 0.0016462054336443543
iteration 93, loss = 0.0015030804788693786
iteration 94, loss = 0.0014840333024039865
iteration 95, loss = 0.0012686413247138262
iteration 96, loss = 0.003866243874654174
iteration 97, loss = 0.0014196927659213543
iteration 98, loss = 0.002895626937970519
iteration 99, loss = 0.0012646839022636414
iteration 100, loss = 0.0022156969644129276
iteration 101, loss = 0.0011854490730911493
iteration 102, loss = 0.0016258845571428537
iteration 103, loss = 0.0013966287951916456
iteration 104, loss = 0.001548261265270412
iteration 105, loss = 0.002067423425614834
iteration 106, loss = 0.0026896626222878695
iteration 107, loss = 0.0017525358125567436
iteration 108, loss = 0.0011411119485273957
iteration 109, loss = 0.0014771667774766684
iteration 110, loss = 0.0013604044215753675
iteration 111, loss = 0.0021624218206852674
iteration 112, loss = 0.0015808208845555782
iteration 113, loss = 0.0015240623615682125
iteration 114, loss = 0.0019661756232380867
iteration 115, loss = 0.001573210465721786
iteration 116, loss = 0.0017717578448355198
iteration 117, loss = 0.002753619570285082
iteration 118, loss = 0.0023868249263614416
iteration 119, loss = 0.0020795923192054033
iteration 120, loss = 0.0011520374100655317
iteration 121, loss = 0.0018630143022164702
iteration 122, loss = 0.0019011072581633925
iteration 123, loss = 0.0015526367351412773
iteration 124, loss = 0.0045046573504805565
iteration 125, loss = 0.0011967318132519722
iteration 126, loss = 0.0017598140984773636
iteration 127, loss = 0.0017454162007197738
iteration 128, loss = 0.001219085301272571
iteration 129, loss = 0.0013731563230976462
iteration 130, loss = 0.0011001329403370619
iteration 131, loss = 0.0013223863206803799
iteration 132, loss = 0.002263637725263834
iteration 133, loss = 0.0014501120895147324
iteration 134, loss = 0.0012621323112398386
iteration 135, loss = 0.0010229174513369799
iteration 136, loss = 0.001292363042011857
iteration 137, loss = 0.0019576435443013906
iteration 138, loss = 0.0016577474307268858
iteration 139, loss = 0.002386133186519146
iteration 140, loss = 0.004172614309936762
iteration 141, loss = 0.0013199124950915575
iteration 142, loss = 0.0028589332941919565
iteration 143, loss = 0.002107862615957856
iteration 144, loss = 0.0017971292836591601
iteration 145, loss = 0.0013530784053727984
iteration 146, loss = 0.0011083742137998343
iteration 147, loss = 0.004225783981382847
iteration 148, loss = 0.004026901908218861
iteration 149, loss = 0.0009986090008169413
iteration 150, loss = 0.001551662920974195
iteration 151, loss = 0.001659243949688971
iteration 152, loss = 0.001787135610356927
iteration 153, loss = 0.001841099583543837
iteration 154, loss = 0.0016872313572093844
iteration 155, loss = 0.002027614274993539
iteration 156, loss = 0.003613989567384124
iteration 157, loss = 0.0025018416345119476
iteration 158, loss = 0.0013562253443524241
iteration 159, loss = 0.0014969565672799945
iteration 160, loss = 0.0014963827561587095
iteration 161, loss = 0.003016324946656823
iteration 162, loss = 0.001269074040465057
iteration 163, loss = 0.0015059825964272022
iteration 164, loss = 0.001743818400427699
iteration 165, loss = 0.0023668459616601467
iteration 166, loss = 0.0015903348103165627
iteration 167, loss = 0.0013940359931439161
iteration 168, loss = 0.0014445844572037458
iteration 169, loss = 0.0017163082957267761
iteration 170, loss = 0.0016048674006015062
iteration 171, loss = 0.0012066531926393509
iteration 172, loss = 0.0016739380080252886
iteration 173, loss = 0.0016885933000594378
iteration 174, loss = 0.0010053699370473623
iteration 175, loss = 0.0010944625828415155
iteration 176, loss = 0.0028592594899237156
iteration 177, loss = 0.00128454202786088
iteration 178, loss = 0.002114859875291586
iteration 179, loss = 0.0008464281563647091
iteration 180, loss = 0.0016839103773236275
iteration 181, loss = 0.0016473374562337995
iteration 182, loss = 0.0019310829229652882
iteration 183, loss = 0.0016211284091696143
iteration 184, loss = 0.0042161536403000355
iteration 185, loss = 0.0018364423885941505
iteration 186, loss = 0.001712651108391583
iteration 187, loss = 0.0014570908388122916
iteration 188, loss = 0.004710224457085133
iteration 189, loss = 0.0012833395740017295
iteration 190, loss = 0.0036147318314760923
iteration 191, loss = 0.001808130880817771
iteration 192, loss = 0.0015541036846116185
iteration 193, loss = 0.0026246998459100723
iteration 194, loss = 0.0024131410755217075
iteration 195, loss = 0.002489222679287195
iteration 196, loss = 0.003240999300032854
iteration 197, loss = 0.0021546019706875086
iteration 198, loss = 0.0022565904073417187
iteration 199, loss = 0.0015912834787741303
iteration 200, loss = 0.0017429394647479057
iteration 201, loss = 0.0011675970163196325
iteration 202, loss = 0.002583073452115059
iteration 203, loss = 0.0011119674891233444
iteration 204, loss = 0.003170096781104803
iteration 205, loss = 0.001214927644468844
iteration 206, loss = 0.0015146508812904358
iteration 207, loss = 0.0018516040872782469
iteration 208, loss = 0.0012096223654225469
iteration 209, loss = 0.0011995140230283141
iteration 210, loss = 0.0011805450776591897
iteration 211, loss = 0.0014938744716346264
iteration 212, loss = 0.002756555564701557
iteration 213, loss = 0.0014134335797280073
iteration 214, loss = 0.003973965998739004
iteration 215, loss = 0.002356695244088769
iteration 216, loss = 0.0015721136005595326
iteration 217, loss = 0.0033310032449662685
iteration 218, loss = 0.001342211733572185
iteration 219, loss = 0.0011910323519259691
iteration 220, loss = 0.002231618855148554
iteration 221, loss = 0.0013167092110961676
iteration 222, loss = 0.0014390593860298395
iteration 223, loss = 0.0010487380204722285
iteration 224, loss = 0.001530607114546001
iteration 225, loss = 0.0015739938244223595
iteration 226, loss = 0.002183101372793317
iteration 227, loss = 0.0012272573076188564
iteration 228, loss = 0.0011002066312357783
iteration 229, loss = 0.0015520169399678707
iteration 230, loss = 0.004222068935632706
iteration 231, loss = 0.0017356056487187743
iteration 232, loss = 0.0026804665103554726
iteration 233, loss = 0.0012731646420434117
iteration 234, loss = 0.001904177013784647
iteration 235, loss = 0.0032982875127345324
iteration 236, loss = 0.0013924690429121256
iteration 237, loss = 0.002290551085025072
iteration 238, loss = 0.0024923619348555803
iteration 239, loss = 0.002271400298923254
iteration 240, loss = 0.0023131826892495155
iteration 241, loss = 0.0013007272500544786
iteration 242, loss = 0.003369635436683893
iteration 243, loss = 0.001508493209257722
iteration 244, loss = 0.00223759887740016
iteration 245, loss = 0.0015079003060236573
iteration 246, loss = 0.0017773911822587252
iteration 247, loss = 0.0016193188494071364
iteration 248, loss = 0.001091153360903263
iteration 249, loss = 0.0015523771289736032
iteration 250, loss = 0.0012883124873042107
iteration 251, loss = 0.0014287298545241356
iteration 252, loss = 0.0013810595264658332
iteration 253, loss = 0.001224635518155992
iteration 254, loss = 0.0013126489939168096
iteration 255, loss = 0.0012476646807044744
iteration 256, loss = 0.0022404100745916367
iteration 257, loss = 0.0012778766686096787
iteration 258, loss = 0.002298161154612899
iteration 259, loss = 0.0025613794568926096
iteration 260, loss = 0.001744199194945395
iteration 261, loss = 0.0013998495414853096
iteration 262, loss = 0.000979384407401085
iteration 263, loss = 0.0010181376710534096
iteration 264, loss = 0.0009034847025759518
iteration 265, loss = 0.00244839396327734
iteration 266, loss = 0.0014256910653784871
iteration 267, loss = 0.0031918876338750124
iteration 268, loss = 0.0015075206756591797
iteration 269, loss = 0.0011932841734960675
iteration 270, loss = 0.0014904072741046548
iteration 271, loss = 0.0017770664999261498
iteration 272, loss = 0.0014040753012523055
iteration 273, loss = 0.0015989820240065455
iteration 274, loss = 0.0015097996219992638
iteration 275, loss = 0.0010886244708672166
iteration 276, loss = 0.0012463878374546766
iteration 277, loss = 0.002064767759293318
iteration 278, loss = 0.0014137695543467999
iteration 279, loss = 0.0025677047669887543
iteration 280, loss = 0.0018557708244770765
iteration 281, loss = 0.0023446292616426945
iteration 282, loss = 0.0015371888875961304
iteration 283, loss = 0.0011898360680788755
iteration 284, loss = 0.0017803660593926907
iteration 285, loss = 0.001646458520554006
iteration 286, loss = 0.0011497854720801115
iteration 287, loss = 0.0012721300590783358
iteration 288, loss = 0.002350306138396263
iteration 289, loss = 0.0009869212517514825
iteration 290, loss = 0.002069820649921894
iteration 291, loss = 0.0010345899499952793
iteration 292, loss = 0.0014880166854709387
iteration 293, loss = 0.0023712972179055214
iteration 294, loss = 0.0018189173424616456
iteration 295, loss = 0.0015424826415255666
iteration 296, loss = 0.003440846921876073
iteration 297, loss = 0.0016146220732480288
iteration 298, loss = 0.0010554865002632141
iteration 299, loss = 0.0016907029785215855
iteration 0, loss = 0.0013695517554879189
iteration 1, loss = 0.0011765462113544345
iteration 2, loss = 0.004553178790956736
iteration 3, loss = 0.0016017158050090075
iteration 4, loss = 0.0040666488930583
iteration 5, loss = 0.0013687339378520846
iteration 6, loss = 0.0016209320165216923
iteration 7, loss = 0.002316382247954607
iteration 8, loss = 0.0012861012946814299
iteration 9, loss = 0.000954626128077507
iteration 10, loss = 0.001103888382203877
iteration 11, loss = 0.0018510149093344808
iteration 12, loss = 0.0012289906153455377
iteration 13, loss = 0.0018904284806922078
iteration 14, loss = 0.001435149577446282
iteration 15, loss = 0.00224098889157176
iteration 16, loss = 0.0027693049050867558
iteration 17, loss = 0.0011834407923743129
iteration 18, loss = 0.0027787378057837486
iteration 19, loss = 0.0018206487875431776
iteration 20, loss = 0.0011742390925064683
iteration 21, loss = 0.0033172364346683025
iteration 22, loss = 0.001097854576073587
iteration 23, loss = 0.0012421272695064545
iteration 24, loss = 0.0014884502161294222
iteration 25, loss = 0.0016290704952552915
iteration 26, loss = 0.0033729884307831526
iteration 27, loss = 0.0013086352264508605
iteration 28, loss = 0.0011624378385022283
iteration 29, loss = 0.001513616181910038
iteration 30, loss = 0.0010864477371796966
iteration 31, loss = 0.0012300647795200348
iteration 32, loss = 0.001266520586796105
iteration 33, loss = 0.0013032901333644986
iteration 34, loss = 0.0011941533302888274
iteration 35, loss = 0.0016370356315746903
iteration 36, loss = 0.0013909799745306373
iteration 37, loss = 0.0011561064748093486
iteration 38, loss = 0.0021459104027599096
iteration 39, loss = 0.0018175638979300857
iteration 40, loss = 0.0013603927800431848
iteration 41, loss = 0.0016837926814332604
iteration 42, loss = 0.0012582980562001467
iteration 43, loss = 0.0014938374515622854
iteration 44, loss = 0.0013291898649185896
iteration 45, loss = 0.0014985492452979088
iteration 46, loss = 0.0017773767467588186
iteration 47, loss = 0.0017098013777285814
iteration 48, loss = 0.0021709613502025604
iteration 49, loss = 0.003562459722161293
iteration 50, loss = 0.0015016146935522556
iteration 51, loss = 0.0015500555746257305
iteration 52, loss = 0.0010698362020775676
iteration 53, loss = 0.001076323795132339
iteration 54, loss = 0.0011411013547331095
iteration 55, loss = 0.0012447639601305127
iteration 56, loss = 0.0012199219781905413
iteration 57, loss = 0.0014403827954083681
iteration 58, loss = 0.0017892156029120088
iteration 59, loss = 0.002127829473465681
iteration 60, loss = 0.0021460135467350483
iteration 61, loss = 0.0018803698476403952
iteration 62, loss = 0.0011187359923496842
iteration 63, loss = 0.001879165880382061
iteration 64, loss = 0.001229666406288743
iteration 65, loss = 0.0025520846247673035
iteration 66, loss = 0.0013630654430016875
iteration 67, loss = 0.001415348844602704
iteration 68, loss = 0.0009646853432059288
iteration 69, loss = 0.0015595480799674988
iteration 70, loss = 0.0018603280186653137
iteration 71, loss = 0.0013356966665014625
iteration 72, loss = 0.0016145339468494058
iteration 73, loss = 0.002372520975768566
iteration 74, loss = 0.0012008256744593382
iteration 75, loss = 0.0014671834651380777
iteration 76, loss = 0.0019119537901133299
iteration 77, loss = 0.0019319291459396482
iteration 78, loss = 0.0009491875534877181
iteration 79, loss = 0.003239481942728162
iteration 80, loss = 0.0018136214930564165
iteration 81, loss = 0.0011788628762587905
iteration 82, loss = 0.0017965917941182852
iteration 83, loss = 0.0019598836079239845
iteration 84, loss = 0.0014713102718815207
iteration 85, loss = 0.0009261554223485291
iteration 86, loss = 0.0010207422310486436
iteration 87, loss = 0.0017386599211022258
iteration 88, loss = 0.0011017211945727468
iteration 89, loss = 0.0015520269516855478
iteration 90, loss = 0.0018261382356286049
iteration 91, loss = 0.0028243716806173325
iteration 92, loss = 0.001683981972746551
iteration 93, loss = 0.0027016757521778345
iteration 94, loss = 0.00104687106795609
iteration 95, loss = 0.002770192688331008
iteration 96, loss = 0.0013429165119305253
iteration 97, loss = 0.0014919975074008107
iteration 98, loss = 0.002322954125702381
iteration 99, loss = 0.0014903915580362082
iteration 100, loss = 0.003984013572335243
iteration 101, loss = 0.0021024916786700487
iteration 102, loss = 0.001361048431135714
iteration 103, loss = 0.0013681260170415044
iteration 104, loss = 0.0016044439980760217
iteration 105, loss = 0.001042171847075224
iteration 106, loss = 0.0009676914196461439
iteration 107, loss = 0.0012973009143024683
iteration 108, loss = 0.0015823573339730501
iteration 109, loss = 0.0017711299005895853
iteration 110, loss = 0.0013223302084952593
iteration 111, loss = 0.0012175480369478464
iteration 112, loss = 0.002215347019955516
iteration 113, loss = 0.0035607218742370605
iteration 114, loss = 0.0017723554046824574
iteration 115, loss = 0.0011487057199701667
iteration 116, loss = 0.0013112180167809129
iteration 117, loss = 0.0035540261305868626
iteration 118, loss = 0.002227462362498045
iteration 119, loss = 0.0027497224509716034
iteration 120, loss = 0.0016106284456327558
iteration 121, loss = 0.0018451061332598329
iteration 122, loss = 0.0010537159396335483
iteration 123, loss = 0.0019078935729339719
iteration 124, loss = 0.0017048018053174019
iteration 125, loss = 0.0012409031623974442
iteration 126, loss = 0.001980487024411559
iteration 127, loss = 0.0024333533365279436
iteration 128, loss = 0.0013725751778110862
iteration 129, loss = 0.0014612965751439333
iteration 130, loss = 0.002198603469878435
iteration 131, loss = 0.001397131010890007
iteration 132, loss = 0.001458355924114585
iteration 133, loss = 0.0012364255962893367
iteration 134, loss = 0.0015838409308344126
iteration 135, loss = 0.0017586014000698924
iteration 136, loss = 0.0011095835361629725
iteration 137, loss = 0.001634454121813178
iteration 138, loss = 0.0012475011171773076
iteration 139, loss = 0.0010569950100034475
iteration 140, loss = 0.001488152309320867
iteration 141, loss = 0.0014536569360643625
iteration 142, loss = 0.0009429570054635406
iteration 143, loss = 0.0016581425443291664
iteration 144, loss = 0.0019148419378325343
iteration 145, loss = 0.0014183747116476297
iteration 146, loss = 0.002671662252396345
iteration 147, loss = 0.0012757197255268693
iteration 148, loss = 0.001638665096834302
iteration 149, loss = 0.0013247811002656817
iteration 150, loss = 0.002405444160103798
iteration 151, loss = 0.0012565790675580502
iteration 152, loss = 0.0016612883191555738
iteration 153, loss = 0.001595901558175683
iteration 154, loss = 0.0014428823487833142
iteration 155, loss = 0.004482756834477186
iteration 156, loss = 0.0014067869633436203
iteration 157, loss = 0.0021794396452605724
iteration 158, loss = 0.00168874382507056
iteration 159, loss = 0.0028127185069024563
iteration 160, loss = 0.0014700083993375301
iteration 161, loss = 0.0013981354422867298
iteration 162, loss = 0.003557708580046892
iteration 163, loss = 0.001692724647000432
iteration 164, loss = 0.001238599419593811
iteration 165, loss = 0.0014179461868479848
iteration 166, loss = 0.0017771599814295769
iteration 167, loss = 0.003236213233321905
iteration 168, loss = 0.0028502545319497585
iteration 169, loss = 0.0010014029685407877
iteration 170, loss = 0.001871903776191175
iteration 171, loss = 0.0014521933626383543
iteration 172, loss = 0.0008889773744158447
iteration 173, loss = 0.0024050234351307154
iteration 174, loss = 0.0014018206857144833
iteration 175, loss = 0.0012672723969444633
iteration 176, loss = 0.0022685397416353226
iteration 177, loss = 0.001182250096462667
iteration 178, loss = 0.0013117266353219748
iteration 179, loss = 0.0019121317891404033
iteration 180, loss = 0.001270829699933529
iteration 181, loss = 0.0011705965735018253
iteration 182, loss = 0.0024920497089624405
iteration 183, loss = 0.0014443143736571074
iteration 184, loss = 0.0010053659789264202
iteration 185, loss = 0.003681051777675748
iteration 186, loss = 0.0023377505131065845
iteration 187, loss = 0.0018237419426441193
iteration 188, loss = 0.0012197294272482395
iteration 189, loss = 0.0010881880298256874
iteration 190, loss = 0.001473352313041687
iteration 191, loss = 0.0014072103658691049
iteration 192, loss = 0.001359787886030972
iteration 193, loss = 0.0015465280739590526
iteration 194, loss = 0.0016784624895080924
iteration 195, loss = 0.0018979223677888513
iteration 196, loss = 0.0023979623802006245
iteration 197, loss = 0.0034049139358103275
iteration 198, loss = 0.001594955800101161
iteration 199, loss = 0.001391128869727254
iteration 200, loss = 0.0009304966079071164
iteration 201, loss = 0.002970354398712516
iteration 202, loss = 0.0016024888027459383
iteration 203, loss = 0.0026850965805351734
iteration 204, loss = 0.0016953281592577696
iteration 205, loss = 0.0016760085709393024
iteration 206, loss = 0.001478382502682507
iteration 207, loss = 0.0010374868288636208
iteration 208, loss = 0.0015155235305428505
iteration 209, loss = 0.0013712523505091667
iteration 210, loss = 0.0014127688482403755
iteration 211, loss = 0.0014341198839247227
iteration 212, loss = 0.0012600559275597334
iteration 213, loss = 0.0017664299812167883
iteration 214, loss = 0.0018453040393069386
iteration 215, loss = 0.003169821575284004
iteration 216, loss = 0.0018040263094007969
iteration 217, loss = 0.0017122466815635562
iteration 218, loss = 0.0015255617909133434
iteration 219, loss = 0.0009757191874086857
iteration 220, loss = 0.004279419779777527
iteration 221, loss = 0.0009981406619772315
iteration 222, loss = 0.0019741379655897617
iteration 223, loss = 0.0016805995255708694
iteration 224, loss = 0.001078558387234807
iteration 225, loss = 0.0013595090713351965
iteration 226, loss = 0.0010293209925293922
iteration 227, loss = 0.0008333219448104501
iteration 228, loss = 0.0011886274442076683
iteration 229, loss = 0.0019016492879018188
iteration 230, loss = 0.0008978776168078184
iteration 231, loss = 0.001176348072476685
iteration 232, loss = 0.0014508499298244715
iteration 233, loss = 0.002258164808154106
iteration 234, loss = 0.0008802199736237526
iteration 235, loss = 0.002119682729244232
iteration 236, loss = 0.002869728021323681
iteration 237, loss = 0.0022618859075009823
iteration 238, loss = 0.001260527060367167
iteration 239, loss = 0.0008929665200412273
iteration 240, loss = 0.0013977965572848916
iteration 241, loss = 0.001560189644806087
iteration 242, loss = 0.001475874101743102
iteration 243, loss = 0.003490959294140339
iteration 244, loss = 0.0020527339074760675
iteration 245, loss = 0.0010951985605061054
iteration 246, loss = 0.001434242818504572
iteration 247, loss = 0.00120606052223593
iteration 248, loss = 0.0015062888851389289
iteration 249, loss = 0.0013369749067351222
iteration 250, loss = 0.0023474448826164007
iteration 251, loss = 0.0016667975578457117
iteration 252, loss = 0.0008836133638396859
iteration 253, loss = 0.001018747454509139
iteration 254, loss = 0.0012521259486675262
iteration 255, loss = 0.0012577144661918283
iteration 256, loss = 0.003543698461726308
iteration 257, loss = 0.0012680499348789454
iteration 258, loss = 0.0011156908003613353
iteration 259, loss = 0.001132895820774138
iteration 260, loss = 0.0011707799276337028
iteration 261, loss = 0.0019266983726993203
iteration 262, loss = 0.0011939987307414412
iteration 263, loss = 0.0016922325594350696
iteration 264, loss = 0.001738417660817504
iteration 265, loss = 0.0031588564161211252
iteration 266, loss = 0.0014643162721768022
iteration 267, loss = 0.0011047300649806857
iteration 268, loss = 0.002117183292284608
iteration 269, loss = 0.0015226368559524417
iteration 270, loss = 0.0020980078261345625
iteration 271, loss = 0.0012214549351483583
iteration 272, loss = 0.001931997248902917
iteration 273, loss = 0.0016566298436373472
iteration 274, loss = 0.0011294778669252992
iteration 275, loss = 0.0012101288884878159
iteration 276, loss = 0.0015035534743219614
iteration 277, loss = 0.001085798372514546
iteration 278, loss = 0.0017324945656582713
iteration 279, loss = 0.0016055881278589368
iteration 280, loss = 0.0016401692992076278
iteration 281, loss = 0.0022636307403445244
iteration 282, loss = 0.0012799646938219666
iteration 283, loss = 0.0013687179889529943
iteration 284, loss = 0.001490861875936389
iteration 285, loss = 0.0038290228694677353
iteration 286, loss = 0.0019282500725239515
iteration 287, loss = 0.0009506683563813567
iteration 288, loss = 0.002054298995062709
iteration 289, loss = 0.0013249145122244954
iteration 290, loss = 0.0014300874900072813
iteration 291, loss = 0.0033790436573326588
iteration 292, loss = 0.0015529283555224538
iteration 293, loss = 0.002191666979342699
iteration 294, loss = 0.0011049533495679498
iteration 295, loss = 0.002948635956272483
iteration 296, loss = 0.0024127070792019367
iteration 297, loss = 0.0018886596662923694
iteration 298, loss = 0.0014557228423655033
iteration 299, loss = 0.0012553769629448652
iteration 0, loss = 0.0009536065626889467
iteration 1, loss = 0.0010486333630979061
iteration 2, loss = 0.0015740825328975916
iteration 3, loss = 0.0011009927839040756
iteration 4, loss = 0.0018367919838055968
iteration 5, loss = 0.0010470692068338394
iteration 6, loss = 0.0009414995438419282
iteration 7, loss = 0.0016219670651480556
iteration 8, loss = 0.0014181118458509445
iteration 9, loss = 0.0018663289956748486
iteration 10, loss = 0.0034464187920093536
iteration 11, loss = 0.0010705364402383566
iteration 12, loss = 0.0018334286287426949
iteration 13, loss = 0.00087027094559744
iteration 14, loss = 0.002332964912056923
iteration 15, loss = 0.001096680760383606
iteration 16, loss = 0.001375990454107523
iteration 17, loss = 0.0016148688737303019
iteration 18, loss = 0.0010622774716466665
iteration 19, loss = 0.0022957941982895136
iteration 20, loss = 0.0009295614436268806
iteration 21, loss = 0.0007485362584702671
iteration 22, loss = 0.002179090864956379
iteration 23, loss = 0.0016398007282987237
iteration 24, loss = 0.0013126247795298696
iteration 25, loss = 0.0031458099838346243
iteration 26, loss = 0.001065326388925314
iteration 27, loss = 0.001390146091580391
iteration 28, loss = 0.0022009732201695442
iteration 29, loss = 0.001941665424965322
iteration 30, loss = 0.0007975352928042412
iteration 31, loss = 0.002005001762881875
iteration 32, loss = 0.0015880974242463708
iteration 33, loss = 0.0016457466408610344
iteration 34, loss = 0.001123786554671824
iteration 35, loss = 0.0018819932593032718
iteration 36, loss = 0.0011883501429110765
iteration 37, loss = 0.0010735224932432175
iteration 38, loss = 0.0012471242807805538
iteration 39, loss = 0.001728296047076583
iteration 40, loss = 0.0011353552108630538
iteration 41, loss = 0.0024575195275247097
iteration 42, loss = 0.0016184295527637005
iteration 43, loss = 0.0013761060545220971
iteration 44, loss = 0.0015933950198814273
iteration 45, loss = 0.0012789213797077537
iteration 46, loss = 0.0026996631640940905
iteration 47, loss = 0.0016125913243740797
iteration 48, loss = 0.00130313856061548
iteration 49, loss = 0.002176528563722968
iteration 50, loss = 0.000938993354793638
iteration 51, loss = 0.0014327492099255323
iteration 52, loss = 0.0013070148415863514
iteration 53, loss = 0.0013881792547181249
iteration 54, loss = 0.002387381624430418
iteration 55, loss = 0.0009353499044664204
iteration 56, loss = 0.001587226870469749
iteration 57, loss = 0.0010663012508302927
iteration 58, loss = 0.0015167510136961937
iteration 59, loss = 0.0010608293814584613
iteration 60, loss = 0.0015140494797378778
iteration 61, loss = 0.001021173084154725
iteration 62, loss = 0.000867354916408658
iteration 63, loss = 0.0022278628312051296
iteration 64, loss = 0.0012990429531782866
iteration 65, loss = 0.0010757484706118703
iteration 66, loss = 0.0021002255380153656
iteration 67, loss = 0.0013858731836080551
iteration 68, loss = 0.0010373560944572091
iteration 69, loss = 0.0029212827794253826
iteration 70, loss = 0.0015757904620841146
iteration 71, loss = 0.002119080862030387
iteration 72, loss = 0.000997411785647273
iteration 73, loss = 0.003859937656670809
iteration 74, loss = 0.0008810342405922711
iteration 75, loss = 0.0013022918719798326
iteration 76, loss = 0.0017057046061381698
iteration 77, loss = 0.0010319326538592577
iteration 78, loss = 0.004332968033850193
iteration 79, loss = 0.0013324045576155186
iteration 80, loss = 0.0013941043289378285
iteration 81, loss = 0.0010390718234702945
iteration 82, loss = 0.0010700952261686325
iteration 83, loss = 0.000979532254859805
iteration 84, loss = 0.0010432761628180742
iteration 85, loss = 0.0014756935415789485
iteration 86, loss = 0.001039893482811749
iteration 87, loss = 0.002311680233106017
iteration 88, loss = 0.0010866359807550907
iteration 89, loss = 0.001154501223936677
iteration 90, loss = 0.001460877712816
iteration 91, loss = 0.0012305749114602804
iteration 92, loss = 0.0011903481790795922
iteration 93, loss = 0.0012226705439388752
iteration 94, loss = 0.0013416088186204433
iteration 95, loss = 0.002296430990099907
iteration 96, loss = 0.002605016576126218
iteration 97, loss = 0.001558335847221315
iteration 98, loss = 0.000884206616319716
iteration 99, loss = 0.0008690616814419627
iteration 100, loss = 0.0013704054290428758
iteration 101, loss = 0.0014187858905643225
iteration 102, loss = 0.0011475461069494486
iteration 103, loss = 0.0010920874774456024
iteration 104, loss = 0.001954763662070036
iteration 105, loss = 0.0014192392118275166
iteration 106, loss = 0.0010845782235264778
iteration 107, loss = 0.0019304589368402958
iteration 108, loss = 0.0011659828014671803
iteration 109, loss = 0.0014292111154645681
iteration 110, loss = 0.0011164379538968205
iteration 111, loss = 0.0008825280820019543
iteration 112, loss = 0.0014041735557839274
iteration 113, loss = 0.001753875520080328
iteration 114, loss = 0.0010784231126308441
iteration 115, loss = 0.0017739018658176064
iteration 116, loss = 0.0009974344866350293
iteration 117, loss = 0.0013027727836742997
iteration 118, loss = 0.0017654618714004755
iteration 119, loss = 0.0013797155115753412
iteration 120, loss = 0.0010004999348893762
iteration 121, loss = 0.0012589303078129888
iteration 122, loss = 0.0015484609175473452
iteration 123, loss = 0.0012083870824426413
iteration 124, loss = 0.001129580195993185
iteration 125, loss = 0.00106727983802557
iteration 126, loss = 0.0017362922662869096
iteration 127, loss = 0.0019472851417958736
iteration 128, loss = 0.0010400274768471718
iteration 129, loss = 0.0016579568618908525
iteration 130, loss = 0.0011523852590471506
iteration 131, loss = 0.0012475934345275164
iteration 132, loss = 0.0013341214507818222
iteration 133, loss = 0.0012102923355996609
iteration 134, loss = 0.0012720439117401838
iteration 135, loss = 0.0009302312973886728
iteration 136, loss = 0.0012853473890572786
iteration 137, loss = 0.0032951596658676863
iteration 138, loss = 0.0008976358221843839
iteration 139, loss = 0.0018400386907160282
iteration 140, loss = 0.002293817698955536
iteration 141, loss = 0.001188534195534885
iteration 142, loss = 0.0014501330442726612
iteration 143, loss = 0.0012747392756864429
iteration 144, loss = 0.0020131177734583616
iteration 145, loss = 0.002543471287935972
iteration 146, loss = 0.003159713000059128
iteration 147, loss = 0.0011044875718653202
iteration 148, loss = 0.0011272161500528455
iteration 149, loss = 0.001901258947327733
iteration 150, loss = 0.001256281859241426
iteration 151, loss = 0.0013870896073058248
iteration 152, loss = 0.0014512718189507723
iteration 153, loss = 0.0012479872675612569
iteration 154, loss = 0.0011782951187342405
iteration 155, loss = 0.001141245593316853
iteration 156, loss = 0.0011729467660188675
iteration 157, loss = 0.0012279730290174484
iteration 158, loss = 0.0016705755842849612
iteration 159, loss = 0.0012384022120386362
iteration 160, loss = 0.0014516289811581373
iteration 161, loss = 0.001832466688938439
iteration 162, loss = 0.0015131215332075953
iteration 163, loss = 0.0014492672635242343
iteration 164, loss = 0.0019686345476657152
iteration 165, loss = 0.003830202389508486
iteration 166, loss = 0.004611758515238762
iteration 167, loss = 0.0011583961313590407
iteration 168, loss = 0.003756927791982889
iteration 169, loss = 0.0011696554720401764
iteration 170, loss = 0.0011617618147283792
iteration 171, loss = 0.0017315089935436845
iteration 172, loss = 0.0021116086281836033
iteration 173, loss = 0.0010711868526414037
iteration 174, loss = 0.0020264696795493364
iteration 175, loss = 0.0009351606713607907
iteration 176, loss = 0.0015521084424108267
iteration 177, loss = 0.0014484691200777888
iteration 178, loss = 0.0018959705485031009
iteration 179, loss = 0.0008739710319787264
iteration 180, loss = 0.002355750882998109
iteration 181, loss = 0.0009650505380704999
iteration 182, loss = 0.00320709147490561
iteration 183, loss = 0.001499002450145781
iteration 184, loss = 0.0009920325828716159
iteration 185, loss = 0.001377444714307785
iteration 186, loss = 0.0015923714963719249
iteration 187, loss = 0.0010780039010569453
iteration 188, loss = 0.0013626879081130028
iteration 189, loss = 0.0018015778623521328
iteration 190, loss = 0.0009466322371736169
iteration 191, loss = 0.001068420009687543
iteration 192, loss = 0.001180299324914813
iteration 193, loss = 0.0007968484424054623
iteration 194, loss = 0.0010897311149165034
iteration 195, loss = 0.0012664298992604017
iteration 196, loss = 0.003789374604821205
iteration 197, loss = 0.0021683108061552048
iteration 198, loss = 0.0032737310975790024
iteration 199, loss = 0.0016309203347191215
iteration 200, loss = 0.0014582898002117872
iteration 201, loss = 0.0014507098821923137
iteration 202, loss = 0.0015205912059172988
iteration 203, loss = 0.0014207171043381095
iteration 204, loss = 0.003243859391659498
iteration 205, loss = 0.0015124590136110783
iteration 206, loss = 0.0015724236145615578
iteration 207, loss = 0.0011397323105484247
iteration 208, loss = 0.002135132672265172
iteration 209, loss = 0.000846104696393013
iteration 210, loss = 0.0008882741676643491
iteration 211, loss = 0.0007654607761651278
iteration 212, loss = 0.0012836409732699394
iteration 213, loss = 0.0012054244289174676
iteration 214, loss = 0.0009443192393518984
iteration 215, loss = 0.0010251144412904978
iteration 216, loss = 0.001998130464926362
iteration 217, loss = 0.0011306059313938022
iteration 218, loss = 0.0031221096869558096
iteration 219, loss = 0.0016403113259002566
iteration 220, loss = 0.002938126213848591
iteration 221, loss = 0.0015621912898495793
iteration 222, loss = 0.0013937536859884858
iteration 223, loss = 0.0009233733871951699
iteration 224, loss = 0.0011047484586015344
iteration 225, loss = 0.0010088324779644608
iteration 226, loss = 0.001969957258552313
iteration 227, loss = 0.0015389540931209922
iteration 228, loss = 0.0013158695073798299
iteration 229, loss = 0.0020335272420197725
iteration 230, loss = 0.003246778156608343
iteration 231, loss = 0.0022391516249626875
iteration 232, loss = 0.001975015504285693
iteration 233, loss = 0.0015380242839455605
iteration 234, loss = 0.0015970133244991302
iteration 235, loss = 0.0009067454957403243
iteration 236, loss = 0.0031407345086336136
iteration 237, loss = 0.0025668838061392307
iteration 238, loss = 0.0012581562623381615
iteration 239, loss = 0.0011440571397542953
iteration 240, loss = 0.0013767317868769169
iteration 241, loss = 0.0016710258787497878
iteration 242, loss = 0.0012541876640170813
iteration 243, loss = 0.0012939450098201632
iteration 244, loss = 0.001290736487135291
iteration 245, loss = 0.0011791720753535628
iteration 246, loss = 0.0010842775227501988
iteration 247, loss = 0.0012496252311393619
iteration 248, loss = 0.001321623451076448
iteration 249, loss = 0.0010439405450597405
iteration 250, loss = 0.002355397678911686
iteration 251, loss = 0.0012110266834497452
iteration 252, loss = 0.0016100595239549875
iteration 253, loss = 0.0023728928063064814
iteration 254, loss = 0.0019465541699901223
iteration 255, loss = 0.0011909292079508305
iteration 256, loss = 0.0011814030585810542
iteration 257, loss = 0.0016515250317752361
iteration 258, loss = 0.001177891273982823
iteration 259, loss = 0.0029842883814126253
iteration 260, loss = 0.0012472468661144376
iteration 261, loss = 0.0015202842187136412
iteration 262, loss = 0.0021174028515815735
iteration 263, loss = 0.0016143445391207933
iteration 264, loss = 0.0008896749350242317
iteration 265, loss = 0.001862723845988512
iteration 266, loss = 0.0006824841839261353
iteration 267, loss = 0.0016022012569010258
iteration 268, loss = 0.0017854368779808283
iteration 269, loss = 0.0017020865343511105
iteration 270, loss = 0.001122014131397009
iteration 271, loss = 0.0006493644323199987
iteration 272, loss = 0.0014370399294421077
iteration 273, loss = 0.0016546715050935745
iteration 274, loss = 0.0014264221535995603
iteration 275, loss = 0.0022940936032682657
iteration 276, loss = 0.0014002512907609344
iteration 277, loss = 0.0010608203010633588
iteration 278, loss = 0.0027086606714874506
iteration 279, loss = 0.0015655929455533624
iteration 280, loss = 0.001205855282023549
iteration 281, loss = 0.0016176144126802683
iteration 282, loss = 0.001243371982127428
iteration 283, loss = 0.0009830072522163391
iteration 284, loss = 0.003283738624304533
iteration 285, loss = 0.0015481214504688978
iteration 286, loss = 0.0011510113254189491
iteration 287, loss = 0.0013001044280827045
iteration 288, loss = 0.0013114981120452285
iteration 289, loss = 0.0011646640487015247
iteration 290, loss = 0.001673348480835557
iteration 291, loss = 0.00407419865950942
iteration 292, loss = 0.0015749320155009627
iteration 293, loss = 0.001039413851685822
iteration 294, loss = 0.0012354858918115497
iteration 295, loss = 0.0021333498880267143
iteration 296, loss = 0.00150486349593848
iteration 297, loss = 0.00152847811114043
iteration 298, loss = 0.0013405843637883663
iteration 299, loss = 0.001326176105067134
iteration 0, loss = 0.0012762505793944001
iteration 1, loss = 0.0010005258955061436
iteration 2, loss = 0.001258735079318285
iteration 3, loss = 0.0014849130529910326
iteration 4, loss = 0.0011724205687642097
iteration 5, loss = 0.0012599570909515023
iteration 6, loss = 0.0012192686554044485
iteration 7, loss = 0.0018850781489163637
iteration 8, loss = 0.002328122965991497
iteration 9, loss = 0.002700308570638299
iteration 10, loss = 0.0020340229384601116
iteration 11, loss = 0.004604216665029526
iteration 12, loss = 0.0018645764794200659
iteration 13, loss = 0.0010830597020685673
iteration 14, loss = 0.001174957724288106
iteration 15, loss = 0.0011879325611516833
iteration 16, loss = 0.0029259519651532173
iteration 17, loss = 0.0014371796278283
iteration 18, loss = 0.002010685158893466
iteration 19, loss = 0.0021272022277116776
iteration 20, loss = 0.0012701309751719236
iteration 21, loss = 0.0011698987800627947
iteration 22, loss = 0.0011666754726320505
iteration 23, loss = 0.0009204962989315391
iteration 24, loss = 0.001771849230863154
iteration 25, loss = 0.00213980907574296
iteration 26, loss = 0.0012646964751183987
iteration 27, loss = 0.0009012020891532302
iteration 28, loss = 0.0012236654292792082
iteration 29, loss = 0.0009171611745841801
iteration 30, loss = 0.0009623233927413821
iteration 31, loss = 0.0010633524507284164
iteration 32, loss = 0.0035909104626625776
iteration 33, loss = 0.0014715654542669654
iteration 34, loss = 0.0016408590599894524
iteration 35, loss = 0.0013187624281272292
iteration 36, loss = 0.0019702580757439137
iteration 37, loss = 0.0011316791642457247
iteration 38, loss = 0.001915956730954349
iteration 39, loss = 0.0011668904917314649
iteration 40, loss = 0.0033788997679948807
iteration 41, loss = 0.0014698309823870659
iteration 42, loss = 0.0014107360038906336
iteration 43, loss = 0.001028715749271214
iteration 44, loss = 0.0013224286958575249
iteration 45, loss = 0.0008742230711504817
iteration 46, loss = 0.0013134356122463942
iteration 47, loss = 0.0014001440722495317
iteration 48, loss = 0.001088531338609755
iteration 49, loss = 0.0015593267744407058
iteration 50, loss = 0.0020555569790303707
iteration 51, loss = 0.001205440261401236
iteration 52, loss = 0.0009494380210526288
iteration 53, loss = 0.0013672398636117578
iteration 54, loss = 0.0012596399756148458
iteration 55, loss = 0.0009228575509041548
iteration 56, loss = 0.0023249988444149494
iteration 57, loss = 0.0018294858746230602
iteration 58, loss = 0.0019596826750785112
iteration 59, loss = 0.0010063853114843369
iteration 60, loss = 0.004860989283770323
iteration 61, loss = 0.0014482320984825492
iteration 62, loss = 0.0010881393682211637
iteration 63, loss = 0.001299023861065507
iteration 64, loss = 0.001418070518411696
iteration 65, loss = 0.0011962107382714748
iteration 66, loss = 0.003198544029146433
iteration 67, loss = 0.0016053959261626005
iteration 68, loss = 0.0011430963641032577
iteration 69, loss = 0.0013215538347139955
iteration 70, loss = 0.0007457226747646928
iteration 71, loss = 0.0009727751603350043
iteration 72, loss = 0.0009453096427023411
iteration 73, loss = 0.0017823169473558664
iteration 74, loss = 0.0010731739457696676
iteration 75, loss = 0.001582430675625801
iteration 76, loss = 0.0009750152239575982
iteration 77, loss = 0.001149671385064721
iteration 78, loss = 0.0010513156885281205
iteration 79, loss = 0.001945377909578383
iteration 80, loss = 0.002167189260944724
iteration 81, loss = 0.001139579457230866
iteration 82, loss = 0.0021303240209817886
iteration 83, loss = 0.0015172134153544903
iteration 84, loss = 0.00315404892899096
iteration 85, loss = 0.0011290088295936584
iteration 86, loss = 0.0018976220162585378
iteration 87, loss = 0.0010225399164482951
iteration 88, loss = 0.0011374951573088765
iteration 89, loss = 0.0009141374612227082
iteration 90, loss = 0.0012787091545760632
iteration 91, loss = 0.0013201301917433739
iteration 92, loss = 0.0012315320782363415
iteration 93, loss = 0.0010022277710959315
iteration 94, loss = 0.0009092625114135444
iteration 95, loss = 0.0022974968887865543
iteration 96, loss = 0.0010385764762759209
iteration 97, loss = 0.0012038350105285645
iteration 98, loss = 0.00203353026881814
iteration 99, loss = 0.0009808274917304516
iteration 100, loss = 0.00292470445856452
iteration 101, loss = 0.0013767166528850794
iteration 102, loss = 0.0008890941389836371
iteration 103, loss = 0.0007158785592764616
iteration 104, loss = 0.0009420011192560196
iteration 105, loss = 0.001314831431955099
iteration 106, loss = 0.0023869636934250593
iteration 107, loss = 0.0017161091091111302
iteration 108, loss = 0.0008469803724437952
iteration 109, loss = 0.001117668580263853
iteration 110, loss = 0.0009994313586503267
iteration 111, loss = 0.0012556035071611404
iteration 112, loss = 0.0011428224388509989
iteration 113, loss = 0.0011738076573237777
iteration 114, loss = 0.002533595310524106
iteration 115, loss = 0.001440571271814406
iteration 116, loss = 0.0013524715323001146
iteration 117, loss = 0.0010936497710645199
iteration 118, loss = 0.0015321718528866768
iteration 119, loss = 0.0009082257165573537
iteration 120, loss = 0.0023325360380113125
iteration 121, loss = 0.0008217461872845888
iteration 122, loss = 0.0022726496681571007
iteration 123, loss = 0.0034423046745359898
iteration 124, loss = 0.0011122343130409718
iteration 125, loss = 0.0019908000249415636
iteration 126, loss = 0.0010688009206205606
iteration 127, loss = 0.0013525554677471519
iteration 128, loss = 0.0012394546065479517
iteration 129, loss = 0.001062564318999648
iteration 130, loss = 0.0007905596285127103
iteration 131, loss = 0.0008161614532582462
iteration 132, loss = 0.0008411181042902172
iteration 133, loss = 0.000970737193711102
iteration 134, loss = 0.0009117722511291504
iteration 135, loss = 0.0014333061408251524
iteration 136, loss = 0.0007801628671586514
iteration 137, loss = 0.0008372061420232058
iteration 138, loss = 0.0011462201364338398
iteration 139, loss = 0.0007186735165305436
iteration 140, loss = 0.0008941300329752266
iteration 141, loss = 0.0009244804386980832
iteration 142, loss = 0.0011782554211094975
iteration 143, loss = 0.0011006921995431185
iteration 144, loss = 0.0007336220587603748
iteration 145, loss = 0.0010794081026688218
iteration 146, loss = 0.0013061320642009377
iteration 147, loss = 0.0011962840799242258
iteration 148, loss = 0.004111855290830135
iteration 149, loss = 0.000792759470641613
iteration 150, loss = 0.0015971213579177856
iteration 151, loss = 0.0011561720166355371
iteration 152, loss = 0.0011971778003498912
iteration 153, loss = 0.0009988578967750072
iteration 154, loss = 0.0008970149210654199
iteration 155, loss = 0.0013153981417417526
iteration 156, loss = 0.0033911336213350296
iteration 157, loss = 0.0011889688903465867
iteration 158, loss = 0.0013984389370307326
iteration 159, loss = 0.0009433550876565278
iteration 160, loss = 0.0008577731787227094
iteration 161, loss = 0.001800747006200254
iteration 162, loss = 0.0007470387499779463
iteration 163, loss = 0.0019267814932391047
iteration 164, loss = 0.0015754422638565302
iteration 165, loss = 0.0009897671407088637
iteration 166, loss = 0.0010196310468018055
iteration 167, loss = 0.0008977132965810597
iteration 168, loss = 0.0011436932254582644
iteration 169, loss = 0.0013142225798219442
iteration 170, loss = 0.0020945691503584385
iteration 171, loss = 0.0010143688414245844
iteration 172, loss = 0.0010205014841631055
iteration 173, loss = 0.001098959124647081
iteration 174, loss = 0.0010050364071503282
iteration 175, loss = 0.0009957092115655541
iteration 176, loss = 0.0012169672409072518
iteration 177, loss = 0.0010982390958815813
iteration 178, loss = 0.001160057494416833
iteration 179, loss = 0.001478384481742978
iteration 180, loss = 0.0012857752153649926
iteration 181, loss = 0.0010123475221917033
iteration 182, loss = 0.0008552204817533493
iteration 183, loss = 0.002161900047212839
iteration 184, loss = 0.0009845333406701684
iteration 185, loss = 0.0035189909394830465
iteration 186, loss = 0.0018679689383134246
iteration 187, loss = 0.0022423083428293467
iteration 188, loss = 0.0008776210597716272
iteration 189, loss = 0.002187037840485573
iteration 190, loss = 0.0022605867125093937
iteration 191, loss = 0.0018334765918552876
iteration 192, loss = 0.001119883032515645
iteration 193, loss = 0.0009040378499776125
iteration 194, loss = 0.0011124833254143596
iteration 195, loss = 0.001098480774089694
iteration 196, loss = 0.0008803631644695997
iteration 197, loss = 0.0034301585983484983
iteration 198, loss = 0.0006973638664931059
iteration 199, loss = 0.0017205062322318554
iteration 200, loss = 0.0010605608113110065
iteration 201, loss = 0.001021365518681705
iteration 202, loss = 0.0011147510958835483
iteration 203, loss = 0.0008511097403243184
iteration 204, loss = 0.0015626183012500405
iteration 205, loss = 0.0012141541810706258
iteration 206, loss = 0.000879403087310493
iteration 207, loss = 0.0017967002931982279
iteration 208, loss = 0.0025647117290645838
iteration 209, loss = 0.00136757327709347
iteration 210, loss = 0.0009801529813557863
iteration 211, loss = 0.0009551334660500288
iteration 212, loss = 0.0011544758453965187
iteration 213, loss = 0.0011676402064040303
iteration 214, loss = 0.0009548157104291022
iteration 215, loss = 0.001476989476941526
iteration 216, loss = 0.0010337691055610776
iteration 217, loss = 0.001921431627124548
iteration 218, loss = 0.001419415813870728
iteration 219, loss = 0.001673094229772687
iteration 220, loss = 0.0007145070703700185
iteration 221, loss = 0.003155446844175458
iteration 222, loss = 0.00201890105381608
iteration 223, loss = 0.001204083557240665
iteration 224, loss = 0.00164029689040035
iteration 225, loss = 0.0011308460962027311
iteration 226, loss = 0.0008550224592909217
iteration 227, loss = 0.000765632139518857
iteration 228, loss = 0.001082587637938559
iteration 229, loss = 0.0011808540439233184
iteration 230, loss = 0.0008651686366647482
iteration 231, loss = 0.0009714934276416898
iteration 232, loss = 0.0029196804389357567
iteration 233, loss = 0.0008854657062329352
iteration 234, loss = 0.0015472355298697948
iteration 235, loss = 0.000728723534848541
iteration 236, loss = 0.0016314528184011579
iteration 237, loss = 0.0009176981402561069
iteration 238, loss = 0.00099501246586442
iteration 239, loss = 0.001485668821260333
iteration 240, loss = 0.000989752821624279
iteration 241, loss = 0.0014262718614190817
iteration 242, loss = 0.0015481035225093365
iteration 243, loss = 0.000860790372826159
iteration 244, loss = 0.0016744707245379686
iteration 245, loss = 0.0009867777116596699
iteration 246, loss = 0.0026749877724796534
iteration 247, loss = 0.0011692957486957312
iteration 248, loss = 0.001385803334414959
iteration 249, loss = 0.0015941292513161898
iteration 250, loss = 0.0011862798128277063
iteration 251, loss = 0.001296579372137785
iteration 252, loss = 0.0014558634720742702
iteration 253, loss = 0.0011272167321294546
iteration 254, loss = 0.0013873368734493852
iteration 255, loss = 0.0007344332407228649
iteration 256, loss = 0.0014921871479600668
iteration 257, loss = 0.0013691690983250737
iteration 258, loss = 0.0017303251661360264
iteration 259, loss = 0.0014133539516478777
iteration 260, loss = 0.0015413382789120078
iteration 261, loss = 0.0007455966551788151
iteration 262, loss = 0.0010478838812559843
iteration 263, loss = 0.0012552207335829735
iteration 264, loss = 0.0013984154211357236
iteration 265, loss = 0.001556252595037222
iteration 266, loss = 0.0014125739689916372
iteration 267, loss = 0.0012861831346526742
iteration 268, loss = 0.0010544529650360346
iteration 269, loss = 0.0017820388311520219
iteration 270, loss = 0.0012739760568365455
iteration 271, loss = 0.000840033171698451
iteration 272, loss = 0.0016970543656498194
iteration 273, loss = 0.0017354500014334917
iteration 274, loss = 0.0013807015493512154
iteration 275, loss = 0.0012542386539280415
iteration 276, loss = 0.0012675040634348989
iteration 277, loss = 0.002138006268069148
iteration 278, loss = 0.0015796422958374023
iteration 279, loss = 0.0009482406312599778
iteration 280, loss = 0.0013402535114437342
iteration 281, loss = 0.0014705444918945432
iteration 282, loss = 0.003242582082748413
iteration 283, loss = 0.0013527553528547287
iteration 284, loss = 0.0017793311271816492
iteration 285, loss = 0.0008594503742642701
iteration 286, loss = 0.0027719170320779085
iteration 287, loss = 0.0010853896383196115
iteration 288, loss = 0.000961958197876811
iteration 289, loss = 0.0018086298368871212
iteration 290, loss = 0.0014087918680161238
iteration 291, loss = 0.0008400528458878398
iteration 292, loss = 0.0032846841495484114
iteration 293, loss = 0.0009765322902239859
iteration 294, loss = 0.001385469688102603
iteration 295, loss = 0.0010652150958776474
iteration 296, loss = 0.0010800715535879135
iteration 297, loss = 0.000949653796851635
iteration 298, loss = 0.0022034442517906427
iteration 299, loss = 0.001032213564030826
iteration 0, loss = 0.0008271376718766987
iteration 1, loss = 0.0010985595872625709
iteration 2, loss = 0.0021190380211919546
iteration 3, loss = 0.0010925652459263802
iteration 4, loss = 0.0012099368032068014
iteration 5, loss = 0.0010896445019170642
iteration 6, loss = 0.002403716091066599
iteration 7, loss = 0.0008661883766762912
iteration 8, loss = 0.001904314267449081
iteration 9, loss = 0.0009051044471561909
iteration 10, loss = 0.0009898112621158361
iteration 11, loss = 0.0008113104850053787
iteration 12, loss = 0.0014814636670053005
iteration 13, loss = 0.0035224836319684982
iteration 14, loss = 0.0022833170369267464
iteration 15, loss = 0.0012261430965736508
iteration 16, loss = 0.001294266781769693
iteration 17, loss = 0.0007855765288695693
iteration 18, loss = 0.0009418780682608485
iteration 19, loss = 0.0010852282866835594
iteration 20, loss = 0.000983432400971651
iteration 21, loss = 0.0011327593820169568
iteration 22, loss = 0.0006201533833518624
iteration 23, loss = 0.0015544312773272395
iteration 24, loss = 0.0011520367115736008
iteration 25, loss = 0.0008476657094433904
iteration 26, loss = 0.0008115902892313898
iteration 27, loss = 0.001025203731842339
iteration 28, loss = 0.002372096525505185
iteration 29, loss = 0.0018316195346415043
iteration 30, loss = 0.0015107284998521209
iteration 31, loss = 0.0020198593847453594
iteration 32, loss = 0.0007798944134265184
iteration 33, loss = 0.003150323173031211
iteration 34, loss = 0.000984999118372798
iteration 35, loss = 0.0018371760379523039
iteration 36, loss = 0.0011116312816739082
iteration 37, loss = 0.0017111124470829964
iteration 38, loss = 0.0012821498094126582
iteration 39, loss = 0.0019175406778231263
iteration 40, loss = 0.0008231476531364024
iteration 41, loss = 0.0007973703905008733
iteration 42, loss = 0.0012205952079966664
iteration 43, loss = 0.0008801266085356474
iteration 44, loss = 0.0018166251247748733
iteration 45, loss = 0.001979009248316288
iteration 46, loss = 0.0010233694920316339
iteration 47, loss = 0.0012309849262237549
iteration 48, loss = 0.0011210517259314656
iteration 49, loss = 0.0027284231036901474
iteration 50, loss = 0.0007871979614719748
iteration 51, loss = 0.0006984156789258122
iteration 52, loss = 0.0010315310209989548
iteration 53, loss = 0.0014161538565531373
iteration 54, loss = 0.0012823217548429966
iteration 55, loss = 0.000940630619879812
iteration 56, loss = 0.0019720690324902534
iteration 57, loss = 0.0026294044218957424
iteration 58, loss = 0.003306800266727805
iteration 59, loss = 0.0012516413116827607
iteration 60, loss = 0.0029196140822023153
iteration 61, loss = 0.000754049513489008
iteration 62, loss = 0.0008998378179967403
iteration 63, loss = 0.002931463997811079
iteration 64, loss = 0.0010956489713862538
iteration 65, loss = 0.0012770947068929672
iteration 66, loss = 0.0008728152606636286
iteration 67, loss = 0.0016346911434084177
iteration 68, loss = 0.0010794277768582106
iteration 69, loss = 0.0013373404508456588
iteration 70, loss = 0.0010347594507038593
iteration 71, loss = 0.0013482082867994905
iteration 72, loss = 0.0011293817078694701
iteration 73, loss = 0.0007544569671154022
iteration 74, loss = 0.0008278707973659039
iteration 75, loss = 0.0014851903542876244
iteration 76, loss = 0.0014467560686171055
iteration 77, loss = 0.0015059052966535091
iteration 78, loss = 0.0011499670799821615
iteration 79, loss = 0.0008893803460523486
iteration 80, loss = 0.0008340596687048674
iteration 81, loss = 0.0008519982220605016
iteration 82, loss = 0.0010456403251737356
iteration 83, loss = 0.0013339805882424116
iteration 84, loss = 0.0013481981586664915
iteration 85, loss = 0.002170763909816742
iteration 86, loss = 0.0011098458198830485
iteration 87, loss = 0.001425029244273901
iteration 88, loss = 0.0015433364314958453
iteration 89, loss = 0.003297319170087576
iteration 90, loss = 0.0012152133276686072
iteration 91, loss = 0.0012411985080689192
iteration 92, loss = 0.0007649754406884313
iteration 93, loss = 0.0011440705275163054
iteration 94, loss = 0.0017403539968654513
iteration 95, loss = 0.0008823208627291024
iteration 96, loss = 0.0010324330069124699
iteration 97, loss = 0.0011551500065252185
iteration 98, loss = 0.001074913190677762
iteration 99, loss = 0.0009167024400085211
iteration 100, loss = 0.0008174022659659386
iteration 101, loss = 0.0008729020482860506
iteration 102, loss = 0.0010917546460404992
iteration 103, loss = 0.001123763038776815
iteration 104, loss = 0.0007091935840435326
iteration 105, loss = 0.0012647979892790318
iteration 106, loss = 0.0011502290144562721
iteration 107, loss = 0.0014718433376401663
iteration 108, loss = 0.0007531568990088999
iteration 109, loss = 0.0014489475870504975
iteration 110, loss = 0.002240108558908105
iteration 111, loss = 0.001658164313994348
iteration 112, loss = 0.0009381509735248983
iteration 113, loss = 0.0027382175903767347
iteration 114, loss = 0.0008129160851240158
iteration 115, loss = 0.000987075618468225
iteration 116, loss = 0.0007358283619396389
iteration 117, loss = 0.0011822811793535948
iteration 118, loss = 0.0012358862441033125
iteration 119, loss = 0.0009356586378999054
iteration 120, loss = 0.0008553166990168393
iteration 121, loss = 0.0010811140527948737
iteration 122, loss = 0.0015075663104653358
iteration 123, loss = 0.001265341299585998
iteration 124, loss = 0.001904119155369699
iteration 125, loss = 0.0015591393457725644
iteration 126, loss = 0.0009797003585845232
iteration 127, loss = 0.0015514296246692538
iteration 128, loss = 0.0013380760792642832
iteration 129, loss = 0.0016826611245051026
iteration 130, loss = 0.0012706468114629388
iteration 131, loss = 0.0011269771493971348
iteration 132, loss = 0.001134052174165845
iteration 133, loss = 0.0012882209848612547
iteration 134, loss = 0.0011581011349335313
iteration 135, loss = 0.0012634980957955122
iteration 136, loss = 0.0010138439247384667
iteration 137, loss = 0.0010868211975321174
iteration 138, loss = 0.0012088138610124588
iteration 139, loss = 0.0008843144751153886
iteration 140, loss = 0.0010252068750560284
iteration 141, loss = 0.0017182005103677511
iteration 142, loss = 0.0010357998544350266
iteration 143, loss = 0.0018098494037985802
iteration 144, loss = 0.0010774692054837942
iteration 145, loss = 0.0009679017239250243
iteration 146, loss = 0.0030441468115895987
iteration 147, loss = 0.0012374510988593102
iteration 148, loss = 0.0007858887547627091
iteration 149, loss = 0.0009109271923080087
iteration 150, loss = 0.0010128736030310392
iteration 151, loss = 0.001626487821340561
iteration 152, loss = 0.0008803847595117986
iteration 153, loss = 0.0009281137608923018
iteration 154, loss = 0.0024813730269670486
iteration 155, loss = 0.0013663163408637047
iteration 156, loss = 0.001206806511618197
iteration 157, loss = 0.000920131744351238
iteration 158, loss = 0.0021104104816913605
iteration 159, loss = 0.0009942224714905024
iteration 160, loss = 0.001898901304230094
iteration 161, loss = 0.0011890728492289782
iteration 162, loss = 0.0010207502637058496
iteration 163, loss = 0.0015271855518221855
iteration 164, loss = 0.0009512240067124367
iteration 165, loss = 0.0011615020921453834
iteration 166, loss = 0.0008597025880590081
iteration 167, loss = 0.0013429401442408562
iteration 168, loss = 0.0012730114394798875
iteration 169, loss = 0.0010575653286650777
iteration 170, loss = 0.0008603507303632796
iteration 171, loss = 0.0020355076994746923
iteration 172, loss = 0.0013214834034442902
iteration 173, loss = 0.003227994544431567
iteration 174, loss = 0.0007754662656225264
iteration 175, loss = 0.0012060422450304031
iteration 176, loss = 0.001844735350459814
iteration 177, loss = 0.001192481373436749
iteration 178, loss = 0.0011621414450928569
iteration 179, loss = 0.0011877025244757533
iteration 180, loss = 0.0008788061677478254
iteration 181, loss = 0.0007654082728549838
iteration 182, loss = 0.001531739835627377
iteration 183, loss = 0.0012538445880636573
iteration 184, loss = 0.0010098384227603674
iteration 185, loss = 0.0009881029836833477
iteration 186, loss = 0.0014322095084935427
iteration 187, loss = 0.0018521553138270974
iteration 188, loss = 0.0011680828174576163
iteration 189, loss = 0.0007132201571948826
iteration 190, loss = 0.001789034460671246
iteration 191, loss = 0.00133089954033494
iteration 192, loss = 0.000623233150690794
iteration 193, loss = 0.0011158424895256758
iteration 194, loss = 0.003374343039467931
iteration 195, loss = 0.0008223034092225134
iteration 196, loss = 0.0009976873407140374
iteration 197, loss = 0.0013907842803746462
iteration 198, loss = 0.0016654549399390817
iteration 199, loss = 0.0009070489322766662
iteration 200, loss = 0.0011133186053484678
iteration 201, loss = 0.0008888411684893072
iteration 202, loss = 0.0022631287574768066
iteration 203, loss = 0.0009729076991789043
iteration 204, loss = 0.0013257412938401103
iteration 205, loss = 0.0008727724198251963
iteration 206, loss = 0.001246684230864048
iteration 207, loss = 0.0008052168996073306
iteration 208, loss = 0.0009207903640344739
iteration 209, loss = 0.0012399250408634543
iteration 210, loss = 0.000706215447280556
iteration 211, loss = 0.0016108773415908217
iteration 212, loss = 0.0022088198456913233
iteration 213, loss = 0.0009324464481323957
iteration 214, loss = 0.0030648699030280113
iteration 215, loss = 0.0020165678579360247
iteration 216, loss = 0.0011819681385532022
iteration 217, loss = 0.0009843690786510706
iteration 218, loss = 0.0015941947931423783
iteration 219, loss = 0.0012834555236622691
iteration 220, loss = 0.0018960395827889442
iteration 221, loss = 0.0009714227053336799
iteration 222, loss = 0.0028663701377809048
iteration 223, loss = 0.0017642192542552948
iteration 224, loss = 0.0009193418663926423
iteration 225, loss = 0.0018682797672227025
iteration 226, loss = 0.0011061251861974597
iteration 227, loss = 0.0019645956344902515
iteration 228, loss = 0.0015635305317118764
iteration 229, loss = 0.0010260018752887845
iteration 230, loss = 0.0011036340147256851
iteration 231, loss = 0.0011299116304144263
iteration 232, loss = 0.0009836166864261031
iteration 233, loss = 0.0009502896573394537
iteration 234, loss = 0.0007610513130202889
iteration 235, loss = 0.001043478841893375
iteration 236, loss = 0.0029625792521983385
iteration 237, loss = 0.002377610420808196
iteration 238, loss = 0.001679314998909831
iteration 239, loss = 0.0016723559238016605
iteration 240, loss = 0.0008729688706807792
iteration 241, loss = 0.001489633577875793
iteration 242, loss = 0.0013892946299165487
iteration 243, loss = 0.0010428791865706444
iteration 244, loss = 0.0013349122600629926
iteration 245, loss = 0.0020894743502140045
iteration 246, loss = 0.0014447381254285574
iteration 247, loss = 0.0017596650868654251
iteration 248, loss = 0.0008742954814806581
iteration 249, loss = 0.0012263563694432378
iteration 250, loss = 0.0010679680854082108
iteration 251, loss = 0.0010752922389656305
iteration 252, loss = 0.001110181212425232
iteration 253, loss = 0.0008288691751658916
iteration 254, loss = 0.0009338122908957303
iteration 255, loss = 0.0009051832021214068
iteration 256, loss = 0.0009732352918945253
iteration 257, loss = 0.0012095370329916477
iteration 258, loss = 0.0008782917284406722
iteration 259, loss = 0.000785685027949512
iteration 260, loss = 0.0020499664824455976
iteration 261, loss = 0.001226480701006949
iteration 262, loss = 0.0014372210716828704
iteration 263, loss = 0.0014438724610954523
iteration 264, loss = 0.0011027746368199587
iteration 265, loss = 0.0006718888180330396
iteration 266, loss = 0.0008752244175411761
iteration 267, loss = 0.0010109007125720382
iteration 268, loss = 0.001138849533163011
iteration 269, loss = 0.0011270690010860562
iteration 270, loss = 0.0016509683337062597
iteration 271, loss = 0.0019241154659539461
iteration 272, loss = 0.0009801262058317661
iteration 273, loss = 0.0011149048805236816
iteration 274, loss = 0.0012533561093732715
iteration 275, loss = 0.0011126010213047266
iteration 276, loss = 0.0030973656103014946
iteration 277, loss = 0.0016982639208436012
iteration 278, loss = 0.0007432727725245059
iteration 279, loss = 0.0008031009929254651
iteration 280, loss = 0.0009469058713875711
iteration 281, loss = 0.001086176373064518
iteration 282, loss = 0.0009059935109689832
iteration 283, loss = 0.00166505912784487
iteration 284, loss = 0.0009420802816748619
iteration 285, loss = 0.0007378432201221585
iteration 286, loss = 0.000939933757763356
iteration 287, loss = 0.0016004793578758836
iteration 288, loss = 0.0013895010342821479
iteration 289, loss = 0.0009846070315688848
iteration 290, loss = 0.0009428109624423087
iteration 291, loss = 0.0013078975025564432
iteration 292, loss = 0.0010336849372833967
iteration 293, loss = 0.0011638605501502752
iteration 294, loss = 0.001783260377123952
iteration 295, loss = 0.0008342185756191611
iteration 296, loss = 0.000787129276432097
iteration 297, loss = 0.0012213101144880056
iteration 298, loss = 0.0007460947381332517
iteration 299, loss = 0.001353381434455514
iteration 0, loss = 0.0008746347739361227
iteration 1, loss = 0.0009215286117978394
iteration 2, loss = 0.0010762893361970782
iteration 3, loss = 0.0017407187260687351
iteration 4, loss = 0.0008651494281366467
iteration 5, loss = 0.0011296976590529084
iteration 6, loss = 0.0015322945546358824
iteration 7, loss = 0.0015151944244280457
iteration 8, loss = 0.0010769452201202512
iteration 9, loss = 0.0008717364398762584
iteration 10, loss = 0.0010253021027892828
iteration 11, loss = 0.0008635105332359672
iteration 12, loss = 0.0009004630846902728
iteration 13, loss = 0.0014411583542823792
iteration 14, loss = 0.0014497784432023764
iteration 15, loss = 0.0009154065628536046
iteration 16, loss = 0.0027922734152525663
iteration 17, loss = 0.0010952407028526068
iteration 18, loss = 0.0007136190542951226
iteration 19, loss = 0.0007400893955491483
iteration 20, loss = 0.003065942320972681
iteration 21, loss = 0.0013940130593255162
iteration 22, loss = 0.0011077238013967872
iteration 23, loss = 0.0009963951306417584
iteration 24, loss = 0.0010869309771806002
iteration 25, loss = 0.0011627787025645375
iteration 26, loss = 0.001382329617626965
iteration 27, loss = 0.0009408207843080163
iteration 28, loss = 0.0010893963044509292
iteration 29, loss = 0.0009955818532034755
iteration 30, loss = 0.000721454678568989
iteration 31, loss = 0.0018465690081939101
iteration 32, loss = 0.0006867380579933524
iteration 33, loss = 0.000995353446342051
iteration 34, loss = 0.0008541663410142064
iteration 35, loss = 0.0037737537641078234
iteration 36, loss = 0.0009927883511409163
iteration 37, loss = 0.0012130343820899725
iteration 38, loss = 0.000996904680505395
iteration 39, loss = 0.0012278950307518244
iteration 40, loss = 0.0007243306026794016
iteration 41, loss = 0.0023078089579939842
iteration 42, loss = 0.001906924182549119
iteration 43, loss = 0.0009179235785268247
iteration 44, loss = 0.0012102427426725626
iteration 45, loss = 0.000745956611353904
iteration 46, loss = 0.0015018858248367906
iteration 47, loss = 0.00275491364300251
iteration 48, loss = 0.003091835416853428
iteration 49, loss = 0.0015090539818629622
iteration 50, loss = 0.002106025815010071
iteration 51, loss = 0.0007948908023536205
iteration 52, loss = 0.0027453226502984762
iteration 53, loss = 0.0011568674817681313
iteration 54, loss = 0.0012624189257621765
iteration 55, loss = 0.0010770214721560478
iteration 56, loss = 0.0009676127228885889
iteration 57, loss = 0.0017603692831471562
iteration 58, loss = 0.0020969726610928774
iteration 59, loss = 0.003572694957256317
iteration 60, loss = 0.0006937471334822476
iteration 61, loss = 0.002121158642694354
iteration 62, loss = 0.0009493514662608504
iteration 63, loss = 0.001111238612793386
iteration 64, loss = 0.0013434423599392176
iteration 65, loss = 0.0008153936360031366
iteration 66, loss = 0.0010024893563240767
iteration 67, loss = 0.0010532733285799623
iteration 68, loss = 0.0008797039627097547
iteration 69, loss = 0.002567664487287402
iteration 70, loss = 0.000648225424811244
iteration 71, loss = 0.0007716041873209178
iteration 72, loss = 0.0007197035592980683
iteration 73, loss = 0.001196430646814406
iteration 74, loss = 0.00216709915548563
iteration 75, loss = 0.0009002808947116137
iteration 76, loss = 0.001641125068999827
iteration 77, loss = 0.0013406682992354035
iteration 78, loss = 0.0012521845055744052
iteration 79, loss = 0.0008366376860067248
iteration 80, loss = 0.0017714333953335881
iteration 81, loss = 0.0008345578680746257
iteration 82, loss = 0.0011066532460972667
iteration 83, loss = 0.0010177850490435958
iteration 84, loss = 0.0007949225837364793
iteration 85, loss = 0.0008614531834609807
iteration 86, loss = 0.0010078249033540487
iteration 87, loss = 0.001094539649784565
iteration 88, loss = 0.0009620801429264247
iteration 89, loss = 0.0011196201667189598
iteration 90, loss = 0.0009677077177911997
iteration 91, loss = 0.0007387325749732554
iteration 92, loss = 0.0011874003103002906
iteration 93, loss = 0.0009563300991430879
iteration 94, loss = 0.001012361142784357
iteration 95, loss = 0.0006862339796498418
iteration 96, loss = 0.0010421524057164788
iteration 97, loss = 0.0012267414713278413
iteration 98, loss = 0.0014368307311087847
iteration 99, loss = 0.0016615252243354917
iteration 100, loss = 0.0007604454876855016
iteration 101, loss = 0.0011491503100842237
iteration 102, loss = 0.0015028866473585367
iteration 103, loss = 0.0013497213367372751
iteration 104, loss = 0.0013152069877833128
iteration 105, loss = 0.0006654367316514254
iteration 106, loss = 0.000880569510627538
iteration 107, loss = 0.0009576044394634664
iteration 108, loss = 0.0006033905083313584
iteration 109, loss = 0.0008328137919306755
iteration 110, loss = 0.00262585561722517
iteration 111, loss = 0.0010547911515459418
iteration 112, loss = 0.001474379445426166
iteration 113, loss = 0.0009561012266203761
iteration 114, loss = 0.0010391237447038293
iteration 115, loss = 0.0009148821700364351
iteration 116, loss = 0.0017040681559592485
iteration 117, loss = 0.0010817128932103515
iteration 118, loss = 0.0010649480391293764
iteration 119, loss = 0.0009298932855017483
iteration 120, loss = 0.0008404820109717548
iteration 121, loss = 0.0009339053649455309
iteration 122, loss = 0.0014590172795578837
iteration 123, loss = 0.0009083431796170771
iteration 124, loss = 0.0006542498013004661
iteration 125, loss = 0.0016126158880069852
iteration 126, loss = 0.003079525660723448
iteration 127, loss = 0.0009775105863809586
iteration 128, loss = 0.0009968801168724895
iteration 129, loss = 0.0011617705458775163
iteration 130, loss = 0.000978071941062808
iteration 131, loss = 0.0032060518860816956
iteration 132, loss = 0.0008327323012053967
iteration 133, loss = 0.0008780689095146954
iteration 134, loss = 0.0008981545688584447
iteration 135, loss = 0.0011341419303789735
iteration 136, loss = 0.0009484543115831912
iteration 137, loss = 0.0008582648006267846
iteration 138, loss = 0.0015948059735819697
iteration 139, loss = 0.0013216687366366386
iteration 140, loss = 0.0011166291078552604
iteration 141, loss = 0.001072683953680098
iteration 142, loss = 0.001925861113704741
iteration 143, loss = 0.0008009045850485563
iteration 144, loss = 0.0009410128695890307
iteration 145, loss = 0.0013013766147196293
iteration 146, loss = 0.0011089866748079658
iteration 147, loss = 0.001415308564901352
iteration 148, loss = 0.0008704761858098209
iteration 149, loss = 0.0012852190993726254
iteration 150, loss = 0.0010757686104625463
iteration 151, loss = 0.0018093391554430127
iteration 152, loss = 0.0014719321625307202
iteration 153, loss = 0.001038342947140336
iteration 154, loss = 0.002744182012975216
iteration 155, loss = 0.0014983696164563298
iteration 156, loss = 0.0008696777513250709
iteration 157, loss = 0.0007115409825928509
iteration 158, loss = 0.0011334813898429275
iteration 159, loss = 0.0012831646017730236
iteration 160, loss = 0.0007870323024690151
iteration 161, loss = 0.0025844292249530554
iteration 162, loss = 0.0007288767956197262
iteration 163, loss = 0.0007649653125554323
iteration 164, loss = 0.0011590453796088696
iteration 165, loss = 0.001485000248067081
iteration 166, loss = 0.0016476045129820704
iteration 167, loss = 0.001441939384676516
iteration 168, loss = 0.0012343485141173005
iteration 169, loss = 0.0010031982092186809
iteration 170, loss = 0.0009506981587037444
iteration 171, loss = 0.0010592907201498747
iteration 172, loss = 0.0013365403283387423
iteration 173, loss = 0.0017564259469509125
iteration 174, loss = 0.0008445889689028263
iteration 175, loss = 0.0015923489117994905
iteration 176, loss = 0.000624987471383065
iteration 177, loss = 0.0007374155102297664
iteration 178, loss = 0.0012167940149083734
iteration 179, loss = 0.00073945929761976
iteration 180, loss = 0.0013717965921387076
iteration 181, loss = 0.0010150482412427664
iteration 182, loss = 0.001306310179643333
iteration 183, loss = 0.00127306894864887
iteration 184, loss = 0.0008927236194722354
iteration 185, loss = 0.00188068940769881
iteration 186, loss = 0.0014147242764011025
iteration 187, loss = 0.0016398438019677997
iteration 188, loss = 0.002507732482627034
iteration 189, loss = 0.0028020243626087904
iteration 190, loss = 0.0008925158763304353
iteration 191, loss = 0.0010528796119615436
iteration 192, loss = 0.0006869559292681515
iteration 193, loss = 0.001038515823893249
iteration 194, loss = 0.001879438292235136
iteration 195, loss = 0.0011321126949042082
iteration 196, loss = 0.0006670369766652584
iteration 197, loss = 0.0013573236064985394
iteration 198, loss = 0.0006849426426924765
iteration 199, loss = 0.0016938585322350264
iteration 200, loss = 0.0021769332233816385
iteration 201, loss = 0.0011364204110577703
iteration 202, loss = 0.0011213681427761912
iteration 203, loss = 0.0007793417316861451
iteration 204, loss = 0.0009831686038523912
iteration 205, loss = 0.0005195841658860445
iteration 206, loss = 0.0010215204674750566
iteration 207, loss = 0.0011990648927167058
iteration 208, loss = 0.0009722896502353251
iteration 209, loss = 0.0011661688331514597
iteration 210, loss = 0.001208383240737021
iteration 211, loss = 0.0007905749953351915
iteration 212, loss = 0.0010151369497179985
iteration 213, loss = 0.0019850374665111303
iteration 214, loss = 0.001535592251457274
iteration 215, loss = 0.0010533560998737812
iteration 216, loss = 0.0011858227662742138
iteration 217, loss = 0.0011994995875284076
iteration 218, loss = 0.0009887765627354383
iteration 219, loss = 0.0009438627166673541
iteration 220, loss = 0.0013055589515715837
iteration 221, loss = 0.0008647931972518563
iteration 222, loss = 0.000980163342319429
iteration 223, loss = 0.0012051714584231377
iteration 224, loss = 0.0007149059092625976
iteration 225, loss = 0.0008169352659024298
iteration 226, loss = 0.0011979829287156463
iteration 227, loss = 0.0009246995905414224
iteration 228, loss = 0.0009201978100463748
iteration 229, loss = 0.0014665329363197088
iteration 230, loss = 0.0009577448363415897
iteration 231, loss = 0.0012240521609783173
iteration 232, loss = 0.0019643555860966444
iteration 233, loss = 0.0013401869218796492
iteration 234, loss = 0.002376451389864087
iteration 235, loss = 0.0008186659542843699
iteration 236, loss = 0.0007380169117823243
iteration 237, loss = 0.001151671982370317
iteration 238, loss = 0.0007462957873940468
iteration 239, loss = 0.0007681639399379492
iteration 240, loss = 0.000976314942818135
iteration 241, loss = 0.0016638214001432061
iteration 242, loss = 0.0007881568162702024
iteration 243, loss = 0.0009922274621203542
iteration 244, loss = 0.0006516361027024686
iteration 245, loss = 0.000911646755412221
iteration 246, loss = 0.0007853613933548331
iteration 247, loss = 0.0011476479703560472
iteration 248, loss = 0.0006921178428456187
iteration 249, loss = 0.0009418234694749117
iteration 250, loss = 0.0008363020024262369
iteration 251, loss = 0.0011317573953419924
iteration 252, loss = 0.0009292589966207743
iteration 253, loss = 0.0008293171995319426
iteration 254, loss = 0.001006702776066959
iteration 255, loss = 0.0018877480179071426
iteration 256, loss = 0.0007014590664766729
iteration 257, loss = 0.0008926060982048512
iteration 258, loss = 0.0006867328193038702
iteration 259, loss = 0.0008125066524371505
iteration 260, loss = 0.002828309079632163
iteration 261, loss = 0.0011206595227122307
iteration 262, loss = 0.0006461525335907936
iteration 263, loss = 0.0012967032380402088
iteration 264, loss = 0.001621801988221705
iteration 265, loss = 0.0014142482541501522
iteration 266, loss = 0.0008160072611644864
iteration 267, loss = 0.0006031394586898386
iteration 268, loss = 0.0015370309120044112
iteration 269, loss = 0.0008603355381637812
iteration 270, loss = 0.0009026415646076202
iteration 271, loss = 0.0008381952648051083
iteration 272, loss = 0.0008062781998887658
iteration 273, loss = 0.00332969194278121
iteration 274, loss = 0.0033462310675531626
iteration 275, loss = 0.0019611315801739693
iteration 276, loss = 0.0018477598205208778
iteration 277, loss = 0.0012246676487848163
iteration 278, loss = 0.0005825592670589685
iteration 279, loss = 0.0006747081060893834
iteration 280, loss = 0.0010405054781585932
iteration 281, loss = 0.0007611867040395737
iteration 282, loss = 0.0015727814752608538
iteration 283, loss = 0.0008238614536821842
iteration 284, loss = 0.0016224581049755216
iteration 285, loss = 0.0010991704184561968
iteration 286, loss = 0.0007548971916548908
iteration 287, loss = 0.0006988603272475302
iteration 288, loss = 0.0009454322862438858
iteration 289, loss = 0.001015508663840592
iteration 290, loss = 0.000676934199873358
iteration 291, loss = 0.0012528301449492574
iteration 292, loss = 0.0008614349062554538
iteration 293, loss = 0.0017721345648169518
iteration 294, loss = 0.0008361085201613605
iteration 295, loss = 0.0018544013146311045
iteration 296, loss = 0.0008545906748622656
iteration 297, loss = 0.0008063284331001341
iteration 298, loss = 0.0005697889719158411
iteration 299, loss = 0.0005604933248832822
iteration 0, loss = 0.0010704481974244118
iteration 1, loss = 0.0009752599289640784
iteration 2, loss = 0.000869114650413394
iteration 3, loss = 0.0010781600140035152
iteration 4, loss = 0.0009868957567960024
iteration 5, loss = 0.0008501051343046129
iteration 6, loss = 0.0008028183365240693
iteration 7, loss = 0.0011904833372682333
iteration 8, loss = 0.0010577342472970486
iteration 9, loss = 0.0006438126438297331
iteration 10, loss = 0.0013450097758322954
iteration 11, loss = 0.001406417810358107
iteration 12, loss = 0.000949752691667527
iteration 13, loss = 0.0008284288342110813
iteration 14, loss = 0.0012580472975969315
iteration 15, loss = 0.0014997996622696519
iteration 16, loss = 0.0012436555698513985
iteration 17, loss = 0.0007808381924405694
iteration 18, loss = 0.0012205728562548757
iteration 19, loss = 0.000724398938473314
iteration 20, loss = 0.0010632933117449284
iteration 21, loss = 0.0006525938515551388
iteration 22, loss = 0.0009717652574181557
iteration 23, loss = 0.0009235068573616445
iteration 24, loss = 0.002898670034483075
iteration 25, loss = 0.0008684095228090882
iteration 26, loss = 0.0010301475413143635
iteration 27, loss = 0.0013506471877917647
iteration 28, loss = 0.0006657082121819258
iteration 29, loss = 0.0024063207674771547
iteration 30, loss = 0.00138171820435673
iteration 31, loss = 0.0007169360178522766
iteration 32, loss = 0.0007477592444047332
iteration 33, loss = 0.0007576972711831331
iteration 34, loss = 0.0011214511469006538
iteration 35, loss = 0.0009400509297847748
iteration 36, loss = 0.0011622257297858596
iteration 37, loss = 0.0015801502158865333
iteration 38, loss = 0.0011277974117547274
iteration 39, loss = 0.0006635908503085375
iteration 40, loss = 0.00113192037679255
iteration 41, loss = 0.0019891129340976477
iteration 42, loss = 0.0007022476638667285
iteration 43, loss = 0.0006834667292423546
iteration 44, loss = 0.0010065995156764984
iteration 45, loss = 0.001638881047256291
iteration 46, loss = 0.002322266809642315
iteration 47, loss = 0.0006159450276754797
iteration 48, loss = 0.0008315878221765161
iteration 49, loss = 0.0008633117540739477
iteration 50, loss = 0.0010785139165818691
iteration 51, loss = 0.0009252477902919054
iteration 52, loss = 0.0007462922367267311
iteration 53, loss = 0.001025493023917079
iteration 54, loss = 0.0009583761566318572
iteration 55, loss = 0.0007481464999727905
iteration 56, loss = 0.0005731004639528692
iteration 57, loss = 0.0009416048997081816
iteration 58, loss = 0.001635394524782896
iteration 59, loss = 0.0014548292383551598
iteration 60, loss = 0.0009298010263592005
iteration 61, loss = 0.0016185999847948551
iteration 62, loss = 0.0008512535132467747
iteration 63, loss = 0.0010350178927183151
iteration 64, loss = 0.000705067883245647
iteration 65, loss = 0.0018475386314094067
iteration 66, loss = 0.001461949199438095
iteration 67, loss = 0.0010101408697664738
iteration 68, loss = 0.0011073584901168942
iteration 69, loss = 0.0008940866100601852
iteration 70, loss = 0.0007747208001092076
iteration 71, loss = 0.001301097683608532
iteration 72, loss = 0.000642777478788048
iteration 73, loss = 0.0009882096201181412
iteration 74, loss = 0.0009172160062007606
iteration 75, loss = 0.0018324017291888595
iteration 76, loss = 0.0006969055975787342
iteration 77, loss = 0.0009110797545872629
iteration 78, loss = 0.0007512049051001668
iteration 79, loss = 0.0015425399178639054
iteration 80, loss = 0.0008221343159675598
iteration 81, loss = 0.0011384815443307161
iteration 82, loss = 0.0011767242103815079
iteration 83, loss = 0.0007493228767998517
iteration 84, loss = 0.0007599687669426203
iteration 85, loss = 0.00117736856918782
iteration 86, loss = 0.00132877251598984
iteration 87, loss = 0.0008510510670021176
iteration 88, loss = 0.0007444284856319427
iteration 89, loss = 0.0007886412204243243
iteration 90, loss = 0.0009842464933171868
iteration 91, loss = 0.0010925629176199436
iteration 92, loss = 0.0010249614715576172
iteration 93, loss = 0.0008784189121797681
iteration 94, loss = 0.0011484307469800115
iteration 95, loss = 0.001029524253681302
iteration 96, loss = 0.0011139275738969445
iteration 97, loss = 0.0011590640060603619
iteration 98, loss = 0.0010826410725712776
iteration 99, loss = 0.0012214808957651258
iteration 100, loss = 0.0008984702872112393
iteration 101, loss = 0.0010068760020658374
iteration 102, loss = 0.0017384856473654509
iteration 103, loss = 0.0009140064939856529
iteration 104, loss = 0.0016845514765009284
iteration 105, loss = 0.0010477444157004356
iteration 106, loss = 0.0006570637342520058
iteration 107, loss = 0.0015072266105562449
iteration 108, loss = 0.0009602745994925499
iteration 109, loss = 0.0026072761975228786
iteration 110, loss = 0.0008733242866583169
iteration 111, loss = 0.000622666091658175
iteration 112, loss = 0.0009235229808837175
iteration 113, loss = 0.002028717892244458
iteration 114, loss = 0.0018960214219987392
iteration 115, loss = 0.0011398086789995432
iteration 116, loss = 0.0007418253226205707
iteration 117, loss = 0.001706062932498753
iteration 118, loss = 0.002303210087120533
iteration 119, loss = 0.0008596882107667625
iteration 120, loss = 0.0006688564899377525
iteration 121, loss = 0.0007141317473724484
iteration 122, loss = 0.0009453496313653886
iteration 123, loss = 0.002883612411096692
iteration 124, loss = 0.0007667747559025884
iteration 125, loss = 0.0010511740110814571
iteration 126, loss = 0.0013359406730160117
iteration 127, loss = 0.0020279765594750643
iteration 128, loss = 0.0016851649852469563
iteration 129, loss = 0.0024051249492913485
iteration 130, loss = 0.002456595888361335
iteration 131, loss = 0.0009198245825245976
iteration 132, loss = 0.001087747747078538
iteration 133, loss = 0.0007648331811651587
iteration 134, loss = 0.0008812877349555492
iteration 135, loss = 0.0011098124086856842
iteration 136, loss = 0.0008305439259856939
iteration 137, loss = 0.0008381133084185421
iteration 138, loss = 0.00048773197340779006
iteration 139, loss = 0.0007497857441194355
iteration 140, loss = 0.00092910579405725
iteration 141, loss = 0.0010015284642577171
iteration 142, loss = 0.0007793571567162871
iteration 143, loss = 0.0014925148570910096
iteration 144, loss = 0.002858435269445181
iteration 145, loss = 0.000712449022103101
iteration 146, loss = 0.000877767859492451
iteration 147, loss = 0.0005503864376805723
iteration 148, loss = 0.0007348764920607209
iteration 149, loss = 0.0015774009516462684
iteration 150, loss = 0.0005468750605359674
iteration 151, loss = 0.0010126250563189387
iteration 152, loss = 0.0007991660968400538
iteration 153, loss = 0.0007536773337051272
iteration 154, loss = 0.0006020780419930816
iteration 155, loss = 0.002312220400199294
iteration 156, loss = 0.0009416856919415295
iteration 157, loss = 0.0007529057911597192
iteration 158, loss = 0.0008013448677957058
iteration 159, loss = 0.0011900430545210838
iteration 160, loss = 0.0009111113613471389
iteration 161, loss = 0.0012293951585888863
iteration 162, loss = 0.0009635630995035172
iteration 163, loss = 0.0008118925616145134
iteration 164, loss = 0.0010047937976196408
iteration 165, loss = 0.0007709463825449347
iteration 166, loss = 0.0007852563285268843
iteration 167, loss = 0.0008826267439872026
iteration 168, loss = 0.0007089741993695498
iteration 169, loss = 0.001060547772794962
iteration 170, loss = 0.0014002062380313873
iteration 171, loss = 0.0015909670619294047
iteration 172, loss = 0.0008624275214970112
iteration 173, loss = 0.0007133692852221429
iteration 174, loss = 0.001142729539424181
iteration 175, loss = 0.0017586920876055956
iteration 176, loss = 0.0011307579698041081
iteration 177, loss = 0.0007652573985978961
iteration 178, loss = 0.0012185388477519155
iteration 179, loss = 0.0014357637846842408
iteration 180, loss = 0.0007732111844234169
iteration 181, loss = 0.0008275797590613365
iteration 182, loss = 0.001600846415385604
iteration 183, loss = 0.0012919773580506444
iteration 184, loss = 0.001397491549141705
iteration 185, loss = 0.0024551721289753914
iteration 186, loss = 0.000990776577964425
iteration 187, loss = 0.0011486783623695374
iteration 188, loss = 0.0014513812493532896
iteration 189, loss = 0.0006577990134246647
iteration 190, loss = 0.0005049236351624131
iteration 191, loss = 0.0009212426375597715
iteration 192, loss = 0.0006370117189362645
iteration 193, loss = 0.00180238694883883
iteration 194, loss = 0.001279219170100987
iteration 195, loss = 0.0013963606907054782
iteration 196, loss = 0.001076076878234744
iteration 197, loss = 0.001727489521726966
iteration 198, loss = 0.0016163064865395427
iteration 199, loss = 0.0015604892978444695
iteration 200, loss = 0.0016429187962785363
iteration 201, loss = 0.0008325271774083376
iteration 202, loss = 0.0006730319000780582
iteration 203, loss = 0.001614619162864983
iteration 204, loss = 0.0015054791001603007
iteration 205, loss = 0.0007081749499775469
iteration 206, loss = 0.0007897019968368113
iteration 207, loss = 0.0008361804066225886
iteration 208, loss = 0.0007478986517526209
iteration 209, loss = 0.0008706668158993125
iteration 210, loss = 0.0016286976169794798
iteration 211, loss = 0.000863497203681618
iteration 212, loss = 0.0009111243416555226
iteration 213, loss = 0.000916399760171771
iteration 214, loss = 0.0007101068622432649
iteration 215, loss = 0.0010098495986312628
iteration 216, loss = 0.0010190235916525126
iteration 217, loss = 0.0011074813082814217
iteration 218, loss = 0.0037274183705449104
iteration 219, loss = 0.00085114233661443
iteration 220, loss = 0.0011317796306684613
iteration 221, loss = 0.0024724008981138468
iteration 222, loss = 0.0017940306570380926
iteration 223, loss = 0.0009307398577220738
iteration 224, loss = 0.0011000934755429626
iteration 225, loss = 0.0022840506862848997
iteration 226, loss = 0.0007397402077913284
iteration 227, loss = 0.000799971807282418
iteration 228, loss = 0.001357190776616335
iteration 229, loss = 0.0016200579702854156
iteration 230, loss = 0.0027657162863761187
iteration 231, loss = 0.001117922947742045
iteration 232, loss = 0.0007047505932860076
iteration 233, loss = 0.0005540521815419197
iteration 234, loss = 0.0017838996136561036
iteration 235, loss = 0.0007476582541130483
iteration 236, loss = 0.0017762185307219625
iteration 237, loss = 0.0009691850282251835
iteration 238, loss = 0.0009445743635296822
iteration 239, loss = 0.0013412274420261383
iteration 240, loss = 0.0006225351244211197
iteration 241, loss = 0.0008910574251785874
iteration 242, loss = 0.002152263419702649
iteration 243, loss = 0.0010325245093554258
iteration 244, loss = 0.0005222448380663991
iteration 245, loss = 0.001161901280283928
iteration 246, loss = 0.0005581272998824716
iteration 247, loss = 0.0007673137006349862
iteration 248, loss = 0.0005850010202266276
iteration 249, loss = 0.0009478748543187976
iteration 250, loss = 0.0008909449679777026
iteration 251, loss = 0.000880769919604063
iteration 252, loss = 0.00104970159009099
iteration 253, loss = 0.0009222757071256638
iteration 254, loss = 0.0013465116498991847
iteration 255, loss = 0.0011799621861428022
iteration 256, loss = 0.0009835814125835896
iteration 257, loss = 0.0012973406119272113
iteration 258, loss = 0.0010519773932173848
iteration 259, loss = 0.0006291231838986278
iteration 260, loss = 0.0010693769436329603
iteration 261, loss = 0.0009883727179840207
iteration 262, loss = 0.0012387968599796295
iteration 263, loss = 0.0009402912110090256
iteration 264, loss = 0.0012668222188949585
iteration 265, loss = 0.0020038634538650513
iteration 266, loss = 0.0008927725139074028
iteration 267, loss = 0.0009062211611308157
iteration 268, loss = 0.0007341975579038262
iteration 269, loss = 0.00255010137334466
iteration 270, loss = 0.0008185649057850242
iteration 271, loss = 0.0008982805302366614
iteration 272, loss = 0.0010003825882449746
iteration 273, loss = 0.0010781603632494807
iteration 274, loss = 0.0009144446812570095
iteration 275, loss = 0.0010477341711521149
iteration 276, loss = 0.0022768774069845676
iteration 277, loss = 0.0013265380403026938
iteration 278, loss = 0.00088725914247334
iteration 279, loss = 0.0010426290100440383
iteration 280, loss = 0.0015038741985335946
iteration 281, loss = 0.0010091452859342098
iteration 282, loss = 0.0016003951895982027
iteration 283, loss = 0.0009007976623252034
iteration 284, loss = 0.0008762425277382135
iteration 285, loss = 0.00088530033826828
iteration 286, loss = 0.0010254726512357593
iteration 287, loss = 0.0033127854112535715
iteration 288, loss = 0.0017959126271307468
iteration 289, loss = 0.0006151737179607153
iteration 290, loss = 0.0006968699744902551
iteration 291, loss = 0.000730536412447691
iteration 292, loss = 0.0016436880687251687
iteration 293, loss = 0.0010878702159970999
iteration 294, loss = 0.0007221370469778776
iteration 295, loss = 0.0007224415894597769
iteration 296, loss = 0.0008783950470387936
iteration 297, loss = 0.000907456676941365
iteration 298, loss = 0.0006479209987446666
iteration 299, loss = 0.0009345608414150774
iteration 0, loss = 0.002272428246214986
iteration 1, loss = 0.001218414749018848
iteration 2, loss = 0.0025870336685329676
iteration 3, loss = 0.0007085722172632813
iteration 4, loss = 0.001412701210938394
iteration 5, loss = 0.0007249487098306417
iteration 6, loss = 0.0010719398269429803
iteration 7, loss = 0.0009110851678997278
iteration 8, loss = 0.0009367165039293468
iteration 9, loss = 0.000834606762509793
iteration 10, loss = 0.0006540248868986964
iteration 11, loss = 0.0013522283406928182
iteration 12, loss = 0.000867141701746732
iteration 13, loss = 0.0015358717646449804
iteration 14, loss = 0.0006207172409631312
iteration 15, loss = 0.0005624937475658953
iteration 16, loss = 0.0007566731655970216
iteration 17, loss = 0.0006587757961824536
iteration 18, loss = 0.003478331957012415
iteration 19, loss = 0.0007433532155118883
iteration 20, loss = 0.0008861940586939454
iteration 21, loss = 0.001734359422698617
iteration 22, loss = 0.001156032201834023
iteration 23, loss = 0.0010276747634634376
iteration 24, loss = 0.0009082265314646065
iteration 25, loss = 0.001530520268715918
iteration 26, loss = 0.002056726021692157
iteration 27, loss = 0.0006558328168466687
iteration 28, loss = 0.0005442398833110929
iteration 29, loss = 0.0012028985656797886
iteration 30, loss = 0.0008467515581287444
iteration 31, loss = 0.0006394945667125285
iteration 32, loss = 0.00078553706407547
iteration 33, loss = 0.0007201144471764565
iteration 34, loss = 0.0007957780035212636
iteration 35, loss = 0.0012183076469227672
iteration 36, loss = 0.0005911763291805983
iteration 37, loss = 0.0006603572401218116
iteration 38, loss = 0.000656372751109302
iteration 39, loss = 0.0007828568923287094
iteration 40, loss = 0.0007261436549015343
iteration 41, loss = 0.0009200251661241055
iteration 42, loss = 0.0006229177815839648
iteration 43, loss = 0.0007684752345085144
iteration 44, loss = 0.0010691169882193208
iteration 45, loss = 0.0009877971606329083
iteration 46, loss = 0.0009689950966276228
iteration 47, loss = 0.0006776961963623762
iteration 48, loss = 0.000656791147775948
iteration 49, loss = 0.0007810594397597015
iteration 50, loss = 0.0009307290310971439
iteration 51, loss = 0.0007018160540610552
iteration 52, loss = 0.0009601167985238135
iteration 53, loss = 0.0018447054317221045
iteration 54, loss = 0.0009837038815021515
iteration 55, loss = 0.0016037640161812305
iteration 56, loss = 0.0009869483765214682
iteration 57, loss = 0.0008287912933155894
iteration 58, loss = 0.0014776672469452024
iteration 59, loss = 0.0009337594965472817
iteration 60, loss = 0.0011956680100411177
iteration 61, loss = 0.0007631393964402378
iteration 62, loss = 0.0009096914436668158
iteration 63, loss = 0.0007234616205096245
iteration 64, loss = 0.0006894671823829412
iteration 65, loss = 0.0009905516635626554
iteration 66, loss = 0.0007338556461036205
iteration 67, loss = 0.001027813646942377
iteration 68, loss = 0.0005614836700260639
iteration 69, loss = 0.0009992530103772879
iteration 70, loss = 0.0006665861583314836
iteration 71, loss = 0.0008750313427299261
iteration 72, loss = 0.0012838799739256501
iteration 73, loss = 0.0007471594726666808
iteration 74, loss = 0.0007763743633404374
iteration 75, loss = 0.0006543274503201246
iteration 76, loss = 0.0016581128584221005
iteration 77, loss = 0.0005934150540269911
iteration 78, loss = 0.0005548009648919106
iteration 79, loss = 0.0010826928773894906
iteration 80, loss = 0.001554686576128006
iteration 81, loss = 0.0006669799913652241
iteration 82, loss = 0.0012731854803860188
iteration 83, loss = 0.0005157255800440907
iteration 84, loss = 0.0009856405667960644
iteration 85, loss = 0.0032612355425953865
iteration 86, loss = 0.0005692773847840726
iteration 87, loss = 0.001305083162151277
iteration 88, loss = 0.0013877326855435967
iteration 89, loss = 0.0008576303953304887
iteration 90, loss = 0.0017585748573765159
iteration 91, loss = 0.0026403991505503654
iteration 92, loss = 0.0009474331745877862
iteration 93, loss = 0.0013740882277488708
iteration 94, loss = 0.0012924551265314221
iteration 95, loss = 0.001285847625695169
iteration 96, loss = 0.0020629954524338245
iteration 97, loss = 0.0015001153806224465
iteration 98, loss = 0.001377673470415175
iteration 99, loss = 0.0012127358932048082
iteration 100, loss = 0.0010400393512099981
iteration 101, loss = 0.0016900631599128246
iteration 102, loss = 0.001008561230264604
iteration 103, loss = 0.0014217469142749906
iteration 104, loss = 0.0011110542109236121
iteration 105, loss = 0.0014857277274131775
iteration 106, loss = 0.0005816775956191123
iteration 107, loss = 0.0006499504088424146
iteration 108, loss = 0.0005683420458808541
iteration 109, loss = 0.0007966789999045432
iteration 110, loss = 0.0009535139543004334
iteration 111, loss = 0.0005672717234119773
iteration 112, loss = 0.0006021842709742486
iteration 113, loss = 0.000719140050932765
iteration 114, loss = 0.0010730100329965353
iteration 115, loss = 0.0007267032633535564
iteration 116, loss = 0.0007242591236717999
iteration 117, loss = 0.0008679641759954393
iteration 118, loss = 0.0031987428665161133
iteration 119, loss = 0.0006859513814561069
iteration 120, loss = 0.0012920943554490805
iteration 121, loss = 0.0008989115594886243
iteration 122, loss = 0.001164780929684639
iteration 123, loss = 0.0010618551168590784
iteration 124, loss = 0.0006684166728518903
iteration 125, loss = 0.0007931721047498286
iteration 126, loss = 0.0009278095094487071
iteration 127, loss = 0.0008162068552337587
iteration 128, loss = 0.0005435504717752337
iteration 129, loss = 0.0006801863783039153
iteration 130, loss = 0.0010423415806144476
iteration 131, loss = 0.0007569909212179482
iteration 132, loss = 0.0005904218414798379
iteration 133, loss = 0.0005794107564724982
iteration 134, loss = 0.000515513529535383
iteration 135, loss = 0.0007185100112110376
iteration 136, loss = 0.0008923120331019163
iteration 137, loss = 0.0006988625973463058
iteration 138, loss = 0.0016808341024443507
iteration 139, loss = 0.0012065479531884193
iteration 140, loss = 0.0012645276729017496
iteration 141, loss = 0.000567086273804307
iteration 142, loss = 0.0006298910011537373
iteration 143, loss = 0.001018004259094596
iteration 144, loss = 0.0013811505632475019
iteration 145, loss = 0.0006368512404151261
iteration 146, loss = 0.0009959196904674172
iteration 147, loss = 0.0010260678827762604
iteration 148, loss = 0.0009450839716009796
iteration 149, loss = 0.0006083590560592711
iteration 150, loss = 0.0008945827721618116
iteration 151, loss = 0.0008705202490091324
iteration 152, loss = 0.0008582370355725288
iteration 153, loss = 0.0007598624215461314
iteration 154, loss = 0.0009320087847299874
iteration 155, loss = 0.0007591585745103657
iteration 156, loss = 0.0008441280806437135
iteration 157, loss = 0.00078297354048118
iteration 158, loss = 0.001431528595276177
iteration 159, loss = 0.0010239500552415848
iteration 160, loss = 0.0008482768898829818
iteration 161, loss = 0.0006497970316559076
iteration 162, loss = 0.0024165573995560408
iteration 163, loss = 0.0014466813299804926
iteration 164, loss = 0.0009021267178468406
iteration 165, loss = 0.0014714684803038836
iteration 166, loss = 0.0014437059871852398
iteration 167, loss = 0.0025007633958011866
iteration 168, loss = 0.0007293017697520554
iteration 169, loss = 0.0006906769121997058
iteration 170, loss = 0.002007964299991727
iteration 171, loss = 0.001235143980011344
iteration 172, loss = 0.0006245292024686933
iteration 173, loss = 0.0010207336163148284
iteration 174, loss = 0.0012957548024132848
iteration 175, loss = 0.0013946393737569451
iteration 176, loss = 0.000903474516235292
iteration 177, loss = 0.0007915353635326028
iteration 178, loss = 0.001141563057899475
iteration 179, loss = 0.0004964165273122489
iteration 180, loss = 0.000797754677478224
iteration 181, loss = 0.0026806327514350414
iteration 182, loss = 0.0010868881363421679
iteration 183, loss = 0.001103768008761108
iteration 184, loss = 0.0010964126558974385
iteration 185, loss = 0.0009021635632961988
iteration 186, loss = 0.0014623472234234214
iteration 187, loss = 0.0010906653478741646
iteration 188, loss = 0.0015607040841132402
iteration 189, loss = 0.0007140978705137968
iteration 190, loss = 0.0023754979483783245
iteration 191, loss = 0.0007158046355471015
iteration 192, loss = 0.0007396511500701308
iteration 193, loss = 0.000768918136600405
iteration 194, loss = 0.0008314033038914204
iteration 195, loss = 0.001005343277938664
iteration 196, loss = 0.0012746151769533753
iteration 197, loss = 0.0009257192723453045
iteration 198, loss = 0.0011069877073168755
iteration 199, loss = 0.0011458888184279203
iteration 200, loss = 0.001486745080910623
iteration 201, loss = 0.0021842236164957285
iteration 202, loss = 0.0011054789647459984
iteration 203, loss = 0.000875389261636883
iteration 204, loss = 0.0008327333489432931
iteration 205, loss = 0.0013495394960045815
iteration 206, loss = 0.001049893326126039
iteration 207, loss = 0.0015827418537810445
iteration 208, loss = 0.0011325556552037597
iteration 209, loss = 0.0007895921007730067
iteration 210, loss = 0.0007973501924425364
iteration 211, loss = 0.0006355717196129262
iteration 212, loss = 0.0014472616603597999
iteration 213, loss = 0.001114338869228959
iteration 214, loss = 0.0009508410003036261
iteration 215, loss = 0.002334152814000845
iteration 216, loss = 0.0007845138316042721
iteration 217, loss = 0.0005818824283778667
iteration 218, loss = 0.000540301320143044
iteration 219, loss = 0.0005898362724110484
iteration 220, loss = 0.0015833204379305243
iteration 221, loss = 0.0009370726766064763
iteration 222, loss = 0.0007839163299649954
iteration 223, loss = 0.001028269762173295
iteration 224, loss = 0.0007975538028404117
iteration 225, loss = 0.001036076690070331
iteration 226, loss = 0.00221874937415123
iteration 227, loss = 0.0010299530113115907
iteration 228, loss = 0.002892209915444255
iteration 229, loss = 0.0005242698243819177
iteration 230, loss = 0.0004497435875236988
iteration 231, loss = 0.0015105714555829763
iteration 232, loss = 0.0013874596916139126
iteration 233, loss = 0.0009361005504615605
iteration 234, loss = 0.0005659128655679524
iteration 235, loss = 0.0008490210166200995
iteration 236, loss = 0.000635651231277734
iteration 237, loss = 0.0007942037773318589
iteration 238, loss = 0.0009268609574064612
iteration 239, loss = 0.0009174515726044774
iteration 240, loss = 0.0015630039852112532
iteration 241, loss = 0.0006982628838159144
iteration 242, loss = 0.0007236262899823487
iteration 243, loss = 0.0009761917171999812
iteration 244, loss = 0.0006387330358847976
iteration 245, loss = 0.0025422300677746534
iteration 246, loss = 0.0007338275318033993
iteration 247, loss = 0.0008293017745018005
iteration 248, loss = 0.00047158935922198
iteration 249, loss = 0.0013511828146874905
iteration 250, loss = 0.0013802755856886506
iteration 251, loss = 0.0012166062369942665
iteration 252, loss = 0.0008831243612803519
iteration 253, loss = 0.0007711783400736749
iteration 254, loss = 0.0007448726682923734
iteration 255, loss = 0.0009437183616682887
iteration 256, loss = 0.0017651519738137722
iteration 257, loss = 0.001478569465689361
iteration 258, loss = 0.0007475598249584436
iteration 259, loss = 0.0010813003173097968
iteration 260, loss = 0.001572997192852199
iteration 261, loss = 0.001044172910042107
iteration 262, loss = 0.0006083825137466192
iteration 263, loss = 0.0011922612320631742
iteration 264, loss = 0.0013154172338545322
iteration 265, loss = 0.0007663212018087506
iteration 266, loss = 0.0013525380054488778
iteration 267, loss = 0.0011151422513648868
iteration 268, loss = 0.0012904384639114141
iteration 269, loss = 0.0005827203276567161
iteration 270, loss = 0.0016163558466359973
iteration 271, loss = 0.0012150646653026342
iteration 272, loss = 0.0006413224618881941
iteration 273, loss = 0.0006817882531322539
iteration 274, loss = 0.0010143216932192445
iteration 275, loss = 0.0013266067253425717
iteration 276, loss = 0.0007605527061969042
iteration 277, loss = 0.0008587641641497612
iteration 278, loss = 0.002399108838289976
iteration 279, loss = 0.0010864099022001028
iteration 280, loss = 0.0005728653050027788
iteration 281, loss = 0.000912906660232693
iteration 282, loss = 0.0010964118409901857
iteration 283, loss = 0.0007327974308282137
iteration 284, loss = 0.0005300026969052851
iteration 285, loss = 0.0008203249890357256
iteration 286, loss = 0.0014132820069789886
iteration 287, loss = 0.0006629819399677217
iteration 288, loss = 0.0021436617244035006
iteration 289, loss = 0.0013181004906073213
iteration 290, loss = 0.0012710716109722853
iteration 291, loss = 0.0008921106928028166
iteration 292, loss = 0.000809645454864949
iteration 293, loss = 0.0008843124378472567
iteration 294, loss = 0.0006656518089585006
iteration 295, loss = 0.0009224663954228163
iteration 296, loss = 0.0008240697206929326
iteration 297, loss = 0.0011326808016747236
iteration 298, loss = 0.0009620269411243498
iteration 299, loss = 0.0006807389436289668
iteration 0, loss = 0.0007577021606266499
iteration 1, loss = 0.0007857800810597837
iteration 2, loss = 0.0012334240600466728
iteration 3, loss = 0.0015555019490420818
iteration 4, loss = 0.0006479061557911336
iteration 5, loss = 0.0010725955944508314
iteration 6, loss = 0.0007559953373856843
iteration 7, loss = 0.0009292364120483398
iteration 8, loss = 0.00232755858451128
iteration 9, loss = 0.0016249403124675155
iteration 10, loss = 0.0008664779597893357
iteration 11, loss = 0.0030996128916740417
iteration 12, loss = 0.0016468694666400552
iteration 13, loss = 0.0015729153528809547
iteration 14, loss = 0.000769195903558284
iteration 15, loss = 0.0007720636203885078
iteration 16, loss = 0.0009585536317899823
iteration 17, loss = 0.0006310184835456312
iteration 18, loss = 0.0006499559385702014
iteration 19, loss = 0.0025094097945839167
iteration 20, loss = 0.000973302754573524
iteration 21, loss = 0.0006676215562038124
iteration 22, loss = 0.002240660134702921
iteration 23, loss = 0.0008816084009595215
iteration 24, loss = 0.0007972035091370344
iteration 25, loss = 0.0010704564629122615
iteration 26, loss = 0.0010581397218629718
iteration 27, loss = 0.0009258225909434259
iteration 28, loss = 0.0006255607586354017
iteration 29, loss = 0.0006895985570736229
iteration 30, loss = 0.001286409329622984
iteration 31, loss = 0.00061379699036479
iteration 32, loss = 0.0009952109539881349
iteration 33, loss = 0.000681999372318387
iteration 34, loss = 0.0018479127902537584
iteration 35, loss = 0.001269474858418107
iteration 36, loss = 0.0007720457506366074
iteration 37, loss = 0.0014441091334447265
iteration 38, loss = 0.0007286968757398427
iteration 39, loss = 0.0010386176872998476
iteration 40, loss = 0.0006811465718783438
iteration 41, loss = 0.0010502535151317716
iteration 42, loss = 0.0006932635442353785
iteration 43, loss = 0.000860273779835552
iteration 44, loss = 0.0006170581327751279
iteration 45, loss = 0.001154806581325829
iteration 46, loss = 0.0007070361752994359
iteration 47, loss = 0.0006388809997588396
iteration 48, loss = 0.001227700151503086
iteration 49, loss = 0.0007209064206108451
iteration 50, loss = 0.000696516886819154
iteration 51, loss = 0.0009115309221670032
iteration 52, loss = 0.0005749372649006546
iteration 53, loss = 0.0011550068156793714
iteration 54, loss = 0.0006030985969118774
iteration 55, loss = 0.0010096461046487093
iteration 56, loss = 0.0007553994073532522
iteration 57, loss = 0.0011392930755391717
iteration 58, loss = 0.0008696772856637836
iteration 59, loss = 0.0020779722835868597
iteration 60, loss = 0.0021559183951467276
iteration 61, loss = 0.0006746301660314202
iteration 62, loss = 0.0007987373392097652
iteration 63, loss = 0.0009553247364237905
iteration 64, loss = 0.0008565604803152382
iteration 65, loss = 0.0009087470243684947
iteration 66, loss = 0.0007812397088855505
iteration 67, loss = 0.002453030087053776
iteration 68, loss = 0.0006864758906885982
iteration 69, loss = 0.001840370474383235
iteration 70, loss = 0.0004930674913339317
iteration 71, loss = 0.002462938195094466
iteration 72, loss = 0.0021244408562779427
iteration 73, loss = 0.0009695607586763799
iteration 74, loss = 0.0008868718286976218
iteration 75, loss = 0.0007124319090507925
iteration 76, loss = 0.0009865808533504605
iteration 77, loss = 0.0005670263199135661
iteration 78, loss = 0.0010619553504511714
iteration 79, loss = 0.0008535225642845035
iteration 80, loss = 0.0009287529392167926
iteration 81, loss = 0.0006084279739297926
iteration 82, loss = 0.0009726565913297236
iteration 83, loss = 0.0010633463971316814
iteration 84, loss = 0.00080057920422405
iteration 85, loss = 0.0007382351323030889
iteration 86, loss = 0.0012213749578222632
iteration 87, loss = 0.0011862576939165592
iteration 88, loss = 0.0007643779390491545
iteration 89, loss = 0.0015512940008193254
iteration 90, loss = 0.0009293801849707961
iteration 91, loss = 0.0007895383168943226
iteration 92, loss = 0.000985515071079135
iteration 93, loss = 0.0006964993081055582
iteration 94, loss = 0.0011942104902118444
iteration 95, loss = 0.0012256851186975837
iteration 96, loss = 0.001271584304049611
iteration 97, loss = 0.0007720078574493527
iteration 98, loss = 0.0006095455610193312
iteration 99, loss = 0.0016712775686755776
iteration 100, loss = 0.0018857725663110614
iteration 101, loss = 0.0010304450988769531
iteration 102, loss = 0.000539394561201334
iteration 103, loss = 0.001141043845564127
iteration 104, loss = 0.0005232134135439992
iteration 105, loss = 0.0013507581315934658
iteration 106, loss = 0.0006759506068192422
iteration 107, loss = 0.0005624317564070225
iteration 108, loss = 0.0012406735913828015
iteration 109, loss = 0.0011376244947314262
iteration 110, loss = 0.000823239388410002
iteration 111, loss = 0.0012052964884787798
iteration 112, loss = 0.000552141631487757
iteration 113, loss = 0.0005244857165962458
iteration 114, loss = 0.0005822556558996439
iteration 115, loss = 0.0007519355276599526
iteration 116, loss = 0.0006629530107602477
iteration 117, loss = 0.0011282966006547213
iteration 118, loss = 0.0008151468937285244
iteration 119, loss = 0.00172788230702281
iteration 120, loss = 0.00106358143966645
iteration 121, loss = 0.0009653383167460561
iteration 122, loss = 0.0007989471196196973
iteration 123, loss = 0.0007007503882050514
iteration 124, loss = 0.0014288356760516763
iteration 125, loss = 0.0004604353744070977
iteration 126, loss = 0.001400381326675415
iteration 127, loss = 0.0005438351072371006
iteration 128, loss = 0.0006519997259601951
iteration 129, loss = 0.0006090033566579223
iteration 130, loss = 0.0005695882136933506
iteration 131, loss = 0.0006813332438468933
iteration 132, loss = 0.0008065123693086207
iteration 133, loss = 0.0005632668035104871
iteration 134, loss = 0.0007809428498148918
iteration 135, loss = 0.0012371690245345235
iteration 136, loss = 0.0014143349835649133
iteration 137, loss = 0.0007092340383678675
iteration 138, loss = 0.0007737788837403059
iteration 139, loss = 0.00116050336509943
iteration 140, loss = 0.0012934365076944232
iteration 141, loss = 0.000938235898502171
iteration 142, loss = 0.0010225384030491114
iteration 143, loss = 0.000692354456987232
iteration 144, loss = 0.001206834800541401
iteration 145, loss = 0.0007589540909975767
iteration 146, loss = 0.0008030400495044887
iteration 147, loss = 0.000970736495219171
iteration 148, loss = 0.001043651602230966
iteration 149, loss = 0.00048174953553825617
iteration 150, loss = 0.0010645308066159487
iteration 151, loss = 0.0006846126052550972
iteration 152, loss = 0.0009257657802663743
iteration 153, loss = 0.0012822162825614214
iteration 154, loss = 0.000790732039604336
iteration 155, loss = 0.00105083710514009
iteration 156, loss = 0.0010329261422157288
iteration 157, loss = 0.0007408614619635046
iteration 158, loss = 0.0005054324865341187
iteration 159, loss = 0.000855176302138716
iteration 160, loss = 0.0007616776274517179
iteration 161, loss = 0.000830696546472609
iteration 162, loss = 0.0006482757744379342
iteration 163, loss = 0.0006018687854520977
iteration 164, loss = 0.0005587227060459554
iteration 165, loss = 0.0015425337478518486
iteration 166, loss = 0.000938013254199177
iteration 167, loss = 0.0006387854227796197
iteration 168, loss = 0.0017277824226766825
iteration 169, loss = 0.0006988316308706999
iteration 170, loss = 0.0010058411862701178
iteration 171, loss = 0.00098401156719774
iteration 172, loss = 0.001772549469023943
iteration 173, loss = 0.0006238746573217213
iteration 174, loss = 0.0011880025267601013
iteration 175, loss = 0.0007370678940787911
iteration 176, loss = 0.0005471787299029529
iteration 177, loss = 0.0005004141130484641
iteration 178, loss = 0.0009313896298408508
iteration 179, loss = 0.0007307952619157732
iteration 180, loss = 0.0010406359797343612
iteration 181, loss = 0.0007290131761692464
iteration 182, loss = 0.0011952196946367621
iteration 183, loss = 0.0006717026699334383
iteration 184, loss = 0.0008626189664937556
iteration 185, loss = 0.001131138182245195
iteration 186, loss = 0.0014157688710838556
iteration 187, loss = 0.0007760137668810785
iteration 188, loss = 0.0012634492013603449
iteration 189, loss = 0.0006155003211461008
iteration 190, loss = 0.0006770039908587933
iteration 191, loss = 0.0004903881344944239
iteration 192, loss = 0.0024119156878441572
iteration 193, loss = 0.0010285971220582724
iteration 194, loss = 0.0007592707988806069
iteration 195, loss = 0.0011881958926096559
iteration 196, loss = 0.001296780537813902
iteration 197, loss = 0.0006412734510377049
iteration 198, loss = 0.0008109509362839162
iteration 199, loss = 0.0005793253658339381
iteration 200, loss = 0.0006361262057907879
iteration 201, loss = 0.0010696669341996312
iteration 202, loss = 0.0007722426671534777
iteration 203, loss = 0.0011150483042001724
iteration 204, loss = 0.000659075565636158
iteration 205, loss = 0.001043266849592328
iteration 206, loss = 0.0009172842837870121
iteration 207, loss = 0.0009507649810984731
iteration 208, loss = 0.0013826696667820215
iteration 209, loss = 0.0009391634957864881
iteration 210, loss = 0.0011618102435022593
iteration 211, loss = 0.0005160808213986456
iteration 212, loss = 0.00236709788441658
iteration 213, loss = 0.0009455831022933125
iteration 214, loss = 0.0010536377085372806
iteration 215, loss = 0.0007867244421504438
iteration 216, loss = 0.0008043462294153869
iteration 217, loss = 0.001047205412760377
iteration 218, loss = 0.000770054932218045
iteration 219, loss = 0.0005776274483650923
iteration 220, loss = 0.0008819667855277658
iteration 221, loss = 0.0007086440455168486
iteration 222, loss = 0.000575567246414721
iteration 223, loss = 0.0016502111684530973
iteration 224, loss = 0.001024074386805296
iteration 225, loss = 0.0005601817974820733
iteration 226, loss = 0.0007498800987377763
iteration 227, loss = 0.0007572637987323105
iteration 228, loss = 0.0006514318520203233
iteration 229, loss = 0.000837777741253376
iteration 230, loss = 0.0024543378967791796
iteration 231, loss = 0.000720253330655396
iteration 232, loss = 0.000756441499106586
iteration 233, loss = 0.000947925669606775
iteration 234, loss = 0.0024735433980822563
iteration 235, loss = 0.0012092560064047575
iteration 236, loss = 0.0007438361644744873
iteration 237, loss = 0.0008445089915767312
iteration 238, loss = 0.0012939968146383762
iteration 239, loss = 0.001194835640490055
iteration 240, loss = 0.0014327337266877294
iteration 241, loss = 0.0008087268797680736
iteration 242, loss = 0.0009405790478922427
iteration 243, loss = 0.0007643022108823061
iteration 244, loss = 0.0012914700200781226
iteration 245, loss = 0.0006807807949371636
iteration 246, loss = 0.0019213262712582946
iteration 247, loss = 0.0006126638036221266
iteration 248, loss = 0.0006890364456921816
iteration 249, loss = 0.0006323934649117291
iteration 250, loss = 0.002422505524009466
iteration 251, loss = 0.0010617900406941772
iteration 252, loss = 0.0022157663479447365
iteration 253, loss = 0.0013881747145205736
iteration 254, loss = 0.00048479504766874015
iteration 255, loss = 0.0009703593095764518
iteration 256, loss = 0.0006792309577576816
iteration 257, loss = 0.0005309407133609056
iteration 258, loss = 0.0007883939542807639
iteration 259, loss = 0.0009618225158192217
iteration 260, loss = 0.0008754916489124298
iteration 261, loss = 0.0007496947655454278
iteration 262, loss = 0.0007939243223518133
iteration 263, loss = 0.0009826845489442348
iteration 264, loss = 0.0007358911680057645
iteration 265, loss = 0.0008006890420801938
iteration 266, loss = 0.0009670868748798966
iteration 267, loss = 0.0015691795852035284
iteration 268, loss = 0.0010894867591559887
iteration 269, loss = 0.0008384212851524353
iteration 270, loss = 0.0005622481112368405
iteration 271, loss = 0.0012358117382973433
iteration 272, loss = 0.001734233577735722
iteration 273, loss = 0.000938661687541753
iteration 274, loss = 0.000801252550445497
iteration 275, loss = 0.0012220201315358281
iteration 276, loss = 0.0005209374940022826
iteration 277, loss = 0.001275231596082449
iteration 278, loss = 0.0008562039001844823
iteration 279, loss = 0.0006944690831005573
iteration 280, loss = 0.0005616010748781264
iteration 281, loss = 0.0010332980891689658
iteration 282, loss = 0.0005172074306756258
iteration 283, loss = 0.0013412354746833444
iteration 284, loss = 0.001040344825014472
iteration 285, loss = 0.0008371990406885743
iteration 286, loss = 0.0007182287517935038
iteration 287, loss = 0.0008192809182219207
iteration 288, loss = 0.0004885821836069226
iteration 289, loss = 0.0007673882646486163
iteration 290, loss = 0.0006836694665253162
iteration 291, loss = 0.0004483537923078984
iteration 292, loss = 0.0006502869073301554
iteration 293, loss = 0.0007440451881848276
iteration 294, loss = 0.0008617654675617814
iteration 295, loss = 0.0013614733470603824
iteration 296, loss = 0.0020145622547715902
iteration 297, loss = 0.0022159793879836798
iteration 298, loss = 0.0005777455517090857
iteration 299, loss = 0.0007845060899853706
iteration 0, loss = 0.0009388519101776183
iteration 1, loss = 0.0021091001108288765
iteration 2, loss = 0.0007186835864558816
iteration 3, loss = 0.0007305220351554453
iteration 4, loss = 0.0023261590395122766
iteration 5, loss = 0.0007380323368124664
iteration 6, loss = 0.000863926368765533
iteration 7, loss = 0.002035195007920265
iteration 8, loss = 0.001120392233133316
iteration 9, loss = 0.0020908324513584375
iteration 10, loss = 0.00109420798253268
iteration 11, loss = 0.0005225046188570559
iteration 12, loss = 0.001435373560525477
iteration 13, loss = 0.0006234406027942896
iteration 14, loss = 0.0007619558018632233
iteration 15, loss = 0.0007261040736921132
iteration 16, loss = 0.0020791476126760244
iteration 17, loss = 0.0010872117709368467
iteration 18, loss = 0.002948717214167118
iteration 19, loss = 0.0012399281840771437
iteration 20, loss = 0.0007169223972596228
iteration 21, loss = 0.0012971075484529138
iteration 22, loss = 0.001049272483214736
iteration 23, loss = 0.0014230723027139902
iteration 24, loss = 0.0007656221278011799
iteration 25, loss = 0.0007904159720055759
iteration 26, loss = 0.0006894449470564723
iteration 27, loss = 0.0005938296089880168
iteration 28, loss = 0.0006065182387828827
iteration 29, loss = 0.0006323374691419303
iteration 30, loss = 0.001064556185156107
iteration 31, loss = 0.0009702668176032603
iteration 32, loss = 0.002373156836256385
iteration 33, loss = 0.0006778738461434841
iteration 34, loss = 0.0014662245521321893
iteration 35, loss = 0.000911769806407392
iteration 36, loss = 0.0008302920032292604
iteration 37, loss = 0.0006135114235803485
iteration 38, loss = 0.001065347227267921
iteration 39, loss = 0.000876838166732341
iteration 40, loss = 0.0007382199401035905
iteration 41, loss = 0.00047872960567474365
iteration 42, loss = 0.000632336363196373
iteration 43, loss = 0.0008371215080842376
iteration 44, loss = 0.0007550913142040372
iteration 45, loss = 0.0009994118008762598
iteration 46, loss = 0.0016086087562143803
iteration 47, loss = 0.0010304385796189308
iteration 48, loss = 0.0008621994638815522
iteration 49, loss = 0.0006263731047511101
iteration 50, loss = 0.000859603751450777
iteration 51, loss = 0.0008526204619556665
iteration 52, loss = 0.000774682906921953
iteration 53, loss = 0.0007693825755268335
iteration 54, loss = 0.0006710521411150694
iteration 55, loss = 0.0005748628173023462
iteration 56, loss = 0.0009274451876990497
iteration 57, loss = 0.0007468878175131977
iteration 58, loss = 0.0008069764589890838
iteration 59, loss = 0.001066433498635888
iteration 60, loss = 0.0006450936198234558
iteration 61, loss = 0.0008521886193193495
iteration 62, loss = 0.0005724941147491336
iteration 63, loss = 0.0005684426287189126
iteration 64, loss = 0.0011482913978397846
iteration 65, loss = 0.0007899401825852692
iteration 66, loss = 0.0005227788351476192
iteration 67, loss = 0.00043782772263512015
iteration 68, loss = 0.0011278190650045872
iteration 69, loss = 0.0009084166958928108
iteration 70, loss = 0.0004321191809140146
iteration 71, loss = 0.0007340520969592035
iteration 72, loss = 0.0025317424442619085
iteration 73, loss = 0.0012058172142133117
iteration 74, loss = 0.0016021926421672106
iteration 75, loss = 0.0007166672148741782
iteration 76, loss = 0.001102021662518382
iteration 77, loss = 0.001399009837768972
iteration 78, loss = 0.0006213648011907935
iteration 79, loss = 0.0007151082390919328
iteration 80, loss = 0.000608834670856595
iteration 81, loss = 0.0013880041660740972
iteration 82, loss = 0.0004787064390257001
iteration 83, loss = 0.0013449506368488073
iteration 84, loss = 0.0008084046421572566
iteration 85, loss = 0.0013441200135275722
iteration 86, loss = 0.0006428467459045351
iteration 87, loss = 0.0009007969056256115
iteration 88, loss = 0.0008022086694836617
iteration 89, loss = 0.0007513539749197662
iteration 90, loss = 0.0008606664487160742
iteration 91, loss = 0.0007890433189459145
iteration 92, loss = 0.0007945059332996607
iteration 93, loss = 0.002037148457020521
iteration 94, loss = 0.0004365975910332054
iteration 95, loss = 0.0007530577713623643
iteration 96, loss = 0.0015039267018437386
iteration 97, loss = 0.0015000217827036977
iteration 98, loss = 0.0007711501093581319
iteration 99, loss = 0.0021604953799396753
iteration 100, loss = 0.0017614917596802115
iteration 101, loss = 0.0008283280767500401
iteration 102, loss = 0.0012277377536520362
iteration 103, loss = 0.0008042288245633245
iteration 104, loss = 0.0013158777728676796
iteration 105, loss = 0.0005197363207116723
iteration 106, loss = 0.0012806743616238236
iteration 107, loss = 0.0007365563651546836
iteration 108, loss = 0.0009560486651025712
iteration 109, loss = 0.0007661362760700285
iteration 110, loss = 0.0009137351298704743
iteration 111, loss = 0.0007618491654284298
iteration 112, loss = 0.0005074701039120555
iteration 113, loss = 0.0007219198159873486
iteration 114, loss = 0.0008539974223822355
iteration 115, loss = 0.00104550423566252
iteration 116, loss = 0.0006381693528965116
iteration 117, loss = 0.0005638084840029478
iteration 118, loss = 0.0019109895220026374
iteration 119, loss = 0.0006055455305613577
iteration 120, loss = 0.0008202913450077176
iteration 121, loss = 0.0007277692784555256
iteration 122, loss = 0.0008711025584489107
iteration 123, loss = 0.0013572489842772484
iteration 124, loss = 0.0007549417205154896
iteration 125, loss = 0.0008729906985536218
iteration 126, loss = 0.0006988446111790836
iteration 127, loss = 0.0008221859461627901
iteration 128, loss = 0.001192074385471642
iteration 129, loss = 0.0005588140920735896
iteration 130, loss = 0.0009802167769521475
iteration 131, loss = 0.0008556035463698208
iteration 132, loss = 0.0009820774430409074
iteration 133, loss = 0.0006463351892307401
iteration 134, loss = 0.0007487587281502783
iteration 135, loss = 0.0008183341124095023
iteration 136, loss = 0.0007592816837131977
iteration 137, loss = 0.000856004364322871
iteration 138, loss = 0.0015840926207602024
iteration 139, loss = 0.0008171763038262725
iteration 140, loss = 0.0005761929205618799
iteration 141, loss = 0.0009459916036576033
iteration 142, loss = 0.0006442698650062084
iteration 143, loss = 0.0015717610949650407
iteration 144, loss = 0.0005579195567406714
iteration 145, loss = 0.0010294308885931969
iteration 146, loss = 0.0013597498182207346
iteration 147, loss = 0.00048586566117592156
iteration 148, loss = 0.0014351005665957928
iteration 149, loss = 0.0007381390896625817
iteration 150, loss = 0.0012905108742415905
iteration 151, loss = 0.0005277700838632882
iteration 152, loss = 0.0006156157469376922
iteration 153, loss = 0.0004774063709191978
iteration 154, loss = 0.000560521672014147
iteration 155, loss = 0.0013760175788775086
iteration 156, loss = 0.001775751356035471
iteration 157, loss = 0.0008994081290438771
iteration 158, loss = 0.0006819279515184462
iteration 159, loss = 0.0010810934472829103
iteration 160, loss = 0.0005334109882824123
iteration 161, loss = 0.0013110617874190211
iteration 162, loss = 0.0007785410853102803
iteration 163, loss = 0.0005750290583819151
iteration 164, loss = 0.0021093501709401608
iteration 165, loss = 0.001159972744062543
iteration 166, loss = 0.0008520829142071307
iteration 167, loss = 0.0011085947044193745
iteration 168, loss = 0.0008347303955815732
iteration 169, loss = 0.0006948068621568382
iteration 170, loss = 0.0009791312040761113
iteration 171, loss = 0.0009705373668111861
iteration 172, loss = 0.0010707398178055882
iteration 173, loss = 0.0008468154701404274
iteration 174, loss = 0.0005532827344723046
iteration 175, loss = 0.000666755368001759
iteration 176, loss = 0.0006435635732486844
iteration 177, loss = 0.0008896690560504794
iteration 178, loss = 0.0005107346223667264
iteration 179, loss = 0.0012276994530111551
iteration 180, loss = 0.0006043201428838074
iteration 181, loss = 0.00048804745892994106
iteration 182, loss = 0.0005945004522800446
iteration 183, loss = 0.0011787374969571829
iteration 184, loss = 0.0010453325230628252
iteration 185, loss = 0.0006545186042785645
iteration 186, loss = 0.0010873400606215
iteration 187, loss = 0.0007900441996753216
iteration 188, loss = 0.0006354162469506264
iteration 189, loss = 0.0006813328946009278
iteration 190, loss = 0.0004620881227310747
iteration 191, loss = 0.0007607861771248281
iteration 192, loss = 0.0007697271648794413
iteration 193, loss = 0.0013936065370216966
iteration 194, loss = 0.001111667137593031
iteration 195, loss = 0.0007581952377222478
iteration 196, loss = 0.0007698994595557451
iteration 197, loss = 0.0008293839637190104
iteration 198, loss = 0.0007124582771211863
iteration 199, loss = 0.0010460165794938803
iteration 200, loss = 0.001065317657776177
iteration 201, loss = 0.002064803382381797
iteration 202, loss = 0.0008273344137705863
iteration 203, loss = 0.002247360534965992
iteration 204, loss = 0.001553255831822753
iteration 205, loss = 0.0006250088335946202
iteration 206, loss = 0.0009314361377619207
iteration 207, loss = 0.0005812191520817578
iteration 208, loss = 0.0007953664753586054
iteration 209, loss = 0.0011434926418587565
iteration 210, loss = 0.000900514074601233
iteration 211, loss = 0.000640261045191437
iteration 212, loss = 0.000927408691495657
iteration 213, loss = 0.0022623909171670675
iteration 214, loss = 0.0010322333546355367
iteration 215, loss = 0.0006021824665367603
iteration 216, loss = 0.0011542851570993662
iteration 217, loss = 0.0009905004408210516
iteration 218, loss = 0.0012010809732601047
iteration 219, loss = 0.0005340821226127446
iteration 220, loss = 0.0007778541184961796
iteration 221, loss = 0.0010913795558735728
iteration 222, loss = 0.0020831830333918333
iteration 223, loss = 0.0005644499906338751
iteration 224, loss = 0.0004743935423903167
iteration 225, loss = 0.0020327267702668905
iteration 226, loss = 0.0006867431220598519
iteration 227, loss = 0.000551994948182255
iteration 228, loss = 0.0007111310260370374
iteration 229, loss = 0.0013715403620153666
iteration 230, loss = 0.0009421305148862302
iteration 231, loss = 0.0005821338854730129
iteration 232, loss = 0.0009400172275491059
iteration 233, loss = 0.0006978220189921558
iteration 234, loss = 0.0005901642143726349
iteration 235, loss = 0.000773435749579221
iteration 236, loss = 0.00040586083196103573
iteration 237, loss = 0.0006734004709869623
iteration 238, loss = 0.0005593412788584828
iteration 239, loss = 0.0005173776880837977
iteration 240, loss = 0.0008544896845705807
iteration 241, loss = 0.0012717463541775942
iteration 242, loss = 0.0005623857141472399
iteration 243, loss = 0.001161185558885336
iteration 244, loss = 0.0009317912627011538
iteration 245, loss = 0.0005054370267316699
iteration 246, loss = 0.0005397190689109266
iteration 247, loss = 0.0006611241260543466
iteration 248, loss = 0.0007219535182230175
iteration 249, loss = 0.0004954293835908175
iteration 250, loss = 0.00048602602328173816
iteration 251, loss = 0.0006682519451715052
iteration 252, loss = 0.0005758634069934487
iteration 253, loss = 0.000842137320432812
iteration 254, loss = 0.002347164321690798
iteration 255, loss = 0.0006261313101276755
iteration 256, loss = 0.0010130363516509533
iteration 257, loss = 0.0008894386701285839
iteration 258, loss = 0.0006013401434756815
iteration 259, loss = 0.0006086902576498687
iteration 260, loss = 0.0009063362376764417
iteration 261, loss = 0.000653355848044157
iteration 262, loss = 0.00048690426046960056
iteration 263, loss = 0.0009656951879151165
iteration 264, loss = 0.0006585255614481866
iteration 265, loss = 0.0007730311481282115
iteration 266, loss = 0.0007259484846144915
iteration 267, loss = 0.001178909675218165
iteration 268, loss = 0.0014240714954212308
iteration 269, loss = 0.00042158958967775106
iteration 270, loss = 0.0007844844367355108
iteration 271, loss = 0.0006942139589227736
iteration 272, loss = 0.0007717764237895608
iteration 273, loss = 0.0008469151216559112
iteration 274, loss = 0.0005239676684141159
iteration 275, loss = 0.0007448047399520874
iteration 276, loss = 0.0005317803006619215
iteration 277, loss = 0.0007425759104080498
iteration 278, loss = 0.0008016523206606507
iteration 279, loss = 0.0021797544322907925
iteration 280, loss = 0.0005535347736440599
iteration 281, loss = 0.0014270689571276307
iteration 282, loss = 0.0013551585143432021
iteration 283, loss = 0.0007380573078989983
iteration 284, loss = 0.0008623776957392693
iteration 285, loss = 0.0009052171953953803
iteration 286, loss = 0.0006996557349339128
iteration 287, loss = 0.0008201415184885263
iteration 288, loss = 0.0007334665860980749
iteration 289, loss = 0.0007441921625286341
iteration 290, loss = 0.0006850367644801736
iteration 291, loss = 0.0006922265165485442
iteration 292, loss = 0.001200745115056634
iteration 293, loss = 0.0006264125695452094
iteration 294, loss = 0.0006908642826601863
iteration 295, loss = 0.0008354252204298973
iteration 296, loss = 0.0005747668328694999
iteration 297, loss = 0.001110500656068325
iteration 298, loss = 0.0006966894143261015
iteration 299, loss = 0.0008497057715430856
iteration 0, loss = 0.0005835008341819048
iteration 1, loss = 0.0009791830088943243
iteration 2, loss = 0.0012758707161992788
iteration 3, loss = 0.0021563400514423847
iteration 4, loss = 0.00048813456669449806
iteration 5, loss = 0.0007690281490795314
iteration 6, loss = 0.0005660398164764047
iteration 7, loss = 0.0013128555146977305
iteration 8, loss = 0.0007697270484641194
iteration 9, loss = 0.00046183192171156406
iteration 10, loss = 0.0005395784974098206
iteration 11, loss = 0.000472374027594924
iteration 12, loss = 0.000813610851764679
iteration 13, loss = 0.0008255873690359294
iteration 14, loss = 0.0007314964896067977
iteration 15, loss = 0.0005677829030901194
iteration 16, loss = 0.0011986546451225877
iteration 17, loss = 0.0004970236914232373
iteration 18, loss = 0.0006203630473464727
iteration 19, loss = 0.0007680846028961241
iteration 20, loss = 0.0006509821978397667
iteration 21, loss = 0.0008864501724019647
iteration 22, loss = 0.0009932180400937796
iteration 23, loss = 0.0008768076077103615
iteration 24, loss = 0.0010813571279868484
iteration 25, loss = 0.0008044120040722191
iteration 26, loss = 0.0005084635922685266
iteration 27, loss = 0.00105671479832381
iteration 28, loss = 0.001433790777809918
iteration 29, loss = 0.0005757480976171792
iteration 30, loss = 0.0020240868907421827
iteration 31, loss = 0.0012323124101385474
iteration 32, loss = 0.0008444656850770116
iteration 33, loss = 0.00046710940659977496
iteration 34, loss = 0.000977253308519721
iteration 35, loss = 0.0007950142025947571
iteration 36, loss = 0.002270993310958147
iteration 37, loss = 0.0011597506236284971
iteration 38, loss = 0.0011056101648136973
iteration 39, loss = 0.0005708481185138226
iteration 40, loss = 0.001361725153401494
iteration 41, loss = 0.0006701796082779765
iteration 42, loss = 0.0005535362288355827
iteration 43, loss = 0.000990897067822516
iteration 44, loss = 0.0008925475995056331
iteration 45, loss = 0.0013582917163148522
iteration 46, loss = 0.0005478099919855595
iteration 47, loss = 0.0009898022981360555
iteration 48, loss = 0.0006322305998764932
iteration 49, loss = 0.0006899453583173454
iteration 50, loss = 0.0010925015667453408
iteration 51, loss = 0.0008798581548035145
iteration 52, loss = 0.0009634127491153777
iteration 53, loss = 0.0006289494340308011
iteration 54, loss = 0.0009696939378045499
iteration 55, loss = 0.00048783753300085664
iteration 56, loss = 0.0006174656446091831
iteration 57, loss = 0.0015057040145620704
iteration 58, loss = 0.0010520101059228182
iteration 59, loss = 0.000837193161714822
iteration 60, loss = 0.0005817119963467121
iteration 61, loss = 0.0006305365823209286
iteration 62, loss = 0.0012471262598410249
iteration 63, loss = 0.0006088328082114458
iteration 64, loss = 0.0007416228763759136
iteration 65, loss = 0.0005748309777118266
iteration 66, loss = 0.001233294722624123
iteration 67, loss = 0.0007831460679881275
iteration 68, loss = 0.0007186614675447345
iteration 69, loss = 0.0005636587738990784
iteration 70, loss = 0.0005564763559959829
iteration 71, loss = 0.0006974501302465796
iteration 72, loss = 0.0005823848769068718
iteration 73, loss = 0.0006688483990728855
iteration 74, loss = 0.001086871256120503
iteration 75, loss = 0.0006182727520354092
iteration 76, loss = 0.0016720762941986322
iteration 77, loss = 0.0013910331763327122
iteration 78, loss = 0.0008249125094152987
iteration 79, loss = 0.0009022776503115892
iteration 80, loss = 0.0006749893655069172
iteration 81, loss = 0.0007619400857947767
iteration 82, loss = 0.002608473179861903
iteration 83, loss = 0.0007495658937841654
iteration 84, loss = 0.0005672089755535126
iteration 85, loss = 0.0006208954146131873
iteration 86, loss = 0.0007781338645145297
iteration 87, loss = 0.0006571211852133274
iteration 88, loss = 0.0015824967995285988
iteration 89, loss = 0.0006942561594769359
iteration 90, loss = 0.0008115267264656723
iteration 91, loss = 0.0011216964339837432
iteration 92, loss = 0.0008985013701021671
iteration 93, loss = 0.0005556867690756917
iteration 94, loss = 0.001330434693954885
iteration 95, loss = 0.0007070992141962051
iteration 96, loss = 0.0007642167620360851
iteration 97, loss = 0.0006311472388915718
iteration 98, loss = 0.0018890714272856712
iteration 99, loss = 0.0009991468396037817
iteration 100, loss = 0.0007282410515472293
iteration 101, loss = 0.0004109138099011034
iteration 102, loss = 0.0005918286624364555
iteration 103, loss = 0.0004396618460305035
iteration 104, loss = 0.0007964048418216407
iteration 105, loss = 0.0011042151600122452
iteration 106, loss = 0.0009532903204672039
iteration 107, loss = 0.0007956903427839279
iteration 108, loss = 0.0007331672823056579
iteration 109, loss = 0.0007039192132651806
iteration 110, loss = 0.00045212951954454184
iteration 111, loss = 0.0006125577492639422
iteration 112, loss = 0.0005242634215392172
iteration 113, loss = 0.0012141534825786948
iteration 114, loss = 0.0006914319819770753
iteration 115, loss = 0.0007586334249936044
iteration 116, loss = 0.000525252369698137
iteration 117, loss = 0.0020212989766150713
iteration 118, loss = 0.0006431304500438273
iteration 119, loss = 0.0016281964490190148
iteration 120, loss = 0.0003427614865358919
iteration 121, loss = 0.0005923899589106441
iteration 122, loss = 0.0006768402527086437
iteration 123, loss = 0.0008133999072015285
iteration 124, loss = 0.001369390869513154
iteration 125, loss = 0.0005471831536851823
iteration 126, loss = 0.001095745712518692
iteration 127, loss = 0.0007056151516735554
iteration 128, loss = 0.0006708638975396752
iteration 129, loss = 0.0006514396518468857
iteration 130, loss = 0.0006107918452471495
iteration 131, loss = 0.0006930659292265773
iteration 132, loss = 0.000708404986653477
iteration 133, loss = 0.0007482666405849159
iteration 134, loss = 0.0006929698865860701
iteration 135, loss = 0.0009400878916494548
iteration 136, loss = 0.0005918523529544473
iteration 137, loss = 0.00048810194130055606
iteration 138, loss = 0.0009248549467884004
iteration 139, loss = 0.0009384725126437843
iteration 140, loss = 0.00037714658537879586
iteration 141, loss = 0.0006880085566081107
iteration 142, loss = 0.0006403651786968112
iteration 143, loss = 0.0012191066052764654
iteration 144, loss = 0.0008859080844558775
iteration 145, loss = 0.0008264319039881229
iteration 146, loss = 0.0012588136596605182
iteration 147, loss = 0.0005268788663670421
iteration 148, loss = 0.00043075127177871764
iteration 149, loss = 0.0007828493835404515
iteration 150, loss = 0.0006384698208421469
iteration 151, loss = 0.001105050672776997
iteration 152, loss = 0.0011398032074794173
iteration 153, loss = 0.0008968813926912844
iteration 154, loss = 0.0020767657551914454
iteration 155, loss = 0.0005997818661853671
iteration 156, loss = 0.0010610842145979404
iteration 157, loss = 0.0012232983717694879
iteration 158, loss = 0.0005141693982295692
iteration 159, loss = 0.0005344215896911919
iteration 160, loss = 0.00045928091276437044
iteration 161, loss = 0.0010626288130879402
iteration 162, loss = 0.0007874608272686601
iteration 163, loss = 0.0007445120718330145
iteration 164, loss = 0.001688751275651157
iteration 165, loss = 0.0009028562344610691
iteration 166, loss = 0.0007617566734552383
iteration 167, loss = 0.0011517939856275916
iteration 168, loss = 0.0004791252431459725
iteration 169, loss = 0.0005674638086929917
iteration 170, loss = 0.002112617250531912
iteration 171, loss = 0.0014403096865862608
iteration 172, loss = 0.0006522072362713516
iteration 173, loss = 0.0005796002224087715
iteration 174, loss = 0.0015847530448809266
iteration 175, loss = 0.0006435455870814621
iteration 176, loss = 0.0006910519441589713
iteration 177, loss = 0.0006695848423987627
iteration 178, loss = 0.0019518646877259016
iteration 179, loss = 0.0019113152520731091
iteration 180, loss = 0.0008396822959184647
iteration 181, loss = 0.001053056214004755
iteration 182, loss = 0.002430599182844162
iteration 183, loss = 0.0013295968528836966
iteration 184, loss = 0.0008697878802195191
iteration 185, loss = 0.0008978371624834836
iteration 186, loss = 0.0007132684113457799
iteration 187, loss = 0.0004983131075277925
iteration 188, loss = 0.0029172138310968876
iteration 189, loss = 0.0007218987448140979
iteration 190, loss = 0.0005852914182469249
iteration 191, loss = 0.000774949265178293
iteration 192, loss = 0.0010534758912399411
iteration 193, loss = 0.001240047626197338
iteration 194, loss = 0.0007349135121330619
iteration 195, loss = 0.0005273108836263418
iteration 196, loss = 0.0006957664154469967
iteration 197, loss = 0.001069323276169598
iteration 198, loss = 0.0006850718054920435
iteration 199, loss = 0.0007189172902144492
iteration 200, loss = 0.000661168247461319
iteration 201, loss = 0.0005187108181416988
iteration 202, loss = 0.00042602134635671973
iteration 203, loss = 0.0006328304298222065
iteration 204, loss = 0.0005956877139396966
iteration 205, loss = 0.0008619845029897988
iteration 206, loss = 0.0008918727398850024
iteration 207, loss = 0.0007035175804048777
iteration 208, loss = 0.0006432972149923444
iteration 209, loss = 0.0005660445895045996
iteration 210, loss = 0.0006148497923277318
iteration 211, loss = 0.0008010346791706979
iteration 212, loss = 0.0005411524325609207
iteration 213, loss = 0.0007566895219497383
iteration 214, loss = 0.0006419218261726201
iteration 215, loss = 0.0011616699630394578
iteration 216, loss = 0.0006314264028333127
iteration 217, loss = 0.0005824207910336554
iteration 218, loss = 0.0017903511179611087
iteration 219, loss = 0.0007730656070634723
iteration 220, loss = 0.0010039993794634938
iteration 221, loss = 0.0010230728657916188
iteration 222, loss = 0.001090091303922236
iteration 223, loss = 0.001137249288149178
iteration 224, loss = 0.0007406054646708071
iteration 225, loss = 0.0011506294831633568
iteration 226, loss = 0.0005743346409872174
iteration 227, loss = 0.001052958075888455
iteration 228, loss = 0.0010738099226728082
iteration 229, loss = 0.0005065393634140491
iteration 230, loss = 0.0005042653065174818
iteration 231, loss = 0.0010601073736324906
iteration 232, loss = 0.0008075727964751422
iteration 233, loss = 0.0012170732952654362
iteration 234, loss = 0.0006068814545869827
iteration 235, loss = 0.0021600283216685057
iteration 236, loss = 0.0006609402480535209
iteration 237, loss = 0.0006652140873484313
iteration 238, loss = 0.0004987978027202189
iteration 239, loss = 0.0008901089895516634
iteration 240, loss = 0.0009324265411123633
iteration 241, loss = 0.0010464001679793
iteration 242, loss = 0.0007094639586284757
iteration 243, loss = 0.000952575879637152
iteration 244, loss = 0.0009428169578313828
iteration 245, loss = 0.0007042344659566879
iteration 246, loss = 0.0006395584205165505
iteration 247, loss = 0.0004894356243312359
iteration 248, loss = 0.0004967955756001174
iteration 249, loss = 0.0007536369957961142
iteration 250, loss = 0.0007865672814659774
iteration 251, loss = 0.0006869571516290307
iteration 252, loss = 0.0005142068839631975
iteration 253, loss = 0.000402364123146981
iteration 254, loss = 0.0019201172981411219
iteration 255, loss = 0.00038019774365238845
iteration 256, loss = 0.0005620599258691072
iteration 257, loss = 0.0008618777501396835
iteration 258, loss = 0.0011834176257252693
iteration 259, loss = 0.0007262282888405025
iteration 260, loss = 0.0013094040332362056
iteration 261, loss = 0.0009073400869965553
iteration 262, loss = 0.0006012287340126932
iteration 263, loss = 0.0006272431346587837
iteration 264, loss = 0.0010234781075268984
iteration 265, loss = 0.0006549928220920265
iteration 266, loss = 0.0006878373678773642
iteration 267, loss = 0.0020112553611397743
iteration 268, loss = 0.0013339247088879347
iteration 269, loss = 0.0005363677046261728
iteration 270, loss = 0.0009018394048325717
iteration 271, loss = 0.0007571951136924326
iteration 272, loss = 0.001201875158585608
iteration 273, loss = 0.0007878688629716635
iteration 274, loss = 0.0012216633185744286
iteration 275, loss = 0.000591606367379427
iteration 276, loss = 0.000403202255256474
iteration 277, loss = 0.0010127276182174683
iteration 278, loss = 0.0007641640258952975
iteration 279, loss = 0.002115309238433838
iteration 280, loss = 0.0006937404396012425
iteration 281, loss = 0.001410568249411881
iteration 282, loss = 0.0006564217037521303
iteration 283, loss = 0.0008001747191883624
iteration 284, loss = 0.0009882180020213127
iteration 285, loss = 0.000828231917694211
iteration 286, loss = 0.000542919326107949
iteration 287, loss = 0.0011952670756727457
iteration 288, loss = 0.0005026517319492996
iteration 289, loss = 0.0007323875324800611
iteration 290, loss = 0.00047845314838923514
iteration 291, loss = 0.0006623402005061507
iteration 292, loss = 0.0006819596164859831
iteration 293, loss = 0.00044265162432566285
iteration 294, loss = 0.000663293176330626
iteration 295, loss = 0.00040527910459786654
iteration 296, loss = 0.0007393002742901444
iteration 297, loss = 0.0008084219880402088
iteration 298, loss = 0.0010912681464105844
iteration 299, loss = 0.001087608514353633
iteration 0, loss = 0.0006130877882242203
iteration 1, loss = 0.00044887117110192776
iteration 2, loss = 0.0007616545772179961
iteration 3, loss = 0.0009144456125795841
iteration 4, loss = 0.0009214045712724328
iteration 5, loss = 0.000718519848305732
iteration 6, loss = 0.0007246591849252582
iteration 7, loss = 0.0005842948448844254
iteration 8, loss = 0.0006710500456392765
iteration 9, loss = 0.0020300622563809156
iteration 10, loss = 0.0006232824525795877
iteration 11, loss = 0.0008063582936301827
iteration 12, loss = 0.0004329538787715137
iteration 13, loss = 0.0006351578049361706
iteration 14, loss = 0.0010237802052870393
iteration 15, loss = 0.000650158675853163
iteration 16, loss = 0.00041135618812404573
iteration 17, loss = 0.0011661919998005033
iteration 18, loss = 0.0005614942638203502
iteration 19, loss = 0.0009349677129648626
iteration 20, loss = 0.0006528924568556249
iteration 21, loss = 0.000797992863226682
iteration 22, loss = 0.0005230175447650254
iteration 23, loss = 0.0005642263568006456
iteration 24, loss = 0.000735848443582654
iteration 25, loss = 0.0008935588411986828
iteration 26, loss = 0.0007294307579286397
iteration 27, loss = 0.0005373356398195028
iteration 28, loss = 0.00044933578465133905
iteration 29, loss = 0.0010114243486896157
iteration 30, loss = 0.00048242363845929503
iteration 31, loss = 0.0005907420418225229
iteration 32, loss = 0.000447590573458001
iteration 33, loss = 0.0006165832746773958
iteration 34, loss = 0.0015435742679983377
iteration 35, loss = 0.0008641132153570652
iteration 36, loss = 0.0006760903634130955
iteration 37, loss = 0.0007224367000162601
iteration 38, loss = 0.00044504969264380634
iteration 39, loss = 0.0005719750188291073
iteration 40, loss = 0.0019183780532330275
iteration 41, loss = 0.0033067078329622746
iteration 42, loss = 0.0006642945809289813
iteration 43, loss = 0.0005221984465606511
iteration 44, loss = 0.0006206008838489652
iteration 45, loss = 0.0007604850106872618
iteration 46, loss = 0.0006196676404215395
iteration 47, loss = 0.0006228042766451836
iteration 48, loss = 0.000686934741679579
iteration 49, loss = 0.0007103554671630263
iteration 50, loss = 0.0004895022138953209
iteration 51, loss = 0.0005741814384236932
iteration 52, loss = 0.0005613624234683812
iteration 53, loss = 0.0004563434631563723
iteration 54, loss = 0.0008420582162216306
iteration 55, loss = 0.0007083663949742913
iteration 56, loss = 0.001827223226428032
iteration 57, loss = 0.0006608128896914423
iteration 58, loss = 0.0006471979431807995
iteration 59, loss = 0.0006905695772729814
iteration 60, loss = 0.0009866092586889863
iteration 61, loss = 0.0012633232399821281
iteration 62, loss = 0.0007840836187824607
iteration 63, loss = 0.0007427548989653587
iteration 64, loss = 0.0008941289270296693
iteration 65, loss = 0.0009636601316742599
iteration 66, loss = 0.0010332390666007996
iteration 67, loss = 0.00043263318366371095
iteration 68, loss = 0.000989548279903829
iteration 69, loss = 0.0006766875158064067
iteration 70, loss = 0.0015859858831390738
iteration 71, loss = 0.0005691954866051674
iteration 72, loss = 0.0008362486260011792
iteration 73, loss = 0.0009582455386407673
iteration 74, loss = 0.00038048630813136697
iteration 75, loss = 0.0005694656865671277
iteration 76, loss = 0.00047149613965302706
iteration 77, loss = 0.0005097571993246675
iteration 78, loss = 0.00043086864752694964
iteration 79, loss = 0.0006880788132548332
iteration 80, loss = 0.000703226076439023
iteration 81, loss = 0.0005751494900323451
iteration 82, loss = 0.0004350151866674423
iteration 83, loss = 0.0011065565049648285
iteration 84, loss = 0.0007501364452764392
iteration 85, loss = 0.0007463784422725439
iteration 86, loss = 0.0020780216436833143
iteration 87, loss = 0.0007690087659284472
iteration 88, loss = 0.0008503086864948273
iteration 89, loss = 0.0008875438943505287
iteration 90, loss = 0.000898252590559423
iteration 91, loss = 0.0006080633029341698
iteration 92, loss = 0.0005270442925393581
iteration 93, loss = 0.0008784747333265841
iteration 94, loss = 0.001579573261551559
iteration 95, loss = 0.0007029063999652863
iteration 96, loss = 0.0008778134360909462
iteration 97, loss = 0.000852745259180665
iteration 98, loss = 0.0021570436656475067
iteration 99, loss = 0.0005192251410335302
iteration 100, loss = 0.0010684337466955185
iteration 101, loss = 0.0006299265660345554
iteration 102, loss = 0.0015932890819385648
iteration 103, loss = 0.0006014343234710395
iteration 104, loss = 0.0007983193499967456
iteration 105, loss = 0.00043584342347458005
iteration 106, loss = 0.00068103481316939
iteration 107, loss = 0.0008973724907264113
iteration 108, loss = 0.0018083557952195406
iteration 109, loss = 0.0008919116808101535
iteration 110, loss = 0.0007441577035933733
iteration 111, loss = 0.000820586399640888
iteration 112, loss = 0.0009253527969121933
iteration 113, loss = 0.00044867987162433565
iteration 114, loss = 0.0006868530763313174
iteration 115, loss = 0.00042020317050628364
iteration 116, loss = 0.0006731594912707806
iteration 117, loss = 0.00047777759027667344
iteration 118, loss = 0.0007455385639332235
iteration 119, loss = 0.0008289313409477472
iteration 120, loss = 0.0007991842576302588
iteration 121, loss = 0.0005662065814249218
iteration 122, loss = 0.0013292545918375254
iteration 123, loss = 0.0009629132691770792
iteration 124, loss = 0.0004118860815651715
iteration 125, loss = 0.000767253921367228
iteration 126, loss = 0.0007142620161175728
iteration 127, loss = 0.0015008723130449653
iteration 128, loss = 0.00106516620144248
iteration 129, loss = 0.0008073479402810335
iteration 130, loss = 0.0006724410923197865
iteration 131, loss = 0.0007892664289101958
iteration 132, loss = 0.0007981418166309595
iteration 133, loss = 0.0009188803960569203
iteration 134, loss = 0.0006483333418145776
iteration 135, loss = 0.00048329218407161534
iteration 136, loss = 0.000878181483130902
iteration 137, loss = 0.0004233380313962698
iteration 138, loss = 0.0009116656146943569
iteration 139, loss = 0.0008318223408423364
iteration 140, loss = 0.0007037390023469925
iteration 141, loss = 0.00047203589929267764
iteration 142, loss = 0.001407956937327981
iteration 143, loss = 0.0008842031820677221
iteration 144, loss = 0.000521836627740413
iteration 145, loss = 0.0013387793442234397
iteration 146, loss = 0.0009907400235533714
iteration 147, loss = 0.0005047937738709152
iteration 148, loss = 0.0008066345471888781
iteration 149, loss = 0.0005902100820094347
iteration 150, loss = 0.0006157428142614663
iteration 151, loss = 0.0006468085921369493
iteration 152, loss = 0.0008672436815686524
iteration 153, loss = 0.0008683425257913768
iteration 154, loss = 0.001588984508998692
iteration 155, loss = 0.0007267572218552232
iteration 156, loss = 0.0005406680284067988
iteration 157, loss = 0.0011463324772194028
iteration 158, loss = 0.0026986270677298307
iteration 159, loss = 0.0010127524146810174
iteration 160, loss = 0.0006743755657225847
iteration 161, loss = 0.0006278700311668217
iteration 162, loss = 0.0008956963429227471
iteration 163, loss = 0.0009356669033877552
iteration 164, loss = 0.00047758949222043157
iteration 165, loss = 0.000649946101475507
iteration 166, loss = 0.0007321257144212723
iteration 167, loss = 0.0005778928752988577
iteration 168, loss = 0.000518499466124922
iteration 169, loss = 0.001197410630993545
iteration 170, loss = 0.0007403801428154111
iteration 171, loss = 0.0005915361107327044
iteration 172, loss = 0.0006829071789979935
iteration 173, loss = 0.0007450791308656335
iteration 174, loss = 0.001117758802138269
iteration 175, loss = 0.0004414401773829013
iteration 176, loss = 0.001727336086332798
iteration 177, loss = 0.0017571252537891269
iteration 178, loss = 0.0008858496439643204
iteration 179, loss = 0.0005343263037502766
iteration 180, loss = 0.0005392710445448756
iteration 181, loss = 0.0009050396620295942
iteration 182, loss = 0.0013822463806718588
iteration 183, loss = 0.0006817114190198481
iteration 184, loss = 0.0005198267754167318
iteration 185, loss = 0.0007775421254336834
iteration 186, loss = 0.0007006938685663044
iteration 187, loss = 0.0012558093294501305
iteration 188, loss = 0.0006068906513974071
iteration 189, loss = 0.0005062101408839226
iteration 190, loss = 0.0021039496641606092
iteration 191, loss = 0.0007072891457937658
iteration 192, loss = 0.0007283262675628066
iteration 193, loss = 0.0005491112824529409
iteration 194, loss = 0.0004924942622892559
iteration 195, loss = 0.00042599011794663966
iteration 196, loss = 0.0006102173356339335
iteration 197, loss = 0.0005799457430839539
iteration 198, loss = 0.0006552289123646915
iteration 199, loss = 0.0004662326828110963
iteration 200, loss = 0.001222691498696804
iteration 201, loss = 0.0007613767520524561
iteration 202, loss = 0.0005416776984930038
iteration 203, loss = 0.000683564052451402
iteration 204, loss = 0.0012053993996232748
iteration 205, loss = 0.0010951096192002296
iteration 206, loss = 0.001094318926334381
iteration 207, loss = 0.0006598404725082219
iteration 208, loss = 0.0008291173726320267
iteration 209, loss = 0.0021297712810337543
iteration 210, loss = 0.0007883008220233023
iteration 211, loss = 0.0007745741750113666
iteration 212, loss = 0.0008038711384870112
iteration 213, loss = 0.0005548555636778474
iteration 214, loss = 0.0005313578294590116
iteration 215, loss = 0.0007022293284535408
iteration 216, loss = 0.0004876538005191833
iteration 217, loss = 0.0005645380588248372
iteration 218, loss = 0.0005912436754442751
iteration 219, loss = 0.0011503171408548951
iteration 220, loss = 0.0017701087053865194
iteration 221, loss = 0.0006533354753628373
iteration 222, loss = 0.0004231897182762623
iteration 223, loss = 0.0005642381147481501
iteration 224, loss = 0.0007424239884130657
iteration 225, loss = 0.0013732928782701492
iteration 226, loss = 0.0012562359916046262
iteration 227, loss = 0.0005335231544449925
iteration 228, loss = 0.0008912543416954577
iteration 229, loss = 0.001984960399568081
iteration 230, loss = 0.0005482094129547477
iteration 231, loss = 0.0004062448279000819
iteration 232, loss = 0.0005806206609122455
iteration 233, loss = 0.0007926294929347932
iteration 234, loss = 0.0007957202615216374
iteration 235, loss = 0.0005609806976281106
iteration 236, loss = 0.0008840681985020638
iteration 237, loss = 0.0017700414173305035
iteration 238, loss = 0.001244547194801271
iteration 239, loss = 0.0007169443415477872
iteration 240, loss = 0.0005942527786828578
iteration 241, loss = 0.0007015973096713424
iteration 242, loss = 0.0009809751063585281
iteration 243, loss = 0.0009070920059457421
iteration 244, loss = 0.000968218024354428
iteration 245, loss = 0.0006365670706145465
iteration 246, loss = 0.0008152819937095046
iteration 247, loss = 0.0020370667334645987
iteration 248, loss = 0.0007082900847308338
iteration 249, loss = 0.0005068745813332498
iteration 250, loss = 0.0012044929899275303
iteration 251, loss = 0.0014621319714933634
iteration 252, loss = 0.0008247754303738475
iteration 253, loss = 0.0005321851349435747
iteration 254, loss = 0.0007510849973186851
iteration 255, loss = 0.0010429538087919354
iteration 256, loss = 0.0007800148450769484
iteration 257, loss = 0.001039327820762992
iteration 258, loss = 0.0006287166615948081
iteration 259, loss = 0.0008113264921121299
iteration 260, loss = 0.0012021958827972412
iteration 261, loss = 0.0008247277000918984
iteration 262, loss = 0.0009864082094281912
iteration 263, loss = 0.0005543464212678373
iteration 264, loss = 0.0008107749745249748
iteration 265, loss = 0.001234529190696776
iteration 266, loss = 0.0006398149416781962
iteration 267, loss = 0.0004193481581751257
iteration 268, loss = 0.0006503161275759339
iteration 269, loss = 0.0011857786448672414
iteration 270, loss = 0.0006769264582544565
iteration 271, loss = 0.001790483365766704
iteration 272, loss = 0.00154279510024935
iteration 273, loss = 0.0005170903168618679
iteration 274, loss = 0.0018525448394939303
iteration 275, loss = 0.0009047598578035831
iteration 276, loss = 0.0004882457433268428
iteration 277, loss = 0.0005992334336042404
iteration 278, loss = 0.0004844551149290055
iteration 279, loss = 0.0008821465307846665
iteration 280, loss = 0.0006824307492934167
iteration 281, loss = 0.0004402559425216168
iteration 282, loss = 0.0006119093159213662
iteration 283, loss = 0.00045841268729418516
iteration 284, loss = 0.0005094846128486097
iteration 285, loss = 0.0003708545700646937
iteration 286, loss = 0.0005044892895966768
iteration 287, loss = 0.0005938286194577813
iteration 288, loss = 0.0010654772631824017
iteration 289, loss = 0.00048308540135622025
iteration 290, loss = 0.0006190597778186202
iteration 291, loss = 0.0005696011357940733
iteration 292, loss = 0.0007826393120922148
iteration 293, loss = 0.0006882633897475898
iteration 294, loss = 0.0006604719674214721
iteration 295, loss = 0.0012567670783028007
iteration 296, loss = 0.0006129820831120014
iteration 297, loss = 0.0005558709963224828
iteration 298, loss = 0.0004448510007932782
iteration 299, loss = 0.0008142400183714926
iteration 0, loss = 0.0005762916407547891
iteration 1, loss = 0.0006711284513585269
iteration 2, loss = 0.0004519241920206696
iteration 3, loss = 0.0005866895662620664
iteration 4, loss = 0.0009119589230976999
iteration 5, loss = 0.0011042843107134104
iteration 6, loss = 0.0005033623310737312
iteration 7, loss = 0.0004140905220992863
iteration 8, loss = 0.0006901848828420043
iteration 9, loss = 0.0005271931295283139
iteration 10, loss = 0.0018067862838506699
iteration 11, loss = 0.0005635152338072658
iteration 12, loss = 0.0010561628732830286
iteration 13, loss = 0.0005193025572225451
iteration 14, loss = 0.0006488015060313046
iteration 15, loss = 0.0005568237975239754
iteration 16, loss = 0.0007811373798176646
iteration 17, loss = 0.0008599063148722053
iteration 18, loss = 0.000553160032723099
iteration 19, loss = 0.0008487884188070893
iteration 20, loss = 0.000875354278832674
iteration 21, loss = 0.0011318568140268326
iteration 22, loss = 0.0005856279167346656
iteration 23, loss = 0.00039468929753638804
iteration 24, loss = 0.0008320766501128674
iteration 25, loss = 0.00086427410133183
iteration 26, loss = 0.00039442782872356474
iteration 27, loss = 0.001081734779290855
iteration 28, loss = 0.0004811192920897156
iteration 29, loss = 0.00044838926987722516
iteration 30, loss = 0.0007124513504095376
iteration 31, loss = 0.0005629326915368438
iteration 32, loss = 0.000629367190413177
iteration 33, loss = 0.0005063562421128154
iteration 34, loss = 0.0008223868790082633
iteration 35, loss = 0.0006601102650165558
iteration 36, loss = 0.001181960804387927
iteration 37, loss = 0.0008506515296176076
iteration 38, loss = 0.0007396634900942445
iteration 39, loss = 0.0006747965817339718
iteration 40, loss = 0.0007164565031416714
iteration 41, loss = 0.000740849704016
iteration 42, loss = 0.0007119093788787723
iteration 43, loss = 0.00041122082620859146
iteration 44, loss = 0.0007893607253208756
iteration 45, loss = 0.0007805817876942456
iteration 46, loss = 0.0011643014149740338
iteration 47, loss = 0.0006858834531158209
iteration 48, loss = 0.0004804363416042179
iteration 49, loss = 0.0009028048953041434
iteration 50, loss = 0.0007395409047603607
iteration 51, loss = 0.0019171872409060597
iteration 52, loss = 0.0016210112953558564
iteration 53, loss = 0.0012300316011533141
iteration 54, loss = 0.0007936510955914855
iteration 55, loss = 0.00116356136277318
iteration 56, loss = 0.0006526624201796949
iteration 57, loss = 0.0006363342399708927
iteration 58, loss = 0.0006585969822481275
iteration 59, loss = 0.0006302252877503633
iteration 60, loss = 0.002028385177254677
iteration 61, loss = 0.001141575281508267
iteration 62, loss = 0.0005692397826351225
iteration 63, loss = 0.0024890878703445196
iteration 64, loss = 0.000487475044792518
iteration 65, loss = 0.0008946445886977017
iteration 66, loss = 0.0005670111859217286
iteration 67, loss = 0.00045638965093530715
iteration 68, loss = 0.002153450157493353
iteration 69, loss = 0.0006630776333622634
iteration 70, loss = 0.0008332043653354049
iteration 71, loss = 0.001049924991093576
iteration 72, loss = 0.00188958621583879
iteration 73, loss = 0.0005372246378101408
iteration 74, loss = 0.0006308314623311162
iteration 75, loss = 0.001134656136855483
iteration 76, loss = 0.0005766433896496892
iteration 77, loss = 0.0004116346244700253
iteration 78, loss = 0.0008139381534419954
iteration 79, loss = 0.0005342910881154239
iteration 80, loss = 0.000745454162824899
iteration 81, loss = 0.0005217649741098285
iteration 82, loss = 0.0004356501449365169
iteration 83, loss = 0.0013778163120150566
iteration 84, loss = 0.0006802746211178601
iteration 85, loss = 0.0005560357822105289
iteration 86, loss = 0.0005701513728126884
iteration 87, loss = 0.0007853217539377511
iteration 88, loss = 0.0007001663907431066
iteration 89, loss = 0.0004998391959816217
iteration 90, loss = 0.0005957474349997938
iteration 91, loss = 0.0007889699190855026
iteration 92, loss = 0.0005576820694841444
iteration 93, loss = 0.00047118522343225777
iteration 94, loss = 0.000768335594329983
iteration 95, loss = 0.001874669105745852
iteration 96, loss = 0.00047788460506126285
iteration 97, loss = 0.0006518930313177407
iteration 98, loss = 0.0005849632434546947
iteration 99, loss = 0.0005877027870155871
iteration 100, loss = 0.00048475584480911493
iteration 101, loss = 0.0007188295712694526
iteration 102, loss = 0.00062671082559973
iteration 103, loss = 0.0004943660460412502
iteration 104, loss = 0.0014349715784192085
iteration 105, loss = 0.0005732991849072278
iteration 106, loss = 0.0006439388962462544
iteration 107, loss = 0.0007351137464866042
iteration 108, loss = 0.00044889358105137944
iteration 109, loss = 0.0009253257303498685
iteration 110, loss = 0.000939193123485893
iteration 111, loss = 0.0007061150390654802
iteration 112, loss = 0.0007327624480240047
iteration 113, loss = 0.00089220458175987
iteration 114, loss = 0.0010728235356509686
iteration 115, loss = 0.0004908828414045274
iteration 116, loss = 0.001994011690840125
iteration 117, loss = 0.000697776791639626
iteration 118, loss = 0.000621389364823699
iteration 119, loss = 0.0005136799300089478
iteration 120, loss = 0.0012455638498067856
iteration 121, loss = 0.0007376317516900599
iteration 122, loss = 0.00049786944873631
iteration 123, loss = 0.0006629486451856792
iteration 124, loss = 0.0010483460500836372
iteration 125, loss = 0.0009004758903756738
iteration 126, loss = 0.0006269494770094752
iteration 127, loss = 0.0008693257695995271
iteration 128, loss = 0.0004966157139278948
iteration 129, loss = 0.0006309743621386588
iteration 130, loss = 0.0005719598848372698
iteration 131, loss = 0.000795537605881691
iteration 132, loss = 0.0007032962748780847
iteration 133, loss = 0.001730090705677867
iteration 134, loss = 0.0008799688657745719
iteration 135, loss = 0.001788510475307703
iteration 136, loss = 0.00130511075258255
iteration 137, loss = 0.001179867540486157
iteration 138, loss = 0.0006936507998034358
iteration 139, loss = 0.00042978921555913985
iteration 140, loss = 0.0007654170040041208
iteration 141, loss = 0.0006874857936054468
iteration 142, loss = 0.000755888584535569
iteration 143, loss = 0.0017923227278515697
iteration 144, loss = 0.00044669356429949403
iteration 145, loss = 0.000732493179384619
iteration 146, loss = 0.00042584529728628695
iteration 147, loss = 0.0004927505506202579
iteration 148, loss = 0.0003322784323245287
iteration 149, loss = 0.0006186377722769976
iteration 150, loss = 0.0004818693269044161
iteration 151, loss = 0.0005593074019998312
iteration 152, loss = 0.0006065226625651121
iteration 153, loss = 0.0006716781063005328
iteration 154, loss = 0.0017849425785243511
iteration 155, loss = 0.001116584287956357
iteration 156, loss = 0.0005806267727166414
iteration 157, loss = 0.00047709926730021834
iteration 158, loss = 0.000552335346583277
iteration 159, loss = 0.000553409568965435
iteration 160, loss = 0.0004833914281334728
iteration 161, loss = 0.00038233937812037766
iteration 162, loss = 0.0005502435378730297
iteration 163, loss = 0.0008238208247348666
iteration 164, loss = 0.0006770293111912906
iteration 165, loss = 0.0006237414781935513
iteration 166, loss = 0.0006745231221430004
iteration 167, loss = 0.0007374542765319347
iteration 168, loss = 0.0005556106334552169
iteration 169, loss = 0.0006925638299435377
iteration 170, loss = 0.0011746888048946857
iteration 171, loss = 0.0010216750670224428
iteration 172, loss = 0.00044071796583011746
iteration 173, loss = 0.0023504230193793774
iteration 174, loss = 0.0005940015544183552
iteration 175, loss = 0.000519718392752111
iteration 176, loss = 0.0006943655898794532
iteration 177, loss = 0.0005131701473146677
iteration 178, loss = 0.0010599243687465787
iteration 179, loss = 0.000686932762619108
iteration 180, loss = 0.0008699852623976767
iteration 181, loss = 0.0007782356115058064
iteration 182, loss = 0.000588755588978529
iteration 183, loss = 0.0019206322031095624
iteration 184, loss = 0.0005385956028476357
iteration 185, loss = 0.0012223628582432866
iteration 186, loss = 0.0007299247663468122
iteration 187, loss = 0.0004218776011839509
iteration 188, loss = 0.0005422577960416675
iteration 189, loss = 0.0003965846844948828
iteration 190, loss = 0.0010744323953986168
iteration 191, loss = 0.0009379526018165052
iteration 192, loss = 0.0010424399515613914
iteration 193, loss = 0.0005513918586075306
iteration 194, loss = 0.0009746305295266211
iteration 195, loss = 0.0009752553887665272
iteration 196, loss = 0.001500044367276132
iteration 197, loss = 0.0004535713233053684
iteration 198, loss = 0.000647693988867104
iteration 199, loss = 0.000587242073379457
iteration 200, loss = 0.0006013663369230926
iteration 201, loss = 0.0006551558617502451
iteration 202, loss = 0.0008035372011363506
iteration 203, loss = 0.0008591220248490572
iteration 204, loss = 0.0005427149590104818
iteration 205, loss = 0.0010782278841361403
iteration 206, loss = 0.0010978009086102247
iteration 207, loss = 0.0004405408399179578
iteration 208, loss = 0.0020816056057810783
iteration 209, loss = 0.0006472092936746776
iteration 210, loss = 0.0003581990604288876
iteration 211, loss = 0.0009267210843972862
iteration 212, loss = 0.0008290824480354786
iteration 213, loss = 0.0005947590107098222
iteration 214, loss = 0.0006486863130703568
iteration 215, loss = 0.0010184539714828134
iteration 216, loss = 0.0005739321932196617
iteration 217, loss = 0.0006810022750869393
iteration 218, loss = 0.0010709901107475162
iteration 219, loss = 0.0006119957542978227
iteration 220, loss = 0.0004742957535199821
iteration 221, loss = 0.000728445069398731
iteration 222, loss = 0.0006768289022147655
iteration 223, loss = 0.0006606606184504926
iteration 224, loss = 0.0005897502414882183
iteration 225, loss = 0.00312212947756052
iteration 226, loss = 0.0006472311215475202
iteration 227, loss = 0.0005024485290050507
iteration 228, loss = 0.0010913267033174634
iteration 229, loss = 0.0008587805787101388
iteration 230, loss = 0.0006477519636973739
iteration 231, loss = 0.0005252894479781389
iteration 232, loss = 0.00048413092736154795
iteration 233, loss = 0.0006423623999580741
iteration 234, loss = 0.0006421211874112487
iteration 235, loss = 0.000502757728099823
iteration 236, loss = 0.0006391839124262333
iteration 237, loss = 0.00038935308111831546
iteration 238, loss = 0.0006793305510655046
iteration 239, loss = 0.0005660668248310685
iteration 240, loss = 0.0007501997170038521
iteration 241, loss = 0.0009539841557852924
iteration 242, loss = 0.000460869719972834
iteration 243, loss = 0.0010843639029189944
iteration 244, loss = 0.0009193405858241022
iteration 245, loss = 0.0006770936306566
iteration 246, loss = 0.0010821548057720065
iteration 247, loss = 0.0006918215076439083
iteration 248, loss = 0.00046444806503131986
iteration 249, loss = 0.0006295524071902037
iteration 250, loss = 0.0006644968525506556
iteration 251, loss = 0.0004502468218561262
iteration 252, loss = 0.000545750604942441
iteration 253, loss = 0.0004369478265289217
iteration 254, loss = 0.0009492738172411919
iteration 255, loss = 0.0005215429700911045
iteration 256, loss = 0.0009257971541956067
iteration 257, loss = 0.0006380794220604002
iteration 258, loss = 0.0005747332470491529
iteration 259, loss = 0.0006395259988494217
iteration 260, loss = 0.0009045106708072126
iteration 261, loss = 0.0006012250669300556
iteration 262, loss = 0.0009829504415392876
iteration 263, loss = 0.0005721683846786618
iteration 264, loss = 0.00048467598389834166
iteration 265, loss = 0.0009206552640534937
iteration 266, loss = 0.0013232679339125752
iteration 267, loss = 0.00039380291127599776
iteration 268, loss = 0.0010420485632494092
iteration 269, loss = 0.0005304436781443655
iteration 270, loss = 0.0013996058842167258
iteration 271, loss = 0.000682209269143641
iteration 272, loss = 0.0005812534946016967
iteration 273, loss = 0.0016478820471093059
iteration 274, loss = 0.0005296875024214387
iteration 275, loss = 0.0006115742726251483
iteration 276, loss = 0.0006338943494483829
iteration 277, loss = 0.00036769150756299496
iteration 278, loss = 0.0006255469052121043
iteration 279, loss = 0.0008737897733226418
iteration 280, loss = 0.0006587469251826406
iteration 281, loss = 0.0007583586848340929
iteration 282, loss = 0.0009149776888079941
iteration 283, loss = 0.0005995904211886227
iteration 284, loss = 0.00047510056174360216
iteration 285, loss = 0.0006915454869158566
iteration 286, loss = 0.0008203241159208119
iteration 287, loss = 0.00048584758769720793
iteration 288, loss = 0.0006632929434999824
iteration 289, loss = 0.0019312705844640732
iteration 290, loss = 0.0007922967779450119
iteration 291, loss = 0.0005388435092754662
iteration 292, loss = 0.00044268337660469115
iteration 293, loss = 0.0003280799719505012
iteration 294, loss = 0.0006930639501661062
iteration 295, loss = 0.0005610844236798584
iteration 296, loss = 0.0009216631297022104
iteration 297, loss = 0.0011717673623934388
iteration 298, loss = 0.0005396160413511097
iteration 299, loss = 0.00043441232992336154
iteration 0, loss = 0.0009573202114552259
iteration 1, loss = 0.0010721341241151094
iteration 2, loss = 0.0007897120667621493
iteration 3, loss = 0.00098195590544492
iteration 4, loss = 0.0005316843162290752
iteration 5, loss = 0.00034619588404893875
iteration 6, loss = 0.0012361138360574841
iteration 7, loss = 0.0003454021643847227
iteration 8, loss = 0.0007781379390507936
iteration 9, loss = 0.0016772878589108586
iteration 10, loss = 0.000471885985461995
iteration 11, loss = 0.0005314854206517339
iteration 12, loss = 0.0005650712409988046
iteration 13, loss = 0.0005520806880667806
iteration 14, loss = 0.0005658646114170551
iteration 15, loss = 0.0007155821076594293
iteration 16, loss = 0.001615892630070448
iteration 17, loss = 0.0020478612277656794
iteration 18, loss = 0.0005633790278807282
iteration 19, loss = 0.0005793493473902345
iteration 20, loss = 0.0007034040754660964
iteration 21, loss = 0.0012469696812331676
iteration 22, loss = 0.00048240210162475705
iteration 23, loss = 0.000498988782055676
iteration 24, loss = 0.00044488179264590144
iteration 25, loss = 0.000714688329026103
iteration 26, loss = 0.0006217288319021463
iteration 27, loss = 0.0010765093611553311
iteration 28, loss = 0.0006142977508716285
iteration 29, loss = 0.0009601256460882723
iteration 30, loss = 0.001824278850108385
iteration 31, loss = 0.0008819643408060074
iteration 32, loss = 0.0002986587060149759
iteration 33, loss = 0.0004911270225420594
iteration 34, loss = 0.0006968170637264848
iteration 35, loss = 0.0006268787547014654
iteration 36, loss = 0.00048184546176344156
iteration 37, loss = 0.0006935824640095234
iteration 38, loss = 0.0007913318695500493
iteration 39, loss = 0.001041923649609089
iteration 40, loss = 0.0004780655726790428
iteration 41, loss = 0.0010374485282227397
iteration 42, loss = 0.0005355553585104644
iteration 43, loss = 0.0006725595449097455
iteration 44, loss = 0.0008949865587055683
iteration 45, loss = 0.0004049215349368751
iteration 46, loss = 0.0005752624128945172
iteration 47, loss = 0.001156070502474904
iteration 48, loss = 0.0004582753172144294
iteration 49, loss = 0.0007919978233985603
iteration 50, loss = 0.0006043724133633077
iteration 51, loss = 0.0005104279262013733
iteration 52, loss = 0.0006020648870617151
iteration 53, loss = 0.0008521048002876341
iteration 54, loss = 0.0004025453235954046
iteration 55, loss = 0.00039992795791476965
iteration 56, loss = 0.0004404476785566658
iteration 57, loss = 0.0008103266009129584
iteration 58, loss = 0.0007262579165399075
iteration 59, loss = 0.0008736469317227602
iteration 60, loss = 0.0005631869425997138
iteration 61, loss = 0.0004434094007592648
iteration 62, loss = 0.0003875554248224944
iteration 63, loss = 0.001095250598154962
iteration 64, loss = 0.0011044987477362156
iteration 65, loss = 0.0007232129573822021
iteration 66, loss = 0.0006281993701122701
iteration 67, loss = 0.0008738682372495532
iteration 68, loss = 0.000438879884313792
iteration 69, loss = 0.0007006882806308568
iteration 70, loss = 0.0005772172007709742
iteration 71, loss = 0.000608638976700604
iteration 72, loss = 0.00042998220305889845
iteration 73, loss = 0.0006870607030577958
iteration 74, loss = 0.0009161431225948036
iteration 75, loss = 0.0008009682642295957
iteration 76, loss = 0.0006287330761551857
iteration 77, loss = 0.0004684056621044874
iteration 78, loss = 0.0003694320621434599
iteration 79, loss = 0.0007257766555994749
iteration 80, loss = 0.00033778510987758636
iteration 81, loss = 0.00035410927375778556
iteration 82, loss = 0.0006995339645072818
iteration 83, loss = 0.0008652793476358056
iteration 84, loss = 0.00160824169870466
iteration 85, loss = 0.0005934889195486903
iteration 86, loss = 0.0003447558847256005
iteration 87, loss = 0.0010582046816125512
iteration 88, loss = 0.0018103545298799872
iteration 89, loss = 0.001104252296499908
iteration 90, loss = 0.0008608808275312185
iteration 91, loss = 0.0007548035937361419
iteration 92, loss = 0.0011818869970738888
iteration 93, loss = 0.0006811310886405408
iteration 94, loss = 0.0007537256460636854
iteration 95, loss = 0.0009359786054119468
iteration 96, loss = 0.0012790021719411016
iteration 97, loss = 0.0005842316313646734
iteration 98, loss = 0.0006285831332206726
iteration 99, loss = 0.00049314193893224
iteration 100, loss = 0.0008879838278517127
iteration 101, loss = 0.0008202529861591756
iteration 102, loss = 0.00044095300836488605
iteration 103, loss = 0.00039273948641493917
iteration 104, loss = 0.000865454669110477
iteration 105, loss = 0.000961892306804657
iteration 106, loss = 0.0010493118315935135
iteration 107, loss = 0.00044291396625339985
iteration 108, loss = 0.0004919034545309842
iteration 109, loss = 0.000636212294921279
iteration 110, loss = 0.0008062486303970218
iteration 111, loss = 0.0008815244655124843
iteration 112, loss = 0.0005944856093265116
iteration 113, loss = 0.0004751302767544985
iteration 114, loss = 0.0006245559779927135
iteration 115, loss = 0.0004418549360707402
iteration 116, loss = 0.0005990426288917661
iteration 117, loss = 0.0010375987039878964
iteration 118, loss = 0.0006315485225059092
iteration 119, loss = 0.0006968798697926104
iteration 120, loss = 0.0008487485465593636
iteration 121, loss = 0.0007034269510768354
iteration 122, loss = 0.0006538908928632736
iteration 123, loss = 0.0011110537452623248
iteration 124, loss = 0.0006164441001601517
iteration 125, loss = 0.0007513649179600179
iteration 126, loss = 0.001212053233757615
iteration 127, loss = 0.0006606117822229862
iteration 128, loss = 0.0007413690909743309
iteration 129, loss = 0.0007794930133968592
iteration 130, loss = 0.0005576329422183335
iteration 131, loss = 0.0005101581919007003
iteration 132, loss = 0.0014358452754095197
iteration 133, loss = 0.0005166665650904179
iteration 134, loss = 0.0005703483475372195
iteration 135, loss = 0.000462160911411047
iteration 136, loss = 0.0008867960423231125
iteration 137, loss = 0.000771201157476753
iteration 138, loss = 0.001179893035441637
iteration 139, loss = 0.0006352333002723753
iteration 140, loss = 0.0016625621356070042
iteration 141, loss = 0.0004905516980215907
iteration 142, loss = 0.000505605130456388
iteration 143, loss = 0.000611674040555954
iteration 144, loss = 0.0009492149110883474
iteration 145, loss = 0.0005884559359401464
iteration 146, loss = 0.000528174452483654
iteration 147, loss = 0.001174773322418332
iteration 148, loss = 0.00047595013165846467
iteration 149, loss = 0.0003405055031180382
iteration 150, loss = 0.0004678053082898259
iteration 151, loss = 0.0016225965227931738
iteration 152, loss = 0.0010360166197642684
iteration 153, loss = 0.0005736079183407128
iteration 154, loss = 0.0003266405255999416
iteration 155, loss = 0.0015843419823795557
iteration 156, loss = 0.000538923719432205
iteration 157, loss = 0.0006481443415395916
iteration 158, loss = 0.000861948006786406
iteration 159, loss = 0.000575548445340246
iteration 160, loss = 0.0008512460626661777
iteration 161, loss = 0.0009810561314225197
iteration 162, loss = 0.0005878803203813732
iteration 163, loss = 0.0005862032994627953
iteration 164, loss = 0.0006260167574509978
iteration 165, loss = 0.0006963686319068074
iteration 166, loss = 0.0007109921425580978
iteration 167, loss = 0.0004136522184126079
iteration 168, loss = 0.0004076867480762303
iteration 169, loss = 0.0005302751669660211
iteration 170, loss = 0.0005391305894590914
iteration 171, loss = 0.0004942832747474313
iteration 172, loss = 0.0012106471695005894
iteration 173, loss = 0.000720946816727519
iteration 174, loss = 0.0004075026954524219
iteration 175, loss = 0.0010597913060337305
iteration 176, loss = 0.0005565984756685793
iteration 177, loss = 0.0004306359332986176
iteration 178, loss = 0.0006442628218792379
iteration 179, loss = 0.0009489827789366245
iteration 180, loss = 0.0004736940027214587
iteration 181, loss = 0.0007240839768201113
iteration 182, loss = 0.0003963495255447924
iteration 183, loss = 0.0006005617324262857
iteration 184, loss = 0.0006694736075587571
iteration 185, loss = 0.0006882059387862682
iteration 186, loss = 0.0007293178932741284
iteration 187, loss = 0.0004737126000691205
iteration 188, loss = 0.0005067070596851408
iteration 189, loss = 0.0011502854758873582
iteration 190, loss = 0.0007139249355532229
iteration 191, loss = 0.001106145791709423
iteration 192, loss = 0.0006583017529919744
iteration 193, loss = 0.00047647347673773766
iteration 194, loss = 0.000653863069601357
iteration 195, loss = 0.0009281291859224439
iteration 196, loss = 0.0005334208253771067
iteration 197, loss = 0.0007182770641520619
iteration 198, loss = 0.0008980059064924717
iteration 199, loss = 0.0007610690081492066
iteration 200, loss = 0.002389599336311221
iteration 201, loss = 0.0008123295265249908
iteration 202, loss = 0.000971040572039783
iteration 203, loss = 0.0006454358226619661
iteration 204, loss = 0.0015399511903524399
iteration 205, loss = 0.0007043396471999586
iteration 206, loss = 0.00046690454473719
iteration 207, loss = 0.0004160144890192896
iteration 208, loss = 0.0006384202861227095
iteration 209, loss = 0.0010307045886293054
iteration 210, loss = 0.000387944164685905
iteration 211, loss = 0.0009801882551982999
iteration 212, loss = 0.00035938675864599645
iteration 213, loss = 0.000567258452065289
iteration 214, loss = 0.000458130962215364
iteration 215, loss = 0.0006482130847871304
iteration 216, loss = 0.0006813323125243187
iteration 217, loss = 0.001933795283548534
iteration 218, loss = 0.0008431758033111691
iteration 219, loss = 0.0005874285125173628
iteration 220, loss = 0.00039482698775827885
iteration 221, loss = 0.00042673779535107315
iteration 222, loss = 0.0006761082331649959
iteration 223, loss = 0.0003762095875572413
iteration 224, loss = 0.00044538971269503236
iteration 225, loss = 0.0008301438647322357
iteration 226, loss = 0.000760832685045898
iteration 227, loss = 0.0004169917665421963
iteration 228, loss = 0.0006092529511079192
iteration 229, loss = 0.00039078883128240705
iteration 230, loss = 0.0010831559775397182
iteration 231, loss = 0.000997775001451373
iteration 232, loss = 0.0009129379177466035
iteration 233, loss = 0.0004852714773733169
iteration 234, loss = 0.0006477665738202631
iteration 235, loss = 0.00031672808108851314
iteration 236, loss = 0.0014344118535518646
iteration 237, loss = 0.00042189774103462696
iteration 238, loss = 0.0004268184711690992
iteration 239, loss = 0.0009753773338161409
iteration 240, loss = 0.0005720610497519374
iteration 241, loss = 0.0018643561052158475
iteration 242, loss = 0.0008447155123576522
iteration 243, loss = 0.0005864552222192287
iteration 244, loss = 0.0005652851541526616
iteration 245, loss = 0.001639894093386829
iteration 246, loss = 0.0007599276723340154
iteration 247, loss = 0.0008383792592212558
iteration 248, loss = 0.00036539818393066525
iteration 249, loss = 0.0008486699662171304
iteration 250, loss = 0.000793523620814085
iteration 251, loss = 0.0005212161340750754
iteration 252, loss = 0.0012665482936426997
iteration 253, loss = 0.0016968087293207645
iteration 254, loss = 0.0007437971653416753
iteration 255, loss = 0.0007086923578754067
iteration 256, loss = 0.0010941169457510114
iteration 257, loss = 0.0006974044954404235
iteration 258, loss = 0.0006769579485990107
iteration 259, loss = 0.0007085750112310052
iteration 260, loss = 0.0008607793133705854
iteration 261, loss = 0.0011102617718279362
iteration 262, loss = 0.00041077204514294863
iteration 263, loss = 0.0004256156680639833
iteration 264, loss = 0.0004022371140308678
iteration 265, loss = 0.0009281396050937474
iteration 266, loss = 0.0005207840004004538
iteration 267, loss = 0.0007833250565454364
iteration 268, loss = 0.0010861371411010623
iteration 269, loss = 0.001095274114049971
iteration 270, loss = 0.0006038442952558398
iteration 271, loss = 0.0008140993886627257
iteration 272, loss = 0.0006322225090116262
iteration 273, loss = 0.0006548501551151276
iteration 274, loss = 0.0008780324133113027
iteration 275, loss = 0.0004677683173213154
iteration 276, loss = 0.0011820473009720445
iteration 277, loss = 0.0005635589477606118
iteration 278, loss = 0.0005930172046646476
iteration 279, loss = 0.0008827699930407107
iteration 280, loss = 0.0019273904617875814
iteration 281, loss = 0.0006465857732109725
iteration 282, loss = 0.00040094347787089646
iteration 283, loss = 0.0007805654895491898
iteration 284, loss = 0.0003864819591399282
iteration 285, loss = 0.0006017669220454991
iteration 286, loss = 0.0004886226379312575
iteration 287, loss = 0.0007501114159822464
iteration 288, loss = 0.00036316487239673734
iteration 289, loss = 0.00044695031829178333
iteration 290, loss = 0.0004991535097360611
iteration 291, loss = 0.0004408507375046611
iteration 292, loss = 0.0006917942082509398
iteration 293, loss = 0.0007069769199006259
iteration 294, loss = 0.0006285269628278911
iteration 295, loss = 0.00033529038773849607
iteration 296, loss = 0.0005665763164870441
iteration 297, loss = 0.0003677571367006749
iteration 298, loss = 0.0004174764035269618
iteration 299, loss = 0.0019142673118039966
iteration 0, loss = 0.0008584202732890844
iteration 1, loss = 0.0009777909144759178
iteration 2, loss = 0.0004960988298989832
iteration 3, loss = 0.0004438211617525667
iteration 4, loss = 0.0006534322747029364
iteration 5, loss = 0.0005485500441864133
iteration 6, loss = 0.0009457562118768692
iteration 7, loss = 0.0005707696545869112
iteration 8, loss = 0.001002499833703041
iteration 9, loss = 0.0011252652620896697
iteration 10, loss = 0.0006306159775704145
iteration 11, loss = 0.0012566850055009127
iteration 12, loss = 0.000587129092309624
iteration 13, loss = 0.0007499054772779346
iteration 14, loss = 0.0003734534257091582
iteration 15, loss = 0.0006967921508476138
iteration 16, loss = 0.0018707314739003778
iteration 17, loss = 0.0004936216282658279
iteration 18, loss = 0.0007372964755631983
iteration 19, loss = 0.0005903718410991132
iteration 20, loss = 0.0017432291060686111
iteration 21, loss = 0.0006638438208028674
iteration 22, loss = 0.0008080382249318063
iteration 23, loss = 0.0007399009773507714
iteration 24, loss = 0.0005469644092954695
iteration 25, loss = 0.0011483868584036827
iteration 26, loss = 0.0011222339235246181
iteration 27, loss = 0.0008818617789074779
iteration 28, loss = 0.0006460185977630317
iteration 29, loss = 0.0005909046158194542
iteration 30, loss = 0.0005297168390825391
iteration 31, loss = 0.0007320927106775343
iteration 32, loss = 0.0007266397005878389
iteration 33, loss = 0.000503897259477526
iteration 34, loss = 0.0003630921710282564
iteration 35, loss = 0.00048550154315307736
iteration 36, loss = 0.0010898308828473091
iteration 37, loss = 0.00037830433575436473
iteration 38, loss = 0.0004839490575250238
iteration 39, loss = 0.0005065071163699031
iteration 40, loss = 0.0005825985572300851
iteration 41, loss = 0.0015941603342071176
iteration 42, loss = 0.0007597053772769868
iteration 43, loss = 0.0006354041397571564
iteration 44, loss = 0.00037198729114606977
iteration 45, loss = 0.0008168903877958655
iteration 46, loss = 0.0006007564370520413
iteration 47, loss = 0.001101933652535081
iteration 48, loss = 0.0006712991162203252
iteration 49, loss = 0.0004863795475102961
iteration 50, loss = 0.0003939722082577646
iteration 51, loss = 0.0005113696679472923
iteration 52, loss = 0.0008589096833020449
iteration 53, loss = 0.00038920502993278205
iteration 54, loss = 0.0004506722907535732
iteration 55, loss = 0.001018857816234231
iteration 56, loss = 0.0006225843681022525
iteration 57, loss = 0.0005044311983510852
iteration 58, loss = 0.0009500227170065045
iteration 59, loss = 0.0006442444864660501
iteration 60, loss = 0.000387257372494787
iteration 61, loss = 0.0005190655938349664
iteration 62, loss = 0.0005836095078848302
iteration 63, loss = 0.0005496692610904574
iteration 64, loss = 0.00046255701454356313
iteration 65, loss = 0.0003841295256279409
iteration 66, loss = 0.0004999414086341858
iteration 67, loss = 0.0008360791835002601
iteration 68, loss = 0.0005198387079872191
iteration 69, loss = 0.0006265282281674445
iteration 70, loss = 0.0003930173988919705
iteration 71, loss = 0.000625832995865494
iteration 72, loss = 0.0004136379575356841
iteration 73, loss = 0.0006108563393354416
iteration 74, loss = 0.0003624394885264337
iteration 75, loss = 0.0006655967445112765
iteration 76, loss = 0.0005775520694442093
iteration 77, loss = 0.0008139251731336117
iteration 78, loss = 0.00029580030241049826
iteration 79, loss = 0.00047503449604846537
iteration 80, loss = 0.000756527588237077
iteration 81, loss = 0.0008257306180894375
iteration 82, loss = 0.00043068965896964073
iteration 83, loss = 0.0002991229121107608
iteration 84, loss = 0.000568438321352005
iteration 85, loss = 0.0010931675788015127
iteration 86, loss = 0.0005669081583619118
iteration 87, loss = 0.0008426817948929965
iteration 88, loss = 0.0005886985454708338
iteration 89, loss = 0.0006275023333728313
iteration 90, loss = 0.0007661049603484571
iteration 91, loss = 0.0006981462938711047
iteration 92, loss = 0.0004925086977891624
iteration 93, loss = 0.0012197887990623713
iteration 94, loss = 0.0006673018797300756
iteration 95, loss = 0.0008936035446822643
iteration 96, loss = 0.0004270350909791887
iteration 97, loss = 0.00046948896488174796
iteration 98, loss = 0.0008893169579096138
iteration 99, loss = 0.000578216218855232
iteration 100, loss = 0.0011752956779673696
iteration 101, loss = 0.00043551716953516006
iteration 102, loss = 0.0005468096351251006
iteration 103, loss = 0.0007084953831508756
iteration 104, loss = 0.0017561579588800669
iteration 105, loss = 0.0012966771610081196
iteration 106, loss = 0.00046520435716956854
iteration 107, loss = 0.00041984274866990745
iteration 108, loss = 0.0004685087187681347
iteration 109, loss = 0.0006660416256636381
iteration 110, loss = 0.0005365134566091001
iteration 111, loss = 0.0006615830934606493
iteration 112, loss = 0.0005423342227004468
iteration 113, loss = 0.0004027563554700464
iteration 114, loss = 0.0007288387860171497
iteration 115, loss = 0.0005559578421525657
iteration 116, loss = 0.000594716053456068
iteration 117, loss = 0.00034441761090420187
iteration 118, loss = 0.00042451766785234213
iteration 119, loss = 0.0005911491462029517
iteration 120, loss = 0.0003427595365792513
iteration 121, loss = 0.0009294430492445827
iteration 122, loss = 0.0009874757379293442
iteration 123, loss = 0.001030043000355363
iteration 124, loss = 0.00040953795542009175
iteration 125, loss = 0.0010925274109467864
iteration 126, loss = 0.0006660895887762308
iteration 127, loss = 0.0005168137140572071
iteration 128, loss = 0.0015484616160392761
iteration 129, loss = 0.0005378820933401585
iteration 130, loss = 0.0005612198146991432
iteration 131, loss = 0.0004319380677770823
iteration 132, loss = 0.0007111593149602413
iteration 133, loss = 0.0011530934134498239
iteration 134, loss = 0.0006051516393199563
iteration 135, loss = 0.0008553179213777184
iteration 136, loss = 0.0006148831453174353
iteration 137, loss = 0.00046950188698247075
iteration 138, loss = 0.0005873529007658362
iteration 139, loss = 0.0007411083206534386
iteration 140, loss = 0.0007083416567184031
iteration 141, loss = 0.0005780532956123352
iteration 142, loss = 0.0008316354360431433
iteration 143, loss = 0.0007177681545726955
iteration 144, loss = 0.00036202743649482727
iteration 145, loss = 0.0008581929723732173
iteration 146, loss = 0.0006495037814602256
iteration 147, loss = 0.0007705523166805506
iteration 148, loss = 0.0007267094333656132
iteration 149, loss = 0.00037446236819960177
iteration 150, loss = 0.0013428914826363325
iteration 151, loss = 0.00048562680603936315
iteration 152, loss = 0.0005780021310783923
iteration 153, loss = 0.0010213748319074512
iteration 154, loss = 0.0003596252645365894
iteration 155, loss = 0.00047660304699093103
iteration 156, loss = 0.0003529152018018067
iteration 157, loss = 0.0008713601855561137
iteration 158, loss = 0.0008692409028299153
iteration 159, loss = 0.0005340693751350045
iteration 160, loss = 0.0004671220085583627
iteration 161, loss = 0.00035467330599203706
iteration 162, loss = 0.0003869954380206764
iteration 163, loss = 0.0006150415283627808
iteration 164, loss = 0.001182090025395155
iteration 165, loss = 0.0006618820480071008
iteration 166, loss = 0.0005511998315341771
iteration 167, loss = 0.00041873310692608356
iteration 168, loss = 0.0018658807966858149
iteration 169, loss = 0.0005470411852002144
iteration 170, loss = 0.0007994673796929419
iteration 171, loss = 0.0011420571245253086
iteration 172, loss = 0.00038188297185115516
iteration 173, loss = 0.0006378538673743606
iteration 174, loss = 0.0005020187236368656
iteration 175, loss = 0.0007885073428042233
iteration 176, loss = 0.0007279571727849543
iteration 177, loss = 0.0006087018409743905
iteration 178, loss = 0.0004277061962056905
iteration 179, loss = 0.0005205591442063451
iteration 180, loss = 0.0007025334052741528
iteration 181, loss = 0.0014807983534410596
iteration 182, loss = 0.0004096971242688596
iteration 183, loss = 0.0008192819077521563
iteration 184, loss = 0.0006355051882565022
iteration 185, loss = 0.0007564263651147485
iteration 186, loss = 0.0007198294042609632
iteration 187, loss = 0.0006087892688810825
iteration 188, loss = 0.0008135561365634203
iteration 189, loss = 0.00037179398350417614
iteration 190, loss = 0.00046475775889120996
iteration 191, loss = 0.0017726491205394268
iteration 192, loss = 0.0004603175329975784
iteration 193, loss = 0.00048264648648910224
iteration 194, loss = 0.001965655479580164
iteration 195, loss = 0.0004357870202511549
iteration 196, loss = 0.0003967065713368356
iteration 197, loss = 0.0006337694358080626
iteration 198, loss = 0.00042956930701620877
iteration 199, loss = 0.0006313509657047689
iteration 200, loss = 0.0008489487809129059
iteration 201, loss = 0.0005803544190712273
iteration 202, loss = 0.0006194122834131122
iteration 203, loss = 0.0005017952062189579
iteration 204, loss = 0.0008449830929748714
iteration 205, loss = 0.0010501674842089415
iteration 206, loss = 0.0007280828431248665
iteration 207, loss = 0.000981078832410276
iteration 208, loss = 0.00040748686296865344
iteration 209, loss = 0.0006924907793290913
iteration 210, loss = 0.0018442805157974362
iteration 211, loss = 0.0009316119248978794
iteration 212, loss = 0.0006158663309179246
iteration 213, loss = 0.0005695869331248105
iteration 214, loss = 0.0007257834658958018
iteration 215, loss = 0.001692992402240634
iteration 216, loss = 0.00038206507451832294
iteration 217, loss = 0.0016042832285165787
iteration 218, loss = 0.001680965768173337
iteration 219, loss = 0.00037811382208019495
iteration 220, loss = 0.0010055705206468701
iteration 221, loss = 0.0007481324719265103
iteration 222, loss = 0.0004954163450747728
iteration 223, loss = 0.0016497918404638767
iteration 224, loss = 0.0003823389997705817
iteration 225, loss = 0.00046288903104141355
iteration 226, loss = 0.0010223769349977374
iteration 227, loss = 0.0006223628297448158
iteration 228, loss = 0.0006961064063943923
iteration 229, loss = 0.0003672222956083715
iteration 230, loss = 0.0005003669066354632
iteration 231, loss = 0.0005703899660147727
iteration 232, loss = 0.0005690568941645324
iteration 233, loss = 0.0005481353728100657
iteration 234, loss = 0.0006214813911356032
iteration 235, loss = 0.000707367667928338
iteration 236, loss = 0.0006603003712370992
iteration 237, loss = 0.0004037508915644139
iteration 238, loss = 0.0004189294995740056
iteration 239, loss = 0.0004008527903351933
iteration 240, loss = 0.0008399129728786647
iteration 241, loss = 0.0006387683097273111
iteration 242, loss = 0.0010268642799928784
iteration 243, loss = 0.000673300470225513
iteration 244, loss = 0.0027548426296561956
iteration 245, loss = 0.00048617832362651825
iteration 246, loss = 0.0003483662148937583
iteration 247, loss = 0.00048454193165525794
iteration 248, loss = 0.0017173168016597629
iteration 249, loss = 0.000982716679573059
iteration 250, loss = 0.0007090111612342298
iteration 251, loss = 0.0010369069641456008
iteration 252, loss = 0.0005804505199193954
iteration 253, loss = 0.0011089423205703497
iteration 254, loss = 0.0010929220588877797
iteration 255, loss = 0.0003048576763831079
iteration 256, loss = 0.00046866945922374725
iteration 257, loss = 0.00041006863466463983
iteration 258, loss = 0.0009341175318695605
iteration 259, loss = 0.0005064940778538585
iteration 260, loss = 0.0006712307222187519
iteration 261, loss = 0.00040093064308166504
iteration 262, loss = 0.00047258383710868657
iteration 263, loss = 0.0007840223843231797
iteration 264, loss = 0.00048145081382244825
iteration 265, loss = 0.0010402477346360683
iteration 266, loss = 0.0007129408768378198
iteration 267, loss = 0.00040691642789170146
iteration 268, loss = 0.000685857841745019
iteration 269, loss = 0.0005558510310947895
iteration 270, loss = 0.0006104107014834881
iteration 271, loss = 0.0004890450509265065
iteration 272, loss = 0.00048818349023349583
iteration 273, loss = 0.0005873678019270301
iteration 274, loss = 0.0008844046969898045
iteration 275, loss = 0.0005838391371071339
iteration 276, loss = 0.0015867269830778241
iteration 277, loss = 0.0006085237255319953
iteration 278, loss = 0.0014963593566790223
iteration 279, loss = 0.0004587332659866661
iteration 280, loss = 0.0004459326446522027
iteration 281, loss = 0.0009511777898296714
iteration 282, loss = 0.0006868918426334858
iteration 283, loss = 0.00037454694393090904
iteration 284, loss = 0.000667161017190665
iteration 285, loss = 0.0006176323513500392
iteration 286, loss = 0.001575905131176114
iteration 287, loss = 0.00043873838149011135
iteration 288, loss = 0.0011037730146199465
iteration 289, loss = 0.0005357681075111032
iteration 290, loss = 0.00058457424165681
iteration 291, loss = 0.0005462710396386683
iteration 292, loss = 0.00036886311136186123
iteration 293, loss = 0.00043406771146692336
iteration 294, loss = 0.0003749965981114656
iteration 295, loss = 0.0006026837509125471
iteration 296, loss = 0.0004888203693553805
iteration 297, loss = 0.0003923502517864108
iteration 298, loss = 0.00048554339446127415
iteration 299, loss = 0.0006556371226906776
iteration 0, loss = 0.0005481151747517288
iteration 1, loss = 0.00037533044815063477
iteration 2, loss = 0.0007847135420888662
iteration 3, loss = 0.0008171109948307276
iteration 4, loss = 0.0005757583421654999
iteration 5, loss = 0.00032953289337456226
iteration 6, loss = 0.0006622973596677184
iteration 7, loss = 0.0004753386601805687
iteration 8, loss = 0.0006221802905201912
iteration 9, loss = 0.00035268603824079037
iteration 10, loss = 0.0004117153584957123
iteration 11, loss = 0.0007055455935187638
iteration 12, loss = 0.0005577697302214801
iteration 13, loss = 0.0007644781726412475
iteration 14, loss = 0.0006160662742331624
iteration 15, loss = 0.0009362620767205954
iteration 16, loss = 0.001850356813520193
iteration 17, loss = 0.00040900130989030004
iteration 18, loss = 0.00045440124813467264
iteration 19, loss = 0.00041239490383304656
iteration 20, loss = 0.0004086257249582559
iteration 21, loss = 0.0007466885726898909
iteration 22, loss = 0.0004633630160242319
iteration 23, loss = 0.0014409954892471433
iteration 24, loss = 0.000528372940607369
iteration 25, loss = 0.00041173631325364113
iteration 26, loss = 0.0005535906529985368
iteration 27, loss = 0.0006984362844377756
iteration 28, loss = 0.0008565733442083001
iteration 29, loss = 0.0009587687673047185
iteration 30, loss = 0.0006117227021604776
iteration 31, loss = 0.0005346311372704804
iteration 32, loss = 0.0006075082346796989
iteration 33, loss = 0.0005547719774767756
iteration 34, loss = 0.0008232810068875551
iteration 35, loss = 0.0006929491646587849
iteration 36, loss = 0.0006757953087799251
iteration 37, loss = 0.00048295853775925934
iteration 38, loss = 0.0005259820027276874
iteration 39, loss = 0.0005571437650360167
iteration 40, loss = 0.00030154758132994175
iteration 41, loss = 0.0010902268113568425
iteration 42, loss = 0.0004834734427276999
iteration 43, loss = 0.0008007531869225204
iteration 44, loss = 0.0006446869228966534
iteration 45, loss = 0.0005566378240473568
iteration 46, loss = 0.0004731957451440394
iteration 47, loss = 0.00035326884244568646
iteration 48, loss = 0.00034663721453398466
iteration 49, loss = 0.0006281850510276854
iteration 50, loss = 0.0007734741666354239
iteration 51, loss = 0.0005754255107603967
iteration 52, loss = 0.00035966740688309073
iteration 53, loss = 0.0004493810411076993
iteration 54, loss = 0.00046260899398475885
iteration 55, loss = 0.0006418896955437958
iteration 56, loss = 0.0008402967941947281
iteration 57, loss = 0.00036678448668681085
iteration 58, loss = 0.00034167172270826995
iteration 59, loss = 0.00033950942452065647
iteration 60, loss = 0.0020564363803714514
iteration 61, loss = 0.00043979071779176593
iteration 62, loss = 0.0011837026104331017
iteration 63, loss = 0.0007252240320667624
iteration 64, loss = 0.0007135597988963127
iteration 65, loss = 0.0004968497669324279
iteration 66, loss = 0.0004713173257187009
iteration 67, loss = 0.000319451472023502
iteration 68, loss = 0.0009448094642721117
iteration 69, loss = 0.0006031280499882996
iteration 70, loss = 0.0004484537639655173
iteration 71, loss = 0.000596860540099442
iteration 72, loss = 0.0007156141218729317
iteration 73, loss = 0.00162198964972049
iteration 74, loss = 0.0010704905726015568
iteration 75, loss = 0.00047787255607545376
iteration 76, loss = 0.0010207763407379389
iteration 77, loss = 0.0003770013281609863
iteration 78, loss = 0.0005227403016760945
iteration 79, loss = 0.001329035614617169
iteration 80, loss = 0.0005170372896827757
iteration 81, loss = 0.000732766289729625
iteration 82, loss = 0.0009122706251218915
iteration 83, loss = 0.00035341971670277417
iteration 84, loss = 0.0008250706596300006
iteration 85, loss = 0.0006479842122644186
iteration 86, loss = 0.0005162261077202857
iteration 87, loss = 0.0003158554609399289
iteration 88, loss = 0.0005416086059994996
iteration 89, loss = 0.0007905163802206516
iteration 90, loss = 0.00057311198906973
iteration 91, loss = 0.00039766603731550276
iteration 92, loss = 0.0004945815308019519
iteration 93, loss = 0.0003109368262812495
iteration 94, loss = 0.0006184948142617941
iteration 95, loss = 0.0017146283062174916
iteration 96, loss = 0.0010515623725950718
iteration 97, loss = 0.0006687215645797551
iteration 98, loss = 0.0011446344433352351
iteration 99, loss = 0.00040507031371816993
iteration 100, loss = 0.0004487400292418897
iteration 101, loss = 0.0005821427912451327
iteration 102, loss = 0.00032152608036994934
iteration 103, loss = 0.0011204271577298641
iteration 104, loss = 0.00039423900307156146
iteration 105, loss = 0.000497223692946136
iteration 106, loss = 0.00071187992580235
iteration 107, loss = 0.0010744082974269986
iteration 108, loss = 0.0005434187478385866
iteration 109, loss = 0.00048004239215515554
iteration 110, loss = 0.0006278280634433031
iteration 111, loss = 0.00047954003093764186
iteration 112, loss = 0.0014061813708394766
iteration 113, loss = 0.0007315474795177579
iteration 114, loss = 0.0008106670575216413
iteration 115, loss = 0.00037210623850114644
iteration 116, loss = 0.000774275278672576
iteration 117, loss = 0.0005253602867014706
iteration 118, loss = 0.0003234197210986167
iteration 119, loss = 0.0004357761354185641
iteration 120, loss = 0.000546115159522742
iteration 121, loss = 0.0006120530306361616
iteration 122, loss = 0.000754680426325649
iteration 123, loss = 0.0005009364103898406
iteration 124, loss = 0.0008391744340769947
iteration 125, loss = 0.0006487344508059323
iteration 126, loss = 0.0007337212446145713
iteration 127, loss = 0.00041980823152698576
iteration 128, loss = 0.0005671951803378761
iteration 129, loss = 0.0008928598836064339
iteration 130, loss = 0.0004604579880833626
iteration 131, loss = 0.000372809823602438
iteration 132, loss = 0.0011412848252803087
iteration 133, loss = 0.0006224307580851018
iteration 134, loss = 0.0016301936702802777
iteration 135, loss = 0.000534702034201473
iteration 136, loss = 0.00043940136674791574
iteration 137, loss = 0.0007202031556516886
iteration 138, loss = 0.0006331840995699167
iteration 139, loss = 0.00038493439205922186
iteration 140, loss = 0.00043265052954666317
iteration 141, loss = 0.0007923064404167235
iteration 142, loss = 0.0006412495858967304
iteration 143, loss = 0.0015926787164062262
iteration 144, loss = 0.0006196363246999681
iteration 145, loss = 0.0007100735092535615
iteration 146, loss = 0.0005051767802797258
iteration 147, loss = 0.001157275284640491
iteration 148, loss = 0.00040027324575930834
iteration 149, loss = 0.0007317352574318647
iteration 150, loss = 0.00036736036418005824
iteration 151, loss = 0.00040425933548249304
iteration 152, loss = 0.0005713980062864721
iteration 153, loss = 0.0006356571102514863
iteration 154, loss = 0.00046532636042684317
iteration 155, loss = 0.0003630347200669348
iteration 156, loss = 0.000917514378670603
iteration 157, loss = 0.0009186634561046958
iteration 158, loss = 0.0004916355246677995
iteration 159, loss = 0.0017802658258005977
iteration 160, loss = 0.0007296333787962794
iteration 161, loss = 0.0017289830138906837
iteration 162, loss = 0.000818696862552315
iteration 163, loss = 0.00036664382787421346
iteration 164, loss = 0.0003583848010748625
iteration 165, loss = 0.0019090519053861499
iteration 166, loss = 0.0003478095168247819
iteration 167, loss = 0.0019320747815072536
iteration 168, loss = 0.0009822793072089553
iteration 169, loss = 0.0006796986563131213
iteration 170, loss = 0.0011845725821331143
iteration 171, loss = 0.0014653197722509503
iteration 172, loss = 0.001007742015644908
iteration 173, loss = 0.0005147173069417477
iteration 174, loss = 0.0008086995803751051
iteration 175, loss = 0.0004823111230507493
iteration 176, loss = 0.0008650791714899242
iteration 177, loss = 0.0004255069070495665
iteration 178, loss = 0.0005182754248380661
iteration 179, loss = 0.0006343346904031932
iteration 180, loss = 0.0006189498235471547
iteration 181, loss = 0.0007393247215077281
iteration 182, loss = 0.0004146372666582465
iteration 183, loss = 0.0006330701289698482
iteration 184, loss = 0.0005900571122765541
iteration 185, loss = 0.0007534933392889798
iteration 186, loss = 0.000965695537161082
iteration 187, loss = 0.00043361823190934956
iteration 188, loss = 0.0006264061667025089
iteration 189, loss = 0.0008970434428192675
iteration 190, loss = 0.000322227890137583
iteration 191, loss = 0.0005377109628170729
iteration 192, loss = 0.000543383473996073
iteration 193, loss = 0.0004810427490156144
iteration 194, loss = 0.00039744062814861536
iteration 195, loss = 0.0005424941191449761
iteration 196, loss = 0.001679490553215146
iteration 197, loss = 0.0006360365077853203
iteration 198, loss = 0.00046504189958795905
iteration 199, loss = 0.0004626048030331731
iteration 200, loss = 0.0005808657151646912
iteration 201, loss = 0.0005623114411719143
iteration 202, loss = 0.00040747717139311135
iteration 203, loss = 0.00029066394199617207
iteration 204, loss = 0.0005498977843672037
iteration 205, loss = 0.0008659534505568445
iteration 206, loss = 0.000957823300268501
iteration 207, loss = 0.0006182618089951575
iteration 208, loss = 0.00040696607902646065
iteration 209, loss = 0.0006830626516602933
iteration 210, loss = 0.0006616726750507951
iteration 211, loss = 0.0004302507149986923
iteration 212, loss = 0.0009298282093368471
iteration 213, loss = 0.0005778739578090608
iteration 214, loss = 0.0014779109042137861
iteration 215, loss = 0.00045130608486942947
iteration 216, loss = 0.0005240045138634741
iteration 217, loss = 0.0005158985150046647
iteration 218, loss = 0.0004216266970615834
iteration 219, loss = 0.0008751316927373409
iteration 220, loss = 0.000711411121301353
iteration 221, loss = 0.0004276256950106472
iteration 222, loss = 0.00043294636998325586
iteration 223, loss = 0.0005265348590910435
iteration 224, loss = 0.0009755286737345159
iteration 225, loss = 0.0004283736343495548
iteration 226, loss = 0.0004295570543035865
iteration 227, loss = 0.0004270040662959218
iteration 228, loss = 0.0006309449672698975
iteration 229, loss = 0.00033408976742066443
iteration 230, loss = 0.0007096987101249397
iteration 231, loss = 0.0005787867121398449
iteration 232, loss = 0.0008016254869289696
iteration 233, loss = 0.001568663865327835
iteration 234, loss = 0.00039049575570970774
iteration 235, loss = 0.0005310776177793741
iteration 236, loss = 0.0008869771263562143
iteration 237, loss = 0.0008649200899526477
iteration 238, loss = 0.000672749534714967
iteration 239, loss = 0.0006952049443498254
iteration 240, loss = 0.0004209822218399495
iteration 241, loss = 0.00034628776484169066
iteration 242, loss = 0.0005686618387699127
iteration 243, loss = 0.0005678330780938268
iteration 244, loss = 0.0005773241282440722
iteration 245, loss = 0.00044793394044972956
iteration 246, loss = 0.0021817374508827925
iteration 247, loss = 0.0016110135475173593
iteration 248, loss = 0.0009996362496167421
iteration 249, loss = 0.0014609391801059246
iteration 250, loss = 0.0006491504609584808
iteration 251, loss = 0.0006288638105615973
iteration 252, loss = 0.0003880498406942934
iteration 253, loss = 0.0006747532752342522
iteration 254, loss = 0.00037230929592624307
iteration 255, loss = 0.0006959943566471338
iteration 256, loss = 0.0008301484631374478
iteration 257, loss = 0.0007962192757986486
iteration 258, loss = 0.0003585059894248843
iteration 259, loss = 0.0005118061671964824
iteration 260, loss = 0.0005509888287633657
iteration 261, loss = 0.0004539836663752794
iteration 262, loss = 0.0004759033618029207
iteration 263, loss = 0.0004474938032217324
iteration 264, loss = 0.00042239786125719547
iteration 265, loss = 0.0009181997156701982
iteration 266, loss = 0.00041995805804617703
iteration 267, loss = 0.000836321385577321
iteration 268, loss = 0.0010177742224186659
iteration 269, loss = 0.0005978418048471212
iteration 270, loss = 0.0004840016772504896
iteration 271, loss = 0.0004735681868623942
iteration 272, loss = 0.0005903443088755012
iteration 273, loss = 0.0012085707858204842
iteration 274, loss = 0.0005039034294895828
iteration 275, loss = 0.0009255952900275588
iteration 276, loss = 0.00031777878757566214
iteration 277, loss = 0.0005685697542503476
iteration 278, loss = 0.00045337993651628494
iteration 279, loss = 0.0007498793420381844
iteration 280, loss = 0.00044746120693162084
iteration 281, loss = 0.0009737672517076135
iteration 282, loss = 0.0005960061680525541
iteration 283, loss = 0.0005597717245109379
iteration 284, loss = 0.0006575353327207267
iteration 285, loss = 0.0004734228423330933
iteration 286, loss = 0.0004694013623520732
iteration 287, loss = 0.0006239221547730267
iteration 288, loss = 0.0009857519762590528
iteration 289, loss = 0.0005547998589463532
iteration 290, loss = 0.0005556009709835052
iteration 291, loss = 0.00040403526509180665
iteration 292, loss = 0.00045300633064471185
iteration 293, loss = 0.0005566061590798199
iteration 294, loss = 0.0010402500629425049
iteration 295, loss = 0.000739828625228256
iteration 296, loss = 0.00037951942067593336
iteration 297, loss = 0.0003914466069545597
iteration 298, loss = 0.0006349555333144963
iteration 299, loss = 0.00032806856324896216
iteration 0, loss = 0.00038485226104967296
iteration 1, loss = 0.0016902367351576686
iteration 2, loss = 0.0004004902730230242
iteration 3, loss = 0.000731030770111829
iteration 4, loss = 0.0004697343392763287
iteration 5, loss = 0.0010791317326948047
iteration 6, loss = 0.0008949084440246224
iteration 7, loss = 0.00048617401625961065
iteration 8, loss = 0.0003761500120162964
iteration 9, loss = 0.0005360082723200321
iteration 10, loss = 0.0006654729950241745
iteration 11, loss = 0.0003867726190946996
iteration 12, loss = 0.000592585769481957
iteration 13, loss = 0.0007610534084960818
iteration 14, loss = 0.0006223951932042837
iteration 15, loss = 0.00047280400758609176
iteration 16, loss = 0.0004889242118224502
iteration 17, loss = 0.0004212268686387688
iteration 18, loss = 0.0005282340571284294
iteration 19, loss = 0.0005415639607235789
iteration 20, loss = 0.0005501818959601223
iteration 21, loss = 0.0004771757812704891
iteration 22, loss = 0.00045549095375463367
iteration 23, loss = 0.0003697781066875905
iteration 24, loss = 0.0005245432257652283
iteration 25, loss = 0.0007459732005372643
iteration 26, loss = 0.0005218551959842443
iteration 27, loss = 0.0006431269575841725
iteration 28, loss = 0.00038714244146831334
iteration 29, loss = 0.001408731215633452
iteration 30, loss = 0.0003845960018225014
iteration 31, loss = 0.0006664624088443816
iteration 32, loss = 0.00031436310382559896
iteration 33, loss = 0.00047200050903484225
iteration 34, loss = 0.0011136987013742328
iteration 35, loss = 0.0005412001628428698
iteration 36, loss = 0.0005305574741214514
iteration 37, loss = 0.0006128594977781177
iteration 38, loss = 0.0007310039945878088
iteration 39, loss = 0.0008674617274664342
iteration 40, loss = 0.0019639867823570967
iteration 41, loss = 0.0006560284527949989
iteration 42, loss = 0.0007744543836452067
iteration 43, loss = 0.0005286142113618553
iteration 44, loss = 0.0005500412080436945
iteration 45, loss = 0.0021323736291378736
iteration 46, loss = 0.0003382862196303904
iteration 47, loss = 0.0008022217662073672
iteration 48, loss = 0.000728542567230761
iteration 49, loss = 0.00044552682084031403
iteration 50, loss = 0.0004926147521473467
iteration 51, loss = 0.0003028872888535261
iteration 52, loss = 0.0006664115353487432
iteration 53, loss = 0.0010940569918602705
iteration 54, loss = 0.0003864132158923894
iteration 55, loss = 0.0003798665420617908
iteration 56, loss = 0.0004108090652152896
iteration 57, loss = 0.0005011959583498538
iteration 58, loss = 0.0013928412226960063
iteration 59, loss = 0.0004738568386528641
iteration 60, loss = 0.00034306428278796375
iteration 61, loss = 0.0007066706893965602
iteration 62, loss = 0.0009766084840521216
iteration 63, loss = 0.0008009967859834433
iteration 64, loss = 0.0004069741989951581
iteration 65, loss = 0.0016562917735427618
iteration 66, loss = 0.0005615615518763661
iteration 67, loss = 0.0005315867019817233
iteration 68, loss = 0.00035450156428851187
iteration 69, loss = 0.0006218309863470495
iteration 70, loss = 0.0006122438935562968
iteration 71, loss = 0.00045640766620635986
iteration 72, loss = 0.000262705230852589
iteration 73, loss = 0.00025846462813206017
iteration 74, loss = 0.00046413912787102163
iteration 75, loss = 0.0007846924709156156
iteration 76, loss = 0.0009156012092716992
iteration 77, loss = 0.001541351666674018
iteration 78, loss = 0.0008313775761052966
iteration 79, loss = 0.0007331434753723443
iteration 80, loss = 0.0004547399003058672
iteration 81, loss = 0.0009505627676844597
iteration 82, loss = 0.0006942010950297117
iteration 83, loss = 0.0004288859199732542
iteration 84, loss = 0.0006326419534161687
iteration 85, loss = 0.0005397053901106119
iteration 86, loss = 0.0002886407892219722
iteration 87, loss = 0.0005056333029642701
iteration 88, loss = 0.0008428941364400089
iteration 89, loss = 0.000448435137514025
iteration 90, loss = 0.0020770884584635496
iteration 91, loss = 0.0004607856390066445
iteration 92, loss = 0.0004529645957518369
iteration 93, loss = 0.000812417478300631
iteration 94, loss = 0.0006388117326423526
iteration 95, loss = 0.00048765557585284114
iteration 96, loss = 0.0004280472348909825
iteration 97, loss = 0.00048475980293005705
iteration 98, loss = 0.0005415308405645192
iteration 99, loss = 0.0004930555587634444
iteration 100, loss = 0.0006759777897968888
iteration 101, loss = 0.0003944195050280541
iteration 102, loss = 0.0005055119981989264
iteration 103, loss = 0.00045992829836905
iteration 104, loss = 0.0005627708742395043
iteration 105, loss = 0.0005550757050514221
iteration 106, loss = 0.0006513241096399724
iteration 107, loss = 0.0005847219144925475
iteration 108, loss = 0.000531539146322757
iteration 109, loss = 0.0004572552861645818
iteration 110, loss = 0.00045354722533375025
iteration 111, loss = 0.000441867858171463
iteration 112, loss = 0.0006775899673812091
iteration 113, loss = 0.00037768628681078553
iteration 114, loss = 0.0006143688224256039
iteration 115, loss = 0.0009818854741752148
iteration 116, loss = 0.0009080562740564346
iteration 117, loss = 0.0006939275190234184
iteration 118, loss = 0.0006462640012614429
iteration 119, loss = 0.0014010838931426406
iteration 120, loss = 0.0007028952823020518
iteration 121, loss = 0.0010680268751457334
iteration 122, loss = 0.0008805054239928722
iteration 123, loss = 0.0011291441041976213
iteration 124, loss = 0.00038606932503171265
iteration 125, loss = 0.0004102056846022606
iteration 126, loss = 0.0003449325740803033
iteration 127, loss = 0.0005370209692046046
iteration 128, loss = 0.00046459626173600554
iteration 129, loss = 0.0007975869229994714
iteration 130, loss = 0.00039974573883228004
iteration 131, loss = 0.00046448269858956337
iteration 132, loss = 0.000578078324906528
iteration 133, loss = 0.0015113367699086666
iteration 134, loss = 0.00046855874825268984
iteration 135, loss = 0.0003501239698380232
iteration 136, loss = 0.0009543566266074777
iteration 137, loss = 0.00029676954727619886
iteration 138, loss = 0.00042785334517247975
iteration 139, loss = 0.00035700039006769657
iteration 140, loss = 0.0003320851537864655
iteration 141, loss = 0.0010051586432382464
iteration 142, loss = 0.000668202294036746
iteration 143, loss = 0.0005947500467300415
iteration 144, loss = 0.0003369180194567889
iteration 145, loss = 0.00036785879638046026
iteration 146, loss = 0.0006854924140498042
iteration 147, loss = 0.0007980188820511103
iteration 148, loss = 0.0004862058558501303
iteration 149, loss = 0.0005315685411915183
iteration 150, loss = 0.0007701413123868406
iteration 151, loss = 0.001467615831643343
iteration 152, loss = 0.0016034541185945272
iteration 153, loss = 0.000642180151771754
iteration 154, loss = 0.00046552595449611545
iteration 155, loss = 0.0004514916508924216
iteration 156, loss = 0.0004996557254344225
iteration 157, loss = 0.0007645916775800288
iteration 158, loss = 0.000726351689081639
iteration 159, loss = 0.0005534370429813862
iteration 160, loss = 0.0006369483307935297
iteration 161, loss = 0.0003784711589105427
iteration 162, loss = 0.0004760879382956773
iteration 163, loss = 0.0005838663782924414
iteration 164, loss = 0.0006558892782777548
iteration 165, loss = 0.0003597544855438173
iteration 166, loss = 0.00032269128132611513
iteration 167, loss = 0.0006537712179124355
iteration 168, loss = 0.0005008585867471993
iteration 169, loss = 0.0003162675420753658
iteration 170, loss = 0.0013904923107475042
iteration 171, loss = 0.00039689367986284196
iteration 172, loss = 0.0010141065577045083
iteration 173, loss = 0.0005231127142906189
iteration 174, loss = 0.0004624668217729777
iteration 175, loss = 0.0006051938398741186
iteration 176, loss = 0.0013910946436226368
iteration 177, loss = 0.0009755479404702783
iteration 178, loss = 0.0005639638402499259
iteration 179, loss = 0.00040086903027258813
iteration 180, loss = 0.0004175940703134984
iteration 181, loss = 0.00039913778891786933
iteration 182, loss = 0.0007901924545876682
iteration 183, loss = 0.0014894255436956882
iteration 184, loss = 0.00048467449960298836
iteration 185, loss = 0.0003974353603553027
iteration 186, loss = 0.00047844694927334785
iteration 187, loss = 0.0003963092458434403
iteration 188, loss = 0.0003451760276220739
iteration 189, loss = 0.0005201224703341722
iteration 190, loss = 0.000571909942664206
iteration 191, loss = 0.0005856690113432705
iteration 192, loss = 0.0004346727509982884
iteration 193, loss = 0.0005461235414259136
iteration 194, loss = 0.00034237204818055034
iteration 195, loss = 0.00037384816096164286
iteration 196, loss = 0.0006194502348080277
iteration 197, loss = 0.0004504211829043925
iteration 198, loss = 0.0007210875046439469
iteration 199, loss = 0.00041689988574944437
iteration 200, loss = 0.00030570742092095315
iteration 201, loss = 0.001046217861585319
iteration 202, loss = 0.0005284353974275291
iteration 203, loss = 0.0005040463292971253
iteration 204, loss = 0.0007279102574102581
iteration 205, loss = 0.0016556488117203116
iteration 206, loss = 0.00032234296668320894
iteration 207, loss = 0.0006004386814311147
iteration 208, loss = 0.0009833197109401226
iteration 209, loss = 0.0005759917548857629
iteration 210, loss = 0.00037978889304213226
iteration 211, loss = 0.0006883709575049579
iteration 212, loss = 0.0006496893474832177
iteration 213, loss = 0.0009874108945950866
iteration 214, loss = 0.0004877902683801949
iteration 215, loss = 0.0004802978364750743
iteration 216, loss = 0.0006590331322513521
iteration 217, loss = 0.0005090542254038155
iteration 218, loss = 0.001096491701900959
iteration 219, loss = 0.0011133123189210892
iteration 220, loss = 0.0004199694376438856
iteration 221, loss = 0.0006424408056773245
iteration 222, loss = 0.0008733419235795736
iteration 223, loss = 0.00048276185407303274
iteration 224, loss = 0.0005595357506535947
iteration 225, loss = 0.000291215896140784
iteration 226, loss = 0.000681233243085444
iteration 227, loss = 0.0004733846872113645
iteration 228, loss = 0.0008172204252332449
iteration 229, loss = 0.0005310717970132828
iteration 230, loss = 0.0004900656058453023
iteration 231, loss = 0.000847942428663373
iteration 232, loss = 0.0005341737414710224
iteration 233, loss = 0.0006148297688923776
iteration 234, loss = 0.0004756013222504407
iteration 235, loss = 0.00042971415678039193
iteration 236, loss = 0.0009740691748447716
iteration 237, loss = 0.00041345873614773154
iteration 238, loss = 0.0006652415031567216
iteration 239, loss = 0.00038134021451696754
iteration 240, loss = 0.0007638945826329291
iteration 241, loss = 0.0005694803548976779
iteration 242, loss = 0.0008304866496473551
iteration 243, loss = 0.0005832374445162714
iteration 244, loss = 0.0005385514814406633
iteration 245, loss = 0.0008139205747283995
iteration 246, loss = 0.00030966068152338266
iteration 247, loss = 0.0012208165135234594
iteration 248, loss = 0.0005035860813222826
iteration 249, loss = 0.00044937082566320896
iteration 250, loss = 0.00048580640577711165
iteration 251, loss = 0.0004831980913877487
iteration 252, loss = 0.0003753591445274651
iteration 253, loss = 0.001087609794922173
iteration 254, loss = 0.0005724324728362262
iteration 255, loss = 0.0006230254657566547
iteration 256, loss = 0.0006183377117849886
iteration 257, loss = 0.0005113817751407623
iteration 258, loss = 0.0008055234793573618
iteration 259, loss = 0.0004226589808240533
iteration 260, loss = 0.0007745723705738783
iteration 261, loss = 0.0009268964058719575
iteration 262, loss = 0.00047164433635771275
iteration 263, loss = 0.00046396785182878375
iteration 264, loss = 0.0005739639746025205
iteration 265, loss = 0.0006445429753512144
iteration 266, loss = 0.0005019016680307686
iteration 267, loss = 0.0003270279266871512
iteration 268, loss = 0.0009028112981468439
iteration 269, loss = 0.0003580243210308254
iteration 270, loss = 0.00046179036144167185
iteration 271, loss = 0.0010195353534072638
iteration 272, loss = 0.00037374443490989506
iteration 273, loss = 0.0003764001012314111
iteration 274, loss = 0.0007586066494695842
iteration 275, loss = 0.000457409507362172
iteration 276, loss = 0.0004360522434581071
iteration 277, loss = 0.0005319623160175979
iteration 278, loss = 0.0005438071675598621
iteration 279, loss = 0.0006790281622670591
iteration 280, loss = 0.0004124511615373194
iteration 281, loss = 0.00243210606276989
iteration 282, loss = 0.0002922142739407718
iteration 283, loss = 0.00031973011209629476
iteration 284, loss = 0.0010652854107320309
iteration 285, loss = 0.000892054638825357
iteration 286, loss = 0.0004815512802451849
iteration 287, loss = 0.000551623641513288
iteration 288, loss = 0.001571762259118259
iteration 289, loss = 0.0003702351823449135
iteration 290, loss = 0.0013490221463143826
iteration 291, loss = 0.0004024083027616143
iteration 292, loss = 0.0003959250752814114
iteration 293, loss = 0.0008472820045426488
iteration 294, loss = 0.0007878579199314117
iteration 295, loss = 0.0004284032038412988
iteration 296, loss = 0.000591510848607868
iteration 297, loss = 0.00045029405737295747
iteration 298, loss = 0.00045493460493162274
iteration 299, loss = 0.00046585703967139125
iteration 0, loss = 0.00034264902933500707
iteration 1, loss = 0.0004434167640283704
iteration 2, loss = 0.0007907829130999744
iteration 3, loss = 0.0002554412349127233
iteration 4, loss = 0.0004226026649121195
iteration 5, loss = 0.0005602574674412608
iteration 6, loss = 0.000397380325011909
iteration 7, loss = 0.00044643261935561895
iteration 8, loss = 0.0005393415922299027
iteration 9, loss = 0.0006441315053962171
iteration 10, loss = 0.00042954651871696115
iteration 11, loss = 0.0006067935610190034
iteration 12, loss = 0.0005141374422237277
iteration 13, loss = 0.0003051353560294956
iteration 14, loss = 0.0006671393057331443
iteration 15, loss = 0.0005722481873817742
iteration 16, loss = 0.00046811660286039114
iteration 17, loss = 0.0003573969588615
iteration 18, loss = 0.0009276391356252134
iteration 19, loss = 0.0002979896089527756
iteration 20, loss = 0.0007052609580568969
iteration 21, loss = 0.00032214901875704527
iteration 22, loss = 0.00042704353109002113
iteration 23, loss = 0.0009910353692248464
iteration 24, loss = 0.0008719287579879165
iteration 25, loss = 0.0003951824037358165
iteration 26, loss = 0.00039237391320057213
iteration 27, loss = 0.0005043644923716784
iteration 28, loss = 0.0004325773916207254
iteration 29, loss = 0.0006597607862204313
iteration 30, loss = 0.0008544549345970154
iteration 31, loss = 0.000557884864974767
iteration 32, loss = 0.0004008380929008126
iteration 33, loss = 0.0013971091248095036
iteration 34, loss = 0.0005066260928288102
iteration 35, loss = 0.0006283457623794675
iteration 36, loss = 0.0005839707446284592
iteration 37, loss = 0.0007172304904088378
iteration 38, loss = 0.0005723134381696582
iteration 39, loss = 0.0005988243501633406
iteration 40, loss = 0.001387590542435646
iteration 41, loss = 0.0004782566102221608
iteration 42, loss = 0.0006258615176193416
iteration 43, loss = 0.0005499450489878654
iteration 44, loss = 0.00031451089307665825
iteration 45, loss = 0.00039942347211763263
iteration 46, loss = 0.000866832910105586
iteration 47, loss = 0.00040444641490466893
iteration 48, loss = 0.00040783078293316066
iteration 49, loss = 0.0007122745155356824
iteration 50, loss = 0.0006365185836330056
iteration 51, loss = 0.00047076138434931636
iteration 52, loss = 0.00038164766738191247
iteration 53, loss = 0.0008183446479961276
iteration 54, loss = 0.0007522822124883533
iteration 55, loss = 0.00046734066563658416
iteration 56, loss = 0.0004813463019672781
iteration 57, loss = 0.0006363163702189922
iteration 58, loss = 0.0007828781381249428
iteration 59, loss = 0.0007520053186453879
iteration 60, loss = 0.0004634783254005015
iteration 61, loss = 0.00044103252002969384
iteration 62, loss = 0.000488555699121207
iteration 63, loss = 0.0006118296878412366
iteration 64, loss = 0.00033189766691066325
iteration 65, loss = 0.0005285018123686314
iteration 66, loss = 0.00022629671730101109
iteration 67, loss = 0.00041502848034724593
iteration 68, loss = 0.000404951861128211
iteration 69, loss = 0.000626811699476093
iteration 70, loss = 0.0004767870414070785
iteration 71, loss = 0.000849876319989562
iteration 72, loss = 0.0010524141835048795
iteration 73, loss = 0.0003598705225158483
iteration 74, loss = 0.0002914664219133556
iteration 75, loss = 0.001425974303856492
iteration 76, loss = 0.0009363994468003511
iteration 77, loss = 0.0005172677338123322
iteration 78, loss = 0.000815938925370574
iteration 79, loss = 0.0005664502386935055
iteration 80, loss = 0.0006513558328151703
iteration 81, loss = 0.0008809532155282795
iteration 82, loss = 0.001026384299620986
iteration 83, loss = 0.0015763508854433894
iteration 84, loss = 0.0006291116587817669
iteration 85, loss = 0.0014497328083962202
iteration 86, loss = 0.0010657061357051134
iteration 87, loss = 0.0004138450603932142
iteration 88, loss = 0.0005069541512057185
iteration 89, loss = 0.000489995174575597
iteration 90, loss = 0.0005607820930890739
iteration 91, loss = 0.0004356869321782142
iteration 92, loss = 0.0005614346591755748
iteration 93, loss = 0.00040755068766884506
iteration 94, loss = 0.000505871488712728
iteration 95, loss = 0.0003411582438275218
iteration 96, loss = 0.0004966250853613019
iteration 97, loss = 0.001031692372635007
iteration 98, loss = 0.0007739131106063724
iteration 99, loss = 0.0005270303227007389
iteration 100, loss = 0.0007483785157091916
iteration 101, loss = 0.0005124261369928718
iteration 102, loss = 0.000736006535589695
iteration 103, loss = 0.00033575366251170635
iteration 104, loss = 0.0005002449615858495
iteration 105, loss = 0.0007199038518592715
iteration 106, loss = 0.0014228189829736948
iteration 107, loss = 0.00038188922917470336
iteration 108, loss = 0.00030861981213092804
iteration 109, loss = 0.0004060440987814218
iteration 110, loss = 0.00043564196676015854
iteration 111, loss = 0.000418652780354023
iteration 112, loss = 0.0002760736970230937
iteration 113, loss = 0.0009209955460391939
iteration 114, loss = 0.0007074509630911052
iteration 115, loss = 0.0006774410721845925
iteration 116, loss = 0.0004395944415591657
iteration 117, loss = 0.0007267065811902285
iteration 118, loss = 0.0005015868227928877
iteration 119, loss = 0.0005984770250506699
iteration 120, loss = 0.0008286878583021462
iteration 121, loss = 0.0008509072358720005
iteration 122, loss = 0.00036094343522563577
iteration 123, loss = 0.0009934769477695227
iteration 124, loss = 0.0006127165397629142
iteration 125, loss = 0.0002981267462018877
iteration 126, loss = 0.0005720510962419212
iteration 127, loss = 0.0005875726346857846
iteration 128, loss = 0.00037269931635819376
iteration 129, loss = 0.0003894483670592308
iteration 130, loss = 0.0003521856269799173
iteration 131, loss = 0.000605677894782275
iteration 132, loss = 0.0006661656079813838
iteration 133, loss = 0.0005439876113086939
iteration 134, loss = 0.00027713869349099696
iteration 135, loss = 0.00028416188433766365
iteration 136, loss = 0.0005950783379375935
iteration 137, loss = 0.0004044139932375401
iteration 138, loss = 0.0004920845967717469
iteration 139, loss = 0.0005634685512632132
iteration 140, loss = 0.00030652739224024117
iteration 141, loss = 0.0006008428754284978
iteration 142, loss = 0.0005498297396115959
iteration 143, loss = 0.0007739729480817914
iteration 144, loss = 0.0005692598060704768
iteration 145, loss = 0.0010905868839472532
iteration 146, loss = 0.0013395218411460519
iteration 147, loss = 0.00047029287088662386
iteration 148, loss = 0.0011139739071950316
iteration 149, loss = 0.0006252528401091695
iteration 150, loss = 0.00048657594015821815
iteration 151, loss = 0.0011830049334093928
iteration 152, loss = 0.0007958614150993526
iteration 153, loss = 0.0013274516677483916
iteration 154, loss = 0.0006736728246323764
iteration 155, loss = 0.00036136788548901677
iteration 156, loss = 0.00035588565515354276
iteration 157, loss = 0.000981452059932053
iteration 158, loss = 0.00045102875446900725
iteration 159, loss = 0.0006649955175817013
iteration 160, loss = 0.0008761057397350669
iteration 161, loss = 0.00047880163765512407
iteration 162, loss = 0.0006043553003109992
iteration 163, loss = 0.0003925828787032515
iteration 164, loss = 0.00064799067331478
iteration 165, loss = 0.0003406569012440741
iteration 166, loss = 0.0016574709443375468
iteration 167, loss = 0.0005252434057183564
iteration 168, loss = 0.0004119085497222841
iteration 169, loss = 0.0006582493078894913
iteration 170, loss = 0.0004710173234343529
iteration 171, loss = 0.0005566956242546439
iteration 172, loss = 0.0003355440858285874
iteration 173, loss = 0.0011031095637008548
iteration 174, loss = 0.00037753820652142167
iteration 175, loss = 0.0006176612805575132
iteration 176, loss = 0.00048799498472362757
iteration 177, loss = 0.0004176067595835775
iteration 178, loss = 0.0014269641833379865
iteration 179, loss = 0.00038180279079824686
iteration 180, loss = 0.00047264451859518886
iteration 181, loss = 0.0007391942199319601
iteration 182, loss = 0.000895353383384645
iteration 183, loss = 0.0005003152182325721
iteration 184, loss = 0.0006429449422284961
iteration 185, loss = 0.0005496734520420432
iteration 186, loss = 0.0004180444229859859
iteration 187, loss = 0.000856459082569927
iteration 188, loss = 0.0004895948804914951
iteration 189, loss = 0.00043962488416582346
iteration 190, loss = 0.0004302073211874813
iteration 191, loss = 0.00034569870331324637
iteration 192, loss = 0.0008240491151809692
iteration 193, loss = 0.0006033536046743393
iteration 194, loss = 0.0004021352797280997
iteration 195, loss = 0.000977015937678516
iteration 196, loss = 0.00035080115776509047
iteration 197, loss = 0.00045407406287267804
iteration 198, loss = 0.0007810479728505015
iteration 199, loss = 0.0007680324488319457
iteration 200, loss = 0.000473529304144904
iteration 201, loss = 0.0004346570640336722
iteration 202, loss = 0.0010480365017428994
iteration 203, loss = 0.0005037717055529356
iteration 204, loss = 0.00035012225271202624
iteration 205, loss = 0.0003972853592131287
iteration 206, loss = 0.0005759907653555274
iteration 207, loss = 0.0005499590188264847
iteration 208, loss = 0.000533870595972985
iteration 209, loss = 0.000494467094540596
iteration 210, loss = 0.0005113253719173372
iteration 211, loss = 0.00046774864313192666
iteration 212, loss = 0.0007705017342232168
iteration 213, loss = 0.0014943601563572884
iteration 214, loss = 0.0005726901581510901
iteration 215, loss = 0.0013432669220492244
iteration 216, loss = 0.00067242665681988
iteration 217, loss = 0.0011607349151745439
iteration 218, loss = 0.0003542603808455169
iteration 219, loss = 0.00035738220321945846
iteration 220, loss = 0.0006021527224220335
iteration 221, loss = 0.0007149471784941852
iteration 222, loss = 0.0013439255999401212
iteration 223, loss = 0.00032524141715839505
iteration 224, loss = 0.0006864535389468074
iteration 225, loss = 0.0006541598122566938
iteration 226, loss = 0.0005310291307978332
iteration 227, loss = 0.0003783056163229048
iteration 228, loss = 0.000899363833013922
iteration 229, loss = 0.00038860092172399163
iteration 230, loss = 0.0005685866926796734
iteration 231, loss = 0.0014759099576622248
iteration 232, loss = 0.0003823431325145066
iteration 233, loss = 0.0004133699112571776
iteration 234, loss = 0.0005761603242717683
iteration 235, loss = 0.0003696464700624347
iteration 236, loss = 0.0005183870671316981
iteration 237, loss = 0.0003120789478998631
iteration 238, loss = 0.0007452646386809647
iteration 239, loss = 0.00036961850128136575
iteration 240, loss = 0.0006707311840727925
iteration 241, loss = 0.00046580092748627067
iteration 242, loss = 0.00031225685961544514
iteration 243, loss = 0.0006857499829493463
iteration 244, loss = 0.00036174769047647715
iteration 245, loss = 0.0004674954107031226
iteration 246, loss = 0.000794555526226759
iteration 247, loss = 0.00044731865637004375
iteration 248, loss = 0.0006741820252500474
iteration 249, loss = 0.0006668560672551394
iteration 250, loss = 0.0004149033047724515
iteration 251, loss = 0.0003470839583314955
iteration 252, loss = 0.0007880760822445154
iteration 253, loss = 0.0002764879318419844
iteration 254, loss = 0.0008610303048044443
iteration 255, loss = 0.0009272891329601407
iteration 256, loss = 0.0010465442901477218
iteration 257, loss = 0.001371904043480754
iteration 258, loss = 0.000610056915320456
iteration 259, loss = 0.0010352653916925192
iteration 260, loss = 0.00037943830830045044
iteration 261, loss = 0.0004209770413581282
iteration 262, loss = 0.0003899119619745761
iteration 263, loss = 0.0019295382080599666
iteration 264, loss = 0.0004239128320477903
iteration 265, loss = 0.0006458782590925694
iteration 266, loss = 0.0003831771609839052
iteration 267, loss = 0.00044296783744357526
iteration 268, loss = 0.0003813571238424629
iteration 269, loss = 0.000566743197850883
iteration 270, loss = 0.0017239596927538514
iteration 271, loss = 0.00036503729643300176
iteration 272, loss = 0.0006337824161164463
iteration 273, loss = 0.00029040590743534267
iteration 274, loss = 0.00045541193685494363
iteration 275, loss = 0.0004284097230993211
iteration 276, loss = 0.00044607542804442346
iteration 277, loss = 0.0004810253158211708
iteration 278, loss = 0.0004408961976878345
iteration 279, loss = 0.00036695320159196854
iteration 280, loss = 0.0007655533263459802
iteration 281, loss = 0.0009324342245236039
iteration 282, loss = 0.0006429810309782624
iteration 283, loss = 0.00034493618295527995
iteration 284, loss = 0.000508253404404968
iteration 285, loss = 0.000679200398735702
iteration 286, loss = 0.00045745185343548656
iteration 287, loss = 0.000283873057924211
iteration 288, loss = 0.000531540485098958
iteration 289, loss = 0.00025568425189703703
iteration 290, loss = 0.0008312000427395105
iteration 291, loss = 0.0009249277645722032
iteration 292, loss = 0.00033691214048303664
iteration 293, loss = 0.0004038663173560053
iteration 294, loss = 0.0015181525377556682
iteration 295, loss = 0.0004119143704883754
iteration 296, loss = 0.0006158107426017523
iteration 297, loss = 0.00041028117993846536
iteration 298, loss = 0.0003204987442586571
iteration 299, loss = 0.0004420009790919721
iteration 0, loss = 0.0006633073789998889
iteration 1, loss = 0.0003845661412924528
iteration 2, loss = 0.00043364372686482966
iteration 3, loss = 0.00041950843296945095
iteration 4, loss = 0.0010236863745376468
iteration 5, loss = 0.0011115833185613155
iteration 6, loss = 0.0010147122666239738
iteration 7, loss = 0.0003443855093792081
iteration 8, loss = 0.00040471943793818355
iteration 9, loss = 0.00040779286064207554
iteration 10, loss = 0.0007371508399955928
iteration 11, loss = 0.0005500647821463645
iteration 12, loss = 0.0009533572010695934
iteration 13, loss = 0.000409533065976575
iteration 14, loss = 0.0003206170513294637
iteration 15, loss = 0.00045141304144635797
iteration 16, loss = 0.0003403339069336653
iteration 17, loss = 0.0007809183443896472
iteration 18, loss = 0.00036884393193759024
iteration 19, loss = 0.001383471768349409
iteration 20, loss = 0.0009013565722852945
iteration 21, loss = 0.0005423154216259718
iteration 22, loss = 0.00041712462552823126
iteration 23, loss = 0.000484717107610777
iteration 24, loss = 0.0009611203568056226
iteration 25, loss = 0.00043991650454699993
iteration 26, loss = 0.00043306604493409395
iteration 27, loss = 0.0004546015989035368
iteration 28, loss = 0.0006852794904261827
iteration 29, loss = 0.00042954256059601903
iteration 30, loss = 0.00029636319959536195
iteration 31, loss = 0.0011067049345001578
iteration 32, loss = 0.00044974946649745107
iteration 33, loss = 0.0006540497415699065
iteration 34, loss = 0.000519083347171545
iteration 35, loss = 0.0005008764564990997
iteration 36, loss = 0.0005692935665138066
iteration 37, loss = 0.00039984763134270906
iteration 38, loss = 0.0003802698047365993
iteration 39, loss = 0.0005977341788820922
iteration 40, loss = 0.0006949634407646954
iteration 41, loss = 0.00035698790452443063
iteration 42, loss = 0.0004079410573467612
iteration 43, loss = 0.0003129520919173956
iteration 44, loss = 0.0007481019711121917
iteration 45, loss = 0.0004205040750093758
iteration 46, loss = 0.00047184820869006217
iteration 47, loss = 0.0004133629845455289
iteration 48, loss = 0.000851642747875303
iteration 49, loss = 0.0005292021669447422
iteration 50, loss = 0.00037589247222058475
iteration 51, loss = 0.0004744506150018424
iteration 52, loss = 0.0005113431252539158
iteration 53, loss = 0.0005546996835619211
iteration 54, loss = 0.0008077482925727963
iteration 55, loss = 0.00041935776243917644
iteration 56, loss = 0.0007582453545182943
iteration 57, loss = 0.0003205963294021785
iteration 58, loss = 0.00043106655357405543
iteration 59, loss = 0.0008392684976570308
iteration 60, loss = 0.0003998679167125374
iteration 61, loss = 0.00035795834264717996
iteration 62, loss = 0.0007334735128097236
iteration 63, loss = 0.0005220631137490273
iteration 64, loss = 0.0003137469757348299
iteration 65, loss = 0.0005602685851044953
iteration 66, loss = 0.000515950727276504
iteration 67, loss = 0.0003615843306761235
iteration 68, loss = 0.00045535137178376317
iteration 69, loss = 0.0002989752683788538
iteration 70, loss = 0.00033749008434824646
iteration 71, loss = 0.0003614080196712166
iteration 72, loss = 0.0006174599402584136
iteration 73, loss = 0.0005563835729844868
iteration 74, loss = 0.0003939580637961626
iteration 75, loss = 0.000538813357707113
iteration 76, loss = 0.0003235807816963643
iteration 77, loss = 0.0007287029875442386
iteration 78, loss = 0.0003985182847827673
iteration 79, loss = 0.00033033941872417927
iteration 80, loss = 0.00042240158654749393
iteration 81, loss = 0.00034502416383475065
iteration 82, loss = 0.0006286396528594196
iteration 83, loss = 0.0005436324863694608
iteration 84, loss = 0.00048009358579292893
iteration 85, loss = 0.00030921190045773983
iteration 86, loss = 0.00034640784724615514
iteration 87, loss = 0.0003634649910964072
iteration 88, loss = 0.00035163923166692257
iteration 89, loss = 0.0016215627547353506
iteration 90, loss = 0.0007040753844194114
iteration 91, loss = 0.0003686239942908287
iteration 92, loss = 0.0005403837421908975
iteration 93, loss = 0.0006224787794053555
iteration 94, loss = 0.0008921000408008695
iteration 95, loss = 0.0004562756512314081
iteration 96, loss = 0.00043142674257978797
iteration 97, loss = 0.0003685980336740613
iteration 98, loss = 0.0007790544768795371
iteration 99, loss = 0.0019453360000625253
iteration 100, loss = 0.0004468391998670995
iteration 101, loss = 0.0003553497954271734
iteration 102, loss = 0.0005063109565526247
iteration 103, loss = 0.00039457157254219055
iteration 104, loss = 0.0003200749633833766
iteration 105, loss = 0.00034500897163525224
iteration 106, loss = 0.0005463275010697544
iteration 107, loss = 0.0006270441226661205
iteration 108, loss = 0.0007195323123596609
iteration 109, loss = 0.0006166790262795985
iteration 110, loss = 0.0015981089090928435
iteration 111, loss = 0.001335016218945384
iteration 112, loss = 0.0005149503704160452
iteration 113, loss = 0.0005196395795792341
iteration 114, loss = 0.0005741677014157176
iteration 115, loss = 0.0002980360295623541
iteration 116, loss = 0.0006525713833980262
iteration 117, loss = 0.0004924537497572601
iteration 118, loss = 0.00025759550044313073
iteration 119, loss = 0.0007059590425342321
iteration 120, loss = 0.0008736841846257448
iteration 121, loss = 0.00044029022683389485
iteration 122, loss = 0.0010251501807942986
iteration 123, loss = 0.00042139337165281177
iteration 124, loss = 0.0005408789147622883
iteration 125, loss = 0.0004256760294083506
iteration 126, loss = 0.0010845588985830545
iteration 127, loss = 0.0003788274188991636
iteration 128, loss = 0.0005070533370599151
iteration 129, loss = 0.0005931421765126288
iteration 130, loss = 0.0004129072476644069
iteration 131, loss = 0.0003696517087519169
iteration 132, loss = 0.0008900932734832168
iteration 133, loss = 0.0006463656900450587
iteration 134, loss = 0.0006369452457875013
iteration 135, loss = 0.00033880944829434156
iteration 136, loss = 0.0005909967003390193
iteration 137, loss = 0.0002634074480738491
iteration 138, loss = 0.000517631066031754
iteration 139, loss = 0.000650730449706316
iteration 140, loss = 0.00055838463595137
iteration 141, loss = 0.00030442155548371375
iteration 142, loss = 0.0003585747617762536
iteration 143, loss = 0.0004722141311503947
iteration 144, loss = 0.0003627454861998558
iteration 145, loss = 0.0005630632513202727
iteration 146, loss = 0.00043197692139074206
iteration 147, loss = 0.0004774685949087143
iteration 148, loss = 0.0004638211685232818
iteration 149, loss = 0.0004756439884658903
iteration 150, loss = 0.00047849156544543803
iteration 151, loss = 0.0004163768608123064
iteration 152, loss = 0.0003973172861151397
iteration 153, loss = 0.0005178474239073694
iteration 154, loss = 0.0004196607624180615
iteration 155, loss = 0.00047033446026034653
iteration 156, loss = 0.0005012635374441743
iteration 157, loss = 0.0005433787009678781
iteration 158, loss = 0.0006112547707743943
iteration 159, loss = 0.0004677391843870282
iteration 160, loss = 0.0003202926891390234
iteration 161, loss = 0.0006205991376191378
iteration 162, loss = 0.00048590972437523305
iteration 163, loss = 0.0015082285972312093
iteration 164, loss = 0.00030899554258212447
iteration 165, loss = 0.0002610672963783145
iteration 166, loss = 0.000784959876909852
iteration 167, loss = 0.000880978477653116
iteration 168, loss = 0.0007724409806542099
iteration 169, loss = 0.000594841898418963
iteration 170, loss = 0.0004481957294046879
iteration 171, loss = 0.00033801948302425444
iteration 172, loss = 0.0007874817820265889
iteration 173, loss = 0.00032634788658469915
iteration 174, loss = 0.00043963990174233913
iteration 175, loss = 0.0004325913032516837
iteration 176, loss = 0.0007484501111321151
iteration 177, loss = 0.00044621093547903
iteration 178, loss = 0.00039539154386147857
iteration 179, loss = 0.00036246998934075236
iteration 180, loss = 0.00034826231421902776
iteration 181, loss = 0.0016512523870915174
iteration 182, loss = 0.0007855781586840749
iteration 183, loss = 0.00038600293919444084
iteration 184, loss = 0.0009146087686531246
iteration 185, loss = 0.0006436243420466781
iteration 186, loss = 0.0006982724880799651
iteration 187, loss = 0.0007146230200305581
iteration 188, loss = 0.0007101115770637989
iteration 189, loss = 0.0005415836349129677
iteration 190, loss = 0.0005108500481583178
iteration 191, loss = 0.0003948112716898322
iteration 192, loss = 0.0005438859225250781
iteration 193, loss = 0.0006999874021857977
iteration 194, loss = 0.00045201837201602757
iteration 195, loss = 0.0004464125959202647
iteration 196, loss = 0.000587900576647371
iteration 197, loss = 0.0011568045010790229
iteration 198, loss = 0.0004836592997889966
iteration 199, loss = 0.0010826769284904003
iteration 200, loss = 0.0005062292329967022
iteration 201, loss = 0.0014056485379114747
iteration 202, loss = 0.0003619586059357971
iteration 203, loss = 0.00048468797467648983
iteration 204, loss = 0.0011220644228160381
iteration 205, loss = 0.0006471993401646614
iteration 206, loss = 0.0004859513137489557
iteration 207, loss = 0.00032577553065493703
iteration 208, loss = 0.0006288203876465559
iteration 209, loss = 0.00030365248676389456
iteration 210, loss = 0.0008909269818104804
iteration 211, loss = 0.00043292230111546814
iteration 212, loss = 0.0009075953275896609
iteration 213, loss = 0.00026272935792803764
iteration 214, loss = 0.001057794434018433
iteration 215, loss = 0.0010697194375097752
iteration 216, loss = 0.000797649787273258
iteration 217, loss = 0.000316594319883734
iteration 218, loss = 0.0004008189425803721
iteration 219, loss = 0.0004311072698328644
iteration 220, loss = 0.0006425223546102643
iteration 221, loss = 0.0009840290294960141
iteration 222, loss = 0.00034570798743516207
iteration 223, loss = 0.00046721604303456843
iteration 224, loss = 0.0004854105063714087
iteration 225, loss = 0.00044288570643402636
iteration 226, loss = 0.00036485359305515885
iteration 227, loss = 0.0005701287882402539
iteration 228, loss = 0.0012166198575869203
iteration 229, loss = 0.0004061235813423991
iteration 230, loss = 0.0003846708277706057
iteration 231, loss = 0.0005615776753984392
iteration 232, loss = 0.0003629977290984243
iteration 233, loss = 0.00030460581183433533
iteration 234, loss = 0.0007707333425059915
iteration 235, loss = 0.0004661822458729148
iteration 236, loss = 0.0006176084280014038
iteration 237, loss = 0.0005909447791054845
iteration 238, loss = 0.0005715300794690847
iteration 239, loss = 0.0007256263634189963
iteration 240, loss = 0.0013000231701880693
iteration 241, loss = 0.0003109416284132749
iteration 242, loss = 0.0012947811046615243
iteration 243, loss = 0.00037121010245755315
iteration 244, loss = 0.0009380331030115485
iteration 245, loss = 0.0004357867583166808
iteration 246, loss = 0.0004479752969928086
iteration 247, loss = 0.0008619961445219815
iteration 248, loss = 0.0005390746518969536
iteration 249, loss = 0.0004806314827874303
iteration 250, loss = 0.0003952246333938092
iteration 251, loss = 0.00040509868995286524
iteration 252, loss = 0.0004529553698375821
iteration 253, loss = 0.00037079909816384315
iteration 254, loss = 0.0003123542992398143
iteration 255, loss = 0.0005937261157669127
iteration 256, loss = 0.0007861701305955648
iteration 257, loss = 0.0003650932922028005
iteration 258, loss = 0.00039225464570336044
iteration 259, loss = 0.00038854475133121014
iteration 260, loss = 0.0009250548901036382
iteration 261, loss = 0.00033849425381049514
iteration 262, loss = 0.000671005342155695
iteration 263, loss = 0.0005018531228415668
iteration 264, loss = 0.0008516358211636543
iteration 265, loss = 0.00035211440990678966
iteration 266, loss = 0.0004873447760473937
iteration 267, loss = 0.0004863046342507005
iteration 268, loss = 0.001568719744682312
iteration 269, loss = 0.0010193366324529052
iteration 270, loss = 0.0006642835796810687
iteration 271, loss = 0.0006136295851320028
iteration 272, loss = 0.0007762477034702897
iteration 273, loss = 0.00037144444650039077
iteration 274, loss = 0.0018502719467505813
iteration 275, loss = 0.00042127168853767216
iteration 276, loss = 0.000582185632083565
iteration 277, loss = 0.00043334756628610194
iteration 278, loss = 0.0008184355683624744
iteration 279, loss = 0.0005668586236424744
iteration 280, loss = 0.001300352276302874
iteration 281, loss = 0.000876436592079699
iteration 282, loss = 0.0005381142254918814
iteration 283, loss = 0.0005199137376621366
iteration 284, loss = 0.0004905522218905389
iteration 285, loss = 0.0005431894678622484
iteration 286, loss = 0.0006611855933442712
iteration 287, loss = 0.0003152711724396795
iteration 288, loss = 0.0007377408328466117
iteration 289, loss = 0.001287563587538898
iteration 290, loss = 0.0007512078736908734
iteration 291, loss = 0.0004346700734458864
iteration 292, loss = 0.00040815444663167
iteration 293, loss = 0.0006132964626885951
iteration 294, loss = 0.0003225946566089988
iteration 295, loss = 0.0003593944711610675
iteration 296, loss = 0.0005795370088890195
iteration 297, loss = 0.000420829514041543
iteration 298, loss = 0.0004783948534168303
iteration 299, loss = 0.0013781834859400988
iteration 0, loss = 0.00040837039705365896
iteration 1, loss = 0.0003580304328352213
iteration 2, loss = 0.0006795328226871789
iteration 3, loss = 0.00033290492137894034
iteration 4, loss = 0.0003826804459095001
iteration 5, loss = 0.000242208392592147
iteration 6, loss = 0.0003437197010498494
iteration 7, loss = 0.000447568716481328
iteration 8, loss = 0.00034855573903769255
iteration 9, loss = 0.000576402002479881
iteration 10, loss = 0.0004783478216268122
iteration 11, loss = 0.00040218286449089646
iteration 12, loss = 0.00037067895755171776
iteration 13, loss = 0.00047150260070338845
iteration 14, loss = 0.0006161946803331375
iteration 15, loss = 0.0005780634237453341
iteration 16, loss = 0.0010641792323440313
iteration 17, loss = 0.00033604653435759246
iteration 18, loss = 0.00045479804975911975
iteration 19, loss = 0.0003565657534636557
iteration 20, loss = 0.0015059171710163355
iteration 21, loss = 0.000298744416795671
iteration 22, loss = 0.00043739198008552194
iteration 23, loss = 0.0007868536049500108
iteration 24, loss = 0.000743125332519412
iteration 25, loss = 0.0004736729897558689
iteration 26, loss = 0.0014975850936025381
iteration 27, loss = 0.00042036210652440786
iteration 28, loss = 0.0005127227632328868
iteration 29, loss = 0.00037696820800192654
iteration 30, loss = 0.0007331114029511809
iteration 31, loss = 0.00028194428887218237
iteration 32, loss = 0.0005829036817885935
iteration 33, loss = 0.0004679652920458466
iteration 34, loss = 0.00035505497362464666
iteration 35, loss = 0.0004059828061144799
iteration 36, loss = 0.000342329207342118
iteration 37, loss = 0.0009031472145579755
iteration 38, loss = 0.0004901846987195313
iteration 39, loss = 0.0006645548273809254
iteration 40, loss = 0.000430753396358341
iteration 41, loss = 0.0003952148836106062
iteration 42, loss = 0.000718164024874568
iteration 43, loss = 0.0007591801695525646
iteration 44, loss = 0.0003880947770085186
iteration 45, loss = 0.001271149143576622
iteration 46, loss = 0.0009784968569874763
iteration 47, loss = 0.0007101732771843672
iteration 48, loss = 0.0008090297342278063
iteration 49, loss = 0.0006029524956829846
iteration 50, loss = 0.000617742829490453
iteration 51, loss = 0.00041699950816109776
iteration 52, loss = 0.000533723272383213
iteration 53, loss = 0.0005243835621513426
iteration 54, loss = 0.0005228754016570747
iteration 55, loss = 0.0005633587134070694
iteration 56, loss = 0.0005003867554478347
iteration 57, loss = 0.0014344733208417892
iteration 58, loss = 0.00042751230648718774
iteration 59, loss = 0.0006742606055922806
iteration 60, loss = 0.0015115003334358335
iteration 61, loss = 0.0005131258512847126
iteration 62, loss = 0.000339203659677878
iteration 63, loss = 0.0003812601789832115
iteration 64, loss = 0.0006530602695420384
iteration 65, loss = 0.0002860013337340206
iteration 66, loss = 0.0006607284885831177
iteration 67, loss = 0.0005461933324113488
iteration 68, loss = 0.0009908858919516206
iteration 69, loss = 0.000339834310580045
iteration 70, loss = 0.0004416470183059573
iteration 71, loss = 0.0005311467684805393
iteration 72, loss = 0.0007812431431375444
iteration 73, loss = 0.00030080307624302804
iteration 74, loss = 0.0003329104511067271
iteration 75, loss = 0.00044648227049037814
iteration 76, loss = 0.0005556493997573853
iteration 77, loss = 0.0004207032034173608
iteration 78, loss = 0.00058645976241678
iteration 79, loss = 0.0009526442154310644
iteration 80, loss = 0.0002741217613220215
iteration 81, loss = 0.0007445543305948377
iteration 82, loss = 0.0008295314037241042
iteration 83, loss = 0.0004701177531387657
iteration 84, loss = 0.0007409001118503511
iteration 85, loss = 0.0008009530720300972
iteration 86, loss = 0.0004322935710661113
iteration 87, loss = 0.00034366556792519987
iteration 88, loss = 0.0006154798902571201
iteration 89, loss = 0.0009214464807882905
iteration 90, loss = 0.0009116130531765521
iteration 91, loss = 0.0002800491638481617
iteration 92, loss = 0.00040224427357316017
iteration 93, loss = 0.000593338452745229
iteration 94, loss = 0.0003733221092261374
iteration 95, loss = 0.00037549060652963817
iteration 96, loss = 0.0003095458960160613
iteration 97, loss = 0.0005412017344497144
iteration 98, loss = 0.00037884776247665286
iteration 99, loss = 0.0007680054986849427
iteration 100, loss = 0.000786048942245543
iteration 101, loss = 0.00032842866494320333
iteration 102, loss = 0.0003320805262774229
iteration 103, loss = 0.00036846852162852883
iteration 104, loss = 0.0003570638073142618
iteration 105, loss = 0.0006506871432065964
iteration 106, loss = 0.0006968482048250735
iteration 107, loss = 0.0004131263995077461
iteration 108, loss = 0.0006772266933694482
iteration 109, loss = 0.00040022144094109535
iteration 110, loss = 0.0007059269119054079
iteration 111, loss = 0.0008649167139083147
iteration 112, loss = 0.0010128705762326717
iteration 113, loss = 0.0004202581476420164
iteration 114, loss = 0.000663316692225635
iteration 115, loss = 0.00032303904299624264
iteration 116, loss = 0.00089976005256176
iteration 117, loss = 0.00047764412011019886
iteration 118, loss = 0.0007659277180209756
iteration 119, loss = 0.0005389823927544057
iteration 120, loss = 0.0005381584051065147
iteration 121, loss = 0.000326817185850814
iteration 122, loss = 0.00057161261793226
iteration 123, loss = 0.0007011017296463251
iteration 124, loss = 0.0003336939262226224
iteration 125, loss = 0.0005856469506397843
iteration 126, loss = 0.00033767963759601116
iteration 127, loss = 0.00041882717050611973
iteration 128, loss = 0.00046329054748639464
iteration 129, loss = 0.00040730403270572424
iteration 130, loss = 0.00048390915617346764
iteration 131, loss = 0.0003790261980611831
iteration 132, loss = 0.000739031471312046
iteration 133, loss = 0.0003007750492542982
iteration 134, loss = 0.00042129220673814416
iteration 135, loss = 0.00030378447263501585
iteration 136, loss = 0.0007781256572343409
iteration 137, loss = 0.0005803180974908173
iteration 138, loss = 0.00033919455017894506
iteration 139, loss = 0.000401803437853232
iteration 140, loss = 0.00025396596174687147
iteration 141, loss = 0.00027995248092338443
iteration 142, loss = 0.0005171539960429072
iteration 143, loss = 0.0003090417303610593
iteration 144, loss = 0.00037264678394421935
iteration 145, loss = 0.0008444202248938382
iteration 146, loss = 0.0007846138323657215
iteration 147, loss = 0.0005269220564514399
iteration 148, loss = 0.00045471946941688657
iteration 149, loss = 0.0003048221697099507
iteration 150, loss = 0.0005043794517405331
iteration 151, loss = 0.0012292098253965378
iteration 152, loss = 0.00034210350713692605
iteration 153, loss = 0.00028079445473849773
iteration 154, loss = 0.00069813133450225
iteration 155, loss = 0.0007951107108965516
iteration 156, loss = 0.00036177883157506585
iteration 157, loss = 0.000468680402263999
iteration 158, loss = 0.00041680128197185695
iteration 159, loss = 0.0005409022560343146
iteration 160, loss = 0.00024543999461457133
iteration 161, loss = 0.0013541000662371516
iteration 162, loss = 0.000513802224304527
iteration 163, loss = 0.0005580068682320416
iteration 164, loss = 0.000534643535502255
iteration 165, loss = 0.0014444353291764855
iteration 166, loss = 0.0006182970246300101
iteration 167, loss = 0.0003965132054872811
iteration 168, loss = 0.0006148453103378415
iteration 169, loss = 0.0010623510461300611
iteration 170, loss = 0.0004652064526453614
iteration 171, loss = 0.0008380872313864529
iteration 172, loss = 0.0007083547534421086
iteration 173, loss = 0.00033729069400578737
iteration 174, loss = 0.00044579533278010786
iteration 175, loss = 0.0005284669459797442
iteration 176, loss = 0.0004322481981944293
iteration 177, loss = 0.0009717250359244645
iteration 178, loss = 0.0008907981100492179
iteration 179, loss = 0.00045052130008116364
iteration 180, loss = 0.00039937582914717495
iteration 181, loss = 0.00032633705995976925
iteration 182, loss = 0.00024774891790002584
iteration 183, loss = 0.0003652337472885847
iteration 184, loss = 0.0007876421441324055
iteration 185, loss = 0.0003924978373106569
iteration 186, loss = 0.0004034653538838029
iteration 187, loss = 0.000404022925067693
iteration 188, loss = 0.0006259831134229898
iteration 189, loss = 0.0002727619430515915
iteration 190, loss = 0.0005434272461570799
iteration 191, loss = 0.0006670990260317922
iteration 192, loss = 0.0013044052757322788
iteration 193, loss = 0.0004235226078890264
iteration 194, loss = 0.000719014904461801
iteration 195, loss = 0.00081015401519835
iteration 196, loss = 0.0010320366127416492
iteration 197, loss = 0.001421131077222526
iteration 198, loss = 0.00047539814841002226
iteration 199, loss = 0.000427565595600754
iteration 200, loss = 0.0012912866659462452
iteration 201, loss = 0.0006888732896186411
iteration 202, loss = 0.00033949839416891336
iteration 203, loss = 0.00028756592655554414
iteration 204, loss = 0.0003899534058291465
iteration 205, loss = 0.0012619808549061418
iteration 206, loss = 0.0005579149583354592
iteration 207, loss = 0.0006071163224987686
iteration 208, loss = 0.00032287772046402097
iteration 209, loss = 0.0004136495990678668
iteration 210, loss = 0.0003161626518703997
iteration 211, loss = 0.0002894089266192168
iteration 212, loss = 0.000507520220708102
iteration 213, loss = 0.000627145345788449
iteration 214, loss = 0.00037655001506209373
iteration 215, loss = 0.0005636338028125465
iteration 216, loss = 0.0005848149885423481
iteration 217, loss = 0.0011051928158849478
iteration 218, loss = 0.0005051476182416081
iteration 219, loss = 0.0006579587352462113
iteration 220, loss = 0.0007759750005789101
iteration 221, loss = 0.0003302978875581175
iteration 222, loss = 0.00035920782829634845
iteration 223, loss = 0.00031348608899861574
iteration 224, loss = 0.00048085543676279485
iteration 225, loss = 0.0003650487051345408
iteration 226, loss = 0.00041212502401322126
iteration 227, loss = 0.000707489438354969
iteration 228, loss = 0.000519885215908289
iteration 229, loss = 0.0006341199041344225
iteration 230, loss = 0.00038484687684103847
iteration 231, loss = 0.0005828943103551865
iteration 232, loss = 0.000698916963301599
iteration 233, loss = 0.0003211549192201346
iteration 234, loss = 0.0009247371926903725
iteration 235, loss = 0.0004101130471099168
iteration 236, loss = 0.00047331597306765616
iteration 237, loss = 0.0004107300192117691
iteration 238, loss = 0.0005456936196424067
iteration 239, loss = 0.0009072069078683853
iteration 240, loss = 0.00031962880166247487
iteration 241, loss = 0.0004308742063585669
iteration 242, loss = 0.00041225628228858113
iteration 243, loss = 0.00032923551043495536
iteration 244, loss = 0.0022690696641802788
iteration 245, loss = 0.00043054227717220783
iteration 246, loss = 0.000275873055215925
iteration 247, loss = 0.0005792856682091951
iteration 248, loss = 0.000533389684278518
iteration 249, loss = 0.0006411334616132081
iteration 250, loss = 0.0003834107192233205
iteration 251, loss = 0.00047455140156671405
iteration 252, loss = 0.00046051855315454304
iteration 253, loss = 0.00044115306809544563
iteration 254, loss = 0.0003046912606805563
iteration 255, loss = 0.0017474021296948195
iteration 256, loss = 0.0006454465328715742
iteration 257, loss = 0.0004657088138628751
iteration 258, loss = 0.00045902677811682224
iteration 259, loss = 0.0006693932227790356
iteration 260, loss = 0.0003165002563036978
iteration 261, loss = 0.0003414119710214436
iteration 262, loss = 0.0007906174287199974
iteration 263, loss = 0.0003659934736788273
iteration 264, loss = 0.001299520954489708
iteration 265, loss = 0.00036171742249280214
iteration 266, loss = 0.0003092912957072258
iteration 267, loss = 0.0004978989018127322
iteration 268, loss = 0.0015120409661903977
iteration 269, loss = 0.0005133267841301858
iteration 270, loss = 0.00039840617682784796
iteration 271, loss = 0.0014099791878834367
iteration 272, loss = 0.00030121178133413196
iteration 273, loss = 0.0004305451293475926
iteration 274, loss = 0.00037466365029104054
iteration 275, loss = 0.0005503729917109013
iteration 276, loss = 0.00043204156099818647
iteration 277, loss = 0.0004273852682672441
iteration 278, loss = 0.00032721145544201136
iteration 279, loss = 0.000483220093883574
iteration 280, loss = 0.0003449512005317956
iteration 281, loss = 0.00032095564529299736
iteration 282, loss = 0.0004318976425565779
iteration 283, loss = 0.00036743481177836657
iteration 284, loss = 0.0009671304142102599
iteration 285, loss = 0.00036590220406651497
iteration 286, loss = 0.0003660228685475886
iteration 287, loss = 0.0004518963978625834
iteration 288, loss = 0.0005697263986803591
iteration 289, loss = 0.0003040751034859568
iteration 290, loss = 0.0003549371613189578
iteration 291, loss = 0.00034100489574484527
iteration 292, loss = 0.0005282153142616153
iteration 293, loss = 0.0004766422207467258
iteration 294, loss = 0.0007756485720165074
iteration 295, loss = 0.0003499127342365682
iteration 296, loss = 0.0006278748624026775
iteration 297, loss = 0.00026480492670089006
iteration 298, loss = 0.0005131739308126271
iteration 299, loss = 0.00031219987431541085
iteration 0, loss = 0.0005603214376606047
iteration 1, loss = 0.00032176781678572297
iteration 2, loss = 0.0004601585678756237
iteration 3, loss = 0.00048405726556666195
iteration 4, loss = 0.0005215086857788265
iteration 5, loss = 0.000742611417081207
iteration 6, loss = 0.000562033848837018
iteration 7, loss = 0.0011979175033047795
iteration 8, loss = 0.000340561440680176
iteration 9, loss = 0.0006374095100909472
iteration 10, loss = 0.00028041531913913786
iteration 11, loss = 0.0005573692033067346
iteration 12, loss = 0.000874059391207993
iteration 13, loss = 0.0003337118250783533
iteration 14, loss = 0.00041707121999934316
iteration 15, loss = 0.00032720566377975047
iteration 16, loss = 0.00046717471559531987
iteration 17, loss = 0.0005702127818949521
iteration 18, loss = 0.0004105934058316052
iteration 19, loss = 0.0005688732489943504
iteration 20, loss = 0.0004164822166785598
iteration 21, loss = 0.0005001319805160165
iteration 22, loss = 0.0002906553854700178
iteration 23, loss = 0.0009139195317402482
iteration 24, loss = 0.00024743727408349514
iteration 25, loss = 0.0003948601079173386
iteration 26, loss = 0.0003820069250650704
iteration 27, loss = 0.00030767652788199484
iteration 28, loss = 0.00029313776758499444
iteration 29, loss = 0.0007787974900566041
iteration 30, loss = 0.0006064720801077783
iteration 31, loss = 0.0005150425713509321
iteration 32, loss = 0.0002182920288760215
iteration 33, loss = 0.0005588837084360421
iteration 34, loss = 0.00027867811149917543
iteration 35, loss = 0.0003868970088660717
iteration 36, loss = 0.0011390865547582507
iteration 37, loss = 0.0002959221019409597
iteration 38, loss = 0.0004553423495963216
iteration 39, loss = 0.00039793059113435447
iteration 40, loss = 0.00023887297720648348
iteration 41, loss = 0.0004758038558065891
iteration 42, loss = 0.0004386664950288832
iteration 43, loss = 0.0004072480369359255
iteration 44, loss = 0.00033019299735315144
iteration 45, loss = 0.0005422233953140676
iteration 46, loss = 0.0003823700244538486
iteration 47, loss = 0.0004931595176458359
iteration 48, loss = 0.0005789423012174666
iteration 49, loss = 0.0002909541944973171
iteration 50, loss = 0.00046550206025131047
iteration 51, loss = 0.0005084695294499397
iteration 52, loss = 0.0003429623902775347
iteration 53, loss = 0.00037068623350933194
iteration 54, loss = 0.0002909954637289047
iteration 55, loss = 0.0007180420798249543
iteration 56, loss = 0.0008163944585248828
iteration 57, loss = 0.000365271553164348
iteration 58, loss = 0.00028405862394720316
iteration 59, loss = 0.0003545472864061594
iteration 60, loss = 0.00037088379031047225
iteration 61, loss = 0.0006417682161554694
iteration 62, loss = 0.0004376182914711535
iteration 63, loss = 0.0013166784774512053
iteration 64, loss = 0.00039830664172768593
iteration 65, loss = 0.0006476416019722819
iteration 66, loss = 0.00023066505673341453
iteration 67, loss = 0.0006425332394428551
iteration 68, loss = 0.0004402294580359012
iteration 69, loss = 0.0005744278314523399
iteration 70, loss = 0.00040050005191005766
iteration 71, loss = 0.000693249749019742
iteration 72, loss = 0.00021915115939918905
iteration 73, loss = 0.000498585868626833
iteration 74, loss = 0.001254985691048205
iteration 75, loss = 0.00038500461960211396
iteration 76, loss = 0.00043053628178313375
iteration 77, loss = 0.0005114980158396065
iteration 78, loss = 0.00031117384787648916
iteration 79, loss = 0.000341801845934242
iteration 80, loss = 0.0004735311958938837
iteration 81, loss = 0.0008423032704740763
iteration 82, loss = 0.00036793245817534626
iteration 83, loss = 0.00045559037243947387
iteration 84, loss = 0.0003807080793194473
iteration 85, loss = 0.000779294001404196
iteration 86, loss = 0.0007751294178888202
iteration 87, loss = 0.0004127689462620765
iteration 88, loss = 0.0002898067468777299
iteration 89, loss = 0.00038627159665338695
iteration 90, loss = 0.0003781398700084537
iteration 91, loss = 0.00031594198662787676
iteration 92, loss = 0.000584673834964633
iteration 93, loss = 0.0005024394486099482
iteration 94, loss = 0.0006877625710330904
iteration 95, loss = 0.0005334364250302315
iteration 96, loss = 0.0008284766227006912
iteration 97, loss = 0.00037743098801001906
iteration 98, loss = 0.0005563277518376708
iteration 99, loss = 0.0005041713593527675
iteration 100, loss = 0.0005561674479395151
iteration 101, loss = 0.0002299688057973981
iteration 102, loss = 0.0003208340785931796
iteration 103, loss = 0.0004105457046534866
iteration 104, loss = 0.0007785246125422418
iteration 105, loss = 0.0006358548998832703
iteration 106, loss = 0.0004144638660363853
iteration 107, loss = 0.0002442806726321578
iteration 108, loss = 0.0003008419298566878
iteration 109, loss = 0.0003252536407671869
iteration 110, loss = 0.0005189074436202645
iteration 111, loss = 0.00041988707380369306
iteration 112, loss = 0.0003881199809256941
iteration 113, loss = 0.000869107199832797
iteration 114, loss = 0.0005677401786670089
iteration 115, loss = 0.000758822716306895
iteration 116, loss = 0.0005171161610633135
iteration 117, loss = 0.0013341029407456517
iteration 118, loss = 0.00044536692439578474
iteration 119, loss = 0.00063956348458305
iteration 120, loss = 0.0005259996396489441
iteration 121, loss = 0.0004192286869511008
iteration 122, loss = 0.0005401861271820962
iteration 123, loss = 0.0006546797230839729
iteration 124, loss = 0.0025120761711150408
iteration 125, loss = 0.0008594078244641423
iteration 126, loss = 0.0004806376527994871
iteration 127, loss = 0.0005218662554398179
iteration 128, loss = 0.0008348109549842775
iteration 129, loss = 0.0003920613962691277
iteration 130, loss = 0.0013144484255462885
iteration 131, loss = 0.0005258768796920776
iteration 132, loss = 0.00024527072673663497
iteration 133, loss = 0.0003765277215279639
iteration 134, loss = 0.0002631665556691587
iteration 135, loss = 0.0003794589720200747
iteration 136, loss = 0.00030034754308871925
iteration 137, loss = 0.00032113466295413673
iteration 138, loss = 0.0007949502905830741
iteration 139, loss = 0.00041543098632246256
iteration 140, loss = 0.0004526078701019287
iteration 141, loss = 0.0005442859255708754
iteration 142, loss = 0.0014973437646403909
iteration 143, loss = 0.00026390296989120543
iteration 144, loss = 0.000712525041308254
iteration 145, loss = 0.0007718502311035991
iteration 146, loss = 0.0006092493422329426
iteration 147, loss = 0.0007669784827157855
iteration 148, loss = 0.0009268489666283131
iteration 149, loss = 0.0005534645169973373
iteration 150, loss = 0.0002452978806104511
iteration 151, loss = 0.00037176557816565037
iteration 152, loss = 0.0002978916745632887
iteration 153, loss = 0.00031087317620404065
iteration 154, loss = 0.0006639902712777257
iteration 155, loss = 0.001236627227626741
iteration 156, loss = 0.0005029583699069917
iteration 157, loss = 0.0003183452063240111
iteration 158, loss = 0.0004490789142437279
iteration 159, loss = 0.00026009860448539257
iteration 160, loss = 0.0005216600839048624
iteration 161, loss = 0.000899571692571044
iteration 162, loss = 0.0008004760602489114
iteration 163, loss = 0.00045903652790002525
iteration 164, loss = 0.0010820614406839013
iteration 165, loss = 0.0008140653371810913
iteration 166, loss = 0.0005384120158851147
iteration 167, loss = 0.00025358478887937963
iteration 168, loss = 0.0008960516424849629
iteration 169, loss = 0.0005081405979581177
iteration 170, loss = 0.00034474412677809596
iteration 171, loss = 0.00036436558002606034
iteration 172, loss = 0.0003013293899130076
iteration 173, loss = 0.0004745878395624459
iteration 174, loss = 0.0004645947483368218
iteration 175, loss = 0.00041550869354978204
iteration 176, loss = 0.0007391548715531826
iteration 177, loss = 0.0004114330222364515
iteration 178, loss = 0.0005877965013496578
iteration 179, loss = 0.000585199159104377
iteration 180, loss = 0.0004153159388806671
iteration 181, loss = 0.0008264308562502265
iteration 182, loss = 0.0003707037540152669
iteration 183, loss = 0.0007478104089386761
iteration 184, loss = 0.0006259622750803828
iteration 185, loss = 0.00046882248716428876
iteration 186, loss = 0.0014380909269675612
iteration 187, loss = 0.0016737928381189704
iteration 188, loss = 0.0007450178964063525
iteration 189, loss = 0.0002920036786235869
iteration 190, loss = 0.0003956599975936115
iteration 191, loss = 0.0008197973947972059
iteration 192, loss = 0.00035464021493680775
iteration 193, loss = 0.0003033919492736459
iteration 194, loss = 0.0005574575625360012
iteration 195, loss = 0.0008245231583714485
iteration 196, loss = 0.00039445384754799306
iteration 197, loss = 0.0003442867600824684
iteration 198, loss = 0.0003678510256577283
iteration 199, loss = 0.0003279319789726287
iteration 200, loss = 0.00044889800483360887
iteration 201, loss = 0.0003456487029325217
iteration 202, loss = 0.0003502638719510287
iteration 203, loss = 0.00032001762883737683
iteration 204, loss = 0.0005771174328401685
iteration 205, loss = 0.0012986855581402779
iteration 206, loss = 0.0009618983021937311
iteration 207, loss = 0.00044034028542228043
iteration 208, loss = 0.0002959868870675564
iteration 209, loss = 0.00040873108082450926
iteration 210, loss = 0.0008917486411519349
iteration 211, loss = 0.00045479965046979487
iteration 212, loss = 0.0014374613529071212
iteration 213, loss = 0.0004748891806229949
iteration 214, loss = 0.0006095561548136175
iteration 215, loss = 0.001358464010991156
iteration 216, loss = 0.0006474059191532433
iteration 217, loss = 0.0009678030619397759
iteration 218, loss = 0.00028985360404476523
iteration 219, loss = 0.000318119244184345
iteration 220, loss = 0.0002346820547245443
iteration 221, loss = 0.0005061363917775452
iteration 222, loss = 0.00044821249321103096
iteration 223, loss = 0.0003962163464166224
iteration 224, loss = 0.0013566192938014865
iteration 225, loss = 0.0005426880670711398
iteration 226, loss = 0.00021755488705821335
iteration 227, loss = 0.0006501901661977172
iteration 228, loss = 0.0008434478077106178
iteration 229, loss = 0.0010511622531339526
iteration 230, loss = 0.0004309669020585716
iteration 231, loss = 0.00033625232754275203
iteration 232, loss = 0.00041523907566443086
iteration 233, loss = 0.0005871035973541439
iteration 234, loss = 0.0004263076407369226
iteration 235, loss = 0.0002379599609412253
iteration 236, loss = 0.0002973235968966037
iteration 237, loss = 0.0003864034079015255
iteration 238, loss = 0.0004453098517842591
iteration 239, loss = 0.0005100836278870702
iteration 240, loss = 0.0006334292120300233
iteration 241, loss = 0.0009090116363950074
iteration 242, loss = 0.0013664818834513426
iteration 243, loss = 0.0008596042171120644
iteration 244, loss = 0.0003905179910361767
iteration 245, loss = 0.0004501149815041572
iteration 246, loss = 0.0002549015625845641
iteration 247, loss = 0.00042643400956876576
iteration 248, loss = 0.0003987565578427166
iteration 249, loss = 0.0006411327631212771
iteration 250, loss = 0.00030843826243653893
iteration 251, loss = 0.00040667291614226997
iteration 252, loss = 0.0005589608917944133
iteration 253, loss = 0.0007519710925407708
iteration 254, loss = 0.0004515053005889058
iteration 255, loss = 0.001284856116399169
iteration 256, loss = 0.0012263816315680742
iteration 257, loss = 0.00023891948512755334
iteration 258, loss = 0.0002403297694399953
iteration 259, loss = 0.0004144235572312027
iteration 260, loss = 0.0004672739887610078
iteration 261, loss = 0.00047858874313533306
iteration 262, loss = 0.0002861474931705743
iteration 263, loss = 0.00046554108848795295
iteration 264, loss = 0.00029969369643367827
iteration 265, loss = 0.00043826494948007166
iteration 266, loss = 0.0008676727302372456
iteration 267, loss = 0.0003515383286867291
iteration 268, loss = 0.0003155767044518143
iteration 269, loss = 0.000396555638872087
iteration 270, loss = 0.0003255434276070446
iteration 271, loss = 0.00030130980303511024
iteration 272, loss = 0.00048129537026397884
iteration 273, loss = 0.0003941635659430176
iteration 274, loss = 0.00046134201693348587
iteration 275, loss = 0.00032645626924932003
iteration 276, loss = 0.0010552708990871906
iteration 277, loss = 0.00045262323692440987
iteration 278, loss = 0.0008448966545984149
iteration 279, loss = 0.0007674038060940802
iteration 280, loss = 0.0004690131463576108
iteration 281, loss = 0.00027296924963593483
iteration 282, loss = 0.00037854997208341956
iteration 283, loss = 0.0003083720221184194
iteration 284, loss = 0.000275895232334733
iteration 285, loss = 0.0002788082929328084
iteration 286, loss = 0.0006557396845892072
iteration 287, loss = 0.0007011797279119492
iteration 288, loss = 0.0005299008917063475
iteration 289, loss = 0.000494708598125726
iteration 290, loss = 0.00042822357499971986
iteration 291, loss = 0.0005878655938431621
iteration 292, loss = 0.00029735025600530207
iteration 293, loss = 0.00047867256216704845
iteration 294, loss = 0.0005537408287636936
iteration 295, loss = 0.00031032919650897384
iteration 296, loss = 0.0003935480199288577
iteration 297, loss = 0.00030188722303137183
iteration 298, loss = 0.0003744052955880761
iteration 299, loss = 0.0004527847922872752
iteration 0, loss = 0.00041214143857359886
iteration 1, loss = 0.0002778038033284247
iteration 2, loss = 0.000328455789713189
iteration 3, loss = 0.00034201674861833453
iteration 4, loss = 0.0005443596746772528
iteration 5, loss = 0.00038890523137524724
iteration 6, loss = 0.00031989102717489004
iteration 7, loss = 0.00029202408040873706
iteration 8, loss = 0.0003627403057180345
iteration 9, loss = 0.000580940512008965
iteration 10, loss = 0.0012135449796915054
iteration 11, loss = 0.0005270171095617115
iteration 12, loss = 0.0004326586495153606
iteration 13, loss = 0.00034727418096736073
iteration 14, loss = 0.00036062204162590206
iteration 15, loss = 0.0005706138326786458
iteration 16, loss = 0.0006843062583357096
iteration 17, loss = 0.001335649285465479
iteration 18, loss = 0.0002960773999802768
iteration 19, loss = 0.0008224123739637434
iteration 20, loss = 0.0004242222639732063
iteration 21, loss = 0.0003941193863283843
iteration 22, loss = 0.00041892071021720767
iteration 23, loss = 0.0004894927842542529
iteration 24, loss = 0.0005246299551799893
iteration 25, loss = 0.0005371496081352234
iteration 26, loss = 0.0013906578533351421
iteration 27, loss = 0.0003029072831850499
iteration 28, loss = 0.00041487140697427094
iteration 29, loss = 0.0004371540853753686
iteration 30, loss = 0.0008979820413514972
iteration 31, loss = 0.0006033580284565687
iteration 32, loss = 0.0004316006670705974
iteration 33, loss = 0.0005367246922105551
iteration 34, loss = 0.00037480206810869277
iteration 35, loss = 0.00039208351518027484
iteration 36, loss = 0.0006708544678986073
iteration 37, loss = 0.001212773029692471
iteration 38, loss = 0.00041598023381084204
iteration 39, loss = 0.0003675378975458443
iteration 40, loss = 0.0004501319781411439
iteration 41, loss = 0.0003667278215289116
iteration 42, loss = 0.0006315895589068532
iteration 43, loss = 0.0013376771239563823
iteration 44, loss = 0.00027037045219913125
iteration 45, loss = 0.0005490216426551342
iteration 46, loss = 0.00036292916047386825
iteration 47, loss = 0.00037165943649597466
iteration 48, loss = 0.0010192266199737787
iteration 49, loss = 0.0003366491000633687
iteration 50, loss = 0.0002327904076082632
iteration 51, loss = 0.0003513325355015695
iteration 52, loss = 0.0014645197661593556
iteration 53, loss = 0.0006087130750529468
iteration 54, loss = 0.00029805838130414486
iteration 55, loss = 0.00031845420016907156
iteration 56, loss = 0.0003907946520484984
iteration 57, loss = 0.00029005578835494816
iteration 58, loss = 0.0004843689384870231
iteration 59, loss = 0.0004600644460879266
iteration 60, loss = 0.0005664880154654384
iteration 61, loss = 0.00022179883671924472
iteration 62, loss = 0.0003238483623135835
iteration 63, loss = 0.0002568991621956229
iteration 64, loss = 0.0003802509163506329
iteration 65, loss = 0.00038539248635061085
iteration 66, loss = 0.0005022012628614902
iteration 67, loss = 0.0013496937463060021
iteration 68, loss = 0.0002907808229792863
iteration 69, loss = 0.0005435865023173392
iteration 70, loss = 0.00030410749604925513
iteration 71, loss = 0.0004842858179472387
iteration 72, loss = 0.0003322107077110559
iteration 73, loss = 0.00030330094159580767
iteration 74, loss = 0.001372035825625062
iteration 75, loss = 0.0006249725120142102
iteration 76, loss = 0.00029742118204012513
iteration 77, loss = 0.00063540373230353
iteration 78, loss = 0.0011724492069333792
iteration 79, loss = 0.00036634306889027357
iteration 80, loss = 0.00026715968851931393
iteration 81, loss = 0.0005717523163184524
iteration 82, loss = 0.00043618501513265073
iteration 83, loss = 0.0004336506826803088
iteration 84, loss = 0.0007857907330617309
iteration 85, loss = 0.00046899387962184846
iteration 86, loss = 0.00046893651597201824
iteration 87, loss = 0.00030209109536372125
iteration 88, loss = 0.0005049009341746569
iteration 89, loss = 0.00035281683085486293
iteration 90, loss = 0.00043373298831284046
iteration 91, loss = 0.00048533731023781
iteration 92, loss = 0.0007457502651959658
iteration 93, loss = 0.0003157586033921689
iteration 94, loss = 0.00029390427516773343
iteration 95, loss = 0.001244960119947791
iteration 96, loss = 0.00044097250793129206
iteration 97, loss = 0.00047280878061428666
iteration 98, loss = 0.000881635642144829
iteration 99, loss = 0.0005431710742413998
iteration 100, loss = 0.0006316082435660064
iteration 101, loss = 0.0009341553668491542
iteration 102, loss = 0.0003563746577128768
iteration 103, loss = 0.0005071201594546437
iteration 104, loss = 0.0007507075206376612
iteration 105, loss = 0.0008078392711468041
iteration 106, loss = 0.0007293768576346338
iteration 107, loss = 0.0003756331279873848
iteration 108, loss = 0.0004800411988981068
iteration 109, loss = 0.0005588802159763873
iteration 110, loss = 0.0006971537950448692
iteration 111, loss = 0.0006301039247773588
iteration 112, loss = 0.0007833577692508698
iteration 113, loss = 0.00026007089763879776
iteration 114, loss = 0.00026762549532577395
iteration 115, loss = 0.0009022199083119631
iteration 116, loss = 0.00030385423451662064
iteration 117, loss = 0.0002384296094533056
iteration 118, loss = 0.00027541263261809945
iteration 119, loss = 0.0006220871000550687
iteration 120, loss = 0.0003358794783707708
iteration 121, loss = 0.0003701590176206082
iteration 122, loss = 0.0007752004894427955
iteration 123, loss = 0.0004174319328740239
iteration 124, loss = 0.0005928847240284085
iteration 125, loss = 0.00038537729415111244
iteration 126, loss = 0.000579992076382041
iteration 127, loss = 0.0004698001721408218
iteration 128, loss = 0.0005247422959655523
iteration 129, loss = 0.0003629459533840418
iteration 130, loss = 0.00048283892101608217
iteration 131, loss = 0.0004892704891972244
iteration 132, loss = 0.0006832099170424044
iteration 133, loss = 0.0005250083049759269
iteration 134, loss = 0.0003243697574362159
iteration 135, loss = 0.0008517471724189818
iteration 136, loss = 0.00026493429322727025
iteration 137, loss = 0.0004944527172483504
iteration 138, loss = 0.0006703140097670257
iteration 139, loss = 0.00029723241459578276
iteration 140, loss = 0.00033077882835641503
iteration 141, loss = 0.0002497228852007538
iteration 142, loss = 0.0006543275667354465
iteration 143, loss = 0.00047930312575772405
iteration 144, loss = 0.0005614971159957349
iteration 145, loss = 0.0003638869966380298
iteration 146, loss = 0.0005533245275728405
iteration 147, loss = 0.00047517154598608613
iteration 148, loss = 0.0005558197153732181
iteration 149, loss = 0.00044084928231313825
iteration 150, loss = 0.00040415802504867315
iteration 151, loss = 0.0002506756572984159
iteration 152, loss = 0.00025752244982868433
iteration 153, loss = 0.00044024671660736203
iteration 154, loss = 0.0005740857450291514
iteration 155, loss = 0.0004769556981045753
iteration 156, loss = 0.0003323064011055976
iteration 157, loss = 0.00035333793493919075
iteration 158, loss = 0.001219139900058508
iteration 159, loss = 0.0006442921003326774
iteration 160, loss = 0.0002319281193194911
iteration 161, loss = 0.0007584302802570164
iteration 162, loss = 0.0003981232875958085
iteration 163, loss = 0.00019996518676634878
iteration 164, loss = 0.0003062433679588139
iteration 165, loss = 0.00034872256219387054
iteration 166, loss = 0.00038510560989379883
iteration 167, loss = 0.0005827180575579405
iteration 168, loss = 0.00041420484194532037
iteration 169, loss = 0.0003447900526225567
iteration 170, loss = 0.00048470264300704
iteration 171, loss = 0.0009538463782519102
iteration 172, loss = 0.0003614436718635261
iteration 173, loss = 0.00030347215943038464
iteration 174, loss = 0.0004209015751257539
iteration 175, loss = 0.00039876389200799167
iteration 176, loss = 0.0007500671781599522
iteration 177, loss = 0.0005730132688768208
iteration 178, loss = 0.0005086934543214738
iteration 179, loss = 0.0011021997779607773
iteration 180, loss = 0.0002633410331327468
iteration 181, loss = 0.00021974090486764908
iteration 182, loss = 0.0003199817438144237
iteration 183, loss = 0.000407202634960413
iteration 184, loss = 0.00046222942182794213
iteration 185, loss = 0.0008789838175289333
iteration 186, loss = 0.0003147004172205925
iteration 187, loss = 0.0004847812815569341
iteration 188, loss = 0.0004935870529152453
iteration 189, loss = 0.0011711913393810391
iteration 190, loss = 0.00027604331262409687
iteration 191, loss = 0.00045265318476594985
iteration 192, loss = 0.0009087174548767507
iteration 193, loss = 0.0007892693975009024
iteration 194, loss = 0.00047765386989340186
iteration 195, loss = 0.0004319655417930335
iteration 196, loss = 0.0008149993955157697
iteration 197, loss = 0.0006839690613560379
iteration 198, loss = 0.0004849272081628442
iteration 199, loss = 0.0004406477091833949
iteration 200, loss = 0.00032535585341975093
iteration 201, loss = 0.0006468997453339398
iteration 202, loss = 0.0007620207034051418
iteration 203, loss = 0.0005450926837511361
iteration 204, loss = 0.0003030392690561712
iteration 205, loss = 0.0004894950543530285
iteration 206, loss = 0.0008916861843317747
iteration 207, loss = 0.0004965460393577814
iteration 208, loss = 0.0005144844762980938
iteration 209, loss = 0.0004632172058336437
iteration 210, loss = 0.000767040066421032
iteration 211, loss = 0.0003912376705557108
iteration 212, loss = 0.0009916600538417697
iteration 213, loss = 0.0004097605706192553
iteration 214, loss = 0.0006761609110981226
iteration 215, loss = 0.0004023965448141098
iteration 216, loss = 0.00044018810149282217
iteration 217, loss = 0.0005566583713516593
iteration 218, loss = 0.00041344997589476407
iteration 219, loss = 0.0010438062017783523
iteration 220, loss = 0.0004946531262248755
iteration 221, loss = 0.0002628058136906475
iteration 222, loss = 0.00043452894897200167
iteration 223, loss = 0.0006694752373732626
iteration 224, loss = 0.0003302069380879402
iteration 225, loss = 0.00040268563316203654
iteration 226, loss = 0.00026386158424429595
iteration 227, loss = 0.00042313389712944627
iteration 228, loss = 0.0012707896530628204
iteration 229, loss = 0.0005259186727926135
iteration 230, loss = 0.0004495789180509746
iteration 231, loss = 0.0003376012318767607
iteration 232, loss = 0.0004285434843041003
iteration 233, loss = 0.0003581363125704229
iteration 234, loss = 0.00034063932253047824
iteration 235, loss = 0.0005623708129860461
iteration 236, loss = 0.000501612201333046
iteration 237, loss = 0.00028151972219347954
iteration 238, loss = 0.000355576747097075
iteration 239, loss = 0.00023415293253492564
iteration 240, loss = 0.0003694636980071664
iteration 241, loss = 0.00034368562046438456
iteration 242, loss = 0.00035834271693602204
iteration 243, loss = 0.0007795749697834253
iteration 244, loss = 0.0004010226111859083
iteration 245, loss = 0.00139081000816077
iteration 246, loss = 0.0007397646550089121
iteration 247, loss = 0.0003900261071976274
iteration 248, loss = 0.0003148870309814811
iteration 249, loss = 0.0004764396289829165
iteration 250, loss = 0.000356212054612115
iteration 251, loss = 0.0004625807632692158
iteration 252, loss = 0.00038522627437487245
iteration 253, loss = 0.0003629425773397088
iteration 254, loss = 0.00032785075018182397
iteration 255, loss = 0.00041922542732208967
iteration 256, loss = 0.00046070211101323366
iteration 257, loss = 0.000572438701055944
iteration 258, loss = 0.00038975162897258997
iteration 259, loss = 0.00043923803605139256
iteration 260, loss = 0.0005369449499994516
iteration 261, loss = 0.00033278565388172865
iteration 262, loss = 0.0005380281945690513
iteration 263, loss = 0.0006772935157641768
iteration 264, loss = 0.0003637518093455583
iteration 265, loss = 0.0009534439886920154
iteration 266, loss = 0.00035759765887632966
iteration 267, loss = 0.0005832029273733497
iteration 268, loss = 0.00029024097602814436
iteration 269, loss = 0.0002385209663771093
iteration 270, loss = 0.0004333571996539831
iteration 271, loss = 0.0004885552916675806
iteration 272, loss = 0.0003646351397037506
iteration 273, loss = 0.000561786291655153
iteration 274, loss = 0.00039497806574217975
iteration 275, loss = 0.0007767154020257294
iteration 276, loss = 0.0008176903356797993
iteration 277, loss = 0.0007065820973366499
iteration 278, loss = 0.0016080253990367055
iteration 279, loss = 0.00040745455771684647
iteration 280, loss = 0.00022589621949009597
iteration 281, loss = 0.0004913252196274698
iteration 282, loss = 0.00036739144707098603
iteration 283, loss = 0.0009391592466272414
iteration 284, loss = 0.00040894889389164746
iteration 285, loss = 0.0003522803308442235
iteration 286, loss = 0.0003101141192018986
iteration 287, loss = 0.0005229826783761382
iteration 288, loss = 0.0003590797132346779
iteration 289, loss = 0.0004113635222893208
iteration 290, loss = 0.00038313778350129724
iteration 291, loss = 0.00042925201705656946
iteration 292, loss = 0.0003714900813065469
iteration 293, loss = 0.0009975871071219444
iteration 294, loss = 0.0005264417268335819
iteration 295, loss = 0.00043794489465653896
iteration 296, loss = 0.0006917069549672306
iteration 297, loss = 0.00037987768882885575
iteration 298, loss = 0.00048411620082333684
iteration 299, loss = 0.0007572098984383047
iteration 0, loss = 0.000714935187716037
iteration 1, loss = 0.0004420402692630887
iteration 2, loss = 0.0005230825627222657
iteration 3, loss = 0.0006550088874064386
iteration 4, loss = 0.00033567188074812293
iteration 5, loss = 0.00024578889133408666
iteration 6, loss = 0.00025390140945091844
iteration 7, loss = 0.0005422424874268472
iteration 8, loss = 0.001533153816126287
iteration 9, loss = 0.0003891687374562025
iteration 10, loss = 0.0003420314169488847
iteration 11, loss = 0.0008658767328597605
iteration 12, loss = 0.0005166525952517986
iteration 13, loss = 0.0004926585825160146
iteration 14, loss = 0.00021033818484283984
iteration 15, loss = 0.0005440748645924032
iteration 16, loss = 0.0006013867096044123
iteration 17, loss = 0.00035403287620283663
iteration 18, loss = 0.00033417154918424785
iteration 19, loss = 0.000486831646412611
iteration 20, loss = 0.0004133684269618243
iteration 21, loss = 0.00022934545995667577
iteration 22, loss = 0.0004104526014998555
iteration 23, loss = 0.0003893695247825235
iteration 24, loss = 0.0006446171319112182
iteration 25, loss = 0.0011864581611007452
iteration 26, loss = 0.0003398739791009575
iteration 27, loss = 0.00043125482625328004
iteration 28, loss = 0.00022813791292719543
iteration 29, loss = 0.00028530257986858487
iteration 30, loss = 0.0009259430808015168
iteration 31, loss = 0.0005250787362456322
iteration 32, loss = 0.000389155640732497
iteration 33, loss = 0.0012660293141379952
iteration 34, loss = 0.00020833862072322518
iteration 35, loss = 0.00036758556962013245
iteration 36, loss = 0.00038357434095814824
iteration 37, loss = 0.00033080705907195807
iteration 38, loss = 0.000325022847391665
iteration 39, loss = 0.0009449840290471911
iteration 40, loss = 0.00029202760197222233
iteration 41, loss = 0.0007278776611201465
iteration 42, loss = 0.00038826290983706713
iteration 43, loss = 0.00032717507565394044
iteration 44, loss = 0.0007912106229923666
iteration 45, loss = 0.00022604732657782733
iteration 46, loss = 0.00032019283389672637
iteration 47, loss = 0.0006417421391233802
iteration 48, loss = 0.00048692728159949183
iteration 49, loss = 0.00042436999501660466
iteration 50, loss = 0.0006992071866989136
iteration 51, loss = 0.0003319139941595495
iteration 52, loss = 0.000813780352473259
iteration 53, loss = 0.0012256953632459044
iteration 54, loss = 0.0004797896835952997
iteration 55, loss = 0.0006556112784892321
iteration 56, loss = 0.0005135063547641039
iteration 57, loss = 0.00020928215235471725
iteration 58, loss = 0.00027880215202458203
iteration 59, loss = 0.00031872812542133033
iteration 60, loss = 0.0005499154212884605
iteration 61, loss = 0.0002868467418011278
iteration 62, loss = 0.00031630462035536766
iteration 63, loss = 0.0005520867416635156
iteration 64, loss = 0.0009314918424934149
iteration 65, loss = 0.00042854074854403734
iteration 66, loss = 0.00043569551780819893
iteration 67, loss = 0.00039444517460651696
iteration 68, loss = 0.00046974726137705147
iteration 69, loss = 0.0005125337047502398
iteration 70, loss = 0.0003899794246535748
iteration 71, loss = 0.00025347559130750597
iteration 72, loss = 0.0003710634773597121
iteration 73, loss = 0.0013490031706169248
iteration 74, loss = 0.0006402413710020483
iteration 75, loss = 0.0004083268577232957
iteration 76, loss = 0.0004712631634902209
iteration 77, loss = 0.00032102916156873107
iteration 78, loss = 0.0002181476738769561
iteration 79, loss = 0.0007738396525382996
iteration 80, loss = 0.0002872295444831252
iteration 81, loss = 0.00028559615020640194
iteration 82, loss = 0.0006120065227150917
iteration 83, loss = 0.0003746689762920141
iteration 84, loss = 0.0003671869053505361
iteration 85, loss = 0.00031104753725230694
iteration 86, loss = 0.00028862961335107684
iteration 87, loss = 0.000482489907881245
iteration 88, loss = 0.0009550376562401652
iteration 89, loss = 0.0004403828934300691
iteration 90, loss = 0.0006499591981992126
iteration 91, loss = 0.0003109746321570128
iteration 92, loss = 0.0008196911658160388
iteration 93, loss = 0.0003133022109977901
iteration 94, loss = 0.0005530985654331744
iteration 95, loss = 0.0005937974783591926
iteration 96, loss = 0.00030911684734746814
iteration 97, loss = 0.0003312586050014943
iteration 98, loss = 0.00042109269998036325
iteration 99, loss = 0.0008311616838909686
iteration 100, loss = 0.0004479826893657446
iteration 101, loss = 0.0013719488633796573
iteration 102, loss = 0.0004885554662905633
iteration 103, loss = 0.0002658026642166078
iteration 104, loss = 0.0002698362513910979
iteration 105, loss = 0.00018671940779313445
iteration 106, loss = 0.0004205484874546528
iteration 107, loss = 0.0003547010419424623
iteration 108, loss = 0.0006967956433072686
iteration 109, loss = 0.0005072046769782901
iteration 110, loss = 0.0007092520827427506
iteration 111, loss = 0.0005700408946722746
iteration 112, loss = 0.0002525646414142102
iteration 113, loss = 0.0005080699338577688
iteration 114, loss = 0.00036575953708961606
iteration 115, loss = 0.0002785361430142075
iteration 116, loss = 0.0005453115445561707
iteration 117, loss = 0.0003257678763475269
iteration 118, loss = 0.0005063958815298975
iteration 119, loss = 0.00031729330657981336
iteration 120, loss = 0.0009540049941278994
iteration 121, loss = 0.0003352640778757632
iteration 122, loss = 0.00047094534966163337
iteration 123, loss = 0.00024504028260707855
iteration 124, loss = 0.0005459868116304278
iteration 125, loss = 0.000985706690698862
iteration 126, loss = 0.0002689595567062497
iteration 127, loss = 0.00032609677873551846
iteration 128, loss = 0.00047105399426072836
iteration 129, loss = 0.0005737212486565113
iteration 130, loss = 0.00033803971018642187
iteration 131, loss = 0.0009343845304101706
iteration 132, loss = 0.00029187268228270113
iteration 133, loss = 0.0007579243392683566
iteration 134, loss = 0.0006352986674755812
iteration 135, loss = 0.0006455559632740915
iteration 136, loss = 0.0004961562226526439
iteration 137, loss = 0.00030083220917731524
iteration 138, loss = 0.0003044505137950182
iteration 139, loss = 0.0002914179931394756
iteration 140, loss = 0.0002864724083337933
iteration 141, loss = 0.0006261550588533282
iteration 142, loss = 0.0011624173494055867
iteration 143, loss = 0.0003319733077660203
iteration 144, loss = 0.00046352535719051957
iteration 145, loss = 0.00030982098542153835
iteration 146, loss = 0.00041836415766738355
iteration 147, loss = 0.000460832699900493
iteration 148, loss = 0.0004237618122715503
iteration 149, loss = 0.0004337980644777417
iteration 150, loss = 0.00045300222700461745
iteration 151, loss = 0.0005247864173725247
iteration 152, loss = 0.0006149511900730431
iteration 153, loss = 0.0006054239929653704
iteration 154, loss = 0.0003385058371350169
iteration 155, loss = 0.00044308920041657984
iteration 156, loss = 0.0005138514679856598
iteration 157, loss = 0.00046585884410887957
iteration 158, loss = 0.0005670650280080736
iteration 159, loss = 0.0003141872293781489
iteration 160, loss = 0.0004713615926448256
iteration 161, loss = 0.00041494728066027164
iteration 162, loss = 0.0004149197193328291
iteration 163, loss = 0.00034717185189947486
iteration 164, loss = 0.00045963990851305425
iteration 165, loss = 0.0003988741955254227
iteration 166, loss = 0.0003967302036471665
iteration 167, loss = 0.000388406595448032
iteration 168, loss = 0.00032282876782119274
iteration 169, loss = 0.0014987310860306025
iteration 170, loss = 0.00026202265871688724
iteration 171, loss = 0.00021041149739176035
iteration 172, loss = 0.0012234316673129797
iteration 173, loss = 0.0002430761232972145
iteration 174, loss = 0.00025670003378763795
iteration 175, loss = 0.0008011260651983321
iteration 176, loss = 0.000484165531815961
iteration 177, loss = 0.000477870024042204
iteration 178, loss = 0.0005900079850107431
iteration 179, loss = 0.0005366040277294815
iteration 180, loss = 0.0002620703889988363
iteration 181, loss = 0.0005300402408465743
iteration 182, loss = 0.00044540385715663433
iteration 183, loss = 0.00025768508203327656
iteration 184, loss = 0.0004951556911692023
iteration 185, loss = 0.000441558106103912
iteration 186, loss = 0.000533152197021991
iteration 187, loss = 0.0005617575370706618
iteration 188, loss = 0.00044455091119743884
iteration 189, loss = 0.00042070148629136384
iteration 190, loss = 0.0005146308685652912
iteration 191, loss = 0.00046964248758740723
iteration 192, loss = 0.0016128809656947851
iteration 193, loss = 0.0005329494597390294
iteration 194, loss = 0.00038064413820393384
iteration 195, loss = 0.0004940507933497429
iteration 196, loss = 0.0012410402996465564
iteration 197, loss = 0.0005581150762736797
iteration 198, loss = 0.0007749536307528615
iteration 199, loss = 0.0009418015251867473
iteration 200, loss = 0.0004971941816620529
iteration 201, loss = 0.0008252575062215328
iteration 202, loss = 0.000338647048920393
iteration 203, loss = 0.00030818828963674605
iteration 204, loss = 0.00029861004441045225
iteration 205, loss = 0.000691247871145606
iteration 206, loss = 0.0006578406319022179
iteration 207, loss = 0.00038471538573503494
iteration 208, loss = 0.0008482723496854305
iteration 209, loss = 0.00040712085319682956
iteration 210, loss = 0.0007665124139748514
iteration 211, loss = 0.00018034924869425595
iteration 212, loss = 0.00045120331924408674
iteration 213, loss = 0.0005085498560220003
iteration 214, loss = 0.0004118149518035352
iteration 215, loss = 0.00023434161266777664
iteration 216, loss = 0.0006095697754062712
iteration 217, loss = 0.00045394428889267147
iteration 218, loss = 0.0003213562013115734
iteration 219, loss = 0.0009163333452306688
iteration 220, loss = 0.00025259878020733595
iteration 221, loss = 0.00027065453468821943
iteration 222, loss = 0.0004264127346687019
iteration 223, loss = 0.0002726113307289779
iteration 224, loss = 0.0006606568349525332
iteration 225, loss = 0.0004647948080673814
iteration 226, loss = 0.0003922962350770831
iteration 227, loss = 0.0004195612855255604
iteration 228, loss = 0.000330292503349483
iteration 229, loss = 0.0008793887682259083
iteration 230, loss = 0.0002485617296770215
iteration 231, loss = 0.0005553613882511854
iteration 232, loss = 0.0003117033338639885
iteration 233, loss = 0.00032251173979602754
iteration 234, loss = 0.0005902446573600173
iteration 235, loss = 0.00030209269607439637
iteration 236, loss = 0.00047680753050372005
iteration 237, loss = 0.0001822696503950283
iteration 238, loss = 0.0002797938068397343
iteration 239, loss = 0.0005382125382311642
iteration 240, loss = 0.00041765940841287374
iteration 241, loss = 0.0008767505642026663
iteration 242, loss = 0.0003640604845713824
iteration 243, loss = 0.0004329098737798631
iteration 244, loss = 0.00035363793722353876
iteration 245, loss = 0.00038394128205254674
iteration 246, loss = 0.00028521311469376087
iteration 247, loss = 0.0002830399025697261
iteration 248, loss = 0.0004790662496816367
iteration 249, loss = 0.0003155821468681097
iteration 250, loss = 0.00023847748525440693
iteration 251, loss = 0.00030643938225694
iteration 252, loss = 0.0003120702167507261
iteration 253, loss = 0.00023921928368508816
iteration 254, loss = 0.00034897541627287865
iteration 255, loss = 0.0005462246481329203
iteration 256, loss = 0.00044441077625378966
iteration 257, loss = 0.0005135720712132752
iteration 258, loss = 0.00042419778765179217
iteration 259, loss = 0.001322546391747892
iteration 260, loss = 0.0013191248290240765
iteration 261, loss = 0.00031419273000210524
iteration 262, loss = 0.0006747148581780493
iteration 263, loss = 0.0005275123403407633
iteration 264, loss = 0.0007935778121463954
iteration 265, loss = 0.0007973465835675597
iteration 266, loss = 0.0006135696894489229
iteration 267, loss = 0.0007315140683203936
iteration 268, loss = 0.00039001114782877266
iteration 269, loss = 0.0011473468039184809
iteration 270, loss = 0.0006034524412825704
iteration 271, loss = 0.0002719050389714539
iteration 272, loss = 0.00025302349240519106
iteration 273, loss = 0.00026897809584625065
iteration 274, loss = 0.00028814683901146054
iteration 275, loss = 0.0003309952444396913
iteration 276, loss = 0.00029930740129202604
iteration 277, loss = 0.0003285327402409166
iteration 278, loss = 0.000496699649374932
iteration 279, loss = 0.0003982273628935218
iteration 280, loss = 0.0003875315305776894
iteration 281, loss = 0.0003287817817181349
iteration 282, loss = 0.0003299725067336112
iteration 283, loss = 0.0011843963293358684
iteration 284, loss = 0.0005380880320444703
iteration 285, loss = 0.00041464343667030334
iteration 286, loss = 0.00021448532061185688
iteration 287, loss = 0.00023840753419790417
iteration 288, loss = 0.0004046536050736904
iteration 289, loss = 0.0004732170491479337
iteration 290, loss = 0.0005539851263165474
iteration 291, loss = 0.0003974288410972804
iteration 292, loss = 0.001124733709730208
iteration 293, loss = 0.00034818678977899253
iteration 294, loss = 0.000566477479878813
iteration 295, loss = 0.00027928617782890797
iteration 296, loss = 0.00042179759475402534
iteration 297, loss = 0.00045083864824846387
iteration 298, loss = 0.0011471707839518785
iteration 299, loss = 0.0005847043357789516
iteration 0, loss = 0.0004806279903277755
iteration 1, loss = 0.0007171855540946126
iteration 2, loss = 0.0004015039885416627
iteration 3, loss = 0.00023133961076382548
iteration 4, loss = 0.000520485860761255
iteration 5, loss = 0.0004496172768995166
iteration 6, loss = 0.0003135001752525568
iteration 7, loss = 0.001445492496713996
iteration 8, loss = 0.0002645505010150373
iteration 9, loss = 0.00029819353949278593
iteration 10, loss = 0.0004724777245428413
iteration 11, loss = 0.000984058016911149
iteration 12, loss = 0.00029573720530606806
iteration 13, loss = 0.000871922995429486
iteration 14, loss = 0.0002635710989125073
iteration 15, loss = 0.0006344966823235154
iteration 16, loss = 0.0004078150959685445
iteration 17, loss = 0.0008791620493866503
iteration 18, loss = 0.0005096156382933259
iteration 19, loss = 0.00034737016540020704
iteration 20, loss = 0.0005234976997599006
iteration 21, loss = 0.00030864356085658073
iteration 22, loss = 0.000326037232298404
iteration 23, loss = 0.00043349681072868407
iteration 24, loss = 0.0003720733511727303
iteration 25, loss = 0.0002920459082815796
iteration 26, loss = 0.0006133511778898537
iteration 27, loss = 0.001215810189023614
iteration 28, loss = 0.0003257746575400233
iteration 29, loss = 0.0003358813119120896
iteration 30, loss = 0.0003508767404127866
iteration 31, loss = 0.00031162111554294825
iteration 32, loss = 0.0012284930562600493
iteration 33, loss = 0.0003072170657105744
iteration 34, loss = 0.0004460324707906693
iteration 35, loss = 0.0004954008618369699
iteration 36, loss = 0.0013313745148479939
iteration 37, loss = 0.00040778311085887253
iteration 38, loss = 0.00033829177846200764
iteration 39, loss = 0.00031625808333046734
iteration 40, loss = 0.0003821656573563814
iteration 41, loss = 0.00019535809406079352
iteration 42, loss = 0.00033901873393915594
iteration 43, loss = 0.0010156338103115559
iteration 44, loss = 0.0006202292279340327
iteration 45, loss = 0.00043948242091573775
iteration 46, loss = 0.0005133937811478972
iteration 47, loss = 0.00047074651229195297
iteration 48, loss = 0.00030019524274393916
iteration 49, loss = 0.0009017844568006694
iteration 50, loss = 0.0003559433971531689
iteration 51, loss = 0.00034242888796143234
iteration 52, loss = 0.00030975777190178633
iteration 53, loss = 0.00040766040910966694
iteration 54, loss = 0.00022940090275369585
iteration 55, loss = 0.0008919002721086144
iteration 56, loss = 0.0003064634511247277
iteration 57, loss = 0.0007315087714232504
iteration 58, loss = 0.0002647399960551411
iteration 59, loss = 0.0007092964951880276
iteration 60, loss = 0.0009549258393235505
iteration 61, loss = 0.0003244632389396429
iteration 62, loss = 0.00033501628786325455
iteration 63, loss = 0.0004093324241694063
iteration 64, loss = 0.000305644964100793
iteration 65, loss = 0.00076452293433249
iteration 66, loss = 0.0003138258762191981
iteration 67, loss = 0.00027884141309186816
iteration 68, loss = 0.0006563859642483294
iteration 69, loss = 0.0007598886732012033
iteration 70, loss = 0.00039921331335790455
iteration 71, loss = 0.0004175866488367319
iteration 72, loss = 0.00036000018008053303
iteration 73, loss = 0.0005770133575424552
iteration 74, loss = 0.00031724348082207143
iteration 75, loss = 0.00046940005267970264
iteration 76, loss = 0.00037016154965385795
iteration 77, loss = 0.0005368554266169667
iteration 78, loss = 0.0006517016445286572
iteration 79, loss = 0.0003519102174323052
iteration 80, loss = 0.0007055863388814032
iteration 81, loss = 0.0003180764615535736
iteration 82, loss = 0.0002590232761576772
iteration 83, loss = 0.0006648346898145974
iteration 84, loss = 0.0007318761199712753
iteration 85, loss = 0.00042658537859097123
iteration 86, loss = 0.0005293818539939821
iteration 87, loss = 0.000437783426605165
iteration 88, loss = 0.00023070804309099913
iteration 89, loss = 0.00024788486189208925
iteration 90, loss = 0.0003345606673974544
iteration 91, loss = 0.00042412462062202394
iteration 92, loss = 0.00028439503512345254
iteration 93, loss = 0.0005481609841808677
iteration 94, loss = 0.0007636385853402317
iteration 95, loss = 0.0003839144192170352
iteration 96, loss = 0.00023031044111121446
iteration 97, loss = 0.00023151053756009787
iteration 98, loss = 0.0002608056820463389
iteration 99, loss = 0.0002319706545677036
iteration 100, loss = 0.0007782251923345029
iteration 101, loss = 0.00047883085790090263
iteration 102, loss = 0.00019923178479075432
iteration 103, loss = 0.0007260769489221275
iteration 104, loss = 0.00034796426189132035
iteration 105, loss = 0.0002771680592559278
iteration 106, loss = 0.000546528841368854
iteration 107, loss = 0.0005348364938981831
iteration 108, loss = 0.00040246863500215113
iteration 109, loss = 0.0005758938496001065
iteration 110, loss = 0.0003642737865447998
iteration 111, loss = 0.0003067165962420404
iteration 112, loss = 0.0003244512190576643
iteration 113, loss = 0.0002866478462237865
iteration 114, loss = 0.0005296560702845454
iteration 115, loss = 0.0007209207396954298
iteration 116, loss = 0.0005719022010453045
iteration 117, loss = 0.0011274933349341154
iteration 118, loss = 0.0003690956218633801
iteration 119, loss = 0.0004142519028391689
iteration 120, loss = 0.0003820207784883678
iteration 121, loss = 0.0006989588146097958
iteration 122, loss = 0.00043197086779400706
iteration 123, loss = 0.00025229493621736765
iteration 124, loss = 0.0005530735361389816
iteration 125, loss = 0.0004975908668711782
iteration 126, loss = 0.0006849637138657272
iteration 127, loss = 0.0002142125740647316
iteration 128, loss = 0.00044385233195498586
iteration 129, loss = 0.0002683355996850878
iteration 130, loss = 0.00029643141897395253
iteration 131, loss = 0.0006834156229160726
iteration 132, loss = 0.00037235423224046826
iteration 133, loss = 0.0003283550322521478
iteration 134, loss = 0.0002094821393257007
iteration 135, loss = 0.00030588905792683363
iteration 136, loss = 0.0002381486992817372
iteration 137, loss = 0.0008972871582955122
iteration 138, loss = 0.0004246046592015773
iteration 139, loss = 0.0006078516016714275
iteration 140, loss = 0.00039608392398804426
iteration 141, loss = 0.00030115764820948243
iteration 142, loss = 0.000484896037960425
iteration 143, loss = 0.000485535099869594
iteration 144, loss = 0.00023046269780024886
iteration 145, loss = 0.00040846707997843623
iteration 146, loss = 0.00032997000380419195
iteration 147, loss = 0.0008639884763397276
iteration 148, loss = 0.00035353153361938894
iteration 149, loss = 0.0005395765183493495
iteration 150, loss = 0.0004106772830709815
iteration 151, loss = 0.0004600313550326973
iteration 152, loss = 0.0003886570339091122
iteration 153, loss = 0.0004412480047903955
iteration 154, loss = 0.0002094270457746461
iteration 155, loss = 0.0002703556383494288
iteration 156, loss = 0.0004118915821891278
iteration 157, loss = 0.0012854468077421188
iteration 158, loss = 0.0002613390679471195
iteration 159, loss = 0.00037917940062470734
iteration 160, loss = 0.0011487220181152225
iteration 161, loss = 0.0003244828840252012
iteration 162, loss = 0.0007149468292482197
iteration 163, loss = 0.0003239500510971993
iteration 164, loss = 0.00044713131501339376
iteration 165, loss = 0.0003008117782883346
iteration 166, loss = 0.0003417458210606128
iteration 167, loss = 0.0004120019148103893
iteration 168, loss = 0.001659409492276609
iteration 169, loss = 0.0003134524740744382
iteration 170, loss = 0.0011032427428290248
iteration 171, loss = 0.0004434962465893477
iteration 172, loss = 0.0007996226195245981
iteration 173, loss = 0.00025256522349081933
iteration 174, loss = 0.00018600767361931503
iteration 175, loss = 0.00047534838085994124
iteration 176, loss = 0.00038805516669526696
iteration 177, loss = 0.0006196891772560775
iteration 178, loss = 0.000979105127044022
iteration 179, loss = 0.00020287757797632366
iteration 180, loss = 0.00031904043862596154
iteration 181, loss = 0.00031997147016227245
iteration 182, loss = 0.0006963419727981091
iteration 183, loss = 0.0002666996151674539
iteration 184, loss = 0.0005194818950258195
iteration 185, loss = 0.002005186164751649
iteration 186, loss = 0.0002460780960973352
iteration 187, loss = 0.0003045271150767803
iteration 188, loss = 0.00042042139102704823
iteration 189, loss = 0.000533920421730727
iteration 190, loss = 0.0005154619575478137
iteration 191, loss = 0.000321378669468686
iteration 192, loss = 0.00023461801174562424
iteration 193, loss = 0.00032651860965415835
iteration 194, loss = 0.00034074639552272856
iteration 195, loss = 0.00043425941839814186
iteration 196, loss = 0.0002500863920431584
iteration 197, loss = 0.00021216095774434507
iteration 198, loss = 0.00045241875341162086
iteration 199, loss = 0.00022374412219505757
iteration 200, loss = 0.00043637878843583167
iteration 201, loss = 0.0003156850580126047
iteration 202, loss = 0.0006125421496108174
iteration 203, loss = 0.00048569217324256897
iteration 204, loss = 0.00035439591738395393
iteration 205, loss = 0.000775833148509264
iteration 206, loss = 0.00046975022996775806
iteration 207, loss = 0.00030551874078810215
iteration 208, loss = 0.0008243272895924747
iteration 209, loss = 0.0005605413462035358
iteration 210, loss = 0.0005700923502445221
iteration 211, loss = 0.0007354295812547207
iteration 212, loss = 0.00045376273919828236
iteration 213, loss = 0.0005395931657403708
iteration 214, loss = 0.0004966360284015536
iteration 215, loss = 0.0003692998143378645
iteration 216, loss = 0.00028978317277505994
iteration 217, loss = 0.0005079535185359418
iteration 218, loss = 0.0004001845372840762
iteration 219, loss = 0.00046889204531908035
iteration 220, loss = 0.0003047737409360707
iteration 221, loss = 0.00043150843703188
iteration 222, loss = 0.0004351451643742621
iteration 223, loss = 0.00029105113935656846
iteration 224, loss = 0.0003862919402308762
iteration 225, loss = 0.00022456709120888263
iteration 226, loss = 0.000414134468883276
iteration 227, loss = 0.0004608194576576352
iteration 228, loss = 0.0004697935073636472
iteration 229, loss = 0.0003098239249084145
iteration 230, loss = 0.00037575257010757923
iteration 231, loss = 0.000262335961451754
iteration 232, loss = 0.00035373869468457997
iteration 233, loss = 0.0005122598959133029
iteration 234, loss = 0.0008571110665798187
iteration 235, loss = 0.0003440103027969599
iteration 236, loss = 0.0011784107191488147
iteration 237, loss = 0.00048055523075163364
iteration 238, loss = 0.00022531580179929733
iteration 239, loss = 0.0012990954564884305
iteration 240, loss = 0.0012419582344591618
iteration 241, loss = 0.0011246141511946917
iteration 242, loss = 0.0008710373076610267
iteration 243, loss = 0.00030420086113736033
iteration 244, loss = 0.00035723656765185297
iteration 245, loss = 0.0006274879560805857
iteration 246, loss = 0.0008927665185183287
iteration 247, loss = 0.00028265692526474595
iteration 248, loss = 0.00045471423072740436
iteration 249, loss = 0.0005533273797482252
iteration 250, loss = 0.000514918880071491
iteration 251, loss = 0.0002573970705270767
iteration 252, loss = 0.00031757092801854014
iteration 253, loss = 0.00041831424459815025
iteration 254, loss = 0.00033804416307248175
iteration 255, loss = 0.00030973064713180065
iteration 256, loss = 0.0004854174330830574
iteration 257, loss = 0.00026838338817469776
iteration 258, loss = 0.0004783189797308296
iteration 259, loss = 0.00024514266988262534
iteration 260, loss = 0.0002648487570695579
iteration 261, loss = 0.0004168860905338079
iteration 262, loss = 0.0004012415884062648
iteration 263, loss = 0.0013071275316178799
iteration 264, loss = 0.0005330065032467246
iteration 265, loss = 0.0002485507575329393
iteration 266, loss = 0.0007447304087691009
iteration 267, loss = 0.0012351657496765256
iteration 268, loss = 0.0002715396985877305
iteration 269, loss = 0.00037306162994354963
iteration 270, loss = 0.0003769959439523518
iteration 271, loss = 0.00020329475228209049
iteration 272, loss = 0.00043969767284579575
iteration 273, loss = 0.00020702018809970468
iteration 274, loss = 0.0004517383349593729
iteration 275, loss = 0.0002464154385961592
iteration 276, loss = 0.00047341262688860297
iteration 277, loss = 0.0005204114713706076
iteration 278, loss = 0.00040887697832658887
iteration 279, loss = 0.0006035880651324987
iteration 280, loss = 0.0005088555044494569
iteration 281, loss = 0.0005773638840764761
iteration 282, loss = 0.0002862655674107373
iteration 283, loss = 0.0003938970039598644
iteration 284, loss = 0.0004659018595702946
iteration 285, loss = 0.0003076502471230924
iteration 286, loss = 0.0007880594348534942
iteration 287, loss = 0.0005139975692145526
iteration 288, loss = 0.00040238600922748446
iteration 289, loss = 0.00029888152494095266
iteration 290, loss = 0.00038655742537230253
iteration 291, loss = 0.0004181296390015632
iteration 292, loss = 0.00041354630957357585
iteration 293, loss = 0.0006623757653869689
iteration 294, loss = 0.00034912614501081407
iteration 295, loss = 0.0003744592540897429
iteration 296, loss = 0.0002856850333046168
iteration 297, loss = 0.0003817233082372695
iteration 298, loss = 0.0003685071424115449
iteration 299, loss = 0.00027819955721497536
iteration 0, loss = 0.00041525508277118206
iteration 1, loss = 0.00036007232847623527
iteration 2, loss = 0.0003272430913057178
iteration 3, loss = 0.00023121095728129148
iteration 4, loss = 0.00041180261177942157
iteration 5, loss = 0.0004331595846451819
iteration 6, loss = 0.00028602476231753826
iteration 7, loss = 0.000793742947280407
iteration 8, loss = 0.0010644474532455206
iteration 9, loss = 0.00027042743749916553
iteration 10, loss = 0.0006712191388942301
iteration 11, loss = 0.0006251669838093221
iteration 12, loss = 0.0007251749048009515
iteration 13, loss = 0.0004210749175399542
iteration 14, loss = 0.0014731220435351133
iteration 15, loss = 0.00027174176648259163
iteration 16, loss = 0.0003218349302187562
iteration 17, loss = 0.0005433849291875958
iteration 18, loss = 0.0004286800976842642
iteration 19, loss = 0.00028680081595666707
iteration 20, loss = 0.00042293925071135163
iteration 21, loss = 0.0003134972939733416
iteration 22, loss = 0.0011186314513906837
iteration 23, loss = 0.001167166163213551
iteration 24, loss = 0.00023681118909735233
iteration 25, loss = 0.0005619172006845474
iteration 26, loss = 0.00035277940332889557
iteration 27, loss = 0.0003866410697810352
iteration 28, loss = 0.0007126384298317134
iteration 29, loss = 0.0002373379684286192
iteration 30, loss = 0.00043585768435150385
iteration 31, loss = 0.0005736005259677768
iteration 32, loss = 0.00025331764481961727
iteration 33, loss = 0.0004920201026834548
iteration 34, loss = 0.00030263446387834847
iteration 35, loss = 0.0007895715534687042
iteration 36, loss = 0.0003110671241302043
iteration 37, loss = 0.00031165845575742424
iteration 38, loss = 0.0004119565128348768
iteration 39, loss = 0.0004667442699428648
iteration 40, loss = 0.0004295491671655327
iteration 41, loss = 0.00021644904336426407
iteration 42, loss = 0.0005149786011315882
iteration 43, loss = 0.00026604626327753067
iteration 44, loss = 0.0002756735775619745
iteration 45, loss = 0.00047686733887530863
iteration 46, loss = 0.00042338858474977314
iteration 47, loss = 0.0004539130022749305
iteration 48, loss = 0.0002388593857176602
iteration 49, loss = 0.00039474177174270153
iteration 50, loss = 0.0007355574052780867
iteration 51, loss = 0.00026001979131251574
iteration 52, loss = 0.0006771547487005591
iteration 53, loss = 0.00022655294742435217
iteration 54, loss = 0.00035005901008844376
iteration 55, loss = 0.00116369326133281
iteration 56, loss = 0.00027251843130216
iteration 57, loss = 0.0007626665174029768
iteration 58, loss = 0.00030391878681257367
iteration 59, loss = 0.0003084583440795541
iteration 60, loss = 0.0002552740043029189
iteration 61, loss = 0.0001973641337826848
iteration 62, loss = 0.0004072105512022972
iteration 63, loss = 0.0002546385512687266
iteration 64, loss = 0.0006529425154440105
iteration 65, loss = 0.0004914887831546366
iteration 66, loss = 0.00034083781065419316
iteration 67, loss = 0.0006857254193164408
iteration 68, loss = 0.0004933723248541355
iteration 69, loss = 0.00026914713089354336
iteration 70, loss = 0.0002770974824670702
iteration 71, loss = 0.00022454935242421925
iteration 72, loss = 0.0004233350628055632
iteration 73, loss = 0.0005466079455800354
iteration 74, loss = 0.0005256818840280175
iteration 75, loss = 0.0006285639246925712
iteration 76, loss = 0.00043960308539681137
iteration 77, loss = 0.00026269315276294947
iteration 78, loss = 0.0005371609586291015
iteration 79, loss = 0.00036957880365662277
iteration 80, loss = 0.0006314204656518996
iteration 81, loss = 0.0003845339233521372
iteration 82, loss = 0.00027014812803827226
iteration 83, loss = 0.00030350833549164236
iteration 84, loss = 0.00025188899599015713
iteration 85, loss = 0.001317962072789669
iteration 86, loss = 0.00035806503728963435
iteration 87, loss = 0.00040730671025812626
iteration 88, loss = 0.000711640517693013
iteration 89, loss = 0.00037922951742075384
iteration 90, loss = 0.0014742898056283593
iteration 91, loss = 0.0008376810001209378
iteration 92, loss = 0.00029613342485390604
iteration 93, loss = 0.0005092541687190533
iteration 94, loss = 0.00047331376117654145
iteration 95, loss = 0.0008234780980274081
iteration 96, loss = 0.0003928505175281316
iteration 97, loss = 0.0003555678704287857
iteration 98, loss = 0.00024323076650034636
iteration 99, loss = 0.00046887961798347533
iteration 100, loss = 0.00030886128661222756
iteration 101, loss = 0.0004117305506952107
iteration 102, loss = 0.00032073105103336275
iteration 103, loss = 0.0005519356927834451
iteration 104, loss = 0.0004972428432665765
iteration 105, loss = 0.0004946807748638093
iteration 106, loss = 0.0005233878619037569
iteration 107, loss = 0.0004143229452893138
iteration 108, loss = 0.0005472700577229261
iteration 109, loss = 0.0004076031327713281
iteration 110, loss = 0.0004842505441047251
iteration 111, loss = 0.000784101546742022
iteration 112, loss = 0.00028960383497178555
iteration 113, loss = 0.00042121452861465514
iteration 114, loss = 0.0008480356773361564
iteration 115, loss = 0.0005584577447734773
iteration 116, loss = 0.00026356527814641595
iteration 117, loss = 0.00033286301186308265
iteration 118, loss = 0.0006149057298898697
iteration 119, loss = 0.0003523407503962517
iteration 120, loss = 0.0002910729090217501
iteration 121, loss = 0.00032648720662109554
iteration 122, loss = 0.000765525153838098
iteration 123, loss = 0.0002292583230882883
iteration 124, loss = 0.0004401091136969626
iteration 125, loss = 0.0004490171268116683
iteration 126, loss = 0.0005956172244623303
iteration 127, loss = 0.0007189600728452206
iteration 128, loss = 0.00027908809715881944
iteration 129, loss = 0.00035520142409950495
iteration 130, loss = 0.00040005205664783716
iteration 131, loss = 0.0002097530523315072
iteration 132, loss = 0.0006585011724382639
iteration 133, loss = 0.0001869245315901935
iteration 134, loss = 0.0005021198885515332
iteration 135, loss = 0.0006034969701431692
iteration 136, loss = 0.0003306256840005517
iteration 137, loss = 0.0005237830337136984
iteration 138, loss = 0.0003604261437430978
iteration 139, loss = 0.00036733667366206646
iteration 140, loss = 0.00048553181113675237
iteration 141, loss = 0.0017117634415626526
iteration 142, loss = 0.00024389430473092943
iteration 143, loss = 0.0004263789451215416
iteration 144, loss = 0.00023769513063598424
iteration 145, loss = 0.0004291866789571941
iteration 146, loss = 0.00026510696625337005
iteration 147, loss = 0.00031838129507377744
iteration 148, loss = 0.0003347732708789408
iteration 149, loss = 0.00036955164978280663
iteration 150, loss = 0.0003298814408481121
iteration 151, loss = 0.00030817402875982225
iteration 152, loss = 0.0005185799091123044
iteration 153, loss = 0.00036190907121635973
iteration 154, loss = 0.0002108199114445597
iteration 155, loss = 0.001034633140079677
iteration 156, loss = 0.0004182361299172044
iteration 157, loss = 0.0004587828298099339
iteration 158, loss = 0.0011261876206845045
iteration 159, loss = 0.0003964899806305766
iteration 160, loss = 0.000270639342488721
iteration 161, loss = 0.0006744113052263856
iteration 162, loss = 0.0004639115068130195
iteration 163, loss = 0.0003083001065533608
iteration 164, loss = 0.00040475805872119963
iteration 165, loss = 0.0005934041691944003
iteration 166, loss = 0.0002976029645651579
iteration 167, loss = 0.00032416501198895276
iteration 168, loss = 0.00044669146882370114
iteration 169, loss = 0.0003072547260671854
iteration 170, loss = 0.00041320163290947676
iteration 171, loss = 0.0004148322041146457
iteration 172, loss = 0.0003002826124429703
iteration 173, loss = 0.00030961379525251687
iteration 174, loss = 0.0003928957739844918
iteration 175, loss = 0.00035320795723237097
iteration 176, loss = 0.00042578967986628413
iteration 177, loss = 0.00035219197161495686
iteration 178, loss = 0.000365673826308921
iteration 179, loss = 0.0002059904218185693
iteration 180, loss = 0.0007593028713017702
iteration 181, loss = 0.00025461046607233584
iteration 182, loss = 0.00027576673892326653
iteration 183, loss = 0.0006012688390910625
iteration 184, loss = 0.000421458447817713
iteration 185, loss = 0.0004037210892420262
iteration 186, loss = 0.0003281670797150582
iteration 187, loss = 0.00043611519504338503
iteration 188, loss = 0.0004728265048470348
iteration 189, loss = 0.00018425552116241306
iteration 190, loss = 0.0005004815175198019
iteration 191, loss = 0.0006492994725704193
iteration 192, loss = 0.00042358116479590535
iteration 193, loss = 0.0005989743513055146
iteration 194, loss = 0.000381642603315413
iteration 195, loss = 0.0002680035540834069
iteration 196, loss = 0.0004632523632608354
iteration 197, loss = 0.0002549602650105953
iteration 198, loss = 0.0003304088313598186
iteration 199, loss = 0.0002400878438493237
iteration 200, loss = 0.00033043368603102863
iteration 201, loss = 0.0006948588415980339
iteration 202, loss = 0.00030619834433309734
iteration 203, loss = 0.000612159725278616
iteration 204, loss = 0.0012167813256382942
iteration 205, loss = 0.0007259072735905647
iteration 206, loss = 0.0008780168136581779
iteration 207, loss = 0.0008104043663479388
iteration 208, loss = 0.00039479369297623634
iteration 209, loss = 0.0003943239862564951
iteration 210, loss = 0.00037751751369796693
iteration 211, loss = 0.00033813592744991183
iteration 212, loss = 0.0004605027206707746
iteration 213, loss = 0.0006540700560435653
iteration 214, loss = 0.0004346303176134825
iteration 215, loss = 0.000393257214454934
iteration 216, loss = 0.0004174421774223447
iteration 217, loss = 0.0003614510642364621
iteration 218, loss = 0.000549411284737289
iteration 219, loss = 0.00038350705290213227
iteration 220, loss = 0.0011585456086322665
iteration 221, loss = 0.0007708696066401899
iteration 222, loss = 0.0003160353226121515
iteration 223, loss = 0.00041696266271173954
iteration 224, loss = 0.0005205479683354497
iteration 225, loss = 0.0005950131453573704
iteration 226, loss = 0.000231647165492177
iteration 227, loss = 0.0012379519175738096
iteration 228, loss = 0.0003461448068264872
iteration 229, loss = 0.00041866395622491837
iteration 230, loss = 0.00017851855955086648
iteration 231, loss = 0.0003347728052176535
iteration 232, loss = 0.0006925058551132679
iteration 233, loss = 0.000291474221739918
iteration 234, loss = 0.0003482416504994035
iteration 235, loss = 0.00028788362396880984
iteration 236, loss = 0.0005440890090540051
iteration 237, loss = 0.0002461762633174658
iteration 238, loss = 0.0007094889879226685
iteration 239, loss = 0.00038985873106867075
iteration 240, loss = 0.0003279394004493952
iteration 241, loss = 0.00037152384174987674
iteration 242, loss = 0.0003801968414336443
iteration 243, loss = 0.00022534429444931448
iteration 244, loss = 0.000231410056585446
iteration 245, loss = 0.0005078240064904094
iteration 246, loss = 0.0003079547896049917
iteration 247, loss = 0.0002659621532075107
iteration 248, loss = 0.0004184411955066025
iteration 249, loss = 0.0006477214046753943
iteration 250, loss = 0.0005129387136548758
iteration 251, loss = 0.0003053894033655524
iteration 252, loss = 0.00031345567549578846
iteration 253, loss = 0.0006878785789012909
iteration 254, loss = 0.0003990567638538778
iteration 255, loss = 0.0003144883958157152
iteration 256, loss = 0.00032533740159124136
iteration 257, loss = 0.0006195167661644518
iteration 258, loss = 0.0006050254451110959
iteration 259, loss = 0.0002620325540192425
iteration 260, loss = 0.0007117982022464275
iteration 261, loss = 0.0005702654016204178
iteration 262, loss = 0.0003081375325564295
iteration 263, loss = 0.0003578150935936719
iteration 264, loss = 0.0004690753703471273
iteration 265, loss = 0.0006477462593466043
iteration 266, loss = 0.0004249392659403384
iteration 267, loss = 0.0007574443006888032
iteration 268, loss = 0.0005826590349897742
iteration 269, loss = 0.00037334818625822663
iteration 270, loss = 0.0003204352979082614
iteration 271, loss = 0.00035249325446784496
iteration 272, loss = 0.0004892696160823107
iteration 273, loss = 0.0003192127915099263
iteration 274, loss = 0.00032833742443472147
iteration 275, loss = 0.00037277472438290715
iteration 276, loss = 0.0002752733416855335
iteration 277, loss = 0.00036517344415187836
iteration 278, loss = 0.00026177105610258877
iteration 279, loss = 0.0004881303175352514
iteration 280, loss = 0.0005043529090471566
iteration 281, loss = 0.0002902954875025898
iteration 282, loss = 0.00018750200979411602
iteration 283, loss = 0.00036290340358391404
iteration 284, loss = 0.0003244425752200186
iteration 285, loss = 0.00037421638262458146
iteration 286, loss = 0.0010959296487271786
iteration 287, loss = 0.000296383019303903
iteration 288, loss = 0.0008343418012373149
iteration 289, loss = 0.0004077307239640504
iteration 290, loss = 0.0010870692785829306
iteration 291, loss = 0.00040341707062907517
iteration 292, loss = 0.000219266046769917
iteration 293, loss = 0.0011717630550265312
iteration 294, loss = 0.0004242028808221221
iteration 295, loss = 0.00033927630283869803
iteration 296, loss = 0.0002597429556772113
iteration 297, loss = 0.00034988747211173177
iteration 298, loss = 0.0005182876484468579
iteration 299, loss = 0.00043788354378193617
iteration 0, loss = 0.0008014474296942353
iteration 1, loss = 0.00039337031194008887
iteration 2, loss = 0.0002893786004278809
iteration 3, loss = 0.00040804053423926234
iteration 4, loss = 0.00038332169060595334
iteration 5, loss = 0.000673081201966852
iteration 6, loss = 0.0003070599341299385
iteration 7, loss = 0.0002759186609182507
iteration 8, loss = 0.0007263517472893
iteration 9, loss = 0.00024133955594152212
iteration 10, loss = 0.00033786852145567536
iteration 11, loss = 0.0003075815038755536
iteration 12, loss = 0.0004750556545332074
iteration 13, loss = 0.00019525706011336297
iteration 14, loss = 0.0004270298813935369
iteration 15, loss = 0.00036856887163594365
iteration 16, loss = 0.00039884663419798017
iteration 17, loss = 0.00045225254143588245
iteration 18, loss = 0.0006280486704781651
iteration 19, loss = 0.00046596009633503854
iteration 20, loss = 0.00029942725086584687
iteration 21, loss = 0.0011420899536460638
iteration 22, loss = 0.0003522701153997332
iteration 23, loss = 0.0002645420900080353
iteration 24, loss = 0.0003935645509045571
iteration 25, loss = 0.0004960080259479582
iteration 26, loss = 0.00030392847838811576
iteration 27, loss = 0.0007618703530170023
iteration 28, loss = 0.00022914480359759182
iteration 29, loss = 0.0003566720988601446
iteration 30, loss = 0.0010494710877537727
iteration 31, loss = 0.00041837216122075915
iteration 32, loss = 0.0005741619970649481
iteration 33, loss = 0.0003901190939359367
iteration 34, loss = 0.00043691048631444573
iteration 35, loss = 0.00034475064603611827
iteration 36, loss = 0.00021111639216542244
iteration 37, loss = 0.00025854160776361823
iteration 38, loss = 0.000302957312669605
iteration 39, loss = 0.0003021387383341789
iteration 40, loss = 0.000356055999873206
iteration 41, loss = 0.0008374103344976902
iteration 42, loss = 0.00027547101490199566
iteration 43, loss = 0.0004841108457185328
iteration 44, loss = 0.00028906139777973294
iteration 45, loss = 0.000634034164249897
iteration 46, loss = 0.0005214622360654175
iteration 47, loss = 0.001054182299412787
iteration 48, loss = 0.00025946140522137284
iteration 49, loss = 0.0004287401679903269
iteration 50, loss = 0.00033493159571662545
iteration 51, loss = 0.00039820114034228027
iteration 52, loss = 0.0007981167873367667
iteration 53, loss = 0.00027422787388786674
iteration 54, loss = 0.0005115231615491211
iteration 55, loss = 0.00045708450488746166
iteration 56, loss = 0.0005497451056726277
iteration 57, loss = 0.00022055109729990363
iteration 58, loss = 0.0003006921906489879
iteration 59, loss = 0.0003565427614375949
iteration 60, loss = 0.0004134060873184353
iteration 61, loss = 0.0003267468127887696
iteration 62, loss = 0.0003506802022457123
iteration 63, loss = 0.0003665404801722616
iteration 64, loss = 0.0005479565588757396
iteration 65, loss = 0.0006260369555093348
iteration 66, loss = 0.0002452664775773883
iteration 67, loss = 0.00044929637806490064
iteration 68, loss = 0.0004083021485712379
iteration 69, loss = 0.00026632403023540974
iteration 70, loss = 0.001107209944166243
iteration 71, loss = 0.0003238838689867407
iteration 72, loss = 0.00036816339707002044
iteration 73, loss = 0.000358159450115636
iteration 74, loss = 0.0004488196864258498
iteration 75, loss = 0.0003884031902998686
iteration 76, loss = 0.0004602771659847349
iteration 77, loss = 0.0005692648701369762
iteration 78, loss = 0.0002980396384373307
iteration 79, loss = 0.0006805789307691157
iteration 80, loss = 0.0010809435043483973
iteration 81, loss = 0.000493874482344836
iteration 82, loss = 0.0002918993413913995
iteration 83, loss = 0.00047458335757255554
iteration 84, loss = 0.00033181466278620064
iteration 85, loss = 0.0003413744270801544
iteration 86, loss = 0.0003111210244242102
iteration 87, loss = 0.0004564041446428746
iteration 88, loss = 0.00020160467829555273
iteration 89, loss = 0.0004898679908365011
iteration 90, loss = 0.0003572542918846011
iteration 91, loss = 0.00020881004456896335
iteration 92, loss = 0.00035167697933502495
iteration 93, loss = 0.0005269425455480814
iteration 94, loss = 0.0006117900484241545
iteration 95, loss = 0.00047322516911663115
iteration 96, loss = 0.00047589896712452173
iteration 97, loss = 0.0005450116004794836
iteration 98, loss = 0.0002907771267928183
iteration 99, loss = 0.0018706885166466236
iteration 100, loss = 0.0004553138860501349
iteration 101, loss = 0.0009504760382696986
iteration 102, loss = 0.0005372341256588697
iteration 103, loss = 0.0006664891261607409
iteration 104, loss = 0.0002490567567292601
iteration 105, loss = 0.0005298447795212269
iteration 106, loss = 0.0006029491778463125
iteration 107, loss = 0.00024962984025478363
iteration 108, loss = 0.0005980802234262228
iteration 109, loss = 0.0007254670490510762
iteration 110, loss = 0.00030175282154232264
iteration 111, loss = 0.0004391418769955635
iteration 112, loss = 0.00027784303529188037
iteration 113, loss = 0.0003076144203078002
iteration 114, loss = 0.00041571009205654263
iteration 115, loss = 0.00023330972180701792
iteration 116, loss = 0.0002872105105780065
iteration 117, loss = 0.00020553881768137217
iteration 118, loss = 0.0008101878338493407
iteration 119, loss = 0.00041716729174368083
iteration 120, loss = 0.00029699146398343146
iteration 121, loss = 0.00035669791395775974
iteration 122, loss = 0.00030101495212875307
iteration 123, loss = 0.0004459023766685277
iteration 124, loss = 0.0002600499428808689
iteration 125, loss = 0.0005999563727527857
iteration 126, loss = 0.00024314103939104825
iteration 127, loss = 0.0002671625406946987
iteration 128, loss = 0.0006024083704687655
iteration 129, loss = 0.0004409567336551845
iteration 130, loss = 0.000415378890465945
iteration 131, loss = 0.000288861570879817
iteration 132, loss = 0.0003574186412151903
iteration 133, loss = 0.0004180523392278701
iteration 134, loss = 0.000400726858060807
iteration 135, loss = 0.0003576908493414521
iteration 136, loss = 0.0002872948534786701
iteration 137, loss = 0.00035126955481246114
iteration 138, loss = 0.0004537478380370885
iteration 139, loss = 0.0002516904496587813
iteration 140, loss = 0.000477277091704309
iteration 141, loss = 0.0005166453192941844
iteration 142, loss = 0.0004731323861051351
iteration 143, loss = 0.0004830846737604588
iteration 144, loss = 0.000323536922223866
iteration 145, loss = 0.0006091255345381796
iteration 146, loss = 0.00029944253037683666
iteration 147, loss = 0.0003571624984033406
iteration 148, loss = 0.00032595288939774036
iteration 149, loss = 0.00044986401917412877
iteration 150, loss = 0.0005268406239338219
iteration 151, loss = 0.00032477619242854416
iteration 152, loss = 0.00035908070276491344
iteration 153, loss = 0.0002555182436481118
iteration 154, loss = 0.00040199756040237844
iteration 155, loss = 0.000244219321757555
iteration 156, loss = 0.00030968684586696327
iteration 157, loss = 0.000799206318333745
iteration 158, loss = 0.0005765901296399534
iteration 159, loss = 0.00038862371002323925
iteration 160, loss = 0.00040453660767525434
iteration 161, loss = 0.0006728605949319899
iteration 162, loss = 0.000345270469551906
iteration 163, loss = 0.0003232458548154682
iteration 164, loss = 0.00017358773038722575
iteration 165, loss = 0.0003915473644156009
iteration 166, loss = 0.00023273995611816645
iteration 167, loss = 0.0007631367188878357
iteration 168, loss = 0.00039918377296999097
iteration 169, loss = 0.00033526698825880885
iteration 170, loss = 0.0002811186423059553
iteration 171, loss = 0.0003234665491618216
iteration 172, loss = 0.0004927494446747005
iteration 173, loss = 0.00029714900301769376
iteration 174, loss = 0.0005401537055149674
iteration 175, loss = 0.00027258580666966736
iteration 176, loss = 0.0007369451341219246
iteration 177, loss = 0.0002830483135767281
iteration 178, loss = 0.0003776343946810812
iteration 179, loss = 0.0007486195536330342
iteration 180, loss = 0.00036081718280911446
iteration 181, loss = 0.0007247188477776945
iteration 182, loss = 0.00023242621682584286
iteration 183, loss = 0.00038464320823550224
iteration 184, loss = 0.0004248720651958138
iteration 185, loss = 0.00036728507257066667
iteration 186, loss = 0.00024571767426095903
iteration 187, loss = 0.00023981872072909027
iteration 188, loss = 0.0003328571328893304
iteration 189, loss = 0.0004078760102856904
iteration 190, loss = 0.00027124417829327285
iteration 191, loss = 0.00037674998748116195
iteration 192, loss = 0.0003396680695004761
iteration 193, loss = 0.0007138646324165165
iteration 194, loss = 0.0002821108209900558
iteration 195, loss = 0.0004321587330196053
iteration 196, loss = 0.001072156592272222
iteration 197, loss = 0.000320997933158651
iteration 198, loss = 0.0007799278246238828
iteration 199, loss = 0.00030297969351522624
iteration 200, loss = 0.00037118291947990656
iteration 201, loss = 0.0004291083896532655
iteration 202, loss = 0.00028473889688029885
iteration 203, loss = 0.0005602431483566761
iteration 204, loss = 0.00029369560070335865
iteration 205, loss = 0.00045543324085883796
iteration 206, loss = 0.00047506665578112006
iteration 207, loss = 0.00032887322595342994
iteration 208, loss = 0.00029851566068828106
iteration 209, loss = 0.0002828022406902164
iteration 210, loss = 0.000514645129442215
iteration 211, loss = 0.0003107288503088057
iteration 212, loss = 0.00034128138213418424
iteration 213, loss = 0.001239742967300117
iteration 214, loss = 0.0006961458711884916
iteration 215, loss = 0.0004548613214865327
iteration 216, loss = 0.0002880413376260549
iteration 217, loss = 0.00029322822229005396
iteration 218, loss = 0.00045337696792557836
iteration 219, loss = 0.0004681488499045372
iteration 220, loss = 0.0003653681487776339
iteration 221, loss = 0.0006401317077688873
iteration 222, loss = 0.0007544554537162185
iteration 223, loss = 0.0006754412315785885
iteration 224, loss = 0.00031119806226342916
iteration 225, loss = 0.00026695989072322845
iteration 226, loss = 0.00030371875618584454
iteration 227, loss = 0.0003206300316378474
iteration 228, loss = 0.0004570743767544627
iteration 229, loss = 0.00022453995188698173
iteration 230, loss = 0.0002560605062171817
iteration 231, loss = 0.0004710685752797872
iteration 232, loss = 0.0007571539026685059
iteration 233, loss = 0.00026744799106381834
iteration 234, loss = 0.000416342809330672
iteration 235, loss = 0.0002530061756260693
iteration 236, loss = 0.0008965044980868697
iteration 237, loss = 0.0002757989859674126
iteration 238, loss = 0.00030320719815790653
iteration 239, loss = 0.0004794052802026272
iteration 240, loss = 0.0002695166622288525
iteration 241, loss = 0.00021703516540583223
iteration 242, loss = 0.0004691504000220448
iteration 243, loss = 0.0003306591243017465
iteration 244, loss = 0.001121910521760583
iteration 245, loss = 0.0002556422841735184
iteration 246, loss = 0.0002515694941394031
iteration 247, loss = 0.0004490670398809016
iteration 248, loss = 0.0005397184286266565
iteration 249, loss = 0.00045643519842997193
iteration 250, loss = 0.0003212665324099362
iteration 251, loss = 0.0003696037456393242
iteration 252, loss = 0.000323355954606086
iteration 253, loss = 0.00021939625730738044
iteration 254, loss = 0.0004266305186320096
iteration 255, loss = 0.0010441162157803774
iteration 256, loss = 0.00043589415145106614
iteration 257, loss = 0.0006981944316066802
iteration 258, loss = 0.0002510157064534724
iteration 259, loss = 0.00038356290315277874
iteration 260, loss = 0.00026571008493192494
iteration 261, loss = 0.00027975672855973244
iteration 262, loss = 0.0010431943228468299
iteration 263, loss = 0.0007856856100261211
iteration 264, loss = 0.001106332871131599
iteration 265, loss = 0.0003246850392315537
iteration 266, loss = 0.0004856215964537114
iteration 267, loss = 0.001113601610995829
iteration 268, loss = 0.0004084846004843712
iteration 269, loss = 0.0003708868462126702
iteration 270, loss = 0.0005006117862649262
iteration 271, loss = 0.0011758421314880252
iteration 272, loss = 0.0005300339544191957
iteration 273, loss = 0.0001853026042226702
iteration 274, loss = 0.0010510970605537295
iteration 275, loss = 0.0002638853038661182
iteration 276, loss = 0.00032070011366158724
iteration 277, loss = 0.0005417628563009202
iteration 278, loss = 0.0005400097579695284
iteration 279, loss = 0.000246401788899675
iteration 280, loss = 0.0001838925963966176
iteration 281, loss = 0.0001848506071837619
iteration 282, loss = 0.0002736939932219684
iteration 283, loss = 0.0005629523075185716
iteration 284, loss = 0.0006193197914399207
iteration 285, loss = 0.0003166054957546294
iteration 286, loss = 0.00031799491262063384
iteration 287, loss = 0.0011030023451894522
iteration 288, loss = 0.00042661448242142797
iteration 289, loss = 0.00023615884128957987
iteration 290, loss = 0.0004463356744963676
iteration 291, loss = 0.000334546115482226
iteration 292, loss = 0.00029305770294740796
iteration 293, loss = 0.0004666463646572083
iteration 294, loss = 0.0009904082398861647
iteration 295, loss = 0.00032700086012482643
iteration 296, loss = 0.00039242219645529985
iteration 297, loss = 0.0005241414182819426
iteration 298, loss = 0.0003548464155755937
iteration 299, loss = 0.000224522955249995
iteration 0, loss = 0.00025228140293620527
iteration 1, loss = 0.0006258192006498575
iteration 2, loss = 0.00034859825973398983
iteration 3, loss = 0.0002928439062088728
iteration 4, loss = 0.0003019764262717217
iteration 5, loss = 0.0001965990086318925
iteration 6, loss = 0.0005737664760090411
iteration 7, loss = 0.0004121892270632088
iteration 8, loss = 0.00028284324798732996
iteration 9, loss = 0.00022667180746793747
iteration 10, loss = 0.000461630173958838
iteration 11, loss = 0.00047717863344587386
iteration 12, loss = 0.00045924438745714724
iteration 13, loss = 0.0011489035096019506
iteration 14, loss = 0.00037720773252658546
iteration 15, loss = 0.00035522086545825005
iteration 16, loss = 0.00120172172319144
iteration 17, loss = 0.0003046014462597668
iteration 18, loss = 0.00027856940869241953
iteration 19, loss = 0.0004972858005203307
iteration 20, loss = 0.00019972145673818886
iteration 21, loss = 0.00044444730156101286
iteration 22, loss = 0.00023552108905278146
iteration 23, loss = 0.00040828317287378013
iteration 24, loss = 0.0007139635272324085
iteration 25, loss = 0.00038292884710244834
iteration 26, loss = 0.00029034403269179165
iteration 27, loss = 0.00038488328573293984
iteration 28, loss = 0.0002617571735754609
iteration 29, loss = 0.001120887347497046
iteration 30, loss = 0.00026008329587057233
iteration 31, loss = 0.000286756141576916
iteration 32, loss = 0.00037249858723953366
iteration 33, loss = 0.0005141365109011531
iteration 34, loss = 0.0010568415746092796
iteration 35, loss = 0.000492541934363544
iteration 36, loss = 0.0002661225153133273
iteration 37, loss = 0.0005903028650209308
iteration 38, loss = 0.0006293630576692522
iteration 39, loss = 0.0005135845276527107
iteration 40, loss = 0.00023084881831891835
iteration 41, loss = 0.0012312954058870673
iteration 42, loss = 0.0014237727737054229
iteration 43, loss = 0.0007739480352029204
iteration 44, loss = 0.0002657220757100731
iteration 45, loss = 0.00029832168365828693
iteration 46, loss = 0.00022113969316706061
iteration 47, loss = 0.00037452401011250913
iteration 48, loss = 0.00036382093094289303
iteration 49, loss = 0.0003691435558721423
iteration 50, loss = 0.00034171860897913575
iteration 51, loss = 0.0003651154402177781
iteration 52, loss = 0.00024015137751121074
iteration 53, loss = 0.0005353635642677546
iteration 54, loss = 0.0006978711462579668
iteration 55, loss = 0.0013315307442098856
iteration 56, loss = 0.00027852930361405015
iteration 57, loss = 0.0005084479344077408
iteration 58, loss = 0.00019594488549046218
iteration 59, loss = 0.0010190863395109773
iteration 60, loss = 0.00042886988376267254
iteration 61, loss = 0.00031004450283944607
iteration 62, loss = 0.0005274938885122538
iteration 63, loss = 0.0010641361586749554
iteration 64, loss = 0.000323128595482558
iteration 65, loss = 0.0003236676275264472
iteration 66, loss = 0.0002912438940256834
iteration 67, loss = 0.0003675922052934766
iteration 68, loss = 0.00031894317362457514
iteration 69, loss = 0.0005318437470123172
iteration 70, loss = 0.0002721152559388429
iteration 71, loss = 0.0004059700295329094
iteration 72, loss = 0.0007060154457576573
iteration 73, loss = 0.0002652054827194661
iteration 74, loss = 0.0006461853627115488
iteration 75, loss = 0.0005166828632354736
iteration 76, loss = 0.00046109280083328485
iteration 77, loss = 0.0006821663700975478
iteration 78, loss = 0.0003286962164565921
iteration 79, loss = 0.0005147730116732419
iteration 80, loss = 0.00041417722241021693
iteration 81, loss = 0.0005272721173241735
iteration 82, loss = 0.0004630653711501509
iteration 83, loss = 0.000320245890179649
iteration 84, loss = 0.00037551214336417615
iteration 85, loss = 0.00036342983366921544
iteration 86, loss = 0.0011897770455107093
iteration 87, loss = 0.0009992404375225306
iteration 88, loss = 0.0002740552881732583
iteration 89, loss = 0.0004268527263775468
iteration 90, loss = 0.0002693987335078418
iteration 91, loss = 0.00037714882637374103
iteration 92, loss = 0.00019256185623817146
iteration 93, loss = 0.0003195600293111056
iteration 94, loss = 0.00027475348906591535
iteration 95, loss = 0.0006816444220021367
iteration 96, loss = 0.00023526465520262718
iteration 97, loss = 0.0003875751281157136
iteration 98, loss = 0.00030195745057426393
iteration 99, loss = 0.00020607902843039483
iteration 100, loss = 0.0003578142204787582
iteration 101, loss = 0.0007817731820978224
iteration 102, loss = 0.00040602489025332034
iteration 103, loss = 0.00035322768962942064
iteration 104, loss = 0.00045192823745310307
iteration 105, loss = 0.0005216535646468401
iteration 106, loss = 0.0003168692346662283
iteration 107, loss = 0.0012097032740712166
iteration 108, loss = 0.00022686395095661283
iteration 109, loss = 0.00038192543433979154
iteration 110, loss = 0.000621294544544071
iteration 111, loss = 0.0003000899450853467
iteration 112, loss = 0.0005454337224364281
iteration 113, loss = 0.00021102924074511975
iteration 114, loss = 0.00041516972123645246
iteration 115, loss = 0.00038038144703023136
iteration 116, loss = 0.00019709754269570112
iteration 117, loss = 0.00039876814116723835
iteration 118, loss = 0.00025851221289485693
iteration 119, loss = 0.00033381168032065034
iteration 120, loss = 0.0003907392092514783
iteration 121, loss = 0.0002126379986293614
iteration 122, loss = 0.0002494633081369102
iteration 123, loss = 0.00034085349761880934
iteration 124, loss = 0.0002572152588982135
iteration 125, loss = 0.00043053299305029213
iteration 126, loss = 0.00022130490106064826
iteration 127, loss = 0.00024316232884302735
iteration 128, loss = 0.0003072003601118922
iteration 129, loss = 0.0003396775864530355
iteration 130, loss = 0.000854270241688937
iteration 131, loss = 0.0003049803199246526
iteration 132, loss = 0.0005502502899616957
iteration 133, loss = 0.00033053767401725054
iteration 134, loss = 0.00031714781653136015
iteration 135, loss = 0.000701167737133801
iteration 136, loss = 0.00024714082246646285
iteration 137, loss = 0.00037405587499961257
iteration 138, loss = 0.00029105760040692985
iteration 139, loss = 0.0004865561204496771
iteration 140, loss = 0.0002990241046063602
iteration 141, loss = 0.00031161331571638584
iteration 142, loss = 0.0004055446479469538
iteration 143, loss = 0.00021340424427762628
iteration 144, loss = 0.0013255418743938208
iteration 145, loss = 0.0004362640029285103
iteration 146, loss = 0.000286456779576838
iteration 147, loss = 0.00033731444273144007
iteration 148, loss = 0.00037545981467701495
iteration 149, loss = 0.00047357924631796777
iteration 150, loss = 0.0002596375998109579
iteration 151, loss = 0.0007075866451486945
iteration 152, loss = 0.00019490972044877708
iteration 153, loss = 0.000382542290026322
iteration 154, loss = 0.0002365887921769172
iteration 155, loss = 0.000459231436252594
iteration 156, loss = 0.00027261421200819314
iteration 157, loss = 0.0002950236666947603
iteration 158, loss = 0.0002362705854466185
iteration 159, loss = 0.0003476765414234251
iteration 160, loss = 0.0002602766908239573
iteration 161, loss = 0.00023294752463698387
iteration 162, loss = 0.0003503472253214568
iteration 163, loss = 0.0002243109338451177
iteration 164, loss = 0.00031384272733703256
iteration 165, loss = 0.0005479774554260075
iteration 166, loss = 0.00019083126971963793
iteration 167, loss = 0.0006863141315989196
iteration 168, loss = 0.00038337305886670947
iteration 169, loss = 0.00023338047321885824
iteration 170, loss = 0.0002764320815913379
iteration 171, loss = 0.00037759251426905394
iteration 172, loss = 0.0007346963393501937
iteration 173, loss = 0.00027883489383384585
iteration 174, loss = 0.00045906950253993273
iteration 175, loss = 0.0004920794162899256
iteration 176, loss = 0.0003776112280320376
iteration 177, loss = 0.0004233793879393488
iteration 178, loss = 0.0006851506186649203
iteration 179, loss = 0.0005141915753483772
iteration 180, loss = 0.0002894848003052175
iteration 181, loss = 0.00028774028760381043
iteration 182, loss = 0.00043835441465489566
iteration 183, loss = 0.000382664060452953
iteration 184, loss = 0.00025310844648629427
iteration 185, loss = 0.0004469534906093031
iteration 186, loss = 0.00046012975508347154
iteration 187, loss = 0.0004024131048936397
iteration 188, loss = 0.0005040024989284575
iteration 189, loss = 0.0005092438077554107
iteration 190, loss = 0.0003471453092060983
iteration 191, loss = 0.0005083861760795116
iteration 192, loss = 0.00035994191421195865
iteration 193, loss = 0.0007101173978298903
iteration 194, loss = 0.00045345371472649276
iteration 195, loss = 0.000810370605904609
iteration 196, loss = 0.00023571369820274413
iteration 197, loss = 0.00033648364478722215
iteration 198, loss = 0.000429831474320963
iteration 199, loss = 0.0005200683372095227
iteration 200, loss = 0.0004139492230024189
iteration 201, loss = 0.0010709889465942979
iteration 202, loss = 0.0002894995268434286
iteration 203, loss = 0.000809593009762466
iteration 204, loss = 0.00042734359158203006
iteration 205, loss = 0.00021052159718237817
iteration 206, loss = 0.0004037609323859215
iteration 207, loss = 0.0001993330370169133
iteration 208, loss = 0.00029362717759795487
iteration 209, loss = 0.0002450016909278929
iteration 210, loss = 0.00020733423298224807
iteration 211, loss = 0.00021590772666968405
iteration 212, loss = 0.0002944078005384654
iteration 213, loss = 0.0003842762380372733
iteration 214, loss = 0.0005908309249207377
iteration 215, loss = 0.0006060058949515224
iteration 216, loss = 0.00023827538825571537
iteration 217, loss = 0.0007144227274693549
iteration 218, loss = 0.00025472816196270287
iteration 219, loss = 0.00042817299254238605
iteration 220, loss = 0.0002242768241558224
iteration 221, loss = 0.00018305220874026418
iteration 222, loss = 0.00022277313109952956
iteration 223, loss = 0.0003876240807585418
iteration 224, loss = 0.0005284667131491005
iteration 225, loss = 0.00021298855426721275
iteration 226, loss = 0.00027711648726835847
iteration 227, loss = 0.0003762131673283875
iteration 228, loss = 0.0006382389692589641
iteration 229, loss = 0.0002448445011395961
iteration 230, loss = 0.0009816203964874148
iteration 231, loss = 0.0009884993778541684
iteration 232, loss = 0.0005459190579131246
iteration 233, loss = 0.000396482995711267
iteration 234, loss = 0.0009514445555396378
iteration 235, loss = 0.0004517674969974905
iteration 236, loss = 0.00028997010667808354
iteration 237, loss = 0.00041796299046836793
iteration 238, loss = 0.0004565770213957876
iteration 239, loss = 0.0005381570663303137
iteration 240, loss = 0.00026847710250876844
iteration 241, loss = 0.0004756594425998628
iteration 242, loss = 0.0003690942539833486
iteration 243, loss = 0.0006819527479819953
iteration 244, loss = 0.0003591623390093446
iteration 245, loss = 0.0003769222239498049
iteration 246, loss = 0.0005112219951115549
iteration 247, loss = 0.0003628212143667042
iteration 248, loss = 0.0006846270407550037
iteration 249, loss = 0.00033584763878025115
iteration 250, loss = 0.00039686274249106646
iteration 251, loss = 0.0002558167034294456
iteration 252, loss = 0.00034602833329699934
iteration 253, loss = 0.0002877978258766234
iteration 254, loss = 0.0002674267743714154
iteration 255, loss = 0.0006696908385492861
iteration 256, loss = 0.00042892861529253423
iteration 257, loss = 0.00027930468786507845
iteration 258, loss = 0.0002622936153784394
iteration 259, loss = 0.000640798476524651
iteration 260, loss = 0.00024749524891376495
iteration 261, loss = 0.00031496197334490716
iteration 262, loss = 0.0002395881456322968
iteration 263, loss = 0.0002888442832045257
iteration 264, loss = 0.00023939955281093717
iteration 265, loss = 0.0002218262234237045
iteration 266, loss = 0.00035226542968302965
iteration 267, loss = 0.00032265824847854674
iteration 268, loss = 0.0002994158712681383
iteration 269, loss = 0.001182402716949582
iteration 270, loss = 0.0003557115560397506
iteration 271, loss = 0.0003404156886972487
iteration 272, loss = 0.0003938282316084951
iteration 273, loss = 0.0005166261689737439
iteration 274, loss = 0.0003140686603728682
iteration 275, loss = 0.00019793014507740736
iteration 276, loss = 0.0005558156408369541
iteration 277, loss = 0.0004203045682515949
iteration 278, loss = 0.00041208378388546407
iteration 279, loss = 0.0003280001110397279
iteration 280, loss = 0.0008031371980905533
iteration 281, loss = 0.00028492312412709
iteration 282, loss = 0.0004775947309099138
iteration 283, loss = 0.0004293469828553498
iteration 284, loss = 0.00034694536589086056
iteration 285, loss = 0.0004689160268753767
iteration 286, loss = 0.00021507703058887273
iteration 287, loss = 0.0003744054993148893
iteration 288, loss = 0.0002691026602406055
iteration 289, loss = 0.0002743433869909495
iteration 290, loss = 0.0004584587295539677
iteration 291, loss = 0.0006868705386295915
iteration 292, loss = 0.0002779550268314779
iteration 293, loss = 0.00022215016360860318
iteration 294, loss = 0.0005605221958830953
iteration 295, loss = 0.00041501325904391706
iteration 296, loss = 0.0006406071479432285
iteration 297, loss = 0.0006905713235028088
iteration 298, loss = 0.00026017354684881866
iteration 299, loss = 0.0003346916346345097
iteration 0, loss = 0.00020843208767473698
iteration 1, loss = 0.00040184849058277905
iteration 2, loss = 0.00043591437861323357
iteration 3, loss = 0.0002035869110841304
iteration 4, loss = 0.00023685710038989782
iteration 5, loss = 0.0010603365954011679
iteration 6, loss = 0.00035310653038322926
iteration 7, loss = 0.00040344297303818166
iteration 8, loss = 0.0010031504789367318
iteration 9, loss = 0.0002903146087191999
iteration 10, loss = 0.0005094704101793468
iteration 11, loss = 0.00036104919854551554
iteration 12, loss = 0.0005569130880758166
iteration 13, loss = 0.00022812029055785388
iteration 14, loss = 0.0003490144736133516
iteration 15, loss = 0.00021672052389476448
iteration 16, loss = 0.0002682655758690089
iteration 17, loss = 0.00021035221288911998
iteration 18, loss = 0.00034582935040816665
iteration 19, loss = 0.0003962018236052245
iteration 20, loss = 0.00028328743064776063
iteration 21, loss = 0.0010672546923160553
iteration 22, loss = 0.00023001876252237707
iteration 23, loss = 0.00026723809423856437
iteration 24, loss = 0.0002398573560640216
iteration 25, loss = 0.00025080531486310065
iteration 26, loss = 0.0005961786373518407
iteration 27, loss = 0.00027592218248173594
iteration 28, loss = 0.00042525134631432593
iteration 29, loss = 0.0001968586875591427
iteration 30, loss = 0.00035730647505261004
iteration 31, loss = 0.0003380451234988868
iteration 32, loss = 0.0004822563787456602
iteration 33, loss = 0.0002067232271656394
iteration 34, loss = 0.0003691930614877492
iteration 35, loss = 0.00020514859352260828
iteration 36, loss = 0.0003300015814602375
iteration 37, loss = 0.00044985482236370444
iteration 38, loss = 0.0002454704081173986
iteration 39, loss = 0.0004606933507602662
iteration 40, loss = 0.0003735163772944361
iteration 41, loss = 0.0007464991067536175
iteration 42, loss = 0.0010710450587794185
iteration 43, loss = 0.0002941400744020939
iteration 44, loss = 0.00031123036751523614
iteration 45, loss = 0.0005884779966436327
iteration 46, loss = 0.0004264150629751384
iteration 47, loss = 0.0004163981357123703
iteration 48, loss = 0.0002888977760449052
iteration 49, loss = 0.000464387412648648
iteration 50, loss = 0.00021572971309069544
iteration 51, loss = 0.0002224448398919776
iteration 52, loss = 0.0006255530752241611
iteration 53, loss = 0.00045321136713027954
iteration 54, loss = 0.0005472873453982174
iteration 55, loss = 0.00025352201191708446
iteration 56, loss = 0.0018790788017213345
iteration 57, loss = 0.00033203818020410836
iteration 58, loss = 0.0008086065645329654
iteration 59, loss = 0.00024090417718980461
iteration 60, loss = 0.0006512666586786509
iteration 61, loss = 0.00020194069657009095
iteration 62, loss = 0.00029348256066441536
iteration 63, loss = 0.00015403069846797734
iteration 64, loss = 0.00028018481680192053
iteration 65, loss = 0.0003608833940234035
iteration 66, loss = 0.00015912781236693263
iteration 67, loss = 0.0002314699231646955
iteration 68, loss = 0.00027212052373215556
iteration 69, loss = 0.0003181425854563713
iteration 70, loss = 0.0003392944927327335
iteration 71, loss = 0.0003380818525329232
iteration 72, loss = 0.0003463952452875674
iteration 73, loss = 0.0004271728394087404
iteration 74, loss = 0.00025028930394910276
iteration 75, loss = 0.00038872085860930383
iteration 76, loss = 0.00031432995456270874
iteration 77, loss = 0.0003298552765045315
iteration 78, loss = 0.00038446206599473953
iteration 79, loss = 0.00040548050310462713
iteration 80, loss = 0.0004153342451900244
iteration 81, loss = 0.0007065263926051557
iteration 82, loss = 0.00033585040364414454
iteration 83, loss = 0.0006478374125435948
iteration 84, loss = 0.001142237801104784
iteration 85, loss = 0.0003197407640982419
iteration 86, loss = 0.0002482052950654179
iteration 87, loss = 0.00040599735802970827
iteration 88, loss = 0.0003360937407705933
iteration 89, loss = 0.0002495904336683452
iteration 90, loss = 0.00025874891434796154
iteration 91, loss = 0.0004514904576353729
iteration 92, loss = 0.000704231730196625
iteration 93, loss = 0.0003872729139402509
iteration 94, loss = 0.00046920194290578365
iteration 95, loss = 0.00046880391892045736
iteration 96, loss = 0.00026448973221704364
iteration 97, loss = 0.0002802397939376533
iteration 98, loss = 0.0004726208280771971
iteration 99, loss = 0.0004223505675327033
iteration 100, loss = 0.00042245452641509473
iteration 101, loss = 0.0004181576077826321
iteration 102, loss = 0.0002644265186972916
iteration 103, loss = 0.00030925730243325233
iteration 104, loss = 0.0004616923979483545
iteration 105, loss = 0.00019949217676185071
iteration 106, loss = 0.00033898543915711343
iteration 107, loss = 0.00032179304980672896
iteration 108, loss = 0.00029865303076803684
iteration 109, loss = 0.0005068005411885679
iteration 110, loss = 0.00022386739146895707
iteration 111, loss = 0.0005403825198300183
iteration 112, loss = 0.0007368098595179617
iteration 113, loss = 0.00040683470433577895
iteration 114, loss = 0.0002446997386869043
iteration 115, loss = 0.0003295642673037946
iteration 116, loss = 0.0002031171170528978
iteration 117, loss = 0.001146339694969356
iteration 118, loss = 0.0005127654876559973
iteration 119, loss = 0.0003617934125941247
iteration 120, loss = 0.00032958967494778335
iteration 121, loss = 0.0002691158151719719
iteration 122, loss = 0.00041233416413888335
iteration 123, loss = 0.00024785156711004674
iteration 124, loss = 0.0003912835964001715
iteration 125, loss = 0.0005664504133164883
iteration 126, loss = 0.0004775816632900387
iteration 127, loss = 0.0004040727508254349
iteration 128, loss = 0.000622964755166322
iteration 129, loss = 0.00033208972308784723
iteration 130, loss = 0.00039663014467805624
iteration 131, loss = 0.00031390052754431963
iteration 132, loss = 0.00045649547246284783
iteration 133, loss = 0.0010741192381829023
iteration 134, loss = 0.00028795565594919026
iteration 135, loss = 0.00021807187295053154
iteration 136, loss = 0.0004576278442982584
iteration 137, loss = 0.000282347114989534
iteration 138, loss = 0.0001943405659403652
iteration 139, loss = 0.00023965259606484324
iteration 140, loss = 0.00016089680138975382
iteration 141, loss = 0.00037873777910135686
iteration 142, loss = 0.00027726139524020255
iteration 143, loss = 0.0007259489502757788
iteration 144, loss = 0.0003064376942347735
iteration 145, loss = 0.0004874071746598929
iteration 146, loss = 0.0014591303188353777
iteration 147, loss = 0.0002121001307386905
iteration 148, loss = 0.00024203052453231066
iteration 149, loss = 0.0004688069748226553
iteration 150, loss = 0.00032601531711407006
iteration 151, loss = 0.00046995063894428313
iteration 152, loss = 0.00044756801798939705
iteration 153, loss = 0.0002457528607919812
iteration 154, loss = 0.0003044704208150506
iteration 155, loss = 0.00035929170553572476
iteration 156, loss = 0.0006048774812370539
iteration 157, loss = 0.0007600174867548048
iteration 158, loss = 0.00040446745697408915
iteration 159, loss = 0.00033686967799440026
iteration 160, loss = 0.0004368130466900766
iteration 161, loss = 0.0004595393256749958
iteration 162, loss = 0.00023881718516349792
iteration 163, loss = 0.0006906492635607719
iteration 164, loss = 0.00028698527603410184
iteration 165, loss = 0.0007769971271045506
iteration 166, loss = 0.0006563225178979337
iteration 167, loss = 0.00041741639142856
iteration 168, loss = 0.00047260476276278496
iteration 169, loss = 0.000621470098849386
iteration 170, loss = 0.00019314984092488885
iteration 171, loss = 0.0005655333516187966
iteration 172, loss = 0.0004200127732474357
iteration 173, loss = 0.0006232601590454578
iteration 174, loss = 0.00038687256164848804
iteration 175, loss = 0.00027884889277629554
iteration 176, loss = 0.0002128909109160304
iteration 177, loss = 0.0006681905360892415
iteration 178, loss = 0.00047469037235714495
iteration 179, loss = 0.0007585327257402241
iteration 180, loss = 0.0003812214417848736
iteration 181, loss = 0.00028415635460987687
iteration 182, loss = 0.0006546276272274554
iteration 183, loss = 0.0003239172510802746
iteration 184, loss = 0.0003689223376568407
iteration 185, loss = 0.0002009537274716422
iteration 186, loss = 0.0010886933887377381
iteration 187, loss = 0.0003761005646083504
iteration 188, loss = 0.0005580834113061428
iteration 189, loss = 0.0005753093282692134
iteration 190, loss = 0.00036547158379107714
iteration 191, loss = 0.00035709619987756014
iteration 192, loss = 0.00024944631149992347
iteration 193, loss = 0.0002536223619244993
iteration 194, loss = 0.000443758413894102
iteration 195, loss = 0.00042020081309601665
iteration 196, loss = 0.0003018441784661263
iteration 197, loss = 0.0005363571108318865
iteration 198, loss = 0.0003984797222074121
iteration 199, loss = 0.0006457449053414166
iteration 200, loss = 0.00039330273284576833
iteration 201, loss = 0.0002942768041975796
iteration 202, loss = 0.001109774922952056
iteration 203, loss = 0.00033297904883511364
iteration 204, loss = 0.00029160809936001897
iteration 205, loss = 0.00035339363967068493
iteration 206, loss = 0.0005967461038380861
iteration 207, loss = 0.00027732033049687743
iteration 208, loss = 0.00021820895199198276
iteration 209, loss = 0.0002750567509792745
iteration 210, loss = 0.00026897230418398976
iteration 211, loss = 0.0005955569795332849
iteration 212, loss = 0.0010910334531217813
iteration 213, loss = 0.00023009133292362094
iteration 214, loss = 0.0003821796562988311
iteration 215, loss = 0.0002647869405336678
iteration 216, loss = 0.00020887079881504178
iteration 217, loss = 0.00048603888717480004
iteration 218, loss = 0.00021792756160721183
iteration 219, loss = 0.0004144779813941568
iteration 220, loss = 0.0006695265765301883
iteration 221, loss = 0.0005879332311451435
iteration 222, loss = 0.00023787980899214745
iteration 223, loss = 0.0002548515913076699
iteration 224, loss = 0.00019410806999076158
iteration 225, loss = 0.00036170604289509356
iteration 226, loss = 0.0003285196435172111
iteration 227, loss = 0.0006099347956478596
iteration 228, loss = 0.0008014501654542983
iteration 229, loss = 0.0002573177625890821
iteration 230, loss = 0.00030651254928670824
iteration 231, loss = 0.0005133765516802669
iteration 232, loss = 0.0012335710925981402
iteration 233, loss = 0.00037891091778874397
iteration 234, loss = 0.0003691152960527688
iteration 235, loss = 0.00027012533973902464
iteration 236, loss = 0.0003244174877181649
iteration 237, loss = 0.00035915710031986237
iteration 238, loss = 0.0007780027808621526
iteration 239, loss = 0.0003273139300290495
iteration 240, loss = 0.0005406683194451034
iteration 241, loss = 0.00019291894568596035
iteration 242, loss = 0.0009429951896890998
iteration 243, loss = 0.0003387283650226891
iteration 244, loss = 0.00027263775700703263
iteration 245, loss = 0.0003608705010265112
iteration 246, loss = 0.00037494811112992465
iteration 247, loss = 0.00035086990101262927
iteration 248, loss = 0.0002247855009045452
iteration 249, loss = 0.00044188275933265686
iteration 250, loss = 0.0004744661273434758
iteration 251, loss = 0.00038526547723449767
iteration 252, loss = 0.00040821978473104537
iteration 253, loss = 0.0002482327981851995
iteration 254, loss = 0.00027950748335570097
iteration 255, loss = 0.00029441644437611103
iteration 256, loss = 0.0004326733760535717
iteration 257, loss = 0.00028105999808758497
iteration 258, loss = 0.0003209463902749121
iteration 259, loss = 0.0003838776610791683
iteration 260, loss = 0.0006232978194020689
iteration 261, loss = 0.00033282628282904625
iteration 262, loss = 0.001173136057332158
iteration 263, loss = 0.00048241656622849405
iteration 264, loss = 0.00021192852000240237
iteration 265, loss = 0.0002329917624592781
iteration 266, loss = 0.00029998860554769635
iteration 267, loss = 0.00044776065624319017
iteration 268, loss = 0.0003022890305146575
iteration 269, loss = 0.00025582691887393594
iteration 270, loss = 0.00030339526711031795
iteration 271, loss = 0.0004136486095376313
iteration 272, loss = 0.000452554551884532
iteration 273, loss = 0.0006627956172451377
iteration 274, loss = 0.000306429632473737
iteration 275, loss = 0.00033724657259881496
iteration 276, loss = 0.0005181653541512787
iteration 277, loss = 0.00031994690652936697
iteration 278, loss = 0.00028089797706343234
iteration 279, loss = 0.00028181736706756055
iteration 280, loss = 0.0006320953834801912
iteration 281, loss = 0.0003049819788429886
iteration 282, loss = 0.0003584552905522287
iteration 283, loss = 0.0003154927690047771
iteration 284, loss = 0.0005282016354613006
iteration 285, loss = 0.00026651989901438355
iteration 286, loss = 0.0006000197608955204
iteration 287, loss = 0.0003936206630896777
iteration 288, loss = 0.0002918503596447408
iteration 289, loss = 0.00023860424698796123
iteration 290, loss = 0.00030245163361541927
iteration 291, loss = 0.00019154255278408527
iteration 292, loss = 0.00032894156174734235
iteration 293, loss = 0.00026022939709946513
iteration 294, loss = 0.00035674998071044683
iteration 295, loss = 0.00034020686871372163
iteration 296, loss = 0.000542931433301419
iteration 297, loss = 0.00020687910728156567
iteration 298, loss = 0.0006999477045610547
iteration 299, loss = 0.000419300893554464
iteration 0, loss = 0.00030684200464747846
iteration 1, loss = 0.0002062659477815032
iteration 2, loss = 0.00036144378827884793
iteration 3, loss = 0.00022444690694101155
iteration 4, loss = 0.0009662917582318187
iteration 5, loss = 0.00028717139502987266
iteration 6, loss = 0.00030062819132581353
iteration 7, loss = 0.00032682542223483324
iteration 8, loss = 0.0001954848412424326
iteration 9, loss = 0.0002125053433701396
iteration 10, loss = 0.0003833212540484965
iteration 11, loss = 0.000471349194413051
iteration 12, loss = 0.000592828553635627
iteration 13, loss = 0.000374912895495072
iteration 14, loss = 0.00032201301655732095
iteration 15, loss = 0.0002406833227723837
iteration 16, loss = 0.00042949459748342633
iteration 17, loss = 0.00038926914567127824
iteration 18, loss = 0.00023377418983727694
iteration 19, loss = 0.00034443478216417134
iteration 20, loss = 0.0002979475539177656
iteration 21, loss = 0.00021348157315514982
iteration 22, loss = 0.00041104236152023077
iteration 23, loss = 0.00015738322690594941
iteration 24, loss = 0.00042108280467800796
iteration 25, loss = 0.0002882384287659079
iteration 26, loss = 0.000241507645114325
iteration 27, loss = 0.0006842825096100569
iteration 28, loss = 0.0004159415257163346
iteration 29, loss = 0.00022200614330358803
iteration 30, loss = 0.0004480545758269727
iteration 31, loss = 0.0002872771874535829
iteration 32, loss = 0.0005213092663325369
iteration 33, loss = 0.0005864282720722258
iteration 34, loss = 0.00030168163357302547
iteration 35, loss = 0.0002804571413435042
iteration 36, loss = 0.0003342638665344566
iteration 37, loss = 0.00030620009056292474
iteration 38, loss = 0.00030578611767850816
iteration 39, loss = 0.0003639110364019871
iteration 40, loss = 0.00042109168134629726
iteration 41, loss = 0.00036455076769925654
iteration 42, loss = 0.00032542619737796485
iteration 43, loss = 0.00021957483841106296
iteration 44, loss = 0.0003070749226026237
iteration 45, loss = 0.00039215743890963495
iteration 46, loss = 0.0006154794828034937
iteration 47, loss = 0.000272075500106439
iteration 48, loss = 0.00026234539109282196
iteration 49, loss = 0.0002668103261385113
iteration 50, loss = 0.0005026963190175593
iteration 51, loss = 0.000330414593918249
iteration 52, loss = 0.0003603561781346798
iteration 53, loss = 0.0002123403100995347
iteration 54, loss = 0.0003325883881188929
iteration 55, loss = 0.0002132477966370061
iteration 56, loss = 0.00024254537129309028
iteration 57, loss = 0.000474397063953802
iteration 58, loss = 0.0004495552566368133
iteration 59, loss = 0.0009192084544338286
iteration 60, loss = 0.00028923596255481243
iteration 61, loss = 0.0006622879882343113
iteration 62, loss = 0.0004635527729988098
iteration 63, loss = 0.0003800010308623314
iteration 64, loss = 0.0005528287147171795
iteration 65, loss = 0.0002564862370491028
iteration 66, loss = 0.00027795732603408396
iteration 67, loss = 0.00030367576982825994
iteration 68, loss = 0.0002906136796809733
iteration 69, loss = 0.0003294668276794255
iteration 70, loss = 0.0002549027558416128
iteration 71, loss = 0.0002816766791511327
iteration 72, loss = 0.0002645116182975471
iteration 73, loss = 0.00036995194386690855
iteration 74, loss = 0.0010728695197030902
iteration 75, loss = 0.0003623687371145934
iteration 76, loss = 0.0010328578064218163
iteration 77, loss = 0.00026559107936918736
iteration 78, loss = 0.00028686955920420587
iteration 79, loss = 0.0006180472555570304
iteration 80, loss = 0.00033496797550469637
iteration 81, loss = 0.0006951866671442986
iteration 82, loss = 0.00016675214283168316
iteration 83, loss = 0.0002178988215746358
iteration 84, loss = 0.0009635639726184309
iteration 85, loss = 0.00019145762780681252
iteration 86, loss = 0.0004868330142926425
iteration 87, loss = 0.00034933543065562844
iteration 88, loss = 0.000371229718439281
iteration 89, loss = 0.0006101100007072091
iteration 90, loss = 0.00024190630938392133
iteration 91, loss = 0.00024445366580039263
iteration 92, loss = 0.0003100652829743922
iteration 93, loss = 0.0002511976636014879
iteration 94, loss = 0.0007159156957641244
iteration 95, loss = 0.0002397143980488181
iteration 96, loss = 0.00017168468912132084
iteration 97, loss = 0.00026457873173058033
iteration 98, loss = 0.0006299225497059524
iteration 99, loss = 0.0001990845485124737
iteration 100, loss = 0.00027712827431969345
iteration 101, loss = 0.00037961616180837154
iteration 102, loss = 0.0002799894427880645
iteration 103, loss = 0.0003976175212301314
iteration 104, loss = 0.0009379740804433823
iteration 105, loss = 0.00037552809226326644
iteration 106, loss = 0.00030596464057452977
iteration 107, loss = 0.0005266751395538449
iteration 108, loss = 0.00012543133925646544
iteration 109, loss = 0.00022862356854602695
iteration 110, loss = 0.0005363690434023738
iteration 111, loss = 0.00018894630193244666
iteration 112, loss = 0.00023951075854711235
iteration 113, loss = 0.0008771444554440677
iteration 114, loss = 0.00048374495236203074
iteration 115, loss = 0.0003923168987967074
iteration 116, loss = 0.00028327404288575053
iteration 117, loss = 0.0001716886181384325
iteration 118, loss = 0.00041310524102300406
iteration 119, loss = 0.000334004609612748
iteration 120, loss = 0.00035796608426608145
iteration 121, loss = 0.0002645032072905451
iteration 122, loss = 0.00036617289879359305
iteration 123, loss = 0.000362905440852046
iteration 124, loss = 0.00016814896662253886
iteration 125, loss = 0.0002679977915249765
iteration 126, loss = 0.000890018476638943
iteration 127, loss = 0.0002281135239172727
iteration 128, loss = 0.0004118652723263949
iteration 129, loss = 0.0003492855466902256
iteration 130, loss = 0.0003145816153846681
iteration 131, loss = 0.00034808332566171885
iteration 132, loss = 0.0003870798973366618
iteration 133, loss = 0.0003987077798228711
iteration 134, loss = 0.00018205484957434237
iteration 135, loss = 0.0007419564062729478
iteration 136, loss = 0.0005664267227984965
iteration 137, loss = 0.00030214793514460325
iteration 138, loss = 0.000971522182226181
iteration 139, loss = 0.0003510938840918243
iteration 140, loss = 0.00048115249956026673
iteration 141, loss = 0.00046109629329293966
iteration 142, loss = 0.00027008343022316694
iteration 143, loss = 0.0003454324323683977
iteration 144, loss = 0.0003210531431250274
iteration 145, loss = 0.0003802046994678676
iteration 146, loss = 0.00042895731166936457
iteration 147, loss = 0.00027491082437336445
iteration 148, loss = 0.0007254494121298194
iteration 149, loss = 0.00024546441272832453
iteration 150, loss = 0.00032381259370595217
iteration 151, loss = 0.0002561885630711913
iteration 152, loss = 0.0003013341629412025
iteration 153, loss = 0.00023619797138962895
iteration 154, loss = 0.0003006035985890776
iteration 155, loss = 0.0004997676005586982
iteration 156, loss = 0.0007100752554833889
iteration 157, loss = 0.0005660251481458545
iteration 158, loss = 0.00030332658207044005
iteration 159, loss = 0.0003314077912364155
iteration 160, loss = 0.0006436920375563204
iteration 161, loss = 0.00047745852498337626
iteration 162, loss = 0.00019955830066464841
iteration 163, loss = 0.00022162609093356878
iteration 164, loss = 0.0006831203354522586
iteration 165, loss = 0.0005564630264416337
iteration 166, loss = 0.0004347320937085897
iteration 167, loss = 0.00032374224974773824
iteration 168, loss = 0.0003300676471553743
iteration 169, loss = 0.0002359601785428822
iteration 170, loss = 0.0005353422602638602
iteration 171, loss = 0.00022935515153221786
iteration 172, loss = 0.0004213775391690433
iteration 173, loss = 0.0006007496267557144
iteration 174, loss = 0.0003581183264032006
iteration 175, loss = 0.0004319696454331279
iteration 176, loss = 0.0002911576593760401
iteration 177, loss = 0.0003177256148774177
iteration 178, loss = 0.0003732899494934827
iteration 179, loss = 0.0009987222729250789
iteration 180, loss = 0.0007222940912470222
iteration 181, loss = 0.0003518269513733685
iteration 182, loss = 0.0002819556975737214
iteration 183, loss = 0.0002343966334592551
iteration 184, loss = 0.00021336872305255383
iteration 185, loss = 0.00049014255637303
iteration 186, loss = 0.0002787698758766055
iteration 187, loss = 0.0002896865480579436
iteration 188, loss = 0.0005504340515471995
iteration 189, loss = 0.0002687579835765064
iteration 190, loss = 0.0002940981648862362
iteration 191, loss = 0.0009849390480667353
iteration 192, loss = 0.0005935969529673457
iteration 193, loss = 0.00022510300914291292
iteration 194, loss = 0.001381535897962749
iteration 195, loss = 0.0003610449784900993
iteration 196, loss = 0.0007490337593480945
iteration 197, loss = 0.0002620295563247055
iteration 198, loss = 0.00031505728838965297
iteration 199, loss = 0.0002294493024237454
iteration 200, loss = 0.0002864710404537618
iteration 201, loss = 0.000597895123064518
iteration 202, loss = 0.0005801117513328791
iteration 203, loss = 0.00028634490445256233
iteration 204, loss = 0.0004144923004787415
iteration 205, loss = 0.00034806830808520317
iteration 206, loss = 0.0003747807932086289
iteration 207, loss = 0.0003608830738812685
iteration 208, loss = 0.001092765131033957
iteration 209, loss = 0.00027479484560899436
iteration 210, loss = 0.0006785622681491077
iteration 211, loss = 0.00032629468478262424
iteration 212, loss = 0.001276073046028614
iteration 213, loss = 0.00028651184402406216
iteration 214, loss = 0.00029478632495738566
iteration 215, loss = 0.0006935026030987501
iteration 216, loss = 0.0005721913767047226
iteration 217, loss = 0.0003060845483560115
iteration 218, loss = 0.00033608617377467453
iteration 219, loss = 0.0004925206303596497
iteration 220, loss = 0.00034808932105079293
iteration 221, loss = 0.00038879489875398576
iteration 222, loss = 0.0003229947469662875
iteration 223, loss = 0.0004049883282277733
iteration 224, loss = 0.0009373428765684366
iteration 225, loss = 0.00032676334376446903
iteration 226, loss = 0.0002851592726074159
iteration 227, loss = 0.00025457015726715326
iteration 228, loss = 0.00024516251869499683
iteration 229, loss = 0.00019913871074095368
iteration 230, loss = 0.00031251920154318213
iteration 231, loss = 0.0002528943296056241
iteration 232, loss = 0.0002446355065330863
iteration 233, loss = 0.0003588430117815733
iteration 234, loss = 0.000468918529804796
iteration 235, loss = 0.00047907140105962753
iteration 236, loss = 0.0004645691078621894
iteration 237, loss = 0.0005801779334433377
iteration 238, loss = 0.00025354453828185797
iteration 239, loss = 0.00048216767027042806
iteration 240, loss = 0.0005682980408892035
iteration 241, loss = 0.00047514878679066896
iteration 242, loss = 0.00044447791879065335
iteration 243, loss = 0.0004766247293446213
iteration 244, loss = 0.00045054336078464985
iteration 245, loss = 0.0002906165609601885
iteration 246, loss = 0.0002637681027408689
iteration 247, loss = 0.00031243672128766775
iteration 248, loss = 0.0003328236925881356
iteration 249, loss = 0.0003698018263094127
iteration 250, loss = 0.00040155326132662594
iteration 251, loss = 0.00044125152635388076
iteration 252, loss = 0.0003467131173238158
iteration 253, loss = 0.00032312169787473977
iteration 254, loss = 0.00035485366242937744
iteration 255, loss = 0.0001992738398257643
iteration 256, loss = 0.0003019616415258497
iteration 257, loss = 0.00034170717117376626
iteration 258, loss = 0.0006758197559975088
iteration 259, loss = 0.00038599493564106524
iteration 260, loss = 0.0001440807245671749
iteration 261, loss = 0.00027621479239314795
iteration 262, loss = 0.00035912543535232544
iteration 263, loss = 0.0003117947489954531
iteration 264, loss = 0.00042069610208272934
iteration 265, loss = 0.0005032825865782797
iteration 266, loss = 0.0002896734804380685
iteration 267, loss = 0.000516322092153132
iteration 268, loss = 0.001247525680810213
iteration 269, loss = 0.0005469764000736177
iteration 270, loss = 0.00033499006531201303
iteration 271, loss = 0.0005413712933659554
iteration 272, loss = 0.00029867474222555757
iteration 273, loss = 0.00018309532606508583
iteration 274, loss = 0.00027970445808023214
iteration 275, loss = 0.00036423676647245884
iteration 276, loss = 0.0004046225512865931
iteration 277, loss = 0.00026817183243110776
iteration 278, loss = 0.000264198228251189
iteration 279, loss = 0.00045694527216255665
iteration 280, loss = 0.0005988552002236247
iteration 281, loss = 0.00017194946121890098
iteration 282, loss = 0.00034899695310741663
iteration 283, loss = 0.0011597916018217802
iteration 284, loss = 0.00019867694936692715
iteration 285, loss = 0.0002883767301682383
iteration 286, loss = 0.00032552791526541114
iteration 287, loss = 0.00045417636283673346
iteration 288, loss = 0.0010295971296727657
iteration 289, loss = 0.0006881405133754015
iteration 290, loss = 0.00026939401868730783
iteration 291, loss = 0.0004356252611614764
iteration 292, loss = 0.00026593785150907934
iteration 293, loss = 0.00021745824778918177
iteration 294, loss = 0.0013006154913455248
iteration 295, loss = 0.00021540069428738207
iteration 296, loss = 0.0002731457643676549
iteration 297, loss = 0.00027022307040169835
iteration 298, loss = 0.00022397808788809925
iteration 299, loss = 0.00022863784397486597
iteration 0, loss = 0.0004999968223273754
iteration 1, loss = 0.000227426178753376
iteration 2, loss = 0.00037843710742890835
iteration 3, loss = 0.0004270245262887329
iteration 4, loss = 0.00019954548042733222
iteration 5, loss = 0.00037731105112470686
iteration 6, loss = 0.00027876748936250806
iteration 7, loss = 0.0002071804483421147
iteration 8, loss = 0.0002839230000972748
iteration 9, loss = 0.00040342059219256043
iteration 10, loss = 0.00029429010464809835
iteration 11, loss = 0.00030568608781322837
iteration 12, loss = 0.0004762466414831579
iteration 13, loss = 0.0003943747142329812
iteration 14, loss = 0.0002538333064876497
iteration 15, loss = 0.0002492483181413263
iteration 16, loss = 0.00037021213211119175
iteration 17, loss = 0.00032772470149211586
iteration 18, loss = 0.000748692371416837
iteration 19, loss = 0.00043202750384807587
iteration 20, loss = 0.0003079437301494181
iteration 21, loss = 0.0003027707862202078
iteration 22, loss = 0.00031975278398022056
iteration 23, loss = 0.00023385797976516187
iteration 24, loss = 0.0003295873466413468
iteration 25, loss = 0.0010188851738348603
iteration 26, loss = 0.0009343627607449889
iteration 27, loss = 0.0002325743407709524
iteration 28, loss = 0.0002726130187511444
iteration 29, loss = 0.0004385011561680585
iteration 30, loss = 0.00021030099014751613
iteration 31, loss = 0.0002630842209327966
iteration 32, loss = 0.0011904295533895493
iteration 33, loss = 0.0003622208023443818
iteration 34, loss = 0.00020489917369559407
iteration 35, loss = 0.0009780105901882052
iteration 36, loss = 0.0003221239894628525
iteration 37, loss = 0.00029509287560358644
iteration 38, loss = 0.00035068916622549295
iteration 39, loss = 0.00038011110154911876
iteration 40, loss = 0.0008830165606923401
iteration 41, loss = 0.00019913254072889686
iteration 42, loss = 0.0002864524140022695
iteration 43, loss = 0.0002902213018387556
iteration 44, loss = 0.0004560923553071916
iteration 45, loss = 0.0004457384638953954
iteration 46, loss = 0.0006986036314629018
iteration 47, loss = 0.00035566481528803706
iteration 48, loss = 0.00047095533227548003
iteration 49, loss = 0.0013513497542589903
iteration 50, loss = 0.0002467616577632725
iteration 51, loss = 0.00019970697758253664
iteration 52, loss = 0.0002967376494780183
iteration 53, loss = 0.001006323378533125
iteration 54, loss = 0.000352701434167102
iteration 55, loss = 0.00030345830600708723
iteration 56, loss = 0.0002902560809161514
iteration 57, loss = 0.00032872857991605997
iteration 58, loss = 0.0006506769568659365
iteration 59, loss = 0.00024065827892627567
iteration 60, loss = 0.00020628643687814474
iteration 61, loss = 0.0006457183044403791
iteration 62, loss = 0.0002156038099201396
iteration 63, loss = 0.0002658003941178322
iteration 64, loss = 0.00019942797371186316
iteration 65, loss = 0.00030872595380060375
iteration 66, loss = 0.00022168815485201776
iteration 67, loss = 0.0003961788024753332
iteration 68, loss = 0.0002714823931455612
iteration 69, loss = 0.0002617475693114102
iteration 70, loss = 0.00031383195891976357
iteration 71, loss = 0.00022965158859733492
iteration 72, loss = 0.000579827232286334
iteration 73, loss = 0.00032984005520120263
iteration 74, loss = 0.0004748975916299969
iteration 75, loss = 0.00019886717200279236
iteration 76, loss = 0.0005208867369219661
iteration 77, loss = 0.0003392054932191968
iteration 78, loss = 0.000380875775590539
iteration 79, loss = 0.0004322571912780404
iteration 80, loss = 0.00025437475414946675
iteration 81, loss = 0.00020194053649902344
iteration 82, loss = 0.00034094444708898664
iteration 83, loss = 0.0003025039623025805
iteration 84, loss = 0.0007050300482660532
iteration 85, loss = 0.00018353942141402513
iteration 86, loss = 0.00032989378087222576
iteration 87, loss = 0.0005147650954313576
iteration 88, loss = 0.0006160952616482973
iteration 89, loss = 0.0006704620318487287
iteration 90, loss = 0.000337870791554451
iteration 91, loss = 0.00021602981723845005
iteration 92, loss = 0.0005527976318262517
iteration 93, loss = 0.000417442643083632
iteration 94, loss = 0.0007112109451554716
iteration 95, loss = 0.00031000503804534674
iteration 96, loss = 0.00037942660856060684
iteration 97, loss = 0.00020282590412534773
iteration 98, loss = 0.00025144993560388684
iteration 99, loss = 0.0002743897493928671
iteration 100, loss = 0.0005431760801002383
iteration 101, loss = 0.00036976669798605144
iteration 102, loss = 0.0003247674903832376
iteration 103, loss = 0.0002988209016621113
iteration 104, loss = 0.00037510410766117275
iteration 105, loss = 0.0010427973465994
iteration 106, loss = 0.0003505650965962559
iteration 107, loss = 0.0005378978094086051
iteration 108, loss = 0.0006822009454481304
iteration 109, loss = 0.00037458259612321854
iteration 110, loss = 0.00021117560390848666
iteration 111, loss = 0.0003129381802864373
iteration 112, loss = 0.0008797112968750298
iteration 113, loss = 0.00021665208623744547
iteration 114, loss = 0.00025411529350094497
iteration 115, loss = 0.00041964638512581587
iteration 116, loss = 0.0003779101825784892
iteration 117, loss = 0.0002532030048314482
iteration 118, loss = 0.0004373787378426641
iteration 119, loss = 0.0003263943362981081
iteration 120, loss = 0.000251421908615157
iteration 121, loss = 0.00025818904396146536
iteration 122, loss = 0.000278945779427886
iteration 123, loss = 0.00027464935556054115
iteration 124, loss = 0.00022171717137098312
iteration 125, loss = 0.0003385670715942979
iteration 126, loss = 0.00023300485918298364
iteration 127, loss = 0.0006487193168140948
iteration 128, loss = 0.00046513680717907846
iteration 129, loss = 0.0003467738861218095
iteration 130, loss = 0.0006046677008271217
iteration 131, loss = 0.0006355481455102563
iteration 132, loss = 0.0005306057864800096
iteration 133, loss = 0.00022287116735242307
iteration 134, loss = 0.0005136535037308931
iteration 135, loss = 0.00023140391567721963
iteration 136, loss = 0.0002622622123453766
iteration 137, loss = 0.00039714903687126935
iteration 138, loss = 0.00043151748832315207
iteration 139, loss = 0.00023009194410406053
iteration 140, loss = 0.00048528469051234424
iteration 141, loss = 0.00024114554980769753
iteration 142, loss = 0.0002483708958607167
iteration 143, loss = 0.000265421811491251
iteration 144, loss = 0.00039972562808543444
iteration 145, loss = 0.0006067113135941327
iteration 146, loss = 0.00038659150595776737
iteration 147, loss = 0.00035319209564477205
iteration 148, loss = 0.0001758204452926293
iteration 149, loss = 0.0003072711988352239
iteration 150, loss = 0.0002838151704054326
iteration 151, loss = 0.0004853240097872913
iteration 152, loss = 0.00025330905918963253
iteration 153, loss = 0.0003707002615556121
iteration 154, loss = 0.0003327222657389939
iteration 155, loss = 0.0003283856203779578
iteration 156, loss = 0.0003174323937855661
iteration 157, loss = 0.0003404152812436223
iteration 158, loss = 0.0005146758048795164
iteration 159, loss = 0.0003844651218969375
iteration 160, loss = 0.000562269997317344
iteration 161, loss = 0.000529626733623445
iteration 162, loss = 0.0005302788922563195
iteration 163, loss = 0.0002409668522886932
iteration 164, loss = 0.0005289044929668307
iteration 165, loss = 0.0007596635259687901
iteration 166, loss = 0.00032229229691438377
iteration 167, loss = 0.0004674318479374051
iteration 168, loss = 0.00038402751670219004
iteration 169, loss = 0.00019276699458714575
iteration 170, loss = 0.00045204535126686096
iteration 171, loss = 0.0007520960061810911
iteration 172, loss = 0.00031493164715357125
iteration 173, loss = 0.00022031979460734874
iteration 174, loss = 0.0006154042785055935
iteration 175, loss = 0.0003326082951389253
iteration 176, loss = 0.0002728746912907809
iteration 177, loss = 0.00032801475026644766
iteration 178, loss = 0.00042611401295289397
iteration 179, loss = 0.00035846669925376773
iteration 180, loss = 0.00037886211066506803
iteration 181, loss = 0.0007442397763952613
iteration 182, loss = 0.0006672039162367582
iteration 183, loss = 0.0005849479348398745
iteration 184, loss = 0.0003210162103641778
iteration 185, loss = 0.00039484765147790313
iteration 186, loss = 0.0010114972246810794
iteration 187, loss = 0.001022147829644382
iteration 188, loss = 0.00036030777846463025
iteration 189, loss = 0.001007602782920003
iteration 190, loss = 0.00017379155906382948
iteration 191, loss = 0.00026923371478915215
iteration 192, loss = 0.00047819147584959865
iteration 193, loss = 0.0004454549343790859
iteration 194, loss = 0.00034615735057741404
iteration 195, loss = 0.00029845096287317574
iteration 196, loss = 0.00026258634170517325
iteration 197, loss = 0.00019743849406950176
iteration 198, loss = 0.0002344336680835113
iteration 199, loss = 0.00031330695492215455
iteration 200, loss = 0.0005269197863526642
iteration 201, loss = 0.00020155738457106054
iteration 202, loss = 0.0003393110819160938
iteration 203, loss = 0.00023283116752281785
iteration 204, loss = 0.0005682940245606005
iteration 205, loss = 0.0009148239623755217
iteration 206, loss = 0.0005082069546915591
iteration 207, loss = 0.0004252995131537318
iteration 208, loss = 0.0003469737130217254
iteration 209, loss = 0.00028420856688171625
iteration 210, loss = 0.0002824952534865588
iteration 211, loss = 0.0002497126115486026
iteration 212, loss = 0.0002730618289206177
iteration 213, loss = 0.0002404922852292657
iteration 214, loss = 0.00022586378327105194
iteration 215, loss = 0.00017134587687905878
iteration 216, loss = 0.0002612775133457035
iteration 217, loss = 0.0002476402441971004
iteration 218, loss = 0.0002996467228513211
iteration 219, loss = 0.00042636157013475895
iteration 220, loss = 0.0002421060489723459
iteration 221, loss = 0.00016485042579006404
iteration 222, loss = 0.0004861851339228451
iteration 223, loss = 0.00013117668277118355
iteration 224, loss = 0.0002443167904857546
iteration 225, loss = 0.00019383919425308704
iteration 226, loss = 0.00035889827995561063
iteration 227, loss = 0.00025313792866654694
iteration 228, loss = 0.0004780259041581303
iteration 229, loss = 0.0002908076567109674
iteration 230, loss = 0.0004628853057511151
iteration 231, loss = 0.00022045813966542482
iteration 232, loss = 0.0006363098509609699
iteration 233, loss = 0.0003142240457236767
iteration 234, loss = 0.0003812094801105559
iteration 235, loss = 0.00033063601586036384
iteration 236, loss = 0.00025291097699664533
iteration 237, loss = 0.0002449416206218302
iteration 238, loss = 0.00046193739399313927
iteration 239, loss = 0.0002978899865411222
iteration 240, loss = 0.0001706156472209841
iteration 241, loss = 0.00017712300177663565
iteration 242, loss = 0.0006429619388654828
iteration 243, loss = 0.0005957976100035012
iteration 244, loss = 0.0006685433909296989
iteration 245, loss = 0.0007915915921330452
iteration 246, loss = 0.0002725240192376077
iteration 247, loss = 0.00018774445925373584
iteration 248, loss = 0.0002777429763227701
iteration 249, loss = 0.0003912112151738256
iteration 250, loss = 0.000216215179534629
iteration 251, loss = 0.0004004783113487065
iteration 252, loss = 0.00028708775062114
iteration 253, loss = 0.00020338829199317843
iteration 254, loss = 0.0001856673479778692
iteration 255, loss = 0.00026161453570239246
iteration 256, loss = 0.0010469644330441952
iteration 257, loss = 0.0005194738623686135
iteration 258, loss = 0.00022763264132663608
iteration 259, loss = 0.00026631407672539353
iteration 260, loss = 0.0006508914520964026
iteration 261, loss = 0.00024722638772800565
iteration 262, loss = 0.0005017747171223164
iteration 263, loss = 0.00028678926173597574
iteration 264, loss = 0.0001992645557038486
iteration 265, loss = 0.00019952631555497646
iteration 266, loss = 0.00023843937378842384
iteration 267, loss = 0.00021954154362902045
iteration 268, loss = 0.0003436629194766283
iteration 269, loss = 0.00034341387799941003
iteration 270, loss = 0.00026000404614023864
iteration 271, loss = 0.0016167364083230495
iteration 272, loss = 0.0005444381386041641
iteration 273, loss = 0.0003169993287883699
iteration 274, loss = 0.00021471420768648386
iteration 275, loss = 0.0004250988713465631
iteration 276, loss = 0.0002796329208649695
iteration 277, loss = 0.0003966153890360147
iteration 278, loss = 0.00042082768050022423
iteration 279, loss = 0.000347171415342018
iteration 280, loss = 0.0009133369894698262
iteration 281, loss = 0.00019317708211019635
iteration 282, loss = 0.0003060052986256778
iteration 283, loss = 0.0003451152879279107
iteration 284, loss = 0.00027127255452796817
iteration 285, loss = 0.0002996716066263616
iteration 286, loss = 0.0005986244650557637
iteration 287, loss = 0.0002223591727670282
iteration 288, loss = 0.000580585328862071
iteration 289, loss = 0.0001543597027193755
iteration 290, loss = 0.00042781903175637126
iteration 291, loss = 0.0001966468116734177
iteration 292, loss = 0.00022898268071003258
iteration 293, loss = 0.00030209292890504
iteration 294, loss = 0.0001770347880665213
iteration 295, loss = 0.0006781002739444375
iteration 296, loss = 0.00045561802107840776
iteration 297, loss = 0.00039004793507047
iteration 298, loss = 0.00015451254148501903
iteration 299, loss = 0.0005510467453859746
iteration 0, loss = 0.000251918681897223
iteration 1, loss = 0.0005439351662062109
iteration 2, loss = 0.0002737196919042617
iteration 3, loss = 0.00023069806047715247
iteration 4, loss = 0.00022460873879026622
iteration 5, loss = 0.0004969653091393411
iteration 6, loss = 0.0003670862060971558
iteration 7, loss = 0.0004515673208516091
iteration 8, loss = 0.00046118482714518905
iteration 9, loss = 0.00030157697619870305
iteration 10, loss = 0.00018511313828639686
iteration 11, loss = 0.0006990890833549201
iteration 12, loss = 0.0003132941492367536
iteration 13, loss = 0.0002599058789201081
iteration 14, loss = 0.000340801605489105
iteration 15, loss = 0.0003520012542139739
iteration 16, loss = 0.0005151976947672665
iteration 17, loss = 0.00031615045736543834
iteration 18, loss = 0.000296260928735137
iteration 19, loss = 0.00025667803129181266
iteration 20, loss = 0.000205811855266802
iteration 21, loss = 0.0004980904632247984
iteration 22, loss = 0.0002087563625536859
iteration 23, loss = 0.000183754600584507
iteration 24, loss = 0.00037017391878180206
iteration 25, loss = 0.00031457460136152804
iteration 26, loss = 0.0003126313677057624
iteration 27, loss = 0.0003024488396476954
iteration 28, loss = 0.0003684505063574761
iteration 29, loss = 0.0006425695028156042
iteration 30, loss = 0.00042764548561535776
iteration 31, loss = 0.0002617174759507179
iteration 32, loss = 0.00014886636927258223
iteration 33, loss = 0.000369602523278445
iteration 34, loss = 0.0004504792741499841
iteration 35, loss = 0.00028436389402486384
iteration 36, loss = 0.000535642437171191
iteration 37, loss = 0.00036827308940701187
iteration 38, loss = 0.0004812674305867404
iteration 39, loss = 0.0002322421787539497
iteration 40, loss = 0.0004702202568296343
iteration 41, loss = 0.0008648676448501647
iteration 42, loss = 0.0002799578651320189
iteration 43, loss = 0.000211212201975286
iteration 44, loss = 0.00013665991718880832
iteration 45, loss = 0.0008399980142712593
iteration 46, loss = 0.0003248355642426759
iteration 47, loss = 0.00048796203918755054
iteration 48, loss = 0.00039106127223931253
iteration 49, loss = 0.0003326282021589577
iteration 50, loss = 0.00031126695103012025
iteration 51, loss = 0.000459324917756021
iteration 52, loss = 0.0002930650662165135
iteration 53, loss = 0.00029909604927524924
iteration 54, loss = 0.00030687800608575344
iteration 55, loss = 0.0002946467138826847
iteration 56, loss = 0.0002147281775251031
iteration 57, loss = 0.0002801761729642749
iteration 58, loss = 0.0005571263027377427
iteration 59, loss = 0.0002611175295896828
iteration 60, loss = 0.0003394275554455817
iteration 61, loss = 0.00022253417409956455
iteration 62, loss = 0.00028792221564799547
iteration 63, loss = 0.0001648303004913032
iteration 64, loss = 0.00026473437901586294
iteration 65, loss = 0.00031940333428792655
iteration 66, loss = 0.00017871122690849006
iteration 67, loss = 0.00040489929961040616
iteration 68, loss = 0.0003918950096704066
iteration 69, loss = 0.00037423643516376615
iteration 70, loss = 0.00127959914971143
iteration 71, loss = 0.0006026644259691238
iteration 72, loss = 0.0008927901508286595
iteration 73, loss = 0.0002798787609208375
iteration 74, loss = 0.00019270775374025106
iteration 75, loss = 0.00016238688840530813
iteration 76, loss = 0.0005230599781498313
iteration 77, loss = 0.0002840817323885858
iteration 78, loss = 0.0005573638482019305
iteration 79, loss = 0.0009355302317999303
iteration 80, loss = 0.0003655250766314566
iteration 81, loss = 0.0002903142594732344
iteration 82, loss = 0.00018432101933285594
iteration 83, loss = 0.0006315399659797549
iteration 84, loss = 0.0002960825222544372
iteration 85, loss = 0.00022993220773059875
iteration 86, loss = 0.0006050535594113171
iteration 87, loss = 0.00024588711676187813
iteration 88, loss = 0.00030921545112505555
iteration 89, loss = 0.00026059424271807075
iteration 90, loss = 0.0002733311557676643
iteration 91, loss = 0.000390794244594872
iteration 92, loss = 0.0003780822444241494
iteration 93, loss = 0.0002998247800860554
iteration 94, loss = 0.0001968153374036774
iteration 95, loss = 0.00047771912068128586
iteration 96, loss = 0.0003128206299152225
iteration 97, loss = 0.00033184370840899646
iteration 98, loss = 0.00021389019093476236
iteration 99, loss = 0.0007147298310883343
iteration 100, loss = 0.0003180471248924732
iteration 101, loss = 0.00031957996543496847
iteration 102, loss = 0.0004344301705714315
iteration 103, loss = 0.00019851555407512933
iteration 104, loss = 0.00033124489709734917
iteration 105, loss = 0.0002746678947005421
iteration 106, loss = 0.0006385002634488046
iteration 107, loss = 0.00021174244466237724
iteration 108, loss = 0.00021414225921034813
iteration 109, loss = 0.00016503412916790694
iteration 110, loss = 0.0003157272294629365
iteration 111, loss = 0.00026672540116123855
iteration 112, loss = 0.0003609233826864511
iteration 113, loss = 0.0002586555783636868
iteration 114, loss = 0.00021963819744996727
iteration 115, loss = 0.00027096137637272477
iteration 116, loss = 0.0005006032297387719
iteration 117, loss = 0.0010473335860297084
iteration 118, loss = 0.00045458966633304954
iteration 119, loss = 0.00024453451624140143
iteration 120, loss = 0.0006648628041148186
iteration 121, loss = 0.00018434968660585582
iteration 122, loss = 0.0002000512758968398
iteration 123, loss = 0.00028109975391998887
iteration 124, loss = 0.00036347683635540307
iteration 125, loss = 0.0005935356020927429
iteration 126, loss = 0.0005238943849690259
iteration 127, loss = 0.0013173951301723719
iteration 128, loss = 0.0008744143415242434
iteration 129, loss = 0.00027582317125052214
iteration 130, loss = 0.00046665643458254635
iteration 131, loss = 0.00023154413793236017
iteration 132, loss = 0.00023272441467270255
iteration 133, loss = 0.00029540504328906536
iteration 134, loss = 0.0016216986114159226
iteration 135, loss = 0.00018857997201848775
iteration 136, loss = 0.0006554116844199598
iteration 137, loss = 0.00021514925174415112
iteration 138, loss = 0.0007095080218277872
iteration 139, loss = 0.0005828486755490303
iteration 140, loss = 0.00031024482450447977
iteration 141, loss = 0.00022345365141518414
iteration 142, loss = 0.0002354699099669233
iteration 143, loss = 0.0002385493426118046
iteration 144, loss = 0.00023572403006255627
iteration 145, loss = 0.00028067175298929214
iteration 146, loss = 0.00022676227672491223
iteration 147, loss = 0.0002640355087351054
iteration 148, loss = 0.0002132022345904261
iteration 149, loss = 0.0002988064952660352
iteration 150, loss = 0.00022610084852203727
iteration 151, loss = 0.0002302139182575047
iteration 152, loss = 0.00021500066213775426
iteration 153, loss = 0.0001671345526119694
iteration 154, loss = 0.00031184032559394836
iteration 155, loss = 0.0003560746554285288
iteration 156, loss = 0.0002314783923793584
iteration 157, loss = 0.0002010180032812059
iteration 158, loss = 0.0009510869858786464
iteration 159, loss = 0.00047160920803435147
iteration 160, loss = 0.00031445856438949704
iteration 161, loss = 0.0005847392603754997
iteration 162, loss = 0.00022603460820391774
iteration 163, loss = 0.0002795183681882918
iteration 164, loss = 0.00031817462877370417
iteration 165, loss = 0.0009175651939585805
iteration 166, loss = 0.00023368478287011385
iteration 167, loss = 0.0009303265833295882
iteration 168, loss = 0.00057392189046368
iteration 169, loss = 0.0003398653643671423
iteration 170, loss = 0.0003532652335707098
iteration 171, loss = 0.0005818172357976437
iteration 172, loss = 0.00032706386991776526
iteration 173, loss = 0.0006916163838468492
iteration 174, loss = 0.00022536782489623874
iteration 175, loss = 0.0002827388816513121
iteration 176, loss = 0.000493336352519691
iteration 177, loss = 0.00024869071785360575
iteration 178, loss = 0.0004278982523828745
iteration 179, loss = 0.00028194423066452146
iteration 180, loss = 0.00012854134547524154
iteration 181, loss = 0.00040164648089557886
iteration 182, loss = 0.0002712029963731766
iteration 183, loss = 0.00023573028738610446
iteration 184, loss = 0.00045174494152888656
iteration 185, loss = 0.00027570832753553987
iteration 186, loss = 0.0004210435727145523
iteration 187, loss = 0.0003899031435139477
iteration 188, loss = 0.00034309999318793416
iteration 189, loss = 0.0002718357718549669
iteration 190, loss = 0.0002435839851386845
iteration 191, loss = 0.00031131115974858403
iteration 192, loss = 0.0005679468158632517
iteration 193, loss = 0.00035684084286913276
iteration 194, loss = 0.0003166481910739094
iteration 195, loss = 0.0004311322409193963
iteration 196, loss = 0.00024262041551992297
iteration 197, loss = 0.0004021923814434558
iteration 198, loss = 0.00023774022702127695
iteration 199, loss = 0.0003042281896341592
iteration 200, loss = 0.0001934272877406329
iteration 201, loss = 0.0004530643345788121
iteration 202, loss = 0.0004728077619802207
iteration 203, loss = 0.0005664883647114038
iteration 204, loss = 0.000253507838351652
iteration 205, loss = 0.0002094143274007365
iteration 206, loss = 0.00024939837749116123
iteration 207, loss = 0.0003604730882216245
iteration 208, loss = 0.0001926080440171063
iteration 209, loss = 0.00037189849535934627
iteration 210, loss = 0.0004535260086413473
iteration 211, loss = 0.00046956396545283496
iteration 212, loss = 0.0003734409110620618
iteration 213, loss = 0.0002212507533840835
iteration 214, loss = 0.00045198548468761146
iteration 215, loss = 0.0009652904700487852
iteration 216, loss = 0.00034363169106654823
iteration 217, loss = 0.0008665102068334818
iteration 218, loss = 0.00039000288234092295
iteration 219, loss = 0.00040108521352522075
iteration 220, loss = 0.0002568060008343309
iteration 221, loss = 0.0003734409110620618
iteration 222, loss = 0.0005459034000523388
iteration 223, loss = 0.0005189490620978177
iteration 224, loss = 0.0004947668639943004
iteration 225, loss = 0.00027429862529970706
iteration 226, loss = 0.00035623900475911796
iteration 227, loss = 0.00023510253231506795
iteration 228, loss = 0.000937255856115371
iteration 229, loss = 0.0005427316064015031
iteration 230, loss = 0.000262163084698841
iteration 231, loss = 0.00031225543352775276
iteration 232, loss = 0.0003454399120528251
iteration 233, loss = 0.00017890646995510906
iteration 234, loss = 0.00021057580306660384
iteration 235, loss = 0.0002138210111297667
iteration 236, loss = 0.0009466526680625975
iteration 237, loss = 0.0003850396315101534
iteration 238, loss = 0.0002504391595721245
iteration 239, loss = 0.0006390149355866015
iteration 240, loss = 0.00021664075029548258
iteration 241, loss = 0.00020676848362199962
iteration 242, loss = 0.000694948248565197
iteration 243, loss = 0.00026710014208219945
iteration 244, loss = 0.00023129316105041653
iteration 245, loss = 0.0003199197817593813
iteration 246, loss = 0.0003907138598151505
iteration 247, loss = 0.00022818749130237848
iteration 248, loss = 0.00017613658565096557
iteration 249, loss = 0.0004360441816970706
iteration 250, loss = 0.00038670439971610904
iteration 251, loss = 0.00018482084851711988
iteration 252, loss = 0.0003351755440235138
iteration 253, loss = 0.0003361720882821828
iteration 254, loss = 0.00031389418290928006
iteration 255, loss = 0.00039138636202551425
iteration 256, loss = 0.0006095049902796745
iteration 257, loss = 0.0003217930207028985
iteration 258, loss = 0.0002432164765195921
iteration 259, loss = 0.0004724862810689956
iteration 260, loss = 0.0004373278934508562
iteration 261, loss = 0.0003569495747797191
iteration 262, loss = 0.0005303775542415679
iteration 263, loss = 0.0002708482788875699
iteration 264, loss = 0.0005736249149776995
iteration 265, loss = 0.000258721032878384
iteration 266, loss = 0.00047172739868983626
iteration 267, loss = 0.0006568533135578036
iteration 268, loss = 0.00014246987120714039
iteration 269, loss = 0.0002382388338446617
iteration 270, loss = 0.00038916332414373755
iteration 271, loss = 0.0002596080594230443
iteration 272, loss = 0.0008026679861359298
iteration 273, loss = 0.0005252791452221572
iteration 274, loss = 0.00015162420459091663
iteration 275, loss = 0.0003098274755757302
iteration 276, loss = 0.0003372712235432118
iteration 277, loss = 0.0006092791445553303
iteration 278, loss = 0.00025163774262182415
iteration 279, loss = 0.0003743944107554853
iteration 280, loss = 0.0004297627310734242
iteration 281, loss = 0.0002182910538977012
iteration 282, loss = 0.00024601357290521264
iteration 283, loss = 0.0006416039541363716
iteration 284, loss = 0.00024633953580632806
iteration 285, loss = 0.0004719831340480596
iteration 286, loss = 0.0003746953443624079
iteration 287, loss = 0.0002130463981302455
iteration 288, loss = 0.0005833911709487438
iteration 289, loss = 0.00019680446712300181
iteration 290, loss = 0.00039619486778974533
iteration 291, loss = 0.0002887561859097332
iteration 292, loss = 0.0002073652867693454
iteration 293, loss = 0.00023025390692055225
iteration 294, loss = 0.00017553554789628834
iteration 295, loss = 0.0002844013797584921
iteration 296, loss = 0.0003018493880517781
iteration 297, loss = 0.0003845553728751838
iteration 298, loss = 0.00024362509429920465
iteration 299, loss = 0.0008876319625414908
iteration 0, loss = 0.000212094746530056
iteration 1, loss = 0.0002842873800545931
iteration 2, loss = 0.00039791062590666115
iteration 3, loss = 0.0003163580549880862
iteration 4, loss = 0.0002632050891406834
iteration 5, loss = 0.0002892977499868721
iteration 6, loss = 0.00020054145716130733
iteration 7, loss = 0.0005240656319074333
iteration 8, loss = 0.0004597516090143472
iteration 9, loss = 0.00019632371549960226
iteration 10, loss = 0.00046403351007029414
iteration 11, loss = 0.0004131910973228514
iteration 12, loss = 0.0002161639858968556
iteration 13, loss = 0.00035946408752352
iteration 14, loss = 0.0010130933951586485
iteration 15, loss = 0.00040389306377619505
iteration 16, loss = 0.0004134220362175256
iteration 17, loss = 0.0003322608536109328
iteration 18, loss = 0.00037814542884007096
iteration 19, loss = 0.00022969191195443273
iteration 20, loss = 0.0006122077465988696
iteration 21, loss = 0.00018534611444920301
iteration 22, loss = 0.00030928864725865424
iteration 23, loss = 0.0004564130213111639
iteration 24, loss = 0.0002866128634195775
iteration 25, loss = 0.00029771713889203966
iteration 26, loss = 0.0003478681028354913
iteration 27, loss = 0.00018717853527050465
iteration 28, loss = 0.0002218980371253565
iteration 29, loss = 0.00022008278756402433
iteration 30, loss = 0.00027306066476739943
iteration 31, loss = 0.0003297678485978395
iteration 32, loss = 0.0002333418815396726
iteration 33, loss = 0.00018625093798618764
iteration 34, loss = 0.0002911399642471224
iteration 35, loss = 0.0003303541161585599
iteration 36, loss = 0.000300241430522874
iteration 37, loss = 0.00013187587319407612
iteration 38, loss = 0.0002570895303506404
iteration 39, loss = 0.0002532470098230988
iteration 40, loss = 0.00019103277008980513
iteration 41, loss = 0.00017686923092696816
iteration 42, loss = 0.00024342242977581918
iteration 43, loss = 0.0004540077061392367
iteration 44, loss = 0.0008913184865377843
iteration 45, loss = 0.00025387125788256526
iteration 46, loss = 0.0006428853957913816
iteration 47, loss = 0.0004076216428074986
iteration 48, loss = 0.00021810244652442634
iteration 49, loss = 0.00023188839259091765
iteration 50, loss = 0.0001606945152161643
iteration 51, loss = 0.0011465330608189106
iteration 52, loss = 0.00041541841346770525
iteration 53, loss = 0.0002651093527674675
iteration 54, loss = 0.0002689767861738801
iteration 55, loss = 0.00042100308928638697
iteration 56, loss = 0.00030563457403331995
iteration 57, loss = 0.00014760502381250262
iteration 58, loss = 0.00027546894853003323
iteration 59, loss = 0.00020328011305537075
iteration 60, loss = 0.00020596676040440798
iteration 61, loss = 0.0005646487115882337
iteration 62, loss = 0.0002611604577396065
iteration 63, loss = 0.0004656868986785412
iteration 64, loss = 0.0005572719965130091
iteration 65, loss = 0.00047637804527767
iteration 66, loss = 0.00026978645473718643
iteration 67, loss = 0.00029874173924326897
iteration 68, loss = 0.00028363295132294297
iteration 69, loss = 0.00048174476251006126
iteration 70, loss = 0.0003997540334239602
iteration 71, loss = 0.0002829043660312891
iteration 72, loss = 0.0005097404355183244
iteration 73, loss = 0.00018000431009568274
iteration 74, loss = 0.00028674426721408963
iteration 75, loss = 0.00016720594430807978
iteration 76, loss = 0.0005143344751559198
iteration 77, loss = 0.0003253856557421386
iteration 78, loss = 0.0003447211638558656
iteration 79, loss = 0.0005531000788323581
iteration 80, loss = 0.0002690525143407285
iteration 81, loss = 0.0003468341601546854
iteration 82, loss = 0.00027662035427056253
iteration 83, loss = 0.0005367515259422362
iteration 84, loss = 0.0009190203272737563
iteration 85, loss = 0.00027464484446682036
iteration 86, loss = 0.00022558333876077086
iteration 87, loss = 0.00031613654573448
iteration 88, loss = 0.0001670050114626065
iteration 89, loss = 0.00042644242057576776
iteration 90, loss = 0.00032591517083346844
iteration 91, loss = 0.0004612517077475786
iteration 92, loss = 0.0005164762842468917
iteration 93, loss = 0.0004327854258008301
iteration 94, loss = 0.00022910736151970923
iteration 95, loss = 0.00024229373957496136
iteration 96, loss = 0.00014941414701752365
iteration 97, loss = 0.0003208302950952202
iteration 98, loss = 0.00046029279474169016
iteration 99, loss = 0.0003671000595204532
iteration 100, loss = 0.0006484570330940187
iteration 101, loss = 0.00032483154791407287
iteration 102, loss = 0.0001975233608391136
iteration 103, loss = 0.000202013470698148
iteration 104, loss = 0.00021813252533320338
iteration 105, loss = 0.00024343063705600798
iteration 106, loss = 0.00042832797043956816
iteration 107, loss = 0.0005148837226442993
iteration 108, loss = 0.00021708912390749902
iteration 109, loss = 0.00029603001894429326
iteration 110, loss = 0.00025351785006932914
iteration 111, loss = 0.0001968923315871507
iteration 112, loss = 0.0004274009552318603
iteration 113, loss = 0.0002813194878399372
iteration 114, loss = 0.000716219306923449
iteration 115, loss = 0.00018507472123019397
iteration 116, loss = 0.00020344210497569293
iteration 117, loss = 0.0002723291690926999
iteration 118, loss = 0.000245999894104898
iteration 119, loss = 0.00047466770047321916
iteration 120, loss = 0.00028971355641260743
iteration 121, loss = 0.0004443708749022335
iteration 122, loss = 0.0003405812312848866
iteration 123, loss = 0.00028498651226982474
iteration 124, loss = 0.00027372001204639673
iteration 125, loss = 0.0003486044006422162
iteration 126, loss = 0.00038557880907319486
iteration 127, loss = 0.0002838164218701422
iteration 128, loss = 0.0004145427665207535
iteration 129, loss = 0.0003917148569598794
iteration 130, loss = 0.00020817111362703145
iteration 131, loss = 0.0002862572728190571
iteration 132, loss = 0.00019166007405146956
iteration 133, loss = 0.00022326692123897374
iteration 134, loss = 0.0002215447457274422
iteration 135, loss = 0.00016318081179633737
iteration 136, loss = 0.0009489483782090247
iteration 137, loss = 0.00029037692002020776
iteration 138, loss = 0.0004306553164497018
iteration 139, loss = 0.0003854492970276624
iteration 140, loss = 0.00039475454832427204
iteration 141, loss = 0.00029034080216661096
iteration 142, loss = 0.00016287079779431224
iteration 143, loss = 0.00020613150263670832
iteration 144, loss = 0.000594449695199728
iteration 145, loss = 0.0004327244241721928
iteration 146, loss = 0.0010088597191497684
iteration 147, loss = 0.00028001389000564814
iteration 148, loss = 0.0006151995039545
iteration 149, loss = 0.0005122615257278085
iteration 150, loss = 0.00019989073916804045
iteration 151, loss = 0.0004272277874406427
iteration 152, loss = 0.0006203667726367712
iteration 153, loss = 0.0004268714983481914
iteration 154, loss = 0.00021910082432441413
iteration 155, loss = 0.00020534724171739072
iteration 156, loss = 0.00022274286311585456
iteration 157, loss = 0.00023859695647843182
iteration 158, loss = 0.00023970783513505012
iteration 159, loss = 0.0009846388129517436
iteration 160, loss = 0.000978046446107328
iteration 161, loss = 0.00025262346025556326
iteration 162, loss = 0.00026360570336692035
iteration 163, loss = 0.00041240264545194805
iteration 164, loss = 0.00023337712627835572
iteration 165, loss = 0.00023190840147435665
iteration 166, loss = 0.0004288720665499568
iteration 167, loss = 0.0002315261517651379
iteration 168, loss = 0.0006737088551744819
iteration 169, loss = 0.0004430054104886949
iteration 170, loss = 0.0005227201618254185
iteration 171, loss = 0.00035410895361565053
iteration 172, loss = 0.00018502914463169873
iteration 173, loss = 0.000308410992147401
iteration 174, loss = 0.00025574618484824896
iteration 175, loss = 0.0007876945310272276
iteration 176, loss = 0.000936366559471935
iteration 177, loss = 0.00018112434190697968
iteration 178, loss = 0.00021076673874631524
iteration 179, loss = 0.00017681479221209884
iteration 180, loss = 0.000622883380856365
iteration 181, loss = 0.0003054851258639246
iteration 182, loss = 0.0006072254618629813
iteration 183, loss = 0.0009695783373899758
iteration 184, loss = 0.0005082859424874187
iteration 185, loss = 0.00020064780255779624
iteration 186, loss = 0.00018014316447079182
iteration 187, loss = 0.0005949186743237078
iteration 188, loss = 0.00021286829723976552
iteration 189, loss = 0.00013632283662445843
iteration 190, loss = 0.00036409994936548173
iteration 191, loss = 0.0002038025704678148
iteration 192, loss = 0.00034784633317030966
iteration 193, loss = 0.0002366384578635916
iteration 194, loss = 0.00013628308079205453
iteration 195, loss = 0.0004497242043726146
iteration 196, loss = 0.00026969524333253503
iteration 197, loss = 0.00026229440118186176
iteration 198, loss = 0.0001857598399510607
iteration 199, loss = 0.0003301291726529598
iteration 200, loss = 0.0006122704944573343
iteration 201, loss = 0.0003395522362552583
iteration 202, loss = 0.00013865850633010268
iteration 203, loss = 0.00027306564152240753
iteration 204, loss = 0.0002333861484657973
iteration 205, loss = 0.00038241749280132353
iteration 206, loss = 0.00023914489429444075
iteration 207, loss = 0.0003646067634690553
iteration 208, loss = 0.00020978989778086543
iteration 209, loss = 0.0005664939526468515
iteration 210, loss = 0.00041180875268764794
iteration 211, loss = 0.0002213054394815117
iteration 212, loss = 0.0005176174454391003
iteration 213, loss = 0.000290277210297063
iteration 214, loss = 0.00023983382561709732
iteration 215, loss = 0.00023749019601382315
iteration 216, loss = 0.00029043518588878214
iteration 217, loss = 0.0002524360315874219
iteration 218, loss = 0.00040563111542724073
iteration 219, loss = 0.0008145558531396091
iteration 220, loss = 0.0003156757156830281
iteration 221, loss = 0.00019028295355383307
iteration 222, loss = 0.0002363892417633906
iteration 223, loss = 0.00028836732963100076
iteration 224, loss = 0.0004502490337472409
iteration 225, loss = 0.0009060294833034277
iteration 226, loss = 0.00035721517633646727
iteration 227, loss = 0.00020677009888458997
iteration 228, loss = 0.00042832386679947376
iteration 229, loss = 0.0003965653304476291
iteration 230, loss = 0.0011554794618859887
iteration 231, loss = 0.000543556991033256
iteration 232, loss = 0.0003733163757715374
iteration 233, loss = 0.0003705138515215367
iteration 234, loss = 0.00016776443226262927
iteration 235, loss = 0.00030244450317695737
iteration 236, loss = 0.0003394822415430099
iteration 237, loss = 0.00018835783703252673
iteration 238, loss = 0.00030529394280165434
iteration 239, loss = 0.000248498166911304
iteration 240, loss = 0.0002791185106616467
iteration 241, loss = 0.0001662122376728803
iteration 242, loss = 0.0002603197062853724
iteration 243, loss = 0.00043641068623401225
iteration 244, loss = 0.0001583133271196857
iteration 245, loss = 0.00018770102178677917
iteration 246, loss = 0.0003345885197632015
iteration 247, loss = 0.00020790846610907465
iteration 248, loss = 0.00021478695271071047
iteration 249, loss = 0.0005455095088109374
iteration 250, loss = 0.00024598283926025033
iteration 251, loss = 0.0005395035841502249
iteration 252, loss = 0.0002937690878752619
iteration 253, loss = 0.0002813163155224174
iteration 254, loss = 0.00041024258825927973
iteration 255, loss = 0.0002768405538517982
iteration 256, loss = 0.00047501662629656494
iteration 257, loss = 0.0005737832398153841
iteration 258, loss = 0.00016187570872716606
iteration 259, loss = 0.0005161362350918353
iteration 260, loss = 0.0004265562165528536
iteration 261, loss = 0.00023610409698449075
iteration 262, loss = 0.00027203396894037724
iteration 263, loss = 0.00022392933897208422
iteration 264, loss = 0.0004016821039840579
iteration 265, loss = 0.0002896049409173429
iteration 266, loss = 0.0003722641267813742
iteration 267, loss = 0.0002215910644736141
iteration 268, loss = 0.00024390812905039638
iteration 269, loss = 0.0016223735874518752
iteration 270, loss = 0.0009725703275762498
iteration 271, loss = 0.00019337391131557524
iteration 272, loss = 0.0002603140310384333
iteration 273, loss = 0.0008115913951769471
iteration 274, loss = 0.0005798233323730528
iteration 275, loss = 0.0006465377518907189
iteration 276, loss = 0.0002554467646405101
iteration 277, loss = 0.0005692209815606475
iteration 278, loss = 0.0009164607035927474
iteration 279, loss = 0.0002515733940526843
iteration 280, loss = 0.0006115688011050224
iteration 281, loss = 0.0003561422345228493
iteration 282, loss = 0.0004973321338184178
iteration 283, loss = 0.0002446180151309818
iteration 284, loss = 0.00020438813953660429
iteration 285, loss = 0.00020438052888493985
iteration 286, loss = 0.0008777708862908185
iteration 287, loss = 0.0002578986750449985
iteration 288, loss = 0.0005551830399781466
iteration 289, loss = 0.00022273039212450385
iteration 290, loss = 0.000225621071876958
iteration 291, loss = 0.00043791800271719694
iteration 292, loss = 0.00016401003813371062
iteration 293, loss = 0.00024308454885613173
iteration 294, loss = 0.00045607396168634295
iteration 295, loss = 0.0005788779235444963
iteration 296, loss = 0.0006354853394441307
iteration 297, loss = 0.00036755643668584526
iteration 298, loss = 0.00021508421923499554
iteration 299, loss = 0.00047817674931138754
iteration 0, loss = 0.0002393469330854714
iteration 1, loss = 0.00035038721398450434
iteration 2, loss = 0.0002505113370716572
iteration 3, loss = 0.0003332743654027581
iteration 4, loss = 0.00029737711884081364
iteration 5, loss = 0.0003068215155508369
iteration 6, loss = 0.00064464082242921
iteration 7, loss = 0.0003803333092946559
iteration 8, loss = 0.00018414344231132418
iteration 9, loss = 0.0004849184479098767
iteration 10, loss = 0.00031165225664153695
iteration 11, loss = 0.0005662449984811246
iteration 12, loss = 0.0003595875750761479
iteration 13, loss = 0.00024763611145317554
iteration 14, loss = 0.0003275586059316993
iteration 15, loss = 0.00020465461420826614
iteration 16, loss = 0.0004444511141628027
iteration 17, loss = 0.00035282515455037355
iteration 18, loss = 0.0005846771528013051
iteration 19, loss = 0.000322484178468585
iteration 20, loss = 0.00028969382401555777
iteration 21, loss = 0.00014991831267252564
iteration 22, loss = 0.000698464922606945
iteration 23, loss = 0.00027270044665783644
iteration 24, loss = 0.00017629416834097356
iteration 25, loss = 0.0002550811623223126
iteration 26, loss = 0.00044968692236579955
iteration 27, loss = 0.0002636276767589152
iteration 28, loss = 0.00025689523317851126
iteration 29, loss = 0.0003089972597081214
iteration 30, loss = 0.0008701321785338223
iteration 31, loss = 0.0007273954688571393
iteration 32, loss = 0.00030343292746692896
iteration 33, loss = 0.0005571766523644328
iteration 34, loss = 0.00022429469390772283
iteration 35, loss = 0.00034226177376694977
iteration 36, loss = 0.0003050530795007944
iteration 37, loss = 0.00037039886228740215
iteration 38, loss = 0.001333156251348555
iteration 39, loss = 0.00033777838689275086
iteration 40, loss = 0.0003073934931308031
iteration 41, loss = 0.00032050078152678907
iteration 42, loss = 0.0004788781516253948
iteration 43, loss = 0.00037916027940809727
iteration 44, loss = 0.00016802320897113532
iteration 45, loss = 0.00031032823608256876
iteration 46, loss = 0.00042100934660993516
iteration 47, loss = 0.00014611285587307066
iteration 48, loss = 0.000610304472502321
iteration 49, loss = 0.00021649480913765728
iteration 50, loss = 0.0002711178094614297
iteration 51, loss = 0.00025185709819197655
iteration 52, loss = 0.00019796579726971686
iteration 53, loss = 0.0009991925908252597
iteration 54, loss = 0.0002096123353112489
iteration 55, loss = 0.0003006796760018915
iteration 56, loss = 0.00023821787908673286
iteration 57, loss = 0.0001757888385327533
iteration 58, loss = 0.00018654533778317273
iteration 59, loss = 0.0003589978441596031
iteration 60, loss = 0.00031199344084598124
iteration 61, loss = 0.00026115847867913544
iteration 62, loss = 0.00022350110521074384
iteration 63, loss = 0.00045950751518830657
iteration 64, loss = 0.0002886249858420342
iteration 65, loss = 0.00021642769570462406
iteration 66, loss = 0.00026056659407913685
iteration 67, loss = 0.00018067355267703533
iteration 68, loss = 0.000191147206351161
iteration 69, loss = 0.0003430411161389202
iteration 70, loss = 0.00019532702572178096
iteration 71, loss = 0.0005925110308453441
iteration 72, loss = 0.00041922761010937393
iteration 73, loss = 0.00044419983169063926
iteration 74, loss = 0.0001880845957202837
iteration 75, loss = 0.00019888154929503798
iteration 76, loss = 0.00020316441077739
iteration 77, loss = 0.0005564899183809757
iteration 78, loss = 0.0012827330501750112
iteration 79, loss = 0.00031249976018443704
iteration 80, loss = 0.0001771113893482834
iteration 81, loss = 0.00034228069125674665
iteration 82, loss = 0.00037691471516154706
iteration 83, loss = 0.0004398277960717678
iteration 84, loss = 0.00028079349431209266
iteration 85, loss = 0.00025284194271080196
iteration 86, loss = 0.0003934607666451484
iteration 87, loss = 0.00023249260266311467
iteration 88, loss = 0.0004977828357368708
iteration 89, loss = 0.0006336788064800203
iteration 90, loss = 0.0005709769320674241
iteration 91, loss = 0.0010991838062182069
iteration 92, loss = 0.00029305522912181914
iteration 93, loss = 0.0007684522424824536
iteration 94, loss = 0.0004502688825596124
iteration 95, loss = 0.000392639689380303
iteration 96, loss = 0.0004308958596084267
iteration 97, loss = 0.00022645050194114447
iteration 98, loss = 0.0003492426476441324
iteration 99, loss = 0.0003420623834244907
iteration 100, loss = 0.00017516844673082232
iteration 101, loss = 0.0002469149767421186
iteration 102, loss = 0.0002730131382122636
iteration 103, loss = 0.0003355233056936413
iteration 104, loss = 0.00032789044780656695
iteration 105, loss = 0.00021693883172702044
iteration 106, loss = 0.0002023847628151998
iteration 107, loss = 0.0006313177873380482
iteration 108, loss = 0.0002267655509058386
iteration 109, loss = 0.0003935132408514619
iteration 110, loss = 0.0005508348112925887
iteration 111, loss = 0.0009610623819753528
iteration 112, loss = 0.0004500070062931627
iteration 113, loss = 0.00015210240962915123
iteration 114, loss = 0.0003876244882121682
iteration 115, loss = 0.0005427374853752553
iteration 116, loss = 0.000194542997633107
iteration 117, loss = 0.0002301466156495735
iteration 118, loss = 0.00015230695134960115
iteration 119, loss = 0.0003959529276471585
iteration 120, loss = 0.000475046515930444
iteration 121, loss = 0.0006528574740514159
iteration 122, loss = 0.00021486959303729236
iteration 123, loss = 0.00094932212959975
iteration 124, loss = 0.0005108750192448497
iteration 125, loss = 0.00017761479830369353
iteration 126, loss = 0.00022204223205335438
iteration 127, loss = 0.0003581111377570778
iteration 128, loss = 0.00018578785238787532
iteration 129, loss = 0.00015277921920642257
iteration 130, loss = 0.00039905571611598134
iteration 131, loss = 0.00020146346651017666
iteration 132, loss = 0.0003189413982909173
iteration 133, loss = 0.0004932459560222924
iteration 134, loss = 0.0002989118220284581
iteration 135, loss = 0.000692293921019882
iteration 136, loss = 0.00042511470383033156
iteration 137, loss = 0.0004363057087175548
iteration 138, loss = 0.001046566991135478
iteration 139, loss = 0.0002194289700128138
iteration 140, loss = 0.0002803658717311919
iteration 141, loss = 0.00022882019402459264
iteration 142, loss = 0.00034277664963155985
iteration 143, loss = 0.0002441987453494221
iteration 144, loss = 0.0001863050856627524
iteration 145, loss = 0.00034541485365480185
iteration 146, loss = 0.00020390294957906008
iteration 147, loss = 0.0007098322384990752
iteration 148, loss = 0.0002102875296259299
iteration 149, loss = 0.00018556953000370413
iteration 150, loss = 0.00020705049973912537
iteration 151, loss = 0.0003044785698875785
iteration 152, loss = 0.0002617203281261027
iteration 153, loss = 0.0003207868430763483
iteration 154, loss = 0.00020274741109460592
iteration 155, loss = 0.00021772024047095329
iteration 156, loss = 0.000342774874297902
iteration 157, loss = 0.00018857073155231774
iteration 158, loss = 0.00023614522069692612
iteration 159, loss = 0.0002714141446631402
iteration 160, loss = 0.00025889946846291423
iteration 161, loss = 0.0001878673065220937
iteration 162, loss = 0.00036153130349703133
iteration 163, loss = 0.000257214269367978
iteration 164, loss = 0.000286133901681751
iteration 165, loss = 0.00022266953601501882
iteration 166, loss = 0.0003499448939692229
iteration 167, loss = 0.0003258266078773886
iteration 168, loss = 0.0002482829731889069
iteration 169, loss = 0.0003973276761826128
iteration 170, loss = 0.00022095974418334663
iteration 171, loss = 0.00031804514583200216
iteration 172, loss = 0.00030580026214011014
iteration 173, loss = 0.0003276827046647668
iteration 174, loss = 0.0006412090151570737
iteration 175, loss = 0.00018844132137019187
iteration 176, loss = 0.00017315604782197624
iteration 177, loss = 0.0015490759396925569
iteration 178, loss = 0.0004586086724884808
iteration 179, loss = 0.000223686991375871
iteration 180, loss = 0.0007186508155427873
iteration 181, loss = 0.00021044802269898355
iteration 182, loss = 0.00020432683231774718
iteration 183, loss = 0.00023615246755070984
iteration 184, loss = 0.00018400800763629377
iteration 185, loss = 0.0002721307391766459
iteration 186, loss = 0.00024126286734826863
iteration 187, loss = 0.0003431895165704191
iteration 188, loss = 0.00025504358927719295
iteration 189, loss = 0.00031618974753655493
iteration 190, loss = 0.0006174187874421477
iteration 191, loss = 0.00045657670125365257
iteration 192, loss = 0.00020815996685996652
iteration 193, loss = 0.0009773429483175278
iteration 194, loss = 0.00019415223505347967
iteration 195, loss = 0.0003268117143306881
iteration 196, loss = 0.00021387844753917307
iteration 197, loss = 0.00036850597825832665
iteration 198, loss = 0.0002795568434521556
iteration 199, loss = 0.00039084083982743323
iteration 200, loss = 0.00035088867298327386
iteration 201, loss = 0.00027360435342416167
iteration 202, loss = 0.0003378185210749507
iteration 203, loss = 0.00022334307141136378
iteration 204, loss = 0.00022338223061524332
iteration 205, loss = 0.0002840299566742033
iteration 206, loss = 0.00038301944732666016
iteration 207, loss = 0.0005942961433902383
iteration 208, loss = 0.0003045338671654463
iteration 209, loss = 0.00020490218594204634
iteration 210, loss = 0.000227779833949171
iteration 211, loss = 0.0002626103814691305
iteration 212, loss = 0.000437593407696113
iteration 213, loss = 0.00023864806280471385
iteration 214, loss = 0.00025617878418415785
iteration 215, loss = 0.00029110227478668094
iteration 216, loss = 0.00033614711719565094
iteration 217, loss = 0.00016462554049212486
iteration 218, loss = 0.0002204453048761934
iteration 219, loss = 0.00020664214389398694
iteration 220, loss = 0.00025650791940279305
iteration 221, loss = 0.00016964729002211243
iteration 222, loss = 0.0009582856437191367
iteration 223, loss = 0.00018582699703983963
iteration 224, loss = 0.0008232567342929542
iteration 225, loss = 0.0003620619245339185
iteration 226, loss = 0.0003650955040939152
iteration 227, loss = 0.0001513881143182516
iteration 228, loss = 0.000889013463165611
iteration 229, loss = 0.0007372468826361
iteration 230, loss = 0.000626707507763058
iteration 231, loss = 0.0006694715702906251
iteration 232, loss = 0.00021449507039505988
iteration 233, loss = 0.0006085878703743219
iteration 234, loss = 0.0005198083817958832
iteration 235, loss = 0.0002694717259146273
iteration 236, loss = 0.0002563090529292822
iteration 237, loss = 0.00019034567230846733
iteration 238, loss = 0.0004390912363305688
iteration 239, loss = 0.0002516724052838981
iteration 240, loss = 0.00028983893571421504
iteration 241, loss = 0.0003370496560819447
iteration 242, loss = 0.00020569772459566593
iteration 243, loss = 0.00022133439779281616
iteration 244, loss = 0.0003894082619808614
iteration 245, loss = 0.0010223875287920237
iteration 246, loss = 0.00022435739811044186
iteration 247, loss = 0.0004258410772308707
iteration 248, loss = 0.0003070896491408348
iteration 249, loss = 0.0002976646937895566
iteration 250, loss = 0.00018978194566443563
iteration 251, loss = 0.00018999690655618906
iteration 252, loss = 0.000261679757386446
iteration 253, loss = 0.00024532031966373324
iteration 254, loss = 0.00032679372816346586
iteration 255, loss = 0.00017077299708034843
iteration 256, loss = 0.00035611854400485754
iteration 257, loss = 0.0005346522084437311
iteration 258, loss = 0.00032277670106850564
iteration 259, loss = 0.000430657877586782
iteration 260, loss = 0.00028380192816257477
iteration 261, loss = 0.00019039057951886207
iteration 262, loss = 0.0003736735670827329
iteration 263, loss = 0.0004919777857139707
iteration 264, loss = 0.0002782597439363599
iteration 265, loss = 0.00041091139428317547
iteration 266, loss = 0.0001517128257546574
iteration 267, loss = 0.0003185886889696121
iteration 268, loss = 0.00022086029639467597
iteration 269, loss = 0.0003889020881615579
iteration 270, loss = 0.00022714586521033198
iteration 271, loss = 0.0002066434099106118
iteration 272, loss = 0.0003016788396053016
iteration 273, loss = 0.00019910908304154873
iteration 274, loss = 0.00020812367438338697
iteration 275, loss = 0.00042262757779099047
iteration 276, loss = 0.0003508096269797534
iteration 277, loss = 0.0008135764510370791
iteration 278, loss = 0.0001990675227716565
iteration 279, loss = 0.0002274757280247286
iteration 280, loss = 0.0003464499022811651
iteration 281, loss = 0.00021639284386765212
iteration 282, loss = 0.0002759186609182507
iteration 283, loss = 0.0002623498730827123
iteration 284, loss = 0.00032763619674369693
iteration 285, loss = 0.00020768627291545272
iteration 286, loss = 0.0002841836540028453
iteration 287, loss = 0.0003194563032593578
iteration 288, loss = 0.0002733226865530014
iteration 289, loss = 0.0006367388996295631
iteration 290, loss = 0.00019475353474263102
iteration 291, loss = 0.0003096248838119209
iteration 292, loss = 0.000863610883243382
iteration 293, loss = 0.0004486578400246799
iteration 294, loss = 0.00016977888299152255
iteration 295, loss = 0.0006277445354498923
iteration 296, loss = 0.00021487454068847
iteration 297, loss = 0.0001902518852148205
iteration 298, loss = 0.00025626379647292197
iteration 299, loss = 0.0002597530256025493
iteration 0, loss = 0.0003056239220313728
iteration 1, loss = 0.00028643186669796705
iteration 2, loss = 0.00028851599199697375
iteration 3, loss = 0.00016799219883978367
iteration 4, loss = 0.00023192491789814085
iteration 5, loss = 0.0003734751371666789
iteration 6, loss = 0.0001891144347609952
iteration 7, loss = 0.00037257024087011814
iteration 8, loss = 0.0002772956795524806
iteration 9, loss = 0.0008289302932098508
iteration 10, loss = 0.0002002125111175701
iteration 11, loss = 0.0002216494467575103
iteration 12, loss = 0.00047664425801485777
iteration 13, loss = 0.00028265625587664545
iteration 14, loss = 0.0002199819718953222
iteration 15, loss = 0.00023450105800293386
iteration 16, loss = 0.00029555981745943427
iteration 17, loss = 0.0005400151130743325
iteration 18, loss = 0.00019755058747250587
iteration 19, loss = 0.0006059131119400263
iteration 20, loss = 0.00014987163012847304
iteration 21, loss = 0.00034288052120245993
iteration 22, loss = 0.0003108691889792681
iteration 23, loss = 0.00015347251610364765
iteration 24, loss = 0.0004926680703647435
iteration 25, loss = 0.00046906390343792737
iteration 26, loss = 0.00019933865405619144
iteration 27, loss = 0.0003767721354961395
iteration 28, loss = 0.00042171450331807137
iteration 29, loss = 0.00044933208846487105
iteration 30, loss = 0.00037306779995560646
iteration 31, loss = 0.00027121795574203134
iteration 32, loss = 0.0002855975762940943
iteration 33, loss = 0.00027070080977864563
iteration 34, loss = 0.0002472191408742219
iteration 35, loss = 0.0003640363283921033
iteration 36, loss = 0.00019515767053235322
iteration 37, loss = 0.0005163262248970568
iteration 38, loss = 0.0001754992117639631
iteration 39, loss = 0.0007910067797638476
iteration 40, loss = 0.000130724860355258
iteration 41, loss = 0.00032204604940488935
iteration 42, loss = 0.00016065323143266141
iteration 43, loss = 0.00031928226235322654
iteration 44, loss = 0.00023350412084255368
iteration 45, loss = 0.0001646276214160025
iteration 46, loss = 0.0003785704029724002
iteration 47, loss = 0.00021384612773545086
iteration 48, loss = 0.00023429766588378698
iteration 49, loss = 0.0005896401708014309
iteration 50, loss = 0.00043978405301459134
iteration 51, loss = 0.00036126963095739484
iteration 52, loss = 0.001001683878712356
iteration 53, loss = 0.0002098752884194255
iteration 54, loss = 0.0008689992246218026
iteration 55, loss = 0.0005349264247342944
iteration 56, loss = 0.0003224052779842168
iteration 57, loss = 0.0009094663546420634
iteration 58, loss = 0.0005803540116176009
iteration 59, loss = 0.00014010377344675362
iteration 60, loss = 0.0005216457648202777
iteration 61, loss = 0.0001764775370247662
iteration 62, loss = 0.0003500687889754772
iteration 63, loss = 0.0002446852158755064
iteration 64, loss = 0.00014722933701705188
iteration 65, loss = 0.00018979112792294472
iteration 66, loss = 0.00037644198164343834
iteration 67, loss = 0.0003767113375943154
iteration 68, loss = 0.00039833318442106247
iteration 69, loss = 0.00043932785047218204
iteration 70, loss = 0.0003660956281237304
iteration 71, loss = 0.0010318298591300845
iteration 72, loss = 0.0001941903174156323
iteration 73, loss = 0.00019757339032366872
iteration 74, loss = 0.0007792935939505696
iteration 75, loss = 0.00022206950234249234
iteration 76, loss = 0.0002853103796951473
iteration 77, loss = 0.0002947182219941169
iteration 78, loss = 0.0008839435176923871
iteration 79, loss = 0.00048533783410675824
iteration 80, loss = 0.00027620632317848504
iteration 81, loss = 0.0006901795277372003
iteration 82, loss = 0.0004490342107601464
iteration 83, loss = 0.00039524363819509745
iteration 84, loss = 0.0003224457905162126
iteration 85, loss = 0.0002154490357497707
iteration 86, loss = 0.0002005675050895661
iteration 87, loss = 0.00016424263594672084
iteration 88, loss = 0.0003317996743135154
iteration 89, loss = 0.0002116620453307405
iteration 90, loss = 0.0003162912908010185
iteration 91, loss = 0.00021439697593450546
iteration 92, loss = 0.0004779880109708756
iteration 93, loss = 0.0002868116134777665
iteration 94, loss = 0.00015882922161836177
iteration 95, loss = 0.00021075460244901478
iteration 96, loss = 0.00045359780779108405
iteration 97, loss = 0.0009016642579808831
iteration 98, loss = 0.0002383102400926873
iteration 99, loss = 0.00020292200497351587
iteration 100, loss = 0.0003207149275112897
iteration 101, loss = 0.0005369303980842233
iteration 102, loss = 0.0003036154084838927
iteration 103, loss = 0.0002754764282144606
iteration 104, loss = 0.0005581071600317955
iteration 105, loss = 0.00039601573371328413
iteration 106, loss = 0.0007990618469193578
iteration 107, loss = 0.00034536654129624367
iteration 108, loss = 0.0002426546416245401
iteration 109, loss = 0.0006667213747277856
iteration 110, loss = 0.00028792707598768175
iteration 111, loss = 0.000283006695099175
iteration 112, loss = 0.00038574798963963985
iteration 113, loss = 0.000262862624367699
iteration 114, loss = 0.00035978248342871666
iteration 115, loss = 0.00030358496587723494
iteration 116, loss = 0.0006535295397043228
iteration 117, loss = 0.00021915871184319258
iteration 118, loss = 0.0005657110013999045
iteration 119, loss = 0.000480782677186653
iteration 120, loss = 0.00017169571947306395
iteration 121, loss = 0.00012298818910494447
iteration 122, loss = 0.0005247612134553492
iteration 123, loss = 0.00027568155201151967
iteration 124, loss = 0.00036243180511519313
iteration 125, loss = 0.00016046853852458298
iteration 126, loss = 0.0008394467295147479
iteration 127, loss = 0.00019850330136250705
iteration 128, loss = 0.0006001375149935484
iteration 129, loss = 0.0004175841168034822
iteration 130, loss = 0.00028499873587861657
iteration 131, loss = 0.00035300906165502965
iteration 132, loss = 0.00036243023350834846
iteration 133, loss = 0.00019226266886107624
iteration 134, loss = 0.00024268105335067958
iteration 135, loss = 0.00015325582353398204
iteration 136, loss = 0.0010489692213013768
iteration 137, loss = 0.00018056701810564846
iteration 138, loss = 0.00021892038057558239
iteration 139, loss = 0.0006109373061917722
iteration 140, loss = 0.0005739346379414201
iteration 141, loss = 0.0006412183865904808
iteration 142, loss = 0.00019279634580016136
iteration 143, loss = 0.00022167465067468584
iteration 144, loss = 0.0002365371328778565
iteration 145, loss = 0.0002897983940783888
iteration 146, loss = 0.00033809247543103993
iteration 147, loss = 0.0004208707541693002
iteration 148, loss = 0.000201418501092121
iteration 149, loss = 0.0002742922806646675
iteration 150, loss = 0.00024829863104969263
iteration 151, loss = 0.00023196570691652596
iteration 152, loss = 0.00025103698135353625
iteration 153, loss = 0.00030096288537606597
iteration 154, loss = 0.00023125146981328726
iteration 155, loss = 0.00035833939909935
iteration 156, loss = 0.00019738016999326646
iteration 157, loss = 0.00023308796517085284
iteration 158, loss = 0.00029622248257510364
iteration 159, loss = 0.000281717162579298
iteration 160, loss = 0.000180501738213934
iteration 161, loss = 0.0001984417176572606
iteration 162, loss = 0.0005394047475419939
iteration 163, loss = 0.0002781662333291024
iteration 164, loss = 0.0004246087046340108
iteration 165, loss = 0.0002285746595589444
iteration 166, loss = 0.0002575002145022154
iteration 167, loss = 0.0005343228694982827
iteration 168, loss = 0.0002847641590051353
iteration 169, loss = 0.0001965542760444805
iteration 170, loss = 0.0002555729588493705
iteration 171, loss = 0.00041849486296996474
iteration 172, loss = 0.00037192663876339793
iteration 173, loss = 0.00024448250769637525
iteration 174, loss = 0.0002776586916297674
iteration 175, loss = 0.00021664920495823026
iteration 176, loss = 0.00017333694268018007
iteration 177, loss = 0.0005321209318935871
iteration 178, loss = 0.000260208616964519
iteration 179, loss = 0.00019247080490458757
iteration 180, loss = 0.0002829924051184207
iteration 181, loss = 0.0009259990183636546
iteration 182, loss = 0.00048200186574831605
iteration 183, loss = 0.00035366087104193866
iteration 184, loss = 0.00018488972273189574
iteration 185, loss = 0.0005890103057026863
iteration 186, loss = 0.0001619348768144846
iteration 187, loss = 0.00017309715622104704
iteration 188, loss = 0.00021435745293274522
iteration 189, loss = 0.00017200120782945305
iteration 190, loss = 0.0004591057077050209
iteration 191, loss = 0.00023187394253909588
iteration 192, loss = 0.0004904039669781923
iteration 193, loss = 0.0003386913740541786
iteration 194, loss = 0.00029129988979548216
iteration 195, loss = 0.00028322808793745935
iteration 196, loss = 0.0006377215031534433
iteration 197, loss = 0.00046463447506539524
iteration 198, loss = 0.00041356723522767425
iteration 199, loss = 0.00021237954206299037
iteration 200, loss = 0.00047136511420831084
iteration 201, loss = 0.00019146877457387745
iteration 202, loss = 0.00023011855955701321
iteration 203, loss = 0.00022850223467685282
iteration 204, loss = 0.00048021538532339036
iteration 205, loss = 0.00023122699349187315
iteration 206, loss = 0.0004221515264362097
iteration 207, loss = 0.000889298680704087
iteration 208, loss = 0.0004986845888197422
iteration 209, loss = 0.0002346920082345605
iteration 210, loss = 0.00024181502521969378
iteration 211, loss = 0.00048681351472623646
iteration 212, loss = 0.00027717655757442117
iteration 213, loss = 0.0006413836381398141
iteration 214, loss = 0.0003142955538351089
iteration 215, loss = 0.00021666090469807386
iteration 216, loss = 0.00024988024961203337
iteration 217, loss = 0.00022637896472588181
iteration 218, loss = 0.0003273664915468544
iteration 219, loss = 0.00027108532958664
iteration 220, loss = 0.0001900815695989877
iteration 221, loss = 0.0010031235869973898
iteration 222, loss = 0.00022854893177282065
iteration 223, loss = 0.00024359812960028648
iteration 224, loss = 0.0002553641388658434
iteration 225, loss = 0.0005654343403875828
iteration 226, loss = 0.0004940557410009205
iteration 227, loss = 0.000574050412978977
iteration 228, loss = 0.0002175418339902535
iteration 229, loss = 0.000314663746394217
iteration 230, loss = 0.00023796092136763036
iteration 231, loss = 0.0008148938650265336
iteration 232, loss = 0.00020949923782609403
iteration 233, loss = 0.00022452088887803257
iteration 234, loss = 0.00032338150776922703
iteration 235, loss = 0.00048309689736925066
iteration 236, loss = 0.00017556171223986894
iteration 237, loss = 0.0002157187118427828
iteration 238, loss = 0.0001745384361129254
iteration 239, loss = 0.0002724634250625968
iteration 240, loss = 0.0004071746370755136
iteration 241, loss = 0.00021047037444077432
iteration 242, loss = 0.00020642504387069494
iteration 243, loss = 0.00031619134824723005
iteration 244, loss = 0.00020711799152195454
iteration 245, loss = 0.00023699901066720486
iteration 246, loss = 0.00027796492213383317
iteration 247, loss = 0.0002269981341669336
iteration 248, loss = 0.00020161463180556893
iteration 249, loss = 0.00043590759742073715
iteration 250, loss = 0.0005087330937385559
iteration 251, loss = 0.000898072321433574
iteration 252, loss = 0.00018030658247880638
iteration 253, loss = 0.00032165541779249907
iteration 254, loss = 0.00022178201470524073
iteration 255, loss = 0.00042533682426437736
iteration 256, loss = 0.0003422622103244066
iteration 257, loss = 0.0003053683612961322
iteration 258, loss = 0.0002020475803874433
iteration 259, loss = 0.0003012550878338516
iteration 260, loss = 0.000578306324314326
iteration 261, loss = 0.00041634036460891366
iteration 262, loss = 0.00027802123804576695
iteration 263, loss = 0.0003039063303731382
iteration 264, loss = 0.0003828686894848943
iteration 265, loss = 0.0009410841157659888
iteration 266, loss = 0.00021137000294402242
iteration 267, loss = 0.00033628076198510826
iteration 268, loss = 0.00026316638104617596
iteration 269, loss = 0.0003969803801737726
iteration 270, loss = 0.0003671230806503445
iteration 271, loss = 0.0002397035714238882
iteration 272, loss = 0.00026490818709135056
iteration 273, loss = 0.00012215827882755548
iteration 274, loss = 0.0002478236856404692
iteration 275, loss = 0.0004836387815885246
iteration 276, loss = 0.0002158113056793809
iteration 277, loss = 0.00016006379155442119
iteration 278, loss = 0.0003338676178827882
iteration 279, loss = 0.0002777368645183742
iteration 280, loss = 0.0002458697708789259
iteration 281, loss = 0.00029438029741868377
iteration 282, loss = 0.0003221257356926799
iteration 283, loss = 0.000529331446159631
iteration 284, loss = 0.0003807770845014602
iteration 285, loss = 0.0003205611719749868
iteration 286, loss = 0.0002870082389563322
iteration 287, loss = 0.00013314306852407753
iteration 288, loss = 0.0002272009151056409
iteration 289, loss = 0.0003311355831101537
iteration 290, loss = 0.000335416232701391
iteration 291, loss = 0.00019619902013801038
iteration 292, loss = 0.00020527903689071536
iteration 293, loss = 0.0005669684032909572
iteration 294, loss = 0.00020702759502455592
iteration 295, loss = 0.00020659653819166124
iteration 296, loss = 0.000198864727281034
iteration 297, loss = 0.00036102411104366183
iteration 298, loss = 0.00018159757019020617
iteration 299, loss = 0.00019563647219911218

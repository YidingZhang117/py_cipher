iteration 1, loss = 2.290634870529175
iteration 2, loss = 2.2840254306793213
iteration 3, loss = 2.3141727447509766
iteration 4, loss = 2.32658052444458
iteration 5, loss = 2.3154184818267822
iteration 6, loss = 2.102741241455078
iteration 7, loss = 2.196686267852783
iteration 8, loss = 2.15097975730896
iteration 9, loss = 2.040921688079834
iteration 10, loss = 2.036557197570801
iteration 11, loss = 1.953214168548584
iteration 12, loss = 1.9172661304473877
iteration 13, loss = 1.8645427227020264
iteration 14, loss = 1.7737064361572266
iteration 15, loss = 1.789149522781372
iteration 16, loss = 1.6147466897964478
iteration 17, loss = 1.6559292078018188
iteration 18, loss = 1.6430621147155762
iteration 19, loss = 1.5774258375167847
iteration 20, loss = 1.4925457239151
iteration 21, loss = 1.3879362344741821
iteration 22, loss = 1.4290316104888916
iteration 23, loss = 1.38400137424469
iteration 24, loss = 1.239348292350769
iteration 25, loss = 1.1873195171356201
iteration 26, loss = 1.1652469635009766
iteration 27, loss = 1.1045037508010864
iteration 28, loss = 1.1754337549209595
iteration 29, loss = 0.9480388760566711
iteration 30, loss = 1.1773117780685425
iteration 31, loss = 0.9843696355819702
iteration 32, loss = 1.0373971462249756
iteration 33, loss = 0.9939092397689819
iteration 34, loss = 0.9547052979469299
iteration 35, loss = 0.9218661785125732
iteration 36, loss = 0.9427382946014404
iteration 37, loss = 0.9041482210159302
iteration 38, loss = 0.8409887552261353
iteration 39, loss = 0.8400450348854065
iteration 40, loss = 0.8518778085708618
iteration 41, loss = 0.7160824537277222
iteration 42, loss = 0.7971802353858948
iteration 43, loss = 0.7994003295898438
iteration 44, loss = 0.8099754452705383
iteration 45, loss = 0.9058078527450562
iteration 46, loss = 0.8259881734848022
iteration 47, loss = 0.7811578512191772
iteration 48, loss = 0.7342875599861145
iteration 49, loss = 0.7325998544692993
iteration 50, loss = 0.8171238303184509
iteration 51, loss = 0.6789070963859558
iteration 52, loss = 0.8374165296554565
iteration 53, loss = 0.7570751905441284
iteration 54, loss = 0.8132268190383911
iteration 55, loss = 0.720066249370575
iteration 56, loss = 0.7529457807540894
iteration 57, loss = 0.6849601864814758
iteration 58, loss = 0.7055790424346924
iteration 59, loss = 0.7186733484268188
iteration 60, loss = 0.7409589290618896
iteration 61, loss = 0.7581251859664917
iteration 62, loss = 0.6644342541694641
iteration 63, loss = 0.6906659007072449
iteration 64, loss = 0.6558862924575806
iteration 65, loss = 0.7111461758613586
iteration 66, loss = 0.7174416184425354
iteration 67, loss = 0.70212721824646
iteration 68, loss = 0.6488369703292847
iteration 69, loss = 0.6670998930931091
iteration 70, loss = 0.6872276663780212
iteration 71, loss = 0.7165864109992981
iteration 72, loss = 0.6479116678237915
iteration 73, loss = 0.6346533894538879
iteration 74, loss = 0.6367208361625671
iteration 75, loss = 0.6656769514083862
iteration 76, loss = 0.7219645977020264
iteration 77, loss = 0.6705096960067749
iteration 78, loss = 0.7283577919006348
iteration 79, loss = 0.7263579964637756
iteration 80, loss = 0.6668261885643005
iteration 81, loss = 0.6575053930282593
iteration 82, loss = 0.6226513981819153
iteration 83, loss = 0.6497719883918762
iteration 84, loss = 0.6253790855407715
iteration 85, loss = 0.6228715777397156
iteration 86, loss = 0.6303149461746216
iteration 87, loss = 0.6300556063652039
iteration 88, loss = 0.6470222473144531
iteration 89, loss = 0.6740878224372864
iteration 90, loss = 0.6688830256462097
iteration 91, loss = 0.6385542154312134
iteration 92, loss = 0.6211652755737305
iteration 93, loss = 0.6405192017555237
iteration 94, loss = 0.615494430065155
iteration 95, loss = 0.6731826066970825
iteration 96, loss = 0.6903173327445984
iteration 97, loss = 0.6083137392997742
iteration 98, loss = 0.6417999863624573
iteration 99, loss = 0.6969361901283264
iteration 100, loss = 0.66837078332901
iteration 101, loss = 0.6382319331169128
iteration 102, loss = 0.6375266313552856
iteration 103, loss = 0.6320383548736572
iteration 104, loss = 0.6298748254776001
iteration 105, loss = 0.6383308172225952
iteration 106, loss = 0.6470754146575928
iteration 107, loss = 0.6375495791435242
iteration 108, loss = 0.7010337710380554
iteration 109, loss = 0.6225549578666687
iteration 110, loss = 0.6277093291282654
iteration 111, loss = 0.6207662224769592
iteration 112, loss = 0.6024487614631653
iteration 113, loss = 0.618820309638977
iteration 114, loss = 0.6349489092826843
iteration 115, loss = 0.6350307464599609
iteration 116, loss = 0.6179401874542236
iteration 117, loss = 0.6704926490783691
iteration 118, loss = 0.610395610332489
iteration 119, loss = 0.5771698951721191
iteration 120, loss = 0.6352491974830627
iteration 121, loss = 0.6192623972892761
iteration 122, loss = 0.6479530334472656
iteration 123, loss = 0.629906415939331
iteration 124, loss = 0.6319082379341125
iteration 125, loss = 0.5989803075790405
iteration 126, loss = 0.6325697898864746
iteration 127, loss = 0.6379536986351013
iteration 128, loss = 0.6114982962608337
iteration 129, loss = 0.6207706332206726
iteration 130, loss = 0.6157673597335815
iteration 131, loss = 0.5997172594070435
iteration 132, loss = 0.6132306456565857
iteration 133, loss = 0.6170374751091003
iteration 134, loss = 0.6127240061759949
iteration 135, loss = 0.5920588374137878
iteration 136, loss = 0.6229615211486816
iteration 137, loss = 0.6639255285263062
iteration 138, loss = 0.5963788032531738
iteration 139, loss = 0.6429245471954346
iteration 140, loss = 0.6060203909873962
iteration 141, loss = 0.6238628029823303
iteration 142, loss = 0.665023684501648
iteration 143, loss = 0.5567193627357483
iteration 144, loss = 0.6375775337219238
iteration 145, loss = 0.5841538906097412
iteration 146, loss = 0.6393350958824158
iteration 147, loss = 0.5885850191116333
iteration 148, loss = 0.5992195010185242
iteration 149, loss = 0.5817884802818298
iteration 150, loss = 0.5917092561721802
iteration 151, loss = 0.5925717353820801
iteration 152, loss = 0.6149927973747253
iteration 153, loss = 0.5783296227455139
iteration 154, loss = 0.5604931116104126
iteration 155, loss = 0.5959646105766296
iteration 156, loss = 0.6221024394035339
iteration 157, loss = 0.5884107351303101
iteration 158, loss = 0.5660658478736877
iteration 159, loss = 0.6167115569114685
iteration 160, loss = 0.5815297365188599
iteration 161, loss = 0.5953651666641235
iteration 162, loss = 0.6048076152801514
iteration 163, loss = 0.6059010624885559
iteration 164, loss = 0.571578860282898
iteration 165, loss = 0.5657355785369873
iteration 166, loss = 0.6450994610786438
iteration 167, loss = 0.5952942967414856
iteration 168, loss = 0.580306887626648
iteration 169, loss = 0.558786153793335
iteration 170, loss = 0.5717713832855225
iteration 171, loss = 0.5826713442802429
iteration 172, loss = 0.5634230971336365
iteration 173, loss = 0.5642662048339844
iteration 174, loss = 0.5418661832809448
iteration 175, loss = 0.5794102549552917
iteration 176, loss = 0.5230713486671448
iteration 177, loss = 0.623467743396759
iteration 178, loss = 0.6350646615028381
iteration 179, loss = 0.610751211643219
iteration 180, loss = 0.5426327586174011
iteration 181, loss = 0.604319155216217
iteration 182, loss = 0.5425958037376404
iteration 183, loss = 0.5899854302406311
iteration 184, loss = 0.5468303561210632
iteration 185, loss = 0.5808650255203247
iteration 186, loss = 0.5841862559318542
iteration 187, loss = 0.5489033460617065
iteration 188, loss = 0.5491872429847717
iteration 189, loss = 0.5751460194587708
iteration 190, loss = 0.5183722376823425
iteration 191, loss = 0.5971027612686157
iteration 192, loss = 0.5669744610786438
iteration 193, loss = 0.5614228844642639
iteration 194, loss = 0.5255176424980164
iteration 195, loss = 0.59001225233078
iteration 196, loss = 0.542599081993103
iteration 197, loss = 0.5814904570579529
iteration 198, loss = 0.5351133942604065
iteration 199, loss = 0.5636682510375977
iteration 200, loss = 0.5331707000732422
iteration 201, loss = 0.5291402339935303
iteration 202, loss = 0.5371408462524414
iteration 203, loss = 0.6283940076828003
iteration 204, loss = 0.5666864514350891
iteration 205, loss = 0.5500017404556274
iteration 206, loss = 0.5939847230911255
iteration 207, loss = 0.5581625699996948
iteration 208, loss = 0.522045373916626
iteration 209, loss = 0.6094087958335876
iteration 210, loss = 0.5718848705291748
iteration 211, loss = 0.5609968900680542
iteration 212, loss = 0.5559197068214417
iteration 213, loss = 0.5487352013587952
iteration 214, loss = 0.5669344663619995
iteration 215, loss = 0.506733775138855
iteration 216, loss = 0.5147393941879272
iteration 217, loss = 0.5507322549819946
iteration 218, loss = 0.5594218373298645
iteration 219, loss = 0.5221654772758484
iteration 220, loss = 0.5095226764678955
iteration 221, loss = 0.5282654166221619
iteration 222, loss = 0.5435495972633362
iteration 223, loss = 0.4978223145008087
iteration 224, loss = 0.5212754011154175
iteration 225, loss = 0.516671895980835
iteration 226, loss = 0.5167548656463623
iteration 227, loss = 0.546367883682251
iteration 228, loss = 0.5175947546958923
iteration 229, loss = 0.5294966101646423
iteration 230, loss = 0.4901958405971527
iteration 231, loss = 0.49687430262565613
iteration 232, loss = 0.4848853349685669
iteration 233, loss = 0.4980955719947815
iteration 234, loss = 0.5398120284080505
iteration 235, loss = 0.48257431387901306
iteration 236, loss = 0.5702695846557617
iteration 237, loss = 0.4788934588432312
iteration 238, loss = 0.5962863564491272
iteration 239, loss = 0.5305488109588623
iteration 240, loss = 0.5340436697006226
iteration 241, loss = 0.5754607915878296
iteration 242, loss = 0.5096962451934814
iteration 243, loss = 0.5046097636222839
iteration 244, loss = 0.53564453125
iteration 245, loss = 0.5225597023963928
iteration 246, loss = 0.5086052417755127
iteration 247, loss = 0.4872126877307892
iteration 248, loss = 0.5139399170875549
iteration 249, loss = 0.46694043278694153
iteration 250, loss = 0.5082255005836487
iteration 251, loss = 0.518394947052002
iteration 252, loss = 0.5403768420219421
iteration 253, loss = 0.44874581694602966
iteration 254, loss = 0.5166547298431396
iteration 255, loss = 0.5098342299461365
iteration 256, loss = 0.4689599275588989
iteration 257, loss = 0.5455254912376404
iteration 258, loss = 0.4735438823699951
iteration 259, loss = 0.48610758781433105
iteration 260, loss = 0.4743008613586426
iteration 261, loss = 0.4449961483478546
iteration 262, loss = 0.5044535398483276
iteration 263, loss = 0.5580494403839111
iteration 264, loss = 0.4661157727241516
iteration 265, loss = 0.47451356053352356
iteration 266, loss = 0.46497076749801636
iteration 267, loss = 0.5392501354217529
iteration 268, loss = 0.4524820148944855
iteration 269, loss = 0.48815226554870605
iteration 270, loss = 0.494644433259964
iteration 271, loss = 0.4924825131893158
iteration 272, loss = 0.5310075879096985
iteration 273, loss = 0.504368007183075
iteration 274, loss = 0.46353283524513245
iteration 275, loss = 0.5145505666732788
iteration 276, loss = 0.41990482807159424
iteration 277, loss = 0.4555188715457916
iteration 278, loss = 0.4879646301269531
iteration 279, loss = 0.47181278467178345
iteration 280, loss = 0.4713609516620636
iteration 281, loss = 0.45018690824508667
iteration 282, loss = 0.5405277609825134
iteration 283, loss = 0.45164138078689575
iteration 284, loss = 0.4444466233253479
iteration 285, loss = 0.5208234786987305
iteration 286, loss = 0.4489539563655853
iteration 287, loss = 0.5101425051689148
iteration 288, loss = 0.46095868945121765
iteration 289, loss = 0.5009873509407043
iteration 290, loss = 0.5217809677124023
iteration 291, loss = 0.44448015093803406
iteration 292, loss = 0.43798431754112244
iteration 293, loss = 0.5331345200538635
iteration 294, loss = 0.4630272686481476
iteration 295, loss = 0.43587127327919006
iteration 296, loss = 0.4908403158187866
iteration 297, loss = 0.45687755942344666
iteration 298, loss = 0.4429517388343811
iteration 299, loss = 0.42941978573799133
iteration 300, loss = 0.41283267736434937
iteration 1, loss = 0.4266040027141571
iteration 2, loss = 0.4015440046787262
iteration 3, loss = 0.4766309857368469
iteration 4, loss = 0.4067555367946625
iteration 5, loss = 0.42625102400779724
iteration 6, loss = 0.42130881547927856
iteration 7, loss = 0.43222030997276306
iteration 8, loss = 0.44054126739501953
iteration 9, loss = 0.446758896112442
iteration 10, loss = 0.3983381390571594
iteration 11, loss = 0.4213370680809021
iteration 12, loss = 0.4385014772415161
iteration 13, loss = 0.40656211972236633
iteration 14, loss = 0.387287974357605
iteration 15, loss = 0.40605494379997253
iteration 16, loss = 0.5040609836578369
iteration 17, loss = 0.47395044565200806
iteration 18, loss = 0.47563499212265015
iteration 19, loss = 0.4289117753505707
iteration 20, loss = 0.44014716148376465
iteration 21, loss = 0.4063750207424164
iteration 22, loss = 0.41894909739494324
iteration 23, loss = 0.4450962543487549
iteration 24, loss = 0.4119934141635895
iteration 25, loss = 0.42613691091537476
iteration 26, loss = 0.4225156307220459
iteration 27, loss = 0.41973254084587097
iteration 28, loss = 0.4433406591415405
iteration 29, loss = 0.42415890097618103
iteration 30, loss = 0.4635189473628998
iteration 31, loss = 0.38096562027931213
iteration 32, loss = 0.3824183940887451
iteration 33, loss = 0.4445114731788635
iteration 34, loss = 0.3920372426509857
iteration 35, loss = 0.44341176748275757
iteration 36, loss = 0.4042402505874634
iteration 37, loss = 0.3760433793067932
iteration 38, loss = 0.38135257363319397
iteration 39, loss = 0.3918555974960327
iteration 40, loss = 0.4006747305393219
iteration 41, loss = 0.38335689902305603
iteration 42, loss = 0.47870108485221863
iteration 43, loss = 0.4140540361404419
iteration 44, loss = 0.47057732939720154
iteration 45, loss = 0.3913252651691437
iteration 46, loss = 0.365224689245224
iteration 47, loss = 0.4563629627227783
iteration 48, loss = 0.4157700538635254
iteration 49, loss = 0.45467665791511536
iteration 50, loss = 0.3788204789161682
iteration 51, loss = 0.37453392148017883
iteration 52, loss = 0.3890836834907532
iteration 53, loss = 0.40947768092155457
iteration 54, loss = 0.3990660607814789
iteration 55, loss = 0.4267270863056183
iteration 56, loss = 0.3664848804473877
iteration 57, loss = 0.41479167342185974
iteration 58, loss = 0.3847498893737793
iteration 59, loss = 0.3660448491573334
iteration 60, loss = 0.3973325490951538
iteration 61, loss = 0.38266870379447937
iteration 62, loss = 0.3903481066226959
iteration 63, loss = 0.3898252248764038
iteration 64, loss = 0.4258069097995758
iteration 65, loss = 0.423353374004364
iteration 66, loss = 0.38541746139526367
iteration 67, loss = 0.3683759570121765
iteration 68, loss = 0.40276578068733215
iteration 69, loss = 0.3662183880805969
iteration 70, loss = 0.36444705724716187
iteration 71, loss = 0.3661329746246338
iteration 72, loss = 0.4038669168949127
iteration 73, loss = 0.324345201253891
iteration 74, loss = 0.3920038938522339
iteration 75, loss = 0.37163376808166504
iteration 76, loss = 0.3741888701915741
iteration 77, loss = 0.3717493712902069
iteration 78, loss = 0.336844265460968
iteration 79, loss = 0.39803510904312134
iteration 80, loss = 0.34819456934928894
iteration 81, loss = 0.3974516987800598
iteration 82, loss = 0.45867031812667847
iteration 83, loss = 0.4036386013031006
iteration 84, loss = 0.4066367745399475
iteration 85, loss = 0.34080785512924194
iteration 86, loss = 0.452502578496933
iteration 87, loss = 0.40468019247055054
iteration 88, loss = 0.36935728788375854
iteration 89, loss = 0.35797157883644104
iteration 90, loss = 0.3777819275856018
iteration 91, loss = 0.34681758284568787
iteration 92, loss = 0.3565429151058197
iteration 93, loss = 0.3204787075519562
iteration 94, loss = 0.3435288965702057
iteration 95, loss = 0.3250684440135956
iteration 96, loss = 0.3409268260002136
iteration 97, loss = 0.31101298332214355
iteration 98, loss = 0.322697252035141
iteration 99, loss = 0.32787442207336426
iteration 100, loss = 0.336122989654541
iteration 101, loss = 0.3471609354019165
iteration 102, loss = 0.35163265466690063
iteration 103, loss = 0.34656986594200134
iteration 104, loss = 0.33069726824760437
iteration 105, loss = 0.32412368059158325
iteration 106, loss = 0.34573182463645935
iteration 107, loss = 0.3242892324924469
iteration 108, loss = 0.3448522090911865
iteration 109, loss = 0.32256171107292175
iteration 110, loss = 0.3648306727409363
iteration 111, loss = 0.4043113589286804
iteration 112, loss = 0.35698774456977844
iteration 113, loss = 0.3779214024543762
iteration 114, loss = 0.4271463453769684
iteration 115, loss = 0.3013547360897064
iteration 116, loss = 0.36867260932922363
iteration 117, loss = 0.29807761311531067
iteration 118, loss = 0.3724362254142761
iteration 119, loss = 0.31166425347328186
iteration 120, loss = 0.3161235451698303
iteration 121, loss = 0.36677563190460205
iteration 122, loss = 0.3763003349304199
iteration 123, loss = 0.38220909237861633
iteration 124, loss = 0.3273093104362488
iteration 125, loss = 0.32008984684944153
iteration 126, loss = 0.3701820969581604
iteration 127, loss = 0.315605491399765
iteration 128, loss = 0.3098674416542053
iteration 129, loss = 0.3083464205265045
iteration 130, loss = 0.332974374294281
iteration 131, loss = 0.3343580663204193
iteration 132, loss = 0.29470372200012207
iteration 133, loss = 0.29636505246162415
iteration 134, loss = 0.374270498752594
iteration 135, loss = 0.29559949040412903
iteration 136, loss = 0.29127320647239685
iteration 137, loss = 0.2865244150161743
iteration 138, loss = 0.3607324957847595
iteration 139, loss = 0.3238682746887207
iteration 140, loss = 0.3241020739078522
iteration 141, loss = 0.31741729378700256
iteration 142, loss = 0.3270796835422516
iteration 143, loss = 0.279476523399353
iteration 144, loss = 0.2868826687335968
iteration 145, loss = 0.2760865390300751
iteration 146, loss = 0.29361477494239807
iteration 147, loss = 0.2792220413684845
iteration 148, loss = 0.29632794857025146
iteration 149, loss = 0.3160317540168762
iteration 150, loss = 0.31425830721855164
iteration 151, loss = 0.2872178554534912
iteration 152, loss = 0.3078239858150482
iteration 153, loss = 0.2783503234386444
iteration 154, loss = 0.3973054587841034
iteration 155, loss = 0.36279192566871643
iteration 156, loss = 0.2885395884513855
iteration 157, loss = 0.2711602449417114
iteration 158, loss = 0.2642902731895447
iteration 159, loss = 0.34452390670776367
iteration 160, loss = 0.31133201718330383
iteration 161, loss = 0.3477475643157959
iteration 162, loss = 0.3477809429168701
iteration 163, loss = 0.278755247592926
iteration 164, loss = 0.2772963345050812
iteration 165, loss = 0.2791629731655121
iteration 166, loss = 0.29471728205680847
iteration 167, loss = 0.2979963421821594
iteration 168, loss = 0.2608562111854553
iteration 169, loss = 0.2805217504501343
iteration 170, loss = 0.2927612364292145
iteration 171, loss = 0.2592635154724121
iteration 172, loss = 0.2982056438922882
iteration 173, loss = 0.25628915429115295
iteration 174, loss = 0.26834636926651
iteration 175, loss = 0.27472326159477234
iteration 176, loss = 0.29455363750457764
iteration 177, loss = 0.2520582973957062
iteration 178, loss = 0.2842964231967926
iteration 179, loss = 0.2528345286846161
iteration 180, loss = 0.2837740182876587
iteration 181, loss = 0.30370032787323
iteration 182, loss = 0.27591773867607117
iteration 183, loss = 0.2963206470012665
iteration 184, loss = 0.32139676809310913
iteration 185, loss = 0.30480408668518066
iteration 186, loss = 0.32201656699180603
iteration 187, loss = 0.2573365867137909
iteration 188, loss = 0.2522185742855072
iteration 189, loss = 0.2916369140148163
iteration 190, loss = 0.23674839735031128
iteration 191, loss = 0.23971712589263916
iteration 192, loss = 0.24849139153957367
iteration 193, loss = 0.235622376203537
iteration 194, loss = 0.2914109230041504
iteration 195, loss = 0.25938937067985535
iteration 196, loss = 0.2684582471847534
iteration 197, loss = 0.278941810131073
iteration 198, loss = 0.22778210043907166
iteration 199, loss = 0.25368058681488037
iteration 200, loss = 0.2587212324142456
iteration 201, loss = 0.2587422728538513
iteration 202, loss = 0.24003303050994873
iteration 203, loss = 0.23466584086418152
iteration 204, loss = 0.24591073393821716
iteration 205, loss = 0.24773156642913818
iteration 206, loss = 0.22550034523010254
iteration 207, loss = 0.22957934439182281
iteration 208, loss = 0.3067260682582855
iteration 209, loss = 0.24088267982006073
iteration 210, loss = 0.26633989810943604
iteration 211, loss = 0.2711249589920044
iteration 212, loss = 0.30654168128967285
iteration 213, loss = 0.2700522243976593
iteration 214, loss = 0.2536364197731018
iteration 215, loss = 0.24460048973560333
iteration 216, loss = 0.23400264978408813
iteration 217, loss = 0.29158350825309753
iteration 218, loss = 0.3276057839393616
iteration 219, loss = 0.2222001999616623
iteration 220, loss = 0.24816656112670898
iteration 221, loss = 0.2291509211063385
iteration 222, loss = 0.3303646445274353
iteration 223, loss = 0.21758994460105896
iteration 224, loss = 0.2913549244403839
iteration 225, loss = 0.23427343368530273
iteration 226, loss = 0.2752159535884857
iteration 227, loss = 0.2155168652534485
iteration 228, loss = 0.2686932682991028
iteration 229, loss = 0.289015531539917
iteration 230, loss = 0.20328696072101593
iteration 231, loss = 0.2616024315357208
iteration 232, loss = 0.2640019953250885
iteration 233, loss = 0.2076832801103592
iteration 234, loss = 0.25248557329177856
iteration 235, loss = 0.24400922656059265
iteration 236, loss = 0.26159167289733887
iteration 237, loss = 0.2585579752922058
iteration 238, loss = 0.21661917865276337
iteration 239, loss = 0.21072345972061157
iteration 240, loss = 0.21412533521652222
iteration 241, loss = 0.22805707156658173
iteration 242, loss = 0.2363399863243103
iteration 243, loss = 0.2512996792793274
iteration 244, loss = 0.2308555692434311
iteration 245, loss = 0.21667277812957764
iteration 246, loss = 0.26333141326904297
iteration 247, loss = 0.21545052528381348
iteration 248, loss = 0.2184969186782837
iteration 249, loss = 0.1999572515487671
iteration 250, loss = 0.22130681574344635
iteration 251, loss = 0.2075112760066986
iteration 252, loss = 0.2491147369146347
iteration 253, loss = 0.20389139652252197
iteration 254, loss = 0.21374240517616272
iteration 255, loss = 0.2770001292228699
iteration 256, loss = 0.23511193692684174
iteration 257, loss = 0.2159489393234253
iteration 258, loss = 0.31788989901542664
iteration 259, loss = 0.2509452700614929
iteration 260, loss = 0.23184850811958313
iteration 261, loss = 0.2667354643344879
iteration 262, loss = 0.22052370011806488
iteration 263, loss = 0.20088881254196167
iteration 264, loss = 0.1993766874074936
iteration 265, loss = 0.1998635232448578
iteration 266, loss = 0.2184787094593048
iteration 267, loss = 0.21128416061401367
iteration 268, loss = 0.19465333223342896
iteration 269, loss = 0.20928792655467987
iteration 270, loss = 0.18307946622371674
iteration 271, loss = 0.21384768187999725
iteration 272, loss = 0.19827166199684143
iteration 273, loss = 0.23291707038879395
iteration 274, loss = 0.25070932507514954
iteration 275, loss = 0.2036767452955246
iteration 276, loss = 0.2492343634366989
iteration 277, loss = 0.1940523087978363
iteration 278, loss = 0.21261030435562134
iteration 279, loss = 0.1861463338136673
iteration 280, loss = 0.20450815558433533
iteration 281, loss = 0.19712962210178375
iteration 282, loss = 0.2178935408592224
iteration 283, loss = 0.21186839044094086
iteration 284, loss = 0.29530686140060425
iteration 285, loss = 0.19685110449790955
iteration 286, loss = 0.19500139355659485
iteration 287, loss = 0.18954236805438995
iteration 288, loss = 0.20954576134681702
iteration 289, loss = 0.18134768307209015
iteration 290, loss = 0.23445306718349457
iteration 291, loss = 0.1857103407382965
iteration 292, loss = 0.17024384438991547
iteration 293, loss = 0.21924099326133728
iteration 294, loss = 0.18960341811180115
iteration 295, loss = 0.20270390808582306
iteration 296, loss = 0.20013496279716492
iteration 297, loss = 0.23766182363033295
iteration 298, loss = 0.22574801743030548
iteration 299, loss = 0.24646338820457458
iteration 300, loss = 0.19122552871704102
iteration 1, loss = 0.22073949873447418
iteration 2, loss = 0.16238583624362946
iteration 3, loss = 0.24382524192333221
iteration 4, loss = 0.19274523854255676
iteration 5, loss = 0.20802082121372223
iteration 6, loss = 0.19738386571407318
iteration 7, loss = 0.18859516084194183
iteration 8, loss = 0.2270040214061737
iteration 9, loss = 0.21005743741989136
iteration 10, loss = 0.21030795574188232
iteration 11, loss = 0.18299883604049683
iteration 12, loss = 0.19530582427978516
iteration 13, loss = 0.17584794759750366
iteration 14, loss = 0.19524243474006653
iteration 15, loss = 0.1798141449689865
iteration 16, loss = 0.16987282037734985
iteration 17, loss = 0.19536763429641724
iteration 18, loss = 0.17255939543247223
iteration 19, loss = 0.20166267454624176
iteration 20, loss = 0.18569520115852356
iteration 21, loss = 0.15608851611614227
iteration 22, loss = 0.23618517816066742
iteration 23, loss = 0.186490997672081
iteration 24, loss = 0.20374324917793274
iteration 25, loss = 0.17280615866184235
iteration 26, loss = 0.15610583126544952
iteration 27, loss = 0.17257677018642426
iteration 28, loss = 0.19545817375183105
iteration 29, loss = 0.1717880368232727
iteration 30, loss = 0.17296840250492096
iteration 31, loss = 0.1644650548696518
iteration 32, loss = 0.17863225936889648
iteration 33, loss = 0.16109886765480042
iteration 34, loss = 0.15378564596176147
iteration 35, loss = 0.16974247992038727
iteration 36, loss = 0.1468704342842102
iteration 37, loss = 0.17414452135562897
iteration 38, loss = 0.22759003937244415
iteration 39, loss = 0.15358160436153412
iteration 40, loss = 0.1549360454082489
iteration 41, loss = 0.22529685497283936
iteration 42, loss = 0.20268456637859344
iteration 43, loss = 0.14824557304382324
iteration 44, loss = 0.16788780689239502
iteration 45, loss = 0.15797337889671326
iteration 46, loss = 0.17333775758743286
iteration 47, loss = 0.2183748036623001
iteration 48, loss = 0.19312596321105957
iteration 49, loss = 0.1978076845407486
iteration 50, loss = 0.16782253980636597
iteration 51, loss = 0.16030333936214447
iteration 52, loss = 0.15519864857196808
iteration 53, loss = 0.1626979261636734
iteration 54, loss = 0.15074662864208221
iteration 55, loss = 0.15974007546901703
iteration 56, loss = 0.13567857444286346
iteration 57, loss = 0.2002463936805725
iteration 58, loss = 0.15354391932487488
iteration 59, loss = 0.14875636994838715
iteration 60, loss = 0.16122391819953918
iteration 61, loss = 0.20982378721237183
iteration 62, loss = 0.14667345583438873
iteration 63, loss = 0.14686812460422516
iteration 64, loss = 0.1577650010585785
iteration 65, loss = 0.14243075251579285
iteration 66, loss = 0.2124561071395874
iteration 67, loss = 0.15778234601020813
iteration 68, loss = 0.1844925433397293
iteration 69, loss = 0.13822220265865326
iteration 70, loss = 0.20310840010643005
iteration 71, loss = 0.16088449954986572
iteration 72, loss = 0.13755542039871216
iteration 73, loss = 0.19019369781017303
iteration 74, loss = 0.14701153337955475
iteration 75, loss = 0.21728065609931946
iteration 76, loss = 0.1320432722568512
iteration 77, loss = 0.14890342950820923
iteration 78, loss = 0.15165813267230988
iteration 79, loss = 0.17310424149036407
iteration 80, loss = 0.15766294300556183
iteration 81, loss = 0.14275124669075012
iteration 82, loss = 0.14513170719146729
iteration 83, loss = 0.13478891551494598
iteration 84, loss = 0.18761767446994781
iteration 85, loss = 0.1353873908519745
iteration 86, loss = 0.1743921935558319
iteration 87, loss = 0.13443833589553833
iteration 88, loss = 0.15035448968410492
iteration 89, loss = 0.1692987084388733
iteration 90, loss = 0.13890895247459412
iteration 91, loss = 0.136088028550148
iteration 92, loss = 0.13602426648139954
iteration 93, loss = 0.16243910789489746
iteration 94, loss = 0.1227136179804802
iteration 95, loss = 0.14291945099830627
iteration 96, loss = 0.20090286433696747
iteration 97, loss = 0.1591682732105255
iteration 98, loss = 0.13439957797527313
iteration 99, loss = 0.13876503705978394
iteration 100, loss = 0.13717211782932281
iteration 101, loss = 0.16974304616451263
iteration 102, loss = 0.1721917986869812
iteration 103, loss = 0.12958195805549622
iteration 104, loss = 0.17122912406921387
iteration 105, loss = 0.14749479293823242
iteration 106, loss = 0.13122297823429108
iteration 107, loss = 0.13942888379096985
iteration 108, loss = 0.15682528913021088
iteration 109, loss = 0.12413456290960312
iteration 110, loss = 0.1632450520992279
iteration 111, loss = 0.12537194788455963
iteration 112, loss = 0.12812930345535278
iteration 113, loss = 0.164438396692276
iteration 114, loss = 0.1280045211315155
iteration 115, loss = 0.13633795082569122
iteration 116, loss = 0.13305453956127167
iteration 117, loss = 0.1313459277153015
iteration 118, loss = 0.1914556920528412
iteration 119, loss = 0.14589981734752655
iteration 120, loss = 0.1259547472000122
iteration 121, loss = 0.12360329180955887
iteration 122, loss = 0.12257450073957443
iteration 123, loss = 0.12462253868579865
iteration 124, loss = 0.1284702718257904
iteration 125, loss = 0.1680435985326767
iteration 126, loss = 0.1557900458574295
iteration 127, loss = 0.1496579945087433
iteration 128, loss = 0.1272633671760559
iteration 129, loss = 0.11513228714466095
iteration 130, loss = 0.1293640285730362
iteration 131, loss = 0.1418522745370865
iteration 132, loss = 0.13032102584838867
iteration 133, loss = 0.14281319081783295
iteration 134, loss = 0.12745827436447144
iteration 135, loss = 0.13489170372486115
iteration 136, loss = 0.11234192550182343
iteration 137, loss = 0.15530191361904144
iteration 138, loss = 0.17626799643039703
iteration 139, loss = 0.16495804488658905
iteration 140, loss = 0.11356266587972641
iteration 141, loss = 0.17198286950588226
iteration 142, loss = 0.12449130415916443
iteration 143, loss = 0.12435567378997803
iteration 144, loss = 0.12873871624469757
iteration 145, loss = 0.14783203601837158
iteration 146, loss = 0.11879562586545944
iteration 147, loss = 0.14691545069217682
iteration 148, loss = 0.1552557796239853
iteration 149, loss = 0.11830580234527588
iteration 150, loss = 0.11870206892490387
iteration 151, loss = 0.13329869508743286
iteration 152, loss = 0.11343275010585785
iteration 153, loss = 0.12662972509860992
iteration 154, loss = 0.12461773306131363
iteration 155, loss = 0.13340303301811218
iteration 156, loss = 0.13208544254302979
iteration 157, loss = 0.12859652936458588
iteration 158, loss = 0.11727134138345718
iteration 159, loss = 0.13577046990394592
iteration 160, loss = 0.11923997104167938
iteration 161, loss = 0.1067017912864685
iteration 162, loss = 0.11567443609237671
iteration 163, loss = 0.11864092200994492
iteration 164, loss = 0.1120009794831276
iteration 165, loss = 0.11096436530351639
iteration 166, loss = 0.13722968101501465
iteration 167, loss = 0.1224149763584137
iteration 168, loss = 0.11129520833492279
iteration 169, loss = 0.11217767745256424
iteration 170, loss = 0.16439369320869446
iteration 171, loss = 0.11105135083198547
iteration 172, loss = 0.11058717221021652
iteration 173, loss = 0.1465056985616684
iteration 174, loss = 0.16574516892433167
iteration 175, loss = 0.12134990096092224
iteration 176, loss = 0.12346332520246506
iteration 177, loss = 0.11552800983190536
iteration 178, loss = 0.11009876430034637
iteration 179, loss = 0.14805245399475098
iteration 180, loss = 0.11269132047891617
iteration 181, loss = 0.11863240599632263
iteration 182, loss = 0.1743002086877823
iteration 183, loss = 0.1154695525765419
iteration 184, loss = 0.14301395416259766
iteration 185, loss = 0.12480219453573227
iteration 186, loss = 0.1301463544368744
iteration 187, loss = 0.10679511725902557
iteration 188, loss = 0.12057207524776459
iteration 189, loss = 0.11062610149383545
iteration 190, loss = 0.10159370303153992
iteration 191, loss = 0.14401616156101227
iteration 192, loss = 0.11198735237121582
iteration 193, loss = 0.1072918027639389
iteration 194, loss = 0.11950719356536865
iteration 195, loss = 0.09782549738883972
iteration 196, loss = 0.14712029695510864
iteration 197, loss = 0.10260802507400513
iteration 198, loss = 0.10423470288515091
iteration 199, loss = 0.12776502966880798
iteration 200, loss = 0.12737886607646942
iteration 201, loss = 0.11709501594305038
iteration 202, loss = 0.11054795980453491
iteration 203, loss = 0.111508809030056
iteration 204, loss = 0.13902826607227325
iteration 205, loss = 0.10276462882757187
iteration 206, loss = 0.1247345358133316
iteration 207, loss = 0.10261272639036179
iteration 208, loss = 0.09648185223340988
iteration 209, loss = 0.10640493780374527
iteration 210, loss = 0.12193948030471802
iteration 211, loss = 0.11519204080104828
iteration 212, loss = 0.12521342933177948
iteration 213, loss = 0.09988869726657867
iteration 214, loss = 0.10806364566087723
iteration 215, loss = 0.0990438312292099
iteration 216, loss = 0.1070428192615509
iteration 217, loss = 0.09276378899812698
iteration 218, loss = 0.09402433782815933
iteration 219, loss = 0.10341019928455353
iteration 220, loss = 0.10140356421470642
iteration 221, loss = 0.10500814020633698
iteration 222, loss = 0.10446327179670334
iteration 223, loss = 0.11364329606294632
iteration 224, loss = 0.09247574955224991
iteration 225, loss = 0.10900147259235382
iteration 226, loss = 0.13124842941761017
iteration 227, loss = 0.09606101363897324
iteration 228, loss = 0.1069084107875824
iteration 229, loss = 0.09840228408575058
iteration 230, loss = 0.14824816584587097
iteration 231, loss = 0.09119927138090134
iteration 232, loss = 0.10753855109214783
iteration 233, loss = 0.09965737164020538
iteration 234, loss = 0.10995129495859146
iteration 235, loss = 0.1287834644317627
iteration 236, loss = 0.09552942961454391
iteration 237, loss = 0.13418585062026978
iteration 238, loss = 0.09095250815153122
iteration 239, loss = 0.11274874955415726
iteration 240, loss = 0.10016779601573944
iteration 241, loss = 0.1012154370546341
iteration 242, loss = 0.09394577145576477
iteration 243, loss = 0.09410177171230316
iteration 244, loss = 0.09476971626281738
iteration 245, loss = 0.08734704554080963
iteration 246, loss = 0.10203077644109726
iteration 247, loss = 0.093187116086483
iteration 248, loss = 0.09077993035316467
iteration 249, loss = 0.08950193971395493
iteration 250, loss = 0.09321039170026779
iteration 251, loss = 0.09218600392341614
iteration 252, loss = 0.10374897718429565
iteration 253, loss = 0.09480950236320496
iteration 254, loss = 0.09724961221218109
iteration 255, loss = 0.10279784351587296
iteration 256, loss = 0.09009076654911041
iteration 257, loss = 0.09225276857614517
iteration 258, loss = 0.10487468540668488
iteration 259, loss = 0.08848024904727936
iteration 260, loss = 0.08943778276443481
iteration 261, loss = 0.11331713199615479
iteration 262, loss = 0.10431509464979172
iteration 263, loss = 0.0828186497092247
iteration 264, loss = 0.09766130149364471
iteration 265, loss = 0.09134087711572647
iteration 266, loss = 0.09204281866550446
iteration 267, loss = 0.08334771543741226
iteration 268, loss = 0.1012076810002327
iteration 269, loss = 0.09316378831863403
iteration 270, loss = 0.09686852991580963
iteration 271, loss = 0.0954880565404892
iteration 272, loss = 0.11094485968351364
iteration 273, loss = 0.09384341537952423
iteration 274, loss = 0.10567199438810349
iteration 275, loss = 0.13624463975429535
iteration 276, loss = 0.08563124388456345
iteration 277, loss = 0.0834634080529213
iteration 278, loss = 0.09110119193792343
iteration 279, loss = 0.0984564870595932
iteration 280, loss = 0.0929388776421547
iteration 281, loss = 0.08875591307878494
iteration 282, loss = 0.09255003929138184
iteration 283, loss = 0.08188050240278244
iteration 284, loss = 0.08344458043575287
iteration 285, loss = 0.08980483561754227
iteration 286, loss = 0.09340663999319077
iteration 287, loss = 0.10788320749998093
iteration 288, loss = 0.08160276710987091
iteration 289, loss = 0.07973513752222061
iteration 290, loss = 0.08583022654056549
iteration 291, loss = 0.08327838778495789
iteration 292, loss = 0.08339927345514297
iteration 293, loss = 0.10260855406522751
iteration 294, loss = 0.09137173742055893
iteration 295, loss = 0.07880657911300659
iteration 296, loss = 0.09874782711267471
iteration 297, loss = 0.08530446141958237
iteration 298, loss = 0.11620207130908966
iteration 299, loss = 0.09277637302875519
iteration 300, loss = 0.1008358746767044
iteration 1, loss = 0.08839240670204163
iteration 2, loss = 0.08643010258674622
iteration 3, loss = 0.07471022754907608
iteration 4, loss = 0.09049610048532486
iteration 5, loss = 0.09192974865436554
iteration 6, loss = 0.0823100134730339
iteration 7, loss = 0.07942351698875427
iteration 8, loss = 0.10102399438619614
iteration 9, loss = 0.10825566947460175
iteration 10, loss = 0.07558855414390564
iteration 11, loss = 0.09016318619251251
iteration 12, loss = 0.09410449117422104
iteration 13, loss = 0.09810314327478409
iteration 14, loss = 0.08050306886434555
iteration 15, loss = 0.09469131380319595
iteration 16, loss = 0.09045466035604477
iteration 17, loss = 0.07451333105564117
iteration 18, loss = 0.09765069931745529
iteration 19, loss = 0.08506840467453003
iteration 20, loss = 0.07445042580366135
iteration 21, loss = 0.07834137231111526
iteration 22, loss = 0.08125053346157074
iteration 23, loss = 0.07751133292913437
iteration 24, loss = 0.07977123558521271
iteration 25, loss = 0.07815085351467133
iteration 26, loss = 0.09969817101955414
iteration 27, loss = 0.07690851390361786
iteration 28, loss = 0.08120826631784439
iteration 29, loss = 0.08175326883792877
iteration 30, loss = 0.10431339591741562
iteration 31, loss = 0.09211171418428421
iteration 32, loss = 0.11310893297195435
iteration 33, loss = 0.07531998306512833
iteration 34, loss = 0.08323416113853455
iteration 35, loss = 0.07539958506822586
iteration 36, loss = 0.07410290092229843
iteration 37, loss = 0.07460859417915344
iteration 38, loss = 0.07734224200248718
iteration 39, loss = 0.07536730170249939
iteration 40, loss = 0.1047608032822609
iteration 41, loss = 0.07002701610326767
iteration 42, loss = 0.11873462796211243
iteration 43, loss = 0.07131652534008026
iteration 44, loss = 0.08506803214550018
iteration 45, loss = 0.07206445187330246
iteration 46, loss = 0.0792110487818718
iteration 47, loss = 0.07856868207454681
iteration 48, loss = 0.08002118021249771
iteration 49, loss = 0.07154261320829391
iteration 50, loss = 0.07573862373828888
iteration 51, loss = 0.07246945053339005
iteration 52, loss = 0.09415345638990402
iteration 53, loss = 0.08486603200435638
iteration 54, loss = 0.09764331579208374
iteration 55, loss = 0.07293568551540375
iteration 56, loss = 0.07012797892093658
iteration 57, loss = 0.07551246881484985
iteration 58, loss = 0.06792006641626358
iteration 59, loss = 0.06916559487581253
iteration 60, loss = 0.07017093896865845
iteration 61, loss = 0.07010552287101746
iteration 62, loss = 0.07232145220041275
iteration 63, loss = 0.07340668886899948
iteration 64, loss = 0.07021629810333252
iteration 65, loss = 0.07118374854326248
iteration 66, loss = 0.07198181003332138
iteration 67, loss = 0.06612084805965424
iteration 68, loss = 0.0793066918849945
iteration 69, loss = 0.07568994164466858
iteration 70, loss = 0.07690934091806412
iteration 71, loss = 0.06784292310476303
iteration 72, loss = 0.08114144951105118
iteration 73, loss = 0.0674845352768898
iteration 74, loss = 0.06896111369132996
iteration 75, loss = 0.07264815270900726
iteration 76, loss = 0.0838117003440857
iteration 77, loss = 0.07935042679309845
iteration 78, loss = 0.09236282855272293
iteration 79, loss = 0.06487063318490982
iteration 80, loss = 0.07096176594495773
iteration 81, loss = 0.08108744770288467
iteration 82, loss = 0.06580673158168793
iteration 83, loss = 0.06713540107011795
iteration 84, loss = 0.06729694455862045
iteration 85, loss = 0.07252538204193115
iteration 86, loss = 0.07424435764551163
iteration 87, loss = 0.0857173502445221
iteration 88, loss = 0.08944301307201385
iteration 89, loss = 0.066684789955616
iteration 90, loss = 0.09215628355741501
iteration 91, loss = 0.06960344314575195
iteration 92, loss = 0.06506596505641937
iteration 93, loss = 0.06964469701051712
iteration 94, loss = 0.06436237692832947
iteration 95, loss = 0.07314001023769379
iteration 96, loss = 0.09707201272249222
iteration 97, loss = 0.07402345538139343
iteration 98, loss = 0.07694277167320251
iteration 99, loss = 0.0703250989317894
iteration 100, loss = 0.0607917383313179
iteration 101, loss = 0.07056671380996704
iteration 102, loss = 0.06507904827594757
iteration 103, loss = 0.09916858375072479
iteration 104, loss = 0.0678618997335434
iteration 105, loss = 0.06481074541807175
iteration 106, loss = 0.06776603311300278
iteration 107, loss = 0.061563387513160706
iteration 108, loss = 0.06151137128472328
iteration 109, loss = 0.06275123357772827
iteration 110, loss = 0.06453445553779602
iteration 111, loss = 0.0858779102563858
iteration 112, loss = 0.09448796510696411
iteration 113, loss = 0.06751745194196701
iteration 114, loss = 0.06598148494958878
iteration 115, loss = 0.08027040213346481
iteration 116, loss = 0.06570928543806076
iteration 117, loss = 0.10597239434719086
iteration 118, loss = 0.07938485592603683
iteration 119, loss = 0.062126923352479935
iteration 120, loss = 0.0678984597325325
iteration 121, loss = 0.062163494527339935
iteration 122, loss = 0.08104006201028824
iteration 123, loss = 0.10302745550870895
iteration 124, loss = 0.060588955879211426
iteration 125, loss = 0.0621902272105217
iteration 126, loss = 0.08054113388061523
iteration 127, loss = 0.07060687243938446
iteration 128, loss = 0.05955693870782852
iteration 129, loss = 0.07716275751590729
iteration 130, loss = 0.063587486743927
iteration 131, loss = 0.05993371829390526
iteration 132, loss = 0.059351786971092224
iteration 133, loss = 0.06698127090930939
iteration 134, loss = 0.07979252934455872
iteration 135, loss = 0.0780896469950676
iteration 136, loss = 0.06252949684858322
iteration 137, loss = 0.06007438525557518
iteration 138, loss = 0.08822169154882431
iteration 139, loss = 0.06053059175610542
iteration 140, loss = 0.06607083976268768
iteration 141, loss = 0.07315534353256226
iteration 142, loss = 0.06089138984680176
iteration 143, loss = 0.06085311993956566
iteration 144, loss = 0.0635693371295929
iteration 145, loss = 0.05795031040906906
iteration 146, loss = 0.06603029370307922
iteration 147, loss = 0.05837839096784592
iteration 148, loss = 0.07033298164606094
iteration 149, loss = 0.06901603192090988
iteration 150, loss = 0.06483723223209381
iteration 151, loss = 0.06439335644245148
iteration 152, loss = 0.06531691551208496
iteration 153, loss = 0.05940572917461395
iteration 154, loss = 0.06149839237332344
iteration 155, loss = 0.05809168517589569
iteration 156, loss = 0.05685873702168465
iteration 157, loss = 0.05937550589442253
iteration 158, loss = 0.06476269662380219
iteration 159, loss = 0.06053421273827553
iteration 160, loss = 0.057591456919908524
iteration 161, loss = 0.06289003789424896
iteration 162, loss = 0.08276942372322083
iteration 163, loss = 0.0726676657795906
iteration 164, loss = 0.05705886334180832
iteration 165, loss = 0.07318789511919022
iteration 166, loss = 0.06496720761060715
iteration 167, loss = 0.06619399785995483
iteration 168, loss = 0.06169905886054039
iteration 169, loss = 0.052170608192682266
iteration 170, loss = 0.05518593266606331
iteration 171, loss = 0.055343303829431534
iteration 172, loss = 0.060831137001514435
iteration 173, loss = 0.0554414838552475
iteration 174, loss = 0.08135542273521423
iteration 175, loss = 0.056406572461128235
iteration 176, loss = 0.057910896837711334
iteration 177, loss = 0.06351008266210556
iteration 178, loss = 0.05391208082437515
iteration 179, loss = 0.05362129583954811
iteration 180, loss = 0.07694534957408905
iteration 181, loss = 0.058969657868146896
iteration 182, loss = 0.057325128465890884
iteration 183, loss = 0.06861724704504013
iteration 184, loss = 0.05604979023337364
iteration 185, loss = 0.08166700601577759
iteration 186, loss = 0.054478567093610764
iteration 187, loss = 0.06343278288841248
iteration 188, loss = 0.07005950808525085
iteration 189, loss = 0.051886945962905884
iteration 190, loss = 0.05432530492544174
iteration 191, loss = 0.05505293980240822
iteration 192, loss = 0.053105421364307404
iteration 193, loss = 0.06452589482069016
iteration 194, loss = 0.07736808806657791
iteration 195, loss = 0.0550435408949852
iteration 196, loss = 0.052879635244607925
iteration 197, loss = 0.08053568005561829
iteration 198, loss = 0.05517248809337616
iteration 199, loss = 0.06447865813970566
iteration 200, loss = 0.05660185590386391
iteration 201, loss = 0.05362452566623688
iteration 202, loss = 0.06126924604177475
iteration 203, loss = 0.05227646231651306
iteration 204, loss = 0.06729266047477722
iteration 205, loss = 0.061399154365062714
iteration 206, loss = 0.07072865962982178
iteration 207, loss = 0.06988205760717392
iteration 208, loss = 0.055666327476501465
iteration 209, loss = 0.052020374685525894
iteration 210, loss = 0.05259506776928902
iteration 211, loss = 0.05314317345619202
iteration 212, loss = 0.0539233461022377
iteration 213, loss = 0.06545861065387726
iteration 214, loss = 0.04848524183034897
iteration 215, loss = 0.05944150686264038
iteration 216, loss = 0.04903378710150719
iteration 217, loss = 0.051392775028944016
iteration 218, loss = 0.05101510509848595
iteration 219, loss = 0.05214281007647514
iteration 220, loss = 0.05256139487028122
iteration 221, loss = 0.05003616213798523
iteration 222, loss = 0.05101487413048744
iteration 223, loss = 0.050773367285728455
iteration 224, loss = 0.049532800912857056
iteration 225, loss = 0.05160515755414963
iteration 226, loss = 0.059806760400533676
iteration 227, loss = 0.04957300424575806
iteration 228, loss = 0.05253602936863899
iteration 229, loss = 0.04853995889425278
iteration 230, loss = 0.05312974005937576
iteration 231, loss = 0.049289196729660034
iteration 232, loss = 0.0521586574614048
iteration 233, loss = 0.05334271118044853
iteration 234, loss = 0.049246154725551605
iteration 235, loss = 0.052492883056402206
iteration 236, loss = 0.05729864910244942
iteration 237, loss = 0.049803316593170166
iteration 238, loss = 0.05507521331310272
iteration 239, loss = 0.057013798505067825
iteration 240, loss = 0.052924469113349915
iteration 241, loss = 0.05545193329453468
iteration 242, loss = 0.0464828722178936
iteration 243, loss = 0.05080944672226906
iteration 244, loss = 0.05638442933559418
iteration 245, loss = 0.07028109580278397
iteration 246, loss = 0.06569719314575195
iteration 247, loss = 0.04815708100795746
iteration 248, loss = 0.06962433457374573
iteration 249, loss = 0.05199475586414337
iteration 250, loss = 0.04726824909448624
iteration 251, loss = 0.049525272101163864
iteration 252, loss = 0.07041361927986145
iteration 253, loss = 0.073155477643013
iteration 254, loss = 0.047380492091178894
iteration 255, loss = 0.04836784675717354
iteration 256, loss = 0.04559643566608429
iteration 257, loss = 0.053715530782938004
iteration 258, loss = 0.046632785350084305
iteration 259, loss = 0.06426902115345001
iteration 260, loss = 0.04619613289833069
iteration 261, loss = 0.04762836918234825
iteration 262, loss = 0.05375107377767563
iteration 263, loss = 0.047631971538066864
iteration 264, loss = 0.0466390959918499
iteration 265, loss = 0.044862035661935806
iteration 266, loss = 0.053460024297237396
iteration 267, loss = 0.04891026020050049
iteration 268, loss = 0.0449305884540081
iteration 269, loss = 0.05886136740446091
iteration 270, loss = 0.04570728912949562
iteration 271, loss = 0.04734339565038681
iteration 272, loss = 0.0508330836892128
iteration 273, loss = 0.04669235274195671
iteration 274, loss = 0.06159031391143799
iteration 275, loss = 0.048860300332307816
iteration 276, loss = 0.054164327681064606
iteration 277, loss = 0.0490148551762104
iteration 278, loss = 0.05197661370038986
iteration 279, loss = 0.05520276725292206
iteration 280, loss = 0.0461101308465004
iteration 281, loss = 0.05111066624522209
iteration 282, loss = 0.0554482527077198
iteration 283, loss = 0.04922773689031601
iteration 284, loss = 0.04826128110289574
iteration 285, loss = 0.0445769764482975
iteration 286, loss = 0.04714377596974373
iteration 287, loss = 0.05298498272895813
iteration 288, loss = 0.04434724897146225
iteration 289, loss = 0.046845391392707825
iteration 290, loss = 0.04764644056558609
iteration 291, loss = 0.05698177590966225
iteration 292, loss = 0.04460747539997101
iteration 293, loss = 0.04993940889835358
iteration 294, loss = 0.0518808551132679
iteration 295, loss = 0.04828774183988571
iteration 296, loss = 0.043168094009160995
iteration 297, loss = 0.04782884940505028
iteration 298, loss = 0.05242786556482315
iteration 299, loss = 0.06230298429727554
iteration 300, loss = 0.04419857636094093
iteration 1, loss = 0.05069354176521301
iteration 2, loss = 0.044365666806697845
iteration 3, loss = 0.04937778413295746
iteration 4, loss = 0.04514379799365997
iteration 5, loss = 0.043632034212350845
iteration 6, loss = 0.04890541732311249
iteration 7, loss = 0.08373304456472397
iteration 8, loss = 0.04817788675427437
iteration 9, loss = 0.04445897787809372
iteration 10, loss = 0.04340255260467529
iteration 11, loss = 0.0431559756398201
iteration 12, loss = 0.04320229962468147
iteration 13, loss = 0.0450095608830452
iteration 14, loss = 0.04166495427489281
iteration 15, loss = 0.06431707739830017
iteration 16, loss = 0.04276910424232483
iteration 17, loss = 0.04378775507211685
iteration 18, loss = 0.04345357045531273
iteration 19, loss = 0.041193146258592606
iteration 20, loss = 0.05756823718547821
iteration 21, loss = 0.04561397433280945
iteration 22, loss = 0.043065521866083145
iteration 23, loss = 0.05926383659243584
iteration 24, loss = 0.04090949147939682
iteration 25, loss = 0.04616734758019447
iteration 26, loss = 0.04657420516014099
iteration 27, loss = 0.047635164111852646
iteration 28, loss = 0.06357947736978531
iteration 29, loss = 0.04081271216273308
iteration 30, loss = 0.061047375202178955
iteration 31, loss = 0.04244115576148033
iteration 32, loss = 0.04840441420674324
iteration 33, loss = 0.04354231432080269
iteration 34, loss = 0.05248881131410599
iteration 35, loss = 0.039629094302654266
iteration 36, loss = 0.042029768228530884
iteration 37, loss = 0.04058750718832016
iteration 38, loss = 0.039911799132823944
iteration 39, loss = 0.039981406182050705
iteration 40, loss = 0.04136049002408981
iteration 41, loss = 0.05033447593450546
iteration 42, loss = 0.06161428615450859
iteration 43, loss = 0.04517769813537598
iteration 44, loss = 0.05746212229132652
iteration 45, loss = 0.04131515696644783
iteration 46, loss = 0.03931938484311104
iteration 47, loss = 0.03803026303648949
iteration 48, loss = 0.06259270757436752
iteration 49, loss = 0.04646679013967514
iteration 50, loss = 0.04139034450054169
iteration 51, loss = 0.04409557208418846
iteration 52, loss = 0.03940552845597267
iteration 53, loss = 0.03899172693490982
iteration 54, loss = 0.0405588261783123
iteration 55, loss = 0.04033346474170685
iteration 56, loss = 0.04415574297308922
iteration 57, loss = 0.06185554713010788
iteration 58, loss = 0.040833499282598495
iteration 59, loss = 0.04035552591085434
iteration 60, loss = 0.041652657091617584
iteration 61, loss = 0.0382811576128006
iteration 62, loss = 0.0432676337659359
iteration 63, loss = 0.03714374452829361
iteration 64, loss = 0.044427212327718735
iteration 65, loss = 0.038832273334264755
iteration 66, loss = 0.03986134007573128
iteration 67, loss = 0.04025060683488846
iteration 68, loss = 0.038691624999046326
iteration 69, loss = 0.039259172976017
iteration 70, loss = 0.03914621099829674
iteration 71, loss = 0.04634449630975723
iteration 72, loss = 0.04072996601462364
iteration 73, loss = 0.05047408863902092
iteration 74, loss = 0.03931861370801926
iteration 75, loss = 0.055490538477897644
iteration 76, loss = 0.04231297969818115
iteration 77, loss = 0.042682986706495285
iteration 78, loss = 0.05283379554748535
iteration 79, loss = 0.038387589156627655
iteration 80, loss = 0.04385434091091156
iteration 81, loss = 0.035675279796123505
iteration 82, loss = 0.04073511064052582
iteration 83, loss = 0.03655565157532692
iteration 84, loss = 0.036890991032123566
iteration 85, loss = 0.046322714537382126
iteration 86, loss = 0.04303688555955887
iteration 87, loss = 0.04314078763127327
iteration 88, loss = 0.043996747583150864
iteration 89, loss = 0.04180653393268585
iteration 90, loss = 0.03630433976650238
iteration 91, loss = 0.047606632113456726
iteration 92, loss = 0.04414188489317894
iteration 93, loss = 0.03774154931306839
iteration 94, loss = 0.03720633685588837
iteration 95, loss = 0.04365311563014984
iteration 96, loss = 0.03772510588169098
iteration 97, loss = 0.04081320762634277
iteration 98, loss = 0.042157065123319626
iteration 99, loss = 0.03781276196241379
iteration 100, loss = 0.036215655505657196
iteration 101, loss = 0.042887914925813675
iteration 102, loss = 0.04152505472302437
iteration 103, loss = 0.04650857672095299
iteration 104, loss = 0.05138804018497467
iteration 105, loss = 0.03787477687001228
iteration 106, loss = 0.046875033527612686
iteration 107, loss = 0.03690999373793602
iteration 108, loss = 0.03563961014151573
iteration 109, loss = 0.037157248705625534
iteration 110, loss = 0.03800205886363983
iteration 111, loss = 0.035998936742544174
iteration 112, loss = 0.046301182359457016
iteration 113, loss = 0.038551732897758484
iteration 114, loss = 0.03634588047862053
iteration 115, loss = 0.03737185522913933
iteration 116, loss = 0.04834358021616936
iteration 117, loss = 0.05373036488890648
iteration 118, loss = 0.048657726496458054
iteration 119, loss = 0.05646027252078056
iteration 120, loss = 0.03665058687329292
iteration 121, loss = 0.03676735609769821
iteration 122, loss = 0.03544628247618675
iteration 123, loss = 0.04073066636919975
iteration 124, loss = 0.03369341045618057
iteration 125, loss = 0.04473679140210152
iteration 126, loss = 0.03742731362581253
iteration 127, loss = 0.04323453828692436
iteration 128, loss = 0.042377863079309464
iteration 129, loss = 0.03672441095113754
iteration 130, loss = 0.0356091745197773
iteration 131, loss = 0.04114065319299698
iteration 132, loss = 0.04366806149482727
iteration 133, loss = 0.037967853248119354
iteration 134, loss = 0.03683319315314293
iteration 135, loss = 0.03938424587249756
iteration 136, loss = 0.03884262219071388
iteration 137, loss = 0.039837177842855453
iteration 138, loss = 0.03679720312356949
iteration 139, loss = 0.03493657335639
iteration 140, loss = 0.03609268367290497
iteration 141, loss = 0.0397554449737072
iteration 142, loss = 0.047778476029634476
iteration 143, loss = 0.03966955840587616
iteration 144, loss = 0.03596699610352516
iteration 145, loss = 0.035730160772800446
iteration 146, loss = 0.03634203225374222
iteration 147, loss = 0.035962872207164764
iteration 148, loss = 0.034696660935878754
iteration 149, loss = 0.03527061641216278
iteration 150, loss = 0.03410998359322548
iteration 151, loss = 0.03474191576242447
iteration 152, loss = 0.03962276130914688
iteration 153, loss = 0.03681812807917595
iteration 154, loss = 0.03797638788819313
iteration 155, loss = 0.03472111374139786
iteration 156, loss = 0.03478533402085304
iteration 157, loss = 0.0355752632021904
iteration 158, loss = 0.05287845432758331
iteration 159, loss = 0.03491746261715889
iteration 160, loss = 0.035621482878923416
iteration 161, loss = 0.03310408070683479
iteration 162, loss = 0.05499773100018501
iteration 163, loss = 0.034890975803136826
iteration 164, loss = 0.03313109278678894
iteration 165, loss = 0.04137493669986725
iteration 166, loss = 0.049283117055892944
iteration 167, loss = 0.0384056493639946
iteration 168, loss = 0.033281926065683365
iteration 169, loss = 0.04211612045764923
iteration 170, loss = 0.042774733155965805
iteration 171, loss = 0.03646022453904152
iteration 172, loss = 0.03714815527200699
iteration 173, loss = 0.03195304796099663
iteration 174, loss = 0.04735870286822319
iteration 175, loss = 0.031728263944387436
iteration 176, loss = 0.03832516446709633
iteration 177, loss = 0.03257118538022041
iteration 178, loss = 0.03294375538825989
iteration 179, loss = 0.036576978862285614
iteration 180, loss = 0.03622317686676979
iteration 181, loss = 0.033175982534885406
iteration 182, loss = 0.03791492059826851
iteration 183, loss = 0.031925834715366364
iteration 184, loss = 0.03198202699422836
iteration 185, loss = 0.04045826569199562
iteration 186, loss = 0.03354301676154137
iteration 187, loss = 0.03169126808643341
iteration 188, loss = 0.038397762924432755
iteration 189, loss = 0.032187867909669876
iteration 190, loss = 0.03379344940185547
iteration 191, loss = 0.03638816252350807
iteration 192, loss = 0.03730735182762146
iteration 193, loss = 0.04413311183452606
iteration 194, loss = 0.034119054675102234
iteration 195, loss = 0.03528469055891037
iteration 196, loss = 0.032026201486587524
iteration 197, loss = 0.0329134576022625
iteration 198, loss = 0.03924921154975891
iteration 199, loss = 0.033253755420446396
iteration 200, loss = 0.03720628842711449
iteration 201, loss = 0.03374246507883072
iteration 202, loss = 0.03274058178067207
iteration 203, loss = 0.029215849936008453
iteration 204, loss = 0.03270725533366203
iteration 205, loss = 0.032616760581731796
iteration 206, loss = 0.031166795641183853
iteration 207, loss = 0.03441108018159866
iteration 208, loss = 0.031071020290255547
iteration 209, loss = 0.03749678283929825
iteration 210, loss = 0.03630286455154419
iteration 211, loss = 0.03269913047552109
iteration 212, loss = 0.03274743631482124
iteration 213, loss = 0.032230764627456665
iteration 214, loss = 0.03944480046629906
iteration 215, loss = 0.04125616326928139
iteration 216, loss = 0.03182871267199516
iteration 217, loss = 0.031640078872442245
iteration 218, loss = 0.03764556720852852
iteration 219, loss = 0.031352560967206955
iteration 220, loss = 0.04673044756054878
iteration 221, loss = 0.029460424557328224
iteration 222, loss = 0.032587409019470215
iteration 223, loss = 0.03378039225935936
iteration 224, loss = 0.030237022787332535
iteration 225, loss = 0.029944365844130516
iteration 226, loss = 0.031537558883428574
iteration 227, loss = 0.035076308995485306
iteration 228, loss = 0.029336070641875267
iteration 229, loss = 0.030667873099446297
iteration 230, loss = 0.047998081892728806
iteration 231, loss = 0.03145598620176315
iteration 232, loss = 0.03125583752989769
iteration 233, loss = 0.03284502029418945
iteration 234, loss = 0.033807072788476944
iteration 235, loss = 0.04396890848875046
iteration 236, loss = 0.04027501121163368
iteration 237, loss = 0.030427653342485428
iteration 238, loss = 0.030344024300575256
iteration 239, loss = 0.03374994173645973
iteration 240, loss = 0.03146540746092796
iteration 241, loss = 0.03130727261304855
iteration 242, loss = 0.032040923833847046
iteration 243, loss = 0.03224970027804375
iteration 244, loss = 0.0281057171523571
iteration 245, loss = 0.028117967769503593
iteration 246, loss = 0.029866402968764305
iteration 247, loss = 0.03394317999482155
iteration 248, loss = 0.03199709951877594
iteration 249, loss = 0.03207840025424957
iteration 250, loss = 0.02907150611281395
iteration 251, loss = 0.033413272351026535
iteration 252, loss = 0.03276488184928894
iteration 253, loss = 0.03033120557665825
iteration 254, loss = 0.030236830934882164
iteration 255, loss = 0.03242024406790733
iteration 256, loss = 0.02822781167924404
iteration 257, loss = 0.03113967552781105
iteration 258, loss = 0.02921929396688938
iteration 259, loss = 0.03276528790593147
iteration 260, loss = 0.039720773696899414
iteration 261, loss = 0.03904953598976135
iteration 262, loss = 0.029935698956251144
iteration 263, loss = 0.029994400218129158
iteration 264, loss = 0.02796638198196888
iteration 265, loss = 0.029047738760709763
iteration 266, loss = 0.03205518424510956
iteration 267, loss = 0.028436705470085144
iteration 268, loss = 0.03012915886938572
iteration 269, loss = 0.030338570475578308
iteration 270, loss = 0.033426035195589066
iteration 271, loss = 0.029470250010490417
iteration 272, loss = 0.02813383936882019
iteration 273, loss = 0.027019478380680084
iteration 274, loss = 0.0276639461517334
iteration 275, loss = 0.027913561090826988
iteration 276, loss = 0.02804461121559143
iteration 277, loss = 0.028085075318813324
iteration 278, loss = 0.028807150200009346
iteration 279, loss = 0.02938968315720558
iteration 280, loss = 0.028873475268483162
iteration 281, loss = 0.02714589424431324
iteration 282, loss = 0.027100734412670135
iteration 283, loss = 0.042411599308252335
iteration 284, loss = 0.034604161977767944
iteration 285, loss = 0.033822014927864075
iteration 286, loss = 0.0278623104095459
iteration 287, loss = 0.036680858582258224
iteration 288, loss = 0.027458595111966133
iteration 289, loss = 0.029827244579792023
iteration 290, loss = 0.0320928692817688
iteration 291, loss = 0.0381593219935894
iteration 292, loss = 0.03498225286602974
iteration 293, loss = 0.028415393084287643
iteration 294, loss = 0.027056580409407616
iteration 295, loss = 0.029763149097561836
iteration 296, loss = 0.03985237330198288
iteration 297, loss = 0.028292736038565636
iteration 298, loss = 0.026654893532395363
iteration 299, loss = 0.02974814549088478
iteration 300, loss = 0.026284832507371902
iteration 1, loss = 0.033378683030605316
iteration 2, loss = 0.0314960703253746
iteration 3, loss = 0.029069386422634125
iteration 4, loss = 0.030096003785729408
iteration 5, loss = 0.033116415143013
iteration 6, loss = 0.03060898184776306
iteration 7, loss = 0.027543706819415092
iteration 8, loss = 0.03068472445011139
iteration 9, loss = 0.02810107171535492
iteration 10, loss = 0.04188867285847664
iteration 11, loss = 0.030419860035181046
iteration 12, loss = 0.03266344964504242
iteration 13, loss = 0.027623135596513748
iteration 14, loss = 0.029125690460205078
iteration 15, loss = 0.03024078719317913
iteration 16, loss = 0.03204336762428284
iteration 17, loss = 0.040977559983730316
iteration 18, loss = 0.02677803859114647
iteration 19, loss = 0.029346656054258347
iteration 20, loss = 0.03055848740041256
iteration 21, loss = 0.03705735132098198
iteration 22, loss = 0.027315350249409676
iteration 23, loss = 0.04342448338866234
iteration 24, loss = 0.03282712399959564
iteration 25, loss = 0.02701689302921295
iteration 26, loss = 0.025943497195839882
iteration 27, loss = 0.026384014636278152
iteration 28, loss = 0.02766367793083191
iteration 29, loss = 0.02676137536764145
iteration 30, loss = 0.02849799022078514
iteration 31, loss = 0.029779527336359024
iteration 32, loss = 0.028605209663510323
iteration 33, loss = 0.027154210954904556
iteration 34, loss = 0.026739390566945076
iteration 35, loss = 0.027443155646324158
iteration 36, loss = 0.028481557965278625
iteration 37, loss = 0.029979819431900978
iteration 38, loss = 0.033561013638973236
iteration 39, loss = 0.03063378669321537
iteration 40, loss = 0.03617729991674423
iteration 41, loss = 0.027274396270513535
iteration 42, loss = 0.027817873284220695
iteration 43, loss = 0.031073853373527527
iteration 44, loss = 0.033530913293361664
iteration 45, loss = 0.029699526727199554
iteration 46, loss = 0.029632315039634705
iteration 47, loss = 0.028345705941319466
iteration 48, loss = 0.026892658323049545
iteration 49, loss = 0.02907499112188816
iteration 50, loss = 0.029244638979434967
iteration 51, loss = 0.02964828908443451
iteration 52, loss = 0.040345050394535065
iteration 53, loss = 0.02930273301899433
iteration 54, loss = 0.030518611893057823
iteration 55, loss = 0.026252450421452522
iteration 56, loss = 0.029818689450621605
iteration 57, loss = 0.02751190960407257
iteration 58, loss = 0.0291566401720047
iteration 59, loss = 0.026057712733745575
iteration 60, loss = 0.028735073283314705
iteration 61, loss = 0.038082055747509
iteration 62, loss = 0.03326022997498512
iteration 63, loss = 0.03960367292165756
iteration 64, loss = 0.032413288950920105
iteration 65, loss = 0.026687707751989365
iteration 66, loss = 0.038269322365522385
iteration 67, loss = 0.028560033068060875
iteration 68, loss = 0.027528254315257072
iteration 69, loss = 0.027090152725577354
iteration 70, loss = 0.04226485639810562
iteration 71, loss = 0.02797164022922516
iteration 72, loss = 0.03366128355264664
iteration 73, loss = 0.029088757932186127
iteration 74, loss = 0.02884802408516407
iteration 75, loss = 0.029877319931983948
iteration 76, loss = 0.031911227852106094
iteration 77, loss = 0.028090011328458786
iteration 78, loss = 0.03086056187748909
iteration 79, loss = 0.02870059758424759
iteration 80, loss = 0.02739039622247219
iteration 81, loss = 0.027148300781846046
iteration 82, loss = 0.03053334541618824
iteration 83, loss = 0.027739768847823143
iteration 84, loss = 0.02630412019789219
iteration 85, loss = 0.032144419848918915
iteration 86, loss = 0.02988666109740734
iteration 87, loss = 0.044337548315525055
iteration 88, loss = 0.026442065834999084
iteration 89, loss = 0.02763550914824009
iteration 90, loss = 0.025704532861709595
iteration 91, loss = 0.026920847594738007
iteration 92, loss = 0.02638711780309677
iteration 93, loss = 0.02668350748717785
iteration 94, loss = 0.02674143575131893
iteration 95, loss = 0.02968650683760643
iteration 96, loss = 0.028126563876867294
iteration 97, loss = 0.02702895924448967
iteration 98, loss = 0.027414601296186447
iteration 99, loss = 0.030978158116340637
iteration 100, loss = 0.03273932635784149
iteration 101, loss = 0.026298733428120613
iteration 102, loss = 0.02845832332968712
iteration 103, loss = 0.026683837175369263
iteration 104, loss = 0.030227214097976685
iteration 105, loss = 0.029005173593759537
iteration 106, loss = 0.026897646486759186
iteration 107, loss = 0.02806982398033142
iteration 108, loss = 0.03825113922357559
iteration 109, loss = 0.027293400838971138
iteration 110, loss = 0.02919541299343109
iteration 111, loss = 0.04141666740179062
iteration 112, loss = 0.030260100960731506
iteration 113, loss = 0.026716288179159164
iteration 114, loss = 0.041455041617155075
iteration 115, loss = 0.030305558815598488
iteration 116, loss = 0.025711771100759506
iteration 117, loss = 0.02653534896671772
iteration 118, loss = 0.027648191899061203
iteration 119, loss = 0.02811543270945549
iteration 120, loss = 0.026124119758605957
iteration 121, loss = 0.02737542986869812
iteration 122, loss = 0.026839198544621468
iteration 123, loss = 0.029994748532772064
iteration 124, loss = 0.02975984662771225
iteration 125, loss = 0.02666611224412918
iteration 126, loss = 0.04237520322203636
iteration 127, loss = 0.039192527532577515
iteration 128, loss = 0.028532689437270164
iteration 129, loss = 0.03803719952702522
iteration 130, loss = 0.02708120457828045
iteration 131, loss = 0.027179058641195297
iteration 132, loss = 0.031728412955999374
iteration 133, loss = 0.029721757397055626
iteration 134, loss = 0.02686312422156334
iteration 135, loss = 0.0265014860779047
iteration 136, loss = 0.029941122978925705
iteration 137, loss = 0.030043497681617737
iteration 138, loss = 0.041703082621097565
iteration 139, loss = 0.025995424017310143
iteration 140, loss = 0.030276408419013023
iteration 141, loss = 0.026905592530965805
iteration 142, loss = 0.027598418295383453
iteration 143, loss = 0.026224050670862198
iteration 144, loss = 0.030417127534747124
iteration 145, loss = 0.02526455745100975
iteration 146, loss = 0.027135521173477173
iteration 147, loss = 0.027499645948410034
iteration 148, loss = 0.0287528857588768
iteration 149, loss = 0.02876557596027851
iteration 150, loss = 0.027089552953839302
iteration 151, loss = 0.03099507838487625
iteration 152, loss = 0.029367588460445404
iteration 153, loss = 0.029531314969062805
iteration 154, loss = 0.02878357470035553
iteration 155, loss = 0.029241541400551796
iteration 156, loss = 0.026284094899892807
iteration 157, loss = 0.029252324253320694
iteration 158, loss = 0.02684362232685089
iteration 159, loss = 0.027063440531492233
iteration 160, loss = 0.027478739619255066
iteration 161, loss = 0.04195640608668327
iteration 162, loss = 0.02761736325919628
iteration 163, loss = 0.027529144659638405
iteration 164, loss = 0.02958645671606064
iteration 165, loss = 0.027287520468235016
iteration 166, loss = 0.03173442557454109
iteration 167, loss = 0.026119492948055267
iteration 168, loss = 0.03371858224272728
iteration 169, loss = 0.027856595814228058
iteration 170, loss = 0.02880069427192211
iteration 171, loss = 0.02901248261332512
iteration 172, loss = 0.028765888884663582
iteration 173, loss = 0.03131303936243057
iteration 174, loss = 0.02644159272313118
iteration 175, loss = 0.029881175607442856
iteration 176, loss = 0.029051126912236214
iteration 177, loss = 0.02903510443866253
iteration 178, loss = 0.028656739741563797
iteration 179, loss = 0.0272841639816761
iteration 180, loss = 0.03129389509558678
iteration 181, loss = 0.03083263337612152
iteration 182, loss = 0.03706224262714386
iteration 183, loss = 0.027769114822149277
iteration 184, loss = 0.02625160850584507
iteration 185, loss = 0.02660220116376877
iteration 186, loss = 0.03911033645272255
iteration 187, loss = 0.029730699956417084
iteration 188, loss = 0.029384208843111992
iteration 189, loss = 0.032018110156059265
iteration 190, loss = 0.02725060097873211
iteration 191, loss = 0.02532513067126274
iteration 192, loss = 0.03259233012795448
iteration 193, loss = 0.02666596882045269
iteration 194, loss = 0.03745458275079727
iteration 195, loss = 0.032882366329431534
iteration 196, loss = 0.031959664076566696
iteration 197, loss = 0.036962367594242096
iteration 198, loss = 0.026800494641065598
iteration 199, loss = 0.02594187669456005
iteration 200, loss = 0.02492218278348446
iteration 201, loss = 0.02841745875775814
iteration 202, loss = 0.027777299284934998
iteration 203, loss = 0.03339556232094765
iteration 204, loss = 0.026555761694908142
iteration 205, loss = 0.0375569686293602
iteration 206, loss = 0.026902833953499794
iteration 207, loss = 0.028563493862748146
iteration 208, loss = 0.029610993340611458
iteration 209, loss = 0.02770289219915867
iteration 210, loss = 0.027624664828181267
iteration 211, loss = 0.04083540663123131
iteration 212, loss = 0.028234856203198433
iteration 213, loss = 0.02895216830074787
iteration 214, loss = 0.028275562450289726
iteration 215, loss = 0.025621339678764343
iteration 216, loss = 0.026345791295170784
iteration 217, loss = 0.02938450686633587
iteration 218, loss = 0.026539500802755356
iteration 219, loss = 0.02645459957420826
iteration 220, loss = 0.027556611225008965
iteration 221, loss = 0.028937185183167458
iteration 222, loss = 0.02776319347321987
iteration 223, loss = 0.025098618119955063
iteration 224, loss = 0.029887733981013298
iteration 225, loss = 0.031701814383268356
iteration 226, loss = 0.031057408079504967
iteration 227, loss = 0.029273495078086853
iteration 228, loss = 0.026796802878379822
iteration 229, loss = 0.04083912819623947
iteration 230, loss = 0.02949744276702404
iteration 231, loss = 0.035697873681783676
iteration 232, loss = 0.026111803948879242
iteration 233, loss = 0.027663150802254677
iteration 234, loss = 0.0324978306889534
iteration 235, loss = 0.026387790217995644
iteration 236, loss = 0.026046067476272583
iteration 237, loss = 0.02725883759558201
iteration 238, loss = 0.02884422428905964
iteration 239, loss = 0.03185540437698364
iteration 240, loss = 0.02648160234093666
iteration 241, loss = 0.025881992653012276
iteration 242, loss = 0.04717329889535904
iteration 243, loss = 0.025638122111558914
iteration 244, loss = 0.027366161346435547
iteration 245, loss = 0.026516029611229897
iteration 246, loss = 0.03102712333202362
iteration 247, loss = 0.025223642587661743
iteration 248, loss = 0.030622486025094986
iteration 249, loss = 0.02626936323940754
iteration 250, loss = 0.026756834238767624
iteration 251, loss = 0.026037709787487984
iteration 252, loss = 0.026790838688611984
iteration 253, loss = 0.03878171369433403
iteration 254, loss = 0.026264622807502747
iteration 255, loss = 0.02605718933045864
iteration 256, loss = 0.02739119343459606
iteration 257, loss = 0.02984432876110077
iteration 258, loss = 0.025940541177988052
iteration 259, loss = 0.027463652193546295
iteration 260, loss = 0.025481538847088814
iteration 261, loss = 0.02937226928770542
iteration 262, loss = 0.029900241643190384
iteration 263, loss = 0.027394991368055344
iteration 264, loss = 0.026505572721362114
iteration 265, loss = 0.02710014395415783
iteration 266, loss = 0.02647324837744236
iteration 267, loss = 0.032559335231781006
iteration 268, loss = 0.02650965377688408
iteration 269, loss = 0.025788241997361183
iteration 270, loss = 0.028962958604097366
iteration 271, loss = 0.028849070891737938
iteration 272, loss = 0.027426304295659065
iteration 273, loss = 0.029657507315278053
iteration 274, loss = 0.029759012162685394
iteration 275, loss = 0.025941265746951103
iteration 276, loss = 0.02812313660979271
iteration 277, loss = 0.029603177681565285
iteration 278, loss = 0.025606360286474228
iteration 279, loss = 0.0290681105107069
iteration 280, loss = 0.028323432430624962
iteration 281, loss = 0.03029516339302063
iteration 282, loss = 0.025752101093530655
iteration 283, loss = 0.026059415191411972
iteration 284, loss = 0.047727569937705994
iteration 285, loss = 0.028386151418089867
iteration 286, loss = 0.0332677960395813
iteration 287, loss = 0.026134535670280457
iteration 288, loss = 0.03893337398767471
iteration 289, loss = 0.026346756145358086
iteration 290, loss = 0.02689535915851593
iteration 291, loss = 0.03874535486102104
iteration 292, loss = 0.026361405849456787
iteration 293, loss = 0.027026768773794174
iteration 294, loss = 0.02715092897415161
iteration 295, loss = 0.025366324931383133
iteration 296, loss = 0.03485843166708946
iteration 297, loss = 0.02528899535536766
iteration 298, loss = 0.02641056478023529
iteration 299, loss = 0.028035638853907585
iteration 300, loss = 0.02652394026517868
iteration 1, loss = 0.03021232783794403
iteration 2, loss = 0.0265969131141901
iteration 3, loss = 0.026654843240976334
iteration 4, loss = 0.031128322705626488
iteration 5, loss = 0.04473985731601715
iteration 6, loss = 0.029931548982858658
iteration 7, loss = 0.026128554716706276
iteration 8, loss = 0.03202769160270691
iteration 9, loss = 0.02688899263739586
iteration 10, loss = 0.02729596570134163
iteration 11, loss = 0.026154775172472
iteration 12, loss = 0.02485143393278122
iteration 13, loss = 0.02879803255200386
iteration 14, loss = 0.029756005853414536
iteration 15, loss = 0.037731073796749115
iteration 16, loss = 0.02645741030573845
iteration 17, loss = 0.02780592069029808
iteration 18, loss = 0.028133248910307884
iteration 19, loss = 0.029258079826831818
iteration 20, loss = 0.025365939363837242
iteration 21, loss = 0.04251662269234657
iteration 22, loss = 0.028771039098501205
iteration 23, loss = 0.026759754866361618
iteration 24, loss = 0.026087205857038498
iteration 25, loss = 0.029047241434454918
iteration 26, loss = 0.028810754418373108
iteration 27, loss = 0.02719111181795597
iteration 28, loss = 0.026670172810554504
iteration 29, loss = 0.02688281610608101
iteration 30, loss = 0.025490520521998405
iteration 31, loss = 0.026052430272102356
iteration 32, loss = 0.03352480009198189
iteration 33, loss = 0.033883702009916306
iteration 34, loss = 0.026349686086177826
iteration 35, loss = 0.026377661153674126
iteration 36, loss = 0.026305077597498894
iteration 37, loss = 0.026386108249425888
iteration 38, loss = 0.02786622941493988
iteration 39, loss = 0.027518030256032944
iteration 40, loss = 0.025751767680048943
iteration 41, loss = 0.026529068127274513
iteration 42, loss = 0.026841197162866592
iteration 43, loss = 0.027148444205522537
iteration 44, loss = 0.025496041402220726
iteration 45, loss = 0.02697952836751938
iteration 46, loss = 0.027955127879977226
iteration 47, loss = 0.02670246548950672
iteration 48, loss = 0.027555270120501518
iteration 49, loss = 0.02575722336769104
iteration 50, loss = 0.02721467800438404
iteration 51, loss = 0.028988754376769066
iteration 52, loss = 0.02573653869330883
iteration 53, loss = 0.027157045900821686
iteration 54, loss = 0.024448871612548828
iteration 55, loss = 0.030625056475400925
iteration 56, loss = 0.029489517211914062
iteration 57, loss = 0.025446398183703423
iteration 58, loss = 0.025894636288285255
iteration 59, loss = 0.03463507816195488
iteration 60, loss = 0.025719095021486282
iteration 61, loss = 0.02812984026968479
iteration 62, loss = 0.035064391791820526
iteration 63, loss = 0.04111182689666748
iteration 64, loss = 0.0274194423109293
iteration 65, loss = 0.025715243071317673
iteration 66, loss = 0.026876147836446762
iteration 67, loss = 0.028656549751758575
iteration 68, loss = 0.0272285845130682
iteration 69, loss = 0.02822691947221756
iteration 70, loss = 0.02738940715789795
iteration 71, loss = 0.030911030247807503
iteration 72, loss = 0.025009378790855408
iteration 73, loss = 0.028810162097215652
iteration 74, loss = 0.036328691989183426
iteration 75, loss = 0.027128642424941063
iteration 76, loss = 0.027811948210000992
iteration 77, loss = 0.025601759552955627
iteration 78, loss = 0.025714056566357613
iteration 79, loss = 0.02863123081624508
iteration 80, loss = 0.025360438972711563
iteration 81, loss = 0.02737121284008026
iteration 82, loss = 0.026575572788715363
iteration 83, loss = 0.03756171464920044
iteration 84, loss = 0.030664721503853798
iteration 85, loss = 0.03055764175951481
iteration 86, loss = 0.03137049078941345
iteration 87, loss = 0.02615286037325859
iteration 88, loss = 0.029855767264962196
iteration 89, loss = 0.02904054895043373
iteration 90, loss = 0.02947596274316311
iteration 91, loss = 0.028775375336408615
iteration 92, loss = 0.026047194376587868
iteration 93, loss = 0.026110896840691566
iteration 94, loss = 0.024874774739146233
iteration 95, loss = 0.024733081459999084
iteration 96, loss = 0.030354084447026253
iteration 97, loss = 0.03201650455594063
iteration 98, loss = 0.02939106523990631
iteration 99, loss = 0.03756329417228699
iteration 100, loss = 0.03434528782963753
iteration 101, loss = 0.03635115921497345
iteration 102, loss = 0.024907538667321205
iteration 103, loss = 0.026131488382816315
iteration 104, loss = 0.02558727189898491
iteration 105, loss = 0.027490606531500816
iteration 106, loss = 0.026795877143740654
iteration 107, loss = 0.030742447823286057
iteration 108, loss = 0.027214936912059784
iteration 109, loss = 0.026480067521333694
iteration 110, loss = 0.02712147682905197
iteration 111, loss = 0.02543998323380947
iteration 112, loss = 0.02528083510696888
iteration 113, loss = 0.02555854059755802
iteration 114, loss = 0.024433910846710205
iteration 115, loss = 0.02535402774810791
iteration 116, loss = 0.026245135813951492
iteration 117, loss = 0.02803897112607956
iteration 118, loss = 0.02604696899652481
iteration 119, loss = 0.03510721027851105
iteration 120, loss = 0.02600875496864319
iteration 121, loss = 0.026645837351679802
iteration 122, loss = 0.025380060076713562
iteration 123, loss = 0.025199363008141518
iteration 124, loss = 0.02750362455844879
iteration 125, loss = 0.025593381375074387
iteration 126, loss = 0.031598858535289764
iteration 127, loss = 0.031223541125655174
iteration 128, loss = 0.024881167337298393
iteration 129, loss = 0.025477122515439987
iteration 130, loss = 0.03701481595635414
iteration 131, loss = 0.024204131215810776
iteration 132, loss = 0.0262979194521904
iteration 133, loss = 0.026550358161330223
iteration 134, loss = 0.02778293564915657
iteration 135, loss = 0.030967604368925095
iteration 136, loss = 0.029629532247781754
iteration 137, loss = 0.027022549882531166
iteration 138, loss = 0.024887729436159134
iteration 139, loss = 0.027167988941073418
iteration 140, loss = 0.029009662568569183
iteration 141, loss = 0.04149852693080902
iteration 142, loss = 0.02757028490304947
iteration 143, loss = 0.025538474321365356
iteration 144, loss = 0.027409981936216354
iteration 145, loss = 0.028714513406157494
iteration 146, loss = 0.025961587205529213
iteration 147, loss = 0.03829865902662277
iteration 148, loss = 0.031637996435165405
iteration 149, loss = 0.026888400316238403
iteration 150, loss = 0.03019186668097973
iteration 151, loss = 0.031220221891999245
iteration 152, loss = 0.030743582174181938
iteration 153, loss = 0.02700043097138405
iteration 154, loss = 0.026436496526002884
iteration 155, loss = 0.02810532972216606
iteration 156, loss = 0.02762175351381302
iteration 157, loss = 0.026665231212973595
iteration 158, loss = 0.024867702275514603
iteration 159, loss = 0.0290509220212698
iteration 160, loss = 0.035785011947155
iteration 161, loss = 0.027783451601862907
iteration 162, loss = 0.025458334013819695
iteration 163, loss = 0.025485044345259666
iteration 164, loss = 0.026033449918031693
iteration 165, loss = 0.029176902025938034
iteration 166, loss = 0.03033527545630932
iteration 167, loss = 0.035731445997953415
iteration 168, loss = 0.025433311238884926
iteration 169, loss = 0.02495182305574417
iteration 170, loss = 0.03766327723860741
iteration 171, loss = 0.03459463641047478
iteration 172, loss = 0.02508816123008728
iteration 173, loss = 0.03572218120098114
iteration 174, loss = 0.03265093266963959
iteration 175, loss = 0.029115159064531326
iteration 176, loss = 0.0287751704454422
iteration 177, loss = 0.03020516224205494
iteration 178, loss = 0.025418179109692574
iteration 179, loss = 0.025972094386816025
iteration 180, loss = 0.026263894513249397
iteration 181, loss = 0.024556832388043404
iteration 182, loss = 0.02609127387404442
iteration 183, loss = 0.025543680414557457
iteration 184, loss = 0.023708371445536613
iteration 185, loss = 0.03247572481632233
iteration 186, loss = 0.04685444012284279
iteration 187, loss = 0.02840389683842659
iteration 188, loss = 0.0242812130600214
iteration 189, loss = 0.026513224467635155
iteration 190, loss = 0.02490226738154888
iteration 191, loss = 0.03847488388419151
iteration 192, loss = 0.026702459901571274
iteration 193, loss = 0.027419472113251686
iteration 194, loss = 0.032197486609220505
iteration 195, loss = 0.0383896678686142
iteration 196, loss = 0.02532089129090309
iteration 197, loss = 0.02786165662109852
iteration 198, loss = 0.029094446450471878
iteration 199, loss = 0.030741525813937187
iteration 200, loss = 0.027416765689849854
iteration 201, loss = 0.028442736715078354
iteration 202, loss = 0.026125146076083183
iteration 203, loss = 0.02584419772028923
iteration 204, loss = 0.026679039001464844
iteration 205, loss = 0.02374838851392269
iteration 206, loss = 0.026252880692481995
iteration 207, loss = 0.026421088725328445
iteration 208, loss = 0.025415591895580292
iteration 209, loss = 0.02485438995063305
iteration 210, loss = 0.03859437629580498
iteration 211, loss = 0.02528306096792221
iteration 212, loss = 0.024926738813519478
iteration 213, loss = 0.02642716094851494
iteration 214, loss = 0.031180614605545998
iteration 215, loss = 0.027705073356628418
iteration 216, loss = 0.031052853912115097
iteration 217, loss = 0.02630196139216423
iteration 218, loss = 0.029700977727770805
iteration 219, loss = 0.028704088181257248
iteration 220, loss = 0.02803105302155018
iteration 221, loss = 0.02399420365691185
iteration 222, loss = 0.028824293985962868
iteration 223, loss = 0.037478189915418625
iteration 224, loss = 0.02502932958304882
iteration 225, loss = 0.02587982453405857
iteration 226, loss = 0.03901245817542076
iteration 227, loss = 0.029564952477812767
iteration 228, loss = 0.027754703536629677
iteration 229, loss = 0.028972091153264046
iteration 230, loss = 0.025935057550668716
iteration 231, loss = 0.025444116443395615
iteration 232, loss = 0.03263959661126137
iteration 233, loss = 0.02916094660758972
iteration 234, loss = 0.03703749552369118
iteration 235, loss = 0.024512093514204025
iteration 236, loss = 0.024709708988666534
iteration 237, loss = 0.02486661821603775
iteration 238, loss = 0.026478368788957596
iteration 239, loss = 0.02582007274031639
iteration 240, loss = 0.029619719833135605
iteration 241, loss = 0.02444671094417572
iteration 242, loss = 0.037589360028505325
iteration 243, loss = 0.02457120269536972
iteration 244, loss = 0.024401020258665085
iteration 245, loss = 0.03139950707554817
iteration 246, loss = 0.02473343536257744
iteration 247, loss = 0.02549372985959053
iteration 248, loss = 0.026682371273636818
iteration 249, loss = 0.026090344414114952
iteration 250, loss = 0.024205338209867477
iteration 251, loss = 0.02770369127392769
iteration 252, loss = 0.026201138272881508
iteration 253, loss = 0.028170395642518997
iteration 254, loss = 0.029913166537880898
iteration 255, loss = 0.026668813079595566
iteration 256, loss = 0.028231626376509666
iteration 257, loss = 0.025486590340733528
iteration 258, loss = 0.03348269313573837
iteration 259, loss = 0.026146642863750458
iteration 260, loss = 0.026689548045396805
iteration 261, loss = 0.028322909027338028
iteration 262, loss = 0.024746505543589592
iteration 263, loss = 0.026120822876691818
iteration 264, loss = 0.024558302015066147
iteration 265, loss = 0.023986337706446648
iteration 266, loss = 0.02623482048511505
iteration 267, loss = 0.03687024861574173
iteration 268, loss = 0.028066610917448997
iteration 269, loss = 0.027228526771068573
iteration 270, loss = 0.038858812302351
iteration 271, loss = 0.02651539072394371
iteration 272, loss = 0.03182104229927063
iteration 273, loss = 0.02804482914507389
iteration 274, loss = 0.041740864515304565
iteration 275, loss = 0.027310973033308983
iteration 276, loss = 0.027723265811800957
iteration 277, loss = 0.02651631087064743
iteration 278, loss = 0.02637794055044651
iteration 279, loss = 0.02563605085015297
iteration 280, loss = 0.026029543951153755
iteration 281, loss = 0.028909651562571526
iteration 282, loss = 0.02880593203008175
iteration 283, loss = 0.037962619215250015
iteration 284, loss = 0.034699756652116776
iteration 285, loss = 0.031040212139487267
iteration 286, loss = 0.03684254735708237
iteration 287, loss = 0.024410037323832512
iteration 288, loss = 0.025851469486951828
iteration 289, loss = 0.027006451040506363
iteration 290, loss = 0.02582567371428013
iteration 291, loss = 0.02566373348236084
iteration 292, loss = 0.026222942396998405
iteration 293, loss = 0.024177225306630135
iteration 294, loss = 0.024623911827802658
iteration 295, loss = 0.028665661811828613
iteration 296, loss = 0.028899962082505226
iteration 297, loss = 0.024845074862241745
iteration 298, loss = 0.03187039867043495
iteration 299, loss = 0.02457476407289505
iteration 300, loss = 0.029679011553525925
iteration 1, loss = 0.027353746816515923
iteration 2, loss = 0.0319073423743248
iteration 3, loss = 0.024027056992053986
iteration 4, loss = 0.02648298628628254
iteration 5, loss = 0.04099688678979874
iteration 6, loss = 0.02517758123576641
iteration 7, loss = 0.024674931541085243
iteration 8, loss = 0.028204292058944702
iteration 9, loss = 0.02359778806567192
iteration 10, loss = 0.038371749222278595
iteration 11, loss = 0.03913125395774841
iteration 12, loss = 0.024036141112446785
iteration 13, loss = 0.0414426252245903
iteration 14, loss = 0.02771887555718422
iteration 15, loss = 0.028560487553477287
iteration 16, loss = 0.024995936080813408
iteration 17, loss = 0.02946571260690689
iteration 18, loss = 0.02505193091928959
iteration 19, loss = 0.02464819699525833
iteration 20, loss = 0.027656709775328636
iteration 21, loss = 0.025369293987751007
iteration 22, loss = 0.024246590211987495
iteration 23, loss = 0.024776553735136986
iteration 24, loss = 0.024812640622258186
iteration 25, loss = 0.026567209511995316
iteration 26, loss = 0.024809038266539574
iteration 27, loss = 0.027509717270731926
iteration 28, loss = 0.026521727442741394
iteration 29, loss = 0.025102699175477028
iteration 30, loss = 0.026233255863189697
iteration 31, loss = 0.026610199362039566
iteration 32, loss = 0.02848004549741745
iteration 33, loss = 0.024895889684557915
iteration 34, loss = 0.02735828049480915
iteration 35, loss = 0.026721768081188202
iteration 36, loss = 0.026554331183433533
iteration 37, loss = 0.025026120245456696
iteration 38, loss = 0.031613875180482864
iteration 39, loss = 0.025645148009061813
iteration 40, loss = 0.027129126712679863
iteration 41, loss = 0.029117198660969734
iteration 42, loss = 0.027744214981794357
iteration 43, loss = 0.028349578380584717
iteration 44, loss = 0.0290618147701025
iteration 45, loss = 0.03485871106386185
iteration 46, loss = 0.02573576755821705
iteration 47, loss = 0.027447335422039032
iteration 48, loss = 0.02622106857597828
iteration 49, loss = 0.0263304952532053
iteration 50, loss = 0.025791071355342865
iteration 51, loss = 0.025237547233700752
iteration 52, loss = 0.028511522337794304
iteration 53, loss = 0.025731123983860016
iteration 54, loss = 0.027650900185108185
iteration 55, loss = 0.032622575759887695
iteration 56, loss = 0.030171455815434456
iteration 57, loss = 0.026892967522144318
iteration 58, loss = 0.03845110535621643
iteration 59, loss = 0.025647468864917755
iteration 60, loss = 0.024595675989985466
iteration 61, loss = 0.02519671991467476
iteration 62, loss = 0.024335866793990135
iteration 63, loss = 0.025526951998472214
iteration 64, loss = 0.029977940022945404
iteration 65, loss = 0.02549227699637413
iteration 66, loss = 0.027655363082885742
iteration 67, loss = 0.027120551094412804
iteration 68, loss = 0.024610722437500954
iteration 69, loss = 0.024003835394978523
iteration 70, loss = 0.026122285053133965
iteration 71, loss = 0.023843517526984215
iteration 72, loss = 0.02813538908958435
iteration 73, loss = 0.027519887313246727
iteration 74, loss = 0.02444988675415516
iteration 75, loss = 0.027984119951725006
iteration 76, loss = 0.030049379914999008
iteration 77, loss = 0.02891923114657402
iteration 78, loss = 0.02524338848888874
iteration 79, loss = 0.03737570717930794
iteration 80, loss = 0.024881713092327118
iteration 81, loss = 0.0278537068516016
iteration 82, loss = 0.028260862454771996
iteration 83, loss = 0.027973446995019913
iteration 84, loss = 0.025698699057102203
iteration 85, loss = 0.03001767210662365
iteration 86, loss = 0.02568940445780754
iteration 87, loss = 0.0273989699780941
iteration 88, loss = 0.03803912177681923
iteration 89, loss = 0.025283776223659515
iteration 90, loss = 0.03886307030916214
iteration 91, loss = 0.03429412841796875
iteration 92, loss = 0.033258479088544846
iteration 93, loss = 0.024896524846553802
iteration 94, loss = 0.026341162621974945
iteration 95, loss = 0.025434449315071106
iteration 96, loss = 0.02571161463856697
iteration 97, loss = 0.026484321802854538
iteration 98, loss = 0.026166703552007675
iteration 99, loss = 0.02418607845902443
iteration 100, loss = 0.02355136163532734
iteration 101, loss = 0.029641564935445786
iteration 102, loss = 0.029545359313488007
iteration 103, loss = 0.04065917059779167
iteration 104, loss = 0.03302081301808357
iteration 105, loss = 0.02727510780096054
iteration 106, loss = 0.024091072380542755
iteration 107, loss = 0.02405046671628952
iteration 108, loss = 0.024508535861968994
iteration 109, loss = 0.03393709659576416
iteration 110, loss = 0.02371453121304512
iteration 111, loss = 0.027626883238554
iteration 112, loss = 0.026432329788804054
iteration 113, loss = 0.025451865047216415
iteration 114, loss = 0.025793397799134254
iteration 115, loss = 0.024405507370829582
iteration 116, loss = 0.02574625052511692
iteration 117, loss = 0.025635946542024612
iteration 118, loss = 0.03421775996685028
iteration 119, loss = 0.024127524346113205
iteration 120, loss = 0.023369744420051575
iteration 121, loss = 0.02484118938446045
iteration 122, loss = 0.030972447246313095
iteration 123, loss = 0.027316676452755928
iteration 124, loss = 0.02793603017926216
iteration 125, loss = 0.03716738149523735
iteration 126, loss = 0.026984892785549164
iteration 127, loss = 0.027226151898503304
iteration 128, loss = 0.024813184514641762
iteration 129, loss = 0.025017529726028442
iteration 130, loss = 0.025234051048755646
iteration 131, loss = 0.02823200263082981
iteration 132, loss = 0.02340787649154663
iteration 133, loss = 0.034849632531404495
iteration 134, loss = 0.024754734709858894
iteration 135, loss = 0.028101295232772827
iteration 136, loss = 0.027559567242860794
iteration 137, loss = 0.025467729195952415
iteration 138, loss = 0.026873547583818436
iteration 139, loss = 0.025157766416668892
iteration 140, loss = 0.028874143958091736
iteration 141, loss = 0.03106195665895939
iteration 142, loss = 0.03017711080610752
iteration 143, loss = 0.02652997523546219
iteration 144, loss = 0.02795008197426796
iteration 145, loss = 0.024681299924850464
iteration 146, loss = 0.02565220184624195
iteration 147, loss = 0.026621833443641663
iteration 148, loss = 0.025875285267829895
iteration 149, loss = 0.02917385846376419
iteration 150, loss = 0.03530250862240791
iteration 151, loss = 0.024393577128648758
iteration 152, loss = 0.025117557495832443
iteration 153, loss = 0.026508299633860588
iteration 154, loss = 0.02507888525724411
iteration 155, loss = 0.023866118863224983
iteration 156, loss = 0.028384115546941757
iteration 157, loss = 0.025155194103717804
iteration 158, loss = 0.028588812798261642
iteration 159, loss = 0.02443678304553032
iteration 160, loss = 0.030245743691921234
iteration 161, loss = 0.0240138228982687
iteration 162, loss = 0.02487005852162838
iteration 163, loss = 0.02575192041695118
iteration 164, loss = 0.02488240972161293
iteration 165, loss = 0.027854593470692635
iteration 166, loss = 0.02602323144674301
iteration 167, loss = 0.02768671326339245
iteration 168, loss = 0.027032004669308662
iteration 169, loss = 0.02488054893910885
iteration 170, loss = 0.04017604887485504
iteration 171, loss = 0.02506861463189125
iteration 172, loss = 0.02460964396595955
iteration 173, loss = 0.02779914252460003
iteration 174, loss = 0.035033904016017914
iteration 175, loss = 0.02492874674499035
iteration 176, loss = 0.02777923084795475
iteration 177, loss = 0.024924203753471375
iteration 178, loss = 0.02518344856798649
iteration 179, loss = 0.029026350006461143
iteration 180, loss = 0.02783801779150963
iteration 181, loss = 0.025746850296854973
iteration 182, loss = 0.037336140871047974
iteration 183, loss = 0.02371051535010338
iteration 184, loss = 0.026387374848127365
iteration 185, loss = 0.02572840265929699
iteration 186, loss = 0.025618605315685272
iteration 187, loss = 0.04208305478096008
iteration 188, loss = 0.02301996387541294
iteration 189, loss = 0.02510729618370533
iteration 190, loss = 0.026391377672553062
iteration 191, loss = 0.023479796946048737
iteration 192, loss = 0.024069666862487793
iteration 193, loss = 0.030257033184170723
iteration 194, loss = 0.024706032127141953
iteration 195, loss = 0.0280807763338089
iteration 196, loss = 0.02450983226299286
iteration 197, loss = 0.04032651707530022
iteration 198, loss = 0.025387153029441833
iteration 199, loss = 0.02546927146613598
iteration 200, loss = 0.024580981582403183
iteration 201, loss = 0.04114464670419693
iteration 202, loss = 0.024037517607212067
iteration 203, loss = 0.025114338845014572
iteration 204, loss = 0.02457183040678501
iteration 205, loss = 0.025826796889305115
iteration 206, loss = 0.026783447712659836
iteration 207, loss = 0.027215132489800453
iteration 208, loss = 0.025516891852021217
iteration 209, loss = 0.027123507112264633
iteration 210, loss = 0.0241305623203516
iteration 211, loss = 0.025134462863206863
iteration 212, loss = 0.026991726830601692
iteration 213, loss = 0.03878425061702728
iteration 214, loss = 0.025130487978458405
iteration 215, loss = 0.024755744263529778
iteration 216, loss = 0.023526586592197418
iteration 217, loss = 0.02352694422006607
iteration 218, loss = 0.02707364782691002
iteration 219, loss = 0.024390816688537598
iteration 220, loss = 0.026436099782586098
iteration 221, loss = 0.030691005289554596
iteration 222, loss = 0.02585340291261673
iteration 223, loss = 0.02830190770328045
iteration 224, loss = 0.024484191089868546
iteration 225, loss = 0.03927158936858177
iteration 226, loss = 0.0401574932038784
iteration 227, loss = 0.02938651852309704
iteration 228, loss = 0.03721873462200165
iteration 229, loss = 0.03424648940563202
iteration 230, loss = 0.024718839675188065
iteration 231, loss = 0.02739591710269451
iteration 232, loss = 0.025840280577540398
iteration 233, loss = 0.025143172591924667
iteration 234, loss = 0.024515405297279358
iteration 235, loss = 0.02845647931098938
iteration 236, loss = 0.02774583175778389
iteration 237, loss = 0.02366398274898529
iteration 238, loss = 0.024291496723890305
iteration 239, loss = 0.024599110707640648
iteration 240, loss = 0.024109765887260437
iteration 241, loss = 0.0334562323987484
iteration 242, loss = 0.026270831003785133
iteration 243, loss = 0.0252391304820776
iteration 244, loss = 0.027269776910543442
iteration 245, loss = 0.02510884217917919
iteration 246, loss = 0.03289414942264557
iteration 247, loss = 0.03558572754263878
iteration 248, loss = 0.026300011202692986
iteration 249, loss = 0.02871181257069111
iteration 250, loss = 0.02765500172972679
iteration 251, loss = 0.023261133581399918
iteration 252, loss = 0.028570836409926414
iteration 253, loss = 0.027373822405934334
iteration 254, loss = 0.02390541136264801
iteration 255, loss = 0.025492336601018906
iteration 256, loss = 0.0239174272865057
iteration 257, loss = 0.02434542402625084
iteration 258, loss = 0.025251511484384537
iteration 259, loss = 0.024439353495836258
iteration 260, loss = 0.030047476291656494
iteration 261, loss = 0.024935221299529076
iteration 262, loss = 0.02404969371855259
iteration 263, loss = 0.024336548522114754
iteration 264, loss = 0.023754585534334183
iteration 265, loss = 0.0251847542822361
iteration 266, loss = 0.029957275837659836
iteration 267, loss = 0.02345849946141243
iteration 268, loss = 0.023807980120182037
iteration 269, loss = 0.029330190271139145
iteration 270, loss = 0.03317053243517876
iteration 271, loss = 0.02446121908724308
iteration 272, loss = 0.023918742313981056
iteration 273, loss = 0.034324731677770615
iteration 274, loss = 0.023578235879540443
iteration 275, loss = 0.02458425983786583
iteration 276, loss = 0.02663298510015011
iteration 277, loss = 0.025313416495919228
iteration 278, loss = 0.02698494866490364
iteration 279, loss = 0.023564284667372704
iteration 280, loss = 0.024128101766109467
iteration 281, loss = 0.023236989974975586
iteration 282, loss = 0.02374747209250927
iteration 283, loss = 0.029656555503606796
iteration 284, loss = 0.026657404378056526
iteration 285, loss = 0.025519702583551407
iteration 286, loss = 0.023582281544804573
iteration 287, loss = 0.02430606633424759
iteration 288, loss = 0.027471086010336876
iteration 289, loss = 0.023973479866981506
iteration 290, loss = 0.023668916895985603
iteration 291, loss = 0.023983867838978767
iteration 292, loss = 0.02617471292614937
iteration 293, loss = 0.034950483590364456
iteration 294, loss = 0.024947142228484154
iteration 295, loss = 0.025248991325497627
iteration 296, loss = 0.02491169422864914
iteration 297, loss = 0.027635661885142326
iteration 298, loss = 0.02372436597943306
iteration 299, loss = 0.0246526338160038
iteration 300, loss = 0.02707047015428543
iteration 1, loss = 0.023811327293515205
iteration 2, loss = 0.025654464960098267
iteration 3, loss = 0.02370551973581314
iteration 4, loss = 0.023729601874947548
iteration 5, loss = 0.026931846514344215
iteration 6, loss = 0.025023678317666054
iteration 7, loss = 0.023762159049510956
iteration 8, loss = 0.025734949856996536
iteration 9, loss = 0.025062549859285355
iteration 10, loss = 0.025451645255088806
iteration 11, loss = 0.029512038454413414
iteration 12, loss = 0.02671842835843563
iteration 13, loss = 0.02548026107251644
iteration 14, loss = 0.024858813732862473
iteration 15, loss = 0.025197461247444153
iteration 16, loss = 0.025389522314071655
iteration 17, loss = 0.024219390004873276
iteration 18, loss = 0.026204820722341537
iteration 19, loss = 0.02493685483932495
iteration 20, loss = 0.03644486516714096
iteration 21, loss = 0.024048231542110443
iteration 22, loss = 0.024147838354110718
iteration 23, loss = 0.02693983167409897
iteration 24, loss = 0.02410649135708809
iteration 25, loss = 0.025012796744704247
iteration 26, loss = 0.030415676534175873
iteration 27, loss = 0.027335867285728455
iteration 28, loss = 0.022834815084934235
iteration 29, loss = 0.024353433400392532
iteration 30, loss = 0.03187725692987442
iteration 31, loss = 0.028865549713373184
iteration 32, loss = 0.02344287373125553
iteration 33, loss = 0.026993954554200172
iteration 34, loss = 0.026561474427580833
iteration 35, loss = 0.024661649018526077
iteration 36, loss = 0.0246749185025692
iteration 37, loss = 0.024888280779123306
iteration 38, loss = 0.023417962715029716
iteration 39, loss = 0.024785948917269707
iteration 40, loss = 0.03442450985312462
iteration 41, loss = 0.024783175438642502
iteration 42, loss = 0.023277780041098595
iteration 43, loss = 0.025181371718645096
iteration 44, loss = 0.03932473808526993
iteration 45, loss = 0.026995694264769554
iteration 46, loss = 0.027443062514066696
iteration 47, loss = 0.02739732153713703
iteration 48, loss = 0.03337911516427994
iteration 49, loss = 0.025357339531183243
iteration 50, loss = 0.02635156363248825
iteration 51, loss = 0.03045104444026947
iteration 52, loss = 0.023338759317994118
iteration 53, loss = 0.0228613018989563
iteration 54, loss = 0.023386791348457336
iteration 55, loss = 0.024657417088747025
iteration 56, loss = 0.023528212681412697
iteration 57, loss = 0.02383732981979847
iteration 58, loss = 0.024337783455848694
iteration 59, loss = 0.02656257525086403
iteration 60, loss = 0.025150347501039505
iteration 61, loss = 0.024296151474118233
iteration 62, loss = 0.024603605270385742
iteration 63, loss = 0.03093130886554718
iteration 64, loss = 0.02401590347290039
iteration 65, loss = 0.035538919270038605
iteration 66, loss = 0.028940316289663315
iteration 67, loss = 0.03730800747871399
iteration 68, loss = 0.027899302542209625
iteration 69, loss = 0.023740144446492195
iteration 70, loss = 0.027837703004479408
iteration 71, loss = 0.02362513355910778
iteration 72, loss = 0.02364557608962059
iteration 73, loss = 0.02356451004743576
iteration 74, loss = 0.023907098919153214
iteration 75, loss = 0.02345620095729828
iteration 76, loss = 0.02778250351548195
iteration 77, loss = 0.029200395569205284
iteration 78, loss = 0.022783858701586723
iteration 79, loss = 0.027454007416963577
iteration 80, loss = 0.02499978244304657
iteration 81, loss = 0.026195261627435684
iteration 82, loss = 0.03278933838009834
iteration 83, loss = 0.022678418084979057
iteration 84, loss = 0.02352214604616165
iteration 85, loss = 0.023900343105196953
iteration 86, loss = 0.02627357840538025
iteration 87, loss = 0.024665234610438347
iteration 88, loss = 0.023696111515164375
iteration 89, loss = 0.0265921950340271
iteration 90, loss = 0.03620344400405884
iteration 91, loss = 0.028492452576756477
iteration 92, loss = 0.02461060881614685
iteration 93, loss = 0.027118783444166183
iteration 94, loss = 0.02629447914659977
iteration 95, loss = 0.023175451904535294
iteration 96, loss = 0.023815633729100227
iteration 97, loss = 0.02493845671415329
iteration 98, loss = 0.0242290236055851
iteration 99, loss = 0.025051299482584
iteration 100, loss = 0.02758960798382759
iteration 101, loss = 0.02637365832924843
iteration 102, loss = 0.03093278594315052
iteration 103, loss = 0.024049822241067886
iteration 104, loss = 0.023424819111824036
iteration 105, loss = 0.02349085360765457
iteration 106, loss = 0.02337673306465149
iteration 107, loss = 0.024453364312648773
iteration 108, loss = 0.029779059812426567
iteration 109, loss = 0.026826802641153336
iteration 110, loss = 0.03463668003678322
iteration 111, loss = 0.03246308118104935
iteration 112, loss = 0.028800304979085922
iteration 113, loss = 0.03902367874979973
iteration 114, loss = 0.024600422009825706
iteration 115, loss = 0.028274383395910263
iteration 116, loss = 0.022476214915513992
iteration 117, loss = 0.023722751066088676
iteration 118, loss = 0.02498660609126091
iteration 119, loss = 0.03024648316204548
iteration 120, loss = 0.027799976989626884
iteration 121, loss = 0.024417508393526077
iteration 122, loss = 0.03509481996297836
iteration 123, loss = 0.025669556111097336
iteration 124, loss = 0.02693341113626957
iteration 125, loss = 0.02484775148332119
iteration 126, loss = 0.025797642767429352
iteration 127, loss = 0.04251791164278984
iteration 128, loss = 0.026583002880215645
iteration 129, loss = 0.02647901140153408
iteration 130, loss = 0.026076043024659157
iteration 131, loss = 0.03674079105257988
iteration 132, loss = 0.022874288260936737
iteration 133, loss = 0.02513457089662552
iteration 134, loss = 0.023331595584750175
iteration 135, loss = 0.026322150602936745
iteration 136, loss = 0.023538455367088318
iteration 137, loss = 0.025110144168138504
iteration 138, loss = 0.023473581299185753
iteration 139, loss = 0.025373587384819984
iteration 140, loss = 0.025590557605028152
iteration 141, loss = 0.02372647263109684
iteration 142, loss = 0.02770301140844822
iteration 143, loss = 0.026731830090284348
iteration 144, loss = 0.02367303892970085
iteration 145, loss = 0.02667274884879589
iteration 146, loss = 0.02363296039402485
iteration 147, loss = 0.027662018314003944
iteration 148, loss = 0.024610845372080803
iteration 149, loss = 0.02808026224374771
iteration 150, loss = 0.027304572984576225
iteration 151, loss = 0.03604646027088165
iteration 152, loss = 0.027500981464982033
iteration 153, loss = 0.025194736197590828
iteration 154, loss = 0.02509504370391369
iteration 155, loss = 0.025804048404097557
iteration 156, loss = 0.027141675353050232
iteration 157, loss = 0.025094743818044662
iteration 158, loss = 0.02332702837884426
iteration 159, loss = 0.037462178617715836
iteration 160, loss = 0.02785266563296318
iteration 161, loss = 0.03200940787792206
iteration 162, loss = 0.030064569786190987
iteration 163, loss = 0.024657009169459343
iteration 164, loss = 0.023670267313718796
iteration 165, loss = 0.026092756539583206
iteration 166, loss = 0.023109663277864456
iteration 167, loss = 0.02415427565574646
iteration 168, loss = 0.023909766227006912
iteration 169, loss = 0.023935969918966293
iteration 170, loss = 0.024820713326334953
iteration 171, loss = 0.02308419905602932
iteration 172, loss = 0.028418758884072304
iteration 173, loss = 0.025081684812903404
iteration 174, loss = 0.02886773645877838
iteration 175, loss = 0.02636675164103508
iteration 176, loss = 0.0232275128364563
iteration 177, loss = 0.02458876557648182
iteration 178, loss = 0.024746522307395935
iteration 179, loss = 0.0238149706274271
iteration 180, loss = 0.023938309401273727
iteration 181, loss = 0.02674650028347969
iteration 182, loss = 0.023801708593964577
iteration 183, loss = 0.023909026756882668
iteration 184, loss = 0.027576589956879616
iteration 185, loss = 0.024397658184170723
iteration 186, loss = 0.02661038562655449
iteration 187, loss = 0.024408018216490746
iteration 188, loss = 0.023645266890525818
iteration 189, loss = 0.023385843262076378
iteration 190, loss = 0.03636592626571655
iteration 191, loss = 0.03794623166322708
iteration 192, loss = 0.022457383573055267
iteration 193, loss = 0.022843897342681885
iteration 194, loss = 0.026678133755922318
iteration 195, loss = 0.024312639608979225
iteration 196, loss = 0.0271290373057127
iteration 197, loss = 0.024220632389187813
iteration 198, loss = 0.023883426561951637
iteration 199, loss = 0.02254122495651245
iteration 200, loss = 0.026829002425074577
iteration 201, loss = 0.023477979004383087
iteration 202, loss = 0.02323453314602375
iteration 203, loss = 0.022806508466601372
iteration 204, loss = 0.022762125357985497
iteration 205, loss = 0.02593979425728321
iteration 206, loss = 0.025329239666461945
iteration 207, loss = 0.03438767418265343
iteration 208, loss = 0.026615815237164497
iteration 209, loss = 0.026076914742588997
iteration 210, loss = 0.03279879316687584
iteration 211, loss = 0.03057919628918171
iteration 212, loss = 0.026834076270461082
iteration 213, loss = 0.023353468626737595
iteration 214, loss = 0.024797551333904266
iteration 215, loss = 0.024619538336992264
iteration 216, loss = 0.02769540436565876
iteration 217, loss = 0.02469981089234352
iteration 218, loss = 0.02352684549987316
iteration 219, loss = 0.03406687453389168
iteration 220, loss = 0.025718679651618004
iteration 221, loss = 0.023073313757777214
iteration 222, loss = 0.025119416415691376
iteration 223, loss = 0.022906923666596413
iteration 224, loss = 0.0245643500238657
iteration 225, loss = 0.02873721905052662
iteration 226, loss = 0.03492902219295502
iteration 227, loss = 0.02456953562796116
iteration 228, loss = 0.027609750628471375
iteration 229, loss = 0.023324772715568542
iteration 230, loss = 0.036426421254873276
iteration 231, loss = 0.024527136236429214
iteration 232, loss = 0.02763153240084648
iteration 233, loss = 0.03580483794212341
iteration 234, loss = 0.023202814161777496
iteration 235, loss = 0.024830779060721397
iteration 236, loss = 0.03277226537466049
iteration 237, loss = 0.024184253066778183
iteration 238, loss = 0.0285246130079031
iteration 239, loss = 0.02474522776901722
iteration 240, loss = 0.023099105805158615
iteration 241, loss = 0.02571987360715866
iteration 242, loss = 0.04591156169772148
iteration 243, loss = 0.02508026920258999
iteration 244, loss = 0.022920263931155205
iteration 245, loss = 0.025229182094335556
iteration 246, loss = 0.0234004445374012
iteration 247, loss = 0.026326343417167664
iteration 248, loss = 0.023166798055171967
iteration 249, loss = 0.022511139512062073
iteration 250, loss = 0.0229196697473526
iteration 251, loss = 0.02568620629608631
iteration 252, loss = 0.02357037551701069
iteration 253, loss = 0.0259866826236248
iteration 254, loss = 0.022929707542061806
iteration 255, loss = 0.022674238309264183
iteration 256, loss = 0.024747628718614578
iteration 257, loss = 0.02230825647711754
iteration 258, loss = 0.029284263029694557
iteration 259, loss = 0.024036556482315063
iteration 260, loss = 0.024400237947702408
iteration 261, loss = 0.025235038250684738
iteration 262, loss = 0.026246044784784317
iteration 263, loss = 0.023701369762420654
iteration 264, loss = 0.025793954730033875
iteration 265, loss = 0.02335921674966812
iteration 266, loss = 0.02439199574291706
iteration 267, loss = 0.024003364145755768
iteration 268, loss = 0.023490848019719124
iteration 269, loss = 0.027208417654037476
iteration 270, loss = 0.026965375989675522
iteration 271, loss = 0.02745906077325344
iteration 272, loss = 0.03641210123896599
iteration 273, loss = 0.02672547474503517
iteration 274, loss = 0.02787717990577221
iteration 275, loss = 0.02476947009563446
iteration 276, loss = 0.022848570719361305
iteration 277, loss = 0.024897411465644836
iteration 278, loss = 0.02418711595237255
iteration 279, loss = 0.02311975695192814
iteration 280, loss = 0.02266966924071312
iteration 281, loss = 0.03586837276816368
iteration 282, loss = 0.025949642062187195
iteration 283, loss = 0.02553124912083149
iteration 284, loss = 0.025314755737781525
iteration 285, loss = 0.02422693371772766
iteration 286, loss = 0.023176444694399834
iteration 287, loss = 0.026152972131967545
iteration 288, loss = 0.028608810156583786
iteration 289, loss = 0.023155488073825836
iteration 290, loss = 0.03473588451743126
iteration 291, loss = 0.029338333755731583
iteration 292, loss = 0.024145735427737236
iteration 293, loss = 0.023779628798365593
iteration 294, loss = 0.023961327970027924
iteration 295, loss = 0.023067455738782883
iteration 296, loss = 0.02546224370598793
iteration 297, loss = 0.026591980829834938
iteration 298, loss = 0.024319393560290337
iteration 299, loss = 0.022761650383472443
iteration 300, loss = 0.02502107247710228
iteration 1, loss = 0.022115178406238556
iteration 2, loss = 0.025378616526722908
iteration 3, loss = 0.026054488494992256
iteration 4, loss = 0.02339722029864788
iteration 5, loss = 0.021881291642785072
iteration 6, loss = 0.025300677865743637
iteration 7, loss = 0.029423652216792107
iteration 8, loss = 0.02375568263232708
iteration 9, loss = 0.023034358397126198
iteration 10, loss = 0.024014435708522797
iteration 11, loss = 0.025324324145913124
iteration 12, loss = 0.02407609485089779
iteration 13, loss = 0.023837357759475708
iteration 14, loss = 0.023084618151187897
iteration 15, loss = 0.035029493272304535
iteration 16, loss = 0.028799166902899742
iteration 17, loss = 0.026577986776828766
iteration 18, loss = 0.023420851677656174
iteration 19, loss = 0.03517608717083931
iteration 20, loss = 0.023790348321199417
iteration 21, loss = 0.022958608344197273
iteration 22, loss = 0.024872001260519028
iteration 23, loss = 0.022751459851861
iteration 24, loss = 0.027519401162862778
iteration 25, loss = 0.023477330803871155
iteration 26, loss = 0.023868843913078308
iteration 27, loss = 0.022653894498944283
iteration 28, loss = 0.023956013843417168
iteration 29, loss = 0.02593480795621872
iteration 30, loss = 0.024555735290050507
iteration 31, loss = 0.028168458491563797
iteration 32, loss = 0.02275206707417965
iteration 33, loss = 0.027765268459916115
iteration 34, loss = 0.02363579347729683
iteration 35, loss = 0.02274763584136963
iteration 36, loss = 0.026218894869089127
iteration 37, loss = 0.03256216645240784
iteration 38, loss = 0.024156302213668823
iteration 39, loss = 0.028229596093297005
iteration 40, loss = 0.028063323348760605
iteration 41, loss = 0.023228449746966362
iteration 42, loss = 0.022830266505479813
iteration 43, loss = 0.0254707969725132
iteration 44, loss = 0.02273550070822239
iteration 45, loss = 0.025090089067816734
iteration 46, loss = 0.03059585578739643
iteration 47, loss = 0.022978974506258965
iteration 48, loss = 0.02263595163822174
iteration 49, loss = 0.022364690899848938
iteration 50, loss = 0.023209864273667336
iteration 51, loss = 0.027993038296699524
iteration 52, loss = 0.026198530569672585
iteration 53, loss = 0.02298642322421074
iteration 54, loss = 0.026768533512949944
iteration 55, loss = 0.022825291380286217
iteration 56, loss = 0.022411290556192398
iteration 57, loss = 0.024431806057691574
iteration 58, loss = 0.02225174754858017
iteration 59, loss = 0.023184850811958313
iteration 60, loss = 0.033526692539453506
iteration 61, loss = 0.023675374686717987
iteration 62, loss = 0.027669696137309074
iteration 63, loss = 0.029184861108660698
iteration 64, loss = 0.02379225753247738
iteration 65, loss = 0.02383171021938324
iteration 66, loss = 0.03442525863647461
iteration 67, loss = 0.032276567071676254
iteration 68, loss = 0.02268826588988304
iteration 69, loss = 0.02468108758330345
iteration 70, loss = 0.02383887767791748
iteration 71, loss = 0.03158240020275116
iteration 72, loss = 0.024300467222929
iteration 73, loss = 0.02822442166507244
iteration 74, loss = 0.024023013189435005
iteration 75, loss = 0.02471601404249668
iteration 76, loss = 0.023218082264065742
iteration 77, loss = 0.02745259739458561
iteration 78, loss = 0.022455357015132904
iteration 79, loss = 0.024404646828770638
iteration 80, loss = 0.022852428257465363
iteration 81, loss = 0.022780586034059525
iteration 82, loss = 0.02672027237713337
iteration 83, loss = 0.0221328716725111
iteration 84, loss = 0.023565927520394325
iteration 85, loss = 0.024293769150972366
iteration 86, loss = 0.022530915215611458
iteration 87, loss = 0.026271305978298187
iteration 88, loss = 0.024074502289295197
iteration 89, loss = 0.026917368173599243
iteration 90, loss = 0.026171771809458733
iteration 91, loss = 0.02535373345017433
iteration 92, loss = 0.024118388071656227
iteration 93, loss = 0.031656090170145035
iteration 94, loss = 0.025905515998601913
iteration 95, loss = 0.023067478090524673
iteration 96, loss = 0.023136688396334648
iteration 97, loss = 0.026138920336961746
iteration 98, loss = 0.027030304074287415
iteration 99, loss = 0.025227611884474754
iteration 100, loss = 0.03399139642715454
iteration 101, loss = 0.029158536344766617
iteration 102, loss = 0.022343873977661133
iteration 103, loss = 0.02342912368476391
iteration 104, loss = 0.035028617829084396
iteration 105, loss = 0.02151215821504593
iteration 106, loss = 0.025298943743109703
iteration 107, loss = 0.03425503522157669
iteration 108, loss = 0.02524588257074356
iteration 109, loss = 0.025782844051718712
iteration 110, loss = 0.02222336269915104
iteration 111, loss = 0.03233097121119499
iteration 112, loss = 0.03500868380069733
iteration 113, loss = 0.024469193071126938
iteration 114, loss = 0.02274435944855213
iteration 115, loss = 0.036746054887771606
iteration 116, loss = 0.028072522953152657
iteration 117, loss = 0.02933715097606182
iteration 118, loss = 0.034637004137039185
iteration 119, loss = 0.023801708593964577
iteration 120, loss = 0.024023164063692093
iteration 121, loss = 0.024552201852202415
iteration 122, loss = 0.023529404774308205
iteration 123, loss = 0.02353869006037712
iteration 124, loss = 0.024642255157232285
iteration 125, loss = 0.03549981117248535
iteration 126, loss = 0.02407901920378208
iteration 127, loss = 0.023246152326464653
iteration 128, loss = 0.03417905792593956
iteration 129, loss = 0.025330442935228348
iteration 130, loss = 0.0225899126380682
iteration 131, loss = 0.03413988649845123
iteration 132, loss = 0.0402776300907135
iteration 133, loss = 0.027259819209575653
iteration 134, loss = 0.02309081330895424
iteration 135, loss = 0.02207721769809723
iteration 136, loss = 0.022593457251787186
iteration 137, loss = 0.024791516363620758
iteration 138, loss = 0.024857083335518837
iteration 139, loss = 0.02448893152177334
iteration 140, loss = 0.0220931563526392
iteration 141, loss = 0.02367267571389675
iteration 142, loss = 0.033327993005514145
iteration 143, loss = 0.022720979526638985
iteration 144, loss = 0.0284861009567976
iteration 145, loss = 0.028132816776633263
iteration 146, loss = 0.023832174018025398
iteration 147, loss = 0.024829022586345673
iteration 148, loss = 0.022677496075630188
iteration 149, loss = 0.022875115275382996
iteration 150, loss = 0.025421611964702606
iteration 151, loss = 0.027407629415392876
iteration 152, loss = 0.02225479856133461
iteration 153, loss = 0.0397888720035553
iteration 154, loss = 0.025016460567712784
iteration 155, loss = 0.024528391659259796
iteration 156, loss = 0.03175686299800873
iteration 157, loss = 0.022420605644583702
iteration 158, loss = 0.023654870688915253
iteration 159, loss = 0.02233266644179821
iteration 160, loss = 0.024875814095139503
iteration 161, loss = 0.022965949028730392
iteration 162, loss = 0.025495776906609535
iteration 163, loss = 0.022865446284413338
iteration 164, loss = 0.023115403950214386
iteration 165, loss = 0.021968187764286995
iteration 166, loss = 0.02858404815196991
iteration 167, loss = 0.022976519539952278
iteration 168, loss = 0.02741093561053276
iteration 169, loss = 0.02245345339179039
iteration 170, loss = 0.02479829080402851
iteration 171, loss = 0.026459239423274994
iteration 172, loss = 0.024338090792298317
iteration 173, loss = 0.023904969915747643
iteration 174, loss = 0.037178538739681244
iteration 175, loss = 0.02312272973358631
iteration 176, loss = 0.02385835163295269
iteration 177, loss = 0.024890771135687828
iteration 178, loss = 0.024952737614512444
iteration 179, loss = 0.02497582510113716
iteration 180, loss = 0.02487545646727085
iteration 181, loss = 0.023695221170783043
iteration 182, loss = 0.02270808257162571
iteration 183, loss = 0.03222521394491196
iteration 184, loss = 0.022854726761579514
iteration 185, loss = 0.022185901179909706
iteration 186, loss = 0.025259222835302353
iteration 187, loss = 0.02451830543577671
iteration 188, loss = 0.021848835051059723
iteration 189, loss = 0.033014219254255295
iteration 190, loss = 0.023933133110404015
iteration 191, loss = 0.02367876097559929
iteration 192, loss = 0.02385714463889599
iteration 193, loss = 0.030372416600584984
iteration 194, loss = 0.022192614153027534
iteration 195, loss = 0.022980349138379097
iteration 196, loss = 0.02497268095612526
iteration 197, loss = 0.022859586402773857
iteration 198, loss = 0.022877950221300125
iteration 199, loss = 0.02452479861676693
iteration 200, loss = 0.023590680211782455
iteration 201, loss = 0.02313014306128025
iteration 202, loss = 0.026196297258138657
iteration 203, loss = 0.024197110906243324
iteration 204, loss = 0.028026415035128593
iteration 205, loss = 0.02644415758550167
iteration 206, loss = 0.022899601608514786
iteration 207, loss = 0.02360808104276657
iteration 208, loss = 0.023433944210410118
iteration 209, loss = 0.0243593268096447
iteration 210, loss = 0.022723296657204628
iteration 211, loss = 0.02442033588886261
iteration 212, loss = 0.024459831416606903
iteration 213, loss = 0.024356432259082794
iteration 214, loss = 0.024010449647903442
iteration 215, loss = 0.023424381390213966
iteration 216, loss = 0.025376973673701286
iteration 217, loss = 0.036391731351614
iteration 218, loss = 0.02910323068499565
iteration 219, loss = 0.022285405546426773
iteration 220, loss = 0.022689666599035263
iteration 221, loss = 0.02254299633204937
iteration 222, loss = 0.02322329580783844
iteration 223, loss = 0.023654239252209663
iteration 224, loss = 0.02196587063372135
iteration 225, loss = 0.02209498919546604
iteration 226, loss = 0.029913796111941338
iteration 227, loss = 0.02372020296752453
iteration 228, loss = 0.02455531805753708
iteration 229, loss = 0.02344655618071556
iteration 230, loss = 0.022049525752663612
iteration 231, loss = 0.022105498239398003
iteration 232, loss = 0.022652465850114822
iteration 233, loss = 0.02262517437338829
iteration 234, loss = 0.025094565004110336
iteration 235, loss = 0.031769994646310806
iteration 236, loss = 0.022791916504502296
iteration 237, loss = 0.02430247887969017
iteration 238, loss = 0.024047838523983955
iteration 239, loss = 0.0228110421448946
iteration 240, loss = 0.022746628150343895
iteration 241, loss = 0.02472872845828533
iteration 242, loss = 0.021376224234700203
iteration 243, loss = 0.02265344373881817
iteration 244, loss = 0.02755441516637802
iteration 245, loss = 0.026445388793945312
iteration 246, loss = 0.02857823297381401
iteration 247, loss = 0.022558586671948433
iteration 248, loss = 0.02281786873936653
iteration 249, loss = 0.02604316920042038
iteration 250, loss = 0.022462831810116768
iteration 251, loss = 0.022540980949997902
iteration 252, loss = 0.022786028683185577
iteration 253, loss = 0.023193364962935448
iteration 254, loss = 0.02227640710771084
iteration 255, loss = 0.022789135575294495
iteration 256, loss = 0.0255278293043375
iteration 257, loss = 0.02351897582411766
iteration 258, loss = 0.022553112357854843
iteration 259, loss = 0.022456295788288116
iteration 260, loss = 0.027531698346138
iteration 261, loss = 0.03358606994152069
iteration 262, loss = 0.023931341245770454
iteration 263, loss = 0.024913353845477104
iteration 264, loss = 0.021471504122018814
iteration 265, loss = 0.02261403016746044
iteration 266, loss = 0.02587308920919895
iteration 267, loss = 0.02505607157945633
iteration 268, loss = 0.02685459703207016
iteration 269, loss = 0.023576192557811737
iteration 270, loss = 0.028629088774323463
iteration 271, loss = 0.02194979041814804
iteration 272, loss = 0.02209499105811119
iteration 273, loss = 0.025442499667406082
iteration 274, loss = 0.041969913989305496
iteration 275, loss = 0.023589160293340683
iteration 276, loss = 0.022461121901869774
iteration 277, loss = 0.022642668336629868
iteration 278, loss = 0.022312453016638756
iteration 279, loss = 0.022020084783434868
iteration 280, loss = 0.022124096751213074
iteration 281, loss = 0.025458255782723427
iteration 282, loss = 0.022355256602168083
iteration 283, loss = 0.02349187806248665
iteration 284, loss = 0.02622513473033905
iteration 285, loss = 0.022786641493439674
iteration 286, loss = 0.022042373195290565
iteration 287, loss = 0.02597372978925705
iteration 288, loss = 0.02259630337357521
iteration 289, loss = 0.026142343878746033
iteration 290, loss = 0.023689616471529007
iteration 291, loss = 0.03841426223516464
iteration 292, loss = 0.02226097695529461
iteration 293, loss = 0.022186242043972015
iteration 294, loss = 0.02424033358693123
iteration 295, loss = 0.024663789197802544
iteration 296, loss = 0.02547675557434559
iteration 297, loss = 0.0229253638535738
iteration 298, loss = 0.023417245596647263
iteration 299, loss = 0.025112221017479897
iteration 300, loss = 0.02728264592587948
iteration 1, loss = 0.03138240426778793
iteration 2, loss = 0.023554135113954544
iteration 3, loss = 0.022287091240286827
iteration 4, loss = 0.02279469557106495
iteration 5, loss = 0.024966150522232056
iteration 6, loss = 0.02320615015923977
iteration 7, loss = 0.026133542880415916
iteration 8, loss = 0.023035701364278793
iteration 9, loss = 0.022306349128484726
iteration 10, loss = 0.021926183253526688
iteration 11, loss = 0.022240303456783295
iteration 12, loss = 0.025267384946346283
iteration 13, loss = 0.025564664974808693
iteration 14, loss = 0.032877277582883835
iteration 15, loss = 0.022569598630070686
iteration 16, loss = 0.022312643006443977
iteration 17, loss = 0.023765921592712402
iteration 18, loss = 0.025341659784317017
iteration 19, loss = 0.022551491856575012
iteration 20, loss = 0.022922519594430923
iteration 21, loss = 0.022136056795716286
iteration 22, loss = 0.022616881877183914
iteration 23, loss = 0.024280164390802383
iteration 24, loss = 0.025995414704084396
iteration 25, loss = 0.023020485416054726
iteration 26, loss = 0.031031154096126556
iteration 27, loss = 0.022302379831671715
iteration 28, loss = 0.03262346237897873
iteration 29, loss = 0.022811362519860268
iteration 30, loss = 0.022702109068632126
iteration 31, loss = 0.03348090872168541
iteration 32, loss = 0.023346349596977234
iteration 33, loss = 0.02127339318394661
iteration 34, loss = 0.02219630591571331
iteration 35, loss = 0.03397534042596817
iteration 36, loss = 0.024275653064250946
iteration 37, loss = 0.021910058334469795
iteration 38, loss = 0.024598201736807823
iteration 39, loss = 0.02429421991109848
iteration 40, loss = 0.021370427682995796
iteration 41, loss = 0.024617711082100868
iteration 42, loss = 0.022480571642518044
iteration 43, loss = 0.026343457400798798
iteration 44, loss = 0.022805381566286087
iteration 45, loss = 0.022611405700445175
iteration 46, loss = 0.022862032055854797
iteration 47, loss = 0.021532747894525528
iteration 48, loss = 0.038027651607990265
iteration 49, loss = 0.024679133668541908
iteration 50, loss = 0.022310936823487282
iteration 51, loss = 0.03306751325726509
iteration 52, loss = 0.025076434016227722
iteration 53, loss = 0.023313861340284348
iteration 54, loss = 0.028297757729887962
iteration 55, loss = 0.02243961952626705
iteration 56, loss = 0.02665228769183159
iteration 57, loss = 0.021293168887495995
iteration 58, loss = 0.022193532437086105
iteration 59, loss = 0.04225454479455948
iteration 60, loss = 0.021170156076550484
iteration 61, loss = 0.026237454265356064
iteration 62, loss = 0.022538553923368454
iteration 63, loss = 0.024529067799448967
iteration 64, loss = 0.02332625351846218
iteration 65, loss = 0.022760814055800438
iteration 66, loss = 0.025255434215068817
iteration 67, loss = 0.022987347096204758
iteration 68, loss = 0.025071507319808006
iteration 69, loss = 0.03826494142413139
iteration 70, loss = 0.021927570924162865
iteration 71, loss = 0.025512458756566048
iteration 72, loss = 0.024061303585767746
iteration 73, loss = 0.03432510048151016
iteration 74, loss = 0.021552711725234985
iteration 75, loss = 0.035990193486213684
iteration 76, loss = 0.02733778767287731
iteration 77, loss = 0.023456044495105743
iteration 78, loss = 0.02350926212966442
iteration 79, loss = 0.031198617070913315
iteration 80, loss = 0.026608016341924667
iteration 81, loss = 0.02260740101337433
iteration 82, loss = 0.027598027139902115
iteration 83, loss = 0.023540273308753967
iteration 84, loss = 0.024800026789307594
iteration 85, loss = 0.028265265747904778
iteration 86, loss = 0.023595500737428665
iteration 87, loss = 0.024637440219521523
iteration 88, loss = 0.024013090878725052
iteration 89, loss = 0.02250385656952858
iteration 90, loss = 0.022122716531157494
iteration 91, loss = 0.023801367729902267
iteration 92, loss = 0.02306760475039482
iteration 93, loss = 0.02298828959465027
iteration 94, loss = 0.02339661680161953
iteration 95, loss = 0.023571781814098358
iteration 96, loss = 0.025934632867574692
iteration 97, loss = 0.022739840671420097
iteration 98, loss = 0.029561851173639297
iteration 99, loss = 0.02463378757238388
iteration 100, loss = 0.02182011865079403
iteration 101, loss = 0.021693475544452667
iteration 102, loss = 0.033387936651706696
iteration 103, loss = 0.022661404684185982
iteration 104, loss = 0.02297145687043667
iteration 105, loss = 0.024695277214050293
iteration 106, loss = 0.023813817650079727
iteration 107, loss = 0.022855758666992188
iteration 108, loss = 0.023807063698768616
iteration 109, loss = 0.022091589868068695
iteration 110, loss = 0.023537902161478996
iteration 111, loss = 0.02273673191666603
iteration 112, loss = 0.0262605007737875
iteration 113, loss = 0.024875270202755928
iteration 114, loss = 0.023408642038702965
iteration 115, loss = 0.02716583013534546
iteration 116, loss = 0.026515327394008636
iteration 117, loss = 0.021438593044877052
iteration 118, loss = 0.02167118340730667
iteration 119, loss = 0.0363415889441967
iteration 120, loss = 0.02075725793838501
iteration 121, loss = 0.026352914050221443
iteration 122, loss = 0.02137376368045807
iteration 123, loss = 0.022115599364042282
iteration 124, loss = 0.022661643102765083
iteration 125, loss = 0.02288680709898472
iteration 126, loss = 0.025557715445756912
iteration 127, loss = 0.024794790893793106
iteration 128, loss = 0.021931685507297516
iteration 129, loss = 0.032809458673000336
iteration 130, loss = 0.023281194269657135
iteration 131, loss = 0.02495959959924221
iteration 132, loss = 0.02324109710752964
iteration 133, loss = 0.025314010679721832
iteration 134, loss = 0.026467520743608475
iteration 135, loss = 0.022298257797956467
iteration 136, loss = 0.03201073408126831
iteration 137, loss = 0.021648898720741272
iteration 138, loss = 0.02447322942316532
iteration 139, loss = 0.021611083298921585
iteration 140, loss = 0.02566784992814064
iteration 141, loss = 0.022144146263599396
iteration 142, loss = 0.022880060598254204
iteration 143, loss = 0.02311565913259983
iteration 144, loss = 0.02085212431848049
iteration 145, loss = 0.025557802990078926
iteration 146, loss = 0.021572036668658257
iteration 147, loss = 0.024397321045398712
iteration 148, loss = 0.0244329534471035
iteration 149, loss = 0.024909652769565582
iteration 150, loss = 0.030705299228429794
iteration 151, loss = 0.02162792906165123
iteration 152, loss = 0.022434109821915627
iteration 153, loss = 0.022911589592695236
iteration 154, loss = 0.02495039440691471
iteration 155, loss = 0.024578595533967018
iteration 156, loss = 0.027175815775990486
iteration 157, loss = 0.02251860685646534
iteration 158, loss = 0.022067073732614517
iteration 159, loss = 0.03160158917307854
iteration 160, loss = 0.024506278336048126
iteration 161, loss = 0.021733872592449188
iteration 162, loss = 0.02337855100631714
iteration 163, loss = 0.021909015253186226
iteration 164, loss = 0.021086568012833595
iteration 165, loss = 0.02150285430252552
iteration 166, loss = 0.023368919268250465
iteration 167, loss = 0.023296644911170006
iteration 168, loss = 0.02152988687157631
iteration 169, loss = 0.024021074175834656
iteration 170, loss = 0.024741875007748604
iteration 171, loss = 0.030827442184090614
iteration 172, loss = 0.026384510099887848
iteration 173, loss = 0.02400551550090313
iteration 174, loss = 0.02116674929857254
iteration 175, loss = 0.022616833448410034
iteration 176, loss = 0.024485405534505844
iteration 177, loss = 0.021325785666704178
iteration 178, loss = 0.02066933363676071
iteration 179, loss = 0.02313707210123539
iteration 180, loss = 0.024588901549577713
iteration 181, loss = 0.022649740800261497
iteration 182, loss = 0.026703309267759323
iteration 183, loss = 0.03313489630818367
iteration 184, loss = 0.022825604304671288
iteration 185, loss = 0.022564087063074112
iteration 186, loss = 0.024988438934087753
iteration 187, loss = 0.024463610723614693
iteration 188, loss = 0.02342001162469387
iteration 189, loss = 0.023065650835633278
iteration 190, loss = 0.02198498509824276
iteration 191, loss = 0.022786812856793404
iteration 192, loss = 0.02268914319574833
iteration 193, loss = 0.025136824697256088
iteration 194, loss = 0.031237225979566574
iteration 195, loss = 0.024332160130143166
iteration 196, loss = 0.03402144834399223
iteration 197, loss = 0.02219979278743267
iteration 198, loss = 0.02340811677277088
iteration 199, loss = 0.022184327244758606
iteration 200, loss = 0.022579390555620193
iteration 201, loss = 0.022239960730075836
iteration 202, loss = 0.03647514432668686
iteration 203, loss = 0.02269154042005539
iteration 204, loss = 0.02181146666407585
iteration 205, loss = 0.0247874204069376
iteration 206, loss = 0.024280257523059845
iteration 207, loss = 0.02898801863193512
iteration 208, loss = 0.023520439863204956
iteration 209, loss = 0.023570818826556206
iteration 210, loss = 0.02447199635207653
iteration 211, loss = 0.027593839913606644
iteration 212, loss = 0.022450068965554237
iteration 213, loss = 0.024512283504009247
iteration 214, loss = 0.022689005360007286
iteration 215, loss = 0.02095085009932518
iteration 216, loss = 0.030140073969960213
iteration 217, loss = 0.02230110578238964
iteration 218, loss = 0.02124534174799919
iteration 219, loss = 0.02142595686018467
iteration 220, loss = 0.021863063797354698
iteration 221, loss = 0.021603478118777275
iteration 222, loss = 0.021689588204026222
iteration 223, loss = 0.02261682040989399
iteration 224, loss = 0.023085827007889748
iteration 225, loss = 0.021060345694422722
iteration 226, loss = 0.021094808354973793
iteration 227, loss = 0.021836943924427032
iteration 228, loss = 0.022961139678955078
iteration 229, loss = 0.02201606146991253
iteration 230, loss = 0.0222440455108881
iteration 231, loss = 0.02132832445204258
iteration 232, loss = 0.02511562965810299
iteration 233, loss = 0.022621117532253265
iteration 234, loss = 0.022477634251117706
iteration 235, loss = 0.033589139580726624
iteration 236, loss = 0.0230318084359169
iteration 237, loss = 0.03462469205260277
iteration 238, loss = 0.0276645440608263
iteration 239, loss = 0.023952728137373924
iteration 240, loss = 0.02589871734380722
iteration 241, loss = 0.023540854454040527
iteration 242, loss = 0.02412286587059498
iteration 243, loss = 0.024880897253751755
iteration 244, loss = 0.03671262040734291
iteration 245, loss = 0.021561458706855774
iteration 246, loss = 0.022068949416279793
iteration 247, loss = 0.023093556985259056
iteration 248, loss = 0.021935591474175453
iteration 249, loss = 0.021507157012820244
iteration 250, loss = 0.023984909057617188
iteration 251, loss = 0.021125545725226402
iteration 252, loss = 0.023092716932296753
iteration 253, loss = 0.022816533222794533
iteration 254, loss = 0.02574433945119381
iteration 255, loss = 0.02319127507507801
iteration 256, loss = 0.02421615645289421
iteration 257, loss = 0.02150909975171089
iteration 258, loss = 0.020268147811293602
iteration 259, loss = 0.0227789506316185
iteration 260, loss = 0.022893868386745453
iteration 261, loss = 0.022792520001530647
iteration 262, loss = 0.02222955971956253
iteration 263, loss = 0.021542083472013474
iteration 264, loss = 0.021904680877923965
iteration 265, loss = 0.025177543982863426
iteration 266, loss = 0.022422945126891136
iteration 267, loss = 0.022653603926301003
iteration 268, loss = 0.021306868642568588
iteration 269, loss = 0.027783166617155075
iteration 270, loss = 0.021470237523317337
iteration 271, loss = 0.02418043278157711
iteration 272, loss = 0.02545834518969059
iteration 273, loss = 0.03239758312702179
iteration 274, loss = 0.021110350266098976
iteration 275, loss = 0.022448111325502396
iteration 276, loss = 0.02233533002436161
iteration 277, loss = 0.02531195618212223
iteration 278, loss = 0.028466613963246346
iteration 279, loss = 0.022431356832385063
iteration 280, loss = 0.02222924865782261
iteration 281, loss = 0.024004364386200905
iteration 282, loss = 0.023782774806022644
iteration 283, loss = 0.023724835366010666
iteration 284, loss = 0.021451124921441078
iteration 285, loss = 0.022978927940130234
iteration 286, loss = 0.026043983176350594
iteration 287, loss = 0.024524645879864693
iteration 288, loss = 0.02171875350177288
iteration 289, loss = 0.021853908896446228
iteration 290, loss = 0.02210203744471073
iteration 291, loss = 0.025867918506264687
iteration 292, loss = 0.02256130799651146
iteration 293, loss = 0.025292539969086647
iteration 294, loss = 0.026735326275229454
iteration 295, loss = 0.021050484851002693
iteration 296, loss = 0.021650897338986397
iteration 297, loss = 0.022382007911801338
iteration 298, loss = 0.021914809942245483
iteration 299, loss = 0.021094171330332756
iteration 300, loss = 0.02230243757367134
iteration 1, loss = 0.021354999393224716
iteration 2, loss = 0.022111600264906883
iteration 3, loss = 0.02543199434876442
iteration 4, loss = 0.02269861102104187
iteration 5, loss = 0.022771865129470825
iteration 6, loss = 0.021067338064312935
iteration 7, loss = 0.021358372643589973
iteration 8, loss = 0.02431902289390564
iteration 9, loss = 0.024728858843445778
iteration 10, loss = 0.02120203711092472
iteration 11, loss = 0.03067614883184433
iteration 12, loss = 0.02194412797689438
iteration 13, loss = 0.021561233326792717
iteration 14, loss = 0.020990490913391113
iteration 15, loss = 0.020568368956446648
iteration 16, loss = 0.023837976157665253
iteration 17, loss = 0.021140964701771736
iteration 18, loss = 0.02362772263586521
iteration 19, loss = 0.02098890393972397
iteration 20, loss = 0.022839820012450218
iteration 21, loss = 0.024881770834326744
iteration 22, loss = 0.03251497447490692
iteration 23, loss = 0.02600186876952648
iteration 24, loss = 0.0268942229449749
iteration 25, loss = 0.023349732160568237
iteration 26, loss = 0.020993342623114586
iteration 27, loss = 0.023554177954792976
iteration 28, loss = 0.021471945568919182
iteration 29, loss = 0.02887396328151226
iteration 30, loss = 0.022307002916932106
iteration 31, loss = 0.024297622963786125
iteration 32, loss = 0.025399770587682724
iteration 33, loss = 0.02122662030160427
iteration 34, loss = 0.023534439504146576
iteration 35, loss = 0.021255772560834885
iteration 36, loss = 0.022220976650714874
iteration 37, loss = 0.021652787923812866
iteration 38, loss = 0.02332780510187149
iteration 39, loss = 0.022288063541054726
iteration 40, loss = 0.020437652245163918
iteration 41, loss = 0.02207852341234684
iteration 42, loss = 0.02463500387966633
iteration 43, loss = 0.026889806613326073
iteration 44, loss = 0.032865699380636215
iteration 45, loss = 0.021563144400715828
iteration 46, loss = 0.02310468815267086
iteration 47, loss = 0.02636544406414032
iteration 48, loss = 0.021132882684469223
iteration 49, loss = 0.02122160978615284
iteration 50, loss = 0.022655969485640526
iteration 51, loss = 0.02241448685526848
iteration 52, loss = 0.02169088087975979
iteration 53, loss = 0.025141747668385506
iteration 54, loss = 0.02050819806754589
iteration 55, loss = 0.026825981214642525
iteration 56, loss = 0.021468564867973328
iteration 57, loss = 0.022702250629663467
iteration 58, loss = 0.025604428723454475
iteration 59, loss = 0.023236729204654694
iteration 60, loss = 0.02090008743107319
iteration 61, loss = 0.02160465344786644
iteration 62, loss = 0.022504426538944244
iteration 63, loss = 0.023240135982632637
iteration 64, loss = 0.023299288004636765
iteration 65, loss = 0.0222960002720356
iteration 66, loss = 0.025239048525691032
iteration 67, loss = 0.020502032712101936
iteration 68, loss = 0.02350771799683571
iteration 69, loss = 0.02396218851208687
iteration 70, loss = 0.02465703710913658
iteration 71, loss = 0.02460087649524212
iteration 72, loss = 0.022069284692406654
iteration 73, loss = 0.032411839812994
iteration 74, loss = 0.02298630028963089
iteration 75, loss = 0.020783565938472748
iteration 76, loss = 0.025304293259978294
iteration 77, loss = 0.030093323439359665
iteration 78, loss = 0.021758640184998512
iteration 79, loss = 0.031594522297382355
iteration 80, loss = 0.022443193942308426
iteration 81, loss = 0.02182699181139469
iteration 82, loss = 0.02180357463657856
iteration 83, loss = 0.02473101019859314
iteration 84, loss = 0.021548042073845863
iteration 85, loss = 0.022303719073534012
iteration 86, loss = 0.021264653652906418
iteration 87, loss = 0.021379603073000908
iteration 88, loss = 0.023332009091973305
iteration 89, loss = 0.023058993741869926
iteration 90, loss = 0.025787673890590668
iteration 91, loss = 0.020612401887774467
iteration 92, loss = 0.02531237155199051
iteration 93, loss = 0.02325824461877346
iteration 94, loss = 0.021144073456525803
iteration 95, loss = 0.023654412478208542
iteration 96, loss = 0.026387033984065056
iteration 97, loss = 0.021263960748910904
iteration 98, loss = 0.021615387871861458
iteration 99, loss = 0.023788630962371826
iteration 100, loss = 0.02077995426952839
iteration 101, loss = 0.02312401682138443
iteration 102, loss = 0.020492641255259514
iteration 103, loss = 0.02261696383357048
iteration 104, loss = 0.029873903840780258
iteration 105, loss = 0.021529467776417732
iteration 106, loss = 0.02255014143884182
iteration 107, loss = 0.023683346807956696
iteration 108, loss = 0.02020978182554245
iteration 109, loss = 0.02087838388979435
iteration 110, loss = 0.02117764949798584
iteration 111, loss = 0.02021690085530281
iteration 112, loss = 0.02307692915201187
iteration 113, loss = 0.020484501495957375
iteration 114, loss = 0.021266432479023933
iteration 115, loss = 0.022688280791044235
iteration 116, loss = 0.022119149565696716
iteration 117, loss = 0.0217080470174551
iteration 118, loss = 0.021709300577640533
iteration 119, loss = 0.02176043577492237
iteration 120, loss = 0.0227157361805439
iteration 121, loss = 0.02046809531748295
iteration 122, loss = 0.020335940644145012
iteration 123, loss = 0.020929090678691864
iteration 124, loss = 0.02188124693930149
iteration 125, loss = 0.02102794498205185
iteration 126, loss = 0.02360570803284645
iteration 127, loss = 0.024342335760593414
iteration 128, loss = 0.02109365537762642
iteration 129, loss = 0.02206549607217312
iteration 130, loss = 0.022752512246370316
iteration 131, loss = 0.02473459765315056
iteration 132, loss = 0.021708521991968155
iteration 133, loss = 0.021418215706944466
iteration 134, loss = 0.02082165516912937
iteration 135, loss = 0.026139093562960625
iteration 136, loss = 0.020897405222058296
iteration 137, loss = 0.03472930192947388
iteration 138, loss = 0.021126121282577515
iteration 139, loss = 0.023649897426366806
iteration 140, loss = 0.022141434252262115
iteration 141, loss = 0.025754187256097794
iteration 142, loss = 0.026549026370048523
iteration 143, loss = 0.021366219967603683
iteration 144, loss = 0.022155366837978363
iteration 145, loss = 0.024684587493538857
iteration 146, loss = 0.020885800942778587
iteration 147, loss = 0.028303014114499092
iteration 148, loss = 0.022253088653087616
iteration 149, loss = 0.031576190143823624
iteration 150, loss = 0.024292176589369774
iteration 151, loss = 0.02247127704322338
iteration 152, loss = 0.021704329177737236
iteration 153, loss = 0.023721396923065186
iteration 154, loss = 0.02134815976023674
iteration 155, loss = 0.021184131503105164
iteration 156, loss = 0.022826921194791794
iteration 157, loss = 0.02476443722844124
iteration 158, loss = 0.021662624552845955
iteration 159, loss = 0.02227245457470417
iteration 160, loss = 0.02451840415596962
iteration 161, loss = 0.023748520761728287
iteration 162, loss = 0.023019708693027496
iteration 163, loss = 0.021559355780482292
iteration 164, loss = 0.03273279219865799
iteration 165, loss = 0.0223986953496933
iteration 166, loss = 0.023938238620758057
iteration 167, loss = 0.02022572048008442
iteration 168, loss = 0.02990126609802246
iteration 169, loss = 0.023030145093798637
iteration 170, loss = 0.026913801208138466
iteration 171, loss = 0.023428304120898247
iteration 172, loss = 0.023297078907489777
iteration 173, loss = 0.024433530867099762
iteration 174, loss = 0.024330226704478264
iteration 175, loss = 0.022575244307518005
iteration 176, loss = 0.022591860964894295
iteration 177, loss = 0.027287617325782776
iteration 178, loss = 0.031007882207632065
iteration 179, loss = 0.023262564092874527
iteration 180, loss = 0.021701864898204803
iteration 181, loss = 0.021964523941278458
iteration 182, loss = 0.02241215482354164
iteration 183, loss = 0.020873792469501495
iteration 184, loss = 0.020859306678175926
iteration 185, loss = 0.022433988749980927
iteration 186, loss = 0.020975764840841293
iteration 187, loss = 0.025115877389907837
iteration 188, loss = 0.02381204068660736
iteration 189, loss = 0.02089216560125351
iteration 190, loss = 0.020321335643529892
iteration 191, loss = 0.03186274319887161
iteration 192, loss = 0.023083575069904327
iteration 193, loss = 0.020847661420702934
iteration 194, loss = 0.03009064868092537
iteration 195, loss = 0.024352796375751495
iteration 196, loss = 0.020692311227321625
iteration 197, loss = 0.02115652523934841
iteration 198, loss = 0.020839622244238853
iteration 199, loss = 0.022851960733532906
iteration 200, loss = 0.02531438320875168
iteration 201, loss = 0.021361488848924637
iteration 202, loss = 0.027693388983607292
iteration 203, loss = 0.021401500329375267
iteration 204, loss = 0.024837125092744827
iteration 205, loss = 0.02130565047264099
iteration 206, loss = 0.020275676622986794
iteration 207, loss = 0.030786912888288498
iteration 208, loss = 0.021494360640645027
iteration 209, loss = 0.019947441294789314
iteration 210, loss = 0.032147474586963654
iteration 211, loss = 0.02081361785531044
iteration 212, loss = 0.019635017961263657
iteration 213, loss = 0.02111673355102539
iteration 214, loss = 0.02093701809644699
iteration 215, loss = 0.021600665524601936
iteration 216, loss = 0.020419873297214508
iteration 217, loss = 0.02069522812962532
iteration 218, loss = 0.021373232826590538
iteration 219, loss = 0.021059339866042137
iteration 220, loss = 0.021543430164456367
iteration 221, loss = 0.030421361327171326
iteration 222, loss = 0.019788814708590508
iteration 223, loss = 0.021631112322211266
iteration 224, loss = 0.021909207105636597
iteration 225, loss = 0.024168670177459717
iteration 226, loss = 0.021413136273622513
iteration 227, loss = 0.02172844111919403
iteration 228, loss = 0.02067708782851696
iteration 229, loss = 0.02425520122051239
iteration 230, loss = 0.02356923371553421
iteration 231, loss = 0.029790617525577545
iteration 232, loss = 0.0226398017257452
iteration 233, loss = 0.020740896463394165
iteration 234, loss = 0.023417562246322632
iteration 235, loss = 0.021400079131126404
iteration 236, loss = 0.020729029551148415
iteration 237, loss = 0.02480749785900116
iteration 238, loss = 0.021103600040078163
iteration 239, loss = 0.02222331240773201
iteration 240, loss = 0.032286107540130615
iteration 241, loss = 0.023544246330857277
iteration 242, loss = 0.03178652748465538
iteration 243, loss = 0.02035304717719555
iteration 244, loss = 0.022571224719285965
iteration 245, loss = 0.02086864598095417
iteration 246, loss = 0.020917823538184166
iteration 247, loss = 0.022354355081915855
iteration 248, loss = 0.021306121721863747
iteration 249, loss = 0.03442799299955368
iteration 250, loss = 0.031566545367240906
iteration 251, loss = 0.023363392800092697
iteration 252, loss = 0.021684641018509865
iteration 253, loss = 0.02087494358420372
iteration 254, loss = 0.03193431347608566
iteration 255, loss = 0.02713038958609104
iteration 256, loss = 0.02522622048854828
iteration 257, loss = 0.022372758015990257
iteration 258, loss = 0.040331289172172546
iteration 259, loss = 0.02526051737368107
iteration 260, loss = 0.030205758288502693
iteration 261, loss = 0.02108844742178917
iteration 262, loss = 0.02471300959587097
iteration 263, loss = 0.029441680759191513
iteration 264, loss = 0.023015540093183517
iteration 265, loss = 0.022656816989183426
iteration 266, loss = 0.02180006355047226
iteration 267, loss = 0.020247923210263252
iteration 268, loss = 0.020411713048815727
iteration 269, loss = 0.020419256761670113
iteration 270, loss = 0.02307906746864319
iteration 271, loss = 0.03209046646952629
iteration 272, loss = 0.022529441863298416
iteration 273, loss = 0.021939067170023918
iteration 274, loss = 0.020599104464054108
iteration 275, loss = 0.023538516834378242
iteration 276, loss = 0.03111990913748741
iteration 277, loss = 0.02184164710342884
iteration 278, loss = 0.020843878388404846
iteration 279, loss = 0.020996712148189545
iteration 280, loss = 0.0223018079996109
iteration 281, loss = 0.023237058892846107
iteration 282, loss = 0.022495651617646217
iteration 283, loss = 0.025257674977183342
iteration 284, loss = 0.020489109680056572
iteration 285, loss = 0.024367092177271843
iteration 286, loss = 0.021288059651851654
iteration 287, loss = 0.02183762565255165
iteration 288, loss = 0.022774308919906616
iteration 289, loss = 0.020659349858760834
iteration 290, loss = 0.032271768897771835
iteration 291, loss = 0.022478614002466202
iteration 292, loss = 0.022158050909638405
iteration 293, loss = 0.020956171676516533
iteration 294, loss = 0.03175146132707596
iteration 295, loss = 0.02482254058122635
iteration 296, loss = 0.042784690856933594
iteration 297, loss = 0.022936241701245308
iteration 298, loss = 0.022045202553272247
iteration 299, loss = 0.021062012761831284
iteration 300, loss = 0.020165709778666496
iteration 1, loss = 0.0205615796148777
iteration 2, loss = 0.020234905183315277
iteration 3, loss = 0.020373888313770294
iteration 4, loss = 0.021754832938313484
iteration 5, loss = 0.023536400869488716
iteration 6, loss = 0.02073916234076023
iteration 7, loss = 0.030874013900756836
iteration 8, loss = 0.02015676535665989
iteration 9, loss = 0.02054068073630333
iteration 10, loss = 0.02051144279539585
iteration 11, loss = 0.021056845784187317
iteration 12, loss = 0.022245025262236595
iteration 13, loss = 0.019802145659923553
iteration 14, loss = 0.01962990313768387
iteration 15, loss = 0.021595031023025513
iteration 16, loss = 0.02123691514134407
iteration 17, loss = 0.020887156948447227
iteration 18, loss = 0.020006336271762848
iteration 19, loss = 0.034753669053316116
iteration 20, loss = 0.02476109378039837
iteration 21, loss = 0.02101512998342514
iteration 22, loss = 0.021817345172166824
iteration 23, loss = 0.020798830315470695
iteration 24, loss = 0.02439681813120842
iteration 25, loss = 0.020010920241475105
iteration 26, loss = 0.020749781280755997
iteration 27, loss = 0.02518055960536003
iteration 28, loss = 0.02090499922633171
iteration 29, loss = 0.022785279899835587
iteration 30, loss = 0.020538058131933212
iteration 31, loss = 0.022664383053779602
iteration 32, loss = 0.02410244010388851
iteration 33, loss = 0.031448207795619965
iteration 34, loss = 0.021483972668647766
iteration 35, loss = 0.021580027416348457
iteration 36, loss = 0.022463107481598854
iteration 37, loss = 0.02077224850654602
iteration 38, loss = 0.02535371482372284
iteration 39, loss = 0.022287067025899887
iteration 40, loss = 0.02362504042685032
iteration 41, loss = 0.031949203461408615
iteration 42, loss = 0.020668745040893555
iteration 43, loss = 0.033539287745952606
iteration 44, loss = 0.02296367846429348
iteration 45, loss = 0.03017825447022915
iteration 46, loss = 0.020292803645133972
iteration 47, loss = 0.02111084572970867
iteration 48, loss = 0.022027142345905304
iteration 49, loss = 0.02082478441298008
iteration 50, loss = 0.02448074333369732
iteration 51, loss = 0.023023750633001328
iteration 52, loss = 0.023247865960001945
iteration 53, loss = 0.021903149783611298
iteration 54, loss = 0.021468594670295715
iteration 55, loss = 0.020370496436953545
iteration 56, loss = 0.020976733416318893
iteration 57, loss = 0.020592402666807175
iteration 58, loss = 0.023014571517705917
iteration 59, loss = 0.023563360795378685
iteration 60, loss = 0.020339548587799072
iteration 61, loss = 0.021075956523418427
iteration 62, loss = 0.02813943661749363
iteration 63, loss = 0.03207534924149513
iteration 64, loss = 0.02282547578215599
iteration 65, loss = 0.021276263520121574
iteration 66, loss = 0.02012069895863533
iteration 67, loss = 0.021532122045755386
iteration 68, loss = 0.02067212201654911
iteration 69, loss = 0.021857168525457382
iteration 70, loss = 0.023393843322992325
iteration 71, loss = 0.020673973485827446
iteration 72, loss = 0.021349672228097916
iteration 73, loss = 0.022314054891467094
iteration 74, loss = 0.02130465768277645
iteration 75, loss = 0.022313397377729416
iteration 76, loss = 0.02058802917599678
iteration 77, loss = 0.02175719104707241
iteration 78, loss = 0.02527528628706932
iteration 79, loss = 0.021085225045681
iteration 80, loss = 0.02212892659008503
iteration 81, loss = 0.021501479670405388
iteration 82, loss = 0.0216952096670866
iteration 83, loss = 0.03335804119706154
iteration 84, loss = 0.022019758820533752
iteration 85, loss = 0.024252090603113174
iteration 86, loss = 0.028814777731895447
iteration 87, loss = 0.023581821471452713
iteration 88, loss = 0.021932536736130714
iteration 89, loss = 0.028659429401159286
iteration 90, loss = 0.023967063054442406
iteration 91, loss = 0.020160099491477013
iteration 92, loss = 0.02363305166363716
iteration 93, loss = 0.02473914995789528
iteration 94, loss = 0.023894835263490677
iteration 95, loss = 0.021608866751194
iteration 96, loss = 0.021145954728126526
iteration 97, loss = 0.02063954994082451
iteration 98, loss = 0.021241530776023865
iteration 99, loss = 0.02467663399875164
iteration 100, loss = 0.021394096314907074
iteration 101, loss = 0.01994978077709675
iteration 102, loss = 0.020169679075479507
iteration 103, loss = 0.024155208840966225
iteration 104, loss = 0.02804522030055523
iteration 105, loss = 0.02175128273665905
iteration 106, loss = 0.021111445501446724
iteration 107, loss = 0.021334324032068253
iteration 108, loss = 0.019042490050196648
iteration 109, loss = 0.02057803049683571
iteration 110, loss = 0.02510269731283188
iteration 111, loss = 0.02950058877468109
iteration 112, loss = 0.02009657397866249
iteration 113, loss = 0.02937747724354267
iteration 114, loss = 0.020200323313474655
iteration 115, loss = 0.024361180141568184
iteration 116, loss = 0.022310195490717888
iteration 117, loss = 0.02090291678905487
iteration 118, loss = 0.02075326442718506
iteration 119, loss = 0.021232692524790764
iteration 120, loss = 0.03124399483203888
iteration 121, loss = 0.02166948840022087
iteration 122, loss = 0.02032277174293995
iteration 123, loss = 0.019621243700385094
iteration 124, loss = 0.019830254837870598
iteration 125, loss = 0.023861119523644447
iteration 126, loss = 0.020856384187936783
iteration 127, loss = 0.02368447557091713
iteration 128, loss = 0.020787622779607773
iteration 129, loss = 0.0231286883354187
iteration 130, loss = 0.02273726649582386
iteration 131, loss = 0.02212333120405674
iteration 132, loss = 0.022137248888611794
iteration 133, loss = 0.022125383839011192
iteration 134, loss = 0.022876041010022163
iteration 135, loss = 0.02575470320880413
iteration 136, loss = 0.01949131116271019
iteration 137, loss = 0.020220035687088966
iteration 138, loss = 0.01950976997613907
iteration 139, loss = 0.03114832192659378
iteration 140, loss = 0.02070213109254837
iteration 141, loss = 0.021427961066365242
iteration 142, loss = 0.02430807426571846
iteration 143, loss = 0.020935000851750374
iteration 144, loss = 0.020415572449564934
iteration 145, loss = 0.020724445581436157
iteration 146, loss = 0.022872645407915115
iteration 147, loss = 0.03877968713641167
iteration 148, loss = 0.024320775642991066
iteration 149, loss = 0.020445849746465683
iteration 150, loss = 0.025269800797104836
iteration 151, loss = 0.023266427218914032
iteration 152, loss = 0.019596219062805176
iteration 153, loss = 0.022453676909208298
iteration 154, loss = 0.02099808305501938
iteration 155, loss = 0.024345021694898605
iteration 156, loss = 0.02111436054110527
iteration 157, loss = 0.022567864507436752
iteration 158, loss = 0.02107144333422184
iteration 159, loss = 0.028831053525209427
iteration 160, loss = 0.020467134192585945
iteration 161, loss = 0.020739763975143433
iteration 162, loss = 0.021834447979927063
iteration 163, loss = 0.020897330716252327
iteration 164, loss = 0.023359697312116623
iteration 165, loss = 0.034057263284921646
iteration 166, loss = 0.021672897040843964
iteration 167, loss = 0.0229392908513546
iteration 168, loss = 0.02230558544397354
iteration 169, loss = 0.020717984065413475
iteration 170, loss = 0.020689982920885086
iteration 171, loss = 0.022359684109687805
iteration 172, loss = 0.0255181435495615
iteration 173, loss = 0.020257432013750076
iteration 174, loss = 0.019338613376021385
iteration 175, loss = 0.02482614666223526
iteration 176, loss = 0.02071189135313034
iteration 177, loss = 0.020967446267604828
iteration 178, loss = 0.020498616620898247
iteration 179, loss = 0.01965327002108097
iteration 180, loss = 0.02146383933722973
iteration 181, loss = 0.021455420181155205
iteration 182, loss = 0.0199924074113369
iteration 183, loss = 0.02463843673467636
iteration 184, loss = 0.0202830508351326
iteration 185, loss = 0.019979659467935562
iteration 186, loss = 0.02036220394074917
iteration 187, loss = 0.021640945225954056
iteration 188, loss = 0.0254074577242136
iteration 189, loss = 0.0297055896371603
iteration 190, loss = 0.02112872712314129
iteration 191, loss = 0.02952152118086815
iteration 192, loss = 0.023230254650115967
iteration 193, loss = 0.02028970792889595
iteration 194, loss = 0.020385485142469406
iteration 195, loss = 0.02585720643401146
iteration 196, loss = 0.021308831870555878
iteration 197, loss = 0.023928984999656677
iteration 198, loss = 0.02026989497244358
iteration 199, loss = 0.025504611432552338
iteration 200, loss = 0.021529169753193855
iteration 201, loss = 0.02551906183362007
iteration 202, loss = 0.019517188891768456
iteration 203, loss = 0.031888123601675034
iteration 204, loss = 0.022595619782805443
iteration 205, loss = 0.021916178986430168
iteration 206, loss = 0.022755375131964684
iteration 207, loss = 0.01987568475306034
iteration 208, loss = 0.0214894637465477
iteration 209, loss = 0.019661102443933487
iteration 210, loss = 0.03264959901571274
iteration 211, loss = 0.03043442592024803
iteration 212, loss = 0.024059129878878593
iteration 213, loss = 0.02981691062450409
iteration 214, loss = 0.019897742196917534
iteration 215, loss = 0.020335819572210312
iteration 216, loss = 0.023513533174991608
iteration 217, loss = 0.021287918090820312
iteration 218, loss = 0.021019376814365387
iteration 219, loss = 0.02508825808763504
iteration 220, loss = 0.019861362874507904
iteration 221, loss = 0.021373575553297997
iteration 222, loss = 0.03501247242093086
iteration 223, loss = 0.023050084710121155
iteration 224, loss = 0.024245044216513634
iteration 225, loss = 0.01912751980125904
iteration 226, loss = 0.02045990526676178
iteration 227, loss = 0.023053279146552086
iteration 228, loss = 0.02150574140250683
iteration 229, loss = 0.02300170809030533
iteration 230, loss = 0.021961219608783722
iteration 231, loss = 0.019800545647740364
iteration 232, loss = 0.019232749938964844
iteration 233, loss = 0.02059919573366642
iteration 234, loss = 0.02279224619269371
iteration 235, loss = 0.02070685848593712
iteration 236, loss = 0.019819200038909912
iteration 237, loss = 0.019387243315577507
iteration 238, loss = 0.028099425137043
iteration 239, loss = 0.023492753505706787
iteration 240, loss = 0.020564857870340347
iteration 241, loss = 0.02505340240895748
iteration 242, loss = 0.019277209416031837
iteration 243, loss = 0.025133753195405006
iteration 244, loss = 0.023703718557953835
iteration 245, loss = 0.020547106862068176
iteration 246, loss = 0.020091624930500984
iteration 247, loss = 0.02055269666016102
iteration 248, loss = 0.019778892397880554
iteration 249, loss = 0.023052141070365906
iteration 250, loss = 0.020439187064766884
iteration 251, loss = 0.02114543691277504
iteration 252, loss = 0.021898303180933
iteration 253, loss = 0.0193241648375988
iteration 254, loss = 0.022350212559103966
iteration 255, loss = 0.019807131960988045
iteration 256, loss = 0.02460825815796852
iteration 257, loss = 0.024366287514567375
iteration 258, loss = 0.022649697959423065
iteration 259, loss = 0.02316238544881344
iteration 260, loss = 0.024414964020252228
iteration 261, loss = 0.021080823615193367
iteration 262, loss = 0.02856353297829628
iteration 263, loss = 0.020997757092118263
iteration 264, loss = 0.019962424412369728
iteration 265, loss = 0.022762365639209747
iteration 266, loss = 0.0209805890917778
iteration 267, loss = 0.02029869146645069
iteration 268, loss = 0.018810436129570007
iteration 269, loss = 0.030212920159101486
iteration 270, loss = 0.019895507022738457
iteration 271, loss = 0.02049855887889862
iteration 272, loss = 0.019992025569081306
iteration 273, loss = 0.03109181858599186
iteration 274, loss = 0.021236596629023552
iteration 275, loss = 0.02024780958890915
iteration 276, loss = 0.02209087833762169
iteration 277, loss = 0.021222740411758423
iteration 278, loss = 0.02331678941845894
iteration 279, loss = 0.023233134299516678
iteration 280, loss = 0.020632179453969002
iteration 281, loss = 0.021641049534082413
iteration 282, loss = 0.02060246653854847
iteration 283, loss = 0.023524362593889236
iteration 284, loss = 0.024479014798998833
iteration 285, loss = 0.02125607430934906
iteration 286, loss = 0.020534150302410126
iteration 287, loss = 0.02418484352529049
iteration 288, loss = 0.020345188677310944
iteration 289, loss = 0.02982213720679283
iteration 290, loss = 0.02163965068757534
iteration 291, loss = 0.019983842968940735
iteration 292, loss = 0.02440205030143261
iteration 293, loss = 0.02013430930674076
iteration 294, loss = 0.01968352682888508
iteration 295, loss = 0.022478630766272545
iteration 296, loss = 0.021504037082195282
iteration 297, loss = 0.021178724244236946
iteration 298, loss = 0.023040616884827614
iteration 299, loss = 0.0199575237929821
iteration 300, loss = 0.019590336829423904
iteration 1, loss = 0.019442381337285042
iteration 2, loss = 0.01950470730662346
iteration 3, loss = 0.02143850363790989
iteration 4, loss = 0.02207578532397747
iteration 5, loss = 0.02043445222079754
iteration 6, loss = 0.021834643557667732
iteration 7, loss = 0.019774697721004486
iteration 8, loss = 0.019859451800584793
iteration 9, loss = 0.020535698160529137
iteration 10, loss = 0.02185038849711418
iteration 11, loss = 0.019370267167687416
iteration 12, loss = 0.021025732159614563
iteration 13, loss = 0.02301308885216713
iteration 14, loss = 0.018869072198867798
iteration 15, loss = 0.020934175699949265
iteration 16, loss = 0.021325726062059402
iteration 17, loss = 0.01984202302992344
iteration 18, loss = 0.023217450827360153
iteration 19, loss = 0.019915953278541565
iteration 20, loss = 0.019709844142198563
iteration 21, loss = 0.020588992163538933
iteration 22, loss = 0.01988060586154461
iteration 23, loss = 0.0199848935008049
iteration 24, loss = 0.03064333274960518
iteration 25, loss = 0.02355177327990532
iteration 26, loss = 0.021432915702462196
iteration 27, loss = 0.027412941679358482
iteration 28, loss = 0.019373824819922447
iteration 29, loss = 0.02597850002348423
iteration 30, loss = 0.025489872321486473
iteration 31, loss = 0.02060149982571602
iteration 32, loss = 0.024224143475294113
iteration 33, loss = 0.021398846060037613
iteration 34, loss = 0.02354413829743862
iteration 35, loss = 0.019783729687333107
iteration 36, loss = 0.020051410421729088
iteration 37, loss = 0.024866636842489243
iteration 38, loss = 0.02002943679690361
iteration 39, loss = 0.024692319333553314
iteration 40, loss = 0.021409152075648308
iteration 41, loss = 0.02121635526418686
iteration 42, loss = 0.025593064725399017
iteration 43, loss = 0.019734594970941544
iteration 44, loss = 0.02027975022792816
iteration 45, loss = 0.030287181958556175
iteration 46, loss = 0.020960334688425064
iteration 47, loss = 0.019774185493588448
iteration 48, loss = 0.026374876499176025
iteration 49, loss = 0.0188587699085474
iteration 50, loss = 0.020110664889216423
iteration 51, loss = 0.020289767533540726
iteration 52, loss = 0.028763407841324806
iteration 53, loss = 0.020733339712023735
iteration 54, loss = 0.019072294235229492
iteration 55, loss = 0.02142653986811638
iteration 56, loss = 0.019222047179937363
iteration 57, loss = 0.02219877764582634
iteration 58, loss = 0.02013828232884407
iteration 59, loss = 0.0208286065608263
iteration 60, loss = 0.022449679672718048
iteration 61, loss = 0.020047953352332115
iteration 62, loss = 0.01927909627556801
iteration 63, loss = 0.023984543979167938
iteration 64, loss = 0.019660333171486855
iteration 65, loss = 0.022608213126659393
iteration 66, loss = 0.021375391632318497
iteration 67, loss = 0.02060435339808464
iteration 68, loss = 0.022281091660261154
iteration 69, loss = 0.019661547616124153
iteration 70, loss = 0.020510699599981308
iteration 71, loss = 0.026005348190665245
iteration 72, loss = 0.022104807198047638
iteration 73, loss = 0.02417762018740177
iteration 74, loss = 0.019890081137418747
iteration 75, loss = 0.02976108528673649
iteration 76, loss = 0.02067847177386284
iteration 77, loss = 0.018969906494021416
iteration 78, loss = 0.022332653403282166
iteration 79, loss = 0.020686345174908638
iteration 80, loss = 0.019871309399604797
iteration 81, loss = 0.020818185061216354
iteration 82, loss = 0.022975971922278404
iteration 83, loss = 0.021087126806378365
iteration 84, loss = 0.022476602345705032
iteration 85, loss = 0.01987275294959545
iteration 86, loss = 0.022431815043091774
iteration 87, loss = 0.021284207701683044
iteration 88, loss = 0.022848492488265038
iteration 89, loss = 0.019335247576236725
iteration 90, loss = 0.030969327315688133
iteration 91, loss = 0.020403658971190453
iteration 92, loss = 0.01984420232474804
iteration 93, loss = 0.019906645640730858
iteration 94, loss = 0.022806912660598755
iteration 95, loss = 0.023697834461927414
iteration 96, loss = 0.02970021404325962
iteration 97, loss = 0.021498823538422585
iteration 98, loss = 0.020547688007354736
iteration 99, loss = 0.02042781375348568
iteration 100, loss = 0.02348288521170616
iteration 101, loss = 0.02127791754901409
iteration 102, loss = 0.022990001365542412
iteration 103, loss = 0.02789040468633175
iteration 104, loss = 0.022140314802527428
iteration 105, loss = 0.019232269376516342
iteration 106, loss = 0.02187109738588333
iteration 107, loss = 0.022180529311299324
iteration 108, loss = 0.022510161623358727
iteration 109, loss = 0.019748618826270103
iteration 110, loss = 0.022849377244710922
iteration 111, loss = 0.022610077634453773
iteration 112, loss = 0.021670565009117126
iteration 113, loss = 0.022152429446578026
iteration 114, loss = 0.029910100623965263
iteration 115, loss = 0.020444411784410477
iteration 116, loss = 0.020264076068997383
iteration 117, loss = 0.020801464095711708
iteration 118, loss = 0.030314134433865547
iteration 119, loss = 0.02009989134967327
iteration 120, loss = 0.01943259686231613
iteration 121, loss = 0.01892421580851078
iteration 122, loss = 0.020734429359436035
iteration 123, loss = 0.020236723124980927
iteration 124, loss = 0.03156694024801254
iteration 125, loss = 0.022277425974607468
iteration 126, loss = 0.02133616991341114
iteration 127, loss = 0.03149314969778061
iteration 128, loss = 0.019730964675545692
iteration 129, loss = 0.023988135159015656
iteration 130, loss = 0.01991317979991436
iteration 131, loss = 0.01972145028412342
iteration 132, loss = 0.020024802535772324
iteration 133, loss = 0.022069985046982765
iteration 134, loss = 0.026152024045586586
iteration 135, loss = 0.02154044434428215
iteration 136, loss = 0.01978226937353611
iteration 137, loss = 0.01941804029047489
iteration 138, loss = 0.01930432952940464
iteration 139, loss = 0.020079365000128746
iteration 140, loss = 0.018714820966124535
iteration 141, loss = 0.019616317003965378
iteration 142, loss = 0.022164445370435715
iteration 143, loss = 0.01921633817255497
iteration 144, loss = 0.02207419089972973
iteration 145, loss = 0.019219687208533287
iteration 146, loss = 0.019469156861305237
iteration 147, loss = 0.023296892642974854
iteration 148, loss = 0.021373368799686432
iteration 149, loss = 0.022777270525693893
iteration 150, loss = 0.020437542349100113
iteration 151, loss = 0.02312609925866127
iteration 152, loss = 0.01877904124557972
iteration 153, loss = 0.021239427849650383
iteration 154, loss = 0.020231401547789574
iteration 155, loss = 0.020361144095659256
iteration 156, loss = 0.029004506766796112
iteration 157, loss = 0.022651376202702522
iteration 158, loss = 0.02192392386496067
iteration 159, loss = 0.019707508385181427
iteration 160, loss = 0.020467907190322876
iteration 161, loss = 0.018568130210042
iteration 162, loss = 0.024090927094221115
iteration 163, loss = 0.019901204854249954
iteration 164, loss = 0.019679797813296318
iteration 165, loss = 0.020605409517884254
iteration 166, loss = 0.02087937481701374
iteration 167, loss = 0.022203871980309486
iteration 168, loss = 0.02177075669169426
iteration 169, loss = 0.02004127763211727
iteration 170, loss = 0.018873294815421104
iteration 171, loss = 0.021837569773197174
iteration 172, loss = 0.027871260419487953
iteration 173, loss = 0.028789097443223
iteration 174, loss = 0.02387990429997444
iteration 175, loss = 0.021105077117681503
iteration 176, loss = 0.019552428275346756
iteration 177, loss = 0.0199580118060112
iteration 178, loss = 0.024064868688583374
iteration 179, loss = 0.0219027791172266
iteration 180, loss = 0.021675851196050644
iteration 181, loss = 0.018386047333478928
iteration 182, loss = 0.020189782604575157
iteration 183, loss = 0.01908760704100132
iteration 184, loss = 0.021522708237171173
iteration 185, loss = 0.019270339980721474
iteration 186, loss = 0.020452026277780533
iteration 187, loss = 0.021374128758907318
iteration 188, loss = 0.028618251904845238
iteration 189, loss = 0.020206067711114883
iteration 190, loss = 0.022371279075741768
iteration 191, loss = 0.02097293734550476
iteration 192, loss = 0.019383275881409645
iteration 193, loss = 0.026592496782541275
iteration 194, loss = 0.020937779918313026
iteration 195, loss = 0.019746366888284683
iteration 196, loss = 0.02173597924411297
iteration 197, loss = 0.020169269293546677
iteration 198, loss = 0.019926777109503746
iteration 199, loss = 0.019807036966085434
iteration 200, loss = 0.01968195289373398
iteration 201, loss = 0.019797440618276596
iteration 202, loss = 0.02107015624642372
iteration 203, loss = 0.02955413982272148
iteration 204, loss = 0.019066622480750084
iteration 205, loss = 0.018777484074234962
iteration 206, loss = 0.019919270649552345
iteration 207, loss = 0.020849181339144707
iteration 208, loss = 0.01901489682495594
iteration 209, loss = 0.01961110532283783
iteration 210, loss = 0.02843419648706913
iteration 211, loss = 0.01967758685350418
iteration 212, loss = 0.021528396755456924
iteration 213, loss = 0.01896820217370987
iteration 214, loss = 0.029341069981455803
iteration 215, loss = 0.020273951813578606
iteration 216, loss = 0.02062736451625824
iteration 217, loss = 0.020797645673155785
iteration 218, loss = 0.020392239093780518
iteration 219, loss = 0.02380247227847576
iteration 220, loss = 0.023115389049053192
iteration 221, loss = 0.020754337310791016
iteration 222, loss = 0.020649872720241547
iteration 223, loss = 0.020468290895223618
iteration 224, loss = 0.022280683740973473
iteration 225, loss = 0.01918824017047882
iteration 226, loss = 0.020320113748311996
iteration 227, loss = 0.019913552328944206
iteration 228, loss = 0.02021847851574421
iteration 229, loss = 0.022092603147029877
iteration 230, loss = 0.02155541256070137
iteration 231, loss = 0.026292789727449417
iteration 232, loss = 0.023030223324894905
iteration 233, loss = 0.020595697686076164
iteration 234, loss = 0.02154712565243244
iteration 235, loss = 0.019502095878124237
iteration 236, loss = 0.019199207425117493
iteration 237, loss = 0.022694412618875504
iteration 238, loss = 0.019752971827983856
iteration 239, loss = 0.02397073805332184
iteration 240, loss = 0.024376891553401947
iteration 241, loss = 0.019489111378788948
iteration 242, loss = 0.020596249029040337
iteration 243, loss = 0.019996939226984978
iteration 244, loss = 0.020094189792871475
iteration 245, loss = 0.019441867247223854
iteration 246, loss = 0.02243509516119957
iteration 247, loss = 0.022890817373991013
iteration 248, loss = 0.03300558775663376
iteration 249, loss = 0.020548183470964432
iteration 250, loss = 0.022202258929610252
iteration 251, loss = 0.023194437846541405
iteration 252, loss = 0.023584900423884392
iteration 253, loss = 0.019116485491394997
iteration 254, loss = 0.027552204206585884
iteration 255, loss = 0.019404228776693344
iteration 256, loss = 0.021337192505598068
iteration 257, loss = 0.019579660147428513
iteration 258, loss = 0.02252425067126751
iteration 259, loss = 0.021698614582419395
iteration 260, loss = 0.03825821354985237
iteration 261, loss = 0.020270604640245438
iteration 262, loss = 0.021485190838575363
iteration 263, loss = 0.019645337015390396
iteration 264, loss = 0.021324509754776955
iteration 265, loss = 0.021171022206544876
iteration 266, loss = 0.020688878372311592
iteration 267, loss = 0.027883412316441536
iteration 268, loss = 0.019518177956342697
iteration 269, loss = 0.029267558827996254
iteration 270, loss = 0.01906796172261238
iteration 271, loss = 0.0220488328486681
iteration 272, loss = 0.019215261563658714
iteration 273, loss = 0.02948799543082714
iteration 274, loss = 0.020470812916755676
iteration 275, loss = 0.02294052764773369
iteration 276, loss = 0.020623356103897095
iteration 277, loss = 0.029211973771452904
iteration 278, loss = 0.02060834690928459
iteration 279, loss = 0.02108170837163925
iteration 280, loss = 0.02301514893770218
iteration 281, loss = 0.020499899983406067
iteration 282, loss = 0.024853302165865898
iteration 283, loss = 0.021785425022244453
iteration 284, loss = 0.019989056512713432
iteration 285, loss = 0.03847934678196907
iteration 286, loss = 0.02335907518863678
iteration 287, loss = 0.020793354138731956
iteration 288, loss = 0.02167109213769436
iteration 289, loss = 0.019393377006053925
iteration 290, loss = 0.024111704900860786
iteration 291, loss = 0.018079271540045738
iteration 292, loss = 0.019009627401828766
iteration 293, loss = 0.019533509388566017
iteration 294, loss = 0.02147028222680092
iteration 295, loss = 0.01864486373960972
iteration 296, loss = 0.019106753170490265
iteration 297, loss = 0.023694610223174095
iteration 298, loss = 0.030228905379772186
iteration 299, loss = 0.018802577629685402
iteration 300, loss = 0.01971334218978882
iteration 1, loss = 0.019340066239237785
iteration 2, loss = 0.021065251901745796
iteration 3, loss = 0.019591329619288445
iteration 4, loss = 0.0207431111484766
iteration 5, loss = 0.020123273134231567
iteration 6, loss = 0.02105022594332695
iteration 7, loss = 0.018936555832624435
iteration 8, loss = 0.020670151337981224
iteration 9, loss = 0.01871645636856556
iteration 10, loss = 0.01899128407239914
iteration 11, loss = 0.019307954236865044
iteration 12, loss = 0.021087156608700752
iteration 13, loss = 0.022567182779312134
iteration 14, loss = 0.020763104781508446
iteration 15, loss = 0.019963013008236885
iteration 16, loss = 0.021176422014832497
iteration 17, loss = 0.02069789171218872
iteration 18, loss = 0.023038744926452637
iteration 19, loss = 0.021568596363067627
iteration 20, loss = 0.020243151113390923
iteration 21, loss = 0.022363726049661636
iteration 22, loss = 0.022676073014736176
iteration 23, loss = 0.01939617097377777
iteration 24, loss = 0.0400875061750412
iteration 25, loss = 0.01950039528310299
iteration 26, loss = 0.01843281276524067
iteration 27, loss = 0.01992763765156269
iteration 28, loss = 0.019589809700846672
iteration 29, loss = 0.0195927657186985
iteration 30, loss = 0.0223219096660614
iteration 31, loss = 0.019065750762820244
iteration 32, loss = 0.02017012983560562
iteration 33, loss = 0.027202043682336807
iteration 34, loss = 0.028786128386855125
iteration 35, loss = 0.020554548129439354
iteration 36, loss = 0.02222246676683426
iteration 37, loss = 0.022138722240924835
iteration 38, loss = 0.020976528525352478
iteration 39, loss = 0.021311121061444283
iteration 40, loss = 0.020806247368454933
iteration 41, loss = 0.019603753462433815
iteration 42, loss = 0.019305530935525894
iteration 43, loss = 0.02036784030497074
iteration 44, loss = 0.01901732198894024
iteration 45, loss = 0.020488355308771133
iteration 46, loss = 0.021888088434934616
iteration 47, loss = 0.022442402318120003
iteration 48, loss = 0.019755808636546135
iteration 49, loss = 0.02198934368789196
iteration 50, loss = 0.019232092425227165
iteration 51, loss = 0.020227424800395966
iteration 52, loss = 0.02735137566924095
iteration 53, loss = 0.019714659079909325
iteration 54, loss = 0.020562168210744858
iteration 55, loss = 0.01963040418922901
iteration 56, loss = 0.020573433488607407
iteration 57, loss = 0.020141376182436943
iteration 58, loss = 0.020726215094327927
iteration 59, loss = 0.019324280321598053
iteration 60, loss = 0.025780659168958664
iteration 61, loss = 0.021395187824964523
iteration 62, loss = 0.019712058827280998
iteration 63, loss = 0.020485656335949898
iteration 64, loss = 0.019637057557702065
iteration 65, loss = 0.018636371940374374
iteration 66, loss = 0.018420880660414696
iteration 67, loss = 0.018382178619503975
iteration 68, loss = 0.01958317495882511
iteration 69, loss = 0.018091144040226936
iteration 70, loss = 0.01849730685353279
iteration 71, loss = 0.01905144937336445
iteration 72, loss = 0.02267390489578247
iteration 73, loss = 0.02077438309788704
iteration 74, loss = 0.02469584532082081
iteration 75, loss = 0.023592719808220863
iteration 76, loss = 0.018801046535372734
iteration 77, loss = 0.0213809534907341
iteration 78, loss = 0.01884569227695465
iteration 79, loss = 0.022022061049938202
iteration 80, loss = 0.02204000949859619
iteration 81, loss = 0.018845058977603912
iteration 82, loss = 0.022536378353834152
iteration 83, loss = 0.01876590959727764
iteration 84, loss = 0.02210637740790844
iteration 85, loss = 0.01899639144539833
iteration 86, loss = 0.021304119378328323
iteration 87, loss = 0.027905644848942757
iteration 88, loss = 0.02096995711326599
iteration 89, loss = 0.019294867292046547
iteration 90, loss = 0.01923471689224243
iteration 91, loss = 0.020022636279463768
iteration 92, loss = 0.02158692665398121
iteration 93, loss = 0.02210238017141819
iteration 94, loss = 0.020214064046740532
iteration 95, loss = 0.02148854359984398
iteration 96, loss = 0.0196831114590168
iteration 97, loss = 0.027110567316412926
iteration 98, loss = 0.021695571020245552
iteration 99, loss = 0.02035754732787609
iteration 100, loss = 0.019831432029604912
iteration 101, loss = 0.019587868824601173
iteration 102, loss = 0.018535763025283813
iteration 103, loss = 0.02143223024904728
iteration 104, loss = 0.018680106848478317
iteration 105, loss = 0.019012436270713806
iteration 106, loss = 0.01794685609638691
iteration 107, loss = 0.021522969007492065
iteration 108, loss = 0.02698165737092495
iteration 109, loss = 0.023324724286794662
iteration 110, loss = 0.01865697093307972
iteration 111, loss = 0.020248573273420334
iteration 112, loss = 0.020213786512613297
iteration 113, loss = 0.019905801862478256
iteration 114, loss = 0.02034166269004345
iteration 115, loss = 0.021486563608050346
iteration 116, loss = 0.01839076355099678
iteration 117, loss = 0.025180909782648087
iteration 118, loss = 0.01963823288679123
iteration 119, loss = 0.033389270305633545
iteration 120, loss = 0.019015010446310043
iteration 121, loss = 0.020110907033085823
iteration 122, loss = 0.01869300939142704
iteration 123, loss = 0.020833639428019524
iteration 124, loss = 0.031103968620300293
iteration 125, loss = 0.020677246153354645
iteration 126, loss = 0.01972785033285618
iteration 127, loss = 0.019993258640170097
iteration 128, loss = 0.019006529822945595
iteration 129, loss = 0.0233601201325655
iteration 130, loss = 0.01916923001408577
iteration 131, loss = 0.01906435191631317
iteration 132, loss = 0.019329173490405083
iteration 133, loss = 0.020512066781520844
iteration 134, loss = 0.02316373959183693
iteration 135, loss = 0.019402846693992615
iteration 136, loss = 0.022349024191498756
iteration 137, loss = 0.018894128501415253
iteration 138, loss = 0.027544807642698288
iteration 139, loss = 0.019947588443756104
iteration 140, loss = 0.022680265828967094
iteration 141, loss = 0.020232798531651497
iteration 142, loss = 0.018826212733983994
iteration 143, loss = 0.020495790988206863
iteration 144, loss = 0.02151886746287346
iteration 145, loss = 0.020757708698511124
iteration 146, loss = 0.021370483562350273
iteration 147, loss = 0.024976378306746483
iteration 148, loss = 0.01967936009168625
iteration 149, loss = 0.01901928335428238
iteration 150, loss = 0.022243961691856384
iteration 151, loss = 0.02988196723163128
iteration 152, loss = 0.018480924889445305
iteration 153, loss = 0.02000955492258072
iteration 154, loss = 0.020691292360424995
iteration 155, loss = 0.020397428423166275
iteration 156, loss = 0.022930581122636795
iteration 157, loss = 0.019586637616157532
iteration 158, loss = 0.01982845738530159
iteration 159, loss = 0.018037481233477592
iteration 160, loss = 0.02380038984119892
iteration 161, loss = 0.019637543708086014
iteration 162, loss = 0.01919664815068245
iteration 163, loss = 0.01960141956806183
iteration 164, loss = 0.019533701241016388
iteration 165, loss = 0.019466547295451164
iteration 166, loss = 0.02187163569033146
iteration 167, loss = 0.023356076329946518
iteration 168, loss = 0.01829318329691887
iteration 169, loss = 0.020545974373817444
iteration 170, loss = 0.02070765383541584
iteration 171, loss = 0.019587313756346703
iteration 172, loss = 0.020001715049147606
iteration 173, loss = 0.020092079415917397
iteration 174, loss = 0.029523568227887154
iteration 175, loss = 0.038266513496637344
iteration 176, loss = 0.02064383774995804
iteration 177, loss = 0.018750067800283432
iteration 178, loss = 0.023845713585615158
iteration 179, loss = 0.02024153620004654
iteration 180, loss = 0.019657306373119354
iteration 181, loss = 0.019444823265075684
iteration 182, loss = 0.023637402802705765
iteration 183, loss = 0.028339847922325134
iteration 184, loss = 0.01890494115650654
iteration 185, loss = 0.0192601028829813
iteration 186, loss = 0.018512006849050522
iteration 187, loss = 0.02270786464214325
iteration 188, loss = 0.020169448107481003
iteration 189, loss = 0.018957413733005524
iteration 190, loss = 0.022401917725801468
iteration 191, loss = 0.01895998977124691
iteration 192, loss = 0.033237650990486145
iteration 193, loss = 0.01931091956794262
iteration 194, loss = 0.019072333350777626
iteration 195, loss = 0.02033349499106407
iteration 196, loss = 0.02163105085492134
iteration 197, loss = 0.02837267518043518
iteration 198, loss = 0.0186674352735281
iteration 199, loss = 0.022678464651107788
iteration 200, loss = 0.019047701731324196
iteration 201, loss = 0.022709524258971214
iteration 202, loss = 0.02119445987045765
iteration 203, loss = 0.0221225805580616
iteration 204, loss = 0.032200951129198074
iteration 205, loss = 0.01843201369047165
iteration 206, loss = 0.019374577328562737
iteration 207, loss = 0.022751763463020325
iteration 208, loss = 0.020124688744544983
iteration 209, loss = 0.01867823489010334
iteration 210, loss = 0.02710174210369587
iteration 211, loss = 0.029830601066350937
iteration 212, loss = 0.021801963448524475
iteration 213, loss = 0.01860615238547325
iteration 214, loss = 0.0211581252515316
iteration 215, loss = 0.02029186487197876
iteration 216, loss = 0.0207139253616333
iteration 217, loss = 0.019245421513915062
iteration 218, loss = 0.02672864869236946
iteration 219, loss = 0.020018553361296654
iteration 220, loss = 0.019124040380120277
iteration 221, loss = 0.018476352095603943
iteration 222, loss = 0.020362578332424164
iteration 223, loss = 0.017753727734088898
iteration 224, loss = 0.018509255722165108
iteration 225, loss = 0.021049870178103447
iteration 226, loss = 0.019981198012828827
iteration 227, loss = 0.019304292276501656
iteration 228, loss = 0.029202748090028763
iteration 229, loss = 0.019896043464541435
iteration 230, loss = 0.03075273521244526
iteration 231, loss = 0.01799137517809868
iteration 232, loss = 0.018905136734247208
iteration 233, loss = 0.019156377762556076
iteration 234, loss = 0.018792252987623215
iteration 235, loss = 0.021516192704439163
iteration 236, loss = 0.018535664305090904
iteration 237, loss = 0.030039047822356224
iteration 238, loss = 0.030747782438993454
iteration 239, loss = 0.019300075247883797
iteration 240, loss = 0.019071541726589203
iteration 241, loss = 0.027208922430872917
iteration 242, loss = 0.019941098988056183
iteration 243, loss = 0.019511837512254715
iteration 244, loss = 0.018747875466942787
iteration 245, loss = 0.01917213387787342
iteration 246, loss = 0.01963263377547264
iteration 247, loss = 0.019739923998713493
iteration 248, loss = 0.020845629274845123
iteration 249, loss = 0.0187851469963789
iteration 250, loss = 0.019253527745604515
iteration 251, loss = 0.021783966571092606
iteration 252, loss = 0.02001718431711197
iteration 253, loss = 0.02100658416748047
iteration 254, loss = 0.019645536318421364
iteration 255, loss = 0.019578220322728157
iteration 256, loss = 0.018129020929336548
iteration 257, loss = 0.019616657868027687
iteration 258, loss = 0.021673042327165604
iteration 259, loss = 0.018549345433712006
iteration 260, loss = 0.02210942469537258
iteration 261, loss = 0.02029971219599247
iteration 262, loss = 0.020075123757123947
iteration 263, loss = 0.022283224388957024
iteration 264, loss = 0.02636147104203701
iteration 265, loss = 0.01923334412276745
iteration 266, loss = 0.019700344651937485
iteration 267, loss = 0.020443115383386612
iteration 268, loss = 0.018099229782819748
iteration 269, loss = 0.020904816687107086
iteration 270, loss = 0.018920067697763443
iteration 271, loss = 0.019113298505544662
iteration 272, loss = 0.021513376384973526
iteration 273, loss = 0.02186233177781105
iteration 274, loss = 0.021605627611279488
iteration 275, loss = 0.02151358872652054
iteration 276, loss = 0.019842099398374557
iteration 277, loss = 0.01766311749815941
iteration 278, loss = 0.01811373420059681
iteration 279, loss = 0.022002169862389565
iteration 280, loss = 0.02867802418768406
iteration 281, loss = 0.021961236372590065
iteration 282, loss = 0.019949663430452347
iteration 283, loss = 0.01913502812385559
iteration 284, loss = 0.019987115636467934
iteration 285, loss = 0.02300877496600151
iteration 286, loss = 0.02050831727683544
iteration 287, loss = 0.01843990758061409
iteration 288, loss = 0.01846727356314659
iteration 289, loss = 0.019577687606215477
iteration 290, loss = 0.018055366352200508
iteration 291, loss = 0.018843624740839005
iteration 292, loss = 0.021417131647467613
iteration 293, loss = 0.01956939324736595
iteration 294, loss = 0.02866869419813156
iteration 295, loss = 0.018924303352832794
iteration 296, loss = 0.019052304327487946
iteration 297, loss = 0.022447651252150536
iteration 298, loss = 0.02277546375989914
iteration 299, loss = 0.020291879773139954
iteration 300, loss = 0.01928800158202648
iteration 1, loss = 0.01926002837717533
iteration 2, loss = 0.018206676468253136
iteration 3, loss = 0.021036535501480103
iteration 4, loss = 0.021155333146452904
iteration 5, loss = 0.022189948707818985
iteration 6, loss = 0.02722548320889473
iteration 7, loss = 0.020709656178951263
iteration 8, loss = 0.018521346151828766
iteration 9, loss = 0.019646532833576202
iteration 10, loss = 0.018969766795635223
iteration 11, loss = 0.017735101282596588
iteration 12, loss = 0.018848249688744545
iteration 13, loss = 0.019583428278565407
iteration 14, loss = 0.018216127529740334
iteration 15, loss = 0.018774736672639847
iteration 16, loss = 0.020894426852464676
iteration 17, loss = 0.020343251526355743
iteration 18, loss = 0.019808566197752953
iteration 19, loss = 0.018370097503066063
iteration 20, loss = 0.019285066053271294
iteration 21, loss = 0.018963199108839035
iteration 22, loss = 0.01899755373597145
iteration 23, loss = 0.018314585089683533
iteration 24, loss = 0.018681738525629044
iteration 25, loss = 0.02874491550028324
iteration 26, loss = 0.02075907774269581
iteration 27, loss = 0.020436419174075127
iteration 28, loss = 0.020238064229488373
iteration 29, loss = 0.0206221304833889
iteration 30, loss = 0.018654318526387215
iteration 31, loss = 0.019472209736704826
iteration 32, loss = 0.022764889523386955
iteration 33, loss = 0.021671852096915245
iteration 34, loss = 0.018911132588982582
iteration 35, loss = 0.02488873340189457
iteration 36, loss = 0.01887870579957962
iteration 37, loss = 0.02289905771613121
iteration 38, loss = 0.02186822146177292
iteration 39, loss = 0.021826982498168945
iteration 40, loss = 0.020788252353668213
iteration 41, loss = 0.01974857598543167
iteration 42, loss = 0.02003706619143486
iteration 43, loss = 0.02079908177256584
iteration 44, loss = 0.019008656963706017
iteration 45, loss = 0.018996406346559525
iteration 46, loss = 0.021417612209916115
iteration 47, loss = 0.01991366222500801
iteration 48, loss = 0.02253020368516445
iteration 49, loss = 0.019546378403902054
iteration 50, loss = 0.019777953624725342
iteration 51, loss = 0.02092059701681137
iteration 52, loss = 0.018362397328019142
iteration 53, loss = 0.027857705950737
iteration 54, loss = 0.018532583490014076
iteration 55, loss = 0.019840184599161148
iteration 56, loss = 0.01935812458395958
iteration 57, loss = 0.018682783469557762
iteration 58, loss = 0.021369250491261482
iteration 59, loss = 0.018873685970902443
iteration 60, loss = 0.019401920959353447
iteration 61, loss = 0.019147424027323723
iteration 62, loss = 0.017782950773835182
iteration 63, loss = 0.021712128072977066
iteration 64, loss = 0.017360862344503403
iteration 65, loss = 0.02141398750245571
iteration 66, loss = 0.031422484666109085
iteration 67, loss = 0.018745452165603638
iteration 68, loss = 0.018639881163835526
iteration 69, loss = 0.018525155261158943
iteration 70, loss = 0.02123795635998249
iteration 71, loss = 0.018990155309438705
iteration 72, loss = 0.021453294903039932
iteration 73, loss = 0.018211670219898224
iteration 74, loss = 0.021634327247738838
iteration 75, loss = 0.0191762987524271
iteration 76, loss = 0.01998366415500641
iteration 77, loss = 0.01918177679181099
iteration 78, loss = 0.017981616780161858
iteration 79, loss = 0.017533954232931137
iteration 80, loss = 0.029980190098285675
iteration 81, loss = 0.022141316905617714
iteration 82, loss = 0.02059962786734104
iteration 83, loss = 0.020834730938076973
iteration 84, loss = 0.020513899624347687
iteration 85, loss = 0.0192580446600914
iteration 86, loss = 0.020517535507678986
iteration 87, loss = 0.02122359350323677
iteration 88, loss = 0.018353454768657684
iteration 89, loss = 0.02167278528213501
iteration 90, loss = 0.01822371780872345
iteration 91, loss = 0.01758817583322525
iteration 92, loss = 0.019053254276514053
iteration 93, loss = 0.01885054260492325
iteration 94, loss = 0.018634051084518433
iteration 95, loss = 0.020903443917632103
iteration 96, loss = 0.018133390694856644
iteration 97, loss = 0.018769020214676857
iteration 98, loss = 0.031076887622475624
iteration 99, loss = 0.020496485754847527
iteration 100, loss = 0.018538646399974823
iteration 101, loss = 0.020706254988908768
iteration 102, loss = 0.020649416372179985
iteration 103, loss = 0.019817329943180084
iteration 104, loss = 0.01858203299343586
iteration 105, loss = 0.02101607248187065
iteration 106, loss = 0.02824651263654232
iteration 107, loss = 0.01894502528011799
iteration 108, loss = 0.019070487469434738
iteration 109, loss = 0.018679596483707428
iteration 110, loss = 0.018807196989655495
iteration 111, loss = 0.01945834793150425
iteration 112, loss = 0.02799701876938343
iteration 113, loss = 0.020376358181238174
iteration 114, loss = 0.020853402093052864
iteration 115, loss = 0.021420862525701523
iteration 116, loss = 0.01799006015062332
iteration 117, loss = 0.019326101988554
iteration 118, loss = 0.02323094569146633
iteration 119, loss = 0.019840369001030922
iteration 120, loss = 0.02130996622145176
iteration 121, loss = 0.02450299635529518
iteration 122, loss = 0.01882978342473507
iteration 123, loss = 0.018731998279690742
iteration 124, loss = 0.019326377660036087
iteration 125, loss = 0.017666688188910484
iteration 126, loss = 0.02215312421321869
iteration 127, loss = 0.029166221618652344
iteration 128, loss = 0.02639469876885414
iteration 129, loss = 0.01793762296438217
iteration 130, loss = 0.018811404705047607
iteration 131, loss = 0.01841290295124054
iteration 132, loss = 0.02037382312119007
iteration 133, loss = 0.020976001396775246
iteration 134, loss = 0.021618247032165527
iteration 135, loss = 0.018521592020988464
iteration 136, loss = 0.01768479309976101
iteration 137, loss = 0.021907417103648186
iteration 138, loss = 0.027930675074458122
iteration 139, loss = 0.018904875963926315
iteration 140, loss = 0.019027644768357277
iteration 141, loss = 0.01858140528202057
iteration 142, loss = 0.020197011530399323
iteration 143, loss = 0.022239187732338905
iteration 144, loss = 0.017506904900074005
iteration 145, loss = 0.02078283205628395
iteration 146, loss = 0.020851675420999527
iteration 147, loss = 0.019893839955329895
iteration 148, loss = 0.021386470645666122
iteration 149, loss = 0.019879566505551338
iteration 150, loss = 0.02012941800057888
iteration 151, loss = 0.02130468562245369
iteration 152, loss = 0.020725438371300697
iteration 153, loss = 0.02743406966328621
iteration 154, loss = 0.018339572474360466
iteration 155, loss = 0.026771876960992813
iteration 156, loss = 0.03531470149755478
iteration 157, loss = 0.027557983994483948
iteration 158, loss = 0.026918189600110054
iteration 159, loss = 0.01840769499540329
iteration 160, loss = 0.018169574439525604
iteration 161, loss = 0.020967725664377213
iteration 162, loss = 0.030762184411287308
iteration 163, loss = 0.018021749332547188
iteration 164, loss = 0.020038031041622162
iteration 165, loss = 0.03141847997903824
iteration 166, loss = 0.026738379150629044
iteration 167, loss = 0.021702252328395844
iteration 168, loss = 0.019389474764466286
iteration 169, loss = 0.018159447237849236
iteration 170, loss = 0.02129475213587284
iteration 171, loss = 0.01776745729148388
iteration 172, loss = 0.018055496737360954
iteration 173, loss = 0.01892429031431675
iteration 174, loss = 0.024412527680397034
iteration 175, loss = 0.0218698438256979
iteration 176, loss = 0.019039804115891457
iteration 177, loss = 0.017717737704515457
iteration 178, loss = 0.025350067764520645
iteration 179, loss = 0.018079422414302826
iteration 180, loss = 0.019949063658714294
iteration 181, loss = 0.018371568992733955
iteration 182, loss = 0.017572224140167236
iteration 183, loss = 0.017459288239479065
iteration 184, loss = 0.023984983563423157
iteration 185, loss = 0.018701063469052315
iteration 186, loss = 0.02043813467025757
iteration 187, loss = 0.027817903086543083
iteration 188, loss = 0.018060382455587387
iteration 189, loss = 0.021128658205270767
iteration 190, loss = 0.018016472458839417
iteration 191, loss = 0.020819829776883125
iteration 192, loss = 0.01849008910357952
iteration 193, loss = 0.019083544611930847
iteration 194, loss = 0.01857314258813858
iteration 195, loss = 0.01836443692445755
iteration 196, loss = 0.017964810132980347
iteration 197, loss = 0.01879769004881382
iteration 198, loss = 0.022152826189994812
iteration 199, loss = 0.01999662257730961
iteration 200, loss = 0.018209373578429222
iteration 201, loss = 0.017884086817502975
iteration 202, loss = 0.02806834876537323
iteration 203, loss = 0.017478588968515396
iteration 204, loss = 0.019796790555119514
iteration 205, loss = 0.02273084595799446
iteration 206, loss = 0.018570061773061752
iteration 207, loss = 0.017724717035889626
iteration 208, loss = 0.020462853834033012
iteration 209, loss = 0.021093061193823814
iteration 210, loss = 0.019666403532028198
iteration 211, loss = 0.01903298869729042
iteration 212, loss = 0.02262893132865429
iteration 213, loss = 0.017805445939302444
iteration 214, loss = 0.0183241106569767
iteration 215, loss = 0.026662996038794518
iteration 216, loss = 0.01902218908071518
iteration 217, loss = 0.028316909447312355
iteration 218, loss = 0.018804222345352173
iteration 219, loss = 0.018343396484851837
iteration 220, loss = 0.018328610807657242
iteration 221, loss = 0.02119857259094715
iteration 222, loss = 0.01861700415611267
iteration 223, loss = 0.019068384543061256
iteration 224, loss = 0.021388713270425797
iteration 225, loss = 0.018703199923038483
iteration 226, loss = 0.017796769738197327
iteration 227, loss = 0.018352346494793892
iteration 228, loss = 0.028397930786013603
iteration 229, loss = 0.018386000767350197
iteration 230, loss = 0.020276907831430435
iteration 231, loss = 0.01723823882639408
iteration 232, loss = 0.02203022874891758
iteration 233, loss = 0.01937813311815262
iteration 234, loss = 0.018059205263853073
iteration 235, loss = 0.019055649638175964
iteration 236, loss = 0.01851329579949379
iteration 237, loss = 0.018550148233771324
iteration 238, loss = 0.018292458727955818
iteration 239, loss = 0.0292220339179039
iteration 240, loss = 0.0188189297914505
iteration 241, loss = 0.018695533275604248
iteration 242, loss = 0.020112812519073486
iteration 243, loss = 0.017493439838290215
iteration 244, loss = 0.01894054189324379
iteration 245, loss = 0.0191710963845253
iteration 246, loss = 0.01897415518760681
iteration 247, loss = 0.01921795681118965
iteration 248, loss = 0.020273983478546143
iteration 249, loss = 0.017867108806967735
iteration 250, loss = 0.020330948755145073
iteration 251, loss = 0.018128974363207817
iteration 252, loss = 0.026096375659108162
iteration 253, loss = 0.020593838766217232
iteration 254, loss = 0.019964247941970825
iteration 255, loss = 0.028487762436270714
iteration 256, loss = 0.019345417618751526
iteration 257, loss = 0.021276161074638367
iteration 258, loss = 0.01776663027703762
iteration 259, loss = 0.0205681174993515
iteration 260, loss = 0.019886920228600502
iteration 261, loss = 0.01793147251009941
iteration 262, loss = 0.021067796275019646
iteration 263, loss = 0.01959376223385334
iteration 264, loss = 0.01730423793196678
iteration 265, loss = 0.019075237214565277
iteration 266, loss = 0.018047435209155083
iteration 267, loss = 0.016885897144675255
iteration 268, loss = 0.021345805376768112
iteration 269, loss = 0.018994541838765144
iteration 270, loss = 0.01801549829542637
iteration 271, loss = 0.018712863326072693
iteration 272, loss = 0.019033795222640038
iteration 273, loss = 0.018799621611833572
iteration 274, loss = 0.022380219772458076
iteration 275, loss = 0.01848515495657921
iteration 276, loss = 0.020527802407741547
iteration 277, loss = 0.02020246535539627
iteration 278, loss = 0.018977534025907516
iteration 279, loss = 0.021112458780407906
iteration 280, loss = 0.022294070571660995
iteration 281, loss = 0.022119909524917603
iteration 282, loss = 0.017267759889364243
iteration 283, loss = 0.020933646708726883
iteration 284, loss = 0.02069927006959915
iteration 285, loss = 0.030554812401533127
iteration 286, loss = 0.017413537949323654
iteration 287, loss = 0.017996346578001976
iteration 288, loss = 0.020032770931720734
iteration 289, loss = 0.018422603607177734
iteration 290, loss = 0.017867231741547585
iteration 291, loss = 0.018073810264468193
iteration 292, loss = 0.020440174266695976
iteration 293, loss = 0.018153415992856026
iteration 294, loss = 0.01914137788116932
iteration 295, loss = 0.01857103779911995
iteration 296, loss = 0.019252397119998932
iteration 297, loss = 0.020822059363126755
iteration 298, loss = 0.0186624675989151
iteration 299, loss = 0.01845911145210266
iteration 300, loss = 0.019305773079395294
iteration 1, loss = 0.01772519201040268
iteration 2, loss = 0.01911497674882412
iteration 3, loss = 0.01905062608420849
iteration 4, loss = 0.01938774809241295
iteration 5, loss = 0.018157826736569405
iteration 6, loss = 0.017919372767210007
iteration 7, loss = 0.01836724951863289
iteration 8, loss = 0.018917469307780266
iteration 9, loss = 0.022301390767097473
iteration 10, loss = 0.01911144331097603
iteration 11, loss = 0.019580725580453873
iteration 12, loss = 0.018540216609835625
iteration 13, loss = 0.017466099932789803
iteration 14, loss = 0.017397580668330193
iteration 15, loss = 0.01866382174193859
iteration 16, loss = 0.0209449864923954
iteration 17, loss = 0.020538171753287315
iteration 18, loss = 0.018016407266259193
iteration 19, loss = 0.017685620114207268
iteration 20, loss = 0.018196601420640945
iteration 21, loss = 0.019730066880583763
iteration 22, loss = 0.017916036769747734
iteration 23, loss = 0.019396739080548286
iteration 24, loss = 0.019172152504324913
iteration 25, loss = 0.01770179346203804
iteration 26, loss = 0.017761556431651115
iteration 27, loss = 0.019529001787304878
iteration 28, loss = 0.018519992008805275
iteration 29, loss = 0.01840328611433506
iteration 30, loss = 0.01831107959151268
iteration 31, loss = 0.019686739891767502
iteration 32, loss = 0.01853276789188385
iteration 33, loss = 0.018665580078959465
iteration 34, loss = 0.023515259847044945
iteration 35, loss = 0.019033759832382202
iteration 36, loss = 0.018119201064109802
iteration 37, loss = 0.01813129521906376
iteration 38, loss = 0.019559666514396667
iteration 39, loss = 0.019916627556085587
iteration 40, loss = 0.018272023648023605
iteration 41, loss = 0.018869835883378983
iteration 42, loss = 0.018140465021133423
iteration 43, loss = 0.02029888890683651
iteration 44, loss = 0.018689433112740517
iteration 45, loss = 0.01836688071489334
iteration 46, loss = 0.021705761551856995
iteration 47, loss = 0.01886736787855625
iteration 48, loss = 0.01861230842769146
iteration 49, loss = 0.018082724884152412
iteration 50, loss = 0.027752093970775604
iteration 51, loss = 0.01978657767176628
iteration 52, loss = 0.018460558727383614
iteration 53, loss = 0.02321375533938408
iteration 54, loss = 0.01801886223256588
iteration 55, loss = 0.018892982974648476
iteration 56, loss = 0.022756332531571388
iteration 57, loss = 0.01967838779091835
iteration 58, loss = 0.017145654186606407
iteration 59, loss = 0.01811044104397297
iteration 60, loss = 0.018536152318120003
iteration 61, loss = 0.024519478902220726
iteration 62, loss = 0.020123586058616638
iteration 63, loss = 0.018555663526058197
iteration 64, loss = 0.018753325566649437
iteration 65, loss = 0.019555557519197464
iteration 66, loss = 0.017493853345513344
iteration 67, loss = 0.020612535998225212
iteration 68, loss = 0.017834821715950966
iteration 69, loss = 0.01887528784573078
iteration 70, loss = 0.019326280802488327
iteration 71, loss = 0.01853356696665287
iteration 72, loss = 0.02161853015422821
iteration 73, loss = 0.019004905596375465
iteration 74, loss = 0.018606483936309814
iteration 75, loss = 0.018139446154236794
iteration 76, loss = 0.019231712445616722
iteration 77, loss = 0.02106289379298687
iteration 78, loss = 0.02155843935906887
iteration 79, loss = 0.029847130179405212
iteration 80, loss = 0.018789678812026978
iteration 81, loss = 0.019288714975118637
iteration 82, loss = 0.02125542424619198
iteration 83, loss = 0.01985086314380169
iteration 84, loss = 0.020257990807294846
iteration 85, loss = 0.029069505631923676
iteration 86, loss = 0.017959611490368843
iteration 87, loss = 0.01811087317764759
iteration 88, loss = 0.02233467623591423
iteration 89, loss = 0.01946696825325489
iteration 90, loss = 0.020051000639796257
iteration 91, loss = 0.02018781192600727
iteration 92, loss = 0.019857147708535194
iteration 93, loss = 0.018133340403437614
iteration 94, loss = 0.018944690003991127
iteration 95, loss = 0.019448688253760338
iteration 96, loss = 0.01945338398218155
iteration 97, loss = 0.02694038301706314
iteration 98, loss = 0.017676111310720444
iteration 99, loss = 0.023632047697901726
iteration 100, loss = 0.01819821074604988
iteration 101, loss = 0.021507250145077705
iteration 102, loss = 0.018473606556653976
iteration 103, loss = 0.03044014610350132
iteration 104, loss = 0.01740427315235138
iteration 105, loss = 0.020994940772652626
iteration 106, loss = 0.017926735803484917
iteration 107, loss = 0.018412860110402107
iteration 108, loss = 0.01831134781241417
iteration 109, loss = 0.01778387650847435
iteration 110, loss = 0.018709784373641014
iteration 111, loss = 0.019011572003364563
iteration 112, loss = 0.01750144176185131
iteration 113, loss = 0.018577830865979195
iteration 114, loss = 0.017653947696089745
iteration 115, loss = 0.021521583199501038
iteration 116, loss = 0.021580975502729416
iteration 117, loss = 0.017749622464179993
iteration 118, loss = 0.018810149282217026
iteration 119, loss = 0.017908504232764244
iteration 120, loss = 0.01988080143928528
iteration 121, loss = 0.018333612009882927
iteration 122, loss = 0.017410922795534134
iteration 123, loss = 0.021755075082182884
iteration 124, loss = 0.02055465616285801
iteration 125, loss = 0.018010446801781654
iteration 126, loss = 0.02121713198721409
iteration 127, loss = 0.01752035692334175
iteration 128, loss = 0.02567370980978012
iteration 129, loss = 0.01960882544517517
iteration 130, loss = 0.01799914427101612
iteration 131, loss = 0.017544982954859734
iteration 132, loss = 0.01911051571369171
iteration 133, loss = 0.017416108399629593
iteration 134, loss = 0.01764451153576374
iteration 135, loss = 0.019152970984578133
iteration 136, loss = 0.020861109718680382
iteration 137, loss = 0.017085323110222816
iteration 138, loss = 0.028207480907440186
iteration 139, loss = 0.02715771645307541
iteration 140, loss = 0.022233210504055023
iteration 141, loss = 0.019238509237766266
iteration 142, loss = 0.017773140221834183
iteration 143, loss = 0.019025418907403946
iteration 144, loss = 0.01829848624765873
iteration 145, loss = 0.017156459391117096
iteration 146, loss = 0.01835118606686592
iteration 147, loss = 0.018656838685274124
iteration 148, loss = 0.018580123782157898
iteration 149, loss = 0.020619334653019905
iteration 150, loss = 0.026012571528553963
iteration 151, loss = 0.01688791997730732
iteration 152, loss = 0.017663221806287766
iteration 153, loss = 0.019704550504684448
iteration 154, loss = 0.01892797462642193
iteration 155, loss = 0.018001211807131767
iteration 156, loss = 0.018110889941453934
iteration 157, loss = 0.018281374126672745
iteration 158, loss = 0.01786559820175171
iteration 159, loss = 0.01787135750055313
iteration 160, loss = 0.018168682232499123
iteration 161, loss = 0.018187308683991432
iteration 162, loss = 0.028791528195142746
iteration 163, loss = 0.017483659088611603
iteration 164, loss = 0.016637858003377914
iteration 165, loss = 0.02166825346648693
iteration 166, loss = 0.018205763772130013
iteration 167, loss = 0.020220594480633736
iteration 168, loss = 0.020242098718881607
iteration 169, loss = 0.018523255363106728
iteration 170, loss = 0.018277844414114952
iteration 171, loss = 0.017996985465288162
iteration 172, loss = 0.02067360281944275
iteration 173, loss = 0.01885278895497322
iteration 174, loss = 0.018512189388275146
iteration 175, loss = 0.017063654959201813
iteration 176, loss = 0.01911989599466324
iteration 177, loss = 0.01797659695148468
iteration 178, loss = 0.01798035390675068
iteration 179, loss = 0.017100531607866287
iteration 180, loss = 0.02078392170369625
iteration 181, loss = 0.01699100434780121
iteration 182, loss = 0.018036488443613052
iteration 183, loss = 0.02502976916730404
iteration 184, loss = 0.018560772761702538
iteration 185, loss = 0.021826205775141716
iteration 186, loss = 0.02651730552315712
iteration 187, loss = 0.018605904653668404
iteration 188, loss = 0.0250712763518095
iteration 189, loss = 0.017479289323091507
iteration 190, loss = 0.029667332768440247
iteration 191, loss = 0.018108105286955833
iteration 192, loss = 0.01814928837120533
iteration 193, loss = 0.027636131271719933
iteration 194, loss = 0.017479276284575462
iteration 195, loss = 0.018470510840415955
iteration 196, loss = 0.01886099949479103
iteration 197, loss = 0.027201034128665924
iteration 198, loss = 0.019779467955231667
iteration 199, loss = 0.02882891520857811
iteration 200, loss = 0.017709964886307716
iteration 201, loss = 0.017304405570030212
iteration 202, loss = 0.016844840720295906
iteration 203, loss = 0.018811311572790146
iteration 204, loss = 0.01905183494091034
iteration 205, loss = 0.019460836425423622
iteration 206, loss = 0.017007844522595406
iteration 207, loss = 0.01749860681593418
iteration 208, loss = 0.01905902475118637
iteration 209, loss = 0.02148071862757206
iteration 210, loss = 0.017031671479344368
iteration 211, loss = 0.016934240236878395
iteration 212, loss = 0.018482280895113945
iteration 213, loss = 0.020305313169956207
iteration 214, loss = 0.017832275480031967
iteration 215, loss = 0.01926007680594921
iteration 216, loss = 0.018340017646551132
iteration 217, loss = 0.027475345879793167
iteration 218, loss = 0.036421000957489014
iteration 219, loss = 0.019156480208039284
iteration 220, loss = 0.017886776477098465
iteration 221, loss = 0.020461488515138626
iteration 222, loss = 0.018912523984909058
iteration 223, loss = 0.018243394792079926
iteration 224, loss = 0.020791711285710335
iteration 225, loss = 0.01940540410578251
iteration 226, loss = 0.01831912435591221
iteration 227, loss = 0.023254159837961197
iteration 228, loss = 0.018575338646769524
iteration 229, loss = 0.033272884786129
iteration 230, loss = 0.02030479721724987
iteration 231, loss = 0.017123090103268623
iteration 232, loss = 0.0254388265311718
iteration 233, loss = 0.017647800967097282
iteration 234, loss = 0.018807779997587204
iteration 235, loss = 0.0170388612896204
iteration 236, loss = 0.016839485615491867
iteration 237, loss = 0.019484641030430794
iteration 238, loss = 0.018077753484249115
iteration 239, loss = 0.01882958598434925
iteration 240, loss = 0.01908200979232788
iteration 241, loss = 0.02102939784526825
iteration 242, loss = 0.02061878703534603
iteration 243, loss = 0.017378509044647217
iteration 244, loss = 0.01768646389245987
iteration 245, loss = 0.02162194810807705
iteration 246, loss = 0.02168041281402111
iteration 247, loss = 0.01751045323908329
iteration 248, loss = 0.01813596673309803
iteration 249, loss = 0.01944369077682495
iteration 250, loss = 0.01899421587586403
iteration 251, loss = 0.018492436036467552
iteration 252, loss = 0.022362254559993744
iteration 253, loss = 0.01939862221479416
iteration 254, loss = 0.017778420820832253
iteration 255, loss = 0.019720062613487244
iteration 256, loss = 0.018927212804555893
iteration 257, loss = 0.018554551526904106
iteration 258, loss = 0.01870010979473591
iteration 259, loss = 0.018174367025494576
iteration 260, loss = 0.017005732282996178
iteration 261, loss = 0.01661844179034233
iteration 262, loss = 0.020632369443774223
iteration 263, loss = 0.016908662393689156
iteration 264, loss = 0.029200468212366104
iteration 265, loss = 0.018272293731570244
iteration 266, loss = 0.02155045047402382
iteration 267, loss = 0.027822287753224373
iteration 268, loss = 0.018954994156956673
iteration 269, loss = 0.017805878072977066
iteration 270, loss = 0.01939212903380394
iteration 271, loss = 0.01806793361902237
iteration 272, loss = 0.017447272315621376
iteration 273, loss = 0.016828937456011772
iteration 274, loss = 0.021794429048895836
iteration 275, loss = 0.022353870794177055
iteration 276, loss = 0.01812446303665638
iteration 277, loss = 0.02162828855216503
iteration 278, loss = 0.017354443669319153
iteration 279, loss = 0.019847720861434937
iteration 280, loss = 0.016992973163723946
iteration 281, loss = 0.01693347841501236
iteration 282, loss = 0.02683299034833908
iteration 283, loss = 0.020361527800559998
iteration 284, loss = 0.02797783724963665
iteration 285, loss = 0.018866341561079025
iteration 286, loss = 0.026557868346571922
iteration 287, loss = 0.018975604325532913
iteration 288, loss = 0.021120674908161163
iteration 289, loss = 0.017106255516409874
iteration 290, loss = 0.016876697540283203
iteration 291, loss = 0.019675375893712044
iteration 292, loss = 0.01997189223766327
iteration 293, loss = 0.026185009628534317
iteration 294, loss = 0.01788013055920601
iteration 295, loss = 0.017596902325749397
iteration 296, loss = 0.02030881680548191
iteration 297, loss = 0.017686108127236366
iteration 298, loss = 0.025516530498862267
iteration 299, loss = 0.018893111497163773
iteration 300, loss = 0.017097122967243195
iteration 1, loss = 0.017471788451075554
iteration 2, loss = 0.018760256469249725
iteration 3, loss = 0.02893766015768051
iteration 4, loss = 0.01924150437116623
iteration 5, loss = 0.018871605396270752
iteration 6, loss = 0.019930655136704445
iteration 7, loss = 0.018992042168974876
iteration 8, loss = 0.01742585003376007
iteration 9, loss = 0.018610594794154167
iteration 10, loss = 0.019737251102924347
iteration 11, loss = 0.0176105760037899
iteration 12, loss = 0.020407674834132195
iteration 13, loss = 0.02631964161992073
iteration 14, loss = 0.017082633450627327
iteration 15, loss = 0.023927554488182068
iteration 16, loss = 0.017872406169772148
iteration 17, loss = 0.019251875579357147
iteration 18, loss = 0.026438910514116287
iteration 19, loss = 0.016687113791704178
iteration 20, loss = 0.0362839475274086
iteration 21, loss = 0.01919371262192726
iteration 22, loss = 0.01780940592288971
iteration 23, loss = 0.01898285746574402
iteration 24, loss = 0.02084808610379696
iteration 25, loss = 0.016501082107424736
iteration 26, loss = 0.018525414168834686
iteration 27, loss = 0.0193178690969944
iteration 28, loss = 0.02699659764766693
iteration 29, loss = 0.019128454849123955
iteration 30, loss = 0.018796829506754875
iteration 31, loss = 0.026337604969739914
iteration 32, loss = 0.020271586254239082
iteration 33, loss = 0.018485698848962784
iteration 34, loss = 0.017922811210155487
iteration 35, loss = 0.018231628462672234
iteration 36, loss = 0.020032890141010284
iteration 37, loss = 0.01775803603231907
iteration 38, loss = 0.02054934948682785
iteration 39, loss = 0.021971041336655617
iteration 40, loss = 0.01918054185807705
iteration 41, loss = 0.017877066507935524
iteration 42, loss = 0.01774653233587742
iteration 43, loss = 0.026212139055132866
iteration 44, loss = 0.018190640956163406
iteration 45, loss = 0.016994675621390343
iteration 46, loss = 0.018239805474877357
iteration 47, loss = 0.01952529326081276
iteration 48, loss = 0.019151844084262848
iteration 49, loss = 0.021786129102110863
iteration 50, loss = 0.017656847834587097
iteration 51, loss = 0.019376099109649658
iteration 52, loss = 0.021793372929096222
iteration 53, loss = 0.01766037568449974
iteration 54, loss = 0.01976480521261692
iteration 55, loss = 0.01835300400853157
iteration 56, loss = 0.017083782702684402
iteration 57, loss = 0.016976334154605865
iteration 58, loss = 0.016830258071422577
iteration 59, loss = 0.016786998137831688
iteration 60, loss = 0.02441469579935074
iteration 61, loss = 0.01911361701786518
iteration 62, loss = 0.01788860373198986
iteration 63, loss = 0.017743539065122604
iteration 64, loss = 0.01810108684003353
iteration 65, loss = 0.017648929730057716
iteration 66, loss = 0.018911931663751602
iteration 67, loss = 0.01894456334412098
iteration 68, loss = 0.01735970564186573
iteration 69, loss = 0.016386045143008232
iteration 70, loss = 0.021188048645853996
iteration 71, loss = 0.02478889562189579
iteration 72, loss = 0.016389328986406326
iteration 73, loss = 0.020486706867814064
iteration 74, loss = 0.017755722627043724
iteration 75, loss = 0.01860935240983963
iteration 76, loss = 0.016683615744113922
iteration 77, loss = 0.026112496852874756
iteration 78, loss = 0.01768646389245987
iteration 79, loss = 0.018642878159880638
iteration 80, loss = 0.02655329927802086
iteration 81, loss = 0.02283276617527008
iteration 82, loss = 0.017057547345757484
iteration 83, loss = 0.019471727311611176
iteration 84, loss = 0.02026377245783806
iteration 85, loss = 0.018208857625722885
iteration 86, loss = 0.016700219362974167
iteration 87, loss = 0.017688246443867683
iteration 88, loss = 0.018729938194155693
iteration 89, loss = 0.018080884590744972
iteration 90, loss = 0.017924437299370766
iteration 91, loss = 0.018254756927490234
iteration 92, loss = 0.017135154455900192
iteration 93, loss = 0.017745090648531914
iteration 94, loss = 0.018943337723612785
iteration 95, loss = 0.018126869574189186
iteration 96, loss = 0.01898561790585518
iteration 97, loss = 0.01795792020857334
iteration 98, loss = 0.016499433666467667
iteration 99, loss = 0.018808690831065178
iteration 100, loss = 0.02722949907183647
iteration 101, loss = 0.018533505499362946
iteration 102, loss = 0.025497857481241226
iteration 103, loss = 0.01816011592745781
iteration 104, loss = 0.0180502962321043
iteration 105, loss = 0.018143732100725174
iteration 106, loss = 0.01946440525352955
iteration 107, loss = 0.018139809370040894
iteration 108, loss = 0.017898187041282654
iteration 109, loss = 0.0176674947142601
iteration 110, loss = 0.016531335189938545
iteration 111, loss = 0.017557118088006973
iteration 112, loss = 0.01705712080001831
iteration 113, loss = 0.021904146298766136
iteration 114, loss = 0.01791790872812271
iteration 115, loss = 0.020841455087065697
iteration 116, loss = 0.016083411872386932
iteration 117, loss = 0.018575340509414673
iteration 118, loss = 0.01849362626671791
iteration 119, loss = 0.01651335507631302
iteration 120, loss = 0.021231429651379585
iteration 121, loss = 0.0189503263682127
iteration 122, loss = 0.016655512154102325
iteration 123, loss = 0.017299143597483635
iteration 124, loss = 0.019457612186670303
iteration 125, loss = 0.017598802223801613
iteration 126, loss = 0.021118657663464546
iteration 127, loss = 0.020704563707113266
iteration 128, loss = 0.018149474635720253
iteration 129, loss = 0.018904175609350204
iteration 130, loss = 0.01995868608355522
iteration 131, loss = 0.019886745139956474
iteration 132, loss = 0.020289123058319092
iteration 133, loss = 0.017976542934775352
iteration 134, loss = 0.017329076305031776
iteration 135, loss = 0.017796054482460022
iteration 136, loss = 0.021115973591804504
iteration 137, loss = 0.01660299487411976
iteration 138, loss = 0.01762945018708706
iteration 139, loss = 0.01898474432528019
iteration 140, loss = 0.016421711072325706
iteration 141, loss = 0.017837433144450188
iteration 142, loss = 0.01895001158118248
iteration 143, loss = 0.019890252500772476
iteration 144, loss = 0.019587136805057526
iteration 145, loss = 0.017157629132270813
iteration 146, loss = 0.01942044496536255
iteration 147, loss = 0.01799733005464077
iteration 148, loss = 0.016743626445531845
iteration 149, loss = 0.0192861370742321
iteration 150, loss = 0.018949350342154503
iteration 151, loss = 0.016870420426130295
iteration 152, loss = 0.01728651486337185
iteration 153, loss = 0.01931055262684822
iteration 154, loss = 0.017424119636416435
iteration 155, loss = 0.02099752426147461
iteration 156, loss = 0.018191669136285782
iteration 157, loss = 0.017613954842090607
iteration 158, loss = 0.017463624477386475
iteration 159, loss = 0.016414320096373558
iteration 160, loss = 0.01736348122358322
iteration 161, loss = 0.01701309345662594
iteration 162, loss = 0.01796141266822815
iteration 163, loss = 0.017424076795578003
iteration 164, loss = 0.017843956127762794
iteration 165, loss = 0.018618151545524597
iteration 166, loss = 0.017331015318632126
iteration 167, loss = 0.01712581142783165
iteration 168, loss = 0.019286852329969406
iteration 169, loss = 0.015910780057311058
iteration 170, loss = 0.01923644170165062
iteration 171, loss = 0.018297551199793816
iteration 172, loss = 0.017289157956838608
iteration 173, loss = 0.020642392337322235
iteration 174, loss = 0.017540205270051956
iteration 175, loss = 0.017493311315774918
iteration 176, loss = 0.01968047022819519
iteration 177, loss = 0.01899886503815651
iteration 178, loss = 0.017122728750109673
iteration 179, loss = 0.016641397029161453
iteration 180, loss = 0.018039437010884285
iteration 181, loss = 0.022650698199868202
iteration 182, loss = 0.01885380409657955
iteration 183, loss = 0.019082218408584595
iteration 184, loss = 0.01760973036289215
iteration 185, loss = 0.01720649190247059
iteration 186, loss = 0.01833231747150421
iteration 187, loss = 0.01786179095506668
iteration 188, loss = 0.016828307881951332
iteration 189, loss = 0.017646968364715576
iteration 190, loss = 0.018167171627283096
iteration 191, loss = 0.02397463284432888
iteration 192, loss = 0.016539106145501137
iteration 193, loss = 0.026279039680957794
iteration 194, loss = 0.019247038289904594
iteration 195, loss = 0.01766940765082836
iteration 196, loss = 0.018771175295114517
iteration 197, loss = 0.016346050426363945
iteration 198, loss = 0.02003409154713154
iteration 199, loss = 0.02104026824235916
iteration 200, loss = 0.01903640478849411
iteration 201, loss = 0.017850574105978012
iteration 202, loss = 0.017558086663484573
iteration 203, loss = 0.016379456967115402
iteration 204, loss = 0.01907861791551113
iteration 205, loss = 0.0186842679977417
iteration 206, loss = 0.021193262189626694
iteration 207, loss = 0.016962280496954918
iteration 208, loss = 0.01715298369526863
iteration 209, loss = 0.01967632584273815
iteration 210, loss = 0.017016099765896797
iteration 211, loss = 0.018287619575858116
iteration 212, loss = 0.0180272925645113
iteration 213, loss = 0.02104821801185608
iteration 214, loss = 0.017863720655441284
iteration 215, loss = 0.017201410606503487
iteration 216, loss = 0.018227439373731613
iteration 217, loss = 0.01944439485669136
iteration 218, loss = 0.018934106454253197
iteration 219, loss = 0.02073877863585949
iteration 220, loss = 0.03296365588903427
iteration 221, loss = 0.017448781058192253
iteration 222, loss = 0.0173212718218565
iteration 223, loss = 0.019111469388008118
iteration 224, loss = 0.025925355032086372
iteration 225, loss = 0.01857713982462883
iteration 226, loss = 0.027390923351049423
iteration 227, loss = 0.025678539648652077
iteration 228, loss = 0.01695088855922222
iteration 229, loss = 0.026289207860827446
iteration 230, loss = 0.017695961520075798
iteration 231, loss = 0.018091121688485146
iteration 232, loss = 0.02065875381231308
iteration 233, loss = 0.017476018518209457
iteration 234, loss = 0.032909095287323
iteration 235, loss = 0.017243023961782455
iteration 236, loss = 0.018345583230257034
iteration 237, loss = 0.01657007448375225
iteration 238, loss = 0.017169225960969925
iteration 239, loss = 0.01836271397769451
iteration 240, loss = 0.017062822356820107
iteration 241, loss = 0.019674476236104965
iteration 242, loss = 0.016407400369644165
iteration 243, loss = 0.016269652172923088
iteration 244, loss = 0.01939121074974537
iteration 245, loss = 0.01680826209485531
iteration 246, loss = 0.018430326133966446
iteration 247, loss = 0.01720242202281952
iteration 248, loss = 0.017694836482405663
iteration 249, loss = 0.02634439989924431
iteration 250, loss = 0.01647740975022316
iteration 251, loss = 0.024959491565823555
iteration 252, loss = 0.01753183640539646
iteration 253, loss = 0.02501748502254486
iteration 254, loss = 0.024951569736003876
iteration 255, loss = 0.02502509020268917
iteration 256, loss = 0.023873640224337578
iteration 257, loss = 0.01754985749721527
iteration 258, loss = 0.01719214767217636
iteration 259, loss = 0.017589146271348
iteration 260, loss = 0.016883691772818565
iteration 261, loss = 0.017293326556682587
iteration 262, loss = 0.01672685518860817
iteration 263, loss = 0.019397074356675148
iteration 264, loss = 0.01921096071600914
iteration 265, loss = 0.017748113721609116
iteration 266, loss = 0.01793736405670643
iteration 267, loss = 0.017458125948905945
iteration 268, loss = 0.027400001883506775
iteration 269, loss = 0.01825273595750332
iteration 270, loss = 0.01639350689947605
iteration 271, loss = 0.0169159434735775
iteration 272, loss = 0.01713760383427143
iteration 273, loss = 0.018272163346409798
iteration 274, loss = 0.01692867837846279
iteration 275, loss = 0.020930908620357513
iteration 276, loss = 0.018507925793528557
iteration 277, loss = 0.01944631338119507
iteration 278, loss = 0.02155170775949955
iteration 279, loss = 0.01774316094815731
iteration 280, loss = 0.019565695896744728
iteration 281, loss = 0.01741921901702881
iteration 282, loss = 0.016718627884984016
iteration 283, loss = 0.01629958301782608
iteration 284, loss = 0.017795773223042488
iteration 285, loss = 0.01667729578912258
iteration 286, loss = 0.018121251836419106
iteration 287, loss = 0.018279967829585075
iteration 288, loss = 0.017563778907060623
iteration 289, loss = 0.017035052180290222
iteration 290, loss = 0.017972178757190704
iteration 291, loss = 0.017082655802369118
iteration 292, loss = 0.01809191331267357
iteration 293, loss = 0.02027095854282379
iteration 294, loss = 0.0171901136636734
iteration 295, loss = 0.019301194697618484
iteration 296, loss = 0.025865884497761726
iteration 297, loss = 0.018513953313231468
iteration 298, loss = 0.018295662477612495
iteration 299, loss = 0.01719927415251732
iteration 300, loss = 0.02158559486269951
iteration 1, loss = 0.026407545432448387
iteration 2, loss = 0.01637088693678379
iteration 3, loss = 0.016924837604165077
iteration 4, loss = 0.022584332153201103
iteration 5, loss = 0.02591424062848091
iteration 6, loss = 0.01838856376707554
iteration 7, loss = 0.021029114723205566
iteration 8, loss = 0.016388624906539917
iteration 9, loss = 0.01800428330898285
iteration 10, loss = 0.0224910881370306
iteration 11, loss = 0.01777593605220318
iteration 12, loss = 0.019453994929790497
iteration 13, loss = 0.016642898321151733
iteration 14, loss = 0.017912372946739197
iteration 15, loss = 0.017262807115912437
iteration 16, loss = 0.018842797726392746
iteration 17, loss = 0.023836888372898102
iteration 18, loss = 0.017730731517076492
iteration 19, loss = 0.01699904352426529
iteration 20, loss = 0.01936217211186886
iteration 21, loss = 0.0186677984893322
iteration 22, loss = 0.02049984782934189
iteration 23, loss = 0.017641164362430573
iteration 24, loss = 0.01644611731171608
iteration 25, loss = 0.017319276928901672
iteration 26, loss = 0.024162961170077324
iteration 27, loss = 0.019150564447045326
iteration 28, loss = 0.01722625643014908
iteration 29, loss = 0.018459074199199677
iteration 30, loss = 0.019613558426499367
iteration 31, loss = 0.02029144950211048
iteration 32, loss = 0.017027435824275017
iteration 33, loss = 0.024287452921271324
iteration 34, loss = 0.016629187390208244
iteration 35, loss = 0.016310788691043854
iteration 36, loss = 0.0194732453674078
iteration 37, loss = 0.01780184730887413
iteration 38, loss = 0.01663833111524582
iteration 39, loss = 0.016516009345650673
iteration 40, loss = 0.016431517899036407
iteration 41, loss = 0.016292095184326172
iteration 42, loss = 0.018040329217910767
iteration 43, loss = 0.016087902709841728
iteration 44, loss = 0.016735609620809555
iteration 45, loss = 0.019056135788559914
iteration 46, loss = 0.025247415527701378
iteration 47, loss = 0.018571659922599792
iteration 48, loss = 0.017896706238389015
iteration 49, loss = 0.020040541887283325
iteration 50, loss = 0.01865781657397747
iteration 51, loss = 0.01684536598622799
iteration 52, loss = 0.024725399911403656
iteration 53, loss = 0.017821140587329865
iteration 54, loss = 0.019279541447758675
iteration 55, loss = 0.016044771298766136
iteration 56, loss = 0.017119083553552628
iteration 57, loss = 0.016267206519842148
iteration 58, loss = 0.01867450587451458
iteration 59, loss = 0.016571322456002235
iteration 60, loss = 0.016463199630379677
iteration 61, loss = 0.01944620907306671
iteration 62, loss = 0.016582323238253593
iteration 63, loss = 0.027613820508122444
iteration 64, loss = 0.018737483769655228
iteration 65, loss = 0.017191434279084206
iteration 66, loss = 0.02360576018691063
iteration 67, loss = 0.02783641591668129
iteration 68, loss = 0.018995996564626694
iteration 69, loss = 0.017721101641654968
iteration 70, loss = 0.017240628600120544
iteration 71, loss = 0.021859070286154747
iteration 72, loss = 0.02776663564145565
iteration 73, loss = 0.01741919480264187
iteration 74, loss = 0.02001960761845112
iteration 75, loss = 0.01729024015367031
iteration 76, loss = 0.017261330038309097
iteration 77, loss = 0.017826011404395103
iteration 78, loss = 0.020177505910396576
iteration 79, loss = 0.019289176911115646
iteration 80, loss = 0.016425907611846924
iteration 81, loss = 0.01857583597302437
iteration 82, loss = 0.01754729449748993
iteration 83, loss = 0.017101557925343513
iteration 84, loss = 0.016720836982131004
iteration 85, loss = 0.023862160742282867
iteration 86, loss = 0.01874583214521408
iteration 87, loss = 0.017524128779768944
iteration 88, loss = 0.021837936714291573
iteration 89, loss = 0.02386285550892353
iteration 90, loss = 0.016582904383540154
iteration 91, loss = 0.016655568033456802
iteration 92, loss = 0.017136525362730026
iteration 93, loss = 0.019954361021518707
iteration 94, loss = 0.017498599365353584
iteration 95, loss = 0.01812831684947014
iteration 96, loss = 0.016910070553421974
iteration 97, loss = 0.019137514755129814
iteration 98, loss = 0.01720665954053402
iteration 99, loss = 0.019177526235580444
iteration 100, loss = 0.017250770702958107
iteration 101, loss = 0.01628742925822735
iteration 102, loss = 0.017512990161776543
iteration 103, loss = 0.019005658105015755
iteration 104, loss = 0.016673315316438675
iteration 105, loss = 0.01709812693297863
iteration 106, loss = 0.017828628420829773
iteration 107, loss = 0.020261358469724655
iteration 108, loss = 0.021560387685894966
iteration 109, loss = 0.024743421003222466
iteration 110, loss = 0.02422315813601017
iteration 111, loss = 0.015691202133893967
iteration 112, loss = 0.0166638046503067
iteration 113, loss = 0.019183265045285225
iteration 114, loss = 0.01774434745311737
iteration 115, loss = 0.017323283478617668
iteration 116, loss = 0.018090108409523964
iteration 117, loss = 0.017683321610093117
iteration 118, loss = 0.018634134903550148
iteration 119, loss = 0.019371984526515007
iteration 120, loss = 0.019202757626771927
iteration 121, loss = 0.017939629033207893
iteration 122, loss = 0.017241910099983215
iteration 123, loss = 0.01656080037355423
iteration 124, loss = 0.016203660517930984
iteration 125, loss = 0.019429123029112816
iteration 126, loss = 0.020868269726634026
iteration 127, loss = 0.018432294949889183
iteration 128, loss = 0.019196515902876854
iteration 129, loss = 0.017340881749987602
iteration 130, loss = 0.01812971755862236
iteration 131, loss = 0.01660572551190853
iteration 132, loss = 0.016839219257235527
iteration 133, loss = 0.020189207047224045
iteration 134, loss = 0.016548873856663704
iteration 135, loss = 0.024860236793756485
iteration 136, loss = 0.01749996840953827
iteration 137, loss = 0.025255804881453514
iteration 138, loss = 0.01889008656144142
iteration 139, loss = 0.018526405096054077
iteration 140, loss = 0.016580605879426003
iteration 141, loss = 0.028574608266353607
iteration 142, loss = 0.017712537199258804
iteration 143, loss = 0.01563689112663269
iteration 144, loss = 0.017105428501963615
iteration 145, loss = 0.020206836983561516
iteration 146, loss = 0.0188556220382452
iteration 147, loss = 0.017601054161787033
iteration 148, loss = 0.01777721382677555
iteration 149, loss = 0.01838529109954834
iteration 150, loss = 0.020735912024974823
iteration 151, loss = 0.01571919023990631
iteration 152, loss = 0.018054993823170662
iteration 153, loss = 0.01698550023138523
iteration 154, loss = 0.01879122108221054
iteration 155, loss = 0.01862681843340397
iteration 156, loss = 0.016844797879457474
iteration 157, loss = 0.01996283233165741
iteration 158, loss = 0.016648661345243454
iteration 159, loss = 0.018347587436437607
iteration 160, loss = 0.016064925119280815
iteration 161, loss = 0.019743556156754494
iteration 162, loss = 0.02491155080497265
iteration 163, loss = 0.016905739903450012
iteration 164, loss = 0.01646977663040161
iteration 165, loss = 0.018359322100877762
iteration 166, loss = 0.02015179768204689
iteration 167, loss = 0.01675402745604515
iteration 168, loss = 0.017810890451073647
iteration 169, loss = 0.016929229721426964
iteration 170, loss = 0.017735328525304794
iteration 171, loss = 0.017029736191034317
iteration 172, loss = 0.01684815064072609
iteration 173, loss = 0.016958849504590034
iteration 174, loss = 0.017912210896611214
iteration 175, loss = 0.025113238021731377
iteration 176, loss = 0.01635870710015297
iteration 177, loss = 0.016357146203517914
iteration 178, loss = 0.017015373334288597
iteration 179, loss = 0.01640438847243786
iteration 180, loss = 0.03195272758603096
iteration 181, loss = 0.0163255725055933
iteration 182, loss = 0.01672457531094551
iteration 183, loss = 0.017085334286093712
iteration 184, loss = 0.01812382973730564
iteration 185, loss = 0.01699383556842804
iteration 186, loss = 0.016325604170560837
iteration 187, loss = 0.017590640112757683
iteration 188, loss = 0.017160000279545784
iteration 189, loss = 0.019155245274305344
iteration 190, loss = 0.018192224204540253
iteration 191, loss = 0.018386170268058777
iteration 192, loss = 0.026274342089891434
iteration 193, loss = 0.01853501982986927
iteration 194, loss = 0.017421061173081398
iteration 195, loss = 0.0167551189661026
iteration 196, loss = 0.019415203481912613
iteration 197, loss = 0.019591940566897392
iteration 198, loss = 0.016319958493113518
iteration 199, loss = 0.015770426020026207
iteration 200, loss = 0.016646401956677437
iteration 201, loss = 0.016804123297333717
iteration 202, loss = 0.016127290204167366
iteration 203, loss = 0.017780859023332596
iteration 204, loss = 0.017896613106131554
iteration 205, loss = 0.017614828422665596
iteration 206, loss = 0.01735733449459076
iteration 207, loss = 0.032551359385252
iteration 208, loss = 0.018600944429636
iteration 209, loss = 0.015550059266388416
iteration 210, loss = 0.023665448650717735
iteration 211, loss = 0.017795195803046227
iteration 212, loss = 0.019640497863292694
iteration 213, loss = 0.017186496406793594
iteration 214, loss = 0.016601556912064552
iteration 215, loss = 0.018511608242988586
iteration 216, loss = 0.025347772985696793
iteration 217, loss = 0.01884777657687664
iteration 218, loss = 0.02516402304172516
iteration 219, loss = 0.01664162427186966
iteration 220, loss = 0.015998676419258118
iteration 221, loss = 0.024520762264728546
iteration 222, loss = 0.016633950173854828
iteration 223, loss = 0.016480769962072372
iteration 224, loss = 0.01567923277616501
iteration 225, loss = 0.017585227265954018
iteration 226, loss = 0.018854813650250435
iteration 227, loss = 0.020615432411432266
iteration 228, loss = 0.016291767358779907
iteration 229, loss = 0.018245011568069458
iteration 230, loss = 0.016560450196266174
iteration 231, loss = 0.0169130340218544
iteration 232, loss = 0.018522299826145172
iteration 233, loss = 0.01666063256561756
iteration 234, loss = 0.017318537458777428
iteration 235, loss = 0.018581392243504524
iteration 236, loss = 0.016361592337489128
iteration 237, loss = 0.015832696110010147
iteration 238, loss = 0.017187977209687233
iteration 239, loss = 0.016821840777993202
iteration 240, loss = 0.01665346883237362
iteration 241, loss = 0.01791914738714695
iteration 242, loss = 0.01679081842303276
iteration 243, loss = 0.0164960790425539
iteration 244, loss = 0.017894502729177475
iteration 245, loss = 0.017760194838047028
iteration 246, loss = 0.016430459916591644
iteration 247, loss = 0.016993887722492218
iteration 248, loss = 0.0211147703230381
iteration 249, loss = 0.01640898361802101
iteration 250, loss = 0.02217828296124935
iteration 251, loss = 0.01616539992392063
iteration 252, loss = 0.017917808145284653
iteration 253, loss = 0.01805289275944233
iteration 254, loss = 0.017049040645360947
iteration 255, loss = 0.017561541870236397
iteration 256, loss = 0.016542229801416397
iteration 257, loss = 0.015572097152471542
iteration 258, loss = 0.01655631512403488
iteration 259, loss = 0.018760642036795616
iteration 260, loss = 0.02079298160970211
iteration 261, loss = 0.017407389357686043
iteration 262, loss = 0.016461094841361046
iteration 263, loss = 0.017486710101366043
iteration 264, loss = 0.017108997330069542
iteration 265, loss = 0.017661400139331818
iteration 266, loss = 0.02302497625350952
iteration 267, loss = 0.01629975624382496
iteration 268, loss = 0.01820840686559677
iteration 269, loss = 0.01931961067020893
iteration 270, loss = 0.018233226612210274
iteration 271, loss = 0.017562689259648323
iteration 272, loss = 0.01895611360669136
iteration 273, loss = 0.01641964167356491
iteration 274, loss = 0.01977473683655262
iteration 275, loss = 0.016462303698062897
iteration 276, loss = 0.01617618277668953
iteration 277, loss = 0.018191276118159294
iteration 278, loss = 0.018219348043203354
iteration 279, loss = 0.01743362657725811
iteration 280, loss = 0.01622149534523487
iteration 281, loss = 0.017598411068320274
iteration 282, loss = 0.018525829538702965
iteration 283, loss = 0.017233222723007202
iteration 284, loss = 0.019366728141903877
iteration 285, loss = 0.018041733652353287
iteration 286, loss = 0.016843287274241447
iteration 287, loss = 0.01563909836113453
iteration 288, loss = 0.016278352588415146
iteration 289, loss = 0.017598291859030724
iteration 290, loss = 0.017215952277183533
iteration 291, loss = 0.019010085612535477
iteration 292, loss = 0.024227682501077652
iteration 293, loss = 0.018711896613240242
iteration 294, loss = 0.01583205536007881
iteration 295, loss = 0.019344817847013474
iteration 296, loss = 0.017576493322849274
iteration 297, loss = 0.019359279423952103
iteration 298, loss = 0.018034882843494415
iteration 299, loss = 0.017337989062070847
iteration 300, loss = 0.018128855153918266
iteration 1, loss = 0.01803736388683319
iteration 2, loss = 0.0345575213432312
iteration 3, loss = 0.01569105125963688
iteration 4, loss = 0.01738412119448185
iteration 5, loss = 0.016740988940000534
iteration 6, loss = 0.018629159778356552
iteration 7, loss = 0.016752226278185844
iteration 8, loss = 0.016884084790945053
iteration 9, loss = 0.023351991549134254
iteration 10, loss = 0.015871107578277588
iteration 11, loss = 0.025815287604928017
iteration 12, loss = 0.01736455224454403
iteration 13, loss = 0.01759576052427292
iteration 14, loss = 0.016198495402932167
iteration 15, loss = 0.015397336333990097
iteration 16, loss = 0.015590508468449116
iteration 17, loss = 0.016971314325928688
iteration 18, loss = 0.01683742366731167
iteration 19, loss = 0.016998453065752983
iteration 20, loss = 0.018327701836824417
iteration 21, loss = 0.01735841855406761
iteration 22, loss = 0.021384604275226593
iteration 23, loss = 0.017991743981838226
iteration 24, loss = 0.020503884181380272
iteration 25, loss = 0.015897417441010475
iteration 26, loss = 0.024878840893507004
iteration 27, loss = 0.017085909843444824
iteration 28, loss = 0.017603524029254913
iteration 29, loss = 0.016945619136095047
iteration 30, loss = 0.026494890451431274
iteration 31, loss = 0.016580458730459213
iteration 32, loss = 0.017463363707065582
iteration 33, loss = 0.01827458292245865
iteration 34, loss = 0.0161762572824955
iteration 35, loss = 0.01802670583128929
iteration 36, loss = 0.016140278428792953
iteration 37, loss = 0.01662134751677513
iteration 38, loss = 0.024258147925138474
iteration 39, loss = 0.01933816447854042
iteration 40, loss = 0.024852126836776733
iteration 41, loss = 0.016914168372750282
iteration 42, loss = 0.016346223652362823
iteration 43, loss = 0.024575108662247658
iteration 44, loss = 0.01605929806828499
iteration 45, loss = 0.01593504101037979
iteration 46, loss = 0.022842364385724068
iteration 47, loss = 0.016067560762166977
iteration 48, loss = 0.02047380618751049
iteration 49, loss = 0.01595350168645382
iteration 50, loss = 0.016965050250291824
iteration 51, loss = 0.01686231791973114
iteration 52, loss = 0.025857647880911827
iteration 53, loss = 0.018868209794163704
iteration 54, loss = 0.018865013495087624
iteration 55, loss = 0.024885550141334534
iteration 56, loss = 0.018699154257774353
iteration 57, loss = 0.01722419634461403
iteration 58, loss = 0.015910889953374863
iteration 59, loss = 0.01621418446302414
iteration 60, loss = 0.018797172233462334
iteration 61, loss = 0.01651749759912491
iteration 62, loss = 0.015837568789720535
iteration 63, loss = 0.022839466109871864
iteration 64, loss = 0.021058136597275734
iteration 65, loss = 0.016722263768315315
iteration 66, loss = 0.017715182155370712
iteration 67, loss = 0.018057765439152718
iteration 68, loss = 0.020267417654395103
iteration 69, loss = 0.016834570094943047
iteration 70, loss = 0.015849292278289795
iteration 71, loss = 0.016192179173231125
iteration 72, loss = 0.018161052837967873
iteration 73, loss = 0.017092913389205933
iteration 74, loss = 0.017365712672472
iteration 75, loss = 0.02036885917186737
iteration 76, loss = 0.015809720382094383
iteration 77, loss = 0.01792340911924839
iteration 78, loss = 0.01854037493467331
iteration 79, loss = 0.017457937821745872
iteration 80, loss = 0.017372610047459602
iteration 81, loss = 0.015456004999577999
iteration 82, loss = 0.017589151859283447
iteration 83, loss = 0.016425058245658875
iteration 84, loss = 0.01854059472680092
iteration 85, loss = 0.015981225296854973
iteration 86, loss = 0.017762091010808945
iteration 87, loss = 0.026726124808192253
iteration 88, loss = 0.01602713018655777
iteration 89, loss = 0.0158259067684412
iteration 90, loss = 0.016309194266796112
iteration 91, loss = 0.017751730978488922
iteration 92, loss = 0.01629064790904522
iteration 93, loss = 0.015612313523888588
iteration 94, loss = 0.0163908489048481
iteration 95, loss = 0.016198471188545227
iteration 96, loss = 0.016329538077116013
iteration 97, loss = 0.01899041421711445
iteration 98, loss = 0.02007906697690487
iteration 99, loss = 0.02297467552125454
iteration 100, loss = 0.016020499169826508
iteration 101, loss = 0.01660924032330513
iteration 102, loss = 0.02715667523443699
iteration 103, loss = 0.016574915498495102
iteration 104, loss = 0.01738610677421093
iteration 105, loss = 0.016614597290754318
iteration 106, loss = 0.016882391646504402
iteration 107, loss = 0.018440885469317436
iteration 108, loss = 0.015510779805481434
iteration 109, loss = 0.016418365761637688
iteration 110, loss = 0.016790209338068962
iteration 111, loss = 0.0266641266644001
iteration 112, loss = 0.015989156439900398
iteration 113, loss = 0.024438142776489258
iteration 114, loss = 0.016436193138360977
iteration 115, loss = 0.01805180311203003
iteration 116, loss = 0.016980187967419624
iteration 117, loss = 0.017638981342315674
iteration 118, loss = 0.019199155271053314
iteration 119, loss = 0.017406655475497246
iteration 120, loss = 0.015569018200039864
iteration 121, loss = 0.016742879524827003
iteration 122, loss = 0.015824882313609123
iteration 123, loss = 0.016990043222904205
iteration 124, loss = 0.016720890998840332
iteration 125, loss = 0.025988511741161346
iteration 126, loss = 0.01963082142174244
iteration 127, loss = 0.016733163967728615
iteration 128, loss = 0.017153695225715637
iteration 129, loss = 0.016454387456178665
iteration 130, loss = 0.01723945140838623
iteration 131, loss = 0.01782575435936451
iteration 132, loss = 0.01769314892590046
iteration 133, loss = 0.017718149349093437
iteration 134, loss = 0.023382222279906273
iteration 135, loss = 0.016179168596863747
iteration 136, loss = 0.01607222855091095
iteration 137, loss = 0.016802793368697166
iteration 138, loss = 0.0175276268273592
iteration 139, loss = 0.018295690417289734
iteration 140, loss = 0.016738245263695717
iteration 141, loss = 0.0169017743319273
iteration 142, loss = 0.016734350472688675
iteration 143, loss = 0.016870204359292984
iteration 144, loss = 0.016678573563694954
iteration 145, loss = 0.018576785922050476
iteration 146, loss = 0.015756025910377502
iteration 147, loss = 0.017937391996383667
iteration 148, loss = 0.020867077633738518
iteration 149, loss = 0.017404723912477493
iteration 150, loss = 0.02375013940036297
iteration 151, loss = 0.016263099387288094
iteration 152, loss = 0.01676967181265354
iteration 153, loss = 0.01672317646443844
iteration 154, loss = 0.024280361831188202
iteration 155, loss = 0.017650267109274864
iteration 156, loss = 0.016107778996229172
iteration 157, loss = 0.016385864466428757
iteration 158, loss = 0.017983071506023407
iteration 159, loss = 0.01688222773373127
iteration 160, loss = 0.015736382454633713
iteration 161, loss = 0.01960308477282524
iteration 162, loss = 0.016511576250195503
iteration 163, loss = 0.01736419089138508
iteration 164, loss = 0.017196111381053925
iteration 165, loss = 0.017458109185099602
iteration 166, loss = 0.017818808555603027
iteration 167, loss = 0.026654740795493126
iteration 168, loss = 0.01696459949016571
iteration 169, loss = 0.016498494893312454
iteration 170, loss = 0.0183015875518322
iteration 171, loss = 0.02488519810140133
iteration 172, loss = 0.016529293730854988
iteration 173, loss = 0.01720539852976799
iteration 174, loss = 0.017987171187996864
iteration 175, loss = 0.017730845138430595
iteration 176, loss = 0.016274690628051758
iteration 177, loss = 0.017024017870426178
iteration 178, loss = 0.01638232171535492
iteration 179, loss = 0.01794532872736454
iteration 180, loss = 0.015894129872322083
iteration 181, loss = 0.02050844579935074
iteration 182, loss = 0.025524485856294632
iteration 183, loss = 0.017414454370737076
iteration 184, loss = 0.016354473307728767
iteration 185, loss = 0.01525175292044878
iteration 186, loss = 0.016087351366877556
iteration 187, loss = 0.01902797259390354
iteration 188, loss = 0.015793120488524437
iteration 189, loss = 0.019742704927921295
iteration 190, loss = 0.020311037078499794
iteration 191, loss = 0.01579536683857441
iteration 192, loss = 0.017339687794446945
iteration 193, loss = 0.01661546528339386
iteration 194, loss = 0.016214264556765556
iteration 195, loss = 0.018819406628608704
iteration 196, loss = 0.016835268586874008
iteration 197, loss = 0.01611763797700405
iteration 198, loss = 0.016167011111974716
iteration 199, loss = 0.019197799265384674
iteration 200, loss = 0.020119542255997658
iteration 201, loss = 0.01589254103600979
iteration 202, loss = 0.01582641899585724
iteration 203, loss = 0.018212849274277687
iteration 204, loss = 0.017122702673077583
iteration 205, loss = 0.02029075287282467
iteration 206, loss = 0.019145604223012924
iteration 207, loss = 0.016109919175505638
iteration 208, loss = 0.018955493345856667
iteration 209, loss = 0.016522057354450226
iteration 210, loss = 0.016066381707787514
iteration 211, loss = 0.01864236406981945
iteration 212, loss = 0.016557849943637848
iteration 213, loss = 0.02475421130657196
iteration 214, loss = 0.01612052135169506
iteration 215, loss = 0.01869824156165123
iteration 216, loss = 0.018678927794098854
iteration 217, loss = 0.01716519705951214
iteration 218, loss = 0.016974084079265594
iteration 219, loss = 0.01798902079463005
iteration 220, loss = 0.016188621520996094
iteration 221, loss = 0.018285203725099564
iteration 222, loss = 0.01563451439142227
iteration 223, loss = 0.017398662865161896
iteration 224, loss = 0.01610361784696579
iteration 225, loss = 0.015552371740341187
iteration 226, loss = 0.017409928143024445
iteration 227, loss = 0.015618995763361454
iteration 228, loss = 0.017230665311217308
iteration 229, loss = 0.017119046300649643
iteration 230, loss = 0.016160106286406517
iteration 231, loss = 0.016790451481938362
iteration 232, loss = 0.017635421827435493
iteration 233, loss = 0.01846621371805668
iteration 234, loss = 0.023248719051480293
iteration 235, loss = 0.026702512055635452
iteration 236, loss = 0.019184637814760208
iteration 237, loss = 0.018882334232330322
iteration 238, loss = 0.016441280022263527
iteration 239, loss = 0.01729488931596279
iteration 240, loss = 0.01610855758190155
iteration 241, loss = 0.016523873433470726
iteration 242, loss = 0.016162944957613945
iteration 243, loss = 0.017232870683073997
iteration 244, loss = 0.018619760870933533
iteration 245, loss = 0.02273394726216793
iteration 246, loss = 0.016201205551624298
iteration 247, loss = 0.017471950501203537
iteration 248, loss = 0.015393311157822609
iteration 249, loss = 0.016911912709474564
iteration 250, loss = 0.015964385122060776
iteration 251, loss = 0.015775691717863083
iteration 252, loss = 0.019505994394421577
iteration 253, loss = 0.017049264162778854
iteration 254, loss = 0.016145670786499977
iteration 255, loss = 0.017449811100959778
iteration 256, loss = 0.017119403928518295
iteration 257, loss = 0.018424326553940773
iteration 258, loss = 0.015530289150774479
iteration 259, loss = 0.026238733902573586
iteration 260, loss = 0.01636006124317646
iteration 261, loss = 0.01847446896135807
iteration 262, loss = 0.01653352566063404
iteration 263, loss = 0.02018590271472931
iteration 264, loss = 0.016712848097085953
iteration 265, loss = 0.017505928874015808
iteration 266, loss = 0.015789905562996864
iteration 267, loss = 0.015558603219687939
iteration 268, loss = 0.023724930360913277
iteration 269, loss = 0.01819690130650997
iteration 270, loss = 0.015347269363701344
iteration 271, loss = 0.019527573138475418
iteration 272, loss = 0.015334864147007465
iteration 273, loss = 0.01641666144132614
iteration 274, loss = 0.017866551876068115
iteration 275, loss = 0.016376972198486328
iteration 276, loss = 0.0188838429749012
iteration 277, loss = 0.016565175727009773
iteration 278, loss = 0.017964977771043777
iteration 279, loss = 0.016423629596829414
iteration 280, loss = 0.016348913311958313
iteration 281, loss = 0.018414728343486786
iteration 282, loss = 0.015888549387454987
iteration 283, loss = 0.016303516924381256
iteration 284, loss = 0.017487650737166405
iteration 285, loss = 0.014887815341353416
iteration 286, loss = 0.015371065586805344
iteration 287, loss = 0.01681634783744812
iteration 288, loss = 0.01637781597673893
iteration 289, loss = 0.016083307564258575
iteration 290, loss = 0.018417414277791977
iteration 291, loss = 0.018584007397294044
iteration 292, loss = 0.017060240730643272
iteration 293, loss = 0.01619025506079197
iteration 294, loss = 0.018053052946925163
iteration 295, loss = 0.0157775841653347
iteration 296, loss = 0.01580781862139702
iteration 297, loss = 0.015942569822072983
iteration 298, loss = 0.016322821378707886
iteration 299, loss = 0.01673276536166668
iteration 300, loss = 0.016101377084851265
iteration 1, loss = 0.02290898934006691
iteration 2, loss = 0.02500457689166069
iteration 3, loss = 0.017165441066026688
iteration 4, loss = 0.016396239399909973
iteration 5, loss = 0.016360005363821983
iteration 6, loss = 0.017321690917015076
iteration 7, loss = 0.015932168811559677
iteration 8, loss = 0.018680639564990997
iteration 9, loss = 0.015755638480186462
iteration 10, loss = 0.01588672772049904
iteration 11, loss = 0.0236850343644619
iteration 12, loss = 0.0172274187207222
iteration 13, loss = 0.016451386734843254
iteration 14, loss = 0.017947698011994362
iteration 15, loss = 0.015897361561655998
iteration 16, loss = 0.0166535172611475
iteration 17, loss = 0.01673574000597
iteration 18, loss = 0.015463922172784805
iteration 19, loss = 0.016765138134360313
iteration 20, loss = 0.016533024609088898
iteration 21, loss = 0.018278658390045166
iteration 22, loss = 0.019305959343910217
iteration 23, loss = 0.022744862362742424
iteration 24, loss = 0.015521250665187836
iteration 25, loss = 0.01548011600971222
iteration 26, loss = 0.016048626974225044
iteration 27, loss = 0.02403211034834385
iteration 28, loss = 0.014969090931117535
iteration 29, loss = 0.015855124220252037
iteration 30, loss = 0.0162804014980793
iteration 31, loss = 0.016279421746730804
iteration 32, loss = 0.019248835742473602
iteration 33, loss = 0.016613794490695
iteration 34, loss = 0.01573997363448143
iteration 35, loss = 0.017895694822072983
iteration 36, loss = 0.016461960971355438
iteration 37, loss = 0.015557433478534222
iteration 38, loss = 0.015449806116521358
iteration 39, loss = 0.015511948615312576
iteration 40, loss = 0.01859418861567974
iteration 41, loss = 0.015559243969619274
iteration 42, loss = 0.01583811827003956
iteration 43, loss = 0.027603231370449066
iteration 44, loss = 0.01579970121383667
iteration 45, loss = 0.016230881214141846
iteration 46, loss = 0.015301351435482502
iteration 47, loss = 0.01617986336350441
iteration 48, loss = 0.015621498227119446
iteration 49, loss = 0.016429010778665543
iteration 50, loss = 0.018678823485970497
iteration 51, loss = 0.017458146438002586
iteration 52, loss = 0.015981921926140785
iteration 53, loss = 0.02700135111808777
iteration 54, loss = 0.0154240932315588
iteration 55, loss = 0.018491879105567932
iteration 56, loss = 0.016243625432252884
iteration 57, loss = 0.01641242764890194
iteration 58, loss = 0.015347822569310665
iteration 59, loss = 0.025373203679919243
iteration 60, loss = 0.015852609649300575
iteration 61, loss = 0.016129175201058388
iteration 62, loss = 0.015626341104507446
iteration 63, loss = 0.016112208366394043
iteration 64, loss = 0.01753438450396061
iteration 65, loss = 0.01659066043794155
iteration 66, loss = 0.015721291303634644
iteration 67, loss = 0.017914941534399986
iteration 68, loss = 0.01635247841477394
iteration 69, loss = 0.016369495540857315
iteration 70, loss = 0.015989741310477257
iteration 71, loss = 0.015711376443505287
iteration 72, loss = 0.016009557992219925
iteration 73, loss = 0.01513651479035616
iteration 74, loss = 0.01610480435192585
iteration 75, loss = 0.016895011067390442
iteration 76, loss = 0.018352322280406952
iteration 77, loss = 0.015964530408382416
iteration 78, loss = 0.01645992510020733
iteration 79, loss = 0.019417164847254753
iteration 80, loss = 0.016162719577550888
iteration 81, loss = 0.015798363834619522
iteration 82, loss = 0.01717197895050049
iteration 83, loss = 0.017622539773583412
iteration 84, loss = 0.022876909002661705
iteration 85, loss = 0.01989978924393654
iteration 86, loss = 0.020246446132659912
iteration 87, loss = 0.017149511724710464
iteration 88, loss = 0.01753104105591774
iteration 89, loss = 0.018093233928084373
iteration 90, loss = 0.018292341381311417
iteration 91, loss = 0.016429999843239784
iteration 92, loss = 0.01611941121518612
iteration 93, loss = 0.015378057025372982
iteration 94, loss = 0.014788223430514336
iteration 95, loss = 0.020068366080522537
iteration 96, loss = 0.01952710561454296
iteration 97, loss = 0.017370685935020447
iteration 98, loss = 0.017084043473005295
iteration 99, loss = 0.018615884706377983
iteration 100, loss = 0.016558822244405746
iteration 101, loss = 0.016836049035191536
iteration 102, loss = 0.022003933787345886
iteration 103, loss = 0.019118916243314743
iteration 104, loss = 0.016337715089321136
iteration 105, loss = 0.015422494150698185
iteration 106, loss = 0.01716442033648491
iteration 107, loss = 0.02113119699060917
iteration 108, loss = 0.015669822692871094
iteration 109, loss = 0.023292027413845062
iteration 110, loss = 0.014905865304172039
iteration 111, loss = 0.025747282430529594
iteration 112, loss = 0.01751186139881611
iteration 113, loss = 0.0167201180011034
iteration 114, loss = 0.018625745549798012
iteration 115, loss = 0.019615739583969116
iteration 116, loss = 0.018815016373991966
iteration 117, loss = 0.016778193414211273
iteration 118, loss = 0.015453608706593513
iteration 119, loss = 0.015703510493040085
iteration 120, loss = 0.0262399073690176
iteration 121, loss = 0.01841840147972107
iteration 122, loss = 0.01627604104578495
iteration 123, loss = 0.014611892402172089
iteration 124, loss = 0.016966944560408592
iteration 125, loss = 0.015726419165730476
iteration 126, loss = 0.016684265807271004
iteration 127, loss = 0.017848273739218712
iteration 128, loss = 0.023702574893832207
iteration 129, loss = 0.017270561307668686
iteration 130, loss = 0.01874009519815445
iteration 131, loss = 0.017254088073968887
iteration 132, loss = 0.017022667452692986
iteration 133, loss = 0.018406298011541367
iteration 134, loss = 0.01785379648208618
iteration 135, loss = 0.017524870112538338
iteration 136, loss = 0.016451625153422356
iteration 137, loss = 0.018166493624448776
iteration 138, loss = 0.015862783417105675
iteration 139, loss = 0.0177985318005085
iteration 140, loss = 0.016132064163684845
iteration 141, loss = 0.024829968810081482
iteration 142, loss = 0.01665472239255905
iteration 143, loss = 0.015820251777768135
iteration 144, loss = 0.016604360193014145
iteration 145, loss = 0.016732096672058105
iteration 146, loss = 0.018778517842292786
iteration 147, loss = 0.01653423346579075
iteration 148, loss = 0.017194246873259544
iteration 149, loss = 0.01855987310409546
iteration 150, loss = 0.018740195780992508
iteration 151, loss = 0.018610456958413124
iteration 152, loss = 0.017677683383226395
iteration 153, loss = 0.014715040102601051
iteration 154, loss = 0.015873614698648453
iteration 155, loss = 0.0166146382689476
iteration 156, loss = 0.015640417113900185
iteration 157, loss = 0.024170072749257088
iteration 158, loss = 0.01847894676029682
iteration 159, loss = 0.0163275059312582
iteration 160, loss = 0.017681559547781944
iteration 161, loss = 0.015249728225171566
iteration 162, loss = 0.01680714823305607
iteration 163, loss = 0.02482590079307556
iteration 164, loss = 0.0160976629704237
iteration 165, loss = 0.015658622607588768
iteration 166, loss = 0.018322709947824478
iteration 167, loss = 0.018926985561847687
iteration 168, loss = 0.017234861850738525
iteration 169, loss = 0.017848694697022438
iteration 170, loss = 0.018063753843307495
iteration 171, loss = 0.016431832686066628
iteration 172, loss = 0.01555728167295456
iteration 173, loss = 0.014990031719207764
iteration 174, loss = 0.016835374757647514
iteration 175, loss = 0.01835225149989128
iteration 176, loss = 0.01616949588060379
iteration 177, loss = 0.016535546630620956
iteration 178, loss = 0.017023691907525063
iteration 179, loss = 0.015998998656868935
iteration 180, loss = 0.018022846430540085
iteration 181, loss = 0.018155453726649284
iteration 182, loss = 0.01630835421383381
iteration 183, loss = 0.02406812272965908
iteration 184, loss = 0.01886548474431038
iteration 185, loss = 0.01971551775932312
iteration 186, loss = 0.01987624354660511
iteration 187, loss = 0.015033598057925701
iteration 188, loss = 0.01672450266778469
iteration 189, loss = 0.02580631524324417
iteration 190, loss = 0.015617815777659416
iteration 191, loss = 0.01572207175195217
iteration 192, loss = 0.017345188185572624
iteration 193, loss = 0.017295071855187416
iteration 194, loss = 0.01603022962808609
iteration 195, loss = 0.01725057326257229
iteration 196, loss = 0.019122088328003883
iteration 197, loss = 0.01551408227533102
iteration 198, loss = 0.017596213147044182
iteration 199, loss = 0.016930220648646355
iteration 200, loss = 0.016983209177851677
iteration 201, loss = 0.01815256103873253
iteration 202, loss = 0.015795648097991943
iteration 203, loss = 0.018722232431173325
iteration 204, loss = 0.022993208840489388
iteration 205, loss = 0.01589636132121086
iteration 206, loss = 0.015818553045392036
iteration 207, loss = 0.016149962320923805
iteration 208, loss = 0.018088651821017265
iteration 209, loss = 0.015473605133593082
iteration 210, loss = 0.020148523151874542
iteration 211, loss = 0.023822657763957977
iteration 212, loss = 0.023980766534805298
iteration 213, loss = 0.015660271048545837
iteration 214, loss = 0.01828363724052906
iteration 215, loss = 0.02519202046096325
iteration 216, loss = 0.018482765182852745
iteration 217, loss = 0.016138385981321335
iteration 218, loss = 0.015115201473236084
iteration 219, loss = 0.01647273078560829
iteration 220, loss = 0.020877225324511528
iteration 221, loss = 0.015194153413176537
iteration 222, loss = 0.019135070964694023
iteration 223, loss = 0.01837760955095291
iteration 224, loss = 0.0165728647261858
iteration 225, loss = 0.018609406426548958
iteration 226, loss = 0.014894396997988224
iteration 227, loss = 0.01945568062365055
iteration 228, loss = 0.016325892880558968
iteration 229, loss = 0.024469777941703796
iteration 230, loss = 0.023616427555680275
iteration 231, loss = 0.016329871490597725
iteration 232, loss = 0.019508956000208855
iteration 233, loss = 0.018154894933104515
iteration 234, loss = 0.016385382041335106
iteration 235, loss = 0.016557862982153893
iteration 236, loss = 0.01700335554778576
iteration 237, loss = 0.016220442950725555
iteration 238, loss = 0.01780383288860321
iteration 239, loss = 0.015945522114634514
iteration 240, loss = 0.01606682687997818
iteration 241, loss = 0.017116963863372803
iteration 242, loss = 0.01554267667233944
iteration 243, loss = 0.015298564918339252
iteration 244, loss = 0.019337201490998268
iteration 245, loss = 0.020972389727830887
iteration 246, loss = 0.018165256828069687
iteration 247, loss = 0.01877070590853691
iteration 248, loss = 0.0160656850785017
iteration 249, loss = 0.01769348420202732
iteration 250, loss = 0.015099586918950081
iteration 251, loss = 0.01632647030055523
iteration 252, loss = 0.025326453149318695
iteration 253, loss = 0.015955884009599686
iteration 254, loss = 0.017655592411756516
iteration 255, loss = 0.01733097806572914
iteration 256, loss = 0.019354989752173424
iteration 257, loss = 0.016355343163013458
iteration 258, loss = 0.01621316373348236
iteration 259, loss = 0.017883779481053352
iteration 260, loss = 0.016240086406469345
iteration 261, loss = 0.016311023384332657
iteration 262, loss = 0.017208512872457504
iteration 263, loss = 0.01600584387779236
iteration 264, loss = 0.015981797128915787
iteration 265, loss = 0.016105007380247116
iteration 266, loss = 0.015942562371492386
iteration 267, loss = 0.016957895830273628
iteration 268, loss = 0.016468185931444168
iteration 269, loss = 0.016319287940859795
iteration 270, loss = 0.016308043152093887
iteration 271, loss = 0.01703188754618168
iteration 272, loss = 0.015377950854599476
iteration 273, loss = 0.015508920885622501
iteration 274, loss = 0.01648581586778164
iteration 275, loss = 0.01521961111575365
iteration 276, loss = 0.015879767015576363
iteration 277, loss = 0.017539799213409424
iteration 278, loss = 0.015285072848200798
iteration 279, loss = 0.018341071903705597
iteration 280, loss = 0.015550755895674229
iteration 281, loss = 0.02336350455880165
iteration 282, loss = 0.016600826755166054
iteration 283, loss = 0.015931077301502228
iteration 284, loss = 0.016623681411147118
iteration 285, loss = 0.03197035565972328
iteration 286, loss = 0.01628030836582184
iteration 287, loss = 0.020545151084661484
iteration 288, loss = 0.017613375559449196
iteration 289, loss = 0.016840605065226555
iteration 290, loss = 0.020993757992982864
iteration 291, loss = 0.01655249483883381
iteration 292, loss = 0.015217206440865993
iteration 293, loss = 0.015410985797643661
iteration 294, loss = 0.016903497278690338
iteration 295, loss = 0.015548678115010262
iteration 296, loss = 0.01543968915939331
iteration 297, loss = 0.023857582360506058
iteration 298, loss = 0.015515821985900402
iteration 299, loss = 0.01634032651782036
iteration 300, loss = 0.01704908348619938
iteration 1, loss = 0.01623488962650299
iteration 2, loss = 0.017329875379800797
iteration 3, loss = 0.017056288197636604
iteration 4, loss = 0.015740562230348587
iteration 5, loss = 0.01476181112229824
iteration 6, loss = 0.01571224257349968
iteration 7, loss = 0.015079368837177753
iteration 8, loss = 0.01657799817621708
iteration 9, loss = 0.01757269725203514
iteration 10, loss = 0.01620335690677166
iteration 11, loss = 0.015603816136717796
iteration 12, loss = 0.015397855080664158
iteration 13, loss = 0.01910805143415928
iteration 14, loss = 0.0186285600066185
iteration 15, loss = 0.016273867338895798
iteration 16, loss = 0.01990026980638504
iteration 17, loss = 0.017447136342525482
iteration 18, loss = 0.016267377883195877
iteration 19, loss = 0.019042527303099632
iteration 20, loss = 0.016120590269565582
iteration 21, loss = 0.018579205498099327
iteration 22, loss = 0.01634170673787594
iteration 23, loss = 0.01734936237335205
iteration 24, loss = 0.016341188922524452
iteration 25, loss = 0.01649327576160431
iteration 26, loss = 0.019750824198126793
iteration 27, loss = 0.015397552400827408
iteration 28, loss = 0.01701536774635315
iteration 29, loss = 0.018229536712169647
iteration 30, loss = 0.015978701412677765
iteration 31, loss = 0.0162324421107769
iteration 32, loss = 0.016390537843108177
iteration 33, loss = 0.016218727454543114
iteration 34, loss = 0.015631798654794693
iteration 35, loss = 0.0317387580871582
iteration 36, loss = 0.018923314288258553
iteration 37, loss = 0.01633705198764801
iteration 38, loss = 0.01535572949796915
iteration 39, loss = 0.02489938959479332
iteration 40, loss = 0.016435613855719566
iteration 41, loss = 0.015952441841363907
iteration 42, loss = 0.015749357640743256
iteration 43, loss = 0.017298869788646698
iteration 44, loss = 0.01645631343126297
iteration 45, loss = 0.015623247250914574
iteration 46, loss = 0.015506209805607796
iteration 47, loss = 0.01795661449432373
iteration 48, loss = 0.01747436262667179
iteration 49, loss = 0.016715792939066887
iteration 50, loss = 0.015544525347650051
iteration 51, loss = 0.018452422693371773
iteration 52, loss = 0.016741978004574776
iteration 53, loss = 0.017764506861567497
iteration 54, loss = 0.015866877511143684
iteration 55, loss = 0.015720460563898087
iteration 56, loss = 0.01615440472960472
iteration 57, loss = 0.01581130549311638
iteration 58, loss = 0.017774567008018494
iteration 59, loss = 0.015379293821752071
iteration 60, loss = 0.01749492436647415
iteration 61, loss = 0.016884978860616684
iteration 62, loss = 0.023049822077155113
iteration 63, loss = 0.015299700200557709
iteration 64, loss = 0.016762863844633102
iteration 65, loss = 0.017488688230514526
iteration 66, loss = 0.016311613842844963
iteration 67, loss = 0.015506192110478878
iteration 68, loss = 0.015440868213772774
iteration 69, loss = 0.01863524504005909
iteration 70, loss = 0.018397847190499306
iteration 71, loss = 0.016773415729403496
iteration 72, loss = 0.025249414145946503
iteration 73, loss = 0.015164180658757687
iteration 74, loss = 0.01788233406841755
iteration 75, loss = 0.023909009993076324
iteration 76, loss = 0.016301942989230156
iteration 77, loss = 0.01590743102133274
iteration 78, loss = 0.025967709720134735
iteration 79, loss = 0.016028817743062973
iteration 80, loss = 0.026626184582710266
iteration 81, loss = 0.015735507011413574
iteration 82, loss = 0.01998801715672016
iteration 83, loss = 0.015120663680136204
iteration 84, loss = 0.016657933592796326
iteration 85, loss = 0.01930386573076248
iteration 86, loss = 0.01708146370947361
iteration 87, loss = 0.019516125321388245
iteration 88, loss = 0.0173079501837492
iteration 89, loss = 0.016571085900068283
iteration 90, loss = 0.015173457562923431
iteration 91, loss = 0.01592865213751793
iteration 92, loss = 0.017763473093509674
iteration 93, loss = 0.01842626743018627
iteration 94, loss = 0.018408257514238358
iteration 95, loss = 0.017582280561327934
iteration 96, loss = 0.017684290185570717
iteration 97, loss = 0.015505925752222538
iteration 98, loss = 0.018411356955766678
iteration 99, loss = 0.018279138952493668
iteration 100, loss = 0.01596045307815075
iteration 101, loss = 0.023662632331252098
iteration 102, loss = 0.015526208095252514
iteration 103, loss = 0.026687154546380043
iteration 104, loss = 0.015866924077272415
iteration 105, loss = 0.017237305641174316
iteration 106, loss = 0.015266005881130695
iteration 107, loss = 0.019337844103574753
iteration 108, loss = 0.015982143580913544
iteration 109, loss = 0.015243353322148323
iteration 110, loss = 0.018236564472317696
iteration 111, loss = 0.01587698794901371
iteration 112, loss = 0.017371485009789467
iteration 113, loss = 0.016542751342058182
iteration 114, loss = 0.024091197177767754
iteration 115, loss = 0.016492551192641258
iteration 116, loss = 0.01681942120194435
iteration 117, loss = 0.01590351015329361
iteration 118, loss = 0.025074180215597153
iteration 119, loss = 0.015000465326011181
iteration 120, loss = 0.017558617517352104
iteration 121, loss = 0.016226625069975853
iteration 122, loss = 0.016854217275977135
iteration 123, loss = 0.018677063286304474
iteration 124, loss = 0.025068076327443123
iteration 125, loss = 0.016438206657767296
iteration 126, loss = 0.016092389822006226
iteration 127, loss = 0.015483435243368149
iteration 128, loss = 0.016224250197410583
iteration 129, loss = 0.02012626640498638
iteration 130, loss = 0.015607615001499653
iteration 131, loss = 0.01579306833446026
iteration 132, loss = 0.015354305505752563
iteration 133, loss = 0.015950549393892288
iteration 134, loss = 0.020230546593666077
iteration 135, loss = 0.01609429344534874
iteration 136, loss = 0.022482434287667274
iteration 137, loss = 0.018064619973301888
iteration 138, loss = 0.02542487159371376
iteration 139, loss = 0.016735918819904327
iteration 140, loss = 0.016983309760689735
iteration 141, loss = 0.015270968899130821
iteration 142, loss = 0.01613554172217846
iteration 143, loss = 0.01755138672888279
iteration 144, loss = 0.0171173345297575
iteration 145, loss = 0.015403050929307938
iteration 146, loss = 0.015422686003148556
iteration 147, loss = 0.016097087413072586
iteration 148, loss = 0.017058221623301506
iteration 149, loss = 0.015909822657704353
iteration 150, loss = 0.024339664727449417
iteration 151, loss = 0.016768157482147217
iteration 152, loss = 0.01617863029241562
iteration 153, loss = 0.016002777963876724
iteration 154, loss = 0.015337381511926651
iteration 155, loss = 0.0248890221118927
iteration 156, loss = 0.016427788883447647
iteration 157, loss = 0.016566507518291473
iteration 158, loss = 0.01770845614373684
iteration 159, loss = 0.022868812084197998
iteration 160, loss = 0.01557664293795824
iteration 161, loss = 0.0168334748595953
iteration 162, loss = 0.018017971888184547
iteration 163, loss = 0.01574118249118328
iteration 164, loss = 0.017507607117295265
iteration 165, loss = 0.016898715868592262
iteration 166, loss = 0.018676089122891426
iteration 167, loss = 0.015448223799467087
iteration 168, loss = 0.01532972976565361
iteration 169, loss = 0.01772059127688408
iteration 170, loss = 0.01789015345275402
iteration 171, loss = 0.01486469991505146
iteration 172, loss = 0.023102615028619766
iteration 173, loss = 0.018647456541657448
iteration 174, loss = 0.0162595734000206
iteration 175, loss = 0.016916494816541672
iteration 176, loss = 0.020427072420716286
iteration 177, loss = 0.024074027314782143
iteration 178, loss = 0.016483310610055923
iteration 179, loss = 0.01566043123602867
iteration 180, loss = 0.01633913442492485
iteration 181, loss = 0.01613297499716282
iteration 182, loss = 0.017303911969065666
iteration 183, loss = 0.017007911577820778
iteration 184, loss = 0.01595621556043625
iteration 185, loss = 0.01607746258378029
iteration 186, loss = 0.020323118194937706
iteration 187, loss = 0.016965635120868683
iteration 188, loss = 0.01565565913915634
iteration 189, loss = 0.024020247161388397
iteration 190, loss = 0.01610352098941803
iteration 191, loss = 0.01518360897898674
iteration 192, loss = 0.01616191864013672
iteration 193, loss = 0.018059872090816498
iteration 194, loss = 0.017145458608865738
iteration 195, loss = 0.017405932769179344
iteration 196, loss = 0.017421014606952667
iteration 197, loss = 0.018573470413684845
iteration 198, loss = 0.02472757175564766
iteration 199, loss = 0.015176396816968918
iteration 200, loss = 0.01671746000647545
iteration 201, loss = 0.015809258446097374
iteration 202, loss = 0.02488875761628151
iteration 203, loss = 0.01760958321392536
iteration 204, loss = 0.01570722460746765
iteration 205, loss = 0.01542068924754858
iteration 206, loss = 0.01478515099734068
iteration 207, loss = 0.016369180753827095
iteration 208, loss = 0.015910876914858818
iteration 209, loss = 0.016848087310791016
iteration 210, loss = 0.015218784101307392
iteration 211, loss = 0.025169385597109795
iteration 212, loss = 0.024061813950538635
iteration 213, loss = 0.021269381046295166
iteration 214, loss = 0.018442491069436073
iteration 215, loss = 0.023104380816221237
iteration 216, loss = 0.017198774963617325
iteration 217, loss = 0.017819300293922424
iteration 218, loss = 0.017473336309194565
iteration 219, loss = 0.01536831259727478
iteration 220, loss = 0.01611560769379139
iteration 221, loss = 0.014839326962828636
iteration 222, loss = 0.01783791184425354
iteration 223, loss = 0.01584908924996853
iteration 224, loss = 0.025228701531887054
iteration 225, loss = 0.018595604225993156
iteration 226, loss = 0.0160464346408844
iteration 227, loss = 0.015860339626669884
iteration 228, loss = 0.016966968774795532
iteration 229, loss = 0.015907106921076775
iteration 230, loss = 0.016255654394626617
iteration 231, loss = 0.01783694140613079
iteration 232, loss = 0.01583876647055149
iteration 233, loss = 0.015649626031517982
iteration 234, loss = 0.017111187800765038
iteration 235, loss = 0.0159139484167099
iteration 236, loss = 0.016450878232717514
iteration 237, loss = 0.017618561163544655
iteration 238, loss = 0.017193611711263657
iteration 239, loss = 0.019426964223384857
iteration 240, loss = 0.019367855042219162
iteration 241, loss = 0.017623664811253548
iteration 242, loss = 0.015815505757927895
iteration 243, loss = 0.016250118613243103
iteration 244, loss = 0.016872171312570572
iteration 245, loss = 0.018598206341266632
iteration 246, loss = 0.01572745479643345
iteration 247, loss = 0.016168341040611267
iteration 248, loss = 0.019164521247148514
iteration 249, loss = 0.015160220675170422
iteration 250, loss = 0.015825849026441574
iteration 251, loss = 0.01932520419359207
iteration 252, loss = 0.01629834622144699
iteration 253, loss = 0.016548344865441322
iteration 254, loss = 0.024765396490693092
iteration 255, loss = 0.026242924854159355
iteration 256, loss = 0.0163861270993948
iteration 257, loss = 0.024682307615876198
iteration 258, loss = 0.016838518902659416
iteration 259, loss = 0.015377623960375786
iteration 260, loss = 0.01667489856481552
iteration 261, loss = 0.015877313911914825
iteration 262, loss = 0.018316563218832016
iteration 263, loss = 0.015283294953405857
iteration 264, loss = 0.018813828006386757
iteration 265, loss = 0.015966594219207764
iteration 266, loss = 0.016028109937906265
iteration 267, loss = 0.018883323296904564
iteration 268, loss = 0.01860557496547699
iteration 269, loss = 0.01648082211613655
iteration 270, loss = 0.016186203807592392
iteration 271, loss = 0.02045559324324131
iteration 272, loss = 0.01562892273068428
iteration 273, loss = 0.01616276241838932
iteration 274, loss = 0.015949998050928116
iteration 275, loss = 0.01728755421936512
iteration 276, loss = 0.018180368468165398
iteration 277, loss = 0.017051825299859047
iteration 278, loss = 0.017955293878912926
iteration 279, loss = 0.01591050624847412
iteration 280, loss = 0.016342731192708015
iteration 281, loss = 0.01601002924144268
iteration 282, loss = 0.017212266102433205
iteration 283, loss = 0.017116881906986237
iteration 284, loss = 0.015469776466488838
iteration 285, loss = 0.015835264697670937
iteration 286, loss = 0.017392368987202644
iteration 287, loss = 0.014999857172369957
iteration 288, loss = 0.018271857872605324
iteration 289, loss = 0.017639918252825737
iteration 290, loss = 0.01521237287670374
iteration 291, loss = 0.016486693173646927
iteration 292, loss = 0.015341244637966156
iteration 293, loss = 0.0178908109664917
iteration 294, loss = 0.015615838579833508
iteration 295, loss = 0.017714526504278183
iteration 296, loss = 0.018753748387098312
iteration 297, loss = 0.015191763639450073
iteration 298, loss = 0.01807638630270958
iteration 299, loss = 0.025292852893471718
iteration 300, loss = 0.020288808271288872
iteration 1, loss = 0.016802778467535973
iteration 2, loss = 0.025734059512615204
iteration 3, loss = 0.017057914286851883
iteration 4, loss = 0.016215382143855095
iteration 5, loss = 0.0162049513310194
iteration 6, loss = 0.016508590430021286
iteration 7, loss = 0.022172516211867332
iteration 8, loss = 0.018451884388923645
iteration 9, loss = 0.01648791693150997
iteration 10, loss = 0.01603669300675392
iteration 11, loss = 0.016234399750828743
iteration 12, loss = 0.017492009326815605
iteration 13, loss = 0.01664254255592823
iteration 14, loss = 0.01647118106484413
iteration 15, loss = 0.015384863130748272
iteration 16, loss = 0.01803969033062458
iteration 17, loss = 0.018072862178087234
iteration 18, loss = 0.01652345433831215
iteration 19, loss = 0.016923531889915466
iteration 20, loss = 0.01741466112434864
iteration 21, loss = 0.01740887016057968
iteration 22, loss = 0.0160367451608181
iteration 23, loss = 0.018362807109951973
iteration 24, loss = 0.015635941177606583
iteration 25, loss = 0.016086114570498466
iteration 26, loss = 0.017251869663596153
iteration 27, loss = 0.015961043536663055
iteration 28, loss = 0.01576925627887249
iteration 29, loss = 0.016528859734535217
iteration 30, loss = 0.02362401969730854
iteration 31, loss = 0.015570919960737228
iteration 32, loss = 0.01616080477833748
iteration 33, loss = 0.01795343868434429
iteration 34, loss = 0.0173669271171093
iteration 35, loss = 0.016132067888975143
iteration 36, loss = 0.0152434092015028
iteration 37, loss = 0.0157314445823431
iteration 38, loss = 0.015431352891027927
iteration 39, loss = 0.016172414645552635
iteration 40, loss = 0.02477576956152916
iteration 41, loss = 0.01589847169816494
iteration 42, loss = 0.016115248203277588
iteration 43, loss = 0.01862470433115959
iteration 44, loss = 0.0182336512953043
iteration 45, loss = 0.015921413898468018
iteration 46, loss = 0.01724536344408989
iteration 47, loss = 0.016320206224918365
iteration 48, loss = 0.019125154241919518
iteration 49, loss = 0.014951197430491447
iteration 50, loss = 0.015713069587945938
iteration 51, loss = 0.016895268112421036
iteration 52, loss = 0.01633894070982933
iteration 53, loss = 0.01909596659243107
iteration 54, loss = 0.01828213408589363
iteration 55, loss = 0.015388846397399902
iteration 56, loss = 0.016400128602981567
iteration 57, loss = 0.02454802580177784
iteration 58, loss = 0.017688455060124397
iteration 59, loss = 0.02366209402680397
iteration 60, loss = 0.01873619481921196
iteration 61, loss = 0.016331801190972328
iteration 62, loss = 0.015585820190608501
iteration 63, loss = 0.0165470652282238
iteration 64, loss = 0.01630062609910965
iteration 65, loss = 0.015151942148804665
iteration 66, loss = 0.015196488238871098
iteration 67, loss = 0.01987646147608757
iteration 68, loss = 0.01839434914290905
iteration 69, loss = 0.016999563202261925
iteration 70, loss = 0.01786443218588829
iteration 71, loss = 0.017055630683898926
iteration 72, loss = 0.019278764724731445
iteration 73, loss = 0.01667284034192562
iteration 74, loss = 0.023075155913829803
iteration 75, loss = 0.01615060307085514
iteration 76, loss = 0.017825981602072716
iteration 77, loss = 0.016117705032229424
iteration 78, loss = 0.017443131655454636
iteration 79, loss = 0.01581166870892048
iteration 80, loss = 0.015156609006226063
iteration 81, loss = 0.017997579649090767
iteration 82, loss = 0.015814103186130524
iteration 83, loss = 0.02630031667649746
iteration 84, loss = 0.015239418484270573
iteration 85, loss = 0.016892537474632263
iteration 86, loss = 0.015883052721619606
iteration 87, loss = 0.01892995648086071
iteration 88, loss = 0.017676856368780136
iteration 89, loss = 0.0152148911729455
iteration 90, loss = 0.015217713080346584
iteration 91, loss = 0.015941409394145012
iteration 92, loss = 0.015920672565698624
iteration 93, loss = 0.01714370585978031
iteration 94, loss = 0.015817254781723022
iteration 95, loss = 0.017498936504125595
iteration 96, loss = 0.01806465908885002
iteration 97, loss = 0.015472520142793655
iteration 98, loss = 0.01927645318210125
iteration 99, loss = 0.017400186508893967
iteration 100, loss = 0.016686808317899704
iteration 101, loss = 0.016140449792146683
iteration 102, loss = 0.016881398856639862
iteration 103, loss = 0.01554816123098135
iteration 104, loss = 0.01565004326403141
iteration 105, loss = 0.015827719122171402
iteration 106, loss = 0.016057364642620087
iteration 107, loss = 0.018085690215229988
iteration 108, loss = 0.019846145063638687
iteration 109, loss = 0.020778318867087364
iteration 110, loss = 0.017158273607492447
iteration 111, loss = 0.025842219591140747
iteration 112, loss = 0.018269186839461327
iteration 113, loss = 0.018555346876382828
iteration 114, loss = 0.017709534615278244
iteration 115, loss = 0.020368561148643494
iteration 116, loss = 0.017284108325839043
iteration 117, loss = 0.015088980086147785
iteration 118, loss = 0.018357962369918823
iteration 119, loss = 0.015843050554394722
iteration 120, loss = 0.015697315335273743
iteration 121, loss = 0.016523100435733795
iteration 122, loss = 0.01484082080423832
iteration 123, loss = 0.01652519777417183
iteration 124, loss = 0.025304200127720833
iteration 125, loss = 0.019683152437210083
iteration 126, loss = 0.01697804406285286
iteration 127, loss = 0.015310435555875301
iteration 128, loss = 0.017507851123809814
iteration 129, loss = 0.01656954362988472
iteration 130, loss = 0.015429995022714138
iteration 131, loss = 0.0163604486733675
iteration 132, loss = 0.01501107681542635
iteration 133, loss = 0.021167289465665817
iteration 134, loss = 0.014635284431278706
iteration 135, loss = 0.015866169705986977
iteration 136, loss = 0.018613867461681366
iteration 137, loss = 0.02050144411623478
iteration 138, loss = 0.017587820068001747
iteration 139, loss = 0.01608544960618019
iteration 140, loss = 0.015276086516678333
iteration 141, loss = 0.018997536972165108
iteration 142, loss = 0.01763644628226757
iteration 143, loss = 0.015469597652554512
iteration 144, loss = 0.01580384001135826
iteration 145, loss = 0.016721637919545174
iteration 146, loss = 0.01539040356874466
iteration 147, loss = 0.016140928491950035
iteration 148, loss = 0.015460622496902943
iteration 149, loss = 0.017424244433641434
iteration 150, loss = 0.016036830842494965
iteration 151, loss = 0.01605585776269436
iteration 152, loss = 0.018275542184710503
iteration 153, loss = 0.01866057887673378
iteration 154, loss = 0.016408199444413185
iteration 155, loss = 0.015336825512349606
iteration 156, loss = 0.01636510156095028
iteration 157, loss = 0.015789387747645378
iteration 158, loss = 0.016376996412873268
iteration 159, loss = 0.01621333137154579
iteration 160, loss = 0.02495855838060379
iteration 161, loss = 0.016231637448072433
iteration 162, loss = 0.01629827916622162
iteration 163, loss = 0.01598447561264038
iteration 164, loss = 0.017613613978028297
iteration 165, loss = 0.01656055636703968
iteration 166, loss = 0.015508657321333885
iteration 167, loss = 0.015149560756981373
iteration 168, loss = 0.016655676066875458
iteration 169, loss = 0.017954431474208832
iteration 170, loss = 0.017296526581048965
iteration 171, loss = 0.01645779050886631
iteration 172, loss = 0.0182768814265728
iteration 173, loss = 0.01582498475909233
iteration 174, loss = 0.016523605212569237
iteration 175, loss = 0.019260607659816742
iteration 176, loss = 0.016986224800348282
iteration 177, loss = 0.017072197049856186
iteration 178, loss = 0.015590274706482887
iteration 179, loss = 0.018485695123672485
iteration 180, loss = 0.023168915882706642
iteration 181, loss = 0.01841554418206215
iteration 182, loss = 0.019010692834854126
iteration 183, loss = 0.01956426538527012
iteration 184, loss = 0.016362406313419342
iteration 185, loss = 0.018567688763141632
iteration 186, loss = 0.017770811915397644
iteration 187, loss = 0.01610875129699707
iteration 188, loss = 0.023362884297966957
iteration 189, loss = 0.01894495263695717
iteration 190, loss = 0.01691184937953949
iteration 191, loss = 0.01538089569658041
iteration 192, loss = 0.02424640581011772
iteration 193, loss = 0.025735756382346153
iteration 194, loss = 0.016155218705534935
iteration 195, loss = 0.016212426126003265
iteration 196, loss = 0.016264351084828377
iteration 197, loss = 0.024862149730324745
iteration 198, loss = 0.024716828018426895
iteration 199, loss = 0.017066333442926407
iteration 200, loss = 0.01516205444931984
iteration 201, loss = 0.016562102362513542
iteration 202, loss = 0.01602954976260662
iteration 203, loss = 0.01637551560997963
iteration 204, loss = 0.0164912398904562
iteration 205, loss = 0.015985453501343727
iteration 206, loss = 0.015897873789072037
iteration 207, loss = 0.015452554449439049
iteration 208, loss = 0.01687023602426052
iteration 209, loss = 0.02471759542822838
iteration 210, loss = 0.01537599228322506
iteration 211, loss = 0.015156683512032032
iteration 212, loss = 0.019173959270119667
iteration 213, loss = 0.01620628871023655
iteration 214, loss = 0.02360941655933857
iteration 215, loss = 0.01666983589529991
iteration 216, loss = 0.03140915557742119
iteration 217, loss = 0.01772953011095524
iteration 218, loss = 0.01566147990524769
iteration 219, loss = 0.017468038946390152
iteration 220, loss = 0.016366181895136833
iteration 221, loss = 0.022342639043927193
iteration 222, loss = 0.018338846042752266
iteration 223, loss = 0.01688886247575283
iteration 224, loss = 0.019432194530963898
iteration 225, loss = 0.017602548003196716
iteration 226, loss = 0.015431269071996212
iteration 227, loss = 0.015451398678123951
iteration 228, loss = 0.021271824836730957
iteration 229, loss = 0.025758109986782074
iteration 230, loss = 0.015806779265403748
iteration 231, loss = 0.01690119132399559
iteration 232, loss = 0.015995895490050316
iteration 233, loss = 0.01741761341691017
iteration 234, loss = 0.015429956838488579
iteration 235, loss = 0.02356511726975441
iteration 236, loss = 0.016011623665690422
iteration 237, loss = 0.01670573651790619
iteration 238, loss = 0.016882577911019325
iteration 239, loss = 0.016744790598750114
iteration 240, loss = 0.017650490626692772
iteration 241, loss = 0.015357238240540028
iteration 242, loss = 0.015445707365870476
iteration 243, loss = 0.01577397808432579
iteration 244, loss = 0.024782819673419
iteration 245, loss = 0.016373982653021812
iteration 246, loss = 0.01437420304864645
iteration 247, loss = 0.017961625009775162
iteration 248, loss = 0.01582365855574608
iteration 249, loss = 0.017170939594507217
iteration 250, loss = 0.015609633177518845
iteration 251, loss = 0.024435658007860184
iteration 252, loss = 0.024447960779070854
iteration 253, loss = 0.018990661948919296
iteration 254, loss = 0.01772233657538891
iteration 255, loss = 0.01587766967713833
iteration 256, loss = 0.01941000670194626
iteration 257, loss = 0.018028652295470238
iteration 258, loss = 0.01605003885924816
iteration 259, loss = 0.014951794408261776
iteration 260, loss = 0.018458262085914612
iteration 261, loss = 0.016624556854367256
iteration 262, loss = 0.017491048201918602
iteration 263, loss = 0.01637270860373974
iteration 264, loss = 0.016507435590028763
iteration 265, loss = 0.0184260793030262
iteration 266, loss = 0.016636596992611885
iteration 267, loss = 0.01598147116601467
iteration 268, loss = 0.016212986782193184
iteration 269, loss = 0.01606995239853859
iteration 270, loss = 0.019902881234884262
iteration 271, loss = 0.016923410817980766
iteration 272, loss = 0.01838424801826477
iteration 273, loss = 0.01613438129425049
iteration 274, loss = 0.016193855553865433
iteration 275, loss = 0.015428425744175911
iteration 276, loss = 0.015438004396855831
iteration 277, loss = 0.01583045721054077
iteration 278, loss = 0.016079692170023918
iteration 279, loss = 0.016666486859321594
iteration 280, loss = 0.016880584880709648
iteration 281, loss = 0.0236403476446867
iteration 282, loss = 0.016699273139238358
iteration 283, loss = 0.02093600481748581
iteration 284, loss = 0.02316688559949398
iteration 285, loss = 0.017832599580287933
iteration 286, loss = 0.017063384875655174
iteration 287, loss = 0.018322814255952835
iteration 288, loss = 0.016591768711805344
iteration 289, loss = 0.01549898274242878
iteration 290, loss = 0.016797512769699097
iteration 291, loss = 0.01849672757089138
iteration 292, loss = 0.015973946079611778
iteration 293, loss = 0.016526155173778534
iteration 294, loss = 0.015223121270537376
iteration 295, loss = 0.016867011785507202
iteration 296, loss = 0.015239398926496506
iteration 297, loss = 0.015122547745704651
iteration 298, loss = 0.015472185797989368
iteration 299, loss = 0.01591339148581028
iteration 300, loss = 0.02213296853005886
iteration 1, loss = 0.017284993082284927
iteration 2, loss = 0.015947997570037842
iteration 3, loss = 0.023936262354254723
iteration 4, loss = 0.014896946027874947
iteration 5, loss = 0.015888769179582596
iteration 6, loss = 0.017807703465223312
iteration 7, loss = 0.022914322093129158
iteration 8, loss = 0.01612631417810917
iteration 9, loss = 0.015246288850903511
iteration 10, loss = 0.0156460739672184
iteration 11, loss = 0.01705385558307171
iteration 12, loss = 0.015960076823830605
iteration 13, loss = 0.019856689497828484
iteration 14, loss = 0.024100402370095253
iteration 15, loss = 0.016561508178710938
iteration 16, loss = 0.015495328232645988
iteration 17, loss = 0.015230047516524792
iteration 18, loss = 0.016109224408864975
iteration 19, loss = 0.0166504867374897
iteration 20, loss = 0.017689859494566917
iteration 21, loss = 0.016244376078248024
iteration 22, loss = 0.01523540448397398
iteration 23, loss = 0.015501974150538445
iteration 24, loss = 0.015776151791214943
iteration 25, loss = 0.01506679505109787
iteration 26, loss = 0.018504705280065536
iteration 27, loss = 0.01728827878832817
iteration 28, loss = 0.017463546246290207
iteration 29, loss = 0.02327309176325798
iteration 30, loss = 0.018258849158883095
iteration 31, loss = 0.015269092284142971
iteration 32, loss = 0.02458592690527439
iteration 33, loss = 0.01834208332002163
iteration 34, loss = 0.015807518735527992
iteration 35, loss = 0.017368139699101448
iteration 36, loss = 0.021909276023507118
iteration 37, loss = 0.015958553180098534
iteration 38, loss = 0.015065417625010014
iteration 39, loss = 0.015161888673901558
iteration 40, loss = 0.024518713355064392
iteration 41, loss = 0.015781285241246223
iteration 42, loss = 0.015308257192373276
iteration 43, loss = 0.015925824642181396
iteration 44, loss = 0.021600887179374695
iteration 45, loss = 0.01714187115430832
iteration 46, loss = 0.01648736000061035
iteration 47, loss = 0.016115130856633186
iteration 48, loss = 0.015286113135516644
iteration 49, loss = 0.016740864142775536
iteration 50, loss = 0.01519782468676567
iteration 51, loss = 0.016096116974949837
iteration 52, loss = 0.016097791492938995
iteration 53, loss = 0.016714101657271385
iteration 54, loss = 0.020703770220279694
iteration 55, loss = 0.016144318506121635
iteration 56, loss = 0.016686445102095604
iteration 57, loss = 0.015549944713711739
iteration 58, loss = 0.015880078077316284
iteration 59, loss = 0.015328826382756233
iteration 60, loss = 0.017115246504545212
iteration 61, loss = 0.017377816140651703
iteration 62, loss = 0.01629474014043808
iteration 63, loss = 0.01816830039024353
iteration 64, loss = 0.017367690801620483
iteration 65, loss = 0.022728856652975082
iteration 66, loss = 0.018705245107412338
iteration 67, loss = 0.024718644097447395
iteration 68, loss = 0.016842270269989967
iteration 69, loss = 0.016359509900212288
iteration 70, loss = 0.01815366931259632
iteration 71, loss = 0.016469145193696022
iteration 72, loss = 0.01624322682619095
iteration 73, loss = 0.018406856805086136
iteration 74, loss = 0.01548418402671814
iteration 75, loss = 0.015713689848780632
iteration 76, loss = 0.015880703926086426
iteration 77, loss = 0.01772257313132286
iteration 78, loss = 0.01864788495004177
iteration 79, loss = 0.01671288162469864
iteration 80, loss = 0.015555785968899727
iteration 81, loss = 0.01739558018743992
iteration 82, loss = 0.016684161499142647
iteration 83, loss = 0.022793998941779137
iteration 84, loss = 0.015550502575933933
iteration 85, loss = 0.016048025339841843
iteration 86, loss = 0.015681583434343338
iteration 87, loss = 0.015759671106934547
iteration 88, loss = 0.017025146633386612
iteration 89, loss = 0.015187414363026619
iteration 90, loss = 0.016002411022782326
iteration 91, loss = 0.016556639224290848
iteration 92, loss = 0.01759599894285202
iteration 93, loss = 0.017503300681710243
iteration 94, loss = 0.015157023444771767
iteration 95, loss = 0.015104828402400017
iteration 96, loss = 0.015070970170199871
iteration 97, loss = 0.018738064914941788
iteration 98, loss = 0.017510805279016495
iteration 99, loss = 0.019528450444340706
iteration 100, loss = 0.016635706648230553
iteration 101, loss = 0.019679846242070198
iteration 102, loss = 0.016127120703458786
iteration 103, loss = 0.023210812360048294
iteration 104, loss = 0.01638074964284897
iteration 105, loss = 0.016373593360185623
iteration 106, loss = 0.018336059525609016
iteration 107, loss = 0.015100022777915001
iteration 108, loss = 0.01584896445274353
iteration 109, loss = 0.015480788424611092
iteration 110, loss = 0.01750972494482994
iteration 111, loss = 0.0177504513412714
iteration 112, loss = 0.014914313331246376
iteration 113, loss = 0.018554599955677986
iteration 114, loss = 0.01624441146850586
iteration 115, loss = 0.017801009118556976
iteration 116, loss = 0.015294011682271957
iteration 117, loss = 0.01598392240703106
iteration 118, loss = 0.017877520993351936
iteration 119, loss = 0.016159437596797943
iteration 120, loss = 0.018345193937420845
iteration 121, loss = 0.016324959695339203
iteration 122, loss = 0.015588251873850822
iteration 123, loss = 0.015549651347100735
iteration 124, loss = 0.018887769430875778
iteration 125, loss = 0.01612032949924469
iteration 126, loss = 0.016536319628357887
iteration 127, loss = 0.018441280350089073
iteration 128, loss = 0.018118763342499733
iteration 129, loss = 0.016835346817970276
iteration 130, loss = 0.01660338044166565
iteration 131, loss = 0.015920506790280342
iteration 132, loss = 0.017277101054787636
iteration 133, loss = 0.015797441825270653
iteration 134, loss = 0.01666416972875595
iteration 135, loss = 0.016148364171385765
iteration 136, loss = 0.018031932413578033
iteration 137, loss = 0.016214914619922638
iteration 138, loss = 0.016513003036379814
iteration 139, loss = 0.01676812209188938
iteration 140, loss = 0.015692025423049927
iteration 141, loss = 0.015290198847651482
iteration 142, loss = 0.016083482652902603
iteration 143, loss = 0.015373802743852139
iteration 144, loss = 0.015952300280332565
iteration 145, loss = 0.015526444651186466
iteration 146, loss = 0.017654919996857643
iteration 147, loss = 0.01528718788176775
iteration 148, loss = 0.01936095952987671
iteration 149, loss = 0.01622713916003704
iteration 150, loss = 0.016247160732746124
iteration 151, loss = 0.01788785681128502
iteration 152, loss = 0.01576567254960537
iteration 153, loss = 0.019584888592362404
iteration 154, loss = 0.02569539099931717
iteration 155, loss = 0.014985665678977966
iteration 156, loss = 0.01770397275686264
iteration 157, loss = 0.015781564638018608
iteration 158, loss = 0.020134512335062027
iteration 159, loss = 0.015733014792203903
iteration 160, loss = 0.01541132852435112
iteration 161, loss = 0.015847746282815933
iteration 162, loss = 0.023634502664208412
iteration 163, loss = 0.018891531974077225
iteration 164, loss = 0.015749230980873108
iteration 165, loss = 0.015600892715156078
iteration 166, loss = 0.018274303525686264
iteration 167, loss = 0.025217916816473007
iteration 168, loss = 0.01720619946718216
iteration 169, loss = 0.016622912138700485
iteration 170, loss = 0.016837945207953453
iteration 171, loss = 0.01536643598228693
iteration 172, loss = 0.02321850135922432
iteration 173, loss = 0.025259552523493767
iteration 174, loss = 0.02101752534508705
iteration 175, loss = 0.01579107716679573
iteration 176, loss = 0.01476071123033762
iteration 177, loss = 0.01615949347615242
iteration 178, loss = 0.01590544544160366
iteration 179, loss = 0.025577925145626068
iteration 180, loss = 0.01621735468506813
iteration 181, loss = 0.01603293977677822
iteration 182, loss = 0.018906820565462112
iteration 183, loss = 0.01538859773427248
iteration 184, loss = 0.014925903640687466
iteration 185, loss = 0.01773480325937271
iteration 186, loss = 0.016268448904156685
iteration 187, loss = 0.016685863956809044
iteration 188, loss = 0.016451407223939896
iteration 189, loss = 0.019331658259034157
iteration 190, loss = 0.016807058826088905
iteration 191, loss = 0.024277621880173683
iteration 192, loss = 0.018677569925785065
iteration 193, loss = 0.01588594727218151
iteration 194, loss = 0.018625181168317795
iteration 195, loss = 0.0161820650100708
iteration 196, loss = 0.017688745632767677
iteration 197, loss = 0.016496242955327034
iteration 198, loss = 0.01644616201519966
iteration 199, loss = 0.0185557808727026
iteration 200, loss = 0.01594936102628708
iteration 201, loss = 0.015386482700705528
iteration 202, loss = 0.01797647774219513
iteration 203, loss = 0.01947004348039627
iteration 204, loss = 0.026106679812073708
iteration 205, loss = 0.015605919063091278
iteration 206, loss = 0.024520697072148323
iteration 207, loss = 0.022366248071193695
iteration 208, loss = 0.016261273995041847
iteration 209, loss = 0.015801118686795235
iteration 210, loss = 0.016427207738161087
iteration 211, loss = 0.018958568572998047
iteration 212, loss = 0.01497358363121748
iteration 213, loss = 0.018481217324733734
iteration 214, loss = 0.02258552424609661
iteration 215, loss = 0.017822762951254845
iteration 216, loss = 0.019985413178801537
iteration 217, loss = 0.015839649364352226
iteration 218, loss = 0.01568085514008999
iteration 219, loss = 0.014676098711788654
iteration 220, loss = 0.01620134338736534
iteration 221, loss = 0.017027169466018677
iteration 222, loss = 0.027516672387719154
iteration 223, loss = 0.027249563485383987
iteration 224, loss = 0.018071040511131287
iteration 225, loss = 0.016244590282440186
iteration 226, loss = 0.023653462529182434
iteration 227, loss = 0.015408968552947044
iteration 228, loss = 0.01574346050620079
iteration 229, loss = 0.017165929079055786
iteration 230, loss = 0.016416767612099648
iteration 231, loss = 0.0184999518096447
iteration 232, loss = 0.015021925792098045
iteration 233, loss = 0.017069613561034203
iteration 234, loss = 0.016688790172338486
iteration 235, loss = 0.017428109422326088
iteration 236, loss = 0.015683867037296295
iteration 237, loss = 0.017079412937164307
iteration 238, loss = 0.018112698569893837
iteration 239, loss = 0.0163367111235857
iteration 240, loss = 0.01646946556866169
iteration 241, loss = 0.0171598419547081
iteration 242, loss = 0.017217520624399185
iteration 243, loss = 0.016663257032632828
iteration 244, loss = 0.015601367689669132
iteration 245, loss = 0.01650567352771759
iteration 246, loss = 0.016361970454454422
iteration 247, loss = 0.016638342291116714
iteration 248, loss = 0.016415880993008614
iteration 249, loss = 0.01627844199538231
iteration 250, loss = 0.026843011379241943
iteration 251, loss = 0.017592856660485268
iteration 252, loss = 0.015985704958438873
iteration 253, loss = 0.016843874007463455
iteration 254, loss = 0.017316611483693123
iteration 255, loss = 0.01758386194705963
iteration 256, loss = 0.01492857001721859
iteration 257, loss = 0.017710797488689423
iteration 258, loss = 0.014819723553955555
iteration 259, loss = 0.023278765380382538
iteration 260, loss = 0.01801455207169056
iteration 261, loss = 0.016768058761954308
iteration 262, loss = 0.015518426895141602
iteration 263, loss = 0.015185853466391563
iteration 264, loss = 0.024531550705432892
iteration 265, loss = 0.015025720931589603
iteration 266, loss = 0.019799530506134033
iteration 267, loss = 0.016029734164476395
iteration 268, loss = 0.018034912645816803
iteration 269, loss = 0.018681107088923454
iteration 270, loss = 0.016774706542491913
iteration 271, loss = 0.014900139532983303
iteration 272, loss = 0.01622937247157097
iteration 273, loss = 0.02524475008249283
iteration 274, loss = 0.015864789485931396
iteration 275, loss = 0.019910408183932304
iteration 276, loss = 0.0157919991761446
iteration 277, loss = 0.01637580618262291
iteration 278, loss = 0.01582198217511177
iteration 279, loss = 0.01712334342300892
iteration 280, loss = 0.016885673627257347
iteration 281, loss = 0.023757321760058403
iteration 282, loss = 0.014715727418661118
iteration 283, loss = 0.01652713119983673
iteration 284, loss = 0.01560269109904766
iteration 285, loss = 0.020031597465276718
iteration 286, loss = 0.016108280047774315
iteration 287, loss = 0.016732744872570038
iteration 288, loss = 0.015224536880850792
iteration 289, loss = 0.01791081577539444
iteration 290, loss = 0.01695491373538971
iteration 291, loss = 0.017642151564359665
iteration 292, loss = 0.017044560983777046
iteration 293, loss = 0.016897102817893028
iteration 294, loss = 0.020920738577842712
iteration 295, loss = 0.01664871908724308
iteration 296, loss = 0.02208479307591915
iteration 297, loss = 0.0154505530372262
iteration 298, loss = 0.01602173037827015
iteration 299, loss = 0.0189836323261261
iteration 300, loss = 0.01723449118435383
iteration 1, loss = 0.018656937405467033
iteration 2, loss = 0.01623929850757122
iteration 3, loss = 0.016605151817202568
iteration 4, loss = 0.02241695299744606
iteration 5, loss = 0.01710568740963936
iteration 6, loss = 0.017921406775712967
iteration 7, loss = 0.01616993546485901
iteration 8, loss = 0.019799476489424706
iteration 9, loss = 0.016277996823191643
iteration 10, loss = 0.014674246311187744
iteration 11, loss = 0.015373838134109974
iteration 12, loss = 0.015591587871313095
iteration 13, loss = 0.018793296068906784
iteration 14, loss = 0.015483276918530464
iteration 15, loss = 0.015354495495557785
iteration 16, loss = 0.017455803230404854
iteration 17, loss = 0.016120728105306625
iteration 18, loss = 0.027338651940226555
iteration 19, loss = 0.016187850385904312
iteration 20, loss = 0.017443997785449028
iteration 21, loss = 0.02462027221918106
iteration 22, loss = 0.01722254976630211
iteration 23, loss = 0.014613691717386246
iteration 24, loss = 0.015670206397771835
iteration 25, loss = 0.018696455284953117
iteration 26, loss = 0.01562906801700592
iteration 27, loss = 0.015294075012207031
iteration 28, loss = 0.018427103757858276
iteration 29, loss = 0.017638344317674637
iteration 30, loss = 0.018899790942668915
iteration 31, loss = 0.022905487567186356
iteration 32, loss = 0.024506451562047005
iteration 33, loss = 0.016869595274329185
iteration 34, loss = 0.015715807676315308
iteration 35, loss = 0.017635926604270935
iteration 36, loss = 0.01741989701986313
iteration 37, loss = 0.018450098112225533
iteration 38, loss = 0.016680486500263214
iteration 39, loss = 0.018795425072312355
iteration 40, loss = 0.017803851515054703
iteration 41, loss = 0.016790583729743958
iteration 42, loss = 0.015604829415678978
iteration 43, loss = 0.017999278381466866
iteration 44, loss = 0.016209783032536507
iteration 45, loss = 0.016628704965114594
iteration 46, loss = 0.015759071335196495
iteration 47, loss = 0.01915897987782955
iteration 48, loss = 0.015595339238643646
iteration 49, loss = 0.017169591039419174
iteration 50, loss = 0.01498156227171421
iteration 51, loss = 0.017616234719753265
iteration 52, loss = 0.025374913588166237
iteration 53, loss = 0.024449989199638367
iteration 54, loss = 0.016854792833328247
iteration 55, loss = 0.017345430329442024
iteration 56, loss = 0.022552520036697388
iteration 57, loss = 0.024131350219249725
iteration 58, loss = 0.023630933836102486
iteration 59, loss = 0.019779779016971588
iteration 60, loss = 0.015356520190834999
iteration 61, loss = 0.0167457927018404
iteration 62, loss = 0.017849426716566086
iteration 63, loss = 0.015124413184821606
iteration 64, loss = 0.016480328515172005
iteration 65, loss = 0.016728010028600693
iteration 66, loss = 0.02574656717479229
iteration 67, loss = 0.016403989866375923
iteration 68, loss = 0.016068708151578903
iteration 69, loss = 0.01682260073721409
iteration 70, loss = 0.015558836050331593
iteration 71, loss = 0.026250962167978287
iteration 72, loss = 0.015671726316213608
iteration 73, loss = 0.0194088164716959
iteration 74, loss = 0.016754498705267906
iteration 75, loss = 0.01622745208442211
iteration 76, loss = 0.016413439065217972
iteration 77, loss = 0.015734439715743065
iteration 78, loss = 0.01566992700099945
iteration 79, loss = 0.015446944162249565
iteration 80, loss = 0.015125073492527008
iteration 81, loss = 0.016279209405183792
iteration 82, loss = 0.01877894438803196
iteration 83, loss = 0.01649528741836548
iteration 84, loss = 0.01603957638144493
iteration 85, loss = 0.01665514148771763
iteration 86, loss = 0.015553348697721958
iteration 87, loss = 0.015608147718012333
iteration 88, loss = 0.016627255827188492
iteration 89, loss = 0.016842123121023178
iteration 90, loss = 0.01503059733659029
iteration 91, loss = 0.014981838874518871
iteration 92, loss = 0.018194064497947693
iteration 93, loss = 0.01957199163734913
iteration 94, loss = 0.01576922833919525
iteration 95, loss = 0.01646173559129238
iteration 96, loss = 0.015448401682078838
iteration 97, loss = 0.022560540586709976
iteration 98, loss = 0.01516987755894661
iteration 99, loss = 0.01462486945092678
iteration 100, loss = 0.016520259901881218
iteration 101, loss = 0.016876274719834328
iteration 102, loss = 0.016190674155950546
iteration 103, loss = 0.018480969592928886
iteration 104, loss = 0.02002742700278759
iteration 105, loss = 0.015283197164535522
iteration 106, loss = 0.0190687645226717
iteration 107, loss = 0.01846674084663391
iteration 108, loss = 0.017916755750775337
iteration 109, loss = 0.016118844971060753
iteration 110, loss = 0.015619061887264252
iteration 111, loss = 0.01537436991930008
iteration 112, loss = 0.022662023082375526
iteration 113, loss = 0.016787569969892502
iteration 114, loss = 0.01857689395546913
iteration 115, loss = 0.017048746347427368
iteration 116, loss = 0.018184930086135864
iteration 117, loss = 0.015407280996441841
iteration 118, loss = 0.01586184650659561
iteration 119, loss = 0.018064429983496666
iteration 120, loss = 0.017251506447792053
iteration 121, loss = 0.01609487645328045
iteration 122, loss = 0.024993248283863068
iteration 123, loss = 0.015226242132484913
iteration 124, loss = 0.01683143898844719
iteration 125, loss = 0.01607818529009819
iteration 126, loss = 0.018176786601543427
iteration 127, loss = 0.01565385051071644
iteration 128, loss = 0.015815839171409607
iteration 129, loss = 0.016000021249055862
iteration 130, loss = 0.02435368113219738
iteration 131, loss = 0.015104541555047035
iteration 132, loss = 0.015241819433867931
iteration 133, loss = 0.015582071617245674
iteration 134, loss = 0.018744032829999924
iteration 135, loss = 0.01637839898467064
iteration 136, loss = 0.015334343537688255
iteration 137, loss = 0.016442518681287766
iteration 138, loss = 0.015038362704217434
iteration 139, loss = 0.01596696488559246
iteration 140, loss = 0.021941017359495163
iteration 141, loss = 0.014648099429905415
iteration 142, loss = 0.015764925628900528
iteration 143, loss = 0.017340952530503273
iteration 144, loss = 0.016285942867398262
iteration 145, loss = 0.01589822582900524
iteration 146, loss = 0.016471896320581436
iteration 147, loss = 0.02476630173623562
iteration 148, loss = 0.015454125590622425
iteration 149, loss = 0.016667919233441353
iteration 150, loss = 0.018691793084144592
iteration 151, loss = 0.016166135668754578
iteration 152, loss = 0.017008205875754356
iteration 153, loss = 0.016391757875680923
iteration 154, loss = 0.018787452951073647
iteration 155, loss = 0.017116812989115715
iteration 156, loss = 0.015461026690900326
iteration 157, loss = 0.016133368015289307
iteration 158, loss = 0.015590392984449863
iteration 159, loss = 0.01651458814740181
iteration 160, loss = 0.018080441281199455
iteration 161, loss = 0.01772559992969036
iteration 162, loss = 0.016364432871341705
iteration 163, loss = 0.015707409009337425
iteration 164, loss = 0.01732812449336052
iteration 165, loss = 0.01697779819369316
iteration 166, loss = 0.017182914540171623
iteration 167, loss = 0.01732102409005165
iteration 168, loss = 0.01718161813914776
iteration 169, loss = 0.01902647875249386
iteration 170, loss = 0.01551821455359459
iteration 171, loss = 0.01546829380095005
iteration 172, loss = 0.02479020692408085
iteration 173, loss = 0.014659402891993523
iteration 174, loss = 0.02287396229803562
iteration 175, loss = 0.016260024160146713
iteration 176, loss = 0.015568641014397144
iteration 177, loss = 0.016631539911031723
iteration 178, loss = 0.019947094842791557
iteration 179, loss = 0.015366429463028908
iteration 180, loss = 0.018378712236881256
iteration 181, loss = 0.015845507383346558
iteration 182, loss = 0.026387274265289307
iteration 183, loss = 0.015467535704374313
iteration 184, loss = 0.016980063170194626
iteration 185, loss = 0.017549093812704086
iteration 186, loss = 0.015320760197937489
iteration 187, loss = 0.016641031950712204
iteration 188, loss = 0.01576923206448555
iteration 189, loss = 0.019822780042886734
iteration 190, loss = 0.015767764300107956
iteration 191, loss = 0.017551898956298828
iteration 192, loss = 0.01778707280755043
iteration 193, loss = 0.016091521829366684
iteration 194, loss = 0.017805328592658043
iteration 195, loss = 0.01524700690060854
iteration 196, loss = 0.01718289405107498
iteration 197, loss = 0.01767631806433201
iteration 198, loss = 0.02044583112001419
iteration 199, loss = 0.015417760238051414
iteration 200, loss = 0.017762133851647377
iteration 201, loss = 0.01570085436105728
iteration 202, loss = 0.01653149537742138
iteration 203, loss = 0.014946125447750092
iteration 204, loss = 0.015955869108438492
iteration 205, loss = 0.01629621349275112
iteration 206, loss = 0.0155255738645792
iteration 207, loss = 0.01539179403334856
iteration 208, loss = 0.015822602435946465
iteration 209, loss = 0.015970710664987564
iteration 210, loss = 0.01679767295718193
iteration 211, loss = 0.01618996262550354
iteration 212, loss = 0.02500576339662075
iteration 213, loss = 0.02355288527905941
iteration 214, loss = 0.015704436227679253
iteration 215, loss = 0.01723618060350418
iteration 216, loss = 0.016091739758849144
iteration 217, loss = 0.016890157014131546
iteration 218, loss = 0.015414875000715256
iteration 219, loss = 0.01583433896303177
iteration 220, loss = 0.016065949574112892
iteration 221, loss = 0.01718202605843544
iteration 222, loss = 0.015781061723828316
iteration 223, loss = 0.01657702960073948
iteration 224, loss = 0.015931319445371628
iteration 225, loss = 0.015218961052596569
iteration 226, loss = 0.025601154193282127
iteration 227, loss = 0.015418827533721924
iteration 228, loss = 0.015619417652487755
iteration 229, loss = 0.01731290854513645
iteration 230, loss = 0.017306435853242874
iteration 231, loss = 0.016000406816601753
iteration 232, loss = 0.016836734488606453
iteration 233, loss = 0.018889321014285088
iteration 234, loss = 0.016036072745919228
iteration 235, loss = 0.022671371698379517
iteration 236, loss = 0.017092207446694374
iteration 237, loss = 0.016313619911670685
iteration 238, loss = 0.015476180240511894
iteration 239, loss = 0.015088509768247604
iteration 240, loss = 0.018438130617141724
iteration 241, loss = 0.01661253534257412
iteration 242, loss = 0.01600828766822815
iteration 243, loss = 0.015453512780368328
iteration 244, loss = 0.018237905576825142
iteration 245, loss = 0.015911946073174477
iteration 246, loss = 0.01808922365307808
iteration 247, loss = 0.02448665164411068
iteration 248, loss = 0.016605446115136147
iteration 249, loss = 0.018071500584483147
iteration 250, loss = 0.016277942806482315
iteration 251, loss = 0.018612196668982506
iteration 252, loss = 0.016883274540305138
iteration 253, loss = 0.018346477299928665
iteration 254, loss = 0.015334495343267918
iteration 255, loss = 0.014908513985574245
iteration 256, loss = 0.026811227202415466
iteration 257, loss = 0.017116732895374298
iteration 258, loss = 0.018793320283293724
iteration 259, loss = 0.01668003760278225
iteration 260, loss = 0.01558702439069748
iteration 261, loss = 0.016323063522577286
iteration 262, loss = 0.018535520881414413
iteration 263, loss = 0.022818179801106453
iteration 264, loss = 0.01774432137608528
iteration 265, loss = 0.01654544658958912
iteration 266, loss = 0.016217879951000214
iteration 267, loss = 0.016087733209133148
iteration 268, loss = 0.019102629274129868
iteration 269, loss = 0.015713244676589966
iteration 270, loss = 0.01576033979654312
iteration 271, loss = 0.016227183863520622
iteration 272, loss = 0.02048550173640251
iteration 273, loss = 0.01682579331099987
iteration 274, loss = 0.015434814617037773
iteration 275, loss = 0.017122570425271988
iteration 276, loss = 0.015479065477848053
iteration 277, loss = 0.015752239152789116
iteration 278, loss = 0.015050496906042099
iteration 279, loss = 0.024918006733059883
iteration 280, loss = 0.017072971910238266
iteration 281, loss = 0.0167952012270689
iteration 282, loss = 0.023381033912301064
iteration 283, loss = 0.021270303055644035
iteration 284, loss = 0.018003597855567932
iteration 285, loss = 0.017207376658916473
iteration 286, loss = 0.01962892711162567
iteration 287, loss = 0.015530510805547237
iteration 288, loss = 0.016449986025691032
iteration 289, loss = 0.01719062030315399
iteration 290, loss = 0.01671500690281391
iteration 291, loss = 0.015536738559603691
iteration 292, loss = 0.01825590431690216
iteration 293, loss = 0.01568790152668953
iteration 294, loss = 0.01754610426723957
iteration 295, loss = 0.014926932752132416
iteration 296, loss = 0.014986991882324219
iteration 297, loss = 0.01620081625878811
iteration 298, loss = 0.015998920425772667
iteration 299, loss = 0.015373341739177704
iteration 300, loss = 0.0221585463732481
iteration 1, loss = 0.01640336588025093
iteration 2, loss = 0.015588456764817238
iteration 3, loss = 0.015274882316589355
iteration 4, loss = 0.016998453065752983
iteration 5, loss = 0.02539345994591713
iteration 6, loss = 0.016658632084727287
iteration 7, loss = 0.015757666900753975
iteration 8, loss = 0.019973227754235268
iteration 9, loss = 0.015128889121115208
iteration 10, loss = 0.01579919084906578
iteration 11, loss = 0.017298797145485878
iteration 12, loss = 0.023134570568799973
iteration 13, loss = 0.020980607718229294
iteration 14, loss = 0.016309689730405807
iteration 15, loss = 0.0182393416762352
iteration 16, loss = 0.017563723027706146
iteration 17, loss = 0.024779269471764565
iteration 18, loss = 0.016661372035741806
iteration 19, loss = 0.01572132669389248
iteration 20, loss = 0.014642834663391113
iteration 21, loss = 0.01832403987646103
iteration 22, loss = 0.015364963561296463
iteration 23, loss = 0.019095653668045998
iteration 24, loss = 0.015247642993927002
iteration 25, loss = 0.0156756229698658
iteration 26, loss = 0.016339831054210663
iteration 27, loss = 0.0176483616232872
iteration 28, loss = 0.020752321928739548
iteration 29, loss = 0.015825651586055756
iteration 30, loss = 0.017776137217879295
iteration 31, loss = 0.017446037381887436
iteration 32, loss = 0.01980011910200119
iteration 33, loss = 0.018256589770317078
iteration 34, loss = 0.015075352042913437
iteration 35, loss = 0.022303903475403786
iteration 36, loss = 0.0196461733430624
iteration 37, loss = 0.018519902601838112
iteration 38, loss = 0.01535324938595295
iteration 39, loss = 0.019932923838496208
iteration 40, loss = 0.017593009397387505
iteration 41, loss = 0.01559114083647728
iteration 42, loss = 0.016274837777018547
iteration 43, loss = 0.015748314559459686
iteration 44, loss = 0.023172782734036446
iteration 45, loss = 0.018977921456098557
iteration 46, loss = 0.018014580011367798
iteration 47, loss = 0.01594565436244011
iteration 48, loss = 0.018968062475323677
iteration 49, loss = 0.017109958454966545
iteration 50, loss = 0.016418499872088432
iteration 51, loss = 0.01743738166987896
iteration 52, loss = 0.014840859919786453
iteration 53, loss = 0.0167130958288908
iteration 54, loss = 0.01534348912537098
iteration 55, loss = 0.014991284348070621
iteration 56, loss = 0.02475723996758461
iteration 57, loss = 0.016873203217983246
iteration 58, loss = 0.014929297380149364
iteration 59, loss = 0.015544604510068893
iteration 60, loss = 0.01852300949394703
iteration 61, loss = 0.017595535144209862
iteration 62, loss = 0.016361787915229797
iteration 63, loss = 0.016047609969973564
iteration 64, loss = 0.01759735681116581
iteration 65, loss = 0.016354383900761604
iteration 66, loss = 0.017775483429431915
iteration 67, loss = 0.017766881734132767
iteration 68, loss = 0.01637968234717846
iteration 69, loss = 0.01504007913172245
iteration 70, loss = 0.01910577155649662
iteration 71, loss = 0.016200797632336617
iteration 72, loss = 0.02508722059428692
iteration 73, loss = 0.0168401300907135
iteration 74, loss = 0.016888892278075218
iteration 75, loss = 0.01610802486538887
iteration 76, loss = 0.01574578322470188
iteration 77, loss = 0.015868786722421646
iteration 78, loss = 0.015365510247647762
iteration 79, loss = 0.015430285595357418
iteration 80, loss = 0.01797114498913288
iteration 81, loss = 0.01564307138323784
iteration 82, loss = 0.020400134846568108
iteration 83, loss = 0.017836706712841988
iteration 84, loss = 0.015984298661351204
iteration 85, loss = 0.017336862161755562
iteration 86, loss = 0.016618330031633377
iteration 87, loss = 0.015700802206993103
iteration 88, loss = 0.018251320347189903
iteration 89, loss = 0.0182783380150795
iteration 90, loss = 0.015466645359992981
iteration 91, loss = 0.01796044409275055
iteration 92, loss = 0.017249776050448418
iteration 93, loss = 0.01489809900522232
iteration 94, loss = 0.015390085056424141
iteration 95, loss = 0.014966635964810848
iteration 96, loss = 0.015726832672953606
iteration 97, loss = 0.019137512892484665
iteration 98, loss = 0.015696538612246513
iteration 99, loss = 0.015902237966656685
iteration 100, loss = 0.01980038359761238
iteration 101, loss = 0.01945413276553154
iteration 102, loss = 0.01593250408768654
iteration 103, loss = 0.016766216605901718
iteration 104, loss = 0.02457500249147415
iteration 105, loss = 0.024205680936574936
iteration 106, loss = 0.015287037007510662
iteration 107, loss = 0.017006512731313705
iteration 108, loss = 0.017286112532019615
iteration 109, loss = 0.016503242775797844
iteration 110, loss = 0.01643400825560093
iteration 111, loss = 0.015081481076776981
iteration 112, loss = 0.015357289463281631
iteration 113, loss = 0.01575694978237152
iteration 114, loss = 0.01584692671895027
iteration 115, loss = 0.017727484926581383
iteration 116, loss = 0.015945276245474815
iteration 117, loss = 0.01550302468240261
iteration 118, loss = 0.015303192660212517
iteration 119, loss = 0.015858806669712067
iteration 120, loss = 0.016170762479305267
iteration 121, loss = 0.015949374064803123
iteration 122, loss = 0.014821880497038364
iteration 123, loss = 0.015661105513572693
iteration 124, loss = 0.016439124941825867
iteration 125, loss = 0.019557079300284386
iteration 126, loss = 0.016033362597227097
iteration 127, loss = 0.015470566228032112
iteration 128, loss = 0.0147044463083148
iteration 129, loss = 0.01479171309620142
iteration 130, loss = 0.015685036778450012
iteration 131, loss = 0.01605166867375374
iteration 132, loss = 0.014906417578458786
iteration 133, loss = 0.015240971930325031
iteration 134, loss = 0.015940098091959953
iteration 135, loss = 0.015282467938959599
iteration 136, loss = 0.01532723382115364
iteration 137, loss = 0.016833193600177765
iteration 138, loss = 0.015951404348015785
iteration 139, loss = 0.026621747761964798
iteration 140, loss = 0.016625089570879936
iteration 141, loss = 0.0243181474506855
iteration 142, loss = 0.0174552034586668
iteration 143, loss = 0.01714254915714264
iteration 144, loss = 0.015589164569973946
iteration 145, loss = 0.015762178227305412
iteration 146, loss = 0.016640206798911095
iteration 147, loss = 0.01565140299499035
iteration 148, loss = 0.017880529165267944
iteration 149, loss = 0.01798325404524803
iteration 150, loss = 0.017015570774674416
iteration 151, loss = 0.022469261661171913
iteration 152, loss = 0.01547424215823412
iteration 153, loss = 0.016142411157488823
iteration 154, loss = 0.016862040385603905
iteration 155, loss = 0.016988176852464676
iteration 156, loss = 0.01623684912919998
iteration 157, loss = 0.016007238999009132
iteration 158, loss = 0.015590377151966095
iteration 159, loss = 0.023824064061045647
iteration 160, loss = 0.015858113765716553
iteration 161, loss = 0.015263820067048073
iteration 162, loss = 0.01854688860476017
iteration 163, loss = 0.023937499150633812
iteration 164, loss = 0.01601145602762699
iteration 165, loss = 0.01584543101489544
iteration 166, loss = 0.016445517539978027
iteration 167, loss = 0.015823466703295708
iteration 168, loss = 0.0191790834069252
iteration 169, loss = 0.01880832388997078
iteration 170, loss = 0.014821736142039299
iteration 171, loss = 0.015631575137376785
iteration 172, loss = 0.02363005094230175
iteration 173, loss = 0.017737360671162605
iteration 174, loss = 0.017077291384339333
iteration 175, loss = 0.016303472220897675
iteration 176, loss = 0.017111267894506454
iteration 177, loss = 0.014950629323720932
iteration 178, loss = 0.017055129632353783
iteration 179, loss = 0.01629849709570408
iteration 180, loss = 0.015332721173763275
iteration 181, loss = 0.015940610319375992
iteration 182, loss = 0.01778019219636917
iteration 183, loss = 0.01908036507666111
iteration 184, loss = 0.01601254940032959
iteration 185, loss = 0.01759330742061138
iteration 186, loss = 0.017540307715535164
iteration 187, loss = 0.015211410820484161
iteration 188, loss = 0.017319709062576294
iteration 189, loss = 0.016272922977805138
iteration 190, loss = 0.01694084331393242
iteration 191, loss = 0.016391081735491753
iteration 192, loss = 0.015816517174243927
iteration 193, loss = 0.016095731407403946
iteration 194, loss = 0.01781645230948925
iteration 195, loss = 0.015496436506509781
iteration 196, loss = 0.016840768977999687
iteration 197, loss = 0.015901245176792145
iteration 198, loss = 0.016304992139339447
iteration 199, loss = 0.01750187948346138
iteration 200, loss = 0.019024983048439026
iteration 201, loss = 0.015921205282211304
iteration 202, loss = 0.02533971332013607
iteration 203, loss = 0.017397793009877205
iteration 204, loss = 0.016789909452199936
iteration 205, loss = 0.015382721088826656
iteration 206, loss = 0.01731843873858452
iteration 207, loss = 0.01686137542128563
iteration 208, loss = 0.016780871897935867
iteration 209, loss = 0.017215315252542496
iteration 210, loss = 0.020072322338819504
iteration 211, loss = 0.014911666512489319
iteration 212, loss = 0.017895180732011795
iteration 213, loss = 0.016622694209218025
iteration 214, loss = 0.016107602044939995
iteration 215, loss = 0.016322152689099312
iteration 216, loss = 0.015616212040185928
iteration 217, loss = 0.015775246545672417
iteration 218, loss = 0.015369520522654057
iteration 219, loss = 0.01563207618892193
iteration 220, loss = 0.01737833581864834
iteration 221, loss = 0.015940137207508087
iteration 222, loss = 0.014470844529569149
iteration 223, loss = 0.01760459505021572
iteration 224, loss = 0.01792573556303978
iteration 225, loss = 0.01908782124519348
iteration 226, loss = 0.015436843037605286
iteration 227, loss = 0.015851425006985664
iteration 228, loss = 0.024657486006617546
iteration 229, loss = 0.025793224573135376
iteration 230, loss = 0.01610627770423889
iteration 231, loss = 0.016403676941990852
iteration 232, loss = 0.01614099182188511
iteration 233, loss = 0.016002502292394638
iteration 234, loss = 0.017363615334033966
iteration 235, loss = 0.01956045813858509
iteration 236, loss = 0.01536264456808567
iteration 237, loss = 0.015462514013051987
iteration 238, loss = 0.022037165239453316
iteration 239, loss = 0.019568435847759247
iteration 240, loss = 0.01611151732504368
iteration 241, loss = 0.016992677003145218
iteration 242, loss = 0.0169990174472332
iteration 243, loss = 0.01490070391446352
iteration 244, loss = 0.01503378339111805
iteration 245, loss = 0.01730075292289257
iteration 246, loss = 0.015484096482396126
iteration 247, loss = 0.026389189064502716
iteration 248, loss = 0.016315937042236328
iteration 249, loss = 0.024437932297587395
iteration 250, loss = 0.02347995899617672
iteration 251, loss = 0.016770485788583755
iteration 252, loss = 0.018136752769351006
iteration 253, loss = 0.015445095486938953
iteration 254, loss = 0.018932873383164406
iteration 255, loss = 0.017956281080842018
iteration 256, loss = 0.01542423665523529
iteration 257, loss = 0.023489193990826607
iteration 258, loss = 0.016770770773291588
iteration 259, loss = 0.01593567058444023
iteration 260, loss = 0.015888221561908722
iteration 261, loss = 0.016545714810490608
iteration 262, loss = 0.01744692400097847
iteration 263, loss = 0.01617969386279583
iteration 264, loss = 0.01629672199487686
iteration 265, loss = 0.015545282512903214
iteration 266, loss = 0.015233321115374565
iteration 267, loss = 0.016203828155994415
iteration 268, loss = 0.022274352610111237
iteration 269, loss = 0.015678860247135162
iteration 270, loss = 0.01565535180270672
iteration 271, loss = 0.023922143504023552
iteration 272, loss = 0.02269059605896473
iteration 273, loss = 0.015555033460259438
iteration 274, loss = 0.016794081777334213
iteration 275, loss = 0.01604081131517887
iteration 276, loss = 0.016116416081786156
iteration 277, loss = 0.016463954001665115
iteration 278, loss = 0.020692862570285797
iteration 279, loss = 0.01556820422410965
iteration 280, loss = 0.018052877858281136
iteration 281, loss = 0.015919174998998642
iteration 282, loss = 0.01499741617590189
iteration 283, loss = 0.02005237713456154
iteration 284, loss = 0.0235320832580328
iteration 285, loss = 0.02500382624566555
iteration 286, loss = 0.01642366126179695
iteration 287, loss = 0.016878144815564156
iteration 288, loss = 0.018253790214657784
iteration 289, loss = 0.015797339379787445
iteration 290, loss = 0.015320678241550922
iteration 291, loss = 0.0241030752658844
iteration 292, loss = 0.015256707556545734
iteration 293, loss = 0.017951149493455887
iteration 294, loss = 0.015277388505637646
iteration 295, loss = 0.01627461239695549
iteration 296, loss = 0.01547348964959383
iteration 297, loss = 0.027621015906333923
iteration 298, loss = 0.018244262784719467
iteration 299, loss = 0.01747206412255764
iteration 300, loss = 0.0156421959400177
iteration 1, loss = 0.018260592594742775
iteration 2, loss = 0.015696009621024132
iteration 3, loss = 0.018093999475240707
iteration 4, loss = 0.0177003126591444
iteration 5, loss = 0.015099383890628815
iteration 6, loss = 0.017241889610886574
iteration 7, loss = 0.022976230829954147
iteration 8, loss = 0.016319796442985535
iteration 9, loss = 0.015785451978445053
iteration 10, loss = 0.018236659467220306
iteration 11, loss = 0.016136396676301956
iteration 12, loss = 0.015066993422806263
iteration 13, loss = 0.01655975542962551
iteration 14, loss = 0.016811219975352287
iteration 15, loss = 0.01769399829208851
iteration 16, loss = 0.01609354466199875
iteration 17, loss = 0.017199620604515076
iteration 18, loss = 0.022304829210042953
iteration 19, loss = 0.01702749915421009
iteration 20, loss = 0.014768736436963081
iteration 21, loss = 0.016328787431120872
iteration 22, loss = 0.017594633623957634
iteration 23, loss = 0.015491010621190071
iteration 24, loss = 0.01828269101679325
iteration 25, loss = 0.01615000329911709
iteration 26, loss = 0.016453899443149567
iteration 27, loss = 0.015509877353906631
iteration 28, loss = 0.015793148428201675
iteration 29, loss = 0.018058493733406067
iteration 30, loss = 0.0196260716766119
iteration 31, loss = 0.017974043264985085
iteration 32, loss = 0.021929748356342316
iteration 33, loss = 0.016778351739048958
iteration 34, loss = 0.015204337425529957
iteration 35, loss = 0.022471437230706215
iteration 36, loss = 0.017282309010624886
iteration 37, loss = 0.016103338450193405
iteration 38, loss = 0.016893262043595314
iteration 39, loss = 0.015733016654849052
iteration 40, loss = 0.019815895706415176
iteration 41, loss = 0.016407666727900505
iteration 42, loss = 0.01775193214416504
iteration 43, loss = 0.016737909987568855
iteration 44, loss = 0.016492383554577827
iteration 45, loss = 0.02411620318889618
iteration 46, loss = 0.016085464507341385
iteration 47, loss = 0.02348177134990692
iteration 48, loss = 0.018751056864857674
iteration 49, loss = 0.015292158350348473
iteration 50, loss = 0.01642747037112713
iteration 51, loss = 0.015476636588573456
iteration 52, loss = 0.015992622822523117
iteration 53, loss = 0.015163901261985302
iteration 54, loss = 0.01578362099826336
iteration 55, loss = 0.018329964950680733
iteration 56, loss = 0.014469034038484097
iteration 57, loss = 0.023458926007151604
iteration 58, loss = 0.017718801274895668
iteration 59, loss = 0.016803396865725517
iteration 60, loss = 0.015320327132940292
iteration 61, loss = 0.02563595399260521
iteration 62, loss = 0.016176430508494377
iteration 63, loss = 0.015198851004242897
iteration 64, loss = 0.017146021127700806
iteration 65, loss = 0.016340309754014015
iteration 66, loss = 0.018476013094186783
iteration 67, loss = 0.0164167582988739
iteration 68, loss = 0.01530629862099886
iteration 69, loss = 0.016694918274879456
iteration 70, loss = 0.01687142625451088
iteration 71, loss = 0.016455288976430893
iteration 72, loss = 0.015240475535392761
iteration 73, loss = 0.01597662642598152
iteration 74, loss = 0.018223032355308533
iteration 75, loss = 0.015701813623309135
iteration 76, loss = 0.02732015587389469
iteration 77, loss = 0.016267523169517517
iteration 78, loss = 0.01627146638929844
iteration 79, loss = 0.018007881939411163
iteration 80, loss = 0.017847951501607895
iteration 81, loss = 0.01734575815498829
iteration 82, loss = 0.015468258410692215
iteration 83, loss = 0.01961389183998108
iteration 84, loss = 0.01630237139761448
iteration 85, loss = 0.020243046805262566
iteration 86, loss = 0.018778812140226364
iteration 87, loss = 0.017296651378273964
iteration 88, loss = 0.016428444534540176
iteration 89, loss = 0.015863966196775436
iteration 90, loss = 0.015924299135804176
iteration 91, loss = 0.015002427622675896
iteration 92, loss = 0.01959598809480667
iteration 93, loss = 0.015076984651386738
iteration 94, loss = 0.016604606062173843
iteration 95, loss = 0.017225809395313263
iteration 96, loss = 0.015530525706708431
iteration 97, loss = 0.015399607829749584
iteration 98, loss = 0.01600663550198078
iteration 99, loss = 0.01817166432738304
iteration 100, loss = 0.01592707820236683
iteration 101, loss = 0.016118638217449188
iteration 102, loss = 0.0196844469755888
iteration 103, loss = 0.0178423710167408
iteration 104, loss = 0.015523065812885761
iteration 105, loss = 0.015185407362878323
iteration 106, loss = 0.015021623112261295
iteration 107, loss = 0.015271184034645557
iteration 108, loss = 0.015965530648827553
iteration 109, loss = 0.016065482050180435
iteration 110, loss = 0.015963345766067505
iteration 111, loss = 0.01541228499263525
iteration 112, loss = 0.017340414226055145
iteration 113, loss = 0.016269609332084656
iteration 114, loss = 0.016400838270783424
iteration 115, loss = 0.018016492947936058
iteration 116, loss = 0.016216281801462173
iteration 117, loss = 0.016175273805856705
iteration 118, loss = 0.018791427835822105
iteration 119, loss = 0.016078457236289978
iteration 120, loss = 0.02338291145861149
iteration 121, loss = 0.02250056341290474
iteration 122, loss = 0.017163246870040894
iteration 123, loss = 0.0183305274695158
iteration 124, loss = 0.015676354989409447
iteration 125, loss = 0.017575914040207863
iteration 126, loss = 0.016574041917920113
iteration 127, loss = 0.016118889674544334
iteration 128, loss = 0.0317559652030468
iteration 129, loss = 0.017187392339110374
iteration 130, loss = 0.02073761448264122
iteration 131, loss = 0.017453929409384727
iteration 132, loss = 0.023228267207741737
iteration 133, loss = 0.01758527383208275
iteration 134, loss = 0.01631314866244793
iteration 135, loss = 0.01576019637286663
iteration 136, loss = 0.016155458986759186
iteration 137, loss = 0.01553395576775074
iteration 138, loss = 0.01650971733033657
iteration 139, loss = 0.016781779006123543
iteration 140, loss = 0.016188135370612144
iteration 141, loss = 0.01588999666273594
iteration 142, loss = 0.015560464933514595
iteration 143, loss = 0.01568150520324707
iteration 144, loss = 0.018180876970291138
iteration 145, loss = 0.01550169475376606
iteration 146, loss = 0.016835300251841545
iteration 147, loss = 0.024182641878724098
iteration 148, loss = 0.016069069504737854
iteration 149, loss = 0.01904541626572609
iteration 150, loss = 0.01886279694736004
iteration 151, loss = 0.020054245367646217
iteration 152, loss = 0.017255986109375954
iteration 153, loss = 0.015836067497730255
iteration 154, loss = 0.0186003427952528
iteration 155, loss = 0.017798878252506256
iteration 156, loss = 0.01545163244009018
iteration 157, loss = 0.019583184272050858
iteration 158, loss = 0.016757318750023842
iteration 159, loss = 0.017049582675099373
iteration 160, loss = 0.01635213941335678
iteration 161, loss = 0.014937504194676876
iteration 162, loss = 0.01767287217080593
iteration 163, loss = 0.015458310022950172
iteration 164, loss = 0.015452936291694641
iteration 165, loss = 0.015998754650354385
iteration 166, loss = 0.01763719506561756
iteration 167, loss = 0.022654687985777855
iteration 168, loss = 0.015343374572694302
iteration 169, loss = 0.014512610621750355
iteration 170, loss = 0.015607870183885098
iteration 171, loss = 0.015300676226615906
iteration 172, loss = 0.024448616430163383
iteration 173, loss = 0.017249181866645813
iteration 174, loss = 0.0173934418708086
iteration 175, loss = 0.019229596480727196
iteration 176, loss = 0.016816971823573112
iteration 177, loss = 0.016470585018396378
iteration 178, loss = 0.019232073798775673
iteration 179, loss = 0.022648194804787636
iteration 180, loss = 0.015335003845393658
iteration 181, loss = 0.015574028715491295
iteration 182, loss = 0.032793112099170685
iteration 183, loss = 0.017416784539818764
iteration 184, loss = 0.017828337848186493
iteration 185, loss = 0.016174137592315674
iteration 186, loss = 0.016232911497354507
iteration 187, loss = 0.018363187089562416
iteration 188, loss = 0.015839314088225365
iteration 189, loss = 0.02388390339910984
iteration 190, loss = 0.01603783667087555
iteration 191, loss = 0.0163465216755867
iteration 192, loss = 0.022989053279161453
iteration 193, loss = 0.015439641661942005
iteration 194, loss = 0.0183070320636034
iteration 195, loss = 0.018152331933379173
iteration 196, loss = 0.015379033982753754
iteration 197, loss = 0.019428517669439316
iteration 198, loss = 0.016193633899092674
iteration 199, loss = 0.016003359109163284
iteration 200, loss = 0.019335253164172173
iteration 201, loss = 0.017378145828843117
iteration 202, loss = 0.018134597688913345
iteration 203, loss = 0.01702287048101425
iteration 204, loss = 0.024400152266025543
iteration 205, loss = 0.016052555292844772
iteration 206, loss = 0.016658931970596313
iteration 207, loss = 0.015367411077022552
iteration 208, loss = 0.017460502684116364
iteration 209, loss = 0.016272682696580887
iteration 210, loss = 0.014727650210261345
iteration 211, loss = 0.01591082103550434
iteration 212, loss = 0.016162924468517303
iteration 213, loss = 0.015051417984068394
iteration 214, loss = 0.014686239883303642
iteration 215, loss = 0.01798759400844574
iteration 216, loss = 0.015056340023875237
iteration 217, loss = 0.01763537898659706
iteration 218, loss = 0.025039365515112877
iteration 219, loss = 0.01757919043302536
iteration 220, loss = 0.01779627613723278
iteration 221, loss = 0.01588689535856247
iteration 222, loss = 0.01563151180744171
iteration 223, loss = 0.015439920127391815
iteration 224, loss = 0.016819067299365997
iteration 225, loss = 0.016345469281077385
iteration 226, loss = 0.01607123762369156
iteration 227, loss = 0.025187397375702858
iteration 228, loss = 0.024002837017178535
iteration 229, loss = 0.016753308475017548
iteration 230, loss = 0.01771037094295025
iteration 231, loss = 0.018945666030049324
iteration 232, loss = 0.01714000478386879
iteration 233, loss = 0.017274264246225357
iteration 234, loss = 0.01517317071557045
iteration 235, loss = 0.015299959108233452
iteration 236, loss = 0.015180458314716816
iteration 237, loss = 0.016728058457374573
iteration 238, loss = 0.01504448801279068
iteration 239, loss = 0.01552171353250742
iteration 240, loss = 0.016412490978837013
iteration 241, loss = 0.01512296125292778
iteration 242, loss = 0.015481621958315372
iteration 243, loss = 0.01883007027208805
iteration 244, loss = 0.017239652574062347
iteration 245, loss = 0.015781162306666374
iteration 246, loss = 0.019322723150253296
iteration 247, loss = 0.017618142068386078
iteration 248, loss = 0.017041785642504692
iteration 249, loss = 0.014831108041107655
iteration 250, loss = 0.01676701195538044
iteration 251, loss = 0.017717625945806503
iteration 252, loss = 0.024263881146907806
iteration 253, loss = 0.015002314932644367
iteration 254, loss = 0.015105340629816055
iteration 255, loss = 0.01867375150322914
iteration 256, loss = 0.01588798314332962
iteration 257, loss = 0.01761944778263569
iteration 258, loss = 0.015211445279419422
iteration 259, loss = 0.01688871532678604
iteration 260, loss = 0.015932317823171616
iteration 261, loss = 0.015700137242674828
iteration 262, loss = 0.015593977645039558
iteration 263, loss = 0.01692437194287777
iteration 264, loss = 0.01591816358268261
iteration 265, loss = 0.01576540805399418
iteration 266, loss = 0.015640033408999443
iteration 267, loss = 0.014968684874475002
iteration 268, loss = 0.016743645071983337
iteration 269, loss = 0.01855257898569107
iteration 270, loss = 0.015506868250668049
iteration 271, loss = 0.024599602445960045
iteration 272, loss = 0.015223978087306023
iteration 273, loss = 0.016864772886037827
iteration 274, loss = 0.025868646800518036
iteration 275, loss = 0.016009021550416946
iteration 276, loss = 0.024164291098713875
iteration 277, loss = 0.01556994765996933
iteration 278, loss = 0.014943665824830532
iteration 279, loss = 0.017414191737771034
iteration 280, loss = 0.015176713466644287
iteration 281, loss = 0.01572457142174244
iteration 282, loss = 0.016373174265027046
iteration 283, loss = 0.015415122732520103
iteration 284, loss = 0.015402241609990597
iteration 285, loss = 0.01641257293522358
iteration 286, loss = 0.015496880747377872
iteration 287, loss = 0.01644204556941986
iteration 288, loss = 0.015099507756531239
iteration 289, loss = 0.017757846042513847
iteration 290, loss = 0.015196994878351688
iteration 291, loss = 0.01524412538856268
iteration 292, loss = 0.015947505831718445
iteration 293, loss = 0.016445856541395187
iteration 294, loss = 0.01598721370100975
iteration 295, loss = 0.015552498400211334
iteration 296, loss = 0.015177951194345951
iteration 297, loss = 0.019167177379131317
iteration 298, loss = 0.0168624147772789
iteration 299, loss = 0.015532335266470909
iteration 300, loss = 0.01673828437924385
iteration 1, loss = 0.015993308275938034
iteration 2, loss = 0.015212525613605976
iteration 3, loss = 0.015354912728071213
iteration 4, loss = 0.01619931496679783
iteration 5, loss = 0.019105467945337296
iteration 6, loss = 0.019172677770256996
iteration 7, loss = 0.015354173257946968
iteration 8, loss = 0.01871677115559578
iteration 9, loss = 0.02407618798315525
iteration 10, loss = 0.0179428830742836
iteration 11, loss = 0.018738556653261185
iteration 12, loss = 0.01468350738286972
iteration 13, loss = 0.01600675843656063
iteration 14, loss = 0.01591918058693409
iteration 15, loss = 0.014844946563243866
iteration 16, loss = 0.01571694016456604
iteration 17, loss = 0.019359275698661804
iteration 18, loss = 0.014929462224245071
iteration 19, loss = 0.015464684925973415
iteration 20, loss = 0.019778093323111534
iteration 21, loss = 0.014934450387954712
iteration 22, loss = 0.01521756500005722
iteration 23, loss = 0.01910763420164585
iteration 24, loss = 0.0170717965811491
iteration 25, loss = 0.01568707264959812
iteration 26, loss = 0.017114821821451187
iteration 27, loss = 0.01657027192413807
iteration 28, loss = 0.015189879573881626
iteration 29, loss = 0.018479013815522194
iteration 30, loss = 0.015351956710219383
iteration 31, loss = 0.016212696209549904
iteration 32, loss = 0.017222564667463303
iteration 33, loss = 0.015583351254463196
iteration 34, loss = 0.017181966453790665
iteration 35, loss = 0.01798190549015999
iteration 36, loss = 0.01490685623139143
iteration 37, loss = 0.015818122774362564
iteration 38, loss = 0.015493303537368774
iteration 39, loss = 0.01593790575861931
iteration 40, loss = 0.01761571876704693
iteration 41, loss = 0.015491004101932049
iteration 42, loss = 0.017726052552461624
iteration 43, loss = 0.015199514105916023
iteration 44, loss = 0.015671387314796448
iteration 45, loss = 0.018191080540418625
iteration 46, loss = 0.015547215938568115
iteration 47, loss = 0.014985330402851105
iteration 48, loss = 0.01842990331351757
iteration 49, loss = 0.014988387003540993
iteration 50, loss = 0.016491051763296127
iteration 51, loss = 0.015192928723990917
iteration 52, loss = 0.026639794930815697
iteration 53, loss = 0.015077328309416771
iteration 54, loss = 0.015115301124751568
iteration 55, loss = 0.01845020428299904
iteration 56, loss = 0.015470110811293125
iteration 57, loss = 0.015723368152976036
iteration 58, loss = 0.015310335904359818
iteration 59, loss = 0.017827697098255157
iteration 60, loss = 0.017275121062994003
iteration 61, loss = 0.02155858278274536
iteration 62, loss = 0.018787449225783348
iteration 63, loss = 0.015746910125017166
iteration 64, loss = 0.01538908202201128
iteration 65, loss = 0.016047867015004158
iteration 66, loss = 0.016647519543766975
iteration 67, loss = 0.018016988411545753
iteration 68, loss = 0.022597499191761017
iteration 69, loss = 0.01574169658124447
iteration 70, loss = 0.015926877036690712
iteration 71, loss = 0.018816649913787842
iteration 72, loss = 0.024391723796725273
iteration 73, loss = 0.01576017215847969
iteration 74, loss = 0.018981274217367172
iteration 75, loss = 0.015143955126404762
iteration 76, loss = 0.01565532013773918
iteration 77, loss = 0.016214735805988312
iteration 78, loss = 0.015794998034834862
iteration 79, loss = 0.01551060564815998
iteration 80, loss = 0.015402564778923988
iteration 81, loss = 0.014986412599682808
iteration 82, loss = 0.015411235392093658
iteration 83, loss = 0.02625565603375435
iteration 84, loss = 0.014756345190107822
iteration 85, loss = 0.017025860026478767
iteration 86, loss = 0.01518571749329567
iteration 87, loss = 0.01585652120411396
iteration 88, loss = 0.024785075336694717
iteration 89, loss = 0.015955917537212372
iteration 90, loss = 0.017855610698461533
iteration 91, loss = 0.015185423195362091
iteration 92, loss = 0.015875324606895447
iteration 93, loss = 0.017075426876544952
iteration 94, loss = 0.01651664637029171
iteration 95, loss = 0.022669846192002296
iteration 96, loss = 0.01690841093659401
iteration 97, loss = 0.016718300059437752
iteration 98, loss = 0.017222514376044273
iteration 99, loss = 0.015859737992286682
iteration 100, loss = 0.016341958194971085
iteration 101, loss = 0.023191938176751137
iteration 102, loss = 0.01559471059590578
iteration 103, loss = 0.017377279698848724
iteration 104, loss = 0.015012928284704685
iteration 105, loss = 0.018133217468857765
iteration 106, loss = 0.01882193610072136
iteration 107, loss = 0.02433411404490471
iteration 108, loss = 0.023231055587530136
iteration 109, loss = 0.01633739098906517
iteration 110, loss = 0.01652216911315918
iteration 111, loss = 0.015898535028100014
iteration 112, loss = 0.017309105023741722
iteration 113, loss = 0.016585519537329674
iteration 114, loss = 0.025285538285970688
iteration 115, loss = 0.015936722978949547
iteration 116, loss = 0.015587152913212776
iteration 117, loss = 0.015306620858609676
iteration 118, loss = 0.015415328554809093
iteration 119, loss = 0.01760100945830345
iteration 120, loss = 0.016037842258810997
iteration 121, loss = 0.018864110112190247
iteration 122, loss = 0.01860455982387066
iteration 123, loss = 0.02310076914727688
iteration 124, loss = 0.015559537336230278
iteration 125, loss = 0.01596697047352791
iteration 126, loss = 0.01671721786260605
iteration 127, loss = 0.015535147860646248
iteration 128, loss = 0.017468394711613655
iteration 129, loss = 0.016779987141489983
iteration 130, loss = 0.015102345496416092
iteration 131, loss = 0.015578123740851879
iteration 132, loss = 0.01576296053826809
iteration 133, loss = 0.01695413887500763
iteration 134, loss = 0.014734572730958462
iteration 135, loss = 0.016975194215774536
iteration 136, loss = 0.01585819572210312
iteration 137, loss = 0.02210402861237526
iteration 138, loss = 0.01571744866669178
iteration 139, loss = 0.01601891964673996
iteration 140, loss = 0.015448994934558868
iteration 141, loss = 0.016492262482643127
iteration 142, loss = 0.01595853827893734
iteration 143, loss = 0.015672950074076653
iteration 144, loss = 0.016989387571811676
iteration 145, loss = 0.016030041500926018
iteration 146, loss = 0.015787214040756226
iteration 147, loss = 0.02560054324567318
iteration 148, loss = 0.015260729007422924
iteration 149, loss = 0.014717268757522106
iteration 150, loss = 0.015409642830491066
iteration 151, loss = 0.01625359244644642
iteration 152, loss = 0.016652114689350128
iteration 153, loss = 0.01604638062417507
iteration 154, loss = 0.018024051561951637
iteration 155, loss = 0.02423868142068386
iteration 156, loss = 0.015414424240589142
iteration 157, loss = 0.016971329227089882
iteration 158, loss = 0.015624954365193844
iteration 159, loss = 0.016750434413552284
iteration 160, loss = 0.01569378562271595
iteration 161, loss = 0.02372688613831997
iteration 162, loss = 0.018776625394821167
iteration 163, loss = 0.017276927828788757
iteration 164, loss = 0.014626673422753811
iteration 165, loss = 0.015744496136903763
iteration 166, loss = 0.01563103124499321
iteration 167, loss = 0.023556778207421303
iteration 168, loss = 0.014732075855135918
iteration 169, loss = 0.016737183555960655
iteration 170, loss = 0.016550326719880104
iteration 171, loss = 0.015611348673701286
iteration 172, loss = 0.014677749015390873
iteration 173, loss = 0.019360238686203957
iteration 174, loss = 0.01612490601837635
iteration 175, loss = 0.015635967254638672
iteration 176, loss = 0.017367878928780556
iteration 177, loss = 0.01708642579615116
iteration 178, loss = 0.016662098467350006
iteration 179, loss = 0.01621296815574169
iteration 180, loss = 0.016531888395547867
iteration 181, loss = 0.015368396416306496
iteration 182, loss = 0.01448159758001566
iteration 183, loss = 0.017224431037902832
iteration 184, loss = 0.02950378879904747
iteration 185, loss = 0.01709027588367462
iteration 186, loss = 0.016537442803382874
iteration 187, loss = 0.016355065628886223
iteration 188, loss = 0.018378451466560364
iteration 189, loss = 0.020182568579912186
iteration 190, loss = 0.01674843579530716
iteration 191, loss = 0.01746443100273609
iteration 192, loss = 0.015124029479920864
iteration 193, loss = 0.017995286732912064
iteration 194, loss = 0.022304480895400047
iteration 195, loss = 0.017067911103367805
iteration 196, loss = 0.01635652780532837
iteration 197, loss = 0.016239620745182037
iteration 198, loss = 0.017460646107792854
iteration 199, loss = 0.017739761620759964
iteration 200, loss = 0.01992766000330448
iteration 201, loss = 0.02698114886879921
iteration 202, loss = 0.016761640086770058
iteration 203, loss = 0.015000216662883759
iteration 204, loss = 0.014849115163087845
iteration 205, loss = 0.016682319343090057
iteration 206, loss = 0.01971469074487686
iteration 207, loss = 0.015159032307565212
iteration 208, loss = 0.018575379624962807
iteration 209, loss = 0.016721583902835846
iteration 210, loss = 0.01687271147966385
iteration 211, loss = 0.01505680475383997
iteration 212, loss = 0.015052595175802708
iteration 213, loss = 0.015718858689069748
iteration 214, loss = 0.019827425479888916
iteration 215, loss = 0.017227863892912865
iteration 216, loss = 0.018003812059760094
iteration 217, loss = 0.024606509134173393
iteration 218, loss = 0.016231805086135864
iteration 219, loss = 0.01548636145889759
iteration 220, loss = 0.024995166808366776
iteration 221, loss = 0.014991299249231815
iteration 222, loss = 0.01960964873433113
iteration 223, loss = 0.027765125036239624
iteration 224, loss = 0.016905084252357483
iteration 225, loss = 0.016248144209384918
iteration 226, loss = 0.016160873696208
iteration 227, loss = 0.016096550971269608
iteration 228, loss = 0.017084231600165367
iteration 229, loss = 0.015543743968009949
iteration 230, loss = 0.01578492857515812
iteration 231, loss = 0.014864536933600903
iteration 232, loss = 0.02414240501821041
iteration 233, loss = 0.0173456072807312
iteration 234, loss = 0.01604999601840973
iteration 235, loss = 0.023838384076952934
iteration 236, loss = 0.01606435328722
iteration 237, loss = 0.018371470272541046
iteration 238, loss = 0.015394349582493305
iteration 239, loss = 0.014610006473958492
iteration 240, loss = 0.0176214799284935
iteration 241, loss = 0.02433517947793007
iteration 242, loss = 0.01735914871096611
iteration 243, loss = 0.015926910564303398
iteration 244, loss = 0.016611233353614807
iteration 245, loss = 0.01793249137699604
iteration 246, loss = 0.01570291817188263
iteration 247, loss = 0.016536615788936615
iteration 248, loss = 0.0219347532838583
iteration 249, loss = 0.015945594757795334
iteration 250, loss = 0.01577068865299225
iteration 251, loss = 0.01631290279328823
iteration 252, loss = 0.018632685765624046
iteration 253, loss = 0.016018139198422432
iteration 254, loss = 0.015148867852985859
iteration 255, loss = 0.015610182657837868
iteration 256, loss = 0.019494807347655296
iteration 257, loss = 0.01764216087758541
iteration 258, loss = 0.015023513697087765
iteration 259, loss = 0.016287412494421005
iteration 260, loss = 0.015894412994384766
iteration 261, loss = 0.015087769366800785
iteration 262, loss = 0.016368970274925232
iteration 263, loss = 0.014831749722361565
iteration 264, loss = 0.017264293506741524
iteration 265, loss = 0.016196558251976967
iteration 266, loss = 0.015599770471453667
iteration 267, loss = 0.015483943745493889
iteration 268, loss = 0.017112312838435173
iteration 269, loss = 0.017627356573939323
iteration 270, loss = 0.01629572920501232
iteration 271, loss = 0.016941018402576447
iteration 272, loss = 0.017875511199235916
iteration 273, loss = 0.02267531119287014
iteration 274, loss = 0.015582057647407055
iteration 275, loss = 0.01592087186872959
iteration 276, loss = 0.01675894856452942
iteration 277, loss = 0.01584050990641117
iteration 278, loss = 0.01659734919667244
iteration 279, loss = 0.016121087595820427
iteration 280, loss = 0.017774008214473724
iteration 281, loss = 0.018169892951846123
iteration 282, loss = 0.015421473421156406
iteration 283, loss = 0.015558729879558086
iteration 284, loss = 0.017121771350502968
iteration 285, loss = 0.024919796735048294
iteration 286, loss = 0.016221188008785248
iteration 287, loss = 0.01623806729912758
iteration 288, loss = 0.015207882039248943
iteration 289, loss = 0.016552962362766266
iteration 290, loss = 0.015692157670855522
iteration 291, loss = 0.0164482444524765
iteration 292, loss = 0.02457571215927601
iteration 293, loss = 0.01638122834265232
iteration 294, loss = 0.01725444570183754
iteration 295, loss = 0.017588315531611443
iteration 296, loss = 0.01626415364444256
iteration 297, loss = 0.023353520780801773
iteration 298, loss = 0.015576123259961605
iteration 299, loss = 0.017117703333497047
iteration 300, loss = 0.015549304895102978
iteration 1, loss = 0.016519855707883835
iteration 2, loss = 0.015506639145314693
iteration 3, loss = 0.01762028969824314
iteration 4, loss = 0.018098313361406326
iteration 5, loss = 0.01675831340253353
iteration 6, loss = 0.016790948808193207
iteration 7, loss = 0.018552104011178017
iteration 8, loss = 0.015731899067759514
iteration 9, loss = 0.016308920457959175
iteration 10, loss = 0.022402333095669746
iteration 11, loss = 0.015363997779786587
iteration 12, loss = 0.015382382087409496
iteration 13, loss = 0.015415373258292675
iteration 14, loss = 0.015352354384958744
iteration 15, loss = 0.016692927107214928
iteration 16, loss = 0.01559344120323658
iteration 17, loss = 0.01519357692450285
iteration 18, loss = 0.017873601987957954
iteration 19, loss = 0.01563476398587227
iteration 20, loss = 0.020168950781226158
iteration 21, loss = 0.014974978752434254
iteration 22, loss = 0.016255656257271767
iteration 23, loss = 0.016307983547449112
iteration 24, loss = 0.015121026895940304
iteration 25, loss = 0.01611918956041336
iteration 26, loss = 0.016724349930882454
iteration 27, loss = 0.016085458919405937
iteration 28, loss = 0.0163405928760767
iteration 29, loss = 0.017100602388381958
iteration 30, loss = 0.016369294375181198
iteration 31, loss = 0.014717081561684608
iteration 32, loss = 0.01566595770418644
iteration 33, loss = 0.02721496857702732
iteration 34, loss = 0.016274336725473404
iteration 35, loss = 0.016648879274725914
iteration 36, loss = 0.016457101330161095
iteration 37, loss = 0.016375258564949036
iteration 38, loss = 0.016237635165452957
iteration 39, loss = 0.031394921243190765
iteration 40, loss = 0.01579977571964264
iteration 41, loss = 0.015927623957395554
iteration 42, loss = 0.024366158992052078
iteration 43, loss = 0.01650194451212883
iteration 44, loss = 0.01512631680816412
iteration 45, loss = 0.017464589327573776
iteration 46, loss = 0.02675674855709076
iteration 47, loss = 0.022654572501778603
iteration 48, loss = 0.01690782979130745
iteration 49, loss = 0.016194269061088562
iteration 50, loss = 0.01703430525958538
iteration 51, loss = 0.0183840561658144
iteration 52, loss = 0.015248763374984264
iteration 53, loss = 0.015161230228841305
iteration 54, loss = 0.01606832630932331
iteration 55, loss = 0.01663448102772236
iteration 56, loss = 0.015217199921607971
iteration 57, loss = 0.015875425189733505
iteration 58, loss = 0.01671980321407318
iteration 59, loss = 0.015883907675743103
iteration 60, loss = 0.01669495552778244
iteration 61, loss = 0.018006697297096252
iteration 62, loss = 0.026341285556554794
iteration 63, loss = 0.015264925546944141
iteration 64, loss = 0.017799532040953636
iteration 65, loss = 0.01544010080397129
iteration 66, loss = 0.015513269230723381
iteration 67, loss = 0.015154304914176464
iteration 68, loss = 0.016706958413124084
iteration 69, loss = 0.015081623569130898
iteration 70, loss = 0.01496988907456398
iteration 71, loss = 0.015667907893657684
iteration 72, loss = 0.01448285486549139
iteration 73, loss = 0.022194134071469307
iteration 74, loss = 0.016211286187171936
iteration 75, loss = 0.016947051510214806
iteration 76, loss = 0.01753741130232811
iteration 77, loss = 0.018150512129068375
iteration 78, loss = 0.02222791127860546
iteration 79, loss = 0.016115544363856316
iteration 80, loss = 0.016115181148052216
iteration 81, loss = 0.016131045296788216
iteration 82, loss = 0.015875905752182007
iteration 83, loss = 0.01528348308056593
iteration 84, loss = 0.01746203564107418
iteration 85, loss = 0.017291728407144547
iteration 86, loss = 0.016901997849345207
iteration 87, loss = 0.015475133433938026
iteration 88, loss = 0.02338757924735546
iteration 89, loss = 0.016205713152885437
iteration 90, loss = 0.0173591710627079
iteration 91, loss = 0.016204098239541054
iteration 92, loss = 0.01591738499701023
iteration 93, loss = 0.0176260843873024
iteration 94, loss = 0.015334438532590866
iteration 95, loss = 0.01735616847872734
iteration 96, loss = 0.01775188371539116
iteration 97, loss = 0.015898482874035835
iteration 98, loss = 0.018690939992666245
iteration 99, loss = 0.015399151481688023
iteration 100, loss = 0.01744568720459938
iteration 101, loss = 0.0158071331679821
iteration 102, loss = 0.016692783683538437
iteration 103, loss = 0.017893502488732338
iteration 104, loss = 0.015289079397916794
iteration 105, loss = 0.015251591801643372
iteration 106, loss = 0.017878303304314613
iteration 107, loss = 0.016097882762551308
iteration 108, loss = 0.017927976325154305
iteration 109, loss = 0.01573408581316471
iteration 110, loss = 0.0157197006046772
iteration 111, loss = 0.01615709811449051
iteration 112, loss = 0.015039922669529915
iteration 113, loss = 0.015894556418061256
iteration 114, loss = 0.014629772864282131
iteration 115, loss = 0.01468941941857338
iteration 116, loss = 0.016624294221401215
iteration 117, loss = 0.02180790714919567
iteration 118, loss = 0.02881169319152832
iteration 119, loss = 0.01497148908674717
iteration 120, loss = 0.015276623889803886
iteration 121, loss = 0.024917835369706154
iteration 122, loss = 0.01825743541121483
iteration 123, loss = 0.01496709231287241
iteration 124, loss = 0.01601456105709076
iteration 125, loss = 0.01530361920595169
iteration 126, loss = 0.0170587245374918
iteration 127, loss = 0.019332008436322212
iteration 128, loss = 0.016132473945617676
iteration 129, loss = 0.01739075779914856
iteration 130, loss = 0.017625847831368446
iteration 131, loss = 0.017992202192544937
iteration 132, loss = 0.02696319855749607
iteration 133, loss = 0.016299067065119743
iteration 134, loss = 0.014903831295669079
iteration 135, loss = 0.019569173455238342
iteration 136, loss = 0.015507983043789864
iteration 137, loss = 0.016300546005368233
iteration 138, loss = 0.017175359651446342
iteration 139, loss = 0.015890542417764664
iteration 140, loss = 0.023319024592638016
iteration 141, loss = 0.021834969520568848
iteration 142, loss = 0.01732585020363331
iteration 143, loss = 0.016346631571650505
iteration 144, loss = 0.0203106626868248
iteration 145, loss = 0.016454480588436127
iteration 146, loss = 0.024368852376937866
iteration 147, loss = 0.023964382708072662
iteration 148, loss = 0.02223757468163967
iteration 149, loss = 0.016239091753959656
iteration 150, loss = 0.014622228220105171
iteration 151, loss = 0.022199271246790886
iteration 152, loss = 0.015274567529559135
iteration 153, loss = 0.015818271785974503
iteration 154, loss = 0.015255299396812916
iteration 155, loss = 0.01570056565105915
iteration 156, loss = 0.015114898793399334
iteration 157, loss = 0.015953270718455315
iteration 158, loss = 0.01502896286547184
iteration 159, loss = 0.015443067066371441
iteration 160, loss = 0.017709311097860336
iteration 161, loss = 0.01581958867609501
iteration 162, loss = 0.016058815643191338
iteration 163, loss = 0.016542434692382812
iteration 164, loss = 0.01734485849738121
iteration 165, loss = 0.0177334975451231
iteration 166, loss = 0.025321414694190025
iteration 167, loss = 0.018519682809710503
iteration 168, loss = 0.01574794575572014
iteration 169, loss = 0.01898588240146637
iteration 170, loss = 0.019067563116550446
iteration 171, loss = 0.016126368194818497
iteration 172, loss = 0.018272390589118004
iteration 173, loss = 0.016571881249547005
iteration 174, loss = 0.01643245853483677
iteration 175, loss = 0.01515317801386118
iteration 176, loss = 0.015977786853909492
iteration 177, loss = 0.016108816489577293
iteration 178, loss = 0.017184501513838768
iteration 179, loss = 0.018258916214108467
iteration 180, loss = 0.017844483256340027
iteration 181, loss = 0.016248183324933052
iteration 182, loss = 0.015276187099516392
iteration 183, loss = 0.01737687736749649
iteration 184, loss = 0.01609480381011963
iteration 185, loss = 0.016846753656864166
iteration 186, loss = 0.02432159334421158
iteration 187, loss = 0.018979530781507492
iteration 188, loss = 0.015661103650927544
iteration 189, loss = 0.01581655628979206
iteration 190, loss = 0.01849084161221981
iteration 191, loss = 0.014864886179566383
iteration 192, loss = 0.015039326623082161
iteration 193, loss = 0.016559360548853874
iteration 194, loss = 0.015379819087684155
iteration 195, loss = 0.015315010212361813
iteration 196, loss = 0.015696905553340912
iteration 197, loss = 0.014505527913570404
iteration 198, loss = 0.01574760675430298
iteration 199, loss = 0.015412341803312302
iteration 200, loss = 0.01581805944442749
iteration 201, loss = 0.015005619265139103
iteration 202, loss = 0.016496052965521812
iteration 203, loss = 0.016696635633707047
iteration 204, loss = 0.017079394310712814
iteration 205, loss = 0.014903939329087734
iteration 206, loss = 0.015417181886732578
iteration 207, loss = 0.015925485640764236
iteration 208, loss = 0.017843378707766533
iteration 209, loss = 0.015949690714478493
iteration 210, loss = 0.01848994754254818
iteration 211, loss = 0.017476502805948257
iteration 212, loss = 0.019017398357391357
iteration 213, loss = 0.015694256871938705
iteration 214, loss = 0.015854723751544952
iteration 215, loss = 0.016113130375742912
iteration 216, loss = 0.023289524018764496
iteration 217, loss = 0.01540375966578722
iteration 218, loss = 0.016094092279672623
iteration 219, loss = 0.022065771743655205
iteration 220, loss = 0.026156894862651825
iteration 221, loss = 0.01775881089270115
iteration 222, loss = 0.01651214249432087
iteration 223, loss = 0.01658698543906212
iteration 224, loss = 0.015397634357213974
iteration 225, loss = 0.017025340348482132
iteration 226, loss = 0.015675535425543785
iteration 227, loss = 0.02641301043331623
iteration 228, loss = 0.015885092318058014
iteration 229, loss = 0.01592930778861046
iteration 230, loss = 0.014759205281734467
iteration 231, loss = 0.01527189090847969
iteration 232, loss = 0.01639539562165737
iteration 233, loss = 0.019202958792448044
iteration 234, loss = 0.015271042473614216
iteration 235, loss = 0.018811838701367378
iteration 236, loss = 0.01546289399266243
iteration 237, loss = 0.015828821808099747
iteration 238, loss = 0.01561810914427042
iteration 239, loss = 0.016689663752913475
iteration 240, loss = 0.020560812205076218
iteration 241, loss = 0.015365010127425194
iteration 242, loss = 0.014998482540249825
iteration 243, loss = 0.015467114746570587
iteration 244, loss = 0.0185084231197834
iteration 245, loss = 0.01724190078675747
iteration 246, loss = 0.015872521325945854
iteration 247, loss = 0.015797793865203857
iteration 248, loss = 0.017265424132347107
iteration 249, loss = 0.016253596171736717
iteration 250, loss = 0.024413930252194405
iteration 251, loss = 0.016675135120749474
iteration 252, loss = 0.015131155960261822
iteration 253, loss = 0.016293618828058243
iteration 254, loss = 0.015367350541055202
iteration 255, loss = 0.016368549317121506
iteration 256, loss = 0.01531167421489954
iteration 257, loss = 0.016187451779842377
iteration 258, loss = 0.017636174336075783
iteration 259, loss = 0.014963648281991482
iteration 260, loss = 0.016420340165495872
iteration 261, loss = 0.016835803166031837
iteration 262, loss = 0.015008565038442612
iteration 263, loss = 0.016672028228640556
iteration 264, loss = 0.02381318435072899
iteration 265, loss = 0.014716310426592827
iteration 266, loss = 0.015461260452866554
iteration 267, loss = 0.018155038356781006
iteration 268, loss = 0.017733994871377945
iteration 269, loss = 0.029125822708010674
iteration 270, loss = 0.01805003546178341
iteration 271, loss = 0.016032574698328972
iteration 272, loss = 0.01584194228053093
iteration 273, loss = 0.01576848141849041
iteration 274, loss = 0.016970543190836906
iteration 275, loss = 0.016248680651187897
iteration 276, loss = 0.018103832378983498
iteration 277, loss = 0.021389134228229523
iteration 278, loss = 0.017155315726995468
iteration 279, loss = 0.015881868079304695
iteration 280, loss = 0.014926914125680923
iteration 281, loss = 0.015141770243644714
iteration 282, loss = 0.016163339838385582
iteration 283, loss = 0.016907384619116783
iteration 284, loss = 0.016636019572615623
iteration 285, loss = 0.017014360055327415
iteration 286, loss = 0.018675295636057854
iteration 287, loss = 0.019061241298913956
iteration 288, loss = 0.0153285451233387
iteration 289, loss = 0.015142424963414669
iteration 290, loss = 0.023872192949056625
iteration 291, loss = 0.01482648029923439
iteration 292, loss = 0.014784560538828373
iteration 293, loss = 0.01764027588069439
iteration 294, loss = 0.016130059957504272
iteration 295, loss = 0.01702948659658432
iteration 296, loss = 0.014851603657007217
iteration 297, loss = 0.02019340731203556
iteration 298, loss = 0.014661595225334167
iteration 299, loss = 0.016945555806159973
iteration 300, loss = 0.01610158197581768
iteration 1, loss = 0.017461437731981277
iteration 2, loss = 0.01596410572528839
iteration 3, loss = 0.016813689842820168
iteration 4, loss = 0.016606967896223068
iteration 5, loss = 0.02242821827530861
iteration 6, loss = 0.017414391040802002
iteration 7, loss = 0.019222799688577652
iteration 8, loss = 0.01627020165324211
iteration 9, loss = 0.01615944132208824
iteration 10, loss = 0.015860075131058693
iteration 11, loss = 0.016511134803295135
iteration 12, loss = 0.017060332000255585
iteration 13, loss = 0.015735378488898277
iteration 14, loss = 0.01583547331392765
iteration 15, loss = 0.024741334840655327
iteration 16, loss = 0.01591792330145836
iteration 17, loss = 0.015454931184649467
iteration 18, loss = 0.015302188694477081
iteration 19, loss = 0.021797029301524162
iteration 20, loss = 0.017793364822864532
iteration 21, loss = 0.016897542402148247
iteration 22, loss = 0.023299021646380424
iteration 23, loss = 0.015679331496357918
iteration 24, loss = 0.015657704323530197
iteration 25, loss = 0.01582435891032219
iteration 26, loss = 0.017030129209160805
iteration 27, loss = 0.015524458140134811
iteration 28, loss = 0.01579260639846325
iteration 29, loss = 0.015548487193882465
iteration 30, loss = 0.017321238294243813
iteration 31, loss = 0.01475860457867384
iteration 32, loss = 0.016271386295557022
iteration 33, loss = 0.01861261948943138
iteration 34, loss = 0.015745598822832108
iteration 35, loss = 0.023259902372956276
iteration 36, loss = 0.015152575448155403
iteration 37, loss = 0.017767544835805893
iteration 38, loss = 0.01814635470509529
iteration 39, loss = 0.015515143983066082
iteration 40, loss = 0.015506704337894917
iteration 41, loss = 0.018149184063076973
iteration 42, loss = 0.01766473799943924
iteration 43, loss = 0.01606622152030468
iteration 44, loss = 0.031163159757852554
iteration 45, loss = 0.022307101637125015
iteration 46, loss = 0.015022845938801765
iteration 47, loss = 0.01737147942185402
iteration 48, loss = 0.015553259290754795
iteration 49, loss = 0.0160851888358593
iteration 50, loss = 0.01924542896449566
iteration 51, loss = 0.015201299451291561
iteration 52, loss = 0.01754116080701351
iteration 53, loss = 0.01513594388961792
iteration 54, loss = 0.01532934233546257
iteration 55, loss = 0.01716011017560959
iteration 56, loss = 0.015515062026679516
iteration 57, loss = 0.016009822487831116
iteration 58, loss = 0.0164900254458189
iteration 59, loss = 0.017102843150496483
iteration 60, loss = 0.016707252711057663
iteration 61, loss = 0.014392098411917686
iteration 62, loss = 0.014945082366466522
iteration 63, loss = 0.016797998920083046
iteration 64, loss = 0.017167063429951668
iteration 65, loss = 0.017363417893648148
iteration 66, loss = 0.024304771795868874
iteration 67, loss = 0.017396705225110054
iteration 68, loss = 0.022291604429483414
iteration 69, loss = 0.015613961033523083
iteration 70, loss = 0.01773633435368538
iteration 71, loss = 0.015569561161100864
iteration 72, loss = 0.017233632504940033
iteration 73, loss = 0.017504675313830376
iteration 74, loss = 0.016899308189749718
iteration 75, loss = 0.01542569324374199
iteration 76, loss = 0.016823984682559967
iteration 77, loss = 0.01969093643128872
iteration 78, loss = 0.016472304239869118
iteration 79, loss = 0.015430716797709465
iteration 80, loss = 0.01614237204194069
iteration 81, loss = 0.017330698668956757
iteration 82, loss = 0.018394608050584793
iteration 83, loss = 0.015352383255958557
iteration 84, loss = 0.015188664197921753
iteration 85, loss = 0.015362934209406376
iteration 86, loss = 0.017099900171160698
iteration 87, loss = 0.018520090728998184
iteration 88, loss = 0.017301563173532486
iteration 89, loss = 0.018633795902132988
iteration 90, loss = 0.01796143129467964
iteration 91, loss = 0.01685585081577301
iteration 92, loss = 0.01620018482208252
iteration 93, loss = 0.018826033920049667
iteration 94, loss = 0.016649045050144196
iteration 95, loss = 0.01612807624042034
iteration 96, loss = 0.0171426422894001
iteration 97, loss = 0.024296188727021217
iteration 98, loss = 0.016024457290768623
iteration 99, loss = 0.014961175620555878
iteration 100, loss = 0.021395940333604813
iteration 101, loss = 0.01838652975857258
iteration 102, loss = 0.01744530349969864
iteration 103, loss = 0.017035482451319695
iteration 104, loss = 0.015997685492038727
iteration 105, loss = 0.023536890745162964
iteration 106, loss = 0.021260367706418037
iteration 107, loss = 0.0170028954744339
iteration 108, loss = 0.022683221846818924
iteration 109, loss = 0.015036024153232574
iteration 110, loss = 0.017734576016664505
iteration 111, loss = 0.01751575618982315
iteration 112, loss = 0.01634267531335354
iteration 113, loss = 0.015270345844328403
iteration 114, loss = 0.01654355227947235
iteration 115, loss = 0.015175031498074532
iteration 116, loss = 0.018030652776360512
iteration 117, loss = 0.016303222626447678
iteration 118, loss = 0.015611960552632809
iteration 119, loss = 0.015803100541234016
iteration 120, loss = 0.015359178185462952
iteration 121, loss = 0.018416142091155052
iteration 122, loss = 0.018001850694417953
iteration 123, loss = 0.019291752949357033
iteration 124, loss = 0.01625649444758892
iteration 125, loss = 0.015437524765729904
iteration 126, loss = 0.014727897942066193
iteration 127, loss = 0.016039831563830376
iteration 128, loss = 0.024090806022286415
iteration 129, loss = 0.015712939202785492
iteration 130, loss = 0.015717219561338425
iteration 131, loss = 0.01709662564098835
iteration 132, loss = 0.016741901636123657
iteration 133, loss = 0.01718023419380188
iteration 134, loss = 0.016462331637740135
iteration 135, loss = 0.015290061011910439
iteration 136, loss = 0.014832494780421257
iteration 137, loss = 0.015604824759066105
iteration 138, loss = 0.015898102894425392
iteration 139, loss = 0.015109911561012268
iteration 140, loss = 0.015755848959088326
iteration 141, loss = 0.01526579074561596
iteration 142, loss = 0.015541759319603443
iteration 143, loss = 0.024815134704113007
iteration 144, loss = 0.016218850389122963
iteration 145, loss = 0.01504105981439352
iteration 146, loss = 0.017274729907512665
iteration 147, loss = 0.015747208148241043
iteration 148, loss = 0.015802815556526184
iteration 149, loss = 0.01736510545015335
iteration 150, loss = 0.01721901446580887
iteration 151, loss = 0.017121294513344765
iteration 152, loss = 0.018338419497013092
iteration 153, loss = 0.015451708808541298
iteration 154, loss = 0.0299094095826149
iteration 155, loss = 0.02469407021999359
iteration 156, loss = 0.01653607189655304
iteration 157, loss = 0.017771439626812935
iteration 158, loss = 0.015687428414821625
iteration 159, loss = 0.015379992313683033
iteration 160, loss = 0.01448118593543768
iteration 161, loss = 0.017217084765434265
iteration 162, loss = 0.014780846424400806
iteration 163, loss = 0.016953403130173683
iteration 164, loss = 0.015557467937469482
iteration 165, loss = 0.015884453430771828
iteration 166, loss = 0.02312549017369747
iteration 167, loss = 0.016430899500846863
iteration 168, loss = 0.016480427235364914
iteration 169, loss = 0.015083394013345242
iteration 170, loss = 0.01573328673839569
iteration 171, loss = 0.015422439202666283
iteration 172, loss = 0.017251938581466675
iteration 173, loss = 0.018489520996809006
iteration 174, loss = 0.016306405887007713
iteration 175, loss = 0.01635417900979519
iteration 176, loss = 0.015325632877647877
iteration 177, loss = 0.015966225415468216
iteration 178, loss = 0.01853984221816063
iteration 179, loss = 0.02345266379415989
iteration 180, loss = 0.015600154176354408
iteration 181, loss = 0.01658780127763748
iteration 182, loss = 0.01666828989982605
iteration 183, loss = 0.01638416200876236
iteration 184, loss = 0.015695588663220406
iteration 185, loss = 0.017625859007239342
iteration 186, loss = 0.01739470101892948
iteration 187, loss = 0.018559414893388748
iteration 188, loss = 0.01682312786579132
iteration 189, loss = 0.014479657635092735
iteration 190, loss = 0.020313283428549767
iteration 191, loss = 0.016542624682188034
iteration 192, loss = 0.01794803887605667
iteration 193, loss = 0.017245005816221237
iteration 194, loss = 0.016748683527112007
iteration 195, loss = 0.014315227046608925
iteration 196, loss = 0.019075147807598114
iteration 197, loss = 0.016127608716487885
iteration 198, loss = 0.022926563397049904
iteration 199, loss = 0.0161704383790493
iteration 200, loss = 0.017023049294948578
iteration 201, loss = 0.016564154997467995
iteration 202, loss = 0.014928394928574562
iteration 203, loss = 0.017215605825185776
iteration 204, loss = 0.017890511080622673
iteration 205, loss = 0.015267537906765938
iteration 206, loss = 0.016344169154763222
iteration 207, loss = 0.01707388646900654
iteration 208, loss = 0.015389987267553806
iteration 209, loss = 0.015285910107195377
iteration 210, loss = 0.018102386966347694
iteration 211, loss = 0.01610352098941803
iteration 212, loss = 0.014420803636312485
iteration 213, loss = 0.015341117978096008
iteration 214, loss = 0.01920061558485031
iteration 215, loss = 0.015895230695605278
iteration 216, loss = 0.017307966947555542
iteration 217, loss = 0.017211951315402985
iteration 218, loss = 0.016221478581428528
iteration 219, loss = 0.016720544546842575
iteration 220, loss = 0.016974259167909622
iteration 221, loss = 0.024306075647473335
iteration 222, loss = 0.015360355377197266
iteration 223, loss = 0.01623641513288021
iteration 224, loss = 0.014180970378220081
iteration 225, loss = 0.015282180160284042
iteration 226, loss = 0.016741327941417694
iteration 227, loss = 0.015726987272500992
iteration 228, loss = 0.01595444232225418
iteration 229, loss = 0.01575804501771927
iteration 230, loss = 0.015440577641129494
iteration 231, loss = 0.01753409206867218
iteration 232, loss = 0.023829342797398567
iteration 233, loss = 0.015872661024332047
iteration 234, loss = 0.01824052818119526
iteration 235, loss = 0.015612040646374226
iteration 236, loss = 0.017413632944226265
iteration 237, loss = 0.015079229138791561
iteration 238, loss = 0.015996061265468597
iteration 239, loss = 0.016537263989448547
iteration 240, loss = 0.015624658204615116
iteration 241, loss = 0.014866665005683899
iteration 242, loss = 0.02531898394227028
iteration 243, loss = 0.014982940629124641
iteration 244, loss = 0.018699288368225098
iteration 245, loss = 0.023118486627936363
iteration 246, loss = 0.017976932227611542
iteration 247, loss = 0.019171210005879402
iteration 248, loss = 0.014558392576873302
iteration 249, loss = 0.015178988687694073
iteration 250, loss = 0.01566227898001671
iteration 251, loss = 0.01556289754807949
iteration 252, loss = 0.017205307260155678
iteration 253, loss = 0.024092216044664383
iteration 254, loss = 0.015918413177132607
iteration 255, loss = 0.015335733070969582
iteration 256, loss = 0.019646422937512398
iteration 257, loss = 0.016344228759407997
iteration 258, loss = 0.016816668212413788
iteration 259, loss = 0.017707988619804382
iteration 260, loss = 0.0172239039093256
iteration 261, loss = 0.01597464270889759
iteration 262, loss = 0.018224628642201424
iteration 263, loss = 0.015957636758685112
iteration 264, loss = 0.015767700970172882
iteration 265, loss = 0.017121447250247
iteration 266, loss = 0.025695297867059708
iteration 267, loss = 0.0165732279419899
iteration 268, loss = 0.015465134754776955
iteration 269, loss = 0.01520615629851818
iteration 270, loss = 0.01489452738314867
iteration 271, loss = 0.015089565888047218
iteration 272, loss = 0.018980486318469048
iteration 273, loss = 0.014864838682115078
iteration 274, loss = 0.015672694891691208
iteration 275, loss = 0.014742367900907993
iteration 276, loss = 0.022570839151740074
iteration 277, loss = 0.01630796119570732
iteration 278, loss = 0.015682712197303772
iteration 279, loss = 0.01589110493659973
iteration 280, loss = 0.015068004839122295
iteration 281, loss = 0.024345090612769127
iteration 282, loss = 0.016268642619252205
iteration 283, loss = 0.0168753731995821
iteration 284, loss = 0.014809632673859596
iteration 285, loss = 0.023152029141783714
iteration 286, loss = 0.016001978889107704
iteration 287, loss = 0.014784249477088451
iteration 288, loss = 0.014985647052526474
iteration 289, loss = 0.014828145503997803
iteration 290, loss = 0.01526164822280407
iteration 291, loss = 0.016319112852215767
iteration 292, loss = 0.01602012664079666
iteration 293, loss = 0.015477682463824749
iteration 294, loss = 0.016125669702887535
iteration 295, loss = 0.01617276482284069
iteration 296, loss = 0.015941590070724487
iteration 297, loss = 0.023558340966701508
iteration 298, loss = 0.01730593852698803
iteration 299, loss = 0.016172269359230995
iteration 300, loss = 0.015546180307865143

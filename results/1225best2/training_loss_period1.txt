iteration 0, loss = 0.4912721514701843
iteration 1, loss = 0.4972515404224396
iteration 2, loss = 0.49839287996292114
iteration 3, loss = 0.4993637204170227
iteration 4, loss = 0.49459531903266907
iteration 5, loss = 0.48711615800857544
iteration 6, loss = 0.4954378306865692
iteration 7, loss = 0.49649739265441895
iteration 8, loss = 0.5029299259185791
iteration 9, loss = 0.5009353160858154
iteration 10, loss = 0.4923745095729828
iteration 11, loss = 0.5041511654853821
iteration 12, loss = 0.4882284998893738
iteration 13, loss = 0.501594603061676
iteration 14, loss = 0.4970044195652008
iteration 15, loss = 0.4987851083278656
iteration 16, loss = 0.4844847321510315
iteration 17, loss = 0.4981744885444641
iteration 18, loss = 0.5002813339233398
iteration 19, loss = 0.5037941932678223
iteration 20, loss = 0.4962632656097412
iteration 21, loss = 0.503681480884552
iteration 22, loss = 0.49315688014030457
iteration 23, loss = 0.4953112006187439
iteration 24, loss = 0.49727773666381836
iteration 25, loss = 0.5000216364860535
iteration 26, loss = 0.4995552897453308
iteration 27, loss = 0.4916050434112549
iteration 28, loss = 0.4940437972545624
iteration 29, loss = 0.5004321336746216
iteration 30, loss = 0.4950282573699951
iteration 31, loss = 0.4986848831176758
iteration 32, loss = 0.49023672938346863
iteration 33, loss = 0.49956732988357544
iteration 34, loss = 0.4998025894165039
iteration 35, loss = 0.4951210021972656
iteration 36, loss = 0.49379992485046387
iteration 37, loss = 0.5013048648834229
iteration 38, loss = 0.4940115809440613
iteration 39, loss = 0.49040669202804565
iteration 40, loss = 0.49312275648117065
iteration 41, loss = 0.4902611970901489
iteration 42, loss = 0.48495906591415405
iteration 43, loss = 0.5068742036819458
iteration 44, loss = 0.49615031480789185
iteration 45, loss = 0.49576497077941895
iteration 46, loss = 0.4936665892601013
iteration 47, loss = 0.4883614778518677
iteration 48, loss = 0.5041356682777405
iteration 49, loss = 0.48647481203079224
iteration 50, loss = 0.4938945174217224
iteration 51, loss = 0.485481321811676
iteration 52, loss = 0.49414563179016113
iteration 53, loss = 0.4877782464027405
iteration 54, loss = 0.4865453839302063
iteration 55, loss = 0.4999838173389435
iteration 56, loss = 0.48820704221725464
iteration 57, loss = 0.49098825454711914
iteration 58, loss = 0.4899546802043915
iteration 59, loss = 0.49314600229263306
iteration 60, loss = 0.4912227392196655
iteration 61, loss = 0.49682384729385376
iteration 62, loss = 0.48413699865341187
iteration 63, loss = 0.49195343255996704
iteration 64, loss = 0.4939083755016327
iteration 65, loss = 0.48751017451286316
iteration 66, loss = 0.47950509190559387
iteration 67, loss = 0.49386751651763916
iteration 68, loss = 0.48657068610191345
iteration 69, loss = 0.4898539185523987
iteration 70, loss = 0.4844769239425659
iteration 71, loss = 0.49705976247787476
iteration 72, loss = 0.4950776994228363
iteration 73, loss = 0.4919091463088989
iteration 74, loss = 0.4916532635688782
iteration 75, loss = 0.48939332365989685
iteration 76, loss = 0.4920906126499176
iteration 77, loss = 0.489587664604187
iteration 78, loss = 0.4901127219200134
iteration 79, loss = 0.49738603830337524
iteration 80, loss = 0.49527648091316223
iteration 81, loss = 0.5021795034408569
iteration 82, loss = 0.48428505659103394
iteration 83, loss = 0.48992642760276794
iteration 84, loss = 0.48795992136001587
iteration 85, loss = 0.49011269211769104
iteration 86, loss = 0.4950556755065918
iteration 87, loss = 0.49347132444381714
iteration 88, loss = 0.49861031770706177
iteration 89, loss = 0.4941693842411041
iteration 90, loss = 0.4881567358970642
iteration 91, loss = 0.48480701446533203
iteration 92, loss = 0.4967409372329712
iteration 93, loss = 0.4941486716270447
iteration 94, loss = 0.4936590790748596
iteration 95, loss = 0.4906010925769806
iteration 96, loss = 0.4924151301383972
iteration 97, loss = 0.4861796498298645
iteration 98, loss = 0.4996287226676941
iteration 99, loss = 0.4849974513053894
iteration 100, loss = 0.4928237795829773
iteration 101, loss = 0.4883008599281311
iteration 102, loss = 0.4803623557090759
iteration 103, loss = 0.48742228746414185
iteration 104, loss = 0.4945380687713623
iteration 105, loss = 0.4939039647579193
iteration 106, loss = 0.48782238364219666
iteration 107, loss = 0.48246875405311584
iteration 108, loss = 0.49328193068504333
iteration 109, loss = 0.49259865283966064
iteration 110, loss = 0.4954700469970703
iteration 111, loss = 0.5043537616729736
iteration 112, loss = 0.4826774597167969
iteration 113, loss = 0.4894372820854187
iteration 114, loss = 0.48875701427459717
iteration 115, loss = 0.490960955619812
iteration 116, loss = 0.487315833568573
iteration 117, loss = 0.4893944263458252
iteration 118, loss = 0.49565982818603516
iteration 119, loss = 0.4785599708557129
iteration 120, loss = 0.4794182777404785
iteration 121, loss = 0.4944150149822235
iteration 122, loss = 0.4835672378540039
iteration 123, loss = 0.4915359318256378
iteration 124, loss = 0.49033018946647644
iteration 125, loss = 0.48638391494750977
iteration 126, loss = 0.4906170666217804
iteration 127, loss = 0.4897998869419098
iteration 128, loss = 0.49039122462272644
iteration 129, loss = 0.47740447521209717
iteration 130, loss = 0.4857673645019531
iteration 131, loss = 0.4940704107284546
iteration 132, loss = 0.49028080701828003
iteration 133, loss = 0.49209660291671753
iteration 134, loss = 0.480729341506958
iteration 135, loss = 0.4856237471103668
iteration 136, loss = 0.49524223804473877
iteration 137, loss = 0.4941917955875397
iteration 138, loss = 0.4801296591758728
iteration 139, loss = 0.49165892601013184
iteration 140, loss = 0.4934452176094055
iteration 141, loss = 0.473057359457016
iteration 142, loss = 0.48027217388153076
iteration 143, loss = 0.4931079149246216
iteration 144, loss = 0.4763239324092865
iteration 145, loss = 0.49572494626045227
iteration 146, loss = 0.4924103021621704
iteration 147, loss = 0.4772200584411621
iteration 148, loss = 0.5007017254829407
iteration 149, loss = 0.4771423935890198
iteration 150, loss = 0.4920227527618408
iteration 151, loss = 0.4871745705604553
iteration 152, loss = 0.49417340755462646
iteration 153, loss = 0.48350971937179565
iteration 154, loss = 0.49399876594543457
iteration 155, loss = 0.4767036437988281
iteration 156, loss = 0.49129706621170044
iteration 157, loss = 0.4842927157878876
iteration 158, loss = 0.47836780548095703
iteration 159, loss = 0.4909941256046295
iteration 160, loss = 0.4852943420410156
iteration 161, loss = 0.48054277896881104
iteration 162, loss = 0.48746830224990845
iteration 163, loss = 0.4872434139251709
iteration 164, loss = 0.4843977987766266
iteration 165, loss = 0.48268479108810425
iteration 166, loss = 0.48595547676086426
iteration 167, loss = 0.49001604318618774
iteration 168, loss = 0.4861721396446228
iteration 169, loss = 0.4877568185329437
iteration 170, loss = 0.48313266038894653
iteration 171, loss = 0.4861300587654114
iteration 172, loss = 0.48098573088645935
iteration 173, loss = 0.4842562675476074
iteration 174, loss = 0.4882854223251343
iteration 175, loss = 0.48560166358947754
iteration 176, loss = 0.473150372505188
iteration 177, loss = 0.47670042514801025
iteration 178, loss = 0.47272223234176636
iteration 179, loss = 0.48834228515625
iteration 180, loss = 0.4876801371574402
iteration 181, loss = 0.4909578561782837
iteration 182, loss = 0.4795606732368469
iteration 183, loss = 0.4824299216270447
iteration 184, loss = 0.4873616099357605
iteration 185, loss = 0.4915354251861572
iteration 186, loss = 0.4804645776748657
iteration 187, loss = 0.48244068026542664
iteration 188, loss = 0.492218554019928
iteration 189, loss = 0.47784385085105896
iteration 190, loss = 0.48142826557159424
iteration 191, loss = 0.4881746172904968
iteration 192, loss = 0.47904759645462036
iteration 193, loss = 0.47939175367355347
iteration 194, loss = 0.479085773229599
iteration 195, loss = 0.4761000871658325
iteration 196, loss = 0.4823843240737915
iteration 197, loss = 0.4955715835094452
iteration 198, loss = 0.4777015447616577
iteration 199, loss = 0.47583097219467163
iteration 200, loss = 0.48176097869873047
iteration 201, loss = 0.47231534123420715
iteration 202, loss = 0.48465603590011597
iteration 203, loss = 0.4820203185081482
iteration 204, loss = 0.4994173049926758
iteration 205, loss = 0.47860419750213623
iteration 206, loss = 0.4688578248023987
iteration 207, loss = 0.4723849594593048
iteration 208, loss = 0.4931272566318512
iteration 209, loss = 0.47311168909072876
iteration 210, loss = 0.48471593856811523
iteration 211, loss = 0.4817586839199066
iteration 212, loss = 0.47796642780303955
iteration 213, loss = 0.48757556080818176
iteration 214, loss = 0.48449718952178955
iteration 215, loss = 0.47912880778312683
iteration 216, loss = 0.4814324975013733
iteration 217, loss = 0.48881328105926514
iteration 218, loss = 0.49079442024230957
iteration 219, loss = 0.4826754629611969
iteration 220, loss = 0.4722995460033417
iteration 221, loss = 0.48054367303848267
iteration 222, loss = 0.46916335821151733
iteration 223, loss = 0.47925859689712524
iteration 224, loss = 0.48587679862976074
iteration 225, loss = 0.4892677664756775
iteration 226, loss = 0.48278236389160156
iteration 227, loss = 0.4953742027282715
iteration 228, loss = 0.47498559951782227
iteration 229, loss = 0.47118285298347473
iteration 230, loss = 0.4946250319480896
iteration 231, loss = 0.47755104303359985
iteration 232, loss = 0.47668129205703735
iteration 233, loss = 0.4848020076751709
iteration 234, loss = 0.473488450050354
iteration 235, loss = 0.4742216169834137
iteration 236, loss = 0.4790043830871582
iteration 237, loss = 0.47940880060195923
iteration 238, loss = 0.4768991470336914
iteration 239, loss = 0.48345619440078735
iteration 240, loss = 0.47581011056900024
iteration 241, loss = 0.48011207580566406
iteration 242, loss = 0.48439136147499084
iteration 243, loss = 0.4902415871620178
iteration 244, loss = 0.4852628707885742
iteration 245, loss = 0.4800302982330322
iteration 246, loss = 0.48157739639282227
iteration 247, loss = 0.4830775856971741
iteration 248, loss = 0.4797573685646057
iteration 249, loss = 0.4875403046607971
iteration 250, loss = 0.47607213258743286
iteration 251, loss = 0.4803856611251831
iteration 252, loss = 0.4855419993400574
iteration 253, loss = 0.47621241211891174
iteration 254, loss = 0.4854493737220764
iteration 255, loss = 0.46687185764312744
iteration 256, loss = 0.4777730107307434
iteration 257, loss = 0.47687584161758423
iteration 258, loss = 0.4822751581668854
iteration 259, loss = 0.47564131021499634
iteration 260, loss = 0.47341442108154297
iteration 261, loss = 0.48828452825546265
iteration 262, loss = 0.4761463403701782
iteration 263, loss = 0.47768348455429077
iteration 264, loss = 0.468705415725708
iteration 265, loss = 0.46892690658569336
iteration 266, loss = 0.4742334485054016
iteration 267, loss = 0.48157235980033875
iteration 268, loss = 0.47338515520095825
iteration 269, loss = 0.4867349863052368
iteration 270, loss = 0.47792479395866394
iteration 271, loss = 0.4761168956756592
iteration 272, loss = 0.48914748430252075
iteration 273, loss = 0.4808412194252014
iteration 274, loss = 0.46592628955841064
iteration 275, loss = 0.48150205612182617
iteration 276, loss = 0.48195895552635193
iteration 277, loss = 0.4632655382156372
iteration 278, loss = 0.4717300832271576
iteration 279, loss = 0.4723159968852997
iteration 280, loss = 0.48188915848731995
iteration 281, loss = 0.476142942905426
iteration 282, loss = 0.478873074054718
iteration 283, loss = 0.476806104183197
iteration 284, loss = 0.4699357748031616
iteration 285, loss = 0.47862792015075684
iteration 286, loss = 0.4706486165523529
iteration 287, loss = 0.46951502561569214
iteration 288, loss = 0.468680739402771
iteration 289, loss = 0.4753796458244324
iteration 290, loss = 0.46708792448043823
iteration 291, loss = 0.4765048623085022
iteration 292, loss = 0.4814012348651886
iteration 293, loss = 0.4649167060852051
iteration 294, loss = 0.4788590967655182
iteration 295, loss = 0.47774001955986023
iteration 296, loss = 0.4805527925491333
iteration 297, loss = 0.4854247272014618
iteration 298, loss = 0.4665659964084625
iteration 299, loss = 0.46763095259666443
iteration 0, loss = 0.48049846291542053
iteration 1, loss = 0.4785245656967163
iteration 2, loss = 0.4863775372505188
iteration 3, loss = 0.47437047958374023
iteration 4, loss = 0.4832446575164795
iteration 5, loss = 0.4586312174797058
iteration 6, loss = 0.4706386923789978
iteration 7, loss = 0.4797612726688385
iteration 8, loss = 0.47317618131637573
iteration 9, loss = 0.4766600728034973
iteration 10, loss = 0.46742838621139526
iteration 11, loss = 0.4826502799987793
iteration 12, loss = 0.47738897800445557
iteration 13, loss = 0.480242520570755
iteration 14, loss = 0.4820294976234436
iteration 15, loss = 0.47284701466560364
iteration 16, loss = 0.4775237739086151
iteration 17, loss = 0.47124558687210083
iteration 18, loss = 0.47265052795410156
iteration 19, loss = 0.4700550436973572
iteration 20, loss = 0.4767577052116394
iteration 21, loss = 0.46889013051986694
iteration 22, loss = 0.4757038652896881
iteration 23, loss = 0.48612546920776367
iteration 24, loss = 0.478988915681839
iteration 25, loss = 0.47703200578689575
iteration 26, loss = 0.47029656171798706
iteration 27, loss = 0.48222166299819946
iteration 28, loss = 0.47339409589767456
iteration 29, loss = 0.4668836295604706
iteration 30, loss = 0.46629637479782104
iteration 31, loss = 0.4748830199241638
iteration 32, loss = 0.4617786407470703
iteration 33, loss = 0.47227227687835693
iteration 34, loss = 0.4786146283149719
iteration 35, loss = 0.4815465211868286
iteration 36, loss = 0.4748965799808502
iteration 37, loss = 0.47198206186294556
iteration 38, loss = 0.47586458921432495
iteration 39, loss = 0.4634539484977722
iteration 40, loss = 0.4676308333873749
iteration 41, loss = 0.4718691408634186
iteration 42, loss = 0.46715182065963745
iteration 43, loss = 0.46985387802124023
iteration 44, loss = 0.47688499093055725
iteration 45, loss = 0.47277921438217163
iteration 46, loss = 0.47493913769721985
iteration 47, loss = 0.46569710969924927
iteration 48, loss = 0.4742690324783325
iteration 49, loss = 0.46635210514068604
iteration 50, loss = 0.4731631875038147
iteration 51, loss = 0.4767078757286072
iteration 52, loss = 0.47904425859451294
iteration 53, loss = 0.47922733426094055
iteration 54, loss = 0.4746997356414795
iteration 55, loss = 0.4687146842479706
iteration 56, loss = 0.48043733835220337
iteration 57, loss = 0.4686662554740906
iteration 58, loss = 0.48040327429771423
iteration 59, loss = 0.46699801087379456
iteration 60, loss = 0.47619563341140747
iteration 61, loss = 0.4803959131240845
iteration 62, loss = 0.47430315613746643
iteration 63, loss = 0.47023284435272217
iteration 64, loss = 0.4763776659965515
iteration 65, loss = 0.4670757055282593
iteration 66, loss = 0.4712706208229065
iteration 67, loss = 0.4700624942779541
iteration 68, loss = 0.4674997329711914
iteration 69, loss = 0.4640933871269226
iteration 70, loss = 0.4686926007270813
iteration 71, loss = 0.4674447774887085
iteration 72, loss = 0.4717826843261719
iteration 73, loss = 0.46553516387939453
iteration 74, loss = 0.4612647593021393
iteration 75, loss = 0.46156686544418335
iteration 76, loss = 0.46695905923843384
iteration 77, loss = 0.4675063490867615
iteration 78, loss = 0.47373902797698975
iteration 79, loss = 0.468727707862854
iteration 80, loss = 0.4602251648902893
iteration 81, loss = 0.47663724422454834
iteration 82, loss = 0.47723737359046936
iteration 83, loss = 0.4658251404762268
iteration 84, loss = 0.46521392464637756
iteration 85, loss = 0.4753139019012451
iteration 86, loss = 0.46781015396118164
iteration 87, loss = 0.4728858172893524
iteration 88, loss = 0.4706552028656006
iteration 89, loss = 0.4725457727909088
iteration 90, loss = 0.46887868642807007
iteration 91, loss = 0.4701966345310211
iteration 92, loss = 0.4647984504699707
iteration 93, loss = 0.4682011604309082
iteration 94, loss = 0.4560195803642273
iteration 95, loss = 0.47402700781822205
iteration 96, loss = 0.4670225977897644
iteration 97, loss = 0.4712359309196472
iteration 98, loss = 0.47088757157325745
iteration 99, loss = 0.4736900329589844
iteration 100, loss = 0.46888795495033264
iteration 101, loss = 0.4611198902130127
iteration 102, loss = 0.4654812216758728
iteration 103, loss = 0.4743955433368683
iteration 104, loss = 0.4724595844745636
iteration 105, loss = 0.4688616693019867
iteration 106, loss = 0.46622127294540405
iteration 107, loss = 0.46360790729522705
iteration 108, loss = 0.4673275351524353
iteration 109, loss = 0.4587530493736267
iteration 110, loss = 0.4626527428627014
iteration 111, loss = 0.4705328941345215
iteration 112, loss = 0.47464942932128906
iteration 113, loss = 0.48609769344329834
iteration 114, loss = 0.4696914553642273
iteration 115, loss = 0.46333110332489014
iteration 116, loss = 0.4619876742362976
iteration 117, loss = 0.4673989713191986
iteration 118, loss = 0.4714323878288269
iteration 119, loss = 0.4597706198692322
iteration 120, loss = 0.46393656730651855
iteration 121, loss = 0.4603293836116791
iteration 122, loss = 0.4755774736404419
iteration 123, loss = 0.46895545721054077
iteration 124, loss = 0.4700530767440796
iteration 125, loss = 0.4647828936576843
iteration 126, loss = 0.46931660175323486
iteration 127, loss = 0.4566100835800171
iteration 128, loss = 0.4598786234855652
iteration 129, loss = 0.47818583250045776
iteration 130, loss = 0.47063779830932617
iteration 131, loss = 0.4572190046310425
iteration 132, loss = 0.4651448726654053
iteration 133, loss = 0.46765363216400146
iteration 134, loss = 0.466988742351532
iteration 135, loss = 0.4674932360649109
iteration 136, loss = 0.4652858078479767
iteration 137, loss = 0.4638683795928955
iteration 138, loss = 0.46201878786087036
iteration 139, loss = 0.47386306524276733
iteration 140, loss = 0.45898139476776123
iteration 141, loss = 0.4616149961948395
iteration 142, loss = 0.46877413988113403
iteration 143, loss = 0.47124868631362915
iteration 144, loss = 0.47789376974105835
iteration 145, loss = 0.4601619839668274
iteration 146, loss = 0.4603857398033142
iteration 147, loss = 0.45675724744796753
iteration 148, loss = 0.47606873512268066
iteration 149, loss = 0.4565463662147522
iteration 150, loss = 0.46696075797080994
iteration 151, loss = 0.4593557119369507
iteration 152, loss = 0.44930410385131836
iteration 153, loss = 0.4670258164405823
iteration 154, loss = 0.4646162986755371
iteration 155, loss = 0.45809289813041687
iteration 156, loss = 0.46728622913360596
iteration 157, loss = 0.4725309908390045
iteration 158, loss = 0.4611842930316925
iteration 159, loss = 0.4591028690338135
iteration 160, loss = 0.469541072845459
iteration 161, loss = 0.46917152404785156
iteration 162, loss = 0.4635689854621887
iteration 163, loss = 0.4631401300430298
iteration 164, loss = 0.4701803922653198
iteration 165, loss = 0.4585843086242676
iteration 166, loss = 0.4606782793998718
iteration 167, loss = 0.46759259700775146
iteration 168, loss = 0.45340242981910706
iteration 169, loss = 0.4720454216003418
iteration 170, loss = 0.4665525555610657
iteration 171, loss = 0.47716712951660156
iteration 172, loss = 0.4599343538284302
iteration 173, loss = 0.46156784892082214
iteration 174, loss = 0.4620386064052582
iteration 175, loss = 0.4552428722381592
iteration 176, loss = 0.4608023762702942
iteration 177, loss = 0.4673842191696167
iteration 178, loss = 0.45994359254837036
iteration 179, loss = 0.45678436756134033
iteration 180, loss = 0.4727117121219635
iteration 181, loss = 0.4627450108528137
iteration 182, loss = 0.46047869324684143
iteration 183, loss = 0.4733065366744995
iteration 184, loss = 0.4726174473762512
iteration 185, loss = 0.4627750515937805
iteration 186, loss = 0.4589768648147583
iteration 187, loss = 0.4546167254447937
iteration 188, loss = 0.4603358507156372
iteration 189, loss = 0.4680001735687256
iteration 190, loss = 0.4618602693080902
iteration 191, loss = 0.44941651821136475
iteration 192, loss = 0.458387166261673
iteration 193, loss = 0.4621592164039612
iteration 194, loss = 0.46071088314056396
iteration 195, loss = 0.45763951539993286
iteration 196, loss = 0.45424553751945496
iteration 197, loss = 0.46192920207977295
iteration 198, loss = 0.465233713388443
iteration 199, loss = 0.4597262144088745
iteration 200, loss = 0.46823686361312866
iteration 201, loss = 0.4616219401359558
iteration 202, loss = 0.4574824571609497
iteration 203, loss = 0.45993560552597046
iteration 204, loss = 0.46293699741363525
iteration 205, loss = 0.4607374668121338
iteration 206, loss = 0.45850563049316406
iteration 207, loss = 0.46166205406188965
iteration 208, loss = 0.4646048843860626
iteration 209, loss = 0.4588972330093384
iteration 210, loss = 0.4512113928794861
iteration 211, loss = 0.4624338746070862
iteration 212, loss = 0.4622930884361267
iteration 213, loss = 0.4508161246776581
iteration 214, loss = 0.44741901755332947
iteration 215, loss = 0.4606345295906067
iteration 216, loss = 0.441100150346756
iteration 217, loss = 0.47168684005737305
iteration 218, loss = 0.4563482999801636
iteration 219, loss = 0.4537559151649475
iteration 220, loss = 0.45210015773773193
iteration 221, loss = 0.46063899993896484
iteration 222, loss = 0.45465701818466187
iteration 223, loss = 0.45682209730148315
iteration 224, loss = 0.47487032413482666
iteration 225, loss = 0.4530491530895233
iteration 226, loss = 0.4562825560569763
iteration 227, loss = 0.44853636622428894
iteration 228, loss = 0.457706093788147
iteration 229, loss = 0.45152217149734497
iteration 230, loss = 0.4581562876701355
iteration 231, loss = 0.4630244970321655
iteration 232, loss = 0.45213139057159424
iteration 233, loss = 0.46246516704559326
iteration 234, loss = 0.4527379274368286
iteration 235, loss = 0.4535502791404724
iteration 236, loss = 0.4641057252883911
iteration 237, loss = 0.4568694233894348
iteration 238, loss = 0.4438996911048889
iteration 239, loss = 0.46492400765419006
iteration 240, loss = 0.4675549566745758
iteration 241, loss = 0.4605199694633484
iteration 242, loss = 0.45947837829589844
iteration 243, loss = 0.4549604654312134
iteration 244, loss = 0.4615001678466797
iteration 245, loss = 0.4625561237335205
iteration 246, loss = 0.45084601640701294
iteration 247, loss = 0.461314857006073
iteration 248, loss = 0.45309382677078247
iteration 249, loss = 0.4513743221759796
iteration 250, loss = 0.4576323628425598
iteration 251, loss = 0.44424888491630554
iteration 252, loss = 0.46421509981155396
iteration 253, loss = 0.46340763568878174
iteration 254, loss = 0.4478214383125305
iteration 255, loss = 0.4551130533218384
iteration 256, loss = 0.44003891944885254
iteration 257, loss = 0.4395657479763031
iteration 258, loss = 0.4594689905643463
iteration 259, loss = 0.4720837473869324
iteration 260, loss = 0.4561617374420166
iteration 261, loss = 0.44864147901535034
iteration 262, loss = 0.45807069540023804
iteration 263, loss = 0.45716166496276855
iteration 264, loss = 0.45988866686820984
iteration 265, loss = 0.4394600987434387
iteration 266, loss = 0.4681167006492615
iteration 267, loss = 0.459458589553833
iteration 268, loss = 0.4566270709037781
iteration 269, loss = 0.4572555422782898
iteration 270, loss = 0.4524269104003906
iteration 271, loss = 0.4489097595214844
iteration 272, loss = 0.44847849011421204
iteration 273, loss = 0.4464857280254364
iteration 274, loss = 0.4458807706832886
iteration 275, loss = 0.46874359250068665
iteration 276, loss = 0.4622327983379364
iteration 277, loss = 0.45315733551979065
iteration 278, loss = 0.4427257180213928
iteration 279, loss = 0.45113563537597656
iteration 280, loss = 0.44531601667404175
iteration 281, loss = 0.4488031566143036
iteration 282, loss = 0.45526373386383057
iteration 283, loss = 0.45072388648986816
iteration 284, loss = 0.4645894169807434
iteration 285, loss = 0.4639539122581482
iteration 286, loss = 0.4519803524017334
iteration 287, loss = 0.4383965730667114
iteration 288, loss = 0.46297067403793335
iteration 289, loss = 0.4556617736816406
iteration 290, loss = 0.45396101474761963
iteration 291, loss = 0.44960176944732666
iteration 292, loss = 0.44692718982696533
iteration 293, loss = 0.4425087869167328
iteration 294, loss = 0.44943124055862427
iteration 295, loss = 0.45031630992889404
iteration 296, loss = 0.44198811054229736
iteration 297, loss = 0.4597378373146057
iteration 298, loss = 0.4464656710624695
iteration 299, loss = 0.4503917098045349
iteration 0, loss = 0.45117926597595215
iteration 1, loss = 0.4543861448764801
iteration 2, loss = 0.44438016414642334
iteration 3, loss = 0.45785465836524963
iteration 4, loss = 0.45655232667922974
iteration 5, loss = 0.4551595151424408
iteration 6, loss = 0.45709627866744995
iteration 7, loss = 0.4514007866382599
iteration 8, loss = 0.44886019825935364
iteration 9, loss = 0.4478611350059509
iteration 10, loss = 0.44374585151672363
iteration 11, loss = 0.4507019519805908
iteration 12, loss = 0.4465840458869934
iteration 13, loss = 0.4514964818954468
iteration 14, loss = 0.4470933675765991
iteration 15, loss = 0.4533345699310303
iteration 16, loss = 0.4662187099456787
iteration 17, loss = 0.45519912242889404
iteration 18, loss = 0.4614122211933136
iteration 19, loss = 0.4517494738101959
iteration 20, loss = 0.4541100263595581
iteration 21, loss = 0.4522493779659271
iteration 22, loss = 0.4428665339946747
iteration 23, loss = 0.45471328496932983
iteration 24, loss = 0.4484197795391083
iteration 25, loss = 0.4579557776451111
iteration 26, loss = 0.4448966383934021
iteration 27, loss = 0.4603193402290344
iteration 28, loss = 0.4432417154312134
iteration 29, loss = 0.46197497844696045
iteration 30, loss = 0.4442688226699829
iteration 31, loss = 0.45521098375320435
iteration 32, loss = 0.44067782163619995
iteration 33, loss = 0.44770896434783936
iteration 34, loss = 0.44283127784729004
iteration 35, loss = 0.445539653301239
iteration 36, loss = 0.4499604105949402
iteration 37, loss = 0.44985121488571167
iteration 38, loss = 0.46107667684555054
iteration 39, loss = 0.4531017541885376
iteration 40, loss = 0.44890040159225464
iteration 41, loss = 0.44905948638916016
iteration 42, loss = 0.45334023237228394
iteration 43, loss = 0.42806676030158997
iteration 44, loss = 0.4370441138744354
iteration 45, loss = 0.44295334815979004
iteration 46, loss = 0.4344181418418884
iteration 47, loss = 0.4480358958244324
iteration 48, loss = 0.4603710174560547
iteration 49, loss = 0.44202566146850586
iteration 50, loss = 0.44339537620544434
iteration 51, loss = 0.4429923892021179
iteration 52, loss = 0.45902055501937866
iteration 53, loss = 0.4423006772994995
iteration 54, loss = 0.4532703757286072
iteration 55, loss = 0.44763022661209106
iteration 56, loss = 0.4479658603668213
iteration 57, loss = 0.45501184463500977
iteration 58, loss = 0.4567951560020447
iteration 59, loss = 0.45820772647857666
iteration 60, loss = 0.44335171580314636
iteration 61, loss = 0.44248485565185547
iteration 62, loss = 0.45811742544174194
iteration 63, loss = 0.4493039846420288
iteration 64, loss = 0.46050509810447693
iteration 65, loss = 0.453682005405426
iteration 66, loss = 0.4434344172477722
iteration 67, loss = 0.4542505145072937
iteration 68, loss = 0.436029314994812
iteration 69, loss = 0.4528884291648865
iteration 70, loss = 0.4442404508590698
iteration 71, loss = 0.45035320520401
iteration 72, loss = 0.4462214708328247
iteration 73, loss = 0.43125513195991516
iteration 74, loss = 0.4447905421257019
iteration 75, loss = 0.4504534602165222
iteration 76, loss = 0.4539957046508789
iteration 77, loss = 0.4486940801143646
iteration 78, loss = 0.45801836252212524
iteration 79, loss = 0.45352399349212646
iteration 80, loss = 0.4629744291305542
iteration 81, loss = 0.43749868869781494
iteration 82, loss = 0.4339490830898285
iteration 83, loss = 0.4438425600528717
iteration 84, loss = 0.46127790212631226
iteration 85, loss = 0.4468972086906433
iteration 86, loss = 0.44705474376678467
iteration 87, loss = 0.4456990957260132
iteration 88, loss = 0.45627450942993164
iteration 89, loss = 0.43754011392593384
iteration 90, loss = 0.4607340693473816
iteration 91, loss = 0.44409751892089844
iteration 92, loss = 0.4347918927669525
iteration 93, loss = 0.4536263942718506
iteration 94, loss = 0.4535287022590637
iteration 95, loss = 0.44277217984199524
iteration 96, loss = 0.43140995502471924
iteration 97, loss = 0.43847811222076416
iteration 98, loss = 0.4432602524757385
iteration 99, loss = 0.4390735626220703
iteration 100, loss = 0.44565173983573914
iteration 101, loss = 0.4471556842327118
iteration 102, loss = 0.4428721070289612
iteration 103, loss = 0.45129117369651794
iteration 104, loss = 0.44319212436676025
iteration 105, loss = 0.4343366026878357
iteration 106, loss = 0.44885680079460144
iteration 107, loss = 0.4522782564163208
iteration 108, loss = 0.44232189655303955
iteration 109, loss = 0.439395010471344
iteration 110, loss = 0.45702189207077026
iteration 111, loss = 0.43418002128601074
iteration 112, loss = 0.43921616673469543
iteration 113, loss = 0.46151435375213623
iteration 114, loss = 0.4520455598831177
iteration 115, loss = 0.44844961166381836
iteration 116, loss = 0.42881572246551514
iteration 117, loss = 0.4263830780982971
iteration 118, loss = 0.4365968406200409
iteration 119, loss = 0.44114577770233154
iteration 120, loss = 0.4332353472709656
iteration 121, loss = 0.4260428249835968
iteration 122, loss = 0.45293551683425903
iteration 123, loss = 0.4400385916233063
iteration 124, loss = 0.43959471583366394
iteration 125, loss = 0.4375396966934204
iteration 126, loss = 0.4507468342781067
iteration 127, loss = 0.4391002058982849
iteration 128, loss = 0.4445241689682007
iteration 129, loss = 0.4396074712276459
iteration 130, loss = 0.4491249620914459
iteration 131, loss = 0.44757765531539917
iteration 132, loss = 0.450567364692688
iteration 133, loss = 0.432834655046463
iteration 134, loss = 0.4337107539176941
iteration 135, loss = 0.4494777023792267
iteration 136, loss = 0.4324958920478821
iteration 137, loss = 0.4370840787887573
iteration 138, loss = 0.429540753364563
iteration 139, loss = 0.4431874454021454
iteration 140, loss = 0.4592573642730713
iteration 141, loss = 0.4362064599990845
iteration 142, loss = 0.4543183743953705
iteration 143, loss = 0.45228707790374756
iteration 144, loss = 0.4454936683177948
iteration 145, loss = 0.453335165977478
iteration 146, loss = 0.43431103229522705
iteration 147, loss = 0.44278213381767273
iteration 148, loss = 0.43712952733039856
iteration 149, loss = 0.4435901641845703
iteration 150, loss = 0.43563005328178406
iteration 151, loss = 0.441597580909729
iteration 152, loss = 0.43014776706695557
iteration 153, loss = 0.46052753925323486
iteration 154, loss = 0.4377577304840088
iteration 155, loss = 0.43866968154907227
iteration 156, loss = 0.42971527576446533
iteration 157, loss = 0.45111650228500366
iteration 158, loss = 0.43660590052604675
iteration 159, loss = 0.44135791063308716
iteration 160, loss = 0.4506911039352417
iteration 161, loss = 0.429996132850647
iteration 162, loss = 0.44066122174263
iteration 163, loss = 0.43533051013946533
iteration 164, loss = 0.42075300216674805
iteration 165, loss = 0.4409535825252533
iteration 166, loss = 0.4329225420951843
iteration 167, loss = 0.43937087059020996
iteration 168, loss = 0.4340921938419342
iteration 169, loss = 0.4308156967163086
iteration 170, loss = 0.4531030058860779
iteration 171, loss = 0.4369843602180481
iteration 172, loss = 0.43220338225364685
iteration 173, loss = 0.43054813146591187
iteration 174, loss = 0.44178372621536255
iteration 175, loss = 0.42850667238235474
iteration 176, loss = 0.4368082284927368
iteration 177, loss = 0.442142128944397
iteration 178, loss = 0.42533767223358154
iteration 179, loss = 0.44354116916656494
iteration 180, loss = 0.43079429864883423
iteration 181, loss = 0.42713403701782227
iteration 182, loss = 0.43741264939308167
iteration 183, loss = 0.4268844425678253
iteration 184, loss = 0.4327336549758911
iteration 185, loss = 0.44516870379447937
iteration 186, loss = 0.44093427062034607
iteration 187, loss = 0.43601590394973755
iteration 188, loss = 0.44779443740844727
iteration 189, loss = 0.42876577377319336
iteration 190, loss = 0.456913560628891
iteration 191, loss = 0.4487327039241791
iteration 192, loss = 0.4309479594230652
iteration 193, loss = 0.43610721826553345
iteration 194, loss = 0.42111146450042725
iteration 195, loss = 0.44352009892463684
iteration 196, loss = 0.42890465259552
iteration 197, loss = 0.43240657448768616
iteration 198, loss = 0.43533533811569214
iteration 199, loss = 0.43467435240745544
iteration 200, loss = 0.44231194257736206
iteration 201, loss = 0.42893457412719727
iteration 202, loss = 0.43480658531188965
iteration 203, loss = 0.4321565330028534
iteration 204, loss = 0.432666540145874
iteration 205, loss = 0.41591876745224
iteration 206, loss = 0.43744295835494995
iteration 207, loss = 0.42979562282562256
iteration 208, loss = 0.42539486289024353
iteration 209, loss = 0.429968923330307
iteration 210, loss = 0.44460660219192505
iteration 211, loss = 0.4159613251686096
iteration 212, loss = 0.4342425465583801
iteration 213, loss = 0.4454924762248993
iteration 214, loss = 0.4371834397315979
iteration 215, loss = 0.4338037371635437
iteration 216, loss = 0.42712903022766113
iteration 217, loss = 0.4306847155094147
iteration 218, loss = 0.43199586868286133
iteration 219, loss = 0.43309521675109863
iteration 220, loss = 0.4305775463581085
iteration 221, loss = 0.4386137127876282
iteration 222, loss = 0.4327569305896759
iteration 223, loss = 0.4480983018875122
iteration 224, loss = 0.434833824634552
iteration 225, loss = 0.42730581760406494
iteration 226, loss = 0.44334566593170166
iteration 227, loss = 0.43275415897369385
iteration 228, loss = 0.4286416471004486
iteration 229, loss = 0.4195507764816284
iteration 230, loss = 0.43275851011276245
iteration 231, loss = 0.43207716941833496
iteration 232, loss = 0.44791093468666077
iteration 233, loss = 0.41747570037841797
iteration 234, loss = 0.4264254570007324
iteration 235, loss = 0.4153960943222046
iteration 236, loss = 0.4446886479854584
iteration 237, loss = 0.4479408264160156
iteration 238, loss = 0.431534081697464
iteration 239, loss = 0.4216703176498413
iteration 240, loss = 0.43003207445144653
iteration 241, loss = 0.42161881923675537
iteration 242, loss = 0.42584028840065
iteration 243, loss = 0.44260573387145996
iteration 244, loss = 0.4515112042427063
iteration 245, loss = 0.4196440577507019
iteration 246, loss = 0.437378466129303
iteration 247, loss = 0.41868722438812256
iteration 248, loss = 0.43594813346862793
iteration 249, loss = 0.44212162494659424
iteration 250, loss = 0.43427208065986633
iteration 251, loss = 0.41848233342170715
iteration 252, loss = 0.4332940876483917
iteration 253, loss = 0.4317570924758911
iteration 254, loss = 0.44993871450424194
iteration 255, loss = 0.42132580280303955
iteration 256, loss = 0.4345039129257202
iteration 257, loss = 0.4285544753074646
iteration 258, loss = 0.41265469789505005
iteration 259, loss = 0.4372852146625519
iteration 260, loss = 0.4207885265350342
iteration 261, loss = 0.43339186906814575
iteration 262, loss = 0.42791420221328735
iteration 263, loss = 0.43180468678474426
iteration 264, loss = 0.4305216670036316
iteration 265, loss = 0.4216318726539612
iteration 266, loss = 0.41694796085357666
iteration 267, loss = 0.42343300580978394
iteration 268, loss = 0.42115455865859985
iteration 269, loss = 0.43063247203826904
iteration 270, loss = 0.43261897563934326
iteration 271, loss = 0.43949541449546814
iteration 272, loss = 0.4227871298789978
iteration 273, loss = 0.4196091890335083
iteration 274, loss = 0.4219738245010376
iteration 275, loss = 0.43430498242378235
iteration 276, loss = 0.43246471881866455
iteration 277, loss = 0.42911621928215027
iteration 278, loss = 0.43703901767730713
iteration 279, loss = 0.4579593539237976
iteration 280, loss = 0.45648694038391113
iteration 281, loss = 0.4260498583316803
iteration 282, loss = 0.4221702218055725
iteration 283, loss = 0.4311293959617615
iteration 284, loss = 0.4389633238315582
iteration 285, loss = 0.43282389640808105
iteration 286, loss = 0.4376412630081177
iteration 287, loss = 0.4245901107788086
iteration 288, loss = 0.4263882339000702
iteration 289, loss = 0.4202424883842468
iteration 290, loss = 0.43433836102485657
iteration 291, loss = 0.411307692527771
iteration 292, loss = 0.4299784004688263
iteration 293, loss = 0.41832971572875977
iteration 294, loss = 0.4227195382118225
iteration 295, loss = 0.42032474279403687
iteration 296, loss = 0.44324350357055664
iteration 297, loss = 0.4236350655555725
iteration 298, loss = 0.44347572326660156
iteration 299, loss = 0.4234175384044647
iteration 0, loss = 0.4231703281402588
iteration 1, loss = 0.4228018820285797
iteration 2, loss = 0.45791956782341003
iteration 3, loss = 0.4055131673812866
iteration 4, loss = 0.4232971668243408
iteration 5, loss = 0.41639018058776855
iteration 6, loss = 0.4054334759712219
iteration 7, loss = 0.42090538144111633
iteration 8, loss = 0.4149104952812195
iteration 9, loss = 0.455626517534256
iteration 10, loss = 0.40948402881622314
iteration 11, loss = 0.418931245803833
iteration 12, loss = 0.4152412414550781
iteration 13, loss = 0.414342999458313
iteration 14, loss = 0.4283524751663208
iteration 15, loss = 0.43857383728027344
iteration 16, loss = 0.4264165461063385
iteration 17, loss = 0.42538660764694214
iteration 18, loss = 0.42139220237731934
iteration 19, loss = 0.4188971221446991
iteration 20, loss = 0.43583911657333374
iteration 21, loss = 0.4240264892578125
iteration 22, loss = 0.4393671154975891
iteration 23, loss = 0.40989965200424194
iteration 24, loss = 0.4181651473045349
iteration 25, loss = 0.4182468354701996
iteration 26, loss = 0.4340209364891052
iteration 27, loss = 0.4181637167930603
iteration 28, loss = 0.4302830398082733
iteration 29, loss = 0.4436294436454773
iteration 30, loss = 0.42326390743255615
iteration 31, loss = 0.4213981628417969
iteration 32, loss = 0.4460618495941162
iteration 33, loss = 0.4289039075374603
iteration 34, loss = 0.4119853973388672
iteration 35, loss = 0.41326165199279785
iteration 36, loss = 0.41191503405570984
iteration 37, loss = 0.4150257706642151
iteration 38, loss = 0.40720629692077637
iteration 39, loss = 0.41110122203826904
iteration 40, loss = 0.4212929606437683
iteration 41, loss = 0.43355464935302734
iteration 42, loss = 0.4347432255744934
iteration 43, loss = 0.4218229055404663
iteration 44, loss = 0.4329717755317688
iteration 45, loss = 0.4360913634300232
iteration 46, loss = 0.42397549748420715
iteration 47, loss = 0.41513997316360474
iteration 48, loss = 0.40489065647125244
iteration 49, loss = 0.4230618476867676
iteration 50, loss = 0.4197653532028198
iteration 51, loss = 0.4272325038909912
iteration 52, loss = 0.41514354944229126
iteration 53, loss = 0.40590643882751465
iteration 54, loss = 0.4237884283065796
iteration 55, loss = 0.40582823753356934
iteration 56, loss = 0.41186052560806274
iteration 57, loss = 0.4237179160118103
iteration 58, loss = 0.43552154302597046
iteration 59, loss = 0.4360434412956238
iteration 60, loss = 0.43463289737701416
iteration 61, loss = 0.42122411727905273
iteration 62, loss = 0.4220738112926483
iteration 63, loss = 0.41520172357559204
iteration 64, loss = 0.41581857204437256
iteration 65, loss = 0.4103601574897766
iteration 66, loss = 0.4269143342971802
iteration 67, loss = 0.4171706438064575
iteration 68, loss = 0.41986846923828125
iteration 69, loss = 0.4155019223690033
iteration 70, loss = 0.41796013712882996
iteration 71, loss = 0.4166630506515503
iteration 72, loss = 0.4224396049976349
iteration 73, loss = 0.4011058211326599
iteration 74, loss = 0.426408976316452
iteration 75, loss = 0.40363919734954834
iteration 76, loss = 0.40336844325065613
iteration 77, loss = 0.4218297600746155
iteration 78, loss = 0.4196019172668457
iteration 79, loss = 0.4074723720550537
iteration 80, loss = 0.4052591919898987
iteration 81, loss = 0.4183133542537689
iteration 82, loss = 0.4040048122406006
iteration 83, loss = 0.4316929578781128
iteration 84, loss = 0.40509510040283203
iteration 85, loss = 0.4171527624130249
iteration 86, loss = 0.40752726793289185
iteration 87, loss = 0.4024317264556885
iteration 88, loss = 0.4243500232696533
iteration 89, loss = 0.42837628722190857
iteration 90, loss = 0.4169178605079651
iteration 91, loss = 0.4060625433921814
iteration 92, loss = 0.4231935739517212
iteration 93, loss = 0.4220500588417053
iteration 94, loss = 0.4366694390773773
iteration 95, loss = 0.4139266014099121
iteration 96, loss = 0.4111442565917969
iteration 97, loss = 0.4032892882823944
iteration 98, loss = 0.41936665773391724
iteration 99, loss = 0.4112318158149719
iteration 100, loss = 0.4223673343658447
iteration 101, loss = 0.4192606210708618
iteration 102, loss = 0.42274361848831177
iteration 103, loss = 0.420676589012146
iteration 104, loss = 0.40604162216186523
iteration 105, loss = 0.40892499685287476
iteration 106, loss = 0.41577857732772827
iteration 107, loss = 0.4029110074043274
iteration 108, loss = 0.41974711418151855
iteration 109, loss = 0.4020686745643616
iteration 110, loss = 0.4261482059955597
iteration 111, loss = 0.4323574900627136
iteration 112, loss = 0.4011659622192383
iteration 113, loss = 0.4172612130641937
iteration 114, loss = 0.41835272312164307
iteration 115, loss = 0.4121640920639038
iteration 116, loss = 0.42527633905410767
iteration 117, loss = 0.4360259771347046
iteration 118, loss = 0.419990599155426
iteration 119, loss = 0.40852174162864685
iteration 120, loss = 0.42043936252593994
iteration 121, loss = 0.44552159309387207
iteration 122, loss = 0.41555631160736084
iteration 123, loss = 0.43381768465042114
iteration 124, loss = 0.4143705368041992
iteration 125, loss = 0.4200282096862793
iteration 126, loss = 0.39252397418022156
iteration 127, loss = 0.4017997086048126
iteration 128, loss = 0.40314769744873047
iteration 129, loss = 0.3922986388206482
iteration 130, loss = 0.4139730930328369
iteration 131, loss = 0.40569615364074707
iteration 132, loss = 0.4093143939971924
iteration 133, loss = 0.41321051120758057
iteration 134, loss = 0.4131612777709961
iteration 135, loss = 0.4112088084220886
iteration 136, loss = 0.42372748255729675
iteration 137, loss = 0.4156198501586914
iteration 138, loss = 0.4083142876625061
iteration 139, loss = 0.4103955030441284
iteration 140, loss = 0.39362984895706177
iteration 141, loss = 0.40167170763015747
iteration 142, loss = 0.41808435320854187
iteration 143, loss = 0.4099613428115845
iteration 144, loss = 0.40113192796707153
iteration 145, loss = 0.4242573380470276
iteration 146, loss = 0.4242148995399475
iteration 147, loss = 0.43540656566619873
iteration 148, loss = 0.41033047437667847
iteration 149, loss = 0.4109342694282532
iteration 150, loss = 0.3969489336013794
iteration 151, loss = 0.42402011156082153
iteration 152, loss = 0.40830546617507935
iteration 153, loss = 0.42915207147598267
iteration 154, loss = 0.39365553855895996
iteration 155, loss = 0.4223671853542328
iteration 156, loss = 0.4042637348175049
iteration 157, loss = 0.40535372495651245
iteration 158, loss = 0.4085569381713867
iteration 159, loss = 0.4246300458908081
iteration 160, loss = 0.3861713707447052
iteration 161, loss = 0.40712305903434753
iteration 162, loss = 0.42944538593292236
iteration 163, loss = 0.3952045440673828
iteration 164, loss = 0.4152144193649292
iteration 165, loss = 0.4074721336364746
iteration 166, loss = 0.4242951273918152
iteration 167, loss = 0.4257585406303406
iteration 168, loss = 0.41960829496383667
iteration 169, loss = 0.4399886131286621
iteration 170, loss = 0.41728219389915466
iteration 171, loss = 0.42199957370758057
iteration 172, loss = 0.3940007984638214
iteration 173, loss = 0.4080175757408142
iteration 174, loss = 0.42953285574913025
iteration 175, loss = 0.40081411600112915
iteration 176, loss = 0.41277164220809937
iteration 177, loss = 0.4013940989971161
iteration 178, loss = 0.4091668725013733
iteration 179, loss = 0.3989419937133789
iteration 180, loss = 0.4078320860862732
iteration 181, loss = 0.39590972661972046
iteration 182, loss = 0.38490957021713257
iteration 183, loss = 0.39596837759017944
iteration 184, loss = 0.3962101936340332
iteration 185, loss = 0.433144211769104
iteration 186, loss = 0.4063419699668884
iteration 187, loss = 0.4176037311553955
iteration 188, loss = 0.4307197630405426
iteration 189, loss = 0.3921159505844116
iteration 190, loss = 0.40216583013534546
iteration 191, loss = 0.4030950963497162
iteration 192, loss = 0.4002448618412018
iteration 193, loss = 0.4386717677116394
iteration 194, loss = 0.4044622480869293
iteration 195, loss = 0.4081844091415405
iteration 196, loss = 0.4158620536327362
iteration 197, loss = 0.4176279306411743
iteration 198, loss = 0.4232325553894043
iteration 199, loss = 0.3981095254421234
iteration 200, loss = 0.41033118963241577
iteration 201, loss = 0.4072381556034088
iteration 202, loss = 0.3921605348587036
iteration 203, loss = 0.4064705967903137
iteration 204, loss = 0.423187792301178
iteration 205, loss = 0.4067766070365906
iteration 206, loss = 0.40598273277282715
iteration 207, loss = 0.4115811288356781
iteration 208, loss = 0.41161981225013733
iteration 209, loss = 0.42933189868927
iteration 210, loss = 0.4048584997653961
iteration 211, loss = 0.39735889434814453
iteration 212, loss = 0.3806820809841156
iteration 213, loss = 0.41608238220214844
iteration 214, loss = 0.4043533205986023
iteration 215, loss = 0.3888852298259735
iteration 216, loss = 0.3976287841796875
iteration 217, loss = 0.42942047119140625
iteration 218, loss = 0.39725255966186523
iteration 219, loss = 0.41469427943229675
iteration 220, loss = 0.41538435220718384
iteration 221, loss = 0.4083663821220398
iteration 222, loss = 0.3970146179199219
iteration 223, loss = 0.3870653510093689
iteration 224, loss = 0.41101670265197754
iteration 225, loss = 0.40083277225494385
iteration 226, loss = 0.3967946767807007
iteration 227, loss = 0.41182541847229004
iteration 228, loss = 0.4117996096611023
iteration 229, loss = 0.4106796383857727
iteration 230, loss = 0.38999661803245544
iteration 231, loss = 0.3838185667991638
iteration 232, loss = 0.4041823446750641
iteration 233, loss = 0.3967490792274475
iteration 234, loss = 0.4144718050956726
iteration 235, loss = 0.4154864549636841
iteration 236, loss = 0.3908568024635315
iteration 237, loss = 0.39961743354797363
iteration 238, loss = 0.38027089834213257
iteration 239, loss = 0.4087201654911041
iteration 240, loss = 0.4042714238166809
iteration 241, loss = 0.42294174432754517
iteration 242, loss = 0.41212230920791626
iteration 243, loss = 0.3863676190376282
iteration 244, loss = 0.40922871232032776
iteration 245, loss = 0.4006469249725342
iteration 246, loss = 0.38895171880722046
iteration 247, loss = 0.40569210052490234
iteration 248, loss = 0.4146121144294739
iteration 249, loss = 0.3984268307685852
iteration 250, loss = 0.3842543959617615
iteration 251, loss = 0.41231799125671387
iteration 252, loss = 0.3909897804260254
iteration 253, loss = 0.39151430130004883
iteration 254, loss = 0.39379382133483887
iteration 255, loss = 0.4237312376499176
iteration 256, loss = 0.3951870799064636
iteration 257, loss = 0.3923877477645874
iteration 258, loss = 0.3966067433357239
iteration 259, loss = 0.42587459087371826
iteration 260, loss = 0.3931065797805786
iteration 261, loss = 0.4040329158306122
iteration 262, loss = 0.40687593817710876
iteration 263, loss = 0.40983110666275024
iteration 264, loss = 0.4101197421550751
iteration 265, loss = 0.3898993134498596
iteration 266, loss = 0.4075748324394226
iteration 267, loss = 0.4019797146320343
iteration 268, loss = 0.3827846050262451
iteration 269, loss = 0.4136016368865967
iteration 270, loss = 0.4105404019355774
iteration 271, loss = 0.3954887390136719
iteration 272, loss = 0.4017627239227295
iteration 273, loss = 0.38428229093551636
iteration 274, loss = 0.37503522634506226
iteration 275, loss = 0.38518625497817993
iteration 276, loss = 0.3828098773956299
iteration 277, loss = 0.41723519563674927
iteration 278, loss = 0.3986563980579376
iteration 279, loss = 0.4039192795753479
iteration 280, loss = 0.38342034816741943
iteration 281, loss = 0.41087719798088074
iteration 282, loss = 0.42523688077926636
iteration 283, loss = 0.3983520269393921
iteration 284, loss = 0.4273739457130432
iteration 285, loss = 0.38688790798187256
iteration 286, loss = 0.38022422790527344
iteration 287, loss = 0.3814276456832886
iteration 288, loss = 0.40549513697624207
iteration 289, loss = 0.39009028673171997
iteration 290, loss = 0.4046666920185089
iteration 291, loss = 0.405819296836853
iteration 292, loss = 0.3916645050048828
iteration 293, loss = 0.384673148393631
iteration 294, loss = 0.3920602798461914
iteration 295, loss = 0.38922977447509766
iteration 296, loss = 0.39175909757614136
iteration 297, loss = 0.3827314078807831
iteration 298, loss = 0.39557307958602905
iteration 299, loss = 0.40282323956489563
iteration 0, loss = 0.400587797164917
iteration 1, loss = 0.38606059551239014
iteration 2, loss = 0.3776888847351074
iteration 3, loss = 0.38448458909988403
iteration 4, loss = 0.3924257755279541
iteration 5, loss = 0.384663462638855
iteration 6, loss = 0.4008164405822754
iteration 7, loss = 0.37937718629837036
iteration 8, loss = 0.3760000467300415
iteration 9, loss = 0.39011842012405396
iteration 10, loss = 0.3896622061729431
iteration 11, loss = 0.4169512391090393
iteration 12, loss = 0.4006670117378235
iteration 13, loss = 0.4015716314315796
iteration 14, loss = 0.39736101031303406
iteration 15, loss = 0.3938260078430176
iteration 16, loss = 0.4100518226623535
iteration 17, loss = 0.39953839778900146
iteration 18, loss = 0.36730676889419556
iteration 19, loss = 0.3832727074623108
iteration 20, loss = 0.3945152759552002
iteration 21, loss = 0.3989379405975342
iteration 22, loss = 0.3831164538860321
iteration 23, loss = 0.3838001489639282
iteration 24, loss = 0.401192843914032
iteration 25, loss = 0.4327773451805115
iteration 26, loss = 0.39047226309776306
iteration 27, loss = 0.38248685002326965
iteration 28, loss = 0.39224839210510254
iteration 29, loss = 0.38502758741378784
iteration 30, loss = 0.3948380947113037
iteration 31, loss = 0.40230077505111694
iteration 32, loss = 0.40529316663742065
iteration 33, loss = 0.3806334137916565
iteration 34, loss = 0.37088871002197266
iteration 35, loss = 0.40006691217422485
iteration 36, loss = 0.4061744213104248
iteration 37, loss = 0.4035987854003906
iteration 38, loss = 0.38060370087623596
iteration 39, loss = 0.3733817934989929
iteration 40, loss = 0.37001150846481323
iteration 41, loss = 0.38137000799179077
iteration 42, loss = 0.3932757079601288
iteration 43, loss = 0.3889625668525696
iteration 44, loss = 0.37375444173812866
iteration 45, loss = 0.3816169500350952
iteration 46, loss = 0.3919053375720978
iteration 47, loss = 0.38559892773628235
iteration 48, loss = 0.3974336087703705
iteration 49, loss = 0.3884812593460083
iteration 50, loss = 0.41312021017074585
iteration 51, loss = 0.40113258361816406
iteration 52, loss = 0.40008431673049927
iteration 53, loss = 0.38601887226104736
iteration 54, loss = 0.3897477388381958
iteration 55, loss = 0.3919680416584015
iteration 56, loss = 0.3756929636001587
iteration 57, loss = 0.3766815662384033
iteration 58, loss = 0.37995994091033936
iteration 59, loss = 0.3764227032661438
iteration 60, loss = 0.3833325505256653
iteration 61, loss = 0.3733551502227783
iteration 62, loss = 0.37638530135154724
iteration 63, loss = 0.35676926374435425
iteration 64, loss = 0.3837255537509918
iteration 65, loss = 0.40803438425064087
iteration 66, loss = 0.3920785188674927
iteration 67, loss = 0.4039674997329712
iteration 68, loss = 0.3736068606376648
iteration 69, loss = 0.4054824113845825
iteration 70, loss = 0.38635507225990295
iteration 71, loss = 0.3884581923484802
iteration 72, loss = 0.38315635919570923
iteration 73, loss = 0.3804001212120056
iteration 74, loss = 0.38612908124923706
iteration 75, loss = 0.392535924911499
iteration 76, loss = 0.3783840537071228
iteration 77, loss = 0.4138893187046051
iteration 78, loss = 0.36097460985183716
iteration 79, loss = 0.38155651092529297
iteration 80, loss = 0.3714819550514221
iteration 81, loss = 0.38508468866348267
iteration 82, loss = 0.39058780670166016
iteration 83, loss = 0.39891374111175537
iteration 84, loss = 0.40384459495544434
iteration 85, loss = 0.3671378493309021
iteration 86, loss = 0.36757761240005493
iteration 87, loss = 0.3830859661102295
iteration 88, loss = 0.3678877353668213
iteration 89, loss = 0.39239341020584106
iteration 90, loss = 0.3809892535209656
iteration 91, loss = 0.3827746510505676
iteration 92, loss = 0.3730246424674988
iteration 93, loss = 0.38275134563446045
iteration 94, loss = 0.35968393087387085
iteration 95, loss = 0.37960994243621826
iteration 96, loss = 0.3784857988357544
iteration 97, loss = 0.4072733223438263
iteration 98, loss = 0.38662341237068176
iteration 99, loss = 0.4080389440059662
iteration 100, loss = 0.38727039098739624
iteration 101, loss = 0.3711445927619934
iteration 102, loss = 0.3759569525718689
iteration 103, loss = 0.3847144842147827
iteration 104, loss = 0.3967072069644928
iteration 105, loss = 0.3783542811870575
iteration 106, loss = 0.3665022850036621
iteration 107, loss = 0.37860625982284546
iteration 108, loss = 0.386618435382843
iteration 109, loss = 0.3903483748435974
iteration 110, loss = 0.37207531929016113
iteration 111, loss = 0.3879331946372986
iteration 112, loss = 0.3790934085845947
iteration 113, loss = 0.3734819293022156
iteration 114, loss = 0.3857654333114624
iteration 115, loss = 0.37821710109710693
iteration 116, loss = 0.3537723124027252
iteration 117, loss = 0.3647557497024536
iteration 118, loss = 0.37519752979278564
iteration 119, loss = 0.3737560212612152
iteration 120, loss = 0.39878326654434204
iteration 121, loss = 0.3731430768966675
iteration 122, loss = 0.38841482996940613
iteration 123, loss = 0.3844373822212219
iteration 124, loss = 0.3808688521385193
iteration 125, loss = 0.3702886402606964
iteration 126, loss = 0.3857329785823822
iteration 127, loss = 0.3800795078277588
iteration 128, loss = 0.35700565576553345
iteration 129, loss = 0.4006772041320801
iteration 130, loss = 0.37866950035095215
iteration 131, loss = 0.38455694913864136
iteration 132, loss = 0.35991793870925903
iteration 133, loss = 0.3949604630470276
iteration 134, loss = 0.36133772134780884
iteration 135, loss = 0.3841553330421448
iteration 136, loss = 0.3682846128940582
iteration 137, loss = 0.3525741696357727
iteration 138, loss = 0.37323054671287537
iteration 139, loss = 0.372664213180542
iteration 140, loss = 0.381236732006073
iteration 141, loss = 0.3830885887145996
iteration 142, loss = 0.37448644638061523
iteration 143, loss = 0.3829382061958313
iteration 144, loss = 0.3731606602668762
iteration 145, loss = 0.39292868971824646
iteration 146, loss = 0.4043065905570984
iteration 147, loss = 0.37996700406074524
iteration 148, loss = 0.37923359870910645
iteration 149, loss = 0.3755277991294861
iteration 150, loss = 0.3684001863002777
iteration 151, loss = 0.3720531463623047
iteration 152, loss = 0.3903205394744873
iteration 153, loss = 0.3427770733833313
iteration 154, loss = 0.3884947896003723
iteration 155, loss = 0.36288008093833923
iteration 156, loss = 0.3788737654685974
iteration 157, loss = 0.38029032945632935
iteration 158, loss = 0.3709578514099121
iteration 159, loss = 0.3876250982284546
iteration 160, loss = 0.3752884864807129
iteration 161, loss = 0.3480179011821747
iteration 162, loss = 0.37148675322532654
iteration 163, loss = 0.3824579119682312
iteration 164, loss = 0.37387824058532715
iteration 165, loss = 0.34856927394866943
iteration 166, loss = 0.37706458568573
iteration 167, loss = 0.358971506357193
iteration 168, loss = 0.3510785400867462
iteration 169, loss = 0.3693300187587738
iteration 170, loss = 0.3753966689109802
iteration 171, loss = 0.37526196241378784
iteration 172, loss = 0.35916268825531006
iteration 173, loss = 0.36556053161621094
iteration 174, loss = 0.3703664541244507
iteration 175, loss = 0.3583652973175049
iteration 176, loss = 0.40257924795150757
iteration 177, loss = 0.38852429389953613
iteration 178, loss = 0.3813460171222687
iteration 179, loss = 0.37114715576171875
iteration 180, loss = 0.3742636442184448
iteration 181, loss = 0.3517383933067322
iteration 182, loss = 0.38619914650917053
iteration 183, loss = 0.3862913250923157
iteration 184, loss = 0.392488956451416
iteration 185, loss = 0.36346685886383057
iteration 186, loss = 0.37414586544036865
iteration 187, loss = 0.3769378066062927
iteration 188, loss = 0.36108043789863586
iteration 189, loss = 0.35287368297576904
iteration 190, loss = 0.3776819705963135
iteration 191, loss = 0.3810528814792633
iteration 192, loss = 0.3366924822330475
iteration 193, loss = 0.37647485733032227
iteration 194, loss = 0.374342679977417
iteration 195, loss = 0.35665494203567505
iteration 196, loss = 0.3566254675388336
iteration 197, loss = 0.36106425523757935
iteration 198, loss = 0.37379977107048035
iteration 199, loss = 0.366007924079895
iteration 200, loss = 0.39652198553085327
iteration 201, loss = 0.36947956681251526
iteration 202, loss = 0.3793308138847351
iteration 203, loss = 0.37623709440231323
iteration 204, loss = 0.384368360042572
iteration 205, loss = 0.3779464662075043
iteration 206, loss = 0.3823416829109192
iteration 207, loss = 0.38636982440948486
iteration 208, loss = 0.37415456771850586
iteration 209, loss = 0.3883846402168274
iteration 210, loss = 0.3785237967967987
iteration 211, loss = 0.35815876722335815
iteration 212, loss = 0.36818835139274597
iteration 213, loss = 0.37132254242897034
iteration 214, loss = 0.37013059854507446
iteration 215, loss = 0.3764214515686035
iteration 216, loss = 0.352191686630249
iteration 217, loss = 0.4002547264099121
iteration 218, loss = 0.407236248254776
iteration 219, loss = 0.34842753410339355
iteration 220, loss = 0.3875185251235962
iteration 221, loss = 0.3791188895702362
iteration 222, loss = 0.3712943494319916
iteration 223, loss = 0.3564426302909851
iteration 224, loss = 0.37109607458114624
iteration 225, loss = 0.3651648759841919
iteration 226, loss = 0.36446118354797363
iteration 227, loss = 0.39102882146835327
iteration 228, loss = 0.3324314057826996
iteration 229, loss = 0.35936641693115234
iteration 230, loss = 0.3541814386844635
iteration 231, loss = 0.3741883337497711
iteration 232, loss = 0.3641062378883362
iteration 233, loss = 0.35893869400024414
iteration 234, loss = 0.4063871204853058
iteration 235, loss = 0.355350136756897
iteration 236, loss = 0.390606164932251
iteration 237, loss = 0.362970232963562
iteration 238, loss = 0.3957343101501465
iteration 239, loss = 0.3786243796348572
iteration 240, loss = 0.36902108788490295
iteration 241, loss = 0.3535013794898987
iteration 242, loss = 0.36784952878952026
iteration 243, loss = 0.3561405539512634
iteration 244, loss = 0.33843693137168884
iteration 245, loss = 0.35726654529571533
iteration 246, loss = 0.3691098690032959
iteration 247, loss = 0.34945133328437805
iteration 248, loss = 0.3559216856956482
iteration 249, loss = 0.3397471010684967
iteration 250, loss = 0.3504882752895355
iteration 251, loss = 0.3373469412326813
iteration 252, loss = 0.37446337938308716
iteration 253, loss = 0.38126713037490845
iteration 254, loss = 0.39357230067253113
iteration 255, loss = 0.34921273589134216
iteration 256, loss = 0.3725377023220062
iteration 257, loss = 0.3644157648086548
iteration 258, loss = 0.3654954135417938
iteration 259, loss = 0.3698331415653229
iteration 260, loss = 0.36972126364707947
iteration 261, loss = 0.3738054633140564
iteration 262, loss = 0.3783223032951355
iteration 263, loss = 0.37546777725219727
iteration 264, loss = 0.34032711386680603
iteration 265, loss = 0.3982635736465454
iteration 266, loss = 0.372425377368927
iteration 267, loss = 0.35832691192626953
iteration 268, loss = 0.37602686882019043
iteration 269, loss = 0.3413180410861969
iteration 270, loss = 0.3730103075504303
iteration 271, loss = 0.36399614810943604
iteration 272, loss = 0.34218305349349976
iteration 273, loss = 0.3845917284488678
iteration 274, loss = 0.33939433097839355
iteration 275, loss = 0.36273351311683655
iteration 276, loss = 0.3743785619735718
iteration 277, loss = 0.37648871541023254
iteration 278, loss = 0.36782217025756836
iteration 279, loss = 0.3538376986980438
iteration 280, loss = 0.35287532210350037
iteration 281, loss = 0.36043769121170044
iteration 282, loss = 0.3605864644050598
iteration 283, loss = 0.36340221762657166
iteration 284, loss = 0.35797274112701416
iteration 285, loss = 0.3881520628929138
iteration 286, loss = 0.3458820581436157
iteration 287, loss = 0.3697228729724884
iteration 288, loss = 0.352742075920105
iteration 289, loss = 0.3805515766143799
iteration 290, loss = 0.36316174268722534
iteration 291, loss = 0.38388335704803467
iteration 292, loss = 0.38276445865631104
iteration 293, loss = 0.3663182854652405
iteration 294, loss = 0.3435848355293274
iteration 295, loss = 0.3651493191719055
iteration 296, loss = 0.343670129776001
iteration 297, loss = 0.35083460807800293
iteration 298, loss = 0.36732035875320435
iteration 299, loss = 0.36260688304901123
iteration 0, loss = 0.35995304584503174
iteration 1, loss = 0.35473668575286865
iteration 2, loss = 0.3350716531276703
iteration 3, loss = 0.3445712625980377
iteration 4, loss = 0.35552164912223816
iteration 5, loss = 0.3586813807487488
iteration 6, loss = 0.32683661580085754
iteration 7, loss = 0.36273616552352905
iteration 8, loss = 0.3606260418891907
iteration 9, loss = 0.3595165014266968
iteration 10, loss = 0.34292760491371155
iteration 11, loss = 0.37552058696746826
iteration 12, loss = 0.360758513212204
iteration 13, loss = 0.358248233795166
iteration 14, loss = 0.36922937631607056
iteration 15, loss = 0.35845282673835754
iteration 16, loss = 0.3445861041545868
iteration 17, loss = 0.33508211374282837
iteration 18, loss = 0.37992656230926514
iteration 19, loss = 0.3662300109863281
iteration 20, loss = 0.3383093476295471
iteration 21, loss = 0.3918190896511078
iteration 22, loss = 0.36403873562812805
iteration 23, loss = 0.33223456144332886
iteration 24, loss = 0.3630271553993225
iteration 25, loss = 0.3636392652988434
iteration 26, loss = 0.366909384727478
iteration 27, loss = 0.35215330123901367
iteration 28, loss = 0.35331448912620544
iteration 29, loss = 0.34693601727485657
iteration 30, loss = 0.3412572741508484
iteration 31, loss = 0.3629041910171509
iteration 32, loss = 0.3420994281768799
iteration 33, loss = 0.327603816986084
iteration 34, loss = 0.36545148491859436
iteration 35, loss = 0.36083221435546875
iteration 36, loss = 0.3341015577316284
iteration 37, loss = 0.36009645462036133
iteration 38, loss = 0.3186683654785156
iteration 39, loss = 0.36158543825149536
iteration 40, loss = 0.34731605648994446
iteration 41, loss = 0.35904550552368164
iteration 42, loss = 0.3822198808193207
iteration 43, loss = 0.38029569387435913
iteration 44, loss = 0.36542344093322754
iteration 45, loss = 0.3720836043357849
iteration 46, loss = 0.3340899348258972
iteration 47, loss = 0.3459851145744324
iteration 48, loss = 0.35942143201828003
iteration 49, loss = 0.3528698682785034
iteration 50, loss = 0.32496902346611023
iteration 51, loss = 0.330822616815567
iteration 52, loss = 0.3159782886505127
iteration 53, loss = 0.36368119716644287
iteration 54, loss = 0.34688687324523926
iteration 55, loss = 0.32364514470100403
iteration 56, loss = 0.36533987522125244
iteration 57, loss = 0.3477958142757416
iteration 58, loss = 0.3367288410663605
iteration 59, loss = 0.34433937072753906
iteration 60, loss = 0.35702043771743774
iteration 61, loss = 0.3421160578727722
iteration 62, loss = 0.33739322423934937
iteration 63, loss = 0.3668321669101715
iteration 64, loss = 0.3394111394882202
iteration 65, loss = 0.33798983693122864
iteration 66, loss = 0.3458777666091919
iteration 67, loss = 0.3792990446090698
iteration 68, loss = 0.33650511503219604
iteration 69, loss = 0.34193137288093567
iteration 70, loss = 0.35912683606147766
iteration 71, loss = 0.34702426195144653
iteration 72, loss = 0.3670538365840912
iteration 73, loss = 0.359757661819458
iteration 74, loss = 0.34469887614250183
iteration 75, loss = 0.3475538492202759
iteration 76, loss = 0.35206809639930725
iteration 77, loss = 0.3564906716346741
iteration 78, loss = 0.3509935140609741
iteration 79, loss = 0.3138546049594879
iteration 80, loss = 0.3507763147354126
iteration 81, loss = 0.3464816212654114
iteration 82, loss = 0.3198843002319336
iteration 83, loss = 0.3428525924682617
iteration 84, loss = 0.3512095808982849
iteration 85, loss = 0.35382312536239624
iteration 86, loss = 0.3414456844329834
iteration 87, loss = 0.34118103981018066
iteration 88, loss = 0.3676965832710266
iteration 89, loss = 0.34402966499328613
iteration 90, loss = 0.3407805860042572
iteration 91, loss = 0.34753653407096863
iteration 92, loss = 0.3212399482727051
iteration 93, loss = 0.33161768317222595
iteration 94, loss = 0.32740819454193115
iteration 95, loss = 0.36513659358024597
iteration 96, loss = 0.30779150128364563
iteration 97, loss = 0.3290955722332001
iteration 98, loss = 0.3289133906364441
iteration 99, loss = 0.35095444321632385
iteration 100, loss = 0.36360883712768555
iteration 101, loss = 0.35163646936416626
iteration 102, loss = 0.35185012221336365
iteration 103, loss = 0.3334754407405853
iteration 104, loss = 0.3443954586982727
iteration 105, loss = 0.33677858114242554
iteration 106, loss = 0.3420355021953583
iteration 107, loss = 0.3224501311779022
iteration 108, loss = 0.3558759093284607
iteration 109, loss = 0.3509601652622223
iteration 110, loss = 0.3457404375076294
iteration 111, loss = 0.3295400142669678
iteration 112, loss = 0.3613978624343872
iteration 113, loss = 0.32280632853507996
iteration 114, loss = 0.3580373525619507
iteration 115, loss = 0.3558290898799896
iteration 116, loss = 0.3310489058494568
iteration 117, loss = 0.347324013710022
iteration 118, loss = 0.35254424810409546
iteration 119, loss = 0.3280373513698578
iteration 120, loss = 0.3288153409957886
iteration 121, loss = 0.34689581394195557
iteration 122, loss = 0.32698696851730347
iteration 123, loss = 0.34157562255859375
iteration 124, loss = 0.33745259046554565
iteration 125, loss = 0.35977160930633545
iteration 126, loss = 0.32585376501083374
iteration 127, loss = 0.33064138889312744
iteration 128, loss = 0.3792046308517456
iteration 129, loss = 0.31461358070373535
iteration 130, loss = 0.3539222180843353
iteration 131, loss = 0.31405243277549744
iteration 132, loss = 0.3287288546562195
iteration 133, loss = 0.3490915894508362
iteration 134, loss = 0.3242199718952179
iteration 135, loss = 0.3260746896266937
iteration 136, loss = 0.35505667328834534
iteration 137, loss = 0.3512570261955261
iteration 138, loss = 0.33525025844573975
iteration 139, loss = 0.34702256321907043
iteration 140, loss = 0.35534265637397766
iteration 141, loss = 0.3180955946445465
iteration 142, loss = 0.3330313563346863
iteration 143, loss = 0.3398285508155823
iteration 144, loss = 0.32441169023513794
iteration 145, loss = 0.32054227590560913
iteration 146, loss = 0.3616340756416321
iteration 147, loss = 0.35957658290863037
iteration 148, loss = 0.3159945607185364
iteration 149, loss = 0.3301198184490204
iteration 150, loss = 0.3371606767177582
iteration 151, loss = 0.3387792408466339
iteration 152, loss = 0.3270737826824188
iteration 153, loss = 0.3295283913612366
iteration 154, loss = 0.3258834481239319
iteration 155, loss = 0.3586304187774658
iteration 156, loss = 0.34718459844589233
iteration 157, loss = 0.32089322805404663
iteration 158, loss = 0.34843432903289795
iteration 159, loss = 0.33771246671676636
iteration 160, loss = 0.32939061522483826
iteration 161, loss = 0.32918184995651245
iteration 162, loss = 0.3237118721008301
iteration 163, loss = 0.3059841990470886
iteration 164, loss = 0.3633871078491211
iteration 165, loss = 0.3380856215953827
iteration 166, loss = 0.33901405334472656
iteration 167, loss = 0.35192635655403137
iteration 168, loss = 0.35004380345344543
iteration 169, loss = 0.33265233039855957
iteration 170, loss = 0.32507240772247314
iteration 171, loss = 0.33712804317474365
iteration 172, loss = 0.31473925709724426
iteration 173, loss = 0.3393568992614746
iteration 174, loss = 0.3130037486553192
iteration 175, loss = 0.3511662483215332
iteration 176, loss = 0.3203379511833191
iteration 177, loss = 0.3342300057411194
iteration 178, loss = 0.3169158697128296
iteration 179, loss = 0.34807613492012024
iteration 180, loss = 0.33901363611221313
iteration 181, loss = 0.3321256637573242
iteration 182, loss = 0.33814525604248047
iteration 183, loss = 0.32799214124679565
iteration 184, loss = 0.31262362003326416
iteration 185, loss = 0.31839871406555176
iteration 186, loss = 0.3264865577220917
iteration 187, loss = 0.3290027976036072
iteration 188, loss = 0.34305307269096375
iteration 189, loss = 0.3765345811843872
iteration 190, loss = 0.32617539167404175
iteration 191, loss = 0.3439013957977295
iteration 192, loss = 0.3116694390773773
iteration 193, loss = 0.32682767510414124
iteration 194, loss = 0.3190438151359558
iteration 195, loss = 0.33611220121383667
iteration 196, loss = 0.3222580552101135
iteration 197, loss = 0.34395602345466614
iteration 198, loss = 0.32316145300865173
iteration 199, loss = 0.3526604175567627
iteration 200, loss = 0.3099405765533447
iteration 201, loss = 0.32068634033203125
iteration 202, loss = 0.30876046419143677
iteration 203, loss = 0.32980501651763916
iteration 204, loss = 0.30712956190109253
iteration 205, loss = 0.3477754294872284
iteration 206, loss = 0.31277093291282654
iteration 207, loss = 0.31663283705711365
iteration 208, loss = 0.3510487675666809
iteration 209, loss = 0.3397931158542633
iteration 210, loss = 0.3281840682029724
iteration 211, loss = 0.3094581365585327
iteration 212, loss = 0.32242876291275024
iteration 213, loss = 0.3128353953361511
iteration 214, loss = 0.3275136947631836
iteration 215, loss = 0.36419254541397095
iteration 216, loss = 0.3353846073150635
iteration 217, loss = 0.32900023460388184
iteration 218, loss = 0.3380352556705475
iteration 219, loss = 0.32261162996292114
iteration 220, loss = 0.3565340042114258
iteration 221, loss = 0.32238906621932983
iteration 222, loss = 0.3295927941799164
iteration 223, loss = 0.29800719022750854
iteration 224, loss = 0.29852229356765747
iteration 225, loss = 0.3332318067550659
iteration 226, loss = 0.31893301010131836
iteration 227, loss = 0.34501224756240845
iteration 228, loss = 0.34768036007881165
iteration 229, loss = 0.3314932882785797
iteration 230, loss = 0.3347313702106476
iteration 231, loss = 0.3080832064151764
iteration 232, loss = 0.34097108244895935
iteration 233, loss = 0.321784645318985
iteration 234, loss = 0.3225786089897156
iteration 235, loss = 0.3158924877643585
iteration 236, loss = 0.3255723714828491
iteration 237, loss = 0.3120501637458801
iteration 238, loss = 0.3083064556121826
iteration 239, loss = 0.3110169768333435
iteration 240, loss = 0.3269657492637634
iteration 241, loss = 0.3268203139305115
iteration 242, loss = 0.29297494888305664
iteration 243, loss = 0.34603142738342285
iteration 244, loss = 0.3086482882499695
iteration 245, loss = 0.32274100184440613
iteration 246, loss = 0.3342810273170471
iteration 247, loss = 0.31894075870513916
iteration 248, loss = 0.32272976636886597
iteration 249, loss = 0.317402720451355
iteration 250, loss = 0.29401639103889465
iteration 251, loss = 0.3089498281478882
iteration 252, loss = 0.3016536831855774
iteration 253, loss = 0.29169097542762756
iteration 254, loss = 0.3153378963470459
iteration 255, loss = 0.31222933530807495
iteration 256, loss = 0.3101808726787567
iteration 257, loss = 0.3248583972454071
iteration 258, loss = 0.30695316195487976
iteration 259, loss = 0.3050522804260254
iteration 260, loss = 0.3062814474105835
iteration 261, loss = 0.32359349727630615
iteration 262, loss = 0.3213939070701599
iteration 263, loss = 0.2965897023677826
iteration 264, loss = 0.3148370385169983
iteration 265, loss = 0.3049278259277344
iteration 266, loss = 0.3304368853569031
iteration 267, loss = 0.3119451105594635
iteration 268, loss = 0.3285428583621979
iteration 269, loss = 0.29407840967178345
iteration 270, loss = 0.32169869542121887
iteration 271, loss = 0.3131478726863861
iteration 272, loss = 0.3560837209224701
iteration 273, loss = 0.31862688064575195
iteration 274, loss = 0.33050379157066345
iteration 275, loss = 0.3205600380897522
iteration 276, loss = 0.2977600693702698
iteration 277, loss = 0.34277617931365967
iteration 278, loss = 0.33037400245666504
iteration 279, loss = 0.2888435125350952
iteration 280, loss = 0.3324493169784546
iteration 281, loss = 0.3076261281967163
iteration 282, loss = 0.29387226700782776
iteration 283, loss = 0.34010979533195496
iteration 284, loss = 0.3375391364097595
iteration 285, loss = 0.31335321068763733
iteration 286, loss = 0.32225501537323
iteration 287, loss = 0.32004109025001526
iteration 288, loss = 0.28809085488319397
iteration 289, loss = 0.3372342586517334
iteration 290, loss = 0.32611745595932007
iteration 291, loss = 0.3139954209327698
iteration 292, loss = 0.33793359994888306
iteration 293, loss = 0.29501044750213623
iteration 294, loss = 0.31700897216796875
iteration 295, loss = 0.32413965463638306
iteration 296, loss = 0.2897622585296631
iteration 297, loss = 0.27703937888145447
iteration 298, loss = 0.3405503034591675
iteration 299, loss = 0.3138355016708374
iteration 0, loss = 0.30798718333244324
iteration 1, loss = 0.2801330089569092
iteration 2, loss = 0.32238301634788513
iteration 3, loss = 0.32626694440841675
iteration 4, loss = 0.32703492045402527
iteration 5, loss = 0.34276872873306274
iteration 6, loss = 0.3237916827201843
iteration 7, loss = 0.29501932859420776
iteration 8, loss = 0.3312858045101166
iteration 9, loss = 0.29424145817756653
iteration 10, loss = 0.2975754141807556
iteration 11, loss = 0.2853057384490967
iteration 12, loss = 0.30017682909965515
iteration 13, loss = 0.28557050228118896
iteration 14, loss = 0.32955434918403625
iteration 15, loss = 0.2930881977081299
iteration 16, loss = 0.30231842398643494
iteration 17, loss = 0.33145320415496826
iteration 18, loss = 0.3384518325328827
iteration 19, loss = 0.2928698658943176
iteration 20, loss = 0.29707813262939453
iteration 21, loss = 0.3369179666042328
iteration 22, loss = 0.31869375705718994
iteration 23, loss = 0.3026924431324005
iteration 24, loss = 0.30389919877052307
iteration 25, loss = 0.29445943236351013
iteration 26, loss = 0.2948184907436371
iteration 27, loss = 0.25709235668182373
iteration 28, loss = 0.3089877963066101
iteration 29, loss = 0.30333149433135986
iteration 30, loss = 0.3163539171218872
iteration 31, loss = 0.32721012830734253
iteration 32, loss = 0.3253434896469116
iteration 33, loss = 0.29302963614463806
iteration 34, loss = 0.2921462059020996
iteration 35, loss = 0.303768515586853
iteration 36, loss = 0.35591191053390503
iteration 37, loss = 0.3223070800304413
iteration 38, loss = 0.32230693101882935
iteration 39, loss = 0.31041255593299866
iteration 40, loss = 0.3157522678375244
iteration 41, loss = 0.30214494466781616
iteration 42, loss = 0.34690821170806885
iteration 43, loss = 0.31208181381225586
iteration 44, loss = 0.31753697991371155
iteration 45, loss = 0.3126884698867798
iteration 46, loss = 0.31535881757736206
iteration 47, loss = 0.32695356011390686
iteration 48, loss = 0.29617807269096375
iteration 49, loss = 0.31916648149490356
iteration 50, loss = 0.30750760436058044
iteration 51, loss = 0.27771249413490295
iteration 52, loss = 0.3092767000198364
iteration 53, loss = 0.27027571201324463
iteration 54, loss = 0.28996041417121887
iteration 55, loss = 0.2959352731704712
iteration 56, loss = 0.2923533022403717
iteration 57, loss = 0.2991597652435303
iteration 58, loss = 0.3075045645236969
iteration 59, loss = 0.2822294533252716
iteration 60, loss = 0.3088217079639435
iteration 61, loss = 0.3012166917324066
iteration 62, loss = 0.3335382342338562
iteration 63, loss = 0.3016592264175415
iteration 64, loss = 0.31545817852020264
iteration 65, loss = 0.2945742607116699
iteration 66, loss = 0.27814552187919617
iteration 67, loss = 0.3123035430908203
iteration 68, loss = 0.32823359966278076
iteration 69, loss = 0.316999226808548
iteration 70, loss = 0.27297747135162354
iteration 71, loss = 0.2921220064163208
iteration 72, loss = 0.3128625750541687
iteration 73, loss = 0.29237550497055054
iteration 74, loss = 0.30153363943099976
iteration 75, loss = 0.2919591963291168
iteration 76, loss = 0.2779242992401123
iteration 77, loss = 0.31089410185813904
iteration 78, loss = 0.3033856749534607
iteration 79, loss = 0.2798953056335449
iteration 80, loss = 0.3052706718444824
iteration 81, loss = 0.2851083278656006
iteration 82, loss = 0.30743956565856934
iteration 83, loss = 0.3022885024547577
iteration 84, loss = 0.3173258304595947
iteration 85, loss = 0.3107147812843323
iteration 86, loss = 0.3106229901313782
iteration 87, loss = 0.27944478392601013
iteration 88, loss = 0.3289974331855774
iteration 89, loss = 0.29447972774505615
iteration 90, loss = 0.31033822894096375
iteration 91, loss = 0.27061712741851807
iteration 92, loss = 0.31604447960853577
iteration 93, loss = 0.2982793152332306
iteration 94, loss = 0.30054527521133423
iteration 95, loss = 0.32922035455703735
iteration 96, loss = 0.26402920484542847
iteration 97, loss = 0.2928192615509033
iteration 98, loss = 0.3040943741798401
iteration 99, loss = 0.29778018593788147
iteration 100, loss = 0.3059272766113281
iteration 101, loss = 0.31771671772003174
iteration 102, loss = 0.282290518283844
iteration 103, loss = 0.300615131855011
iteration 104, loss = 0.2921965718269348
iteration 105, loss = 0.31835606694221497
iteration 106, loss = 0.2880755364894867
iteration 107, loss = 0.2998509109020233
iteration 108, loss = 0.27134594321250916
iteration 109, loss = 0.28874802589416504
iteration 110, loss = 0.32556065917015076
iteration 111, loss = 0.31709009408950806
iteration 112, loss = 0.29959553480148315
iteration 113, loss = 0.27678075432777405
iteration 114, loss = 0.3051849603652954
iteration 115, loss = 0.29671990871429443
iteration 116, loss = 0.2968815863132477
iteration 117, loss = 0.3168972432613373
iteration 118, loss = 0.27635228633880615
iteration 119, loss = 0.2938629388809204
iteration 120, loss = 0.33259713649749756
iteration 121, loss = 0.2655254602432251
iteration 122, loss = 0.305738627910614
iteration 123, loss = 0.26204022765159607
iteration 124, loss = 0.29724282026290894
iteration 125, loss = 0.2854812741279602
iteration 126, loss = 0.2861395478248596
iteration 127, loss = 0.2687174379825592
iteration 128, loss = 0.32740524411201477
iteration 129, loss = 0.29991233348846436
iteration 130, loss = 0.2696738839149475
iteration 131, loss = 0.2701348066329956
iteration 132, loss = 0.29640698432922363
iteration 133, loss = 0.2908748686313629
iteration 134, loss = 0.3024984896183014
iteration 135, loss = 0.31154555082321167
iteration 136, loss = 0.30106106400489807
iteration 137, loss = 0.30119505524635315
iteration 138, loss = 0.27218136191368103
iteration 139, loss = 0.2641006112098694
iteration 140, loss = 0.30633634328842163
iteration 141, loss = 0.2945738434791565
iteration 142, loss = 0.27595260739326477
iteration 143, loss = 0.29450857639312744
iteration 144, loss = 0.2895991802215576
iteration 145, loss = 0.2803298234939575
iteration 146, loss = 0.28690779209136963
iteration 147, loss = 0.2839725613594055
iteration 148, loss = 0.29974013566970825
iteration 149, loss = 0.279327392578125
iteration 150, loss = 0.3125530779361725
iteration 151, loss = 0.31850680708885193
iteration 152, loss = 0.29190999269485474
iteration 153, loss = 0.28819072246551514
iteration 154, loss = 0.3050439953804016
iteration 155, loss = 0.2702726423740387
iteration 156, loss = 0.3076888918876648
iteration 157, loss = 0.27688002586364746
iteration 158, loss = 0.30191606283187866
iteration 159, loss = 0.32292047142982483
iteration 160, loss = 0.2996126711368561
iteration 161, loss = 0.3078264594078064
iteration 162, loss = 0.2905188500881195
iteration 163, loss = 0.2659473717212677
iteration 164, loss = 0.2672100067138672
iteration 165, loss = 0.29380565881729126
iteration 166, loss = 0.2596476674079895
iteration 167, loss = 0.2963293194770813
iteration 168, loss = 0.28911665081977844
iteration 169, loss = 0.26368504762649536
iteration 170, loss = 0.3258545696735382
iteration 171, loss = 0.2810846269130707
iteration 172, loss = 0.2803354263305664
iteration 173, loss = 0.3065333962440491
iteration 174, loss = 0.29775968194007874
iteration 175, loss = 0.2825709283351898
iteration 176, loss = 0.29704567790031433
iteration 177, loss = 0.27989494800567627
iteration 178, loss = 0.2852745056152344
iteration 179, loss = 0.3070797622203827
iteration 180, loss = 0.3352869153022766
iteration 181, loss = 0.2683703601360321
iteration 182, loss = 0.3001205623149872
iteration 183, loss = 0.30019158124923706
iteration 184, loss = 0.2677720785140991
iteration 185, loss = 0.2925040125846863
iteration 186, loss = 0.28658780455589294
iteration 187, loss = 0.2700551748275757
iteration 188, loss = 0.27679815888404846
iteration 189, loss = 0.28589850664138794
iteration 190, loss = 0.25437915325164795
iteration 191, loss = 0.27336758375167847
iteration 192, loss = 0.2643689513206482
iteration 193, loss = 0.2868500053882599
iteration 194, loss = 0.3034943640232086
iteration 195, loss = 0.2985765337944031
iteration 196, loss = 0.33025485277175903
iteration 197, loss = 0.28260353207588196
iteration 198, loss = 0.2938666045665741
iteration 199, loss = 0.27115917205810547
iteration 200, loss = 0.25294041633605957
iteration 201, loss = 0.283109188079834
iteration 202, loss = 0.25808724761009216
iteration 203, loss = 0.29538610577583313
iteration 204, loss = 0.28656286001205444
iteration 205, loss = 0.28396379947662354
iteration 206, loss = 0.2501474618911743
iteration 207, loss = 0.3134061098098755
iteration 208, loss = 0.26562026143074036
iteration 209, loss = 0.2974810004234314
iteration 210, loss = 0.23850969970226288
iteration 211, loss = 0.29675930738449097
iteration 212, loss = 0.24293392896652222
iteration 213, loss = 0.2675013542175293
iteration 214, loss = 0.27825015783309937
iteration 215, loss = 0.30749067664146423
iteration 216, loss = 0.28547587990760803
iteration 217, loss = 0.27275413274765015
iteration 218, loss = 0.2538798749446869
iteration 219, loss = 0.3031887114048004
iteration 220, loss = 0.28586360812187195
iteration 221, loss = 0.2750712037086487
iteration 222, loss = 0.28802812099456787
iteration 223, loss = 0.26404160261154175
iteration 224, loss = 0.28569772839546204
iteration 225, loss = 0.2737669050693512
iteration 226, loss = 0.282848060131073
iteration 227, loss = 0.27759984135627747
iteration 228, loss = 0.27757149934768677
iteration 229, loss = 0.2722073793411255
iteration 230, loss = 0.28740569949150085
iteration 231, loss = 0.2908865809440613
iteration 232, loss = 0.2471722662448883
iteration 233, loss = 0.26418444514274597
iteration 234, loss = 0.28495919704437256
iteration 235, loss = 0.28152307868003845
iteration 236, loss = 0.28044772148132324
iteration 237, loss = 0.2423907220363617
iteration 238, loss = 0.28033190965652466
iteration 239, loss = 0.2655690908432007
iteration 240, loss = 0.2731435000896454
iteration 241, loss = 0.26248541474342346
iteration 242, loss = 0.2658460736274719
iteration 243, loss = 0.2815144956111908
iteration 244, loss = 0.2625296115875244
iteration 245, loss = 0.2654585838317871
iteration 246, loss = 0.2613620460033417
iteration 247, loss = 0.26500535011291504
iteration 248, loss = 0.23613771796226501
iteration 249, loss = 0.302631139755249
iteration 250, loss = 0.2769876718521118
iteration 251, loss = 0.28717541694641113
iteration 252, loss = 0.2531449794769287
iteration 253, loss = 0.2595607340335846
iteration 254, loss = 0.27626702189445496
iteration 255, loss = 0.2933560013771057
iteration 256, loss = 0.2838320732116699
iteration 257, loss = 0.2675130367279053
iteration 258, loss = 0.2842250466346741
iteration 259, loss = 0.2515676021575928
iteration 260, loss = 0.2718474566936493
iteration 261, loss = 0.26968806982040405
iteration 262, loss = 0.26212644577026367
iteration 263, loss = 0.2733306884765625
iteration 264, loss = 0.28964149951934814
iteration 265, loss = 0.28187406063079834
iteration 266, loss = 0.2553139328956604
iteration 267, loss = 0.232619047164917
iteration 268, loss = 0.23831987380981445
iteration 269, loss = 0.30178511142730713
iteration 270, loss = 0.246982604265213
iteration 271, loss = 0.26425179839134216
iteration 272, loss = 0.29200097918510437
iteration 273, loss = 0.2652204632759094
iteration 274, loss = 0.2446693629026413
iteration 275, loss = 0.2595306932926178
iteration 276, loss = 0.23620565235614777
iteration 277, loss = 0.2757053077220917
iteration 278, loss = 0.27433115243911743
iteration 279, loss = 0.26039960980415344
iteration 280, loss = 0.2466939091682434
iteration 281, loss = 0.24785085022449493
iteration 282, loss = 0.2698877453804016
iteration 283, loss = 0.2471608966588974
iteration 284, loss = 0.23916427791118622
iteration 285, loss = 0.27906548976898193
iteration 286, loss = 0.23601992428302765
iteration 287, loss = 0.27772974967956543
iteration 288, loss = 0.2746727168560028
iteration 289, loss = 0.2962993085384369
iteration 290, loss = 0.2693243622779846
iteration 291, loss = 0.2716214060783386
iteration 292, loss = 0.23131440579891205
iteration 293, loss = 0.27536487579345703
iteration 294, loss = 0.24844375252723694
iteration 295, loss = 0.26614952087402344
iteration 296, loss = 0.25947096943855286
iteration 297, loss = 0.25633835792541504
iteration 298, loss = 0.2651549279689789
iteration 299, loss = 0.2699323296546936
iteration 0, loss = 0.25083503127098083
iteration 1, loss = 0.29362571239471436
iteration 2, loss = 0.30341729521751404
iteration 3, loss = 0.2291092872619629
iteration 4, loss = 0.22756949067115784
iteration 5, loss = 0.22911012172698975
iteration 6, loss = 0.28176599740982056
iteration 7, loss = 0.24838164448738098
iteration 8, loss = 0.2721301019191742
iteration 9, loss = 0.2723601460456848
iteration 10, loss = 0.25882488489151
iteration 11, loss = 0.24451199173927307
iteration 12, loss = 0.28564876317977905
iteration 13, loss = 0.27449411153793335
iteration 14, loss = 0.2819623649120331
iteration 15, loss = 0.24581244587898254
iteration 16, loss = 0.27767449617385864
iteration 17, loss = 0.2764272391796112
iteration 18, loss = 0.27693575620651245
iteration 19, loss = 0.30516546964645386
iteration 20, loss = 0.24758519232273102
iteration 21, loss = 0.2842903137207031
iteration 22, loss = 0.23817262053489685
iteration 23, loss = 0.31274136900901794
iteration 24, loss = 0.25396615266799927
iteration 25, loss = 0.25038230419158936
iteration 26, loss = 0.2354106605052948
iteration 27, loss = 0.23538245260715485
iteration 28, loss = 0.2735694348812103
iteration 29, loss = 0.25621896982192993
iteration 30, loss = 0.27385836839675903
iteration 31, loss = 0.2590036690235138
iteration 32, loss = 0.2559215724468231
iteration 33, loss = 0.25197240710258484
iteration 34, loss = 0.2975943982601166
iteration 35, loss = 0.25966763496398926
iteration 36, loss = 0.2560313045978546
iteration 37, loss = 0.24709613621234894
iteration 38, loss = 0.2416117787361145
iteration 39, loss = 0.275288850069046
iteration 40, loss = 0.27297189831733704
iteration 41, loss = 0.2657359540462494
iteration 42, loss = 0.29625365138053894
iteration 43, loss = 0.2560078501701355
iteration 44, loss = 0.2536875307559967
iteration 45, loss = 0.25079524517059326
iteration 46, loss = 0.23698902130126953
iteration 47, loss = 0.23032844066619873
iteration 48, loss = 0.2652561366558075
iteration 49, loss = 0.23878127336502075
iteration 50, loss = 0.24378496408462524
iteration 51, loss = 0.262421190738678
iteration 52, loss = 0.29727301001548767
iteration 53, loss = 0.2628844380378723
iteration 54, loss = 0.2851957380771637
iteration 55, loss = 0.23248416185379028
iteration 56, loss = 0.27183446288108826
iteration 57, loss = 0.2613586187362671
iteration 58, loss = 0.29385316371917725
iteration 59, loss = 0.25468775629997253
iteration 60, loss = 0.23275135457515717
iteration 61, loss = 0.27817314863204956
iteration 62, loss = 0.23350346088409424
iteration 63, loss = 0.2448619306087494
iteration 64, loss = 0.26083722710609436
iteration 65, loss = 0.2228105366230011
iteration 66, loss = 0.23506440222263336
iteration 67, loss = 0.24953345954418182
iteration 68, loss = 0.23715564608573914
iteration 69, loss = 0.3089677095413208
iteration 70, loss = 0.2883172631263733
iteration 71, loss = 0.24718701839447021
iteration 72, loss = 0.25488054752349854
iteration 73, loss = 0.23886767029762268
iteration 74, loss = 0.23662269115447998
iteration 75, loss = 0.24290427565574646
iteration 76, loss = 0.23301024734973907
iteration 77, loss = 0.251790314912796
iteration 78, loss = 0.23999843001365662
iteration 79, loss = 0.24875247478485107
iteration 80, loss = 0.22859828174114227
iteration 81, loss = 0.25606703758239746
iteration 82, loss = 0.3073680102825165
iteration 83, loss = 0.24503278732299805
iteration 84, loss = 0.29136815667152405
iteration 85, loss = 0.23678308725357056
iteration 86, loss = 0.265351802110672
iteration 87, loss = 0.21973884105682373
iteration 88, loss = 0.24621233344078064
iteration 89, loss = 0.22498339414596558
iteration 90, loss = 0.24556195735931396
iteration 91, loss = 0.2753066122531891
iteration 92, loss = 0.25079143047332764
iteration 93, loss = 0.25108540058135986
iteration 94, loss = 0.22580638527870178
iteration 95, loss = 0.25638633966445923
iteration 96, loss = 0.24148035049438477
iteration 97, loss = 0.2374226599931717
iteration 98, loss = 0.2442580908536911
iteration 99, loss = 0.25255075097084045
iteration 100, loss = 0.24311167001724243
iteration 101, loss = 0.3022906482219696
iteration 102, loss = 0.2644340395927429
iteration 103, loss = 0.24498382210731506
iteration 104, loss = 0.24128466844558716
iteration 105, loss = 0.22409310936927795
iteration 106, loss = 0.24709343910217285
iteration 107, loss = 0.24035324156284332
iteration 108, loss = 0.26853251457214355
iteration 109, loss = 0.30375951528549194
iteration 110, loss = 0.2034042924642563
iteration 111, loss = 0.2536320090293884
iteration 112, loss = 0.2349960207939148
iteration 113, loss = 0.26602834463119507
iteration 114, loss = 0.2497977912425995
iteration 115, loss = 0.26439082622528076
iteration 116, loss = 0.27083516120910645
iteration 117, loss = 0.24057909846305847
iteration 118, loss = 0.27399858832359314
iteration 119, loss = 0.23053835332393646
iteration 120, loss = 0.25359347462654114
iteration 121, loss = 0.3023766577243805
iteration 122, loss = 0.27703917026519775
iteration 123, loss = 0.25221458077430725
iteration 124, loss = 0.24185585975646973
iteration 125, loss = 0.2402169108390808
iteration 126, loss = 0.247404545545578
iteration 127, loss = 0.2194606065750122
iteration 128, loss = 0.2647927701473236
iteration 129, loss = 0.2161083072423935
iteration 130, loss = 0.24489806592464447
iteration 131, loss = 0.26816996932029724
iteration 132, loss = 0.25562721490859985
iteration 133, loss = 0.20841915905475616
iteration 134, loss = 0.25028201937675476
iteration 135, loss = 0.25441065430641174
iteration 136, loss = 0.27364370226860046
iteration 137, loss = 0.2714632451534271
iteration 138, loss = 0.23157736659049988
iteration 139, loss = 0.2637147903442383
iteration 140, loss = 0.24185523390769958
iteration 141, loss = 0.2489723116159439
iteration 142, loss = 0.21650972962379456
iteration 143, loss = 0.23365752398967743
iteration 144, loss = 0.22751903533935547
iteration 145, loss = 0.2548816204071045
iteration 146, loss = 0.2303386777639389
iteration 147, loss = 0.23487335443496704
iteration 148, loss = 0.24772989749908447
iteration 149, loss = 0.262905091047287
iteration 150, loss = 0.2675938606262207
iteration 151, loss = 0.266249418258667
iteration 152, loss = 0.22383089363574982
iteration 153, loss = 0.23973001539707184
iteration 154, loss = 0.24369657039642334
iteration 155, loss = 0.22437816858291626
iteration 156, loss = 0.24906054139137268
iteration 157, loss = 0.2743620276451111
iteration 158, loss = 0.23576854169368744
iteration 159, loss = 0.24934759736061096
iteration 160, loss = 0.25005465745925903
iteration 161, loss = 0.2571530044078827
iteration 162, loss = 0.23793604969978333
iteration 163, loss = 0.22279496490955353
iteration 164, loss = 0.22326385974884033
iteration 165, loss = 0.2365344762802124
iteration 166, loss = 0.1942962110042572
iteration 167, loss = 0.23283229768276215
iteration 168, loss = 0.25912103056907654
iteration 169, loss = 0.2927844524383545
iteration 170, loss = 0.19735997915267944
iteration 171, loss = 0.20655232667922974
iteration 172, loss = 0.22315877676010132
iteration 173, loss = 0.23841603100299835
iteration 174, loss = 0.24928030371665955
iteration 175, loss = 0.22260823845863342
iteration 176, loss = 0.22919961810112
iteration 177, loss = 0.23551201820373535
iteration 178, loss = 0.23376506567001343
iteration 179, loss = 0.21853680908679962
iteration 180, loss = 0.20279516279697418
iteration 181, loss = 0.2913244366645813
iteration 182, loss = 0.23904982209205627
iteration 183, loss = 0.2395898699760437
iteration 184, loss = 0.23590239882469177
iteration 185, loss = 0.20209148526191711
iteration 186, loss = 0.24329501390457153
iteration 187, loss = 0.25645196437835693
iteration 188, loss = 0.24717669188976288
iteration 189, loss = 0.21605579555034637
iteration 190, loss = 0.24801088869571686
iteration 191, loss = 0.24188941717147827
iteration 192, loss = 0.22150492668151855
iteration 193, loss = 0.21592020988464355
iteration 194, loss = 0.24827280640602112
iteration 195, loss = 0.21832375228405
iteration 196, loss = 0.2272203266620636
iteration 197, loss = 0.22871679067611694
iteration 198, loss = 0.21474233269691467
iteration 199, loss = 0.2434418648481369
iteration 200, loss = 0.2614126205444336
iteration 201, loss = 0.2254343032836914
iteration 202, loss = 0.24623209238052368
iteration 203, loss = 0.21210992336273193
iteration 204, loss = 0.20340010523796082
iteration 205, loss = 0.2550574243068695
iteration 206, loss = 0.20460397005081177
iteration 207, loss = 0.23169422149658203
iteration 208, loss = 0.23067092895507812
iteration 209, loss = 0.20171970129013062
iteration 210, loss = 0.263234406709671
iteration 211, loss = 0.21883252263069153
iteration 212, loss = 0.22699254751205444
iteration 213, loss = 0.20180867612361908
iteration 214, loss = 0.2112247347831726
iteration 215, loss = 0.2515193819999695
iteration 216, loss = 0.20657068490982056
iteration 217, loss = 0.2267657220363617
iteration 218, loss = 0.2424413561820984
iteration 219, loss = 0.22345726191997528
iteration 220, loss = 0.2212515026330948
iteration 221, loss = 0.23349948227405548
iteration 222, loss = 0.20529475808143616
iteration 223, loss = 0.24316620826721191
iteration 224, loss = 0.2171049267053604
iteration 225, loss = 0.20958150923252106
iteration 226, loss = 0.2109360247850418
iteration 227, loss = 0.2501488924026489
iteration 228, loss = 0.21568259596824646
iteration 229, loss = 0.24182763695716858
iteration 230, loss = 0.20540191233158112
iteration 231, loss = 0.2034066915512085
iteration 232, loss = 0.21049907803535461
iteration 233, loss = 0.20970574021339417
iteration 234, loss = 0.2180412858724594
iteration 235, loss = 0.22466963529586792
iteration 236, loss = 0.2036786675453186
iteration 237, loss = 0.21688508987426758
iteration 238, loss = 0.23377525806427002
iteration 239, loss = 0.22860020399093628
iteration 240, loss = 0.20014777779579163
iteration 241, loss = 0.2393171191215515
iteration 242, loss = 0.2423846572637558
iteration 243, loss = 0.20010849833488464
iteration 244, loss = 0.23985673487186432
iteration 245, loss = 0.20695173740386963
iteration 246, loss = 0.2120295763015747
iteration 247, loss = 0.2169499695301056
iteration 248, loss = 0.20273742079734802
iteration 249, loss = 0.2125074416399002
iteration 250, loss = 0.21899820864200592
iteration 251, loss = 0.19753438234329224
iteration 252, loss = 0.25681430101394653
iteration 253, loss = 0.2245945930480957
iteration 254, loss = 0.19674302637577057
iteration 255, loss = 0.18821404874324799
iteration 256, loss = 0.25319579243659973
iteration 257, loss = 0.2267075777053833
iteration 258, loss = 0.2262972742319107
iteration 259, loss = 0.20921432971954346
iteration 260, loss = 0.21144333481788635
iteration 261, loss = 0.22366085648536682
iteration 262, loss = 0.24525690078735352
iteration 263, loss = 0.2192954272031784
iteration 264, loss = 0.21906819939613342
iteration 265, loss = 0.22854898869991302
iteration 266, loss = 0.21933911740779877
iteration 267, loss = 0.23351597785949707
iteration 268, loss = 0.1970711648464203
iteration 269, loss = 0.22039452195167542
iteration 270, loss = 0.23775291442871094
iteration 271, loss = 0.21043412387371063
iteration 272, loss = 0.19429896771907806
iteration 273, loss = 0.2248658835887909
iteration 274, loss = 0.21116551756858826
iteration 275, loss = 0.2136136144399643
iteration 276, loss = 0.21047580242156982
iteration 277, loss = 0.23969772458076477
iteration 278, loss = 0.22578833997249603
iteration 279, loss = 0.22365987300872803
iteration 280, loss = 0.24047572910785675
iteration 281, loss = 0.2235097587108612
iteration 282, loss = 0.21279166638851166
iteration 283, loss = 0.24080021679401398
iteration 284, loss = 0.22133250534534454
iteration 285, loss = 0.22646382451057434
iteration 286, loss = 0.21970249712467194
iteration 287, loss = 0.22124646604061127
iteration 288, loss = 0.2062567174434662
iteration 289, loss = 0.2175574004650116
iteration 290, loss = 0.23092718422412872
iteration 291, loss = 0.19942378997802734
iteration 292, loss = 0.25266045331954956
iteration 293, loss = 0.22358408570289612
iteration 294, loss = 0.21324783563613892
iteration 295, loss = 0.2248072326183319
iteration 296, loss = 0.1901317536830902
iteration 297, loss = 0.23823155462741852
iteration 298, loss = 0.2258775681257248
iteration 299, loss = 0.2425815910100937
iteration 0, loss = 0.18124927580356598
iteration 1, loss = 0.24384266138076782
iteration 2, loss = 0.20204591751098633
iteration 3, loss = 0.21880704164505005
iteration 4, loss = 0.22721432149410248
iteration 5, loss = 0.23364198207855225
iteration 6, loss = 0.1850052922964096
iteration 7, loss = 0.1942296326160431
iteration 8, loss = 0.21530646085739136
iteration 9, loss = 0.19398215413093567
iteration 10, loss = 0.20866718888282776
iteration 11, loss = 0.21604681015014648
iteration 12, loss = 0.21156863868236542
iteration 13, loss = 0.2512222230434418
iteration 14, loss = 0.1890011876821518
iteration 15, loss = 0.1986948549747467
iteration 16, loss = 0.1801789104938507
iteration 17, loss = 0.2206859290599823
iteration 18, loss = 0.19230973720550537
iteration 19, loss = 0.2097039818763733
iteration 20, loss = 0.22393959760665894
iteration 21, loss = 0.2329522967338562
iteration 22, loss = 0.23566339910030365
iteration 23, loss = 0.2425188273191452
iteration 24, loss = 0.24134200811386108
iteration 25, loss = 0.17386750876903534
iteration 26, loss = 0.20547780394554138
iteration 27, loss = 0.23000234365463257
iteration 28, loss = 0.21915581822395325
iteration 29, loss = 0.20579084753990173
iteration 30, loss = 0.22475546598434448
iteration 31, loss = 0.2299201488494873
iteration 32, loss = 0.21102356910705566
iteration 33, loss = 0.20758448541164398
iteration 34, loss = 0.2433032989501953
iteration 35, loss = 0.20985406637191772
iteration 36, loss = 0.21493372321128845
iteration 37, loss = 0.2028491348028183
iteration 38, loss = 0.21449285745620728
iteration 39, loss = 0.2407056838274002
iteration 40, loss = 0.19995978474617004
iteration 41, loss = 0.24120759963989258
iteration 42, loss = 0.22355782985687256
iteration 43, loss = 0.21920697391033173
iteration 44, loss = 0.23805099725723267
iteration 45, loss = 0.22351887822151184
iteration 46, loss = 0.21890759468078613
iteration 47, loss = 0.2278563529253006
iteration 48, loss = 0.21234266459941864
iteration 49, loss = 0.19504112005233765
iteration 50, loss = 0.1955088973045349
iteration 51, loss = 0.2256452888250351
iteration 52, loss = 0.22579316794872284
iteration 53, loss = 0.21647998690605164
iteration 54, loss = 0.20437407493591309
iteration 55, loss = 0.2290637493133545
iteration 56, loss = 0.23529665172100067
iteration 57, loss = 0.1890229433774948
iteration 58, loss = 0.21712574362754822
iteration 59, loss = 0.22838285565376282
iteration 60, loss = 0.2275834083557129
iteration 61, loss = 0.20418763160705566
iteration 62, loss = 0.20268023014068604
iteration 63, loss = 0.2430068701505661
iteration 64, loss = 0.2258961945772171
iteration 65, loss = 0.18552178144454956
iteration 66, loss = 0.20667897164821625
iteration 67, loss = 0.2069379985332489
iteration 68, loss = 0.19366562366485596
iteration 69, loss = 0.17799310386180878
iteration 70, loss = 0.19894087314605713
iteration 71, loss = 0.20857354998588562
iteration 72, loss = 0.2045726776123047
iteration 73, loss = 0.19976098835468292
iteration 74, loss = 0.18585073947906494
iteration 75, loss = 0.21701885759830475
iteration 76, loss = 0.1798802614212036
iteration 77, loss = 0.19370898604393005
iteration 78, loss = 0.1919216811656952
iteration 79, loss = 0.21263229846954346
iteration 80, loss = 0.20596709847450256
iteration 81, loss = 0.21634799242019653
iteration 82, loss = 0.20856907963752747
iteration 83, loss = 0.19778160750865936
iteration 84, loss = 0.18653318285942078
iteration 85, loss = 0.19601190090179443
iteration 86, loss = 0.2334361970424652
iteration 87, loss = 0.2003273069858551
iteration 88, loss = 0.18277308344841003
iteration 89, loss = 0.20707769691944122
iteration 90, loss = 0.22194378077983856
iteration 91, loss = 0.21637432277202606
iteration 92, loss = 0.20197930932044983
iteration 93, loss = 0.23611263930797577
iteration 94, loss = 0.21807563304901123
iteration 95, loss = 0.17359259724617004
iteration 96, loss = 0.18744607269763947
iteration 97, loss = 0.1868521124124527
iteration 98, loss = 0.17772233486175537
iteration 99, loss = 0.1964256465435028
iteration 100, loss = 0.17973165214061737
iteration 101, loss = 0.1779405176639557
iteration 102, loss = 0.22773370146751404
iteration 103, loss = 0.21236470341682434
iteration 104, loss = 0.2313099503517151
iteration 105, loss = 0.1935146600008011
iteration 106, loss = 0.2060026228427887
iteration 107, loss = 0.22444647550582886
iteration 108, loss = 0.19602662324905396
iteration 109, loss = 0.1802898347377777
iteration 110, loss = 0.19970187544822693
iteration 111, loss = 0.1615668535232544
iteration 112, loss = 0.20545947551727295
iteration 113, loss = 0.16863201558589935
iteration 114, loss = 0.2076740562915802
iteration 115, loss = 0.18048904836177826
iteration 116, loss = 0.21107172966003418
iteration 117, loss = 0.2029990553855896
iteration 118, loss = 0.17082694172859192
iteration 119, loss = 0.18953551352024078
iteration 120, loss = 0.21845851838588715
iteration 121, loss = 0.18339753150939941
iteration 122, loss = 0.21099050343036652
iteration 123, loss = 0.1809079349040985
iteration 124, loss = 0.1780238300561905
iteration 125, loss = 0.22189664840698242
iteration 126, loss = 0.21578480303287506
iteration 127, loss = 0.22589322924613953
iteration 128, loss = 0.17617055773735046
iteration 129, loss = 0.22695055603981018
iteration 130, loss = 0.2143094688653946
iteration 131, loss = 0.20190131664276123
iteration 132, loss = 0.21016095578670502
iteration 133, loss = 0.19018881022930145
iteration 134, loss = 0.17784862220287323
iteration 135, loss = 0.1882612109184265
iteration 136, loss = 0.1759890913963318
iteration 137, loss = 0.18582957983016968
iteration 138, loss = 0.1776261180639267
iteration 139, loss = 0.1960151642560959
iteration 140, loss = 0.19300000369548798
iteration 141, loss = 0.18493661284446716
iteration 142, loss = 0.17270833253860474
iteration 143, loss = 0.19419008493423462
iteration 144, loss = 0.19992192089557648
iteration 145, loss = 0.19521456956863403
iteration 146, loss = 0.2056059092283249
iteration 147, loss = 0.20180192589759827
iteration 148, loss = 0.1829686313867569
iteration 149, loss = 0.22128474712371826
iteration 150, loss = 0.18436311185359955
iteration 151, loss = 0.18903261423110962
iteration 152, loss = 0.20019662380218506
iteration 153, loss = 0.19576506316661835
iteration 154, loss = 0.18464885652065277
iteration 155, loss = 0.21496106684207916
iteration 156, loss = 0.21625149250030518
iteration 157, loss = 0.1753484308719635
iteration 158, loss = 0.18416601419448853
iteration 159, loss = 0.1776430606842041
iteration 160, loss = 0.17817425727844238
iteration 161, loss = 0.17220133543014526
iteration 162, loss = 0.2104330211877823
iteration 163, loss = 0.15347208082675934
iteration 164, loss = 0.1670602411031723
iteration 165, loss = 0.18649010360240936
iteration 166, loss = 0.17503131926059723
iteration 167, loss = 0.20329992473125458
iteration 168, loss = 0.15423087775707245
iteration 169, loss = 0.1974826157093048
iteration 170, loss = 0.19051454961299896
iteration 171, loss = 0.20410627126693726
iteration 172, loss = 0.17216430604457855
iteration 173, loss = 0.15337809920310974
iteration 174, loss = 0.18277223408222198
iteration 175, loss = 0.21578988432884216
iteration 176, loss = 0.19105809926986694
iteration 177, loss = 0.16060346364974976
iteration 178, loss = 0.19387106597423553
iteration 179, loss = 0.20175476372241974
iteration 180, loss = 0.18914029002189636
iteration 181, loss = 0.21887728571891785
iteration 182, loss = 0.21198856830596924
iteration 183, loss = 0.2013518363237381
iteration 184, loss = 0.19376029074192047
iteration 185, loss = 0.23113758862018585
iteration 186, loss = 0.19012585282325745
iteration 187, loss = 0.16953113675117493
iteration 188, loss = 0.2063637375831604
iteration 189, loss = 0.19195674359798431
iteration 190, loss = 0.18343022465705872
iteration 191, loss = 0.18769057095050812
iteration 192, loss = 0.18439015746116638
iteration 193, loss = 0.19960708916187286
iteration 194, loss = 0.19204604625701904
iteration 195, loss = 0.17180205881595612
iteration 196, loss = 0.17140352725982666
iteration 197, loss = 0.1855039745569229
iteration 198, loss = 0.17585372924804688
iteration 199, loss = 0.1958286464214325
iteration 200, loss = 0.1629529595375061
iteration 201, loss = 0.183770552277565
iteration 202, loss = 0.1909686028957367
iteration 203, loss = 0.16495929658412933
iteration 204, loss = 0.2181445211172104
iteration 205, loss = 0.17118681967258453
iteration 206, loss = 0.1911444514989853
iteration 207, loss = 0.17483177781105042
iteration 208, loss = 0.22662879526615143
iteration 209, loss = 0.1784706711769104
iteration 210, loss = 0.16229811310768127
iteration 211, loss = 0.21426868438720703
iteration 212, loss = 0.22320812940597534
iteration 213, loss = 0.18850135803222656
iteration 214, loss = 0.17957937717437744
iteration 215, loss = 0.2045307159423828
iteration 216, loss = 0.19252857565879822
iteration 217, loss = 0.19489535689353943
iteration 218, loss = 0.19553107023239136
iteration 219, loss = 0.16065458953380585
iteration 220, loss = 0.2037397027015686
iteration 221, loss = 0.18339096009731293
iteration 222, loss = 0.1936941295862198
iteration 223, loss = 0.1876661479473114
iteration 224, loss = 0.21664345264434814
iteration 225, loss = 0.18206293880939484
iteration 226, loss = 0.17633534967899323
iteration 227, loss = 0.17731495201587677
iteration 228, loss = 0.16414496302604675
iteration 229, loss = 0.19513700902462006
iteration 230, loss = 0.15779435634613037
iteration 231, loss = 0.2074182629585266
iteration 232, loss = 0.19493266940116882
iteration 233, loss = 0.20989316701889038
iteration 234, loss = 0.16148710250854492
iteration 235, loss = 0.17861314117908478
iteration 236, loss = 0.2077697068452835
iteration 237, loss = 0.19546936452388763
iteration 238, loss = 0.16390086710453033
iteration 239, loss = 0.17155057191848755
iteration 240, loss = 0.19799163937568665
iteration 241, loss = 0.19575771689414978
iteration 242, loss = 0.1720615178346634
iteration 243, loss = 0.17066901922225952
iteration 244, loss = 0.16611561179161072
iteration 245, loss = 0.17330729961395264
iteration 246, loss = 0.15955454111099243
iteration 247, loss = 0.14657074213027954
iteration 248, loss = 0.17327816784381866
iteration 249, loss = 0.17236372828483582
iteration 250, loss = 0.16907627880573273
iteration 251, loss = 0.16854596138000488
iteration 252, loss = 0.18112684786319733
iteration 253, loss = 0.21657781302928925
iteration 254, loss = 0.19728237390518188
iteration 255, loss = 0.18325716257095337
iteration 256, loss = 0.18832853436470032
iteration 257, loss = 0.17118819057941437
iteration 258, loss = 0.17389462888240814
iteration 259, loss = 0.19197425246238708
iteration 260, loss = 0.17313756048679352
iteration 261, loss = 0.1691579520702362
iteration 262, loss = 0.15490448474884033
iteration 263, loss = 0.16573162376880646
iteration 264, loss = 0.17093217372894287
iteration 265, loss = 0.16825778782367706
iteration 266, loss = 0.22729355096817017
iteration 267, loss = 0.16608253121376038
iteration 268, loss = 0.17183247208595276
iteration 269, loss = 0.1406971961259842
iteration 270, loss = 0.17617495357990265
iteration 271, loss = 0.20480521023273468
iteration 272, loss = 0.1810397058725357
iteration 273, loss = 0.15982268750667572
iteration 274, loss = 0.20278400182724
iteration 275, loss = 0.20243337750434875
iteration 276, loss = 0.16991576552391052
iteration 277, loss = 0.1677151918411255
iteration 278, loss = 0.17928853631019592
iteration 279, loss = 0.17138166725635529
iteration 280, loss = 0.16589482128620148
iteration 281, loss = 0.17169028520584106
iteration 282, loss = 0.16737613081932068
iteration 283, loss = 0.15867386758327484
iteration 284, loss = 0.16658350825309753
iteration 285, loss = 0.17948785424232483
iteration 286, loss = 0.20732897520065308
iteration 287, loss = 0.16754800081253052
iteration 288, loss = 0.15902924537658691
iteration 289, loss = 0.18678852915763855
iteration 290, loss = 0.19088266789913177
iteration 291, loss = 0.20947523415088654
iteration 292, loss = 0.1652287244796753
iteration 293, loss = 0.20522025227546692
iteration 294, loss = 0.16125354170799255
iteration 295, loss = 0.17040282487869263
iteration 296, loss = 0.1923915445804596
iteration 297, loss = 0.16502247750759125
iteration 298, loss = 0.20706382393836975
iteration 299, loss = 0.16176468133926392
iteration 0, loss = 0.15173843502998352
iteration 1, loss = 0.15164557099342346
iteration 2, loss = 0.17955423891544342
iteration 3, loss = 0.16632170975208282
iteration 4, loss = 0.1913239061832428
iteration 5, loss = 0.13654178380966187
iteration 6, loss = 0.1939469873905182
iteration 7, loss = 0.1787249892950058
iteration 8, loss = 0.17243905365467072
iteration 9, loss = 0.21538835763931274
iteration 10, loss = 0.19054624438285828
iteration 11, loss = 0.17205870151519775
iteration 12, loss = 0.17838972806930542
iteration 13, loss = 0.18224775791168213
iteration 14, loss = 0.16664989292621613
iteration 15, loss = 0.1692752242088318
iteration 16, loss = 0.16013717651367188
iteration 17, loss = 0.1561623513698578
iteration 18, loss = 0.15485037863254547
iteration 19, loss = 0.22384899854660034
iteration 20, loss = 0.17137718200683594
iteration 21, loss = 0.2013864368200302
iteration 22, loss = 0.16666831076145172
iteration 23, loss = 0.17020359635353088
iteration 24, loss = 0.167766734957695
iteration 25, loss = 0.1772870123386383
iteration 26, loss = 0.17569300532341003
iteration 27, loss = 0.19784143567085266
iteration 28, loss = 0.18088167905807495
iteration 29, loss = 0.19348350167274475
iteration 30, loss = 0.15957000851631165
iteration 31, loss = 0.1576903611421585
iteration 32, loss = 0.15014046430587769
iteration 33, loss = 0.16516011953353882
iteration 34, loss = 0.16963717341423035
iteration 35, loss = 0.14523571729660034
iteration 36, loss = 0.1669568419456482
iteration 37, loss = 0.1661890149116516
iteration 38, loss = 0.16947275400161743
iteration 39, loss = 0.1664426326751709
iteration 40, loss = 0.1748523712158203
iteration 41, loss = 0.17516754567623138
iteration 42, loss = 0.167079359292984
iteration 43, loss = 0.17301031947135925
iteration 44, loss = 0.14157353341579437
iteration 45, loss = 0.15108607709407806
iteration 46, loss = 0.1566573977470398
iteration 47, loss = 0.14882150292396545
iteration 48, loss = 0.19081225991249084
iteration 49, loss = 0.13902893662452698
iteration 50, loss = 0.16147859394550323
iteration 51, loss = 0.1549069583415985
iteration 52, loss = 0.16986864805221558
iteration 53, loss = 0.17046217620372772
iteration 54, loss = 0.1500004529953003
iteration 55, loss = 0.17998823523521423
iteration 56, loss = 0.14988207817077637
iteration 57, loss = 0.19082403182983398
iteration 58, loss = 0.19050005078315735
iteration 59, loss = 0.16932347416877747
iteration 60, loss = 0.1320485770702362
iteration 61, loss = 0.17726823687553406
iteration 62, loss = 0.14703506231307983
iteration 63, loss = 0.1547420769929886
iteration 64, loss = 0.17995868623256683
iteration 65, loss = 0.14998802542686462
iteration 66, loss = 0.15922433137893677
iteration 67, loss = 0.15761390328407288
iteration 68, loss = 0.1752914935350418
iteration 69, loss = 0.14935116469860077
iteration 70, loss = 0.19624188542366028
iteration 71, loss = 0.15087255835533142
iteration 72, loss = 0.15571311116218567
iteration 73, loss = 0.17179502546787262
iteration 74, loss = 0.17163819074630737
iteration 75, loss = 0.17880979180335999
iteration 76, loss = 0.17138397693634033
iteration 77, loss = 0.15670184791088104
iteration 78, loss = 0.1392289102077484
iteration 79, loss = 0.13675375282764435
iteration 80, loss = 0.1469595730304718
iteration 81, loss = 0.1496429741382599
iteration 82, loss = 0.15630152821540833
iteration 83, loss = 0.13766001164913177
iteration 84, loss = 0.14674639701843262
iteration 85, loss = 0.1501963883638382
iteration 86, loss = 0.16249658167362213
iteration 87, loss = 0.1681760847568512
iteration 88, loss = 0.14779198169708252
iteration 89, loss = 0.15536735951900482
iteration 90, loss = 0.16428178548812866
iteration 91, loss = 0.1878090798854828
iteration 92, loss = 0.16171927750110626
iteration 93, loss = 0.15481942892074585
iteration 94, loss = 0.14937007427215576
iteration 95, loss = 0.18196672201156616
iteration 96, loss = 0.12623175978660583
iteration 97, loss = 0.16795192658901215
iteration 98, loss = 0.14459183812141418
iteration 99, loss = 0.1610257774591446
iteration 100, loss = 0.14135242998600006
iteration 101, loss = 0.17736679315567017
iteration 102, loss = 0.17156051099300385
iteration 103, loss = 0.16873909533023834
iteration 104, loss = 0.16133034229278564
iteration 105, loss = 0.17170558869838715
iteration 106, loss = 0.16761445999145508
iteration 107, loss = 0.14318761229515076
iteration 108, loss = 0.1398906260728836
iteration 109, loss = 0.1739812195301056
iteration 110, loss = 0.13665278255939484
iteration 111, loss = 0.16042011976242065
iteration 112, loss = 0.15596641600131989
iteration 113, loss = 0.178115114569664
iteration 114, loss = 0.1677490770816803
iteration 115, loss = 0.17959468066692352
iteration 116, loss = 0.14784809947013855
iteration 117, loss = 0.15059137344360352
iteration 118, loss = 0.1258257031440735
iteration 119, loss = 0.14843472838401794
iteration 120, loss = 0.16997045278549194
iteration 121, loss = 0.13518401980400085
iteration 122, loss = 0.17618034780025482
iteration 123, loss = 0.15466910600662231
iteration 124, loss = 0.1632148027420044
iteration 125, loss = 0.16087639331817627
iteration 126, loss = 0.16614091396331787
iteration 127, loss = 0.16718384623527527
iteration 128, loss = 0.1634068638086319
iteration 129, loss = 0.14549627900123596
iteration 130, loss = 0.15866895020008087
iteration 131, loss = 0.1329648196697235
iteration 132, loss = 0.1384143829345703
iteration 133, loss = 0.1665475070476532
iteration 134, loss = 0.1578260213136673
iteration 135, loss = 0.1412997841835022
iteration 136, loss = 0.1354188621044159
iteration 137, loss = 0.16204875707626343
iteration 138, loss = 0.15821024775505066
iteration 139, loss = 0.17035603523254395
iteration 140, loss = 0.15544383227825165
iteration 141, loss = 0.1302442103624344
iteration 142, loss = 0.14568307995796204
iteration 143, loss = 0.1429339349269867
iteration 144, loss = 0.16261892020702362
iteration 145, loss = 0.14714635908603668
iteration 146, loss = 0.14798346161842346
iteration 147, loss = 0.15543845295906067
iteration 148, loss = 0.14149385690689087
iteration 149, loss = 0.1233331710100174
iteration 150, loss = 0.16391810774803162
iteration 151, loss = 0.16852092742919922
iteration 152, loss = 0.1715211570262909
iteration 153, loss = 0.160369411110878
iteration 154, loss = 0.11852744966745377
iteration 155, loss = 0.14934420585632324
iteration 156, loss = 0.1403888761997223
iteration 157, loss = 0.1556902825832367
iteration 158, loss = 0.143715500831604
iteration 159, loss = 0.15694481134414673
iteration 160, loss = 0.18222177028656006
iteration 161, loss = 0.15728993713855743
iteration 162, loss = 0.14851537346839905
iteration 163, loss = 0.17338085174560547
iteration 164, loss = 0.1903889775276184
iteration 165, loss = 0.17771971225738525
iteration 166, loss = 0.1628023386001587
iteration 167, loss = 0.13086996972560883
iteration 168, loss = 0.14008504152297974
iteration 169, loss = 0.15136733651161194
iteration 170, loss = 0.15274986624717712
iteration 171, loss = 0.15669158101081848
iteration 172, loss = 0.15420731902122498
iteration 173, loss = 0.15492506325244904
iteration 174, loss = 0.13409937918186188
iteration 175, loss = 0.13100507855415344
iteration 176, loss = 0.13367027044296265
iteration 177, loss = 0.15610051155090332
iteration 178, loss = 0.18316519260406494
iteration 179, loss = 0.13648371398448944
iteration 180, loss = 0.16827143728733063
iteration 181, loss = 0.17252248525619507
iteration 182, loss = 0.14248406887054443
iteration 183, loss = 0.16178151965141296
iteration 184, loss = 0.12874861061573029
iteration 185, loss = 0.1337175816297531
iteration 186, loss = 0.15592354536056519
iteration 187, loss = 0.12160089612007141
iteration 188, loss = 0.1435914933681488
iteration 189, loss = 0.16673152148723602
iteration 190, loss = 0.12528076767921448
iteration 191, loss = 0.18212568759918213
iteration 192, loss = 0.1521557867527008
iteration 193, loss = 0.12966158986091614
iteration 194, loss = 0.15276217460632324
iteration 195, loss = 0.14189669489860535
iteration 196, loss = 0.14294323325157166
iteration 197, loss = 0.13388222455978394
iteration 198, loss = 0.14518003165721893
iteration 199, loss = 0.16597074270248413
iteration 200, loss = 0.16041384637355804
iteration 201, loss = 0.16497406363487244
iteration 202, loss = 0.14657947421073914
iteration 203, loss = 0.159602552652359
iteration 204, loss = 0.15861691534519196
iteration 205, loss = 0.14176921546459198
iteration 206, loss = 0.15413598716259003
iteration 207, loss = 0.13840359449386597
iteration 208, loss = 0.16156445443630219
iteration 209, loss = 0.1568644642829895
iteration 210, loss = 0.17192687094211578
iteration 211, loss = 0.15776625275611877
iteration 212, loss = 0.15482665598392487
iteration 213, loss = 0.1410982608795166
iteration 214, loss = 0.14685410261154175
iteration 215, loss = 0.1420293152332306
iteration 216, loss = 0.1299067586660385
iteration 217, loss = 0.12640516459941864
iteration 218, loss = 0.13933584094047546
iteration 219, loss = 0.14622217416763306
iteration 220, loss = 0.16359803080558777
iteration 221, loss = 0.13287453353405
iteration 222, loss = 0.15724055469036102
iteration 223, loss = 0.12307801842689514
iteration 224, loss = 0.15431374311447144
iteration 225, loss = 0.13766543567180634
iteration 226, loss = 0.16106852889060974
iteration 227, loss = 0.1432168334722519
iteration 228, loss = 0.14737901091575623
iteration 229, loss = 0.13946634531021118
iteration 230, loss = 0.1292085349559784
iteration 231, loss = 0.13339059054851532
iteration 232, loss = 0.15053176879882812
iteration 233, loss = 0.16571226716041565
iteration 234, loss = 0.15392951667308807
iteration 235, loss = 0.1663219928741455
iteration 236, loss = 0.15865401923656464
iteration 237, loss = 0.14922954142093658
iteration 238, loss = 0.12606772780418396
iteration 239, loss = 0.15698419511318207
iteration 240, loss = 0.14479368925094604
iteration 241, loss = 0.12524493038654327
iteration 242, loss = 0.14307047426700592
iteration 243, loss = 0.14409516751766205
iteration 244, loss = 0.1200190931558609
iteration 245, loss = 0.15063905715942383
iteration 246, loss = 0.1613103300333023
iteration 247, loss = 0.14759749174118042
iteration 248, loss = 0.11788838356733322
iteration 249, loss = 0.13248515129089355
iteration 250, loss = 0.13210071623325348
iteration 251, loss = 0.12473908066749573
iteration 252, loss = 0.11522277444601059
iteration 253, loss = 0.13170243799686432
iteration 254, loss = 0.12528492510318756
iteration 255, loss = 0.15089572966098785
iteration 256, loss = 0.15219497680664062
iteration 257, loss = 0.1480560600757599
iteration 258, loss = 0.10939975827932358
iteration 259, loss = 0.1324767768383026
iteration 260, loss = 0.15858544409275055
iteration 261, loss = 0.1323026567697525
iteration 262, loss = 0.11837614327669144
iteration 263, loss = 0.14712636172771454
iteration 264, loss = 0.15532322227954865
iteration 265, loss = 0.1305367797613144
iteration 266, loss = 0.13127641379833221
iteration 267, loss = 0.14272315800189972
iteration 268, loss = 0.16560675203800201
iteration 269, loss = 0.15098518133163452
iteration 270, loss = 0.13355359435081482
iteration 271, loss = 0.15067215263843536
iteration 272, loss = 0.12466105073690414
iteration 273, loss = 0.19834554195404053
iteration 274, loss = 0.13893696665763855
iteration 275, loss = 0.15452706813812256
iteration 276, loss = 0.146424800157547
iteration 277, loss = 0.13772550225257874
iteration 278, loss = 0.1539757400751114
iteration 279, loss = 0.14033640921115875
iteration 280, loss = 0.13589425384998322
iteration 281, loss = 0.1327960044145584
iteration 282, loss = 0.136921226978302
iteration 283, loss = 0.12324418127536774
iteration 284, loss = 0.12107275426387787
iteration 285, loss = 0.14225439727306366
iteration 286, loss = 0.19276441633701324
iteration 287, loss = 0.11906180530786514
iteration 288, loss = 0.1567026674747467
iteration 289, loss = 0.16988390684127808
iteration 290, loss = 0.1322963833808899
iteration 291, loss = 0.147110715508461
iteration 292, loss = 0.1555306613445282
iteration 293, loss = 0.1509987860918045
iteration 294, loss = 0.12472004443407059
iteration 295, loss = 0.1624959260225296
iteration 296, loss = 0.14161160588264465
iteration 297, loss = 0.1412329524755478
iteration 298, loss = 0.1304267942905426
iteration 299, loss = 0.12763017416000366
iteration 0, loss = 0.11613550782203674
iteration 1, loss = 0.1060367077589035
iteration 2, loss = 0.16476395726203918
iteration 3, loss = 0.1196618378162384
iteration 4, loss = 0.13893531262874603
iteration 5, loss = 0.12499798089265823
iteration 6, loss = 0.13169188797473907
iteration 7, loss = 0.11739206314086914
iteration 8, loss = 0.1288987100124359
iteration 9, loss = 0.12747922539710999
iteration 10, loss = 0.16059283912181854
iteration 11, loss = 0.1420477032661438
iteration 12, loss = 0.1132940798997879
iteration 13, loss = 0.13567712903022766
iteration 14, loss = 0.1585378348827362
iteration 15, loss = 0.12127065658569336
iteration 16, loss = 0.14414732158184052
iteration 17, loss = 0.1061488687992096
iteration 18, loss = 0.1284889578819275
iteration 19, loss = 0.12494102865457535
iteration 20, loss = 0.1158919706940651
iteration 21, loss = 0.12692084908485413
iteration 22, loss = 0.1196538507938385
iteration 23, loss = 0.1165497750043869
iteration 24, loss = 0.16798800230026245
iteration 25, loss = 0.13784097135066986
iteration 26, loss = 0.12407473474740982
iteration 27, loss = 0.13574235141277313
iteration 28, loss = 0.10965046286582947
iteration 29, loss = 0.138972669839859
iteration 30, loss = 0.11335749179124832
iteration 31, loss = 0.1306511014699936
iteration 32, loss = 0.12104631960391998
iteration 33, loss = 0.13051286339759827
iteration 34, loss = 0.12067030370235443
iteration 35, loss = 0.1446988582611084
iteration 36, loss = 0.1226329505443573
iteration 37, loss = 0.18138329684734344
iteration 38, loss = 0.13823439180850983
iteration 39, loss = 0.11430100351572037
iteration 40, loss = 0.12191876769065857
iteration 41, loss = 0.12723742425441742
iteration 42, loss = 0.1180962398648262
iteration 43, loss = 0.15697510540485382
iteration 44, loss = 0.14355258643627167
iteration 45, loss = 0.11691101640462875
iteration 46, loss = 0.12503847479820251
iteration 47, loss = 0.1292644441127777
iteration 48, loss = 0.14035621285438538
iteration 49, loss = 0.12676577270030975
iteration 50, loss = 0.14009594917297363
iteration 51, loss = 0.11541341990232468
iteration 52, loss = 0.12341423332691193
iteration 53, loss = 0.12198948860168457
iteration 54, loss = 0.11521629989147186
iteration 55, loss = 0.12983250617980957
iteration 56, loss = 0.14039644598960876
iteration 57, loss = 0.13136187195777893
iteration 58, loss = 0.13771389424800873
iteration 59, loss = 0.1446281224489212
iteration 60, loss = 0.12750554084777832
iteration 61, loss = 0.13067898154258728
iteration 62, loss = 0.1437729299068451
iteration 63, loss = 0.10730431228876114
iteration 64, loss = 0.1381145715713501
iteration 65, loss = 0.14245480298995972
iteration 66, loss = 0.14546217024326324
iteration 67, loss = 0.13871626555919647
iteration 68, loss = 0.12623485922813416
iteration 69, loss = 0.13462841510772705
iteration 70, loss = 0.11883674561977386
iteration 71, loss = 0.11542177945375443
iteration 72, loss = 0.13337349891662598
iteration 73, loss = 0.13265684247016907
iteration 74, loss = 0.10704725980758667
iteration 75, loss = 0.12813377380371094
iteration 76, loss = 0.12588584423065186
iteration 77, loss = 0.1389102041721344
iteration 78, loss = 0.15440034866333008
iteration 79, loss = 0.1182822436094284
iteration 80, loss = 0.1309501826763153
iteration 81, loss = 0.1252349317073822
iteration 82, loss = 0.15116184949874878
iteration 83, loss = 0.11602715402841568
iteration 84, loss = 0.1180991381406784
iteration 85, loss = 0.12436439096927643
iteration 86, loss = 0.12493960559368134
iteration 87, loss = 0.11996645480394363
iteration 88, loss = 0.1326037347316742
iteration 89, loss = 0.1496894210577011
iteration 90, loss = 0.14478059113025665
iteration 91, loss = 0.10952690243721008
iteration 92, loss = 0.10815806686878204
iteration 93, loss = 0.12480001151561737
iteration 94, loss = 0.11442513018846512
iteration 95, loss = 0.1412816196680069
iteration 96, loss = 0.12737107276916504
iteration 97, loss = 0.12176220118999481
iteration 98, loss = 0.11477912217378616
iteration 99, loss = 0.165226548910141
iteration 100, loss = 0.1324506402015686
iteration 101, loss = 0.12436868250370026
iteration 102, loss = 0.10793318599462509
iteration 103, loss = 0.1021958738565445
iteration 104, loss = 0.10380546748638153
iteration 105, loss = 0.12633009254932404
iteration 106, loss = 0.13166514039039612
iteration 107, loss = 0.1552375853061676
iteration 108, loss = 0.1406880021095276
iteration 109, loss = 0.1271209567785263
iteration 110, loss = 0.16503889858722687
iteration 111, loss = 0.11058692634105682
iteration 112, loss = 0.11275488138198853
iteration 113, loss = 0.11817924678325653
iteration 114, loss = 0.1277262270450592
iteration 115, loss = 0.09614308923482895
iteration 116, loss = 0.11447808146476746
iteration 117, loss = 0.12336369603872299
iteration 118, loss = 0.1401907056570053
iteration 119, loss = 0.11309157311916351
iteration 120, loss = 0.11094105243682861
iteration 121, loss = 0.10853055119514465
iteration 122, loss = 0.13865265250205994
iteration 123, loss = 0.12773878872394562
iteration 124, loss = 0.10665010660886765
iteration 125, loss = 0.1155315637588501
iteration 126, loss = 0.10438590496778488
iteration 127, loss = 0.11478309333324432
iteration 128, loss = 0.13758349418640137
iteration 129, loss = 0.1331305056810379
iteration 130, loss = 0.14726006984710693
iteration 131, loss = 0.14107143878936768
iteration 132, loss = 0.10805860161781311
iteration 133, loss = 0.12351474910974503
iteration 134, loss = 0.10971732437610626
iteration 135, loss = 0.12672623991966248
iteration 136, loss = 0.11232604086399078
iteration 137, loss = 0.12812572717666626
iteration 138, loss = 0.09435370564460754
iteration 139, loss = 0.12267334759235382
iteration 140, loss = 0.11705708503723145
iteration 141, loss = 0.12150225788354874
iteration 142, loss = 0.14177823066711426
iteration 143, loss = 0.11758343130350113
iteration 144, loss = 0.16784119606018066
iteration 145, loss = 0.13636718690395355
iteration 146, loss = 0.15339680016040802
iteration 147, loss = 0.10733158886432648
iteration 148, loss = 0.10045646876096725
iteration 149, loss = 0.14222997426986694
iteration 150, loss = 0.14244450628757477
iteration 151, loss = 0.11013849079608917
iteration 152, loss = 0.1168394610285759
iteration 153, loss = 0.11474683880805969
iteration 154, loss = 0.11128019541501999
iteration 155, loss = 0.10899385809898376
iteration 156, loss = 0.12481760233640671
iteration 157, loss = 0.1351126730442047
iteration 158, loss = 0.13449233770370483
iteration 159, loss = 0.12709078192710876
iteration 160, loss = 0.13387367129325867
iteration 161, loss = 0.13975799083709717
iteration 162, loss = 0.1449936181306839
iteration 163, loss = 0.14129847288131714
iteration 164, loss = 0.1351909041404724
iteration 165, loss = 0.12850341200828552
iteration 166, loss = 0.11051413416862488
iteration 167, loss = 0.10836344212293625
iteration 168, loss = 0.12185287475585938
iteration 169, loss = 0.12457440048456192
iteration 170, loss = 0.11304021626710892
iteration 171, loss = 0.12948599457740784
iteration 172, loss = 0.14279861748218536
iteration 173, loss = 0.11909948289394379
iteration 174, loss = 0.12129558622837067
iteration 175, loss = 0.10005110502243042
iteration 176, loss = 0.09318506717681885
iteration 177, loss = 0.12190091609954834
iteration 178, loss = 0.10269933938980103
iteration 179, loss = 0.11294352263212204
iteration 180, loss = 0.09251103550195694
iteration 181, loss = 0.13724285364151
iteration 182, loss = 0.12268459051847458
iteration 183, loss = 0.11048968136310577
iteration 184, loss = 0.1326519399881363
iteration 185, loss = 0.124735027551651
iteration 186, loss = 0.10044267773628235
iteration 187, loss = 0.11318618804216385
iteration 188, loss = 0.1336623877286911
iteration 189, loss = 0.14905183017253876
iteration 190, loss = 0.10219790041446686
iteration 191, loss = 0.09918208420276642
iteration 192, loss = 0.10343527793884277
iteration 193, loss = 0.11954662948846817
iteration 194, loss = 0.11868201196193695
iteration 195, loss = 0.11044833809137344
iteration 196, loss = 0.12535688281059265
iteration 197, loss = 0.12122085690498352
iteration 198, loss = 0.12591126561164856
iteration 199, loss = 0.12735088169574738
iteration 200, loss = 0.10979212820529938
iteration 201, loss = 0.10776206851005554
iteration 202, loss = 0.10369377583265305
iteration 203, loss = 0.10516701638698578
iteration 204, loss = 0.13023394346237183
iteration 205, loss = 0.14190533757209778
iteration 206, loss = 0.08952125906944275
iteration 207, loss = 0.09995478391647339
iteration 208, loss = 0.10767471790313721
iteration 209, loss = 0.10517983883619308
iteration 210, loss = 0.14134447276592255
iteration 211, loss = 0.09857459366321564
iteration 212, loss = 0.09515754878520966
iteration 213, loss = 0.10242018103599548
iteration 214, loss = 0.10149430483579636
iteration 215, loss = 0.11326336860656738
iteration 216, loss = 0.09510062634944916
iteration 217, loss = 0.096558578312397
iteration 218, loss = 0.11173725873231888
iteration 219, loss = 0.12333840131759644
iteration 220, loss = 0.09972532093524933
iteration 221, loss = 0.11855032294988632
iteration 222, loss = 0.11387279629707336
iteration 223, loss = 0.13145071268081665
iteration 224, loss = 0.11289878189563751
iteration 225, loss = 0.09672996401786804
iteration 226, loss = 0.10078011453151703
iteration 227, loss = 0.09183617681264877
iteration 228, loss = 0.11297480762004852
iteration 229, loss = 0.10784509778022766
iteration 230, loss = 0.09334717690944672
iteration 231, loss = 0.11655999720096588
iteration 232, loss = 0.1348065584897995
iteration 233, loss = 0.12803304195404053
iteration 234, loss = 0.11857283115386963
iteration 235, loss = 0.10169881582260132
iteration 236, loss = 0.09995761513710022
iteration 237, loss = 0.11655230820178986
iteration 238, loss = 0.1292145550251007
iteration 239, loss = 0.11365412920713425
iteration 240, loss = 0.11098036915063858
iteration 241, loss = 0.10270872712135315
iteration 242, loss = 0.11018960177898407
iteration 243, loss = 0.11614266782999039
iteration 244, loss = 0.11126932501792908
iteration 245, loss = 0.10895248502492905
iteration 246, loss = 0.10331860929727554
iteration 247, loss = 0.1280147284269333
iteration 248, loss = 0.10380399227142334
iteration 249, loss = 0.10088105499744415
iteration 250, loss = 0.11786133050918579
iteration 251, loss = 0.10665944218635559
iteration 252, loss = 0.11279963701963425
iteration 253, loss = 0.11582539975643158
iteration 254, loss = 0.11896823346614838
iteration 255, loss = 0.1274212896823883
iteration 256, loss = 0.1364155113697052
iteration 257, loss = 0.13220366835594177
iteration 258, loss = 0.10734210908412933
iteration 259, loss = 0.11308088898658752
iteration 260, loss = 0.08846426755189896
iteration 261, loss = 0.13595634698867798
iteration 262, loss = 0.126837819814682
iteration 263, loss = 0.12062186747789383
iteration 264, loss = 0.09780833125114441
iteration 265, loss = 0.14100541174411774
iteration 266, loss = 0.09583213925361633
iteration 267, loss = 0.12466005980968475
iteration 268, loss = 0.10744588822126389
iteration 269, loss = 0.08684780448675156
iteration 270, loss = 0.11243931949138641
iteration 271, loss = 0.10640958696603775
iteration 272, loss = 0.10747741162776947
iteration 273, loss = 0.11250344663858414
iteration 274, loss = 0.08509379625320435
iteration 275, loss = 0.12119410187005997
iteration 276, loss = 0.09368722885847092
iteration 277, loss = 0.10768235474824905
iteration 278, loss = 0.10826829075813293
iteration 279, loss = 0.12119603157043457
iteration 280, loss = 0.10416065901517868
iteration 281, loss = 0.10001643002033234
iteration 282, loss = 0.11013438552618027
iteration 283, loss = 0.1148953065276146
iteration 284, loss = 0.09645123779773712
iteration 285, loss = 0.11019794642925262
iteration 286, loss = 0.0864848792552948
iteration 287, loss = 0.11411067843437195
iteration 288, loss = 0.1268080472946167
iteration 289, loss = 0.09074776619672775
iteration 290, loss = 0.1029602438211441
iteration 291, loss = 0.09905754029750824
iteration 292, loss = 0.10546077787876129
iteration 293, loss = 0.11737385392189026
iteration 294, loss = 0.09861969947814941
iteration 295, loss = 0.08995069563388824
iteration 296, loss = 0.09723935276269913
iteration 297, loss = 0.133964404463768
iteration 298, loss = 0.09486652165651321
iteration 299, loss = 0.09386232495307922
iteration 0, loss = 0.09309979528188705
iteration 1, loss = 0.10834366083145142
iteration 2, loss = 0.09890728443861008
iteration 3, loss = 0.11052446812391281
iteration 4, loss = 0.10645340383052826
iteration 5, loss = 0.12864843010902405
iteration 6, loss = 0.09420589357614517
iteration 7, loss = 0.09137426316738129
iteration 8, loss = 0.10845352709293365
iteration 9, loss = 0.10175095498561859
iteration 10, loss = 0.10913596302270889
iteration 11, loss = 0.10180126875638962
iteration 12, loss = 0.09449417889118195
iteration 13, loss = 0.10351134091615677
iteration 14, loss = 0.1017109602689743
iteration 15, loss = 0.0958225205540657
iteration 16, loss = 0.10736303776502609
iteration 17, loss = 0.11052308976650238
iteration 18, loss = 0.10339809954166412
iteration 19, loss = 0.09621554613113403
iteration 20, loss = 0.09179358184337616
iteration 21, loss = 0.14683260023593903
iteration 22, loss = 0.13706143200397491
iteration 23, loss = 0.10460469871759415
iteration 24, loss = 0.09586197882890701
iteration 25, loss = 0.10488870739936829
iteration 26, loss = 0.10486198961734772
iteration 27, loss = 0.0938582718372345
iteration 28, loss = 0.11255413293838501
iteration 29, loss = 0.11004692316055298
iteration 30, loss = 0.0802905485033989
iteration 31, loss = 0.13182301819324493
iteration 32, loss = 0.09946098923683167
iteration 33, loss = 0.0862826555967331
iteration 34, loss = 0.11849121749401093
iteration 35, loss = 0.09560056030750275
iteration 36, loss = 0.09700855612754822
iteration 37, loss = 0.10087250918149948
iteration 38, loss = 0.09954862296581268
iteration 39, loss = 0.10606451332569122
iteration 40, loss = 0.08621390908956528
iteration 41, loss = 0.08793973177671432
iteration 42, loss = 0.10242978483438492
iteration 43, loss = 0.1150146871805191
iteration 44, loss = 0.11337477713823318
iteration 45, loss = 0.11803758889436722
iteration 46, loss = 0.09861413389444351
iteration 47, loss = 0.11051411926746368
iteration 48, loss = 0.10816660523414612
iteration 49, loss = 0.0873635858297348
iteration 50, loss = 0.094243623316288
iteration 51, loss = 0.08208177238702774
iteration 52, loss = 0.10072554647922516
iteration 53, loss = 0.09337262809276581
iteration 54, loss = 0.10226836800575256
iteration 55, loss = 0.08668315410614014
iteration 56, loss = 0.09453745186328888
iteration 57, loss = 0.08478017151355743
iteration 58, loss = 0.10373064875602722
iteration 59, loss = 0.08917542546987534
iteration 60, loss = 0.09733469784259796
iteration 61, loss = 0.11648853123188019
iteration 62, loss = 0.10892502963542938
iteration 63, loss = 0.1252165138721466
iteration 64, loss = 0.09962006658315659
iteration 65, loss = 0.08802032470703125
iteration 66, loss = 0.10124761611223221
iteration 67, loss = 0.10118719935417175
iteration 68, loss = 0.10004857182502747
iteration 69, loss = 0.09715452790260315
iteration 70, loss = 0.11894385516643524
iteration 71, loss = 0.08442925661802292
iteration 72, loss = 0.1039692759513855
iteration 73, loss = 0.10221558064222336
iteration 74, loss = 0.08455678075551987
iteration 75, loss = 0.11814498156309128
iteration 76, loss = 0.07486866414546967
iteration 77, loss = 0.08741609752178192
iteration 78, loss = 0.08920865505933762
iteration 79, loss = 0.10759551078081131
iteration 80, loss = 0.08983433246612549
iteration 81, loss = 0.10157984495162964
iteration 82, loss = 0.1093115359544754
iteration 83, loss = 0.14008945226669312
iteration 84, loss = 0.09981140494346619
iteration 85, loss = 0.11626143753528595
iteration 86, loss = 0.10522674769163132
iteration 87, loss = 0.09829867631196976
iteration 88, loss = 0.09953419864177704
iteration 89, loss = 0.10761797428131104
iteration 90, loss = 0.08706837892532349
iteration 91, loss = 0.09956230223178864
iteration 92, loss = 0.08888684213161469
iteration 93, loss = 0.09469550848007202
iteration 94, loss = 0.08708611130714417
iteration 95, loss = 0.07990853488445282
iteration 96, loss = 0.10570289194583893
iteration 97, loss = 0.0887683629989624
iteration 98, loss = 0.11233635246753693
iteration 99, loss = 0.08138012886047363
iteration 100, loss = 0.10416748374700546
iteration 101, loss = 0.09444358944892883
iteration 102, loss = 0.08036647737026215
iteration 103, loss = 0.08843804150819778
iteration 104, loss = 0.08584924042224884
iteration 105, loss = 0.10005880892276764
iteration 106, loss = 0.10651980340480804
iteration 107, loss = 0.08035268634557724
iteration 108, loss = 0.1440441757440567
iteration 109, loss = 0.09924522042274475
iteration 110, loss = 0.10623331367969513
iteration 111, loss = 0.06667644530534744
iteration 112, loss = 0.11638276278972626
iteration 113, loss = 0.09600353240966797
iteration 114, loss = 0.09950699657201767
iteration 115, loss = 0.11828397959470749
iteration 116, loss = 0.1100342869758606
iteration 117, loss = 0.08589272201061249
iteration 118, loss = 0.08196110278367996
iteration 119, loss = 0.09711676836013794
iteration 120, loss = 0.11797007918357849
iteration 121, loss = 0.08573679625988007
iteration 122, loss = 0.09254325926303864
iteration 123, loss = 0.09820444136857986
iteration 124, loss = 0.07883086800575256
iteration 125, loss = 0.08582989126443863
iteration 126, loss = 0.09005697816610336
iteration 127, loss = 0.1024860143661499
iteration 128, loss = 0.11963358521461487
iteration 129, loss = 0.08196749538183212
iteration 130, loss = 0.12095008045434952
iteration 131, loss = 0.08228503167629242
iteration 132, loss = 0.1016106978058815
iteration 133, loss = 0.0907178446650505
iteration 134, loss = 0.11355280876159668
iteration 135, loss = 0.08044303953647614
iteration 136, loss = 0.09461727738380432
iteration 137, loss = 0.0918375700712204
iteration 138, loss = 0.09727922081947327
iteration 139, loss = 0.13123176991939545
iteration 140, loss = 0.09340380877256393
iteration 141, loss = 0.08676929026842117
iteration 142, loss = 0.09986232221126556
iteration 143, loss = 0.0953407734632492
iteration 144, loss = 0.09427811205387115
iteration 145, loss = 0.11131098121404648
iteration 146, loss = 0.07887490093708038
iteration 147, loss = 0.10332595556974411
iteration 148, loss = 0.12039295583963394
iteration 149, loss = 0.08956566452980042
iteration 150, loss = 0.10127668082714081
iteration 151, loss = 0.10223706811666489
iteration 152, loss = 0.09320355951786041
iteration 153, loss = 0.08769358694553375
iteration 154, loss = 0.11739832162857056
iteration 155, loss = 0.08737153559923172
iteration 156, loss = 0.07225070893764496
iteration 157, loss = 0.07918628305196762
iteration 158, loss = 0.09025773406028748
iteration 159, loss = 0.10852614045143127
iteration 160, loss = 0.09738493710756302
iteration 161, loss = 0.11028976738452911
iteration 162, loss = 0.10427737236022949
iteration 163, loss = 0.09895513951778412
iteration 164, loss = 0.10078269243240356
iteration 165, loss = 0.08843293786048889
iteration 166, loss = 0.0944066047668457
iteration 167, loss = 0.09304624795913696
iteration 168, loss = 0.0773642510175705
iteration 169, loss = 0.07937948405742645
iteration 170, loss = 0.09278762340545654
iteration 171, loss = 0.09002585709095001
iteration 172, loss = 0.09113109111785889
iteration 173, loss = 0.11266370862722397
iteration 174, loss = 0.09205217659473419
iteration 175, loss = 0.11680318415164948
iteration 176, loss = 0.0901033952832222
iteration 177, loss = 0.08622537553310394
iteration 178, loss = 0.07644934952259064
iteration 179, loss = 0.07780229300260544
iteration 180, loss = 0.0925251692533493
iteration 181, loss = 0.07776705175638199
iteration 182, loss = 0.07909376919269562
iteration 183, loss = 0.09063905477523804
iteration 184, loss = 0.08600830286741257
iteration 185, loss = 0.11942819505929947
iteration 186, loss = 0.08047723025083542
iteration 187, loss = 0.1073046326637268
iteration 188, loss = 0.08630585670471191
iteration 189, loss = 0.09399480372667313
iteration 190, loss = 0.09570294618606567
iteration 191, loss = 0.08546297997236252
iteration 192, loss = 0.10837598145008087
iteration 193, loss = 0.06774240732192993
iteration 194, loss = 0.14478176832199097
iteration 195, loss = 0.07248062640428543
iteration 196, loss = 0.08872538059949875
iteration 197, loss = 0.11569015681743622
iteration 198, loss = 0.08329115808010101
iteration 199, loss = 0.10103064775466919
iteration 200, loss = 0.08073703944683075
iteration 201, loss = 0.0802399218082428
iteration 202, loss = 0.07128001749515533
iteration 203, loss = 0.0836690366268158
iteration 204, loss = 0.11340909451246262
iteration 205, loss = 0.07680261135101318
iteration 206, loss = 0.10026532411575317
iteration 207, loss = 0.08028580993413925
iteration 208, loss = 0.07312984019517899
iteration 209, loss = 0.08301334083080292
iteration 210, loss = 0.09151717275381088
iteration 211, loss = 0.08695077151060104
iteration 212, loss = 0.0716991052031517
iteration 213, loss = 0.09841214120388031
iteration 214, loss = 0.10683485120534897
iteration 215, loss = 0.07299666106700897
iteration 216, loss = 0.09775283932685852
iteration 217, loss = 0.06595423072576523
iteration 218, loss = 0.08929358422756195
iteration 219, loss = 0.07367540150880814
iteration 220, loss = 0.10148119926452637
iteration 221, loss = 0.08731627464294434
iteration 222, loss = 0.10609486699104309
iteration 223, loss = 0.07661198824644089
iteration 224, loss = 0.0880909264087677
iteration 225, loss = 0.08239451050758362
iteration 226, loss = 0.10675062239170074
iteration 227, loss = 0.09193596243858337
iteration 228, loss = 0.10036885738372803
iteration 229, loss = 0.07581856846809387
iteration 230, loss = 0.09614938497543335
iteration 231, loss = 0.07843557745218277
iteration 232, loss = 0.09374643117189407
iteration 233, loss = 0.11006596684455872
iteration 234, loss = 0.09548968076705933
iteration 235, loss = 0.09430429339408875
iteration 236, loss = 0.0951794683933258
iteration 237, loss = 0.08217138051986694
iteration 238, loss = 0.07633087038993835
iteration 239, loss = 0.0766790360212326
iteration 240, loss = 0.08131543546915054
iteration 241, loss = 0.10168389230966568
iteration 242, loss = 0.07708338648080826
iteration 243, loss = 0.08882525563240051
iteration 244, loss = 0.06994767487049103
iteration 245, loss = 0.07789555191993713
iteration 246, loss = 0.10132327675819397
iteration 247, loss = 0.09701796621084213
iteration 248, loss = 0.08384730666875839
iteration 249, loss = 0.10998097062110901
iteration 250, loss = 0.06974206119775772
iteration 251, loss = 0.06934568285942078
iteration 252, loss = 0.0941123217344284
iteration 253, loss = 0.09222369641065598
iteration 254, loss = 0.08083834499120712
iteration 255, loss = 0.07274346798658371
iteration 256, loss = 0.10259607434272766
iteration 257, loss = 0.07823282480239868
iteration 258, loss = 0.10080015659332275
iteration 259, loss = 0.08984082192182541
iteration 260, loss = 0.0859728753566742
iteration 261, loss = 0.1078493744134903
iteration 262, loss = 0.09799958765506744
iteration 263, loss = 0.08573592454195023
iteration 264, loss = 0.07491171360015869
iteration 265, loss = 0.0888071283698082
iteration 266, loss = 0.0773300975561142
iteration 267, loss = 0.07684701681137085
iteration 268, loss = 0.07240250706672668
iteration 269, loss = 0.08341588079929352
iteration 270, loss = 0.06184070184826851
iteration 271, loss = 0.06666615605354309
iteration 272, loss = 0.0774959996342659
iteration 273, loss = 0.08914150297641754
iteration 274, loss = 0.07743096351623535
iteration 275, loss = 0.08717679977416992
iteration 276, loss = 0.08174743503332138
iteration 277, loss = 0.06995440274477005
iteration 278, loss = 0.08394312113523483
iteration 279, loss = 0.07195809483528137
iteration 280, loss = 0.05977394059300423
iteration 281, loss = 0.09963029623031616
iteration 282, loss = 0.07378006726503372
iteration 283, loss = 0.09676926583051682
iteration 284, loss = 0.10122864693403244
iteration 285, loss = 0.10168933868408203
iteration 286, loss = 0.08145009726285934
iteration 287, loss = 0.08174990862607956
iteration 288, loss = 0.10136745870113373
iteration 289, loss = 0.06935650110244751
iteration 290, loss = 0.07222472876310349
iteration 291, loss = 0.06970414519309998
iteration 292, loss = 0.11454284191131592
iteration 293, loss = 0.07851368933916092
iteration 294, loss = 0.08534681797027588
iteration 295, loss = 0.10074552893638611
iteration 296, loss = 0.1160748228430748
iteration 297, loss = 0.08138583600521088
iteration 298, loss = 0.09817569702863693
iteration 299, loss = 0.0699959397315979
iteration 0, loss = 0.08285122364759445
iteration 1, loss = 0.06593434512615204
iteration 2, loss = 0.0770321637392044
iteration 3, loss = 0.0784137025475502
iteration 4, loss = 0.08470681309700012
iteration 5, loss = 0.07831574976444244
iteration 6, loss = 0.0713559091091156
iteration 7, loss = 0.09843020141124725
iteration 8, loss = 0.08942139148712158
iteration 9, loss = 0.08783436566591263
iteration 10, loss = 0.0821545422077179
iteration 11, loss = 0.08589787036180496
iteration 12, loss = 0.11250564455986023
iteration 13, loss = 0.0989544689655304
iteration 14, loss = 0.06919918954372406
iteration 15, loss = 0.06809356063604355
iteration 16, loss = 0.0980270728468895
iteration 17, loss = 0.07372172921895981
iteration 18, loss = 0.07639089971780777
iteration 19, loss = 0.07884211838245392
iteration 20, loss = 0.10303930938243866
iteration 21, loss = 0.07521987706422806
iteration 22, loss = 0.09675976634025574
iteration 23, loss = 0.0922248363494873
iteration 24, loss = 0.07080383598804474
iteration 25, loss = 0.06490862369537354
iteration 26, loss = 0.10042832046747208
iteration 27, loss = 0.07912644743919373
iteration 28, loss = 0.06674228608608246
iteration 29, loss = 0.07513221353292465
iteration 30, loss = 0.07283686101436615
iteration 31, loss = 0.10312141478061676
iteration 32, loss = 0.08118925243616104
iteration 33, loss = 0.08542854338884354
iteration 34, loss = 0.08939172327518463
iteration 35, loss = 0.0813523530960083
iteration 36, loss = 0.08211581408977509
iteration 37, loss = 0.06377368420362473
iteration 38, loss = 0.07489539682865143
iteration 39, loss = 0.05877019464969635
iteration 40, loss = 0.08896305412054062
iteration 41, loss = 0.08223508298397064
iteration 42, loss = 0.06727425754070282
iteration 43, loss = 0.1016174927353859
iteration 44, loss = 0.06899853050708771
iteration 45, loss = 0.06742843240499496
iteration 46, loss = 0.08196377009153366
iteration 47, loss = 0.06874065101146698
iteration 48, loss = 0.09403161704540253
iteration 49, loss = 0.07210353761911392
iteration 50, loss = 0.11002768576145172
iteration 51, loss = 0.09234224259853363
iteration 52, loss = 0.09752733260393143
iteration 53, loss = 0.07147414982318878
iteration 54, loss = 0.07875318080186844
iteration 55, loss = 0.08555614948272705
iteration 56, loss = 0.06378044933080673
iteration 57, loss = 0.07251159101724625
iteration 58, loss = 0.07269030809402466
iteration 59, loss = 0.06906870007514954
iteration 60, loss = 0.08203696459531784
iteration 61, loss = 0.08455787599086761
iteration 62, loss = 0.08181177079677582
iteration 63, loss = 0.06984623521566391
iteration 64, loss = 0.08240167796611786
iteration 65, loss = 0.07887445390224457
iteration 66, loss = 0.06428493559360504
iteration 67, loss = 0.0839170292019844
iteration 68, loss = 0.0708487331867218
iteration 69, loss = 0.06408457458019257
iteration 70, loss = 0.09273169934749603
iteration 71, loss = 0.0707547515630722
iteration 72, loss = 0.0784090906381607
iteration 73, loss = 0.0681767612695694
iteration 74, loss = 0.07487764954566956
iteration 75, loss = 0.10143502056598663
iteration 76, loss = 0.07212487608194351
iteration 77, loss = 0.07692621648311615
iteration 78, loss = 0.08178795129060745
iteration 79, loss = 0.0578937754034996
iteration 80, loss = 0.11500923335552216
iteration 81, loss = 0.0654473826289177
iteration 82, loss = 0.09894128888845444
iteration 83, loss = 0.06823237240314484
iteration 84, loss = 0.07397321611642838
iteration 85, loss = 0.08245574682950974
iteration 86, loss = 0.06289273500442505
iteration 87, loss = 0.07461181282997131
iteration 88, loss = 0.08135142922401428
iteration 89, loss = 0.11595308780670166
iteration 90, loss = 0.07720369845628738
iteration 91, loss = 0.06902879476547241
iteration 92, loss = 0.07628107070922852
iteration 93, loss = 0.06952549517154694
iteration 94, loss = 0.07861780375242233
iteration 95, loss = 0.06537988036870956
iteration 96, loss = 0.08227158337831497
iteration 97, loss = 0.08635607361793518
iteration 98, loss = 0.06609684228897095
iteration 99, loss = 0.06014684960246086
iteration 100, loss = 0.06211952120065689
iteration 101, loss = 0.09711383283138275
iteration 102, loss = 0.0680469423532486
iteration 103, loss = 0.08228430151939392
iteration 104, loss = 0.07987921684980392
iteration 105, loss = 0.08746940642595291
iteration 106, loss = 0.08166895061731339
iteration 107, loss = 0.08232993632555008
iteration 108, loss = 0.08216112852096558
iteration 109, loss = 0.08863118290901184
iteration 110, loss = 0.08564422279596329
iteration 111, loss = 0.08405810594558716
iteration 112, loss = 0.07750516384840012
iteration 113, loss = 0.07458539307117462
iteration 114, loss = 0.07070384174585342
iteration 115, loss = 0.08012311905622482
iteration 116, loss = 0.07361097633838654
iteration 117, loss = 0.07908204197883606
iteration 118, loss = 0.09724802523851395
iteration 119, loss = 0.08750633895397186
iteration 120, loss = 0.0652005672454834
iteration 121, loss = 0.08232428133487701
iteration 122, loss = 0.06486380100250244
iteration 123, loss = 0.06656844913959503
iteration 124, loss = 0.07228735834360123
iteration 125, loss = 0.0662011131644249
iteration 126, loss = 0.08577536791563034
iteration 127, loss = 0.07044661790132523
iteration 128, loss = 0.10416728258132935
iteration 129, loss = 0.08382920920848846
iteration 130, loss = 0.08106251060962677
iteration 131, loss = 0.09844300150871277
iteration 132, loss = 0.07416711747646332
iteration 133, loss = 0.06124938279390335
iteration 134, loss = 0.06642411649227142
iteration 135, loss = 0.061735473573207855
iteration 136, loss = 0.06159905344247818
iteration 137, loss = 0.06506507843732834
iteration 138, loss = 0.096690833568573
iteration 139, loss = 0.08558831363916397
iteration 140, loss = 0.07063037902116776
iteration 141, loss = 0.08192899078130722
iteration 142, loss = 0.08230817317962646
iteration 143, loss = 0.08036147803068161
iteration 144, loss = 0.07698202133178711
iteration 145, loss = 0.06564006209373474
iteration 146, loss = 0.06282757222652435
iteration 147, loss = 0.07932806760072708
iteration 148, loss = 0.08521918207406998
iteration 149, loss = 0.06688667833805084
iteration 150, loss = 0.05974830687046051
iteration 151, loss = 0.08936215192079544
iteration 152, loss = 0.06279388070106506
iteration 153, loss = 0.0773836001753807
iteration 154, loss = 0.07342854142189026
iteration 155, loss = 0.0752859115600586
iteration 156, loss = 0.07996490597724915
iteration 157, loss = 0.06660890579223633
iteration 158, loss = 0.05505048856139183
iteration 159, loss = 0.07942446321249008
iteration 160, loss = 0.06741055101156235
iteration 161, loss = 0.07393782585859299
iteration 162, loss = 0.054776743054389954
iteration 163, loss = 0.07375852763652802
iteration 164, loss = 0.06699572503566742
iteration 165, loss = 0.07727061957120895
iteration 166, loss = 0.0792556181550026
iteration 167, loss = 0.07076694071292877
iteration 168, loss = 0.07871513068675995
iteration 169, loss = 0.06304313987493515
iteration 170, loss = 0.05254290997982025
iteration 171, loss = 0.06732258200645447
iteration 172, loss = 0.05706411227583885
iteration 173, loss = 0.07537831366062164
iteration 174, loss = 0.093754842877388
iteration 175, loss = 0.06517543643712997
iteration 176, loss = 0.08325210213661194
iteration 177, loss = 0.06163022294640541
iteration 178, loss = 0.06991827487945557
iteration 179, loss = 0.06353338062763214
iteration 180, loss = 0.06029953062534332
iteration 181, loss = 0.07768454402685165
iteration 182, loss = 0.08536075800657272
iteration 183, loss = 0.08502927422523499
iteration 184, loss = 0.08113320171833038
iteration 185, loss = 0.06126706302165985
iteration 186, loss = 0.06413004547357559
iteration 187, loss = 0.06913420557975769
iteration 188, loss = 0.07918798178434372
iteration 189, loss = 0.06483405828475952
iteration 190, loss = 0.07474832981824875
iteration 191, loss = 0.06514682620763779
iteration 192, loss = 0.06894120573997498
iteration 193, loss = 0.07241615653038025
iteration 194, loss = 0.06400340050458908
iteration 195, loss = 0.07338257133960724
iteration 196, loss = 0.10334208607673645
iteration 197, loss = 0.055467672646045685
iteration 198, loss = 0.07582395523786545
iteration 199, loss = 0.06074849143624306
iteration 200, loss = 0.06854180246591568
iteration 201, loss = 0.05230943486094475
iteration 202, loss = 0.06065819784998894
iteration 203, loss = 0.07051204890012741
iteration 204, loss = 0.07346107810735703
iteration 205, loss = 0.0667259693145752
iteration 206, loss = 0.09925701469182968
iteration 207, loss = 0.06153173744678497
iteration 208, loss = 0.07498643547296524
iteration 209, loss = 0.08003666996955872
iteration 210, loss = 0.05663887411355972
iteration 211, loss = 0.08215498924255371
iteration 212, loss = 0.06955581903457642
iteration 213, loss = 0.08110947161912918
iteration 214, loss = 0.06420740485191345
iteration 215, loss = 0.07564347237348557
iteration 216, loss = 0.06044968217611313
iteration 217, loss = 0.05808957666158676
iteration 218, loss = 0.05431602895259857
iteration 219, loss = 0.05910469591617584
iteration 220, loss = 0.07434343546628952
iteration 221, loss = 0.0687616690993309
iteration 222, loss = 0.06579537689685822
iteration 223, loss = 0.0620560348033905
iteration 224, loss = 0.07937905192375183
iteration 225, loss = 0.08676399290561676
iteration 226, loss = 0.06878849118947983
iteration 227, loss = 0.05675332993268967
iteration 228, loss = 0.0734020248055458
iteration 229, loss = 0.0790429413318634
iteration 230, loss = 0.06707093119621277
iteration 231, loss = 0.07270700484514236
iteration 232, loss = 0.06469810754060745
iteration 233, loss = 0.05859379842877388
iteration 234, loss = 0.08979979157447815
iteration 235, loss = 0.07039465755224228
iteration 236, loss = 0.05614667385816574
iteration 237, loss = 0.06911545246839523
iteration 238, loss = 0.06983545422554016
iteration 239, loss = 0.07932037860155106
iteration 240, loss = 0.06481775641441345
iteration 241, loss = 0.06617623567581177
iteration 242, loss = 0.06676919013261795
iteration 243, loss = 0.055134158581495285
iteration 244, loss = 0.0753471702337265
iteration 245, loss = 0.05851190164685249
iteration 246, loss = 0.08762025833129883
iteration 247, loss = 0.09125547111034393
iteration 248, loss = 0.0736001506447792
iteration 249, loss = 0.06087827682495117
iteration 250, loss = 0.07143671810626984
iteration 251, loss = 0.07901285588741302
iteration 252, loss = 0.08773647248744965
iteration 253, loss = 0.07203838974237442
iteration 254, loss = 0.07625718414783478
iteration 255, loss = 0.05639797821640968
iteration 256, loss = 0.05779825150966644
iteration 257, loss = 0.05559339374303818
iteration 258, loss = 0.0748513862490654
iteration 259, loss = 0.08616551756858826
iteration 260, loss = 0.07983987778425217
iteration 261, loss = 0.059863317757844925
iteration 262, loss = 0.07506832480430603
iteration 263, loss = 0.06506054103374481
iteration 264, loss = 0.054152682423591614
iteration 265, loss = 0.07030902802944183
iteration 266, loss = 0.0718485489487648
iteration 267, loss = 0.062022723257541656
iteration 268, loss = 0.05750252306461334
iteration 269, loss = 0.06037825718522072
iteration 270, loss = 0.06788908690214157
iteration 271, loss = 0.04995635151863098
iteration 272, loss = 0.07563747465610504
iteration 273, loss = 0.08000872284173965
iteration 274, loss = 0.05912256985902786
iteration 275, loss = 0.06171252205967903
iteration 276, loss = 0.0726117417216301
iteration 277, loss = 0.07991775125265121
iteration 278, loss = 0.08320720493793488
iteration 279, loss = 0.0666879266500473
iteration 280, loss = 0.08687054365873337
iteration 281, loss = 0.060852959752082825
iteration 282, loss = 0.06144709885120392
iteration 283, loss = 0.08876235783100128
iteration 284, loss = 0.05468360334634781
iteration 285, loss = 0.0779334157705307
iteration 286, loss = 0.07373951375484467
iteration 287, loss = 0.047848206013441086
iteration 288, loss = 0.05814598500728607
iteration 289, loss = 0.07068035751581192
iteration 290, loss = 0.07619919627904892
iteration 291, loss = 0.075131356716156
iteration 292, loss = 0.09942034631967545
iteration 293, loss = 0.05641615390777588
iteration 294, loss = 0.05370461195707321
iteration 295, loss = 0.06864332407712936
iteration 296, loss = 0.06276646256446838
iteration 297, loss = 0.07949790358543396
iteration 298, loss = 0.0544380322098732
iteration 299, loss = 0.07576011121273041
iteration 0, loss = 0.08507894724607468
iteration 1, loss = 0.06813577562570572
iteration 2, loss = 0.07101782411336899
iteration 3, loss = 0.05877716839313507
iteration 4, loss = 0.09629561007022858
iteration 5, loss = 0.06324877589941025
iteration 6, loss = 0.06885912269353867
iteration 7, loss = 0.045497097074985504
iteration 8, loss = 0.07096774131059647
iteration 9, loss = 0.06635195761919022
iteration 10, loss = 0.04644273221492767
iteration 11, loss = 0.07665405422449112
iteration 12, loss = 0.0600346103310585
iteration 13, loss = 0.06652416288852692
iteration 14, loss = 0.057855598628520966
iteration 15, loss = 0.05115925893187523
iteration 16, loss = 0.05861670523881912
iteration 17, loss = 0.06619523465633392
iteration 18, loss = 0.05544636771082878
iteration 19, loss = 0.06371907889842987
iteration 20, loss = 0.059259459376335144
iteration 21, loss = 0.0674288272857666
iteration 22, loss = 0.07891284674406052
iteration 23, loss = 0.0707346573472023
iteration 24, loss = 0.06746107339859009
iteration 25, loss = 0.08877646178007126
iteration 26, loss = 0.06470682471990585
iteration 27, loss = 0.08064466714859009
iteration 28, loss = 0.08215651661157608
iteration 29, loss = 0.05629391223192215
iteration 30, loss = 0.060872774571180344
iteration 31, loss = 0.07172980904579163
iteration 32, loss = 0.05401485040783882
iteration 33, loss = 0.0528116375207901
iteration 34, loss = 0.05480857193470001
iteration 35, loss = 0.0619778074324131
iteration 36, loss = 0.05034619942307472
iteration 37, loss = 0.06506878137588501
iteration 38, loss = 0.0829668641090393
iteration 39, loss = 0.0786481499671936
iteration 40, loss = 0.06520289927721024
iteration 41, loss = 0.050453778356313705
iteration 42, loss = 0.05906124413013458
iteration 43, loss = 0.05872906744480133
iteration 44, loss = 0.07367774099111557
iteration 45, loss = 0.06922587007284164
iteration 46, loss = 0.05860688164830208
iteration 47, loss = 0.059095598757267
iteration 48, loss = 0.06415970623493195
iteration 49, loss = 0.08291319757699966
iteration 50, loss = 0.04808661714196205
iteration 51, loss = 0.06602874398231506
iteration 52, loss = 0.06070494279265404
iteration 53, loss = 0.061847276985645294
iteration 54, loss = 0.06079978495836258
iteration 55, loss = 0.06536072492599487
iteration 56, loss = 0.05634225904941559
iteration 57, loss = 0.05135715752840042
iteration 58, loss = 0.05587922781705856
iteration 59, loss = 0.059804242104291916
iteration 60, loss = 0.07017837464809418
iteration 61, loss = 0.04521366208791733
iteration 62, loss = 0.06278879195451736
iteration 63, loss = 0.06592322885990143
iteration 64, loss = 0.0521133691072464
iteration 65, loss = 0.07697473466396332
iteration 66, loss = 0.05335530638694763
iteration 67, loss = 0.049996353685855865
iteration 68, loss = 0.07227998226881027
iteration 69, loss = 0.049379847943782806
iteration 70, loss = 0.06532333046197891
iteration 71, loss = 0.06150646507740021
iteration 72, loss = 0.07397852838039398
iteration 73, loss = 0.08594328165054321
iteration 74, loss = 0.07592117786407471
iteration 75, loss = 0.057954709976911545
iteration 76, loss = 0.04920061677694321
iteration 77, loss = 0.06683037430047989
iteration 78, loss = 0.0628482848405838
iteration 79, loss = 0.05548767000436783
iteration 80, loss = 0.060653019696474075
iteration 81, loss = 0.06869865953922272
iteration 82, loss = 0.04820461571216583
iteration 83, loss = 0.04626161605119705
iteration 84, loss = 0.06158492714166641
iteration 85, loss = 0.0670999214053154
iteration 86, loss = 0.04955814406275749
iteration 87, loss = 0.050491444766521454
iteration 88, loss = 0.05065593123435974
iteration 89, loss = 0.07371285557746887
iteration 90, loss = 0.059417352080345154
iteration 91, loss = 0.07612276822328568
iteration 92, loss = 0.055735740810632706
iteration 93, loss = 0.0541609451174736
iteration 94, loss = 0.05364833027124405
iteration 95, loss = 0.07959631830453873
iteration 96, loss = 0.057425081729888916
iteration 97, loss = 0.055696673691272736
iteration 98, loss = 0.051170967519283295
iteration 99, loss = 0.04357972368597984
iteration 100, loss = 0.04781946539878845
iteration 101, loss = 0.05880335718393326
iteration 102, loss = 0.05520978942513466
iteration 103, loss = 0.09250098466873169
iteration 104, loss = 0.052201639860868454
iteration 105, loss = 0.0762234777212143
iteration 106, loss = 0.06645374000072479
iteration 107, loss = 0.07315554469823837
iteration 108, loss = 0.05966827645897865
iteration 109, loss = 0.0584205761551857
iteration 110, loss = 0.08374232798814774
iteration 111, loss = 0.052178703248500824
iteration 112, loss = 0.0677502453327179
iteration 113, loss = 0.060654640197753906
iteration 114, loss = 0.054327819496393204
iteration 115, loss = 0.0442943274974823
iteration 116, loss = 0.06613116711378098
iteration 117, loss = 0.07635331153869629
iteration 118, loss = 0.053945355117321014
iteration 119, loss = 0.07586027681827545
iteration 120, loss = 0.04234859347343445
iteration 121, loss = 0.06192832067608833
iteration 122, loss = 0.06931771337985992
iteration 123, loss = 0.0555172860622406
iteration 124, loss = 0.07108300924301147
iteration 125, loss = 0.04873105511069298
iteration 126, loss = 0.06501410901546478
iteration 127, loss = 0.06850124895572662
iteration 128, loss = 0.059597261250019073
iteration 129, loss = 0.04936276748776436
iteration 130, loss = 0.04384756088256836
iteration 131, loss = 0.06083882227540016
iteration 132, loss = 0.0602354034781456
iteration 133, loss = 0.056412361562252045
iteration 134, loss = 0.05042204633355141
iteration 135, loss = 0.05730213597416878
iteration 136, loss = 0.0443691611289978
iteration 137, loss = 0.06531871110200882
iteration 138, loss = 0.06420682370662689
iteration 139, loss = 0.05547824501991272
iteration 140, loss = 0.06836310029029846
iteration 141, loss = 0.04325314611196518
iteration 142, loss = 0.07400871068239212
iteration 143, loss = 0.05699798837304115
iteration 144, loss = 0.05596834421157837
iteration 145, loss = 0.06961468607187271
iteration 146, loss = 0.06241372227668762
iteration 147, loss = 0.07027897238731384
iteration 148, loss = 0.0639217346906662
iteration 149, loss = 0.07711591571569443
iteration 150, loss = 0.04927084967494011
iteration 151, loss = 0.04477128013968468
iteration 152, loss = 0.051854681223630905
iteration 153, loss = 0.08507880568504333
iteration 154, loss = 0.0727425143122673
iteration 155, loss = 0.05113965645432472
iteration 156, loss = 0.0626254677772522
iteration 157, loss = 0.06046975404024124
iteration 158, loss = 0.06544125825166702
iteration 159, loss = 0.07134118676185608
iteration 160, loss = 0.053289975970983505
iteration 161, loss = 0.05240098759531975
iteration 162, loss = 0.06496191024780273
iteration 163, loss = 0.05589178204536438
iteration 164, loss = 0.043763455003499985
iteration 165, loss = 0.041593506932258606
iteration 166, loss = 0.057837728410959244
iteration 167, loss = 0.0679144412279129
iteration 168, loss = 0.05422413349151611
iteration 169, loss = 0.04250282049179077
iteration 170, loss = 0.04346935823559761
iteration 171, loss = 0.05189626291394234
iteration 172, loss = 0.059337109327316284
iteration 173, loss = 0.055017802864313126
iteration 174, loss = 0.05715782940387726
iteration 175, loss = 0.06351245939731598
iteration 176, loss = 0.06705839186906815
iteration 177, loss = 0.05073720961809158
iteration 178, loss = 0.054289840161800385
iteration 179, loss = 0.04814484342932701
iteration 180, loss = 0.04870002344250679
iteration 181, loss = 0.046957217156887054
iteration 182, loss = 0.04845396429300308
iteration 183, loss = 0.05728521570563316
iteration 184, loss = 0.06155212223529816
iteration 185, loss = 0.040528859943151474
iteration 186, loss = 0.046613603830337524
iteration 187, loss = 0.060469016432762146
iteration 188, loss = 0.05890234559774399
iteration 189, loss = 0.06591733545064926
iteration 190, loss = 0.0571649968624115
iteration 191, loss = 0.05200953781604767
iteration 192, loss = 0.07329659909009933
iteration 193, loss = 0.06424041092395782
iteration 194, loss = 0.050937630236148834
iteration 195, loss = 0.053298577666282654
iteration 196, loss = 0.08396141976118088
iteration 197, loss = 0.050981372594833374
iteration 198, loss = 0.06491653621196747
iteration 199, loss = 0.05320814251899719
iteration 200, loss = 0.06752327084541321
iteration 201, loss = 0.057243429124355316
iteration 202, loss = 0.05853684991598129
iteration 203, loss = 0.07304342091083527
iteration 204, loss = 0.061704978346824646
iteration 205, loss = 0.046991415321826935
iteration 206, loss = 0.04800228402018547
iteration 207, loss = 0.06221489608287811
iteration 208, loss = 0.04332621768116951
iteration 209, loss = 0.052569083869457245
iteration 210, loss = 0.06597279757261276
iteration 211, loss = 0.06066319718956947
iteration 212, loss = 0.044340167194604874
iteration 213, loss = 0.06206866726279259
iteration 214, loss = 0.05646444857120514
iteration 215, loss = 0.03565578907728195
iteration 216, loss = 0.045085109770298004
iteration 217, loss = 0.07799776643514633
iteration 218, loss = 0.05430631339550018
iteration 219, loss = 0.0626441240310669
iteration 220, loss = 0.044588543474674225
iteration 221, loss = 0.055729687213897705
iteration 222, loss = 0.06506069004535675
iteration 223, loss = 0.06554561108350754
iteration 224, loss = 0.06156865507364273
iteration 225, loss = 0.05124970152974129
iteration 226, loss = 0.06092047691345215
iteration 227, loss = 0.04500799998641014
iteration 228, loss = 0.06557735055685043
iteration 229, loss = 0.05777096748352051
iteration 230, loss = 0.05874594300985336
iteration 231, loss = 0.05965995788574219
iteration 232, loss = 0.05747818574309349
iteration 233, loss = 0.055137909948825836
iteration 234, loss = 0.06174185872077942
iteration 235, loss = 0.0899864211678505
iteration 236, loss = 0.07484079152345657
iteration 237, loss = 0.06135669723153114
iteration 238, loss = 0.0494452603161335
iteration 239, loss = 0.056269269436597824
iteration 240, loss = 0.06231614202260971
iteration 241, loss = 0.05393986776471138
iteration 242, loss = 0.07805101573467255
iteration 243, loss = 0.08829998970031738
iteration 244, loss = 0.06522020697593689
iteration 245, loss = 0.04034911096096039
iteration 246, loss = 0.0546322837471962
iteration 247, loss = 0.05096593499183655
iteration 248, loss = 0.0765574648976326
iteration 249, loss = 0.08940701186656952
iteration 250, loss = 0.04584670066833496
iteration 251, loss = 0.043607741594314575
iteration 252, loss = 0.054355647414922714
iteration 253, loss = 0.048063863068819046
iteration 254, loss = 0.04361148178577423
iteration 255, loss = 0.0630948469042778
iteration 256, loss = 0.047561224550008774
iteration 257, loss = 0.0397983193397522
iteration 258, loss = 0.05957604944705963
iteration 259, loss = 0.06852994114160538
iteration 260, loss = 0.05555655062198639
iteration 261, loss = 0.07430453598499298
iteration 262, loss = 0.05102928727865219
iteration 263, loss = 0.045058198273181915
iteration 264, loss = 0.06698384881019592
iteration 265, loss = 0.08033829927444458
iteration 266, loss = 0.04915616661310196
iteration 267, loss = 0.05600397288799286
iteration 268, loss = 0.04801056161522865
iteration 269, loss = 0.048469752073287964
iteration 270, loss = 0.04398147761821747
iteration 271, loss = 0.058745209127664566
iteration 272, loss = 0.050175201147794724
iteration 273, loss = 0.05630093812942505
iteration 274, loss = 0.06404551863670349
iteration 275, loss = 0.05141480267047882
iteration 276, loss = 0.03970540314912796
iteration 277, loss = 0.054084934294223785
iteration 278, loss = 0.07784059643745422
iteration 279, loss = 0.052338652312755585
iteration 280, loss = 0.06354112178087234
iteration 281, loss = 0.046546436846256256
iteration 282, loss = 0.040567390620708466
iteration 283, loss = 0.05097154155373573
iteration 284, loss = 0.04880424216389656
iteration 285, loss = 0.05297427624464035
iteration 286, loss = 0.06281957030296326
iteration 287, loss = 0.0717993751168251
iteration 288, loss = 0.049297530204057693
iteration 289, loss = 0.07269273698329926
iteration 290, loss = 0.044915296137332916
iteration 291, loss = 0.05815687030553818
iteration 292, loss = 0.05189428851008415
iteration 293, loss = 0.05434035137295723
iteration 294, loss = 0.05216434970498085
iteration 295, loss = 0.03977438807487488
iteration 296, loss = 0.04762345179915428
iteration 297, loss = 0.05110307037830353
iteration 298, loss = 0.03885321319103241
iteration 299, loss = 0.04835497587919235
iteration 0, loss = 0.048551663756370544
iteration 1, loss = 0.0479915551841259
iteration 2, loss = 0.04180282726883888
iteration 3, loss = 0.06302838027477264
iteration 4, loss = 0.048554353415966034
iteration 5, loss = 0.0678379088640213
iteration 6, loss = 0.0438782162964344
iteration 7, loss = 0.07049189507961273
iteration 8, loss = 0.07382258027791977
iteration 9, loss = 0.051609210669994354
iteration 10, loss = 0.05351581424474716
iteration 11, loss = 0.05164608731865883
iteration 12, loss = 0.05071011185646057
iteration 13, loss = 0.047389738261699677
iteration 14, loss = 0.048802800476551056
iteration 15, loss = 0.042452067136764526
iteration 16, loss = 0.03591316565871239
iteration 17, loss = 0.06032705307006836
iteration 18, loss = 0.046787701547145844
iteration 19, loss = 0.03721880540251732
iteration 20, loss = 0.04568023979663849
iteration 21, loss = 0.03781075403094292
iteration 22, loss = 0.053102992475032806
iteration 23, loss = 0.04843619838356972
iteration 24, loss = 0.05848032236099243
iteration 25, loss = 0.05521160736680031
iteration 26, loss = 0.05617717653512955
iteration 27, loss = 0.04893674701452255
iteration 28, loss = 0.046390023082494736
iteration 29, loss = 0.049260493367910385
iteration 30, loss = 0.043720439076423645
iteration 31, loss = 0.05573287233710289
iteration 32, loss = 0.03300907462835312
iteration 33, loss = 0.060271814465522766
iteration 34, loss = 0.06779155135154724
iteration 35, loss = 0.05301879346370697
iteration 36, loss = 0.04947303980588913
iteration 37, loss = 0.06403838098049164
iteration 38, loss = 0.06450194120407104
iteration 39, loss = 0.04517519474029541
iteration 40, loss = 0.039648354053497314
iteration 41, loss = 0.04222703352570534
iteration 42, loss = 0.06487110257148743
iteration 43, loss = 0.045867837965488434
iteration 44, loss = 0.06140148267149925
iteration 45, loss = 0.06950394064188004
iteration 46, loss = 0.04532403498888016
iteration 47, loss = 0.05415058881044388
iteration 48, loss = 0.050231657922267914
iteration 49, loss = 0.053749848157167435
iteration 50, loss = 0.040872666984796524
iteration 51, loss = 0.04352087154984474
iteration 52, loss = 0.052903108298778534
iteration 53, loss = 0.053282685577869415
iteration 54, loss = 0.05685337632894516
iteration 55, loss = 0.05450323224067688
iteration 56, loss = 0.04846174642443657
iteration 57, loss = 0.049543894827365875
iteration 58, loss = 0.08204147964715958
iteration 59, loss = 0.043874021619558334
iteration 60, loss = 0.05248243734240532
iteration 61, loss = 0.05789855867624283
iteration 62, loss = 0.04615440219640732
iteration 63, loss = 0.041672930121421814
iteration 64, loss = 0.048214711248874664
iteration 65, loss = 0.05352135747671127
iteration 66, loss = 0.05421782284975052
iteration 67, loss = 0.051782820373773575
iteration 68, loss = 0.054419826716184616
iteration 69, loss = 0.09751425683498383
iteration 70, loss = 0.04158005490899086
iteration 71, loss = 0.03347284719347954
iteration 72, loss = 0.03946613892912865
iteration 73, loss = 0.04457349330186844
iteration 74, loss = 0.04108436405658722
iteration 75, loss = 0.040979500859975815
iteration 76, loss = 0.05139140412211418
iteration 77, loss = 0.05602515488862991
iteration 78, loss = 0.05245206877589226
iteration 79, loss = 0.05020315945148468
iteration 80, loss = 0.039162442088127136
iteration 81, loss = 0.06553062051534653
iteration 82, loss = 0.05184796079993248
iteration 83, loss = 0.051794134080410004
iteration 84, loss = 0.043152760714292526
iteration 85, loss = 0.04249285161495209
iteration 86, loss = 0.041878484189510345
iteration 87, loss = 0.04772688075900078
iteration 88, loss = 0.05715158209204674
iteration 89, loss = 0.04114069044589996
iteration 90, loss = 0.05117239058017731
iteration 91, loss = 0.04164816066622734
iteration 92, loss = 0.04544585943222046
iteration 93, loss = 0.04503731057047844
iteration 94, loss = 0.04258858412504196
iteration 95, loss = 0.05223245918750763
iteration 96, loss = 0.06414398550987244
iteration 97, loss = 0.06196364015340805
iteration 98, loss = 0.043187398463487625
iteration 99, loss = 0.05800073593854904
iteration 100, loss = 0.04774763435125351
iteration 101, loss = 0.05305260047316551
iteration 102, loss = 0.06791006028652191
iteration 103, loss = 0.04492469131946564
iteration 104, loss = 0.04712224379181862
iteration 105, loss = 0.038951992988586426
iteration 106, loss = 0.04828326031565666
iteration 107, loss = 0.061756737530231476
iteration 108, loss = 0.0539553128182888
iteration 109, loss = 0.060175880789756775
iteration 110, loss = 0.051839664578437805
iteration 111, loss = 0.05675084516406059
iteration 112, loss = 0.04125833138823509
iteration 113, loss = 0.04356854036450386
iteration 114, loss = 0.05512708052992821
iteration 115, loss = 0.0469689778983593
iteration 116, loss = 0.045321106910705566
iteration 117, loss = 0.06013674661517143
iteration 118, loss = 0.04764508455991745
iteration 119, loss = 0.04167091101408005
iteration 120, loss = 0.04414782300591469
iteration 121, loss = 0.04467694088816643
iteration 122, loss = 0.04414273798465729
iteration 123, loss = 0.039397843182086945
iteration 124, loss = 0.06783260405063629
iteration 125, loss = 0.04030643403530121
iteration 126, loss = 0.05524611100554466
iteration 127, loss = 0.043274447321891785
iteration 128, loss = 0.05066005140542984
iteration 129, loss = 0.05467654764652252
iteration 130, loss = 0.055083151906728745
iteration 131, loss = 0.05948677286505699
iteration 132, loss = 0.05567125231027603
iteration 133, loss = 0.049316998571157455
iteration 134, loss = 0.07887375354766846
iteration 135, loss = 0.04358752816915512
iteration 136, loss = 0.04818339645862579
iteration 137, loss = 0.05420602113008499
iteration 138, loss = 0.03697595000267029
iteration 139, loss = 0.04452364891767502
iteration 140, loss = 0.05093201994895935
iteration 141, loss = 0.06508529931306839
iteration 142, loss = 0.04851846396923065
iteration 143, loss = 0.04352119565010071
iteration 144, loss = 0.045932479202747345
iteration 145, loss = 0.03663548082113266
iteration 146, loss = 0.05258052051067352
iteration 147, loss = 0.04625498130917549
iteration 148, loss = 0.04408599063754082
iteration 149, loss = 0.05065383017063141
iteration 150, loss = 0.05213407799601555
iteration 151, loss = 0.045120369642972946
iteration 152, loss = 0.040500979870557785
iteration 153, loss = 0.036889441311359406
iteration 154, loss = 0.05775010958313942
iteration 155, loss = 0.05507859215140343
iteration 156, loss = 0.04929441958665848
iteration 157, loss = 0.0460752472281456
iteration 158, loss = 0.0507289282977581
iteration 159, loss = 0.05367373302578926
iteration 160, loss = 0.03962790220975876
iteration 161, loss = 0.06516145914793015
iteration 162, loss = 0.047195836901664734
iteration 163, loss = 0.04708625376224518
iteration 164, loss = 0.06673863530158997
iteration 165, loss = 0.038201749324798584
iteration 166, loss = 0.0440792590379715
iteration 167, loss = 0.05703624337911606
iteration 168, loss = 0.04224763438105583
iteration 169, loss = 0.046026986092329025
iteration 170, loss = 0.05003289133310318
iteration 171, loss = 0.03400145471096039
iteration 172, loss = 0.05069086328148842
iteration 173, loss = 0.0535459965467453
iteration 174, loss = 0.04119761288166046
iteration 175, loss = 0.04784635454416275
iteration 176, loss = 0.046306923031806946
iteration 177, loss = 0.05022413656115532
iteration 178, loss = 0.04762507602572441
iteration 179, loss = 0.04309522360563278
iteration 180, loss = 0.042763423174619675
iteration 181, loss = 0.06719178706407547
iteration 182, loss = 0.05339431017637253
iteration 183, loss = 0.06402826309204102
iteration 184, loss = 0.038091178983449936
iteration 185, loss = 0.04004618152976036
iteration 186, loss = 0.037706851959228516
iteration 187, loss = 0.0571523979306221
iteration 188, loss = 0.03880063444375992
iteration 189, loss = 0.048712097108364105
iteration 190, loss = 0.03978973254561424
iteration 191, loss = 0.05086338892579079
iteration 192, loss = 0.0412086546421051
iteration 193, loss = 0.043301135301589966
iteration 194, loss = 0.0787278339266777
iteration 195, loss = 0.0553324893116951
iteration 196, loss = 0.045567866414785385
iteration 197, loss = 0.054515331983566284
iteration 198, loss = 0.06269523501396179
iteration 199, loss = 0.03690371289849281
iteration 200, loss = 0.04452580586075783
iteration 201, loss = 0.06404299288988113
iteration 202, loss = 0.034015122801065445
iteration 203, loss = 0.06709932535886765
iteration 204, loss = 0.044899068772792816
iteration 205, loss = 0.06063585355877876
iteration 206, loss = 0.042004141956567764
iteration 207, loss = 0.035661060363054276
iteration 208, loss = 0.04378456249833107
iteration 209, loss = 0.04979740083217621
iteration 210, loss = 0.04775835573673248
iteration 211, loss = 0.03488533943891525
iteration 212, loss = 0.03932707756757736
iteration 213, loss = 0.042136527597904205
iteration 214, loss = 0.03900532424449921
iteration 215, loss = 0.05247441679239273
iteration 216, loss = 0.053638167679309845
iteration 217, loss = 0.041637323796749115
iteration 218, loss = 0.03245731070637703
iteration 219, loss = 0.04906085878610611
iteration 220, loss = 0.03883221000432968
iteration 221, loss = 0.0430130660533905
iteration 222, loss = 0.04505154490470886
iteration 223, loss = 0.051598988473415375
iteration 224, loss = 0.038776010274887085
iteration 225, loss = 0.050941772758960724
iteration 226, loss = 0.03673332929611206
iteration 227, loss = 0.054321352392435074
iteration 228, loss = 0.050568837672472
iteration 229, loss = 0.04741072654724121
iteration 230, loss = 0.05431634187698364
iteration 231, loss = 0.04234054684638977
iteration 232, loss = 0.03899538516998291
iteration 233, loss = 0.04330930486321449
iteration 234, loss = 0.06255049258470535
iteration 235, loss = 0.044984277337789536
iteration 236, loss = 0.0326278954744339
iteration 237, loss = 0.04043447971343994
iteration 238, loss = 0.037659574300050735
iteration 239, loss = 0.033005621284246445
iteration 240, loss = 0.04150880128145218
iteration 241, loss = 0.0512041300535202
iteration 242, loss = 0.0402771532535553
iteration 243, loss = 0.052531104534864426
iteration 244, loss = 0.03135572001338005
iteration 245, loss = 0.04707987606525421
iteration 246, loss = 0.058122165501117706
iteration 247, loss = 0.0714995414018631
iteration 248, loss = 0.041893936693668365
iteration 249, loss = 0.035723790526390076
iteration 250, loss = 0.03767342492938042
iteration 251, loss = 0.05453963205218315
iteration 252, loss = 0.048632122576236725
iteration 253, loss = 0.0487028993666172
iteration 254, loss = 0.03428279608488083
iteration 255, loss = 0.03228107839822769
iteration 256, loss = 0.03486381098628044
iteration 257, loss = 0.038978222757577896
iteration 258, loss = 0.04647134616971016
iteration 259, loss = 0.03855132311582565
iteration 260, loss = 0.052669573575258255
iteration 261, loss = 0.05335767939686775
iteration 262, loss = 0.03164808824658394
iteration 263, loss = 0.048850927501916885
iteration 264, loss = 0.04407908767461777
iteration 265, loss = 0.05795511230826378
iteration 266, loss = 0.03042318858206272
iteration 267, loss = 0.0723288357257843
iteration 268, loss = 0.05064042657613754
iteration 269, loss = 0.04331681504845619
iteration 270, loss = 0.035671163350343704
iteration 271, loss = 0.033803246915340424
iteration 272, loss = 0.04822086542844772
iteration 273, loss = 0.03916033357381821
iteration 274, loss = 0.054811030626297
iteration 275, loss = 0.03595888987183571
iteration 276, loss = 0.02686343714594841
iteration 277, loss = 0.05399712175130844
iteration 278, loss = 0.048709578812122345
iteration 279, loss = 0.038362111896276474
iteration 280, loss = 0.04465481638908386
iteration 281, loss = 0.034545812755823135
iteration 282, loss = 0.0633249282836914
iteration 283, loss = 0.03578739985823631
iteration 284, loss = 0.05034390836954117
iteration 285, loss = 0.034430790692567825
iteration 286, loss = 0.03446784242987633
iteration 287, loss = 0.03609081357717514
iteration 288, loss = 0.03710796684026718
iteration 289, loss = 0.036416810005903244
iteration 290, loss = 0.036855973303318024
iteration 291, loss = 0.04685415327548981
iteration 292, loss = 0.03404882177710533
iteration 293, loss = 0.046741098165512085
iteration 294, loss = 0.05124936252832413
iteration 295, loss = 0.058528754860162735
iteration 296, loss = 0.04337877035140991
iteration 297, loss = 0.03275180235505104
iteration 298, loss = 0.04161962494254112
iteration 299, loss = 0.04435904696583748
iteration 0, loss = 0.037210892885923386
iteration 1, loss = 0.03663163259625435
iteration 2, loss = 0.041473712772130966
iteration 3, loss = 0.0366412028670311
iteration 4, loss = 0.041519246995449066
iteration 5, loss = 0.03862335532903671
iteration 6, loss = 0.04480302333831787
iteration 7, loss = 0.05366070196032524
iteration 8, loss = 0.030110176652669907
iteration 9, loss = 0.04124518483877182
iteration 10, loss = 0.04082414507865906
iteration 11, loss = 0.037881895899772644
iteration 12, loss = 0.05123678594827652
iteration 13, loss = 0.03632816672325134
iteration 14, loss = 0.04614832252264023
iteration 15, loss = 0.056087713688611984
iteration 16, loss = 0.038477227091789246
iteration 17, loss = 0.061122309416532516
iteration 18, loss = 0.053113631904125214
iteration 19, loss = 0.039213065057992935
iteration 20, loss = 0.04113614186644554
iteration 21, loss = 0.034865062683820724
iteration 22, loss = 0.04114792123436928
iteration 23, loss = 0.048469334840774536
iteration 24, loss = 0.04334494471549988
iteration 25, loss = 0.03915804624557495
iteration 26, loss = 0.04254215955734253
iteration 27, loss = 0.034031931310892105
iteration 28, loss = 0.05518558993935585
iteration 29, loss = 0.04159839078783989
iteration 30, loss = 0.05003483593463898
iteration 31, loss = 0.036889806389808655
iteration 32, loss = 0.03618553653359413
iteration 33, loss = 0.036544762551784515
iteration 34, loss = 0.03992113843560219
iteration 35, loss = 0.04805058240890503
iteration 36, loss = 0.045158904045820236
iteration 37, loss = 0.06130225583910942
iteration 38, loss = 0.03308585658669472
iteration 39, loss = 0.03183846175670624
iteration 40, loss = 0.04405813664197922
iteration 41, loss = 0.03806803375482559
iteration 42, loss = 0.0496843121945858
iteration 43, loss = 0.044186610728502274
iteration 44, loss = 0.03692812845110893
iteration 45, loss = 0.04769444465637207
iteration 46, loss = 0.04003533720970154
iteration 47, loss = 0.031491659581661224
iteration 48, loss = 0.053622420877218246
iteration 49, loss = 0.028672389686107635
iteration 50, loss = 0.03953559696674347
iteration 51, loss = 0.04334809631109238
iteration 52, loss = 0.03256342560052872
iteration 53, loss = 0.035161685198545456
iteration 54, loss = 0.05232122540473938
iteration 55, loss = 0.03837205469608307
iteration 56, loss = 0.03299222141504288
iteration 57, loss = 0.03635850548744202
iteration 58, loss = 0.03627439960837364
iteration 59, loss = 0.03232262283563614
iteration 60, loss = 0.03342553228139877
iteration 61, loss = 0.032412365078926086
iteration 62, loss = 0.04194021597504616
iteration 63, loss = 0.04330368712544441
iteration 64, loss = 0.03484379127621651
iteration 65, loss = 0.04214141517877579
iteration 66, loss = 0.037245288491249084
iteration 67, loss = 0.037026770412921906
iteration 68, loss = 0.039973631501197815
iteration 69, loss = 0.03483900800347328
iteration 70, loss = 0.04021221399307251
iteration 71, loss = 0.042365189641714096
iteration 72, loss = 0.029882140457630157
iteration 73, loss = 0.0525510236620903
iteration 74, loss = 0.03937295451760292
iteration 75, loss = 0.027745090425014496
iteration 76, loss = 0.0307813361287117
iteration 77, loss = 0.04248442128300667
iteration 78, loss = 0.04840023070573807
iteration 79, loss = 0.05335994437336922
iteration 80, loss = 0.03804410994052887
iteration 81, loss = 0.02898181788623333
iteration 82, loss = 0.06077227368950844
iteration 83, loss = 0.04410676285624504
iteration 84, loss = 0.0394727997481823
iteration 85, loss = 0.03533119708299637
iteration 86, loss = 0.05062786117196083
iteration 87, loss = 0.05878844112157822
iteration 88, loss = 0.041736703366041183
iteration 89, loss = 0.043572742491960526
iteration 90, loss = 0.03267129510641098
iteration 91, loss = 0.04580366984009743
iteration 92, loss = 0.069929338991642
iteration 93, loss = 0.028018856421113014
iteration 94, loss = 0.0410248339176178
iteration 95, loss = 0.03665464371442795
iteration 96, loss = 0.05189020559191704
iteration 97, loss = 0.029147256165742874
iteration 98, loss = 0.05417656525969505
iteration 99, loss = 0.055515289306640625
iteration 100, loss = 0.035840071737766266
iteration 101, loss = 0.038838859647512436
iteration 102, loss = 0.032320037484169006
iteration 103, loss = 0.04891971871256828
iteration 104, loss = 0.0288399625569582
iteration 105, loss = 0.04278815537691116
iteration 106, loss = 0.04638555645942688
iteration 107, loss = 0.03526761382818222
iteration 108, loss = 0.032743558287620544
iteration 109, loss = 0.05848568305373192
iteration 110, loss = 0.03307734429836273
iteration 111, loss = 0.04588784649968147
iteration 112, loss = 0.03438981622457504
iteration 113, loss = 0.041151583194732666
iteration 114, loss = 0.05559339374303818
iteration 115, loss = 0.03626173734664917
iteration 116, loss = 0.04307004436850548
iteration 117, loss = 0.03514523059129715
iteration 118, loss = 0.04849541187286377
iteration 119, loss = 0.03984351083636284
iteration 120, loss = 0.041101571172475815
iteration 121, loss = 0.03599681705236435
iteration 122, loss = 0.0521513856947422
iteration 123, loss = 0.034290146082639694
iteration 124, loss = 0.03315684571862221
iteration 125, loss = 0.0518067441880703
iteration 126, loss = 0.05324050039052963
iteration 127, loss = 0.03236537426710129
iteration 128, loss = 0.042335499078035355
iteration 129, loss = 0.04429059848189354
iteration 130, loss = 0.04222128540277481
iteration 131, loss = 0.05621672421693802
iteration 132, loss = 0.03064780682325363
iteration 133, loss = 0.04907090216875076
iteration 134, loss = 0.033126574009656906
iteration 135, loss = 0.04773899167776108
iteration 136, loss = 0.04731326922774315
iteration 137, loss = 0.04181727021932602
iteration 138, loss = 0.04204988107085228
iteration 139, loss = 0.056429725140333176
iteration 140, loss = 0.04854521155357361
iteration 141, loss = 0.04404443874955177
iteration 142, loss = 0.03663152456283569
iteration 143, loss = 0.025608588010072708
iteration 144, loss = 0.027358032763004303
iteration 145, loss = 0.032808367162942886
iteration 146, loss = 0.041860565543174744
iteration 147, loss = 0.05476497858762741
iteration 148, loss = 0.04757692664861679
iteration 149, loss = 0.027430472895503044
iteration 150, loss = 0.038366567343473434
iteration 151, loss = 0.030537720769643784
iteration 152, loss = 0.04364964738488197
iteration 153, loss = 0.03036736696958542
iteration 154, loss = 0.06766901910305023
iteration 155, loss = 0.02834172174334526
iteration 156, loss = 0.059636328369379044
iteration 157, loss = 0.05675315484404564
iteration 158, loss = 0.03232948109507561
iteration 159, loss = 0.04865209758281708
iteration 160, loss = 0.03034166432917118
iteration 161, loss = 0.029655642807483673
iteration 162, loss = 0.03286612033843994
iteration 163, loss = 0.04953102767467499
iteration 164, loss = 0.034466829150915146
iteration 165, loss = 0.03332198038697243
iteration 166, loss = 0.04781677573919296
iteration 167, loss = 0.04583251103758812
iteration 168, loss = 0.044464632868766785
iteration 169, loss = 0.038540199398994446
iteration 170, loss = 0.03831597417593002
iteration 171, loss = 0.0353056900203228
iteration 172, loss = 0.04985101521015167
iteration 173, loss = 0.03188211843371391
iteration 174, loss = 0.051389072090387344
iteration 175, loss = 0.04062763974070549
iteration 176, loss = 0.04234011098742485
iteration 177, loss = 0.029030971229076385
iteration 178, loss = 0.04223866015672684
iteration 179, loss = 0.03379499912261963
iteration 180, loss = 0.03567129373550415
iteration 181, loss = 0.03858482092618942
iteration 182, loss = 0.042806945741176605
iteration 183, loss = 0.042948830872774124
iteration 184, loss = 0.039750583469867706
iteration 185, loss = 0.04827149584889412
iteration 186, loss = 0.04410308226943016
iteration 187, loss = 0.02711748331785202
iteration 188, loss = 0.03173491358757019
iteration 189, loss = 0.0381338968873024
iteration 190, loss = 0.03490019217133522
iteration 191, loss = 0.028218306601047516
iteration 192, loss = 0.046494290232658386
iteration 193, loss = 0.029618779197335243
iteration 194, loss = 0.03105097822844982
iteration 195, loss = 0.04353693127632141
iteration 196, loss = 0.04597091302275658
iteration 197, loss = 0.04675281420350075
iteration 198, loss = 0.03257829323410988
iteration 199, loss = 0.043588828295469284
iteration 200, loss = 0.04509931430220604
iteration 201, loss = 0.04250815510749817
iteration 202, loss = 0.043888695538043976
iteration 203, loss = 0.03289983049035072
iteration 204, loss = 0.03490297123789787
iteration 205, loss = 0.06436169892549515
iteration 206, loss = 0.03089267760515213
iteration 207, loss = 0.02780093066394329
iteration 208, loss = 0.045236796140670776
iteration 209, loss = 0.03429030254483223
iteration 210, loss = 0.045240744948387146
iteration 211, loss = 0.03873994201421738
iteration 212, loss = 0.0484929159283638
iteration 213, loss = 0.02815285511314869
iteration 214, loss = 0.056049905717372894
iteration 215, loss = 0.02893991768360138
iteration 216, loss = 0.03347589820623398
iteration 217, loss = 0.04109029471874237
iteration 218, loss = 0.03278317674994469
iteration 219, loss = 0.026510361582040787
iteration 220, loss = 0.02804391458630562
iteration 221, loss = 0.04956230893731117
iteration 222, loss = 0.02941865846514702
iteration 223, loss = 0.033289194107055664
iteration 224, loss = 0.030389009043574333
iteration 225, loss = 0.033228952437639236
iteration 226, loss = 0.029120134189724922
iteration 227, loss = 0.032795581966638565
iteration 228, loss = 0.029927363619208336
iteration 229, loss = 0.03260183706879616
iteration 230, loss = 0.027017781510949135
iteration 231, loss = 0.04589465260505676
iteration 232, loss = 0.030320866033434868
iteration 233, loss = 0.03889099881052971
iteration 234, loss = 0.029839683324098587
iteration 235, loss = 0.034467488527297974
iteration 236, loss = 0.04297912120819092
iteration 237, loss = 0.03993895277380943
iteration 238, loss = 0.060667604207992554
iteration 239, loss = 0.036296866834163666
iteration 240, loss = 0.025431865826249123
iteration 241, loss = 0.03463944420218468
iteration 242, loss = 0.03614845126867294
iteration 243, loss = 0.04677437245845795
iteration 244, loss = 0.03716228902339935
iteration 245, loss = 0.03857780247926712
iteration 246, loss = 0.03484008461236954
iteration 247, loss = 0.028333693742752075
iteration 248, loss = 0.036337390542030334
iteration 249, loss = 0.05404962599277496
iteration 250, loss = 0.03370150178670883
iteration 251, loss = 0.028824323788285255
iteration 252, loss = 0.05368885397911072
iteration 253, loss = 0.05190316215157509
iteration 254, loss = 0.03654744476079941
iteration 255, loss = 0.03643297776579857
iteration 256, loss = 0.02735152840614319
iteration 257, loss = 0.039330944418907166
iteration 258, loss = 0.03022024966776371
iteration 259, loss = 0.04324036091566086
iteration 260, loss = 0.03914441540837288
iteration 261, loss = 0.030872108414769173
iteration 262, loss = 0.040839239954948425
iteration 263, loss = 0.030559953302145004
iteration 264, loss = 0.03925453871488571
iteration 265, loss = 0.036698952317237854
iteration 266, loss = 0.035801567137241364
iteration 267, loss = 0.026540283113718033
iteration 268, loss = 0.02859959751367569
iteration 269, loss = 0.037683989852666855
iteration 270, loss = 0.046981051564216614
iteration 271, loss = 0.028072478249669075
iteration 272, loss = 0.03496246039867401
iteration 273, loss = 0.036801308393478394
iteration 274, loss = 0.0350789874792099
iteration 275, loss = 0.03503744304180145
iteration 276, loss = 0.026574520394206047
iteration 277, loss = 0.055141836404800415
iteration 278, loss = 0.03162091225385666
iteration 279, loss = 0.03440701588988304
iteration 280, loss = 0.028189726173877716
iteration 281, loss = 0.05310438573360443
iteration 282, loss = 0.033886268734931946
iteration 283, loss = 0.027089396491646767
iteration 284, loss = 0.027553781867027283
iteration 285, loss = 0.033578112721443176
iteration 286, loss = 0.05449657142162323
iteration 287, loss = 0.037145864218473434
iteration 288, loss = 0.046249788254499435
iteration 289, loss = 0.02424863912165165
iteration 290, loss = 0.02836151048541069
iteration 291, loss = 0.029652900993824005
iteration 292, loss = 0.034166887402534485
iteration 293, loss = 0.03358837962150574
iteration 294, loss = 0.045005254447460175
iteration 295, loss = 0.05041906237602234
iteration 296, loss = 0.05113419517874718
iteration 297, loss = 0.05025847628712654
iteration 298, loss = 0.03099825233221054
iteration 299, loss = 0.05444103851914406
iteration 0, loss = 0.05726496875286102
iteration 1, loss = 0.028772711753845215
iteration 2, loss = 0.03592634201049805
iteration 3, loss = 0.035171881318092346
iteration 4, loss = 0.050550203770399094
iteration 5, loss = 0.03147294372320175
iteration 6, loss = 0.04240051284432411
iteration 7, loss = 0.03258730098605156
iteration 8, loss = 0.03216952085494995
iteration 9, loss = 0.047919850796461105
iteration 10, loss = 0.027897832915186882
iteration 11, loss = 0.04454714432358742
iteration 12, loss = 0.033546723425388336
iteration 13, loss = 0.03028474934399128
iteration 14, loss = 0.05525892227888107
iteration 15, loss = 0.031596846878528595
iteration 16, loss = 0.02594543807208538
iteration 17, loss = 0.026251738891005516
iteration 18, loss = 0.02970447763800621
iteration 19, loss = 0.030990637838840485
iteration 20, loss = 0.02474898099899292
iteration 21, loss = 0.03756536543369293
iteration 22, loss = 0.04564690589904785
iteration 23, loss = 0.03534657135605812
iteration 24, loss = 0.038926731795072556
iteration 25, loss = 0.05399350821971893
iteration 26, loss = 0.02126188762485981
iteration 27, loss = 0.026303444057703018
iteration 28, loss = 0.038054753094911575
iteration 29, loss = 0.0342467837035656
iteration 30, loss = 0.03509366512298584
iteration 31, loss = 0.0413050577044487
iteration 32, loss = 0.045246053487062454
iteration 33, loss = 0.03811728581786156
iteration 34, loss = 0.03358164429664612
iteration 35, loss = 0.03422907739877701
iteration 36, loss = 0.035033177584409714
iteration 37, loss = 0.022828610613942146
iteration 38, loss = 0.027890656143426895
iteration 39, loss = 0.027342241257429123
iteration 40, loss = 0.029867174103856087
iteration 41, loss = 0.03671954944729805
iteration 42, loss = 0.0502658486366272
iteration 43, loss = 0.0296774972230196
iteration 44, loss = 0.029429897665977478
iteration 45, loss = 0.02502155303955078
iteration 46, loss = 0.03834262490272522
iteration 47, loss = 0.035688355565071106
iteration 48, loss = 0.03279268741607666
iteration 49, loss = 0.029069114476442337
iteration 50, loss = 0.03526274487376213
iteration 51, loss = 0.04101990535855293
iteration 52, loss = 0.05163652449846268
iteration 53, loss = 0.03599030524492264
iteration 54, loss = 0.035453446209430695
iteration 55, loss = 0.02767036110162735
iteration 56, loss = 0.03587886318564415
iteration 57, loss = 0.04375246539711952
iteration 58, loss = 0.046627312898635864
iteration 59, loss = 0.03605363517999649
iteration 60, loss = 0.032386910170316696
iteration 61, loss = 0.03747870773077011
iteration 62, loss = 0.043201424181461334
iteration 63, loss = 0.028908737003803253
iteration 64, loss = 0.0403999499976635
iteration 65, loss = 0.033411964774131775
iteration 66, loss = 0.048402246087789536
iteration 67, loss = 0.03407663851976395
iteration 68, loss = 0.029472503811120987
iteration 69, loss = 0.03883139789104462
iteration 70, loss = 0.034095413982868195
iteration 71, loss = 0.04620647430419922
iteration 72, loss = 0.039276495575904846
iteration 73, loss = 0.033193349838256836
iteration 74, loss = 0.038388628512620926
iteration 75, loss = 0.02765793912112713
iteration 76, loss = 0.04287257045507431
iteration 77, loss = 0.054593104869127274
iteration 78, loss = 0.027219653129577637
iteration 79, loss = 0.0319879911839962
iteration 80, loss = 0.030986536294221878
iteration 81, loss = 0.03914959728717804
iteration 82, loss = 0.027717621996998787
iteration 83, loss = 0.033247023820877075
iteration 84, loss = 0.03438464179635048
iteration 85, loss = 0.03082219883799553
iteration 86, loss = 0.024139339104294777
iteration 87, loss = 0.03719564154744148
iteration 88, loss = 0.03677412495017052
iteration 89, loss = 0.031144538894295692
iteration 90, loss = 0.03692129626870155
iteration 91, loss = 0.036379337310791016
iteration 92, loss = 0.02385655790567398
iteration 93, loss = 0.024535223841667175
iteration 94, loss = 0.05503497272729874
iteration 95, loss = 0.02919742651283741
iteration 96, loss = 0.04778701439499855
iteration 97, loss = 0.034350909292697906
iteration 98, loss = 0.023673515766859055
iteration 99, loss = 0.026832999661564827
iteration 100, loss = 0.035660237073898315
iteration 101, loss = 0.03712395951151848
iteration 102, loss = 0.03131948783993721
iteration 103, loss = 0.042314715683460236
iteration 104, loss = 0.02668805979192257
iteration 105, loss = 0.046432677656412125
iteration 106, loss = 0.025662895292043686
iteration 107, loss = 0.03549908474087715
iteration 108, loss = 0.044461943209171295
iteration 109, loss = 0.03449469804763794
iteration 110, loss = 0.029132751747965813
iteration 111, loss = 0.048285145312547684
iteration 112, loss = 0.03819431737065315
iteration 113, loss = 0.031035002321004868
iteration 114, loss = 0.03455105423927307
iteration 115, loss = 0.031146276742219925
iteration 116, loss = 0.03851575404405594
iteration 117, loss = 0.026804553344845772
iteration 118, loss = 0.033542782068252563
iteration 119, loss = 0.037130992859601974
iteration 120, loss = 0.04178552329540253
iteration 121, loss = 0.03874563053250313
iteration 122, loss = 0.03511054068803787
iteration 123, loss = 0.03378051891922951
iteration 124, loss = 0.038683440536260605
iteration 125, loss = 0.029629060998558998
iteration 126, loss = 0.027569979429244995
iteration 127, loss = 0.023756464943289757
iteration 128, loss = 0.027354607358574867
iteration 129, loss = 0.024675775319337845
iteration 130, loss = 0.03068484365940094
iteration 131, loss = 0.0313170962035656
iteration 132, loss = 0.03920438140630722
iteration 133, loss = 0.03871031478047371
iteration 134, loss = 0.04417070746421814
iteration 135, loss = 0.04764757305383682
iteration 136, loss = 0.05960920825600624
iteration 137, loss = 0.04116375371813774
iteration 138, loss = 0.030297130346298218
iteration 139, loss = 0.04677301272749901
iteration 140, loss = 0.033162921667099
iteration 141, loss = 0.031964655965566635
iteration 142, loss = 0.023749928921461105
iteration 143, loss = 0.029893115162849426
iteration 144, loss = 0.034181710332632065
iteration 145, loss = 0.024787932634353638
iteration 146, loss = 0.025511514395475388
iteration 147, loss = 0.0440809465944767
iteration 148, loss = 0.04268146678805351
iteration 149, loss = 0.031882669776678085
iteration 150, loss = 0.02462591417133808
iteration 151, loss = 0.03809139132499695
iteration 152, loss = 0.029768526554107666
iteration 153, loss = 0.026081588119268417
iteration 154, loss = 0.04646150395274162
iteration 155, loss = 0.03299985080957413
iteration 156, loss = 0.05257079377770424
iteration 157, loss = 0.023206068202853203
iteration 158, loss = 0.03747881203889847
iteration 159, loss = 0.04095954820513725
iteration 160, loss = 0.03546535223722458
iteration 161, loss = 0.03175047039985657
iteration 162, loss = 0.028246039524674416
iteration 163, loss = 0.034900616854429245
iteration 164, loss = 0.02805626392364502
iteration 165, loss = 0.03593278303742409
iteration 166, loss = 0.02945021167397499
iteration 167, loss = 0.026360832154750824
iteration 168, loss = 0.04694858938455582
iteration 169, loss = 0.03544862940907478
iteration 170, loss = 0.030614547431468964
iteration 171, loss = 0.03197397291660309
iteration 172, loss = 0.030152376741170883
iteration 173, loss = 0.028364257887005806
iteration 174, loss = 0.03713346645236015
iteration 175, loss = 0.04324566572904587
iteration 176, loss = 0.03232039883732796
iteration 177, loss = 0.03280921280384064
iteration 178, loss = 0.03593837842345238
iteration 179, loss = 0.03296241909265518
iteration 180, loss = 0.026275936514139175
iteration 181, loss = 0.03192250803112984
iteration 182, loss = 0.043595682829618454
iteration 183, loss = 0.029111139476299286
iteration 184, loss = 0.03800644725561142
iteration 185, loss = 0.03635459020733833
iteration 186, loss = 0.023277102038264275
iteration 187, loss = 0.046968504786491394
iteration 188, loss = 0.0358680859208107
iteration 189, loss = 0.03041176125407219
iteration 190, loss = 0.032179053872823715
iteration 191, loss = 0.024850795045495033
iteration 192, loss = 0.026233643293380737
iteration 193, loss = 0.02931174449622631
iteration 194, loss = 0.03400777280330658
iteration 195, loss = 0.02622576244175434
iteration 196, loss = 0.03618044778704643
iteration 197, loss = 0.02855481579899788
iteration 198, loss = 0.022571265697479248
iteration 199, loss = 0.03263426572084427
iteration 200, loss = 0.025153806433081627
iteration 201, loss = 0.026615340262651443
iteration 202, loss = 0.03656316548585892
iteration 203, loss = 0.027911540120840073
iteration 204, loss = 0.03713575005531311
iteration 205, loss = 0.02418043650686741
iteration 206, loss = 0.032222699373960495
iteration 207, loss = 0.03520648181438446
iteration 208, loss = 0.02379355952143669
iteration 209, loss = 0.03489124774932861
iteration 210, loss = 0.02790403738617897
iteration 211, loss = 0.024869902059435844
iteration 212, loss = 0.035147301852703094
iteration 213, loss = 0.028780724853277206
iteration 214, loss = 0.03197634965181351
iteration 215, loss = 0.030911071226000786
iteration 216, loss = 0.02990846335887909
iteration 217, loss = 0.02495674043893814
iteration 218, loss = 0.023722177371382713
iteration 219, loss = 0.04163207858800888
iteration 220, loss = 0.030273983255028725
iteration 221, loss = 0.02326037362217903
iteration 222, loss = 0.031569965183734894
iteration 223, loss = 0.03606988862156868
iteration 224, loss = 0.02695673145353794
iteration 225, loss = 0.027600303292274475
iteration 226, loss = 0.031880032271146774
iteration 227, loss = 0.027560286223888397
iteration 228, loss = 0.030712025240063667
iteration 229, loss = 0.036788713186979294
iteration 230, loss = 0.029588820412755013
iteration 231, loss = 0.026710467413067818
iteration 232, loss = 0.03071073442697525
iteration 233, loss = 0.038942672312259674
iteration 234, loss = 0.03992239385843277
iteration 235, loss = 0.03269629925489426
iteration 236, loss = 0.02255254238843918
iteration 237, loss = 0.036787763237953186
iteration 238, loss = 0.02493996173143387
iteration 239, loss = 0.03157850727438927
iteration 240, loss = 0.038839492946863174
iteration 241, loss = 0.026875197887420654
iteration 242, loss = 0.03481249138712883
iteration 243, loss = 0.03529845178127289
iteration 244, loss = 0.026390254497528076
iteration 245, loss = 0.05074569955468178
iteration 246, loss = 0.037780310958623886
iteration 247, loss = 0.029014842584729195
iteration 248, loss = 0.026750478893518448
iteration 249, loss = 0.028932422399520874
iteration 250, loss = 0.029079096391797066
iteration 251, loss = 0.027678586542606354
iteration 252, loss = 0.029717830941081047
iteration 253, loss = 0.03477047383785248
iteration 254, loss = 0.022582022473216057
iteration 255, loss = 0.0288382675498724
iteration 256, loss = 0.03157297149300575
iteration 257, loss = 0.02626708894968033
iteration 258, loss = 0.020629912614822388
iteration 259, loss = 0.024580035358667374
iteration 260, loss = 0.04486662149429321
iteration 261, loss = 0.02898269146680832
iteration 262, loss = 0.03135756775736809
iteration 263, loss = 0.02496577985584736
iteration 264, loss = 0.02256600372493267
iteration 265, loss = 0.029837757349014282
iteration 266, loss = 0.029434025287628174
iteration 267, loss = 0.02313072420656681
iteration 268, loss = 0.037349455058574677
iteration 269, loss = 0.03170561045408249
iteration 270, loss = 0.034976761788129807
iteration 271, loss = 0.04556293040513992
iteration 272, loss = 0.024058319628238678
iteration 273, loss = 0.0335601344704628
iteration 274, loss = 0.030677922070026398
iteration 275, loss = 0.0318497009575367
iteration 276, loss = 0.03365422412753105
iteration 277, loss = 0.025781288743019104
iteration 278, loss = 0.03705338388681412
iteration 279, loss = 0.024676594883203506
iteration 280, loss = 0.028970643877983093
iteration 281, loss = 0.028011171147227287
iteration 282, loss = 0.027473438531160355
iteration 283, loss = 0.027458127588033676
iteration 284, loss = 0.03860485926270485
iteration 285, loss = 0.03392350673675537
iteration 286, loss = 0.043111905455589294
iteration 287, loss = 0.020011693239212036
iteration 288, loss = 0.02276473678648472
iteration 289, loss = 0.032423246651887894
iteration 290, loss = 0.025151044130325317
iteration 291, loss = 0.030600568279623985
iteration 292, loss = 0.03535709157586098
iteration 293, loss = 0.02828119695186615
iteration 294, loss = 0.02156725898385048
iteration 295, loss = 0.035154879093170166
iteration 296, loss = 0.02339181676506996
iteration 297, loss = 0.018818065524101257
iteration 298, loss = 0.02029673382639885
iteration 299, loss = 0.032391760498285294
iteration 0, loss = 0.025124618783593178
iteration 1, loss = 0.02956848219037056
iteration 2, loss = 0.025423675775527954
iteration 3, loss = 0.023641325533390045
iteration 4, loss = 0.017169451341032982
iteration 5, loss = 0.03157893568277359
iteration 6, loss = 0.04111660644412041
iteration 7, loss = 0.027713211253285408
iteration 8, loss = 0.026478361338377
iteration 9, loss = 0.034227561205625534
iteration 10, loss = 0.019204551354050636
iteration 11, loss = 0.033052507787942886
iteration 12, loss = 0.022417303174734116
iteration 13, loss = 0.03213496506214142
iteration 14, loss = 0.03998425230383873
iteration 15, loss = 0.024399196729063988
iteration 16, loss = 0.03523537889122963
iteration 17, loss = 0.028437504544854164
iteration 18, loss = 0.029809841886162758
iteration 19, loss = 0.022506099194288254
iteration 20, loss = 0.02199466899037361
iteration 21, loss = 0.03378148749470711
iteration 22, loss = 0.04338788613677025
iteration 23, loss = 0.021460728719830513
iteration 24, loss = 0.037728387862443924
iteration 25, loss = 0.031552571803331375
iteration 26, loss = 0.04096483066678047
iteration 27, loss = 0.03294166177511215
iteration 28, loss = 0.022157687693834305
iteration 29, loss = 0.0547451376914978
iteration 30, loss = 0.034177303314208984
iteration 31, loss = 0.02891812101006508
iteration 32, loss = 0.035332851111888885
iteration 33, loss = 0.02434413507580757
iteration 34, loss = 0.02320546656847
iteration 35, loss = 0.022518709301948547
iteration 36, loss = 0.029430730268359184
iteration 37, loss = 0.025161731988191605
iteration 38, loss = 0.02748997136950493
iteration 39, loss = 0.029356762766838074
iteration 40, loss = 0.029258765280246735
iteration 41, loss = 0.029027583077549934
iteration 42, loss = 0.03745424002408981
iteration 43, loss = 0.0431118942797184
iteration 44, loss = 0.03191906213760376
iteration 45, loss = 0.028176365420222282
iteration 46, loss = 0.02425208128988743
iteration 47, loss = 0.04346888139843941
iteration 48, loss = 0.04359613358974457
iteration 49, loss = 0.02928861789405346
iteration 50, loss = 0.03594525158405304
iteration 51, loss = 0.027062367647886276
iteration 52, loss = 0.019886674359440804
iteration 53, loss = 0.03457803651690483
iteration 54, loss = 0.033299706876277924
iteration 55, loss = 0.021885443478822708
iteration 56, loss = 0.0478220172226429
iteration 57, loss = 0.037695422768592834
iteration 58, loss = 0.027203191071748734
iteration 59, loss = 0.021294662728905678
iteration 60, loss = 0.02359134703874588
iteration 61, loss = 0.03191112354397774
iteration 62, loss = 0.022127876058220863
iteration 63, loss = 0.028514685109257698
iteration 64, loss = 0.028644602745771408
iteration 65, loss = 0.027283204719424248
iteration 66, loss = 0.040391672402620316
iteration 67, loss = 0.022243034094572067
iteration 68, loss = 0.033979762345552444
iteration 69, loss = 0.03732213377952576
iteration 70, loss = 0.022325513884425163
iteration 71, loss = 0.03610638901591301
iteration 72, loss = 0.02727597765624523
iteration 73, loss = 0.03261483088135719
iteration 74, loss = 0.02184697613120079
iteration 75, loss = 0.036321159452199936
iteration 76, loss = 0.032407309859991074
iteration 77, loss = 0.0283876471221447
iteration 78, loss = 0.026482589542865753
iteration 79, loss = 0.024816062301397324
iteration 80, loss = 0.034782662987709045
iteration 81, loss = 0.03496222570538521
iteration 82, loss = 0.03387940302491188
iteration 83, loss = 0.019521377980709076
iteration 84, loss = 0.030417224392294884
iteration 85, loss = 0.03874136507511139
iteration 86, loss = 0.02855381742119789
iteration 87, loss = 0.02303025871515274
iteration 88, loss = 0.027482502162456512
iteration 89, loss = 0.019931931048631668
iteration 90, loss = 0.019827233627438545
iteration 91, loss = 0.024109840393066406
iteration 92, loss = 0.027544651180505753
iteration 93, loss = 0.02497732639312744
iteration 94, loss = 0.024767905473709106
iteration 95, loss = 0.025227900594472885
iteration 96, loss = 0.027623793110251427
iteration 97, loss = 0.02874574065208435
iteration 98, loss = 0.029775798320770264
iteration 99, loss = 0.02760501019656658
iteration 100, loss = 0.027861258015036583
iteration 101, loss = 0.032319076359272
iteration 102, loss = 0.022824758663773537
iteration 103, loss = 0.024643920361995697
iteration 104, loss = 0.02311055362224579
iteration 105, loss = 0.022312385961413383
iteration 106, loss = 0.03757546842098236
iteration 107, loss = 0.023670759052038193
iteration 108, loss = 0.02705169841647148
iteration 109, loss = 0.020843621343374252
iteration 110, loss = 0.02023075707256794
iteration 111, loss = 0.03091922588646412
iteration 112, loss = 0.03324952349066734
iteration 113, loss = 0.03692227602005005
iteration 114, loss = 0.025607649236917496
iteration 115, loss = 0.023305373266339302
iteration 116, loss = 0.03333592414855957
iteration 117, loss = 0.022452261298894882
iteration 118, loss = 0.030810583382844925
iteration 119, loss = 0.02822437323629856
iteration 120, loss = 0.02182627096772194
iteration 121, loss = 0.028923863545060158
iteration 122, loss = 0.033188845962285995
iteration 123, loss = 0.025275621563196182
iteration 124, loss = 0.022952772676944733
iteration 125, loss = 0.02946382947266102
iteration 126, loss = 0.025749705731868744
iteration 127, loss = 0.026560476049780846
iteration 128, loss = 0.03893410041928291
iteration 129, loss = 0.04038418084383011
iteration 130, loss = 0.02537587098777294
iteration 131, loss = 0.03496905043721199
iteration 132, loss = 0.026417210698127747
iteration 133, loss = 0.0401182547211647
iteration 134, loss = 0.03765735402703285
iteration 135, loss = 0.025117218494415283
iteration 136, loss = 0.026663267984986305
iteration 137, loss = 0.03256303817033768
iteration 138, loss = 0.02510376274585724
iteration 139, loss = 0.029488878324627876
iteration 140, loss = 0.03112829104065895
iteration 141, loss = 0.020883439108729362
iteration 142, loss = 0.03106943517923355
iteration 143, loss = 0.035568952560424805
iteration 144, loss = 0.036801427602767944
iteration 145, loss = 0.042104195803403854
iteration 146, loss = 0.021794987842440605
iteration 147, loss = 0.03447135537862778
iteration 148, loss = 0.018003128468990326
iteration 149, loss = 0.039862390607595444
iteration 150, loss = 0.03936406224966049
iteration 151, loss = 0.02227136678993702
iteration 152, loss = 0.020969584584236145
iteration 153, loss = 0.024827763438224792
iteration 154, loss = 0.026268713176250458
iteration 155, loss = 0.024009734392166138
iteration 156, loss = 0.02768617868423462
iteration 157, loss = 0.027057453989982605
iteration 158, loss = 0.030558055266737938
iteration 159, loss = 0.02767910435795784
iteration 160, loss = 0.040873896330595016
iteration 161, loss = 0.0391218475997448
iteration 162, loss = 0.034284718334674835
iteration 163, loss = 0.02308092825114727
iteration 164, loss = 0.023793796077370644
iteration 165, loss = 0.029545394703745842
iteration 166, loss = 0.020154140889644623
iteration 167, loss = 0.030068300664424896
iteration 168, loss = 0.020626425743103027
iteration 169, loss = 0.024913804605603218
iteration 170, loss = 0.02336306869983673
iteration 171, loss = 0.02273498848080635
iteration 172, loss = 0.01940331980586052
iteration 173, loss = 0.03336912393569946
iteration 174, loss = 0.03713211417198181
iteration 175, loss = 0.03796498849987984
iteration 176, loss = 0.022453373298048973
iteration 177, loss = 0.02183905616402626
iteration 178, loss = 0.033095940947532654
iteration 179, loss = 0.030146365985274315
iteration 180, loss = 0.026047293096780777
iteration 181, loss = 0.02238045260310173
iteration 182, loss = 0.03290993347764015
iteration 183, loss = 0.023090412840247154
iteration 184, loss = 0.026499643921852112
iteration 185, loss = 0.02492900937795639
iteration 186, loss = 0.023441001772880554
iteration 187, loss = 0.03476255387067795
iteration 188, loss = 0.019794750958681107
iteration 189, loss = 0.025199241936206818
iteration 190, loss = 0.040753062814474106
iteration 191, loss = 0.01842106692492962
iteration 192, loss = 0.033926233649253845
iteration 193, loss = 0.03884246200323105
iteration 194, loss = 0.034096866846084595
iteration 195, loss = 0.026409829035401344
iteration 196, loss = 0.02259775996208191
iteration 197, loss = 0.026229433715343475
iteration 198, loss = 0.01944463700056076
iteration 199, loss = 0.02155720256268978
iteration 200, loss = 0.019674718379974365
iteration 201, loss = 0.026947636157274246
iteration 202, loss = 0.029548298567533493
iteration 203, loss = 0.022773168981075287
iteration 204, loss = 0.036264047026634216
iteration 205, loss = 0.02877206914126873
iteration 206, loss = 0.031843677163124084
iteration 207, loss = 0.026333309710025787
iteration 208, loss = 0.022473227232694626
iteration 209, loss = 0.03776019811630249
iteration 210, loss = 0.028313612565398216
iteration 211, loss = 0.02646251581609249
iteration 212, loss = 0.0185408815741539
iteration 213, loss = 0.021218709647655487
iteration 214, loss = 0.01785871386528015
iteration 215, loss = 0.028105784207582474
iteration 216, loss = 0.02468867041170597
iteration 217, loss = 0.04617335647344589
iteration 218, loss = 0.02357240952551365
iteration 219, loss = 0.020808275789022446
iteration 220, loss = 0.01933964714407921
iteration 221, loss = 0.025656504556536674
iteration 222, loss = 0.037455543875694275
iteration 223, loss = 0.03048943541944027
iteration 224, loss = 0.027881555259227753
iteration 225, loss = 0.03284532576799393
iteration 226, loss = 0.024106647819280624
iteration 227, loss = 0.030829261988401413
iteration 228, loss = 0.026713525876402855
iteration 229, loss = 0.02126995660364628
iteration 230, loss = 0.022585611790418625
iteration 231, loss = 0.030417529866099358
iteration 232, loss = 0.024110257625579834
iteration 233, loss = 0.03786461055278778
iteration 234, loss = 0.022674739360809326
iteration 235, loss = 0.024105452001094818
iteration 236, loss = 0.035392917692661285
iteration 237, loss = 0.028489015996456146
iteration 238, loss = 0.034974873065948486
iteration 239, loss = 0.027730999514460564
iteration 240, loss = 0.027202090248465538
iteration 241, loss = 0.03797981142997742
iteration 242, loss = 0.02572602592408657
iteration 243, loss = 0.030739756301045418
iteration 244, loss = 0.02499655820429325
iteration 245, loss = 0.016847344115376472
iteration 246, loss = 0.021917063742876053
iteration 247, loss = 0.0190788134932518
iteration 248, loss = 0.027668537572026253
iteration 249, loss = 0.035566844046115875
iteration 250, loss = 0.026483099907636642
iteration 251, loss = 0.02255781926214695
iteration 252, loss = 0.02373497001826763
iteration 253, loss = 0.02424064464867115
iteration 254, loss = 0.026932692155241966
iteration 255, loss = 0.02223220095038414
iteration 256, loss = 0.024525362998247147
iteration 257, loss = 0.019101738929748535
iteration 258, loss = 0.020504483953118324
iteration 259, loss = 0.034542761743068695
iteration 260, loss = 0.029523365199565887
iteration 261, loss = 0.021026192232966423
iteration 262, loss = 0.023825880140066147
iteration 263, loss = 0.02896301820874214
iteration 264, loss = 0.03145688399672508
iteration 265, loss = 0.027020981535315514
iteration 266, loss = 0.02280188724398613
iteration 267, loss = 0.018697526305913925
iteration 268, loss = 0.02292567491531372
iteration 269, loss = 0.019106555730104446
iteration 270, loss = 0.033823203295469284
iteration 271, loss = 0.021375903859734535
iteration 272, loss = 0.04284113273024559
iteration 273, loss = 0.02524343878030777
iteration 274, loss = 0.03262929245829582
iteration 275, loss = 0.046631090342998505
iteration 276, loss = 0.02699989452958107
iteration 277, loss = 0.026219632476568222
iteration 278, loss = 0.026555795222520828
iteration 279, loss = 0.018429521471261978
iteration 280, loss = 0.025508057326078415
iteration 281, loss = 0.02855418622493744
iteration 282, loss = 0.02619096264243126
iteration 283, loss = 0.02191660925745964
iteration 284, loss = 0.022840626537799835
iteration 285, loss = 0.030512213706970215
iteration 286, loss = 0.018066944554448128
iteration 287, loss = 0.045669082552194595
iteration 288, loss = 0.02455267310142517
iteration 289, loss = 0.01754976250231266
iteration 290, loss = 0.023836268112063408
iteration 291, loss = 0.022412676364183426
iteration 292, loss = 0.021195292472839355
iteration 293, loss = 0.028779027983546257
iteration 294, loss = 0.022661617025732994
iteration 295, loss = 0.03256116062402725
iteration 296, loss = 0.017490902915596962
iteration 297, loss = 0.030084649100899696
iteration 298, loss = 0.038983069360256195
iteration 299, loss = 0.021485719829797745
iteration 0, loss = 0.019684597849845886
iteration 1, loss = 0.02560136653482914
iteration 2, loss = 0.02102472633123398
iteration 3, loss = 0.023076394572854042
iteration 4, loss = 0.031239613890647888
iteration 5, loss = 0.025929417461156845
iteration 6, loss = 0.02461555413901806
iteration 7, loss = 0.02169230207800865
iteration 8, loss = 0.025649797171354294
iteration 9, loss = 0.022035280242562294
iteration 10, loss = 0.021425511687994003
iteration 11, loss = 0.03036569431424141
iteration 12, loss = 0.0243286844342947
iteration 13, loss = 0.020730726420879364
iteration 14, loss = 0.03372789919376373
iteration 15, loss = 0.03208281844854355
iteration 16, loss = 0.03729671239852905
iteration 17, loss = 0.030225245282053947
iteration 18, loss = 0.027319390326738358
iteration 19, loss = 0.023269172757864
iteration 20, loss = 0.01765245944261551
iteration 21, loss = 0.03296764940023422
iteration 22, loss = 0.022347789257764816
iteration 23, loss = 0.03205886110663414
iteration 24, loss = 0.026945095509290695
iteration 25, loss = 0.01749552972614765
iteration 26, loss = 0.035496655851602554
iteration 27, loss = 0.02165820449590683
iteration 28, loss = 0.026366379112005234
iteration 29, loss = 0.046388860791921616
iteration 30, loss = 0.016837134957313538
iteration 31, loss = 0.021338894963264465
iteration 32, loss = 0.016672763973474503
iteration 33, loss = 0.034028105437755585
iteration 34, loss = 0.028811130672693253
iteration 35, loss = 0.026621675118803978
iteration 36, loss = 0.019817620515823364
iteration 37, loss = 0.027201533317565918
iteration 38, loss = 0.031218893826007843
iteration 39, loss = 0.03134758397936821
iteration 40, loss = 0.020306618884205818
iteration 41, loss = 0.02847970277070999
iteration 42, loss = 0.02935400977730751
iteration 43, loss = 0.02892075665295124
iteration 44, loss = 0.026972603052854538
iteration 45, loss = 0.022311506792902946
iteration 46, loss = 0.02653026580810547
iteration 47, loss = 0.01814759336411953
iteration 48, loss = 0.024982817471027374
iteration 49, loss = 0.022678915411233902
iteration 50, loss = 0.03675047308206558
iteration 51, loss = 0.026144377887248993
iteration 52, loss = 0.017795100808143616
iteration 53, loss = 0.02143435925245285
iteration 54, loss = 0.018092947080731392
iteration 55, loss = 0.019757768139243126
iteration 56, loss = 0.019299451261758804
iteration 57, loss = 0.03191830590367317
iteration 58, loss = 0.03273623809218407
iteration 59, loss = 0.03116535395383835
iteration 60, loss = 0.01823773980140686
iteration 61, loss = 0.027536191046237946
iteration 62, loss = 0.03374338522553444
iteration 63, loss = 0.02736971154808998
iteration 64, loss = 0.017070220783352852
iteration 65, loss = 0.02061242051422596
iteration 66, loss = 0.03003070317208767
iteration 67, loss = 0.02693469077348709
iteration 68, loss = 0.029610762372612953
iteration 69, loss = 0.021940656006336212
iteration 70, loss = 0.02069602906703949
iteration 71, loss = 0.028488855808973312
iteration 72, loss = 0.022171270102262497
iteration 73, loss = 0.03585131838917732
iteration 74, loss = 0.02137380838394165
iteration 75, loss = 0.029654204845428467
iteration 76, loss = 0.0300563033670187
iteration 77, loss = 0.016276074573397636
iteration 78, loss = 0.019141167402267456
iteration 79, loss = 0.03405357152223587
iteration 80, loss = 0.027432138100266457
iteration 81, loss = 0.02972595952451229
iteration 82, loss = 0.02039366029202938
iteration 83, loss = 0.023870907723903656
iteration 84, loss = 0.03633736073970795
iteration 85, loss = 0.026777993887662888
iteration 86, loss = 0.022544873878359795
iteration 87, loss = 0.021584264934062958
iteration 88, loss = 0.02383490651845932
iteration 89, loss = 0.020287062972784042
iteration 90, loss = 0.020694108679890633
iteration 91, loss = 0.018887005746364594
iteration 92, loss = 0.031146163120865822
iteration 93, loss = 0.016444947570562363
iteration 94, loss = 0.020491426810622215
iteration 95, loss = 0.022717906162142754
iteration 96, loss = 0.01702849008142948
iteration 97, loss = 0.022364996373653412
iteration 98, loss = 0.03011949174106121
iteration 99, loss = 0.045624908059835434
iteration 100, loss = 0.017525047063827515
iteration 101, loss = 0.017908647656440735
iteration 102, loss = 0.041326854377985
iteration 103, loss = 0.028176985681056976
iteration 104, loss = 0.025595195591449738
iteration 105, loss = 0.024830248206853867
iteration 106, loss = 0.024342499673366547
iteration 107, loss = 0.023009024560451508
iteration 108, loss = 0.02794213593006134
iteration 109, loss = 0.015423104166984558
iteration 110, loss = 0.01864204928278923
iteration 111, loss = 0.02084876410663128
iteration 112, loss = 0.03418443351984024
iteration 113, loss = 0.038123901933431625
iteration 114, loss = 0.01572815328836441
iteration 115, loss = 0.03106110170483589
iteration 116, loss = 0.045588403940200806
iteration 117, loss = 0.02812867984175682
iteration 118, loss = 0.022167030721902847
iteration 119, loss = 0.02586130052804947
iteration 120, loss = 0.020261917263269424
iteration 121, loss = 0.021370498463511467
iteration 122, loss = 0.025994576513767242
iteration 123, loss = 0.02413671463727951
iteration 124, loss = 0.016541622579097748
iteration 125, loss = 0.01881183311343193
iteration 126, loss = 0.017611978575587273
iteration 127, loss = 0.018790312111377716
iteration 128, loss = 0.025597043335437775
iteration 129, loss = 0.03173138201236725
iteration 130, loss = 0.020694810897111893
iteration 131, loss = 0.026014747098088264
iteration 132, loss = 0.016244884580373764
iteration 133, loss = 0.023147329688072205
iteration 134, loss = 0.01837441697716713
iteration 135, loss = 0.019147712737321854
iteration 136, loss = 0.02577359229326248
iteration 137, loss = 0.02345900796353817
iteration 138, loss = 0.018348893150687218
iteration 139, loss = 0.017223872244358063
iteration 140, loss = 0.02054544910788536
iteration 141, loss = 0.026368552818894386
iteration 142, loss = 0.025479290634393692
iteration 143, loss = 0.027410292997956276
iteration 144, loss = 0.03040432557463646
iteration 145, loss = 0.035919804126024246
iteration 146, loss = 0.03935643285512924
iteration 147, loss = 0.01862035132944584
iteration 148, loss = 0.019103357568383217
iteration 149, loss = 0.02860378473997116
iteration 150, loss = 0.01987977884709835
iteration 151, loss = 0.017352428287267685
iteration 152, loss = 0.02342594601213932
iteration 153, loss = 0.025325264781713486
iteration 154, loss = 0.026303546503186226
iteration 155, loss = 0.02790253981947899
iteration 156, loss = 0.025508519262075424
iteration 157, loss = 0.025188874453306198
iteration 158, loss = 0.027111787348985672
iteration 159, loss = 0.02568179741501808
iteration 160, loss = 0.021936796605587006
iteration 161, loss = 0.01947612874209881
iteration 162, loss = 0.021684549748897552
iteration 163, loss = 0.027828359976410866
iteration 164, loss = 0.02391214668750763
iteration 165, loss = 0.028596259653568268
iteration 166, loss = 0.024236097931861877
iteration 167, loss = 0.02342935837805271
iteration 168, loss = 0.025115540251135826
iteration 169, loss = 0.02571786195039749
iteration 170, loss = 0.025245292112231255
iteration 171, loss = 0.021398775279521942
iteration 172, loss = 0.02098904550075531
iteration 173, loss = 0.02357367053627968
iteration 174, loss = 0.014308175072073936
iteration 175, loss = 0.030534125864505768
iteration 176, loss = 0.02581460401415825
iteration 177, loss = 0.024205993860960007
iteration 178, loss = 0.017924968153238297
iteration 179, loss = 0.025196485221385956
iteration 180, loss = 0.01823916845023632
iteration 181, loss = 0.02390388399362564
iteration 182, loss = 0.017718933522701263
iteration 183, loss = 0.024057216942310333
iteration 184, loss = 0.027966272085905075
iteration 185, loss = 0.019384115934371948
iteration 186, loss = 0.025786157697439194
iteration 187, loss = 0.019710540771484375
iteration 188, loss = 0.02694437839090824
iteration 189, loss = 0.02207615226507187
iteration 190, loss = 0.020699340850114822
iteration 191, loss = 0.025355393067002296
iteration 192, loss = 0.026272516697645187
iteration 193, loss = 0.02059565670788288
iteration 194, loss = 0.018255069851875305
iteration 195, loss = 0.02512199617922306
iteration 196, loss = 0.015957510098814964
iteration 197, loss = 0.02831234782934189
iteration 198, loss = 0.025019681081175804
iteration 199, loss = 0.023679442703723907
iteration 200, loss = 0.023299232125282288
iteration 201, loss = 0.01562787964940071
iteration 202, loss = 0.03219260275363922
iteration 203, loss = 0.01616665907204151
iteration 204, loss = 0.03298800066113472
iteration 205, loss = 0.03002673014998436
iteration 206, loss = 0.040798529982566833
iteration 207, loss = 0.045495498925447464
iteration 208, loss = 0.03892764076590538
iteration 209, loss = 0.015244605019688606
iteration 210, loss = 0.036110054701566696
iteration 211, loss = 0.020837008953094482
iteration 212, loss = 0.018554676324129105
iteration 213, loss = 0.031764160841703415
iteration 214, loss = 0.018019357696175575
iteration 215, loss = 0.022380661219358444
iteration 216, loss = 0.015320715494453907
iteration 217, loss = 0.02145606279373169
iteration 218, loss = 0.020805567502975464
iteration 219, loss = 0.01747557893395424
iteration 220, loss = 0.03257698938250542
iteration 221, loss = 0.029309332370758057
iteration 222, loss = 0.026113035157322884
iteration 223, loss = 0.03510449826717377
iteration 224, loss = 0.04191174358129501
iteration 225, loss = 0.014886768534779549
iteration 226, loss = 0.030575823038816452
iteration 227, loss = 0.019367577508091927
iteration 228, loss = 0.02025231160223484
iteration 229, loss = 0.021507233381271362
iteration 230, loss = 0.025357816368341446
iteration 231, loss = 0.025621330365538597
iteration 232, loss = 0.023658426478505135
iteration 233, loss = 0.019287915900349617
iteration 234, loss = 0.024398360401391983
iteration 235, loss = 0.03074301779270172
iteration 236, loss = 0.018374968320131302
iteration 237, loss = 0.024762319400906563
iteration 238, loss = 0.02780861407518387
iteration 239, loss = 0.02186056226491928
iteration 240, loss = 0.024685021489858627
iteration 241, loss = 0.016489583998918533
iteration 242, loss = 0.026713963598012924
iteration 243, loss = 0.02119290642440319
iteration 244, loss = 0.018835628405213356
iteration 245, loss = 0.018711131066083908
iteration 246, loss = 0.02623220533132553
iteration 247, loss = 0.016124838963150978
iteration 248, loss = 0.023387808352708817
iteration 249, loss = 0.0186905600130558
iteration 250, loss = 0.026344921439886093
iteration 251, loss = 0.02926495485007763
iteration 252, loss = 0.01804145611822605
iteration 253, loss = 0.025743504986166954
iteration 254, loss = 0.02197156846523285
iteration 255, loss = 0.017411110922694206
iteration 256, loss = 0.018695374950766563
iteration 257, loss = 0.020267795771360397
iteration 258, loss = 0.03668190538883209
iteration 259, loss = 0.0209142304956913
iteration 260, loss = 0.015469402074813843
iteration 261, loss = 0.02308088354766369
iteration 262, loss = 0.023160789161920547
iteration 263, loss = 0.031235381960868835
iteration 264, loss = 0.019454335793852806
iteration 265, loss = 0.021163910627365112
iteration 266, loss = 0.020191464573144913
iteration 267, loss = 0.02215307205915451
iteration 268, loss = 0.01976153999567032
iteration 269, loss = 0.015219433233141899
iteration 270, loss = 0.023799873888492584
iteration 271, loss = 0.016824891790747643
iteration 272, loss = 0.022028926759958267
iteration 273, loss = 0.019339602440595627
iteration 274, loss = 0.022253554314374924
iteration 275, loss = 0.017728030681610107
iteration 276, loss = 0.027393348515033722
iteration 277, loss = 0.02110251970589161
iteration 278, loss = 0.02463844232261181
iteration 279, loss = 0.019457362592220306
iteration 280, loss = 0.0195446964353323
iteration 281, loss = 0.01705450937151909
iteration 282, loss = 0.01636371947824955
iteration 283, loss = 0.023373132571578026
iteration 284, loss = 0.02320464700460434
iteration 285, loss = 0.017366614192724228
iteration 286, loss = 0.029457245022058487
iteration 287, loss = 0.02890976518392563
iteration 288, loss = 0.016908694058656693
iteration 289, loss = 0.017692238092422485
iteration 290, loss = 0.023642152547836304
iteration 291, loss = 0.01907026208937168
iteration 292, loss = 0.02244596555829048
iteration 293, loss = 0.021822357550263405
iteration 294, loss = 0.016683800145983696
iteration 295, loss = 0.01858186349272728
iteration 296, loss = 0.014879784546792507
iteration 297, loss = 0.03227437287569046
iteration 298, loss = 0.01926756277680397
iteration 299, loss = 0.03209321200847626
iteration 0, loss = 0.020663274452090263
iteration 1, loss = 0.01770796999335289
iteration 2, loss = 0.02661914750933647
iteration 3, loss = 0.02016933634877205
iteration 4, loss = 0.022321265190839767
iteration 5, loss = 0.03533872961997986
iteration 6, loss = 0.011647353880107403
iteration 7, loss = 0.016646765172481537
iteration 8, loss = 0.03175277262926102
iteration 9, loss = 0.013510056771337986
iteration 10, loss = 0.023918021470308304
iteration 11, loss = 0.026996765285730362
iteration 12, loss = 0.014155471697449684
iteration 13, loss = 0.02510860376060009
iteration 14, loss = 0.025093460455536842
iteration 15, loss = 0.018409285694360733
iteration 16, loss = 0.01455742958933115
iteration 17, loss = 0.03588923439383507
iteration 18, loss = 0.020319541916251183
iteration 19, loss = 0.01898469775915146
iteration 20, loss = 0.015450894832611084
iteration 21, loss = 0.013706959784030914
iteration 22, loss = 0.017479045316576958
iteration 23, loss = 0.019445572048425674
iteration 24, loss = 0.02566549926996231
iteration 25, loss = 0.019039932638406754
iteration 26, loss = 0.02820511721074581
iteration 27, loss = 0.01619734801352024
iteration 28, loss = 0.02615988999605179
iteration 29, loss = 0.016390787437558174
iteration 30, loss = 0.022105421870946884
iteration 31, loss = 0.025188425555825233
iteration 32, loss = 0.015437113121151924
iteration 33, loss = 0.025347912684082985
iteration 34, loss = 0.016682615503668785
iteration 35, loss = 0.027530178427696228
iteration 36, loss = 0.029252154752612114
iteration 37, loss = 0.015555353835225105
iteration 38, loss = 0.029031820595264435
iteration 39, loss = 0.02231879159808159
iteration 40, loss = 0.014665160328149796
iteration 41, loss = 0.01258555706590414
iteration 42, loss = 0.020513443276286125
iteration 43, loss = 0.016248095780611038
iteration 44, loss = 0.020722350105643272
iteration 45, loss = 0.031829774379730225
iteration 46, loss = 0.017969826236367226
iteration 47, loss = 0.02390168234705925
iteration 48, loss = 0.020373985171318054
iteration 49, loss = 0.021009981632232666
iteration 50, loss = 0.01468569040298462
iteration 51, loss = 0.025579456239938736
iteration 52, loss = 0.022996295243501663
iteration 53, loss = 0.016843333840370178
iteration 54, loss = 0.017698336392641068
iteration 55, loss = 0.03876033052802086
iteration 56, loss = 0.0152040496468544
iteration 57, loss = 0.01909472793340683
iteration 58, loss = 0.021904008463025093
iteration 59, loss = 0.0305213313549757
iteration 60, loss = 0.014989396557211876
iteration 61, loss = 0.030650442466139793
iteration 62, loss = 0.016196176409721375
iteration 63, loss = 0.02022211253643036
iteration 64, loss = 0.02789115533232689
iteration 65, loss = 0.0358717106282711
iteration 66, loss = 0.02456621639430523
iteration 67, loss = 0.023610755801200867
iteration 68, loss = 0.041086118668317795
iteration 69, loss = 0.020831752568483353
iteration 70, loss = 0.021481385454535484
iteration 71, loss = 0.015215501189231873
iteration 72, loss = 0.014817783609032631
iteration 73, loss = 0.025291334837675095
iteration 74, loss = 0.028268985450267792
iteration 75, loss = 0.0152482520788908
iteration 76, loss = 0.015972504392266273
iteration 77, loss = 0.0341055691242218
iteration 78, loss = 0.01377123687416315
iteration 79, loss = 0.01967891864478588
iteration 80, loss = 0.025473404675722122
iteration 81, loss = 0.027083169668912888
iteration 82, loss = 0.02307756431400776
iteration 83, loss = 0.01803460717201233
iteration 84, loss = 0.024355284869670868
iteration 85, loss = 0.02469189465045929
iteration 86, loss = 0.01654992625117302
iteration 87, loss = 0.02143087238073349
iteration 88, loss = 0.01714126579463482
iteration 89, loss = 0.023027462884783745
iteration 90, loss = 0.022029437124729156
iteration 91, loss = 0.020192565396428108
iteration 92, loss = 0.01848316378891468
iteration 93, loss = 0.015196092426776886
iteration 94, loss = 0.026482107117772102
iteration 95, loss = 0.01952178031206131
iteration 96, loss = 0.02160756289958954
iteration 97, loss = 0.0183856301009655
iteration 98, loss = 0.0159889105707407
iteration 99, loss = 0.026518555358052254
iteration 100, loss = 0.01860707439482212
iteration 101, loss = 0.027334041893482208
iteration 102, loss = 0.015840590000152588
iteration 103, loss = 0.023839415982365608
iteration 104, loss = 0.014737587422132492
iteration 105, loss = 0.023638632148504257
iteration 106, loss = 0.022143468260765076
iteration 107, loss = 0.021078143268823624
iteration 108, loss = 0.014571177773177624
iteration 109, loss = 0.02283324860036373
iteration 110, loss = 0.01414728257805109
iteration 111, loss = 0.027776312083005905
iteration 112, loss = 0.01713506132364273
iteration 113, loss = 0.016440464183688164
iteration 114, loss = 0.017482643947005272
iteration 115, loss = 0.02069116197526455
iteration 116, loss = 0.025346651673316956
iteration 117, loss = 0.025225531309843063
iteration 118, loss = 0.028661148622632027
iteration 119, loss = 0.02337348461151123
iteration 120, loss = 0.029254576191306114
iteration 121, loss = 0.02107277512550354
iteration 122, loss = 0.02271747961640358
iteration 123, loss = 0.0175455491989851
iteration 124, loss = 0.01862800121307373
iteration 125, loss = 0.015410193242132664
iteration 126, loss = 0.019838977605104446
iteration 127, loss = 0.04141843318939209
iteration 128, loss = 0.027623994275927544
iteration 129, loss = 0.025462999939918518
iteration 130, loss = 0.02127945050597191
iteration 131, loss = 0.017208274453878403
iteration 132, loss = 0.0294036827981472
iteration 133, loss = 0.027919607236981392
iteration 134, loss = 0.01512700691819191
iteration 135, loss = 0.031236402690410614
iteration 136, loss = 0.01672319881618023
iteration 137, loss = 0.022470561787486076
iteration 138, loss = 0.030019689351320267
iteration 139, loss = 0.01614898070693016
iteration 140, loss = 0.023928023874759674
iteration 141, loss = 0.024622173979878426
iteration 142, loss = 0.02243395894765854
iteration 143, loss = 0.022378887981176376
iteration 144, loss = 0.016915887594223022
iteration 145, loss = 0.015378358773887157
iteration 146, loss = 0.014849868603050709
iteration 147, loss = 0.027817267924547195
iteration 148, loss = 0.030926339328289032
iteration 149, loss = 0.01214873231947422
iteration 150, loss = 0.02131323516368866
iteration 151, loss = 0.020938072353601456
iteration 152, loss = 0.012562024407088757
iteration 153, loss = 0.020434409379959106
iteration 154, loss = 0.017300765961408615
iteration 155, loss = 0.016120992600917816
iteration 156, loss = 0.017093291506171227
iteration 157, loss = 0.026212692260742188
iteration 158, loss = 0.015609515830874443
iteration 159, loss = 0.025542572140693665
iteration 160, loss = 0.02235053852200508
iteration 161, loss = 0.014118874445557594
iteration 162, loss = 0.03319745510816574
iteration 163, loss = 0.016266997903585434
iteration 164, loss = 0.014993204735219479
iteration 165, loss = 0.023133937269449234
iteration 166, loss = 0.02936907298862934
iteration 167, loss = 0.01709749549627304
iteration 168, loss = 0.021167203783988953
iteration 169, loss = 0.03257114812731743
iteration 170, loss = 0.017622733488678932
iteration 171, loss = 0.02174367383122444
iteration 172, loss = 0.018611319363117218
iteration 173, loss = 0.016203688457608223
iteration 174, loss = 0.016172494739294052
iteration 175, loss = 0.013469567522406578
iteration 176, loss = 0.02325916849076748
iteration 177, loss = 0.015986179932951927
iteration 178, loss = 0.016286691650748253
iteration 179, loss = 0.014896713197231293
iteration 180, loss = 0.015376155264675617
iteration 181, loss = 0.028820402920246124
iteration 182, loss = 0.01565166562795639
iteration 183, loss = 0.02256271243095398
iteration 184, loss = 0.017860477790236473
iteration 185, loss = 0.012340348213911057
iteration 186, loss = 0.01674368791282177
iteration 187, loss = 0.02255258336663246
iteration 188, loss = 0.01610039919614792
iteration 189, loss = 0.018766770139336586
iteration 190, loss = 0.017177922651171684
iteration 191, loss = 0.01566053181886673
iteration 192, loss = 0.013902628794312477
iteration 193, loss = 0.014082664623856544
iteration 194, loss = 0.02978394366800785
iteration 195, loss = 0.01710404083132744
iteration 196, loss = 0.01987721025943756
iteration 197, loss = 0.04347476735711098
iteration 198, loss = 0.013618874363601208
iteration 199, loss = 0.021105121821165085
iteration 200, loss = 0.02561548724770546
iteration 201, loss = 0.014287251979112625
iteration 202, loss = 0.024245833978056908
iteration 203, loss = 0.03441518545150757
iteration 204, loss = 0.01726699061691761
iteration 205, loss = 0.03182274103164673
iteration 206, loss = 0.015465086326003075
iteration 207, loss = 0.020440157502889633
iteration 208, loss = 0.015323726460337639
iteration 209, loss = 0.01902325451374054
iteration 210, loss = 0.022325647994875908
iteration 211, loss = 0.01514337956905365
iteration 212, loss = 0.020589254796504974
iteration 213, loss = 0.018090615049004555
iteration 214, loss = 0.01836916245520115
iteration 215, loss = 0.015020077116787434
iteration 216, loss = 0.01341155357658863
iteration 217, loss = 0.029299361631274223
iteration 218, loss = 0.01905728131532669
iteration 219, loss = 0.014099445194005966
iteration 220, loss = 0.030374769121408463
iteration 221, loss = 0.01977410539984703
iteration 222, loss = 0.018170174211263657
iteration 223, loss = 0.013185553252696991
iteration 224, loss = 0.022620078176259995
iteration 225, loss = 0.021582677960395813
iteration 226, loss = 0.01851492002606392
iteration 227, loss = 0.019318856298923492
iteration 228, loss = 0.019495664164423943
iteration 229, loss = 0.01705918088555336
iteration 230, loss = 0.01939249597489834
iteration 231, loss = 0.019212758168578148
iteration 232, loss = 0.021778253838419914
iteration 233, loss = 0.017995035275816917
iteration 234, loss = 0.01980236917734146
iteration 235, loss = 0.01891186088323593
iteration 236, loss = 0.024548258632421494
iteration 237, loss = 0.02792155183851719
iteration 238, loss = 0.023983735591173172
iteration 239, loss = 0.026305854320526123
iteration 240, loss = 0.021967586129903793
iteration 241, loss = 0.014691147953271866
iteration 242, loss = 0.015634777024388313
iteration 243, loss = 0.016110634431242943
iteration 244, loss = 0.029794227331876755
iteration 245, loss = 0.022514399141073227
iteration 246, loss = 0.02786801941692829
iteration 247, loss = 0.02340162731707096
iteration 248, loss = 0.020539723336696625
iteration 249, loss = 0.01387374009937048
iteration 250, loss = 0.013674701564013958
iteration 251, loss = 0.01565837487578392
iteration 252, loss = 0.014430200681090355
iteration 253, loss = 0.02366795763373375
iteration 254, loss = 0.018038172274827957
iteration 255, loss = 0.018519271165132523
iteration 256, loss = 0.018811574205756187
iteration 257, loss = 0.01348040159791708
iteration 258, loss = 0.014076539315283298
iteration 259, loss = 0.03512737527489662
iteration 260, loss = 0.02797040529549122
iteration 261, loss = 0.041285060346126556
iteration 262, loss = 0.023666467517614365
iteration 263, loss = 0.02403547614812851
iteration 264, loss = 0.025272751227021217
iteration 265, loss = 0.012998512014746666
iteration 266, loss = 0.02249675616621971
iteration 267, loss = 0.03252346068620682
iteration 268, loss = 0.025041429325938225
iteration 269, loss = 0.023890499025583267
iteration 270, loss = 0.019310221076011658
iteration 271, loss = 0.01733492873609066
iteration 272, loss = 0.017277054488658905
iteration 273, loss = 0.021722707897424698
iteration 274, loss = 0.010831373743712902
iteration 275, loss = 0.023670222610235214
iteration 276, loss = 0.027028657495975494
iteration 277, loss = 0.020230935886502266
iteration 278, loss = 0.025932632386684418
iteration 279, loss = 0.014048995450139046
iteration 280, loss = 0.01510988175868988
iteration 281, loss = 0.023624703288078308
iteration 282, loss = 0.025986313819885254
iteration 283, loss = 0.01593082584440708
iteration 284, loss = 0.020075732842087746
iteration 285, loss = 0.015829920768737793
iteration 286, loss = 0.02273489721119404
iteration 287, loss = 0.0344046987593174
iteration 288, loss = 0.02382267266511917
iteration 289, loss = 0.012890593148767948
iteration 290, loss = 0.014643214643001556
iteration 291, loss = 0.023887965828180313
iteration 292, loss = 0.014370026998221874
iteration 293, loss = 0.01933492347598076
iteration 294, loss = 0.01702418178319931
iteration 295, loss = 0.014451833441853523
iteration 296, loss = 0.02411835826933384
iteration 297, loss = 0.021342750638723373
iteration 298, loss = 0.029668163508176804
iteration 299, loss = 0.019090604037046432

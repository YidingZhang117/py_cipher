iteration 1, loss = 0.03985104709863663
iteration 2, loss = 0.03129538893699646
iteration 3, loss = 0.2804751694202423
iteration 4, loss = 0.035774312913417816
iteration 5, loss = 0.02203056588768959
iteration 6, loss = 0.020128872245550156
iteration 7, loss = 0.24034787714481354
iteration 8, loss = 0.026050223037600517
iteration 9, loss = 0.020163491368293762
iteration 10, loss = 0.02172163687646389
iteration 11, loss = 0.01991504803299904
iteration 12, loss = 0.17813247442245483
iteration 13, loss = 0.021235620602965355
iteration 14, loss = 0.020042819902300835
iteration 15, loss = 0.023751838132739067
iteration 16, loss = 0.02969389408826828
iteration 17, loss = 0.10659011453390121
iteration 18, loss = 0.019582388922572136
iteration 19, loss = 0.019310815259814262
iteration 20, loss = 0.043921664357185364
iteration 21, loss = 0.020271696150302887
iteration 22, loss = 0.030540553852915764
iteration 23, loss = 0.02987462654709816
iteration 24, loss = 0.018916936591267586
iteration 25, loss = 0.018929965794086456
iteration 26, loss = 0.020190447568893433
iteration 27, loss = 0.025411978363990784
iteration 28, loss = 0.02766641043126583
iteration 29, loss = 0.026351159438490868
iteration 30, loss = 0.024540331214666367
iteration 31, loss = 0.03354383260011673
iteration 32, loss = 0.02745947614312172
iteration 33, loss = 0.022174321115016937
iteration 34, loss = 0.02444957010447979
iteration 35, loss = 0.018670588731765747
iteration 36, loss = 0.02745954319834709
iteration 37, loss = 0.01814720407128334
iteration 38, loss = 0.018714558333158493
iteration 39, loss = 0.023547716438770294
iteration 40, loss = 0.06376074254512787
iteration 41, loss = 0.05197571590542793
iteration 42, loss = 0.02555808611214161
iteration 43, loss = 0.021060161292552948
iteration 44, loss = 0.049404457211494446
iteration 45, loss = 0.01912153698503971
iteration 46, loss = 0.018554573878645897
iteration 47, loss = 0.03057878464460373
iteration 48, loss = 0.018148940056562424
iteration 49, loss = 0.01982223242521286
iteration 50, loss = 0.019131029024720192
iteration 51, loss = 0.02524411678314209
iteration 52, loss = 0.06308160722255707
iteration 53, loss = 0.05453331023454666
iteration 54, loss = 0.02205980010330677
iteration 55, loss = 0.045535117387771606
iteration 56, loss = 0.01886787638068199
iteration 57, loss = 0.017254935577511787
iteration 58, loss = 0.022920874878764153
iteration 59, loss = 0.02037077024579048
iteration 60, loss = 0.03713294863700867
iteration 61, loss = 0.021962296217679977
iteration 62, loss = 0.02191358432173729
iteration 63, loss = 0.03829752281308174
iteration 64, loss = 0.023143665865063667
iteration 65, loss = 0.021098647266626358
iteration 66, loss = 0.03725973516702652
iteration 67, loss = 0.029482899233698845
iteration 68, loss = 0.018174948170781136
iteration 69, loss = 0.03351441025733948
iteration 70, loss = 0.02089649811387062
iteration 71, loss = 0.04727630317211151
iteration 72, loss = 0.024678165093064308
iteration 73, loss = 0.03280511125922203
iteration 74, loss = 0.017245810478925705
iteration 75, loss = 0.040530115365982056
iteration 76, loss = 0.031585436314344406
iteration 77, loss = 0.026092788204550743
iteration 78, loss = 0.03924919664859772
iteration 79, loss = 0.036224860697984695
iteration 80, loss = 0.0308519434183836
iteration 81, loss = 0.04081618785858154
iteration 82, loss = 0.01809599995613098
iteration 83, loss = 0.017570268362760544
iteration 84, loss = 0.03910377249121666
iteration 85, loss = 0.017509790137410164
iteration 86, loss = 0.024618977680802345
iteration 87, loss = 0.03279571235179901
iteration 88, loss = 0.017747217789292336
iteration 89, loss = 0.02762904018163681
iteration 90, loss = 0.01855662651360035
iteration 91, loss = 0.023005995899438858
iteration 92, loss = 0.017714479938149452
iteration 93, loss = 0.029986903071403503
iteration 94, loss = 0.01736113429069519
iteration 95, loss = 0.02861122228205204
iteration 96, loss = 0.01930214837193489
iteration 97, loss = 0.019027721136808395
iteration 98, loss = 0.020790206268429756
iteration 99, loss = 0.029055267572402954
iteration 100, loss = 0.018261820077896118
iteration 101, loss = 0.030900340527296066
iteration 102, loss = 0.018106747418642044
iteration 103, loss = 0.027132637798786163
iteration 104, loss = 0.018127653747797012
iteration 105, loss = 0.0435301810503006
iteration 106, loss = 0.025016706436872482
iteration 107, loss = 0.026806404814124107
iteration 108, loss = 0.02759302407503128
iteration 109, loss = 0.0213802270591259
iteration 110, loss = 0.01675277203321457
iteration 111, loss = 0.026662170886993408
iteration 112, loss = 0.04570579528808594
iteration 113, loss = 0.03454333543777466
iteration 114, loss = 0.01839601807296276
iteration 115, loss = 0.018324220553040504
iteration 116, loss = 0.018326759338378906
iteration 117, loss = 0.019823919981718063
iteration 118, loss = 0.03594431281089783
iteration 119, loss = 0.02218705788254738
iteration 120, loss = 0.020967796444892883
iteration 121, loss = 0.021721601486206055
iteration 122, loss = 0.018305383622646332
iteration 123, loss = 0.03828120604157448
iteration 124, loss = 0.027429167181253433
iteration 125, loss = 0.02448810078203678
iteration 126, loss = 0.015574254095554352
iteration 127, loss = 0.023862067610025406
iteration 128, loss = 0.019426599144935608
iteration 129, loss = 0.020726602524518967
iteration 130, loss = 0.016139723360538483
iteration 131, loss = 0.017470616847276688
iteration 132, loss = 0.02138545736670494
iteration 133, loss = 0.02404450811445713
iteration 134, loss = 0.01592273637652397
iteration 135, loss = 0.02541474625468254
iteration 136, loss = 0.020652804523706436
iteration 137, loss = 0.031005309894680977
iteration 138, loss = 0.029785500839352608
iteration 139, loss = 0.016719654202461243
iteration 140, loss = 0.015992581844329834
iteration 141, loss = 0.022477107122540474
iteration 142, loss = 0.015795299783349037
iteration 143, loss = 0.018595481291413307
iteration 144, loss = 0.021380135789513588
iteration 145, loss = 0.019487667828798294
iteration 146, loss = 0.025651898235082626
iteration 147, loss = 0.017351003363728523
iteration 148, loss = 0.02648191712796688
iteration 149, loss = 0.01820596307516098
iteration 150, loss = 0.03571973368525505
iteration 151, loss = 0.015507410280406475
iteration 152, loss = 0.01794338971376419
iteration 153, loss = 0.017782995477318764
iteration 154, loss = 0.018042460083961487
iteration 155, loss = 0.022531557828187943
iteration 156, loss = 0.015991181135177612
iteration 157, loss = 0.020553696900606155
iteration 158, loss = 0.014645470306277275
iteration 159, loss = 0.019796263426542282
iteration 160, loss = 0.017324605956673622
iteration 161, loss = 0.026627004146575928
iteration 162, loss = 0.01612103171646595
iteration 163, loss = 0.021861523389816284
iteration 164, loss = 0.014933480881154537
iteration 165, loss = 0.022454457357525826
iteration 166, loss = 0.01686890982091427
iteration 167, loss = 0.03144979476928711
iteration 168, loss = 0.015756448730826378
iteration 169, loss = 0.017221495509147644
iteration 170, loss = 0.02328375354409218
iteration 171, loss = 0.01946035586297512
iteration 172, loss = 0.025431785732507706
iteration 173, loss = 0.014074558392167091
iteration 174, loss = 0.027135683223605156
iteration 175, loss = 0.015447995625436306
iteration 176, loss = 0.014373266138136387
iteration 177, loss = 0.020390914753079414
iteration 178, loss = 0.016066811978816986
iteration 179, loss = 0.01680244877934456
iteration 180, loss = 0.021299142390489578
iteration 181, loss = 0.03168250620365143
iteration 182, loss = 0.016075987368822098
iteration 183, loss = 0.025400632992386818
iteration 184, loss = 0.015417051501572132
iteration 185, loss = 0.015522544272243977
iteration 186, loss = 0.016223113983869553
iteration 187, loss = 0.03250421583652496
iteration 188, loss = 0.027558408677577972
iteration 189, loss = 0.02099783346056938
iteration 190, loss = 0.024369068443775177
iteration 191, loss = 0.014908936806023121
iteration 192, loss = 0.015111520886421204
iteration 193, loss = 0.017404161393642426
iteration 194, loss = 0.015826579183340073
iteration 195, loss = 0.016105135902762413
iteration 196, loss = 0.016038993373513222
iteration 197, loss = 0.024605073034763336
iteration 198, loss = 0.026562104001641273
iteration 199, loss = 0.019196951761841774
iteration 200, loss = 0.014194068498909473
iteration 201, loss = 0.01492004282772541
iteration 202, loss = 0.022702649235725403
iteration 203, loss = 0.025319311767816544
iteration 204, loss = 0.03390729799866676
iteration 205, loss = 0.027521103620529175
iteration 206, loss = 0.025370707735419273
iteration 207, loss = 0.019845686852931976
iteration 208, loss = 0.018353743478655815
iteration 209, loss = 0.04065164923667908
iteration 210, loss = 0.02510620653629303
iteration 211, loss = 0.028963621705770493
iteration 212, loss = 0.0202871672809124
iteration 213, loss = 0.013538663275539875
iteration 214, loss = 0.025438200682401657
iteration 215, loss = 0.015168494544923306
iteration 216, loss = 0.017135366797447205
iteration 217, loss = 0.015593552961945534
iteration 218, loss = 0.01468268595635891
iteration 219, loss = 0.01555786095559597
iteration 220, loss = 0.019829973578453064
iteration 221, loss = 0.033135734498500824
iteration 222, loss = 0.013919499702751637
iteration 223, loss = 0.018515881150960922
iteration 224, loss = 0.022315505892038345
iteration 225, loss = 0.022381765767931938
iteration 226, loss = 0.015380932949483395
iteration 227, loss = 0.016400862485170364
iteration 228, loss = 0.02250332571566105
iteration 229, loss = 0.015414608642458916
iteration 230, loss = 0.01707998663187027
iteration 231, loss = 0.01797320321202278
iteration 232, loss = 0.01815846562385559
iteration 233, loss = 0.018182506784796715
iteration 234, loss = 0.018815331161022186
iteration 235, loss = 0.020505938678979874
iteration 236, loss = 0.02299363911151886
iteration 237, loss = 0.025000954046845436
iteration 238, loss = 0.021838650107383728
iteration 239, loss = 0.021738916635513306
iteration 240, loss = 0.02302536554634571
iteration 241, loss = 0.021959250792860985
iteration 242, loss = 0.02121681347489357
iteration 243, loss = 0.01841837726533413
iteration 244, loss = 0.015267513692378998
iteration 245, loss = 0.01664522849023342
iteration 246, loss = 0.017316004261374474
iteration 247, loss = 0.016637349501252174
iteration 248, loss = 0.030617710202932358
iteration 249, loss = 0.021270453929901123
iteration 250, loss = 0.013871700502932072
iteration 251, loss = 0.022390058264136314
iteration 252, loss = 0.013635523617267609
iteration 253, loss = 0.016745608299970627
iteration 254, loss = 0.019647931680083275
iteration 255, loss = 0.0284662377089262
iteration 256, loss = 0.037453629076480865
iteration 257, loss = 0.03192908316850662
iteration 258, loss = 0.020436840131878853
iteration 259, loss = 0.014003480784595013
iteration 260, loss = 0.031191226094961166
iteration 261, loss = 0.016423197463154793
iteration 262, loss = 0.014078069478273392
iteration 263, loss = 0.014784712344408035
iteration 264, loss = 0.01814521849155426
iteration 265, loss = 0.02317628636956215
iteration 266, loss = 0.024881841614842415
iteration 267, loss = 0.013319969177246094
iteration 268, loss = 0.013477576896548271
iteration 269, loss = 0.012745163403451443
iteration 270, loss = 0.013718177564442158
iteration 271, loss = 0.013779377564787865
iteration 272, loss = 0.019911522045731544
iteration 273, loss = 0.016990231350064278
iteration 274, loss = 0.0135756004601717
iteration 275, loss = 0.021889762952923775
iteration 276, loss = 0.01965584233403206
iteration 277, loss = 0.016331549733877182
iteration 278, loss = 0.015799272805452347
iteration 279, loss = 0.021092301234602928
iteration 280, loss = 0.020364878699183464
iteration 281, loss = 0.02366303652524948
iteration 282, loss = 0.014262029901146889
iteration 283, loss = 0.024017374962568283
iteration 284, loss = 0.01700931042432785
iteration 285, loss = 0.020737299695611
iteration 286, loss = 0.015703465789556503
iteration 287, loss = 0.015398385934531689
iteration 288, loss = 0.012918472290039062
iteration 289, loss = 0.019431427121162415
iteration 290, loss = 0.018220262601971626
iteration 291, loss = 0.015636291354894638
iteration 292, loss = 0.014467790722846985
iteration 293, loss = 0.014406811445951462
iteration 294, loss = 0.011964707635343075
iteration 295, loss = 0.0119186881929636
iteration 296, loss = 0.02090136893093586
iteration 297, loss = 0.01700109802186489
iteration 298, loss = 0.015230782330036163
iteration 299, loss = 0.013477283529937267
iteration 300, loss = 0.01767994835972786
iteration 1, loss = 0.015581965446472168
iteration 2, loss = 0.016321560367941856
iteration 3, loss = 0.020831415429711342
iteration 4, loss = 0.012775232084095478
iteration 5, loss = 0.015330201014876366
iteration 6, loss = 0.019399069249629974
iteration 7, loss = 0.014917362481355667
iteration 8, loss = 0.02069631963968277
iteration 9, loss = 0.016337454319000244
iteration 10, loss = 0.013676617294549942
iteration 11, loss = 0.01703641191124916
iteration 12, loss = 0.012299798429012299
iteration 13, loss = 0.01905875653028488
iteration 14, loss = 0.03190194070339203
iteration 15, loss = 0.02146008610725403
iteration 16, loss = 0.013462575152516365
iteration 17, loss = 0.01372270192950964
iteration 18, loss = 0.01335643045604229
iteration 19, loss = 0.01603054255247116
iteration 20, loss = 0.011768735945224762
iteration 21, loss = 0.011908833868801594
iteration 22, loss = 0.02186548337340355
iteration 23, loss = 0.015180150978267193
iteration 24, loss = 0.013333261013031006
iteration 25, loss = 0.012444837018847466
iteration 26, loss = 0.01567857898771763
iteration 27, loss = 0.016829973086714745
iteration 28, loss = 0.017836518585681915
iteration 29, loss = 0.014568479731678963
iteration 30, loss = 0.014979494735598564
iteration 31, loss = 0.014945965260267258
iteration 32, loss = 0.017062313854694366
iteration 33, loss = 0.016189968213438988
iteration 34, loss = 0.017775215208530426
iteration 35, loss = 0.011560970917344093
iteration 36, loss = 0.012577103450894356
iteration 37, loss = 0.015123328194022179
iteration 38, loss = 0.019466180354356766
iteration 39, loss = 0.013269280083477497
iteration 40, loss = 0.012025544419884682
iteration 41, loss = 0.02387487143278122
iteration 42, loss = 0.026993926614522934
iteration 43, loss = 0.01685488037765026
iteration 44, loss = 0.02630710043013096
iteration 45, loss = 0.0123743312433362
iteration 46, loss = 0.014533030800521374
iteration 47, loss = 0.011846901848912239
iteration 48, loss = 0.012050427496433258
iteration 49, loss = 0.019739380106329918
iteration 50, loss = 0.017124949023127556
iteration 51, loss = 0.011522123590111732
iteration 52, loss = 0.016425158828496933
iteration 53, loss = 0.014369465410709381
iteration 54, loss = 0.016068916767835617
iteration 55, loss = 0.015820791944861412
iteration 56, loss = 0.01677441969513893
iteration 57, loss = 0.01507335901260376
iteration 58, loss = 0.014883221127092838
iteration 59, loss = 0.014752677641808987
iteration 60, loss = 0.013341291807591915
iteration 61, loss = 0.011120266281068325
iteration 62, loss = 0.011478493921458721
iteration 63, loss = 0.013082573190331459
iteration 64, loss = 0.016717100515961647
iteration 65, loss = 0.013242026790976524
iteration 66, loss = 0.021354276686906815
iteration 67, loss = 0.015871336683630943
iteration 68, loss = 0.013413144275546074
iteration 69, loss = 0.012639795430004597
iteration 70, loss = 0.013370496220886707
iteration 71, loss = 0.025303345173597336
iteration 72, loss = 0.013985874131321907
iteration 73, loss = 0.014331705868244171
iteration 74, loss = 0.012523024342954159
iteration 75, loss = 0.014762393198907375
iteration 76, loss = 0.016363579779863358
iteration 77, loss = 0.014911272563040257
iteration 78, loss = 0.013304930180311203
iteration 79, loss = 0.013012801297008991
iteration 80, loss = 0.011591334827244282
iteration 81, loss = 0.012510018423199654
iteration 82, loss = 0.020676374435424805
iteration 83, loss = 0.01255805604159832
iteration 84, loss = 0.014650056138634682
iteration 85, loss = 0.01259111799299717
iteration 86, loss = 0.018516531214118004
iteration 87, loss = 0.011798846535384655
iteration 88, loss = 0.010832319967448711
iteration 89, loss = 0.012632418423891068
iteration 90, loss = 0.012917283922433853
iteration 91, loss = 0.0151646938174963
iteration 92, loss = 0.01269727572798729
iteration 93, loss = 0.01640712097287178
iteration 94, loss = 0.01687437668442726
iteration 95, loss = 0.01170075498521328
iteration 96, loss = 0.01406488474458456
iteration 97, loss = 0.012242532335221767
iteration 98, loss = 0.011220080778002739
iteration 99, loss = 0.01264210045337677
iteration 100, loss = 0.01762642152607441
iteration 101, loss = 0.012158616445958614
iteration 102, loss = 0.012169736437499523
iteration 103, loss = 0.014213609509170055
iteration 104, loss = 0.01332258339971304
iteration 105, loss = 0.013196910731494427
iteration 106, loss = 0.016520049422979355
iteration 107, loss = 0.012385349720716476
iteration 108, loss = 0.010187548585236073
iteration 109, loss = 0.011931823566555977
iteration 110, loss = 0.011092067696154118
iteration 111, loss = 0.01553109847009182
iteration 112, loss = 0.02804703265428543
iteration 113, loss = 0.024096256121993065
iteration 114, loss = 0.02442185953259468
iteration 115, loss = 0.01301121711730957
iteration 116, loss = 0.012263542972505093
iteration 117, loss = 0.010834925808012486
iteration 118, loss = 0.019676925614476204
iteration 119, loss = 0.014045070856809616
iteration 120, loss = 0.012607814744114876
iteration 121, loss = 0.020507609471678734
iteration 122, loss = 0.011594927869737148
iteration 123, loss = 0.01044081337749958
iteration 124, loss = 0.010961037129163742
iteration 125, loss = 0.01296960562467575
iteration 126, loss = 0.012387311086058617
iteration 127, loss = 0.014255630783736706
iteration 128, loss = 0.018866943195462227
iteration 129, loss = 0.018182050436735153
iteration 130, loss = 0.010790487751364708
iteration 131, loss = 0.012109793722629547
iteration 132, loss = 0.011726238764822483
iteration 133, loss = 0.011930109933018684
iteration 134, loss = 0.011957161128520966
iteration 135, loss = 0.016344796866178513
iteration 136, loss = 0.011176050640642643
iteration 137, loss = 0.011076020076870918
iteration 138, loss = 0.011614582501351833
iteration 139, loss = 0.012664207257330418
iteration 140, loss = 0.01134451199322939
iteration 141, loss = 0.01413041539490223
iteration 142, loss = 0.010656225495040417
iteration 143, loss = 0.01288542989641428
iteration 144, loss = 0.01097281463444233
iteration 145, loss = 0.01307710912078619
iteration 146, loss = 0.02296690084040165
iteration 147, loss = 0.013758387416601181
iteration 148, loss = 0.02362816222012043
iteration 149, loss = 0.010153437964618206
iteration 150, loss = 0.011047248728573322
iteration 151, loss = 0.015442811883985996
iteration 152, loss = 0.009845479391515255
iteration 153, loss = 0.012458683922886848
iteration 154, loss = 0.020345287397503853
iteration 155, loss = 0.017029378563165665
iteration 156, loss = 0.011528508737683296
iteration 157, loss = 0.023476876318454742
iteration 158, loss = 0.01023472473025322
iteration 159, loss = 0.01294433232396841
iteration 160, loss = 0.01591590791940689
iteration 161, loss = 0.010236785747110844
iteration 162, loss = 0.01213076151907444
iteration 163, loss = 0.010758543387055397
iteration 164, loss = 0.01459729764610529
iteration 165, loss = 0.010219250805675983
iteration 166, loss = 0.021084798499941826
iteration 167, loss = 0.011789537966251373
iteration 168, loss = 0.01206560991704464
iteration 169, loss = 0.011783860623836517
iteration 170, loss = 0.012126111425459385
iteration 171, loss = 0.0160669032484293
iteration 172, loss = 0.011517301201820374
iteration 173, loss = 0.01081724464893341
iteration 174, loss = 0.011980393901467323
iteration 175, loss = 0.010075836442410946
iteration 176, loss = 0.009989537298679352
iteration 177, loss = 0.010309147648513317
iteration 178, loss = 0.02208988182246685
iteration 179, loss = 0.011967623606324196
iteration 180, loss = 0.009675786830484867
iteration 181, loss = 0.015851417556405067
iteration 182, loss = 0.01594606600701809
iteration 183, loss = 0.012642169371247292
iteration 184, loss = 0.014064366929233074
iteration 185, loss = 0.010138364508748055
iteration 186, loss = 0.010263534262776375
iteration 187, loss = 0.019444124773144722
iteration 188, loss = 0.014821164309978485
iteration 189, loss = 0.01055127289146185
iteration 190, loss = 0.010065194219350815
iteration 191, loss = 0.02240360900759697
iteration 192, loss = 0.009769960306584835
iteration 193, loss = 0.014960695058107376
iteration 194, loss = 0.010082803666591644
iteration 195, loss = 0.011100191622972488
iteration 196, loss = 0.011276058852672577
iteration 197, loss = 0.011409545317292213
iteration 198, loss = 0.012895907275378704
iteration 199, loss = 0.016633911058306694
iteration 200, loss = 0.011570286937057972
iteration 201, loss = 0.011811699718236923
iteration 202, loss = 0.009936406277120113
iteration 203, loss = 0.0338730663061142
iteration 204, loss = 0.016341129317879677
iteration 205, loss = 0.01466409396380186
iteration 206, loss = 0.012842810712754726
iteration 207, loss = 0.01278998889029026
iteration 208, loss = 0.01105532981455326
iteration 209, loss = 0.011939840391278267
iteration 210, loss = 0.015781404450535774
iteration 211, loss = 0.015743199735879898
iteration 212, loss = 0.023611925542354584
iteration 213, loss = 0.012761869467794895
iteration 214, loss = 0.015188074670732021
iteration 215, loss = 0.015987982973456383
iteration 216, loss = 0.012019195593893528
iteration 217, loss = 0.010380546562373638
iteration 218, loss = 0.012559197843074799
iteration 219, loss = 0.00895591452717781
iteration 220, loss = 0.011213107034564018
iteration 221, loss = 0.01102994754910469
iteration 222, loss = 0.012430801056325436
iteration 223, loss = 0.020509695634245872
iteration 224, loss = 0.011167817749083042
iteration 225, loss = 0.01239774003624916
iteration 226, loss = 0.011028856970369816
iteration 227, loss = 0.010870900005102158
iteration 228, loss = 0.01701068878173828
iteration 229, loss = 0.009468381293118
iteration 230, loss = 0.014435183256864548
iteration 231, loss = 0.009288661181926727
iteration 232, loss = 0.01454863976687193
iteration 233, loss = 0.012735442258417606
iteration 234, loss = 0.011610470712184906
iteration 235, loss = 0.014667096547782421
iteration 236, loss = 0.01215320359915495
iteration 237, loss = 0.010820607654750347
iteration 238, loss = 0.009940428659319878
iteration 239, loss = 0.01057791244238615
iteration 240, loss = 0.010213829576969147
iteration 241, loss = 0.01630878634750843
iteration 242, loss = 0.01748930662870407
iteration 243, loss = 0.017980387434363365
iteration 244, loss = 0.013516027480363846
iteration 245, loss = 0.019729215651750565
iteration 246, loss = 0.013657795265316963
iteration 247, loss = 0.01545877568423748
iteration 248, loss = 0.009501590393483639
iteration 249, loss = 0.01976400427520275
iteration 250, loss = 0.010071221739053726
iteration 251, loss = 0.009703571908175945
iteration 252, loss = 0.009741566143929958
iteration 253, loss = 0.021439766511321068
iteration 254, loss = 0.009292916394770145
iteration 255, loss = 0.012086596339941025
iteration 256, loss = 0.010190949775278568
iteration 257, loss = 0.011781387962400913
iteration 258, loss = 0.01146873738616705
iteration 259, loss = 0.014728305861353874
iteration 260, loss = 0.009457079693675041
iteration 261, loss = 0.009836479090154171
iteration 262, loss = 0.00897523295134306
iteration 263, loss = 0.014628437347710133
iteration 264, loss = 0.010984391905367374
iteration 265, loss = 0.011387551203370094
iteration 266, loss = 0.013438117690384388
iteration 267, loss = 0.01932820864021778
iteration 268, loss = 0.012083151377737522
iteration 269, loss = 0.009848350659012794
iteration 270, loss = 0.01127623300999403
iteration 271, loss = 0.009947298094630241
iteration 272, loss = 0.010582282207906246
iteration 273, loss = 0.01339919213205576
iteration 274, loss = 0.01031489484012127
iteration 275, loss = 0.011289280839264393
iteration 276, loss = 0.011127321049571037
iteration 277, loss = 0.013670491985976696
iteration 278, loss = 0.019275879487395287
iteration 279, loss = 0.01001397892832756
iteration 280, loss = 0.019808895885944366
iteration 281, loss = 0.012680385261774063
iteration 282, loss = 0.009519640356302261
iteration 283, loss = 0.01021441351622343
iteration 284, loss = 0.018454549834132195
iteration 285, loss = 0.013716627843677998
iteration 286, loss = 0.011479363776743412
iteration 287, loss = 0.017515620216727257
iteration 288, loss = 0.009451843798160553
iteration 289, loss = 0.014902657829225063
iteration 290, loss = 0.00981191173195839
iteration 291, loss = 0.011397439055144787
iteration 292, loss = 0.014662876725196838
iteration 293, loss = 0.009465768933296204
iteration 294, loss = 0.008850875310599804
iteration 295, loss = 0.014325282536447048
iteration 296, loss = 0.011077062226831913
iteration 297, loss = 0.012386322021484375
iteration 298, loss = 0.019681695848703384
iteration 299, loss = 0.009693766012787819
iteration 300, loss = 0.010383816435933113
iteration 1, loss = 0.010646985843777657
iteration 2, loss = 0.011286639608442783
iteration 3, loss = 0.009345781989395618
iteration 4, loss = 0.01037033461034298
iteration 5, loss = 0.013268882408738136
iteration 6, loss = 0.01035753171890974
iteration 7, loss = 0.010341704823076725
iteration 8, loss = 0.008225838653743267
iteration 9, loss = 0.010771095752716064
iteration 10, loss = 0.01845768839120865
iteration 11, loss = 0.011617254465818405
iteration 12, loss = 0.011066718958318233
iteration 13, loss = 0.009382127784192562
iteration 14, loss = 0.014801818877458572
iteration 15, loss = 0.00866423174738884
iteration 16, loss = 0.010048883967101574
iteration 17, loss = 0.010409457609057426
iteration 18, loss = 0.013119441457092762
iteration 19, loss = 0.015225750394165516
iteration 20, loss = 0.012978669255971909
iteration 21, loss = 0.00855429656803608
iteration 22, loss = 0.012887896969914436
iteration 23, loss = 0.009419279173016548
iteration 24, loss = 0.009269794449210167
iteration 25, loss = 0.011692672967910767
iteration 26, loss = 0.009037318639457226
iteration 27, loss = 0.009966952726244926
iteration 28, loss = 0.010824949480593204
iteration 29, loss = 0.009955166839063168
iteration 30, loss = 0.009077825583517551
iteration 31, loss = 0.008747141808271408
iteration 32, loss = 0.010309776291251183
iteration 33, loss = 0.013794797472655773
iteration 34, loss = 0.009042598307132721
iteration 35, loss = 0.010844363830983639
iteration 36, loss = 0.0165291428565979
iteration 37, loss = 0.00908897165209055
iteration 38, loss = 0.008190074935555458
iteration 39, loss = 0.009536994621157646
iteration 40, loss = 0.01137409545481205
iteration 41, loss = 0.0081308763474226
iteration 42, loss = 0.009511333890259266
iteration 43, loss = 0.012210655957460403
iteration 44, loss = 0.013024751096963882
iteration 45, loss = 0.009550098329782486
iteration 46, loss = 0.009500187821686268
iteration 47, loss = 0.010613972321152687
iteration 48, loss = 0.012050013989210129
iteration 49, loss = 0.019227728247642517
iteration 50, loss = 0.02071709930896759
iteration 51, loss = 0.010027715936303139
iteration 52, loss = 0.013931619934737682
iteration 53, loss = 0.00845564715564251
iteration 54, loss = 0.01681489311158657
iteration 55, loss = 0.008173376321792603
iteration 56, loss = 0.013109772466123104
iteration 57, loss = 0.009198686107993126
iteration 58, loss = 0.01173574011772871
iteration 59, loss = 0.009526724927127361
iteration 60, loss = 0.008855911903083324
iteration 61, loss = 0.01003277488052845
iteration 62, loss = 0.010495511814951897
iteration 63, loss = 0.009128881618380547
iteration 64, loss = 0.00875092577189207
iteration 65, loss = 0.00875699520111084
iteration 66, loss = 0.009526803158223629
iteration 67, loss = 0.011231577955186367
iteration 68, loss = 0.00980756338685751
iteration 69, loss = 0.015317129902541637
iteration 70, loss = 0.008624672889709473
iteration 71, loss = 0.008472119458019733
iteration 72, loss = 0.009652670472860336
iteration 73, loss = 0.0102047985419631
iteration 74, loss = 0.008006085641682148
iteration 75, loss = 0.009906107559800148
iteration 76, loss = 0.009081993252038956
iteration 77, loss = 0.009124573320150375
iteration 78, loss = 0.012126241810619831
iteration 79, loss = 0.011791294440627098
iteration 80, loss = 0.008689415641129017
iteration 81, loss = 0.016318636015057564
iteration 82, loss = 0.01017846167087555
iteration 83, loss = 0.00932746659964323
iteration 84, loss = 0.013717281632125378
iteration 85, loss = 0.009124037809669971
iteration 86, loss = 0.010911320336163044
iteration 87, loss = 0.009092670865356922
iteration 88, loss = 0.012417342513799667
iteration 89, loss = 0.007549028843641281
iteration 90, loss = 0.007839947007596493
iteration 91, loss = 0.022360434755682945
iteration 92, loss = 0.017698073759675026
iteration 93, loss = 0.013744384050369263
iteration 94, loss = 0.018509699031710625
iteration 95, loss = 0.012937822379171848
iteration 96, loss = 0.013310762122273445
iteration 97, loss = 0.008648212999105453
iteration 98, loss = 0.009378629736602306
iteration 99, loss = 0.010564967058598995
iteration 100, loss = 0.007421290501952171
iteration 101, loss = 0.012797551229596138
iteration 102, loss = 0.008908087387681007
iteration 103, loss = 0.012480741366744041
iteration 104, loss = 0.011033987626433372
iteration 105, loss = 0.008706316351890564
iteration 106, loss = 0.012601119466125965
iteration 107, loss = 0.01057029515504837
iteration 108, loss = 0.008726927451789379
iteration 109, loss = 0.016086580231785774
iteration 110, loss = 0.008424482308328152
iteration 111, loss = 0.009701772592961788
iteration 112, loss = 0.010591289028525352
iteration 113, loss = 0.010448111221194267
iteration 114, loss = 0.009889493696391582
iteration 115, loss = 0.008960546925663948
iteration 116, loss = 0.007916991598904133
iteration 117, loss = 0.008680169470608234
iteration 118, loss = 0.009377897717058659
iteration 119, loss = 0.007878469303250313
iteration 120, loss = 0.009254380129277706
iteration 121, loss = 0.009241700172424316
iteration 122, loss = 0.01065261010080576
iteration 123, loss = 0.010184072889387608
iteration 124, loss = 0.010589257813990116
iteration 125, loss = 0.010395820252597332
iteration 126, loss = 0.009076781570911407
iteration 127, loss = 0.01277164276689291
iteration 128, loss = 0.009080741554498672
iteration 129, loss = 0.016426531597971916
iteration 130, loss = 0.009225944057106972
iteration 131, loss = 0.009069654159247875
iteration 132, loss = 0.007647609803825617
iteration 133, loss = 0.00934592355042696
iteration 134, loss = 0.00707637146115303
iteration 135, loss = 0.012373819947242737
iteration 136, loss = 0.012592446058988571
iteration 137, loss = 0.011064397171139717
iteration 138, loss = 0.00791158527135849
iteration 139, loss = 0.008363557048141956
iteration 140, loss = 0.009577828459441662
iteration 141, loss = 0.00905227568000555
iteration 142, loss = 0.008646895177662373
iteration 143, loss = 0.011967687867581844
iteration 144, loss = 0.007464170455932617
iteration 145, loss = 0.007698806934058666
iteration 146, loss = 0.01403314620256424
iteration 147, loss = 0.008822144940495491
iteration 148, loss = 0.008336939848959446
iteration 149, loss = 0.008849999867379665
iteration 150, loss = 0.012800002470612526
iteration 151, loss = 0.007841872051358223
iteration 152, loss = 0.009206358343362808
iteration 153, loss = 0.01004144735634327
iteration 154, loss = 0.011419098824262619
iteration 155, loss = 0.008082977496087551
iteration 156, loss = 0.010636072605848312
iteration 157, loss = 0.011967022903263569
iteration 158, loss = 0.008864942006766796
iteration 159, loss = 0.00965857319533825
iteration 160, loss = 0.008873467333614826
iteration 161, loss = 0.00922108069062233
iteration 162, loss = 0.008087022230029106
iteration 163, loss = 0.010428364388644695
iteration 164, loss = 0.007243969943374395
iteration 165, loss = 0.008423017337918282
iteration 166, loss = 0.008054657839238644
iteration 167, loss = 0.018837301060557365
iteration 168, loss = 0.008282247930765152
iteration 169, loss = 0.00943150743842125
iteration 170, loss = 0.007875947281718254
iteration 171, loss = 0.007416374981403351
iteration 172, loss = 0.008241282775998116
iteration 173, loss = 0.013094047084450722
iteration 174, loss = 0.011664438992738724
iteration 175, loss = 0.010952928103506565
iteration 176, loss = 0.008085607551038265
iteration 177, loss = 0.007278961129486561
iteration 178, loss = 0.0081293610855937
iteration 179, loss = 0.007212440017610788
iteration 180, loss = 0.014225305058062077
iteration 181, loss = 0.011125349439680576
iteration 182, loss = 0.010361327789723873
iteration 183, loss = 0.007455391343683004
iteration 184, loss = 0.0069660767912864685
iteration 185, loss = 0.012946962378919125
iteration 186, loss = 0.008278688415884972
iteration 187, loss = 0.007714744191616774
iteration 188, loss = 0.008515916764736176
iteration 189, loss = 0.008831041865050793
iteration 190, loss = 0.007399159949272871
iteration 191, loss = 0.009006388485431671
iteration 192, loss = 0.011607883498072624
iteration 193, loss = 0.007944120094180107
iteration 194, loss = 0.008633431047201157
iteration 195, loss = 0.010218440555036068
iteration 196, loss = 0.010422526858747005
iteration 197, loss = 0.008982247672975063
iteration 198, loss = 0.017583947628736496
iteration 199, loss = 0.011028583161532879
iteration 200, loss = 0.007279209326952696
iteration 201, loss = 0.011049507185816765
iteration 202, loss = 0.010371019132435322
iteration 203, loss = 0.006823963951319456
iteration 204, loss = 0.007767504546791315
iteration 205, loss = 0.0076201194897294044
iteration 206, loss = 0.0074857426807284355
iteration 207, loss = 0.008377748541533947
iteration 208, loss = 0.007634318899363279
iteration 209, loss = 0.010851477272808552
iteration 210, loss = 0.008994512259960175
iteration 211, loss = 0.011067072860896587
iteration 212, loss = 0.0079279076308012
iteration 213, loss = 0.00938261579722166
iteration 214, loss = 0.00767242768779397
iteration 215, loss = 0.007154712453484535
iteration 216, loss = 0.011399954557418823
iteration 217, loss = 0.011306189000606537
iteration 218, loss = 0.007524420507252216
iteration 219, loss = 0.009174814447760582
iteration 220, loss = 0.007710231933742762
iteration 221, loss = 0.013645435683429241
iteration 222, loss = 0.007918842136859894
iteration 223, loss = 0.009585470892488956
iteration 224, loss = 0.007969522848725319
iteration 225, loss = 0.007460579741746187
iteration 226, loss = 0.007721462287008762
iteration 227, loss = 0.008357556536793709
iteration 228, loss = 0.008619222790002823
iteration 229, loss = 0.007769112475216389
iteration 230, loss = 0.008364117704331875
iteration 231, loss = 0.008938989602029324
iteration 232, loss = 0.006945880129933357
iteration 233, loss = 0.009075315669178963
iteration 234, loss = 0.007353182882070541
iteration 235, loss = 0.009244216606020927
iteration 236, loss = 0.008384384214878082
iteration 237, loss = 0.007971887476742268
iteration 238, loss = 0.007835638709366322
iteration 239, loss = 0.009153138846158981
iteration 240, loss = 0.009006092324852943
iteration 241, loss = 0.0077334302477538586
iteration 242, loss = 0.0077184392139315605
iteration 243, loss = 0.0070810928009450436
iteration 244, loss = 0.01125079020857811
iteration 245, loss = 0.01511862501502037
iteration 246, loss = 0.007590515073388815
iteration 247, loss = 0.00860805157572031
iteration 248, loss = 0.00654150964692235
iteration 249, loss = 0.013632861897349358
iteration 250, loss = 0.008258793503046036
iteration 251, loss = 0.0070536211133003235
iteration 252, loss = 0.007614416535943747
iteration 253, loss = 0.010076854377985
iteration 254, loss = 0.010583753697574139
iteration 255, loss = 0.010429363697767258
iteration 256, loss = 0.011748802848160267
iteration 257, loss = 0.00925655011087656
iteration 258, loss = 0.010942678898572922
iteration 259, loss = 0.008642725646495819
iteration 260, loss = 0.006681456696242094
iteration 261, loss = 0.009417016059160233
iteration 262, loss = 0.014787848107516766
iteration 263, loss = 0.00674646720290184
iteration 264, loss = 0.007939397357404232
iteration 265, loss = 0.01118387933820486
iteration 266, loss = 0.012736158445477486
iteration 267, loss = 0.011472134850919247
iteration 268, loss = 0.010163924656808376
iteration 269, loss = 0.00876376312226057
iteration 270, loss = 0.009711210615932941
iteration 271, loss = 0.009677201509475708
iteration 272, loss = 0.007763122208416462
iteration 273, loss = 0.0071492064744234085
iteration 274, loss = 0.007981324568390846
iteration 275, loss = 0.006732436362653971
iteration 276, loss = 0.006346020847558975
iteration 277, loss = 0.0070929634384810925
iteration 278, loss = 0.01840745098888874
iteration 279, loss = 0.00814279355108738
iteration 280, loss = 0.007508337032049894
iteration 281, loss = 0.006864381954073906
iteration 282, loss = 0.009359629824757576
iteration 283, loss = 0.007440517190843821
iteration 284, loss = 0.006911992095410824
iteration 285, loss = 0.009573427028954029
iteration 286, loss = 0.006939318962395191
iteration 287, loss = 0.015208235941827297
iteration 288, loss = 0.006984882988035679
iteration 289, loss = 0.00821472518146038
iteration 290, loss = 0.007964180782437325
iteration 291, loss = 0.007657984271645546
iteration 292, loss = 0.00914805755019188
iteration 293, loss = 0.008335770107805729
iteration 294, loss = 0.02119428664445877
iteration 295, loss = 0.014131776988506317
iteration 296, loss = 0.008311305195093155
iteration 297, loss = 0.012273707427084446
iteration 298, loss = 0.010715914890170097
iteration 299, loss = 0.008157413452863693
iteration 300, loss = 0.007571578491479158
iteration 1, loss = 0.013420075178146362
iteration 2, loss = 0.007693661842495203
iteration 3, loss = 0.012824187986552715
iteration 4, loss = 0.012795981019735336
iteration 5, loss = 0.008169993758201599
iteration 6, loss = 0.011551898904144764
iteration 7, loss = 0.006224417593330145
iteration 8, loss = 0.008266357704997063
iteration 9, loss = 0.011769266799092293
iteration 10, loss = 0.010915304534137249
iteration 11, loss = 0.009934177622199059
iteration 12, loss = 0.010150286369025707
iteration 13, loss = 0.009447562508285046
iteration 14, loss = 0.007858910597860813
iteration 15, loss = 0.008496121503412724
iteration 16, loss = 0.010191494598984718
iteration 17, loss = 0.007420182228088379
iteration 18, loss = 0.008648880757391453
iteration 19, loss = 0.006851701531559229
iteration 20, loss = 0.008343944326043129
iteration 21, loss = 0.008053641766309738
iteration 22, loss = 0.00661477679386735
iteration 23, loss = 0.006491398438811302
iteration 24, loss = 0.006115366704761982
iteration 25, loss = 0.007045169360935688
iteration 26, loss = 0.007022606208920479
iteration 27, loss = 0.007310959510505199
iteration 28, loss = 0.018385637551546097
iteration 29, loss = 0.006369202863425016
iteration 30, loss = 0.007617496419698
iteration 31, loss = 0.009972329251468182
iteration 32, loss = 0.006768590770661831
iteration 33, loss = 0.008035331033170223
iteration 34, loss = 0.007118782494217157
iteration 35, loss = 0.006449112202972174
iteration 36, loss = 0.007052483968436718
iteration 37, loss = 0.008534910157322884
iteration 38, loss = 0.00643222127109766
iteration 39, loss = 0.006488409359008074
iteration 40, loss = 0.00781202781945467
iteration 41, loss = 0.008003168739378452
iteration 42, loss = 0.008059137500822544
iteration 43, loss = 0.007047380320727825
iteration 44, loss = 0.011003417894244194
iteration 45, loss = 0.006319643929600716
iteration 46, loss = 0.0071858312003314495
iteration 47, loss = 0.006987800821661949
iteration 48, loss = 0.00752992881461978
iteration 49, loss = 0.006871184334158897
iteration 50, loss = 0.018014898523688316
iteration 51, loss = 0.009418746456503868
iteration 52, loss = 0.007036530412733555
iteration 53, loss = 0.00619343901053071
iteration 54, loss = 0.00802586879581213
iteration 55, loss = 0.006482872646301985
iteration 56, loss = 0.008136429823935032
iteration 57, loss = 0.007148666772991419
iteration 58, loss = 0.007738166954368353
iteration 59, loss = 0.008131619542837143
iteration 60, loss = 0.008846096694469452
iteration 61, loss = 0.01011638157069683
iteration 62, loss = 0.010387305170297623
iteration 63, loss = 0.007084546610713005
iteration 64, loss = 0.006428343243896961
iteration 65, loss = 0.006586714647710323
iteration 66, loss = 0.007406284101307392
iteration 67, loss = 0.006444617640227079
iteration 68, loss = 0.008435443043708801
iteration 69, loss = 0.007814932614564896
iteration 70, loss = 0.006853574886918068
iteration 71, loss = 0.006877325475215912
iteration 72, loss = 0.005678851157426834
iteration 73, loss = 0.00712175015360117
iteration 74, loss = 0.005605608690530062
iteration 75, loss = 0.005820636171847582
iteration 76, loss = 0.010873115621507168
iteration 77, loss = 0.006261927075684071
iteration 78, loss = 0.006092428229749203
iteration 79, loss = 0.006780864670872688
iteration 80, loss = 0.006246387958526611
iteration 81, loss = 0.00732762273401022
iteration 82, loss = 0.011178873479366302
iteration 83, loss = 0.007771968375891447
iteration 84, loss = 0.008003845810890198
iteration 85, loss = 0.00936021376401186
iteration 86, loss = 0.010016795247793198
iteration 87, loss = 0.00939183495938778
iteration 88, loss = 0.006601287983357906
iteration 89, loss = 0.007229282986372709
iteration 90, loss = 0.005921275354921818
iteration 91, loss = 0.005984851624816656
iteration 92, loss = 0.006784047465771437
iteration 93, loss = 0.00767549080774188
iteration 94, loss = 0.006779690273106098
iteration 95, loss = 0.006319406442344189
iteration 96, loss = 0.007329946383833885
iteration 97, loss = 0.006360197439789772
iteration 98, loss = 0.006398976780474186
iteration 99, loss = 0.011644430458545685
iteration 100, loss = 0.009220801293849945
iteration 101, loss = 0.005711453966796398
iteration 102, loss = 0.0077748470939695835
iteration 103, loss = 0.007074740715324879
iteration 104, loss = 0.006542227230966091
iteration 105, loss = 0.006014607381075621
iteration 106, loss = 0.008371889591217041
iteration 107, loss = 0.009886160492897034
iteration 108, loss = 0.007220981642603874
iteration 109, loss = 0.0075827669352293015
iteration 110, loss = 0.006278517656028271
iteration 111, loss = 0.007326173596084118
iteration 112, loss = 0.013334093615412712
iteration 113, loss = 0.006219638045877218
iteration 114, loss = 0.00809254590421915
iteration 115, loss = 0.006553757004439831
iteration 116, loss = 0.006828153971582651
iteration 117, loss = 0.007854634895920753
iteration 118, loss = 0.006912068463861942
iteration 119, loss = 0.00814482755959034
iteration 120, loss = 0.0065749641507864
iteration 121, loss = 0.006887568160891533
iteration 122, loss = 0.006798247806727886
iteration 123, loss = 0.005841128993779421
iteration 124, loss = 0.008632147684693336
iteration 125, loss = 0.006508158519864082
iteration 126, loss = 0.007842693477869034
iteration 127, loss = 0.008951905183494091
iteration 128, loss = 0.008300237357616425
iteration 129, loss = 0.007877243682742119
iteration 130, loss = 0.00929955393075943
iteration 131, loss = 0.015016199089586735
iteration 132, loss = 0.00736146280542016
iteration 133, loss = 0.00612330436706543
iteration 134, loss = 0.01250526774674654
iteration 135, loss = 0.00910821184515953
iteration 136, loss = 0.006736209616065025
iteration 137, loss = 0.0072870925068855286
iteration 138, loss = 0.006203647702932358
iteration 139, loss = 0.007745087146759033
iteration 140, loss = 0.007551289163529873
iteration 141, loss = 0.006427805870771408
iteration 142, loss = 0.012966413982212543
iteration 143, loss = 0.008630325086414814
iteration 144, loss = 0.006161605939269066
iteration 145, loss = 0.0071920608170330524
iteration 146, loss = 0.009178410284221172
iteration 147, loss = 0.006497259251773357
iteration 148, loss = 0.008573951199650764
iteration 149, loss = 0.005731579847633839
iteration 150, loss = 0.0056942179799079895
iteration 151, loss = 0.006474665366113186
iteration 152, loss = 0.0071091619320213795
iteration 153, loss = 0.005958292633295059
iteration 154, loss = 0.007190133444964886
iteration 155, loss = 0.009250426664948463
iteration 156, loss = 0.005986681208014488
iteration 157, loss = 0.007419757544994354
iteration 158, loss = 0.006029251962900162
iteration 159, loss = 0.0063663083128631115
iteration 160, loss = 0.008055852726101875
iteration 161, loss = 0.006317460909485817
iteration 162, loss = 0.0065572261810302734
iteration 163, loss = 0.011920263059437275
iteration 164, loss = 0.008248751983046532
iteration 165, loss = 0.007910146377980709
iteration 166, loss = 0.007391081657260656
iteration 167, loss = 0.0062385136261582375
iteration 168, loss = 0.006433771923184395
iteration 169, loss = 0.013589099980890751
iteration 170, loss = 0.0117537472397089
iteration 171, loss = 0.008937137201428413
iteration 172, loss = 0.007621657103300095
iteration 173, loss = 0.006360415369272232
iteration 174, loss = 0.007353589404374361
iteration 175, loss = 0.005487035494297743
iteration 176, loss = 0.005505995824933052
iteration 177, loss = 0.008812605403363705
iteration 178, loss = 0.005804978311061859
iteration 179, loss = 0.006982610560953617
iteration 180, loss = 0.005702940747141838
iteration 181, loss = 0.006364310625940561
iteration 182, loss = 0.005970328114926815
iteration 183, loss = 0.009600745514035225
iteration 184, loss = 0.005943421274423599
iteration 185, loss = 0.012864358723163605
iteration 186, loss = 0.007473406381905079
iteration 187, loss = 0.006885451730340719
iteration 188, loss = 0.007636199705302715
iteration 189, loss = 0.007208808790892363
iteration 190, loss = 0.006272298749536276
iteration 191, loss = 0.005805269815027714
iteration 192, loss = 0.005674968007951975
iteration 193, loss = 0.006155501119792461
iteration 194, loss = 0.006788938771933317
iteration 195, loss = 0.007427926640957594
iteration 196, loss = 0.007101589813828468
iteration 197, loss = 0.008950801566243172
iteration 198, loss = 0.006791769061237574
iteration 199, loss = 0.010360388085246086
iteration 200, loss = 0.005871564615517855
iteration 201, loss = 0.006610462442040443
iteration 202, loss = 0.00597434351220727
iteration 203, loss = 0.005370626226067543
iteration 204, loss = 0.008973099291324615
iteration 205, loss = 0.006707631517201662
iteration 206, loss = 0.007631157524883747
iteration 207, loss = 0.006796278990805149
iteration 208, loss = 0.005889922846108675
iteration 209, loss = 0.007752359379082918
iteration 210, loss = 0.0055737001821398735
iteration 211, loss = 0.006572169251739979
iteration 212, loss = 0.0056497217155992985
iteration 213, loss = 0.0057869828306138515
iteration 214, loss = 0.006080972496420145
iteration 215, loss = 0.008638927713036537
iteration 216, loss = 0.005520269740372896
iteration 217, loss = 0.012597448192536831
iteration 218, loss = 0.00576083455234766
iteration 219, loss = 0.006138660479336977
iteration 220, loss = 0.01006623450666666
iteration 221, loss = 0.0063572777435183525
iteration 222, loss = 0.0055853333324193954
iteration 223, loss = 0.006212135311216116
iteration 224, loss = 0.008366311900317669
iteration 225, loss = 0.00520322797819972
iteration 226, loss = 0.009140744805335999
iteration 227, loss = 0.005923119839280844
iteration 228, loss = 0.006382257677614689
iteration 229, loss = 0.007000368554145098
iteration 230, loss = 0.008205296471714973
iteration 231, loss = 0.006552721839398146
iteration 232, loss = 0.005851466208696365
iteration 233, loss = 0.011734235100448132
iteration 234, loss = 0.007318824529647827
iteration 235, loss = 0.00530999805778265
iteration 236, loss = 0.0071426634676754475
iteration 237, loss = 0.0068367053754627705
iteration 238, loss = 0.006286446005105972
iteration 239, loss = 0.005735527724027634
iteration 240, loss = 0.005119737237691879
iteration 241, loss = 0.006811836734414101
iteration 242, loss = 0.009365709498524666
iteration 243, loss = 0.008673369884490967
iteration 244, loss = 0.007046033628284931
iteration 245, loss = 0.00609224708750844
iteration 246, loss = 0.006628125905990601
iteration 247, loss = 0.007571264170110226
iteration 248, loss = 0.010436546057462692
iteration 249, loss = 0.008999654091894627
iteration 250, loss = 0.006558536551892757
iteration 251, loss = 0.005275625269860029
iteration 252, loss = 0.005125025287270546
iteration 253, loss = 0.010847809724509716
iteration 254, loss = 0.00749203423038125
iteration 255, loss = 0.006165794096887112
iteration 256, loss = 0.012663392350077629
iteration 257, loss = 0.013033188879489899
iteration 258, loss = 0.008338194340467453
iteration 259, loss = 0.008580004796385765
iteration 260, loss = 0.0067240167409181595
iteration 261, loss = 0.005070602521300316
iteration 262, loss = 0.00736377015709877
iteration 263, loss = 0.00613412307575345
iteration 264, loss = 0.00701627042144537
iteration 265, loss = 0.007223023567348719
iteration 266, loss = 0.005388359539210796
iteration 267, loss = 0.006147466134279966
iteration 268, loss = 0.013041450642049313
iteration 269, loss = 0.006848539691418409
iteration 270, loss = 0.005776183679699898
iteration 271, loss = 0.0060064722783863544
iteration 272, loss = 0.005890736822038889
iteration 273, loss = 0.008659575134515762
iteration 274, loss = 0.011994138360023499
iteration 275, loss = 0.005453531630337238
iteration 276, loss = 0.006331440526992083
iteration 277, loss = 0.009052084758877754
iteration 278, loss = 0.006652559619396925
iteration 279, loss = 0.012534430250525475
iteration 280, loss = 0.005356377921998501
iteration 281, loss = 0.009899789467453957
iteration 282, loss = 0.00582463201135397
iteration 283, loss = 0.005656233988702297
iteration 284, loss = 0.005449383985251188
iteration 285, loss = 0.009260060265660286
iteration 286, loss = 0.005948720965534449
iteration 287, loss = 0.007311091758310795
iteration 288, loss = 0.005910166073590517
iteration 289, loss = 0.005898330360651016
iteration 290, loss = 0.006261532660573721
iteration 291, loss = 0.006520092487335205
iteration 292, loss = 0.006173908244818449
iteration 293, loss = 0.0051025147549808025
iteration 294, loss = 0.010426923632621765
iteration 295, loss = 0.008564386516809464
iteration 296, loss = 0.0061260489746928215
iteration 297, loss = 0.005479512270539999
iteration 298, loss = 0.00682177534326911
iteration 299, loss = 0.005135675426572561
iteration 300, loss = 0.0070748887956142426
iteration 1, loss = 0.006042436230927706
iteration 2, loss = 0.006247932091355324
iteration 3, loss = 0.006816050037741661
iteration 4, loss = 0.006630143616348505
iteration 5, loss = 0.0068653905764222145
iteration 6, loss = 0.008188972249627113
iteration 7, loss = 0.0065054334700107574
iteration 8, loss = 0.010020975023508072
iteration 9, loss = 0.004807901103049517
iteration 10, loss = 0.005968876648694277
iteration 11, loss = 0.00716672046110034
iteration 12, loss = 0.004770117811858654
iteration 13, loss = 0.006378683727234602
iteration 14, loss = 0.00746483588591218
iteration 15, loss = 0.00527249975129962
iteration 16, loss = 0.006848668213933706
iteration 17, loss = 0.007986541837453842
iteration 18, loss = 0.00861671008169651
iteration 19, loss = 0.011478844098746777
iteration 20, loss = 0.006981539074331522
iteration 21, loss = 0.0052488138899207115
iteration 22, loss = 0.006170888431370258
iteration 23, loss = 0.006007437128573656
iteration 24, loss = 0.0071810707449913025
iteration 25, loss = 0.005843868479132652
iteration 26, loss = 0.007641140837222338
iteration 27, loss = 0.005455380771309137
iteration 28, loss = 0.005774900782853365
iteration 29, loss = 0.0054059261456131935
iteration 30, loss = 0.005060283467173576
iteration 31, loss = 0.004598776809871197
iteration 32, loss = 0.005912899971008301
iteration 33, loss = 0.0055650873109698296
iteration 34, loss = 0.0072332583367824554
iteration 35, loss = 0.008571451529860497
iteration 36, loss = 0.004792722873389721
iteration 37, loss = 0.008225497789680958
iteration 38, loss = 0.005498464219272137
iteration 39, loss = 0.0067229075357317924
iteration 40, loss = 0.005852144677191973
iteration 41, loss = 0.009565141052007675
iteration 42, loss = 0.005107800010591745
iteration 43, loss = 0.005077306646853685
iteration 44, loss = 0.009625625796616077
iteration 45, loss = 0.008296094834804535
iteration 46, loss = 0.005531043745577335
iteration 47, loss = 0.0051810890436172485
iteration 48, loss = 0.005026639439165592
iteration 49, loss = 0.007250373251736164
iteration 50, loss = 0.0071907052770257
iteration 51, loss = 0.006128006149083376
iteration 52, loss = 0.006027293857187033
iteration 53, loss = 0.011929506435990334
iteration 54, loss = 0.005349797662347555
iteration 55, loss = 0.004807178862392902
iteration 56, loss = 0.006144944578409195
iteration 57, loss = 0.005397967994213104
iteration 58, loss = 0.010689402930438519
iteration 59, loss = 0.010884644463658333
iteration 60, loss = 0.007411070633679628
iteration 61, loss = 0.005291811190545559
iteration 62, loss = 0.006454532500356436
iteration 63, loss = 0.0072516934014856815
iteration 64, loss = 0.005922141019254923
iteration 65, loss = 0.005567649845033884
iteration 66, loss = 0.005808886140584946
iteration 67, loss = 0.014063908718526363
iteration 68, loss = 0.004783479496836662
iteration 69, loss = 0.005847717635333538
iteration 70, loss = 0.005005659535527229
iteration 71, loss = 0.005119736306369305
iteration 72, loss = 0.005320851691067219
iteration 73, loss = 0.00536071602255106
iteration 74, loss = 0.007439234759658575
iteration 75, loss = 0.005715646781027317
iteration 76, loss = 0.006085419096052647
iteration 77, loss = 0.005889852065593004
iteration 78, loss = 0.006065648049116135
iteration 79, loss = 0.0058950139209628105
iteration 80, loss = 0.005399747751653194
iteration 81, loss = 0.005898847244679928
iteration 82, loss = 0.007387195248156786
iteration 83, loss = 0.006513915024697781
iteration 84, loss = 0.0065222615376114845
iteration 85, loss = 0.005368385463953018
iteration 86, loss = 0.005074723158031702
iteration 87, loss = 0.007720937021076679
iteration 88, loss = 0.006019582971930504
iteration 89, loss = 0.00522397318854928
iteration 90, loss = 0.00505010737106204
iteration 91, loss = 0.00555307324975729
iteration 92, loss = 0.00619890633970499
iteration 93, loss = 0.005532470997422934
iteration 94, loss = 0.005994532257318497
iteration 95, loss = 0.00534253241494298
iteration 96, loss = 0.010502174496650696
iteration 97, loss = 0.0046708653680980206
iteration 98, loss = 0.004944501910358667
iteration 99, loss = 0.008858002722263336
iteration 100, loss = 0.004730492364615202
iteration 101, loss = 0.006509392987936735
iteration 102, loss = 0.007121080067008734
iteration 103, loss = 0.006560710724443197
iteration 104, loss = 0.009957673028111458
iteration 105, loss = 0.005596505478024483
iteration 106, loss = 0.00796446856111288
iteration 107, loss = 0.005549950059503317
iteration 108, loss = 0.005857326555997133
iteration 109, loss = 0.005248975940048695
iteration 110, loss = 0.01049061119556427
iteration 111, loss = 0.006903278175741434
iteration 112, loss = 0.008761932142078876
iteration 113, loss = 0.00975656509399414
iteration 114, loss = 0.005212145391851664
iteration 115, loss = 0.005855109542608261
iteration 116, loss = 0.006373592186719179
iteration 117, loss = 0.005642314907163382
iteration 118, loss = 0.005857800133526325
iteration 119, loss = 0.004515215288847685
iteration 120, loss = 0.004564334638416767
iteration 121, loss = 0.006326803006231785
iteration 122, loss = 0.008979982696473598
iteration 123, loss = 0.004847140982747078
iteration 124, loss = 0.007808052934706211
iteration 125, loss = 0.004744258243590593
iteration 126, loss = 0.005236267577856779
iteration 127, loss = 0.010874706320464611
iteration 128, loss = 0.005098903086036444
iteration 129, loss = 0.0047935363836586475
iteration 130, loss = 0.005304732359945774
iteration 131, loss = 0.006411849055439234
iteration 132, loss = 0.006430992856621742
iteration 133, loss = 0.004550015088170767
iteration 134, loss = 0.006823389325290918
iteration 135, loss = 0.005884995684027672
iteration 136, loss = 0.00484284944832325
iteration 137, loss = 0.00542131531983614
iteration 138, loss = 0.005293328780680895
iteration 139, loss = 0.005200237035751343
iteration 140, loss = 0.0055723185651004314
iteration 141, loss = 0.008218275383114815
iteration 142, loss = 0.006727006286382675
iteration 143, loss = 0.0055131008848547935
iteration 144, loss = 0.005138864740729332
iteration 145, loss = 0.006002993788570166
iteration 146, loss = 0.00569922523573041
iteration 147, loss = 0.004676490556448698
iteration 148, loss = 0.0059838080778717995
iteration 149, loss = 0.00522582745179534
iteration 150, loss = 0.00543699786067009
iteration 151, loss = 0.004626513458788395
iteration 152, loss = 0.005389920435845852
iteration 153, loss = 0.005155589897185564
iteration 154, loss = 0.005188947543501854
iteration 155, loss = 0.004576530773192644
iteration 156, loss = 0.004482831805944443
iteration 157, loss = 0.006022016517817974
iteration 158, loss = 0.00559755926951766
iteration 159, loss = 0.004976944532245398
iteration 160, loss = 0.007678053807467222
iteration 161, loss = 0.005670273210853338
iteration 162, loss = 0.0048114531673491
iteration 163, loss = 0.004831103608012199
iteration 164, loss = 0.005022931843996048
iteration 165, loss = 0.013952823355793953
iteration 166, loss = 0.011785930022597313
iteration 167, loss = 0.005651962943375111
iteration 168, loss = 0.00567852146923542
iteration 169, loss = 0.0047450438141822815
iteration 170, loss = 0.009541756473481655
iteration 171, loss = 0.0046144695952534676
iteration 172, loss = 0.00492107542231679
iteration 173, loss = 0.004933957010507584
iteration 174, loss = 0.005690151825547218
iteration 175, loss = 0.00588363129645586
iteration 176, loss = 0.006787366699427366
iteration 177, loss = 0.004539925139397383
iteration 178, loss = 0.005659273825585842
iteration 179, loss = 0.0070148552767932415
iteration 180, loss = 0.004536374006420374
iteration 181, loss = 0.004271712154150009
iteration 182, loss = 0.004939754959195852
iteration 183, loss = 0.004516106564551592
iteration 184, loss = 0.007031999994069338
iteration 185, loss = 0.004711082670837641
iteration 186, loss = 0.004627178888767958
iteration 187, loss = 0.007181418128311634
iteration 188, loss = 0.006418264005333185
iteration 189, loss = 0.0069447848945856094
iteration 190, loss = 0.005192826967686415
iteration 191, loss = 0.004702053032815456
iteration 192, loss = 0.006195304449647665
iteration 193, loss = 0.006159372627735138
iteration 194, loss = 0.004762031603604555
iteration 195, loss = 0.0062818764708936214
iteration 196, loss = 0.00789332203567028
iteration 197, loss = 0.006364251486957073
iteration 198, loss = 0.004866417031735182
iteration 199, loss = 0.005433324258774519
iteration 200, loss = 0.006381968967616558
iteration 201, loss = 0.005377678200602531
iteration 202, loss = 0.0052452245727181435
iteration 203, loss = 0.007679107133299112
iteration 204, loss = 0.005224505439400673
iteration 205, loss = 0.008770273998379707
iteration 206, loss = 0.007069287821650505
iteration 207, loss = 0.007532392628490925
iteration 208, loss = 0.00488524604588747
iteration 209, loss = 0.006258599925786257
iteration 210, loss = 0.0050279381684958935
iteration 211, loss = 0.00494597340002656
iteration 212, loss = 0.004209733102470636
iteration 213, loss = 0.0046400586143136024
iteration 214, loss = 0.0049711549654603004
iteration 215, loss = 0.004590124823153019
iteration 216, loss = 0.004604802001267672
iteration 217, loss = 0.006881851702928543
iteration 218, loss = 0.005952031351625919
iteration 219, loss = 0.00569468829780817
iteration 220, loss = 0.006363645661622286
iteration 221, loss = 0.0044420696794986725
iteration 222, loss = 0.006040404550731182
iteration 223, loss = 0.0077619473449885845
iteration 224, loss = 0.004734292160719633
iteration 225, loss = 0.004473469685763121
iteration 226, loss = 0.005429569166153669
iteration 227, loss = 0.004261467605829239
iteration 228, loss = 0.005387800745666027
iteration 229, loss = 0.00654698396101594
iteration 230, loss = 0.00849570706486702
iteration 231, loss = 0.004174042958766222
iteration 232, loss = 0.004528086166828871
iteration 233, loss = 0.004355833865702152
iteration 234, loss = 0.005734316073358059
iteration 235, loss = 0.005023448262363672
iteration 236, loss = 0.008798118680715561
iteration 237, loss = 0.006479074712842703
iteration 238, loss = 0.004956502467393875
iteration 239, loss = 0.006271605845540762
iteration 240, loss = 0.009109576232731342
iteration 241, loss = 0.006151434499770403
iteration 242, loss = 0.004376842174679041
iteration 243, loss = 0.00461548613384366
iteration 244, loss = 0.006089665926992893
iteration 245, loss = 0.00445930240675807
iteration 246, loss = 0.005061331205070019
iteration 247, loss = 0.006284916773438454
iteration 248, loss = 0.004823942668735981
iteration 249, loss = 0.007487252354621887
iteration 250, loss = 0.005753419827669859
iteration 251, loss = 0.005720772780478001
iteration 252, loss = 0.004921214189380407
iteration 253, loss = 0.005026348866522312
iteration 254, loss = 0.004809563979506493
iteration 255, loss = 0.005126115400344133
iteration 256, loss = 0.005657674744725227
iteration 257, loss = 0.007743445690721273
iteration 258, loss = 0.005116566084325314
iteration 259, loss = 0.004202142357826233
iteration 260, loss = 0.004963160492479801
iteration 261, loss = 0.004266068339347839
iteration 262, loss = 0.00481331255286932
iteration 263, loss = 0.004493738990277052
iteration 264, loss = 0.005142177455127239
iteration 265, loss = 0.008776101283729076
iteration 266, loss = 0.009257383644580841
iteration 267, loss = 0.004739998374134302
iteration 268, loss = 0.005654800217598677
iteration 269, loss = 0.00497765839099884
iteration 270, loss = 0.00500304251909256
iteration 271, loss = 0.005274792667478323
iteration 272, loss = 0.007081999443471432
iteration 273, loss = 0.0046509443782269955
iteration 274, loss = 0.004934567958116531
iteration 275, loss = 0.004700900055468082
iteration 276, loss = 0.006908324547111988
iteration 277, loss = 0.005707712844014168
iteration 278, loss = 0.004212499596178532
iteration 279, loss = 0.005601472221314907
iteration 280, loss = 0.004451476037502289
iteration 281, loss = 0.005613934714347124
iteration 282, loss = 0.0048382277600467205
iteration 283, loss = 0.009974869899451733
iteration 284, loss = 0.004364857915788889
iteration 285, loss = 0.005885640624910593
iteration 286, loss = 0.004733121022582054
iteration 287, loss = 0.00489823380485177
iteration 288, loss = 0.004721704870462418
iteration 289, loss = 0.004673022776842117
iteration 290, loss = 0.005178252700716257
iteration 291, loss = 0.009076690301299095
iteration 292, loss = 0.00702666025608778
iteration 293, loss = 0.004286016803234816
iteration 294, loss = 0.005096498876810074
iteration 295, loss = 0.004626136738806963
iteration 296, loss = 0.005877072922885418
iteration 297, loss = 0.005611038766801357
iteration 298, loss = 0.004792764782905579
iteration 299, loss = 0.0058759041130542755
iteration 300, loss = 0.005084428004920483
iteration 1, loss = 0.004552156664431095
iteration 2, loss = 0.003851934103295207
iteration 3, loss = 0.005024246405810118
iteration 4, loss = 0.003878719871863723
iteration 5, loss = 0.0043489038944244385
iteration 6, loss = 0.007757505401968956
iteration 7, loss = 0.005335496738553047
iteration 8, loss = 0.006741049699485302
iteration 9, loss = 0.005153382197022438
iteration 10, loss = 0.00951016042381525
iteration 11, loss = 0.004293684847652912
iteration 12, loss = 0.004328466486185789
iteration 13, loss = 0.004030516836792231
iteration 14, loss = 0.004114978946745396
iteration 15, loss = 0.006551509723067284
iteration 16, loss = 0.006584630813449621
iteration 17, loss = 0.004802817478775978
iteration 18, loss = 0.0037725730799138546
iteration 19, loss = 0.004288998898118734
iteration 20, loss = 0.006211733911186457
iteration 21, loss = 0.005464231129735708
iteration 22, loss = 0.004781418479979038
iteration 23, loss = 0.004496546927839518
iteration 24, loss = 0.004465244244784117
iteration 25, loss = 0.004921092186123133
iteration 26, loss = 0.005042036063969135
iteration 27, loss = 0.004065279383212328
iteration 28, loss = 0.005092767998576164
iteration 29, loss = 0.004280992317944765
iteration 30, loss = 0.00470490287989378
iteration 31, loss = 0.004745081998407841
iteration 32, loss = 0.008074356243014336
iteration 33, loss = 0.005877199117094278
iteration 34, loss = 0.004357070196419954
iteration 35, loss = 0.004680898040533066
iteration 36, loss = 0.0046946387737989426
iteration 37, loss = 0.004855281673371792
iteration 38, loss = 0.004588182549923658
iteration 39, loss = 0.010888352058827877
iteration 40, loss = 0.006951155140995979
iteration 41, loss = 0.004185784608125687
iteration 42, loss = 0.004079241771250963
iteration 43, loss = 0.004815402906388044
iteration 44, loss = 0.004873441066592932
iteration 45, loss = 0.005518630146980286
iteration 46, loss = 0.005302727222442627
iteration 47, loss = 0.0064096394926309586
iteration 48, loss = 0.007207795511931181
iteration 49, loss = 0.004766529425978661
iteration 50, loss = 0.005098914261907339
iteration 51, loss = 0.00456009479239583
iteration 52, loss = 0.004329445771872997
iteration 53, loss = 0.005165996495634317
iteration 54, loss = 0.004581750370562077
iteration 55, loss = 0.004193434491753578
iteration 56, loss = 0.008155865594744682
iteration 57, loss = 0.004686180502176285
iteration 58, loss = 0.004607046954333782
iteration 59, loss = 0.004333185032010078
iteration 60, loss = 0.0045227790251374245
iteration 61, loss = 0.004144636448472738
iteration 62, loss = 0.006136475130915642
iteration 63, loss = 0.005961704533547163
iteration 64, loss = 0.005906118545681238
iteration 65, loss = 0.007224430795758963
iteration 66, loss = 0.00930981244891882
iteration 67, loss = 0.004701984114944935
iteration 68, loss = 0.0034790080972015858
iteration 69, loss = 0.004052719101309776
iteration 70, loss = 0.004598383791744709
iteration 71, loss = 0.004602809902280569
iteration 72, loss = 0.004315181635320187
iteration 73, loss = 0.004511220380663872
iteration 74, loss = 0.007161352317780256
iteration 75, loss = 0.004463434685021639
iteration 76, loss = 0.0039022769778966904
iteration 77, loss = 0.004923543892800808
iteration 78, loss = 0.004662571009248495
iteration 79, loss = 0.004910971503704786
iteration 80, loss = 0.0077782971784472466
iteration 81, loss = 0.008129908703267574
iteration 82, loss = 0.004377806093543768
iteration 83, loss = 0.004638130776584148
iteration 84, loss = 0.005205716006457806
iteration 85, loss = 0.005027393810451031
iteration 86, loss = 0.004845932126045227
iteration 87, loss = 0.009206085465848446
iteration 88, loss = 0.004686837084591389
iteration 89, loss = 0.008781958371400833
iteration 90, loss = 0.005178206600248814
iteration 91, loss = 0.004566776566207409
iteration 92, loss = 0.004988185130059719
iteration 93, loss = 0.003961948677897453
iteration 94, loss = 0.004756087437272072
iteration 95, loss = 0.004669876303523779
iteration 96, loss = 0.006012866739183664
iteration 97, loss = 0.005262536928057671
iteration 98, loss = 0.00473753223195672
iteration 99, loss = 0.004832057747989893
iteration 100, loss = 0.004382943268865347
iteration 101, loss = 0.005360139533877373
iteration 102, loss = 0.004283204674720764
iteration 103, loss = 0.00442273635417223
iteration 104, loss = 0.00550311803817749
iteration 105, loss = 0.0052085695788264275
iteration 106, loss = 0.003857644274830818
iteration 107, loss = 0.0038738371804356575
iteration 108, loss = 0.009180305525660515
iteration 109, loss = 0.00450626527890563
iteration 110, loss = 0.006929907947778702
iteration 111, loss = 0.007237398065626621
iteration 112, loss = 0.004980915226042271
iteration 113, loss = 0.0038220316637307405
iteration 114, loss = 0.007115884684026241
iteration 115, loss = 0.0053236279636621475
iteration 116, loss = 0.004799888469278812
iteration 117, loss = 0.004767481703311205
iteration 118, loss = 0.004153897985816002
iteration 119, loss = 0.005356209818273783
iteration 120, loss = 0.00510979862883687
iteration 121, loss = 0.005101257469505072
iteration 122, loss = 0.009718725457787514
iteration 123, loss = 0.0039919717237353325
iteration 124, loss = 0.003998802043497562
iteration 125, loss = 0.0116038229316473
iteration 126, loss = 0.004588473588228226
iteration 127, loss = 0.005482093896716833
iteration 128, loss = 0.004836621694266796
iteration 129, loss = 0.005101604387164116
iteration 130, loss = 0.004540732130408287
iteration 131, loss = 0.0039847856387495995
iteration 132, loss = 0.004502992611378431
iteration 133, loss = 0.005614840425550938
iteration 134, loss = 0.004014409612864256
iteration 135, loss = 0.004018393345177174
iteration 136, loss = 0.005337042734026909
iteration 137, loss = 0.004471641965210438
iteration 138, loss = 0.008032207377254963
iteration 139, loss = 0.0038687579799443483
iteration 140, loss = 0.004947336856275797
iteration 141, loss = 0.006112697999924421
iteration 142, loss = 0.005034209229052067
iteration 143, loss = 0.004021087661385536
iteration 144, loss = 0.006199898198246956
iteration 145, loss = 0.003889627754688263
iteration 146, loss = 0.0044214725494384766
iteration 147, loss = 0.008698351681232452
iteration 148, loss = 0.00472200708463788
iteration 149, loss = 0.004293660633265972
iteration 150, loss = 0.0035727412905544043
iteration 151, loss = 0.003964447416365147
iteration 152, loss = 0.004734072834253311
iteration 153, loss = 0.00405993964523077
iteration 154, loss = 0.00402879761531949
iteration 155, loss = 0.004459739662706852
iteration 156, loss = 0.005972153972834349
iteration 157, loss = 0.005244174040853977
iteration 158, loss = 0.0056807431392371655
iteration 159, loss = 0.004045628476887941
iteration 160, loss = 0.0039527504704892635
iteration 161, loss = 0.004613200202584267
iteration 162, loss = 0.004803324118256569
iteration 163, loss = 0.004408147651702166
iteration 164, loss = 0.004250751342624426
iteration 165, loss = 0.004140917211771011
iteration 166, loss = 0.0037651406601071358
iteration 167, loss = 0.004879605956375599
iteration 168, loss = 0.0048143318854272366
iteration 169, loss = 0.004184979014098644
iteration 170, loss = 0.004575480706989765
iteration 171, loss = 0.0044166408479213715
iteration 172, loss = 0.0036900353152304888
iteration 173, loss = 0.004671324044466019
iteration 174, loss = 0.0065855118446052074
iteration 175, loss = 0.004045550711452961
iteration 176, loss = 0.003626388730481267
iteration 177, loss = 0.004511973354965448
iteration 178, loss = 0.0046900371089577675
iteration 179, loss = 0.003779547056183219
iteration 180, loss = 0.004031177144497633
iteration 181, loss = 0.0037068896926939487
iteration 182, loss = 0.007380343042314053
iteration 183, loss = 0.0035268524661660194
iteration 184, loss = 0.004670060705393553
iteration 185, loss = 0.004613511264324188
iteration 186, loss = 0.004124220926314592
iteration 187, loss = 0.004992432426661253
iteration 188, loss = 0.0044527119025588036
iteration 189, loss = 0.0036992765963077545
iteration 190, loss = 0.004311160184442997
iteration 191, loss = 0.0038051826413720846
iteration 192, loss = 0.004413580521941185
iteration 193, loss = 0.0036711632274091244
iteration 194, loss = 0.0036124903708696365
iteration 195, loss = 0.00587853929027915
iteration 196, loss = 0.005916643887758255
iteration 197, loss = 0.004384475294500589
iteration 198, loss = 0.003957934211939573
iteration 199, loss = 0.004387384280562401
iteration 200, loss = 0.003866579383611679
iteration 201, loss = 0.0036323689855635166
iteration 202, loss = 0.003958765882998705
iteration 203, loss = 0.0035358162131160498
iteration 204, loss = 0.004271948244422674
iteration 205, loss = 0.0037321988493204117
iteration 206, loss = 0.0039055049419403076
iteration 207, loss = 0.004018768202513456
iteration 208, loss = 0.0032604567240923643
iteration 209, loss = 0.00425382237881422
iteration 210, loss = 0.0036915980745106936
iteration 211, loss = 0.0044374847784638405
iteration 212, loss = 0.008600559085607529
iteration 213, loss = 0.004051699303090572
iteration 214, loss = 0.004014087375253439
iteration 215, loss = 0.0037105788942426443
iteration 216, loss = 0.006260376889258623
iteration 217, loss = 0.005408033262938261
iteration 218, loss = 0.00386877846904099
iteration 219, loss = 0.004184207413345575
iteration 220, loss = 0.004514584317803383
iteration 221, loss = 0.0049555362202227116
iteration 222, loss = 0.007781207095831633
iteration 223, loss = 0.004336439538747072
iteration 224, loss = 0.003828869666904211
iteration 225, loss = 0.009968752041459084
iteration 226, loss = 0.003700142726302147
iteration 227, loss = 0.003286084858700633
iteration 228, loss = 0.0035634448286145926
iteration 229, loss = 0.003584518563002348
iteration 230, loss = 0.004282213281840086
iteration 231, loss = 0.004090918693691492
iteration 232, loss = 0.004520019982010126
iteration 233, loss = 0.003548759501427412
iteration 234, loss = 0.003294153604656458
iteration 235, loss = 0.004918744321912527
iteration 236, loss = 0.0038232908118516207
iteration 237, loss = 0.00376842706464231
iteration 238, loss = 0.006395909935235977
iteration 239, loss = 0.0038818337488919497
iteration 240, loss = 0.0064377873204648495
iteration 241, loss = 0.0037647741846740246
iteration 242, loss = 0.007886266335844994
iteration 243, loss = 0.003996846731752157
iteration 244, loss = 0.006455802358686924
iteration 245, loss = 0.006543966941535473
iteration 246, loss = 0.006609075702726841
iteration 247, loss = 0.004735281225293875
iteration 248, loss = 0.004742724820971489
iteration 249, loss = 0.004216877277940512
iteration 250, loss = 0.006709928158670664
iteration 251, loss = 0.0061485921032726765
iteration 252, loss = 0.003413519589230418
iteration 253, loss = 0.005496256519109011
iteration 254, loss = 0.004271135665476322
iteration 255, loss = 0.004181902855634689
iteration 256, loss = 0.00394136318936944
iteration 257, loss = 0.004691797308623791
iteration 258, loss = 0.003452459117397666
iteration 259, loss = 0.0035856140311807394
iteration 260, loss = 0.004261939320713282
iteration 261, loss = 0.004592006094753742
iteration 262, loss = 0.0041449363343417645
iteration 263, loss = 0.0035521492827683687
iteration 264, loss = 0.008196105249226093
iteration 265, loss = 0.004607388749718666
iteration 266, loss = 0.006499181967228651
iteration 267, loss = 0.003559183096513152
iteration 268, loss = 0.004301564767956734
iteration 269, loss = 0.006141413003206253
iteration 270, loss = 0.00543743371963501
iteration 271, loss = 0.005508138798177242
iteration 272, loss = 0.003852453548461199
iteration 273, loss = 0.004892594181001186
iteration 274, loss = 0.009213064797222614
iteration 275, loss = 0.003970927093178034
iteration 276, loss = 0.005179091822355986
iteration 277, loss = 0.008286934345960617
iteration 278, loss = 0.0033705660607665777
iteration 279, loss = 0.0032290283124893904
iteration 280, loss = 0.00373595068231225
iteration 281, loss = 0.0037667257711291313
iteration 282, loss = 0.005272462032735348
iteration 283, loss = 0.00407914025709033
iteration 284, loss = 0.007904637604951859
iteration 285, loss = 0.003702034242451191
iteration 286, loss = 0.004497775342315435
iteration 287, loss = 0.005694829858839512
iteration 288, loss = 0.004795730113983154
iteration 289, loss = 0.006838101428002119
iteration 290, loss = 0.005382233299314976
iteration 291, loss = 0.007634473033249378
iteration 292, loss = 0.003431337419897318
iteration 293, loss = 0.004496030509471893
iteration 294, loss = 0.004492559935897589
iteration 295, loss = 0.0035807762760668993
iteration 296, loss = 0.00746149942278862
iteration 297, loss = 0.0040750689804553986
iteration 298, loss = 0.0034196265041828156
iteration 299, loss = 0.00381237524561584
iteration 300, loss = 0.004298033658415079
iteration 1, loss = 0.004304211121052504
iteration 2, loss = 0.0032522394321858883
iteration 3, loss = 0.0036331326700747013
iteration 4, loss = 0.006031694822013378
iteration 5, loss = 0.00731626246124506
iteration 6, loss = 0.003693913808092475
iteration 7, loss = 0.003930270206183195
iteration 8, loss = 0.003434747224673629
iteration 9, loss = 0.004216452594846487
iteration 10, loss = 0.003986124414950609
iteration 11, loss = 0.007341396529227495
iteration 12, loss = 0.004588019102811813
iteration 13, loss = 0.0036882671993225813
iteration 14, loss = 0.004526366479694843
iteration 15, loss = 0.004043130204081535
iteration 16, loss = 0.0039013512432575226
iteration 17, loss = 0.005845098756253719
iteration 18, loss = 0.004061645362526178
iteration 19, loss = 0.006469356827437878
iteration 20, loss = 0.0075932154431939125
iteration 21, loss = 0.004069877788424492
iteration 22, loss = 0.003573180176317692
iteration 23, loss = 0.0036166198551654816
iteration 24, loss = 0.0047841304913163185
iteration 25, loss = 0.0034566260874271393
iteration 26, loss = 0.004642965272068977
iteration 27, loss = 0.003945210482925177
iteration 28, loss = 0.004161147866398096
iteration 29, loss = 0.004385312087833881
iteration 30, loss = 0.004251000005751848
iteration 31, loss = 0.0039055573288351297
iteration 32, loss = 0.0034658582881093025
iteration 33, loss = 0.007661162875592709
iteration 34, loss = 0.004005498718470335
iteration 35, loss = 0.003393609309569001
iteration 36, loss = 0.0041112517938017845
iteration 37, loss = 0.0034969020634889603
iteration 38, loss = 0.0038399710319936275
iteration 39, loss = 0.0034715018700808287
iteration 40, loss = 0.0039874291978776455
iteration 41, loss = 0.006439859978854656
iteration 42, loss = 0.0031558400951325893
iteration 43, loss = 0.003749293042346835
iteration 44, loss = 0.003949678502976894
iteration 45, loss = 0.004261412192136049
iteration 46, loss = 0.0065138982608914375
iteration 47, loss = 0.004216791596263647
iteration 48, loss = 0.0049077835865318775
iteration 49, loss = 0.006450810935348272
iteration 50, loss = 0.0036811246536672115
iteration 51, loss = 0.0033878327812999487
iteration 52, loss = 0.005656579043716192
iteration 53, loss = 0.0031979160849004984
iteration 54, loss = 0.0038144048303365707
iteration 55, loss = 0.006539040710777044
iteration 56, loss = 0.00529581680893898
iteration 57, loss = 0.004173689056187868
iteration 58, loss = 0.004476801957935095
iteration 59, loss = 0.0032399934716522694
iteration 60, loss = 0.004531245678663254
iteration 61, loss = 0.003484352957457304
iteration 62, loss = 0.004047332797199488
iteration 63, loss = 0.004655693657696247
iteration 64, loss = 0.004241634160280228
iteration 65, loss = 0.004682671744376421
iteration 66, loss = 0.0039378502406179905
iteration 67, loss = 0.004037492908537388
iteration 68, loss = 0.005790155380964279
iteration 69, loss = 0.0036972961388528347
iteration 70, loss = 0.003664345247671008
iteration 71, loss = 0.003466881113126874
iteration 72, loss = 0.0034297287929803133
iteration 73, loss = 0.0046697561629116535
iteration 74, loss = 0.004106577951461077
iteration 75, loss = 0.0034925159998238087
iteration 76, loss = 0.0042223152704536915
iteration 77, loss = 0.0069886259734630585
iteration 78, loss = 0.003367202589288354
iteration 79, loss = 0.0040181661024689674
iteration 80, loss = 0.005164877511560917
iteration 81, loss = 0.004329067654907703
iteration 82, loss = 0.0033632495906203985
iteration 83, loss = 0.0037159689236432314
iteration 84, loss = 0.0038775578141212463
iteration 85, loss = 0.006341680884361267
iteration 86, loss = 0.0033516446128487587
iteration 87, loss = 0.005585852079093456
iteration 88, loss = 0.0041823610663414
iteration 89, loss = 0.004968415945768356
iteration 90, loss = 0.004691708367317915
iteration 91, loss = 0.004088346380740404
iteration 92, loss = 0.0035272985696792603
iteration 93, loss = 0.007730298675596714
iteration 94, loss = 0.0032277461141347885
iteration 95, loss = 0.0038190141785889864
iteration 96, loss = 0.004356663674116135
iteration 97, loss = 0.004659160040318966
iteration 98, loss = 0.004755055531859398
iteration 99, loss = 0.0036618250887840986
iteration 100, loss = 0.004373827483505011
iteration 101, loss = 0.004562690854072571
iteration 102, loss = 0.003628787351772189
iteration 103, loss = 0.003542909864336252
iteration 104, loss = 0.0048929196782410145
iteration 105, loss = 0.005336897913366556
iteration 106, loss = 0.0035235213581472635
iteration 107, loss = 0.0034212409518659115
iteration 108, loss = 0.006972678005695343
iteration 109, loss = 0.006315814331173897
iteration 110, loss = 0.004218851216137409
iteration 111, loss = 0.004865990020334721
iteration 112, loss = 0.003443948458880186
iteration 113, loss = 0.003602697979658842
iteration 114, loss = 0.004204340744763613
iteration 115, loss = 0.00400883611291647
iteration 116, loss = 0.0069199977442622185
iteration 117, loss = 0.0032316716387867928
iteration 118, loss = 0.005125162657350302
iteration 119, loss = 0.003771928371861577
iteration 120, loss = 0.004449227824807167
iteration 121, loss = 0.0034030601382255554
iteration 122, loss = 0.003910068888217211
iteration 123, loss = 0.0036924665328115225
iteration 124, loss = 0.004209563136100769
iteration 125, loss = 0.0033557761926203966
iteration 126, loss = 0.003119545290246606
iteration 127, loss = 0.004133076872676611
iteration 128, loss = 0.0032164333388209343
iteration 129, loss = 0.0037668258883059025
iteration 130, loss = 0.003409769618883729
iteration 131, loss = 0.0042799911461770535
iteration 132, loss = 0.003599300980567932
iteration 133, loss = 0.007801750209182501
iteration 134, loss = 0.0038155638612806797
iteration 135, loss = 0.0031526880338788033
iteration 136, loss = 0.004230210557579994
iteration 137, loss = 0.0032780123874545097
iteration 138, loss = 0.004529722034931183
iteration 139, loss = 0.007332487963140011
iteration 140, loss = 0.004312753211706877
iteration 141, loss = 0.0040068067610263824
iteration 142, loss = 0.0038495734333992004
iteration 143, loss = 0.0034529338590800762
iteration 144, loss = 0.004801926203072071
iteration 145, loss = 0.004400799982249737
iteration 146, loss = 0.004197763744741678
iteration 147, loss = 0.0035993196070194244
iteration 148, loss = 0.0033056684769690037
iteration 149, loss = 0.0035070099402219057
iteration 150, loss = 0.004851114936172962
iteration 151, loss = 0.004560405388474464
iteration 152, loss = 0.0033487703185528517
iteration 153, loss = 0.004877281375229359
iteration 154, loss = 0.003661099588498473
iteration 155, loss = 0.005835543852299452
iteration 156, loss = 0.0032160941045731306
iteration 157, loss = 0.003847361309453845
iteration 158, loss = 0.003465526271611452
iteration 159, loss = 0.0032210389617830515
iteration 160, loss = 0.0037881312891840935
iteration 161, loss = 0.0035437624901533127
iteration 162, loss = 0.0035222964361310005
iteration 163, loss = 0.0051459078676998615
iteration 164, loss = 0.0054318965412676334
iteration 165, loss = 0.004587910603731871
iteration 166, loss = 0.0035932722967118025
iteration 167, loss = 0.004067873582243919
iteration 168, loss = 0.002766486955806613
iteration 169, loss = 0.006054668687283993
iteration 170, loss = 0.0039227195084095
iteration 171, loss = 0.0034441272728145123
iteration 172, loss = 0.003632544307038188
iteration 173, loss = 0.0031867893412709236
iteration 174, loss = 0.003807417117059231
iteration 175, loss = 0.003399164415895939
iteration 176, loss = 0.003613174892961979
iteration 177, loss = 0.0032027256675064564
iteration 178, loss = 0.004511576145887375
iteration 179, loss = 0.003418128238990903
iteration 180, loss = 0.00453406386077404
iteration 181, loss = 0.0057296971790492535
iteration 182, loss = 0.003614103887230158
iteration 183, loss = 0.002996009076014161
iteration 184, loss = 0.003425273345783353
iteration 185, loss = 0.0033492345828562975
iteration 186, loss = 0.0030329807195812464
iteration 187, loss = 0.00386601104401052
iteration 188, loss = 0.0032745390199124813
iteration 189, loss = 0.0030769098084419966
iteration 190, loss = 0.00360307889059186
iteration 191, loss = 0.0063268574886024
iteration 192, loss = 0.0056225270964205265
iteration 193, loss = 0.003847032319754362
iteration 194, loss = 0.003336057299748063
iteration 195, loss = 0.004532477352768183
iteration 196, loss = 0.0048575145192444324
iteration 197, loss = 0.004909709095954895
iteration 198, loss = 0.0036485884338617325
iteration 199, loss = 0.003916489891707897
iteration 200, loss = 0.005474686622619629
iteration 201, loss = 0.0028840932063758373
iteration 202, loss = 0.0047169895842671394
iteration 203, loss = 0.004269969649612904
iteration 204, loss = 0.003390291705727577
iteration 205, loss = 0.004102247767150402
iteration 206, loss = 0.0028885488864034414
iteration 207, loss = 0.0037317986134439707
iteration 208, loss = 0.0029646793846040964
iteration 209, loss = 0.0030859429389238358
iteration 210, loss = 0.004294105805456638
iteration 211, loss = 0.00597121799364686
iteration 212, loss = 0.004074599593877792
iteration 213, loss = 0.004157340619713068
iteration 214, loss = 0.0038134155329316854
iteration 215, loss = 0.003709074342623353
iteration 216, loss = 0.005843126680701971
iteration 217, loss = 0.0034996499307453632
iteration 218, loss = 0.00330526614561677
iteration 219, loss = 0.008651703596115112
iteration 220, loss = 0.003484283806756139
iteration 221, loss = 0.0032428260892629623
iteration 222, loss = 0.0034236956853419542
iteration 223, loss = 0.003991485107690096
iteration 224, loss = 0.005381683818995953
iteration 225, loss = 0.0035905027762055397
iteration 226, loss = 0.003977946937084198
iteration 227, loss = 0.003306735772639513
iteration 228, loss = 0.00295907910913229
iteration 229, loss = 0.0033416280057281256
iteration 230, loss = 0.003341890871524811
iteration 231, loss = 0.003062798175960779
iteration 232, loss = 0.0035474193282425404
iteration 233, loss = 0.003082484006881714
iteration 234, loss = 0.004915039986371994
iteration 235, loss = 0.0035410046111792326
iteration 236, loss = 0.003354666754603386
iteration 237, loss = 0.00346160214394331
iteration 238, loss = 0.003448488423600793
iteration 239, loss = 0.003603125922381878
iteration 240, loss = 0.006280303467065096
iteration 241, loss = 0.003150072880089283
iteration 242, loss = 0.00424266466870904
iteration 243, loss = 0.003588541643694043
iteration 244, loss = 0.0031160200014710426
iteration 245, loss = 0.004185730125755072
iteration 246, loss = 0.0038096029311418533
iteration 247, loss = 0.0038178609684109688
iteration 248, loss = 0.003111688420176506
iteration 249, loss = 0.003563732374459505
iteration 250, loss = 0.0030085211619734764
iteration 251, loss = 0.004222846124321222
iteration 252, loss = 0.0031180211808532476
iteration 253, loss = 0.003659565467387438
iteration 254, loss = 0.006267112214118242
iteration 255, loss = 0.0038137882947921753
iteration 256, loss = 0.003489189315587282
iteration 257, loss = 0.003419227199628949
iteration 258, loss = 0.003317995462566614
iteration 259, loss = 0.004080436658114195
iteration 260, loss = 0.0034517543390393257
iteration 261, loss = 0.0032332930713891983
iteration 262, loss = 0.0033281322102993727
iteration 263, loss = 0.0036332709714770317
iteration 264, loss = 0.003596497466787696
iteration 265, loss = 0.0028914064168930054
iteration 266, loss = 0.0037949038669466972
iteration 267, loss = 0.003487072419375181
iteration 268, loss = 0.0034449934028089046
iteration 269, loss = 0.0035843688528984785
iteration 270, loss = 0.004501920659095049
iteration 271, loss = 0.005062412470579147
iteration 272, loss = 0.0069108400493860245
iteration 273, loss = 0.003520603757351637
iteration 274, loss = 0.0032964483834803104
iteration 275, loss = 0.004398638848215342
iteration 276, loss = 0.003139528911560774
iteration 277, loss = 0.005016739945858717
iteration 278, loss = 0.004840098321437836
iteration 279, loss = 0.0043494850397109985
iteration 280, loss = 0.006697762757539749
iteration 281, loss = 0.007015973329544067
iteration 282, loss = 0.0034148942213505507
iteration 283, loss = 0.0035387726966291666
iteration 284, loss = 0.006411452312022448
iteration 285, loss = 0.0036940895952284336
iteration 286, loss = 0.0049829026684165
iteration 287, loss = 0.004692722577601671
iteration 288, loss = 0.0032956053037196398
iteration 289, loss = 0.0071530393324792385
iteration 290, loss = 0.0032570883631706238
iteration 291, loss = 0.0049171168357133865
iteration 292, loss = 0.0031491885893046856
iteration 293, loss = 0.0038545550778508186
iteration 294, loss = 0.0034286489244550467
iteration 295, loss = 0.0030015273950994015
iteration 296, loss = 0.005484252702444792
iteration 297, loss = 0.0037161633372306824
iteration 298, loss = 0.004639027174562216
iteration 299, loss = 0.003681161440908909
iteration 300, loss = 0.003389242570847273
iteration 1, loss = 0.004197678063064814
iteration 2, loss = 0.005606772843748331
iteration 3, loss = 0.003154836595058441
iteration 4, loss = 0.004274954553693533
iteration 5, loss = 0.003343445248901844
iteration 6, loss = 0.003210207214578986
iteration 7, loss = 0.0028157660271972418
iteration 8, loss = 0.004407205618917942
iteration 9, loss = 0.003049764083698392
iteration 10, loss = 0.003541340585798025
iteration 11, loss = 0.00521183293312788
iteration 12, loss = 0.002788102487102151
iteration 13, loss = 0.003053812775760889
iteration 14, loss = 0.0062247393652796745
iteration 15, loss = 0.003207810688763857
iteration 16, loss = 0.00504265446215868
iteration 17, loss = 0.003818719182163477
iteration 18, loss = 0.0034801035653799772
iteration 19, loss = 0.003908390179276466
iteration 20, loss = 0.0036944125313311815
iteration 21, loss = 0.0027348105795681477
iteration 22, loss = 0.002869582502171397
iteration 23, loss = 0.003085789969190955
iteration 24, loss = 0.005294326227158308
iteration 25, loss = 0.0031573695596307516
iteration 26, loss = 0.0033838239032775164
iteration 27, loss = 0.0031011432874947786
iteration 28, loss = 0.00281010614708066
iteration 29, loss = 0.003625402692705393
iteration 30, loss = 0.0029077185317873955
iteration 31, loss = 0.003713162848725915
iteration 32, loss = 0.0032198247499763966
iteration 33, loss = 0.003150422591716051
iteration 34, loss = 0.00292267301119864
iteration 35, loss = 0.003477152669802308
iteration 36, loss = 0.003805378917604685
iteration 37, loss = 0.003490760223940015
iteration 38, loss = 0.0034095740411430597
iteration 39, loss = 0.0032363508362323046
iteration 40, loss = 0.004428846295922995
iteration 41, loss = 0.0036313480231910944
iteration 42, loss = 0.003344569820910692
iteration 43, loss = 0.003368097823113203
iteration 44, loss = 0.002980712568387389
iteration 45, loss = 0.0073807621374726295
iteration 46, loss = 0.0031966627575457096
iteration 47, loss = 0.00293346936814487
iteration 48, loss = 0.0032800252083688974
iteration 49, loss = 0.00314154801890254
iteration 50, loss = 0.0035514021292328835
iteration 51, loss = 0.003191168187186122
iteration 52, loss = 0.003456556936725974
iteration 53, loss = 0.003225100925192237
iteration 54, loss = 0.00258684647269547
iteration 55, loss = 0.003942474257200956
iteration 56, loss = 0.0033626272343099117
iteration 57, loss = 0.005195708945393562
iteration 58, loss = 0.00319083034992218
iteration 59, loss = 0.0046696909703314304
iteration 60, loss = 0.006071772426366806
iteration 61, loss = 0.003569670021533966
iteration 62, loss = 0.0027807261794805527
iteration 63, loss = 0.004807800520211458
iteration 64, loss = 0.0043334499932825565
iteration 65, loss = 0.0033202627673745155
iteration 66, loss = 0.0033279219642281532
iteration 67, loss = 0.002665797946974635
iteration 68, loss = 0.0061632851138710976
iteration 69, loss = 0.0029118626844137907
iteration 70, loss = 0.0032910790760070086
iteration 71, loss = 0.0030752350576221943
iteration 72, loss = 0.0034777228720486164
iteration 73, loss = 0.003523770719766617
iteration 74, loss = 0.0034907222725450993
iteration 75, loss = 0.0032741620671004057
iteration 76, loss = 0.0026453447062522173
iteration 77, loss = 0.0040222699753940105
iteration 78, loss = 0.003441363573074341
iteration 79, loss = 0.0032856096513569355
iteration 80, loss = 0.0030625953804701567
iteration 81, loss = 0.002786683151498437
iteration 82, loss = 0.003309926949441433
iteration 83, loss = 0.003601131960749626
iteration 84, loss = 0.0032341808546334505
iteration 85, loss = 0.0034987060353159904
iteration 86, loss = 0.009294436313211918
iteration 87, loss = 0.0031587001867592335
iteration 88, loss = 0.0033349874429404736
iteration 89, loss = 0.003997545689344406
iteration 90, loss = 0.004227472934871912
iteration 91, loss = 0.003046605037525296
iteration 92, loss = 0.004279492422938347
iteration 93, loss = 0.003391108475625515
iteration 94, loss = 0.00284207914955914
iteration 95, loss = 0.006059540901333094
iteration 96, loss = 0.003163368906825781
iteration 97, loss = 0.004374672658741474
iteration 98, loss = 0.0026106289587914944
iteration 99, loss = 0.003612767904996872
iteration 100, loss = 0.003379028756171465
iteration 101, loss = 0.0030298358760774136
iteration 102, loss = 0.004371192771941423
iteration 103, loss = 0.002792217070236802
iteration 104, loss = 0.003160669934004545
iteration 105, loss = 0.003315290668979287
iteration 106, loss = 0.004496988840401173
iteration 107, loss = 0.0030112937092781067
iteration 108, loss = 0.0029990638140589
iteration 109, loss = 0.0034014685079455376
iteration 110, loss = 0.003990337252616882
iteration 111, loss = 0.003170951269567013
iteration 112, loss = 0.003822025842964649
iteration 113, loss = 0.0031713899224996567
iteration 114, loss = 0.0028438037261366844
iteration 115, loss = 0.0034350999630987644
iteration 116, loss = 0.004970187321305275
iteration 117, loss = 0.00413745641708374
iteration 118, loss = 0.003338388167321682
iteration 119, loss = 0.003126750234514475
iteration 120, loss = 0.0025385331828147173
iteration 121, loss = 0.006183548830449581
iteration 122, loss = 0.003461228683590889
iteration 123, loss = 0.0028742142021656036
iteration 124, loss = 0.002879102947190404
iteration 125, loss = 0.0029441285878419876
iteration 126, loss = 0.003489577444270253
iteration 127, loss = 0.004285018891096115
iteration 128, loss = 0.003320184536278248
iteration 129, loss = 0.003113386919721961
iteration 130, loss = 0.005551737733185291
iteration 131, loss = 0.004136451054364443
iteration 132, loss = 0.006125593092292547
iteration 133, loss = 0.002855163300409913
iteration 134, loss = 0.004831132013350725
iteration 135, loss = 0.004624524153769016
iteration 136, loss = 0.0029466175474226475
iteration 137, loss = 0.0028344078455120325
iteration 138, loss = 0.004857078194618225
iteration 139, loss = 0.0026523838751018047
iteration 140, loss = 0.0033442953135818243
iteration 141, loss = 0.0036810627207159996
iteration 142, loss = 0.002768684644252062
iteration 143, loss = 0.00338394264690578
iteration 144, loss = 0.0032981252297759056
iteration 145, loss = 0.006819568574428558
iteration 146, loss = 0.002808463294059038
iteration 147, loss = 0.0028378632850944996
iteration 148, loss = 0.0027585404459387064
iteration 149, loss = 0.0027542614843696356
iteration 150, loss = 0.002623294945806265
iteration 151, loss = 0.0037462632171809673
iteration 152, loss = 0.002937723882496357
iteration 153, loss = 0.003634108230471611
iteration 154, loss = 0.003035343950614333
iteration 155, loss = 0.0029395483434200287
iteration 156, loss = 0.0034462823532521725
iteration 157, loss = 0.00504901260137558
iteration 158, loss = 0.002787321340292692
iteration 159, loss = 0.0030607697553932667
iteration 160, loss = 0.0027945619076490402
iteration 161, loss = 0.0037583622615784407
iteration 162, loss = 0.003064461750909686
iteration 163, loss = 0.005610775668174028
iteration 164, loss = 0.003355972236022353
iteration 165, loss = 0.004080341197550297
iteration 166, loss = 0.003127539996057749
iteration 167, loss = 0.003035096451640129
iteration 168, loss = 0.0023822789080441
iteration 169, loss = 0.007149875164031982
iteration 170, loss = 0.0034769605845212936
iteration 171, loss = 0.002695363713428378
iteration 172, loss = 0.003994382917881012
iteration 173, loss = 0.002763558179140091
iteration 174, loss = 0.0028642311226576567
iteration 175, loss = 0.0031147124245762825
iteration 176, loss = 0.002856283914297819
iteration 177, loss = 0.0033050300553441048
iteration 178, loss = 0.0025312425568699837
iteration 179, loss = 0.004741830751299858
iteration 180, loss = 0.0032092509791254997
iteration 181, loss = 0.00292953965254128
iteration 182, loss = 0.003002091310918331
iteration 183, loss = 0.002898953855037689
iteration 184, loss = 0.003557838499546051
iteration 185, loss = 0.0045655095018446445
iteration 186, loss = 0.003950260113924742
iteration 187, loss = 0.0033905545715242624
iteration 188, loss = 0.005459792912006378
iteration 189, loss = 0.0031051086261868477
iteration 190, loss = 0.0032501767855137587
iteration 191, loss = 0.0033383250702172518
iteration 192, loss = 0.00272449292242527
iteration 193, loss = 0.0030597937293350697
iteration 194, loss = 0.002809481928125024
iteration 195, loss = 0.003117633517831564
iteration 196, loss = 0.0032745078206062317
iteration 197, loss = 0.0032516242936253548
iteration 198, loss = 0.0029909720178693533
iteration 199, loss = 0.003481407416984439
iteration 200, loss = 0.0026582072023302317
iteration 201, loss = 0.002679378492757678
iteration 202, loss = 0.003768099006265402
iteration 203, loss = 0.0026023415848612785
iteration 204, loss = 0.0030132231768220663
iteration 205, loss = 0.0025014036800712347
iteration 206, loss = 0.00271361181512475
iteration 207, loss = 0.00472421757876873
iteration 208, loss = 0.0032944045960903168
iteration 209, loss = 0.0029049061704427004
iteration 210, loss = 0.006544103845953941
iteration 211, loss = 0.003600043710321188
iteration 212, loss = 0.003243848215788603
iteration 213, loss = 0.002839307999238372
iteration 214, loss = 0.003399910405278206
iteration 215, loss = 0.0033846325241029263
iteration 216, loss = 0.006692350376397371
iteration 217, loss = 0.0036631759721785784
iteration 218, loss = 0.0029017915949225426
iteration 219, loss = 0.002740604802966118
iteration 220, loss = 0.0025078088510781527
iteration 221, loss = 0.004582972731441259
iteration 222, loss = 0.004188659135252237
iteration 223, loss = 0.005602219607681036
iteration 224, loss = 0.0034405547194182873
iteration 225, loss = 0.0032912699971348047
iteration 226, loss = 0.003224369138479233
iteration 227, loss = 0.0034181324299424887
iteration 228, loss = 0.003774060169234872
iteration 229, loss = 0.0027386671863496304
iteration 230, loss = 0.0027971984818577766
iteration 231, loss = 0.003218539524823427
iteration 232, loss = 0.0028584878891706467
iteration 233, loss = 0.004737995099276304
iteration 234, loss = 0.0033126897178590298
iteration 235, loss = 0.003235854208469391
iteration 236, loss = 0.004392401780933142
iteration 237, loss = 0.005382446572184563
iteration 238, loss = 0.00261353119276464
iteration 239, loss = 0.003075776156038046
iteration 240, loss = 0.004255959298461676
iteration 241, loss = 0.0027820176910609007
iteration 242, loss = 0.002998512703925371
iteration 243, loss = 0.0032590192276984453
iteration 244, loss = 0.003277697367593646
iteration 245, loss = 0.002869762945920229
iteration 246, loss = 0.003070646431297064
iteration 247, loss = 0.00306577212177217
iteration 248, loss = 0.003374355146661401
iteration 249, loss = 0.002963422331959009
iteration 250, loss = 0.0028739050030708313
iteration 251, loss = 0.0031696727965027094
iteration 252, loss = 0.005116621498018503
iteration 253, loss = 0.0031732190400362015
iteration 254, loss = 0.002741802018135786
iteration 255, loss = 0.003425592090934515
iteration 256, loss = 0.004942018538713455
iteration 257, loss = 0.002922241808846593
iteration 258, loss = 0.0028215679340064526
iteration 259, loss = 0.002895843470469117
iteration 260, loss = 0.003119194181635976
iteration 261, loss = 0.0028737138491123915
iteration 262, loss = 0.0033264062367379665
iteration 263, loss = 0.0031816973350942135
iteration 264, loss = 0.006056290119886398
iteration 265, loss = 0.004819086287170649
iteration 266, loss = 0.0026412575971335173
iteration 267, loss = 0.0028184629045426846
iteration 268, loss = 0.00251793023198843
iteration 269, loss = 0.004067455418407917
iteration 270, loss = 0.004972054157406092
iteration 271, loss = 0.0034482257906347513
iteration 272, loss = 0.002810780191794038
iteration 273, loss = 0.0026644861791282892
iteration 274, loss = 0.0038092671893537045
iteration 275, loss = 0.0056818765588104725
iteration 276, loss = 0.0027299586217850447
iteration 277, loss = 0.002521177288144827
iteration 278, loss = 0.0037081565242260695
iteration 279, loss = 0.004821081645786762
iteration 280, loss = 0.0032342998310923576
iteration 281, loss = 0.003009566804394126
iteration 282, loss = 0.002694040536880493
iteration 283, loss = 0.0035488572902977467
iteration 284, loss = 0.0028685403522104025
iteration 285, loss = 0.0030737868510186672
iteration 286, loss = 0.005712843034416437
iteration 287, loss = 0.005416568834334612
iteration 288, loss = 0.003435899969190359
iteration 289, loss = 0.0038098380900919437
iteration 290, loss = 0.0027306536212563515
iteration 291, loss = 0.00385815161280334
iteration 292, loss = 0.0032204079907387495
iteration 293, loss = 0.0044396668672561646
iteration 294, loss = 0.003338805167004466
iteration 295, loss = 0.0031418446451425552
iteration 296, loss = 0.004447292070835829
iteration 297, loss = 0.0034515107981860638
iteration 298, loss = 0.0055830590426921844
iteration 299, loss = 0.0032464182004332542
iteration 300, loss = 0.005105892661958933
iteration 1, loss = 0.0035105012357234955
iteration 2, loss = 0.002759587951004505
iteration 3, loss = 0.002725172322243452
iteration 4, loss = 0.003188690170645714
iteration 5, loss = 0.002576926490291953
iteration 6, loss = 0.0028051957488059998
iteration 7, loss = 0.00416185287758708
iteration 8, loss = 0.0036408286541700363
iteration 9, loss = 0.0025510473642498255
iteration 10, loss = 0.003918671049177647
iteration 11, loss = 0.003044905373826623
iteration 12, loss = 0.002731411485001445
iteration 13, loss = 0.005661017261445522
iteration 14, loss = 0.0051758247427642345
iteration 15, loss = 0.0046699149534106255
iteration 16, loss = 0.003211801405996084
iteration 17, loss = 0.0026681215967983007
iteration 18, loss = 0.003459518076851964
iteration 19, loss = 0.004366626497358084
iteration 20, loss = 0.004934982396662235
iteration 21, loss = 0.002620935207232833
iteration 22, loss = 0.003099021501839161
iteration 23, loss = 0.0031192954629659653
iteration 24, loss = 0.003594926092773676
iteration 25, loss = 0.0025951932184398174
iteration 26, loss = 0.002982066012918949
iteration 27, loss = 0.002595209516584873
iteration 28, loss = 0.005127012263983488
iteration 29, loss = 0.0027854738291352987
iteration 30, loss = 0.0036429213359951973
iteration 31, loss = 0.004565881099551916
iteration 32, loss = 0.003062603995203972
iteration 33, loss = 0.0026048519648611546
iteration 34, loss = 0.004600819665938616
iteration 35, loss = 0.004516206681728363
iteration 36, loss = 0.002622413681820035
iteration 37, loss = 0.0029430282302200794
iteration 38, loss = 0.00322267459705472
iteration 39, loss = 0.00577986054122448
iteration 40, loss = 0.0028110002167522907
iteration 41, loss = 0.0031338424887508154
iteration 42, loss = 0.002811534097418189
iteration 43, loss = 0.0054170722141861916
iteration 44, loss = 0.004027387127280235
iteration 45, loss = 0.0033378342632204294
iteration 46, loss = 0.005505870562046766
iteration 47, loss = 0.0035026161931455135
iteration 48, loss = 0.0025502045173197985
iteration 49, loss = 0.0028713210485875607
iteration 50, loss = 0.0042138355784118176
iteration 51, loss = 0.002867068164050579
iteration 52, loss = 0.0026552516501396894
iteration 53, loss = 0.004199805203825235
iteration 54, loss = 0.004178270231932402
iteration 55, loss = 0.0028994863387197256
iteration 56, loss = 0.0039730495773255825
iteration 57, loss = 0.003714479273185134
iteration 58, loss = 0.002704282756894827
iteration 59, loss = 0.002849469194188714
iteration 60, loss = 0.0024668085388839245
iteration 61, loss = 0.0023773801513016224
iteration 62, loss = 0.004256187938153744
iteration 63, loss = 0.0024963344912976027
iteration 64, loss = 0.004102399107068777
iteration 65, loss = 0.0026561638806015253
iteration 66, loss = 0.0026005427353084087
iteration 67, loss = 0.0030881850980222225
iteration 68, loss = 0.004140306729823351
iteration 69, loss = 0.003604576690122485
iteration 70, loss = 0.0025998728815466166
iteration 71, loss = 0.0026493503246456385
iteration 72, loss = 0.002793689491227269
iteration 73, loss = 0.0026452599558979273
iteration 74, loss = 0.0027240407653152943
iteration 75, loss = 0.0034811031073331833
iteration 76, loss = 0.0028198761865496635
iteration 77, loss = 0.0026600398123264313
iteration 78, loss = 0.0038803033530712128
iteration 79, loss = 0.003131466219201684
iteration 80, loss = 0.0027671493589878082
iteration 81, loss = 0.0056434995494782925
iteration 82, loss = 0.004118402488529682
iteration 83, loss = 0.00313297170214355
iteration 84, loss = 0.0024876452516764402
iteration 85, loss = 0.002672654576599598
iteration 86, loss = 0.002524184063076973
iteration 87, loss = 0.0033221105113625526
iteration 88, loss = 0.002771898405626416
iteration 89, loss = 0.002650047652423382
iteration 90, loss = 0.004492314998060465
iteration 91, loss = 0.002802101196721196
iteration 92, loss = 0.004548212047666311
iteration 93, loss = 0.003270449349656701
iteration 94, loss = 0.002293200697749853
iteration 95, loss = 0.0030508809722959995
iteration 96, loss = 0.0027469368651509285
iteration 97, loss = 0.003706812858581543
iteration 98, loss = 0.004533835686743259
iteration 99, loss = 0.0027030790224671364
iteration 100, loss = 0.0026533082127571106
iteration 101, loss = 0.0036972633097320795
iteration 102, loss = 0.004637367092072964
iteration 103, loss = 0.0035817690659314394
iteration 104, loss = 0.002837922889739275
iteration 105, loss = 0.002776609268039465
iteration 106, loss = 0.00511889485642314
iteration 107, loss = 0.002855690661817789
iteration 108, loss = 0.002308170311152935
iteration 109, loss = 0.00543150445446372
iteration 110, loss = 0.0022514204028993845
iteration 111, loss = 0.0033058435656130314
iteration 112, loss = 0.002819669898599386
iteration 113, loss = 0.005160302389413118
iteration 114, loss = 0.003195693250745535
iteration 115, loss = 0.00314822094514966
iteration 116, loss = 0.002748791128396988
iteration 117, loss = 0.0026927748695015907
iteration 118, loss = 0.002236107597127557
iteration 119, loss = 0.005029400810599327
iteration 120, loss = 0.0022759803105145693
iteration 121, loss = 0.003359387395903468
iteration 122, loss = 0.0046831886284053326
iteration 123, loss = 0.0026630552019923925
iteration 124, loss = 0.0025815812405198812
iteration 125, loss = 0.0024070239160209894
iteration 126, loss = 0.0023520607501268387
iteration 127, loss = 0.003168758237734437
iteration 128, loss = 0.0026108245365321636
iteration 129, loss = 0.002396669937297702
iteration 130, loss = 0.0030452501960098743
iteration 131, loss = 0.002466328674927354
iteration 132, loss = 0.0025901540648192167
iteration 133, loss = 0.0029730491805821657
iteration 134, loss = 0.002648686058819294
iteration 135, loss = 0.0024800405371934175
iteration 136, loss = 0.002743686083704233
iteration 137, loss = 0.002612073440104723
iteration 138, loss = 0.002625830937176943
iteration 139, loss = 0.0033852308988571167
iteration 140, loss = 0.0025862271431833506
iteration 141, loss = 0.0026945131830871105
iteration 142, loss = 0.002584701171144843
iteration 143, loss = 0.004167378880083561
iteration 144, loss = 0.002197750611230731
iteration 145, loss = 0.0028782275039702654
iteration 146, loss = 0.003133832011371851
iteration 147, loss = 0.002508630743250251
iteration 148, loss = 0.0024732567835599184
iteration 149, loss = 0.002289900556206703
iteration 150, loss = 0.003236224874854088
iteration 151, loss = 0.0024792233016341925
iteration 152, loss = 0.0029012011364102364
iteration 153, loss = 0.002725116442888975
iteration 154, loss = 0.0031691070180386305
iteration 155, loss = 0.0031723440624773502
iteration 156, loss = 0.002720961347222328
iteration 157, loss = 0.0025585207622498274
iteration 158, loss = 0.00421766797080636
iteration 159, loss = 0.0032272646203637123
iteration 160, loss = 0.002785841003060341
iteration 161, loss = 0.0027914319653064013
iteration 162, loss = 0.002976566320285201
iteration 163, loss = 0.005482210777699947
iteration 164, loss = 0.0024397859815508127
iteration 165, loss = 0.0026050331071019173
iteration 166, loss = 0.0031794693786650896
iteration 167, loss = 0.002553944941610098
iteration 168, loss = 0.0026289620436728
iteration 169, loss = 0.00263834442012012
iteration 170, loss = 0.002615611534565687
iteration 171, loss = 0.002329994458705187
iteration 172, loss = 0.002621716819703579
iteration 173, loss = 0.002710482571274042
iteration 174, loss = 0.0027638981118798256
iteration 175, loss = 0.0028792009688913822
iteration 176, loss = 0.002344219945371151
iteration 177, loss = 0.0025934195145964622
iteration 178, loss = 0.00296012032777071
iteration 179, loss = 0.0025637648068368435
iteration 180, loss = 0.0027561006136238575
iteration 181, loss = 0.003744910005480051
iteration 182, loss = 0.0029212706722319126
iteration 183, loss = 0.00491662323474884
iteration 184, loss = 0.0024917779956012964
iteration 185, loss = 0.0037501901388168335
iteration 186, loss = 0.002289860974997282
iteration 187, loss = 0.0028182202950119972
iteration 188, loss = 0.0035534605849534273
iteration 189, loss = 0.0032222478184849024
iteration 190, loss = 0.0024667566176503897
iteration 191, loss = 0.002591220196336508
iteration 192, loss = 0.0023948191665112972
iteration 193, loss = 0.0026280933525413275
iteration 194, loss = 0.003745383583009243
iteration 195, loss = 0.0025939601473510265
iteration 196, loss = 0.002952100709080696
iteration 197, loss = 0.002521274611353874
iteration 198, loss = 0.002709138672798872
iteration 199, loss = 0.002226572483778
iteration 200, loss = 0.0036768903955817223
iteration 201, loss = 0.002721281722187996
iteration 202, loss = 0.00393692497164011
iteration 203, loss = 0.002742548007518053
iteration 204, loss = 0.0024132318794727325
iteration 205, loss = 0.003255062736570835
iteration 206, loss = 0.004072817042469978
iteration 207, loss = 0.0038123782724142075
iteration 208, loss = 0.002605845918878913
iteration 209, loss = 0.002538602566346526
iteration 210, loss = 0.0030353539623320103
iteration 211, loss = 0.002221490256488323
iteration 212, loss = 0.004774503875523806
iteration 213, loss = 0.0022260695695877075
iteration 214, loss = 0.0026969260070472956
iteration 215, loss = 0.0031767506152391434
iteration 216, loss = 0.0030568987131118774
iteration 217, loss = 0.0021749278530478477
iteration 218, loss = 0.0035960497334599495
iteration 219, loss = 0.0031416164711117744
iteration 220, loss = 0.0046388981863856316
iteration 221, loss = 0.002679531229659915
iteration 222, loss = 0.005555775947868824
iteration 223, loss = 0.0023981903214007616
iteration 224, loss = 0.002924999687820673
iteration 225, loss = 0.003091022837907076
iteration 226, loss = 0.002546067349612713
iteration 227, loss = 0.0032180193811655045
iteration 228, loss = 0.0024485052563250065
iteration 229, loss = 0.0027699621859937906
iteration 230, loss = 0.004130762070417404
iteration 231, loss = 0.0027522738091647625
iteration 232, loss = 0.0032541113905608654
iteration 233, loss = 0.0028515025041997433
iteration 234, loss = 0.0024677314795553684
iteration 235, loss = 0.0023929134476929903
iteration 236, loss = 0.002930178539827466
iteration 237, loss = 0.003281435463577509
iteration 238, loss = 0.0032705082558095455
iteration 239, loss = 0.0023230828810483217
iteration 240, loss = 0.0036851167678833008
iteration 241, loss = 0.0022891596890985966
iteration 242, loss = 0.003074826905503869
iteration 243, loss = 0.002448745770379901
iteration 244, loss = 0.0024265851825475693
iteration 245, loss = 0.002528079319745302
iteration 246, loss = 0.002354253549128771
iteration 247, loss = 0.0027141044847667217
iteration 248, loss = 0.002838540356606245
iteration 249, loss = 0.0027286841068416834
iteration 250, loss = 0.002533821389079094
iteration 251, loss = 0.002505695214495063
iteration 252, loss = 0.0029115788638591766
iteration 253, loss = 0.0034243983682245016
iteration 254, loss = 0.004748177248984575
iteration 255, loss = 0.002208981430158019
iteration 256, loss = 0.002753770910203457
iteration 257, loss = 0.0024235504679381847
iteration 258, loss = 0.0027326098643243313
iteration 259, loss = 0.0025702714920043945
iteration 260, loss = 0.002366065513342619
iteration 261, loss = 0.002977552358061075
iteration 262, loss = 0.002827354008331895
iteration 263, loss = 0.002861359156668186
iteration 264, loss = 0.004898296669125557
iteration 265, loss = 0.0029224983882158995
iteration 266, loss = 0.0036355317570269108
iteration 267, loss = 0.0030159959569573402
iteration 268, loss = 0.002752948086708784
iteration 269, loss = 0.002742499578744173
iteration 270, loss = 0.0035800859332084656
iteration 271, loss = 0.002452810062095523
iteration 272, loss = 0.0058142878115177155
iteration 273, loss = 0.0027235124725848436
iteration 274, loss = 0.0022734003141522408
iteration 275, loss = 0.003806557273492217
iteration 276, loss = 0.0022563731763511896
iteration 277, loss = 0.002615785924717784
iteration 278, loss = 0.004978742450475693
iteration 279, loss = 0.0020003642421215773
iteration 280, loss = 0.002727273851633072
iteration 281, loss = 0.002732492284849286
iteration 282, loss = 0.0024388921447098255
iteration 283, loss = 0.0022264013532549143
iteration 284, loss = 0.0024496056139469147
iteration 285, loss = 0.002336766105145216
iteration 286, loss = 0.004976444412022829
iteration 287, loss = 0.004394675139337778
iteration 288, loss = 0.0024786561261862516
iteration 289, loss = 0.0026673886459320784
iteration 290, loss = 0.0037563175428658724
iteration 291, loss = 0.0022126645781099796
iteration 292, loss = 0.0023413733579218388
iteration 293, loss = 0.002368711633607745
iteration 294, loss = 0.0030310607980936766
iteration 295, loss = 0.004613843280822039
iteration 296, loss = 0.0026396391913294792
iteration 297, loss = 0.0028054898139089346
iteration 298, loss = 0.0026758373714983463
iteration 299, loss = 0.0028382623568177223
iteration 300, loss = 0.0022549161221832037
iteration 1, loss = 0.002539505250751972
iteration 2, loss = 0.0044758738949894905
iteration 3, loss = 0.004408990032970905
iteration 4, loss = 0.00485521974042058
iteration 5, loss = 0.0021999499294906855
iteration 6, loss = 0.00466848723590374
iteration 7, loss = 0.003970023710280657
iteration 8, loss = 0.00317811849527061
iteration 9, loss = 0.004344580695033073
iteration 10, loss = 0.002772445324808359
iteration 11, loss = 0.0023970892652869225
iteration 12, loss = 0.003207448637112975
iteration 13, loss = 0.0023403011728078127
iteration 14, loss = 0.002085154177621007
iteration 15, loss = 0.0027952753007411957
iteration 16, loss = 0.0028652390465140343
iteration 17, loss = 0.002898643258959055
iteration 18, loss = 0.002532710786908865
iteration 19, loss = 0.0034733228385448456
iteration 20, loss = 0.0027428194880485535
iteration 21, loss = 0.0026957085356116295
iteration 22, loss = 0.0024723580572754145
iteration 23, loss = 0.0024419466499239206
iteration 24, loss = 0.0034199871588498354
iteration 25, loss = 0.0025166799314320087
iteration 26, loss = 0.0019594812765717506
iteration 27, loss = 0.0028019705787301064
iteration 28, loss = 0.0022176518104970455
iteration 29, loss = 0.002442678203806281
iteration 30, loss = 0.002191878156736493
iteration 31, loss = 0.002599559258669615
iteration 32, loss = 0.0021648225374519825
iteration 33, loss = 0.0028089063707739115
iteration 34, loss = 0.002759117167443037
iteration 35, loss = 0.00318676745519042
iteration 36, loss = 0.002562407637014985
iteration 37, loss = 0.0032731471583247185
iteration 38, loss = 0.003491251962259412
iteration 39, loss = 0.0024924154859036207
iteration 40, loss = 0.002562144072726369
iteration 41, loss = 0.002503438154235482
iteration 42, loss = 0.006172188092023134
iteration 43, loss = 0.0024731457233428955
iteration 44, loss = 0.003441013628616929
iteration 45, loss = 0.002107626525685191
iteration 46, loss = 0.0027885339222848415
iteration 47, loss = 0.002743207383900881
iteration 48, loss = 0.003032298292964697
iteration 49, loss = 0.0033048081677407026
iteration 50, loss = 0.002587701426818967
iteration 51, loss = 0.002485805656760931
iteration 52, loss = 0.0023508539889007807
iteration 53, loss = 0.00286513427272439
iteration 54, loss = 0.0024848650209605694
iteration 55, loss = 0.002294692676514387
iteration 56, loss = 0.002042773412540555
iteration 57, loss = 0.0020784055814146996
iteration 58, loss = 0.002258408349007368
iteration 59, loss = 0.002196038141846657
iteration 60, loss = 0.002906280104070902
iteration 61, loss = 0.004588388372212648
iteration 62, loss = 0.002435870934277773
iteration 63, loss = 0.003035468515008688
iteration 64, loss = 0.002501813694834709
iteration 65, loss = 0.0029954491183161736
iteration 66, loss = 0.002170538529753685
iteration 67, loss = 0.0028503306675702333
iteration 68, loss = 0.0022848243825137615
iteration 69, loss = 0.0028698421083390713
iteration 70, loss = 0.0023715971037745476
iteration 71, loss = 0.0025027235969901085
iteration 72, loss = 0.002897111000493169
iteration 73, loss = 0.004583475645631552
iteration 74, loss = 0.0026814178563654423
iteration 75, loss = 0.0026832858566194773
iteration 76, loss = 0.0032695550471544266
iteration 77, loss = 0.0019568714778870344
iteration 78, loss = 0.0032018718775361776
iteration 79, loss = 0.0034734010696411133
iteration 80, loss = 0.0025047725066542625
iteration 81, loss = 0.0022434708662331104
iteration 82, loss = 0.0022788611240684986
iteration 83, loss = 0.002191290259361267
iteration 84, loss = 0.00326384324580431
iteration 85, loss = 0.002384318970143795
iteration 86, loss = 0.0034994720481336117
iteration 87, loss = 0.0038822260685265064
iteration 88, loss = 0.002890863688662648
iteration 89, loss = 0.002314187353476882
iteration 90, loss = 0.0023416532203555107
iteration 91, loss = 0.0022571198642253876
iteration 92, loss = 0.0025217013899236917
iteration 93, loss = 0.0032372488640248775
iteration 94, loss = 0.0022632640320807695
iteration 95, loss = 0.0022649792954325676
iteration 96, loss = 0.0024354022461920977
iteration 97, loss = 0.003487157169729471
iteration 98, loss = 0.002177343936637044
iteration 99, loss = 0.004848497454077005
iteration 100, loss = 0.0021828515455126762
iteration 101, loss = 0.0026259594596922398
iteration 102, loss = 0.002042435109615326
iteration 103, loss = 0.0025061354972422123
iteration 104, loss = 0.0020241825841367245
iteration 105, loss = 0.00315890577621758
iteration 106, loss = 0.0023731666151434183
iteration 107, loss = 0.0026293976698070765
iteration 108, loss = 0.0023547299206256866
iteration 109, loss = 0.002371350070461631
iteration 110, loss = 0.0031715771183371544
iteration 111, loss = 0.002445799298584461
iteration 112, loss = 0.0022878958843648434
iteration 113, loss = 0.0025458442978560925
iteration 114, loss = 0.0025685664732009172
iteration 115, loss = 0.002855516504496336
iteration 116, loss = 0.0024087238125503063
iteration 117, loss = 0.002732730470597744
iteration 118, loss = 0.004063453990966082
iteration 119, loss = 0.00209517078474164
iteration 120, loss = 0.0022196986246854067
iteration 121, loss = 0.0023528181482106447
iteration 122, loss = 0.0023896393831819296
iteration 123, loss = 0.002801928436383605
iteration 124, loss = 0.002550768665969372
iteration 125, loss = 0.0019178709480911493
iteration 126, loss = 0.0034209410659968853
iteration 127, loss = 0.0021678595803678036
iteration 128, loss = 0.002683216705918312
iteration 129, loss = 0.0023942412808537483
iteration 130, loss = 0.0027562580071389675
iteration 131, loss = 0.00311968638561666
iteration 132, loss = 0.002324759727343917
iteration 133, loss = 0.0029071453027427197
iteration 134, loss = 0.002626679837703705
iteration 135, loss = 0.0032234564423561096
iteration 136, loss = 0.0027212882414460182
iteration 137, loss = 0.0023579117842018604
iteration 138, loss = 0.0026666675694286823
iteration 139, loss = 0.0021336167119443417
iteration 140, loss = 0.0026876211632043123
iteration 141, loss = 0.002573359990492463
iteration 142, loss = 0.0030532064847648144
iteration 143, loss = 0.00235956278629601
iteration 144, loss = 0.0025566413532942533
iteration 145, loss = 0.0035326690413057804
iteration 146, loss = 0.0029333196580410004
iteration 147, loss = 0.0022046086378395557
iteration 148, loss = 0.002311995718628168
iteration 149, loss = 0.004401588346809149
iteration 150, loss = 0.002503000432625413
iteration 151, loss = 0.0035952760372310877
iteration 152, loss = 0.004456688184291124
iteration 153, loss = 0.002187960548326373
iteration 154, loss = 0.003531800117343664
iteration 155, loss = 0.0026322025805711746
iteration 156, loss = 0.0040688300505280495
iteration 157, loss = 0.002320730360224843
iteration 158, loss = 0.0021607340313494205
iteration 159, loss = 0.0024048599880188704
iteration 160, loss = 0.004667946603149176
iteration 161, loss = 0.0024047100450843573
iteration 162, loss = 0.002127520740032196
iteration 163, loss = 0.002525593154132366
iteration 164, loss = 0.002848199103027582
iteration 165, loss = 0.002405662788078189
iteration 166, loss = 0.0022368680220097303
iteration 167, loss = 0.0024694548919796944
iteration 168, loss = 0.0023083030246198177
iteration 169, loss = 0.0023472695611417294
iteration 170, loss = 0.002836957573890686
iteration 171, loss = 0.0020169257186353207
iteration 172, loss = 0.0023967954330146313
iteration 173, loss = 0.0024932704400271177
iteration 174, loss = 0.0026155475061386824
iteration 175, loss = 0.007287506014108658
iteration 176, loss = 0.0022670794278383255
iteration 177, loss = 0.0029639939311891794
iteration 178, loss = 0.001881095115095377
iteration 179, loss = 0.0021455371752381325
iteration 180, loss = 0.002298701787367463
iteration 181, loss = 0.004721881356090307
iteration 182, loss = 0.002252106787636876
iteration 183, loss = 0.0023675933480262756
iteration 184, loss = 0.0026704384945333004
iteration 185, loss = 0.0026592507492750883
iteration 186, loss = 0.0019049049587920308
iteration 187, loss = 0.0037258281372487545
iteration 188, loss = 0.0042494554072618484
iteration 189, loss = 0.003062936943024397
iteration 190, loss = 0.0020635381806641817
iteration 191, loss = 0.004679402336478233
iteration 192, loss = 0.002529286779463291
iteration 193, loss = 0.002456942107528448
iteration 194, loss = 0.0029222420416772366
iteration 195, loss = 0.004453687462955713
iteration 196, loss = 0.002492230851203203
iteration 197, loss = 0.0027316499035805464
iteration 198, loss = 0.005997383967041969
iteration 199, loss = 0.0021955748088657856
iteration 200, loss = 0.0030962126329541206
iteration 201, loss = 0.002417181385681033
iteration 202, loss = 0.0024063934106379747
iteration 203, loss = 0.002185957506299019
iteration 204, loss = 0.002302654553204775
iteration 205, loss = 0.002025960013270378
iteration 206, loss = 0.0026383381336927414
iteration 207, loss = 0.0021377168595790863
iteration 208, loss = 0.0022350517101585865
iteration 209, loss = 0.0022441318724304438
iteration 210, loss = 0.004556590691208839
iteration 211, loss = 0.003968691918998957
iteration 212, loss = 0.0018691596342250705
iteration 213, loss = 0.0024853378999978304
iteration 214, loss = 0.002520458772778511
iteration 215, loss = 0.002395111136138439
iteration 216, loss = 0.0025742272846400738
iteration 217, loss = 0.0022962279617786407
iteration 218, loss = 0.003032038686797023
iteration 219, loss = 0.0023871054872870445
iteration 220, loss = 0.003921871073544025
iteration 221, loss = 0.002204948803409934
iteration 222, loss = 0.0031210794113576412
iteration 223, loss = 0.0018984542693942785
iteration 224, loss = 0.0023996401578187943
iteration 225, loss = 0.0019660387188196182
iteration 226, loss = 0.002272189361974597
iteration 227, loss = 0.002555891638621688
iteration 228, loss = 0.0038932552561163902
iteration 229, loss = 0.0019686829764395952
iteration 230, loss = 0.002106861909851432
iteration 231, loss = 0.002989677246659994
iteration 232, loss = 0.002445434918627143
iteration 233, loss = 0.0026775472797453403
iteration 234, loss = 0.002516571432352066
iteration 235, loss = 0.0020600585266947746
iteration 236, loss = 0.002227327786386013
iteration 237, loss = 0.002267864067107439
iteration 238, loss = 0.0020969086326658726
iteration 239, loss = 0.001930450089275837
iteration 240, loss = 0.00234510051086545
iteration 241, loss = 0.0028046718798577785
iteration 242, loss = 0.003463169327005744
iteration 243, loss = 0.002694603055715561
iteration 244, loss = 0.002051794435828924
iteration 245, loss = 0.0019424736965447664
iteration 246, loss = 0.003449114039540291
iteration 247, loss = 0.002665905747562647
iteration 248, loss = 0.0023352804128080606
iteration 249, loss = 0.002461413387209177
iteration 250, loss = 0.002391684101894498
iteration 251, loss = 0.0019895797595381737
iteration 252, loss = 0.002188102575019002
iteration 253, loss = 0.0028370902873575687
iteration 254, loss = 0.002457191003486514
iteration 255, loss = 0.0021284790709614754
iteration 256, loss = 0.0023489189334213734
iteration 257, loss = 0.0025456829462200403
iteration 258, loss = 0.002164838369935751
iteration 259, loss = 0.002463137498125434
iteration 260, loss = 0.0016763468738645315
iteration 261, loss = 0.002482461277395487
iteration 262, loss = 0.003700701054185629
iteration 263, loss = 0.0024884892627596855
iteration 264, loss = 0.0027839087415486574
iteration 265, loss = 0.0025432331021875143
iteration 266, loss = 0.0030680387280881405
iteration 267, loss = 0.004367041867226362
iteration 268, loss = 0.002090793801471591
iteration 269, loss = 0.0026095048524439335
iteration 270, loss = 0.0020328988321125507
iteration 271, loss = 0.002195515437051654
iteration 272, loss = 0.004929632879793644
iteration 273, loss = 0.001990312710404396
iteration 274, loss = 0.002302988665178418
iteration 275, loss = 0.0023070655297487974
iteration 276, loss = 0.0036797437351197004
iteration 277, loss = 0.0037653567269444466
iteration 278, loss = 0.0021463874727487564
iteration 279, loss = 0.001963431481271982
iteration 280, loss = 0.0019407406216487288
iteration 281, loss = 0.0024909530766308308
iteration 282, loss = 0.003732877317816019
iteration 283, loss = 0.0023170930799096823
iteration 284, loss = 0.0026452327147126198
iteration 285, loss = 0.004194829612970352
iteration 286, loss = 0.0022950165439397097
iteration 287, loss = 0.002980850636959076
iteration 288, loss = 0.0023210602812469006
iteration 289, loss = 0.002623911714181304
iteration 290, loss = 0.0023524609860032797
iteration 291, loss = 0.0019094381714239717
iteration 292, loss = 0.0024602056946605444
iteration 293, loss = 0.0038537406362593174
iteration 294, loss = 0.002158964052796364
iteration 295, loss = 0.0026643809396773577
iteration 296, loss = 0.00199418724514544
iteration 297, loss = 0.0026658293791115284
iteration 298, loss = 0.004121574107557535
iteration 299, loss = 0.002954112598672509
iteration 300, loss = 0.002334480406716466
iteration 1, loss = 0.002054705284535885
iteration 2, loss = 0.001969399629160762
iteration 3, loss = 0.0021619852632284164
iteration 4, loss = 0.0028002546168863773
iteration 5, loss = 0.002446768106892705
iteration 6, loss = 0.0021659242920577526
iteration 7, loss = 0.002370234113186598
iteration 8, loss = 0.0022887729573994875
iteration 9, loss = 0.002102383179590106
iteration 10, loss = 0.0026715146377682686
iteration 11, loss = 0.00182129826862365
iteration 12, loss = 0.002342761494219303
iteration 13, loss = 0.002009417861700058
iteration 14, loss = 0.0024020010605454445
iteration 15, loss = 0.002531724749132991
iteration 16, loss = 0.0023755584843456745
iteration 17, loss = 0.0023968524765223265
iteration 18, loss = 0.0021084649488329887
iteration 19, loss = 0.00264775101095438
iteration 20, loss = 0.0022046295925974846
iteration 21, loss = 0.0018910442013293505
iteration 22, loss = 0.00196988508105278
iteration 23, loss = 0.002271815203130245
iteration 24, loss = 0.0030905622988939285
iteration 25, loss = 0.0022245633881539106
iteration 26, loss = 0.0028946951497346163
iteration 27, loss = 0.0024008508771657944
iteration 28, loss = 0.0026471756864339113
iteration 29, loss = 0.002146264770999551
iteration 30, loss = 0.0022832974791526794
iteration 31, loss = 0.0022389162331819534
iteration 32, loss = 0.0027014983352273703
iteration 33, loss = 0.0020126434974372387
iteration 34, loss = 0.0022310661152005196
iteration 35, loss = 0.0023382091894745827
iteration 36, loss = 0.0017972355708479881
iteration 37, loss = 0.002615873236209154
iteration 38, loss = 0.0019262622809037566
iteration 39, loss = 0.0017472298350185156
iteration 40, loss = 0.002308711176738143
iteration 41, loss = 0.002176875714212656
iteration 42, loss = 0.0026527591980993748
iteration 43, loss = 0.003140753135085106
iteration 44, loss = 0.0024593255948275328
iteration 45, loss = 0.0019486001692712307
iteration 46, loss = 0.0026671630330383778
iteration 47, loss = 0.003550210501998663
iteration 48, loss = 0.0024430372286587954
iteration 49, loss = 0.0022544520907104015
iteration 50, loss = 0.0019252186175435781
iteration 51, loss = 0.0022249475587159395
iteration 52, loss = 0.0027689917478710413
iteration 53, loss = 0.0023572708014398813
iteration 54, loss = 0.0030966359190642834
iteration 55, loss = 0.002140655415132642
iteration 56, loss = 0.00201883795671165
iteration 57, loss = 0.0027280086651444435
iteration 58, loss = 0.0029902758542448282
iteration 59, loss = 0.004361455328762531
iteration 60, loss = 0.002180664334446192
iteration 61, loss = 0.002507296623662114
iteration 62, loss = 0.002673147711902857
iteration 63, loss = 0.0033539575524628162
iteration 64, loss = 0.002735244808718562
iteration 65, loss = 0.0021390190813690424
iteration 66, loss = 0.002354829339310527
iteration 67, loss = 0.0022460580803453922
iteration 68, loss = 0.0043240864761173725
iteration 69, loss = 0.0016362747410312295
iteration 70, loss = 0.0032875598408281803
iteration 71, loss = 0.0025996165350079536
iteration 72, loss = 0.002593269106000662
iteration 73, loss = 0.001966336742043495
iteration 74, loss = 0.001993385376408696
iteration 75, loss = 0.0026761197950690985
iteration 76, loss = 0.0017907947767525911
iteration 77, loss = 0.002808069810271263
iteration 78, loss = 0.002881124150007963
iteration 79, loss = 0.003275300608947873
iteration 80, loss = 0.0024222589563578367
iteration 81, loss = 0.0018335975473746657
iteration 82, loss = 0.0023076841607689857
iteration 83, loss = 0.002897724974900484
iteration 84, loss = 0.0029415376484394073
iteration 85, loss = 0.0026500027161091566
iteration 86, loss = 0.0026168222539126873
iteration 87, loss = 0.002582643646746874
iteration 88, loss = 0.003604469122365117
iteration 89, loss = 0.0020386199466884136
iteration 90, loss = 0.0021504261530935764
iteration 91, loss = 0.001931648817844689
iteration 92, loss = 0.003032602369785309
iteration 93, loss = 0.0031382509041577578
iteration 94, loss = 0.0022692354395985603
iteration 95, loss = 0.0023291900288313627
iteration 96, loss = 0.0038681216537952423
iteration 97, loss = 0.0017329377587884665
iteration 98, loss = 0.002182327676564455
iteration 99, loss = 0.0020945570431649685
iteration 100, loss = 0.002100420882925391
iteration 101, loss = 0.001991906436160207
iteration 102, loss = 0.003665101481601596
iteration 103, loss = 0.004185030702501535
iteration 104, loss = 0.004343721549957991
iteration 105, loss = 0.002448625862598419
iteration 106, loss = 0.003112522419542074
iteration 107, loss = 0.0028671128675341606
iteration 108, loss = 0.002879104809835553
iteration 109, loss = 0.0021290320437401533
iteration 110, loss = 0.004560136701911688
iteration 111, loss = 0.0023988690227270126
iteration 112, loss = 0.0031140591017901897
iteration 113, loss = 0.002386419102549553
iteration 114, loss = 0.0019205586286261678
iteration 115, loss = 0.0020682222675532103
iteration 116, loss = 0.0018914365209639072
iteration 117, loss = 0.0019241032423451543
iteration 118, loss = 0.001875771675258875
iteration 119, loss = 0.0045868828892707825
iteration 120, loss = 0.0025661801919341087
iteration 121, loss = 0.0036373711191117764
iteration 122, loss = 0.0025639343075454235
iteration 123, loss = 0.0025911740958690643
iteration 124, loss = 0.005426577292382717
iteration 125, loss = 0.002491205930709839
iteration 126, loss = 0.003084595547989011
iteration 127, loss = 0.0023646033369004726
iteration 128, loss = 0.003194049932062626
iteration 129, loss = 0.0019114251481369138
iteration 130, loss = 0.0024291910231113434
iteration 131, loss = 0.0022512543946504593
iteration 132, loss = 0.00365216052159667
iteration 133, loss = 0.0019461826886981726
iteration 134, loss = 0.00437428243458271
iteration 135, loss = 0.0020357302855700254
iteration 136, loss = 0.0030790879391133785
iteration 137, loss = 0.002302988898009062
iteration 138, loss = 0.0023565548472106457
iteration 139, loss = 0.002221672795712948
iteration 140, loss = 0.0023398082703351974
iteration 141, loss = 0.002167870756238699
iteration 142, loss = 0.002646277891471982
iteration 143, loss = 0.0023630945943295956
iteration 144, loss = 0.002964832354336977
iteration 145, loss = 0.002011733828112483
iteration 146, loss = 0.0019101148936897516
iteration 147, loss = 0.002430131658911705
iteration 148, loss = 0.00396356638520956
iteration 149, loss = 0.002616718178614974
iteration 150, loss = 0.0024225455708801746
iteration 151, loss = 0.001986440271139145
iteration 152, loss = 0.00346635770983994
iteration 153, loss = 0.004325555171817541
iteration 154, loss = 0.0022141074296087027
iteration 155, loss = 0.002012526150792837
iteration 156, loss = 0.004021278116852045
iteration 157, loss = 0.0025302469730377197
iteration 158, loss = 0.002198594156652689
iteration 159, loss = 0.002636256394907832
iteration 160, loss = 0.003067584242671728
iteration 161, loss = 0.004298856947571039
iteration 162, loss = 0.0042603895999491215
iteration 163, loss = 0.0025364584289491177
iteration 164, loss = 0.002328940201550722
iteration 165, loss = 0.00252044340595603
iteration 166, loss = 0.00306121539324522
iteration 167, loss = 0.002068656263872981
iteration 168, loss = 0.0030773936305195093
iteration 169, loss = 0.003166854614391923
iteration 170, loss = 0.0019421736942604184
iteration 171, loss = 0.003007354913279414
iteration 172, loss = 0.001896236906759441
iteration 173, loss = 0.0029946397989988327
iteration 174, loss = 0.0020201709121465683
iteration 175, loss = 0.0023388981353491545
iteration 176, loss = 0.0022437928710132837
iteration 177, loss = 0.0026534348726272583
iteration 178, loss = 0.002440862124785781
iteration 179, loss = 0.0027664266526699066
iteration 180, loss = 0.002396974479779601
iteration 181, loss = 0.00437857024371624
iteration 182, loss = 0.004143386147916317
iteration 183, loss = 0.002419327385723591
iteration 184, loss = 0.002875494072213769
iteration 185, loss = 0.0025896301958709955
iteration 186, loss = 0.0023467731662094593
iteration 187, loss = 0.0029104582499712706
iteration 188, loss = 0.0023640990257263184
iteration 189, loss = 0.002207786776125431
iteration 190, loss = 0.0021498468704521656
iteration 191, loss = 0.0028515879530459642
iteration 192, loss = 0.0020917048677802086
iteration 193, loss = 0.0024032602086663246
iteration 194, loss = 0.0018824634607881308
iteration 195, loss = 0.0030157382134348154
iteration 196, loss = 0.0032639303244650364
iteration 197, loss = 0.0026125037111341953
iteration 198, loss = 0.00307168485596776
iteration 199, loss = 0.001961482921615243
iteration 200, loss = 0.002338938182219863
iteration 201, loss = 0.002114467089995742
iteration 202, loss = 0.0021399876568466425
iteration 203, loss = 0.0021292229648679495
iteration 204, loss = 0.0021712230518460274
iteration 205, loss = 0.0025893300771713257
iteration 206, loss = 0.0033113795798271894
iteration 207, loss = 0.0017765017691999674
iteration 208, loss = 0.002539064036682248
iteration 209, loss = 0.0023531271144747734
iteration 210, loss = 0.003249972593039274
iteration 211, loss = 0.002802071627229452
iteration 212, loss = 0.001891294727101922
iteration 213, loss = 0.0030696250032633543
iteration 214, loss = 0.001876667607575655
iteration 215, loss = 0.0019512996077537537
iteration 216, loss = 0.002303163055330515
iteration 217, loss = 0.002311212010681629
iteration 218, loss = 0.0021816566586494446
iteration 219, loss = 0.0020089156460016966
iteration 220, loss = 0.0028236794751137495
iteration 221, loss = 0.0019155574264004827
iteration 222, loss = 0.0023732087574899197
iteration 223, loss = 0.0024391862098127604
iteration 224, loss = 0.002065625274553895
iteration 225, loss = 0.0021923338063061237
iteration 226, loss = 0.0020376977045089006
iteration 227, loss = 0.003077337983995676
iteration 228, loss = 0.0024228240363299847
iteration 229, loss = 0.0034914538264274597
iteration 230, loss = 0.0018672628793865442
iteration 231, loss = 0.0022155942860990763
iteration 232, loss = 0.0033420100808143616
iteration 233, loss = 0.002281677210703492
iteration 234, loss = 0.002410700311884284
iteration 235, loss = 0.004451454151421785
iteration 236, loss = 0.0020919404923915863
iteration 237, loss = 0.0019382752943783998
iteration 238, loss = 0.0022081020288169384
iteration 239, loss = 0.0031353796366602182
iteration 240, loss = 0.0020412197336554527
iteration 241, loss = 0.001989941578358412
iteration 242, loss = 0.0025691657792776823
iteration 243, loss = 0.0041258931159973145
iteration 244, loss = 0.005759130232036114
iteration 245, loss = 0.002018736908212304
iteration 246, loss = 0.002059792634099722
iteration 247, loss = 0.0019824914634227753
iteration 248, loss = 0.002018124796450138
iteration 249, loss = 0.0026469414588063955
iteration 250, loss = 0.0029208187479525805
iteration 251, loss = 0.001954901497811079
iteration 252, loss = 0.001829079003073275
iteration 253, loss = 0.002520148176699877
iteration 254, loss = 0.002443106845021248
iteration 255, loss = 0.0031354816164821386
iteration 256, loss = 0.0024234072770923376
iteration 257, loss = 0.0025889563839882612
iteration 258, loss = 0.0023931178729981184
iteration 259, loss = 0.001866256003268063
iteration 260, loss = 0.003122912021353841
iteration 261, loss = 0.0022678526584059
iteration 262, loss = 0.003868367290124297
iteration 263, loss = 0.0029243805911391973
iteration 264, loss = 0.002278734464198351
iteration 265, loss = 0.002038858598098159
iteration 266, loss = 0.0021323305554687977
iteration 267, loss = 0.002717578550800681
iteration 268, loss = 0.003780432976782322
iteration 269, loss = 0.0018252020236104727
iteration 270, loss = 0.004295383580029011
iteration 271, loss = 0.0020754244178533554
iteration 272, loss = 0.0016793152317404747
iteration 273, loss = 0.0022718326654285192
iteration 274, loss = 0.0026428818237036467
iteration 275, loss = 0.0021725986152887344
iteration 276, loss = 0.004311128985136747
iteration 277, loss = 0.0022190609015524387
iteration 278, loss = 0.0022560576908290386
iteration 279, loss = 0.002263573696836829
iteration 280, loss = 0.0018706588307395577
iteration 281, loss = 0.002504594624042511
iteration 282, loss = 0.0021301591768860817
iteration 283, loss = 0.002062276704236865
iteration 284, loss = 0.0023988191969692707
iteration 285, loss = 0.0024139436427503824
iteration 286, loss = 0.002161184325814247
iteration 287, loss = 0.0022300786804407835
iteration 288, loss = 0.0018460783176124096
iteration 289, loss = 0.004312298260629177
iteration 290, loss = 0.0033738436177372932
iteration 291, loss = 0.002083344617858529
iteration 292, loss = 0.002263180213049054
iteration 293, loss = 0.0028377254493534565
iteration 294, loss = 0.002205177675932646
iteration 295, loss = 0.0020983137656003237
iteration 296, loss = 0.0025190336164087057
iteration 297, loss = 0.0027202125638723373
iteration 298, loss = 0.0028394008986651897
iteration 299, loss = 0.0025791022926568985
iteration 300, loss = 0.002329884795472026
iteration 1, loss = 0.002053548814728856
iteration 2, loss = 0.003347540507093072
iteration 3, loss = 0.00295026833191514
iteration 4, loss = 0.0021773239132016897
iteration 5, loss = 0.003041442483663559
iteration 6, loss = 0.002023035427555442
iteration 7, loss = 0.003177095903083682
iteration 8, loss = 0.002045293338596821
iteration 9, loss = 0.002012406475841999
iteration 10, loss = 0.0034099039621651173
iteration 11, loss = 0.00260647083632648
iteration 12, loss = 0.002492455765604973
iteration 13, loss = 0.0018353099003434181
iteration 14, loss = 0.0022452257107943296
iteration 15, loss = 0.0025125921238213778
iteration 16, loss = 0.0019108849810436368
iteration 17, loss = 0.006473703775554895
iteration 18, loss = 0.002097155200317502
iteration 19, loss = 0.002866702852770686
iteration 20, loss = 0.0017970972694456577
iteration 21, loss = 0.0032181290443986654
iteration 22, loss = 0.0021433127112686634
iteration 23, loss = 0.004258312284946442
iteration 24, loss = 0.00379824242554605
iteration 25, loss = 0.0024437534157186747
iteration 26, loss = 0.003780034137889743
iteration 27, loss = 0.0023387547116726637
iteration 28, loss = 0.0024259150959551334
iteration 29, loss = 0.0019568209536373615
iteration 30, loss = 0.0024357493966817856
iteration 31, loss = 0.0018924182513728738
iteration 32, loss = 0.002969736233353615
iteration 33, loss = 0.002912403317168355
iteration 34, loss = 0.0024275791365653276
iteration 35, loss = 0.0023103649728000164
iteration 36, loss = 0.0039014858193695545
iteration 37, loss = 0.0034672555048018694
iteration 38, loss = 0.0025605536065995693
iteration 39, loss = 0.002481277333572507
iteration 40, loss = 0.0021481080912053585
iteration 41, loss = 0.0021039482671767473
iteration 42, loss = 0.0026709921658039093
iteration 43, loss = 0.0022814497351646423
iteration 44, loss = 0.0025856089778244495
iteration 45, loss = 0.0027471319772303104
iteration 46, loss = 0.0018870558124035597
iteration 47, loss = 0.0025615100748836994
iteration 48, loss = 0.0033401967957615852
iteration 49, loss = 0.0027402660343796015
iteration 50, loss = 0.002219923073425889
iteration 51, loss = 0.0021840562112629414
iteration 52, loss = 0.0029696556739509106
iteration 53, loss = 0.0021934318356215954
iteration 54, loss = 0.002472873777151108
iteration 55, loss = 0.0020289586391299963
iteration 56, loss = 0.002592324512079358
iteration 57, loss = 0.0024275698233395815
iteration 58, loss = 0.0019950191490352154
iteration 59, loss = 0.0021901896689087152
iteration 60, loss = 0.0029106084257364273
iteration 61, loss = 0.0028371692169457674
iteration 62, loss = 0.004205706529319286
iteration 63, loss = 0.0046150521375238895
iteration 64, loss = 0.001962075475603342
iteration 65, loss = 0.0020778123289346695
iteration 66, loss = 0.002367153763771057
iteration 67, loss = 0.004327994771301746
iteration 68, loss = 0.0028523763176053762
iteration 69, loss = 0.002043650019913912
iteration 70, loss = 0.002939886413514614
iteration 71, loss = 0.0021510154474526644
iteration 72, loss = 0.002356364857405424
iteration 73, loss = 0.004460275173187256
iteration 74, loss = 0.0027398739475756884
iteration 75, loss = 0.0017610191134735942
iteration 76, loss = 0.0024328900035470724
iteration 77, loss = 0.001883867895230651
iteration 78, loss = 0.002342876745387912
iteration 79, loss = 0.003385507967323065
iteration 80, loss = 0.0019248193129897118
iteration 81, loss = 0.0019197568763047457
iteration 82, loss = 0.002491607563570142
iteration 83, loss = 0.0018899819115176797
iteration 84, loss = 0.0020526498556137085
iteration 85, loss = 0.001823175698518753
iteration 86, loss = 0.0023659737780690193
iteration 87, loss = 0.0032021559309214354
iteration 88, loss = 0.002598782302811742
iteration 89, loss = 0.0018669685814529657
iteration 90, loss = 0.002396038267761469
iteration 91, loss = 0.0023273788392543793
iteration 92, loss = 0.00238356227055192
iteration 93, loss = 0.0021737352944910526
iteration 94, loss = 0.0030706205870956182
iteration 95, loss = 0.0020138162653893232
iteration 96, loss = 0.002272707410156727
iteration 97, loss = 0.0025415283162146807
iteration 98, loss = 0.002118012635037303
iteration 99, loss = 0.0020736476872116327
iteration 100, loss = 0.0021116742864251137
iteration 101, loss = 0.001965719973668456
iteration 102, loss = 0.002026803558692336
iteration 103, loss = 0.002070867922157049
iteration 104, loss = 0.0024155881255865097
iteration 105, loss = 0.0026368096005171537
iteration 106, loss = 0.004287037067115307
iteration 107, loss = 0.003026583231985569
iteration 108, loss = 0.0021534182596951723
iteration 109, loss = 0.0035519630182534456
iteration 110, loss = 0.002112408634275198
iteration 111, loss = 0.0021646118257194757
iteration 112, loss = 0.002221727976575494
iteration 113, loss = 0.0024265109095722437
iteration 114, loss = 0.002337878802791238
iteration 115, loss = 0.0020471459720283747
iteration 116, loss = 0.005251546856015921
iteration 117, loss = 0.004617108963429928
iteration 118, loss = 0.0026332561392337084
iteration 119, loss = 0.0021265794057399035
iteration 120, loss = 0.001713161589577794
iteration 121, loss = 0.0020384907256811857
iteration 122, loss = 0.0022002600599080324
iteration 123, loss = 0.001957848435267806
iteration 124, loss = 0.0020112607162445784
iteration 125, loss = 0.0021761353127658367
iteration 126, loss = 0.0021391070913523436
iteration 127, loss = 0.002180742332711816
iteration 128, loss = 0.0019103512167930603
iteration 129, loss = 0.002431973349303007
iteration 130, loss = 0.0021758617367595434
iteration 131, loss = 0.0019860072061419487
iteration 132, loss = 0.0026559927500784397
iteration 133, loss = 0.002145749982446432
iteration 134, loss = 0.002063244814053178
iteration 135, loss = 0.002231346210464835
iteration 136, loss = 0.002225724281743169
iteration 137, loss = 0.003705814480781555
iteration 138, loss = 0.0022029802203178406
iteration 139, loss = 0.00246719759888947
iteration 140, loss = 0.0019703353755176067
iteration 141, loss = 0.0019025112269446254
iteration 142, loss = 0.0020631402730941772
iteration 143, loss = 0.002245428739115596
iteration 144, loss = 0.001793376635760069
iteration 145, loss = 0.0017728932434692979
iteration 146, loss = 0.0022146282717585564
iteration 147, loss = 0.0023653339594602585
iteration 148, loss = 0.002339113038033247
iteration 149, loss = 0.00407593697309494
iteration 150, loss = 0.0020317453891038895
iteration 151, loss = 0.0024403524585068226
iteration 152, loss = 0.0035594096407294273
iteration 153, loss = 0.002448837971314788
iteration 154, loss = 0.003547322005033493
iteration 155, loss = 0.002632478252053261
iteration 156, loss = 0.002157830633223057
iteration 157, loss = 0.001831744913943112
iteration 158, loss = 0.0021121285390108824
iteration 159, loss = 0.0045161121524870396
iteration 160, loss = 0.002495324471965432
iteration 161, loss = 0.002111307345330715
iteration 162, loss = 0.003519313409924507
iteration 163, loss = 0.002180604264140129
iteration 164, loss = 0.003035208210349083
iteration 165, loss = 0.0026606889441609383
iteration 166, loss = 0.0031919500324875116
iteration 167, loss = 0.0021828622557222843
iteration 168, loss = 0.0025545996613800526
iteration 169, loss = 0.0021794126369059086
iteration 170, loss = 0.0031004254706203938
iteration 171, loss = 0.0018310261657461524
iteration 172, loss = 0.003021683543920517
iteration 173, loss = 0.002331458730623126
iteration 174, loss = 0.0020660203881561756
iteration 175, loss = 0.002033427357673645
iteration 176, loss = 0.0019472779240459204
iteration 177, loss = 0.0023673514369875193
iteration 178, loss = 0.0025075641460716724
iteration 179, loss = 0.002293110592290759
iteration 180, loss = 0.0026998985558748245
iteration 181, loss = 0.004041682928800583
iteration 182, loss = 0.0028351640794426203
iteration 183, loss = 0.0024181369226425886
iteration 184, loss = 0.002248634584248066
iteration 185, loss = 0.0020403231028467417
iteration 186, loss = 0.0025611959863454103
iteration 187, loss = 0.002455259906128049
iteration 188, loss = 0.002404372673481703
iteration 189, loss = 0.0021581496112048626
iteration 190, loss = 0.0021550364326685667
iteration 191, loss = 0.0018137244042009115
iteration 192, loss = 0.00229165842756629
iteration 193, loss = 0.002108203247189522
iteration 194, loss = 0.00265428121201694
iteration 195, loss = 0.0023516365326941013
iteration 196, loss = 0.003628210164606571
iteration 197, loss = 0.0018595745787024498
iteration 198, loss = 0.004728322383016348
iteration 199, loss = 0.001927764038555324
iteration 200, loss = 0.0018403494032099843
iteration 201, loss = 0.0027753165923058987
iteration 202, loss = 0.003937148489058018
iteration 203, loss = 0.002228184137493372
iteration 204, loss = 0.002384075429290533
iteration 205, loss = 0.0031673721969127655
iteration 206, loss = 0.0021811011247336864
iteration 207, loss = 0.001981043256819248
iteration 208, loss = 0.0022279636468738317
iteration 209, loss = 0.002068340079858899
iteration 210, loss = 0.003226799890398979
iteration 211, loss = 0.002333417534828186
iteration 212, loss = 0.0020354227162897587
iteration 213, loss = 0.0020491930190473795
iteration 214, loss = 0.002358640544116497
iteration 215, loss = 0.0024510775692760944
iteration 216, loss = 0.003927487879991531
iteration 217, loss = 0.004006008617579937
iteration 218, loss = 0.0031046061776578426
iteration 219, loss = 0.00214252807199955
iteration 220, loss = 0.0022316232789307833
iteration 221, loss = 0.0029016248881816864
iteration 222, loss = 0.0025007145013660192
iteration 223, loss = 0.0027707861736416817
iteration 224, loss = 0.0022946223616600037
iteration 225, loss = 0.0029593780636787415
iteration 226, loss = 0.0027789906598627567
iteration 227, loss = 0.004073666874319315
iteration 228, loss = 0.0021452312357723713
iteration 229, loss = 0.0020930934697389603
iteration 230, loss = 0.0018840497359633446
iteration 231, loss = 0.002201048657298088
iteration 232, loss = 0.0022799985017627478
iteration 233, loss = 0.004310133401304483
iteration 234, loss = 0.003216702723875642
iteration 235, loss = 0.0026683509349823
iteration 236, loss = 0.0018926042830571532
iteration 237, loss = 0.0038661956787109375
iteration 238, loss = 0.004625505767762661
iteration 239, loss = 0.0033441130071878433
iteration 240, loss = 0.0019412474939599633
iteration 241, loss = 0.0019996976479887962
iteration 242, loss = 0.0028486931696534157
iteration 243, loss = 0.0018836278468370438
iteration 244, loss = 0.0027421158738434315
iteration 245, loss = 0.0020865562837570906
iteration 246, loss = 0.002285568742081523
iteration 247, loss = 0.0023394632153213024
iteration 248, loss = 0.002925337990745902
iteration 249, loss = 0.002229120349511504
iteration 250, loss = 0.001970479264855385
iteration 251, loss = 0.0023296691942960024
iteration 252, loss = 0.002572601893916726
iteration 253, loss = 0.0022674209903925657
iteration 254, loss = 0.0022378885187208652
iteration 255, loss = 0.002424921840429306
iteration 256, loss = 0.00281954905949533
iteration 257, loss = 0.002090974012389779
iteration 258, loss = 0.0018583092605695128
iteration 259, loss = 0.003198395948857069
iteration 260, loss = 0.0037361476570367813
iteration 261, loss = 0.0025880923494696617
iteration 262, loss = 0.003812202485278249
iteration 263, loss = 0.0021725688129663467
iteration 264, loss = 0.0021978903096169233
iteration 265, loss = 0.0019779030699282885
iteration 266, loss = 0.0022095937747508287
iteration 267, loss = 0.00219318107701838
iteration 268, loss = 0.003974507097154856
iteration 269, loss = 0.002021599328145385
iteration 270, loss = 0.0020548386964946985
iteration 271, loss = 0.0022188660223037004
iteration 272, loss = 0.001895416877232492
iteration 273, loss = 0.002300019608810544
iteration 274, loss = 0.0021413182839751244
iteration 275, loss = 0.0019938992336392403
iteration 276, loss = 0.00274644629098475
iteration 277, loss = 0.001893540844321251
iteration 278, loss = 0.0019349368521943688
iteration 279, loss = 0.002375698648393154
iteration 280, loss = 0.002216096967458725
iteration 281, loss = 0.002316477242857218
iteration 282, loss = 0.004091012291610241
iteration 283, loss = 0.0038055977784097195
iteration 284, loss = 0.0023515389766544104
iteration 285, loss = 0.003334884298965335
iteration 286, loss = 0.0028011430986225605
iteration 287, loss = 0.0028047978412359953
iteration 288, loss = 0.0022245924919843674
iteration 289, loss = 0.0023188909981399775
iteration 290, loss = 0.0022481477353721857
iteration 291, loss = 0.0017845164984464645
iteration 292, loss = 0.0025204329285770655
iteration 293, loss = 0.002128270221874118
iteration 294, loss = 0.0017980514094233513
iteration 295, loss = 0.0019317282130941749
iteration 296, loss = 0.0020736469887197018
iteration 297, loss = 0.001999258529394865
iteration 298, loss = 0.002862494671717286
iteration 299, loss = 0.002601064508780837
iteration 300, loss = 0.001946422504261136
iteration 1, loss = 0.0033902404829859734
iteration 2, loss = 0.0019253387581557035
iteration 3, loss = 0.002283086534589529
iteration 4, loss = 0.001960446359589696
iteration 5, loss = 0.002217068336904049
iteration 6, loss = 0.0019357966957613826
iteration 7, loss = 0.0022737893741577864
iteration 8, loss = 0.002201319206506014
iteration 9, loss = 0.0023255599662661552
iteration 10, loss = 0.0032025370746850967
iteration 11, loss = 0.0021230296697467566
iteration 12, loss = 0.001799807883799076
iteration 13, loss = 0.0023035777267068624
iteration 14, loss = 0.002054469659924507
iteration 15, loss = 0.002127966610714793
iteration 16, loss = 0.0026832006406039
iteration 17, loss = 0.002359474077820778
iteration 18, loss = 0.0021416875533759594
iteration 19, loss = 0.0021817118395119905
iteration 20, loss = 0.0024832338094711304
iteration 21, loss = 0.0021061503794044256
iteration 22, loss = 0.0020732032135128975
iteration 23, loss = 0.0025149653665721416
iteration 24, loss = 0.005899909883737564
iteration 25, loss = 0.0019611879251897335
iteration 26, loss = 0.0017019292572513223
iteration 27, loss = 0.002146217506378889
iteration 28, loss = 0.0028733143117278814
iteration 29, loss = 0.0021099774166941643
iteration 30, loss = 0.00210635201074183
iteration 31, loss = 0.002240079455077648
iteration 32, loss = 0.0037492529954761267
iteration 33, loss = 0.003271496156230569
iteration 34, loss = 0.0026171759236603975
iteration 35, loss = 0.001926521072164178
iteration 36, loss = 0.002168390667065978
iteration 37, loss = 0.0030177508015185595
iteration 38, loss = 0.004334536846727133
iteration 39, loss = 0.002041629748418927
iteration 40, loss = 0.003078263020142913
iteration 41, loss = 0.002262591850012541
iteration 42, loss = 0.0031108101829886436
iteration 43, loss = 0.001799923600628972
iteration 44, loss = 0.0023143792059272528
iteration 45, loss = 0.0017827979754656553
iteration 46, loss = 0.0024881744757294655
iteration 47, loss = 0.0030421072151511908
iteration 48, loss = 0.001620556227862835
iteration 49, loss = 0.0021439148113131523
iteration 50, loss = 0.0019446274964138865
iteration 51, loss = 0.0026018409989774227
iteration 52, loss = 0.0020740716718137264
iteration 53, loss = 0.003378004767000675
iteration 54, loss = 0.0023847674019634724
iteration 55, loss = 0.0025185714475810528
iteration 56, loss = 0.0025465870276093483
iteration 57, loss = 0.002117529511451721
iteration 58, loss = 0.0028978330083191395
iteration 59, loss = 0.002989990171045065
iteration 60, loss = 0.003146850038319826
iteration 61, loss = 0.004178338684141636
iteration 62, loss = 0.0018266645492985845
iteration 63, loss = 0.001823810045607388
iteration 64, loss = 0.0018917431589215994
iteration 65, loss = 0.001917347894050181
iteration 66, loss = 0.0017756567103788257
iteration 67, loss = 0.002376900054514408
iteration 68, loss = 0.0020780449267476797
iteration 69, loss = 0.0031682830303907394
iteration 70, loss = 0.004377533681690693
iteration 71, loss = 0.0027935549151152372
iteration 72, loss = 0.0021881437860429287
iteration 73, loss = 0.0019346707267686725
iteration 74, loss = 0.004338367842137814
iteration 75, loss = 0.0037373178638517857
iteration 76, loss = 0.002120332093909383
iteration 77, loss = 0.0032893067691475153
iteration 78, loss = 0.0032759581226855516
iteration 79, loss = 0.0022330046631395817
iteration 80, loss = 0.0017597406404092908
iteration 81, loss = 0.0021356185898184776
iteration 82, loss = 0.00286726257763803
iteration 83, loss = 0.0021018015686422586
iteration 84, loss = 0.0019118039635941386
iteration 85, loss = 0.0021846715826541185
iteration 86, loss = 0.002315716352313757
iteration 87, loss = 0.002092798938974738
iteration 88, loss = 0.002663062186911702
iteration 89, loss = 0.002336985431611538
iteration 90, loss = 0.0023649197537451982
iteration 91, loss = 0.0032932045869529247
iteration 92, loss = 0.003685923060402274
iteration 93, loss = 0.0022449849639087915
iteration 94, loss = 0.0017196782864630222
iteration 95, loss = 0.0023212465457618237
iteration 96, loss = 0.002116721821948886
iteration 97, loss = 0.002185935853049159
iteration 98, loss = 0.00233463360927999
iteration 99, loss = 0.002770279999822378
iteration 100, loss = 0.0020647612400352955
iteration 101, loss = 0.0018227497348561883
iteration 102, loss = 0.0019707893952727318
iteration 103, loss = 0.0020060939714312553
iteration 104, loss = 0.002550613833591342
iteration 105, loss = 0.00190593465231359
iteration 106, loss = 0.002573676872998476
iteration 107, loss = 0.00247584143653512
iteration 108, loss = 0.0018478699494153261
iteration 109, loss = 0.0020942145492881536
iteration 110, loss = 0.0022651897743344307
iteration 111, loss = 0.0023792844731360674
iteration 112, loss = 0.002782238181680441
iteration 113, loss = 0.0030337530188262463
iteration 114, loss = 0.002160316566005349
iteration 115, loss = 0.002169242827221751
iteration 116, loss = 0.003166225738823414
iteration 117, loss = 0.0049224356189370155
iteration 118, loss = 0.003954773768782616
iteration 119, loss = 0.002574942074716091
iteration 120, loss = 0.0026541673578321934
iteration 121, loss = 0.002272791927680373
iteration 122, loss = 0.0027637467719614506
iteration 123, loss = 0.0025031648110598326
iteration 124, loss = 0.0023656715638935566
iteration 125, loss = 0.0021349561866372824
iteration 126, loss = 0.0021718288771808147
iteration 127, loss = 0.0023674359545111656
iteration 128, loss = 0.0024042038712650537
iteration 129, loss = 0.0024848126340657473
iteration 130, loss = 0.0020588443148881197
iteration 131, loss = 0.0021348986774683
iteration 132, loss = 0.002545005176216364
iteration 133, loss = 0.00213940953835845
iteration 134, loss = 0.0021937291603535414
iteration 135, loss = 0.0020646981429308653
iteration 136, loss = 0.0018305054400116205
iteration 137, loss = 0.001955815590918064
iteration 138, loss = 0.0021484852768480778
iteration 139, loss = 0.0025065590161830187
iteration 140, loss = 0.0020306843798607588
iteration 141, loss = 0.002363730687648058
iteration 142, loss = 0.0022261347621679306
iteration 143, loss = 0.00449650501832366
iteration 144, loss = 0.004088371526449919
iteration 145, loss = 0.0022397541906684637
iteration 146, loss = 0.0026148599572479725
iteration 147, loss = 0.0024565826170146465
iteration 148, loss = 0.0032716651912778616
iteration 149, loss = 0.0021725944243371487
iteration 150, loss = 0.002265983261168003
iteration 151, loss = 0.0020694308914244175
iteration 152, loss = 0.0020975058432668447
iteration 153, loss = 0.00419343588873744
iteration 154, loss = 0.0044877976179122925
iteration 155, loss = 0.002503524301573634
iteration 156, loss = 0.003090427489951253
iteration 157, loss = 0.0029127071611583233
iteration 158, loss = 0.0021065538749098778
iteration 159, loss = 0.001823699101805687
iteration 160, loss = 0.0022885699290782213
iteration 161, loss = 0.0022729970514774323
iteration 162, loss = 0.0021190776024013758
iteration 163, loss = 0.0022509766276925802
iteration 164, loss = 0.004154996946454048
iteration 165, loss = 0.003148786723613739
iteration 166, loss = 0.0035295472480356693
iteration 167, loss = 0.00458082789555192
iteration 168, loss = 0.0027082122396677732
iteration 169, loss = 0.0021384884603321552
iteration 170, loss = 0.002333593787625432
iteration 171, loss = 0.0020783045329153538
iteration 172, loss = 0.0021202415227890015
iteration 173, loss = 0.002163864206522703
iteration 174, loss = 0.002063063671812415
iteration 175, loss = 0.002050720388069749
iteration 176, loss = 0.0022532932925969362
iteration 177, loss = 0.0021996537689119577
iteration 178, loss = 0.0018719519721344113
iteration 179, loss = 0.0023021346423774958
iteration 180, loss = 0.002150163520127535
iteration 181, loss = 0.002861647866666317
iteration 182, loss = 0.004574463237076998
iteration 183, loss = 0.001853380585089326
iteration 184, loss = 0.003603934543207288
iteration 185, loss = 0.0023408466950058937
iteration 186, loss = 0.0022422047331929207
iteration 187, loss = 0.0032751087564975023
iteration 188, loss = 0.001962285954505205
iteration 189, loss = 0.002072627656161785
iteration 190, loss = 0.002068936126306653
iteration 191, loss = 0.001982400193810463
iteration 192, loss = 0.0023281415924429893
iteration 193, loss = 0.0034381740260869265
iteration 194, loss = 0.0020421361550688744
iteration 195, loss = 0.002232525497674942
iteration 196, loss = 0.0023637297563254833
iteration 197, loss = 0.002704253885895014
iteration 198, loss = 0.0019375195261090994
iteration 199, loss = 0.0022182133980095387
iteration 200, loss = 0.0038224190939217806
iteration 201, loss = 0.004408388864248991
iteration 202, loss = 0.003486241679638624
iteration 203, loss = 0.0038162493146955967
iteration 204, loss = 0.0024286413099616766
iteration 205, loss = 0.0030734101310372353
iteration 206, loss = 0.004764310084283352
iteration 207, loss = 0.0020895309280604124
iteration 208, loss = 0.0021567451767623425
iteration 209, loss = 0.0023239620495587587
iteration 210, loss = 0.0022245983127504587
iteration 211, loss = 0.0033958437852561474
iteration 212, loss = 0.002328184898942709
iteration 213, loss = 0.002719817915931344
iteration 214, loss = 0.002428668085485697
iteration 215, loss = 0.001921917893923819
iteration 216, loss = 0.002177529502660036
iteration 217, loss = 0.002736268099397421
iteration 218, loss = 0.0020369591657072306
iteration 219, loss = 0.004718323238193989
iteration 220, loss = 0.002201222814619541
iteration 221, loss = 0.0026891850866377354
iteration 222, loss = 0.002162139629945159
iteration 223, loss = 0.0018473027739673853
iteration 224, loss = 0.002246056217700243
iteration 225, loss = 0.0021261621732264757
iteration 226, loss = 0.0027206363156437874
iteration 227, loss = 0.0033032044302672148
iteration 228, loss = 0.003455596975982189
iteration 229, loss = 0.003434091806411743
iteration 230, loss = 0.002111820038408041
iteration 231, loss = 0.00237264484167099
iteration 232, loss = 0.002234356477856636
iteration 233, loss = 0.0021706095430999994
iteration 234, loss = 0.002426690887659788
iteration 235, loss = 0.0021123432088643312
iteration 236, loss = 0.0018129930831491947
iteration 237, loss = 0.0019149306463077664
iteration 238, loss = 0.0017893193289637566
iteration 239, loss = 0.0029118573293089867
iteration 240, loss = 0.002887098817154765
iteration 241, loss = 0.0021808084566146135
iteration 242, loss = 0.0018386885058134794
iteration 243, loss = 0.0021719997748732567
iteration 244, loss = 0.0019151393789798021
iteration 245, loss = 0.0020356331951916218
iteration 246, loss = 0.0026219207793474197
iteration 247, loss = 0.0031100742053240538
iteration 248, loss = 0.0034322147257626057
iteration 249, loss = 0.003663561539724469
iteration 250, loss = 0.001985708950087428
iteration 251, loss = 0.002414619317278266
iteration 252, loss = 0.003060257760807872
iteration 253, loss = 0.0024821716360747814
iteration 254, loss = 0.0025233812630176544
iteration 255, loss = 0.0018450007773935795
iteration 256, loss = 0.0025176480412483215
iteration 257, loss = 0.002809752244502306
iteration 258, loss = 0.002325256122276187
iteration 259, loss = 0.002475630957633257
iteration 260, loss = 0.00392216257750988
iteration 261, loss = 0.003512793220579624
iteration 262, loss = 0.0036612453404814005
iteration 263, loss = 0.0019240520196035504
iteration 264, loss = 0.0020185699686408043
iteration 265, loss = 0.0019625260028988123
iteration 266, loss = 0.0022781281732022762
iteration 267, loss = 0.0024274815805256367
iteration 268, loss = 0.0020582510624080896
iteration 269, loss = 0.001969237346202135
iteration 270, loss = 0.0020472181495279074
iteration 271, loss = 0.002593826036900282
iteration 272, loss = 0.0021062877494841814
iteration 273, loss = 0.002064463682472706
iteration 274, loss = 0.0022322935983538628
iteration 275, loss = 0.0018689417047426105
iteration 276, loss = 0.0021033254452049732
iteration 277, loss = 0.005082015413790941
iteration 278, loss = 0.002397756325080991
iteration 279, loss = 0.002129275118932128
iteration 280, loss = 0.0021124996710568666
iteration 281, loss = 0.001866862177848816
iteration 282, loss = 0.0017701826291158795
iteration 283, loss = 0.0024351587053388357
iteration 284, loss = 0.0023432194720953703
iteration 285, loss = 0.00197295518592
iteration 286, loss = 0.002188173122704029
iteration 287, loss = 0.0024039740674197674
iteration 288, loss = 0.002049385802820325
iteration 289, loss = 0.00199895934201777
iteration 290, loss = 0.0022809510119259357
iteration 291, loss = 0.0021960027515888214
iteration 292, loss = 0.0025365843903273344
iteration 293, loss = 0.0025657350197434425
iteration 294, loss = 0.004345917608588934
iteration 295, loss = 0.002296671736985445
iteration 296, loss = 0.001821680343709886
iteration 297, loss = 0.0019906689412891865
iteration 298, loss = 0.002105628838762641
iteration 299, loss = 0.001997681800276041
iteration 300, loss = 0.0019186295103281736
iteration 1, loss = 0.0022284751757979393
iteration 2, loss = 0.002872113138437271
iteration 3, loss = 0.0022966372780501842
iteration 4, loss = 0.0018676001345738769
iteration 5, loss = 0.002203648444265127
iteration 6, loss = 0.0022966989781707525
iteration 7, loss = 0.0016924736555665731
iteration 8, loss = 0.0022282477002590895
iteration 9, loss = 0.0023488132283091545
iteration 10, loss = 0.002258357359096408
iteration 11, loss = 0.002828377764672041
iteration 12, loss = 0.00204555899836123
iteration 13, loss = 0.0022672174964100122
iteration 14, loss = 0.004604744259268045
iteration 15, loss = 0.0019461711635813117
iteration 16, loss = 0.004562307149171829
iteration 17, loss = 0.004545406438410282
iteration 18, loss = 0.0019878875464200974
iteration 19, loss = 0.0018737844657152891
iteration 20, loss = 0.0023333358112722635
iteration 21, loss = 0.002304844092577696
iteration 22, loss = 0.002320190891623497
iteration 23, loss = 0.002257668413221836
iteration 24, loss = 0.0024370241444557905
iteration 25, loss = 0.0026052393950521946
iteration 26, loss = 0.00238667125813663
iteration 27, loss = 0.0020255157724022865
iteration 28, loss = 0.002170432824641466
iteration 29, loss = 0.0020417433697730303
iteration 30, loss = 0.0023085628636181355
iteration 31, loss = 0.002358811441808939
iteration 32, loss = 0.002364039421081543
iteration 33, loss = 0.002636294113472104
iteration 34, loss = 0.002837183652445674
iteration 35, loss = 0.002806385513395071
iteration 36, loss = 0.0019690331537276506
iteration 37, loss = 0.002006657887250185
iteration 38, loss = 0.002249979181215167
iteration 39, loss = 0.003413211554288864
iteration 40, loss = 0.002000953536480665
iteration 41, loss = 0.0017320741899311543
iteration 42, loss = 0.0019636577926576138
iteration 43, loss = 0.002082332270219922
iteration 44, loss = 0.0036269109696149826
iteration 45, loss = 0.0025455409195274115
iteration 46, loss = 0.003206483321264386
iteration 47, loss = 0.004862059839069843
iteration 48, loss = 0.0018314467743039131
iteration 49, loss = 0.0035499457735568285
iteration 50, loss = 0.003419244196265936
iteration 51, loss = 0.0020594988018274307
iteration 52, loss = 0.0021084758918732405
iteration 53, loss = 0.004679031670093536
iteration 54, loss = 0.0022647075820714235
iteration 55, loss = 0.0018643222283571959
iteration 56, loss = 0.0025932949502021074
iteration 57, loss = 0.0019649933092296124
iteration 58, loss = 0.0018130543176084757
iteration 59, loss = 0.003001728793606162
iteration 60, loss = 0.002166138030588627
iteration 61, loss = 0.0025295272935181856
iteration 62, loss = 0.001994253369048238
iteration 63, loss = 0.0035266918130218983
iteration 64, loss = 0.0018623571377247572
iteration 65, loss = 0.0029490168672055006
iteration 66, loss = 0.0019714192021638155
iteration 67, loss = 0.002512111095711589
iteration 68, loss = 0.0027742129750549793
iteration 69, loss = 0.0017705054488033056
iteration 70, loss = 0.0019129420397803187
iteration 71, loss = 0.0025188287254422903
iteration 72, loss = 0.002688096836209297
iteration 73, loss = 0.0027216793969273567
iteration 74, loss = 0.0032849768176674843
iteration 75, loss = 0.002090317662805319
iteration 76, loss = 0.002002876717597246
iteration 77, loss = 0.0019978401251137257
iteration 78, loss = 0.002168153179809451
iteration 79, loss = 0.00207513477653265
iteration 80, loss = 0.00200224039144814
iteration 81, loss = 0.0019432344706729054
iteration 82, loss = 0.0022378675639629364
iteration 83, loss = 0.0021959522273391485
iteration 84, loss = 0.002252919366583228
iteration 85, loss = 0.00215914286673069
iteration 86, loss = 0.0025307582691311836
iteration 87, loss = 0.0018581748008728027
iteration 88, loss = 0.003237479366362095
iteration 89, loss = 0.002176061272621155
iteration 90, loss = 0.002965967170894146
iteration 91, loss = 0.0018255095928907394
iteration 92, loss = 0.0018608124228194356
iteration 93, loss = 0.001984888920560479
iteration 94, loss = 0.004008549265563488
iteration 95, loss = 0.0022982521913945675
iteration 96, loss = 0.004871751647442579
iteration 97, loss = 0.0022127951961010695
iteration 98, loss = 0.001962970243766904
iteration 99, loss = 0.002793616382405162
iteration 100, loss = 0.0020992173813283443
iteration 101, loss = 0.0023074201308190823
iteration 102, loss = 0.0022800711449235678
iteration 103, loss = 0.0018831355264410377
iteration 104, loss = 0.001967529533430934
iteration 105, loss = 0.004320537205785513
iteration 106, loss = 0.002416213508695364
iteration 107, loss = 0.0036903670988976955
iteration 108, loss = 0.002926646498963237
iteration 109, loss = 0.0020413915626704693
iteration 110, loss = 0.0024449797347187996
iteration 111, loss = 0.002124129794538021
iteration 112, loss = 0.002845469629392028
iteration 113, loss = 0.0030086105689406395
iteration 114, loss = 0.004372497089207172
iteration 115, loss = 0.0029090868774801493
iteration 116, loss = 0.002321473555639386
iteration 117, loss = 0.0018553560366854072
iteration 118, loss = 0.0022066396195441484
iteration 119, loss = 0.0036059170961380005
iteration 120, loss = 0.0018827930325642228
iteration 121, loss = 0.0021759250666946173
iteration 122, loss = 0.004107099492102861
iteration 123, loss = 0.0023295944556593895
iteration 124, loss = 0.0021778803784400225
iteration 125, loss = 0.0022633448243141174
iteration 126, loss = 0.0027984248008579016
iteration 127, loss = 0.002150109503418207
iteration 128, loss = 0.0018123392947018147
iteration 129, loss = 0.0021531605161726475
iteration 130, loss = 0.0020044802222400904
iteration 131, loss = 0.0029554814100265503
iteration 132, loss = 0.00185343564953655
iteration 133, loss = 0.002318007405847311
iteration 134, loss = 0.002877620980143547
iteration 135, loss = 0.0017485622083768249
iteration 136, loss = 0.0020222561433911324
iteration 137, loss = 0.0020218202844262123
iteration 138, loss = 0.0022616637870669365
iteration 139, loss = 0.001930144615471363
iteration 140, loss = 0.0018665522802621126
iteration 141, loss = 0.002137094736099243
iteration 142, loss = 0.0028019403107464314
iteration 143, loss = 0.002316602971404791
iteration 144, loss = 0.0020295935682952404
iteration 145, loss = 0.0022983120288699865
iteration 146, loss = 0.0021469963248819113
iteration 147, loss = 0.0021087999921292067
iteration 148, loss = 0.0028987207915633917
iteration 149, loss = 0.0042825667187571526
iteration 150, loss = 0.0019639981910586357
iteration 151, loss = 0.0028102565556764603
iteration 152, loss = 0.0020720786415040493
iteration 153, loss = 0.0021342476829886436
iteration 154, loss = 0.0020580338314175606
iteration 155, loss = 0.002660680329427123
iteration 156, loss = 0.0021433145739138126
iteration 157, loss = 0.002157901879400015
iteration 158, loss = 0.0019426281796768308
iteration 159, loss = 0.002275436883792281
iteration 160, loss = 0.0040842159651219845
iteration 161, loss = 0.002321559237316251
iteration 162, loss = 0.0022949145641177893
iteration 163, loss = 0.002255719620734453
iteration 164, loss = 0.0019069697009399533
iteration 165, loss = 0.0019112047739326954
iteration 166, loss = 0.002148325089365244
iteration 167, loss = 0.0023442646488547325
iteration 168, loss = 0.002979437354952097
iteration 169, loss = 0.0021620956249535084
iteration 170, loss = 0.0020066227298229933
iteration 171, loss = 0.0021820219699293375
iteration 172, loss = 0.002468894235789776
iteration 173, loss = 0.004296055994927883
iteration 174, loss = 0.0018764609703794122
iteration 175, loss = 0.0029579447582364082
iteration 176, loss = 0.0028293549548834562
iteration 177, loss = 0.0022976736072450876
iteration 178, loss = 0.0037359795533120632
iteration 179, loss = 0.0032795588485896587
iteration 180, loss = 0.0021231125574558973
iteration 181, loss = 0.0021475222893059254
iteration 182, loss = 0.0038289837539196014
iteration 183, loss = 0.0025945259258151054
iteration 184, loss = 0.0019632643088698387
iteration 185, loss = 0.0025404691696166992
iteration 186, loss = 0.0022980303037911654
iteration 187, loss = 0.0020057421643286943
iteration 188, loss = 0.002093633869662881
iteration 189, loss = 0.00265626166947186
iteration 190, loss = 0.0020371670834720135
iteration 191, loss = 0.004456373397260904
iteration 192, loss = 0.0019255403894931078
iteration 193, loss = 0.003072693245485425
iteration 194, loss = 0.003419965272769332
iteration 195, loss = 0.0020114071667194366
iteration 196, loss = 0.0027628110256046057
iteration 197, loss = 0.00246412493288517
iteration 198, loss = 0.001961425645276904
iteration 199, loss = 0.002269577467814088
iteration 200, loss = 0.002091095084324479
iteration 201, loss = 0.0017645091284066439
iteration 202, loss = 0.0017417487688362598
iteration 203, loss = 0.003416541265323758
iteration 204, loss = 0.002062531653791666
iteration 205, loss = 0.002509784186258912
iteration 206, loss = 0.002100621582940221
iteration 207, loss = 0.002528351964429021
iteration 208, loss = 0.0023418318014591932
iteration 209, loss = 0.0020789687987416983
iteration 210, loss = 0.0016223627608269453
iteration 211, loss = 0.0025538899935781956
iteration 212, loss = 0.0020838126074522734
iteration 213, loss = 0.0034759235568344593
iteration 214, loss = 0.002537686610594392
iteration 215, loss = 0.0020899702794849873
iteration 216, loss = 0.0021717341151088476
iteration 217, loss = 0.002125032711774111
iteration 218, loss = 0.002012895653024316
iteration 219, loss = 0.002090803347527981
iteration 220, loss = 0.002171065891161561
iteration 221, loss = 0.00248000118881464
iteration 222, loss = 0.00295617594383657
iteration 223, loss = 0.0021470070350915194
iteration 224, loss = 0.0019266536692157388
iteration 225, loss = 0.004038256127387285
iteration 226, loss = 0.004319808445870876
iteration 227, loss = 0.002205978147685528
iteration 228, loss = 0.0030122585594654083
iteration 229, loss = 0.0034284070134162903
iteration 230, loss = 0.0023947060108184814
iteration 231, loss = 0.0018792039481922984
iteration 232, loss = 0.0030957357957959175
iteration 233, loss = 0.0034306165762245655
iteration 234, loss = 0.002090117195621133
iteration 235, loss = 0.0023246724158525467
iteration 236, loss = 0.002127500018104911
iteration 237, loss = 0.00395839661359787
iteration 238, loss = 0.0021624586079269648
iteration 239, loss = 0.0021538599394261837
iteration 240, loss = 0.0019595283083617687
iteration 241, loss = 0.001833491143770516
iteration 242, loss = 0.001932601211592555
iteration 243, loss = 0.0023016720078885555
iteration 244, loss = 0.0049731116741895676
iteration 245, loss = 0.002977178432047367
iteration 246, loss = 0.002042388077825308
iteration 247, loss = 0.0020357477478682995
iteration 248, loss = 0.002292102435603738
iteration 249, loss = 0.003069094382226467
iteration 250, loss = 0.0023470870219171047
iteration 251, loss = 0.002096716780215502
iteration 252, loss = 0.002262299181893468
iteration 253, loss = 0.00244378624483943
iteration 254, loss = 0.002333060372620821
iteration 255, loss = 0.0018582805059850216
iteration 256, loss = 0.002355557633563876
iteration 257, loss = 0.0030472450889647007
iteration 258, loss = 0.002216288121417165
iteration 259, loss = 0.002092689974233508
iteration 260, loss = 0.0024085859768092632
iteration 261, loss = 0.0025596790947020054
iteration 262, loss = 0.0019485707161948085
iteration 263, loss = 0.001907196594402194
iteration 264, loss = 0.0019774390384554863
iteration 265, loss = 0.0019745079334825277
iteration 266, loss = 0.0018967223586514592
iteration 267, loss = 0.001935996930114925
iteration 268, loss = 0.004229813348501921
iteration 269, loss = 0.002190075349062681
iteration 270, loss = 0.0027026424650102854
iteration 271, loss = 0.0022769984789192677
iteration 272, loss = 0.0018878834089264274
iteration 273, loss = 0.003634444670751691
iteration 274, loss = 0.0026143493596464396
iteration 275, loss = 0.00168323190882802
iteration 276, loss = 0.002366715809330344
iteration 277, loss = 0.0020365389063954353
iteration 278, loss = 0.002126760547980666
iteration 279, loss = 0.0022235719952732325
iteration 280, loss = 0.0023974350187927485
iteration 281, loss = 0.002392982831224799
iteration 282, loss = 0.002879328792914748
iteration 283, loss = 0.0019449802348390222
iteration 284, loss = 0.004646559711545706
iteration 285, loss = 0.0019728525076061487
iteration 286, loss = 0.0029369001276791096
iteration 287, loss = 0.0018912655068561435
iteration 288, loss = 0.0021533221006393433
iteration 289, loss = 0.0021608504466712475
iteration 290, loss = 0.0018003558507189155
iteration 291, loss = 0.0035732039250433445
iteration 292, loss = 0.002039089100435376
iteration 293, loss = 0.002732290420681238
iteration 294, loss = 0.002808119636029005
iteration 295, loss = 0.003110612975433469
iteration 296, loss = 0.0016041210619732738
iteration 297, loss = 0.003084412310272455
iteration 298, loss = 0.003983930218964815
iteration 299, loss = 0.0021450696513056755
iteration 300, loss = 0.003683270886540413
iteration 1, loss = 0.001946859760209918
iteration 2, loss = 0.0017905861604958773
iteration 3, loss = 0.0037600367795675993
iteration 4, loss = 0.002779680537059903
iteration 5, loss = 0.002375675132498145
iteration 6, loss = 0.002288799500092864
iteration 7, loss = 0.0021436975803226233
iteration 8, loss = 0.0020779233891516924
iteration 9, loss = 0.002086412627249956
iteration 10, loss = 0.0026557324454188347
iteration 11, loss = 0.0018444227753207088
iteration 12, loss = 0.0023523743730038404
iteration 13, loss = 0.0019771705847233534
iteration 14, loss = 0.0024164877831935883
iteration 15, loss = 0.0017511030891910195
iteration 16, loss = 0.001817565644159913
iteration 17, loss = 0.0023439540527760983
iteration 18, loss = 0.0027266473043709993
iteration 19, loss = 0.0017513857455924153
iteration 20, loss = 0.002072559669613838
iteration 21, loss = 0.002714744070544839
iteration 22, loss = 0.0026448294520378113
iteration 23, loss = 0.002131252782419324
iteration 24, loss = 0.0037663676775991917
iteration 25, loss = 0.00342644308693707
iteration 26, loss = 0.0020826123654842377
iteration 27, loss = 0.002084441250190139
iteration 28, loss = 0.0015976079739630222
iteration 29, loss = 0.0024421317502856255
iteration 30, loss = 0.002328162780031562
iteration 31, loss = 0.004805292002856731
iteration 32, loss = 0.002137433737516403
iteration 33, loss = 0.00297787063755095
iteration 34, loss = 0.0026069744490087032
iteration 35, loss = 0.002591704251244664
iteration 36, loss = 0.002240511355921626
iteration 37, loss = 0.001967643154785037
iteration 38, loss = 0.002050885697826743
iteration 39, loss = 0.0018373912898823619
iteration 40, loss = 0.002186128869652748
iteration 41, loss = 0.0022492166608572006
iteration 42, loss = 0.002088100416585803
iteration 43, loss = 0.002108565764501691
iteration 44, loss = 0.0040727341547608376
iteration 45, loss = 0.002005665795877576
iteration 46, loss = 0.00200635869987309
iteration 47, loss = 0.002181986579671502
iteration 48, loss = 0.0019930112175643444
iteration 49, loss = 0.003182645421475172
iteration 50, loss = 0.00445146206766367
iteration 51, loss = 0.004020383581519127
iteration 52, loss = 0.0018579823663458228
iteration 53, loss = 0.0020673354156315327
iteration 54, loss = 0.0018582267221063375
iteration 55, loss = 0.003076745430007577
iteration 56, loss = 0.002206199336796999
iteration 57, loss = 0.0030173412524163723
iteration 58, loss = 0.0030189738608896732
iteration 59, loss = 0.0019509779522195458
iteration 60, loss = 0.0025115027092397213
iteration 61, loss = 0.002305479720234871
iteration 62, loss = 0.002387907588854432
iteration 63, loss = 0.0019315085373818874
iteration 64, loss = 0.0018810961628332734
iteration 65, loss = 0.0023030960001051426
iteration 66, loss = 0.004283461254090071
iteration 67, loss = 0.0033276143949478865
iteration 68, loss = 0.002515712520107627
iteration 69, loss = 0.0032387084793299437
iteration 70, loss = 0.005712954327464104
iteration 71, loss = 0.00450868159532547
iteration 72, loss = 0.0029589999467134476
iteration 73, loss = 0.002553254598751664
iteration 74, loss = 0.0026340815238654613
iteration 75, loss = 0.0018617503810673952
iteration 76, loss = 0.002327082911506295
iteration 77, loss = 0.003987559583038092
iteration 78, loss = 0.002599580679088831
iteration 79, loss = 0.0020958329550921917
iteration 80, loss = 0.0025406063068658113
iteration 81, loss = 0.003920284099876881
iteration 82, loss = 0.0019266532035544515
iteration 83, loss = 0.0019687970634549856
iteration 84, loss = 0.003446802729740739
iteration 85, loss = 0.001896408386528492
iteration 86, loss = 0.0022143349051475525
iteration 87, loss = 0.0030726338736712933
iteration 88, loss = 0.0038315835408866405
iteration 89, loss = 0.0024299942888319492
iteration 90, loss = 0.0020026653073728085
iteration 91, loss = 0.003064072458073497
iteration 92, loss = 0.002534625818952918
iteration 93, loss = 0.0032190117053687572
iteration 94, loss = 0.00183966220356524
iteration 95, loss = 0.0037865061312913895
iteration 96, loss = 0.0021033640950918198
iteration 97, loss = 0.0020367149263620377
iteration 98, loss = 0.003099312772974372
iteration 99, loss = 0.0022687760647386312
iteration 100, loss = 0.0029256329871714115
iteration 101, loss = 0.002988928696140647
iteration 102, loss = 0.002316606231033802
iteration 103, loss = 0.0025090561248362064
iteration 104, loss = 0.0017494894564151764
iteration 105, loss = 0.002116246847435832
iteration 106, loss = 0.004235452972352505
iteration 107, loss = 0.0025607969146221876
iteration 108, loss = 0.0020260424353182316
iteration 109, loss = 0.0022622777614742517
iteration 110, loss = 0.002342749387025833
iteration 111, loss = 0.0019042376661673188
iteration 112, loss = 0.0034751445055007935
iteration 113, loss = 0.0026020866353064775
iteration 114, loss = 0.0022914884611964226
iteration 115, loss = 0.0029539307579398155
iteration 116, loss = 0.0021337992511689663
iteration 117, loss = 0.0017473476473242044
iteration 118, loss = 0.0020482903346419334
iteration 119, loss = 0.0018112924881279469
iteration 120, loss = 0.0022185544949024916
iteration 121, loss = 0.0020776167511940002
iteration 122, loss = 0.003374860854819417
iteration 123, loss = 0.0036413201596587896
iteration 124, loss = 0.004278860054910183
iteration 125, loss = 0.0020332583226263523
iteration 126, loss = 0.0028276380617171526
iteration 127, loss = 0.0021005624439567327
iteration 128, loss = 0.0018656908068805933
iteration 129, loss = 0.002151629887521267
iteration 130, loss = 0.0019305204041302204
iteration 131, loss = 0.0019399611046537757
iteration 132, loss = 0.0019200460519641638
iteration 133, loss = 0.002269954653456807
iteration 134, loss = 0.0021147504448890686
iteration 135, loss = 0.0018581310287117958
iteration 136, loss = 0.004491597414016724
iteration 137, loss = 0.0022268265020102262
iteration 138, loss = 0.0020273602567613125
iteration 139, loss = 0.002333824522793293
iteration 140, loss = 0.0023002687375992537
iteration 141, loss = 0.0033813747577369213
iteration 142, loss = 0.0019463027128949761
iteration 143, loss = 0.0018763309344649315
iteration 144, loss = 0.0036607906222343445
iteration 145, loss = 0.0029544730205088854
iteration 146, loss = 0.002044742228463292
iteration 147, loss = 0.0019456101581454277
iteration 148, loss = 0.002258308930322528
iteration 149, loss = 0.002422827761620283
iteration 150, loss = 0.0018857961986213923
iteration 151, loss = 0.0021741322707384825
iteration 152, loss = 0.0020288454834371805
iteration 153, loss = 0.0019315519602969289
iteration 154, loss = 0.0026442818343639374
iteration 155, loss = 0.0022629089653491974
iteration 156, loss = 0.0021234804298728704
iteration 157, loss = 0.0024403491988778114
iteration 158, loss = 0.002432651584967971
iteration 159, loss = 0.0026920889504253864
iteration 160, loss = 0.00245766737498343
iteration 161, loss = 0.0032719811424613
iteration 162, loss = 0.003015415742993355
iteration 163, loss = 0.0026111784391105175
iteration 164, loss = 0.0038580100517719984
iteration 165, loss = 0.0031422923784703016
iteration 166, loss = 0.0019782106392085552
iteration 167, loss = 0.0019718301482498646
iteration 168, loss = 0.0027050618082284927
iteration 169, loss = 0.002188065554946661
iteration 170, loss = 0.0024676783941686153
iteration 171, loss = 0.0024229639675468206
iteration 172, loss = 0.0018444814486429095
iteration 173, loss = 0.0016721261199563742
iteration 174, loss = 0.0019504332449287176
iteration 175, loss = 0.0019161672098562121
iteration 176, loss = 0.00312933512032032
iteration 177, loss = 0.0020596017129719257
iteration 178, loss = 0.0023355516605079174
iteration 179, loss = 0.0024585784412920475
iteration 180, loss = 0.0030672720167785883
iteration 181, loss = 0.0020300131291151047
iteration 182, loss = 0.002155858092010021
iteration 183, loss = 0.001943821320310235
iteration 184, loss = 0.002054201439023018
iteration 185, loss = 0.002236759988591075
iteration 186, loss = 0.0031328340992331505
iteration 187, loss = 0.0020004042889922857
iteration 188, loss = 0.0019803515169769526
iteration 189, loss = 0.001752953976392746
iteration 190, loss = 0.0020109035540372133
iteration 191, loss = 0.0020601539872586727
iteration 192, loss = 0.002025269204750657
iteration 193, loss = 0.0021591896656900644
iteration 194, loss = 0.0028478065505623817
iteration 195, loss = 0.0019547510892152786
iteration 196, loss = 0.001964238937944174
iteration 197, loss = 0.002611675066873431
iteration 198, loss = 0.002248362870886922
iteration 199, loss = 0.002463696524500847
iteration 200, loss = 0.0025410945527255535
iteration 201, loss = 0.002420766744762659
iteration 202, loss = 0.001964971423149109
iteration 203, loss = 0.0018697625491768122
iteration 204, loss = 0.0020105147268623114
iteration 205, loss = 0.0020449564326554537
iteration 206, loss = 0.001714806305244565
iteration 207, loss = 0.0023607893381267786
iteration 208, loss = 0.002077056560665369
iteration 209, loss = 0.0019502858631312847
iteration 210, loss = 0.002596542239189148
iteration 211, loss = 0.0030166301876306534
iteration 212, loss = 0.003973931074142456
iteration 213, loss = 0.0035768169909715652
iteration 214, loss = 0.0022494716104120016
iteration 215, loss = 0.002003189641982317
iteration 216, loss = 0.0018896689871326089
iteration 217, loss = 0.0020460139494389296
iteration 218, loss = 0.0025756736285984516
iteration 219, loss = 0.002653953619301319
iteration 220, loss = 0.00270336726680398
iteration 221, loss = 0.002014954574406147
iteration 222, loss = 0.0018994818674400449
iteration 223, loss = 0.0017591265495866537
iteration 224, loss = 0.001935044303536415
iteration 225, loss = 0.0036202275659888983
iteration 226, loss = 0.002113698050379753
iteration 227, loss = 0.002207291079685092
iteration 228, loss = 0.0019587166607379913
iteration 229, loss = 0.0020677591674029827
iteration 230, loss = 0.003067566081881523
iteration 231, loss = 0.0019340007565915585
iteration 232, loss = 0.001981971086934209
iteration 233, loss = 0.002148665487766266
iteration 234, loss = 0.0027345370035618544
iteration 235, loss = 0.002380034187808633
iteration 236, loss = 0.0037162648513913155
iteration 237, loss = 0.0033987760543823242
iteration 238, loss = 0.002599552506580949
iteration 239, loss = 0.004535558633506298
iteration 240, loss = 0.002329367445781827
iteration 241, loss = 0.0019386170897632837
iteration 242, loss = 0.0018175785662606359
iteration 243, loss = 0.0035952392499893904
iteration 244, loss = 0.002229621633887291
iteration 245, loss = 0.001979630207642913
iteration 246, loss = 0.0021357277873903513
iteration 247, loss = 0.001959343208000064
iteration 248, loss = 0.0019148895516991615
iteration 249, loss = 0.0017436204943805933
iteration 250, loss = 0.001896776258945465
iteration 251, loss = 0.0018638382898643613
iteration 252, loss = 0.0020760786719620228
iteration 253, loss = 0.003961484879255295
iteration 254, loss = 0.0019361770246177912
iteration 255, loss = 0.0026642594020813704
iteration 256, loss = 0.0021230624988675117
iteration 257, loss = 0.0025301692076027393
iteration 258, loss = 0.003372739301994443
iteration 259, loss = 0.0021069436334073544
iteration 260, loss = 0.0019324133172631264
iteration 261, loss = 0.002205349737778306
iteration 262, loss = 0.0036684968508780003
iteration 263, loss = 0.002018252620473504
iteration 264, loss = 0.0021818308159708977
iteration 265, loss = 0.0020825082901865244
iteration 266, loss = 0.0018795279320329428
iteration 267, loss = 0.002477589063346386
iteration 268, loss = 0.002294303849339485
iteration 269, loss = 0.003344483207911253
iteration 270, loss = 0.002286151982843876
iteration 271, loss = 0.002568477764725685
iteration 272, loss = 0.0020052094478160143
iteration 273, loss = 0.002282489323988557
iteration 274, loss = 0.0033105039037764072
iteration 275, loss = 0.0028957612812519073
iteration 276, loss = 0.0023161068093031645
iteration 277, loss = 0.002196037210524082
iteration 278, loss = 0.0019732676446437836
iteration 279, loss = 0.002072317060083151
iteration 280, loss = 0.002017765771597624
iteration 281, loss = 0.0019987011328339577
iteration 282, loss = 0.001803337363526225
iteration 283, loss = 0.002490653656423092
iteration 284, loss = 0.0022729416377842426
iteration 285, loss = 0.002461124211549759
iteration 286, loss = 0.0017823176458477974
iteration 287, loss = 0.002129671396687627
iteration 288, loss = 0.0036154231056571007
iteration 289, loss = 0.002232043771073222
iteration 290, loss = 0.0021847288589924574
iteration 291, loss = 0.0026249634101986885
iteration 292, loss = 0.0021759190130978823
iteration 293, loss = 0.001979960361495614
iteration 294, loss = 0.0038487452547997236
iteration 295, loss = 0.0026253771502524614
iteration 296, loss = 0.0023349965922534466
iteration 297, loss = 0.0020293262787163258
iteration 298, loss = 0.002126783598214388
iteration 299, loss = 0.0031197499483823776
iteration 300, loss = 0.0020831008441746235
iteration 1, loss = 0.0034452665131539106
iteration 2, loss = 0.0022439598105847836
iteration 3, loss = 0.0018660820787772536
iteration 4, loss = 0.002757804235443473
iteration 5, loss = 0.0023567508906126022
iteration 6, loss = 0.00193659751676023
iteration 7, loss = 0.0019318356644362211
iteration 8, loss = 0.0019402143079787493
iteration 9, loss = 0.002915711374953389
iteration 10, loss = 0.0019411190878599882
iteration 11, loss = 0.0034167030826210976
iteration 12, loss = 0.002060175407677889
iteration 13, loss = 0.004495730623602867
iteration 14, loss = 0.0019966440740972757
iteration 15, loss = 0.003169924020767212
iteration 16, loss = 0.0024179830215871334
iteration 17, loss = 0.0026045145932585
iteration 18, loss = 0.005069663282483816
iteration 19, loss = 0.0021719217766076326
iteration 20, loss = 0.0037080603651702404
iteration 21, loss = 0.0021127378568053246
iteration 22, loss = 0.002001390093937516
iteration 23, loss = 0.004136004019528627
iteration 24, loss = 0.002699347212910652
iteration 25, loss = 0.002281762193888426
iteration 26, loss = 0.0017973064677789807
iteration 27, loss = 0.0021843090653419495
iteration 28, loss = 0.0025281510315835476
iteration 29, loss = 0.0018800944089889526
iteration 30, loss = 0.0023153945803642273
iteration 31, loss = 0.005059825722128153
iteration 32, loss = 0.0016709467163309455
iteration 33, loss = 0.0020423433743417263
iteration 34, loss = 0.0019577930215746164
iteration 35, loss = 0.0021062754094600677
iteration 36, loss = 0.0030440117698162794
iteration 37, loss = 0.0028164410032331944
iteration 38, loss = 0.001994247082620859
iteration 39, loss = 0.0020977831445634365
iteration 40, loss = 0.0026747507508844137
iteration 41, loss = 0.0024081894662231207
iteration 42, loss = 0.002401711419224739
iteration 43, loss = 0.0018908753991127014
iteration 44, loss = 0.0025295394007116556
iteration 45, loss = 0.0017759286565706134
iteration 46, loss = 0.002423647791147232
iteration 47, loss = 0.0023654059041291475
iteration 48, loss = 0.002413006965070963
iteration 49, loss = 0.0019816476851701736
iteration 50, loss = 0.002038216684013605
iteration 51, loss = 0.001970167038962245
iteration 52, loss = 0.0019224942661821842
iteration 53, loss = 0.0024354043416678905
iteration 54, loss = 0.0037574460729956627
iteration 55, loss = 0.0025327997282147408
iteration 56, loss = 0.002296566031873226
iteration 57, loss = 0.0020792449358850718
iteration 58, loss = 0.002566494280472398
iteration 59, loss = 0.0022895997390151024
iteration 60, loss = 0.0024069759529083967
iteration 61, loss = 0.0021233579609543085
iteration 62, loss = 0.0018601252231746912
iteration 63, loss = 0.0022016000002622604
iteration 64, loss = 0.002837623003870249
iteration 65, loss = 0.0019511658465489745
iteration 66, loss = 0.003630520310252905
iteration 67, loss = 0.0023169508203864098
iteration 68, loss = 0.0020674976985901594
iteration 69, loss = 0.0025118766352534294
iteration 70, loss = 0.002054535783827305
iteration 71, loss = 0.002109931083396077
iteration 72, loss = 0.0027627251110970974
iteration 73, loss = 0.002237859647721052
iteration 74, loss = 0.002293905708938837
iteration 75, loss = 0.0018294979818165302
iteration 76, loss = 0.0020019435323774815
iteration 77, loss = 0.0022316891700029373
iteration 78, loss = 0.0030444166623055935
iteration 79, loss = 0.002064281841740012
iteration 80, loss = 0.002118918811902404
iteration 81, loss = 0.0021578704472631216
iteration 82, loss = 0.00158130272757262
iteration 83, loss = 0.0031650078017264605
iteration 84, loss = 0.002971276640892029
iteration 85, loss = 0.0020368979312479496
iteration 86, loss = 0.0017999490955844522
iteration 87, loss = 0.0021743373945355415
iteration 88, loss = 0.0017232540994882584
iteration 89, loss = 0.001588325365446508
iteration 90, loss = 0.00276876799762249
iteration 91, loss = 0.0017103003337979317
iteration 92, loss = 0.0021076533012092113
iteration 93, loss = 0.0019760250579565763
iteration 94, loss = 0.004514768254011869
iteration 95, loss = 0.0018495931290090084
iteration 96, loss = 0.0020016408525407314
iteration 97, loss = 0.001867929589934647
iteration 98, loss = 0.002202701522037387
iteration 99, loss = 0.0024131089448928833
iteration 100, loss = 0.0019700219854712486
iteration 101, loss = 0.003948756027966738
iteration 102, loss = 0.0034699926618486643
iteration 103, loss = 0.0018177191959694028
iteration 104, loss = 0.002586950547993183
iteration 105, loss = 0.002280102577060461
iteration 106, loss = 0.002505395095795393
iteration 107, loss = 0.0017439608927816153
iteration 108, loss = 0.00252159615047276
iteration 109, loss = 0.001745158457197249
iteration 110, loss = 0.0025978810153901577
iteration 111, loss = 0.0028534960001707077
iteration 112, loss = 0.002921303501352668
iteration 113, loss = 0.002169250976294279
iteration 114, loss = 0.0025479644536972046
iteration 115, loss = 0.00232028984464705
iteration 116, loss = 0.0019186148419976234
iteration 117, loss = 0.0031338867265731096
iteration 118, loss = 0.0025344863533973694
iteration 119, loss = 0.0041044848039746284
iteration 120, loss = 0.002540124813094735
iteration 121, loss = 0.003131682053208351
iteration 122, loss = 0.0019361390732228756
iteration 123, loss = 0.0017936807125806808
iteration 124, loss = 0.004191809333860874
iteration 125, loss = 0.0025317843537777662
iteration 126, loss = 0.002119210083037615
iteration 127, loss = 0.0020641630981117487
iteration 128, loss = 0.0025013925042003393
iteration 129, loss = 0.0020085847936570644
iteration 130, loss = 0.0017948708264157176
iteration 131, loss = 0.0021509763319045305
iteration 132, loss = 0.0017682359321042895
iteration 133, loss = 0.0019380970625206828
iteration 134, loss = 0.0023472192697227
iteration 135, loss = 0.002236587693914771
iteration 136, loss = 0.003944853320717812
iteration 137, loss = 0.0018937501590698957
iteration 138, loss = 0.0017184007447212934
iteration 139, loss = 0.0017459826776757836
iteration 140, loss = 0.0018425444141030312
iteration 141, loss = 0.003005712293088436
iteration 142, loss = 0.0026584365405142307
iteration 143, loss = 0.0039652613922953606
iteration 144, loss = 0.0022016444709151983
iteration 145, loss = 0.0020920317620038986
iteration 146, loss = 0.0018961627501994371
iteration 147, loss = 0.0021756526548415422
iteration 148, loss = 0.001992554636672139
iteration 149, loss = 0.0025564555544406176
iteration 150, loss = 0.0020849595312029123
iteration 151, loss = 0.002281046938151121
iteration 152, loss = 0.0022179249208420515
iteration 153, loss = 0.0023754769936203957
iteration 154, loss = 0.003385272342711687
iteration 155, loss = 0.0023934824857860804
iteration 156, loss = 0.0025550355203449726
iteration 157, loss = 0.00223280256614089
iteration 158, loss = 0.0020658723078668118
iteration 159, loss = 0.00243800925090909
iteration 160, loss = 0.001939259236678481
iteration 161, loss = 0.0018818880198523402
iteration 162, loss = 0.0022397423163056374
iteration 163, loss = 0.0021070297807455063
iteration 164, loss = 0.0020241078455001116
iteration 165, loss = 0.002061570528894663
iteration 166, loss = 0.0019392528338357806
iteration 167, loss = 0.0017881056992337108
iteration 168, loss = 0.0020358175970613956
iteration 169, loss = 0.002953942632302642
iteration 170, loss = 0.0020110008772462606
iteration 171, loss = 0.002181870164349675
iteration 172, loss = 0.0020501408725976944
iteration 173, loss = 0.0022862807381898165
iteration 174, loss = 0.002057881560176611
iteration 175, loss = 0.0019436179427430034
iteration 176, loss = 0.0024283064994961023
iteration 177, loss = 0.0017767007229849696
iteration 178, loss = 0.002853922313079238
iteration 179, loss = 0.002960394136607647
iteration 180, loss = 0.004804923664778471
iteration 181, loss = 0.0022665865253657103
iteration 182, loss = 0.0020479727536439896
iteration 183, loss = 0.0020366981625556946
iteration 184, loss = 0.003133088117465377
iteration 185, loss = 0.002534747589379549
iteration 186, loss = 0.0022323576267808676
iteration 187, loss = 0.0019728494808077812
iteration 188, loss = 0.0018270587315782905
iteration 189, loss = 0.0025787195190787315
iteration 190, loss = 0.0016688884934410453
iteration 191, loss = 0.0016791175585240126
iteration 192, loss = 0.0023077409714460373
iteration 193, loss = 0.0021882413420826197
iteration 194, loss = 0.0021512832026928663
iteration 195, loss = 0.00207136245444417
iteration 196, loss = 0.0019235113868489861
iteration 197, loss = 0.0020506693981587887
iteration 198, loss = 0.002541234949603677
iteration 199, loss = 0.0037652759347110987
iteration 200, loss = 0.0040198080241680145
iteration 201, loss = 0.0019689491018652916
iteration 202, loss = 0.005933533888310194
iteration 203, loss = 0.002951087662950158
iteration 204, loss = 0.002854895079508424
iteration 205, loss = 0.0022870851680636406
iteration 206, loss = 0.002219580579549074
iteration 207, loss = 0.0024502300657331944
iteration 208, loss = 0.0023373691365122795
iteration 209, loss = 0.001789629110135138
iteration 210, loss = 0.0024912459775805473
iteration 211, loss = 0.0030388946179300547
iteration 212, loss = 0.002979722572490573
iteration 213, loss = 0.0017962101846933365
iteration 214, loss = 0.0019172343891113997
iteration 215, loss = 0.0019115502946078777
iteration 216, loss = 0.002308598719537258
iteration 217, loss = 0.002768917940557003
iteration 218, loss = 0.001826839055866003
iteration 219, loss = 0.002118109492585063
iteration 220, loss = 0.0020622347947210073
iteration 221, loss = 0.0020738900639116764
iteration 222, loss = 0.001737154321745038
iteration 223, loss = 0.0024211835116147995
iteration 224, loss = 0.001958449138328433
iteration 225, loss = 0.0028708658646792173
iteration 226, loss = 0.0019573469180613756
iteration 227, loss = 0.0019562505185604095
iteration 228, loss = 0.0021575000137090683
iteration 229, loss = 0.0024961179587990046
iteration 230, loss = 0.002202140400186181
iteration 231, loss = 0.00219891220331192
iteration 232, loss = 0.0021806922741234303
iteration 233, loss = 0.002062723273411393
iteration 234, loss = 0.0018812943017110229
iteration 235, loss = 0.0020660539157688618
iteration 236, loss = 0.002115868031978607
iteration 237, loss = 0.0032534210477024317
iteration 238, loss = 0.002933997893705964
iteration 239, loss = 0.002880755113437772
iteration 240, loss = 0.0031845313496887684
iteration 241, loss = 0.0020116069354116917
iteration 242, loss = 0.003095613094046712
iteration 243, loss = 0.002049391157925129
iteration 244, loss = 0.004346237983554602
iteration 245, loss = 0.001891913590952754
iteration 246, loss = 0.003934387117624283
iteration 247, loss = 0.0019445288926362991
iteration 248, loss = 0.0022394040133804083
iteration 249, loss = 0.0020414707250893116
iteration 250, loss = 0.001865220838226378
iteration 251, loss = 0.002056756289675832
iteration 252, loss = 0.0033874730579555035
iteration 253, loss = 0.001954400911927223
iteration 254, loss = 0.001954343868419528
iteration 255, loss = 0.0031713771168142557
iteration 256, loss = 0.002158255083486438
iteration 257, loss = 0.003155178390443325
iteration 258, loss = 0.0026851287111639977
iteration 259, loss = 0.0032711599487811327
iteration 260, loss = 0.002283121459186077
iteration 261, loss = 0.0025920646730810404
iteration 262, loss = 0.0029309047386050224
iteration 263, loss = 0.0034119663760066032
iteration 264, loss = 0.0019954435992985964
iteration 265, loss = 0.0023117519449442625
iteration 266, loss = 0.004409635439515114
iteration 267, loss = 0.0026291983667761087
iteration 268, loss = 0.002215268788859248
iteration 269, loss = 0.0020292066037654877
iteration 270, loss = 0.002315301913768053
iteration 271, loss = 0.0038195678498595953
iteration 272, loss = 0.003543365281075239
iteration 273, loss = 0.001971303950995207
iteration 274, loss = 0.002905429108068347
iteration 275, loss = 0.0022558134514838457
iteration 276, loss = 0.0031910191755741835
iteration 277, loss = 0.003699437715113163
iteration 278, loss = 0.002047695219516754
iteration 279, loss = 0.0018552012043073773
iteration 280, loss = 0.0037930479738861322
iteration 281, loss = 0.0022450252436101437
iteration 282, loss = 0.0022172145545482635
iteration 283, loss = 0.001988846343010664
iteration 284, loss = 0.0024018806871026754
iteration 285, loss = 0.0023284286726266146
iteration 286, loss = 0.0022486315574496984
iteration 287, loss = 0.0017727228114381433
iteration 288, loss = 0.0021201414056122303
iteration 289, loss = 0.0035975466016680002
iteration 290, loss = 0.0019451446132734418
iteration 291, loss = 0.00198698160238564
iteration 292, loss = 0.0022374452091753483
iteration 293, loss = 0.002168529201298952
iteration 294, loss = 0.002593889134004712
iteration 295, loss = 0.00206215912476182
iteration 296, loss = 0.002047908026725054
iteration 297, loss = 0.002133541041985154
iteration 298, loss = 0.002029063180088997
iteration 299, loss = 0.0027670771814882755
iteration 300, loss = 0.002727310638874769
iteration 1, loss = 0.0020642406307160854
iteration 2, loss = 0.001942533883266151
iteration 3, loss = 0.0021963934414088726
iteration 4, loss = 0.0020928916055709124
iteration 5, loss = 0.002855826634913683
iteration 6, loss = 0.0022634731139987707
iteration 7, loss = 0.002531861187890172
iteration 8, loss = 0.0022717288229614496
iteration 9, loss = 0.0019416776485741138
iteration 10, loss = 0.002049824455752969
iteration 11, loss = 0.0023126958403736353
iteration 12, loss = 0.0026137353852391243
iteration 13, loss = 0.0026891769375652075
iteration 14, loss = 0.0025206697173416615
iteration 15, loss = 0.0022446508519351482
iteration 16, loss = 0.0021543572656810284
iteration 17, loss = 0.002521493937820196
iteration 18, loss = 0.0028691315092146397
iteration 19, loss = 0.0024009954649955034
iteration 20, loss = 0.0020012117456644773
iteration 21, loss = 0.001978730084374547
iteration 22, loss = 0.0024226431269198656
iteration 23, loss = 0.004593834280967712
iteration 24, loss = 0.0020417445339262486
iteration 25, loss = 0.0021140924654901028
iteration 26, loss = 0.0021564243361353874
iteration 27, loss = 0.005930100567638874
iteration 28, loss = 0.002190448110923171
iteration 29, loss = 0.0017966639716178179
iteration 30, loss = 0.0020492400508373976
iteration 31, loss = 0.0019015803700312972
iteration 32, loss = 0.0022234898060560226
iteration 33, loss = 0.0017859714571386576
iteration 34, loss = 0.002331749303266406
iteration 35, loss = 0.0015799711691215634
iteration 36, loss = 0.0028940450865775347
iteration 37, loss = 0.002994197653606534
iteration 38, loss = 0.0021767020225524902
iteration 39, loss = 0.0020554771181195974
iteration 40, loss = 0.0020347752142697573
iteration 41, loss = 0.0026479274965822697
iteration 42, loss = 0.002037649042904377
iteration 43, loss = 0.001974707003682852
iteration 44, loss = 0.00319195375777781
iteration 45, loss = 0.004066559486091137
iteration 46, loss = 0.002221910282969475
iteration 47, loss = 0.0028002532199025154
iteration 48, loss = 0.002684539183974266
iteration 49, loss = 0.002386433072388172
iteration 50, loss = 0.004070804920047522
iteration 51, loss = 0.0019856079015880823
iteration 52, loss = 0.0022996547631919384
iteration 53, loss = 0.002617968013510108
iteration 54, loss = 0.0024114977568387985
iteration 55, loss = 0.0018774974159896374
iteration 56, loss = 0.002973446622490883
iteration 57, loss = 0.003626888617873192
iteration 58, loss = 0.0023538516834378242
iteration 59, loss = 0.0021492058876901865
iteration 60, loss = 0.0022498895414173603
iteration 61, loss = 0.0037961737252771854
iteration 62, loss = 0.0021105383057147264
iteration 63, loss = 0.0018712212331593037
iteration 64, loss = 0.002786260098218918
iteration 65, loss = 0.0019006977090612054
iteration 66, loss = 0.0021391762420535088
iteration 67, loss = 0.003950852435082197
iteration 68, loss = 0.001977914245799184
iteration 69, loss = 0.002661398844793439
iteration 70, loss = 0.0020618729759007692
iteration 71, loss = 0.0021703727543354034
iteration 72, loss = 0.0022256658412516117
iteration 73, loss = 0.0018297022907063365
iteration 74, loss = 0.002016917336732149
iteration 75, loss = 0.0019639197271317244
iteration 76, loss = 0.0020146239548921585
iteration 77, loss = 0.0019748699851334095
iteration 78, loss = 0.00441151624545455
iteration 79, loss = 0.0028841858729720116
iteration 80, loss = 0.004566911607980728
iteration 81, loss = 0.001789386267773807
iteration 82, loss = 0.0018944909097626805
iteration 83, loss = 0.0020710076205432415
iteration 84, loss = 0.0019757896661758423
iteration 85, loss = 0.0020562910940498114
iteration 86, loss = 0.0021588378585875034
iteration 87, loss = 0.0016071534482762218
iteration 88, loss = 0.0020920850802212954
iteration 89, loss = 0.00193260726518929
iteration 90, loss = 0.002619262086227536
iteration 91, loss = 0.0018085766350850463
iteration 92, loss = 0.0022533051669597626
iteration 93, loss = 0.003978263586759567
iteration 94, loss = 0.0020408369600772858
iteration 95, loss = 0.002123695332556963
iteration 96, loss = 0.002353152260184288
iteration 97, loss = 0.0020399726927280426
iteration 98, loss = 0.002244476228952408
iteration 99, loss = 0.003498583333566785
iteration 100, loss = 0.002349734539166093
iteration 101, loss = 0.0018883032025769353
iteration 102, loss = 0.002271424513310194
iteration 103, loss = 0.002235283376649022
iteration 104, loss = 0.002170293126255274
iteration 105, loss = 0.001895805587992072
iteration 106, loss = 0.002410817425698042
iteration 107, loss = 0.0022994994651526213
iteration 108, loss = 0.004062183201313019
iteration 109, loss = 0.002430284395813942
iteration 110, loss = 0.003552352311089635
iteration 111, loss = 0.0020815713796764612
iteration 112, loss = 0.0021831609774380922
iteration 113, loss = 0.002544925082474947
iteration 114, loss = 0.0027158975135535
iteration 115, loss = 0.0019123608944937587
iteration 116, loss = 0.0019561361987143755
iteration 117, loss = 0.0018645500531420112
iteration 118, loss = 0.0019889501854777336
iteration 119, loss = 0.001797121367417276
iteration 120, loss = 0.002735138637945056
iteration 121, loss = 0.002725117141380906
iteration 122, loss = 0.002161875134333968
iteration 123, loss = 0.0019146772101521492
iteration 124, loss = 0.002452526241540909
iteration 125, loss = 0.0021337056532502174
iteration 126, loss = 0.002200936898589134
iteration 127, loss = 0.0026163700968027115
iteration 128, loss = 0.001832842011936009
iteration 129, loss = 0.003037427319213748
iteration 130, loss = 0.002294569741934538
iteration 131, loss = 0.0017699891468510032
iteration 132, loss = 0.0018078044522553682
iteration 133, loss = 0.0022294430527836084
iteration 134, loss = 0.002151812193915248
iteration 135, loss = 0.0018056846456602216
iteration 136, loss = 0.0028271772898733616
iteration 137, loss = 0.0027865166775882244
iteration 138, loss = 0.0024107005447149277
iteration 139, loss = 0.0020903930999338627
iteration 140, loss = 0.003577729221433401
iteration 141, loss = 0.0026604975573718548
iteration 142, loss = 0.0025247950106859207
iteration 143, loss = 0.003272458678111434
iteration 144, loss = 0.0017817209009081125
iteration 145, loss = 0.0023801696952432394
iteration 146, loss = 0.002145713660866022
iteration 147, loss = 0.002996905706822872
iteration 148, loss = 0.0033683474175632
iteration 149, loss = 0.0017071712063625455
iteration 150, loss = 0.0026037092320621014
iteration 151, loss = 0.0020487592555582523
iteration 152, loss = 0.002075919881463051
iteration 153, loss = 0.0017906216671690345
iteration 154, loss = 0.0021866513416171074
iteration 155, loss = 0.002461182652041316
iteration 156, loss = 0.0024152062833309174
iteration 157, loss = 0.0023427424021065235
iteration 158, loss = 0.0018531305249780416
iteration 159, loss = 0.002119337907060981
iteration 160, loss = 0.0018981349421665072
iteration 161, loss = 0.0017768110847100616
iteration 162, loss = 0.002781371818855405
iteration 163, loss = 0.0026118531823158264
iteration 164, loss = 0.0025070009287446737
iteration 165, loss = 0.00234237196855247
iteration 166, loss = 0.0022001878824084997
iteration 167, loss = 0.003445613430812955
iteration 168, loss = 0.0020463294349610806
iteration 169, loss = 0.001911883126012981
iteration 170, loss = 0.004615369252860546
iteration 171, loss = 0.0020250268280506134
iteration 172, loss = 0.002134654438123107
iteration 173, loss = 0.0021496815606951714
iteration 174, loss = 0.0021980521269142628
iteration 175, loss = 0.0022709108889102936
iteration 176, loss = 0.0017688889056444168
iteration 177, loss = 0.001995314145460725
iteration 178, loss = 0.001978921936824918
iteration 179, loss = 0.0019663621205836535
iteration 180, loss = 0.002824691589921713
iteration 181, loss = 0.004010334145277739
iteration 182, loss = 0.0022601590026170015
iteration 183, loss = 0.00196383660659194
iteration 184, loss = 0.0021964635234326124
iteration 185, loss = 0.002177477814257145
iteration 186, loss = 0.003867154009640217
iteration 187, loss = 0.0025150871369987726
iteration 188, loss = 0.0018619816983118653
iteration 189, loss = 0.0026201338041573763
iteration 190, loss = 0.002098233439028263
iteration 191, loss = 0.0020668846555054188
iteration 192, loss = 0.002158522605895996
iteration 193, loss = 0.0018021224532276392
iteration 194, loss = 0.002226355019956827
iteration 195, loss = 0.0021920937579125166
iteration 196, loss = 0.0031084027141332626
iteration 197, loss = 0.004501220770180225
iteration 198, loss = 0.0021949619986116886
iteration 199, loss = 0.0041183591820299625
iteration 200, loss = 0.0019504602532833815
iteration 201, loss = 0.002499277237802744
iteration 202, loss = 0.0018749984446913004
iteration 203, loss = 0.001988767646253109
iteration 204, loss = 0.0017754781292751431
iteration 205, loss = 0.003126173745840788
iteration 206, loss = 0.0019225806463509798
iteration 207, loss = 0.001925288699567318
iteration 208, loss = 0.0027898706030100584
iteration 209, loss = 0.0015797391533851624
iteration 210, loss = 0.002413983689621091
iteration 211, loss = 0.0034149461425840855
iteration 212, loss = 0.002562203910201788
iteration 213, loss = 0.003022784134373069
iteration 214, loss = 0.002075732918456197
iteration 215, loss = 0.001744820736348629
iteration 216, loss = 0.00251559866592288
iteration 217, loss = 0.001737056183628738
iteration 218, loss = 0.0025465418584644794
iteration 219, loss = 0.0020928785670548677
iteration 220, loss = 0.002274508122354746
iteration 221, loss = 0.0029524913989007473
iteration 222, loss = 0.0019050926202908158
iteration 223, loss = 0.002198029775172472
iteration 224, loss = 0.002055587712675333
iteration 225, loss = 0.0015507345087826252
iteration 226, loss = 0.0020532605703920126
iteration 227, loss = 0.0018140183528885245
iteration 228, loss = 0.002548552118241787
iteration 229, loss = 0.0027660089544951916
iteration 230, loss = 0.002172524342313409
iteration 231, loss = 0.001832538633607328
iteration 232, loss = 0.001840086653828621
iteration 233, loss = 0.0019877152517437935
iteration 234, loss = 0.0019674794748425484
iteration 235, loss = 0.0019047067034989595
iteration 236, loss = 0.002168115461245179
iteration 237, loss = 0.0015988056547939777
iteration 238, loss = 0.001941267866641283
iteration 239, loss = 0.002036097925156355
iteration 240, loss = 0.0024253325536847115
iteration 241, loss = 0.002615597564727068
iteration 242, loss = 0.0018786354921758175
iteration 243, loss = 0.001830823253840208
iteration 244, loss = 0.0034831014927476645
iteration 245, loss = 0.002251747762784362
iteration 246, loss = 0.002400462282821536
iteration 247, loss = 0.0018465281464159489
iteration 248, loss = 0.001851204433478415
iteration 249, loss = 0.0020271039102226496
iteration 250, loss = 0.0024170316755771637
iteration 251, loss = 0.0020346944220364094
iteration 252, loss = 0.002814596053212881
iteration 253, loss = 0.001944496063515544
iteration 254, loss = 0.0019887094385921955
iteration 255, loss = 0.0017890608869493008
iteration 256, loss = 0.002069339156150818
iteration 257, loss = 0.0022919366601854563
iteration 258, loss = 0.0022798592690378428
iteration 259, loss = 0.002101148711517453
iteration 260, loss = 0.0021484813187271357
iteration 261, loss = 0.0021400463301688433
iteration 262, loss = 0.0021668619010597467
iteration 263, loss = 0.0021677359472960234
iteration 264, loss = 0.002367574255913496
iteration 265, loss = 0.004841479007154703
iteration 266, loss = 0.001792951487004757
iteration 267, loss = 0.001688114833086729
iteration 268, loss = 0.0019000401953235269
iteration 269, loss = 0.002235431456938386
iteration 270, loss = 0.004184103570878506
iteration 271, loss = 0.0020685370545834303
iteration 272, loss = 0.003413937985897064
iteration 273, loss = 0.0033923787996172905
iteration 274, loss = 0.0023754583671689034
iteration 275, loss = 0.0021769411396235228
iteration 276, loss = 0.002151313005015254
iteration 277, loss = 0.0021568185184150934
iteration 278, loss = 0.004244515206664801
iteration 279, loss = 0.0028043733909726143
iteration 280, loss = 0.002599248895421624
iteration 281, loss = 0.004168605897575617
iteration 282, loss = 0.0021167176309973
iteration 283, loss = 0.0027013157960027456
iteration 284, loss = 0.0039623938500881195
iteration 285, loss = 0.0023971672635525465
iteration 286, loss = 0.0031267181038856506
iteration 287, loss = 0.0018280892400071025
iteration 288, loss = 0.0033230711705982685
iteration 289, loss = 0.0020082013215869665
iteration 290, loss = 0.002847283147275448
iteration 291, loss = 0.0027661696076393127
iteration 292, loss = 0.0016564576653763652
iteration 293, loss = 0.0024601873010396957
iteration 294, loss = 0.002294839359819889
iteration 295, loss = 0.002057940699160099
iteration 296, loss = 0.003277068492025137
iteration 297, loss = 0.0016744942404329777
iteration 298, loss = 0.004697943571954966
iteration 299, loss = 0.002209447557106614
iteration 300, loss = 0.001991068944334984
iteration 1, loss = 0.0017967668827623129
iteration 2, loss = 0.001716136233881116
iteration 3, loss = 0.0017992428038269281
iteration 4, loss = 0.0020881174132227898
iteration 5, loss = 0.0018843041034415364
iteration 6, loss = 0.002232748782262206
iteration 7, loss = 0.0017500362591817975
iteration 8, loss = 0.002127544954419136
iteration 9, loss = 0.002253294223919511
iteration 10, loss = 0.002082976745441556
iteration 11, loss = 0.0020645337644964457
iteration 12, loss = 0.0041572717018425465
iteration 13, loss = 0.0026283494662493467
iteration 14, loss = 0.0024192749988287687
iteration 15, loss = 0.0020886154379695654
iteration 16, loss = 0.002292902208864689
iteration 17, loss = 0.0023231261875480413
iteration 18, loss = 0.0033435416407883167
iteration 19, loss = 0.0019496686290949583
iteration 20, loss = 0.003547034692019224
iteration 21, loss = 0.0017371164867654443
iteration 22, loss = 0.0016334477113559842
iteration 23, loss = 0.002454608678817749
iteration 24, loss = 0.0037153633311390877
iteration 25, loss = 0.0022634195629507303
iteration 26, loss = 0.0024315824266523123
iteration 27, loss = 0.0017726155929267406
iteration 28, loss = 0.0018484739121049643
iteration 29, loss = 0.001965802162885666
iteration 30, loss = 0.002032805234193802
iteration 31, loss = 0.002045591128990054
iteration 32, loss = 0.002044192748144269
iteration 33, loss = 0.0022097143810242414
iteration 34, loss = 0.0020362285431474447
iteration 35, loss = 0.0031411827076226473
iteration 36, loss = 0.0022571594454348087
iteration 37, loss = 0.0021200755145400763
iteration 38, loss = 0.0023727004881948233
iteration 39, loss = 0.0025354516692459583
iteration 40, loss = 0.001830551540479064
iteration 41, loss = 0.002043338492512703
iteration 42, loss = 0.005027457606047392
iteration 43, loss = 0.0020689188968390226
iteration 44, loss = 0.0028278296813368797
iteration 45, loss = 0.002740070689469576
iteration 46, loss = 0.0020339167676866055
iteration 47, loss = 0.0020130183547735214
iteration 48, loss = 0.0024962963070720434
iteration 49, loss = 0.002156699076294899
iteration 50, loss = 0.0021545488853007555
iteration 51, loss = 0.002049318514764309
iteration 52, loss = 0.0024300934746861458
iteration 53, loss = 0.003911323379725218
iteration 54, loss = 0.002154739573597908
iteration 55, loss = 0.0017139463452622294
iteration 56, loss = 0.002483619377017021
iteration 57, loss = 0.0022295827511698008
iteration 58, loss = 0.0027152972761541605
iteration 59, loss = 0.0017078716773539782
iteration 60, loss = 0.0019779279828071594
iteration 61, loss = 0.0024120539892464876
iteration 62, loss = 0.002023066394031048
iteration 63, loss = 0.0016195030184462667
iteration 64, loss = 0.002248960779979825
iteration 65, loss = 0.0023926591966301203
iteration 66, loss = 0.0020052543841302395
iteration 67, loss = 0.0034956715535372496
iteration 68, loss = 0.002175892237573862
iteration 69, loss = 0.0019314588280394673
iteration 70, loss = 0.0037320072297006845
iteration 71, loss = 0.005762296728789806
iteration 72, loss = 0.002121169352903962
iteration 73, loss = 0.004269219469279051
iteration 74, loss = 0.002831546124070883
iteration 75, loss = 0.002771960571408272
iteration 76, loss = 0.00189026421867311
iteration 77, loss = 0.0020588708575814962
iteration 78, loss = 0.001749316230416298
iteration 79, loss = 0.0038456402253359556
iteration 80, loss = 0.0019314337987452745
iteration 81, loss = 0.0025921857450157404
iteration 82, loss = 0.00201965868473053
iteration 83, loss = 0.002278613392263651
iteration 84, loss = 0.002102859318256378
iteration 85, loss = 0.0022518280893564224
iteration 86, loss = 0.0019554712343961
iteration 87, loss = 0.002436885144561529
iteration 88, loss = 0.0032629743218421936
iteration 89, loss = 0.002305838279426098
iteration 90, loss = 0.0026218874845653772
iteration 91, loss = 0.0020007318817079067
iteration 92, loss = 0.002378164092078805
iteration 93, loss = 0.003932508639991283
iteration 94, loss = 0.001954593462869525
iteration 95, loss = 0.001928128069266677
iteration 96, loss = 0.0021260720677673817
iteration 97, loss = 0.0022539650090038776
iteration 98, loss = 0.0020422169473022223
iteration 99, loss = 0.00187481299508363
iteration 100, loss = 0.0021797860972583294
iteration 101, loss = 0.004831231199204922
iteration 102, loss = 0.00204299483448267
iteration 103, loss = 0.0020826859399676323
iteration 104, loss = 0.0018023377051576972
iteration 105, loss = 0.0019165752455592155
iteration 106, loss = 0.002333597978577018
iteration 107, loss = 0.0032922932878136635
iteration 108, loss = 0.0018907837802544236
iteration 109, loss = 0.0017435996560379863
iteration 110, loss = 0.0022882758639752865
iteration 111, loss = 0.0018532536923885345
iteration 112, loss = 0.0018800521502271295
iteration 113, loss = 0.001819727011024952
iteration 114, loss = 0.0020019616931676865
iteration 115, loss = 0.002105221152305603
iteration 116, loss = 0.0022438927553594112
iteration 117, loss = 0.002623489359393716
iteration 118, loss = 0.0028105671517550945
iteration 119, loss = 0.001972669968381524
iteration 120, loss = 0.0027173487469553947
iteration 121, loss = 0.001882188837043941
iteration 122, loss = 0.00225174892693758
iteration 123, loss = 0.0029258239082992077
iteration 124, loss = 0.001933861873112619
iteration 125, loss = 0.001774382428266108
iteration 126, loss = 0.002352456096559763
iteration 127, loss = 0.002053842879831791
iteration 128, loss = 0.0020867837592959404
iteration 129, loss = 0.002040778985247016
iteration 130, loss = 0.0033422717824578285
iteration 131, loss = 0.0027739834040403366
iteration 132, loss = 0.002214191248640418
iteration 133, loss = 0.002100026933476329
iteration 134, loss = 0.0021620774641633034
iteration 135, loss = 0.002232407918199897
iteration 136, loss = 0.0017533991485834122
iteration 137, loss = 0.0041715591214597225
iteration 138, loss = 0.004793868865817785
iteration 139, loss = 0.0018252526642754674
iteration 140, loss = 0.0020503976847976446
iteration 141, loss = 0.002007467905059457
iteration 142, loss = 0.003261509584262967
iteration 143, loss = 0.002242727903649211
iteration 144, loss = 0.0029357573948800564
iteration 145, loss = 0.0018138213781639934
iteration 146, loss = 0.0018169607501477003
iteration 147, loss = 0.003178176935762167
iteration 148, loss = 0.0029861684888601303
iteration 149, loss = 0.0021424833685159683
iteration 150, loss = 0.0041749500669538975
iteration 151, loss = 0.002541101537644863
iteration 152, loss = 0.002261733403429389
iteration 153, loss = 0.0020805220119655132
iteration 154, loss = 0.0024248070549219847
iteration 155, loss = 0.002121038269251585
iteration 156, loss = 0.0026852686423808336
iteration 157, loss = 0.004730422515422106
iteration 158, loss = 0.0017429296858608723
iteration 159, loss = 0.0019617779180407524
iteration 160, loss = 0.003350170562043786
iteration 161, loss = 0.002755430992692709
iteration 162, loss = 0.0023868000134825706
iteration 163, loss = 0.0031683032866567373
iteration 164, loss = 0.0021835812367498875
iteration 165, loss = 0.0017913965275511146
iteration 166, loss = 0.0021706672850996256
iteration 167, loss = 0.0022296588867902756
iteration 168, loss = 0.003912498243153095
iteration 169, loss = 0.0020744551438838243
iteration 170, loss = 0.0018717225175350904
iteration 171, loss = 0.002049538539722562
iteration 172, loss = 0.0023493636399507523
iteration 173, loss = 0.002150759333744645
iteration 174, loss = 0.0028806552290916443
iteration 175, loss = 0.0018675001338124275
iteration 176, loss = 0.0032454754691570997
iteration 177, loss = 0.0030044671148061752
iteration 178, loss = 0.002409740351140499
iteration 179, loss = 0.0018254182068631053
iteration 180, loss = 0.0032833735458552837
iteration 181, loss = 0.0027346396818757057
iteration 182, loss = 0.0020612827502191067
iteration 183, loss = 0.003098874120041728
iteration 184, loss = 0.002400590106844902
iteration 185, loss = 0.003917459398508072
iteration 186, loss = 0.004082766827195883
iteration 187, loss = 0.0020847665145993233
iteration 188, loss = 0.002120506251230836
iteration 189, loss = 0.0017709782114252448
iteration 190, loss = 0.0019666608422994614
iteration 191, loss = 0.0025030120741575956
iteration 192, loss = 0.00210554082877934
iteration 193, loss = 0.0017509062308818102
iteration 194, loss = 0.0031854070257395506
iteration 195, loss = 0.003445614594966173
iteration 196, loss = 0.0021407529711723328
iteration 197, loss = 0.002293674973770976
iteration 198, loss = 0.0020070176105946302
iteration 199, loss = 0.0025608239229768515
iteration 200, loss = 0.0026632254011929035
iteration 201, loss = 0.00177031010389328
iteration 202, loss = 0.001930668717250228
iteration 203, loss = 0.0018884579185396433
iteration 204, loss = 0.0021771928295493126
iteration 205, loss = 0.0019261081470176578
iteration 206, loss = 0.0019593569450080395
iteration 207, loss = 0.0021806133445352316
iteration 208, loss = 0.00297830649651587
iteration 209, loss = 0.0016928217373788357
iteration 210, loss = 0.0019189707236364484
iteration 211, loss = 0.0020582822617143393
iteration 212, loss = 0.0019555080216377974
iteration 213, loss = 0.0020829627756029367
iteration 214, loss = 0.0018207558896392584
iteration 215, loss = 0.0025273854844272137
iteration 216, loss = 0.0020196628756821156
iteration 217, loss = 0.0019467842066660523
iteration 218, loss = 0.0022814234253019094
iteration 219, loss = 0.0019323679152876139
iteration 220, loss = 0.0019682941492646933
iteration 221, loss = 0.0017990593332797289
iteration 222, loss = 0.0021467178594321012
iteration 223, loss = 0.002248575445264578
iteration 224, loss = 0.0023905064444988966
iteration 225, loss = 0.0018418794497847557
iteration 226, loss = 0.002382974373176694
iteration 227, loss = 0.00258114212192595
iteration 228, loss = 0.00226827641017735
iteration 229, loss = 0.003989508841186762
iteration 230, loss = 0.002805135678499937
iteration 231, loss = 0.0027643812354654074
iteration 232, loss = 0.0018865111051127315
iteration 233, loss = 0.002104777842760086
iteration 234, loss = 0.0019648585002869368
iteration 235, loss = 0.004140624310821295
iteration 236, loss = 0.001841775723733008
iteration 237, loss = 0.0024434959050267935
iteration 238, loss = 0.0026037052739411592
iteration 239, loss = 0.0018593171844258904
iteration 240, loss = 0.0018463663291186094
iteration 241, loss = 0.0019890412222594023
iteration 242, loss = 0.0022539901547133923
iteration 243, loss = 0.00307315681129694
iteration 244, loss = 0.002152759348973632
iteration 245, loss = 0.0026600700803101063
iteration 246, loss = 0.0023008910939097404
iteration 247, loss = 0.0033866502344608307
iteration 248, loss = 0.0027480272110551596
iteration 249, loss = 0.0037214835174381733
iteration 250, loss = 0.003745193127542734
iteration 251, loss = 0.0021367473527789116
iteration 252, loss = 0.003277428448200226
iteration 253, loss = 0.0017864629626274109
iteration 254, loss = 0.002098007593303919
iteration 255, loss = 0.0025020332541316748
iteration 256, loss = 0.002035612240433693
iteration 257, loss = 0.002561774803325534
iteration 258, loss = 0.0020755622535943985
iteration 259, loss = 0.002926056506112218
iteration 260, loss = 0.0020857262425124645
iteration 261, loss = 0.001932719605974853
iteration 262, loss = 0.0023169093765318394
iteration 263, loss = 0.0018835144583135843
iteration 264, loss = 0.002041066763922572
iteration 265, loss = 0.0025178727228194475
iteration 266, loss = 0.0019738541450351477
iteration 267, loss = 0.0018958565779030323
iteration 268, loss = 0.0019774013198912144
iteration 269, loss = 0.003529648995026946
iteration 270, loss = 0.0019618412479758263
iteration 271, loss = 0.0023028459399938583
iteration 272, loss = 0.002241320675238967
iteration 273, loss = 0.0027076215483248234
iteration 274, loss = 0.0019538518972694874
iteration 275, loss = 0.0023949199821799994
iteration 276, loss = 0.002060883678495884
iteration 277, loss = 0.0019091048743575811
iteration 278, loss = 0.0018786769360303879
iteration 279, loss = 0.001999686239287257
iteration 280, loss = 0.0015444372547790408
iteration 281, loss = 0.0024520934093743563
iteration 282, loss = 0.002883144421502948
iteration 283, loss = 0.0021018153056502342
iteration 284, loss = 0.0025815372355282307
iteration 285, loss = 0.0019167698919773102
iteration 286, loss = 0.0021778810769319534
iteration 287, loss = 0.0020586124155670404
iteration 288, loss = 0.0021025571040809155
iteration 289, loss = 0.0017571807838976383
iteration 290, loss = 0.002382255159318447
iteration 291, loss = 0.002043797168880701
iteration 292, loss = 0.0016978462226688862
iteration 293, loss = 0.002099808072671294
iteration 294, loss = 0.0021123122423887253
iteration 295, loss = 0.002549191704019904
iteration 296, loss = 0.0026754795107990503
iteration 297, loss = 0.0019031118135899305
iteration 298, loss = 0.0016707101603969932
iteration 299, loss = 0.0022751300130039454
iteration 300, loss = 0.002220949623733759
iteration 1, loss = 0.0023146781604737043
iteration 2, loss = 0.0017385458340868354
iteration 3, loss = 0.004039519000798464
iteration 4, loss = 0.0026715118438005447
iteration 5, loss = 0.0022227242588996887
iteration 6, loss = 0.0017713605193421245
iteration 7, loss = 0.0026664987672120333
iteration 8, loss = 0.0018933910178020597
iteration 9, loss = 0.0022098005283623934
iteration 10, loss = 0.0022889061365276575
iteration 11, loss = 0.0019395827548578382
iteration 12, loss = 0.0016563740791752934
iteration 13, loss = 0.0031263185665011406
iteration 14, loss = 0.002137413015589118
iteration 15, loss = 0.0023879213258624077
iteration 16, loss = 0.0019202758558094501
iteration 17, loss = 0.002094926778227091
iteration 18, loss = 0.0018217884935438633
iteration 19, loss = 0.0028669650200754404
iteration 20, loss = 0.002105580409988761
iteration 21, loss = 0.0018349902238696814
iteration 22, loss = 0.0020827564876526594
iteration 23, loss = 0.002025137422606349
iteration 24, loss = 0.0038926969282329082
iteration 25, loss = 0.004112462978810072
iteration 26, loss = 0.002457487629726529
iteration 27, loss = 0.0021263700909912586
iteration 28, loss = 0.0026720217429101467
iteration 29, loss = 0.001683526556007564
iteration 30, loss = 0.00256811804138124
iteration 31, loss = 0.0019056035671383142
iteration 32, loss = 0.001657070592045784
iteration 33, loss = 0.002197218593209982
iteration 34, loss = 0.003436682280153036
iteration 35, loss = 0.003073025494813919
iteration 36, loss = 0.00284547358751297
iteration 37, loss = 0.0025782056618481874
iteration 38, loss = 0.0029598716646432877
iteration 39, loss = 0.0025766652543097734
iteration 40, loss = 0.002915211720392108
iteration 41, loss = 0.002460018964484334
iteration 42, loss = 0.0016997449565678835
iteration 43, loss = 0.0018070213263854384
iteration 44, loss = 0.002225906355306506
iteration 45, loss = 0.0017439201474189758
iteration 46, loss = 0.0017518274253234267
iteration 47, loss = 0.002091467147693038
iteration 48, loss = 0.0023596223909407854
iteration 49, loss = 0.003052741987630725
iteration 50, loss = 0.0023186709731817245
iteration 51, loss = 0.001843252102844417
iteration 52, loss = 0.0017162533476948738
iteration 53, loss = 0.0021463145967572927
iteration 54, loss = 0.004214345943182707
iteration 55, loss = 0.002779141068458557
iteration 56, loss = 0.0023340375628322363
iteration 57, loss = 0.0022335743997246027
iteration 58, loss = 0.002559207146987319
iteration 59, loss = 0.002677604090422392
iteration 60, loss = 0.00194546056445688
iteration 61, loss = 0.0020435850601643324
iteration 62, loss = 0.0018918955465778708
iteration 63, loss = 0.002113581635057926
iteration 64, loss = 0.0033508436754345894
iteration 65, loss = 0.0018137533916160464
iteration 66, loss = 0.0030723894014954567
iteration 67, loss = 0.0021096407435834408
iteration 68, loss = 0.0018438773695379496
iteration 69, loss = 0.0025511980056762695
iteration 70, loss = 0.0019413112895563245
iteration 71, loss = 0.00206198962405324
iteration 72, loss = 0.003214758588001132
iteration 73, loss = 0.0020174451638013124
iteration 74, loss = 0.0020740970503538847
iteration 75, loss = 0.002206671517342329
iteration 76, loss = 0.0022545987740159035
iteration 77, loss = 0.002236950444057584
iteration 78, loss = 0.0023254570551216602
iteration 79, loss = 0.0018728852737694979
iteration 80, loss = 0.002032119780778885
iteration 81, loss = 0.0017605379689484835
iteration 82, loss = 0.002209364902228117
iteration 83, loss = 0.002824681345373392
iteration 84, loss = 0.0016795211704447865
iteration 85, loss = 0.002047296380624175
iteration 86, loss = 0.001829082379117608
iteration 87, loss = 0.0030146148055791855
iteration 88, loss = 0.002128300489857793
iteration 89, loss = 0.0017912877956405282
iteration 90, loss = 0.0019073934527114034
iteration 91, loss = 0.003375820815563202
iteration 92, loss = 0.001836635172367096
iteration 93, loss = 0.002621878171339631
iteration 94, loss = 0.0024129946250468493
iteration 95, loss = 0.0025371871888637543
iteration 96, loss = 0.003028209088370204
iteration 97, loss = 0.0018277867930009961
iteration 98, loss = 0.0017307735979557037
iteration 99, loss = 0.0018295992631465197
iteration 100, loss = 0.0028553414158523083
iteration 101, loss = 0.0020744726061820984
iteration 102, loss = 0.001725036301650107
iteration 103, loss = 0.003028209786862135
iteration 104, loss = 0.0027884014416486025
iteration 105, loss = 0.0024534438271075487
iteration 106, loss = 0.001983446767553687
iteration 107, loss = 0.0017393752932548523
iteration 108, loss = 0.0017277903389185667
iteration 109, loss = 0.002786867320537567
iteration 110, loss = 0.0020588792394846678
iteration 111, loss = 0.0017787875840440392
iteration 112, loss = 0.0028887793887406588
iteration 113, loss = 0.001966125098988414
iteration 114, loss = 0.001713909674435854
iteration 115, loss = 0.002300491789355874
iteration 116, loss = 0.0017261330503970385
iteration 117, loss = 0.002146962797269225
iteration 118, loss = 0.0020133901853114367
iteration 119, loss = 0.0019704513251781464
iteration 120, loss = 0.0020556626841425896
iteration 121, loss = 0.0036956584081053734
iteration 122, loss = 0.003944963216781616
iteration 123, loss = 0.004185195546597242
iteration 124, loss = 0.00283057545311749
iteration 125, loss = 0.002365362597629428
iteration 126, loss = 0.0020050506573170424
iteration 127, loss = 0.0023188991472125053
iteration 128, loss = 0.001911522471345961
iteration 129, loss = 0.002530454657971859
iteration 130, loss = 0.002089360263198614
iteration 131, loss = 0.0020462735556066036
iteration 132, loss = 0.0025114677846431732
iteration 133, loss = 0.001884733559563756
iteration 134, loss = 0.002266418654471636
iteration 135, loss = 0.0026891077868640423
iteration 136, loss = 0.001969881821423769
iteration 137, loss = 0.0022420785389840603
iteration 138, loss = 0.001878757611848414
iteration 139, loss = 0.0022556607145816088
iteration 140, loss = 0.0018630907870829105
iteration 141, loss = 0.00188533088658005
iteration 142, loss = 0.0020951428450644016
iteration 143, loss = 0.002183720702305436
iteration 144, loss = 0.003280435223132372
iteration 145, loss = 0.0033019324764609337
iteration 146, loss = 0.0019373324466869235
iteration 147, loss = 0.0035633931402117014
iteration 148, loss = 0.0016160316299647093
iteration 149, loss = 0.0022603520192205906
iteration 150, loss = 0.002316091675311327
iteration 151, loss = 0.0016907076351344585
iteration 152, loss = 0.0018764480482786894
iteration 153, loss = 0.0035220752470195293
iteration 154, loss = 0.00205131690017879
iteration 155, loss = 0.0018169332761317492
iteration 156, loss = 0.001957925269380212
iteration 157, loss = 0.001956038875505328
iteration 158, loss = 0.0024248475674539804
iteration 159, loss = 0.0019663653802126646
iteration 160, loss = 0.002855694154277444
iteration 161, loss = 0.0020312159322202206
iteration 162, loss = 0.003941791132092476
iteration 163, loss = 0.0022313569206744432
iteration 164, loss = 0.0021127560175955296
iteration 165, loss = 0.0018956302665174007
iteration 166, loss = 0.003810826689004898
iteration 167, loss = 0.0019311730284243822
iteration 168, loss = 0.002660767873749137
iteration 169, loss = 0.0017711056862026453
iteration 170, loss = 0.001900611910969019
iteration 171, loss = 0.0018936115084215999
iteration 172, loss = 0.0019091395661234856
iteration 173, loss = 0.0022377280984073877
iteration 174, loss = 0.0018832292407751083
iteration 175, loss = 0.002616083947941661
iteration 176, loss = 0.0021326185669749975
iteration 177, loss = 0.0019583790563046932
iteration 178, loss = 0.0022821666207164526
iteration 179, loss = 0.0023327008821070194
iteration 180, loss = 0.0021089736837893724
iteration 181, loss = 0.0018539116717875004
iteration 182, loss = 0.002248168922960758
iteration 183, loss = 0.005822193808853626
iteration 184, loss = 0.0034720548428595066
iteration 185, loss = 0.003208423964679241
iteration 186, loss = 0.004416957031935453
iteration 187, loss = 0.0023704662453383207
iteration 188, loss = 0.0030449084006249905
iteration 189, loss = 0.002810359001159668
iteration 190, loss = 0.0019207934383302927
iteration 191, loss = 0.002189446007832885
iteration 192, loss = 0.002187073463574052
iteration 193, loss = 0.0018507854547351599
iteration 194, loss = 0.0021174701396375895
iteration 195, loss = 0.001996187958866358
iteration 196, loss = 0.00207707192748785
iteration 197, loss = 0.0032856985926628113
iteration 198, loss = 0.005384984891861677
iteration 199, loss = 0.0021389967296272516
iteration 200, loss = 0.002054358832538128
iteration 201, loss = 0.0018984312191605568
iteration 202, loss = 0.001974594546481967
iteration 203, loss = 0.0025237498339265585
iteration 204, loss = 0.001912765554152429
iteration 205, loss = 0.0019206444267183542
iteration 206, loss = 0.0026237545534968376
iteration 207, loss = 0.0033472469076514244
iteration 208, loss = 0.0020267185755074024
iteration 209, loss = 0.0017320542829111218
iteration 210, loss = 0.002604889217764139
iteration 211, loss = 0.0028588443528860807
iteration 212, loss = 0.003174151759594679
iteration 213, loss = 0.001794171635992825
iteration 214, loss = 0.003863552352413535
iteration 215, loss = 0.0018896542023867369
iteration 216, loss = 0.003179361578077078
iteration 217, loss = 0.0019069297704845667
iteration 218, loss = 0.0019518302287906408
iteration 219, loss = 0.003000367432832718
iteration 220, loss = 0.0021982737816870213
iteration 221, loss = 0.001850030035711825
iteration 222, loss = 0.0024566641077399254
iteration 223, loss = 0.0018340845126658678
iteration 224, loss = 0.001876033958978951
iteration 225, loss = 0.002422984689474106
iteration 226, loss = 0.0018562398618087173
iteration 227, loss = 0.0024947174824774265
iteration 228, loss = 0.00277466862462461
iteration 229, loss = 0.0026450648438185453
iteration 230, loss = 0.0019436897709965706
iteration 231, loss = 0.0019374429248273373
iteration 232, loss = 0.001826477237045765
iteration 233, loss = 0.0021239640191197395
iteration 234, loss = 0.0021782107651233673
iteration 235, loss = 0.002993455622345209
iteration 236, loss = 0.002255416242405772
iteration 237, loss = 0.0019046497764065862
iteration 238, loss = 0.0022387190256267786
iteration 239, loss = 0.0021648239344358444
iteration 240, loss = 0.003352305619046092
iteration 241, loss = 0.002313886769115925
iteration 242, loss = 0.001964470138773322
iteration 243, loss = 0.00196633068844676
iteration 244, loss = 0.0020269202068448067
iteration 245, loss = 0.001912979525513947
iteration 246, loss = 0.0019369652727618814
iteration 247, loss = 0.002388294320553541
iteration 248, loss = 0.0023338054306805134
iteration 249, loss = 0.0019506376702338457
iteration 250, loss = 0.0021986900828778744
iteration 251, loss = 0.002461147727444768
iteration 252, loss = 0.0031479874160140753
iteration 253, loss = 0.0027507913764566183
iteration 254, loss = 0.002711755922064185
iteration 255, loss = 0.0021552343387156725
iteration 256, loss = 0.0026578025426715612
iteration 257, loss = 0.0019201069371774793
iteration 258, loss = 0.001596213085576892
iteration 259, loss = 0.0023282961919903755
iteration 260, loss = 0.0018924495670944452
iteration 261, loss = 0.0027562230825424194
iteration 262, loss = 0.0019481431227177382
iteration 263, loss = 0.0017475755885243416
iteration 264, loss = 0.002433944493532181
iteration 265, loss = 0.0035972881596535444
iteration 266, loss = 0.0019265013979747891
iteration 267, loss = 0.0019006907241418958
iteration 268, loss = 0.0020483091939240694
iteration 269, loss = 0.002119659911841154
iteration 270, loss = 0.0019434795249253511
iteration 271, loss = 0.001583619974553585
iteration 272, loss = 0.0021113138645887375
iteration 273, loss = 0.004118919372558594
iteration 274, loss = 0.002518669469282031
iteration 275, loss = 0.002166468882933259
iteration 276, loss = 0.0017880918458104134
iteration 277, loss = 0.002084556734189391
iteration 278, loss = 0.002295304322615266
iteration 279, loss = 0.0020735934376716614
iteration 280, loss = 0.0019969982095062733
iteration 281, loss = 0.0028311065398156643
iteration 282, loss = 0.0017672701505944133
iteration 283, loss = 0.0019350908696651459
iteration 284, loss = 0.003845835570245981
iteration 285, loss = 0.002480912022292614
iteration 286, loss = 0.0023166914470493793
iteration 287, loss = 0.0022403099574148655
iteration 288, loss = 0.004725424572825432
iteration 289, loss = 0.004537254571914673
iteration 290, loss = 0.001876584254205227
iteration 291, loss = 0.0030086443293839693
iteration 292, loss = 0.0021917782723903656
iteration 293, loss = 0.001975497230887413
iteration 294, loss = 0.0019371716771274805
iteration 295, loss = 0.0017885721754282713
iteration 296, loss = 0.0019031103001907468
iteration 297, loss = 0.0019105176907032728
iteration 298, loss = 0.00214823754504323
iteration 299, loss = 0.0018031985964626074
iteration 300, loss = 0.001771529670804739
iteration 1, loss = 0.0016852266853675246
iteration 2, loss = 0.002342172432690859
iteration 3, loss = 0.0018467678455635905
iteration 4, loss = 0.0022330302745103836
iteration 5, loss = 0.004023890942335129
iteration 6, loss = 0.0019945825915783644
iteration 7, loss = 0.0024296410847455263
iteration 8, loss = 0.0018797360826283693
iteration 9, loss = 0.0037644023541361094
iteration 10, loss = 0.004596210550516844
iteration 11, loss = 0.0022668750025331974
iteration 12, loss = 0.0019786048214882612
iteration 13, loss = 0.0026562458369880915
iteration 14, loss = 0.0017123946454375982
iteration 15, loss = 0.0024192265700548887
iteration 16, loss = 0.001851877779699862
iteration 17, loss = 0.0017231883248314261
iteration 18, loss = 0.0017901263199746609
iteration 19, loss = 0.001770285307429731
iteration 20, loss = 0.0023267464712262154
iteration 21, loss = 0.0030422075651586056
iteration 22, loss = 0.0020829339046031237
iteration 23, loss = 0.00229985686019063
iteration 24, loss = 0.0023017001803964376
iteration 25, loss = 0.0034116310998797417
iteration 26, loss = 0.0025007983203977346
iteration 27, loss = 0.0029534650966525078
iteration 28, loss = 0.001928094308823347
iteration 29, loss = 0.001652186969295144
iteration 30, loss = 0.0019725169986486435
iteration 31, loss = 0.0017046778229996562
iteration 32, loss = 0.0025053478311747313
iteration 33, loss = 0.0019383342005312443
iteration 34, loss = 0.0018350564641878009
iteration 35, loss = 0.002102096565067768
iteration 36, loss = 0.001566999009810388
iteration 37, loss = 0.0015397453680634499
iteration 38, loss = 0.002067871391773224
iteration 39, loss = 0.002073647454380989
iteration 40, loss = 0.002652164548635483
iteration 41, loss = 0.0018001380376517773
iteration 42, loss = 0.004175675567239523
iteration 43, loss = 0.0019795375410467386
iteration 44, loss = 0.0018134916899725795
iteration 45, loss = 0.0016157124191522598
iteration 46, loss = 0.0021235477179288864
iteration 47, loss = 0.002505198121070862
iteration 48, loss = 0.00196523847989738
iteration 49, loss = 0.001831638510338962
iteration 50, loss = 0.0033747972920536995
iteration 51, loss = 0.003537723794579506
iteration 52, loss = 0.0026109800674021244
iteration 53, loss = 0.002656834200024605
iteration 54, loss = 0.0019343419698998332
iteration 55, loss = 0.0016569922445341945
iteration 56, loss = 0.0021202885545790195
iteration 57, loss = 0.0029097904916852713
iteration 58, loss = 0.002121932804584503
iteration 59, loss = 0.0018583773635327816
iteration 60, loss = 0.0020286287181079388
iteration 61, loss = 0.0022780559957027435
iteration 62, loss = 0.002026760019361973
iteration 63, loss = 0.0017901698593050241
iteration 64, loss = 0.0031376234255731106
iteration 65, loss = 0.0023447643034160137
iteration 66, loss = 0.0021460247226059437
iteration 67, loss = 0.0017081720288842916
iteration 68, loss = 0.0017329182010143995
iteration 69, loss = 0.001964238937944174
iteration 70, loss = 0.0022184252738952637
iteration 71, loss = 0.0028942893259227276
iteration 72, loss = 0.0019316520774737
iteration 73, loss = 0.001869207015261054
iteration 74, loss = 0.001929368358105421
iteration 75, loss = 0.0018292018212378025
iteration 76, loss = 0.002226767363026738
iteration 77, loss = 0.0018556975992396474
iteration 78, loss = 0.0018976785941049457
iteration 79, loss = 0.0025254448410123587
iteration 80, loss = 0.0023550745099782944
iteration 81, loss = 0.0027944911271333694
iteration 82, loss = 0.0021267286501824856
iteration 83, loss = 0.004751446656882763
iteration 84, loss = 0.0024723163805902004
iteration 85, loss = 0.002220271620899439
iteration 86, loss = 0.0020204423926770687
iteration 87, loss = 0.002840110333636403
iteration 88, loss = 0.0018845843151211739
iteration 89, loss = 0.001841601449996233
iteration 90, loss = 0.002015974372625351
iteration 91, loss = 0.0021706395782530308
iteration 92, loss = 0.0019028660608455539
iteration 93, loss = 0.0022914186120033264
iteration 94, loss = 0.002027146751061082
iteration 95, loss = 0.0023090210743248463
iteration 96, loss = 0.001953667728230357
iteration 97, loss = 0.0024275463074445724
iteration 98, loss = 0.00199048756621778
iteration 99, loss = 0.0018775161588564515
iteration 100, loss = 0.0020477203652262688
iteration 101, loss = 0.002043174346908927
iteration 102, loss = 0.0019825012423098087
iteration 103, loss = 0.0035630131606012583
iteration 104, loss = 0.0020704094786196947
iteration 105, loss = 0.0018852791981771588
iteration 106, loss = 0.002360471524298191
iteration 107, loss = 0.0018448194023221731
iteration 108, loss = 0.0021115245763212442
iteration 109, loss = 0.0040384079329669476
iteration 110, loss = 0.0018725760746747255
iteration 111, loss = 0.003039306728169322
iteration 112, loss = 0.0021802671253681183
iteration 113, loss = 0.0023020326625555754
iteration 114, loss = 0.0022390440572053194
iteration 115, loss = 0.0034236751962453127
iteration 116, loss = 0.002253859769552946
iteration 117, loss = 0.0030437263194471598
iteration 118, loss = 0.0022571603767573833
iteration 119, loss = 0.0019734904635697603
iteration 120, loss = 0.003058910369873047
iteration 121, loss = 0.0019963227678090334
iteration 122, loss = 0.0019075238378718495
iteration 123, loss = 0.0027502572629600763
iteration 124, loss = 0.0019141098018735647
iteration 125, loss = 0.0015348303131759167
iteration 126, loss = 0.0017665110062807798
iteration 127, loss = 0.0020402120426297188
iteration 128, loss = 0.0017241744790226221
iteration 129, loss = 0.0034692310728132725
iteration 130, loss = 0.0017318776808679104
iteration 131, loss = 0.002073324052616954
iteration 132, loss = 0.0018253321759402752
iteration 133, loss = 0.0017474741907790303
iteration 134, loss = 0.0019855452701449394
iteration 135, loss = 0.0019467625534161925
iteration 136, loss = 0.0018418566323816776
iteration 137, loss = 0.002219930524006486
iteration 138, loss = 0.001891152816824615
iteration 139, loss = 0.0022517619654536247
iteration 140, loss = 0.0024138912558555603
iteration 141, loss = 0.0026426641270518303
iteration 142, loss = 0.002756501082330942
iteration 143, loss = 0.002906229579821229
iteration 144, loss = 0.0019468709360808134
iteration 145, loss = 0.0023586242459714413
iteration 146, loss = 0.005344957113265991
iteration 147, loss = 0.0019383366452530026
iteration 148, loss = 0.002625640481710434
iteration 149, loss = 0.002863661153241992
iteration 150, loss = 0.0018026396865025163
iteration 151, loss = 0.0018199441256001592
iteration 152, loss = 0.004698268137872219
iteration 153, loss = 0.002753759501501918
iteration 154, loss = 0.0021141848992556334
iteration 155, loss = 0.002991578308865428
iteration 156, loss = 0.0019866181537508965
iteration 157, loss = 0.0026848926208913326
iteration 158, loss = 0.0018235830357298255
iteration 159, loss = 0.002663594437763095
iteration 160, loss = 0.0016488798428326845
iteration 161, loss = 0.002596974605694413
iteration 162, loss = 0.0018258149502798915
iteration 163, loss = 0.003124980255961418
iteration 164, loss = 0.003845255821943283
iteration 165, loss = 0.0036482259165495634
iteration 166, loss = 0.0041281902231276035
iteration 167, loss = 0.0017512862104922533
iteration 168, loss = 0.0016127736307680607
iteration 169, loss = 0.0020031349267810583
iteration 170, loss = 0.001978484680876136
iteration 171, loss = 0.002127649262547493
iteration 172, loss = 0.002204009098932147
iteration 173, loss = 0.0018203412182629108
iteration 174, loss = 0.003163387766107917
iteration 175, loss = 0.0021748540457338095
iteration 176, loss = 0.0023693805560469627
iteration 177, loss = 0.0020061396062374115
iteration 178, loss = 0.003337756497785449
iteration 179, loss = 0.0037355399690568447
iteration 180, loss = 0.0021873335354030132
iteration 181, loss = 0.0019005113281309605
iteration 182, loss = 0.0017494222847744823
iteration 183, loss = 0.002044430933892727
iteration 184, loss = 0.0018729036673903465
iteration 185, loss = 0.0026429262943565845
iteration 186, loss = 0.0018307764548808336
iteration 187, loss = 0.0019831466488540173
iteration 188, loss = 0.0018538268050178885
iteration 189, loss = 0.0018326743738725781
iteration 190, loss = 0.0018842712743207812
iteration 191, loss = 0.0023615844547748566
iteration 192, loss = 0.003338378155604005
iteration 193, loss = 0.0036668337415903807
iteration 194, loss = 0.0021983981132507324
iteration 195, loss = 0.0022393285762518644
iteration 196, loss = 0.0036905875895172358
iteration 197, loss = 0.002439862582832575
iteration 198, loss = 0.0017046912107616663
iteration 199, loss = 0.0033397849183529615
iteration 200, loss = 0.0021325137931853533
iteration 201, loss = 0.002114573260769248
iteration 202, loss = 0.0036852422636002302
iteration 203, loss = 0.0023928869049996138
iteration 204, loss = 0.0022981467191129923
iteration 205, loss = 0.001886460348032415
iteration 206, loss = 0.0018511945381760597
iteration 207, loss = 0.0017033759504556656
iteration 208, loss = 0.0017911263275891542
iteration 209, loss = 0.0031715009827166796
iteration 210, loss = 0.0021523332688957453
iteration 211, loss = 0.00158199283760041
iteration 212, loss = 0.0017771758139133453
iteration 213, loss = 0.002105137100443244
iteration 214, loss = 0.0027874126099050045
iteration 215, loss = 0.0026540658436715603
iteration 216, loss = 0.002657908946275711
iteration 217, loss = 0.0027250535786151886
iteration 218, loss = 0.0019067079992964864
iteration 219, loss = 0.0016885558143258095
iteration 220, loss = 0.0020985009614378214
iteration 221, loss = 0.0028746011666953564
iteration 222, loss = 0.002168715465813875
iteration 223, loss = 0.0016328137135133147
iteration 224, loss = 0.0025562336668372154
iteration 225, loss = 0.0029522411059588194
iteration 226, loss = 0.002213028259575367
iteration 227, loss = 0.00213326676748693
iteration 228, loss = 0.002154257381334901
iteration 229, loss = 0.0021212189458310604
iteration 230, loss = 0.002522998722270131
iteration 231, loss = 0.002136332681402564
iteration 232, loss = 0.00206905510276556
iteration 233, loss = 0.0020899870432913303
iteration 234, loss = 0.001873808796517551
iteration 235, loss = 0.00191743194591254
iteration 236, loss = 0.0023143752478063107
iteration 237, loss = 0.0021815390791743994
iteration 238, loss = 0.0018862613942474127
iteration 239, loss = 0.0029568802565336227
iteration 240, loss = 0.0019262863788753748
iteration 241, loss = 0.001971309306100011
iteration 242, loss = 0.002364181214943528
iteration 243, loss = 0.002416076138615608
iteration 244, loss = 0.0018167411908507347
iteration 245, loss = 0.002661852166056633
iteration 246, loss = 0.0020519807003438473
iteration 247, loss = 0.0018376840744167566
iteration 248, loss = 0.002046607667580247
iteration 249, loss = 0.001841255696490407
iteration 250, loss = 0.001982354559004307
iteration 251, loss = 0.0024982059840112925
iteration 252, loss = 0.002018022583797574
iteration 253, loss = 0.0017414437606930733
iteration 254, loss = 0.0021508955396711826
iteration 255, loss = 0.002111520618200302
iteration 256, loss = 0.002525477670133114
iteration 257, loss = 0.0017933719791471958
iteration 258, loss = 0.0042478106915950775
iteration 259, loss = 0.0019871825352311134
iteration 260, loss = 0.001703699235804379
iteration 261, loss = 0.002032145392149687
iteration 262, loss = 0.0019448717357590795
iteration 263, loss = 0.001899772323668003
iteration 264, loss = 0.0017033166950568557
iteration 265, loss = 0.001916662440635264
iteration 266, loss = 0.0023146658204495907
iteration 267, loss = 0.0022275038063526154
iteration 268, loss = 0.002001682296395302
iteration 269, loss = 0.0022858120501041412
iteration 270, loss = 0.002178721595555544
iteration 271, loss = 0.0019434371497482061
iteration 272, loss = 0.004107764922082424
iteration 273, loss = 0.0019942158833146095
iteration 274, loss = 0.003722734749317169
iteration 275, loss = 0.002092787530273199
iteration 276, loss = 0.0041587213054299355
iteration 277, loss = 0.004534564446657896
iteration 278, loss = 0.002006846247240901
iteration 279, loss = 0.001890202285721898
iteration 280, loss = 0.0016262594144791365
iteration 281, loss = 0.0036044521257281303
iteration 282, loss = 0.0018103431211784482
iteration 283, loss = 0.002049976261332631
iteration 284, loss = 0.0021348553709685802
iteration 285, loss = 0.0020376036409288645
iteration 286, loss = 0.002011395525187254
iteration 287, loss = 0.0022637706715613604
iteration 288, loss = 0.0017773250583559275
iteration 289, loss = 0.0017668937798589468
iteration 290, loss = 0.0021352365147322416
iteration 291, loss = 0.0026158448308706284
iteration 292, loss = 0.0028291114140301943
iteration 293, loss = 0.002638446167111397
iteration 294, loss = 0.002664723200723529
iteration 295, loss = 0.0019147198181599379
iteration 296, loss = 0.00393036799505353
iteration 297, loss = 0.0046037849970161915
iteration 298, loss = 0.0021542925387620926
iteration 299, loss = 0.0025514268781989813
iteration 300, loss = 0.001971983350813389
iteration 1, loss = 0.002501249313354492
iteration 2, loss = 0.003723237430676818
iteration 3, loss = 0.0018537382129579782
iteration 4, loss = 0.003779645776376128
iteration 5, loss = 0.002094563329592347
iteration 6, loss = 0.0022092130966484547
iteration 7, loss = 0.0035047694109380245
iteration 8, loss = 0.002303388202562928
iteration 9, loss = 0.001872749300673604
iteration 10, loss = 0.0026208460330963135
iteration 11, loss = 0.002304796129465103
iteration 12, loss = 0.0018584404606372118
iteration 13, loss = 0.002947351662442088
iteration 14, loss = 0.0018123207846656442
iteration 15, loss = 0.002547960262745619
iteration 16, loss = 0.0021285961847752333
iteration 17, loss = 0.0021229039411991835
iteration 18, loss = 0.0019289935007691383
iteration 19, loss = 0.0024753869511187077
iteration 20, loss = 0.0020880361553281546
iteration 21, loss = 0.0017758028116077185
iteration 22, loss = 0.0024434204678982496
iteration 23, loss = 0.0019050298724323511
iteration 24, loss = 0.0024004168808460236
iteration 25, loss = 0.0031892575789242983
iteration 26, loss = 0.001992645440623164
iteration 27, loss = 0.0018267793348059058
iteration 28, loss = 0.002046808833256364
iteration 29, loss = 0.0030734126921743155
iteration 30, loss = 0.001696899184025824
iteration 31, loss = 0.002385887783020735
iteration 32, loss = 0.0018392479978501797
iteration 33, loss = 0.0021356858778744936
iteration 34, loss = 0.0018995128339156508
iteration 35, loss = 0.0028397333808243275
iteration 36, loss = 0.002116295276209712
iteration 37, loss = 0.002480051713064313
iteration 38, loss = 0.003326796693727374
iteration 39, loss = 0.002447376726195216
iteration 40, loss = 0.0023467927239835262
iteration 41, loss = 0.0020129892509430647
iteration 42, loss = 0.0024296718183904886
iteration 43, loss = 0.0019133843015879393
iteration 44, loss = 0.001809595851227641
iteration 45, loss = 0.0018033734522759914
iteration 46, loss = 0.002451301319524646
iteration 47, loss = 0.0031895735301077366
iteration 48, loss = 0.0018854377558454871
iteration 49, loss = 0.0028135869652032852
iteration 50, loss = 0.0020193634554743767
iteration 51, loss = 0.0021836317609995604
iteration 52, loss = 0.004387404769659042
iteration 53, loss = 0.0020994499791413546
iteration 54, loss = 0.0036345673725008965
iteration 55, loss = 0.0016048146644607186
iteration 56, loss = 0.003717747749760747
iteration 57, loss = 0.0017062128754332662
iteration 58, loss = 0.0019481164636090398
iteration 59, loss = 0.0020148681942373514
iteration 60, loss = 0.0017162347212433815
iteration 61, loss = 0.001996241509914398
iteration 62, loss = 0.0021088775247335434
iteration 63, loss = 0.00237166415899992
iteration 64, loss = 0.0018067649798467755
iteration 65, loss = 0.001901861047372222
iteration 66, loss = 0.0020993021316826344
iteration 67, loss = 0.0019709435291588306
iteration 68, loss = 0.002373605500906706
iteration 69, loss = 0.0021765476558357477
iteration 70, loss = 0.0023711458779871464
iteration 71, loss = 0.00197422644123435
iteration 72, loss = 0.0031038394663482904
iteration 73, loss = 0.0019491896964609623
iteration 74, loss = 0.002416966948658228
iteration 75, loss = 0.002183994511142373
iteration 76, loss = 0.003364295233041048
iteration 77, loss = 0.001923791947774589
iteration 78, loss = 0.0018565724603831768
iteration 79, loss = 0.003874858608469367
iteration 80, loss = 0.0017667461652308702
iteration 81, loss = 0.0021747483406215906
iteration 82, loss = 0.002386204432696104
iteration 83, loss = 0.00225132261402905
iteration 84, loss = 0.002828264608979225
iteration 85, loss = 0.002322294283658266
iteration 86, loss = 0.001777965109795332
iteration 87, loss = 0.0019940331112593412
iteration 88, loss = 0.0038363279309123755
iteration 89, loss = 0.001754616154357791
iteration 90, loss = 0.0020546712912619114
iteration 91, loss = 0.002023976296186447
iteration 92, loss = 0.0017605058383196592
iteration 93, loss = 0.002121618716046214
iteration 94, loss = 0.0030047206673771143
iteration 95, loss = 0.0020141107961535454
iteration 96, loss = 0.0018509619403630495
iteration 97, loss = 0.005185564514249563
iteration 98, loss = 0.002222437411546707
iteration 99, loss = 0.002096886048093438
iteration 100, loss = 0.002236455213278532
iteration 101, loss = 0.0025704428553581238
iteration 102, loss = 0.002259599044919014
iteration 103, loss = 0.0030447233002632856
iteration 104, loss = 0.002208622870966792
iteration 105, loss = 0.0017193220555782318
iteration 106, loss = 0.0020210561342537403
iteration 107, loss = 0.0017726084915921092
iteration 108, loss = 0.0019403455080464482
iteration 109, loss = 0.002349738497287035
iteration 110, loss = 0.00384615920484066
iteration 111, loss = 0.0022233629133552313
iteration 112, loss = 0.00401617307215929
iteration 113, loss = 0.002027037786319852
iteration 114, loss = 0.002194946398958564
iteration 115, loss = 0.0029682526364922523
iteration 116, loss = 0.0022784785833209753
iteration 117, loss = 0.0028064835350960493
iteration 118, loss = 0.0020722653716802597
iteration 119, loss = 0.002670235000550747
iteration 120, loss = 0.002500334521755576
iteration 121, loss = 0.0038332093972712755
iteration 122, loss = 0.0018686552066355944
iteration 123, loss = 0.0017619648715481162
iteration 124, loss = 0.0019921022467315197
iteration 125, loss = 0.002243604976683855
iteration 126, loss = 0.0019209843594580889
iteration 127, loss = 0.0016828752122819424
iteration 128, loss = 0.0032953883055597544
iteration 129, loss = 0.002914342563599348
iteration 130, loss = 0.002443847479298711
iteration 131, loss = 0.003687136108055711
iteration 132, loss = 0.003360312432050705
iteration 133, loss = 0.0022180734667927027
iteration 134, loss = 0.002635473618283868
iteration 135, loss = 0.0019367762142792344
iteration 136, loss = 0.0031712865456938744
iteration 137, loss = 0.002134254202246666
iteration 138, loss = 0.002224781783297658
iteration 139, loss = 0.0022782546002417803
iteration 140, loss = 0.0037783749867230654
iteration 141, loss = 0.0017537507228553295
iteration 142, loss = 0.0020278990268707275
iteration 143, loss = 0.0016673364443704486
iteration 144, loss = 0.0018093440448865294
iteration 145, loss = 0.0021734600886702538
iteration 146, loss = 0.002106248866766691
iteration 147, loss = 0.0031953405123203993
iteration 148, loss = 0.0019422550685703754
iteration 149, loss = 0.0018459685379639268
iteration 150, loss = 0.0018683508969843388
iteration 151, loss = 0.00427677296102047
iteration 152, loss = 0.002603171393275261
iteration 153, loss = 0.0019395702984184027
iteration 154, loss = 0.0018664314411580563
iteration 155, loss = 0.0020034415647387505
iteration 156, loss = 0.0017419556388631463
iteration 157, loss = 0.0017117370152845979
iteration 158, loss = 0.0021910422947257757
iteration 159, loss = 0.0017335285665467381
iteration 160, loss = 0.0019775601103901863
iteration 161, loss = 0.0016120483633130789
iteration 162, loss = 0.0021625948138535023
iteration 163, loss = 0.002597077051177621
iteration 164, loss = 0.0018429746851325035
iteration 165, loss = 0.00181527528911829
iteration 166, loss = 0.0024546461645513773
iteration 167, loss = 0.0018849868793040514
iteration 168, loss = 0.003257110947743058
iteration 169, loss = 0.001948782941326499
iteration 170, loss = 0.0036906898021698
iteration 171, loss = 0.0019711507484316826
iteration 172, loss = 0.002843345981091261
iteration 173, loss = 0.002009105868637562
iteration 174, loss = 0.001992490841075778
iteration 175, loss = 0.0018205151427537203
iteration 176, loss = 0.0028896734584122896
iteration 177, loss = 0.002051077550277114
iteration 178, loss = 0.0022493391297757626
iteration 179, loss = 0.0028434325940907
iteration 180, loss = 0.0019178392831236124
iteration 181, loss = 0.0020767818205058575
iteration 182, loss = 0.002234238665550947
iteration 183, loss = 0.0016441287007182837
iteration 184, loss = 0.0030141922179609537
iteration 185, loss = 0.001757733291015029
iteration 186, loss = 0.001924210460856557
iteration 187, loss = 0.0020297146402299404
iteration 188, loss = 0.003487825160846114
iteration 189, loss = 0.0018835002556443214
iteration 190, loss = 0.0018684881506487727
iteration 191, loss = 0.0022225237917155027
iteration 192, loss = 0.0028737594839185476
iteration 193, loss = 0.0022447535302489996
iteration 194, loss = 0.001963695976883173
iteration 195, loss = 0.0018786686705425382
iteration 196, loss = 0.003801740240305662
iteration 197, loss = 0.002020058920606971
iteration 198, loss = 0.0027543066535145044
iteration 199, loss = 0.0017327675595879555
iteration 200, loss = 0.0017889321316033602
iteration 201, loss = 0.0025276055093854666
iteration 202, loss = 0.0020071675535291433
iteration 203, loss = 0.002648091409355402
iteration 204, loss = 0.0019837694708257914
iteration 205, loss = 0.001773236901499331
iteration 206, loss = 0.002136443741619587
iteration 207, loss = 0.0024905491154640913
iteration 208, loss = 0.00229731947183609
iteration 209, loss = 0.0022224239073693752
iteration 210, loss = 0.0020416313782334328
iteration 211, loss = 0.002048210706561804
iteration 212, loss = 0.0021273682359606028
iteration 213, loss = 0.0020045542623847723
iteration 214, loss = 0.002526255324482918
iteration 215, loss = 0.001994549063965678
iteration 216, loss = 0.002541043097153306
iteration 217, loss = 0.004040718078613281
iteration 218, loss = 0.0022199596278369427
iteration 219, loss = 0.0017926781438291073
iteration 220, loss = 0.0016900117043405771
iteration 221, loss = 0.002237562322989106
iteration 222, loss = 0.002715275390073657
iteration 223, loss = 0.001894522923976183
iteration 224, loss = 0.001979640917852521
iteration 225, loss = 0.0030732322484254837
iteration 226, loss = 0.0038902501109987497
iteration 227, loss = 0.0017039758386090398
iteration 228, loss = 0.001588375074788928
iteration 229, loss = 0.0018970288801938295
iteration 230, loss = 0.002610589377582073
iteration 231, loss = 0.0021090502850711346
iteration 232, loss = 0.0018403318244963884
iteration 233, loss = 0.001901646377518773
iteration 234, loss = 0.0021030085626989603
iteration 235, loss = 0.0026174024678766727
iteration 236, loss = 0.0026044538244605064
iteration 237, loss = 0.0026649734936654568
iteration 238, loss = 0.0017796906176954508
iteration 239, loss = 0.0018881021533161402
iteration 240, loss = 0.0023401423823088408
iteration 241, loss = 0.0019007751252502203
iteration 242, loss = 0.0021893950179219246
iteration 243, loss = 0.0030757442582398653
iteration 244, loss = 0.0016411555698141456
iteration 245, loss = 0.0025240425020456314
iteration 246, loss = 0.001964356517419219
iteration 247, loss = 0.002165482845157385
iteration 248, loss = 0.0028410768136382103
iteration 249, loss = 0.001886223442852497
iteration 250, loss = 0.0018145120702683926
iteration 251, loss = 0.005172274075448513
iteration 252, loss = 0.002133880276232958
iteration 253, loss = 0.0018985784845426679
iteration 254, loss = 0.0014350240817293525
iteration 255, loss = 0.0026734978891909122
iteration 256, loss = 0.0018845864105969667
iteration 257, loss = 0.0018828081665560603
iteration 258, loss = 0.0019115989562124014
iteration 259, loss = 0.0021395867224782705
iteration 260, loss = 0.0022072908468544483
iteration 261, loss = 0.00183858722448349
iteration 262, loss = 0.002022053813561797
iteration 263, loss = 0.0018965647323057055
iteration 264, loss = 0.00208843476139009
iteration 265, loss = 0.0019776576664298773
iteration 266, loss = 0.0019463622011244297
iteration 267, loss = 0.0034766830503940582
iteration 268, loss = 0.0021374328061938286
iteration 269, loss = 0.001892310450784862
iteration 270, loss = 0.0019269653130322695
iteration 271, loss = 0.0019819228909909725
iteration 272, loss = 0.0018333981279283762
iteration 273, loss = 0.0020501425024122
iteration 274, loss = 0.003564442042261362
iteration 275, loss = 0.0020724013447761536
iteration 276, loss = 0.0026456275954842567
iteration 277, loss = 0.0020919162780046463
iteration 278, loss = 0.002206159522756934
iteration 279, loss = 0.002659659134224057
iteration 280, loss = 0.002126494888216257
iteration 281, loss = 0.002046293579041958
iteration 282, loss = 0.0018098655855283141
iteration 283, loss = 0.0021471944637596607
iteration 284, loss = 0.0022988365963101387
iteration 285, loss = 0.0019106052350252867
iteration 286, loss = 0.0022119046188890934
iteration 287, loss = 0.0017522447742521763
iteration 288, loss = 0.001842478639446199
iteration 289, loss = 0.002100207144394517
iteration 290, loss = 0.0023550994228571653
iteration 291, loss = 0.001949238358065486
iteration 292, loss = 0.0021552585531026125
iteration 293, loss = 0.0018386198207736015
iteration 294, loss = 0.0019596414640545845
iteration 295, loss = 0.0037756424862891436
iteration 296, loss = 0.002042594365775585
iteration 297, loss = 0.002728818915784359
iteration 298, loss = 0.0024790966417640448
iteration 299, loss = 0.0015746077988296747
iteration 300, loss = 0.004048620350658894
iteration 1, loss = 0.002125277416780591
iteration 2, loss = 0.0016061085043475032
iteration 3, loss = 0.002978626172989607
iteration 4, loss = 0.002043242799118161
iteration 5, loss = 0.002621603664010763
iteration 6, loss = 0.003576083341613412
iteration 7, loss = 0.001918856636621058
iteration 8, loss = 0.0018875012174248695
iteration 9, loss = 0.0032195390667766333
iteration 10, loss = 0.00382310152053833
iteration 11, loss = 0.001806417596526444
iteration 12, loss = 0.002855885773897171
iteration 13, loss = 0.0015488974750041962
iteration 14, loss = 0.0018371595069766045
iteration 15, loss = 0.0023069321177899837
iteration 16, loss = 0.001611747546121478
iteration 17, loss = 0.0020882831886410713
iteration 18, loss = 0.001980592729523778
iteration 19, loss = 0.0021668877452611923
iteration 20, loss = 0.0017189792124554515
iteration 21, loss = 0.0018050700891762972
iteration 22, loss = 0.0018138341838493943
iteration 23, loss = 0.0021055813413113356
iteration 24, loss = 0.001894267275929451
iteration 25, loss = 0.0036270369309931993
iteration 26, loss = 0.001853489433415234
iteration 27, loss = 0.0022562737576663494
iteration 28, loss = 0.003753232304006815
iteration 29, loss = 0.0018980250461027026
iteration 30, loss = 0.0020545958541333675
iteration 31, loss = 0.002400806872174144
iteration 32, loss = 0.0018682301742956042
iteration 33, loss = 0.002227646764367819
iteration 34, loss = 0.00411984184756875
iteration 35, loss = 0.001982039073482156
iteration 36, loss = 0.00199360609985888
iteration 37, loss = 0.003360865404829383
iteration 38, loss = 0.00199512648396194
iteration 39, loss = 0.0026163982693105936
iteration 40, loss = 0.0019132716115564108
iteration 41, loss = 0.003183899447321892
iteration 42, loss = 0.002071363851428032
iteration 43, loss = 0.004281911998987198
iteration 44, loss = 0.001877299277111888
iteration 45, loss = 0.0019072124268859625
iteration 46, loss = 0.0020592412911355495
iteration 47, loss = 0.0017637429991737008
iteration 48, loss = 0.0019832653924822807
iteration 49, loss = 0.0020957589149475098
iteration 50, loss = 0.002800317946821451
iteration 51, loss = 0.0018042922019958496
iteration 52, loss = 0.0017031823517754674
iteration 53, loss = 0.002122332574799657
iteration 54, loss = 0.00235298415645957
iteration 55, loss = 0.0016114673344418406
iteration 56, loss = 0.0022104945965111256
iteration 57, loss = 0.0022311985958367586
iteration 58, loss = 0.0025297754909843206
iteration 59, loss = 0.0024458691477775574
iteration 60, loss = 0.0027015763334929943
iteration 61, loss = 0.0018841096898540854
iteration 62, loss = 0.0020720125176012516
iteration 63, loss = 0.0023185955360531807
iteration 64, loss = 0.0020781897474080324
iteration 65, loss = 0.0018362582195550203
iteration 66, loss = 0.0022056601010262966
iteration 67, loss = 0.0023269313387572765
iteration 68, loss = 0.0023585513699799776
iteration 69, loss = 0.0022893918212503195
iteration 70, loss = 0.0020186263136565685
iteration 71, loss = 0.0037357034161686897
iteration 72, loss = 0.0019491398707032204
iteration 73, loss = 0.0019797179847955704
iteration 74, loss = 0.0020852412562817335
iteration 75, loss = 0.0019284356385469437
iteration 76, loss = 0.002259215572848916
iteration 77, loss = 0.002358070109039545
iteration 78, loss = 0.0023649316281080246
iteration 79, loss = 0.0024301656521856785
iteration 80, loss = 0.0019294529920443892
iteration 81, loss = 0.0037665104027837515
iteration 82, loss = 0.0020676208660006523
iteration 83, loss = 0.004625836852937937
iteration 84, loss = 0.0019359233556315303
iteration 85, loss = 0.0020992064382880926
iteration 86, loss = 0.0020782328210771084
iteration 87, loss = 0.0022012139670550823
iteration 88, loss = 0.0025728242471814156
iteration 89, loss = 0.0015540552558377385
iteration 90, loss = 0.0026029045693576336
iteration 91, loss = 0.002071201568469405
iteration 92, loss = 0.0019019468454644084
iteration 93, loss = 0.0021313666366040707
iteration 94, loss = 0.00401049992069602
iteration 95, loss = 0.0021063885651528835
iteration 96, loss = 0.0023043048568069935
iteration 97, loss = 0.002764539560303092
iteration 98, loss = 0.0036326732952147722
iteration 99, loss = 0.0020293844863772392
iteration 100, loss = 0.002737916773185134
iteration 101, loss = 0.0030466713942587376
iteration 102, loss = 0.0017140292329713702
iteration 103, loss = 0.0017331988783553243
iteration 104, loss = 0.002868167357519269
iteration 105, loss = 0.003690483281388879
iteration 106, loss = 0.002300600754097104
iteration 107, loss = 0.001731624361127615
iteration 108, loss = 0.003428851719945669
iteration 109, loss = 0.0020530291367322206
iteration 110, loss = 0.002294099424034357
iteration 111, loss = 0.0017732729902490973
iteration 112, loss = 0.0019832169637084007
iteration 113, loss = 0.001625582342967391
iteration 114, loss = 0.003861335339024663
iteration 115, loss = 0.002626835834234953
iteration 116, loss = 0.0016938479384407401
iteration 117, loss = 0.0021907531190663576
iteration 118, loss = 0.003636626061052084
iteration 119, loss = 0.0029941764660179615
iteration 120, loss = 0.002844281028956175
iteration 121, loss = 0.001969872508198023
iteration 122, loss = 0.0019116358598694205
iteration 123, loss = 0.0028665773570537567
iteration 124, loss = 0.0020526894368231297
iteration 125, loss = 0.0017996466485783458
iteration 126, loss = 0.0026589154731482267
iteration 127, loss = 0.0033069862984120846
iteration 128, loss = 0.002421088982373476
iteration 129, loss = 0.0033223924692720175
iteration 130, loss = 0.0026706319767981768
iteration 131, loss = 0.002429506042972207
iteration 132, loss = 0.0033095127437263727
iteration 133, loss = 0.0017525164876133204
iteration 134, loss = 0.0026883697137236595
iteration 135, loss = 0.0018730469746515155
iteration 136, loss = 0.0018562295008450747
iteration 137, loss = 0.0020119338296353817
iteration 138, loss = 0.0028273356147110462
iteration 139, loss = 0.003445132402703166
iteration 140, loss = 0.0021364169660955667
iteration 141, loss = 0.002211205894127488
iteration 142, loss = 0.0020222302991896868
iteration 143, loss = 0.0018587844679132104
iteration 144, loss = 0.0017367394175380468
iteration 145, loss = 0.0019394289702177048
iteration 146, loss = 0.0016980856889858842
iteration 147, loss = 0.0017828941345214844
iteration 148, loss = 0.0016531010624021292
iteration 149, loss = 0.002078443067148328
iteration 150, loss = 0.0022164504043757915
iteration 151, loss = 0.00245477631688118
iteration 152, loss = 0.0021128864027559757
iteration 153, loss = 0.0019440732430666685
iteration 154, loss = 0.002855384722352028
iteration 155, loss = 0.0016220498364418745
iteration 156, loss = 0.0019505275413393974
iteration 157, loss = 0.0028599693905562162
iteration 158, loss = 0.0023979058023542166
iteration 159, loss = 0.002585252281278372
iteration 160, loss = 0.002809065394103527
iteration 161, loss = 0.0033224779181182384
iteration 162, loss = 0.002046821406111121
iteration 163, loss = 0.0018569710664451122
iteration 164, loss = 0.00160288589540869
iteration 165, loss = 0.002626105211675167
iteration 166, loss = 0.0022416491992771626
iteration 167, loss = 0.0017945303115993738
iteration 168, loss = 0.002382251899689436
iteration 169, loss = 0.0018357745138928294
iteration 170, loss = 0.0023352871648967266
iteration 171, loss = 0.0028982223011553288
iteration 172, loss = 0.0036569437943398952
iteration 173, loss = 0.0018320695962756872
iteration 174, loss = 0.0021234729792922735
iteration 175, loss = 0.001953468192368746
iteration 176, loss = 0.0022450126707553864
iteration 177, loss = 0.0018810101319104433
iteration 178, loss = 0.0018171851988881826
iteration 179, loss = 0.001747170346789062
iteration 180, loss = 0.0020732099656015635
iteration 181, loss = 0.001755044679157436
iteration 182, loss = 0.0024402597919106483
iteration 183, loss = 0.0017513977363705635
iteration 184, loss = 0.0037059204187244177
iteration 185, loss = 0.0026728517841547728
iteration 186, loss = 0.0025970262940973043
iteration 187, loss = 0.0022222050465643406
iteration 188, loss = 0.0019921548664569855
iteration 189, loss = 0.0023164560552686453
iteration 190, loss = 0.001602926873601973
iteration 191, loss = 0.0019026054069399834
iteration 192, loss = 0.0016654968494549394
iteration 193, loss = 0.0025572534650564194
iteration 194, loss = 0.002906038425862789
iteration 195, loss = 0.002066178247332573
iteration 196, loss = 0.002028993796557188
iteration 197, loss = 0.0027284175157546997
iteration 198, loss = 0.001990959048271179
iteration 199, loss = 0.0021477295085787773
iteration 200, loss = 0.002663911785930395
iteration 201, loss = 0.0016782002057880163
iteration 202, loss = 0.0023927942384034395
iteration 203, loss = 0.0026656242553144693
iteration 204, loss = 0.001865279278717935
iteration 205, loss = 0.001705179107375443
iteration 206, loss = 0.0020113529171794653
iteration 207, loss = 0.0017111556371673942
iteration 208, loss = 0.0022738752886652946
iteration 209, loss = 0.003958382178097963
iteration 210, loss = 0.002320563420653343
iteration 211, loss = 0.0015312241157516837
iteration 212, loss = 0.0019530483987182379
iteration 213, loss = 0.0022952156141400337
iteration 214, loss = 0.0017784377560019493
iteration 215, loss = 0.0023983879946172237
iteration 216, loss = 0.0022734275553375483
iteration 217, loss = 0.0020628105849027634
iteration 218, loss = 0.0023589807096868753
iteration 219, loss = 0.002210837323218584
iteration 220, loss = 0.0019209744641557336
iteration 221, loss = 0.002247738419100642
iteration 222, loss = 0.002640737220644951
iteration 223, loss = 0.004078469704836607
iteration 224, loss = 0.0017821697983890772
iteration 225, loss = 0.0018264169339090586
iteration 226, loss = 0.001716144965030253
iteration 227, loss = 0.0029487174469977617
iteration 228, loss = 0.0019179338123649359
iteration 229, loss = 0.0017939293757081032
iteration 230, loss = 0.002004454145208001
iteration 231, loss = 0.0021681415382772684
iteration 232, loss = 0.0016374657861888409
iteration 233, loss = 0.003727216273546219
iteration 234, loss = 0.002027428476139903
iteration 235, loss = 0.0017874157056212425
iteration 236, loss = 0.003809149842709303
iteration 237, loss = 0.0015493348473683
iteration 238, loss = 0.0018387361196801066
iteration 239, loss = 0.001808646833524108
iteration 240, loss = 0.002040959196165204
iteration 241, loss = 0.004038346000015736
iteration 242, loss = 0.0038504970725625753
iteration 243, loss = 0.0022979711648076773
iteration 244, loss = 0.0022241389378905296
iteration 245, loss = 0.0029921105597168207
iteration 246, loss = 0.002062613843008876
iteration 247, loss = 0.0025850108359009027
iteration 248, loss = 0.002067198511213064
iteration 249, loss = 0.0016754177631810308
iteration 250, loss = 0.0021873856894671917
iteration 251, loss = 0.0018372780177742243
iteration 252, loss = 0.003426972543820739
iteration 253, loss = 0.0025938176549971104
iteration 254, loss = 0.0019606638234108686
iteration 255, loss = 0.0018028554040938616
iteration 256, loss = 0.001969168661162257
iteration 257, loss = 0.0020684190094470978
iteration 258, loss = 0.0022087316028773785
iteration 259, loss = 0.0019551862496882677
iteration 260, loss = 0.0018445659661665559
iteration 261, loss = 0.002099693287163973
iteration 262, loss = 0.0024803902488201857
iteration 263, loss = 0.0020488628651946783
iteration 264, loss = 0.001963506918400526
iteration 265, loss = 0.0017571788048371673
iteration 266, loss = 0.002361989114433527
iteration 267, loss = 0.003241209778934717
iteration 268, loss = 0.0018755370983853936
iteration 269, loss = 0.0018165954388678074
iteration 270, loss = 0.0023046003188937902
iteration 271, loss = 0.0024194587022066116
iteration 272, loss = 0.0022074924781918526
iteration 273, loss = 0.0022761980071663857
iteration 274, loss = 0.0030854777432978153
iteration 275, loss = 0.001619524322450161
iteration 276, loss = 0.0034388061612844467
iteration 277, loss = 0.0018698042258620262
iteration 278, loss = 0.002690398134291172
iteration 279, loss = 0.003309267107397318
iteration 280, loss = 0.002150945132598281
iteration 281, loss = 0.002337696962058544
iteration 282, loss = 0.002033861353993416
iteration 283, loss = 0.001851350418291986
iteration 284, loss = 0.002180370967835188
iteration 285, loss = 0.0035603863652795553
iteration 286, loss = 0.0021071885712444782
iteration 287, loss = 0.0016994335455819964
iteration 288, loss = 0.0022894348949193954
iteration 289, loss = 0.0015133428387343884
iteration 290, loss = 0.00200511422008276
iteration 291, loss = 0.003647041041404009
iteration 292, loss = 0.0017036384670063853
iteration 293, loss = 0.0022015541326254606
iteration 294, loss = 0.002306971000507474
iteration 295, loss = 0.002665839623659849
iteration 296, loss = 0.0016974409809336066
iteration 297, loss = 0.0028023230843245983
iteration 298, loss = 0.002126371953636408
iteration 299, loss = 0.002331758150830865
iteration 300, loss = 0.0014671491226181388
iteration 1, loss = 0.0031843814067542553
iteration 2, loss = 0.0019405491184443235
iteration 3, loss = 0.0017528962343931198
iteration 4, loss = 0.002123688580468297
iteration 5, loss = 0.0039060881827026606
iteration 6, loss = 0.004873153753578663
iteration 7, loss = 0.0028765429742634296
iteration 8, loss = 0.003075030166655779
iteration 9, loss = 0.0020786442328244448
iteration 10, loss = 0.003773486241698265
iteration 11, loss = 0.0021957261487841606
iteration 12, loss = 0.0022979076020419598
iteration 13, loss = 0.0019861659966409206
iteration 14, loss = 0.0030279732309281826
iteration 15, loss = 0.0031091044656932354
iteration 16, loss = 0.0020592836663126945
iteration 17, loss = 0.00273580732755363
iteration 18, loss = 0.002022793749347329
iteration 19, loss = 0.0030893331859260798
iteration 20, loss = 0.002671201713383198
iteration 21, loss = 0.002734243171289563
iteration 22, loss = 0.003983123227953911
iteration 23, loss = 0.002095787785947323
iteration 24, loss = 0.0016036045271903276
iteration 25, loss = 0.002297926926985383
iteration 26, loss = 0.002119051292538643
iteration 27, loss = 0.002716925460845232
iteration 28, loss = 0.0019953413866460323
iteration 29, loss = 0.001969745848327875
iteration 30, loss = 0.0018249964341521263
iteration 31, loss = 0.002069556387141347
iteration 32, loss = 0.0017579901032149792
iteration 33, loss = 0.0017970658373087645
iteration 34, loss = 0.0025185663253068924
iteration 35, loss = 0.0018702076049521565
iteration 36, loss = 0.0018192005809396505
iteration 37, loss = 0.0016524525126442313
iteration 38, loss = 0.0018105200724676251
iteration 39, loss = 0.0028739762492477894
iteration 40, loss = 0.002427711384370923
iteration 41, loss = 0.0021555176936089993
iteration 42, loss = 0.0021075024269521236
iteration 43, loss = 0.0029099625535309315
iteration 44, loss = 0.0022919990587979555
iteration 45, loss = 0.002289187628775835
iteration 46, loss = 0.0027903474401682615
iteration 47, loss = 0.0022714128717780113
iteration 48, loss = 0.002003008732572198
iteration 49, loss = 0.0022479703184217215
iteration 50, loss = 0.00391869992017746
iteration 51, loss = 0.001777266850695014
iteration 52, loss = 0.0021061343140900135
iteration 53, loss = 0.0018963264301419258
iteration 54, loss = 0.002042085863649845
iteration 55, loss = 0.0028627091087400913
iteration 56, loss = 0.0019054734148085117
iteration 57, loss = 0.0019057905301451683
iteration 58, loss = 0.0021797018125653267
iteration 59, loss = 0.0023596675600856543
iteration 60, loss = 0.002316997619345784
iteration 61, loss = 0.001884222379885614
iteration 62, loss = 0.0020995899103581905
iteration 63, loss = 0.0019455845467746258
iteration 64, loss = 0.00255669467151165
iteration 65, loss = 0.0019660168327391148
iteration 66, loss = 0.0020327670499682426
iteration 67, loss = 0.0018446955364197493
iteration 68, loss = 0.002612386830151081
iteration 69, loss = 0.0019190091406926513
iteration 70, loss = 0.0027958073187619448
iteration 71, loss = 0.0020554272923618555
iteration 72, loss = 0.00194767233915627
iteration 73, loss = 0.0019221327966079116
iteration 74, loss = 0.0018362930277362466
iteration 75, loss = 0.001958351582288742
iteration 76, loss = 0.00217736279591918
iteration 77, loss = 0.002080306177958846
iteration 78, loss = 0.0029512043111026287
iteration 79, loss = 0.0019314959645271301
iteration 80, loss = 0.0018748575821518898
iteration 81, loss = 0.00235144910402596
iteration 82, loss = 0.0018485412001609802
iteration 83, loss = 0.0024356862995773554
iteration 84, loss = 0.002183343982324004
iteration 85, loss = 0.003984787967056036
iteration 86, loss = 0.0026447875425219536
iteration 87, loss = 0.002062614541500807
iteration 88, loss = 0.002011692151427269
iteration 89, loss = 0.0020059782546013594
iteration 90, loss = 0.0019560102373361588
iteration 91, loss = 0.0023845238611102104
iteration 92, loss = 0.0025543137453496456
iteration 93, loss = 0.0021017647814005613
iteration 94, loss = 0.003912797663360834
iteration 95, loss = 0.003020436968654394
iteration 96, loss = 0.0024926827754825354
iteration 97, loss = 0.0021179080940783024
iteration 98, loss = 0.0019837345462292433
iteration 99, loss = 0.0020701840985566378
iteration 100, loss = 0.004053502343595028
iteration 101, loss = 0.0020519865211099386
iteration 102, loss = 0.004090471658855677
iteration 103, loss = 0.002108929678797722
iteration 104, loss = 0.0020520370453596115
iteration 105, loss = 0.0020070199389010668
iteration 106, loss = 0.0019228513119742274
iteration 107, loss = 0.0021458060946315527
iteration 108, loss = 0.0020523241255432367
iteration 109, loss = 0.0019551452714949846
iteration 110, loss = 0.002092698821797967
iteration 111, loss = 0.0016909100813791156
iteration 112, loss = 0.0021568969823420048
iteration 113, loss = 0.002035825978964567
iteration 114, loss = 0.0019797959830611944
iteration 115, loss = 0.0019212150946259499
iteration 116, loss = 0.0017875641351565719
iteration 117, loss = 0.00273117795586586
iteration 118, loss = 0.0021152766421437263
iteration 119, loss = 0.0034402094315737486
iteration 120, loss = 0.0016867326339706779
iteration 121, loss = 0.002184552140533924
iteration 122, loss = 0.0018108926014974713
iteration 123, loss = 0.0017282806802541018
iteration 124, loss = 0.0019407932413741946
iteration 125, loss = 0.003099308582022786
iteration 126, loss = 0.0023633730597794056
iteration 127, loss = 0.0017993762157857418
iteration 128, loss = 0.0018418916733935475
iteration 129, loss = 0.0035825029481202364
iteration 130, loss = 0.002070122631266713
iteration 131, loss = 0.0020797818433493376
iteration 132, loss = 0.002350485185161233
iteration 133, loss = 0.0019054376753047109
iteration 134, loss = 0.002014375990256667
iteration 135, loss = 0.0028199446387588978
iteration 136, loss = 0.0026897238567471504
iteration 137, loss = 0.002312838565558195
iteration 138, loss = 0.0019635739736258984
iteration 139, loss = 0.0018291076412424445
iteration 140, loss = 0.002818949753418565
iteration 141, loss = 0.002094235271215439
iteration 142, loss = 0.00244678882881999
iteration 143, loss = 0.0027088341303169727
iteration 144, loss = 0.0023214465472847223
iteration 145, loss = 0.0019762194715440273
iteration 146, loss = 0.0021793856285512447
iteration 147, loss = 0.0020070152822881937
iteration 148, loss = 0.0018308196449652314
iteration 149, loss = 0.0020936348009854555
iteration 150, loss = 0.002211820799857378
iteration 151, loss = 0.001768206711858511
iteration 152, loss = 0.0019424476195126772
iteration 153, loss = 0.002301473170518875
iteration 154, loss = 0.0031076031737029552
iteration 155, loss = 0.0031265232246369123
iteration 156, loss = 0.0028506603557616472
iteration 157, loss = 0.0017662866739556193
iteration 158, loss = 0.0019491505809128284
iteration 159, loss = 0.0019766981713473797
iteration 160, loss = 0.0020452679600566626
iteration 161, loss = 0.0017015362391248345
iteration 162, loss = 0.001969673437997699
iteration 163, loss = 0.003927696030586958
iteration 164, loss = 0.001794465002603829
iteration 165, loss = 0.002456510905176401
iteration 166, loss = 0.0021253067534416914
iteration 167, loss = 0.0018975904677063227
iteration 168, loss = 0.003226606175303459
iteration 169, loss = 0.0018544020131230354
iteration 170, loss = 0.0021755800116807222
iteration 171, loss = 0.0025652069598436356
iteration 172, loss = 0.0021123657934367657
iteration 173, loss = 0.0018191433046013117
iteration 174, loss = 0.0018999477615579963
iteration 175, loss = 0.0023481526877731085
iteration 176, loss = 0.002676315139979124
iteration 177, loss = 0.0022242183331400156
iteration 178, loss = 0.0023855254985392094
iteration 179, loss = 0.002558885607868433
iteration 180, loss = 0.004100417718291283
iteration 181, loss = 0.0024972730316221714
iteration 182, loss = 0.0028197579085826874
iteration 183, loss = 0.0032584434375166893
iteration 184, loss = 0.0019196715438738465
iteration 185, loss = 0.0027883020229637623
iteration 186, loss = 0.0017814775928854942
iteration 187, loss = 0.0018078020075336099
iteration 188, loss = 0.0024784021079540253
iteration 189, loss = 0.0017577999969944358
iteration 190, loss = 0.0020485075656324625
iteration 191, loss = 0.001963148359209299
iteration 192, loss = 0.002695786766707897
iteration 193, loss = 0.0021757662761956453
iteration 194, loss = 0.0022959932684898376
iteration 195, loss = 0.0017959545366466045
iteration 196, loss = 0.0017205452313646674
iteration 197, loss = 0.0018051686929538846
iteration 198, loss = 0.004170612897723913
iteration 199, loss = 0.002137450035661459
iteration 200, loss = 0.002522196853533387
iteration 201, loss = 0.004235514439642429
iteration 202, loss = 0.0031701091211289167
iteration 203, loss = 0.0018747997237369418
iteration 204, loss = 0.0019173508044332266
iteration 205, loss = 0.002635136479511857
iteration 206, loss = 0.0039791446179151535
iteration 207, loss = 0.0018157060258090496
iteration 208, loss = 0.0023046990390866995
iteration 209, loss = 0.0015930235385894775
iteration 210, loss = 0.0031455797143280506
iteration 211, loss = 0.00194271351210773
iteration 212, loss = 0.0019354044925421476
iteration 213, loss = 0.0019333797972649336
iteration 214, loss = 0.002659878693521023
iteration 215, loss = 0.0015781780239194632
iteration 216, loss = 0.0016452120617032051
iteration 217, loss = 0.0018737473292276263
iteration 218, loss = 0.0021410933695733547
iteration 219, loss = 0.003743619192391634
iteration 220, loss = 0.0032936385832726955
iteration 221, loss = 0.002105609979480505
iteration 222, loss = 0.002065809676423669
iteration 223, loss = 0.0017514909850433469
iteration 224, loss = 0.0017834050813689828
iteration 225, loss = 0.001634416519664228
iteration 226, loss = 0.0021555162966251373
iteration 227, loss = 0.0014899153029546142
iteration 228, loss = 0.0015622912906110287
iteration 229, loss = 0.0018692954909056425
iteration 230, loss = 0.002078801626339555
iteration 231, loss = 0.0021621347405016422
iteration 232, loss = 0.0023802900686860085
iteration 233, loss = 0.002099819015711546
iteration 234, loss = 0.002462563803419471
iteration 235, loss = 0.00342742120847106
iteration 236, loss = 0.0022297054529190063
iteration 237, loss = 0.0016619324451312423
iteration 238, loss = 0.002003381960093975
iteration 239, loss = 0.003722849767655134
iteration 240, loss = 0.003119691042229533
iteration 241, loss = 0.0018451055511832237
iteration 242, loss = 0.00205361214466393
iteration 243, loss = 0.001944140181876719
iteration 244, loss = 0.0020385540556162596
iteration 245, loss = 0.0019313190132379532
iteration 246, loss = 0.0020521758124232292
iteration 247, loss = 0.0017641657032072544
iteration 248, loss = 0.0017520826077088714
iteration 249, loss = 0.0020737149752676487
iteration 250, loss = 0.0025307887699455023
iteration 251, loss = 0.0028918664902448654
iteration 252, loss = 0.0021902057342231274
iteration 253, loss = 0.0019088091794401407
iteration 254, loss = 0.0036536839324980974
iteration 255, loss = 0.0027919230051338673
iteration 256, loss = 0.002041921718046069
iteration 257, loss = 0.0017895150231197476
iteration 258, loss = 0.002725078258663416
iteration 259, loss = 0.002497927751392126
iteration 260, loss = 0.0024714721366763115
iteration 261, loss = 0.001984986010938883
iteration 262, loss = 0.0017787680262699723
iteration 263, loss = 0.0017082208069041371
iteration 264, loss = 0.0018567454535514116
iteration 265, loss = 0.0021527064964175224
iteration 266, loss = 0.0017853995086625218
iteration 267, loss = 0.003332586493343115
iteration 268, loss = 0.002076853299513459
iteration 269, loss = 0.0021260783541947603
iteration 270, loss = 0.0019569681026041508
iteration 271, loss = 0.0019483594223856926
iteration 272, loss = 0.002376389456912875
iteration 273, loss = 0.0019234828650951385
iteration 274, loss = 0.0019068135879933834
iteration 275, loss = 0.002117202850058675
iteration 276, loss = 0.0015687826089560986
iteration 277, loss = 0.002168759237974882
iteration 278, loss = 0.0033640521578490734
iteration 279, loss = 0.004395243711769581
iteration 280, loss = 0.00320773315615952
iteration 281, loss = 0.00178825156763196
iteration 282, loss = 0.0017124024452641606
iteration 283, loss = 0.002715472597628832
iteration 284, loss = 0.0021526471246033907
iteration 285, loss = 0.0022409348748624325
iteration 286, loss = 0.002013993449509144
iteration 287, loss = 0.0017210603691637516
iteration 288, loss = 0.0016005797078832984
iteration 289, loss = 0.0024802458938211203
iteration 290, loss = 0.002099153585731983
iteration 291, loss = 0.0020870298612862825
iteration 292, loss = 0.001885623554699123
iteration 293, loss = 0.002221483737230301
iteration 294, loss = 0.002009371295571327
iteration 295, loss = 0.0018002187134698033
iteration 296, loss = 0.0036574595142155886
iteration 297, loss = 0.0030256188474595547
iteration 298, loss = 0.0018551037646830082
iteration 299, loss = 0.0016131685115396976
iteration 300, loss = 0.0036147567443549633
iteration 1, loss = 0.0019014603458344936
iteration 2, loss = 0.002085982821881771
iteration 3, loss = 0.0020553190261125565
iteration 4, loss = 0.001766202156431973
iteration 5, loss = 0.0023310978431254625
iteration 6, loss = 0.0019402590114623308
iteration 7, loss = 0.0022479798644781113
iteration 8, loss = 0.0027023020666092634
iteration 9, loss = 0.003068402409553528
iteration 10, loss = 0.002539970213547349
iteration 11, loss = 0.002085038460791111
iteration 12, loss = 0.002166649093851447
iteration 13, loss = 0.001960806781426072
iteration 14, loss = 0.0021050916984677315
iteration 15, loss = 0.0047709159553050995
iteration 16, loss = 0.0024051652289927006
iteration 17, loss = 0.0017862000968307257
iteration 18, loss = 0.0037536376621574163
iteration 19, loss = 0.0018023638986051083
iteration 20, loss = 0.001956724328920245
iteration 21, loss = 0.002053730422630906
iteration 22, loss = 0.001787993824109435
iteration 23, loss = 0.002088978188112378
iteration 24, loss = 0.003958594519644976
iteration 25, loss = 0.0020526794251054525
iteration 26, loss = 0.0025673750787973404
iteration 27, loss = 0.002888584043830633
iteration 28, loss = 0.002092324895784259
iteration 29, loss = 0.0022355488035827875
iteration 30, loss = 0.001703360234387219
iteration 31, loss = 0.0027950573712587357
iteration 32, loss = 0.002808421617373824
iteration 33, loss = 0.001993879908695817
iteration 34, loss = 0.002155669266358018
iteration 35, loss = 0.0023295083083212376
iteration 36, loss = 0.0016341768205165863
iteration 37, loss = 0.001987963216379285
iteration 38, loss = 0.0018160927575081587
iteration 39, loss = 0.0018428940093144774
iteration 40, loss = 0.0022800234146416187
iteration 41, loss = 0.002416752278804779
iteration 42, loss = 0.0015924416948109865
iteration 43, loss = 0.003269235137850046
iteration 44, loss = 0.001859718351624906
iteration 45, loss = 0.0038048240821808577
iteration 46, loss = 0.0029935326892882586
iteration 47, loss = 0.00215002428740263
iteration 48, loss = 0.0023651716765016317
iteration 49, loss = 0.002543580951169133
iteration 50, loss = 0.002041694475337863
iteration 51, loss = 0.0017910111928358674
iteration 52, loss = 0.0021157951559871435
iteration 53, loss = 0.0022890232503414154
iteration 54, loss = 0.002119437325745821
iteration 55, loss = 0.0027957193087786436
iteration 56, loss = 0.0019995125476270914
iteration 57, loss = 0.0016426006332039833
iteration 58, loss = 0.0019204267300665379
iteration 59, loss = 0.002234793035313487
iteration 60, loss = 0.002145244274288416
iteration 61, loss = 0.001987483585253358
iteration 62, loss = 0.001789995701983571
iteration 63, loss = 0.0017755849985405803
iteration 64, loss = 0.002205607946962118
iteration 65, loss = 0.0032929980661720037
iteration 66, loss = 0.0017717508599162102
iteration 67, loss = 0.001779115991666913
iteration 68, loss = 0.0022485859226435423
iteration 69, loss = 0.0020027589052915573
iteration 70, loss = 0.0021986125502735376
iteration 71, loss = 0.0023683737963438034
iteration 72, loss = 0.0019515545573085546
iteration 73, loss = 0.0019748895429074764
iteration 74, loss = 0.002148144878447056
iteration 75, loss = 0.002895127981901169
iteration 76, loss = 0.001969503005966544
iteration 77, loss = 0.0019713020883500576
iteration 78, loss = 0.001962990267202258
iteration 79, loss = 0.001840153825469315
iteration 80, loss = 0.0020286443177610636
iteration 81, loss = 0.002515434520319104
iteration 82, loss = 0.002376658609136939
iteration 83, loss = 0.002066784305498004
iteration 84, loss = 0.002609741408377886
iteration 85, loss = 0.0016926779644563794
iteration 86, loss = 0.003303070552647114
iteration 87, loss = 0.001564924605190754
iteration 88, loss = 0.0021279032807797194
iteration 89, loss = 0.0036794974002987146
iteration 90, loss = 0.001935031614266336
iteration 91, loss = 0.002165165264159441
iteration 92, loss = 0.0018352397019043565
iteration 93, loss = 0.003875077236443758
iteration 94, loss = 0.002010803436860442
iteration 95, loss = 0.004002911504358053
iteration 96, loss = 0.0018891107756644487
iteration 97, loss = 0.0019851713441312313
iteration 98, loss = 0.0018498573917895555
iteration 99, loss = 0.0019135058391839266
iteration 100, loss = 0.0019408472580835223
iteration 101, loss = 0.002650884445756674
iteration 102, loss = 0.002065806882455945
iteration 103, loss = 0.002043049782514572
iteration 104, loss = 0.0018051957013085485
iteration 105, loss = 0.0017482687253504992
iteration 106, loss = 0.004158298019319773
iteration 107, loss = 0.003854320617392659
iteration 108, loss = 0.0018603738863021135
iteration 109, loss = 0.0019708273466676474
iteration 110, loss = 0.0018169258255511522
iteration 111, loss = 0.0018176488811150193
iteration 112, loss = 0.003738244529813528
iteration 113, loss = 0.0027397943194955587
iteration 114, loss = 0.002343561500310898
iteration 115, loss = 0.0026207698974758387
iteration 116, loss = 0.0026152306236326694
iteration 117, loss = 0.0023669409565627575
iteration 118, loss = 0.0020805159583687782
iteration 119, loss = 0.0022117658518254757
iteration 120, loss = 0.0025485747028142214
iteration 121, loss = 0.002385080559179187
iteration 122, loss = 0.0016853541601449251
iteration 123, loss = 0.0015972426626831293
iteration 124, loss = 0.0018255019094794989
iteration 125, loss = 0.002127622254192829
iteration 126, loss = 0.0018380910623818636
iteration 127, loss = 0.0033122024033218622
iteration 128, loss = 0.002185818739235401
iteration 129, loss = 0.0019799843430519104
iteration 130, loss = 0.0017872287426143885
iteration 131, loss = 0.0020638664718717337
iteration 132, loss = 0.001966797513887286
iteration 133, loss = 0.0020387114491313696
iteration 134, loss = 0.0019980538636446
iteration 135, loss = 0.001836477778851986
iteration 136, loss = 0.0027630075346678495
iteration 137, loss = 0.0023349972907453775
iteration 138, loss = 0.004603983834385872
iteration 139, loss = 0.0021075992844998837
iteration 140, loss = 0.0019716660026460886
iteration 141, loss = 0.0031706015579402447
iteration 142, loss = 0.0031299730762839317
iteration 143, loss = 0.0035148817114531994
iteration 144, loss = 0.002088272012770176
iteration 145, loss = 0.0020636366680264473
iteration 146, loss = 0.0029491600580513477
iteration 147, loss = 0.0026155454106628895
iteration 148, loss = 0.0016393488040193915
iteration 149, loss = 0.002476854482665658
iteration 150, loss = 0.0018572572153061628
iteration 151, loss = 0.0023231878876686096
iteration 152, loss = 0.0020110541954636574
iteration 153, loss = 0.001531126443296671
iteration 154, loss = 0.0023373356088995934
iteration 155, loss = 0.0017502738628536463
iteration 156, loss = 0.002467489568516612
iteration 157, loss = 0.0017887066351249814
iteration 158, loss = 0.004278726410120726
iteration 159, loss = 0.0030787154100835323
iteration 160, loss = 0.0019290561322122812
iteration 161, loss = 0.0025129287969321012
iteration 162, loss = 0.002582903252914548
iteration 163, loss = 0.0023608256597071886
iteration 164, loss = 0.0019879331812262535
iteration 165, loss = 0.00389445130713284
iteration 166, loss = 0.0023776483722031116
iteration 167, loss = 0.002466328674927354
iteration 168, loss = 0.0030770322773605585
iteration 169, loss = 0.0026524916756898165
iteration 170, loss = 0.0026797608006745577
iteration 171, loss = 0.002285528229549527
iteration 172, loss = 0.0018765705171972513
iteration 173, loss = 0.0021250969730317593
iteration 174, loss = 0.0016973435413092375
iteration 175, loss = 0.0016455004224553704
iteration 176, loss = 0.003697574371472001
iteration 177, loss = 0.002183915115892887
iteration 178, loss = 0.0027284082025289536
iteration 179, loss = 0.002041853964328766
iteration 180, loss = 0.0018369085155427456
iteration 181, loss = 0.0030391414184123278
iteration 182, loss = 0.0019844078924506903
iteration 183, loss = 0.0030837825033813715
iteration 184, loss = 0.0019039297476410866
iteration 185, loss = 0.0017062828410416842
iteration 186, loss = 0.0025431429967284203
iteration 187, loss = 0.0020101701375097036
iteration 188, loss = 0.0034099158365279436
iteration 189, loss = 0.0030141377355903387
iteration 190, loss = 0.001822750549763441
iteration 191, loss = 0.001946620992384851
iteration 192, loss = 0.002858355874195695
iteration 193, loss = 0.002318659331649542
iteration 194, loss = 0.0021073927637189627
iteration 195, loss = 0.0018593375571072102
iteration 196, loss = 0.0024858051910996437
iteration 197, loss = 0.0017660382436588407
iteration 198, loss = 0.0019560283981263638
iteration 199, loss = 0.0023553771898150444
iteration 200, loss = 0.0037312519270926714
iteration 201, loss = 0.0037051753606647253
iteration 202, loss = 0.001964476890861988
iteration 203, loss = 0.0019228169694542885
iteration 204, loss = 0.0021850676275789738
iteration 205, loss = 0.0018197089666500688
iteration 206, loss = 0.0020892817992717028
iteration 207, loss = 0.002114644041284919
iteration 208, loss = 0.0018725200789049268
iteration 209, loss = 0.0017637369455769658
iteration 210, loss = 0.004075460135936737
iteration 211, loss = 0.0020834142342209816
iteration 212, loss = 0.0020240533631294966
iteration 213, loss = 0.0019482792122289538
iteration 214, loss = 0.002477449830621481
iteration 215, loss = 0.002197080757468939
iteration 216, loss = 0.0024925395846366882
iteration 217, loss = 0.0038181052077561617
iteration 218, loss = 0.002104054670780897
iteration 219, loss = 0.0023140476550906897
iteration 220, loss = 0.002004096517339349
iteration 221, loss = 0.0023533832281827927
iteration 222, loss = 0.004140225239098072
iteration 223, loss = 0.001842221710830927
iteration 224, loss = 0.0022278984542936087
iteration 225, loss = 0.0017820514040067792
iteration 226, loss = 0.0018033626256510615
iteration 227, loss = 0.002213661791756749
iteration 228, loss = 0.00417347252368927
iteration 229, loss = 0.0022922507487237453
iteration 230, loss = 0.0019168404396623373
iteration 231, loss = 0.0018478400306776166
iteration 232, loss = 0.002382746897637844
iteration 233, loss = 0.0021049887873232365
iteration 234, loss = 0.0015291981399059296
iteration 235, loss = 0.0027209874242544174
iteration 236, loss = 0.002618416678160429
iteration 237, loss = 0.00210764748044312
iteration 238, loss = 0.0021352688781917095
iteration 239, loss = 0.0018131010001525283
iteration 240, loss = 0.0018907993799075484
iteration 241, loss = 0.002047077054157853
iteration 242, loss = 0.00195127516053617
iteration 243, loss = 0.002527217147871852
iteration 244, loss = 0.0019118590280413628
iteration 245, loss = 0.002045649802312255
iteration 246, loss = 0.0019503100775182247
iteration 247, loss = 0.0019876286387443542
iteration 248, loss = 0.0020070390310138464
iteration 249, loss = 0.0017771550919860601
iteration 250, loss = 0.0018970551900565624
iteration 251, loss = 0.0019681951962411404
iteration 252, loss = 0.002015176462009549
iteration 253, loss = 0.0024881926365196705
iteration 254, loss = 0.0019468208774924278
iteration 255, loss = 0.002481504576280713
iteration 256, loss = 0.001822211081162095
iteration 257, loss = 0.0018662653164938092
iteration 258, loss = 0.0026688165962696075
iteration 259, loss = 0.00216905539855361
iteration 260, loss = 0.0018561647739261389
iteration 261, loss = 0.0016236871015280485
iteration 262, loss = 0.002036383142694831
iteration 263, loss = 0.002655130811035633
iteration 264, loss = 0.002086150459945202
iteration 265, loss = 0.0017311063129454851
iteration 266, loss = 0.0024508077185600996
iteration 267, loss = 0.001975191989913583
iteration 268, loss = 0.0020824563689529896
iteration 269, loss = 0.0017196785192936659
iteration 270, loss = 0.0022895876318216324
iteration 271, loss = 0.0018884161254391074
iteration 272, loss = 0.0020977517124265432
iteration 273, loss = 0.0016501493519172072
iteration 274, loss = 0.001878385548479855
iteration 275, loss = 0.0026514464989304543
iteration 276, loss = 0.001818669494241476
iteration 277, loss = 0.002968474756926298
iteration 278, loss = 0.004000195302069187
iteration 279, loss = 0.002571672899648547
iteration 280, loss = 0.0020300180185586214
iteration 281, loss = 0.00237951404415071
iteration 282, loss = 0.0026291876565665007
iteration 283, loss = 0.0018236003816127777
iteration 284, loss = 0.0028378888964653015
iteration 285, loss = 0.0021228771656751633
iteration 286, loss = 0.0018215915188193321
iteration 287, loss = 0.0022357783745974302
iteration 288, loss = 0.0017653999384492636
iteration 289, loss = 0.003110163612291217
iteration 290, loss = 0.0019772022496908903
iteration 291, loss = 0.0026648342609405518
iteration 292, loss = 0.002178601222112775
iteration 293, loss = 0.002156426664441824
iteration 294, loss = 0.0019744224846363068
iteration 295, loss = 0.0021429939661175013
iteration 296, loss = 0.002364004496484995
iteration 297, loss = 0.002075123367831111
iteration 298, loss = 0.0021389308385550976
iteration 299, loss = 0.005185381043702364
iteration 300, loss = 0.0020524528808891773
iteration 1, loss = 0.00382516672834754
iteration 2, loss = 0.0029933033511042595
iteration 3, loss = 0.0015527618816122413
iteration 4, loss = 0.002539976267144084
iteration 5, loss = 0.001814108807593584
iteration 6, loss = 0.0016900337068364024
iteration 7, loss = 0.0018576282309368253
iteration 8, loss = 0.0018265595426782966
iteration 9, loss = 0.0018000181298702955
iteration 10, loss = 0.0018129507079720497
iteration 11, loss = 0.0027138330042362213
iteration 12, loss = 0.0019346113549545407
iteration 13, loss = 0.0018476815894246101
iteration 14, loss = 0.0026021720841526985
iteration 15, loss = 0.003578901756554842
iteration 16, loss = 0.0016307190526276827
iteration 17, loss = 0.002057867357507348
iteration 18, loss = 0.0018887121696025133
iteration 19, loss = 0.0018661792855709791
iteration 20, loss = 0.0027686862740665674
iteration 21, loss = 0.0027238752227276564
iteration 22, loss = 0.0018576151924207807
iteration 23, loss = 0.0022423688787966967
iteration 24, loss = 0.004281898029148579
iteration 25, loss = 0.0015617185272276402
iteration 26, loss = 0.00206765322946012
iteration 27, loss = 0.001699530752375722
iteration 28, loss = 0.0019425638020038605
iteration 29, loss = 0.0018454729579389095
iteration 30, loss = 0.0019795529078692198
iteration 31, loss = 0.0037854635156691074
iteration 32, loss = 0.002583242254331708
iteration 33, loss = 0.002103043021634221
iteration 34, loss = 0.0019109577406197786
iteration 35, loss = 0.004233562853187323
iteration 36, loss = 0.0023928051814436913
iteration 37, loss = 0.0026340445037931204
iteration 38, loss = 0.0030451694037765265
iteration 39, loss = 0.002470579696819186
iteration 40, loss = 0.0021247370168566704
iteration 41, loss = 0.002044837223365903
iteration 42, loss = 0.00241133407689631
iteration 43, loss = 0.0023348270915448666
iteration 44, loss = 0.0017183050513267517
iteration 45, loss = 0.005410191603004932
iteration 46, loss = 0.0019178002839908004
iteration 47, loss = 0.0026906034909188747
iteration 48, loss = 0.0020489732269197702
iteration 49, loss = 0.001884675701148808
iteration 50, loss = 0.0023477980867028236
iteration 51, loss = 0.003572275396436453
iteration 52, loss = 0.002073724055662751
iteration 53, loss = 0.0020888124126940966
iteration 54, loss = 0.0019411214161664248
iteration 55, loss = 0.002186456462368369
iteration 56, loss = 0.002146388404071331
iteration 57, loss = 0.0018487279303371906
iteration 58, loss = 0.0020333556458353996
iteration 59, loss = 0.0021462261211127043
iteration 60, loss = 0.0017696411814540625
iteration 61, loss = 0.002100131008774042
iteration 62, loss = 0.0022208078298717737
iteration 63, loss = 0.001711570774205029
iteration 64, loss = 0.0023571914061903954
iteration 65, loss = 0.002364792861044407
iteration 66, loss = 0.001850611763074994
iteration 67, loss = 0.0030792453326284885
iteration 68, loss = 0.0016356940614059567
iteration 69, loss = 0.002022125758230686
iteration 70, loss = 0.002454539528116584
iteration 71, loss = 0.0024374641943722963
iteration 72, loss = 0.0023846463300287724
iteration 73, loss = 0.0017503750277683139
iteration 74, loss = 0.002269537653774023
iteration 75, loss = 0.0019617921207100153
iteration 76, loss = 0.002149619162082672
iteration 77, loss = 0.0017561872955411673
iteration 78, loss = 0.001838788972236216
iteration 79, loss = 0.002285484690219164
iteration 80, loss = 0.001961209112778306
iteration 81, loss = 0.0017178753623738885
iteration 82, loss = 0.001915337285026908
iteration 83, loss = 0.001993115060031414
iteration 84, loss = 0.0018497725250199437
iteration 85, loss = 0.0031724805012345314
iteration 86, loss = 0.0018798590172082186
iteration 87, loss = 0.001885188277810812
iteration 88, loss = 0.0018926206976175308
iteration 89, loss = 0.003840263467282057
iteration 90, loss = 0.002101165009662509
iteration 91, loss = 0.002124030375853181
iteration 92, loss = 0.002754808869212866
iteration 93, loss = 0.0022216502111405134
iteration 94, loss = 0.0020665740594267845
iteration 95, loss = 0.002481514820829034
iteration 96, loss = 0.001982305897399783
iteration 97, loss = 0.002239490859210491
iteration 98, loss = 0.0016986002447083592
iteration 99, loss = 0.0025844313204288483
iteration 100, loss = 0.002190726576372981
iteration 101, loss = 0.0025170324370265007
iteration 102, loss = 0.0021175919100642204
iteration 103, loss = 0.0018288798164576292
iteration 104, loss = 0.002019326202571392
iteration 105, loss = 0.0017975253285840154
iteration 106, loss = 0.0016762075247243047
iteration 107, loss = 0.0026921972166746855
iteration 108, loss = 0.0017797608161345124
iteration 109, loss = 0.0029190662316977978
iteration 110, loss = 0.001688307267613709
iteration 111, loss = 0.0031738472171127796
iteration 112, loss = 0.0020711508113890886
iteration 113, loss = 0.0019306931644678116
iteration 114, loss = 0.0023770094849169254
iteration 115, loss = 0.0029407290276139975
iteration 116, loss = 0.003848302410915494
iteration 117, loss = 0.0023533992934972048
iteration 118, loss = 0.002357662655413151
iteration 119, loss = 0.003304597456008196
iteration 120, loss = 0.001922529423609376
iteration 121, loss = 0.0017648315988481045
iteration 122, loss = 0.001984156435355544
iteration 123, loss = 0.0019928535912185907
iteration 124, loss = 0.0021682744845747948
iteration 125, loss = 0.004111189395189285
iteration 126, loss = 0.0028843285981565714
iteration 127, loss = 0.0028895034920424223
iteration 128, loss = 0.0026403211522847414
iteration 129, loss = 0.002102155704051256
iteration 130, loss = 0.001969172153621912
iteration 131, loss = 0.002319362945854664
iteration 132, loss = 0.002269425895065069
iteration 133, loss = 0.002439682837575674
iteration 134, loss = 0.001989862648770213
iteration 135, loss = 0.002045353874564171
iteration 136, loss = 0.0018091064412146807
iteration 137, loss = 0.0018757169600576162
iteration 138, loss = 0.0024859386030584574
iteration 139, loss = 0.0018946126801893115
iteration 140, loss = 0.002157609909772873
iteration 141, loss = 0.003379576373845339
iteration 142, loss = 0.004020801745355129
iteration 143, loss = 0.0018704108661040664
iteration 144, loss = 0.0020053572952747345
iteration 145, loss = 0.0018439993727952242
iteration 146, loss = 0.004115235526114702
iteration 147, loss = 0.0015827651368454099
iteration 148, loss = 0.0016184120904654264
iteration 149, loss = 0.0019716781098395586
iteration 150, loss = 0.002005272079259157
iteration 151, loss = 0.003010602667927742
iteration 152, loss = 0.0019063043873757124
iteration 153, loss = 0.0024345824494957924
iteration 154, loss = 0.0021316250786185265
iteration 155, loss = 0.0024260252248495817
iteration 156, loss = 0.002585895359516144
iteration 157, loss = 0.0018441790016368032
iteration 158, loss = 0.0017828247509896755
iteration 159, loss = 0.002508508274331689
iteration 160, loss = 0.00193599343765527
iteration 161, loss = 0.0017071240581572056
iteration 162, loss = 0.002144967205822468
iteration 163, loss = 0.0018709915457293391
iteration 164, loss = 0.0025683953426778316
iteration 165, loss = 0.0026312938425689936
iteration 166, loss = 0.0035319572780281305
iteration 167, loss = 0.003396170912310481
iteration 168, loss = 0.002554116304963827
iteration 169, loss = 0.003289293497800827
iteration 170, loss = 0.004309054464101791
iteration 171, loss = 0.001645060139708221
iteration 172, loss = 0.0019629355520009995
iteration 173, loss = 0.002596695674583316
iteration 174, loss = 0.0018403740832582116
iteration 175, loss = 0.002177784452214837
iteration 176, loss = 0.00187804049346596
iteration 177, loss = 0.0017810353310778737
iteration 178, loss = 0.0019208942539989948
iteration 179, loss = 0.0017221994930878282
iteration 180, loss = 0.0015755861531943083
iteration 181, loss = 0.0017564429435878992
iteration 182, loss = 0.0019512298749759793
iteration 183, loss = 0.0018135079881176353
iteration 184, loss = 0.002584411296993494
iteration 185, loss = 0.0018453553784638643
iteration 186, loss = 0.0019278700929135084
iteration 187, loss = 0.002135452814400196
iteration 188, loss = 0.002313395030796528
iteration 189, loss = 0.002266299445182085
iteration 190, loss = 0.0038113542832434177
iteration 191, loss = 0.003659934503957629
iteration 192, loss = 0.0021626988891512156
iteration 193, loss = 0.0021597628947347403
iteration 194, loss = 0.0023253473918884993
iteration 195, loss = 0.0018455572426319122
iteration 196, loss = 0.0024084397591650486
iteration 197, loss = 0.0018425177549943328
iteration 198, loss = 0.001963729504495859
iteration 199, loss = 0.0019545378163456917
iteration 200, loss = 0.0019033905118703842
iteration 201, loss = 0.0022994722239673138
iteration 202, loss = 0.0022434364072978497
iteration 203, loss = 0.0017361161299049854
iteration 204, loss = 0.0026610177010297775
iteration 205, loss = 0.002592317294329405
iteration 206, loss = 0.0027519313152879477
iteration 207, loss = 0.0023006375413388014
iteration 208, loss = 0.0031899437308311462
iteration 209, loss = 0.0023401351645588875
iteration 210, loss = 0.0026388410478830338
iteration 211, loss = 0.0017743377247825265
iteration 212, loss = 0.003268770407885313
iteration 213, loss = 0.0018892943626269698
iteration 214, loss = 0.0024131587706506252
iteration 215, loss = 0.001873472472652793
iteration 216, loss = 0.002242133254185319
iteration 217, loss = 0.0016676434315741062
iteration 218, loss = 0.0018850063206627965
iteration 219, loss = 0.004114705137908459
iteration 220, loss = 0.0017148833721876144
iteration 221, loss = 0.0025233880151063204
iteration 222, loss = 0.0024466481991112232
iteration 223, loss = 0.001623296644538641
iteration 224, loss = 0.00200050906278193
iteration 225, loss = 0.0020916606299579144
iteration 226, loss = 0.0028620767407119274
iteration 227, loss = 0.0015788465971127152
iteration 228, loss = 0.003665181342512369
iteration 229, loss = 0.003070579143241048
iteration 230, loss = 0.001963334158062935
iteration 231, loss = 0.0026779184117913246
iteration 232, loss = 0.002870706608518958
iteration 233, loss = 0.002265737857669592
iteration 234, loss = 0.002047601155936718
iteration 235, loss = 0.0023105721920728683
iteration 236, loss = 0.0020268962252885103
iteration 237, loss = 0.0018463346641510725
iteration 238, loss = 0.0022793803364038467
iteration 239, loss = 0.002075811615213752
iteration 240, loss = 0.002670571208000183
iteration 241, loss = 0.002071883762255311
iteration 242, loss = 0.001967278541997075
iteration 243, loss = 0.002204257296398282
iteration 244, loss = 0.0027700006030499935
iteration 245, loss = 0.0019865031354129314
iteration 246, loss = 0.003057802328839898
iteration 247, loss = 0.0017487002769485116
iteration 248, loss = 0.00174369674641639
iteration 249, loss = 0.0031194696202874184
iteration 250, loss = 0.0022181239910423756
iteration 251, loss = 0.0034317283425480127
iteration 252, loss = 0.0024950774386525154
iteration 253, loss = 0.001494392054155469
iteration 254, loss = 0.0025471015833318233
iteration 255, loss = 0.0019231574842706323
iteration 256, loss = 0.0021011154167354107
iteration 257, loss = 0.0017072622431442142
iteration 258, loss = 0.0021726947743445635
iteration 259, loss = 0.0033732105512171984
iteration 260, loss = 0.002217762405052781
iteration 261, loss = 0.002235913649201393
iteration 262, loss = 0.0018065950134769082
iteration 263, loss = 0.003061202820390463
iteration 264, loss = 0.0034644727129489183
iteration 265, loss = 0.002614546800032258
iteration 266, loss = 0.0017125419108197093
iteration 267, loss = 0.0018979223677888513
iteration 268, loss = 0.0018316414207220078
iteration 269, loss = 0.0027532775420695543
iteration 270, loss = 0.002213401021435857
iteration 271, loss = 0.0018959614681079984
iteration 272, loss = 0.0016325166216120124
iteration 273, loss = 0.002087060594931245
iteration 274, loss = 0.002038633916527033
iteration 275, loss = 0.0036984432954341173
iteration 276, loss = 0.002031509531661868
iteration 277, loss = 0.002216666005551815
iteration 278, loss = 0.0017159765120595694
iteration 279, loss = 0.0022168131545186043
iteration 280, loss = 0.0019956326577812433
iteration 281, loss = 0.003316217102110386
iteration 282, loss = 0.0018368283053860068
iteration 283, loss = 0.002038643229752779
iteration 284, loss = 0.002050472190603614
iteration 285, loss = 0.0020519269164651632
iteration 286, loss = 0.0020662255119532347
iteration 287, loss = 0.0015743293333798647
iteration 288, loss = 0.0027317104395478964
iteration 289, loss = 0.00194428744725883
iteration 290, loss = 0.0016902093775570393
iteration 291, loss = 0.003362067509442568
iteration 292, loss = 0.002162302378565073
iteration 293, loss = 0.00225741695612669
iteration 294, loss = 0.003945305477827787
iteration 295, loss = 0.0025035073049366474
iteration 296, loss = 0.0018133481498807669
iteration 297, loss = 0.002360557671636343
iteration 298, loss = 0.0022888625971972942
iteration 299, loss = 0.0021117734722793102
iteration 300, loss = 0.00278997840359807
iteration 1, loss = 0.0030645928345620632
iteration 2, loss = 0.0019087139517068863
iteration 3, loss = 0.001987283816561103
iteration 4, loss = 0.0019027541857212782
iteration 5, loss = 0.0019020205363631248
iteration 6, loss = 0.0016974214231595397
iteration 7, loss = 0.004053071141242981
iteration 8, loss = 0.0019003722118213773
iteration 9, loss = 0.0019899681210517883
iteration 10, loss = 0.0019201189279556274
iteration 11, loss = 0.002057087142020464
iteration 12, loss = 0.0018599102040752769
iteration 13, loss = 0.0018736626952886581
iteration 14, loss = 0.0029268725775182247
iteration 15, loss = 0.0047273277305066586
iteration 16, loss = 0.004529146011918783
iteration 17, loss = 0.0019861895125359297
iteration 18, loss = 0.0024226363748311996
iteration 19, loss = 0.0042954543605446815
iteration 20, loss = 0.001967574004083872
iteration 21, loss = 0.0030009860638529062
iteration 22, loss = 0.0021530319936573505
iteration 23, loss = 0.001939662965014577
iteration 24, loss = 0.0018529832595959306
iteration 25, loss = 0.002224396448582411
iteration 26, loss = 0.0017088917084038258
iteration 27, loss = 0.0024256904143840075
iteration 28, loss = 0.001972188474610448
iteration 29, loss = 0.0017879243241623044
iteration 30, loss = 0.0017274841666221619
iteration 31, loss = 0.0018195670563727617
iteration 32, loss = 0.0034115214366465807
iteration 33, loss = 0.001923938631080091
iteration 34, loss = 0.001961956499144435
iteration 35, loss = 0.002111974870786071
iteration 36, loss = 0.002081194194033742
iteration 37, loss = 0.0016931893769651651
iteration 38, loss = 0.0021586997900158167
iteration 39, loss = 0.0022358817514032125
iteration 40, loss = 0.0038194460794329643
iteration 41, loss = 0.005125698167830706
iteration 42, loss = 0.001845184713602066
iteration 43, loss = 0.00214332970790565
iteration 44, loss = 0.001925649936310947
iteration 45, loss = 0.002051503397524357
iteration 46, loss = 0.0018138482701033354
iteration 47, loss = 0.0019923977088183165
iteration 48, loss = 0.0018484811298549175
iteration 49, loss = 0.0025382982566952705
iteration 50, loss = 0.0030492208898067474
iteration 51, loss = 0.0022774257231503725
iteration 52, loss = 0.0017382889054715633
iteration 53, loss = 0.0017364735249429941
iteration 54, loss = 0.0016765370965003967
iteration 55, loss = 0.0019340498838573694
iteration 56, loss = 0.001983418595045805
iteration 57, loss = 0.0019917995668947697
iteration 58, loss = 0.002639979589730501
iteration 59, loss = 0.0023270791862159967
iteration 60, loss = 0.0032825847156345844
iteration 61, loss = 0.002521442947909236
iteration 62, loss = 0.0033274174202233553
iteration 63, loss = 0.0016473822761327028
iteration 64, loss = 0.002649587346240878
iteration 65, loss = 0.0017223621252924204
iteration 66, loss = 0.003853532951325178
iteration 67, loss = 0.0021310155279934406
iteration 68, loss = 0.0019078584155067801
iteration 69, loss = 0.0018100956222042441
iteration 70, loss = 0.0015567096415907145
iteration 71, loss = 0.004614022560417652
iteration 72, loss = 0.002668269444257021
iteration 73, loss = 0.0019838439766317606
iteration 74, loss = 0.0024695617612451315
iteration 75, loss = 0.0019328673370182514
iteration 76, loss = 0.0020992336794734
iteration 77, loss = 0.0019399295561015606
iteration 78, loss = 0.0026091726031154394
iteration 79, loss = 0.001922432566061616
iteration 80, loss = 0.0017523004207760096
iteration 81, loss = 0.0018874143715947866
iteration 82, loss = 0.002357449382543564
iteration 83, loss = 0.0019131626468151808
iteration 84, loss = 0.0023208344355225563
iteration 85, loss = 0.0016730476636439562
iteration 86, loss = 0.003993941470980644
iteration 87, loss = 0.0022626861464232206
iteration 88, loss = 0.002993928035721183
iteration 89, loss = 0.0016271197237074375
iteration 90, loss = 0.0020582748111337423
iteration 91, loss = 0.00265200762078166
iteration 92, loss = 0.0021108065266162157
iteration 93, loss = 0.0019001509062945843
iteration 94, loss = 0.00483399024233222
iteration 95, loss = 0.002197269583120942
iteration 96, loss = 0.0016295945970341563
iteration 97, loss = 0.0019184192642569542
iteration 98, loss = 0.0019384403713047504
iteration 99, loss = 0.0015747124562039971
iteration 100, loss = 0.0023307171650230885
iteration 101, loss = 0.0029977825470268726
iteration 102, loss = 0.0015833559446036816
iteration 103, loss = 0.0020893386099487543
iteration 104, loss = 0.002267698757350445
iteration 105, loss = 0.0021531686652451754
iteration 106, loss = 0.003022614633664489
iteration 107, loss = 0.002739345422014594
iteration 108, loss = 0.002223492134362459
iteration 109, loss = 0.0017992723733186722
iteration 110, loss = 0.0020168747287243605
iteration 111, loss = 0.0032785777002573013
iteration 112, loss = 0.0018667448312044144
iteration 113, loss = 0.0017011093441396952
iteration 114, loss = 0.001790605136193335
iteration 115, loss = 0.0024370127357542515
iteration 116, loss = 0.0025218138471245766
iteration 117, loss = 0.0021773772314190865
iteration 118, loss = 0.0016222459962591529
iteration 119, loss = 0.002241835230961442
iteration 120, loss = 0.0017807454569265246
iteration 121, loss = 0.002311434829607606
iteration 122, loss = 0.0018570818938314915
iteration 123, loss = 0.004194511566311121
iteration 124, loss = 0.0018697679042816162
iteration 125, loss = 0.00209609093144536
iteration 126, loss = 0.0018024056917056441
iteration 127, loss = 0.001996678998693824
iteration 128, loss = 0.0020742565393447876
iteration 129, loss = 0.0037253573536872864
iteration 130, loss = 0.0025117804761976004
iteration 131, loss = 0.002183466451242566
iteration 132, loss = 0.0026485170237720013
iteration 133, loss = 0.0033105663023889065
iteration 134, loss = 0.002255932427942753
iteration 135, loss = 0.0027203860227018595
iteration 136, loss = 0.002466011792421341
iteration 137, loss = 0.0016717220423743129
iteration 138, loss = 0.002147078514099121
iteration 139, loss = 0.0026774979196488857
iteration 140, loss = 0.004020866937935352
iteration 141, loss = 0.002829355886206031
iteration 142, loss = 0.003480908926576376
iteration 143, loss = 0.001908396719954908
iteration 144, loss = 0.002070788526907563
iteration 145, loss = 0.0022679429966956377
iteration 146, loss = 0.002118810312822461
iteration 147, loss = 0.002071435796096921
iteration 148, loss = 0.0024018380790948868
iteration 149, loss = 0.001963448477908969
iteration 150, loss = 0.002854996593669057
iteration 151, loss = 0.002125881612300873
iteration 152, loss = 0.0018538587028160691
iteration 153, loss = 0.002700288314372301
iteration 154, loss = 0.002284435322508216
iteration 155, loss = 0.0018526730127632618
iteration 156, loss = 0.0033405048307031393
iteration 157, loss = 0.0020614140667021275
iteration 158, loss = 0.0021160177420824766
iteration 159, loss = 0.002844266127794981
iteration 160, loss = 0.0018266132101416588
iteration 161, loss = 0.002044627210125327
iteration 162, loss = 0.0018874675733968616
iteration 163, loss = 0.001725691370666027
iteration 164, loss = 0.0017802934162318707
iteration 165, loss = 0.003059981158003211
iteration 166, loss = 0.00179907470010221
iteration 167, loss = 0.001968531869351864
iteration 168, loss = 0.0032128796447068453
iteration 169, loss = 0.002859370782971382
iteration 170, loss = 0.0019958780612796545
iteration 171, loss = 0.002172528300434351
iteration 172, loss = 0.0021816117223352194
iteration 173, loss = 0.0020031267777085304
iteration 174, loss = 0.002974366070702672
iteration 175, loss = 0.0024195939768105745
iteration 176, loss = 0.001887791440822184
iteration 177, loss = 0.0034519783221185207
iteration 178, loss = 0.0029586749151349068
iteration 179, loss = 0.0023118508979678154
iteration 180, loss = 0.0018646614626049995
iteration 181, loss = 0.0019492068095132709
iteration 182, loss = 0.003975612111389637
iteration 183, loss = 0.0020369498524814844
iteration 184, loss = 0.001718296087346971
iteration 185, loss = 0.0028958627954125404
iteration 186, loss = 0.0023013560567051172
iteration 187, loss = 0.0028249002061784267
iteration 188, loss = 0.0017725869547575712
iteration 189, loss = 0.0017946138978004456
iteration 190, loss = 0.0019116089679300785
iteration 191, loss = 0.0018132851691916585
iteration 192, loss = 0.0021000956185162067
iteration 193, loss = 0.002143003512173891
iteration 194, loss = 0.001848837360739708
iteration 195, loss = 0.0017835829639807343
iteration 196, loss = 0.001987574389204383
iteration 197, loss = 0.00180722470395267
iteration 198, loss = 0.0018725476693361998
iteration 199, loss = 0.002309778006747365
iteration 200, loss = 0.003419649787247181
iteration 201, loss = 0.0025561065413057804
iteration 202, loss = 0.002274444792419672
iteration 203, loss = 0.003375076223164797
iteration 204, loss = 0.0025674167554825544
iteration 205, loss = 0.002041235100477934
iteration 206, loss = 0.0019771161023527384
iteration 207, loss = 0.0035094781778752804
iteration 208, loss = 0.0019331673393025994
iteration 209, loss = 0.0019257278181612492
iteration 210, loss = 0.0015774734783917665
iteration 211, loss = 0.002082846127450466
iteration 212, loss = 0.0017118944087997079
iteration 213, loss = 0.002081202808767557
iteration 214, loss = 0.0024969857186079025
iteration 215, loss = 0.002302770037204027
iteration 216, loss = 0.0017997028771787882
iteration 217, loss = 0.0024992553517222404
iteration 218, loss = 0.001928177778609097
iteration 219, loss = 0.002193986903876066
iteration 220, loss = 0.0022051145788282156
iteration 221, loss = 0.003392668440937996
iteration 222, loss = 0.001829321845434606
iteration 223, loss = 0.0018031103536486626
iteration 224, loss = 0.003230967093259096
iteration 225, loss = 0.0018194112926721573
iteration 226, loss = 0.0021165823563933372
iteration 227, loss = 0.0018734732875600457
iteration 228, loss = 0.0017707404913380742
iteration 229, loss = 0.00192237738519907
iteration 230, loss = 0.0017448444850742817
iteration 231, loss = 0.002026800997555256
iteration 232, loss = 0.0018293146276846528
iteration 233, loss = 0.0016166497953236103
iteration 234, loss = 0.002196862827986479
iteration 235, loss = 0.0022569105494767427
iteration 236, loss = 0.0020091584883630276
iteration 237, loss = 0.002061513252556324
iteration 238, loss = 0.0024479252751916647
iteration 239, loss = 0.0016760360449552536
iteration 240, loss = 0.0018827976891770959
iteration 241, loss = 0.004161590710282326
iteration 242, loss = 0.0018458941485732794
iteration 243, loss = 0.0019429402891546488
iteration 244, loss = 0.0018662421498447657
iteration 245, loss = 0.0020941218826919794
iteration 246, loss = 0.002030491130426526
iteration 247, loss = 0.0018457801779732108
iteration 248, loss = 0.0021262678783386946
iteration 249, loss = 0.0019762006122618914
iteration 250, loss = 0.0018192236311733723
iteration 251, loss = 0.0039077321998775005
iteration 252, loss = 0.0019063783111050725
iteration 253, loss = 0.002365176333114505
iteration 254, loss = 0.003913203254342079
iteration 255, loss = 0.002017204649746418
iteration 256, loss = 0.0027824623975902796
iteration 257, loss = 0.0031525446102023125
iteration 258, loss = 0.0017508241580799222
iteration 259, loss = 0.0017284663626924157
iteration 260, loss = 0.0019056963501498103
iteration 261, loss = 0.002039065584540367
iteration 262, loss = 0.0019597227219492197
iteration 263, loss = 0.002838592976331711
iteration 264, loss = 0.002092821290716529
iteration 265, loss = 0.0018857428804039955
iteration 266, loss = 0.0019568682182580233
iteration 267, loss = 0.0020944117568433285
iteration 268, loss = 0.002096744952723384
iteration 269, loss = 0.0019848081283271313
iteration 270, loss = 0.0020014748442918062
iteration 271, loss = 0.002094120951369405
iteration 272, loss = 0.002400700468569994
iteration 273, loss = 0.0019452105043455958
iteration 274, loss = 0.0020304860081523657
iteration 275, loss = 0.001986939460039139
iteration 276, loss = 0.001630940125323832
iteration 277, loss = 0.001930098980665207
iteration 278, loss = 0.003068464808166027
iteration 279, loss = 0.002576518105342984
iteration 280, loss = 0.002067915629595518
iteration 281, loss = 0.004009028896689415
iteration 282, loss = 0.0019091396825388074
iteration 283, loss = 0.003184561850503087
iteration 284, loss = 0.002564525930210948
iteration 285, loss = 0.0021244448143988848
iteration 286, loss = 0.002194036263972521
iteration 287, loss = 0.002271967474371195
iteration 288, loss = 0.0025544867385178804
iteration 289, loss = 0.001996624516323209
iteration 290, loss = 0.0016890591941773891
iteration 291, loss = 0.002541938563808799
iteration 292, loss = 0.0025123078376054764
iteration 293, loss = 0.0024862647987902164
iteration 294, loss = 0.0017777644097805023
iteration 295, loss = 0.0024017447140067816
iteration 296, loss = 0.0019163447432219982
iteration 297, loss = 0.0027111175004392862
iteration 298, loss = 0.002242455491796136
iteration 299, loss = 0.0035681105218827724
iteration 300, loss = 0.0019999309442937374
iteration 1, loss = 0.0022045564837753773
iteration 2, loss = 0.0018369201570749283
iteration 3, loss = 0.002536472398787737
iteration 4, loss = 0.002860122825950384
iteration 5, loss = 0.002481047762557864
iteration 6, loss = 0.002401004545390606
iteration 7, loss = 0.0022464068606495857
iteration 8, loss = 0.003933836240321398
iteration 9, loss = 0.0021489770151674747
iteration 10, loss = 0.002251021098345518
iteration 11, loss = 0.002398124663159251
iteration 12, loss = 0.002012117300182581
iteration 13, loss = 0.0023579164408147335
iteration 14, loss = 0.002045535482466221
iteration 15, loss = 0.002493090694770217
iteration 16, loss = 0.0018707807175815105
iteration 17, loss = 0.0018810465699061751
iteration 18, loss = 0.0021690521389245987
iteration 19, loss = 0.0023290079552680254
iteration 20, loss = 0.0018734681652858853
iteration 21, loss = 0.0019973961170762777
iteration 22, loss = 0.0018728983122855425
iteration 23, loss = 0.004520845599472523
iteration 24, loss = 0.0020256247371435165
iteration 25, loss = 0.0021357936784625053
iteration 26, loss = 0.005131545476615429
iteration 27, loss = 0.0018246009713038802
iteration 28, loss = 0.0023029125295579433
iteration 29, loss = 0.0021107953507453203
iteration 30, loss = 0.0017252536490559578
iteration 31, loss = 0.002109482418745756
iteration 32, loss = 0.0017961810808628798
iteration 33, loss = 0.0016510894056409597
iteration 34, loss = 0.0025378528516739607
iteration 35, loss = 0.0018571809632703662
iteration 36, loss = 0.0017280292231589556
iteration 37, loss = 0.003282204968854785
iteration 38, loss = 0.004193623550236225
iteration 39, loss = 0.0017020090017467737
iteration 40, loss = 0.0020766248926520348
iteration 41, loss = 0.001693629426881671
iteration 42, loss = 0.002087934873998165
iteration 43, loss = 0.0016803063917905092
iteration 44, loss = 0.0019813382532447577
iteration 45, loss = 0.0017010445008054376
iteration 46, loss = 0.0023357232566922903
iteration 47, loss = 0.0023330075200647116
iteration 48, loss = 0.0024577006697654724
iteration 49, loss = 0.0018995702266693115
iteration 50, loss = 0.002642950275912881
iteration 51, loss = 0.0021225600503385067
iteration 52, loss = 0.0028779120184481144
iteration 53, loss = 0.003811886068433523
iteration 54, loss = 0.0032329841051250696
iteration 55, loss = 0.0024774186313152313
iteration 56, loss = 0.0027919351123273373
iteration 57, loss = 0.002069374779239297
iteration 58, loss = 0.003603643737733364
iteration 59, loss = 0.00198614364489913
iteration 60, loss = 0.0039389836601912975
iteration 61, loss = 0.0028423424810171127
iteration 62, loss = 0.0025717751123011112
iteration 63, loss = 0.002225311705842614
iteration 64, loss = 0.002513096434995532
iteration 65, loss = 0.001966070383787155
iteration 66, loss = 0.001982171321287751
iteration 67, loss = 0.0020665598567575216
iteration 68, loss = 0.003210016991943121
iteration 69, loss = 0.002749847946688533
iteration 70, loss = 0.0017845687689259648
iteration 71, loss = 0.003078466048464179
iteration 72, loss = 0.0019055712036788464
iteration 73, loss = 0.00394402677193284
iteration 74, loss = 0.0025712428614497185
iteration 75, loss = 0.0030718743801116943
iteration 76, loss = 0.001833014888688922
iteration 77, loss = 0.0018067930359393358
iteration 78, loss = 0.001847712555900216
iteration 79, loss = 0.0019324880558997393
iteration 80, loss = 0.0019697509706020355
iteration 81, loss = 0.0041787209920585155
iteration 82, loss = 0.0021936052944511175
iteration 83, loss = 0.0018993222620338202
iteration 84, loss = 0.0022134187165647745
iteration 85, loss = 0.0034827461931854486
iteration 86, loss = 0.0017888371367007494
iteration 87, loss = 0.0036647545639425516
iteration 88, loss = 0.0017093390924856067
iteration 89, loss = 0.002717047929763794
iteration 90, loss = 0.003245260799303651
iteration 91, loss = 0.0020831357687711716
iteration 92, loss = 0.0030179116874933243
iteration 93, loss = 0.00239842152222991
iteration 94, loss = 0.0026585934683680534
iteration 95, loss = 0.0019471830455586314
iteration 96, loss = 0.001650500576943159
iteration 97, loss = 0.002148326486349106
iteration 98, loss = 0.0021576739381998777
iteration 99, loss = 0.0018347392324358225
iteration 100, loss = 0.001925966003909707
iteration 101, loss = 0.0023743286728858948
iteration 102, loss = 0.0017934050410985947
iteration 103, loss = 0.001886096433736384
iteration 104, loss = 0.001851704902946949
iteration 105, loss = 0.0025088395923376083
iteration 106, loss = 0.004068928770720959
iteration 107, loss = 0.0020898953080177307
iteration 108, loss = 0.002176651731133461
iteration 109, loss = 0.001691665267571807
iteration 110, loss = 0.0026547834277153015
iteration 111, loss = 0.003446186427026987
iteration 112, loss = 0.0036411865148693323
iteration 113, loss = 0.001989685231819749
iteration 114, loss = 0.0021919813007116318
iteration 115, loss = 0.002104927320033312
iteration 116, loss = 0.0021797637455165386
iteration 117, loss = 0.0036422342527657747
iteration 118, loss = 0.003082672134041786
iteration 119, loss = 0.0019134620670229197
iteration 120, loss = 0.001966443844139576
iteration 121, loss = 0.0025394242256879807
iteration 122, loss = 0.004117876291275024
iteration 123, loss = 0.0020127242896705866
iteration 124, loss = 0.0018179100006818771
iteration 125, loss = 0.0028124158270657063
iteration 126, loss = 0.0021322418469935656
iteration 127, loss = 0.0025425502099096775
iteration 128, loss = 0.002005939604714513
iteration 129, loss = 0.0025879517197608948
iteration 130, loss = 0.0029861535876989365
iteration 131, loss = 0.0020146577153354883
iteration 132, loss = 0.0017728616949170828
iteration 133, loss = 0.0027089964132755995
iteration 134, loss = 0.0017343433573842049
iteration 135, loss = 0.0028941859491169453
iteration 136, loss = 0.0026640966534614563
iteration 137, loss = 0.0019762672018259764
iteration 138, loss = 0.0025479146279394627
iteration 139, loss = 0.0021790573373436928
iteration 140, loss = 0.001785375876352191
iteration 141, loss = 0.001751807751134038
iteration 142, loss = 0.0016610121820122004
iteration 143, loss = 0.0017934278585016727
iteration 144, loss = 0.0017046495340764523
iteration 145, loss = 0.0017078884411603212
iteration 146, loss = 0.00184314651414752
iteration 147, loss = 0.002088259207084775
iteration 148, loss = 0.002278480678796768
iteration 149, loss = 0.0020723785273730755
iteration 150, loss = 0.0017970496555790305
iteration 151, loss = 0.0020220750011503696
iteration 152, loss = 0.0018598510650917888
iteration 153, loss = 0.002081208862364292
iteration 154, loss = 0.002288123359903693
iteration 155, loss = 0.0024943423923105
iteration 156, loss = 0.0019591329619288445
iteration 157, loss = 0.002529280260205269
iteration 158, loss = 0.0025743243750184774
iteration 159, loss = 0.0017948520835489035
iteration 160, loss = 0.0018212762661278248
iteration 161, loss = 0.0022836162243038416
iteration 162, loss = 0.0019370312802493572
iteration 163, loss = 0.0017181404400616884
iteration 164, loss = 0.0030241243075579405
iteration 165, loss = 0.0022462178021669388
iteration 166, loss = 0.0019662207923829556
iteration 167, loss = 0.0021952600218355656
iteration 168, loss = 0.0019971965812146664
iteration 169, loss = 0.0019011322874575853
iteration 170, loss = 0.001850754488259554
iteration 171, loss = 0.002227636519819498
iteration 172, loss = 0.002109842374920845
iteration 173, loss = 0.002466072328388691
iteration 174, loss = 0.00226691085845232
iteration 175, loss = 0.0025837409775704145
iteration 176, loss = 0.0023010512813925743
iteration 177, loss = 0.0020532875787466764
iteration 178, loss = 0.00198344886302948
iteration 179, loss = 0.0027549765072762966
iteration 180, loss = 0.0021214871667325497
iteration 181, loss = 0.0019397474825382233
iteration 182, loss = 0.0020608529448509216
iteration 183, loss = 0.0021465816535055637
iteration 184, loss = 0.0019115112954750657
iteration 185, loss = 0.001999471802264452
iteration 186, loss = 0.001682406640611589
iteration 187, loss = 0.0020848154090344906
iteration 188, loss = 0.0020896135829389095
iteration 189, loss = 0.0024246503598988056
iteration 190, loss = 0.0018944586627185345
iteration 191, loss = 0.0018704513786360621
iteration 192, loss = 0.004384148400276899
iteration 193, loss = 0.0016846760408952832
iteration 194, loss = 0.002091151662170887
iteration 195, loss = 0.002620898187160492
iteration 196, loss = 0.0018209685804322362
iteration 197, loss = 0.003784256288781762
iteration 198, loss = 0.0016865199431777
iteration 199, loss = 0.0024160929024219513
iteration 200, loss = 0.0020625486504286528
iteration 201, loss = 0.0018362689297646284
iteration 202, loss = 0.0018907161429524422
iteration 203, loss = 0.002119742799550295
iteration 204, loss = 0.0016776671400293708
iteration 205, loss = 0.0018228065455332398
iteration 206, loss = 0.001802860526368022
iteration 207, loss = 0.001914068590849638
iteration 208, loss = 0.0017801763024181128
iteration 209, loss = 0.001846844214014709
iteration 210, loss = 0.0033010144252330065
iteration 211, loss = 0.001963810296729207
iteration 212, loss = 0.003931413870304823
iteration 213, loss = 0.0017818312626332045
iteration 214, loss = 0.00287674181163311
iteration 215, loss = 0.0017865293193608522
iteration 216, loss = 0.001986241899430752
iteration 217, loss = 0.0018592975102365017
iteration 218, loss = 0.002075731521472335
iteration 219, loss = 0.002007466973736882
iteration 220, loss = 0.0017902154941111803
iteration 221, loss = 0.002507508499547839
iteration 222, loss = 0.0020177739206701517
iteration 223, loss = 0.0022318700794130564
iteration 224, loss = 0.001582432771101594
iteration 225, loss = 0.0019514522282406688
iteration 226, loss = 0.002253923797979951
iteration 227, loss = 0.002235130872577429
iteration 228, loss = 0.0034222069662064314
iteration 229, loss = 0.00194424984510988
iteration 230, loss = 0.0023494623601436615
iteration 231, loss = 0.001856235321611166
iteration 232, loss = 0.0017647661734372377
iteration 233, loss = 0.0021497942507267
iteration 234, loss = 0.0024835607036948204
iteration 235, loss = 0.003526734421029687
iteration 236, loss = 0.0033424952998757362
iteration 237, loss = 0.0017261754255741835
iteration 238, loss = 0.001897358801215887
iteration 239, loss = 0.0024337605573236942
iteration 240, loss = 0.002266298746690154
iteration 241, loss = 0.0018816862720996141
iteration 242, loss = 0.003019999247044325
iteration 243, loss = 0.002236400032415986
iteration 244, loss = 0.0021593719720840454
iteration 245, loss = 0.001649471465498209
iteration 246, loss = 0.002095571020618081
iteration 247, loss = 0.0026087728329002857
iteration 248, loss = 0.0019618039950728416
iteration 249, loss = 0.0017023885156959295
iteration 250, loss = 0.0026494404301047325
iteration 251, loss = 0.0015997698064893484
iteration 252, loss = 0.002010433468967676
iteration 253, loss = 0.0035610750783234835
iteration 254, loss = 0.0020625703036785126
iteration 255, loss = 0.001720310072414577
iteration 256, loss = 0.0019767307676374912
iteration 257, loss = 0.001857285969890654
iteration 258, loss = 0.002272792626172304
iteration 259, loss = 0.001695944112725556
iteration 260, loss = 0.002465350553393364
iteration 261, loss = 0.0023801689967513084
iteration 262, loss = 0.0032693054527044296
iteration 263, loss = 0.0017569815972819924
iteration 264, loss = 0.002662113169208169
iteration 265, loss = 0.002079639583826065
iteration 266, loss = 0.003788798116147518
iteration 267, loss = 0.0021589442621916533
iteration 268, loss = 0.002866731258109212
iteration 269, loss = 0.0014356246683746576
iteration 270, loss = 0.0016884139040485024
iteration 271, loss = 0.0019027662929147482
iteration 272, loss = 0.002413468901067972
iteration 273, loss = 0.002277038060128689
iteration 274, loss = 0.0025060614570975304
iteration 275, loss = 0.0020074641797691584
iteration 276, loss = 0.001783130457624793
iteration 277, loss = 0.0020249534863978624
iteration 278, loss = 0.0016860486939549446
iteration 279, loss = 0.0020806463435292244
iteration 280, loss = 0.0020798523910343647
iteration 281, loss = 0.0021811523474752903
iteration 282, loss = 0.001802906859666109
iteration 283, loss = 0.0037502627819776535
iteration 284, loss = 0.002112742979079485
iteration 285, loss = 0.002181748393923044
iteration 286, loss = 0.002492527011781931
iteration 287, loss = 0.002174764173105359
iteration 288, loss = 0.0021299489308148623
iteration 289, loss = 0.002971349284052849
iteration 290, loss = 0.001608783844858408
iteration 291, loss = 0.002672296017408371
iteration 292, loss = 0.0034021104220300913
iteration 293, loss = 0.001853426219895482
iteration 294, loss = 0.0032948406878858805
iteration 295, loss = 0.0025002681650221348
iteration 296, loss = 0.002490365644916892
iteration 297, loss = 0.0023717726580798626
iteration 298, loss = 0.0018983030458912253
iteration 299, loss = 0.0017582150176167488
iteration 300, loss = 0.002210624050348997
iteration 1, loss = 0.0019889066461473703
iteration 2, loss = 0.002394281327724457
iteration 3, loss = 0.002112417481839657
iteration 4, loss = 0.0021501414012163877
iteration 5, loss = 0.002046849112957716
iteration 6, loss = 0.0018757733050733805
iteration 7, loss = 0.0026597112882882357
iteration 8, loss = 0.0019426399376243353
iteration 9, loss = 0.0018579536117613316
iteration 10, loss = 0.0017935560317710042
iteration 11, loss = 0.0019004119094461203
iteration 12, loss = 0.0018239241326227784
iteration 13, loss = 0.0018898784182965755
iteration 14, loss = 0.003481669118627906
iteration 15, loss = 0.0016726871253922582
iteration 16, loss = 0.002291862154379487
iteration 17, loss = 0.003993386402726173
iteration 18, loss = 0.0028696635272353888
iteration 19, loss = 0.0022291976492851973
iteration 20, loss = 0.003470583353191614
iteration 21, loss = 0.0019311456708237529
iteration 22, loss = 0.0018099561566486955
iteration 23, loss = 0.0027485103346407413
iteration 24, loss = 0.0019456667359918356
iteration 25, loss = 0.001964193070307374
iteration 26, loss = 0.002310981974005699
iteration 27, loss = 0.0018417745595797896
iteration 28, loss = 0.003668403485789895
iteration 29, loss = 0.0031734025105834007
iteration 30, loss = 0.0017918060766533017
iteration 31, loss = 0.002688801381736994
iteration 32, loss = 0.00248334975913167
iteration 33, loss = 0.0029527246952056885
iteration 34, loss = 0.0019821682944893837
iteration 35, loss = 0.0022239251993596554
iteration 36, loss = 0.0018518030410632491
iteration 37, loss = 0.0019449926912784576
iteration 38, loss = 0.0023020056542009115
iteration 39, loss = 0.0030996741261333227
iteration 40, loss = 0.002098886528983712
iteration 41, loss = 0.0019573105964809656
iteration 42, loss = 0.002150928135961294
iteration 43, loss = 0.0020518405362963676
iteration 44, loss = 0.0024435792583972216
iteration 45, loss = 0.0023937788791954517
iteration 46, loss = 0.0021201285999268293
iteration 47, loss = 0.003233091440051794
iteration 48, loss = 0.0020021572709083557
iteration 49, loss = 0.003074239706620574
iteration 50, loss = 0.0026197151746600866
iteration 51, loss = 0.001725703477859497
iteration 52, loss = 0.0020989403128623962
iteration 53, loss = 0.0019291368080303073
iteration 54, loss = 0.0018381592817604542
iteration 55, loss = 0.0029739313758909702
iteration 56, loss = 0.002364168642088771
iteration 57, loss = 0.0033062677830457687
iteration 58, loss = 0.0034033851698040962
iteration 59, loss = 0.0018179590115323663
iteration 60, loss = 0.0021109841763973236
iteration 61, loss = 0.0018676023464649916
iteration 62, loss = 0.0016347096534445882
iteration 63, loss = 0.0020149308256804943
iteration 64, loss = 0.0019043058855459094
iteration 65, loss = 0.0017258357256650925
iteration 66, loss = 0.002206037752330303
iteration 67, loss = 0.004396504256874323
iteration 68, loss = 0.0017987254541367292
iteration 69, loss = 0.001947175944224
iteration 70, loss = 0.002186046913266182
iteration 71, loss = 0.002237905515357852
iteration 72, loss = 0.0022231293842196465
iteration 73, loss = 0.004592765588313341
iteration 74, loss = 0.0016541383229196072
iteration 75, loss = 0.0021960115991532803
iteration 76, loss = 0.0022894302383065224
iteration 77, loss = 0.001882966491393745
iteration 78, loss = 0.0019809429068118334
iteration 79, loss = 0.0017054369673132896
iteration 80, loss = 0.0020846351981163025
iteration 81, loss = 0.001986393239349127
iteration 82, loss = 0.002581765642389655
iteration 83, loss = 0.002129481639713049
iteration 84, loss = 0.002069987589493394
iteration 85, loss = 0.0017646611668169498
iteration 86, loss = 0.0016836822032928467
iteration 87, loss = 0.002062160987406969
iteration 88, loss = 0.0022291343193501234
iteration 89, loss = 0.0017340407939627767
iteration 90, loss = 0.0021446074824780226
iteration 91, loss = 0.0029083045665174723
iteration 92, loss = 0.002038970123976469
iteration 93, loss = 0.0035690483637154102
iteration 94, loss = 0.0015852723736315966
iteration 95, loss = 0.0017555409576743841
iteration 96, loss = 0.0023049646988511086
iteration 97, loss = 0.0022428035736083984
iteration 98, loss = 0.00173871498554945
iteration 99, loss = 0.0034614705946296453
iteration 100, loss = 0.003098582848906517
iteration 101, loss = 0.0018909776117652655
iteration 102, loss = 0.00417607044801116
iteration 103, loss = 0.002129243453964591
iteration 104, loss = 0.004223257303237915
iteration 105, loss = 0.003682148177176714
iteration 106, loss = 0.002118946984410286
iteration 107, loss = 0.0016465539811179042
iteration 108, loss = 0.002182814758270979
iteration 109, loss = 0.0019055503653362393
iteration 110, loss = 0.0018617992755025625
iteration 111, loss = 0.0020162577275186777
iteration 112, loss = 0.0018027354963123798
iteration 113, loss = 0.002178722992539406
iteration 114, loss = 0.00217822240665555
iteration 115, loss = 0.0016311140498146415
iteration 116, loss = 0.0020308210514485836
iteration 117, loss = 0.0036789304576814175
iteration 118, loss = 0.0024806056171655655
iteration 119, loss = 0.0021193516440689564
iteration 120, loss = 0.0015782125992700458
iteration 121, loss = 0.003274155082181096
iteration 122, loss = 0.0015854549128562212
iteration 123, loss = 0.0018895490793511271
iteration 124, loss = 0.0028738565742969513
iteration 125, loss = 0.001853229128755629
iteration 126, loss = 0.0025948239490389824
iteration 127, loss = 0.0020147268660366535
iteration 128, loss = 0.002077887300401926
iteration 129, loss = 0.001957207452505827
iteration 130, loss = 0.002670431975275278
iteration 131, loss = 0.0017653621034696698
iteration 132, loss = 0.001749302726238966
iteration 133, loss = 0.0019263180438429117
iteration 134, loss = 0.0017120151314884424
iteration 135, loss = 0.002269899006932974
iteration 136, loss = 0.0021821861155331135
iteration 137, loss = 0.0020275278948247433
iteration 138, loss = 0.0021524298936128616
iteration 139, loss = 0.001832492183893919
iteration 140, loss = 0.0017541013658046722
iteration 141, loss = 0.0016271622152999043
iteration 142, loss = 0.0038590049371123314
iteration 143, loss = 0.00181815295945853
iteration 144, loss = 0.0018592780688777566
iteration 145, loss = 0.0024365477729588747
iteration 146, loss = 0.0024685696698725224
iteration 147, loss = 0.0020378760527819395
iteration 148, loss = 0.0018531522946432233
iteration 149, loss = 0.0019269018666818738
iteration 150, loss = 0.002078302903100848
iteration 151, loss = 0.002759060589596629
iteration 152, loss = 0.001880466123111546
iteration 153, loss = 0.002127577317878604
iteration 154, loss = 0.0031345714814960957
iteration 155, loss = 0.002330171409994364
iteration 156, loss = 0.002738551702350378
iteration 157, loss = 0.0017468002624809742
iteration 158, loss = 0.0019458585884422064
iteration 159, loss = 0.0021559856832027435
iteration 160, loss = 0.00402087764814496
iteration 161, loss = 0.003088720142841339
iteration 162, loss = 0.002083397703245282
iteration 163, loss = 0.002418842865154147
iteration 164, loss = 0.0021756207570433617
iteration 165, loss = 0.0016140532679855824
iteration 166, loss = 0.0018192173447459936
iteration 167, loss = 0.0017163400771096349
iteration 168, loss = 0.0018963934853672981
iteration 169, loss = 0.002060886472463608
iteration 170, loss = 0.0018118226435035467
iteration 171, loss = 0.0017209436045959592
iteration 172, loss = 0.0018912630621343851
iteration 173, loss = 0.0017234531696885824
iteration 174, loss = 0.002731508109718561
iteration 175, loss = 0.001996114384382963
iteration 176, loss = 0.002601221203804016
iteration 177, loss = 0.0025630502495914698
iteration 178, loss = 0.001988873118534684
iteration 179, loss = 0.0025924930814653635
iteration 180, loss = 0.002025782596319914
iteration 181, loss = 0.0019628102891147137
iteration 182, loss = 0.0020536892116069794
iteration 183, loss = 0.0032316595315933228
iteration 184, loss = 0.0030040224082767963
iteration 185, loss = 0.0021383462008088827
iteration 186, loss = 0.002852937439456582
iteration 187, loss = 0.0019209235906600952
iteration 188, loss = 0.002211102982982993
iteration 189, loss = 0.0024778731167316437
iteration 190, loss = 0.0021670577116310596
iteration 191, loss = 0.0019479937618598342
iteration 192, loss = 0.00193747878074646
iteration 193, loss = 0.001976506784558296
iteration 194, loss = 0.002614520490169525
iteration 195, loss = 0.0027295812033116817
iteration 196, loss = 0.001974749378859997
iteration 197, loss = 0.002130181062966585
iteration 198, loss = 0.0021441893186420202
iteration 199, loss = 0.0018900216091424227
iteration 200, loss = 0.0019571727607399225
iteration 201, loss = 0.001999279484152794
iteration 202, loss = 0.0023335772566497326
iteration 203, loss = 0.0016897094901651144
iteration 204, loss = 0.0018231801223009825
iteration 205, loss = 0.001860044663771987
iteration 206, loss = 0.0016971988370642066
iteration 207, loss = 0.0021201290655881166
iteration 208, loss = 0.002050958573818207
iteration 209, loss = 0.0023331220727413893
iteration 210, loss = 0.002081586280837655
iteration 211, loss = 0.001878401730209589
iteration 212, loss = 0.004042342305183411
iteration 213, loss = 0.004151088651269674
iteration 214, loss = 0.0018020824063569307
iteration 215, loss = 0.0022553580347448587
iteration 216, loss = 0.0020428085699677467
iteration 217, loss = 0.002135364105924964
iteration 218, loss = 0.001749102957546711
iteration 219, loss = 0.0016038245521485806
iteration 220, loss = 0.0022913580760359764
iteration 221, loss = 0.003000758122652769
iteration 222, loss = 0.001873021712526679
iteration 223, loss = 0.0027745035476982594
iteration 224, loss = 0.0033918048720806837
iteration 225, loss = 0.002699950709939003
iteration 226, loss = 0.002619931474328041
iteration 227, loss = 0.0020471978932619095
iteration 228, loss = 0.0022409686353057623
iteration 229, loss = 0.0035317079164087772
iteration 230, loss = 0.0018713651224970818
iteration 231, loss = 0.002024400280788541
iteration 232, loss = 0.0018163265194743872
iteration 233, loss = 0.0019518157932907343
iteration 234, loss = 0.0016380645101889968
iteration 235, loss = 0.0030401875264942646
iteration 236, loss = 0.0019497688626870513
iteration 237, loss = 0.002057519042864442
iteration 238, loss = 0.002296899911016226
iteration 239, loss = 0.0016584397526457906
iteration 240, loss = 0.0022245373111218214
iteration 241, loss = 0.0016361626330763102
iteration 242, loss = 0.0019067191751673818
iteration 243, loss = 0.0017417205963283777
iteration 244, loss = 0.0020809017587453127
iteration 245, loss = 0.0026889625005424023
iteration 246, loss = 0.0032611615024507046
iteration 247, loss = 0.0024565174244344234
iteration 248, loss = 0.0023893460165709257
iteration 249, loss = 0.002761443378403783
iteration 250, loss = 0.0017634056275710464
iteration 251, loss = 0.0029897806234657764
iteration 252, loss = 0.0020352150313556194
iteration 253, loss = 0.0016676688101142645
iteration 254, loss = 0.0021156449802219868
iteration 255, loss = 0.0024797660298645496
iteration 256, loss = 0.002101550344377756
iteration 257, loss = 0.003769713221117854
iteration 258, loss = 0.0016164863482117653
iteration 259, loss = 0.0029203963931649923
iteration 260, loss = 0.00453324057161808
iteration 261, loss = 0.005009492859244347
iteration 262, loss = 0.0024508784990757704
iteration 263, loss = 0.004620785359293222
iteration 264, loss = 0.001862992998212576
iteration 265, loss = 0.002669527428224683
iteration 266, loss = 0.00268618599511683
iteration 267, loss = 0.002502326387912035
iteration 268, loss = 0.0018792904447764158
iteration 269, loss = 0.0019810195080935955
iteration 270, loss = 0.0023472134489566088
iteration 271, loss = 0.003132362151518464
iteration 272, loss = 0.0018848441541194916
iteration 273, loss = 0.002578403102234006
iteration 274, loss = 0.0036715087480843067
iteration 275, loss = 0.001963000511750579
iteration 276, loss = 0.001811472699046135
iteration 277, loss = 0.004779304377734661
iteration 278, loss = 0.0023604528978466988
iteration 279, loss = 0.001670562312938273
iteration 280, loss = 0.002344890497624874
iteration 281, loss = 0.0018188243266195059
iteration 282, loss = 0.0024655694141983986
iteration 283, loss = 0.001935550826601684
iteration 284, loss = 0.002100159414112568
iteration 285, loss = 0.0018736267229542136
iteration 286, loss = 0.0024896699469536543
iteration 287, loss = 0.0017712520202621818
iteration 288, loss = 0.0018500139703974128
iteration 289, loss = 0.0025183833204209805
iteration 290, loss = 0.002209955593571067
iteration 291, loss = 0.0017600731225684285
iteration 292, loss = 0.0019206354627385736
iteration 293, loss = 0.002093071583658457
iteration 294, loss = 0.0017374588642269373
iteration 295, loss = 0.0020394595339894295
iteration 296, loss = 0.001884200144559145
iteration 297, loss = 0.001871342770755291
iteration 298, loss = 0.002005819696933031
iteration 299, loss = 0.003543904982507229
iteration 300, loss = 0.0019918009638786316
iteration 1, loss = 0.0021831118501722813
iteration 2, loss = 0.0029181360732764006
iteration 3, loss = 0.001581895281560719
iteration 4, loss = 0.002244178205728531
iteration 5, loss = 0.0019942433573305607
iteration 6, loss = 0.002754039131104946
iteration 7, loss = 0.0021080186124891043
iteration 8, loss = 0.0018587306840345263
iteration 9, loss = 0.0025337140541523695
iteration 10, loss = 0.0017513869097456336
iteration 11, loss = 0.0023193845991045237
iteration 12, loss = 0.0020625798497349024
iteration 13, loss = 0.001956380670890212
iteration 14, loss = 0.001646749209612608
iteration 15, loss = 0.002759483177214861
iteration 16, loss = 0.0017350689740851521
iteration 17, loss = 0.0031960862688720226
iteration 18, loss = 0.0026025783736258745
iteration 19, loss = 0.0030978075228631496
iteration 20, loss = 0.001818732125684619
iteration 21, loss = 0.0023787887766957283
iteration 22, loss = 0.0023162325378507376
iteration 23, loss = 0.0020687454380095005
iteration 24, loss = 0.00180552969686687
iteration 25, loss = 0.0016438068123534322
iteration 26, loss = 0.002222155686467886
iteration 27, loss = 0.0016595933120697737
iteration 28, loss = 0.0020554792135953903
iteration 29, loss = 0.0016586107667535543
iteration 30, loss = 0.0020133168436586857
iteration 31, loss = 0.0020958201494067907
iteration 32, loss = 0.0018329337472096086
iteration 33, loss = 0.0022143633104860783
iteration 34, loss = 0.002793506719172001
iteration 35, loss = 0.002172899665310979
iteration 36, loss = 0.0018983042100444436
iteration 37, loss = 0.002421934623271227
iteration 38, loss = 0.0021774566266685724
iteration 39, loss = 0.001636254834011197
iteration 40, loss = 0.0019251159392297268
iteration 41, loss = 0.0030341690871864557
iteration 42, loss = 0.0017151539213955402
iteration 43, loss = 0.0025194331537932158
iteration 44, loss = 0.002747700782492757
iteration 45, loss = 0.004040522500872612
iteration 46, loss = 0.0022530099377036095
iteration 47, loss = 0.0019774348475039005
iteration 48, loss = 0.002273929538205266
iteration 49, loss = 0.0023667283821851015
iteration 50, loss = 0.0018632971914485097
iteration 51, loss = 0.0019786690827459097
iteration 52, loss = 0.002319739665836096
iteration 53, loss = 0.0023516470100730658
iteration 54, loss = 0.0016218130476772785
iteration 55, loss = 0.002449288032948971
iteration 56, loss = 0.002402712358161807
iteration 57, loss = 0.0022050391416996717
iteration 58, loss = 0.005132855847477913
iteration 59, loss = 0.002168399980291724
iteration 60, loss = 0.001974544720724225
iteration 61, loss = 0.002030615694820881
iteration 62, loss = 0.002298644743859768
iteration 63, loss = 0.0018317351350560784
iteration 64, loss = 0.0024260948412120342
iteration 65, loss = 0.002134857466444373
iteration 66, loss = 0.002942838938906789
iteration 67, loss = 0.0018476239638403058
iteration 68, loss = 0.002568068215623498
iteration 69, loss = 0.002802690491080284
iteration 70, loss = 0.0022175561171025038
iteration 71, loss = 0.0031588273122906685
iteration 72, loss = 0.0026866658590734005
iteration 73, loss = 0.003423742949962616
iteration 74, loss = 0.0015038815326988697
iteration 75, loss = 0.0019934976007789373
iteration 76, loss = 0.004539500921964645
iteration 77, loss = 0.0018598337192088366
iteration 78, loss = 0.0022969688288867474
iteration 79, loss = 0.0023803734220564365
iteration 80, loss = 0.0028352034278213978
iteration 81, loss = 0.0016773601528257132
iteration 82, loss = 0.002556370571255684
iteration 83, loss = 0.00185788341332227
iteration 84, loss = 0.002317744307219982
iteration 85, loss = 0.0037648107390850782
iteration 86, loss = 0.0016569315921515226
iteration 87, loss = 0.0036135874688625336
iteration 88, loss = 0.001661262707784772
iteration 89, loss = 0.0020940115209668875
iteration 90, loss = 0.002089437562972307
iteration 91, loss = 0.001973587553948164
iteration 92, loss = 0.002406087936833501
iteration 93, loss = 0.0019059448968619108
iteration 94, loss = 0.002150102285668254
iteration 95, loss = 0.0018418394029140472
iteration 96, loss = 0.0019494779407978058
iteration 97, loss = 0.002158597344532609
iteration 98, loss = 0.0018833951326087117
iteration 99, loss = 0.0020162619184702635
iteration 100, loss = 0.0020260915625840425
iteration 101, loss = 0.0017576732207089663
iteration 102, loss = 0.002261633053421974
iteration 103, loss = 0.001932711573317647
iteration 104, loss = 0.0019332473166286945
iteration 105, loss = 0.0023020710796117783
iteration 106, loss = 0.002585552167147398
iteration 107, loss = 0.0022127709817141294
iteration 108, loss = 0.0020430516451597214
iteration 109, loss = 0.0021524708718061447
iteration 110, loss = 0.003609322477132082
iteration 111, loss = 0.002177948597818613
iteration 112, loss = 0.0021919349674135447
iteration 113, loss = 0.0024785506539046764
iteration 114, loss = 0.0018383210990577936
iteration 115, loss = 0.002612421987578273
iteration 116, loss = 0.00544722517952323
iteration 117, loss = 0.0022296514362096786
iteration 118, loss = 0.0017743684584274888
iteration 119, loss = 0.0035741215106099844
iteration 120, loss = 0.00322335003875196
iteration 121, loss = 0.0021088924258947372
iteration 122, loss = 0.001983320340514183
iteration 123, loss = 0.001969004049897194
iteration 124, loss = 0.002370113506913185
iteration 125, loss = 0.0024184947833418846
iteration 126, loss = 0.0028927503153681755
iteration 127, loss = 0.004597815684974194
iteration 128, loss = 0.0020692013204097748
iteration 129, loss = 0.002171931555494666
iteration 130, loss = 0.003066023113206029
iteration 131, loss = 0.0017699836753308773
iteration 132, loss = 0.003318115370348096
iteration 133, loss = 0.002022883389145136
iteration 134, loss = 0.00166798522695899
iteration 135, loss = 0.001907442812807858
iteration 136, loss = 0.0019808150827884674
iteration 137, loss = 0.0019622750114649534
iteration 138, loss = 0.0036990151274949312
iteration 139, loss = 0.0027730276342481375
iteration 140, loss = 0.0017144548473879695
iteration 141, loss = 0.0018224776722490788
iteration 142, loss = 0.0020545991137623787
iteration 143, loss = 0.0019641933031380177
iteration 144, loss = 0.004458633251488209
iteration 145, loss = 0.002052953699603677
iteration 146, loss = 0.0021108167711645365
iteration 147, loss = 0.0017059750389307737
iteration 148, loss = 0.002235308289527893
iteration 149, loss = 0.0021372719202190638
iteration 150, loss = 0.002161921700462699
iteration 151, loss = 0.0022636232897639275
iteration 152, loss = 0.002017921768128872
iteration 153, loss = 0.0019285070011392236
iteration 154, loss = 0.0018713573226705194
iteration 155, loss = 0.0019342339364811778
iteration 156, loss = 0.0035193488001823425
iteration 157, loss = 0.0018350451719015837
iteration 158, loss = 0.002643442479893565
iteration 159, loss = 0.0019596319179981947
iteration 160, loss = 0.002028172602877021
iteration 161, loss = 0.001993573037907481
iteration 162, loss = 0.0018130672397091985
iteration 163, loss = 0.00202593463473022
iteration 164, loss = 0.0020049260929226875
iteration 165, loss = 0.0016050244448706508
iteration 166, loss = 0.0036929226480424404
iteration 167, loss = 0.0021811972837895155
iteration 168, loss = 0.002115993294864893
iteration 169, loss = 0.0020188121125102043
iteration 170, loss = 0.0019848740193992853
iteration 171, loss = 0.0017682717880234122
iteration 172, loss = 0.001977374777197838
iteration 173, loss = 0.0021210366394370794
iteration 174, loss = 0.0026024607941508293
iteration 175, loss = 0.0018161009065806866
iteration 176, loss = 0.0023763305507600307
iteration 177, loss = 0.0027215208392590284
iteration 178, loss = 0.0018975217826664448
iteration 179, loss = 0.002904655411839485
iteration 180, loss = 0.0025740060955286026
iteration 181, loss = 0.0034690559841692448
iteration 182, loss = 0.0020920003298670053
iteration 183, loss = 0.0021271128207445145
iteration 184, loss = 0.0022179950028657913
iteration 185, loss = 0.002008435083553195
iteration 186, loss = 0.0026103253476321697
iteration 187, loss = 0.002142607932910323
iteration 188, loss = 0.0018908424535766244
iteration 189, loss = 0.003338366746902466
iteration 190, loss = 0.001973791979253292
iteration 191, loss = 0.001938994275406003
iteration 192, loss = 0.0018561726901680231
iteration 193, loss = 0.00217045727185905
iteration 194, loss = 0.0016857879236340523
iteration 195, loss = 0.0020286631770431995
iteration 196, loss = 0.0024755410850048065
iteration 197, loss = 0.003632688894867897
iteration 198, loss = 0.001692154910415411
iteration 199, loss = 0.0021045678295195103
iteration 200, loss = 0.0016605299897491932
iteration 201, loss = 0.002222138224169612
iteration 202, loss = 0.003915646579116583
iteration 203, loss = 0.0017765886150300503
iteration 204, loss = 0.0027847150340676308
iteration 205, loss = 0.0031346026808023453
iteration 206, loss = 0.0021036979742348194
iteration 207, loss = 0.0018355885986238718
iteration 208, loss = 0.0017407459672540426
iteration 209, loss = 0.001965728821232915
iteration 210, loss = 0.0018074143445119262
iteration 211, loss = 0.002277611056342721
iteration 212, loss = 0.0019226899603381753
iteration 213, loss = 0.0023961563128978014
iteration 214, loss = 0.004197587259113789
iteration 215, loss = 0.001679004984907806
iteration 216, loss = 0.002104815561324358
iteration 217, loss = 0.002178570721298456
iteration 218, loss = 0.002105953171849251
iteration 219, loss = 0.002421041950583458
iteration 220, loss = 0.0019886591471731663
iteration 221, loss = 0.003127143019810319
iteration 222, loss = 0.0021979438606649637
iteration 223, loss = 0.002100710989907384
iteration 224, loss = 0.002490042243152857
iteration 225, loss = 0.0017693451372906566
iteration 226, loss = 0.0018423600122332573
iteration 227, loss = 0.0037641481030732393
iteration 228, loss = 0.00455588661134243
iteration 229, loss = 0.0023660650476813316
iteration 230, loss = 0.0016793070826679468
iteration 231, loss = 0.001754719647578895
iteration 232, loss = 0.0016299624694511294
iteration 233, loss = 0.0023507392033934593
iteration 234, loss = 0.0021337568759918213
iteration 235, loss = 0.001965738134458661
iteration 236, loss = 0.003991673234850168
iteration 237, loss = 0.002380248624831438
iteration 238, loss = 0.0018647753167897463
iteration 239, loss = 0.0022383469622582197
iteration 240, loss = 0.0019329230999574065
iteration 241, loss = 0.002550671109929681
iteration 242, loss = 0.002730837557464838
iteration 243, loss = 0.0026545883156359196
iteration 244, loss = 0.0023473105393350124
iteration 245, loss = 0.0019974817987531424
iteration 246, loss = 0.0020763780921697617
iteration 247, loss = 0.001795695279724896
iteration 248, loss = 0.0038002857472747564
iteration 249, loss = 0.0015840178821235895
iteration 250, loss = 0.002383684040978551
iteration 251, loss = 0.0019271166529506445
iteration 252, loss = 0.002374076982960105
iteration 253, loss = 0.0017668057698756456
iteration 254, loss = 0.0016990808071568608
iteration 255, loss = 0.001964835450053215
iteration 256, loss = 0.0022720734123140574
iteration 257, loss = 0.002167833736166358
iteration 258, loss = 0.002122818026691675
iteration 259, loss = 0.001835361705161631
iteration 260, loss = 0.0026092533953487873
iteration 261, loss = 0.0020126590970903635
iteration 262, loss = 0.0017405153485015035
iteration 263, loss = 0.0025750810746103525
iteration 264, loss = 0.0022125705145299435
iteration 265, loss = 0.0018067776691168547
iteration 266, loss = 0.002716961083933711
iteration 267, loss = 0.0017834330210462213
iteration 268, loss = 0.0022098273038864136
iteration 269, loss = 0.0017534970538690686
iteration 270, loss = 0.0018844587029889226
iteration 271, loss = 0.0018504717154428363
iteration 272, loss = 0.002336721634492278
iteration 273, loss = 0.002082719001919031
iteration 274, loss = 0.0031904829666018486
iteration 275, loss = 0.0017194922547787428
iteration 276, loss = 0.0019348931964486837
iteration 277, loss = 0.002010117284953594
iteration 278, loss = 0.002636492718011141
iteration 279, loss = 0.002721995348110795
iteration 280, loss = 0.002422118093818426
iteration 281, loss = 0.0018631282728165388
iteration 282, loss = 0.0018629205878823996
iteration 283, loss = 0.0017691546818241477
iteration 284, loss = 0.001968264812603593
iteration 285, loss = 0.0027328194119036198
iteration 286, loss = 0.0027191899716854095
iteration 287, loss = 0.0018275768961757421
iteration 288, loss = 0.002346981083974242
iteration 289, loss = 0.002136112190783024
iteration 290, loss = 0.0018761822720989585
iteration 291, loss = 0.0018549746600911021
iteration 292, loss = 0.0021041538566350937
iteration 293, loss = 0.0028041682671755552
iteration 294, loss = 0.0019189075101166964
iteration 295, loss = 0.0022921361960470676
iteration 296, loss = 0.001839572680182755
iteration 297, loss = 0.0035390271805226803
iteration 298, loss = 0.0026973807252943516
iteration 299, loss = 0.0027824961580336094
iteration 300, loss = 0.0021695788018405437
iteration 1, loss = 0.0024200293701142073
iteration 2, loss = 0.002416370902210474
iteration 3, loss = 0.0024240571074187756
iteration 4, loss = 0.0022390480153262615
iteration 5, loss = 0.001869427738711238
iteration 6, loss = 0.0019893681164830923
iteration 7, loss = 0.002111585810780525
iteration 8, loss = 0.003890336025506258
iteration 9, loss = 0.002292018383741379
iteration 10, loss = 0.002091525588184595
iteration 11, loss = 0.0016220323741436005
iteration 12, loss = 0.0025275805965065956
iteration 13, loss = 0.0017587339971214533
iteration 14, loss = 0.0018603692296892405
iteration 15, loss = 0.0028549586422741413
iteration 16, loss = 0.002531678881496191
iteration 17, loss = 0.0037115896120667458
iteration 18, loss = 0.0021960933227092028
iteration 19, loss = 0.001836970797739923
iteration 20, loss = 0.0017470534658059478
iteration 21, loss = 0.004301737993955612
iteration 22, loss = 0.001945281052030623
iteration 23, loss = 0.0018442600267007947
iteration 24, loss = 0.004667953122407198
iteration 25, loss = 0.002040495164692402
iteration 26, loss = 0.0018468557391315699
iteration 27, loss = 0.001959600020200014
iteration 28, loss = 0.0018467748304829001
iteration 29, loss = 0.0019327241461724043
iteration 30, loss = 0.0019968014676123857
iteration 31, loss = 0.0020313484128564596
iteration 32, loss = 0.002851748839020729
iteration 33, loss = 0.00339247053489089
iteration 34, loss = 0.002049257978796959
iteration 35, loss = 0.0028748465701937675
iteration 36, loss = 0.002156174974516034
iteration 37, loss = 0.0018366267904639244
iteration 38, loss = 0.002060155849903822
iteration 39, loss = 0.001989026553928852
iteration 40, loss = 0.0017638503341004252
iteration 41, loss = 0.0020576019305735826
iteration 42, loss = 0.0022372533567249775
iteration 43, loss = 0.0017711393302306533
iteration 44, loss = 0.0018294332548975945
iteration 45, loss = 0.00218582758679986
iteration 46, loss = 0.0018894111271947622
iteration 47, loss = 0.0018722000531852245
iteration 48, loss = 0.0016157024074345827
iteration 49, loss = 0.00218750792555511
iteration 50, loss = 0.0019086230313405395
iteration 51, loss = 0.002013909164816141
iteration 52, loss = 0.0018687364645302296
iteration 53, loss = 0.0027041651774197817
iteration 54, loss = 0.0022120012436062098
iteration 55, loss = 0.0020758467726409435
iteration 56, loss = 0.0031817900016903877
iteration 57, loss = 0.002125713275745511
iteration 58, loss = 0.004534655250608921
iteration 59, loss = 0.001658691093325615
iteration 60, loss = 0.0018009060295298696
iteration 61, loss = 0.0018263058736920357
iteration 62, loss = 0.0017993866931647062
iteration 63, loss = 0.002013011835515499
iteration 64, loss = 0.002184620127081871
iteration 65, loss = 0.004717633593827486
iteration 66, loss = 0.001953989267349243
iteration 67, loss = 0.0018328184960409999
iteration 68, loss = 0.0017577431863173842
iteration 69, loss = 0.0019009977113455534
iteration 70, loss = 0.002716406248509884
iteration 71, loss = 0.002802264178171754
iteration 72, loss = 0.0018016373505815864
iteration 73, loss = 0.002566759241744876
iteration 74, loss = 0.0018886214820668101
iteration 75, loss = 0.002020336454734206
iteration 76, loss = 0.0016250128392130136
iteration 77, loss = 0.0018970526289194822
iteration 78, loss = 0.0023166234605014324
iteration 79, loss = 0.002184286480769515
iteration 80, loss = 0.002343117957934737
iteration 81, loss = 0.0017462832620367408
iteration 82, loss = 0.0017457568319514394
iteration 83, loss = 0.001710016978904605
iteration 84, loss = 0.002118724165484309
iteration 85, loss = 0.002001906745135784
iteration 86, loss = 0.002377802040427923
iteration 87, loss = 0.001585404621437192
iteration 88, loss = 0.0015939546283334494
iteration 89, loss = 0.0021353622432798147
iteration 90, loss = 0.0026069246232509613
iteration 91, loss = 0.0018997942097485065
iteration 92, loss = 0.002552282065153122
iteration 93, loss = 0.001757800462655723
iteration 94, loss = 0.0030224910005927086
iteration 95, loss = 0.0020676597487181425
iteration 96, loss = 0.0021005752496421337
iteration 97, loss = 0.0017701847245916724
iteration 98, loss = 0.0029920556116849184
iteration 99, loss = 0.0034151545260101557
iteration 100, loss = 0.0020755967125296593
iteration 101, loss = 0.0028741457499563694
iteration 102, loss = 0.004171284381300211
iteration 103, loss = 0.0029758557211607695
iteration 104, loss = 0.00368828559294343
iteration 105, loss = 0.003928254824131727
iteration 106, loss = 0.0020491331815719604
iteration 107, loss = 0.0034618093632161617
iteration 108, loss = 0.0017832075245678425
iteration 109, loss = 0.003606529673561454
iteration 110, loss = 0.0023120655678212643
iteration 111, loss = 0.002167799510061741
iteration 112, loss = 0.0021250639110803604
iteration 113, loss = 0.0024460465647280216
iteration 114, loss = 0.001960044028237462
iteration 115, loss = 0.0018764941487461329
iteration 116, loss = 0.003187918569892645
iteration 117, loss = 0.001956454012542963
iteration 118, loss = 0.00216024462133646
iteration 119, loss = 0.0023879811633378267
iteration 120, loss = 0.0032544019632041454
iteration 121, loss = 0.001744949957355857
iteration 122, loss = 0.0019347828347235918
iteration 123, loss = 0.0019341694423928857
iteration 124, loss = 0.0019545007962733507
iteration 125, loss = 0.0018030639039352536
iteration 126, loss = 0.002771723549813032
iteration 127, loss = 0.002209156984463334
iteration 128, loss = 0.002236707368865609
iteration 129, loss = 0.002019288716837764
iteration 130, loss = 0.0019696371164172888
iteration 131, loss = 0.003829091554507613
iteration 132, loss = 0.002197910100221634
iteration 133, loss = 0.0021175129804760218
iteration 134, loss = 0.0029447446577250957
iteration 135, loss = 0.002827208023518324
iteration 136, loss = 0.002510134829208255
iteration 137, loss = 0.002153873210772872
iteration 138, loss = 0.0019710734486579895
iteration 139, loss = 0.003281490644440055
iteration 140, loss = 0.0017004209803417325
iteration 141, loss = 0.001723882625810802
iteration 142, loss = 0.0023555858060717583
iteration 143, loss = 0.003225386142730713
iteration 144, loss = 0.0022573985625058413
iteration 145, loss = 0.002807692624628544
iteration 146, loss = 0.0018252991139888763
iteration 147, loss = 0.004164054058492184
iteration 148, loss = 0.0021960812155157328
iteration 149, loss = 0.002261179033666849
iteration 150, loss = 0.002077110344544053
iteration 151, loss = 0.0026803475338965654
iteration 152, loss = 0.00211260374635458
iteration 153, loss = 0.002523656003177166
iteration 154, loss = 0.0017239167355000973
iteration 155, loss = 0.001931151025928557
iteration 156, loss = 0.003312734654173255
iteration 157, loss = 0.001747163594700396
iteration 158, loss = 0.0026560830883681774
iteration 159, loss = 0.0018023010343313217
iteration 160, loss = 0.0021294469479471445
iteration 161, loss = 0.001710306154564023
iteration 162, loss = 0.002104884944856167
iteration 163, loss = 0.0018603154458105564
iteration 164, loss = 0.0022034570574760437
iteration 165, loss = 0.0019866093061864376
iteration 166, loss = 0.0019229991594329476
iteration 167, loss = 0.0030820840038359165
iteration 168, loss = 0.003570043481886387
iteration 169, loss = 0.0016890924889594316
iteration 170, loss = 0.001872829394415021
iteration 171, loss = 0.0018427938921377063
iteration 172, loss = 0.0017226581694558263
iteration 173, loss = 0.0016289283521473408
iteration 174, loss = 0.002461760537698865
iteration 175, loss = 0.0019494150765240192
iteration 176, loss = 0.002251898404210806
iteration 177, loss = 0.002217027824372053
iteration 178, loss = 0.001942698610946536
iteration 179, loss = 0.001698959618806839
iteration 180, loss = 0.0020725452341139317
iteration 181, loss = 0.002093384275212884
iteration 182, loss = 0.0018418192630633712
iteration 183, loss = 0.00455772690474987
iteration 184, loss = 0.002090411726385355
iteration 185, loss = 0.0018562122713774443
iteration 186, loss = 0.0019055369775742292
iteration 187, loss = 0.002534922445192933
iteration 188, loss = 0.0022831326350569725
iteration 189, loss = 0.0018972695106640458
iteration 190, loss = 0.0024514293763786554
iteration 191, loss = 0.0032475977204740047
iteration 192, loss = 0.0021653794683516026
iteration 193, loss = 0.0018851556815207005
iteration 194, loss = 0.0015373175265267491
iteration 195, loss = 0.0018069894285872579
iteration 196, loss = 0.0021880739368498325
iteration 197, loss = 0.0017234429251402617
iteration 198, loss = 0.0020511415787041187
iteration 199, loss = 0.002329349983483553
iteration 200, loss = 0.002049617934972048
iteration 201, loss = 0.00145496625918895
iteration 202, loss = 0.0024478854611516
iteration 203, loss = 0.0020777794998139143
iteration 204, loss = 0.002464567543938756
iteration 205, loss = 0.0022208523005247116
iteration 206, loss = 0.0017201052978634834
iteration 207, loss = 0.0021399669349193573
iteration 208, loss = 0.002007504925131798
iteration 209, loss = 0.0018321604002267122
iteration 210, loss = 0.003745873924344778
iteration 211, loss = 0.0016835229471325874
iteration 212, loss = 0.0021459353156387806
iteration 213, loss = 0.0017399624921381474
iteration 214, loss = 0.0018079848960042
iteration 215, loss = 0.0018730608280748129
iteration 216, loss = 0.001813543844036758
iteration 217, loss = 0.002566332696005702
iteration 218, loss = 0.004082865081727505
iteration 219, loss = 0.002208423800766468
iteration 220, loss = 0.0019374064868316054
iteration 221, loss = 0.0022605794947594404
iteration 222, loss = 0.0021785320714116096
iteration 223, loss = 0.0017513325437903404
iteration 224, loss = 0.0016430951654911041
iteration 225, loss = 0.0018868765328079462
iteration 226, loss = 0.0029075073543936014
iteration 227, loss = 0.0020223164465278387
iteration 228, loss = 0.00233886088244617
iteration 229, loss = 0.002727050334215164
iteration 230, loss = 0.0033275310415774584
iteration 231, loss = 0.0021608504466712475
iteration 232, loss = 0.0020057836081832647
iteration 233, loss = 0.0024651261046528816
iteration 234, loss = 0.002146594924852252
iteration 235, loss = 0.0038936191704124212
iteration 236, loss = 0.0018627257086336613
iteration 237, loss = 0.001973705133423209
iteration 238, loss = 0.0018908875063061714
iteration 239, loss = 0.0032549917232245207
iteration 240, loss = 0.00199151411652565
iteration 241, loss = 0.0019905483350157738
iteration 242, loss = 0.001779083046130836
iteration 243, loss = 0.0022246239241212606
iteration 244, loss = 0.002249186858534813
iteration 245, loss = 0.00191026262473315
iteration 246, loss = 0.0022174224723130465
iteration 247, loss = 0.002527986653149128
iteration 248, loss = 0.002861934481188655
iteration 249, loss = 0.0022831279784440994
iteration 250, loss = 0.001954011619091034
iteration 251, loss = 0.002264544600620866
iteration 252, loss = 0.0026975818909704685
iteration 253, loss = 0.002101316349580884
iteration 254, loss = 0.0019120319047942758
iteration 255, loss = 0.0018651029095053673
iteration 256, loss = 0.001903638825751841
iteration 257, loss = 0.001784063410013914
iteration 258, loss = 0.0027452358044683933
iteration 259, loss = 0.003930761944502592
iteration 260, loss = 0.002073198091238737
iteration 261, loss = 0.0017447156133130193
iteration 262, loss = 0.002347528235986829
iteration 263, loss = 0.002218782901763916
iteration 264, loss = 0.0016763415187597275
iteration 265, loss = 0.0017844882095232606
iteration 266, loss = 0.0020571912173181772
iteration 267, loss = 0.0017024797853082418
iteration 268, loss = 0.002429486718028784
iteration 269, loss = 0.002373359864577651
iteration 270, loss = 0.002221478149294853
iteration 271, loss = 0.0036765397526323795
iteration 272, loss = 0.0016833065310493112
iteration 273, loss = 0.002226003911346197
iteration 274, loss = 0.0026819028425961733
iteration 275, loss = 0.003656524233520031
iteration 276, loss = 0.0024779431987553835
iteration 277, loss = 0.0018333063926547766
iteration 278, loss = 0.002093423390761018
iteration 279, loss = 0.002015979029238224
iteration 280, loss = 0.003340631490573287
iteration 281, loss = 0.0020752085838466883
iteration 282, loss = 0.002265250775963068
iteration 283, loss = 0.0020802412182092667
iteration 284, loss = 0.0022169386502355337
iteration 285, loss = 0.001622361014597118
iteration 286, loss = 0.002505721990019083
iteration 287, loss = 0.0033338339999318123
iteration 288, loss = 0.0022087793331593275
iteration 289, loss = 0.002110806293785572
iteration 290, loss = 0.00213543139398098
iteration 291, loss = 0.0024722618982195854
iteration 292, loss = 0.0028828447684645653
iteration 293, loss = 0.001810857909731567
iteration 294, loss = 0.0028614308685064316
iteration 295, loss = 0.0025269081816077232
iteration 296, loss = 0.0036610786337405443
iteration 297, loss = 0.0019182213582098484
iteration 298, loss = 0.0022496054880321026
iteration 299, loss = 0.0029240832664072514
iteration 300, loss = 0.0018130306852981448
iteration 1, loss = 0.0021520154550671577
iteration 2, loss = 0.002229375531896949
iteration 3, loss = 0.0022471053525805473
iteration 4, loss = 0.002051334362477064
iteration 5, loss = 0.002057510893791914
iteration 6, loss = 0.0019415246788412333
iteration 7, loss = 0.0028658476658165455
iteration 8, loss = 0.0017411798471584916
iteration 9, loss = 0.002365418942645192
iteration 10, loss = 0.0020756302401423454
iteration 11, loss = 0.002111438661813736
iteration 12, loss = 0.0022414682898670435
iteration 13, loss = 0.0023204225581139326
iteration 14, loss = 0.0022609822917729616
iteration 15, loss = 0.002081015147268772
iteration 16, loss = 0.002457587979733944
iteration 17, loss = 0.001816361676901579
iteration 18, loss = 0.0024666704703122377
iteration 19, loss = 0.0021506501361727715
iteration 20, loss = 0.0019300752319395542
iteration 21, loss = 0.002140556462109089
iteration 22, loss = 0.0017728672828525305
iteration 23, loss = 0.002357775578275323
iteration 24, loss = 0.001960478723049164
iteration 25, loss = 0.004039254039525986
iteration 26, loss = 0.0019315701210871339
iteration 27, loss = 0.002016417682170868
iteration 28, loss = 0.0037940533366054296
iteration 29, loss = 0.0018952247919514775
iteration 30, loss = 0.002733343280851841
iteration 31, loss = 0.0031038024462759495
iteration 32, loss = 0.002496201079338789
iteration 33, loss = 0.003416485385969281
iteration 34, loss = 0.0018988113151863217
iteration 35, loss = 0.0023467889986932278
iteration 36, loss = 0.0027440558187663555
iteration 37, loss = 0.003126947907730937
iteration 38, loss = 0.0036373049952089787
iteration 39, loss = 0.0018299256917089224
iteration 40, loss = 0.0025645564310252666
iteration 41, loss = 0.001983934547752142
iteration 42, loss = 0.0020736504811793566
iteration 43, loss = 0.0020888939034193754
iteration 44, loss = 0.002125360071659088
iteration 45, loss = 0.002138311741873622
iteration 46, loss = 0.0017520919209346175
iteration 47, loss = 0.0019177886424586177
iteration 48, loss = 0.0020390173885971308
iteration 49, loss = 0.002016916638240218
iteration 50, loss = 0.0020606869366019964
iteration 51, loss = 0.0020671403035521507
iteration 52, loss = 0.003965130541473627
iteration 53, loss = 0.001993795158341527
iteration 54, loss = 0.0019840120803564787
iteration 55, loss = 0.002300887368619442
iteration 56, loss = 0.0037772823125123978
iteration 57, loss = 0.00263719679787755
iteration 58, loss = 0.0018219157354906201
iteration 59, loss = 0.0019196206703782082
iteration 60, loss = 0.003398163476958871
iteration 61, loss = 0.0022236015647649765
iteration 62, loss = 0.0021756121423095465
iteration 63, loss = 0.0019713668152689934
iteration 64, loss = 0.0021331659518182278
iteration 65, loss = 0.0016879807226359844
iteration 66, loss = 0.0023287555668503046
iteration 67, loss = 0.002193389693275094
iteration 68, loss = 0.0020025745034217834
iteration 69, loss = 0.0037420792505145073
iteration 70, loss = 0.0026174765080213547
iteration 71, loss = 0.0023249462246894836
iteration 72, loss = 0.002074918942525983
iteration 73, loss = 0.003331455634906888
iteration 74, loss = 0.0017444968689233065
iteration 75, loss = 0.0036139683797955513
iteration 76, loss = 0.00257713720202446
iteration 77, loss = 0.003275939030572772
iteration 78, loss = 0.005647755693644285
iteration 79, loss = 0.0027098662685602903
iteration 80, loss = 0.0020285057835280895
iteration 81, loss = 0.0018321792595088482
iteration 82, loss = 0.0022971255239099264
iteration 83, loss = 0.002161989687010646
iteration 84, loss = 0.001671775127761066
iteration 85, loss = 0.0019053840078413486
iteration 86, loss = 0.002030604053288698
iteration 87, loss = 0.0022184643894433975
iteration 88, loss = 0.0020246640779078007
iteration 89, loss = 0.001966430339962244
iteration 90, loss = 0.0019762725569307804
iteration 91, loss = 0.0024210503324866295
iteration 92, loss = 0.0019460819894447923
iteration 93, loss = 0.002379069337621331
iteration 94, loss = 0.0019573664758354425
iteration 95, loss = 0.001882878364995122
iteration 96, loss = 0.0027056343387812376
iteration 97, loss = 0.001969453413039446
iteration 98, loss = 0.0019029208924621344
iteration 99, loss = 0.0021505162585526705
iteration 100, loss = 0.0018689349526539445
iteration 101, loss = 0.001882754499092698
iteration 102, loss = 0.00174448371399194
iteration 103, loss = 0.0031428008805960417
iteration 104, loss = 0.001822751248255372
iteration 105, loss = 0.0022219896782189608
iteration 106, loss = 0.0018876953981816769
iteration 107, loss = 0.002629105933010578
iteration 108, loss = 0.002035636454820633
iteration 109, loss = 0.0037402231246232986
iteration 110, loss = 0.0017496265936642885
iteration 111, loss = 0.003316330024972558
iteration 112, loss = 0.0023703943006694317
iteration 113, loss = 0.0020404078532010317
iteration 114, loss = 0.002233971608802676
iteration 115, loss = 0.0016355810221284628
iteration 116, loss = 0.0016216610092669725
iteration 117, loss = 0.001931721344590187
iteration 118, loss = 0.004751716274768114
iteration 119, loss = 0.0016727758338674903
iteration 120, loss = 0.0023260489106178284
iteration 121, loss = 0.0017911677714437246
iteration 122, loss = 0.0025896832812577486
iteration 123, loss = 0.001992828445509076
iteration 124, loss = 0.002030455507338047
iteration 125, loss = 0.0036254585720598698
iteration 126, loss = 0.0032937703654170036
iteration 127, loss = 0.0019531503785401583
iteration 128, loss = 0.0016992435557767749
iteration 129, loss = 0.0024220463819801807
iteration 130, loss = 0.0017632071394473314
iteration 131, loss = 0.00215831957757473
iteration 132, loss = 0.0023675584234297276
iteration 133, loss = 0.00324263540096581
iteration 134, loss = 0.003072346094995737
iteration 135, loss = 0.002316389000043273
iteration 136, loss = 0.0019394645933061838
iteration 137, loss = 0.0021354383789002895
iteration 138, loss = 0.003951972350478172
iteration 139, loss = 0.002181924879550934
iteration 140, loss = 0.0018882285803556442
iteration 141, loss = 0.0018065916374325752
iteration 142, loss = 0.0020467382855713367
iteration 143, loss = 0.0020412495359778404
iteration 144, loss = 0.0019393914844840765
iteration 145, loss = 0.0022025841753929853
iteration 146, loss = 0.0021241181530058384
iteration 147, loss = 0.0022242481354624033
iteration 148, loss = 0.0017937548691406846
iteration 149, loss = 0.0025115166790783405
iteration 150, loss = 0.0025585321709513664
iteration 151, loss = 0.002038366859778762
iteration 152, loss = 0.0017428025603294373
iteration 153, loss = 0.0021734233014285564
iteration 154, loss = 0.002303390298038721
iteration 155, loss = 0.0021716912742704153
iteration 156, loss = 0.002003517234697938
iteration 157, loss = 0.0021506675984710455
iteration 158, loss = 0.0018745248671621084
iteration 159, loss = 0.002535817213356495
iteration 160, loss = 0.0017999173142015934
iteration 161, loss = 0.0037632144521921873
iteration 162, loss = 0.0023378287442028522
iteration 163, loss = 0.001962144859135151
iteration 164, loss = 0.0023231685627251863
iteration 165, loss = 0.0017892398172989488
iteration 166, loss = 0.0023543983697891235
iteration 167, loss = 0.002458005677908659
iteration 168, loss = 0.0022287205792963505
iteration 169, loss = 0.001840152544900775
iteration 170, loss = 0.0032476410269737244
iteration 171, loss = 0.002397930948063731
iteration 172, loss = 0.0018122829496860504
iteration 173, loss = 0.0019757733680307865
iteration 174, loss = 0.001581587828695774
iteration 175, loss = 0.002030514646321535
iteration 176, loss = 0.002376033691689372
iteration 177, loss = 0.004654527176171541
iteration 178, loss = 0.0018526054918766022
iteration 179, loss = 0.0017617313424125314
iteration 180, loss = 0.0018799507524818182
iteration 181, loss = 0.0022353327367454767
iteration 182, loss = 0.0018429587362334132
iteration 183, loss = 0.00205795606598258
iteration 184, loss = 0.0015819418476894498
iteration 185, loss = 0.0017935363575816154
iteration 186, loss = 0.0020377226173877716
iteration 187, loss = 0.0016715872334316373
iteration 188, loss = 0.0023130131885409355
iteration 189, loss = 0.001765853725373745
iteration 190, loss = 0.002040425082668662
iteration 191, loss = 0.002429407322779298
iteration 192, loss = 0.002051709918305278
iteration 193, loss = 0.0025616290513426065
iteration 194, loss = 0.0020325437653809786
iteration 195, loss = 0.0022421686444431543
iteration 196, loss = 0.0022417984437197447
iteration 197, loss = 0.0028725385200232267
iteration 198, loss = 0.0023676182609051466
iteration 199, loss = 0.003153125988319516
iteration 200, loss = 0.0018899431452155113
iteration 201, loss = 0.003785565495491028
iteration 202, loss = 0.001979961758479476
iteration 203, loss = 0.0027137247379869223
iteration 204, loss = 0.002241863403469324
iteration 205, loss = 0.0016415123827755451
iteration 206, loss = 0.0034155510365962982
iteration 207, loss = 0.002399966586381197
iteration 208, loss = 0.00184512953273952
iteration 209, loss = 0.003565809689462185
iteration 210, loss = 0.0021387487649917603
iteration 211, loss = 0.0018199717160314322
iteration 212, loss = 0.0030079642310738564
iteration 213, loss = 0.002163501223549247
iteration 214, loss = 0.0016347869532182813
iteration 215, loss = 0.001884176512248814
iteration 216, loss = 0.0019517482724040747
iteration 217, loss = 0.0032399899791926146
iteration 218, loss = 0.0019230757607147098
iteration 219, loss = 0.0021126829087734222
iteration 220, loss = 0.002160273026674986
iteration 221, loss = 0.003415000857785344
iteration 222, loss = 0.0017337454482913017
iteration 223, loss = 0.001991305500268936
iteration 224, loss = 0.0033640223555266857
iteration 225, loss = 0.0031397249549627304
iteration 226, loss = 0.001786037697456777
iteration 227, loss = 0.0016768331406638026
iteration 228, loss = 0.0021156235598027706
iteration 229, loss = 0.0021723839454352856
iteration 230, loss = 0.0019020469626411796
iteration 231, loss = 0.0018640621565282345
iteration 232, loss = 0.0021169239189475775
iteration 233, loss = 0.001438107923604548
iteration 234, loss = 0.0031348031479865313
iteration 235, loss = 0.0025845328345894814
iteration 236, loss = 0.001862174249254167
iteration 237, loss = 0.001924334210343659
iteration 238, loss = 0.0017093635397031903
iteration 239, loss = 0.00235943915322423
iteration 240, loss = 0.0018310562008991838
iteration 241, loss = 0.0017753413412719965
iteration 242, loss = 0.003759002313017845
iteration 243, loss = 0.0017606322653591633
iteration 244, loss = 0.002879522042348981
iteration 245, loss = 0.0017495239153504372
iteration 246, loss = 0.0020718537271022797
iteration 247, loss = 0.0018245110986754298
iteration 248, loss = 0.001489608664996922
iteration 249, loss = 0.001842194120399654
iteration 250, loss = 0.0027498884592205286
iteration 251, loss = 0.001827245345339179
iteration 252, loss = 0.0020231495145708323
iteration 253, loss = 0.0018159366445615888
iteration 254, loss = 0.001779093174263835
iteration 255, loss = 0.0018848696490749717
iteration 256, loss = 0.0026149556506425142
iteration 257, loss = 0.0017886781133711338
iteration 258, loss = 0.002222527517005801
iteration 259, loss = 0.002238115295767784
iteration 260, loss = 0.0021398081444203854
iteration 261, loss = 0.0024357486981898546
iteration 262, loss = 0.002160634147003293
iteration 263, loss = 0.0021369755268096924
iteration 264, loss = 0.0019233886850997806
iteration 265, loss = 0.0024544009938836098
iteration 266, loss = 0.0017794749001041055
iteration 267, loss = 0.001939072273671627
iteration 268, loss = 0.0019806711934506893
iteration 269, loss = 0.002014185767620802
iteration 270, loss = 0.002460133284330368
iteration 271, loss = 0.0019651404581964016
iteration 272, loss = 0.002308010822162032
iteration 273, loss = 0.0026758958119899035
iteration 274, loss = 0.0017380063654854894
iteration 275, loss = 0.0017367741093039513
iteration 276, loss = 0.0018910389626398683
iteration 277, loss = 0.0017386124236509204
iteration 278, loss = 0.0018885578028857708
iteration 279, loss = 0.002095649251714349
iteration 280, loss = 0.0018329413142055273
iteration 281, loss = 0.0017154526431113482
iteration 282, loss = 0.003220951184630394
iteration 283, loss = 0.00213179481215775
iteration 284, loss = 0.0017811546567827463
iteration 285, loss = 0.002020393731072545
iteration 286, loss = 0.0028606392443180084
iteration 287, loss = 0.0032312003895640373
iteration 288, loss = 0.0018922925228253007
iteration 289, loss = 0.0028916916344314814
iteration 290, loss = 0.0020195436663925648
iteration 291, loss = 0.002843338530510664
iteration 292, loss = 0.0020486577413976192
iteration 293, loss = 0.0041390107944607735
iteration 294, loss = 0.0018105944618582726
iteration 295, loss = 0.0026337855961173773
iteration 296, loss = 0.0018641198985278606
iteration 297, loss = 0.00574082974344492
iteration 298, loss = 0.001950362347997725
iteration 299, loss = 0.0024495988618582487
iteration 300, loss = 0.001859990763477981
iteration 1, loss = 0.002042063046246767
iteration 2, loss = 0.0019570181611925364
iteration 3, loss = 0.0032187295146286488
iteration 4, loss = 0.002380181336775422
iteration 5, loss = 0.0024592988193035126
iteration 6, loss = 0.0019727214239537716
iteration 7, loss = 0.0016630348982289433
iteration 8, loss = 0.00233922665938735
iteration 9, loss = 0.002121893921867013
iteration 10, loss = 0.0018498236313462257
iteration 11, loss = 0.001677213585935533
iteration 12, loss = 0.0022808655630797148
iteration 13, loss = 0.0031688339076936245
iteration 14, loss = 0.001635792781598866
iteration 15, loss = 0.004143440630286932
iteration 16, loss = 0.002076883800327778
iteration 17, loss = 0.0029554730281233788
iteration 18, loss = 0.0018091669771820307
iteration 19, loss = 0.002057379111647606
iteration 20, loss = 0.002011961303651333
iteration 21, loss = 0.002131800865754485
iteration 22, loss = 0.001860251184552908
iteration 23, loss = 0.0022657187655568123
iteration 24, loss = 0.004330641124397516
iteration 25, loss = 0.0020375638268887997
iteration 26, loss = 0.0021404654253274202
iteration 27, loss = 0.0022248090244829655
iteration 28, loss = 0.0021606935188174248
iteration 29, loss = 0.0020346944220364094
iteration 30, loss = 0.0033160292077809572
iteration 31, loss = 0.0037612877786159515
iteration 32, loss = 0.002365555614233017
iteration 33, loss = 0.0020576503593474627
iteration 34, loss = 0.002808581106364727
iteration 35, loss = 0.0028766978066414595
iteration 36, loss = 0.0017204415053129196
iteration 37, loss = 0.0018640239723026752
iteration 38, loss = 0.002123570768162608
iteration 39, loss = 0.00245001632720232
iteration 40, loss = 0.0020617605186998844
iteration 41, loss = 0.0035063631366938353
iteration 42, loss = 0.0018638992914929986
iteration 43, loss = 0.0017391991568729281
iteration 44, loss = 0.002041954779997468
iteration 45, loss = 0.0029629592318087816
iteration 46, loss = 0.002026374451816082
iteration 47, loss = 0.0023269448429346085
iteration 48, loss = 0.0029695427510887384
iteration 49, loss = 0.002141376491636038
iteration 50, loss = 0.001786270528100431
iteration 51, loss = 0.0020058765076100826
iteration 52, loss = 0.0019360720179975033
iteration 53, loss = 0.0019057438476011157
iteration 54, loss = 0.004161548800766468
iteration 55, loss = 0.0024274801835417747
iteration 56, loss = 0.0018158177845180035
iteration 57, loss = 0.0020435822661966085
iteration 58, loss = 0.002084198174998164
iteration 59, loss = 0.0018605099758133292
iteration 60, loss = 0.0018979641608893871
iteration 61, loss = 0.0025459839962422848
iteration 62, loss = 0.0020489285234361887
iteration 63, loss = 0.0024662462528795004
iteration 64, loss = 0.0016593970358371735
iteration 65, loss = 0.0018909033387899399
iteration 66, loss = 0.0016786233754828572
iteration 67, loss = 0.0018032045336440206
iteration 68, loss = 0.001730027492158115
iteration 69, loss = 0.002551647834479809
iteration 70, loss = 0.001835758681409061
iteration 71, loss = 0.0029784860089421272
iteration 72, loss = 0.003120435867458582
iteration 73, loss = 0.0028708744794130325
iteration 74, loss = 0.001968085067346692
iteration 75, loss = 0.0033614530693739653
iteration 76, loss = 0.0019003155175596476
iteration 77, loss = 0.0018133162520825863
iteration 78, loss = 0.0019390806555747986
iteration 79, loss = 0.0018004750600084662
iteration 80, loss = 0.002074497053399682
iteration 81, loss = 0.0025089450646191835
iteration 82, loss = 0.002457893919199705
iteration 83, loss = 0.002195297973230481
iteration 84, loss = 0.0025599943473935127
iteration 85, loss = 0.0021092607639729977
iteration 86, loss = 0.0018210432026535273
iteration 87, loss = 0.0020428344141691923
iteration 88, loss = 0.0020061139948666096
iteration 89, loss = 0.0023362680803984404
iteration 90, loss = 0.0018632017308846116
iteration 91, loss = 0.002268417039886117
iteration 92, loss = 0.0017528163734823465
iteration 93, loss = 0.0017142195720225573
iteration 94, loss = 0.0017524599097669125
iteration 95, loss = 0.0018632059218361974
iteration 96, loss = 0.001747393631376326
iteration 97, loss = 0.0029372423887252808
iteration 98, loss = 0.0018595813307911158
iteration 99, loss = 0.0032465029507875443
iteration 100, loss = 0.0025997741613537073
iteration 101, loss = 0.0019275776576250792
iteration 102, loss = 0.001724862726405263
iteration 103, loss = 0.0018210085108876228
iteration 104, loss = 0.003378987079486251
iteration 105, loss = 0.0023432839661836624
iteration 106, loss = 0.002888286719098687
iteration 107, loss = 0.0017208777135238051
iteration 108, loss = 0.0016190868336707354
iteration 109, loss = 0.002605929970741272
iteration 110, loss = 0.00209133792668581
iteration 111, loss = 0.0016275070374831557
iteration 112, loss = 0.001698576263152063
iteration 113, loss = 0.0018449965864419937
iteration 114, loss = 0.00198515341617167
iteration 115, loss = 0.0026280686724931
iteration 116, loss = 0.003108763601630926
iteration 117, loss = 0.0024103373289108276
iteration 118, loss = 0.0019546563271433115
iteration 119, loss = 0.003976223524659872
iteration 120, loss = 0.0022186809219419956
iteration 121, loss = 0.003753999248147011
iteration 122, loss = 0.0024172435514628887
iteration 123, loss = 0.0027250591665506363
iteration 124, loss = 0.002258637920022011
iteration 125, loss = 0.0021006239112466574
iteration 126, loss = 0.0038160209078341722
iteration 127, loss = 0.0016647783340886235
iteration 128, loss = 0.0022859973832964897
iteration 129, loss = 0.002006496535614133
iteration 130, loss = 0.004577278159558773
iteration 131, loss = 0.0020675596315413713
iteration 132, loss = 0.002764733275398612
iteration 133, loss = 0.0026150057092309
iteration 134, loss = 0.0020618336275219917
iteration 135, loss = 0.0031439068261533976
iteration 136, loss = 0.002033946802839637
iteration 137, loss = 0.002037137746810913
iteration 138, loss = 0.0018387633608654141
iteration 139, loss = 0.0028926017694175243
iteration 140, loss = 0.0018454205710440874
iteration 141, loss = 0.0030443540308624506
iteration 142, loss = 0.0019983486272394657
iteration 143, loss = 0.002744111930951476
iteration 144, loss = 0.0017144351731985807
iteration 145, loss = 0.003907729405909777
iteration 146, loss = 0.0025135313626378775
iteration 147, loss = 0.0021270615980029106
iteration 148, loss = 0.0018502195598557591
iteration 149, loss = 0.002064203377813101
iteration 150, loss = 0.002059396356344223
iteration 151, loss = 0.002012555953115225
iteration 152, loss = 0.003359503112733364
iteration 153, loss = 0.003938916604965925
iteration 154, loss = 0.0018355671782046556
iteration 155, loss = 0.002130761742591858
iteration 156, loss = 0.0016300815623253584
iteration 157, loss = 0.0026782932691276073
iteration 158, loss = 0.0017091715708374977
iteration 159, loss = 0.001972730504348874
iteration 160, loss = 0.002707680221647024
iteration 161, loss = 0.0017792513826861978
iteration 162, loss = 0.0019075985765084624
iteration 163, loss = 0.0018174545839428902
iteration 164, loss = 0.0038310387171804905
iteration 165, loss = 0.0017654681578278542
iteration 166, loss = 0.0016947324620559812
iteration 167, loss = 0.0024089165963232517
iteration 168, loss = 0.0020575160160660744
iteration 169, loss = 0.0023508607409894466
iteration 170, loss = 0.002231234684586525
iteration 171, loss = 0.0021379059180617332
iteration 172, loss = 0.0025633862242102623
iteration 173, loss = 0.0025189178995788097
iteration 174, loss = 0.0022360789589583874
iteration 175, loss = 0.002013445133343339
iteration 176, loss = 0.002255613449960947
iteration 177, loss = 0.0017131245695054531
iteration 178, loss = 0.0019430335378274322
iteration 179, loss = 0.0020124688744544983
iteration 180, loss = 0.0019470455590635538
iteration 181, loss = 0.0029722414910793304
iteration 182, loss = 0.0032733248081058264
iteration 183, loss = 0.0020659896545112133
iteration 184, loss = 0.0020482309628278017
iteration 185, loss = 0.002159443683922291
iteration 186, loss = 0.0017364008817821741
iteration 187, loss = 0.0019669930916279554
iteration 188, loss = 0.001968845259398222
iteration 189, loss = 0.0029013659805059433
iteration 190, loss = 0.0017928601009771228
iteration 191, loss = 0.0020863006357103586
iteration 192, loss = 0.0018397053936496377
iteration 193, loss = 0.0018915316322818398
iteration 194, loss = 0.0020624371245503426
iteration 195, loss = 0.0028514466248452663
iteration 196, loss = 0.0018004439771175385
iteration 197, loss = 0.0016467090463265777
iteration 198, loss = 0.00346538913436234
iteration 199, loss = 0.0019249661127105355
iteration 200, loss = 0.0019278561230748892
iteration 201, loss = 0.00373426778241992
iteration 202, loss = 0.00245347386226058
iteration 203, loss = 0.001692469697445631
iteration 204, loss = 0.0016297352267429233
iteration 205, loss = 0.002343481872230768
iteration 206, loss = 0.0018326420104131103
iteration 207, loss = 0.0017753801075741649
iteration 208, loss = 0.0021607503294944763
iteration 209, loss = 0.0020528167951852083
iteration 210, loss = 0.0017327801324427128
iteration 211, loss = 0.0027487087063491344
iteration 212, loss = 0.00210224324837327
iteration 213, loss = 0.002702085766941309
iteration 214, loss = 0.0032575917430222034
iteration 215, loss = 0.0017410805448889732
iteration 216, loss = 0.0018808298045769334
iteration 217, loss = 0.001965730218216777
iteration 218, loss = 0.0018907824996858835
iteration 219, loss = 0.004420822486281395
iteration 220, loss = 0.002052201656624675
iteration 221, loss = 0.00226896395906806
iteration 222, loss = 0.0019200100796297193
iteration 223, loss = 0.002012968761846423
iteration 224, loss = 0.002165283774957061
iteration 225, loss = 0.0024983675684779882
iteration 226, loss = 0.002212938852608204
iteration 227, loss = 0.0018658791668713093
iteration 228, loss = 0.0019381227903068066
iteration 229, loss = 0.002001374028623104
iteration 230, loss = 0.0017081709811463952
iteration 231, loss = 0.0022719723638147116
iteration 232, loss = 0.002209305763244629
iteration 233, loss = 0.00194133585318923
iteration 234, loss = 0.0032225148752331734
iteration 235, loss = 0.002085203304886818
iteration 236, loss = 0.001992444507777691
iteration 237, loss = 0.002184269716963172
iteration 238, loss = 0.0020744341891258955
iteration 239, loss = 0.0018778132507577538
iteration 240, loss = 0.0018585867946967483
iteration 241, loss = 0.0038521115202456713
iteration 242, loss = 0.0021022388245910406
iteration 243, loss = 0.001795327290892601
iteration 244, loss = 0.0021123243495821953
iteration 245, loss = 0.00191225646995008
iteration 246, loss = 0.0025670030154287815
iteration 247, loss = 0.003137323772534728
iteration 248, loss = 0.0018288150895386934
iteration 249, loss = 0.001922479597851634
iteration 250, loss = 0.001705389004200697
iteration 251, loss = 0.0024330406449735165
iteration 252, loss = 0.0022860574536025524
iteration 253, loss = 0.002308023627847433
iteration 254, loss = 0.0018593118293210864
iteration 255, loss = 0.003459641709923744
iteration 256, loss = 0.0020310087129473686
iteration 257, loss = 0.0016887607052922249
iteration 258, loss = 0.0018562392797321081
iteration 259, loss = 0.002499418566003442
iteration 260, loss = 0.001805450883693993
iteration 261, loss = 0.0019706622697412968
iteration 262, loss = 0.0024891928769648075
iteration 263, loss = 0.002937890123575926
iteration 264, loss = 0.0026429451536387205
iteration 265, loss = 0.0019353709649294615
iteration 266, loss = 0.0018581388285383582
iteration 267, loss = 0.0024342145770788193
iteration 268, loss = 0.0017796432366594672
iteration 269, loss = 0.001746265683323145
iteration 270, loss = 0.002725795144215226
iteration 271, loss = 0.00266833184286952
iteration 272, loss = 0.0018953434191644192
iteration 273, loss = 0.0016624744748696685
iteration 274, loss = 0.0027068681083619595
iteration 275, loss = 0.0021469788625836372
iteration 276, loss = 0.0020639861468225718
iteration 277, loss = 0.0039564939215779305
iteration 278, loss = 0.00403562281280756
iteration 279, loss = 0.004743455909192562
iteration 280, loss = 0.0017145442543551326
iteration 281, loss = 0.0019115903414785862
iteration 282, loss = 0.0023265001364052296
iteration 283, loss = 0.001896771602332592
iteration 284, loss = 0.002685873070731759
iteration 285, loss = 0.002158248331397772
iteration 286, loss = 0.0033744315151125193
iteration 287, loss = 0.0024665386881679296
iteration 288, loss = 0.0023726006038486958
iteration 289, loss = 0.0022187812719494104
iteration 290, loss = 0.0015902629820629954
iteration 291, loss = 0.0022637087386101484
iteration 292, loss = 0.003825424239039421
iteration 293, loss = 0.0019612512551248074
iteration 294, loss = 0.0022992894519120455
iteration 295, loss = 0.0016543674282729626
iteration 296, loss = 0.0017307741800323129
iteration 297, loss = 0.0021791639737784863
iteration 298, loss = 0.0016381579916924238
iteration 299, loss = 0.0014723778003826737
iteration 300, loss = 0.0022552968002855778
iteration 1, loss = 0.0022528907284140587
iteration 2, loss = 0.004095216281712055
iteration 3, loss = 0.0024640329647809267
iteration 4, loss = 0.0041475603356957436
iteration 5, loss = 0.0018764236010611057
iteration 6, loss = 0.002308169612661004
iteration 7, loss = 0.0017242198809981346
iteration 8, loss = 0.0022312402725219727
iteration 9, loss = 0.0018656918546184897
iteration 10, loss = 0.0028596289921551943
iteration 11, loss = 0.002496076514944434
iteration 12, loss = 0.0016194169875234365
iteration 13, loss = 0.0036838240921497345
iteration 14, loss = 0.0016251073684543371
iteration 15, loss = 0.0017526597948744893
iteration 16, loss = 0.0015713416505604982
iteration 17, loss = 0.0025601894594728947
iteration 18, loss = 0.0024648169055581093
iteration 19, loss = 0.0018564643105491996
iteration 20, loss = 0.0017642684979364276
iteration 21, loss = 0.0018027641344815493
iteration 22, loss = 0.003168320981785655
iteration 23, loss = 0.0022207750007510185
iteration 24, loss = 0.0018757202196866274
iteration 25, loss = 0.001997394487261772
iteration 26, loss = 0.0019891797564923763
iteration 27, loss = 0.0019739700946956873
iteration 28, loss = 0.002045550849288702
iteration 29, loss = 0.0017712131375446916
iteration 30, loss = 0.001991723431274295
iteration 31, loss = 0.0018389341421425343
iteration 32, loss = 0.0020891353487968445
iteration 33, loss = 0.002068744506686926
iteration 34, loss = 0.0016255088848993182
iteration 35, loss = 0.0020830060821026564
iteration 36, loss = 0.004482885356992483
iteration 37, loss = 0.0017304593930020928
iteration 38, loss = 0.00217343564145267
iteration 39, loss = 0.0024543381296098232
iteration 40, loss = 0.0019799962174147367
iteration 41, loss = 0.0020697335712611675
iteration 42, loss = 0.0019758588168770075
iteration 43, loss = 0.003251524642109871
iteration 44, loss = 0.002052286406978965
iteration 45, loss = 0.002064380096271634
iteration 46, loss = 0.0025802708696573973
iteration 47, loss = 0.001959031680598855
iteration 48, loss = 0.0028906920924782753
iteration 49, loss = 0.0019526967080309987
iteration 50, loss = 0.00344270677305758
iteration 51, loss = 0.0019372005481272936
iteration 52, loss = 0.0015588586684316397
iteration 53, loss = 0.0017046178691089153
iteration 54, loss = 0.002012361539527774
iteration 55, loss = 0.002085343236103654
iteration 56, loss = 0.0019793324172496796
iteration 57, loss = 0.0020428700372576714
iteration 58, loss = 0.0023034580517560244
iteration 59, loss = 0.0019940787460654974
iteration 60, loss = 0.003481226973235607
iteration 61, loss = 0.0018506518099457026
iteration 62, loss = 0.0020541504491120577
iteration 63, loss = 0.0019443329656496644
iteration 64, loss = 0.0021859256085008383
iteration 65, loss = 0.00196926761418581
iteration 66, loss = 0.0019477753667160869
iteration 67, loss = 0.00230255420319736
iteration 68, loss = 0.002031097188591957
iteration 69, loss = 0.001885364530608058
iteration 70, loss = 0.002934257499873638
iteration 71, loss = 0.0018899156711995602
iteration 72, loss = 0.002669100183993578
iteration 73, loss = 0.0029493083711713552
iteration 74, loss = 0.00399768864735961
iteration 75, loss = 0.0021667086984962225
iteration 76, loss = 0.0019031999399885535
iteration 77, loss = 0.0019253569189459085
iteration 78, loss = 0.0015623790677636862
iteration 79, loss = 0.002170136198401451
iteration 80, loss = 0.002751776948571205
iteration 81, loss = 0.002203273121267557
iteration 82, loss = 0.001707059913314879
iteration 83, loss = 0.001987538067623973
iteration 84, loss = 0.002074616961181164
iteration 85, loss = 0.0022544681560248137
iteration 86, loss = 0.002004948677495122
iteration 87, loss = 0.004104784689843655
iteration 88, loss = 0.001628302619792521
iteration 89, loss = 0.0029099576640874147
iteration 90, loss = 0.0028075233567506075
iteration 91, loss = 0.0019124479731544852
iteration 92, loss = 0.002193715190514922
iteration 93, loss = 0.0018320588860660791
iteration 94, loss = 0.001825832761824131
iteration 95, loss = 0.003074940759688616
iteration 96, loss = 0.002566306386142969
iteration 97, loss = 0.002384927123785019
iteration 98, loss = 0.0022637394722551107
iteration 99, loss = 0.0020026785787194967
iteration 100, loss = 0.001839404576458037
iteration 101, loss = 0.0021044097375124693
iteration 102, loss = 0.002316092373803258
iteration 103, loss = 0.002039074432104826
iteration 104, loss = 0.0018538994481787086
iteration 105, loss = 0.001742763677611947
iteration 106, loss = 0.0018545594066381454
iteration 107, loss = 0.003154338803142309
iteration 108, loss = 0.004129048436880112
iteration 109, loss = 0.0023229026701301336
iteration 110, loss = 0.0023800027556717396
iteration 111, loss = 0.0018788689048960805
iteration 112, loss = 0.0036445907317101955
iteration 113, loss = 0.0020600338466465473
iteration 114, loss = 0.0024495804682374
iteration 115, loss = 0.0016548035200685263
iteration 116, loss = 0.0019608770962804556
iteration 117, loss = 0.004602689296007156
iteration 118, loss = 0.001786731882020831
iteration 119, loss = 0.0017525304574519396
iteration 120, loss = 0.0020842254161834717
iteration 121, loss = 0.0016211250331252813
iteration 122, loss = 0.003677041968330741
iteration 123, loss = 0.0018162999767810106
iteration 124, loss = 0.002802242524921894
iteration 125, loss = 0.0022335448302328587
iteration 126, loss = 0.0019335057586431503
iteration 127, loss = 0.0018972025718539953
iteration 128, loss = 0.0018492991803213954
iteration 129, loss = 0.0033154156990349293
iteration 130, loss = 0.0022937392350286245
iteration 131, loss = 0.0019123062957078218
iteration 132, loss = 0.0017439256189391017
iteration 133, loss = 0.0017580786952748895
iteration 134, loss = 0.0017071717884391546
iteration 135, loss = 0.001919076661579311
iteration 136, loss = 0.0016702848952263594
iteration 137, loss = 0.0028204030822962523
iteration 138, loss = 0.0017997639952227473
iteration 139, loss = 0.0020807324908673763
iteration 140, loss = 0.0024374080821871758
iteration 141, loss = 0.0032569081522524357
iteration 142, loss = 0.0020863162353634834
iteration 143, loss = 0.005937839858233929
iteration 144, loss = 0.002508144360035658
iteration 145, loss = 0.001721955370157957
iteration 146, loss = 0.0030495093669742346
iteration 147, loss = 0.002586720045655966
iteration 148, loss = 0.0019865911453962326
iteration 149, loss = 0.002163718920201063
iteration 150, loss = 0.0018560811877250671
iteration 151, loss = 0.002314038574695587
iteration 152, loss = 0.002406251383945346
iteration 153, loss = 0.0019358992576599121
iteration 154, loss = 0.002191052306443453
iteration 155, loss = 0.0017477244837209582
iteration 156, loss = 0.0018399122636765242
iteration 157, loss = 0.0017729776445776224
iteration 158, loss = 0.003347609890624881
iteration 159, loss = 0.0024735049810260534
iteration 160, loss = 0.0024288208223879337
iteration 161, loss = 0.0022443453781306744
iteration 162, loss = 0.003090094542130828
iteration 163, loss = 0.001958630047738552
iteration 164, loss = 0.00401521660387516
iteration 165, loss = 0.0018573782872408628
iteration 166, loss = 0.004162977449595928
iteration 167, loss = 0.00190259818919003
iteration 168, loss = 0.001894309651106596
iteration 169, loss = 0.0017701471224427223
iteration 170, loss = 0.0019425975624471903
iteration 171, loss = 0.0015134378336369991
iteration 172, loss = 0.0018397545209154487
iteration 173, loss = 0.0022719961125403643
iteration 174, loss = 0.002261287299916148
iteration 175, loss = 0.0022124473471194506
iteration 176, loss = 0.0019164580153301358
iteration 177, loss = 0.0023443156387656927
iteration 178, loss = 0.00203963415697217
iteration 179, loss = 0.001990905497223139
iteration 180, loss = 0.0017347144894301891
iteration 181, loss = 0.0016741164727136493
iteration 182, loss = 0.0019199205562472343
iteration 183, loss = 0.0023755631409585476
iteration 184, loss = 0.0017692659748718143
iteration 185, loss = 0.0024393924977630377
iteration 186, loss = 0.0026421190705150366
iteration 187, loss = 0.0020898578222841024
iteration 188, loss = 0.0025534096639603376
iteration 189, loss = 0.0027810516767203808
iteration 190, loss = 0.002239187713712454
iteration 191, loss = 0.0024246410466730595
iteration 192, loss = 0.0017731141997501254
iteration 193, loss = 0.0022428918164223433
iteration 194, loss = 0.0024068704806268215
iteration 195, loss = 0.001823957310989499
iteration 196, loss = 0.0017866117414087057
iteration 197, loss = 0.0019050630507990718
iteration 198, loss = 0.002413034439086914
iteration 199, loss = 0.0035120737738907337
iteration 200, loss = 0.002621143124997616
iteration 201, loss = 0.001979718916118145
iteration 202, loss = 0.004105790052562952
iteration 203, loss = 0.0020232857204973698
iteration 204, loss = 0.0019153077155351639
iteration 205, loss = 0.0018552937544882298
iteration 206, loss = 0.0030640424229204655
iteration 207, loss = 0.0021287528797984123
iteration 208, loss = 0.0017235736595466733
iteration 209, loss = 0.002675111871212721
iteration 210, loss = 0.002067413879558444
iteration 211, loss = 0.0016790825175121427
iteration 212, loss = 0.002896043239161372
iteration 213, loss = 0.0019643083214759827
iteration 214, loss = 0.0021249307319521904
iteration 215, loss = 0.0023585709277540445
iteration 216, loss = 0.001711128861643374
iteration 217, loss = 0.0022242763079702854
iteration 218, loss = 0.0022505507804453373
iteration 219, loss = 0.0016659151297062635
iteration 220, loss = 0.001730160671286285
iteration 221, loss = 0.0028485723305493593
iteration 222, loss = 0.0018384961877018213
iteration 223, loss = 0.00159258337225765
iteration 224, loss = 0.0019354363903403282
iteration 225, loss = 0.002110530622303486
iteration 226, loss = 0.0019396563293412328
iteration 227, loss = 0.0016433164710178971
iteration 228, loss = 0.0019784194882959127
iteration 229, loss = 0.0020506461150944233
iteration 230, loss = 0.002466086996719241
iteration 231, loss = 0.002105092629790306
iteration 232, loss = 0.003345680423080921
iteration 233, loss = 0.002111447509378195
iteration 234, loss = 0.0019785116892307997
iteration 235, loss = 0.002091642701998353
iteration 236, loss = 0.002178200287744403
iteration 237, loss = 0.002591172931715846
iteration 238, loss = 0.002631744369864464
iteration 239, loss = 0.0031815574038773775
iteration 240, loss = 0.002477313159033656
iteration 241, loss = 0.0035643544979393482
iteration 242, loss = 0.0016097667394205928
iteration 243, loss = 0.002230799989774823
iteration 244, loss = 0.002279834356158972
iteration 245, loss = 0.001873972825706005
iteration 246, loss = 0.0019644792191684246
iteration 247, loss = 0.0022863170597702265
iteration 248, loss = 0.0020287048537284136
iteration 249, loss = 0.0029972281772643328
iteration 250, loss = 0.0017723558703437448
iteration 251, loss = 0.001765674096532166
iteration 252, loss = 0.003956697881221771
iteration 253, loss = 0.003288014093413949
iteration 254, loss = 0.0021415262017399073
iteration 255, loss = 0.0023500409442931414
iteration 256, loss = 0.0018142512999475002
iteration 257, loss = 0.0018800919642671943
iteration 258, loss = 0.0018472321098670363
iteration 259, loss = 0.0020886629354208708
iteration 260, loss = 0.002674966584891081
iteration 261, loss = 0.003000436583533883
iteration 262, loss = 0.0020159215200692415
iteration 263, loss = 0.0025849188677966595
iteration 264, loss = 0.0022867489606142044
iteration 265, loss = 0.0019107350381091237
iteration 266, loss = 0.0019985472317785025
iteration 267, loss = 0.002605249872431159
iteration 268, loss = 0.0018729615258052945
iteration 269, loss = 0.0017534677172079682
iteration 270, loss = 0.0023684571497142315
iteration 271, loss = 0.0018151013646274805
iteration 272, loss = 0.001956355758011341
iteration 273, loss = 0.0016191076720133424
iteration 274, loss = 0.002672822680324316
iteration 275, loss = 0.0018978138687089086
iteration 276, loss = 0.0021699233911931515
iteration 277, loss = 0.003724757581949234
iteration 278, loss = 0.0021748270373791456
iteration 279, loss = 0.0021670893765985966
iteration 280, loss = 0.004122857470065355
iteration 281, loss = 0.002174974884837866
iteration 282, loss = 0.0019584542606025934
iteration 283, loss = 0.002052481286227703
iteration 284, loss = 0.0021147585939615965
iteration 285, loss = 0.00274607609026134
iteration 286, loss = 0.0019476612797006965
iteration 287, loss = 0.004266500473022461
iteration 288, loss = 0.002170389983803034
iteration 289, loss = 0.0019210991449654102
iteration 290, loss = 0.0023828460834920406
iteration 291, loss = 0.0021570087410509586
iteration 292, loss = 0.0034215389750897884
iteration 293, loss = 0.0019647954031825066
iteration 294, loss = 0.002316974103450775
iteration 295, loss = 0.0021878306288272142
iteration 296, loss = 0.0026290607638657093
iteration 297, loss = 0.0027359728701412678
iteration 298, loss = 0.0023288745433092117
iteration 299, loss = 0.0015800470719113946
iteration 300, loss = 0.0019403112819418311
iteration 1, loss = 0.0021854082588106394
iteration 2, loss = 0.0018507966306060553
iteration 3, loss = 0.0021864688023924828
iteration 4, loss = 0.001998127670958638
iteration 5, loss = 0.001975008752197027
iteration 6, loss = 0.004053561948239803
iteration 7, loss = 0.002024262212216854
iteration 8, loss = 0.002001584507524967
iteration 9, loss = 0.002054237760603428
iteration 10, loss = 0.002094007795676589
iteration 11, loss = 0.001637882785871625
iteration 12, loss = 0.0026114140637218952
iteration 13, loss = 0.0024465066380798817
iteration 14, loss = 0.0019283693982288241
iteration 15, loss = 0.0017818465130403638
iteration 16, loss = 0.0016740183345973492
iteration 17, loss = 0.001871605170890689
iteration 18, loss = 0.0022605648264288902
iteration 19, loss = 0.0031344627495855093
iteration 20, loss = 0.001953090075403452
iteration 21, loss = 0.0021271423902362585
iteration 22, loss = 0.0021745299454778433
iteration 23, loss = 0.002023842418566346
iteration 24, loss = 0.004078144207596779
iteration 25, loss = 0.0027261320501565933
iteration 26, loss = 0.001959994900971651
iteration 27, loss = 0.0028016201686114073
iteration 28, loss = 0.002682235324755311
iteration 29, loss = 0.003729515476152301
iteration 30, loss = 0.0016387139912694693
iteration 31, loss = 0.0031774393282830715
iteration 32, loss = 0.002213503234088421
iteration 33, loss = 0.0018250870052725077
iteration 34, loss = 0.0019970734138041735
iteration 35, loss = 0.0022652274928987026
iteration 36, loss = 0.003977039363235235
iteration 37, loss = 0.003516044933348894
iteration 38, loss = 0.0017395764589309692
iteration 39, loss = 0.0020426027476787567
iteration 40, loss = 0.0018873173976317048
iteration 41, loss = 0.0019101999932900071
iteration 42, loss = 0.002083342522382736
iteration 43, loss = 0.0021846573799848557
iteration 44, loss = 0.0023904393892735243
iteration 45, loss = 0.0016015477012842894
iteration 46, loss = 0.002195118460804224
iteration 47, loss = 0.001716842409223318
iteration 48, loss = 0.0015492833917960525
iteration 49, loss = 0.0019393491093069315
iteration 50, loss = 0.002048544818535447
iteration 51, loss = 0.0018131716642528772
iteration 52, loss = 0.002133931266143918
iteration 53, loss = 0.002461525611579418
iteration 54, loss = 0.0017440563533455133
iteration 55, loss = 0.0019801815506070852
iteration 56, loss = 0.002369981724768877
iteration 57, loss = 0.0020989193581044674
iteration 58, loss = 0.002116733230650425
iteration 59, loss = 0.0028480198234319687
iteration 60, loss = 0.0017147645121440291
iteration 61, loss = 0.001806622720323503
iteration 62, loss = 0.002146568149328232
iteration 63, loss = 0.003455819096416235
iteration 64, loss = 0.0018686230760067701
iteration 65, loss = 0.001619547139853239
iteration 66, loss = 0.0023072431795299053
iteration 67, loss = 0.0024536345154047012
iteration 68, loss = 0.0024316543713212013
iteration 69, loss = 0.0019402356119826436
iteration 70, loss = 0.0025663140695542097
iteration 71, loss = 0.001968038734048605
iteration 72, loss = 0.0015823377761989832
iteration 73, loss = 0.002596758073195815
iteration 74, loss = 0.002084990730509162
iteration 75, loss = 0.0018675167812034488
iteration 76, loss = 0.0020797718316316605
iteration 77, loss = 0.0019504062365740538
iteration 78, loss = 0.0021972465328872204
iteration 79, loss = 0.0017226747004315257
iteration 80, loss = 0.001999503467231989
iteration 81, loss = 0.003976376727223396
iteration 82, loss = 0.0019613332115113735
iteration 83, loss = 0.00260190200060606
iteration 84, loss = 0.0022298330441117287
iteration 85, loss = 0.0021486086770892143
iteration 86, loss = 0.00210096244700253
iteration 87, loss = 0.0024610261898487806
iteration 88, loss = 0.0020032734610140324
iteration 89, loss = 0.0019407282816246152
iteration 90, loss = 0.0030375304631888866
iteration 91, loss = 0.0027363852132111788
iteration 92, loss = 0.0018127474468201399
iteration 93, loss = 0.0018349220044910908
iteration 94, loss = 0.0025572951417416334
iteration 95, loss = 0.0022531147114932537
iteration 96, loss = 0.002580551430583
iteration 97, loss = 0.0018043190939351916
iteration 98, loss = 0.0018648480763658881
iteration 99, loss = 0.00406310660764575
iteration 100, loss = 0.00234694080427289
iteration 101, loss = 0.0023412336595356464
iteration 102, loss = 0.0020586298778653145
iteration 103, loss = 0.0022426787763834
iteration 104, loss = 0.0027133203111588955
iteration 105, loss = 0.0029873629100620747
iteration 106, loss = 0.002107195556163788
iteration 107, loss = 0.003656569868326187
iteration 108, loss = 0.0034384019672870636
iteration 109, loss = 0.001777496887370944
iteration 110, loss = 0.0036180377937853336
iteration 111, loss = 0.002477147849276662
iteration 112, loss = 0.0017620337894186378
iteration 113, loss = 0.0020666795317083597
iteration 114, loss = 0.001893437816761434
iteration 115, loss = 0.0021504319738596678
iteration 116, loss = 0.0023051195312291384
iteration 117, loss = 0.0016233868664130569
iteration 118, loss = 0.0020956804510205984
iteration 119, loss = 0.002073810203000903
iteration 120, loss = 0.002371126087382436
iteration 121, loss = 0.0020811075810343027
iteration 122, loss = 0.001691020093858242
iteration 123, loss = 0.001875916961580515
iteration 124, loss = 0.0021775090135633945
iteration 125, loss = 0.0021560711320489645
iteration 126, loss = 0.0020134395454078913
iteration 127, loss = 0.002916886704042554
iteration 128, loss = 0.0021892543882131577
iteration 129, loss = 0.0018153581768274307
iteration 130, loss = 0.0017428917344659567
iteration 131, loss = 0.0020586138125509024
iteration 132, loss = 0.0019422834739089012
iteration 133, loss = 0.0038512255996465683
iteration 134, loss = 0.0037625334225594997
iteration 135, loss = 0.0018864864250645041
iteration 136, loss = 0.002935536904260516
iteration 137, loss = 0.004291465040296316
iteration 138, loss = 0.0020973917562514544
iteration 139, loss = 0.0018387279706075788
iteration 140, loss = 0.0018529522931203246
iteration 141, loss = 0.0023843622766435146
iteration 142, loss = 0.002043146174401045
iteration 143, loss = 0.0029769097454845905
iteration 144, loss = 0.0029266783967614174
iteration 145, loss = 0.0019249460892751813
iteration 146, loss = 0.0017958261305466294
iteration 147, loss = 0.001962759532034397
iteration 148, loss = 0.0026143474970012903
iteration 149, loss = 0.0018969866214320064
iteration 150, loss = 0.002562150126323104
iteration 151, loss = 0.0021496769040822983
iteration 152, loss = 0.004396981094032526
iteration 153, loss = 0.002118967706337571
iteration 154, loss = 0.0019195020431652665
iteration 155, loss = 0.0018021685536950827
iteration 156, loss = 0.0023402466904371977
iteration 157, loss = 0.0026606256142258644
iteration 158, loss = 0.0018059100257232785
iteration 159, loss = 0.0019105677492916584
iteration 160, loss = 0.002373591996729374
iteration 161, loss = 0.0036988018546253443
iteration 162, loss = 0.0035292431712150574
iteration 163, loss = 0.001955813728272915
iteration 164, loss = 0.0023565837182104588
iteration 165, loss = 0.00334221706725657
iteration 166, loss = 0.0031484472565352917
iteration 167, loss = 0.0037020810414105654
iteration 168, loss = 0.0025585414841771126
iteration 169, loss = 0.0016563861863687634
iteration 170, loss = 0.004264345858246088
iteration 171, loss = 0.001783084124326706
iteration 172, loss = 0.001820875215344131
iteration 173, loss = 0.003603077493607998
iteration 174, loss = 0.0025861579924821854
iteration 175, loss = 0.003989518154412508
iteration 176, loss = 0.0022487114183604717
iteration 177, loss = 0.0020789229311048985
iteration 178, loss = 0.0021288800053298473
iteration 179, loss = 0.0017101195408031344
iteration 180, loss = 0.002023695968091488
iteration 181, loss = 0.0016080592758953571
iteration 182, loss = 0.0019143566023558378
iteration 183, loss = 0.0019231329206377268
iteration 184, loss = 0.003845594823360443
iteration 185, loss = 0.0038761687465012074
iteration 186, loss = 0.0028538645710796118
iteration 187, loss = 0.0022375155240297318
iteration 188, loss = 0.0019564502872526646
iteration 189, loss = 0.0018033382948487997
iteration 190, loss = 0.002105118241161108
iteration 191, loss = 0.0015383127611130476
iteration 192, loss = 0.002176117617636919
iteration 193, loss = 0.0017824872629716992
iteration 194, loss = 0.0019274931401014328
iteration 195, loss = 0.0023851238656789064
iteration 196, loss = 0.002665905049070716
iteration 197, loss = 0.0019548663403838873
iteration 198, loss = 0.0018356217769905925
iteration 199, loss = 0.0017421662341803312
iteration 200, loss = 0.002701031742617488
iteration 201, loss = 0.0029072894249111414
iteration 202, loss = 0.002957261400297284
iteration 203, loss = 0.001682145637460053
iteration 204, loss = 0.0025943706277757883
iteration 205, loss = 0.001955740386620164
iteration 206, loss = 0.0020086881704628468
iteration 207, loss = 0.001895340858027339
iteration 208, loss = 0.001894690445624292
iteration 209, loss = 0.001975510036572814
iteration 210, loss = 0.002230058889836073
iteration 211, loss = 0.002801173133775592
iteration 212, loss = 0.0018458986887708306
iteration 213, loss = 0.001848895102739334
iteration 214, loss = 0.001788092078641057
iteration 215, loss = 0.0018343864940106869
iteration 216, loss = 0.002410495886579156
iteration 217, loss = 0.0017907829023897648
iteration 218, loss = 0.001978909596800804
iteration 219, loss = 0.0024251698050647974
iteration 220, loss = 0.0017676879651844501
iteration 221, loss = 0.0025388028007000685
iteration 222, loss = 0.0016416580183431506
iteration 223, loss = 0.0020485534332692623
iteration 224, loss = 0.0018350329482927918
iteration 225, loss = 0.002354528522118926
iteration 226, loss = 0.0019796902779489756
iteration 227, loss = 0.0021127781365066767
iteration 228, loss = 0.0020879812072962523
iteration 229, loss = 0.002033631782978773
iteration 230, loss = 0.0020192370284348726
iteration 231, loss = 0.002197460737079382
iteration 232, loss = 0.0025958274491131306
iteration 233, loss = 0.0023561667185276747
iteration 234, loss = 0.0030131537932902575
iteration 235, loss = 0.0017275899881497025
iteration 236, loss = 0.0026907562278211117
iteration 237, loss = 0.0018100995803251863
iteration 238, loss = 0.0018292369786649942
iteration 239, loss = 0.0017346731619909406
iteration 240, loss = 0.0021145925857126713
iteration 241, loss = 0.0018330923048779368
iteration 242, loss = 0.0019541983492672443
iteration 243, loss = 0.002809960162267089
iteration 244, loss = 0.005910896696150303
iteration 245, loss = 0.0018623578362166882
iteration 246, loss = 0.0020214493852108717
iteration 247, loss = 0.001876823604106903
iteration 248, loss = 0.001760925748385489
iteration 249, loss = 0.002176936948671937
iteration 250, loss = 0.0018744835397228599
iteration 251, loss = 0.001857758965343237
iteration 252, loss = 0.001763554522767663
iteration 253, loss = 0.002603331580758095
iteration 254, loss = 0.001769614522345364
iteration 255, loss = 0.0020699193701148033
iteration 256, loss = 0.0016683628782629967
iteration 257, loss = 0.0024176507722586393
iteration 258, loss = 0.0023601932916790247
iteration 259, loss = 0.0019486609380692244
iteration 260, loss = 0.0017672121757641435
iteration 261, loss = 0.0016366152558475733
iteration 262, loss = 0.0025695953518152237
iteration 263, loss = 0.0018258042400702834
iteration 264, loss = 0.0020776470191776752
iteration 265, loss = 0.0022561668884009123
iteration 266, loss = 0.002035844372585416
iteration 267, loss = 0.001749456743709743
iteration 268, loss = 0.0017849034629762173
iteration 269, loss = 0.0028856692370027304
iteration 270, loss = 0.003407124662771821
iteration 271, loss = 0.0023857250344008207
iteration 272, loss = 0.0019112518057227135
iteration 273, loss = 0.0036673876456916332
iteration 274, loss = 0.002528791781514883
iteration 275, loss = 0.00197419011965394
iteration 276, loss = 0.001926566124893725
iteration 277, loss = 0.003142875386402011
iteration 278, loss = 0.0018551837420091033
iteration 279, loss = 0.0018855328671634197
iteration 280, loss = 0.002242001937702298
iteration 281, loss = 0.00176509958691895
iteration 282, loss = 0.0038667016196995974
iteration 283, loss = 0.002205078722909093
iteration 284, loss = 0.0017450855812057853
iteration 285, loss = 0.0024229767732322216
iteration 286, loss = 0.0016227458836510777
iteration 287, loss = 0.002320965752005577
iteration 288, loss = 0.003789694746956229
iteration 289, loss = 0.001830974710173905
iteration 290, loss = 0.001896766247227788
iteration 291, loss = 0.0026740971952676773
iteration 292, loss = 0.0023962301202118397
iteration 293, loss = 0.001889003673568368
iteration 294, loss = 0.0016771203372627497
iteration 295, loss = 0.0019299699924886227
iteration 296, loss = 0.0023638817947357893
iteration 297, loss = 0.001886258483864367
iteration 298, loss = 0.0016963958041742444
iteration 299, loss = 0.0020356900058686733
iteration 300, loss = 0.0022236336953938007
iteration 1, loss = 0.001808968372642994
iteration 2, loss = 0.0021746335551142693
iteration 3, loss = 0.0020839055068790913
iteration 4, loss = 0.00215202197432518
iteration 5, loss = 0.0017591057112440467
iteration 6, loss = 0.0020961612462997437
iteration 7, loss = 0.002110785339027643
iteration 8, loss = 0.00165878317784518
iteration 9, loss = 0.001823726575821638
iteration 10, loss = 0.002074679359793663
iteration 11, loss = 0.0019097797339782119
iteration 12, loss = 0.0024242696817964315
iteration 13, loss = 0.0031197720672935247
iteration 14, loss = 0.0025217635557055473
iteration 15, loss = 0.0031997051555663347
iteration 16, loss = 0.0020660050213336945
iteration 17, loss = 0.001901315408758819
iteration 18, loss = 0.0031494065187871456
iteration 19, loss = 0.001850442378781736
iteration 20, loss = 0.0021856031380593777
iteration 21, loss = 0.0015173887368291616
iteration 22, loss = 0.001971375895664096
iteration 23, loss = 0.0021380300167948008
iteration 24, loss = 0.0017521711997687817
iteration 25, loss = 0.0022897105664014816
iteration 26, loss = 0.0017094193026423454
iteration 27, loss = 0.0024403424467891455
iteration 28, loss = 0.001956499880179763
iteration 29, loss = 0.0016843278426676989
iteration 30, loss = 0.0022096391767263412
iteration 31, loss = 0.0016479352489113808
iteration 32, loss = 0.0017002308741211891
iteration 33, loss = 0.0019192155450582504
iteration 34, loss = 0.0015998771414160728
iteration 35, loss = 0.0020401491783559322
iteration 36, loss = 0.0017952446360141039
iteration 37, loss = 0.002303479239344597
iteration 38, loss = 0.0025677536614239216
iteration 39, loss = 0.0027643057983368635
iteration 40, loss = 0.0024159345775842667
iteration 41, loss = 0.0026366643141955137
iteration 42, loss = 0.0021447560284286737
iteration 43, loss = 0.0016456098528578877
iteration 44, loss = 0.0016545626567676663
iteration 45, loss = 0.002356872893869877
iteration 46, loss = 0.0018535309936851263
iteration 47, loss = 0.002546523930504918
iteration 48, loss = 0.0019136969931423664
iteration 49, loss = 0.0024744682013988495
iteration 50, loss = 0.0025066311936825514
iteration 51, loss = 0.0020489697344601154
iteration 52, loss = 0.0021521751768887043
iteration 53, loss = 0.001821428188122809
iteration 54, loss = 0.0027710485737770796
iteration 55, loss = 0.0022029378451406956
iteration 56, loss = 0.0019343297462910414
iteration 57, loss = 0.002001085551455617
iteration 58, loss = 0.003322033677250147
iteration 59, loss = 0.004519653506577015
iteration 60, loss = 0.002087425207719207
iteration 61, loss = 0.0038603160064667463
iteration 62, loss = 0.0016821022145450115
iteration 63, loss = 0.0030998261645436287
iteration 64, loss = 0.002281803637742996
iteration 65, loss = 0.0019454899011179805
iteration 66, loss = 0.002064125146716833
iteration 67, loss = 0.002072959905490279
iteration 68, loss = 0.001664221752434969
iteration 69, loss = 0.0021352411713451147
iteration 70, loss = 0.0017814243910834193
iteration 71, loss = 0.002123749814927578
iteration 72, loss = 0.0018373875645920634
iteration 73, loss = 0.0017790618585422635
iteration 74, loss = 0.0016744311433285475
iteration 75, loss = 0.0019083283841609955
iteration 76, loss = 0.00219010841101408
iteration 77, loss = 0.00204003369435668
iteration 78, loss = 0.0024788673035800457
iteration 79, loss = 0.0026373190339654684
iteration 80, loss = 0.002105681225657463
iteration 81, loss = 0.0020525152795016766
iteration 82, loss = 0.0030502642039209604
iteration 83, loss = 0.0016543071251362562
iteration 84, loss = 0.003072514897212386
iteration 85, loss = 0.003629503073170781
iteration 86, loss = 0.0022858616430312395
iteration 87, loss = 0.003410010365769267
iteration 88, loss = 0.0023581732530146837
iteration 89, loss = 0.0022466806694865227
iteration 90, loss = 0.0016470205737277865
iteration 91, loss = 0.001864936319179833
iteration 92, loss = 0.0023472958710044622
iteration 93, loss = 0.0021507928613573313
iteration 94, loss = 0.0019473906140774488
iteration 95, loss = 0.0018517740536481142
iteration 96, loss = 0.0015426133759319782
iteration 97, loss = 0.0018634966108947992
iteration 98, loss = 0.0029823128134012222
iteration 99, loss = 0.0017076333751901984
iteration 100, loss = 0.001674023806117475
iteration 101, loss = 0.002190557075664401
iteration 102, loss = 0.0017471896717324853
iteration 103, loss = 0.002006236929446459
iteration 104, loss = 0.003155643818899989
iteration 105, loss = 0.0019832353573292494
iteration 106, loss = 0.005013763904571533
iteration 107, loss = 0.0030324901454150677
iteration 108, loss = 0.004314585588872433
iteration 109, loss = 0.0020050310995429754
iteration 110, loss = 0.0024355920031666756
iteration 111, loss = 0.0035674762912094593
iteration 112, loss = 0.0019641206599771976
iteration 113, loss = 0.0015953169204294682
iteration 114, loss = 0.002615131437778473
iteration 115, loss = 0.0032422577496618032
iteration 116, loss = 0.0019283628789708018
iteration 117, loss = 0.0019108089618384838
iteration 118, loss = 0.0022693651262670755
iteration 119, loss = 0.0021270769648253918
iteration 120, loss = 0.0017451902385801077
iteration 121, loss = 0.0020312066189944744
iteration 122, loss = 0.002087981905788183
iteration 123, loss = 0.003193803131580353
iteration 124, loss = 0.0022719488479197025
iteration 125, loss = 0.0019261884735897183
iteration 126, loss = 0.0018779444508254528
iteration 127, loss = 0.0026608591433614492
iteration 128, loss = 0.001815725234337151
iteration 129, loss = 0.002235156949609518
iteration 130, loss = 0.0019376880954951048
iteration 131, loss = 0.0018053771927952766
iteration 132, loss = 0.0017915472853928804
iteration 133, loss = 0.002943797269836068
iteration 134, loss = 0.002045516623184085
iteration 135, loss = 0.0029116973746567965
iteration 136, loss = 0.0022678859531879425
iteration 137, loss = 0.0020371258724480867
iteration 138, loss = 0.0015795258805155754
iteration 139, loss = 0.0021498145069926977
iteration 140, loss = 0.0031512626446783543
iteration 141, loss = 0.002650545910000801
iteration 142, loss = 0.0017382687656208873
iteration 143, loss = 0.0022003408521413803
iteration 144, loss = 0.0018291051965206861
iteration 145, loss = 0.0021973850671201944
iteration 146, loss = 0.0020462702959775925
iteration 147, loss = 0.002325884299352765
iteration 148, loss = 0.002070884918794036
iteration 149, loss = 0.00176819390617311
iteration 150, loss = 0.00208811997435987
iteration 151, loss = 0.0022925380617380142
iteration 152, loss = 0.001955736195668578
iteration 153, loss = 0.0038202947471290827
iteration 154, loss = 0.00221861619502306
iteration 155, loss = 0.0020409454591572285
iteration 156, loss = 0.002053002594038844
iteration 157, loss = 0.001526526757515967
iteration 158, loss = 0.001944229705259204
iteration 159, loss = 0.0018504959298297763
iteration 160, loss = 0.0017771885031834245
iteration 161, loss = 0.005967853125184774
iteration 162, loss = 0.002682983409613371
iteration 163, loss = 0.0016917722532525659
iteration 164, loss = 0.001865565893240273
iteration 165, loss = 0.003552055452018976
iteration 166, loss = 0.002293040044605732
iteration 167, loss = 0.003254247596487403
iteration 168, loss = 0.0018921884475275874
iteration 169, loss = 0.002595956902951002
iteration 170, loss = 0.001850499538704753
iteration 171, loss = 0.0029389422852545977
iteration 172, loss = 0.002424081088975072
iteration 173, loss = 0.0022251042537391186
iteration 174, loss = 0.0019358915742486715
iteration 175, loss = 0.002119035692885518
iteration 176, loss = 0.002708585001528263
iteration 177, loss = 0.0018489116337150335
iteration 178, loss = 0.002447976730763912
iteration 179, loss = 0.002579576801508665
iteration 180, loss = 0.0022342985030263662
iteration 181, loss = 0.004420102573931217
iteration 182, loss = 0.001962292240932584
iteration 183, loss = 0.0016451532719656825
iteration 184, loss = 0.002474097767844796
iteration 185, loss = 0.0017577203689143062
iteration 186, loss = 0.002175640081986785
iteration 187, loss = 0.0021426479797810316
iteration 188, loss = 0.0021217092871665955
iteration 189, loss = 0.0017738293390721083
iteration 190, loss = 0.0027346836868673563
iteration 191, loss = 0.0024585630744695663
iteration 192, loss = 0.0032760780304670334
iteration 193, loss = 0.00169608264695853
iteration 194, loss = 0.004070787690579891
iteration 195, loss = 0.003764734137803316
iteration 196, loss = 0.0025630600284785032
iteration 197, loss = 0.0020641060546040535
iteration 198, loss = 0.0027369605377316475
iteration 199, loss = 0.002390114823356271
iteration 200, loss = 0.001970617100596428
iteration 201, loss = 0.001956852385774255
iteration 202, loss = 0.002382572740316391
iteration 203, loss = 0.0020671896636486053
iteration 204, loss = 0.004079978913068771
iteration 205, loss = 0.002358054742217064
iteration 206, loss = 0.0017801417270675302
iteration 207, loss = 0.0027083703316748142
iteration 208, loss = 0.0019372075330466032
iteration 209, loss = 0.002146343467757106
iteration 210, loss = 0.0019028421957045794
iteration 211, loss = 0.001951560378074646
iteration 212, loss = 0.0019184446427971125
iteration 213, loss = 0.003067606594413519
iteration 214, loss = 0.0015368607128039002
iteration 215, loss = 0.002017322229221463
iteration 216, loss = 0.002240563975647092
iteration 217, loss = 0.002484419848769903
iteration 218, loss = 0.0017822225345298648
iteration 219, loss = 0.0019776069093495607
iteration 220, loss = 0.002440859330818057
iteration 221, loss = 0.002537894295528531
iteration 222, loss = 0.00201916741207242
iteration 223, loss = 0.0016754467505961657
iteration 224, loss = 0.0018670348217710853
iteration 225, loss = 0.0018514388939365745
iteration 226, loss = 0.0021628346294164658
iteration 227, loss = 0.002419054973870516
iteration 228, loss = 0.0017594151431694627
iteration 229, loss = 0.0028939954936504364
iteration 230, loss = 0.0020880450028926134
iteration 231, loss = 0.001848550164140761
iteration 232, loss = 0.0037550923880189657
iteration 233, loss = 0.0018122992478311062
iteration 234, loss = 0.0018001592252403498
iteration 235, loss = 0.002094317227602005
iteration 236, loss = 0.002300319029018283
iteration 237, loss = 0.00210106885060668
iteration 238, loss = 0.0021229004487395287
iteration 239, loss = 0.0018496522679924965
iteration 240, loss = 0.004289786331355572
iteration 241, loss = 0.002575123915448785
iteration 242, loss = 0.003156703896820545
iteration 243, loss = 0.0016636913642287254
iteration 244, loss = 0.0032842163927853107
iteration 245, loss = 0.0018934991676360369
iteration 246, loss = 0.0017070156754925847
iteration 247, loss = 0.0022129539866000414
iteration 248, loss = 0.0020909702870994806
iteration 249, loss = 0.0027243136428296566
iteration 250, loss = 0.0017521667759865522
iteration 251, loss = 0.001934050116688013
iteration 252, loss = 0.0018254111055284739
iteration 253, loss = 0.003186363261193037
iteration 254, loss = 0.002116173505783081
iteration 255, loss = 0.0032032961025834084
iteration 256, loss = 0.002595167141407728
iteration 257, loss = 0.0024704730603843927
iteration 258, loss = 0.0018251016736030579
iteration 259, loss = 0.0028603211976587772
iteration 260, loss = 0.0029468827415257692
iteration 261, loss = 0.0016645295545458794
iteration 262, loss = 0.001927269622683525
iteration 263, loss = 0.0014669193187728524
iteration 264, loss = 0.0028664846904575825
iteration 265, loss = 0.002488506492227316
iteration 266, loss = 0.0016519497148692608
iteration 267, loss = 0.001845559454523027
iteration 268, loss = 0.002261105226352811
iteration 269, loss = 0.0029462033417075872
iteration 270, loss = 0.003107269061729312
iteration 271, loss = 0.0020999896805733442
iteration 272, loss = 0.002057345351204276
iteration 273, loss = 0.0020182589069008827
iteration 274, loss = 0.002131044864654541
iteration 275, loss = 0.0018637567991390824
iteration 276, loss = 0.0017542069545015693
iteration 277, loss = 0.0022121055517345667
iteration 278, loss = 0.00210525537841022
iteration 279, loss = 0.0020163622684776783
iteration 280, loss = 0.002141134347766638
iteration 281, loss = 0.0038037181366235018
iteration 282, loss = 0.0018687009578570724
iteration 283, loss = 0.0022465393412858248
iteration 284, loss = 0.0018857673276215792
iteration 285, loss = 0.0021571889519691467
iteration 286, loss = 0.0016925677191466093
iteration 287, loss = 0.0018845712766051292
iteration 288, loss = 0.0030484532471746206
iteration 289, loss = 0.002265391405671835
iteration 290, loss = 0.0026701565366238356
iteration 291, loss = 0.003049033461138606
iteration 292, loss = 0.0017736577428877354
iteration 293, loss = 0.0019558784551918507
iteration 294, loss = 0.0019885499496012926
iteration 295, loss = 0.0019697891548275948
iteration 296, loss = 0.002269582124426961
iteration 297, loss = 0.00277437805198133
iteration 298, loss = 0.002034490928053856
iteration 299, loss = 0.0021057198755443096
iteration 300, loss = 0.0024625184014439583
iteration 1, loss = 0.0024696134496480227
iteration 2, loss = 0.0017749398248270154
iteration 3, loss = 0.0021405741572380066
iteration 4, loss = 0.0039022178389132023
iteration 5, loss = 0.002426240360364318
iteration 6, loss = 0.0017664978513494134
iteration 7, loss = 0.0021349815651774406
iteration 8, loss = 0.0022330274805426598
iteration 9, loss = 0.002335959579795599
iteration 10, loss = 0.0019101185025647283
iteration 11, loss = 0.002206571865826845
iteration 12, loss = 0.00189410790335387
iteration 13, loss = 0.0024340490344911814
iteration 14, loss = 0.0032227234914898872
iteration 15, loss = 0.0018683087546378374
iteration 16, loss = 0.0018727321876212955
iteration 17, loss = 0.0019472581334412098
iteration 18, loss = 0.0025581151712685823
iteration 19, loss = 0.0033972770906984806
iteration 20, loss = 0.0018532355315983295
iteration 21, loss = 0.001843722420744598
iteration 22, loss = 0.002897529862821102
iteration 23, loss = 0.0023061337415128946
iteration 24, loss = 0.0019355731783434749
iteration 25, loss = 0.0018137401202693582
iteration 26, loss = 0.0017043668776750565
iteration 27, loss = 0.002023979090154171
iteration 28, loss = 0.0018587502418085933
iteration 29, loss = 0.002319892169907689
iteration 30, loss = 0.0021758645307272673
iteration 31, loss = 0.003210092429071665
iteration 32, loss = 0.0018652265425771475
iteration 33, loss = 0.002614694181829691
iteration 34, loss = 0.0023111007176339626
iteration 35, loss = 0.001886681653559208
iteration 36, loss = 0.0018056323751807213
iteration 37, loss = 0.0027269283309578896
iteration 38, loss = 0.0018811261979863048
iteration 39, loss = 0.0016765890177339315
iteration 40, loss = 0.0019693300127983093
iteration 41, loss = 0.0017499055247753859
iteration 42, loss = 0.001788895227946341
iteration 43, loss = 0.0019520005444064736
iteration 44, loss = 0.001617129542864859
iteration 45, loss = 0.0019459633622318506
iteration 46, loss = 0.0018175776349380612
iteration 47, loss = 0.0017477066721767187
iteration 48, loss = 0.0019197379006072879
iteration 49, loss = 0.0029154671356081963
iteration 50, loss = 0.002082805847749114
iteration 51, loss = 0.0019321403233334422
iteration 52, loss = 0.0016920901834964752
iteration 53, loss = 0.0018608735408633947
iteration 54, loss = 0.001819862867705524
iteration 55, loss = 0.0021876003593206406
iteration 56, loss = 0.0019203213742002845
iteration 57, loss = 0.0018366759177297354
iteration 58, loss = 0.0017813025042414665
iteration 59, loss = 0.002429910469800234
iteration 60, loss = 0.0023014741018414497
iteration 61, loss = 0.003935534041374922
iteration 62, loss = 0.002815524348989129
iteration 63, loss = 0.0022058438044041395
iteration 64, loss = 0.0022149060387164354
iteration 65, loss = 0.0023948291782289743
iteration 66, loss = 0.002312092576175928
iteration 67, loss = 0.002055005868896842
iteration 68, loss = 0.0019114540191367269
iteration 69, loss = 0.0028750270139425993
iteration 70, loss = 0.001917374785989523
iteration 71, loss = 0.0020593442022800446
iteration 72, loss = 0.0020488279405981302
iteration 73, loss = 0.0022788099013268948
iteration 74, loss = 0.001762178260833025
iteration 75, loss = 0.001974507700651884
iteration 76, loss = 0.002197063062340021
iteration 77, loss = 0.002109161112457514
iteration 78, loss = 0.0023161128628998995
iteration 79, loss = 0.002913739299401641
iteration 80, loss = 0.0018233655719086528
iteration 81, loss = 0.0017028327565640211
iteration 82, loss = 0.0026710566598922014
iteration 83, loss = 0.0018669317942112684
iteration 84, loss = 0.0018026534235104918
iteration 85, loss = 0.002422973047941923
iteration 86, loss = 0.0019346424378454685
iteration 87, loss = 0.0019759817514568567
iteration 88, loss = 0.00180439290124923
iteration 89, loss = 0.0026876386255025864
iteration 90, loss = 0.0018202993087470531
iteration 91, loss = 0.0018700039945542812
iteration 92, loss = 0.0020094059873372316
iteration 93, loss = 0.002898038364946842
iteration 94, loss = 0.003899950534105301
iteration 95, loss = 0.005044936202466488
iteration 96, loss = 0.001971979159861803
iteration 97, loss = 0.0018495949916541576
iteration 98, loss = 0.0027900796849280596
iteration 99, loss = 0.002003294415771961
iteration 100, loss = 0.002293709898367524
iteration 101, loss = 0.0018927339697256684
iteration 102, loss = 0.0019786052871495485
iteration 103, loss = 0.0017914671916514635
iteration 104, loss = 0.002091677626594901
iteration 105, loss = 0.0020162235014140606
iteration 106, loss = 0.001940527348779142
iteration 107, loss = 0.0019415346905589104
iteration 108, loss = 0.0034036755096167326
iteration 109, loss = 0.0017148564802482724
iteration 110, loss = 0.0021975398994982243
iteration 111, loss = 0.002199705457314849
iteration 112, loss = 0.001889869337901473
iteration 113, loss = 0.0028165315743535757
iteration 114, loss = 0.0015410587657243013
iteration 115, loss = 0.0041150315664708614
iteration 116, loss = 0.0028428249061107635
iteration 117, loss = 0.0020089363679289818
iteration 118, loss = 0.0016021511983126402
iteration 119, loss = 0.003974189981818199
iteration 120, loss = 0.002141969744116068
iteration 121, loss = 0.0018501768354326487
iteration 122, loss = 0.0016863143537193537
iteration 123, loss = 0.0017937913071364164
iteration 124, loss = 0.0018833141075447202
iteration 125, loss = 0.0018748299917206168
iteration 126, loss = 0.0020764453802257776
iteration 127, loss = 0.001683114911429584
iteration 128, loss = 0.0023137754760682583
iteration 129, loss = 0.0022311601787805557
iteration 130, loss = 0.0016441709594801068
iteration 131, loss = 0.0021193502470850945
iteration 132, loss = 0.001983286812901497
iteration 133, loss = 0.003178154584020376
iteration 134, loss = 0.0029185444582253695
iteration 135, loss = 0.0017602138686925173
iteration 136, loss = 0.002148341853171587
iteration 137, loss = 0.0020987761672586203
iteration 138, loss = 0.0019864141941070557
iteration 139, loss = 0.0020444064866751432
iteration 140, loss = 0.004395271651446819
iteration 141, loss = 0.0020167618058621883
iteration 142, loss = 0.0015806485898792744
iteration 143, loss = 0.00291683292016387
iteration 144, loss = 0.0019373311661183834
iteration 145, loss = 0.0022364691831171513
iteration 146, loss = 0.001959185814484954
iteration 147, loss = 0.0021470978390425444
iteration 148, loss = 0.0020985607989132404
iteration 149, loss = 0.0018031579675152898
iteration 150, loss = 0.0021865030284971
iteration 151, loss = 0.001853436348028481
iteration 152, loss = 0.0019038268364965916
iteration 153, loss = 0.003601094475015998
iteration 154, loss = 0.0017001426313072443
iteration 155, loss = 0.0016794815892353654
iteration 156, loss = 0.0016819547163322568
iteration 157, loss = 0.0035873851738870144
iteration 158, loss = 0.0020405342802405357
iteration 159, loss = 0.0025740168057382107
iteration 160, loss = 0.004404982551932335
iteration 161, loss = 0.002029148628935218
iteration 162, loss = 0.0016744115855544806
iteration 163, loss = 0.0022585175465792418
iteration 164, loss = 0.0037769326008856297
iteration 165, loss = 0.0018509147921577096
iteration 166, loss = 0.003408349584788084
iteration 167, loss = 0.0021965105552226305
iteration 168, loss = 0.0016273120418190956
iteration 169, loss = 0.002145324135199189
iteration 170, loss = 0.002512912731617689
iteration 171, loss = 0.0021495826076716185
iteration 172, loss = 0.002249375218525529
iteration 173, loss = 0.001906093442812562
iteration 174, loss = 0.003652770770713687
iteration 175, loss = 0.0015167284291237593
iteration 176, loss = 0.0018010700587183237
iteration 177, loss = 0.002468384336680174
iteration 178, loss = 0.0032115676440298557
iteration 179, loss = 0.0020761629566550255
iteration 180, loss = 0.0022339695133268833
iteration 181, loss = 0.0017927327426150441
iteration 182, loss = 0.0021077559795230627
iteration 183, loss = 0.001833883230574429
iteration 184, loss = 0.002104345243424177
iteration 185, loss = 0.001736324280500412
iteration 186, loss = 0.002836441621184349
iteration 187, loss = 0.001666221534833312
iteration 188, loss = 0.0022399125155061483
iteration 189, loss = 0.001640548463910818
iteration 190, loss = 0.0026582027785480022
iteration 191, loss = 0.0025865063071250916
iteration 192, loss = 0.002878979779779911
iteration 193, loss = 0.0019863895140588284
iteration 194, loss = 0.001980715664103627
iteration 195, loss = 0.0017377367476001382
iteration 196, loss = 0.0030888866167515516
iteration 197, loss = 0.002018153667449951
iteration 198, loss = 0.0019061948405578732
iteration 199, loss = 0.0025104894302785397
iteration 200, loss = 0.001944120740517974
iteration 201, loss = 0.0021657394245266914
iteration 202, loss = 0.002330807037651539
iteration 203, loss = 0.002294478937983513
iteration 204, loss = 0.0020285388454794884
iteration 205, loss = 0.004495557863265276
iteration 206, loss = 0.002076643519103527
iteration 207, loss = 0.0019426231738179922
iteration 208, loss = 0.0019191527971997857
iteration 209, loss = 0.002421311568468809
iteration 210, loss = 0.0017872692551463842
iteration 211, loss = 0.0038907169364392757
iteration 212, loss = 0.0022040707990527153
iteration 213, loss = 0.002465273020789027
iteration 214, loss = 0.0017556477105244994
iteration 215, loss = 0.002357903402298689
iteration 216, loss = 0.0019873646087944508
iteration 217, loss = 0.0021195486187934875
iteration 218, loss = 0.0030893320217728615
iteration 219, loss = 0.001966293668374419
iteration 220, loss = 0.002635437296703458
iteration 221, loss = 0.002136939438059926
iteration 222, loss = 0.0021914858371019363
iteration 223, loss = 0.001647316850721836
iteration 224, loss = 0.0021696477197110653
iteration 225, loss = 0.0026024559047073126
iteration 226, loss = 0.0024225637316703796
iteration 227, loss = 0.0017649057554081082
iteration 228, loss = 0.0034005464985966682
iteration 229, loss = 0.002662024460732937
iteration 230, loss = 0.0027304908726364374
iteration 231, loss = 0.0015172319253906608
iteration 232, loss = 0.0019466446246951818
iteration 233, loss = 0.001704904716461897
iteration 234, loss = 0.0030225026421248913
iteration 235, loss = 0.002413996495306492
iteration 236, loss = 0.0035516475327312946
iteration 237, loss = 0.0019997614435851574
iteration 238, loss = 0.0018424789886921644
iteration 239, loss = 0.0016775332624092698
iteration 240, loss = 0.0020666064228862524
iteration 241, loss = 0.002438971307128668
iteration 242, loss = 0.0019065074156969786
iteration 243, loss = 0.0027949500363320112
iteration 244, loss = 0.0020471157040446997
iteration 245, loss = 0.0016954992897808552
iteration 246, loss = 0.0028741308487951756
iteration 247, loss = 0.0028407161589711905
iteration 248, loss = 0.0015504355542361736
iteration 249, loss = 0.0020195147953927517
iteration 250, loss = 0.0021739560179412365
iteration 251, loss = 0.002383959013968706
iteration 252, loss = 0.0016732902731746435
iteration 253, loss = 0.0025951266288757324
iteration 254, loss = 0.0023959996178746223
iteration 255, loss = 0.001939610461704433
iteration 256, loss = 0.0019616112112998962
iteration 257, loss = 0.0025652300100773573
iteration 258, loss = 0.002707388484850526
iteration 259, loss = 0.0019939206540584564
iteration 260, loss = 0.001686021569184959
iteration 261, loss = 0.0018804047722369432
iteration 262, loss = 0.0017138754483312368
iteration 263, loss = 0.001974124927073717
iteration 264, loss = 0.0017933930503204465
iteration 265, loss = 0.004023268353193998
iteration 266, loss = 0.0016006316291168332
iteration 267, loss = 0.0025039997417479753
iteration 268, loss = 0.003149922238662839
iteration 269, loss = 0.0022940640337765217
iteration 270, loss = 0.002021790249273181
iteration 271, loss = 0.001617065747268498
iteration 272, loss = 0.003761722007766366
iteration 273, loss = 0.0018671054858714342
iteration 274, loss = 0.003064575372263789
iteration 275, loss = 0.0018129439558833838
iteration 276, loss = 0.002098462078720331
iteration 277, loss = 0.003928472753614187
iteration 278, loss = 0.0022492625284940004
iteration 279, loss = 0.003762373700737953
iteration 280, loss = 0.002114713191986084
iteration 281, loss = 0.002244755160063505
iteration 282, loss = 0.0017048859735950828
iteration 283, loss = 0.004406143445521593
iteration 284, loss = 0.0017228317447006702
iteration 285, loss = 0.0037306956946849823
iteration 286, loss = 0.0031645912677049637
iteration 287, loss = 0.0031986720860004425
iteration 288, loss = 0.0020340364426374435
iteration 289, loss = 0.002290083561092615
iteration 290, loss = 0.001941195921972394
iteration 291, loss = 0.004939397796988487
iteration 292, loss = 0.0020375156309455633
iteration 293, loss = 0.0037526532541960478
iteration 294, loss = 0.002128290943801403
iteration 295, loss = 0.001829843851737678
iteration 296, loss = 0.0017410913715139031
iteration 297, loss = 0.0020354092121124268
iteration 298, loss = 0.002170325955376029
iteration 299, loss = 0.001787643413990736
iteration 300, loss = 0.0015760118840262294
iteration 1, loss = 0.002103543607518077
iteration 2, loss = 0.003929866477847099
iteration 3, loss = 0.0020763897337019444
iteration 4, loss = 0.0019517061300575733
iteration 5, loss = 0.0024981594178825617
iteration 6, loss = 0.0020299318712204695
iteration 7, loss = 0.0016243309946730733
iteration 8, loss = 0.0030116443522274494
iteration 9, loss = 0.0019073054427281022
iteration 10, loss = 0.0029868839774280787
iteration 11, loss = 0.001639719121158123
iteration 12, loss = 0.001792683033272624
iteration 13, loss = 0.0027478730771690607
iteration 14, loss = 0.0024052460212260485
iteration 15, loss = 0.001935740583576262
iteration 16, loss = 0.0017392588779330254
iteration 17, loss = 0.0016170857707038522
iteration 18, loss = 0.002535439794883132
iteration 19, loss = 0.002956978976726532
iteration 20, loss = 0.002387088257819414
iteration 21, loss = 0.0024966096971184015
iteration 22, loss = 0.0026085854042321444
iteration 23, loss = 0.0019796371925622225
iteration 24, loss = 0.002489679493010044
iteration 25, loss = 0.002493495587259531
iteration 26, loss = 0.0022016954608261585
iteration 27, loss = 0.0019454702269285917
iteration 28, loss = 0.0019244924187660217
iteration 29, loss = 0.001728793722577393
iteration 30, loss = 0.0018716729246079922
iteration 31, loss = 0.002457906026393175
iteration 32, loss = 0.0025830173399299383
iteration 33, loss = 0.0032343657221645117
iteration 34, loss = 0.0022422338370233774
iteration 35, loss = 0.0026435223408043385
iteration 36, loss = 0.0037237447686493397
iteration 37, loss = 0.0032469069119542837
iteration 38, loss = 0.0019414707785472274
iteration 39, loss = 0.0019638536032289267
iteration 40, loss = 0.0022045935038477182
iteration 41, loss = 0.003028025384992361
iteration 42, loss = 0.0016682111890986562
iteration 43, loss = 0.002456854097545147
iteration 44, loss = 0.0034518735483288765
iteration 45, loss = 0.0018607261590659618
iteration 46, loss = 0.001714884303510189
iteration 47, loss = 0.002631496638059616
iteration 48, loss = 0.002607809379696846
iteration 49, loss = 0.00212764460593462
iteration 50, loss = 0.003667560638859868
iteration 51, loss = 0.0021291724406182766
iteration 52, loss = 0.0018902677111327648
iteration 53, loss = 0.0017230358207598329
iteration 54, loss = 0.003628751263022423
iteration 55, loss = 0.0021047357004135847
iteration 56, loss = 0.0034006903879344463
iteration 57, loss = 0.0018949373625218868
iteration 58, loss = 0.0018453600350767374
iteration 59, loss = 0.002087309956550598
iteration 60, loss = 0.0017604425083845854
iteration 61, loss = 0.0016451835399493575
iteration 62, loss = 0.0022541952785104513
iteration 63, loss = 0.001797957462258637
iteration 64, loss = 0.0016843369230628014
iteration 65, loss = 0.0015719895018264651
iteration 66, loss = 0.0019935774616897106
iteration 67, loss = 0.002080333884805441
iteration 68, loss = 0.0020074998028576374
iteration 69, loss = 0.0023166178725659847
iteration 70, loss = 0.0017217438435181975
iteration 71, loss = 0.0018273105379194021
iteration 72, loss = 0.0016793529503047466
iteration 73, loss = 0.0021586285438388586
iteration 74, loss = 0.0018788896268233657
iteration 75, loss = 0.001881617121398449
iteration 76, loss = 0.0019371057860553265
iteration 77, loss = 0.00264094234444201
iteration 78, loss = 0.0018932217499241233
iteration 79, loss = 0.0017416157061234117
iteration 80, loss = 0.0017152114305645227
iteration 81, loss = 0.0019181143725290895
iteration 82, loss = 0.0018108384683728218
iteration 83, loss = 0.0017270268872380257
iteration 84, loss = 0.0017820425564423203
iteration 85, loss = 0.0017447881400585175
iteration 86, loss = 0.0016378596192225814
iteration 87, loss = 0.0016141652595251799
iteration 88, loss = 0.002715763868764043
iteration 89, loss = 0.002348114736378193
iteration 90, loss = 0.0018953544786199927
iteration 91, loss = 0.0017826034454628825
iteration 92, loss = 0.0037368249613791704
iteration 93, loss = 0.002349304733797908
iteration 94, loss = 0.0026729912497103214
iteration 95, loss = 0.0017702050972729921
iteration 96, loss = 0.0018988241208717227
iteration 97, loss = 0.001764704822562635
iteration 98, loss = 0.0018471525982022285
iteration 99, loss = 0.0023767182137817144
iteration 100, loss = 0.0021252448204904795
iteration 101, loss = 0.001767056412063539
iteration 102, loss = 0.0021231165155768394
iteration 103, loss = 0.0027341260574758053
iteration 104, loss = 0.002191439038142562
iteration 105, loss = 0.0020192053634673357
iteration 106, loss = 0.0037211906164884567
iteration 107, loss = 0.001805924461223185
iteration 108, loss = 0.001866225153207779
iteration 109, loss = 0.0018210376147180796
iteration 110, loss = 0.0019077425822615623
iteration 111, loss = 0.001804795814678073
iteration 112, loss = 0.003921764902770519
iteration 113, loss = 0.0032011978328227997
iteration 114, loss = 0.0020606513135135174
iteration 115, loss = 0.0018382048001512885
iteration 116, loss = 0.004024476744234562
iteration 117, loss = 0.0021516582928597927
iteration 118, loss = 0.001778975361958146
iteration 119, loss = 0.00205901637673378
iteration 120, loss = 0.0024753520265221596
iteration 121, loss = 0.001769926631823182
iteration 122, loss = 0.0023541562259197235
iteration 123, loss = 0.002006569877266884
iteration 124, loss = 0.001994142774492502
iteration 125, loss = 0.003028102219104767
iteration 126, loss = 0.001978459767997265
iteration 127, loss = 0.0017588668270036578
iteration 128, loss = 0.0019131047884002328
iteration 129, loss = 0.0037483787164092064
iteration 130, loss = 0.0018110844539478421
iteration 131, loss = 0.0019197768997401
iteration 132, loss = 0.001926689874380827
iteration 133, loss = 0.0028841188177466393
iteration 134, loss = 0.002886183327063918
iteration 135, loss = 0.0016873941058292985
iteration 136, loss = 0.0023576635867357254
iteration 137, loss = 0.0018978462321683764
iteration 138, loss = 0.0017837771447375417
iteration 139, loss = 0.003335949033498764
iteration 140, loss = 0.002354954369366169
iteration 141, loss = 0.0022009103558957577
iteration 142, loss = 0.0018942884635180235
iteration 143, loss = 0.002485494827851653
iteration 144, loss = 0.0025390188675373793
iteration 145, loss = 0.0019083950901404023
iteration 146, loss = 0.0018797207158058882
iteration 147, loss = 0.001934441039338708
iteration 148, loss = 0.0028800279833376408
iteration 149, loss = 0.001930685481056571
iteration 150, loss = 0.0020396029576659203
iteration 151, loss = 0.00391846988350153
iteration 152, loss = 0.0019155375193804502
iteration 153, loss = 0.004043444525450468
iteration 154, loss = 0.0018274601316079497
iteration 155, loss = 0.002029330236837268
iteration 156, loss = 0.0014312954153865576
iteration 157, loss = 0.0017130455235019326
iteration 158, loss = 0.0020845404360443354
iteration 159, loss = 0.0027917539700865746
iteration 160, loss = 0.0022013604175299406
iteration 161, loss = 0.0025923713110387325
iteration 162, loss = 0.001750798779539764
iteration 163, loss = 0.0017393046291545033
iteration 164, loss = 0.0025493872817605734
iteration 165, loss = 0.0025832271203398705
iteration 166, loss = 0.002736737485975027
iteration 167, loss = 0.001796753378584981
iteration 168, loss = 0.0017970497719943523
iteration 169, loss = 0.0018995143473148346
iteration 170, loss = 0.002431731205433607
iteration 171, loss = 0.0021984807681292295
iteration 172, loss = 0.0019526274409145117
iteration 173, loss = 0.002226246288046241
iteration 174, loss = 0.002259638160467148
iteration 175, loss = 0.0020412772428244352
iteration 176, loss = 0.002067078370600939
iteration 177, loss = 0.00192617392167449
iteration 178, loss = 0.0021649920381605625
iteration 179, loss = 0.0020701251924037933
iteration 180, loss = 0.002560672117397189
iteration 181, loss = 0.0018557222792878747
iteration 182, loss = 0.002256606938317418
iteration 183, loss = 0.00201011192984879
iteration 184, loss = 0.005238459445536137
iteration 185, loss = 0.0021747855935245752
iteration 186, loss = 0.0022786795161664486
iteration 187, loss = 0.0019002556800842285
iteration 188, loss = 0.001910060876980424
iteration 189, loss = 0.003649036632850766
iteration 190, loss = 0.002199394628405571
iteration 191, loss = 0.001619627932086587
iteration 192, loss = 0.0020745107904076576
iteration 193, loss = 0.002358971629291773
iteration 194, loss = 0.0018164855428040028
iteration 195, loss = 0.0019442238844931126
iteration 196, loss = 0.001965582836419344
iteration 197, loss = 0.0020362327340990305
iteration 198, loss = 0.0016464738873764873
iteration 199, loss = 0.002825723262503743
iteration 200, loss = 0.001944958814419806
iteration 201, loss = 0.0039383783005177975
iteration 202, loss = 0.0018556560389697552
iteration 203, loss = 0.001991949276998639
iteration 204, loss = 0.0015367807354778051
iteration 205, loss = 0.0017937843222171068
iteration 206, loss = 0.0018177783349528909
iteration 207, loss = 0.0021728486754000187
iteration 208, loss = 0.002305187052115798
iteration 209, loss = 0.002173132263123989
iteration 210, loss = 0.002621012507006526
iteration 211, loss = 0.002198945265263319
iteration 212, loss = 0.0019231503829360008
iteration 213, loss = 0.002546856878325343
iteration 214, loss = 0.002271476434543729
iteration 215, loss = 0.002186155878007412
iteration 216, loss = 0.0020024185068905354
iteration 217, loss = 0.0020782731007784605
iteration 218, loss = 0.003444987116381526
iteration 219, loss = 0.002190802711993456
iteration 220, loss = 0.0017832554876804352
iteration 221, loss = 0.0017737692687660456
iteration 222, loss = 0.0027215019799768925
iteration 223, loss = 0.002421230310574174
iteration 224, loss = 0.0022027669474482536
iteration 225, loss = 0.0034150397405028343
iteration 226, loss = 0.00210976623930037
iteration 227, loss = 0.002039617858827114
iteration 228, loss = 0.0019201865652576089
iteration 229, loss = 0.003250671550631523
iteration 230, loss = 0.0016123696696013212
iteration 231, loss = 0.002420651726424694
iteration 232, loss = 0.0017522340640425682
iteration 233, loss = 0.0017945892177522182
iteration 234, loss = 0.0025398493744432926
iteration 235, loss = 0.0026310086250305176
iteration 236, loss = 0.0017756952438503504
iteration 237, loss = 0.00423657801002264
iteration 238, loss = 0.002396163297817111
iteration 239, loss = 0.0019856509752571583
iteration 240, loss = 0.0018673436716198921
iteration 241, loss = 0.002156077418476343
iteration 242, loss = 0.001634778454899788
iteration 243, loss = 0.002834866289049387
iteration 244, loss = 0.0019742900040000677
iteration 245, loss = 0.00164851697627455
iteration 246, loss = 0.0016522295773029327
iteration 247, loss = 0.001832685898989439
iteration 248, loss = 0.002402285812422633
iteration 249, loss = 0.004073896445333958
iteration 250, loss = 0.0019198297522962093
iteration 251, loss = 0.002754407236352563
iteration 252, loss = 0.0018076746491715312
iteration 253, loss = 0.0025107692927122116
iteration 254, loss = 0.001724177855066955
iteration 255, loss = 0.0018740404630079865
iteration 256, loss = 0.002270803786814213
iteration 257, loss = 0.0017381226643919945
iteration 258, loss = 0.004128694534301758
iteration 259, loss = 0.002033487893640995
iteration 260, loss = 0.0018249924760311842
iteration 261, loss = 0.003475531004369259
iteration 262, loss = 0.0020984618458896875
iteration 263, loss = 0.0017284282948821783
iteration 264, loss = 0.002915354911237955
iteration 265, loss = 0.0018117005238309503
iteration 266, loss = 0.0019610063172876835
iteration 267, loss = 0.0016136926133185625
iteration 268, loss = 0.004188153427094221
iteration 269, loss = 0.0015945652266964316
iteration 270, loss = 0.0019921299535781145
iteration 271, loss = 0.003265966661274433
iteration 272, loss = 0.0018306431593373418
iteration 273, loss = 0.002099404577165842
iteration 274, loss = 0.0023706001229584217
iteration 275, loss = 0.0019774497486650944
iteration 276, loss = 0.003305667545646429
iteration 277, loss = 0.003547009779140353
iteration 278, loss = 0.0031123976223170757
iteration 279, loss = 0.0019589245785027742
iteration 280, loss = 0.0019534104503691196
iteration 281, loss = 0.0019831135869026184
iteration 282, loss = 0.0022821337915956974
iteration 283, loss = 0.0033090412616729736
iteration 284, loss = 0.003823437262326479
iteration 285, loss = 0.001923260511830449
iteration 286, loss = 0.003084781114012003
iteration 287, loss = 0.0022685921285301447
iteration 288, loss = 0.0020237909629940987
iteration 289, loss = 0.0027256240136921406
iteration 290, loss = 0.001936273998580873
iteration 291, loss = 0.002025990979745984
iteration 292, loss = 0.0020680066663771868
iteration 293, loss = 0.0038248065393418074
iteration 294, loss = 0.0026766513474285603
iteration 295, loss = 0.0032382949721068144
iteration 296, loss = 0.0023725219070911407
iteration 297, loss = 0.0017371869180351496
iteration 298, loss = 0.0021001924760639668
iteration 299, loss = 0.001733659883029759
iteration 300, loss = 0.002936600474640727
iteration 1, loss = 0.0025437427684664726
iteration 2, loss = 0.0021557386498898268
iteration 3, loss = 0.0019708327017724514
iteration 4, loss = 0.003500412218272686
iteration 5, loss = 0.001847593579441309
iteration 6, loss = 0.003480566432699561
iteration 7, loss = 0.001544689992442727
iteration 8, loss = 0.002087913453578949
iteration 9, loss = 0.0032344586215913296
iteration 10, loss = 0.00205445964820683
iteration 11, loss = 0.002158538671210408
iteration 12, loss = 0.0022369723301380873
iteration 13, loss = 0.0017773894360288978
iteration 14, loss = 0.0020195823162794113
iteration 15, loss = 0.0024302443489432335
iteration 16, loss = 0.0020600121933966875
iteration 17, loss = 0.002146430080756545
iteration 18, loss = 0.0032559530809521675
iteration 19, loss = 0.0021512042731046677
iteration 20, loss = 0.001812916249036789
iteration 21, loss = 0.0022790085058659315
iteration 22, loss = 0.003072143765166402
iteration 23, loss = 0.0021815334912389517
iteration 24, loss = 0.0017397352494299412
iteration 25, loss = 0.0018410140182822943
iteration 26, loss = 0.0026886032428592443
iteration 27, loss = 0.0030205994844436646
iteration 28, loss = 0.0017936748918145895
iteration 29, loss = 0.002533084712922573
iteration 30, loss = 0.0016398723237216473
iteration 31, loss = 0.0016245633596554399
iteration 32, loss = 0.0021573747508227825
iteration 33, loss = 0.0018044844036921859
iteration 34, loss = 0.0017792620928958058
iteration 35, loss = 0.002346952212974429
iteration 36, loss = 0.0029409208800643682
iteration 37, loss = 0.0022299890406429768
iteration 38, loss = 0.00180895219091326
iteration 39, loss = 0.002131507033482194
iteration 40, loss = 0.0018271266017109156
iteration 41, loss = 0.0018199968617409468
iteration 42, loss = 0.0017360098427161574
iteration 43, loss = 0.0016670048935338855
iteration 44, loss = 0.002132679335772991
iteration 45, loss = 0.001824504230171442
iteration 46, loss = 0.0035735329147428274
iteration 47, loss = 0.0024577693548053503
iteration 48, loss = 0.002093984978273511
iteration 49, loss = 0.001931999810039997
iteration 50, loss = 0.0019019772298634052
iteration 51, loss = 0.002765119541436434
iteration 52, loss = 0.0019352835370227695
iteration 53, loss = 0.0017205950571224093
iteration 54, loss = 0.0018155518919229507
iteration 55, loss = 0.0017361202044412494
iteration 56, loss = 0.0021397126838564873
iteration 57, loss = 0.0031959484331309795
iteration 58, loss = 0.00209835940040648
iteration 59, loss = 0.0017806431278586388
iteration 60, loss = 0.0032072144094854593
iteration 61, loss = 0.0022075511515140533
iteration 62, loss = 0.0020220852456986904
iteration 63, loss = 0.0016782808816060424
iteration 64, loss = 0.001746402122080326
iteration 65, loss = 0.0020874738693237305
iteration 66, loss = 0.002357898047193885
iteration 67, loss = 0.004003431648015976
iteration 68, loss = 0.0018014447996392846
iteration 69, loss = 0.0021673995070159435
iteration 70, loss = 0.002129883039742708
iteration 71, loss = 0.001684685004875064
iteration 72, loss = 0.003990283235907555
iteration 73, loss = 0.002507454715669155
iteration 74, loss = 0.0038730711676180363
iteration 75, loss = 0.0026686005294322968
iteration 76, loss = 0.0020924718119204044
iteration 77, loss = 0.0031920899637043476
iteration 78, loss = 0.002128270221874118
iteration 79, loss = 0.002059259917587042
iteration 80, loss = 0.002829248085618019
iteration 81, loss = 0.002004835521802306
iteration 82, loss = 0.003980097360908985
iteration 83, loss = 0.0017083027632907033
iteration 84, loss = 0.0017457441426813602
iteration 85, loss = 0.0031731626950204372
iteration 86, loss = 0.0022856718860566616
iteration 87, loss = 0.0018174332799389958
iteration 88, loss = 0.0020090057514607906
iteration 89, loss = 0.0017594691598787904
iteration 90, loss = 0.0019435720751062036
iteration 91, loss = 0.0027532773092389107
iteration 92, loss = 0.0019124189857393503
iteration 93, loss = 0.0023116900119930506
iteration 94, loss = 0.0018501002341508865
iteration 95, loss = 0.002244378440082073
iteration 96, loss = 0.0021178529132157564
iteration 97, loss = 0.0021672651637345552
iteration 98, loss = 0.0019284519366919994
iteration 99, loss = 0.0019146432168781757
iteration 100, loss = 0.0020224046893417835
iteration 101, loss = 0.0021284357644617558
iteration 102, loss = 0.0016364770708605647
iteration 103, loss = 0.0027235602028667927
iteration 104, loss = 0.0017006179550662637
iteration 105, loss = 0.002374386414885521
iteration 106, loss = 0.0027618124149739742
iteration 107, loss = 0.0016975677572190762
iteration 108, loss = 0.002727439161390066
iteration 109, loss = 0.0015196253079921007
iteration 110, loss = 0.0031181846279650927
iteration 111, loss = 0.0020809590350836515
iteration 112, loss = 0.0018498627468943596
iteration 113, loss = 0.0020958730019629
iteration 114, loss = 0.0024003235157579184
iteration 115, loss = 0.0027769829612225294
iteration 116, loss = 0.004015620332211256
iteration 117, loss = 0.004167506005614996
iteration 118, loss = 0.002236562315374613
iteration 119, loss = 0.0019007277442142367
iteration 120, loss = 0.002157386625185609
iteration 121, loss = 0.0023275529965758324
iteration 122, loss = 0.0018134588608518243
iteration 123, loss = 0.004047191236168146
iteration 124, loss = 0.002301212167367339
iteration 125, loss = 0.0017798201879486442
iteration 126, loss = 0.0023576910607516766
iteration 127, loss = 0.0028068453539162874
iteration 128, loss = 0.002925773151218891
iteration 129, loss = 0.0020876508206129074
iteration 130, loss = 0.0019708445761352777
iteration 131, loss = 0.001963826362043619
iteration 132, loss = 0.001961661037057638
iteration 133, loss = 0.002715082373470068
iteration 134, loss = 0.0018094999250024557
iteration 135, loss = 0.0032769041135907173
iteration 136, loss = 0.001978058833628893
iteration 137, loss = 0.002155095571652055
iteration 138, loss = 0.0017348132096230984
iteration 139, loss = 0.002129075350239873
iteration 140, loss = 0.0020093796774744987
iteration 141, loss = 0.004758070223033428
iteration 142, loss = 0.002129715634509921
iteration 143, loss = 0.0030203377828001976
iteration 144, loss = 0.0019947064574807882
iteration 145, loss = 0.0017381637590005994
iteration 146, loss = 0.001985584618523717
iteration 147, loss = 0.0019055148586630821
iteration 148, loss = 0.0018110384698957205
iteration 149, loss = 0.0030006561428308487
iteration 150, loss = 0.001991403056308627
iteration 151, loss = 0.0015074261464178562
iteration 152, loss = 0.0016462767962366343
iteration 153, loss = 0.0035766116343438625
iteration 154, loss = 0.0032009759452193975
iteration 155, loss = 0.00273797451518476
iteration 156, loss = 0.0019416153663769364
iteration 157, loss = 0.0023205431643873453
iteration 158, loss = 0.0018879867857322097
iteration 159, loss = 0.0018658876651898026
iteration 160, loss = 0.0037349185440689325
iteration 161, loss = 0.001571724540553987
iteration 162, loss = 0.0023943481501191854
iteration 163, loss = 0.001751455245539546
iteration 164, loss = 0.0025592667516320944
iteration 165, loss = 0.0018503773026168346
iteration 166, loss = 0.0026039276272058487
iteration 167, loss = 0.003226929111406207
iteration 168, loss = 0.0020305197685956955
iteration 169, loss = 0.002031793585047126
iteration 170, loss = 0.0019441584590822458
iteration 171, loss = 0.0026732508558779955
iteration 172, loss = 0.002812715480104089
iteration 173, loss = 0.003027921076864004
iteration 174, loss = 0.0018398616230115294
iteration 175, loss = 0.001952414633706212
iteration 176, loss = 0.0018679371569305658
iteration 177, loss = 0.0029032835736870766
iteration 178, loss = 0.0018060988513752818
iteration 179, loss = 0.0026801968924701214
iteration 180, loss = 0.0026637762784957886
iteration 181, loss = 0.0015972157707437873
iteration 182, loss = 0.0020015586633235216
iteration 183, loss = 0.0015979665331542492
iteration 184, loss = 0.0017187396297231317
iteration 185, loss = 0.002240143483504653
iteration 186, loss = 0.002317570149898529
iteration 187, loss = 0.0018606657395139337
iteration 188, loss = 0.0017315397271886468
iteration 189, loss = 0.0023422655649483204
iteration 190, loss = 0.0023732769768685102
iteration 191, loss = 0.0019107075640931726
iteration 192, loss = 0.0022825333289802074
iteration 193, loss = 0.0023958866950124502
iteration 194, loss = 0.0018413618672639132
iteration 195, loss = 0.0019564093090593815
iteration 196, loss = 0.0018836933886632323
iteration 197, loss = 0.0022925862576812506
iteration 198, loss = 0.002092741196975112
iteration 199, loss = 0.0021099045407027006
iteration 200, loss = 0.0037933397106826305
iteration 201, loss = 0.002576508792117238
iteration 202, loss = 0.00203206529840827
iteration 203, loss = 0.001884083729237318
iteration 204, loss = 0.0024649586994200945
iteration 205, loss = 0.002476784400641918
iteration 206, loss = 0.0037630905862897635
iteration 207, loss = 0.0020943491254001856
iteration 208, loss = 0.0029036742635071278
iteration 209, loss = 0.0019974324386566877
iteration 210, loss = 0.001629855833016336
iteration 211, loss = 0.003898546565324068
iteration 212, loss = 0.0018948728684335947
iteration 213, loss = 0.0025336574763059616
iteration 214, loss = 0.0020866349805146456
iteration 215, loss = 0.0018873035442084074
iteration 216, loss = 0.0021644579246640205
iteration 217, loss = 0.0026033520698547363
iteration 218, loss = 0.001887756516225636
iteration 219, loss = 0.0015647844411432743
iteration 220, loss = 0.002003330970183015
iteration 221, loss = 0.0020794719457626343
iteration 222, loss = 0.0021883922163397074
iteration 223, loss = 0.0034074350260198116
iteration 224, loss = 0.0025331571232527494
iteration 225, loss = 0.00223081954754889
iteration 226, loss = 0.0023717882577329874
iteration 227, loss = 0.0018090865341946483
iteration 228, loss = 0.0018184827640652657
iteration 229, loss = 0.0021769553422927856
iteration 230, loss = 0.002424759790301323
iteration 231, loss = 0.0035107522271573544
iteration 232, loss = 0.0017811459256336093
iteration 233, loss = 0.0020337114110589027
iteration 234, loss = 0.0025128666311502457
iteration 235, loss = 0.002024122979491949
iteration 236, loss = 0.0021922120358794928
iteration 237, loss = 0.0023878165520727634
iteration 238, loss = 0.0020971549674868584
iteration 239, loss = 0.0017535907682031393
iteration 240, loss = 0.0019104181556031108
iteration 241, loss = 0.0029875850304961205
iteration 242, loss = 0.0019287080504000187
iteration 243, loss = 0.001693241880275309
iteration 244, loss = 0.001750732073560357
iteration 245, loss = 0.0019363430328667164
iteration 246, loss = 0.0020268033258616924
iteration 247, loss = 0.001983936410397291
iteration 248, loss = 0.0019861990585923195
iteration 249, loss = 0.001958031440153718
iteration 250, loss = 0.002300483174622059
iteration 251, loss = 0.002150881802663207
iteration 252, loss = 0.0020117354579269886
iteration 253, loss = 0.0032348386012017727
iteration 254, loss = 0.002199222333729267
iteration 255, loss = 0.0033202925696969032
iteration 256, loss = 0.002160485601052642
iteration 257, loss = 0.001965600997209549
iteration 258, loss = 0.0016485899686813354
iteration 259, loss = 0.0020517203956842422
iteration 260, loss = 0.0017500504618510604
iteration 261, loss = 0.0018789159366860986
iteration 262, loss = 0.003776499070227146
iteration 263, loss = 0.001828707056120038
iteration 264, loss = 0.0017162233125418425
iteration 265, loss = 0.0019073060248047113
iteration 266, loss = 0.001791623653843999
iteration 267, loss = 0.001789156929589808
iteration 268, loss = 0.0019394224509596825
iteration 269, loss = 0.0027608098462224007
iteration 270, loss = 0.002670380985364318
iteration 271, loss = 0.0016961321234703064
iteration 272, loss = 0.0017703770427033305
iteration 273, loss = 0.002866980619728565
iteration 274, loss = 0.0017658901633694768
iteration 275, loss = 0.001946970820426941
iteration 276, loss = 0.0022095225285738707
iteration 277, loss = 0.0014312707353383303
iteration 278, loss = 0.0020985601004213095
iteration 279, loss = 0.0018551961984485388
iteration 280, loss = 0.003336366731673479
iteration 281, loss = 0.004146934021264315
iteration 282, loss = 0.0023519075475633144
iteration 283, loss = 0.003364893840625882
iteration 284, loss = 0.002936066361144185
iteration 285, loss = 0.002312472090125084
iteration 286, loss = 0.0028893165290355682
iteration 287, loss = 0.0019755824469029903
iteration 288, loss = 0.0019355153199285269
iteration 289, loss = 0.0021004893351346254
iteration 290, loss = 0.00163942645303905
iteration 291, loss = 0.002511003753170371
iteration 292, loss = 0.0022424343042075634
iteration 293, loss = 0.003359616734087467
iteration 294, loss = 0.0024776586797088385
iteration 295, loss = 0.0015749295707792044
iteration 296, loss = 0.0018752639880403876
iteration 297, loss = 0.002202631440013647
iteration 298, loss = 0.0017987099708989263
iteration 299, loss = 0.0022486697416752577
iteration 300, loss = 0.0020908229053020477
iteration 1, loss = 0.0040541235357522964
iteration 2, loss = 0.001898686750791967
iteration 3, loss = 0.0019772611558437347
iteration 4, loss = 0.0023847436532378197
iteration 5, loss = 0.0021255463361740112
iteration 6, loss = 0.0016212896443903446
iteration 7, loss = 0.0021047904156148434
iteration 8, loss = 0.0021686162799596786
iteration 9, loss = 0.0020375773310661316
iteration 10, loss = 0.002199301030486822
iteration 11, loss = 0.0018530742963775992
iteration 12, loss = 0.0018306716810911894
iteration 13, loss = 0.00190242740791291
iteration 14, loss = 0.0038152653723955154
iteration 15, loss = 0.0017535077640786767
iteration 16, loss = 0.0019750436767935753
iteration 17, loss = 0.002045066561549902
iteration 18, loss = 0.0022770059294998646
iteration 19, loss = 0.0018657792825251818
iteration 20, loss = 0.003500426886603236
iteration 21, loss = 0.0023804493248462677
iteration 22, loss = 0.0014357814798131585
iteration 23, loss = 0.0019441992044448853
iteration 24, loss = 0.002677207114174962
iteration 25, loss = 0.0036606104113161564
iteration 26, loss = 0.0019762623123824596
iteration 27, loss = 0.0020458539947867393
iteration 28, loss = 0.0016403375193476677
iteration 29, loss = 0.0018225866369903088
iteration 30, loss = 0.0017789078410714865
iteration 31, loss = 0.001877676579169929
iteration 32, loss = 0.002551000565290451
iteration 33, loss = 0.0018137574661523104
iteration 34, loss = 0.0023405614774674177
iteration 35, loss = 0.0020520405378192663
iteration 36, loss = 0.0017906279535964131
iteration 37, loss = 0.0018250050488859415
iteration 38, loss = 0.0016323709860444069
iteration 39, loss = 0.0032977601513266563
iteration 40, loss = 0.0020886831916868687
iteration 41, loss = 0.0033362824469804764
iteration 42, loss = 0.0038647903129458427
iteration 43, loss = 0.0017199733993038535
iteration 44, loss = 0.0018536625429987907
iteration 45, loss = 0.001809753943234682
iteration 46, loss = 0.001738645019941032
iteration 47, loss = 0.0024409322068095207
iteration 48, loss = 0.0020302284974604845
iteration 49, loss = 0.0016097822226583958
iteration 50, loss = 0.0026922549586743116
iteration 51, loss = 0.002180384937673807
iteration 52, loss = 0.0023172348737716675
iteration 53, loss = 0.002631882904097438
iteration 54, loss = 0.002586562652140856
iteration 55, loss = 0.0036657580640167
iteration 56, loss = 0.0019994887989014387
iteration 57, loss = 0.0020980602130293846
iteration 58, loss = 0.0017931238980963826
iteration 59, loss = 0.0017815879546105862
iteration 60, loss = 0.0015417591203004122
iteration 61, loss = 0.001769548049196601
iteration 62, loss = 0.0019239319954067469
iteration 63, loss = 0.002204533899202943
iteration 64, loss = 0.0032863751985132694
iteration 65, loss = 0.0018496739212423563
iteration 66, loss = 0.003916345536708832
iteration 67, loss = 0.0019934852607548237
iteration 68, loss = 0.001719114719890058
iteration 69, loss = 0.001937591703608632
iteration 70, loss = 0.0022589382715523243
iteration 71, loss = 0.0023363593500107527
iteration 72, loss = 0.0015752344625070691
iteration 73, loss = 0.0019489064579829574
iteration 74, loss = 0.0022909357212483883
iteration 75, loss = 0.002131054410710931
iteration 76, loss = 0.0019378903089091182
iteration 77, loss = 0.002038423903286457
iteration 78, loss = 0.002782717812806368
iteration 79, loss = 0.0016872372943907976
iteration 80, loss = 0.002450711326673627
iteration 81, loss = 0.0029837097972631454
iteration 82, loss = 0.0021768100559711456
iteration 83, loss = 0.0023842002265155315
iteration 84, loss = 0.0023372333962470293
iteration 85, loss = 0.0017296475125476718
iteration 86, loss = 0.002020160900428891
iteration 87, loss = 0.0024143895134329796
iteration 88, loss = 0.0021802056580781937
iteration 89, loss = 0.0022210367023944855
iteration 90, loss = 0.004146877210587263
iteration 91, loss = 0.0038707167841494083
iteration 92, loss = 0.001788128400221467
iteration 93, loss = 0.002347308909520507
iteration 94, loss = 0.00224837614223361
iteration 95, loss = 0.002530452096834779
iteration 96, loss = 0.0020724120549857616
iteration 97, loss = 0.0023155889939516783
iteration 98, loss = 0.002311434131115675
iteration 99, loss = 0.0017465469427406788
iteration 100, loss = 0.0022557820193469524
iteration 101, loss = 0.0018931515514850616
iteration 102, loss = 0.0026681367307901382
iteration 103, loss = 0.001681743422523141
iteration 104, loss = 0.003157851053401828
iteration 105, loss = 0.001955152954906225
iteration 106, loss = 0.0016240801196545362
iteration 107, loss = 0.002091569360345602
iteration 108, loss = 0.0017509344033896923
iteration 109, loss = 0.0020262692123651505
iteration 110, loss = 0.0020580929704010487
iteration 111, loss = 0.002418950665742159
iteration 112, loss = 0.002437862567603588
iteration 113, loss = 0.0020021453965455294
iteration 114, loss = 0.0023781827185302973
iteration 115, loss = 0.0025703497231006622
iteration 116, loss = 0.0023248556535691023
iteration 117, loss = 0.002052050782367587
iteration 118, loss = 0.0038141333498060703
iteration 119, loss = 0.0021636055316776037
iteration 120, loss = 0.0020127720199525356
iteration 121, loss = 0.0028464964125305414
iteration 122, loss = 0.001657809130847454
iteration 123, loss = 0.001826321822591126
iteration 124, loss = 0.0021570217795670033
iteration 125, loss = 0.0022096503525972366
iteration 126, loss = 0.002107863314449787
iteration 127, loss = 0.0016544560203328729
iteration 128, loss = 0.0023470958694815636
iteration 129, loss = 0.0018736274214461446
iteration 130, loss = 0.002123329322785139
iteration 131, loss = 0.0018918565474450588
iteration 132, loss = 0.002175775822252035
iteration 133, loss = 0.003084082156419754
iteration 134, loss = 0.0029778636526316404
iteration 135, loss = 0.001935063861310482
iteration 136, loss = 0.0032141695264726877
iteration 137, loss = 0.0023159633856266737
iteration 138, loss = 0.0027549315709620714
iteration 139, loss = 0.0019024298526346684
iteration 140, loss = 0.0025510152336210012
iteration 141, loss = 0.0035718397703021765
iteration 142, loss = 0.0016143718967214227
iteration 143, loss = 0.0038994441274553537
iteration 144, loss = 0.0025003007613122463
iteration 145, loss = 0.001774667645804584
iteration 146, loss = 0.0024491548538208008
iteration 147, loss = 0.0018616414163261652
iteration 148, loss = 0.0016690607881173491
iteration 149, loss = 0.0022554402239620686
iteration 150, loss = 0.002187031088396907
iteration 151, loss = 0.0020473957993090153
iteration 152, loss = 0.0027356804348528385
iteration 153, loss = 0.0025961720384657383
iteration 154, loss = 0.002599751576781273
iteration 155, loss = 0.0031455312855541706
iteration 156, loss = 0.0016331669175997376
iteration 157, loss = 0.0018497498240321875
iteration 158, loss = 0.002274772385135293
iteration 159, loss = 0.0017268063966184855
iteration 160, loss = 0.001879835850559175
iteration 161, loss = 0.0027903886511921883
iteration 162, loss = 0.0018317312933504581
iteration 163, loss = 0.0018182884668931365
iteration 164, loss = 0.003259140532463789
iteration 165, loss = 0.002084712265059352
iteration 166, loss = 0.002132918918505311
iteration 167, loss = 0.0017745813820511103
iteration 168, loss = 0.0018789772875607014
iteration 169, loss = 0.002900851657614112
iteration 170, loss = 0.0028478619642555714
iteration 171, loss = 0.0016440276522189379
iteration 172, loss = 0.0021268620621412992
iteration 173, loss = 0.0023867462296038866
iteration 174, loss = 0.002234629588201642
iteration 175, loss = 0.0020368038676679134
iteration 176, loss = 0.0020632606465369463
iteration 177, loss = 0.002219575457274914
iteration 178, loss = 0.0018385322764515877
iteration 179, loss = 0.0016184768173843622
iteration 180, loss = 0.002367041539400816
iteration 181, loss = 0.004120716825127602
iteration 182, loss = 0.0025579568464308977
iteration 183, loss = 0.0020607076585292816
iteration 184, loss = 0.0042225974611938
iteration 185, loss = 0.003978975582867861
iteration 186, loss = 0.002071864902973175
iteration 187, loss = 0.0020736651495099068
iteration 188, loss = 0.002288646763190627
iteration 189, loss = 0.0018618317553773522
iteration 190, loss = 0.0017180386930704117
iteration 191, loss = 0.0018803395796567202
iteration 192, loss = 0.0026903615798801184
iteration 193, loss = 0.0024933426175266504
iteration 194, loss = 0.002658249344676733
iteration 195, loss = 0.002089073183014989
iteration 196, loss = 0.003926118370145559
iteration 197, loss = 0.0019721821881830692
iteration 198, loss = 0.0019308867631480098
iteration 199, loss = 0.0018298598006367683
iteration 200, loss = 0.00290990574285388
iteration 201, loss = 0.0018298120703548193
iteration 202, loss = 0.001757149351760745
iteration 203, loss = 0.002120556775480509
iteration 204, loss = 0.003570032771676779
iteration 205, loss = 0.002110303146764636
iteration 206, loss = 0.0028593926690518856
iteration 207, loss = 0.0021490103099495173
iteration 208, loss = 0.0018883072771131992
iteration 209, loss = 0.0026674575638026
iteration 210, loss = 0.0022928703110665083
iteration 211, loss = 0.002586890710517764
iteration 212, loss = 0.0031543474178761244
iteration 213, loss = 0.002162359654903412
iteration 214, loss = 0.002354675903916359
iteration 215, loss = 0.0021494526881724596
iteration 216, loss = 0.0019760960713028908
iteration 217, loss = 0.0023982764687389135
iteration 218, loss = 0.0017569431802257895
iteration 219, loss = 0.0020975028164684772
iteration 220, loss = 0.0028490142431110144
iteration 221, loss = 0.0018815159564837813
iteration 222, loss = 0.001796903321519494
iteration 223, loss = 0.001865224912762642
iteration 224, loss = 0.002409267472103238
iteration 225, loss = 0.0020304652862250805
iteration 226, loss = 0.0015772838378325105
iteration 227, loss = 0.001622774638235569
iteration 228, loss = 0.002239667344838381
iteration 229, loss = 0.0022381518501788378
iteration 230, loss = 0.0020926091820001602
iteration 231, loss = 0.0027612028643488884
iteration 232, loss = 0.002395983785390854
iteration 233, loss = 0.001981664216145873
iteration 234, loss = 0.0019340255530551076
iteration 235, loss = 0.0043481034226715565
iteration 236, loss = 0.001949646626599133
iteration 237, loss = 0.0027206544764339924
iteration 238, loss = 0.002032837364822626
iteration 239, loss = 0.0018003572477027774
iteration 240, loss = 0.0019443619530647993
iteration 241, loss = 0.002365205902606249
iteration 242, loss = 0.0026564253494143486
iteration 243, loss = 0.0019834162667393684
iteration 244, loss = 0.0017967985477298498
iteration 245, loss = 0.0018957825377583504
iteration 246, loss = 0.0017472084145992994
iteration 247, loss = 0.0024699654895812273
iteration 248, loss = 0.002102744532749057
iteration 249, loss = 0.002097178716212511
iteration 250, loss = 0.002954931929707527
iteration 251, loss = 0.0016406213399022818
iteration 252, loss = 0.001716948114335537
iteration 253, loss = 0.001987809780985117
iteration 254, loss = 0.002040024846792221
iteration 255, loss = 0.0019289589254185557
iteration 256, loss = 0.0023183333687484264
iteration 257, loss = 0.0029119320679455996
iteration 258, loss = 0.0018604391952976584
iteration 259, loss = 0.001831947360187769
iteration 260, loss = 0.0019197313813492656
iteration 261, loss = 0.0015679042553529143
iteration 262, loss = 0.0026351632550358772
iteration 263, loss = 0.0021937042474746704
iteration 264, loss = 0.00229441374540329
iteration 265, loss = 0.0017884214175865054
iteration 266, loss = 0.001960969064384699
iteration 267, loss = 0.0015474981628358364
iteration 268, loss = 0.0023915665224194527
iteration 269, loss = 0.0016991280717775226
iteration 270, loss = 0.0035305775236338377
iteration 271, loss = 0.002358643338084221
iteration 272, loss = 0.004293401725590229
iteration 273, loss = 0.001559718162752688
iteration 274, loss = 0.0015724841505289078
iteration 275, loss = 0.0019410982495173812
iteration 276, loss = 0.001976944971829653
iteration 277, loss = 0.0032871956937015057
iteration 278, loss = 0.001831843052059412
iteration 279, loss = 0.002071050927042961
iteration 280, loss = 0.001892966334708035
iteration 281, loss = 0.0019388063810765743
iteration 282, loss = 0.001662922790274024
iteration 283, loss = 0.0039464449509978294
iteration 284, loss = 0.002305608708411455
iteration 285, loss = 0.0026933446060866117
iteration 286, loss = 0.0036670840345323086
iteration 287, loss = 0.0020080008544027805
iteration 288, loss = 0.0033345730043947697
iteration 289, loss = 0.002361341379582882
iteration 290, loss = 0.0021718372590839863
iteration 291, loss = 0.0017541665583848953
iteration 292, loss = 0.0018561392789706588
iteration 293, loss = 0.0017905804561451077
iteration 294, loss = 0.0017295554280281067
iteration 295, loss = 0.0024757448118180037
iteration 296, loss = 0.001618682872503996
iteration 297, loss = 0.0019031682750210166
iteration 298, loss = 0.0019551499281078577
iteration 299, loss = 0.0021877398248761892
iteration 300, loss = 0.004532494582235813
iteration 1, loss = 0.002067368011921644
iteration 2, loss = 0.003183117602020502
iteration 3, loss = 0.0019331437069922686
iteration 4, loss = 0.0024129615630954504
iteration 5, loss = 0.0014908391749486327
iteration 6, loss = 0.002081762533634901
iteration 7, loss = 0.002496682573109865
iteration 8, loss = 0.0017306174850091338
iteration 9, loss = 0.002456542570143938
iteration 10, loss = 0.002327383030205965
iteration 11, loss = 0.0021308311261236668
iteration 12, loss = 0.003387661185115576
iteration 13, loss = 0.0024392209015786648
iteration 14, loss = 0.002700078533962369
iteration 15, loss = 0.002029340248554945
iteration 16, loss = 0.0018733347533270717
iteration 17, loss = 0.003778815735131502
iteration 18, loss = 0.0020090003963559866
iteration 19, loss = 0.0018004346638917923
iteration 20, loss = 0.0017013283213600516
iteration 21, loss = 0.001797487260773778
iteration 22, loss = 0.0026731209363788366
iteration 23, loss = 0.00221733539365232
iteration 24, loss = 0.002674100687727332
iteration 25, loss = 0.001778521342203021
iteration 26, loss = 0.0020043367985635996
iteration 27, loss = 0.0018541766330599785
iteration 28, loss = 0.001622220384888351
iteration 29, loss = 0.0025829991791397333
iteration 30, loss = 0.0019516219617798924
iteration 31, loss = 0.0027163554914295673
iteration 32, loss = 0.002222981071099639
iteration 33, loss = 0.002118147676810622
iteration 34, loss = 0.003352901665493846
iteration 35, loss = 0.002315691439434886
iteration 36, loss = 0.002110840054228902
iteration 37, loss = 0.002351122908294201
iteration 38, loss = 0.0015322529943659902
iteration 39, loss = 0.002204661723226309
iteration 40, loss = 0.0019148498540744185
iteration 41, loss = 0.00167303835041821
iteration 42, loss = 0.001655403058975935
iteration 43, loss = 0.0017945924773812294
iteration 44, loss = 0.0019709381740540266
iteration 45, loss = 0.0020511934999376535
iteration 46, loss = 0.002961699152365327
iteration 47, loss = 0.0026929464656859636
iteration 48, loss = 0.001711093238554895
iteration 49, loss = 0.002101551741361618
iteration 50, loss = 0.003014822257682681
iteration 51, loss = 0.0020114737562835217
iteration 52, loss = 0.001990326913073659
iteration 53, loss = 0.0019221033435314894
iteration 54, loss = 0.002488907193765044
iteration 55, loss = 0.002094466704875231
iteration 56, loss = 0.0020747093949466944
iteration 57, loss = 0.0019582847598940134
iteration 58, loss = 0.0018864169251173735
iteration 59, loss = 0.0018400985281914473
iteration 60, loss = 0.00187933468259871
iteration 61, loss = 0.0026181747671216726
iteration 62, loss = 0.0021667657420039177
iteration 63, loss = 0.0021182838827371597
iteration 64, loss = 0.002649340545758605
iteration 65, loss = 0.0024859560653567314
iteration 66, loss = 0.0027929123025387526
iteration 67, loss = 0.00282634561881423
iteration 68, loss = 0.00390627421438694
iteration 69, loss = 0.002640844089910388
iteration 70, loss = 0.002562739420682192
iteration 71, loss = 0.0029556825757026672
iteration 72, loss = 0.003099981462582946
iteration 73, loss = 0.0019758131820708513
iteration 74, loss = 0.001733776880428195
iteration 75, loss = 0.0020393412560224533
iteration 76, loss = 0.0022531538270413876
iteration 77, loss = 0.0022619646042585373
iteration 78, loss = 0.0018005191814154387
iteration 79, loss = 0.0032199386041611433
iteration 80, loss = 0.002204818883910775
iteration 81, loss = 0.0028666784055531025
iteration 82, loss = 0.0021293724421411753
iteration 83, loss = 0.002236243337392807
iteration 84, loss = 0.0019176971400156617
iteration 85, loss = 0.001868621096946299
iteration 86, loss = 0.0019173226319253445
iteration 87, loss = 0.0018880506977438927
iteration 88, loss = 0.0018180971965193748
iteration 89, loss = 0.0018020733259618282
iteration 90, loss = 0.0018843762809410691
iteration 91, loss = 0.0022338605485856533
iteration 92, loss = 0.0019132751040160656
iteration 93, loss = 0.002239325549453497
iteration 94, loss = 0.001950998092070222
iteration 95, loss = 0.002464590361341834
iteration 96, loss = 0.002653012052178383
iteration 97, loss = 0.0019043933134526014
iteration 98, loss = 0.002957642078399658
iteration 99, loss = 0.0027938084676861763
iteration 100, loss = 0.0027220749761909246
iteration 101, loss = 0.0043996102176606655
iteration 102, loss = 0.003681877627968788
iteration 103, loss = 0.0033548471983522177
iteration 104, loss = 0.002173328772187233
iteration 105, loss = 0.001649918151088059
iteration 106, loss = 0.003158640582114458
iteration 107, loss = 0.0019319476559758186
iteration 108, loss = 0.0018735177582129836
iteration 109, loss = 0.002755667082965374
iteration 110, loss = 0.0020745613146573305
iteration 111, loss = 0.002386312233284116
iteration 112, loss = 0.002173252636566758
iteration 113, loss = 0.0016237974632531404
iteration 114, loss = 0.002016200451180339
iteration 115, loss = 0.0019364318577572703
iteration 116, loss = 0.0031226894352585077
iteration 117, loss = 0.0021290334407240152
iteration 118, loss = 0.0016075933817774057
iteration 119, loss = 0.0017075370997190475
iteration 120, loss = 0.002206639852374792
iteration 121, loss = 0.0025865044444799423
iteration 122, loss = 0.001820286619476974
iteration 123, loss = 0.0017514681676402688
iteration 124, loss = 0.001576795824803412
iteration 125, loss = 0.0021968691144138575
iteration 126, loss = 0.003853284753859043
iteration 127, loss = 0.0026277205906808376
iteration 128, loss = 0.0019487009849399328
iteration 129, loss = 0.001850781962275505
iteration 130, loss = 0.002420108299702406
iteration 131, loss = 0.001781054656021297
iteration 132, loss = 0.0016882639611139894
iteration 133, loss = 0.004456859547644854
iteration 134, loss = 0.003677797270938754
iteration 135, loss = 0.0019774753600358963
iteration 136, loss = 0.001812476897612214
iteration 137, loss = 0.001974700251594186
iteration 138, loss = 0.0017618653364479542
iteration 139, loss = 0.00200703670270741
iteration 140, loss = 0.0021514578256756067
iteration 141, loss = 0.0022180641535669565
iteration 142, loss = 0.002324023749679327
iteration 143, loss = 0.0024126607459038496
iteration 144, loss = 0.0016314934473484755
iteration 145, loss = 0.0020827932748943567
iteration 146, loss = 0.003718501888215542
iteration 147, loss = 0.0019419504096731544
iteration 148, loss = 0.002782303374260664
iteration 149, loss = 0.0030107018537819386
iteration 150, loss = 0.0020620753057301044
iteration 151, loss = 0.002480371855199337
iteration 152, loss = 0.0033497053664177656
iteration 153, loss = 0.002505771117284894
iteration 154, loss = 0.002252433216199279
iteration 155, loss = 0.0034471782855689526
iteration 156, loss = 0.0017596982652321458
iteration 157, loss = 0.002108690096065402
iteration 158, loss = 0.0015553843695670366
iteration 159, loss = 0.0020950569305568933
iteration 160, loss = 0.002101742895320058
iteration 161, loss = 0.0016451987903565168
iteration 162, loss = 0.0019505659583956003
iteration 163, loss = 0.001614183303900063
iteration 164, loss = 0.0018671127036213875
iteration 165, loss = 0.0017235430423170328
iteration 166, loss = 0.001861714874394238
iteration 167, loss = 0.0017873673932626843
iteration 168, loss = 0.0023783387150615454
iteration 169, loss = 0.0019072990398854017
iteration 170, loss = 0.0018969894153997302
iteration 171, loss = 0.002078013261780143
iteration 172, loss = 0.0031565616372972727
iteration 173, loss = 0.0019834977574646473
iteration 174, loss = 0.0029412144795060158
iteration 175, loss = 0.001750292256474495
iteration 176, loss = 0.0015717404894530773
iteration 177, loss = 0.0018005655147135258
iteration 178, loss = 0.001848096726462245
iteration 179, loss = 0.002023788169026375
iteration 180, loss = 0.0021432291250675917
iteration 181, loss = 0.0037476671859622
iteration 182, loss = 0.002135582733899355
iteration 183, loss = 0.0019423628691583872
iteration 184, loss = 0.003912418149411678
iteration 185, loss = 0.0020239767618477345
iteration 186, loss = 0.0019384338520467281
iteration 187, loss = 0.0028281807899475098
iteration 188, loss = 0.002067012246698141
iteration 189, loss = 0.002452787710353732
iteration 190, loss = 0.0021860888227820396
iteration 191, loss = 0.002665797946974635
iteration 192, loss = 0.0018637485336512327
iteration 193, loss = 0.0020493241026997566
iteration 194, loss = 0.0017693715635687113
iteration 195, loss = 0.0021082961466163397
iteration 196, loss = 0.0021531186066567898
iteration 197, loss = 0.0017043970292434096
iteration 198, loss = 0.001953894505277276
iteration 199, loss = 0.0025199726223945618
iteration 200, loss = 0.0016717043472453952
iteration 201, loss = 0.0016208912711590528
iteration 202, loss = 0.0022674594074487686
iteration 203, loss = 0.002873274264857173
iteration 204, loss = 0.0025657452642917633
iteration 205, loss = 0.0035281511954963207
iteration 206, loss = 0.0022906765807420015
iteration 207, loss = 0.0024417387321591377
iteration 208, loss = 0.0025603354442864656
iteration 209, loss = 0.0019419791642576456
iteration 210, loss = 0.002943595638498664
iteration 211, loss = 0.002212108578532934
iteration 212, loss = 0.0023946785368025303
iteration 213, loss = 0.0017713240813463926
iteration 214, loss = 0.00203225901350379
iteration 215, loss = 0.0023991470225155354
iteration 216, loss = 0.0037095665466040373
iteration 217, loss = 0.0019264814909547567
iteration 218, loss = 0.004877879284322262
iteration 219, loss = 0.0020398306660354137
iteration 220, loss = 0.0018528047949075699
iteration 221, loss = 0.00234875176101923
iteration 222, loss = 0.0024473359808325768
iteration 223, loss = 0.0021431015338748693
iteration 224, loss = 0.001906109508126974
iteration 225, loss = 0.0020119757391512394
iteration 226, loss = 0.0022801232989877462
iteration 227, loss = 0.0034787754993885756
iteration 228, loss = 0.0019031016854569316
iteration 229, loss = 0.002322620479390025
iteration 230, loss = 0.002074642339721322
iteration 231, loss = 0.001979057677090168
iteration 232, loss = 0.001671455567702651
iteration 233, loss = 0.0016202754341065884
iteration 234, loss = 0.0017128055915236473
iteration 235, loss = 0.002098447410389781
iteration 236, loss = 0.00570108275860548
iteration 237, loss = 0.001840726938098669
iteration 238, loss = 0.0032764612697064877
iteration 239, loss = 0.002634220290929079
iteration 240, loss = 0.0028322553262114525
iteration 241, loss = 0.0017483275150880218
iteration 242, loss = 0.0029475323390215635
iteration 243, loss = 0.0019816586282104254
iteration 244, loss = 0.001924239913932979
iteration 245, loss = 0.0016118899220600724
iteration 246, loss = 0.0021446715109050274
iteration 247, loss = 0.002341660438105464
iteration 248, loss = 0.0019205815624445677
iteration 249, loss = 0.0017935647629201412
iteration 250, loss = 0.001640979666262865
iteration 251, loss = 0.0017457602079957724
iteration 252, loss = 0.0018296384951099753
iteration 253, loss = 0.001724694506265223
iteration 254, loss = 0.0019151753513142467
iteration 255, loss = 0.0018078545108437538
iteration 256, loss = 0.0024465990718454123
iteration 257, loss = 0.001618484384380281
iteration 258, loss = 0.001825949759222567
iteration 259, loss = 0.0037476993165910244
iteration 260, loss = 0.001977274427190423
iteration 261, loss = 0.002047927351668477
iteration 262, loss = 0.002100794343277812
iteration 263, loss = 0.001974152633920312
iteration 264, loss = 0.002997894072905183
iteration 265, loss = 0.0018124871421605349
iteration 266, loss = 0.003891877830028534
iteration 267, loss = 0.0025141513906419277
iteration 268, loss = 0.001894822926260531
iteration 269, loss = 0.002572629600763321
iteration 270, loss = 0.0025543563533574343
iteration 271, loss = 0.002744137542322278
iteration 272, loss = 0.002753789769485593
iteration 273, loss = 0.0022175295744091272
iteration 274, loss = 0.0024256007745862007
iteration 275, loss = 0.0018157338490709662
iteration 276, loss = 0.001865650643594563
iteration 277, loss = 0.0016618918161839247
iteration 278, loss = 0.0029544522985816
iteration 279, loss = 0.0023140148259699345
iteration 280, loss = 0.002188997808843851
iteration 281, loss = 0.0018432638607919216
iteration 282, loss = 0.002637302502989769
iteration 283, loss = 0.0015323592815548182
iteration 284, loss = 0.00160591013263911
iteration 285, loss = 0.0016850326210260391
iteration 286, loss = 0.0022583967074751854
iteration 287, loss = 0.0017368285916745663
iteration 288, loss = 0.002120119519531727
iteration 289, loss = 0.001671685022301972
iteration 290, loss = 0.002514429157599807
iteration 291, loss = 0.0021250280551612377
iteration 292, loss = 0.0016698752297088504
iteration 293, loss = 0.0046107349917292595
iteration 294, loss = 0.0018410496413707733
iteration 295, loss = 0.0017007458955049515
iteration 296, loss = 0.002100640907883644
iteration 297, loss = 0.0020118304528295994
iteration 298, loss = 0.0022105006501078606
iteration 299, loss = 0.00232612993568182
iteration 300, loss = 0.0018141062464565039
iteration 1, loss = 0.0034540861379355192
iteration 2, loss = 0.0021573167759925127
iteration 3, loss = 0.0022633906919509172
iteration 4, loss = 0.0019468783866614103
iteration 5, loss = 0.0019759023562073708
iteration 6, loss = 0.0027603120543062687
iteration 7, loss = 0.0019249541219323874
iteration 8, loss = 0.0033169297967106104
iteration 9, loss = 0.001977107720449567
iteration 10, loss = 0.0018489628564566374
iteration 11, loss = 0.0020158104598522186
iteration 12, loss = 0.0018246863037347794
iteration 13, loss = 0.0022663266863673925
iteration 14, loss = 0.003948673140257597
iteration 15, loss = 0.003015188965946436
iteration 16, loss = 0.0019103671656921506
iteration 17, loss = 0.0020936785731464624
iteration 18, loss = 0.0020113554783165455
iteration 19, loss = 0.0017848927527666092
iteration 20, loss = 0.0022489672992378473
iteration 21, loss = 0.0025292420759797096
iteration 22, loss = 0.002609287155792117
iteration 23, loss = 0.0023438166826963425
iteration 24, loss = 0.003779985709115863
iteration 25, loss = 0.002376310061663389
iteration 26, loss = 0.0017806586110964417
iteration 27, loss = 0.002032142598181963
iteration 28, loss = 0.001749752671457827
iteration 29, loss = 0.001994506223127246
iteration 30, loss = 0.0019456911832094193
iteration 31, loss = 0.0017739528557285666
iteration 32, loss = 0.0029401532374322414
iteration 33, loss = 0.002295000245794654
iteration 34, loss = 0.001968910451978445
iteration 35, loss = 0.0023216132540255785
iteration 36, loss = 0.0024749478325247765
iteration 37, loss = 0.002015066333115101
iteration 38, loss = 0.0027349358424544334
iteration 39, loss = 0.0022938454058021307
iteration 40, loss = 0.0018775803036987782
iteration 41, loss = 0.001826295512728393
iteration 42, loss = 0.0020269895903766155
iteration 43, loss = 0.0024905602913349867
iteration 44, loss = 0.002181844785809517
iteration 45, loss = 0.0017656235722824931
iteration 46, loss = 0.002773268148303032
iteration 47, loss = 0.002060411497950554
iteration 48, loss = 0.0018536308780312538
iteration 49, loss = 0.0015609798720106483
iteration 50, loss = 0.002643410349264741
iteration 51, loss = 0.003347039921209216
iteration 52, loss = 0.0019557939376682043
iteration 53, loss = 0.0020588040351867676
iteration 54, loss = 0.0019464282086119056
iteration 55, loss = 0.0016900874907150865
iteration 56, loss = 0.0017467966536059976
iteration 57, loss = 0.0020394509192556143
iteration 58, loss = 0.0021660011261701584
iteration 59, loss = 0.0019467349629849195
iteration 60, loss = 0.0020999412517994642
iteration 61, loss = 0.0018133020494133234
iteration 62, loss = 0.002208454767242074
iteration 63, loss = 0.002495609922334552
iteration 64, loss = 0.0029594674706459045
iteration 65, loss = 0.0026664796750992537
iteration 66, loss = 0.002310364507138729
iteration 67, loss = 0.002230328042060137
iteration 68, loss = 0.002480626106262207
iteration 69, loss = 0.0023694392293691635
iteration 70, loss = 0.002585881156846881
iteration 71, loss = 0.0018513890681788325
iteration 72, loss = 0.0020636292174458504
iteration 73, loss = 0.0024923249147832394
iteration 74, loss = 0.002574734389781952
iteration 75, loss = 0.0017322080675512552
iteration 76, loss = 0.002333608688786626
iteration 77, loss = 0.001939216861501336
iteration 78, loss = 0.0037740517873317003
iteration 79, loss = 0.00284012034535408
iteration 80, loss = 0.0021974630653858185
iteration 81, loss = 0.0017698043957352638
iteration 82, loss = 0.0018114178674295545
iteration 83, loss = 0.0021774484775960445
iteration 84, loss = 0.002301938831806183
iteration 85, loss = 0.0034936279989778996
iteration 86, loss = 0.002107807667925954
iteration 87, loss = 0.0019487454555928707
iteration 88, loss = 0.00345449335873127
iteration 89, loss = 0.001835651695728302
iteration 90, loss = 0.001996205421164632
iteration 91, loss = 0.002674113493412733
iteration 92, loss = 0.0028794200625270605
iteration 93, loss = 0.0016105344984680414
iteration 94, loss = 0.003518969751894474
iteration 95, loss = 0.0018292852910235524
iteration 96, loss = 0.001746459398418665
iteration 97, loss = 0.0016817986033856869
iteration 98, loss = 0.001959696179255843
iteration 99, loss = 0.0018583801575005054
iteration 100, loss = 0.002001221291720867
iteration 101, loss = 0.002363904844969511
iteration 102, loss = 0.0020457047503441572
iteration 103, loss = 0.0040781330317258835
iteration 104, loss = 0.0033134184777736664
iteration 105, loss = 0.0018495571566745639
iteration 106, loss = 0.0023302047047764063
iteration 107, loss = 0.001988441916182637
iteration 108, loss = 0.0039365291595458984
iteration 109, loss = 0.0017125564627349377
iteration 110, loss = 0.0024827062152326107
iteration 111, loss = 0.002148226136341691
iteration 112, loss = 0.002408327767625451
iteration 113, loss = 0.002443161327391863
iteration 114, loss = 0.0020796391181647778
iteration 115, loss = 0.002083667553961277
iteration 116, loss = 0.002813641680404544
iteration 117, loss = 0.004373421426862478
iteration 118, loss = 0.0019584018737077713
iteration 119, loss = 0.0030100392177700996
iteration 120, loss = 0.002317204372957349
iteration 121, loss = 0.001885388744994998
iteration 122, loss = 0.0017301840707659721
iteration 123, loss = 0.0019074708689004183
iteration 124, loss = 0.0016800012672320008
iteration 125, loss = 0.0021773925982415676
iteration 126, loss = 0.0021531216334551573
iteration 127, loss = 0.0023523278068751097
iteration 128, loss = 0.0020930825266987085
iteration 129, loss = 0.002100431825965643
iteration 130, loss = 0.00248968624509871
iteration 131, loss = 0.003091928781941533
iteration 132, loss = 0.0017970053013414145
iteration 133, loss = 0.0031236675567924976
iteration 134, loss = 0.0020764274522662163
iteration 135, loss = 0.0017535974038764834
iteration 136, loss = 0.0018484736792743206
iteration 137, loss = 0.0021648311521857977
iteration 138, loss = 0.002630482893437147
iteration 139, loss = 0.001957913162186742
iteration 140, loss = 0.0018197562312707305
iteration 141, loss = 0.0017281362088397145
iteration 142, loss = 0.002378910081461072
iteration 143, loss = 0.0017058863304555416
iteration 144, loss = 0.0019800576847046614
iteration 145, loss = 0.0035610764753073454
iteration 146, loss = 0.0033201389014720917
iteration 147, loss = 0.0021759867668151855
iteration 148, loss = 0.0020631374791264534
iteration 149, loss = 0.0019617341458797455
iteration 150, loss = 0.002720503369346261
iteration 151, loss = 0.002557411091402173
iteration 152, loss = 0.001922051189467311
iteration 153, loss = 0.0016121391672641039
iteration 154, loss = 0.0018590829567983747
iteration 155, loss = 0.0018919471185654402
iteration 156, loss = 0.001818583463318646
iteration 157, loss = 0.001728423754684627
iteration 158, loss = 0.0019123712554574013
iteration 159, loss = 0.0026649492792785168
iteration 160, loss = 0.0017304358771070838
iteration 161, loss = 0.002191417384892702
iteration 162, loss = 0.0020887786522507668
iteration 163, loss = 0.002045772736892104
iteration 164, loss = 0.0019512134604156017
iteration 165, loss = 0.0039000518154352903
iteration 166, loss = 0.0020791273564100266
iteration 167, loss = 0.0022886055521667004
iteration 168, loss = 0.002540372312068939
iteration 169, loss = 0.0030725495889782906
iteration 170, loss = 0.0032699063885957003
iteration 171, loss = 0.002002576133236289
iteration 172, loss = 0.0024216491729021072
iteration 173, loss = 0.00204154709354043
iteration 174, loss = 0.00270217377692461
iteration 175, loss = 0.0019104904495179653
iteration 176, loss = 0.0019347327761352062
iteration 177, loss = 0.0020619353745132685
iteration 178, loss = 0.0027073067612946033
iteration 179, loss = 0.0015567559748888016
iteration 180, loss = 0.0035725990310311317
iteration 181, loss = 0.002131624845787883
iteration 182, loss = 0.0022669420577585697
iteration 183, loss = 0.0018187338719144464
iteration 184, loss = 0.0018913696985691786
iteration 185, loss = 0.001681560999713838
iteration 186, loss = 0.0016232873313128948
iteration 187, loss = 0.0018006215104833245
iteration 188, loss = 0.0019207316217944026
iteration 189, loss = 0.0030338468495756388
iteration 190, loss = 0.0024708914570510387
iteration 191, loss = 0.002140480326488614
iteration 192, loss = 0.0018502897582948208
iteration 193, loss = 0.0031648692674934864
iteration 194, loss = 0.0018618579488247633
iteration 195, loss = 0.0017420161748304963
iteration 196, loss = 0.0019239808898419142
iteration 197, loss = 0.0017886231653392315
iteration 198, loss = 0.002106504747644067
iteration 199, loss = 0.0016149290604516864
iteration 200, loss = 0.0015460953582078218
iteration 201, loss = 0.0016005636425688863
iteration 202, loss = 0.0023108567111194134
iteration 203, loss = 0.0018715463811531663
iteration 204, loss = 0.003915156237781048
iteration 205, loss = 0.0028405459597706795
iteration 206, loss = 0.0020522086415439844
iteration 207, loss = 0.002151138847693801
iteration 208, loss = 0.002217123517766595
iteration 209, loss = 0.002056692959740758
iteration 210, loss = 0.0026126238517463207
iteration 211, loss = 0.0020946210715919733
iteration 212, loss = 0.0020846351981163025
iteration 213, loss = 0.0034517713356763124
iteration 214, loss = 0.002404590370133519
iteration 215, loss = 0.002847871044650674
iteration 216, loss = 0.001750256516970694
iteration 217, loss = 0.0017948690801858902
iteration 218, loss = 0.0020790337584912777
iteration 219, loss = 0.002284088870510459
iteration 220, loss = 0.0019550472497940063
iteration 221, loss = 0.0017626488115638494
iteration 222, loss = 0.0036333731841295958
iteration 223, loss = 0.0020069752354174852
iteration 224, loss = 0.0019704888109117746
iteration 225, loss = 0.0020972187630832195
iteration 226, loss = 0.0025829309597611427
iteration 227, loss = 0.002295306883752346
iteration 228, loss = 0.0018673315644264221
iteration 229, loss = 0.0031939770560711622
iteration 230, loss = 0.0018145046196877956
iteration 231, loss = 0.003027627943083644
iteration 232, loss = 0.001832693233154714
iteration 233, loss = 0.0018623261712491512
iteration 234, loss = 0.0024824661668390036
iteration 235, loss = 0.0022715323138982058
iteration 236, loss = 0.0017653952818363905
iteration 237, loss = 0.0018665633397176862
iteration 238, loss = 0.0017038402147591114
iteration 239, loss = 0.0024990281090140343
iteration 240, loss = 0.0016936054453253746
iteration 241, loss = 0.0019065699307247996
iteration 242, loss = 0.0031738076359033585
iteration 243, loss = 0.0023840665817260742
iteration 244, loss = 0.0027469578199088573
iteration 245, loss = 0.003119500121101737
iteration 246, loss = 0.003734813304618001
iteration 247, loss = 0.001956481486558914
iteration 248, loss = 0.0017706567887216806
iteration 249, loss = 0.002077835611999035
iteration 250, loss = 0.0027802048716694117
iteration 251, loss = 0.0017541592242196202
iteration 252, loss = 0.002149531152099371
iteration 253, loss = 0.0018799877725541592
iteration 254, loss = 0.0017856310587376356
iteration 255, loss = 0.003806216409429908
iteration 256, loss = 0.0024171750992536545
iteration 257, loss = 0.0018024687888100743
iteration 258, loss = 0.002262091264128685
iteration 259, loss = 0.0016898199683055282
iteration 260, loss = 0.0020199441350996494
iteration 261, loss = 0.0021007347386330366
iteration 262, loss = 0.0030382501427084208
iteration 263, loss = 0.0016594631597399712
iteration 264, loss = 0.0022392533719539642
iteration 265, loss = 0.0018571462715044618
iteration 266, loss = 0.002087488304823637
iteration 267, loss = 0.0033115434926003218
iteration 268, loss = 0.0021599114406853914
iteration 269, loss = 0.002122660866007209
iteration 270, loss = 0.0022248916793614626
iteration 271, loss = 0.0019134385511279106
iteration 272, loss = 0.0015650968998670578
iteration 273, loss = 0.0021692849695682526
iteration 274, loss = 0.00282579124905169
iteration 275, loss = 0.002305277157574892
iteration 276, loss = 0.0021368651650846004
iteration 277, loss = 0.0017764908261597157
iteration 278, loss = 0.0020911400206387043
iteration 279, loss = 0.0022598057985305786
iteration 280, loss = 0.003283939091488719
iteration 281, loss = 0.0021857621613889933
iteration 282, loss = 0.0018493742682039738
iteration 283, loss = 0.0023874198086559772
iteration 284, loss = 0.0015602009370923042
iteration 285, loss = 0.0022554611787199974
iteration 286, loss = 0.0016842487966641784
iteration 287, loss = 0.0018785507418215275
iteration 288, loss = 0.0019138295901939273
iteration 289, loss = 0.002399181015789509
iteration 290, loss = 0.0037600803188979626
iteration 291, loss = 0.002087206579744816
iteration 292, loss = 0.001976700033992529
iteration 293, loss = 0.0017803157679736614
iteration 294, loss = 0.0018009566701948643
iteration 295, loss = 0.002025766996666789
iteration 296, loss = 0.0023722206242382526
iteration 297, loss = 0.0018979108426719904
iteration 298, loss = 0.0027208628598600626
iteration 299, loss = 0.002334082033485174
iteration 300, loss = 0.0037446569185703993
iteration 1, loss = 0.0020389470737427473
iteration 2, loss = 0.003808491164818406
iteration 3, loss = 0.0015924633480608463
iteration 4, loss = 0.00264732469804585
iteration 5, loss = 0.0021128440275788307
iteration 6, loss = 0.0019377775024622679
iteration 7, loss = 0.002222582697868347
iteration 8, loss = 0.005354813300073147
iteration 9, loss = 0.0017354816664010286
iteration 10, loss = 0.0029901061207056046
iteration 11, loss = 0.0020274955313652754
iteration 12, loss = 0.0018603017088025808
iteration 13, loss = 0.002621047431603074
iteration 14, loss = 0.0016047429526224732
iteration 15, loss = 0.0019043049542233348
iteration 16, loss = 0.0038768481463193893
iteration 17, loss = 0.0019140138756483793
iteration 18, loss = 0.001897831680253148
iteration 19, loss = 0.0031555306632071733
iteration 20, loss = 0.0024055703543126583
iteration 21, loss = 0.0020390674471855164
iteration 22, loss = 0.0019181349780410528
iteration 23, loss = 0.0019730334170162678
iteration 24, loss = 0.0021958036813884974
iteration 25, loss = 0.0026064575649797916
iteration 26, loss = 0.0022178953513503075
iteration 27, loss = 0.0019384176703169942
iteration 28, loss = 0.0029268506914377213
iteration 29, loss = 0.0023969183675944805
iteration 30, loss = 0.0014890123857185245
iteration 31, loss = 0.001672220416367054
iteration 32, loss = 0.0021297717466950417
iteration 33, loss = 0.0024780859239399433
iteration 34, loss = 0.002172569278627634
iteration 35, loss = 0.0018632843857631087
iteration 36, loss = 0.001614792039617896
iteration 37, loss = 0.0016050415579229593
iteration 38, loss = 0.001991111319512129
iteration 39, loss = 0.002843191148713231
iteration 40, loss = 0.002006050432100892
iteration 41, loss = 0.0042944373562932014
iteration 42, loss = 0.0017509885365143418
iteration 43, loss = 0.001963632646948099
iteration 44, loss = 0.0020380113273859024
iteration 45, loss = 0.0016828635707497597
iteration 46, loss = 0.0020848677959293127
iteration 47, loss = 0.0018672473961487412
iteration 48, loss = 0.0025550422724336386
iteration 49, loss = 0.0021901624277234077
iteration 50, loss = 0.0016972855664789677
iteration 51, loss = 0.0019152447348460555
iteration 52, loss = 0.0018891775980591774
iteration 53, loss = 0.002211743500083685
iteration 54, loss = 0.0017725470243021846
iteration 55, loss = 0.0035226508043706417
iteration 56, loss = 0.0017100854311138391
iteration 57, loss = 0.001865515485405922
iteration 58, loss = 0.003596619935706258
iteration 59, loss = 0.003478218102827668
iteration 60, loss = 0.0027165557257831097
iteration 61, loss = 0.001956893363967538
iteration 62, loss = 0.002007273258641362
iteration 63, loss = 0.001873060711659491
iteration 64, loss = 0.001785437110811472
iteration 65, loss = 0.001747122616507113
iteration 66, loss = 0.0020513576455414295
iteration 67, loss = 0.002501733135432005
iteration 68, loss = 0.001630841987207532
iteration 69, loss = 0.0019803247414529324
iteration 70, loss = 0.0021930555813014507
iteration 71, loss = 0.0019410605309531093
iteration 72, loss = 0.0018676924519240856
iteration 73, loss = 0.003460818901658058
iteration 74, loss = 0.0016559899086132646
iteration 75, loss = 0.0018403904978185892
iteration 76, loss = 0.0017690281383693218
iteration 77, loss = 0.0020353610161691904
iteration 78, loss = 0.001760403742082417
iteration 79, loss = 0.0026302908081561327
iteration 80, loss = 0.0020354343578219414
iteration 81, loss = 0.00353976059705019
iteration 82, loss = 0.0022207368165254593
iteration 83, loss = 0.0017602600855752826
iteration 84, loss = 0.0021233363077044487
iteration 85, loss = 0.002190682105720043
iteration 86, loss = 0.0020054089836776257
iteration 87, loss = 0.0037774827796965837
iteration 88, loss = 0.0017744358628988266
iteration 89, loss = 0.002636428689584136
iteration 90, loss = 0.0020530913025140762
iteration 91, loss = 0.0019081791397184134
iteration 92, loss = 0.0021461746655404568
iteration 93, loss = 0.001999534899368882
iteration 94, loss = 0.004221020266413689
iteration 95, loss = 0.0019994403701275587
iteration 96, loss = 0.0016742777079343796
iteration 97, loss = 0.0023173578083515167
iteration 98, loss = 0.003233521245419979
iteration 99, loss = 0.0020566382445394993
iteration 100, loss = 0.0018389220349490643
iteration 101, loss = 0.0028837514109909534
iteration 102, loss = 0.001894867978990078
iteration 103, loss = 0.0015482831513509154
iteration 104, loss = 0.004019512794911861
iteration 105, loss = 0.001985350623726845
iteration 106, loss = 0.00217202166095376
iteration 107, loss = 0.0019817196298390627
iteration 108, loss = 0.0019474625587463379
iteration 109, loss = 0.001956624910235405
iteration 110, loss = 0.0016901049530133605
iteration 111, loss = 0.00397338904440403
iteration 112, loss = 0.0030863326974213123
iteration 113, loss = 0.002985297469422221
iteration 114, loss = 0.002375025535002351
iteration 115, loss = 0.0016309074126183987
iteration 116, loss = 0.0018475353717803955
iteration 117, loss = 0.001772631541825831
iteration 118, loss = 0.001534758834168315
iteration 119, loss = 0.00442171934992075
iteration 120, loss = 0.0018706372939050198
iteration 121, loss = 0.0017937087686732411
iteration 122, loss = 0.0026953737251460552
iteration 123, loss = 0.0020331915002316236
iteration 124, loss = 0.002628267277032137
iteration 125, loss = 0.0019563469104468822
iteration 126, loss = 0.001959674060344696
iteration 127, loss = 0.0020485909190028906
iteration 128, loss = 0.0020061794202774763
iteration 129, loss = 0.004033765755593777
iteration 130, loss = 0.0044733150862157345
iteration 131, loss = 0.0018788633169606328
iteration 132, loss = 0.0016962704248726368
iteration 133, loss = 0.001995152560994029
iteration 134, loss = 0.0017865517875179648
iteration 135, loss = 0.001839854521676898
iteration 136, loss = 0.0017026313580572605
iteration 137, loss = 0.002090245485305786
iteration 138, loss = 0.0021314495243132114
iteration 139, loss = 0.002079443773254752
iteration 140, loss = 0.002455334644764662
iteration 141, loss = 0.0016940342029556632
iteration 142, loss = 0.001987406285479665
iteration 143, loss = 0.0019475152948871255
iteration 144, loss = 0.0019677123054862022
iteration 145, loss = 0.002170054940506816
iteration 146, loss = 0.002408781088888645
iteration 147, loss = 0.0015570094110444188
iteration 148, loss = 0.00257742777466774
iteration 149, loss = 0.0019950850401073694
iteration 150, loss = 0.0016024071956053376
iteration 151, loss = 0.0018119068117812276
iteration 152, loss = 0.003187805414199829
iteration 153, loss = 0.003060548799112439
iteration 154, loss = 0.00212191604077816
iteration 155, loss = 0.0022503191139549017
iteration 156, loss = 0.0017419985961169004
iteration 157, loss = 0.0018866562750190496
iteration 158, loss = 0.0022944894153624773
iteration 159, loss = 0.0027244386728852987
iteration 160, loss = 0.0018366865115240216
iteration 161, loss = 0.0026208283379673958
iteration 162, loss = 0.0018722708337008953
iteration 163, loss = 0.00249377079308033
iteration 164, loss = 0.0018296497873961926
iteration 165, loss = 0.0016996918711811304
iteration 166, loss = 0.0017862125532701612
iteration 167, loss = 0.002062157029286027
iteration 168, loss = 0.0019386669155210257
iteration 169, loss = 0.0020045624114573
iteration 170, loss = 0.003230517730116844
iteration 171, loss = 0.0021484168246388435
iteration 172, loss = 0.0023192143999040127
iteration 173, loss = 0.002066807122901082
iteration 174, loss = 0.003924081102013588
iteration 175, loss = 0.0019307697657495737
iteration 176, loss = 0.002029088092967868
iteration 177, loss = 0.002200138522312045
iteration 178, loss = 0.0021822990383952856
iteration 179, loss = 0.00195577135309577
iteration 180, loss = 0.0019481971394270658
iteration 181, loss = 0.0022710778284817934
iteration 182, loss = 0.0018396099330857396
iteration 183, loss = 0.001890330808237195
iteration 184, loss = 0.0018767999717965722
iteration 185, loss = 0.0016674284124746919
iteration 186, loss = 0.0019794467370957136
iteration 187, loss = 0.0016532076988369226
iteration 188, loss = 0.0019700981210917234
iteration 189, loss = 0.0018364512361586094
iteration 190, loss = 0.002150503220036626
iteration 191, loss = 0.00216388376429677
iteration 192, loss = 0.002174613531678915
iteration 193, loss = 0.0019708031322807074
iteration 194, loss = 0.002654616953805089
iteration 195, loss = 0.0023982608690857887
iteration 196, loss = 0.0031792670488357544
iteration 197, loss = 0.0019242942798882723
iteration 198, loss = 0.0018334317719563842
iteration 199, loss = 0.001662221271544695
iteration 200, loss = 0.0018498720601201057
iteration 201, loss = 0.001973259262740612
iteration 202, loss = 0.0018361201509833336
iteration 203, loss = 0.0020464423578232527
iteration 204, loss = 0.0025189886800944805
iteration 205, loss = 0.002441493095830083
iteration 206, loss = 0.003981378860771656
iteration 207, loss = 0.0016629823949187994
iteration 208, loss = 0.0017502588452771306
iteration 209, loss = 0.003622236894443631
iteration 210, loss = 0.001484310021623969
iteration 211, loss = 0.0021645519882440567
iteration 212, loss = 0.0020483715925365686
iteration 213, loss = 0.0021509865764528513
iteration 214, loss = 0.0022104980889707804
iteration 215, loss = 0.0018839020049199462
iteration 216, loss = 0.0016092709265649319
iteration 217, loss = 0.0017965743318200111
iteration 218, loss = 0.0018537756986916065
iteration 219, loss = 0.002004769630730152
iteration 220, loss = 0.003293269081041217
iteration 221, loss = 0.002037150552496314
iteration 222, loss = 0.002454272471368313
iteration 223, loss = 0.00399750517681241
iteration 224, loss = 0.002197342226281762
iteration 225, loss = 0.002656742464751005
iteration 226, loss = 0.00399731146171689
iteration 227, loss = 0.0024994504638016224
iteration 228, loss = 0.0019587851129472256
iteration 229, loss = 0.0019157907227054238
iteration 230, loss = 0.0024137648288160563
iteration 231, loss = 0.001990905264392495
iteration 232, loss = 0.0019097691401839256
iteration 233, loss = 0.0026340975891798735
iteration 234, loss = 0.0027572328690439463
iteration 235, loss = 0.0029341012705117464
iteration 236, loss = 0.002778031397610903
iteration 237, loss = 0.0017819523345679045
iteration 238, loss = 0.0016908276593312621
iteration 239, loss = 0.0035722292959690094
iteration 240, loss = 0.002168837236240506
iteration 241, loss = 0.0017994203371927142
iteration 242, loss = 0.001927721663378179
iteration 243, loss = 0.003252271097153425
iteration 244, loss = 0.0026938917580991983
iteration 245, loss = 0.0016425491776317358
iteration 246, loss = 0.0028273863717913628
iteration 247, loss = 0.0026163675356656313
iteration 248, loss = 0.0019539548084139824
iteration 249, loss = 0.0035778346937149763
iteration 250, loss = 0.0017812139121815562
iteration 251, loss = 0.0018372774356976151
iteration 252, loss = 0.001760787912644446
iteration 253, loss = 0.002475044457241893
iteration 254, loss = 0.002890034345909953
iteration 255, loss = 0.0015750840539112687
iteration 256, loss = 0.0019237070810049772
iteration 257, loss = 0.002153990790247917
iteration 258, loss = 0.0036417257506400347
iteration 259, loss = 0.0019172574393451214
iteration 260, loss = 0.0016791822854429483
iteration 261, loss = 0.0021001133136451244
iteration 262, loss = 0.0027960785664618015
iteration 263, loss = 0.003234586911275983
iteration 264, loss = 0.004075877368450165
iteration 265, loss = 0.0018489790381863713
iteration 266, loss = 0.0020111757330596447
iteration 267, loss = 0.0017530724871903658
iteration 268, loss = 0.0019296174868941307
iteration 269, loss = 0.003569952445104718
iteration 270, loss = 0.0020688010845333338
iteration 271, loss = 0.002558173146098852
iteration 272, loss = 0.0017529805190861225
iteration 273, loss = 0.0028620478697121143
iteration 274, loss = 0.003172732889652252
iteration 275, loss = 0.001952222315594554
iteration 276, loss = 0.0021729392465204
iteration 277, loss = 0.002394226612523198
iteration 278, loss = 0.0020255043637007475
iteration 279, loss = 0.002169638406485319
iteration 280, loss = 0.0018445573514327407
iteration 281, loss = 0.0028791131917387247
iteration 282, loss = 0.0024065955076366663
iteration 283, loss = 0.0017288506496697664
iteration 284, loss = 0.002678130054846406
iteration 285, loss = 0.0021021924912929535
iteration 286, loss = 0.0019165060948580503
iteration 287, loss = 0.0024500484578311443
iteration 288, loss = 0.0015970345120877028
iteration 289, loss = 0.0018148302333429456
iteration 290, loss = 0.0029917280189692974
iteration 291, loss = 0.0019087942782789469
iteration 292, loss = 0.0025740726850926876
iteration 293, loss = 0.0023757528979331255
iteration 294, loss = 0.0017947128508239985
iteration 295, loss = 0.001999170985072851
iteration 296, loss = 0.0020519227255135775
iteration 297, loss = 0.0021480023860931396
iteration 298, loss = 0.002332341857254505
iteration 299, loss = 0.002680218545719981
iteration 300, loss = 0.0018118390580639243
iteration 1, loss = 0.003384636016562581
iteration 2, loss = 0.0025009731762111187
iteration 3, loss = 0.002247386611998081
iteration 4, loss = 0.0016783338505774736
iteration 5, loss = 0.0016815756680443883
iteration 6, loss = 0.0017141615971922874
iteration 7, loss = 0.0019027143716812134
iteration 8, loss = 0.0028652604669332504
iteration 9, loss = 0.001811477355659008
iteration 10, loss = 0.0020757182501256466
iteration 11, loss = 0.0018173780990764499
iteration 12, loss = 0.0022325022146105766
iteration 13, loss = 0.0020420411601662636
iteration 14, loss = 0.001990698743611574
iteration 15, loss = 0.0021209625992923975
iteration 16, loss = 0.0026078755035996437
iteration 17, loss = 0.001670973957516253
iteration 18, loss = 0.0035668041091412306
iteration 19, loss = 0.001965529751032591
iteration 20, loss = 0.0034096494782716036
iteration 21, loss = 0.0015255420003086329
iteration 22, loss = 0.0036197116132825613
iteration 23, loss = 0.001986860763281584
iteration 24, loss = 0.0028306327294558287
iteration 25, loss = 0.0025058032479137182
iteration 26, loss = 0.001811175374314189
iteration 27, loss = 0.0033317306078970432
iteration 28, loss = 0.00201042159460485
iteration 29, loss = 0.002336037578061223
iteration 30, loss = 0.0019758373964577913
iteration 31, loss = 0.0028301908168941736
iteration 32, loss = 0.0022347383201122284
iteration 33, loss = 0.0019242073176428676
iteration 34, loss = 0.0025931678246706724
iteration 35, loss = 0.002729307161644101
iteration 36, loss = 0.001936839777044952
iteration 37, loss = 0.0015956329880282283
iteration 38, loss = 0.0018894477980211377
iteration 39, loss = 0.0019419079180806875
iteration 40, loss = 0.001812444068491459
iteration 41, loss = 0.001987108727917075
iteration 42, loss = 0.001800853991881013
iteration 43, loss = 0.0038784092757850885
iteration 44, loss = 0.0020021870732307434
iteration 45, loss = 0.0018929725047200918
iteration 46, loss = 0.0016056559979915619
iteration 47, loss = 0.0021178568713366985
iteration 48, loss = 0.0022431041579693556
iteration 49, loss = 0.0029716575518250465
iteration 50, loss = 0.0027371037285774946
iteration 51, loss = 0.003558148629963398
iteration 52, loss = 0.0016179935773834586
iteration 53, loss = 0.0019183749100193381
iteration 54, loss = 0.0017614987445995212
iteration 55, loss = 0.0018143168417736888
iteration 56, loss = 0.0024550831876695156
iteration 57, loss = 0.0022797551937401295
iteration 58, loss = 0.0017860759980976582
iteration 59, loss = 0.0018132911063730717
iteration 60, loss = 0.0017807523254305124
iteration 61, loss = 0.0018765806453302503
iteration 62, loss = 0.0031888149678707123
iteration 63, loss = 0.0026725928764790297
iteration 64, loss = 0.00377671979367733
iteration 65, loss = 0.0017552687786519527
iteration 66, loss = 0.002042402047663927
iteration 67, loss = 0.002339055296033621
iteration 68, loss = 0.0019021981861442327
iteration 69, loss = 0.001735421479679644
iteration 70, loss = 0.0019986946135759354
iteration 71, loss = 0.0017635133117437363
iteration 72, loss = 0.001801513833925128
iteration 73, loss = 0.002042382722720504
iteration 74, loss = 0.001796975382603705
iteration 75, loss = 0.0024146276991814375
iteration 76, loss = 0.0015243865782395005
iteration 77, loss = 0.003127213567495346
iteration 78, loss = 0.002196446992456913
iteration 79, loss = 0.003761834930628538
iteration 80, loss = 0.0017349758418276906
iteration 81, loss = 0.0024168596137315035
iteration 82, loss = 0.0016017805319279432
iteration 83, loss = 0.0021486994810402393
iteration 84, loss = 0.00221115630120039
iteration 85, loss = 0.001658177119679749
iteration 86, loss = 0.0021598897874355316
iteration 87, loss = 0.0020483299158513546
iteration 88, loss = 0.0031712360214442015
iteration 89, loss = 0.004373002331703901
iteration 90, loss = 0.0019577210769057274
iteration 91, loss = 0.0016821827739477158
iteration 92, loss = 0.0017804040107876062
iteration 93, loss = 0.0015480834990739822
iteration 94, loss = 0.0017592618241906166
iteration 95, loss = 0.0031853264663368464
iteration 96, loss = 0.0024679587222635746
iteration 97, loss = 0.0017929314635694027
iteration 98, loss = 0.002182417083531618
iteration 99, loss = 0.0018703251844272017
iteration 100, loss = 0.002147508319467306
iteration 101, loss = 0.0021447306498885155
iteration 102, loss = 0.0017341473139822483
iteration 103, loss = 0.0032364358194172382
iteration 104, loss = 0.002477225847542286
iteration 105, loss = 0.0024448451586067677
iteration 106, loss = 0.0020366194657981396
iteration 107, loss = 0.0021357801742851734
iteration 108, loss = 0.002082280581817031
iteration 109, loss = 0.003713171696290374
iteration 110, loss = 0.001842177240177989
iteration 111, loss = 0.003294955473393202
iteration 112, loss = 0.0037772459909319878
iteration 113, loss = 0.002273094840347767
iteration 114, loss = 0.002053066622465849
iteration 115, loss = 0.0022716016974300146
iteration 116, loss = 0.0020048245787620544
iteration 117, loss = 0.002263881964609027
iteration 118, loss = 0.001981580862775445
iteration 119, loss = 0.00201845308765769
iteration 120, loss = 0.002574903191998601
iteration 121, loss = 0.001826543128117919
iteration 122, loss = 0.002408550586551428
iteration 123, loss = 0.001979522407054901
iteration 124, loss = 0.0023223720490932465
iteration 125, loss = 0.0017894312040880322
iteration 126, loss = 0.002685908228158951
iteration 127, loss = 0.001909524668008089
iteration 128, loss = 0.0017028262373059988
iteration 129, loss = 0.0018123314948752522
iteration 130, loss = 0.002058541402220726
iteration 131, loss = 0.001927220611833036
iteration 132, loss = 0.0019898205064237118
iteration 133, loss = 0.0018449779599905014
iteration 134, loss = 0.0017463242402300239
iteration 135, loss = 0.0017148368060588837
iteration 136, loss = 0.001995757920667529
iteration 137, loss = 0.0017527928575873375
iteration 138, loss = 0.0023055775091052055
iteration 139, loss = 0.003550310619175434
iteration 140, loss = 0.002224657451733947
iteration 141, loss = 0.001883525401353836
iteration 142, loss = 0.0035228466149419546
iteration 143, loss = 0.0032584499567747116
iteration 144, loss = 0.001993281301110983
iteration 145, loss = 0.0018233644077554345
iteration 146, loss = 0.0016993364552035928
iteration 147, loss = 0.0022103488445281982
iteration 148, loss = 0.0020286175422370434
iteration 149, loss = 0.0022833654657006264
iteration 150, loss = 0.0016735185636207461
iteration 151, loss = 0.0020100583788007498
iteration 152, loss = 0.004037604667246342
iteration 153, loss = 0.0021187581587582827
iteration 154, loss = 0.0016004524659365416
iteration 155, loss = 0.002052708063274622
iteration 156, loss = 0.003440572414547205
iteration 157, loss = 0.0017954343929886818
iteration 158, loss = 0.0016377115389332175
iteration 159, loss = 0.0021166489459574223
iteration 160, loss = 0.002113591879606247
iteration 161, loss = 0.0022293066140264273
iteration 162, loss = 0.003211264731362462
iteration 163, loss = 0.0025971385184675455
iteration 164, loss = 0.0019473624415695667
iteration 165, loss = 0.0018326689023524523
iteration 166, loss = 0.001938458764925599
iteration 167, loss = 0.002114103641360998
iteration 168, loss = 0.0022905089426785707
iteration 169, loss = 0.0027395158540457487
iteration 170, loss = 0.0022536872420459986
iteration 171, loss = 0.0019786187913268805
iteration 172, loss = 0.0033056912943720818
iteration 173, loss = 0.00183806661516428
iteration 174, loss = 0.0028168102726340294
iteration 175, loss = 0.0027979747392237186
iteration 176, loss = 0.002149498788639903
iteration 177, loss = 0.0019716275855898857
iteration 178, loss = 0.0023633972741663456
iteration 179, loss = 0.001901341718621552
iteration 180, loss = 0.002443714067339897
iteration 181, loss = 0.001872767461463809
iteration 182, loss = 0.001945737050846219
iteration 183, loss = 0.001962132751941681
iteration 184, loss = 0.0016231888439506292
iteration 185, loss = 0.0025709187611937523
iteration 186, loss = 0.00249980459921062
iteration 187, loss = 0.002085503190755844
iteration 188, loss = 0.001650506048463285
iteration 189, loss = 0.0017225680639967322
iteration 190, loss = 0.0017458060756325722
iteration 191, loss = 0.0019279788248240948
iteration 192, loss = 0.0020879788789898157
iteration 193, loss = 0.0024813711643218994
iteration 194, loss = 0.0017241677269339561
iteration 195, loss = 0.00169711175840348
iteration 196, loss = 0.0029294961132109165
iteration 197, loss = 0.001930199097841978
iteration 198, loss = 0.002709451597183943
iteration 199, loss = 0.0018106097122654319
iteration 200, loss = 0.002991087269037962
iteration 201, loss = 0.002630931092426181
iteration 202, loss = 0.0034725922159850597
iteration 203, loss = 0.002806526143103838
iteration 204, loss = 0.003571608569473028
iteration 205, loss = 0.0020377621985971928
iteration 206, loss = 0.001796139171347022
iteration 207, loss = 0.0016927974065765738
iteration 208, loss = 0.0024331826716661453
iteration 209, loss = 0.0019287090981379151
iteration 210, loss = 0.002037710975855589
iteration 211, loss = 0.002211031736806035
iteration 212, loss = 0.002270049648359418
iteration 213, loss = 0.0019110277062281966
iteration 214, loss = 0.003168858587741852
iteration 215, loss = 0.002644577529281378
iteration 216, loss = 0.002045770175755024
iteration 217, loss = 0.001951265032403171
iteration 218, loss = 0.003093149745836854
iteration 219, loss = 0.0019482639618217945
iteration 220, loss = 0.002250189892947674
iteration 221, loss = 0.0018259539501741529
iteration 222, loss = 0.003393848892301321
iteration 223, loss = 0.002042657695710659
iteration 224, loss = 0.0017669190419837832
iteration 225, loss = 0.0018276185728609562
iteration 226, loss = 0.0018241278594359756
iteration 227, loss = 0.003800207283347845
iteration 228, loss = 0.002407802501693368
iteration 229, loss = 0.0021978181321173906
iteration 230, loss = 0.0016948687843978405
iteration 231, loss = 0.0023350128903985023
iteration 232, loss = 0.00248627457767725
iteration 233, loss = 0.002153918845579028
iteration 234, loss = 0.002791241742670536
iteration 235, loss = 0.0037282647099345922
iteration 236, loss = 0.002131558721885085
iteration 237, loss = 0.002113936236128211
iteration 238, loss = 0.0020457161590456963
iteration 239, loss = 0.0016553786117583513
iteration 240, loss = 0.0019189442973583937
iteration 241, loss = 0.002621889114379883
iteration 242, loss = 0.002062539104372263
iteration 243, loss = 0.0021463902667164803
iteration 244, loss = 0.0025736906100064516
iteration 245, loss = 0.0018847084138542414
iteration 246, loss = 0.002390427514910698
iteration 247, loss = 0.004093867726624012
iteration 248, loss = 0.0017345163505524397
iteration 249, loss = 0.002002557972446084
iteration 250, loss = 0.0029993089847266674
iteration 251, loss = 0.0019581420347094536
iteration 252, loss = 0.0021300737280398607
iteration 253, loss = 0.0029121283441781998
iteration 254, loss = 0.003513164119794965
iteration 255, loss = 0.0038842102512717247
iteration 256, loss = 0.0021198843605816364
iteration 257, loss = 0.003655410138890147
iteration 258, loss = 0.0020594769157469273
iteration 259, loss = 0.002483055694028735
iteration 260, loss = 0.001924498355947435
iteration 261, loss = 0.002562836976721883
iteration 262, loss = 0.002701301360502839
iteration 263, loss = 0.0018687357660382986
iteration 264, loss = 0.0014461044920608401
iteration 265, loss = 0.0022179868537932634
iteration 266, loss = 0.0020229497458785772
iteration 267, loss = 0.001766937435604632
iteration 268, loss = 0.002116417046636343
iteration 269, loss = 0.0018835481023415923
iteration 270, loss = 0.0024011072237044573
iteration 271, loss = 0.002009784569963813
iteration 272, loss = 0.0018573112320154905
iteration 273, loss = 0.0026733584236353636
iteration 274, loss = 0.0018381455447524786
iteration 275, loss = 0.00369379878975451
iteration 276, loss = 0.001904154778458178
iteration 277, loss = 0.001953743863850832
iteration 278, loss = 0.003935013432055712
iteration 279, loss = 0.002197965979576111
iteration 280, loss = 0.001729687792249024
iteration 281, loss = 0.0016558157512918115
iteration 282, loss = 0.0021041890140622854
iteration 283, loss = 0.002067429246380925
iteration 284, loss = 0.001973648788407445
iteration 285, loss = 0.0023918538354337215
iteration 286, loss = 0.0021272110752761364
iteration 287, loss = 0.001915976987220347
iteration 288, loss = 0.0018154194112867117
iteration 289, loss = 0.0019436090951785445
iteration 290, loss = 0.0023595988750457764
iteration 291, loss = 0.0020051910541951656
iteration 292, loss = 0.0017794626764953136
iteration 293, loss = 0.0022643485572189093
iteration 294, loss = 0.0020044520497322083
iteration 295, loss = 0.0019994217436760664
iteration 296, loss = 0.0020337202586233616
iteration 297, loss = 0.00409128749743104
iteration 298, loss = 0.0019250144250690937
iteration 299, loss = 0.0017004272667691112
iteration 300, loss = 0.0017493158811703324
iteration 1, loss = 0.002351101953536272
iteration 2, loss = 0.0025490636471658945
iteration 3, loss = 0.0037249198649078608
iteration 4, loss = 0.004979268182069063
iteration 5, loss = 0.0018700739601626992
iteration 6, loss = 0.0022410601377487183
iteration 7, loss = 0.0038234926760196686
iteration 8, loss = 0.004707814659923315
iteration 9, loss = 0.002286400878801942
iteration 10, loss = 0.002446682658046484
iteration 11, loss = 0.0017684698104858398
iteration 12, loss = 0.001821274170652032
iteration 13, loss = 0.0019903031643480062
iteration 14, loss = 0.0021067047491669655
iteration 15, loss = 0.001905961544252932
iteration 16, loss = 0.0018789017340168357
iteration 17, loss = 0.0024243334773927927
iteration 18, loss = 0.002695197705179453
iteration 19, loss = 0.0019038340542465448
iteration 20, loss = 0.002168893814086914
iteration 21, loss = 0.0018986177165061235
iteration 22, loss = 0.0023877446074038744
iteration 23, loss = 0.00213767122477293
iteration 24, loss = 0.002337069483473897
iteration 25, loss = 0.0017078068340197206
iteration 26, loss = 0.0017946870066225529
iteration 27, loss = 0.002485348144546151
iteration 28, loss = 0.002091805450618267
iteration 29, loss = 0.0021607878152281046
iteration 30, loss = 0.004122419282793999
iteration 31, loss = 0.0020740600302815437
iteration 32, loss = 0.002110049594193697
iteration 33, loss = 0.0017233986873179674
iteration 34, loss = 0.0018600595649331808
iteration 35, loss = 0.001912147388793528
iteration 36, loss = 0.0030190087854862213
iteration 37, loss = 0.0021245870739221573
iteration 38, loss = 0.001875885296612978
iteration 39, loss = 0.001930421101860702
iteration 40, loss = 0.0016237216768786311
iteration 41, loss = 0.0016684652073308825
iteration 42, loss = 0.0015843547880649567
iteration 43, loss = 0.0028098279144614935
iteration 44, loss = 0.0020901483949273825
iteration 45, loss = 0.0021556871943175793
iteration 46, loss = 0.0017857917118817568
iteration 47, loss = 0.002096727956086397
iteration 48, loss = 0.0018666790565475821
iteration 49, loss = 0.002307844813913107
iteration 50, loss = 0.0020083372946828604
iteration 51, loss = 0.003713276470080018
iteration 52, loss = 0.002167851198464632
iteration 53, loss = 0.002283984562382102
iteration 54, loss = 0.0015833739889785647
iteration 55, loss = 0.004111173562705517
iteration 56, loss = 0.0018470522481948137
iteration 57, loss = 0.001863017212599516
iteration 58, loss = 0.001681396970525384
iteration 59, loss = 0.00183884147554636
iteration 60, loss = 0.0020036788191646338
iteration 61, loss = 0.002428429201245308
iteration 62, loss = 0.001779488637112081
iteration 63, loss = 0.0028071890119463205
iteration 64, loss = 0.001915232976898551
iteration 65, loss = 0.002426343970000744
iteration 66, loss = 0.0023367947433143854
iteration 67, loss = 0.0033838271629065275
iteration 68, loss = 0.0016284098383039236
iteration 69, loss = 0.0022286665625870228
iteration 70, loss = 0.0019800153095275164
iteration 71, loss = 0.0017841131193563342
iteration 72, loss = 0.003821220248937607
iteration 73, loss = 0.0019918796606361866
iteration 74, loss = 0.0026454785838723183
iteration 75, loss = 0.0018712200690060854
iteration 76, loss = 0.001984753180295229
iteration 77, loss = 0.004032914061099291
iteration 78, loss = 0.0018348705489188433
iteration 79, loss = 0.00304815755225718
iteration 80, loss = 0.0034699097741395235
iteration 81, loss = 0.0032444456592202187
iteration 82, loss = 0.001706784823909402
iteration 83, loss = 0.001863105222582817
iteration 84, loss = 0.0017093021888285875
iteration 85, loss = 0.0035448987036943436
iteration 86, loss = 0.00203360291197896
iteration 87, loss = 0.0017329927068203688
iteration 88, loss = 0.0021972202230244875
iteration 89, loss = 0.0018286495469510555
iteration 90, loss = 0.003924513701349497
iteration 91, loss = 0.0023505487479269505
iteration 92, loss = 0.0021149644162505865
iteration 93, loss = 0.002313442761078477
iteration 94, loss = 0.002053228672593832
iteration 95, loss = 0.00195295549929142
iteration 96, loss = 0.0020196607802063227
iteration 97, loss = 0.0018658392364159226
iteration 98, loss = 0.001998180290684104
iteration 99, loss = 0.0018211539136245847
iteration 100, loss = 0.002158054616302252
iteration 101, loss = 0.0019272265490144491
iteration 102, loss = 0.0027658334001898766
iteration 103, loss = 0.0020789846312254667
iteration 104, loss = 0.0021231852006167173
iteration 105, loss = 0.0017953943461179733
iteration 106, loss = 0.0016115467296913266
iteration 107, loss = 0.0024716893676668406
iteration 108, loss = 0.0027909069322049618
iteration 109, loss = 0.0018766254652291536
iteration 110, loss = 0.002488462720066309
iteration 111, loss = 0.0016780284931883216
iteration 112, loss = 0.0023037891369313
iteration 113, loss = 0.002075282158330083
iteration 114, loss = 0.0019468421814963222
iteration 115, loss = 0.0017118350369855762
iteration 116, loss = 0.0019994108006358147
iteration 117, loss = 0.0025983264204114676
iteration 118, loss = 0.002126643666997552
iteration 119, loss = 0.00401624571532011
iteration 120, loss = 0.0018323714612051845
iteration 121, loss = 0.002089073183014989
iteration 122, loss = 0.0017840159125626087
iteration 123, loss = 0.00257880543358624
iteration 124, loss = 0.002642508130520582
iteration 125, loss = 0.0016199933597818017
iteration 126, loss = 0.001545421197079122
iteration 127, loss = 0.0020056557841598988
iteration 128, loss = 0.0017752449493855238
iteration 129, loss = 0.0020312995184212923
iteration 130, loss = 0.002307447139173746
iteration 131, loss = 0.0018769874004647136
iteration 132, loss = 0.002616910031065345
iteration 133, loss = 0.0015493725659325719
iteration 134, loss = 0.0023979968391358852
iteration 135, loss = 0.0040857186540961266
iteration 136, loss = 0.0017994800582528114
iteration 137, loss = 0.0017043168190866709
iteration 138, loss = 0.0021759120281785727
iteration 139, loss = 0.002335631288588047
iteration 140, loss = 0.002272008452564478
iteration 141, loss = 0.002394453389570117
iteration 142, loss = 0.0024116490967571735
iteration 143, loss = 0.0022300269920378923
iteration 144, loss = 0.0018678545020520687
iteration 145, loss = 0.002047845395281911
iteration 146, loss = 0.0017627252964302897
iteration 147, loss = 0.0023207245394587517
iteration 148, loss = 0.0016506564570590854
iteration 149, loss = 0.0018614616710692644
iteration 150, loss = 0.001701599219813943
iteration 151, loss = 0.0019806451164186
iteration 152, loss = 0.002105412306264043
iteration 153, loss = 0.002209685742855072
iteration 154, loss = 0.001992616569623351
iteration 155, loss = 0.0025325161404907703
iteration 156, loss = 0.0020270301029086113
iteration 157, loss = 0.0018077563727274537
iteration 158, loss = 0.0025246066506952047
iteration 159, loss = 0.0031055829022079706
iteration 160, loss = 0.0019338041311129928
iteration 161, loss = 0.002468898193910718
iteration 162, loss = 0.0030545960180461407
iteration 163, loss = 0.0019699030090123415
iteration 164, loss = 0.0018672016449272633
iteration 165, loss = 0.0023035279009491205
iteration 166, loss = 0.0018401308916509151
iteration 167, loss = 0.002135707763954997
iteration 168, loss = 0.0015253310557454824
iteration 169, loss = 0.0018842824501916766
iteration 170, loss = 0.0018059071153402328
iteration 171, loss = 0.0019932042341679335
iteration 172, loss = 0.0026358270552009344
iteration 173, loss = 0.0029829111881554127
iteration 174, loss = 0.0022822029422968626
iteration 175, loss = 0.0016888686222955585
iteration 176, loss = 0.001992681995034218
iteration 177, loss = 0.002454970730468631
iteration 178, loss = 0.002055700169876218
iteration 179, loss = 0.0022853962145745754
iteration 180, loss = 0.002045293804258108
iteration 181, loss = 0.0019128862768411636
iteration 182, loss = 0.0017752081621438265
iteration 183, loss = 0.0018344834679737687
iteration 184, loss = 0.0020724486093968153
iteration 185, loss = 0.0019176385831087828
iteration 186, loss = 0.0020317635498940945
iteration 187, loss = 0.0021512426901608706
iteration 188, loss = 0.00201550149358809
iteration 189, loss = 0.003014789428561926
iteration 190, loss = 0.004287916235625744
iteration 191, loss = 0.002173781394958496
iteration 192, loss = 0.0026468592695891857
iteration 193, loss = 0.002128246705979109
iteration 194, loss = 0.004815007094293833
iteration 195, loss = 0.003087311750277877
iteration 196, loss = 0.0018166527152061462
iteration 197, loss = 0.0021152528934180737
iteration 198, loss = 0.0021942462772130966
iteration 199, loss = 0.002162996446713805
iteration 200, loss = 0.001959184417501092
iteration 201, loss = 0.003998140804469585
iteration 202, loss = 0.0021873069927096367
iteration 203, loss = 0.003901867428794503
iteration 204, loss = 0.0017740511102601886
iteration 205, loss = 0.0019375173142179847
iteration 206, loss = 0.0020338930189609528
iteration 207, loss = 0.0017518082167953253
iteration 208, loss = 0.0021588888484984636
iteration 209, loss = 0.002857764018699527
iteration 210, loss = 0.0020849250722676516
iteration 211, loss = 0.0016245396109297872
iteration 212, loss = 0.0016867914237082005
iteration 213, loss = 0.00199303962290287
iteration 214, loss = 0.0023684431798756123
iteration 215, loss = 0.003767611924558878
iteration 216, loss = 0.0025581601075828075
iteration 217, loss = 0.0017538397805765271
iteration 218, loss = 0.0022223209962248802
iteration 219, loss = 0.0020884054247289896
iteration 220, loss = 0.0023518726229667664
iteration 221, loss = 0.00172616436611861
iteration 222, loss = 0.0017873578472062945
iteration 223, loss = 0.003457622602581978
iteration 224, loss = 0.0022121442016214132
iteration 225, loss = 0.0018694103928282857
iteration 226, loss = 0.0019791463855654
iteration 227, loss = 0.0024968013167381287
iteration 228, loss = 0.0030639944598078728
iteration 229, loss = 0.0018942359602078795
iteration 230, loss = 0.002080776961520314
iteration 231, loss = 0.0016547789564356208
iteration 232, loss = 0.003063349984586239
iteration 233, loss = 0.0021284057293087244
iteration 234, loss = 0.0022120990324765444
iteration 235, loss = 0.0038450430147349834
iteration 236, loss = 0.0021603552158921957
iteration 237, loss = 0.001830686698667705
iteration 238, loss = 0.002737237373366952
iteration 239, loss = 0.001980237429961562
iteration 240, loss = 0.00255594658665359
iteration 241, loss = 0.0017668463988229632
iteration 242, loss = 0.002555617829784751
iteration 243, loss = 0.001977162202820182
iteration 244, loss = 0.002231568330898881
iteration 245, loss = 0.0017813182203099132
iteration 246, loss = 0.001768383663147688
iteration 247, loss = 0.002267066854983568
iteration 248, loss = 0.0032314537093043327
iteration 249, loss = 0.002024332294240594
iteration 250, loss = 0.0016339269932359457
iteration 251, loss = 0.0018531004898250103
iteration 252, loss = 0.0017219135770574212
iteration 253, loss = 0.001877354457974434
iteration 254, loss = 0.004100208170711994
iteration 255, loss = 0.00156506581697613
iteration 256, loss = 0.0031358287669718266
iteration 257, loss = 0.0023100809194147587
iteration 258, loss = 0.0020546671003103256
iteration 259, loss = 0.0036061033606529236
iteration 260, loss = 0.0018561742035672069
iteration 261, loss = 0.0022262954153120518
iteration 262, loss = 0.0016002380289137363
iteration 263, loss = 0.0020897502545267344
iteration 264, loss = 0.0018177144229412079
iteration 265, loss = 0.003307292005047202
iteration 266, loss = 0.0017993159126490355
iteration 267, loss = 0.002206402365118265
iteration 268, loss = 0.0029117551166564226
iteration 269, loss = 0.0025565880350768566
iteration 270, loss = 0.0038308287039399147
iteration 271, loss = 0.0018193721771240234
iteration 272, loss = 0.001964172348380089
iteration 273, loss = 0.0020859709475189447
iteration 274, loss = 0.002111646346747875
iteration 275, loss = 0.0022555161267518997
iteration 276, loss = 0.0026422832161188126
iteration 277, loss = 0.0022934696171432734
iteration 278, loss = 0.003011522814631462
iteration 279, loss = 0.0019229018362239003
iteration 280, loss = 0.00234867911785841
iteration 281, loss = 0.0017317385645583272
iteration 282, loss = 0.0018752951873466372
iteration 283, loss = 0.0020768269896507263
iteration 284, loss = 0.0022583291865885258
iteration 285, loss = 0.0020650685764849186
iteration 286, loss = 0.001779767218977213
iteration 287, loss = 0.0021540436428040266
iteration 288, loss = 0.0018379377434030175
iteration 289, loss = 0.0019523634109646082
iteration 290, loss = 0.0017876068595796824
iteration 291, loss = 0.001814414979889989
iteration 292, loss = 0.0022126389667391777
iteration 293, loss = 0.0024799422826617956
iteration 294, loss = 0.0019683418795466423
iteration 295, loss = 0.0020552605856209993
iteration 296, loss = 0.002108868909999728
iteration 297, loss = 0.001688393298536539
iteration 298, loss = 0.0017347350949421525
iteration 299, loss = 0.002091481350362301
iteration 300, loss = 0.002227868651971221
iteration 1, loss = 0.0018057320266962051
iteration 2, loss = 0.0015543007757514715
iteration 3, loss = 0.002439359901472926
iteration 4, loss = 0.002004177076742053
iteration 5, loss = 0.001547468127682805
iteration 6, loss = 0.002221368718892336
iteration 7, loss = 0.0017759057227522135
iteration 8, loss = 0.002374825533479452
iteration 9, loss = 0.0017470064340159297
iteration 10, loss = 0.002165053505450487
iteration 11, loss = 0.0027744537219405174
iteration 12, loss = 0.0033074519596993923
iteration 13, loss = 0.0038714164402335882
iteration 14, loss = 0.0023463775869458914
iteration 15, loss = 0.0025548990815877914
iteration 16, loss = 0.0022193528711795807
iteration 17, loss = 0.0025655438657850027
iteration 18, loss = 0.0017035378841683269
iteration 19, loss = 0.0017637229757383466
iteration 20, loss = 0.0021439462434500456
iteration 21, loss = 0.0015221578069031239
iteration 22, loss = 0.001729104551486671
iteration 23, loss = 0.001885864301584661
iteration 24, loss = 0.002242865040898323
iteration 25, loss = 0.001880690106190741
iteration 26, loss = 0.002182846190407872
iteration 27, loss = 0.0037128101103007793
iteration 28, loss = 0.002132940571755171
iteration 29, loss = 0.0021925356704741716
iteration 30, loss = 0.002517303917557001
iteration 31, loss = 0.0027419794350862503
iteration 32, loss = 0.0019334438256919384
iteration 33, loss = 0.0021519155707210302
iteration 34, loss = 0.0021824962459504604
iteration 35, loss = 0.002219429472461343
iteration 36, loss = 0.0020922627300024033
iteration 37, loss = 0.002144909230992198
iteration 38, loss = 0.001868689781986177
iteration 39, loss = 0.001651689992286265
iteration 40, loss = 0.002683066762983799
iteration 41, loss = 0.002609421731904149
iteration 42, loss = 0.002201929222792387
iteration 43, loss = 0.0016378199215978384
iteration 44, loss = 0.002288110787048936
iteration 45, loss = 0.0018067683558911085
iteration 46, loss = 0.001962542301043868
iteration 47, loss = 0.002233424922451377
iteration 48, loss = 0.002009699586778879
iteration 49, loss = 0.0022013287525624037
iteration 50, loss = 0.0018771435134112835
iteration 51, loss = 0.0026721868198364973
iteration 52, loss = 0.0018536789575591683
iteration 53, loss = 0.0016515074530616403
iteration 54, loss = 0.003330659121274948
iteration 55, loss = 0.002445073099806905
iteration 56, loss = 0.0019043884240090847
iteration 57, loss = 0.00391059136018157
iteration 58, loss = 0.001905153039842844
iteration 59, loss = 0.002303905552253127
iteration 60, loss = 0.0029090414755046368
iteration 61, loss = 0.0016442666528746486
iteration 62, loss = 0.0021137541625648737
iteration 63, loss = 0.0017112151253968477
iteration 64, loss = 0.0042606014758348465
iteration 65, loss = 0.0020065330900251865
iteration 66, loss = 0.0023695798590779305
iteration 67, loss = 0.0020962515845894814
iteration 68, loss = 0.0021022213622927666
iteration 69, loss = 0.0015455031534656882
iteration 70, loss = 0.0021567796356976032
iteration 71, loss = 0.0023852528538554907
iteration 72, loss = 0.0017832373268902302
iteration 73, loss = 0.002273787744343281
iteration 74, loss = 0.00228677480481565
iteration 75, loss = 0.0019473585998639464
iteration 76, loss = 0.0025676898658275604
iteration 77, loss = 0.0019768006168305874
iteration 78, loss = 0.0019748168997466564
iteration 79, loss = 0.0018346045399084687
iteration 80, loss = 0.003204554319381714
iteration 81, loss = 0.0017766057280823588
iteration 82, loss = 0.001654373249039054
iteration 83, loss = 0.0016716517275199294
iteration 84, loss = 0.0018214802257716656
iteration 85, loss = 0.002051230752840638
iteration 86, loss = 0.00207505002617836
iteration 87, loss = 0.0019818695727735758
iteration 88, loss = 0.002591640455648303
iteration 89, loss = 0.0019403420155867934
iteration 90, loss = 0.0021225339733064175
iteration 91, loss = 0.002086004940792918
iteration 92, loss = 0.002770884893834591
iteration 93, loss = 0.002129925647750497
iteration 94, loss = 0.003981391433626413
iteration 95, loss = 0.001704484922811389
iteration 96, loss = 0.002611033618450165
iteration 97, loss = 0.0022762445732951164
iteration 98, loss = 0.0017771236598491669
iteration 99, loss = 0.0018928650533780456
iteration 100, loss = 0.0022419514134526253
iteration 101, loss = 0.0019763584714382887
iteration 102, loss = 0.002021974651142955
iteration 103, loss = 0.0017801437061280012
iteration 104, loss = 0.00290109496563673
iteration 105, loss = 0.0017131888307631016
iteration 106, loss = 0.001714018639177084
iteration 107, loss = 0.0022773942910134792
iteration 108, loss = 0.002907349029555917
iteration 109, loss = 0.0017512512858957052
iteration 110, loss = 0.0019412118708714843
iteration 111, loss = 0.003250707872211933
iteration 112, loss = 0.0027240142226219177
iteration 113, loss = 0.0022678228560835123
iteration 114, loss = 0.0018950841622427106
iteration 115, loss = 0.0034580612555146217
iteration 116, loss = 0.0019260706612840295
iteration 117, loss = 0.0018201986094936728
iteration 118, loss = 0.001800837111659348
iteration 119, loss = 0.0032812899444252253
iteration 120, loss = 0.0018557519651949406
iteration 121, loss = 0.0018156670266762376
iteration 122, loss = 0.0018138026352971792
iteration 123, loss = 0.003130212426185608
iteration 124, loss = 0.002342058578506112
iteration 125, loss = 0.0018003322184085846
iteration 126, loss = 0.002019632374867797
iteration 127, loss = 0.002103374805301428
iteration 128, loss = 0.0016748679336160421
iteration 129, loss = 0.001906469464302063
iteration 130, loss = 0.003921488765627146
iteration 131, loss = 0.002387426095083356
iteration 132, loss = 0.002635976066812873
iteration 133, loss = 0.003998180385679007
iteration 134, loss = 0.0015985347563400865
iteration 135, loss = 0.0020511809270828962
iteration 136, loss = 0.002021653112024069
iteration 137, loss = 0.003323057433590293
iteration 138, loss = 0.0019005835056304932
iteration 139, loss = 0.0022833580151200294
iteration 140, loss = 0.0020565888844430447
iteration 141, loss = 0.001821622485294938
iteration 142, loss = 0.0023718206211924553
iteration 143, loss = 0.0019061851780861616
iteration 144, loss = 0.0027172730769962072
iteration 145, loss = 0.0018522883765399456
iteration 146, loss = 0.0018114184495061636
iteration 147, loss = 0.002697189338505268
iteration 148, loss = 0.0018680146895349026
iteration 149, loss = 0.0018401638371869922
iteration 150, loss = 0.0031105666421353817
iteration 151, loss = 0.002133741741999984
iteration 152, loss = 0.0035349433310329914
iteration 153, loss = 0.0018029583152383566
iteration 154, loss = 0.0017521934350952506
iteration 155, loss = 0.0024592294357717037
iteration 156, loss = 0.002135569229722023
iteration 157, loss = 0.0018302281387150288
iteration 158, loss = 0.0020190924406051636
iteration 159, loss = 0.0018868471961468458
iteration 160, loss = 0.0018001417629420757
iteration 161, loss = 0.002231915947049856
iteration 162, loss = 0.002018536673858762
iteration 163, loss = 0.0017515304498374462
iteration 164, loss = 0.0036337689962238073
iteration 165, loss = 0.002074552234262228
iteration 166, loss = 0.0019143962999805808
iteration 167, loss = 0.0020999531261622906
iteration 168, loss = 0.0016972646117210388
iteration 169, loss = 0.002901989035308361
iteration 170, loss = 0.002617600141093135
iteration 171, loss = 0.00264484784565866
iteration 172, loss = 0.002898080972954631
iteration 173, loss = 0.0019891788251698017
iteration 174, loss = 0.0018702652305364609
iteration 175, loss = 0.0017685547936707735
iteration 176, loss = 0.0020539346151053905
iteration 177, loss = 0.0017570388736203313
iteration 178, loss = 0.002648040885105729
iteration 179, loss = 0.0019091434078291059
iteration 180, loss = 0.0017190998187288642
iteration 181, loss = 0.0019412667024880648
iteration 182, loss = 0.0026555638760328293
iteration 183, loss = 0.0020702725742012262
iteration 184, loss = 0.0018839305266737938
iteration 185, loss = 0.0019089165143668652
iteration 186, loss = 0.002037499565631151
iteration 187, loss = 0.0017975461669266224
iteration 188, loss = 0.0033798147924244404
iteration 189, loss = 0.002041405066847801
iteration 190, loss = 0.001978210173547268
iteration 191, loss = 0.0021869686897844076
iteration 192, loss = 0.0025911168195307255
iteration 193, loss = 0.0024125371128320694
iteration 194, loss = 0.002047798130661249
iteration 195, loss = 0.0025022225454449654
iteration 196, loss = 0.0018700453219935298
iteration 197, loss = 0.0019441840704530478
iteration 198, loss = 0.0030722639057785273
iteration 199, loss = 0.0018594564171507955
iteration 200, loss = 0.00197274936363101
iteration 201, loss = 0.0020392981823533773
iteration 202, loss = 0.0017543999711051583
iteration 203, loss = 0.0015998147428035736
iteration 204, loss = 0.002159146824851632
iteration 205, loss = 0.0020674148108810186
iteration 206, loss = 0.004367239307612181
iteration 207, loss = 0.0037368321791291237
iteration 208, loss = 0.00233854609541595
iteration 209, loss = 0.001804935745894909
iteration 210, loss = 0.002204076386988163
iteration 211, loss = 0.0018286006525158882
iteration 212, loss = 0.0015606050146743655
iteration 213, loss = 0.0020499646198004484
iteration 214, loss = 0.0018014416564255953
iteration 215, loss = 0.0020546463783830404
iteration 216, loss = 0.002261457731947303
iteration 217, loss = 0.0017843437381088734
iteration 218, loss = 0.002003255533054471
iteration 219, loss = 0.0019610081799328327
iteration 220, loss = 0.002241076435893774
iteration 221, loss = 0.001954962033778429
iteration 222, loss = 0.002029948402196169
iteration 223, loss = 0.002529705874621868
iteration 224, loss = 0.0018570551183074713
iteration 225, loss = 0.0036343527026474476
iteration 226, loss = 0.002297675469890237
iteration 227, loss = 0.0035040301736444235
iteration 228, loss = 0.0020435680635273457
iteration 229, loss = 0.0033078561536967754
iteration 230, loss = 0.0019658301025629044
iteration 231, loss = 0.0019944533705711365
iteration 232, loss = 0.0021182510536164045
iteration 233, loss = 0.0026735698338598013
iteration 234, loss = 0.0036874315701425076
iteration 235, loss = 0.0018009558552876115
iteration 236, loss = 0.0021820866968482733
iteration 237, loss = 0.002089266199618578
iteration 238, loss = 0.00171837885864079
iteration 239, loss = 0.0017975384835153818
iteration 240, loss = 0.0018820578698068857
iteration 241, loss = 0.002174598630517721
iteration 242, loss = 0.0020130430348217487
iteration 243, loss = 0.0035685268230736256
iteration 244, loss = 0.002082248218357563
iteration 245, loss = 0.0030831494368612766
iteration 246, loss = 0.0015856981044635177
iteration 247, loss = 0.0022729013580828905
iteration 248, loss = 0.004144371021538973
iteration 249, loss = 0.0024976134300231934
iteration 250, loss = 0.00203069020062685
iteration 251, loss = 0.0019447564845904708
iteration 252, loss = 0.0038965390995144844
iteration 253, loss = 0.0018559480085968971
iteration 254, loss = 0.0018600671319290996
iteration 255, loss = 0.002020935295149684
iteration 256, loss = 0.0026433432940393686
iteration 257, loss = 0.001779563957825303
iteration 258, loss = 0.001978611806407571
iteration 259, loss = 0.003749560099095106
iteration 260, loss = 0.0026429996360093355
iteration 261, loss = 0.002771242056041956
iteration 262, loss = 0.0018362220143899322
iteration 263, loss = 0.0035065000411123037
iteration 264, loss = 0.0016978211933746934
iteration 265, loss = 0.0024277924094349146
iteration 266, loss = 0.0017374471062794328
iteration 267, loss = 0.001784959458746016
iteration 268, loss = 0.001897588954307139
iteration 269, loss = 0.002141036558896303
iteration 270, loss = 0.0017774050356820226
iteration 271, loss = 0.002145289210602641
iteration 272, loss = 0.0021824450232088566
iteration 273, loss = 0.003703658701851964
iteration 274, loss = 0.0017022175015881658
iteration 275, loss = 0.0015881131403148174
iteration 276, loss = 0.0018111332319676876
iteration 277, loss = 0.003802335122600198
iteration 278, loss = 0.003238927572965622
iteration 279, loss = 0.0018628116231411695
iteration 280, loss = 0.002001830842345953
iteration 281, loss = 0.0017984331352636218
iteration 282, loss = 0.0017084557330235839
iteration 283, loss = 0.002079596044495702
iteration 284, loss = 0.003385871881619096
iteration 285, loss = 0.0018992067780345678
iteration 286, loss = 0.0038233872037380934
iteration 287, loss = 0.0026461142115294933
iteration 288, loss = 0.0021554590202867985
iteration 289, loss = 0.003272630274295807
iteration 290, loss = 0.0020334317814558744
iteration 291, loss = 0.0034930864349007607
iteration 292, loss = 0.0018407978350296617
iteration 293, loss = 0.002037647645920515
iteration 294, loss = 0.0018707977142184973
iteration 295, loss = 0.00179360993206501
iteration 296, loss = 0.0033357199281454086
iteration 297, loss = 0.0023855557665228844
iteration 298, loss = 0.0016482119681313634
iteration 299, loss = 0.0018742065876722336
iteration 300, loss = 0.002134013455361128
iteration 1, loss = 0.0021121057216078043
iteration 2, loss = 0.0021320716477930546
iteration 3, loss = 0.001677933381870389
iteration 4, loss = 0.0017594967503100634
iteration 5, loss = 0.004525847267359495
iteration 6, loss = 0.002438289113342762
iteration 7, loss = 0.0020331228151917458
iteration 8, loss = 0.0028740272391587496
iteration 9, loss = 0.002084882464259863
iteration 10, loss = 0.0034775345120579004
iteration 11, loss = 0.0016923752846196294
iteration 12, loss = 0.0021008981857448816
iteration 13, loss = 0.002679645549505949
iteration 14, loss = 0.003749281167984009
iteration 15, loss = 0.003807800356298685
iteration 16, loss = 0.001890593208372593
iteration 17, loss = 0.0025274031795561314
iteration 18, loss = 0.0024942948948591948
iteration 19, loss = 0.0022575228940695524
iteration 20, loss = 0.0019943767692893744
iteration 21, loss = 0.0038703610189259052
iteration 22, loss = 0.0020419973880052567
iteration 23, loss = 0.0023380324710160494
iteration 24, loss = 0.0017231456004083157
iteration 25, loss = 0.0020845774561166763
iteration 26, loss = 0.0016735875979065895
iteration 27, loss = 0.0018933535320684314
iteration 28, loss = 0.0018190430710092187
iteration 29, loss = 0.0016399306477978826
iteration 30, loss = 0.0017868593567982316
iteration 31, loss = 0.003211116651073098
iteration 32, loss = 0.0019944636151194572
iteration 33, loss = 0.0017371673602610826
iteration 34, loss = 0.0023860654328018427
iteration 35, loss = 0.0017149413470178843
iteration 36, loss = 0.004183928947895765
iteration 37, loss = 0.0019921481143683195
iteration 38, loss = 0.0018641229253262281
iteration 39, loss = 0.0018691367004066706
iteration 40, loss = 0.0016227017622441053
iteration 41, loss = 0.0016616402426734567
iteration 42, loss = 0.0017938563833013177
iteration 43, loss = 0.00200516520999372
iteration 44, loss = 0.0016956828767433763
iteration 45, loss = 0.002254257444292307
iteration 46, loss = 0.002754148794338107
iteration 47, loss = 0.002215899061411619
iteration 48, loss = 0.0019051044946536422
iteration 49, loss = 0.002112553920596838
iteration 50, loss = 0.002201070776209235
iteration 51, loss = 0.0020325176883488894
iteration 52, loss = 0.0022179926745593548
iteration 53, loss = 0.0017175502143800259
iteration 54, loss = 0.002049149014055729
iteration 55, loss = 0.003900919808074832
iteration 56, loss = 0.0020975228399038315
iteration 57, loss = 0.0020278070587664843
iteration 58, loss = 0.001884551951661706
iteration 59, loss = 0.003224071813747287
iteration 60, loss = 0.00210264278575778
iteration 61, loss = 0.0032291628886014223
iteration 62, loss = 0.001634483691304922
iteration 63, loss = 0.0018448736518621445
iteration 64, loss = 0.0014145893510431051
iteration 65, loss = 0.0018363406416028738
iteration 66, loss = 0.0023522654082626104
iteration 67, loss = 0.0019665686413645744
iteration 68, loss = 0.002339036436751485
iteration 69, loss = 0.0016424867790192366
iteration 70, loss = 0.0017869793809950352
iteration 71, loss = 0.0025398284196853638
iteration 72, loss = 0.0022382314782589674
iteration 73, loss = 0.003201338229700923
iteration 74, loss = 0.0017911548493430018
iteration 75, loss = 0.0024405813310295343
iteration 76, loss = 0.0019341728184372187
iteration 77, loss = 0.0023693228140473366
iteration 78, loss = 0.002146146958693862
iteration 79, loss = 0.0030217505991458893
iteration 80, loss = 0.001940353075042367
iteration 81, loss = 0.0019135104957967997
iteration 82, loss = 0.001616771100088954
iteration 83, loss = 0.0020181480795145035
iteration 84, loss = 0.0021369471214711666
iteration 85, loss = 0.001899491180665791
iteration 86, loss = 0.002449020277708769
iteration 87, loss = 0.0016193806659430265
iteration 88, loss = 0.0032389904372394085
iteration 89, loss = 0.0018166755326092243
iteration 90, loss = 0.002647930756211281
iteration 91, loss = 0.0024346073623746634
iteration 92, loss = 0.0019088581902906299
iteration 93, loss = 0.005504967179149389
iteration 94, loss = 0.0017139158444479108
iteration 95, loss = 0.0021540371235460043
iteration 96, loss = 0.0020245101768523455
iteration 97, loss = 0.0019274477381259203
iteration 98, loss = 0.0020262745674699545
iteration 99, loss = 0.0017815097235143185
iteration 100, loss = 0.003668789053335786
iteration 101, loss = 0.0018676028121262789
iteration 102, loss = 0.0019171915482729673
iteration 103, loss = 0.0022221256513148546
iteration 104, loss = 0.002205010736361146
iteration 105, loss = 0.0021893298253417015
iteration 106, loss = 0.0026280456222593784
iteration 107, loss = 0.002725357888266444
iteration 108, loss = 0.002037639729678631
iteration 109, loss = 0.001644182950258255
iteration 110, loss = 0.003532474860548973
iteration 111, loss = 0.002797164022922516
iteration 112, loss = 0.003695875173434615
iteration 113, loss = 0.002896158490329981
iteration 114, loss = 0.0025120696518570185
iteration 115, loss = 0.0020612499210983515
iteration 116, loss = 0.0018005110323429108
iteration 117, loss = 0.001759418286383152
iteration 118, loss = 0.002253674902021885
iteration 119, loss = 0.0019994345493614674
iteration 120, loss = 0.0020154034718871117
iteration 121, loss = 0.002353204181417823
iteration 122, loss = 0.002004917711019516
iteration 123, loss = 0.0018333412008360028
iteration 124, loss = 0.002077427925541997
iteration 125, loss = 0.0017861027736216784
iteration 126, loss = 0.0019078092882409692
iteration 127, loss = 0.0024047137703746557
iteration 128, loss = 0.0022743239533156157
iteration 129, loss = 0.0018590261461213231
iteration 130, loss = 0.0023148718755692244
iteration 131, loss = 0.0030232088174670935
iteration 132, loss = 0.0035963028203696012
iteration 133, loss = 0.0016941685462370515
iteration 134, loss = 0.0016031689010560513
iteration 135, loss = 0.0026429856661707163
iteration 136, loss = 0.0018526767380535603
iteration 137, loss = 0.0017565913731232285
iteration 138, loss = 0.0016576857306063175
iteration 139, loss = 0.0020315488800406456
iteration 140, loss = 0.00362218520604074
iteration 141, loss = 0.0016750963404774666
iteration 142, loss = 0.0015747258439660072
iteration 143, loss = 0.003899096744135022
iteration 144, loss = 0.0035043295938521624
iteration 145, loss = 0.003733523190021515
iteration 146, loss = 0.0023203932214528322
iteration 147, loss = 0.0026010358706116676
iteration 148, loss = 0.002091767732053995
iteration 149, loss = 0.0018471959047019482
iteration 150, loss = 0.001810927875339985
iteration 151, loss = 0.0021418160758912563
iteration 152, loss = 0.001869981293566525
iteration 153, loss = 0.002022600034251809
iteration 154, loss = 0.0018853223882615566
iteration 155, loss = 0.002132517984136939
iteration 156, loss = 0.001963483402505517
iteration 157, loss = 0.002260669134557247
iteration 158, loss = 0.0015094960108399391
iteration 159, loss = 0.00274614873342216
iteration 160, loss = 0.001714217709377408
iteration 161, loss = 0.0022564507089555264
iteration 162, loss = 0.001940410933457315
iteration 163, loss = 0.0023807280231267214
iteration 164, loss = 0.002631855197250843
iteration 165, loss = 0.0025893927086144686
iteration 166, loss = 0.0017736104782670736
iteration 167, loss = 0.0017105222214013338
iteration 168, loss = 0.002021270338445902
iteration 169, loss = 0.001941424678079784
iteration 170, loss = 0.0019670003093779087
iteration 171, loss = 0.0018951832316815853
iteration 172, loss = 0.0026366643141955137
iteration 173, loss = 0.0025239295791834593
iteration 174, loss = 0.002506091957911849
iteration 175, loss = 0.001769874943420291
iteration 176, loss = 0.0020728970412164927
iteration 177, loss = 0.0020198924466967583
iteration 178, loss = 0.002450687112286687
iteration 179, loss = 0.0019822961185127497
iteration 180, loss = 0.0016297773690894246
iteration 181, loss = 0.00223467405885458
iteration 182, loss = 0.00253477250225842
iteration 183, loss = 0.004350104369223118
iteration 184, loss = 0.0021978141739964485
iteration 185, loss = 0.002046241657808423
iteration 186, loss = 0.002165673766285181
iteration 187, loss = 0.0020599120762199163
iteration 188, loss = 0.0018413844518363476
iteration 189, loss = 0.0018374574137851596
iteration 190, loss = 0.001819295808672905
iteration 191, loss = 0.002499992959201336
iteration 192, loss = 0.0015996426809579134
iteration 193, loss = 0.0017871614545583725
iteration 194, loss = 0.002027584705501795
iteration 195, loss = 0.0020935500506311655
iteration 196, loss = 0.0021009594202041626
iteration 197, loss = 0.002036439022049308
iteration 198, loss = 0.0022773081436753273
iteration 199, loss = 0.0019413242116570473
iteration 200, loss = 0.0021052020601928234
iteration 201, loss = 0.0017625194741412997
iteration 202, loss = 0.001996623817831278
iteration 203, loss = 0.0022016819566488266
iteration 204, loss = 0.002047289628535509
iteration 205, loss = 0.0027857187669724226
iteration 206, loss = 0.0024872305803000927
iteration 207, loss = 0.002892959862947464
iteration 208, loss = 0.0018834490329027176
iteration 209, loss = 0.0021767953876405954
iteration 210, loss = 0.0018136597936972976
iteration 211, loss = 0.002220326568931341
iteration 212, loss = 0.0025637594517320395
iteration 213, loss = 0.0033092128578573465
iteration 214, loss = 0.003320267889648676
iteration 215, loss = 0.0018422603607177734
iteration 216, loss = 0.0021463714074343443
iteration 217, loss = 0.0015433569205924869
iteration 218, loss = 0.0020796339958906174
iteration 219, loss = 0.002577614039182663
iteration 220, loss = 0.001979971304535866
iteration 221, loss = 0.0030665083322674036
iteration 222, loss = 0.0018078911816701293
iteration 223, loss = 0.0022722547873854637
iteration 224, loss = 0.004204895813018084
iteration 225, loss = 0.0019907252863049507
iteration 226, loss = 0.0020362420473247766
iteration 227, loss = 0.0020225001499056816
iteration 228, loss = 0.0018660641508176923
iteration 229, loss = 0.002887552371248603
iteration 230, loss = 0.0017268770607188344
iteration 231, loss = 0.0020592526998370886
iteration 232, loss = 0.0019829822704195976
iteration 233, loss = 0.0019523678347468376
iteration 234, loss = 0.0018031320068985224
iteration 235, loss = 0.002734208246693015
iteration 236, loss = 0.001865277299657464
iteration 237, loss = 0.00276571954600513
iteration 238, loss = 0.003334496868774295
iteration 239, loss = 0.0021077976562082767
iteration 240, loss = 0.0018807186279445887
iteration 241, loss = 0.002699650125578046
iteration 242, loss = 0.0017800978384912014
iteration 243, loss = 0.003781048348173499
iteration 244, loss = 0.002092397306114435
iteration 245, loss = 0.004261348862200975
iteration 246, loss = 0.0019149297149851918
iteration 247, loss = 0.004493034910410643
iteration 248, loss = 0.0032765811774879694
iteration 249, loss = 0.001801960519514978
iteration 250, loss = 0.0020518722012639046
iteration 251, loss = 0.0020443815737962723
iteration 252, loss = 0.002094328636303544
iteration 253, loss = 0.001790092559531331
iteration 254, loss = 0.0019078594632446766
iteration 255, loss = 0.0021292795427143574
iteration 256, loss = 0.0020171564538031816
iteration 257, loss = 0.0021481337025761604
iteration 258, loss = 0.0017842149827629328
iteration 259, loss = 0.001960023073479533
iteration 260, loss = 0.0032032483723014593
iteration 261, loss = 0.0020575369708240032
iteration 262, loss = 0.003218326484784484
iteration 263, loss = 0.0023368948604911566
iteration 264, loss = 0.0027477950789034367
iteration 265, loss = 0.0023484493140131235
iteration 266, loss = 0.0019341246224939823
iteration 267, loss = 0.002209318336099386
iteration 268, loss = 0.002614164724946022
iteration 269, loss = 0.0018620038172230124
iteration 270, loss = 0.0016552344895899296
iteration 271, loss = 0.0024414013605564833
iteration 272, loss = 0.001910873339511454
iteration 273, loss = 0.0021264981478452682
iteration 274, loss = 0.0021408628672361374
iteration 275, loss = 0.0022017036098986864
iteration 276, loss = 0.0018935776315629482
iteration 277, loss = 0.0016559703508391976
iteration 278, loss = 0.001865261816419661
iteration 279, loss = 0.0017298826714977622
iteration 280, loss = 0.0020980210974812508
iteration 281, loss = 0.0018817600794136524
iteration 282, loss = 0.0018062819726765156
iteration 283, loss = 0.0018591033294796944
iteration 284, loss = 0.002265634946525097
iteration 285, loss = 0.0019444392528384924
iteration 286, loss = 0.0020967903546988964
iteration 287, loss = 0.0020823695231229067
iteration 288, loss = 0.0016964937094599009
iteration 289, loss = 0.0023569748736917973
iteration 290, loss = 0.0027876957319676876
iteration 291, loss = 0.001945997355505824
iteration 292, loss = 0.0017042865511029959
iteration 293, loss = 0.0018804582068696618
iteration 294, loss = 0.002378938253968954
iteration 295, loss = 0.002407543594017625
iteration 296, loss = 0.0032168710604310036
iteration 297, loss = 0.001984396018087864
iteration 298, loss = 0.002779285190626979
iteration 299, loss = 0.0023093009367585182
iteration 300, loss = 0.0020315793808549643
iteration 1, loss = 0.002603838685899973
iteration 2, loss = 0.00245564803481102
iteration 3, loss = 0.0027448278851807117
iteration 4, loss = 0.0019579045474529266
iteration 5, loss = 0.002166479593142867
iteration 6, loss = 0.0020660217851400375
iteration 7, loss = 0.0018457300029695034
iteration 8, loss = 0.0016867088852450252
iteration 9, loss = 0.001631657825782895
iteration 10, loss = 0.0024474849924445152
iteration 11, loss = 0.0015709451399743557
iteration 12, loss = 0.0015110501553863287
iteration 13, loss = 0.0036812906619161367
iteration 14, loss = 0.0018022474832832813
iteration 15, loss = 0.001869612606242299
iteration 16, loss = 0.0018970039673149586
iteration 17, loss = 0.0021711611188948154
iteration 18, loss = 0.002337046666070819
iteration 19, loss = 0.0023261490277945995
iteration 20, loss = 0.001841012854129076
iteration 21, loss = 0.001912841573357582
iteration 22, loss = 0.0018660330679267645
iteration 23, loss = 0.0017522444250062108
iteration 24, loss = 0.001716093858703971
iteration 25, loss = 0.0031497906893491745
iteration 26, loss = 0.00346077186986804
iteration 27, loss = 0.001456448808312416
iteration 28, loss = 0.0017787351971492171
iteration 29, loss = 0.0018353101331740618
iteration 30, loss = 0.0029309538658708334
iteration 31, loss = 0.0019533729646354914
iteration 32, loss = 0.002372350310906768
iteration 33, loss = 0.0016259778058156371
iteration 34, loss = 0.002146395156159997
iteration 35, loss = 0.0018620078917592764
iteration 36, loss = 0.0019711481872946024
iteration 37, loss = 0.002027584705501795
iteration 38, loss = 0.002077643061056733
iteration 39, loss = 0.003326085861772299
iteration 40, loss = 0.0019294327357783914
iteration 41, loss = 0.0041460637003183365
iteration 42, loss = 0.0022926332894712687
iteration 43, loss = 0.0023499338421970606
iteration 44, loss = 0.002201399067416787
iteration 45, loss = 0.0018243668600916862
iteration 46, loss = 0.001708939322270453
iteration 47, loss = 0.003968012519180775
iteration 48, loss = 0.0036831535398960114
iteration 49, loss = 0.0018226042157039046
iteration 50, loss = 0.0018423383589833975
iteration 51, loss = 0.001876938622444868
iteration 52, loss = 0.0018468424677848816
iteration 53, loss = 0.002026960253715515
iteration 54, loss = 0.0022860271856188774
iteration 55, loss = 0.003780744969844818
iteration 56, loss = 0.0027589835226535797
iteration 57, loss = 0.0016128679271787405
iteration 58, loss = 0.001858150353655219
iteration 59, loss = 0.0016433672280982137
iteration 60, loss = 0.0017141303978860378
iteration 61, loss = 0.0017677312716841698
iteration 62, loss = 0.0017428019782528281
iteration 63, loss = 0.00226248474791646
iteration 64, loss = 0.002150610089302063
iteration 65, loss = 0.002994545502588153
iteration 66, loss = 0.0019876668229699135
iteration 67, loss = 0.002132078632712364
iteration 68, loss = 0.0028092486318200827
iteration 69, loss = 0.003327365266159177
iteration 70, loss = 0.001977235311642289
iteration 71, loss = 0.002583698835223913
iteration 72, loss = 0.0018918553832918406
iteration 73, loss = 0.0023022075183689594
iteration 74, loss = 0.00207146187312901
iteration 75, loss = 0.0052972212433815
iteration 76, loss = 0.0023004859685897827
iteration 77, loss = 0.0020041523966938257
iteration 78, loss = 0.0021522287279367447
iteration 79, loss = 0.0017047077417373657
iteration 80, loss = 0.0020953374914824963
iteration 81, loss = 0.002692424925044179
iteration 82, loss = 0.0019821927417069674
iteration 83, loss = 0.002482352312654257
iteration 84, loss = 0.0018823330756276846
iteration 85, loss = 0.0019805384799838066
iteration 86, loss = 0.0019830958917737007
iteration 87, loss = 0.0029684901237487793
iteration 88, loss = 0.0023013793397694826
iteration 89, loss = 0.002833873964846134
iteration 90, loss = 0.0020799676422029734
iteration 91, loss = 0.0036730896681547165
iteration 92, loss = 0.0037653306499123573
iteration 93, loss = 0.003559782635420561
iteration 94, loss = 0.002398395910859108
iteration 95, loss = 0.0017962435958907008
iteration 96, loss = 0.0033307792618870735
iteration 97, loss = 0.001695559942163527
iteration 98, loss = 0.001876210910268128
iteration 99, loss = 0.0014698192244395614
iteration 100, loss = 0.004477584734559059
iteration 101, loss = 0.002222407143563032
iteration 102, loss = 0.0024500067811459303
iteration 103, loss = 0.001954165752977133
iteration 104, loss = 0.0026929685845971107
iteration 105, loss = 0.0018290560692548752
iteration 106, loss = 0.002296013757586479
iteration 107, loss = 0.0022267901804298162
iteration 108, loss = 0.0021081853192299604
iteration 109, loss = 0.0017988476902246475
iteration 110, loss = 0.0031879679299890995
iteration 111, loss = 0.002321458887308836
iteration 112, loss = 0.0017419334035366774
iteration 113, loss = 0.0020128365140408278
iteration 114, loss = 0.0016850391402840614
iteration 115, loss = 0.002974086906760931
iteration 116, loss = 0.004196153488010168
iteration 117, loss = 0.0021409071050584316
iteration 118, loss = 0.001939588226377964
iteration 119, loss = 0.0018318897346034646
iteration 120, loss = 0.001825496437959373
iteration 121, loss = 0.0018806073348969221
iteration 122, loss = 0.0019120349315926433
iteration 123, loss = 0.0017984944861382246
iteration 124, loss = 0.0023018463980406523
iteration 125, loss = 0.0036547568161040545
iteration 126, loss = 0.0019328033085912466
iteration 127, loss = 0.0017142968717962503
iteration 128, loss = 0.0019436292350292206
iteration 129, loss = 0.002192447194829583
iteration 130, loss = 0.0016983631066977978
iteration 131, loss = 0.0023685016203671694
iteration 132, loss = 0.0019713544752448797
iteration 133, loss = 0.0017220202134922147
iteration 134, loss = 0.0016875930596143007
iteration 135, loss = 0.0017799974884837866
iteration 136, loss = 0.002645671833306551
iteration 137, loss = 0.002399334916844964
iteration 138, loss = 0.001698140986263752
iteration 139, loss = 0.002613620599731803
iteration 140, loss = 0.001861143158748746
iteration 141, loss = 0.0022862451151013374
iteration 142, loss = 0.0021038849372416735
iteration 143, loss = 0.002005642279982567
iteration 144, loss = 0.0016255266964435577
iteration 145, loss = 0.002000476699322462
iteration 146, loss = 0.003585196565836668
iteration 147, loss = 0.0018114799167960882
iteration 148, loss = 0.003028088016435504
iteration 149, loss = 0.0015989895910024643
iteration 150, loss = 0.0019003736088052392
iteration 151, loss = 0.001812133239582181
iteration 152, loss = 0.002910630078986287
iteration 153, loss = 0.002490972401574254
iteration 154, loss = 0.00206152000464499
iteration 155, loss = 0.003783685155212879
iteration 156, loss = 0.0017384784296154976
iteration 157, loss = 0.0018703803652897477
iteration 158, loss = 0.002089406130835414
iteration 159, loss = 0.0018824903527274728
iteration 160, loss = 0.0029072246979922056
iteration 161, loss = 0.001889397157356143
iteration 162, loss = 0.0027796823997050524
iteration 163, loss = 0.0024234456941485405
iteration 164, loss = 0.0025485223159193993
iteration 165, loss = 0.0015344860730692744
iteration 166, loss = 0.0021432056091725826
iteration 167, loss = 0.001976639498025179
iteration 168, loss = 0.0016221797559410334
iteration 169, loss = 0.001947562675923109
iteration 170, loss = 0.001774830394424498
iteration 171, loss = 0.0018593971617519855
iteration 172, loss = 0.0024475594982504845
iteration 173, loss = 0.0019158514915034175
iteration 174, loss = 0.004210683982819319
iteration 175, loss = 0.0018431306816637516
iteration 176, loss = 0.0030401439871639013
iteration 177, loss = 0.003114226507022977
iteration 178, loss = 0.002115634735673666
iteration 179, loss = 0.0033092719968408346
iteration 180, loss = 0.0018441238207742572
iteration 181, loss = 0.003546441439539194
iteration 182, loss = 0.00216953968629241
iteration 183, loss = 0.0017219451256096363
iteration 184, loss = 0.0021932118106633425
iteration 185, loss = 0.0026890842709690332
iteration 186, loss = 0.002511017257347703
iteration 187, loss = 0.0019173752516508102
iteration 188, loss = 0.002126422245055437
iteration 189, loss = 0.0018307435093447566
iteration 190, loss = 0.0020355628803372383
iteration 191, loss = 0.0019900088664144278
iteration 192, loss = 0.002172395121306181
iteration 193, loss = 0.0020506333094090223
iteration 194, loss = 0.001624898985028267
iteration 195, loss = 0.002419030759483576
iteration 196, loss = 0.0018753174226731062
iteration 197, loss = 0.002046058652922511
iteration 198, loss = 0.0019500053022056818
iteration 199, loss = 0.0016931734280660748
iteration 200, loss = 0.0019143649842590094
iteration 201, loss = 0.001766957575455308
iteration 202, loss = 0.0019305101595818996
iteration 203, loss = 0.0028690542094409466
iteration 204, loss = 0.0028546059038490057
iteration 205, loss = 0.002929198555648327
iteration 206, loss = 0.003277938114479184
iteration 207, loss = 0.0018811795162037015
iteration 208, loss = 0.001448154798708856
iteration 209, loss = 0.0023946999572217464
iteration 210, loss = 0.0018422679277136922
iteration 211, loss = 0.001583754550665617
iteration 212, loss = 0.0017567058093845844
iteration 213, loss = 0.0019356128759682178
iteration 214, loss = 0.0020703424233943224
iteration 215, loss = 0.0021931708324700594
iteration 216, loss = 0.002154463203623891
iteration 217, loss = 0.002046616282314062
iteration 218, loss = 0.0018216969911009073
iteration 219, loss = 0.004277730826288462
iteration 220, loss = 0.0017919354140758514
iteration 221, loss = 0.0023867380805313587
iteration 222, loss = 0.003397963009774685
iteration 223, loss = 0.0021639298647642136
iteration 224, loss = 0.0019534756429493427
iteration 225, loss = 0.0021343976259231567
iteration 226, loss = 0.0020601910073310137
iteration 227, loss = 0.0021162645425647497
iteration 228, loss = 0.0030409670434892178
iteration 229, loss = 0.0018962501781061292
iteration 230, loss = 0.0026462972164154053
iteration 231, loss = 0.002361882012337446
iteration 232, loss = 0.0026324186474084854
iteration 233, loss = 0.002209515543654561
iteration 234, loss = 0.004141373094171286
iteration 235, loss = 0.002269336488097906
iteration 236, loss = 0.0015488138888031244
iteration 237, loss = 0.0017337914323434234
iteration 238, loss = 0.0018198153702542186
iteration 239, loss = 0.002178214956074953
iteration 240, loss = 0.002054053358733654
iteration 241, loss = 0.001973511418327689
iteration 242, loss = 0.0016083416994661093
iteration 243, loss = 0.0024847062304615974
iteration 244, loss = 0.0027153147384524345
iteration 245, loss = 0.00202262494713068
iteration 246, loss = 0.0017413634341210127
iteration 247, loss = 0.002289238851517439
iteration 248, loss = 0.0018514592666178942
iteration 249, loss = 0.0019120874349027872
iteration 250, loss = 0.002034345641732216
iteration 251, loss = 0.0022196273785084486
iteration 252, loss = 0.0025896949227899313
iteration 253, loss = 0.0020763452630490065
iteration 254, loss = 0.002080574631690979
iteration 255, loss = 0.0018034586682915688
iteration 256, loss = 0.002076550154015422
iteration 257, loss = 0.0021975087001919746
iteration 258, loss = 0.0025328428018838167
iteration 259, loss = 0.0021748878061771393
iteration 260, loss = 0.0029940425883978605
iteration 261, loss = 0.0025654227938503027
iteration 262, loss = 0.0019501358037814498
iteration 263, loss = 0.0018178343307226896
iteration 264, loss = 0.0020375773310661316
iteration 265, loss = 0.003292615059763193
iteration 266, loss = 0.002055044751614332
iteration 267, loss = 0.0021828541066497564
iteration 268, loss = 0.00218210625462234
iteration 269, loss = 0.002794977044686675
iteration 270, loss = 0.002244164003059268
iteration 271, loss = 0.002822152804583311
iteration 272, loss = 0.0017557606333866715
iteration 273, loss = 0.0017443845281377435
iteration 274, loss = 0.003081649076193571
iteration 275, loss = 0.0016944209346547723
iteration 276, loss = 0.003272427711635828
iteration 277, loss = 0.0020047826692461967
iteration 278, loss = 0.0019940068013966084
iteration 279, loss = 0.002162161050364375
iteration 280, loss = 0.0017191132064908743
iteration 281, loss = 0.002561375731602311
iteration 282, loss = 0.0018366972217336297
iteration 283, loss = 0.002189086517319083
iteration 284, loss = 0.002362407511100173
iteration 285, loss = 0.001954541075974703
iteration 286, loss = 0.0018154792487621307
iteration 287, loss = 0.0016910668928176165
iteration 288, loss = 0.002289828844368458
iteration 289, loss = 0.0019156853668391705
iteration 290, loss = 0.0020917945075780153
iteration 291, loss = 0.002892828779295087
iteration 292, loss = 0.002249613171443343
iteration 293, loss = 0.0027125284541398287
iteration 294, loss = 0.001896221423521638
iteration 295, loss = 0.001921211020089686
iteration 296, loss = 0.0016923155635595322
iteration 297, loss = 0.002391896676272154
iteration 298, loss = 0.0016046584350988269
iteration 299, loss = 0.0021123497281223536
iteration 300, loss = 0.0035909770522266626
iteration 1, loss = 0.001844249782152474
iteration 2, loss = 0.0020327549427747726
iteration 3, loss = 0.0019281830172985792
iteration 4, loss = 0.0022535156458616257
iteration 5, loss = 0.0018419770058244467
iteration 6, loss = 0.0018068952485918999
iteration 7, loss = 0.0016050609992817044
iteration 8, loss = 0.0013677848037332296
iteration 9, loss = 0.0018113978439942002
iteration 10, loss = 0.0022470562253147364
iteration 11, loss = 0.0019612228497862816
iteration 12, loss = 0.0016823066398501396
iteration 13, loss = 0.0020180412102490664
iteration 14, loss = 0.0020873236935585737
iteration 15, loss = 0.0018838171381503344
iteration 16, loss = 0.0021009540650993586
iteration 17, loss = 0.003277731826528907
iteration 18, loss = 0.0018369404133409262
iteration 19, loss = 0.0019711649511009455
iteration 20, loss = 0.0019200644455850124
iteration 21, loss = 0.0021306301932781935
iteration 22, loss = 0.002133132889866829
iteration 23, loss = 0.0021338858641684055
iteration 24, loss = 0.001991602825000882
iteration 25, loss = 0.0024235581513494253
iteration 26, loss = 0.0037420650478452444
iteration 27, loss = 0.0023383204825222492
iteration 28, loss = 0.0039135473780334
iteration 29, loss = 0.0016660051187500358
iteration 30, loss = 0.0018657410982996225
iteration 31, loss = 0.002294550184160471
iteration 32, loss = 0.002323505934327841
iteration 33, loss = 0.0015676618786528707
iteration 34, loss = 0.002133213449269533
iteration 35, loss = 0.0017051993636414409
iteration 36, loss = 0.002066659042611718
iteration 37, loss = 0.003077910514548421
iteration 38, loss = 0.002356669632717967
iteration 39, loss = 0.002644093008711934
iteration 40, loss = 0.0016622301191091537
iteration 41, loss = 0.001702125184237957
iteration 42, loss = 0.0020209522917866707
iteration 43, loss = 0.0036426519509404898
iteration 44, loss = 0.0023817881010472775
iteration 45, loss = 0.0021547621581703424
iteration 46, loss = 0.0014381026849150658
iteration 47, loss = 0.0022918968461453915
iteration 48, loss = 0.0018933236133307219
iteration 49, loss = 0.003730595577508211
iteration 50, loss = 0.0017516773659735918
iteration 51, loss = 0.0019492771243676543
iteration 52, loss = 0.002666403539478779
iteration 53, loss = 0.002431030385196209
iteration 54, loss = 0.0021111101377755404
iteration 55, loss = 0.0020306590013206005
iteration 56, loss = 0.001987376483157277
iteration 57, loss = 0.002143922494724393
iteration 58, loss = 0.001961308531463146
iteration 59, loss = 0.002530712401494384
iteration 60, loss = 0.0024783050175756216
iteration 61, loss = 0.002042211592197418
iteration 62, loss = 0.0036107124760746956
iteration 63, loss = 0.002770914463326335
iteration 64, loss = 0.001916074426844716
iteration 65, loss = 0.0035222028382122517
iteration 66, loss = 0.002799379639327526
iteration 67, loss = 0.004445651080459356
iteration 68, loss = 0.002611993346363306
iteration 69, loss = 0.00205968227237463
iteration 70, loss = 0.0019560465589165688
iteration 71, loss = 0.0015940666198730469
iteration 72, loss = 0.0019308410119265318
iteration 73, loss = 0.0020345645025372505
iteration 74, loss = 0.0021246583200991154
iteration 75, loss = 0.0034656173083931208
iteration 76, loss = 0.001783638959750533
iteration 77, loss = 0.0021379459649324417
iteration 78, loss = 0.0015637277392670512
iteration 79, loss = 0.00317862955853343
iteration 80, loss = 0.0019234309438616037
iteration 81, loss = 0.0031101438216865063
iteration 82, loss = 0.0020556256640702486
iteration 83, loss = 0.0021571158431470394
iteration 84, loss = 0.0027748511638492346
iteration 85, loss = 0.002147478051483631
iteration 86, loss = 0.0019842388574033976
iteration 87, loss = 0.0018710617441684008
iteration 88, loss = 0.00196481472812593
iteration 89, loss = 0.002881087362766266
iteration 90, loss = 0.001636567641980946
iteration 91, loss = 0.0020826910622417927
iteration 92, loss = 0.0018613929860293865
iteration 93, loss = 0.0019105426035821438
iteration 94, loss = 0.0028205111157149076
iteration 95, loss = 0.0033722389489412308
iteration 96, loss = 0.00188656453974545
iteration 97, loss = 0.0020477045327425003
iteration 98, loss = 0.002022909000515938
iteration 99, loss = 0.002006611553952098
iteration 100, loss = 0.0020547094754874706
iteration 101, loss = 0.0016897008754312992
iteration 102, loss = 0.002003362635150552
iteration 103, loss = 0.0016367369098588824
iteration 104, loss = 0.002145227510482073
iteration 105, loss = 0.0017701243050396442
iteration 106, loss = 0.002352359239012003
iteration 107, loss = 0.0021233384031802416
iteration 108, loss = 0.0016239952528849244
iteration 109, loss = 0.003243015380576253
iteration 110, loss = 0.0028728381730616093
iteration 111, loss = 0.0017853560857474804
iteration 112, loss = 0.0030169414822012186
iteration 113, loss = 0.001901398878544569
iteration 114, loss = 0.0034470302052795887
iteration 115, loss = 0.0018503557657822967
iteration 116, loss = 0.002710843924432993
iteration 117, loss = 0.002119984943419695
iteration 118, loss = 0.0017934013158082962
iteration 119, loss = 0.0017475681379437447
iteration 120, loss = 0.0020363221410661936
iteration 121, loss = 0.0019631623290479183
iteration 122, loss = 0.0034928047098219395
iteration 123, loss = 0.00254182331264019
iteration 124, loss = 0.0025184527039527893
iteration 125, loss = 0.0016962903318926692
iteration 126, loss = 0.002324059372767806
iteration 127, loss = 0.0017951162299141288
iteration 128, loss = 0.0017136408714577556
iteration 129, loss = 0.002970547415316105
iteration 130, loss = 0.0036415879148989916
iteration 131, loss = 0.00219323905184865
iteration 132, loss = 0.0020389126148074865
iteration 133, loss = 0.0021206282544881105
iteration 134, loss = 0.0023653507232666016
iteration 135, loss = 0.0021185921505093575
iteration 136, loss = 0.002294433070346713
iteration 137, loss = 0.0024545101914554834
iteration 138, loss = 0.0017867290880531073
iteration 139, loss = 0.002060546539723873
iteration 140, loss = 0.0015110218664631248
iteration 141, loss = 0.0021897549740970135
iteration 142, loss = 0.003608294762670994
iteration 143, loss = 0.0018643655348569155
iteration 144, loss = 0.0017387756379321218
iteration 145, loss = 0.0020594955421984196
iteration 146, loss = 0.0017527637537568808
iteration 147, loss = 0.0019557871855795383
iteration 148, loss = 0.0022971201688051224
iteration 149, loss = 0.0018529868684709072
iteration 150, loss = 0.0022273834329098463
iteration 151, loss = 0.0021446221508085728
iteration 152, loss = 0.0050138263031840324
iteration 153, loss = 0.0016620168462395668
iteration 154, loss = 0.0020022564567625523
iteration 155, loss = 0.002179747447371483
iteration 156, loss = 0.0022337702102959156
iteration 157, loss = 0.0024188931565731764
iteration 158, loss = 0.001865505357272923
iteration 159, loss = 0.00265756924636662
iteration 160, loss = 0.0017074455972760916
iteration 161, loss = 0.0019303663866594434
iteration 162, loss = 0.0032289032824337482
iteration 163, loss = 0.0035143764689564705
iteration 164, loss = 0.0016978132771328092
iteration 165, loss = 0.0020660534501075745
iteration 166, loss = 0.001613421132788062
iteration 167, loss = 0.002190246479585767
iteration 168, loss = 0.002498571528121829
iteration 169, loss = 0.0016799685545265675
iteration 170, loss = 0.003659001551568508
iteration 171, loss = 0.0019013590645045042
iteration 172, loss = 0.002182094380259514
iteration 173, loss = 0.0017974950606003404
iteration 174, loss = 0.002104901010170579
iteration 175, loss = 0.0016811444656923413
iteration 176, loss = 0.0019313935190439224
iteration 177, loss = 0.001933421939611435
iteration 178, loss = 0.0027073498349636793
iteration 179, loss = 0.0017383055528625846
iteration 180, loss = 0.0022168559953570366
iteration 181, loss = 0.0031727678142488003
iteration 182, loss = 0.0017880982486531138
iteration 183, loss = 0.0018164667999371886
iteration 184, loss = 0.0018670042045414448
iteration 185, loss = 0.0017410279251635075
iteration 186, loss = 0.002990110544487834
iteration 187, loss = 0.0017339375335723162
iteration 188, loss = 0.003722485853359103
iteration 189, loss = 0.0020425491966307163
iteration 190, loss = 0.0025540972128510475
iteration 191, loss = 0.0019695316441357136
iteration 192, loss = 0.0020936550572514534
iteration 193, loss = 0.002661218401044607
iteration 194, loss = 0.0022317911498248577
iteration 195, loss = 0.001988989068195224
iteration 196, loss = 0.0025553458835929632
iteration 197, loss = 0.002412587171420455
iteration 198, loss = 0.002413784386590123
iteration 199, loss = 0.0018780432874336839
iteration 200, loss = 0.0018375145737081766
iteration 201, loss = 0.0021294245962053537
iteration 202, loss = 0.0020348632242530584
iteration 203, loss = 0.0018842119025066495
iteration 204, loss = 0.0019020938780158758
iteration 205, loss = 0.0019515566527843475
iteration 206, loss = 0.0021988924127072096
iteration 207, loss = 0.0027395812794566154
iteration 208, loss = 0.0025837498251348734
iteration 209, loss = 0.0018786658765748143
iteration 210, loss = 0.0028069415129721165
iteration 211, loss = 0.0022978507913649082
iteration 212, loss = 0.002365425694733858
iteration 213, loss = 0.0017481878167018294
iteration 214, loss = 0.0025223528500646353
iteration 215, loss = 0.0029430408030748367
iteration 216, loss = 0.0027573031838983297
iteration 217, loss = 0.0017363205552101135
iteration 218, loss = 0.0019596898928284645
iteration 219, loss = 0.001800833153538406
iteration 220, loss = 0.002308132592588663
iteration 221, loss = 0.002386576496064663
iteration 222, loss = 0.0022051804699003696
iteration 223, loss = 0.0017911121249198914
iteration 224, loss = 0.0018687763949856162
iteration 225, loss = 0.0018699910724535584
iteration 226, loss = 0.002163431141525507
iteration 227, loss = 0.0018870191415771842
iteration 228, loss = 0.0014401768567040563
iteration 229, loss = 0.003373493440449238
iteration 230, loss = 0.001662346301600337
iteration 231, loss = 0.002161554992198944
iteration 232, loss = 0.004029677715152502
iteration 233, loss = 0.003081562463194132
iteration 234, loss = 0.0016358653083443642
iteration 235, loss = 0.0017848885618150234
iteration 236, loss = 0.0016293644439429045
iteration 237, loss = 0.0020511727780103683
iteration 238, loss = 0.002692056354135275
iteration 239, loss = 0.00202962476760149
iteration 240, loss = 0.0024298387579619884
iteration 241, loss = 0.0018638849724084139
iteration 242, loss = 0.002620517974719405
iteration 243, loss = 0.004320635460317135
iteration 244, loss = 0.0018849654588848352
iteration 245, loss = 0.0027413438074290752
iteration 246, loss = 0.002683741506189108
iteration 247, loss = 0.00203001219779253
iteration 248, loss = 0.0023710234090685844
iteration 249, loss = 0.0018174302531406283
iteration 250, loss = 0.002054222859442234
iteration 251, loss = 0.0018597960006445646
iteration 252, loss = 0.0018749884329736233
iteration 253, loss = 0.0018954253755509853
iteration 254, loss = 0.0015529743395745754
iteration 255, loss = 0.0030743356328457594
iteration 256, loss = 0.002181730233132839
iteration 257, loss = 0.0019069085828959942
iteration 258, loss = 0.0022752732038497925
iteration 259, loss = 0.001535638701170683
iteration 260, loss = 0.001949604949913919
iteration 261, loss = 0.0019225325668230653
iteration 262, loss = 0.0019852169789373875
iteration 263, loss = 0.0021345769055187702
iteration 264, loss = 0.0015241927467286587
iteration 265, loss = 0.0020654990803450346
iteration 266, loss = 0.0020263558253645897
iteration 267, loss = 0.0023215189576148987
iteration 268, loss = 0.0016117752529680729
iteration 269, loss = 0.0017360469792038202
iteration 270, loss = 0.0021507693454623222
iteration 271, loss = 0.002089922083541751
iteration 272, loss = 0.0023313541896641254
iteration 273, loss = 0.003287722822278738
iteration 274, loss = 0.002248692326247692
iteration 275, loss = 0.002421300858259201
iteration 276, loss = 0.004446611274033785
iteration 277, loss = 0.0026262381579726934
iteration 278, loss = 0.002473114989697933
iteration 279, loss = 0.0015845795860514045
iteration 280, loss = 0.002054966986179352
iteration 281, loss = 0.0023718494921922684
iteration 282, loss = 0.0018203654326498508
iteration 283, loss = 0.0030957143753767014
iteration 284, loss = 0.003423754358664155
iteration 285, loss = 0.0025083546061068773
iteration 286, loss = 0.002304916735738516
iteration 287, loss = 0.0017084564315155149
iteration 288, loss = 0.0026396119501441717
iteration 289, loss = 0.0035950597375631332
iteration 290, loss = 0.0017879025544971228
iteration 291, loss = 0.0020084597636014223
iteration 292, loss = 0.001968944910913706
iteration 293, loss = 0.0021816606167703867
iteration 294, loss = 0.001832401379942894
iteration 295, loss = 0.003106906311586499
iteration 296, loss = 0.0017185156466439366
iteration 297, loss = 0.002237641718238592
iteration 298, loss = 0.0021380651742219925
iteration 299, loss = 0.0025528159458190203
iteration 300, loss = 0.0037618461064994335
iteration 1, loss = 0.001896279864013195
iteration 2, loss = 0.0019174781627953053
iteration 3, loss = 0.0016192215261980891
iteration 4, loss = 0.0024255546741187572
iteration 5, loss = 0.0017207340570166707
iteration 6, loss = 0.002156683010980487
iteration 7, loss = 0.0023423433303833008
iteration 8, loss = 0.001879196148365736
iteration 9, loss = 0.0020308103412389755
iteration 10, loss = 0.001815193798393011
iteration 11, loss = 0.0018537158612161875
iteration 12, loss = 0.0032547670416533947
iteration 13, loss = 0.002341517712920904
iteration 14, loss = 0.002112459624186158
iteration 15, loss = 0.0021046986803412437
iteration 16, loss = 0.0017981536220759153
iteration 17, loss = 0.0023220551665872335
iteration 18, loss = 0.0025165656115859747
iteration 19, loss = 0.0019711137283593416
iteration 20, loss = 0.003832403337582946
iteration 21, loss = 0.0020345598459243774
iteration 22, loss = 0.0026339171454310417
iteration 23, loss = 0.003721640445291996
iteration 24, loss = 0.002057485980913043
iteration 25, loss = 0.003851504996418953
iteration 26, loss = 0.002017076127231121
iteration 27, loss = 0.001953182276338339
iteration 28, loss = 0.003246086183935404
iteration 29, loss = 0.0028858163859695196
iteration 30, loss = 0.001988522708415985
iteration 31, loss = 0.001768371439538896
iteration 32, loss = 0.003293234622105956
iteration 33, loss = 0.001756993937306106
iteration 34, loss = 0.002174962544813752
iteration 35, loss = 0.0021621156483888626
iteration 36, loss = 0.0016824753256514668
iteration 37, loss = 0.001691915444098413
iteration 38, loss = 0.0019873057026416063
iteration 39, loss = 0.003422780428081751
iteration 40, loss = 0.003367641707882285
iteration 41, loss = 0.0023479415103793144
iteration 42, loss = 0.002351989271119237
iteration 43, loss = 0.0017359210178256035
iteration 44, loss = 0.0019397265277802944
iteration 45, loss = 0.0017321466002613306
iteration 46, loss = 0.0015881434082984924
iteration 47, loss = 0.0020653654355555773
iteration 48, loss = 0.002008914016187191
iteration 49, loss = 0.001738724415190518
iteration 50, loss = 0.0022493083961308002
iteration 51, loss = 0.004677387420088053
iteration 52, loss = 0.002003998029977083
iteration 53, loss = 0.0020945961587131023
iteration 54, loss = 0.0018307259306311607
iteration 55, loss = 0.0017972912173718214
iteration 56, loss = 0.0018792981281876564
iteration 57, loss = 0.001752602169290185
iteration 58, loss = 0.003818060038611293
iteration 59, loss = 0.002525423187762499
iteration 60, loss = 0.0017730076797306538
iteration 61, loss = 0.0027153112459927797
iteration 62, loss = 0.0018347010482102633
iteration 63, loss = 0.0032486964482814074
iteration 64, loss = 0.001713631092570722
iteration 65, loss = 0.004203884396702051
iteration 66, loss = 0.001848773448728025
iteration 67, loss = 0.0017413494642823935
iteration 68, loss = 0.002335107885301113
iteration 69, loss = 0.002510384190827608
iteration 70, loss = 0.0019213099731132388
iteration 71, loss = 0.0019574258476495743
iteration 72, loss = 0.0018858399707823992
iteration 73, loss = 0.0022311864886432886
iteration 74, loss = 0.0020526819862425327
iteration 75, loss = 0.002256442792713642
iteration 76, loss = 0.00208755093626678
iteration 77, loss = 0.0017643044702708721
iteration 78, loss = 0.001980850240215659
iteration 79, loss = 0.002249373123049736
iteration 80, loss = 0.0038713121321052313
iteration 81, loss = 0.0019045050721615553
iteration 82, loss = 0.0016747430199757218
iteration 83, loss = 0.001889249193482101
iteration 84, loss = 0.0020364474039524794
iteration 85, loss = 0.001965017057955265
iteration 86, loss = 0.0025540683418512344
iteration 87, loss = 0.004616125952452421
iteration 88, loss = 0.0027571392711251974
iteration 89, loss = 0.0021940215956419706
iteration 90, loss = 0.0020240964367985725
iteration 91, loss = 0.0024825327564030886
iteration 92, loss = 0.002001088811084628
iteration 93, loss = 0.0014449979644268751
iteration 94, loss = 0.002031434327363968
iteration 95, loss = 0.0021209781989455223
iteration 96, loss = 0.0017408068524673581
iteration 97, loss = 0.0018146024085581303
iteration 98, loss = 0.0028830349911004305
iteration 99, loss = 0.0019691763445734978
iteration 100, loss = 0.001752796582877636
iteration 101, loss = 0.0022180459927767515
iteration 102, loss = 0.0021693380549550056
iteration 103, loss = 0.0017001362284645438
iteration 104, loss = 0.0025638111401349306
iteration 105, loss = 0.0020273595582693815
iteration 106, loss = 0.0018794548232108355
iteration 107, loss = 0.002122278558090329
iteration 108, loss = 0.0016567076090723276
iteration 109, loss = 0.002177588641643524
iteration 110, loss = 0.002015448175370693
iteration 111, loss = 0.002046286826953292
iteration 112, loss = 0.0015605817316100001
iteration 113, loss = 0.002408668864518404
iteration 114, loss = 0.002386466134339571
iteration 115, loss = 0.002200143411755562
iteration 116, loss = 0.0021793206688016653
iteration 117, loss = 0.0017677654977887869
iteration 118, loss = 0.0018805705476552248
iteration 119, loss = 0.001817610114812851
iteration 120, loss = 0.002674632938578725
iteration 121, loss = 0.0018321750685572624
iteration 122, loss = 0.002454446628689766
iteration 123, loss = 0.002794713946059346
iteration 124, loss = 0.0016969448188319802
iteration 125, loss = 0.0016478694742545485
iteration 126, loss = 0.0017447683494538069
iteration 127, loss = 0.0021091613452881575
iteration 128, loss = 0.002390617039054632
iteration 129, loss = 0.0019691369961947203
iteration 130, loss = 0.002025393769145012
iteration 131, loss = 0.0021011708304286003
iteration 132, loss = 0.0028908541426062584
iteration 133, loss = 0.0015095383860170841
iteration 134, loss = 0.0018002172000706196
iteration 135, loss = 0.0018990696407854557
iteration 136, loss = 0.002621452324092388
iteration 137, loss = 0.0018342684488743544
iteration 138, loss = 0.001862620934844017
iteration 139, loss = 0.0017027599969878793
iteration 140, loss = 0.0018593970453366637
iteration 141, loss = 0.0020875786431133747
iteration 142, loss = 0.0017834611935541034
iteration 143, loss = 0.001916163950227201
iteration 144, loss = 0.004296004772186279
iteration 145, loss = 0.0017149840714409947
iteration 146, loss = 0.0026127519086003304
iteration 147, loss = 0.001969944452866912
iteration 148, loss = 0.0017090251203626394
iteration 149, loss = 0.0018689234275370836
iteration 150, loss = 0.0016609925078228116
iteration 151, loss = 0.002226318698376417
iteration 152, loss = 0.003001926001161337
iteration 153, loss = 0.0018982759211212397
iteration 154, loss = 0.0020427261479198933
iteration 155, loss = 0.0026653765235096216
iteration 156, loss = 0.0016844617202877998
iteration 157, loss = 0.0025213356129825115
iteration 158, loss = 0.003270277753472328
iteration 159, loss = 0.0016973280580714345
iteration 160, loss = 0.0018457506084814668
iteration 161, loss = 0.004072352312505245
iteration 162, loss = 0.001951996935531497
iteration 163, loss = 0.002633846364915371
iteration 164, loss = 0.0018320087110623717
iteration 165, loss = 0.002627738518640399
iteration 166, loss = 0.0018530806992202997
iteration 167, loss = 0.0020253495313227177
iteration 168, loss = 0.001760243671014905
iteration 169, loss = 0.004400517325848341
iteration 170, loss = 0.0023608854971826077
iteration 171, loss = 0.001979571534320712
iteration 172, loss = 0.0017867179121822119
iteration 173, loss = 0.003943528980016708
iteration 174, loss = 0.002683700993657112
iteration 175, loss = 0.0020162812434136868
iteration 176, loss = 0.002963725011795759
iteration 177, loss = 0.002512127161026001
iteration 178, loss = 0.002571605611592531
iteration 179, loss = 0.002676840405911207
iteration 180, loss = 0.002260675188153982
iteration 181, loss = 0.00365485530346632
iteration 182, loss = 0.002580086002126336
iteration 183, loss = 0.0015585789224132895
iteration 184, loss = 0.002459610113874078
iteration 185, loss = 0.0019369532819837332
iteration 186, loss = 0.0028064812067896128
iteration 187, loss = 0.0027715752366930246
iteration 188, loss = 0.0033827226143330336
iteration 189, loss = 0.002083555096760392
iteration 190, loss = 0.0017308141104876995
iteration 191, loss = 0.0036167697980999947
iteration 192, loss = 0.004119583405554295
iteration 193, loss = 0.0024026427417993546
iteration 194, loss = 0.001762563013471663
iteration 195, loss = 0.0017879714723676443
iteration 196, loss = 0.0017905549611896276
iteration 197, loss = 0.0017394094029441476
iteration 198, loss = 0.0026779877953231335
iteration 199, loss = 0.0017643101746216416
iteration 200, loss = 0.0026934430934488773
iteration 201, loss = 0.0023909001611173153
iteration 202, loss = 0.0025660821702331305
iteration 203, loss = 0.003165045054629445
iteration 204, loss = 0.002002945402637124
iteration 205, loss = 0.001774011878296733
iteration 206, loss = 0.0021304909605532885
iteration 207, loss = 0.0016993586905300617
iteration 208, loss = 0.0018189914990216494
iteration 209, loss = 0.0017910150345414877
iteration 210, loss = 0.0018967688083648682
iteration 211, loss = 0.002145705046132207
iteration 212, loss = 0.0018601578194648027
iteration 213, loss = 0.00176746875513345
iteration 214, loss = 0.0021029827184975147
iteration 215, loss = 0.00269867992028594
iteration 216, loss = 0.0026294244453310966
iteration 217, loss = 0.003552307840436697
iteration 218, loss = 0.0020269148517400026
iteration 219, loss = 0.0020042110700160265
iteration 220, loss = 0.0019548176787793636
iteration 221, loss = 0.0027746036648750305
iteration 222, loss = 0.002383000683039427
iteration 223, loss = 0.0038726883940398693
iteration 224, loss = 0.002539682434871793
iteration 225, loss = 0.0018406105227768421
iteration 226, loss = 0.0021470265928655863
iteration 227, loss = 0.0029211780056357384
iteration 228, loss = 0.0016907573444768786
iteration 229, loss = 0.0020440348889678717
iteration 230, loss = 0.0024059233255684376
iteration 231, loss = 0.0020237406715750694
iteration 232, loss = 0.0017019713995978236
iteration 233, loss = 0.001741511281579733
iteration 234, loss = 0.0017712665721774101
iteration 235, loss = 0.0017948838649317622
iteration 236, loss = 0.00188915163744241
iteration 237, loss = 0.0016879080794751644
iteration 238, loss = 0.00232302094809711
iteration 239, loss = 0.002369724214076996
iteration 240, loss = 0.0017646154155954719
iteration 241, loss = 0.0018226240063086152
iteration 242, loss = 0.0018114196136593819
iteration 243, loss = 0.0019038640893995762
iteration 244, loss = 0.0017342986539006233
iteration 245, loss = 0.0038554833736270666
iteration 246, loss = 0.002260666573420167
iteration 247, loss = 0.002032181713730097
iteration 248, loss = 0.0031638597138226032
iteration 249, loss = 0.002231611404567957
iteration 250, loss = 0.002250111661851406
iteration 251, loss = 0.0020899996161460876
iteration 252, loss = 0.0017207787605002522
iteration 253, loss = 0.0033269443083554506
iteration 254, loss = 0.002340601058676839
iteration 255, loss = 0.001854624249972403
iteration 256, loss = 0.0016402325127273798
iteration 257, loss = 0.0019156482303515077
iteration 258, loss = 0.0030177433509379625
iteration 259, loss = 0.0019000432221218944
iteration 260, loss = 0.0017935532378032804
iteration 261, loss = 0.00193133600987494
iteration 262, loss = 0.001939810230396688
iteration 263, loss = 0.0018149124225601554
iteration 264, loss = 0.0018553778063505888
iteration 265, loss = 0.00203178683295846
iteration 266, loss = 0.002074547577649355
iteration 267, loss = 0.002018596976995468
iteration 268, loss = 0.0018038204871118069
iteration 269, loss = 0.0017280400497838855
iteration 270, loss = 0.0036338355857878923
iteration 271, loss = 0.0017639829311519861
iteration 272, loss = 0.00252093025483191
iteration 273, loss = 0.0020731650292873383
iteration 274, loss = 0.0018630879931151867
iteration 275, loss = 0.0037692321930080652
iteration 276, loss = 0.0037714247591793537
iteration 277, loss = 0.0022863016929477453
iteration 278, loss = 0.0020662513561546803
iteration 279, loss = 0.002181008458137512
iteration 280, loss = 0.002006184309720993
iteration 281, loss = 0.002016539452597499
iteration 282, loss = 0.002208549529314041
iteration 283, loss = 0.0023545902222394943
iteration 284, loss = 0.0021889156196266413
iteration 285, loss = 0.001983919180929661
iteration 286, loss = 0.0020290683023631573
iteration 287, loss = 0.0028702891431748867
iteration 288, loss = 0.0017757101450115442
iteration 289, loss = 0.0019797689747065306
iteration 290, loss = 0.002684152452275157
iteration 291, loss = 0.0022269294131547213
iteration 292, loss = 0.0019320256542414427
iteration 293, loss = 0.0018396571977064013
iteration 294, loss = 0.0018777695950120687
iteration 295, loss = 0.0023223399184644222
iteration 296, loss = 0.002650330774486065
iteration 297, loss = 0.0017732697306200862
iteration 298, loss = 0.0022129348944872618
iteration 299, loss = 0.0021731939632445574
iteration 300, loss = 0.0028082416392862797
iteration 1, loss = 0.001827920088544488
iteration 2, loss = 0.0026090294122695923
iteration 3, loss = 0.001855777227319777
iteration 4, loss = 0.001908357022330165
iteration 5, loss = 0.0018890381325036287
iteration 6, loss = 0.0019137337803840637
iteration 7, loss = 0.0034059814643114805
iteration 8, loss = 0.002646239474415779
iteration 9, loss = 0.0021518399007618427
iteration 10, loss = 0.0017056175274774432
iteration 11, loss = 0.0019112530862912536
iteration 12, loss = 0.0025255046784877777
iteration 13, loss = 0.0023249289952218533
iteration 14, loss = 0.001932986662723124
iteration 15, loss = 0.0025315494276583195
iteration 16, loss = 0.0016771570080891252
iteration 17, loss = 0.0021550124511122704
iteration 18, loss = 0.0018555035348981619
iteration 19, loss = 0.002447206759825349
iteration 20, loss = 0.0018476282712072134
iteration 21, loss = 0.0015947273932397366
iteration 22, loss = 0.001736205886118114
iteration 23, loss = 0.0023763433564454317
iteration 24, loss = 0.0020978841930627823
iteration 25, loss = 0.0024191325064748526
iteration 26, loss = 0.0019558160565793514
iteration 27, loss = 0.0025900318287312984
iteration 28, loss = 0.0017019330989569426
iteration 29, loss = 0.0018111231038346887
iteration 30, loss = 0.002479871269315481
iteration 31, loss = 0.0018238062039017677
iteration 32, loss = 0.0017677288269624114
iteration 33, loss = 0.0020897448994219303
iteration 34, loss = 0.0024420111440122128
iteration 35, loss = 0.0017689181258901954
iteration 36, loss = 0.001981571549549699
iteration 37, loss = 0.0020968345925211906
iteration 38, loss = 0.0016523144440725446
iteration 39, loss = 0.0018201391212642193
iteration 40, loss = 0.0018947162898257375
iteration 41, loss = 0.0023474949412047863
iteration 42, loss = 0.0019727693870663643
iteration 43, loss = 0.0021897009573876858
iteration 44, loss = 0.0017430413281545043
iteration 45, loss = 0.0018453883239999413
iteration 46, loss = 0.002166207879781723
iteration 47, loss = 0.0017388348933309317
iteration 48, loss = 0.001906835357658565
iteration 49, loss = 0.0028615056071430445
iteration 50, loss = 0.00174991087988019
iteration 51, loss = 0.001883840188384056
iteration 52, loss = 0.001792713301256299
iteration 53, loss = 0.0016417510341852903
iteration 54, loss = 0.0017763414653018117
iteration 55, loss = 0.0024297437630593777
iteration 56, loss = 0.001969339093193412
iteration 57, loss = 0.002063986612483859
iteration 58, loss = 0.0022896244190633297
iteration 59, loss = 0.0018760755192488432
iteration 60, loss = 0.0019102486548945308
iteration 61, loss = 0.003824563231319189
iteration 62, loss = 0.001887678517960012
iteration 63, loss = 0.003965491894632578
iteration 64, loss = 0.002258708467707038
iteration 65, loss = 0.003608534811064601
iteration 66, loss = 0.0018402280984446406
iteration 67, loss = 0.0023675940465182066
iteration 68, loss = 0.0018770935712382197
iteration 69, loss = 0.0023299765307456255
iteration 70, loss = 0.0020297577138990164
iteration 71, loss = 0.0018136374419555068
iteration 72, loss = 0.002537160413339734
iteration 73, loss = 0.004275249782949686
iteration 74, loss = 0.0025886688381433487
iteration 75, loss = 0.00202937051653862
iteration 76, loss = 0.0017466287827119231
iteration 77, loss = 0.0019943539518862963
iteration 78, loss = 0.001626215991564095
iteration 79, loss = 0.0026493044570088387
iteration 80, loss = 0.0020312757696956396
iteration 81, loss = 0.001800354104489088
iteration 82, loss = 0.002710280241444707
iteration 83, loss = 0.0018857454415410757
iteration 84, loss = 0.0023084632121026516
iteration 85, loss = 0.0016917446628212929
iteration 86, loss = 0.0018657408654689789
iteration 87, loss = 0.0018852580105885863
iteration 88, loss = 0.0019720003474503756
iteration 89, loss = 0.003066426143050194
iteration 90, loss = 0.0033325511030852795
iteration 91, loss = 0.001861645607277751
iteration 92, loss = 0.0023521820548921824
iteration 93, loss = 0.003672208869829774
iteration 94, loss = 0.00270099681802094
iteration 95, loss = 0.001904306118376553
iteration 96, loss = 0.0016242710407823324
iteration 97, loss = 0.002166351769119501
iteration 98, loss = 0.0017519649118185043
iteration 99, loss = 0.002047270303592086
iteration 100, loss = 0.003595408983528614
iteration 101, loss = 0.0017769566038623452
iteration 102, loss = 0.0019136033952236176
iteration 103, loss = 0.0019124060636386275
iteration 104, loss = 0.0042695458978414536
iteration 105, loss = 0.0022357054986059666
iteration 106, loss = 0.0017818906344473362
iteration 107, loss = 0.0022411642130464315
iteration 108, loss = 0.0018590849358588457
iteration 109, loss = 0.0019256258383393288
iteration 110, loss = 0.001775316777639091
iteration 111, loss = 0.0025146398693323135
iteration 112, loss = 0.0016334073152393103
iteration 113, loss = 0.002548515796661377
iteration 114, loss = 0.0019047438399866223
iteration 115, loss = 0.002706499071791768
iteration 116, loss = 0.0015584091888740659
iteration 117, loss = 0.001778848236426711
iteration 118, loss = 0.001818023039959371
iteration 119, loss = 0.002375601325184107
iteration 120, loss = 0.0019596831407397985
iteration 121, loss = 0.0020071303006261587
iteration 122, loss = 0.0019554903265088797
iteration 123, loss = 0.002293944824486971
iteration 124, loss = 0.0029319007880985737
iteration 125, loss = 0.0020971540361642838
iteration 126, loss = 0.002330599818378687
iteration 127, loss = 0.001634686952456832
iteration 128, loss = 0.0021546378266066313
iteration 129, loss = 0.0037521279882639647
iteration 130, loss = 0.0020120907574892044
iteration 131, loss = 0.0027184223290532827
iteration 132, loss = 0.0017668800428509712
iteration 133, loss = 0.0021716326009482145
iteration 134, loss = 0.002111664041876793
iteration 135, loss = 0.002172282664105296
iteration 136, loss = 0.0020090551115572453
iteration 137, loss = 0.002121973317116499
iteration 138, loss = 0.0017318936297670007
iteration 139, loss = 0.0032992875203490257
iteration 140, loss = 0.0017502777045592666
iteration 141, loss = 0.003379830392077565
iteration 142, loss = 0.0019284057198092341
iteration 143, loss = 0.001775866374373436
iteration 144, loss = 0.002249690005555749
iteration 145, loss = 0.003818854922428727
iteration 146, loss = 0.0022359807044267654
iteration 147, loss = 0.0020318643655627966
iteration 148, loss = 0.001972984056919813
iteration 149, loss = 0.0017532715573906898
iteration 150, loss = 0.0016983423847705126
iteration 151, loss = 0.0020080404356122017
iteration 152, loss = 0.0025879964232444763
iteration 153, loss = 0.003166025737300515
iteration 154, loss = 0.0033437549136579037
iteration 155, loss = 0.0018559047020971775
iteration 156, loss = 0.0015209439443424344
iteration 157, loss = 0.0017785689560696483
iteration 158, loss = 0.002029325347393751
iteration 159, loss = 0.002029742579907179
iteration 160, loss = 0.001865363446995616
iteration 161, loss = 0.0016676216619089246
iteration 162, loss = 0.001990274991840124
iteration 163, loss = 0.0017774953739717603
iteration 164, loss = 0.0020536165684461594
iteration 165, loss = 0.0019527045078575611
iteration 166, loss = 0.0029980046674609184
iteration 167, loss = 0.00164305348880589
iteration 168, loss = 0.0030221110209822655
iteration 169, loss = 0.0020521448459476233
iteration 170, loss = 0.002077505923807621
iteration 171, loss = 0.0018699405482038856
iteration 172, loss = 0.0018712908495217562
iteration 173, loss = 0.002049817703664303
iteration 174, loss = 0.0025902152992784977
iteration 175, loss = 0.001712112338282168
iteration 176, loss = 0.0016779580619186163
iteration 177, loss = 0.0019449811661615968
iteration 178, loss = 0.0017464614938944578
iteration 179, loss = 0.0019323161104694009
iteration 180, loss = 0.0020726039074361324
iteration 181, loss = 0.0017104822909459472
iteration 182, loss = 0.002645379165187478
iteration 183, loss = 0.001909152022562921
iteration 184, loss = 0.0019896039739251137
iteration 185, loss = 0.00266754487529397
iteration 186, loss = 0.002253730781376362
iteration 187, loss = 0.0029015326872467995
iteration 188, loss = 0.002022904111072421
iteration 189, loss = 0.00202529551461339
iteration 190, loss = 0.0019636736251413822
iteration 191, loss = 0.0020488700829446316
iteration 192, loss = 0.0029443532694131136
iteration 193, loss = 0.0027125116903334856
iteration 194, loss = 0.0037877534050494432
iteration 195, loss = 0.0023352308198809624
iteration 196, loss = 0.0017190594226121902
iteration 197, loss = 0.00183586694765836
iteration 198, loss = 0.0017356923781335354
iteration 199, loss = 0.0017646320629864931
iteration 200, loss = 0.002508037956431508
iteration 201, loss = 0.0024026876781135798
iteration 202, loss = 0.00262754806317389
iteration 203, loss = 0.0022304467856884003
iteration 204, loss = 0.0033834471832960844
iteration 205, loss = 0.0019589881412684917
iteration 206, loss = 0.0017903903499245644
iteration 207, loss = 0.0019031759584322572
iteration 208, loss = 0.0024770277086645365
iteration 209, loss = 0.0021199502516537905
iteration 210, loss = 0.0031800633296370506
iteration 211, loss = 0.002051744842901826
iteration 212, loss = 0.002400370081886649
iteration 213, loss = 0.0036409664899110794
iteration 214, loss = 0.002505646785721183
iteration 215, loss = 0.0017072161426767707
iteration 216, loss = 0.002059530233964324
iteration 217, loss = 0.001997720217332244
iteration 218, loss = 0.0017464798875153065
iteration 219, loss = 0.0024153362028300762
iteration 220, loss = 0.001773673458956182
iteration 221, loss = 0.0017371593276038766
iteration 222, loss = 0.0021925456821918488
iteration 223, loss = 0.0020685563795268536
iteration 224, loss = 0.0018258104100823402
iteration 225, loss = 0.001777459285221994
iteration 226, loss = 0.00216959067620337
iteration 227, loss = 0.0018580213654786348
iteration 228, loss = 0.0029371934942901134
iteration 229, loss = 0.0018200947670266032
iteration 230, loss = 0.002048404421657324
iteration 231, loss = 0.002188572194427252
iteration 232, loss = 0.002992285881191492
iteration 233, loss = 0.002721077762544155
iteration 234, loss = 0.001686690840870142
iteration 235, loss = 0.0038519303780049086
iteration 236, loss = 0.0032771516125649214
iteration 237, loss = 0.0024126737844198942
iteration 238, loss = 0.0019142278470098972
iteration 239, loss = 0.0015377462841570377
iteration 240, loss = 0.0031684741843491793
iteration 241, loss = 0.0036339855287224054
iteration 242, loss = 0.0024173762649297714
iteration 243, loss = 0.002140838885679841
iteration 244, loss = 0.0016567164566367865
iteration 245, loss = 0.0024308909196406603
iteration 246, loss = 0.0019599879160523415
iteration 247, loss = 0.0021765532437711954
iteration 248, loss = 0.0022406717762351036
iteration 249, loss = 0.0036208354867994785
iteration 250, loss = 0.0025105890817940235
iteration 251, loss = 0.001738739782012999
iteration 252, loss = 0.0018489575013518333
iteration 253, loss = 0.004024919122457504
iteration 254, loss = 0.002498013898730278
iteration 255, loss = 0.0017983780708163977
iteration 256, loss = 0.002933152951300144
iteration 257, loss = 0.002283148467540741
iteration 258, loss = 0.0020856247283518314
iteration 259, loss = 0.0021857512183487415
iteration 260, loss = 0.0026471067685633898
iteration 261, loss = 0.002586588030681014
iteration 262, loss = 0.00381354708224535
iteration 263, loss = 0.00212936126627028
iteration 264, loss = 0.002022569300606847
iteration 265, loss = 0.0019008980598300695
iteration 266, loss = 0.0019923036452382803
iteration 267, loss = 0.0016374009428545833
iteration 268, loss = 0.002087879693135619
iteration 269, loss = 0.001959580462425947
iteration 270, loss = 0.0017268155934289098
iteration 271, loss = 0.003594603855162859
iteration 272, loss = 0.0021051394287496805
iteration 273, loss = 0.002971226116642356
iteration 274, loss = 0.002141958801075816
iteration 275, loss = 0.0018208744004368782
iteration 276, loss = 0.0028628804720938206
iteration 277, loss = 0.00279447203502059
iteration 278, loss = 0.003726258408278227
iteration 279, loss = 0.0029837400652468204
iteration 280, loss = 0.0020411082077771425
iteration 281, loss = 0.0018557992298156023
iteration 282, loss = 0.002118009142577648
iteration 283, loss = 0.002624817658215761
iteration 284, loss = 0.001773095689713955
iteration 285, loss = 0.002069567795842886
iteration 286, loss = 0.00182181759737432
iteration 287, loss = 0.002579977037385106
iteration 288, loss = 0.002453300403431058
iteration 289, loss = 0.001919570262543857
iteration 290, loss = 0.0018478265265002847
iteration 291, loss = 0.0030107018537819386
iteration 292, loss = 0.003130163298919797
iteration 293, loss = 0.002088384237140417
iteration 294, loss = 0.0018180296756327152
iteration 295, loss = 0.004899134393781424
iteration 296, loss = 0.0018219692865386605
iteration 297, loss = 0.002318744780495763
iteration 298, loss = 0.001743319327943027
iteration 299, loss = 0.0035862931981682777
iteration 300, loss = 0.0020403701346367598

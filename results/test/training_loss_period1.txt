iteration 1, loss = 2.938500165939331
iteration 2, loss = 2.967930316925049
iteration 3, loss = 2.926020860671997
iteration 4, loss = 2.788336992263794
iteration 5, loss = 2.832146167755127
iteration 6, loss = 2.791684150695801
iteration 7, loss = 2.711637020111084
iteration 8, loss = 2.6688601970672607
iteration 9, loss = 2.576136350631714
iteration 10, loss = 2.5820252895355225
iteration 11, loss = 2.506990432739258
iteration 12, loss = 2.410858154296875
iteration 13, loss = 2.308844566345215
iteration 14, loss = 2.210416078567505
iteration 15, loss = 2.1780576705932617
iteration 16, loss = 2.1808300018310547
iteration 17, loss = 2.0131914615631104
iteration 18, loss = 2.0506393909454346
iteration 19, loss = 1.8929007053375244
iteration 20, loss = 1.76674222946167
iteration 21, loss = 1.771849513053894
iteration 22, loss = 1.6722631454467773
iteration 23, loss = 1.5802024602890015
iteration 24, loss = 1.6038856506347656
iteration 25, loss = 1.4805326461791992
iteration 26, loss = 1.4311603307724
iteration 27, loss = 1.4356892108917236
iteration 28, loss = 1.3005616664886475
iteration 29, loss = 1.416163682937622
iteration 30, loss = 1.2205188274383545
iteration 31, loss = 1.207486867904663
iteration 32, loss = 1.230588436126709
iteration 33, loss = 1.1510511636734009
iteration 34, loss = 1.13288414478302
iteration 35, loss = 1.0863721370697021
iteration 36, loss = 1.063563346862793
iteration 37, loss = 0.9973390102386475
iteration 38, loss = 1.0322071313858032
iteration 39, loss = 1.0118962526321411
iteration 40, loss = 0.9279161691665649
iteration 41, loss = 0.9571927785873413
iteration 42, loss = 0.9153603315353394
iteration 43, loss = 0.900540828704834
iteration 44, loss = 0.850277841091156
iteration 45, loss = 0.8862736821174622
iteration 46, loss = 0.8980424404144287
iteration 47, loss = 0.8656231164932251
iteration 48, loss = 0.8257686495780945
iteration 49, loss = 0.8479070663452148
iteration 50, loss = 0.7685506343841553
iteration 51, loss = 0.8024513125419617
iteration 52, loss = 0.7914246916770935
iteration 53, loss = 0.7539514899253845
iteration 54, loss = 0.7811558246612549
iteration 55, loss = 0.6819583177566528
iteration 56, loss = 0.7763872146606445
iteration 57, loss = 0.7267906069755554
iteration 58, loss = 0.7334389686584473
iteration 59, loss = 0.7721136808395386
iteration 60, loss = 0.7448573112487793
iteration 61, loss = 0.7357670664787292
iteration 62, loss = 0.6783779263496399
iteration 63, loss = 0.7378307580947876
iteration 64, loss = 0.7084416747093201
iteration 65, loss = 0.7291668653488159
iteration 66, loss = 0.7031612396240234
iteration 67, loss = 0.6541241407394409
iteration 68, loss = 0.7108675241470337
iteration 69, loss = 0.7271599769592285
iteration 70, loss = 0.6712808012962341
iteration 71, loss = 0.6952824592590332
iteration 72, loss = 0.7081255912780762
iteration 73, loss = 0.7392674684524536
iteration 74, loss = 0.7058371305465698
iteration 75, loss = 0.6667675971984863
iteration 76, loss = 0.6715730428695679
iteration 77, loss = 0.6866825819015503
iteration 78, loss = 0.6761795282363892
iteration 79, loss = 0.6778721213340759
iteration 80, loss = 0.6603670716285706
iteration 81, loss = 0.7005220651626587
iteration 82, loss = 0.6854261755943298
iteration 83, loss = 0.6630096435546875
iteration 84, loss = 0.6937941312789917
iteration 85, loss = 0.6391311883926392
iteration 86, loss = 0.6447733044624329
iteration 87, loss = 0.7435112595558167
iteration 88, loss = 0.6652013063430786
iteration 89, loss = 0.648159921169281
iteration 90, loss = 0.6888097524642944
iteration 91, loss = 0.7008816599845886
iteration 92, loss = 0.7118056416511536
iteration 93, loss = 0.6646475791931152
iteration 94, loss = 0.694190502166748
iteration 95, loss = 0.6589279770851135
iteration 96, loss = 0.6886191964149475
iteration 97, loss = 0.6507916450500488
iteration 98, loss = 0.6493006348609924
iteration 99, loss = 0.6487597227096558
iteration 100, loss = 0.6603732109069824
iteration 101, loss = 0.6759116649627686
iteration 102, loss = 0.6391716003417969
iteration 103, loss = 0.6245095729827881
iteration 104, loss = 0.6550721526145935
iteration 105, loss = 0.6844947338104248
iteration 106, loss = 0.6372725963592529
iteration 107, loss = 0.6054393649101257
iteration 108, loss = 0.6243654489517212
iteration 109, loss = 0.6768286228179932
iteration 110, loss = 0.6255155205726624
iteration 111, loss = 0.6431217789649963
iteration 112, loss = 0.648683488368988
iteration 113, loss = 0.6710937023162842
iteration 114, loss = 0.6494104862213135
iteration 115, loss = 0.6399745941162109
iteration 116, loss = 0.6234207153320312
iteration 117, loss = 0.6230155825614929
iteration 118, loss = 0.6281928420066833
iteration 119, loss = 0.6219664216041565
iteration 120, loss = 0.6471462249755859
iteration 121, loss = 0.6518779397010803
iteration 122, loss = 0.6276028752326965
iteration 123, loss = 0.6294912695884705
iteration 124, loss = 0.6362990140914917
iteration 125, loss = 0.6427596211433411
iteration 126, loss = 0.6500917077064514
iteration 127, loss = 0.648853600025177
iteration 128, loss = 0.6322683691978455
iteration 129, loss = 0.6201081275939941
iteration 130, loss = 0.6382828950881958
iteration 131, loss = 0.6320749521255493
iteration 132, loss = 0.6467021703720093
iteration 133, loss = 0.6458359956741333
iteration 134, loss = 0.6233864426612854
iteration 135, loss = 0.6084496974945068
iteration 136, loss = 0.6386007070541382
iteration 137, loss = 0.5944738388061523
iteration 138, loss = 0.6042846441268921
iteration 139, loss = 0.6067275404930115
iteration 140, loss = 0.6409808993339539
iteration 141, loss = 0.6300401091575623
iteration 142, loss = 0.5985623598098755
iteration 143, loss = 0.6140826344490051
iteration 144, loss = 0.6160938143730164
iteration 145, loss = 0.6174023747444153
iteration 146, loss = 0.6370337605476379
iteration 147, loss = 0.6251041293144226
iteration 148, loss = 0.609959602355957
iteration 149, loss = 0.6422703862190247
iteration 150, loss = 0.6412143707275391
iteration 151, loss = 0.5832641124725342
iteration 152, loss = 0.5978604555130005
iteration 153, loss = 0.6034353375434875
iteration 154, loss = 0.6051011681556702
iteration 155, loss = 0.6079326868057251
iteration 156, loss = 0.6088373064994812
iteration 157, loss = 0.5969757437705994
iteration 158, loss = 0.6215261220932007
iteration 159, loss = 0.6171181797981262
iteration 160, loss = 0.6255461573600769
iteration 161, loss = 0.6516797542572021
iteration 162, loss = 0.6535019278526306
iteration 163, loss = 0.5827421545982361
iteration 164, loss = 0.5706090927124023
iteration 165, loss = 0.5880163908004761
iteration 166, loss = 0.5619152784347534
iteration 167, loss = 0.5752618908882141
iteration 168, loss = 0.6400462985038757
iteration 169, loss = 0.6072787642478943
iteration 170, loss = 0.5776209831237793
iteration 171, loss = 0.5678543448448181
iteration 172, loss = 0.5716798901557922
iteration 173, loss = 0.5905857086181641
iteration 174, loss = 0.5988879203796387
iteration 175, loss = 0.6038913726806641
iteration 176, loss = 0.5957937240600586
iteration 177, loss = 0.5914701819419861
iteration 178, loss = 0.5941299200057983
iteration 179, loss = 0.5746082067489624
iteration 180, loss = 0.6077907085418701
iteration 181, loss = 0.5556039810180664
iteration 182, loss = 0.570658802986145
iteration 183, loss = 0.6172774434089661
iteration 184, loss = 0.5620813965797424
iteration 185, loss = 0.572514533996582
iteration 186, loss = 0.5975531935691833
iteration 187, loss = 0.5861778855323792
iteration 188, loss = 0.5709197521209717
iteration 189, loss = 0.5621395111083984
iteration 190, loss = 0.6169838309288025
iteration 191, loss = 0.5966190695762634
iteration 192, loss = 0.5644497871398926
iteration 193, loss = 0.5770543813705444
iteration 194, loss = 0.6001178026199341
iteration 195, loss = 0.575774073600769
iteration 196, loss = 0.6141296625137329
iteration 197, loss = 0.5453314781188965
iteration 198, loss = 0.530362606048584
iteration 199, loss = 0.5646274089813232
iteration 200, loss = 0.5639858841896057
iteration 201, loss = 0.6195902824401855
iteration 202, loss = 0.5416824817657471
iteration 203, loss = 0.5863515734672546
iteration 204, loss = 0.5383967757225037
iteration 205, loss = 0.5710146427154541
iteration 206, loss = 0.5314573049545288
iteration 207, loss = 0.5349517464637756
iteration 208, loss = 0.5682844519615173
iteration 209, loss = 0.552433967590332
iteration 210, loss = 0.5719491243362427
iteration 211, loss = 0.5304322242736816
iteration 212, loss = 0.6051065325737
iteration 213, loss = 0.5300667881965637
iteration 214, loss = 0.5214704871177673
iteration 215, loss = 0.6077730059623718
iteration 216, loss = 0.5704589486122131
iteration 217, loss = 0.5161854028701782
iteration 218, loss = 0.5678737163543701
iteration 219, loss = 0.5482125878334045
iteration 220, loss = 0.540048360824585
iteration 221, loss = 0.5085492730140686
iteration 222, loss = 0.5500009655952454
iteration 223, loss = 0.5111497044563293
iteration 224, loss = 0.5632466673851013
iteration 225, loss = 0.5710300803184509
iteration 226, loss = 0.5007669925689697
iteration 227, loss = 0.6125133633613586
iteration 228, loss = 0.5561548471450806
iteration 229, loss = 0.4923689067363739
iteration 230, loss = 0.5210970044136047
iteration 231, loss = 0.5129886269569397
iteration 232, loss = 0.520592451095581
iteration 233, loss = 0.6180702447891235
iteration 234, loss = 0.5131857991218567
iteration 235, loss = 0.5196581482887268
iteration 236, loss = 0.5051344037055969
iteration 237, loss = 0.5597279667854309
iteration 238, loss = 0.5015631318092346
iteration 239, loss = 0.509544849395752
iteration 240, loss = 0.5367463231086731
iteration 241, loss = 0.48078298568725586
iteration 242, loss = 0.49392932653427124
iteration 243, loss = 0.5193970799446106
iteration 244, loss = 0.5327762961387634
iteration 245, loss = 0.5337069630622864
iteration 246, loss = 0.5709551572799683
iteration 247, loss = 0.5306306481361389
iteration 248, loss = 0.5122464895248413
iteration 249, loss = 0.48651906847953796
iteration 250, loss = 0.5452439188957214
iteration 251, loss = 0.508662223815918
iteration 252, loss = 0.5518048405647278
iteration 253, loss = 0.47930777072906494
iteration 254, loss = 0.5571604371070862
iteration 255, loss = 0.48427268862724304
iteration 256, loss = 0.5076678991317749
iteration 257, loss = 0.5584673285484314
iteration 258, loss = 0.5125015377998352
iteration 259, loss = 0.48031163215637207
iteration 260, loss = 0.497555673122406
iteration 261, loss = 0.530748188495636
iteration 262, loss = 0.5539146065711975
iteration 263, loss = 0.4879137873649597
iteration 264, loss = 0.4899045526981354
iteration 265, loss = 0.5435856580734253
iteration 266, loss = 0.45013999938964844
iteration 267, loss = 0.4656423032283783
iteration 268, loss = 0.507806658744812
iteration 269, loss = 0.47507792711257935
iteration 270, loss = 0.48750507831573486
iteration 271, loss = 0.48354655504226685
iteration 272, loss = 0.4562976062297821
iteration 273, loss = 0.4874604046344757
iteration 274, loss = 0.4775259792804718
iteration 275, loss = 0.49634024500846863
iteration 276, loss = 0.4923784136772156
iteration 277, loss = 0.47581207752227783
iteration 278, loss = 0.5420860648155212
iteration 279, loss = 0.44951778650283813
iteration 280, loss = 0.4724665880203247
iteration 281, loss = 0.4749455153942108
iteration 282, loss = 0.5050950646400452
iteration 283, loss = 0.43528151512145996
iteration 284, loss = 0.47830188274383545
iteration 285, loss = 0.4661712944507599
iteration 286, loss = 0.49426811933517456
iteration 287, loss = 0.46943819522857666
iteration 288, loss = 0.45339086651802063
iteration 289, loss = 0.48908910155296326
iteration 290, loss = 0.4432627260684967
iteration 291, loss = 0.486595094203949
iteration 292, loss = 0.45664942264556885
iteration 293, loss = 0.44530269503593445
iteration 294, loss = 0.46894967555999756
iteration 295, loss = 0.470499724149704
iteration 296, loss = 0.4557925760746002
iteration 297, loss = 0.5356473922729492
iteration 298, loss = 0.4652708172798157
iteration 299, loss = 0.45489701628685
iteration 300, loss = 0.4967561364173889
iteration 1, loss = 0.4467015862464905
iteration 2, loss = 0.42735201120376587
iteration 3, loss = 0.43345022201538086
iteration 4, loss = 0.4169970452785492
iteration 5, loss = 0.43221792578697205
iteration 6, loss = 0.4871467351913452
iteration 7, loss = 0.4124837815761566
iteration 8, loss = 0.44310879707336426
iteration 9, loss = 0.4506164789199829
iteration 10, loss = 0.419141560792923
iteration 11, loss = 0.4051295816898346
iteration 12, loss = 0.44606176018714905
iteration 13, loss = 0.4349851906299591
iteration 14, loss = 0.43578213453292847
iteration 15, loss = 0.4639780819416046
iteration 16, loss = 0.4561759829521179
iteration 17, loss = 0.3980062007904053
iteration 18, loss = 0.45471474528312683
iteration 19, loss = 0.4159122407436371
iteration 20, loss = 0.40462708473205566
iteration 21, loss = 0.4374770522117615
iteration 22, loss = 0.44273680448532104
iteration 23, loss = 0.39659154415130615
iteration 24, loss = 0.39908331632614136
iteration 25, loss = 0.42193832993507385
iteration 26, loss = 0.46681052446365356
iteration 27, loss = 0.4147001802921295
iteration 28, loss = 0.4121132791042328
iteration 29, loss = 0.4183609187602997
iteration 30, loss = 0.4044337272644043
iteration 31, loss = 0.47983479499816895
iteration 32, loss = 0.3997419774532318
iteration 33, loss = 0.3839763104915619
iteration 34, loss = 0.4146249294281006
iteration 35, loss = 0.47069841623306274
iteration 36, loss = 0.4248823821544647
iteration 37, loss = 0.42504748702049255
iteration 38, loss = 0.43956291675567627
iteration 39, loss = 0.38485777378082275
iteration 40, loss = 0.44143223762512207
iteration 41, loss = 0.43397554755210876
iteration 42, loss = 0.457279235124588
iteration 43, loss = 0.3973367214202881
iteration 44, loss = 0.44521093368530273
iteration 45, loss = 0.4481387138366699
iteration 46, loss = 0.37967705726623535
iteration 47, loss = 0.398689866065979
iteration 48, loss = 0.3774445652961731
iteration 49, loss = 0.3597354292869568
iteration 50, loss = 0.38455930352211
iteration 51, loss = 0.36414188146591187
iteration 52, loss = 0.37464067339897156
iteration 53, loss = 0.34442776441574097
iteration 54, loss = 0.3583005964756012
iteration 55, loss = 0.3684304356575012
iteration 56, loss = 0.39503905177116394
iteration 57, loss = 0.3535006046295166
iteration 58, loss = 0.3724157512187958
iteration 59, loss = 0.513416588306427
iteration 60, loss = 0.39620089530944824
iteration 61, loss = 0.41453391313552856
iteration 62, loss = 0.3827180862426758
iteration 63, loss = 0.38414788246154785
iteration 64, loss = 0.3744312822818756
iteration 65, loss = 0.3604903817176819
iteration 66, loss = 0.37171751260757446
iteration 67, loss = 0.36539044976234436
iteration 68, loss = 0.34232181310653687
iteration 69, loss = 0.3842211365699768
iteration 70, loss = 0.36519667506217957
iteration 71, loss = 0.4603966772556305
iteration 72, loss = 0.33471405506134033
iteration 73, loss = 0.37214285135269165
iteration 74, loss = 0.3475569486618042
iteration 75, loss = 0.33174896240234375
iteration 76, loss = 0.36245912313461304
iteration 77, loss = 0.47253987193107605
iteration 78, loss = 0.3799006938934326
iteration 79, loss = 0.3318040668964386
iteration 80, loss = 0.33923542499542236
iteration 81, loss = 0.39904341101646423
iteration 82, loss = 0.38858288526535034
iteration 83, loss = 0.3620140254497528
iteration 84, loss = 0.336895614862442
iteration 85, loss = 0.3389286696910858
iteration 86, loss = 0.3466222882270813
iteration 87, loss = 0.36471042037010193
iteration 88, loss = 0.31702056527137756
iteration 89, loss = 0.4021056592464447
iteration 90, loss = 0.3279736638069153
iteration 91, loss = 0.3438054025173187
iteration 92, loss = 0.3312927782535553
iteration 93, loss = 0.325449138879776
iteration 94, loss = 0.39292389154434204
iteration 95, loss = 0.390776127576828
iteration 96, loss = 0.3569958209991455
iteration 97, loss = 0.3836944103240967
iteration 98, loss = 0.3174951672554016
iteration 99, loss = 0.35685980319976807
iteration 100, loss = 0.33691534399986267
iteration 101, loss = 0.34231138229370117
iteration 102, loss = 0.3173500895500183
iteration 103, loss = 0.3526875078678131
iteration 104, loss = 0.3814387321472168
iteration 105, loss = 0.35783255100250244
iteration 106, loss = 0.3309127688407898
iteration 107, loss = 0.3545165956020355
iteration 108, loss = 0.2977031171321869
iteration 109, loss = 0.3280918300151825
iteration 110, loss = 0.30006247758865356
iteration 111, loss = 0.34776273369789124
iteration 112, loss = 0.3391547203063965
iteration 113, loss = 0.3488449454307556
iteration 114, loss = 0.33254140615463257
iteration 115, loss = 0.3004075586795807
iteration 116, loss = 0.2857653796672821
iteration 117, loss = 0.37511178851127625
iteration 118, loss = 0.31640610098838806
iteration 119, loss = 0.30473047494888306
iteration 120, loss = 0.3359962999820709
iteration 121, loss = 0.29784727096557617
iteration 122, loss = 0.34299570322036743
iteration 123, loss = 0.3391835689544678
iteration 124, loss = 0.31234943866729736
iteration 125, loss = 0.34295374155044556
iteration 126, loss = 0.3099081516265869
iteration 127, loss = 0.31327271461486816
iteration 128, loss = 0.2714402973651886
iteration 129, loss = 0.35884207487106323
iteration 130, loss = 0.34298425912857056
iteration 131, loss = 0.2988100051879883
iteration 132, loss = 0.309988409280777
iteration 133, loss = 0.33672192692756653
iteration 134, loss = 0.2860914170742035
iteration 135, loss = 0.2933061420917511
iteration 136, loss = 0.2917613685131073
iteration 137, loss = 0.3370625972747803
iteration 138, loss = 0.30505600571632385
iteration 139, loss = 0.2708437144756317
iteration 140, loss = 0.28035756945610046
iteration 141, loss = 0.3044031262397766
iteration 142, loss = 0.36070936918258667
iteration 143, loss = 0.27200162410736084
iteration 144, loss = 0.3066607713699341
iteration 145, loss = 0.2989214360713959
iteration 146, loss = 0.3038811683654785
iteration 147, loss = 0.26563775539398193
iteration 148, loss = 0.39215752482414246
iteration 149, loss = 0.26656389236450195
iteration 150, loss = 0.32724249362945557
iteration 151, loss = 0.2734568119049072
iteration 152, loss = 0.2798345386981964
iteration 153, loss = 0.33735761046409607
iteration 154, loss = 0.25915634632110596
iteration 155, loss = 0.2704927325248718
iteration 156, loss = 0.2603664994239807
iteration 157, loss = 0.29000890254974365
iteration 158, loss = 0.2558518946170807
iteration 159, loss = 0.2886153757572174
iteration 160, loss = 0.29938796162605286
iteration 161, loss = 0.27887067198753357
iteration 162, loss = 0.2572096586227417
iteration 163, loss = 0.2590237259864807
iteration 164, loss = 0.2678418457508087
iteration 165, loss = 0.2427733689546585
iteration 166, loss = 0.30361998081207275
iteration 167, loss = 0.25310516357421875
iteration 168, loss = 0.25637444853782654
iteration 169, loss = 0.37552520632743835
iteration 170, loss = 0.24712258577346802
iteration 171, loss = 0.3092581033706665
iteration 172, loss = 0.25437837839126587
iteration 173, loss = 0.25869429111480713
iteration 174, loss = 0.23183616995811462
iteration 175, loss = 0.23387229442596436
iteration 176, loss = 0.2333258092403412
iteration 177, loss = 0.2663857936859131
iteration 178, loss = 0.296755850315094
iteration 179, loss = 0.2737618386745453
iteration 180, loss = 0.2749001085758209
iteration 181, loss = 0.2731538414955139
iteration 182, loss = 0.23770985007286072
iteration 183, loss = 0.26925745606422424
iteration 184, loss = 0.22981177270412445
iteration 185, loss = 0.2581237256526947
iteration 186, loss = 0.2566080093383789
iteration 187, loss = 0.23704242706298828
iteration 188, loss = 0.2241811603307724
iteration 189, loss = 0.35435590147972107
iteration 190, loss = 0.2296520173549652
iteration 191, loss = 0.25407519936561584
iteration 192, loss = 0.23678894340991974
iteration 193, loss = 0.274815171957016
iteration 194, loss = 0.2295749932527542
iteration 195, loss = 0.22884880006313324
iteration 196, loss = 0.22130464017391205
iteration 197, loss = 0.24386686086654663
iteration 198, loss = 0.24556368589401245
iteration 199, loss = 0.2583816945552826
iteration 200, loss = 0.22622676193714142
iteration 201, loss = 0.3096359968185425
iteration 202, loss = 0.22000622749328613
iteration 203, loss = 0.27722084522247314
iteration 204, loss = 0.23218348622322083
iteration 205, loss = 0.20708869397640228
iteration 206, loss = 0.2613111138343811
iteration 207, loss = 0.24743609130382538
iteration 208, loss = 0.3037435710430145
iteration 209, loss = 0.22222071886062622
iteration 210, loss = 0.23745986819267273
iteration 211, loss = 0.2083054929971695
iteration 212, loss = 0.20164385437965393
iteration 213, loss = 0.21151649951934814
iteration 214, loss = 0.25229617953300476
iteration 215, loss = 0.26752346754074097
iteration 216, loss = 0.2460896521806717
iteration 217, loss = 0.22033727169036865
iteration 218, loss = 0.2551100552082062
iteration 219, loss = 0.23609378933906555
iteration 220, loss = 0.2111242115497589
iteration 221, loss = 0.2888000011444092
iteration 222, loss = 0.2290034145116806
iteration 223, loss = 0.2104690670967102
iteration 224, loss = 0.20320159196853638
iteration 225, loss = 0.23080727458000183
iteration 226, loss = 0.18877094984054565
iteration 227, loss = 0.2146170288324356
iteration 228, loss = 0.2154354304075241
iteration 229, loss = 0.20815759897232056
iteration 230, loss = 0.19754023849964142
iteration 231, loss = 0.2283344566822052
iteration 232, loss = 0.20472925901412964
iteration 233, loss = 0.24976162612438202
iteration 234, loss = 0.26039984822273254
iteration 235, loss = 0.20583750307559967
iteration 236, loss = 0.22265590727329254
iteration 237, loss = 0.20141787827014923
iteration 238, loss = 0.21471361815929413
iteration 239, loss = 0.1961449235677719
iteration 240, loss = 0.2010432779788971
iteration 241, loss = 0.18557798862457275
iteration 242, loss = 0.18920284509658813
iteration 243, loss = 0.2270445078611374
iteration 244, loss = 0.19987711310386658
iteration 245, loss = 0.23567667603492737
iteration 246, loss = 0.19526870548725128
iteration 247, loss = 0.18775224685668945
iteration 248, loss = 0.18287238478660583
iteration 249, loss = 0.19621670246124268
iteration 250, loss = 0.21956875920295715
iteration 251, loss = 0.17928574979305267
iteration 252, loss = 0.20085829496383667
iteration 253, loss = 0.2201552838087082
iteration 254, loss = 0.17914718389511108
iteration 255, loss = 0.18541981279850006
iteration 256, loss = 0.19229421019554138
iteration 257, loss = 0.17275117337703705
iteration 258, loss = 0.22568589448928833
iteration 259, loss = 0.19670939445495605
iteration 260, loss = 0.18764935433864594
iteration 261, loss = 0.18478527665138245
iteration 262, loss = 0.16864296793937683
iteration 263, loss = 0.18421068787574768
iteration 264, loss = 0.21052324771881104
iteration 265, loss = 0.19239164888858795
iteration 266, loss = 0.20443683862686157
iteration 267, loss = 0.2658328413963318
iteration 268, loss = 0.1920596957206726
iteration 269, loss = 0.17291417717933655
iteration 270, loss = 0.20091073215007782
iteration 271, loss = 0.16576217114925385
iteration 272, loss = 0.1733745038509369
iteration 273, loss = 0.20628240704536438
iteration 274, loss = 0.16640673577785492
iteration 275, loss = 0.188444122672081
iteration 276, loss = 0.17936964333057404
iteration 277, loss = 0.2575041949748993
iteration 278, loss = 0.16614162921905518
iteration 279, loss = 0.17728829383850098
iteration 280, loss = 0.1751982718706131
iteration 281, loss = 0.16634757816791534
iteration 282, loss = 0.17861609160900116
iteration 283, loss = 0.18310759961605072
iteration 284, loss = 0.15764743089675903
iteration 285, loss = 0.22369073331356049
iteration 286, loss = 0.18684452772140503
iteration 287, loss = 0.2411823272705078
iteration 288, loss = 0.175085186958313
iteration 289, loss = 0.22826030850410461
iteration 290, loss = 0.19180826842784882
iteration 291, loss = 0.1743042767047882
iteration 292, loss = 0.23549866676330566
iteration 293, loss = 0.1766394078731537
iteration 294, loss = 0.1899712085723877
iteration 295, loss = 0.17956604063510895
iteration 296, loss = 0.17530488967895508
iteration 297, loss = 0.17649632692337036
iteration 298, loss = 0.16895411908626556
iteration 299, loss = 0.16775184869766235
iteration 300, loss = 0.196974515914917
iteration 1, loss = 0.1662544459104538
iteration 2, loss = 0.1615026295185089
iteration 3, loss = 0.17363768815994263
iteration 4, loss = 0.15928924083709717
iteration 5, loss = 0.20303483307361603
iteration 6, loss = 0.1554557979106903
iteration 7, loss = 0.1878969967365265
iteration 8, loss = 0.14563322067260742
iteration 9, loss = 0.17732422053813934
iteration 10, loss = 0.14446929097175598
iteration 11, loss = 0.14560186862945557
iteration 12, loss = 0.19562768936157227
iteration 13, loss = 0.15528525412082672
iteration 14, loss = 0.15329769253730774
iteration 15, loss = 0.14845585823059082
iteration 16, loss = 0.17852313816547394
iteration 17, loss = 0.18920642137527466
iteration 18, loss = 0.1469135284423828
iteration 19, loss = 0.1581040620803833
iteration 20, loss = 0.18240438401699066
iteration 21, loss = 0.16250726580619812
iteration 22, loss = 0.16487498581409454
iteration 23, loss = 0.13602131605148315
iteration 24, loss = 0.16694952547550201
iteration 25, loss = 0.19911015033721924
iteration 26, loss = 0.16066668927669525
iteration 27, loss = 0.1458364874124527
iteration 28, loss = 0.15731483697891235
iteration 29, loss = 0.15379130840301514
iteration 30, loss = 0.15517887473106384
iteration 31, loss = 0.15566597878932953
iteration 32, loss = 0.14594072103500366
iteration 33, loss = 0.13582707941532135
iteration 34, loss = 0.1521816849708557
iteration 35, loss = 0.13711348176002502
iteration 36, loss = 0.1389586627483368
iteration 37, loss = 0.1477101594209671
iteration 38, loss = 0.16451126337051392
iteration 39, loss = 0.18472060561180115
iteration 40, loss = 0.16104109585285187
iteration 41, loss = 0.18337635695934296
iteration 42, loss = 0.15222948789596558
iteration 43, loss = 0.13428552448749542
iteration 44, loss = 0.159239262342453
iteration 45, loss = 0.15306004881858826
iteration 46, loss = 0.14247021079063416
iteration 47, loss = 0.13894766569137573
iteration 48, loss = 0.13922633230686188
iteration 49, loss = 0.16188304126262665
iteration 50, loss = 0.16741062700748444
iteration 51, loss = 0.13257449865341187
iteration 52, loss = 0.14635437726974487
iteration 53, loss = 0.15077543258666992
iteration 54, loss = 0.13225865364074707
iteration 55, loss = 0.1477425992488861
iteration 56, loss = 0.14470675587654114
iteration 57, loss = 0.13257378339767456
iteration 58, loss = 0.14781978726387024
iteration 59, loss = 0.16813869774341583
iteration 60, loss = 0.1275431513786316
iteration 61, loss = 0.16828027367591858
iteration 62, loss = 0.16361796855926514
iteration 63, loss = 0.14883479475975037
iteration 64, loss = 0.14781615138053894
iteration 65, loss = 0.13086257874965668
iteration 66, loss = 0.14454543590545654
iteration 67, loss = 0.147428497672081
iteration 68, loss = 0.16577062010765076
iteration 69, loss = 0.11708594858646393
iteration 70, loss = 0.12254621833562851
iteration 71, loss = 0.16498024761676788
iteration 72, loss = 0.12335079908370972
iteration 73, loss = 0.15047220885753632
iteration 74, loss = 0.12370261549949646
iteration 75, loss = 0.12768372893333435
iteration 76, loss = 0.15034884214401245
iteration 77, loss = 0.14376991987228394
iteration 78, loss = 0.12338097393512726
iteration 79, loss = 0.1339074969291687
iteration 80, loss = 0.14502236247062683
iteration 81, loss = 0.15018174052238464
iteration 82, loss = 0.12496528774499893
iteration 83, loss = 0.1133468747138977
iteration 84, loss = 0.1382562816143036
iteration 85, loss = 0.14992985129356384
iteration 86, loss = 0.15204322338104248
iteration 87, loss = 0.16527050733566284
iteration 88, loss = 0.13681460916996002
iteration 89, loss = 0.12583525478839874
iteration 90, loss = 0.12662054598331451
iteration 91, loss = 0.11242437362670898
iteration 92, loss = 0.11462704837322235
iteration 93, loss = 0.11180111020803452
iteration 94, loss = 0.14742860198020935
iteration 95, loss = 0.13013187050819397
iteration 96, loss = 0.15191030502319336
iteration 97, loss = 0.15478211641311646
iteration 98, loss = 0.13265366852283478
iteration 99, loss = 0.11729055643081665
iteration 100, loss = 0.11089500039815903
iteration 101, loss = 0.13262170553207397
iteration 102, loss = 0.11219608038663864
iteration 103, loss = 0.12102025747299194
iteration 104, loss = 0.1207166388630867
iteration 105, loss = 0.14961686730384827
iteration 106, loss = 0.11754879355430603
iteration 107, loss = 0.10651055723428726
iteration 108, loss = 0.1423201560974121
iteration 109, loss = 0.10711244493722916
iteration 110, loss = 0.1141747459769249
iteration 111, loss = 0.16950011253356934
iteration 112, loss = 0.12622565031051636
iteration 113, loss = 0.1232982724905014
iteration 114, loss = 0.11368194967508316
iteration 115, loss = 0.12666986882686615
iteration 116, loss = 0.11771105229854584
iteration 117, loss = 0.1161549836397171
iteration 118, loss = 0.10783080756664276
iteration 119, loss = 0.12174884974956512
iteration 120, loss = 0.11637204885482788
iteration 121, loss = 0.14670979976654053
iteration 122, loss = 0.10678723454475403
iteration 123, loss = 0.1130513995885849
iteration 124, loss = 0.10661707073450089
iteration 125, loss = 0.13770601153373718
iteration 126, loss = 0.12201059609651566
iteration 127, loss = 0.1206154078245163
iteration 128, loss = 0.1291753053665161
iteration 129, loss = 0.13123376667499542
iteration 130, loss = 0.1246754378080368
iteration 131, loss = 0.11525169759988785
iteration 132, loss = 0.1107385978102684
iteration 133, loss = 0.11196300387382507
iteration 134, loss = 0.11063523590564728
iteration 135, loss = 0.11576392501592636
iteration 136, loss = 0.11788026243448257
iteration 137, loss = 0.1004827469587326
iteration 138, loss = 0.11321189999580383
iteration 139, loss = 0.1112288162112236
iteration 140, loss = 0.09840282052755356
iteration 141, loss = 0.11135395616292953
iteration 142, loss = 0.11597395688295364
iteration 143, loss = 0.12672650814056396
iteration 144, loss = 0.09238213300704956
iteration 145, loss = 0.1009136438369751
iteration 146, loss = 0.10408815741539001
iteration 147, loss = 0.10245680809020996
iteration 148, loss = 0.11405035853385925
iteration 149, loss = 0.09843146055936813
iteration 150, loss = 0.11160927265882492
iteration 151, loss = 0.10724455118179321
iteration 152, loss = 0.11681962013244629
iteration 153, loss = 0.12971767783164978
iteration 154, loss = 0.11068044602870941
iteration 155, loss = 0.09702044725418091
iteration 156, loss = 0.11498300731182098
iteration 157, loss = 0.1111784279346466
iteration 158, loss = 0.11310506612062454
iteration 159, loss = 0.13271911442279816
iteration 160, loss = 0.13184984028339386
iteration 161, loss = 0.09834514558315277
iteration 162, loss = 0.15232416987419128
iteration 163, loss = 0.0910901352763176
iteration 164, loss = 0.1413709968328476
iteration 165, loss = 0.09570673108100891
iteration 166, loss = 0.10808717459440231
iteration 167, loss = 0.09995978325605392
iteration 168, loss = 0.0969134196639061
iteration 169, loss = 0.09787816554307938
iteration 170, loss = 0.12297461926937103
iteration 171, loss = 0.0901719331741333
iteration 172, loss = 0.1002989113330841
iteration 173, loss = 0.09064996242523193
iteration 174, loss = 0.09304052591323853
iteration 175, loss = 0.08132922649383545
iteration 176, loss = 0.09830336272716522
iteration 177, loss = 0.10938039422035217
iteration 178, loss = 0.1105223149061203
iteration 179, loss = 0.11624358594417572
iteration 180, loss = 0.10748231410980225
iteration 181, loss = 0.12316745519638062
iteration 182, loss = 0.10276691615581512
iteration 183, loss = 0.10529108345508575
iteration 184, loss = 0.09102097898721695
iteration 185, loss = 0.09299781918525696
iteration 186, loss = 0.08625897020101547
iteration 187, loss = 0.09013721346855164
iteration 188, loss = 0.09269088506698608
iteration 189, loss = 0.095144122838974
iteration 190, loss = 0.1053636372089386
iteration 191, loss = 0.1095920130610466
iteration 192, loss = 0.09688948839902878
iteration 193, loss = 0.08551394194364548
iteration 194, loss = 0.091361403465271
iteration 195, loss = 0.09470796585083008
iteration 196, loss = 0.13464504480361938
iteration 197, loss = 0.10758915543556213
iteration 198, loss = 0.10528036952018738
iteration 199, loss = 0.0878133699297905
iteration 200, loss = 0.12365905940532684
iteration 201, loss = 0.10068485885858536
iteration 202, loss = 0.08613292872905731
iteration 203, loss = 0.08816123753786087
iteration 204, loss = 0.08792836964130402
iteration 205, loss = 0.1034945398569107
iteration 206, loss = 0.08155018091201782
iteration 207, loss = 0.08531326800584793
iteration 208, loss = 0.08672542124986649
iteration 209, loss = 0.07896549254655838
iteration 210, loss = 0.08512907475233078
iteration 211, loss = 0.08642089366912842
iteration 212, loss = 0.08162855356931686
iteration 213, loss = 0.08954884111881256
iteration 214, loss = 0.09191624820232391
iteration 215, loss = 0.08494112640619278
iteration 216, loss = 0.09580786526203156
iteration 217, loss = 0.07984840869903564
iteration 218, loss = 0.08513007313013077
iteration 219, loss = 0.08386003971099854
iteration 220, loss = 0.10187459737062454
iteration 221, loss = 0.09258699417114258
iteration 222, loss = 0.07490499317646027
iteration 223, loss = 0.08495102822780609
iteration 224, loss = 0.074981190264225
iteration 225, loss = 0.07515230029821396
iteration 226, loss = 0.07989547401666641
iteration 227, loss = 0.10138650983572006
iteration 228, loss = 0.0762777030467987
iteration 229, loss = 0.0883333683013916
iteration 230, loss = 0.1062067374587059
iteration 231, loss = 0.0827120840549469
iteration 232, loss = 0.11098217964172363
iteration 233, loss = 0.08812964707612991
iteration 234, loss = 0.09685942530632019
iteration 235, loss = 0.07627779990434647
iteration 236, loss = 0.07825218141078949
iteration 237, loss = 0.07857818156480789
iteration 238, loss = 0.07296949625015259
iteration 239, loss = 0.08540591597557068
iteration 240, loss = 0.07317700237035751
iteration 241, loss = 0.09113083779811859
iteration 242, loss = 0.11550528556108475
iteration 243, loss = 0.09693799912929535
iteration 244, loss = 0.07638584822416306
iteration 245, loss = 0.09242293983697891
iteration 246, loss = 0.10497701913118362
iteration 247, loss = 0.07757619023323059
iteration 248, loss = 0.08496291190385818
iteration 249, loss = 0.10107463598251343
iteration 250, loss = 0.08733461797237396
iteration 251, loss = 0.09291347861289978
iteration 252, loss = 0.07491503655910492
iteration 253, loss = 0.07977651059627533
iteration 254, loss = 0.07917390018701553
iteration 255, loss = 0.06856495141983032
iteration 256, loss = 0.0825793668627739
iteration 257, loss = 0.07677266001701355
iteration 258, loss = 0.0760272890329361
iteration 259, loss = 0.0762675479054451
iteration 260, loss = 0.0794520452618599
iteration 261, loss = 0.08477410674095154
iteration 262, loss = 0.06999656558036804
iteration 263, loss = 0.08905749768018723
iteration 264, loss = 0.07623392343521118
iteration 265, loss = 0.08955231308937073
iteration 266, loss = 0.0739496648311615
iteration 267, loss = 0.07224476337432861
iteration 268, loss = 0.09590492397546768
iteration 269, loss = 0.1013401597738266
iteration 270, loss = 0.07963794469833374
iteration 271, loss = 0.08364503085613251
iteration 272, loss = 0.07487551122903824
iteration 273, loss = 0.10450169444084167
iteration 274, loss = 0.06745180487632751
iteration 275, loss = 0.0719476193189621
iteration 276, loss = 0.06746107339859009
iteration 277, loss = 0.07618659734725952
iteration 278, loss = 0.07889172434806824
iteration 279, loss = 0.0674385353922844
iteration 280, loss = 0.0855560377240181
iteration 281, loss = 0.06910780072212219
iteration 282, loss = 0.09009487181901932
iteration 283, loss = 0.06980980187654495
iteration 284, loss = 0.06751305609941483
iteration 285, loss = 0.0883650854229927
iteration 286, loss = 0.06872566044330597
iteration 287, loss = 0.07127546519041061
iteration 288, loss = 0.08247416466474533
iteration 289, loss = 0.09261816740036011
iteration 290, loss = 0.06331034749746323
iteration 291, loss = 0.08957279473543167
iteration 292, loss = 0.09509187191724777
iteration 293, loss = 0.06914868205785751
iteration 294, loss = 0.07369871437549591
iteration 295, loss = 0.07030701637268066
iteration 296, loss = 0.06739746034145355
iteration 297, loss = 0.06213796138763428
iteration 298, loss = 0.06200223043560982
iteration 299, loss = 0.07961048185825348
iteration 300, loss = 0.0688653364777565
iteration 1, loss = 0.06584817171096802
iteration 2, loss = 0.08051283657550812
iteration 3, loss = 0.06762075424194336
iteration 4, loss = 0.07105110585689545
iteration 5, loss = 0.10495708137750626
iteration 6, loss = 0.06131337583065033
iteration 7, loss = 0.06851757317781448
iteration 8, loss = 0.08806965500116348
iteration 9, loss = 0.06790333986282349
iteration 10, loss = 0.07366370409727097
iteration 11, loss = 0.0704793632030487
iteration 12, loss = 0.08920243382453918
iteration 13, loss = 0.0795530304312706
iteration 14, loss = 0.07138291001319885
iteration 15, loss = 0.06268980354070663
iteration 16, loss = 0.0747411847114563
iteration 17, loss = 0.08580192178487778
iteration 18, loss = 0.06229698285460472
iteration 19, loss = 0.06205441802740097
iteration 20, loss = 0.06049264222383499
iteration 21, loss = 0.0639553964138031
iteration 22, loss = 0.0802910104393959
iteration 23, loss = 0.06215829774737358
iteration 24, loss = 0.06624531000852585
iteration 25, loss = 0.061504945158958435
iteration 26, loss = 0.07610198110342026
iteration 27, loss = 0.059989407658576965
iteration 28, loss = 0.07851212471723557
iteration 29, loss = 0.06757687777280807
iteration 30, loss = 0.06324949860572815
iteration 31, loss = 0.05826921388506889
iteration 32, loss = 0.07474267482757568
iteration 33, loss = 0.06563931703567505
iteration 34, loss = 0.06353899836540222
iteration 35, loss = 0.0752088874578476
iteration 36, loss = 0.0831461176276207
iteration 37, loss = 0.06598621606826782
iteration 38, loss = 0.09118108451366425
iteration 39, loss = 0.07051080465316772
iteration 40, loss = 0.05609440430998802
iteration 41, loss = 0.05655169114470482
iteration 42, loss = 0.061615392565727234
iteration 43, loss = 0.06805144250392914
iteration 44, loss = 0.06583140790462494
iteration 45, loss = 0.06212250143289566
iteration 46, loss = 0.06514377146959305
iteration 47, loss = 0.0733758732676506
iteration 48, loss = 0.05549676716327667
iteration 49, loss = 0.05697614327073097
iteration 50, loss = 0.06933633983135223
iteration 51, loss = 0.06266675144433975
iteration 52, loss = 0.05794380232691765
iteration 53, loss = 0.05540667846798897
iteration 54, loss = 0.056711990386247635
iteration 55, loss = 0.057936958968639374
iteration 56, loss = 0.056437186896800995
iteration 57, loss = 0.05716073513031006
iteration 58, loss = 0.05558595061302185
iteration 59, loss = 0.055782318115234375
iteration 60, loss = 0.05568527802824974
iteration 61, loss = 0.06361786276102066
iteration 62, loss = 0.05704427510499954
iteration 63, loss = 0.055237628519535065
iteration 64, loss = 0.060001589357852936
iteration 65, loss = 0.06392370909452438
iteration 66, loss = 0.056389741599559784
iteration 67, loss = 0.054964691400527954
iteration 68, loss = 0.06368334591388702
iteration 69, loss = 0.052805885672569275
iteration 70, loss = 0.05656009167432785
iteration 71, loss = 0.057817742228507996
iteration 72, loss = 0.062486305832862854
iteration 73, loss = 0.0525483563542366
iteration 74, loss = 0.054162878543138504
iteration 75, loss = 0.05361311137676239
iteration 76, loss = 0.07096776366233826
iteration 77, loss = 0.053462233394384384
iteration 78, loss = 0.06256266683340073
iteration 79, loss = 0.05116773024201393
iteration 80, loss = 0.07591775059700012
iteration 81, loss = 0.07246986776590347
iteration 82, loss = 0.05870802327990532
iteration 83, loss = 0.06592670828104019
iteration 84, loss = 0.05696994438767433
iteration 85, loss = 0.06532705575227737
iteration 86, loss = 0.0495014451444149
iteration 87, loss = 0.05380629375576973
iteration 88, loss = 0.047743070870637894
iteration 89, loss = 0.06957796216011047
iteration 90, loss = 0.05821071192622185
iteration 91, loss = 0.0640190839767456
iteration 92, loss = 0.05961275473237038
iteration 93, loss = 0.0720253586769104
iteration 94, loss = 0.04920706897974014
iteration 95, loss = 0.05942445993423462
iteration 96, loss = 0.05411411076784134
iteration 97, loss = 0.06775067001581192
iteration 98, loss = 0.05023450031876564
iteration 99, loss = 0.0525272898375988
iteration 100, loss = 0.055247433483600616
iteration 101, loss = 0.053157296031713486
iteration 102, loss = 0.056839775294065475
iteration 103, loss = 0.04792482405900955
iteration 104, loss = 0.04916239529848099
iteration 105, loss = 0.0705251693725586
iteration 106, loss = 0.04806825518608093
iteration 107, loss = 0.061353594064712524
iteration 108, loss = 0.052939675748348236
iteration 109, loss = 0.07401381433010101
iteration 110, loss = 0.050211045891046524
iteration 111, loss = 0.05687211826443672
iteration 112, loss = 0.058909349143505096
iteration 113, loss = 0.049942851066589355
iteration 114, loss = 0.04650900140404701
iteration 115, loss = 0.053517743945121765
iteration 116, loss = 0.04999595880508423
iteration 117, loss = 0.05314560979604721
iteration 118, loss = 0.057528723031282425
iteration 119, loss = 0.048599280416965485
iteration 120, loss = 0.07137402892112732
iteration 121, loss = 0.0598842017352581
iteration 122, loss = 0.05778433382511139
iteration 123, loss = 0.06446933001279831
iteration 124, loss = 0.04949445277452469
iteration 125, loss = 0.04999082162976265
iteration 126, loss = 0.05612953379750252
iteration 127, loss = 0.05976645648479462
iteration 128, loss = 0.05956045910716057
iteration 129, loss = 0.04558785632252693
iteration 130, loss = 0.055597081780433655
iteration 131, loss = 0.04822817072272301
iteration 132, loss = 0.04975035414099693
iteration 133, loss = 0.055628713220357895
iteration 134, loss = 0.055739667266607285
iteration 135, loss = 0.06832993030548096
iteration 136, loss = 0.06963440030813217
iteration 137, loss = 0.05678294599056244
iteration 138, loss = 0.04772447049617767
iteration 139, loss = 0.050879720598459244
iteration 140, loss = 0.062061116099357605
iteration 141, loss = 0.04801372438669205
iteration 142, loss = 0.051284950226545334
iteration 143, loss = 0.05883420631289482
iteration 144, loss = 0.04994611814618111
iteration 145, loss = 0.06338717043399811
iteration 146, loss = 0.0464051328599453
iteration 147, loss = 0.048274073749780655
iteration 148, loss = 0.050085365772247314
iteration 149, loss = 0.06370281428098679
iteration 150, loss = 0.045338425785303116
iteration 151, loss = 0.04540811479091644
iteration 152, loss = 0.046252429485321045
iteration 153, loss = 0.045133307576179504
iteration 154, loss = 0.044723208993673325
iteration 155, loss = 0.05513826385140419
iteration 156, loss = 0.04748997837305069
iteration 157, loss = 0.04545644670724869
iteration 158, loss = 0.04909265786409378
iteration 159, loss = 0.05526656657457352
iteration 160, loss = 0.04407110437750816
iteration 161, loss = 0.04188920930027962
iteration 162, loss = 0.04692908003926277
iteration 163, loss = 0.04719270020723343
iteration 164, loss = 0.044615551829338074
iteration 165, loss = 0.052331071346998215
iteration 166, loss = 0.05252419039607048
iteration 167, loss = 0.050007253885269165
iteration 168, loss = 0.049836959689855576
iteration 169, loss = 0.04474491998553276
iteration 170, loss = 0.051531367003917694
iteration 171, loss = 0.055195923894643784
iteration 172, loss = 0.04437118396162987
iteration 173, loss = 0.05885640159249306
iteration 174, loss = 0.04872743785381317
iteration 175, loss = 0.04607141762971878
iteration 176, loss = 0.04248415678739548
iteration 177, loss = 0.04285033047199249
iteration 178, loss = 0.056906431913375854
iteration 179, loss = 0.05009938031435013
iteration 180, loss = 0.04642273858189583
iteration 181, loss = 0.04479561746120453
iteration 182, loss = 0.046379365026950836
iteration 183, loss = 0.04664382338523865
iteration 184, loss = 0.06055372208356857
iteration 185, loss = 0.04546237364411354
iteration 186, loss = 0.042626552283763885
iteration 187, loss = 0.04675229266285896
iteration 188, loss = 0.04168689623475075
iteration 189, loss = 0.04510295018553734
iteration 190, loss = 0.04171206057071686
iteration 191, loss = 0.04457474127411842
iteration 192, loss = 0.0537835918366909
iteration 193, loss = 0.039693206548690796
iteration 194, loss = 0.054007016122341156
iteration 195, loss = 0.041473012417554855
iteration 196, loss = 0.05267595499753952
iteration 197, loss = 0.04567137733101845
iteration 198, loss = 0.044503841549158096
iteration 199, loss = 0.041551295667886734
iteration 200, loss = 0.040696881711483
iteration 201, loss = 0.03904104232788086
iteration 202, loss = 0.04292111098766327
iteration 203, loss = 0.051899444311857224
iteration 204, loss = 0.039663106203079224
iteration 205, loss = 0.04235248267650604
iteration 206, loss = 0.056571897119283676
iteration 207, loss = 0.03731841966509819
iteration 208, loss = 0.03925503045320511
iteration 209, loss = 0.04292234778404236
iteration 210, loss = 0.04288175702095032
iteration 211, loss = 0.04829452931880951
iteration 212, loss = 0.039659515023231506
iteration 213, loss = 0.050390709191560745
iteration 214, loss = 0.05358075350522995
iteration 215, loss = 0.039594415575265884
iteration 216, loss = 0.04473848640918732
iteration 217, loss = 0.04224957153201103
iteration 218, loss = 0.04019513726234436
iteration 219, loss = 0.037221722304821014
iteration 220, loss = 0.03886885941028595
iteration 221, loss = 0.04062005132436752
iteration 222, loss = 0.04820740222930908
iteration 223, loss = 0.06357426941394806
iteration 224, loss = 0.038650356233119965
iteration 225, loss = 0.04183988273143768
iteration 226, loss = 0.03662987798452377
iteration 227, loss = 0.043380893766880035
iteration 228, loss = 0.04025090113282204
iteration 229, loss = 0.03863963112235069
iteration 230, loss = 0.04105664789676666
iteration 231, loss = 0.03691759705543518
iteration 232, loss = 0.03752735257148743
iteration 233, loss = 0.036319926381111145
iteration 234, loss = 0.04929058998823166
iteration 235, loss = 0.03771171718835831
iteration 236, loss = 0.037784554064273834
iteration 237, loss = 0.04755741357803345
iteration 238, loss = 0.04072100296616554
iteration 239, loss = 0.0448375903069973
iteration 240, loss = 0.035871703177690506
iteration 241, loss = 0.036618467420339584
iteration 242, loss = 0.036219511181116104
iteration 243, loss = 0.035384926944971085
iteration 244, loss = 0.04381608963012695
iteration 245, loss = 0.04059484973549843
iteration 246, loss = 0.03848867118358612
iteration 247, loss = 0.048424188047647476
iteration 248, loss = 0.03940548747777939
iteration 249, loss = 0.03898642584681511
iteration 250, loss = 0.03952852264046669
iteration 251, loss = 0.036667585372924805
iteration 252, loss = 0.058081138879060745
iteration 253, loss = 0.035176921635866165
iteration 254, loss = 0.03852907195687294
iteration 255, loss = 0.03442254662513733
iteration 256, loss = 0.0358855314552784
iteration 257, loss = 0.03320755809545517
iteration 258, loss = 0.04420603811740875
iteration 259, loss = 0.03888535127043724
iteration 260, loss = 0.03955017402768135
iteration 261, loss = 0.03664957359433174
iteration 262, loss = 0.03954046964645386
iteration 263, loss = 0.049582697451114655
iteration 264, loss = 0.03874416649341583
iteration 265, loss = 0.0576658695936203
iteration 266, loss = 0.03483138233423233
iteration 267, loss = 0.03513707220554352
iteration 268, loss = 0.03638637438416481
iteration 269, loss = 0.04391203820705414
iteration 270, loss = 0.03686729818582535
iteration 271, loss = 0.045559681951999664
iteration 272, loss = 0.03577808290719986
iteration 273, loss = 0.04871783405542374
iteration 274, loss = 0.06824389100074768
iteration 275, loss = 0.037461765110492706
iteration 276, loss = 0.04108445346355438
iteration 277, loss = 0.055783674120903015
iteration 278, loss = 0.03434111177921295
iteration 279, loss = 0.04800677299499512
iteration 280, loss = 0.05914478003978729
iteration 281, loss = 0.04511129856109619
iteration 282, loss = 0.034032195806503296
iteration 283, loss = 0.03674491494894028
iteration 284, loss = 0.03651037439703941
iteration 285, loss = 0.03638265281915665
iteration 286, loss = 0.03378293663263321
iteration 287, loss = 0.05079201981425285
iteration 288, loss = 0.03406321629881859
iteration 289, loss = 0.03464697301387787
iteration 290, loss = 0.05265703424811363
iteration 291, loss = 0.033532094210386276
iteration 292, loss = 0.034973278641700745
iteration 293, loss = 0.03829565644264221
iteration 294, loss = 0.03441353142261505
iteration 295, loss = 0.03504496067762375
iteration 296, loss = 0.034353747963905334
iteration 297, loss = 0.03994046151638031
iteration 298, loss = 0.036294788122177124
iteration 299, loss = 0.03254104033112526
iteration 300, loss = 0.03690287098288536
iteration 1, loss = 0.04858546704053879
iteration 2, loss = 0.03532654792070389
iteration 3, loss = 0.03615507110953331
iteration 4, loss = 0.03665135055780411
iteration 5, loss = 0.037375617772340775
iteration 6, loss = 0.055008839815855026
iteration 7, loss = 0.03163555637001991
iteration 8, loss = 0.034581754356622696
iteration 9, loss = 0.03397455811500549
iteration 10, loss = 0.03015359304845333
iteration 11, loss = 0.0327632874250412
iteration 12, loss = 0.03321687504649162
iteration 13, loss = 0.0405346117913723
iteration 14, loss = 0.03174996376037598
iteration 15, loss = 0.03605491667985916
iteration 16, loss = 0.03331262990832329
iteration 17, loss = 0.04718809947371483
iteration 18, loss = 0.03262031078338623
iteration 19, loss = 0.030474917963147163
iteration 20, loss = 0.05384165793657303
iteration 21, loss = 0.0411100760102272
iteration 22, loss = 0.04024535417556763
iteration 23, loss = 0.03641969710588455
iteration 24, loss = 0.04683937877416611
iteration 25, loss = 0.03364355117082596
iteration 26, loss = 0.031637243926525116
iteration 27, loss = 0.033296164125204086
iteration 28, loss = 0.03217145428061485
iteration 29, loss = 0.033367037773132324
iteration 30, loss = 0.03548399731516838
iteration 31, loss = 0.030062109231948853
iteration 32, loss = 0.029818056151270866
iteration 33, loss = 0.03954112529754639
iteration 34, loss = 0.03031943365931511
iteration 35, loss = 0.031150784343481064
iteration 36, loss = 0.030879253521561623
iteration 37, loss = 0.031386662274599075
iteration 38, loss = 0.03381292149424553
iteration 39, loss = 0.029079880565404892
iteration 40, loss = 0.030672674998641014
iteration 41, loss = 0.03241662308573723
iteration 42, loss = 0.031234316527843475
iteration 43, loss = 0.03940892219543457
iteration 44, loss = 0.030558578670024872
iteration 45, loss = 0.03818859905004501
iteration 46, loss = 0.032407741993665695
iteration 47, loss = 0.04106190428137779
iteration 48, loss = 0.034948281943798065
iteration 49, loss = 0.03508893772959709
iteration 50, loss = 0.04093809053301811
iteration 51, loss = 0.03054983541369438
iteration 52, loss = 0.029971551150083542
iteration 53, loss = 0.030185429379343987
iteration 54, loss = 0.03838016092777252
iteration 55, loss = 0.03736427053809166
iteration 56, loss = 0.029530642554163933
iteration 57, loss = 0.030823292210698128
iteration 58, loss = 0.030717873945832253
iteration 59, loss = 0.030799834057688713
iteration 60, loss = 0.03393259271979332
iteration 61, loss = 0.02895391173660755
iteration 62, loss = 0.029242880642414093
iteration 63, loss = 0.02985248900949955
iteration 64, loss = 0.032809142023324966
iteration 65, loss = 0.03342856094241142
iteration 66, loss = 0.03518138825893402
iteration 67, loss = 0.028954895213246346
iteration 68, loss = 0.037796054035425186
iteration 69, loss = 0.043156519532203674
iteration 70, loss = 0.028027651831507683
iteration 71, loss = 0.02985200844705105
iteration 72, loss = 0.0404757484793663
iteration 73, loss = 0.027785170823335648
iteration 74, loss = 0.030684944242239
iteration 75, loss = 0.03032238781452179
iteration 76, loss = 0.027095148339867592
iteration 77, loss = 0.035144198685884476
iteration 78, loss = 0.027477987110614777
iteration 79, loss = 0.0379093699157238
iteration 80, loss = 0.02751961350440979
iteration 81, loss = 0.040903836488723755
iteration 82, loss = 0.02834438532590866
iteration 83, loss = 0.027218922972679138
iteration 84, loss = 0.037075623869895935
iteration 85, loss = 0.03004356659948826
iteration 86, loss = 0.03232470899820328
iteration 87, loss = 0.029586147516965866
iteration 88, loss = 0.0404382199048996
iteration 89, loss = 0.02620697394013405
iteration 90, loss = 0.037539366632699966
iteration 91, loss = 0.052204232662916183
iteration 92, loss = 0.026882905513048172
iteration 93, loss = 0.03596850112080574
iteration 94, loss = 0.030164439231157303
iteration 95, loss = 0.028063790872693062
iteration 96, loss = 0.027055414393544197
iteration 97, loss = 0.027160227298736572
iteration 98, loss = 0.03518185392022133
iteration 99, loss = 0.027390699833631516
iteration 100, loss = 0.02741059474647045
iteration 101, loss = 0.033179912716150284
iteration 102, loss = 0.030662938952445984
iteration 103, loss = 0.041590366512537
iteration 104, loss = 0.02667415514588356
iteration 105, loss = 0.030505944043397903
iteration 106, loss = 0.028332460671663284
iteration 107, loss = 0.03648731857538223
iteration 108, loss = 0.02931264229118824
iteration 109, loss = 0.030402354896068573
iteration 110, loss = 0.028477836400270462
iteration 111, loss = 0.030563686043024063
iteration 112, loss = 0.026829954236745834
iteration 113, loss = 0.030452005565166473
iteration 114, loss = 0.030377419665455818
iteration 115, loss = 0.03503051772713661
iteration 116, loss = 0.026541752740740776
iteration 117, loss = 0.030484464019536972
iteration 118, loss = 0.026350537315011024
iteration 119, loss = 0.027305196970701218
iteration 120, loss = 0.027882147580385208
iteration 121, loss = 0.0262399110943079
iteration 122, loss = 0.02851964719593525
iteration 123, loss = 0.027226489037275314
iteration 124, loss = 0.03721068426966667
iteration 125, loss = 0.03140261024236679
iteration 126, loss = 0.029299819841980934
iteration 127, loss = 0.027721818536520004
iteration 128, loss = 0.03566305711865425
iteration 129, loss = 0.026726968586444855
iteration 130, loss = 0.04062316194176674
iteration 131, loss = 0.02936195768415928
iteration 132, loss = 0.024363266304135323
iteration 133, loss = 0.029025660827755928
iteration 134, loss = 0.02559356763958931
iteration 135, loss = 0.02925032190978527
iteration 136, loss = 0.024523528292775154
iteration 137, loss = 0.03262629359960556
iteration 138, loss = 0.03484455868601799
iteration 139, loss = 0.029190275818109512
iteration 140, loss = 0.026210078969597816
iteration 141, loss = 0.02809816226363182
iteration 142, loss = 0.034173112362623215
iteration 143, loss = 0.027393510565161705
iteration 144, loss = 0.028578314930200577
iteration 145, loss = 0.028312213718891144
iteration 146, loss = 0.026682130992412567
iteration 147, loss = 0.023417677730321884
iteration 148, loss = 0.0252741277217865
iteration 149, loss = 0.023824844509363174
iteration 150, loss = 0.03085535205900669
iteration 151, loss = 0.025993425399065018
iteration 152, loss = 0.02578394114971161
iteration 153, loss = 0.027176978066563606
iteration 154, loss = 0.025540271773934364
iteration 155, loss = 0.033826664090156555
iteration 156, loss = 0.025609774515032768
iteration 157, loss = 0.02774629555642605
iteration 158, loss = 0.026470599696040154
iteration 159, loss = 0.03407701104879379
iteration 160, loss = 0.03220463544130325
iteration 161, loss = 0.02788388542830944
iteration 162, loss = 0.032723601907491684
iteration 163, loss = 0.024223273620009422
iteration 164, loss = 0.04217660054564476
iteration 165, loss = 0.03397224843502045
iteration 166, loss = 0.027167385444045067
iteration 167, loss = 0.0259750634431839
iteration 168, loss = 0.026220550760626793
iteration 169, loss = 0.024656299501657486
iteration 170, loss = 0.033284395933151245
iteration 171, loss = 0.02482779510319233
iteration 172, loss = 0.02506617084145546
iteration 173, loss = 0.027339523658156395
iteration 174, loss = 0.026700209826231003
iteration 175, loss = 0.026691578328609467
iteration 176, loss = 0.02347748912870884
iteration 177, loss = 0.03535079583525658
iteration 178, loss = 0.02381959930062294
iteration 179, loss = 0.03703322261571884
iteration 180, loss = 0.030910419300198555
iteration 181, loss = 0.031564656645059586
iteration 182, loss = 0.035154446959495544
iteration 183, loss = 0.02392086759209633
iteration 184, loss = 0.030529016628861427
iteration 185, loss = 0.025427309796214104
iteration 186, loss = 0.03561803698539734
iteration 187, loss = 0.022737162187695503
iteration 188, loss = 0.03140456974506378
iteration 189, loss = 0.032174304127693176
iteration 190, loss = 0.02433205023407936
iteration 191, loss = 0.033093810081481934
iteration 192, loss = 0.02246338687837124
iteration 193, loss = 0.03074587509036064
iteration 194, loss = 0.02236323244869709
iteration 195, loss = 0.023422636091709137
iteration 196, loss = 0.03127707540988922
iteration 197, loss = 0.022748403251171112
iteration 198, loss = 0.027728989720344543
iteration 199, loss = 0.024857277050614357
iteration 200, loss = 0.02479524165391922
iteration 201, loss = 0.023734312504529953
iteration 202, loss = 0.023427745327353477
iteration 203, loss = 0.02586410939693451
iteration 204, loss = 0.026093266904354095
iteration 205, loss = 0.024068735539913177
iteration 206, loss = 0.022473832592368126
iteration 207, loss = 0.024499207735061646
iteration 208, loss = 0.03127802535891533
iteration 209, loss = 0.02733685076236725
iteration 210, loss = 0.02187301404774189
iteration 211, loss = 0.022176649421453476
iteration 212, loss = 0.02326815016567707
iteration 213, loss = 0.026343559846282005
iteration 214, loss = 0.031157158315181732
iteration 215, loss = 0.02313106320798397
iteration 216, loss = 0.02422052063047886
iteration 217, loss = 0.021577708423137665
iteration 218, loss = 0.028786800801753998
iteration 219, loss = 0.02617042325437069
iteration 220, loss = 0.030660782009363174
iteration 221, loss = 0.02358587272465229
iteration 222, loss = 0.029422931373119354
iteration 223, loss = 0.023876039311289787
iteration 224, loss = 0.02302548848092556
iteration 225, loss = 0.022177191451191902
iteration 226, loss = 0.021724548190832138
iteration 227, loss = 0.020858874544501305
iteration 228, loss = 0.031295936554670334
iteration 229, loss = 0.02873947098851204
iteration 230, loss = 0.02596755139529705
iteration 231, loss = 0.0258683729916811
iteration 232, loss = 0.02782042883336544
iteration 233, loss = 0.021914325654506683
iteration 234, loss = 0.03317912295460701
iteration 235, loss = 0.021553393453359604
iteration 236, loss = 0.022756602615118027
iteration 237, loss = 0.0206379946321249
iteration 238, loss = 0.03462802618741989
iteration 239, loss = 0.026713628321886063
iteration 240, loss = 0.025684136897325516
iteration 241, loss = 0.02112017571926117
iteration 242, loss = 0.025961223989725113
iteration 243, loss = 0.022622426971793175
iteration 244, loss = 0.027242014184594154
iteration 245, loss = 0.029681503772735596
iteration 246, loss = 0.03093753196299076
iteration 247, loss = 0.0285720806568861
iteration 248, loss = 0.027352720499038696
iteration 249, loss = 0.02798248454928398
iteration 250, loss = 0.022186104208230972
iteration 251, loss = 0.020326759666204453
iteration 252, loss = 0.02085128426551819
iteration 253, loss = 0.02430410124361515
iteration 254, loss = 0.022405048832297325
iteration 255, loss = 0.023787330836057663
iteration 256, loss = 0.024481864646077156
iteration 257, loss = 0.020656857639551163
iteration 258, loss = 0.02063930220901966
iteration 259, loss = 0.019639335572719574
iteration 260, loss = 0.02079293131828308
iteration 261, loss = 0.02056700736284256
iteration 262, loss = 0.026715142652392387
iteration 263, loss = 0.023859484121203423
iteration 264, loss = 0.022364497184753418
iteration 265, loss = 0.0354979932308197
iteration 266, loss = 0.024964310228824615
iteration 267, loss = 0.020998897030949593
iteration 268, loss = 0.023922262713313103
iteration 269, loss = 0.02437800168991089
iteration 270, loss = 0.019895099103450775
iteration 271, loss = 0.022425919771194458
iteration 272, loss = 0.0208856500685215
iteration 273, loss = 0.020680993795394897
iteration 274, loss = 0.022893520072102547
iteration 275, loss = 0.022964652627706528
iteration 276, loss = 0.02177574671804905
iteration 277, loss = 0.023244470357894897
iteration 278, loss = 0.026391301304101944
iteration 279, loss = 0.02028231881558895
iteration 280, loss = 0.021395182237029076
iteration 281, loss = 0.026035752147436142
iteration 282, loss = 0.024258887395262718
iteration 283, loss = 0.020386237651109695
iteration 284, loss = 0.024657614529132843
iteration 285, loss = 0.020940322428941727
iteration 286, loss = 0.02079131454229355
iteration 287, loss = 0.02970820665359497
iteration 288, loss = 0.02190570905804634
iteration 289, loss = 0.025859953835606575
iteration 290, loss = 0.022364173084497452
iteration 291, loss = 0.01878993771970272
iteration 292, loss = 0.019404448568820953
iteration 293, loss = 0.019896985962986946
iteration 294, loss = 0.021450838074088097
iteration 295, loss = 0.03253352269530296
iteration 296, loss = 0.018861984834074974
iteration 297, loss = 0.02080683223903179
iteration 298, loss = 0.018827078863978386
iteration 299, loss = 0.02648690901696682
iteration 300, loss = 0.02179590053856373
iteration 1, loss = 0.020852483808994293
iteration 2, loss = 0.02178777940571308
iteration 3, loss = 0.019848283380270004
iteration 4, loss = 0.030523397028446198
iteration 5, loss = 0.024441728368401527
iteration 6, loss = 0.019044730812311172
iteration 7, loss = 0.024595757946372032
iteration 8, loss = 0.02552395686507225
iteration 9, loss = 0.02676941268146038
iteration 10, loss = 0.022503355517983437
iteration 11, loss = 0.01924721896648407
iteration 12, loss = 0.028531882911920547
iteration 13, loss = 0.020194513723254204
iteration 14, loss = 0.021075351163744926
iteration 15, loss = 0.024672571569681168
iteration 16, loss = 0.01894039288163185
iteration 17, loss = 0.019008370116353035
iteration 18, loss = 0.024732423946261406
iteration 19, loss = 0.02081895060837269
iteration 20, loss = 0.02021406590938568
iteration 21, loss = 0.023307546973228455
iteration 22, loss = 0.01905061863362789
iteration 23, loss = 0.030923064798116684
iteration 24, loss = 0.022107545286417007
iteration 25, loss = 0.018927771598100662
iteration 26, loss = 0.02008352056145668
iteration 27, loss = 0.018381526693701744
iteration 28, loss = 0.020810315385460854
iteration 29, loss = 0.01939305290579796
iteration 30, loss = 0.018957236781716347
iteration 31, loss = 0.02933952584862709
iteration 32, loss = 0.025857802480459213
iteration 33, loss = 0.02796832099556923
iteration 34, loss = 0.022130131721496582
iteration 35, loss = 0.02401157096028328
iteration 36, loss = 0.020115675404667854
iteration 37, loss = 0.021166153252124786
iteration 38, loss = 0.03368992358446121
iteration 39, loss = 0.019918546080589294
iteration 40, loss = 0.025622934103012085
iteration 41, loss = 0.020026590675115585
iteration 42, loss = 0.01973695680499077
iteration 43, loss = 0.020925451070070267
iteration 44, loss = 0.026940662413835526
iteration 45, loss = 0.019117681309580803
iteration 46, loss = 0.022508209571242332
iteration 47, loss = 0.024118546396493912
iteration 48, loss = 0.022359775379300117
iteration 49, loss = 0.020577754825353622
iteration 50, loss = 0.02601713128387928
iteration 51, loss = 0.02786092832684517
iteration 52, loss = 0.02596297860145569
iteration 53, loss = 0.019620472565293312
iteration 54, loss = 0.022815823554992676
iteration 55, loss = 0.01907513104379177
iteration 56, loss = 0.02040458843111992
iteration 57, loss = 0.021276302635669708
iteration 58, loss = 0.01962573267519474
iteration 59, loss = 0.020918812602758408
iteration 60, loss = 0.021508507430553436
iteration 61, loss = 0.03034401498734951
iteration 62, loss = 0.02072479948401451
iteration 63, loss = 0.019496092572808266
iteration 64, loss = 0.019742542877793312
iteration 65, loss = 0.020946023985743523
iteration 66, loss = 0.028257841244339943
iteration 67, loss = 0.021712470799684525
iteration 68, loss = 0.025367513298988342
iteration 69, loss = 0.019867857918143272
iteration 70, loss = 0.01842261664569378
iteration 71, loss = 0.02605563774704933
iteration 72, loss = 0.019626086577773094
iteration 73, loss = 0.0192366074770689
iteration 74, loss = 0.021187184378504753
iteration 75, loss = 0.019758928567171097
iteration 76, loss = 0.022816451266407967
iteration 77, loss = 0.021142510697245598
iteration 78, loss = 0.019839059561491013
iteration 79, loss = 0.022549577057361603
iteration 80, loss = 0.025360481813549995
iteration 81, loss = 0.019748356193304062
iteration 82, loss = 0.025814760476350784
iteration 83, loss = 0.018770448863506317
iteration 84, loss = 0.019065089523792267
iteration 85, loss = 0.021901201456785202
iteration 86, loss = 0.021650686860084534
iteration 87, loss = 0.019444994628429413
iteration 88, loss = 0.022686433047056198
iteration 89, loss = 0.01990770734846592
iteration 90, loss = 0.01938982680439949
iteration 91, loss = 0.021768834441900253
iteration 92, loss = 0.019850153475999832
iteration 93, loss = 0.02153898775577545
iteration 94, loss = 0.020242705941200256
iteration 95, loss = 0.01944076083600521
iteration 96, loss = 0.021989135071635246
iteration 97, loss = 0.02040378376841545
iteration 98, loss = 0.01829008013010025
iteration 99, loss = 0.029577147215604782
iteration 100, loss = 0.019264113157987595
iteration 101, loss = 0.024722246453166008
iteration 102, loss = 0.020492520183324814
iteration 103, loss = 0.020624563097953796
iteration 104, loss = 0.018146716058254242
iteration 105, loss = 0.01820516586303711
iteration 106, loss = 0.020092949271202087
iteration 107, loss = 0.020511146634817123
iteration 108, loss = 0.021198349073529243
iteration 109, loss = 0.03636930510401726
iteration 110, loss = 0.020340600982308388
iteration 111, loss = 0.027048829942941666
iteration 112, loss = 0.025181900709867477
iteration 113, loss = 0.021038714796304703
iteration 114, loss = 0.02026325836777687
iteration 115, loss = 0.028233304619789124
iteration 116, loss = 0.027501417323946953
iteration 117, loss = 0.01962059922516346
iteration 118, loss = 0.023120000958442688
iteration 119, loss = 0.0192734245210886
iteration 120, loss = 0.01865581050515175
iteration 121, loss = 0.021612124517560005
iteration 122, loss = 0.026402976363897324
iteration 123, loss = 0.02499423362314701
iteration 124, loss = 0.02075018920004368
iteration 125, loss = 0.023007187992334366
iteration 126, loss = 0.019182026386260986
iteration 127, loss = 0.027976732701063156
iteration 128, loss = 0.018863528966903687
iteration 129, loss = 0.019397731870412827
iteration 130, loss = 0.02554827369749546
iteration 131, loss = 0.02092892676591873
iteration 132, loss = 0.03316403925418854
iteration 133, loss = 0.022568557411432266
iteration 134, loss = 0.020477071404457092
iteration 135, loss = 0.020655129104852676
iteration 136, loss = 0.021419299766421318
iteration 137, loss = 0.02092679962515831
iteration 138, loss = 0.019905611872673035
iteration 139, loss = 0.026975812390446663
iteration 140, loss = 0.027793342247605324
iteration 141, loss = 0.02693808451294899
iteration 142, loss = 0.027288587763905525
iteration 143, loss = 0.021364104002714157
iteration 144, loss = 0.019114412367343903
iteration 145, loss = 0.02046496421098709
iteration 146, loss = 0.02052280679345131
iteration 147, loss = 0.02147897705435753
iteration 148, loss = 0.025357544422149658
iteration 149, loss = 0.019433487206697464
iteration 150, loss = 0.019358951598405838
iteration 151, loss = 0.02026449330151081
iteration 152, loss = 0.01892857439815998
iteration 153, loss = 0.019444622099399567
iteration 154, loss = 0.0183628611266613
iteration 155, loss = 0.02498527243733406
iteration 156, loss = 0.021478669717907906
iteration 157, loss = 0.021589843556284904
iteration 158, loss = 0.019485190510749817
iteration 159, loss = 0.022505290806293488
iteration 160, loss = 0.02718794159591198
iteration 161, loss = 0.020627731457352638
iteration 162, loss = 0.025366848334670067
iteration 163, loss = 0.02719784528017044
iteration 164, loss = 0.02605007402598858
iteration 165, loss = 0.020706940442323685
iteration 166, loss = 0.025110041722655296
iteration 167, loss = 0.025292737409472466
iteration 168, loss = 0.021095791831612587
iteration 169, loss = 0.026686951518058777
iteration 170, loss = 0.021592888981103897
iteration 171, loss = 0.019772473722696304
iteration 172, loss = 0.020110713317990303
iteration 173, loss = 0.029489677399396896
iteration 174, loss = 0.018285732716321945
iteration 175, loss = 0.022947607561945915
iteration 176, loss = 0.019126219674944878
iteration 177, loss = 0.019123224541544914
iteration 178, loss = 0.02233349159359932
iteration 179, loss = 0.022689281031489372
iteration 180, loss = 0.02087348699569702
iteration 181, loss = 0.0290333554148674
iteration 182, loss = 0.02464347332715988
iteration 183, loss = 0.019276408478617668
iteration 184, loss = 0.022887591272592545
iteration 185, loss = 0.019707076251506805
iteration 186, loss = 0.019945843145251274
iteration 187, loss = 0.032279688864946365
iteration 188, loss = 0.023196175694465637
iteration 189, loss = 0.021119266748428345
iteration 190, loss = 0.020380228757858276
iteration 191, loss = 0.019963763654232025
iteration 192, loss = 0.02572295442223549
iteration 193, loss = 0.01836489513516426
iteration 194, loss = 0.022632038220763206
iteration 195, loss = 0.02031988464295864
iteration 196, loss = 0.01822698302567005
iteration 197, loss = 0.019826442003250122
iteration 198, loss = 0.019708920270204544
iteration 199, loss = 0.023373138159513474
iteration 200, loss = 0.0194146279245615
iteration 201, loss = 0.02051728218793869
iteration 202, loss = 0.025026991963386536
iteration 203, loss = 0.0183658879250288
iteration 204, loss = 0.023894239217042923
iteration 205, loss = 0.022918468341231346
iteration 206, loss = 0.02450866438448429
iteration 207, loss = 0.025225268676877022
iteration 208, loss = 0.01970844343304634
iteration 209, loss = 0.028730280697345734
iteration 210, loss = 0.01849101297557354
iteration 211, loss = 0.017839254811406136
iteration 212, loss = 0.019454559311270714
iteration 213, loss = 0.020440084859728813
iteration 214, loss = 0.025501245632767677
iteration 215, loss = 0.01990531198680401
iteration 216, loss = 0.01819622330367565
iteration 217, loss = 0.019535567611455917
iteration 218, loss = 0.02428324893116951
iteration 219, loss = 0.020786577835679054
iteration 220, loss = 0.019020752981305122
iteration 221, loss = 0.021522199735045433
iteration 222, loss = 0.024925552308559418
iteration 223, loss = 0.02021983079612255
iteration 224, loss = 0.019398920238018036
iteration 225, loss = 0.01976785436272621
iteration 226, loss = 0.018910076469182968
iteration 227, loss = 0.020267076790332794
iteration 228, loss = 0.018630027770996094
iteration 229, loss = 0.021165931597352028
iteration 230, loss = 0.024023286998271942
iteration 231, loss = 0.022100159898400307
iteration 232, loss = 0.019218510016798973
iteration 233, loss = 0.0200178325176239
iteration 234, loss = 0.019234241917729378
iteration 235, loss = 0.017922325059771538
iteration 236, loss = 0.022495146840810776
iteration 237, loss = 0.02059190720319748
iteration 238, loss = 0.02568637952208519
iteration 239, loss = 0.020704319700598717
iteration 240, loss = 0.02449125424027443
iteration 241, loss = 0.024351760745048523
iteration 242, loss = 0.024860987439751625
iteration 243, loss = 0.021192671731114388
iteration 244, loss = 0.019885117188096046
iteration 245, loss = 0.02382875420153141
iteration 246, loss = 0.019987717270851135
iteration 247, loss = 0.02097821980714798
iteration 248, loss = 0.020282335579395294
iteration 249, loss = 0.024901434779167175
iteration 250, loss = 0.023379091173410416
iteration 251, loss = 0.01952802576124668
iteration 252, loss = 0.026583556085824966
iteration 253, loss = 0.020277734845876694
iteration 254, loss = 0.017746668308973312
iteration 255, loss = 0.019239261746406555
iteration 256, loss = 0.018632151186466217
iteration 257, loss = 0.019691605120897293
iteration 258, loss = 0.031340066343545914
iteration 259, loss = 0.019985556602478027
iteration 260, loss = 0.02160402201116085
iteration 261, loss = 0.021209904924035072
iteration 262, loss = 0.018802639096975327
iteration 263, loss = 0.026510361582040787
iteration 264, loss = 0.024027761071920395
iteration 265, loss = 0.019276639446616173
iteration 266, loss = 0.018418611958622932
iteration 267, loss = 0.027445945888757706
iteration 268, loss = 0.030603863298892975
iteration 269, loss = 0.029383137822151184
iteration 270, loss = 0.022152753546833992
iteration 271, loss = 0.018612006679177284
iteration 272, loss = 0.02418048493564129
iteration 273, loss = 0.019303204491734505
iteration 274, loss = 0.02623538114130497
iteration 275, loss = 0.019974026829004288
iteration 276, loss = 0.02558768168091774
iteration 277, loss = 0.018939198926091194
iteration 278, loss = 0.02372603863477707
iteration 279, loss = 0.01872863806784153
iteration 280, loss = 0.019910989329218864
iteration 281, loss = 0.021072855219244957
iteration 282, loss = 0.020967207849025726
iteration 283, loss = 0.019279953092336655
iteration 284, loss = 0.020256483927369118
iteration 285, loss = 0.018075766041874886
iteration 286, loss = 0.030472438782453537
iteration 287, loss = 0.020559577271342278
iteration 288, loss = 0.018594812601804733
iteration 289, loss = 0.01921853795647621
iteration 290, loss = 0.02485404722392559
iteration 291, loss = 0.019883792847394943
iteration 292, loss = 0.01859656162559986
iteration 293, loss = 0.019218210130929947
iteration 294, loss = 0.027155539020895958
iteration 295, loss = 0.019434912130236626
iteration 296, loss = 0.022574404254555702
iteration 297, loss = 0.021550806239247322
iteration 298, loss = 0.019091453403234482
iteration 299, loss = 0.023914698511362076
iteration 300, loss = 0.02246216870844364
iteration 1, loss = 0.018063247203826904
iteration 2, loss = 0.018946468830108643
iteration 3, loss = 0.025892021134495735
iteration 4, loss = 0.021009769290685654
iteration 5, loss = 0.020248018205165863
iteration 6, loss = 0.02027045749127865
iteration 7, loss = 0.01911550760269165
iteration 8, loss = 0.028141921386122704
iteration 9, loss = 0.025320211425423622
iteration 10, loss = 0.01970754563808441
iteration 11, loss = 0.023558391258120537
iteration 12, loss = 0.017767921090126038
iteration 13, loss = 0.01986601948738098
iteration 14, loss = 0.033186379820108414
iteration 15, loss = 0.018601009622216225
iteration 16, loss = 0.01943996362388134
iteration 17, loss = 0.025518277660012245
iteration 18, loss = 0.020240049809217453
iteration 19, loss = 0.021870827302336693
iteration 20, loss = 0.019668439403176308
iteration 21, loss = 0.020844630897045135
iteration 22, loss = 0.020042987540364265
iteration 23, loss = 0.01936955563724041
iteration 24, loss = 0.019190816208720207
iteration 25, loss = 0.019663792103528976
iteration 26, loss = 0.018645508214831352
iteration 27, loss = 0.018389951437711716
iteration 28, loss = 0.02067466638982296
iteration 29, loss = 0.023702390491962433
iteration 30, loss = 0.0255995262414217
iteration 31, loss = 0.024323174729943275
iteration 32, loss = 0.020542509853839874
iteration 33, loss = 0.01923285983502865
iteration 34, loss = 0.021354366093873978
iteration 35, loss = 0.020007705315947533
iteration 36, loss = 0.024415595456957817
iteration 37, loss = 0.017955271527171135
iteration 38, loss = 0.01863679848611355
iteration 39, loss = 0.02339622750878334
iteration 40, loss = 0.024414759129285812
iteration 41, loss = 0.023221515119075775
iteration 42, loss = 0.018812617287039757
iteration 43, loss = 0.03181163966655731
iteration 44, loss = 0.019624892622232437
iteration 45, loss = 0.022269241511821747
iteration 46, loss = 0.03111081011593342
iteration 47, loss = 0.019829321652650833
iteration 48, loss = 0.02030167728662491
iteration 49, loss = 0.01919509842991829
iteration 50, loss = 0.02192884311079979
iteration 51, loss = 0.021438391879200935
iteration 52, loss = 0.019525358453392982
iteration 53, loss = 0.018748285248875618
iteration 54, loss = 0.01792168617248535
iteration 55, loss = 0.024491559714078903
iteration 56, loss = 0.02665923908352852
iteration 57, loss = 0.024239666759967804
iteration 58, loss = 0.017959890887141228
iteration 59, loss = 0.03062289208173752
iteration 60, loss = 0.025501135736703873
iteration 61, loss = 0.018130188807845116
iteration 62, loss = 0.027463670819997787
iteration 63, loss = 0.018606439232826233
iteration 64, loss = 0.02220430225133896
iteration 65, loss = 0.021321499720215797
iteration 66, loss = 0.028426337987184525
iteration 67, loss = 0.017451265826821327
iteration 68, loss = 0.02046941965818405
iteration 69, loss = 0.020350804552435875
iteration 70, loss = 0.021654335781931877
iteration 71, loss = 0.020284129306674004
iteration 72, loss = 0.018710460513830185
iteration 73, loss = 0.02416994795203209
iteration 74, loss = 0.019182901829481125
iteration 75, loss = 0.019365334883332253
iteration 76, loss = 0.03252384066581726
iteration 77, loss = 0.018265405669808388
iteration 78, loss = 0.018863677978515625
iteration 79, loss = 0.019230416044592857
iteration 80, loss = 0.01953090727329254
iteration 81, loss = 0.019318457692861557
iteration 82, loss = 0.018660157918930054
iteration 83, loss = 0.018890589475631714
iteration 84, loss = 0.019088231027126312
iteration 85, loss = 0.01870470494031906
iteration 86, loss = 0.01953701674938202
iteration 87, loss = 0.019796572625637054
iteration 88, loss = 0.02411983720958233
iteration 89, loss = 0.027804022654891014
iteration 90, loss = 0.025507116690278053
iteration 91, loss = 0.025195371359586716
iteration 92, loss = 0.020429635420441628
iteration 93, loss = 0.01970355212688446
iteration 94, loss = 0.024368319660425186
iteration 95, loss = 0.018392285332083702
iteration 96, loss = 0.026446444913744926
iteration 97, loss = 0.02442830055952072
iteration 98, loss = 0.022659718990325928
iteration 99, loss = 0.018693335354328156
iteration 100, loss = 0.026370208710432053
iteration 101, loss = 0.018809933215379715
iteration 102, loss = 0.02101653628051281
iteration 103, loss = 0.019398009404540062
iteration 104, loss = 0.018849538639187813
iteration 105, loss = 0.019730219617486
iteration 106, loss = 0.02288651466369629
iteration 107, loss = 0.01979707181453705
iteration 108, loss = 0.01904369704425335
iteration 109, loss = 0.022471707314252853
iteration 110, loss = 0.03394722566008568
iteration 111, loss = 0.02428174763917923
iteration 112, loss = 0.01820804551243782
iteration 113, loss = 0.01838935911655426
iteration 114, loss = 0.024833891540765762
iteration 115, loss = 0.01909055933356285
iteration 116, loss = 0.02035750076174736
iteration 117, loss = 0.01988605037331581
iteration 118, loss = 0.018753213807940483
iteration 119, loss = 0.01943645067512989
iteration 120, loss = 0.018515419214963913
iteration 121, loss = 0.018384035676717758
iteration 122, loss = 0.018327292054891586
iteration 123, loss = 0.019294440746307373
iteration 124, loss = 0.019917411729693413
iteration 125, loss = 0.02417057380080223
iteration 126, loss = 0.019081488251686096
iteration 127, loss = 0.020294198766350746
iteration 128, loss = 0.01957215555012226
iteration 129, loss = 0.019187025725841522
iteration 130, loss = 0.027096329256892204
iteration 131, loss = 0.023582613095641136
iteration 132, loss = 0.01930101029574871
iteration 133, loss = 0.017734531313180923
iteration 134, loss = 0.02651010826230049
iteration 135, loss = 0.018907105550169945
iteration 136, loss = 0.024141579866409302
iteration 137, loss = 0.019655292853713036
iteration 138, loss = 0.02064373530447483
iteration 139, loss = 0.019970038905739784
iteration 140, loss = 0.019416674971580505
iteration 141, loss = 0.01804283633828163
iteration 142, loss = 0.019778303802013397
iteration 143, loss = 0.028813056647777557
iteration 144, loss = 0.025287315249443054
iteration 145, loss = 0.02121034637093544
iteration 146, loss = 0.019980598241090775
iteration 147, loss = 0.018149565905332565
iteration 148, loss = 0.019433891400694847
iteration 149, loss = 0.02035915106534958
iteration 150, loss = 0.01818033680319786
iteration 151, loss = 0.024053506553173065
iteration 152, loss = 0.025821462273597717
iteration 153, loss = 0.017305713146924973
iteration 154, loss = 0.01911948248744011
iteration 155, loss = 0.035260505974292755
iteration 156, loss = 0.0288905818015337
iteration 157, loss = 0.023049181327223778
iteration 158, loss = 0.02099090814590454
iteration 159, loss = 0.018754027783870697
iteration 160, loss = 0.021686162799596786
iteration 161, loss = 0.018593620508909225
iteration 162, loss = 0.017425328493118286
iteration 163, loss = 0.01857566460967064
iteration 164, loss = 0.017324911430478096
iteration 165, loss = 0.01906237006187439
iteration 166, loss = 0.031173240393400192
iteration 167, loss = 0.017732027918100357
iteration 168, loss = 0.020549487322568893
iteration 169, loss = 0.020145101472735405
iteration 170, loss = 0.020001471042633057
iteration 171, loss = 0.018019577488303185
iteration 172, loss = 0.024831021204590797
iteration 173, loss = 0.018608588725328445
iteration 174, loss = 0.019182564690709114
iteration 175, loss = 0.02274327538907528
iteration 176, loss = 0.018459001556038857
iteration 177, loss = 0.017738688737154007
iteration 178, loss = 0.023806214332580566
iteration 179, loss = 0.019758110865950584
iteration 180, loss = 0.01995711401104927
iteration 181, loss = 0.027241438627243042
iteration 182, loss = 0.02367500774562359
iteration 183, loss = 0.017352581024169922
iteration 184, loss = 0.02125510200858116
iteration 185, loss = 0.02020258642733097
iteration 186, loss = 0.024400195106863976
iteration 187, loss = 0.0176591407507658
iteration 188, loss = 0.028230026364326477
iteration 189, loss = 0.018760595470666885
iteration 190, loss = 0.024822119623422623
iteration 191, loss = 0.01941210776567459
iteration 192, loss = 0.024205531924962997
iteration 193, loss = 0.018039802089333534
iteration 194, loss = 0.017661994323134422
iteration 195, loss = 0.025387678295373917
iteration 196, loss = 0.0263698548078537
iteration 197, loss = 0.021631374955177307
iteration 198, loss = 0.024017641320824623
iteration 199, loss = 0.0199943445622921
iteration 200, loss = 0.024701794609427452
iteration 201, loss = 0.020651519298553467
iteration 202, loss = 0.02887108363211155
iteration 203, loss = 0.018215136602520943
iteration 204, loss = 0.01860997825860977
iteration 205, loss = 0.01780104450881481
iteration 206, loss = 0.02134001813828945
iteration 207, loss = 0.01824105717241764
iteration 208, loss = 0.021846413612365723
iteration 209, loss = 0.023450011387467384
iteration 210, loss = 0.017529593780636787
iteration 211, loss = 0.01787751168012619
iteration 212, loss = 0.02364993840456009
iteration 213, loss = 0.018390964716672897
iteration 214, loss = 0.01758977398276329
iteration 215, loss = 0.01857016421854496
iteration 216, loss = 0.021904606372117996
iteration 217, loss = 0.01955464296042919
iteration 218, loss = 0.019192088395357132
iteration 219, loss = 0.020698558539152145
iteration 220, loss = 0.018540795892477036
iteration 221, loss = 0.018163088709115982
iteration 222, loss = 0.020106812939047813
iteration 223, loss = 0.018674325197935104
iteration 224, loss = 0.019605625420808792
iteration 225, loss = 0.024099374189972878
iteration 226, loss = 0.020532844588160515
iteration 227, loss = 0.0195345226675272
iteration 228, loss = 0.017807859927415848
iteration 229, loss = 0.029358504340052605
iteration 230, loss = 0.0197199247777462
iteration 231, loss = 0.019472964107990265
iteration 232, loss = 0.02232912927865982
iteration 233, loss = 0.01812714897096157
iteration 234, loss = 0.02063077501952648
iteration 235, loss = 0.018512923270463943
iteration 236, loss = 0.01923397369682789
iteration 237, loss = 0.023786187171936035
iteration 238, loss = 0.020136691629886627
iteration 239, loss = 0.017935937270522118
iteration 240, loss = 0.020088983699679375
iteration 241, loss = 0.020447857677936554
iteration 242, loss = 0.01806212030351162
iteration 243, loss = 0.030024852603673935
iteration 244, loss = 0.019016200676560402
iteration 245, loss = 0.020575473085045815
iteration 246, loss = 0.020102687180042267
iteration 247, loss = 0.02581218257546425
iteration 248, loss = 0.023698557168245316
iteration 249, loss = 0.01969686709344387
iteration 250, loss = 0.02002163790166378
iteration 251, loss = 0.01925837993621826
iteration 252, loss = 0.027722785249352455
iteration 253, loss = 0.017883405089378357
iteration 254, loss = 0.018517855554819107
iteration 255, loss = 0.018884694203734398
iteration 256, loss = 0.018277151510119438
iteration 257, loss = 0.019138390198349953
iteration 258, loss = 0.018242724239826202
iteration 259, loss = 0.018084760755300522
iteration 260, loss = 0.018004324287176132
iteration 261, loss = 0.018221167847514153
iteration 262, loss = 0.02902057394385338
iteration 263, loss = 0.025777479633688927
iteration 264, loss = 0.018575018271803856
iteration 265, loss = 0.01942620612680912
iteration 266, loss = 0.022559698671102524
iteration 267, loss = 0.01749677024781704
iteration 268, loss = 0.02064279466867447
iteration 269, loss = 0.024938251823186874
iteration 270, loss = 0.03005402907729149
iteration 271, loss = 0.016924075782299042
iteration 272, loss = 0.021947268396615982
iteration 273, loss = 0.018441347405314445
iteration 274, loss = 0.02227654866874218
iteration 275, loss = 0.018344348296523094
iteration 276, loss = 0.019432466477155685
iteration 277, loss = 0.02521909959614277
iteration 278, loss = 0.018320847302675247
iteration 279, loss = 0.019286302849650383
iteration 280, loss = 0.017995938658714294
iteration 281, loss = 0.021634334698319435
iteration 282, loss = 0.018893606960773468
iteration 283, loss = 0.01712842471897602
iteration 284, loss = 0.017242366448044777
iteration 285, loss = 0.019152147695422173
iteration 286, loss = 0.018202727660536766
iteration 287, loss = 0.018837090581655502
iteration 288, loss = 0.019394628703594208
iteration 289, loss = 0.019035570323467255
iteration 290, loss = 0.024931486696004868
iteration 291, loss = 0.021859876811504364
iteration 292, loss = 0.01777591183781624
iteration 293, loss = 0.01973593421280384
iteration 294, loss = 0.021529562771320343
iteration 295, loss = 0.018740041181445122
iteration 296, loss = 0.023288138210773468
iteration 297, loss = 0.020403068512678146
iteration 298, loss = 0.023221122100949287
iteration 299, loss = 0.022800475358963013
iteration 300, loss = 0.02541944943368435
iteration 1, loss = 0.017682459205389023
iteration 2, loss = 0.018387960270047188
iteration 3, loss = 0.01929422654211521
iteration 4, loss = 0.029902568086981773
iteration 5, loss = 0.02117316797375679
iteration 6, loss = 0.017438624054193497
iteration 7, loss = 0.019802629947662354
iteration 8, loss = 0.01785367913544178
iteration 9, loss = 0.02598261833190918
iteration 10, loss = 0.01943768933415413
iteration 11, loss = 0.022073917090892792
iteration 12, loss = 0.01771015301346779
iteration 13, loss = 0.020228181034326553
iteration 14, loss = 0.01940842717885971
iteration 15, loss = 0.017963791266083717
iteration 16, loss = 0.025916971266269684
iteration 17, loss = 0.023139098659157753
iteration 18, loss = 0.02005484513938427
iteration 19, loss = 0.029233872890472412
iteration 20, loss = 0.017884578555822372
iteration 21, loss = 0.019079752266407013
iteration 22, loss = 0.02031475119292736
iteration 23, loss = 0.021021312102675438
iteration 24, loss = 0.018974024802446365
iteration 25, loss = 0.01880684494972229
iteration 26, loss = 0.024448474869132042
iteration 27, loss = 0.019941704347729683
iteration 28, loss = 0.017659859731793404
iteration 29, loss = 0.020193446427583694
iteration 30, loss = 0.021926313638687134
iteration 31, loss = 0.026045717298984528
iteration 32, loss = 0.017745718359947205
iteration 33, loss = 0.022174213081598282
iteration 34, loss = 0.022148340940475464
iteration 35, loss = 0.021013280376791954
iteration 36, loss = 0.018062924966216087
iteration 37, loss = 0.01924879662692547
iteration 38, loss = 0.028533905744552612
iteration 39, loss = 0.019167855381965637
iteration 40, loss = 0.023233678191900253
iteration 41, loss = 0.021508798003196716
iteration 42, loss = 0.02126416191458702
iteration 43, loss = 0.025885360315442085
iteration 44, loss = 0.031565017998218536
iteration 45, loss = 0.01906011253595352
iteration 46, loss = 0.017831072211265564
iteration 47, loss = 0.018275970593094826
iteration 48, loss = 0.019480334594845772
iteration 49, loss = 0.018753962591290474
iteration 50, loss = 0.018898598849773407
iteration 51, loss = 0.02524404041469097
iteration 52, loss = 0.022849852219223976
iteration 53, loss = 0.02358657494187355
iteration 54, loss = 0.018153786659240723
iteration 55, loss = 0.02209252491593361
iteration 56, loss = 0.022422200068831444
iteration 57, loss = 0.018431097269058228
iteration 58, loss = 0.018612293526530266
iteration 59, loss = 0.018126316368579865
iteration 60, loss = 0.022843390703201294
iteration 61, loss = 0.01791876181960106
iteration 62, loss = 0.018829485401511192
iteration 63, loss = 0.01780800148844719
iteration 64, loss = 0.017670726403594017
iteration 65, loss = 0.018988432362675667
iteration 66, loss = 0.02517680451273918
iteration 67, loss = 0.02122810110449791
iteration 68, loss = 0.019356127828359604
iteration 69, loss = 0.0223966296762228
iteration 70, loss = 0.022367045283317566
iteration 71, loss = 0.02488015964627266
iteration 72, loss = 0.01902889274060726
iteration 73, loss = 0.02316218428313732
iteration 74, loss = 0.028098441660404205
iteration 75, loss = 0.018076013773679733
iteration 76, loss = 0.018130134791135788
iteration 77, loss = 0.01879313960671425
iteration 78, loss = 0.02398848347365856
iteration 79, loss = 0.022201068699359894
iteration 80, loss = 0.018774131312966347
iteration 81, loss = 0.020281068980693817
iteration 82, loss = 0.018427729606628418
iteration 83, loss = 0.024950558319687843
iteration 84, loss = 0.02341631054878235
iteration 85, loss = 0.020579958334565163
iteration 86, loss = 0.017441770061850548
iteration 87, loss = 0.024380985647439957
iteration 88, loss = 0.019967351108789444
iteration 89, loss = 0.019247310236096382
iteration 90, loss = 0.019348472356796265
iteration 91, loss = 0.01785605028271675
iteration 92, loss = 0.01988670974969864
iteration 93, loss = 0.024435920640826225
iteration 94, loss = 0.01771034486591816
iteration 95, loss = 0.02002422697842121
iteration 96, loss = 0.019346822053194046
iteration 97, loss = 0.01769060082733631
iteration 98, loss = 0.019684461876749992
iteration 99, loss = 0.0175456702709198
iteration 100, loss = 0.017431646585464478
iteration 101, loss = 0.029514476656913757
iteration 102, loss = 0.019614538177847862
iteration 103, loss = 0.022009246051311493
iteration 104, loss = 0.01893964409828186
iteration 105, loss = 0.018323397263884544
iteration 106, loss = 0.017110105603933334
iteration 107, loss = 0.022225528955459595
iteration 108, loss = 0.023056721314787865
iteration 109, loss = 0.017067868262529373
iteration 110, loss = 0.017936281859874725
iteration 111, loss = 0.017550092190504074
iteration 112, loss = 0.02085285633802414
iteration 113, loss = 0.01692943647503853
iteration 114, loss = 0.018541090190410614
iteration 115, loss = 0.018251527100801468
iteration 116, loss = 0.02050262689590454
iteration 117, loss = 0.024621406570076942
iteration 118, loss = 0.01808965392410755
iteration 119, loss = 0.023921355605125427
iteration 120, loss = 0.018119219690561295
iteration 121, loss = 0.017748994752764702
iteration 122, loss = 0.016863731667399406
iteration 123, loss = 0.019033577293157578
iteration 124, loss = 0.019413774833083153
iteration 125, loss = 0.021546149626374245
iteration 126, loss = 0.03124172054231167
iteration 127, loss = 0.02011457458138466
iteration 128, loss = 0.019000213593244553
iteration 129, loss = 0.02526884339749813
iteration 130, loss = 0.023593662306666374
iteration 131, loss = 0.01987319253385067
iteration 132, loss = 0.017338547855615616
iteration 133, loss = 0.017548149451613426
iteration 134, loss = 0.018289431929588318
iteration 135, loss = 0.022624816745519638
iteration 136, loss = 0.02104158140718937
iteration 137, loss = 0.018328409641981125
iteration 138, loss = 0.018491175025701523
iteration 139, loss = 0.017339631915092468
iteration 140, loss = 0.01837298460304737
iteration 141, loss = 0.018452264368534088
iteration 142, loss = 0.01814970001578331
iteration 143, loss = 0.021947313100099564
iteration 144, loss = 0.01774025149643421
iteration 145, loss = 0.0242630485445261
iteration 146, loss = 0.019160009920597076
iteration 147, loss = 0.021319543942809105
iteration 148, loss = 0.017428254708647728
iteration 149, loss = 0.018602440133690834
iteration 150, loss = 0.018023299053311348
iteration 151, loss = 0.018917778506875038
iteration 152, loss = 0.018159471452236176
iteration 153, loss = 0.01841142773628235
iteration 154, loss = 0.0190645232796669
iteration 155, loss = 0.018378453329205513
iteration 156, loss = 0.018244240432977676
iteration 157, loss = 0.019138241186738014
iteration 158, loss = 0.023587623611092567
iteration 159, loss = 0.021242188289761543
iteration 160, loss = 0.01794038712978363
iteration 161, loss = 0.02203850820660591
iteration 162, loss = 0.017899906262755394
iteration 163, loss = 0.019160980358719826
iteration 164, loss = 0.01922404021024704
iteration 165, loss = 0.03180738538503647
iteration 166, loss = 0.03171100467443466
iteration 167, loss = 0.020418599247932434
iteration 168, loss = 0.01844746433198452
iteration 169, loss = 0.023195859044790268
iteration 170, loss = 0.025618676096200943
iteration 171, loss = 0.02491329051554203
iteration 172, loss = 0.020075246691703796
iteration 173, loss = 0.01942894235253334
iteration 174, loss = 0.017970602959394455
iteration 175, loss = 0.017647895961999893
iteration 176, loss = 0.023699114099144936
iteration 177, loss = 0.0192536860704422
iteration 178, loss = 0.016986554488539696
iteration 179, loss = 0.023410428315401077
iteration 180, loss = 0.01870332844555378
iteration 181, loss = 0.017483986914157867
iteration 182, loss = 0.019283903762698174
iteration 183, loss = 0.022038476541638374
iteration 184, loss = 0.01991756074130535
iteration 185, loss = 0.0232069194316864
iteration 186, loss = 0.017937155440449715
iteration 187, loss = 0.01839354634284973
iteration 188, loss = 0.01885775662958622
iteration 189, loss = 0.0181341003626585
iteration 190, loss = 0.026132844388484955
iteration 191, loss = 0.018869861960411072
iteration 192, loss = 0.017121966928243637
iteration 193, loss = 0.02306283265352249
iteration 194, loss = 0.02443760447204113
iteration 195, loss = 0.022536834701895714
iteration 196, loss = 0.017503298819065094
iteration 197, loss = 0.01910739205777645
iteration 198, loss = 0.023755034431815147
iteration 199, loss = 0.018652454018592834
iteration 200, loss = 0.016597308218479156
iteration 201, loss = 0.02028694748878479
iteration 202, loss = 0.023440463468432426
iteration 203, loss = 0.017368271946907043
iteration 204, loss = 0.020198356360197067
iteration 205, loss = 0.024850143119692802
iteration 206, loss = 0.01924891211092472
iteration 207, loss = 0.018009774386882782
iteration 208, loss = 0.025090418756008148
iteration 209, loss = 0.01838638260960579
iteration 210, loss = 0.023897578939795494
iteration 211, loss = 0.01669716276228428
iteration 212, loss = 0.01687348634004593
iteration 213, loss = 0.017861517146229744
iteration 214, loss = 0.018538404256105423
iteration 215, loss = 0.01684408262372017
iteration 216, loss = 0.01756468415260315
iteration 217, loss = 0.01967652514576912
iteration 218, loss = 0.018674002960324287
iteration 219, loss = 0.018508663401007652
iteration 220, loss = 0.02274329960346222
iteration 221, loss = 0.023738296702504158
iteration 222, loss = 0.020916685461997986
iteration 223, loss = 0.01687745749950409
iteration 224, loss = 0.016839222982525826
iteration 225, loss = 0.017842181026935577
iteration 226, loss = 0.025953393429517746
iteration 227, loss = 0.01905326545238495
iteration 228, loss = 0.019762782379984856
iteration 229, loss = 0.025321247056126595
iteration 230, loss = 0.01955360174179077
iteration 231, loss = 0.0175453033298254
iteration 232, loss = 0.02033015713095665
iteration 233, loss = 0.025974446907639503
iteration 234, loss = 0.0235556960105896
iteration 235, loss = 0.022517461329698563
iteration 236, loss = 0.01711707003414631
iteration 237, loss = 0.02563297748565674
iteration 238, loss = 0.021429335698485374
iteration 239, loss = 0.021411698311567307
iteration 240, loss = 0.016678189858794212
iteration 241, loss = 0.021982848644256592
iteration 242, loss = 0.021103400737047195
iteration 243, loss = 0.01899440586566925
iteration 244, loss = 0.02494443580508232
iteration 245, loss = 0.017768152058124542
iteration 246, loss = 0.017739495262503624
iteration 247, loss = 0.017910510301589966
iteration 248, loss = 0.02368851937353611
iteration 249, loss = 0.019609935581684113
iteration 250, loss = 0.018620846793055534
iteration 251, loss = 0.023741373792290688
iteration 252, loss = 0.017902670428156853
iteration 253, loss = 0.01867840811610222
iteration 254, loss = 0.0238769743591547
iteration 255, loss = 0.021789222955703735
iteration 256, loss = 0.01859084516763687
iteration 257, loss = 0.018704140558838844
iteration 258, loss = 0.022784609347581863
iteration 259, loss = 0.01801341027021408
iteration 260, loss = 0.018784508109092712
iteration 261, loss = 0.022100022062659264
iteration 262, loss = 0.022671693935990334
iteration 263, loss = 0.018258003517985344
iteration 264, loss = 0.020390214398503304
iteration 265, loss = 0.01708075776696205
iteration 266, loss = 0.022100504487752914
iteration 267, loss = 0.022583624348044395
iteration 268, loss = 0.017044581472873688
iteration 269, loss = 0.0184563547372818
iteration 270, loss = 0.01636180654168129
iteration 271, loss = 0.016961654648184776
iteration 272, loss = 0.017849810421466827
iteration 273, loss = 0.01746206358075142
iteration 274, loss = 0.017924554646015167
iteration 275, loss = 0.022430259734392166
iteration 276, loss = 0.01938674785196781
iteration 277, loss = 0.021139290183782578
iteration 278, loss = 0.018552646040916443
iteration 279, loss = 0.022382250055670738
iteration 280, loss = 0.0230002049356699
iteration 281, loss = 0.02382488176226616
iteration 282, loss = 0.01634536311030388
iteration 283, loss = 0.017605071887373924
iteration 284, loss = 0.016417408362030983
iteration 285, loss = 0.020425086840987206
iteration 286, loss = 0.020397432148456573
iteration 287, loss = 0.019582757726311684
iteration 288, loss = 0.01714973710477352
iteration 289, loss = 0.017874477431178093
iteration 290, loss = 0.017571669071912766
iteration 291, loss = 0.024296754971146584
iteration 292, loss = 0.016907626762986183
iteration 293, loss = 0.018854642286896706
iteration 294, loss = 0.017544016242027283
iteration 295, loss = 0.021619023755192757
iteration 296, loss = 0.01717308722436428
iteration 297, loss = 0.016417259350419044
iteration 298, loss = 0.017601992934942245
iteration 299, loss = 0.02033321186900139
iteration 300, loss = 0.017901554703712463
iteration 1, loss = 0.018147844821214676
iteration 2, loss = 0.028392799198627472
iteration 3, loss = 0.017214138060808182
iteration 4, loss = 0.017530396580696106
iteration 5, loss = 0.019200026988983154
iteration 6, loss = 0.01791103556752205
iteration 7, loss = 0.02625134587287903
iteration 8, loss = 0.01673520915210247
iteration 9, loss = 0.01801910065114498
iteration 10, loss = 0.02076110430061817
iteration 11, loss = 0.019917061552405357
iteration 12, loss = 0.017519770190119743
iteration 13, loss = 0.017697129398584366
iteration 14, loss = 0.01859467849135399
iteration 15, loss = 0.017646001651883125
iteration 16, loss = 0.023502763360738754
iteration 17, loss = 0.018328331410884857
iteration 18, loss = 0.017231939360499382
iteration 19, loss = 0.02479035221040249
iteration 20, loss = 0.019420647993683815
iteration 21, loss = 0.01725204475224018
iteration 22, loss = 0.01876714825630188
iteration 23, loss = 0.022779367864131927
iteration 24, loss = 0.020169278606772423
iteration 25, loss = 0.022613460198044777
iteration 26, loss = 0.022017071023583412
iteration 27, loss = 0.017191430553793907
iteration 28, loss = 0.023931773379445076
iteration 29, loss = 0.01634933613240719
iteration 30, loss = 0.019132867455482483
iteration 31, loss = 0.01948646456003189
iteration 32, loss = 0.016678350046277046
iteration 33, loss = 0.017323201522231102
iteration 34, loss = 0.020393744111061096
iteration 35, loss = 0.01631646603345871
iteration 36, loss = 0.018688559532165527
iteration 37, loss = 0.019949760288000107
iteration 38, loss = 0.01691542938351631
iteration 39, loss = 0.01862090639770031
iteration 40, loss = 0.016507824882864952
iteration 41, loss = 0.01942838728427887
iteration 42, loss = 0.021739400923252106
iteration 43, loss = 0.017853979021310806
iteration 44, loss = 0.0230459813028574
iteration 45, loss = 0.022039145231246948
iteration 46, loss = 0.01990802399814129
iteration 47, loss = 0.019638467580080032
iteration 48, loss = 0.01793953962624073
iteration 49, loss = 0.019236134365200996
iteration 50, loss = 0.016747966408729553
iteration 51, loss = 0.019073249772191048
iteration 52, loss = 0.0165538489818573
iteration 53, loss = 0.020651210099458694
iteration 54, loss = 0.01770056039094925
iteration 55, loss = 0.019510654732584953
iteration 56, loss = 0.018813185393810272
iteration 57, loss = 0.017213884741067886
iteration 58, loss = 0.019550595432519913
iteration 59, loss = 0.019547415897250175
iteration 60, loss = 0.018990473821759224
iteration 61, loss = 0.019583366811275482
iteration 62, loss = 0.01791973225772381
iteration 63, loss = 0.0189579539000988
iteration 64, loss = 0.018202222883701324
iteration 65, loss = 0.016770578920841217
iteration 66, loss = 0.01911735162138939
iteration 67, loss = 0.02641736902296543
iteration 68, loss = 0.01858570985496044
iteration 69, loss = 0.017540274187922478
iteration 70, loss = 0.01978367753326893
iteration 71, loss = 0.019161798059940338
iteration 72, loss = 0.017834633588790894
iteration 73, loss = 0.01849224418401718
iteration 74, loss = 0.021469993516802788
iteration 75, loss = 0.0179009810090065
iteration 76, loss = 0.020154988393187523
iteration 77, loss = 0.017493514344096184
iteration 78, loss = 0.01839051954448223
iteration 79, loss = 0.017149075865745544
iteration 80, loss = 0.01842324435710907
iteration 81, loss = 0.022112805396318436
iteration 82, loss = 0.016447611153125763
iteration 83, loss = 0.01721290312707424
iteration 84, loss = 0.01738639362156391
iteration 85, loss = 0.018367435783147812
iteration 86, loss = 0.01748785190284252
iteration 87, loss = 0.021646153181791306
iteration 88, loss = 0.016648808494210243
iteration 89, loss = 0.018915574997663498
iteration 90, loss = 0.016740020364522934
iteration 91, loss = 0.023878419771790504
iteration 92, loss = 0.01791404001414776
iteration 93, loss = 0.018212979659438133
iteration 94, loss = 0.02161565236747265
iteration 95, loss = 0.018543092533946037
iteration 96, loss = 0.017364976927638054
iteration 97, loss = 0.01707298681139946
iteration 98, loss = 0.01668808050453663
iteration 99, loss = 0.022439008578658104
iteration 100, loss = 0.016543393954634666
iteration 101, loss = 0.018939387053251266
iteration 102, loss = 0.017757205292582512
iteration 103, loss = 0.027657758444547653
iteration 104, loss = 0.018612142652273178
iteration 105, loss = 0.01732686720788479
iteration 106, loss = 0.01634231209754944
iteration 107, loss = 0.01873307302594185
iteration 108, loss = 0.01925700157880783
iteration 109, loss = 0.024842675775289536
iteration 110, loss = 0.017696255818009377
iteration 111, loss = 0.018490470945835114
iteration 112, loss = 0.017259985208511353
iteration 113, loss = 0.01804017461836338
iteration 114, loss = 0.016790013760328293
iteration 115, loss = 0.01621147058904171
iteration 116, loss = 0.022146103903651237
iteration 117, loss = 0.01693023182451725
iteration 118, loss = 0.019038325175642967
iteration 119, loss = 0.020027555525302887
iteration 120, loss = 0.018230952322483063
iteration 121, loss = 0.016426801681518555
iteration 122, loss = 0.01750606670975685
iteration 123, loss = 0.01625441387295723
iteration 124, loss = 0.017327869310975075
iteration 125, loss = 0.016671333461999893
iteration 126, loss = 0.029780816286802292
iteration 127, loss = 0.01748814806342125
iteration 128, loss = 0.016423748806118965
iteration 129, loss = 0.024763019755482674
iteration 130, loss = 0.019780974835157394
iteration 131, loss = 0.020014021545648575
iteration 132, loss = 0.01745608076453209
iteration 133, loss = 0.01659955456852913
iteration 134, loss = 0.022493436932563782
iteration 135, loss = 0.017311232164502144
iteration 136, loss = 0.017005877569317818
iteration 137, loss = 0.018428707495331764
iteration 138, loss = 0.017549429088830948
iteration 139, loss = 0.017768874764442444
iteration 140, loss = 0.02285507321357727
iteration 141, loss = 0.016374966129660606
iteration 142, loss = 0.021243559196591377
iteration 143, loss = 0.019965505227446556
iteration 144, loss = 0.024249238893389702
iteration 145, loss = 0.024895748123526573
iteration 146, loss = 0.017004063352942467
iteration 147, loss = 0.01799420267343521
iteration 148, loss = 0.023315543308854103
iteration 149, loss = 0.019048109650611877
iteration 150, loss = 0.01675882190465927
iteration 151, loss = 0.018043367192149162
iteration 152, loss = 0.02307337149977684
iteration 153, loss = 0.02408626675605774
iteration 154, loss = 0.018774786964058876
iteration 155, loss = 0.025742514058947563
iteration 156, loss = 0.017166191712021828
iteration 157, loss = 0.016511889174580574
iteration 158, loss = 0.01802155375480652
iteration 159, loss = 0.017136115580797195
iteration 160, loss = 0.01874527335166931
iteration 161, loss = 0.029546275734901428
iteration 162, loss = 0.018144527450203896
iteration 163, loss = 0.01731443591415882
iteration 164, loss = 0.017565926536917686
iteration 165, loss = 0.017695579677820206
iteration 166, loss = 0.030854474753141403
iteration 167, loss = 0.021379003301262856
iteration 168, loss = 0.020097283646464348
iteration 169, loss = 0.021864445880055428
iteration 170, loss = 0.019435754045844078
iteration 171, loss = 0.017431730404496193
iteration 172, loss = 0.018062271177768707
iteration 173, loss = 0.016897285357117653
iteration 174, loss = 0.022302577272057533
iteration 175, loss = 0.01648327335715294
iteration 176, loss = 0.01664784923195839
iteration 177, loss = 0.01800614595413208
iteration 178, loss = 0.01746332459151745
iteration 179, loss = 0.022771388292312622
iteration 180, loss = 0.01816572993993759
iteration 181, loss = 0.017748814076185226
iteration 182, loss = 0.016845088452100754
iteration 183, loss = 0.01610960066318512
iteration 184, loss = 0.018472224473953247
iteration 185, loss = 0.01808669976890087
iteration 186, loss = 0.016320200636982918
iteration 187, loss = 0.019313668832182884
iteration 188, loss = 0.021894125267863274
iteration 189, loss = 0.017264768481254578
iteration 190, loss = 0.021127192303538322
iteration 191, loss = 0.01626739278435707
iteration 192, loss = 0.023627575486898422
iteration 193, loss = 0.01645725965499878
iteration 194, loss = 0.017341969534754753
iteration 195, loss = 0.021839605644345284
iteration 196, loss = 0.016385164111852646
iteration 197, loss = 0.022249659523367882
iteration 198, loss = 0.02494044043123722
iteration 199, loss = 0.019149817526340485
iteration 200, loss = 0.016865486279129982
iteration 201, loss = 0.01816505566239357
iteration 202, loss = 0.01870155707001686
iteration 203, loss = 0.018670786172151566
iteration 204, loss = 0.016291994601488113
iteration 205, loss = 0.019510407000780106
iteration 206, loss = 0.019488384947180748
iteration 207, loss = 0.017999708652496338
iteration 208, loss = 0.017106153070926666
iteration 209, loss = 0.019100837409496307
iteration 210, loss = 0.01763031631708145
iteration 211, loss = 0.023321498185396194
iteration 212, loss = 0.020354216918349266
iteration 213, loss = 0.019153358414769173
iteration 214, loss = 0.028258763253688812
iteration 215, loss = 0.02438807114958763
iteration 216, loss = 0.017187848687171936
iteration 217, loss = 0.01704304851591587
iteration 218, loss = 0.017796402797102928
iteration 219, loss = 0.022889157757163048
iteration 220, loss = 0.018257057294249535
iteration 221, loss = 0.02600284479558468
iteration 222, loss = 0.022163942456245422
iteration 223, loss = 0.0219133123755455
iteration 224, loss = 0.018293840810656548
iteration 225, loss = 0.021330945193767548
iteration 226, loss = 0.01757662743330002
iteration 227, loss = 0.01825619488954544
iteration 228, loss = 0.017419546842575073
iteration 229, loss = 0.01658351719379425
iteration 230, loss = 0.03290198743343353
iteration 231, loss = 0.01722072809934616
iteration 232, loss = 0.01946405880153179
iteration 233, loss = 0.0165608748793602
iteration 234, loss = 0.01755683869123459
iteration 235, loss = 0.019664838910102844
iteration 236, loss = 0.018227823078632355
iteration 237, loss = 0.028889277949929237
iteration 238, loss = 0.01750565692782402
iteration 239, loss = 0.018549693748354912
iteration 240, loss = 0.02610379084944725
iteration 241, loss = 0.016927562654018402
iteration 242, loss = 0.018042750656604767
iteration 243, loss = 0.016854483634233475
iteration 244, loss = 0.018537942320108414
iteration 245, loss = 0.02202986367046833
iteration 246, loss = 0.022081661969423294
iteration 247, loss = 0.029663294553756714
iteration 248, loss = 0.029767759144306183
iteration 249, loss = 0.01609804481267929
iteration 250, loss = 0.01821232959628105
iteration 251, loss = 0.01627882942557335
iteration 252, loss = 0.021875478327274323
iteration 253, loss = 0.016873648390173912
iteration 254, loss = 0.016877824440598488
iteration 255, loss = 0.0173186045140028
iteration 256, loss = 0.01642283797264099
iteration 257, loss = 0.02058614231646061
iteration 258, loss = 0.02513727732002735
iteration 259, loss = 0.016836702823638916
iteration 260, loss = 0.024366745725274086
iteration 261, loss = 0.026589037850499153
iteration 262, loss = 0.025789104402065277
iteration 263, loss = 0.01773344911634922
iteration 264, loss = 0.02188650704920292
iteration 265, loss = 0.022720498964190483
iteration 266, loss = 0.016150809824466705
iteration 267, loss = 0.01822986640036106
iteration 268, loss = 0.017418181523680687
iteration 269, loss = 0.017023365944623947
iteration 270, loss = 0.022384904325008392
iteration 271, loss = 0.01751992665231228
iteration 272, loss = 0.016665704548358917
iteration 273, loss = 0.017084361985325813
iteration 274, loss = 0.0234205462038517
iteration 275, loss = 0.021733028814196587
iteration 276, loss = 0.01806100457906723
iteration 277, loss = 0.02756284736096859
iteration 278, loss = 0.020338574424386024
iteration 279, loss = 0.01619149185717106
iteration 280, loss = 0.020164208486676216
iteration 281, loss = 0.024808328598737717
iteration 282, loss = 0.017401617020368576
iteration 283, loss = 0.018941732123494148
iteration 284, loss = 0.024626092985272408
iteration 285, loss = 0.016495276242494583
iteration 286, loss = 0.017144568264484406
iteration 287, loss = 0.025631355121731758
iteration 288, loss = 0.023606978356838226
iteration 289, loss = 0.02665310725569725
iteration 290, loss = 0.01594730280339718
iteration 291, loss = 0.022249288856983185
iteration 292, loss = 0.022992435842752457
iteration 293, loss = 0.017337922006845474
iteration 294, loss = 0.016983387991786003
iteration 295, loss = 0.01651015318930149
iteration 296, loss = 0.016098717227578163
iteration 297, loss = 0.018896182999014854
iteration 298, loss = 0.017759667709469795
iteration 299, loss = 0.017752232030034065
iteration 300, loss = 0.020434655249118805
iteration 1, loss = 0.020756585523486137
iteration 2, loss = 0.027036573737859726
iteration 3, loss = 0.017451224848628044
iteration 4, loss = 0.024985123425722122
iteration 5, loss = 0.018113860860466957
iteration 6, loss = 0.017885932698845863
iteration 7, loss = 0.01709209755063057
iteration 8, loss = 0.020594635978341103
iteration 9, loss = 0.01716143824160099
iteration 10, loss = 0.015939410775899887
iteration 11, loss = 0.02113119512796402
iteration 12, loss = 0.018086785450577736
iteration 13, loss = 0.018630480393767357
iteration 14, loss = 0.01675656996667385
iteration 15, loss = 0.016728965565562248
iteration 16, loss = 0.017276601865887642
iteration 17, loss = 0.022108251228928566
iteration 18, loss = 0.018191052600741386
iteration 19, loss = 0.016571495682001114
iteration 20, loss = 0.02052634209394455
iteration 21, loss = 0.019651534035801888
iteration 22, loss = 0.016720205545425415
iteration 23, loss = 0.0165703222155571
iteration 24, loss = 0.01804778352379799
iteration 25, loss = 0.018307924270629883
iteration 26, loss = 0.0174717977643013
iteration 27, loss = 0.01878223940730095
iteration 28, loss = 0.01594211347401142
iteration 29, loss = 0.018360145390033722
iteration 30, loss = 0.018267769366502762
iteration 31, loss = 0.01838034763932228
iteration 32, loss = 0.016255971044301987
iteration 33, loss = 0.016789589077234268
iteration 34, loss = 0.015598172321915627
iteration 35, loss = 0.022693097591400146
iteration 36, loss = 0.01747223176062107
iteration 37, loss = 0.02188265323638916
iteration 38, loss = 0.02355154976248741
iteration 39, loss = 0.019323458895087242
iteration 40, loss = 0.017150552943348885
iteration 41, loss = 0.021031243726611137
iteration 42, loss = 0.019443780183792114
iteration 43, loss = 0.02102762833237648
iteration 44, loss = 0.0195317305624485
iteration 45, loss = 0.01764475367963314
iteration 46, loss = 0.022659456357359886
iteration 47, loss = 0.01717897690832615
iteration 48, loss = 0.017051992937922478
iteration 49, loss = 0.01670227013528347
iteration 50, loss = 0.023297002539038658
iteration 51, loss = 0.020747577771544456
iteration 52, loss = 0.025203103199601173
iteration 53, loss = 0.02161920629441738
iteration 54, loss = 0.024988213554024696
iteration 55, loss = 0.01672947220504284
iteration 56, loss = 0.02468486689031124
iteration 57, loss = 0.03255106881260872
iteration 58, loss = 0.01687474548816681
iteration 59, loss = 0.01818455010652542
iteration 60, loss = 0.016069483011960983
iteration 61, loss = 0.023452095687389374
iteration 62, loss = 0.016517067328095436
iteration 63, loss = 0.023479508236050606
iteration 64, loss = 0.016778970137238503
iteration 65, loss = 0.0209815576672554
iteration 66, loss = 0.017213771119713783
iteration 67, loss = 0.01696389727294445
iteration 68, loss = 0.026692014187574387
iteration 69, loss = 0.022544588893651962
iteration 70, loss = 0.016448386013507843
iteration 71, loss = 0.01704218238592148
iteration 72, loss = 0.020281897857785225
iteration 73, loss = 0.01975102722644806
iteration 74, loss = 0.017932454124093056
iteration 75, loss = 0.016479987651109695
iteration 76, loss = 0.025370260700583458
iteration 77, loss = 0.021845512092113495
iteration 78, loss = 0.019309859722852707
iteration 79, loss = 0.01695047691464424
iteration 80, loss = 0.016734974458813667
iteration 81, loss = 0.02876170538365841
iteration 82, loss = 0.0167156420648098
iteration 83, loss = 0.01637423038482666
iteration 84, loss = 0.01765263080596924
iteration 85, loss = 0.016356080770492554
iteration 86, loss = 0.016656246036291122
iteration 87, loss = 0.016646696254611015
iteration 88, loss = 0.01743624359369278
iteration 89, loss = 0.01998860575258732
iteration 90, loss = 0.01850905455648899
iteration 91, loss = 0.018317386507987976
iteration 92, loss = 0.024984436109662056
iteration 93, loss = 0.02171231620013714
iteration 94, loss = 0.02096324786543846
iteration 95, loss = 0.015757177025079727
iteration 96, loss = 0.017172738909721375
iteration 97, loss = 0.017043571919202805
iteration 98, loss = 0.01814640872180462
iteration 99, loss = 0.016134412959218025
iteration 100, loss = 0.018499892204999924
iteration 101, loss = 0.016426341608166695
iteration 102, loss = 0.017480431124567986
iteration 103, loss = 0.022849304601550102
iteration 104, loss = 0.016558684408664703
iteration 105, loss = 0.02363678440451622
iteration 106, loss = 0.017766589298844337
iteration 107, loss = 0.024266738444566727
iteration 108, loss = 0.017618635669350624
iteration 109, loss = 0.018502721562981606
iteration 110, loss = 0.01674794778227806
iteration 111, loss = 0.021687166765332222
iteration 112, loss = 0.02350093238055706
iteration 113, loss = 0.023724744096398354
iteration 114, loss = 0.01635734736919403
iteration 115, loss = 0.016492588445544243
iteration 116, loss = 0.01932545192539692
iteration 117, loss = 0.018978165462613106
iteration 118, loss = 0.01577790640294552
iteration 119, loss = 0.01664453186094761
iteration 120, loss = 0.01707051508128643
iteration 121, loss = 0.01597636565566063
iteration 122, loss = 0.017735587432980537
iteration 123, loss = 0.02303663082420826
iteration 124, loss = 0.019497377797961235
iteration 125, loss = 0.016760244965553284
iteration 126, loss = 0.021660450845956802
iteration 127, loss = 0.02320707030594349
iteration 128, loss = 0.018080081790685654
iteration 129, loss = 0.02304057963192463
iteration 130, loss = 0.01708444580435753
iteration 131, loss = 0.018219666555523872
iteration 132, loss = 0.016570312902331352
iteration 133, loss = 0.0158835556358099
iteration 134, loss = 0.017641665413975716
iteration 135, loss = 0.018180765211582184
iteration 136, loss = 0.01649847812950611
iteration 137, loss = 0.01666569523513317
iteration 138, loss = 0.01689651422202587
iteration 139, loss = 0.0293126218020916
iteration 140, loss = 0.01604003645479679
iteration 141, loss = 0.01517963781952858
iteration 142, loss = 0.0168145801872015
iteration 143, loss = 0.018713543191552162
iteration 144, loss = 0.01837056688964367
iteration 145, loss = 0.01595727726817131
iteration 146, loss = 0.015945445746183395
iteration 147, loss = 0.01548356469720602
iteration 148, loss = 0.016816116869449615
iteration 149, loss = 0.0230267271399498
iteration 150, loss = 0.017313934862613678
iteration 151, loss = 0.01947159878909588
iteration 152, loss = 0.023498687893152237
iteration 153, loss = 0.016177913174033165
iteration 154, loss = 0.022506553679704666
iteration 155, loss = 0.01657642051577568
iteration 156, loss = 0.016566907986998558
iteration 157, loss = 0.016062401235103607
iteration 158, loss = 0.015569046139717102
iteration 159, loss = 0.01834537461400032
iteration 160, loss = 0.01750432327389717
iteration 161, loss = 0.020114365965127945
iteration 162, loss = 0.019036784768104553
iteration 163, loss = 0.017602212727069855
iteration 164, loss = 0.01807144284248352
iteration 165, loss = 0.018214227631688118
iteration 166, loss = 0.018984131515026093
iteration 167, loss = 0.017036063596606255
iteration 168, loss = 0.01608390547335148
iteration 169, loss = 0.022029172629117966
iteration 170, loss = 0.020209727808833122
iteration 171, loss = 0.016203751787543297
iteration 172, loss = 0.01785818487405777
iteration 173, loss = 0.021061386913061142
iteration 174, loss = 0.015462322160601616
iteration 175, loss = 0.023365838453173637
iteration 176, loss = 0.022666215896606445
iteration 177, loss = 0.016275811940431595
iteration 178, loss = 0.021399961784482002
iteration 179, loss = 0.017292983829975128
iteration 180, loss = 0.017108220607042313
iteration 181, loss = 0.016477806493639946
iteration 182, loss = 0.01970747858285904
iteration 183, loss = 0.015446554869413376
iteration 184, loss = 0.022694310173392296
iteration 185, loss = 0.02120126225054264
iteration 186, loss = 0.020504502579569817
iteration 187, loss = 0.02002747170627117
iteration 188, loss = 0.026316197589039803
iteration 189, loss = 0.014974120073020458
iteration 190, loss = 0.020528187975287437
iteration 191, loss = 0.026321344077587128
iteration 192, loss = 0.01702711172401905
iteration 193, loss = 0.022995907813310623
iteration 194, loss = 0.01712307333946228
iteration 195, loss = 0.017018668353557587
iteration 196, loss = 0.021469861268997192
iteration 197, loss = 0.01753547415137291
iteration 198, loss = 0.019992800429463387
iteration 199, loss = 0.025487415492534637
iteration 200, loss = 0.022952819243073463
iteration 201, loss = 0.022637197747826576
iteration 202, loss = 0.021149257197976112
iteration 203, loss = 0.01632501743733883
iteration 204, loss = 0.017382213845849037
iteration 205, loss = 0.021130776032805443
iteration 206, loss = 0.01825488917529583
iteration 207, loss = 0.015404035337269306
iteration 208, loss = 0.018547361716628075
iteration 209, loss = 0.01680818945169449
iteration 210, loss = 0.01824815943837166
iteration 211, loss = 0.018125928938388824
iteration 212, loss = 0.019737377762794495
iteration 213, loss = 0.017065608873963356
iteration 214, loss = 0.01692882552742958
iteration 215, loss = 0.017500339075922966
iteration 216, loss = 0.017167385667562485
iteration 217, loss = 0.015472851693630219
iteration 218, loss = 0.01629093661904335
iteration 219, loss = 0.01798636093735695
iteration 220, loss = 0.01690957136452198
iteration 221, loss = 0.01607546955347061
iteration 222, loss = 0.01705080270767212
iteration 223, loss = 0.022176658734679222
iteration 224, loss = 0.01709180511534214
iteration 225, loss = 0.01627677120268345
iteration 226, loss = 0.015970483422279358
iteration 227, loss = 0.023036589846014977
iteration 228, loss = 0.016859835013747215
iteration 229, loss = 0.01769103668630123
iteration 230, loss = 0.021032482385635376
iteration 231, loss = 0.017283547669649124
iteration 232, loss = 0.017146160826086998
iteration 233, loss = 0.015837840735912323
iteration 234, loss = 0.016132161021232605
iteration 235, loss = 0.015759877860546112
iteration 236, loss = 0.017083628103137016
iteration 237, loss = 0.0167623832821846
iteration 238, loss = 0.016305703669786453
iteration 239, loss = 0.020312469452619553
iteration 240, loss = 0.018093455582857132
iteration 241, loss = 0.02107354626059532
iteration 242, loss = 0.015579232014715672
iteration 243, loss = 0.022488920018076897
iteration 244, loss = 0.020602691918611526
iteration 245, loss = 0.016645705327391624
iteration 246, loss = 0.020529530942440033
iteration 247, loss = 0.024181507527828217
iteration 248, loss = 0.015269789844751358
iteration 249, loss = 0.018829595297574997
iteration 250, loss = 0.015752049162983894
iteration 251, loss = 0.017480863258242607
iteration 252, loss = 0.020417625084519386
iteration 253, loss = 0.015904270112514496
iteration 254, loss = 0.023355213925242424
iteration 255, loss = 0.016303954645991325
iteration 256, loss = 0.022679975256323814
iteration 257, loss = 0.014915143139660358
iteration 258, loss = 0.01828579418361187
iteration 259, loss = 0.015300392173230648
iteration 260, loss = 0.015523076988756657
iteration 261, loss = 0.017069483175873756
iteration 262, loss = 0.017571186646819115
iteration 263, loss = 0.01721435785293579
iteration 264, loss = 0.019618917256593704
iteration 265, loss = 0.015515570528805256
iteration 266, loss = 0.015828214585781097
iteration 267, loss = 0.018665166571736336
iteration 268, loss = 0.02480737678706646
iteration 269, loss = 0.01679166406393051
iteration 270, loss = 0.015337072312831879
iteration 271, loss = 0.02272750996053219
iteration 272, loss = 0.017887918278574944
iteration 273, loss = 0.01634952984750271
iteration 274, loss = 0.015755437314510345
iteration 275, loss = 0.023854073137044907
iteration 276, loss = 0.019195569679141045
iteration 277, loss = 0.017376892268657684
iteration 278, loss = 0.014938761480152607
iteration 279, loss = 0.01603267528116703
iteration 280, loss = 0.016335036605596542
iteration 281, loss = 0.02375973016023636
iteration 282, loss = 0.017615266144275665
iteration 283, loss = 0.016751842573285103
iteration 284, loss = 0.016072671860456467
iteration 285, loss = 0.017297273501753807
iteration 286, loss = 0.01566348224878311
iteration 287, loss = 0.01806715875864029
iteration 288, loss = 0.020122626796364784
iteration 289, loss = 0.01625058427453041
iteration 290, loss = 0.01792481541633606
iteration 291, loss = 0.02237989380955696
iteration 292, loss = 0.018179960548877716
iteration 293, loss = 0.01542486809194088
iteration 294, loss = 0.01924925297498703
iteration 295, loss = 0.020093638449907303
iteration 296, loss = 0.01557476818561554
iteration 297, loss = 0.01542524341493845
iteration 298, loss = 0.01575414277613163
iteration 299, loss = 0.018396221101284027
iteration 300, loss = 0.01717986725270748
iteration 1, loss = 0.01626932993531227
iteration 2, loss = 0.017510516569018364
iteration 3, loss = 0.016837002709507942
iteration 4, loss = 0.021139049902558327
iteration 5, loss = 0.016307152807712555
iteration 6, loss = 0.01732676289975643
iteration 7, loss = 0.017094481736421585
iteration 8, loss = 0.01512815523892641
iteration 9, loss = 0.024758070707321167
iteration 10, loss = 0.02435874007642269
iteration 11, loss = 0.015287223272025585
iteration 12, loss = 0.015985840931534767
iteration 13, loss = 0.024960823357105255
iteration 14, loss = 0.030190736055374146
iteration 15, loss = 0.016621101647615433
iteration 16, loss = 0.018571456894278526
iteration 17, loss = 0.017586449161171913
iteration 18, loss = 0.015706084668636322
iteration 19, loss = 0.016046423465013504
iteration 20, loss = 0.015394032001495361
iteration 21, loss = 0.019806522876024246
iteration 22, loss = 0.025713475421071053
iteration 23, loss = 0.01819518394768238
iteration 24, loss = 0.017301078885793686
iteration 25, loss = 0.017269058153033257
iteration 26, loss = 0.022496720775961876
iteration 27, loss = 0.015484442934393883
iteration 28, loss = 0.016310827806591988
iteration 29, loss = 0.021845800802111626
iteration 30, loss = 0.016824660822749138
iteration 31, loss = 0.015979504212737083
iteration 32, loss = 0.014939024113118649
iteration 33, loss = 0.017455201596021652
iteration 34, loss = 0.015434172935783863
iteration 35, loss = 0.015320523642003536
iteration 36, loss = 0.022316033020615578
iteration 37, loss = 0.02065103128552437
iteration 38, loss = 0.017085280269384384
iteration 39, loss = 0.017954031005501747
iteration 40, loss = 0.021280894055962563
iteration 41, loss = 0.016353510320186615
iteration 42, loss = 0.01976805552840233
iteration 43, loss = 0.01704753376543522
iteration 44, loss = 0.019495889544487
iteration 45, loss = 0.015616250224411488
iteration 46, loss = 0.018025614321231842
iteration 47, loss = 0.015451253391802311
iteration 48, loss = 0.016170572489500046
iteration 49, loss = 0.01704607717692852
iteration 50, loss = 0.016089273616671562
iteration 51, loss = 0.020847614854574203
iteration 52, loss = 0.016642043367028236
iteration 53, loss = 0.0166153684258461
iteration 54, loss = 0.01935793273150921
iteration 55, loss = 0.020720412954688072
iteration 56, loss = 0.01978546567261219
iteration 57, loss = 0.016490042209625244
iteration 58, loss = 0.016652831807732582
iteration 59, loss = 0.016274625435471535
iteration 60, loss = 0.022950032725930214
iteration 61, loss = 0.016611024737358093
iteration 62, loss = 0.016387060284614563
iteration 63, loss = 0.01698390021920204
iteration 64, loss = 0.016739705577492714
iteration 65, loss = 0.01640864834189415
iteration 66, loss = 0.023064928129315376
iteration 67, loss = 0.014736584387719631
iteration 68, loss = 0.020758410915732384
iteration 69, loss = 0.017994020134210587
iteration 70, loss = 0.016138261184096336
iteration 71, loss = 0.01665659435093403
iteration 72, loss = 0.01668853871524334
iteration 73, loss = 0.01641703210771084
iteration 74, loss = 0.0211077518761158
iteration 75, loss = 0.016113407909870148
iteration 76, loss = 0.022517891600728035
iteration 77, loss = 0.015933807939291
iteration 78, loss = 0.015073999762535095
iteration 79, loss = 0.016557583585381508
iteration 80, loss = 0.015050404705107212
iteration 81, loss = 0.017799250781536102
iteration 82, loss = 0.016103345900774002
iteration 83, loss = 0.016627373173832893
iteration 84, loss = 0.018221918493509293
iteration 85, loss = 0.02072261832654476
iteration 86, loss = 0.015964196994900703
iteration 87, loss = 0.017314383760094643
iteration 88, loss = 0.015889212489128113
iteration 89, loss = 0.018263056874275208
iteration 90, loss = 0.016550643369555473
iteration 91, loss = 0.016327742487192154
iteration 92, loss = 0.016490956768393517
iteration 93, loss = 0.017076125368475914
iteration 94, loss = 0.015024629421532154
iteration 95, loss = 0.018312599509954453
iteration 96, loss = 0.027187012135982513
iteration 97, loss = 0.017513373866677284
iteration 98, loss = 0.022061700001358986
iteration 99, loss = 0.015012353658676147
iteration 100, loss = 0.020835284143686295
iteration 101, loss = 0.015253555960953236
iteration 102, loss = 0.016957610845565796
iteration 103, loss = 0.017229527235031128
iteration 104, loss = 0.016159072518348694
iteration 105, loss = 0.021106449887156487
iteration 106, loss = 0.01482008770108223
iteration 107, loss = 0.016468709334731102
iteration 108, loss = 0.016201697289943695
iteration 109, loss = 0.01789155788719654
iteration 110, loss = 0.01808042824268341
iteration 111, loss = 0.016738567501306534
iteration 112, loss = 0.018188131973147392
iteration 113, loss = 0.017365770414471626
iteration 114, loss = 0.015740416944026947
iteration 115, loss = 0.020865635946393013
iteration 116, loss = 0.016181835904717445
iteration 117, loss = 0.01551096886396408
iteration 118, loss = 0.02185131423175335
iteration 119, loss = 0.015616964548826218
iteration 120, loss = 0.02091060020029545
iteration 121, loss = 0.01666824333369732
iteration 122, loss = 0.02313278429210186
iteration 123, loss = 0.016130663454532623
iteration 124, loss = 0.021153604611754417
iteration 125, loss = 0.01986563764512539
iteration 126, loss = 0.01689809001982212
iteration 127, loss = 0.02088441513478756
iteration 128, loss = 0.017000693827867508
iteration 129, loss = 0.016270820051431656
iteration 130, loss = 0.017121143639087677
iteration 131, loss = 0.01598365046083927
iteration 132, loss = 0.015740111470222473
iteration 133, loss = 0.01737978868186474
iteration 134, loss = 0.019612930715084076
iteration 135, loss = 0.016105301678180695
iteration 136, loss = 0.017049206420779228
iteration 137, loss = 0.01729501597583294
iteration 138, loss = 0.028947804123163223
iteration 139, loss = 0.014880883507430553
iteration 140, loss = 0.01575625129044056
iteration 141, loss = 0.024827420711517334
iteration 142, loss = 0.019543180242180824
iteration 143, loss = 0.01753680780529976
iteration 144, loss = 0.016040069982409477
iteration 145, loss = 0.018609289079904556
iteration 146, loss = 0.015050682239234447
iteration 147, loss = 0.016376197338104248
iteration 148, loss = 0.015016291290521622
iteration 149, loss = 0.017241712659597397
iteration 150, loss = 0.019385645166039467
iteration 151, loss = 0.0168740414083004
iteration 152, loss = 0.01581786572933197
iteration 153, loss = 0.02134762890636921
iteration 154, loss = 0.017581474035978317
iteration 155, loss = 0.01641291193664074
iteration 156, loss = 0.015988752245903015
iteration 157, loss = 0.020070746541023254
iteration 158, loss = 0.01509779877960682
iteration 159, loss = 0.015151486732065678
iteration 160, loss = 0.01597082056105137
iteration 161, loss = 0.027121247723698616
iteration 162, loss = 0.01514074020087719
iteration 163, loss = 0.020513271912932396
iteration 164, loss = 0.015779580920934677
iteration 165, loss = 0.015407325699925423
iteration 166, loss = 0.022732438519597054
iteration 167, loss = 0.02193591371178627
iteration 168, loss = 0.02245708741247654
iteration 169, loss = 0.02044808119535446
iteration 170, loss = 0.016618045046925545
iteration 171, loss = 0.016446052119135857
iteration 172, loss = 0.015143981203436852
iteration 173, loss = 0.015243006870150566
iteration 174, loss = 0.016595588997006416
iteration 175, loss = 0.01583430916070938
iteration 176, loss = 0.015724660828709602
iteration 177, loss = 0.015484697185456753
iteration 178, loss = 0.01979009062051773
iteration 179, loss = 0.021411525085568428
iteration 180, loss = 0.01682310178875923
iteration 181, loss = 0.018345262855291367
iteration 182, loss = 0.016658276319503784
iteration 183, loss = 0.022971048951148987
iteration 184, loss = 0.01526179350912571
iteration 185, loss = 0.02671949937939644
iteration 186, loss = 0.015457463450729847
iteration 187, loss = 0.015315614640712738
iteration 188, loss = 0.025207001715898514
iteration 189, loss = 0.015790969133377075
iteration 190, loss = 0.01549204159528017
iteration 191, loss = 0.015775155276060104
iteration 192, loss = 0.01984473317861557
iteration 193, loss = 0.01728634722530842
iteration 194, loss = 0.016850128769874573
iteration 195, loss = 0.015926990658044815
iteration 196, loss = 0.021722931414842606
iteration 197, loss = 0.015176236629486084
iteration 198, loss = 0.022291649132966995
iteration 199, loss = 0.016394954174757004
iteration 200, loss = 0.02116582542657852
iteration 201, loss = 0.01595166325569153
iteration 202, loss = 0.015078474767506123
iteration 203, loss = 0.016251608729362488
iteration 204, loss = 0.016012052074074745
iteration 205, loss = 0.016961945220828056
iteration 206, loss = 0.016767198219895363
iteration 207, loss = 0.01771525852382183
iteration 208, loss = 0.016467608511447906
iteration 209, loss = 0.015299299731850624
iteration 210, loss = 0.01576213166117668
iteration 211, loss = 0.016231443732976913
iteration 212, loss = 0.02334563061594963
iteration 213, loss = 0.020182013511657715
iteration 214, loss = 0.015683172270655632
iteration 215, loss = 0.023411178961396217
iteration 216, loss = 0.016572939231991768
iteration 217, loss = 0.020733535289764404
iteration 218, loss = 0.015439734794199467
iteration 219, loss = 0.020151155069470406
iteration 220, loss = 0.019911423325538635
iteration 221, loss = 0.02328817918896675
iteration 222, loss = 0.015505267307162285
iteration 223, loss = 0.02196698635816574
iteration 224, loss = 0.01866154372692108
iteration 225, loss = 0.0164724700152874
iteration 226, loss = 0.015457513742148876
iteration 227, loss = 0.01648247241973877
iteration 228, loss = 0.01562700979411602
iteration 229, loss = 0.015463728457689285
iteration 230, loss = 0.021149344742298126
iteration 231, loss = 0.015784496441483498
iteration 232, loss = 0.01597864180803299
iteration 233, loss = 0.019693223759531975
iteration 234, loss = 0.015709666535258293
iteration 235, loss = 0.020787259563803673
iteration 236, loss = 0.020616421476006508
iteration 237, loss = 0.015923723578453064
iteration 238, loss = 0.021691543981432915
iteration 239, loss = 0.024036306887865067
iteration 240, loss = 0.019999470561742783
iteration 241, loss = 0.027397818863391876
iteration 242, loss = 0.01793990470468998
iteration 243, loss = 0.0189518965780735
iteration 244, loss = 0.020774085074663162
iteration 245, loss = 0.016129694879055023
iteration 246, loss = 0.015281004831194878
iteration 247, loss = 0.01605812832713127
iteration 248, loss = 0.017420437186956406
iteration 249, loss = 0.0174122154712677
iteration 250, loss = 0.016182154417037964
iteration 251, loss = 0.01555212214589119
iteration 252, loss = 0.016287589445710182
iteration 253, loss = 0.019845668226480484
iteration 254, loss = 0.02587125077843666
iteration 255, loss = 0.016646454110741615
iteration 256, loss = 0.016082530841231346
iteration 257, loss = 0.015360180288553238
iteration 258, loss = 0.015951791778206825
iteration 259, loss = 0.01613610051572323
iteration 260, loss = 0.015785198658704758
iteration 261, loss = 0.016685379669070244
iteration 262, loss = 0.019283577799797058
iteration 263, loss = 0.017587780952453613
iteration 264, loss = 0.01996237225830555
iteration 265, loss = 0.02130398154258728
iteration 266, loss = 0.014836027286946774
iteration 267, loss = 0.028042970225214958
iteration 268, loss = 0.015636252239346504
iteration 269, loss = 0.01600075699388981
iteration 270, loss = 0.016963621601462364
iteration 271, loss = 0.019624611362814903
iteration 272, loss = 0.015952738001942635
iteration 273, loss = 0.014932168647646904
iteration 274, loss = 0.01601920649409294
iteration 275, loss = 0.015180889517068863
iteration 276, loss = 0.02109796740114689
iteration 277, loss = 0.015070287510752678
iteration 278, loss = 0.017342019826173782
iteration 279, loss = 0.014898277819156647
iteration 280, loss = 0.017372148111462593
iteration 281, loss = 0.017748311161994934
iteration 282, loss = 0.016301441937685013
iteration 283, loss = 0.018813934177160263
iteration 284, loss = 0.016585391014814377
iteration 285, loss = 0.017031408846378326
iteration 286, loss = 0.018889497965574265
iteration 287, loss = 0.01458414550870657
iteration 288, loss = 0.016073858365416527
iteration 289, loss = 0.014240062795579433
iteration 290, loss = 0.017496144399046898
iteration 291, loss = 0.015631552785634995
iteration 292, loss = 0.020344072952866554
iteration 293, loss = 0.016592955216765404
iteration 294, loss = 0.023536432534456253
iteration 295, loss = 0.02133277617394924
iteration 296, loss = 0.019368276000022888
iteration 297, loss = 0.020031750202178955
iteration 298, loss = 0.022363871335983276
iteration 299, loss = 0.02458943799138069
iteration 300, loss = 0.019632218405604362
iteration 1, loss = 0.020284246653318405
iteration 2, loss = 0.017110127955675125
iteration 3, loss = 0.02065901830792427
iteration 4, loss = 0.020094653591513634
iteration 5, loss = 0.02237783372402191
iteration 6, loss = 0.01584193855524063
iteration 7, loss = 0.01539447158575058
iteration 8, loss = 0.022076524794101715
iteration 9, loss = 0.015886448323726654
iteration 10, loss = 0.018556509166955948
iteration 11, loss = 0.02034827321767807
iteration 12, loss = 0.02606567181646824
iteration 13, loss = 0.017778536304831505
iteration 14, loss = 0.015258670784533024
iteration 15, loss = 0.02161698415875435
iteration 16, loss = 0.015809709206223488
iteration 17, loss = 0.022072678431868553
iteration 18, loss = 0.015310986898839474
iteration 19, loss = 0.015580348670482635
iteration 20, loss = 0.02117554470896721
iteration 21, loss = 0.015646547079086304
iteration 22, loss = 0.015300475060939789
iteration 23, loss = 0.02015857957303524
iteration 24, loss = 0.017816763371229172
iteration 25, loss = 0.019676899537444115
iteration 26, loss = 0.015981400385499
iteration 27, loss = 0.016464363783597946
iteration 28, loss = 0.016767114400863647
iteration 29, loss = 0.015723833814263344
iteration 30, loss = 0.014773067086935043
iteration 31, loss = 0.014455363154411316
iteration 32, loss = 0.022376006469130516
iteration 33, loss = 0.01843821443617344
iteration 34, loss = 0.01821224018931389
iteration 35, loss = 0.024092387408018112
iteration 36, loss = 0.02226276695728302
iteration 37, loss = 0.016842195764183998
iteration 38, loss = 0.016594944521784782
iteration 39, loss = 0.015293281525373459
iteration 40, loss = 0.018446430563926697
iteration 41, loss = 0.022400815039873123
iteration 42, loss = 0.015275388956069946
iteration 43, loss = 0.01733667403459549
iteration 44, loss = 0.01880883239209652
iteration 45, loss = 0.01512411329895258
iteration 46, loss = 0.015063706785440445
iteration 47, loss = 0.015614356845617294
iteration 48, loss = 0.019210591912269592
iteration 49, loss = 0.01575641892850399
iteration 50, loss = 0.016325324773788452
iteration 51, loss = 0.017987072467803955
iteration 52, loss = 0.02049398422241211
iteration 53, loss = 0.01705825887620449
iteration 54, loss = 0.016732536256313324
iteration 55, loss = 0.02038188837468624
iteration 56, loss = 0.015279700979590416
iteration 57, loss = 0.020299196243286133
iteration 58, loss = 0.015199028886854649
iteration 59, loss = 0.015163913369178772
iteration 60, loss = 0.015942182391881943
iteration 61, loss = 0.019732855260372162
iteration 62, loss = 0.0157745610922575
iteration 63, loss = 0.015850242227315903
iteration 64, loss = 0.02888348139822483
iteration 65, loss = 0.014776726253330708
iteration 66, loss = 0.015486352145671844
iteration 67, loss = 0.014782722108066082
iteration 68, loss = 0.015374423936009407
iteration 69, loss = 0.01559562236070633
iteration 70, loss = 0.020205361768603325
iteration 71, loss = 0.016805576160550117
iteration 72, loss = 0.01643446460366249
iteration 73, loss = 0.020564349368214607
iteration 74, loss = 0.014945480041205883
iteration 75, loss = 0.027228040620684624
iteration 76, loss = 0.016239425167441368
iteration 77, loss = 0.017611591145396233
iteration 78, loss = 0.015529322437942028
iteration 79, loss = 0.0156660508364439
iteration 80, loss = 0.018660377711057663
iteration 81, loss = 0.019968237727880478
iteration 82, loss = 0.015614145435392857
iteration 83, loss = 0.013841740787029266
iteration 84, loss = 0.015256569720804691
iteration 85, loss = 0.015134477987885475
iteration 86, loss = 0.015269954688847065
iteration 87, loss = 0.01893814653158188
iteration 88, loss = 0.02577875554561615
iteration 89, loss = 0.02258073166012764
iteration 90, loss = 0.020585978403687477
iteration 91, loss = 0.017977312207221985
iteration 92, loss = 0.020518286153674126
iteration 93, loss = 0.01576591096818447
iteration 94, loss = 0.01611981727182865
iteration 95, loss = 0.015601531602442265
iteration 96, loss = 0.015071672387421131
iteration 97, loss = 0.016435906291007996
iteration 98, loss = 0.015006667003035545
iteration 99, loss = 0.015901587903499603
iteration 100, loss = 0.02013443224132061
iteration 101, loss = 0.01620299555361271
iteration 102, loss = 0.016465535387396812
iteration 103, loss = 0.016845978796482086
iteration 104, loss = 0.01958303526043892
iteration 105, loss = 0.01623895764350891
iteration 106, loss = 0.014921214431524277
iteration 107, loss = 0.01776898466050625
iteration 108, loss = 0.01954437792301178
iteration 109, loss = 0.016179965808987617
iteration 110, loss = 0.02313002198934555
iteration 111, loss = 0.018491877242922783
iteration 112, loss = 0.01606236957013607
iteration 113, loss = 0.016829589381814003
iteration 114, loss = 0.021015632897615433
iteration 115, loss = 0.01618691347539425
iteration 116, loss = 0.02487189695239067
iteration 117, loss = 0.015539928339421749
iteration 118, loss = 0.015171065926551819
iteration 119, loss = 0.014886628836393356
iteration 120, loss = 0.016027402132749557
iteration 121, loss = 0.016746990382671356
iteration 122, loss = 0.020009102299809456
iteration 123, loss = 0.015180142596364021
iteration 124, loss = 0.01633768156170845
iteration 125, loss = 0.017182860523462296
iteration 126, loss = 0.015352086164057255
iteration 127, loss = 0.015890950337052345
iteration 128, loss = 0.015253915451467037
iteration 129, loss = 0.022084245458245277
iteration 130, loss = 0.016843095421791077
iteration 131, loss = 0.016052784398198128
iteration 132, loss = 0.0183935035020113
iteration 133, loss = 0.019689075648784637
iteration 134, loss = 0.014726273715496063
iteration 135, loss = 0.0180328618735075
iteration 136, loss = 0.015374850481748581
iteration 137, loss = 0.023154206573963165
iteration 138, loss = 0.014884471893310547
iteration 139, loss = 0.01580340415239334
iteration 140, loss = 0.014306096360087395
iteration 141, loss = 0.015782097354531288
iteration 142, loss = 0.01614765077829361
iteration 143, loss = 0.0191258005797863
iteration 144, loss = 0.01578586921095848
iteration 145, loss = 0.021177897229790688
iteration 146, loss = 0.01922021619975567
iteration 147, loss = 0.015347039327025414
iteration 148, loss = 0.01593884825706482
iteration 149, loss = 0.015837091952562332
iteration 150, loss = 0.019083339720964432
iteration 151, loss = 0.01608482375741005
iteration 152, loss = 0.01647140271961689
iteration 153, loss = 0.03169987350702286
iteration 154, loss = 0.02312573790550232
iteration 155, loss = 0.015509126707911491
iteration 156, loss = 0.01469181664288044
iteration 157, loss = 0.015173227526247501
iteration 158, loss = 0.02077850140631199
iteration 159, loss = 0.01622619293630123
iteration 160, loss = 0.0186972226947546
iteration 161, loss = 0.02294464223086834
iteration 162, loss = 0.015462854877114296
iteration 163, loss = 0.01914244517683983
iteration 164, loss = 0.014452722854912281
iteration 165, loss = 0.014523672871291637
iteration 166, loss = 0.01577279530465603
iteration 167, loss = 0.016617845743894577
iteration 168, loss = 0.020869890227913857
iteration 169, loss = 0.014679566025733948
iteration 170, loss = 0.016699476167559624
iteration 171, loss = 0.01461560744792223
iteration 172, loss = 0.015841428190469742
iteration 173, loss = 0.01502094604074955
iteration 174, loss = 0.016539419069886208
iteration 175, loss = 0.01467496994882822
iteration 176, loss = 0.015223979018628597
iteration 177, loss = 0.020559048280119896
iteration 178, loss = 0.015518341213464737
iteration 179, loss = 0.01625523716211319
iteration 180, loss = 0.01764534041285515
iteration 181, loss = 0.015142067335546017
iteration 182, loss = 0.014726248569786549
iteration 183, loss = 0.014875911176204681
iteration 184, loss = 0.01704799383878708
iteration 185, loss = 0.015687985345721245
iteration 186, loss = 0.01948438584804535
iteration 187, loss = 0.017023667693138123
iteration 188, loss = 0.016699980944395065
iteration 189, loss = 0.01535855419933796
iteration 190, loss = 0.014851968735456467
iteration 191, loss = 0.018876230344176292
iteration 192, loss = 0.014429674483835697
iteration 193, loss = 0.020612116903066635
iteration 194, loss = 0.016860049217939377
iteration 195, loss = 0.01674569398164749
iteration 196, loss = 0.019742706790566444
iteration 197, loss = 0.015568844974040985
iteration 198, loss = 0.015344547107815742
iteration 199, loss = 0.02028927579522133
iteration 200, loss = 0.014999665319919586
iteration 201, loss = 0.015548573806881905
iteration 202, loss = 0.019277572631835938
iteration 203, loss = 0.01632501184940338
iteration 204, loss = 0.014915521256625652
iteration 205, loss = 0.01586981490254402
iteration 206, loss = 0.01486005075275898
iteration 207, loss = 0.014021950773894787
iteration 208, loss = 0.01589936949312687
iteration 209, loss = 0.015103503130376339
iteration 210, loss = 0.014851768501102924
iteration 211, loss = 0.01639650948345661
iteration 212, loss = 0.016796860843896866
iteration 213, loss = 0.015851153060793877
iteration 214, loss = 0.013937188312411308
iteration 215, loss = 0.0160213615745306
iteration 216, loss = 0.02168871834874153
iteration 217, loss = 0.018646683543920517
iteration 218, loss = 0.015276608988642693
iteration 219, loss = 0.018231838941574097
iteration 220, loss = 0.015416106209158897
iteration 221, loss = 0.017358655110001564
iteration 222, loss = 0.014669753611087799
iteration 223, loss = 0.01642277091741562
iteration 224, loss = 0.019164705649018288
iteration 225, loss = 0.016081392765045166
iteration 226, loss = 0.016212092712521553
iteration 227, loss = 0.01999453641474247
iteration 228, loss = 0.014697371050715446
iteration 229, loss = 0.02018951065838337
iteration 230, loss = 0.016217850148677826
iteration 231, loss = 0.015151109546422958
iteration 232, loss = 0.01425878144800663
iteration 233, loss = 0.01516652014106512
iteration 234, loss = 0.015812234953045845
iteration 235, loss = 0.01962342858314514
iteration 236, loss = 0.015494393184781075
iteration 237, loss = 0.018807023763656616
iteration 238, loss = 0.015167569741606712
iteration 239, loss = 0.01701650209724903
iteration 240, loss = 0.014048963785171509
iteration 241, loss = 0.01915023662149906
iteration 242, loss = 0.016463156789541245
iteration 243, loss = 0.016686666756868362
iteration 244, loss = 0.021208997815847397
iteration 245, loss = 0.014973057433962822
iteration 246, loss = 0.018476692959666252
iteration 247, loss = 0.020415034145116806
iteration 248, loss = 0.016422197222709656
iteration 249, loss = 0.019394122064113617
iteration 250, loss = 0.014620080590248108
iteration 251, loss = 0.016590379178524017
iteration 252, loss = 0.015356681309640408
iteration 253, loss = 0.02499893307685852
iteration 254, loss = 0.01931796781718731
iteration 255, loss = 0.015195278450846672
iteration 256, loss = 0.014466343447566032
iteration 257, loss = 0.016376374289393425
iteration 258, loss = 0.014480868354439735
iteration 259, loss = 0.01892535574734211
iteration 260, loss = 0.021620504558086395
iteration 261, loss = 0.01816609501838684
iteration 262, loss = 0.01728164032101631
iteration 263, loss = 0.01629411056637764
iteration 264, loss = 0.017803503200411797
iteration 265, loss = 0.016611464321613312
iteration 266, loss = 0.018316414207220078
iteration 267, loss = 0.015126810409128666
iteration 268, loss = 0.02306845411658287
iteration 269, loss = 0.020021827891469002
iteration 270, loss = 0.018363432958722115
iteration 271, loss = 0.013950841501355171
iteration 272, loss = 0.015923788771033287
iteration 273, loss = 0.015410961583256721
iteration 274, loss = 0.015155849047005177
iteration 275, loss = 0.014079533517360687
iteration 276, loss = 0.02046217769384384
iteration 277, loss = 0.014811317436397076
iteration 278, loss = 0.01581619493663311
iteration 279, loss = 0.02076423168182373
iteration 280, loss = 0.014729651622474194
iteration 281, loss = 0.016426917165517807
iteration 282, loss = 0.015539846383035183
iteration 283, loss = 0.023216499015688896
iteration 284, loss = 0.026021763682365417
iteration 285, loss = 0.015209284611046314
iteration 286, loss = 0.015070931054651737
iteration 287, loss = 0.017420629039406776
iteration 288, loss = 0.016468504443764687
iteration 289, loss = 0.01615605130791664
iteration 290, loss = 0.014893228188157082
iteration 291, loss = 0.01476468425244093
iteration 292, loss = 0.01695232279598713
iteration 293, loss = 0.015824835747480392
iteration 294, loss = 0.017802488058805466
iteration 295, loss = 0.019689546898007393
iteration 296, loss = 0.015676461160182953
iteration 297, loss = 0.015407261438667774
iteration 298, loss = 0.017450442537665367
iteration 299, loss = 0.021486755460500717
iteration 300, loss = 0.015416082926094532
iteration 1, loss = 0.019930027425289154
iteration 2, loss = 0.019320063292980194
iteration 3, loss = 0.014206035993993282
iteration 4, loss = 0.014374013058841228
iteration 5, loss = 0.01877022720873356
iteration 6, loss = 0.019237244501709938
iteration 7, loss = 0.02332957088947296
iteration 8, loss = 0.013996390625834465
iteration 9, loss = 0.013937883079051971
iteration 10, loss = 0.022132504731416702
iteration 11, loss = 0.02073531039059162
iteration 12, loss = 0.017357580363750458
iteration 13, loss = 0.01977228932082653
iteration 14, loss = 0.016306737437844276
iteration 15, loss = 0.020035013556480408
iteration 16, loss = 0.019275937229394913
iteration 17, loss = 0.015130719169974327
iteration 18, loss = 0.024603359401226044
iteration 19, loss = 0.017648331820964813
iteration 20, loss = 0.014804542064666748
iteration 21, loss = 0.015102135948836803
iteration 22, loss = 0.015516308136284351
iteration 23, loss = 0.016917839646339417
iteration 24, loss = 0.016074983403086662
iteration 25, loss = 0.01849554106593132
iteration 26, loss = 0.020217860117554665
iteration 27, loss = 0.019720492884516716
iteration 28, loss = 0.016645431518554688
iteration 29, loss = 0.016148656606674194
iteration 30, loss = 0.017402557656168938
iteration 31, loss = 0.024991411715745926
iteration 32, loss = 0.01598045416176319
iteration 33, loss = 0.013963787816464901
iteration 34, loss = 0.014781626872718334
iteration 35, loss = 0.015064177103340626
iteration 36, loss = 0.015562298707664013
iteration 37, loss = 0.01630622148513794
iteration 38, loss = 0.01870041899383068
iteration 39, loss = 0.020635413005948067
iteration 40, loss = 0.015122152864933014
iteration 41, loss = 0.015396215952932835
iteration 42, loss = 0.01588352955877781
iteration 43, loss = 0.015980102121829987
iteration 44, loss = 0.01609089784324169
iteration 45, loss = 0.014850652776658535
iteration 46, loss = 0.016686610877513885
iteration 47, loss = 0.017487507313489914
iteration 48, loss = 0.014643477275967598
iteration 49, loss = 0.01990925334393978
iteration 50, loss = 0.023036208003759384
iteration 51, loss = 0.018713193014264107
iteration 52, loss = 0.017654191702604294
iteration 53, loss = 0.021200047805905342
iteration 54, loss = 0.015782436355948448
iteration 55, loss = 0.014889161102473736
iteration 56, loss = 0.018957339227199554
iteration 57, loss = 0.015722181648015976
iteration 58, loss = 0.01508617214858532
iteration 59, loss = 0.015610292553901672
iteration 60, loss = 0.025717681273818016
iteration 61, loss = 0.017623377963900566
iteration 62, loss = 0.014993586577475071
iteration 63, loss = 0.014895156025886536
iteration 64, loss = 0.014667683281004429
iteration 65, loss = 0.014151263050734997
iteration 66, loss = 0.01577039062976837
iteration 67, loss = 0.014662749134004116
iteration 68, loss = 0.01927275024354458
iteration 69, loss = 0.02221321314573288
iteration 70, loss = 0.0212007537484169
iteration 71, loss = 0.01890486851334572
iteration 72, loss = 0.016586624085903168
iteration 73, loss = 0.016293993219733238
iteration 74, loss = 0.01683475449681282
iteration 75, loss = 0.01594426855444908
iteration 76, loss = 0.014751745387911797
iteration 77, loss = 0.015932105481624603
iteration 78, loss = 0.015682348981499672
iteration 79, loss = 0.014163687825202942
iteration 80, loss = 0.019808365032076836
iteration 81, loss = 0.015674607828259468
iteration 82, loss = 0.020271142944693565
iteration 83, loss = 0.015600412152707577
iteration 84, loss = 0.016056140884757042
iteration 85, loss = 0.015627285465598106
iteration 86, loss = 0.014481304213404655
iteration 87, loss = 0.016373371705412865
iteration 88, loss = 0.019342869520187378
iteration 89, loss = 0.016158968210220337
iteration 90, loss = 0.01945463754236698
iteration 91, loss = 0.014902864582836628
iteration 92, loss = 0.018967101350426674
iteration 93, loss = 0.01571817323565483
iteration 94, loss = 0.015794584527611732
iteration 95, loss = 0.02129281498491764
iteration 96, loss = 0.017899315804243088
iteration 97, loss = 0.017485013231635094
iteration 98, loss = 0.015006029047071934
iteration 99, loss = 0.014503947459161282
iteration 100, loss = 0.01464352197945118
iteration 101, loss = 0.02209552749991417
iteration 102, loss = 0.0176946222782135
iteration 103, loss = 0.016092726960778236
iteration 104, loss = 0.016876133158802986
iteration 105, loss = 0.0204028133302927
iteration 106, loss = 0.016536246985197067
iteration 107, loss = 0.01738998293876648
iteration 108, loss = 0.01499172393232584
iteration 109, loss = 0.019991930574178696
iteration 110, loss = 0.016191519796848297
iteration 111, loss = 0.01588844135403633
iteration 112, loss = 0.017428334802389145
iteration 113, loss = 0.020161205902695656
iteration 114, loss = 0.0159414391964674
iteration 115, loss = 0.018747998401522636
iteration 116, loss = 0.017306338995695114
iteration 117, loss = 0.01478245947510004
iteration 118, loss = 0.016100574284791946
iteration 119, loss = 0.015782227739691734
iteration 120, loss = 0.013865561224520206
iteration 121, loss = 0.014338131994009018
iteration 122, loss = 0.015233305282890797
iteration 123, loss = 0.013757987879216671
iteration 124, loss = 0.014860640279948711
iteration 125, loss = 0.015032174065709114
iteration 126, loss = 0.01951991766691208
iteration 127, loss = 0.01782124675810337
iteration 128, loss = 0.015208093449473381
iteration 129, loss = 0.014451941475272179
iteration 130, loss = 0.015363454818725586
iteration 131, loss = 0.015871532261371613
iteration 132, loss = 0.014284288510680199
iteration 133, loss = 0.015368213877081871
iteration 134, loss = 0.017112823203206062
iteration 135, loss = 0.014276474714279175
iteration 136, loss = 0.018864791840314865
iteration 137, loss = 0.016987020149827003
iteration 138, loss = 0.020350633189082146
iteration 139, loss = 0.016712278127670288
iteration 140, loss = 0.013677594251930714
iteration 141, loss = 0.015227332711219788
iteration 142, loss = 0.01561701763421297
iteration 143, loss = 0.01504665520042181
iteration 144, loss = 0.015639666467905045
iteration 145, loss = 0.015038899146020412
iteration 146, loss = 0.024403344839811325
iteration 147, loss = 0.014525018632411957
iteration 148, loss = 0.0205137450248003
iteration 149, loss = 0.016026753932237625
iteration 150, loss = 0.017199866473674774
iteration 151, loss = 0.016533436253666878
iteration 152, loss = 0.019499540328979492
iteration 153, loss = 0.015253765508532524
iteration 154, loss = 0.02247847616672516
iteration 155, loss = 0.014104075729846954
iteration 156, loss = 0.014531616121530533
iteration 157, loss = 0.014786120504140854
iteration 158, loss = 0.019124947488307953
iteration 159, loss = 0.013785694725811481
iteration 160, loss = 0.015127849765121937
iteration 161, loss = 0.018181413412094116
iteration 162, loss = 0.014600065536797047
iteration 163, loss = 0.014204002916812897
iteration 164, loss = 0.01477472111582756
iteration 165, loss = 0.014951430261135101
iteration 166, loss = 0.01431688666343689
iteration 167, loss = 0.01498962938785553
iteration 168, loss = 0.015337341465055943
iteration 169, loss = 0.014970210380852222
iteration 170, loss = 0.019239516928792
iteration 171, loss = 0.019214268773794174
iteration 172, loss = 0.0149120157584548
iteration 173, loss = 0.014922848902642727
iteration 174, loss = 0.024819782003760338
iteration 175, loss = 0.014718025922775269
iteration 176, loss = 0.01621147245168686
iteration 177, loss = 0.019116144627332687
iteration 178, loss = 0.015325519256293774
iteration 179, loss = 0.018762359395623207
iteration 180, loss = 0.014879019930958748
iteration 181, loss = 0.0252082459628582
iteration 182, loss = 0.014832261949777603
iteration 183, loss = 0.020467184484004974
iteration 184, loss = 0.01561927329748869
iteration 185, loss = 0.01526157557964325
iteration 186, loss = 0.02032293938100338
iteration 187, loss = 0.01628166064620018
iteration 188, loss = 0.014961066655814648
iteration 189, loss = 0.019086929038167
iteration 190, loss = 0.015230724588036537
iteration 191, loss = 0.015706071630120277
iteration 192, loss = 0.01424377877265215
iteration 193, loss = 0.01571374572813511
iteration 194, loss = 0.018161660060286522
iteration 195, loss = 0.015276774764060974
iteration 196, loss = 0.013606048189103603
iteration 197, loss = 0.013787953183054924
iteration 198, loss = 0.01505249459296465
iteration 199, loss = 0.020122259855270386
iteration 200, loss = 0.016352420672774315
iteration 201, loss = 0.01947622373700142
iteration 202, loss = 0.019294824451208115
iteration 203, loss = 0.017763391137123108
iteration 204, loss = 0.015337751246988773
iteration 205, loss = 0.018044114112854004
iteration 206, loss = 0.022328760474920273
iteration 207, loss = 0.01602691039443016
iteration 208, loss = 0.014584255404770374
iteration 209, loss = 0.015527424402534962
iteration 210, loss = 0.014821413904428482
iteration 211, loss = 0.014059491455554962
iteration 212, loss = 0.019743796437978745
iteration 213, loss = 0.016158003360033035
iteration 214, loss = 0.014773158356547356
iteration 215, loss = 0.019962957128882408
iteration 216, loss = 0.015764355659484863
iteration 217, loss = 0.014453211799263954
iteration 218, loss = 0.013932734727859497
iteration 219, loss = 0.013822942040860653
iteration 220, loss = 0.014390520751476288
iteration 221, loss = 0.01746886968612671
iteration 222, loss = 0.015657760202884674
iteration 223, loss = 0.01859137788414955
iteration 224, loss = 0.020213916897773743
iteration 225, loss = 0.015127290971577168
iteration 226, loss = 0.013636255636811256
iteration 227, loss = 0.01466764323413372
iteration 228, loss = 0.013715757057070732
iteration 229, loss = 0.015332873910665512
iteration 230, loss = 0.014376248233020306
iteration 231, loss = 0.016834726557135582
iteration 232, loss = 0.015548412688076496
iteration 233, loss = 0.017397943884134293
iteration 234, loss = 0.016110951080918312
iteration 235, loss = 0.014497528783977032
iteration 236, loss = 0.014455229975283146
iteration 237, loss = 0.01889725960791111
iteration 238, loss = 0.017913661897182465
iteration 239, loss = 0.013537385500967503
iteration 240, loss = 0.013948475010693073
iteration 241, loss = 0.025268470868468285
iteration 242, loss = 0.015313150361180305
iteration 243, loss = 0.013634749688208103
iteration 244, loss = 0.014229949563741684
iteration 245, loss = 0.016565583646297455
iteration 246, loss = 0.01467298623174429
iteration 247, loss = 0.015507243573665619
iteration 248, loss = 0.020071670413017273
iteration 249, loss = 0.014139523729681969
iteration 250, loss = 0.014931686222553253
iteration 251, loss = 0.015760721638798714
iteration 252, loss = 0.014781455509364605
iteration 253, loss = 0.02457144483923912
iteration 254, loss = 0.014057478867471218
iteration 255, loss = 0.018591515719890594
iteration 256, loss = 0.01516239158809185
iteration 257, loss = 0.015677213668823242
iteration 258, loss = 0.013697080314159393
iteration 259, loss = 0.013716339133679867
iteration 260, loss = 0.013754043728113174
iteration 261, loss = 0.013770772144198418
iteration 262, loss = 0.01555195078253746
iteration 263, loss = 0.017646536231040955
iteration 264, loss = 0.014882425777614117
iteration 265, loss = 0.0157399233430624
iteration 266, loss = 0.01573391631245613
iteration 267, loss = 0.020691951736807823
iteration 268, loss = 0.014242365024983883
iteration 269, loss = 0.014957932755351067
iteration 270, loss = 0.01607777178287506
iteration 271, loss = 0.020864741876721382
iteration 272, loss = 0.0145319988951087
iteration 273, loss = 0.014912862330675125
iteration 274, loss = 0.020807668566703796
iteration 275, loss = 0.014636742882430553
iteration 276, loss = 0.01848265714943409
iteration 277, loss = 0.014744251035153866
iteration 278, loss = 0.015777379274368286
iteration 279, loss = 0.014574876055121422
iteration 280, loss = 0.015350350178778172
iteration 281, loss = 0.016407977789640427
iteration 282, loss = 0.023415787145495415
iteration 283, loss = 0.013940798118710518
iteration 284, loss = 0.021198516711592674
iteration 285, loss = 0.014545274898409843
iteration 286, loss = 0.015191671438515186
iteration 287, loss = 0.014258095994591713
iteration 288, loss = 0.014663748443126678
iteration 289, loss = 0.01873302273452282
iteration 290, loss = 0.016351643949747086
iteration 291, loss = 0.015274965204298496
iteration 292, loss = 0.014649951830506325
iteration 293, loss = 0.017930995672941208
iteration 294, loss = 0.019481079652905464
iteration 295, loss = 0.015367485582828522
iteration 296, loss = 0.01566239446401596
iteration 297, loss = 0.02379131317138672
iteration 298, loss = 0.014745651744306087
iteration 299, loss = 0.016124408692121506
iteration 300, loss = 0.013976247049868107
iteration 1, loss = 0.017673660069704056
iteration 2, loss = 0.019523948431015015
iteration 3, loss = 0.01580878533422947
iteration 4, loss = 0.014701425097882748
iteration 5, loss = 0.020020825788378716
iteration 6, loss = 0.02589583769440651
iteration 7, loss = 0.016806738451123238
iteration 8, loss = 0.013277717866003513
iteration 9, loss = 0.02450317144393921
iteration 10, loss = 0.019759012386202812
iteration 11, loss = 0.01404128409922123
iteration 12, loss = 0.01523226872086525
iteration 13, loss = 0.013813977129757404
iteration 14, loss = 0.016203435137867928
iteration 15, loss = 0.01615998148918152
iteration 16, loss = 0.013798616826534271
iteration 17, loss = 0.016939857974648476
iteration 18, loss = 0.015358135104179382
iteration 19, loss = 0.014434141106903553
iteration 20, loss = 0.015749989077448845
iteration 21, loss = 0.014265768229961395
iteration 22, loss = 0.015255728736519814
iteration 23, loss = 0.016592564061284065
iteration 24, loss = 0.019610431045293808
iteration 25, loss = 0.017891084775328636
iteration 26, loss = 0.014496620744466782
iteration 27, loss = 0.017779260873794556
iteration 28, loss = 0.01505272090435028
iteration 29, loss = 0.02344885654747486
iteration 30, loss = 0.01391696184873581
iteration 31, loss = 0.01600722037255764
iteration 32, loss = 0.014496556483209133
iteration 33, loss = 0.015514853410422802
iteration 34, loss = 0.013604594394564629
iteration 35, loss = 0.01382932998239994
iteration 36, loss = 0.013777020387351513
iteration 37, loss = 0.017683017998933792
iteration 38, loss = 0.013970539905130863
iteration 39, loss = 0.014676051214337349
iteration 40, loss = 0.021924534812569618
iteration 41, loss = 0.01557885017246008
iteration 42, loss = 0.013993372209370136
iteration 43, loss = 0.021873531863093376
iteration 44, loss = 0.019628632813692093
iteration 45, loss = 0.02387944795191288
iteration 46, loss = 0.01786206103861332
iteration 47, loss = 0.014947301708161831
iteration 48, loss = 0.013110234402120113
iteration 49, loss = 0.015120120719075203
iteration 50, loss = 0.013204041868448257
iteration 51, loss = 0.014167442917823792
iteration 52, loss = 0.013433339074254036
iteration 53, loss = 0.014402090571820736
iteration 54, loss = 0.018057063221931458
iteration 55, loss = 0.014441479928791523
iteration 56, loss = 0.014490660279989243
iteration 57, loss = 0.014088691212236881
iteration 58, loss = 0.01697145588696003
iteration 59, loss = 0.01943524368107319
iteration 60, loss = 0.013743668794631958
iteration 61, loss = 0.017779510468244553
iteration 62, loss = 0.015103177167475224
iteration 63, loss = 0.025224942713975906
iteration 64, loss = 0.015669982880353928
iteration 65, loss = 0.014291053637862206
iteration 66, loss = 0.015108653344213963
iteration 67, loss = 0.016874341294169426
iteration 68, loss = 0.014286059886217117
iteration 69, loss = 0.01699952222406864
iteration 70, loss = 0.01444747019559145
iteration 71, loss = 0.014787986874580383
iteration 72, loss = 0.015222758054733276
iteration 73, loss = 0.014960628002882004
iteration 74, loss = 0.01565423421561718
iteration 75, loss = 0.013933836482465267
iteration 76, loss = 0.014737340621650219
iteration 77, loss = 0.01699560135602951
iteration 78, loss = 0.014261975884437561
iteration 79, loss = 0.020557405427098274
iteration 80, loss = 0.014043346047401428
iteration 81, loss = 0.018156420439481735
iteration 82, loss = 0.01888720691204071
iteration 83, loss = 0.015091674402356148
iteration 84, loss = 0.013402898795902729
iteration 85, loss = 0.015600278973579407
iteration 86, loss = 0.013530424796044827
iteration 87, loss = 0.014314397238194942
iteration 88, loss = 0.018665364012122154
iteration 89, loss = 0.014205297455191612
iteration 90, loss = 0.019988777115941048
iteration 91, loss = 0.018385333940386772
iteration 92, loss = 0.013616036623716354
iteration 93, loss = 0.013676171191036701
iteration 94, loss = 0.015049576759338379
iteration 95, loss = 0.020519360899925232
iteration 96, loss = 0.0144197978079319
iteration 97, loss = 0.017845621332526207
iteration 98, loss = 0.015382032841444016
iteration 99, loss = 0.019736578688025475
iteration 100, loss = 0.015405846759676933
iteration 101, loss = 0.014605077914893627
iteration 102, loss = 0.015928156673908234
iteration 103, loss = 0.014859726652503014
iteration 104, loss = 0.015064776875078678
iteration 105, loss = 0.021848861128091812
iteration 106, loss = 0.013939030468463898
iteration 107, loss = 0.019471943378448486
iteration 108, loss = 0.013986301608383656
iteration 109, loss = 0.01460581086575985
iteration 110, loss = 0.013407466933131218
iteration 111, loss = 0.013381213881075382
iteration 112, loss = 0.013505599461495876
iteration 113, loss = 0.014321213588118553
iteration 114, loss = 0.013747290708124638
iteration 115, loss = 0.014948288910090923
iteration 116, loss = 0.01400567963719368
iteration 117, loss = 0.015197737142443657
iteration 118, loss = 0.0178871788084507
iteration 119, loss = 0.013829105533659458
iteration 120, loss = 0.013457371853291988
iteration 121, loss = 0.018080012872815132
iteration 122, loss = 0.01644536480307579
iteration 123, loss = 0.01722192019224167
iteration 124, loss = 0.015502609312534332
iteration 125, loss = 0.014034366235136986
iteration 126, loss = 0.016239017248153687
iteration 127, loss = 0.013454824686050415
iteration 128, loss = 0.01360531896352768
iteration 129, loss = 0.015793101862072945
iteration 130, loss = 0.014137474820017815
iteration 131, loss = 0.015139847062528133
iteration 132, loss = 0.01536533236503601
iteration 133, loss = 0.014484323561191559
iteration 134, loss = 0.014326217584311962
iteration 135, loss = 0.028650514781475067
iteration 136, loss = 0.02438616193830967
iteration 137, loss = 0.014627022668719292
iteration 138, loss = 0.015638813376426697
iteration 139, loss = 0.021648388355970383
iteration 140, loss = 0.014814591035246849
iteration 141, loss = 0.015718691051006317
iteration 142, loss = 0.02114914357662201
iteration 143, loss = 0.01424690056592226
iteration 144, loss = 0.015018923208117485
iteration 145, loss = 0.018360793590545654
iteration 146, loss = 0.015107973478734493
iteration 147, loss = 0.014609485864639282
iteration 148, loss = 0.018892180174589157
iteration 149, loss = 0.013911314308643341
iteration 150, loss = 0.014101194217801094
iteration 151, loss = 0.013668198138475418
iteration 152, loss = 0.015608496963977814
iteration 153, loss = 0.014426344074308872
iteration 154, loss = 0.01497221365571022
iteration 155, loss = 0.020946025848388672
iteration 156, loss = 0.014506720006465912
iteration 157, loss = 0.014314697124063969
iteration 158, loss = 0.01485533732920885
iteration 159, loss = 0.013801027089357376
iteration 160, loss = 0.01796366274356842
iteration 161, loss = 0.01822764426469803
iteration 162, loss = 0.015411091968417168
iteration 163, loss = 0.013833727687597275
iteration 164, loss = 0.01812756061553955
iteration 165, loss = 0.016468066722154617
iteration 166, loss = 0.013575529679656029
iteration 167, loss = 0.018859922885894775
iteration 168, loss = 0.014726080931723118
iteration 169, loss = 0.0188013706356287
iteration 170, loss = 0.014426198787987232
iteration 171, loss = 0.02319011278450489
iteration 172, loss = 0.015072651207447052
iteration 173, loss = 0.015608290210366249
iteration 174, loss = 0.020304057747125626
iteration 175, loss = 0.013935771770775318
iteration 176, loss = 0.017487671226263046
iteration 177, loss = 0.016575919464230537
iteration 178, loss = 0.013279760256409645
iteration 179, loss = 0.01993468776345253
iteration 180, loss = 0.015321220271289349
iteration 181, loss = 0.017897004261612892
iteration 182, loss = 0.016532592475414276
iteration 183, loss = 0.018070101737976074
iteration 184, loss = 0.022439440712332726
iteration 185, loss = 0.013504210859537125
iteration 186, loss = 0.014492729678750038
iteration 187, loss = 0.01646978035569191
iteration 188, loss = 0.020795579999685287
iteration 189, loss = 0.01432131975889206
iteration 190, loss = 0.02354721538722515
iteration 191, loss = 0.013328762724995613
iteration 192, loss = 0.01430058665573597
iteration 193, loss = 0.014872817322611809
iteration 194, loss = 0.013849731534719467
iteration 195, loss = 0.014732619747519493
iteration 196, loss = 0.014517008326947689
iteration 197, loss = 0.0137434471398592
iteration 198, loss = 0.014608941972255707
iteration 199, loss = 0.025799676775932312
iteration 200, loss = 0.015225009061396122
iteration 201, loss = 0.014778131619095802
iteration 202, loss = 0.015520216897130013
iteration 203, loss = 0.017124934121966362
iteration 204, loss = 0.013991013169288635
iteration 205, loss = 0.013309424743056297
iteration 206, loss = 0.014889388345181942
iteration 207, loss = 0.013702976517379284
iteration 208, loss = 0.014635864645242691
iteration 209, loss = 0.016399260610342026
iteration 210, loss = 0.01614338718354702
iteration 211, loss = 0.01807319186627865
iteration 212, loss = 0.01783600077033043
iteration 213, loss = 0.014424692839384079
iteration 214, loss = 0.013399490155279636
iteration 215, loss = 0.015127848833799362
iteration 216, loss = 0.01978159509599209
iteration 217, loss = 0.014755363576114178
iteration 218, loss = 0.017434637993574142
iteration 219, loss = 0.013804927468299866
iteration 220, loss = 0.017728688195347786
iteration 221, loss = 0.016991345211863518
iteration 222, loss = 0.01505069900304079
iteration 223, loss = 0.013593338429927826
iteration 224, loss = 0.01825849898159504
iteration 225, loss = 0.014329594559967518
iteration 226, loss = 0.014646083116531372
iteration 227, loss = 0.014628659002482891
iteration 228, loss = 0.013699434697628021
iteration 229, loss = 0.014916421845555305
iteration 230, loss = 0.014771847985684872
iteration 231, loss = 0.013582848012447357
iteration 232, loss = 0.015439922921359539
iteration 233, loss = 0.013361894525587559
iteration 234, loss = 0.013775672763586044
iteration 235, loss = 0.013302776962518692
iteration 236, loss = 0.01400082465261221
iteration 237, loss = 0.014879915863275528
iteration 238, loss = 0.014537185430526733
iteration 239, loss = 0.025642961263656616
iteration 240, loss = 0.013203579001128674
iteration 241, loss = 0.021561890840530396
iteration 242, loss = 0.017884759232401848
iteration 243, loss = 0.01569286175072193
iteration 244, loss = 0.01706555485725403
iteration 245, loss = 0.014933119527995586
iteration 246, loss = 0.0145726278424263
iteration 247, loss = 0.01853298768401146
iteration 248, loss = 0.013723639771342278
iteration 249, loss = 0.01431990321725607
iteration 250, loss = 0.01758955791592598
iteration 251, loss = 0.019309066236019135
iteration 252, loss = 0.014518674463033676
iteration 253, loss = 0.019293740391731262
iteration 254, loss = 0.018057025969028473
iteration 255, loss = 0.01633612997829914
iteration 256, loss = 0.01494225487112999
iteration 257, loss = 0.015408298932015896
iteration 258, loss = 0.016692625358700752
iteration 259, loss = 0.01837216503918171
iteration 260, loss = 0.013671474531292915
iteration 261, loss = 0.01905304193496704
iteration 262, loss = 0.013115021400153637
iteration 263, loss = 0.01626541279256344
iteration 264, loss = 0.019135594367980957
iteration 265, loss = 0.013641191646456718
iteration 266, loss = 0.015098595060408115
iteration 267, loss = 0.014097511768341064
iteration 268, loss = 0.014168357476592064
iteration 269, loss = 0.01938430592417717
iteration 270, loss = 0.014239617623388767
iteration 271, loss = 0.014674363657832146
iteration 272, loss = 0.016767097637057304
iteration 273, loss = 0.018963295966386795
iteration 274, loss = 0.016062047332525253
iteration 275, loss = 0.017421789467334747
iteration 276, loss = 0.01921846903860569
iteration 277, loss = 0.013524472713470459
iteration 278, loss = 0.01793217658996582
iteration 279, loss = 0.012952517718076706
iteration 280, loss = 0.021109750494360924
iteration 281, loss = 0.014103918336331844
iteration 282, loss = 0.013747490011155605
iteration 283, loss = 0.013878819532692432
iteration 284, loss = 0.014438324607908726
iteration 285, loss = 0.014539928175508976
iteration 286, loss = 0.017584677785634995
iteration 287, loss = 0.020182142034173012
iteration 288, loss = 0.018432123586535454
iteration 289, loss = 0.01429422851651907
iteration 290, loss = 0.0214102640748024
iteration 291, loss = 0.015349269844591618
iteration 292, loss = 0.016027100384235382
iteration 293, loss = 0.014204239472746849
iteration 294, loss = 0.01384885422885418
iteration 295, loss = 0.01734859310090542
iteration 296, loss = 0.01434984989464283
iteration 297, loss = 0.018675249069929123
iteration 298, loss = 0.013443473726511002
iteration 299, loss = 0.015278445556759834
iteration 300, loss = 0.014416784979403019
iteration 1, loss = 0.01604485511779785
iteration 2, loss = 0.014606150798499584
iteration 3, loss = 0.013251674361526966
iteration 4, loss = 0.01807631552219391
iteration 5, loss = 0.015782194212079048
iteration 6, loss = 0.017262069508433342
iteration 7, loss = 0.020264403894543648
iteration 8, loss = 0.015297180972993374
iteration 9, loss = 0.014550183899700642
iteration 10, loss = 0.013826288282871246
iteration 11, loss = 0.013536269776523113
iteration 12, loss = 0.017886251211166382
iteration 13, loss = 0.013757873326539993
iteration 14, loss = 0.01733742468059063
iteration 15, loss = 0.021112320944666862
iteration 16, loss = 0.013532733544707298
iteration 17, loss = 0.014745752327144146
iteration 18, loss = 0.018246036022901535
iteration 19, loss = 0.013347110711038113
iteration 20, loss = 0.013924112543463707
iteration 21, loss = 0.015303811058402061
iteration 22, loss = 0.02004358358681202
iteration 23, loss = 0.01390774268656969
iteration 24, loss = 0.018655894324183464
iteration 25, loss = 0.013278261758387089
iteration 26, loss = 0.014558959752321243
iteration 27, loss = 0.01398251298815012
iteration 28, loss = 0.017277922481298447
iteration 29, loss = 0.013886700384318829
iteration 30, loss = 0.013380054384469986
iteration 31, loss = 0.01312619261443615
iteration 32, loss = 0.024298589676618576
iteration 33, loss = 0.02856285311281681
iteration 34, loss = 0.013896477408707142
iteration 35, loss = 0.013975834473967552
iteration 36, loss = 0.01916486583650112
iteration 37, loss = 0.02114553563296795
iteration 38, loss = 0.013291711919009686
iteration 39, loss = 0.015738725662231445
iteration 40, loss = 0.01951410248875618
iteration 41, loss = 0.015350142493844032
iteration 42, loss = 0.013624098151922226
iteration 43, loss = 0.01489595789462328
iteration 44, loss = 0.014453142881393433
iteration 45, loss = 0.013966493308544159
iteration 46, loss = 0.013254374265670776
iteration 47, loss = 0.016511863097548485
iteration 48, loss = 0.015567454509437084
iteration 49, loss = 0.013872713781893253
iteration 50, loss = 0.019013117998838425
iteration 51, loss = 0.014369526877999306
iteration 52, loss = 0.015866000205278397
iteration 53, loss = 0.017386309802532196
iteration 54, loss = 0.015277796424925327
iteration 55, loss = 0.019054897129535675
iteration 56, loss = 0.01437013316899538
iteration 57, loss = 0.020291270688176155
iteration 58, loss = 0.020813431590795517
iteration 59, loss = 0.01937527023255825
iteration 60, loss = 0.015147965401411057
iteration 61, loss = 0.014256912283599377
iteration 62, loss = 0.013178491033613682
iteration 63, loss = 0.014355480670928955
iteration 64, loss = 0.014174489304423332
iteration 65, loss = 0.016425849869847298
iteration 66, loss = 0.014021012932062149
iteration 67, loss = 0.013361012563109398
iteration 68, loss = 0.014179715886712074
iteration 69, loss = 0.015567027032375336
iteration 70, loss = 0.013307777233421803
iteration 71, loss = 0.015313698910176754
iteration 72, loss = 0.02066977508366108
iteration 73, loss = 0.019641127437353134
iteration 74, loss = 0.014207283966243267
iteration 75, loss = 0.013958066701889038
iteration 76, loss = 0.019977763295173645
iteration 77, loss = 0.01783876121044159
iteration 78, loss = 0.01900305040180683
iteration 79, loss = 0.021528346464037895
iteration 80, loss = 0.015235912054777145
iteration 81, loss = 0.015241071581840515
iteration 82, loss = 0.013611972332000732
iteration 83, loss = 0.014557993970811367
iteration 84, loss = 0.01465575024485588
iteration 85, loss = 0.013306201435625553
iteration 86, loss = 0.02132786437869072
iteration 87, loss = 0.014579536393284798
iteration 88, loss = 0.019747357815504074
iteration 89, loss = 0.013382055796682835
iteration 90, loss = 0.013755767606198788
iteration 91, loss = 0.01736556738615036
iteration 92, loss = 0.015410903841257095
iteration 93, loss = 0.013422936201095581
iteration 94, loss = 0.01327261421829462
iteration 95, loss = 0.01786443591117859
iteration 96, loss = 0.01468561589717865
iteration 97, loss = 0.01851145550608635
iteration 98, loss = 0.022488653659820557
iteration 99, loss = 0.01453806646168232
iteration 100, loss = 0.014642312191426754
iteration 101, loss = 0.013683452270925045
iteration 102, loss = 0.013965528458356857
iteration 103, loss = 0.019893497228622437
iteration 104, loss = 0.014108863659203053
iteration 105, loss = 0.014927378855645657
iteration 106, loss = 0.015048665925860405
iteration 107, loss = 0.017019079998135567
iteration 108, loss = 0.01384675595909357
iteration 109, loss = 0.019147127866744995
iteration 110, loss = 0.014557198621332645
iteration 111, loss = 0.015301808714866638
iteration 112, loss = 0.016302432864904404
iteration 113, loss = 0.014306836761534214
iteration 114, loss = 0.01741994172334671
iteration 115, loss = 0.013724463060498238
iteration 116, loss = 0.020573578774929047
iteration 117, loss = 0.012563702650368214
iteration 118, loss = 0.013989409431815147
iteration 119, loss = 0.013066371902823448
iteration 120, loss = 0.01517348550260067
iteration 121, loss = 0.014897412620484829
iteration 122, loss = 0.013286886736750603
iteration 123, loss = 0.013121049851179123
iteration 124, loss = 0.0164632648229599
iteration 125, loss = 0.013703765347599983
iteration 126, loss = 0.013446355238556862
iteration 127, loss = 0.020227419212460518
iteration 128, loss = 0.013590202666819096
iteration 129, loss = 0.01358877681195736
iteration 130, loss = 0.016850708052515984
iteration 131, loss = 0.023612309247255325
iteration 132, loss = 0.02201664261519909
iteration 133, loss = 0.019165415316820145
iteration 134, loss = 0.017895445227622986
iteration 135, loss = 0.013206636533141136
iteration 136, loss = 0.01444297842681408
iteration 137, loss = 0.01966092549264431
iteration 138, loss = 0.014848602935671806
iteration 139, loss = 0.013019693084061146
iteration 140, loss = 0.018003888428211212
iteration 141, loss = 0.013493809849023819
iteration 142, loss = 0.013512997888028622
iteration 143, loss = 0.016249265521764755
iteration 144, loss = 0.013794852420687675
iteration 145, loss = 0.013512803241610527
iteration 146, loss = 0.017954235896468163
iteration 147, loss = 0.015092323534190655
iteration 148, loss = 0.014746712520718575
iteration 149, loss = 0.01503919530659914
iteration 150, loss = 0.01460861787199974
iteration 151, loss = 0.01306664664298296
iteration 152, loss = 0.015247227624058723
iteration 153, loss = 0.013859398663043976
iteration 154, loss = 0.014014472253620625
iteration 155, loss = 0.013063650578260422
iteration 156, loss = 0.01350607629865408
iteration 157, loss = 0.019937166944146156
iteration 158, loss = 0.018561413511633873
iteration 159, loss = 0.022746741771697998
iteration 160, loss = 0.0141681469976902
iteration 161, loss = 0.013802362605929375
iteration 162, loss = 0.014749143272638321
iteration 163, loss = 0.013063638471066952
iteration 164, loss = 0.014211046509444714
iteration 165, loss = 0.01323428563773632
iteration 166, loss = 0.013463267125189304
iteration 167, loss = 0.017040865495800972
iteration 168, loss = 0.01904706098139286
iteration 169, loss = 0.012998041696846485
iteration 170, loss = 0.014469231478869915
iteration 171, loss = 0.014780541881918907
iteration 172, loss = 0.016481300815939903
iteration 173, loss = 0.013013356365263462
iteration 174, loss = 0.013034005649387836
iteration 175, loss = 0.013982593081891537
iteration 176, loss = 0.013720260933041573
iteration 177, loss = 0.013003951869904995
iteration 178, loss = 0.013113484717905521
iteration 179, loss = 0.01318998634815216
iteration 180, loss = 0.012904894538223743
iteration 181, loss = 0.013515317812561989
iteration 182, loss = 0.013821626082062721
iteration 183, loss = 0.014309586957097054
iteration 184, loss = 0.013898713514208794
iteration 185, loss = 0.01878342032432556
iteration 186, loss = 0.013966698199510574
iteration 187, loss = 0.016143042594194412
iteration 188, loss = 0.015050023794174194
iteration 189, loss = 0.018301531672477722
iteration 190, loss = 0.01385523658245802
iteration 191, loss = 0.01790658012032509
iteration 192, loss = 0.014180582016706467
iteration 193, loss = 0.017650345340371132
iteration 194, loss = 0.02491253800690174
iteration 195, loss = 0.013986947014927864
iteration 196, loss = 0.01781637966632843
iteration 197, loss = 0.014065912924706936
iteration 198, loss = 0.014280153438448906
iteration 199, loss = 0.019890695810317993
iteration 200, loss = 0.01528714969754219
iteration 201, loss = 0.014586258679628372
iteration 202, loss = 0.014250730164349079
iteration 203, loss = 0.01538889016956091
iteration 204, loss = 0.01367946621030569
iteration 205, loss = 0.017148546874523163
iteration 206, loss = 0.0145389623939991
iteration 207, loss = 0.012915496714413166
iteration 208, loss = 0.013851783238351345
iteration 209, loss = 0.01265128143131733
iteration 210, loss = 0.01419081725180149
iteration 211, loss = 0.013374293223023415
iteration 212, loss = 0.020759355276823044
iteration 213, loss = 0.014040052890777588
iteration 214, loss = 0.01878182403743267
iteration 215, loss = 0.014787048101425171
iteration 216, loss = 0.012827695347368717
iteration 217, loss = 0.016057102009654045
iteration 218, loss = 0.014216329902410507
iteration 219, loss = 0.013168472796678543
iteration 220, loss = 0.014362565241754055
iteration 221, loss = 0.02073582075536251
iteration 222, loss = 0.014505489729344845
iteration 223, loss = 0.01492470595985651
iteration 224, loss = 0.0163985975086689
iteration 225, loss = 0.014820468612015247
iteration 226, loss = 0.013963660225272179
iteration 227, loss = 0.015621657483279705
iteration 228, loss = 0.01648431457579136
iteration 229, loss = 0.02016635611653328
iteration 230, loss = 0.013893334195017815
iteration 231, loss = 0.014839474111795425
iteration 232, loss = 0.01589403674006462
iteration 233, loss = 0.013674387708306313
iteration 234, loss = 0.014252157881855965
iteration 235, loss = 0.017057806253433228
iteration 236, loss = 0.015207827091217041
iteration 237, loss = 0.01927081122994423
iteration 238, loss = 0.013902503997087479
iteration 239, loss = 0.014486557804048061
iteration 240, loss = 0.021583158522844315
iteration 241, loss = 0.017589164897799492
iteration 242, loss = 0.014104502275586128
iteration 243, loss = 0.01814207248389721
iteration 244, loss = 0.017227349802851677
iteration 245, loss = 0.012607882730662823
iteration 246, loss = 0.017253205180168152
iteration 247, loss = 0.01506516057997942
iteration 248, loss = 0.014806819148361683
iteration 249, loss = 0.014646238647401333
iteration 250, loss = 0.012899749912321568
iteration 251, loss = 0.014440669678151608
iteration 252, loss = 0.013516562059521675
iteration 253, loss = 0.013601739890873432
iteration 254, loss = 0.01903390884399414
iteration 255, loss = 0.01452068891376257
iteration 256, loss = 0.013468001037836075
iteration 257, loss = 0.016401909291744232
iteration 258, loss = 0.019314425066113472
iteration 259, loss = 0.013541338965296745
iteration 260, loss = 0.014851769432425499
iteration 261, loss = 0.01380931492894888
iteration 262, loss = 0.014256478287279606
iteration 263, loss = 0.01794336549937725
iteration 264, loss = 0.012750901281833649
iteration 265, loss = 0.01618550717830658
iteration 266, loss = 0.017499033361673355
iteration 267, loss = 0.0163223035633564
iteration 268, loss = 0.017131788656115532
iteration 269, loss = 0.013782057911157608
iteration 270, loss = 0.013876010663807392
iteration 271, loss = 0.01371224969625473
iteration 272, loss = 0.01727166213095188
iteration 273, loss = 0.013882948085665703
iteration 274, loss = 0.014239339157938957
iteration 275, loss = 0.014243668876588345
iteration 276, loss = 0.014021909795701504
iteration 277, loss = 0.013166422955691814
iteration 278, loss = 0.013831968419253826
iteration 279, loss = 0.016146337613463402
iteration 280, loss = 0.020869823172688484
iteration 281, loss = 0.013237127102911472
iteration 282, loss = 0.014549444429576397
iteration 283, loss = 0.01376317534595728
iteration 284, loss = 0.013169527985155582
iteration 285, loss = 0.01314315665513277
iteration 286, loss = 0.016005760058760643
iteration 287, loss = 0.013264833018183708
iteration 288, loss = 0.016555292531847954
iteration 289, loss = 0.012668745592236519
iteration 290, loss = 0.01617918163537979
iteration 291, loss = 0.01346394419670105
iteration 292, loss = 0.014707302674651146
iteration 293, loss = 0.017783071845769882
iteration 294, loss = 0.014984238892793655
iteration 295, loss = 0.012730930931866169
iteration 296, loss = 0.01345471665263176
iteration 297, loss = 0.01513576414436102
iteration 298, loss = 0.013973170891404152
iteration 299, loss = 0.013129075057804585
iteration 300, loss = 0.013014729134738445
iteration 1, loss = 0.016805516555905342
iteration 2, loss = 0.013170076534152031
iteration 3, loss = 0.01613863930106163
iteration 4, loss = 0.019399870187044144
iteration 5, loss = 0.019370511174201965
iteration 6, loss = 0.01888258010149002
iteration 7, loss = 0.01629399135708809
iteration 8, loss = 0.012923170812427998
iteration 9, loss = 0.023896565660834312
iteration 10, loss = 0.015308074653148651
iteration 11, loss = 0.018669048324227333
iteration 12, loss = 0.015475401654839516
iteration 13, loss = 0.016180727630853653
iteration 14, loss = 0.01338811032474041
iteration 15, loss = 0.014849239960312843
iteration 16, loss = 0.014231870882213116
iteration 17, loss = 0.013482591137290001
iteration 18, loss = 0.016947785392403603
iteration 19, loss = 0.013257564045488834
iteration 20, loss = 0.019658474251627922
iteration 21, loss = 0.019269807264208794
iteration 22, loss = 0.012446497566998005
iteration 23, loss = 0.013366461731493473
iteration 24, loss = 0.0178571417927742
iteration 25, loss = 0.013733317144215107
iteration 26, loss = 0.013179527595639229
iteration 27, loss = 0.018537988886237144
iteration 28, loss = 0.019594134762883186
iteration 29, loss = 0.013663621619343758
iteration 30, loss = 0.015779826790094376
iteration 31, loss = 0.014079789631068707
iteration 32, loss = 0.016000235453248024
iteration 33, loss = 0.013227181509137154
iteration 34, loss = 0.013006319291889668
iteration 35, loss = 0.014597693458199501
iteration 36, loss = 0.01316855289041996
iteration 37, loss = 0.012437451630830765
iteration 38, loss = 0.01225232146680355
iteration 39, loss = 0.014034145511686802
iteration 40, loss = 0.01640947163105011
iteration 41, loss = 0.013578188605606556
iteration 42, loss = 0.018160279840230942
iteration 43, loss = 0.013926037587225437
iteration 44, loss = 0.013692032545804977
iteration 45, loss = 0.013331886380910873
iteration 46, loss = 0.013075330294668674
iteration 47, loss = 0.01808958873152733
iteration 48, loss = 0.013238519430160522
iteration 49, loss = 0.013927057385444641
iteration 50, loss = 0.012186866253614426
iteration 51, loss = 0.015193205326795578
iteration 52, loss = 0.017666690051555634
iteration 53, loss = 0.021807627752423286
iteration 54, loss = 0.01866891235113144
iteration 55, loss = 0.018477875739336014
iteration 56, loss = 0.013399509713053703
iteration 57, loss = 0.012287230230867863
iteration 58, loss = 0.014756297692656517
iteration 59, loss = 0.01791383884847164
iteration 60, loss = 0.013030782341957092
iteration 61, loss = 0.01827109232544899
iteration 62, loss = 0.01948322169482708
iteration 63, loss = 0.013294593431055546
iteration 64, loss = 0.016921717673540115
iteration 65, loss = 0.014942111447453499
iteration 66, loss = 0.014119409024715424
iteration 67, loss = 0.013123448938131332
iteration 68, loss = 0.01319905649870634
iteration 69, loss = 0.015036176890134811
iteration 70, loss = 0.014910699799656868
iteration 71, loss = 0.013355933129787445
iteration 72, loss = 0.012599758803844452
iteration 73, loss = 0.014506390318274498
iteration 74, loss = 0.01233510673046112
iteration 75, loss = 0.01293508242815733
iteration 76, loss = 0.015235479921102524
iteration 77, loss = 0.013974374160170555
iteration 78, loss = 0.013347499072551727
iteration 79, loss = 0.017202798277139664
iteration 80, loss = 0.0190761536359787
iteration 81, loss = 0.012889553792774677
iteration 82, loss = 0.015582425519824028
iteration 83, loss = 0.017828229814767838
iteration 84, loss = 0.013085905462503433
iteration 85, loss = 0.013508956879377365
iteration 86, loss = 0.01314790453761816
iteration 87, loss = 0.01262642815709114
iteration 88, loss = 0.01412828080356121
iteration 89, loss = 0.012856503948569298
iteration 90, loss = 0.013867609202861786
iteration 91, loss = 0.017041847109794617
iteration 92, loss = 0.015530997887253761
iteration 93, loss = 0.016055483371019363
iteration 94, loss = 0.014334062114357948
iteration 95, loss = 0.012490012682974339
iteration 96, loss = 0.016065360978245735
iteration 97, loss = 0.01696125976741314
iteration 98, loss = 0.012750960886478424
iteration 99, loss = 0.013906587846577168
iteration 100, loss = 0.013551930896937847
iteration 101, loss = 0.0207623690366745
iteration 102, loss = 0.01712249033153057
iteration 103, loss = 0.012990837916731834
iteration 104, loss = 0.015098165720701218
iteration 105, loss = 0.014428299851715565
iteration 106, loss = 0.017138708382844925
iteration 107, loss = 0.01506271306425333
iteration 108, loss = 0.019573470577597618
iteration 109, loss = 0.019793730229139328
iteration 110, loss = 0.013428108766674995
iteration 111, loss = 0.012780807912349701
iteration 112, loss = 0.012999547645449638
iteration 113, loss = 0.013795136474072933
iteration 114, loss = 0.018577730283141136
iteration 115, loss = 0.018049422651529312
iteration 116, loss = 0.01712077669799328
iteration 117, loss = 0.013366942293941975
iteration 118, loss = 0.013553520664572716
iteration 119, loss = 0.01639476791024208
iteration 120, loss = 0.01854846067726612
iteration 121, loss = 0.012974011711776257
iteration 122, loss = 0.017855800688266754
iteration 123, loss = 0.01831747218966484
iteration 124, loss = 0.013646411709487438
iteration 125, loss = 0.016156814992427826
iteration 126, loss = 0.013917080126702785
iteration 127, loss = 0.01266119722276926
iteration 128, loss = 0.013362429104745388
iteration 129, loss = 0.012859818525612354
iteration 130, loss = 0.016645081341266632
iteration 131, loss = 0.014191032387316227
iteration 132, loss = 0.014738287776708603
iteration 133, loss = 0.013798976317048073
iteration 134, loss = 0.012617051601409912
iteration 135, loss = 0.01557528879493475
iteration 136, loss = 0.01601589471101761
iteration 137, loss = 0.013368952088057995
iteration 138, loss = 0.016031702980399132
iteration 139, loss = 0.01636088266968727
iteration 140, loss = 0.01352680567651987
iteration 141, loss = 0.01830614171922207
iteration 142, loss = 0.013330378569662571
iteration 143, loss = 0.013599351048469543
iteration 144, loss = 0.015827221795916557
iteration 145, loss = 0.012728413566946983
iteration 146, loss = 0.022308344021439552
iteration 147, loss = 0.013198668137192726
iteration 148, loss = 0.013273934833705425
iteration 149, loss = 0.015378140844404697
iteration 150, loss = 0.013262152671813965
iteration 151, loss = 0.01557483896613121
iteration 152, loss = 0.014061618596315384
iteration 153, loss = 0.014244099147617817
iteration 154, loss = 0.013073110952973366
iteration 155, loss = 0.013495112769305706
iteration 156, loss = 0.014502489939332008
iteration 157, loss = 0.012279761955142021
iteration 158, loss = 0.01342392060905695
iteration 159, loss = 0.013390036299824715
iteration 160, loss = 0.017375806346535683
iteration 161, loss = 0.013963657431304455
iteration 162, loss = 0.013585278764367104
iteration 163, loss = 0.013565100729465485
iteration 164, loss = 0.012293674051761627
iteration 165, loss = 0.013496113941073418
iteration 166, loss = 0.01275070384144783
iteration 167, loss = 0.012651536613702774
iteration 168, loss = 0.013952288776636124
iteration 169, loss = 0.013829461298882961
iteration 170, loss = 0.019036078825592995
iteration 171, loss = 0.012496152892708778
iteration 172, loss = 0.012496039271354675
iteration 173, loss = 0.01733437180519104
iteration 174, loss = 0.020891893655061722
iteration 175, loss = 0.013561791740357876
iteration 176, loss = 0.013146977871656418
iteration 177, loss = 0.01763427071273327
iteration 178, loss = 0.01318982895463705
iteration 179, loss = 0.017142174765467644
iteration 180, loss = 0.011980936862528324
iteration 181, loss = 0.022208012640476227
iteration 182, loss = 0.01216922514140606
iteration 183, loss = 0.014442243613302708
iteration 184, loss = 0.01316920481622219
iteration 185, loss = 0.014769637025892735
iteration 186, loss = 0.013480720110237598
iteration 187, loss = 0.012805821374058723
iteration 188, loss = 0.012546021491289139
iteration 189, loss = 0.023801244795322418
iteration 190, loss = 0.012794386595487595
iteration 191, loss = 0.013355957344174385
iteration 192, loss = 0.01839229092001915
iteration 193, loss = 0.013410796411335468
iteration 194, loss = 0.013307798653841019
iteration 195, loss = 0.016489345580339432
iteration 196, loss = 0.01390653382986784
iteration 197, loss = 0.012792465277016163
iteration 198, loss = 0.013923866674304008
iteration 199, loss = 0.013355158269405365
iteration 200, loss = 0.012495492585003376
iteration 201, loss = 0.012140149250626564
iteration 202, loss = 0.01590849459171295
iteration 203, loss = 0.01725565828382969
iteration 204, loss = 0.014402010478079319
iteration 205, loss = 0.01301377173513174
iteration 206, loss = 0.012380539439618587
iteration 207, loss = 0.014218496158719063
iteration 208, loss = 0.016091085970401764
iteration 209, loss = 0.018232369795441628
iteration 210, loss = 0.01727887988090515
iteration 211, loss = 0.012701340951025486
iteration 212, loss = 0.012823518365621567
iteration 213, loss = 0.016051582992076874
iteration 214, loss = 0.013963240198791027
iteration 215, loss = 0.013234683312475681
iteration 216, loss = 0.01452399417757988
iteration 217, loss = 0.012919873930513859
iteration 218, loss = 0.013917677104473114
iteration 219, loss = 0.02049846015870571
iteration 220, loss = 0.017043337225914
iteration 221, loss = 0.012385447509586811
iteration 222, loss = 0.01338427234441042
iteration 223, loss = 0.01266215555369854
iteration 224, loss = 0.020822394639253616
iteration 225, loss = 0.014755915850400925
iteration 226, loss = 0.016108136624097824
iteration 227, loss = 0.015534354373812675
iteration 228, loss = 0.012476002797484398
iteration 229, loss = 0.014080312103033066
iteration 230, loss = 0.014132771641016006
iteration 231, loss = 0.012429802678525448
iteration 232, loss = 0.013404094614088535
iteration 233, loss = 0.021866073831915855
iteration 234, loss = 0.01875932514667511
iteration 235, loss = 0.016938569024205208
iteration 236, loss = 0.01708345301449299
iteration 237, loss = 0.013460088521242142
iteration 238, loss = 0.017306415364146233
iteration 239, loss = 0.01517866924405098
iteration 240, loss = 0.016780896112322807
iteration 241, loss = 0.013332834467291832
iteration 242, loss = 0.016945730894804
iteration 243, loss = 0.014119885861873627
iteration 244, loss = 0.016756173223257065
iteration 245, loss = 0.012946844100952148
iteration 246, loss = 0.017393318936228752
iteration 247, loss = 0.01756901480257511
iteration 248, loss = 0.013148481957614422
iteration 249, loss = 0.012340164743363857
iteration 250, loss = 0.0186568982899189
iteration 251, loss = 0.013661753386259079
iteration 252, loss = 0.017185399308800697
iteration 253, loss = 0.013623274862766266
iteration 254, loss = 0.013544184155762196
iteration 255, loss = 0.014715509489178658
iteration 256, loss = 0.01340076606720686
iteration 257, loss = 0.01371319405734539
iteration 258, loss = 0.018522262573242188
iteration 259, loss = 0.0172647163271904
iteration 260, loss = 0.01255291048437357
iteration 261, loss = 0.012585184536874294
iteration 262, loss = 0.01784956268966198
iteration 263, loss = 0.01426774449646473
iteration 264, loss = 0.017179859802126884
iteration 265, loss = 0.015858691185712814
iteration 266, loss = 0.014717700891196728
iteration 267, loss = 0.013271278701722622
iteration 268, loss = 0.013509671203792095
iteration 269, loss = 0.016556065529584885
iteration 270, loss = 0.014318209141492844
iteration 271, loss = 0.013870890252292156
iteration 272, loss = 0.014283828437328339
iteration 273, loss = 0.012840590439736843
iteration 274, loss = 0.013802267611026764
iteration 275, loss = 0.014632697217166424
iteration 276, loss = 0.01330449990928173
iteration 277, loss = 0.018640173599123955
iteration 278, loss = 0.01699414849281311
iteration 279, loss = 0.013572022318840027
iteration 280, loss = 0.012787937186658382
iteration 281, loss = 0.013919328339397907
iteration 282, loss = 0.0160677433013916
iteration 283, loss = 0.02027532272040844
iteration 284, loss = 0.013182471506297588
iteration 285, loss = 0.013390269130468369
iteration 286, loss = 0.013097106479108334
iteration 287, loss = 0.013833589851856232
iteration 288, loss = 0.013197866268455982
iteration 289, loss = 0.012972069904208183
iteration 290, loss = 0.017612162977457047
iteration 291, loss = 0.014975566416978836
iteration 292, loss = 0.013604290783405304
iteration 293, loss = 0.012751370668411255
iteration 294, loss = 0.012375974096357822
iteration 295, loss = 0.023524988442659378
iteration 296, loss = 0.017116481438279152
iteration 297, loss = 0.013667809776961803
iteration 298, loss = 0.012642739340662956
iteration 299, loss = 0.013604818843305111
iteration 300, loss = 0.01670299470424652
iteration 1, loss = 0.01405474916100502
iteration 2, loss = 0.025716127827763557
iteration 3, loss = 0.01460812333971262
iteration 4, loss = 0.01818171702325344
iteration 5, loss = 0.013563727028667927
iteration 6, loss = 0.0128542585298419
iteration 7, loss = 0.016710830852389336
iteration 8, loss = 0.013335227966308594
iteration 9, loss = 0.01239765901118517
iteration 10, loss = 0.013584195636212826
iteration 11, loss = 0.013881436549127102
iteration 12, loss = 0.01539622899144888
iteration 13, loss = 0.013645072467625141
iteration 14, loss = 0.014257444068789482
iteration 15, loss = 0.0207059383392334
iteration 16, loss = 0.01384715922176838
iteration 17, loss = 0.016054050996899605
iteration 18, loss = 0.014048323966562748
iteration 19, loss = 0.01905856654047966
iteration 20, loss = 0.01873653568327427
iteration 21, loss = 0.01541243027895689
iteration 22, loss = 0.01679728552699089
iteration 23, loss = 0.013636988587677479
iteration 24, loss = 0.019372545182704926
iteration 25, loss = 0.012652910314500332
iteration 26, loss = 0.01684952899813652
iteration 27, loss = 0.012509633786976337
iteration 28, loss = 0.013174875639379025
iteration 29, loss = 0.014051268808543682
iteration 30, loss = 0.012328977696597576
iteration 31, loss = 0.01265548262745142
iteration 32, loss = 0.014986492693424225
iteration 33, loss = 0.013239339925348759
iteration 34, loss = 0.017095113173127174
iteration 35, loss = 0.01252170093357563
iteration 36, loss = 0.013494623824954033
iteration 37, loss = 0.017140083014965057
iteration 38, loss = 0.013752272352576256
iteration 39, loss = 0.012719494290649891
iteration 40, loss = 0.01915808767080307
iteration 41, loss = 0.015347621403634548
iteration 42, loss = 0.015401274897158146
iteration 43, loss = 0.014049955643713474
iteration 44, loss = 0.01557763759046793
iteration 45, loss = 0.016520559787750244
iteration 46, loss = 0.013577737845480442
iteration 47, loss = 0.014238749630749226
iteration 48, loss = 0.01196989230811596
iteration 49, loss = 0.01682763732969761
iteration 50, loss = 0.013332806527614594
iteration 51, loss = 0.017788924276828766
iteration 52, loss = 0.017477603629231453
iteration 53, loss = 0.013197176158428192
iteration 54, loss = 0.012715807184576988
iteration 55, loss = 0.012435944750905037
iteration 56, loss = 0.013352197594940662
iteration 57, loss = 0.015844538807868958
iteration 58, loss = 0.012022288516163826
iteration 59, loss = 0.013648655265569687
iteration 60, loss = 0.012196458876132965
iteration 61, loss = 0.012145785614848137
iteration 62, loss = 0.013940841890871525
iteration 63, loss = 0.01243197824805975
iteration 64, loss = 0.013195987790822983
iteration 65, loss = 0.016829246655106544
iteration 66, loss = 0.015721658244729042
iteration 67, loss = 0.013667990453541279
iteration 68, loss = 0.01652752235531807
iteration 69, loss = 0.013877244666218758
iteration 70, loss = 0.02113577164709568
iteration 71, loss = 0.015932215377688408
iteration 72, loss = 0.021990356966853142
iteration 73, loss = 0.012986765243113041
iteration 74, loss = 0.012851689010858536
iteration 75, loss = 0.013243998400866985
iteration 76, loss = 0.017398271709680557
iteration 77, loss = 0.011883636936545372
iteration 78, loss = 0.0140617610886693
iteration 79, loss = 0.020266616716980934
iteration 80, loss = 0.014507211744785309
iteration 81, loss = 0.012975758872926235
iteration 82, loss = 0.012873333878815174
iteration 83, loss = 0.013405381701886654
iteration 84, loss = 0.012959305197000504
iteration 85, loss = 0.01201413944363594
iteration 86, loss = 0.016706185415387154
iteration 87, loss = 0.013577774167060852
iteration 88, loss = 0.01691395230591297
iteration 89, loss = 0.01270519569516182
iteration 90, loss = 0.011853653006255627
iteration 91, loss = 0.014216877520084381
iteration 92, loss = 0.013119632378220558
iteration 93, loss = 0.012603072449564934
iteration 94, loss = 0.012080663815140724
iteration 95, loss = 0.014528393745422363
iteration 96, loss = 0.012535564601421356
iteration 97, loss = 0.016180777922272682
iteration 98, loss = 0.016057830303907394
iteration 99, loss = 0.018078651279211044
iteration 100, loss = 0.01959049329161644
iteration 101, loss = 0.013629216700792313
iteration 102, loss = 0.01294703222811222
iteration 103, loss = 0.014895148575305939
iteration 104, loss = 0.012460516765713692
iteration 105, loss = 0.013639586977660656
iteration 106, loss = 0.014679682441055775
iteration 107, loss = 0.01262540090829134
iteration 108, loss = 0.011942599900066853
iteration 109, loss = 0.014957182109355927
iteration 110, loss = 0.012998919934034348
iteration 111, loss = 0.013328410685062408
iteration 112, loss = 0.017521487548947334
iteration 113, loss = 0.016574431210756302
iteration 114, loss = 0.014006809331476688
iteration 115, loss = 0.012757227756083012
iteration 116, loss = 0.012380700558423996
iteration 117, loss = 0.012058589607477188
iteration 118, loss = 0.01605038531124592
iteration 119, loss = 0.013006044551730156
iteration 120, loss = 0.013729482889175415
iteration 121, loss = 0.015218163840472698
iteration 122, loss = 0.0158933587372303
iteration 123, loss = 0.012743371538817883
iteration 124, loss = 0.012226060032844543
iteration 125, loss = 0.012615975923836231
iteration 126, loss = 0.013398515991866589
iteration 127, loss = 0.015070944093167782
iteration 128, loss = 0.01799636334180832
iteration 129, loss = 0.0130259208381176
iteration 130, loss = 0.014124969951808453
iteration 131, loss = 0.013235470280051231
iteration 132, loss = 0.017454076558351517
iteration 133, loss = 0.01815616711974144
iteration 134, loss = 0.01793302781879902
iteration 135, loss = 0.012486927211284637
iteration 136, loss = 0.012782016769051552
iteration 137, loss = 0.015355929732322693
iteration 138, loss = 0.018096189945936203
iteration 139, loss = 0.02301710471510887
iteration 140, loss = 0.013035383075475693
iteration 141, loss = 0.013291061855852604
iteration 142, loss = 0.016862638294696808
iteration 143, loss = 0.013115513138473034
iteration 144, loss = 0.015894228592514992
iteration 145, loss = 0.013529077172279358
iteration 146, loss = 0.014161374419927597
iteration 147, loss = 0.013802861794829369
iteration 148, loss = 0.011976641602814198
iteration 149, loss = 0.016193363815546036
iteration 150, loss = 0.011952204629778862
iteration 151, loss = 0.014502570033073425
iteration 152, loss = 0.01622791774570942
iteration 153, loss = 0.011873439885675907
iteration 154, loss = 0.012527230195701122
iteration 155, loss = 0.01833152212202549
iteration 156, loss = 0.016766943037509918
iteration 157, loss = 0.01300649344921112
iteration 158, loss = 0.013872227631509304
iteration 159, loss = 0.013530156575143337
iteration 160, loss = 0.012240261770784855
iteration 161, loss = 0.012528762221336365
iteration 162, loss = 0.01585363782942295
iteration 163, loss = 0.017992636188864708
iteration 164, loss = 0.01917094737291336
iteration 165, loss = 0.01402132585644722
iteration 166, loss = 0.0206599161028862
iteration 167, loss = 0.013522384688258171
iteration 168, loss = 0.01701575703918934
iteration 169, loss = 0.013973532244563103
iteration 170, loss = 0.012591227889060974
iteration 171, loss = 0.018688082695007324
iteration 172, loss = 0.01779228448867798
iteration 173, loss = 0.013062277808785439
iteration 174, loss = 0.014546658843755722
iteration 175, loss = 0.016213519498705864
iteration 176, loss = 0.016552947461605072
iteration 177, loss = 0.012751701287925243
iteration 178, loss = 0.012632650323212147
iteration 179, loss = 0.01683335192501545
iteration 180, loss = 0.016832426190376282
iteration 181, loss = 0.011989625170826912
iteration 182, loss = 0.012390063144266605
iteration 183, loss = 0.013245922513306141
iteration 184, loss = 0.013260886073112488
iteration 185, loss = 0.014746368862688541
iteration 186, loss = 0.016530262306332588
iteration 187, loss = 0.014347007498145103
iteration 188, loss = 0.012980650179088116
iteration 189, loss = 0.01498416531831026
iteration 190, loss = 0.01296987384557724
iteration 191, loss = 0.016717534512281418
iteration 192, loss = 0.01343953050673008
iteration 193, loss = 0.013020367361605167
iteration 194, loss = 0.016371548175811768
iteration 195, loss = 0.01187836192548275
iteration 196, loss = 0.013120884075760841
iteration 197, loss = 0.020200064405798912
iteration 198, loss = 0.016556818038225174
iteration 199, loss = 0.013510181568562984
iteration 200, loss = 0.01619115099310875
iteration 201, loss = 0.012787843123078346
iteration 202, loss = 0.014576459303498268
iteration 203, loss = 0.011797401122748852
iteration 204, loss = 0.013089965097606182
iteration 205, loss = 0.01300971768796444
iteration 206, loss = 0.01216062717139721
iteration 207, loss = 0.013227831572294235
iteration 208, loss = 0.017108352854847908
iteration 209, loss = 0.01243608072400093
iteration 210, loss = 0.013647196814417839
iteration 211, loss = 0.013438434340059757
iteration 212, loss = 0.017992787063121796
iteration 213, loss = 0.014721808955073357
iteration 214, loss = 0.014849398285150528
iteration 215, loss = 0.012101828120648861
iteration 216, loss = 0.012478510849177837
iteration 217, loss = 0.012707903981208801
iteration 218, loss = 0.020305640995502472
iteration 219, loss = 0.013061905279755592
iteration 220, loss = 0.012869482859969139
iteration 221, loss = 0.013945666141808033
iteration 222, loss = 0.013511392287909985
iteration 223, loss = 0.01248119119554758
iteration 224, loss = 0.013067895546555519
iteration 225, loss = 0.012596896849572659
iteration 226, loss = 0.01464143954217434
iteration 227, loss = 0.01642623171210289
iteration 228, loss = 0.01484671514481306
iteration 229, loss = 0.017021499574184418
iteration 230, loss = 0.016743358224630356
iteration 231, loss = 0.011849995702505112
iteration 232, loss = 0.012593129649758339
iteration 233, loss = 0.013663288205862045
iteration 234, loss = 0.01155197061598301
iteration 235, loss = 0.012273826636373997
iteration 236, loss = 0.02204032428562641
iteration 237, loss = 0.012669793330132961
iteration 238, loss = 0.012092787772417068
iteration 239, loss = 0.01209193654358387
iteration 240, loss = 0.013640882447361946
iteration 241, loss = 0.01675885170698166
iteration 242, loss = 0.017432179301977158
iteration 243, loss = 0.013968176208436489
iteration 244, loss = 0.020233135670423508
iteration 245, loss = 0.015942750498652458
iteration 246, loss = 0.018158845603466034
iteration 247, loss = 0.012540832161903381
iteration 248, loss = 0.012803608551621437
iteration 249, loss = 0.013240130618214607
iteration 250, loss = 0.012601671740412712
iteration 251, loss = 0.013333901762962341
iteration 252, loss = 0.012981179170310497
iteration 253, loss = 0.012962316162884235
iteration 254, loss = 0.013221223838627338
iteration 255, loss = 0.012478078715503216
iteration 256, loss = 0.013062725774943829
iteration 257, loss = 0.019950021058321
iteration 258, loss = 0.012351647950708866
iteration 259, loss = 0.013776518404483795
iteration 260, loss = 0.01269371248781681
iteration 261, loss = 0.012654377147555351
iteration 262, loss = 0.013540665619075298
iteration 263, loss = 0.017473148182034492
iteration 264, loss = 0.012695426121354103
iteration 265, loss = 0.012930641882121563
iteration 266, loss = 0.0133946118876338
iteration 267, loss = 0.013265354558825493
iteration 268, loss = 0.013011384755373001
iteration 269, loss = 0.013318263925611973
iteration 270, loss = 0.01578887365758419
iteration 271, loss = 0.011631307192146778
iteration 272, loss = 0.013184897601604462
iteration 273, loss = 0.013037063181400299
iteration 274, loss = 0.014647972770035267
iteration 275, loss = 0.01341894082725048
iteration 276, loss = 0.01611916348338127
iteration 277, loss = 0.013540629297494888
iteration 278, loss = 0.013678215444087982
iteration 279, loss = 0.011759055778384209
iteration 280, loss = 0.011893419548869133
iteration 281, loss = 0.017938686534762383
iteration 282, loss = 0.01606311835348606
iteration 283, loss = 0.016990287229418755
iteration 284, loss = 0.012399449944496155
iteration 285, loss = 0.01649179682135582
iteration 286, loss = 0.01343401800841093
iteration 287, loss = 0.013375652022659779
iteration 288, loss = 0.012349140830338001
iteration 289, loss = 0.016236547380685806
iteration 290, loss = 0.012550956569612026
iteration 291, loss = 0.01498926430940628
iteration 292, loss = 0.013021002523601055
iteration 293, loss = 0.01378941535949707
iteration 294, loss = 0.013324782252311707
iteration 295, loss = 0.012479799799621105
iteration 296, loss = 0.015076340176165104
iteration 297, loss = 0.011461216025054455
iteration 298, loss = 0.021000057458877563
iteration 299, loss = 0.012450279667973518
iteration 300, loss = 0.013211408630013466
iteration 1, loss = 0.012539699673652649
iteration 2, loss = 0.012175611220300198
iteration 3, loss = 0.016639703884720802
iteration 4, loss = 0.013176703825592995
iteration 5, loss = 0.013457417488098145
iteration 6, loss = 0.01491617877036333
iteration 7, loss = 0.014051919803023338
iteration 8, loss = 0.012462303973734379
iteration 9, loss = 0.012846969068050385
iteration 10, loss = 0.012287581339478493
iteration 11, loss = 0.015722651034593582
iteration 12, loss = 0.012141641229391098
iteration 13, loss = 0.012590804137289524
iteration 14, loss = 0.012897642329335213
iteration 15, loss = 0.01393741462379694
iteration 16, loss = 0.012216615490615368
iteration 17, loss = 0.012377348728477955
iteration 18, loss = 0.01302136667072773
iteration 19, loss = 0.017139609903097153
iteration 20, loss = 0.015789251774549484
iteration 21, loss = 0.013051236048340797
iteration 22, loss = 0.01617772877216339
iteration 23, loss = 0.014459403231739998
iteration 24, loss = 0.01587260700762272
iteration 25, loss = 0.01292959414422512
iteration 26, loss = 0.012462150305509567
iteration 27, loss = 0.014483703300356865
iteration 28, loss = 0.012554250657558441
iteration 29, loss = 0.016178922727704048
iteration 30, loss = 0.012956296093761921
iteration 31, loss = 0.0148784713819623
iteration 32, loss = 0.015075977891683578
iteration 33, loss = 0.011717348359525204
iteration 34, loss = 0.01805885322391987
iteration 35, loss = 0.012391837313771248
iteration 36, loss = 0.01242970209568739
iteration 37, loss = 0.01149655506014824
iteration 38, loss = 0.0167155172675848
iteration 39, loss = 0.015660276636481285
iteration 40, loss = 0.013810645788908005
iteration 41, loss = 0.013186413794755936
iteration 42, loss = 0.018251990899443626
iteration 43, loss = 0.022269200533628464
iteration 44, loss = 0.011894317343831062
iteration 45, loss = 0.013615493662655354
iteration 46, loss = 0.012887821532785892
iteration 47, loss = 0.01519422885030508
iteration 48, loss = 0.013523506000638008
iteration 49, loss = 0.012775545008480549
iteration 50, loss = 0.01252434030175209
iteration 51, loss = 0.017622750252485275
iteration 52, loss = 0.016614330932497978
iteration 53, loss = 0.016702786087989807
iteration 54, loss = 0.01200854405760765
iteration 55, loss = 0.013167493045330048
iteration 56, loss = 0.013095656409859657
iteration 57, loss = 0.012601744383573532
iteration 58, loss = 0.01370842382311821
iteration 59, loss = 0.015453939326107502
iteration 60, loss = 0.012372229248285294
iteration 61, loss = 0.013736206106841564
iteration 62, loss = 0.012723643332719803
iteration 63, loss = 0.013468364253640175
iteration 64, loss = 0.01680183596909046
iteration 65, loss = 0.018432846292853355
iteration 66, loss = 0.013492850586771965
iteration 67, loss = 0.011855081655085087
iteration 68, loss = 0.012542317621409893
iteration 69, loss = 0.014328954741358757
iteration 70, loss = 0.011678018607199192
iteration 71, loss = 0.013235747814178467
iteration 72, loss = 0.016713876277208328
iteration 73, loss = 0.014022349379956722
iteration 74, loss = 0.014577939175069332
iteration 75, loss = 0.013278564438223839
iteration 76, loss = 0.017791545018553734
iteration 77, loss = 0.012342464178800583
iteration 78, loss = 0.012152111157774925
iteration 79, loss = 0.017724495381116867
iteration 80, loss = 0.012880254536867142
iteration 81, loss = 0.016750570386648178
iteration 82, loss = 0.012439819052815437
iteration 83, loss = 0.012498314492404461
iteration 84, loss = 0.01766049675643444
iteration 85, loss = 0.012746265158057213
iteration 86, loss = 0.012394840829074383
iteration 87, loss = 0.012060126289725304
iteration 88, loss = 0.012564212083816528
iteration 89, loss = 0.015860892832279205
iteration 90, loss = 0.016523726284503937
iteration 91, loss = 0.014637632295489311
iteration 92, loss = 0.017781196162104607
iteration 93, loss = 0.01575973629951477
iteration 94, loss = 0.016930563375353813
iteration 95, loss = 0.01761222817003727
iteration 96, loss = 0.013658205978572369
iteration 97, loss = 0.015569315291941166
iteration 98, loss = 0.012275446206331253
iteration 99, loss = 0.011670999228954315
iteration 100, loss = 0.013598554767668247
iteration 101, loss = 0.011590416543185711
iteration 102, loss = 0.016064448282122612
iteration 103, loss = 0.012497641146183014
iteration 104, loss = 0.01285936776548624
iteration 105, loss = 0.015539341606199741
iteration 106, loss = 0.012256837449967861
iteration 107, loss = 0.018748050555586815
iteration 108, loss = 0.012300106696784496
iteration 109, loss = 0.01657806523144245
iteration 110, loss = 0.012749116867780685
iteration 111, loss = 0.013174532912671566
iteration 112, loss = 0.012510431930422783
iteration 113, loss = 0.013002031482756138
iteration 114, loss = 0.011565659195184708
iteration 115, loss = 0.011630196124315262
iteration 116, loss = 0.01427857019007206
iteration 117, loss = 0.016781670972704887
iteration 118, loss = 0.014507008716464043
iteration 119, loss = 0.011845503933727741
iteration 120, loss = 0.013193964958190918
iteration 121, loss = 0.011612012051045895
iteration 122, loss = 0.014835471287369728
iteration 123, loss = 0.011960922740399837
iteration 124, loss = 0.01754135638475418
iteration 125, loss = 0.013853086158633232
iteration 126, loss = 0.01239398866891861
iteration 127, loss = 0.016076643019914627
iteration 128, loss = 0.016710402444005013
iteration 129, loss = 0.012460591271519661
iteration 130, loss = 0.015928108245134354
iteration 131, loss = 0.01901075430214405
iteration 132, loss = 0.015025950968265533
iteration 133, loss = 0.01239048782736063
iteration 134, loss = 0.012761829420924187
iteration 135, loss = 0.013694040477275848
iteration 136, loss = 0.016095567494630814
iteration 137, loss = 0.012653522193431854
iteration 138, loss = 0.015062449499964714
iteration 139, loss = 0.01244022510945797
iteration 140, loss = 0.016642222180962563
iteration 141, loss = 0.012150678783655167
iteration 142, loss = 0.014039230532944202
iteration 143, loss = 0.01771928369998932
iteration 144, loss = 0.013168412260711193
iteration 145, loss = 0.011961597949266434
iteration 146, loss = 0.01209255401045084
iteration 147, loss = 0.012499689124524593
iteration 148, loss = 0.013137530535459518
iteration 149, loss = 0.01324909832328558
iteration 150, loss = 0.01303146779537201
iteration 151, loss = 0.012111958116292953
iteration 152, loss = 0.012417377904057503
iteration 153, loss = 0.017734752967953682
iteration 154, loss = 0.012915251776576042
iteration 155, loss = 0.015942521393299103
iteration 156, loss = 0.012674115598201752
iteration 157, loss = 0.017148349434137344
iteration 158, loss = 0.013220071792602539
iteration 159, loss = 0.014319957233965397
iteration 160, loss = 0.011977974325418472
iteration 161, loss = 0.012475188821554184
iteration 162, loss = 0.012898895889520645
iteration 163, loss = 0.013104764744639397
iteration 164, loss = 0.012059175409376621
iteration 165, loss = 0.01724671944975853
iteration 166, loss = 0.01679825410246849
iteration 167, loss = 0.014804058708250523
iteration 168, loss = 0.012895813211798668
iteration 169, loss = 0.019970610737800598
iteration 170, loss = 0.01487955916672945
iteration 171, loss = 0.01593213714659214
iteration 172, loss = 0.011823379434645176
iteration 173, loss = 0.01139040943235159
iteration 174, loss = 0.011989632621407509
iteration 175, loss = 0.017726656049489975
iteration 176, loss = 0.015103588812053204
iteration 177, loss = 0.012310932390391827
iteration 178, loss = 0.01847871020436287
iteration 179, loss = 0.016352947801351547
iteration 180, loss = 0.012180879712104797
iteration 181, loss = 0.014498952776193619
iteration 182, loss = 0.013485701754689217
iteration 183, loss = 0.011563489213585854
iteration 184, loss = 0.017074456438422203
iteration 185, loss = 0.011537148617208004
iteration 186, loss = 0.01607188768684864
iteration 187, loss = 0.012524991296231747
iteration 188, loss = 0.0204172320663929
iteration 189, loss = 0.013132942840456963
iteration 190, loss = 0.01292731985449791
iteration 191, loss = 0.012191756628453732
iteration 192, loss = 0.012404417619109154
iteration 193, loss = 0.014310668222606182
iteration 194, loss = 0.013537484221160412
iteration 195, loss = 0.017302049323916435
iteration 196, loss = 0.013268450275063515
iteration 197, loss = 0.015440329909324646
iteration 198, loss = 0.012529737316071987
iteration 199, loss = 0.011805260553956032
iteration 200, loss = 0.011859153397381306
iteration 201, loss = 0.013446951285004616
iteration 202, loss = 0.01630166918039322
iteration 203, loss = 0.013177622109651566
iteration 204, loss = 0.012042487040162086
iteration 205, loss = 0.012638559564948082
iteration 206, loss = 0.011686590500175953
iteration 207, loss = 0.017570173367857933
iteration 208, loss = 0.012098900973796844
iteration 209, loss = 0.01196044310927391
iteration 210, loss = 0.012383932247757912
iteration 211, loss = 0.01338727306574583
iteration 212, loss = 0.012067912146449089
iteration 213, loss = 0.0123892892152071
iteration 214, loss = 0.012859095819294453
iteration 215, loss = 0.01773325726389885
iteration 216, loss = 0.015941724181175232
iteration 217, loss = 0.01608530431985855
iteration 218, loss = 0.013100150972604752
iteration 219, loss = 0.015075089409947395
iteration 220, loss = 0.011757826432585716
iteration 221, loss = 0.01200755126774311
iteration 222, loss = 0.012293711304664612
iteration 223, loss = 0.011663448065519333
iteration 224, loss = 0.01691405288875103
iteration 225, loss = 0.021952008828520775
iteration 226, loss = 0.013668464496731758
iteration 227, loss = 0.01836603321135044
iteration 228, loss = 0.01266091875731945
iteration 229, loss = 0.01838110201060772
iteration 230, loss = 0.011821559630334377
iteration 231, loss = 0.014738202095031738
iteration 232, loss = 0.012292037717998028
iteration 233, loss = 0.012896105647087097
iteration 234, loss = 0.015835803002119064
iteration 235, loss = 0.013080201111733913
iteration 236, loss = 0.016723012551665306
iteration 237, loss = 0.011744983494281769
iteration 238, loss = 0.02007981389760971
iteration 239, loss = 0.011972487904131413
iteration 240, loss = 0.017669564113020897
iteration 241, loss = 0.012627895921468735
iteration 242, loss = 0.013635825365781784
iteration 243, loss = 0.016360148787498474
iteration 244, loss = 0.016813425347208977
iteration 245, loss = 0.013441598042845726
iteration 246, loss = 0.012466156855225563
iteration 247, loss = 0.01227619219571352
iteration 248, loss = 0.011037811636924744
iteration 249, loss = 0.012958725914359093
iteration 250, loss = 0.013005716726183891
iteration 251, loss = 0.012257158756256104
iteration 252, loss = 0.015450263395905495
iteration 253, loss = 0.01583915948867798
iteration 254, loss = 0.015842128545045853
iteration 255, loss = 0.01555672287940979
iteration 256, loss = 0.015660107135772705
iteration 257, loss = 0.011627023108303547
iteration 258, loss = 0.016665197908878326
iteration 259, loss = 0.012683464214205742
iteration 260, loss = 0.012704912573099136
iteration 261, loss = 0.013204239308834076
iteration 262, loss = 0.011255488730967045
iteration 263, loss = 0.01259821467101574
iteration 264, loss = 0.017847716808319092
iteration 265, loss = 0.012293234467506409
iteration 266, loss = 0.012281173840165138
iteration 267, loss = 0.013455377891659737
iteration 268, loss = 0.011692782863974571
iteration 269, loss = 0.011650013737380505
iteration 270, loss = 0.015212798491120338
iteration 271, loss = 0.013734797015786171
iteration 272, loss = 0.012394283898174763
iteration 273, loss = 0.013396063819527626
iteration 274, loss = 0.015792226418852806
iteration 275, loss = 0.01720063015818596
iteration 276, loss = 0.015148649923503399
iteration 277, loss = 0.01138347014784813
iteration 278, loss = 0.014298485592007637
iteration 279, loss = 0.013252179138362408
iteration 280, loss = 0.016065407544374466
iteration 281, loss = 0.013369543477892876
iteration 282, loss = 0.011433357372879982
iteration 283, loss = 0.015532204881310463
iteration 284, loss = 0.015908198431134224
iteration 285, loss = 0.012532902881503105
iteration 286, loss = 0.012798462994396687
iteration 287, loss = 0.01480958517640829
iteration 288, loss = 0.011988985352218151
iteration 289, loss = 0.012110641226172447
iteration 290, loss = 0.011768714524805546
iteration 291, loss = 0.012806279584765434
iteration 292, loss = 0.01171130407601595
iteration 293, loss = 0.012583945877850056
iteration 294, loss = 0.011774148792028427
iteration 295, loss = 0.013752259314060211
iteration 296, loss = 0.01250160951167345
iteration 297, loss = 0.012929439544677734
iteration 298, loss = 0.014229102991521358
iteration 299, loss = 0.015416600741446018
iteration 300, loss = 0.016836654394865036
iteration 1, loss = 0.017908602952957153
iteration 2, loss = 0.012248259969055653
iteration 3, loss = 0.01374108623713255
iteration 4, loss = 0.01170770637691021
iteration 5, loss = 0.014676456339657307
iteration 6, loss = 0.019050350412726402
iteration 7, loss = 0.013530165888369083
iteration 8, loss = 0.014874963089823723
iteration 9, loss = 0.011267370544373989
iteration 10, loss = 0.015306862071156502
iteration 11, loss = 0.011894674971699715
iteration 12, loss = 0.011905325576663017
iteration 13, loss = 0.013260630890727043
iteration 14, loss = 0.013649117201566696
iteration 15, loss = 0.01198352687060833
iteration 16, loss = 0.0122308861464262
iteration 17, loss = 0.01646277867257595
iteration 18, loss = 0.018580380827188492
iteration 19, loss = 0.012122880667448044
iteration 20, loss = 0.014123122207820415
iteration 21, loss = 0.012564580887556076
iteration 22, loss = 0.01273072324693203
iteration 23, loss = 0.012964041903614998
iteration 24, loss = 0.013577375560998917
iteration 25, loss = 0.011640263721346855
iteration 26, loss = 0.01645113155245781
iteration 27, loss = 0.011404821649193764
iteration 28, loss = 0.013676815666258335
iteration 29, loss = 0.01556453201919794
iteration 30, loss = 0.014838489703834057
iteration 31, loss = 0.01225644163787365
iteration 32, loss = 0.016714803874492645
iteration 33, loss = 0.012330488301813602
iteration 34, loss = 0.012606825679540634
iteration 35, loss = 0.016119586303830147
iteration 36, loss = 0.011956466361880302
iteration 37, loss = 0.01214746292680502
iteration 38, loss = 0.018396180123090744
iteration 39, loss = 0.011587260290980339
iteration 40, loss = 0.012792278081178665
iteration 41, loss = 0.01607033796608448
iteration 42, loss = 0.011808603070676327
iteration 43, loss = 0.012311971746385098
iteration 44, loss = 0.011243393644690514
iteration 45, loss = 0.014820556156337261
iteration 46, loss = 0.015585045330226421
iteration 47, loss = 0.011363744735717773
iteration 48, loss = 0.014372685924172401
iteration 49, loss = 0.016768239438533783
iteration 50, loss = 0.011496671475470066
iteration 51, loss = 0.016697244718670845
iteration 52, loss = 0.012166650965809822
iteration 53, loss = 0.0125277703627944
iteration 54, loss = 0.013104235753417015
iteration 55, loss = 0.012731743045151234
iteration 56, loss = 0.012393036857247353
iteration 57, loss = 0.012169145047664642
iteration 58, loss = 0.01587866060435772
iteration 59, loss = 0.014237952418625355
iteration 60, loss = 0.015223505906760693
iteration 61, loss = 0.014903679490089417
iteration 62, loss = 0.019672950729727745
iteration 63, loss = 0.01306062750518322
iteration 64, loss = 0.014870202168822289
iteration 65, loss = 0.013414659537374973
iteration 66, loss = 0.0129942512139678
iteration 67, loss = 0.015949517488479614
iteration 68, loss = 0.017353370785713196
iteration 69, loss = 0.012079240754246712
iteration 70, loss = 0.011927291750907898
iteration 71, loss = 0.011818882077932358
iteration 72, loss = 0.012411616742610931
iteration 73, loss = 0.017715364694595337
iteration 74, loss = 0.01140967383980751
iteration 75, loss = 0.012049808166921139
iteration 76, loss = 0.015143097378313541
iteration 77, loss = 0.011790229938924313
iteration 78, loss = 0.012450010515749454
iteration 79, loss = 0.01258891075849533
iteration 80, loss = 0.012671263888478279
iteration 81, loss = 0.014891090802848339
iteration 82, loss = 0.012644797563552856
iteration 83, loss = 0.011697374284267426
iteration 84, loss = 0.014950254932045937
iteration 85, loss = 0.016952667385339737
iteration 86, loss = 0.01247155386954546
iteration 87, loss = 0.017562061548233032
iteration 88, loss = 0.014958083629608154
iteration 89, loss = 0.01738104410469532
iteration 90, loss = 0.012252187356352806
iteration 91, loss = 0.019124336540699005
iteration 92, loss = 0.014719696715474129
iteration 93, loss = 0.01262015663087368
iteration 94, loss = 0.011555306613445282
iteration 95, loss = 0.0158537607640028
iteration 96, loss = 0.016080502420663834
iteration 97, loss = 0.01698123663663864
iteration 98, loss = 0.015596067532896996
iteration 99, loss = 0.012836719863116741
iteration 100, loss = 0.012024840340018272
iteration 101, loss = 0.013688446953892708
iteration 102, loss = 0.013535435311496258
iteration 103, loss = 0.0137691805139184
iteration 104, loss = 0.01642799749970436
iteration 105, loss = 0.013596580363810062
iteration 106, loss = 0.015487856231629848
iteration 107, loss = 0.014788144268095493
iteration 108, loss = 0.011245393194258213
iteration 109, loss = 0.01513624843209982
iteration 110, loss = 0.011814790777862072
iteration 111, loss = 0.013387230224907398
iteration 112, loss = 0.01203257404267788
iteration 113, loss = 0.01515260711312294
iteration 114, loss = 0.014641284011304379
iteration 115, loss = 0.013321311213076115
iteration 116, loss = 0.011596323922276497
iteration 117, loss = 0.011457586660981178
iteration 118, loss = 0.012186647392809391
iteration 119, loss = 0.011274897493422031
iteration 120, loss = 0.017448993399739265
iteration 121, loss = 0.01902334950864315
iteration 122, loss = 0.012446858920156956
iteration 123, loss = 0.01176816038787365
iteration 124, loss = 0.013956001959741116
iteration 125, loss = 0.017923539504408836
iteration 126, loss = 0.010866782627999783
iteration 127, loss = 0.012580549344420433
iteration 128, loss = 0.012330476194620132
iteration 129, loss = 0.011553355492651463
iteration 130, loss = 0.015398072078824043
iteration 131, loss = 0.01459509413689375
iteration 132, loss = 0.010656118392944336
iteration 133, loss = 0.011934659443795681
iteration 134, loss = 0.013260590843856335
iteration 135, loss = 0.012353374622762203
iteration 136, loss = 0.015965931117534637
iteration 137, loss = 0.011783584021031857
iteration 138, loss = 0.01216639019548893
iteration 139, loss = 0.012068954296410084
iteration 140, loss = 0.016101889312267303
iteration 141, loss = 0.012256497517228127
iteration 142, loss = 0.012928033247590065
iteration 143, loss = 0.012384265661239624
iteration 144, loss = 0.011211328208446503
iteration 145, loss = 0.014032426290214062
iteration 146, loss = 0.011701725423336029
iteration 147, loss = 0.012510177679359913
iteration 148, loss = 0.011323133483529091
iteration 149, loss = 0.011916541494429111
iteration 150, loss = 0.013138579204678535
iteration 151, loss = 0.02154078707098961
iteration 152, loss = 0.010935314930975437
iteration 153, loss = 0.011016939766705036
iteration 154, loss = 0.01642640493810177
iteration 155, loss = 0.012939844280481339
iteration 156, loss = 0.013237102888524532
iteration 157, loss = 0.01305326446890831
iteration 158, loss = 0.013026807457208633
iteration 159, loss = 0.01141838077455759
iteration 160, loss = 0.014938903041183949
iteration 161, loss = 0.011294646188616753
iteration 162, loss = 0.011457306332886219
iteration 163, loss = 0.014992060139775276
iteration 164, loss = 0.015007932670414448
iteration 165, loss = 0.012116757221519947
iteration 166, loss = 0.01191769726574421
iteration 167, loss = 0.01207316480576992
iteration 168, loss = 0.013084416277706623
iteration 169, loss = 0.012177715077996254
iteration 170, loss = 0.012277028523385525
iteration 171, loss = 0.01433480717241764
iteration 172, loss = 0.012337171472609043
iteration 173, loss = 0.012325190007686615
iteration 174, loss = 0.011098362505435944
iteration 175, loss = 0.011600249446928501
iteration 176, loss = 0.011902177706360817
iteration 177, loss = 0.011984611861407757
iteration 178, loss = 0.012042286805808544
iteration 179, loss = 0.0124879265204072
iteration 180, loss = 0.011727694422006607
iteration 181, loss = 0.011592145077884197
iteration 182, loss = 0.02042909525334835
iteration 183, loss = 0.018890785053372383
iteration 184, loss = 0.015239943750202656
iteration 185, loss = 0.011448537930846214
iteration 186, loss = 0.011205424554646015
iteration 187, loss = 0.01595647819340229
iteration 188, loss = 0.011417953297495842
iteration 189, loss = 0.011622595600783825
iteration 190, loss = 0.01143692061305046
iteration 191, loss = 0.014002272859215736
iteration 192, loss = 0.016974693164229393
iteration 193, loss = 0.015753338113427162
iteration 194, loss = 0.014438151381909847
iteration 195, loss = 0.01589450240135193
iteration 196, loss = 0.013483308255672455
iteration 197, loss = 0.015082723461091518
iteration 198, loss = 0.01606057770550251
iteration 199, loss = 0.011859806254506111
iteration 200, loss = 0.015975702553987503
iteration 201, loss = 0.011649361811578274
iteration 202, loss = 0.01171872764825821
iteration 203, loss = 0.012368540279567242
iteration 204, loss = 0.014748788438737392
iteration 205, loss = 0.012578395195305347
iteration 206, loss = 0.017206605523824692
iteration 207, loss = 0.011605222709476948
iteration 208, loss = 0.011175233870744705
iteration 209, loss = 0.01777535118162632
iteration 210, loss = 0.01307290606200695
iteration 211, loss = 0.015012633055448532
iteration 212, loss = 0.012202274985611439
iteration 213, loss = 0.011999393813312054
iteration 214, loss = 0.015853211283683777
iteration 215, loss = 0.011778496205806732
iteration 216, loss = 0.012543855234980583
iteration 217, loss = 0.011714027263224125
iteration 218, loss = 0.012071816250681877
iteration 219, loss = 0.01182796061038971
iteration 220, loss = 0.01545547042042017
iteration 221, loss = 0.011549854651093483
iteration 222, loss = 0.0122919213026762
iteration 223, loss = 0.011244963854551315
iteration 224, loss = 0.017494281753897667
iteration 225, loss = 0.015164568088948727
iteration 226, loss = 0.012077461928129196
iteration 227, loss = 0.011807790957391262
iteration 228, loss = 0.012440193444490433
iteration 229, loss = 0.015607145614922047
iteration 230, loss = 0.012462733313441277
iteration 231, loss = 0.011737690307199955
iteration 232, loss = 0.012816071510314941
iteration 233, loss = 0.01446536649018526
iteration 234, loss = 0.019886989146471024
iteration 235, loss = 0.015982698649168015
iteration 236, loss = 0.011527770198881626
iteration 237, loss = 0.0157672930508852
iteration 238, loss = 0.014039456844329834
iteration 239, loss = 0.011620378121733665
iteration 240, loss = 0.01227123849093914
iteration 241, loss = 0.012677507475018501
iteration 242, loss = 0.015105347149074078
iteration 243, loss = 0.011226213537156582
iteration 244, loss = 0.011628951877355576
iteration 245, loss = 0.011874066665768623
iteration 246, loss = 0.011250325478613377
iteration 247, loss = 0.015506145544350147
iteration 248, loss = 0.01438527274876833
iteration 249, loss = 0.017927363514900208
iteration 250, loss = 0.014856545254588127
iteration 251, loss = 0.01230358611792326
iteration 252, loss = 0.013278882019221783
iteration 253, loss = 0.011765140108764172
iteration 254, loss = 0.011747382581233978
iteration 255, loss = 0.011961579322814941
iteration 256, loss = 0.011899134144186974
iteration 257, loss = 0.011570911854505539
iteration 258, loss = 0.011914225295186043
iteration 259, loss = 0.012112114578485489
iteration 260, loss = 0.012656738981604576
iteration 261, loss = 0.012028169818222523
iteration 262, loss = 0.011524557135999203
iteration 263, loss = 0.016074981540441513
iteration 264, loss = 0.015894049778580666
iteration 265, loss = 0.011979548260569572
iteration 266, loss = 0.014595849439501762
iteration 267, loss = 0.013261965475976467
iteration 268, loss = 0.013076289556920528
iteration 269, loss = 0.020321553573012352
iteration 270, loss = 0.012949997559189796
iteration 271, loss = 0.011784457601606846
iteration 272, loss = 0.01197381317615509
iteration 273, loss = 0.01745818927884102
iteration 274, loss = 0.011581436730921268
iteration 275, loss = 0.01674133539199829
iteration 276, loss = 0.012791293673217297
iteration 277, loss = 0.01792776584625244
iteration 278, loss = 0.01574946939945221
iteration 279, loss = 0.011255236342549324
iteration 280, loss = 0.012299972586333752
iteration 281, loss = 0.016152415424585342
iteration 282, loss = 0.012673108838498592
iteration 283, loss = 0.012064739130437374
iteration 284, loss = 0.011364909820258617
iteration 285, loss = 0.014858090318739414
iteration 286, loss = 0.014458196237683296
iteration 287, loss = 0.015699628740549088
iteration 288, loss = 0.01611371710896492
iteration 289, loss = 0.012263767421245575
iteration 290, loss = 0.011163236573338509
iteration 291, loss = 0.012454276904463768
iteration 292, loss = 0.011601656675338745
iteration 293, loss = 0.011645292863249779
iteration 294, loss = 0.011896834708750248
iteration 295, loss = 0.013546030037105083
iteration 296, loss = 0.013843636959791183
iteration 297, loss = 0.0125660989433527
iteration 298, loss = 0.020049739629030228
iteration 299, loss = 0.011440576985478401
iteration 300, loss = 0.012198501266539097
iteration 1, loss = 0.011946972459554672
iteration 2, loss = 0.01138719916343689
iteration 3, loss = 0.011260313913226128
iteration 4, loss = 0.012897292152047157
iteration 5, loss = 0.011887384578585625
iteration 6, loss = 0.02001415565609932
iteration 7, loss = 0.012466874904930592
iteration 8, loss = 0.011911607347428799
iteration 9, loss = 0.015688953921198845
iteration 10, loss = 0.01563621498644352
iteration 11, loss = 0.015278715640306473
iteration 12, loss = 0.013985766097903252
iteration 13, loss = 0.017049167305231094
iteration 14, loss = 0.020448461174964905
iteration 15, loss = 0.015413368120789528
iteration 16, loss = 0.011430476792156696
iteration 17, loss = 0.014542334713041782
iteration 18, loss = 0.015528528019785881
iteration 19, loss = 0.013052666559815407
iteration 20, loss = 0.012292075902223587
iteration 21, loss = 0.012001433409750462
iteration 22, loss = 0.011114911176264286
iteration 23, loss = 0.011700710281729698
iteration 24, loss = 0.011419570073485374
iteration 25, loss = 0.01676434464752674
iteration 26, loss = 0.012242483906447887
iteration 27, loss = 0.010976238176226616
iteration 28, loss = 0.011021684855222702
iteration 29, loss = 0.01241347473114729
iteration 30, loss = 0.014837375842034817
iteration 31, loss = 0.011262394487857819
iteration 32, loss = 0.0150851309299469
iteration 33, loss = 0.011040725745260715
iteration 34, loss = 0.01752978377044201
iteration 35, loss = 0.011884120292961597
iteration 36, loss = 0.011493347585201263
iteration 37, loss = 0.011393742635846138
iteration 38, loss = 0.015027151443064213
iteration 39, loss = 0.012712456285953522
iteration 40, loss = 0.011496128514409065
iteration 41, loss = 0.013166198506951332
iteration 42, loss = 0.01534535363316536
iteration 43, loss = 0.01342856977134943
iteration 44, loss = 0.010791320353746414
iteration 45, loss = 0.01603541150689125
iteration 46, loss = 0.014358044601976871
iteration 47, loss = 0.01547790877521038
iteration 48, loss = 0.013122035190463066
iteration 49, loss = 0.01151539757847786
iteration 50, loss = 0.011908132582902908
iteration 51, loss = 0.011797083541750908
iteration 52, loss = 0.011378713883459568
iteration 53, loss = 0.011961492709815502
iteration 54, loss = 0.01108936220407486
iteration 55, loss = 0.01245127059519291
iteration 56, loss = 0.011329570785164833
iteration 57, loss = 0.010691776871681213
iteration 58, loss = 0.016985880210995674
iteration 59, loss = 0.013203288428485394
iteration 60, loss = 0.012182003818452358
iteration 61, loss = 0.010666283778846264
iteration 62, loss = 0.0122970687225461
iteration 63, loss = 0.016082383692264557
iteration 64, loss = 0.01245550811290741
iteration 65, loss = 0.011514663696289062
iteration 66, loss = 0.012651432305574417
iteration 67, loss = 0.011767905205488205
iteration 68, loss = 0.013992615975439548
iteration 69, loss = 0.017130525782704353
iteration 70, loss = 0.01077522337436676
iteration 71, loss = 0.016405360773205757
iteration 72, loss = 0.012647796422243118
iteration 73, loss = 0.012029465287923813
iteration 74, loss = 0.010990793816745281
iteration 75, loss = 0.011651057749986649
iteration 76, loss = 0.010863271541893482
iteration 77, loss = 0.01085799839347601
iteration 78, loss = 0.014405397698283195
iteration 79, loss = 0.01087111420929432
iteration 80, loss = 0.013706185854971409
iteration 81, loss = 0.012913787737488747
iteration 82, loss = 0.011805104091763496
iteration 83, loss = 0.011323025450110435
iteration 84, loss = 0.013268163427710533
iteration 85, loss = 0.011525986716151237
iteration 86, loss = 0.011029520072042942
iteration 87, loss = 0.012685224413871765
iteration 88, loss = 0.011740220710635185
iteration 89, loss = 0.01541126985102892
iteration 90, loss = 0.012011995539069176
iteration 91, loss = 0.011505364440381527
iteration 92, loss = 0.013638013042509556
iteration 93, loss = 0.012526359409093857
iteration 94, loss = 0.012776879593729973
iteration 95, loss = 0.01280225906521082
iteration 96, loss = 0.011798040010035038
iteration 97, loss = 0.01265502069145441
iteration 98, loss = 0.012591173872351646
iteration 99, loss = 0.013310235925018787
iteration 100, loss = 0.011688160710036755
iteration 101, loss = 0.012381430715322495
iteration 102, loss = 0.014578139409422874
iteration 103, loss = 0.014528457075357437
iteration 104, loss = 0.0131319435313344
iteration 105, loss = 0.012384388595819473
iteration 106, loss = 0.012111220508813858
iteration 107, loss = 0.015319973230361938
iteration 108, loss = 0.016300270333886147
iteration 109, loss = 0.013592153787612915
iteration 110, loss = 0.0115917157381773
iteration 111, loss = 0.012904707342386246
iteration 112, loss = 0.01296092476695776
iteration 113, loss = 0.012466787360608578
iteration 114, loss = 0.011571276932954788
iteration 115, loss = 0.012281963601708412
iteration 116, loss = 0.01469222828745842
iteration 117, loss = 0.012287868186831474
iteration 118, loss = 0.010684735141694546
iteration 119, loss = 0.01481207087635994
iteration 120, loss = 0.013521257787942886
iteration 121, loss = 0.011230453848838806
iteration 122, loss = 0.011922994628548622
iteration 123, loss = 0.0149209750816226
iteration 124, loss = 0.011539284139871597
iteration 125, loss = 0.012289012782275677
iteration 126, loss = 0.011269990354776382
iteration 127, loss = 0.011655593290925026
iteration 128, loss = 0.01277206465601921
iteration 129, loss = 0.011671867221593857
iteration 130, loss = 0.015188721008598804
iteration 131, loss = 0.01104064006358385
iteration 132, loss = 0.010715645737946033
iteration 133, loss = 0.010914681479334831
iteration 134, loss = 0.012446335516870022
iteration 135, loss = 0.015230421908199787
iteration 136, loss = 0.013475442305207253
iteration 137, loss = 0.017664344981312752
iteration 138, loss = 0.011950742453336716
iteration 139, loss = 0.017658943310379982
iteration 140, loss = 0.012526917271316051
iteration 141, loss = 0.016930269077420235
iteration 142, loss = 0.013768230564892292
iteration 143, loss = 0.011567801237106323
iteration 144, loss = 0.01217727456241846
iteration 145, loss = 0.011249402537941933
iteration 146, loss = 0.014071177691221237
iteration 147, loss = 0.018489327281713486
iteration 148, loss = 0.01477181538939476
iteration 149, loss = 0.015038669109344482
iteration 150, loss = 0.011400834657251835
iteration 151, loss = 0.011328216642141342
iteration 152, loss = 0.011714708991348743
iteration 153, loss = 0.011881434358656406
iteration 154, loss = 0.015296293422579765
iteration 155, loss = 0.012507721781730652
iteration 156, loss = 0.012900082394480705
iteration 157, loss = 0.010908425785601139
iteration 158, loss = 0.013806439004838467
iteration 159, loss = 0.012152893468737602
iteration 160, loss = 0.011601612903177738
iteration 161, loss = 0.013951495289802551
iteration 162, loss = 0.013368806801736355
iteration 163, loss = 0.011467956006526947
iteration 164, loss = 0.011162902228534222
iteration 165, loss = 0.01216618437319994
iteration 166, loss = 0.014553006738424301
iteration 167, loss = 0.011588666588068008
iteration 168, loss = 0.021369721740484238
iteration 169, loss = 0.012433718889951706
iteration 170, loss = 0.011417949572205544
iteration 171, loss = 0.011326659470796585
iteration 172, loss = 0.011072607710957527
iteration 173, loss = 0.012009330093860626
iteration 174, loss = 0.02070477418601513
iteration 175, loss = 0.012147863395512104
iteration 176, loss = 0.013478017412126064
iteration 177, loss = 0.014314057305455208
iteration 178, loss = 0.011247760616242886
iteration 179, loss = 0.011911295354366302
iteration 180, loss = 0.011274527758359909
iteration 181, loss = 0.01347618829458952
iteration 182, loss = 0.016945699229836464
iteration 183, loss = 0.012843761593103409
iteration 184, loss = 0.011585289612412453
iteration 185, loss = 0.014284282922744751
iteration 186, loss = 0.012233426794409752
iteration 187, loss = 0.01151727233082056
iteration 188, loss = 0.01209823228418827
iteration 189, loss = 0.011765103787183762
iteration 190, loss = 0.012274348177015781
iteration 191, loss = 0.011389078572392464
iteration 192, loss = 0.014974718913435936
iteration 193, loss = 0.013787215575575829
iteration 194, loss = 0.011029642075300217
iteration 195, loss = 0.017435919493436813
iteration 196, loss = 0.010923033580183983
iteration 197, loss = 0.017422817647457123
iteration 198, loss = 0.010910638608038425
iteration 199, loss = 0.011201482266187668
iteration 200, loss = 0.015121642500162125
iteration 201, loss = 0.013251797296106815
iteration 202, loss = 0.011605631560087204
iteration 203, loss = 0.012474380433559418
iteration 204, loss = 0.014476346783339977
iteration 205, loss = 0.011637798510491848
iteration 206, loss = 0.01522785797715187
iteration 207, loss = 0.01340235024690628
iteration 208, loss = 0.012552069500088692
iteration 209, loss = 0.01806570403277874
iteration 210, loss = 0.014100553467869759
iteration 211, loss = 0.011251475661993027
iteration 212, loss = 0.012773916125297546
iteration 213, loss = 0.015498775988817215
iteration 214, loss = 0.011239435523748398
iteration 215, loss = 0.011773302219808102
iteration 216, loss = 0.01093662716448307
iteration 217, loss = 0.010950002819299698
iteration 218, loss = 0.011769752949476242
iteration 219, loss = 0.018500179052352905
iteration 220, loss = 0.01173909567296505
iteration 221, loss = 0.015465389005839825
iteration 222, loss = 0.01229521818459034
iteration 223, loss = 0.012278069742023945
iteration 224, loss = 0.012956403195858002
iteration 225, loss = 0.011881283484399319
iteration 226, loss = 0.019954951480031013
iteration 227, loss = 0.013925474137067795
iteration 228, loss = 0.014715799130499363
iteration 229, loss = 0.013020617887377739
iteration 230, loss = 0.011367613449692726
iteration 231, loss = 0.012700081802904606
iteration 232, loss = 0.014807499945163727
iteration 233, loss = 0.011869113892316818
iteration 234, loss = 0.012864524498581886
iteration 235, loss = 0.01426912285387516
iteration 236, loss = 0.01198812946677208
iteration 237, loss = 0.010858186520636082
iteration 238, loss = 0.011366989463567734
iteration 239, loss = 0.015261508524417877
iteration 240, loss = 0.017553068697452545
iteration 241, loss = 0.012462673708796501
iteration 242, loss = 0.012987714260816574
iteration 243, loss = 0.016498126089572906
iteration 244, loss = 0.011794859543442726
iteration 245, loss = 0.014241626486182213
iteration 246, loss = 0.01634289138019085
iteration 247, loss = 0.018422797322273254
iteration 248, loss = 0.011230917647480965
iteration 249, loss = 0.014644931070506573
iteration 250, loss = 0.014467562548816204
iteration 251, loss = 0.011152232997119427
iteration 252, loss = 0.011034646071493626
iteration 253, loss = 0.014247557148337364
iteration 254, loss = 0.013204284012317657
iteration 255, loss = 0.013972688466310501
iteration 256, loss = 0.010805984027683735
iteration 257, loss = 0.011996929533779621
iteration 258, loss = 0.011641268618404865
iteration 259, loss = 0.01603713259100914
iteration 260, loss = 0.010919369757175446
iteration 261, loss = 0.015278059989213943
iteration 262, loss = 0.016278428956866264
iteration 263, loss = 0.012296713888645172
iteration 264, loss = 0.013170700520277023
iteration 265, loss = 0.014362351037561893
iteration 266, loss = 0.014734790660440922
iteration 267, loss = 0.012005483731627464
iteration 268, loss = 0.01081316452473402
iteration 269, loss = 0.011180848814547062
iteration 270, loss = 0.011791745200753212
iteration 271, loss = 0.015082496218383312
iteration 272, loss = 0.011149524711072445
iteration 273, loss = 0.011569881811738014
iteration 274, loss = 0.011184072121977806
iteration 275, loss = 0.01153615303337574
iteration 276, loss = 0.019084887579083443
iteration 277, loss = 0.014326991513371468
iteration 278, loss = 0.013291487470269203
iteration 279, loss = 0.011287768371403217
iteration 280, loss = 0.011703160591423512
iteration 281, loss = 0.011354136280715466
iteration 282, loss = 0.010634617879986763
iteration 283, loss = 0.014320455491542816
iteration 284, loss = 0.011375617235898972
iteration 285, loss = 0.016248760744929314
iteration 286, loss = 0.011404460296034813
iteration 287, loss = 0.017266618087887764
iteration 288, loss = 0.014571398496627808
iteration 289, loss = 0.011850758455693722
iteration 290, loss = 0.016047729179263115
iteration 291, loss = 0.014029215089976788
iteration 292, loss = 0.017298437654972076
iteration 293, loss = 0.011464374139904976
iteration 294, loss = 0.018028246238827705
iteration 295, loss = 0.01644515059888363
iteration 296, loss = 0.011621548794209957
iteration 297, loss = 0.011185870505869389
iteration 298, loss = 0.012681825086474419
iteration 299, loss = 0.01066720299422741
iteration 300, loss = 0.015207277610898018
iteration 1, loss = 0.01367428619414568
iteration 2, loss = 0.016154302284121513
iteration 3, loss = 0.011170418001711369
iteration 4, loss = 0.011777186766266823
iteration 5, loss = 0.014214521273970604
iteration 6, loss = 0.01056643109768629
iteration 7, loss = 0.010851623490452766
iteration 8, loss = 0.012217424809932709
iteration 9, loss = 0.011565525084733963
iteration 10, loss = 0.013755257241427898
iteration 11, loss = 0.01071151252835989
iteration 12, loss = 0.013593972660601139
iteration 13, loss = 0.012756040319800377
iteration 14, loss = 0.012394911609590054
iteration 15, loss = 0.011416306719183922
iteration 16, loss = 0.0104568125680089
iteration 17, loss = 0.014788886532187462
iteration 18, loss = 0.011679530143737793
iteration 19, loss = 0.011103893630206585
iteration 20, loss = 0.012501982972025871
iteration 21, loss = 0.010231905616819859
iteration 22, loss = 0.011527293361723423
iteration 23, loss = 0.010442953556776047
iteration 24, loss = 0.012079548090696335
iteration 25, loss = 0.011941243894398212
iteration 26, loss = 0.01916479505598545
iteration 27, loss = 0.010702908970415592
iteration 28, loss = 0.01412451546639204
iteration 29, loss = 0.010728491470217705
iteration 30, loss = 0.011155534535646439
iteration 31, loss = 0.012958395294845104
iteration 32, loss = 0.014540094882249832
iteration 33, loss = 0.01957103982567787
iteration 34, loss = 0.01579596847295761
iteration 35, loss = 0.011711858212947845
iteration 36, loss = 0.012625600211322308
iteration 37, loss = 0.012580299749970436
iteration 38, loss = 0.013128532096743584
iteration 39, loss = 0.01227269321680069
iteration 40, loss = 0.011435878463089466
iteration 41, loss = 0.015233811922371387
iteration 42, loss = 0.01112000085413456
iteration 43, loss = 0.013315131887793541
iteration 44, loss = 0.014536522328853607
iteration 45, loss = 0.01560887135565281
iteration 46, loss = 0.011357882060110569
iteration 47, loss = 0.011074685491621494
iteration 48, loss = 0.016605306416749954
iteration 49, loss = 0.011493333615362644
iteration 50, loss = 0.012102347798645496
iteration 51, loss = 0.012532921507954597
iteration 52, loss = 0.010714832693338394
iteration 53, loss = 0.010967977344989777
iteration 54, loss = 0.011512672528624535
iteration 55, loss = 0.011266582645475864
iteration 56, loss = 0.011267740279436111
iteration 57, loss = 0.01156560331583023
iteration 58, loss = 0.012932468205690384
iteration 59, loss = 0.011353012174367905
iteration 60, loss = 0.011573906987905502
iteration 61, loss = 0.013787566684186459
iteration 62, loss = 0.012554855085909367
iteration 63, loss = 0.011134114116430283
iteration 64, loss = 0.011828729882836342
iteration 65, loss = 0.011046142317354679
iteration 66, loss = 0.010822249576449394
iteration 67, loss = 0.014840919524431229
iteration 68, loss = 0.011325839906930923
iteration 69, loss = 0.010647742077708244
iteration 70, loss = 0.012606617994606495
iteration 71, loss = 0.01254377979785204
iteration 72, loss = 0.015086185187101364
iteration 73, loss = 0.011433008126914501
iteration 74, loss = 0.010912911966443062
iteration 75, loss = 0.01556231640279293
iteration 76, loss = 0.015378884971141815
iteration 77, loss = 0.011740637011826038
iteration 78, loss = 0.011631766334176064
iteration 79, loss = 0.0125349722802639
iteration 80, loss = 0.010874221101403236
iteration 81, loss = 0.013742866925895214
iteration 82, loss = 0.010734807699918747
iteration 83, loss = 0.014205293729901314
iteration 84, loss = 0.014715400524437428
iteration 85, loss = 0.013347555883228779
iteration 86, loss = 0.014772028662264347
iteration 87, loss = 0.011749252676963806
iteration 88, loss = 0.012841504998505116
iteration 89, loss = 0.011420764029026031
iteration 90, loss = 0.01320645771920681
iteration 91, loss = 0.012824548408389091
iteration 92, loss = 0.013918460346758366
iteration 93, loss = 0.013749590143561363
iteration 94, loss = 0.012000770308077335
iteration 95, loss = 0.013121862895786762
iteration 96, loss = 0.011998786590993404
iteration 97, loss = 0.010633992031216621
iteration 98, loss = 0.013484370894730091
iteration 99, loss = 0.014442062005400658
iteration 100, loss = 0.011994509026408195
iteration 101, loss = 0.014703189954161644
iteration 102, loss = 0.01649070903658867
iteration 103, loss = 0.0153990862891078
iteration 104, loss = 0.012051187455654144
iteration 105, loss = 0.014969956129789352
iteration 106, loss = 0.019845906645059586
iteration 107, loss = 0.014030342921614647
iteration 108, loss = 0.013430004939436913
iteration 109, loss = 0.012346017174422741
iteration 110, loss = 0.014809778891503811
iteration 111, loss = 0.012548435479402542
iteration 112, loss = 0.011063938960433006
iteration 113, loss = 0.012364587746560574
iteration 114, loss = 0.017472535371780396
iteration 115, loss = 0.011230695061385632
iteration 116, loss = 0.014531449414789677
iteration 117, loss = 0.011771795339882374
iteration 118, loss = 0.01558744627982378
iteration 119, loss = 0.012142488732933998
iteration 120, loss = 0.01110406406223774
iteration 121, loss = 0.016842879354953766
iteration 122, loss = 0.011214269325137138
iteration 123, loss = 0.010500911623239517
iteration 124, loss = 0.012938745319843292
iteration 125, loss = 0.011630717664957047
iteration 126, loss = 0.010778590105473995
iteration 127, loss = 0.012948360294103622
iteration 128, loss = 0.014819828793406487
iteration 129, loss = 0.011739852838218212
iteration 130, loss = 0.018559832125902176
iteration 131, loss = 0.011385640129446983
iteration 132, loss = 0.012313827872276306
iteration 133, loss = 0.011113257147371769
iteration 134, loss = 0.012228503823280334
iteration 135, loss = 0.013169157318770885
iteration 136, loss = 0.013694023713469505
iteration 137, loss = 0.012332904152572155
iteration 138, loss = 0.016003083437681198
iteration 139, loss = 0.010982113890349865
iteration 140, loss = 0.0183842983096838
iteration 141, loss = 0.01476618554443121
iteration 142, loss = 0.011958627961575985
iteration 143, loss = 0.014142303727567196
iteration 144, loss = 0.010923933237791061
iteration 145, loss = 0.011143872514367104
iteration 146, loss = 0.011083434335887432
iteration 147, loss = 0.01855287328362465
iteration 148, loss = 0.010682754218578339
iteration 149, loss = 0.013340260833501816
iteration 150, loss = 0.010975909419357777
iteration 151, loss = 0.011640060693025589
iteration 152, loss = 0.011118817143142223
iteration 153, loss = 0.01199033111333847
iteration 154, loss = 0.01503072027117014
iteration 155, loss = 0.011387113481760025
iteration 156, loss = 0.011213011108338833
iteration 157, loss = 0.011389813385903835
iteration 158, loss = 0.016948077827692032
iteration 159, loss = 0.011718766763806343
iteration 160, loss = 0.011843942105770111
iteration 161, loss = 0.01127646490931511
iteration 162, loss = 0.015820806846022606
iteration 163, loss = 0.01162666268646717
iteration 164, loss = 0.015128383412957191
iteration 165, loss = 0.011872859671711922
iteration 166, loss = 0.015125464648008347
iteration 167, loss = 0.011454577557742596
iteration 168, loss = 0.01062840037047863
iteration 169, loss = 0.011851215735077858
iteration 170, loss = 0.012294228188693523
iteration 171, loss = 0.01229415275156498
iteration 172, loss = 0.013331571593880653
iteration 173, loss = 0.013668807223439217
iteration 174, loss = 0.011091469787061214
iteration 175, loss = 0.014882566407322884
iteration 176, loss = 0.012418007478117943
iteration 177, loss = 0.019106626510620117
iteration 178, loss = 0.012625250034034252
iteration 179, loss = 0.014968695119023323
iteration 180, loss = 0.012709729373455048
iteration 181, loss = 0.013622624799609184
iteration 182, loss = 0.01211046427488327
iteration 183, loss = 0.011743762530386448
iteration 184, loss = 0.011685067787766457
iteration 185, loss = 0.011193593963980675
iteration 186, loss = 0.0109044648706913
iteration 187, loss = 0.014623008668422699
iteration 188, loss = 0.014870340004563332
iteration 189, loss = 0.014458958059549332
iteration 190, loss = 0.011786196380853653
iteration 191, loss = 0.012457749806344509
iteration 192, loss = 0.011619152501225471
iteration 193, loss = 0.015208804048597813
iteration 194, loss = 0.016176192089915276
iteration 195, loss = 0.015098610892891884
iteration 196, loss = 0.011460213921964169
iteration 197, loss = 0.01867654174566269
iteration 198, loss = 0.011962713673710823
iteration 199, loss = 0.011085016652941704
iteration 200, loss = 0.017324572429060936
iteration 201, loss = 0.01475138496607542
iteration 202, loss = 0.016560757532715797
iteration 203, loss = 0.011244210414588451
iteration 204, loss = 0.012205768376588821
iteration 205, loss = 0.013292454183101654
iteration 206, loss = 0.0123800253495574
iteration 207, loss = 0.012635981664061546
iteration 208, loss = 0.010990319773554802
iteration 209, loss = 0.011488661170005798
iteration 210, loss = 0.011409332044422626
iteration 211, loss = 0.011618813499808311
iteration 212, loss = 0.017838478088378906
iteration 213, loss = 0.01099671982228756
iteration 214, loss = 0.011619466356933117
iteration 215, loss = 0.018571525812149048
iteration 216, loss = 0.010805384255945683
iteration 217, loss = 0.012234560213983059
iteration 218, loss = 0.01078539714217186
iteration 219, loss = 0.014844155870378017
iteration 220, loss = 0.011136466637253761
iteration 221, loss = 0.013658707030117512
iteration 222, loss = 0.011121196672320366
iteration 223, loss = 0.011367150582373142
iteration 224, loss = 0.011268384754657745
iteration 225, loss = 0.010950949043035507
iteration 226, loss = 0.014599986374378204
iteration 227, loss = 0.01149686984717846
iteration 228, loss = 0.011338422074913979
iteration 229, loss = 0.010709147900342941
iteration 230, loss = 0.01309624407440424
iteration 231, loss = 0.011367982253432274
iteration 232, loss = 0.01240867655724287
iteration 233, loss = 0.014852495864033699
iteration 234, loss = 0.010666008107364178
iteration 235, loss = 0.012660875916481018
iteration 236, loss = 0.015203873626887798
iteration 237, loss = 0.011614549905061722
iteration 238, loss = 0.015004267916083336
iteration 239, loss = 0.012811104767024517
iteration 240, loss = 0.010588324628770351
iteration 241, loss = 0.011416283436119556
iteration 242, loss = 0.013877724297344685
iteration 243, loss = 0.018596647307276726
iteration 244, loss = 0.014791964553296566
iteration 245, loss = 0.013514004647731781
iteration 246, loss = 0.011144928634166718
iteration 247, loss = 0.011714919470250607
iteration 248, loss = 0.010967282578349113
iteration 249, loss = 0.01132107526063919
iteration 250, loss = 0.017755350098013878
iteration 251, loss = 0.010304744355380535
iteration 252, loss = 0.01124367956072092
iteration 253, loss = 0.01172078400850296
iteration 254, loss = 0.0111878402531147
iteration 255, loss = 0.01158825121819973
iteration 256, loss = 0.01722704991698265
iteration 257, loss = 0.011117941699922085
iteration 258, loss = 0.013871775008738041
iteration 259, loss = 0.011377781629562378
iteration 260, loss = 0.014515139162540436
iteration 261, loss = 0.015139341354370117
iteration 262, loss = 0.010983534157276154
iteration 263, loss = 0.01703619211912155
iteration 264, loss = 0.011067382991313934
iteration 265, loss = 0.011111210100352764
iteration 266, loss = 0.0170673169195652
iteration 267, loss = 0.011661947704851627
iteration 268, loss = 0.011229592375457287
iteration 269, loss = 0.012374546378850937
iteration 270, loss = 0.011547930538654327
iteration 271, loss = 0.012688197195529938
iteration 272, loss = 0.012292447499930859
iteration 273, loss = 0.012114126235246658
iteration 274, loss = 0.011504140682518482
iteration 275, loss = 0.01832686737179756
iteration 276, loss = 0.012266618199646473
iteration 277, loss = 0.011961530894041061
iteration 278, loss = 0.012059501372277737
iteration 279, loss = 0.015278538689017296
iteration 280, loss = 0.011432727798819542
iteration 281, loss = 0.011279063299298286
iteration 282, loss = 0.015206757001578808
iteration 283, loss = 0.016345767304301262
iteration 284, loss = 0.015245584771037102
iteration 285, loss = 0.014672661200165749
iteration 286, loss = 0.014346315525472164
iteration 287, loss = 0.010958079248666763
iteration 288, loss = 0.013071154244244099
iteration 289, loss = 0.012259230017662048
iteration 290, loss = 0.012031570076942444
iteration 291, loss = 0.010893047787249088
iteration 292, loss = 0.011253239586949348
iteration 293, loss = 0.011290832422673702
iteration 294, loss = 0.012119724415242672
iteration 295, loss = 0.013693895190954208
iteration 296, loss = 0.01694132573902607
iteration 297, loss = 0.011574449017643929
iteration 298, loss = 0.011939696967601776
iteration 299, loss = 0.020859431475400925
iteration 300, loss = 0.014591580256819725
iteration 1, loss = 0.014138850383460522
iteration 2, loss = 0.014303487725555897
iteration 3, loss = 0.012017475441098213
iteration 4, loss = 0.01074895728379488
iteration 5, loss = 0.01373794861137867
iteration 6, loss = 0.01181608997285366
iteration 7, loss = 0.011530999094247818
iteration 8, loss = 0.012829199433326721
iteration 9, loss = 0.018934616819024086
iteration 10, loss = 0.018170930445194244
iteration 11, loss = 0.01242813654243946
iteration 12, loss = 0.012409906834363937
iteration 13, loss = 0.01191791146993637
iteration 14, loss = 0.012494937516748905
iteration 15, loss = 0.011593242175877094
iteration 16, loss = 0.012244703248143196
iteration 17, loss = 0.015193535014986992
iteration 18, loss = 0.012743782252073288
iteration 19, loss = 0.01095128059387207
iteration 20, loss = 0.011497026309370995
iteration 21, loss = 0.01218731515109539
iteration 22, loss = 0.014773968607187271
iteration 23, loss = 0.014978229999542236
iteration 24, loss = 0.013080094009637833
iteration 25, loss = 0.012167681008577347
iteration 26, loss = 0.013520471751689911
iteration 27, loss = 0.011466911993920803
iteration 28, loss = 0.012167236767709255
iteration 29, loss = 0.01168837957084179
iteration 30, loss = 0.015512087382376194
iteration 31, loss = 0.011832592077553272
iteration 32, loss = 0.013991443440318108
iteration 33, loss = 0.013306799344718456
iteration 34, loss = 0.012232783250510693
iteration 35, loss = 0.014714716002345085
iteration 36, loss = 0.012999401427805424
iteration 37, loss = 0.012022517621517181
iteration 38, loss = 0.011348030529916286
iteration 39, loss = 0.0110222939401865
iteration 40, loss = 0.01110390666872263
iteration 41, loss = 0.011916211806237698
iteration 42, loss = 0.012937115505337715
iteration 43, loss = 0.014623615890741348
iteration 44, loss = 0.011675016023218632
iteration 45, loss = 0.014554875902831554
iteration 46, loss = 0.011791177093982697
iteration 47, loss = 0.010678151622414589
iteration 48, loss = 0.01455408614128828
iteration 49, loss = 0.012444524094462395
iteration 50, loss = 0.011818485334515572
iteration 51, loss = 0.010564099997282028
iteration 52, loss = 0.012591495178639889
iteration 53, loss = 0.013140086084604263
iteration 54, loss = 0.01630289852619171
iteration 55, loss = 0.012005550786852837
iteration 56, loss = 0.013538451865315437
iteration 57, loss = 0.014576718211174011
iteration 58, loss = 0.015797244384884834
iteration 59, loss = 0.010837857611477375
iteration 60, loss = 0.011683476157486439
iteration 61, loss = 0.013200463727116585
iteration 62, loss = 0.010851831175386906
iteration 63, loss = 0.011201403103768826
iteration 64, loss = 0.017519664019346237
iteration 65, loss = 0.010372481308877468
iteration 66, loss = 0.017285795882344246
iteration 67, loss = 0.013248467817902565
iteration 68, loss = 0.011766456067562103
iteration 69, loss = 0.011030617170035839
iteration 70, loss = 0.011031636968255043
iteration 71, loss = 0.012883848510682583
iteration 72, loss = 0.011569160968065262
iteration 73, loss = 0.013775509782135487
iteration 74, loss = 0.012259596958756447
iteration 75, loss = 0.014824165031313896
iteration 76, loss = 0.01327767688781023
iteration 77, loss = 0.014585874043405056
iteration 78, loss = 0.01189809013158083
iteration 79, loss = 0.011677714064717293
iteration 80, loss = 0.011294782161712646
iteration 81, loss = 0.010612208396196365
iteration 82, loss = 0.014112883247435093
iteration 83, loss = 0.012378303334116936
iteration 84, loss = 0.014713620766997337
iteration 85, loss = 0.010920018889009953
iteration 86, loss = 0.012665932066738605
iteration 87, loss = 0.01187217142432928
iteration 88, loss = 0.010924444533884525
iteration 89, loss = 0.011120829731225967
iteration 90, loss = 0.013015802018344402
iteration 91, loss = 0.013235542923212051
iteration 92, loss = 0.011876439675688744
iteration 93, loss = 0.011059608310461044
iteration 94, loss = 0.010968832299113274
iteration 95, loss = 0.0110410051420331
iteration 96, loss = 0.016087781637907028
iteration 97, loss = 0.011558955535292625
iteration 98, loss = 0.012034855782985687
iteration 99, loss = 0.01047368161380291
iteration 100, loss = 0.016050640493631363
iteration 101, loss = 0.012282907031476498
iteration 102, loss = 0.011945732869207859
iteration 103, loss = 0.0161959957331419
iteration 104, loss = 0.010585560463368893
iteration 105, loss = 0.011109347455203533
iteration 106, loss = 0.011217506602406502
iteration 107, loss = 0.011306709609925747
iteration 108, loss = 0.01185537688434124
iteration 109, loss = 0.011111517436802387
iteration 110, loss = 0.010723008774220943
iteration 111, loss = 0.017329392954707146
iteration 112, loss = 0.011042478494346142
iteration 113, loss = 0.01686442270874977
iteration 114, loss = 0.014943469315767288
iteration 115, loss = 0.011331556364893913
iteration 116, loss = 0.012068793177604675
iteration 117, loss = 0.014891408383846283
iteration 118, loss = 0.011319335550069809
iteration 119, loss = 0.014700740575790405
iteration 120, loss = 0.012057962827384472
iteration 121, loss = 0.012896856293082237
iteration 122, loss = 0.013958461582660675
iteration 123, loss = 0.011624643579125404
iteration 124, loss = 0.010568141005933285
iteration 125, loss = 0.011372754350304604
iteration 126, loss = 0.011148368939757347
iteration 127, loss = 0.011443634517490864
iteration 128, loss = 0.013720214366912842
iteration 129, loss = 0.016544127836823463
iteration 130, loss = 0.012336553074419498
iteration 131, loss = 0.012415162287652493
iteration 132, loss = 0.012337230145931244
iteration 133, loss = 0.010713625699281693
iteration 134, loss = 0.010953824035823345
iteration 135, loss = 0.011142898350954056
iteration 136, loss = 0.013332118280231953
iteration 137, loss = 0.010762014426290989
iteration 138, loss = 0.012811688706278801
iteration 139, loss = 0.01216181367635727
iteration 140, loss = 0.01270546019077301
iteration 141, loss = 0.014927772805094719
iteration 142, loss = 0.01289461925625801
iteration 143, loss = 0.01464287843555212
iteration 144, loss = 0.015640025958418846
iteration 145, loss = 0.011682886630296707
iteration 146, loss = 0.011778241023421288
iteration 147, loss = 0.01834816485643387
iteration 148, loss = 0.011724958196282387
iteration 149, loss = 0.012291175313293934
iteration 150, loss = 0.011075285263359547
iteration 151, loss = 0.014648841693997383
iteration 152, loss = 0.013078400865197182
iteration 153, loss = 0.01685563288629055
iteration 154, loss = 0.011700461618602276
iteration 155, loss = 0.021280495449900627
iteration 156, loss = 0.013130765408277512
iteration 157, loss = 0.014272942207753658
iteration 158, loss = 0.01774919591844082
iteration 159, loss = 0.010782759636640549
iteration 160, loss = 0.015512836165726185
iteration 161, loss = 0.012548426166176796
iteration 162, loss = 0.013637615367770195
iteration 163, loss = 0.010548107326030731
iteration 164, loss = 0.014080964028835297
iteration 165, loss = 0.015102622099220753
iteration 166, loss = 0.011851741932332516
iteration 167, loss = 0.011683802120387554
iteration 168, loss = 0.011314736679196358
iteration 169, loss = 0.010775099508464336
iteration 170, loss = 0.011527457274496555
iteration 171, loss = 0.01093429047614336
iteration 172, loss = 0.010599183849990368
iteration 173, loss = 0.01630444824695587
iteration 174, loss = 0.012852436862885952
iteration 175, loss = 0.011819621548056602
iteration 176, loss = 0.014542192220687866
iteration 177, loss = 0.014511809684336185
iteration 178, loss = 0.01108749583363533
iteration 179, loss = 0.01571820303797722
iteration 180, loss = 0.010771029628813267
iteration 181, loss = 0.015026220120489597
iteration 182, loss = 0.011847504414618015
iteration 183, loss = 0.01545854564756155
iteration 184, loss = 0.01065939012914896
iteration 185, loss = 0.011382557451725006
iteration 186, loss = 0.011647757142782211
iteration 187, loss = 0.011794035322964191
iteration 188, loss = 0.011907348409295082
iteration 189, loss = 0.010906602256000042
iteration 190, loss = 0.01175107154995203
iteration 191, loss = 0.014513417147099972
iteration 192, loss = 0.014925653114914894
iteration 193, loss = 0.011316483840346336
iteration 194, loss = 0.016585823148489
iteration 195, loss = 0.011694826185703278
iteration 196, loss = 0.015785684809088707
iteration 197, loss = 0.013265630230307579
iteration 198, loss = 0.010722124949097633
iteration 199, loss = 0.015187936834990978
iteration 200, loss = 0.017710400745272636
iteration 201, loss = 0.011052893474698067
iteration 202, loss = 0.011438518762588501
iteration 203, loss = 0.020131327211856842
iteration 204, loss = 0.011328213848173618
iteration 205, loss = 0.010723462328314781
iteration 206, loss = 0.012149767950177193
iteration 207, loss = 0.012332096695899963
iteration 208, loss = 0.011565634980797768
iteration 209, loss = 0.011499366723001003
iteration 210, loss = 0.012449703179299831
iteration 211, loss = 0.011297634802758694
iteration 212, loss = 0.01503495778888464
iteration 213, loss = 0.014864658005535603
iteration 214, loss = 0.012208431959152222
iteration 215, loss = 0.01128423772752285
iteration 216, loss = 0.0117636164650321
iteration 217, loss = 0.011492426507174969
iteration 218, loss = 0.010810015723109245
iteration 219, loss = 0.011762958019971848
iteration 220, loss = 0.014519523829221725
iteration 221, loss = 0.012471566908061504
iteration 222, loss = 0.014869406819343567
iteration 223, loss = 0.012699663639068604
iteration 224, loss = 0.011001401580870152
iteration 225, loss = 0.01566881313920021
iteration 226, loss = 0.011201268061995506
iteration 227, loss = 0.014329593628644943
iteration 228, loss = 0.01437901146709919
iteration 229, loss = 0.011291074566543102
iteration 230, loss = 0.011398965492844582
iteration 231, loss = 0.013751511462032795
iteration 232, loss = 0.014244905672967434
iteration 233, loss = 0.01449073851108551
iteration 234, loss = 0.019351443275809288
iteration 235, loss = 0.011220789514482021
iteration 236, loss = 0.012076914310455322
iteration 237, loss = 0.01130167581140995
iteration 238, loss = 0.018632715567946434
iteration 239, loss = 0.013948955573141575
iteration 240, loss = 0.017361724749207497
iteration 241, loss = 0.011949817650020123
iteration 242, loss = 0.01404578611254692
iteration 243, loss = 0.014208556152880192
iteration 244, loss = 0.01616552658379078
iteration 245, loss = 0.011079177260398865
iteration 246, loss = 0.010696756653487682
iteration 247, loss = 0.01055155135691166
iteration 248, loss = 0.010902163572609425
iteration 249, loss = 0.010628235526382923
iteration 250, loss = 0.018476851284503937
iteration 251, loss = 0.017423167824745178
iteration 252, loss = 0.01027945801615715
iteration 253, loss = 0.01145908236503601
iteration 254, loss = 0.010867328383028507
iteration 255, loss = 0.011891494505107403
iteration 256, loss = 0.010974489152431488
iteration 257, loss = 0.011624456383287907
iteration 258, loss = 0.011206291615962982
iteration 259, loss = 0.012148945592343807
iteration 260, loss = 0.01213955320417881
iteration 261, loss = 0.0113636814057827
iteration 262, loss = 0.01646728813648224
iteration 263, loss = 0.011461853981018066
iteration 264, loss = 0.014141003601253033
iteration 265, loss = 0.011440949514508247
iteration 266, loss = 0.011724773794412613
iteration 267, loss = 0.012414216063916683
iteration 268, loss = 0.011411716230213642
iteration 269, loss = 0.011157418601214886
iteration 270, loss = 0.019862642511725426
iteration 271, loss = 0.0113229900598526
iteration 272, loss = 0.015935400500893593
iteration 273, loss = 0.014106337912380695
iteration 274, loss = 0.010747184976935387
iteration 275, loss = 0.011114868335425854
iteration 276, loss = 0.011415963992476463
iteration 277, loss = 0.012160834856331348
iteration 278, loss = 0.012381135486066341
iteration 279, loss = 0.011420099064707756
iteration 280, loss = 0.012092029675841331
iteration 281, loss = 0.014794467017054558
iteration 282, loss = 0.013132080435752869
iteration 283, loss = 0.01821267604827881
iteration 284, loss = 0.010923953726887703
iteration 285, loss = 0.014584788121283054
iteration 286, loss = 0.011144842952489853
iteration 287, loss = 0.011074257083237171
iteration 288, loss = 0.012000009417533875
iteration 289, loss = 0.015357217751443386
iteration 290, loss = 0.01206089835613966
iteration 291, loss = 0.011751683428883553
iteration 292, loss = 0.011158213950693607
iteration 293, loss = 0.01387687586247921
iteration 294, loss = 0.016592957079410553
iteration 295, loss = 0.01082367543131113
iteration 296, loss = 0.012289474718272686
iteration 297, loss = 0.011736218817532063
iteration 298, loss = 0.012101107276976109
iteration 299, loss = 0.013642904348671436
iteration 300, loss = 0.014144976623356342
iteration 1, loss = 0.010791385546326637
iteration 2, loss = 0.012377972714602947
iteration 3, loss = 0.016222449019551277
iteration 4, loss = 0.016220519319176674
iteration 5, loss = 0.011849964037537575
iteration 6, loss = 0.01646646298468113
iteration 7, loss = 0.01330018974840641
iteration 8, loss = 0.013657335191965103
iteration 9, loss = 0.015285401605069637
iteration 10, loss = 0.011931703425943851
iteration 11, loss = 0.011303219944238663
iteration 12, loss = 0.018629536032676697
iteration 13, loss = 0.015893757343292236
iteration 14, loss = 0.013321323320269585
iteration 15, loss = 0.011963790282607079
iteration 16, loss = 0.01560311857610941
iteration 17, loss = 0.011110430583357811
iteration 18, loss = 0.010669577866792679
iteration 19, loss = 0.011135226115584373
iteration 20, loss = 0.011335304006934166
iteration 21, loss = 0.012138847261667252
iteration 22, loss = 0.010898350737988949
iteration 23, loss = 0.019451500847935677
iteration 24, loss = 0.011952875182032585
iteration 25, loss = 0.011227522045373917
iteration 26, loss = 0.011982735246419907
iteration 27, loss = 0.011136538349092007
iteration 28, loss = 0.01469215378165245
iteration 29, loss = 0.014809789136052132
iteration 30, loss = 0.013401295058429241
iteration 31, loss = 0.010374786332249641
iteration 32, loss = 0.018920619040727615
iteration 33, loss = 0.015159144066274166
iteration 34, loss = 0.011446548625826836
iteration 35, loss = 0.01269114576280117
iteration 36, loss = 0.01243422832340002
iteration 37, loss = 0.018302518874406815
iteration 38, loss = 0.01420206855982542
iteration 39, loss = 0.013109379447996616
iteration 40, loss = 0.01658308506011963
iteration 41, loss = 0.015801219269633293
iteration 42, loss = 0.01268074382096529
iteration 43, loss = 0.012758820317685604
iteration 44, loss = 0.019816532731056213
iteration 45, loss = 0.011243932880461216
iteration 46, loss = 0.011440712958574295
iteration 47, loss = 0.015576321631669998
iteration 48, loss = 0.01157276052981615
iteration 49, loss = 0.01150589995086193
iteration 50, loss = 0.010456430725753307
iteration 51, loss = 0.010955678299069405
iteration 52, loss = 0.011027483269572258
iteration 53, loss = 0.012375210411846638
iteration 54, loss = 0.011237945407629013
iteration 55, loss = 0.010748186148703098
iteration 56, loss = 0.01640264317393303
iteration 57, loss = 0.01122253853827715
iteration 58, loss = 0.01163163036108017
iteration 59, loss = 0.011561872437596321
iteration 60, loss = 0.01620972529053688
iteration 61, loss = 0.010580850765109062
iteration 62, loss = 0.012241953983902931
iteration 63, loss = 0.014741714112460613
iteration 64, loss = 0.010716021060943604
iteration 65, loss = 0.010695960372686386
iteration 66, loss = 0.014963534660637379
iteration 67, loss = 0.015679961070418358
iteration 68, loss = 0.01149698719382286
iteration 69, loss = 0.01090617012232542
iteration 70, loss = 0.012688251212239265
iteration 71, loss = 0.011304926127195358
iteration 72, loss = 0.01258841622620821
iteration 73, loss = 0.01603621430695057
iteration 74, loss = 0.010978486388921738
iteration 75, loss = 0.011856041848659515
iteration 76, loss = 0.011667519807815552
iteration 77, loss = 0.011782476678490639
iteration 78, loss = 0.010867922566831112
iteration 79, loss = 0.01643291860818863
iteration 80, loss = 0.017293687909841537
iteration 81, loss = 0.010955308564007282
iteration 82, loss = 0.018977360799908638
iteration 83, loss = 0.018102968111634254
iteration 84, loss = 0.010335140861570835
iteration 85, loss = 0.01132563129067421
iteration 86, loss = 0.011562258936464787
iteration 87, loss = 0.011944079771637917
iteration 88, loss = 0.011518901214003563
iteration 89, loss = 0.015144934877753258
iteration 90, loss = 0.01623464748263359
iteration 91, loss = 0.014400949701666832
iteration 92, loss = 0.014916104264557362
iteration 93, loss = 0.016572050750255585
iteration 94, loss = 0.012297756969928741
iteration 95, loss = 0.012199069373309612
iteration 96, loss = 0.014682062901556492
iteration 97, loss = 0.01518325600773096
iteration 98, loss = 0.011935102753341198
iteration 99, loss = 0.013760846108198166
iteration 100, loss = 0.010505891405045986
iteration 101, loss = 0.011012512259185314
iteration 102, loss = 0.011416166089475155
iteration 103, loss = 0.01110333763062954
iteration 104, loss = 0.011002051644027233
iteration 105, loss = 0.011777135543525219
iteration 106, loss = 0.011873583309352398
iteration 107, loss = 0.010845376178622246
iteration 108, loss = 0.012445143423974514
iteration 109, loss = 0.01265265978872776
iteration 110, loss = 0.013518020510673523
iteration 111, loss = 0.011506933718919754
iteration 112, loss = 0.013633334077894688
iteration 113, loss = 0.013996582478284836
iteration 114, loss = 0.018086494877934456
iteration 115, loss = 0.013789153657853603
iteration 116, loss = 0.012871626764535904
iteration 117, loss = 0.015452489256858826
iteration 118, loss = 0.011315525509417057
iteration 119, loss = 0.011521960608661175
iteration 120, loss = 0.010780670680105686
iteration 121, loss = 0.015342767350375652
iteration 122, loss = 0.010972194373607635
iteration 123, loss = 0.01229853369295597
iteration 124, loss = 0.014164121821522713
iteration 125, loss = 0.011681763455271721
iteration 126, loss = 0.01597731187939644
iteration 127, loss = 0.012425481341779232
iteration 128, loss = 0.011259866878390312
iteration 129, loss = 0.01510255690664053
iteration 130, loss = 0.012365316040813923
iteration 131, loss = 0.01194726675748825
iteration 132, loss = 0.011594068259000778
iteration 133, loss = 0.011657330207526684
iteration 134, loss = 0.013907707296311855
iteration 135, loss = 0.011267805472016335
iteration 136, loss = 0.010555966757237911
iteration 137, loss = 0.015384731814265251
iteration 138, loss = 0.01259321067482233
iteration 139, loss = 0.015751687809824944
iteration 140, loss = 0.010570618323981762
iteration 141, loss = 0.011644360609352589
iteration 142, loss = 0.011552168987691402
iteration 143, loss = 0.01836322620511055
iteration 144, loss = 0.011509189382195473
iteration 145, loss = 0.01521797850728035
iteration 146, loss = 0.014061184599995613
iteration 147, loss = 0.010991535149514675
iteration 148, loss = 0.014518633484840393
iteration 149, loss = 0.012574129737913609
iteration 150, loss = 0.01134131383150816
iteration 151, loss = 0.012590264901518822
iteration 152, loss = 0.015843937173485756
iteration 153, loss = 0.010842429473996162
iteration 154, loss = 0.01737399771809578
iteration 155, loss = 0.013037829659879208
iteration 156, loss = 0.015306364744901657
iteration 157, loss = 0.011637977324426174
iteration 158, loss = 0.011134170927107334
iteration 159, loss = 0.015368020161986351
iteration 160, loss = 0.014355839230120182
iteration 161, loss = 0.011173468083143234
iteration 162, loss = 0.011209404096007347
iteration 163, loss = 0.012132717296481133
iteration 164, loss = 0.011320501565933228
iteration 165, loss = 0.01492376159876585
iteration 166, loss = 0.016167104244232178
iteration 167, loss = 0.01083209365606308
iteration 168, loss = 0.010283938609063625
iteration 169, loss = 0.015158995054662228
iteration 170, loss = 0.011259129270911217
iteration 171, loss = 0.012472720816731453
iteration 172, loss = 0.013187910430133343
iteration 173, loss = 0.014743485487997532
iteration 174, loss = 0.010816404595971107
iteration 175, loss = 0.011216416954994202
iteration 176, loss = 0.013163941912353039
iteration 177, loss = 0.012026147916913033
iteration 178, loss = 0.010440947487950325
iteration 179, loss = 0.011226994916796684
iteration 180, loss = 0.011849201284348965
iteration 181, loss = 0.011774376034736633
iteration 182, loss = 0.011448318138718605
iteration 183, loss = 0.010310633108019829
iteration 184, loss = 0.012563941068947315
iteration 185, loss = 0.011291214264929295
iteration 186, loss = 0.01644580438733101
iteration 187, loss = 0.015188800171017647
iteration 188, loss = 0.011271575465798378
iteration 189, loss = 0.010482380166649818
iteration 190, loss = 0.011868069879710674
iteration 191, loss = 0.01718270406126976
iteration 192, loss = 0.011014748364686966
iteration 193, loss = 0.013014609925448895
iteration 194, loss = 0.014109558425843716
iteration 195, loss = 0.012294085696339607
iteration 196, loss = 0.014190416783094406
iteration 197, loss = 0.011761778965592384
iteration 198, loss = 0.011807544156908989
iteration 199, loss = 0.011844995431602001
iteration 200, loss = 0.023487044498324394
iteration 201, loss = 0.011729936115443707
iteration 202, loss = 0.015416404232382774
iteration 203, loss = 0.010972436517477036
iteration 204, loss = 0.010455472394824028
iteration 205, loss = 0.011073100380599499
iteration 206, loss = 0.01520269550383091
iteration 207, loss = 0.011814736761152744
iteration 208, loss = 0.011383261531591415
iteration 209, loss = 0.01159228291362524
iteration 210, loss = 0.0113203264772892
iteration 211, loss = 0.010918845422565937
iteration 212, loss = 0.011677255854010582
iteration 213, loss = 0.01119157113134861
iteration 214, loss = 0.011170228943228722
iteration 215, loss = 0.010896701365709305
iteration 216, loss = 0.014208367094397545
iteration 217, loss = 0.011078441515564919
iteration 218, loss = 0.01387783419340849
iteration 219, loss = 0.012005409225821495
iteration 220, loss = 0.01261422410607338
iteration 221, loss = 0.010667620226740837
iteration 222, loss = 0.01234331727027893
iteration 223, loss = 0.012798605486750603
iteration 224, loss = 0.011853944510221481
iteration 225, loss = 0.013076717965304852
iteration 226, loss = 0.01423167996108532
iteration 227, loss = 0.01564835011959076
iteration 228, loss = 0.012116914615035057
iteration 229, loss = 0.010976728051900864
iteration 230, loss = 0.011555148288607597
iteration 231, loss = 0.011222580447793007
iteration 232, loss = 0.01089305616915226
iteration 233, loss = 0.01451137289404869
iteration 234, loss = 0.011860095895826817
iteration 235, loss = 0.011488020420074463
iteration 236, loss = 0.011909553781151772
iteration 237, loss = 0.011541950516402721
iteration 238, loss = 0.011453228071331978
iteration 239, loss = 0.014674645848572254
iteration 240, loss = 0.012224334292113781
iteration 241, loss = 0.01420199777930975
iteration 242, loss = 0.01082692388445139
iteration 243, loss = 0.011415554210543633
iteration 244, loss = 0.011745570227503777
iteration 245, loss = 0.014265074394643307
iteration 246, loss = 0.011040830984711647
iteration 247, loss = 0.016142508015036583
iteration 248, loss = 0.012586084194481373
iteration 249, loss = 0.011403867974877357
iteration 250, loss = 0.01186866033822298
iteration 251, loss = 0.011880842968821526
iteration 252, loss = 0.015864107757806778
iteration 253, loss = 0.011148731224238873
iteration 254, loss = 0.016466049477458
iteration 255, loss = 0.012460482306778431
iteration 256, loss = 0.012329195626080036
iteration 257, loss = 0.012958423234522343
iteration 258, loss = 0.014663801528513432
iteration 259, loss = 0.012107642367482185
iteration 260, loss = 0.014528249390423298
iteration 261, loss = 0.013685043901205063
iteration 262, loss = 0.013085179962217808
iteration 263, loss = 0.011362629011273384
iteration 264, loss = 0.011290734633803368
iteration 265, loss = 0.01219229493290186
iteration 266, loss = 0.010686702094972134
iteration 267, loss = 0.01221221499145031
iteration 268, loss = 0.010783172212541103
iteration 269, loss = 0.011189629323780537
iteration 270, loss = 0.011770399287343025
iteration 271, loss = 0.01141451671719551
iteration 272, loss = 0.011151823215186596
iteration 273, loss = 0.015451877377927303
iteration 274, loss = 0.01180419884622097
iteration 275, loss = 0.01241590641438961
iteration 276, loss = 0.01154634915292263
iteration 277, loss = 0.012912197038531303
iteration 278, loss = 0.01096342783421278
iteration 279, loss = 0.012395697645843029
iteration 280, loss = 0.012614026665687561
iteration 281, loss = 0.010817388072609901
iteration 282, loss = 0.01279087271541357
iteration 283, loss = 0.011761562898755074
iteration 284, loss = 0.01406038273125887
iteration 285, loss = 0.01100014429539442
iteration 286, loss = 0.011380063369870186
iteration 287, loss = 0.011602402664721012
iteration 288, loss = 0.01272105798125267
iteration 289, loss = 0.016933158040046692
iteration 290, loss = 0.011087002232670784
iteration 291, loss = 0.012068555690348148
iteration 292, loss = 0.014771100133657455
iteration 293, loss = 0.013555281795561314
iteration 294, loss = 0.010584549978375435
iteration 295, loss = 0.01564236916601658
iteration 296, loss = 0.01146589033305645
iteration 297, loss = 0.01071412954479456
iteration 298, loss = 0.014070631936192513
iteration 299, loss = 0.012477895244956017
iteration 300, loss = 0.013903828337788582
iteration 1, loss = 0.010498186573386192
iteration 2, loss = 0.01272717397660017
iteration 3, loss = 0.01400661002844572
iteration 4, loss = 0.011229953728616238
iteration 5, loss = 0.015553349629044533
iteration 6, loss = 0.013907797634601593
iteration 7, loss = 0.015413387678563595
iteration 8, loss = 0.011934780515730381
iteration 9, loss = 0.011676298454403877
iteration 10, loss = 0.016495659947395325
iteration 11, loss = 0.01610657386481762
iteration 12, loss = 0.015118623152375221
iteration 13, loss = 0.012096211314201355
iteration 14, loss = 0.011846122331917286
iteration 15, loss = 0.011235859245061874
iteration 16, loss = 0.01544729433953762
iteration 17, loss = 0.013947977684438229
iteration 18, loss = 0.01980849727988243
iteration 19, loss = 0.010945232585072517
iteration 20, loss = 0.01674741320312023
iteration 21, loss = 0.010997191071510315
iteration 22, loss = 0.011363601312041283
iteration 23, loss = 0.0106735210865736
iteration 24, loss = 0.01109524816274643
iteration 25, loss = 0.0125043373554945
iteration 26, loss = 0.016447456553578377
iteration 27, loss = 0.011371402069926262
iteration 28, loss = 0.011633634567260742
iteration 29, loss = 0.011448591016232967
iteration 30, loss = 0.013840935193002224
iteration 31, loss = 0.011465927585959435
iteration 32, loss = 0.013224254362285137
iteration 33, loss = 0.020485330373048782
iteration 34, loss = 0.011917496100068092
iteration 35, loss = 0.01237462181597948
iteration 36, loss = 0.011726458556950092
iteration 37, loss = 0.013084270060062408
iteration 38, loss = 0.011035802774131298
iteration 39, loss = 0.010509457439184189
iteration 40, loss = 0.012839565984904766
iteration 41, loss = 0.01383703388273716
iteration 42, loss = 0.011852435767650604
iteration 43, loss = 0.010847114957869053
iteration 44, loss = 0.014242994599044323
iteration 45, loss = 0.011745400726795197
iteration 46, loss = 0.015437877736985683
iteration 47, loss = 0.011050079017877579
iteration 48, loss = 0.01134639885276556
iteration 49, loss = 0.010813899338245392
iteration 50, loss = 0.011300219222903252
iteration 51, loss = 0.011738337576389313
iteration 52, loss = 0.010921400040388107
iteration 53, loss = 0.012017089873552322
iteration 54, loss = 0.01528927031904459
iteration 55, loss = 0.010655186139047146
iteration 56, loss = 0.011082770302891731
iteration 57, loss = 0.010499863885343075
iteration 58, loss = 0.01286056637763977
iteration 59, loss = 0.014955924823880196
iteration 60, loss = 0.011047386564314365
iteration 61, loss = 0.011306039057672024
iteration 62, loss = 0.010651061311364174
iteration 63, loss = 0.011357595212757587
iteration 64, loss = 0.018917182460427284
iteration 65, loss = 0.014448361471295357
iteration 66, loss = 0.013948465697467327
iteration 67, loss = 0.014211034402251244
iteration 68, loss = 0.017359258607029915
iteration 69, loss = 0.0158667154610157
iteration 70, loss = 0.01572248712182045
iteration 71, loss = 0.011118060909211636
iteration 72, loss = 0.011556422337889671
iteration 73, loss = 0.0112784868106246
iteration 74, loss = 0.011749130673706532
iteration 75, loss = 0.012242033146321774
iteration 76, loss = 0.011017212644219398
iteration 77, loss = 0.010973935946822166
iteration 78, loss = 0.011519886553287506
iteration 79, loss = 0.011326189152896404
iteration 80, loss = 0.011880236677825451
iteration 81, loss = 0.011681284755468369
iteration 82, loss = 0.011633019894361496
iteration 83, loss = 0.018066320568323135
iteration 84, loss = 0.012246456928551197
iteration 85, loss = 0.012152960523962975
iteration 86, loss = 0.015152343548834324
iteration 87, loss = 0.011802859604358673
iteration 88, loss = 0.011356272734701633
iteration 89, loss = 0.01490122638642788
iteration 90, loss = 0.010767401196062565
iteration 91, loss = 0.011338426731526852
iteration 92, loss = 0.01679004356265068
iteration 93, loss = 0.011546127498149872
iteration 94, loss = 0.01391941960901022
iteration 95, loss = 0.010765952989459038
iteration 96, loss = 0.012546050362288952
iteration 97, loss = 0.011420030146837234
iteration 98, loss = 0.010897135362029076
iteration 99, loss = 0.011337422765791416
iteration 100, loss = 0.01165662333369255
iteration 101, loss = 0.013670750893652439
iteration 102, loss = 0.014133550226688385
iteration 103, loss = 0.012396114878356457
iteration 104, loss = 0.011126894503831863
iteration 105, loss = 0.01128384843468666
iteration 106, loss = 0.018672538921236992
iteration 107, loss = 0.012190494686365128
iteration 108, loss = 0.011522510088980198
iteration 109, loss = 0.011866269633173943
iteration 110, loss = 0.01200326532125473
iteration 111, loss = 0.011754360981285572
iteration 112, loss = 0.013583870604634285
iteration 113, loss = 0.011486678384244442
iteration 114, loss = 0.014743407256901264
iteration 115, loss = 0.011923359706997871
iteration 116, loss = 0.013144944794476032
iteration 117, loss = 0.011152652092278004
iteration 118, loss = 0.015487704426050186
iteration 119, loss = 0.010552234016358852
iteration 120, loss = 0.011186668649315834
iteration 121, loss = 0.014138140715658665
iteration 122, loss = 0.011380591429769993
iteration 123, loss = 0.01850626803934574
iteration 124, loss = 0.015850983560085297
iteration 125, loss = 0.01774468459188938
iteration 126, loss = 0.010809575207531452
iteration 127, loss = 0.013729054480791092
iteration 128, loss = 0.013985097408294678
iteration 129, loss = 0.011033627204596996
iteration 130, loss = 0.014863144606351852
iteration 131, loss = 0.011470505967736244
iteration 132, loss = 0.015978841111063957
iteration 133, loss = 0.012084602378308773
iteration 134, loss = 0.011533142998814583
iteration 135, loss = 0.013037865981459618
iteration 136, loss = 0.011426720768213272
iteration 137, loss = 0.013753561303019524
iteration 138, loss = 0.012662699446082115
iteration 139, loss = 0.011329005472362041
iteration 140, loss = 0.014511083252727985
iteration 141, loss = 0.012167772278189659
iteration 142, loss = 0.012939352542161942
iteration 143, loss = 0.014131484553217888
iteration 144, loss = 0.010914752259850502
iteration 145, loss = 0.0169050469994545
iteration 146, loss = 0.015560963191092014
iteration 147, loss = 0.016517428681254387
iteration 148, loss = 0.011139496229588985
iteration 149, loss = 0.015497957356274128
iteration 150, loss = 0.012673028744757175
iteration 151, loss = 0.0111059146001935
iteration 152, loss = 0.01179931778460741
iteration 153, loss = 0.011252461932599545
iteration 154, loss = 0.010784339159727097
iteration 155, loss = 0.011292724870145321
iteration 156, loss = 0.01068210881203413
iteration 157, loss = 0.011803529225289822
iteration 158, loss = 0.012369302101433277
iteration 159, loss = 0.01077350415289402
iteration 160, loss = 0.011213161051273346
iteration 161, loss = 0.01130689587444067
iteration 162, loss = 0.012060697190463543
iteration 163, loss = 0.011150241829454899
iteration 164, loss = 0.015042727813124657
iteration 165, loss = 0.011928768828511238
iteration 166, loss = 0.01091441698372364
iteration 167, loss = 0.010337097570300102
iteration 168, loss = 0.015571138821542263
iteration 169, loss = 0.014825200662016869
iteration 170, loss = 0.014942340552806854
iteration 171, loss = 0.014465944841504097
iteration 172, loss = 0.010617617517709732
iteration 173, loss = 0.0115360664203763
iteration 174, loss = 0.011051162146031857
iteration 175, loss = 0.010907774791121483
iteration 176, loss = 0.011268784292042255
iteration 177, loss = 0.012322647497057915
iteration 178, loss = 0.015506453812122345
iteration 179, loss = 0.011448832228779793
iteration 180, loss = 0.010830809362232685
iteration 181, loss = 0.011129825375974178
iteration 182, loss = 0.015205025672912598
iteration 183, loss = 0.011119897477328777
iteration 184, loss = 0.011832037009298801
iteration 185, loss = 0.011005822569131851
iteration 186, loss = 0.011284182779490948
iteration 187, loss = 0.014557067304849625
iteration 188, loss = 0.015169215388596058
iteration 189, loss = 0.010736151598393917
iteration 190, loss = 0.012234781868755817
iteration 191, loss = 0.014131956733763218
iteration 192, loss = 0.0127483569085598
iteration 193, loss = 0.013791454955935478
iteration 194, loss = 0.014303658157587051
iteration 195, loss = 0.014508260414004326
iteration 196, loss = 0.01483064703643322
iteration 197, loss = 0.014635035768151283
iteration 198, loss = 0.015049535781145096
iteration 199, loss = 0.010616335086524487
iteration 200, loss = 0.01819472573697567
iteration 201, loss = 0.01477580051869154
iteration 202, loss = 0.012828152626752853
iteration 203, loss = 0.011450272053480148
iteration 204, loss = 0.014588273130357265
iteration 205, loss = 0.014932254329323769
iteration 206, loss = 0.015286148525774479
iteration 207, loss = 0.011541694402694702
iteration 208, loss = 0.01099915150552988
iteration 209, loss = 0.011479118838906288
iteration 210, loss = 0.021709468215703964
iteration 211, loss = 0.011050504632294178
iteration 212, loss = 0.011324250139296055
iteration 213, loss = 0.011086590588092804
iteration 214, loss = 0.010923628695309162
iteration 215, loss = 0.014510724693536758
iteration 216, loss = 0.011175473220646381
iteration 217, loss = 0.011042078025639057
iteration 218, loss = 0.02072617970407009
iteration 219, loss = 0.013881084509193897
iteration 220, loss = 0.012083016335964203
iteration 221, loss = 0.011350823566317558
iteration 222, loss = 0.012065915390849113
iteration 223, loss = 0.011781825684010983
iteration 224, loss = 0.019229335710406303
iteration 225, loss = 0.010912262834608555
iteration 226, loss = 0.010551928542554379
iteration 227, loss = 0.011430367827415466
iteration 228, loss = 0.011081138625741005
iteration 229, loss = 0.010687604546546936
iteration 230, loss = 0.010976918041706085
iteration 231, loss = 0.011413345113396645
iteration 232, loss = 0.011173488572239876
iteration 233, loss = 0.011867837980389595
iteration 234, loss = 0.011916079558432102
iteration 235, loss = 0.011561035178601742
iteration 236, loss = 0.018821071833372116
iteration 237, loss = 0.011131854727864265
iteration 238, loss = 0.013938356190919876
iteration 239, loss = 0.01438288576900959
iteration 240, loss = 0.014042600989341736
iteration 241, loss = 0.013988243415951729
iteration 242, loss = 0.010708627291023731
iteration 243, loss = 0.011043930426239967
iteration 244, loss = 0.012104220688343048
iteration 245, loss = 0.011140567250549793
iteration 246, loss = 0.014979725703597069
iteration 247, loss = 0.01181982271373272
iteration 248, loss = 0.011775290593504906
iteration 249, loss = 0.010824354365468025
iteration 250, loss = 0.0127019714564085
iteration 251, loss = 0.01378378551453352
iteration 252, loss = 0.015696262940764427
iteration 253, loss = 0.011025079526007175
iteration 254, loss = 0.010808012448251247
iteration 255, loss = 0.014885468408465385
iteration 256, loss = 0.01107247918844223
iteration 257, loss = 0.01134594064205885
iteration 258, loss = 0.011211137287318707
iteration 259, loss = 0.014565120451152325
iteration 260, loss = 0.013441033661365509
iteration 261, loss = 0.010907738469541073
iteration 262, loss = 0.01041208952665329
iteration 263, loss = 0.015942657366394997
iteration 264, loss = 0.018178697675466537
iteration 265, loss = 0.013719405978918076
iteration 266, loss = 0.012569035403430462
iteration 267, loss = 0.012096856720745564
iteration 268, loss = 0.01217997632920742
iteration 269, loss = 0.011685189791023731
iteration 270, loss = 0.013076793402433395
iteration 271, loss = 0.010977472178637981
iteration 272, loss = 0.0123161431401968
iteration 273, loss = 0.01191797573119402
iteration 274, loss = 0.0141095370054245
iteration 275, loss = 0.015018428675830364
iteration 276, loss = 0.011289767920970917
iteration 277, loss = 0.010763062164187431
iteration 278, loss = 0.01131831668317318
iteration 279, loss = 0.011156330816447735
iteration 280, loss = 0.010807640850543976
iteration 281, loss = 0.010774854570627213
iteration 282, loss = 0.0186686459928751
iteration 283, loss = 0.010812526568770409
iteration 284, loss = 0.01279747299849987
iteration 285, loss = 0.013563076965510845
iteration 286, loss = 0.011227871291339397
iteration 287, loss = 0.011586727574467659
iteration 288, loss = 0.010891741141676903
iteration 289, loss = 0.017309796065092087
iteration 290, loss = 0.012312477454543114
iteration 291, loss = 0.013052827678620815
iteration 292, loss = 0.011575085110962391
iteration 293, loss = 0.011001192964613438
iteration 294, loss = 0.016643982380628586
iteration 295, loss = 0.013569891452789307
iteration 296, loss = 0.01187740359455347
iteration 297, loss = 0.01085126306861639
iteration 298, loss = 0.012312717735767365
iteration 299, loss = 0.011494596488773823
iteration 300, loss = 0.013233263045549393
iteration 1, loss = 0.013176318258047104
iteration 2, loss = 0.01574399508535862
iteration 3, loss = 0.014852011576294899
iteration 4, loss = 0.01445290818810463
iteration 5, loss = 0.01240787748247385
iteration 6, loss = 0.013532268814742565
iteration 7, loss = 0.010613747872412205
iteration 8, loss = 0.011513492092490196
iteration 9, loss = 0.011116991750895977
iteration 10, loss = 0.012931907549500465
iteration 11, loss = 0.010890904814004898
iteration 12, loss = 0.012257510796189308
iteration 13, loss = 0.011166977696120739
iteration 14, loss = 0.014123744331300259
iteration 15, loss = 0.016633594408631325
iteration 16, loss = 0.011732972227036953
iteration 17, loss = 0.018569806590676308
iteration 18, loss = 0.014032820239663124
iteration 19, loss = 0.012069526128470898
iteration 20, loss = 0.010947379283607006
iteration 21, loss = 0.015554001554846764
iteration 22, loss = 0.0135151706635952
iteration 23, loss = 0.016289930790662766
iteration 24, loss = 0.013823342509567738
iteration 25, loss = 0.017141425982117653
iteration 26, loss = 0.012381820008158684
iteration 27, loss = 0.012491848319768906
iteration 28, loss = 0.010979860089719296
iteration 29, loss = 0.011109390296041965
iteration 30, loss = 0.011133581399917603
iteration 31, loss = 0.01115835178643465
iteration 32, loss = 0.011452838778495789
iteration 33, loss = 0.013125328347086906
iteration 34, loss = 0.016036825254559517
iteration 35, loss = 0.01162696536630392
iteration 36, loss = 0.011433422565460205
iteration 37, loss = 0.010911481454968452
iteration 38, loss = 0.01212112046778202
iteration 39, loss = 0.014898940920829773
iteration 40, loss = 0.010363470762968063
iteration 41, loss = 0.012097114697098732
iteration 42, loss = 0.012680727057158947
iteration 43, loss = 0.011679170653223991
iteration 44, loss = 0.01504859421402216
iteration 45, loss = 0.012064523063600063
iteration 46, loss = 0.011219176463782787
iteration 47, loss = 0.011819427832961082
iteration 48, loss = 0.017712537199258804
iteration 49, loss = 0.011366493068635464
iteration 50, loss = 0.0140825305134058
iteration 51, loss = 0.012041879817843437
iteration 52, loss = 0.010426793247461319
iteration 53, loss = 0.012206428684294224
iteration 54, loss = 0.014842988923192024
iteration 55, loss = 0.01163058914244175
iteration 56, loss = 0.011731692589819431
iteration 57, loss = 0.011294687166810036
iteration 58, loss = 0.011849282309412956
iteration 59, loss = 0.011707613244652748
iteration 60, loss = 0.01432669349014759
iteration 61, loss = 0.015589118003845215
iteration 62, loss = 0.01464647427201271
iteration 63, loss = 0.011178922839462757
iteration 64, loss = 0.010691897943615913
iteration 65, loss = 0.011692821979522705
iteration 66, loss = 0.012076065875589848
iteration 67, loss = 0.016422584652900696
iteration 68, loss = 0.011917533352971077
iteration 69, loss = 0.010967859998345375
iteration 70, loss = 0.011857442557811737
iteration 71, loss = 0.01199151948094368
iteration 72, loss = 0.011151226237416267
iteration 73, loss = 0.010718981735408306
iteration 74, loss = 0.012321321293711662
iteration 75, loss = 0.010657581500709057
iteration 76, loss = 0.011490613222122192
iteration 77, loss = 0.011250682175159454
iteration 78, loss = 0.015605014748871326
iteration 79, loss = 0.011143752373754978
iteration 80, loss = 0.014970177784562111
iteration 81, loss = 0.013613490387797356
iteration 82, loss = 0.013782004825770855
iteration 83, loss = 0.011498345993459225
iteration 84, loss = 0.011721303686499596
iteration 85, loss = 0.011036371812224388
iteration 86, loss = 0.017316780984401703
iteration 87, loss = 0.01125373039394617
iteration 88, loss = 0.014013596810400486
iteration 89, loss = 0.013282802887260914
iteration 90, loss = 0.016115359961986542
iteration 91, loss = 0.01234886422753334
iteration 92, loss = 0.012186399661004543
iteration 93, loss = 0.012124941684305668
iteration 94, loss = 0.010937778279185295
iteration 95, loss = 0.01104968786239624
iteration 96, loss = 0.011457046493887901
iteration 97, loss = 0.01174318976700306
iteration 98, loss = 0.01097653154283762
iteration 99, loss = 0.010895300656557083
iteration 100, loss = 0.011334756389260292
iteration 101, loss = 0.010837678797543049
iteration 102, loss = 0.014322088100016117
iteration 103, loss = 0.012309061363339424
iteration 104, loss = 0.014088527299463749
iteration 105, loss = 0.011918394826352596
iteration 106, loss = 0.014811653643846512
iteration 107, loss = 0.010483278892934322
iteration 108, loss = 0.01504620909690857
iteration 109, loss = 0.012037694454193115
iteration 110, loss = 0.013092088513076305
iteration 111, loss = 0.012276330962777138
iteration 112, loss = 0.01207822933793068
iteration 113, loss = 0.019534051418304443
iteration 114, loss = 0.01167504582554102
iteration 115, loss = 0.011107848025858402
iteration 116, loss = 0.013003435917198658
iteration 117, loss = 0.011437594890594482
iteration 118, loss = 0.012022124603390694
iteration 119, loss = 0.010989729315042496
iteration 120, loss = 0.010990338400006294
iteration 121, loss = 0.011206941679120064
iteration 122, loss = 0.014461172744631767
iteration 123, loss = 0.01562584936618805
iteration 124, loss = 0.011491981334984303
iteration 125, loss = 0.011594589799642563
iteration 126, loss = 0.014094553887844086
iteration 127, loss = 0.012372075580060482
iteration 128, loss = 0.013891473412513733
iteration 129, loss = 0.016631340608000755
iteration 130, loss = 0.01592058502137661
iteration 131, loss = 0.011938828974962234
iteration 132, loss = 0.015601420775055885
iteration 133, loss = 0.01180416438728571
iteration 134, loss = 0.011858844198286533
iteration 135, loss = 0.01472887210547924
iteration 136, loss = 0.014187652617692947
iteration 137, loss = 0.010420243255794048
iteration 138, loss = 0.011301064863801003
iteration 139, loss = 0.021606851369142532
iteration 140, loss = 0.011549493297934532
iteration 141, loss = 0.010820159688591957
iteration 142, loss = 0.018111586570739746
iteration 143, loss = 0.013524278998374939
iteration 144, loss = 0.011962487362325191
iteration 145, loss = 0.012258818373084068
iteration 146, loss = 0.011773337610065937
iteration 147, loss = 0.01479172334074974
iteration 148, loss = 0.01115555688738823
iteration 149, loss = 0.010820728726685047
iteration 150, loss = 0.014788354746997356
iteration 151, loss = 0.010628479532897472
iteration 152, loss = 0.01622268371284008
iteration 153, loss = 0.014864658936858177
iteration 154, loss = 0.014379230327904224
iteration 155, loss = 0.01051355991512537
iteration 156, loss = 0.011737825348973274
iteration 157, loss = 0.010933231562376022
iteration 158, loss = 0.015856925398111343
iteration 159, loss = 0.01313665509223938
iteration 160, loss = 0.01147769670933485
iteration 161, loss = 0.010776838287711143
iteration 162, loss = 0.011653791181743145
iteration 163, loss = 0.014321364462375641
iteration 164, loss = 0.010654705576598644
iteration 165, loss = 0.011511964723467827
iteration 166, loss = 0.018070634454488754
iteration 167, loss = 0.013546427711844444
iteration 168, loss = 0.012144919484853745
iteration 169, loss = 0.011669119819998741
iteration 170, loss = 0.01136645209044218
iteration 171, loss = 0.01918306201696396
iteration 172, loss = 0.012206497602164745
iteration 173, loss = 0.011092809028923512
iteration 174, loss = 0.014913700520992279
iteration 175, loss = 0.012915413826704025
iteration 176, loss = 0.011031027883291245
iteration 177, loss = 0.014664717949926853
iteration 178, loss = 0.011313745751976967
iteration 179, loss = 0.011060523800551891
iteration 180, loss = 0.01367853581905365
iteration 181, loss = 0.010656662285327911
iteration 182, loss = 0.011750415898859501
iteration 183, loss = 0.01097214687615633
iteration 184, loss = 0.016006484627723694
iteration 185, loss = 0.012127839960157871
iteration 186, loss = 0.013346114195883274
iteration 187, loss = 0.01825406216084957
iteration 188, loss = 0.01109353918582201
iteration 189, loss = 0.014726231805980206
iteration 190, loss = 0.011432171799242496
iteration 191, loss = 0.011280866339802742
iteration 192, loss = 0.014455212280154228
iteration 193, loss = 0.01448867004364729
iteration 194, loss = 0.017979217693209648
iteration 195, loss = 0.011795432306826115
iteration 196, loss = 0.01073160395026207
iteration 197, loss = 0.010366315953433514
iteration 198, loss = 0.011641694232821465
iteration 199, loss = 0.015844227746129036
iteration 200, loss = 0.011165830306708813
iteration 201, loss = 0.011734399944543839
iteration 202, loss = 0.011309487745165825
iteration 203, loss = 0.010610018856823444
iteration 204, loss = 0.010865100659430027
iteration 205, loss = 0.012558446265757084
iteration 206, loss = 0.010678849183022976
iteration 207, loss = 0.01219999510794878
iteration 208, loss = 0.011044256389141083
iteration 209, loss = 0.011261014267802238
iteration 210, loss = 0.01344857458025217
iteration 211, loss = 0.011204895563423634
iteration 212, loss = 0.011234989389777184
iteration 213, loss = 0.014584510587155819
iteration 214, loss = 0.010742745362222195
iteration 215, loss = 0.012108514085412025
iteration 216, loss = 0.01621152088046074
iteration 217, loss = 0.010761896148324013
iteration 218, loss = 0.016687307506799698
iteration 219, loss = 0.01970924809575081
iteration 220, loss = 0.015794213861227036
iteration 221, loss = 0.011505838483572006
iteration 222, loss = 0.012110326439142227
iteration 223, loss = 0.013091353699564934
iteration 224, loss = 0.01136515662074089
iteration 225, loss = 0.01072127465158701
iteration 226, loss = 0.014231652952730656
iteration 227, loss = 0.016949042677879333
iteration 228, loss = 0.011163369752466679
iteration 229, loss = 0.01376870833337307
iteration 230, loss = 0.016582360491156578
iteration 231, loss = 0.014278072863817215
iteration 232, loss = 0.011553696356713772
iteration 233, loss = 0.010501537472009659
iteration 234, loss = 0.011571839451789856
iteration 235, loss = 0.010224522091448307
iteration 236, loss = 0.011146976612508297
iteration 237, loss = 0.012293007224798203
iteration 238, loss = 0.012211128138005733
iteration 239, loss = 0.011348661035299301
iteration 240, loss = 0.011306545697152615
iteration 241, loss = 0.011222591623663902
iteration 242, loss = 0.012349515222012997
iteration 243, loss = 0.013677041977643967
iteration 244, loss = 0.010821636766195297
iteration 245, loss = 0.015235438011586666
iteration 246, loss = 0.015549498610198498
iteration 247, loss = 0.011410348117351532
iteration 248, loss = 0.01210132334381342
iteration 249, loss = 0.014614086598157883
iteration 250, loss = 0.013799481093883514
iteration 251, loss = 0.013959445059299469
iteration 252, loss = 0.013813398778438568
iteration 253, loss = 0.011927291750907898
iteration 254, loss = 0.011027572676539421
iteration 255, loss = 0.010979924350976944
iteration 256, loss = 0.010525709949433804
iteration 257, loss = 0.01293960027396679
iteration 258, loss = 0.01121275220066309
iteration 259, loss = 0.010506800375878811
iteration 260, loss = 0.02100372314453125
iteration 261, loss = 0.011719565838575363
iteration 262, loss = 0.012713928706943989
iteration 263, loss = 0.012643654830753803
iteration 264, loss = 0.012051060795783997
iteration 265, loss = 0.012838518247008324
iteration 266, loss = 0.011385463178157806
iteration 267, loss = 0.011090428568422794
iteration 268, loss = 0.011322579346597195
iteration 269, loss = 0.012414254248142242
iteration 270, loss = 0.015985306352376938
iteration 271, loss = 0.01787419058382511
iteration 272, loss = 0.011181652545928955
iteration 273, loss = 0.011362185701727867
iteration 274, loss = 0.011502711102366447
iteration 275, loss = 0.015311912633478642
iteration 276, loss = 0.011872396804392338
iteration 277, loss = 0.010872920975089073
iteration 278, loss = 0.014661544933915138
iteration 279, loss = 0.01090284064412117
iteration 280, loss = 0.011985892429947853
iteration 281, loss = 0.012156371958553791
iteration 282, loss = 0.014483703300356865
iteration 283, loss = 0.014355415478348732
iteration 284, loss = 0.011754084378480911
iteration 285, loss = 0.010762355290353298
iteration 286, loss = 0.010835719294846058
iteration 287, loss = 0.015299327671527863
iteration 288, loss = 0.014704483561217785
iteration 289, loss = 0.011938759125769138
iteration 290, loss = 0.010553054511547089
iteration 291, loss = 0.011310395784676075
iteration 292, loss = 0.010449301451444626
iteration 293, loss = 0.0112050361931324
iteration 294, loss = 0.010694609954953194
iteration 295, loss = 0.01097575481981039
iteration 296, loss = 0.014016248285770416
iteration 297, loss = 0.012216491624712944
iteration 298, loss = 0.010507744736969471
iteration 299, loss = 0.016234183683991432
iteration 300, loss = 0.011050586588680744
iteration 1, loss = 0.012508384883403778
iteration 2, loss = 0.014291459694504738
iteration 3, loss = 0.011106216348707676
iteration 4, loss = 0.01064646802842617
iteration 5, loss = 0.016178768128156662
iteration 6, loss = 0.010854287073016167
iteration 7, loss = 0.012273511849343777
iteration 8, loss = 0.01166626438498497
iteration 9, loss = 0.011257614009082317
iteration 10, loss = 0.011286493390798569
iteration 11, loss = 0.01087968610227108
iteration 12, loss = 0.012473168782889843
iteration 13, loss = 0.012804648838937283
iteration 14, loss = 0.011150426231324673
iteration 15, loss = 0.01359705813229084
iteration 16, loss = 0.010961895808577538
iteration 17, loss = 0.013045838102698326
iteration 18, loss = 0.011199241504073143
iteration 19, loss = 0.01770312339067459
iteration 20, loss = 0.01593409851193428
iteration 21, loss = 0.010960422456264496
iteration 22, loss = 0.013365838676691055
iteration 23, loss = 0.010554226115345955
iteration 24, loss = 0.01208485197275877
iteration 25, loss = 0.011572219431400299
iteration 26, loss = 0.01084386557340622
iteration 27, loss = 0.010317674838006496
iteration 28, loss = 0.016702374443411827
iteration 29, loss = 0.01196597982198
iteration 30, loss = 0.011786170303821564
iteration 31, loss = 0.01164328120648861
iteration 32, loss = 0.010312736965715885
iteration 33, loss = 0.014517259784042835
iteration 34, loss = 0.010856302455067635
iteration 35, loss = 0.013131463900208473
iteration 36, loss = 0.012568441219627857
iteration 37, loss = 0.016919206827878952
iteration 38, loss = 0.01469779945909977
iteration 39, loss = 0.014782842248678207
iteration 40, loss = 0.013390307314693928
iteration 41, loss = 0.011497142724692822
iteration 42, loss = 0.01078836340457201
iteration 43, loss = 0.016269998624920845
iteration 44, loss = 0.012889793142676353
iteration 45, loss = 0.010985944420099258
iteration 46, loss = 0.011201509274542332
iteration 47, loss = 0.011520949192345142
iteration 48, loss = 0.010834687389433384
iteration 49, loss = 0.01201781164854765
iteration 50, loss = 0.010724351741373539
iteration 51, loss = 0.012122482992708683
iteration 52, loss = 0.01752178557217121
iteration 53, loss = 0.014017676003277302
iteration 54, loss = 0.011667093262076378
iteration 55, loss = 0.014248338527977467
iteration 56, loss = 0.016228480264544487
iteration 57, loss = 0.011981036514043808
iteration 58, loss = 0.012558208778500557
iteration 59, loss = 0.010579115711152554
iteration 60, loss = 0.01066342182457447
iteration 61, loss = 0.011602569371461868
iteration 62, loss = 0.011032610200345516
iteration 63, loss = 0.012094669044017792
iteration 64, loss = 0.013143228366971016
iteration 65, loss = 0.015040794387459755
iteration 66, loss = 0.01141864713281393
iteration 67, loss = 0.011322821490466595
iteration 68, loss = 0.010578081011772156
iteration 69, loss = 0.015170658007264137
iteration 70, loss = 0.01142400037497282
iteration 71, loss = 0.01568661816418171
iteration 72, loss = 0.011854872107505798
iteration 73, loss = 0.01124504953622818
iteration 74, loss = 0.010955006815493107
iteration 75, loss = 0.012044863775372505
iteration 76, loss = 0.014473672956228256
iteration 77, loss = 0.011275522410869598
iteration 78, loss = 0.01742122881114483
iteration 79, loss = 0.0117295291274786
iteration 80, loss = 0.014324549585580826
iteration 81, loss = 0.01859278604388237
iteration 82, loss = 0.01424622256308794
iteration 83, loss = 0.010644011199474335
iteration 84, loss = 0.01217442937195301
iteration 85, loss = 0.01281071174889803
iteration 86, loss = 0.016565628349781036
iteration 87, loss = 0.011694890446960926
iteration 88, loss = 0.02057241089642048
iteration 89, loss = 0.011667738668620586
iteration 90, loss = 0.011103714816272259
iteration 91, loss = 0.014965256676077843
iteration 92, loss = 0.010694338008761406
iteration 93, loss = 0.012333936057984829
iteration 94, loss = 0.011896719224750996
iteration 95, loss = 0.011777902953326702
iteration 96, loss = 0.01116529293358326
iteration 97, loss = 0.014084176160395145
iteration 98, loss = 0.016673222184181213
iteration 99, loss = 0.015196103602647781
iteration 100, loss = 0.01230541616678238
iteration 101, loss = 0.010989266447722912
iteration 102, loss = 0.014551027677953243
iteration 103, loss = 0.015757130458950996
iteration 104, loss = 0.015176360495388508
iteration 105, loss = 0.013995281420648098
iteration 106, loss = 0.01689028926193714
iteration 107, loss = 0.01292567141354084
iteration 108, loss = 0.01610163226723671
iteration 109, loss = 0.017455345019698143
iteration 110, loss = 0.012313743121922016
iteration 111, loss = 0.01841554045677185
iteration 112, loss = 0.01149076409637928
iteration 113, loss = 0.011228200048208237
iteration 114, loss = 0.014918663538992405
iteration 115, loss = 0.01593838818371296
iteration 116, loss = 0.011072985827922821
iteration 117, loss = 0.011086812242865562
iteration 118, loss = 0.010788886807858944
iteration 119, loss = 0.011373726651072502
iteration 120, loss = 0.017206590622663498
iteration 121, loss = 0.011031976900994778
iteration 122, loss = 0.010604632087051868
iteration 123, loss = 0.013933518901467323
iteration 124, loss = 0.014560048468410969
iteration 125, loss = 0.010246257297694683
iteration 126, loss = 0.013956432230770588
iteration 127, loss = 0.011374495923519135
iteration 128, loss = 0.012545469217002392
iteration 129, loss = 0.015067040920257568
iteration 130, loss = 0.011568174697458744
iteration 131, loss = 0.015286988578736782
iteration 132, loss = 0.011791225522756577
iteration 133, loss = 0.010965544730424881
iteration 134, loss = 0.011414005421102047
iteration 135, loss = 0.014226059429347515
iteration 136, loss = 0.010509515181183815
iteration 137, loss = 0.011802724562585354
iteration 138, loss = 0.01105216983705759
iteration 139, loss = 0.014579764567315578
iteration 140, loss = 0.010608087293803692
iteration 141, loss = 0.011128327809274197
iteration 142, loss = 0.010811854153871536
iteration 143, loss = 0.010436083190143108
iteration 144, loss = 0.014965452253818512
iteration 145, loss = 0.012473360635340214
iteration 146, loss = 0.01175423339009285
iteration 147, loss = 0.011455177329480648
iteration 148, loss = 0.015439450740814209
iteration 149, loss = 0.01226283609867096
iteration 150, loss = 0.011495710350573063
iteration 151, loss = 0.01482220459729433
iteration 152, loss = 0.011677647940814495
iteration 153, loss = 0.013825666159391403
iteration 154, loss = 0.010968302376568317
iteration 155, loss = 0.012548399157822132
iteration 156, loss = 0.011147486045956612
iteration 157, loss = 0.012413302436470985
iteration 158, loss = 0.012065228074789047
iteration 159, loss = 0.012129025533795357
iteration 160, loss = 0.010495657101273537
iteration 161, loss = 0.01137861330062151
iteration 162, loss = 0.010783819481730461
iteration 163, loss = 0.014601964503526688
iteration 164, loss = 0.013352800160646439
iteration 165, loss = 0.010590550489723682
iteration 166, loss = 0.01158699207007885
iteration 167, loss = 0.01419331319630146
iteration 168, loss = 0.011505300179123878
iteration 169, loss = 0.012292884290218353
iteration 170, loss = 0.01124691590666771
iteration 171, loss = 0.01100393570959568
iteration 172, loss = 0.013071177527308464
iteration 173, loss = 0.01644950360059738
iteration 174, loss = 0.0198881383985281
iteration 175, loss = 0.010842323303222656
iteration 176, loss = 0.011500315740704536
iteration 177, loss = 0.011504614725708961
iteration 178, loss = 0.012881996110081673
iteration 179, loss = 0.011976602487266064
iteration 180, loss = 0.011875845491886139
iteration 181, loss = 0.012135780416429043
iteration 182, loss = 0.01201572734862566
iteration 183, loss = 0.010736264288425446
iteration 184, loss = 0.012129389680922031
iteration 185, loss = 0.01116989180445671
iteration 186, loss = 0.011633285321295261
iteration 187, loss = 0.012027693912386894
iteration 188, loss = 0.010970576666295528
iteration 189, loss = 0.016453061252832413
iteration 190, loss = 0.01160777360200882
iteration 191, loss = 0.010741122998297215
iteration 192, loss = 0.011816727928817272
iteration 193, loss = 0.011417586356401443
iteration 194, loss = 0.012584943324327469
iteration 195, loss = 0.010699061676859856
iteration 196, loss = 0.011567316949367523
iteration 197, loss = 0.010639832355082035
iteration 198, loss = 0.010220716707408428
iteration 199, loss = 0.01641871966421604
iteration 200, loss = 0.011520260944962502
iteration 201, loss = 0.014364797621965408
iteration 202, loss = 0.012278949841856956
iteration 203, loss = 0.012800794094800949
iteration 204, loss = 0.01479264535009861
iteration 205, loss = 0.019887253642082214
iteration 206, loss = 0.01508452370762825
iteration 207, loss = 0.011551324278116226
iteration 208, loss = 0.014950745739042759
iteration 209, loss = 0.014794247224926949
iteration 210, loss = 0.011476840823888779
iteration 211, loss = 0.010941621847450733
iteration 212, loss = 0.01169564202427864
iteration 213, loss = 0.013486030511558056
iteration 214, loss = 0.010814453475177288
iteration 215, loss = 0.010777336545288563
iteration 216, loss = 0.011539308354258537
iteration 217, loss = 0.010863776318728924
iteration 218, loss = 0.015196489170193672
iteration 219, loss = 0.011349750682711601
iteration 220, loss = 0.019486119970679283
iteration 221, loss = 0.010563344694674015
iteration 222, loss = 0.011356447823345661
iteration 223, loss = 0.010989971458911896
iteration 224, loss = 0.011240786872804165
iteration 225, loss = 0.01119066122919321
iteration 226, loss = 0.010973011143505573
iteration 227, loss = 0.010744832456111908
iteration 228, loss = 0.010590278543531895
iteration 229, loss = 0.011133597232401371
iteration 230, loss = 0.01146646961569786
iteration 231, loss = 0.012407385744154453
iteration 232, loss = 0.013624497689306736
iteration 233, loss = 0.017283594235777855
iteration 234, loss = 0.011699175462126732
iteration 235, loss = 0.016013886779546738
iteration 236, loss = 0.014986047521233559
iteration 237, loss = 0.012422006577253342
iteration 238, loss = 0.010605422779917717
iteration 239, loss = 0.014341557398438454
iteration 240, loss = 0.012480620294809341
iteration 241, loss = 0.01564398780465126
iteration 242, loss = 0.01197944488376379
iteration 243, loss = 0.011953244917094707
iteration 244, loss = 0.010263258591294289
iteration 245, loss = 0.01089952141046524
iteration 246, loss = 0.014347830787301064
iteration 247, loss = 0.013808288611471653
iteration 248, loss = 0.01961510255932808
iteration 249, loss = 0.013170058839023113
iteration 250, loss = 0.013851159252226353
iteration 251, loss = 0.014022243209183216
iteration 252, loss = 0.01136036030948162
iteration 253, loss = 0.01389660220593214
iteration 254, loss = 0.01152037177234888
iteration 255, loss = 0.010321583598852158
iteration 256, loss = 0.013761837035417557
iteration 257, loss = 0.01194178219884634
iteration 258, loss = 0.018327804282307625
iteration 259, loss = 0.011618122458457947
iteration 260, loss = 0.012202347628772259
iteration 261, loss = 0.012160388752818108
iteration 262, loss = 0.011656266637146473
iteration 263, loss = 0.013535828329622746
iteration 264, loss = 0.010600646957755089
iteration 265, loss = 0.014446962624788284
iteration 266, loss = 0.010750658810138702
iteration 267, loss = 0.015940876677632332
iteration 268, loss = 0.010222803801298141
iteration 269, loss = 0.011340901255607605
iteration 270, loss = 0.01110535953193903
iteration 271, loss = 0.011658916249871254
iteration 272, loss = 0.011732226237654686
iteration 273, loss = 0.012462610378861427
iteration 274, loss = 0.011128386482596397
iteration 275, loss = 0.010825620964169502
iteration 276, loss = 0.012591705657541752
iteration 277, loss = 0.01115466933697462
iteration 278, loss = 0.011059224605560303
iteration 279, loss = 0.011612417176365852
iteration 280, loss = 0.015106664970517159
iteration 281, loss = 0.011410844512283802
iteration 282, loss = 0.011264174245297909
iteration 283, loss = 0.014861784875392914
iteration 284, loss = 0.010591940023005009
iteration 285, loss = 0.017245784401893616
iteration 286, loss = 0.012014549225568771
iteration 287, loss = 0.011005138978362083
iteration 288, loss = 0.01541637908667326
iteration 289, loss = 0.0148697504773736
iteration 290, loss = 0.012821794487535954
iteration 291, loss = 0.020224081352353096
iteration 292, loss = 0.012106440961360931
iteration 293, loss = 0.010914224199950695
iteration 294, loss = 0.011675171554088593
iteration 295, loss = 0.010671098716557026
iteration 296, loss = 0.011196124367415905
iteration 297, loss = 0.012557007372379303
iteration 298, loss = 0.011336459778249264
iteration 299, loss = 0.014509706757962704
iteration 300, loss = 0.010973194614052773
iteration 1, loss = 0.01086697168648243
iteration 2, loss = 0.011564895510673523
iteration 3, loss = 0.012745404615998268
iteration 4, loss = 0.010640829801559448
iteration 5, loss = 0.010876298882067204
iteration 6, loss = 0.016609951853752136
iteration 7, loss = 0.011316273361444473
iteration 8, loss = 0.012008451856672764
iteration 9, loss = 0.012546771205961704
iteration 10, loss = 0.012133319862186909
iteration 11, loss = 0.015615641139447689
iteration 12, loss = 0.01615966111421585
iteration 13, loss = 0.016801970079541206
iteration 14, loss = 0.01063460297882557
iteration 15, loss = 0.010616563260555267
iteration 16, loss = 0.015595993027091026
iteration 17, loss = 0.014566322788596153
iteration 18, loss = 0.014330687932670116
iteration 19, loss = 0.012048115022480488
iteration 20, loss = 0.011773215606808662
iteration 21, loss = 0.01339231338351965
iteration 22, loss = 0.015957830473780632
iteration 23, loss = 0.011186029762029648
iteration 24, loss = 0.01188360434025526
iteration 25, loss = 0.01776185818016529
iteration 26, loss = 0.010958411730825901
iteration 27, loss = 0.01073561329394579
iteration 28, loss = 0.010891744866967201
iteration 29, loss = 0.014067919924855232
iteration 30, loss = 0.014930490404367447
iteration 31, loss = 0.014579479582607746
iteration 32, loss = 0.013163096271455288
iteration 33, loss = 0.01416779961436987
iteration 34, loss = 0.013549696654081345
iteration 35, loss = 0.014276467263698578
iteration 36, loss = 0.014135794714093208
iteration 37, loss = 0.014146480709314346
iteration 38, loss = 0.015047622844576836
iteration 39, loss = 0.014616059139370918
iteration 40, loss = 0.010711787268519402
iteration 41, loss = 0.010773207060992718
iteration 42, loss = 0.014441701583564281
iteration 43, loss = 0.01016117911785841
iteration 44, loss = 0.014178392477333546
iteration 45, loss = 0.01051852572709322
iteration 46, loss = 0.012660929933190346
iteration 47, loss = 0.011299174278974533
iteration 48, loss = 0.015218055807054043
iteration 49, loss = 0.011691042222082615
iteration 50, loss = 0.010557061061263084
iteration 51, loss = 0.012148940935730934
iteration 52, loss = 0.011044847778975964
iteration 53, loss = 0.011768700554966927
iteration 54, loss = 0.010433388873934746
iteration 55, loss = 0.010409350506961346
iteration 56, loss = 0.010382793843746185
iteration 57, loss = 0.018488604575395584
iteration 58, loss = 0.010931507684290409
iteration 59, loss = 0.017634021118283272
iteration 60, loss = 0.011095636524260044
iteration 61, loss = 0.011858577840030193
iteration 62, loss = 0.017553627490997314
iteration 63, loss = 0.010595036670565605
iteration 64, loss = 0.014794180169701576
iteration 65, loss = 0.010692998766899109
iteration 66, loss = 0.01374516449868679
iteration 67, loss = 0.012189063243567944
iteration 68, loss = 0.011154568754136562
iteration 69, loss = 0.016624797135591507
iteration 70, loss = 0.011320015415549278
iteration 71, loss = 0.01477907970547676
iteration 72, loss = 0.012666127644479275
iteration 73, loss = 0.01618022657930851
iteration 74, loss = 0.012497666291892529
iteration 75, loss = 0.015346551313996315
iteration 76, loss = 0.014682057313621044
iteration 77, loss = 0.012252668850123882
iteration 78, loss = 0.011762609705328941
iteration 79, loss = 0.011349313892424107
iteration 80, loss = 0.011344347149133682
iteration 81, loss = 0.010589761659502983
iteration 82, loss = 0.011478944681584835
iteration 83, loss = 0.01472187228500843
iteration 84, loss = 0.015337271615862846
iteration 85, loss = 0.011477051302790642
iteration 86, loss = 0.014717154204845428
iteration 87, loss = 0.011984050273895264
iteration 88, loss = 0.014108207076787949
iteration 89, loss = 0.011651106178760529
iteration 90, loss = 0.0115281380712986
iteration 91, loss = 0.015734994783997536
iteration 92, loss = 0.011535748839378357
iteration 93, loss = 0.0111461840569973
iteration 94, loss = 0.014155876822769642
iteration 95, loss = 0.011000487953424454
iteration 96, loss = 0.010917700827121735
iteration 97, loss = 0.011561691761016846
iteration 98, loss = 0.016313403844833374
iteration 99, loss = 0.014515198767185211
iteration 100, loss = 0.017203567549586296
iteration 101, loss = 0.014094089157879353
iteration 102, loss = 0.014118834398686886
iteration 103, loss = 0.01190692838281393
iteration 104, loss = 0.012242226861417294
iteration 105, loss = 0.010477263480424881
iteration 106, loss = 0.011927318759262562
iteration 107, loss = 0.010726561769843102
iteration 108, loss = 0.013408220373094082
iteration 109, loss = 0.010886094532907009
iteration 110, loss = 0.011682042852044106
iteration 111, loss = 0.012687006033957005
iteration 112, loss = 0.01479300856590271
iteration 113, loss = 0.014088619500398636
iteration 114, loss = 0.010851350612938404
iteration 115, loss = 0.012935388833284378
iteration 116, loss = 0.011648297309875488
iteration 117, loss = 0.01100105606019497
iteration 118, loss = 0.011723698116838932
iteration 119, loss = 0.01158163696527481
iteration 120, loss = 0.010363265872001648
iteration 121, loss = 0.01647355780005455
iteration 122, loss = 0.011949128471314907
iteration 123, loss = 0.011132562533020973
iteration 124, loss = 0.010557493194937706
iteration 125, loss = 0.013539040461182594
iteration 126, loss = 0.017219649627804756
iteration 127, loss = 0.013854376040399075
iteration 128, loss = 0.011884402483701706
iteration 129, loss = 0.010972927324473858
iteration 130, loss = 0.010981099680066109
iteration 131, loss = 0.016560064628720284
iteration 132, loss = 0.011544391512870789
iteration 133, loss = 0.011426142416894436
iteration 134, loss = 0.010910719633102417
iteration 135, loss = 0.01110365055501461
iteration 136, loss = 0.014799694530665874
iteration 137, loss = 0.011140076443552971
iteration 138, loss = 0.010905091650784016
iteration 139, loss = 0.011886212043464184
iteration 140, loss = 0.012212495319545269
iteration 141, loss = 0.011337780393660069
iteration 142, loss = 0.015420593321323395
iteration 143, loss = 0.011832237243652344
iteration 144, loss = 0.011967914178967476
iteration 145, loss = 0.012135757133364677
iteration 146, loss = 0.011273505166172981
iteration 147, loss = 0.010738098993897438
iteration 148, loss = 0.01095782034099102
iteration 149, loss = 0.017281247302889824
iteration 150, loss = 0.011300626210868359
iteration 151, loss = 0.010675996541976929
iteration 152, loss = 0.014719736762344837
iteration 153, loss = 0.011387763544917107
iteration 154, loss = 0.013702953234314919
iteration 155, loss = 0.011776590719819069
iteration 156, loss = 0.013850665651261806
iteration 157, loss = 0.014203321188688278
iteration 158, loss = 0.01524095144122839
iteration 159, loss = 0.011019215919077396
iteration 160, loss = 0.012150147929787636
iteration 161, loss = 0.010700938291847706
iteration 162, loss = 0.011363648809492588
iteration 163, loss = 0.011309457011520863
iteration 164, loss = 0.016312774270772934
iteration 165, loss = 0.013359135016798973
iteration 166, loss = 0.014386987313628197
iteration 167, loss = 0.010515873320400715
iteration 168, loss = 0.01027768012136221
iteration 169, loss = 0.012629219330847263
iteration 170, loss = 0.011706068180501461
iteration 171, loss = 0.01097205001860857
iteration 172, loss = 0.01505507342517376
iteration 173, loss = 0.010560974478721619
iteration 174, loss = 0.010973727330565453
iteration 175, loss = 0.010869331657886505
iteration 176, loss = 0.012006058357656002
iteration 177, loss = 0.012476683594286442
iteration 178, loss = 0.011736362241208553
iteration 179, loss = 0.01045091450214386
iteration 180, loss = 0.011227821931242943
iteration 181, loss = 0.012164043262600899
iteration 182, loss = 0.010942644439637661
iteration 183, loss = 0.011665686964988708
iteration 184, loss = 0.011406952515244484
iteration 185, loss = 0.012824520468711853
iteration 186, loss = 0.010436316952109337
iteration 187, loss = 0.014670171774923801
iteration 188, loss = 0.0135487737134099
iteration 189, loss = 0.010589556768536568
iteration 190, loss = 0.022376103326678276
iteration 191, loss = 0.011327769607305527
iteration 192, loss = 0.016298942267894745
iteration 193, loss = 0.011519772931933403
iteration 194, loss = 0.011177961714565754
iteration 195, loss = 0.010425618849694729
iteration 196, loss = 0.011188719421625137
iteration 197, loss = 0.01944899931550026
iteration 198, loss = 0.011400924064218998
iteration 199, loss = 0.011918778531253338
iteration 200, loss = 0.011451243422925472
iteration 201, loss = 0.01253568846732378
iteration 202, loss = 0.01386609673500061
iteration 203, loss = 0.010511079803109169
iteration 204, loss = 0.014210401102900505
iteration 205, loss = 0.01420323271304369
iteration 206, loss = 0.01069223415106535
iteration 207, loss = 0.012237373739480972
iteration 208, loss = 0.010722719132900238
iteration 209, loss = 0.011804692447185516
iteration 210, loss = 0.013990995474159718
iteration 211, loss = 0.011521298438310623
iteration 212, loss = 0.019848579540848732
iteration 213, loss = 0.011504911817610264
iteration 214, loss = 0.014911073260009289
iteration 215, loss = 0.011099282652139664
iteration 216, loss = 0.015034984797239304
iteration 217, loss = 0.011805173009634018
iteration 218, loss = 0.011042633093893528
iteration 219, loss = 0.01412697322666645
iteration 220, loss = 0.020057734102010727
iteration 221, loss = 0.010542110539972782
iteration 222, loss = 0.015502703376114368
iteration 223, loss = 0.01433313637971878
iteration 224, loss = 0.010750497691333294
iteration 225, loss = 0.01873498596251011
iteration 226, loss = 0.011576718650758266
iteration 227, loss = 0.011859521269798279
iteration 228, loss = 0.014260590076446533
iteration 229, loss = 0.011538785882294178
iteration 230, loss = 0.015944255515933037
iteration 231, loss = 0.011477174237370491
iteration 232, loss = 0.010658703744411469
iteration 233, loss = 0.010638438165187836
iteration 234, loss = 0.011259309016168118
iteration 235, loss = 0.011264650151133537
iteration 236, loss = 0.01675310917198658
iteration 237, loss = 0.013595876283943653
iteration 238, loss = 0.010783645324409008
iteration 239, loss = 0.019984157755970955
iteration 240, loss = 0.01422834675759077
iteration 241, loss = 0.011821305379271507
iteration 242, loss = 0.01084132306277752
iteration 243, loss = 0.011464056558907032
iteration 244, loss = 0.012681378051638603
iteration 245, loss = 0.010735920630395412
iteration 246, loss = 0.01106354035437107
iteration 247, loss = 0.011871539056301117
iteration 248, loss = 0.011106139048933983
iteration 249, loss = 0.011359019204974174
iteration 250, loss = 0.015364130958914757
iteration 251, loss = 0.011024621315300465
iteration 252, loss = 0.01628871262073517
iteration 253, loss = 0.015347857028245926
iteration 254, loss = 0.01216847077012062
iteration 255, loss = 0.01190752349793911
iteration 256, loss = 0.011826506815850735
iteration 257, loss = 0.013847489841282368
iteration 258, loss = 0.011076816357672215
iteration 259, loss = 0.01798202469944954
iteration 260, loss = 0.015100328251719475
iteration 261, loss = 0.015545560047030449
iteration 262, loss = 0.01171938981860876
iteration 263, loss = 0.0139930360019207
iteration 264, loss = 0.010900108143687248
iteration 265, loss = 0.012605691328644753
iteration 266, loss = 0.011304576881229877
iteration 267, loss = 0.010547482408583164
iteration 268, loss = 0.011118694208562374
iteration 269, loss = 0.01423187181353569
iteration 270, loss = 0.01444840244948864
iteration 271, loss = 0.012082500383257866
iteration 272, loss = 0.010964701883494854
iteration 273, loss = 0.011131750419735909
iteration 274, loss = 0.012055100873112679
iteration 275, loss = 0.012811200693249702
iteration 276, loss = 0.011260456405580044
iteration 277, loss = 0.014894301071763039
iteration 278, loss = 0.011188844218850136
iteration 279, loss = 0.010639240965247154
iteration 280, loss = 0.015316632576286793
iteration 281, loss = 0.012301575392484665
iteration 282, loss = 0.010936979204416275
iteration 283, loss = 0.01185434591025114
iteration 284, loss = 0.01155583281069994
iteration 285, loss = 0.01607869379222393
iteration 286, loss = 0.012137237936258316
iteration 287, loss = 0.010723871178925037
iteration 288, loss = 0.011318644508719444
iteration 289, loss = 0.010737190023064613
iteration 290, loss = 0.011534901335835457
iteration 291, loss = 0.0115713682025671
iteration 292, loss = 0.011975057423114777
iteration 293, loss = 0.009898181073367596
iteration 294, loss = 0.01007399708032608
iteration 295, loss = 0.010435360483825207
iteration 296, loss = 0.01202438585460186
iteration 297, loss = 0.010460969060659409
iteration 298, loss = 0.011025051586329937
iteration 299, loss = 0.011933436617255211
iteration 300, loss = 0.010889946483075619
iteration 1, loss = 0.014210425317287445
iteration 2, loss = 0.011141061782836914
iteration 3, loss = 0.014555706642568111
iteration 4, loss = 0.011819309554994106
iteration 5, loss = 0.011015228927135468
iteration 6, loss = 0.014377757906913757
iteration 7, loss = 0.015063658356666565
iteration 8, loss = 0.01090348232537508
iteration 9, loss = 0.011873704381287098
iteration 10, loss = 0.012216990813612938
iteration 11, loss = 0.01044973824173212
iteration 12, loss = 0.011770718730986118
iteration 13, loss = 0.012581964954733849
iteration 14, loss = 0.014023560099303722
iteration 15, loss = 0.011569585651159286
iteration 16, loss = 0.0117953484877944
iteration 17, loss = 0.015336689539253712
iteration 18, loss = 0.010770857334136963
iteration 19, loss = 0.01270636823028326
iteration 20, loss = 0.011423703283071518
iteration 21, loss = 0.011078155599534512
iteration 22, loss = 0.011625280603766441
iteration 23, loss = 0.011104369536042213
iteration 24, loss = 0.017656235024333
iteration 25, loss = 0.012082992121577263
iteration 26, loss = 0.012080174870789051
iteration 27, loss = 0.011099176481366158
iteration 28, loss = 0.010229994542896748
iteration 29, loss = 0.011321666650474072
iteration 30, loss = 0.010854198597371578
iteration 31, loss = 0.011195456609129906
iteration 32, loss = 0.010653545148670673
iteration 33, loss = 0.013500667177140713
iteration 34, loss = 0.012185388244688511
iteration 35, loss = 0.010538188740611076
iteration 36, loss = 0.01107378676533699
iteration 37, loss = 0.012717615813016891
iteration 38, loss = 0.010545062832534313
iteration 39, loss = 0.013497772626578808
iteration 40, loss = 0.010922671295702457
iteration 41, loss = 0.01131301000714302
iteration 42, loss = 0.015850329771637917
iteration 43, loss = 0.014542895369231701
iteration 44, loss = 0.015741439536213875
iteration 45, loss = 0.014863189309835434
iteration 46, loss = 0.021706795319914818
iteration 47, loss = 0.014598168432712555
iteration 48, loss = 0.01280849240720272
iteration 49, loss = 0.010505041107535362
iteration 50, loss = 0.018023252487182617
iteration 51, loss = 0.012011685408651829
iteration 52, loss = 0.010015909560024738
iteration 53, loss = 0.01410145778208971
iteration 54, loss = 0.010998616926372051
iteration 55, loss = 0.013092047534883022
iteration 56, loss = 0.011343964375555515
iteration 57, loss = 0.010742830112576485
iteration 58, loss = 0.01136124599725008
iteration 59, loss = 0.014518216252326965
iteration 60, loss = 0.01317378506064415
iteration 61, loss = 0.012690270319581032
iteration 62, loss = 0.010378262959420681
iteration 63, loss = 0.011509658768773079
iteration 64, loss = 0.014665702357888222
iteration 65, loss = 0.014812636189162731
iteration 66, loss = 0.012327934615314007
iteration 67, loss = 0.015076396986842155
iteration 68, loss = 0.011113145388662815
iteration 69, loss = 0.01267218217253685
iteration 70, loss = 0.01120302826166153
iteration 71, loss = 0.013967492617666721
iteration 72, loss = 0.010511225089430809
iteration 73, loss = 0.01087699830532074
iteration 74, loss = 0.01046030130237341
iteration 75, loss = 0.010697001591324806
iteration 76, loss = 0.010903913527727127
iteration 77, loss = 0.01531214639544487
iteration 78, loss = 0.01307000033557415
iteration 79, loss = 0.011877747252583504
iteration 80, loss = 0.010686350055038929
iteration 81, loss = 0.014341216534376144
iteration 82, loss = 0.01129741221666336
iteration 83, loss = 0.014803990721702576
iteration 84, loss = 0.01318423543125391
iteration 85, loss = 0.014055601321160793
iteration 86, loss = 0.011335962451994419
iteration 87, loss = 0.011923586949706078
iteration 88, loss = 0.010331550613045692
iteration 89, loss = 0.01667097955942154
iteration 90, loss = 0.010926134884357452
iteration 91, loss = 0.01096369232982397
iteration 92, loss = 0.022101018577814102
iteration 93, loss = 0.01925339363515377
iteration 94, loss = 0.01299387775361538
iteration 95, loss = 0.012232525274157524
iteration 96, loss = 0.014778049662709236
iteration 97, loss = 0.015082921832799911
iteration 98, loss = 0.012772291898727417
iteration 99, loss = 0.010696396231651306
iteration 100, loss = 0.011114651337265968
iteration 101, loss = 0.011652398854494095
iteration 102, loss = 0.01128547266125679
iteration 103, loss = 0.01119969505816698
iteration 104, loss = 0.01125436183065176
iteration 105, loss = 0.0120221097022295
iteration 106, loss = 0.013828408904373646
iteration 107, loss = 0.011379051022231579
iteration 108, loss = 0.01101880706846714
iteration 109, loss = 0.010934744030237198
iteration 110, loss = 0.01094677485525608
iteration 111, loss = 0.011389035731554031
iteration 112, loss = 0.010576462373137474
iteration 113, loss = 0.011552422307431698
iteration 114, loss = 0.011691022664308548
iteration 115, loss = 0.011257410980761051
iteration 116, loss = 0.014781111851334572
iteration 117, loss = 0.01394349243491888
iteration 118, loss = 0.011019136756658554
iteration 119, loss = 0.015686141327023506
iteration 120, loss = 0.011744740419089794
iteration 121, loss = 0.012686660513281822
iteration 122, loss = 0.010472049005329609
iteration 123, loss = 0.011129867285490036
iteration 124, loss = 0.011457142420113087
iteration 125, loss = 0.011512655764818192
iteration 126, loss = 0.01489088125526905
iteration 127, loss = 0.0165825467556715
iteration 128, loss = 0.016821108758449554
iteration 129, loss = 0.013221653178334236
iteration 130, loss = 0.011132399551570415
iteration 131, loss = 0.014707903377711773
iteration 132, loss = 0.012448017485439777
iteration 133, loss = 0.011577542871236801
iteration 134, loss = 0.013728303834795952
iteration 135, loss = 0.012291120365262032
iteration 136, loss = 0.012830452062189579
iteration 137, loss = 0.012851848267018795
iteration 138, loss = 0.011714482679963112
iteration 139, loss = 0.011222627945244312
iteration 140, loss = 0.013754925690591335
iteration 141, loss = 0.01136014610528946
iteration 142, loss = 0.015610963106155396
iteration 143, loss = 0.010363087058067322
iteration 144, loss = 0.011320495046675205
iteration 145, loss = 0.016643354669213295
iteration 146, loss = 0.011119164526462555
iteration 147, loss = 0.013165057636797428
iteration 148, loss = 0.011787834577262402
iteration 149, loss = 0.016154835000634193
iteration 150, loss = 0.01625790260732174
iteration 151, loss = 0.011025737971067429
iteration 152, loss = 0.013754473067820072
iteration 153, loss = 0.012422522529959679
iteration 154, loss = 0.011584663763642311
iteration 155, loss = 0.012162132188677788
iteration 156, loss = 0.01551309134811163
iteration 157, loss = 0.016526009887456894
iteration 158, loss = 0.010480263270437717
iteration 159, loss = 0.0157794039696455
iteration 160, loss = 0.011893747374415398
iteration 161, loss = 0.01120164804160595
iteration 162, loss = 0.011685715988278389
iteration 163, loss = 0.011367636732757092
iteration 164, loss = 0.011021952144801617
iteration 165, loss = 0.015237975865602493
iteration 166, loss = 0.011221632361412048
iteration 167, loss = 0.011183816008269787
iteration 168, loss = 0.010841457173228264
iteration 169, loss = 0.010931403376162052
iteration 170, loss = 0.014980022795498371
iteration 171, loss = 0.010837324894964695
iteration 172, loss = 0.012062359601259232
iteration 173, loss = 0.010757830925285816
iteration 174, loss = 0.014935837127268314
iteration 175, loss = 0.010433364659547806
iteration 176, loss = 0.011107703670859337
iteration 177, loss = 0.015140993520617485
iteration 178, loss = 0.013418643735349178
iteration 179, loss = 0.01220700517296791
iteration 180, loss = 0.012756812386214733
iteration 181, loss = 0.014285015873610973
iteration 182, loss = 0.010901708155870438
iteration 183, loss = 0.011362387798726559
iteration 184, loss = 0.015442084521055222
iteration 185, loss = 0.016067465767264366
iteration 186, loss = 0.013721148483455181
iteration 187, loss = 0.01185531634837389
iteration 188, loss = 0.013543223030865192
iteration 189, loss = 0.013701427727937698
iteration 190, loss = 0.01146967988461256
iteration 191, loss = 0.014400798827409744
iteration 192, loss = 0.013051943853497505
iteration 193, loss = 0.01177656278014183
iteration 194, loss = 0.012569497339427471
iteration 195, loss = 0.010478153824806213
iteration 196, loss = 0.01102101057767868
iteration 197, loss = 0.0109714949503541
iteration 198, loss = 0.011924389749765396
iteration 199, loss = 0.015525664202868938
iteration 200, loss = 0.012549169361591339
iteration 201, loss = 0.015386550687253475
iteration 202, loss = 0.010791548527777195
iteration 203, loss = 0.01137181930243969
iteration 204, loss = 0.012171858921647072
iteration 205, loss = 0.014327139593660831
iteration 206, loss = 0.011235308833420277
iteration 207, loss = 0.017133060842752457
iteration 208, loss = 0.014545860700309277
iteration 209, loss = 0.013092607259750366
iteration 210, loss = 0.011678851209580898
iteration 211, loss = 0.011183103546500206
iteration 212, loss = 0.010901132598519325
iteration 213, loss = 0.012480261735618114
iteration 214, loss = 0.012153098359704018
iteration 215, loss = 0.011549269780516624
iteration 216, loss = 0.01067714300006628
iteration 217, loss = 0.012100804597139359
iteration 218, loss = 0.014162243343889713
iteration 219, loss = 0.014955096878111362
iteration 220, loss = 0.010920338332653046
iteration 221, loss = 0.011021948419511318
iteration 222, loss = 0.010977796278893948
iteration 223, loss = 0.011019849218428135
iteration 224, loss = 0.011062931269407272
iteration 225, loss = 0.016735222190618515
iteration 226, loss = 0.014972813427448273
iteration 227, loss = 0.016814986243844032
iteration 228, loss = 0.012317664921283722
iteration 229, loss = 0.010942330583930016
iteration 230, loss = 0.010459025390446186
iteration 231, loss = 0.0113045210018754
iteration 232, loss = 0.010488520376384258
iteration 233, loss = 0.011311217211186886
iteration 234, loss = 0.01101134717464447
iteration 235, loss = 0.011112101376056671
iteration 236, loss = 0.011343824677169323
iteration 237, loss = 0.012642120942473412
iteration 238, loss = 0.01521321665495634
iteration 239, loss = 0.0114133907482028
iteration 240, loss = 0.015265118330717087
iteration 241, loss = 0.011397712863981724
iteration 242, loss = 0.010448076762259007
iteration 243, loss = 0.011539069004356861
iteration 244, loss = 0.015968987718224525
iteration 245, loss = 0.010617001913487911
iteration 246, loss = 0.01592267118394375
iteration 247, loss = 0.01608692854642868
iteration 248, loss = 0.012175961397588253
iteration 249, loss = 0.010503716766834259
iteration 250, loss = 0.016899408772587776
iteration 251, loss = 0.015240907669067383
iteration 252, loss = 0.011271866038441658
iteration 253, loss = 0.0156509168446064
iteration 254, loss = 0.011954599991440773
iteration 255, loss = 0.011899787001311779
iteration 256, loss = 0.011643254198133945
iteration 257, loss = 0.012062075547873974
iteration 258, loss = 0.014860603027045727
iteration 259, loss = 0.010536586865782738
iteration 260, loss = 0.012265367433428764
iteration 261, loss = 0.011327192187309265
iteration 262, loss = 0.011221387423574924
iteration 263, loss = 0.011414832435548306
iteration 264, loss = 0.013245838694274426
iteration 265, loss = 0.01654096134006977
iteration 266, loss = 0.011528429575264454
iteration 267, loss = 0.014490067958831787
iteration 268, loss = 0.019317330792546272
iteration 269, loss = 0.013257432729005814
iteration 270, loss = 0.011089717969298363
iteration 271, loss = 0.01060355082154274
iteration 272, loss = 0.014716804027557373
iteration 273, loss = 0.011505196802318096
iteration 274, loss = 0.010801614262163639
iteration 275, loss = 0.010611793026328087
iteration 276, loss = 0.010818010196089745
iteration 277, loss = 0.01096443459391594
iteration 278, loss = 0.019241705536842346
iteration 279, loss = 0.010786243714392185
iteration 280, loss = 0.01133275032043457
iteration 281, loss = 0.013006657361984253
iteration 282, loss = 0.011839219368994236
iteration 283, loss = 0.011377522721886635
iteration 284, loss = 0.012092198245227337
iteration 285, loss = 0.01165425032377243
iteration 286, loss = 0.01024108100682497
iteration 287, loss = 0.01060621626675129
iteration 288, loss = 0.014368094503879547
iteration 289, loss = 0.010591425001621246
iteration 290, loss = 0.011702586896717548
iteration 291, loss = 0.013052307069301605
iteration 292, loss = 0.010874341242015362
iteration 293, loss = 0.011565939523279667
iteration 294, loss = 0.014087475836277008
iteration 295, loss = 0.014106226153671741
iteration 296, loss = 0.014254258014261723
iteration 297, loss = 0.015167474746704102
iteration 298, loss = 0.012173650786280632
iteration 299, loss = 0.013715095818042755
iteration 300, loss = 0.011932311579585075
iteration 1, loss = 0.01049950160086155
iteration 2, loss = 0.012166006490588188
iteration 3, loss = 0.012427296489477158
iteration 4, loss = 0.012817783281207085
iteration 5, loss = 0.012396757490932941
iteration 6, loss = 0.017541399225592613
iteration 7, loss = 0.01582951284945011
iteration 8, loss = 0.01376009825617075
iteration 9, loss = 0.014187436550855637
iteration 10, loss = 0.01000700518488884
iteration 11, loss = 0.015985995531082153
iteration 12, loss = 0.010830113664269447
iteration 13, loss = 0.01057777926325798
iteration 14, loss = 0.0155144352465868
iteration 15, loss = 0.010467018000781536
iteration 16, loss = 0.014827129431068897
iteration 17, loss = 0.013706261292099953
iteration 18, loss = 0.011387371458113194
iteration 19, loss = 0.01069266814738512
iteration 20, loss = 0.012045301496982574
iteration 21, loss = 0.012145099230110645
iteration 22, loss = 0.015533052384853363
iteration 23, loss = 0.01424537505954504
iteration 24, loss = 0.011646886356174946
iteration 25, loss = 0.010802214965224266
iteration 26, loss = 0.014750980772078037
iteration 27, loss = 0.011107159778475761
iteration 28, loss = 0.011391619220376015
iteration 29, loss = 0.010719934478402138
iteration 30, loss = 0.011355249211192131
iteration 31, loss = 0.011070359498262405
iteration 32, loss = 0.013884956017136574
iteration 33, loss = 0.012034778483211994
iteration 34, loss = 0.013642949983477592
iteration 35, loss = 0.015434681437909603
iteration 36, loss = 0.01288658007979393
iteration 37, loss = 0.013833086006343365
iteration 38, loss = 0.011090720072388649
iteration 39, loss = 0.013842963613569736
iteration 40, loss = 0.016933752223849297
iteration 41, loss = 0.011025249026715755
iteration 42, loss = 0.012406961061060429
iteration 43, loss = 0.015830624848604202
iteration 44, loss = 0.01242033764719963
iteration 45, loss = 0.013126667588949203
iteration 46, loss = 0.011187156662344933
iteration 47, loss = 0.011444300413131714
iteration 48, loss = 0.014861679635941982
iteration 49, loss = 0.01127199549227953
iteration 50, loss = 0.01136946864426136
iteration 51, loss = 0.01458258181810379
iteration 52, loss = 0.010980907827615738
iteration 53, loss = 0.011329998262226582
iteration 54, loss = 0.009990917518734932
iteration 55, loss = 0.012525157071650028
iteration 56, loss = 0.01868388243019581
iteration 57, loss = 0.011450826190412045
iteration 58, loss = 0.011159737594425678
iteration 59, loss = 0.010074570775032043
iteration 60, loss = 0.017764179036021233
iteration 61, loss = 0.011205180548131466
iteration 62, loss = 0.010349188931286335
iteration 63, loss = 0.012089580297470093
iteration 64, loss = 0.011347617022693157
iteration 65, loss = 0.015637431293725967
iteration 66, loss = 0.010463760234415531
iteration 67, loss = 0.011867434717714787
iteration 68, loss = 0.012339770793914795
iteration 69, loss = 0.010744680650532246
iteration 70, loss = 0.011387019418179989
iteration 71, loss = 0.010743839666247368
iteration 72, loss = 0.01202966645359993
iteration 73, loss = 0.011224436573684216
iteration 74, loss = 0.015451462008059025
iteration 75, loss = 0.010148894973099232
iteration 76, loss = 0.011798659339547157
iteration 77, loss = 0.011574018746614456
iteration 78, loss = 0.015388337895274162
iteration 79, loss = 0.01361875981092453
iteration 80, loss = 0.010960436426103115
iteration 81, loss = 0.011830073781311512
iteration 82, loss = 0.010506375692784786
iteration 83, loss = 0.011654108762741089
iteration 84, loss = 0.010355701670050621
iteration 85, loss = 0.012728900648653507
iteration 86, loss = 0.01196688786149025
iteration 87, loss = 0.011687886901199818
iteration 88, loss = 0.012775544077157974
iteration 89, loss = 0.017027346417307854
iteration 90, loss = 0.012040738947689533
iteration 91, loss = 0.014117100276052952
iteration 92, loss = 0.011812995187938213
iteration 93, loss = 0.011113801039755344
iteration 94, loss = 0.014308406971395016
iteration 95, loss = 0.012501534074544907
iteration 96, loss = 0.01081771682947874
iteration 97, loss = 0.011284221895039082
iteration 98, loss = 0.014008928090333939
iteration 99, loss = 0.010727357119321823
iteration 100, loss = 0.011916046030819416
iteration 101, loss = 0.014572899788618088
iteration 102, loss = 0.011116762645542622
iteration 103, loss = 0.010833154432475567
iteration 104, loss = 0.013159342110157013
iteration 105, loss = 0.012405183166265488
iteration 106, loss = 0.014785125851631165
iteration 107, loss = 0.011282321065664291
iteration 108, loss = 0.011016362346708775
iteration 109, loss = 0.011019785888493061
iteration 110, loss = 0.010670261457562447
iteration 111, loss = 0.01135549508035183
iteration 112, loss = 0.010288787074387074
iteration 113, loss = 0.015136020258069038
iteration 114, loss = 0.01766284927725792
iteration 115, loss = 0.014264867641031742
iteration 116, loss = 0.011841367930173874
iteration 117, loss = 0.01254685502499342
iteration 118, loss = 0.010343675501644611
iteration 119, loss = 0.018609071150422096
iteration 120, loss = 0.011124479584395885
iteration 121, loss = 0.016064466908574104
iteration 122, loss = 0.01780725084245205
iteration 123, loss = 0.013168293982744217
iteration 124, loss = 0.015403280034661293
iteration 125, loss = 0.01406249962747097
iteration 126, loss = 0.012465424835681915
iteration 127, loss = 0.016982389613986015
iteration 128, loss = 0.014120428822934628
iteration 129, loss = 0.010964397341012955
iteration 130, loss = 0.010810849256813526
iteration 131, loss = 0.011303754523396492
iteration 132, loss = 0.012442843057215214
iteration 133, loss = 0.012004666961729527
iteration 134, loss = 0.015053334645926952
iteration 135, loss = 0.011845679022371769
iteration 136, loss = 0.010795628651976585
iteration 137, loss = 0.011436638422310352
iteration 138, loss = 0.016475412994623184
iteration 139, loss = 0.011893698014318943
iteration 140, loss = 0.010961737483739853
iteration 141, loss = 0.0142216170206666
iteration 142, loss = 0.011289337649941444
iteration 143, loss = 0.010389161296188831
iteration 144, loss = 0.0173525121062994
iteration 145, loss = 0.010999606922268867
iteration 146, loss = 0.011945330537855625
iteration 147, loss = 0.010457712225615978
iteration 148, loss = 0.015120665542781353
iteration 149, loss = 0.011856555938720703
iteration 150, loss = 0.015134270302951336
iteration 151, loss = 0.010592848993837833
iteration 152, loss = 0.014825188554823399
iteration 153, loss = 0.010653744451701641
iteration 154, loss = 0.01060433965176344
iteration 155, loss = 0.013818494975566864
iteration 156, loss = 0.012438677251338959
iteration 157, loss = 0.014823672361671925
iteration 158, loss = 0.012383813969790936
iteration 159, loss = 0.012175256386399269
iteration 160, loss = 0.014848074875772
iteration 161, loss = 0.013973510824143887
iteration 162, loss = 0.014527615159749985
iteration 163, loss = 0.010615349747240543
iteration 164, loss = 0.014036319218575954
iteration 165, loss = 0.011183164082467556
iteration 166, loss = 0.012982056476175785
iteration 167, loss = 0.015309317968785763
iteration 168, loss = 0.011719386093318462
iteration 169, loss = 0.010703741572797298
iteration 170, loss = 0.013350349850952625
iteration 171, loss = 0.01059487834572792
iteration 172, loss = 0.014256617985665798
iteration 173, loss = 0.010086480528116226
iteration 174, loss = 0.011598506942391396
iteration 175, loss = 0.014029938727617264
iteration 176, loss = 0.013683128170669079
iteration 177, loss = 0.011157558299601078
iteration 178, loss = 0.010946996510028839
iteration 179, loss = 0.0119190514087677
iteration 180, loss = 0.011070964857935905
iteration 181, loss = 0.01087723858654499
iteration 182, loss = 0.01297583058476448
iteration 183, loss = 0.011724106967449188
iteration 184, loss = 0.010775339789688587
iteration 185, loss = 0.012737289071083069
iteration 186, loss = 0.014161838218569756
iteration 187, loss = 0.014367221854627132
iteration 188, loss = 0.017459116876125336
iteration 189, loss = 0.015086233615875244
iteration 190, loss = 0.011842855252325535
iteration 191, loss = 0.011031432077288628
iteration 192, loss = 0.010976557619869709
iteration 193, loss = 0.014198416844010353
iteration 194, loss = 0.011667625978589058
iteration 195, loss = 0.01119337510317564
iteration 196, loss = 0.011813336983323097
iteration 197, loss = 0.011721114628016949
iteration 198, loss = 0.011505287140607834
iteration 199, loss = 0.010688340291380882
iteration 200, loss = 0.011304046027362347
iteration 201, loss = 0.012101296335458755
iteration 202, loss = 0.013061686418950558
iteration 203, loss = 0.012422136031091213
iteration 204, loss = 0.010825738310813904
iteration 205, loss = 0.014328774064779282
iteration 206, loss = 0.011716638691723347
iteration 207, loss = 0.011188069358468056
iteration 208, loss = 0.010572593659162521
iteration 209, loss = 0.010627662762999535
iteration 210, loss = 0.014886743389070034
iteration 211, loss = 0.017419295385479927
iteration 212, loss = 0.01136809028685093
iteration 213, loss = 0.014675339683890343
iteration 214, loss = 0.0118284672498703
iteration 215, loss = 0.01139170303940773
iteration 216, loss = 0.011770810931921005
iteration 217, loss = 0.013168247416615486
iteration 218, loss = 0.010559894144535065
iteration 219, loss = 0.014317318797111511
iteration 220, loss = 0.015234164893627167
iteration 221, loss = 0.010748741216957569
iteration 222, loss = 0.014121390879154205
iteration 223, loss = 0.01735518127679825
iteration 224, loss = 0.012178911827504635
iteration 225, loss = 0.013270111754536629
iteration 226, loss = 0.011583724990487099
iteration 227, loss = 0.011447850614786148
iteration 228, loss = 0.011713853105902672
iteration 229, loss = 0.01142493262887001
iteration 230, loss = 0.013908319175243378
iteration 231, loss = 0.01241381373256445
iteration 232, loss = 0.010976775549352169
iteration 233, loss = 0.011175083927810192
iteration 234, loss = 0.010667692869901657
iteration 235, loss = 0.012546433135867119
iteration 236, loss = 0.011364376172423363
iteration 237, loss = 0.01638181321322918
iteration 238, loss = 0.011286192573606968
iteration 239, loss = 0.010507074184715748
iteration 240, loss = 0.010585017502307892
iteration 241, loss = 0.012674112804234028
iteration 242, loss = 0.012043492868542671
iteration 243, loss = 0.013257874175906181
iteration 244, loss = 0.011229348368942738
iteration 245, loss = 0.011805483140051365
iteration 246, loss = 0.013378575444221497
iteration 247, loss = 0.01245157141238451
iteration 248, loss = 0.014677203260362148
iteration 249, loss = 0.01290319487452507
iteration 250, loss = 0.01115124486386776
iteration 251, loss = 0.011685711331665516
iteration 252, loss = 0.011084603145718575
iteration 253, loss = 0.011180106550455093
iteration 254, loss = 0.010283353738486767
iteration 255, loss = 0.011052410118281841
iteration 256, loss = 0.0110622588545084
iteration 257, loss = 0.015786942094564438
iteration 258, loss = 0.01154482550919056
iteration 259, loss = 0.011535897850990295
iteration 260, loss = 0.013070867396891117
iteration 261, loss = 0.011328271590173244
iteration 262, loss = 0.011815933510661125
iteration 263, loss = 0.010352451354265213
iteration 264, loss = 0.0133375134319067
iteration 265, loss = 0.013161108829081059
iteration 266, loss = 0.013999625109136105
iteration 267, loss = 0.010866377502679825
iteration 268, loss = 0.010831909254193306
iteration 269, loss = 0.011485877446830273
iteration 270, loss = 0.014816383831202984
iteration 271, loss = 0.010943415574729443
iteration 272, loss = 0.011618029326200485
iteration 273, loss = 0.015552850440144539
iteration 274, loss = 0.014304275624454021
iteration 275, loss = 0.013554862700402737
iteration 276, loss = 0.013413036242127419
iteration 277, loss = 0.014197497628629208
iteration 278, loss = 0.011028616689145565
iteration 279, loss = 0.011590023525059223
iteration 280, loss = 0.01214261818677187
iteration 281, loss = 0.010997248813509941
iteration 282, loss = 0.01296607032418251
iteration 283, loss = 0.020570598542690277
iteration 284, loss = 0.011888593435287476
iteration 285, loss = 0.011171543970704079
iteration 286, loss = 0.01106744073331356
iteration 287, loss = 0.013864613138139248
iteration 288, loss = 0.01148604042828083
iteration 289, loss = 0.014686827547848225
iteration 290, loss = 0.01400782261043787
iteration 291, loss = 0.011287681758403778
iteration 292, loss = 0.011383977718651295
iteration 293, loss = 0.011525015346705914
iteration 294, loss = 0.014805447310209274
iteration 295, loss = 0.01706087961792946
iteration 296, loss = 0.011475722305476665
iteration 297, loss = 0.010883326642215252
iteration 298, loss = 0.010842859745025635
iteration 299, loss = 0.013223414309322834
iteration 300, loss = 0.018815532326698303
iteration 1, loss = 0.01597815379500389
iteration 2, loss = 0.014541475102305412
iteration 3, loss = 0.011565746739506721
iteration 4, loss = 0.011407779529690742
iteration 5, loss = 0.010784539394080639
iteration 6, loss = 0.010654089972376823
iteration 7, loss = 0.011148951016366482
iteration 8, loss = 0.011198887601494789
iteration 9, loss = 0.010997331701219082
iteration 10, loss = 0.01071982178837061
iteration 11, loss = 0.023998215794563293
iteration 12, loss = 0.012146556749939919
iteration 13, loss = 0.010822582989931107
iteration 14, loss = 0.014591006562113762
iteration 15, loss = 0.012273089960217476
iteration 16, loss = 0.01401489693671465
iteration 17, loss = 0.014031698927283287
iteration 18, loss = 0.015111023560166359
iteration 19, loss = 0.010464475490152836
iteration 20, loss = 0.010702403262257576
iteration 21, loss = 0.011640737764537334
iteration 22, loss = 0.015518122352659702
iteration 23, loss = 0.010390831157565117
iteration 24, loss = 0.013779256492853165
iteration 25, loss = 0.010568179190158844
iteration 26, loss = 0.015293754637241364
iteration 27, loss = 0.012961084023118019
iteration 28, loss = 0.012387256138026714
iteration 29, loss = 0.014037039130926132
iteration 30, loss = 0.011509195901453495
iteration 31, loss = 0.011506462469696999
iteration 32, loss = 0.010444664396345615
iteration 33, loss = 0.01158542837947607
iteration 34, loss = 0.01243742648512125
iteration 35, loss = 0.01047665998339653
iteration 36, loss = 0.012334548868238926
iteration 37, loss = 0.011158999055624008
iteration 38, loss = 0.012574627995491028
iteration 39, loss = 0.010679101571440697
iteration 40, loss = 0.02456146478652954
iteration 41, loss = 0.010210072621703148
iteration 42, loss = 0.010715167038142681
iteration 43, loss = 0.01172840129584074
iteration 44, loss = 0.010658875107765198
iteration 45, loss = 0.013651564717292786
iteration 46, loss = 0.01219234336167574
iteration 47, loss = 0.01333936583250761
iteration 48, loss = 0.010511902160942554
iteration 49, loss = 0.01482230331748724
iteration 50, loss = 0.010892297141253948
iteration 51, loss = 0.015290851704776287
iteration 52, loss = 0.017895841971039772
iteration 53, loss = 0.010701044462621212
iteration 54, loss = 0.011305338703095913
iteration 55, loss = 0.014468084089457989
iteration 56, loss = 0.011573851108551025
iteration 57, loss = 0.012049316428601742
iteration 58, loss = 0.010336716659367085
iteration 59, loss = 0.010606932453811169
iteration 60, loss = 0.010784091427922249
iteration 61, loss = 0.016840333119034767
iteration 62, loss = 0.010577899403870106
iteration 63, loss = 0.01482546515762806
iteration 64, loss = 0.015027315355837345
iteration 65, loss = 0.013380064629018307
iteration 66, loss = 0.011082050390541553
iteration 67, loss = 0.01337083987891674
iteration 68, loss = 0.010439876466989517
iteration 69, loss = 0.010105150751769543
iteration 70, loss = 0.01113179326057434
iteration 71, loss = 0.011290901340544224
iteration 72, loss = 0.01889961212873459
iteration 73, loss = 0.011213107034564018
iteration 74, loss = 0.015638621523976326
iteration 75, loss = 0.01202765665948391
iteration 76, loss = 0.011689969338476658
iteration 77, loss = 0.01084806863218546
iteration 78, loss = 0.012857259251177311
iteration 79, loss = 0.011056940071284771
iteration 80, loss = 0.01063812430948019
iteration 81, loss = 0.01165432296693325
iteration 82, loss = 0.011182448826730251
iteration 83, loss = 0.013040687888860703
iteration 84, loss = 0.014048370532691479
iteration 85, loss = 0.011297146789729595
iteration 86, loss = 0.011003696359694004
iteration 87, loss = 0.009838142432272434
iteration 88, loss = 0.0102161830291152
iteration 89, loss = 0.012149730697274208
iteration 90, loss = 0.013204650953412056
iteration 91, loss = 0.011273970827460289
iteration 92, loss = 0.011359293013811111
iteration 93, loss = 0.012447966262698174
iteration 94, loss = 0.011196629144251347
iteration 95, loss = 0.018012307584285736
iteration 96, loss = 0.01231126394122839
iteration 97, loss = 0.01144985482096672
iteration 98, loss = 0.010822958312928677
iteration 99, loss = 0.0109441252425313
iteration 100, loss = 0.013741682283580303
iteration 101, loss = 0.014768492430448532
iteration 102, loss = 0.014862162061035633
iteration 103, loss = 0.014220298267900944
iteration 104, loss = 0.012105884030461311
iteration 105, loss = 0.010726319625973701
iteration 106, loss = 0.010341868735849857
iteration 107, loss = 0.010343439877033234
iteration 108, loss = 0.013193026185035706
iteration 109, loss = 0.01147377397865057
iteration 110, loss = 0.011484787799417973
iteration 111, loss = 0.014572259038686752
iteration 112, loss = 0.016280019655823708
iteration 113, loss = 0.011015398427844048
iteration 114, loss = 0.011657726019620895
iteration 115, loss = 0.012195700779557228
iteration 116, loss = 0.011384152807295322
iteration 117, loss = 0.011437485925853252
iteration 118, loss = 0.011708639562129974
iteration 119, loss = 0.010713659226894379
iteration 120, loss = 0.01418936438858509
iteration 121, loss = 0.012882736511528492
iteration 122, loss = 0.011919021606445312
iteration 123, loss = 0.010509496554732323
iteration 124, loss = 0.013844383880496025
iteration 125, loss = 0.014781900681555271
iteration 126, loss = 0.014545215293765068
iteration 127, loss = 0.018494373187422752
iteration 128, loss = 0.010228930972516537
iteration 129, loss = 0.01025453582406044
iteration 130, loss = 0.013410169631242752
iteration 131, loss = 0.010704971849918365
iteration 132, loss = 0.010718240402638912
iteration 133, loss = 0.012164702638983727
iteration 134, loss = 0.011112391017377377
iteration 135, loss = 0.01180566567927599
iteration 136, loss = 0.015047791413962841
iteration 137, loss = 0.011070065200328827
iteration 138, loss = 0.010811354964971542
iteration 139, loss = 0.013401503674685955
iteration 140, loss = 0.013671900145709515
iteration 141, loss = 0.011657696217298508
iteration 142, loss = 0.015198986046016216
iteration 143, loss = 0.014386139810085297
iteration 144, loss = 0.012030365876853466
iteration 145, loss = 0.010338237509131432
iteration 146, loss = 0.011830609291791916
iteration 147, loss = 0.011278127320110798
iteration 148, loss = 0.011358711868524551
iteration 149, loss = 0.012111072428524494
iteration 150, loss = 0.012120876461267471
iteration 151, loss = 0.011389605700969696
iteration 152, loss = 0.011854512616991997
iteration 153, loss = 0.014204606413841248
iteration 154, loss = 0.013717190362513065
iteration 155, loss = 0.011172927916049957
iteration 156, loss = 0.014344092458486557
iteration 157, loss = 0.012427039444446564
iteration 158, loss = 0.010507994331419468
iteration 159, loss = 0.011659272015094757
iteration 160, loss = 0.013586795888841152
iteration 161, loss = 0.0122457230463624
iteration 162, loss = 0.01040466409176588
iteration 163, loss = 0.016275379806756973
iteration 164, loss = 0.012326810508966446
iteration 165, loss = 0.01230310183018446
iteration 166, loss = 0.012371249496936798
iteration 167, loss = 0.011877553537487984
iteration 168, loss = 0.012425126507878304
iteration 169, loss = 0.015531999990344048
iteration 170, loss = 0.014351174235343933
iteration 171, loss = 0.01804969646036625
iteration 172, loss = 0.015297578647732735
iteration 173, loss = 0.010703044943511486
iteration 174, loss = 0.012840918265283108
iteration 175, loss = 0.011054727248847485
iteration 176, loss = 0.014921622350811958
iteration 177, loss = 0.013371252454817295
iteration 178, loss = 0.011891850270330906
iteration 179, loss = 0.013170138001441956
iteration 180, loss = 0.022709736600518227
iteration 181, loss = 0.010912356898188591
iteration 182, loss = 0.010784978047013283
iteration 183, loss = 0.010776645503938198
iteration 184, loss = 0.010566768236458302
iteration 185, loss = 0.012076682411134243
iteration 186, loss = 0.011322177946567535
iteration 187, loss = 0.015549713745713234
iteration 188, loss = 0.011625937186181545
iteration 189, loss = 0.011879348196089268
iteration 190, loss = 0.011623429134488106
iteration 191, loss = 0.013029594905674458
iteration 192, loss = 0.011249139904975891
iteration 193, loss = 0.016773143783211708
iteration 194, loss = 0.015586732886731625
iteration 195, loss = 0.011579965241253376
iteration 196, loss = 0.01674480549991131
iteration 197, loss = 0.011025100015103817
iteration 198, loss = 0.014854283072054386
iteration 199, loss = 0.012161757797002792
iteration 200, loss = 0.010875356383621693
iteration 201, loss = 0.011599685996770859
iteration 202, loss = 0.010801841504871845
iteration 203, loss = 0.014027882367372513
iteration 204, loss = 0.010925003327429295
iteration 205, loss = 0.013822770677506924
iteration 206, loss = 0.01165618933737278
iteration 207, loss = 0.010718985460698605
iteration 208, loss = 0.010817311704158783
iteration 209, loss = 0.01022773515433073
iteration 210, loss = 0.012094348669052124
iteration 211, loss = 0.01352117769420147
iteration 212, loss = 0.013637487776577473
iteration 213, loss = 0.01183414552360773
iteration 214, loss = 0.01164671778678894
iteration 215, loss = 0.012128517031669617
iteration 216, loss = 0.012370603159070015
iteration 217, loss = 0.010711763054132462
iteration 218, loss = 0.014066111296415329
iteration 219, loss = 0.012356842868030071
iteration 220, loss = 0.010868694633245468
iteration 221, loss = 0.012341880239546299
iteration 222, loss = 0.011001789011061192
iteration 223, loss = 0.011562743224203587
iteration 224, loss = 0.012205000966787338
iteration 225, loss = 0.01110575720667839
iteration 226, loss = 0.011189565993845463
iteration 227, loss = 0.01416181493550539
iteration 228, loss = 0.016838690266013145
iteration 229, loss = 0.010406959801912308
iteration 230, loss = 0.011150982230901718
iteration 231, loss = 0.010786834172904491
iteration 232, loss = 0.013000139966607094
iteration 233, loss = 0.011383092030882835
iteration 234, loss = 0.019236017018556595
iteration 235, loss = 0.011060763150453568
iteration 236, loss = 0.013939691707491875
iteration 237, loss = 0.011138579808175564
iteration 238, loss = 0.017343053594231606
iteration 239, loss = 0.013857726007699966
iteration 240, loss = 0.011954433284699917
iteration 241, loss = 0.011548192240297794
iteration 242, loss = 0.010337905026972294
iteration 243, loss = 0.017080282792448997
iteration 244, loss = 0.011499054729938507
iteration 245, loss = 0.010938791558146477
iteration 246, loss = 0.01463885698467493
iteration 247, loss = 0.011559594422578812
iteration 248, loss = 0.01168203353881836
iteration 249, loss = 0.010822873562574387
iteration 250, loss = 0.010608864948153496
iteration 251, loss = 0.012144885957241058
iteration 252, loss = 0.011330329813063145
iteration 253, loss = 0.014300031587481499
iteration 254, loss = 0.012237478978931904
iteration 255, loss = 0.011411397717893124
iteration 256, loss = 0.011005129665136337
iteration 257, loss = 0.016440702602267265
iteration 258, loss = 0.010339174419641495
iteration 259, loss = 0.010689856484532356
iteration 260, loss = 0.011276893317699432
iteration 261, loss = 0.011693703010678291
iteration 262, loss = 0.015382193960249424
iteration 263, loss = 0.013688524253666401
iteration 264, loss = 0.018022511154413223
iteration 265, loss = 0.010869205929338932
iteration 266, loss = 0.011399817653000355
iteration 267, loss = 0.016048947349190712
iteration 268, loss = 0.011101891286671162
iteration 269, loss = 0.014398537576198578
iteration 270, loss = 0.012024840340018272
iteration 271, loss = 0.015585605055093765
iteration 272, loss = 0.011746270582079887
iteration 273, loss = 0.011501144617795944
iteration 274, loss = 0.01064862497150898
iteration 275, loss = 0.01557798869907856
iteration 276, loss = 0.011620364151895046
iteration 277, loss = 0.014273039996623993
iteration 278, loss = 0.010898374952375889
iteration 279, loss = 0.017689945176243782
iteration 280, loss = 0.010637440718710423
iteration 281, loss = 0.012910187244415283
iteration 282, loss = 0.013774340972304344
iteration 283, loss = 0.011916453018784523
iteration 284, loss = 0.012159573845565319
iteration 285, loss = 0.01150086335837841
iteration 286, loss = 0.010255750268697739
iteration 287, loss = 0.013627900741994381
iteration 288, loss = 0.012317906133830547
iteration 289, loss = 0.011301718652248383
iteration 290, loss = 0.012263129465281963
iteration 291, loss = 0.016191085800528526
iteration 292, loss = 0.011138993315398693
iteration 293, loss = 0.014459161087870598
iteration 294, loss = 0.01081122551113367
iteration 295, loss = 0.010837335139513016
iteration 296, loss = 0.016999011859297752
iteration 297, loss = 0.011648241430521011
iteration 298, loss = 0.01076828595250845
iteration 299, loss = 0.010771081782877445
iteration 300, loss = 0.011484039016067982

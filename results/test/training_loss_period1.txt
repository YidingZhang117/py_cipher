iteration 1, loss = 2.8915791511535645
iteration 2, loss = 2.8520212173461914
iteration 3, loss = 2.818471670150757
iteration 4, loss = 2.7942776679992676
iteration 5, loss = 2.7188925743103027
iteration 6, loss = 2.6814358234405518
iteration 7, loss = 2.607928514480591
iteration 8, loss = 2.5697081089019775
iteration 9, loss = 2.5126943588256836
iteration 10, loss = 2.3891379833221436
iteration 11, loss = 2.2587087154388428
iteration 12, loss = 2.169029712677002
iteration 13, loss = 2.2044174671173096
iteration 14, loss = 2.052456855773926
iteration 15, loss = 1.9933364391326904
iteration 16, loss = 1.9049314260482788
iteration 17, loss = 1.8725385665893555
iteration 18, loss = 1.777576208114624
iteration 19, loss = 1.622562050819397
iteration 20, loss = 1.6798584461212158
iteration 21, loss = 1.6772199869155884
iteration 22, loss = 1.6363670825958252
iteration 23, loss = 1.4162994623184204
iteration 24, loss = 1.5112131834030151
iteration 25, loss = 1.2226306200027466
iteration 26, loss = 1.2086303234100342
iteration 27, loss = 1.2767300605773926
iteration 28, loss = 1.121415376663208
iteration 29, loss = 1.032726764678955
iteration 30, loss = 1.1011483669281006
iteration 31, loss = 1.1820061206817627
iteration 32, loss = 0.9662380814552307
iteration 33, loss = 1.0109846591949463
iteration 34, loss = 0.9610399007797241
iteration 35, loss = 1.0108399391174316
iteration 36, loss = 0.9505411982536316
iteration 37, loss = 0.9702102541923523
iteration 38, loss = 0.9306536316871643
iteration 39, loss = 0.887959361076355
iteration 40, loss = 0.8112101554870605
iteration 41, loss = 0.8007816076278687
iteration 42, loss = 0.8446102142333984
iteration 43, loss = 0.7778111696243286
iteration 44, loss = 0.7456438541412354
iteration 45, loss = 0.8203024864196777
iteration 46, loss = 0.6832068562507629
iteration 47, loss = 0.9170049428939819
iteration 48, loss = 0.8056933283805847
iteration 49, loss = 0.8198642730712891
iteration 50, loss = 0.8247436285018921
iteration 51, loss = 0.744225263595581
iteration 52, loss = 0.6434043645858765
iteration 53, loss = 0.8029583096504211
iteration 54, loss = 0.7230523824691772
iteration 55, loss = 0.7980122566223145
iteration 56, loss = 0.6847409009933472
iteration 57, loss = 0.7385743856430054
iteration 58, loss = 0.6913849115371704
iteration 59, loss = 0.6535642147064209
iteration 60, loss = 0.6290639638900757
iteration 61, loss = 0.7243015170097351
iteration 62, loss = 0.679569661617279
iteration 63, loss = 0.7070469856262207
iteration 64, loss = 0.7601627111434937
iteration 65, loss = 0.6695423722267151
iteration 66, loss = 0.7144930362701416
iteration 67, loss = 0.7069914937019348
iteration 68, loss = 0.7140994071960449
iteration 69, loss = 0.6797248125076294
iteration 70, loss = 0.7102697491645813
iteration 71, loss = 0.6780489683151245
iteration 72, loss = 0.7627530097961426
iteration 73, loss = 0.7048261165618896
iteration 74, loss = 0.6742268800735474
iteration 75, loss = 0.6701260209083557
iteration 76, loss = 0.6494784355163574
iteration 77, loss = 0.6985169053077698
iteration 78, loss = 0.7039275765419006
iteration 79, loss = 0.6757545471191406
iteration 80, loss = 0.7345836758613586
iteration 81, loss = 0.7284003496170044
iteration 82, loss = 0.6706122159957886
iteration 83, loss = 0.6672730445861816
iteration 84, loss = 0.6241410374641418
iteration 85, loss = 0.6457561254501343
iteration 86, loss = 0.6625405550003052
iteration 87, loss = 0.6565722823143005
iteration 88, loss = 0.7333720326423645
iteration 89, loss = 0.6357721090316772
iteration 90, loss = 0.625418484210968
iteration 91, loss = 0.696471631526947
iteration 92, loss = 0.7162138819694519
iteration 93, loss = 0.6285994052886963
iteration 94, loss = 0.6471798419952393
iteration 95, loss = 0.7380934953689575
iteration 96, loss = 0.7209702730178833
iteration 97, loss = 0.6138832569122314
iteration 98, loss = 0.6500216126441956
iteration 99, loss = 0.6998120546340942
iteration 100, loss = 0.6648741960525513
iteration 101, loss = 0.6887521743774414
iteration 102, loss = 0.6502257585525513
iteration 103, loss = 0.6629282832145691
iteration 104, loss = 0.6752831935882568
iteration 105, loss = 0.6995320916175842
iteration 106, loss = 0.6366721987724304
iteration 107, loss = 0.6418956518173218
iteration 108, loss = 0.6739054918289185
iteration 109, loss = 0.6353573799133301
iteration 110, loss = 0.6774905323982239
iteration 111, loss = 0.6117002367973328
iteration 112, loss = 0.7188661694526672
iteration 113, loss = 0.6309298276901245
iteration 114, loss = 0.6519272923469543
iteration 115, loss = 0.6397438049316406
iteration 116, loss = 0.6672913432121277
iteration 117, loss = 0.6254665851593018
iteration 118, loss = 0.6906622052192688
iteration 119, loss = 0.6046276688575745
iteration 120, loss = 0.6767653226852417
iteration 121, loss = 0.624438464641571
iteration 122, loss = 0.6536200046539307
iteration 123, loss = 0.6413884162902832
iteration 124, loss = 0.6000893115997314
iteration 125, loss = 0.6201007962226868
iteration 126, loss = 0.6327736377716064
iteration 127, loss = 0.6810842752456665
iteration 128, loss = 0.6132150888442993
iteration 129, loss = 0.7194265127182007
iteration 130, loss = 0.6831636428833008
iteration 131, loss = 0.6649468541145325
iteration 132, loss = 0.6074994206428528
iteration 133, loss = 0.654267430305481
iteration 134, loss = 0.692682683467865
iteration 135, loss = 0.6077519655227661
iteration 136, loss = 0.6366504430770874
iteration 137, loss = 0.6882755160331726
iteration 138, loss = 0.5916551947593689
iteration 139, loss = 0.6422046422958374
iteration 140, loss = 0.6318353414535522
iteration 141, loss = 0.6001641154289246
iteration 142, loss = 0.6797484755516052
iteration 143, loss = 0.6664611101150513
iteration 144, loss = 0.6467165350914001
iteration 145, loss = 0.6948263049125671
iteration 146, loss = 0.6443802714347839
iteration 147, loss = 0.6454885601997375
iteration 148, loss = 0.7025211453437805
iteration 149, loss = 0.6189416646957397
iteration 150, loss = 0.6026064157485962
iteration 151, loss = 0.62570720911026
iteration 152, loss = 0.6107057929039001
iteration 153, loss = 0.6440286636352539
iteration 154, loss = 0.6207475662231445
iteration 155, loss = 0.6158653497695923
iteration 156, loss = 0.6765835285186768
iteration 157, loss = 0.6448594331741333
iteration 158, loss = 0.6324048042297363
iteration 159, loss = 0.616714358329773
iteration 160, loss = 0.6506727337837219
iteration 161, loss = 0.5578189492225647
iteration 162, loss = 0.6356397867202759
iteration 163, loss = 0.6230262517929077
iteration 164, loss = 0.6370505094528198
iteration 165, loss = 0.6042408347129822
iteration 166, loss = 0.6811872720718384
iteration 167, loss = 0.6621172428131104
iteration 168, loss = 0.5993295311927795
iteration 169, loss = 0.541344404220581
iteration 170, loss = 0.6322267651557922
iteration 171, loss = 0.6525801420211792
iteration 172, loss = 0.620657205581665
iteration 173, loss = 0.6377091407775879
iteration 174, loss = 0.6300578117370605
iteration 175, loss = 0.5791504979133606
iteration 176, loss = 0.5805179476737976
iteration 177, loss = 0.5800787806510925
iteration 178, loss = 0.604407548904419
iteration 179, loss = 0.5484432578086853
iteration 180, loss = 0.6356644034385681
iteration 181, loss = 0.6410226225852966
iteration 182, loss = 0.6093475818634033
iteration 183, loss = 0.658332347869873
iteration 184, loss = 0.6139155626296997
iteration 185, loss = 0.6194709539413452
iteration 186, loss = 0.6108189821243286
iteration 187, loss = 0.6176480054855347
iteration 188, loss = 0.6646910905838013
iteration 189, loss = 0.5967491269111633
iteration 190, loss = 0.5717392563819885
iteration 191, loss = 0.6313399076461792
iteration 192, loss = 0.687682569026947
iteration 193, loss = 0.5636271238327026
iteration 194, loss = 0.6259787082672119
iteration 195, loss = 0.5516505241394043
iteration 196, loss = 0.5862236022949219
iteration 197, loss = 0.5367934107780457
iteration 198, loss = 0.5848598480224609
iteration 199, loss = 0.568835973739624
iteration 200, loss = 0.5980601906776428
iteration 201, loss = 0.6203519105911255
iteration 202, loss = 0.6260637044906616
iteration 203, loss = 0.5689467191696167
iteration 204, loss = 0.5781887769699097
iteration 205, loss = 0.608072817325592
iteration 206, loss = 0.5635738372802734
iteration 207, loss = 0.5383851528167725
iteration 208, loss = 0.6160114407539368
iteration 209, loss = 0.5459917187690735
iteration 210, loss = 0.5638294219970703
iteration 211, loss = 0.5804445743560791
iteration 212, loss = 0.5748199820518494
iteration 213, loss = 0.6121819019317627
iteration 214, loss = 0.5980907678604126
iteration 215, loss = 0.5885499119758606
iteration 216, loss = 0.557185709476471
iteration 217, loss = 0.5797052979469299
iteration 218, loss = 0.557564377784729
iteration 219, loss = 0.511796772480011
iteration 220, loss = 0.5641753077507019
iteration 221, loss = 0.6236539483070374
iteration 222, loss = 0.6027303338050842
iteration 223, loss = 0.5432846546173096
iteration 224, loss = 0.5400376319885254
iteration 225, loss = 0.5562664866447449
iteration 226, loss = 0.5865622758865356
iteration 227, loss = 0.5806888341903687
iteration 228, loss = 0.5800014138221741
iteration 229, loss = 0.5297195911407471
iteration 230, loss = 0.5730041861534119
iteration 231, loss = 0.5625149011611938
iteration 232, loss = 0.5331467986106873
iteration 233, loss = 0.563727855682373
iteration 234, loss = 0.5537266135215759
iteration 235, loss = 0.5142845511436462
iteration 236, loss = 0.5837607383728027
iteration 237, loss = 0.5738771557807922
iteration 238, loss = 0.589599609375
iteration 239, loss = 0.552176296710968
iteration 240, loss = 0.5323609709739685
iteration 241, loss = 0.506942093372345
iteration 242, loss = 0.5735214352607727
iteration 243, loss = 0.5339617729187012
iteration 244, loss = 0.5792093873023987
iteration 245, loss = 0.5159311890602112
iteration 246, loss = 0.4815007448196411
iteration 247, loss = 0.501051127910614
iteration 248, loss = 0.5205974578857422
iteration 249, loss = 0.5277334451675415
iteration 250, loss = 0.5286117196083069
iteration 251, loss = 0.5654011368751526
iteration 252, loss = 0.5471948981285095
iteration 253, loss = 0.5008755922317505
iteration 254, loss = 0.5118391513824463
iteration 255, loss = 0.4924597144126892
iteration 256, loss = 0.461361289024353
iteration 257, loss = 0.5185726284980774
iteration 258, loss = 0.49408918619155884
iteration 259, loss = 0.5245978236198425
iteration 260, loss = 0.4860907196998596
iteration 261, loss = 0.5460201501846313
iteration 262, loss = 0.5563516616821289
iteration 263, loss = 0.5117825269699097
iteration 264, loss = 0.5722517371177673
iteration 265, loss = 0.5618367195129395
iteration 266, loss = 0.5197150707244873
iteration 267, loss = 0.5191794633865356
iteration 268, loss = 0.5093988180160522
iteration 269, loss = 0.6490465998649597
iteration 270, loss = 0.5282713174819946
iteration 271, loss = 0.5788073539733887
iteration 272, loss = 0.5277891755104065
iteration 273, loss = 0.5760856866836548
iteration 274, loss = 0.5193032026290894
iteration 275, loss = 0.46283766627311707
iteration 276, loss = 0.5382012128829956
iteration 277, loss = 0.5344071388244629
iteration 278, loss = 0.5313685536384583
iteration 279, loss = 0.5616810917854309
iteration 280, loss = 0.5043743848800659
iteration 281, loss = 0.5139056444168091
iteration 282, loss = 0.527080774307251
iteration 283, loss = 0.5283252596855164
iteration 284, loss = 0.5126516222953796
iteration 285, loss = 0.5528568625450134
iteration 286, loss = 0.5034644603729248
iteration 287, loss = 0.532403290271759
iteration 288, loss = 0.5161744356155396
iteration 289, loss = 0.4620666801929474
iteration 290, loss = 0.526437520980835
iteration 291, loss = 0.5659285187721252
iteration 292, loss = 0.509514331817627
iteration 293, loss = 0.5080658197402954
iteration 294, loss = 0.453816682100296
iteration 295, loss = 0.49737173318862915
iteration 296, loss = 0.5114089846611023
iteration 297, loss = 0.4683302640914917
iteration 298, loss = 0.46155625581741333
iteration 299, loss = 0.46721094846725464
iteration 300, loss = 0.53076171875
iteration 1, loss = 0.49115511775016785
iteration 2, loss = 0.49465933442115784
iteration 3, loss = 0.4989278018474579
iteration 4, loss = 0.462144672870636
iteration 5, loss = 0.5047545433044434
iteration 6, loss = 0.45651775598526
iteration 7, loss = 0.4957609474658966
iteration 8, loss = 0.4646531045436859
iteration 9, loss = 0.5278686285018921
iteration 10, loss = 0.4327254593372345
iteration 11, loss = 0.5656304359436035
iteration 12, loss = 0.46630722284317017
iteration 13, loss = 0.45972564816474915
iteration 14, loss = 0.4405292868614197
iteration 15, loss = 0.4442378878593445
iteration 16, loss = 0.45951950550079346
iteration 17, loss = 0.5110417604446411
iteration 18, loss = 0.44445762038230896
iteration 19, loss = 0.40163561701774597
iteration 20, loss = 0.46584153175354004
iteration 21, loss = 0.49808019399642944
iteration 22, loss = 0.45125430822372437
iteration 23, loss = 0.45445114374160767
iteration 24, loss = 0.467628538608551
iteration 25, loss = 0.5721204876899719
iteration 26, loss = 0.4243886470794678
iteration 27, loss = 0.4462973475456238
iteration 28, loss = 0.5491962432861328
iteration 29, loss = 0.4434605836868286
iteration 30, loss = 0.44329315423965454
iteration 31, loss = 0.4340425729751587
iteration 32, loss = 0.4445037245750427
iteration 33, loss = 0.4339132606983185
iteration 34, loss = 0.5001223087310791
iteration 35, loss = 0.4147939682006836
iteration 36, loss = 0.4247455596923828
iteration 37, loss = 0.44584834575653076
iteration 38, loss = 0.4117741286754608
iteration 39, loss = 0.41659101843833923
iteration 40, loss = 0.4331522583961487
iteration 41, loss = 0.407183438539505
iteration 42, loss = 0.43902525305747986
iteration 43, loss = 0.43111395835876465
iteration 44, loss = 0.461721807718277
iteration 45, loss = 0.4290786385536194
iteration 46, loss = 0.4237746298313141
iteration 47, loss = 0.47582969069480896
iteration 48, loss = 0.4235196113586426
iteration 49, loss = 0.4567094147205353
iteration 50, loss = 0.4504060745239258
iteration 51, loss = 0.5056795477867126
iteration 52, loss = 0.46649670600891113
iteration 53, loss = 0.5199764370918274
iteration 54, loss = 0.42671412229537964
iteration 55, loss = 0.3966600298881531
iteration 56, loss = 0.36448755860328674
iteration 57, loss = 0.45638227462768555
iteration 58, loss = 0.41252243518829346
iteration 59, loss = 0.3905069828033447
iteration 60, loss = 0.42369282245635986
iteration 61, loss = 0.407476544380188
iteration 62, loss = 0.4305267632007599
iteration 63, loss = 0.42905694246292114
iteration 64, loss = 0.42865172028541565
iteration 65, loss = 0.3893158733844757
iteration 66, loss = 0.4028593599796295
iteration 67, loss = 0.44284605979919434
iteration 68, loss = 0.4268399477005005
iteration 69, loss = 0.44470158219337463
iteration 70, loss = 0.4362795948982239
iteration 71, loss = 0.38451820611953735
iteration 72, loss = 0.3944249451160431
iteration 73, loss = 0.37525027990341187
iteration 74, loss = 0.3994288444519043
iteration 75, loss = 0.5184748768806458
iteration 76, loss = 0.4303339719772339
iteration 77, loss = 0.402199923992157
iteration 78, loss = 0.36890825629234314
iteration 79, loss = 0.3835613429546356
iteration 80, loss = 0.4006199240684509
iteration 81, loss = 0.4001731872558594
iteration 82, loss = 0.3737669289112091
iteration 83, loss = 0.4320991337299347
iteration 84, loss = 0.40288859605789185
iteration 85, loss = 0.4009547829627991
iteration 86, loss = 0.3695714473724365
iteration 87, loss = 0.386540949344635
iteration 88, loss = 0.338919460773468
iteration 89, loss = 0.3848244249820709
iteration 90, loss = 0.3941437602043152
iteration 91, loss = 0.38436174392700195
iteration 92, loss = 0.41781753301620483
iteration 93, loss = 0.3761879801750183
iteration 94, loss = 0.37949466705322266
iteration 95, loss = 0.4115760922431946
iteration 96, loss = 0.37921619415283203
iteration 97, loss = 0.3961091935634613
iteration 98, loss = 0.38583117723464966
iteration 99, loss = 0.34871774911880493
iteration 100, loss = 0.36251896619796753
iteration 101, loss = 0.38787880539894104
iteration 102, loss = 0.38071534037590027
iteration 103, loss = 0.42134493589401245
iteration 104, loss = 0.3585352897644043
iteration 105, loss = 0.4009406268596649
iteration 106, loss = 0.33859962224960327
iteration 107, loss = 0.4220988154411316
iteration 108, loss = 0.3519672751426697
iteration 109, loss = 0.3443358838558197
iteration 110, loss = 0.3499224781990051
iteration 111, loss = 0.3947630226612091
iteration 112, loss = 0.3499454855918884
iteration 113, loss = 0.32753002643585205
iteration 114, loss = 0.3287540674209595
iteration 115, loss = 0.3949979841709137
iteration 116, loss = 0.34693288803100586
iteration 117, loss = 0.32332155108451843
iteration 118, loss = 0.3691073954105377
iteration 119, loss = 0.3846176564693451
iteration 120, loss = 0.3684948682785034
iteration 121, loss = 0.3473713994026184
iteration 122, loss = 0.3409790098667145
iteration 123, loss = 0.4240598976612091
iteration 124, loss = 0.3542940616607666
iteration 125, loss = 0.374311625957489
iteration 126, loss = 0.34699809551239014
iteration 127, loss = 0.37324950098991394
iteration 128, loss = 0.35311636328697205
iteration 129, loss = 0.32280850410461426
iteration 130, loss = 0.287413090467453
iteration 131, loss = 0.3136363923549652
iteration 132, loss = 0.3483780324459076
iteration 133, loss = 0.3482617139816284
iteration 134, loss = 0.35146230459213257
iteration 135, loss = 0.29056355357170105
iteration 136, loss = 0.3455839455127716
iteration 137, loss = 0.3500942587852478
iteration 138, loss = 0.39174771308898926
iteration 139, loss = 0.3230321407318115
iteration 140, loss = 0.3377137780189514
iteration 141, loss = 0.34794947504997253
iteration 142, loss = 0.29481470584869385
iteration 143, loss = 0.36837345361709595
iteration 144, loss = 0.3594626784324646
iteration 145, loss = 0.3584073483943939
iteration 146, loss = 0.3298701047897339
iteration 147, loss = 0.2900411784648895
iteration 148, loss = 0.3132422864437103
iteration 149, loss = 0.38315996527671814
iteration 150, loss = 0.33050718903541565
iteration 151, loss = 0.32565170526504517
iteration 152, loss = 0.32052043080329895
iteration 153, loss = 0.3093707263469696
iteration 154, loss = 0.3175358176231384
iteration 155, loss = 0.3809193968772888
iteration 156, loss = 0.302053302526474
iteration 157, loss = 0.33857864141464233
iteration 158, loss = 0.3655865788459778
iteration 159, loss = 0.3210984766483307
iteration 160, loss = 0.3130362629890442
iteration 161, loss = 0.38005852699279785
iteration 162, loss = 0.306233674287796
iteration 163, loss = 0.2959579825401306
iteration 164, loss = 0.35427412390708923
iteration 165, loss = 0.29464343190193176
iteration 166, loss = 0.29064974188804626
iteration 167, loss = 0.4107608497142792
iteration 168, loss = 0.28949666023254395
iteration 169, loss = 0.2692176401615143
iteration 170, loss = 0.288604199886322
iteration 171, loss = 0.2898331582546234
iteration 172, loss = 0.2769755721092224
iteration 173, loss = 0.316711962223053
iteration 174, loss = 0.28282925486564636
iteration 175, loss = 0.25657573342323303
iteration 176, loss = 0.2957322895526886
iteration 177, loss = 0.25716617703437805
iteration 178, loss = 0.2794710695743561
iteration 179, loss = 0.2989805340766907
iteration 180, loss = 0.3121131658554077
iteration 181, loss = 0.3820428252220154
iteration 182, loss = 0.32057273387908936
iteration 183, loss = 0.3038356304168701
iteration 184, loss = 0.2885562479496002
iteration 185, loss = 0.2863883972167969
iteration 186, loss = 0.31282463669776917
iteration 187, loss = 0.3253169655799866
iteration 188, loss = 0.30346426367759705
iteration 189, loss = 0.32026511430740356
iteration 190, loss = 0.3385569453239441
iteration 191, loss = 0.3332650065422058
iteration 192, loss = 0.27545246481895447
iteration 193, loss = 0.26811444759368896
iteration 194, loss = 0.28159934282302856
iteration 195, loss = 0.37714001536369324
iteration 196, loss = 0.30102115869522095
iteration 197, loss = 0.2660970985889435
iteration 198, loss = 0.27784574031829834
iteration 199, loss = 0.26551684737205505
iteration 200, loss = 0.26821473240852356
iteration 201, loss = 0.2608778178691864
iteration 202, loss = 0.29008257389068604
iteration 203, loss = 0.2574302554130554
iteration 204, loss = 0.42629528045654297
iteration 205, loss = 0.23406857252120972
iteration 206, loss = 0.2578147053718567
iteration 207, loss = 0.30989184975624084
iteration 208, loss = 0.2608141303062439
iteration 209, loss = 0.2812064290046692
iteration 210, loss = 0.23433618247509003
iteration 211, loss = 0.2516907751560211
iteration 212, loss = 0.28932985663414
iteration 213, loss = 0.24800485372543335
iteration 214, loss = 0.2794018089771271
iteration 215, loss = 0.22900861501693726
iteration 216, loss = 0.3074556887149811
iteration 217, loss = 0.2696157395839691
iteration 218, loss = 0.2616279721260071
iteration 219, loss = 0.25627249479293823
iteration 220, loss = 0.27395546436309814
iteration 221, loss = 0.31816503405570984
iteration 222, loss = 0.24230803549289703
iteration 223, loss = 0.30636894702911377
iteration 224, loss = 0.28097808361053467
iteration 225, loss = 0.24493883550167084
iteration 226, loss = 0.22977329790592194
iteration 227, loss = 0.24773088097572327
iteration 228, loss = 0.28294637799263
iteration 229, loss = 0.31397610902786255
iteration 230, loss = 0.22395990788936615
iteration 231, loss = 0.26325803995132446
iteration 232, loss = 0.29988041520118713
iteration 233, loss = 0.2395821362733841
iteration 234, loss = 0.2777767479419708
iteration 235, loss = 0.2041277140378952
iteration 236, loss = 0.25284042954444885
iteration 237, loss = 0.24910211563110352
iteration 238, loss = 0.25209423899650574
iteration 239, loss = 0.2673863470554352
iteration 240, loss = 0.2936665117740631
iteration 241, loss = 0.24239884316921234
iteration 242, loss = 0.2470656782388687
iteration 243, loss = 0.24348489940166473
iteration 244, loss = 0.2138705998659134
iteration 245, loss = 0.2758816182613373
iteration 246, loss = 0.21376684308052063
iteration 247, loss = 0.20574623346328735
iteration 248, loss = 0.2694191038608551
iteration 249, loss = 0.19885998964309692
iteration 250, loss = 0.26046591997146606
iteration 251, loss = 0.19860629737377167
iteration 252, loss = 0.21746259927749634
iteration 253, loss = 0.26272326707839966
iteration 254, loss = 0.2379257082939148
iteration 255, loss = 0.36470451951026917
iteration 256, loss = 0.24837669730186462
iteration 257, loss = 0.20957325398921967
iteration 258, loss = 0.2980170249938965
iteration 259, loss = 0.2337200939655304
iteration 260, loss = 0.26380154490470886
iteration 261, loss = 0.23417793214321136
iteration 262, loss = 0.2612197697162628
iteration 263, loss = 0.26341667771339417
iteration 264, loss = 0.2054087519645691
iteration 265, loss = 0.24858888983726501
iteration 266, loss = 0.24134734272956848
iteration 267, loss = 0.2054596096277237
iteration 268, loss = 0.217343270778656
iteration 269, loss = 0.22778519988059998
iteration 270, loss = 0.23528307676315308
iteration 271, loss = 0.22842110693454742
iteration 272, loss = 0.2223919928073883
iteration 273, loss = 0.2361171543598175
iteration 274, loss = 0.178180530667305
iteration 275, loss = 0.219028502702713
iteration 276, loss = 0.2651413381099701
iteration 277, loss = 0.23344939947128296
iteration 278, loss = 0.22770406305789948
iteration 279, loss = 0.2161557525396347
iteration 280, loss = 0.19869808852672577
iteration 281, loss = 0.23940807580947876
iteration 282, loss = 0.19900952279567719
iteration 283, loss = 0.210849791765213
iteration 284, loss = 0.20873279869556427
iteration 285, loss = 0.2076680064201355
iteration 286, loss = 0.1827213019132614
iteration 287, loss = 0.21113896369934082
iteration 288, loss = 0.20408503711223602
iteration 289, loss = 0.21845343708992004
iteration 290, loss = 0.2547011077404022
iteration 291, loss = 0.22042271494865417
iteration 292, loss = 0.22486291825771332
iteration 293, loss = 0.22076444327831268
iteration 294, loss = 0.2033349722623825
iteration 295, loss = 0.17416983842849731
iteration 296, loss = 0.17141814529895782
iteration 297, loss = 0.23096537590026855
iteration 298, loss = 0.16920430958271027
iteration 299, loss = 0.18132272362709045
iteration 300, loss = 0.21321912109851837
iteration 1, loss = 0.23381991684436798
iteration 2, loss = 0.21331718564033508
iteration 3, loss = 0.19353799521923065
iteration 4, loss = 0.2074800431728363
iteration 5, loss = 0.24394166469573975
iteration 6, loss = 0.21511784195899963
iteration 7, loss = 0.16653627157211304
iteration 8, loss = 0.21659335494041443
iteration 9, loss = 0.20155304670333862
iteration 10, loss = 0.20814134180545807
iteration 11, loss = 0.2035759538412094
iteration 12, loss = 0.20847797393798828
iteration 13, loss = 0.1882893443107605
iteration 14, loss = 0.17334705591201782
iteration 15, loss = 0.16719955205917358
iteration 16, loss = 0.16928188502788544
iteration 17, loss = 0.18743093311786652
iteration 18, loss = 0.20824430882930756
iteration 19, loss = 0.23423275351524353
iteration 20, loss = 0.1719387322664261
iteration 21, loss = 0.17703993618488312
iteration 22, loss = 0.17493745684623718
iteration 23, loss = 0.1892179250717163
iteration 24, loss = 0.19870898127555847
iteration 25, loss = 0.1996922492980957
iteration 26, loss = 0.24249990284442902
iteration 27, loss = 0.19327400624752045
iteration 28, loss = 0.18474958837032318
iteration 29, loss = 0.16630315780639648
iteration 30, loss = 0.193314328789711
iteration 31, loss = 0.19095581769943237
iteration 32, loss = 0.1808871179819107
iteration 33, loss = 0.233535498380661
iteration 34, loss = 0.17163759469985962
iteration 35, loss = 0.193431094288826
iteration 36, loss = 0.15430064499378204
iteration 37, loss = 0.19962765276432037
iteration 38, loss = 0.1580197811126709
iteration 39, loss = 0.17440417408943176
iteration 40, loss = 0.1879202276468277
iteration 41, loss = 0.1841701865196228
iteration 42, loss = 0.1704341173171997
iteration 43, loss = 0.17424675822257996
iteration 44, loss = 0.15412543714046478
iteration 45, loss = 0.15571613609790802
iteration 46, loss = 0.15604093670845032
iteration 47, loss = 0.18243499100208282
iteration 48, loss = 0.15339405834674835
iteration 49, loss = 0.20928436517715454
iteration 50, loss = 0.1464911699295044
iteration 51, loss = 0.16831716895103455
iteration 52, loss = 0.15977421402931213
iteration 53, loss = 0.2512856125831604
iteration 54, loss = 0.1505311280488968
iteration 55, loss = 0.20559221506118774
iteration 56, loss = 0.14887388050556183
iteration 57, loss = 0.16016130149364471
iteration 58, loss = 0.15814578533172607
iteration 59, loss = 0.164119154214859
iteration 60, loss = 0.19681943953037262
iteration 61, loss = 0.17701764404773712
iteration 62, loss = 0.1542830765247345
iteration 63, loss = 0.17855075001716614
iteration 64, loss = 0.26071545481681824
iteration 65, loss = 0.14968161284923553
iteration 66, loss = 0.1499674916267395
iteration 67, loss = 0.17183040082454681
iteration 68, loss = 0.14568032324314117
iteration 69, loss = 0.18124975264072418
iteration 70, loss = 0.1360766589641571
iteration 71, loss = 0.1473146378993988
iteration 72, loss = 0.18487951159477234
iteration 73, loss = 0.18454980850219727
iteration 74, loss = 0.16292226314544678
iteration 75, loss = 0.17096814513206482
iteration 76, loss = 0.19238486886024475
iteration 77, loss = 0.1525798738002777
iteration 78, loss = 0.13909290730953217
iteration 79, loss = 0.14156828820705414
iteration 80, loss = 0.13802561163902283
iteration 81, loss = 0.14352841675281525
iteration 82, loss = 0.16141076385974884
iteration 83, loss = 0.14763323962688446
iteration 84, loss = 0.169745072722435
iteration 85, loss = 0.1558140367269516
iteration 86, loss = 0.14327937364578247
iteration 87, loss = 0.14287514984607697
iteration 88, loss = 0.14998535811901093
iteration 89, loss = 0.21363548934459686
iteration 90, loss = 0.1430605947971344
iteration 91, loss = 0.13739687204360962
iteration 92, loss = 0.12544462084770203
iteration 93, loss = 0.1672230064868927
iteration 94, loss = 0.14264284074306488
iteration 95, loss = 0.1494302898645401
iteration 96, loss = 0.15682615339756012
iteration 97, loss = 0.13731002807617188
iteration 98, loss = 0.15626677870750427
iteration 99, loss = 0.13345490396022797
iteration 100, loss = 0.13571393489837646
iteration 101, loss = 0.14895246922969818
iteration 102, loss = 0.13366934657096863
iteration 103, loss = 0.16786575317382812
iteration 104, loss = 0.12640030682086945
iteration 105, loss = 0.13389280438423157
iteration 106, loss = 0.14289212226867676
iteration 107, loss = 0.14875207841396332
iteration 108, loss = 0.15830568969249725
iteration 109, loss = 0.12574243545532227
iteration 110, loss = 0.13192155957221985
iteration 111, loss = 0.12406077235937119
iteration 112, loss = 0.141269251704216
iteration 113, loss = 0.11651935428380966
iteration 114, loss = 0.13896559178829193
iteration 115, loss = 0.14425048232078552
iteration 116, loss = 0.12724827229976654
iteration 117, loss = 0.13446274399757385
iteration 118, loss = 0.14653611183166504
iteration 119, loss = 0.24071428179740906
iteration 120, loss = 0.12282561510801315
iteration 121, loss = 0.1262045055627823
iteration 122, loss = 0.15010981261730194
iteration 123, loss = 0.16428899765014648
iteration 124, loss = 0.14356249570846558
iteration 125, loss = 0.12176971137523651
iteration 126, loss = 0.14149925112724304
iteration 127, loss = 0.1229589432477951
iteration 128, loss = 0.19731228053569794
iteration 129, loss = 0.13399310410022736
iteration 130, loss = 0.13628731667995453
iteration 131, loss = 0.12665927410125732
iteration 132, loss = 0.13965149223804474
iteration 133, loss = 0.12407013028860092
iteration 134, loss = 0.1510215401649475
iteration 135, loss = 0.1225149855017662
iteration 136, loss = 0.12745270133018494
iteration 137, loss = 0.16520673036575317
iteration 138, loss = 0.12496265023946762
iteration 139, loss = 0.11601956188678741
iteration 140, loss = 0.12478502094745636
iteration 141, loss = 0.13662654161453247
iteration 142, loss = 0.19972102344036102
iteration 143, loss = 0.18458333611488342
iteration 144, loss = 0.15544889867305756
iteration 145, loss = 0.13075625896453857
iteration 146, loss = 0.13385413587093353
iteration 147, loss = 0.13687825202941895
iteration 148, loss = 0.13548026978969574
iteration 149, loss = 0.12040729820728302
iteration 150, loss = 0.13912178575992584
iteration 151, loss = 0.12269522249698639
iteration 152, loss = 0.10475585609674454
iteration 153, loss = 0.12025545537471771
iteration 154, loss = 0.11456648260354996
iteration 155, loss = 0.11717528104782104
iteration 156, loss = 0.1191200241446495
iteration 157, loss = 0.12308792769908905
iteration 158, loss = 0.13273873925209045
iteration 159, loss = 0.1187981367111206
iteration 160, loss = 0.13657788932323456
iteration 161, loss = 0.10522419959306717
iteration 162, loss = 0.11185988038778305
iteration 163, loss = 0.13511191308498383
iteration 164, loss = 0.13127979636192322
iteration 165, loss = 0.14020995795726776
iteration 166, loss = 0.11521773785352707
iteration 167, loss = 0.1243027001619339
iteration 168, loss = 0.12213156372308731
iteration 169, loss = 0.12529689073562622
iteration 170, loss = 0.13584168255329132
iteration 171, loss = 0.12500013411045074
iteration 172, loss = 0.12145110219717026
iteration 173, loss = 0.13284967839717865
iteration 174, loss = 0.13898655772209167
iteration 175, loss = 0.11297035962343216
iteration 176, loss = 0.11448812484741211
iteration 177, loss = 0.1174091175198555
iteration 178, loss = 0.12500885128974915
iteration 179, loss = 0.11304136365652084
iteration 180, loss = 0.12517285346984863
iteration 181, loss = 0.152113139629364
iteration 182, loss = 0.1026478260755539
iteration 183, loss = 0.13489867746829987
iteration 184, loss = 0.11796798557043076
iteration 185, loss = 0.12872657179832458
iteration 186, loss = 0.1343604028224945
iteration 187, loss = 0.1112227514386177
iteration 188, loss = 0.11373093724250793
iteration 189, loss = 0.11331664025783539
iteration 190, loss = 0.14033357799053192
iteration 191, loss = 0.12377823144197464
iteration 192, loss = 0.0931519940495491
iteration 193, loss = 0.13321858644485474
iteration 194, loss = 0.12779973447322845
iteration 195, loss = 0.13143624365329742
iteration 196, loss = 0.10997623950242996
iteration 197, loss = 0.11760534346103668
iteration 198, loss = 0.10435067862272263
iteration 199, loss = 0.09986086189746857
iteration 200, loss = 0.0987795740365982
iteration 201, loss = 0.1412912905216217
iteration 202, loss = 0.11306244879961014
iteration 203, loss = 0.11076004803180695
iteration 204, loss = 0.11773279309272766
iteration 205, loss = 0.10789754241704941
iteration 206, loss = 0.10422135144472122
iteration 207, loss = 0.09640665352344513
iteration 208, loss = 0.09835516661405563
iteration 209, loss = 0.10877333581447601
iteration 210, loss = 0.10185324400663376
iteration 211, loss = 0.09818951040506363
iteration 212, loss = 0.10962051153182983
iteration 213, loss = 0.1233590617775917
iteration 214, loss = 0.1168089434504509
iteration 215, loss = 0.11626425385475159
iteration 216, loss = 0.15106631815433502
iteration 217, loss = 0.10965628176927567
iteration 218, loss = 0.1338920146226883
iteration 219, loss = 0.10004779696464539
iteration 220, loss = 0.138652503490448
iteration 221, loss = 0.09734342992305756
iteration 222, loss = 0.12350329756736755
iteration 223, loss = 0.13501210510730743
iteration 224, loss = 0.11100315302610397
iteration 225, loss = 0.13905207812786102
iteration 226, loss = 0.11309923231601715
iteration 227, loss = 0.1530362367630005
iteration 228, loss = 0.12514789402484894
iteration 229, loss = 0.10037686675786972
iteration 230, loss = 0.1247006207704544
iteration 231, loss = 0.0995328351855278
iteration 232, loss = 0.09435530006885529
iteration 233, loss = 0.11870364844799042
iteration 234, loss = 0.09572571516036987
iteration 235, loss = 0.11351928859949112
iteration 236, loss = 0.10843400657176971
iteration 237, loss = 0.09887570142745972
iteration 238, loss = 0.11927860975265503
iteration 239, loss = 0.10382292419672012
iteration 240, loss = 0.10469630360603333
iteration 241, loss = 0.09199339151382446
iteration 242, loss = 0.10774233192205429
iteration 243, loss = 0.09815485030412674
iteration 244, loss = 0.09716922789812088
iteration 245, loss = 0.11206336319446564
iteration 246, loss = 0.08800442516803741
iteration 247, loss = 0.09577975422143936
iteration 248, loss = 0.1302616447210312
iteration 249, loss = 0.09484276920557022
iteration 250, loss = 0.10747826844453812
iteration 251, loss = 0.12873469293117523
iteration 252, loss = 0.09325503557920456
iteration 253, loss = 0.10274109989404678
iteration 254, loss = 0.10243046283721924
iteration 255, loss = 0.08674327284097672
iteration 256, loss = 0.08659489452838898
iteration 257, loss = 0.09781859815120697
iteration 258, loss = 0.09224332869052887
iteration 259, loss = 0.0912526324391365
iteration 260, loss = 0.10187734663486481
iteration 261, loss = 0.10164155066013336
iteration 262, loss = 0.1420319378376007
iteration 263, loss = 0.09392129629850388
iteration 264, loss = 0.15464909374713898
iteration 265, loss = 0.10379820317029953
iteration 266, loss = 0.08335541933774948
iteration 267, loss = 0.1226181909441948
iteration 268, loss = 0.11212991178035736
iteration 269, loss = 0.10601267963647842
iteration 270, loss = 0.08487505465745926
iteration 271, loss = 0.09376703202724457
iteration 272, loss = 0.08546523004770279
iteration 273, loss = 0.11704458296298981
iteration 274, loss = 0.10421107709407806
iteration 275, loss = 0.10293449461460114
iteration 276, loss = 0.0828961506485939
iteration 277, loss = 0.08437243849039078
iteration 278, loss = 0.08154553174972534
iteration 279, loss = 0.08702996373176575
iteration 280, loss = 0.08607658743858337
iteration 281, loss = 0.1041950210928917
iteration 282, loss = 0.10290146619081497
iteration 283, loss = 0.08189953863620758
iteration 284, loss = 0.10623521357774734
iteration 285, loss = 0.10114730894565582
iteration 286, loss = 0.08740227669477463
iteration 287, loss = 0.08494003117084503
iteration 288, loss = 0.08751639723777771
iteration 289, loss = 0.09460343420505524
iteration 290, loss = 0.08664180338382721
iteration 291, loss = 0.1065179780125618
iteration 292, loss = 0.09955817461013794
iteration 293, loss = 0.10458648204803467
iteration 294, loss = 0.08546550571918488
iteration 295, loss = 0.07893390208482742
iteration 296, loss = 0.1186554804444313
iteration 297, loss = 0.07193105667829514
iteration 298, loss = 0.09831271320581436
iteration 299, loss = 0.07879438251256943
iteration 300, loss = 0.09678249061107635
iteration 1, loss = 0.07480017095804214
iteration 2, loss = 0.08399141579866409
iteration 3, loss = 0.08559775352478027
iteration 4, loss = 0.11335106194019318
iteration 5, loss = 0.11708525568246841
iteration 6, loss = 0.08941878378391266
iteration 7, loss = 0.10506514459848404
iteration 8, loss = 0.11804292351007462
iteration 9, loss = 0.08198288828134537
iteration 10, loss = 0.08030375093221664
iteration 11, loss = 0.0745258703827858
iteration 12, loss = 0.08367423713207245
iteration 13, loss = 0.08797498792409897
iteration 14, loss = 0.07606565207242966
iteration 15, loss = 0.11495064198970795
iteration 16, loss = 0.0821017175912857
iteration 17, loss = 0.07866311073303223
iteration 18, loss = 0.09243971109390259
iteration 19, loss = 0.08321651816368103
iteration 20, loss = 0.07979069650173187
iteration 21, loss = 0.09409883618354797
iteration 22, loss = 0.08590967208147049
iteration 23, loss = 0.09082014858722687
iteration 24, loss = 0.0851319283246994
iteration 25, loss = 0.08870533108711243
iteration 26, loss = 0.10392846167087555
iteration 27, loss = 0.10678014904260635
iteration 28, loss = 0.0775148868560791
iteration 29, loss = 0.09065449237823486
iteration 30, loss = 0.09811415523290634
iteration 31, loss = 0.07315678894519806
iteration 32, loss = 0.07995731383562088
iteration 33, loss = 0.08127107471227646
iteration 34, loss = 0.07797747850418091
iteration 35, loss = 0.08612024039030075
iteration 36, loss = 0.08651994913816452
iteration 37, loss = 0.08241812884807587
iteration 38, loss = 0.07617911696434021
iteration 39, loss = 0.11154203861951828
iteration 40, loss = 0.07555932551622391
iteration 41, loss = 0.09111150354146957
iteration 42, loss = 0.08010917156934738
iteration 43, loss = 0.0840654969215393
iteration 44, loss = 0.07106677442789078
iteration 45, loss = 0.08116772025823593
iteration 46, loss = 0.0674925297498703
iteration 47, loss = 0.07297112792730331
iteration 48, loss = 0.09762212634086609
iteration 49, loss = 0.06850158423185349
iteration 50, loss = 0.07864794880151749
iteration 51, loss = 0.09124250710010529
iteration 52, loss = 0.07524970918893814
iteration 53, loss = 0.07605317234992981
iteration 54, loss = 0.0833202674984932
iteration 55, loss = 0.08500827848911285
iteration 56, loss = 0.11591337621212006
iteration 57, loss = 0.07187317311763763
iteration 58, loss = 0.10692518949508667
iteration 59, loss = 0.06992903351783752
iteration 60, loss = 0.07423900812864304
iteration 61, loss = 0.06756117939949036
iteration 62, loss = 0.08346584439277649
iteration 63, loss = 0.0758611336350441
iteration 64, loss = 0.08731257170438766
iteration 65, loss = 0.07444236427545547
iteration 66, loss = 0.08092452585697174
iteration 67, loss = 0.06808169186115265
iteration 68, loss = 0.06561793386936188
iteration 69, loss = 0.07931304723024368
iteration 70, loss = 0.07021679729223251
iteration 71, loss = 0.06840143352746964
iteration 72, loss = 0.0690200924873352
iteration 73, loss = 0.07723703235387802
iteration 74, loss = 0.0897526741027832
iteration 75, loss = 0.07483690977096558
iteration 76, loss = 0.08211179822683334
iteration 77, loss = 0.07518962770700455
iteration 78, loss = 0.07750556617975235
iteration 79, loss = 0.07123924046754837
iteration 80, loss = 0.0803341493010521
iteration 81, loss = 0.07720580697059631
iteration 82, loss = 0.08032241463661194
iteration 83, loss = 0.0683952197432518
iteration 84, loss = 0.06451306492090225
iteration 85, loss = 0.08484640717506409
iteration 86, loss = 0.09541863203048706
iteration 87, loss = 0.06250552088022232
iteration 88, loss = 0.06686011701822281
iteration 89, loss = 0.0687614157795906
iteration 90, loss = 0.07405348122119904
iteration 91, loss = 0.08125921338796616
iteration 92, loss = 0.0858883336186409
iteration 93, loss = 0.09120851010084152
iteration 94, loss = 0.06295259296894073
iteration 95, loss = 0.07105861604213715
iteration 96, loss = 0.0690949335694313
iteration 97, loss = 0.06524219363927841
iteration 98, loss = 0.08684329688549042
iteration 99, loss = 0.06607497483491898
iteration 100, loss = 0.060915399342775345
iteration 101, loss = 0.06329359114170074
iteration 102, loss = 0.06079613044857979
iteration 103, loss = 0.06301257014274597
iteration 104, loss = 0.06573843955993652
iteration 105, loss = 0.06420069187879562
iteration 106, loss = 0.05815138667821884
iteration 107, loss = 0.06968416273593903
iteration 108, loss = 0.06593935191631317
iteration 109, loss = 0.08118880540132523
iteration 110, loss = 0.07652565836906433
iteration 111, loss = 0.07618197053670883
iteration 112, loss = 0.059766680002212524
iteration 113, loss = 0.06106532737612724
iteration 114, loss = 0.05756056681275368
iteration 115, loss = 0.09220945090055466
iteration 116, loss = 0.06652020663022995
iteration 117, loss = 0.07306698709726334
iteration 118, loss = 0.06108013167977333
iteration 119, loss = 0.06762422621250153
iteration 120, loss = 0.0745968148112297
iteration 121, loss = 0.08143300563097
iteration 122, loss = 0.06168566271662712
iteration 123, loss = 0.05822774022817612
iteration 124, loss = 0.05821838974952698
iteration 125, loss = 0.060715362429618835
iteration 126, loss = 0.06149410828948021
iteration 127, loss = 0.05736754089593887
iteration 128, loss = 0.06453890353441238
iteration 129, loss = 0.06297089159488678
iteration 130, loss = 0.06043625995516777
iteration 131, loss = 0.06518439203500748
iteration 132, loss = 0.07244398444890976
iteration 133, loss = 0.057716649025678635
iteration 134, loss = 0.062304481863975525
iteration 135, loss = 0.062490567564964294
iteration 136, loss = 0.06677620112895966
iteration 137, loss = 0.057285964488983154
iteration 138, loss = 0.05929908528923988
iteration 139, loss = 0.06496942043304443
iteration 140, loss = 0.05330763757228851
iteration 141, loss = 0.061258602887392044
iteration 142, loss = 0.05884561687707901
iteration 143, loss = 0.06494438648223877
iteration 144, loss = 0.05627460032701492
iteration 145, loss = 0.05994422733783722
iteration 146, loss = 0.06862316280603409
iteration 147, loss = 0.060259029269218445
iteration 148, loss = 0.06599808484315872
iteration 149, loss = 0.05565580725669861
iteration 150, loss = 0.06555933505296707
iteration 151, loss = 0.056562040001153946
iteration 152, loss = 0.08826018124818802
iteration 153, loss = 0.05908815190196037
iteration 154, loss = 0.059852294623851776
iteration 155, loss = 0.06747494637966156
iteration 156, loss = 0.07158659398555756
iteration 157, loss = 0.052817005664110184
iteration 158, loss = 0.06877297163009644
iteration 159, loss = 0.05376245826482773
iteration 160, loss = 0.0668846145272255
iteration 161, loss = 0.06825226545333862
iteration 162, loss = 0.05688035860657692
iteration 163, loss = 0.07172863930463791
iteration 164, loss = 0.06802767515182495
iteration 165, loss = 0.055700547993183136
iteration 166, loss = 0.05213547125458717
iteration 167, loss = 0.052493032068014145
iteration 168, loss = 0.06138405576348305
iteration 169, loss = 0.05542147159576416
iteration 170, loss = 0.06547100841999054
iteration 171, loss = 0.06642890721559525
iteration 172, loss = 0.05957387387752533
iteration 173, loss = 0.057984039187431335
iteration 174, loss = 0.06236725673079491
iteration 175, loss = 0.05735766142606735
iteration 176, loss = 0.06968137621879578
iteration 177, loss = 0.07391276955604553
iteration 178, loss = 0.052296143025159836
iteration 179, loss = 0.05072646215558052
iteration 180, loss = 0.06357529014348984
iteration 181, loss = 0.05297490954399109
iteration 182, loss = 0.05398564785718918
iteration 183, loss = 0.05270905792713165
iteration 184, loss = 0.06802155822515488
iteration 185, loss = 0.08240847289562225
iteration 186, loss = 0.05109049379825592
iteration 187, loss = 0.07578039169311523
iteration 188, loss = 0.06106702610850334
iteration 189, loss = 0.05543690547347069
iteration 190, loss = 0.0559433288872242
iteration 191, loss = 0.06261025369167328
iteration 192, loss = 0.050457846373319626
iteration 193, loss = 0.07106024026870728
iteration 194, loss = 0.07730405032634735
iteration 195, loss = 0.050480686128139496
iteration 196, loss = 0.057669028639793396
iteration 197, loss = 0.06520058959722519
iteration 198, loss = 0.07419043779373169
iteration 199, loss = 0.049991488456726074
iteration 200, loss = 0.05451177433133125
iteration 201, loss = 0.06011276692152023
iteration 202, loss = 0.05262652039527893
iteration 203, loss = 0.05317280814051628
iteration 204, loss = 0.048204075545072556
iteration 205, loss = 0.05273279920220375
iteration 206, loss = 0.06533139944076538
iteration 207, loss = 0.051203127950429916
iteration 208, loss = 0.051307350397109985
iteration 209, loss = 0.053212158381938934
iteration 210, loss = 0.06464610248804092
iteration 211, loss = 0.05970524623990059
iteration 212, loss = 0.053286198526620865
iteration 213, loss = 0.05077362805604935
iteration 214, loss = 0.06068452075123787
iteration 215, loss = 0.05451196804642677
iteration 216, loss = 0.05330662429332733
iteration 217, loss = 0.055759552866220474
iteration 218, loss = 0.04859570786356926
iteration 219, loss = 0.04754381626844406
iteration 220, loss = 0.05661498382687569
iteration 221, loss = 0.050640739500522614
iteration 222, loss = 0.051433589309453964
iteration 223, loss = 0.055546607822179794
iteration 224, loss = 0.0525725856423378
iteration 225, loss = 0.05804496258497238
iteration 226, loss = 0.04821455106139183
iteration 227, loss = 0.056582268327474594
iteration 228, loss = 0.04967394471168518
iteration 229, loss = 0.05149202048778534
iteration 230, loss = 0.049536868929862976
iteration 231, loss = 0.05872782692313194
iteration 232, loss = 0.05060744658112526
iteration 233, loss = 0.048414260149002075
iteration 234, loss = 0.05003922060132027
iteration 235, loss = 0.05234096944332123
iteration 236, loss = 0.04623258858919144
iteration 237, loss = 0.04530110955238342
iteration 238, loss = 0.06249585375189781
iteration 239, loss = 0.04737932235002518
iteration 240, loss = 0.06645627319812775
iteration 241, loss = 0.04628928378224373
iteration 242, loss = 0.051546208560466766
iteration 243, loss = 0.04929516837000847
iteration 244, loss = 0.04542785882949829
iteration 245, loss = 0.04462985321879387
iteration 246, loss = 0.04734348878264427
iteration 247, loss = 0.05390831083059311
iteration 248, loss = 0.056229084730148315
iteration 249, loss = 0.04616601765155792
iteration 250, loss = 0.05932726711034775
iteration 251, loss = 0.0552772618830204
iteration 252, loss = 0.07625040411949158
iteration 253, loss = 0.04832780361175537
iteration 254, loss = 0.044427573680877686
iteration 255, loss = 0.05580125376582146
iteration 256, loss = 0.04756878316402435
iteration 257, loss = 0.045957546681165695
iteration 258, loss = 0.06254476308822632
iteration 259, loss = 0.045982085168361664
iteration 260, loss = 0.05278013274073601
iteration 261, loss = 0.05677655711770058
iteration 262, loss = 0.06684752553701401
iteration 263, loss = 0.04558423161506653
iteration 264, loss = 0.07783821225166321
iteration 265, loss = 0.04401513561606407
iteration 266, loss = 0.04743702709674835
iteration 267, loss = 0.07092180848121643
iteration 268, loss = 0.05503969267010689
iteration 269, loss = 0.050640325993299484
iteration 270, loss = 0.060710031539201736
iteration 271, loss = 0.05165810137987137
iteration 272, loss = 0.04235381633043289
iteration 273, loss = 0.06832487881183624
iteration 274, loss = 0.05271546542644501
iteration 275, loss = 0.06081965193152428
iteration 276, loss = 0.04787488654255867
iteration 277, loss = 0.06231401860713959
iteration 278, loss = 0.043784089386463165
iteration 279, loss = 0.04480520263314247
iteration 280, loss = 0.04156875982880592
iteration 281, loss = 0.04644321650266647
iteration 282, loss = 0.05199486017227173
iteration 283, loss = 0.04862600192427635
iteration 284, loss = 0.07717211544513702
iteration 285, loss = 0.04450163617730141
iteration 286, loss = 0.04774677753448486
iteration 287, loss = 0.04363727197051048
iteration 288, loss = 0.049958646297454834
iteration 289, loss = 0.062568798661232
iteration 290, loss = 0.04071482643485069
iteration 291, loss = 0.04369944706559181
iteration 292, loss = 0.041856441646814346
iteration 293, loss = 0.04925232008099556
iteration 294, loss = 0.05364689230918884
iteration 295, loss = 0.04418632388114929
iteration 296, loss = 0.052842527627944946
iteration 297, loss = 0.04812208190560341
iteration 298, loss = 0.043208807706832886
iteration 299, loss = 0.04691881313920021
iteration 300, loss = 0.04283622279763222
iteration 1, loss = 0.04229596257209778
iteration 2, loss = 0.04504232108592987
iteration 3, loss = 0.04142129048705101
iteration 4, loss = 0.05933983251452446
iteration 5, loss = 0.042241886258125305
iteration 6, loss = 0.04759073257446289
iteration 7, loss = 0.04312647879123688
iteration 8, loss = 0.04290320724248886
iteration 9, loss = 0.047479502856731415
iteration 10, loss = 0.06102820485830307
iteration 11, loss = 0.04214747995138168
iteration 12, loss = 0.05126998573541641
iteration 13, loss = 0.04482688382267952
iteration 14, loss = 0.04801670089364052
iteration 15, loss = 0.041121263056993484
iteration 16, loss = 0.04238478094339371
iteration 17, loss = 0.05330841988325119
iteration 18, loss = 0.042885150760412216
iteration 19, loss = 0.03947320207953453
iteration 20, loss = 0.041217368096113205
iteration 21, loss = 0.04212680459022522
iteration 22, loss = 0.046437475830316544
iteration 23, loss = 0.05086836218833923
iteration 24, loss = 0.0517134815454483
iteration 25, loss = 0.04908272624015808
iteration 26, loss = 0.045318763703107834
iteration 27, loss = 0.04951731115579605
iteration 28, loss = 0.050442516803741455
iteration 29, loss = 0.04227791354060173
iteration 30, loss = 0.038667090237140656
iteration 31, loss = 0.040520958602428436
iteration 32, loss = 0.0554315447807312
iteration 33, loss = 0.04277518764138222
iteration 34, loss = 0.03835028409957886
iteration 35, loss = 0.04433692619204521
iteration 36, loss = 0.039014849811792374
iteration 37, loss = 0.038945384323596954
iteration 38, loss = 0.04443027824163437
iteration 39, loss = 0.044443100690841675
iteration 40, loss = 0.05176358297467232
iteration 41, loss = 0.05460129678249359
iteration 42, loss = 0.05293547734618187
iteration 43, loss = 0.03812853991985321
iteration 44, loss = 0.05726522207260132
iteration 45, loss = 0.04233468696475029
iteration 46, loss = 0.050979286432266235
iteration 47, loss = 0.04560360312461853
iteration 48, loss = 0.040331337600946426
iteration 49, loss = 0.054832518100738525
iteration 50, loss = 0.05196823179721832
iteration 51, loss = 0.0376289039850235
iteration 52, loss = 0.03885367512702942
iteration 53, loss = 0.04191581904888153
iteration 54, loss = 0.04948477819561958
iteration 55, loss = 0.05007801577448845
iteration 56, loss = 0.03760311007499695
iteration 57, loss = 0.039591263979673386
iteration 58, loss = 0.03970099613070488
iteration 59, loss = 0.03930305317044258
iteration 60, loss = 0.05271552503108978
iteration 61, loss = 0.05400313436985016
iteration 62, loss = 0.039415497332811356
iteration 63, loss = 0.03991733118891716
iteration 64, loss = 0.036055419594049454
iteration 65, loss = 0.04852310195565224
iteration 66, loss = 0.040406372398138046
iteration 67, loss = 0.03764929249882698
iteration 68, loss = 0.03776266425848007
iteration 69, loss = 0.039452508091926575
iteration 70, loss = 0.03857824578881264
iteration 71, loss = 0.050890177488327026
iteration 72, loss = 0.04247082769870758
iteration 73, loss = 0.035925474017858505
iteration 74, loss = 0.03621082752943039
iteration 75, loss = 0.03773121163249016
iteration 76, loss = 0.041419535875320435
iteration 77, loss = 0.039692532271146774
iteration 78, loss = 0.05224621668457985
iteration 79, loss = 0.04477003216743469
iteration 80, loss = 0.03583481162786484
iteration 81, loss = 0.04622524231672287
iteration 82, loss = 0.04617614671587944
iteration 83, loss = 0.03488294407725334
iteration 84, loss = 0.04402939975261688
iteration 85, loss = 0.04177749529480934
iteration 86, loss = 0.04245932400226593
iteration 87, loss = 0.041848886758089066
iteration 88, loss = 0.038561467081308365
iteration 89, loss = 0.03943804278969765
iteration 90, loss = 0.05273207649588585
iteration 91, loss = 0.03957226127386093
iteration 92, loss = 0.049599774181842804
iteration 93, loss = 0.041182685643434525
iteration 94, loss = 0.038009513169527054
iteration 95, loss = 0.038469213992357254
iteration 96, loss = 0.03487059473991394
iteration 97, loss = 0.05232007056474686
iteration 98, loss = 0.039151012897491455
iteration 99, loss = 0.047338273376226425
iteration 100, loss = 0.04842274636030197
iteration 101, loss = 0.04462360963225365
iteration 102, loss = 0.033940162509679794
iteration 103, loss = 0.039387039840221405
iteration 104, loss = 0.03575676679611206
iteration 105, loss = 0.03581182286143303
iteration 106, loss = 0.04113898053765297
iteration 107, loss = 0.03551824018359184
iteration 108, loss = 0.04145826771855354
iteration 109, loss = 0.04587186127901077
iteration 110, loss = 0.03826100379228592
iteration 111, loss = 0.03616221621632576
iteration 112, loss = 0.038507621735334396
iteration 113, loss = 0.040592897683382034
iteration 114, loss = 0.03795451298356056
iteration 115, loss = 0.037489600479602814
iteration 116, loss = 0.03431183472275734
iteration 117, loss = 0.037108078598976135
iteration 118, loss = 0.03493471443653107
iteration 119, loss = 0.036507003009319305
iteration 120, loss = 0.03786976635456085
iteration 121, loss = 0.041265591979026794
iteration 122, loss = 0.0345618799328804
iteration 123, loss = 0.032961368560791016
iteration 124, loss = 0.05145560950040817
iteration 125, loss = 0.03249305859208107
iteration 126, loss = 0.03348662704229355
iteration 127, loss = 0.03483927622437477
iteration 128, loss = 0.047862615436315536
iteration 129, loss = 0.044096581637859344
iteration 130, loss = 0.04384871944785118
iteration 131, loss = 0.035286616533994675
iteration 132, loss = 0.039473507553339005
iteration 133, loss = 0.04673653841018677
iteration 134, loss = 0.04668934643268585
iteration 135, loss = 0.04885871708393097
iteration 136, loss = 0.03477043658494949
iteration 137, loss = 0.0543367825448513
iteration 138, loss = 0.03418099507689476
iteration 139, loss = 0.03295077756047249
iteration 140, loss = 0.04032805934548378
iteration 141, loss = 0.03562077879905701
iteration 142, loss = 0.03121127560734749
iteration 143, loss = 0.03691287338733673
iteration 144, loss = 0.036819711327552795
iteration 145, loss = 0.04064755514264107
iteration 146, loss = 0.0335034504532814
iteration 147, loss = 0.034041374921798706
iteration 148, loss = 0.042666222900152206
iteration 149, loss = 0.03882964700460434
iteration 150, loss = 0.03888282552361488
iteration 151, loss = 0.03379027917981148
iteration 152, loss = 0.043205998837947845
iteration 153, loss = 0.04048774391412735
iteration 154, loss = 0.030675513669848442
iteration 155, loss = 0.03444672003388405
iteration 156, loss = 0.032693035900592804
iteration 157, loss = 0.032540302723646164
iteration 158, loss = 0.04179565608501434
iteration 159, loss = 0.033975616097450256
iteration 160, loss = 0.033041927963495255
iteration 161, loss = 0.03279819339513779
iteration 162, loss = 0.03406985104084015
iteration 163, loss = 0.04077691584825516
iteration 164, loss = 0.03501744568347931
iteration 165, loss = 0.033792171627283096
iteration 166, loss = 0.034108418971300125
iteration 167, loss = 0.030195871368050575
iteration 168, loss = 0.03288409113883972
iteration 169, loss = 0.03506604954600334
iteration 170, loss = 0.033021096140146255
iteration 171, loss = 0.04176048934459686
iteration 172, loss = 0.035170018672943115
iteration 173, loss = 0.04216182976961136
iteration 174, loss = 0.03455303981900215
iteration 175, loss = 0.03522450849413872
iteration 176, loss = 0.040012214332818985
iteration 177, loss = 0.035632211714982986
iteration 178, loss = 0.03680961951613426
iteration 179, loss = 0.03037140890955925
iteration 180, loss = 0.038452692329883575
iteration 181, loss = 0.034078583121299744
iteration 182, loss = 0.03661249205470085
iteration 183, loss = 0.041678182780742645
iteration 184, loss = 0.03170470893383026
iteration 185, loss = 0.03203316405415535
iteration 186, loss = 0.031852368265390396
iteration 187, loss = 0.034358058124780655
iteration 188, loss = 0.03218146786093712
iteration 189, loss = 0.03597888723015785
iteration 190, loss = 0.03143879771232605
iteration 191, loss = 0.034336067736148834
iteration 192, loss = 0.036147959530353546
iteration 193, loss = 0.0458204559981823
iteration 194, loss = 0.037170927971601486
iteration 195, loss = 0.04614212363958359
iteration 196, loss = 0.033617813140153885
iteration 197, loss = 0.03239208832383156
iteration 198, loss = 0.028867295011878014
iteration 199, loss = 0.032605454325675964
iteration 200, loss = 0.03545951843261719
iteration 201, loss = 0.031356584280729294
iteration 202, loss = 0.03274871036410332
iteration 203, loss = 0.030583500862121582
iteration 204, loss = 0.030021239072084427
iteration 205, loss = 0.03070281632244587
iteration 206, loss = 0.04206620901823044
iteration 207, loss = 0.03036860004067421
iteration 208, loss = 0.029781902208924294
iteration 209, loss = 0.02889677695930004
iteration 210, loss = 0.03200004994869232
iteration 211, loss = 0.03408173471689224
iteration 212, loss = 0.03171359375119209
iteration 213, loss = 0.035732317715883255
iteration 214, loss = 0.029519246891140938
iteration 215, loss = 0.02988470159471035
iteration 216, loss = 0.04437711834907532
iteration 217, loss = 0.03179017826914787
iteration 218, loss = 0.028377236798405647
iteration 219, loss = 0.033105283975601196
iteration 220, loss = 0.028970615938305855
iteration 221, loss = 0.032441772520542145
iteration 222, loss = 0.030761564150452614
iteration 223, loss = 0.028687912970781326
iteration 224, loss = 0.036235615611076355
iteration 225, loss = 0.03815285861492157
iteration 226, loss = 0.02981468103826046
iteration 227, loss = 0.031125664710998535
iteration 228, loss = 0.02979332022368908
iteration 229, loss = 0.03006003051996231
iteration 230, loss = 0.03205706924200058
iteration 231, loss = 0.03377201035618782
iteration 232, loss = 0.03359614685177803
iteration 233, loss = 0.03841930255293846
iteration 234, loss = 0.029730413109064102
iteration 235, loss = 0.037241075187921524
iteration 236, loss = 0.04105435311794281
iteration 237, loss = 0.04545166715979576
iteration 238, loss = 0.029333679005503654
iteration 239, loss = 0.02726929821074009
iteration 240, loss = 0.04522969573736191
iteration 241, loss = 0.0378342941403389
iteration 242, loss = 0.029101379215717316
iteration 243, loss = 0.044178109616041183
iteration 244, loss = 0.03973942622542381
iteration 245, loss = 0.038538575172424316
iteration 246, loss = 0.03744670748710632
iteration 247, loss = 0.028841214254498482
iteration 248, loss = 0.04965545982122421
iteration 249, loss = 0.028986793011426926
iteration 250, loss = 0.02878873609006405
iteration 251, loss = 0.04012202098965645
iteration 252, loss = 0.027867494150996208
iteration 253, loss = 0.04113566875457764
iteration 254, loss = 0.030924782156944275
iteration 255, loss = 0.03623029962182045
iteration 256, loss = 0.0299251489341259
iteration 257, loss = 0.030260499566793442
iteration 258, loss = 0.027218913659453392
iteration 259, loss = 0.029703697189688683
iteration 260, loss = 0.031517576426267624
iteration 261, loss = 0.03974772244691849
iteration 262, loss = 0.028409164398908615
iteration 263, loss = 0.037401556968688965
iteration 264, loss = 0.03800540789961815
iteration 265, loss = 0.02747887559235096
iteration 266, loss = 0.03440010920166969
iteration 267, loss = 0.03488704562187195
iteration 268, loss = 0.03180283308029175
iteration 269, loss = 0.027865329757332802
iteration 270, loss = 0.028329677879810333
iteration 271, loss = 0.028042491525411606
iteration 272, loss = 0.03253890573978424
iteration 273, loss = 0.03400964289903641
iteration 274, loss = 0.03674275055527687
iteration 275, loss = 0.03806252032518387
iteration 276, loss = 0.02721681445837021
iteration 277, loss = 0.02810671553015709
iteration 278, loss = 0.029106512665748596
iteration 279, loss = 0.03616072237491608
iteration 280, loss = 0.028552116826176643
iteration 281, loss = 0.04112686216831207
iteration 282, loss = 0.035212352871894836
iteration 283, loss = 0.0341373085975647
iteration 284, loss = 0.027012959122657776
iteration 285, loss = 0.027728872373700142
iteration 286, loss = 0.03412918746471405
iteration 287, loss = 0.02517390437424183
iteration 288, loss = 0.03332894295454025
iteration 289, loss = 0.025012152269482613
iteration 290, loss = 0.028429178521037102
iteration 291, loss = 0.03553934395313263
iteration 292, loss = 0.029298031702637672
iteration 293, loss = 0.027006253600120544
iteration 294, loss = 0.03036281280219555
iteration 295, loss = 0.0284949392080307
iteration 296, loss = 0.03200624883174896
iteration 297, loss = 0.026784293353557587
iteration 298, loss = 0.025675226002931595
iteration 299, loss = 0.028523895889520645
iteration 300, loss = 0.027210203930735588
iteration 1, loss = 0.03391430154442787
iteration 2, loss = 0.033353228121995926
iteration 3, loss = 0.036312639713287354
iteration 4, loss = 0.036097943782806396
iteration 5, loss = 0.02707553654909134
iteration 6, loss = 0.03242957219481468
iteration 7, loss = 0.0254014004021883
iteration 8, loss = 0.030596110969781876
iteration 9, loss = 0.02711169421672821
iteration 10, loss = 0.026892123743891716
iteration 11, loss = 0.03622805327177048
iteration 12, loss = 0.02781843952834606
iteration 13, loss = 0.026824070140719414
iteration 14, loss = 0.03553067892789841
iteration 15, loss = 0.03353600949048996
iteration 16, loss = 0.0283755324780941
iteration 17, loss = 0.025074630975723267
iteration 18, loss = 0.026615919545292854
iteration 19, loss = 0.02507597580552101
iteration 20, loss = 0.02670961804687977
iteration 21, loss = 0.026402704417705536
iteration 22, loss = 0.02567167393863201
iteration 23, loss = 0.027390943840146065
iteration 24, loss = 0.02642929181456566
iteration 25, loss = 0.027179818600416183
iteration 26, loss = 0.02600114792585373
iteration 27, loss = 0.024342065677046776
iteration 28, loss = 0.02527236007153988
iteration 29, loss = 0.03170732408761978
iteration 30, loss = 0.029817819595336914
iteration 31, loss = 0.03531363978981972
iteration 32, loss = 0.032042115926742554
iteration 33, loss = 0.03520737960934639
iteration 34, loss = 0.025413507595658302
iteration 35, loss = 0.02615354023873806
iteration 36, loss = 0.031624600291252136
iteration 37, loss = 0.026233645156025887
iteration 38, loss = 0.025974512100219727
iteration 39, loss = 0.028017353266477585
iteration 40, loss = 0.026286069303750992
iteration 41, loss = 0.02353043667972088
iteration 42, loss = 0.025064218789339066
iteration 43, loss = 0.024169107899069786
iteration 44, loss = 0.0346514955163002
iteration 45, loss = 0.026845280081033707
iteration 46, loss = 0.025311633944511414
iteration 47, loss = 0.03375425189733505
iteration 48, loss = 0.026030154898762703
iteration 49, loss = 0.031064748764038086
iteration 50, loss = 0.031435854732990265
iteration 51, loss = 0.025822224095463753
iteration 52, loss = 0.02495710738003254
iteration 53, loss = 0.02557486481964588
iteration 54, loss = 0.02808341197669506
iteration 55, loss = 0.03027517721056938
iteration 56, loss = 0.04015759006142616
iteration 57, loss = 0.02886001579463482
iteration 58, loss = 0.03504810854792595
iteration 59, loss = 0.03224879503250122
iteration 60, loss = 0.024515293538570404
iteration 61, loss = 0.02929668314754963
iteration 62, loss = 0.027721840888261795
iteration 63, loss = 0.02741958759725094
iteration 64, loss = 0.026043184101581573
iteration 65, loss = 0.028245065361261368
iteration 66, loss = 0.029644286260008812
iteration 67, loss = 0.02465043030679226
iteration 68, loss = 0.03470345586538315
iteration 69, loss = 0.023895027115941048
iteration 70, loss = 0.02458694949746132
iteration 71, loss = 0.033370207995176315
iteration 72, loss = 0.028565220534801483
iteration 73, loss = 0.026079513132572174
iteration 74, loss = 0.025200799107551575
iteration 75, loss = 0.02568294294178486
iteration 76, loss = 0.024374129250645638
iteration 77, loss = 0.02376301772892475
iteration 78, loss = 0.02729088068008423
iteration 79, loss = 0.024713588878512383
iteration 80, loss = 0.03461814299225807
iteration 81, loss = 0.023050149902701378
iteration 82, loss = 0.027845751494169235
iteration 83, loss = 0.024289926514029503
iteration 84, loss = 0.026868153363466263
iteration 85, loss = 0.02579185739159584
iteration 86, loss = 0.02429010346531868
iteration 87, loss = 0.02999395690858364
iteration 88, loss = 0.027621671557426453
iteration 89, loss = 0.02256222628057003
iteration 90, loss = 0.022405223920941353
iteration 91, loss = 0.02655908465385437
iteration 92, loss = 0.024300556629896164
iteration 93, loss = 0.024747377261519432
iteration 94, loss = 0.040114328265190125
iteration 95, loss = 0.023414283990859985
iteration 96, loss = 0.022841963917016983
iteration 97, loss = 0.032113172113895416
iteration 98, loss = 0.02459576725959778
iteration 99, loss = 0.024429211392998695
iteration 100, loss = 0.023921223357319832
iteration 101, loss = 0.025028852745890617
iteration 102, loss = 0.02385818585753441
iteration 103, loss = 0.02312569133937359
iteration 104, loss = 0.02600753679871559
iteration 105, loss = 0.02390000782907009
iteration 106, loss = 0.029550855979323387
iteration 107, loss = 0.025260737165808678
iteration 108, loss = 0.02444031834602356
iteration 109, loss = 0.032317984849214554
iteration 110, loss = 0.024181952700018883
iteration 111, loss = 0.023565486073493958
iteration 112, loss = 0.025049317628145218
iteration 113, loss = 0.03141069784760475
iteration 114, loss = 0.022826118394732475
iteration 115, loss = 0.023791076615452766
iteration 116, loss = 0.02346860244870186
iteration 117, loss = 0.0219282079488039
iteration 118, loss = 0.021693283692002296
iteration 119, loss = 0.022399775683879852
iteration 120, loss = 0.02164878137409687
iteration 121, loss = 0.024703718721866608
iteration 122, loss = 0.024828076362609863
iteration 123, loss = 0.02219937928020954
iteration 124, loss = 0.024012848734855652
iteration 125, loss = 0.022891001775860786
iteration 126, loss = 0.032049790024757385
iteration 127, loss = 0.022824633866548538
iteration 128, loss = 0.029099779203534126
iteration 129, loss = 0.023807905614376068
iteration 130, loss = 0.02146807126700878
iteration 131, loss = 0.037044014781713486
iteration 132, loss = 0.02627849392592907
iteration 133, loss = 0.0272245854139328
iteration 134, loss = 0.024772651493549347
iteration 135, loss = 0.02804511785507202
iteration 136, loss = 0.022460076957941055
iteration 137, loss = 0.029747940599918365
iteration 138, loss = 0.02995445765554905
iteration 139, loss = 0.023097896948456764
iteration 140, loss = 0.022860953584313393
iteration 141, loss = 0.025324681773781776
iteration 142, loss = 0.02395126409828663
iteration 143, loss = 0.022308841347694397
iteration 144, loss = 0.0210714191198349
iteration 145, loss = 0.0216844379901886
iteration 146, loss = 0.022437255829572678
iteration 147, loss = 0.024510622024536133
iteration 148, loss = 0.02633046545088291
iteration 149, loss = 0.031412240117788315
iteration 150, loss = 0.025297114625573158
iteration 151, loss = 0.029769906774163246
iteration 152, loss = 0.025934088975191116
iteration 153, loss = 0.026210665702819824
iteration 154, loss = 0.024027233943343163
iteration 155, loss = 0.020766258239746094
iteration 156, loss = 0.02638799510896206
iteration 157, loss = 0.02127484418451786
iteration 158, loss = 0.023273127153515816
iteration 159, loss = 0.02188717946410179
iteration 160, loss = 0.02900824509561062
iteration 161, loss = 0.022548576816916466
iteration 162, loss = 0.026892438530921936
iteration 163, loss = 0.022629648447036743
iteration 164, loss = 0.028845636174082756
iteration 165, loss = 0.021918999031186104
iteration 166, loss = 0.022224146872758865
iteration 167, loss = 0.02349497564136982
iteration 168, loss = 0.02396129071712494
iteration 169, loss = 0.023951275274157524
iteration 170, loss = 0.021817438304424286
iteration 171, loss = 0.021499086171388626
iteration 172, loss = 0.026471693068742752
iteration 173, loss = 0.022174257785081863
iteration 174, loss = 0.021707311272621155
iteration 175, loss = 0.026204649358987808
iteration 176, loss = 0.02052062563598156
iteration 177, loss = 0.019984768703579903
iteration 178, loss = 0.023247260600328445
iteration 179, loss = 0.020531676709651947
iteration 180, loss = 0.020193787291646004
iteration 181, loss = 0.022727234289050102
iteration 182, loss = 0.022037087008357048
iteration 183, loss = 0.035029686987400055
iteration 184, loss = 0.02951917238533497
iteration 185, loss = 0.02206757292151451
iteration 186, loss = 0.020299987867474556
iteration 187, loss = 0.022050203755497932
iteration 188, loss = 0.02420738711953163
iteration 189, loss = 0.022450612857937813
iteration 190, loss = 0.019963983446359634
iteration 191, loss = 0.023502685129642487
iteration 192, loss = 0.024002423509955406
iteration 193, loss = 0.028850432485342026
iteration 194, loss = 0.021619170904159546
iteration 195, loss = 0.01984582096338272
iteration 196, loss = 0.022141002118587494
iteration 197, loss = 0.020907219499349594
iteration 198, loss = 0.026258928701281548
iteration 199, loss = 0.020592426881194115
iteration 200, loss = 0.01988927088677883
iteration 201, loss = 0.01947304978966713
iteration 202, loss = 0.027897581458091736
iteration 203, loss = 0.028721116483211517
iteration 204, loss = 0.022416573017835617
iteration 205, loss = 0.01962607353925705
iteration 206, loss = 0.029183711856603622
iteration 207, loss = 0.020673029124736786
iteration 208, loss = 0.021532589569687843
iteration 209, loss = 0.021825483068823814
iteration 210, loss = 0.018987929448485374
iteration 211, loss = 0.025825224816799164
iteration 212, loss = 0.02911568619310856
iteration 213, loss = 0.020668188109993935
iteration 214, loss = 0.01998390257358551
iteration 215, loss = 0.026665572077035904
iteration 216, loss = 0.020665759220719337
iteration 217, loss = 0.02607027254998684
iteration 218, loss = 0.02093309350311756
iteration 219, loss = 0.02746955119073391
iteration 220, loss = 0.03805677220225334
iteration 221, loss = 0.020629972219467163
iteration 222, loss = 0.021960340440273285
iteration 223, loss = 0.019315142184495926
iteration 224, loss = 0.01918570138514042
iteration 225, loss = 0.020834974944591522
iteration 226, loss = 0.021273944526910782
iteration 227, loss = 0.0285414420068264
iteration 228, loss = 0.030430475249886513
iteration 229, loss = 0.019906766712665558
iteration 230, loss = 0.026086660102009773
iteration 231, loss = 0.02122778818011284
iteration 232, loss = 0.02405303344130516
iteration 233, loss = 0.018636539578437805
iteration 234, loss = 0.02099558897316456
iteration 235, loss = 0.018505387008190155
iteration 236, loss = 0.024546708911657333
iteration 237, loss = 0.023786861449480057
iteration 238, loss = 0.026077911257743835
iteration 239, loss = 0.019883275032043457
iteration 240, loss = 0.02373712882399559
iteration 241, loss = 0.020084945484995842
iteration 242, loss = 0.020021952688694
iteration 243, loss = 0.020607616752386093
iteration 244, loss = 0.020573418587446213
iteration 245, loss = 0.02471926622092724
iteration 246, loss = 0.0196799598634243
iteration 247, loss = 0.01859671249985695
iteration 248, loss = 0.02213119901716709
iteration 249, loss = 0.01931304670870304
iteration 250, loss = 0.0213055070489645
iteration 251, loss = 0.02012542076408863
iteration 252, loss = 0.019664540886878967
iteration 253, loss = 0.024691080674529076
iteration 254, loss = 0.022662309929728508
iteration 255, loss = 0.020266830921173096
iteration 256, loss = 0.022024832665920258
iteration 257, loss = 0.019927138462662697
iteration 258, loss = 0.020419824868440628
iteration 259, loss = 0.01882578246295452
iteration 260, loss = 0.018849583342671394
iteration 261, loss = 0.020024225115776062
iteration 262, loss = 0.02035476267337799
iteration 263, loss = 0.022498279809951782
iteration 264, loss = 0.01979922503232956
iteration 265, loss = 0.020591337233781815
iteration 266, loss = 0.021691961213946342
iteration 267, loss = 0.02611967921257019
iteration 268, loss = 0.02682011015713215
iteration 269, loss = 0.02553110010921955
iteration 270, loss = 0.020954463630914688
iteration 271, loss = 0.018610937520861626
iteration 272, loss = 0.020207421854138374
iteration 273, loss = 0.020883720368146896
iteration 274, loss = 0.017998266965150833
iteration 275, loss = 0.019601231440901756
iteration 276, loss = 0.023989025503396988
iteration 277, loss = 0.023961210623383522
iteration 278, loss = 0.020039141178131104
iteration 279, loss = 0.018511241301894188
iteration 280, loss = 0.02228735014796257
iteration 281, loss = 0.023058947175741196
iteration 282, loss = 0.019427644088864326
iteration 283, loss = 0.018572863191366196
iteration 284, loss = 0.019304964691400528
iteration 285, loss = 0.01876162365078926
iteration 286, loss = 0.017523663118481636
iteration 287, loss = 0.0248011015355587
iteration 288, loss = 0.026537327095866203
iteration 289, loss = 0.021108027547597885
iteration 290, loss = 0.024142710492014885
iteration 291, loss = 0.018782563507556915
iteration 292, loss = 0.01746472530066967
iteration 293, loss = 0.0186355821788311
iteration 294, loss = 0.019644031301140785
iteration 295, loss = 0.02482447400689125
iteration 296, loss = 0.01824754849076271
iteration 297, loss = 0.023964352905750275
iteration 298, loss = 0.022245151922106743
iteration 299, loss = 0.02330482192337513
iteration 300, loss = 0.020039431750774384
iteration 1, loss = 0.02454208955168724
iteration 2, loss = 0.02043684758245945
iteration 3, loss = 0.02120126597583294
iteration 4, loss = 0.01978527009487152
iteration 5, loss = 0.018498998135328293
iteration 6, loss = 0.018882691860198975
iteration 7, loss = 0.017991406843066216
iteration 8, loss = 0.01875760778784752
iteration 9, loss = 0.019138438627123833
iteration 10, loss = 0.01895000785589218
iteration 11, loss = 0.019219279289245605
iteration 12, loss = 0.019352681934833527
iteration 13, loss = 0.01934001035988331
iteration 14, loss = 0.022736284881830215
iteration 15, loss = 0.017475083470344543
iteration 16, loss = 0.0181099995970726
iteration 17, loss = 0.018657581880688667
iteration 18, loss = 0.017716258764266968
iteration 19, loss = 0.01840001344680786
iteration 20, loss = 0.01971226930618286
iteration 21, loss = 0.028911219909787178
iteration 22, loss = 0.017676973715424538
iteration 23, loss = 0.01911158859729767
iteration 24, loss = 0.01771383173763752
iteration 25, loss = 0.019622284919023514
iteration 26, loss = 0.018157552927732468
iteration 27, loss = 0.018240490928292274
iteration 28, loss = 0.017689956352114677
iteration 29, loss = 0.030362173914909363
iteration 30, loss = 0.02539273351430893
iteration 31, loss = 0.021931618452072144
iteration 32, loss = 0.019190948456525803
iteration 33, loss = 0.0219874270260334
iteration 34, loss = 0.019088301807641983
iteration 35, loss = 0.030133329331874847
iteration 36, loss = 0.023384662345051765
iteration 37, loss = 0.01915614679455757
iteration 38, loss = 0.020969925448298454
iteration 39, loss = 0.018256712704896927
iteration 40, loss = 0.019162287935614586
iteration 41, loss = 0.022966545075178146
iteration 42, loss = 0.01884401962161064
iteration 43, loss = 0.017377499490976334
iteration 44, loss = 0.017172258347272873
iteration 45, loss = 0.019930033013224602
iteration 46, loss = 0.017971931025385857
iteration 47, loss = 0.017525142058730125
iteration 48, loss = 0.019544469192624092
iteration 49, loss = 0.01715848594903946
iteration 50, loss = 0.016773680225014687
iteration 51, loss = 0.02399095706641674
iteration 52, loss = 0.017693528905510902
iteration 53, loss = 0.017861858010292053
iteration 54, loss = 0.017335224896669388
iteration 55, loss = 0.0191404577344656
iteration 56, loss = 0.017161615192890167
iteration 57, loss = 0.018251104280352592
iteration 58, loss = 0.02081637643277645
iteration 59, loss = 0.025893501937389374
iteration 60, loss = 0.023895366117358208
iteration 61, loss = 0.01747964508831501
iteration 62, loss = 0.01838706061244011
iteration 63, loss = 0.021536491811275482
iteration 64, loss = 0.02398127131164074
iteration 65, loss = 0.01779443770647049
iteration 66, loss = 0.01801823265850544
iteration 67, loss = 0.018799664452672005
iteration 68, loss = 0.01735071651637554
iteration 69, loss = 0.018383899703621864
iteration 70, loss = 0.016516659408807755
iteration 71, loss = 0.018154483288526535
iteration 72, loss = 0.01789545826613903
iteration 73, loss = 0.019400035962462425
iteration 74, loss = 0.016889434307813644
iteration 75, loss = 0.021822424605488777
iteration 76, loss = 0.016168763861060143
iteration 77, loss = 0.02335970848798752
iteration 78, loss = 0.021437836810946465
iteration 79, loss = 0.01751520112156868
iteration 80, loss = 0.017573893070220947
iteration 81, loss = 0.018146060407161713
iteration 82, loss = 0.015898745507001877
iteration 83, loss = 0.022351983934640884
iteration 84, loss = 0.019119329750537872
iteration 85, loss = 0.017611484974622726
iteration 86, loss = 0.01712440885603428
iteration 87, loss = 0.021761953830718994
iteration 88, loss = 0.027096383273601532
iteration 89, loss = 0.022222138941287994
iteration 90, loss = 0.02076047472655773
iteration 91, loss = 0.019672751426696777
iteration 92, loss = 0.01656152866780758
iteration 93, loss = 0.01893380656838417
iteration 94, loss = 0.0180891714990139
iteration 95, loss = 0.019952910020947456
iteration 96, loss = 0.01745346002280712
iteration 97, loss = 0.016154954209923744
iteration 98, loss = 0.016359129920601845
iteration 99, loss = 0.021940933540463448
iteration 100, loss = 0.017018558457493782
iteration 101, loss = 0.016589000821113586
iteration 102, loss = 0.016067147254943848
iteration 103, loss = 0.016171328723430634
iteration 104, loss = 0.021143242716789246
iteration 105, loss = 0.016184404492378235
iteration 106, loss = 0.01894283853471279
iteration 107, loss = 0.016980143263936043
iteration 108, loss = 0.0194912888109684
iteration 109, loss = 0.01583484373986721
iteration 110, loss = 0.01719178445637226
iteration 111, loss = 0.01632312871515751
iteration 112, loss = 0.017257412895560265
iteration 113, loss = 0.015832707285881042
iteration 114, loss = 0.015985583886504173
iteration 115, loss = 0.01648136042058468
iteration 116, loss = 0.017569322139024734
iteration 117, loss = 0.022590525448322296
iteration 118, loss = 0.017412688583135605
iteration 119, loss = 0.01499749906361103
iteration 120, loss = 0.018690314143896103
iteration 121, loss = 0.026142675429582596
iteration 122, loss = 0.018616467714309692
iteration 123, loss = 0.01710500568151474
iteration 124, loss = 0.016329336911439896
iteration 125, loss = 0.016268562525510788
iteration 126, loss = 0.01661405712366104
iteration 127, loss = 0.018328115344047546
iteration 128, loss = 0.019419578835368156
iteration 129, loss = 0.01698513701558113
iteration 130, loss = 0.018314432352781296
iteration 131, loss = 0.02080565132200718
iteration 132, loss = 0.015644365921616554
iteration 133, loss = 0.016423624008893967
iteration 134, loss = 0.01621556654572487
iteration 135, loss = 0.015027938410639763
iteration 136, loss = 0.015679312869906425
iteration 137, loss = 0.016386505216360092
iteration 138, loss = 0.02245628833770752
iteration 139, loss = 0.015848826617002487
iteration 140, loss = 0.02007824182510376
iteration 141, loss = 0.014727105386555195
iteration 142, loss = 0.017523761838674545
iteration 143, loss = 0.017184153199195862
iteration 144, loss = 0.020325303077697754
iteration 145, loss = 0.017709214240312576
iteration 146, loss = 0.020260537043213844
iteration 147, loss = 0.015497587621212006
iteration 148, loss = 0.02003847062587738
iteration 149, loss = 0.016683921217918396
iteration 150, loss = 0.017538346350193024
iteration 151, loss = 0.025903401896357536
iteration 152, loss = 0.01759779080748558
iteration 153, loss = 0.016068506985902786
iteration 154, loss = 0.015092860907316208
iteration 155, loss = 0.015338832512497902
iteration 156, loss = 0.016534406691789627
iteration 157, loss = 0.019301677122712135
iteration 158, loss = 0.017432937398552895
iteration 159, loss = 0.020882990211248398
iteration 160, loss = 0.020862411707639694
iteration 161, loss = 0.01798679120838642
iteration 162, loss = 0.015934737399220467
iteration 163, loss = 0.015987517312169075
iteration 164, loss = 0.015160996466875076
iteration 165, loss = 0.017137078568339348
iteration 166, loss = 0.01914241537451744
iteration 167, loss = 0.016873346641659737
iteration 168, loss = 0.01556464470922947
iteration 169, loss = 0.018147291615605354
iteration 170, loss = 0.015460586175322533
iteration 171, loss = 0.015444298274815083
iteration 172, loss = 0.018725525587797165
iteration 173, loss = 0.026687072589993477
iteration 174, loss = 0.015420308336615562
iteration 175, loss = 0.015366638079285622
iteration 176, loss = 0.01502050831913948
iteration 177, loss = 0.014418384060263634
iteration 178, loss = 0.019301120191812515
iteration 179, loss = 0.014210657216608524
iteration 180, loss = 0.01824699155986309
iteration 181, loss = 0.01907525025308132
iteration 182, loss = 0.016272325068712234
iteration 183, loss = 0.015485431998968124
iteration 184, loss = 0.014476264826953411
iteration 185, loss = 0.015514468774199486
iteration 186, loss = 0.01715252920985222
iteration 187, loss = 0.018182825297117233
iteration 188, loss = 0.014731413684785366
iteration 189, loss = 0.017680950462818146
iteration 190, loss = 0.015372825786471367
iteration 191, loss = 0.018763042986392975
iteration 192, loss = 0.016633974388241768
iteration 193, loss = 0.015115697868168354
iteration 194, loss = 0.018406834453344345
iteration 195, loss = 0.01544273179024458
iteration 196, loss = 0.017384471371769905
iteration 197, loss = 0.015053772367537022
iteration 198, loss = 0.014880906790494919
iteration 199, loss = 0.015378243289887905
iteration 200, loss = 0.02340451255440712
iteration 201, loss = 0.01470901072025299
iteration 202, loss = 0.019739855080842972
iteration 203, loss = 0.01651340164244175
iteration 204, loss = 0.01436033844947815
iteration 205, loss = 0.014395015314221382
iteration 206, loss = 0.021879857406020164
iteration 207, loss = 0.014607550576329231
iteration 208, loss = 0.014395806938409805
iteration 209, loss = 0.018719151616096497
iteration 210, loss = 0.01641206443309784
iteration 211, loss = 0.014248891733586788
iteration 212, loss = 0.014360127039253712
iteration 213, loss = 0.016769859939813614
iteration 214, loss = 0.014443186111748219
iteration 215, loss = 0.014726202934980392
iteration 216, loss = 0.015249737538397312
iteration 217, loss = 0.014202604070305824
iteration 218, loss = 0.014533174224197865
iteration 219, loss = 0.01675408147275448
iteration 220, loss = 0.021396800875663757
iteration 221, loss = 0.01437245775014162
iteration 222, loss = 0.018165817484259605
iteration 223, loss = 0.01580062322318554
iteration 224, loss = 0.01480935513973236
iteration 225, loss = 0.01670040376484394
iteration 226, loss = 0.015033682808279991
iteration 227, loss = 0.013875621370971203
iteration 228, loss = 0.02103576622903347
iteration 229, loss = 0.014531753025949001
iteration 230, loss = 0.017326852306723595
iteration 231, loss = 0.0158682931214571
iteration 232, loss = 0.015604198910295963
iteration 233, loss = 0.014410331845283508
iteration 234, loss = 0.014692169614136219
iteration 235, loss = 0.013321978971362114
iteration 236, loss = 0.018253594636917114
iteration 237, loss = 0.019519101828336716
iteration 238, loss = 0.015365533530712128
iteration 239, loss = 0.014713394455611706
iteration 240, loss = 0.01441819779574871
iteration 241, loss = 0.020655645057559013
iteration 242, loss = 0.01525016501545906
iteration 243, loss = 0.014225118793547153
iteration 244, loss = 0.015910234302282333
iteration 245, loss = 0.016019372269511223
iteration 246, loss = 0.015197728760540485
iteration 247, loss = 0.01489176508039236
iteration 248, loss = 0.014426116831600666
iteration 249, loss = 0.014367476105690002
iteration 250, loss = 0.013677588663995266
iteration 251, loss = 0.014800027944147587
iteration 252, loss = 0.016484035179018974
iteration 253, loss = 0.01632446050643921
iteration 254, loss = 0.01650599017739296
iteration 255, loss = 0.014216486364603043
iteration 256, loss = 0.014547173865139484
iteration 257, loss = 0.01663266122341156
iteration 258, loss = 0.018743155524134636
iteration 259, loss = 0.01714494451880455
iteration 260, loss = 0.015896743163466454
iteration 261, loss = 0.014611061662435532
iteration 262, loss = 0.018372736871242523
iteration 263, loss = 0.02148338407278061
iteration 264, loss = 0.01579137146472931
iteration 265, loss = 0.012968319468200207
iteration 266, loss = 0.017336009070277214
iteration 267, loss = 0.015928834676742554
iteration 268, loss = 0.015108448453247547
iteration 269, loss = 0.015935726463794708
iteration 270, loss = 0.013320129364728928
iteration 271, loss = 0.014667017385363579
iteration 272, loss = 0.01386788859963417
iteration 273, loss = 0.014942843466997147
iteration 274, loss = 0.014054189436137676
iteration 275, loss = 0.01911035180091858
iteration 276, loss = 0.023152802139520645
iteration 277, loss = 0.016141362488269806
iteration 278, loss = 0.014451167546212673
iteration 279, loss = 0.01319186482578516
iteration 280, loss = 0.013621714897453785
iteration 281, loss = 0.013618217781186104
iteration 282, loss = 0.01439871545881033
iteration 283, loss = 0.014253470115363598
iteration 284, loss = 0.01779111474752426
iteration 285, loss = 0.01422927901148796
iteration 286, loss = 0.019243784248828888
iteration 287, loss = 0.01392104011029005
iteration 288, loss = 0.018005920574069023
iteration 289, loss = 0.021014325320720673
iteration 290, loss = 0.014640087261795998
iteration 291, loss = 0.014665392227470875
iteration 292, loss = 0.014528989791870117
iteration 293, loss = 0.013869460672140121
iteration 294, loss = 0.013009101152420044
iteration 295, loss = 0.013094277121126652
iteration 296, loss = 0.013012540526688099
iteration 297, loss = 0.017257286235690117
iteration 298, loss = 0.01565328612923622
iteration 299, loss = 0.014023089781403542
iteration 300, loss = 0.01401090994477272
iteration 1, loss = 0.014159497804939747
iteration 2, loss = 0.014218892902135849
iteration 3, loss = 0.016323545947670937
iteration 4, loss = 0.01592663675546646
iteration 5, loss = 0.013952312991023064
iteration 6, loss = 0.013594741001725197
iteration 7, loss = 0.013986618258059025
iteration 8, loss = 0.013008184731006622
iteration 9, loss = 0.01941833831369877
iteration 10, loss = 0.01643766462802887
iteration 11, loss = 0.013514325022697449
iteration 12, loss = 0.014346206560730934
iteration 13, loss = 0.012701941654086113
iteration 14, loss = 0.013681914657354355
iteration 15, loss = 0.01341884583234787
iteration 16, loss = 0.017273161560297012
iteration 17, loss = 0.01853107661008835
iteration 18, loss = 0.014516693539917469
iteration 19, loss = 0.013495194725692272
iteration 20, loss = 0.013547113165259361
iteration 21, loss = 0.01403921190649271
iteration 22, loss = 0.017002835869789124
iteration 23, loss = 0.012963928282260895
iteration 24, loss = 0.016610195860266685
iteration 25, loss = 0.013112401589751244
iteration 26, loss = 0.017899004742503166
iteration 27, loss = 0.012789648026227951
iteration 28, loss = 0.01665393076837063
iteration 29, loss = 0.014650151133537292
iteration 30, loss = 0.014370686374604702
iteration 31, loss = 0.016530929133296013
iteration 32, loss = 0.01512211561203003
iteration 33, loss = 0.01523472461849451
iteration 34, loss = 0.016033653169870377
iteration 35, loss = 0.017638830468058586
iteration 36, loss = 0.017471713945269585
iteration 37, loss = 0.013485666364431381
iteration 38, loss = 0.013403959572315216
iteration 39, loss = 0.016940684989094734
iteration 40, loss = 0.012930355966091156
iteration 41, loss = 0.013021815568208694
iteration 42, loss = 0.013344460166990757
iteration 43, loss = 0.013266331516206264
iteration 44, loss = 0.014908738434314728
iteration 45, loss = 0.01952490769326687
iteration 46, loss = 0.013354555703699589
iteration 47, loss = 0.016460470855236053
iteration 48, loss = 0.02216327004134655
iteration 49, loss = 0.015389351174235344
iteration 50, loss = 0.013359908945858479
iteration 51, loss = 0.016200479120016098
iteration 52, loss = 0.015898266807198524
iteration 53, loss = 0.012403823435306549
iteration 54, loss = 0.016992468386888504
iteration 55, loss = 0.018915526568889618
iteration 56, loss = 0.013004305772483349
iteration 57, loss = 0.012485237792134285
iteration 58, loss = 0.012944638729095459
iteration 59, loss = 0.012613166123628616
iteration 60, loss = 0.012763182632625103
iteration 61, loss = 0.013055860996246338
iteration 62, loss = 0.01580606773495674
iteration 63, loss = 0.016364332288503647
iteration 64, loss = 0.01406434178352356
iteration 65, loss = 0.013752777129411697
iteration 66, loss = 0.01253997627645731
iteration 67, loss = 0.012865444645285606
iteration 68, loss = 0.013647078536450863
iteration 69, loss = 0.015493566170334816
iteration 70, loss = 0.01202000305056572
iteration 71, loss = 0.0129312789067626
iteration 72, loss = 0.013132690452039242
iteration 73, loss = 0.012678057886660099
iteration 74, loss = 0.012170497328042984
iteration 75, loss = 0.01896621845662594
iteration 76, loss = 0.012331199832260609
iteration 77, loss = 0.012715129181742668
iteration 78, loss = 0.01728067360818386
iteration 79, loss = 0.018454192206263542
iteration 80, loss = 0.015812767669558525
iteration 81, loss = 0.014065010473132133
iteration 82, loss = 0.012514564208686352
iteration 83, loss = 0.0124977370724082
iteration 84, loss = 0.01468643732368946
iteration 85, loss = 0.012236627750098705
iteration 86, loss = 0.012493530288338661
iteration 87, loss = 0.012441481463611126
iteration 88, loss = 0.013400005176663399
iteration 89, loss = 0.012831058353185654
iteration 90, loss = 0.012993020936846733
iteration 91, loss = 0.015208631753921509
iteration 92, loss = 0.02125721611082554
iteration 93, loss = 0.015075276605784893
iteration 94, loss = 0.012924231588840485
iteration 95, loss = 0.012339102104306221
iteration 96, loss = 0.013454032130539417
iteration 97, loss = 0.013096144422888756
iteration 98, loss = 0.015117906033992767
iteration 99, loss = 0.012064303271472454
iteration 100, loss = 0.012528023682534695
iteration 101, loss = 0.015545793808996677
iteration 102, loss = 0.015276563353836536
iteration 103, loss = 0.013611562550067902
iteration 104, loss = 0.013822485692799091
iteration 105, loss = 0.013031644746661186
iteration 106, loss = 0.01517486572265625
iteration 107, loss = 0.012114346958696842
iteration 108, loss = 0.011858556419610977
iteration 109, loss = 0.011976465582847595
iteration 110, loss = 0.01313138660043478
iteration 111, loss = 0.013010887429118156
iteration 112, loss = 0.012882701121270657
iteration 113, loss = 0.018579741939902306
iteration 114, loss = 0.013094591908156872
iteration 115, loss = 0.013417636975646019
iteration 116, loss = 0.014092722907662392
iteration 117, loss = 0.013199562206864357
iteration 118, loss = 0.012914680875837803
iteration 119, loss = 0.014009680598974228
iteration 120, loss = 0.013941124081611633
iteration 121, loss = 0.016640573740005493
iteration 122, loss = 0.012139713391661644
iteration 123, loss = 0.012377586215734482
iteration 124, loss = 0.01275251992046833
iteration 125, loss = 0.012804457917809486
iteration 126, loss = 0.012596447952091694
iteration 127, loss = 0.014979924075305462
iteration 128, loss = 0.012916217558085918
iteration 129, loss = 0.011766412295401096
iteration 130, loss = 0.011310645379126072
iteration 131, loss = 0.014526737853884697
iteration 132, loss = 0.015371613204479218
iteration 133, loss = 0.012488610111176968
iteration 134, loss = 0.012722914107143879
iteration 135, loss = 0.011668900959193707
iteration 136, loss = 0.012396392412483692
iteration 137, loss = 0.01237719226628542
iteration 138, loss = 0.015257119201123714
iteration 139, loss = 0.011895855888724327
iteration 140, loss = 0.012185038067400455
iteration 141, loss = 0.012231817469000816
iteration 142, loss = 0.013723664917051792
iteration 143, loss = 0.011608492583036423
iteration 144, loss = 0.015593242831528187
iteration 145, loss = 0.011847867630422115
iteration 146, loss = 0.017215246334671974
iteration 147, loss = 0.012685163877904415
iteration 148, loss = 0.01687733829021454
iteration 149, loss = 0.011824583634734154
iteration 150, loss = 0.011844638735055923
iteration 151, loss = 0.011468049138784409
iteration 152, loss = 0.013725128024816513
iteration 153, loss = 0.014893713407218456
iteration 154, loss = 0.012391215190291405
iteration 155, loss = 0.012055132538080215
iteration 156, loss = 0.012218618765473366
iteration 157, loss = 0.012205973267555237
iteration 158, loss = 0.01133231446146965
iteration 159, loss = 0.017810219898819923
iteration 160, loss = 0.011540694162249565
iteration 161, loss = 0.012910057790577412
iteration 162, loss = 0.012304456904530525
iteration 163, loss = 0.011997776105999947
iteration 164, loss = 0.011541886255145073
iteration 165, loss = 0.011522146873176098
iteration 166, loss = 0.014027765020728111
iteration 167, loss = 0.012056649662554264
iteration 168, loss = 0.012626901268959045
iteration 169, loss = 0.012057296931743622
iteration 170, loss = 0.012346058152616024
iteration 171, loss = 0.011809543706476688
iteration 172, loss = 0.012501227669417858
iteration 173, loss = 0.011871703900396824
iteration 174, loss = 0.011131778359413147
iteration 175, loss = 0.012431181967258453
iteration 176, loss = 0.01396961696445942
iteration 177, loss = 0.012240591458976269
iteration 178, loss = 0.012607407756149769
iteration 179, loss = 0.012137824669480324
iteration 180, loss = 0.01158880814909935
iteration 181, loss = 0.01200434472411871
iteration 182, loss = 0.011839769780635834
iteration 183, loss = 0.011873424053192139
iteration 184, loss = 0.013631638139486313
iteration 185, loss = 0.011048912070691586
iteration 186, loss = 0.012183217331767082
iteration 187, loss = 0.016588272526860237
iteration 188, loss = 0.017175568267703056
iteration 189, loss = 0.014794816263020039
iteration 190, loss = 0.015977993607521057
iteration 191, loss = 0.011185369454324245
iteration 192, loss = 0.011725286953151226
iteration 193, loss = 0.013844390399754047
iteration 194, loss = 0.011608785018324852
iteration 195, loss = 0.016437796875834465
iteration 196, loss = 0.01458271685987711
iteration 197, loss = 0.011116784997284412
iteration 198, loss = 0.011103222146630287
iteration 199, loss = 0.01083207968622446
iteration 200, loss = 0.010577465407550335
iteration 201, loss = 0.011422412469983101
iteration 202, loss = 0.012221168726682663
iteration 203, loss = 0.011957508511841297
iteration 204, loss = 0.01318548247218132
iteration 205, loss = 0.010840101167559624
iteration 206, loss = 0.01132668275386095
iteration 207, loss = 0.012581408023834229
iteration 208, loss = 0.01551890280097723
iteration 209, loss = 0.011079054325819016
iteration 210, loss = 0.013646579347550869
iteration 211, loss = 0.011211642064154148
iteration 212, loss = 0.012338890694081783
iteration 213, loss = 0.013844081200659275
iteration 214, loss = 0.011470749042928219
iteration 215, loss = 0.012546537443995476
iteration 216, loss = 0.015388933010399342
iteration 217, loss = 0.012078472413122654
iteration 218, loss = 0.01225978322327137
iteration 219, loss = 0.011155807413160801
iteration 220, loss = 0.011406974866986275
iteration 221, loss = 0.012684719637036324
iteration 222, loss = 0.0113239586353302
iteration 223, loss = 0.012467949651181698
iteration 224, loss = 0.011188361793756485
iteration 225, loss = 0.012646867893636227
iteration 226, loss = 0.011714784428477287
iteration 227, loss = 0.01371018961071968
iteration 228, loss = 0.011788155883550644
iteration 229, loss = 0.011472286656498909
iteration 230, loss = 0.012060372158885002
iteration 231, loss = 0.011282159015536308
iteration 232, loss = 0.011578833684325218
iteration 233, loss = 0.011013451963663101
iteration 234, loss = 0.01280280202627182
iteration 235, loss = 0.011178514920175076
iteration 236, loss = 0.010896747000515461
iteration 237, loss = 0.010990772396326065
iteration 238, loss = 0.012789085507392883
iteration 239, loss = 0.011619455181062222
iteration 240, loss = 0.01315437350422144
iteration 241, loss = 0.011957951821386814
iteration 242, loss = 0.01142893172800541
iteration 243, loss = 0.010492234490811825
iteration 244, loss = 0.013584406115114689
iteration 245, loss = 0.010493511334061623
iteration 246, loss = 0.01030450314283371
iteration 247, loss = 0.012503606267273426
iteration 248, loss = 0.0113516291603446
iteration 249, loss = 0.014945371076464653
iteration 250, loss = 0.010893912985920906
iteration 251, loss = 0.01208608690649271
iteration 252, loss = 0.011097453534603119
iteration 253, loss = 0.015743885189294815
iteration 254, loss = 0.010976264253258705
iteration 255, loss = 0.013726692646741867
iteration 256, loss = 0.015545266680419445
iteration 257, loss = 0.011353075504302979
iteration 258, loss = 0.012280709110200405
iteration 259, loss = 0.013571611605584621
iteration 260, loss = 0.01104394905269146
iteration 261, loss = 0.01066572405397892
iteration 262, loss = 0.011450616642832756
iteration 263, loss = 0.01400835532695055
iteration 264, loss = 0.017458612099289894
iteration 265, loss = 0.010661998763680458
iteration 266, loss = 0.01119402889162302
iteration 267, loss = 0.011406298726797104
iteration 268, loss = 0.01358734630048275
iteration 269, loss = 0.010708044283092022
iteration 270, loss = 0.016997795552015305
iteration 271, loss = 0.010691000148653984
iteration 272, loss = 0.011962582357227802
iteration 273, loss = 0.013971802778542042
iteration 274, loss = 0.010744336992502213
iteration 275, loss = 0.010629860684275627
iteration 276, loss = 0.015782976523041725
iteration 277, loss = 0.010518936440348625
iteration 278, loss = 0.010943074710667133
iteration 279, loss = 0.011260413564741611
iteration 280, loss = 0.010815425775945187
iteration 281, loss = 0.010683796368539333
iteration 282, loss = 0.010281678289175034
iteration 283, loss = 0.011184406466782093
iteration 284, loss = 0.011042782105505466
iteration 285, loss = 0.01076580211520195
iteration 286, loss = 0.010535157285630703
iteration 287, loss = 0.016636531800031662
iteration 288, loss = 0.011515343561768532
iteration 289, loss = 0.01566968485713005
iteration 290, loss = 0.010921966284513474
iteration 291, loss = 0.010802358388900757
iteration 292, loss = 0.013248976320028305
iteration 293, loss = 0.010966448113322258
iteration 294, loss = 0.012004314921796322
iteration 295, loss = 0.011634552851319313
iteration 296, loss = 0.0110838133841753
iteration 297, loss = 0.010257035493850708
iteration 298, loss = 0.014219343662261963
iteration 299, loss = 0.012696600519120693
iteration 300, loss = 0.01050981692969799
iteration 1, loss = 0.010288016870617867
iteration 2, loss = 0.010183651931583881
iteration 3, loss = 0.01113164983689785
iteration 4, loss = 0.009974098764359951
iteration 5, loss = 0.016065435484051704
iteration 6, loss = 0.010485499165952206
iteration 7, loss = 0.010717041790485382
iteration 8, loss = 0.010823298245668411
iteration 9, loss = 0.009831166826188564
iteration 10, loss = 0.009911523200571537
iteration 11, loss = 0.010111899115145206
iteration 12, loss = 0.009748192504048347
iteration 13, loss = 0.013209295459091663
iteration 14, loss = 0.01330640073865652
iteration 15, loss = 0.010645067319273949
iteration 16, loss = 0.010581906884908676
iteration 17, loss = 0.014764527790248394
iteration 18, loss = 0.010588976554572582
iteration 19, loss = 0.013445749878883362
iteration 20, loss = 0.010828335769474506
iteration 21, loss = 0.010677237063646317
iteration 22, loss = 0.010568497702479362
iteration 23, loss = 0.010008054785430431
iteration 24, loss = 0.010334327816963196
iteration 25, loss = 0.01838204264640808
iteration 26, loss = 0.010528210550546646
iteration 27, loss = 0.013624902814626694
iteration 28, loss = 0.010872322134673595
iteration 29, loss = 0.011207313276827335
iteration 30, loss = 0.010367761366069317
iteration 31, loss = 0.014483995735645294
iteration 32, loss = 0.012749411165714264
iteration 33, loss = 0.011145844124257565
iteration 34, loss = 0.01142894010990858
iteration 35, loss = 0.009957009926438332
iteration 36, loss = 0.010469024069607258
iteration 37, loss = 0.009828935377299786
iteration 38, loss = 0.01194811426103115
iteration 39, loss = 0.010052008554339409
iteration 40, loss = 0.010612145997583866
iteration 41, loss = 0.01064698863774538
iteration 42, loss = 0.009898302145302296
iteration 43, loss = 0.01340241264551878
iteration 44, loss = 0.013675255700945854
iteration 45, loss = 0.009858357720077038
iteration 46, loss = 0.01101971510797739
iteration 47, loss = 0.009944338351488113
iteration 48, loss = 0.01092148944735527
iteration 49, loss = 0.010106117464601994
iteration 50, loss = 0.015195600688457489
iteration 51, loss = 0.010223612189292908
iteration 52, loss = 0.011608942411839962
iteration 53, loss = 0.010967696085572243
iteration 54, loss = 0.010147483088076115
iteration 55, loss = 0.010626562871038914
iteration 56, loss = 0.01168493926525116
iteration 57, loss = 0.012082444503903389
iteration 58, loss = 0.010956211015582085
iteration 59, loss = 0.01369695458561182
iteration 60, loss = 0.009845379739999771
iteration 61, loss = 0.011451294645667076
iteration 62, loss = 0.009872044436633587
iteration 63, loss = 0.010118664242327213
iteration 64, loss = 0.010791808366775513
iteration 65, loss = 0.010803033597767353
iteration 66, loss = 0.010728646069765091
iteration 67, loss = 0.010078261606395245
iteration 68, loss = 0.00930779054760933
iteration 69, loss = 0.010744431056082249
iteration 70, loss = 0.012132805772125721
iteration 71, loss = 0.012769038788974285
iteration 72, loss = 0.010211218148469925
iteration 73, loss = 0.00946927722543478
iteration 74, loss = 0.010171147994697094
iteration 75, loss = 0.011012911796569824
iteration 76, loss = 0.010246784426271915
iteration 77, loss = 0.01247937697917223
iteration 78, loss = 0.009991755709052086
iteration 79, loss = 0.010623829439282417
iteration 80, loss = 0.01185919065028429
iteration 81, loss = 0.014345966279506683
iteration 82, loss = 0.009882616810500622
iteration 83, loss = 0.011330381967127323
iteration 84, loss = 0.012972094118595123
iteration 85, loss = 0.013464448973536491
iteration 86, loss = 0.011944902129471302
iteration 87, loss = 0.012061821296811104
iteration 88, loss = 0.01026022620499134
iteration 89, loss = 0.014470878057181835
iteration 90, loss = 0.00997012760490179
iteration 91, loss = 0.010587835684418678
iteration 92, loss = 0.00914935301989317
iteration 93, loss = 0.00993910152465105
iteration 94, loss = 0.009426672011613846
iteration 95, loss = 0.009948746301233768
iteration 96, loss = 0.009559202939271927
iteration 97, loss = 0.009625702165067196
iteration 98, loss = 0.012593880295753479
iteration 99, loss = 0.012262936681509018
iteration 100, loss = 0.011528264731168747
iteration 101, loss = 0.010209823958575726
iteration 102, loss = 0.010221838019788265
iteration 103, loss = 0.009875120595097542
iteration 104, loss = 0.009221657179296017
iteration 105, loss = 0.009745024144649506
iteration 106, loss = 0.012366203591227531
iteration 107, loss = 0.010419690981507301
iteration 108, loss = 0.010578783228993416
iteration 109, loss = 0.010762570425868034
iteration 110, loss = 0.0130561962723732
iteration 111, loss = 0.010859391652047634
iteration 112, loss = 0.0098577244207263
iteration 113, loss = 0.009360838681459427
iteration 114, loss = 0.009521239437162876
iteration 115, loss = 0.013750615529716015
iteration 116, loss = 0.00993749313056469
iteration 117, loss = 0.010038042441010475
iteration 118, loss = 0.010548749007284641
iteration 119, loss = 0.01017039641737938
iteration 120, loss = 0.010319650173187256
iteration 121, loss = 0.009114488027989864
iteration 122, loss = 0.009602648206055164
iteration 123, loss = 0.010628463700413704
iteration 124, loss = 0.011111878789961338
iteration 125, loss = 0.0101555734872818
iteration 126, loss = 0.010269135236740112
iteration 127, loss = 0.012656932696700096
iteration 128, loss = 0.010134762153029442
iteration 129, loss = 0.011510835960507393
iteration 130, loss = 0.009349492378532887
iteration 131, loss = 0.010098886676132679
iteration 132, loss = 0.014265941455960274
iteration 133, loss = 0.0104864202439785
iteration 134, loss = 0.011241486296057701
iteration 135, loss = 0.009915640577673912
iteration 136, loss = 0.009993677958846092
iteration 137, loss = 0.01146463118493557
iteration 138, loss = 0.0102628692984581
iteration 139, loss = 0.010217186063528061
iteration 140, loss = 0.009341859258711338
iteration 141, loss = 0.009777319617569447
iteration 142, loss = 0.009461681358516216
iteration 143, loss = 0.012397026643157005
iteration 144, loss = 0.012092174030840397
iteration 145, loss = 0.009419127367436886
iteration 146, loss = 0.01054397039115429
iteration 147, loss = 0.009833859279751778
iteration 148, loss = 0.012328321114182472
iteration 149, loss = 0.009123841300606728
iteration 150, loss = 0.009701973758637905
iteration 151, loss = 0.009362808428704739
iteration 152, loss = 0.010434209369122982
iteration 153, loss = 0.00946865789592266
iteration 154, loss = 0.009486990980803967
iteration 155, loss = 0.010159103199839592
iteration 156, loss = 0.00918596051633358
iteration 157, loss = 0.009436039254069328
iteration 158, loss = 0.008789042942225933
iteration 159, loss = 0.009676318615674973
iteration 160, loss = 0.009686237201094627
iteration 161, loss = 0.013030433095991611
iteration 162, loss = 0.009694762527942657
iteration 163, loss = 0.009998714551329613
iteration 164, loss = 0.009493371471762657
iteration 165, loss = 0.00929173082113266
iteration 166, loss = 0.009823800064623356
iteration 167, loss = 0.01196407899260521
iteration 168, loss = 0.008980619721114635
iteration 169, loss = 0.009006115607917309
iteration 170, loss = 0.009168638847768307
iteration 171, loss = 0.013585388660430908
iteration 172, loss = 0.011710860766470432
iteration 173, loss = 0.00890222005546093
iteration 174, loss = 0.009475710801780224
iteration 175, loss = 0.009807480499148369
iteration 176, loss = 0.011071351356804371
iteration 177, loss = 0.009045234881341457
iteration 178, loss = 0.011940459720790386
iteration 179, loss = 0.008826663717627525
iteration 180, loss = 0.008995570242404938
iteration 181, loss = 0.010093539953231812
iteration 182, loss = 0.009565875865519047
iteration 183, loss = 0.009447718970477581
iteration 184, loss = 0.00900601502507925
iteration 185, loss = 0.011244738474488258
iteration 186, loss = 0.010178644210100174
iteration 187, loss = 0.009446223266422749
iteration 188, loss = 0.00897604413330555
iteration 189, loss = 0.008695625700056553
iteration 190, loss = 0.008933136239647865
iteration 191, loss = 0.008812228217720985
iteration 192, loss = 0.009258189238607883
iteration 193, loss = 0.008576962165534496
iteration 194, loss = 0.0118874441832304
iteration 195, loss = 0.009920239448547363
iteration 196, loss = 0.014935179613530636
iteration 197, loss = 0.011107534170150757
iteration 198, loss = 0.008588314987719059
iteration 199, loss = 0.009069542400538921
iteration 200, loss = 0.009087287820875645
iteration 201, loss = 0.008999166078865528
iteration 202, loss = 0.008716518059372902
iteration 203, loss = 0.009454100392758846
iteration 204, loss = 0.009514261968433857
iteration 205, loss = 0.008997727185487747
iteration 206, loss = 0.010104285553097725
iteration 207, loss = 0.011285760439932346
iteration 208, loss = 0.010115607641637325
iteration 209, loss = 0.01055728830397129
iteration 210, loss = 0.009318774566054344
iteration 211, loss = 0.008909626863896847
iteration 212, loss = 0.008876107633113861
iteration 213, loss = 0.009454152546823025
iteration 214, loss = 0.008698184974491596
iteration 215, loss = 0.009948669001460075
iteration 216, loss = 0.009008536115288734
iteration 217, loss = 0.008959682658314705
iteration 218, loss = 0.010842902585864067
iteration 219, loss = 0.008903038688004017
iteration 220, loss = 0.01179917249828577
iteration 221, loss = 0.009776603430509567
iteration 222, loss = 0.008986886590719223
iteration 223, loss = 0.013127614744007587
iteration 224, loss = 0.010850324295461178
iteration 225, loss = 0.008736036717891693
iteration 226, loss = 0.009119106456637383
iteration 227, loss = 0.009061567485332489
iteration 228, loss = 0.008863571099936962
iteration 229, loss = 0.008669877424836159
iteration 230, loss = 0.01044964138418436
iteration 231, loss = 0.008841906674206257
iteration 232, loss = 0.008875980041921139
iteration 233, loss = 0.008872484788298607
iteration 234, loss = 0.01012788899242878
iteration 235, loss = 0.013149523176252842
iteration 236, loss = 0.009004535153508186
iteration 237, loss = 0.012155307456851006
iteration 238, loss = 0.010333201847970486
iteration 239, loss = 0.0095148840919137
iteration 240, loss = 0.0110936239361763
iteration 241, loss = 0.008903538808226585
iteration 242, loss = 0.00903226900845766
iteration 243, loss = 0.008896333165466785
iteration 244, loss = 0.011254324577748775
iteration 245, loss = 0.008783923462033272
iteration 246, loss = 0.011685655452311039
iteration 247, loss = 0.009942040778696537
iteration 248, loss = 0.008632275275886059
iteration 249, loss = 0.015930425375699997
iteration 250, loss = 0.012465212494134903
iteration 251, loss = 0.009041814133524895
iteration 252, loss = 0.00960236694663763
iteration 253, loss = 0.009021067060530186
iteration 254, loss = 0.010788134299218655
iteration 255, loss = 0.010119116865098476
iteration 256, loss = 0.009149814955890179
iteration 257, loss = 0.013744286261498928
iteration 258, loss = 0.008837553672492504
iteration 259, loss = 0.008529871702194214
iteration 260, loss = 0.009408882819116116
iteration 261, loss = 0.009141474030911922
iteration 262, loss = 0.008239501155912876
iteration 263, loss = 0.011995285749435425
iteration 264, loss = 0.008843985386192799
iteration 265, loss = 0.008589470759034157
iteration 266, loss = 0.008648063987493515
iteration 267, loss = 0.010465530678629875
iteration 268, loss = 0.0109154237434268
iteration 269, loss = 0.010858643800020218
iteration 270, loss = 0.008391696028411388
iteration 271, loss = 0.010406490415334702
iteration 272, loss = 0.008410836569964886
iteration 273, loss = 0.01467690709978342
iteration 274, loss = 0.009030777029693127
iteration 275, loss = 0.008140125311911106
iteration 276, loss = 0.008019804023206234
iteration 277, loss = 0.009037679061293602
iteration 278, loss = 0.009710010141134262
iteration 279, loss = 0.01043055858463049
iteration 280, loss = 0.008476142771542072
iteration 281, loss = 0.008395225740969181
iteration 282, loss = 0.008510472252964973
iteration 283, loss = 0.010024811141192913
iteration 284, loss = 0.010156967677175999
iteration 285, loss = 0.00887778028845787
iteration 286, loss = 0.011324596591293812
iteration 287, loss = 0.008117660880088806
iteration 288, loss = 0.008963440544903278
iteration 289, loss = 0.00825570710003376
iteration 290, loss = 0.008400991559028625
iteration 291, loss = 0.008808217011392117
iteration 292, loss = 0.010484340600669384
iteration 293, loss = 0.008306278847157955
iteration 294, loss = 0.014737136662006378
iteration 295, loss = 0.009714405052363873
iteration 296, loss = 0.00915614701807499
iteration 297, loss = 0.008617488667368889
iteration 298, loss = 0.01244441606104374
iteration 299, loss = 0.009897639974951744
iteration 300, loss = 0.00889501627534628
iteration 1, loss = 0.00904431939125061
iteration 2, loss = 0.009773077443242073
iteration 3, loss = 0.008012920618057251
iteration 4, loss = 0.010349607095122337
iteration 5, loss = 0.008075736463069916
iteration 6, loss = 0.01239981222897768
iteration 7, loss = 0.014429572969675064
iteration 8, loss = 0.008244425989687443
iteration 9, loss = 0.00969751924276352
iteration 10, loss = 0.013185792602598667
iteration 11, loss = 0.0091089541092515
iteration 12, loss = 0.011296695098280907
iteration 13, loss = 0.008497899398207664
iteration 14, loss = 0.009283095598220825
iteration 15, loss = 0.008402793668210506
iteration 16, loss = 0.010424533858895302
iteration 17, loss = 0.008878467604517937
iteration 18, loss = 0.008656926453113556
iteration 19, loss = 0.009819471277296543
iteration 20, loss = 0.007954467087984085
iteration 21, loss = 0.009954292327165604
iteration 22, loss = 0.008121282793581486
iteration 23, loss = 0.008135061711072922
iteration 24, loss = 0.008213968016207218
iteration 25, loss = 0.00817124918103218
iteration 26, loss = 0.007909252308309078
iteration 27, loss = 0.008520014584064484
iteration 28, loss = 0.008338140323758125
iteration 29, loss = 0.008337867446243763
iteration 30, loss = 0.009035259485244751
iteration 31, loss = 0.008080147206783295
iteration 32, loss = 0.008074955083429813
iteration 33, loss = 0.007853768765926361
iteration 34, loss = 0.010064411908388138
iteration 35, loss = 0.008213737048208714
iteration 36, loss = 0.008362852036952972
iteration 37, loss = 0.008830007165670395
iteration 38, loss = 0.0082967858761549
iteration 39, loss = 0.007991491816937923
iteration 40, loss = 0.008945868350565434
iteration 41, loss = 0.008541476912796497
iteration 42, loss = 0.008527755737304688
iteration 43, loss = 0.008367574773728848
iteration 44, loss = 0.012561297975480556
iteration 45, loss = 0.008555269800126553
iteration 46, loss = 0.007993180304765701
iteration 47, loss = 0.008406620472669601
iteration 48, loss = 0.00784087460488081
iteration 49, loss = 0.00882688444107771
iteration 50, loss = 0.007652407046407461
iteration 51, loss = 0.007703098002821207
iteration 52, loss = 0.007689335383474827
iteration 53, loss = 0.008355497382581234
iteration 54, loss = 0.01192525215446949
iteration 55, loss = 0.008052172139286995
iteration 56, loss = 0.007782721426337957
iteration 57, loss = 0.010261870920658112
iteration 58, loss = 0.008433615788817406
iteration 59, loss = 0.007895582355558872
iteration 60, loss = 0.011956336908042431
iteration 61, loss = 0.008056214079260826
iteration 62, loss = 0.011634654365479946
iteration 63, loss = 0.008167251944541931
iteration 64, loss = 0.0079111959785223
iteration 65, loss = 0.011793100275099277
iteration 66, loss = 0.008338112384080887
iteration 67, loss = 0.012013053521513939
iteration 68, loss = 0.009626077488064766
iteration 69, loss = 0.008459496311843395
iteration 70, loss = 0.007758248597383499
iteration 71, loss = 0.00995375495404005
iteration 72, loss = 0.009791399352252483
iteration 73, loss = 0.010748217813670635
iteration 74, loss = 0.01044580526649952
iteration 75, loss = 0.0086353225633502
iteration 76, loss = 0.008055713027715683
iteration 77, loss = 0.010102232918143272
iteration 78, loss = 0.008480166085064411
iteration 79, loss = 0.007974906824529171
iteration 80, loss = 0.008242540061473846
iteration 81, loss = 0.008254211395978928
iteration 82, loss = 0.013097709976136684
iteration 83, loss = 0.007570849731564522
iteration 84, loss = 0.008044136688113213
iteration 85, loss = 0.008338699117302895
iteration 86, loss = 0.008146935142576694
iteration 87, loss = 0.009628950618207455
iteration 88, loss = 0.007441766560077667
iteration 89, loss = 0.010152780450880527
iteration 90, loss = 0.00861345324665308
iteration 91, loss = 0.009072909131646156
iteration 92, loss = 0.007830790244042873
iteration 93, loss = 0.007996177300810814
iteration 94, loss = 0.013833433389663696
iteration 95, loss = 0.011907469481229782
iteration 96, loss = 0.007843269035220146
iteration 97, loss = 0.00783549528568983
iteration 98, loss = 0.007813237607479095
iteration 99, loss = 0.007571977563202381
iteration 100, loss = 0.011583113111555576
iteration 101, loss = 0.009086010046303272
iteration 102, loss = 0.009576822631061077
iteration 103, loss = 0.008115652948617935
iteration 104, loss = 0.00809105858206749
iteration 105, loss = 0.008603489026427269
iteration 106, loss = 0.008724458515644073
iteration 107, loss = 0.008259881287813187
iteration 108, loss = 0.009004106745123863
iteration 109, loss = 0.008929209783673286
iteration 110, loss = 0.008030040189623833
iteration 111, loss = 0.007844964973628521
iteration 112, loss = 0.008351095952093601
iteration 113, loss = 0.007621032651513815
iteration 114, loss = 0.00788396317511797
iteration 115, loss = 0.008375545963644981
iteration 116, loss = 0.008443309925496578
iteration 117, loss = 0.009588398039340973
iteration 118, loss = 0.007424299605190754
iteration 119, loss = 0.007857310585677624
iteration 120, loss = 0.008252138271927834
iteration 121, loss = 0.007470446638762951
iteration 122, loss = 0.009554605931043625
iteration 123, loss = 0.007235522381961346
iteration 124, loss = 0.0080984802916646
iteration 125, loss = 0.011725747026503086
iteration 126, loss = 0.007761592045426369
iteration 127, loss = 0.008267287164926529
iteration 128, loss = 0.007595990784466267
iteration 129, loss = 0.008577985689043999
iteration 130, loss = 0.008449375629425049
iteration 131, loss = 0.0077621955424547195
iteration 132, loss = 0.007812073454260826
iteration 133, loss = 0.007627665996551514
iteration 134, loss = 0.007708278484642506
iteration 135, loss = 0.0074410224333405495
iteration 136, loss = 0.00805746577680111
iteration 137, loss = 0.010673150420188904
iteration 138, loss = 0.007909318432211876
iteration 139, loss = 0.007371155079454184
iteration 140, loss = 0.009278759360313416
iteration 141, loss = 0.007500580977648497
iteration 142, loss = 0.009737825952470303
iteration 143, loss = 0.008455404080450535
iteration 144, loss = 0.008864670060575008
iteration 145, loss = 0.0076076677069067955
iteration 146, loss = 0.010150706395506859
iteration 147, loss = 0.00994307454675436
iteration 148, loss = 0.00784104224294424
iteration 149, loss = 0.007227755151689053
iteration 150, loss = 0.00743830855935812
iteration 151, loss = 0.012009210884571075
iteration 152, loss = 0.012848302721977234
iteration 153, loss = 0.008271081373095512
iteration 154, loss = 0.010427919216454029
iteration 155, loss = 0.008283935487270355
iteration 156, loss = 0.007560512982308865
iteration 157, loss = 0.00843736156821251
iteration 158, loss = 0.009508839808404446
iteration 159, loss = 0.00792510062456131
iteration 160, loss = 0.007287478074431419
iteration 161, loss = 0.008338166400790215
iteration 162, loss = 0.009562288410961628
iteration 163, loss = 0.007296128198504448
iteration 164, loss = 0.007302718237042427
iteration 165, loss = 0.007541133556514978
iteration 166, loss = 0.01091928593814373
iteration 167, loss = 0.009619043208658695
iteration 168, loss = 0.010044767521321774
iteration 169, loss = 0.008564121089875698
iteration 170, loss = 0.007434147875756025
iteration 171, loss = 0.0074906740337610245
iteration 172, loss = 0.00784025713801384
iteration 173, loss = 0.008128494955599308
iteration 174, loss = 0.007438415661454201
iteration 175, loss = 0.011172322556376457
iteration 176, loss = 0.00867043063044548
iteration 177, loss = 0.009034540504217148
iteration 178, loss = 0.008105376735329628
iteration 179, loss = 0.00786678772419691
iteration 180, loss = 0.008305305615067482
iteration 181, loss = 0.0075759282335639
iteration 182, loss = 0.007679321337491274
iteration 183, loss = 0.008913282305002213
iteration 184, loss = 0.007295455317944288
iteration 185, loss = 0.009405285120010376
iteration 186, loss = 0.007363703567534685
iteration 187, loss = 0.0077105253003537655
iteration 188, loss = 0.007658290211111307
iteration 189, loss = 0.009680962190032005
iteration 190, loss = 0.009660737589001656
iteration 191, loss = 0.007322454359382391
iteration 192, loss = 0.007271988317370415
iteration 193, loss = 0.0072406986728310585
iteration 194, loss = 0.0073851714842021465
iteration 195, loss = 0.007547980640083551
iteration 196, loss = 0.00866664294153452
iteration 197, loss = 0.007620626594871283
iteration 198, loss = 0.007573691196739674
iteration 199, loss = 0.01198563352227211
iteration 200, loss = 0.006978720426559448
iteration 201, loss = 0.00705247838050127
iteration 202, loss = 0.007975852116942406
iteration 203, loss = 0.007987589575350285
iteration 204, loss = 0.00923079438507557
iteration 205, loss = 0.007247507106512785
iteration 206, loss = 0.007129764184355736
iteration 207, loss = 0.0075698122382164
iteration 208, loss = 0.008624170906841755
iteration 209, loss = 0.007159145548939705
iteration 210, loss = 0.007588209584355354
iteration 211, loss = 0.009882892481982708
iteration 212, loss = 0.00794883444905281
iteration 213, loss = 0.009648356586694717
iteration 214, loss = 0.007506126072257757
iteration 215, loss = 0.007450152188539505
iteration 216, loss = 0.007020698860287666
iteration 217, loss = 0.007327904459089041
iteration 218, loss = 0.00791090726852417
iteration 219, loss = 0.007510821335017681
iteration 220, loss = 0.009350044652819633
iteration 221, loss = 0.0071120914071798325
iteration 222, loss = 0.008552048355340958
iteration 223, loss = 0.007202621549367905
iteration 224, loss = 0.009136303327977657
iteration 225, loss = 0.0075753782875835896
iteration 226, loss = 0.00964012835174799
iteration 227, loss = 0.007618701085448265
iteration 228, loss = 0.007661140523850918
iteration 229, loss = 0.007302495650947094
iteration 230, loss = 0.00713510625064373
iteration 231, loss = 0.00826988648623228
iteration 232, loss = 0.0069612557999789715
iteration 233, loss = 0.008815305307507515
iteration 234, loss = 0.007013744208961725
iteration 235, loss = 0.007723167072981596
iteration 236, loss = 0.007376963272690773
iteration 237, loss = 0.007900814525783062
iteration 238, loss = 0.0070312353782355785
iteration 239, loss = 0.007335953414440155
iteration 240, loss = 0.007589286658912897
iteration 241, loss = 0.007329972926527262
iteration 242, loss = 0.007759725674986839
iteration 243, loss = 0.00756783876568079
iteration 244, loss = 0.008900982327759266
iteration 245, loss = 0.006894027814269066
iteration 246, loss = 0.008843891322612762
iteration 247, loss = 0.006918235216289759
iteration 248, loss = 0.006933797616511583
iteration 249, loss = 0.01042983029037714
iteration 250, loss = 0.007321818731725216
iteration 251, loss = 0.00769999111071229
iteration 252, loss = 0.0071872528642416
iteration 253, loss = 0.007712980732321739
iteration 254, loss = 0.00702664302662015
iteration 255, loss = 0.007802226580679417
iteration 256, loss = 0.008601090870797634
iteration 257, loss = 0.008991697803139687
iteration 258, loss = 0.009309018030762672
iteration 259, loss = 0.006748666986823082
iteration 260, loss = 0.010860907845199108
iteration 261, loss = 0.007222882471978664
iteration 262, loss = 0.010341652669012547
iteration 263, loss = 0.007161304354667664
iteration 264, loss = 0.007221808657050133
iteration 265, loss = 0.009069887921214104
iteration 266, loss = 0.0069303750060498714
iteration 267, loss = 0.008096432313323021
iteration 268, loss = 0.007811984047293663
iteration 269, loss = 0.00841319840401411
iteration 270, loss = 0.007567875552922487
iteration 271, loss = 0.0068828035145998
iteration 272, loss = 0.008817382156848907
iteration 273, loss = 0.009106496348977089
iteration 274, loss = 0.010193299502134323
iteration 275, loss = 0.008417290635406971
iteration 276, loss = 0.006891073193401098
iteration 277, loss = 0.0075948238372802734
iteration 278, loss = 0.00737389363348484
iteration 279, loss = 0.007407122291624546
iteration 280, loss = 0.006999982986599207
iteration 281, loss = 0.006961233913898468
iteration 282, loss = 0.006670357659459114
iteration 283, loss = 0.007096261717379093
iteration 284, loss = 0.008460366167128086
iteration 285, loss = 0.006620704662054777
iteration 286, loss = 0.0070397029630839825
iteration 287, loss = 0.006884528324007988
iteration 288, loss = 0.007365370634943247
iteration 289, loss = 0.007705885451287031
iteration 290, loss = 0.006819504778832197
iteration 291, loss = 0.008633294142782688
iteration 292, loss = 0.007277662865817547
iteration 293, loss = 0.006665789522230625
iteration 294, loss = 0.01005474291741848
iteration 295, loss = 0.006465887650847435
iteration 296, loss = 0.00816561933606863
iteration 297, loss = 0.0065246992744505405
iteration 298, loss = 0.008756213821470737
iteration 299, loss = 0.0066982158459723
iteration 300, loss = 0.006440261844545603
iteration 1, loss = 0.007174146361649036
iteration 2, loss = 0.008421852253377438
iteration 3, loss = 0.0070825605653226376
iteration 4, loss = 0.010847795754671097
iteration 5, loss = 0.007283937186002731
iteration 6, loss = 0.011237266473472118
iteration 7, loss = 0.00651077926158905
iteration 8, loss = 0.006559465080499649
iteration 9, loss = 0.00796425063163042
iteration 10, loss = 0.007140823174268007
iteration 11, loss = 0.0072391158901154995
iteration 12, loss = 0.012057434767484665
iteration 13, loss = 0.007701012305915356
iteration 14, loss = 0.007255312521010637
iteration 15, loss = 0.007025973405689001
iteration 16, loss = 0.006531593389809132
iteration 17, loss = 0.0065804594196379185
iteration 18, loss = 0.006916409824043512
iteration 19, loss = 0.007292117923498154
iteration 20, loss = 0.006991556845605373
iteration 21, loss = 0.007012989837676287
iteration 22, loss = 0.006906058173626661
iteration 23, loss = 0.007867923006415367
iteration 24, loss = 0.007300663273781538
iteration 25, loss = 0.0071008699014782906
iteration 26, loss = 0.007314980495721102
iteration 27, loss = 0.007217720616608858
iteration 28, loss = 0.006925129797309637
iteration 29, loss = 0.010295544750988483
iteration 30, loss = 0.006912766955792904
iteration 31, loss = 0.008859175257384777
iteration 32, loss = 0.007630729582160711
iteration 33, loss = 0.0076829432509839535
iteration 34, loss = 0.007028625346720219
iteration 35, loss = 0.007177203428000212
iteration 36, loss = 0.007439864333719015
iteration 37, loss = 0.006713980343192816
iteration 38, loss = 0.0069880373775959015
iteration 39, loss = 0.007519500330090523
iteration 40, loss = 0.006673344410955906
iteration 41, loss = 0.010563845746219158
iteration 42, loss = 0.007113204337656498
iteration 43, loss = 0.008005763404071331
iteration 44, loss = 0.006560605484992266
iteration 45, loss = 0.007884720340371132
iteration 46, loss = 0.00845629908144474
iteration 47, loss = 0.008449233137071133
iteration 48, loss = 0.00785368774086237
iteration 49, loss = 0.007479351479560137
iteration 50, loss = 0.008365311659872532
iteration 51, loss = 0.009048741310834885
iteration 52, loss = 0.00782649964094162
iteration 53, loss = 0.006904172245413065
iteration 54, loss = 0.010301025584340096
iteration 55, loss = 0.0065322816371917725
iteration 56, loss = 0.006878009531646967
iteration 57, loss = 0.007012262940406799
iteration 58, loss = 0.006311929319053888
iteration 59, loss = 0.0074407681822776794
iteration 60, loss = 0.008506775833666325
iteration 61, loss = 0.0084614222869277
iteration 62, loss = 0.006865769624710083
iteration 63, loss = 0.007597097661346197
iteration 64, loss = 0.009287633001804352
iteration 65, loss = 0.007466224953532219
iteration 66, loss = 0.006670026574283838
iteration 67, loss = 0.01049007661640644
iteration 68, loss = 0.008413839153945446
iteration 69, loss = 0.00890249852091074
iteration 70, loss = 0.00689263641834259
iteration 71, loss = 0.007238234393298626
iteration 72, loss = 0.0066896164789795876
iteration 73, loss = 0.007102833595126867
iteration 74, loss = 0.00654382910579443
iteration 75, loss = 0.006777795031666756
iteration 76, loss = 0.00946036260575056
iteration 77, loss = 0.006914752069860697
iteration 78, loss = 0.0068894424475729465
iteration 79, loss = 0.006574730388820171
iteration 80, loss = 0.010418417863547802
iteration 81, loss = 0.010241215117275715
iteration 82, loss = 0.006767491344362497
iteration 83, loss = 0.010332617908716202
iteration 84, loss = 0.006849265191704035
iteration 85, loss = 0.007435259409248829
iteration 86, loss = 0.008625593967735767
iteration 87, loss = 0.007523845881223679
iteration 88, loss = 0.008115526288747787
iteration 89, loss = 0.0063364095985889435
iteration 90, loss = 0.0066976165398955345
iteration 91, loss = 0.006923922803252935
iteration 92, loss = 0.006899503991007805
iteration 93, loss = 0.006725247949361801
iteration 94, loss = 0.006347314454615116
iteration 95, loss = 0.006815886124968529
iteration 96, loss = 0.007043765392154455
iteration 97, loss = 0.007043157238513231
iteration 98, loss = 0.008319522254168987
iteration 99, loss = 0.006697299890220165
iteration 100, loss = 0.00740032410249114
iteration 101, loss = 0.006958942860364914
iteration 102, loss = 0.006988270208239555
iteration 103, loss = 0.006986026652157307
iteration 104, loss = 0.007841724902391434
iteration 105, loss = 0.0064282286912202835
iteration 106, loss = 0.0066550979390740395
iteration 107, loss = 0.006845744792371988
iteration 108, loss = 0.007039710879325867
iteration 109, loss = 0.007873293943703175
iteration 110, loss = 0.007216811645776033
iteration 111, loss = 0.007709243800491095
iteration 112, loss = 0.007138180546462536
iteration 113, loss = 0.006715408060699701
iteration 114, loss = 0.007664228789508343
iteration 115, loss = 0.006813344080001116
iteration 116, loss = 0.010788287036120892
iteration 117, loss = 0.008813049644231796
iteration 118, loss = 0.0076791890896856785
iteration 119, loss = 0.007201936561614275
iteration 120, loss = 0.007435434032231569
iteration 121, loss = 0.010157178156077862
iteration 122, loss = 0.009057317860424519
iteration 123, loss = 0.006521821022033691
iteration 124, loss = 0.0071627674624323845
iteration 125, loss = 0.006854930426925421
iteration 126, loss = 0.009019782766699791
iteration 127, loss = 0.006918889936059713
iteration 128, loss = 0.006585562601685524
iteration 129, loss = 0.007039838936179876
iteration 130, loss = 0.007069387473165989
iteration 131, loss = 0.009520022198557854
iteration 132, loss = 0.006518566515296698
iteration 133, loss = 0.008042233996093273
iteration 134, loss = 0.007756899110972881
iteration 135, loss = 0.008376890793442726
iteration 136, loss = 0.008185748010873795
iteration 137, loss = 0.007245187647640705
iteration 138, loss = 0.007272301707416773
iteration 139, loss = 0.007221600506454706
iteration 140, loss = 0.006840787827968597
iteration 141, loss = 0.007383014541119337
iteration 142, loss = 0.007203891407698393
iteration 143, loss = 0.008730901405215263
iteration 144, loss = 0.010365582071244717
iteration 145, loss = 0.00673947436735034
iteration 146, loss = 0.007247891277074814
iteration 147, loss = 0.006543602794408798
iteration 148, loss = 0.0075932457111775875
iteration 149, loss = 0.010009950958192348
iteration 150, loss = 0.006768855266273022
iteration 151, loss = 0.009569709189236164
iteration 152, loss = 0.007046319544315338
iteration 153, loss = 0.007295305375009775
iteration 154, loss = 0.007404894568026066
iteration 155, loss = 0.008245294913649559
iteration 156, loss = 0.007032373920083046
iteration 157, loss = 0.008486419916152954
iteration 158, loss = 0.006603206042200327
iteration 159, loss = 0.010469648987054825
iteration 160, loss = 0.007132942788302898
iteration 161, loss = 0.006838805973529816
iteration 162, loss = 0.00718945637345314
iteration 163, loss = 0.007028961554169655
iteration 164, loss = 0.00792179349809885
iteration 165, loss = 0.006427828222513199
iteration 166, loss = 0.007444304414093494
iteration 167, loss = 0.006568233482539654
iteration 168, loss = 0.007548490073531866
iteration 169, loss = 0.006893348880112171
iteration 170, loss = 0.008252717554569244
iteration 171, loss = 0.007299137767404318
iteration 172, loss = 0.0074583254754543304
iteration 173, loss = 0.007022171281278133
iteration 174, loss = 0.008093392476439476
iteration 175, loss = 0.00733787240460515
iteration 176, loss = 0.007002193480730057
iteration 177, loss = 0.006580672226846218
iteration 178, loss = 0.008278554305434227
iteration 179, loss = 0.010260477662086487
iteration 180, loss = 0.00805368460714817
iteration 181, loss = 0.006960137281566858
iteration 182, loss = 0.011600956320762634
iteration 183, loss = 0.007928550243377686
iteration 184, loss = 0.006696122698485851
iteration 185, loss = 0.013399210758507252
iteration 186, loss = 0.00835904385894537
iteration 187, loss = 0.0077196466736495495
iteration 188, loss = 0.0069637601263821125
iteration 189, loss = 0.0070664044469594955
iteration 190, loss = 0.006949843838810921
iteration 191, loss = 0.006646445952355862
iteration 192, loss = 0.007469766773283482
iteration 193, loss = 0.0072268713265657425
iteration 194, loss = 0.006930455565452576
iteration 195, loss = 0.007346957456320524
iteration 196, loss = 0.0068650636821985245
iteration 197, loss = 0.008443993516266346
iteration 198, loss = 0.0072794705629348755
iteration 199, loss = 0.007197355851531029
iteration 200, loss = 0.0066277277655899525
iteration 201, loss = 0.008703766390681267
iteration 202, loss = 0.006399975623935461
iteration 203, loss = 0.0069478354416787624
iteration 204, loss = 0.0066368067637085915
iteration 205, loss = 0.00856260396540165
iteration 206, loss = 0.007074809167534113
iteration 207, loss = 0.006603762041777372
iteration 208, loss = 0.007152467500418425
iteration 209, loss = 0.006719136610627174
iteration 210, loss = 0.007477096281945705
iteration 211, loss = 0.006429676897823811
iteration 212, loss = 0.006793396547436714
iteration 213, loss = 0.006792813539505005
iteration 214, loss = 0.00918476190418005
iteration 215, loss = 0.008745499886572361
iteration 216, loss = 0.0072078038938343525
iteration 217, loss = 0.010524454526603222
iteration 218, loss = 0.006530377548187971
iteration 219, loss = 0.006738872732967138
iteration 220, loss = 0.006872463971376419
iteration 221, loss = 0.007222791668027639
iteration 222, loss = 0.007000748533755541
iteration 223, loss = 0.007855701260268688
iteration 224, loss = 0.008248744532465935
iteration 225, loss = 0.008445031009614468
iteration 226, loss = 0.00702003063634038
iteration 227, loss = 0.007639578077942133
iteration 228, loss = 0.009898390620946884
iteration 229, loss = 0.009080369956791401
iteration 230, loss = 0.008346369490027428
iteration 231, loss = 0.007816949859261513
iteration 232, loss = 0.0070831067860126495
iteration 233, loss = 0.00678594596683979
iteration 234, loss = 0.007380486465990543
iteration 235, loss = 0.006447815801948309
iteration 236, loss = 0.006665177643299103
iteration 237, loss = 0.010213473811745644
iteration 238, loss = 0.006879835389554501
iteration 239, loss = 0.006730745080858469
iteration 240, loss = 0.00718596950173378
iteration 241, loss = 0.007581592071801424
iteration 242, loss = 0.007043800782412291
iteration 243, loss = 0.0069139208644628525
iteration 244, loss = 0.008809145539999008
iteration 245, loss = 0.006824166513979435
iteration 246, loss = 0.00669264979660511
iteration 247, loss = 0.006715788505971432
iteration 248, loss = 0.0069063082337379456
iteration 249, loss = 0.006987147964537144
iteration 250, loss = 0.007161593530327082
iteration 251, loss = 0.007113691419363022
iteration 252, loss = 0.006373195443302393
iteration 253, loss = 0.008856367319822311
iteration 254, loss = 0.009450637735426426
iteration 255, loss = 0.007296734023839235
iteration 256, loss = 0.007018069736659527
iteration 257, loss = 0.006494145840406418
iteration 258, loss = 0.009060083888471127
iteration 259, loss = 0.0068354238756000996
iteration 260, loss = 0.007166357710957527
iteration 261, loss = 0.006595374085009098
iteration 262, loss = 0.0067003038711845875
iteration 263, loss = 0.00749429315328598
iteration 264, loss = 0.00815380085259676
iteration 265, loss = 0.006912045646458864
iteration 266, loss = 0.006778625305742025
iteration 267, loss = 0.007898489013314247
iteration 268, loss = 0.007181102409958839
iteration 269, loss = 0.007974783889949322
iteration 270, loss = 0.007321508601307869
iteration 271, loss = 0.008286470547318459
iteration 272, loss = 0.006856879219412804
iteration 273, loss = 0.009118628688156605
iteration 274, loss = 0.006594821345061064
iteration 275, loss = 0.0079300282523036
iteration 276, loss = 0.006739865988492966
iteration 277, loss = 0.0078590614721179
iteration 278, loss = 0.009232643991708755
iteration 279, loss = 0.0070560527965426445
iteration 280, loss = 0.006931710056960583
iteration 281, loss = 0.0068985470570623875
iteration 282, loss = 0.0072507914155721664
iteration 283, loss = 0.008160753175616264
iteration 284, loss = 0.007069381419569254
iteration 285, loss = 0.007741178385913372
iteration 286, loss = 0.006783521734178066
iteration 287, loss = 0.008531933650374413
iteration 288, loss = 0.006933977827429771
iteration 289, loss = 0.007868252694606781
iteration 290, loss = 0.008577130734920502
iteration 291, loss = 0.007862850092351437
iteration 292, loss = 0.007020460907369852
iteration 293, loss = 0.006814638152718544
iteration 294, loss = 0.008532973937690258
iteration 295, loss = 0.007047004997730255
iteration 296, loss = 0.006854620296508074
iteration 297, loss = 0.0071093132719397545
iteration 298, loss = 0.009423131123185158
iteration 299, loss = 0.006655849050730467
iteration 300, loss = 0.007238967809826136
iteration 1, loss = 0.006568721495568752
iteration 2, loss = 0.010301304049789906
iteration 3, loss = 0.007311738096177578
iteration 4, loss = 0.007220001425594091
iteration 5, loss = 0.006925764959305525
iteration 6, loss = 0.007840513251721859
iteration 7, loss = 0.006754996255040169
iteration 8, loss = 0.0066247424110770226
iteration 9, loss = 0.007174331694841385
iteration 10, loss = 0.007148620672523975
iteration 11, loss = 0.006676862947642803
iteration 12, loss = 0.007271348498761654
iteration 13, loss = 0.00961072463542223
iteration 14, loss = 0.010848094709217548
iteration 15, loss = 0.008989594876766205
iteration 16, loss = 0.008403502404689789
iteration 17, loss = 0.0066919876262545586
iteration 18, loss = 0.008482293225824833
iteration 19, loss = 0.009904643520712852
iteration 20, loss = 0.007159614935517311
iteration 21, loss = 0.006794993299990892
iteration 22, loss = 0.0072790528647601604
iteration 23, loss = 0.006587123963981867
iteration 24, loss = 0.006659688428044319
iteration 25, loss = 0.0072106472216546535
iteration 26, loss = 0.007823958061635494
iteration 27, loss = 0.006983320228755474
iteration 28, loss = 0.006648915819823742
iteration 29, loss = 0.006778450682759285
iteration 30, loss = 0.01011483184993267
iteration 31, loss = 0.008503597229719162
iteration 32, loss = 0.006930502597242594
iteration 33, loss = 0.010757853277027607
iteration 34, loss = 0.010008983314037323
iteration 35, loss = 0.008516160771250725
iteration 36, loss = 0.006796843837946653
iteration 37, loss = 0.007237736601382494
iteration 38, loss = 0.007111852057278156
iteration 39, loss = 0.006842311471700668
iteration 40, loss = 0.008763015270233154
iteration 41, loss = 0.008536051027476788
iteration 42, loss = 0.006793520879000425
iteration 43, loss = 0.006992720998823643
iteration 44, loss = 0.007233906537294388
iteration 45, loss = 0.007303499151021242
iteration 46, loss = 0.006701358128339052
iteration 47, loss = 0.007436033338308334
iteration 48, loss = 0.007450900040566921
iteration 49, loss = 0.007341201417148113
iteration 50, loss = 0.01194788422435522
iteration 51, loss = 0.007156310137361288
iteration 52, loss = 0.007051311433315277
iteration 53, loss = 0.007705734111368656
iteration 54, loss = 0.0069891116581857204
iteration 55, loss = 0.009229470044374466
iteration 56, loss = 0.006699501536786556
iteration 57, loss = 0.006418032106012106
iteration 58, loss = 0.007982021197676659
iteration 59, loss = 0.00697009451687336
iteration 60, loss = 0.006433012429624796
iteration 61, loss = 0.007232623174786568
iteration 62, loss = 0.00815199501812458
iteration 63, loss = 0.00677220756188035
iteration 64, loss = 0.0067736199125647545
iteration 65, loss = 0.008393310941755772
iteration 66, loss = 0.006915464531630278
iteration 67, loss = 0.006640810053795576
iteration 68, loss = 0.007136180065572262
iteration 69, loss = 0.008054018951952457
iteration 70, loss = 0.007163194473832846
iteration 71, loss = 0.006637407932430506
iteration 72, loss = 0.008763519115746021
iteration 73, loss = 0.007027642801403999
iteration 74, loss = 0.008359517902135849
iteration 75, loss = 0.008450444787740707
iteration 76, loss = 0.006817106157541275
iteration 77, loss = 0.007642398122698069
iteration 78, loss = 0.006724147126078606
iteration 79, loss = 0.010448021814227104
iteration 80, loss = 0.007119209971278906
iteration 81, loss = 0.00754619762301445
iteration 82, loss = 0.006701197475194931
iteration 83, loss = 0.0065471092239022255
iteration 84, loss = 0.006914726458489895
iteration 85, loss = 0.006556025240570307
iteration 86, loss = 0.006747265812009573
iteration 87, loss = 0.006902522873133421
iteration 88, loss = 0.008166841231286526
iteration 89, loss = 0.006423981860280037
iteration 90, loss = 0.006894536316394806
iteration 91, loss = 0.008496676571667194
iteration 92, loss = 0.006602915935218334
iteration 93, loss = 0.007509033195674419
iteration 94, loss = 0.006755982991307974
iteration 95, loss = 0.006918018218129873
iteration 96, loss = 0.006550890393555164
iteration 97, loss = 0.006769882515072823
iteration 98, loss = 0.008559494279325008
iteration 99, loss = 0.00641219038516283
iteration 100, loss = 0.00664941081777215
iteration 101, loss = 0.006652910728007555
iteration 102, loss = 0.008403829298913479
iteration 103, loss = 0.00703169871121645
iteration 104, loss = 0.006686457432806492
iteration 105, loss = 0.0065719750709831715
iteration 106, loss = 0.006518482696264982
iteration 107, loss = 0.0071578724309802055
iteration 108, loss = 0.006620057392865419
iteration 109, loss = 0.006901235319674015
iteration 110, loss = 0.006392802111804485
iteration 111, loss = 0.006768212653696537
iteration 112, loss = 0.012056441977620125
iteration 113, loss = 0.006184782832860947
iteration 114, loss = 0.010296899825334549
iteration 115, loss = 0.007711230777204037
iteration 116, loss = 0.0077432310208678246
iteration 117, loss = 0.006991671863943338
iteration 118, loss = 0.00696310680359602
iteration 119, loss = 0.006956982426345348
iteration 120, loss = 0.007089688908308744
iteration 121, loss = 0.0074720787815749645
iteration 122, loss = 0.006817101035267115
iteration 123, loss = 0.006878133863210678
iteration 124, loss = 0.007746851071715355
iteration 125, loss = 0.007365548051893711
iteration 126, loss = 0.007058145012706518
iteration 127, loss = 0.007304940838366747
iteration 128, loss = 0.007259204052388668
iteration 129, loss = 0.0068650757893919945
iteration 130, loss = 0.008703175000846386
iteration 131, loss = 0.007275720126926899
iteration 132, loss = 0.006837841589003801
iteration 133, loss = 0.007377549074590206
iteration 134, loss = 0.008000321686267853
iteration 135, loss = 0.009900116361677647
iteration 136, loss = 0.006796766072511673
iteration 137, loss = 0.006748808082193136
iteration 138, loss = 0.007263469509780407
iteration 139, loss = 0.006294819992035627
iteration 140, loss = 0.013762242160737514
iteration 141, loss = 0.009991743601858616
iteration 142, loss = 0.008313584141433239
iteration 143, loss = 0.007381505332887173
iteration 144, loss = 0.010590534657239914
iteration 145, loss = 0.007137043867260218
iteration 146, loss = 0.008404160849750042
iteration 147, loss = 0.007459964603185654
iteration 148, loss = 0.00953652709722519
iteration 149, loss = 0.006683580577373505
iteration 150, loss = 0.007119690999388695
iteration 151, loss = 0.006746426224708557
iteration 152, loss = 0.006551105063408613
iteration 153, loss = 0.00844158697873354
iteration 154, loss = 0.006795722059905529
iteration 155, loss = 0.008670458570122719
iteration 156, loss = 0.007161233574151993
iteration 157, loss = 0.007520355749875307
iteration 158, loss = 0.0064674317836761475
iteration 159, loss = 0.007039747666567564
iteration 160, loss = 0.008566724136471748
iteration 161, loss = 0.007026288658380508
iteration 162, loss = 0.008525964803993702
iteration 163, loss = 0.007088300306349993
iteration 164, loss = 0.008718475699424744
iteration 165, loss = 0.006523855496197939
iteration 166, loss = 0.0068107470870018005
iteration 167, loss = 0.006726600229740143
iteration 168, loss = 0.006504280958324671
iteration 169, loss = 0.009729990735650063
iteration 170, loss = 0.007281986065208912
iteration 171, loss = 0.0066356416791677475
iteration 172, loss = 0.007889527827501297
iteration 173, loss = 0.006836152169853449
iteration 174, loss = 0.006585515569895506
iteration 175, loss = 0.009620100259780884
iteration 176, loss = 0.010033982805907726
iteration 177, loss = 0.0067080166190862656
iteration 178, loss = 0.006412080954760313
iteration 179, loss = 0.006723975762724876
iteration 180, loss = 0.006832567043602467
iteration 181, loss = 0.008414367213845253
iteration 182, loss = 0.006784876808524132
iteration 183, loss = 0.008092808537185192
iteration 184, loss = 0.006909175775945187
iteration 185, loss = 0.008314712904393673
iteration 186, loss = 0.008487780578434467
iteration 187, loss = 0.00836087390780449
iteration 188, loss = 0.00672047259286046
iteration 189, loss = 0.006946898996829987
iteration 190, loss = 0.006921185180544853
iteration 191, loss = 0.007411152124404907
iteration 192, loss = 0.0073119523003697395
iteration 193, loss = 0.006955939345061779
iteration 194, loss = 0.006895541213452816
iteration 195, loss = 0.0066877324134111404
iteration 196, loss = 0.007082656025886536
iteration 197, loss = 0.006869368255138397
iteration 198, loss = 0.006996599491685629
iteration 199, loss = 0.006924659013748169
iteration 200, loss = 0.0066475593484938145
iteration 201, loss = 0.0065702274441719055
iteration 202, loss = 0.006635160185396671
iteration 203, loss = 0.007112284656614065
iteration 204, loss = 0.006885469425469637
iteration 205, loss = 0.006730093155056238
iteration 206, loss = 0.007646732032299042
iteration 207, loss = 0.006801247596740723
iteration 208, loss = 0.008104123175144196
iteration 209, loss = 0.006300982553511858
iteration 210, loss = 0.006855808664113283
iteration 211, loss = 0.006640540901571512
iteration 212, loss = 0.006638945546001196
iteration 213, loss = 0.006696406286209822
iteration 214, loss = 0.009874111041426659
iteration 215, loss = 0.007617263123393059
iteration 216, loss = 0.006614857353270054
iteration 217, loss = 0.008600477129220963
iteration 218, loss = 0.007251221686601639
iteration 219, loss = 0.006986035965383053
iteration 220, loss = 0.006686924025416374
iteration 221, loss = 0.006542947608977556
iteration 222, loss = 0.007027811836451292
iteration 223, loss = 0.007113416213542223
iteration 224, loss = 0.007937252521514893
iteration 225, loss = 0.00657097389921546
iteration 226, loss = 0.007512006442993879
iteration 227, loss = 0.006589233875274658
iteration 228, loss = 0.006724048405885696
iteration 229, loss = 0.007646777201443911
iteration 230, loss = 0.008451518602669239
iteration 231, loss = 0.006832811050117016
iteration 232, loss = 0.00883340835571289
iteration 233, loss = 0.006472894921898842
iteration 234, loss = 0.00819435901939869
iteration 235, loss = 0.007900387980043888
iteration 236, loss = 0.0068564992398023605
iteration 237, loss = 0.006897688843309879
iteration 238, loss = 0.006719511933624744
iteration 239, loss = 0.006851609330624342
iteration 240, loss = 0.006921011488884687
iteration 241, loss = 0.008698261342942715
iteration 242, loss = 0.006884817034006119
iteration 243, loss = 0.007335103582590818
iteration 244, loss = 0.006333184894174337
iteration 245, loss = 0.007684451062232256
iteration 246, loss = 0.007031883578747511
iteration 247, loss = 0.009950503706932068
iteration 248, loss = 0.006401794962584972
iteration 249, loss = 0.006382675375789404
iteration 250, loss = 0.006323106586933136
iteration 251, loss = 0.0068801348097622395
iteration 252, loss = 0.006377432029694319
iteration 253, loss = 0.009528012946248055
iteration 254, loss = 0.006500050891190767
iteration 255, loss = 0.006568992510437965
iteration 256, loss = 0.00687651289626956
iteration 257, loss = 0.006620967760682106
iteration 258, loss = 0.006438139826059341
iteration 259, loss = 0.006269088480621576
iteration 260, loss = 0.008379445411264896
iteration 261, loss = 0.006520817056298256
iteration 262, loss = 0.006874846760183573
iteration 263, loss = 0.006769060622900724
iteration 264, loss = 0.011877605691552162
iteration 265, loss = 0.00724013801664114
iteration 266, loss = 0.006913977675139904
iteration 267, loss = 0.011101714335381985
iteration 268, loss = 0.006540453061461449
iteration 269, loss = 0.008871937170624733
iteration 270, loss = 0.006638106424361467
iteration 271, loss = 0.006605873815715313
iteration 272, loss = 0.006646212190389633
iteration 273, loss = 0.007091175764799118
iteration 274, loss = 0.006418407894670963
iteration 275, loss = 0.006978711113333702
iteration 276, loss = 0.0068056327290833
iteration 277, loss = 0.0071058995090425014
iteration 278, loss = 0.00686867069453001
iteration 279, loss = 0.00801355391740799
iteration 280, loss = 0.006962488405406475
iteration 281, loss = 0.008649863302707672
iteration 282, loss = 0.0067507545463740826
iteration 283, loss = 0.006643318571150303
iteration 284, loss = 0.008595495484769344
iteration 285, loss = 0.006975788157433271
iteration 286, loss = 0.006677060853689909
iteration 287, loss = 0.011031519621610641
iteration 288, loss = 0.00735247228294611
iteration 289, loss = 0.0063192774541676044
iteration 290, loss = 0.007151526864618063
iteration 291, loss = 0.006807640194892883
iteration 292, loss = 0.007260902319103479
iteration 293, loss = 0.007945625111460686
iteration 294, loss = 0.00663209892809391
iteration 295, loss = 0.007040536962449551
iteration 296, loss = 0.007965335622429848
iteration 297, loss = 0.007749064825475216
iteration 298, loss = 0.0068818144500255585
iteration 299, loss = 0.010384165681898594
iteration 300, loss = 0.008197898976504803
iteration 1, loss = 0.006724983919411898
iteration 2, loss = 0.007562126498669386
iteration 3, loss = 0.007130580022931099
iteration 4, loss = 0.006815281696617603
iteration 5, loss = 0.007115697488188744
iteration 6, loss = 0.006779542192816734
iteration 7, loss = 0.00676408875733614
iteration 8, loss = 0.0067692892625927925
iteration 9, loss = 0.006595779210329056
iteration 10, loss = 0.006658713798969984
iteration 11, loss = 0.006283957976847887
iteration 12, loss = 0.007399565540254116
iteration 13, loss = 0.013035326264798641
iteration 14, loss = 0.009101532399654388
iteration 15, loss = 0.006911367643624544
iteration 16, loss = 0.006882493384182453
iteration 17, loss = 0.006796229165047407
iteration 18, loss = 0.007105851545929909
iteration 19, loss = 0.007784182671457529
iteration 20, loss = 0.008512374944984913
iteration 21, loss = 0.0070917340926826
iteration 22, loss = 0.01006985828280449
iteration 23, loss = 0.00659713800996542
iteration 24, loss = 0.008162899874150753
iteration 25, loss = 0.006611712742596865
iteration 26, loss = 0.010041634552180767
iteration 27, loss = 0.006757183000445366
iteration 28, loss = 0.008670978248119354
iteration 29, loss = 0.006667356472462416
iteration 30, loss = 0.006685004103928804
iteration 31, loss = 0.006738268304616213
iteration 32, loss = 0.006279673893004656
iteration 33, loss = 0.00707377539947629
iteration 34, loss = 0.006539461202919483
iteration 35, loss = 0.0064950366504490376
iteration 36, loss = 0.007493880577385426
iteration 37, loss = 0.006509512197226286
iteration 38, loss = 0.009427801705896854
iteration 39, loss = 0.006929907947778702
iteration 40, loss = 0.007447894662618637
iteration 41, loss = 0.006678898818790913
iteration 42, loss = 0.007908947765827179
iteration 43, loss = 0.006780951749533415
iteration 44, loss = 0.006982048973441124
iteration 45, loss = 0.006256150081753731
iteration 46, loss = 0.0065218545496463776
iteration 47, loss = 0.006878082174807787
iteration 48, loss = 0.007213395554572344
iteration 49, loss = 0.0098080700263381
iteration 50, loss = 0.006872085854411125
iteration 51, loss = 0.006933412980288267
iteration 52, loss = 0.006567292381078005
iteration 53, loss = 0.008196533657610416
iteration 54, loss = 0.006972198840230703
iteration 55, loss = 0.009779236279428005
iteration 56, loss = 0.007681014947593212
iteration 57, loss = 0.006529299542307854
iteration 58, loss = 0.007075687870383263
iteration 59, loss = 0.009325176477432251
iteration 60, loss = 0.006689806468784809
iteration 61, loss = 0.006491390522569418
iteration 62, loss = 0.0071719554252922535
iteration 63, loss = 0.00874829851090908
iteration 64, loss = 0.006373739335685968
iteration 65, loss = 0.006483904551714659
iteration 66, loss = 0.007497719954699278
iteration 67, loss = 0.00641267467290163
iteration 68, loss = 0.006222862284630537
iteration 69, loss = 0.008833194151520729
iteration 70, loss = 0.006513305474072695
iteration 71, loss = 0.00715084420517087
iteration 72, loss = 0.007077954243868589
iteration 73, loss = 0.0070746890269219875
iteration 74, loss = 0.006454573944211006
iteration 75, loss = 0.007294630631804466
iteration 76, loss = 0.007345174439251423
iteration 77, loss = 0.007992248982191086
iteration 78, loss = 0.008822018280625343
iteration 79, loss = 0.00821839738637209
iteration 80, loss = 0.006782849319279194
iteration 81, loss = 0.009088822640478611
iteration 82, loss = 0.008405509404838085
iteration 83, loss = 0.006912307348102331
iteration 84, loss = 0.007363939192146063
iteration 85, loss = 0.008891480043530464
iteration 86, loss = 0.006878325249999762
iteration 87, loss = 0.006851657293736935
iteration 88, loss = 0.006525963544845581
iteration 89, loss = 0.008189603686332703
iteration 90, loss = 0.008313128724694252
iteration 91, loss = 0.006160181947052479
iteration 92, loss = 0.006523987744003534
iteration 93, loss = 0.007336208131164312
iteration 94, loss = 0.006847434677183628
iteration 95, loss = 0.008165932260453701
iteration 96, loss = 0.006779188755899668
iteration 97, loss = 0.006514377426356077
iteration 98, loss = 0.007164178881794214
iteration 99, loss = 0.007296726107597351
iteration 100, loss = 0.007923655211925507
iteration 101, loss = 0.006943968124687672
iteration 102, loss = 0.009717837907373905
iteration 103, loss = 0.006301311310380697
iteration 104, loss = 0.007014652248471975
iteration 105, loss = 0.006998186931014061
iteration 106, loss = 0.008180079981684685
iteration 107, loss = 0.008195013739168644
iteration 108, loss = 0.006519273389130831
iteration 109, loss = 0.0063561792485415936
iteration 110, loss = 0.007084139622747898
iteration 111, loss = 0.006786657962948084
iteration 112, loss = 0.00851813517510891
iteration 113, loss = 0.006743063218891621
iteration 114, loss = 0.009554624557495117
iteration 115, loss = 0.006702403072267771
iteration 116, loss = 0.008920113556087017
iteration 117, loss = 0.006639118306338787
iteration 118, loss = 0.008539737202227116
iteration 119, loss = 0.009972548112273216
iteration 120, loss = 0.006771984975785017
iteration 121, loss = 0.006261158734560013
iteration 122, loss = 0.0068264314904809
iteration 123, loss = 0.00668863020837307
iteration 124, loss = 0.00812879391014576
iteration 125, loss = 0.006999120581895113
iteration 126, loss = 0.006329901982098818
iteration 127, loss = 0.007018501404672861
iteration 128, loss = 0.007300127763301134
iteration 129, loss = 0.007705529220402241
iteration 130, loss = 0.006507253739982843
iteration 131, loss = 0.009702456183731556
iteration 132, loss = 0.00689416890963912
iteration 133, loss = 0.007088648620992899
iteration 134, loss = 0.00822958443313837
iteration 135, loss = 0.007865975610911846
iteration 136, loss = 0.009910422377288342
iteration 137, loss = 0.006294281221926212
iteration 138, loss = 0.0067040701396763325
iteration 139, loss = 0.006951684597879648
iteration 140, loss = 0.007277892902493477
iteration 141, loss = 0.00778660923242569
iteration 142, loss = 0.006634449120610952
iteration 143, loss = 0.0077609834261238575
iteration 144, loss = 0.006525969598442316
iteration 145, loss = 0.006544901058077812
iteration 146, loss = 0.006988824810832739
iteration 147, loss = 0.0064364406280219555
iteration 148, loss = 0.007162114139646292
iteration 149, loss = 0.006497332360595465
iteration 150, loss = 0.007895118556916714
iteration 151, loss = 0.006561844609677792
iteration 152, loss = 0.006822547409683466
iteration 153, loss = 0.006951821036636829
iteration 154, loss = 0.006984331179410219
iteration 155, loss = 0.00648862961679697
iteration 156, loss = 0.008010155521333218
iteration 157, loss = 0.009217100217938423
iteration 158, loss = 0.008163100108504295
iteration 159, loss = 0.008681523613631725
iteration 160, loss = 0.007726714480668306
iteration 161, loss = 0.006411810405552387
iteration 162, loss = 0.006684652529656887
iteration 163, loss = 0.007158007938414812
iteration 164, loss = 0.006495894398540258
iteration 165, loss = 0.007661543320864439
iteration 166, loss = 0.008108522742986679
iteration 167, loss = 0.0066057238727808
iteration 168, loss = 0.006270966026932001
iteration 169, loss = 0.008281074464321136
iteration 170, loss = 0.006637445650994778
iteration 171, loss = 0.006440908648073673
iteration 172, loss = 0.006549470126628876
iteration 173, loss = 0.007131609600037336
iteration 174, loss = 0.007790906820446253
iteration 175, loss = 0.006727195810526609
iteration 176, loss = 0.0067722387611866
iteration 177, loss = 0.006485678721219301
iteration 178, loss = 0.006717613898217678
iteration 179, loss = 0.008278417401015759
iteration 180, loss = 0.006882714107632637
iteration 181, loss = 0.006993541959673166
iteration 182, loss = 0.007072903215885162
iteration 183, loss = 0.007138304878026247
iteration 184, loss = 0.006442525889724493
iteration 185, loss = 0.00718791875988245
iteration 186, loss = 0.006674857810139656
iteration 187, loss = 0.00833738874644041
iteration 188, loss = 0.010578650049865246
iteration 189, loss = 0.00720168324187398
iteration 190, loss = 0.006509366445243359
iteration 191, loss = 0.009841976687312126
iteration 192, loss = 0.008151872083544731
iteration 193, loss = 0.007645225152373314
iteration 194, loss = 0.006462743040174246
iteration 195, loss = 0.008492671884596348
iteration 196, loss = 0.006184758618474007
iteration 197, loss = 0.007016155403107405
iteration 198, loss = 0.011526807211339474
iteration 199, loss = 0.008667273446917534
iteration 200, loss = 0.008249140344560146
iteration 201, loss = 0.006533012259751558
iteration 202, loss = 0.007919625379145145
iteration 203, loss = 0.00937640480697155
iteration 204, loss = 0.006863122805953026
iteration 205, loss = 0.006981031969189644
iteration 206, loss = 0.008721644058823586
iteration 207, loss = 0.006002078764140606
iteration 208, loss = 0.00735571701079607
iteration 209, loss = 0.0070775458589196205
iteration 210, loss = 0.009679299779236317
iteration 211, loss = 0.006680622696876526
iteration 212, loss = 0.007575598079711199
iteration 213, loss = 0.007039292715489864
iteration 214, loss = 0.006798011250793934
iteration 215, loss = 0.008009810000658035
iteration 216, loss = 0.006541518960148096
iteration 217, loss = 0.006881359498947859
iteration 218, loss = 0.00632038339972496
iteration 219, loss = 0.006463820114731789
iteration 220, loss = 0.007454108912497759
iteration 221, loss = 0.007883390411734581
iteration 222, loss = 0.008323600515723228
iteration 223, loss = 0.006314528174698353
iteration 224, loss = 0.00850001536309719
iteration 225, loss = 0.007869112305343151
iteration 226, loss = 0.006427133455872536
iteration 227, loss = 0.006707737687975168
iteration 228, loss = 0.008939417079091072
iteration 229, loss = 0.0066398195922374725
iteration 230, loss = 0.006305090617388487
iteration 231, loss = 0.006770370993763208
iteration 232, loss = 0.006760661024600267
iteration 233, loss = 0.006496823858469725
iteration 234, loss = 0.006600037682801485
iteration 235, loss = 0.0069071706384420395
iteration 236, loss = 0.006305912509560585
iteration 237, loss = 0.006587603595107794
iteration 238, loss = 0.006425496656447649
iteration 239, loss = 0.006980901584029198
iteration 240, loss = 0.007196206133812666
iteration 241, loss = 0.006762796081602573
iteration 242, loss = 0.006850112229585648
iteration 243, loss = 0.007018439471721649
iteration 244, loss = 0.0065797739662230015
iteration 245, loss = 0.008757728151977062
iteration 246, loss = 0.007032317109405994
iteration 247, loss = 0.006462912540882826
iteration 248, loss = 0.006716055329889059
iteration 249, loss = 0.00672313803806901
iteration 250, loss = 0.006674731150269508
iteration 251, loss = 0.006578114349395037
iteration 252, loss = 0.007143935654312372
iteration 253, loss = 0.006774004083126783
iteration 254, loss = 0.007343485485762358
iteration 255, loss = 0.006983127444982529
iteration 256, loss = 0.008452409878373146
iteration 257, loss = 0.008839760906994343
iteration 258, loss = 0.00683610187843442
iteration 259, loss = 0.00786933209747076
iteration 260, loss = 0.0085230004042387
iteration 261, loss = 0.007780307438224554
iteration 262, loss = 0.006177267525345087
iteration 263, loss = 0.006193967070430517
iteration 264, loss = 0.006857798434793949
iteration 265, loss = 0.007088758982717991
iteration 266, loss = 0.008229376748204231
iteration 267, loss = 0.0061875684186816216
iteration 268, loss = 0.006985468324273825
iteration 269, loss = 0.00733213871717453
iteration 270, loss = 0.008262970484793186
iteration 271, loss = 0.008074783720076084
iteration 272, loss = 0.007141151465475559
iteration 273, loss = 0.006336222868412733
iteration 274, loss = 0.008053112775087357
iteration 275, loss = 0.006573539227247238
iteration 276, loss = 0.006383609492331743
iteration 277, loss = 0.008769995532929897
iteration 278, loss = 0.0062745618633925915
iteration 279, loss = 0.008085302077233791
iteration 280, loss = 0.006513633765280247
iteration 281, loss = 0.006624740082770586
iteration 282, loss = 0.006562196649610996
iteration 283, loss = 0.006519557908177376
iteration 284, loss = 0.006148722488433123
iteration 285, loss = 0.01163119450211525
iteration 286, loss = 0.006483290810137987
iteration 287, loss = 0.007662496529519558
iteration 288, loss = 0.006854858249425888
iteration 289, loss = 0.008676743134856224
iteration 290, loss = 0.008183878846466541
iteration 291, loss = 0.006986538879573345
iteration 292, loss = 0.007515839766710997
iteration 293, loss = 0.009041620418429375
iteration 294, loss = 0.00632873922586441
iteration 295, loss = 0.006271072663366795
iteration 296, loss = 0.00711809704080224
iteration 297, loss = 0.006442920304834843
iteration 298, loss = 0.007897303439676762
iteration 299, loss = 0.006892395205795765
iteration 300, loss = 0.006690547335892916
iteration 1, loss = 0.008332707919180393
iteration 2, loss = 0.007517366204410791
iteration 3, loss = 0.007966753095388412
iteration 4, loss = 0.006327640265226364
iteration 5, loss = 0.006541453767567873
iteration 6, loss = 0.008042896166443825
iteration 7, loss = 0.006446907762438059
iteration 8, loss = 0.0070252614095807076
iteration 9, loss = 0.006915084086358547
iteration 10, loss = 0.0074013955891132355
iteration 11, loss = 0.006489897612482309
iteration 12, loss = 0.006534863263368607
iteration 13, loss = 0.006719081662595272
iteration 14, loss = 0.009558340534567833
iteration 15, loss = 0.006883371155709028
iteration 16, loss = 0.008528389036655426
iteration 17, loss = 0.006213811691850424
iteration 18, loss = 0.006368150934576988
iteration 19, loss = 0.006523983087390661
iteration 20, loss = 0.007674443069845438
iteration 21, loss = 0.0071110050193965435
iteration 22, loss = 0.006489479448646307
iteration 23, loss = 0.006514586508274078
iteration 24, loss = 0.006341664120554924
iteration 25, loss = 0.006566678173840046
iteration 26, loss = 0.006853117607533932
iteration 27, loss = 0.007938842289149761
iteration 28, loss = 0.006203236989676952
iteration 29, loss = 0.0074888188391923904
iteration 30, loss = 0.0068687438033521175
iteration 31, loss = 0.009105503559112549
iteration 32, loss = 0.006310699507594109
iteration 33, loss = 0.006759235169738531
iteration 34, loss = 0.007182643748819828
iteration 35, loss = 0.006547756027430296
iteration 36, loss = 0.008919346146285534
iteration 37, loss = 0.0062759811989963055
iteration 38, loss = 0.006260924506932497
iteration 39, loss = 0.006530434358865023
iteration 40, loss = 0.006660669110715389
iteration 41, loss = 0.007114365231245756
iteration 42, loss = 0.006543976254761219
iteration 43, loss = 0.00702770845964551
iteration 44, loss = 0.007801149971783161
iteration 45, loss = 0.007021268364042044
iteration 46, loss = 0.008430816233158112
iteration 47, loss = 0.006792223080992699
iteration 48, loss = 0.007494543679058552
iteration 49, loss = 0.006541960872709751
iteration 50, loss = 0.008184152655303478
iteration 51, loss = 0.008102542720735073
iteration 52, loss = 0.00857872236520052
iteration 53, loss = 0.006493036635220051
iteration 54, loss = 0.00699232891201973
iteration 55, loss = 0.006998461205512285
iteration 56, loss = 0.00785516481846571
iteration 57, loss = 0.006112611386924982
iteration 58, loss = 0.006131208501756191
iteration 59, loss = 0.010005827993154526
iteration 60, loss = 0.008118441328406334
iteration 61, loss = 0.008619681000709534
iteration 62, loss = 0.00857661571353674
iteration 63, loss = 0.006503812037408352
iteration 64, loss = 0.006129730958491564
iteration 65, loss = 0.010400312021374702
iteration 66, loss = 0.0061063324101269245
iteration 67, loss = 0.008403527550399303
iteration 68, loss = 0.006284739356487989
iteration 69, loss = 0.00665687583386898
iteration 70, loss = 0.009561303071677685
iteration 71, loss = 0.0068906499072909355
iteration 72, loss = 0.006918301805853844
iteration 73, loss = 0.012875592336058617
iteration 74, loss = 0.00796657893806696
iteration 75, loss = 0.007175628561526537
iteration 76, loss = 0.007541746832430363
iteration 77, loss = 0.0064691598527133465
iteration 78, loss = 0.006923678331077099
iteration 79, loss = 0.006602148059755564
iteration 80, loss = 0.00660629803314805
iteration 81, loss = 0.009751969017088413
iteration 82, loss = 0.006587049458175898
iteration 83, loss = 0.0062459735199809074
iteration 84, loss = 0.006242372095584869
iteration 85, loss = 0.00850166566669941
iteration 86, loss = 0.00663980096578598
iteration 87, loss = 0.006675593089312315
iteration 88, loss = 0.006833754479885101
iteration 89, loss = 0.006670450326055288
iteration 90, loss = 0.008549830876290798
iteration 91, loss = 0.00691371550783515
iteration 92, loss = 0.009009705856442451
iteration 93, loss = 0.0074896784499287605
iteration 94, loss = 0.008311445824801922
iteration 95, loss = 0.006501099560409784
iteration 96, loss = 0.006490656174719334
iteration 97, loss = 0.00668699387460947
iteration 98, loss = 0.007768076378852129
iteration 99, loss = 0.006981183774769306
iteration 100, loss = 0.005965226795524359
iteration 101, loss = 0.007832102477550507
iteration 102, loss = 0.006883315276354551
iteration 103, loss = 0.008112880401313305
iteration 104, loss = 0.0067049660719931126
iteration 105, loss = 0.006582807283848524
iteration 106, loss = 0.007769955322146416
iteration 107, loss = 0.006604081019759178
iteration 108, loss = 0.006892050616443157
iteration 109, loss = 0.006579860113561153
iteration 110, loss = 0.00835862010717392
iteration 111, loss = 0.006462626624852419
iteration 112, loss = 0.006185230333358049
iteration 113, loss = 0.006316666956990957
iteration 114, loss = 0.006490127183496952
iteration 115, loss = 0.007845488376915455
iteration 116, loss = 0.007876155897974968
iteration 117, loss = 0.006490706000477076
iteration 118, loss = 0.006489934865385294
iteration 119, loss = 0.006527930032461882
iteration 120, loss = 0.0063627371564507484
iteration 121, loss = 0.006553413812071085
iteration 122, loss = 0.007747475057840347
iteration 123, loss = 0.00714436499401927
iteration 124, loss = 0.006597060244530439
iteration 125, loss = 0.007803990505635738
iteration 126, loss = 0.008557490073144436
iteration 127, loss = 0.006742901634424925
iteration 128, loss = 0.007003909908235073
iteration 129, loss = 0.009674480184912682
iteration 130, loss = 0.0068951137363910675
iteration 131, loss = 0.006729427259415388
iteration 132, loss = 0.008278467692434788
iteration 133, loss = 0.006887020077556372
iteration 134, loss = 0.006758087780326605
iteration 135, loss = 0.009989107958972454
iteration 136, loss = 0.00755236716940999
iteration 137, loss = 0.0061923060566186905
iteration 138, loss = 0.007841396145522594
iteration 139, loss = 0.006589906755834818
iteration 140, loss = 0.006479733623564243
iteration 141, loss = 0.0064785112626850605
iteration 142, loss = 0.00621421355754137
iteration 143, loss = 0.007929804734885693
iteration 144, loss = 0.006629256531596184
iteration 145, loss = 0.007899035699665546
iteration 146, loss = 0.0068938531912863255
iteration 147, loss = 0.007054219022393227
iteration 148, loss = 0.008087449707090855
iteration 149, loss = 0.007662361487746239
iteration 150, loss = 0.006492396350950003
iteration 151, loss = 0.006848215125501156
iteration 152, loss = 0.006449910346418619
iteration 153, loss = 0.00932591874152422
iteration 154, loss = 0.012326586060225964
iteration 155, loss = 0.006754874251782894
iteration 156, loss = 0.006275709718465805
iteration 157, loss = 0.009640950709581375
iteration 158, loss = 0.006540963891893625
iteration 159, loss = 0.0062506357207894325
iteration 160, loss = 0.006495120003819466
iteration 161, loss = 0.006156235001981258
iteration 162, loss = 0.0064585451036691666
iteration 163, loss = 0.006466826889663935
iteration 164, loss = 0.00660975556820631
iteration 165, loss = 0.006126973778009415
iteration 166, loss = 0.006407642737030983
iteration 167, loss = 0.006455275695770979
iteration 168, loss = 0.006324409041553736
iteration 169, loss = 0.006298775784671307
iteration 170, loss = 0.008239532820880413
iteration 171, loss = 0.007006461266428232
iteration 172, loss = 0.006618221290409565
iteration 173, loss = 0.007043885998427868
iteration 174, loss = 0.009587743319571018
iteration 175, loss = 0.006873582489788532
iteration 176, loss = 0.009752223268151283
iteration 177, loss = 0.009600227698683739
iteration 178, loss = 0.008425241336226463
iteration 179, loss = 0.007098475471138954
iteration 180, loss = 0.008649209514260292
iteration 181, loss = 0.006789147853851318
iteration 182, loss = 0.006269664037972689
iteration 183, loss = 0.0063777463510632515
iteration 184, loss = 0.00835055485367775
iteration 185, loss = 0.010031296871602535
iteration 186, loss = 0.007037933450192213
iteration 187, loss = 0.0074454741552472115
iteration 188, loss = 0.010269061662256718
iteration 189, loss = 0.006638824939727783
iteration 190, loss = 0.006688153371214867
iteration 191, loss = 0.006978418678045273
iteration 192, loss = 0.006468575913459063
iteration 193, loss = 0.0063463253900408745
iteration 194, loss = 0.006407694425433874
iteration 195, loss = 0.0064553916454315186
iteration 196, loss = 0.00742100365459919
iteration 197, loss = 0.007807076908648014
iteration 198, loss = 0.00622312817722559
iteration 199, loss = 0.00678730383515358
iteration 200, loss = 0.006773396395146847
iteration 201, loss = 0.006652892101556063
iteration 202, loss = 0.006497077643871307
iteration 203, loss = 0.006651329807937145
iteration 204, loss = 0.006869024597108364
iteration 205, loss = 0.00675825634971261
iteration 206, loss = 0.0065524643287062645
iteration 207, loss = 0.007181640714406967
iteration 208, loss = 0.0072091990150511265
iteration 209, loss = 0.006576409563422203
iteration 210, loss = 0.007104001007974148
iteration 211, loss = 0.007864863611757755
iteration 212, loss = 0.007067955564707518
iteration 213, loss = 0.0081177344545722
iteration 214, loss = 0.006351552903652191
iteration 215, loss = 0.008369816467165947
iteration 216, loss = 0.006435579154640436
iteration 217, loss = 0.007025713101029396
iteration 218, loss = 0.009189910255372524
iteration 219, loss = 0.006824068259447813
iteration 220, loss = 0.00638688076287508
iteration 221, loss = 0.00835119653493166
iteration 222, loss = 0.006682565435767174
iteration 223, loss = 0.006219206377863884
iteration 224, loss = 0.008089092560112476
iteration 225, loss = 0.006265710107982159
iteration 226, loss = 0.007608902174979448
iteration 227, loss = 0.006577786989510059
iteration 228, loss = 0.008811569772660732
iteration 229, loss = 0.006514620967209339
iteration 230, loss = 0.006576142739504576
iteration 231, loss = 0.006827903911471367
iteration 232, loss = 0.005966197699308395
iteration 233, loss = 0.006407008972018957
iteration 234, loss = 0.008200951851904392
iteration 235, loss = 0.007017882075160742
iteration 236, loss = 0.008385606110095978
iteration 237, loss = 0.007494828663766384
iteration 238, loss = 0.007441406603902578
iteration 239, loss = 0.006629101932048798
iteration 240, loss = 0.006185469683259726
iteration 241, loss = 0.00650402158498764
iteration 242, loss = 0.00977739505469799
iteration 243, loss = 0.006268829572945833
iteration 244, loss = 0.006210896652191877
iteration 245, loss = 0.006103792227804661
iteration 246, loss = 0.006562555208802223
iteration 247, loss = 0.0062888627871870995
iteration 248, loss = 0.008907378651201725
iteration 249, loss = 0.006446388550102711
iteration 250, loss = 0.0069037070497870445
iteration 251, loss = 0.009528516791760921
iteration 252, loss = 0.006672730203717947
iteration 253, loss = 0.006723059806972742
iteration 254, loss = 0.006603563204407692
iteration 255, loss = 0.006645514629781246
iteration 256, loss = 0.006129014305770397
iteration 257, loss = 0.006494378205388784
iteration 258, loss = 0.006105329841375351
iteration 259, loss = 0.006473470013588667
iteration 260, loss = 0.006353214383125305
iteration 261, loss = 0.006297172047197819
iteration 262, loss = 0.00649594608694315
iteration 263, loss = 0.00649988604709506
iteration 264, loss = 0.008601662702858448
iteration 265, loss = 0.007978912442922592
iteration 266, loss = 0.0076433508656919
iteration 267, loss = 0.0062562222592532635
iteration 268, loss = 0.00959201343357563
iteration 269, loss = 0.006568088196218014
iteration 270, loss = 0.008562207221984863
iteration 271, loss = 0.007137460634112358
iteration 272, loss = 0.007033179514110088
iteration 273, loss = 0.008414078503847122
iteration 274, loss = 0.007071911357343197
iteration 275, loss = 0.007116246502846479
iteration 276, loss = 0.006211475934833288
iteration 277, loss = 0.006454989779740572
iteration 278, loss = 0.006440505851060152
iteration 279, loss = 0.00751079898327589
iteration 280, loss = 0.006624456029385328
iteration 281, loss = 0.012333887629210949
iteration 282, loss = 0.007322348654270172
iteration 283, loss = 0.008709773421287537
iteration 284, loss = 0.008001932874321938
iteration 285, loss = 0.006632107309997082
iteration 286, loss = 0.006065942347049713
iteration 287, loss = 0.006453321315348148
iteration 288, loss = 0.006321502849459648
iteration 289, loss = 0.006487526930868626
iteration 290, loss = 0.0064421105198562145
iteration 291, loss = 0.006311234552413225
iteration 292, loss = 0.006668933667242527
iteration 293, loss = 0.0064246198162436485
iteration 294, loss = 0.00638755364343524
iteration 295, loss = 0.006577332969754934
iteration 296, loss = 0.008104775100946426
iteration 297, loss = 0.006987296044826508
iteration 298, loss = 0.0066918013617396355
iteration 299, loss = 0.006144479848444462
iteration 300, loss = 0.006971534341573715
iteration 1, loss = 0.006511715706437826
iteration 2, loss = 0.006521647330373526
iteration 3, loss = 0.006864818744361401
iteration 4, loss = 0.007322223391383886
iteration 5, loss = 0.006613693665713072
iteration 6, loss = 0.00787512119859457
iteration 7, loss = 0.007090592756867409
iteration 8, loss = 0.006633766461163759
iteration 9, loss = 0.008749510161578655
iteration 10, loss = 0.008420866914093494
iteration 11, loss = 0.006495450157672167
iteration 12, loss = 0.006670654751360416
iteration 13, loss = 0.006831293925642967
iteration 14, loss = 0.007810771465301514
iteration 15, loss = 0.008055977523326874
iteration 16, loss = 0.008048334158957005
iteration 17, loss = 0.006643321365118027
iteration 18, loss = 0.006409022957086563
iteration 19, loss = 0.007691429927945137
iteration 20, loss = 0.006614618934690952
iteration 21, loss = 0.0075788046233356
iteration 22, loss = 0.006351954769343138
iteration 23, loss = 0.006617910694330931
iteration 24, loss = 0.006058267783373594
iteration 25, loss = 0.007120807655155659
iteration 26, loss = 0.007291268091648817
iteration 27, loss = 0.007817035540938377
iteration 28, loss = 0.007040775381028652
iteration 29, loss = 0.0070077902637422085
iteration 30, loss = 0.006925684399902821
iteration 31, loss = 0.0070460145361721516
iteration 32, loss = 0.006518891546875238
iteration 33, loss = 0.007387422490864992
iteration 34, loss = 0.006603597197681665
iteration 35, loss = 0.006373238284140825
iteration 36, loss = 0.006455543451011181
iteration 37, loss = 0.008167539723217487
iteration 38, loss = 0.007058058865368366
iteration 39, loss = 0.007783029228448868
iteration 40, loss = 0.009319797158241272
iteration 41, loss = 0.006301334593445063
iteration 42, loss = 0.006534704007208347
iteration 43, loss = 0.006922631990164518
iteration 44, loss = 0.00962239969521761
iteration 45, loss = 0.0067144338972866535
iteration 46, loss = 0.007435271982103586
iteration 47, loss = 0.007015305105596781
iteration 48, loss = 0.006517478264868259
iteration 49, loss = 0.006733983755111694
iteration 50, loss = 0.007952769286930561
iteration 51, loss = 0.006501803640276194
iteration 52, loss = 0.00781355518847704
iteration 53, loss = 0.011197824031114578
iteration 54, loss = 0.0068032406270504
iteration 55, loss = 0.008317769505083561
iteration 56, loss = 0.00774649903178215
iteration 57, loss = 0.0070186215452849865
iteration 58, loss = 0.007143137510865927
iteration 59, loss = 0.007390964776277542
iteration 60, loss = 0.00642824824899435
iteration 61, loss = 0.006055289879441261
iteration 62, loss = 0.006561663933098316
iteration 63, loss = 0.006841428577899933
iteration 64, loss = 0.009611415676772594
iteration 65, loss = 0.0069543770514428616
iteration 66, loss = 0.008497844450175762
iteration 67, loss = 0.009308998472988605
iteration 68, loss = 0.006641163490712643
iteration 69, loss = 0.006461691576987505
iteration 70, loss = 0.00924629345536232
iteration 71, loss = 0.00782396923750639
iteration 72, loss = 0.0064449431374669075
iteration 73, loss = 0.007854924537241459
iteration 74, loss = 0.008195284754037857
iteration 75, loss = 0.006212465465068817
iteration 76, loss = 0.006643963512033224
iteration 77, loss = 0.007861156016588211
iteration 78, loss = 0.006621574051678181
iteration 79, loss = 0.00668196938931942
iteration 80, loss = 0.009481332264840603
iteration 81, loss = 0.007944602519273758
iteration 82, loss = 0.006330742500722408
iteration 83, loss = 0.006614776328206062
iteration 84, loss = 0.006625994574278593
iteration 85, loss = 0.006461458280682564
iteration 86, loss = 0.006979627534747124
iteration 87, loss = 0.0076865023002028465
iteration 88, loss = 0.006813082378357649
iteration 89, loss = 0.008936773054301739
iteration 90, loss = 0.006581404712051153
iteration 91, loss = 0.008652199059724808
iteration 92, loss = 0.006059847306460142
iteration 93, loss = 0.006807656493037939
iteration 94, loss = 0.006342760752886534
iteration 95, loss = 0.006317905616015196
iteration 96, loss = 0.006875139661133289
iteration 97, loss = 0.0064919437281787395
iteration 98, loss = 0.008390931412577629
iteration 99, loss = 0.007369156461209059
iteration 100, loss = 0.006649210583418608
iteration 101, loss = 0.008389059454202652
iteration 102, loss = 0.00604637898504734
iteration 103, loss = 0.006097741890698671
iteration 104, loss = 0.006618541665375233
iteration 105, loss = 0.006179020740091801
iteration 106, loss = 0.006648951675742865
iteration 107, loss = 0.005982178263366222
iteration 108, loss = 0.00643229391425848
iteration 109, loss = 0.006436782889068127
iteration 110, loss = 0.006602514535188675
iteration 111, loss = 0.005998129025101662
iteration 112, loss = 0.006234896369278431
iteration 113, loss = 0.006396836135536432
iteration 114, loss = 0.00647105323150754
iteration 115, loss = 0.006167570129036903
iteration 116, loss = 0.008679044432938099
iteration 117, loss = 0.006899262312799692
iteration 118, loss = 0.011478409171104431
iteration 119, loss = 0.006867719814181328
iteration 120, loss = 0.00766264321282506
iteration 121, loss = 0.008664417080581188
iteration 122, loss = 0.006178671959787607
iteration 123, loss = 0.010464688763022423
iteration 124, loss = 0.006288738921284676
iteration 125, loss = 0.007921609096229076
iteration 126, loss = 0.0068154241889715195
iteration 127, loss = 0.007068975828588009
iteration 128, loss = 0.006541543174535036
iteration 129, loss = 0.006537206005305052
iteration 130, loss = 0.006289258599281311
iteration 131, loss = 0.006906385067850351
iteration 132, loss = 0.006419353187084198
iteration 133, loss = 0.006597936153411865
iteration 134, loss = 0.007627920247614384
iteration 135, loss = 0.005982415284961462
iteration 136, loss = 0.008244103752076626
iteration 137, loss = 0.006428510881960392
iteration 138, loss = 0.0062741972506046295
iteration 139, loss = 0.007965274155139923
iteration 140, loss = 0.006558673921972513
iteration 141, loss = 0.006335030775517225
iteration 142, loss = 0.006348427385091782
iteration 143, loss = 0.0064439293928444386
iteration 144, loss = 0.0065157972276210785
iteration 145, loss = 0.009430981241166592
iteration 146, loss = 0.009670956060290337
iteration 147, loss = 0.006803557276725769
iteration 148, loss = 0.006258571054786444
iteration 149, loss = 0.009118235670030117
iteration 150, loss = 0.008124792017042637
iteration 151, loss = 0.006579481530934572
iteration 152, loss = 0.007317014038562775
iteration 153, loss = 0.006239769514650106
iteration 154, loss = 0.0061399308033287525
iteration 155, loss = 0.007226643152534962
iteration 156, loss = 0.006666494999080896
iteration 157, loss = 0.006428333930671215
iteration 158, loss = 0.009161565452814102
iteration 159, loss = 0.006654733791947365
iteration 160, loss = 0.006608093623071909
iteration 161, loss = 0.006374879274517298
iteration 162, loss = 0.006440985947847366
iteration 163, loss = 0.008189800195395947
iteration 164, loss = 0.006600336171686649
iteration 165, loss = 0.00640318775549531
iteration 166, loss = 0.006695556454360485
iteration 167, loss = 0.007365239784121513
iteration 168, loss = 0.008292610757052898
iteration 169, loss = 0.006332719698548317
iteration 170, loss = 0.006511337123811245
iteration 171, loss = 0.006770385894924402
iteration 172, loss = 0.006677456200122833
iteration 173, loss = 0.0069366516545414925
iteration 174, loss = 0.0063840607181191444
iteration 175, loss = 0.006007833406329155
iteration 176, loss = 0.006819917354732752
iteration 177, loss = 0.006363300606608391
iteration 178, loss = 0.006339598912745714
iteration 179, loss = 0.006119276862591505
iteration 180, loss = 0.0072160703130066395
iteration 181, loss = 0.008249177597463131
iteration 182, loss = 0.007228084374219179
iteration 183, loss = 0.0075790840201079845
iteration 184, loss = 0.007244807202368975
iteration 185, loss = 0.011195975355803967
iteration 186, loss = 0.006255481857806444
iteration 187, loss = 0.006090573966503143
iteration 188, loss = 0.006237691268324852
iteration 189, loss = 0.006246884819120169
iteration 190, loss = 0.006480813957750797
iteration 191, loss = 0.006144761573523283
iteration 192, loss = 0.007814265787601471
iteration 193, loss = 0.008640006184577942
iteration 194, loss = 0.006948850117623806
iteration 195, loss = 0.007702087517827749
iteration 196, loss = 0.0068755666725337505
iteration 197, loss = 0.00736797833815217
iteration 198, loss = 0.00655742734670639
iteration 199, loss = 0.006450228858739138
iteration 200, loss = 0.006325691007077694
iteration 201, loss = 0.006509078200906515
iteration 202, loss = 0.0067335269413888454
iteration 203, loss = 0.006253870669752359
iteration 204, loss = 0.008430225774645805
iteration 205, loss = 0.006338175386190414
iteration 206, loss = 0.006678096484392881
iteration 207, loss = 0.007181218825280666
iteration 208, loss = 0.006237918511033058
iteration 209, loss = 0.006294144317507744
iteration 210, loss = 0.008360241539776325
iteration 211, loss = 0.0063576847314834595
iteration 212, loss = 0.006407056003808975
iteration 213, loss = 0.007190492004156113
iteration 214, loss = 0.006349057890474796
iteration 215, loss = 0.006536375731229782
iteration 216, loss = 0.007520778104662895
iteration 217, loss = 0.006204608827829361
iteration 218, loss = 0.006203537341207266
iteration 219, loss = 0.007031979970633984
iteration 220, loss = 0.006207051686942577
iteration 221, loss = 0.006450868211686611
iteration 222, loss = 0.008180253207683563
iteration 223, loss = 0.00610395846888423
iteration 224, loss = 0.007045192644000053
iteration 225, loss = 0.006649711634963751
iteration 226, loss = 0.008042729459702969
iteration 227, loss = 0.006160300690680742
iteration 228, loss = 0.0060437931679189205
iteration 229, loss = 0.006818044930696487
iteration 230, loss = 0.006222384516149759
iteration 231, loss = 0.007480592466890812
iteration 232, loss = 0.009169848635792732
iteration 233, loss = 0.006697430741041899
iteration 234, loss = 0.006280812434852123
iteration 235, loss = 0.006392055191099644
iteration 236, loss = 0.00805965531617403
iteration 237, loss = 0.007667950354516506
iteration 238, loss = 0.006851758807897568
iteration 239, loss = 0.0065706148743629456
iteration 240, loss = 0.006065803579986095
iteration 241, loss = 0.007494793739169836
iteration 242, loss = 0.007432206533849239
iteration 243, loss = 0.00645435368642211
iteration 244, loss = 0.00787586160004139
iteration 245, loss = 0.006586775183677673
iteration 246, loss = 0.006354156881570816
iteration 247, loss = 0.00820059236139059
iteration 248, loss = 0.006758696865290403
iteration 249, loss = 0.007840145379304886
iteration 250, loss = 0.006313761230558157
iteration 251, loss = 0.007591810077428818
iteration 252, loss = 0.006541702896356583
iteration 253, loss = 0.006571203004568815
iteration 254, loss = 0.006121156271547079
iteration 255, loss = 0.009104947559535503
iteration 256, loss = 0.006623656488955021
iteration 257, loss = 0.006443135440349579
iteration 258, loss = 0.006355586927384138
iteration 259, loss = 0.0059861536137759686
iteration 260, loss = 0.00756843900308013
iteration 261, loss = 0.0065851882100105286
iteration 262, loss = 0.006603179965168238
iteration 263, loss = 0.007407594472169876
iteration 264, loss = 0.006659029982984066
iteration 265, loss = 0.006320864427834749
iteration 266, loss = 0.006620633881539106
iteration 267, loss = 0.006009002681821585
iteration 268, loss = 0.009551480412483215
iteration 269, loss = 0.006901515647768974
iteration 270, loss = 0.0068322112783789635
iteration 271, loss = 0.007597206626087427
iteration 272, loss = 0.006731169298291206
iteration 273, loss = 0.006391611881554127
iteration 274, loss = 0.0061519090086221695
iteration 275, loss = 0.006549446377903223
iteration 276, loss = 0.006255537737160921
iteration 277, loss = 0.008419197052717209
iteration 278, loss = 0.007159575819969177
iteration 279, loss = 0.0064645204693078995
iteration 280, loss = 0.007479663006961346
iteration 281, loss = 0.0069599575363099575
iteration 282, loss = 0.00943824090063572
iteration 283, loss = 0.005960001610219479
iteration 284, loss = 0.007744353264570236
iteration 285, loss = 0.006224369630217552
iteration 286, loss = 0.006491767708212137
iteration 287, loss = 0.007961726747453213
iteration 288, loss = 0.006848430261015892
iteration 289, loss = 0.006701016798615456
iteration 290, loss = 0.006058143451809883
iteration 291, loss = 0.006718933116644621
iteration 292, loss = 0.00640709837898612
iteration 293, loss = 0.006266869138926268
iteration 294, loss = 0.005961504764854908
iteration 295, loss = 0.00834695715457201
iteration 296, loss = 0.009494781494140625
iteration 297, loss = 0.0067663309164345264
iteration 298, loss = 0.009399710223078728
iteration 299, loss = 0.007226553745567799
iteration 300, loss = 0.007789535913616419
iteration 1, loss = 0.006327506620436907
iteration 2, loss = 0.0062560513615608215
iteration 3, loss = 0.0069185285829007626
iteration 4, loss = 0.006364576984196901
iteration 5, loss = 0.006145938765257597
iteration 6, loss = 0.007070699241012335
iteration 7, loss = 0.007839717902243137
iteration 8, loss = 0.007052638102322817
iteration 9, loss = 0.006233756896108389
iteration 10, loss = 0.006367279216647148
iteration 11, loss = 0.006240991409868002
iteration 12, loss = 0.007040655706077814
iteration 13, loss = 0.009271885268390179
iteration 14, loss = 0.00651888083666563
iteration 15, loss = 0.006088547874242067
iteration 16, loss = 0.0072897644713521
iteration 17, loss = 0.006752248853445053
iteration 18, loss = 0.006481262855231762
iteration 19, loss = 0.0063161603175103664
iteration 20, loss = 0.007600955665111542
iteration 21, loss = 0.007535040378570557
iteration 22, loss = 0.006206892896443605
iteration 23, loss = 0.008161087520420551
iteration 24, loss = 0.0094281155616045
iteration 25, loss = 0.008414794690907001
iteration 26, loss = 0.009506503120064735
iteration 27, loss = 0.007653545122593641
iteration 28, loss = 0.006551109720021486
iteration 29, loss = 0.0069846040569245815
iteration 30, loss = 0.007870514877140522
iteration 31, loss = 0.008798710070550442
iteration 32, loss = 0.008314857259392738
iteration 33, loss = 0.006640111096203327
iteration 34, loss = 0.007917419075965881
iteration 35, loss = 0.008173675276339054
iteration 36, loss = 0.006834171246737242
iteration 37, loss = 0.006047334987670183
iteration 38, loss = 0.006352278403937817
iteration 39, loss = 0.0062073757871985435
iteration 40, loss = 0.006142099387943745
iteration 41, loss = 0.006159380543977022
iteration 42, loss = 0.00641319714486599
iteration 43, loss = 0.006470717489719391
iteration 44, loss = 0.006339238025248051
iteration 45, loss = 0.006729491520673037
iteration 46, loss = 0.00820852629840374
iteration 47, loss = 0.006320524495095015
iteration 48, loss = 0.006064884830266237
iteration 49, loss = 0.007690628059208393
iteration 50, loss = 0.007455413695424795
iteration 51, loss = 0.0076745543628931046
iteration 52, loss = 0.006663828156888485
iteration 53, loss = 0.00678615178912878
iteration 54, loss = 0.006170410662889481
iteration 55, loss = 0.007361085619777441
iteration 56, loss = 0.007077096030116081
iteration 57, loss = 0.0066025531850755215
iteration 58, loss = 0.005995823536068201
iteration 59, loss = 0.006513512693345547
iteration 60, loss = 0.006893672980368137
iteration 61, loss = 0.006643460597842932
iteration 62, loss = 0.0062942663207650185
iteration 63, loss = 0.005819778889417648
iteration 64, loss = 0.00648684985935688
iteration 65, loss = 0.007998119108378887
iteration 66, loss = 0.006209933664649725
iteration 67, loss = 0.006592550314962864
iteration 68, loss = 0.005973479710519314
iteration 69, loss = 0.007506829220801592
iteration 70, loss = 0.0075023421086370945
iteration 71, loss = 0.00627097487449646
iteration 72, loss = 0.006099759601056576
iteration 73, loss = 0.006160266697406769
iteration 74, loss = 0.006625035777688026
iteration 75, loss = 0.007663358002901077
iteration 76, loss = 0.006306491792201996
iteration 77, loss = 0.006278883200138807
iteration 78, loss = 0.006952421274036169
iteration 79, loss = 0.006100624334067106
iteration 80, loss = 0.0071657090447843075
iteration 81, loss = 0.00640934007242322
iteration 82, loss = 0.008136729709804058
iteration 83, loss = 0.006521759554743767
iteration 84, loss = 0.008082727901637554
iteration 85, loss = 0.006364257074892521
iteration 86, loss = 0.006953141186386347
iteration 87, loss = 0.006043895147740841
iteration 88, loss = 0.008292272686958313
iteration 89, loss = 0.008211277425289154
iteration 90, loss = 0.008798600174486637
iteration 91, loss = 0.007362438831478357
iteration 92, loss = 0.007684287615120411
iteration 93, loss = 0.0065184300765395164
iteration 94, loss = 0.006042267195880413
iteration 95, loss = 0.00680753868073225
iteration 96, loss = 0.008225594647228718
iteration 97, loss = 0.006252464838325977
iteration 98, loss = 0.007563712075352669
iteration 99, loss = 0.008527602069079876
iteration 100, loss = 0.008141519501805305
iteration 101, loss = 0.007877870462834835
iteration 102, loss = 0.006863143295049667
iteration 103, loss = 0.006615105085074902
iteration 104, loss = 0.00991897750645876
iteration 105, loss = 0.006181168369948864
iteration 106, loss = 0.009348906576633453
iteration 107, loss = 0.007573510520160198
iteration 108, loss = 0.00632218923419714
iteration 109, loss = 0.006588143762201071
iteration 110, loss = 0.006028562784194946
iteration 111, loss = 0.010698412545025349
iteration 112, loss = 0.006495527923107147
iteration 113, loss = 0.008096219040453434
iteration 114, loss = 0.006624999921768904
iteration 115, loss = 0.007562714163213968
iteration 116, loss = 0.00844988040626049
iteration 117, loss = 0.006775474175810814
iteration 118, loss = 0.00926154013723135
iteration 119, loss = 0.006770336534827948
iteration 120, loss = 0.006998428143560886
iteration 121, loss = 0.005964566953480244
iteration 122, loss = 0.00638437969610095
iteration 123, loss = 0.00614599883556366
iteration 124, loss = 0.006356821395456791
iteration 125, loss = 0.007800357881933451
iteration 126, loss = 0.007873944006860256
iteration 127, loss = 0.00651213014498353
iteration 128, loss = 0.0068925535306334496
iteration 129, loss = 0.006402004510164261
iteration 130, loss = 0.0060157207772135735
iteration 131, loss = 0.006398601457476616
iteration 132, loss = 0.00633385730907321
iteration 133, loss = 0.006023027002811432
iteration 134, loss = 0.006450267508625984
iteration 135, loss = 0.006474640686064959
iteration 136, loss = 0.006135957315564156
iteration 137, loss = 0.006314424332231283
iteration 138, loss = 0.005992337130010128
iteration 139, loss = 0.00627979077398777
iteration 140, loss = 0.0077232965268194675
iteration 141, loss = 0.00629170797765255
iteration 142, loss = 0.006364206317812204
iteration 143, loss = 0.006113737355917692
iteration 144, loss = 0.006887683644890785
iteration 145, loss = 0.0062902322970330715
iteration 146, loss = 0.006558924447745085
iteration 147, loss = 0.009344068355858326
iteration 148, loss = 0.006282323971390724
iteration 149, loss = 0.007185290101915598
iteration 150, loss = 0.007400111760944128
iteration 151, loss = 0.007760139182209969
iteration 152, loss = 0.006637962535023689
iteration 153, loss = 0.008273736573755741
iteration 154, loss = 0.007707209326326847
iteration 155, loss = 0.007800248451530933
iteration 156, loss = 0.006157162599265575
iteration 157, loss = 0.0060174427926540375
iteration 158, loss = 0.006519350688904524
iteration 159, loss = 0.006158455274999142
iteration 160, loss = 0.009584645740687847
iteration 161, loss = 0.006305402144789696
iteration 162, loss = 0.0063295261934399605
iteration 163, loss = 0.00640864996239543
iteration 164, loss = 0.007796602789312601
iteration 165, loss = 0.006114809773862362
iteration 166, loss = 0.005947478115558624
iteration 167, loss = 0.006038963329046965
iteration 168, loss = 0.0068155499175190926
iteration 169, loss = 0.0067694056779146194
iteration 170, loss = 0.006482277996838093
iteration 171, loss = 0.007154643069952726
iteration 172, loss = 0.00681017991155386
iteration 173, loss = 0.006689608097076416
iteration 174, loss = 0.0064108725637197495
iteration 175, loss = 0.0059974780306220055
iteration 176, loss = 0.007658338639885187
iteration 177, loss = 0.006499083247035742
iteration 178, loss = 0.006224616430699825
iteration 179, loss = 0.006428818218410015
iteration 180, loss = 0.007221250794827938
iteration 181, loss = 0.007670250721275806
iteration 182, loss = 0.007900062017142773
iteration 183, loss = 0.00955772865563631
iteration 184, loss = 0.005999417044222355
iteration 185, loss = 0.006806593853980303
iteration 186, loss = 0.00629040040075779
iteration 187, loss = 0.006198359653353691
iteration 188, loss = 0.006520608905702829
iteration 189, loss = 0.006239238195121288
iteration 190, loss = 0.00666438601911068
iteration 191, loss = 0.006611586548388004
iteration 192, loss = 0.006673392374068499
iteration 193, loss = 0.006229551974684
iteration 194, loss = 0.008463156409561634
iteration 195, loss = 0.006239129696041346
iteration 196, loss = 0.007906412705779076
iteration 197, loss = 0.006610216107219458
iteration 198, loss = 0.008288838900625706
iteration 199, loss = 0.006106287706643343
iteration 200, loss = 0.008598371408879757
iteration 201, loss = 0.005992575082927942
iteration 202, loss = 0.006171748507767916
iteration 203, loss = 0.00614535715430975
iteration 204, loss = 0.00606512650847435
iteration 205, loss = 0.00601974967867136
iteration 206, loss = 0.006468679290264845
iteration 207, loss = 0.007973995059728622
iteration 208, loss = 0.011209181509912014
iteration 209, loss = 0.007531759794801474
iteration 210, loss = 0.006587402895092964
iteration 211, loss = 0.0065801991149783134
iteration 212, loss = 0.005811626091599464
iteration 213, loss = 0.010738528333604336
iteration 214, loss = 0.006896035745739937
iteration 215, loss = 0.006405992899090052
iteration 216, loss = 0.006501182913780212
iteration 217, loss = 0.007543067913502455
iteration 218, loss = 0.00904637947678566
iteration 219, loss = 0.006828620098531246
iteration 220, loss = 0.006101409438997507
iteration 221, loss = 0.0061130947433412075
iteration 222, loss = 0.008935421705245972
iteration 223, loss = 0.006240115500986576
iteration 224, loss = 0.006183622404932976
iteration 225, loss = 0.006390366703271866
iteration 226, loss = 0.0062165227718651295
iteration 227, loss = 0.008294867351651192
iteration 228, loss = 0.007613072637468576
iteration 229, loss = 0.006195207592099905
iteration 230, loss = 0.006055382080376148
iteration 231, loss = 0.005964517127722502
iteration 232, loss = 0.007737446576356888
iteration 233, loss = 0.00620411429554224
iteration 234, loss = 0.00645969994366169
iteration 235, loss = 0.00705484626814723
iteration 236, loss = 0.006865783594548702
iteration 237, loss = 0.0062138959765434265
iteration 238, loss = 0.006923052482306957
iteration 239, loss = 0.005945974495261908
iteration 240, loss = 0.008023206144571304
iteration 241, loss = 0.006657054647803307
iteration 242, loss = 0.00814454909414053
iteration 243, loss = 0.006120733451098204
iteration 244, loss = 0.0072562117129564285
iteration 245, loss = 0.0061861006543040276
iteration 246, loss = 0.006006134208291769
iteration 247, loss = 0.007516736164689064
iteration 248, loss = 0.006545640528202057
iteration 249, loss = 0.006132748909294605
iteration 250, loss = 0.00646544061601162
iteration 251, loss = 0.006683693267405033
iteration 252, loss = 0.0079796826466918
iteration 253, loss = 0.007936633192002773
iteration 254, loss = 0.009217817336320877
iteration 255, loss = 0.006525459233671427
iteration 256, loss = 0.0091698057949543
iteration 257, loss = 0.006607189774513245
iteration 258, loss = 0.006121346727013588
iteration 259, loss = 0.007881100289523602
iteration 260, loss = 0.006232425570487976
iteration 261, loss = 0.006558760069310665
iteration 262, loss = 0.006775781977921724
iteration 263, loss = 0.009266426786780357
iteration 264, loss = 0.006710306275635958
iteration 265, loss = 0.0058433436788618565
iteration 266, loss = 0.006858671549707651
iteration 267, loss = 0.007849853485822678
iteration 268, loss = 0.007870220579206944
iteration 269, loss = 0.005902589298784733
iteration 270, loss = 0.006556193344295025
iteration 271, loss = 0.006300592329353094
iteration 272, loss = 0.006805808749049902
iteration 273, loss = 0.006075944285839796
iteration 274, loss = 0.009214338846504688
iteration 275, loss = 0.006570388562977314
iteration 276, loss = 0.007595335599035025
iteration 277, loss = 0.007066352758556604
iteration 278, loss = 0.006573011167347431
iteration 279, loss = 0.00611184211447835
iteration 280, loss = 0.006582236848771572
iteration 281, loss = 0.006204666569828987
iteration 282, loss = 0.0060796733014285564
iteration 283, loss = 0.005875465460121632
iteration 284, loss = 0.00633052084594965
iteration 285, loss = 0.006409956142306328
iteration 286, loss = 0.008465448394417763
iteration 287, loss = 0.006534033454954624
iteration 288, loss = 0.006615151651203632
iteration 289, loss = 0.0064246864058077335
iteration 290, loss = 0.007045416627079248
iteration 291, loss = 0.006445326842367649
iteration 292, loss = 0.005994780920445919
iteration 293, loss = 0.006478338968008757
iteration 294, loss = 0.00670013390481472
iteration 295, loss = 0.0060611567460000515
iteration 296, loss = 0.007850625552237034
iteration 297, loss = 0.006140297278761864
iteration 298, loss = 0.007834896445274353
iteration 299, loss = 0.00690774992108345
iteration 300, loss = 0.006829817779362202
iteration 1, loss = 0.006348021328449249
iteration 2, loss = 0.00627706665545702
iteration 3, loss = 0.006618437357246876
iteration 4, loss = 0.0059923576191067696
iteration 5, loss = 0.006210357416421175
iteration 6, loss = 0.007876303978264332
iteration 7, loss = 0.0065542967058718204
iteration 8, loss = 0.006044212728738785
iteration 9, loss = 0.007972407154738903
iteration 10, loss = 0.006246490869671106
iteration 11, loss = 0.006205110810697079
iteration 12, loss = 0.006401685532182455
iteration 13, loss = 0.007847221568226814
iteration 14, loss = 0.006173975765705109
iteration 15, loss = 0.008393242955207825
iteration 16, loss = 0.006269396282732487
iteration 17, loss = 0.006755708251148462
iteration 18, loss = 0.007134194485843182
iteration 19, loss = 0.006229913327842951
iteration 20, loss = 0.006220276467502117
iteration 21, loss = 0.007672506384551525
iteration 22, loss = 0.006478868890553713
iteration 23, loss = 0.006286303978413343
iteration 24, loss = 0.006500106304883957
iteration 25, loss = 0.007695469073951244
iteration 26, loss = 0.00668417988345027
iteration 27, loss = 0.007617633324116468
iteration 28, loss = 0.0066142575815320015
iteration 29, loss = 0.006607294548302889
iteration 30, loss = 0.006895177997648716
iteration 31, loss = 0.006548592355102301
iteration 32, loss = 0.007427365053445101
iteration 33, loss = 0.006193107925355434
iteration 34, loss = 0.007319063879549503
iteration 35, loss = 0.006741549354046583
iteration 36, loss = 0.008216822519898415
iteration 37, loss = 0.006808327976614237
iteration 38, loss = 0.010372927412390709
iteration 39, loss = 0.009280812926590443
iteration 40, loss = 0.006115418393164873
iteration 41, loss = 0.008045925758779049
iteration 42, loss = 0.009907668456435204
iteration 43, loss = 0.006431121379137039
iteration 44, loss = 0.006248045712709427
iteration 45, loss = 0.0071306535974144936
iteration 46, loss = 0.007019250188022852
iteration 47, loss = 0.00761299068108201
iteration 48, loss = 0.007357242051512003
iteration 49, loss = 0.006175060756504536
iteration 50, loss = 0.00949365645647049
iteration 51, loss = 0.006198010873049498
iteration 52, loss = 0.006678248289972544
iteration 53, loss = 0.006122191436588764
iteration 54, loss = 0.005998270120471716
iteration 55, loss = 0.006007545627653599
iteration 56, loss = 0.00706774927675724
iteration 57, loss = 0.009974144399166107
iteration 58, loss = 0.007930208928883076
iteration 59, loss = 0.006482464261353016
iteration 60, loss = 0.006138517055660486
iteration 61, loss = 0.007835227064788342
iteration 62, loss = 0.006117707118391991
iteration 63, loss = 0.006135667208582163
iteration 64, loss = 0.006333691533654928
iteration 65, loss = 0.006095335818827152
iteration 66, loss = 0.005836821626871824
iteration 67, loss = 0.0072956383228302
iteration 68, loss = 0.006063437554985285
iteration 69, loss = 0.0061553060077130795
iteration 70, loss = 0.0074457344599068165
iteration 71, loss = 0.006870376877486706
iteration 72, loss = 0.008076231926679611
iteration 73, loss = 0.00639000441879034
iteration 74, loss = 0.007756934035569429
iteration 75, loss = 0.005967405159026384
iteration 76, loss = 0.005955362226814032
iteration 77, loss = 0.006430814042687416
iteration 78, loss = 0.008012554608285427
iteration 79, loss = 0.006442266050726175
iteration 80, loss = 0.006113425828516483
iteration 81, loss = 0.00585322268307209
iteration 82, loss = 0.006432803347706795
iteration 83, loss = 0.006431450601667166
iteration 84, loss = 0.00745369540527463
iteration 85, loss = 0.006620879750698805
iteration 86, loss = 0.006432289257645607
iteration 87, loss = 0.007160551846027374
iteration 88, loss = 0.00627667224034667
iteration 89, loss = 0.00613555870950222
iteration 90, loss = 0.009523116052150726
iteration 91, loss = 0.0061378744430840015
iteration 92, loss = 0.008413435891270638
iteration 93, loss = 0.006788973696529865
iteration 94, loss = 0.007586353458464146
iteration 95, loss = 0.006255125626921654
iteration 96, loss = 0.006129121407866478
iteration 97, loss = 0.00626143254339695
iteration 98, loss = 0.007016333285719156
iteration 99, loss = 0.006878692656755447
iteration 100, loss = 0.006202398799359798
iteration 101, loss = 0.008042728528380394
iteration 102, loss = 0.006599475163966417
iteration 103, loss = 0.006282567046582699
iteration 104, loss = 0.006415951065719128
iteration 105, loss = 0.006237199064344168
iteration 106, loss = 0.006773451808840036
iteration 107, loss = 0.006355212535709143
iteration 108, loss = 0.006356917321681976
iteration 109, loss = 0.007558096200227737
iteration 110, loss = 0.006346332374960184
iteration 111, loss = 0.007126434240490198
iteration 112, loss = 0.006458067335188389
iteration 113, loss = 0.0076185655780136585
iteration 114, loss = 0.0061391559429466724
iteration 115, loss = 0.007686857134103775
iteration 116, loss = 0.006123997271060944
iteration 117, loss = 0.008283841423690319
iteration 118, loss = 0.006169963628053665
iteration 119, loss = 0.0062622195109725
iteration 120, loss = 0.007499082945287228
iteration 121, loss = 0.009020785801112652
iteration 122, loss = 0.0060705747455358505
iteration 123, loss = 0.006504877936094999
iteration 124, loss = 0.006613573990762234
iteration 125, loss = 0.006032525096088648
iteration 126, loss = 0.006585345603525639
iteration 127, loss = 0.007090433035045862
iteration 128, loss = 0.0072564780712127686
iteration 129, loss = 0.00618897657841444
iteration 130, loss = 0.0059447139501571655
iteration 131, loss = 0.009129797108471394
iteration 132, loss = 0.006941359490156174
iteration 133, loss = 0.0064129880629479885
iteration 134, loss = 0.006657581776380539
iteration 135, loss = 0.007799990475177765
iteration 136, loss = 0.007220620755106211
iteration 137, loss = 0.007493667304515839
iteration 138, loss = 0.006447398569434881
iteration 139, loss = 0.00603519007563591
iteration 140, loss = 0.006113733630627394
iteration 141, loss = 0.009271180257201195
iteration 142, loss = 0.005806895904242992
iteration 143, loss = 0.006132049486041069
iteration 144, loss = 0.007033970206975937
iteration 145, loss = 0.007426098454743624
iteration 146, loss = 0.006983811967074871
iteration 147, loss = 0.006388407200574875
iteration 148, loss = 0.0063017020002007484
iteration 149, loss = 0.00641091400757432
iteration 150, loss = 0.005972667597234249
iteration 151, loss = 0.006262028589844704
iteration 152, loss = 0.006238905247300863
iteration 153, loss = 0.0062239267863333225
iteration 154, loss = 0.007492392789572477
iteration 155, loss = 0.006128315813839436
iteration 156, loss = 0.006029633339494467
iteration 157, loss = 0.006097253877669573
iteration 158, loss = 0.006011783145368099
iteration 159, loss = 0.00622160779312253
iteration 160, loss = 0.0059156352654099464
iteration 161, loss = 0.00722069526091218
iteration 162, loss = 0.005971891339868307
iteration 163, loss = 0.00601013470441103
iteration 164, loss = 0.006689945235848427
iteration 165, loss = 0.005975786596536636
iteration 166, loss = 0.0061881449073553085
iteration 167, loss = 0.005962972994893789
iteration 168, loss = 0.009475653059780598
iteration 169, loss = 0.006074240431189537
iteration 170, loss = 0.008083493448793888
iteration 171, loss = 0.009277512319386005
iteration 172, loss = 0.006319760344922543
iteration 173, loss = 0.006095915101468563
iteration 174, loss = 0.006142700091004372
iteration 175, loss = 0.0059126936830580235
iteration 176, loss = 0.0062573691830039024
iteration 177, loss = 0.006254249718040228
iteration 178, loss = 0.0062443409115076065
iteration 179, loss = 0.0058128247037529945
iteration 180, loss = 0.0059330035001039505
iteration 181, loss = 0.006226938683539629
iteration 182, loss = 0.006442544050514698
iteration 183, loss = 0.007693457417190075
iteration 184, loss = 0.006834086496382952
iteration 185, loss = 0.0062254793010652065
iteration 186, loss = 0.006220879033207893
iteration 187, loss = 0.006375645287334919
iteration 188, loss = 0.006341082975268364
iteration 189, loss = 0.006273546256124973
iteration 190, loss = 0.006007384974509478
iteration 191, loss = 0.007009997498244047
iteration 192, loss = 0.006140799727290869
iteration 193, loss = 0.0058118561282753944
iteration 194, loss = 0.0061143808998167515
iteration 195, loss = 0.006416418123990297
iteration 196, loss = 0.006311524659395218
iteration 197, loss = 0.009887734428048134
iteration 198, loss = 0.006199397146701813
iteration 199, loss = 0.006010666489601135
iteration 200, loss = 0.0064665391109883785
iteration 201, loss = 0.006649605464190245
iteration 202, loss = 0.007214731071144342
iteration 203, loss = 0.006787726655602455
iteration 204, loss = 0.007315688766539097
iteration 205, loss = 0.005916168913245201
iteration 206, loss = 0.0059739574790000916
iteration 207, loss = 0.007814772427082062
iteration 208, loss = 0.005917350761592388
iteration 209, loss = 0.00621498329564929
iteration 210, loss = 0.006732279434800148
iteration 211, loss = 0.009097778238356113
iteration 212, loss = 0.006607956252992153
iteration 213, loss = 0.00759093975648284
iteration 214, loss = 0.009982418268918991
iteration 215, loss = 0.005876197479665279
iteration 216, loss = 0.00805719569325447
iteration 217, loss = 0.006795231252908707
iteration 218, loss = 0.006601376459002495
iteration 219, loss = 0.007965806871652603
iteration 220, loss = 0.0057184938341379166
iteration 221, loss = 0.00595605093985796
iteration 222, loss = 0.006402617320418358
iteration 223, loss = 0.006387460045516491
iteration 224, loss = 0.006026512943208218
iteration 225, loss = 0.006188576575368643
iteration 226, loss = 0.006063107866793871
iteration 227, loss = 0.006242840085178614
iteration 228, loss = 0.007628514431416988
iteration 229, loss = 0.007229247596114874
iteration 230, loss = 0.006258131470531225
iteration 231, loss = 0.006746670696884394
iteration 232, loss = 0.007586250547319651
iteration 233, loss = 0.009572135284543037
iteration 234, loss = 0.006264876574277878
iteration 235, loss = 0.00788686703890562
iteration 236, loss = 0.008487808518111706
iteration 237, loss = 0.006846889853477478
iteration 238, loss = 0.007191868498921394
iteration 239, loss = 0.006715165451169014
iteration 240, loss = 0.006657244637608528
iteration 241, loss = 0.005986643489450216
iteration 242, loss = 0.010425040498375893
iteration 243, loss = 0.00813345704227686
iteration 244, loss = 0.006030507385730743
iteration 245, loss = 0.007518811617046595
iteration 246, loss = 0.006222108379006386
iteration 247, loss = 0.00651443749666214
iteration 248, loss = 0.006617147475481033
iteration 249, loss = 0.005697672255337238
iteration 250, loss = 0.005968220066279173
iteration 251, loss = 0.006302952766418457
iteration 252, loss = 0.006030241027474403
iteration 253, loss = 0.00775129022076726
iteration 254, loss = 0.00666721910238266
iteration 255, loss = 0.006157473661005497
iteration 256, loss = 0.00620596157386899
iteration 257, loss = 0.00914108194410801
iteration 258, loss = 0.007248682901263237
iteration 259, loss = 0.006562874652445316
iteration 260, loss = 0.006466938182711601
iteration 261, loss = 0.006436871364712715
iteration 262, loss = 0.006394083611667156
iteration 263, loss = 0.007544998079538345
iteration 264, loss = 0.007822226732969284
iteration 265, loss = 0.007506428752094507
iteration 266, loss = 0.006302325986325741
iteration 267, loss = 0.008946677669882774
iteration 268, loss = 0.006271937396377325
iteration 269, loss = 0.00793546438217163
iteration 270, loss = 0.005944648291915655
iteration 271, loss = 0.006080856081098318
iteration 272, loss = 0.005705997813493013
iteration 273, loss = 0.006420469842851162
iteration 274, loss = 0.007901843637228012
iteration 275, loss = 0.0059439376927912235
iteration 276, loss = 0.006234274711459875
iteration 277, loss = 0.009756646119058132
iteration 278, loss = 0.007467236835509539
iteration 279, loss = 0.0076348427683115005
iteration 280, loss = 0.00623062951490283
iteration 281, loss = 0.006882828660309315
iteration 282, loss = 0.005987632554024458
iteration 283, loss = 0.006277884356677532
iteration 284, loss = 0.006072639022022486
iteration 285, loss = 0.007879398763179779
iteration 286, loss = 0.007688462268561125
iteration 287, loss = 0.006476979702711105
iteration 288, loss = 0.007519377861171961
iteration 289, loss = 0.009202348068356514
iteration 290, loss = 0.0069684479385614395
iteration 291, loss = 0.008451838977634907
iteration 292, loss = 0.006003163289278746
iteration 293, loss = 0.005995340179651976
iteration 294, loss = 0.006153128109872341
iteration 295, loss = 0.006370995193719864
iteration 296, loss = 0.007425363175570965
iteration 297, loss = 0.006922923494130373
iteration 298, loss = 0.007668992970138788
iteration 299, loss = 0.006414509378373623
iteration 300, loss = 0.006456354632973671
iteration 1, loss = 0.006348093040287495
iteration 2, loss = 0.0064326380379498005
iteration 3, loss = 0.006484474986791611
iteration 4, loss = 0.00653622392565012
iteration 5, loss = 0.007394015323370695
iteration 6, loss = 0.006780717521905899
iteration 7, loss = 0.0060446662828326225
iteration 8, loss = 0.006339516025036573
iteration 9, loss = 0.007765558548271656
iteration 10, loss = 0.007917025126516819
iteration 11, loss = 0.007351200561970472
iteration 12, loss = 0.006739214528352022
iteration 13, loss = 0.006364735774695873
iteration 14, loss = 0.006434749811887741
iteration 15, loss = 0.006137733813375235
iteration 16, loss = 0.0075018140487372875
iteration 17, loss = 0.009005552157759666
iteration 18, loss = 0.008092774078249931
iteration 19, loss = 0.005818849429488182
iteration 20, loss = 0.005929892882704735
iteration 21, loss = 0.0068200984969735146
iteration 22, loss = 0.006182374898344278
iteration 23, loss = 0.006236714776605368
iteration 24, loss = 0.006076328922063112
iteration 25, loss = 0.007186626549810171
iteration 26, loss = 0.009224232286214828
iteration 27, loss = 0.009283057413995266
iteration 28, loss = 0.005724819842725992
iteration 29, loss = 0.0063500721007585526
iteration 30, loss = 0.006452515721321106
iteration 31, loss = 0.006285396404564381
iteration 32, loss = 0.006370702292770147
iteration 33, loss = 0.0069104572758078575
iteration 34, loss = 0.007918549701571465
iteration 35, loss = 0.006586764939129353
iteration 36, loss = 0.006385011598467827
iteration 37, loss = 0.009596552699804306
iteration 38, loss = 0.006190036423504353
iteration 39, loss = 0.0068879988975822926
iteration 40, loss = 0.006906129885464907
iteration 41, loss = 0.005775345955044031
iteration 42, loss = 0.00625843508169055
iteration 43, loss = 0.005978861358016729
iteration 44, loss = 0.00909719243645668
iteration 45, loss = 0.006918255239725113
iteration 46, loss = 0.006673876196146011
iteration 47, loss = 0.006296976935118437
iteration 48, loss = 0.006320219021290541
iteration 49, loss = 0.006198144983500242
iteration 50, loss = 0.007637744303792715
iteration 51, loss = 0.008925930596888065
iteration 52, loss = 0.0064908284693956375
iteration 53, loss = 0.0060362257063388824
iteration 54, loss = 0.0065993135794997215
iteration 55, loss = 0.006431025918573141
iteration 56, loss = 0.006434459239244461
iteration 57, loss = 0.007367034908384085
iteration 58, loss = 0.0062165153212845325
iteration 59, loss = 0.00613343296572566
iteration 60, loss = 0.006721304729580879
iteration 61, loss = 0.006284398026764393
iteration 62, loss = 0.007034520152956247
iteration 63, loss = 0.005890616215765476
iteration 64, loss = 0.006495591253042221
iteration 65, loss = 0.008059825748205185
iteration 66, loss = 0.006703633349388838
iteration 67, loss = 0.006766175851225853
iteration 68, loss = 0.0061143022030591965
iteration 69, loss = 0.006531624123454094
iteration 70, loss = 0.00646295165643096
iteration 71, loss = 0.0075531648471951485
iteration 72, loss = 0.007160731591284275
iteration 73, loss = 0.0087579395622015
iteration 74, loss = 0.005782782100141048
iteration 75, loss = 0.006509659346193075
iteration 76, loss = 0.006167677231132984
iteration 77, loss = 0.006322608795017004
iteration 78, loss = 0.006222729105502367
iteration 79, loss = 0.006473633460700512
iteration 80, loss = 0.006320703774690628
iteration 81, loss = 0.006676805205643177
iteration 82, loss = 0.006793232634663582
iteration 83, loss = 0.005957166198641062
iteration 84, loss = 0.008049679920077324
iteration 85, loss = 0.006121985614299774
iteration 86, loss = 0.009379872120916843
iteration 87, loss = 0.006356941536068916
iteration 88, loss = 0.006363647989928722
iteration 89, loss = 0.0062529402785003185
iteration 90, loss = 0.005790964234620333
iteration 91, loss = 0.0059419129975140095
iteration 92, loss = 0.0064087663777172565
iteration 93, loss = 0.005878021474927664
iteration 94, loss = 0.006317645777016878
iteration 95, loss = 0.007521644700318575
iteration 96, loss = 0.0064942906610667706
iteration 97, loss = 0.005937500856816769
iteration 98, loss = 0.0057666911743581295
iteration 99, loss = 0.006116730626672506
iteration 100, loss = 0.007517003919929266
iteration 101, loss = 0.005832581780850887
iteration 102, loss = 0.005999954417347908
iteration 103, loss = 0.0075885094702243805
iteration 104, loss = 0.006981539539992809
iteration 105, loss = 0.00577674200758338
iteration 106, loss = 0.0058191088028252125
iteration 107, loss = 0.005996245425194502
iteration 108, loss = 0.0064445482566952705
iteration 109, loss = 0.007525611203163862
iteration 110, loss = 0.008425566367805004
iteration 111, loss = 0.005961479153484106
iteration 112, loss = 0.007869705557823181
iteration 113, loss = 0.006144559942185879
iteration 114, loss = 0.005980751011520624
iteration 115, loss = 0.0073280357755720615
iteration 116, loss = 0.006524371914565563
iteration 117, loss = 0.007446896284818649
iteration 118, loss = 0.008344526402652264
iteration 119, loss = 0.0058964937925338745
iteration 120, loss = 0.006171436049044132
iteration 121, loss = 0.006817784626036882
iteration 122, loss = 0.006570256780833006
iteration 123, loss = 0.008390163071453571
iteration 124, loss = 0.009105621837079525
iteration 125, loss = 0.007891512475907803
iteration 126, loss = 0.006281150504946709
iteration 127, loss = 0.006172694731503725
iteration 128, loss = 0.0062444256618618965
iteration 129, loss = 0.005935905035585165
iteration 130, loss = 0.006213084328919649
iteration 131, loss = 0.006122355815023184
iteration 132, loss = 0.0061820256523787975
iteration 133, loss = 0.0060774702578783035
iteration 134, loss = 0.009140128269791603
iteration 135, loss = 0.005929096601903439
iteration 136, loss = 0.00599678372964263
iteration 137, loss = 0.008515848778188229
iteration 138, loss = 0.009295057505369186
iteration 139, loss = 0.008165214210748672
iteration 140, loss = 0.006453219801187515
iteration 141, loss = 0.009291145950555801
iteration 142, loss = 0.006935301702469587
iteration 143, loss = 0.009309252724051476
iteration 144, loss = 0.010742388665676117
iteration 145, loss = 0.005822074133902788
iteration 146, loss = 0.006234700325876474
iteration 147, loss = 0.006330190226435661
iteration 148, loss = 0.007276799064129591
iteration 149, loss = 0.006038261111825705
iteration 150, loss = 0.008735276758670807
iteration 151, loss = 0.006238863803446293
iteration 152, loss = 0.008464374579489231
iteration 153, loss = 0.005965032149106264
iteration 154, loss = 0.005824155639857054
iteration 155, loss = 0.006108790636062622
iteration 156, loss = 0.0058987499214708805
iteration 157, loss = 0.007380351889878511
iteration 158, loss = 0.006451709661632776
iteration 159, loss = 0.006464904174208641
iteration 160, loss = 0.006156941410154104
iteration 161, loss = 0.005780975800007582
iteration 162, loss = 0.006004436872899532
iteration 163, loss = 0.005992248188704252
iteration 164, loss = 0.0058776624500751495
iteration 165, loss = 0.006912047043442726
iteration 166, loss = 0.006506060715764761
iteration 167, loss = 0.006019825115799904
iteration 168, loss = 0.00589401600882411
iteration 169, loss = 0.007243214175105095
iteration 170, loss = 0.007921583019196987
iteration 171, loss = 0.007543870247900486
iteration 172, loss = 0.006718814373016357
iteration 173, loss = 0.00783830601722002
iteration 174, loss = 0.006211756728589535
iteration 175, loss = 0.005898480303585529
iteration 176, loss = 0.006217608228325844
iteration 177, loss = 0.00882202759385109
iteration 178, loss = 0.006343666929751635
iteration 179, loss = 0.005670925136655569
iteration 180, loss = 0.005943453870713711
iteration 181, loss = 0.008160831406712532
iteration 182, loss = 0.007896382361650467
iteration 183, loss = 0.006244115065783262
iteration 184, loss = 0.007529325783252716
iteration 185, loss = 0.006032855249941349
iteration 186, loss = 0.007203537505120039
iteration 187, loss = 0.00837443582713604
iteration 188, loss = 0.005776130128651857
iteration 189, loss = 0.006047475151717663
iteration 190, loss = 0.00766291469335556
iteration 191, loss = 0.0072807371616363525
iteration 192, loss = 0.006678126752376556
iteration 193, loss = 0.006006351672112942
iteration 194, loss = 0.007541050668805838
iteration 195, loss = 0.006114959716796875
iteration 196, loss = 0.005862602032721043
iteration 197, loss = 0.007137436419725418
iteration 198, loss = 0.006553614046424627
iteration 199, loss = 0.0061590843833982944
iteration 200, loss = 0.006064361892640591
iteration 201, loss = 0.006621035747230053
iteration 202, loss = 0.006108234170824289
iteration 203, loss = 0.007111565675586462
iteration 204, loss = 0.0064749279990792274
iteration 205, loss = 0.007502628955990076
iteration 206, loss = 0.00629272498190403
iteration 207, loss = 0.006041685119271278
iteration 208, loss = 0.0062109180726110935
iteration 209, loss = 0.005941025447100401
iteration 210, loss = 0.007210828363895416
iteration 211, loss = 0.006012278608977795
iteration 212, loss = 0.006259378045797348
iteration 213, loss = 0.006230381317436695
iteration 214, loss = 0.007664429489523172
iteration 215, loss = 0.00830869935452938
iteration 216, loss = 0.0065938993357121944
iteration 217, loss = 0.007248269394040108
iteration 218, loss = 0.005979660898447037
iteration 219, loss = 0.005717382300645113
iteration 220, loss = 0.00628347834572196
iteration 221, loss = 0.0063009923323988914
iteration 222, loss = 0.005925042554736137
iteration 223, loss = 0.005825205240398645
iteration 224, loss = 0.007262448780238628
iteration 225, loss = 0.006068310234695673
iteration 226, loss = 0.005843290127813816
iteration 227, loss = 0.006043043453246355
iteration 228, loss = 0.006988617591559887
iteration 229, loss = 0.0060363090597093105
iteration 230, loss = 0.006070076022297144
iteration 231, loss = 0.006093582138419151
iteration 232, loss = 0.006594664882868528
iteration 233, loss = 0.006333054043352604
iteration 234, loss = 0.006087413057684898
iteration 235, loss = 0.006081935949623585
iteration 236, loss = 0.006446558516472578
iteration 237, loss = 0.006046174094080925
iteration 238, loss = 0.00661085732281208
iteration 239, loss = 0.005957131274044514
iteration 240, loss = 0.007311950903385878
iteration 241, loss = 0.006271434016525745
iteration 242, loss = 0.0070426082238554955
iteration 243, loss = 0.00622395658865571
iteration 244, loss = 0.005712872836738825
iteration 245, loss = 0.006127586588263512
iteration 246, loss = 0.0057062748819589615
iteration 247, loss = 0.007689138874411583
iteration 248, loss = 0.007130865473300219
iteration 249, loss = 0.00756305456161499
iteration 250, loss = 0.00864473357796669
iteration 251, loss = 0.007647184655070305
iteration 252, loss = 0.006061298307031393
iteration 253, loss = 0.00648447684943676
iteration 254, loss = 0.006051305215805769
iteration 255, loss = 0.005872929934412241
iteration 256, loss = 0.005724539514631033
iteration 257, loss = 0.006140605546534061
iteration 258, loss = 0.005789550952613354
iteration 259, loss = 0.006115576718002558
iteration 260, loss = 0.005908265709877014
iteration 261, loss = 0.008338483981788158
iteration 262, loss = 0.005749849136918783
iteration 263, loss = 0.005979226902127266
iteration 264, loss = 0.005791795440018177
iteration 265, loss = 0.005699009634554386
iteration 266, loss = 0.00778901856392622
iteration 267, loss = 0.006146946921944618
iteration 268, loss = 0.0060718245804309845
iteration 269, loss = 0.0062340665608644485
iteration 270, loss = 0.007877521216869354
iteration 271, loss = 0.007703229319304228
iteration 272, loss = 0.007094519678503275
iteration 273, loss = 0.005830238573253155
iteration 274, loss = 0.006655331701040268
iteration 275, loss = 0.006683004088699818
iteration 276, loss = 0.005986342206597328
iteration 277, loss = 0.008938404731452465
iteration 278, loss = 0.006976299919188023
iteration 279, loss = 0.006147080101072788
iteration 280, loss = 0.007086274679750204
iteration 281, loss = 0.00627336697652936
iteration 282, loss = 0.0075342683121562
iteration 283, loss = 0.007571682333946228
iteration 284, loss = 0.006531428545713425
iteration 285, loss = 0.006141141057014465
iteration 286, loss = 0.008530913852155209
iteration 287, loss = 0.006743128411471844
iteration 288, loss = 0.005948161706328392
iteration 289, loss = 0.006063778884708881
iteration 290, loss = 0.008863786235451698
iteration 291, loss = 0.006134149618446827
iteration 292, loss = 0.006022290326654911
iteration 293, loss = 0.007260467391461134
iteration 294, loss = 0.0058839572593569756
iteration 295, loss = 0.006603518035262823
iteration 296, loss = 0.007489435840398073
iteration 297, loss = 0.005955639760941267
iteration 298, loss = 0.006529517471790314
iteration 299, loss = 0.006320360116660595
iteration 300, loss = 0.007807789370417595
iteration 1, loss = 0.006307031027972698
iteration 2, loss = 0.0064493766985833645
iteration 3, loss = 0.005920611787587404
iteration 4, loss = 0.0057462467812001705
iteration 5, loss = 0.00584775023162365
iteration 6, loss = 0.008004401810467243
iteration 7, loss = 0.005731004290282726
iteration 8, loss = 0.00675713736563921
iteration 9, loss = 0.006128248758614063
iteration 10, loss = 0.0070733423344790936
iteration 11, loss = 0.006410136818885803
iteration 12, loss = 0.005982300266623497
iteration 13, loss = 0.009279265999794006
iteration 14, loss = 0.00613965792581439
iteration 15, loss = 0.006462517660111189
iteration 16, loss = 0.005720265209674835
iteration 17, loss = 0.0059989504516124725
iteration 18, loss = 0.005786234978586435
iteration 19, loss = 0.006921879947185516
iteration 20, loss = 0.006071885582059622
iteration 21, loss = 0.008767385967075825
iteration 22, loss = 0.0072805085219442844
iteration 23, loss = 0.0066163912415504456
iteration 24, loss = 0.006228499114513397
iteration 25, loss = 0.0067423004657030106
iteration 26, loss = 0.006492928601801395
iteration 27, loss = 0.006133073475211859
iteration 28, loss = 0.006957711651921272
iteration 29, loss = 0.006299316417425871
iteration 30, loss = 0.006671586073935032
iteration 31, loss = 0.006201987620443106
iteration 32, loss = 0.0058838906697928905
iteration 33, loss = 0.005954338703304529
iteration 34, loss = 0.007406966760754585
iteration 35, loss = 0.0057129403576254845
iteration 36, loss = 0.006309820339083672
iteration 37, loss = 0.006014458369463682
iteration 38, loss = 0.006589092314243317
iteration 39, loss = 0.00581318186596036
iteration 40, loss = 0.006352431606501341
iteration 41, loss = 0.005724059417843819
iteration 42, loss = 0.00900796428322792
iteration 43, loss = 0.006987000349909067
iteration 44, loss = 0.006420603021979332
iteration 45, loss = 0.005628009792417288
iteration 46, loss = 0.007228318136185408
iteration 47, loss = 0.008055985905230045
iteration 48, loss = 0.006467553321272135
iteration 49, loss = 0.006568246521055698
iteration 50, loss = 0.006783378776162863
iteration 51, loss = 0.006115301512181759
iteration 52, loss = 0.00593290850520134
iteration 53, loss = 0.006601525936275721
iteration 54, loss = 0.006060618907213211
iteration 55, loss = 0.0062808627262711525
iteration 56, loss = 0.006296078208833933
iteration 57, loss = 0.005789622198790312
iteration 58, loss = 0.006340129300951958
iteration 59, loss = 0.005935901775956154
iteration 60, loss = 0.005735690705478191
iteration 61, loss = 0.006706145592033863
iteration 62, loss = 0.006254450883716345
iteration 63, loss = 0.006217095535248518
iteration 64, loss = 0.006011823192238808
iteration 65, loss = 0.006178883370012045
iteration 66, loss = 0.00650032190605998
iteration 67, loss = 0.0063112154603004456
iteration 68, loss = 0.005992745980620384
iteration 69, loss = 0.007703317329287529
iteration 70, loss = 0.006439306773245335
iteration 71, loss = 0.005970736034214497
iteration 72, loss = 0.006882413290441036
iteration 73, loss = 0.006651440169662237
iteration 74, loss = 0.0057984828017652035
iteration 75, loss = 0.006133450195193291
iteration 76, loss = 0.005880100652575493
iteration 77, loss = 0.006401093211025
iteration 78, loss = 0.005882886704057455
iteration 79, loss = 0.005743269808590412
iteration 80, loss = 0.006845562253147364
iteration 81, loss = 0.005835291929543018
iteration 82, loss = 0.005730974022299051
iteration 83, loss = 0.007360362447798252
iteration 84, loss = 0.006236837711185217
iteration 85, loss = 0.007690892554819584
iteration 86, loss = 0.006143955513834953
iteration 87, loss = 0.0062333084642887115
iteration 88, loss = 0.00616026995703578
iteration 89, loss = 0.005670300219208002
iteration 90, loss = 0.005926324985921383
iteration 91, loss = 0.005719478707760572
iteration 92, loss = 0.006031719967722893
iteration 93, loss = 0.006440064404159784
iteration 94, loss = 0.009111584164202213
iteration 95, loss = 0.006027299910783768
iteration 96, loss = 0.011575804091989994
iteration 97, loss = 0.005998014938086271
iteration 98, loss = 0.0063752043060958385
iteration 99, loss = 0.0070317271165549755
iteration 100, loss = 0.006003084126859903
iteration 101, loss = 0.0059316204860806465
iteration 102, loss = 0.006385824643075466
iteration 103, loss = 0.008581390604376793
iteration 104, loss = 0.006097377277910709
iteration 105, loss = 0.005796853918582201
iteration 106, loss = 0.006128254346549511
iteration 107, loss = 0.006277686450630426
iteration 108, loss = 0.005971213802695274
iteration 109, loss = 0.007461057975888252
iteration 110, loss = 0.00946188997477293
iteration 111, loss = 0.007056247442960739
iteration 112, loss = 0.0069127073511481285
iteration 113, loss = 0.007717364002019167
iteration 114, loss = 0.0060834926553070545
iteration 115, loss = 0.0067992908880114555
iteration 116, loss = 0.007642844691872597
iteration 117, loss = 0.00634963670745492
iteration 118, loss = 0.006925825960934162
iteration 119, loss = 0.0071263303980231285
iteration 120, loss = 0.007424827665090561
iteration 121, loss = 0.007592008914798498
iteration 122, loss = 0.007511147763580084
iteration 123, loss = 0.007836155593395233
iteration 124, loss = 0.006039212923496962
iteration 125, loss = 0.00623603630810976
iteration 126, loss = 0.006036676466464996
iteration 127, loss = 0.009076147340238094
iteration 128, loss = 0.006121024489402771
iteration 129, loss = 0.007409346289932728
iteration 130, loss = 0.005792664363980293
iteration 131, loss = 0.005905561614781618
iteration 132, loss = 0.006005578674376011
iteration 133, loss = 0.005677519366145134
iteration 134, loss = 0.00594905624166131
iteration 135, loss = 0.005704817362129688
iteration 136, loss = 0.0074323974549770355
iteration 137, loss = 0.007580101490020752
iteration 138, loss = 0.006655002944171429
iteration 139, loss = 0.006670767441391945
iteration 140, loss = 0.005708007141947746
iteration 141, loss = 0.007183672394603491
iteration 142, loss = 0.010398629121482372
iteration 143, loss = 0.006003569345921278
iteration 144, loss = 0.0059290057979524136
iteration 145, loss = 0.005873838905245066
iteration 146, loss = 0.007882204838097095
iteration 147, loss = 0.006400610785931349
iteration 148, loss = 0.006781601347029209
iteration 149, loss = 0.006277089007198811
iteration 150, loss = 0.007538062985986471
iteration 151, loss = 0.006246780510991812
iteration 152, loss = 0.007517987862229347
iteration 153, loss = 0.008878919295966625
iteration 154, loss = 0.006672026123851538
iteration 155, loss = 0.006004191003739834
iteration 156, loss = 0.006253709550946951
iteration 157, loss = 0.00714236032217741
iteration 158, loss = 0.0092212725430727
iteration 159, loss = 0.006462528370320797
iteration 160, loss = 0.006398307625204325
iteration 161, loss = 0.006141596473753452
iteration 162, loss = 0.00586848147213459
iteration 163, loss = 0.0069014388136565685
iteration 164, loss = 0.006002993322908878
iteration 165, loss = 0.00920054130256176
iteration 166, loss = 0.0058664181269705296
iteration 167, loss = 0.00563029944896698
iteration 168, loss = 0.0059846071526408195
iteration 169, loss = 0.006231160368770361
iteration 170, loss = 0.005950721446424723
iteration 171, loss = 0.006289583630859852
iteration 172, loss = 0.006101120728999376
iteration 173, loss = 0.0064760916866362095
iteration 174, loss = 0.007273407187312841
iteration 175, loss = 0.008558442816138268
iteration 176, loss = 0.007179208565503359
iteration 177, loss = 0.005845343228429556
iteration 178, loss = 0.0060740020126104355
iteration 179, loss = 0.006306368391960859
iteration 180, loss = 0.006044278386980295
iteration 181, loss = 0.00710632186383009
iteration 182, loss = 0.005768181290477514
iteration 183, loss = 0.006274829618632793
iteration 184, loss = 0.00792620424181223
iteration 185, loss = 0.007179103791713715
iteration 186, loss = 0.005928257945924997
iteration 187, loss = 0.0060876295901834965
iteration 188, loss = 0.006428299006074667
iteration 189, loss = 0.007559194695204496
iteration 190, loss = 0.0079912468791008
iteration 191, loss = 0.006728722248226404
iteration 192, loss = 0.005938363261520863
iteration 193, loss = 0.007025979459285736
iteration 194, loss = 0.006210373714566231
iteration 195, loss = 0.0074526905082166195
iteration 196, loss = 0.0057492065243422985
iteration 197, loss = 0.006565092131495476
iteration 198, loss = 0.006126314401626587
iteration 199, loss = 0.005945141427218914
iteration 200, loss = 0.005993973929435015
iteration 201, loss = 0.007375330664217472
iteration 202, loss = 0.0057752700522542
iteration 203, loss = 0.007889374159276485
iteration 204, loss = 0.006850231438875198
iteration 205, loss = 0.006728884764015675
iteration 206, loss = 0.005875544622540474
iteration 207, loss = 0.00591558450832963
iteration 208, loss = 0.0061557842418551445
iteration 209, loss = 0.006152003537863493
iteration 210, loss = 0.00602560443803668
iteration 211, loss = 0.007616885006427765
iteration 212, loss = 0.008748237043619156
iteration 213, loss = 0.009042919613420963
iteration 214, loss = 0.006184267811477184
iteration 215, loss = 0.006164971739053726
iteration 216, loss = 0.008236936293542385
iteration 217, loss = 0.0057469699531793594
iteration 218, loss = 0.006889655254781246
iteration 219, loss = 0.006105550564825535
iteration 220, loss = 0.0062919664196670055
iteration 221, loss = 0.006581463385373354
iteration 222, loss = 0.007396082393825054
iteration 223, loss = 0.005676527041941881
iteration 224, loss = 0.007280691526830196
iteration 225, loss = 0.007277357392013073
iteration 226, loss = 0.006521629169583321
iteration 227, loss = 0.005943829193711281
iteration 228, loss = 0.00799371674656868
iteration 229, loss = 0.006275867577642202
iteration 230, loss = 0.010744068771600723
iteration 231, loss = 0.00572745269164443
iteration 232, loss = 0.006096044089645147
iteration 233, loss = 0.007402448449283838
iteration 234, loss = 0.0059180231764912605
iteration 235, loss = 0.006003642920404673
iteration 236, loss = 0.006806236691772938
iteration 237, loss = 0.006472153123468161
iteration 238, loss = 0.0061896247789263725
iteration 239, loss = 0.00814006756991148
iteration 240, loss = 0.005751254037022591
iteration 241, loss = 0.005583664868026972
iteration 242, loss = 0.006225290708243847
iteration 243, loss = 0.005750453099608421
iteration 244, loss = 0.00881850253790617
iteration 245, loss = 0.005528638139367104
iteration 246, loss = 0.0058564129285514355
iteration 247, loss = 0.006940457038581371
iteration 248, loss = 0.005873301066458225
iteration 249, loss = 0.0061987219378352165
iteration 250, loss = 0.008596380241215229
iteration 251, loss = 0.0072081433609128
iteration 252, loss = 0.009417771361768246
iteration 253, loss = 0.007479940075427294
iteration 254, loss = 0.0057866801507771015
iteration 255, loss = 0.006154485046863556
iteration 256, loss = 0.00609446270391345
iteration 257, loss = 0.007352144457399845
iteration 258, loss = 0.005611133761703968
iteration 259, loss = 0.005818357225507498
iteration 260, loss = 0.006295213475823402
iteration 261, loss = 0.007545669097453356
iteration 262, loss = 0.007063704077154398
iteration 263, loss = 0.005833563860505819
iteration 264, loss = 0.007136967498809099
iteration 265, loss = 0.008802514523267746
iteration 266, loss = 0.005971129983663559
iteration 267, loss = 0.0056890021078288555
iteration 268, loss = 0.0056798942387104034
iteration 269, loss = 0.007576667703688145
iteration 270, loss = 0.005796776618808508
iteration 271, loss = 0.006093959324061871
iteration 272, loss = 0.005732561461627483
iteration 273, loss = 0.0068963998928666115
iteration 274, loss = 0.009103631600737572
iteration 275, loss = 0.00722387433052063
iteration 276, loss = 0.0057531073689460754
iteration 277, loss = 0.006287303753197193
iteration 278, loss = 0.007199734449386597
iteration 279, loss = 0.006649328395724297
iteration 280, loss = 0.006124342326074839
iteration 281, loss = 0.005800947081297636
iteration 282, loss = 0.007203262764960527
iteration 283, loss = 0.005782641936093569
iteration 284, loss = 0.006710221990942955
iteration 285, loss = 0.005912062246352434
iteration 286, loss = 0.006006132811307907
iteration 287, loss = 0.005823977291584015
iteration 288, loss = 0.007522947620600462
iteration 289, loss = 0.005637510679662228
iteration 290, loss = 0.006651998497545719
iteration 291, loss = 0.00675654411315918
iteration 292, loss = 0.005814311560243368
iteration 293, loss = 0.007303223945200443
iteration 294, loss = 0.006119957193732262
iteration 295, loss = 0.00606993492692709
iteration 296, loss = 0.0058962698094546795
iteration 297, loss = 0.005883729085326195
iteration 298, loss = 0.006744059734046459
iteration 299, loss = 0.006096741184592247
iteration 300, loss = 0.005674970801919699
iteration 1, loss = 0.005981983616948128
iteration 2, loss = 0.006099145859479904
iteration 3, loss = 0.006120666861534119
iteration 4, loss = 0.006871363148093224
iteration 5, loss = 0.0087426183745265
iteration 6, loss = 0.007069767918437719
iteration 7, loss = 0.006232842803001404
iteration 8, loss = 0.00652144942432642
iteration 9, loss = 0.005618768744170666
iteration 10, loss = 0.006463184952735901
iteration 11, loss = 0.006406812462955713
iteration 12, loss = 0.005971299950033426
iteration 13, loss = 0.005971985869109631
iteration 14, loss = 0.00573688605800271
iteration 15, loss = 0.005787513218820095
iteration 16, loss = 0.008944710716605186
iteration 17, loss = 0.00569927366450429
iteration 18, loss = 0.006383796222507954
iteration 19, loss = 0.006189234554767609
iteration 20, loss = 0.006551974453032017
iteration 21, loss = 0.0069415452890098095
iteration 22, loss = 0.0071808332577347755
iteration 23, loss = 0.0073583354242146015
iteration 24, loss = 0.005892704240977764
iteration 25, loss = 0.005639749113470316
iteration 26, loss = 0.00760088674724102
iteration 27, loss = 0.006960621103644371
iteration 28, loss = 0.006651193834841251
iteration 29, loss = 0.009112194180488586
iteration 30, loss = 0.0055348509922623634
iteration 31, loss = 0.005783547181636095
iteration 32, loss = 0.006015326362103224
iteration 33, loss = 0.005489501636475325
iteration 34, loss = 0.0063893720507621765
iteration 35, loss = 0.005621998105198145
iteration 36, loss = 0.00572867039591074
iteration 37, loss = 0.008060742169618607
iteration 38, loss = 0.009225872345268726
iteration 39, loss = 0.006531779654324055
iteration 40, loss = 0.005991602316498756
iteration 41, loss = 0.006589821074157953
iteration 42, loss = 0.00582924485206604
iteration 43, loss = 0.006256957538425922
iteration 44, loss = 0.00551508879289031
iteration 45, loss = 0.006125588435679674
iteration 46, loss = 0.006117056589573622
iteration 47, loss = 0.006162708159536123
iteration 48, loss = 0.005851242691278458
iteration 49, loss = 0.010715517215430737
iteration 50, loss = 0.00659465417265892
iteration 51, loss = 0.00609742384403944
iteration 52, loss = 0.005936381407082081
iteration 53, loss = 0.007011015899479389
iteration 54, loss = 0.00592157244682312
iteration 55, loss = 0.005964370444417
iteration 56, loss = 0.0057805730029940605
iteration 57, loss = 0.006706162355840206
iteration 58, loss = 0.0063295490108430386
iteration 59, loss = 0.007101699709892273
iteration 60, loss = 0.00565760163590312
iteration 61, loss = 0.005960945971310139
iteration 62, loss = 0.007099069654941559
iteration 63, loss = 0.0059968214482069016
iteration 64, loss = 0.007984122261404991
iteration 65, loss = 0.006562418770045042
iteration 66, loss = 0.007270427420735359
iteration 67, loss = 0.007470293901860714
iteration 68, loss = 0.006062043830752373
iteration 69, loss = 0.007058490067720413
iteration 70, loss = 0.007686053868383169
iteration 71, loss = 0.0058105578646063805
iteration 72, loss = 0.007140930742025375
iteration 73, loss = 0.006014412734657526
iteration 74, loss = 0.006271792110055685
iteration 75, loss = 0.006392808631062508
iteration 76, loss = 0.006189414300024509
iteration 77, loss = 0.0055801901035010815
iteration 78, loss = 0.005960611626505852
iteration 79, loss = 0.006014468614012003
iteration 80, loss = 0.006300970911979675
iteration 81, loss = 0.005615375004708767
iteration 82, loss = 0.00603644410148263
iteration 83, loss = 0.008895957842469215
iteration 84, loss = 0.00592827470973134
iteration 85, loss = 0.005550279747694731
iteration 86, loss = 0.006444317754358053
iteration 87, loss = 0.006172534544020891
iteration 88, loss = 0.006245437078177929
iteration 89, loss = 0.005930996034294367
iteration 90, loss = 0.005968703422695398
iteration 91, loss = 0.007844160310924053
iteration 92, loss = 0.006134440656751394
iteration 93, loss = 0.005929724313318729
iteration 94, loss = 0.005838653072714806
iteration 95, loss = 0.00740966759622097
iteration 96, loss = 0.005843020044267178
iteration 97, loss = 0.008811764419078827
iteration 98, loss = 0.006051183212548494
iteration 99, loss = 0.0056726462207734585
iteration 100, loss = 0.006566009484231472
iteration 101, loss = 0.0062086922116577625
iteration 102, loss = 0.005986344534903765
iteration 103, loss = 0.006894308608025312
iteration 104, loss = 0.005982930772006512
iteration 105, loss = 0.005746250040829182
iteration 106, loss = 0.005917526315897703
iteration 107, loss = 0.005890023894608021
iteration 108, loss = 0.006639166735112667
iteration 109, loss = 0.0064887418411672115
iteration 110, loss = 0.005836183670908213
iteration 111, loss = 0.005692754406481981
iteration 112, loss = 0.007612063083797693
iteration 113, loss = 0.0060808090493083
iteration 114, loss = 0.005743681453168392
iteration 115, loss = 0.0059080361388623714
iteration 116, loss = 0.00584592716768384
iteration 117, loss = 0.005858038552105427
iteration 118, loss = 0.006239310372620821
iteration 119, loss = 0.005755096208304167
iteration 120, loss = 0.00562161672860384
iteration 121, loss = 0.006038003601133823
iteration 122, loss = 0.006454093847423792
iteration 123, loss = 0.007046001963317394
iteration 124, loss = 0.0074130697175860405
iteration 125, loss = 0.00614546611905098
iteration 126, loss = 0.005986623931676149
iteration 127, loss = 0.006753129884600639
iteration 128, loss = 0.005551055073738098
iteration 129, loss = 0.007527962792664766
iteration 130, loss = 0.006195508874952793
iteration 131, loss = 0.005775810685008764
iteration 132, loss = 0.0073656910099089146
iteration 133, loss = 0.007404264062643051
iteration 134, loss = 0.0057886745780706406
iteration 135, loss = 0.007688857614994049
iteration 136, loss = 0.005725083872675896
iteration 137, loss = 0.006372357718646526
iteration 138, loss = 0.00743319746106863
iteration 139, loss = 0.005881429184228182
iteration 140, loss = 0.006445411592721939
iteration 141, loss = 0.00672408752143383
iteration 142, loss = 0.006284401752054691
iteration 143, loss = 0.007528963498771191
iteration 144, loss = 0.005628200713545084
iteration 145, loss = 0.005618377588689327
iteration 146, loss = 0.006243815179914236
iteration 147, loss = 0.005886019207537174
iteration 148, loss = 0.0058786580339074135
iteration 149, loss = 0.00816311128437519
iteration 150, loss = 0.006781050469726324
iteration 151, loss = 0.005979167763143778
iteration 152, loss = 0.005663982126861811
iteration 153, loss = 0.007735473103821278
iteration 154, loss = 0.00594310462474823
iteration 155, loss = 0.007096171844750643
iteration 156, loss = 0.007356435060501099
iteration 157, loss = 0.006385014392435551
iteration 158, loss = 0.0062223379500210285
iteration 159, loss = 0.006143614649772644
iteration 160, loss = 0.005800420884042978
iteration 161, loss = 0.008075951598584652
iteration 162, loss = 0.005929593462496996
iteration 163, loss = 0.008679641410708427
iteration 164, loss = 0.005892239976674318
iteration 165, loss = 0.005952164530754089
iteration 166, loss = 0.005976649466902018
iteration 167, loss = 0.00610727583989501
iteration 168, loss = 0.005716804414987564
iteration 169, loss = 0.007085966877639294
iteration 170, loss = 0.005607101134955883
iteration 171, loss = 0.005742027424275875
iteration 172, loss = 0.005659847520291805
iteration 173, loss = 0.008156094700098038
iteration 174, loss = 0.005959965288639069
iteration 175, loss = 0.005576636176556349
iteration 176, loss = 0.005745569244027138
iteration 177, loss = 0.007570504676550627
iteration 178, loss = 0.005982012487947941
iteration 179, loss = 0.007910292595624924
iteration 180, loss = 0.00639698002487421
iteration 181, loss = 0.006924747955054045
iteration 182, loss = 0.005935381632298231
iteration 183, loss = 0.0059144152328372
iteration 184, loss = 0.005885787773877382
iteration 185, loss = 0.007803226821124554
iteration 186, loss = 0.006348766386508942
iteration 187, loss = 0.005840828642249107
iteration 188, loss = 0.006454864516854286
iteration 189, loss = 0.006503578275442123
iteration 190, loss = 0.005966992117464542
iteration 191, loss = 0.00632058409973979
iteration 192, loss = 0.005822003353387117
iteration 193, loss = 0.006054616533219814
iteration 194, loss = 0.006127829663455486
iteration 195, loss = 0.006531515158712864
iteration 196, loss = 0.005887624807655811
iteration 197, loss = 0.007662555668503046
iteration 198, loss = 0.005841655191034079
iteration 199, loss = 0.005829663947224617
iteration 200, loss = 0.005633327644318342
iteration 201, loss = 0.005998183973133564
iteration 202, loss = 0.006911658216267824
iteration 203, loss = 0.005868894048035145
iteration 204, loss = 0.0071778991259634495
iteration 205, loss = 0.005669055972248316
iteration 206, loss = 0.005922810174524784
iteration 207, loss = 0.007433210965245962
iteration 208, loss = 0.007193765137344599
iteration 209, loss = 0.007300142664462328
iteration 210, loss = 0.007106157019734383
iteration 211, loss = 0.005540263373404741
iteration 212, loss = 0.0064999754540622234
iteration 213, loss = 0.006268808618187904
iteration 214, loss = 0.007889279164373875
iteration 215, loss = 0.010974137112498283
iteration 216, loss = 0.006622985005378723
iteration 217, loss = 0.005629834719002247
iteration 218, loss = 0.006480382289737463
iteration 219, loss = 0.005844135768711567
iteration 220, loss = 0.00561318825930357
iteration 221, loss = 0.007697975262999535
iteration 222, loss = 0.005708497483283281
iteration 223, loss = 0.007115905173122883
iteration 224, loss = 0.007363858632743359
iteration 225, loss = 0.008990724571049213
iteration 226, loss = 0.007003180216997862
iteration 227, loss = 0.007698960602283478
iteration 228, loss = 0.0062369476072490215
iteration 229, loss = 0.008626053109765053
iteration 230, loss = 0.007600975222885609
iteration 231, loss = 0.0059420461766421795
iteration 232, loss = 0.006040515378117561
iteration 233, loss = 0.011465328745543957
iteration 234, loss = 0.007838430814445019
iteration 235, loss = 0.005672810599207878
iteration 236, loss = 0.008822381496429443
iteration 237, loss = 0.006141996011137962
iteration 238, loss = 0.005742937326431274
iteration 239, loss = 0.0058739823289215565
iteration 240, loss = 0.005652557127177715
iteration 241, loss = 0.006676092743873596
iteration 242, loss = 0.0059591662138700485
iteration 243, loss = 0.005892300512641668
iteration 244, loss = 0.006188901606947184
iteration 245, loss = 0.005820947699248791
iteration 246, loss = 0.006745792459696531
iteration 247, loss = 0.0060733347199857235
iteration 248, loss = 0.007322321180254221
iteration 249, loss = 0.005929500795900822
iteration 250, loss = 0.005882567260414362
iteration 251, loss = 0.008518446236848831
iteration 252, loss = 0.007110910024493933
iteration 253, loss = 0.005864214617758989
iteration 254, loss = 0.005798400845378637
iteration 255, loss = 0.008989307098090649
iteration 256, loss = 0.0054034083150327206
iteration 257, loss = 0.007738774176687002
iteration 258, loss = 0.005857387557625771
iteration 259, loss = 0.007026715204119682
iteration 260, loss = 0.006392383947968483
iteration 261, loss = 0.008659358136355877
iteration 262, loss = 0.005939934402704239
iteration 263, loss = 0.00712942611426115
iteration 264, loss = 0.005970066413283348
iteration 265, loss = 0.006030628923326731
iteration 266, loss = 0.007088728714734316
iteration 267, loss = 0.005627903155982494
iteration 268, loss = 0.006081582047045231
iteration 269, loss = 0.005814036820083857
iteration 270, loss = 0.006944743916392326
iteration 271, loss = 0.005694570019841194
iteration 272, loss = 0.00580573920160532
iteration 273, loss = 0.005840998142957687
iteration 274, loss = 0.005798805970698595
iteration 275, loss = 0.009004375897347927
iteration 276, loss = 0.0061303190886974335
iteration 277, loss = 0.0056346263736486435
iteration 278, loss = 0.006009640172123909
iteration 279, loss = 0.008710963651537895
iteration 280, loss = 0.01028768252581358
iteration 281, loss = 0.005990529432892799
iteration 282, loss = 0.00565731106325984
iteration 283, loss = 0.006675880867987871
iteration 284, loss = 0.006007608491927385
iteration 285, loss = 0.00578280258923769
iteration 286, loss = 0.005582071840763092
iteration 287, loss = 0.0071952445432543755
iteration 288, loss = 0.005895091686397791
iteration 289, loss = 0.005892307497560978
iteration 290, loss = 0.0057771108113229275
iteration 291, loss = 0.005521541927009821
iteration 292, loss = 0.005989683326333761
iteration 293, loss = 0.005924247205257416
iteration 294, loss = 0.005964863114058971
iteration 295, loss = 0.0056260935962200165
iteration 296, loss = 0.005975496023893356
iteration 297, loss = 0.005559656769037247
iteration 298, loss = 0.007445483468472958
iteration 299, loss = 0.009974374435842037
iteration 300, loss = 0.005732996854931116

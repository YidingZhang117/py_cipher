<<<<<<< HEAD
iteration 1, loss = 3.4361157417297363
iteration 2, loss = 6.071143627166748
iteration 3, loss = 5.862336158752441
iteration 4, loss = 4.458922863006592
iteration 5, loss = 1.1049779653549194
iteration 6, loss = 1.429640293121338
iteration 7, loss = 0.647943377494812
iteration 8, loss = 0.41024327278137207
iteration 9, loss = 0.6508992910385132
iteration 10, loss = 0.39307624101638794
iteration 11, loss = 0.5162062644958496
iteration 12, loss = 0.4200858175754547
iteration 13, loss = 0.6001100540161133
iteration 14, loss = 0.32590773701667786
iteration 15, loss = 0.36477363109588623
iteration 16, loss = 0.5302391052246094
iteration 17, loss = 0.39782869815826416
iteration 18, loss = 0.42280733585357666
iteration 19, loss = 0.5154947638511658
iteration 20, loss = 0.4732564091682434
iteration 21, loss = 0.46451452374458313
iteration 22, loss = 0.4520829916000366
iteration 23, loss = 0.4579123258590698
iteration 24, loss = 0.5954511761665344
iteration 25, loss = 0.4269496500492096
iteration 26, loss = 0.4479277431964874
iteration 27, loss = 0.4088398218154907
iteration 28, loss = 0.42094162106513977
iteration 29, loss = 0.4991346597671509
iteration 30, loss = 0.3958900570869446
iteration 31, loss = 0.465434193611145
iteration 32, loss = 0.4947252571582794
iteration 33, loss = 0.4580209255218506
iteration 34, loss = 0.4268171787261963
iteration 35, loss = 0.48839515447616577
iteration 36, loss = 0.4822205901145935
iteration 37, loss = 0.4062882363796234
iteration 38, loss = 0.6487762331962585
iteration 39, loss = 0.5642814636230469
iteration 40, loss = 0.49846354126930237
iteration 41, loss = 0.5310416221618652
iteration 42, loss = 0.43800559639930725
iteration 43, loss = 0.39191073179244995
iteration 44, loss = 0.6319937705993652
iteration 45, loss = 0.5597487092018127
iteration 46, loss = 0.46511110663414
iteration 47, loss = 0.4367738366127014
iteration 48, loss = 0.5180878043174744
iteration 49, loss = 0.4573517441749573
iteration 50, loss = 0.51549232006073
iteration 51, loss = 0.4774879217147827
iteration 52, loss = 0.3921782374382019
iteration 53, loss = 0.4952303469181061
iteration 54, loss = 0.3905273675918579
iteration 55, loss = 0.4680050015449524
iteration 56, loss = 0.49738895893096924
iteration 57, loss = 0.4998730421066284
iteration 58, loss = 0.4051121175289154
iteration 59, loss = 0.440396785736084
iteration 60, loss = 0.38495948910713196
iteration 61, loss = 0.48796218633651733
iteration 62, loss = 0.38263607025146484
iteration 63, loss = 0.5161854028701782
iteration 64, loss = 0.5038180351257324
iteration 65, loss = 0.43330588936805725
iteration 66, loss = 0.4530104994773865
iteration 67, loss = 0.46013206243515015
iteration 68, loss = 0.4308493733406067
iteration 69, loss = 0.484953373670578
iteration 70, loss = 0.46955937147140503
iteration 71, loss = 0.42241886258125305
iteration 72, loss = 0.39757853746414185
iteration 73, loss = 0.4491928815841675
iteration 74, loss = 0.45463550090789795
iteration 75, loss = 0.3663075566291809
iteration 76, loss = 0.4029791057109833
iteration 77, loss = 0.4312613606452942
iteration 78, loss = 0.4124738574028015
iteration 79, loss = 0.4903497099876404
iteration 80, loss = 0.4418773949146271
iteration 81, loss = 0.48704972863197327
iteration 82, loss = 0.43780189752578735
iteration 83, loss = 0.39429333806037903
iteration 84, loss = 0.4922229051589966
iteration 85, loss = 0.43895015120506287
iteration 86, loss = 0.4909162223339081
iteration 87, loss = 0.39367005228996277
iteration 88, loss = 0.509906530380249
iteration 89, loss = 0.502801239490509
iteration 90, loss = 0.4396657943725586
iteration 91, loss = 0.5623734593391418
iteration 92, loss = 0.5066326856613159
iteration 93, loss = 0.4102964401245117
iteration 94, loss = 0.44611606001853943
iteration 95, loss = 0.4081885516643524
iteration 96, loss = 0.47542038559913635
iteration 97, loss = 0.43560805916786194
iteration 98, loss = 0.44812145829200745
iteration 99, loss = 0.39388278126716614
iteration 100, loss = 0.4322463274002075
iteration 101, loss = 0.40343111753463745
iteration 102, loss = 0.3994881510734558
iteration 103, loss = 0.43629711866378784
iteration 104, loss = 0.5129212737083435
iteration 105, loss = 0.4810643196105957
iteration 106, loss = 0.3884392976760864
iteration 107, loss = 0.38301965594291687
iteration 108, loss = 0.4577864110469818
iteration 109, loss = 0.39268970489501953
iteration 110, loss = 0.4525267481803894
iteration 111, loss = 0.38666388392448425
iteration 112, loss = 0.46499142050743103
iteration 113, loss = 0.418570876121521
iteration 114, loss = 0.4818631708621979
iteration 115, loss = 0.3603478968143463
iteration 116, loss = 0.4016311764717102
iteration 117, loss = 0.4833788275718689
iteration 118, loss = 0.3783419132232666
iteration 119, loss = 0.4171488285064697
iteration 120, loss = 0.47760099172592163
iteration 121, loss = 0.40315020084381104
iteration 122, loss = 0.4462434947490692
iteration 123, loss = 0.40814104676246643
iteration 124, loss = 0.541291356086731
iteration 125, loss = 0.42801183462142944
iteration 126, loss = 0.432849258184433
iteration 127, loss = 0.42419949173927307
iteration 128, loss = 0.4320961534976959
iteration 129, loss = 0.44490933418273926
iteration 130, loss = 0.3520866632461548
iteration 131, loss = 0.4039158225059509
iteration 132, loss = 0.44683873653411865
iteration 133, loss = 0.37222665548324585
iteration 134, loss = 0.46491530537605286
iteration 135, loss = 0.39099958539009094
iteration 136, loss = 0.40445348620414734
iteration 137, loss = 0.4405464828014374
iteration 138, loss = 0.44310295581817627
iteration 139, loss = 0.39547592401504517
iteration 140, loss = 0.4729611277580261
iteration 141, loss = 0.38853198289871216
iteration 142, loss = 0.4562258720397949
iteration 143, loss = 0.428091436624527
iteration 144, loss = 0.43398258090019226
iteration 145, loss = 0.3741188645362854
iteration 146, loss = 0.5518878698348999
iteration 147, loss = 0.39663511514663696
iteration 148, loss = 0.42686861753463745
iteration 149, loss = 0.3881208300590515
iteration 150, loss = 0.40147048234939575
iteration 151, loss = 0.443073570728302
iteration 152, loss = 0.4476226270198822
iteration 153, loss = 0.48948997259140015
iteration 154, loss = 0.3904991149902344
iteration 155, loss = 0.4358499050140381
iteration 156, loss = 0.37420859932899475
iteration 157, loss = 0.4357033371925354
iteration 158, loss = 0.4588892459869385
iteration 159, loss = 0.38767504692077637
iteration 160, loss = 0.44067704677581787
iteration 161, loss = 0.46477270126342773
iteration 162, loss = 0.43489155173301697
iteration 163, loss = 0.4522732198238373
iteration 164, loss = 0.47501376271247864
iteration 165, loss = 0.45794469118118286
iteration 166, loss = 0.41864413022994995
iteration 167, loss = 0.40217432379722595
iteration 168, loss = 0.39215371012687683
iteration 169, loss = 0.46249255537986755
iteration 170, loss = 0.42183053493499756
iteration 171, loss = 0.427447646856308
iteration 172, loss = 0.38318943977355957
iteration 173, loss = 0.38945120573043823
iteration 174, loss = 0.3688807785511017
iteration 175, loss = 0.45310837030410767
iteration 176, loss = 0.3409762382507324
iteration 177, loss = 0.4417024850845337
iteration 178, loss = 0.43808868527412415
iteration 179, loss = 0.4586533010005951
iteration 180, loss = 0.46425023674964905
iteration 181, loss = 0.4719613194465637
iteration 182, loss = 0.3631889522075653
iteration 183, loss = 0.34123727679252625
iteration 184, loss = 0.4363551735877991
iteration 185, loss = 0.3424403667449951
iteration 186, loss = 0.39714592695236206
iteration 187, loss = 0.34912723302841187
iteration 188, loss = 0.4312435984611511
iteration 189, loss = 0.39611905813217163
iteration 190, loss = 0.4821106791496277
iteration 191, loss = 0.4458554983139038
iteration 192, loss = 0.4240068197250366
iteration 193, loss = 0.45307856798171997
iteration 194, loss = 0.43817830085754395
iteration 195, loss = 0.44437214732170105
iteration 196, loss = 0.4410274028778076
iteration 197, loss = 0.377402126789093
iteration 198, loss = 0.3663861155509949
iteration 199, loss = 0.4650425314903259
iteration 200, loss = 0.3756912648677826
iteration 201, loss = 0.39762452244758606
iteration 202, loss = 0.38817015290260315
iteration 203, loss = 0.42062678933143616
iteration 204, loss = 0.4156745672225952
iteration 205, loss = 0.41541197896003723
iteration 206, loss = 0.39127951860427856
iteration 207, loss = 0.327874094247818
iteration 208, loss = 0.4070613980293274
iteration 209, loss = 0.43449708819389343
iteration 210, loss = 0.4625415802001953
iteration 211, loss = 0.3698805272579193
iteration 212, loss = 0.4232766628265381
iteration 213, loss = 0.4380606412887573
iteration 214, loss = 0.453848659992218
iteration 215, loss = 0.37969523668289185
iteration 216, loss = 0.44338953495025635
iteration 217, loss = 0.3079126179218292
iteration 218, loss = 0.40706923604011536
iteration 219, loss = 0.4253957271575928
iteration 220, loss = 0.3434833586215973
iteration 221, loss = 0.4233705699443817
iteration 222, loss = 0.40315327048301697
iteration 223, loss = 0.38121479749679565
iteration 224, loss = 0.4042636752128601
iteration 225, loss = 0.31454703211784363
iteration 226, loss = 0.43859851360321045
iteration 227, loss = 0.3594176769256592
iteration 228, loss = 0.32587316632270813
iteration 229, loss = 0.4077804386615753
iteration 230, loss = 0.32328641414642334
iteration 231, loss = 0.41645607352256775
iteration 232, loss = 0.40766462683677673
iteration 233, loss = 0.36274221539497375
iteration 234, loss = 0.48550426959991455
iteration 235, loss = 0.3883734345436096
iteration 236, loss = 0.4120129346847534
iteration 237, loss = 0.3714063763618469
iteration 238, loss = 0.368511438369751
iteration 239, loss = 0.3816644549369812
iteration 240, loss = 0.4128246605396271
iteration 241, loss = 0.36959096789360046
iteration 242, loss = 0.36418771743774414
iteration 243, loss = 0.3963735103607178
iteration 244, loss = 0.3941827118396759
iteration 245, loss = 0.3688850402832031
iteration 246, loss = 0.3831908106803894
iteration 247, loss = 0.3890937268733978
iteration 248, loss = 0.38188326358795166
iteration 249, loss = 0.35289159417152405
iteration 250, loss = 0.3796955943107605
iteration 251, loss = 0.3597109019756317
iteration 252, loss = 0.3916357159614563
iteration 253, loss = 0.47146087884902954
iteration 254, loss = 0.3933994770050049
iteration 255, loss = 0.36314788460731506
iteration 256, loss = 0.4040854871273041
iteration 257, loss = 0.3472699224948883
iteration 258, loss = 0.37391114234924316
iteration 259, loss = 0.3873879015445709
iteration 260, loss = 0.3218005895614624
iteration 261, loss = 0.44257286190986633
iteration 262, loss = 0.3818485736846924
iteration 263, loss = 0.3007771372795105
iteration 264, loss = 0.46538904309272766
iteration 265, loss = 0.35394638776779175
iteration 266, loss = 0.36637207865715027
iteration 267, loss = 0.3703218698501587
iteration 268, loss = 0.33201974630355835
iteration 269, loss = 0.3390161395072937
iteration 270, loss = 0.3649219274520874
iteration 271, loss = 0.4416394829750061
iteration 272, loss = 0.4133499264717102
iteration 273, loss = 0.38540998101234436
iteration 274, loss = 0.3539789617061615
iteration 275, loss = 0.41604501008987427
iteration 276, loss = 0.3156360685825348
iteration 277, loss = 0.38543376326560974
iteration 278, loss = 0.3684770166873932
iteration 279, loss = 0.40719711780548096
iteration 280, loss = 0.3200962543487549
iteration 281, loss = 0.3787216246128082
iteration 282, loss = 0.3062341511249542
iteration 283, loss = 0.3820781707763672
iteration 284, loss = 0.42730847001075745
iteration 285, loss = 0.3861362040042877
iteration 286, loss = 0.4198383688926697
iteration 287, loss = 0.4131242334842682
iteration 288, loss = 0.3353814482688904
iteration 289, loss = 0.46446314454078674
iteration 290, loss = 0.3273281753063202
iteration 291, loss = 0.38628047704696655
iteration 292, loss = 0.34159401059150696
iteration 293, loss = 0.35350102186203003
iteration 294, loss = 0.35849788784980774
iteration 295, loss = 0.3614004850387573
iteration 296, loss = 0.3943004012107849
iteration 297, loss = 0.39252257347106934
iteration 298, loss = 0.4018731713294983
iteration 299, loss = 0.35270845890045166
iteration 300, loss = 0.31964853405952454
iteration 1, loss = 0.3894124925136566
iteration 2, loss = 0.38937145471572876
iteration 3, loss = 0.32832881808280945
iteration 4, loss = 0.3453553318977356
iteration 5, loss = 0.38479819893836975
iteration 6, loss = 0.4619350731372833
iteration 7, loss = 0.413148432970047
iteration 8, loss = 0.416914165019989
iteration 9, loss = 0.39173316955566406
iteration 10, loss = 0.36309489607810974
iteration 11, loss = 0.3700753450393677
iteration 12, loss = 0.3584771156311035
iteration 13, loss = 0.3659762740135193
iteration 14, loss = 0.44592657685279846
iteration 15, loss = 0.34996509552001953
iteration 16, loss = 0.2899738550186157
iteration 17, loss = 0.40315350890159607
iteration 18, loss = 0.41405993700027466
iteration 19, loss = 0.36880093812942505
iteration 20, loss = 0.3546997904777527
iteration 21, loss = 0.3273119628429413
iteration 22, loss = 0.40639615058898926
iteration 23, loss = 0.3956681489944458
iteration 24, loss = 0.37757742404937744
iteration 25, loss = 0.3192579448223114
iteration 26, loss = 0.3397732377052307
iteration 27, loss = 0.34440672397613525
iteration 28, loss = 0.3814629912376404
iteration 29, loss = 0.3289550542831421
iteration 30, loss = 0.376735121011734
iteration 31, loss = 0.35121095180511475
iteration 32, loss = 0.3618457019329071
iteration 33, loss = 0.3891758918762207
iteration 34, loss = 0.40998631715774536
iteration 35, loss = 0.3487321138381958
iteration 36, loss = 0.38661009073257446
iteration 37, loss = 0.3812776505947113
iteration 38, loss = 0.35665959119796753
iteration 39, loss = 0.3569927513599396
iteration 40, loss = 0.37649303674697876
iteration 41, loss = 0.3581013083457947
iteration 42, loss = 0.3806554675102234
iteration 43, loss = 0.3971264064311981
iteration 44, loss = 0.38033023476600647
iteration 45, loss = 0.2886272668838501
iteration 46, loss = 0.36384522914886475
iteration 47, loss = 0.3249533176422119
iteration 48, loss = 0.37999317049980164
iteration 49, loss = 0.33416062593460083
iteration 50, loss = 0.3613708019256592
iteration 51, loss = 0.4109269082546234
iteration 52, loss = 0.34311243891716003
iteration 53, loss = 0.32125595211982727
iteration 54, loss = 0.36517027020454407
iteration 55, loss = 0.3262874186038971
iteration 56, loss = 0.3524612486362457
iteration 57, loss = 0.31980791687965393
iteration 58, loss = 0.32237234711647034
iteration 59, loss = 0.32647252082824707
iteration 60, loss = 0.33898553252220154
iteration 61, loss = 0.33852091431617737
iteration 62, loss = 0.37886375188827515
iteration 63, loss = 0.3749297261238098
iteration 64, loss = 0.36801061034202576
iteration 65, loss = 0.3924604058265686
iteration 66, loss = 0.35607191920280457
iteration 67, loss = 0.2923792898654938
iteration 68, loss = 0.3945334851741791
iteration 69, loss = 0.3541472852230072
iteration 70, loss = 0.40765929222106934
iteration 71, loss = 0.34102141857147217
iteration 72, loss = 0.3474310040473938
iteration 73, loss = 0.33491501212120056
iteration 74, loss = 0.3471348285675049
iteration 75, loss = 0.36988359689712524
iteration 76, loss = 0.4164882004261017
iteration 77, loss = 0.3558308780193329
iteration 78, loss = 0.32964178919792175
iteration 79, loss = 0.3908512592315674
iteration 80, loss = 0.33856743574142456
iteration 81, loss = 0.3317858874797821
iteration 82, loss = 0.34687095880508423
iteration 83, loss = 0.30095356702804565
iteration 84, loss = 0.3476463556289673
iteration 85, loss = 0.3431040346622467
iteration 86, loss = 0.36503487825393677
iteration 87, loss = 0.4143645763397217
iteration 88, loss = 0.3030603528022766
iteration 89, loss = 0.35863372683525085
iteration 90, loss = 0.3642596900463104
iteration 91, loss = 0.3029535412788391
iteration 92, loss = 0.32565000653266907
iteration 93, loss = 0.33510926365852356
iteration 94, loss = 0.29862236976623535
iteration 95, loss = 0.37181219458580017
iteration 96, loss = 0.33819326758384705
iteration 97, loss = 0.39441001415252686
iteration 98, loss = 0.3671889305114746
iteration 99, loss = 0.3431629240512848
iteration 100, loss = 0.3564119040966034
iteration 101, loss = 0.35681572556495667
iteration 102, loss = 0.3112111985683441
iteration 103, loss = 0.31948164105415344
iteration 104, loss = 0.3306308090686798
iteration 105, loss = 0.33036115765571594
iteration 106, loss = 0.3287661671638489
iteration 107, loss = 0.3162185251712799
iteration 108, loss = 0.3128364086151123
iteration 109, loss = 0.34414246678352356
iteration 110, loss = 0.3568941056728363
iteration 111, loss = 0.318842351436615
iteration 112, loss = 0.3583976924419403
iteration 113, loss = 0.3414725661277771
iteration 114, loss = 0.31187212467193604
iteration 115, loss = 0.29412999749183655
iteration 116, loss = 0.3251926004886627
iteration 117, loss = 0.30481624603271484
iteration 118, loss = 0.32788267731666565
iteration 119, loss = 0.30541718006134033
iteration 120, loss = 0.3412328064441681
iteration 121, loss = 0.3379247486591339
iteration 122, loss = 0.29117241501808167
iteration 123, loss = 0.43247130513191223
iteration 124, loss = 0.3260287940502167
iteration 125, loss = 0.3131275177001953
iteration 126, loss = 0.3587469756603241
iteration 127, loss = 0.3210175931453705
iteration 128, loss = 0.2830716073513031
iteration 129, loss = 0.29441845417022705
iteration 130, loss = 0.342105507850647
iteration 131, loss = 0.3612167537212372
iteration 132, loss = 0.3233151435852051
iteration 133, loss = 0.3347286581993103
iteration 134, loss = 0.300628662109375
iteration 135, loss = 0.35120347142219543
iteration 136, loss = 0.3041899502277374
iteration 137, loss = 0.3659846782684326
iteration 138, loss = 0.3856404423713684
iteration 139, loss = 0.32902222871780396
iteration 140, loss = 0.3365369141101837
iteration 141, loss = 0.4329233467578888
iteration 142, loss = 0.3038771152496338
iteration 143, loss = 0.39549070596694946
iteration 144, loss = 0.32879605889320374
iteration 145, loss = 0.3073856830596924
iteration 146, loss = 0.3818591237068176
iteration 147, loss = 0.3130027651786804
iteration 148, loss = 0.3674538731575012
iteration 149, loss = 0.2874425947666168
iteration 150, loss = 0.3535882234573364
iteration 151, loss = 0.3161247968673706
iteration 152, loss = 0.34150609374046326
iteration 153, loss = 0.31647059321403503
iteration 154, loss = 0.31635746359825134
iteration 155, loss = 0.3323397636413574
iteration 156, loss = 0.28108832240104675
iteration 157, loss = 0.3315863609313965
iteration 158, loss = 0.34894829988479614
iteration 159, loss = 0.37637168169021606
iteration 160, loss = 0.3174877464771271
iteration 161, loss = 0.31711244583129883
iteration 162, loss = 0.3088948130607605
iteration 163, loss = 0.30667489767074585
iteration 164, loss = 0.3515845537185669
iteration 165, loss = 0.3081732988357544
iteration 166, loss = 0.2987205386161804
iteration 167, loss = 0.3494347929954529
iteration 168, loss = 0.3137836158275604
iteration 169, loss = 0.30346226692199707
iteration 170, loss = 0.35368478298187256
iteration 171, loss = 0.3390001654624939
iteration 172, loss = 0.2768102288246155
iteration 173, loss = 0.26917117834091187
iteration 174, loss = 0.349747896194458
iteration 175, loss = 0.31144431233406067
iteration 176, loss = 0.3312518000602722
iteration 177, loss = 0.3622822165489197
iteration 178, loss = 0.34634101390838623
iteration 179, loss = 0.32000234723091125
iteration 180, loss = 0.3459959924221039
iteration 181, loss = 0.34651830792427063
iteration 182, loss = 0.29159706830978394
iteration 183, loss = 0.33802902698516846
iteration 184, loss = 0.29734551906585693
iteration 185, loss = 0.3023831248283386
iteration 186, loss = 0.3473321199417114
iteration 187, loss = 0.3570851981639862
iteration 188, loss = 0.3584407567977905
iteration 189, loss = 0.3341991603374481
iteration 190, loss = 0.28729143738746643
iteration 191, loss = 0.3234810531139374
iteration 192, loss = 0.35018855333328247
iteration 193, loss = 0.3133401572704315
iteration 194, loss = 0.29412591457366943
iteration 195, loss = 0.3267103135585785
iteration 196, loss = 0.29623761773109436
iteration 197, loss = 0.29120954871177673
iteration 198, loss = 0.31834593415260315
iteration 199, loss = 0.2985580563545227
iteration 200, loss = 0.3847554624080658
iteration 201, loss = 0.30170321464538574
iteration 202, loss = 0.3574344515800476
iteration 203, loss = 0.3605427145957947
iteration 204, loss = 0.31347864866256714
iteration 205, loss = 0.33762550354003906
iteration 206, loss = 0.2962016761302948
iteration 207, loss = 0.327886164188385
iteration 208, loss = 0.317951500415802
iteration 209, loss = 0.3317692279815674
iteration 210, loss = 0.3507147431373596
iteration 211, loss = 0.2929585874080658
iteration 212, loss = 0.3133368194103241
iteration 213, loss = 0.3068721890449524
iteration 214, loss = 0.35336166620254517
iteration 215, loss = 0.32751700282096863
iteration 216, loss = 0.325933575630188
iteration 217, loss = 0.30589962005615234
iteration 218, loss = 0.29345741868019104
iteration 219, loss = 0.28843367099761963
iteration 220, loss = 0.32026419043540955
iteration 221, loss = 0.2893047332763672
iteration 222, loss = 0.32552528381347656
iteration 223, loss = 0.30025172233581543
iteration 224, loss = 0.33970358967781067
iteration 225, loss = 0.3118188977241516
iteration 226, loss = 0.2968699634075165
iteration 227, loss = 0.3618735373020172
iteration 228, loss = 0.3029509484767914
iteration 229, loss = 0.2893645167350769
iteration 230, loss = 0.27680346369743347
iteration 231, loss = 0.26952382922172546
iteration 232, loss = 0.2800694704055786
iteration 233, loss = 0.299050897359848
iteration 234, loss = 0.331160306930542
iteration 235, loss = 0.3259071409702301
iteration 236, loss = 0.3156159520149231
iteration 237, loss = 0.27235642075538635
iteration 238, loss = 0.3664517104625702
iteration 239, loss = 0.3477695882320404
iteration 240, loss = 0.31658199429512024
iteration 241, loss = 0.2733084261417389
iteration 242, loss = 0.3414028286933899
iteration 243, loss = 0.3418805003166199
iteration 244, loss = 0.3163250684738159
iteration 245, loss = 0.3455502390861511
iteration 246, loss = 0.31346920132637024
iteration 247, loss = 0.4171011447906494
iteration 248, loss = 0.3264811038970947
iteration 249, loss = 0.38888052105903625
iteration 250, loss = 0.4047742486000061
iteration 251, loss = 0.3226804733276367
iteration 252, loss = 0.312309205532074
iteration 253, loss = 0.3085523247718811
iteration 254, loss = 0.36673998832702637
iteration 255, loss = 0.3327280879020691
iteration 256, loss = 0.29549673199653625
iteration 257, loss = 0.29320982098579407
iteration 258, loss = 0.27704739570617676
iteration 259, loss = 0.30501043796539307
iteration 260, loss = 0.298907995223999
iteration 261, loss = 0.31659409403800964
iteration 262, loss = 0.3581971228122711
iteration 263, loss = 0.3384596109390259
iteration 264, loss = 0.3534902036190033
iteration 265, loss = 0.3093526065349579
iteration 266, loss = 0.34444308280944824
iteration 267, loss = 0.2896197736263275
iteration 268, loss = 0.25709375739097595
iteration 269, loss = 0.2874186038970947
iteration 270, loss = 0.33931171894073486
iteration 271, loss = 0.33883386850357056
iteration 272, loss = 0.33542194962501526
iteration 273, loss = 0.2821752727031708
iteration 274, loss = 0.3172261416912079
iteration 275, loss = 0.3473231792449951
iteration 276, loss = 0.31965669989585876
iteration 277, loss = 0.33754825592041016
iteration 278, loss = 0.3571024537086487
iteration 279, loss = 0.30893242359161377
iteration 280, loss = 0.3105815649032593
iteration 281, loss = 0.3158838748931885
iteration 282, loss = 0.2957538962364197
iteration 283, loss = 0.3110053539276123
iteration 284, loss = 0.3254077732563019
iteration 285, loss = 0.3084104359149933
iteration 286, loss = 0.3505418598651886
iteration 287, loss = 0.2946886122226715
iteration 288, loss = 0.27881425619125366
iteration 289, loss = 0.28070342540740967
iteration 290, loss = 0.30453693866729736
iteration 291, loss = 0.2738872766494751
iteration 292, loss = 0.28564634919166565
iteration 293, loss = 0.362346887588501
iteration 294, loss = 0.32522597908973694
iteration 295, loss = 0.35927554965019226
iteration 296, loss = 0.28539568185806274
iteration 297, loss = 0.3216457664966583
iteration 298, loss = 0.26339825987815857
iteration 299, loss = 0.33782273530960083
iteration 300, loss = 0.3199390470981598
iteration 1, loss = 0.29497891664505005
iteration 2, loss = 0.3337588310241699
iteration 3, loss = 0.2982291877269745
iteration 4, loss = 0.2956171929836273
iteration 5, loss = 0.3019757568836212
iteration 6, loss = 0.292089581489563
iteration 7, loss = 0.29984986782073975
iteration 8, loss = 0.36648428440093994
iteration 9, loss = 0.2874986231327057
iteration 10, loss = 0.2925194799900055
iteration 11, loss = 0.26617056131362915
iteration 12, loss = 0.32703274488449097
iteration 13, loss = 0.30791690945625305
iteration 14, loss = 0.28809159994125366
iteration 15, loss = 0.35266584157943726
iteration 16, loss = 0.29207727313041687
iteration 17, loss = 0.2656722068786621
iteration 18, loss = 0.3065856993198395
iteration 19, loss = 0.3467597961425781
iteration 20, loss = 0.2735621929168701
iteration 21, loss = 0.2726156413555145
iteration 22, loss = 0.2759784758090973
iteration 23, loss = 0.2530430555343628
iteration 24, loss = 0.3197769224643707
iteration 25, loss = 0.2610083520412445
iteration 26, loss = 0.3234562575817108
iteration 27, loss = 0.27243664860725403
iteration 28, loss = 0.35427188873291016
iteration 29, loss = 0.2788095474243164
iteration 30, loss = 0.2745929956436157
iteration 31, loss = 0.27805402874946594
iteration 32, loss = 0.2782048285007477
iteration 33, loss = 0.2822156548500061
iteration 34, loss = 0.2870531678199768
iteration 35, loss = 0.27697575092315674
iteration 36, loss = 0.2600879669189453
iteration 37, loss = 0.2954835593700409
iteration 38, loss = 0.33705243468284607
iteration 39, loss = 0.29278603196144104
iteration 40, loss = 0.3166796863079071
iteration 41, loss = 0.30284810066223145
iteration 42, loss = 0.300212562084198
iteration 43, loss = 0.26717084646224976
iteration 44, loss = 0.31256410479545593
iteration 45, loss = 0.26326271891593933
iteration 46, loss = 0.31923311948776245
iteration 47, loss = 0.3018835186958313
iteration 48, loss = 0.34880146384239197
iteration 49, loss = 0.27176058292388916
iteration 50, loss = 0.29246821999549866
iteration 51, loss = 0.2639574110507965
iteration 52, loss = 0.26881659030914307
iteration 53, loss = 0.2990947961807251
iteration 54, loss = 0.26989495754241943
iteration 55, loss = 0.3218068480491638
iteration 56, loss = 0.3049565255641937
iteration 57, loss = 0.29912230372428894
iteration 58, loss = 0.3022947609424591
iteration 59, loss = 0.353420227766037
iteration 60, loss = 0.30604755878448486
iteration 61, loss = 0.27026253938674927
iteration 62, loss = 0.29923132061958313
iteration 63, loss = 0.26545819640159607
iteration 64, loss = 0.2560034692287445
iteration 65, loss = 0.29693880677223206
iteration 66, loss = 0.26099228858947754
iteration 67, loss = 0.2801700532436371
iteration 68, loss = 0.27279287576675415
iteration 69, loss = 0.2915821671485901
iteration 70, loss = 0.2818991541862488
iteration 71, loss = 0.3347773551940918
iteration 72, loss = 0.30072206258773804
iteration 73, loss = 0.25231093168258667
iteration 74, loss = 0.33692196011543274
iteration 75, loss = 0.29541274905204773
iteration 76, loss = 0.29135721921920776
iteration 77, loss = 0.28322628140449524
iteration 78, loss = 0.283211350440979
iteration 79, loss = 0.3082508146762848
iteration 80, loss = 0.3066432476043701
iteration 81, loss = 0.2702544629573822
iteration 82, loss = 0.30075883865356445
iteration 83, loss = 0.2940168082714081
iteration 84, loss = 0.30421391129493713
iteration 85, loss = 0.29117998480796814
iteration 86, loss = 0.27419957518577576
iteration 87, loss = 0.2815609574317932
iteration 88, loss = 0.25136908888816833
iteration 89, loss = 0.30352187156677246
iteration 90, loss = 0.29496872425079346
iteration 91, loss = 0.29506367444992065
iteration 92, loss = 0.2617083191871643
iteration 93, loss = 0.32410329580307007
iteration 94, loss = 0.31438350677490234
iteration 95, loss = 0.2887747287750244
iteration 96, loss = 0.2840259075164795
iteration 97, loss = 0.28316083550453186
iteration 98, loss = 0.3134664297103882
iteration 99, loss = 0.2561399042606354
iteration 100, loss = 0.25942301750183105
iteration 101, loss = 0.31188705563545227
iteration 102, loss = 0.2507230043411255
iteration 103, loss = 0.28308144211769104
iteration 104, loss = 0.2728373408317566
iteration 105, loss = 0.30706068873405457
iteration 106, loss = 0.25896698236465454
iteration 107, loss = 0.3074369728565216
iteration 108, loss = 0.2767791748046875
iteration 109, loss = 0.2707354724407196
iteration 110, loss = 0.2740010619163513
iteration 111, loss = 0.2836343050003052
iteration 112, loss = 0.3054627776145935
iteration 113, loss = 0.2948510944843292
iteration 114, loss = 0.2717660665512085
iteration 115, loss = 0.3142576813697815
iteration 116, loss = 0.34629908204078674
iteration 117, loss = 0.2736891806125641
iteration 118, loss = 0.25500136613845825
iteration 119, loss = 0.2699374556541443
iteration 120, loss = 0.29835081100463867
iteration 121, loss = 0.2956041693687439
iteration 122, loss = 0.3148726224899292
iteration 123, loss = 0.25505638122558594
iteration 124, loss = 0.27150264382362366
iteration 125, loss = 0.2557143270969391
iteration 126, loss = 0.26096922159194946
iteration 127, loss = 0.2857331335544586
iteration 128, loss = 0.2850366234779358
iteration 129, loss = 0.25381264090538025
iteration 130, loss = 0.2709309756755829
iteration 131, loss = 0.3149453103542328
iteration 132, loss = 0.27895811200141907
iteration 133, loss = 0.23596414923667908
iteration 134, loss = 0.27221399545669556
iteration 135, loss = 0.2723061442375183
iteration 136, loss = 0.2800365388393402
iteration 137, loss = 0.3063236176967621
iteration 138, loss = 0.29328733682632446
iteration 139, loss = 0.32964596152305603
iteration 140, loss = 0.2661573588848114
iteration 141, loss = 0.3246792256832123
iteration 142, loss = 0.30596044659614563
iteration 143, loss = 0.27447497844696045
iteration 144, loss = 0.2469715178012848
iteration 145, loss = 0.25779643654823303
iteration 146, loss = 0.26915088295936584
iteration 147, loss = 0.2728387713432312
iteration 148, loss = 0.2748831808567047
iteration 149, loss = 0.2668471038341522
iteration 150, loss = 0.2659849524497986
iteration 151, loss = 0.2570786774158478
iteration 152, loss = 0.2613111734390259
iteration 153, loss = 0.2786773443222046
iteration 154, loss = 0.24956469237804413
iteration 155, loss = 0.27744367718696594
iteration 156, loss = 0.3198910653591156
iteration 157, loss = 0.29333406686782837
iteration 158, loss = 0.26698562502861023
iteration 159, loss = 0.2907133996486664
iteration 160, loss = 0.3284478187561035
iteration 161, loss = 0.2554461658000946
iteration 162, loss = 0.2839256227016449
iteration 163, loss = 0.2583829164505005
iteration 164, loss = 0.2716890573501587
iteration 165, loss = 0.253411203622818
iteration 166, loss = 0.27381622791290283
iteration 167, loss = 0.2745882570743561
iteration 168, loss = 0.2742779552936554
iteration 169, loss = 0.3244684934616089
iteration 170, loss = 0.2671648859977722
iteration 171, loss = 0.27539363503456116
iteration 172, loss = 0.2524139881134033
iteration 173, loss = 0.27613168954849243
iteration 174, loss = 0.24955570697784424
iteration 175, loss = 0.27581119537353516
iteration 176, loss = 0.2932060956954956
iteration 177, loss = 0.2626376450061798
iteration 178, loss = 0.2376212775707245
iteration 179, loss = 0.28501152992248535
iteration 180, loss = 0.24845482409000397
iteration 181, loss = 0.24620702862739563
iteration 182, loss = 0.2783015966415405
iteration 183, loss = 0.2362860143184662
iteration 184, loss = 0.2692279517650604
iteration 185, loss = 0.2628675699234009
iteration 186, loss = 0.2726970911026001
iteration 187, loss = 0.26696765422821045
iteration 188, loss = 0.24815616011619568
iteration 189, loss = 0.27288633584976196
iteration 190, loss = 0.24299311637878418
iteration 191, loss = 0.31566041707992554
iteration 192, loss = 0.2461027354001999
iteration 193, loss = 0.2613975703716278
iteration 194, loss = 0.2736945152282715
iteration 195, loss = 0.2211369276046753
iteration 196, loss = 0.31094181537628174
iteration 197, loss = 0.24391597509384155
iteration 198, loss = 0.24220781028270721
iteration 199, loss = 0.30107298493385315
iteration 200, loss = 0.2728498578071594
iteration 201, loss = 0.3006898760795593
iteration 202, loss = 0.24519391357898712
iteration 203, loss = 0.2500860095024109
iteration 204, loss = 0.24642574787139893
iteration 205, loss = 0.317333847284317
iteration 206, loss = 0.27601268887519836
iteration 207, loss = 0.2632405161857605
iteration 208, loss = 0.24831928312778473
iteration 209, loss = 0.23208534717559814
iteration 210, loss = 0.2582100033760071
iteration 211, loss = 0.24111336469650269
iteration 212, loss = 0.24888041615486145
iteration 213, loss = 0.22534288465976715
iteration 214, loss = 0.2813569903373718
iteration 215, loss = 0.2446335405111313
iteration 216, loss = 0.2824352979660034
iteration 217, loss = 0.2505151033401489
iteration 218, loss = 0.27760523557662964
iteration 219, loss = 0.24263475835323334
iteration 220, loss = 0.25754067301750183
iteration 221, loss = 0.28010183572769165
iteration 222, loss = 0.25264662504196167
iteration 223, loss = 0.2504878640174866
iteration 224, loss = 0.24553409218788147
iteration 225, loss = 0.25776922702789307
iteration 226, loss = 0.23424938321113586
iteration 227, loss = 0.24067120254039764
iteration 228, loss = 0.25645631551742554
iteration 229, loss = 0.24423085153102875
iteration 230, loss = 0.24332579970359802
iteration 231, loss = 0.23754122853279114
iteration 232, loss = 0.2904345691204071
iteration 233, loss = 0.25343772768974304
iteration 234, loss = 0.2712984085083008
iteration 235, loss = 0.25607767701148987
iteration 236, loss = 0.2543971538543701
iteration 237, loss = 0.2545889616012573
iteration 238, loss = 0.24997127056121826
iteration 239, loss = 0.2627265453338623
iteration 240, loss = 0.2490265816450119
iteration 241, loss = 0.2646605968475342
iteration 242, loss = 0.2558506429195404
iteration 243, loss = 0.2418259233236313
iteration 244, loss = 0.24886639416217804
iteration 245, loss = 0.2737465798854828
iteration 246, loss = 0.25178107619285583
iteration 247, loss = 0.26472365856170654
iteration 248, loss = 0.25256985425949097
iteration 249, loss = 0.25323551893234253
iteration 250, loss = 0.22515808045864105
iteration 251, loss = 0.24228300154209137
iteration 252, loss = 0.24005533754825592
iteration 253, loss = 0.2581345736980438
iteration 254, loss = 0.23788541555404663
iteration 255, loss = 0.2568915784358978
iteration 256, loss = 0.2359059751033783
iteration 257, loss = 0.2573845386505127
iteration 258, loss = 0.2800888121128082
iteration 259, loss = 0.24415241181850433
iteration 260, loss = 0.2762469947338104
iteration 261, loss = 0.2362017035484314
iteration 262, loss = 0.23715128004550934
iteration 263, loss = 0.23848983645439148
iteration 264, loss = 0.2737902104854584
iteration 265, loss = 0.24603669345378876
iteration 266, loss = 0.2641288936138153
iteration 267, loss = 0.279091477394104
iteration 268, loss = 0.2665603458881378
iteration 269, loss = 0.25026363134384155
iteration 270, loss = 0.2559862732887268
iteration 271, loss = 0.2605913281440735
iteration 272, loss = 0.24191465973854065
iteration 273, loss = 0.29594314098358154
iteration 274, loss = 0.2547328472137451
iteration 275, loss = 0.23571927845478058
iteration 276, loss = 0.2161179929971695
iteration 277, loss = 0.21968933939933777
iteration 278, loss = 0.23984354734420776
iteration 279, loss = 0.21236877143383026
iteration 280, loss = 0.23820970952510834
iteration 281, loss = 0.22539015114307404
iteration 282, loss = 0.22255854308605194
iteration 283, loss = 0.28904473781585693
iteration 284, loss = 0.2891640067100525
iteration 285, loss = 0.24619640409946442
iteration 286, loss = 0.21445950865745544
iteration 287, loss = 0.24344193935394287
iteration 288, loss = 0.22598473727703094
iteration 289, loss = 0.22755613923072815
iteration 290, loss = 0.2451743185520172
iteration 291, loss = 0.25114819407463074
iteration 292, loss = 0.21130576729774475
iteration 293, loss = 0.2417343705892563
iteration 294, loss = 0.26091399788856506
iteration 295, loss = 0.2582170367240906
iteration 296, loss = 0.21471917629241943
iteration 297, loss = 0.2748960554599762
iteration 298, loss = 0.2522420585155487
iteration 299, loss = 0.26663631200790405
iteration 300, loss = 0.23848716914653778
iteration 1, loss = 0.27805668115615845
iteration 2, loss = 0.2714082896709442
iteration 3, loss = 0.259991854429245
iteration 4, loss = 0.23020781576633453
iteration 5, loss = 0.24054259061813354
iteration 6, loss = 0.2522468566894531
iteration 7, loss = 0.23360475897789001
iteration 8, loss = 0.22159624099731445
iteration 9, loss = 0.2584024667739868
iteration 10, loss = 0.2386220097541809
iteration 11, loss = 0.25449109077453613
iteration 12, loss = 0.2279175966978073
iteration 13, loss = 0.22697508335113525
iteration 14, loss = 0.22983747720718384
iteration 15, loss = 0.24192290008068085
iteration 16, loss = 0.2627439796924591
iteration 17, loss = 0.22089582681655884
iteration 18, loss = 0.23595988750457764
iteration 19, loss = 0.24771687388420105
iteration 20, loss = 0.2364984005689621
iteration 21, loss = 0.21468690037727356
iteration 22, loss = 0.2429996281862259
iteration 23, loss = 0.21498408913612366
iteration 24, loss = 0.27288949489593506
iteration 25, loss = 0.21952050924301147
iteration 26, loss = 0.24287106096744537
iteration 27, loss = 0.23154284060001373
iteration 28, loss = 0.2266872227191925
iteration 29, loss = 0.23005583882331848
iteration 30, loss = 0.29065290093421936
iteration 31, loss = 0.23718778789043427
iteration 32, loss = 0.22806960344314575
iteration 33, loss = 0.21585245430469513
iteration 34, loss = 0.19720591604709625
iteration 35, loss = 0.21958519518375397
iteration 36, loss = 0.2991091310977936
iteration 37, loss = 0.2325184941291809
iteration 38, loss = 0.2206050604581833
iteration 39, loss = 0.250052809715271
iteration 40, loss = 0.24566513299942017
iteration 41, loss = 0.23008409142494202
iteration 42, loss = 0.23169687390327454
iteration 43, loss = 0.25065338611602783
iteration 44, loss = 0.23051661252975464
iteration 45, loss = 0.23021918535232544
iteration 46, loss = 0.23677439987659454
iteration 47, loss = 0.2643967866897583
iteration 48, loss = 0.26338428258895874
iteration 49, loss = 0.220893993973732
iteration 50, loss = 0.2319639027118683
iteration 51, loss = 0.22039398550987244
iteration 52, loss = 0.21456556022167206
iteration 53, loss = 0.25285354256629944
iteration 54, loss = 0.22687368094921112
iteration 55, loss = 0.2306240350008011
iteration 56, loss = 0.2191283404827118
iteration 57, loss = 0.25254273414611816
iteration 58, loss = 0.2107011079788208
iteration 59, loss = 0.2525092661380768
iteration 60, loss = 0.24738451838493347
iteration 61, loss = 0.21397417783737183
iteration 62, loss = 0.24130059778690338
iteration 63, loss = 0.2147926688194275
iteration 64, loss = 0.2147974669933319
iteration 65, loss = 0.22849754989147186
iteration 66, loss = 0.21797136962413788
iteration 67, loss = 0.27784472703933716
iteration 68, loss = 0.20617613196372986
iteration 69, loss = 0.237993523478508
iteration 70, loss = 0.2488481104373932
iteration 71, loss = 0.23722437024116516
iteration 72, loss = 0.24016810953617096
iteration 73, loss = 0.26648420095443726
iteration 74, loss = 0.2810892164707184
iteration 75, loss = 0.2391560971736908
iteration 76, loss = 0.20786575973033905
iteration 77, loss = 0.23217841982841492
iteration 78, loss = 0.22903193533420563
iteration 79, loss = 0.23304101824760437
iteration 80, loss = 0.23505936563014984
iteration 81, loss = 0.2424108237028122
iteration 82, loss = 0.22119595110416412
iteration 83, loss = 0.24485240876674652
iteration 84, loss = 0.224777489900589
iteration 85, loss = 0.22489839792251587
iteration 86, loss = 0.22312483191490173
iteration 87, loss = 0.2185065895318985
iteration 88, loss = 0.22469501197338104
iteration 89, loss = 0.23896048963069916
iteration 90, loss = 0.2246333658695221
iteration 91, loss = 0.23392421007156372
iteration 92, loss = 0.20125964283943176
iteration 93, loss = 0.24271592497825623
iteration 94, loss = 0.25075021386146545
iteration 95, loss = 0.25122538208961487
iteration 96, loss = 0.21971116960048676
iteration 97, loss = 0.21710512042045593
iteration 98, loss = 0.22585636377334595
iteration 99, loss = 0.23279926180839539
iteration 100, loss = 0.23699580132961273
iteration 101, loss = 0.23671215772628784
iteration 102, loss = 0.2055133432149887
iteration 103, loss = 0.2459382563829422
iteration 104, loss = 0.20820803940296173
iteration 105, loss = 0.1935259848833084
iteration 106, loss = 0.21333342790603638
iteration 107, loss = 0.2174641340970993
iteration 108, loss = 0.25459927320480347
iteration 109, loss = 0.2394590526819229
iteration 110, loss = 0.2037908136844635
iteration 111, loss = 0.20464259386062622
iteration 112, loss = 0.21011239290237427
iteration 113, loss = 0.21597176790237427
iteration 114, loss = 0.22237031161785126
iteration 115, loss = 0.18350708484649658
iteration 116, loss = 0.24982210993766785
iteration 117, loss = 0.21451401710510254
iteration 118, loss = 0.21338310837745667
iteration 119, loss = 0.23025652766227722
iteration 120, loss = 0.22534380853176117
iteration 121, loss = 0.2264469563961029
iteration 122, loss = 0.2366095930337906
iteration 123, loss = 0.20415692031383514
iteration 124, loss = 0.21988947689533234
iteration 125, loss = 0.20552216470241547
iteration 126, loss = 0.20051071047782898
iteration 127, loss = 0.2488504946231842
iteration 128, loss = 0.2594922184944153
iteration 129, loss = 0.22788652777671814
iteration 130, loss = 0.21306923031806946
iteration 131, loss = 0.22872892022132874
iteration 132, loss = 0.20358997583389282
iteration 133, loss = 0.2282906174659729
iteration 134, loss = 0.208982914686203
iteration 135, loss = 0.2045891135931015
iteration 136, loss = 0.22151657938957214
iteration 137, loss = 0.22531461715698242
iteration 138, loss = 0.21019935607910156
iteration 139, loss = 0.2040027230978012
iteration 140, loss = 0.21938200294971466
iteration 141, loss = 0.2167818546295166
iteration 142, loss = 0.20993879437446594
iteration 143, loss = 0.232169046998024
iteration 144, loss = 0.22039321064949036
iteration 145, loss = 0.20205052196979523
iteration 146, loss = 0.20956778526306152
iteration 147, loss = 0.22415439784526825
iteration 148, loss = 0.28874486684799194
iteration 149, loss = 0.21821996569633484
iteration 150, loss = 0.21325063705444336
iteration 151, loss = 0.21599557995796204
iteration 152, loss = 0.2639855146408081
iteration 153, loss = 0.19480225443840027
iteration 154, loss = 0.21186117827892303
iteration 155, loss = 0.22299093008041382
iteration 156, loss = 0.24435122311115265
iteration 157, loss = 0.21088621020317078
iteration 158, loss = 0.2029128074645996
iteration 159, loss = 0.18550105392932892
iteration 160, loss = 0.1937464475631714
iteration 161, loss = 0.21714746952056885
iteration 162, loss = 0.17675834894180298
iteration 163, loss = 0.20179639756679535
iteration 164, loss = 0.2080034464597702
iteration 165, loss = 0.259034663438797
iteration 166, loss = 0.18908388912677765
iteration 167, loss = 0.226027250289917
iteration 168, loss = 0.1933049112558365
iteration 169, loss = 0.18405744433403015
iteration 170, loss = 0.2035740613937378
iteration 171, loss = 0.2025887370109558
iteration 172, loss = 0.23789754509925842
iteration 173, loss = 0.20853079855442047
iteration 174, loss = 0.193055659532547
iteration 175, loss = 0.20899607241153717
iteration 176, loss = 0.23676621913909912
iteration 177, loss = 0.2223224639892578
iteration 178, loss = 0.19989562034606934
iteration 179, loss = 0.22960910201072693
iteration 180, loss = 0.22192183136940002
iteration 181, loss = 0.19793014228343964
iteration 182, loss = 0.2162380963563919
iteration 183, loss = 0.23547084629535675
iteration 184, loss = 0.2159484475851059
iteration 185, loss = 0.19566601514816284
iteration 186, loss = 0.19521348178386688
iteration 187, loss = 0.24427247047424316
iteration 188, loss = 0.2025558352470398
iteration 189, loss = 0.18770140409469604
iteration 190, loss = 0.2021833062171936
iteration 191, loss = 0.2041109949350357
iteration 192, loss = 0.19578878581523895
iteration 193, loss = 0.18161509931087494
iteration 194, loss = 0.22341516613960266
iteration 195, loss = 0.18746569752693176
iteration 196, loss = 0.2060752511024475
iteration 197, loss = 0.21600031852722168
iteration 198, loss = 0.26355716586112976
iteration 199, loss = 0.19795449078083038
iteration 200, loss = 0.24452270567417145
iteration 201, loss = 0.16899850964546204
iteration 202, loss = 0.19316993653774261
iteration 203, loss = 0.20866741240024567
iteration 204, loss = 0.1709546595811844
iteration 205, loss = 0.22207070887088776
iteration 206, loss = 0.17103734612464905
iteration 207, loss = 0.17146116495132446
iteration 208, loss = 0.22026044130325317
iteration 209, loss = 0.19910678267478943
iteration 210, loss = 0.19170603156089783
iteration 211, loss = 0.17728866636753082
iteration 212, loss = 0.19605495035648346
iteration 213, loss = 0.2287539541721344
iteration 214, loss = 0.21336352825164795
iteration 215, loss = 0.19750532507896423
iteration 216, loss = 0.19851788878440857
iteration 217, loss = 0.21162591874599457
iteration 218, loss = 0.20304661989212036
iteration 219, loss = 0.2149033546447754
iteration 220, loss = 0.21221566200256348
iteration 221, loss = 0.1709917187690735
iteration 222, loss = 0.1977771371603012
iteration 223, loss = 0.2037704437971115
iteration 224, loss = 0.22232216596603394
iteration 225, loss = 0.2007925808429718
iteration 226, loss = 0.2253047227859497
iteration 227, loss = 0.2232331931591034
iteration 228, loss = 0.22628796100616455
iteration 229, loss = 0.2448752373456955
iteration 230, loss = 0.2098310887813568
iteration 231, loss = 0.20136582851409912
iteration 232, loss = 0.18104660511016846
iteration 233, loss = 0.23162609338760376
iteration 234, loss = 0.17683514952659607
iteration 235, loss = 0.19877594709396362
iteration 236, loss = 0.23624269664287567
iteration 237, loss = 0.19470058381557465
iteration 238, loss = 0.2033623456954956
iteration 239, loss = 0.19882410764694214
iteration 240, loss = 0.20139066874980927
iteration 241, loss = 0.19468942284584045
iteration 242, loss = 0.16330215334892273
iteration 243, loss = 0.18845310807228088
iteration 244, loss = 0.20997868478298187
iteration 245, loss = 0.15813800692558289
iteration 246, loss = 0.22079214453697205
iteration 247, loss = 0.17143385112285614
iteration 248, loss = 0.18873140215873718
iteration 249, loss = 0.1947675347328186
iteration 250, loss = 0.21718421578407288
iteration 251, loss = 0.1922190636396408
iteration 252, loss = 0.18619072437286377
iteration 253, loss = 0.19595539569854736
iteration 254, loss = 0.18311604857444763
iteration 255, loss = 0.21815060079097748
iteration 256, loss = 0.18720339238643646
iteration 257, loss = 0.20563384890556335
iteration 258, loss = 0.1775766909122467
iteration 259, loss = 0.19804102182388306
iteration 260, loss = 0.1953020542860031
iteration 261, loss = 0.21096651256084442
iteration 262, loss = 0.19562195241451263
iteration 263, loss = 0.2592337727546692
iteration 264, loss = 0.2360227257013321
iteration 265, loss = 0.20688799023628235
iteration 266, loss = 0.2165931761264801
iteration 267, loss = 0.1966746300458908
iteration 268, loss = 0.17137809097766876
iteration 269, loss = 0.18112674355506897
iteration 270, loss = 0.20595774054527283
iteration 271, loss = 0.17748971283435822
iteration 272, loss = 0.1811486929655075
iteration 273, loss = 0.19884569942951202
iteration 274, loss = 0.1864582598209381
iteration 275, loss = 0.16081146895885468
iteration 276, loss = 0.20570193231105804
iteration 277, loss = 0.17930085957050323
iteration 278, loss = 0.259848952293396
iteration 279, loss = 0.20472343266010284
iteration 280, loss = 0.19216623902320862
iteration 281, loss = 0.17856858670711517
iteration 282, loss = 0.21358107030391693
iteration 283, loss = 0.2380988746881485
iteration 284, loss = 0.18821066617965698
iteration 285, loss = 0.18125347793102264
iteration 286, loss = 0.22514180839061737
iteration 287, loss = 0.21581251919269562
iteration 288, loss = 0.17272885143756866
iteration 289, loss = 0.17575110495090485
iteration 290, loss = 0.19201569259166718
iteration 291, loss = 0.2088853269815445
iteration 292, loss = 0.23084329068660736
iteration 293, loss = 0.17287079989910126
iteration 294, loss = 0.17767083644866943
iteration 295, loss = 0.18129856884479523
iteration 296, loss = 0.17442382872104645
iteration 297, loss = 0.1964494287967682
iteration 298, loss = 0.19389092922210693
iteration 299, loss = 0.16400887072086334
iteration 300, loss = 0.1871260702610016
iteration 1, loss = 0.18803717195987701
iteration 2, loss = 0.22722932696342468
iteration 3, loss = 0.1924441009759903
iteration 4, loss = 0.1406935155391693
iteration 5, loss = 0.18459901213645935
iteration 6, loss = 0.16846652328968048
iteration 7, loss = 0.16150224208831787
iteration 8, loss = 0.22766536474227905
iteration 9, loss = 0.1950787901878357
iteration 10, loss = 0.2008265256881714
iteration 11, loss = 0.1977936327457428
iteration 12, loss = 0.2220013290643692
iteration 13, loss = 0.15698817372322083
iteration 14, loss = 0.18290524184703827
iteration 15, loss = 0.15883122384548187
iteration 16, loss = 0.20437106490135193
iteration 17, loss = 0.18808294832706451
iteration 18, loss = 0.2031271904706955
iteration 19, loss = 0.19432592391967773
iteration 20, loss = 0.1814366728067398
iteration 21, loss = 0.18258720636367798
iteration 22, loss = 0.16593843698501587
iteration 23, loss = 0.19675259292125702
iteration 24, loss = 0.17537233233451843
iteration 25, loss = 0.20967160165309906
iteration 26, loss = 0.17148320376873016
iteration 27, loss = 0.17710919678211212
iteration 28, loss = 0.17071101069450378
iteration 29, loss = 0.19361548125743866
iteration 30, loss = 0.18134301900863647
iteration 31, loss = 0.18708759546279907
iteration 32, loss = 0.18723194301128387
iteration 33, loss = 0.17039036750793457
iteration 34, loss = 0.16015800833702087
iteration 35, loss = 0.19033172726631165
iteration 36, loss = 0.1779695600271225
iteration 37, loss = 0.19129562377929688
iteration 38, loss = 0.19739320874214172
iteration 39, loss = 0.17913153767585754
iteration 40, loss = 0.19876408576965332
iteration 41, loss = 0.17226751148700714
iteration 42, loss = 0.19920702278614044
iteration 43, loss = 0.22823283076286316
iteration 44, loss = 0.19061161577701569
iteration 45, loss = 0.16267487406730652
iteration 46, loss = 0.16295483708381653
iteration 47, loss = 0.15169039368629456
iteration 48, loss = 0.17392095923423767
iteration 49, loss = 0.19671925902366638
iteration 50, loss = 0.20819967985153198
iteration 51, loss = 0.18300983309745789
iteration 52, loss = 0.15157511830329895
iteration 53, loss = 0.1826745569705963
iteration 54, loss = 0.15953123569488525
iteration 55, loss = 0.18196788430213928
iteration 56, loss = 0.18988992273807526
iteration 57, loss = 0.12930983304977417
iteration 58, loss = 0.15800347924232483
iteration 59, loss = 0.19940543174743652
iteration 60, loss = 0.1841987818479538
iteration 61, loss = 0.26168838143348694
iteration 62, loss = 0.15486927330493927
iteration 63, loss = 0.1478257030248642
iteration 64, loss = 0.19420072436332703
iteration 65, loss = 0.18280047178268433
iteration 66, loss = 0.18895624577999115
iteration 67, loss = 0.16940808296203613
iteration 68, loss = 0.18402883410453796
iteration 69, loss = 0.1569945067167282
iteration 70, loss = 0.20587028563022614
iteration 71, loss = 0.16506914794445038
iteration 72, loss = 0.18292587995529175
iteration 73, loss = 0.16555608808994293
iteration 74, loss = 0.14574113488197327
iteration 75, loss = 0.16601888835430145
iteration 76, loss = 0.15565897524356842
iteration 77, loss = 0.17346321046352386
iteration 78, loss = 0.2165725976228714
iteration 79, loss = 0.1749543845653534
iteration 80, loss = 0.19278232753276825
iteration 81, loss = 0.15722455084323883
iteration 82, loss = 0.15386730432510376
iteration 83, loss = 0.1834356188774109
iteration 84, loss = 0.1569744199514389
iteration 85, loss = 0.16548383235931396
iteration 86, loss = 0.19819992780685425
iteration 87, loss = 0.17807868123054504
iteration 88, loss = 0.2248217910528183
iteration 89, loss = 0.20063965022563934
iteration 90, loss = 0.152325838804245
iteration 91, loss = 0.17964620888233185
iteration 92, loss = 0.19696691632270813
iteration 93, loss = 0.1686972975730896
iteration 94, loss = 0.1481739580631256
iteration 95, loss = 0.17128920555114746
iteration 96, loss = 0.14462517201900482
iteration 97, loss = 0.15961682796478271
iteration 98, loss = 0.1884467452764511
iteration 99, loss = 0.18159614503383636
iteration 100, loss = 0.17198123037815094
iteration 101, loss = 0.16547805070877075
iteration 102, loss = 0.16293729841709137
iteration 103, loss = 0.16355787217617035
iteration 104, loss = 0.15473169088363647
iteration 105, loss = 0.18324923515319824
iteration 106, loss = 0.16135869920253754
iteration 107, loss = 0.15556803345680237
iteration 108, loss = 0.1597565859556198
iteration 109, loss = 0.18219099938869476
iteration 110, loss = 0.19390195608139038
iteration 111, loss = 0.20247310400009155
iteration 112, loss = 0.1596245914697647
iteration 113, loss = 0.1574578583240509
iteration 114, loss = 0.13727664947509766
iteration 115, loss = 0.14873282611370087
iteration 116, loss = 0.13829250633716583
iteration 117, loss = 0.167636439204216
iteration 118, loss = 0.14789791405200958
iteration 119, loss = 0.1791391670703888
iteration 120, loss = 0.18698671460151672
iteration 121, loss = 0.16674548387527466
iteration 122, loss = 0.17625345289707184
iteration 123, loss = 0.1705407351255417
iteration 124, loss = 0.17636795341968536
iteration 125, loss = 0.14616316556930542
iteration 126, loss = 0.1728186458349228
iteration 127, loss = 0.1531686782836914
iteration 128, loss = 0.17981581389904022
iteration 129, loss = 0.1546919345855713
iteration 130, loss = 0.13862061500549316
iteration 131, loss = 0.163496196269989
iteration 132, loss = 0.16338329017162323
iteration 133, loss = 0.2220425307750702
iteration 134, loss = 0.15827029943466187
iteration 135, loss = 0.17731116712093353
iteration 136, loss = 0.1613306850194931
iteration 137, loss = 0.16319003701210022
iteration 138, loss = 0.16836147010326385
iteration 139, loss = 0.1476050764322281
iteration 140, loss = 0.16300024092197418
iteration 141, loss = 0.23932403326034546
iteration 142, loss = 0.1667780578136444
iteration 143, loss = 0.17506664991378784
iteration 144, loss = 0.18169179558753967
iteration 145, loss = 0.17410267889499664
iteration 146, loss = 0.1641760766506195
iteration 147, loss = 0.1519566774368286
iteration 148, loss = 0.17176184058189392
iteration 149, loss = 0.18566162884235382
iteration 150, loss = 0.15932561457157135
iteration 151, loss = 0.16116963326931
iteration 152, loss = 0.13659124076366425
iteration 153, loss = 0.16373923420906067
iteration 154, loss = 0.1710188090801239
iteration 155, loss = 0.1587361842393875
iteration 156, loss = 0.16548490524291992
iteration 157, loss = 0.13121463358402252
iteration 158, loss = 0.16031929850578308
iteration 159, loss = 0.1477447897195816
iteration 160, loss = 0.16184833645820618
iteration 161, loss = 0.16819092631340027
iteration 162, loss = 0.13626211881637573
iteration 163, loss = 0.14429549872875214
iteration 164, loss = 0.15391062200069427
iteration 165, loss = 0.17150737345218658
iteration 166, loss = 0.18604202568531036
iteration 167, loss = 0.16768032312393188
iteration 168, loss = 0.13651670515537262
iteration 169, loss = 0.2025751769542694
iteration 170, loss = 0.17763817310333252
iteration 171, loss = 0.17283950746059418
iteration 172, loss = 0.2305508553981781
iteration 173, loss = 0.177952378988266
iteration 174, loss = 0.21926116943359375
iteration 175, loss = 0.1593305617570877
iteration 176, loss = 0.1458187848329544
iteration 177, loss = 0.15922847390174866
iteration 178, loss = 0.16295413672924042
iteration 179, loss = 0.18357406556606293
iteration 180, loss = 0.12945523858070374
iteration 181, loss = 0.1830294132232666
iteration 182, loss = 0.16651052236557007
iteration 183, loss = 0.16945011913776398
iteration 184, loss = 0.14464670419692993
iteration 185, loss = 0.1849040389060974
iteration 186, loss = 0.14609745144844055
iteration 187, loss = 0.1882539987564087
iteration 188, loss = 0.16198423504829407
iteration 189, loss = 0.15537786483764648
iteration 190, loss = 0.1623258739709854
iteration 191, loss = 0.16359516978263855
iteration 192, loss = 0.17197638750076294
iteration 193, loss = 0.1573193520307541
iteration 194, loss = 0.1514485627412796
iteration 195, loss = 0.14912502467632294
iteration 196, loss = 0.1766805797815323
iteration 197, loss = 0.1533626765012741
iteration 198, loss = 0.158238485455513
iteration 199, loss = 0.16943514347076416
iteration 200, loss = 0.15628302097320557
iteration 201, loss = 0.17421849071979523
iteration 202, loss = 0.12108147144317627
iteration 203, loss = 0.1776261329650879
iteration 204, loss = 0.16062700748443604
iteration 205, loss = 0.1523185670375824
iteration 206, loss = 0.1624060422182083
iteration 207, loss = 0.16081318259239197
iteration 208, loss = 0.16916108131408691
iteration 209, loss = 0.12240367382764816
iteration 210, loss = 0.19210034608840942
iteration 211, loss = 0.18221165239810944
iteration 212, loss = 0.11551286280155182
iteration 213, loss = 0.16917282342910767
iteration 214, loss = 0.14012649655342102
iteration 215, loss = 0.12999774515628815
iteration 216, loss = 0.13843181729316711
iteration 217, loss = 0.16507460176944733
iteration 218, loss = 0.14799822866916656
iteration 219, loss = 0.15145178139209747
iteration 220, loss = 0.15050636231899261
iteration 221, loss = 0.13495463132858276
iteration 222, loss = 0.13602733612060547
iteration 223, loss = 0.15160122513771057
iteration 224, loss = 0.1373206079006195
iteration 225, loss = 0.14831431210041046
iteration 226, loss = 0.1432563215494156
iteration 227, loss = 0.14896553754806519
iteration 228, loss = 0.1255016028881073
iteration 229, loss = 0.15553849935531616
iteration 230, loss = 0.1504809558391571
iteration 231, loss = 0.16363593935966492
iteration 232, loss = 0.14768004417419434
iteration 233, loss = 0.16285625100135803
iteration 234, loss = 0.17070642113685608
iteration 235, loss = 0.16559165716171265
iteration 236, loss = 0.17479315400123596
iteration 237, loss = 0.1618567258119583
iteration 238, loss = 0.13391001522541046
iteration 239, loss = 0.20237869024276733
iteration 240, loss = 0.16980458796024323
iteration 241, loss = 0.16777177155017853
iteration 242, loss = 0.1450064480304718
iteration 243, loss = 0.12498334050178528
iteration 244, loss = 0.14393842220306396
iteration 245, loss = 0.16609953343868256
iteration 246, loss = 0.13442376255989075
iteration 247, loss = 0.150089830160141
iteration 248, loss = 0.16467437148094177
iteration 249, loss = 0.11441953480243683
iteration 250, loss = 0.14440834522247314
iteration 251, loss = 0.19183394312858582
iteration 252, loss = 0.14647312462329865
iteration 253, loss = 0.1462826430797577
iteration 254, loss = 0.14827528595924377
iteration 255, loss = 0.17767126858234406
iteration 256, loss = 0.1689194291830063
iteration 257, loss = 0.17419302463531494
iteration 258, loss = 0.1315663754940033
iteration 259, loss = 0.15535607933998108
iteration 260, loss = 0.14784997701644897
iteration 261, loss = 0.14969751238822937
iteration 262, loss = 0.15238294005393982
iteration 263, loss = 0.14383907616138458
iteration 264, loss = 0.14954452216625214
iteration 265, loss = 0.16787025332450867
iteration 266, loss = 0.1471138596534729
iteration 267, loss = 0.14405080676078796
iteration 268, loss = 0.13231724500656128
iteration 269, loss = 0.11210764944553375
iteration 270, loss = 0.15864846110343933
iteration 271, loss = 0.178888201713562
iteration 272, loss = 0.14325085282325745
iteration 273, loss = 0.13995355367660522
iteration 274, loss = 0.12517501413822174
iteration 275, loss = 0.14212840795516968
iteration 276, loss = 0.1349891573190689
iteration 277, loss = 0.13491378724575043
iteration 278, loss = 0.15650728344917297
iteration 279, loss = 0.18107381463050842
iteration 280, loss = 0.1319868564605713
iteration 281, loss = 0.14807972311973572
iteration 282, loss = 0.12541833519935608
iteration 283, loss = 0.10954534262418747
iteration 284, loss = 0.11508964747190475
iteration 285, loss = 0.18119904398918152
iteration 286, loss = 0.11853770911693573
iteration 287, loss = 0.11070272326469421
iteration 288, loss = 0.14584527909755707
iteration 289, loss = 0.15371575951576233
iteration 290, loss = 0.16249343752861023
iteration 291, loss = 0.1807275265455246
iteration 292, loss = 0.1455167829990387
iteration 293, loss = 0.13409292697906494
iteration 294, loss = 0.16436314582824707
iteration 295, loss = 0.17685924470424652
iteration 296, loss = 0.16845980286598206
iteration 297, loss = 0.14761367440223694
iteration 298, loss = 0.14417420327663422
iteration 299, loss = 0.15272632241249084
iteration 300, loss = 0.16203325986862183
iteration 1, loss = 0.14080245792865753
iteration 2, loss = 0.15433178842067719
iteration 3, loss = 0.12363927066326141
iteration 4, loss = 0.2033763825893402
iteration 5, loss = 0.14299045503139496
iteration 6, loss = 0.126497283577919
iteration 7, loss = 0.13943853974342346
iteration 8, loss = 0.12341383099555969
iteration 9, loss = 0.11619574576616287
iteration 10, loss = 0.14972974359989166
iteration 11, loss = 0.14671917259693146
iteration 12, loss = 0.09779056161642075
iteration 13, loss = 0.1646876484155655
iteration 14, loss = 0.15494808554649353
iteration 15, loss = 0.1325361132621765
iteration 16, loss = 0.13715049624443054
iteration 17, loss = 0.1577090620994568
iteration 18, loss = 0.151201531291008
iteration 19, loss = 0.13015979528427124
iteration 20, loss = 0.13479669392108917
iteration 21, loss = 0.13113993406295776
iteration 22, loss = 0.13895028829574585
iteration 23, loss = 0.12707819044589996
iteration 24, loss = 0.16306209564208984
iteration 25, loss = 0.134286031126976
iteration 26, loss = 0.15293818712234497
iteration 27, loss = 0.10962355136871338
iteration 28, loss = 0.1331770122051239
iteration 29, loss = 0.1536363959312439
iteration 30, loss = 0.15047329664230347
iteration 31, loss = 0.13611000776290894
iteration 32, loss = 0.1081516370177269
iteration 33, loss = 0.14957383275032043
iteration 34, loss = 0.138718381524086
iteration 35, loss = 0.12994831800460815
iteration 36, loss = 0.14614197611808777
iteration 37, loss = 0.14647316932678223
iteration 38, loss = 0.14795437455177307
iteration 39, loss = 0.13472887873649597
iteration 40, loss = 0.1273619830608368
iteration 41, loss = 0.1287553906440735
iteration 42, loss = 0.16782918572425842
iteration 43, loss = 0.1351967751979828
iteration 44, loss = 0.17278458178043365
iteration 45, loss = 0.08009400963783264
iteration 46, loss = 0.13388244807720184
iteration 47, loss = 0.1350187212228775
iteration 48, loss = 0.15335053205490112
iteration 49, loss = 0.12253778427839279
iteration 50, loss = 0.11452389508485794
iteration 51, loss = 0.10308259725570679
iteration 52, loss = 0.144767165184021
iteration 53, loss = 0.12883830070495605
iteration 54, loss = 0.12441268563270569
iteration 55, loss = 0.15182700753211975
iteration 56, loss = 0.11332200467586517
iteration 57, loss = 0.18281173706054688
iteration 58, loss = 0.0957421362400055
iteration 59, loss = 0.17787015438079834
iteration 60, loss = 0.13724014163017273
iteration 61, loss = 0.12969990074634552
iteration 62, loss = 0.15867498517036438
iteration 63, loss = 0.13226766884326935
iteration 64, loss = 0.15494240820407867
iteration 65, loss = 0.15457309782505035
iteration 66, loss = 0.16171687841415405
iteration 67, loss = 0.14381666481494904
iteration 68, loss = 0.11652376502752304
iteration 69, loss = 0.15020276606082916
iteration 70, loss = 0.13168998062610626
iteration 71, loss = 0.1255897432565689
iteration 72, loss = 0.11456046998500824
iteration 73, loss = 0.13835854828357697
iteration 74, loss = 0.1277483105659485
iteration 75, loss = 0.10957517474889755
iteration 76, loss = 0.1413157731294632
iteration 77, loss = 0.1312251091003418
iteration 78, loss = 0.12649863958358765
iteration 79, loss = 0.14820200204849243
iteration 80, loss = 0.15732334554195404
iteration 81, loss = 0.16604235768318176
iteration 82, loss = 0.12519054114818573
iteration 83, loss = 0.1300850510597229
iteration 84, loss = 0.1399058997631073
iteration 85, loss = 0.11275184154510498
iteration 86, loss = 0.14371439814567566
iteration 87, loss = 0.1495378315448761
iteration 88, loss = 0.13138285279273987
iteration 89, loss = 0.11219781637191772
iteration 90, loss = 0.13132187724113464
iteration 91, loss = 0.11867724359035492
iteration 92, loss = 0.11653195321559906
iteration 93, loss = 0.11011852324008942
iteration 94, loss = 0.07665718346834183
iteration 95, loss = 0.19406411051750183
iteration 96, loss = 0.15931980311870575
iteration 97, loss = 0.13063812255859375
iteration 98, loss = 0.13772349059581757
iteration 99, loss = 0.1355867087841034
iteration 100, loss = 0.16468077898025513
iteration 101, loss = 0.11891201138496399
iteration 102, loss = 0.1555875986814499
iteration 103, loss = 0.1424686759710312
iteration 104, loss = 0.16273172199726105
iteration 105, loss = 0.12065211683511734
iteration 106, loss = 0.1436721831560135
iteration 107, loss = 0.14523833990097046
iteration 108, loss = 0.12410353124141693
iteration 109, loss = 0.1426979899406433
iteration 110, loss = 0.1287248581647873
iteration 111, loss = 0.11803106218576431
iteration 112, loss = 0.09607475250959396
iteration 113, loss = 0.08925598114728928
iteration 114, loss = 0.14484846591949463
iteration 115, loss = 0.11682547628879547
iteration 116, loss = 0.0790286436676979
iteration 117, loss = 0.08254225552082062
iteration 118, loss = 0.1215495765209198
iteration 119, loss = 0.1687099039554596
iteration 120, loss = 0.12563243508338928
iteration 121, loss = 0.12573397159576416
iteration 122, loss = 0.09821029007434845
iteration 123, loss = 0.1292371153831482
iteration 124, loss = 0.1179814338684082
iteration 125, loss = 0.0923364907503128
iteration 126, loss = 0.09362483769655228
iteration 127, loss = 0.13430413603782654
iteration 128, loss = 0.1617254912853241
iteration 129, loss = 0.10882502049207687
iteration 130, loss = 0.19662868976593018
iteration 131, loss = 0.09513509273529053
iteration 132, loss = 0.12204347550868988
iteration 133, loss = 0.11740124225616455
iteration 134, loss = 0.13021248579025269
iteration 135, loss = 0.13191108405590057
iteration 136, loss = 0.09567108750343323
iteration 137, loss = 0.15711936354637146
iteration 138, loss = 0.1388542354106903
iteration 139, loss = 0.1348280906677246
iteration 140, loss = 0.12700916826725006
iteration 141, loss = 0.10988585650920868
iteration 142, loss = 0.11600484699010849
iteration 143, loss = 0.14285452663898468
iteration 144, loss = 0.14955362677574158
iteration 145, loss = 0.12339033931493759
iteration 146, loss = 0.13570508360862732
iteration 147, loss = 0.10941353440284729
iteration 148, loss = 0.1270856410264969
iteration 149, loss = 0.17816121876239777
iteration 150, loss = 0.09185776859521866
iteration 151, loss = 0.11820048093795776
iteration 152, loss = 0.11289648711681366
iteration 153, loss = 0.13385328650474548
iteration 154, loss = 0.1461070477962494
iteration 155, loss = 0.12544308602809906
iteration 156, loss = 0.12370281666517258
iteration 157, loss = 0.1580754816532135
iteration 158, loss = 0.1216665655374527
iteration 159, loss = 0.14424191415309906
iteration 160, loss = 0.11220258474349976
iteration 161, loss = 0.0958898589015007
iteration 162, loss = 0.14996977150440216
iteration 163, loss = 0.07673138380050659
iteration 164, loss = 0.11449164152145386
iteration 165, loss = 0.10554063320159912
iteration 166, loss = 0.12908419966697693
iteration 167, loss = 0.11430992931127548
iteration 168, loss = 0.09742891043424606
iteration 169, loss = 0.15893998742103577
iteration 170, loss = 0.14011891186237335
iteration 171, loss = 0.1359182745218277
iteration 172, loss = 0.07619680464267731
iteration 173, loss = 0.10834468901157379
iteration 174, loss = 0.1118713989853859
iteration 175, loss = 0.13570262491703033
iteration 176, loss = 0.09843134135007858
iteration 177, loss = 0.11761178076267242
iteration 178, loss = 0.11338920146226883
iteration 179, loss = 0.1451018750667572
iteration 180, loss = 0.13230358064174652
iteration 181, loss = 0.10683020204305649
iteration 182, loss = 0.14084698259830475
iteration 183, loss = 0.1310736984014511
iteration 184, loss = 0.11629105359315872
iteration 185, loss = 0.09615222364664078
iteration 186, loss = 0.12099120020866394
iteration 187, loss = 0.15205726027488708
iteration 188, loss = 0.09318021684885025
iteration 189, loss = 0.10328298062086105
iteration 190, loss = 0.11294083297252655
iteration 191, loss = 0.08608555793762207
iteration 192, loss = 0.09073204547166824
iteration 193, loss = 0.13500799238681793
iteration 194, loss = 0.11181807518005371
iteration 195, loss = 0.10445353388786316
iteration 196, loss = 0.14335386455059052
iteration 197, loss = 0.12348998337984085
iteration 198, loss = 0.13283509016036987
iteration 199, loss = 0.13084547221660614
iteration 200, loss = 0.06292763352394104
iteration 201, loss = 0.11952561140060425
iteration 202, loss = 0.1272614598274231
iteration 203, loss = 0.11967801302671432
iteration 204, loss = 0.09032471477985382
iteration 205, loss = 0.17576169967651367
iteration 206, loss = 0.15972936153411865
iteration 207, loss = 0.11878456175327301
iteration 208, loss = 0.129008948802948
iteration 209, loss = 0.10428119450807571
iteration 210, loss = 0.09103268384933472
iteration 211, loss = 0.0931069478392601
iteration 212, loss = 0.12437062710523605
iteration 213, loss = 0.10330681502819061
iteration 214, loss = 0.11985336989164352
iteration 215, loss = 0.07987253367900848
iteration 216, loss = 0.12245356291532516
iteration 217, loss = 0.11915908008813858
iteration 218, loss = 0.13611440360546112
iteration 219, loss = 0.11814239621162415
iteration 220, loss = 0.10315272957086563
iteration 221, loss = 0.10527686774730682
iteration 222, loss = 0.10723916441202164
iteration 223, loss = 0.07599826157093048
iteration 224, loss = 0.10067692399024963
iteration 225, loss = 0.11272194981575012
iteration 226, loss = 0.13273577392101288
iteration 227, loss = 0.11360680311918259
iteration 228, loss = 0.0912700966000557
iteration 229, loss = 0.1317564845085144
iteration 230, loss = 0.12854768335819244
iteration 231, loss = 0.09645383805036545
iteration 232, loss = 0.10369241237640381
iteration 233, loss = 0.1061357781291008
iteration 234, loss = 0.10224741697311401
iteration 235, loss = 0.12668953835964203
iteration 236, loss = 0.13330048322677612
iteration 237, loss = 0.12868939340114594
iteration 238, loss = 0.12455104291439056
iteration 239, loss = 0.1328263282775879
iteration 240, loss = 0.09590528905391693
iteration 241, loss = 0.10614099353551865
iteration 242, loss = 0.175320565700531
iteration 243, loss = 0.13720238208770752
iteration 244, loss = 0.10548290610313416
iteration 245, loss = 0.09861115366220474
iteration 246, loss = 0.10320727527141571
iteration 247, loss = 0.15201479196548462
iteration 248, loss = 0.11273718625307083
iteration 249, loss = 0.08001358062028885
iteration 250, loss = 0.11909975111484528
iteration 251, loss = 0.10885635763406754
iteration 252, loss = 0.1390419751405716
iteration 253, loss = 0.0985780730843544
iteration 254, loss = 0.13306401669979095
iteration 255, loss = 0.10906337201595306
iteration 256, loss = 0.1113346517086029
iteration 257, loss = 0.08358784765005112
iteration 258, loss = 0.06996843218803406
iteration 259, loss = 0.10136379301548004
iteration 260, loss = 0.11633401364088058
iteration 261, loss = 0.15999476611614227
iteration 262, loss = 0.12306723743677139
iteration 263, loss = 0.14629781246185303
iteration 264, loss = 0.1347624808549881
iteration 265, loss = 0.10552859306335449
iteration 266, loss = 0.11073765158653259
iteration 267, loss = 0.1068260595202446
iteration 268, loss = 0.14315780997276306
iteration 269, loss = 0.10032446682453156
iteration 270, loss = 0.08912342041730881
iteration 271, loss = 0.1024160161614418
iteration 272, loss = 0.1679891049861908
iteration 273, loss = 0.12267327308654785
iteration 274, loss = 0.12012777477502823
iteration 275, loss = 0.08895032107830048
iteration 276, loss = 0.14078910648822784
iteration 277, loss = 0.0840306431055069
iteration 278, loss = 0.12429754436016083
iteration 279, loss = 0.11366980522871017
iteration 280, loss = 0.11455343663692474
iteration 281, loss = 0.09970562160015106
iteration 282, loss = 0.08993697166442871
iteration 283, loss = 0.12743233144283295
iteration 284, loss = 0.06668607890605927
iteration 285, loss = 0.13351963460445404
iteration 286, loss = 0.09243723005056381
iteration 287, loss = 0.1308075189590454
iteration 288, loss = 0.08943582326173782
iteration 289, loss = 0.1124405637383461
iteration 290, loss = 0.11314652860164642
iteration 291, loss = 0.08807825297117233
iteration 292, loss = 0.08919468522071838
iteration 293, loss = 0.08540184050798416
iteration 294, loss = 0.09701289981603622
iteration 295, loss = 0.10153669118881226
iteration 296, loss = 0.11488347500562668
iteration 297, loss = 0.128663569688797
iteration 298, loss = 0.08400531113147736
iteration 299, loss = 0.07988707721233368
iteration 300, loss = 0.10966165363788605
iteration 1, loss = 0.13637413084506989
iteration 2, loss = 0.09969814121723175
iteration 3, loss = 0.11332868784666061
iteration 4, loss = 0.054216690361499786
iteration 5, loss = 0.10728760808706284
iteration 6, loss = 0.13674207031726837
iteration 7, loss = 0.11549437046051025
iteration 8, loss = 0.1253238469362259
iteration 9, loss = 0.09643728286027908
iteration 10, loss = 0.07666166871786118
iteration 11, loss = 0.12723854184150696
iteration 12, loss = 0.16034957766532898
iteration 13, loss = 0.11737243086099625
iteration 14, loss = 0.08439719676971436
iteration 15, loss = 0.11484363675117493
iteration 16, loss = 0.09877317398786545
iteration 17, loss = 0.1384509801864624
iteration 18, loss = 0.07405776530504227
iteration 19, loss = 0.09216731041669846
iteration 20, loss = 0.11054717749357224
iteration 21, loss = 0.09196841716766357
iteration 22, loss = 0.12106461077928543
iteration 23, loss = 0.10522591322660446
iteration 24, loss = 0.1276077926158905
iteration 25, loss = 0.1008925586938858
iteration 26, loss = 0.06481443345546722
iteration 27, loss = 0.13168050348758698
iteration 28, loss = 0.11637508869171143
iteration 29, loss = 0.10469385236501694
iteration 30, loss = 0.08397852629423141
iteration 31, loss = 0.11187934875488281
iteration 32, loss = 0.08800330013036728
iteration 33, loss = 0.08183286339044571
iteration 34, loss = 0.08694274723529816
iteration 35, loss = 0.08227546513080597
iteration 36, loss = 0.10469143837690353
iteration 37, loss = 0.0941198468208313
iteration 38, loss = 0.10437358170747757
iteration 39, loss = 0.11044039577245712
iteration 40, loss = 0.13172154128551483
iteration 41, loss = 0.12772338092327118
iteration 42, loss = 0.09057875722646713
iteration 43, loss = 0.0795031413435936
iteration 44, loss = 0.15291301906108856
iteration 45, loss = 0.12138378620147705
iteration 46, loss = 0.07993283122777939
iteration 47, loss = 0.1142391636967659
iteration 48, loss = 0.06954941898584366
iteration 49, loss = 0.09980510175228119
iteration 50, loss = 0.0778416246175766
iteration 51, loss = 0.10518648475408554
iteration 52, loss = 0.10860180854797363
iteration 53, loss = 0.14592191576957703
iteration 54, loss = 0.09241443127393723
iteration 55, loss = 0.07436308264732361
iteration 56, loss = 0.1016446202993393
iteration 57, loss = 0.10567249357700348
iteration 58, loss = 0.11393310129642487
iteration 59, loss = 0.07217808067798615
iteration 60, loss = 0.11481881886720657
iteration 61, loss = 0.11142796277999878
iteration 62, loss = 0.09844724088907242
iteration 63, loss = 0.12713780999183655
iteration 64, loss = 0.09727383404970169
iteration 65, loss = 0.1137346625328064
iteration 66, loss = 0.07714517414569855
iteration 67, loss = 0.1107698306441307
iteration 68, loss = 0.12451345473527908
iteration 69, loss = 0.13059253990650177
iteration 70, loss = 0.09755560755729675
iteration 71, loss = 0.10282499343156815
iteration 72, loss = 0.047033898532390594
iteration 73, loss = 0.08713097870349884
iteration 74, loss = 0.11097157746553421
iteration 75, loss = 0.10425965487957001
iteration 76, loss = 0.07443374395370483
iteration 77, loss = 0.11019541323184967
iteration 78, loss = 0.0827351063489914
iteration 79, loss = 0.12054568529129028
iteration 80, loss = 0.14937949180603027
iteration 81, loss = 0.09427977353334427
iteration 82, loss = 0.08720400184392929
iteration 83, loss = 0.06061679497361183
iteration 84, loss = 0.09412441402673721
iteration 85, loss = 0.08011303842067719
iteration 86, loss = 0.11370843648910522
iteration 87, loss = 0.10411912947893143
iteration 88, loss = 0.09464388340711594
iteration 89, loss = 0.10820034891366959
iteration 90, loss = 0.08108732849359512
iteration 91, loss = 0.12055423855781555
iteration 92, loss = 0.09424847364425659
iteration 93, loss = 0.09693728387355804
iteration 94, loss = 0.06405498087406158
iteration 95, loss = 0.1053256019949913
iteration 96, loss = 0.11133231222629547
iteration 97, loss = 0.10169171541929245
iteration 98, loss = 0.0853804349899292
iteration 99, loss = 0.08736645430326462
iteration 100, loss = 0.09626206755638123
iteration 101, loss = 0.09373311698436737
iteration 102, loss = 0.10271528363227844
iteration 103, loss = 0.10226529091596603
iteration 104, loss = 0.10945980995893478
iteration 105, loss = 0.1211824044585228
iteration 106, loss = 0.10144613683223724
iteration 107, loss = 0.07405471801757812
iteration 108, loss = 0.09097882360219955
iteration 109, loss = 0.08969198912382126
iteration 110, loss = 0.09803816676139832
iteration 111, loss = 0.08093536645174026
iteration 112, loss = 0.1228221207857132
iteration 113, loss = 0.09095872193574905
iteration 114, loss = 0.08707884699106216
iteration 115, loss = 0.1339683085680008
iteration 116, loss = 0.11057927459478378
iteration 117, loss = 0.10938598215579987
iteration 118, loss = 0.08550198376178741
iteration 119, loss = 0.08477207273244858
iteration 120, loss = 0.10728041082620621
iteration 121, loss = 0.09759020060300827
iteration 122, loss = 0.1040094792842865
iteration 123, loss = 0.07273396849632263
iteration 124, loss = 0.1182471290230751
iteration 125, loss = 0.058387164026498795
iteration 126, loss = 0.11142855137586594
iteration 127, loss = 0.09537236392498016
iteration 128, loss = 0.08466210216283798
iteration 129, loss = 0.10406091064214706
iteration 130, loss = 0.08891996741294861
iteration 131, loss = 0.11191903054714203
iteration 132, loss = 0.07726860791444778
iteration 133, loss = 0.06874208152294159
iteration 134, loss = 0.06736332178115845
iteration 135, loss = 0.08524373173713684
iteration 136, loss = 0.07336460798978806
iteration 137, loss = 0.09739889204502106
iteration 138, loss = 0.10926835983991623
iteration 139, loss = 0.09423108398914337
iteration 140, loss = 0.13933783769607544
iteration 141, loss = 0.08722024410963058
iteration 142, loss = 0.08187984675168991
iteration 143, loss = 0.08540044724941254
iteration 144, loss = 0.046618618071079254
iteration 145, loss = 0.07336486130952835
iteration 146, loss = 0.08184805512428284
iteration 147, loss = 0.06987843662500381
iteration 148, loss = 0.07867249846458435
iteration 149, loss = 0.11706016957759857
iteration 150, loss = 0.1152472048997879
iteration 151, loss = 0.10537350177764893
iteration 152, loss = 0.07211022078990936
iteration 153, loss = 0.08971555531024933
iteration 154, loss = 0.0912124514579773
iteration 155, loss = 0.15195275843143463
iteration 156, loss = 0.15149663388729095
iteration 157, loss = 0.08152297139167786
iteration 158, loss = 0.13999015092849731
iteration 159, loss = 0.11028176546096802
iteration 160, loss = 0.10896036773920059
iteration 161, loss = 0.10281452536582947
iteration 162, loss = 0.08308754116296768
iteration 163, loss = 0.13172611594200134
iteration 164, loss = 0.09826608002185822
iteration 165, loss = 0.09482815861701965
iteration 166, loss = 0.10590453445911407
iteration 167, loss = 0.06632968783378601
iteration 168, loss = 0.08312895894050598
iteration 169, loss = 0.10874732583761215
iteration 170, loss = 0.08881120383739471
iteration 171, loss = 0.07746511697769165
iteration 172, loss = 0.06348118931055069
iteration 173, loss = 0.07578922808170319
iteration 174, loss = 0.06452576071023941
iteration 175, loss = 0.07065987586975098
iteration 176, loss = 0.0894421935081482
iteration 177, loss = 0.1330418586730957
iteration 178, loss = 0.09522541612386703
iteration 179, loss = 0.12597507238388062
iteration 180, loss = 0.09803742915391922
iteration 181, loss = 0.0806552842259407
iteration 182, loss = 0.09054171293973923
iteration 183, loss = 0.05174758657813072
iteration 184, loss = 0.049541935324668884
iteration 185, loss = 0.07811184227466583
iteration 186, loss = 0.09382957220077515
iteration 187, loss = 0.07371963560581207
iteration 188, loss = 0.07529626041650772
iteration 189, loss = 0.1172073557972908
iteration 190, loss = 0.0636477991938591
iteration 191, loss = 0.09679147601127625
iteration 192, loss = 0.12068231403827667
iteration 193, loss = 0.08850111067295074
iteration 194, loss = 0.06948061287403107
iteration 195, loss = 0.07898607850074768
iteration 196, loss = 0.0976906567811966
iteration 197, loss = 0.08362042903900146
iteration 198, loss = 0.08196711540222168
iteration 199, loss = 0.09176138788461685
iteration 200, loss = 0.06460295617580414
iteration 201, loss = 0.14803089201450348
iteration 202, loss = 0.07037237286567688
iteration 203, loss = 0.06380663812160492
iteration 204, loss = 0.0683203712105751
iteration 205, loss = 0.10113386064767838
iteration 206, loss = 0.09953190386295319
iteration 207, loss = 0.05860905349254608
iteration 208, loss = 0.08901769667863846
iteration 209, loss = 0.09237867593765259
iteration 210, loss = 0.1448533535003662
iteration 211, loss = 0.11983692646026611
iteration 212, loss = 0.08437543362379074
iteration 213, loss = 0.04228447377681732
iteration 214, loss = 0.05763319134712219
iteration 215, loss = 0.11727190017700195
iteration 216, loss = 0.0894736498594284
iteration 217, loss = 0.07144278287887573
iteration 218, loss = 0.08368391543626785
iteration 219, loss = 0.060348693281412125
iteration 220, loss = 0.09286532551050186
iteration 221, loss = 0.11852692812681198
iteration 222, loss = 0.09287188202142715
iteration 223, loss = 0.09217900037765503
iteration 224, loss = 0.12191880494356155
iteration 225, loss = 0.11746123433113098
iteration 226, loss = 0.050846442580223083
iteration 227, loss = 0.07104866206645966
iteration 228, loss = 0.062338314950466156
iteration 229, loss = 0.09726182371377945
iteration 230, loss = 0.09662286192178726
iteration 231, loss = 0.0641600489616394
iteration 232, loss = 0.047076933085918427
iteration 233, loss = 0.05011340230703354
iteration 234, loss = 0.07525867968797684
iteration 235, loss = 0.08210593461990356
iteration 236, loss = 0.047073110938072205
iteration 237, loss = 0.1038556843996048
iteration 238, loss = 0.060975320637226105
iteration 239, loss = 0.10614754259586334
iteration 240, loss = 0.1092635989189148
iteration 241, loss = 0.0732305720448494
iteration 242, loss = 0.0623747892677784
iteration 243, loss = 0.05094091594219208
iteration 244, loss = 0.05700337514281273
iteration 245, loss = 0.10687293112277985
iteration 246, loss = 0.0901964083313942
iteration 247, loss = 0.09209959954023361
iteration 248, loss = 0.09492697566747665
iteration 249, loss = 0.11663322895765305
iteration 250, loss = 0.06443265080451965
iteration 251, loss = 0.07668732106685638
iteration 252, loss = 0.11423289775848389
iteration 253, loss = 0.04022889584302902
iteration 254, loss = 0.06645693629980087
iteration 255, loss = 0.11958643049001694
iteration 256, loss = 0.09074109047651291
iteration 257, loss = 0.08493249863386154
iteration 258, loss = 0.11222296953201294
iteration 259, loss = 0.07517844438552856
iteration 260, loss = 0.07222431153059006
iteration 261, loss = 0.09060600399971008
iteration 262, loss = 0.04581742361187935
iteration 263, loss = 0.052168913185596466
iteration 264, loss = 0.08092394471168518
iteration 265, loss = 0.06687542051076889
iteration 266, loss = 0.04767958074808121
iteration 267, loss = 0.05586269497871399
iteration 268, loss = 0.10134820640087128
iteration 269, loss = 0.06235114485025406
iteration 270, loss = 0.11492770165205002
iteration 271, loss = 0.05716860294342041
iteration 272, loss = 0.07043090462684631
iteration 273, loss = 0.059743575751781464
iteration 274, loss = 0.06280424445867538
iteration 275, loss = 0.172137051820755
iteration 276, loss = 0.0728716254234314
iteration 277, loss = 0.052126675844192505
iteration 278, loss = 0.07801549136638641
iteration 279, loss = 0.0654430240392685
iteration 280, loss = 0.08853185176849365
iteration 281, loss = 0.06558113545179367
iteration 282, loss = 0.08278005570173264
iteration 283, loss = 0.11045371741056442
iteration 284, loss = 0.0746203064918518
iteration 285, loss = 0.03654906153678894
iteration 286, loss = 0.08577392250299454
iteration 287, loss = 0.08802541345357895
iteration 288, loss = 0.07707910984754562
iteration 289, loss = 0.08288681507110596
iteration 290, loss = 0.09610863775014877
iteration 291, loss = 0.11422468721866608
iteration 292, loss = 0.05850054696202278
iteration 293, loss = 0.07077661901712418
iteration 294, loss = 0.10847136378288269
iteration 295, loss = 0.11264395713806152
iteration 296, loss = 0.08057376742362976
iteration 297, loss = 0.09881609678268433
iteration 298, loss = 0.09352109581232071
iteration 299, loss = 0.09769580513238907
iteration 300, loss = 0.07772018015384674
iteration 1, loss = 0.04059579223394394
iteration 2, loss = 0.05298297852277756
iteration 3, loss = 0.07149171084165573
iteration 4, loss = 0.09934740513563156
iteration 5, loss = 0.07640169560909271
iteration 6, loss = 0.0597706139087677
iteration 7, loss = 0.0864957943558693
iteration 8, loss = 0.10279042273759842
iteration 9, loss = 0.07249203324317932
iteration 10, loss = 0.09089171886444092
iteration 11, loss = 0.04341452196240425
iteration 12, loss = 0.08057518303394318
iteration 13, loss = 0.04614073783159256
iteration 14, loss = 0.07294423133134842
iteration 15, loss = 0.061736851930618286
iteration 16, loss = 0.13437022268772125
iteration 17, loss = 0.06244035065174103
iteration 18, loss = 0.06893739104270935
iteration 19, loss = 0.04766351357102394
iteration 20, loss = 0.09725096076726913
iteration 21, loss = 0.08188950270414352
iteration 22, loss = 0.07452115416526794
iteration 23, loss = 0.10967257618904114
iteration 24, loss = 0.06293030083179474
iteration 25, loss = 0.08202047646045685
iteration 26, loss = 0.08749127388000488
iteration 27, loss = 0.08938285708427429
iteration 28, loss = 0.09856699407100677
iteration 29, loss = 0.061613161116838455
iteration 30, loss = 0.061047714203596115
iteration 31, loss = 0.047522224485874176
iteration 32, loss = 0.06361112743616104
iteration 33, loss = 0.09537536650896072
iteration 34, loss = 0.08649923652410507
iteration 35, loss = 0.05174495652318001
iteration 36, loss = 0.10471273213624954
iteration 37, loss = 0.08465642482042313
iteration 38, loss = 0.10335013270378113
iteration 39, loss = 0.07795002311468124
iteration 40, loss = 0.06804563105106354
iteration 41, loss = 0.08349822461605072
iteration 42, loss = 0.09766416251659393
iteration 43, loss = 0.06134500354528427
iteration 44, loss = 0.0897754430770874
iteration 45, loss = 0.05626016482710838
iteration 46, loss = 0.03708890452980995
iteration 47, loss = 0.060976721346378326
iteration 48, loss = 0.05205532908439636
iteration 49, loss = 0.07207201421260834
iteration 50, loss = 0.0759633332490921
iteration 51, loss = 0.060838110744953156
iteration 52, loss = 0.07381464540958405
iteration 53, loss = 0.09191525727510452
iteration 54, loss = 0.09820778667926788
iteration 55, loss = 0.0645076259970665
iteration 56, loss = 0.06892643123865128
iteration 57, loss = 0.10083097219467163
iteration 58, loss = 0.05966847017407417
iteration 59, loss = 0.1015598326921463
iteration 60, loss = 0.03551408275961876
iteration 61, loss = 0.12749211490154266
iteration 62, loss = 0.1000104546546936
iteration 63, loss = 0.08857312798500061
iteration 64, loss = 0.1017889678478241
iteration 65, loss = 0.05118804797530174
iteration 66, loss = 0.040068887174129486
iteration 67, loss = 0.05489485710859299
iteration 68, loss = 0.06322780251502991
iteration 69, loss = 0.07011377811431885
iteration 70, loss = 0.07321449369192123
iteration 71, loss = 0.07575446367263794
iteration 72, loss = 0.055694617331027985
iteration 73, loss = 0.044723108410835266
iteration 74, loss = 0.06590355932712555
iteration 75, loss = 0.06838366389274597
iteration 76, loss = 0.05272038280963898
iteration 77, loss = 0.08585727959871292
iteration 78, loss = 0.07455361634492874
iteration 79, loss = 0.09568734467029572
iteration 80, loss = 0.048055876046419144
iteration 81, loss = 0.06687846034765244
iteration 82, loss = 0.0979909598827362
iteration 83, loss = 0.08704154193401337
iteration 84, loss = 0.07541994750499725
iteration 85, loss = 0.06608223915100098
iteration 86, loss = 0.0774364322423935
iteration 87, loss = 0.05466813966631889
iteration 88, loss = 0.055492497980594635
iteration 89, loss = 0.09822385758161545
iteration 90, loss = 0.10387791693210602
iteration 91, loss = 0.06078826263546944
iteration 92, loss = 0.07605671137571335
iteration 93, loss = 0.07872425019741058
iteration 94, loss = 0.05660592019557953
iteration 95, loss = 0.061095863580703735
iteration 96, loss = 0.08104440569877625
iteration 97, loss = 0.10151077806949615
iteration 98, loss = 0.0823570266366005
iteration 99, loss = 0.06927792727947235
iteration 100, loss = 0.10379932820796967
iteration 101, loss = 0.05561657249927521
iteration 102, loss = 0.09804186969995499
iteration 103, loss = 0.07570955902338028
iteration 104, loss = 0.09878096729516983
iteration 105, loss = 0.05184101313352585
iteration 106, loss = 0.1102260947227478
iteration 107, loss = 0.10065334290266037
iteration 108, loss = 0.10200674086809158
iteration 109, loss = 0.043692562729120255
iteration 110, loss = 0.07350321114063263
iteration 111, loss = 0.042092420160770416
iteration 112, loss = 0.06953099370002747
iteration 113, loss = 0.11838173121213913
iteration 114, loss = 0.04330601915717125
iteration 115, loss = 0.04168540984392166
iteration 116, loss = 0.07377618551254272
iteration 117, loss = 0.07918703556060791
iteration 118, loss = 0.08696652203798294
iteration 119, loss = 0.08053673058748245
iteration 120, loss = 0.03735630214214325
iteration 121, loss = 0.07901646941900253
iteration 122, loss = 0.10324233025312424
iteration 123, loss = 0.09349105507135391
iteration 124, loss = 0.042054515331983566
iteration 125, loss = 0.05958888679742813
iteration 126, loss = 0.05316619202494621
iteration 127, loss = 0.06766829639673233
iteration 128, loss = 0.0721263736486435
iteration 129, loss = 0.05902023985981941
iteration 130, loss = 0.048921748995780945
iteration 131, loss = 0.05555400997400284
iteration 132, loss = 0.0964503064751625
iteration 133, loss = 0.054675646126270294
iteration 134, loss = 0.08354032039642334
iteration 135, loss = 0.07051419466733932
iteration 136, loss = 0.08627690374851227
iteration 137, loss = 0.05733748525381088
iteration 138, loss = 0.054128654301166534
iteration 139, loss = 0.05646711587905884
iteration 140, loss = 0.07333941012620926
iteration 141, loss = 0.0568636991083622
iteration 142, loss = 0.11393015086650848
iteration 143, loss = 0.05473516881465912
iteration 144, loss = 0.05879387632012367
iteration 145, loss = 0.05019234120845795
iteration 146, loss = 0.10461278259754181
iteration 147, loss = 0.0557936355471611
iteration 148, loss = 0.047833189368247986
iteration 149, loss = 0.09469062089920044
iteration 150, loss = 0.04328228160738945
iteration 151, loss = 0.09241662919521332
iteration 152, loss = 0.061893001198768616
iteration 153, loss = 0.048567596822977066
iteration 154, loss = 0.06502602994441986
iteration 155, loss = 0.07546304166316986
iteration 156, loss = 0.06286321580410004
iteration 157, loss = 0.04348558932542801
iteration 158, loss = 0.06031298637390137
iteration 159, loss = 0.06052380055189133
iteration 160, loss = 0.06724578887224197
iteration 161, loss = 0.08243755251169205
iteration 162, loss = 0.04336166009306908
iteration 163, loss = 0.09649980068206787
iteration 164, loss = 0.07379749417304993
iteration 165, loss = 0.10199648141860962
iteration 166, loss = 0.06372712552547455
iteration 167, loss = 0.05872034281492233
iteration 168, loss = 0.0556190200150013
iteration 169, loss = 0.04717424139380455
iteration 170, loss = 0.06625404208898544
iteration 171, loss = 0.13170352578163147
iteration 172, loss = 0.08080572634935379
iteration 173, loss = 0.06401115655899048
iteration 174, loss = 0.08107560127973557
iteration 175, loss = 0.060659851878881454
iteration 176, loss = 0.09395348280668259
iteration 177, loss = 0.11563192307949066
iteration 178, loss = 0.07466264069080353
iteration 179, loss = 0.07660513371229172
iteration 180, loss = 0.08253651857376099
iteration 181, loss = 0.10065574944019318
iteration 182, loss = 0.08154872059822083
iteration 183, loss = 0.049035582691431046
iteration 184, loss = 0.07869761437177658
iteration 185, loss = 0.035254716873168945
iteration 186, loss = 0.053088221698999405
iteration 187, loss = 0.10325140506029129
iteration 188, loss = 0.04962440952658653
iteration 189, loss = 0.06744790822267532
iteration 190, loss = 0.03766843304038048
iteration 191, loss = 0.0669471025466919
iteration 192, loss = 0.05573839694261551
iteration 193, loss = 0.07996857166290283
iteration 194, loss = 0.10689859092235565
iteration 195, loss = 0.047836922109127045
iteration 196, loss = 0.049907442182302475
iteration 197, loss = 0.05710038170218468
iteration 198, loss = 0.07282611727714539
iteration 199, loss = 0.11884992569684982
iteration 200, loss = 0.08745316416025162
iteration 201, loss = 0.029408849775791168
iteration 202, loss = 0.11181455850601196
iteration 203, loss = 0.05248907953500748
iteration 204, loss = 0.07206220924854279
iteration 205, loss = 0.06819025427103043
iteration 206, loss = 0.06459097564220428
iteration 207, loss = 0.08242592215538025
iteration 208, loss = 0.06307455897331238
iteration 209, loss = 0.05996808409690857
iteration 210, loss = 0.07198981940746307
iteration 211, loss = 0.06790069490671158
iteration 212, loss = 0.031230786815285683
iteration 213, loss = 0.04802709445357323
iteration 214, loss = 0.0924120619893074
iteration 215, loss = 0.0532926544547081
iteration 216, loss = 0.06623899191617966
iteration 217, loss = 0.07239394634962082
iteration 218, loss = 0.06760622560977936
iteration 219, loss = 0.09836295247077942
iteration 220, loss = 0.04932951182126999
iteration 221, loss = 0.06803088635206223
iteration 222, loss = 0.047112081199884415
iteration 223, loss = 0.03139287978410721
iteration 224, loss = 0.03676122799515724
iteration 225, loss = 0.046498317271471024
iteration 226, loss = 0.0519593320786953
iteration 227, loss = 0.05263754725456238
iteration 228, loss = 0.06414346396923065
iteration 229, loss = 0.0758759081363678
iteration 230, loss = 0.05914336070418358
iteration 231, loss = 0.055525876581668854
iteration 232, loss = 0.05207096040248871
iteration 233, loss = 0.044943712651729584
iteration 234, loss = 0.06521391868591309
iteration 235, loss = 0.025225449353456497
iteration 236, loss = 0.08714594691991806
iteration 237, loss = 0.0463503934442997
iteration 238, loss = 0.04354001209139824
iteration 239, loss = 0.06311488151550293
iteration 240, loss = 0.063029944896698
iteration 241, loss = 0.03700074553489685
iteration 242, loss = 0.10303793847560883
iteration 243, loss = 0.03273486718535423
iteration 244, loss = 0.08382745087146759
iteration 245, loss = 0.06602752953767776
iteration 246, loss = 0.08096058666706085
iteration 247, loss = 0.05525853857398033
iteration 248, loss = 0.07677209377288818
iteration 249, loss = 0.08321209251880646
iteration 250, loss = 0.0666448250412941
iteration 251, loss = 0.09542619436979294
iteration 252, loss = 0.05967046320438385
iteration 253, loss = 0.0451948381960392
iteration 254, loss = 0.05894797295331955
iteration 255, loss = 0.06161938235163689
iteration 256, loss = 0.0773274302482605
iteration 257, loss = 0.05254171043634415
iteration 258, loss = 0.06361495703458786
iteration 259, loss = 0.05423560366034508
iteration 260, loss = 0.07757939398288727
iteration 261, loss = 0.06107301265001297
iteration 262, loss = 0.03714924678206444
iteration 263, loss = 0.07790658622980118
iteration 264, loss = 0.03951549530029297
iteration 265, loss = 0.08380597829818726
iteration 266, loss = 0.08011346310377121
iteration 267, loss = 0.08376538008451462
iteration 268, loss = 0.059428922832012177
iteration 269, loss = 0.0434064120054245
iteration 270, loss = 0.08887802809476852
iteration 271, loss = 0.03389258310198784
iteration 272, loss = 0.10389385372400284
iteration 273, loss = 0.04615767300128937
iteration 274, loss = 0.04907423257827759
iteration 275, loss = 0.045903753489255905
iteration 276, loss = 0.07464437931776047
iteration 277, loss = 0.15511351823806763
iteration 278, loss = 0.05827484279870987
iteration 279, loss = 0.09681615978479385
iteration 280, loss = 0.07328503578901291
iteration 281, loss = 0.039049938321113586
iteration 282, loss = 0.060506295412778854
iteration 283, loss = 0.0728335827589035
iteration 284, loss = 0.05249219387769699
iteration 285, loss = 0.0781913697719574
iteration 286, loss = 0.0814078077673912
iteration 287, loss = 0.05559583753347397
iteration 288, loss = 0.06744275242090225
iteration 289, loss = 0.0378277562558651
iteration 290, loss = 0.056291911751031876
iteration 291, loss = 0.0606277696788311
iteration 292, loss = 0.07760713994503021
iteration 293, loss = 0.04732193425297737
iteration 294, loss = 0.03058212250471115
iteration 295, loss = 0.05679985508322716
iteration 296, loss = 0.05673171952366829
iteration 297, loss = 0.07283861935138702
iteration 298, loss = 0.09889107942581177
iteration 299, loss = 0.0470159649848938
iteration 300, loss = 0.10050566494464874
iteration 1, loss = 0.07135415077209473
iteration 2, loss = 0.07596956938505173
iteration 3, loss = 0.07407980412244797
iteration 4, loss = 0.05498563498258591
iteration 5, loss = 0.045130953192710876
iteration 6, loss = 0.06153115630149841
iteration 7, loss = 0.022217705845832825
iteration 8, loss = 0.0647522434592247
iteration 9, loss = 0.08132173120975494
iteration 10, loss = 0.059671550989151
iteration 11, loss = 0.0863298624753952
iteration 12, loss = 0.0709177777171135
iteration 13, loss = 0.10493908077478409
iteration 14, loss = 0.07924332469701767
iteration 15, loss = 0.03972633555531502
iteration 16, loss = 0.05212097615003586
iteration 17, loss = 0.05585901439189911
iteration 18, loss = 0.048686347901821136
iteration 19, loss = 0.06367740035057068
iteration 20, loss = 0.05986372008919716
iteration 21, loss = 0.05134088546037674
iteration 22, loss = 0.04960501939058304
iteration 23, loss = 0.021556908264756203
iteration 24, loss = 0.03949376195669174
iteration 25, loss = 0.06224794685840607
iteration 26, loss = 0.07538115978240967
iteration 27, loss = 0.047471098601818085
iteration 28, loss = 0.0625758171081543
iteration 29, loss = 0.09550031274557114
iteration 30, loss = 0.0723462849855423
iteration 31, loss = 0.06476371735334396
iteration 32, loss = 0.08095722645521164
iteration 33, loss = 0.05095033347606659
iteration 34, loss = 0.07414852827787399
iteration 35, loss = 0.02403184212744236
iteration 36, loss = 0.050476204603910446
iteration 37, loss = 0.0400233268737793
iteration 38, loss = 0.05533226951956749
iteration 39, loss = 0.042877860367298126
iteration 40, loss = 0.06756878644227982
iteration 41, loss = 0.04968206584453583
iteration 42, loss = 0.07246515154838562
iteration 43, loss = 0.028910398483276367
iteration 44, loss = 0.06637858599424362
iteration 45, loss = 0.06735917925834656
iteration 46, loss = 0.0662294551730156
iteration 47, loss = 0.1082899421453476
iteration 48, loss = 0.09937624633312225
iteration 49, loss = 0.03982717543840408
iteration 50, loss = 0.06256231665611267
iteration 51, loss = 0.061283472925424576
iteration 52, loss = 0.060592781752347946
iteration 53, loss = 0.04362538829445839
iteration 54, loss = 0.03725156560540199
iteration 55, loss = 0.07866544276475906
iteration 56, loss = 0.04991134628653526
iteration 57, loss = 0.0547872819006443
iteration 58, loss = 0.03593682497739792
iteration 59, loss = 0.06062755361199379
iteration 60, loss = 0.06791291385889053
iteration 61, loss = 0.07253162562847137
iteration 62, loss = 0.05244506895542145
iteration 63, loss = 0.0749032273888588
iteration 64, loss = 0.02420089766383171
iteration 65, loss = 0.06558460742235184
iteration 66, loss = 0.070355124771595
iteration 67, loss = 0.050624553114175797
iteration 68, loss = 0.06551618129014969
iteration 69, loss = 0.0986366719007492
iteration 70, loss = 0.06344105303287506
iteration 71, loss = 0.06499925255775452
iteration 72, loss = 0.06627590954303741
iteration 73, loss = 0.04096761718392372
iteration 74, loss = 0.056796375662088394
iteration 75, loss = 0.07367406785488129
iteration 76, loss = 0.04187789186835289
iteration 77, loss = 0.06592430174350739
iteration 78, loss = 0.045965004712343216
iteration 79, loss = 0.08381228893995285
iteration 80, loss = 0.07998308539390564
iteration 81, loss = 0.05633052811026573
iteration 82, loss = 0.06679558753967285
iteration 83, loss = 0.03821689635515213
iteration 84, loss = 0.09483348578214645
iteration 85, loss = 0.03540431335568428
iteration 86, loss = 0.06312959641218185
iteration 87, loss = 0.08846627175807953
iteration 88, loss = 0.04862170293927193
iteration 89, loss = 0.028246117755770683
iteration 90, loss = 0.03503284603357315
iteration 91, loss = 0.055770598351955414
iteration 92, loss = 0.05098181962966919
iteration 93, loss = 0.06112208962440491
iteration 94, loss = 0.031858835369348526
iteration 95, loss = 0.037841279059648514
iteration 96, loss = 0.05581596493721008
iteration 97, loss = 0.04113473370671272
iteration 98, loss = 0.04187675192952156
iteration 99, loss = 0.08145144581794739
iteration 100, loss = 0.06380689889192581
iteration 101, loss = 0.08890166133642197
iteration 102, loss = 0.04449675604701042
iteration 103, loss = 0.03249645233154297
iteration 104, loss = 0.03903374448418617
iteration 105, loss = 0.06900813430547714
iteration 106, loss = 0.0649915561079979
iteration 107, loss = 0.08834035694599152
iteration 108, loss = 0.06703618913888931
iteration 109, loss = 0.0774691253900528
iteration 110, loss = 0.01692945510149002
iteration 111, loss = 0.04645371809601784
iteration 112, loss = 0.0515349917113781
iteration 113, loss = 0.023250820115208626
iteration 114, loss = 0.03742760792374611
iteration 115, loss = 0.03110591322183609
iteration 116, loss = 0.05849127843976021
iteration 117, loss = 0.038139503449201584
iteration 118, loss = 0.06812720745801926
iteration 119, loss = 0.06375429779291153
iteration 120, loss = 0.07810775190591812
iteration 121, loss = 0.035490814596414566
iteration 122, loss = 0.055180132389068604
iteration 123, loss = 0.06872037798166275
iteration 124, loss = 0.07311378419399261
iteration 125, loss = 0.04412839561700821
iteration 126, loss = 0.04509594663977623
iteration 127, loss = 0.07273288071155548
iteration 128, loss = 0.04600144922733307
iteration 129, loss = 0.03403685241937637
iteration 130, loss = 0.06106867268681526
iteration 131, loss = 0.0739675909280777
iteration 132, loss = 0.03778786212205887
iteration 133, loss = 0.039570294320583344
iteration 134, loss = 0.11799539625644684
iteration 135, loss = 0.07092245668172836
iteration 136, loss = 0.08122450858354568
iteration 137, loss = 0.03163357451558113
iteration 138, loss = 0.022801700979471207
iteration 139, loss = 0.03933590278029442
iteration 140, loss = 0.0788625180721283
iteration 141, loss = 0.06713226437568665
iteration 142, loss = 0.04595591872930527
iteration 143, loss = 0.040902651846408844
iteration 144, loss = 0.017267299816012383
iteration 145, loss = 0.03979457914829254
iteration 146, loss = 0.04433063045144081
iteration 147, loss = 0.05981718748807907
iteration 148, loss = 0.08597885817289352
iteration 149, loss = 0.0326957143843174
iteration 150, loss = 0.03377177566289902
iteration 151, loss = 0.05915224924683571
iteration 152, loss = 0.0693364366889
iteration 153, loss = 0.03550673648715019
iteration 154, loss = 0.07428418844938278
iteration 155, loss = 0.05939207971096039
iteration 156, loss = 0.06424861401319504
iteration 157, loss = 0.021154893562197685
iteration 158, loss = 0.040634311735630035
iteration 159, loss = 0.025752373039722443
iteration 160, loss = 0.037446968257427216
iteration 161, loss = 0.0347091406583786
iteration 162, loss = 0.047529205679893494
iteration 163, loss = 0.06327208131551743
iteration 164, loss = 0.06915873289108276
iteration 165, loss = 0.06413841992616653
iteration 166, loss = 0.02647361531853676
iteration 167, loss = 0.04006723687052727
iteration 168, loss = 0.05366123467683792
iteration 169, loss = 0.06791374832391739
iteration 170, loss = 0.05330996587872505
iteration 171, loss = 0.05330264940857887
iteration 172, loss = 0.022179914638400078
iteration 173, loss = 0.0487089566886425
iteration 174, loss = 0.05498915910720825
iteration 175, loss = 0.05016138032078743
iteration 176, loss = 0.05385153740644455
iteration 177, loss = 0.07881667464971542
iteration 178, loss = 0.07565124332904816
iteration 179, loss = 0.05422259122133255
iteration 180, loss = 0.04487431049346924
iteration 181, loss = 0.05098486691713333
iteration 182, loss = 0.06902439892292023
iteration 183, loss = 0.06850409507751465
iteration 184, loss = 0.03704841434955597
iteration 185, loss = 0.061056993901729584
iteration 186, loss = 0.04611489176750183
iteration 187, loss = 0.05291829630732536
iteration 188, loss = 0.041551623493433
iteration 189, loss = 0.037521883845329285
iteration 190, loss = 0.04815692827105522
iteration 191, loss = 0.040481872856616974
iteration 192, loss = 0.07697657495737076
iteration 193, loss = 0.05575008690357208
iteration 194, loss = 0.03377200663089752
iteration 195, loss = 0.02397306263446808
iteration 196, loss = 0.03400330990552902
iteration 197, loss = 0.07332057505846024
iteration 198, loss = 0.042736537754535675
iteration 199, loss = 0.058555781841278076
iteration 200, loss = 0.09016256034374237
iteration 201, loss = 0.061210837215185165
iteration 202, loss = 0.04439182206988335
iteration 203, loss = 0.03209051489830017
iteration 204, loss = 0.04546085745096207
iteration 205, loss = 0.057746436446905136
iteration 206, loss = 0.027604274451732635
iteration 207, loss = 0.04850247874855995
iteration 208, loss = 0.0949273407459259
iteration 209, loss = 0.02753988653421402
iteration 210, loss = 0.07956238090991974
iteration 211, loss = 0.055451925843954086
iteration 212, loss = 0.06824517995119095
iteration 213, loss = 0.04570291191339493
iteration 214, loss = 0.05607740208506584
iteration 215, loss = 0.05829573795199394
iteration 216, loss = 0.06011461094021797
iteration 217, loss = 0.021914441138505936
iteration 218, loss = 0.0907093733549118
iteration 219, loss = 0.06886972486972809
iteration 220, loss = 0.051151249557733536
iteration 221, loss = 0.06033456698060036
iteration 222, loss = 0.0800933688879013
iteration 223, loss = 0.03286157548427582
iteration 224, loss = 0.06005249544978142
iteration 225, loss = 0.05870126932859421
iteration 226, loss = 0.07755270600318909
iteration 227, loss = 0.05844377353787422
iteration 228, loss = 0.06371170282363892
iteration 229, loss = 0.05221666023135185
iteration 230, loss = 0.022011538967490196
iteration 231, loss = 0.05719122663140297
iteration 232, loss = 0.05958745256066322
iteration 233, loss = 0.022416044026613235
iteration 234, loss = 0.03064190410077572
iteration 235, loss = 0.055025048553943634
iteration 236, loss = 0.044272731989622116
iteration 237, loss = 0.02323286049067974
iteration 238, loss = 0.05875907838344574
iteration 239, loss = 0.06103068217635155
iteration 240, loss = 0.05964513123035431
iteration 241, loss = 0.03518640622496605
iteration 242, loss = 0.037927478551864624
iteration 243, loss = 0.04421388730406761
iteration 244, loss = 0.0441243015229702
iteration 245, loss = 0.0694146677851677
iteration 246, loss = 0.04075661674141884
iteration 247, loss = 0.017537666484713554
iteration 248, loss = 0.04640801250934601
iteration 249, loss = 0.08183407783508301
iteration 250, loss = 0.04715718328952789
iteration 251, loss = 0.029836494475603104
iteration 252, loss = 0.058724530041217804
iteration 253, loss = 0.055825259536504745
iteration 254, loss = 0.04845723509788513
iteration 255, loss = 0.022539976984262466
iteration 256, loss = 0.031303368508815765
iteration 257, loss = 0.055534034967422485
iteration 258, loss = 0.03629982843995094
iteration 259, loss = 0.031138617545366287
iteration 260, loss = 0.07260708510875702
iteration 261, loss = 0.033241525292396545
iteration 262, loss = 0.029861455783247948
iteration 263, loss = 0.06221681088209152
iteration 264, loss = 0.04110787808895111
iteration 265, loss = 0.05074164271354675
iteration 266, loss = 0.06767302751541138
iteration 267, loss = 0.025109756737947464
iteration 268, loss = 0.055474068969488144
iteration 269, loss = 0.05243174731731415
iteration 270, loss = 0.04421838000416756
iteration 271, loss = 0.03807484731078148
iteration 272, loss = 0.04263503476977348
iteration 273, loss = 0.045622568577528
iteration 274, loss = 0.03292771801352501
iteration 275, loss = 0.05027725547552109
iteration 276, loss = 0.04672592133283615
iteration 277, loss = 0.04223691672086716
iteration 278, loss = 0.08621035516262054
iteration 279, loss = 0.058861903846263885
iteration 280, loss = 0.05047207325696945
iteration 281, loss = 0.06303376704454422
iteration 282, loss = 0.016013097018003464
iteration 283, loss = 0.03648001700639725
iteration 284, loss = 0.057070255279541016
iteration 285, loss = 0.0309356190264225
iteration 286, loss = 0.032339029014110565
iteration 287, loss = 0.04243091493844986
iteration 288, loss = 0.03719703480601311
iteration 289, loss = 0.044779762625694275
iteration 290, loss = 0.08195406198501587
iteration 291, loss = 0.025050247088074684
iteration 292, loss = 0.08717933297157288
iteration 293, loss = 0.05381907522678375
iteration 294, loss = 0.042853355407714844
iteration 295, loss = 0.05842424929141998
iteration 296, loss = 0.049528200179338455
iteration 297, loss = 0.03513843193650246
iteration 298, loss = 0.029448464512825012
iteration 299, loss = 0.04419710487127304
iteration 300, loss = 0.07314212620258331
iteration 1, loss = 0.04703810065984726
iteration 2, loss = 0.04373793676495552
iteration 3, loss = 0.0651191920042038
iteration 4, loss = 0.03035596013069153
iteration 5, loss = 0.05494508519768715
iteration 6, loss = 0.08577796071767807
iteration 7, loss = 0.049757443368434906
iteration 8, loss = 0.052391961216926575
iteration 9, loss = 0.05595777556300163
iteration 10, loss = 0.04621108993887901
iteration 11, loss = 0.062449052929878235
iteration 12, loss = 0.044818148016929626
iteration 13, loss = 0.03947984427213669
iteration 14, loss = 0.052498120814561844
iteration 15, loss = 0.062398262321949005
iteration 16, loss = 0.055201057344675064
iteration 17, loss = 0.051657333970069885
iteration 18, loss = 0.04387551173567772
iteration 19, loss = 0.03567524999380112
iteration 20, loss = 0.03977375477552414
iteration 21, loss = 0.0440199188888073
iteration 22, loss = 0.044691313058137894
iteration 23, loss = 0.05501168966293335
iteration 24, loss = 0.06082850694656372
iteration 25, loss = 0.06014856696128845
iteration 26, loss = 0.0390753448009491
iteration 27, loss = 0.039260782301425934
iteration 28, loss = 0.04731679707765579
iteration 29, loss = 0.050837546586990356
iteration 30, loss = 0.05445607006549835
iteration 31, loss = 0.037160176783800125
iteration 32, loss = 0.036075953394174576
iteration 33, loss = 0.025675131008028984
iteration 34, loss = 0.08262550830841064
iteration 35, loss = 0.065622478723526
iteration 36, loss = 0.050804972648620605
iteration 37, loss = 0.04749501869082451
iteration 38, loss = 0.0148647241294384
iteration 39, loss = 0.02391625940799713
iteration 40, loss = 0.04540581256151199
iteration 41, loss = 0.019695378839969635
iteration 42, loss = 0.057503875344991684
iteration 43, loss = 0.049185190349817276
iteration 44, loss = 0.07785934209823608
iteration 45, loss = 0.032581739127635956
iteration 46, loss = 0.05656169354915619
iteration 47, loss = 0.020531821995973587
iteration 48, loss = 0.04527461156249046
iteration 49, loss = 0.04790852963924408
iteration 50, loss = 0.057056620717048645
iteration 51, loss = 0.07863107323646545
iteration 52, loss = 0.0308334119617939
iteration 53, loss = 0.04284052178263664
iteration 54, loss = 0.050685133785009384
iteration 55, loss = 0.036771178245544434
iteration 56, loss = 0.036504048854112625
iteration 57, loss = 0.0481283999979496
iteration 58, loss = 0.06201109290122986
iteration 59, loss = 0.01350541040301323
iteration 60, loss = 0.049637842923402786
iteration 61, loss = 0.0660533681511879
iteration 62, loss = 0.029654642567038536
iteration 63, loss = 0.04712711647152901
iteration 64, loss = 0.07239893078804016
iteration 65, loss = 0.02885451540350914
iteration 66, loss = 0.01606963947415352
iteration 67, loss = 0.05811949819326401
iteration 68, loss = 0.029238933697342873
iteration 69, loss = 0.02310360223054886
iteration 70, loss = 0.0484127514064312
iteration 71, loss = 0.051376134157180786
iteration 72, loss = 0.037452876567840576
iteration 73, loss = 0.04132247343659401
iteration 74, loss = 0.06254210323095322
iteration 75, loss = 0.02992093190550804
iteration 76, loss = 0.06678472459316254
iteration 77, loss = 0.01754697598516941
iteration 78, loss = 0.02332986332476139
iteration 79, loss = 0.025903820991516113
iteration 80, loss = 0.05679820105433464
iteration 81, loss = 0.02816801331937313
iteration 82, loss = 0.06484031677246094
iteration 83, loss = 0.053694434463977814
iteration 84, loss = 0.024323314428329468
iteration 85, loss = 0.05353192612528801
iteration 86, loss = 0.04069703072309494
iteration 87, loss = 0.02238198183476925
iteration 88, loss = 0.08240889012813568
iteration 89, loss = 0.07126142084598541
iteration 90, loss = 0.043276287615299225
iteration 91, loss = 0.049706049263477325
iteration 92, loss = 0.0200925525277853
iteration 93, loss = 0.025348583236336708
iteration 94, loss = 0.05450879782438278
iteration 95, loss = 0.05405651777982712
iteration 96, loss = 0.06407149881124496
iteration 97, loss = 0.028391364961862564
iteration 98, loss = 0.05578746274113655
iteration 99, loss = 0.02691756933927536
iteration 100, loss = 0.043997831642627716
iteration 101, loss = 0.048967596143484116
iteration 102, loss = 0.013293089345097542
iteration 103, loss = 0.035200707614421844
iteration 104, loss = 0.04478522762656212
iteration 105, loss = 0.046798355877399445
iteration 106, loss = 0.029682215303182602
iteration 107, loss = 0.04119053855538368
iteration 108, loss = 0.02458706684410572
iteration 109, loss = 0.05635446310043335
iteration 110, loss = 0.0738927498459816
iteration 111, loss = 0.04301939532160759
iteration 112, loss = 0.060847047716379166
iteration 113, loss = 0.02739444002509117
iteration 114, loss = 0.03620715066790581
iteration 115, loss = 0.06792657822370529
iteration 116, loss = 0.034056514501571655
iteration 117, loss = 0.04101095721125603
iteration 118, loss = 0.05321257561445236
iteration 119, loss = 0.06157558038830757
iteration 120, loss = 0.012373154982924461
iteration 121, loss = 0.0548149012029171
iteration 122, loss = 0.04835335537791252
iteration 123, loss = 0.07171697914600372
iteration 124, loss = 0.023916933685541153
iteration 125, loss = 0.05740412697196007
iteration 126, loss = 0.04169449210166931
iteration 127, loss = 0.0592542439699173
iteration 128, loss = 0.025237897410988808
iteration 129, loss = 0.030860166996717453
iteration 130, loss = 0.12644900381565094
iteration 131, loss = 0.05043666064739227
iteration 132, loss = 0.04588142782449722
iteration 133, loss = 0.028129613026976585
iteration 134, loss = 0.031140414997935295
iteration 135, loss = 0.03277644142508507
iteration 136, loss = 0.0645512193441391
iteration 137, loss = 0.039359159767627716
iteration 138, loss = 0.048739880323410034
iteration 139, loss = 0.08446922898292542
iteration 140, loss = 0.045198626816272736
iteration 141, loss = 0.03885025903582573
iteration 142, loss = 0.02859782986342907
iteration 143, loss = 0.028232283890247345
iteration 144, loss = 0.02499997243285179
iteration 145, loss = 0.03159402310848236
iteration 146, loss = 0.048206083476543427
iteration 147, loss = 0.05084235966205597
iteration 148, loss = 0.04339217022061348
iteration 149, loss = 0.03773346543312073
iteration 150, loss = 0.04792773723602295
iteration 151, loss = 0.047031890600919724
iteration 152, loss = 0.03744887188076973
iteration 153, loss = 0.02094167470932007
iteration 154, loss = 0.03588711470365524
iteration 155, loss = 0.02123096026480198
iteration 156, loss = 0.03693036362528801
iteration 157, loss = 0.03784388676285744
iteration 158, loss = 0.039782263338565826
iteration 159, loss = 0.07034516334533691
iteration 160, loss = 0.032462552189826965
iteration 161, loss = 0.054987646639347076
iteration 162, loss = 0.04689175635576248
iteration 163, loss = 0.023723596706986427
iteration 164, loss = 0.023499801754951477
iteration 165, loss = 0.0746612697839737
iteration 166, loss = 0.038144081830978394
iteration 167, loss = 0.016953062266111374
iteration 168, loss = 0.03221261501312256
iteration 169, loss = 0.044332556426525116
iteration 170, loss = 0.03737344965338707
iteration 171, loss = 0.06695470958948135
iteration 172, loss = 0.027549300342798233
iteration 173, loss = 0.042236506938934326
iteration 174, loss = 0.05548335984349251
iteration 175, loss = 0.049703747034072876
iteration 176, loss = 0.06492026150226593
iteration 177, loss = 0.055882129818201065
iteration 178, loss = 0.052662190049886703
iteration 179, loss = 0.042267292737960815
iteration 180, loss = 0.042912695556879044
iteration 181, loss = 0.02835397981107235
iteration 182, loss = 0.027722030878067017
iteration 183, loss = 0.03093327023088932
iteration 184, loss = 0.03882601484656334
iteration 185, loss = 0.029533889144659042
iteration 186, loss = 0.014839867129921913
iteration 187, loss = 0.03692419081926346
iteration 188, loss = 0.04971841350197792
iteration 189, loss = 0.07053624838590622
iteration 190, loss = 0.03149382397532463
iteration 191, loss = 0.028328407555818558
iteration 192, loss = 0.043599117547273636
iteration 193, loss = 0.027179136872291565
iteration 194, loss = 0.046333976089954376
iteration 195, loss = 0.019997520372271538
iteration 196, loss = 0.13556572794914246
iteration 197, loss = 0.03609950467944145
iteration 198, loss = 0.026623133569955826
iteration 199, loss = 0.033702097833156586
iteration 200, loss = 0.014476335607469082
iteration 201, loss = 0.03595734015107155
iteration 202, loss = 0.07639814913272858
iteration 203, loss = 0.020672382786870003
iteration 204, loss = 0.06786102056503296
iteration 205, loss = 0.031201237812638283
iteration 206, loss = 0.03427247330546379
iteration 207, loss = 0.04385427013039589
iteration 208, loss = 0.048824094235897064
iteration 209, loss = 0.03620915114879608
iteration 210, loss = 0.056381676346063614
iteration 211, loss = 0.03657839447259903
iteration 212, loss = 0.0433955043554306
iteration 213, loss = 0.020797131583094597
iteration 214, loss = 0.03430170565843582
iteration 215, loss = 0.028737381100654602
iteration 216, loss = 0.03944819048047066
iteration 217, loss = 0.05570751428604126
iteration 218, loss = 0.056284282356500626
iteration 219, loss = 0.03428393229842186
iteration 220, loss = 0.05852094665169716
iteration 221, loss = 0.04314520210027695
iteration 222, loss = 0.04354177042841911
iteration 223, loss = 0.054701484739780426
iteration 224, loss = 0.02901540696620941
iteration 225, loss = 0.029691101983189583
iteration 226, loss = 0.031683359295129776
iteration 227, loss = 0.028149127960205078
iteration 228, loss = 0.03189463913440704
iteration 229, loss = 0.04242877662181854
iteration 230, loss = 0.027546130120754242
iteration 231, loss = 0.08232057839632034
iteration 232, loss = 0.040987227112054825
iteration 233, loss = 0.04679504781961441
iteration 234, loss = 0.050572752952575684
iteration 235, loss = 0.0212039053440094
iteration 236, loss = 0.01722395233809948
iteration 237, loss = 0.030935432761907578
iteration 238, loss = 0.05317588150501251
iteration 239, loss = 0.03686665743589401
iteration 240, loss = 0.02586212381720543
iteration 241, loss = 0.02603098191320896
iteration 242, loss = 0.04054604470729828
iteration 243, loss = 0.056480370461940765
iteration 244, loss = 0.02914859913289547
iteration 245, loss = 0.045685380697250366
iteration 246, loss = 0.05126611515879631
iteration 247, loss = 0.02550850249826908
iteration 248, loss = 0.07923291623592377
iteration 249, loss = 0.02907518856227398
iteration 250, loss = 0.032594628632068634
iteration 251, loss = 0.009590478613972664
iteration 252, loss = 0.056997139006853104
iteration 253, loss = 0.046242065727710724
iteration 254, loss = 0.03329652175307274
iteration 255, loss = 0.05556867644190788
iteration 256, loss = 0.02584303729236126
iteration 257, loss = 0.05396744981408119
iteration 258, loss = 0.01594575121998787
iteration 259, loss = 0.05865965038537979
iteration 260, loss = 0.05282590538263321
iteration 261, loss = 0.015961550176143646
iteration 262, loss = 0.028368402272462845
iteration 263, loss = 0.04682214930653572
iteration 264, loss = 0.04234715551137924
iteration 265, loss = 0.025177214294672012
iteration 266, loss = 0.02276954986155033
iteration 267, loss = 0.027799420058727264
iteration 268, loss = 0.03714841604232788
iteration 269, loss = 0.026913505047559738
iteration 270, loss = 0.017677174881100655
iteration 271, loss = 0.03464231640100479
iteration 272, loss = 0.03129119798541069
iteration 273, loss = 0.011487623676657677
iteration 274, loss = 0.04331406205892563
iteration 275, loss = 0.011269614100456238
iteration 276, loss = 0.03595995903015137
iteration 277, loss = 0.034546807408332825
iteration 278, loss = 0.038669899106025696
iteration 279, loss = 0.019953390583395958
iteration 280, loss = 0.04974471405148506
iteration 281, loss = 0.054097920656204224
iteration 282, loss = 0.07010245323181152
iteration 283, loss = 0.061921607702970505
iteration 284, loss = 0.011172670871019363
iteration 285, loss = 0.047477174550294876
iteration 286, loss = 0.061812736093997955
iteration 287, loss = 0.04065448045730591
iteration 288, loss = 0.035859011113643646
iteration 289, loss = 0.04558119177818298
iteration 290, loss = 0.02193954586982727
iteration 291, loss = 0.021006811410188675
iteration 292, loss = 0.0376303568482399
iteration 293, loss = 0.05341741815209389
iteration 294, loss = 0.02897394634783268
iteration 295, loss = 0.04128365218639374
iteration 296, loss = 0.01928163506090641
iteration 297, loss = 0.04441278427839279
iteration 298, loss = 0.01701049879193306
iteration 299, loss = 0.029690872877836227
iteration 300, loss = 0.04984402656555176
=======
iteration 1, loss = 2.83931827545166
iteration 2, loss = 2.833791971206665
iteration 3, loss = 2.812041759490967
iteration 4, loss = 2.7654078006744385
iteration 5, loss = 2.7311923503875732
iteration 6, loss = 2.632136821746826
iteration 7, loss = 2.5710031986236572
iteration 8, loss = 2.550978899002075
iteration 9, loss = 2.445693016052246
iteration 10, loss = 2.4653429985046387
iteration 11, loss = 2.2043356895446777
iteration 12, loss = 2.1755709648132324
iteration 13, loss = 2.061410665512085
iteration 14, loss = 2.099560022354126
iteration 15, loss = 1.9920368194580078
iteration 16, loss = 1.8488974571228027
iteration 17, loss = 1.8165013790130615
iteration 18, loss = 1.7442679405212402
iteration 19, loss = 1.8031930923461914
iteration 20, loss = 1.664638876914978
iteration 21, loss = 1.4323770999908447
iteration 22, loss = 1.56233549118042
iteration 23, loss = 1.382582426071167
iteration 24, loss = 1.361390233039856
iteration 25, loss = 1.280623435974121
iteration 26, loss = 1.156245470046997
iteration 27, loss = 1.0893446207046509
iteration 28, loss = 1.2283300161361694
iteration 29, loss = 1.1495082378387451
iteration 30, loss = 1.046791434288025
iteration 31, loss = 1.0553076267242432
iteration 32, loss = 1.1561837196350098
iteration 33, loss = 0.9120980501174927
iteration 34, loss = 1.036378264427185
iteration 35, loss = 1.1323291063308716
iteration 36, loss = 0.9728342890739441
iteration 37, loss = 0.8311809301376343
iteration 38, loss = 0.82259202003479
iteration 39, loss = 0.968283474445343
iteration 40, loss = 0.7201439142227173
iteration 41, loss = 0.8967764377593994
iteration 42, loss = 0.8019179105758667
iteration 43, loss = 0.9751943945884705
iteration 44, loss = 0.8710008263587952
iteration 45, loss = 0.7815061807632446
iteration 46, loss = 0.8821713924407959
iteration 47, loss = 0.8320246934890747
iteration 48, loss = 0.7845994234085083
iteration 49, loss = 0.7527917623519897
iteration 50, loss = 0.9404569864273071
iteration 51, loss = 0.8101783990859985
iteration 52, loss = 0.8444243669509888
iteration 53, loss = 0.7946774959564209
iteration 54, loss = 0.7462878823280334
iteration 55, loss = 0.7083475589752197
iteration 56, loss = 0.7378106117248535
iteration 57, loss = 0.7748656868934631
iteration 58, loss = 0.8049771785736084
iteration 59, loss = 0.7347581386566162
iteration 60, loss = 0.8024836778640747
iteration 61, loss = 0.7196239233016968
iteration 62, loss = 0.7380484342575073
iteration 63, loss = 0.7843202948570251
iteration 64, loss = 0.8059636354446411
iteration 65, loss = 0.7452449202537537
iteration 66, loss = 0.800660252571106
iteration 67, loss = 0.816692590713501
iteration 68, loss = 0.7584443688392639
iteration 69, loss = 0.7220374345779419
iteration 70, loss = 0.6864768266677856
iteration 71, loss = 0.6904134750366211
iteration 72, loss = 0.7678707838058472
iteration 73, loss = 0.717519223690033
iteration 74, loss = 0.8163737058639526
iteration 75, loss = 0.7284868955612183
iteration 76, loss = 0.7297050952911377
iteration 77, loss = 0.7632040977478027
iteration 78, loss = 0.7217157483100891
iteration 79, loss = 0.6920337677001953
iteration 80, loss = 0.7686691284179688
iteration 81, loss = 0.7241506576538086
iteration 82, loss = 0.6795233488082886
iteration 83, loss = 0.6360650658607483
iteration 84, loss = 0.7881850600242615
iteration 85, loss = 0.7077117562294006
iteration 86, loss = 0.6956508159637451
iteration 87, loss = 0.6867303252220154
iteration 88, loss = 0.6146016120910645
iteration 89, loss = 0.7408857941627502
iteration 90, loss = 0.6483252048492432
iteration 91, loss = 0.6512947678565979
iteration 92, loss = 0.7014187574386597
iteration 93, loss = 0.651168942451477
iteration 94, loss = 0.6613729596138
iteration 95, loss = 0.7263113856315613
iteration 96, loss = 0.6961622834205627
iteration 97, loss = 0.6601884365081787
iteration 98, loss = 0.6817963123321533
iteration 99, loss = 0.6659190654754639
iteration 100, loss = 0.6761422157287598
iteration 101, loss = 0.7116658687591553
iteration 102, loss = 0.6601991653442383
iteration 103, loss = 0.612288236618042
iteration 104, loss = 0.7209382653236389
iteration 105, loss = 0.6992197632789612
iteration 106, loss = 0.6172089576721191
iteration 107, loss = 0.6765472888946533
iteration 108, loss = 0.6647762060165405
iteration 109, loss = 0.697258472442627
iteration 110, loss = 0.6701249480247498
iteration 111, loss = 0.6579909324645996
iteration 112, loss = 0.7092316746711731
iteration 113, loss = 0.7121694087982178
iteration 114, loss = 0.7662423253059387
iteration 115, loss = 0.6742342114448547
iteration 116, loss = 0.6713631749153137
iteration 117, loss = 0.6748831272125244
iteration 118, loss = 0.6860421895980835
iteration 119, loss = 0.7061015963554382
iteration 120, loss = 0.6459767818450928
iteration 121, loss = 0.6320803165435791
iteration 122, loss = 0.6353132128715515
iteration 123, loss = 0.6856415867805481
iteration 124, loss = 0.6274245381355286
iteration 125, loss = 0.7603161931037903
iteration 126, loss = 0.7040969729423523
iteration 127, loss = 0.6593318581581116
iteration 128, loss = 0.6213616728782654
iteration 129, loss = 0.7072387933731079
iteration 130, loss = 0.6106800436973572
iteration 131, loss = 0.6003463268280029
iteration 132, loss = 0.6918711066246033
iteration 133, loss = 0.6507925987243652
iteration 134, loss = 0.6597508788108826
iteration 135, loss = 0.684416651725769
iteration 136, loss = 0.6738823056221008
iteration 137, loss = 0.6261638402938843
iteration 138, loss = 0.6668969988822937
iteration 139, loss = 0.6821675896644592
iteration 140, loss = 0.6464429497718811
iteration 141, loss = 0.6448491811752319
iteration 142, loss = 0.6614406108856201
iteration 143, loss = 0.6828030347824097
iteration 144, loss = 0.7043931484222412
iteration 145, loss = 0.6152977347373962
iteration 146, loss = 0.6246293187141418
iteration 147, loss = 0.6408616304397583
iteration 148, loss = 0.6723345518112183
iteration 149, loss = 0.663948655128479
iteration 150, loss = 0.7282876372337341
iteration 151, loss = 0.6238565444946289
iteration 152, loss = 0.6407185196876526
iteration 153, loss = 0.688845157623291
iteration 154, loss = 0.6287569403648376
iteration 155, loss = 0.6801305413246155
iteration 156, loss = 0.6899335980415344
iteration 157, loss = 0.6291368007659912
iteration 158, loss = 0.6768514513969421
iteration 159, loss = 0.6333714127540588
iteration 160, loss = 0.5968260765075684
iteration 161, loss = 0.5958431959152222
iteration 162, loss = 0.5833760499954224
iteration 163, loss = 0.6456516981124878
iteration 164, loss = 0.638766884803772
iteration 165, loss = 0.5994998216629028
iteration 166, loss = 0.6935254335403442
iteration 167, loss = 0.5738834738731384
iteration 168, loss = 0.6562274694442749
iteration 169, loss = 0.6748152375221252
iteration 170, loss = 0.6526674628257751
iteration 171, loss = 0.6772907972335815
iteration 172, loss = 0.6159300804138184
iteration 173, loss = 0.6965635418891907
iteration 174, loss = 0.6079444289207458
iteration 175, loss = 0.6529042720794678
iteration 176, loss = 0.6268401145935059
iteration 177, loss = 0.6370748281478882
iteration 178, loss = 0.6154200434684753
iteration 179, loss = 0.6319118142127991
iteration 180, loss = 0.6102727055549622
iteration 181, loss = 0.6243149042129517
iteration 182, loss = 0.6646122932434082
iteration 183, loss = 0.6017153263092041
iteration 184, loss = 0.6743795871734619
iteration 185, loss = 0.5925337076187134
iteration 186, loss = 0.6635422110557556
iteration 187, loss = 0.5967966318130493
iteration 188, loss = 0.5886641144752502
iteration 189, loss = 0.6723138689994812
iteration 190, loss = 0.5856714248657227
iteration 191, loss = 0.6148437857627869
iteration 192, loss = 0.6243146657943726
iteration 193, loss = 0.653380811214447
iteration 194, loss = 0.6300414204597473
iteration 195, loss = 0.5704469680786133
iteration 196, loss = 0.5759300589561462
iteration 197, loss = 0.6545571684837341
iteration 198, loss = 0.62107253074646
iteration 199, loss = 0.5642964243888855
iteration 200, loss = 0.6756302714347839
iteration 201, loss = 0.6368690729141235
iteration 202, loss = 0.6158683896064758
iteration 203, loss = 0.6012193560600281
iteration 204, loss = 0.6253679394721985
iteration 205, loss = 0.5989663004875183
iteration 206, loss = 0.6604698896408081
iteration 207, loss = 0.5969766974449158
iteration 208, loss = 0.5858829617500305
iteration 209, loss = 0.6580573916435242
iteration 210, loss = 0.630953848361969
iteration 211, loss = 0.6082984209060669
iteration 212, loss = 0.5761041641235352
iteration 213, loss = 0.582761824131012
iteration 214, loss = 0.6351787447929382
iteration 215, loss = 0.6787907481193542
iteration 216, loss = 0.6063823699951172
iteration 217, loss = 0.6341686844825745
iteration 218, loss = 0.5847922563552856
iteration 219, loss = 0.6050956845283508
iteration 220, loss = 0.538118302822113
iteration 221, loss = 0.5643430948257446
iteration 222, loss = 0.6435768604278564
iteration 223, loss = 0.5748667120933533
iteration 224, loss = 0.5955254435539246
iteration 225, loss = 0.6050530672073364
iteration 226, loss = 0.594879686832428
iteration 227, loss = 0.5487987399101257
iteration 228, loss = 0.5948578715324402
iteration 229, loss = 0.6024776697158813
iteration 230, loss = 0.5612547993659973
iteration 231, loss = 0.5744787454605103
iteration 232, loss = 0.6445671319961548
iteration 233, loss = 0.584913969039917
iteration 234, loss = 0.6170163750648499
iteration 235, loss = 0.6163678765296936
iteration 236, loss = 0.5978312492370605
iteration 237, loss = 0.588493824005127
iteration 238, loss = 0.6717877388000488
iteration 239, loss = 0.5419536232948303
iteration 240, loss = 0.5668458938598633
iteration 241, loss = 0.6079019904136658
iteration 242, loss = 0.551693856716156
iteration 243, loss = 0.5333184003829956
iteration 244, loss = 0.5514560341835022
iteration 245, loss = 0.546019971370697
iteration 246, loss = 0.5923033356666565
iteration 247, loss = 0.5770089030265808
iteration 248, loss = 0.6202643513679504
iteration 249, loss = 0.6071622371673584
iteration 250, loss = 0.5460073947906494
iteration 251, loss = 0.5718172192573547
iteration 252, loss = 0.6253771781921387
iteration 253, loss = 0.6189034581184387
iteration 254, loss = 0.5678884983062744
iteration 255, loss = 0.545563817024231
iteration 256, loss = 0.6625697612762451
iteration 257, loss = 0.5922841429710388
iteration 258, loss = 0.5518192052841187
iteration 259, loss = 0.5340648293495178
iteration 260, loss = 0.5583742260932922
iteration 261, loss = 0.5511993169784546
iteration 262, loss = 0.565729558467865
iteration 263, loss = 0.6199744343757629
iteration 264, loss = 0.5174862146377563
iteration 265, loss = 0.563886821269989
iteration 266, loss = 0.5720221400260925
iteration 267, loss = 0.5072816610336304
iteration 268, loss = 0.5918846726417542
iteration 269, loss = 0.5579690337181091
iteration 270, loss = 0.5318894386291504
iteration 271, loss = 0.593333899974823
iteration 272, loss = 0.5126552581787109
iteration 273, loss = 0.5648230314254761
iteration 274, loss = 0.5532315969467163
iteration 275, loss = 0.4938880205154419
iteration 276, loss = 0.561130166053772
iteration 277, loss = 0.5276875495910645
iteration 278, loss = 0.5263012647628784
iteration 279, loss = 0.5475167632102966
iteration 280, loss = 0.5527069568634033
iteration 281, loss = 0.5799793601036072
iteration 282, loss = 0.5501925945281982
iteration 283, loss = 0.5525920391082764
iteration 284, loss = 0.5354595184326172
iteration 285, loss = 0.5073386430740356
iteration 286, loss = 0.5465327501296997
iteration 287, loss = 0.47588926553726196
iteration 288, loss = 0.488445907831192
iteration 289, loss = 0.5789532661437988
iteration 290, loss = 0.5592252612113953
iteration 291, loss = 0.5908983945846558
iteration 292, loss = 0.5245504379272461
iteration 293, loss = 0.4816417694091797
iteration 294, loss = 0.5279883146286011
iteration 295, loss = 0.5418044924736023
iteration 296, loss = 0.5069694519042969
iteration 297, loss = 0.5347012281417847
iteration 298, loss = 0.5639801025390625
iteration 299, loss = 0.4632008969783783
iteration 300, loss = 0.4844914674758911
iteration 1, loss = 0.54505455493927
iteration 2, loss = 0.5787914991378784
iteration 3, loss = 0.5416494011878967
iteration 4, loss = 0.4685502052307129
iteration 5, loss = 0.5598640441894531
iteration 6, loss = 0.5551049113273621
iteration 7, loss = 0.4895446300506592
iteration 8, loss = 0.47554710507392883
iteration 9, loss = 0.4893338978290558
iteration 10, loss = 0.5100253820419312
iteration 11, loss = 0.475119948387146
iteration 12, loss = 0.45999574661254883
iteration 13, loss = 0.46407395601272583
iteration 14, loss = 0.45542386174201965
iteration 15, loss = 0.508621871471405
iteration 16, loss = 0.5668056607246399
iteration 17, loss = 0.5303467512130737
iteration 18, loss = 0.48399117588996887
iteration 19, loss = 0.5268564224243164
iteration 20, loss = 0.4792938232421875
iteration 21, loss = 0.47707509994506836
iteration 22, loss = 0.5928324460983276
iteration 23, loss = 0.4726135730743408
iteration 24, loss = 0.4883323311805725
iteration 25, loss = 0.4455508291721344
iteration 26, loss = 0.4511256217956543
iteration 27, loss = 0.4841267466545105
iteration 28, loss = 0.4786359667778015
iteration 29, loss = 0.5228486061096191
iteration 30, loss = 0.4460560977458954
iteration 31, loss = 0.47217345237731934
iteration 32, loss = 0.5038575530052185
iteration 33, loss = 0.5649477243423462
iteration 34, loss = 0.48051702976226807
iteration 35, loss = 0.46481698751449585
iteration 36, loss = 0.4250307083129883
iteration 37, loss = 0.521712064743042
iteration 38, loss = 0.446836918592453
iteration 39, loss = 0.4756118059158325
iteration 40, loss = 0.532940149307251
iteration 41, loss = 0.48141688108444214
iteration 42, loss = 0.45618095993995667
iteration 43, loss = 0.4948236644268036
iteration 44, loss = 0.5158135294914246
iteration 45, loss = 0.5022284388542175
iteration 46, loss = 0.42183801531791687
iteration 47, loss = 0.49772799015045166
iteration 48, loss = 0.44091686606407166
iteration 49, loss = 0.4474148154258728
iteration 50, loss = 0.46318739652633667
iteration 51, loss = 0.4583081603050232
iteration 52, loss = 0.4473237097263336
iteration 53, loss = 0.47193872928619385
iteration 54, loss = 0.47447600960731506
iteration 55, loss = 0.4296737015247345
iteration 56, loss = 0.47159522771835327
iteration 57, loss = 0.5373795032501221
iteration 58, loss = 0.4132850766181946
iteration 59, loss = 0.4315077066421509
iteration 60, loss = 0.4446892738342285
iteration 61, loss = 0.5508828163146973
iteration 62, loss = 0.5074582695960999
iteration 63, loss = 0.46488359570503235
iteration 64, loss = 0.48072534799575806
iteration 65, loss = 0.40112340450286865
iteration 66, loss = 0.4442591667175293
iteration 67, loss = 0.43354588747024536
iteration 68, loss = 0.45960909128189087
iteration 69, loss = 0.5086498260498047
iteration 70, loss = 0.43674153089523315
iteration 71, loss = 0.47144418954849243
iteration 72, loss = 0.46753668785095215
iteration 73, loss = 0.46897587180137634
iteration 74, loss = 0.4255407750606537
iteration 75, loss = 0.4608939290046692
iteration 76, loss = 0.430847704410553
iteration 77, loss = 0.4572470486164093
iteration 78, loss = 0.419711172580719
iteration 79, loss = 0.4316038489341736
iteration 80, loss = 0.5002496838569641
iteration 81, loss = 0.42340195178985596
iteration 82, loss = 0.4685821831226349
iteration 83, loss = 0.38934630155563354
iteration 84, loss = 0.43084025382995605
iteration 85, loss = 0.3779151141643524
iteration 86, loss = 0.4280547499656677
iteration 87, loss = 0.3913170099258423
iteration 88, loss = 0.52897047996521
iteration 89, loss = 0.4294147491455078
iteration 90, loss = 0.441011905670166
iteration 91, loss = 0.4249520003795624
iteration 92, loss = 0.41534510254859924
iteration 93, loss = 0.466404527425766
iteration 94, loss = 0.399964839220047
iteration 95, loss = 0.48068684339523315
iteration 96, loss = 0.40575626492500305
iteration 97, loss = 0.4251043200492859
iteration 98, loss = 0.4313475489616394
iteration 99, loss = 0.41113364696502686
iteration 100, loss = 0.5133405923843384
iteration 101, loss = 0.4343264102935791
iteration 102, loss = 0.48576927185058594
iteration 103, loss = 0.4296686351299286
iteration 104, loss = 0.4031950533390045
iteration 105, loss = 0.3662596344947815
iteration 106, loss = 0.3765844404697418
iteration 107, loss = 0.38780564069747925
iteration 108, loss = 0.4287658631801605
iteration 109, loss = 0.42040300369262695
iteration 110, loss = 0.391207218170166
iteration 111, loss = 0.4290637671947479
iteration 112, loss = 0.42172515392303467
iteration 113, loss = 0.4229417145252228
iteration 114, loss = 0.39255791902542114
iteration 115, loss = 0.42977800965309143
iteration 116, loss = 0.37000906467437744
iteration 117, loss = 0.431348592042923
iteration 118, loss = 0.4160277545452118
iteration 119, loss = 0.4934670925140381
iteration 120, loss = 0.4099518656730652
iteration 121, loss = 0.37308990955352783
iteration 122, loss = 0.4243125915527344
iteration 123, loss = 0.49675828218460083
iteration 124, loss = 0.3667386770248413
iteration 125, loss = 0.41583219170570374
iteration 126, loss = 0.3941008150577545
iteration 127, loss = 0.42027994990348816
iteration 128, loss = 0.40180814266204834
iteration 129, loss = 0.341076135635376
iteration 130, loss = 0.3350914418697357
iteration 131, loss = 0.4332137703895569
iteration 132, loss = 0.3559233248233795
iteration 133, loss = 0.34243127703666687
iteration 134, loss = 0.3356844186782837
iteration 135, loss = 0.360088586807251
iteration 136, loss = 0.4000466465950012
iteration 137, loss = 0.35125768184661865
iteration 138, loss = 0.3443610966205597
iteration 139, loss = 0.35850173234939575
iteration 140, loss = 0.3634658753871918
iteration 141, loss = 0.35661154985427856
iteration 142, loss = 0.3574277460575104
iteration 143, loss = 0.3444218635559082
iteration 144, loss = 0.34085550904273987
iteration 145, loss = 0.39336398243904114
iteration 146, loss = 0.3400254249572754
iteration 147, loss = 0.4102545976638794
iteration 148, loss = 0.4076076149940491
iteration 149, loss = 0.32246914505958557
iteration 150, loss = 0.36920303106307983
iteration 151, loss = 0.3688122034072876
iteration 152, loss = 0.3604958653450012
iteration 153, loss = 0.37546032667160034
iteration 154, loss = 0.36757388710975647
iteration 155, loss = 0.352037638425827
iteration 156, loss = 0.30965685844421387
iteration 157, loss = 0.4313381612300873
iteration 158, loss = 0.3924422562122345
iteration 159, loss = 0.28885042667388916
iteration 160, loss = 0.38315117359161377
iteration 161, loss = 0.46218499541282654
iteration 162, loss = 0.31255993247032166
iteration 163, loss = 0.3494873642921448
iteration 164, loss = 0.32734301686286926
iteration 165, loss = 0.3300805687904358
iteration 166, loss = 0.3290553390979767
iteration 167, loss = 0.38480448722839355
iteration 168, loss = 0.3481505811214447
iteration 169, loss = 0.35494181513786316
iteration 170, loss = 0.30310481786727905
iteration 171, loss = 0.38944175839424133
iteration 172, loss = 0.36494386196136475
iteration 173, loss = 0.30225586891174316
iteration 174, loss = 0.31220337748527527
iteration 175, loss = 0.37559300661087036
iteration 176, loss = 0.3569214940071106
iteration 177, loss = 0.3331599235534668
iteration 178, loss = 0.3462654948234558
iteration 179, loss = 0.31802821159362793
iteration 180, loss = 0.39629462361335754
iteration 181, loss = 0.3330863416194916
iteration 182, loss = 0.33733364939689636
iteration 183, loss = 0.3358226716518402
iteration 184, loss = 0.35933390259742737
iteration 185, loss = 0.32854726910591125
iteration 186, loss = 0.35258686542510986
iteration 187, loss = 0.3003157675266266
iteration 188, loss = 0.36133888363838196
iteration 189, loss = 0.4007832407951355
iteration 190, loss = 0.3285517990589142
iteration 191, loss = 0.31854307651519775
iteration 192, loss = 0.36540284752845764
iteration 193, loss = 0.38520023226737976
iteration 194, loss = 0.2792539596557617
iteration 195, loss = 0.32488539814949036
iteration 196, loss = 0.3665236830711365
iteration 197, loss = 0.33603888750076294
iteration 198, loss = 0.30224767327308655
iteration 199, loss = 0.27418121695518494
iteration 200, loss = 0.2655576467514038
iteration 201, loss = 0.316764771938324
iteration 202, loss = 0.3574836850166321
iteration 203, loss = 0.3515722155570984
iteration 204, loss = 0.3458530306816101
iteration 205, loss = 0.2736356258392334
iteration 206, loss = 0.26795467734336853
iteration 207, loss = 0.2946285605430603
iteration 208, loss = 0.37808555364608765
iteration 209, loss = 0.3431924283504486
iteration 210, loss = 0.27627941966056824
iteration 211, loss = 0.30410122871398926
iteration 212, loss = 0.29256439208984375
iteration 213, loss = 0.2767227590084076
iteration 214, loss = 0.30735573172569275
iteration 215, loss = 0.32213079929351807
iteration 216, loss = 0.3061371445655823
iteration 217, loss = 0.24974961578845978
iteration 218, loss = 0.2630482017993927
iteration 219, loss = 0.2933725416660309
iteration 220, loss = 0.2592572569847107
iteration 221, loss = 0.30087801814079285
iteration 222, loss = 0.2744400203227997
iteration 223, loss = 0.3266840875148773
iteration 224, loss = 0.23739847540855408
iteration 225, loss = 0.33719536662101746
iteration 226, loss = 0.30397287011146545
iteration 227, loss = 0.2919168472290039
iteration 228, loss = 0.3453274369239807
iteration 229, loss = 0.29486608505249023
iteration 230, loss = 0.35991916060447693
iteration 231, loss = 0.30865299701690674
iteration 232, loss = 0.3191075921058655
iteration 233, loss = 0.2609603703022003
iteration 234, loss = 0.2929464876651764
iteration 235, loss = 0.2760167121887207
iteration 236, loss = 0.28112712502479553
iteration 237, loss = 0.2398339807987213
iteration 238, loss = 0.28617334365844727
iteration 239, loss = 0.2716328799724579
iteration 240, loss = 0.25588351488113403
iteration 241, loss = 0.27962711453437805
iteration 242, loss = 0.2949112057685852
iteration 243, loss = 0.29100093245506287
iteration 244, loss = 0.24411970376968384
iteration 245, loss = 0.2805246114730835
iteration 246, loss = 0.28753748536109924
iteration 247, loss = 0.33391687273979187
iteration 248, loss = 0.27862101793289185
iteration 249, loss = 0.2986753582954407
iteration 250, loss = 0.22984440624713898
iteration 251, loss = 0.3024260103702545
iteration 252, loss = 0.2867524325847626
iteration 253, loss = 0.34593892097473145
iteration 254, loss = 0.2552178204059601
iteration 255, loss = 0.28712648153305054
iteration 256, loss = 0.2836122512817383
iteration 257, loss = 0.3289186358451843
iteration 258, loss = 0.25028595328330994
iteration 259, loss = 0.22954325377941132
iteration 260, loss = 0.2758970260620117
iteration 261, loss = 0.2266777902841568
iteration 262, loss = 0.2508182227611542
iteration 263, loss = 0.21749815344810486
iteration 264, loss = 0.21865195035934448
iteration 265, loss = 0.25701257586479187
iteration 266, loss = 0.25255879759788513
iteration 267, loss = 0.26995712518692017
iteration 268, loss = 0.22160404920578003
iteration 269, loss = 0.2933938503265381
iteration 270, loss = 0.23065273463726044
iteration 271, loss = 0.23785240948200226
iteration 272, loss = 0.2101602405309677
iteration 273, loss = 0.24210989475250244
iteration 274, loss = 0.2562880218029022
iteration 275, loss = 0.23422960937023163
iteration 276, loss = 0.209819495677948
iteration 277, loss = 0.32563120126724243
iteration 278, loss = 0.26203837990760803
iteration 279, loss = 0.23836106061935425
iteration 280, loss = 0.22312742471694946
iteration 281, loss = 0.2648099362850189
iteration 282, loss = 0.24788719415664673
iteration 283, loss = 0.23495332896709442
iteration 284, loss = 0.21174149215221405
iteration 285, loss = 0.26732802391052246
iteration 286, loss = 0.24294623732566833
iteration 287, loss = 0.2110660970211029
iteration 288, loss = 0.2579313814640045
iteration 289, loss = 0.23146402835845947
iteration 290, loss = 0.2555377781391144
iteration 291, loss = 0.20878994464874268
iteration 292, loss = 0.247294619679451
iteration 293, loss = 0.21198298037052155
iteration 294, loss = 0.23127961158752441
iteration 295, loss = 0.2888225317001343
iteration 296, loss = 0.22669841349124908
iteration 297, loss = 0.2503344416618347
iteration 298, loss = 0.3102579712867737
iteration 299, loss = 0.2820316255092621
iteration 300, loss = 0.2772698700428009
iteration 1, loss = 0.28302881121635437
iteration 2, loss = 0.2820349335670471
iteration 3, loss = 0.2253752052783966
iteration 4, loss = 0.19948965311050415
iteration 5, loss = 0.18239136040210724
iteration 6, loss = 0.19306206703186035
iteration 7, loss = 0.26412203907966614
iteration 8, loss = 0.24480091035366058
iteration 9, loss = 0.28705325722694397
iteration 10, loss = 0.22157038748264313
iteration 11, loss = 0.21700413525104523
iteration 12, loss = 0.2168821543455124
iteration 13, loss = 0.2045610249042511
iteration 14, loss = 0.22348293662071228
iteration 15, loss = 0.22304987907409668
iteration 16, loss = 0.25133106112480164
iteration 17, loss = 0.1897534728050232
iteration 18, loss = 0.19465690851211548
iteration 19, loss = 0.2455163300037384
iteration 20, loss = 0.18886041641235352
iteration 21, loss = 0.24293990433216095
iteration 22, loss = 0.18468737602233887
iteration 23, loss = 0.19016185402870178
iteration 24, loss = 0.18848921358585358
iteration 25, loss = 0.1848006695508957
iteration 26, loss = 0.20825733244419098
iteration 27, loss = 0.19644476473331451
iteration 28, loss = 0.1992446631193161
iteration 29, loss = 0.17630234360694885
iteration 30, loss = 0.2120971828699112
iteration 31, loss = 0.24580663442611694
iteration 32, loss = 0.23355412483215332
iteration 33, loss = 0.237400084733963
iteration 34, loss = 0.183734729886055
iteration 35, loss = 0.20286570489406586
iteration 36, loss = 0.22112444043159485
iteration 37, loss = 0.17706315219402313
iteration 38, loss = 0.1752467155456543
iteration 39, loss = 0.18247899413108826
iteration 40, loss = 0.16649070382118225
iteration 41, loss = 0.22601726651191711
iteration 42, loss = 0.1982417106628418
iteration 43, loss = 0.19191396236419678
iteration 44, loss = 0.1675189733505249
iteration 45, loss = 0.172133207321167
iteration 46, loss = 0.2047286331653595
iteration 47, loss = 0.20663921535015106
iteration 48, loss = 0.1735854595899582
iteration 49, loss = 0.192027747631073
iteration 50, loss = 0.2290601134300232
iteration 51, loss = 0.19529661536216736
iteration 52, loss = 0.19445392489433289
iteration 53, loss = 0.1737680733203888
iteration 54, loss = 0.1788003295660019
iteration 55, loss = 0.17314887046813965
iteration 56, loss = 0.1867763102054596
iteration 57, loss = 0.2007126361131668
iteration 58, loss = 0.17534229159355164
iteration 59, loss = 0.1936066895723343
iteration 60, loss = 0.19799846410751343
iteration 61, loss = 0.17719975113868713
iteration 62, loss = 0.18340250849723816
iteration 63, loss = 0.17954283952713013
iteration 64, loss = 0.19694581627845764
iteration 65, loss = 0.14971858263015747
iteration 66, loss = 0.19737114012241364
iteration 67, loss = 0.18870097398757935
iteration 68, loss = 0.1495615690946579
iteration 69, loss = 0.22171087563037872
iteration 70, loss = 0.20994633436203003
iteration 71, loss = 0.17676205933094025
iteration 72, loss = 0.18871410191059113
iteration 73, loss = 0.1709538698196411
iteration 74, loss = 0.1932038962841034
iteration 75, loss = 0.1711469292640686
iteration 76, loss = 0.16012150049209595
iteration 77, loss = 0.2060077041387558
iteration 78, loss = 0.17629124224185944
iteration 79, loss = 0.19527286291122437
iteration 80, loss = 0.19368310272693634
iteration 81, loss = 0.16657587885856628
iteration 82, loss = 0.14723534882068634
iteration 83, loss = 0.20495416224002838
iteration 84, loss = 0.17489685118198395
iteration 85, loss = 0.18592914938926697
iteration 86, loss = 0.18859151005744934
iteration 87, loss = 0.16616082191467285
iteration 88, loss = 0.20201976597309113
iteration 89, loss = 0.20847821235656738
iteration 90, loss = 0.17441576719284058
iteration 91, loss = 0.1699235737323761
iteration 92, loss = 0.14828725159168243
iteration 93, loss = 0.17132321000099182
iteration 94, loss = 0.1895267367362976
iteration 95, loss = 0.1727847307920456
iteration 96, loss = 0.21375980973243713
iteration 97, loss = 0.15882447361946106
iteration 98, loss = 0.2178608626127243
iteration 99, loss = 0.19273610413074493
iteration 100, loss = 0.1587751805782318
iteration 101, loss = 0.1738673597574234
iteration 102, loss = 0.1918177753686905
iteration 103, loss = 0.18644455075263977
iteration 104, loss = 0.13614726066589355
iteration 105, loss = 0.17791950702667236
iteration 106, loss = 0.16343066096305847
iteration 107, loss = 0.2260773479938507
iteration 108, loss = 0.14941053092479706
iteration 109, loss = 0.1708616018295288
iteration 110, loss = 0.1351286768913269
iteration 111, loss = 0.13310328125953674
iteration 112, loss = 0.18067404627799988
iteration 113, loss = 0.20663683116436005
iteration 114, loss = 0.1836157739162445
iteration 115, loss = 0.19143018126487732
iteration 116, loss = 0.18022610247135162
iteration 117, loss = 0.15191152691841125
iteration 118, loss = 0.18242321908473969
iteration 119, loss = 0.1590275913476944
iteration 120, loss = 0.1905723661184311
iteration 121, loss = 0.13146434724330902
iteration 122, loss = 0.1502622663974762
iteration 123, loss = 0.17952591180801392
iteration 124, loss = 0.15487512946128845
iteration 125, loss = 0.1401648372411728
iteration 126, loss = 0.1458885669708252
iteration 127, loss = 0.19305431842803955
iteration 128, loss = 0.14202620089054108
iteration 129, loss = 0.16009017825126648
iteration 130, loss = 0.13384774327278137
iteration 131, loss = 0.12951435148715973
iteration 132, loss = 0.13668674230575562
iteration 133, loss = 0.14527234435081482
iteration 134, loss = 0.148251473903656
iteration 135, loss = 0.1622975468635559
iteration 136, loss = 0.22233501076698303
iteration 137, loss = 0.13102670013904572
iteration 138, loss = 0.15828749537467957
iteration 139, loss = 0.15241922438144684
iteration 140, loss = 0.17231051623821259
iteration 141, loss = 0.18144430220127106
iteration 142, loss = 0.13096259534358978
iteration 143, loss = 0.1481674760580063
iteration 144, loss = 0.18167293071746826
iteration 145, loss = 0.1345473974943161
iteration 146, loss = 0.17766602337360382
iteration 147, loss = 0.22856678068637848
iteration 148, loss = 0.15453238785266876
iteration 149, loss = 0.15803492069244385
iteration 150, loss = 0.12678703665733337
iteration 151, loss = 0.14923414587974548
iteration 152, loss = 0.15908291935920715
iteration 153, loss = 0.1558956354856491
iteration 154, loss = 0.12428402900695801
iteration 155, loss = 0.17171327769756317
iteration 156, loss = 0.15543119609355927
iteration 157, loss = 0.18206487596035004
iteration 158, loss = 0.1308869868516922
iteration 159, loss = 0.12306395173072815
iteration 160, loss = 0.1446923017501831
iteration 161, loss = 0.13145272433757782
iteration 162, loss = 0.12813392281532288
iteration 163, loss = 0.16273871064186096
iteration 164, loss = 0.15496201813220978
iteration 165, loss = 0.1432075798511505
iteration 166, loss = 0.1314459592103958
iteration 167, loss = 0.15595099329948425
iteration 168, loss = 0.12036225199699402
iteration 169, loss = 0.18033313751220703
iteration 170, loss = 0.1538345366716385
iteration 171, loss = 0.11615447700023651
iteration 172, loss = 0.1277955025434494
iteration 173, loss = 0.1530914604663849
iteration 174, loss = 0.1284332275390625
iteration 175, loss = 0.1466374695301056
iteration 176, loss = 0.12513908743858337
iteration 177, loss = 0.12220355868339539
iteration 178, loss = 0.12996220588684082
iteration 179, loss = 0.1268286406993866
iteration 180, loss = 0.13640658557415009
iteration 181, loss = 0.16313086450099945
iteration 182, loss = 0.15412919223308563
iteration 183, loss = 0.14976386725902557
iteration 184, loss = 0.12345221638679504
iteration 185, loss = 0.1307113617658615
iteration 186, loss = 0.12368360161781311
iteration 187, loss = 0.12111634016036987
iteration 188, loss = 0.12648501992225647
iteration 189, loss = 0.12459198385477066
iteration 190, loss = 0.11659764498472214
iteration 191, loss = 0.13339073956012726
iteration 192, loss = 0.12524786591529846
iteration 193, loss = 0.13707220554351807
iteration 194, loss = 0.14058861136436462
iteration 195, loss = 0.16562987864017487
iteration 196, loss = 0.173677459359169
iteration 197, loss = 0.12127935886383057
iteration 198, loss = 0.15493270754814148
iteration 199, loss = 0.10205120593309402
iteration 200, loss = 0.10461797565221786
iteration 201, loss = 0.11918321251869202
iteration 202, loss = 0.1116974800825119
iteration 203, loss = 0.172032430768013
iteration 204, loss = 0.12597453594207764
iteration 205, loss = 0.11568450927734375
iteration 206, loss = 0.11941643804311752
iteration 207, loss = 0.107514388859272
iteration 208, loss = 0.1086534857749939
iteration 209, loss = 0.10718182474374771
iteration 210, loss = 0.10523487627506256
iteration 211, loss = 0.1356230229139328
iteration 212, loss = 0.11888790130615234
iteration 213, loss = 0.11438735574483871
iteration 214, loss = 0.13899341225624084
iteration 215, loss = 0.1153734102845192
iteration 216, loss = 0.11988414824008942
iteration 217, loss = 0.13333874940872192
iteration 218, loss = 0.123222216963768
iteration 219, loss = 0.11984369903802872
iteration 220, loss = 0.12037808448076248
iteration 221, loss = 0.10115735977888107
iteration 222, loss = 0.13952143490314484
iteration 223, loss = 0.12365292012691498
iteration 224, loss = 0.11941587179899216
iteration 225, loss = 0.10307548195123672
iteration 226, loss = 0.12902513146400452
iteration 227, loss = 0.10897574573755264
iteration 228, loss = 0.1037704348564148
iteration 229, loss = 0.10794933140277863
iteration 230, loss = 0.09580767154693604
iteration 231, loss = 0.12292999029159546
iteration 232, loss = 0.10566075146198273
iteration 233, loss = 0.09932386130094528
iteration 234, loss = 0.13744598627090454
iteration 235, loss = 0.1360829472541809
iteration 236, loss = 0.1219339668750763
iteration 237, loss = 0.09502482414245605
iteration 238, loss = 0.15608839690685272
iteration 239, loss = 0.1682267040014267
iteration 240, loss = 0.10776591300964355
iteration 241, loss = 0.15612003207206726
iteration 242, loss = 0.09761428833007812
iteration 243, loss = 0.10368484258651733
iteration 244, loss = 0.10750501602888107
iteration 245, loss = 0.11442358791828156
iteration 246, loss = 0.10512373596429825
iteration 247, loss = 0.11895354092121124
iteration 248, loss = 0.12698769569396973
iteration 249, loss = 0.11238548159599304
iteration 250, loss = 0.12405538558959961
iteration 251, loss = 0.11537059396505356
iteration 252, loss = 0.14059917628765106
iteration 253, loss = 0.09896146506071091
iteration 254, loss = 0.13364139199256897
iteration 255, loss = 0.11978983879089355
iteration 256, loss = 0.08712899684906006
iteration 257, loss = 0.09317611157894135
iteration 258, loss = 0.10653627663850784
iteration 259, loss = 0.09399640560150146
iteration 260, loss = 0.08959517627954483
iteration 261, loss = 0.09457556903362274
iteration 262, loss = 0.09466490149497986
iteration 263, loss = 0.11236309260129929
iteration 264, loss = 0.12708373367786407
iteration 265, loss = 0.10781443864107132
iteration 266, loss = 0.10213117301464081
iteration 267, loss = 0.13351953029632568
iteration 268, loss = 0.09807910025119781
iteration 269, loss = 0.09756647795438766
iteration 270, loss = 0.10327860713005066
iteration 271, loss = 0.10707926005125046
iteration 272, loss = 0.11262356489896774
iteration 273, loss = 0.10642978549003601
iteration 274, loss = 0.08993211388587952
iteration 275, loss = 0.09517724066972733
iteration 276, loss = 0.11672946065664291
iteration 277, loss = 0.1299111396074295
iteration 278, loss = 0.09023670852184296
iteration 279, loss = 0.09675495326519012
iteration 280, loss = 0.09787226468324661
iteration 281, loss = 0.09252731502056122
iteration 282, loss = 0.08967989683151245
iteration 283, loss = 0.10807161033153534
iteration 284, loss = 0.09419010579586029
iteration 285, loss = 0.08723034709692001
iteration 286, loss = 0.08436353504657745
iteration 287, loss = 0.0889430046081543
iteration 288, loss = 0.09621530771255493
iteration 289, loss = 0.0847846120595932
iteration 290, loss = 0.11670150607824326
iteration 291, loss = 0.11226221919059753
iteration 292, loss = 0.09513084590435028
iteration 293, loss = 0.09716442227363586
iteration 294, loss = 0.11444689333438873
iteration 295, loss = 0.08781787753105164
iteration 296, loss = 0.081353560090065
iteration 297, loss = 0.08345888555049896
iteration 298, loss = 0.09988655894994736
iteration 299, loss = 0.09020780026912689
iteration 300, loss = 0.0758519172668457
iteration 1, loss = 0.0810999721288681
iteration 2, loss = 0.12059393525123596
iteration 3, loss = 0.09687750786542892
iteration 4, loss = 0.10179858654737473
iteration 5, loss = 0.08035203069448471
iteration 6, loss = 0.11384100466966629
iteration 7, loss = 0.10539188981056213
iteration 8, loss = 0.0928657129406929
iteration 9, loss = 0.10252036154270172
iteration 10, loss = 0.11386583000421524
iteration 11, loss = 0.07944280654191971
iteration 12, loss = 0.11610464751720428
iteration 13, loss = 0.08015336841344833
iteration 14, loss = 0.08025935292243958
iteration 15, loss = 0.07879261672496796
iteration 16, loss = 0.09716222435235977
iteration 17, loss = 0.0809231698513031
iteration 18, loss = 0.09863441437482834
iteration 19, loss = 0.11044802516698837
iteration 20, loss = 0.09469926357269287
iteration 21, loss = 0.0824044793844223
iteration 22, loss = 0.08363164216279984
iteration 23, loss = 0.08780137449502945
iteration 24, loss = 0.08716437220573425
iteration 25, loss = 0.11185160279273987
iteration 26, loss = 0.0979933962225914
iteration 27, loss = 0.10606501251459122
iteration 28, loss = 0.1435924470424652
iteration 29, loss = 0.07733603566884995
iteration 30, loss = 0.08533598482608795
iteration 31, loss = 0.07688028365373611
iteration 32, loss = 0.09409043937921524
iteration 33, loss = 0.10392685234546661
iteration 34, loss = 0.07726141065359116
iteration 35, loss = 0.08195830881595612
iteration 36, loss = 0.10077835619449615
iteration 37, loss = 0.07930883765220642
iteration 38, loss = 0.09030238538980484
iteration 39, loss = 0.09147896617650986
iteration 40, loss = 0.07380267232656479
iteration 41, loss = 0.08589868247509003
iteration 42, loss = 0.07936844229698181
iteration 43, loss = 0.0904574915766716
iteration 44, loss = 0.0759960412979126
iteration 45, loss = 0.07243913412094116
iteration 46, loss = 0.08789295703172684
iteration 47, loss = 0.0802449882030487
iteration 48, loss = 0.11053622514009476
iteration 49, loss = 0.07858940213918686
iteration 50, loss = 0.09029430150985718
iteration 51, loss = 0.07912987470626831
iteration 52, loss = 0.08092378079891205
iteration 53, loss = 0.07016486674547195
iteration 54, loss = 0.07057574391365051
iteration 55, loss = 0.08084799349308014
iteration 56, loss = 0.08008921146392822
iteration 57, loss = 0.08361082524061203
iteration 58, loss = 0.07839155197143555
iteration 59, loss = 0.07251150906085968
iteration 60, loss = 0.08708187192678452
iteration 61, loss = 0.0692870020866394
iteration 62, loss = 0.07496458292007446
iteration 63, loss = 0.07137048244476318
iteration 64, loss = 0.07667645812034607
iteration 65, loss = 0.07047472149133682
iteration 66, loss = 0.08585378527641296
iteration 67, loss = 0.07055818289518356
iteration 68, loss = 0.086116723716259
iteration 69, loss = 0.08006814867258072
iteration 70, loss = 0.0656982958316803
iteration 71, loss = 0.07583241164684296
iteration 72, loss = 0.09587480127811432
iteration 73, loss = 0.06937985122203827
iteration 74, loss = 0.10031015425920486
iteration 75, loss = 0.06748396903276443
iteration 76, loss = 0.0944104716181755
iteration 77, loss = 0.07034838199615479
iteration 78, loss = 0.0822007954120636
iteration 79, loss = 0.07429245114326477
iteration 80, loss = 0.11516949534416199
iteration 81, loss = 0.09512533247470856
iteration 82, loss = 0.09848794341087341
iteration 83, loss = 0.06583677232265472
iteration 84, loss = 0.08115782588720322
iteration 85, loss = 0.09080281108617783
iteration 86, loss = 0.07331617921590805
iteration 87, loss = 0.090367391705513
iteration 88, loss = 0.06689468026161194
iteration 89, loss = 0.06902456283569336
iteration 90, loss = 0.08049052953720093
iteration 91, loss = 0.0974615290760994
iteration 92, loss = 0.07301947474479675
iteration 93, loss = 0.0673956573009491
iteration 94, loss = 0.06362371891736984
iteration 95, loss = 0.06674487143754959
iteration 96, loss = 0.07599050551652908
iteration 97, loss = 0.07498644292354584
iteration 98, loss = 0.06832046061754227
iteration 99, loss = 0.07786043733358383
iteration 100, loss = 0.07410586625337601
iteration 101, loss = 0.06804635375738144
iteration 102, loss = 0.09131599217653275
iteration 103, loss = 0.09193139523267746
iteration 104, loss = 0.07184454053640366
iteration 105, loss = 0.0825008824467659
iteration 106, loss = 0.07035461068153381
iteration 107, loss = 0.06512360274791718
iteration 108, loss = 0.07936794310808182
iteration 109, loss = 0.0665813609957695
iteration 110, loss = 0.06544572860002518
iteration 111, loss = 0.07059027999639511
iteration 112, loss = 0.07266541570425034
iteration 113, loss = 0.06183541193604469
iteration 114, loss = 0.07889154553413391
iteration 115, loss = 0.0846284031867981
iteration 116, loss = 0.060536857694387436
iteration 117, loss = 0.06041879579424858
iteration 118, loss = 0.07799018174409866
iteration 119, loss = 0.06475131958723068
iteration 120, loss = 0.06660379469394684
iteration 121, loss = 0.058649368584156036
iteration 122, loss = 0.06541693955659866
iteration 123, loss = 0.07466544955968857
iteration 124, loss = 0.057244278490543365
iteration 125, loss = 0.09036731719970703
iteration 126, loss = 0.08763840794563293
iteration 127, loss = 0.08772013336420059
iteration 128, loss = 0.056004077196121216
iteration 129, loss = 0.08018705248832703
iteration 130, loss = 0.058274995535612106
iteration 131, loss = 0.07075057178735733
iteration 132, loss = 0.06766300648450851
iteration 133, loss = 0.06822574883699417
iteration 134, loss = 0.06000174582004547
iteration 135, loss = 0.05882151797413826
iteration 136, loss = 0.08846042305231094
iteration 137, loss = 0.06084912270307541
iteration 138, loss = 0.05930047854781151
iteration 139, loss = 0.06711720675230026
iteration 140, loss = 0.0594097375869751
iteration 141, loss = 0.059901416301727295
iteration 142, loss = 0.06389170140028
iteration 143, loss = 0.05504327267408371
iteration 144, loss = 0.05796044319868088
iteration 145, loss = 0.061907511204481125
iteration 146, loss = 0.062386251986026764
iteration 147, loss = 0.0812150239944458
iteration 148, loss = 0.058075256645679474
iteration 149, loss = 0.08491691946983337
iteration 150, loss = 0.08173791319131851
iteration 151, loss = 0.07928118854761124
iteration 152, loss = 0.07716401666402817
iteration 153, loss = 0.07320641726255417
iteration 154, loss = 0.07118652760982513
iteration 155, loss = 0.06648609042167664
iteration 156, loss = 0.05965925008058548
iteration 157, loss = 0.05915858969092369
iteration 158, loss = 0.06468234211206436
iteration 159, loss = 0.06496084481477737
iteration 160, loss = 0.06950092315673828
iteration 161, loss = 0.060719408094882965
iteration 162, loss = 0.06430988013744354
iteration 163, loss = 0.06271747499704361
iteration 164, loss = 0.06452380865812302
iteration 165, loss = 0.06717182695865631
iteration 166, loss = 0.07476561516523361
iteration 167, loss = 0.06444670259952545
iteration 168, loss = 0.061246197670698166
iteration 169, loss = 0.07002171128988266
iteration 170, loss = 0.0784643217921257
iteration 171, loss = 0.05294835567474365
iteration 172, loss = 0.059594959020614624
iteration 173, loss = 0.06107846274971962
iteration 174, loss = 0.0856291875243187
iteration 175, loss = 0.05240426957607269
iteration 176, loss = 0.053149692714214325
iteration 177, loss = 0.05519038438796997
iteration 178, loss = 0.05081870034337044
iteration 179, loss = 0.1001211553812027
iteration 180, loss = 0.053220346570014954
iteration 181, loss = 0.0627811849117279
iteration 182, loss = 0.05970459431409836
iteration 183, loss = 0.058368392288684845
iteration 184, loss = 0.06072574108839035
iteration 185, loss = 0.05314398556947708
iteration 186, loss = 0.07748687267303467
iteration 187, loss = 0.053569842129945755
iteration 188, loss = 0.05480961501598358
iteration 189, loss = 0.05359167978167534
iteration 190, loss = 0.0767693743109703
iteration 191, loss = 0.050737056881189346
iteration 192, loss = 0.05623435974121094
iteration 193, loss = 0.052769601345062256
iteration 194, loss = 0.05460408329963684
iteration 195, loss = 0.05377211421728134
iteration 196, loss = 0.05862259864807129
iteration 197, loss = 0.07711661607027054
iteration 198, loss = 0.055432744324207306
iteration 199, loss = 0.06378312408924103
iteration 200, loss = 0.053074803203344345
iteration 201, loss = 0.060809604823589325
iteration 202, loss = 0.07770126312971115
iteration 203, loss = 0.05069156736135483
iteration 204, loss = 0.053409770131111145
iteration 205, loss = 0.059043969959020615
iteration 206, loss = 0.0626937597990036
iteration 207, loss = 0.06087413430213928
iteration 208, loss = 0.055066753178834915
iteration 209, loss = 0.053026702255010605
iteration 210, loss = 0.055602315813302994
iteration 211, loss = 0.05607734993100166
iteration 212, loss = 0.07463738322257996
iteration 213, loss = 0.060981385409832
iteration 214, loss = 0.05439520999789238
iteration 215, loss = 0.06281518936157227
iteration 216, loss = 0.054915089160203934
iteration 217, loss = 0.06965622305870056
iteration 218, loss = 0.050892557948827744
iteration 219, loss = 0.04897328466176987
iteration 220, loss = 0.0636880099773407
iteration 221, loss = 0.06000982224941254
iteration 222, loss = 0.048835113644599915
iteration 223, loss = 0.0532684400677681
iteration 224, loss = 0.049003422260284424
iteration 225, loss = 0.056861761957407
iteration 226, loss = 0.06688026338815689
iteration 227, loss = 0.04946122318506241
iteration 228, loss = 0.04980898275971413
iteration 229, loss = 0.08281755447387695
iteration 230, loss = 0.05736584961414337
iteration 231, loss = 0.050243668258190155
iteration 232, loss = 0.048780955374240875
iteration 233, loss = 0.05100380629301071
iteration 234, loss = 0.04864862561225891
iteration 235, loss = 0.06730566173791885
iteration 236, loss = 0.050603944808244705
iteration 237, loss = 0.0563175231218338
iteration 238, loss = 0.048337846994400024
iteration 239, loss = 0.05455297604203224
iteration 240, loss = 0.04933129996061325
iteration 241, loss = 0.0547868013381958
iteration 242, loss = 0.04899386689066887
iteration 243, loss = 0.05169195309281349
iteration 244, loss = 0.04753538966178894
iteration 245, loss = 0.0473877415060997
iteration 246, loss = 0.055261868983507156
iteration 247, loss = 0.05411975830793381
iteration 248, loss = 0.060607653111219406
iteration 249, loss = 0.06037447601556778
iteration 250, loss = 0.045632004737854004
iteration 251, loss = 0.048911578953266144
iteration 252, loss = 0.07474355399608612
iteration 253, loss = 0.061212241649627686
iteration 254, loss = 0.04638787358999252
iteration 255, loss = 0.06526026129722595
iteration 256, loss = 0.05409489944577217
iteration 257, loss = 0.05591781809926033
iteration 258, loss = 0.05535511672496796
iteration 259, loss = 0.048671673983335495
iteration 260, loss = 0.0595136322081089
iteration 261, loss = 0.07058250159025192
iteration 262, loss = 0.06422695517539978
iteration 263, loss = 0.05594959482550621
iteration 264, loss = 0.051726073026657104
iteration 265, loss = 0.06146370992064476
iteration 266, loss = 0.0460205152630806
iteration 267, loss = 0.05003263056278229
iteration 268, loss = 0.056968845427036285
iteration 269, loss = 0.050909098237752914
iteration 270, loss = 0.068959541618824
iteration 271, loss = 0.046012286096811295
iteration 272, loss = 0.06157490238547325
iteration 273, loss = 0.05596105381846428
iteration 274, loss = 0.05403469130396843
iteration 275, loss = 0.04616566747426987
iteration 276, loss = 0.04809020087122917
iteration 277, loss = 0.054715562611818314
iteration 278, loss = 0.04711226746439934
iteration 279, loss = 0.04815685376524925
iteration 280, loss = 0.04630425572395325
iteration 281, loss = 0.046411242336034775
iteration 282, loss = 0.057322464883327484
iteration 283, loss = 0.0567144975066185
iteration 284, loss = 0.06405037641525269
iteration 285, loss = 0.04161494970321655
iteration 286, loss = 0.04939589276909828
iteration 287, loss = 0.06345194578170776
iteration 288, loss = 0.04295462369918823
iteration 289, loss = 0.047710005193948746
iteration 290, loss = 0.05411834642291069
iteration 291, loss = 0.05011504888534546
iteration 292, loss = 0.0447545200586319
iteration 293, loss = 0.044466905295848846
iteration 294, loss = 0.05827578902244568
iteration 295, loss = 0.04537598043680191
iteration 296, loss = 0.06190572306513786
iteration 297, loss = 0.043027326464653015
iteration 298, loss = 0.059740547090768814
iteration 299, loss = 0.05351490154862404
iteration 300, loss = 0.08384842425584793
iteration 1, loss = 0.0512566938996315
iteration 2, loss = 0.04121751710772514
iteration 3, loss = 0.04494369775056839
iteration 4, loss = 0.04567064344882965
iteration 5, loss = 0.04859790578484535
iteration 6, loss = 0.045700110495090485
iteration 7, loss = 0.05050390213727951
iteration 8, loss = 0.04944854602217674
iteration 9, loss = 0.04033687710762024
iteration 10, loss = 0.04608223959803581
iteration 11, loss = 0.04227304831147194
iteration 12, loss = 0.043882232159376144
iteration 13, loss = 0.039751842617988586
iteration 14, loss = 0.049881353974342346
iteration 15, loss = 0.04021931812167168
iteration 16, loss = 0.042566925287246704
iteration 17, loss = 0.043897245079278946
iteration 18, loss = 0.044766806066036224
iteration 19, loss = 0.040523726493120193
iteration 20, loss = 0.04429233446717262
iteration 21, loss = 0.042091794312000275
iteration 22, loss = 0.053887154906988144
iteration 23, loss = 0.05447445437312126
iteration 24, loss = 0.04161756485700607
iteration 25, loss = 0.050065264105796814
iteration 26, loss = 0.03919005021452904
iteration 27, loss = 0.03699828311800957
iteration 28, loss = 0.05908272787928581
iteration 29, loss = 0.05775740370154381
iteration 30, loss = 0.04326504468917847
iteration 31, loss = 0.04178469255566597
iteration 32, loss = 0.042020998895168304
iteration 33, loss = 0.04186989739537239
iteration 34, loss = 0.04768604040145874
iteration 35, loss = 0.044293779879808426
iteration 36, loss = 0.038671448826789856
iteration 37, loss = 0.05355402082204819
iteration 38, loss = 0.03968997672200203
iteration 39, loss = 0.04136655852198601
iteration 40, loss = 0.040619298815727234
iteration 41, loss = 0.046165626496076584
iteration 42, loss = 0.043257031589746475
iteration 43, loss = 0.038155701011419296
iteration 44, loss = 0.03780160844326019
iteration 45, loss = 0.04076044261455536
iteration 46, loss = 0.03868252784013748
iteration 47, loss = 0.03862637281417847
iteration 48, loss = 0.04095137119293213
iteration 49, loss = 0.039171237498521805
iteration 50, loss = 0.04281992092728615
iteration 51, loss = 0.0390283539891243
iteration 52, loss = 0.04556852951645851
iteration 53, loss = 0.062189437448978424
iteration 54, loss = 0.041519153863191605
iteration 55, loss = 0.03847014531493187
iteration 56, loss = 0.047330066561698914
iteration 57, loss = 0.04760236665606499
iteration 58, loss = 0.04193996265530586
iteration 59, loss = 0.059383489191532135
iteration 60, loss = 0.03787513077259064
iteration 61, loss = 0.05280902609229088
iteration 62, loss = 0.05524946004152298
iteration 63, loss = 0.07346542179584503
iteration 64, loss = 0.04845406487584114
iteration 65, loss = 0.039746738970279694
iteration 66, loss = 0.03949270397424698
iteration 67, loss = 0.037192460149526596
iteration 68, loss = 0.047834962606430054
iteration 69, loss = 0.057915933430194855
iteration 70, loss = 0.059811294078826904
iteration 71, loss = 0.04225006327033043
iteration 72, loss = 0.047313276678323746
iteration 73, loss = 0.039047855883836746
iteration 74, loss = 0.051715366542339325
iteration 75, loss = 0.045100510120391846
iteration 76, loss = 0.03815551474690437
iteration 77, loss = 0.03701970353722572
iteration 78, loss = 0.038887862116098404
iteration 79, loss = 0.04153478518128395
iteration 80, loss = 0.04870979115366936
iteration 81, loss = 0.037899941205978394
iteration 82, loss = 0.039909351617097855
iteration 83, loss = 0.04824689403176308
iteration 84, loss = 0.03703383728861809
iteration 85, loss = 0.04027891904115677
iteration 86, loss = 0.04224022105336189
iteration 87, loss = 0.04724195972084999
iteration 88, loss = 0.038965463638305664
iteration 89, loss = 0.03530734404921532
iteration 90, loss = 0.03746930882334709
iteration 91, loss = 0.03490479663014412
iteration 92, loss = 0.04382960498332977
iteration 93, loss = 0.041389141231775284
iteration 94, loss = 0.06529504805803299
iteration 95, loss = 0.0377388671040535
iteration 96, loss = 0.034783460199832916
iteration 97, loss = 0.037040065973997116
iteration 98, loss = 0.037244655191898346
iteration 99, loss = 0.058647796511650085
iteration 100, loss = 0.05648605525493622
iteration 101, loss = 0.0416242741048336
iteration 102, loss = 0.03826938569545746
iteration 103, loss = 0.03622261434793472
iteration 104, loss = 0.0345492884516716
iteration 105, loss = 0.036491669714450836
iteration 106, loss = 0.034242723137140274
iteration 107, loss = 0.0380299836397171
iteration 108, loss = 0.04018757864832878
iteration 109, loss = 0.03499830141663551
iteration 110, loss = 0.03338918462395668
iteration 111, loss = 0.03693586215376854
iteration 112, loss = 0.03938889503479004
iteration 113, loss = 0.041406068950891495
iteration 114, loss = 0.04151865467429161
iteration 115, loss = 0.046153098344802856
iteration 116, loss = 0.037292469292879105
iteration 117, loss = 0.048790041357278824
iteration 118, loss = 0.03710944578051567
iteration 119, loss = 0.050528042018413544
iteration 120, loss = 0.03478090092539787
iteration 121, loss = 0.0431068055331707
iteration 122, loss = 0.0356743298470974
iteration 123, loss = 0.034125033766031265
iteration 124, loss = 0.034519825130701065
iteration 125, loss = 0.036964625120162964
iteration 126, loss = 0.03505159169435501
iteration 127, loss = 0.03655935451388359
iteration 128, loss = 0.04610292613506317
iteration 129, loss = 0.039980288594961166
iteration 130, loss = 0.032252535223960876
iteration 131, loss = 0.03607325255870819
iteration 132, loss = 0.04174124822020531
iteration 133, loss = 0.03481138497591019
iteration 134, loss = 0.047496993094682693
iteration 135, loss = 0.032985858619213104
iteration 136, loss = 0.033367376774549484
iteration 137, loss = 0.03881578892469406
iteration 138, loss = 0.04163765162229538
iteration 139, loss = 0.031450312584638596
iteration 140, loss = 0.03322587162256241
iteration 141, loss = 0.04178665578365326
iteration 142, loss = 0.034971557557582855
iteration 143, loss = 0.04803040251135826
iteration 144, loss = 0.045197419822216034
iteration 145, loss = 0.03459858521819115
iteration 146, loss = 0.03590691089630127
iteration 147, loss = 0.037705451250076294
iteration 148, loss = 0.044350575655698776
iteration 149, loss = 0.03604767099022865
iteration 150, loss = 0.033918384462594986
iteration 151, loss = 0.033252108842134476
iteration 152, loss = 0.03647521883249283
iteration 153, loss = 0.03364713862538338
iteration 154, loss = 0.03608667477965355
iteration 155, loss = 0.03283139318227768
iteration 156, loss = 0.03174499422311783
iteration 157, loss = 0.03568907082080841
iteration 158, loss = 0.030641034245491028
iteration 159, loss = 0.03487187623977661
iteration 160, loss = 0.033092282712459564
iteration 161, loss = 0.03304111212491989
iteration 162, loss = 0.04155074805021286
iteration 163, loss = 0.04481068253517151
iteration 164, loss = 0.031222809106111526
iteration 165, loss = 0.041369903832674026
iteration 166, loss = 0.03491276502609253
iteration 167, loss = 0.03202233836054802
iteration 168, loss = 0.04653953015804291
iteration 169, loss = 0.034664086997509
iteration 170, loss = 0.032313667237758636
iteration 171, loss = 0.03312201797962189
iteration 172, loss = 0.034409262239933014
iteration 173, loss = 0.032729074358940125
iteration 174, loss = 0.03192785382270813
iteration 175, loss = 0.042496636509895325
iteration 176, loss = 0.050158821046352386
iteration 177, loss = 0.032526034861803055
iteration 178, loss = 0.0336809903383255
iteration 179, loss = 0.03826478496193886
iteration 180, loss = 0.030128058046102524
iteration 181, loss = 0.031174909323453903
iteration 182, loss = 0.033201031386852264
iteration 183, loss = 0.05749383941292763
iteration 184, loss = 0.04323333501815796
iteration 185, loss = 0.031765032559633255
iteration 186, loss = 0.03354593366384506
iteration 187, loss = 0.03360426425933838
iteration 188, loss = 0.032279036939144135
iteration 189, loss = 0.03659200668334961
iteration 190, loss = 0.030368253588676453
iteration 191, loss = 0.04699089750647545
iteration 192, loss = 0.033848997205495834
iteration 193, loss = 0.03236518055200577
iteration 194, loss = 0.03779593110084534
iteration 195, loss = 0.032643549144268036
iteration 196, loss = 0.029931677505373955
iteration 197, loss = 0.0352565161883831
iteration 198, loss = 0.03574591875076294
iteration 199, loss = 0.02951624244451523
iteration 200, loss = 0.030927691608667374
iteration 201, loss = 0.04039732366800308
iteration 202, loss = 0.039462774991989136
iteration 203, loss = 0.04208632558584213
iteration 204, loss = 0.0438280813395977
iteration 205, loss = 0.0314921960234642
iteration 206, loss = 0.029811253771185875
iteration 207, loss = 0.030105816200375557
iteration 208, loss = 0.03104904666543007
iteration 209, loss = 0.02993660233914852
iteration 210, loss = 0.028879091143608093
iteration 211, loss = 0.0354250930249691
iteration 212, loss = 0.03135726973414421
iteration 213, loss = 0.03446389362215996
iteration 214, loss = 0.02855333313345909
iteration 215, loss = 0.036536093801259995
iteration 216, loss = 0.03133406490087509
iteration 217, loss = 0.029071450233459473
iteration 218, loss = 0.03682394698262215
iteration 219, loss = 0.03495298698544502
iteration 220, loss = 0.033060066401958466
iteration 221, loss = 0.031637828797101974
iteration 222, loss = 0.031693797558546066
iteration 223, loss = 0.04202798381447792
iteration 224, loss = 0.027988916262984276
iteration 225, loss = 0.032945964485406876
iteration 226, loss = 0.030761834233999252
iteration 227, loss = 0.029399557039141655
iteration 228, loss = 0.0480610728263855
iteration 229, loss = 0.02917053923010826
iteration 230, loss = 0.041238654404878616
iteration 231, loss = 0.027842950075864792
iteration 232, loss = 0.02799876593053341
iteration 233, loss = 0.031961843371391296
iteration 234, loss = 0.040723104029893875
iteration 235, loss = 0.03057091310620308
iteration 236, loss = 0.028119435533881187
iteration 237, loss = 0.03807200491428375
iteration 238, loss = 0.029327837750315666
iteration 239, loss = 0.04018112272024155
iteration 240, loss = 0.02884623408317566
iteration 241, loss = 0.03520524874329567
iteration 242, loss = 0.03596257045865059
iteration 243, loss = 0.02828468382358551
iteration 244, loss = 0.029753539711236954
iteration 245, loss = 0.035299621522426605
iteration 246, loss = 0.025874972343444824
iteration 247, loss = 0.027389980852603912
iteration 248, loss = 0.03752581775188446
iteration 249, loss = 0.03495292365550995
iteration 250, loss = 0.03089974820613861
iteration 251, loss = 0.027676593512296677
iteration 252, loss = 0.02993197925388813
iteration 253, loss = 0.03741035982966423
iteration 254, loss = 0.03443016856908798
iteration 255, loss = 0.03988811746239662
iteration 256, loss = 0.028201639652252197
iteration 257, loss = 0.027862461283802986
iteration 258, loss = 0.02906310372054577
iteration 259, loss = 0.028529342263936996
iteration 260, loss = 0.03831174224615097
iteration 261, loss = 0.030321380123496056
iteration 262, loss = 0.03194255009293556
iteration 263, loss = 0.02985631301999092
iteration 264, loss = 0.026472829282283783
iteration 265, loss = 0.027456795796751976
iteration 266, loss = 0.0308464914560318
iteration 267, loss = 0.02677975781261921
iteration 268, loss = 0.0281522199511528
iteration 269, loss = 0.030468972399830818
iteration 270, loss = 0.031553011387586594
iteration 271, loss = 0.036097295582294464
iteration 272, loss = 0.042629826813936234
iteration 273, loss = 0.03281094878911972
iteration 274, loss = 0.03206036239862442
iteration 275, loss = 0.031415317207574844
iteration 276, loss = 0.026402611285448074
iteration 277, loss = 0.027141697704792023
iteration 278, loss = 0.030498243868350983
iteration 279, loss = 0.04244915395975113
iteration 280, loss = 0.02864452451467514
iteration 281, loss = 0.025550169870257378
iteration 282, loss = 0.025484493002295494
iteration 283, loss = 0.02867237478494644
iteration 284, loss = 0.0272710919380188
iteration 285, loss = 0.025361737236380577
iteration 286, loss = 0.02817249298095703
iteration 287, loss = 0.026317521929740906
iteration 288, loss = 0.027132688090205193
iteration 289, loss = 0.030195925384759903
iteration 290, loss = 0.03605351597070694
iteration 291, loss = 0.03164821118116379
iteration 292, loss = 0.026061778888106346
iteration 293, loss = 0.024655597284436226
iteration 294, loss = 0.024663975462317467
iteration 295, loss = 0.03154057264328003
iteration 296, loss = 0.02639637142419815
iteration 297, loss = 0.025028210133314133
iteration 298, loss = 0.030896663665771484
iteration 299, loss = 0.03221829608082771
iteration 300, loss = 0.026512442156672478
iteration 1, loss = 0.037806425243616104
iteration 2, loss = 0.026850540190935135
iteration 3, loss = 0.02580181322991848
iteration 4, loss = 0.037665851414203644
iteration 5, loss = 0.028639618307352066
iteration 6, loss = 0.03355567157268524
iteration 7, loss = 0.026405280455946922
iteration 8, loss = 0.029292650520801544
iteration 9, loss = 0.027763742953538895
iteration 10, loss = 0.031083861365914345
iteration 11, loss = 0.02490069344639778
iteration 12, loss = 0.027008164674043655
iteration 13, loss = 0.025272570550441742
iteration 14, loss = 0.036298103630542755
iteration 15, loss = 0.03323563560843468
iteration 16, loss = 0.02774701826274395
iteration 17, loss = 0.03314344212412834
iteration 18, loss = 0.036198873072862625
iteration 19, loss = 0.02590823546051979
iteration 20, loss = 0.02688630297780037
iteration 21, loss = 0.02376997284591198
iteration 22, loss = 0.030250243842601776
iteration 23, loss = 0.03533502668142319
iteration 24, loss = 0.025348078459501266
iteration 25, loss = 0.02807212620973587
iteration 26, loss = 0.034894030541181564
iteration 27, loss = 0.024383751675486565
iteration 28, loss = 0.03037080354988575
iteration 29, loss = 0.02839174121618271
iteration 30, loss = 0.025439927354454994
iteration 31, loss = 0.03499044477939606
iteration 32, loss = 0.03167111054062843
iteration 33, loss = 0.02774627134203911
iteration 34, loss = 0.02444988675415516
iteration 35, loss = 0.02512575313448906
iteration 36, loss = 0.032195501029491425
iteration 37, loss = 0.027959782630205154
iteration 38, loss = 0.033443499356508255
iteration 39, loss = 0.02411060594022274
iteration 40, loss = 0.02522103115916252
iteration 41, loss = 0.026732945814728737
iteration 42, loss = 0.025319736450910568
iteration 43, loss = 0.03347589075565338
iteration 44, loss = 0.028923599049448967
iteration 45, loss = 0.022599657997488976
iteration 46, loss = 0.024143395945429802
iteration 47, loss = 0.022256825119256973
iteration 48, loss = 0.02552073821425438
iteration 49, loss = 0.027016039937734604
iteration 50, loss = 0.03129439800977707
iteration 51, loss = 0.024198606610298157
iteration 52, loss = 0.025833161547780037
iteration 53, loss = 0.023980941623449326
iteration 54, loss = 0.025139983743429184
iteration 55, loss = 0.024798844009637833
iteration 56, loss = 0.03280605003237724
iteration 57, loss = 0.023261645808815956
iteration 58, loss = 0.02523234114050865
iteration 59, loss = 0.023942548781633377
iteration 60, loss = 0.02389490231871605
iteration 61, loss = 0.02850761078298092
iteration 62, loss = 0.02340366132557392
iteration 63, loss = 0.026750728487968445
iteration 64, loss = 0.024862512946128845
iteration 65, loss = 0.030951105058193207
iteration 66, loss = 0.024619393050670624
iteration 67, loss = 0.024358754977583885
iteration 68, loss = 0.023457424715161324
iteration 69, loss = 0.022532794624567032
iteration 70, loss = 0.026804879307746887
iteration 71, loss = 0.027017289772629738
iteration 72, loss = 0.03968878835439682
iteration 73, loss = 0.022752493619918823
iteration 74, loss = 0.024919062852859497
iteration 75, loss = 0.02356111630797386
iteration 76, loss = 0.022365273907780647
iteration 77, loss = 0.024273637682199478
iteration 78, loss = 0.023013615980744362
iteration 79, loss = 0.032914817333221436
iteration 80, loss = 0.028632862493395805
iteration 81, loss = 0.021432247012853622
iteration 82, loss = 0.028617748990654945
iteration 83, loss = 0.023113444447517395
iteration 84, loss = 0.023524727672338486
iteration 85, loss = 0.02445516362786293
iteration 86, loss = 0.02505316585302353
iteration 87, loss = 0.027293941006064415
iteration 88, loss = 0.024109357967972755
iteration 89, loss = 0.022773370146751404
iteration 90, loss = 0.021518897265195847
iteration 91, loss = 0.023201989009976387
iteration 92, loss = 0.02301737666130066
iteration 93, loss = 0.026769224554300308
iteration 94, loss = 0.021845931187272072
iteration 95, loss = 0.024367887526750565
iteration 96, loss = 0.025708474218845367
iteration 97, loss = 0.03044901415705681
iteration 98, loss = 0.022906960919499397
iteration 99, loss = 0.022938672453165054
iteration 100, loss = 0.022004665806889534
iteration 101, loss = 0.02258145622909069
iteration 102, loss = 0.021995890885591507
iteration 103, loss = 0.024971192702651024
iteration 104, loss = 0.03200043365359306
iteration 105, loss = 0.020650841295719147
iteration 106, loss = 0.024905331432819366
iteration 107, loss = 0.028600284829735756
iteration 108, loss = 0.023247595876455307
iteration 109, loss = 0.0216913353651762
iteration 110, loss = 0.021480752155184746
iteration 111, loss = 0.028288478031754494
iteration 112, loss = 0.022133778780698776
iteration 113, loss = 0.022443188354372978
iteration 114, loss = 0.023839907720685005
iteration 115, loss = 0.024067729711532593
iteration 116, loss = 0.02465207688510418
iteration 117, loss = 0.021065205335617065
iteration 118, loss = 0.023987701162695885
iteration 119, loss = 0.021580874919891357
iteration 120, loss = 0.02138986811041832
iteration 121, loss = 0.021618731319904327
iteration 122, loss = 0.02010936848819256
iteration 123, loss = 0.021320989355444908
iteration 124, loss = 0.02147902175784111
iteration 125, loss = 0.021222105249762535
iteration 126, loss = 0.02524677850306034
iteration 127, loss = 0.027344776317477226
iteration 128, loss = 0.0211867094039917
iteration 129, loss = 0.02972375974059105
iteration 130, loss = 0.021448029205203056
iteration 131, loss = 0.020039774477481842
iteration 132, loss = 0.02207663655281067
iteration 133, loss = 0.024929285049438477
iteration 134, loss = 0.038843732327222824
iteration 135, loss = 0.022261107340455055
iteration 136, loss = 0.025389011949300766
iteration 137, loss = 0.034452248364686966
iteration 138, loss = 0.02142363041639328
iteration 139, loss = 0.02024412713944912
iteration 140, loss = 0.02330920472741127
iteration 141, loss = 0.024119796231389046
iteration 142, loss = 0.02502916194498539
iteration 143, loss = 0.021213451400399208
iteration 144, loss = 0.028578612953424454
iteration 145, loss = 0.0300801582634449
iteration 146, loss = 0.023701811209321022
iteration 147, loss = 0.02077925018966198
iteration 148, loss = 0.0203892532736063
iteration 149, loss = 0.01947753131389618
iteration 150, loss = 0.022892523556947708
iteration 151, loss = 0.03000178374350071
iteration 152, loss = 0.020325347781181335
iteration 153, loss = 0.02482260763645172
iteration 154, loss = 0.02196478098630905
iteration 155, loss = 0.021275440230965614
iteration 156, loss = 0.019683411344885826
iteration 157, loss = 0.02519816718995571
iteration 158, loss = 0.027076540514826775
iteration 159, loss = 0.028176983818411827
iteration 160, loss = 0.02195640094578266
iteration 161, loss = 0.020883318036794662
iteration 162, loss = 0.026589108631014824
iteration 163, loss = 0.02238694205880165
iteration 164, loss = 0.02462157793343067
iteration 165, loss = 0.022263672202825546
iteration 166, loss = 0.03654274344444275
iteration 167, loss = 0.021806199103593826
iteration 168, loss = 0.019479792565107346
iteration 169, loss = 0.02293056808412075
iteration 170, loss = 0.027244696393609047
iteration 171, loss = 0.01864912360906601
iteration 172, loss = 0.022034024819731712
iteration 173, loss = 0.0204832311719656
iteration 174, loss = 0.019257506355643272
iteration 175, loss = 0.021774666383862495
iteration 176, loss = 0.021001657471060753
iteration 177, loss = 0.025704119354486465
iteration 178, loss = 0.02019011229276657
iteration 179, loss = 0.01877240091562271
iteration 180, loss = 0.02385668456554413
iteration 181, loss = 0.025821831077337265
iteration 182, loss = 0.0218178853392601
iteration 183, loss = 0.020274892449378967
iteration 184, loss = 0.021453164517879486
iteration 185, loss = 0.02031000144779682
iteration 186, loss = 0.022441381588578224
iteration 187, loss = 0.02028651535511017
iteration 188, loss = 0.021752074360847473
iteration 189, loss = 0.020138710737228394
iteration 190, loss = 0.027924368157982826
iteration 191, loss = 0.021001329645514488
iteration 192, loss = 0.022611523047089577
iteration 193, loss = 0.019104180857539177
iteration 194, loss = 0.022685514762997627
iteration 195, loss = 0.017983658239245415
iteration 196, loss = 0.018409350886940956
iteration 197, loss = 0.0235186368227005
iteration 198, loss = 0.020484859123826027
iteration 199, loss = 0.02087298035621643
iteration 200, loss = 0.01869361102581024
iteration 201, loss = 0.01822967827320099
iteration 202, loss = 0.019878502935171127
iteration 203, loss = 0.020136520266532898
iteration 204, loss = 0.01982877589762211
iteration 205, loss = 0.018889227882027626
iteration 206, loss = 0.018528703600168228
iteration 207, loss = 0.03311474621295929
iteration 208, loss = 0.01998988166451454
iteration 209, loss = 0.02345043420791626
iteration 210, loss = 0.01862405240535736
iteration 211, loss = 0.0193175058811903
iteration 212, loss = 0.020933546125888824
iteration 213, loss = 0.01944824680685997
iteration 214, loss = 0.02948514185845852
iteration 215, loss = 0.020013432949781418
iteration 216, loss = 0.021297579631209373
iteration 217, loss = 0.017726195976138115
iteration 218, loss = 0.024959776550531387
iteration 219, loss = 0.01970250904560089
iteration 220, loss = 0.019839255139231682
iteration 221, loss = 0.018664779141545296
iteration 222, loss = 0.019081806764006615
iteration 223, loss = 0.018452007323503494
iteration 224, loss = 0.019397897645831108
iteration 225, loss = 0.02095281332731247
iteration 226, loss = 0.019787093624472618
iteration 227, loss = 0.02141169272363186
iteration 228, loss = 0.021563446149230003
iteration 229, loss = 0.034444764256477356
iteration 230, loss = 0.01973797008395195
iteration 231, loss = 0.018381040543317795
iteration 232, loss = 0.019291097298264503
iteration 233, loss = 0.027080241590738297
iteration 234, loss = 0.018421923741698265
iteration 235, loss = 0.01859264262020588
iteration 236, loss = 0.02476988174021244
iteration 237, loss = 0.019476372748613358
iteration 238, loss = 0.020423181354999542
iteration 239, loss = 0.021116988733410835
iteration 240, loss = 0.03156202659010887
iteration 241, loss = 0.028055092319846153
iteration 242, loss = 0.018936948850750923
iteration 243, loss = 0.01979050412774086
iteration 244, loss = 0.018741482868790627
iteration 245, loss = 0.01834501139819622
iteration 246, loss = 0.024836508557200432
iteration 247, loss = 0.020504025742411613
iteration 248, loss = 0.021218376234173775
iteration 249, loss = 0.01803363300859928
iteration 250, loss = 0.019283626228570938
iteration 251, loss = 0.026563437655568123
iteration 252, loss = 0.018662109971046448
iteration 253, loss = 0.02119367942214012
iteration 254, loss = 0.016580024734139442
iteration 255, loss = 0.02255755104124546
iteration 256, loss = 0.026539377868175507
iteration 257, loss = 0.02003169246017933
iteration 258, loss = 0.020562147721648216
iteration 259, loss = 0.019324250519275665
iteration 260, loss = 0.0171078871935606
iteration 261, loss = 0.01853666827082634
iteration 262, loss = 0.024072760716080666
iteration 263, loss = 0.024294577538967133
iteration 264, loss = 0.025149449706077576
iteration 265, loss = 0.020322194322943687
iteration 266, loss = 0.02516315132379532
iteration 267, loss = 0.021042054519057274
iteration 268, loss = 0.018817631527781487
iteration 269, loss = 0.019165456295013428
iteration 270, loss = 0.023802291601896286
iteration 271, loss = 0.01890629343688488
iteration 272, loss = 0.022600160911679268
iteration 273, loss = 0.0196545273065567
iteration 274, loss = 0.0227351151406765
iteration 275, loss = 0.02411031909286976
iteration 276, loss = 0.023649398237466812
iteration 277, loss = 0.018037470057606697
iteration 278, loss = 0.017461083829402924
iteration 279, loss = 0.01774705946445465
iteration 280, loss = 0.021295011043548584
iteration 281, loss = 0.01784580573439598
iteration 282, loss = 0.01835424266755581
iteration 283, loss = 0.018069956451654434
iteration 284, loss = 0.01886254921555519
iteration 285, loss = 0.018583357334136963
iteration 286, loss = 0.018937159329652786
iteration 287, loss = 0.019185911864042282
iteration 288, loss = 0.0200959462672472
iteration 289, loss = 0.023280981928110123
iteration 290, loss = 0.020799832418560982
iteration 291, loss = 0.024881187826395035
iteration 292, loss = 0.020128127187490463
iteration 293, loss = 0.018927333876490593
iteration 294, loss = 0.018107539042830467
iteration 295, loss = 0.018176063895225525
iteration 296, loss = 0.016673939302563667
iteration 297, loss = 0.027285365387797356
iteration 298, loss = 0.01737087033689022
iteration 299, loss = 0.01651644892990589
iteration 300, loss = 0.017690904438495636
iteration 1, loss = 0.017101487144827843
iteration 2, loss = 0.01645768992602825
iteration 3, loss = 0.016856586560606956
iteration 4, loss = 0.01719570718705654
iteration 5, loss = 0.024549119174480438
iteration 6, loss = 0.024742336943745613
iteration 7, loss = 0.02071797288954258
iteration 8, loss = 0.017532652243971825
iteration 9, loss = 0.018463214859366417
iteration 10, loss = 0.022637533023953438
iteration 11, loss = 0.021100366488099098
iteration 12, loss = 0.02194966934621334
iteration 13, loss = 0.01634383387863636
iteration 14, loss = 0.01769649051129818
iteration 15, loss = 0.017610227689146996
iteration 16, loss = 0.01693400740623474
iteration 17, loss = 0.015654176473617554
iteration 18, loss = 0.0185902938246727
iteration 19, loss = 0.016830924898386
iteration 20, loss = 0.01561893243342638
iteration 21, loss = 0.017947416752576828
iteration 22, loss = 0.017063498497009277
iteration 23, loss = 0.01872986927628517
iteration 24, loss = 0.016765527427196503
iteration 25, loss = 0.0224972702562809
iteration 26, loss = 0.017169957980513573
iteration 27, loss = 0.01854013092815876
iteration 28, loss = 0.016665762290358543
iteration 29, loss = 0.018773870542645454
iteration 30, loss = 0.015646222978830338
iteration 31, loss = 0.01735568419098854
iteration 32, loss = 0.016867103055119514
iteration 33, loss = 0.022471191361546516
iteration 34, loss = 0.016461262479424477
iteration 35, loss = 0.01616065762937069
iteration 36, loss = 0.016625355929136276
iteration 37, loss = 0.017591530457139015
iteration 38, loss = 0.01599784940481186
iteration 39, loss = 0.016836941242218018
iteration 40, loss = 0.017652034759521484
iteration 41, loss = 0.02034481056034565
iteration 42, loss = 0.01914350315928459
iteration 43, loss = 0.015300702303647995
iteration 44, loss = 0.020227927714586258
iteration 45, loss = 0.016004089266061783
iteration 46, loss = 0.016895413398742676
iteration 47, loss = 0.018611378967761993
iteration 48, loss = 0.01619883067905903
iteration 49, loss = 0.015658846125006676
iteration 50, loss = 0.019925104454159737
iteration 51, loss = 0.017831796780228615
iteration 52, loss = 0.02269982174038887
iteration 53, loss = 0.017255134880542755
iteration 54, loss = 0.016840290278196335
iteration 55, loss = 0.019790930673480034
iteration 56, loss = 0.021376220509409904
iteration 57, loss = 0.01636539213359356
iteration 58, loss = 0.015677550807595253
iteration 59, loss = 0.01615888997912407
iteration 60, loss = 0.01673724316060543
iteration 61, loss = 0.022209595888853073
iteration 62, loss = 0.017992790788412094
iteration 63, loss = 0.01571466773748398
iteration 64, loss = 0.034799057990312576
iteration 65, loss = 0.016822021454572678
iteration 66, loss = 0.01688217744231224
iteration 67, loss = 0.01749747432768345
iteration 68, loss = 0.017317725345492363
iteration 69, loss = 0.015363524667918682
iteration 70, loss = 0.014991824515163898
iteration 71, loss = 0.015351471491158009
iteration 72, loss = 0.015833109617233276
iteration 73, loss = 0.018792662769556046
iteration 74, loss = 0.01764359138906002
iteration 75, loss = 0.015222357586026192
iteration 76, loss = 0.01600835844874382
iteration 77, loss = 0.016202373430132866
iteration 78, loss = 0.018907735124230385
iteration 79, loss = 0.015441466122865677
iteration 80, loss = 0.02028433233499527
iteration 81, loss = 0.02034394070506096
iteration 82, loss = 0.017414124682545662
iteration 83, loss = 0.019043995067477226
iteration 84, loss = 0.015862299129366875
iteration 85, loss = 0.014926391653716564
iteration 86, loss = 0.02398190088570118
iteration 87, loss = 0.016559651121497154
iteration 88, loss = 0.015282697975635529
iteration 89, loss = 0.01593521423637867
iteration 90, loss = 0.01581609807908535
iteration 91, loss = 0.014343155547976494
iteration 92, loss = 0.015202668495476246
iteration 93, loss = 0.01572328433394432
iteration 94, loss = 0.016603877767920494
iteration 95, loss = 0.017243482172489166
iteration 96, loss = 0.014730386435985565
iteration 97, loss = 0.015002607367932796
iteration 98, loss = 0.01515145692974329
iteration 99, loss = 0.016858471557497978
iteration 100, loss = 0.02070647105574608
iteration 101, loss = 0.015263702720403671
iteration 102, loss = 0.027268024161458015
iteration 103, loss = 0.020731443539261818
iteration 104, loss = 0.020442502573132515
iteration 105, loss = 0.016082150861620903
iteration 106, loss = 0.01549726165831089
iteration 107, loss = 0.014607060700654984
iteration 108, loss = 0.015512419864535332
iteration 109, loss = 0.018371785059571266
iteration 110, loss = 0.024511869996786118
iteration 111, loss = 0.014948014169931412
iteration 112, loss = 0.01745613105595112
iteration 113, loss = 0.015025271102786064
iteration 114, loss = 0.014057011343538761
iteration 115, loss = 0.014566262252628803
iteration 116, loss = 0.019083436578512192
iteration 117, loss = 0.021832847967743874
iteration 118, loss = 0.01477668434381485
iteration 119, loss = 0.015573839657008648
iteration 120, loss = 0.021960563957691193
iteration 121, loss = 0.018162548542022705
iteration 122, loss = 0.016933659091591835
iteration 123, loss = 0.016756875440478325
iteration 124, loss = 0.016241831704974174
iteration 125, loss = 0.01628250442445278
iteration 126, loss = 0.017042553052306175
iteration 127, loss = 0.01547679677605629
iteration 128, loss = 0.0144866444170475
iteration 129, loss = 0.014031130820512772
iteration 130, loss = 0.015329846180975437
iteration 131, loss = 0.014612775295972824
iteration 132, loss = 0.013996671885251999
iteration 133, loss = 0.015352372080087662
iteration 134, loss = 0.02162790298461914
iteration 135, loss = 0.019963929429650307
iteration 136, loss = 0.015061683021485806
iteration 137, loss = 0.018549209460616112
iteration 138, loss = 0.016697438433766365
iteration 139, loss = 0.01457059383392334
iteration 140, loss = 0.015478002838790417
iteration 141, loss = 0.018312562257051468
iteration 142, loss = 0.02068355679512024
iteration 143, loss = 0.016050763428211212
iteration 144, loss = 0.014982366934418678
iteration 145, loss = 0.015300728380680084
iteration 146, loss = 0.014112766832113266
iteration 147, loss = 0.015023205429315567
iteration 148, loss = 0.015290828421711922
iteration 149, loss = 0.0194669459015131
iteration 150, loss = 0.014403995126485825
iteration 151, loss = 0.01491988729685545
iteration 152, loss = 0.019352275878190994
iteration 153, loss = 0.015307185240089893
iteration 154, loss = 0.014146831817924976
iteration 155, loss = 0.02280260995030403
iteration 156, loss = 0.01908828131854534
iteration 157, loss = 0.015186005271971226
iteration 158, loss = 0.016681209206581116
iteration 159, loss = 0.013554039411246777
iteration 160, loss = 0.016529547050595284
iteration 161, loss = 0.013767064549028873
iteration 162, loss = 0.014547735452651978
iteration 163, loss = 0.022077875211834908
iteration 164, loss = 0.01637072116136551
iteration 165, loss = 0.014262497425079346
iteration 166, loss = 0.014545752666890621
iteration 167, loss = 0.014266564510762691
iteration 168, loss = 0.014589129947125912
iteration 169, loss = 0.01491129957139492
iteration 170, loss = 0.014478739351034164
iteration 171, loss = 0.015924356877803802
iteration 172, loss = 0.018396969884634018
iteration 173, loss = 0.016512732952833176
iteration 174, loss = 0.016149889677762985
iteration 175, loss = 0.015713952481746674
iteration 176, loss = 0.013440854847431183
iteration 177, loss = 0.013735405169427395
iteration 178, loss = 0.013580912724137306
iteration 179, loss = 0.013960412703454494
iteration 180, loss = 0.012874512933194637
iteration 181, loss = 0.018785811960697174
iteration 182, loss = 0.014093318954110146
iteration 183, loss = 0.012651910074055195
iteration 184, loss = 0.013973701745271683
iteration 185, loss = 0.01314403023570776
iteration 186, loss = 0.02237207442522049
iteration 187, loss = 0.01865372434258461
iteration 188, loss = 0.02108757756650448
iteration 189, loss = 0.020813606679439545
iteration 190, loss = 0.017503958195447922
iteration 191, loss = 0.02138352021574974
iteration 192, loss = 0.01910204067826271
iteration 193, loss = 0.014064449816942215
iteration 194, loss = 0.014730136841535568
iteration 195, loss = 0.015026869252324104
iteration 196, loss = 0.014976534992456436
iteration 197, loss = 0.012949955649673939
iteration 198, loss = 0.016566703096032143
iteration 199, loss = 0.013084019534289837
iteration 200, loss = 0.019274605438113213
iteration 201, loss = 0.012661396525800228
iteration 202, loss = 0.013816620223224163
iteration 203, loss = 0.013641357421875
iteration 204, loss = 0.013354064896702766
iteration 205, loss = 0.012539082206785679
iteration 206, loss = 0.018203219398856163
iteration 207, loss = 0.01237261202186346
iteration 208, loss = 0.018848653882741928
iteration 209, loss = 0.014313616789877415
iteration 210, loss = 0.016354739665985107
iteration 211, loss = 0.01290155854076147
iteration 212, loss = 0.016057761386036873
iteration 213, loss = 0.013854392804205418
iteration 214, loss = 0.01620360091328621
iteration 215, loss = 0.015321562066674232
iteration 216, loss = 0.013158826157450676
iteration 217, loss = 0.013580556958913803
iteration 218, loss = 0.014248059131205082
iteration 219, loss = 0.017053376883268356
iteration 220, loss = 0.015920378267765045
iteration 221, loss = 0.013597393408417702
iteration 222, loss = 0.013300136663019657
iteration 223, loss = 0.01354668103158474
iteration 224, loss = 0.014894049614667892
iteration 225, loss = 0.01624249294400215
iteration 226, loss = 0.013101784512400627
iteration 227, loss = 0.01278815045952797
iteration 228, loss = 0.012543603777885437
iteration 229, loss = 0.014172368682920933
iteration 230, loss = 0.021414387971162796
iteration 231, loss = 0.015809670090675354
iteration 232, loss = 0.013203281909227371
iteration 233, loss = 0.020020371302962303
iteration 234, loss = 0.01268415991216898
iteration 235, loss = 0.013998400419950485
iteration 236, loss = 0.01307897549122572
iteration 237, loss = 0.015421818010509014
iteration 238, loss = 0.014490882866084576
iteration 239, loss = 0.017767127603292465
iteration 240, loss = 0.012618167325854301
iteration 241, loss = 0.02057785540819168
iteration 242, loss = 0.014256571419537067
iteration 243, loss = 0.014395692385733128
iteration 244, loss = 0.013048007152974606
iteration 245, loss = 0.013120044022798538
iteration 246, loss = 0.018537482246756554
iteration 247, loss = 0.012621285393834114
iteration 248, loss = 0.013788068667054176
iteration 249, loss = 0.015082847326993942
iteration 250, loss = 0.014350440353155136
iteration 251, loss = 0.012086531147360802
iteration 252, loss = 0.012987801805138588
iteration 253, loss = 0.015044628642499447
iteration 254, loss = 0.015502373687922955
iteration 255, loss = 0.0133218290284276
iteration 256, loss = 0.012484976090490818
iteration 257, loss = 0.015148784965276718
iteration 258, loss = 0.018496589735150337
iteration 259, loss = 0.015455476008355618
iteration 260, loss = 0.013188869692385197
iteration 261, loss = 0.012396672740578651
iteration 262, loss = 0.013000923208892345
iteration 263, loss = 0.015220113098621368
iteration 264, loss = 0.01917891763150692
iteration 265, loss = 0.012240193784236908
iteration 266, loss = 0.015693295747041702
iteration 267, loss = 0.012443002313375473
iteration 268, loss = 0.014750883914530277
iteration 269, loss = 0.012789526022970676
iteration 270, loss = 0.01921394094824791
iteration 271, loss = 0.014333470724523067
iteration 272, loss = 0.012544969096779823
iteration 273, loss = 0.013154060579836369
iteration 274, loss = 0.013027571141719818
iteration 275, loss = 0.012485140934586525
iteration 276, loss = 0.014104370027780533
iteration 277, loss = 0.012195765040814877
iteration 278, loss = 0.01890021376311779
iteration 279, loss = 0.0187821201980114
iteration 280, loss = 0.012914431281387806
iteration 281, loss = 0.012948669493198395
iteration 282, loss = 0.015245501883327961
iteration 283, loss = 0.013080835342407227
iteration 284, loss = 0.013312380760908127
iteration 285, loss = 0.012577039189636707
iteration 286, loss = 0.013263033702969551
iteration 287, loss = 0.012614628300070763
iteration 288, loss = 0.018132204189896584
iteration 289, loss = 0.01294513326138258
iteration 290, loss = 0.01741144433617592
iteration 291, loss = 0.01366792805492878
iteration 292, loss = 0.014394124038517475
iteration 293, loss = 0.015051372349262238
iteration 294, loss = 0.016860730946063995
iteration 295, loss = 0.015560036525130272
iteration 296, loss = 0.017855297774076462
iteration 297, loss = 0.01403411477804184
iteration 298, loss = 0.01860766112804413
iteration 299, loss = 0.015612057410180569
iteration 300, loss = 0.01188519038259983
iteration 1, loss = 0.024257145822048187
iteration 2, loss = 0.011516454629600048
iteration 3, loss = 0.012408111244440079
iteration 4, loss = 0.016414768993854523
iteration 5, loss = 0.012047470547258854
iteration 6, loss = 0.014741449616849422
iteration 7, loss = 0.012145936489105225
iteration 8, loss = 0.013032108545303345
iteration 9, loss = 0.012445024214684963
iteration 10, loss = 0.019220620393753052
iteration 11, loss = 0.01389082707464695
iteration 12, loss = 0.011434609070420265
iteration 13, loss = 0.012684007175266743
iteration 14, loss = 0.013706613332033157
iteration 15, loss = 0.017931196838617325
iteration 16, loss = 0.012054323218762875
iteration 17, loss = 0.012349925935268402
iteration 18, loss = 0.01145769003778696
iteration 19, loss = 0.012486296705901623
iteration 20, loss = 0.01766849495470524
iteration 21, loss = 0.011440618894994259
iteration 22, loss = 0.013292256742715836
iteration 23, loss = 0.013236560858786106
iteration 24, loss = 0.011662310920655727
iteration 25, loss = 0.019183624535799026
iteration 26, loss = 0.013133898377418518
iteration 27, loss = 0.012601555325090885
iteration 28, loss = 0.013593613170087337
iteration 29, loss = 0.01213691383600235
iteration 30, loss = 0.017539452761411667
iteration 31, loss = 0.011587378568947315
iteration 32, loss = 0.012150227092206478
iteration 33, loss = 0.01136181503534317
iteration 34, loss = 0.012137193232774734
iteration 35, loss = 0.013105801306664944
iteration 36, loss = 0.016800887882709503
iteration 37, loss = 0.0165567547082901
iteration 38, loss = 0.011708330363035202
iteration 39, loss = 0.012752100825309753
iteration 40, loss = 0.016026802361011505
iteration 41, loss = 0.013179855421185493
iteration 42, loss = 0.011859563179314137
iteration 43, loss = 0.011936035938560963
iteration 44, loss = 0.011879082769155502
iteration 45, loss = 0.011822710745036602
iteration 46, loss = 0.011589977890253067
iteration 47, loss = 0.01148983370512724
iteration 48, loss = 0.011413007974624634
iteration 49, loss = 0.015106156468391418
iteration 50, loss = 0.015603682957589626
iteration 51, loss = 0.011034523136913776
iteration 52, loss = 0.011467382311820984
iteration 53, loss = 0.012824108824133873
iteration 54, loss = 0.011593992821872234
iteration 55, loss = 0.012295417487621307
iteration 56, loss = 0.011361607350409031
iteration 57, loss = 0.011702308431267738
iteration 58, loss = 0.011814131401479244
iteration 59, loss = 0.011638502590358257
iteration 60, loss = 0.013795106671750546
iteration 61, loss = 0.011309545487165451
iteration 62, loss = 0.013695375062525272
iteration 63, loss = 0.01405537873506546
iteration 64, loss = 0.01720273867249489
iteration 65, loss = 0.011576350778341293
iteration 66, loss = 0.011843736283481121
iteration 67, loss = 0.01127683836966753
iteration 68, loss = 0.012532848864793777
iteration 69, loss = 0.011739356443285942
iteration 70, loss = 0.011680164374411106
iteration 71, loss = 0.011115321889519691
iteration 72, loss = 0.016464510932564735
iteration 73, loss = 0.012846183031797409
iteration 74, loss = 0.012504834681749344
iteration 75, loss = 0.011605040170252323
iteration 76, loss = 0.011952418833971024
iteration 77, loss = 0.01457649189978838
iteration 78, loss = 0.011422501876950264
iteration 79, loss = 0.011261862702667713
iteration 80, loss = 0.017513062804937363
iteration 81, loss = 0.011061638593673706
iteration 82, loss = 0.010296376422047615
iteration 83, loss = 0.010760059580206871
iteration 84, loss = 0.015140131115913391
iteration 85, loss = 0.012601046822965145
iteration 86, loss = 0.013462887145578861
iteration 87, loss = 0.011667297221720219
iteration 88, loss = 0.011505631729960442
iteration 89, loss = 0.011178399436175823
iteration 90, loss = 0.011408006772398949
iteration 91, loss = 0.011178990826010704
iteration 92, loss = 0.011073101311922073
iteration 93, loss = 0.010971948504447937
iteration 94, loss = 0.013828614726662636
iteration 95, loss = 0.010755155235528946
iteration 96, loss = 0.011095534078776836
iteration 97, loss = 0.01137368381023407
iteration 98, loss = 0.013367592357099056
iteration 99, loss = 0.011757271364331245
iteration 100, loss = 0.020642749965190887
iteration 101, loss = 0.010661710985004902
iteration 102, loss = 0.014759812504053116
iteration 103, loss = 0.014209615997970104
iteration 104, loss = 0.015271371230483055
iteration 105, loss = 0.011505775153636932
iteration 106, loss = 0.011305037885904312
iteration 107, loss = 0.011851183138787746
iteration 108, loss = 0.017455032095313072
iteration 109, loss = 0.012339088134467602
iteration 110, loss = 0.015327190980315208
iteration 111, loss = 0.010838782414793968
iteration 112, loss = 0.011378860101103783
iteration 113, loss = 0.010594015941023827
iteration 114, loss = 0.010491210967302322
iteration 115, loss = 0.01098916307091713
iteration 116, loss = 0.01241086795926094
iteration 117, loss = 0.011534454300999641
iteration 118, loss = 0.010639246553182602
iteration 119, loss = 0.011191064491868019
iteration 120, loss = 0.011740285903215408
iteration 121, loss = 0.011542359367012978
iteration 122, loss = 0.011586122214794159
iteration 123, loss = 0.011065407656133175
iteration 124, loss = 0.013252563774585724
iteration 125, loss = 0.013503661379218102
iteration 126, loss = 0.010786132887005806
iteration 127, loss = 0.011034791357815266
iteration 128, loss = 0.0164725873619318
iteration 129, loss = 0.011658343486487865
iteration 130, loss = 0.011117836460471153
iteration 131, loss = 0.012555073946714401
iteration 132, loss = 0.012881020084023476
iteration 133, loss = 0.010515700094401836
iteration 134, loss = 0.011657322756946087
iteration 135, loss = 0.011158343404531479
iteration 136, loss = 0.011539408937096596
iteration 137, loss = 0.010541795752942562
iteration 138, loss = 0.016339557245373726
iteration 139, loss = 0.012696911580860615
iteration 140, loss = 0.011058812029659748
iteration 141, loss = 0.009889398701488972
iteration 142, loss = 0.01540240366011858
iteration 143, loss = 0.010522527620196342
iteration 144, loss = 0.010775547474622726
iteration 145, loss = 0.01660943776369095
iteration 146, loss = 0.01185179315507412
iteration 147, loss = 0.012430310249328613
iteration 148, loss = 0.011506682261824608
iteration 149, loss = 0.011529270559549332
iteration 150, loss = 0.012453887611627579
iteration 151, loss = 0.011124609969556332
iteration 152, loss = 0.009772920981049538
iteration 153, loss = 0.011458238586783409
iteration 154, loss = 0.009943289682269096
iteration 155, loss = 0.010881146416068077
iteration 156, loss = 0.011353765614330769
iteration 157, loss = 0.0102189676836133
iteration 158, loss = 0.010649914853274822
iteration 159, loss = 0.01176372915506363
iteration 160, loss = 0.010305958800017834
iteration 161, loss = 0.01180678978562355
iteration 162, loss = 0.020962297916412354
iteration 163, loss = 0.010743953287601471
iteration 164, loss = 0.019872959703207016
iteration 165, loss = 0.011266395449638367
iteration 166, loss = 0.012968523427844048
iteration 167, loss = 0.011386598460376263
iteration 168, loss = 0.011605046689510345
iteration 169, loss = 0.010315961204469204
iteration 170, loss = 0.010744292289018631
iteration 171, loss = 0.010941335931420326
iteration 172, loss = 0.011220935732126236
iteration 173, loss = 0.010218014940619469
iteration 174, loss = 0.014655840583145618
iteration 175, loss = 0.01649394817650318
iteration 176, loss = 0.00938624981790781
iteration 177, loss = 0.01115558110177517
iteration 178, loss = 0.014018011279404163
iteration 179, loss = 0.01386854238808155
iteration 180, loss = 0.009713117964565754
iteration 181, loss = 0.010574093088507652
iteration 182, loss = 0.01081116870045662
iteration 183, loss = 0.012690211646258831
iteration 184, loss = 0.01134383026510477
iteration 185, loss = 0.015890341252088547
iteration 186, loss = 0.01113367173820734
iteration 187, loss = 0.013063166290521622
iteration 188, loss = 0.010367101058363914
iteration 189, loss = 0.010218679904937744
iteration 190, loss = 0.011424179188907146
iteration 191, loss = 0.01604318618774414
iteration 192, loss = 0.012536448426544666
iteration 193, loss = 0.010410808026790619
iteration 194, loss = 0.009967835620045662
iteration 195, loss = 0.013404129073023796
iteration 196, loss = 0.015135289169847965
iteration 197, loss = 0.010585709474980831
iteration 198, loss = 0.010452156886458397
iteration 199, loss = 0.013361520133912563
iteration 200, loss = 0.00945806223899126
iteration 201, loss = 0.010382414795458317
iteration 202, loss = 0.009972983971238136
iteration 203, loss = 0.009737877175211906
iteration 204, loss = 0.010872571729123592
iteration 205, loss = 0.014010048471391201
iteration 206, loss = 0.009864035062491894
iteration 207, loss = 0.010505061596632004
iteration 208, loss = 0.011999037116765976
iteration 209, loss = 0.015601449646055698
iteration 210, loss = 0.009965716861188412
iteration 211, loss = 0.010797062888741493
iteration 212, loss = 0.010234952904284
iteration 213, loss = 0.009757930412888527
iteration 214, loss = 0.010900735855102539
iteration 215, loss = 0.010587387718260288
iteration 216, loss = 0.00989342201501131
iteration 217, loss = 0.011339999735355377
iteration 218, loss = 0.012802745215594769
iteration 219, loss = 0.011834198608994484
iteration 220, loss = 0.012281744740903378
iteration 221, loss = 0.009835506789386272
iteration 222, loss = 0.010170678608119488
iteration 223, loss = 0.014436786994338036
iteration 224, loss = 0.010123750194907188
iteration 225, loss = 0.010608588345348835
iteration 226, loss = 0.009625419974327087
iteration 227, loss = 0.010778333991765976
iteration 228, loss = 0.01043067779392004
iteration 229, loss = 0.010722336359322071
iteration 230, loss = 0.01690399833023548
iteration 231, loss = 0.013722778297960758
iteration 232, loss = 0.00988398864865303
iteration 233, loss = 0.009305251762270927
iteration 234, loss = 0.01532742939889431
iteration 235, loss = 0.010076875798404217
iteration 236, loss = 0.010051724500954151
iteration 237, loss = 0.012570858001708984
iteration 238, loss = 0.009524359367787838
iteration 239, loss = 0.011038615368306637
iteration 240, loss = 0.009828059002757072
iteration 241, loss = 0.009466206654906273
iteration 242, loss = 0.010727833956480026
iteration 243, loss = 0.010581498965620995
iteration 244, loss = 0.009511283598840237
iteration 245, loss = 0.010977871716022491
iteration 246, loss = 0.010364142246544361
iteration 247, loss = 0.010576397180557251
iteration 248, loss = 0.010559243150055408
iteration 249, loss = 0.011523890309035778
iteration 250, loss = 0.01181226596236229
iteration 251, loss = 0.014144429937005043
iteration 252, loss = 0.009561480954289436
iteration 253, loss = 0.010498439893126488
iteration 254, loss = 0.01044988352805376
iteration 255, loss = 0.00969547126442194
iteration 256, loss = 0.009951289743185043
iteration 257, loss = 0.010628980584442616
iteration 258, loss = 0.011111639440059662
iteration 259, loss = 0.010915174148976803
iteration 260, loss = 0.01115921325981617
iteration 261, loss = 0.01478742528706789
iteration 262, loss = 0.00982059445232153
iteration 263, loss = 0.009666454046964645
iteration 264, loss = 0.013444251380860806
iteration 265, loss = 0.010354248806834221
iteration 266, loss = 0.012520073913037777
iteration 267, loss = 0.009905119426548481
iteration 268, loss = 0.009904585778713226
iteration 269, loss = 0.010246110148727894
iteration 270, loss = 0.011064775288105011
iteration 271, loss = 0.012658846564590931
iteration 272, loss = 0.009738108143210411
iteration 273, loss = 0.009140506386756897
iteration 274, loss = 0.009981772862374783
iteration 275, loss = 0.01120429765433073
iteration 276, loss = 0.009760165587067604
iteration 277, loss = 0.012767890468239784
iteration 278, loss = 0.009770444594323635
iteration 279, loss = 0.010847998782992363
iteration 280, loss = 0.011053730733692646
iteration 281, loss = 0.012413214892148972
iteration 282, loss = 0.01130908913910389
iteration 283, loss = 0.011328324675559998
iteration 284, loss = 0.013545749709010124
iteration 285, loss = 0.00970001146197319
iteration 286, loss = 0.0106705566868186
iteration 287, loss = 0.00951952300965786
iteration 288, loss = 0.01508577261120081
iteration 289, loss = 0.010026701726019382
iteration 290, loss = 0.010340502485632896
iteration 291, loss = 0.009639965370297432
iteration 292, loss = 0.010120440274477005
iteration 293, loss = 0.00897977314889431
iteration 294, loss = 0.013059956021606922
iteration 295, loss = 0.009017853066325188
iteration 296, loss = 0.009088343009352684
iteration 297, loss = 0.009904561564326286
iteration 298, loss = 0.009442692622542381
iteration 299, loss = 0.008850621990859509
iteration 300, loss = 0.012879534624516964
iteration 1, loss = 0.00873457919806242
iteration 2, loss = 0.009183255955576897
iteration 3, loss = 0.009984297677874565
iteration 4, loss = 0.011299191042780876
iteration 5, loss = 0.009170225821435452
iteration 6, loss = 0.010897952131927013
iteration 7, loss = 0.00892642792314291
iteration 8, loss = 0.010471655987203121
iteration 9, loss = 0.009217913262546062
iteration 10, loss = 0.009362875483930111
iteration 11, loss = 0.00950583815574646
iteration 12, loss = 0.011619043536484241
iteration 13, loss = 0.009022882208228111
iteration 14, loss = 0.00964673887938261
iteration 15, loss = 0.011685740202665329
iteration 16, loss = 0.016277549788355827
iteration 17, loss = 0.008955691941082478
iteration 18, loss = 0.009746314957737923
iteration 19, loss = 0.009339543990790844
iteration 20, loss = 0.00922267697751522
iteration 21, loss = 0.013677410781383514
iteration 22, loss = 0.009072897024452686
iteration 23, loss = 0.010424844920635223
iteration 24, loss = 0.009978809393942356
iteration 25, loss = 0.014511444605886936
iteration 26, loss = 0.013656010851264
iteration 27, loss = 0.01081904862076044
iteration 28, loss = 0.014799291267991066
iteration 29, loss = 0.009613262489438057
iteration 30, loss = 0.009339181706309319
iteration 31, loss = 0.00982991699129343
iteration 32, loss = 0.015982598066329956
iteration 33, loss = 0.008924183435738087
iteration 34, loss = 0.010354080237448215
iteration 35, loss = 0.010889219120144844
iteration 36, loss = 0.008858992718160152
iteration 37, loss = 0.009486448019742966
iteration 38, loss = 0.01041871216148138
iteration 39, loss = 0.011161216534674168
iteration 40, loss = 0.012285839766263962
iteration 41, loss = 0.00974518433213234
iteration 42, loss = 0.009593375027179718
iteration 43, loss = 0.009324459359049797
iteration 44, loss = 0.009885547682642937
iteration 45, loss = 0.008437898010015488
iteration 46, loss = 0.009811698459088802
iteration 47, loss = 0.015899470075964928
iteration 48, loss = 0.01065815519541502
iteration 49, loss = 0.012397393584251404
iteration 50, loss = 0.0088267857208848
iteration 51, loss = 0.008541879244148731
iteration 52, loss = 0.00880436971783638
iteration 53, loss = 0.00857161357998848
iteration 54, loss = 0.013037788681685925
iteration 55, loss = 0.00847269780933857
iteration 56, loss = 0.00940028391778469
iteration 57, loss = 0.008906044997274876
iteration 58, loss = 0.00907665491104126
iteration 59, loss = 0.013288157992064953
iteration 60, loss = 0.008937792852520943
iteration 61, loss = 0.009621955454349518
iteration 62, loss = 0.010287544690072536
iteration 63, loss = 0.00836448185145855
iteration 64, loss = 0.008648920804262161
iteration 65, loss = 0.00926842913031578
iteration 66, loss = 0.0097362594678998
iteration 67, loss = 0.0095418281853199
iteration 68, loss = 0.00902748666703701
iteration 69, loss = 0.010307149030268192
iteration 70, loss = 0.01108414214104414
iteration 71, loss = 0.013341964222490788
iteration 72, loss = 0.008811015635728836
iteration 73, loss = 0.008811320178210735
iteration 74, loss = 0.00898517295718193
iteration 75, loss = 0.009203275665640831
iteration 76, loss = 0.00812065601348877
iteration 77, loss = 0.00917855929583311
iteration 78, loss = 0.00849526934325695
iteration 79, loss = 0.009646234102547169
iteration 80, loss = 0.00873663928359747
iteration 81, loss = 0.01052462961524725
iteration 82, loss = 0.009722433984279633
iteration 83, loss = 0.009042572230100632
iteration 84, loss = 0.008602937683463097
iteration 85, loss = 0.008652166463434696
iteration 86, loss = 0.009320636279881
iteration 87, loss = 0.008696110919117928
iteration 88, loss = 0.00838183518499136
iteration 89, loss = 0.011023123748600483
iteration 90, loss = 0.010339370928704739
iteration 91, loss = 0.008708985522389412
iteration 92, loss = 0.008827508427202702
iteration 93, loss = 0.010143453255295753
iteration 94, loss = 0.008551232516765594
iteration 95, loss = 0.01197639387100935
iteration 96, loss = 0.008193343877792358
iteration 97, loss = 0.008455843664705753
iteration 98, loss = 0.008753233589231968
iteration 99, loss = 0.011235407553613186
iteration 100, loss = 0.00888840015977621
iteration 101, loss = 0.011950785294175148
iteration 102, loss = 0.00805650558322668
iteration 103, loss = 0.008928043767809868
iteration 104, loss = 0.008511257357895374
iteration 105, loss = 0.009001720696687698
iteration 106, loss = 0.008498234674334526
iteration 107, loss = 0.008767019957304
iteration 108, loss = 0.010321253910660744
iteration 109, loss = 0.009379889816045761
iteration 110, loss = 0.008098270744085312
iteration 111, loss = 0.008643664419651031
iteration 112, loss = 0.009593768045306206
iteration 113, loss = 0.008653253316879272
iteration 114, loss = 0.008985163643956184
iteration 115, loss = 0.008405697531998158
iteration 116, loss = 0.00872111413627863
iteration 117, loss = 0.010282501578330994
iteration 118, loss = 0.008601208217442036
iteration 119, loss = 0.009345414116978645
iteration 120, loss = 0.009866101667284966
iteration 121, loss = 0.011930685490369797
iteration 122, loss = 0.007913589477539062
iteration 123, loss = 0.0088646886870265
iteration 124, loss = 0.008567734621465206
iteration 125, loss = 0.010130176320672035
iteration 126, loss = 0.00935658160597086
iteration 127, loss = 0.009025312960147858
iteration 128, loss = 0.008438852615654469
iteration 129, loss = 0.011905859224498272
iteration 130, loss = 0.007675386033952236
iteration 131, loss = 0.008242718875408173
iteration 132, loss = 0.009060505777597427
iteration 133, loss = 0.008712499402463436
iteration 134, loss = 0.007931945845484734
iteration 135, loss = 0.00851106084883213
iteration 136, loss = 0.012097637169063091
iteration 137, loss = 0.011510640382766724
iteration 138, loss = 0.008297760970890522
iteration 139, loss = 0.009388822130858898
iteration 140, loss = 0.008734064176678658
iteration 141, loss = 0.008443859405815601
iteration 142, loss = 0.009562135674059391
iteration 143, loss = 0.008886436931788921
iteration 144, loss = 0.009494575671851635
iteration 145, loss = 0.010049399919807911
iteration 146, loss = 0.00773587916046381
iteration 147, loss = 0.009571273811161518
iteration 148, loss = 0.008695529773831367
iteration 149, loss = 0.009017810225486755
iteration 150, loss = 0.009075111709535122
iteration 151, loss = 0.00839965045452118
iteration 152, loss = 0.011651169508695602
iteration 153, loss = 0.011411551386117935
iteration 154, loss = 0.009046075865626335
iteration 155, loss = 0.009771274402737617
iteration 156, loss = 0.009014911949634552
iteration 157, loss = 0.010837935842573643
iteration 158, loss = 0.011196623556315899
iteration 159, loss = 0.008758601732552052
iteration 160, loss = 0.015305492095649242
iteration 161, loss = 0.007991492748260498
iteration 162, loss = 0.007661853451281786
iteration 163, loss = 0.008633296005427837
iteration 164, loss = 0.012339499779045582
iteration 165, loss = 0.007725363597273827
iteration 166, loss = 0.007830237038433552
iteration 167, loss = 0.00948341190814972
iteration 168, loss = 0.009498948231339455
iteration 169, loss = 0.009021427482366562
iteration 170, loss = 0.00840376690030098
iteration 171, loss = 0.00809935387223959
iteration 172, loss = 0.00835348665714264
iteration 173, loss = 0.009254716336727142
iteration 174, loss = 0.009489964693784714
iteration 175, loss = 0.009682297706604004
iteration 176, loss = 0.01010067481547594
iteration 177, loss = 0.009300138801336288
iteration 178, loss = 0.010945780202746391
iteration 179, loss = 0.008230011910200119
iteration 180, loss = 0.008148674853146076
iteration 181, loss = 0.008521640673279762
iteration 182, loss = 0.01175827719271183
iteration 183, loss = 0.007883246056735516
iteration 184, loss = 0.010001427493989468
iteration 185, loss = 0.009075003676116467
iteration 186, loss = 0.008569696918129921
iteration 187, loss = 0.008880366571247578
iteration 188, loss = 0.008126375265419483
iteration 189, loss = 0.00793280079960823
iteration 190, loss = 0.007745683658868074
iteration 191, loss = 0.007945866324007511
iteration 192, loss = 0.015751730650663376
iteration 193, loss = 0.011749903671443462
iteration 194, loss = 0.008988434448838234
iteration 195, loss = 0.008625814691185951
iteration 196, loss = 0.009204447269439697
iteration 197, loss = 0.009803296998143196
iteration 198, loss = 0.007809866219758987
iteration 199, loss = 0.007928585633635521
iteration 200, loss = 0.008028977550566196
iteration 201, loss = 0.010299577377736568
iteration 202, loss = 0.007798284292221069
iteration 203, loss = 0.007968422956764698
iteration 204, loss = 0.009689820930361748
iteration 205, loss = 0.008560970425605774
iteration 206, loss = 0.00748818414285779
iteration 207, loss = 0.007991963066160679
iteration 208, loss = 0.010434522293508053
iteration 209, loss = 0.007441544905304909
iteration 210, loss = 0.007547046057879925
iteration 211, loss = 0.012846851721405983
iteration 212, loss = 0.00776925403624773
iteration 213, loss = 0.007510613650083542
iteration 214, loss = 0.008467847481369972
iteration 215, loss = 0.008145531639456749
iteration 216, loss = 0.009088723920285702
iteration 217, loss = 0.009054704569280148
iteration 218, loss = 0.00782137643545866
iteration 219, loss = 0.011481105349957943
iteration 220, loss = 0.008024328388273716
iteration 221, loss = 0.008674447424709797
iteration 222, loss = 0.007836056873202324
iteration 223, loss = 0.008010179735720158
iteration 224, loss = 0.009480697102844715
iteration 225, loss = 0.007692277431488037
iteration 226, loss = 0.009319484233856201
iteration 227, loss = 0.011002026498317719
iteration 228, loss = 0.007036934606730938
iteration 229, loss = 0.0079942736774683
iteration 230, loss = 0.007739477325230837
iteration 231, loss = 0.0074423011392354965
iteration 232, loss = 0.009077024646103382
iteration 233, loss = 0.007317169103771448
iteration 234, loss = 0.007953227497637272
iteration 235, loss = 0.007492667064070702
iteration 236, loss = 0.008135293610394001
iteration 237, loss = 0.00960269384086132
iteration 238, loss = 0.007983622141182423
iteration 239, loss = 0.008079891093075275
iteration 240, loss = 0.009460857138037682
iteration 241, loss = 0.011976293288171291
iteration 242, loss = 0.007975444197654724
iteration 243, loss = 0.007885201834142208
iteration 244, loss = 0.010962866246700287
iteration 245, loss = 0.008669394999742508
iteration 246, loss = 0.007582531776279211
iteration 247, loss = 0.008242093957960606
iteration 248, loss = 0.007880528457462788
iteration 249, loss = 0.007683700881898403
iteration 250, loss = 0.012508111074566841
iteration 251, loss = 0.007624588906764984
iteration 252, loss = 0.010841292329132557
iteration 253, loss = 0.007365952245891094
iteration 254, loss = 0.008480630815029144
iteration 255, loss = 0.0071988520212471485
iteration 256, loss = 0.008333446457982063
iteration 257, loss = 0.0074236043728888035
iteration 258, loss = 0.008762882091104984
iteration 259, loss = 0.010266135446727276
iteration 260, loss = 0.00765365082770586
iteration 261, loss = 0.007974021136760712
iteration 262, loss = 0.007496335543692112
iteration 263, loss = 0.007748470641672611
iteration 264, loss = 0.011881672777235508
iteration 265, loss = 0.011333031579852104
iteration 266, loss = 0.008113978430628777
iteration 267, loss = 0.008194411173462868
iteration 268, loss = 0.007212742231786251
iteration 269, loss = 0.008108592592179775
iteration 270, loss = 0.007224743254482746
iteration 271, loss = 0.007616786751896143
iteration 272, loss = 0.008725709281861782
iteration 273, loss = 0.007413551677018404
iteration 274, loss = 0.011454225517809391
iteration 275, loss = 0.007746816612780094
iteration 276, loss = 0.010982431471347809
iteration 277, loss = 0.011449537239968777
iteration 278, loss = 0.007869832217693329
iteration 279, loss = 0.007272559683769941
iteration 280, loss = 0.007756431121379137
iteration 281, loss = 0.007657951209694147
iteration 282, loss = 0.007387653924524784
iteration 283, loss = 0.0073681422509253025
iteration 284, loss = 0.007634978275746107
iteration 285, loss = 0.007262723986059427
iteration 286, loss = 0.008204692043364048
iteration 287, loss = 0.008085678331553936
iteration 288, loss = 0.007140390574932098
iteration 289, loss = 0.007876500487327576
iteration 290, loss = 0.008342896588146687
iteration 291, loss = 0.011430501937866211
iteration 292, loss = 0.007169158197939396
iteration 293, loss = 0.010627040639519691
iteration 294, loss = 0.010280748829245567
iteration 295, loss = 0.007506712339818478
iteration 296, loss = 0.006785255391150713
iteration 297, loss = 0.010305401869118214
iteration 298, loss = 0.007298549637198448
iteration 299, loss = 0.009814591147005558
iteration 300, loss = 0.008333989419043064
iteration 1, loss = 0.007589544635266066
iteration 2, loss = 0.00756803760305047
iteration 3, loss = 0.007459672633558512
iteration 4, loss = 0.007493064738810062
iteration 5, loss = 0.007787720765918493
iteration 6, loss = 0.006748360116034746
iteration 7, loss = 0.008042603731155396
iteration 8, loss = 0.007568643894046545
iteration 9, loss = 0.007659617345780134
iteration 10, loss = 0.007752255070954561
iteration 11, loss = 0.007323566358536482
iteration 12, loss = 0.010866954922676086
iteration 13, loss = 0.010025981813669205
iteration 14, loss = 0.007798061240464449
iteration 15, loss = 0.007887385785579681
iteration 16, loss = 0.007317526265978813
iteration 17, loss = 0.00714943977072835
iteration 18, loss = 0.009933250956237316
iteration 19, loss = 0.007475724909454584
iteration 20, loss = 0.010388876311480999
iteration 21, loss = 0.007752928417176008
iteration 22, loss = 0.00712788850069046
iteration 23, loss = 0.007999897003173828
iteration 24, loss = 0.007417390123009682
iteration 25, loss = 0.007946843281388283
iteration 26, loss = 0.0077657937072217464
iteration 27, loss = 0.006436076946556568
iteration 28, loss = 0.007529675494879484
iteration 29, loss = 0.007591222412884235
iteration 30, loss = 0.007240818813443184
iteration 31, loss = 0.009909993968904018
iteration 32, loss = 0.009563557803630829
iteration 33, loss = 0.007375876884907484
iteration 34, loss = 0.008729925379157066
iteration 35, loss = 0.007469367701560259
iteration 36, loss = 0.010560532100498676
iteration 37, loss = 0.014444718137383461
iteration 38, loss = 0.008101653307676315
iteration 39, loss = 0.006536521017551422
iteration 40, loss = 0.007052081637084484
iteration 41, loss = 0.007630938198417425
iteration 42, loss = 0.007064648903906345
iteration 43, loss = 0.007528092712163925
iteration 44, loss = 0.007120819762349129
iteration 45, loss = 0.007074522320181131
iteration 46, loss = 0.007040796801447868
iteration 47, loss = 0.010003751143813133
iteration 48, loss = 0.011377428658306599
iteration 49, loss = 0.007076972164213657
iteration 50, loss = 0.007053465582430363
iteration 51, loss = 0.006817150395363569
iteration 52, loss = 0.007042426150292158
iteration 53, loss = 0.008162958547472954
iteration 54, loss = 0.0067190369591116905
iteration 55, loss = 0.009683401323854923
iteration 56, loss = 0.009691995568573475
iteration 57, loss = 0.006913588382303715
iteration 58, loss = 0.007312230300158262
iteration 59, loss = 0.007247923873364925
iteration 60, loss = 0.009447885677218437
iteration 61, loss = 0.008705479092895985
iteration 62, loss = 0.00850603450089693
iteration 63, loss = 0.008476871997117996
iteration 64, loss = 0.0068903821520507336
iteration 65, loss = 0.006765455938875675
iteration 66, loss = 0.007010028697550297
iteration 67, loss = 0.007375475950539112
iteration 68, loss = 0.006665461231023073
iteration 69, loss = 0.008651120588183403
iteration 70, loss = 0.01077344361692667
iteration 71, loss = 0.006934472359716892
iteration 72, loss = 0.007546070497483015
iteration 73, loss = 0.007546307053416967
iteration 74, loss = 0.00847899541258812
iteration 75, loss = 0.00723061989992857
iteration 76, loss = 0.007301069796085358
iteration 77, loss = 0.007376840338110924
iteration 78, loss = 0.0072648655623197556
iteration 79, loss = 0.007026134058833122
iteration 80, loss = 0.0074912551790475845
iteration 81, loss = 0.0075406101532280445
iteration 82, loss = 0.009805851615965366
iteration 83, loss = 0.007466014940291643
iteration 84, loss = 0.006911982782185078
iteration 85, loss = 0.0069329808466136456
iteration 86, loss = 0.01051434874534607
iteration 87, loss = 0.007705452386289835
iteration 88, loss = 0.006501099560409784
iteration 89, loss = 0.0070235952734947205
iteration 90, loss = 0.007306704763323069
iteration 91, loss = 0.007176746614277363
iteration 92, loss = 0.00724796112626791
iteration 93, loss = 0.007194858510047197
iteration 94, loss = 0.009579910896718502
iteration 95, loss = 0.006906192749738693
iteration 96, loss = 0.008364451117813587
iteration 97, loss = 0.007013200782239437
iteration 98, loss = 0.0073312013410031796
iteration 99, loss = 0.006869717501103878
iteration 100, loss = 0.00732190627604723
iteration 101, loss = 0.006668350659310818
iteration 102, loss = 0.00712402630597353
iteration 103, loss = 0.0076193781569600105
iteration 104, loss = 0.006663835141807795
iteration 105, loss = 0.006636134348809719
iteration 106, loss = 0.0066681429743766785
iteration 107, loss = 0.007246201857924461
iteration 108, loss = 0.010502246208488941
iteration 109, loss = 0.00806294847279787
iteration 110, loss = 0.00682184100151062
iteration 111, loss = 0.007971197366714478
iteration 112, loss = 0.008836843073368073
iteration 113, loss = 0.00888525228947401
iteration 114, loss = 0.007992877624928951
iteration 115, loss = 0.007868634536862373
iteration 116, loss = 0.009602620266377926
iteration 117, loss = 0.00647008465602994
iteration 118, loss = 0.006574196740984917
iteration 119, loss = 0.009753679856657982
iteration 120, loss = 0.0068415021523833275
iteration 121, loss = 0.006553277373313904
iteration 122, loss = 0.0070283073000609875
iteration 123, loss = 0.006557470187544823
iteration 124, loss = 0.008175761438906193
iteration 125, loss = 0.007272570393979549
iteration 126, loss = 0.006765774916857481
iteration 127, loss = 0.007105754688382149
iteration 128, loss = 0.008112994953989983
iteration 129, loss = 0.006678637117147446
iteration 130, loss = 0.007343078963458538
iteration 131, loss = 0.00803262460976839
iteration 132, loss = 0.01047558430582285
iteration 133, loss = 0.007086477242410183
iteration 134, loss = 0.007443653419613838
iteration 135, loss = 0.0061826929450035095
iteration 136, loss = 0.0061976430006325245
iteration 137, loss = 0.00621829042211175
iteration 138, loss = 0.007137127220630646
iteration 139, loss = 0.006321573164314032
iteration 140, loss = 0.006900024134665728
iteration 141, loss = 0.006362740881741047
iteration 142, loss = 0.007693360559642315
iteration 143, loss = 0.007271118927747011
iteration 144, loss = 0.006584744900465012
iteration 145, loss = 0.01002910640090704
iteration 146, loss = 0.006804292090237141
iteration 147, loss = 0.010626304894685745
iteration 148, loss = 0.010326099582016468
iteration 149, loss = 0.0063368831761181355
iteration 150, loss = 0.009815058670938015
iteration 151, loss = 0.006734578404575586
iteration 152, loss = 0.006630846299231052
iteration 153, loss = 0.007185493595898151
iteration 154, loss = 0.007205538917332888
iteration 155, loss = 0.007712619844824076
iteration 156, loss = 0.006620148196816444
iteration 157, loss = 0.006558151915669441
iteration 158, loss = 0.009337758645415306
iteration 159, loss = 0.010876650922000408
iteration 160, loss = 0.006252792663872242
iteration 161, loss = 0.006291270721703768
iteration 162, loss = 0.007955924607813358
iteration 163, loss = 0.006248078774660826
iteration 164, loss = 0.006869395729154348
iteration 165, loss = 0.0065858433954417706
iteration 166, loss = 0.006890317890793085
iteration 167, loss = 0.008268308825790882
iteration 168, loss = 0.006489620544016361
iteration 169, loss = 0.006951706018298864
iteration 170, loss = 0.007366525940597057
iteration 171, loss = 0.00802693422883749
iteration 172, loss = 0.006809266284108162
iteration 173, loss = 0.006194501183927059
iteration 174, loss = 0.006719906814396381
iteration 175, loss = 0.009593755006790161
iteration 176, loss = 0.010472090914845467
iteration 177, loss = 0.006965219043195248
iteration 178, loss = 0.006460265722125769
iteration 179, loss = 0.006425640545785427
iteration 180, loss = 0.009250408969819546
iteration 181, loss = 0.007616085931658745
iteration 182, loss = 0.010023540817201138
iteration 183, loss = 0.0065354593098163605
iteration 184, loss = 0.006438692566007376
iteration 185, loss = 0.006835728883743286
iteration 186, loss = 0.00625346414744854
iteration 187, loss = 0.006387211848050356
iteration 188, loss = 0.00934993103146553
iteration 189, loss = 0.0066083138808608055
iteration 190, loss = 0.006460715550929308
iteration 191, loss = 0.0065416679717600346
iteration 192, loss = 0.007427771110087633
iteration 193, loss = 0.007408562581986189
iteration 194, loss = 0.010027985088527203
iteration 195, loss = 0.008518602699041367
iteration 196, loss = 0.008247455582022667
iteration 197, loss = 0.006678823847323656
iteration 198, loss = 0.0067354850471019745
iteration 199, loss = 0.007585444953292608
iteration 200, loss = 0.007043913938105106
iteration 201, loss = 0.008161740377545357
iteration 202, loss = 0.006682065315544605
iteration 203, loss = 0.007047749124467373
iteration 204, loss = 0.007071961648762226
iteration 205, loss = 0.00875754002481699
iteration 206, loss = 0.009979402646422386
iteration 207, loss = 0.00881805270910263
iteration 208, loss = 0.006477119401097298
iteration 209, loss = 0.0062445043586194515
iteration 210, loss = 0.006880332250148058
iteration 211, loss = 0.0065221646800637245
iteration 212, loss = 0.007929397746920586
iteration 213, loss = 0.0065106614492833614
iteration 214, loss = 0.007014607544988394
iteration 215, loss = 0.005967409815639257
iteration 216, loss = 0.006088039837777615
iteration 217, loss = 0.007221262902021408
iteration 218, loss = 0.008832081221044064
iteration 219, loss = 0.006820689886808395
iteration 220, loss = 0.006437452509999275
iteration 221, loss = 0.012490269728004932
iteration 222, loss = 0.008305097930133343
iteration 223, loss = 0.0069443294778466225
iteration 224, loss = 0.006542470306158066
iteration 225, loss = 0.006541110575199127
iteration 226, loss = 0.008859100751578808
iteration 227, loss = 0.006897057872265577
iteration 228, loss = 0.0064569865353405476
iteration 229, loss = 0.006149670109152794
iteration 230, loss = 0.005638187285512686
iteration 231, loss = 0.006439356133341789
iteration 232, loss = 0.006575023755431175
iteration 233, loss = 0.006401308346539736
iteration 234, loss = 0.006251868326216936
iteration 235, loss = 0.0063033029437065125
iteration 236, loss = 0.00852689053863287
iteration 237, loss = 0.006895056925714016
iteration 238, loss = 0.0077963098883628845
iteration 239, loss = 0.0061444565653800964
iteration 240, loss = 0.00641406886279583
iteration 241, loss = 0.006159990094602108
iteration 242, loss = 0.006166794802993536
iteration 243, loss = 0.00876067765057087
iteration 244, loss = 0.0064238691702485085
iteration 245, loss = 0.007707209326326847
iteration 246, loss = 0.006104831118136644
iteration 247, loss = 0.008323815651237965
iteration 248, loss = 0.006312350742518902
iteration 249, loss = 0.007237124256789684
iteration 250, loss = 0.008518571965396404
iteration 251, loss = 0.006256363354623318
iteration 252, loss = 0.00594177795574069
iteration 253, loss = 0.006856879219412804
iteration 254, loss = 0.005996616557240486
iteration 255, loss = 0.00628249067813158
iteration 256, loss = 0.007248993031680584
iteration 257, loss = 0.006826222874224186
iteration 258, loss = 0.006297918036580086
iteration 259, loss = 0.006613424047827721
iteration 260, loss = 0.006166237406432629
iteration 261, loss = 0.006660624407231808
iteration 262, loss = 0.005631597246974707
iteration 263, loss = 0.006524553056806326
iteration 264, loss = 0.006801324896514416
iteration 265, loss = 0.007780636660754681
iteration 266, loss = 0.009323104284703732
iteration 267, loss = 0.008295935578644276
iteration 268, loss = 0.006843264680355787
iteration 269, loss = 0.007725593168288469
iteration 270, loss = 0.007451753597706556
iteration 271, loss = 0.006463253870606422
iteration 272, loss = 0.006032156758010387
iteration 273, loss = 0.006680704653263092
iteration 274, loss = 0.008707307279109955
iteration 275, loss = 0.0059068878181278706
iteration 276, loss = 0.009487894363701344
iteration 277, loss = 0.005718748085200787
iteration 278, loss = 0.005941811949014664
iteration 279, loss = 0.005510099697858095
iteration 280, loss = 0.006954798940569162
iteration 281, loss = 0.008762643672525883
iteration 282, loss = 0.0054384940303862095
iteration 283, loss = 0.006711818277835846
iteration 284, loss = 0.006418926641345024
iteration 285, loss = 0.0059319958090782166
iteration 286, loss = 0.006207330618053675
iteration 287, loss = 0.005739962216466665
iteration 288, loss = 0.007150989957153797
iteration 289, loss = 0.006122820544987917
iteration 290, loss = 0.006658151745796204
iteration 291, loss = 0.006127097178250551
iteration 292, loss = 0.006093574222177267
iteration 293, loss = 0.006294802762567997
iteration 294, loss = 0.006314593367278576
iteration 295, loss = 0.006180372089147568
iteration 296, loss = 0.007044307887554169
iteration 297, loss = 0.0069173239171504974
iteration 298, loss = 0.006413141265511513
iteration 299, loss = 0.0074085658416152
iteration 300, loss = 0.007694740779697895
iteration 1, loss = 0.007439010310918093
iteration 2, loss = 0.007281332276761532
iteration 3, loss = 0.007072594482451677
iteration 4, loss = 0.007199342828243971
iteration 5, loss = 0.007472666446119547
iteration 6, loss = 0.005796764977276325
iteration 7, loss = 0.006348641589283943
iteration 8, loss = 0.005936331115663052
iteration 9, loss = 0.006009289063513279
iteration 10, loss = 0.008227381855249405
iteration 11, loss = 0.005922191310673952
iteration 12, loss = 0.0063868192955851555
iteration 13, loss = 0.006824473850429058
iteration 14, loss = 0.00614879559725523
iteration 15, loss = 0.00638062646612525
iteration 16, loss = 0.00665296521037817
iteration 17, loss = 0.0063578165136277676
iteration 18, loss = 0.005951469764113426
iteration 19, loss = 0.006532500497996807
iteration 20, loss = 0.005929690320044756
iteration 21, loss = 0.005590023938566446
iteration 22, loss = 0.0062977010384202
iteration 23, loss = 0.006539145950227976
iteration 24, loss = 0.007786987349390984
iteration 25, loss = 0.006324089597910643
iteration 26, loss = 0.008403869345784187
iteration 27, loss = 0.005799515172839165
iteration 28, loss = 0.008095640689134598
iteration 29, loss = 0.008299332112073898
iteration 30, loss = 0.005994588602334261
iteration 31, loss = 0.006023489870131016
iteration 32, loss = 0.0060447221621870995
iteration 33, loss = 0.006655075121670961
iteration 34, loss = 0.005947196390479803
iteration 35, loss = 0.008525723591446877
iteration 36, loss = 0.005819791462272406
iteration 37, loss = 0.006024747621268034
iteration 38, loss = 0.006250975653529167
iteration 39, loss = 0.00590498698875308
iteration 40, loss = 0.008905843831598759
iteration 41, loss = 0.006182944402098656
iteration 42, loss = 0.00891307182610035
iteration 43, loss = 0.00602657301351428
iteration 44, loss = 0.005896704737097025
iteration 45, loss = 0.008808841928839684
iteration 46, loss = 0.006393090356141329
iteration 47, loss = 0.0068581667728722095
iteration 48, loss = 0.005888526327908039
iteration 49, loss = 0.008186581544578075
iteration 50, loss = 0.0062376586720347404
iteration 51, loss = 0.006112994626164436
iteration 52, loss = 0.006421450525522232
iteration 53, loss = 0.005735049024224281
iteration 54, loss = 0.006553918123245239
iteration 55, loss = 0.005629640072584152
iteration 56, loss = 0.007270114962011576
iteration 57, loss = 0.00664136465638876
iteration 58, loss = 0.007907272316515446
iteration 59, loss = 0.009073731489479542
iteration 60, loss = 0.006130922585725784
iteration 61, loss = 0.006138825323432684
iteration 62, loss = 0.0055910819210112095
iteration 63, loss = 0.005849412176758051
iteration 64, loss = 0.008925622329115868
iteration 65, loss = 0.00720510259270668
iteration 66, loss = 0.009116201661527157
iteration 67, loss = 0.0076581258326768875
iteration 68, loss = 0.00699298270046711
iteration 69, loss = 0.00584440678358078
iteration 70, loss = 0.006255584768950939
iteration 71, loss = 0.007125325966626406
iteration 72, loss = 0.006581483408808708
iteration 73, loss = 0.005781644489616156
iteration 74, loss = 0.005689287092536688
iteration 75, loss = 0.0065802959725260735
iteration 76, loss = 0.006222258787602186
iteration 77, loss = 0.005929896142333746
iteration 78, loss = 0.006105663720518351
iteration 79, loss = 0.00845284853130579
iteration 80, loss = 0.006244656164199114
iteration 81, loss = 0.006039065308868885
iteration 82, loss = 0.006452095694839954
iteration 83, loss = 0.006230092607438564
iteration 84, loss = 0.0066841053776443005
iteration 85, loss = 0.006071052979677916
iteration 86, loss = 0.006139874458312988
iteration 87, loss = 0.00600964343175292
iteration 88, loss = 0.006119196303188801
iteration 89, loss = 0.009202375076711178
iteration 90, loss = 0.006608658470213413
iteration 91, loss = 0.007509034592658281
iteration 92, loss = 0.007705318741500378
iteration 93, loss = 0.008597580716013908
iteration 94, loss = 0.006152440793812275
iteration 95, loss = 0.005828778259456158
iteration 96, loss = 0.007257562596350908
iteration 97, loss = 0.005644006188958883
iteration 98, loss = 0.006003604270517826
iteration 99, loss = 0.007742541376501322
iteration 100, loss = 0.008187191560864449
iteration 101, loss = 0.006359650753438473
iteration 102, loss = 0.009308363310992718
iteration 103, loss = 0.005842766724526882
iteration 104, loss = 0.007004457525908947
iteration 105, loss = 0.006714521907269955
iteration 106, loss = 0.006327280309051275
iteration 107, loss = 0.005916288122534752
iteration 108, loss = 0.006655194330960512
iteration 109, loss = 0.007222013548016548
iteration 110, loss = 0.006571148056536913
iteration 111, loss = 0.008293845690786839
iteration 112, loss = 0.005960976239293814
iteration 113, loss = 0.008952051401138306
iteration 114, loss = 0.0068215131759643555
iteration 115, loss = 0.006036963313817978
iteration 116, loss = 0.0055368440225720406
iteration 117, loss = 0.006758702918887138
iteration 118, loss = 0.00678918045014143
iteration 119, loss = 0.008301002904772758
iteration 120, loss = 0.006781409494578838
iteration 121, loss = 0.005777720361948013
iteration 122, loss = 0.00883324071764946
iteration 123, loss = 0.008952304720878601
iteration 124, loss = 0.0059850504621863365
iteration 125, loss = 0.005734530743211508
iteration 126, loss = 0.00839921273291111
iteration 127, loss = 0.006149368826299906
iteration 128, loss = 0.006056115962564945
iteration 129, loss = 0.006177305709570646
iteration 130, loss = 0.00599341094493866
iteration 131, loss = 0.007208666764199734
iteration 132, loss = 0.006910352967679501
iteration 133, loss = 0.005931308958679438
iteration 134, loss = 0.007247696164995432
iteration 135, loss = 0.006847714073956013
iteration 136, loss = 0.006408125627785921
iteration 137, loss = 0.005750961601734161
iteration 138, loss = 0.0065445732325315475
iteration 139, loss = 0.005684347357600927
iteration 140, loss = 0.00605308823287487
iteration 141, loss = 0.005244548432528973
iteration 142, loss = 0.0059648845344781876
iteration 143, loss = 0.006089306436479092
iteration 144, loss = 0.005911568645387888
iteration 145, loss = 0.006705221254378557
iteration 146, loss = 0.006072210613638163
iteration 147, loss = 0.007341815624386072
iteration 148, loss = 0.005778711289167404
iteration 149, loss = 0.008801496587693691
iteration 150, loss = 0.006280767731368542
iteration 151, loss = 0.006528839468955994
iteration 152, loss = 0.00658905366435647
iteration 153, loss = 0.005832270719110966
iteration 154, loss = 0.006364499684423208
iteration 155, loss = 0.005768993403762579
iteration 156, loss = 0.007373603526502848
iteration 157, loss = 0.008158871904015541
iteration 158, loss = 0.00639659259468317
iteration 159, loss = 0.006127386819571257
iteration 160, loss = 0.005398963578045368
iteration 161, loss = 0.006592243909835815
iteration 162, loss = 0.005974271334707737
iteration 163, loss = 0.0059189521707594395
iteration 164, loss = 0.006502317264676094
iteration 165, loss = 0.00866545271128416
iteration 166, loss = 0.006205117795616388
iteration 167, loss = 0.007236368954181671
iteration 168, loss = 0.007922549732029438
iteration 169, loss = 0.006946423090994358
iteration 170, loss = 0.005592870060354471
iteration 171, loss = 0.006094308570027351
iteration 172, loss = 0.006242271047085524
iteration 173, loss = 0.005751661956310272
iteration 174, loss = 0.006250923499464989
iteration 175, loss = 0.005935688968747854
iteration 176, loss = 0.008432726375758648
iteration 177, loss = 0.007549718953669071
iteration 178, loss = 0.008723168633878231
iteration 179, loss = 0.007910845801234245
iteration 180, loss = 0.006509555969387293
iteration 181, loss = 0.007977883331477642
iteration 182, loss = 0.006348687224090099
iteration 183, loss = 0.005521352402865887
iteration 184, loss = 0.008022913709282875
iteration 185, loss = 0.005778401158750057
iteration 186, loss = 0.006939808838069439
iteration 187, loss = 0.006674559321254492
iteration 188, loss = 0.0058601126074790955
iteration 189, loss = 0.005472627468407154
iteration 190, loss = 0.005789495538920164
iteration 191, loss = 0.006833230145275593
iteration 192, loss = 0.0059954640455543995
iteration 193, loss = 0.006181152071803808
iteration 194, loss = 0.006249675992876291
iteration 195, loss = 0.006350746378302574
iteration 196, loss = 0.006437547504901886
iteration 197, loss = 0.0059637706726789474
iteration 198, loss = 0.006228723097592592
iteration 199, loss = 0.006053359247744083
iteration 200, loss = 0.0065955775789916515
iteration 201, loss = 0.006137389224022627
iteration 202, loss = 0.006116728764027357
iteration 203, loss = 0.005894260480999947
iteration 204, loss = 0.0062764179892838
iteration 205, loss = 0.0065758409909904
iteration 206, loss = 0.006990237161517143
iteration 207, loss = 0.005733182188123465
iteration 208, loss = 0.005762170068919659
iteration 209, loss = 0.006333792582154274
iteration 210, loss = 0.006513424217700958
iteration 211, loss = 0.005948617123067379
iteration 212, loss = 0.005937638692557812
iteration 213, loss = 0.005556747782975435
iteration 214, loss = 0.007158215623348951
iteration 215, loss = 0.00861366931349039
iteration 216, loss = 0.006647760514169931
iteration 217, loss = 0.007223184686154127
iteration 218, loss = 0.00735190324485302
iteration 219, loss = 0.0055662826634943485
iteration 220, loss = 0.0067098066210746765
iteration 221, loss = 0.00685889134183526
iteration 222, loss = 0.009303167462348938
iteration 223, loss = 0.0060258605517446995
iteration 224, loss = 0.006712179630994797
iteration 225, loss = 0.006110593676567078
iteration 226, loss = 0.005877810996025801
iteration 227, loss = 0.005422157235443592
iteration 228, loss = 0.005797681864351034
iteration 229, loss = 0.006247889716178179
iteration 230, loss = 0.005898681003600359
iteration 231, loss = 0.005696178413927555
iteration 232, loss = 0.006004362367093563
iteration 233, loss = 0.0060639879666268826
iteration 234, loss = 0.006296719424426556
iteration 235, loss = 0.007505620829761028
iteration 236, loss = 0.008398303762078285
iteration 237, loss = 0.006079060025513172
iteration 238, loss = 0.007852797396481037
iteration 239, loss = 0.007531529758125544
iteration 240, loss = 0.005929975304752588
iteration 241, loss = 0.005731640383601189
iteration 242, loss = 0.00583396852016449
iteration 243, loss = 0.008508999831974506
iteration 244, loss = 0.0059018670581281185
iteration 245, loss = 0.006474325433373451
iteration 246, loss = 0.007057442795485258
iteration 247, loss = 0.007229574024677277
iteration 248, loss = 0.005829864181578159
iteration 249, loss = 0.0068703629076480865
iteration 250, loss = 0.00729088531807065
iteration 251, loss = 0.005590035580098629
iteration 252, loss = 0.008626852184534073
iteration 253, loss = 0.005522935185581446
iteration 254, loss = 0.009367937222123146
iteration 255, loss = 0.005679106339812279
iteration 256, loss = 0.005723982583731413
iteration 257, loss = 0.00679462356492877
iteration 258, loss = 0.005770079325884581
iteration 259, loss = 0.010153742507100105
iteration 260, loss = 0.006705299019813538
iteration 261, loss = 0.005521878600120544
iteration 262, loss = 0.005837440490722656
iteration 263, loss = 0.006137166637927294
iteration 264, loss = 0.00558119360357523
iteration 265, loss = 0.0055509936064481735
iteration 266, loss = 0.0059029278345406055
iteration 267, loss = 0.008494331501424313
iteration 268, loss = 0.006791591178625822
iteration 269, loss = 0.006446871440857649
iteration 270, loss = 0.007028952706605196
iteration 271, loss = 0.008897608146071434
iteration 272, loss = 0.006322133354842663
iteration 273, loss = 0.0055305734276771545
iteration 274, loss = 0.00598122738301754
iteration 275, loss = 0.007988035678863525
iteration 276, loss = 0.005572093650698662
iteration 277, loss = 0.009765409864485264
iteration 278, loss = 0.0063817561604082584
iteration 279, loss = 0.006269755773246288
iteration 280, loss = 0.005899148061871529
iteration 281, loss = 0.008425570093095303
iteration 282, loss = 0.006317462306469679
iteration 283, loss = 0.006750083994120359
iteration 284, loss = 0.005856927949935198
iteration 285, loss = 0.005832336843013763
iteration 286, loss = 0.006625305395573378
iteration 287, loss = 0.0054651712998747826
iteration 288, loss = 0.006069819908589125
iteration 289, loss = 0.0066903806291520596
iteration 290, loss = 0.008133375085890293
iteration 291, loss = 0.005967564880847931
iteration 292, loss = 0.00863824412226677
iteration 293, loss = 0.00670964177697897
iteration 294, loss = 0.006039782892912626
iteration 295, loss = 0.00891515426337719
iteration 296, loss = 0.0055163255892694
iteration 297, loss = 0.006043034605681896
iteration 298, loss = 0.0058501483872532845
iteration 299, loss = 0.007099881302565336
iteration 300, loss = 0.005808466114103794
iteration 1, loss = 0.005655058193951845
iteration 2, loss = 0.007239989936351776
iteration 3, loss = 0.006050319876521826
iteration 4, loss = 0.005892341490834951
iteration 5, loss = 0.006149847526103258
iteration 6, loss = 0.00628643436357379
iteration 7, loss = 0.007039006799459457
iteration 8, loss = 0.005964321084320545
iteration 9, loss = 0.005490968003869057
iteration 10, loss = 0.005762188695371151
iteration 11, loss = 0.005755883641541004
iteration 12, loss = 0.005602434277534485
iteration 13, loss = 0.005806530825793743
iteration 14, loss = 0.005535473581403494
iteration 15, loss = 0.0062143849208951
iteration 16, loss = 0.008580582216382027
iteration 17, loss = 0.007923821918666363
iteration 18, loss = 0.006193287670612335
iteration 19, loss = 0.005652738735079765
iteration 20, loss = 0.0056600929237902164
iteration 21, loss = 0.006017530802637339
iteration 22, loss = 0.0055542695336043835
iteration 23, loss = 0.008333815261721611
iteration 24, loss = 0.006779180373996496
iteration 25, loss = 0.0058157495222985744
iteration 26, loss = 0.006271161139011383
iteration 27, loss = 0.0061439285054802895
iteration 28, loss = 0.005851312540471554
iteration 29, loss = 0.006278672721236944
iteration 30, loss = 0.0059053790755569935
iteration 31, loss = 0.006331967189908028
iteration 32, loss = 0.007522781379520893
iteration 33, loss = 0.005433943122625351
iteration 34, loss = 0.005862194113433361
iteration 35, loss = 0.005947107449173927
iteration 36, loss = 0.009365829639136791
iteration 37, loss = 0.006060662213712931
iteration 38, loss = 0.006540690548717976
iteration 39, loss = 0.005906868726015091
iteration 40, loss = 0.009161706082522869
iteration 41, loss = 0.006694495677947998
iteration 42, loss = 0.006328541785478592
iteration 43, loss = 0.006239333190023899
iteration 44, loss = 0.006187419407069683
iteration 45, loss = 0.008425235748291016
iteration 46, loss = 0.005613235291093588
iteration 47, loss = 0.0058288658037781715
iteration 48, loss = 0.006023857742547989
iteration 49, loss = 0.005965433083474636
iteration 50, loss = 0.005951215513050556
iteration 51, loss = 0.005907285027205944
iteration 52, loss = 0.007428432814776897
iteration 53, loss = 0.005726184230297804
iteration 54, loss = 0.0061711883172392845
iteration 55, loss = 0.005565521772950888
iteration 56, loss = 0.006034992169588804
iteration 57, loss = 0.006365600507706404
iteration 58, loss = 0.005874165799468756
iteration 59, loss = 0.006474536843597889
iteration 60, loss = 0.006244013085961342
iteration 61, loss = 0.0058169253170490265
iteration 62, loss = 0.008758647367358208
iteration 63, loss = 0.0068895709700882435
iteration 64, loss = 0.0060109468176960945
iteration 65, loss = 0.0059786573983728886
iteration 66, loss = 0.005947830621153116
iteration 67, loss = 0.0054508694447577
iteration 68, loss = 0.006729842163622379
iteration 69, loss = 0.005842727143317461
iteration 70, loss = 0.0058911764062941074
iteration 71, loss = 0.005638011731207371
iteration 72, loss = 0.0062554581090807915
iteration 73, loss = 0.005759002175182104
iteration 74, loss = 0.005756353959441185
iteration 75, loss = 0.006213514134287834
iteration 76, loss = 0.0057298000901937485
iteration 77, loss = 0.005897616967558861
iteration 78, loss = 0.008336038328707218
iteration 79, loss = 0.009250760078430176
iteration 80, loss = 0.005948315374553204
iteration 81, loss = 0.009372426196932793
iteration 82, loss = 0.00705140782520175
iteration 83, loss = 0.005981385242193937
iteration 84, loss = 0.005761901382356882
iteration 85, loss = 0.00580444885417819
iteration 86, loss = 0.00582326902076602
iteration 87, loss = 0.0060132695361971855
iteration 88, loss = 0.005956151057034731
iteration 89, loss = 0.0067294505424797535
iteration 90, loss = 0.005979267880320549
iteration 91, loss = 0.006538071669638157
iteration 92, loss = 0.005715408828109503
iteration 93, loss = 0.006558528635650873
iteration 94, loss = 0.005689337383955717
iteration 95, loss = 0.00557554978877306
iteration 96, loss = 0.0061031486839056015
iteration 97, loss = 0.005932996515184641
iteration 98, loss = 0.008442584425210953
iteration 99, loss = 0.00814074743539095
iteration 100, loss = 0.005823417101055384
iteration 101, loss = 0.008852447383105755
iteration 102, loss = 0.00800430029630661
iteration 103, loss = 0.005989650264382362
iteration 104, loss = 0.005467758513987064
iteration 105, loss = 0.0057314420118927956
iteration 106, loss = 0.005599430296570063
iteration 107, loss = 0.00824427604675293
iteration 108, loss = 0.0062690661288797855
iteration 109, loss = 0.006330658216029406
iteration 110, loss = 0.008092262782156467
iteration 111, loss = 0.006030120421200991
iteration 112, loss = 0.006336668506264687
iteration 113, loss = 0.00841420516371727
iteration 114, loss = 0.0066781011410057545
iteration 115, loss = 0.007457865867763758
iteration 116, loss = 0.006337768863886595
iteration 117, loss = 0.007334686815738678
iteration 118, loss = 0.006116058677434921
iteration 119, loss = 0.006187637336552143
iteration 120, loss = 0.0059272656217217445
iteration 121, loss = 0.008574334904551506
iteration 122, loss = 0.00690285861492157
iteration 123, loss = 0.005636576097458601
iteration 124, loss = 0.006187292747199535
iteration 125, loss = 0.007519203703850508
iteration 126, loss = 0.005844038911163807
iteration 127, loss = 0.005875078495591879
iteration 128, loss = 0.005601114127784967
iteration 129, loss = 0.007847308181226254
iteration 130, loss = 0.00580586027354002
iteration 131, loss = 0.005928246304392815
iteration 132, loss = 0.005764874629676342
iteration 133, loss = 0.0062157586216926575
iteration 134, loss = 0.007536916062235832
iteration 135, loss = 0.005789690650999546
iteration 136, loss = 0.010371371172368526
iteration 137, loss = 0.005548024550080299
iteration 138, loss = 0.005815758369863033
iteration 139, loss = 0.009525159373879433
iteration 140, loss = 0.00579887256026268
iteration 141, loss = 0.0056658522225916386
iteration 142, loss = 0.006571152247488499
iteration 143, loss = 0.006848948076367378
iteration 144, loss = 0.007086731027811766
iteration 145, loss = 0.006060613319277763
iteration 146, loss = 0.01054682582616806
iteration 147, loss = 0.00912264920771122
iteration 148, loss = 0.0059752557426691055
iteration 149, loss = 0.009009999223053455
iteration 150, loss = 0.006725172512233257
iteration 151, loss = 0.006454508285969496
iteration 152, loss = 0.0068236845545470715
iteration 153, loss = 0.007078160997480154
iteration 154, loss = 0.006400959566235542
iteration 155, loss = 0.005986854434013367
iteration 156, loss = 0.005803070031106472
iteration 157, loss = 0.008571958169341087
iteration 158, loss = 0.006000661291182041
iteration 159, loss = 0.00602730643004179
iteration 160, loss = 0.005563434213399887
iteration 161, loss = 0.006561805959790945
iteration 162, loss = 0.005483440589159727
iteration 163, loss = 0.005660129711031914
iteration 164, loss = 0.006524599622935057
iteration 165, loss = 0.006114410236477852
iteration 166, loss = 0.007054115645587444
iteration 167, loss = 0.008114723488688469
iteration 168, loss = 0.006022739224135876
iteration 169, loss = 0.005311131477355957
iteration 170, loss = 0.006725665647536516
iteration 171, loss = 0.005595467984676361
iteration 172, loss = 0.005783099215477705
iteration 173, loss = 0.0056251357309520245
iteration 174, loss = 0.006989773828536272
iteration 175, loss = 0.005949728190898895
iteration 176, loss = 0.0058344523422420025
iteration 177, loss = 0.005647148005664349
iteration 178, loss = 0.007401825860142708
iteration 179, loss = 0.006028673145920038
iteration 180, loss = 0.006410233210772276
iteration 181, loss = 0.00598323717713356
iteration 182, loss = 0.005781102925539017
iteration 183, loss = 0.006291575729846954
iteration 184, loss = 0.010315319523215294
iteration 185, loss = 0.005780430510640144
iteration 186, loss = 0.00563648110255599
iteration 187, loss = 0.0073204850777983665
iteration 188, loss = 0.005584715865552425
iteration 189, loss = 0.0058031510561704636
iteration 190, loss = 0.007993809878826141
iteration 191, loss = 0.008531905710697174
iteration 192, loss = 0.006192925851792097
iteration 193, loss = 0.006642696913331747
iteration 194, loss = 0.006002570502460003
iteration 195, loss = 0.006097971461713314
iteration 196, loss = 0.0063114892691373825
iteration 197, loss = 0.005840342957526445
iteration 198, loss = 0.005864383652806282
iteration 199, loss = 0.007557904347777367
iteration 200, loss = 0.008198119699954987
iteration 201, loss = 0.0056274039670825005
iteration 202, loss = 0.006369637791067362
iteration 203, loss = 0.005918297916650772
iteration 204, loss = 0.006489364430308342
iteration 205, loss = 0.0058581409975886345
iteration 206, loss = 0.006623648572713137
iteration 207, loss = 0.007314163725823164
iteration 208, loss = 0.005864986218512058
iteration 209, loss = 0.0059979078359901905
iteration 210, loss = 0.005673763807862997
iteration 211, loss = 0.00574188306927681
iteration 212, loss = 0.006900235079228878
iteration 213, loss = 0.00754521694034338
iteration 214, loss = 0.005729636177420616
iteration 215, loss = 0.005586867220699787
iteration 216, loss = 0.006030888296663761
iteration 217, loss = 0.00822390429675579
iteration 218, loss = 0.005547374952584505
iteration 219, loss = 0.005552617833018303
iteration 220, loss = 0.00609216745942831
iteration 221, loss = 0.005877758376300335
iteration 222, loss = 0.005830933805555105
iteration 223, loss = 0.006633094046264887
iteration 224, loss = 0.0063925692811608315
iteration 225, loss = 0.005631186068058014
iteration 226, loss = 0.006491103209555149
iteration 227, loss = 0.005696142558008432
iteration 228, loss = 0.00891001895070076
iteration 229, loss = 0.005821469705551863
iteration 230, loss = 0.0055725970305502415
iteration 231, loss = 0.006545017007738352
iteration 232, loss = 0.011098685674369335
iteration 233, loss = 0.006397027522325516
iteration 234, loss = 0.006476593669503927
iteration 235, loss = 0.005922539625316858
iteration 236, loss = 0.005479307379573584
iteration 237, loss = 0.005773844663053751
iteration 238, loss = 0.005906226113438606
iteration 239, loss = 0.008484692312777042
iteration 240, loss = 0.006197601091116667
iteration 241, loss = 0.008175472728908062
iteration 242, loss = 0.0074126082472503185
iteration 243, loss = 0.008255256339907646
iteration 244, loss = 0.0065021151676774025
iteration 245, loss = 0.007020210847258568
iteration 246, loss = 0.006986694410443306
iteration 247, loss = 0.008383353240787983
iteration 248, loss = 0.008410106413066387
iteration 249, loss = 0.00587058998644352
iteration 250, loss = 0.008305586874485016
iteration 251, loss = 0.005956749431788921
iteration 252, loss = 0.006132449023425579
iteration 253, loss = 0.006051115691661835
iteration 254, loss = 0.007523033302277327
iteration 255, loss = 0.0062513588927686214
iteration 256, loss = 0.006183268502354622
iteration 257, loss = 0.00680311257019639
iteration 258, loss = 0.008712085895240307
iteration 259, loss = 0.006479896139353514
iteration 260, loss = 0.005817396566271782
iteration 261, loss = 0.005511793773621321
iteration 262, loss = 0.005558912642300129
iteration 263, loss = 0.00857762061059475
iteration 264, loss = 0.007497760932892561
iteration 265, loss = 0.00704062357544899
iteration 266, loss = 0.005958992522209883
iteration 267, loss = 0.010574806481599808
iteration 268, loss = 0.005983816925436258
iteration 269, loss = 0.00792283471673727
iteration 270, loss = 0.005871184170246124
iteration 271, loss = 0.006479784846305847
iteration 272, loss = 0.0058950399979949
iteration 273, loss = 0.00792701169848442
iteration 274, loss = 0.005809182301163673
iteration 275, loss = 0.005794099997729063
iteration 276, loss = 0.005799606908112764
iteration 277, loss = 0.005878969561308622
iteration 278, loss = 0.006462838035076857
iteration 279, loss = 0.006375870201736689
iteration 280, loss = 0.006161779165267944
iteration 281, loss = 0.005883204285055399
iteration 282, loss = 0.00649283779785037
iteration 283, loss = 0.005778033286333084
iteration 284, loss = 0.006261464674025774
iteration 285, loss = 0.00579746114090085
iteration 286, loss = 0.008643202483654022
iteration 287, loss = 0.006843804381787777
iteration 288, loss = 0.006940528750419617
iteration 289, loss = 0.006335468031466007
iteration 290, loss = 0.0061844103038311005
iteration 291, loss = 0.0058716763742268085
iteration 292, loss = 0.008135516196489334
iteration 293, loss = 0.006824034731835127
iteration 294, loss = 0.005771484225988388
iteration 295, loss = 0.0066321128979325294
iteration 296, loss = 0.005551380570977926
iteration 297, loss = 0.006390923634171486
iteration 298, loss = 0.0058626532554626465
iteration 299, loss = 0.006349416449666023
iteration 300, loss = 0.005928604863584042
iteration 1, loss = 0.005756699480116367
iteration 2, loss = 0.007156006991863251
iteration 3, loss = 0.006834406405687332
iteration 4, loss = 0.005228994879871607
iteration 5, loss = 0.005826660897582769
iteration 6, loss = 0.0057122306898236275
iteration 7, loss = 0.005562142003327608
iteration 8, loss = 0.005800510756671429
iteration 9, loss = 0.006081892643123865
iteration 10, loss = 0.0061730993911623955
iteration 11, loss = 0.0058215889148414135
iteration 12, loss = 0.007749826647341251
iteration 13, loss = 0.006435057148337364
iteration 14, loss = 0.00564959179610014
iteration 15, loss = 0.008859584107995033
iteration 16, loss = 0.006353599485009909
iteration 17, loss = 0.005916693713515997
iteration 18, loss = 0.006018259562551975
iteration 19, loss = 0.0071910470724105835
iteration 20, loss = 0.00870286114513874
iteration 21, loss = 0.010067693889141083
iteration 22, loss = 0.007099772337824106
iteration 23, loss = 0.005792959593236446
iteration 24, loss = 0.005614801775664091
iteration 25, loss = 0.006692700553685427
iteration 26, loss = 0.007940554991364479
iteration 27, loss = 0.005635158158838749
iteration 28, loss = 0.006189415231347084
iteration 29, loss = 0.006109227426350117
iteration 30, loss = 0.006290694698691368
iteration 31, loss = 0.00589945400133729
iteration 32, loss = 0.006189868785440922
iteration 33, loss = 0.005755692254751921
iteration 34, loss = 0.0053314147517085075
iteration 35, loss = 0.005712283309549093
iteration 36, loss = 0.006089912727475166
iteration 37, loss = 0.005396864842623472
iteration 38, loss = 0.005857264157384634
iteration 39, loss = 0.005829752888530493
iteration 40, loss = 0.0061013102531433105
iteration 41, loss = 0.005697119049727917
iteration 42, loss = 0.006621428765356541
iteration 43, loss = 0.0057607488706707954
iteration 44, loss = 0.005941936280578375
iteration 45, loss = 0.007522682659327984
iteration 46, loss = 0.005927474237978458
iteration 47, loss = 0.00919127743691206
iteration 48, loss = 0.0057525369338691235
iteration 49, loss = 0.005835576914250851
iteration 50, loss = 0.005753094330430031
iteration 51, loss = 0.00706158671528101
iteration 52, loss = 0.0057996716350317
iteration 53, loss = 0.006583121605217457
iteration 54, loss = 0.006076250225305557
iteration 55, loss = 0.005952319595962763
iteration 56, loss = 0.008380219340324402
iteration 57, loss = 0.006162015255540609
iteration 58, loss = 0.005131847690790892
iteration 59, loss = 0.005753898061811924
iteration 60, loss = 0.007877673022449017
iteration 61, loss = 0.005854669027030468
iteration 62, loss = 0.006125091575086117
iteration 63, loss = 0.005742953158915043
iteration 64, loss = 0.006663762964308262
iteration 65, loss = 0.0064212121069431305
iteration 66, loss = 0.0062324111349880695
iteration 67, loss = 0.006283091846853495
iteration 68, loss = 0.006402859929949045
iteration 69, loss = 0.006336665712296963
iteration 70, loss = 0.006868427619338036
iteration 71, loss = 0.006253801751881838
iteration 72, loss = 0.006342930719256401
iteration 73, loss = 0.006858076900243759
iteration 74, loss = 0.005516241304576397
iteration 75, loss = 0.009364962577819824
iteration 76, loss = 0.005536387208849192
iteration 77, loss = 0.005439486354589462
iteration 78, loss = 0.006277698092162609
iteration 79, loss = 0.006186174228787422
iteration 80, loss = 0.008015710860490799
iteration 81, loss = 0.007881483063101768
iteration 82, loss = 0.005837647244334221
iteration 83, loss = 0.007292271126061678
iteration 84, loss = 0.005657992325723171
iteration 85, loss = 0.007052609696984291
iteration 86, loss = 0.005688318982720375
iteration 87, loss = 0.006035398226231337
iteration 88, loss = 0.006210845895111561
iteration 89, loss = 0.005580123048275709
iteration 90, loss = 0.006190731190145016
iteration 91, loss = 0.0066235968843102455
iteration 92, loss = 0.0065152887254953384
iteration 93, loss = 0.005862895864993334
iteration 94, loss = 0.005723056383430958
iteration 95, loss = 0.005646638572216034
iteration 96, loss = 0.006791273597627878
iteration 97, loss = 0.00613365788012743
iteration 98, loss = 0.005174319259822369
iteration 99, loss = 0.007252286653965712
iteration 100, loss = 0.006768235936760902
iteration 101, loss = 0.005703146569430828
iteration 102, loss = 0.00633374135941267
iteration 103, loss = 0.007863624952733517
iteration 104, loss = 0.008095481432974339
iteration 105, loss = 0.007861143909394741
iteration 106, loss = 0.0057401941157877445
iteration 107, loss = 0.005673517473042011
iteration 108, loss = 0.006108253262937069
iteration 109, loss = 0.005881316028535366
iteration 110, loss = 0.008697704412043095
iteration 111, loss = 0.006864039693027735
iteration 112, loss = 0.005857523996382952
iteration 113, loss = 0.005625223740935326
iteration 114, loss = 0.00832864735275507
iteration 115, loss = 0.005612997803837061
iteration 116, loss = 0.005820917896926403
iteration 117, loss = 0.0058813998475670815
iteration 118, loss = 0.0062253340147435665
iteration 119, loss = 0.006466744467616081
iteration 120, loss = 0.005478472448885441
iteration 121, loss = 0.0057785906828939915
iteration 122, loss = 0.006804509554058313
iteration 123, loss = 0.006051038391888142
iteration 124, loss = 0.005404980853199959
iteration 125, loss = 0.005608642939478159
iteration 126, loss = 0.005628359969705343
iteration 127, loss = 0.00559136550873518
iteration 128, loss = 0.005732259713113308
iteration 129, loss = 0.005609498359262943
iteration 130, loss = 0.006431767717003822
iteration 131, loss = 0.00575121259316802
iteration 132, loss = 0.005737978033721447
iteration 133, loss = 0.007641985546797514
iteration 134, loss = 0.005406002514064312
iteration 135, loss = 0.007215477991849184
iteration 136, loss = 0.0058926804922521114
iteration 137, loss = 0.005999195389449596
iteration 138, loss = 0.007484353147447109
iteration 139, loss = 0.006636244244873524
iteration 140, loss = 0.005920954514294863
iteration 141, loss = 0.008378956466913223
iteration 142, loss = 0.006296979263424873
iteration 143, loss = 0.006410699337720871
iteration 144, loss = 0.005979164969176054
iteration 145, loss = 0.00809788890182972
iteration 146, loss = 0.00630905432626605
iteration 147, loss = 0.00891985185444355
iteration 148, loss = 0.005747264716774225
iteration 149, loss = 0.0060781268402934074
iteration 150, loss = 0.00689746905118227
iteration 151, loss = 0.00599687872454524
iteration 152, loss = 0.005576146766543388
iteration 153, loss = 0.005960124544799328
iteration 154, loss = 0.007369239814579487
iteration 155, loss = 0.005992453079670668
iteration 156, loss = 0.006056675687432289
iteration 157, loss = 0.005426994524896145
iteration 158, loss = 0.005771047435700893
iteration 159, loss = 0.00616034958511591
iteration 160, loss = 0.006976019591093063
iteration 161, loss = 0.0065270415507256985
iteration 162, loss = 0.005839064717292786
iteration 163, loss = 0.005687592551112175
iteration 164, loss = 0.006254231091588736
iteration 165, loss = 0.005909180734306574
iteration 166, loss = 0.005456057842820883
iteration 167, loss = 0.006851506419479847
iteration 168, loss = 0.005923065822571516
iteration 169, loss = 0.005284968297928572
iteration 170, loss = 0.005237508565187454
iteration 171, loss = 0.008683022111654282
iteration 172, loss = 0.005754226353019476
iteration 173, loss = 0.007810607086867094
iteration 174, loss = 0.00567438080906868
iteration 175, loss = 0.006919379346072674
iteration 176, loss = 0.006905582267791033
iteration 177, loss = 0.006429856643080711
iteration 178, loss = 0.005719075445085764
iteration 179, loss = 0.006186929531395435
iteration 180, loss = 0.005478632636368275
iteration 181, loss = 0.005721284542232752
iteration 182, loss = 0.005894159432500601
iteration 183, loss = 0.005585592705756426
iteration 184, loss = 0.00615611532703042
iteration 185, loss = 0.005931301973760128
iteration 186, loss = 0.00906643457710743
iteration 187, loss = 0.0056119211949408054
iteration 188, loss = 0.005765059031546116
iteration 189, loss = 0.006448561325669289
iteration 190, loss = 0.005982104688882828
iteration 191, loss = 0.006004865746945143
iteration 192, loss = 0.008944989182054996
iteration 193, loss = 0.006206341087818146
iteration 194, loss = 0.006400079932063818
iteration 195, loss = 0.006517183966934681
iteration 196, loss = 0.0059446836821734905
iteration 197, loss = 0.005813748575747013
iteration 198, loss = 0.00558818643912673
iteration 199, loss = 0.006585040595382452
iteration 200, loss = 0.0058823237195611
iteration 201, loss = 0.005726125556975603
iteration 202, loss = 0.007148934528231621
iteration 203, loss = 0.005350636783987284
iteration 204, loss = 0.008573981001973152
iteration 205, loss = 0.008025849238038063
iteration 206, loss = 0.007531479932367802
iteration 207, loss = 0.005273512098938227
iteration 208, loss = 0.006248641759157181
iteration 209, loss = 0.005380072630941868
iteration 210, loss = 0.0054692900739610195
iteration 211, loss = 0.00596241932362318
iteration 212, loss = 0.007743928115814924
iteration 213, loss = 0.006515834480524063
iteration 214, loss = 0.0063817473128438
iteration 215, loss = 0.00616450933739543
iteration 216, loss = 0.0054209548979997635
iteration 217, loss = 0.005754685495048761
iteration 218, loss = 0.006223511882126331
iteration 219, loss = 0.00571867311373353
iteration 220, loss = 0.006281043868511915
iteration 221, loss = 0.005662580486387014
iteration 222, loss = 0.006150238215923309
iteration 223, loss = 0.005626842379570007
iteration 224, loss = 0.0077485861256718636
iteration 225, loss = 0.005612056236714125
iteration 226, loss = 0.008156849071383476
iteration 227, loss = 0.006058963015675545
iteration 228, loss = 0.005497933831065893
iteration 229, loss = 0.006010137498378754
iteration 230, loss = 0.007930763997137547
iteration 231, loss = 0.006195815280079842
iteration 232, loss = 0.0057305144146084785
iteration 233, loss = 0.005907453130930662
iteration 234, loss = 0.005757562350481749
iteration 235, loss = 0.00779352243989706
iteration 236, loss = 0.008240318857133389
iteration 237, loss = 0.005429008975625038
iteration 238, loss = 0.008014529943466187
iteration 239, loss = 0.009589264169335365
iteration 240, loss = 0.006234105210751295
iteration 241, loss = 0.006463875062763691
iteration 242, loss = 0.006688201799988747
iteration 243, loss = 0.005324614234268665
iteration 244, loss = 0.0057609728537499905
iteration 245, loss = 0.006966390181332827
iteration 246, loss = 0.008562466129660606
iteration 247, loss = 0.006293571554124355
iteration 248, loss = 0.008595370687544346
iteration 249, loss = 0.006370175164192915
iteration 250, loss = 0.006825145334005356
iteration 251, loss = 0.005738840904086828
iteration 252, loss = 0.008142598904669285
iteration 253, loss = 0.00979462917894125
iteration 254, loss = 0.0057097310200333595
iteration 255, loss = 0.005824776366353035
iteration 256, loss = 0.007772697601467371
iteration 257, loss = 0.00797558855265379
iteration 258, loss = 0.008237022906541824
iteration 259, loss = 0.005842122714966536
iteration 260, loss = 0.005855341907590628
iteration 261, loss = 0.006201735697686672
iteration 262, loss = 0.0066475342027843
iteration 263, loss = 0.005908086895942688
iteration 264, loss = 0.005973050836473703
iteration 265, loss = 0.00875602476298809
iteration 266, loss = 0.007264268584549427
iteration 267, loss = 0.0052287522703409195
iteration 268, loss = 0.007842390798032284
iteration 269, loss = 0.005305989645421505
iteration 270, loss = 0.005105278454720974
iteration 271, loss = 0.006018744315952063
iteration 272, loss = 0.005645730532705784
iteration 273, loss = 0.006951859220862389
iteration 274, loss = 0.005966023541986942
iteration 275, loss = 0.0057617719285190105
iteration 276, loss = 0.0055036237463355064
iteration 277, loss = 0.006803922355175018
iteration 278, loss = 0.006314459256827831
iteration 279, loss = 0.008867498487234116
iteration 280, loss = 0.00536700151860714
iteration 281, loss = 0.006093186791986227
iteration 282, loss = 0.006238093599677086
iteration 283, loss = 0.006912042386829853
iteration 284, loss = 0.008330855518579483
iteration 285, loss = 0.008208882994949818
iteration 286, loss = 0.0053111352026462555
iteration 287, loss = 0.006806056015193462
iteration 288, loss = 0.005215963814407587
iteration 289, loss = 0.005192134063690901
iteration 290, loss = 0.005671289283782244
iteration 291, loss = 0.005615499801933765
iteration 292, loss = 0.0059097884222865105
iteration 293, loss = 0.007464669179171324
iteration 294, loss = 0.005517385900020599
iteration 295, loss = 0.005651993211358786
iteration 296, loss = 0.00847479235380888
iteration 297, loss = 0.006949299946427345
iteration 298, loss = 0.00688265822827816
iteration 299, loss = 0.006711354944854975
iteration 300, loss = 0.005475004203617573
iteration 1, loss = 0.006900396663695574
iteration 2, loss = 0.005814482923597097
iteration 3, loss = 0.005662704352289438
iteration 4, loss = 0.00547205563634634
iteration 5, loss = 0.005412012804299593
iteration 6, loss = 0.005860239267349243
iteration 7, loss = 0.0069497739896178246
iteration 8, loss = 0.0056411754339933395
iteration 9, loss = 0.00684884050861001
iteration 10, loss = 0.005751575343310833
iteration 11, loss = 0.0053830817341804504
iteration 12, loss = 0.00695835193619132
iteration 13, loss = 0.005820786580443382
iteration 14, loss = 0.005873359274119139
iteration 15, loss = 0.00539998896420002
iteration 16, loss = 0.0064946385100483894
iteration 17, loss = 0.005960126873105764
iteration 18, loss = 0.005987423937767744
iteration 19, loss = 0.006159938406199217
iteration 20, loss = 0.006174314301460981
iteration 21, loss = 0.0060469224117696285
iteration 22, loss = 0.007143064867705107
iteration 23, loss = 0.005942150019109249
iteration 24, loss = 0.005731138866394758
iteration 25, loss = 0.006245668977499008
iteration 26, loss = 0.005448218435049057
iteration 27, loss = 0.006072890479117632
iteration 28, loss = 0.005389869213104248
iteration 29, loss = 0.008429467678070068
iteration 30, loss = 0.00518666859716177
iteration 31, loss = 0.008369730785489082
iteration 32, loss = 0.007502516731619835
iteration 33, loss = 0.006031978875398636
iteration 34, loss = 0.00711546465754509
iteration 35, loss = 0.006944852881133556
iteration 36, loss = 0.005776523146778345
iteration 37, loss = 0.00627473508939147
iteration 38, loss = 0.006469438783824444
iteration 39, loss = 0.005538160912692547
iteration 40, loss = 0.00581817701458931
iteration 41, loss = 0.0066613671369850636
iteration 42, loss = 0.008723733015358448
iteration 43, loss = 0.005399488843977451
iteration 44, loss = 0.006540720816701651
iteration 45, loss = 0.006147447507828474
iteration 46, loss = 0.007912701927125454
iteration 47, loss = 0.005361541174352169
iteration 48, loss = 0.006239747162908316
iteration 49, loss = 0.00655888207256794
iteration 50, loss = 0.0052870470099151134
iteration 51, loss = 0.005139187444001436
iteration 52, loss = 0.0060918997041881084
iteration 53, loss = 0.006640301551669836
iteration 54, loss = 0.005918689072132111
iteration 55, loss = 0.005294132512062788
iteration 56, loss = 0.005689092446118593
iteration 57, loss = 0.005526701919734478
iteration 58, loss = 0.005895908921957016
iteration 59, loss = 0.006714871618896723
iteration 60, loss = 0.005828368943184614
iteration 61, loss = 0.006545953452587128
iteration 62, loss = 0.005413718055933714
iteration 63, loss = 0.00813676230609417
iteration 64, loss = 0.006194889545440674
iteration 65, loss = 0.005301509518176317
iteration 66, loss = 0.005725410766899586
iteration 67, loss = 0.005675802938640118
iteration 68, loss = 0.007657261565327644
iteration 69, loss = 0.007654428947716951
iteration 70, loss = 0.006011468358337879
iteration 71, loss = 0.006115170661360025
iteration 72, loss = 0.005588257219642401
iteration 73, loss = 0.005688187200576067
iteration 74, loss = 0.0057386732660233974
iteration 75, loss = 0.005265464074909687
iteration 76, loss = 0.005820122547447681
iteration 77, loss = 0.005492099095135927
iteration 78, loss = 0.006190196145325899
iteration 79, loss = 0.005960864946246147
iteration 80, loss = 0.005730407778173685
iteration 81, loss = 0.0058704884722828865
iteration 82, loss = 0.005561217200011015
iteration 83, loss = 0.010971630923449993
iteration 84, loss = 0.006044433452188969
iteration 85, loss = 0.0068153878673911095
iteration 86, loss = 0.005553421564400196
iteration 87, loss = 0.0068412115797400475
iteration 88, loss = 0.00564543716609478
iteration 89, loss = 0.005340111441910267
iteration 90, loss = 0.006221355404704809
iteration 91, loss = 0.005740090273320675
iteration 92, loss = 0.005708534270524979
iteration 93, loss = 0.0051688747480511665
iteration 94, loss = 0.0064888326451182365
iteration 95, loss = 0.00786801241338253
iteration 96, loss = 0.009225314483046532
iteration 97, loss = 0.005949109327048063
iteration 98, loss = 0.008289258927106857
iteration 99, loss = 0.00612356374040246
iteration 100, loss = 0.006589849013835192
iteration 101, loss = 0.005900493822991848
iteration 102, loss = 0.005991328042000532
iteration 103, loss = 0.0065713198855519295
iteration 104, loss = 0.006571206264197826
iteration 105, loss = 0.00585323479026556
iteration 106, loss = 0.006007706746459007
iteration 107, loss = 0.005372024606913328
iteration 108, loss = 0.008773157373070717
iteration 109, loss = 0.005743924528360367
iteration 110, loss = 0.005637905560433865
iteration 111, loss = 0.005682101007550955
iteration 112, loss = 0.005569910164922476
iteration 113, loss = 0.006639530882239342
iteration 114, loss = 0.005510512739419937
iteration 115, loss = 0.006943766493350267
iteration 116, loss = 0.006135464180260897
iteration 117, loss = 0.008529821410775185
iteration 118, loss = 0.005547091830521822
iteration 119, loss = 0.00751847866922617
iteration 120, loss = 0.0072957975789904594
iteration 121, loss = 0.005521909333765507
iteration 122, loss = 0.0061545115895569324
iteration 123, loss = 0.005414906889200211
iteration 124, loss = 0.0067401668056845665
iteration 125, loss = 0.005910892970860004
iteration 126, loss = 0.007697775959968567
iteration 127, loss = 0.00574528519064188
iteration 128, loss = 0.006271556485444307
iteration 129, loss = 0.006683760788291693
iteration 130, loss = 0.005610540509223938
iteration 131, loss = 0.005633718799799681
iteration 132, loss = 0.005505944136530161
iteration 133, loss = 0.005594904534518719
iteration 134, loss = 0.005751970689743757
iteration 135, loss = 0.005633902736008167
iteration 136, loss = 0.005816171411424875
iteration 137, loss = 0.005265107378363609
iteration 138, loss = 0.005807486828416586
iteration 139, loss = 0.00659420620650053
iteration 140, loss = 0.005541491322219372
iteration 141, loss = 0.005707632750272751
iteration 142, loss = 0.006353253964334726
iteration 143, loss = 0.005521798972040415
iteration 144, loss = 0.005581627134233713
iteration 145, loss = 0.006823332980275154
iteration 146, loss = 0.00582532724365592
iteration 147, loss = 0.008708244189620018
iteration 148, loss = 0.006474027410149574
iteration 149, loss = 0.008902685716748238
iteration 150, loss = 0.006055287551134825
iteration 151, loss = 0.0057050264440476894
iteration 152, loss = 0.010391282849013805
iteration 153, loss = 0.005724452435970306
iteration 154, loss = 0.006155287381261587
iteration 155, loss = 0.00620144372805953
iteration 156, loss = 0.006598529405891895
iteration 157, loss = 0.005928948521614075
iteration 158, loss = 0.008472790941596031
iteration 159, loss = 0.008326025679707527
iteration 160, loss = 0.006382131949067116
iteration 161, loss = 0.006219794973731041
iteration 162, loss = 0.008146149106323719
iteration 163, loss = 0.006701075937598944
iteration 164, loss = 0.007899095304310322
iteration 165, loss = 0.005589770618826151
iteration 166, loss = 0.007289784029126167
iteration 167, loss = 0.005588149651885033
iteration 168, loss = 0.006654500029981136
iteration 169, loss = 0.005185318179428577
iteration 170, loss = 0.00606524758040905
iteration 171, loss = 0.006434640381485224
iteration 172, loss = 0.005911144893616438
iteration 173, loss = 0.006102536804974079
iteration 174, loss = 0.006615804973989725
iteration 175, loss = 0.007655352354049683
iteration 176, loss = 0.0063759442418813705
iteration 177, loss = 0.006551461294293404
iteration 178, loss = 0.005627450067549944
iteration 179, loss = 0.006335539277642965
iteration 180, loss = 0.005589013919234276
iteration 181, loss = 0.005360180512070656
iteration 182, loss = 0.005775985773652792
iteration 183, loss = 0.0057146260514855385
iteration 184, loss = 0.009506833739578724
iteration 185, loss = 0.0059756990522146225
iteration 186, loss = 0.008865212090313435
iteration 187, loss = 0.005589072592556477
iteration 188, loss = 0.006425388157367706
iteration 189, loss = 0.005357044283300638
iteration 190, loss = 0.005574427079409361
iteration 191, loss = 0.008349871262907982
iteration 192, loss = 0.008364359848201275
iteration 193, loss = 0.005449590738862753
iteration 194, loss = 0.0062943194061517715
iteration 195, loss = 0.0059925769455730915
iteration 196, loss = 0.006348522379994392
iteration 197, loss = 0.005103232339024544
iteration 198, loss = 0.007475349586457014
iteration 199, loss = 0.008223377168178558
iteration 200, loss = 0.0059090848080813885
iteration 201, loss = 0.006006741896271706
iteration 202, loss = 0.005639138165861368
iteration 203, loss = 0.005461635068058968
iteration 204, loss = 0.005432551726698875
iteration 205, loss = 0.00558129558339715
iteration 206, loss = 0.005438884254544973
iteration 207, loss = 0.007106774020940065
iteration 208, loss = 0.005720679182559252
iteration 209, loss = 0.006011929363012314
iteration 210, loss = 0.005417334381490946
iteration 211, loss = 0.00636221282184124
iteration 212, loss = 0.0058329286985099316
iteration 213, loss = 0.008560423739254475
iteration 214, loss = 0.005193001590669155
iteration 215, loss = 0.007408060133457184
iteration 216, loss = 0.0051780217327177525
iteration 217, loss = 0.005755015183240175
iteration 218, loss = 0.007168889045715332
iteration 219, loss = 0.00639638165012002
iteration 220, loss = 0.005919029004871845
iteration 221, loss = 0.008251124992966652
iteration 222, loss = 0.006071004085242748
iteration 223, loss = 0.005912826396524906
iteration 224, loss = 0.006107128690928221
iteration 225, loss = 0.00598287396132946
iteration 226, loss = 0.00539786834269762
iteration 227, loss = 0.0062231519259512424
iteration 228, loss = 0.005727105308324099
iteration 229, loss = 0.007037171628326178
iteration 230, loss = 0.005715013947337866
iteration 231, loss = 0.005825309082865715
iteration 232, loss = 0.00543325487524271
iteration 233, loss = 0.006051226984709501
iteration 234, loss = 0.005795235745608807
iteration 235, loss = 0.005137537606060505
iteration 236, loss = 0.005415688268840313
iteration 237, loss = 0.006044023670256138
iteration 238, loss = 0.007754105143249035
iteration 239, loss = 0.005524701904505491
iteration 240, loss = 0.005688006058335304
iteration 241, loss = 0.008438440971076488
iteration 242, loss = 0.005833999253809452
iteration 243, loss = 0.00492989644408226
iteration 244, loss = 0.005492965690791607
iteration 245, loss = 0.005455014295876026
iteration 246, loss = 0.007150586228817701
iteration 247, loss = 0.005741627421230078
iteration 248, loss = 0.006077542435377836
iteration 249, loss = 0.005692461039870977
iteration 250, loss = 0.0054522412829101086
iteration 251, loss = 0.006675199139863253
iteration 252, loss = 0.006218666210770607
iteration 253, loss = 0.006848601624369621
iteration 254, loss = 0.008833402767777443
iteration 255, loss = 0.005628547165542841
iteration 256, loss = 0.005669473670423031
iteration 257, loss = 0.008513950742781162
iteration 258, loss = 0.006008037365972996
iteration 259, loss = 0.012052444741129875
iteration 260, loss = 0.005463253241032362
iteration 261, loss = 0.006739430595189333
iteration 262, loss = 0.005639686249196529
iteration 263, loss = 0.005882062017917633
iteration 264, loss = 0.0054153213277459145
iteration 265, loss = 0.005953629966825247
iteration 266, loss = 0.005673160310834646
iteration 267, loss = 0.005297285970300436
iteration 268, loss = 0.005256087519228458
iteration 269, loss = 0.006469731219112873
iteration 270, loss = 0.005576580297201872
iteration 271, loss = 0.006207632832229137
iteration 272, loss = 0.005173937417566776
iteration 273, loss = 0.00567223783582449
iteration 274, loss = 0.008076933212578297
iteration 275, loss = 0.00589089747518301
iteration 276, loss = 0.005453316029161215
iteration 277, loss = 0.005481479223817587
iteration 278, loss = 0.007373020984232426
iteration 279, loss = 0.006304583046585321
iteration 280, loss = 0.007256474811583757
iteration 281, loss = 0.0101835448294878
iteration 282, loss = 0.005693537183105946
iteration 283, loss = 0.007697778753936291
iteration 284, loss = 0.005619633477181196
iteration 285, loss = 0.005468722898513079
iteration 286, loss = 0.006199917756021023
iteration 287, loss = 0.00555470259860158
iteration 288, loss = 0.005550273694097996
iteration 289, loss = 0.005423740483820438
iteration 290, loss = 0.0070245591923594475
iteration 291, loss = 0.007511903531849384
iteration 292, loss = 0.00597033929079771
iteration 293, loss = 0.0077991122379899025
iteration 294, loss = 0.005824849009513855
iteration 295, loss = 0.005989001132547855
iteration 296, loss = 0.005743126850575209
iteration 297, loss = 0.005840446799993515
iteration 298, loss = 0.005991498474031687
iteration 299, loss = 0.006407796870917082
iteration 300, loss = 0.005710804369300604
iteration 1, loss = 0.00603219959884882
iteration 2, loss = 0.006647102534770966
iteration 3, loss = 0.006034053862094879
iteration 4, loss = 0.0055320593528449535
iteration 5, loss = 0.006468515843153
iteration 6, loss = 0.005620948970317841
iteration 7, loss = 0.005574987269937992
iteration 8, loss = 0.006352204829454422
iteration 9, loss = 0.006776901893317699
iteration 10, loss = 0.005800342187285423
iteration 11, loss = 0.005780613049864769
iteration 12, loss = 0.00555035937577486
iteration 13, loss = 0.006041760556399822
iteration 14, loss = 0.005650791339576244
iteration 15, loss = 0.006775334011763334
iteration 16, loss = 0.007457193452864885
iteration 17, loss = 0.005539671517908573
iteration 18, loss = 0.005289136897772551
iteration 19, loss = 0.006282171234488487
iteration 20, loss = 0.005529722664505243
iteration 21, loss = 0.006155981682240963
iteration 22, loss = 0.005633152090013027
iteration 23, loss = 0.005287318490445614
iteration 24, loss = 0.005641219671815634
iteration 25, loss = 0.005343149416148663
iteration 26, loss = 0.005208078306168318
iteration 27, loss = 0.005411525722593069
iteration 28, loss = 0.005186615511775017
iteration 29, loss = 0.007935995236039162
iteration 30, loss = 0.005586397834122181
iteration 31, loss = 0.006533871870487928
iteration 32, loss = 0.0053211660124361515
iteration 33, loss = 0.005373884923756123
iteration 34, loss = 0.005329469218850136
iteration 35, loss = 0.0054571120999753475
iteration 36, loss = 0.008002565242350101
iteration 37, loss = 0.008480186574161053
iteration 38, loss = 0.005962221883237362
iteration 39, loss = 0.0053960359655320644
iteration 40, loss = 0.008731168694794178
iteration 41, loss = 0.0058396137319505215
iteration 42, loss = 0.006323248613625765
iteration 43, loss = 0.00664098747074604
iteration 44, loss = 0.005898243747651577
iteration 45, loss = 0.006010147277265787
iteration 46, loss = 0.005415096413344145
iteration 47, loss = 0.005987456999719143
iteration 48, loss = 0.006031491793692112
iteration 49, loss = 0.005485475994646549
iteration 50, loss = 0.007427944336086512
iteration 51, loss = 0.005278537981212139
iteration 52, loss = 0.005423975642770529
iteration 53, loss = 0.005856959614902735
iteration 54, loss = 0.005689396057277918
iteration 55, loss = 0.005662820301949978
iteration 56, loss = 0.006714570801705122
iteration 57, loss = 0.007830158807337284
iteration 58, loss = 0.006234345026314259
iteration 59, loss = 0.005506466142833233
iteration 60, loss = 0.008129837922751904
iteration 61, loss = 0.007049539126455784
iteration 62, loss = 0.008840544149279594
iteration 63, loss = 0.005885496269911528
iteration 64, loss = 0.0057418388314545155
iteration 65, loss = 0.005506360437721014
iteration 66, loss = 0.005358385853469372
iteration 67, loss = 0.006465479731559753
iteration 68, loss = 0.006076512858271599
iteration 69, loss = 0.005406381096690893
iteration 70, loss = 0.005733444355428219
iteration 71, loss = 0.005660258699208498
iteration 72, loss = 0.008140665479004383
iteration 73, loss = 0.005702848080545664
iteration 74, loss = 0.008212251588702202
iteration 75, loss = 0.005823666229844093
iteration 76, loss = 0.005292411427944899
iteration 77, loss = 0.00594460591673851
iteration 78, loss = 0.0065863593481481075
iteration 79, loss = 0.00807829387485981
iteration 80, loss = 0.007501291111111641
iteration 81, loss = 0.006008466240018606
iteration 82, loss = 0.005762656219303608
iteration 83, loss = 0.005336964037269354
iteration 84, loss = 0.005879157222807407
iteration 85, loss = 0.005690277088433504
iteration 86, loss = 0.0056690615601837635
iteration 87, loss = 0.0052259378135204315
iteration 88, loss = 0.008047213777899742
iteration 89, loss = 0.005392136052250862
iteration 90, loss = 0.005351542495191097
iteration 91, loss = 0.005542328581213951
iteration 92, loss = 0.00529855489730835
iteration 93, loss = 0.006543906405568123
iteration 94, loss = 0.006515389308333397
iteration 95, loss = 0.008342171087861061
iteration 96, loss = 0.0055287606082856655
iteration 97, loss = 0.007803125306963921
iteration 98, loss = 0.005614297464489937
iteration 99, loss = 0.00580949243158102
iteration 100, loss = 0.006286486983299255
iteration 101, loss = 0.007611272390931845
iteration 102, loss = 0.006637079641222954
iteration 103, loss = 0.005468256305903196
iteration 104, loss = 0.0056676072999835014
iteration 105, loss = 0.006144913844764233
iteration 106, loss = 0.006604762282222509
iteration 107, loss = 0.005726905073970556
iteration 108, loss = 0.006058478262275457
iteration 109, loss = 0.007607678882777691
iteration 110, loss = 0.005725962575525045
iteration 111, loss = 0.007036388386040926
iteration 112, loss = 0.006465219892561436
iteration 113, loss = 0.005260559730231762
iteration 114, loss = 0.005587786436080933
iteration 115, loss = 0.005760720930993557
iteration 116, loss = 0.005672907456755638
iteration 117, loss = 0.006279501132667065
iteration 118, loss = 0.0063653551042079926
iteration 119, loss = 0.0053278193809092045
iteration 120, loss = 0.005342042539268732
iteration 121, loss = 0.005618198774755001
iteration 122, loss = 0.0077108778059482574
iteration 123, loss = 0.0055544497445225716
iteration 124, loss = 0.005741212051361799
iteration 125, loss = 0.006333187688142061
iteration 126, loss = 0.007876952178776264
iteration 127, loss = 0.00522337993606925
iteration 128, loss = 0.005746372044086456
iteration 129, loss = 0.0062821488827466965
iteration 130, loss = 0.0065339114516973495
iteration 131, loss = 0.005735564976930618
iteration 132, loss = 0.005085831508040428
iteration 133, loss = 0.007975209504365921
iteration 134, loss = 0.00531491544097662
iteration 135, loss = 0.007478212937712669
iteration 136, loss = 0.008246884681284428
iteration 137, loss = 0.006071085575968027
iteration 138, loss = 0.0056045702658593655
iteration 139, loss = 0.005658586509525776
iteration 140, loss = 0.005622453056275845
iteration 141, loss = 0.00731266476213932
iteration 142, loss = 0.005433876533061266
iteration 143, loss = 0.005395505111664534
iteration 144, loss = 0.007000658195465803
iteration 145, loss = 0.005107874982059002
iteration 146, loss = 0.005102547351270914
iteration 147, loss = 0.0062608676962554455
iteration 148, loss = 0.00541822612285614
iteration 149, loss = 0.005274046212434769
iteration 150, loss = 0.007371601648628712
iteration 151, loss = 0.005590786691755056
iteration 152, loss = 0.005201792344450951
iteration 153, loss = 0.005834958050400019
iteration 154, loss = 0.006809617858380079
iteration 155, loss = 0.0060645220801234245
iteration 156, loss = 0.005451105535030365
iteration 157, loss = 0.005617983173578978
iteration 158, loss = 0.005099053028970957
iteration 159, loss = 0.008076378144323826
iteration 160, loss = 0.009511256590485573
iteration 161, loss = 0.005361918359994888
iteration 162, loss = 0.005319579038769007
iteration 163, loss = 0.005538440775126219
iteration 164, loss = 0.00672527588903904
iteration 165, loss = 0.0056053269654512405
iteration 166, loss = 0.005920332856476307
iteration 167, loss = 0.007048176601529121
iteration 168, loss = 0.005654192063957453
iteration 169, loss = 0.005567161366343498
iteration 170, loss = 0.006558591965585947
iteration 171, loss = 0.005512828938663006
iteration 172, loss = 0.008700188249349594
iteration 173, loss = 0.007472251541912556
iteration 174, loss = 0.00568814855068922
iteration 175, loss = 0.007470104843378067
iteration 176, loss = 0.0052870637737214565
iteration 177, loss = 0.005504952277988195
iteration 178, loss = 0.005214804783463478
iteration 179, loss = 0.005294173024594784
iteration 180, loss = 0.0077626947313547134
iteration 181, loss = 0.005359550006687641
iteration 182, loss = 0.007585964165627956
iteration 183, loss = 0.0054434421472251415
iteration 184, loss = 0.005239875987172127
iteration 185, loss = 0.006315624341368675
iteration 186, loss = 0.006189965642988682
iteration 187, loss = 0.005357515066862106
iteration 188, loss = 0.005167563911527395
iteration 189, loss = 0.00543997623026371
iteration 190, loss = 0.006022414658218622
iteration 191, loss = 0.0074308281764388084
iteration 192, loss = 0.005725701339542866
iteration 193, loss = 0.005610178224742413
iteration 194, loss = 0.009428413584828377
iteration 195, loss = 0.004989058710634708
iteration 196, loss = 0.006189001724123955
iteration 197, loss = 0.005917226895689964
iteration 198, loss = 0.005692081991583109
iteration 199, loss = 0.005598456598818302
iteration 200, loss = 0.009660176932811737
iteration 201, loss = 0.005469633731991053
iteration 202, loss = 0.005684049800038338
iteration 203, loss = 0.00802337285131216
iteration 204, loss = 0.005548411980271339
iteration 205, loss = 0.00564237218350172
iteration 206, loss = 0.0056550088338553905
iteration 207, loss = 0.00545071717351675
iteration 208, loss = 0.005281445570290089
iteration 209, loss = 0.008116699755191803
iteration 210, loss = 0.005576466675847769
iteration 211, loss = 0.008935248479247093
iteration 212, loss = 0.005464838817715645
iteration 213, loss = 0.0069965156726539135
iteration 214, loss = 0.006380189675837755
iteration 215, loss = 0.0073044560849666595
iteration 216, loss = 0.005570090841501951
iteration 217, loss = 0.005183090455830097
iteration 218, loss = 0.005979803390800953
iteration 219, loss = 0.005442366003990173
iteration 220, loss = 0.005695309955626726
iteration 221, loss = 0.006060178857296705
iteration 222, loss = 0.005829181056469679
iteration 223, loss = 0.005363092292100191
iteration 224, loss = 0.005824176594614983
iteration 225, loss = 0.005365157965570688
iteration 226, loss = 0.005694828927516937
iteration 227, loss = 0.005760891363024712
iteration 228, loss = 0.00791640393435955
iteration 229, loss = 0.006020771339535713
iteration 230, loss = 0.005968831013888121
iteration 231, loss = 0.005429938901215792
iteration 232, loss = 0.00566444406285882
iteration 233, loss = 0.006206457503139973
iteration 234, loss = 0.005684373434633017
iteration 235, loss = 0.005621132906526327
iteration 236, loss = 0.0056709181517362595
iteration 237, loss = 0.005509388167411089
iteration 238, loss = 0.007926655933260918
iteration 239, loss = 0.006780617870390415
iteration 240, loss = 0.006878504063934088
iteration 241, loss = 0.0065634287893772125
iteration 242, loss = 0.006680723745375872
iteration 243, loss = 0.005546849220991135
iteration 244, loss = 0.006588860414922237
iteration 245, loss = 0.005559955723583698
iteration 246, loss = 0.005685932002961636
iteration 247, loss = 0.00628230394795537
iteration 248, loss = 0.010067539289593697
iteration 249, loss = 0.005522932857275009
iteration 250, loss = 0.00519910454750061
iteration 251, loss = 0.00673785200342536
iteration 252, loss = 0.006413040682673454
iteration 253, loss = 0.0059273624792695045
iteration 254, loss = 0.005697289016097784
iteration 255, loss = 0.008285784162580967
iteration 256, loss = 0.0055606612004339695
iteration 257, loss = 0.007592479698359966
iteration 258, loss = 0.005120519082993269
iteration 259, loss = 0.005587468855082989
iteration 260, loss = 0.005104782525449991
iteration 261, loss = 0.006423800252377987
iteration 262, loss = 0.005377080757170916
iteration 263, loss = 0.007688438519835472
iteration 264, loss = 0.0062540508806705475
iteration 265, loss = 0.006074001081287861
iteration 266, loss = 0.007652932778000832
iteration 267, loss = 0.005629841238260269
iteration 268, loss = 0.006376240402460098
iteration 269, loss = 0.005946257617324591
iteration 270, loss = 0.008110372349619865
iteration 271, loss = 0.005349763669073582
iteration 272, loss = 0.005725923925638199
iteration 273, loss = 0.00624821288511157
iteration 274, loss = 0.00536423921585083
iteration 275, loss = 0.006096632685512304
iteration 276, loss = 0.005067780613899231
iteration 277, loss = 0.009097149595618248
iteration 278, loss = 0.008603936061263084
iteration 279, loss = 0.0067983935587108135
iteration 280, loss = 0.006302750203758478
iteration 281, loss = 0.005244199652224779
iteration 282, loss = 0.005200111772865057
iteration 283, loss = 0.008352614939212799
iteration 284, loss = 0.006839624606072903
iteration 285, loss = 0.006243572570383549
iteration 286, loss = 0.005516306031495333
iteration 287, loss = 0.005097840912640095
iteration 288, loss = 0.0051296851597726345
iteration 289, loss = 0.005286918953061104
iteration 290, loss = 0.0054916865192353725
iteration 291, loss = 0.005533256102353334
iteration 292, loss = 0.006289895158261061
iteration 293, loss = 0.006567005999386311
iteration 294, loss = 0.006381659302860498
iteration 295, loss = 0.007287404499948025
iteration 296, loss = 0.005527517758309841
iteration 297, loss = 0.005303284153342247
iteration 298, loss = 0.007634978275746107
iteration 299, loss = 0.006411141715943813
iteration 300, loss = 0.006038503721356392
iteration 1, loss = 0.005596289876848459
iteration 2, loss = 0.006097178906202316
iteration 3, loss = 0.0053552002646028996
iteration 4, loss = 0.0054120859131217
iteration 5, loss = 0.005355470813810825
iteration 6, loss = 0.006904222536832094
iteration 7, loss = 0.005412624683231115
iteration 8, loss = 0.005150106735527515
iteration 9, loss = 0.005548337008804083
iteration 10, loss = 0.0075974599458277225
iteration 11, loss = 0.009041811339557171
iteration 12, loss = 0.005944874137639999
iteration 13, loss = 0.009383557364344597
iteration 14, loss = 0.005639117676764727
iteration 15, loss = 0.006082588341087103
iteration 16, loss = 0.005863514728844166
iteration 17, loss = 0.005485328380018473
iteration 18, loss = 0.006166952662169933
iteration 19, loss = 0.006425315514206886
iteration 20, loss = 0.006541589740663767
iteration 21, loss = 0.0060316817834973335
iteration 22, loss = 0.005500511731952429
iteration 23, loss = 0.007440725341439247
iteration 24, loss = 0.00774321798235178
iteration 25, loss = 0.005169714335352182
iteration 26, loss = 0.0052817706018686295
iteration 27, loss = 0.006004360504448414
iteration 28, loss = 0.005490260198712349
iteration 29, loss = 0.006623110733926296
iteration 30, loss = 0.0054742940701544285
iteration 31, loss = 0.005497344769537449
iteration 32, loss = 0.005158622283488512
iteration 33, loss = 0.007328595034778118
iteration 34, loss = 0.0051397718489170074
iteration 35, loss = 0.005954719614237547
iteration 36, loss = 0.006502537988126278
iteration 37, loss = 0.00813990831375122
iteration 38, loss = 0.0062224483117461205
iteration 39, loss = 0.006159965880215168
iteration 40, loss = 0.005840854719281197
iteration 41, loss = 0.005531965289264917
iteration 42, loss = 0.006611439399421215
iteration 43, loss = 0.005093730054795742
iteration 44, loss = 0.005802281200885773
iteration 45, loss = 0.007392972707748413
iteration 46, loss = 0.006131183356046677
iteration 47, loss = 0.005178561434149742
iteration 48, loss = 0.006466659251600504
iteration 49, loss = 0.0057504684664309025
iteration 50, loss = 0.006167007144540548
iteration 51, loss = 0.006618605926632881
iteration 52, loss = 0.005963116884231567
iteration 53, loss = 0.005692740902304649
iteration 54, loss = 0.005446245893836021
iteration 55, loss = 0.005691767204552889
iteration 56, loss = 0.005269652232527733
iteration 57, loss = 0.00538545660674572
iteration 58, loss = 0.005444343667477369
iteration 59, loss = 0.00550172571092844
iteration 60, loss = 0.005244728177785873
iteration 61, loss = 0.005598463583737612
iteration 62, loss = 0.005501172970980406
iteration 63, loss = 0.006107055116444826
iteration 64, loss = 0.006290749181061983
iteration 65, loss = 0.005835310090333223
iteration 66, loss = 0.005557941272854805
iteration 67, loss = 0.005253190640360117
iteration 68, loss = 0.0053583234548568726
iteration 69, loss = 0.0054447767324745655
iteration 70, loss = 0.005385220050811768
iteration 71, loss = 0.00523476954549551
iteration 72, loss = 0.006553082726895809
iteration 73, loss = 0.006102692801505327
iteration 74, loss = 0.005749118514358997
iteration 75, loss = 0.005421125330030918
iteration 76, loss = 0.006550941150635481
iteration 77, loss = 0.00889544002711773
iteration 78, loss = 0.007037559989839792
iteration 79, loss = 0.00616998877376318
iteration 80, loss = 0.005645296070724726
iteration 81, loss = 0.005177253391593695
iteration 82, loss = 0.005346935708075762
iteration 83, loss = 0.006101004779338837
iteration 84, loss = 0.006081406492739916
iteration 85, loss = 0.006146554835140705
iteration 86, loss = 0.005175097845494747
iteration 87, loss = 0.008486439473927021
iteration 88, loss = 0.006382271647453308
iteration 89, loss = 0.005291981156915426
iteration 90, loss = 0.0053590708412230015
iteration 91, loss = 0.0056995851919054985
iteration 92, loss = 0.005232325755059719
iteration 93, loss = 0.0059678927063941956
iteration 94, loss = 0.005791269708424807
iteration 95, loss = 0.007469740696251392
iteration 96, loss = 0.005280320532619953
iteration 97, loss = 0.00559705775231123
iteration 98, loss = 0.005546006374061108
iteration 99, loss = 0.006244653835892677
iteration 100, loss = 0.005702780093997717
iteration 101, loss = 0.005792500916868448
iteration 102, loss = 0.006769163534045219
iteration 103, loss = 0.004950988106429577
iteration 104, loss = 0.005912202876061201
iteration 105, loss = 0.0049287546426057816
iteration 106, loss = 0.0053878179751336575
iteration 107, loss = 0.006489512976258993
iteration 108, loss = 0.005685013253241777
iteration 109, loss = 0.006214070599526167
iteration 110, loss = 0.0053655365481972694
iteration 111, loss = 0.006083820015192032
iteration 112, loss = 0.005598857067525387
iteration 113, loss = 0.005184202920645475
iteration 114, loss = 0.005241698119789362
iteration 115, loss = 0.00549634825438261
iteration 116, loss = 0.005691969767212868
iteration 117, loss = 0.005370813887566328
iteration 118, loss = 0.0065725259482860565
iteration 119, loss = 0.007038440089672804
iteration 120, loss = 0.007549663074314594
iteration 121, loss = 0.005137682892382145
iteration 122, loss = 0.005718848668038845
iteration 123, loss = 0.005094653926789761
iteration 124, loss = 0.007615161594003439
iteration 125, loss = 0.007960500195622444
iteration 126, loss = 0.0053544798865914345
iteration 127, loss = 0.005911399610340595
iteration 128, loss = 0.0050173490308225155
iteration 129, loss = 0.005437391810119152
iteration 130, loss = 0.006142265163362026
iteration 131, loss = 0.006542871706187725
iteration 132, loss = 0.005212940741330385
iteration 133, loss = 0.005450510885566473
iteration 134, loss = 0.00560788856819272
iteration 135, loss = 0.0058716097846627235
iteration 136, loss = 0.005552425514906645
iteration 137, loss = 0.005674056708812714
iteration 138, loss = 0.005404679570347071
iteration 139, loss = 0.005901269149035215
iteration 140, loss = 0.007527812384068966
iteration 141, loss = 0.005957175046205521
iteration 142, loss = 0.005645181518048048
iteration 143, loss = 0.00606541009619832
iteration 144, loss = 0.007215303368866444
iteration 145, loss = 0.006209914572536945
iteration 146, loss = 0.005420736037194729
iteration 147, loss = 0.008916633203625679
iteration 148, loss = 0.005230315960943699
iteration 149, loss = 0.006700301077216864
iteration 150, loss = 0.005148395895957947
iteration 151, loss = 0.010769419372081757
iteration 152, loss = 0.0059972601011395454
iteration 153, loss = 0.005404595751315355
iteration 154, loss = 0.005699736997485161
iteration 155, loss = 0.005993399769067764
iteration 156, loss = 0.007234956603497267
iteration 157, loss = 0.005812869872897863
iteration 158, loss = 0.006179338321089745
iteration 159, loss = 0.005501524545252323
iteration 160, loss = 0.006627799943089485
iteration 161, loss = 0.006540167145431042
iteration 162, loss = 0.005353429354727268
iteration 163, loss = 0.005460930988192558
iteration 164, loss = 0.005823638290166855
iteration 165, loss = 0.006170456297695637
iteration 166, loss = 0.005722823552787304
iteration 167, loss = 0.005936638452112675
iteration 168, loss = 0.005930078215897083
iteration 169, loss = 0.005023282486945391
iteration 170, loss = 0.005885586608201265
iteration 171, loss = 0.007516898214817047
iteration 172, loss = 0.0065459501929581165
iteration 173, loss = 0.006809932645410299
iteration 174, loss = 0.005826019216328859
iteration 175, loss = 0.0077291736379265785
iteration 176, loss = 0.007178245112299919
iteration 177, loss = 0.006002448499202728
iteration 178, loss = 0.007478976622223854
iteration 179, loss = 0.008488756604492664
iteration 180, loss = 0.005942533258348703
iteration 181, loss = 0.005287739913910627
iteration 182, loss = 0.00716637447476387
iteration 183, loss = 0.005134016741067171
iteration 184, loss = 0.005756962578743696
iteration 185, loss = 0.00496257096529007
iteration 186, loss = 0.005660484079271555
iteration 187, loss = 0.00612334581092
iteration 188, loss = 0.0049068378284573555
iteration 189, loss = 0.0071936110034585
iteration 190, loss = 0.00527796009555459
iteration 191, loss = 0.005442444700747728
iteration 192, loss = 0.005476818419992924
iteration 193, loss = 0.006465502083301544
iteration 194, loss = 0.005786851514130831
iteration 195, loss = 0.005397425033152103
iteration 196, loss = 0.006144272163510323
iteration 197, loss = 0.005517023149877787
iteration 198, loss = 0.007360407616943121
iteration 199, loss = 0.008493454195559025
iteration 200, loss = 0.005785477813333273
iteration 201, loss = 0.005280294455587864
iteration 202, loss = 0.004958156496286392
iteration 203, loss = 0.005182250868529081
iteration 204, loss = 0.005837688222527504
iteration 205, loss = 0.005062411539256573
iteration 206, loss = 0.005956786684691906
iteration 207, loss = 0.006125276908278465
iteration 208, loss = 0.007908743806183338
iteration 209, loss = 0.00528375431895256
iteration 210, loss = 0.005783134140074253
iteration 211, loss = 0.007729668170213699
iteration 212, loss = 0.005471558775752783
iteration 213, loss = 0.006209983490407467
iteration 214, loss = 0.007652509957551956
iteration 215, loss = 0.005615707021206617
iteration 216, loss = 0.005456531886011362
iteration 217, loss = 0.005159582011401653
iteration 218, loss = 0.006430597510188818
iteration 219, loss = 0.005706334952265024
iteration 220, loss = 0.00534496083855629
iteration 221, loss = 0.005484310444444418
iteration 222, loss = 0.00799091812223196
iteration 223, loss = 0.005196564830839634
iteration 224, loss = 0.005576033610850573
iteration 225, loss = 0.007119040936231613
iteration 226, loss = 0.005183792673051357
iteration 227, loss = 0.005669220816344023
iteration 228, loss = 0.005781637970358133
iteration 229, loss = 0.00530105410143733
iteration 230, loss = 0.006085146684199572
iteration 231, loss = 0.005165994167327881
iteration 232, loss = 0.005141907371580601
iteration 233, loss = 0.005861444864422083
iteration 234, loss = 0.007393927313387394
iteration 235, loss = 0.006018581800162792
iteration 236, loss = 0.00612239446491003
iteration 237, loss = 0.007444537244737148
iteration 238, loss = 0.005231714807450771
iteration 239, loss = 0.006388053763657808
iteration 240, loss = 0.006385067477822304
iteration 241, loss = 0.005470325239002705
iteration 242, loss = 0.006476272828876972
iteration 243, loss = 0.005140617489814758
iteration 244, loss = 0.007597486488521099
iteration 245, loss = 0.0053611816838383675
iteration 246, loss = 0.0050201574340462685
iteration 247, loss = 0.005515498109161854
iteration 248, loss = 0.0054290103726089
iteration 249, loss = 0.010855701752007008
iteration 250, loss = 0.005086236167699099
iteration 251, loss = 0.005391763988882303
iteration 252, loss = 0.006149583496153355
iteration 253, loss = 0.0073402393609285355
iteration 254, loss = 0.005225005559623241
iteration 255, loss = 0.0064594014547765255
iteration 256, loss = 0.0056409635581076145
iteration 257, loss = 0.005390170030295849
iteration 258, loss = 0.005367221776396036
iteration 259, loss = 0.006745218764990568
iteration 260, loss = 0.005599017255008221
iteration 261, loss = 0.005150863900780678
iteration 262, loss = 0.005671271588653326
iteration 263, loss = 0.005265876650810242
iteration 264, loss = 0.006511162035167217
iteration 265, loss = 0.005473770201206207
iteration 266, loss = 0.00816645473241806
iteration 267, loss = 0.0054260664619505405
iteration 268, loss = 0.004854168277233839
iteration 269, loss = 0.005424213595688343
iteration 270, loss = 0.007444403134286404
iteration 271, loss = 0.006420908495783806
iteration 272, loss = 0.006110760383307934
iteration 273, loss = 0.005488596856594086
iteration 274, loss = 0.007731106597930193
iteration 275, loss = 0.005487545859068632
iteration 276, loss = 0.005469611380249262
iteration 277, loss = 0.00564996711909771
iteration 278, loss = 0.007251232862472534
iteration 279, loss = 0.006810123100876808
iteration 280, loss = 0.005737322848290205
iteration 281, loss = 0.005500584840774536
iteration 282, loss = 0.007372415624558926
iteration 283, loss = 0.009838901460170746
iteration 284, loss = 0.005619986914098263
iteration 285, loss = 0.005962698254734278
iteration 286, loss = 0.005040773190557957
iteration 287, loss = 0.0063086883164942265
iteration 288, loss = 0.006124213803559542
iteration 289, loss = 0.006345015484839678
iteration 290, loss = 0.005565376952290535
iteration 291, loss = 0.008549858815968037
iteration 292, loss = 0.006402516271919012
iteration 293, loss = 0.006090451031923294
iteration 294, loss = 0.01052755769342184
iteration 295, loss = 0.005680455360561609
iteration 296, loss = 0.00796978548169136
iteration 297, loss = 0.00497752521187067
iteration 298, loss = 0.006102510262280703
iteration 299, loss = 0.005218127742409706
iteration 300, loss = 0.00525959488004446
iteration 1, loss = 0.005346883554011583
iteration 2, loss = 0.00539808813482523
iteration 3, loss = 0.005809361115098
iteration 4, loss = 0.005571348126977682
iteration 5, loss = 0.0053624375723302364
iteration 6, loss = 0.005209203809499741
iteration 7, loss = 0.00542645575478673
iteration 8, loss = 0.00526758749037981
iteration 9, loss = 0.005669783800840378
iteration 10, loss = 0.0051252832636237144
iteration 11, loss = 0.005684408359229565
iteration 12, loss = 0.006180969998240471
iteration 13, loss = 0.007523398380726576
iteration 14, loss = 0.005692849867045879
iteration 15, loss = 0.005580319091677666
iteration 16, loss = 0.005900379735976458
iteration 17, loss = 0.005424455739557743
iteration 18, loss = 0.00736084533855319
iteration 19, loss = 0.005954036954790354
iteration 20, loss = 0.005241758190095425
iteration 21, loss = 0.005690091755241156
iteration 22, loss = 0.005930100567638874
iteration 23, loss = 0.0055181547068059444
iteration 24, loss = 0.005584042519330978
iteration 25, loss = 0.007530934177339077
iteration 26, loss = 0.00547417625784874
iteration 27, loss = 0.007815288379788399
iteration 28, loss = 0.0052136704325675964
iteration 29, loss = 0.005307453218847513
iteration 30, loss = 0.00667528435587883
iteration 31, loss = 0.006290297955274582
iteration 32, loss = 0.006696110591292381
iteration 33, loss = 0.005213676951825619
iteration 34, loss = 0.005671427119523287
iteration 35, loss = 0.006247241515666246
iteration 36, loss = 0.005331045016646385
iteration 37, loss = 0.007844093255698681
iteration 38, loss = 0.005532595794647932
iteration 39, loss = 0.008052499033510685
iteration 40, loss = 0.005616476759314537
iteration 41, loss = 0.005518731195479631
iteration 42, loss = 0.0064073847606778145
iteration 43, loss = 0.008443303406238556
iteration 44, loss = 0.008877537213265896
iteration 45, loss = 0.005759669467806816
iteration 46, loss = 0.007463657762855291
iteration 47, loss = 0.005780347157269716
iteration 48, loss = 0.00667902547866106
iteration 49, loss = 0.00748019153252244
iteration 50, loss = 0.006732556968927383
iteration 51, loss = 0.005578386131674051
iteration 52, loss = 0.007251191418617964
iteration 53, loss = 0.005702874157577753
iteration 54, loss = 0.0052844756282866
iteration 55, loss = 0.007512096781283617
iteration 56, loss = 0.005146889481693506
iteration 57, loss = 0.005486071575433016
iteration 58, loss = 0.0065223583951592445
iteration 59, loss = 0.006253653671592474
iteration 60, loss = 0.0052438760176301
iteration 61, loss = 0.005220238119363785
iteration 62, loss = 0.005428777076303959
iteration 63, loss = 0.0067985630594193935
iteration 64, loss = 0.00484102638438344
iteration 65, loss = 0.005106361582875252
iteration 66, loss = 0.005626993253827095
iteration 67, loss = 0.005020528100430965
iteration 68, loss = 0.005276455543935299
iteration 69, loss = 0.005678579211235046
iteration 70, loss = 0.0059792958199977875
iteration 71, loss = 0.005395359359681606
iteration 72, loss = 0.005773215554654598
iteration 73, loss = 0.0072777727618813515
iteration 74, loss = 0.0051149772480130196
iteration 75, loss = 0.006965831853449345
iteration 76, loss = 0.007502540946006775
iteration 77, loss = 0.006041811779141426
iteration 78, loss = 0.004930476658046246
iteration 79, loss = 0.005954894702881575
iteration 80, loss = 0.0059800478629767895
iteration 81, loss = 0.0059333425015211105
iteration 82, loss = 0.005995843093842268
iteration 83, loss = 0.005395850632339716
iteration 84, loss = 0.005312989000231028
iteration 85, loss = 0.005424900911748409
iteration 86, loss = 0.004942784085869789
iteration 87, loss = 0.005513566546142101
iteration 88, loss = 0.005698448047041893
iteration 89, loss = 0.005958134774118662
iteration 90, loss = 0.009586834348738194
iteration 91, loss = 0.005699930712580681
iteration 92, loss = 0.006745492573827505
iteration 93, loss = 0.005564732477068901
iteration 94, loss = 0.005205466412007809
iteration 95, loss = 0.007624470163136721
iteration 96, loss = 0.005304876249283552
iteration 97, loss = 0.00547425914555788
iteration 98, loss = 0.006751062348484993
iteration 99, loss = 0.008299367502331734
iteration 100, loss = 0.006969192065298557
iteration 101, loss = 0.005528828129172325
iteration 102, loss = 0.004978102166205645
iteration 103, loss = 0.005792516283690929
iteration 104, loss = 0.005734121892601252
iteration 105, loss = 0.005782023072242737
iteration 106, loss = 0.00913611613214016
iteration 107, loss = 0.005120424088090658
iteration 108, loss = 0.00529550900682807
iteration 109, loss = 0.005695229861885309
iteration 110, loss = 0.007141599431633949
iteration 111, loss = 0.005342051852494478
iteration 112, loss = 0.006153706926852465
iteration 113, loss = 0.00622623972594738
iteration 114, loss = 0.005392194259911776
iteration 115, loss = 0.005518930498510599
iteration 116, loss = 0.005612665321677923
iteration 117, loss = 0.005294349044561386
iteration 118, loss = 0.005454516503959894
iteration 119, loss = 0.005685320124030113
iteration 120, loss = 0.005046192556619644
iteration 121, loss = 0.0052525755017995834
iteration 122, loss = 0.007986098527908325
iteration 123, loss = 0.0073820604011416435
iteration 124, loss = 0.005177830345928669
iteration 125, loss = 0.005927570629864931
iteration 126, loss = 0.005045187193900347
iteration 127, loss = 0.005484511144459248
iteration 128, loss = 0.005548929329961538
iteration 129, loss = 0.005010033026337624
iteration 130, loss = 0.0052644177339971066
iteration 131, loss = 0.005524669773876667
iteration 132, loss = 0.008207419887185097
iteration 133, loss = 0.005722959991544485
iteration 134, loss = 0.007077268324792385
iteration 135, loss = 0.005177517421543598
iteration 136, loss = 0.006066871806979179
iteration 137, loss = 0.0063531603664159775
iteration 138, loss = 0.005597815848886967
iteration 139, loss = 0.007955004461109638
iteration 140, loss = 0.005641613621264696
iteration 141, loss = 0.005040784366428852
iteration 142, loss = 0.008115971460938454
iteration 143, loss = 0.005570906680077314
iteration 144, loss = 0.005105516407638788
iteration 145, loss = 0.004922457970678806
iteration 146, loss = 0.005194812081754208
iteration 147, loss = 0.006106744520366192
iteration 148, loss = 0.005173408426344395
iteration 149, loss = 0.007545607630163431
iteration 150, loss = 0.00798707827925682
iteration 151, loss = 0.005450915545225143
iteration 152, loss = 0.007137610577046871
iteration 153, loss = 0.006062887143343687
iteration 154, loss = 0.005327939055860043
iteration 155, loss = 0.0059345862828195095
iteration 156, loss = 0.005090714432299137
iteration 157, loss = 0.007172623183578253
iteration 158, loss = 0.005195567850023508
iteration 159, loss = 0.005519859492778778
iteration 160, loss = 0.006648950278759003
iteration 161, loss = 0.004967824090272188
iteration 162, loss = 0.007959154434502125
iteration 163, loss = 0.005234558135271072
iteration 164, loss = 0.005838848650455475
iteration 165, loss = 0.006536115892231464
iteration 166, loss = 0.005379852373152971
iteration 167, loss = 0.007884275168180466
iteration 168, loss = 0.005672953091561794
iteration 169, loss = 0.0061151073314249516
iteration 170, loss = 0.005645895842462778
iteration 171, loss = 0.006989310961216688
iteration 172, loss = 0.00563761405646801
iteration 173, loss = 0.006486871745437384
iteration 174, loss = 0.005950257182121277
iteration 175, loss = 0.004976541735231876
iteration 176, loss = 0.005513387732207775
iteration 177, loss = 0.0054151504300534725
iteration 178, loss = 0.00490320660173893
iteration 179, loss = 0.006639442406594753
iteration 180, loss = 0.005226690787822008
iteration 181, loss = 0.007396198809146881
iteration 182, loss = 0.0058455923572182655
iteration 183, loss = 0.0052896104753017426
iteration 184, loss = 0.007809347007423639
iteration 185, loss = 0.0056739505380392075
iteration 186, loss = 0.005825408268719912
iteration 187, loss = 0.005267237778753042
iteration 188, loss = 0.005404881201684475
iteration 189, loss = 0.006903272587805986
iteration 190, loss = 0.00492840213701129
iteration 191, loss = 0.005949047394096851
iteration 192, loss = 0.0063550653867423534
iteration 193, loss = 0.005344172939658165
iteration 194, loss = 0.005400740075856447
iteration 195, loss = 0.005963418632745743
iteration 196, loss = 0.005510939285159111
iteration 197, loss = 0.006061111111193895
iteration 198, loss = 0.0059708342887461185
iteration 199, loss = 0.0050804633647203445
iteration 200, loss = 0.006104097235947847
iteration 201, loss = 0.005241198930889368
iteration 202, loss = 0.005156434141099453
iteration 203, loss = 0.00569460354745388
iteration 204, loss = 0.0055763800628483295
iteration 205, loss = 0.007564590312540531
iteration 206, loss = 0.005259046796709299
iteration 207, loss = 0.005030582658946514
iteration 208, loss = 0.0073675550520420074
iteration 209, loss = 0.005119959823787212
iteration 210, loss = 0.005684431176632643
iteration 211, loss = 0.005925221834331751
iteration 212, loss = 0.0062677208334207535
iteration 213, loss = 0.005226878449320793
iteration 214, loss = 0.007589651271700859
iteration 215, loss = 0.007755671162158251
iteration 216, loss = 0.0049639237113296986
iteration 217, loss = 0.0050757890567183495
iteration 218, loss = 0.005153578240424395
iteration 219, loss = 0.005399307236075401
iteration 220, loss = 0.005419185850769281
iteration 221, loss = 0.006209464743733406
iteration 222, loss = 0.006452150642871857
iteration 223, loss = 0.005396936554461718
iteration 224, loss = 0.007954544387757778
iteration 225, loss = 0.005476700607687235
iteration 226, loss = 0.005008306819945574
iteration 227, loss = 0.005711245350539684
iteration 228, loss = 0.0052903625182807446
iteration 229, loss = 0.004934343043714762
iteration 230, loss = 0.006090256851166487
iteration 231, loss = 0.005094552878290415
iteration 232, loss = 0.0054686060175299644
iteration 233, loss = 0.005190291441977024
iteration 234, loss = 0.00506876315921545
iteration 235, loss = 0.0071911318227648735
iteration 236, loss = 0.00540934456512332
iteration 237, loss = 0.00665375916287303
iteration 238, loss = 0.007209645118564367
iteration 239, loss = 0.00623228820040822
iteration 240, loss = 0.005153514910489321
iteration 241, loss = 0.0054789092391729355
iteration 242, loss = 0.00581804383546114
iteration 243, loss = 0.005357963033020496
iteration 244, loss = 0.005905875936150551
iteration 245, loss = 0.006671407260000706
iteration 246, loss = 0.00799894705414772
iteration 247, loss = 0.006190776359289885
iteration 248, loss = 0.005900237243622541
iteration 249, loss = 0.007770808879286051
iteration 250, loss = 0.005581136327236891
iteration 251, loss = 0.006027200724929571
iteration 252, loss = 0.0050416667945683
iteration 253, loss = 0.006396266166120768
iteration 254, loss = 0.005548465996980667
iteration 255, loss = 0.00879494659602642
iteration 256, loss = 0.005267005413770676
iteration 257, loss = 0.00569839496165514
iteration 258, loss = 0.005509871058166027
iteration 259, loss = 0.005744337569922209
iteration 260, loss = 0.005366789177060127
iteration 261, loss = 0.005334485322237015
iteration 262, loss = 0.005430526565760374
iteration 263, loss = 0.0077249351888895035
iteration 264, loss = 0.006438208743929863
iteration 265, loss = 0.006805628538131714
iteration 266, loss = 0.006482334807515144
iteration 267, loss = 0.005382290575653315
iteration 268, loss = 0.005236675497144461
iteration 269, loss = 0.006059087347239256
iteration 270, loss = 0.006315827835351229
iteration 271, loss = 0.0057341004721820354
iteration 272, loss = 0.004747006110846996
iteration 273, loss = 0.005761223379522562
iteration 274, loss = 0.00539756054058671
iteration 275, loss = 0.00618975143879652
iteration 276, loss = 0.005233440548181534
iteration 277, loss = 0.005156771745532751
iteration 278, loss = 0.005734097212553024
iteration 279, loss = 0.0048311506398022175
iteration 280, loss = 0.005564956925809383
iteration 281, loss = 0.005281285382807255
iteration 282, loss = 0.005348801612854004
iteration 283, loss = 0.00540166487917304
iteration 284, loss = 0.005214634817093611
iteration 285, loss = 0.005041304975748062
iteration 286, loss = 0.0055145337246358395
iteration 287, loss = 0.0061593931168317795
iteration 288, loss = 0.005002868361771107
iteration 289, loss = 0.00542441988363862
iteration 290, loss = 0.007350900210440159
iteration 291, loss = 0.00539835449308157
iteration 292, loss = 0.005609406158328056
iteration 293, loss = 0.005151968449354172
iteration 294, loss = 0.004794395063072443
iteration 295, loss = 0.007916280068457127
iteration 296, loss = 0.006100425962358713
iteration 297, loss = 0.006409940309822559
iteration 298, loss = 0.007007718086242676
iteration 299, loss = 0.007364319637417793
iteration 300, loss = 0.00762998778373003
iteration 1, loss = 0.006110249552875757
iteration 2, loss = 0.0071511524729430676
iteration 3, loss = 0.010060647502541542
iteration 4, loss = 0.008270252496004105
iteration 5, loss = 0.0052020433358848095
iteration 6, loss = 0.005099764559417963
iteration 7, loss = 0.0062809656374156475
iteration 8, loss = 0.0077084437943995
iteration 9, loss = 0.005166242830455303
iteration 10, loss = 0.005374539643526077
iteration 11, loss = 0.005833130329847336
iteration 12, loss = 0.00532897375524044
iteration 13, loss = 0.005037662573158741
iteration 14, loss = 0.005645819939672947
iteration 15, loss = 0.005039811134338379
iteration 16, loss = 0.005067430902272463
iteration 17, loss = 0.005109664984047413
iteration 18, loss = 0.006101204082369804
iteration 19, loss = 0.00881889183074236
iteration 20, loss = 0.009153981693089008
iteration 21, loss = 0.005641782656311989
iteration 22, loss = 0.005756720434874296
iteration 23, loss = 0.0051665473729372025
iteration 24, loss = 0.005285652820020914
iteration 25, loss = 0.005448452662676573
iteration 26, loss = 0.004895715042948723
iteration 27, loss = 0.0060743363574147224
iteration 28, loss = 0.007756710052490234
iteration 29, loss = 0.005746101029217243
iteration 30, loss = 0.0077893091365695
iteration 31, loss = 0.005462867207825184
iteration 32, loss = 0.0055493805557489395
iteration 33, loss = 0.0056289914064109325
iteration 34, loss = 0.005123056471347809
iteration 35, loss = 0.005641134921461344
iteration 36, loss = 0.00586670683696866
iteration 37, loss = 0.005512060597538948
iteration 38, loss = 0.005271290894597769
iteration 39, loss = 0.00699223205447197
iteration 40, loss = 0.006796842906624079
iteration 41, loss = 0.007253068499267101
iteration 42, loss = 0.006226375233381987
iteration 43, loss = 0.0051919580437242985
iteration 44, loss = 0.00607667351141572
iteration 45, loss = 0.004992722533643246
iteration 46, loss = 0.007343818433582783
iteration 47, loss = 0.008077449165284634
iteration 48, loss = 0.006531579419970512
iteration 49, loss = 0.007780144922435284
iteration 50, loss = 0.005197964608669281
iteration 51, loss = 0.0050514773465693
iteration 52, loss = 0.005665981210768223
iteration 53, loss = 0.007181691471487284
iteration 54, loss = 0.005264441017061472
iteration 55, loss = 0.005423957947641611
iteration 56, loss = 0.00567775359377265
iteration 57, loss = 0.005278443917632103
iteration 58, loss = 0.005425907205790281
iteration 59, loss = 0.005595777649432421
iteration 60, loss = 0.005696901585906744
iteration 61, loss = 0.0051374295726418495
iteration 62, loss = 0.006224398966878653
iteration 63, loss = 0.005224857944995165
iteration 64, loss = 0.006694549694657326
iteration 65, loss = 0.005066439509391785
iteration 66, loss = 0.005232379771769047
iteration 67, loss = 0.005739966407418251
iteration 68, loss = 0.00525832362473011
iteration 69, loss = 0.0049962494522333145
iteration 70, loss = 0.005262679420411587
iteration 71, loss = 0.005911586340516806
iteration 72, loss = 0.005664032883942127
iteration 73, loss = 0.005258118733763695
iteration 74, loss = 0.005148567724972963
iteration 75, loss = 0.006942159030586481
iteration 76, loss = 0.005391253158450127
iteration 77, loss = 0.00607705395668745
iteration 78, loss = 0.006332041695713997
iteration 79, loss = 0.005703170783817768
iteration 80, loss = 0.007054999936372042
iteration 81, loss = 0.005209243390709162
iteration 82, loss = 0.005492389667779207
iteration 83, loss = 0.005198420956730843
iteration 84, loss = 0.0073654670268297195
iteration 85, loss = 0.006420921999961138
iteration 86, loss = 0.004958040080964565
iteration 87, loss = 0.0050224196165800095
iteration 88, loss = 0.005493423901498318
iteration 89, loss = 0.006094381213188171
iteration 90, loss = 0.0056334007531404495
iteration 91, loss = 0.005452858284115791
iteration 92, loss = 0.007910319603979588
iteration 93, loss = 0.005328397266566753
iteration 94, loss = 0.006125119514763355
iteration 95, loss = 0.0055491435341537
iteration 96, loss = 0.005407965742051601
iteration 97, loss = 0.0073191630654037
iteration 98, loss = 0.005938561633229256
iteration 99, loss = 0.0054436419159173965
iteration 100, loss = 0.0066392389126122
iteration 101, loss = 0.006198541261255741
iteration 102, loss = 0.005207424983382225
iteration 103, loss = 0.0052259196527302265
iteration 104, loss = 0.00522091519087553
iteration 105, loss = 0.005595811177045107
iteration 106, loss = 0.005354760680347681
iteration 107, loss = 0.0058548953384160995
iteration 108, loss = 0.005265875719487667
iteration 109, loss = 0.005144708324223757
iteration 110, loss = 0.005609338171780109
iteration 111, loss = 0.005196935031563044
iteration 112, loss = 0.005117668304592371
iteration 113, loss = 0.005148952826857567
iteration 114, loss = 0.009107524529099464
iteration 115, loss = 0.0050610024482011795
iteration 116, loss = 0.0055350675247609615
iteration 117, loss = 0.0051517849788069725
iteration 118, loss = 0.005143116228282452
iteration 119, loss = 0.005390495993196964
iteration 120, loss = 0.005057375878095627
iteration 121, loss = 0.0052389949560165405
iteration 122, loss = 0.005482215899974108
iteration 123, loss = 0.004776785150170326
iteration 124, loss = 0.00531526654958725
iteration 125, loss = 0.007609581109136343
iteration 126, loss = 0.005665618926286697
iteration 127, loss = 0.005215873941779137
iteration 128, loss = 0.010430887341499329
iteration 129, loss = 0.005997632630169392
iteration 130, loss = 0.005511212628334761
iteration 131, loss = 0.005768081173300743
iteration 132, loss = 0.007202595472335815
iteration 133, loss = 0.0052106971852481365
iteration 134, loss = 0.005241892766207457
iteration 135, loss = 0.005843177437782288
iteration 136, loss = 0.005098390392959118
iteration 137, loss = 0.005346029065549374
iteration 138, loss = 0.005530793219804764
iteration 139, loss = 0.005042389966547489
iteration 140, loss = 0.005388085264712572
iteration 141, loss = 0.005532410927116871
iteration 142, loss = 0.0062122829258441925
iteration 143, loss = 0.005109499674290419
iteration 144, loss = 0.006474363151937723
iteration 145, loss = 0.008366419933736324
iteration 146, loss = 0.005131955724209547
iteration 147, loss = 0.007271208800375462
iteration 148, loss = 0.0059469835832715034
iteration 149, loss = 0.0049557071179151535
iteration 150, loss = 0.006562205497175455
iteration 151, loss = 0.005518518853932619
iteration 152, loss = 0.005420555826276541
iteration 153, loss = 0.005186164751648903
iteration 154, loss = 0.005209443159401417
iteration 155, loss = 0.00599296297878027
iteration 156, loss = 0.005994527600705624
iteration 157, loss = 0.005836508236825466
iteration 158, loss = 0.005145861301571131
iteration 159, loss = 0.0053243995644152164
iteration 160, loss = 0.005131364334374666
iteration 161, loss = 0.0054674469865858555
iteration 162, loss = 0.004935162607580423
iteration 163, loss = 0.005240465514361858
iteration 164, loss = 0.0054962290450930595
iteration 165, loss = 0.006193776149302721
iteration 166, loss = 0.005584334954619408
iteration 167, loss = 0.005691280122846365
iteration 168, loss = 0.005355454050004482
iteration 169, loss = 0.005032812245190144
iteration 170, loss = 0.00578308617696166
iteration 171, loss = 0.007433801889419556
iteration 172, loss = 0.007841845043003559
iteration 173, loss = 0.005739925429224968
iteration 174, loss = 0.007269632536917925
iteration 175, loss = 0.007533355616033077
iteration 176, loss = 0.005392745137214661
iteration 177, loss = 0.006591865327209234
iteration 178, loss = 0.0052846381440758705
iteration 179, loss = 0.005299731157720089
iteration 180, loss = 0.004982699640095234
iteration 181, loss = 0.004525987431406975
iteration 182, loss = 0.005971404258161783
iteration 183, loss = 0.005425749812275171
iteration 184, loss = 0.005142848007380962
iteration 185, loss = 0.005506736692041159
iteration 186, loss = 0.005356443114578724
iteration 187, loss = 0.0078088873997330666
iteration 188, loss = 0.005791459698230028
iteration 189, loss = 0.005575159564614296
iteration 190, loss = 0.0054412661120295525
iteration 191, loss = 0.0049972389824688435
iteration 192, loss = 0.006718185730278492
iteration 193, loss = 0.005376877263188362
iteration 194, loss = 0.005638839676976204
iteration 195, loss = 0.006255796179175377
iteration 196, loss = 0.0049409205093979836
iteration 197, loss = 0.005685974843800068
iteration 198, loss = 0.008531028404831886
iteration 199, loss = 0.006070305593311787
iteration 200, loss = 0.004776062909513712
iteration 201, loss = 0.006914264056831598
iteration 202, loss = 0.0054196882992982864
iteration 203, loss = 0.004987554624676704
iteration 204, loss = 0.004814890678972006
iteration 205, loss = 0.005340178497135639
iteration 206, loss = 0.007744468282908201
iteration 207, loss = 0.005195165053009987
iteration 208, loss = 0.00731456745415926
iteration 209, loss = 0.005558756645768881
iteration 210, loss = 0.005473109893500805
iteration 211, loss = 0.005009586922824383
iteration 212, loss = 0.006399885751307011
iteration 213, loss = 0.004973928909748793
iteration 214, loss = 0.004841146059334278
iteration 215, loss = 0.005607067607343197
iteration 216, loss = 0.005771874450147152
iteration 217, loss = 0.005035359412431717
iteration 218, loss = 0.005880177021026611
iteration 219, loss = 0.006031464785337448
iteration 220, loss = 0.005411076825112104
iteration 221, loss = 0.005934885237365961
iteration 222, loss = 0.005727438721805811
iteration 223, loss = 0.005898553412407637
iteration 224, loss = 0.004896814003586769
iteration 225, loss = 0.007713281083852053
iteration 226, loss = 0.007302692625671625
iteration 227, loss = 0.005107692908495665
iteration 228, loss = 0.00643383851274848
iteration 229, loss = 0.005328207276761532
iteration 230, loss = 0.004937505349516869
iteration 231, loss = 0.007781552150845528
iteration 232, loss = 0.008909345604479313
iteration 233, loss = 0.006006521172821522
iteration 234, loss = 0.005759586580097675
iteration 235, loss = 0.005578780546784401
iteration 236, loss = 0.0064389766193926334
iteration 237, loss = 0.005469554103910923
iteration 238, loss = 0.005506036337465048
iteration 239, loss = 0.005878816824406385
iteration 240, loss = 0.005465999245643616
iteration 241, loss = 0.004888398572802544
iteration 242, loss = 0.005131589248776436
iteration 243, loss = 0.005338226445019245
iteration 244, loss = 0.007506950292736292
iteration 245, loss = 0.005218880716711283
iteration 246, loss = 0.005567568354308605
iteration 247, loss = 0.006085666362196207
iteration 248, loss = 0.007478085346519947
iteration 249, loss = 0.00538672273978591
iteration 250, loss = 0.00583982327952981
iteration 251, loss = 0.005681011825799942
iteration 252, loss = 0.007750032469630241
iteration 253, loss = 0.005622540600597858
iteration 254, loss = 0.0079569760710001
iteration 255, loss = 0.005845943465828896
iteration 256, loss = 0.005071260500699282
iteration 257, loss = 0.004974200390279293
iteration 258, loss = 0.008257224224507809
iteration 259, loss = 0.004873494151979685
iteration 260, loss = 0.005207326263189316
iteration 261, loss = 0.005004681181162596
iteration 262, loss = 0.007597604766488075
iteration 263, loss = 0.005270123947411776
iteration 264, loss = 0.005271305795758963
iteration 265, loss = 0.0053232223726809025
iteration 266, loss = 0.005121381022036076
iteration 267, loss = 0.005565348081290722
iteration 268, loss = 0.0055601573549211025
iteration 269, loss = 0.005148041527718306
iteration 270, loss = 0.005399967078119516
iteration 271, loss = 0.005604867823421955
iteration 272, loss = 0.005461016669869423
iteration 273, loss = 0.005482730455696583
iteration 274, loss = 0.0054930709302425385
iteration 275, loss = 0.005103608593344688
iteration 276, loss = 0.004842696245759726
iteration 277, loss = 0.005237393081188202
iteration 278, loss = 0.005403384566307068
iteration 279, loss = 0.00473577156662941
iteration 280, loss = 0.006218606140464544
iteration 281, loss = 0.005991438869386911
iteration 282, loss = 0.006181181408464909
iteration 283, loss = 0.00497435312718153
iteration 284, loss = 0.005008416716009378
iteration 285, loss = 0.005079459398984909
iteration 286, loss = 0.005343707278370857
iteration 287, loss = 0.0053797317668795586
iteration 288, loss = 0.00636790506541729
iteration 289, loss = 0.00570283317938447
iteration 290, loss = 0.008704260922968388
iteration 291, loss = 0.006266574840992689
iteration 292, loss = 0.005274753551930189
iteration 293, loss = 0.005581204313784838
iteration 294, loss = 0.005871822591871023
iteration 295, loss = 0.006185669451951981
iteration 296, loss = 0.00522714015096426
iteration 297, loss = 0.008060986176133156
iteration 298, loss = 0.004903340712189674
iteration 299, loss = 0.008890046738088131
iteration 300, loss = 0.005206583999097347
iteration 1, loss = 0.005846825893968344
iteration 2, loss = 0.007630314212292433
iteration 3, loss = 0.0054374863393604755
iteration 4, loss = 0.005259075667709112
iteration 5, loss = 0.007508091162890196
iteration 6, loss = 0.006788826547563076
iteration 7, loss = 0.008978174068033695
iteration 8, loss = 0.004787001758813858
iteration 9, loss = 0.00521160289645195
iteration 10, loss = 0.006467294879257679
iteration 11, loss = 0.004826451186090708
iteration 12, loss = 0.005393827334046364
iteration 13, loss = 0.007625975180417299
iteration 14, loss = 0.00935051403939724
iteration 15, loss = 0.005869440734386444
iteration 16, loss = 0.005698577966541052
iteration 17, loss = 0.006779536139219999
iteration 18, loss = 0.004980345722287893
iteration 19, loss = 0.005752261262387037
iteration 20, loss = 0.0065368469804525375
iteration 21, loss = 0.004938558209687471
iteration 22, loss = 0.005072786472737789
iteration 23, loss = 0.0050947405397892
iteration 24, loss = 0.005316043738275766
iteration 25, loss = 0.005167574156075716
iteration 26, loss = 0.006052399054169655
iteration 27, loss = 0.00482537318021059
iteration 28, loss = 0.0049690743908286095
iteration 29, loss = 0.004702018573880196
iteration 30, loss = 0.005809550639241934
iteration 31, loss = 0.005098407156765461
iteration 32, loss = 0.00503457710146904
iteration 33, loss = 0.005449394229799509
iteration 34, loss = 0.0049494244158267975
iteration 35, loss = 0.0050897132605314255
iteration 36, loss = 0.00501779792830348
iteration 37, loss = 0.005071620922535658
iteration 38, loss = 0.005706136580556631
iteration 39, loss = 0.005103162489831448
iteration 40, loss = 0.006109242327511311
iteration 41, loss = 0.005308019928634167
iteration 42, loss = 0.005627153906971216
iteration 43, loss = 0.00505799101665616
iteration 44, loss = 0.0071201748214662075
iteration 45, loss = 0.007099459413439035
iteration 46, loss = 0.004991003777831793
iteration 47, loss = 0.00584502425044775
iteration 48, loss = 0.004981016740202904
iteration 49, loss = 0.007161304820328951
iteration 50, loss = 0.0052677178755402565
iteration 51, loss = 0.005058254115283489
iteration 52, loss = 0.005661195609718561
iteration 53, loss = 0.005068468861281872
iteration 54, loss = 0.005107618402689695
iteration 55, loss = 0.0052338335663080215
iteration 56, loss = 0.005711173173040152
iteration 57, loss = 0.005421290639787912
iteration 58, loss = 0.005877729970961809
iteration 59, loss = 0.00543568842113018
iteration 60, loss = 0.0060721030458807945
iteration 61, loss = 0.00706863310188055
iteration 62, loss = 0.005368707701563835
iteration 63, loss = 0.00468411622568965
iteration 64, loss = 0.005148653406649828
iteration 65, loss = 0.005203153006732464
iteration 66, loss = 0.006463998928666115
iteration 67, loss = 0.005342921707779169
iteration 68, loss = 0.007163872942328453
iteration 69, loss = 0.004976192023605108
iteration 70, loss = 0.005267962347716093
iteration 71, loss = 0.005220831371843815
iteration 72, loss = 0.004736640024930239
iteration 73, loss = 0.0058017391711473465
iteration 74, loss = 0.0055692242458462715
iteration 75, loss = 0.00940077006816864
iteration 76, loss = 0.005434971768409014
iteration 77, loss = 0.008401024155318737
iteration 78, loss = 0.005680081434547901
iteration 79, loss = 0.0049057612195611
iteration 80, loss = 0.004880670923739672
iteration 81, loss = 0.005828953813761473
iteration 82, loss = 0.005102367606014013
iteration 83, loss = 0.0053758881986141205
iteration 84, loss = 0.005161147564649582
iteration 85, loss = 0.005608313716948032
iteration 86, loss = 0.005443656351417303
iteration 87, loss = 0.005128687247633934
iteration 88, loss = 0.006178837735205889
iteration 89, loss = 0.005912873428314924
iteration 90, loss = 0.004786693025380373
iteration 91, loss = 0.005034335423260927
iteration 92, loss = 0.004843076225370169
iteration 93, loss = 0.006143391598016024
iteration 94, loss = 0.007079392205923796
iteration 95, loss = 0.006093889474868774
iteration 96, loss = 0.006149503402411938
iteration 97, loss = 0.004919522907584906
iteration 98, loss = 0.006331080105155706
iteration 99, loss = 0.0057073901407420635
iteration 100, loss = 0.008244856260716915
iteration 101, loss = 0.005699038505554199
iteration 102, loss = 0.008612859062850475
iteration 103, loss = 0.005460841115564108
iteration 104, loss = 0.005078268703073263
iteration 105, loss = 0.006573679391294718
iteration 106, loss = 0.00524174002930522
iteration 107, loss = 0.005721318535506725
iteration 108, loss = 0.005762736778706312
iteration 109, loss = 0.005073525011539459
iteration 110, loss = 0.007229745388031006
iteration 111, loss = 0.005399225279688835
iteration 112, loss = 0.005644785240292549
iteration 113, loss = 0.005237082485109568
iteration 114, loss = 0.008846473880112171
iteration 115, loss = 0.005320434458553791
iteration 116, loss = 0.007979227229952812
iteration 117, loss = 0.004712844267487526
iteration 118, loss = 0.005510847549885511
iteration 119, loss = 0.005575069226324558
iteration 120, loss = 0.0054620420560240746
iteration 121, loss = 0.006196489091962576
iteration 122, loss = 0.005223623476922512
iteration 123, loss = 0.0049919262528419495
iteration 124, loss = 0.006852410733699799
iteration 125, loss = 0.0056130606681108475
iteration 126, loss = 0.005341320764273405
iteration 127, loss = 0.007681684102863073
iteration 128, loss = 0.004980210214853287
iteration 129, loss = 0.005226758308708668
iteration 130, loss = 0.004966599401086569
iteration 131, loss = 0.005424892995506525
iteration 132, loss = 0.007748117670416832
iteration 133, loss = 0.005330848041921854
iteration 134, loss = 0.00602598674595356
iteration 135, loss = 0.005619897041469812
iteration 136, loss = 0.010648342780768871
iteration 137, loss = 0.005399034358561039
iteration 138, loss = 0.005547060165554285
iteration 139, loss = 0.0056960019282996655
iteration 140, loss = 0.005088744685053825
iteration 141, loss = 0.005669443868100643
iteration 142, loss = 0.008567700162529945
iteration 143, loss = 0.0050786565989255905
iteration 144, loss = 0.005397840403020382
iteration 145, loss = 0.005518012680113316
iteration 146, loss = 0.005045153200626373
iteration 147, loss = 0.006018177606165409
iteration 148, loss = 0.007359670475125313
iteration 149, loss = 0.005155863240361214
iteration 150, loss = 0.007892386056482792
iteration 151, loss = 0.004641192499548197
iteration 152, loss = 0.007247270084917545
iteration 153, loss = 0.004976572934538126
iteration 154, loss = 0.005744842812418938
iteration 155, loss = 0.005058403592556715
iteration 156, loss = 0.005368494428694248
iteration 157, loss = 0.005596548784524202
iteration 158, loss = 0.004895463585853577
iteration 159, loss = 0.004766351543366909
iteration 160, loss = 0.0051812962628901005
iteration 161, loss = 0.004803282208740711
iteration 162, loss = 0.004968310706317425
iteration 163, loss = 0.005233682692050934
iteration 164, loss = 0.0051263985224068165
iteration 165, loss = 0.0050587281584739685
iteration 166, loss = 0.005054458510130644
iteration 167, loss = 0.005328387022018433
iteration 168, loss = 0.005571738816797733
iteration 169, loss = 0.0059983255341649055
iteration 170, loss = 0.005883630830794573
iteration 171, loss = 0.007648366503417492
iteration 172, loss = 0.004987554624676704
iteration 173, loss = 0.005766497924923897
iteration 174, loss = 0.004952539689838886
iteration 175, loss = 0.005369267892092466
iteration 176, loss = 0.0051053836941719055
iteration 177, loss = 0.006940433289855719
iteration 178, loss = 0.00563906179741025
iteration 179, loss = 0.005192737560719252
iteration 180, loss = 0.005036122165620327
iteration 181, loss = 0.005946418270468712
iteration 182, loss = 0.005058898124843836
iteration 183, loss = 0.00478860130533576
iteration 184, loss = 0.005666700191795826
iteration 185, loss = 0.005252165254205465
iteration 186, loss = 0.005052605178207159
iteration 187, loss = 0.005181068554520607
iteration 188, loss = 0.00561061967164278
iteration 189, loss = 0.005131504964083433
iteration 190, loss = 0.006735824979841709
iteration 191, loss = 0.005571683868765831
iteration 192, loss = 0.007803220767527819
iteration 193, loss = 0.005452771671116352
iteration 194, loss = 0.004832822363823652
iteration 195, loss = 0.005710910074412823
iteration 196, loss = 0.005060051567852497
iteration 197, loss = 0.005459871608763933
iteration 198, loss = 0.005329279229044914
iteration 199, loss = 0.00511230668053031
iteration 200, loss = 0.006144574843347073
iteration 201, loss = 0.006622678600251675
iteration 202, loss = 0.005024538841098547
iteration 203, loss = 0.004619119223207235
iteration 204, loss = 0.004929114133119583
iteration 205, loss = 0.004868128802627325
iteration 206, loss = 0.005064801778644323
iteration 207, loss = 0.00511485431343317
iteration 208, loss = 0.009986700490117073
iteration 209, loss = 0.007673678454011679
iteration 210, loss = 0.0051861475221812725
iteration 211, loss = 0.005112428218126297
iteration 212, loss = 0.007869778200984001
iteration 213, loss = 0.005202685482800007
iteration 214, loss = 0.005790597293525934
iteration 215, loss = 0.006008422467857599
iteration 216, loss = 0.005350065417587757
iteration 217, loss = 0.005089105106890202
iteration 218, loss = 0.00514306640252471
iteration 219, loss = 0.00484421756118536
iteration 220, loss = 0.005026584956794977
iteration 221, loss = 0.0056167542934417725
iteration 222, loss = 0.005543599370867014
iteration 223, loss = 0.005181669257581234
iteration 224, loss = 0.0054618422873318195
iteration 225, loss = 0.0057722353376448154
iteration 226, loss = 0.00788963120430708
iteration 227, loss = 0.004870423581451178
iteration 228, loss = 0.005576685536652803
iteration 229, loss = 0.004882706329226494
iteration 230, loss = 0.006963780149817467
iteration 231, loss = 0.005121788941323757
iteration 232, loss = 0.006091716233640909
iteration 233, loss = 0.0051556844264268875
iteration 234, loss = 0.0056432392448186874
iteration 235, loss = 0.005258625838905573
iteration 236, loss = 0.005371714010834694
iteration 237, loss = 0.008477185852825642
iteration 238, loss = 0.004934163298457861
iteration 239, loss = 0.005712675862014294
iteration 240, loss = 0.006262548267841339
iteration 241, loss = 0.006324100308120251
iteration 242, loss = 0.005076224450021982
iteration 243, loss = 0.005261977668851614
iteration 244, loss = 0.005976338870823383
iteration 245, loss = 0.005586894694715738
iteration 246, loss = 0.006693091243505478
iteration 247, loss = 0.0058283936232328415
iteration 248, loss = 0.004810649901628494
iteration 249, loss = 0.00556629104539752
iteration 250, loss = 0.004830393474549055
iteration 251, loss = 0.005840610712766647
iteration 252, loss = 0.005079981870949268
iteration 253, loss = 0.005051253363490105
iteration 254, loss = 0.005703882779926062
iteration 255, loss = 0.0051352824084460735
iteration 256, loss = 0.005501834210008383
iteration 257, loss = 0.00572972372174263
iteration 258, loss = 0.004764976911246777
iteration 259, loss = 0.005391629412770271
iteration 260, loss = 0.004949020221829414
iteration 261, loss = 0.006024794187396765
iteration 262, loss = 0.005833033937960863
iteration 263, loss = 0.004821524024009705
iteration 264, loss = 0.0054690721444785595
iteration 265, loss = 0.004824532195925713
iteration 266, loss = 0.005454222206026316
iteration 267, loss = 0.0061989035457372665
iteration 268, loss = 0.0050691175274550915
iteration 269, loss = 0.007236673962324858
iteration 270, loss = 0.004897491540759802
iteration 271, loss = 0.005936744157224894
iteration 272, loss = 0.006018082611262798
iteration 273, loss = 0.00772120663896203
iteration 274, loss = 0.007178580854088068
iteration 275, loss = 0.0050840298645198345
iteration 276, loss = 0.006057417951524258
iteration 277, loss = 0.0059409248642623425
iteration 278, loss = 0.005474809557199478
iteration 279, loss = 0.005198601633310318
iteration 280, loss = 0.005675426684319973
iteration 281, loss = 0.005511879920959473
iteration 282, loss = 0.004818411078304052
iteration 283, loss = 0.0056531997397542
iteration 284, loss = 0.005430799443274736
iteration 285, loss = 0.0069763134233653545
iteration 286, loss = 0.005857765208929777
iteration 287, loss = 0.005728446412831545
iteration 288, loss = 0.007519016973674297
iteration 289, loss = 0.005500809755176306
iteration 290, loss = 0.006319431588053703
iteration 291, loss = 0.007261066231876612
iteration 292, loss = 0.005918790586292744
iteration 293, loss = 0.005885239690542221
iteration 294, loss = 0.005500454921275377
iteration 295, loss = 0.008480871096253395
iteration 296, loss = 0.005286077503114939
iteration 297, loss = 0.0067029474303126335
iteration 298, loss = 0.00511718587949872
iteration 299, loss = 0.005786332301795483
iteration 300, loss = 0.007409354671835899
iteration 1, loss = 0.005287900101393461
iteration 2, loss = 0.004849704448133707
iteration 3, loss = 0.005378303118050098
iteration 4, loss = 0.00563401635736227
iteration 5, loss = 0.005853202193975449
iteration 6, loss = 0.005769138224422932
iteration 7, loss = 0.005290897563099861
iteration 8, loss = 0.005287675186991692
iteration 9, loss = 0.005843657068908215
iteration 10, loss = 0.004913415759801865
iteration 11, loss = 0.0050125583074986935
iteration 12, loss = 0.0056495582684874535
iteration 13, loss = 0.006477858405560255
iteration 14, loss = 0.005194430705159903
iteration 15, loss = 0.006155303679406643
iteration 16, loss = 0.005106582306325436
iteration 17, loss = 0.005126462783664465
iteration 18, loss = 0.005006539635360241
iteration 19, loss = 0.006990180350840092
iteration 20, loss = 0.0056644161231815815
iteration 21, loss = 0.008554354310035706
iteration 22, loss = 0.005021984688937664
iteration 23, loss = 0.004977178759872913
iteration 24, loss = 0.007820432074368
iteration 25, loss = 0.005925461649894714
iteration 26, loss = 0.0058656963519752026
iteration 27, loss = 0.006215330213308334
iteration 28, loss = 0.0057017141953110695
iteration 29, loss = 0.006011981517076492
iteration 30, loss = 0.005423802882432938
iteration 31, loss = 0.005172002594918013
iteration 32, loss = 0.005312277935445309
iteration 33, loss = 0.005037609022110701
iteration 34, loss = 0.0051055774092674255
iteration 35, loss = 0.00535957096144557
iteration 36, loss = 0.008046241477131844
iteration 37, loss = 0.004833400249481201
iteration 38, loss = 0.006952316965907812
iteration 39, loss = 0.004861676599830389
iteration 40, loss = 0.006011778488755226
iteration 41, loss = 0.00515283178538084
iteration 42, loss = 0.004929590038955212
iteration 43, loss = 0.00500079058110714
iteration 44, loss = 0.005038789939135313
iteration 45, loss = 0.005215540062636137
iteration 46, loss = 0.006098901387304068
iteration 47, loss = 0.006016911007463932
iteration 48, loss = 0.006014346145093441
iteration 49, loss = 0.006343407556414604
iteration 50, loss = 0.005238317418843508
iteration 51, loss = 0.005401331000030041
iteration 52, loss = 0.004865657072514296
iteration 53, loss = 0.005834802053868771
iteration 54, loss = 0.006024624221026897
iteration 55, loss = 0.0062106214463710785
iteration 56, loss = 0.00514512462541461
iteration 57, loss = 0.005850636400282383
iteration 58, loss = 0.005173253361135721
iteration 59, loss = 0.005049338098615408
iteration 60, loss = 0.005619832780212164
iteration 61, loss = 0.007028681226074696
iteration 62, loss = 0.005276388023048639
iteration 63, loss = 0.005814059637486935
iteration 64, loss = 0.005055589135736227
iteration 65, loss = 0.005026227794587612
iteration 66, loss = 0.004882085137069225
iteration 67, loss = 0.00888064131140709
iteration 68, loss = 0.005170173477381468
iteration 69, loss = 0.006743425969034433
iteration 70, loss = 0.0057480838149785995
iteration 71, loss = 0.00727427564561367
iteration 72, loss = 0.005109173245728016
iteration 73, loss = 0.004999156109988689
iteration 74, loss = 0.005986433941870928
iteration 75, loss = 0.006394363008439541
iteration 76, loss = 0.008034566417336464
iteration 77, loss = 0.007712323684245348
iteration 78, loss = 0.0053657907992601395
iteration 79, loss = 0.006587682757526636
iteration 80, loss = 0.004968895111232996
iteration 81, loss = 0.005933268461376429
iteration 82, loss = 0.004925982560962439
iteration 83, loss = 0.006462357006967068
iteration 84, loss = 0.0051247007213532925
iteration 85, loss = 0.0055924225598573685
iteration 86, loss = 0.005304556805640459
iteration 87, loss = 0.005125925410538912
iteration 88, loss = 0.004846426658332348
iteration 89, loss = 0.005135742481797934
iteration 90, loss = 0.00487877381965518
iteration 91, loss = 0.00613120011985302
iteration 92, loss = 0.007099391892552376
iteration 93, loss = 0.006532400380820036
iteration 94, loss = 0.005516992416232824
iteration 95, loss = 0.00472323689609766
iteration 96, loss = 0.005441729445010424
iteration 97, loss = 0.0047788964584469795
iteration 98, loss = 0.004842912778258324
iteration 99, loss = 0.0056023006327450275
iteration 100, loss = 0.00569191575050354
iteration 101, loss = 0.0048913173377513885
iteration 102, loss = 0.008145232684910297
iteration 103, loss = 0.00453589390963316
iteration 104, loss = 0.005937226116657257
iteration 105, loss = 0.004349722061306238
iteration 106, loss = 0.006344672758132219
iteration 107, loss = 0.005749384872615337
iteration 108, loss = 0.006114678923040628
iteration 109, loss = 0.006861262023448944
iteration 110, loss = 0.005312377121299505
iteration 111, loss = 0.007345959078520536
iteration 112, loss = 0.0061754826456308365
iteration 113, loss = 0.0075210039503872395
iteration 114, loss = 0.005500754341483116
iteration 115, loss = 0.0053563788533210754
iteration 116, loss = 0.007467670366168022
iteration 117, loss = 0.006088485475629568
iteration 118, loss = 0.005415462423115969
iteration 119, loss = 0.005006017163395882
iteration 120, loss = 0.005035040434449911
iteration 121, loss = 0.0052021886222064495
iteration 122, loss = 0.005281537305563688
iteration 123, loss = 0.00511706480756402
iteration 124, loss = 0.0047928825952112675
iteration 125, loss = 0.0055429465137422085
iteration 126, loss = 0.005548450630158186
iteration 127, loss = 0.005954468622803688
iteration 128, loss = 0.005129652563482523
iteration 129, loss = 0.005176910664886236
iteration 130, loss = 0.005299014039337635
iteration 131, loss = 0.008153364062309265
iteration 132, loss = 0.0054543716832995415
iteration 133, loss = 0.005332648288458586
iteration 134, loss = 0.005367416888475418
iteration 135, loss = 0.006006434094160795
iteration 136, loss = 0.005230581387877464
iteration 137, loss = 0.0077402363531291485
iteration 138, loss = 0.008511784486472607
iteration 139, loss = 0.004450114443898201
iteration 140, loss = 0.004926254507154226
iteration 141, loss = 0.005465781316161156
iteration 142, loss = 0.00531032495200634
iteration 143, loss = 0.0051310863345861435
iteration 144, loss = 0.007048207335174084
iteration 145, loss = 0.006762553006410599
iteration 146, loss = 0.0054909586906433105
iteration 147, loss = 0.006623239256441593
iteration 148, loss = 0.004946622997522354
iteration 149, loss = 0.006085247732698917
iteration 150, loss = 0.005403978284448385
iteration 151, loss = 0.005055887624621391
iteration 152, loss = 0.006930149160325527
iteration 153, loss = 0.007467253599315882
iteration 154, loss = 0.0052203224040567875
iteration 155, loss = 0.0046858517453074455
iteration 156, loss = 0.0054984199814498425
iteration 157, loss = 0.005799306556582451
iteration 158, loss = 0.005378042813390493
iteration 159, loss = 0.005425074137747288
iteration 160, loss = 0.0047379713505506516
iteration 161, loss = 0.004891631193459034
iteration 162, loss = 0.005027417559176683
iteration 163, loss = 0.006888729985803366
iteration 164, loss = 0.005424437578767538
iteration 165, loss = 0.0048079174011945724
iteration 166, loss = 0.0051494380459189415
iteration 167, loss = 0.0072410921566188335
iteration 168, loss = 0.004922168795019388
iteration 169, loss = 0.005941682029515505
iteration 170, loss = 0.0049086324870586395
iteration 171, loss = 0.00497372355312109
iteration 172, loss = 0.004966133739799261
iteration 173, loss = 0.005388653837144375
iteration 174, loss = 0.005254891235381365
iteration 175, loss = 0.005725808907300234
iteration 176, loss = 0.0075838519260287285
iteration 177, loss = 0.005385407246649265
iteration 178, loss = 0.006713594775646925
iteration 179, loss = 0.005161636974662542
iteration 180, loss = 0.005097686313092709
iteration 181, loss = 0.004646236542612314
iteration 182, loss = 0.005717060994356871
iteration 183, loss = 0.0070798275992274284
iteration 184, loss = 0.006420506164431572
iteration 185, loss = 0.005179861094802618
iteration 186, loss = 0.005029180087149143
iteration 187, loss = 0.005262009799480438
iteration 188, loss = 0.004976958967745304
iteration 189, loss = 0.00586231704801321
iteration 190, loss = 0.005332660861313343
iteration 191, loss = 0.005557497497648001
iteration 192, loss = 0.005311089102178812
iteration 193, loss = 0.00618833489716053
iteration 194, loss = 0.00599915673956275
iteration 195, loss = 0.005373321007937193
iteration 196, loss = 0.005131894256919622
iteration 197, loss = 0.004596733953803778
iteration 198, loss = 0.005533836781978607
iteration 199, loss = 0.008340364322066307
iteration 200, loss = 0.0066361078061163425
iteration 201, loss = 0.005278528202325106
iteration 202, loss = 0.005865668877959251
iteration 203, loss = 0.005500132218003273
iteration 204, loss = 0.005291819106787443
iteration 205, loss = 0.005279209464788437
iteration 206, loss = 0.0050007496029138565
iteration 207, loss = 0.00707464013248682
iteration 208, loss = 0.005250049754977226
iteration 209, loss = 0.0049199629575014114
iteration 210, loss = 0.0077690379694104195
iteration 211, loss = 0.005104479845613241
iteration 212, loss = 0.006243033334612846
iteration 213, loss = 0.005282500758767128
iteration 214, loss = 0.006028459407389164
iteration 215, loss = 0.0057310075499117374
iteration 216, loss = 0.00599692203104496
iteration 217, loss = 0.0050863176584243774
iteration 218, loss = 0.004840311128646135
iteration 219, loss = 0.005011264234781265
iteration 220, loss = 0.004848109558224678
iteration 221, loss = 0.005253408569842577
iteration 222, loss = 0.0053246463648974895
iteration 223, loss = 0.005202595144510269
iteration 224, loss = 0.004898161627352238
iteration 225, loss = 0.005148519761860371
iteration 226, loss = 0.007496748119592667
iteration 227, loss = 0.004951260983943939
iteration 228, loss = 0.005074630957096815
iteration 229, loss = 0.005200347397476435
iteration 230, loss = 0.005798890721052885
iteration 231, loss = 0.005031722132116556
iteration 232, loss = 0.005033127963542938
iteration 233, loss = 0.0049303327687084675
iteration 234, loss = 0.005648958962410688
iteration 235, loss = 0.005583696998655796
iteration 236, loss = 0.005053067114204168
iteration 237, loss = 0.006725601851940155
iteration 238, loss = 0.005119197070598602
iteration 239, loss = 0.005520061124116182
iteration 240, loss = 0.0049766236916184425
iteration 241, loss = 0.005124259274452925
iteration 242, loss = 0.008170418441295624
iteration 243, loss = 0.0050031207501888275
iteration 244, loss = 0.004917324520647526
iteration 245, loss = 0.004664142150431871
iteration 246, loss = 0.005201630759984255
iteration 247, loss = 0.004820662550628185
iteration 248, loss = 0.0053205834701657295
iteration 249, loss = 0.005267046857625246
iteration 250, loss = 0.005498780403286219
iteration 251, loss = 0.005190153140574694
iteration 252, loss = 0.0069775087758898735
iteration 253, loss = 0.005805739667266607
iteration 254, loss = 0.007740055210888386
iteration 255, loss = 0.00541269313544035
iteration 256, loss = 0.005498744081705809
iteration 257, loss = 0.004957075230777264
iteration 258, loss = 0.005016915965825319
iteration 259, loss = 0.005552845541387796
iteration 260, loss = 0.0071550593711435795
iteration 261, loss = 0.007327520754188299
iteration 262, loss = 0.0049673630855977535
iteration 263, loss = 0.0050164503045380116
iteration 264, loss = 0.0061799306422472
iteration 265, loss = 0.005156280938535929
iteration 266, loss = 0.004861454013735056
iteration 267, loss = 0.0048433272168040276
iteration 268, loss = 0.004781825467944145
iteration 269, loss = 0.005999590735882521
iteration 270, loss = 0.007706769276410341
iteration 271, loss = 0.007021715398877859
iteration 272, loss = 0.005655345506966114
iteration 273, loss = 0.004850062076002359
iteration 274, loss = 0.004632648080587387
iteration 275, loss = 0.005887551698833704
iteration 276, loss = 0.0049019004218280315
iteration 277, loss = 0.005051648244261742
iteration 278, loss = 0.005012684967368841
iteration 279, loss = 0.005141168832778931
iteration 280, loss = 0.00659142853692174
iteration 281, loss = 0.005375248845666647
iteration 282, loss = 0.005085459910333157
iteration 283, loss = 0.005266913678497076
iteration 284, loss = 0.005232241004705429
iteration 285, loss = 0.005258195102214813
iteration 286, loss = 0.005907509010285139
iteration 287, loss = 0.005126852076500654
iteration 288, loss = 0.007888942025601864
iteration 289, loss = 0.006516892928630114
iteration 290, loss = 0.005041566677391529
iteration 291, loss = 0.004691580310463905
iteration 292, loss = 0.004916658625006676
iteration 293, loss = 0.005775504279881716
iteration 294, loss = 0.008986803703010082
iteration 295, loss = 0.0048589217476546764
iteration 296, loss = 0.005342905875295401
iteration 297, loss = 0.005754877347499132
iteration 298, loss = 0.004590000491589308
iteration 299, loss = 0.004729369189590216
iteration 300, loss = 0.005618337541818619
>>>>>>> parent of f965dc1... add test ind at each sample time; increase the ratio;

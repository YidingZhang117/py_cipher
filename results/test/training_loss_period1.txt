iteration 0, loss = 0.5024135112762451
iteration 1, loss = 0.5146511197090149
iteration 2, loss = 0.49468934535980225
iteration 3, loss = 0.5039638876914978
iteration 4, loss = 0.5010179281234741
iteration 5, loss = 0.4966855049133301
iteration 6, loss = 0.5127118229866028
iteration 7, loss = 0.4947258532047272
iteration 8, loss = 0.4997156858444214
iteration 9, loss = 0.5030186772346497
iteration 10, loss = 0.501270592212677
iteration 11, loss = 0.509520947933197
iteration 12, loss = 0.4981690049171448
iteration 13, loss = 0.4990529716014862
iteration 14, loss = 0.5034489035606384
iteration 15, loss = 0.5093585252761841
iteration 16, loss = 0.5079641342163086
iteration 17, loss = 0.5062559843063354
iteration 18, loss = 0.5009077787399292
iteration 19, loss = 0.4958633780479431
iteration 20, loss = 0.5049509406089783
iteration 21, loss = 0.504237711429596
iteration 22, loss = 0.5093956589698792
iteration 23, loss = 0.5058478116989136
iteration 24, loss = 0.4982717037200928
iteration 25, loss = 0.5108598470687866
iteration 26, loss = 0.4994737505912781
iteration 27, loss = 0.504886269569397
iteration 28, loss = 0.5043976902961731
iteration 29, loss = 0.5085098743438721
iteration 30, loss = 0.5023455619812012
iteration 31, loss = 0.500191330909729
iteration 32, loss = 0.4988801181316376
iteration 33, loss = 0.49667686223983765
iteration 34, loss = 0.49301400780677795
iteration 35, loss = 0.498881995677948
iteration 36, loss = 0.4983808696269989
iteration 37, loss = 0.4999200403690338
iteration 38, loss = 0.5043569803237915
iteration 39, loss = 0.49676162004470825
iteration 40, loss = 0.49857044219970703
iteration 41, loss = 0.4914706349372864
iteration 42, loss = 0.5013347268104553
iteration 43, loss = 0.4972940683364868
iteration 44, loss = 0.4998418688774109
iteration 45, loss = 0.5007990598678589
iteration 46, loss = 0.49826541543006897
iteration 47, loss = 0.5058172941207886
iteration 48, loss = 0.5015175938606262
iteration 49, loss = 0.4936073422431946
iteration 50, loss = 0.5011476278305054
iteration 51, loss = 0.5094377994537354
iteration 52, loss = 0.49428990483283997
iteration 53, loss = 0.502167820930481
iteration 54, loss = 0.5045061707496643
iteration 55, loss = 0.4958033859729767
iteration 56, loss = 0.49684035778045654
iteration 57, loss = 0.49627336859703064
iteration 58, loss = 0.5043299198150635
iteration 59, loss = 0.4971485137939453
iteration 60, loss = 0.49849969148635864
iteration 61, loss = 0.5038638114929199
iteration 62, loss = 0.5006264448165894
iteration 63, loss = 0.5029013156890869
iteration 64, loss = 0.49967706203460693
iteration 65, loss = 0.5060415267944336
iteration 66, loss = 0.5004655122756958
iteration 67, loss = 0.4984925389289856
iteration 68, loss = 0.49954867362976074
iteration 69, loss = 0.5012417435646057
iteration 70, loss = 0.49556195735931396
iteration 71, loss = 0.4978531002998352
iteration 72, loss = 0.4995991885662079
iteration 73, loss = 0.49820977449417114
iteration 74, loss = 0.5042697191238403
iteration 75, loss = 0.49818095564842224
iteration 76, loss = 0.49374520778656006
iteration 77, loss = 0.4975724220275879
iteration 78, loss = 0.4998445510864258
iteration 79, loss = 0.5044460296630859
iteration 80, loss = 0.4950382709503174
iteration 81, loss = 0.49757689237594604
iteration 82, loss = 0.5021635890007019
iteration 83, loss = 0.4994281530380249
iteration 84, loss = 0.502724826335907
iteration 85, loss = 0.5013664364814758
iteration 86, loss = 0.501542866230011
iteration 87, loss = 0.503387451171875
iteration 88, loss = 0.5010956525802612
iteration 89, loss = 0.49830180406570435
iteration 90, loss = 0.4991421103477478
iteration 91, loss = 0.5005035400390625
iteration 92, loss = 0.4954279363155365
iteration 93, loss = 0.4970265030860901
iteration 94, loss = 0.5030158162117004
iteration 95, loss = 0.49797457456588745
iteration 96, loss = 0.494492769241333
iteration 97, loss = 0.5057306885719299
iteration 98, loss = 0.49748438596725464
iteration 99, loss = 0.4990463852882385
iteration 100, loss = 0.4995236396789551
iteration 101, loss = 0.4999237656593323
iteration 102, loss = 0.4938355088233948
iteration 103, loss = 0.499022901058197
iteration 104, loss = 0.4949258267879486
iteration 105, loss = 0.49843207001686096
iteration 106, loss = 0.5005921721458435
iteration 107, loss = 0.497572124004364
iteration 108, loss = 0.4931253492832184
iteration 109, loss = 0.4946087598800659
iteration 110, loss = 0.5012426376342773
iteration 111, loss = 0.49678337574005127
iteration 112, loss = 0.4997364282608032
iteration 113, loss = 0.49795001745224
iteration 114, loss = 0.5023699998855591
iteration 115, loss = 0.4954967498779297
iteration 116, loss = 0.496573805809021
iteration 117, loss = 0.49826639890670776
iteration 118, loss = 0.49352747201919556
iteration 119, loss = 0.49647656083106995
iteration 120, loss = 0.4964771270751953
iteration 121, loss = 0.49360889196395874
iteration 122, loss = 0.49386319518089294
iteration 123, loss = 0.49826580286026
iteration 124, loss = 0.4974678158760071
iteration 125, loss = 0.4950953722000122
iteration 126, loss = 0.49840879440307617
iteration 127, loss = 0.49958735704421997
iteration 128, loss = 0.505420982837677
iteration 129, loss = 0.5025379657745361
iteration 130, loss = 0.4981498122215271
iteration 131, loss = 0.4961513876914978
iteration 132, loss = 0.4979150891304016
iteration 133, loss = 0.5017418265342712
iteration 134, loss = 0.4947170615196228
iteration 135, loss = 0.49894529581069946
iteration 136, loss = 0.5025967359542847
iteration 137, loss = 0.4938108026981354
iteration 138, loss = 0.5004996657371521
iteration 139, loss = 0.4957025349140167
iteration 140, loss = 0.49173712730407715
iteration 141, loss = 0.4939141273498535
iteration 142, loss = 0.5007266998291016
iteration 143, loss = 0.49719008803367615
iteration 144, loss = 0.49585381150245667
iteration 145, loss = 0.49655210971832275
iteration 146, loss = 0.4998230040073395
iteration 147, loss = 0.5005737543106079
iteration 148, loss = 0.4956212043762207
iteration 149, loss = 0.497474730014801
iteration 150, loss = 0.4966968297958374
iteration 151, loss = 0.4989340901374817
iteration 152, loss = 0.5025155544281006
iteration 153, loss = 0.49194806814193726
iteration 154, loss = 0.49574217200279236
iteration 155, loss = 0.49754011631011963
iteration 156, loss = 0.5013729929924011
iteration 157, loss = 0.4993700087070465
iteration 158, loss = 0.49484169483184814
iteration 159, loss = 0.4897945523262024
iteration 160, loss = 0.49457865953445435
iteration 161, loss = 0.5000402331352234
iteration 162, loss = 0.4988476634025574
iteration 163, loss = 0.49546903371810913
iteration 164, loss = 0.5023003816604614
iteration 165, loss = 0.49784770607948303
iteration 166, loss = 0.49847909808158875
iteration 167, loss = 0.5002845525741577
iteration 168, loss = 0.4974513649940491
iteration 169, loss = 0.4963425099849701
iteration 170, loss = 0.4954013526439667
iteration 171, loss = 0.4962639808654785
iteration 172, loss = 0.5021612644195557
iteration 173, loss = 0.49234575033187866
iteration 174, loss = 0.49911069869995117
iteration 175, loss = 0.4953768253326416
iteration 176, loss = 0.4988517165184021
iteration 177, loss = 0.49128443002700806
iteration 178, loss = 0.4963147044181824
iteration 179, loss = 0.4921952486038208
iteration 180, loss = 0.4908795952796936
iteration 181, loss = 0.4920697808265686
iteration 182, loss = 0.49408093094825745
iteration 183, loss = 0.5061123967170715
iteration 184, loss = 0.4950883388519287
iteration 185, loss = 0.4958505630493164
iteration 186, loss = 0.4964984059333801
iteration 187, loss = 0.5032941102981567
iteration 188, loss = 0.4900893270969391
iteration 189, loss = 0.49512550234794617
iteration 190, loss = 0.49619966745376587
iteration 191, loss = 0.5026219487190247
iteration 192, loss = 0.4957042932510376
iteration 193, loss = 0.49127376079559326
iteration 194, loss = 0.49487757682800293
iteration 195, loss = 0.503268837928772
iteration 196, loss = 0.4973783493041992
iteration 197, loss = 0.4946739375591278
iteration 198, loss = 0.49949318170547485
iteration 199, loss = 0.49592506885528564
iteration 200, loss = 0.5002663135528564
iteration 201, loss = 0.5008798837661743
iteration 202, loss = 0.5034013986587524
iteration 203, loss = 0.4918137788772583
iteration 204, loss = 0.4878484904766083
iteration 205, loss = 0.49910300970077515
iteration 206, loss = 0.4924706518650055
iteration 207, loss = 0.5057412385940552
iteration 208, loss = 0.4997357130050659
iteration 209, loss = 0.5005823373794556
iteration 210, loss = 0.4934957027435303
iteration 211, loss = 0.49560779333114624
iteration 212, loss = 0.5056573748588562
iteration 213, loss = 0.48991671204566956
iteration 214, loss = 0.4906361997127533
iteration 215, loss = 0.4927491545677185
iteration 216, loss = 0.49761635065078735
iteration 217, loss = 0.49181413650512695
iteration 218, loss = 0.4898795187473297
iteration 219, loss = 0.496036171913147
iteration 220, loss = 0.5029305815696716
iteration 221, loss = 0.4924408793449402
iteration 222, loss = 0.504483163356781
iteration 223, loss = 0.5013751983642578
iteration 224, loss = 0.4895063042640686
iteration 225, loss = 0.506030797958374
iteration 226, loss = 0.4979427754878998
iteration 227, loss = 0.4909464120864868
iteration 228, loss = 0.4912809133529663
iteration 229, loss = 0.5045562386512756
iteration 230, loss = 0.48826324939727783
iteration 231, loss = 0.49708327651023865
iteration 232, loss = 0.49744200706481934
iteration 233, loss = 0.504318118095398
iteration 234, loss = 0.495307981967926
iteration 235, loss = 0.49899208545684814
iteration 236, loss = 0.501207172870636
iteration 237, loss = 0.49274879693984985
iteration 238, loss = 0.4966985285282135
iteration 239, loss = 0.49670666456222534
iteration 240, loss = 0.49210384488105774
iteration 241, loss = 0.49516502022743225
iteration 242, loss = 0.496174693107605
iteration 243, loss = 0.5023943185806274
iteration 244, loss = 0.4910106658935547
iteration 245, loss = 0.4933948516845703
iteration 246, loss = 0.5045093894004822
iteration 247, loss = 0.49343425035476685
iteration 248, loss = 0.48827552795410156
iteration 249, loss = 0.49640119075775146
iteration 250, loss = 0.4972270131111145
iteration 251, loss = 0.4978055953979492
iteration 252, loss = 0.48842716217041016
iteration 253, loss = 0.49812451004981995
iteration 254, loss = 0.4929491877555847
iteration 255, loss = 0.5015904903411865
iteration 256, loss = 0.4935539960861206
iteration 257, loss = 0.49890029430389404
iteration 258, loss = 0.5004231333732605
iteration 259, loss = 0.49596357345581055
iteration 260, loss = 0.5001100301742554
iteration 261, loss = 0.4980868995189667
iteration 262, loss = 0.4975043535232544
iteration 263, loss = 0.49777013063430786
iteration 264, loss = 0.4829065203666687
iteration 265, loss = 0.49180206656455994
iteration 266, loss = 0.49944037199020386
iteration 267, loss = 0.4967968463897705
iteration 268, loss = 0.4985726475715637
iteration 269, loss = 0.48557543754577637
iteration 270, loss = 0.5017277002334595
iteration 271, loss = 0.4896835684776306
iteration 272, loss = 0.49390164017677307
iteration 273, loss = 0.502215564250946
iteration 274, loss = 0.49540483951568604
iteration 275, loss = 0.5028575658798218
iteration 276, loss = 0.5005754828453064
iteration 277, loss = 0.4977375864982605
iteration 278, loss = 0.49840235710144043
iteration 279, loss = 0.49296462535858154
iteration 280, loss = 0.5019718408584595
iteration 281, loss = 0.49559998512268066
iteration 282, loss = 0.49764370918273926
iteration 283, loss = 0.4959228038787842
iteration 284, loss = 0.49812519550323486
iteration 285, loss = 0.4973880648612976
iteration 286, loss = 0.4973164498806
iteration 287, loss = 0.4974132180213928
iteration 288, loss = 0.503439724445343
iteration 289, loss = 0.49581336975097656
iteration 290, loss = 0.4943277835845947
iteration 291, loss = 0.49269241094589233
iteration 292, loss = 0.5007268190383911
iteration 293, loss = 0.49514609575271606
iteration 294, loss = 0.502835750579834
iteration 295, loss = 0.4957777261734009
iteration 296, loss = 0.5002057552337646
iteration 297, loss = 0.49397605657577515
iteration 298, loss = 0.4934172034263611
iteration 299, loss = 0.4912618398666382
iteration 0, loss = 0.4886206388473511
iteration 1, loss = 0.49424856901168823
iteration 2, loss = 0.4945041537284851
iteration 3, loss = 0.4968797564506531
iteration 4, loss = 0.500494658946991
iteration 5, loss = 0.5000772476196289
iteration 6, loss = 0.49277764558792114
iteration 7, loss = 0.49296867847442627
iteration 8, loss = 0.4981510043144226
iteration 9, loss = 0.4925230145454407
iteration 10, loss = 0.49013495445251465
iteration 11, loss = 0.5018002986907959
iteration 12, loss = 0.48502203822135925
iteration 13, loss = 0.49715203046798706
iteration 14, loss = 0.48800331354141235
iteration 15, loss = 0.49349549412727356
iteration 16, loss = 0.4886859655380249
iteration 17, loss = 0.4933392405509949
iteration 18, loss = 0.49744099378585815
iteration 19, loss = 0.49547117948532104
iteration 20, loss = 0.4894298315048218
iteration 21, loss = 0.501239001750946
iteration 22, loss = 0.49430298805236816
iteration 23, loss = 0.49317556619644165
iteration 24, loss = 0.48451465368270874
iteration 25, loss = 0.5100322961807251
iteration 26, loss = 0.5075139999389648
iteration 27, loss = 0.49986833333969116
iteration 28, loss = 0.49809324741363525
iteration 29, loss = 0.48765918612480164
iteration 30, loss = 0.4959579408168793
iteration 31, loss = 0.4986881613731384
iteration 32, loss = 0.4926697611808777
iteration 33, loss = 0.4981837272644043
iteration 34, loss = 0.5034358501434326
iteration 35, loss = 0.5096681118011475
iteration 36, loss = 0.49531784653663635
iteration 37, loss = 0.4958553910255432
iteration 38, loss = 0.49756720662117004
iteration 39, loss = 0.4986516237258911
iteration 40, loss = 0.4894396662712097
iteration 41, loss = 0.4867013990879059
iteration 42, loss = 0.5018308162689209
iteration 43, loss = 0.4873190224170685
iteration 44, loss = 0.4945095181465149
iteration 45, loss = 0.49762392044067383
iteration 46, loss = 0.4916853904724121
iteration 47, loss = 0.4937760829925537
iteration 48, loss = 0.48912832140922546
iteration 49, loss = 0.48792701959609985
iteration 50, loss = 0.4932848811149597
iteration 51, loss = 0.49021682143211365
iteration 52, loss = 0.49183204770088196
iteration 53, loss = 0.5043119788169861
iteration 54, loss = 0.4936321973800659
iteration 55, loss = 0.5008234977722168
iteration 56, loss = 0.49608880281448364
iteration 57, loss = 0.4981682002544403
iteration 58, loss = 0.5039597153663635
iteration 59, loss = 0.5011644959449768
iteration 60, loss = 0.4993370771408081
iteration 61, loss = 0.4990032911300659
iteration 62, loss = 0.5028655529022217
iteration 63, loss = 0.4993608593940735
iteration 64, loss = 0.4969504475593567
iteration 65, loss = 0.49543076753616333
iteration 66, loss = 0.49887987971305847
iteration 67, loss = 0.500420868396759
iteration 68, loss = 0.4934229254722595
iteration 69, loss = 0.49938827753067017
iteration 70, loss = 0.49256768822669983
iteration 71, loss = 0.4857686758041382
iteration 72, loss = 0.4980698227882385
iteration 73, loss = 0.4992660582065582
iteration 74, loss = 0.49433934688568115
iteration 75, loss = 0.4946563243865967
iteration 76, loss = 0.4997684955596924
iteration 77, loss = 0.506754994392395
iteration 78, loss = 0.4893335700035095
iteration 79, loss = 0.4925670623779297
iteration 80, loss = 0.49888432025909424
iteration 81, loss = 0.48759034276008606
iteration 82, loss = 0.49161550402641296
iteration 83, loss = 0.49573421478271484
iteration 84, loss = 0.5054669380187988
iteration 85, loss = 0.49419546127319336
iteration 86, loss = 0.5010963082313538
iteration 87, loss = 0.49329450726509094
iteration 88, loss = 0.4900778532028198
iteration 89, loss = 0.4852037727832794
iteration 90, loss = 0.4877473711967468
iteration 91, loss = 0.49161893129348755
iteration 92, loss = 0.4977295398712158
iteration 93, loss = 0.4963633418083191
iteration 94, loss = 0.49219948053359985
iteration 95, loss = 0.49432000517845154
iteration 96, loss = 0.4986835718154907
iteration 97, loss = 0.48929280042648315
iteration 98, loss = 0.5028356313705444
iteration 99, loss = 0.49132072925567627
iteration 100, loss = 0.4980245530605316
iteration 101, loss = 0.4941597580909729
iteration 102, loss = 0.495929479598999
iteration 103, loss = 0.4882426857948303
iteration 104, loss = 0.5019724369049072
iteration 105, loss = 0.498530775308609
iteration 106, loss = 0.4967333674430847
iteration 107, loss = 0.4961429238319397
iteration 108, loss = 0.49694496393203735
iteration 109, loss = 0.4932991862297058
iteration 110, loss = 0.5005390644073486
iteration 111, loss = 0.49884527921676636
iteration 112, loss = 0.49695396423339844
iteration 113, loss = 0.498424768447876
iteration 114, loss = 0.49435052275657654
iteration 115, loss = 0.49388793110847473
iteration 116, loss = 0.49332141876220703
iteration 117, loss = 0.49513375759124756
iteration 118, loss = 0.4908296465873718
iteration 119, loss = 0.5077254772186279
iteration 120, loss = 0.48857223987579346
iteration 121, loss = 0.4952237904071808
iteration 122, loss = 0.4960474371910095
iteration 123, loss = 0.4844392240047455
iteration 124, loss = 0.4993969202041626
iteration 125, loss = 0.49181902408599854
iteration 126, loss = 0.4993867576122284
iteration 127, loss = 0.48682767152786255
iteration 128, loss = 0.48848459124565125
iteration 129, loss = 0.498964786529541
iteration 130, loss = 0.4956520199775696
iteration 131, loss = 0.49673908948898315
iteration 132, loss = 0.503292977809906
iteration 133, loss = 0.4916422367095947
iteration 134, loss = 0.49501052498817444
iteration 135, loss = 0.49258923530578613
iteration 136, loss = 0.4906346797943115
iteration 137, loss = 0.49902981519699097
iteration 138, loss = 0.5005785226821899
iteration 139, loss = 0.4838179051876068
iteration 140, loss = 0.48470646142959595
iteration 141, loss = 0.502444863319397
iteration 142, loss = 0.49746060371398926
iteration 143, loss = 0.49805349111557007
iteration 144, loss = 0.49115118384361267
iteration 145, loss = 0.4907207489013672
iteration 146, loss = 0.48645949363708496
iteration 147, loss = 0.4938367009162903
iteration 148, loss = 0.49423331022262573
iteration 149, loss = 0.4826669692993164
iteration 150, loss = 0.5057945847511292
iteration 151, loss = 0.4947206676006317
iteration 152, loss = 0.48876485228538513
iteration 153, loss = 0.5080423355102539
iteration 154, loss = 0.4896499514579773
iteration 155, loss = 0.4989706873893738
iteration 156, loss = 0.49065935611724854
iteration 157, loss = 0.5026903748512268
iteration 158, loss = 0.48778778314590454
iteration 159, loss = 0.4936031103134155
iteration 160, loss = 0.4919135868549347
iteration 161, loss = 0.49386054277420044
iteration 162, loss = 0.49820050597190857
iteration 163, loss = 0.5015385150909424
iteration 164, loss = 0.4977966547012329
iteration 165, loss = 0.48844850063323975
iteration 166, loss = 0.49923789501190186
iteration 167, loss = 0.4846208393573761
iteration 168, loss = 0.49358069896698
iteration 169, loss = 0.494498074054718
iteration 170, loss = 0.48176276683807373
iteration 171, loss = 0.4950309991836548
iteration 172, loss = 0.48347169160842896
iteration 173, loss = 0.4831446409225464
iteration 174, loss = 0.4841214418411255
iteration 175, loss = 0.4896692633628845
iteration 176, loss = 0.49304288625717163
iteration 177, loss = 0.4959245026111603
iteration 178, loss = 0.4997521638870239
iteration 179, loss = 0.4872535467147827
iteration 180, loss = 0.4920925199985504
iteration 181, loss = 0.4973754286766052
iteration 182, loss = 0.49665364623069763
iteration 183, loss = 0.48925480246543884
iteration 184, loss = 0.4992455840110779
iteration 185, loss = 0.4939563274383545
iteration 186, loss = 0.49320054054260254
iteration 187, loss = 0.4903101623058319
iteration 188, loss = 0.4859401285648346
iteration 189, loss = 0.4951110780239105
iteration 190, loss = 0.4856864809989929
iteration 191, loss = 0.49514269828796387
iteration 192, loss = 0.4901634454727173
iteration 193, loss = 0.49038994312286377
iteration 194, loss = 0.4845287501811981
iteration 195, loss = 0.4965960383415222
iteration 196, loss = 0.5053533911705017
iteration 197, loss = 0.5051447749137878
iteration 198, loss = 0.49751073122024536
iteration 199, loss = 0.4862155318260193
iteration 200, loss = 0.49919646978378296
iteration 201, loss = 0.499104380607605
iteration 202, loss = 0.5057826042175293
iteration 203, loss = 0.49250084161758423
iteration 204, loss = 0.4928555488586426
iteration 205, loss = 0.48994117975234985
iteration 206, loss = 0.4912826716899872
iteration 207, loss = 0.486777126789093
iteration 208, loss = 0.49676603078842163
iteration 209, loss = 0.5026363134384155
iteration 210, loss = 0.4808494448661804
iteration 211, loss = 0.5076286792755127
iteration 212, loss = 0.4911978840827942
iteration 213, loss = 0.4948342442512512
iteration 214, loss = 0.48915180563926697
iteration 215, loss = 0.4921497702598572
iteration 216, loss = 0.4957917332649231
iteration 217, loss = 0.4930616021156311
iteration 218, loss = 0.4999127984046936
iteration 219, loss = 0.5050473809242249
iteration 220, loss = 0.498064249753952
iteration 221, loss = 0.48841989040374756
iteration 222, loss = 0.49385035037994385
iteration 223, loss = 0.4956980347633362
iteration 224, loss = 0.489693284034729
iteration 225, loss = 0.47644123435020447
iteration 226, loss = 0.489856481552124
iteration 227, loss = 0.4942927062511444
iteration 228, loss = 0.4953892230987549
iteration 229, loss = 0.5014848709106445
iteration 230, loss = 0.47936761379241943
iteration 231, loss = 0.4913128912448883
iteration 232, loss = 0.490079402923584
iteration 233, loss = 0.4950826168060303
iteration 234, loss = 0.4995141625404358
iteration 235, loss = 0.49656593799591064
iteration 236, loss = 0.4886387586593628
iteration 237, loss = 0.4945603013038635
iteration 238, loss = 0.4847522974014282
iteration 239, loss = 0.4927349090576172
iteration 240, loss = 0.497924268245697
iteration 241, loss = 0.4899621903896332
iteration 242, loss = 0.49188289046287537
iteration 243, loss = 0.4959569573402405
iteration 244, loss = 0.5058736801147461
iteration 245, loss = 0.49310874938964844
iteration 246, loss = 0.4766058325767517
iteration 247, loss = 0.5026718974113464
iteration 248, loss = 0.4877464175224304
iteration 249, loss = 0.4985559284687042
iteration 250, loss = 0.4962373375892639
iteration 251, loss = 0.48167717456817627
iteration 252, loss = 0.49085766077041626
iteration 253, loss = 0.4864344000816345
iteration 254, loss = 0.48534145951271057
iteration 255, loss = 0.4886101484298706
iteration 256, loss = 0.4910232424736023
iteration 257, loss = 0.48865073919296265
iteration 258, loss = 0.4881381392478943
iteration 259, loss = 0.5058277249336243
iteration 260, loss = 0.4909915626049042
iteration 261, loss = 0.49557915329933167
iteration 262, loss = 0.482970654964447
iteration 263, loss = 0.487104594707489
iteration 264, loss = 0.48446327447891235
iteration 265, loss = 0.4815947711467743
iteration 266, loss = 0.4902796149253845
iteration 267, loss = 0.4957692325115204
iteration 268, loss = 0.5053625106811523
iteration 269, loss = 0.4997944235801697
iteration 270, loss = 0.4895316958427429
iteration 271, loss = 0.49196428060531616
iteration 272, loss = 0.4837944507598877
iteration 273, loss = 0.5000230073928833
iteration 274, loss = 0.4948618412017822
iteration 275, loss = 0.4792709946632385
iteration 276, loss = 0.48194992542266846
iteration 277, loss = 0.4955630898475647
iteration 278, loss = 0.48661425709724426
iteration 279, loss = 0.49040040373802185
iteration 280, loss = 0.4889461398124695
iteration 281, loss = 0.47664207220077515
iteration 282, loss = 0.49120503664016724
iteration 283, loss = 0.47722193598747253
iteration 284, loss = 0.5047726631164551
iteration 285, loss = 0.4910425543785095
iteration 286, loss = 0.5004615783691406
iteration 287, loss = 0.4881781339645386
iteration 288, loss = 0.5018362402915955
iteration 289, loss = 0.4928533732891083
iteration 290, loss = 0.4885697364807129
iteration 291, loss = 0.48997655510902405
iteration 292, loss = 0.506018877029419
iteration 293, loss = 0.4975363612174988
iteration 294, loss = 0.49728742241859436
iteration 295, loss = 0.483537495136261
iteration 296, loss = 0.48196640610694885
iteration 297, loss = 0.48751699924468994
iteration 298, loss = 0.48711949586868286
iteration 299, loss = 0.490578293800354
iteration 0, loss = 0.48837336897850037
iteration 1, loss = 0.4839557409286499
iteration 2, loss = 0.4910609722137451
iteration 3, loss = 0.47603851556777954
iteration 4, loss = 0.48523080348968506
iteration 5, loss = 0.493529736995697
iteration 6, loss = 0.49488264322280884
iteration 7, loss = 0.49164512753486633
iteration 8, loss = 0.4926140308380127
iteration 9, loss = 0.49816566705703735
iteration 10, loss = 0.49797630310058594
iteration 11, loss = 0.4977402091026306
iteration 12, loss = 0.5023316144943237
iteration 13, loss = 0.4978558421134949
iteration 14, loss = 0.4829554557800293
iteration 15, loss = 0.48081067204475403
iteration 16, loss = 0.4885023236274719
iteration 17, loss = 0.4818931221961975
iteration 18, loss = 0.49731025099754333
iteration 19, loss = 0.47464653849601746
iteration 20, loss = 0.47974932193756104
iteration 21, loss = 0.4867585301399231
iteration 22, loss = 0.4897667467594147
iteration 23, loss = 0.48563647270202637
iteration 24, loss = 0.5010371208190918
iteration 25, loss = 0.49312824010849
iteration 26, loss = 0.495668888092041
iteration 27, loss = 0.48631489276885986
iteration 28, loss = 0.4886978268623352
iteration 29, loss = 0.5089346170425415
iteration 30, loss = 0.4976707398891449
iteration 31, loss = 0.493196964263916
iteration 32, loss = 0.4932122230529785
iteration 33, loss = 0.48520219326019287
iteration 34, loss = 0.5007870197296143
iteration 35, loss = 0.4885776937007904
iteration 36, loss = 0.4881904721260071
iteration 37, loss = 0.4827493727207184
iteration 38, loss = 0.4887819290161133
iteration 39, loss = 0.48850217461586
iteration 40, loss = 0.48858094215393066
iteration 41, loss = 0.4889635443687439
iteration 42, loss = 0.4842098355293274
iteration 43, loss = 0.4955552816390991
iteration 44, loss = 0.48902246356010437
iteration 45, loss = 0.49324464797973633
iteration 46, loss = 0.5001325607299805
iteration 47, loss = 0.49116283655166626
iteration 48, loss = 0.4944206476211548
iteration 49, loss = 0.4788523316383362
iteration 50, loss = 0.499120831489563
iteration 51, loss = 0.5029638409614563
iteration 52, loss = 0.5002242922782898
iteration 53, loss = 0.49416399002075195
iteration 54, loss = 0.5040432810783386
iteration 55, loss = 0.49200260639190674
iteration 56, loss = 0.4999384582042694
iteration 57, loss = 0.4959702491760254
iteration 58, loss = 0.485106885433197
iteration 59, loss = 0.49382463097572327
iteration 60, loss = 0.48283499479293823
iteration 61, loss = 0.4853476285934448
iteration 62, loss = 0.4986279606819153
iteration 63, loss = 0.47655946016311646
iteration 64, loss = 0.49371039867401123
iteration 65, loss = 0.4937326908111572
iteration 66, loss = 0.4988802671432495
iteration 67, loss = 0.4890289902687073
iteration 68, loss = 0.49593880772590637
iteration 69, loss = 0.47452664375305176
iteration 70, loss = 0.5007967948913574
iteration 71, loss = 0.4934428334236145
iteration 72, loss = 0.4754216969013214
iteration 73, loss = 0.47643497586250305
iteration 74, loss = 0.47247204184532166
iteration 75, loss = 0.4974938631057739
iteration 76, loss = 0.49090147018432617
iteration 77, loss = 0.4907301664352417
iteration 78, loss = 0.5053236484527588
iteration 79, loss = 0.4907420575618744
iteration 80, loss = 0.5010145306587219
iteration 81, loss = 0.4822843372821808
iteration 82, loss = 0.502665102481842
iteration 83, loss = 0.4947739839553833
iteration 84, loss = 0.4803943336009979
iteration 85, loss = 0.4792832136154175
iteration 86, loss = 0.47960513830184937
iteration 87, loss = 0.5034651160240173
iteration 88, loss = 0.49561023712158203
iteration 89, loss = 0.4979267716407776
iteration 90, loss = 0.47356855869293213
iteration 91, loss = 0.48700809478759766
iteration 92, loss = 0.4962500035762787
iteration 93, loss = 0.48215168714523315
iteration 94, loss = 0.4969852566719055
iteration 95, loss = 0.48513203859329224
iteration 96, loss = 0.494223952293396
iteration 97, loss = 0.49757200479507446
iteration 98, loss = 0.4842171370983124
iteration 99, loss = 0.49453431367874146
iteration 100, loss = 0.4904363453388214
iteration 101, loss = 0.4803592264652252
iteration 102, loss = 0.49033716320991516
iteration 103, loss = 0.47926557064056396
iteration 104, loss = 0.501272439956665
iteration 105, loss = 0.49557873606681824
iteration 106, loss = 0.4807560443878174
iteration 107, loss = 0.48681971430778503
iteration 108, loss = 0.4799383282661438
iteration 109, loss = 0.5069130659103394
iteration 110, loss = 0.49712666869163513
iteration 111, loss = 0.49847477674484253
iteration 112, loss = 0.4853748083114624
iteration 113, loss = 0.4930199384689331
iteration 114, loss = 0.4893089830875397
iteration 115, loss = 0.4874311089515686
iteration 116, loss = 0.48409944772720337
iteration 117, loss = 0.4778500497341156
iteration 118, loss = 0.49113547801971436
iteration 119, loss = 0.4799489974975586
iteration 120, loss = 0.49047884345054626
iteration 121, loss = 0.498813658952713
iteration 122, loss = 0.4884968400001526
iteration 123, loss = 0.4988151788711548
iteration 124, loss = 0.4962696433067322
iteration 125, loss = 0.4889024794101715
iteration 126, loss = 0.4863942265510559
iteration 127, loss = 0.5049685835838318
iteration 128, loss = 0.49521368741989136
iteration 129, loss = 0.49601060152053833
iteration 130, loss = 0.5048500299453735
iteration 131, loss = 0.4908747375011444
iteration 132, loss = 0.5109927654266357
iteration 133, loss = 0.4711204767227173
iteration 134, loss = 0.47028225660324097
iteration 135, loss = 0.5002833604812622
iteration 136, loss = 0.4866398572921753
iteration 137, loss = 0.49859845638275146
iteration 138, loss = 0.49480050802230835
iteration 139, loss = 0.4991806745529175
iteration 140, loss = 0.48117971420288086
iteration 141, loss = 0.48411256074905396
iteration 142, loss = 0.48046594858169556
iteration 143, loss = 0.5067554116249084
iteration 144, loss = 0.49787142872810364
iteration 145, loss = 0.48110321164131165
iteration 146, loss = 0.48402664065361023
iteration 147, loss = 0.48805803060531616
iteration 148, loss = 0.5014895796775818
iteration 149, loss = 0.5123574137687683
iteration 150, loss = 0.48892050981521606
iteration 151, loss = 0.48724693059921265
iteration 152, loss = 0.49435707926750183
iteration 153, loss = 0.4783264100551605
iteration 154, loss = 0.5089958906173706
iteration 155, loss = 0.49778878688812256
iteration 156, loss = 0.5028369426727295
iteration 157, loss = 0.47714537382125854
iteration 158, loss = 0.4869025647640228
iteration 159, loss = 0.48148420453071594
iteration 160, loss = 0.495333731174469
iteration 161, loss = 0.5041176676750183
iteration 162, loss = 0.4767090678215027
iteration 163, loss = 0.49653559923171997
iteration 164, loss = 0.48915475606918335
iteration 165, loss = 0.4912831783294678
iteration 166, loss = 0.5019383430480957
iteration 167, loss = 0.5024582147598267
iteration 168, loss = 0.5111436247825623
iteration 169, loss = 0.47115981578826904
iteration 170, loss = 0.47982674837112427
iteration 171, loss = 0.4955016076564789
iteration 172, loss = 0.48397117853164673
iteration 173, loss = 0.48647207021713257
iteration 174, loss = 0.5004644393920898
iteration 175, loss = 0.48976707458496094
iteration 176, loss = 0.48782405257225037
iteration 177, loss = 0.47610175609588623
iteration 178, loss = 0.48836517333984375
iteration 179, loss = 0.4967247545719147
iteration 180, loss = 0.48026368021965027
iteration 181, loss = 0.46953096985816956
iteration 182, loss = 0.4981655478477478
iteration 183, loss = 0.49762752652168274
iteration 184, loss = 0.4849838614463806
iteration 185, loss = 0.4696357846260071
iteration 186, loss = 0.4910263419151306
iteration 187, loss = 0.481484591960907
iteration 188, loss = 0.49475765228271484
iteration 189, loss = 0.4820801019668579
iteration 190, loss = 0.49343544244766235
iteration 191, loss = 0.48502177000045776
iteration 192, loss = 0.4850994646549225
iteration 193, loss = 0.4845879077911377
iteration 194, loss = 0.4808236360549927
iteration 195, loss = 0.48446518182754517
iteration 196, loss = 0.5085933804512024
iteration 197, loss = 0.5074646472930908
iteration 198, loss = 0.48334699869155884
iteration 199, loss = 0.5033373832702637
iteration 200, loss = 0.48606133460998535
iteration 201, loss = 0.4866870641708374
iteration 202, loss = 0.4901235103607178
iteration 203, loss = 0.4823080599308014
iteration 204, loss = 0.4814143776893616
iteration 205, loss = 0.49372583627700806
iteration 206, loss = 0.49221616983413696
iteration 207, loss = 0.4836440086364746
iteration 208, loss = 0.4764154255390167
iteration 209, loss = 0.4838722348213196
iteration 210, loss = 0.48612624406814575
iteration 211, loss = 0.4915003776550293
iteration 212, loss = 0.4821479320526123
iteration 213, loss = 0.494942843914032
iteration 214, loss = 0.46703991293907166
iteration 215, loss = 0.48762834072113037
iteration 216, loss = 0.47949278354644775
iteration 217, loss = 0.4927405118942261
iteration 218, loss = 0.4921862483024597
iteration 219, loss = 0.4818376898765564
iteration 220, loss = 0.514145016670227
iteration 221, loss = 0.48610901832580566
iteration 222, loss = 0.48893749713897705
iteration 223, loss = 0.4951213598251343
iteration 224, loss = 0.47380903363227844
iteration 225, loss = 0.4687803089618683
iteration 226, loss = 0.48328566551208496
iteration 227, loss = 0.4899763762950897
iteration 228, loss = 0.5073747634887695
iteration 229, loss = 0.4977293014526367
iteration 230, loss = 0.48320019245147705
iteration 231, loss = 0.5016034841537476
iteration 232, loss = 0.4947434365749359
iteration 233, loss = 0.49097907543182373
iteration 234, loss = 0.5077285766601562
iteration 235, loss = 0.49003756046295166
iteration 236, loss = 0.49047672748565674
iteration 237, loss = 0.4793934226036072
iteration 238, loss = 0.4861239790916443
iteration 239, loss = 0.4914284944534302
iteration 240, loss = 0.49510079622268677
iteration 241, loss = 0.4894353747367859
iteration 242, loss = 0.484646737575531
iteration 243, loss = 0.4799993634223938
iteration 244, loss = 0.47880542278289795
iteration 245, loss = 0.4994862675666809
iteration 246, loss = 0.47678446769714355
iteration 247, loss = 0.47276949882507324
iteration 248, loss = 0.4724852442741394
iteration 249, loss = 0.48673850297927856
iteration 250, loss = 0.47834479808807373
iteration 251, loss = 0.4791513681411743
iteration 252, loss = 0.47503235936164856
iteration 253, loss = 0.4706398844718933
iteration 254, loss = 0.49327021837234497
iteration 255, loss = 0.4813309907913208
iteration 256, loss = 0.48678362369537354
iteration 257, loss = 0.4662160873413086
iteration 258, loss = 0.4952690601348877
iteration 259, loss = 0.47478389739990234
iteration 260, loss = 0.47132226824760437
iteration 261, loss = 0.48953360319137573
iteration 262, loss = 0.47422099113464355
iteration 263, loss = 0.4930407702922821
iteration 264, loss = 0.4799574613571167
iteration 265, loss = 0.48868852853775024
iteration 266, loss = 0.48713645339012146
iteration 267, loss = 0.4776865243911743
iteration 268, loss = 0.4967620372772217
iteration 269, loss = 0.4884716868400574
iteration 270, loss = 0.4929487705230713
iteration 271, loss = 0.49419572949409485
iteration 272, loss = 0.46946272253990173
iteration 273, loss = 0.469471275806427
iteration 274, loss = 0.46631163358688354
iteration 275, loss = 0.48539793491363525
iteration 276, loss = 0.49044638872146606
iteration 277, loss = 0.4945700168609619
iteration 278, loss = 0.47480687499046326
iteration 279, loss = 0.4782213568687439
iteration 280, loss = 0.47890111804008484
iteration 281, loss = 0.49962499737739563
iteration 282, loss = 0.5007660388946533
iteration 283, loss = 0.463473916053772
iteration 284, loss = 0.4843883514404297
iteration 285, loss = 0.4858728051185608
iteration 286, loss = 0.4884542226791382
iteration 287, loss = 0.5155478119850159
iteration 288, loss = 0.4800715744495392
iteration 289, loss = 0.49294865131378174
iteration 290, loss = 0.480404257774353
iteration 291, loss = 0.4917236566543579
iteration 292, loss = 0.48164084553718567
iteration 293, loss = 0.4930771589279175
iteration 294, loss = 0.4938324987888336
iteration 295, loss = 0.4715076684951782
iteration 296, loss = 0.486970454454422
iteration 297, loss = 0.5028516054153442
iteration 298, loss = 0.487419068813324
iteration 299, loss = 0.465658962726593
iteration 0, loss = 0.49810951948165894
iteration 1, loss = 0.4671301543712616
iteration 2, loss = 0.48339641094207764
iteration 3, loss = 0.4758402109146118
iteration 4, loss = 0.4666159749031067
iteration 5, loss = 0.47966858744621277
iteration 6, loss = 0.47965770959854126
iteration 7, loss = 0.4827793836593628
iteration 8, loss = 0.49167701601982117
iteration 9, loss = 0.48471200466156006
iteration 10, loss = 0.5008194446563721
iteration 11, loss = 0.4985769987106323
iteration 12, loss = 0.45276886224746704
iteration 13, loss = 0.4843069612979889
iteration 14, loss = 0.486385703086853
iteration 15, loss = 0.48920494318008423
iteration 16, loss = 0.4892845153808594
iteration 17, loss = 0.5024970769882202
iteration 18, loss = 0.5014328956604004
iteration 19, loss = 0.4838126301765442
iteration 20, loss = 0.46332868933677673
iteration 21, loss = 0.4768434762954712
iteration 22, loss = 0.4730965495109558
iteration 23, loss = 0.4794735908508301
iteration 24, loss = 0.4739776849746704
iteration 25, loss = 0.4975507855415344
iteration 26, loss = 0.4782646894454956
iteration 27, loss = 0.4781740605831146
iteration 28, loss = 0.48874127864837646
iteration 29, loss = 0.4810439646244049
iteration 30, loss = 0.48077914118766785
iteration 31, loss = 0.5020745396614075
iteration 32, loss = 0.49607783555984497
iteration 33, loss = 0.4846959710121155
iteration 34, loss = 0.4804932475090027
iteration 35, loss = 0.48570770025253296
iteration 36, loss = 0.5089223384857178
iteration 37, loss = 0.48888418078422546
iteration 38, loss = 0.4740931987762451
iteration 39, loss = 0.4730355143547058
iteration 40, loss = 0.4773651957511902
iteration 41, loss = 0.4851524829864502
iteration 42, loss = 0.4919818937778473
iteration 43, loss = 0.4827232360839844
iteration 44, loss = 0.5089861154556274
iteration 45, loss = 0.500159740447998
iteration 46, loss = 0.48478296399116516
iteration 47, loss = 0.4861122965812683
iteration 48, loss = 0.46874094009399414
iteration 49, loss = 0.4578198790550232
iteration 50, loss = 0.5064971446990967
iteration 51, loss = 0.466729998588562
iteration 52, loss = 0.5010497570037842
iteration 53, loss = 0.48472756147384644
iteration 54, loss = 0.47847938537597656
iteration 55, loss = 0.49056747555732727
iteration 56, loss = 0.4626197814941406
iteration 57, loss = 0.4776310920715332
iteration 58, loss = 0.4702576994895935
iteration 59, loss = 0.48920005559921265
iteration 60, loss = 0.4728461503982544
iteration 61, loss = 0.4918972849845886
iteration 62, loss = 0.47329801321029663
iteration 63, loss = 0.4557381272315979
iteration 64, loss = 0.5024809241294861
iteration 65, loss = 0.48230451345443726
iteration 66, loss = 0.4912024736404419
iteration 67, loss = 0.46955645084381104
iteration 68, loss = 0.47611141204833984
iteration 69, loss = 0.4864599108695984
iteration 70, loss = 0.4904492497444153
iteration 71, loss = 0.4927977919578552
iteration 72, loss = 0.49195465445518494
iteration 73, loss = 0.49173539876937866
iteration 74, loss = 0.47837182879447937
iteration 75, loss = 0.483598530292511
iteration 76, loss = 0.4707115888595581
iteration 77, loss = 0.5018181204795837
iteration 78, loss = 0.4932401180267334
iteration 79, loss = 0.4931895136833191
iteration 80, loss = 0.4839279055595398
iteration 81, loss = 0.48288291692733765
iteration 82, loss = 0.46594104170799255
iteration 83, loss = 0.4861372709274292
iteration 84, loss = 0.4749675393104553
iteration 85, loss = 0.4925263226032257
iteration 86, loss = 0.48261845111846924
iteration 87, loss = 0.47797226905822754
iteration 88, loss = 0.4926953911781311
iteration 89, loss = 0.4759005308151245
iteration 90, loss = 0.4837504029273987
iteration 91, loss = 0.5051700472831726
iteration 92, loss = 0.46771812438964844
iteration 93, loss = 0.4976465106010437
iteration 94, loss = 0.4845670461654663
iteration 95, loss = 0.48040342330932617
iteration 96, loss = 0.5010849833488464
iteration 97, loss = 0.48376214504241943
iteration 98, loss = 0.48465728759765625
iteration 99, loss = 0.4637843370437622
iteration 100, loss = 0.4904652535915375
iteration 101, loss = 0.4833981394767761
iteration 102, loss = 0.49042803049087524
iteration 103, loss = 0.47049587965011597
iteration 104, loss = 0.4648851156234741
iteration 105, loss = 0.4867106080055237
iteration 106, loss = 0.48487019538879395
iteration 107, loss = 0.4724756181240082
iteration 108, loss = 0.48313355445861816
iteration 109, loss = 0.4739854037761688
iteration 110, loss = 0.47355765104293823
iteration 111, loss = 0.49145790934562683
iteration 112, loss = 0.47456231713294983
iteration 113, loss = 0.49665457010269165
iteration 114, loss = 0.47271254658699036
iteration 115, loss = 0.47089481353759766
iteration 116, loss = 0.49584436416625977
iteration 117, loss = 0.47842514514923096
iteration 118, loss = 0.47558850049972534
iteration 119, loss = 0.4718345105648041
iteration 120, loss = 0.49725496768951416
iteration 121, loss = 0.4919886589050293
iteration 122, loss = 0.44414854049682617
iteration 123, loss = 0.47711098194122314
iteration 124, loss = 0.4743809700012207
iteration 125, loss = 0.5007288455963135
iteration 126, loss = 0.4713820815086365
iteration 127, loss = 0.47945326566696167
iteration 128, loss = 0.4817247986793518
iteration 129, loss = 0.48282358050346375
iteration 130, loss = 0.4804314374923706
iteration 131, loss = 0.470814049243927
iteration 132, loss = 0.42970728874206543
iteration 133, loss = 0.5015156269073486
iteration 134, loss = 0.4879850447177887
iteration 135, loss = 0.47278323769569397
iteration 136, loss = 0.48977869749069214
iteration 137, loss = 0.4705803692340851
iteration 138, loss = 0.45524048805236816
iteration 139, loss = 0.4606839418411255
iteration 140, loss = 0.4830288290977478
iteration 141, loss = 0.46953344345092773
iteration 142, loss = 0.4836079180240631
iteration 143, loss = 0.5000091195106506
iteration 144, loss = 0.4996768534183502
iteration 145, loss = 0.4491842985153198
iteration 146, loss = 0.4731471538543701
iteration 147, loss = 0.4734507203102112
iteration 148, loss = 0.4692496061325073
iteration 149, loss = 0.5005002021789551
iteration 150, loss = 0.4801837205886841
iteration 151, loss = 0.46650195121765137
iteration 152, loss = 0.4777638614177704
iteration 153, loss = 0.4897884130477905
iteration 154, loss = 0.4705348610877991
iteration 155, loss = 0.4678674638271332
iteration 156, loss = 0.4985160231590271
iteration 157, loss = 0.44983458518981934
iteration 158, loss = 0.4672285318374634
iteration 159, loss = 0.48026371002197266
iteration 160, loss = 0.4932120442390442
iteration 161, loss = 0.5043894052505493
iteration 162, loss = 0.5064611434936523
iteration 163, loss = 0.46997126936912537
iteration 164, loss = 0.4728173613548279
iteration 165, loss = 0.4839504063129425
iteration 166, loss = 0.4968237280845642
iteration 167, loss = 0.4817398190498352
iteration 168, loss = 0.48506951332092285
iteration 169, loss = 0.49607282876968384
iteration 170, loss = 0.48578041791915894
iteration 171, loss = 0.4867883324623108
iteration 172, loss = 0.48887819051742554
iteration 173, loss = 0.4845600128173828
iteration 174, loss = 0.4720331132411957
iteration 175, loss = 0.49376699328422546
iteration 176, loss = 0.4888656437397003
iteration 177, loss = 0.48782604932785034
iteration 178, loss = 0.4816611409187317
iteration 179, loss = 0.4607594609260559
iteration 180, loss = 0.48744553327560425
iteration 181, loss = 0.47884199023246765
iteration 182, loss = 0.48378199338912964
iteration 183, loss = 0.469089150428772
iteration 184, loss = 0.4803925156593323
iteration 185, loss = 0.4929601550102234
iteration 186, loss = 0.4864880442619324
iteration 187, loss = 0.5088382959365845
iteration 188, loss = 0.45279812812805176
iteration 189, loss = 0.4748356342315674
iteration 190, loss = 0.48363515734672546
iteration 191, loss = 0.4827897548675537
iteration 192, loss = 0.4709034562110901
iteration 193, loss = 0.4841153025627136
iteration 194, loss = 0.4952254295349121
iteration 195, loss = 0.489018976688385
iteration 196, loss = 0.45884042978286743
iteration 197, loss = 0.46256303787231445
iteration 198, loss = 0.5040193796157837
iteration 199, loss = 0.4856780171394348
iteration 200, loss = 0.4906437397003174
iteration 201, loss = 0.4920153021812439
iteration 202, loss = 0.49287235736846924
iteration 203, loss = 0.48833903670310974
iteration 204, loss = 0.49794793128967285
iteration 205, loss = 0.4761846363544464
iteration 206, loss = 0.5077641010284424
iteration 207, loss = 0.48914891481399536
iteration 208, loss = 0.4800088107585907
iteration 209, loss = 0.48299264907836914
iteration 210, loss = 0.476406991481781
iteration 211, loss = 0.47045689821243286
iteration 212, loss = 0.4505653381347656
iteration 213, loss = 0.4760284721851349
iteration 214, loss = 0.48631536960601807
iteration 215, loss = 0.4684549570083618
iteration 216, loss = 0.47891488671302795
iteration 217, loss = 0.472383975982666
iteration 218, loss = 0.4792235493659973
iteration 219, loss = 0.48607829213142395
iteration 220, loss = 0.49235907196998596
iteration 221, loss = 0.44193774461746216
iteration 222, loss = 0.4790980815887451
iteration 223, loss = 0.4569927453994751
iteration 224, loss = 0.46443426609039307
iteration 225, loss = 0.4813656806945801
iteration 226, loss = 0.47993409633636475
iteration 227, loss = 0.4626191258430481
iteration 228, loss = 0.48637712001800537
iteration 229, loss = 0.49461379647254944
iteration 230, loss = 0.4825851321220398
iteration 231, loss = 0.46699023246765137
iteration 232, loss = 0.5088419914245605
iteration 233, loss = 0.47839537262916565
iteration 234, loss = 0.48437440395355225
iteration 235, loss = 0.4379141926765442
iteration 236, loss = 0.480712354183197
iteration 237, loss = 0.47280338406562805
iteration 238, loss = 0.501354455947876
iteration 239, loss = 0.4947437047958374
iteration 240, loss = 0.49751296639442444
iteration 241, loss = 0.46054214239120483
iteration 242, loss = 0.49821025133132935
iteration 243, loss = 0.4732120633125305
iteration 244, loss = 0.4697820842266083
iteration 245, loss = 0.46454882621765137
iteration 246, loss = 0.4916142225265503
iteration 247, loss = 0.48535966873168945
iteration 248, loss = 0.48493123054504395
iteration 249, loss = 0.4891360104084015
iteration 250, loss = 0.4812062978744507
iteration 251, loss = 0.47738754749298096
iteration 252, loss = 0.4900740385055542
iteration 253, loss = 0.47641271352767944
iteration 254, loss = 0.4598434567451477
iteration 255, loss = 0.4825934171676636
iteration 256, loss = 0.4624395966529846
iteration 257, loss = 0.46974050998687744
iteration 258, loss = 0.46637243032455444
iteration 259, loss = 0.4817121624946594
iteration 260, loss = 0.48825323581695557
iteration 261, loss = 0.5051348805427551
iteration 262, loss = 0.48466455936431885
iteration 263, loss = 0.48589420318603516
iteration 264, loss = 0.46179506182670593
iteration 265, loss = 0.4902406632900238
iteration 266, loss = 0.47678375244140625
iteration 267, loss = 0.48158180713653564
iteration 268, loss = 0.4870554208755493
iteration 269, loss = 0.4845304489135742
iteration 270, loss = 0.4542970657348633
iteration 271, loss = 0.4593062400817871
iteration 272, loss = 0.45444822311401367
iteration 273, loss = 0.46972548961639404
iteration 274, loss = 0.4805524945259094
iteration 275, loss = 0.42972737550735474
iteration 276, loss = 0.4749930202960968
iteration 277, loss = 0.4400585889816284
iteration 278, loss = 0.4694233536720276
iteration 279, loss = 0.4660543203353882
iteration 280, loss = 0.478801429271698
iteration 281, loss = 0.501745879650116
iteration 282, loss = 0.47961652278900146
iteration 283, loss = 0.4626731276512146
iteration 284, loss = 0.4962920546531677
iteration 285, loss = 0.48649245500564575
iteration 286, loss = 0.4726872444152832
iteration 287, loss = 0.4954521656036377
iteration 288, loss = 0.43064790964126587
iteration 289, loss = 0.4511573314666748
iteration 290, loss = 0.4844357967376709
iteration 291, loss = 0.5031432509422302
iteration 292, loss = 0.47423437237739563
iteration 293, loss = 0.4697151184082031
iteration 294, loss = 0.49576136469841003
iteration 295, loss = 0.4407738447189331
iteration 296, loss = 0.45743346214294434
iteration 297, loss = 0.4662421941757202
iteration 298, loss = 0.49335891008377075
iteration 299, loss = 0.4575073719024658
iteration 0, loss = 0.45183494687080383
iteration 1, loss = 0.474238783121109
iteration 2, loss = 0.4668644666671753
iteration 3, loss = 0.47293251752853394
iteration 4, loss = 0.4859793782234192
iteration 5, loss = 0.46480754017829895
iteration 6, loss = 0.46672481298446655
iteration 7, loss = 0.5115160942077637
iteration 8, loss = 0.42981305718421936
iteration 9, loss = 0.45774340629577637
iteration 10, loss = 0.5035076141357422
iteration 11, loss = 0.46795520186424255
iteration 12, loss = 0.5066637396812439
iteration 13, loss = 0.49661684036254883
iteration 14, loss = 0.4664645195007324
iteration 15, loss = 0.4903579354286194
iteration 16, loss = 0.4614545702934265
iteration 17, loss = 0.46565985679626465
iteration 18, loss = 0.43931686878204346
iteration 19, loss = 0.4480815529823303
iteration 20, loss = 0.4707027077674866
iteration 21, loss = 0.463778555393219
iteration 22, loss = 0.4946405589580536
iteration 23, loss = 0.4706490933895111
iteration 24, loss = 0.49076056480407715
iteration 25, loss = 0.502121090888977
iteration 26, loss = 0.47713208198547363
iteration 27, loss = 0.4381437301635742
iteration 28, loss = 0.4526739716529846
iteration 29, loss = 0.4805215001106262
iteration 30, loss = 0.45155245065689087
iteration 31, loss = 0.48774078488349915
iteration 32, loss = 0.49647682905197144
iteration 33, loss = 0.48016637563705444
iteration 34, loss = 0.5220220685005188
iteration 35, loss = 0.4833083748817444
iteration 36, loss = 0.47526466846466064
iteration 37, loss = 0.4619145691394806
iteration 38, loss = 0.5120763182640076
iteration 39, loss = 0.48932701349258423
iteration 40, loss = 0.47271329164505005
iteration 41, loss = 0.46319130063056946
iteration 42, loss = 0.4636912941932678
iteration 43, loss = 0.5006539821624756
iteration 44, loss = 0.5094177722930908
iteration 45, loss = 0.47282201051712036
iteration 46, loss = 0.468372106552124
iteration 47, loss = 0.49130678176879883
iteration 48, loss = 0.4909987151622772
iteration 49, loss = 0.448864221572876
iteration 50, loss = 0.45096737146377563
iteration 51, loss = 0.49230697751045227
iteration 52, loss = 0.4646950960159302
iteration 53, loss = 0.46530455350875854
iteration 54, loss = 0.4817197024822235
iteration 55, loss = 0.4626689553260803
iteration 56, loss = 0.4943655729293823
iteration 57, loss = 0.4885890483856201
iteration 58, loss = 0.4639957547187805
iteration 59, loss = 0.44391930103302
iteration 60, loss = 0.4611052870750427
iteration 61, loss = 0.45180994272232056
iteration 62, loss = 0.4714556932449341
iteration 63, loss = 0.4955383837223053
iteration 64, loss = 0.455581933259964
iteration 65, loss = 0.47694358229637146
iteration 66, loss = 0.4911002516746521
iteration 67, loss = 0.4825935363769531
iteration 68, loss = 0.4585956335067749
iteration 69, loss = 0.5000623464584351
iteration 70, loss = 0.46889251470565796
iteration 71, loss = 0.5050326585769653
iteration 72, loss = 0.4799153804779053
iteration 73, loss = 0.4714352488517761
iteration 74, loss = 0.4593060612678528
iteration 75, loss = 0.4746190309524536
iteration 76, loss = 0.46887168288230896
iteration 77, loss = 0.4532530605792999
iteration 78, loss = 0.4869323670864105
iteration 79, loss = 0.4722222089767456
iteration 80, loss = 0.44040995836257935
iteration 81, loss = 0.4466613531112671
iteration 82, loss = 0.429519385099411
iteration 83, loss = 0.43713656067848206
iteration 84, loss = 0.46077391505241394
iteration 85, loss = 0.4644869565963745
iteration 86, loss = 0.4615960121154785
iteration 87, loss = 0.48422855138778687
iteration 88, loss = 0.4844338893890381
iteration 89, loss = 0.4459834098815918
iteration 90, loss = 0.44731032848358154
iteration 91, loss = 0.45130348205566406
iteration 92, loss = 0.45523595809936523
iteration 93, loss = 0.4862385094165802
iteration 94, loss = 0.4664210081100464
iteration 95, loss = 0.4758569002151489
iteration 96, loss = 0.5029466152191162
iteration 97, loss = 0.42486751079559326
iteration 98, loss = 0.48325687646865845
iteration 99, loss = 0.49630653858184814
iteration 100, loss = 0.4449470043182373
iteration 101, loss = 0.40720611810684204
iteration 102, loss = 0.4966820478439331
iteration 103, loss = 0.45681601762771606
iteration 104, loss = 0.46785205602645874
iteration 105, loss = 0.4838605225086212
iteration 106, loss = 0.4888550639152527
iteration 107, loss = 0.48142915964126587
iteration 108, loss = 0.4733092188835144
iteration 109, loss = 0.460793137550354
iteration 110, loss = 0.4955529570579529
iteration 111, loss = 0.44794487953186035
iteration 112, loss = 0.46276038885116577
iteration 113, loss = 0.4657610058784485
iteration 114, loss = 0.44598349928855896
iteration 115, loss = 0.45000672340393066
iteration 116, loss = 0.4626634418964386
iteration 117, loss = 0.43375736474990845
iteration 118, loss = 0.473798930644989
iteration 119, loss = 0.4255591332912445
iteration 120, loss = 0.45605772733688354
iteration 121, loss = 0.4531371593475342
iteration 122, loss = 0.4729076623916626
iteration 123, loss = 0.4619830846786499
iteration 124, loss = 0.4787029027938843
iteration 125, loss = 0.4639087915420532
iteration 126, loss = 0.43466490507125854
iteration 127, loss = 0.46649691462516785
iteration 128, loss = 0.46331197023391724
iteration 129, loss = 0.5009070634841919
iteration 130, loss = 0.502380907535553
iteration 131, loss = 0.40657252073287964
iteration 132, loss = 0.49354565143585205
iteration 133, loss = 0.4263586103916168
iteration 134, loss = 0.47064411640167236
iteration 135, loss = 0.47180095314979553
iteration 136, loss = 0.411057710647583
iteration 137, loss = 0.4381568431854248
iteration 138, loss = 0.4317008852958679
iteration 139, loss = 0.4649653434753418
iteration 140, loss = 0.4642615020275116
iteration 141, loss = 0.49948233366012573
iteration 142, loss = 0.4857458472251892
iteration 143, loss = 0.4692840874195099
iteration 144, loss = 0.4652799963951111
iteration 145, loss = 0.5156116485595703
iteration 146, loss = 0.4486611485481262
iteration 147, loss = 0.4460803270339966
iteration 148, loss = 0.4714202880859375
iteration 149, loss = 0.4436907470226288
iteration 150, loss = 0.4593842923641205
iteration 151, loss = 0.4720844626426697
iteration 152, loss = 0.46851086616516113
iteration 153, loss = 0.46457570791244507
iteration 154, loss = 0.4382559657096863
iteration 155, loss = 0.48415249586105347
iteration 156, loss = 0.4660402536392212
iteration 157, loss = 0.4482916295528412
iteration 158, loss = 0.47362732887268066
iteration 159, loss = 0.42379888892173767
iteration 160, loss = 0.4227225184440613
iteration 161, loss = 0.4472840428352356
iteration 162, loss = 0.4647029638290405
iteration 163, loss = 0.4711533486843109
iteration 164, loss = 0.41312095522880554
iteration 165, loss = 0.48416978120803833
iteration 166, loss = 0.4226284623146057
iteration 167, loss = 0.468999445438385
iteration 168, loss = 0.4712560772895813
iteration 169, loss = 0.4532431960105896
iteration 170, loss = 0.5088729858398438
iteration 171, loss = 0.4495224952697754
iteration 172, loss = 0.4690656065940857
iteration 173, loss = 0.43864351511001587
iteration 174, loss = 0.4649827480316162
iteration 175, loss = 0.4953964948654175
iteration 176, loss = 0.4358730912208557
iteration 177, loss = 0.5091873407363892
iteration 178, loss = 0.4836222529411316
iteration 179, loss = 0.5184273719787598
iteration 180, loss = 0.47856462001800537
iteration 181, loss = 0.46901750564575195
iteration 182, loss = 0.5014563798904419
iteration 183, loss = 0.4824327230453491
iteration 184, loss = 0.46602675318717957
iteration 185, loss = 0.4655250012874603
iteration 186, loss = 0.47525373101234436
iteration 187, loss = 0.45286500453948975
iteration 188, loss = 0.4221741557121277
iteration 189, loss = 0.45162028074264526
iteration 190, loss = 0.4142254590988159
iteration 191, loss = 0.45412081480026245
iteration 192, loss = 0.47025376558303833
iteration 193, loss = 0.4520593583583832
iteration 194, loss = 0.4548248052597046
iteration 195, loss = 0.4289417862892151
iteration 196, loss = 0.4597560465335846
iteration 197, loss = 0.4215567708015442
iteration 198, loss = 0.47983530163764954
iteration 199, loss = 0.4770689606666565
iteration 200, loss = 0.4891710877418518
iteration 201, loss = 0.4954526722431183
iteration 202, loss = 0.473865270614624
iteration 203, loss = 0.45944327116012573
iteration 204, loss = 0.42095476388931274
iteration 205, loss = 0.4873846471309662
iteration 206, loss = 0.4246467649936676
iteration 207, loss = 0.4094066619873047
iteration 208, loss = 0.4550989866256714
iteration 209, loss = 0.48052072525024414
iteration 210, loss = 0.4773663282394409
iteration 211, loss = 0.4492153525352478
iteration 212, loss = 0.44948065280914307
iteration 213, loss = 0.5040916800498962
iteration 214, loss = 0.45729556679725647
iteration 215, loss = 0.46927499771118164
iteration 216, loss = 0.43256688117980957
iteration 217, loss = 0.44790327548980713
iteration 218, loss = 0.48277926445007324
iteration 219, loss = 0.4628477990627289
iteration 220, loss = 0.47171950340270996
iteration 221, loss = 0.4864238500595093
iteration 222, loss = 0.48275721073150635
iteration 223, loss = 0.5166996717453003
iteration 224, loss = 0.4786837100982666
iteration 225, loss = 0.4407513737678528
iteration 226, loss = 0.4373475909233093
iteration 227, loss = 0.4646614193916321
iteration 228, loss = 0.46515923738479614
iteration 229, loss = 0.5002365112304688
iteration 230, loss = 0.4144320785999298
iteration 231, loss = 0.39872753620147705
iteration 232, loss = 0.48899155855178833
iteration 233, loss = 0.43216487765312195
iteration 234, loss = 0.4477999210357666
iteration 235, loss = 0.4580491781234741
iteration 236, loss = 0.5009957551956177
iteration 237, loss = 0.4729747772216797
iteration 238, loss = 0.39764726161956787
iteration 239, loss = 0.4674477279186249
iteration 240, loss = 0.436824768781662
iteration 241, loss = 0.4447152614593506
iteration 242, loss = 0.47591155767440796
iteration 243, loss = 0.4948645234107971
iteration 244, loss = 0.44947776198387146
iteration 245, loss = 0.468433678150177
iteration 246, loss = 0.46368223428726196
iteration 247, loss = 0.4583072364330292
iteration 248, loss = 0.45433834195137024
iteration 249, loss = 0.4941558241844177
iteration 250, loss = 0.42535334825515747
iteration 251, loss = 0.47188854217529297
iteration 252, loss = 0.4935781955718994
iteration 253, loss = 0.43090277910232544
iteration 254, loss = 0.4550338387489319
iteration 255, loss = 0.43348559737205505
iteration 256, loss = 0.48156821727752686
iteration 257, loss = 0.46126726269721985
iteration 258, loss = 0.48941460251808167
iteration 259, loss = 0.45428958535194397
iteration 260, loss = 0.3983081877231598
iteration 261, loss = 0.4115808606147766
iteration 262, loss = 0.4404163658618927
iteration 263, loss = 0.44550466537475586
iteration 264, loss = 0.48392099142074585
iteration 265, loss = 0.38281774520874023
iteration 266, loss = 0.42213931679725647
iteration 267, loss = 0.4069705009460449
iteration 268, loss = 0.45779165625572205
iteration 269, loss = 0.4357612729072571
iteration 270, loss = 0.44216886162757874
iteration 271, loss = 0.432162344455719
iteration 272, loss = 0.49170398712158203
iteration 273, loss = 0.45365679264068604
iteration 274, loss = 0.49227792024612427
iteration 275, loss = 0.46944648027420044
iteration 276, loss = 0.4714544415473938
iteration 277, loss = 0.4255908727645874
iteration 278, loss = 0.47710102796554565
iteration 279, loss = 0.4412088990211487
iteration 280, loss = 0.4888017177581787
iteration 281, loss = 0.4149891436100006
iteration 282, loss = 0.4070654511451721
iteration 283, loss = 0.45670777559280396
iteration 284, loss = 0.46647220849990845
iteration 285, loss = 0.43569332361221313
iteration 286, loss = 0.4568358063697815
iteration 287, loss = 0.43059954047203064
iteration 288, loss = 0.4338708519935608
iteration 289, loss = 0.42805248498916626
iteration 290, loss = 0.4453299641609192
iteration 291, loss = 0.45052003860473633
iteration 292, loss = 0.5026645064353943
iteration 293, loss = 0.4519285559654236
iteration 294, loss = 0.4740302562713623
iteration 295, loss = 0.46386775374412537
iteration 296, loss = 0.45990896224975586
iteration 297, loss = 0.45802807807922363
iteration 298, loss = 0.4803599715232849
iteration 299, loss = 0.47347211837768555
iteration 0, loss = 0.46943026781082153
iteration 1, loss = 0.537572979927063
iteration 2, loss = 0.46438318490982056
iteration 3, loss = 0.4989444613456726
iteration 4, loss = 0.43818503618240356
iteration 5, loss = 0.4954354166984558
iteration 6, loss = 0.4473854899406433
iteration 7, loss = 0.47048503160476685
iteration 8, loss = 0.4593014717102051
iteration 9, loss = 0.43432337045669556
iteration 10, loss = 0.4369623363018036
iteration 11, loss = 0.4740743637084961
iteration 12, loss = 0.46909791231155396
iteration 13, loss = 0.49846550822257996
iteration 14, loss = 0.430894672870636
iteration 15, loss = 0.4653651714324951
iteration 16, loss = 0.5059133768081665
iteration 17, loss = 0.47226500511169434
iteration 18, loss = 0.46704959869384766
iteration 19, loss = 0.4743572771549225
iteration 20, loss = 0.4244178533554077
iteration 21, loss = 0.4464705288410187
iteration 22, loss = 0.4548092186450958
iteration 23, loss = 0.43782711029052734
iteration 24, loss = 0.42588257789611816
iteration 25, loss = 0.3668721318244934
iteration 26, loss = 0.40396982431411743
iteration 27, loss = 0.47000792622566223
iteration 28, loss = 0.434185266494751
iteration 29, loss = 0.44311100244522095
iteration 30, loss = 0.4305727481842041
iteration 31, loss = 0.4461413621902466
iteration 32, loss = 0.5021366477012634
iteration 33, loss = 0.40178459882736206
iteration 34, loss = 0.4592690169811249
iteration 35, loss = 0.4889073371887207
iteration 36, loss = 0.4293311834335327
iteration 37, loss = 0.49545007944107056
iteration 38, loss = 0.49571549892425537
iteration 39, loss = 0.4847619831562042
iteration 40, loss = 0.39778342843055725
iteration 41, loss = 0.5288294553756714
iteration 42, loss = 0.4559575915336609
iteration 43, loss = 0.4134957790374756
iteration 44, loss = 0.38487064838409424
iteration 45, loss = 0.42458659410476685
iteration 46, loss = 0.46550315618515015
iteration 47, loss = 0.4642111659049988
iteration 48, loss = 0.39697057008743286
iteration 49, loss = 0.4607629179954529
iteration 50, loss = 0.4811013340950012
iteration 51, loss = 0.46811026334762573
iteration 52, loss = 0.47329217195510864
iteration 53, loss = 0.46330490708351135
iteration 54, loss = 0.44892650842666626
iteration 55, loss = 0.4975086450576782
iteration 56, loss = 0.46156710386276245
iteration 57, loss = 0.42189306020736694
iteration 58, loss = 0.41790875792503357
iteration 59, loss = 0.47432878613471985
iteration 60, loss = 0.3807179033756256
iteration 61, loss = 0.3872181177139282
iteration 62, loss = 0.4527362287044525
iteration 63, loss = 0.46141910552978516
iteration 64, loss = 0.421191930770874
iteration 65, loss = 0.385135293006897
iteration 66, loss = 0.4052371382713318
iteration 67, loss = 0.4449164867401123
iteration 68, loss = 0.5115941762924194
iteration 69, loss = 0.493633896112442
iteration 70, loss = 0.4214792549610138
iteration 71, loss = 0.4344976842403412
iteration 72, loss = 0.44583660364151
iteration 73, loss = 0.4675915837287903
iteration 74, loss = 0.3797224164009094
iteration 75, loss = 0.466041624546051
iteration 76, loss = 0.49421870708465576
iteration 77, loss = 0.3852916359901428
iteration 78, loss = 0.45592647790908813
iteration 79, loss = 0.4182567298412323
iteration 80, loss = 0.5043392777442932
iteration 81, loss = 0.47075197100639343
iteration 82, loss = 0.49416324496269226
iteration 83, loss = 0.45575830340385437
iteration 84, loss = 0.46136343479156494
iteration 85, loss = 0.5109807848930359
iteration 86, loss = 0.42767104506492615
iteration 87, loss = 0.4724026322364807
iteration 88, loss = 0.4863319396972656
iteration 89, loss = 0.4820404052734375
iteration 90, loss = 0.4882194995880127
iteration 91, loss = 0.4103618264198303
iteration 92, loss = 0.4426823854446411
iteration 93, loss = 0.43390321731567383
iteration 94, loss = 0.45064103603363037
iteration 95, loss = 0.4260101318359375
iteration 96, loss = 0.3898402452468872
iteration 97, loss = 0.39316076040267944
iteration 98, loss = 0.4807364344596863
iteration 99, loss = 0.38111937046051025
iteration 100, loss = 0.4396112859249115
iteration 101, loss = 0.45152682065963745
iteration 102, loss = 0.43217453360557556
iteration 103, loss = 0.4425412118434906
iteration 104, loss = 0.4263544976711273
iteration 105, loss = 0.46852993965148926
iteration 106, loss = 0.48208290338516235
iteration 107, loss = 0.53081876039505
iteration 108, loss = 0.42048317193984985
iteration 109, loss = 0.4954148530960083
iteration 110, loss = 0.447368323802948
iteration 111, loss = 0.45485225319862366
iteration 112, loss = 0.46947598457336426
iteration 113, loss = 0.4538934826850891
iteration 114, loss = 0.39035388827323914
iteration 115, loss = 0.40586191415786743
iteration 116, loss = 0.37096792459487915
iteration 117, loss = 0.43733447790145874
iteration 118, loss = 0.4293936491012573
iteration 119, loss = 0.46937239170074463
iteration 120, loss = 0.40360990166664124
iteration 121, loss = 0.36556947231292725
iteration 122, loss = 0.476063072681427
iteration 123, loss = 0.38882192969322205
iteration 124, loss = 0.4211943745613098
iteration 125, loss = 0.47309935092926025
iteration 126, loss = 0.4496597647666931
iteration 127, loss = 0.4866339862346649
iteration 128, loss = 0.4130373001098633
iteration 129, loss = 0.37936270236968994
iteration 130, loss = 0.4129287898540497
iteration 131, loss = 0.4994000792503357
iteration 132, loss = 0.5337487459182739
iteration 133, loss = 0.4584491550922394
iteration 134, loss = 0.40446150302886963
iteration 135, loss = 0.39592209458351135
iteration 136, loss = 0.44945934414863586
iteration 137, loss = 0.495334267616272
iteration 138, loss = 0.41230082511901855
iteration 139, loss = 0.43136754631996155
iteration 140, loss = 0.4300558865070343
iteration 141, loss = 0.48454076051712036
iteration 142, loss = 0.4319115877151489
iteration 143, loss = 0.3860921263694763
iteration 144, loss = 0.3954542875289917
iteration 145, loss = 0.4054034948348999
iteration 146, loss = 0.4721168279647827
iteration 147, loss = 0.42019182443618774
iteration 148, loss = 0.4680914878845215
iteration 149, loss = 0.46196675300598145
iteration 150, loss = 0.4265458285808563
iteration 151, loss = 0.4239819049835205
iteration 152, loss = 0.4419499933719635
iteration 153, loss = 0.4212883710861206
iteration 154, loss = 0.38412773609161377
iteration 155, loss = 0.38560718297958374
iteration 156, loss = 0.42091792821884155
iteration 157, loss = 0.48804599046707153
iteration 158, loss = 0.42463117837905884
iteration 159, loss = 0.49380621314048767
iteration 160, loss = 0.4181002974510193
iteration 161, loss = 0.4618659019470215
iteration 162, loss = 0.41970473527908325
iteration 163, loss = 0.5069656372070312
iteration 164, loss = 0.36504900455474854
iteration 165, loss = 0.41379234194755554
iteration 166, loss = 0.4580168128013611
iteration 167, loss = 0.4727097153663635
iteration 168, loss = 0.4286854863166809
iteration 169, loss = 0.3709677755832672
iteration 170, loss = 0.47276514768600464
iteration 171, loss = 0.5141787528991699
iteration 172, loss = 0.4873460829257965
iteration 173, loss = 0.45665886998176575
iteration 174, loss = 0.44277918338775635
iteration 175, loss = 0.5398669838905334
iteration 176, loss = 0.39027994871139526
iteration 177, loss = 0.4825887680053711
iteration 178, loss = 0.44587308168411255
iteration 179, loss = 0.4568788409233093
iteration 180, loss = 0.4390944838523865
iteration 181, loss = 0.4464724063873291
iteration 182, loss = 0.37852776050567627
iteration 183, loss = 0.33815640211105347
iteration 184, loss = 0.44146955013275146
iteration 185, loss = 0.37788718938827515
iteration 186, loss = 0.45353978872299194
iteration 187, loss = 0.43250375986099243
iteration 188, loss = 0.4574071168899536
iteration 189, loss = 0.39428049325942993
iteration 190, loss = 0.4573037326335907
iteration 191, loss = 0.39143967628479004
iteration 192, loss = 0.44922155141830444
iteration 193, loss = 0.39577847719192505
iteration 194, loss = 0.3925098776817322
iteration 195, loss = 0.3255567252635956
iteration 196, loss = 0.3877255916595459
iteration 197, loss = 0.4585249423980713
iteration 198, loss = 0.46409687399864197
iteration 199, loss = 0.4820418357849121
iteration 200, loss = 0.3855886161327362
iteration 201, loss = 0.4684264063835144
iteration 202, loss = 0.4093056619167328
iteration 203, loss = 0.40880507230758667
iteration 204, loss = 0.45364245772361755
iteration 205, loss = 0.4101181626319885
iteration 206, loss = 0.39350196719169617
iteration 207, loss = 0.3873843550682068
iteration 208, loss = 0.3853774070739746
iteration 209, loss = 0.45537516474723816
iteration 210, loss = 0.42412641644477844
iteration 211, loss = 0.407898485660553
iteration 212, loss = 0.4058358371257782
iteration 213, loss = 0.40584301948547363
iteration 214, loss = 0.446410208940506
iteration 215, loss = 0.4289745092391968
iteration 216, loss = 0.36618945002555847
iteration 217, loss = 0.4329354763031006
iteration 218, loss = 0.47872394323349
iteration 219, loss = 0.3744216561317444
iteration 220, loss = 0.5089150071144104
iteration 221, loss = 0.4440484642982483
iteration 222, loss = 0.4139501750469208
iteration 223, loss = 0.4351978302001953
iteration 224, loss = 0.3501753807067871
iteration 225, loss = 0.47808459401130676
iteration 226, loss = 0.5141715407371521
iteration 227, loss = 0.4510432481765747
iteration 228, loss = 0.4708007574081421
iteration 229, loss = 0.32428979873657227
iteration 230, loss = 0.3872147798538208
iteration 231, loss = 0.28473949432373047
iteration 232, loss = 0.4085918664932251
iteration 233, loss = 0.43128353357315063
iteration 234, loss = 0.4994981288909912
iteration 235, loss = 0.40727946162223816
iteration 236, loss = 0.39776623249053955
iteration 237, loss = 0.47947707772254944
iteration 238, loss = 0.4465843141078949
iteration 239, loss = 0.33372658491134644
iteration 240, loss = 0.43349799513816833
iteration 241, loss = 0.39666205644607544
iteration 242, loss = 0.39075005054473877
iteration 243, loss = 0.37039250135421753
iteration 244, loss = 0.382060706615448
iteration 245, loss = 0.388577938079834
iteration 246, loss = 0.4085925221443176
iteration 247, loss = 0.4755074381828308
iteration 248, loss = 0.3973690867424011
iteration 249, loss = 0.4624248743057251
iteration 250, loss = 0.3900589644908905
iteration 251, loss = 0.4110589027404785
iteration 252, loss = 0.3680516481399536
iteration 253, loss = 0.3838532567024231
iteration 254, loss = 0.33315059542655945
iteration 255, loss = 0.37605708837509155
iteration 256, loss = 0.4436846375465393
iteration 257, loss = 0.3739471733570099
iteration 258, loss = 0.39767956733703613
iteration 259, loss = 0.3812446892261505
iteration 260, loss = 0.3008444905281067
iteration 261, loss = 0.387938916683197
iteration 262, loss = 0.37392139434814453
iteration 263, loss = 0.35913848876953125
iteration 264, loss = 0.4719002842903137
iteration 265, loss = 0.4381569027900696
iteration 266, loss = 0.3725452125072479
iteration 267, loss = 0.4819408059120178
iteration 268, loss = 0.5107482671737671
iteration 269, loss = 0.46285194158554077
iteration 270, loss = 0.43885672092437744
iteration 271, loss = 0.4239426851272583
iteration 272, loss = 0.46733492612838745
iteration 273, loss = 0.4257626533508301
iteration 274, loss = 0.29569125175476074
iteration 275, loss = 0.42998164892196655
iteration 276, loss = 0.3497644066810608
iteration 277, loss = 0.5193902254104614
iteration 278, loss = 0.34907183051109314
iteration 279, loss = 0.365177184343338
iteration 280, loss = 0.4160424768924713
iteration 281, loss = 0.410083532333374
iteration 282, loss = 0.4592698812484741
iteration 283, loss = 0.43975454568862915
iteration 284, loss = 0.42285704612731934
iteration 285, loss = 0.38237786293029785
iteration 286, loss = 0.42444178462028503
iteration 287, loss = 0.42923134565353394
iteration 288, loss = 0.5128344297409058
iteration 289, loss = 0.40344011783599854
iteration 290, loss = 0.3963356614112854
iteration 291, loss = 0.3884529173374176
iteration 292, loss = 0.41891372203826904
iteration 293, loss = 0.307149738073349
iteration 294, loss = 0.44527387619018555
iteration 295, loss = 0.34772953391075134
iteration 296, loss = 0.5127566456794739
iteration 297, loss = 0.3800409436225891
iteration 298, loss = 0.3916977345943451
iteration 299, loss = 0.36376726627349854
iteration 0, loss = 0.3828616142272949
iteration 1, loss = 0.34734609723091125
iteration 2, loss = 0.400510311126709
iteration 3, loss = 0.4399612545967102
iteration 4, loss = 0.46128761768341064
iteration 5, loss = 0.307689368724823
iteration 6, loss = 0.3960523009300232
iteration 7, loss = 0.35917598009109497
iteration 8, loss = 0.42095947265625
iteration 9, loss = 0.44476476311683655
iteration 10, loss = 0.37104862928390503
iteration 11, loss = 0.4577946364879608
iteration 12, loss = 0.46329405903816223
iteration 13, loss = 0.44185400009155273
iteration 14, loss = 0.4133402705192566
iteration 15, loss = 0.4538305997848511
iteration 16, loss = 0.4152924418449402
iteration 17, loss = 0.48734426498413086
iteration 18, loss = 0.43416541814804077
iteration 19, loss = 0.3534145951271057
iteration 20, loss = 0.39454126358032227
iteration 21, loss = 0.4704403281211853
iteration 22, loss = 0.46196722984313965
iteration 23, loss = 0.34013670682907104
iteration 24, loss = 0.4009840488433838
iteration 25, loss = 0.4457886815071106
iteration 26, loss = 0.32967984676361084
iteration 27, loss = 0.48761051893234253
iteration 28, loss = 0.4411347508430481
iteration 29, loss = 0.43954843282699585
iteration 30, loss = 0.48307013511657715
iteration 31, loss = 0.4681839942932129
iteration 32, loss = 0.42180854082107544
iteration 33, loss = 0.31074658036231995
iteration 34, loss = 0.40833526849746704
iteration 35, loss = 0.3919646739959717
iteration 36, loss = 0.4741029739379883
iteration 37, loss = 0.2920999526977539
iteration 38, loss = 0.42913150787353516
iteration 39, loss = 0.3772869408130646
iteration 40, loss = 0.34413382411003113
iteration 41, loss = 0.43069449067115784
iteration 42, loss = 0.43935897946357727
iteration 43, loss = 0.4229813814163208
iteration 44, loss = 0.4490739107131958
iteration 45, loss = 0.43926408886909485
iteration 46, loss = 0.4248840808868408
iteration 47, loss = 0.35694241523742676
iteration 48, loss = 0.37199920415878296
iteration 49, loss = 0.3906131982803345
iteration 50, loss = 0.3765660524368286
iteration 51, loss = 0.3554232716560364
iteration 52, loss = 0.42921751737594604
iteration 53, loss = 0.407140851020813
iteration 54, loss = 0.4578569531440735
iteration 55, loss = 0.40264976024627686
iteration 56, loss = 0.4168166220188141
iteration 57, loss = 0.4280317425727844
iteration 58, loss = 0.54976487159729
iteration 59, loss = 0.4654318690299988
iteration 60, loss = 0.3330516815185547
iteration 61, loss = 0.38320446014404297
iteration 62, loss = 0.40955305099487305
iteration 63, loss = 0.5004914999008179
iteration 64, loss = 0.4140850603580475
iteration 65, loss = 0.2685478925704956
iteration 66, loss = 0.3280830979347229
iteration 67, loss = 0.46059292554855347
iteration 68, loss = 0.4140435457229614
iteration 69, loss = 0.40237507224082947
iteration 70, loss = 0.29794251918792725
iteration 71, loss = 0.3935198485851288
iteration 72, loss = 0.4392363131046295
iteration 73, loss = 0.3873729109764099
iteration 74, loss = 0.40551576018333435
iteration 75, loss = 0.3144122362136841
iteration 76, loss = 0.3214225172996521
iteration 77, loss = 0.5027839541435242
iteration 78, loss = 0.3902779519557953
iteration 79, loss = 0.4384850859642029
iteration 80, loss = 0.3992190659046173
iteration 81, loss = 0.4166063666343689
iteration 82, loss = 0.42157068848609924
iteration 83, loss = 0.26221975684165955
iteration 84, loss = 0.404716432094574
iteration 85, loss = 0.395724892616272
iteration 86, loss = 0.3730970621109009
iteration 87, loss = 0.44361215829849243
iteration 88, loss = 0.28632599115371704
iteration 89, loss = 0.40686601400375366
iteration 90, loss = 0.4370414614677429
iteration 91, loss = 0.41179347038269043
iteration 92, loss = 0.4498555660247803
iteration 93, loss = 0.38783639669418335
iteration 94, loss = 0.42877766489982605
iteration 95, loss = 0.5384279489517212
iteration 96, loss = 0.38648682832717896
iteration 97, loss = 0.315196692943573
iteration 98, loss = 0.3588642477989197
iteration 99, loss = 0.4496695101261139
iteration 100, loss = 0.2886698544025421
iteration 101, loss = 0.4936947226524353
iteration 102, loss = 0.4498223662376404
iteration 103, loss = 0.42123764753341675
iteration 104, loss = 0.5294735431671143
iteration 105, loss = 0.3559889793395996
iteration 106, loss = 0.31885024905204773
iteration 107, loss = 0.34960538148880005
iteration 108, loss = 0.4789077043533325
iteration 109, loss = 0.47036823630332947
iteration 110, loss = 0.4352385401725769
iteration 111, loss = 0.3819739818572998
iteration 112, loss = 0.3450786769390106
iteration 113, loss = 0.3960495889186859
iteration 114, loss = 0.2753773629665375
iteration 115, loss = 0.5445223450660706
iteration 116, loss = 0.4531731605529785
iteration 117, loss = 0.3539087176322937
iteration 118, loss = 0.36395132541656494
iteration 119, loss = 0.3756917119026184
iteration 120, loss = 0.3448125422000885
iteration 121, loss = 0.27542436122894287
iteration 122, loss = 0.30445826053619385
iteration 123, loss = 0.35298240184783936
iteration 124, loss = 0.458069384098053
iteration 125, loss = 0.3978085517883301
iteration 126, loss = 0.2995604872703552
iteration 127, loss = 0.42154204845428467
iteration 128, loss = 0.4138907194137573
iteration 129, loss = 0.48093080520629883
iteration 130, loss = 0.34031033515930176
iteration 131, loss = 0.3661131262779236
iteration 132, loss = 0.38052263855934143
iteration 133, loss = 0.3709602355957031
iteration 134, loss = 0.5449241399765015
iteration 135, loss = 0.3605920076370239
iteration 136, loss = 0.4236343801021576
iteration 137, loss = 0.39315566420555115
iteration 138, loss = 0.35880693793296814
iteration 139, loss = 0.402815580368042
iteration 140, loss = 0.4757227897644043
iteration 141, loss = 0.25122979283332825
iteration 142, loss = 0.41887015104293823
iteration 143, loss = 0.39633214473724365
iteration 144, loss = 0.39741772413253784
iteration 145, loss = 0.2926025390625
iteration 146, loss = 0.3013303875923157
iteration 147, loss = 0.4742698073387146
iteration 148, loss = 0.3514147400856018
iteration 149, loss = 0.30392223596572876
iteration 150, loss = 0.3472074270248413
iteration 151, loss = 0.41104334592819214
iteration 152, loss = 0.4255636930465698
iteration 153, loss = 0.19500133395195007
iteration 154, loss = 0.34432506561279297
iteration 155, loss = 0.47074273228645325
iteration 156, loss = 0.5515550374984741
iteration 157, loss = 0.31126949191093445
iteration 158, loss = 0.451213538646698
iteration 159, loss = 0.39215582609176636
iteration 160, loss = 0.5042668581008911
iteration 161, loss = 0.3829214572906494
iteration 162, loss = 0.2948075532913208
iteration 163, loss = 0.3687797784805298
iteration 164, loss = 0.26661208271980286
iteration 165, loss = 0.3040246069431305
iteration 166, loss = 0.35189998149871826
iteration 167, loss = 0.3604939877986908
iteration 168, loss = 0.43491148948669434
iteration 169, loss = 0.478188693523407
iteration 170, loss = 0.35844385623931885
iteration 171, loss = 0.36708515882492065
iteration 172, loss = 0.41489169001579285
iteration 173, loss = 0.407892644405365
iteration 174, loss = 0.3890534043312073
iteration 175, loss = 0.4783107042312622
iteration 176, loss = 0.3686293363571167
iteration 177, loss = 0.4665318727493286
iteration 178, loss = 0.407317578792572
iteration 179, loss = 0.546025812625885
iteration 180, loss = 0.31875768303871155
iteration 181, loss = 0.4272950291633606
iteration 182, loss = 0.3734496533870697
iteration 183, loss = 0.3811941146850586
iteration 184, loss = 0.4133937954902649
iteration 185, loss = 0.4009521007537842
iteration 186, loss = 0.3794648051261902
iteration 187, loss = 0.4536058008670807
iteration 188, loss = 0.46106699109077454
iteration 189, loss = 0.4717828631401062
iteration 190, loss = 0.3459300100803375
iteration 191, loss = 0.2701636850833893
iteration 192, loss = 0.34212762117385864
iteration 193, loss = 0.3334451913833618
iteration 194, loss = 0.3586519956588745
iteration 195, loss = 0.4495583474636078
iteration 196, loss = 0.40199533104896545
iteration 197, loss = 0.3518977463245392
iteration 198, loss = 0.4032563269138336
iteration 199, loss = 0.3255625069141388
iteration 200, loss = 0.36172226071357727
iteration 201, loss = 0.3415394723415375
iteration 202, loss = 0.2761259078979492
iteration 203, loss = 0.4147433638572693
iteration 204, loss = 0.40488678216934204
iteration 205, loss = 0.4131637513637543
iteration 206, loss = 0.3667778968811035
iteration 207, loss = 0.4277360439300537
iteration 208, loss = 0.6145025491714478
iteration 209, loss = 0.38198843598365784
iteration 210, loss = 0.3590817451477051
iteration 211, loss = 0.5365747213363647
iteration 212, loss = 0.3584882915019989
iteration 213, loss = 0.33844560384750366
iteration 214, loss = 0.44746237993240356
iteration 215, loss = 0.38637903332710266
iteration 216, loss = 0.3537568747997284
iteration 217, loss = 0.33869409561157227
iteration 218, loss = 0.3248082399368286
iteration 219, loss = 0.33288121223449707
iteration 220, loss = 0.4137899875640869
iteration 221, loss = 0.4351164698600769
iteration 222, loss = 0.2326691448688507
iteration 223, loss = 0.44587796926498413
iteration 224, loss = 0.3761138319969177
iteration 225, loss = 0.3574698567390442
iteration 226, loss = 0.27665257453918457
iteration 227, loss = 0.4534769058227539
iteration 228, loss = 0.423961877822876
iteration 229, loss = 0.3996935188770294
iteration 230, loss = 0.3483143448829651
iteration 231, loss = 0.2768961191177368
iteration 232, loss = 0.3671577572822571
iteration 233, loss = 0.1881551593542099
iteration 234, loss = 0.3376982808113098
iteration 235, loss = 0.4552244246006012
iteration 236, loss = 0.34125378727912903
iteration 237, loss = 0.24840368330478668
iteration 238, loss = 0.28726258873939514
iteration 239, loss = 0.31323856115341187
iteration 240, loss = 0.5684819221496582
iteration 241, loss = 0.32973599433898926
iteration 242, loss = 0.36773520708084106
iteration 243, loss = 0.30896008014678955
iteration 244, loss = 0.38786524534225464
iteration 245, loss = 0.4699912369251251
iteration 246, loss = 0.3772721588611603
iteration 247, loss = 0.3530004620552063
iteration 248, loss = 0.3350861072540283
iteration 249, loss = 0.39805400371551514
iteration 250, loss = 0.4568926990032196
iteration 251, loss = 0.52818763256073
iteration 252, loss = 0.3835669159889221
iteration 253, loss = 0.5288466215133667
iteration 254, loss = 0.37103071808815
iteration 255, loss = 0.4057803153991699
iteration 256, loss = 0.269509881734848
iteration 257, loss = 0.3036634027957916
iteration 258, loss = 0.40093663334846497
iteration 259, loss = 0.34421759843826294
iteration 260, loss = 0.41330963373184204
iteration 261, loss = 0.3997609317302704
iteration 262, loss = 0.4961852431297302
iteration 263, loss = 0.3662245273590088
iteration 264, loss = 0.4439837336540222
iteration 265, loss = 0.3723180592060089
iteration 266, loss = 0.39465636014938354
iteration 267, loss = 0.3613069951534271
iteration 268, loss = 0.3560066819190979
iteration 269, loss = 0.39471280574798584
iteration 270, loss = 0.35279861092567444
iteration 271, loss = 0.35042470693588257
iteration 272, loss = 0.3762853145599365
iteration 273, loss = 0.49849629402160645
iteration 274, loss = 0.3832203447818756
iteration 275, loss = 0.4425240159034729
iteration 276, loss = 0.4265044629573822
iteration 277, loss = 0.34248679876327515
iteration 278, loss = 0.39661556482315063
iteration 279, loss = 0.3335503339767456
iteration 280, loss = 0.4485829770565033
iteration 281, loss = 0.255889892578125
iteration 282, loss = 0.32025474309921265
iteration 283, loss = 0.29376092553138733
iteration 284, loss = 0.46228528022766113
iteration 285, loss = 0.3166790306568146
iteration 286, loss = 0.3691575527191162
iteration 287, loss = 0.27033454179763794
iteration 288, loss = 0.3488292098045349
iteration 289, loss = 0.47207140922546387
iteration 290, loss = 0.2747659683227539
iteration 291, loss = 0.23159819841384888
iteration 292, loss = 0.5283629894256592
iteration 293, loss = 0.35197049379348755
iteration 294, loss = 0.41388046741485596
iteration 295, loss = 0.31522995233535767
iteration 296, loss = 0.37327465415000916
iteration 297, loss = 0.5743692517280579
iteration 298, loss = 0.3070499897003174
iteration 299, loss = 0.3565218448638916
iteration 0, loss = 0.3725786805152893
iteration 1, loss = 0.3935942053794861
iteration 2, loss = 0.33182886242866516
iteration 3, loss = 0.3670257031917572
iteration 4, loss = 0.35200706124305725
iteration 5, loss = 0.38090264797210693
iteration 6, loss = 0.4701351225376129
iteration 7, loss = 0.281334787607193
iteration 8, loss = 0.4120291471481323
iteration 9, loss = 0.35007402300834656
iteration 10, loss = 0.2679308354854584
iteration 11, loss = 0.4080570936203003
iteration 12, loss = 0.321313738822937
iteration 13, loss = 0.33071184158325195
iteration 14, loss = 0.3685140609741211
iteration 15, loss = 0.377090185880661
iteration 16, loss = 0.18676058948040009
iteration 17, loss = 0.41370856761932373
iteration 18, loss = 0.4350352883338928
iteration 19, loss = 0.3168671131134033
iteration 20, loss = 0.41290953755378723
iteration 21, loss = 0.5311993360519409
iteration 22, loss = 0.3889927864074707
iteration 23, loss = 0.14835254848003387
iteration 24, loss = 0.3775627613067627
iteration 25, loss = 0.5251138806343079
iteration 26, loss = 0.3881376385688782
iteration 27, loss = 0.35954099893569946
iteration 28, loss = 0.4414949417114258
iteration 29, loss = 0.46383610367774963
iteration 30, loss = 0.3674696981906891
iteration 31, loss = 0.22168180346488953
iteration 32, loss = 0.48198333382606506
iteration 33, loss = 0.24655430018901825
iteration 34, loss = 0.296573281288147
iteration 35, loss = 0.39626920223236084
iteration 36, loss = 0.3844294846057892
iteration 37, loss = 0.43244221806526184
iteration 38, loss = 0.24145954847335815
iteration 39, loss = 0.5453100204467773
iteration 40, loss = 0.3796918988227844
iteration 41, loss = 0.3215997517108917
iteration 42, loss = 0.295156329870224
iteration 43, loss = 0.43061622977256775
iteration 44, loss = 0.4622759222984314
iteration 45, loss = 0.3775228261947632
iteration 46, loss = 0.41665053367614746
iteration 47, loss = 0.42371663451194763
iteration 48, loss = 0.46033191680908203
iteration 49, loss = 0.3360862135887146
iteration 50, loss = 0.28457751870155334
iteration 51, loss = 0.34986764192581177
iteration 52, loss = 0.28540149331092834
iteration 53, loss = 0.38888639211654663
iteration 54, loss = 0.2531491816043854
iteration 55, loss = 0.3757568299770355
iteration 56, loss = 0.3703100085258484
iteration 57, loss = 0.33214670419692993
iteration 58, loss = 0.32139161229133606
iteration 59, loss = 0.3458593487739563
iteration 60, loss = 0.37762385606765747
iteration 61, loss = 0.5011297464370728
iteration 62, loss = 0.17386388778686523
iteration 63, loss = 0.3938913345336914
iteration 64, loss = 0.26579686999320984
iteration 65, loss = 0.38909482955932617
iteration 66, loss = 0.26264357566833496
iteration 67, loss = 0.4562816619873047
iteration 68, loss = 0.19820863008499146
iteration 69, loss = 0.27784255146980286
iteration 70, loss = 0.41010236740112305
iteration 71, loss = 0.34120863676071167
iteration 72, loss = 0.2739642858505249
iteration 73, loss = 0.5245708227157593
iteration 74, loss = 0.41158029437065125
iteration 75, loss = 0.2586832046508789
iteration 76, loss = 0.3435758352279663
iteration 77, loss = 0.32200658321380615
iteration 78, loss = 0.48516789078712463
iteration 79, loss = 0.30702465772628784
iteration 80, loss = 0.2893856465816498
iteration 81, loss = 0.2142772376537323
iteration 82, loss = 0.31239986419677734
iteration 83, loss = 0.3552846610546112
iteration 84, loss = 0.28729432821273804
iteration 85, loss = 0.44435080885887146
iteration 86, loss = 0.33078184723854065
iteration 87, loss = 0.3864715099334717
iteration 88, loss = 0.26194676756858826
iteration 89, loss = 0.44787395000457764
iteration 90, loss = 0.47754228115081787
iteration 91, loss = 0.38891467452049255
iteration 92, loss = 0.33945536613464355
iteration 93, loss = 0.4148027300834656
iteration 94, loss = 0.31339704990386963
iteration 95, loss = 0.4507472515106201
iteration 96, loss = 0.4179345369338989
iteration 97, loss = 0.3259124159812927
iteration 98, loss = 0.3478280305862427
iteration 99, loss = 0.35048285126686096
iteration 100, loss = 0.3948298394680023
iteration 101, loss = 0.34729790687561035
iteration 102, loss = 0.23713883757591248
iteration 103, loss = 0.4756377339363098
iteration 104, loss = 0.572769284248352
iteration 105, loss = 0.5677364468574524
iteration 106, loss = 0.4049310088157654
iteration 107, loss = 0.3109360933303833
iteration 108, loss = 0.3442419767379761
iteration 109, loss = 0.3484155833721161
iteration 110, loss = 0.2521275281906128
iteration 111, loss = 0.4103308916091919
iteration 112, loss = 0.2951911687850952
iteration 113, loss = 0.3317139148712158
iteration 114, loss = 0.49685829877853394
iteration 115, loss = 0.34292933344841003
iteration 116, loss = 0.5570854544639587
iteration 117, loss = 0.49280989170074463
iteration 118, loss = 0.20954406261444092
iteration 119, loss = 0.40052032470703125
iteration 120, loss = 0.25460582971572876
iteration 121, loss = 0.4388631582260132
iteration 122, loss = 0.4154348373413086
iteration 123, loss = 0.34795430302619934
iteration 124, loss = 0.5077126026153564
iteration 125, loss = 0.4051283299922943
iteration 126, loss = 0.423300564289093
iteration 127, loss = 0.34059005975723267
iteration 128, loss = 0.41125452518463135
iteration 129, loss = 0.39585211873054504
iteration 130, loss = 0.26493677496910095
iteration 131, loss = 0.33924609422683716
iteration 132, loss = 0.40389561653137207
iteration 133, loss = 0.5039352178573608
iteration 134, loss = 0.32305479049682617
iteration 135, loss = 0.37189173698425293
iteration 136, loss = 0.23627705872058868
iteration 137, loss = 0.4571555554866791
iteration 138, loss = 0.39255329966545105
iteration 139, loss = 0.31142014265060425
iteration 140, loss = 0.4285327196121216
iteration 141, loss = 0.26338547468185425
iteration 142, loss = 0.37003737688064575
iteration 143, loss = 0.44333845376968384
iteration 144, loss = 0.2252071499824524
iteration 145, loss = 0.4392865300178528
iteration 146, loss = 0.2993130087852478
iteration 147, loss = 0.2688312530517578
iteration 148, loss = 0.5081721544265747
iteration 149, loss = 0.2682228982448578
iteration 150, loss = 0.37963682413101196
iteration 151, loss = 0.31685197353363037
iteration 152, loss = 0.4364504814147949
iteration 153, loss = 0.5182806253433228
iteration 154, loss = 0.24427884817123413
iteration 155, loss = 0.267092227935791
iteration 156, loss = 0.3610581159591675
iteration 157, loss = 0.37201452255249023
iteration 158, loss = 0.3276674747467041
iteration 159, loss = 0.4302423596382141
iteration 160, loss = 0.33707261085510254
iteration 161, loss = 0.3211365342140198
iteration 162, loss = 0.3864925503730774
iteration 163, loss = 0.436115026473999
iteration 164, loss = 0.26676666736602783
iteration 165, loss = 0.365655779838562
iteration 166, loss = 0.3742147982120514
iteration 167, loss = 0.3498266041278839
iteration 168, loss = 0.30343303084373474
iteration 169, loss = 0.3763531446456909
iteration 170, loss = 0.37292319536209106
iteration 171, loss = 0.3175092041492462
iteration 172, loss = 0.22406768798828125
iteration 173, loss = 0.2555740177631378
iteration 174, loss = 0.39985278248786926
iteration 175, loss = 0.2112622857093811
iteration 176, loss = 0.3313639760017395
iteration 177, loss = 0.25840121507644653
iteration 178, loss = 0.4342384338378906
iteration 179, loss = 0.27590128779411316
iteration 180, loss = 0.3033866882324219
iteration 181, loss = 0.3536456823348999
iteration 182, loss = 0.34791821241378784
iteration 183, loss = 0.21653741598129272
iteration 184, loss = 0.22130191326141357
iteration 185, loss = 0.17815889418125153
iteration 186, loss = 0.3516780734062195
iteration 187, loss = 0.3231357932090759
iteration 188, loss = 0.16289633512496948
iteration 189, loss = 0.3161329925060272
iteration 190, loss = 0.2700420320034027
iteration 191, loss = 0.270516574382782
iteration 192, loss = 0.3312038779258728
iteration 193, loss = 0.4350742697715759
iteration 194, loss = 0.4086684584617615
iteration 195, loss = 0.49355870485305786
iteration 196, loss = 0.2477225810289383
iteration 197, loss = 0.25236940383911133
iteration 198, loss = 0.33201491832733154
iteration 199, loss = 0.3567935824394226
iteration 200, loss = 0.46386390924453735
iteration 201, loss = 0.413422167301178
iteration 202, loss = 0.4503200650215149
iteration 203, loss = 0.3664417564868927
iteration 204, loss = 0.40770041942596436
iteration 205, loss = 0.3368387222290039
iteration 206, loss = 0.3660871386528015
iteration 207, loss = 0.23359373211860657
iteration 208, loss = 0.3514840304851532
iteration 209, loss = 0.3344537317752838
iteration 210, loss = 0.3583036959171295
iteration 211, loss = 0.38049983978271484
iteration 212, loss = 0.441413551568985
iteration 213, loss = 0.08686596155166626
iteration 214, loss = 0.38664841651916504
iteration 215, loss = 0.3492089509963989
iteration 216, loss = 0.4348067343235016
iteration 217, loss = 0.295040488243103
iteration 218, loss = 0.39767324924468994
iteration 219, loss = 0.4386677145957947
iteration 220, loss = 0.39698076248168945
iteration 221, loss = 0.2934110164642334
iteration 222, loss = 0.41430073976516724
iteration 223, loss = 0.37296807765960693
iteration 224, loss = 0.3316552937030792
iteration 225, loss = 0.22787286341190338
iteration 226, loss = 0.4256476163864136
iteration 227, loss = 0.37225812673568726
iteration 228, loss = 0.21573615074157715
iteration 229, loss = 0.28382325172424316
iteration 230, loss = 0.28528618812561035
iteration 231, loss = 0.2918507158756256
iteration 232, loss = 0.3574298322200775
iteration 233, loss = 0.34347522258758545
iteration 234, loss = 0.384107381105423
iteration 235, loss = 0.37669962644577026
iteration 236, loss = 0.33582791686058044
iteration 237, loss = 0.2841024398803711
iteration 238, loss = 0.2257557213306427
iteration 239, loss = 0.3549773097038269
iteration 240, loss = 0.35137778520584106
iteration 241, loss = 0.3779149353504181
iteration 242, loss = 0.2555120587348938
iteration 243, loss = 0.36617594957351685
iteration 244, loss = 0.17165033519268036
iteration 245, loss = 0.33116430044174194
iteration 246, loss = 0.32550546526908875
iteration 247, loss = 0.44938719272613525
iteration 248, loss = 0.30115658044815063
iteration 249, loss = 0.4300234019756317
iteration 250, loss = 0.5263515710830688
iteration 251, loss = 0.43122386932373047
iteration 252, loss = 0.4058741629123688
iteration 253, loss = 0.41436684131622314
iteration 254, loss = 0.3412875533103943
iteration 255, loss = 0.3614054322242737
iteration 256, loss = 0.3395988643169403
iteration 257, loss = 0.39291220903396606
iteration 258, loss = 0.3289165496826172
iteration 259, loss = 0.2010955810546875
iteration 260, loss = 0.32356756925582886
iteration 261, loss = 0.3752717673778534
iteration 262, loss = 0.524698793888092
iteration 263, loss = 0.3548627495765686
iteration 264, loss = 0.3644341230392456
iteration 265, loss = 0.3477771580219269
iteration 266, loss = 0.2775280475616455
iteration 267, loss = 0.3338392376899719
iteration 268, loss = 0.5548672080039978
iteration 269, loss = 0.2917800545692444
iteration 270, loss = 0.3242948055267334
iteration 271, loss = 0.43374013900756836
iteration 272, loss = 0.3347337245941162
iteration 273, loss = 0.3757282793521881
iteration 274, loss = 0.24493056535720825
iteration 275, loss = 0.31882452964782715
iteration 276, loss = 0.36926689743995667
iteration 277, loss = 0.3692569136619568
iteration 278, loss = 0.3482987880706787
iteration 279, loss = 0.23088444769382477
iteration 280, loss = 0.26383569836616516
iteration 281, loss = 0.34470394253730774
iteration 282, loss = 0.37160933017730713
iteration 283, loss = 0.1668320596218109
iteration 284, loss = 0.41998669505119324
iteration 285, loss = 0.3192034959793091
iteration 286, loss = 0.2483445703983307
iteration 287, loss = 0.334072470664978
iteration 288, loss = 0.4129642844200134
iteration 289, loss = 0.31772664189338684
iteration 290, loss = 0.22889024019241333
iteration 291, loss = 0.3110302686691284
iteration 292, loss = 0.44693315029144287
iteration 293, loss = 0.5050835609436035
iteration 294, loss = 0.2893308997154236
iteration 295, loss = 0.24023090302944183
iteration 296, loss = 0.19815237820148468
iteration 297, loss = 0.24412111937999725
iteration 298, loss = 0.3824206590652466
iteration 299, loss = 0.33052492141723633
iteration 0, loss = 0.25197067856788635
iteration 1, loss = 0.2663518786430359
iteration 2, loss = 0.3844844400882721
iteration 3, loss = 0.43767568469047546
iteration 4, loss = 0.41780513525009155
iteration 5, loss = 0.22700610756874084
iteration 6, loss = 0.3127746284008026
iteration 7, loss = 0.30071407556533813
iteration 8, loss = 0.45962369441986084
iteration 9, loss = 0.32289281487464905
iteration 10, loss = 0.3194677233695984
iteration 11, loss = 0.2558532953262329
iteration 12, loss = 0.19616103172302246
iteration 13, loss = 0.162040114402771
iteration 14, loss = 0.23953543603420258
iteration 15, loss = 0.34266397356987
iteration 16, loss = 0.32896843552589417
iteration 17, loss = 0.2455310821533203
iteration 18, loss = 0.29377248883247375
iteration 19, loss = 0.27241361141204834
iteration 20, loss = 0.21974056959152222
iteration 21, loss = 0.2232007384300232
iteration 22, loss = 0.3195364773273468
iteration 23, loss = 0.40351513028144836
iteration 24, loss = 0.31490904092788696
iteration 25, loss = 0.4847327470779419
iteration 26, loss = 0.3996555805206299
iteration 27, loss = 0.296599805355072
iteration 28, loss = 0.36306536197662354
iteration 29, loss = 0.522117555141449
iteration 30, loss = 0.31616151332855225
iteration 31, loss = 0.3457598388195038
iteration 32, loss = 0.26917925477027893
iteration 33, loss = 0.2705986201763153
iteration 34, loss = 0.3887660503387451
iteration 35, loss = 0.38108789920806885
iteration 36, loss = 0.2512540817260742
iteration 37, loss = 0.17656365036964417
iteration 38, loss = 0.3225128650665283
iteration 39, loss = 0.21284419298171997
iteration 40, loss = 0.3260343074798584
iteration 41, loss = 0.21597778797149658
iteration 42, loss = 0.36611151695251465
iteration 43, loss = 0.2869921028614044
iteration 44, loss = 0.493360310792923
iteration 45, loss = 0.23723934590816498
iteration 46, loss = 0.5085487365722656
iteration 47, loss = 0.2485509216785431
iteration 48, loss = 0.34491583704948425
iteration 49, loss = 0.13943760097026825
iteration 50, loss = 0.27877652645111084
iteration 51, loss = 0.4777752161026001
iteration 52, loss = 0.23742949962615967
iteration 53, loss = 0.43454545736312866
iteration 54, loss = 0.31386926770210266
iteration 55, loss = 0.2575918734073639
iteration 56, loss = 0.43471208214759827
iteration 57, loss = 0.28065377473831177
iteration 58, loss = 0.26183220744132996
iteration 59, loss = 0.295236736536026
iteration 60, loss = 0.15752172470092773
iteration 61, loss = 0.4329416751861572
iteration 62, loss = 0.47445380687713623
iteration 63, loss = 0.21514198184013367
iteration 64, loss = 0.21646948158740997
iteration 65, loss = 0.3114745318889618
iteration 66, loss = 0.3343347907066345
iteration 67, loss = 0.2944493293762207
iteration 68, loss = 0.2912026345729828
iteration 69, loss = 0.28893184661865234
iteration 70, loss = 0.4336737394332886
iteration 71, loss = 0.34159648418426514
iteration 72, loss = 0.4660119414329529
iteration 73, loss = 0.3516453504562378
iteration 74, loss = 0.5515727996826172
iteration 75, loss = 0.5403486490249634
iteration 76, loss = 0.31563276052474976
iteration 77, loss = 0.4422188401222229
iteration 78, loss = 0.29287976026535034
iteration 79, loss = 0.4440094232559204
iteration 80, loss = 0.39586707949638367
iteration 81, loss = 0.36215105652809143
iteration 82, loss = 0.3008260726928711
iteration 83, loss = 0.30694475769996643
iteration 84, loss = 0.39834582805633545
iteration 85, loss = 0.31680983304977417
iteration 86, loss = 0.325883686542511
iteration 87, loss = 0.2487194836139679
iteration 88, loss = 0.2964382469654083
iteration 89, loss = 0.4574265480041504
iteration 90, loss = 0.4223189055919647
iteration 91, loss = 0.40240269899368286
iteration 92, loss = 0.3885524868965149
iteration 93, loss = 0.3065873980522156
iteration 94, loss = 0.3048209547996521
iteration 95, loss = 0.5080550312995911
iteration 96, loss = 0.279502272605896
iteration 97, loss = 0.3617689311504364
iteration 98, loss = 0.25548669695854187
iteration 99, loss = 0.2723948657512665
iteration 100, loss = 0.4992053210735321
iteration 101, loss = 0.34277012944221497
iteration 102, loss = 0.21264779567718506
iteration 103, loss = 0.29481959342956543
iteration 104, loss = 0.34376591444015503
iteration 105, loss = 0.2620723247528076
iteration 106, loss = 0.3241582214832306
iteration 107, loss = 0.3081110715866089
iteration 108, loss = 0.3051532208919525
iteration 109, loss = 0.18483608961105347
iteration 110, loss = 0.25702857971191406
iteration 111, loss = 0.4384208023548126
iteration 112, loss = 0.2303549349308014
iteration 113, loss = 0.2954750657081604
iteration 114, loss = 0.2897910475730896
iteration 115, loss = 0.11837159097194672
iteration 116, loss = 0.22407260537147522
iteration 117, loss = 0.571042001247406
iteration 118, loss = 0.2990422248840332
iteration 119, loss = 0.31773510575294495
iteration 120, loss = 0.21295802295207977
iteration 121, loss = 0.36241593956947327
iteration 122, loss = 0.3690418303012848
iteration 123, loss = 0.479905903339386
iteration 124, loss = 0.39933013916015625
iteration 125, loss = 0.46187782287597656
iteration 126, loss = 0.1846829205751419
iteration 127, loss = 0.3922095000743866
iteration 128, loss = 0.3031156063079834
iteration 129, loss = 0.31788885593414307
iteration 130, loss = 0.36948221921920776
iteration 131, loss = 0.5353406667709351
iteration 132, loss = 0.41740667819976807
iteration 133, loss = 0.35083985328674316
iteration 134, loss = 0.4592362940311432
iteration 135, loss = 0.33184874057769775
iteration 136, loss = 0.3583905100822449
iteration 137, loss = 0.38208645582199097
iteration 138, loss = 0.16699343919754028
iteration 139, loss = 0.4540998339653015
iteration 140, loss = 0.6316455602645874
iteration 141, loss = 0.21081718802452087
iteration 142, loss = 0.31460893154144287
iteration 143, loss = 0.27026596665382385
iteration 144, loss = 0.26670509576797485
iteration 145, loss = 0.2700890898704529
iteration 146, loss = 0.3388466238975525
iteration 147, loss = 0.48148155212402344
iteration 148, loss = 0.2413853108882904
iteration 149, loss = 0.2587650418281555
iteration 150, loss = 0.31388500332832336
iteration 151, loss = 0.42253440618515015
iteration 152, loss = 0.21369391679763794
iteration 153, loss = 0.3849385678768158
iteration 154, loss = 0.531714677810669
iteration 155, loss = 0.26509571075439453
iteration 156, loss = 0.3168538510799408
iteration 157, loss = 0.43908604979515076
iteration 158, loss = 0.4554630219936371
iteration 159, loss = 0.14139966666698456
iteration 160, loss = 0.25950098037719727
iteration 161, loss = 0.30835115909576416
iteration 162, loss = 0.3084982633590698
iteration 163, loss = 0.17945507168769836
iteration 164, loss = 0.298473596572876
iteration 165, loss = 0.23971541225910187
iteration 166, loss = 0.33526676893234253
iteration 167, loss = 0.4363662600517273
iteration 168, loss = 0.21084628999233246
iteration 169, loss = 0.298733115196228
iteration 170, loss = 0.3609577417373657
iteration 171, loss = 0.41497981548309326
iteration 172, loss = 0.2953152358531952
iteration 173, loss = 0.5321038961410522
iteration 174, loss = 0.2511240839958191
iteration 175, loss = 0.30988115072250366
iteration 176, loss = 0.21849210560321808
iteration 177, loss = 0.26538246870040894
iteration 178, loss = 0.41946691274642944
iteration 179, loss = 0.23067593574523926
iteration 180, loss = 0.4554606080055237
iteration 181, loss = 0.1050935909152031
iteration 182, loss = 0.2216082662343979
iteration 183, loss = 0.37416398525238037
iteration 184, loss = 0.23388269543647766
iteration 185, loss = 0.26116248965263367
iteration 186, loss = 0.21055874228477478
iteration 187, loss = 0.3032316565513611
iteration 188, loss = 0.37287837266921997
iteration 189, loss = 0.26899486780166626
iteration 190, loss = 0.2017282098531723
iteration 191, loss = 0.36768344044685364
iteration 192, loss = 0.3741610646247864
iteration 193, loss = 0.42417606711387634
iteration 194, loss = 0.2568472623825073
iteration 195, loss = 0.3769703805446625
iteration 196, loss = 0.34061723947525024
iteration 197, loss = 0.25782543420791626
iteration 198, loss = 0.3936075270175934
iteration 199, loss = 0.36617687344551086
iteration 200, loss = 0.393584668636322
iteration 201, loss = 0.21598345041275024
iteration 202, loss = 0.3339436948299408
iteration 203, loss = 0.49428802728652954
iteration 204, loss = 0.38285860419273376
iteration 205, loss = 0.3976059854030609
iteration 206, loss = 0.3368609547615051
iteration 207, loss = 0.2595787048339844
iteration 208, loss = 0.3459840416908264
iteration 209, loss = 0.20738093554973602
iteration 210, loss = 0.41470998525619507
iteration 211, loss = 0.2596823573112488
iteration 212, loss = 0.22650781273841858
iteration 213, loss = 0.23039573431015015
iteration 214, loss = 0.37315285205841064
iteration 215, loss = 0.381490558385849
iteration 216, loss = 0.30040326714515686
iteration 217, loss = 0.2687034010887146
iteration 218, loss = 0.16910624504089355
iteration 219, loss = 0.31204548478126526
iteration 220, loss = 0.2697935700416565
iteration 221, loss = 0.2765091061592102
iteration 222, loss = 0.36323174834251404
iteration 223, loss = 0.3339519798755646
iteration 224, loss = 0.33597293496131897
iteration 225, loss = 0.36289680004119873
iteration 226, loss = 0.4755243957042694
iteration 227, loss = 0.36333438754081726
iteration 228, loss = 0.3597635328769684
iteration 229, loss = 0.312961220741272
iteration 230, loss = 0.3428991734981537
iteration 231, loss = 0.33640164136886597
iteration 232, loss = 0.43338847160339355
iteration 233, loss = 0.23036712408065796
iteration 234, loss = 0.29190611839294434
iteration 235, loss = 0.17314830422401428
iteration 236, loss = 0.1283607929944992
iteration 237, loss = 0.4872495234012604
iteration 238, loss = 0.3612716495990753
iteration 239, loss = 0.2717919945716858
iteration 240, loss = 0.246519073843956
iteration 241, loss = 0.48661118745803833
iteration 242, loss = 0.3456488847732544
iteration 243, loss = 0.4515291750431061
iteration 244, loss = 0.32601398229599
iteration 245, loss = 0.24098369479179382
iteration 246, loss = 0.26359817385673523
iteration 247, loss = 0.22020778059959412
iteration 248, loss = 0.3712877035140991
iteration 249, loss = 0.19564339518547058
iteration 250, loss = 0.30408331751823425
iteration 251, loss = 0.42352163791656494
iteration 252, loss = 0.35847795009613037
iteration 253, loss = 0.2967318892478943
iteration 254, loss = 0.3068588972091675
iteration 255, loss = 0.2148064523935318
iteration 256, loss = 0.21887940168380737
iteration 257, loss = 0.37108927965164185
iteration 258, loss = 0.3122466802597046
iteration 259, loss = 0.35305216908454895
iteration 260, loss = 0.37403881549835205
iteration 261, loss = 0.395751953125
iteration 262, loss = 0.23471303284168243
iteration 263, loss = 0.4228050112724304
iteration 264, loss = 0.3456980586051941
iteration 265, loss = 0.17433103919029236
iteration 266, loss = 0.31674903631210327
iteration 267, loss = 0.259043425321579
iteration 268, loss = 0.28547653555870056
iteration 269, loss = 0.22768506407737732
iteration 270, loss = 0.3076522946357727
iteration 271, loss = 0.36865630745887756
iteration 272, loss = 0.3797362446784973
iteration 273, loss = 0.4034070372581482
iteration 274, loss = 0.3061598539352417
iteration 275, loss = 0.334189236164093
iteration 276, loss = 0.28983235359191895
iteration 277, loss = 0.2463243454694748
iteration 278, loss = 0.27050572633743286
iteration 279, loss = 0.28356432914733887
iteration 280, loss = 0.35898125171661377
iteration 281, loss = 0.31123030185699463
iteration 282, loss = 0.22793589532375336
iteration 283, loss = 0.43220847845077515
iteration 284, loss = 0.14304034411907196
iteration 285, loss = 0.26426446437835693
iteration 286, loss = 0.3397597670555115
iteration 287, loss = 0.21438175439834595
iteration 288, loss = 0.1431570053100586
iteration 289, loss = 0.30580610036849976
iteration 290, loss = 0.4168453812599182
iteration 291, loss = 0.31120139360427856
iteration 292, loss = 0.28309836983680725
iteration 293, loss = 0.40270721912384033
iteration 294, loss = 0.18330532312393188
iteration 295, loss = 0.42948058247566223
iteration 296, loss = 0.2243928164243698
iteration 297, loss = 0.192818284034729
iteration 298, loss = 0.26273563504219055
iteration 299, loss = 0.30974698066711426
iteration 0, loss = 0.30951598286628723
iteration 1, loss = 0.5203804969787598
iteration 2, loss = 0.38591790199279785
iteration 3, loss = 0.20556120574474335
iteration 4, loss = 0.3658236861228943
iteration 5, loss = 0.3444250822067261
iteration 6, loss = 0.38044679164886475
iteration 7, loss = 0.2244236171245575
iteration 8, loss = 0.3942263722419739
iteration 9, loss = 0.3319077789783478
iteration 10, loss = 0.4552614390850067
iteration 11, loss = 0.21112844347953796
iteration 12, loss = 0.4274135231971741
iteration 13, loss = 0.45402005314826965
iteration 14, loss = 0.1533341407775879
iteration 15, loss = 0.2485884130001068
iteration 16, loss = 0.3414093852043152
iteration 17, loss = 0.3814840018749237
iteration 18, loss = 0.24571803212165833
iteration 19, loss = 0.40211454033851624
iteration 20, loss = 0.4178348779678345
iteration 21, loss = 0.4118283987045288
iteration 22, loss = 0.23323065042495728
iteration 23, loss = 0.24978327751159668
iteration 24, loss = 0.1460062861442566
iteration 25, loss = 0.5767476558685303
iteration 26, loss = 0.5965191721916199
iteration 27, loss = 0.2671714425086975
iteration 28, loss = 0.3548542559146881
iteration 29, loss = 0.38349807262420654
iteration 30, loss = 0.20664533972740173
iteration 31, loss = 0.2695724368095398
iteration 32, loss = 0.2373548299074173
iteration 33, loss = 0.3640286922454834
iteration 34, loss = 0.34557756781578064
iteration 35, loss = 0.3283488154411316
iteration 36, loss = 0.17888981103897095
iteration 37, loss = 0.22445601224899292
iteration 38, loss = 0.3358917832374573
iteration 39, loss = 0.21413910388946533
iteration 40, loss = 0.18169765174388885
iteration 41, loss = 0.30818289518356323
iteration 42, loss = 0.38402870297431946
iteration 43, loss = 0.4636746942996979
iteration 44, loss = 0.27008572220802307
iteration 45, loss = 0.2243720293045044
iteration 46, loss = 0.3300807774066925
iteration 47, loss = 0.5148253440856934
iteration 48, loss = 0.3499644994735718
iteration 49, loss = 0.3247532248497009
iteration 50, loss = 0.3684002459049225
iteration 51, loss = 0.31048136949539185
iteration 52, loss = 0.35162153840065
iteration 53, loss = 0.1879255622625351
iteration 54, loss = 0.40576469898223877
iteration 55, loss = 0.3823436498641968
iteration 56, loss = 0.27710339426994324
iteration 57, loss = 0.16016888618469238
iteration 58, loss = 0.31104010343551636
iteration 59, loss = 0.17707015573978424
iteration 60, loss = 0.34506261348724365
iteration 61, loss = 0.30598509311676025
iteration 62, loss = 0.275039404630661
iteration 63, loss = 0.2447032481431961
iteration 64, loss = 0.47337275743484497
iteration 65, loss = 0.22677475214004517
iteration 66, loss = 0.2713839113712311
iteration 67, loss = 0.6049758195877075
iteration 68, loss = 0.3618609607219696
iteration 69, loss = 0.24211162328720093
iteration 70, loss = 0.1661941111087799
iteration 71, loss = 0.48091238737106323
iteration 72, loss = 0.26730793714523315
iteration 73, loss = 0.2350483536720276
iteration 74, loss = 0.15996357798576355
iteration 75, loss = 0.29061996936798096
iteration 76, loss = 0.3152204155921936
iteration 77, loss = 0.40585389733314514
iteration 78, loss = 0.30844706296920776
iteration 79, loss = 0.2128097116947174
iteration 80, loss = 0.11579579859972
iteration 81, loss = 0.2449108064174652
iteration 82, loss = 0.1929575800895691
iteration 83, loss = 0.5117067098617554
iteration 84, loss = 0.4310459494590759
iteration 85, loss = 0.29111284017562866
iteration 86, loss = 0.1433148980140686
iteration 87, loss = 0.3496977388858795
iteration 88, loss = 0.35184207558631897
iteration 89, loss = 0.1654268205165863
iteration 90, loss = 0.2650171220302582
iteration 91, loss = 0.55938321352005
iteration 92, loss = 0.17060470581054688
iteration 93, loss = 0.31491923332214355
iteration 94, loss = 0.4052741229534149
iteration 95, loss = 0.31849029660224915
iteration 96, loss = 0.3254176080226898
iteration 97, loss = 0.3922378420829773
iteration 98, loss = 0.5012208819389343
iteration 99, loss = 0.3714295029640198
iteration 100, loss = 0.33720463514328003
iteration 101, loss = 0.32672441005706787
iteration 102, loss = 0.40106093883514404
iteration 103, loss = 0.39868882298469543
iteration 104, loss = 0.3393731117248535
iteration 105, loss = 0.30539625883102417
iteration 106, loss = 0.2643691897392273
iteration 107, loss = 0.3209652900695801
iteration 108, loss = 0.27387213706970215
iteration 109, loss = 0.22799041867256165
iteration 110, loss = 0.11948941648006439
iteration 111, loss = 0.289644718170166
iteration 112, loss = 0.5307518243789673
iteration 113, loss = 0.21619029343128204
iteration 114, loss = 0.3723691701889038
iteration 115, loss = 0.4354184865951538
iteration 116, loss = 0.35901790857315063
iteration 117, loss = 0.3293967843055725
iteration 118, loss = 0.29151880741119385
iteration 119, loss = 0.21600058674812317
iteration 120, loss = 0.274209588766098
iteration 121, loss = 0.24079880118370056
iteration 122, loss = 0.17782121896743774
iteration 123, loss = 0.20098792016506195
iteration 124, loss = 0.45991480350494385
iteration 125, loss = 0.39784759283065796
iteration 126, loss = 0.40119972825050354
iteration 127, loss = 0.23283271491527557
iteration 128, loss = 0.23104166984558105
iteration 129, loss = 0.23890602588653564
iteration 130, loss = 0.28739672899246216
iteration 131, loss = 0.26812493801116943
iteration 132, loss = 0.45147767663002014
iteration 133, loss = 0.12188971042633057
iteration 134, loss = 0.13762812316417694
iteration 135, loss = 0.3002244830131531
iteration 136, loss = 0.24357566237449646
iteration 137, loss = 0.3303069770336151
iteration 138, loss = 0.2187277227640152
iteration 139, loss = 0.48493874073028564
iteration 140, loss = 0.16422227025032043
iteration 141, loss = 0.5194430947303772
iteration 142, loss = 0.29899999499320984
iteration 143, loss = 0.34948617219924927
iteration 144, loss = 0.4043489694595337
iteration 145, loss = 0.28619495034217834
iteration 146, loss = 0.25688549876213074
iteration 147, loss = 0.2888084053993225
iteration 148, loss = 0.35334542393684387
iteration 149, loss = 0.3223235607147217
iteration 150, loss = 0.5212032198905945
iteration 151, loss = 0.3374127149581909
iteration 152, loss = 0.507190465927124
iteration 153, loss = 0.14855234324932098
iteration 154, loss = 0.36588576436042786
iteration 155, loss = 0.30500397086143494
iteration 156, loss = 0.26233068108558655
iteration 157, loss = 0.24488171935081482
iteration 158, loss = 0.18397636711597443
iteration 159, loss = 0.23279023170471191
iteration 160, loss = 0.3288002610206604
iteration 161, loss = 0.49029284715652466
iteration 162, loss = 0.10411540418863297
iteration 163, loss = 0.360880970954895
iteration 164, loss = 0.24757470190525055
iteration 165, loss = 0.3363327383995056
iteration 166, loss = 0.3117626905441284
iteration 167, loss = 0.2828444242477417
iteration 168, loss = 0.2482595145702362
iteration 169, loss = 0.24578917026519775
iteration 170, loss = 0.2898045480251312
iteration 171, loss = 0.38031136989593506
iteration 172, loss = 0.4335401952266693
iteration 173, loss = 0.35406991839408875
iteration 174, loss = 0.4219793975353241
iteration 175, loss = 0.4921802282333374
iteration 176, loss = 0.35405129194259644
iteration 177, loss = 0.45152804255485535
iteration 178, loss = 0.5269855856895447
iteration 179, loss = 0.36724013090133667
iteration 180, loss = 0.29560741782188416
iteration 181, loss = 0.4684319496154785
iteration 182, loss = 0.3377423584461212
iteration 183, loss = 0.3711899518966675
iteration 184, loss = 0.32666492462158203
iteration 185, loss = 0.16583669185638428
iteration 186, loss = 0.24278873205184937
iteration 187, loss = 0.41630566120147705
iteration 188, loss = 0.1384299397468567
iteration 189, loss = 0.3058702051639557
iteration 190, loss = 0.26579833030700684
iteration 191, loss = 0.39592817425727844
iteration 192, loss = 0.4151422083377838
iteration 193, loss = 0.15902858972549438
iteration 194, loss = 0.14213094115257263
iteration 195, loss = 0.2735918164253235
iteration 196, loss = 0.3098289668560028
iteration 197, loss = 0.46545442938804626
iteration 198, loss = 0.2913738489151001
iteration 199, loss = 0.3371709883213043
iteration 200, loss = 0.17903511226177216
iteration 201, loss = 0.2529216408729553
iteration 202, loss = 0.16503073275089264
iteration 203, loss = 0.22581800818443298
iteration 204, loss = 0.3432356119155884
iteration 205, loss = 0.3255717158317566
iteration 206, loss = 0.2971908450126648
iteration 207, loss = 0.13132506608963013
iteration 208, loss = 0.21284356713294983
iteration 209, loss = 0.10063888132572174
iteration 210, loss = 0.3771810829639435
iteration 211, loss = 0.15401029586791992
iteration 212, loss = 0.301052987575531
iteration 213, loss = 0.2990463376045227
iteration 214, loss = 0.4484879970550537
iteration 215, loss = 0.3030897080898285
iteration 216, loss = 0.439434289932251
iteration 217, loss = 0.22008317708969116
iteration 218, loss = 0.32663780450820923
iteration 219, loss = 0.2863885462284088
iteration 220, loss = 0.3390507698059082
iteration 221, loss = 0.28886744379997253
iteration 222, loss = 0.31085139513015747
iteration 223, loss = 0.3882601261138916
iteration 224, loss = 0.20867827534675598
iteration 225, loss = 0.27212685346603394
iteration 226, loss = 0.21567599475383759
iteration 227, loss = 0.21314707398414612
iteration 228, loss = 0.16407382488250732
iteration 229, loss = 0.2150329053401947
iteration 230, loss = 0.4780697822570801
iteration 231, loss = 0.1705344319343567
iteration 232, loss = 0.4062381088733673
iteration 233, loss = 0.11184284090995789
iteration 234, loss = 0.3771841526031494
iteration 235, loss = 0.32918018102645874
iteration 236, loss = 0.33332961797714233
iteration 237, loss = 0.3644951581954956
iteration 238, loss = 0.37064194679260254
iteration 239, loss = 0.1710882931947708
iteration 240, loss = 0.1873561292886734
iteration 241, loss = 0.34631505608558655
iteration 242, loss = 0.23364253342151642
iteration 243, loss = 0.38730698823928833
iteration 244, loss = 0.25226664543151855
iteration 245, loss = 0.2367764413356781
iteration 246, loss = 0.08660463243722916
iteration 247, loss = 0.31566089391708374
iteration 248, loss = 0.3078916072845459
iteration 249, loss = 0.2836271822452545
iteration 250, loss = 0.23705436289310455
iteration 251, loss = 0.339916467666626
iteration 252, loss = 0.26880931854248047
iteration 253, loss = 0.32913994789123535
iteration 254, loss = 0.2767605185508728
iteration 255, loss = 0.36448466777801514
iteration 256, loss = 0.15179844200611115
iteration 257, loss = 0.3581470251083374
iteration 258, loss = 0.3685917854309082
iteration 259, loss = 0.2879657447338104
iteration 260, loss = 0.42882275581359863
iteration 261, loss = 0.23813113570213318
iteration 262, loss = 0.5616570115089417
iteration 263, loss = 0.0503619909286499
iteration 264, loss = 0.3280770182609558
iteration 265, loss = 0.3153134286403656
iteration 266, loss = 0.2881733775138855
iteration 267, loss = 0.2882777154445648
iteration 268, loss = 0.24196204543113708
iteration 269, loss = 0.5380299687385559
iteration 270, loss = 0.31502628326416016
iteration 271, loss = 0.22033792734146118
iteration 272, loss = 0.15306220948696136
iteration 273, loss = 0.21037817001342773
iteration 274, loss = 0.1957838237285614
iteration 275, loss = 0.18908140063285828
iteration 276, loss = 0.34677886962890625
iteration 277, loss = 0.331328421831131
iteration 278, loss = 0.2637050747871399
iteration 279, loss = 0.1853412389755249
iteration 280, loss = 0.3098191022872925
iteration 281, loss = 0.270111083984375
iteration 282, loss = 0.30582964420318604
iteration 283, loss = 0.40631139278411865
iteration 284, loss = 0.36211591958999634
iteration 285, loss = 0.5197678804397583
iteration 286, loss = 0.393209308385849
iteration 287, loss = 0.34985482692718506
iteration 288, loss = 0.4628724753856659
iteration 289, loss = 0.33305853605270386
iteration 290, loss = 0.16185632348060608
iteration 291, loss = 0.27221471071243286
iteration 292, loss = 0.46877944469451904
iteration 293, loss = 0.43500760197639465
iteration 294, loss = 0.30308443307876587
iteration 295, loss = 0.2958676218986511
iteration 296, loss = 0.2022155076265335
iteration 297, loss = 0.5244777798652649
iteration 298, loss = 0.2865113615989685
iteration 299, loss = 0.03909417241811752
iteration 0, loss = 0.3441694974899292
iteration 1, loss = 0.45844054222106934
iteration 2, loss = 0.17295677959918976
iteration 3, loss = 0.44981345534324646
iteration 4, loss = 0.2848662734031677
iteration 5, loss = 0.28831976652145386
iteration 6, loss = 0.30887049436569214
iteration 7, loss = 0.23565074801445007
iteration 8, loss = 0.2181013524532318
iteration 9, loss = 0.28971201181411743
iteration 10, loss = 0.2354615479707718
iteration 11, loss = 0.24811789393424988
iteration 12, loss = 0.2917288541793823
iteration 13, loss = 0.17893150448799133
iteration 14, loss = 0.034872736781835556
iteration 15, loss = 0.4433566927909851
iteration 16, loss = 0.33231398463249207
iteration 17, loss = 0.45292171835899353
iteration 18, loss = 0.33325695991516113
iteration 19, loss = 0.22459563612937927
iteration 20, loss = 0.251053124666214
iteration 21, loss = 0.26205891370773315
iteration 22, loss = 0.1968032568693161
iteration 23, loss = 0.32944467663764954
iteration 24, loss = 0.17631494998931885
iteration 25, loss = 0.10262509435415268
iteration 26, loss = 0.12829594314098358
iteration 27, loss = 0.3128030300140381
iteration 28, loss = 0.18688155710697174
iteration 29, loss = 0.29517924785614014
iteration 30, loss = 0.44427090883255005
iteration 31, loss = 0.4712029695510864
iteration 32, loss = 0.3177773952484131
iteration 33, loss = 0.28058210015296936
iteration 34, loss = 0.14609722793102264
iteration 35, loss = 0.3269290030002594
iteration 36, loss = 0.31910666823387146
iteration 37, loss = 0.29612991213798523
iteration 38, loss = 0.2471507340669632
iteration 39, loss = 0.2642318904399872
iteration 40, loss = 0.2499910295009613
iteration 41, loss = 0.30769121646881104
iteration 42, loss = 0.40010306239128113
iteration 43, loss = 0.2609702944755554
iteration 44, loss = 0.332409530878067
iteration 45, loss = 0.46855664253234863
iteration 46, loss = 0.2587372362613678
iteration 47, loss = 0.1445762813091278
iteration 48, loss = 0.30129480361938477
iteration 49, loss = 0.1880159080028534
iteration 50, loss = 0.30720168352127075
iteration 51, loss = 0.22587133944034576
iteration 52, loss = 0.3597906231880188
iteration 53, loss = 0.2810758352279663
iteration 54, loss = 0.3291573226451874
iteration 55, loss = 0.29834413528442383
iteration 56, loss = 0.20575867593288422
iteration 57, loss = 0.1952057182788849
iteration 58, loss = 0.12850193679332733
iteration 59, loss = 0.2084800899028778
iteration 60, loss = 0.3353160321712494
iteration 61, loss = 0.27212777733802795
iteration 62, loss = 0.23026084899902344
iteration 63, loss = 0.5126710534095764
iteration 64, loss = 0.333858847618103
iteration 65, loss = 0.25520265102386475
iteration 66, loss = 0.26829591393470764
iteration 67, loss = 0.34926846623420715
iteration 68, loss = 0.49615412950515747
iteration 69, loss = 0.3742326498031616
iteration 70, loss = 0.21761715412139893
iteration 71, loss = 0.26922744512557983
iteration 72, loss = 0.2018381506204605
iteration 73, loss = 0.1619642674922943
iteration 74, loss = 0.17835856974124908
iteration 75, loss = 0.22332441806793213
iteration 76, loss = 0.09296819567680359
iteration 77, loss = 0.21337808668613434
iteration 78, loss = 0.29783543944358826
iteration 79, loss = 0.2377028912305832
iteration 80, loss = 0.4490325152873993
iteration 81, loss = 0.34752118587493896
iteration 82, loss = 0.1625315248966217
iteration 83, loss = 0.4082017242908478
iteration 84, loss = 0.31652894616127014
iteration 85, loss = 0.10510169714689255
iteration 86, loss = 0.29542502760887146
iteration 87, loss = 0.268415629863739
iteration 88, loss = 0.15120261907577515
iteration 89, loss = 0.08728950470685959
iteration 90, loss = 0.57477205991745
iteration 91, loss = 0.11835084110498428
iteration 92, loss = 0.5765807032585144
iteration 93, loss = 0.3076634109020233
iteration 94, loss = 0.28283730149269104
iteration 95, loss = 0.30178916454315186
iteration 96, loss = 0.3121934235095978
iteration 97, loss = 0.2402956187725067
iteration 98, loss = 0.4330216646194458
iteration 99, loss = 0.3477586805820465
iteration 100, loss = 0.2994725704193115
iteration 101, loss = 0.4921061098575592
iteration 102, loss = 0.10383263230323792
iteration 103, loss = 0.30692172050476074
iteration 104, loss = 0.22467532753944397
iteration 105, loss = 0.3782466650009155
iteration 106, loss = 0.27527737617492676
iteration 107, loss = 0.3332161605358124
iteration 108, loss = 0.2137533277273178
iteration 109, loss = 0.45753413438796997
iteration 110, loss = 0.3174053430557251
iteration 111, loss = 0.2878342866897583
iteration 112, loss = 0.305411696434021
iteration 113, loss = 0.2539644241333008
iteration 114, loss = 0.4223254919052124
iteration 115, loss = 0.297648161649704
iteration 116, loss = 0.4932258725166321
iteration 117, loss = 0.33000677824020386
iteration 118, loss = 0.37965095043182373
iteration 119, loss = 0.2754489481449127
iteration 120, loss = 0.35748374462127686
iteration 121, loss = 0.31092122197151184
iteration 122, loss = 0.13724488019943237
iteration 123, loss = 0.5105569958686829
iteration 124, loss = 0.2733582556247711
iteration 125, loss = 0.027129583060741425
iteration 126, loss = 0.450610488653183
iteration 127, loss = 0.05551283806562424
iteration 128, loss = 0.4066685736179352
iteration 129, loss = 0.17354992032051086
iteration 130, loss = 0.3050154149532318
iteration 131, loss = 0.267607182264328
iteration 132, loss = 0.33125460147857666
iteration 133, loss = 0.4025218188762665
iteration 134, loss = 0.1947169452905655
iteration 135, loss = 0.19105544686317444
iteration 136, loss = 0.24064776301383972
iteration 137, loss = 0.3301100432872772
iteration 138, loss = 0.33511948585510254
iteration 139, loss = 0.222978875041008
iteration 140, loss = 0.20984604954719543
iteration 141, loss = 0.33659544587135315
iteration 142, loss = 0.20232078433036804
iteration 143, loss = 0.18497467041015625
iteration 144, loss = 0.42902788519859314
iteration 145, loss = 0.27096623182296753
iteration 146, loss = 0.4550602436065674
iteration 147, loss = 0.30461475253105164
iteration 148, loss = 0.2855111360549927
iteration 149, loss = 0.20383447408676147
iteration 150, loss = 0.3764422535896301
iteration 151, loss = 0.21943098306655884
iteration 152, loss = 0.34801191091537476
iteration 153, loss = 0.21948663890361786
iteration 154, loss = 0.2516457140445709
iteration 155, loss = 0.20811642706394196
iteration 156, loss = 0.3588027060031891
iteration 157, loss = 0.22439971566200256
iteration 158, loss = 0.5130271911621094
iteration 159, loss = 0.2775077521800995
iteration 160, loss = 0.37110233306884766
iteration 161, loss = 0.36067700386047363
iteration 162, loss = 0.27017927169799805
iteration 163, loss = 0.1453617513179779
iteration 164, loss = 0.2658471167087555
iteration 165, loss = 0.3867992162704468
iteration 166, loss = 0.19101464748382568
iteration 167, loss = 0.20309415459632874
iteration 168, loss = 0.23769882321357727
iteration 169, loss = 0.29400914907455444
iteration 170, loss = 0.2641429603099823
iteration 171, loss = 0.29313236474990845
iteration 172, loss = 0.2683415114879608
iteration 173, loss = 0.2047414481639862
iteration 174, loss = 0.1534658521413803
iteration 175, loss = 0.11006857454776764
iteration 176, loss = 0.4395306408405304
iteration 177, loss = 0.14217238128185272
iteration 178, loss = 0.49334362149238586
iteration 179, loss = 0.46214133501052856
iteration 180, loss = 0.3527988791465759
iteration 181, loss = 0.43853670358657837
iteration 182, loss = 0.17900928854942322
iteration 183, loss = 0.3967653512954712
iteration 184, loss = 0.16558028757572174
iteration 185, loss = 0.31623366475105286
iteration 186, loss = 0.3072742223739624
iteration 187, loss = 0.16987237334251404
iteration 188, loss = 0.1495479792356491
iteration 189, loss = 0.5061438679695129
iteration 190, loss = 0.4163849353790283
iteration 191, loss = 0.07367368042469025
iteration 192, loss = 0.11448459327220917
iteration 193, loss = 0.5694431066513062
iteration 194, loss = 0.28694358468055725
iteration 195, loss = 0.3354160189628601
iteration 196, loss = 0.22796586155891418
iteration 197, loss = 0.20409253239631653
iteration 198, loss = 0.4444006085395813
iteration 199, loss = 0.25941774249076843
iteration 200, loss = 0.3868058919906616
iteration 201, loss = 0.2751222848892212
iteration 202, loss = 0.08454933017492294
iteration 203, loss = 0.3641132116317749
iteration 204, loss = 0.47039052844047546
iteration 205, loss = 0.2118874043226242
iteration 206, loss = 0.23473525047302246
iteration 207, loss = 0.4399925470352173
iteration 208, loss = 0.15770408511161804
iteration 209, loss = 0.3573368787765503
iteration 210, loss = 0.44180935621261597
iteration 211, loss = 0.40809929370880127
iteration 212, loss = 0.2905650734901428
iteration 213, loss = 0.42573538422584534
iteration 214, loss = 0.43728262186050415
iteration 215, loss = 0.24274736642837524
iteration 216, loss = 0.28825777769088745
iteration 217, loss = 0.19626180827617645
iteration 218, loss = 0.13595108687877655
iteration 219, loss = 0.2841927409172058
iteration 220, loss = 0.15343359112739563
iteration 221, loss = 0.40501147508621216
iteration 222, loss = 0.2405875325202942
iteration 223, loss = 0.45411181449890137
iteration 224, loss = 0.40152204036712646
iteration 225, loss = 0.1766713708639145
iteration 226, loss = 0.2108197659254074
iteration 227, loss = 0.2174987941980362
iteration 228, loss = 0.2055029571056366
iteration 229, loss = 0.322679340839386
iteration 230, loss = 0.22779615223407745
iteration 231, loss = 0.21566703915596008
iteration 232, loss = 0.06887856125831604
iteration 233, loss = 0.08629103004932404
iteration 234, loss = 0.6401975154876709
iteration 235, loss = 0.2389378398656845
iteration 236, loss = 0.3229096531867981
iteration 237, loss = 0.5736265182495117
iteration 238, loss = 0.08610653877258301
iteration 239, loss = 0.4952408969402313
iteration 240, loss = 0.3260233402252197
iteration 241, loss = 0.5485833287239075
iteration 242, loss = 0.4184305667877197
iteration 243, loss = 0.3650479316711426
iteration 244, loss = 0.3100922703742981
iteration 245, loss = 0.4389400780200958
iteration 246, loss = 0.31979820132255554
iteration 247, loss = 0.20269827544689178
iteration 248, loss = 0.3433222472667694
iteration 249, loss = 0.28645455837249756
iteration 250, loss = 0.18128009140491486
iteration 251, loss = 0.4770718812942505
iteration 252, loss = 0.41511350870132446
iteration 253, loss = 0.32348471879959106
iteration 254, loss = 0.13809043169021606
iteration 255, loss = 0.2570076584815979
iteration 256, loss = 0.2812046706676483
iteration 257, loss = 0.4624759256839752
iteration 258, loss = 0.3484812378883362
iteration 259, loss = 0.21367086470127106
iteration 260, loss = 0.3039264976978302
iteration 261, loss = 0.23136059939861298
iteration 262, loss = 0.30368801951408386
iteration 263, loss = 0.30940502882003784
iteration 264, loss = 0.37234997749328613
iteration 265, loss = 0.2789349853992462
iteration 266, loss = 0.31759727001190186
iteration 267, loss = 0.36461177468299866
iteration 268, loss = 0.31452620029449463
iteration 269, loss = 0.2382829189300537
iteration 270, loss = 0.3469234108924866
iteration 271, loss = 0.14014050364494324
iteration 272, loss = 0.3042987585067749
iteration 273, loss = 0.2790250778198242
iteration 274, loss = 0.3609744906425476
iteration 275, loss = 0.291431188583374
iteration 276, loss = 0.42215052247047424
iteration 277, loss = 0.5765812993049622
iteration 278, loss = 0.16768208146095276
iteration 279, loss = 0.4457724690437317
iteration 280, loss = 0.3134482204914093
iteration 281, loss = 0.24569310247898102
iteration 282, loss = 0.6335015296936035
iteration 283, loss = 0.27034538984298706
iteration 284, loss = 0.05391506478190422
iteration 285, loss = 0.22117431461811066
iteration 286, loss = 0.17733345925807953
iteration 287, loss = 0.1970183551311493
iteration 288, loss = 0.1970287561416626
iteration 289, loss = 0.30058231949806213
iteration 290, loss = 0.30432116985321045
iteration 291, loss = 0.33716344833374023
iteration 292, loss = 0.17850306630134583
iteration 293, loss = 0.3700791895389557
iteration 294, loss = 0.2791763246059418
iteration 295, loss = 0.14518918097019196
iteration 296, loss = 0.5880884528160095
iteration 297, loss = 0.27599188685417175
iteration 298, loss = 0.22466681897640228
iteration 299, loss = 0.28247761726379395
iteration 0, loss = 0.23388150334358215
iteration 1, loss = 0.2252485156059265
iteration 2, loss = 0.47070953249931335
iteration 3, loss = 0.3109263777732849
iteration 4, loss = 0.3592410981655121
iteration 5, loss = 0.24366706609725952
iteration 6, loss = 0.4048120975494385
iteration 7, loss = 0.3391020894050598
iteration 8, loss = 0.3865566551685333
iteration 9, loss = 0.2759948670864105
iteration 10, loss = 0.2236078828573227
iteration 11, loss = 0.2425759732723236
iteration 12, loss = 0.34361326694488525
iteration 13, loss = 0.292630136013031
iteration 14, loss = 0.3007868230342865
iteration 15, loss = 0.25335389375686646
iteration 16, loss = 0.39620083570480347
iteration 17, loss = 0.17422303557395935
iteration 18, loss = 0.47947776317596436
iteration 19, loss = 0.12329156696796417
iteration 20, loss = 0.4642166793346405
iteration 21, loss = 0.14252609014511108
iteration 22, loss = 0.4912993013858795
iteration 23, loss = 0.30467158555984497
iteration 24, loss = 0.12880493700504303
iteration 25, loss = 0.3427934944629669
iteration 26, loss = 0.32334306836128235
iteration 27, loss = 0.21017730236053467
iteration 28, loss = 0.22159141302108765
iteration 29, loss = 0.3073353171348572
iteration 30, loss = 0.36220479011535645
iteration 31, loss = 0.5087073445320129
iteration 32, loss = 0.2714855372905731
iteration 33, loss = 0.2035641372203827
iteration 34, loss = 0.251303493976593
iteration 35, loss = 0.23497319221496582
iteration 36, loss = 0.29502686858177185
iteration 37, loss = 0.28661781549453735
iteration 38, loss = 0.13614624738693237
iteration 39, loss = 0.1197144091129303
iteration 40, loss = 0.4445074796676636
iteration 41, loss = 0.1840631365776062
iteration 42, loss = 0.16459520161151886
iteration 43, loss = 0.1069427952170372
iteration 44, loss = 0.23858129978179932
iteration 45, loss = 0.3616476356983185
iteration 46, loss = 0.1909036487340927
iteration 47, loss = 0.412663996219635
iteration 48, loss = 0.41289621591567993
iteration 49, loss = 0.38888853788375854
iteration 50, loss = 0.3354019224643707
iteration 51, loss = 0.2287660837173462
iteration 52, loss = 0.2494596540927887
iteration 53, loss = 0.11451934278011322
iteration 54, loss = 0.3357122838497162
iteration 55, loss = 0.15061138570308685
iteration 56, loss = 0.23777207732200623
iteration 57, loss = 0.5808166861534119
iteration 58, loss = 0.20445892214775085
iteration 59, loss = 0.6644741296768188
iteration 60, loss = 0.15507330000400543
iteration 61, loss = 0.2713184952735901
iteration 62, loss = 0.24376097321510315
iteration 63, loss = 0.3647235631942749
iteration 64, loss = 0.21416544914245605
iteration 65, loss = 0.7065608501434326
iteration 66, loss = 0.4745425581932068
iteration 67, loss = 0.20451360940933228
iteration 68, loss = 0.5100326538085938
iteration 69, loss = 0.28820186853408813
iteration 70, loss = 0.12828396260738373
iteration 71, loss = 0.15447522699832916
iteration 72, loss = 0.3027644753456116
iteration 73, loss = 0.16247610747814178
iteration 74, loss = 0.35369178652763367
iteration 75, loss = 0.3534313440322876
iteration 76, loss = 0.12852725386619568
iteration 77, loss = 0.07508523762226105
iteration 78, loss = 0.3923504948616028
iteration 79, loss = 0.2945185899734497
iteration 80, loss = 0.4313336908817291
iteration 81, loss = 0.47687506675720215
iteration 82, loss = 0.2653045058250427
iteration 83, loss = 0.2748005986213684
iteration 84, loss = 0.36026856303215027
iteration 85, loss = 0.3271261751651764
iteration 86, loss = 0.34579038619995117
iteration 87, loss = 0.3334818482398987
iteration 88, loss = 0.24046412110328674
iteration 89, loss = 0.2241978794336319
iteration 90, loss = 0.25143328309059143
iteration 91, loss = 0.28794100880622864
iteration 92, loss = 0.11596634984016418
iteration 93, loss = 0.3760365843772888
iteration 94, loss = 0.3047720789909363
iteration 95, loss = 0.11012251675128937
iteration 96, loss = 0.3898615837097168
iteration 97, loss = 0.08108235895633698
iteration 98, loss = 0.20396915078163147
iteration 99, loss = 0.3120518624782562
iteration 100, loss = 0.27303656935691833
iteration 101, loss = 0.32368582487106323
iteration 102, loss = 0.1737833321094513
iteration 103, loss = 0.25258350372314453
iteration 104, loss = 0.20679287612438202
iteration 105, loss = 0.4900493025779724
iteration 106, loss = 0.20964109897613525
iteration 107, loss = 0.2937314212322235
iteration 108, loss = 0.15134942531585693
iteration 109, loss = 0.4428726136684418
iteration 110, loss = 0.26312386989593506
iteration 111, loss = 0.39038798213005066
iteration 112, loss = 0.3647473454475403
iteration 113, loss = 0.28030505776405334
iteration 114, loss = 0.2514432966709137
iteration 115, loss = 0.2253655195236206
iteration 116, loss = 0.297850102186203
iteration 117, loss = 0.5073267221450806
iteration 118, loss = 0.2934418320655823
iteration 119, loss = 0.29634687304496765
iteration 120, loss = 0.19648398458957672
iteration 121, loss = 0.30071914196014404
iteration 122, loss = 0.24539577960968018
iteration 123, loss = 0.29240337014198303
iteration 124, loss = 0.3141241669654846
iteration 125, loss = 0.5086125731468201
iteration 126, loss = 0.2676581144332886
iteration 127, loss = 0.3974258005619049
iteration 128, loss = 0.24621891975402832
iteration 129, loss = 0.4839174151420593
iteration 130, loss = 0.1760219931602478
iteration 131, loss = 0.20763416588306427
iteration 132, loss = 0.2551014721393585
iteration 133, loss = 0.3528716564178467
iteration 134, loss = 0.37345993518829346
iteration 135, loss = 0.4208608865737915
iteration 136, loss = 0.24380555748939514
iteration 137, loss = 0.2632858157157898
iteration 138, loss = 0.42531049251556396
iteration 139, loss = 0.30982139706611633
iteration 140, loss = 0.19007368385791779
iteration 141, loss = 0.4608432948589325
iteration 142, loss = 0.43436071276664734
iteration 143, loss = 0.2588946223258972
iteration 144, loss = 0.20039518177509308
iteration 145, loss = 0.17199550569057465
iteration 146, loss = 0.14872993528842926
iteration 147, loss = 0.18684054911136627
iteration 148, loss = 0.14996223151683807
iteration 149, loss = 0.36159271001815796
iteration 150, loss = 0.459145188331604
iteration 151, loss = 0.1835734248161316
iteration 152, loss = 0.2548447847366333
iteration 153, loss = 0.5043880939483643
iteration 154, loss = 0.36946409940719604
iteration 155, loss = 0.1020636111497879
iteration 156, loss = 0.13946519792079926
iteration 157, loss = 0.3508228361606598
iteration 158, loss = 0.5464398264884949
iteration 159, loss = 0.1735611855983734
iteration 160, loss = 0.326576828956604
iteration 161, loss = 0.3432621955871582
iteration 162, loss = 0.09398673474788666
iteration 163, loss = 0.36984753608703613
iteration 164, loss = 0.21463866531848907
iteration 165, loss = 0.2323572337627411
iteration 166, loss = 0.2615206837654114
iteration 167, loss = 0.11593112349510193
iteration 168, loss = 0.2714054584503174
iteration 169, loss = 0.2381150722503662
iteration 170, loss = 0.3209300637245178
iteration 171, loss = 0.11218921840190887
iteration 172, loss = 0.2706442177295685
iteration 173, loss = 0.5004580020904541
iteration 174, loss = 0.2213735282421112
iteration 175, loss = 0.21649114787578583
iteration 176, loss = 0.23682323098182678
iteration 177, loss = 0.27714335918426514
iteration 178, loss = 0.15791669487953186
iteration 179, loss = 0.3195297420024872
iteration 180, loss = 0.2688489854335785
iteration 181, loss = 0.2018636018037796
iteration 182, loss = 0.18054865300655365
iteration 183, loss = 0.293329656124115
iteration 184, loss = 0.18763291835784912
iteration 185, loss = 0.24898375570774078
iteration 186, loss = 0.28516367077827454
iteration 187, loss = 0.22949403524398804
iteration 188, loss = 0.1776386797428131
iteration 189, loss = 0.1645747572183609
iteration 190, loss = 0.3705558478832245
iteration 191, loss = 0.1881321370601654
iteration 192, loss = 0.25401103496551514
iteration 193, loss = 0.575588047504425
iteration 194, loss = 0.1360841989517212
iteration 195, loss = 0.38786131143569946
iteration 196, loss = 0.3113926947116852
iteration 197, loss = 0.21898208558559418
iteration 198, loss = 0.1906612515449524
iteration 199, loss = 0.28018060326576233
iteration 200, loss = 0.1387680172920227
iteration 201, loss = 0.1978120058774948
iteration 202, loss = 0.11802004277706146
iteration 203, loss = 0.14600181579589844
iteration 204, loss = 0.08278010785579681
iteration 205, loss = 0.5757477283477783
iteration 206, loss = 0.4725404679775238
iteration 207, loss = 0.2614046335220337
iteration 208, loss = 0.35751867294311523
iteration 209, loss = 0.36165401339530945
iteration 210, loss = 0.23485693335533142
iteration 211, loss = 0.1550692766904831
iteration 212, loss = 0.18674743175506592
iteration 213, loss = 0.20311404764652252
iteration 214, loss = 0.3107820153236389
iteration 215, loss = 0.23713338375091553
iteration 216, loss = 0.28554415702819824
iteration 217, loss = 0.16836318373680115
iteration 218, loss = 0.362527459859848
iteration 219, loss = 0.14625556766986847
iteration 220, loss = 0.3722292482852936
iteration 221, loss = 0.17801418900489807
iteration 222, loss = 0.2117636501789093
iteration 223, loss = 0.45871198177337646
iteration 224, loss = 0.2806367576122284
iteration 225, loss = 0.2254960834980011
iteration 226, loss = 0.15801966190338135
iteration 227, loss = 0.1599113941192627
iteration 228, loss = 0.36991405487060547
iteration 229, loss = 0.30629485845565796
iteration 230, loss = 0.38746142387390137
iteration 231, loss = 0.3919753432273865
iteration 232, loss = 0.1364031285047531
iteration 233, loss = 0.31185415387153625
iteration 234, loss = 0.3947308659553528
iteration 235, loss = 0.19197499752044678
iteration 236, loss = 0.2515801191329956
iteration 237, loss = 0.5239888429641724
iteration 238, loss = 0.3579545319080353
iteration 239, loss = 0.3002428412437439
iteration 240, loss = 0.20270408689975739
iteration 241, loss = 0.2737746238708496
iteration 242, loss = 0.20094875991344452
iteration 243, loss = 0.23776043951511383
iteration 244, loss = 0.130843847990036
iteration 245, loss = 0.23752668499946594
iteration 246, loss = 0.3372576832771301
iteration 247, loss = 0.4429129660129547
iteration 248, loss = 0.163278728723526
iteration 249, loss = 0.16347768902778625
iteration 250, loss = 0.15180161595344543
iteration 251, loss = 0.2406938076019287
iteration 252, loss = 0.14550672471523285
iteration 253, loss = 0.26604583859443665
iteration 254, loss = 0.20655974745750427
iteration 255, loss = 0.11621662974357605
iteration 256, loss = 0.5264938473701477
iteration 257, loss = 0.3268079161643982
iteration 258, loss = 0.3877972960472107
iteration 259, loss = 0.36768847703933716
iteration 260, loss = 0.5250061750411987
iteration 261, loss = 0.29677528142929077
iteration 262, loss = 0.18318362534046173
iteration 263, loss = 0.46290987730026245
iteration 264, loss = 0.30210864543914795
iteration 265, loss = 0.3648522198200226
iteration 266, loss = 0.29943200945854187
iteration 267, loss = 0.3472324311733246
iteration 268, loss = 0.2605137228965759
iteration 269, loss = 0.2508144676685333
iteration 270, loss = 0.27207279205322266
iteration 271, loss = 0.04024597257375717
iteration 272, loss = 0.2989184558391571
iteration 273, loss = 0.5077517032623291
iteration 274, loss = 0.11980585753917694
iteration 275, loss = 0.34182339906692505
iteration 276, loss = 0.38022086024284363
iteration 277, loss = 0.30198609828948975
iteration 278, loss = 0.2261023372411728
iteration 279, loss = 0.21173538267612457
iteration 280, loss = 0.2719059884548187
iteration 281, loss = 0.12825310230255127
iteration 282, loss = 0.21345913410186768
iteration 283, loss = 0.21813704073429108
iteration 284, loss = 0.10159678757190704
iteration 285, loss = 0.3563397526741028
iteration 286, loss = 0.34335145354270935
iteration 287, loss = 0.16268697381019592
iteration 288, loss = 0.12121085822582245
iteration 289, loss = 0.17484259605407715
iteration 290, loss = 0.11446397751569748
iteration 291, loss = 0.3092750310897827
iteration 292, loss = 0.27218031883239746
iteration 293, loss = 0.10593130439519882
iteration 294, loss = 0.24609285593032837
iteration 295, loss = 0.24238497018814087
iteration 296, loss = 0.27188777923583984
iteration 297, loss = 0.27428728342056274
iteration 298, loss = 0.10855357348918915
iteration 299, loss = 0.35014429688453674
iteration 0, loss = 0.2853435277938843
iteration 1, loss = 0.2882356345653534
iteration 2, loss = 0.3402245342731476
iteration 3, loss = 0.30460262298583984
iteration 4, loss = 0.2902213931083679
iteration 5, loss = 0.3765530586242676
iteration 6, loss = 0.1212451159954071
iteration 7, loss = 0.40118032693862915
iteration 8, loss = 0.13784955441951752
iteration 9, loss = 0.41490882635116577
iteration 10, loss = 0.31196141242980957
iteration 11, loss = 0.6081515550613403
iteration 12, loss = 0.1905784159898758
iteration 13, loss = 0.14620926976203918
iteration 14, loss = 0.2388356328010559
iteration 15, loss = 0.15150856971740723
iteration 16, loss = 0.24993549287319183
iteration 17, loss = 0.43418949842453003
iteration 18, loss = 0.2476174235343933
iteration 19, loss = 0.1483342945575714
iteration 20, loss = 0.19138900935649872
iteration 21, loss = 0.5394929647445679
iteration 22, loss = 0.2939557433128357
iteration 23, loss = 0.1537306010723114
iteration 24, loss = 0.5442587733268738
iteration 25, loss = 0.346410870552063
iteration 26, loss = 0.18629857897758484
iteration 27, loss = 0.17912252247333527
iteration 28, loss = 0.35440245270729065
iteration 29, loss = 0.19903281331062317
iteration 30, loss = 0.30352163314819336
iteration 31, loss = 0.5011995434761047
iteration 32, loss = 0.4099160432815552
iteration 33, loss = 0.29203832149505615
iteration 34, loss = 0.33619940280914307
iteration 35, loss = 0.30324289202690125
iteration 36, loss = 0.24143445491790771
iteration 37, loss = 0.34966540336608887
iteration 38, loss = 0.3805612325668335
iteration 39, loss = 0.2065095603466034
iteration 40, loss = 0.2177780121564865
iteration 41, loss = 0.05783712491393089
iteration 42, loss = 0.2512042820453644
iteration 43, loss = 0.2940778434276581
iteration 44, loss = 0.366141140460968
iteration 45, loss = 0.26968473196029663
iteration 46, loss = 0.30018579959869385
iteration 47, loss = 0.301705002784729
iteration 48, loss = 0.246558278799057
iteration 49, loss = 0.18694281578063965
iteration 50, loss = 0.1014624685049057
iteration 51, loss = 0.12590670585632324
iteration 52, loss = 0.1839769184589386
iteration 53, loss = 0.19185060262680054
iteration 54, loss = 0.05644170939922333
iteration 55, loss = 0.17025956511497498
iteration 56, loss = 0.442088782787323
iteration 57, loss = 0.32602179050445557
iteration 58, loss = 0.1973741054534912
iteration 59, loss = 0.3050342798233032
iteration 60, loss = 0.35074955224990845
iteration 61, loss = 0.04510252922773361
iteration 62, loss = 0.23210568726062775
iteration 63, loss = 0.38896462321281433
iteration 64, loss = 0.22045224905014038
iteration 65, loss = 0.25438937544822693
iteration 66, loss = 0.19975700974464417
iteration 67, loss = 0.08609214425086975
iteration 68, loss = 0.29321587085723877
iteration 69, loss = 0.34502866864204407
iteration 70, loss = 0.27073603868484497
iteration 71, loss = 0.19551822543144226
iteration 72, loss = 0.23772510886192322
iteration 73, loss = 0.5616054534912109
iteration 74, loss = 0.36803218722343445
iteration 75, loss = 0.21586766839027405
iteration 76, loss = 0.4572613835334778
iteration 77, loss = 0.36481279134750366
iteration 78, loss = 0.3060806393623352
iteration 79, loss = 0.22504982352256775
iteration 80, loss = 0.5268933176994324
iteration 81, loss = 0.27940690517425537
iteration 82, loss = 0.08003965765237808
iteration 83, loss = 0.04991982504725456
iteration 84, loss = 0.31592777371406555
iteration 85, loss = 0.29484978318214417
iteration 86, loss = 0.17942841351032257
iteration 87, loss = 0.5362624526023865
iteration 88, loss = 0.08245045691728592
iteration 89, loss = 0.09302659332752228
iteration 90, loss = 0.09272809326648712
iteration 91, loss = 0.44259145855903625
iteration 92, loss = 0.2801471948623657
iteration 93, loss = 0.11245504766702652
iteration 94, loss = 0.30478864908218384
iteration 95, loss = 0.14242002367973328
iteration 96, loss = 0.12537816166877747
iteration 97, loss = 0.13619014620780945
iteration 98, loss = 0.22135281562805176
iteration 99, loss = 0.12739378213882446
iteration 100, loss = 0.12235546857118607
iteration 101, loss = 0.17615358531475067
iteration 102, loss = 0.35124456882476807
iteration 103, loss = 0.13185763359069824
iteration 104, loss = 0.35926464200019836
iteration 105, loss = 0.31844788789749146
iteration 106, loss = 0.28822755813598633
iteration 107, loss = 0.36379021406173706
iteration 108, loss = 0.539076566696167
iteration 109, loss = 0.2343095988035202
iteration 110, loss = 0.1230466216802597
iteration 111, loss = 0.35637596249580383
iteration 112, loss = 0.34270137548446655
iteration 113, loss = 0.25887250900268555
iteration 114, loss = 0.5107647180557251
iteration 115, loss = 0.32986679673194885
iteration 116, loss = 0.36959269642829895
iteration 117, loss = 0.1258137971162796
iteration 118, loss = 0.1457478255033493
iteration 119, loss = 0.35894352197647095
iteration 120, loss = 0.2070687711238861
iteration 121, loss = 0.34461650252342224
iteration 122, loss = 0.3324486315250397
iteration 123, loss = 0.1305014044046402
iteration 124, loss = 0.15677911043167114
iteration 125, loss = 0.2497406303882599
iteration 126, loss = 0.20944643020629883
iteration 127, loss = 0.19565758109092712
iteration 128, loss = 0.19507819414138794
iteration 129, loss = 0.09348592907190323
iteration 130, loss = 0.1606125682592392
iteration 131, loss = 0.38610130548477173
iteration 132, loss = 0.2743793725967407
iteration 133, loss = 0.08511383086442947
iteration 134, loss = 0.3639729917049408
iteration 135, loss = 0.1530468463897705
iteration 136, loss = 0.31022635102272034
iteration 137, loss = 0.37016087770462036
iteration 138, loss = 0.11844120174646378
iteration 139, loss = 0.5490329265594482
iteration 140, loss = 0.1043042317032814
iteration 141, loss = 0.17792992293834686
iteration 142, loss = 0.23900967836380005
iteration 143, loss = 0.2759503424167633
iteration 144, loss = 0.25372716784477234
iteration 145, loss = 0.3185449242591858
iteration 146, loss = 0.17290568351745605
iteration 147, loss = 0.18269337713718414
iteration 148, loss = 0.3532993793487549
iteration 149, loss = 0.42335352301597595
iteration 150, loss = 0.4267269968986511
iteration 151, loss = 0.03434225544333458
iteration 152, loss = 0.3830803632736206
iteration 153, loss = 0.21212121844291687
iteration 154, loss = 0.3012387752532959
iteration 155, loss = 0.3034173548221588
iteration 156, loss = 0.2715383768081665
iteration 157, loss = 0.23712295293807983
iteration 158, loss = 0.2506256401538849
iteration 159, loss = 0.39635059237480164
iteration 160, loss = 0.3536642789840698
iteration 161, loss = 0.13890230655670166
iteration 162, loss = 0.44231098890304565
iteration 163, loss = 0.27145010232925415
iteration 164, loss = 0.3322199583053589
iteration 165, loss = 0.42662084102630615
iteration 166, loss = 0.41316384077072144
iteration 167, loss = 0.47208860516548157
iteration 168, loss = 0.19489146769046783
iteration 169, loss = 0.3147210478782654
iteration 170, loss = 0.23760265111923218
iteration 171, loss = 0.26257726550102234
iteration 172, loss = 0.3988018333911896
iteration 173, loss = 0.41434353590011597
iteration 174, loss = 0.34620845317840576
iteration 175, loss = 0.1524924635887146
iteration 176, loss = 0.38851428031921387
iteration 177, loss = 0.2731441259384155
iteration 178, loss = 0.11134245246648788
iteration 179, loss = 0.28639334440231323
iteration 180, loss = 0.3317069411277771
iteration 181, loss = 0.3213874101638794
iteration 182, loss = 0.29365113377571106
iteration 183, loss = 0.17924878001213074
iteration 184, loss = 0.45320484042167664
iteration 185, loss = 0.22711454331874847
iteration 186, loss = 0.121662937104702
iteration 187, loss = 0.07299787551164627
iteration 188, loss = 0.3630056381225586
iteration 189, loss = 0.2975854277610779
iteration 190, loss = 0.3938678801059723
iteration 191, loss = 0.16744732856750488
iteration 192, loss = 0.20648865401744843
iteration 193, loss = 0.2780968248844147
iteration 194, loss = 0.22959715127944946
iteration 195, loss = 0.17052428424358368
iteration 196, loss = 0.3014066517353058
iteration 197, loss = 0.125649556517601
iteration 198, loss = 0.18289688229560852
iteration 199, loss = 0.2521156072616577
iteration 200, loss = 0.1803264617919922
iteration 201, loss = 0.36860302090644836
iteration 202, loss = 0.14953967928886414
iteration 203, loss = 0.47447288036346436
iteration 204, loss = 0.28204551339149475
iteration 205, loss = 0.4430270791053772
iteration 206, loss = 0.14774949848651886
iteration 207, loss = 0.1461687684059143
iteration 208, loss = 0.18381479382514954
iteration 209, loss = 0.3253195583820343
iteration 210, loss = 0.6219285130500793
iteration 211, loss = 0.09824831783771515
iteration 212, loss = 0.11669468134641647
iteration 213, loss = 0.38069677352905273
iteration 214, loss = 0.16523227095603943
iteration 215, loss = 0.3481556177139282
iteration 216, loss = 0.3003721833229065
iteration 217, loss = 0.28850656747817993
iteration 218, loss = 0.1768268495798111
iteration 219, loss = 0.26587823033332825
iteration 220, loss = 0.23447632789611816
iteration 221, loss = 0.3264102339744568
iteration 222, loss = 0.2883400022983551
iteration 223, loss = 0.12958991527557373
iteration 224, loss = 0.14488597214221954
iteration 225, loss = 0.5688684582710266
iteration 226, loss = 0.29047492146492004
iteration 227, loss = 0.24134202301502228
iteration 228, loss = 0.18846990168094635
iteration 229, loss = 0.460191011428833
iteration 230, loss = 0.13259059190750122
iteration 231, loss = 0.43141767382621765
iteration 232, loss = 0.4839893877506256
iteration 233, loss = 0.43291324377059937
iteration 234, loss = 0.24679677188396454
iteration 235, loss = 0.2696108818054199
iteration 236, loss = 0.10634887963533401
iteration 237, loss = 0.09016017615795135
iteration 238, loss = 0.350603312253952
iteration 239, loss = 0.338177889585495
iteration 240, loss = 0.642989993095398
iteration 241, loss = 0.4121951162815094
iteration 242, loss = 0.2921898663043976
iteration 243, loss = 0.0569557249546051
iteration 244, loss = 0.4831559956073761
iteration 245, loss = 0.3107604384422302
iteration 246, loss = 0.20452164113521576
iteration 247, loss = 0.24697831273078918
iteration 248, loss = 0.2790343761444092
iteration 249, loss = 0.28218117356300354
iteration 250, loss = 0.2433706521987915
iteration 251, loss = 0.26360371708869934
iteration 252, loss = 0.24900436401367188
iteration 253, loss = 0.2183237373828888
iteration 254, loss = 0.23739899694919586
iteration 255, loss = 0.5534771084785461
iteration 256, loss = 0.20787233114242554
iteration 257, loss = 0.24683602154254913
iteration 258, loss = 0.34306392073631287
iteration 259, loss = 0.352402925491333
iteration 260, loss = 0.43912941217422485
iteration 261, loss = 0.34888318181037903
iteration 262, loss = 0.36233463883399963
iteration 263, loss = 0.2699238955974579
iteration 264, loss = 0.33813905715942383
iteration 265, loss = 0.19018037617206573
iteration 266, loss = 0.1368425190448761
iteration 267, loss = 0.3306870460510254
iteration 268, loss = 0.48766595125198364
iteration 269, loss = 0.23324227333068848
iteration 270, loss = 0.3998377323150635
iteration 271, loss = 0.2544390559196472
iteration 272, loss = 0.19302165508270264
iteration 273, loss = 0.18631649017333984
iteration 274, loss = 0.17423930764198303
iteration 275, loss = 0.10812687873840332
iteration 276, loss = 0.36461764574050903
iteration 277, loss = 0.2997153699398041
iteration 278, loss = 0.22275327146053314
iteration 279, loss = 0.5191121101379395
iteration 280, loss = 0.3908995985984802
iteration 281, loss = 0.271310031414032
iteration 282, loss = 0.05468229949474335
iteration 283, loss = 0.15974220633506775
iteration 284, loss = 0.10889078676700592
iteration 285, loss = 0.20728251338005066
iteration 286, loss = 0.19406312704086304
iteration 287, loss = 0.1172604113817215
iteration 288, loss = 0.27076369524002075
iteration 289, loss = 0.3078397512435913
iteration 290, loss = 0.1465567648410797
iteration 291, loss = 0.038939084857702255
iteration 292, loss = 0.40697813034057617
iteration 293, loss = 0.6443451642990112
iteration 294, loss = 0.21820512413978577
iteration 295, loss = 0.22748716175556183
iteration 296, loss = 0.3528047800064087
iteration 297, loss = 0.19516822695732117
iteration 298, loss = 0.41796088218688965
iteration 299, loss = 0.37211185693740845
iteration 0, loss = 0.21040871739387512
iteration 1, loss = 0.33476531505584717
iteration 2, loss = 0.45484408736228943
iteration 3, loss = 0.11979061365127563
iteration 4, loss = 0.2800148129463196
iteration 5, loss = 0.3202129006385803
iteration 6, loss = 0.3627009391784668
iteration 7, loss = 0.20316965878009796
iteration 8, loss = 0.2079172432422638
iteration 9, loss = 0.1934901624917984
iteration 10, loss = 0.47229358553886414
iteration 11, loss = 0.23229649662971497
iteration 12, loss = 0.36866024136543274
iteration 13, loss = 0.40493637323379517
iteration 14, loss = 0.11175113171339035
iteration 15, loss = 0.42527249455451965
iteration 16, loss = 0.24843955039978027
iteration 17, loss = 0.042589250952005386
iteration 18, loss = 0.14156070351600647
iteration 19, loss = 0.21401730179786682
iteration 20, loss = 0.2145366668701172
iteration 21, loss = 0.36046770215034485
iteration 22, loss = 0.30508074164390564
iteration 23, loss = 0.39816370606422424
iteration 24, loss = 0.11461763083934784
iteration 25, loss = 0.39225903153419495
iteration 26, loss = 0.3203694820404053
iteration 27, loss = 0.22367878258228302
iteration 28, loss = 0.23267459869384766
iteration 29, loss = 0.20055292546749115
iteration 30, loss = 0.09616931527853012
iteration 31, loss = 0.30404922366142273
iteration 32, loss = 0.42071333527565
iteration 33, loss = 0.2486901730298996
iteration 34, loss = 0.31994742155075073
iteration 35, loss = 0.22110489010810852
iteration 36, loss = 0.34040889143943787
iteration 37, loss = 0.18136310577392578
iteration 38, loss = 0.18801645934581757
iteration 39, loss = 0.2785475552082062
iteration 40, loss = 0.3676372468471527
iteration 41, loss = 0.21457643806934357
iteration 42, loss = 0.6961055397987366
iteration 43, loss = 0.1578391194343567
iteration 44, loss = 0.12860597670078278
iteration 45, loss = 0.18058472871780396
iteration 46, loss = 0.35508817434310913
iteration 47, loss = 0.21029675006866455
iteration 48, loss = 0.18920756876468658
iteration 49, loss = 0.2593541443347931
iteration 50, loss = 0.13398608565330505
iteration 51, loss = 0.17304913699626923
iteration 52, loss = 0.18760019540786743
iteration 53, loss = 0.3600482642650604
iteration 54, loss = 0.2580362558364868
iteration 55, loss = 0.2761878967285156
iteration 56, loss = 0.18739619851112366
iteration 57, loss = 0.19352397322654724
iteration 58, loss = 0.24596798419952393
iteration 59, loss = 0.32148927450180054
iteration 60, loss = 0.4409570097923279
iteration 61, loss = 0.30654293298721313
iteration 62, loss = 0.3392447233200073
iteration 63, loss = 0.17781513929367065
iteration 64, loss = 0.16391530632972717
iteration 65, loss = 0.1375819891691208
iteration 66, loss = 0.3353739082813263
iteration 67, loss = 0.15741011500358582
iteration 68, loss = 0.18942737579345703
iteration 69, loss = 0.3759215772151947
iteration 70, loss = 0.29382145404815674
iteration 71, loss = 0.3053446114063263
iteration 72, loss = 0.14647233486175537
iteration 73, loss = 0.16187451779842377
iteration 74, loss = 0.3519732356071472
iteration 75, loss = 0.509444534778595
iteration 76, loss = 0.0653696358203888
iteration 77, loss = 0.33426839113235474
iteration 78, loss = 0.27098548412323
iteration 79, loss = 0.39519259333610535
iteration 80, loss = 0.19329142570495605
iteration 81, loss = 0.23995782434940338
iteration 82, loss = 0.1552545428276062
iteration 83, loss = 0.23143720626831055
iteration 84, loss = 0.2643616199493408
iteration 85, loss = 0.3726772665977478
iteration 86, loss = 0.21061599254608154
iteration 87, loss = 0.13619814813137054
iteration 88, loss = 0.35312849283218384
iteration 89, loss = 0.2112956941127777
iteration 90, loss = 0.2956969141960144
iteration 91, loss = 0.2699088454246521
iteration 92, loss = 0.2564440965652466
iteration 93, loss = 0.25210046768188477
iteration 94, loss = 0.41633346676826477
iteration 95, loss = 0.1460162103176117
iteration 96, loss = 0.3495274782180786
iteration 97, loss = 0.15730741620063782
iteration 98, loss = 0.2724439203739166
iteration 99, loss = 0.12650799751281738
iteration 100, loss = 0.3647785484790802
iteration 101, loss = 0.2817462980747223
iteration 102, loss = 0.022521741688251495
iteration 103, loss = 0.48046398162841797
iteration 104, loss = 0.10872238874435425
iteration 105, loss = 0.19516219198703766
iteration 106, loss = 0.10074509680271149
iteration 107, loss = 0.18958276510238647
iteration 108, loss = 0.4366774559020996
iteration 109, loss = 0.19701924920082092
iteration 110, loss = 0.24032625555992126
iteration 111, loss = 0.04123193770647049
iteration 112, loss = 0.3209126889705658
iteration 113, loss = 0.37164977192878723
iteration 114, loss = 0.2497379034757614
iteration 115, loss = 0.1818837821483612
iteration 116, loss = 0.24431422352790833
iteration 117, loss = 0.34329545497894287
iteration 118, loss = 0.21502241492271423
iteration 119, loss = 0.11237116158008575
iteration 120, loss = 0.21975800395011902
iteration 121, loss = 0.2937491536140442
iteration 122, loss = 0.2714674770832062
iteration 123, loss = 0.4580099582672119
iteration 124, loss = 0.08135415613651276
iteration 125, loss = 0.31883203983306885
iteration 126, loss = 0.08910492062568665
iteration 127, loss = 0.24825245141983032
iteration 128, loss = 0.24115915596485138
iteration 129, loss = 0.2214396893978119
iteration 130, loss = 0.5136450529098511
iteration 131, loss = 0.21618425846099854
iteration 132, loss = 0.2544793486595154
iteration 133, loss = 0.09025724232196808
iteration 134, loss = 0.2091691792011261
iteration 135, loss = 0.3266524076461792
iteration 136, loss = 0.09308803081512451
iteration 137, loss = 0.2981686592102051
iteration 138, loss = 0.18482494354248047
iteration 139, loss = 0.1442043036222458
iteration 140, loss = 0.3607122302055359
iteration 141, loss = 0.4709937870502472
iteration 142, loss = 0.3696233034133911
iteration 143, loss = 0.07658296823501587
iteration 144, loss = 0.24891610443592072
iteration 145, loss = 0.29087233543395996
iteration 146, loss = 0.37550413608551025
iteration 147, loss = 0.13542994856834412
iteration 148, loss = 0.20732831954956055
iteration 149, loss = 0.2055753618478775
iteration 150, loss = 0.22464784979820251
iteration 151, loss = 0.4195871949195862
iteration 152, loss = 0.6794158816337585
iteration 153, loss = 0.4223353862762451
iteration 154, loss = 0.3824710249900818
iteration 155, loss = 0.46250081062316895
iteration 156, loss = 0.259331077337265
iteration 157, loss = 0.07660583406686783
iteration 158, loss = 0.24709175527095795
iteration 159, loss = 0.43019747734069824
iteration 160, loss = 0.4048730432987213
iteration 161, loss = 0.3368264138698578
iteration 162, loss = 0.5163720846176147
iteration 163, loss = 0.5305116772651672
iteration 164, loss = 0.19965828955173492
iteration 165, loss = 0.0803571417927742
iteration 166, loss = 0.08850600570440292
iteration 167, loss = 0.07205811142921448
iteration 168, loss = 0.4373813271522522
iteration 169, loss = 0.40860429406166077
iteration 170, loss = 0.5880262851715088
iteration 171, loss = 0.2342376410961151
iteration 172, loss = 0.42797842621803284
iteration 173, loss = 0.36322396993637085
iteration 174, loss = 0.33295246958732605
iteration 175, loss = 0.32191309332847595
iteration 176, loss = 0.1897188127040863
iteration 177, loss = 0.31066805124282837
iteration 178, loss = 0.3439299464225769
iteration 179, loss = 0.3145296573638916
iteration 180, loss = 0.1777929961681366
iteration 181, loss = 0.253837525844574
iteration 182, loss = 0.16168352961540222
iteration 183, loss = 0.14103862643241882
iteration 184, loss = 0.28852489590644836
iteration 185, loss = 0.3677160143852234
iteration 186, loss = 0.29484596848487854
iteration 187, loss = 0.2634105086326599
iteration 188, loss = 0.14936277270317078
iteration 189, loss = 0.14821448922157288
iteration 190, loss = 0.16125722229480743
iteration 191, loss = 0.13002131879329681
iteration 192, loss = 0.18599703907966614
iteration 193, loss = 0.23189860582351685
iteration 194, loss = 0.31353163719177246
iteration 195, loss = 0.3460685610771179
iteration 196, loss = 0.2168222814798355
iteration 197, loss = 0.2016267478466034
iteration 198, loss = 0.13564442098140717
iteration 199, loss = 0.39637425541877747
iteration 200, loss = 0.30124667286872864
iteration 201, loss = 0.2161502242088318
iteration 202, loss = 0.2154705971479416
iteration 203, loss = 0.17832818627357483
iteration 204, loss = 0.29730644822120667
iteration 205, loss = 0.3225826919078827
iteration 206, loss = 0.2791561782360077
iteration 207, loss = 0.3725951015949249
iteration 208, loss = 0.2612924873828888
iteration 209, loss = 0.20861345529556274
iteration 210, loss = 0.2532528340816498
iteration 211, loss = 0.29974377155303955
iteration 212, loss = 0.10584612935781479
iteration 213, loss = 0.5287620425224304
iteration 214, loss = 0.1437782645225525
iteration 215, loss = 0.23051294684410095
iteration 216, loss = 0.3634583353996277
iteration 217, loss = 0.27589359879493713
iteration 218, loss = 0.2787507176399231
iteration 219, loss = 0.19919608533382416
iteration 220, loss = 0.28971898555755615
iteration 221, loss = 0.27186083793640137
iteration 222, loss = 0.3053015470504761
iteration 223, loss = 0.3461397886276245
iteration 224, loss = 0.11293850839138031
iteration 225, loss = 0.37346741557121277
iteration 226, loss = 0.24704493582248688
iteration 227, loss = 0.4366794228553772
iteration 228, loss = 0.3008009195327759
iteration 229, loss = 0.1052352711558342
iteration 230, loss = 0.21294760704040527
iteration 231, loss = 0.14073136448860168
iteration 232, loss = 0.22471818327903748
iteration 233, loss = 0.16267552971839905
iteration 234, loss = 0.19401279091835022
iteration 235, loss = 0.13664382696151733
iteration 236, loss = 0.09953570365905762
iteration 237, loss = 0.27070504426956177
iteration 238, loss = 0.5054634213447571
iteration 239, loss = 0.11419110000133514
iteration 240, loss = 0.39314547181129456
iteration 241, loss = 0.09027636051177979
iteration 242, loss = 0.27655577659606934
iteration 243, loss = 0.18467265367507935
iteration 244, loss = 0.07731693983078003
iteration 245, loss = 0.30086493492126465
iteration 246, loss = 0.34697383642196655
iteration 247, loss = 0.3802393674850464
iteration 248, loss = 0.09103672951459885
iteration 249, loss = 0.285184770822525
iteration 250, loss = 0.3671967387199402
iteration 251, loss = 0.12012551724910736
iteration 252, loss = 0.1533176451921463
iteration 253, loss = 0.06768878549337387
iteration 254, loss = 0.4815569221973419
iteration 255, loss = 0.13246917724609375
iteration 256, loss = 0.3690669536590576
iteration 257, loss = 0.10021830350160599
iteration 258, loss = 0.22447852790355682
iteration 259, loss = 0.1488659530878067
iteration 260, loss = 0.4610214829444885
iteration 261, loss = 0.20019933581352234
iteration 262, loss = 0.20963174104690552
iteration 263, loss = 0.1249886155128479
iteration 264, loss = 0.1314486861228943
iteration 265, loss = 0.31507036089897156
iteration 266, loss = 0.10199432820081711
iteration 267, loss = 0.4678562879562378
iteration 268, loss = 0.21399161219596863
iteration 269, loss = 0.12108476459980011
iteration 270, loss = 0.445307195186615
iteration 271, loss = 0.25778767466545105
iteration 272, loss = 0.2883954644203186
iteration 273, loss = 0.25082841515541077
iteration 274, loss = 0.16129422187805176
iteration 275, loss = 0.39645302295684814
iteration 276, loss = 0.11879201233386993
iteration 277, loss = 0.09538436681032181
iteration 278, loss = 0.2518651783466339
iteration 279, loss = 0.28192126750946045
iteration 280, loss = 0.46076977252960205
iteration 281, loss = 0.4719167947769165
iteration 282, loss = 0.22352677583694458
iteration 283, loss = 0.27338936924934387
iteration 284, loss = 0.4870741367340088
iteration 285, loss = 0.4894469380378723
iteration 286, loss = 0.3837363123893738
iteration 287, loss = 0.4938816428184509
iteration 288, loss = 0.31696009635925293
iteration 289, loss = 0.12066426873207092
iteration 290, loss = 0.1643124222755432
iteration 291, loss = 0.22718888521194458
iteration 292, loss = 0.24645866453647614
iteration 293, loss = 0.4464520215988159
iteration 294, loss = 0.5960238575935364
iteration 295, loss = 0.4399942457675934
iteration 296, loss = 0.17588606476783752
iteration 297, loss = 0.305097758769989
iteration 298, loss = 0.0474935844540596
iteration 299, loss = 0.08444385230541229
iteration 0, loss = 0.13526087999343872
iteration 1, loss = 0.185255229473114
iteration 2, loss = 0.2985895276069641
iteration 3, loss = 0.15322551131248474
iteration 4, loss = 0.07841873168945312
iteration 5, loss = 0.16171053051948547
iteration 6, loss = 0.30938243865966797
iteration 7, loss = 0.15499171614646912
iteration 8, loss = 0.2788360118865967
iteration 9, loss = 0.12801969051361084
iteration 10, loss = 0.18112391233444214
iteration 11, loss = 0.2961018979549408
iteration 12, loss = 0.3101101219654083
iteration 13, loss = 0.2858583629131317
iteration 14, loss = 0.25016045570373535
iteration 15, loss = 0.11389651149511337
iteration 16, loss = 0.36247679591178894
iteration 17, loss = 0.27948808670043945
iteration 18, loss = 0.4096027612686157
iteration 19, loss = 0.08044290542602539
iteration 20, loss = 0.14604109525680542
iteration 21, loss = 0.25928258895874023
iteration 22, loss = 0.07793163508176804
iteration 23, loss = 0.22222422063350677
iteration 24, loss = 0.43663978576660156
iteration 25, loss = 0.3491295278072357
iteration 26, loss = 0.20916083455085754
iteration 27, loss = 0.509076714515686
iteration 28, loss = 0.3057243824005127
iteration 29, loss = 0.39439305663108826
iteration 30, loss = 0.30909010767936707
iteration 31, loss = 0.31724488735198975
iteration 32, loss = 0.2237679809331894
iteration 33, loss = 0.4037019908428192
iteration 34, loss = 0.31925588846206665
iteration 35, loss = 0.3599052429199219
iteration 36, loss = 0.28975263237953186
iteration 37, loss = 0.15768636763095856
iteration 38, loss = 0.27061498165130615
iteration 39, loss = 0.23575960099697113
iteration 40, loss = 0.6676539778709412
iteration 41, loss = 0.4129585027694702
iteration 42, loss = 0.4078347384929657
iteration 43, loss = 0.18586942553520203
iteration 44, loss = 0.03318767994642258
iteration 45, loss = 0.17495357990264893
iteration 46, loss = 0.4107910096645355
iteration 47, loss = 0.48265379667282104
iteration 48, loss = 0.2308340221643448
iteration 49, loss = 0.11715836822986603
iteration 50, loss = 0.27684924006462097
iteration 51, loss = 0.26467373967170715
iteration 52, loss = 0.299884557723999
iteration 53, loss = 0.3800654113292694
iteration 54, loss = 0.15205197036266327
iteration 55, loss = 0.12701158225536346
iteration 56, loss = 0.03510896489024162
iteration 57, loss = 0.2303548902273178
iteration 58, loss = 0.5019553303718567
iteration 59, loss = 0.2335732877254486
iteration 60, loss = 0.41770827770233154
iteration 61, loss = 0.26917150616645813
iteration 62, loss = 0.2778300642967224
iteration 63, loss = 0.46589940786361694
iteration 64, loss = 0.21693018078804016
iteration 65, loss = 0.19437679648399353
iteration 66, loss = 0.24303223192691803
iteration 67, loss = 0.26193350553512573
iteration 68, loss = 0.4128064513206482
iteration 69, loss = 0.48589852452278137
iteration 70, loss = 0.4856302738189697
iteration 71, loss = 0.40135782957077026
iteration 72, loss = 0.2570294737815857
iteration 73, loss = 0.30714961886405945
iteration 74, loss = 0.17553511261940002
iteration 75, loss = 0.18144145607948303
iteration 76, loss = 0.05877171456813812
iteration 77, loss = 0.17840763926506042
iteration 78, loss = 0.31081366539001465
iteration 79, loss = 0.1707574427127838
iteration 80, loss = 0.44035130739212036
iteration 81, loss = 0.1377386599779129
iteration 82, loss = 0.3058260977268219
iteration 83, loss = 0.22664159536361694
iteration 84, loss = 0.29660624265670776
iteration 85, loss = 0.24529382586479187
iteration 86, loss = 0.2248343825340271
iteration 87, loss = 0.41063839197158813
iteration 88, loss = 0.2128801792860031
iteration 89, loss = 0.41057029366493225
iteration 90, loss = 0.29423046112060547
iteration 91, loss = 0.3720508813858032
iteration 92, loss = 0.37376609444618225
iteration 93, loss = 0.30036070942878723
iteration 94, loss = 0.20371578633785248
iteration 95, loss = 0.638965368270874
iteration 96, loss = 0.06412889808416367
iteration 97, loss = 0.07700107246637344
iteration 98, loss = 0.2900759279727936
iteration 99, loss = 0.37916213274002075
iteration 100, loss = 0.2706976532936096
iteration 101, loss = 0.13549542427062988
iteration 102, loss = 0.13468986749649048
iteration 103, loss = 0.24362283945083618
iteration 104, loss = 0.2689824104309082
iteration 105, loss = 0.2869061827659607
iteration 106, loss = 0.23559942841529846
iteration 107, loss = 0.11930707842111588
iteration 108, loss = 0.20228421688079834
iteration 109, loss = 0.09317757189273834
iteration 110, loss = 0.3501393496990204
iteration 111, loss = 0.21513839066028595
iteration 112, loss = 0.18695726990699768
iteration 113, loss = 0.21953320503234863
iteration 114, loss = 0.47224611043930054
iteration 115, loss = 0.1986759603023529
iteration 116, loss = 0.14959801733493805
iteration 117, loss = 0.16854271292686462
iteration 118, loss = 0.1343804895877838
iteration 119, loss = 0.48766273260116577
iteration 120, loss = 0.21272245049476624
iteration 121, loss = 0.39840781688690186
iteration 122, loss = 0.19921448826789856
iteration 123, loss = 0.5424503087997437
iteration 124, loss = 0.17288145422935486
iteration 125, loss = 0.10087854415178299
iteration 126, loss = 0.1303083896636963
iteration 127, loss = 0.23312072455883026
iteration 128, loss = 0.28926464915275574
iteration 129, loss = 0.356600821018219
iteration 130, loss = 0.5717650651931763
iteration 131, loss = 0.33747225999832153
iteration 132, loss = 0.3414483368396759
iteration 133, loss = 0.18652674555778503
iteration 134, loss = 0.18390591442584991
iteration 135, loss = 0.48508524894714355
iteration 136, loss = 0.20122414827346802
iteration 137, loss = 0.3494722247123718
iteration 138, loss = 0.43182507157325745
iteration 139, loss = 0.3068251609802246
iteration 140, loss = 0.21573367714881897
iteration 141, loss = 0.11008242517709732
iteration 142, loss = 0.20600396394729614
iteration 143, loss = 0.12755879759788513
iteration 144, loss = 0.18768733739852905
iteration 145, loss = 0.38466718792915344
iteration 146, loss = 0.11120836436748505
iteration 147, loss = 0.2460395246744156
iteration 148, loss = 0.14361050724983215
iteration 149, loss = 0.2747640013694763
iteration 150, loss = 0.20127223432064056
iteration 151, loss = 0.40128636360168457
iteration 152, loss = 0.17677879333496094
iteration 153, loss = 0.20209059119224548
iteration 154, loss = 0.2733408808708191
iteration 155, loss = 0.3859051764011383
iteration 156, loss = 0.4453347623348236
iteration 157, loss = 0.2499919980764389
iteration 158, loss = 0.11344268918037415
iteration 159, loss = 0.23989585041999817
iteration 160, loss = 0.20936545729637146
iteration 161, loss = 0.1273316890001297
iteration 162, loss = 0.2110956311225891
iteration 163, loss = 0.12014877796173096
iteration 164, loss = 0.38411858677864075
iteration 165, loss = 0.3251612186431885
iteration 166, loss = 0.25799116492271423
iteration 167, loss = 0.12884101271629333
iteration 168, loss = 0.24338698387145996
iteration 169, loss = 0.2728729844093323
iteration 170, loss = 0.20898467302322388
iteration 171, loss = 0.2668946385383606
iteration 172, loss = 0.2291097640991211
iteration 173, loss = 0.30124086141586304
iteration 174, loss = 0.2633848786354065
iteration 175, loss = 0.13679412007331848
iteration 176, loss = 0.33178913593292236
iteration 177, loss = 0.16200590133666992
iteration 178, loss = 0.24433189630508423
iteration 179, loss = 0.16277249157428741
iteration 180, loss = 0.26113995909690857
iteration 181, loss = 0.10228513181209564
iteration 182, loss = 0.08263980597257614
iteration 183, loss = 0.2582297623157501
iteration 184, loss = 0.1735546886920929
iteration 185, loss = 0.2838883399963379
iteration 186, loss = 0.10686533153057098
iteration 187, loss = 0.054431430995464325
iteration 188, loss = 0.10601041465997696
iteration 189, loss = 0.5393993258476257
iteration 190, loss = 0.2463938295841217
iteration 191, loss = 0.2179601639509201
iteration 192, loss = 0.21453328430652618
iteration 193, loss = 0.11468113213777542
iteration 194, loss = 0.04609569162130356
iteration 195, loss = 0.21890506148338318
iteration 196, loss = 0.13498345017433167
iteration 197, loss = 0.09517867863178253
iteration 198, loss = 0.14159782230854034
iteration 199, loss = 0.41963323950767517
iteration 200, loss = 0.2977946400642395
iteration 201, loss = 0.3255006968975067
iteration 202, loss = 0.22994384169578552
iteration 203, loss = 0.135075181722641
iteration 204, loss = 0.1548459827899933
iteration 205, loss = 0.2884863018989563
iteration 206, loss = 0.08623799681663513
iteration 207, loss = 0.05685911327600479
iteration 208, loss = 0.184956356883049
iteration 209, loss = 0.32266512513160706
iteration 210, loss = 0.16612085700035095
iteration 211, loss = 0.20132674276828766
iteration 212, loss = 0.31558650732040405
iteration 213, loss = 0.32903581857681274
iteration 214, loss = 0.48546892404556274
iteration 215, loss = 0.3005426228046417
iteration 216, loss = 0.2354104071855545
iteration 217, loss = 0.1601884365081787
iteration 218, loss = 0.08284565061330795
iteration 219, loss = 0.29289886355400085
iteration 220, loss = 0.2057270109653473
iteration 221, loss = 0.23564495146274567
iteration 222, loss = 0.45761027932167053
iteration 223, loss = 0.06267422437667847
iteration 224, loss = 0.46655726432800293
iteration 225, loss = 0.19025206565856934
iteration 226, loss = 0.2386123239994049
iteration 227, loss = 0.14463913440704346
iteration 228, loss = 0.21908919513225555
iteration 229, loss = 0.3363802134990692
iteration 230, loss = 0.2917211055755615
iteration 231, loss = 0.2142837643623352
iteration 232, loss = 0.28661584854125977
iteration 233, loss = 0.27448999881744385
iteration 234, loss = 0.25502216815948486
iteration 235, loss = 0.2743019163608551
iteration 236, loss = 0.2967814803123474
iteration 237, loss = 0.3370438516139984
iteration 238, loss = 0.3664841651916504
iteration 239, loss = 0.28838980197906494
iteration 240, loss = 0.3269149661064148
iteration 241, loss = 0.21260593831539154
iteration 242, loss = 0.40664142370224
iteration 243, loss = 0.4163403809070587
iteration 244, loss = 0.3289402425289154
iteration 245, loss = 0.22811388969421387
iteration 246, loss = 0.010186535306274891
iteration 247, loss = 0.49086588621139526
iteration 248, loss = 0.3736818730831146
iteration 249, loss = 0.18834523856639862
iteration 250, loss = 0.26001664996147156
iteration 251, loss = 0.13020262122154236
iteration 252, loss = 0.17988799512386322
iteration 253, loss = 0.574662983417511
iteration 254, loss = 0.17009477317333221
iteration 255, loss = 0.17623014748096466
iteration 256, loss = 0.28107577562332153
iteration 257, loss = 0.09329667687416077
iteration 258, loss = 0.061541181057691574
iteration 259, loss = 0.18077591061592102
iteration 260, loss = 0.3872263431549072
iteration 261, loss = 0.40820837020874023
iteration 262, loss = 0.3005216419696808
iteration 263, loss = 0.23867419362068176
iteration 264, loss = 0.2339542955160141
iteration 265, loss = 0.28040841221809387
iteration 266, loss = 0.3620380163192749
iteration 267, loss = 0.3534295856952667
iteration 268, loss = 0.16924899816513062
iteration 269, loss = 0.4164622128009796
iteration 270, loss = 0.08925342559814453
iteration 271, loss = 0.2724161446094513
iteration 272, loss = 0.4242996275424957
iteration 273, loss = 0.18437200784683228
iteration 274, loss = 0.055493101477622986
iteration 275, loss = 0.16462191939353943
iteration 276, loss = 0.42420732975006104
iteration 277, loss = 0.3064936697483063
iteration 278, loss = 0.26305049657821655
iteration 279, loss = 0.1246802881360054
iteration 280, loss = 0.3367254137992859
iteration 281, loss = 0.17715029418468475
iteration 282, loss = 0.3982093930244446
iteration 283, loss = 0.0667015090584755
iteration 284, loss = 0.19418087601661682
iteration 285, loss = 0.3752060830593109
iteration 286, loss = 0.30239856243133545
iteration 287, loss = 0.20923838019371033
iteration 288, loss = 0.22810856997966766
iteration 289, loss = 0.35312551259994507
iteration 290, loss = 0.12882131338119507
iteration 291, loss = 0.26447921991348267
iteration 292, loss = 0.27506551146507263
iteration 293, loss = 0.29427453875541687
iteration 294, loss = 0.31584009528160095
iteration 295, loss = 0.5687124133110046
iteration 296, loss = 0.05027630552649498
iteration 297, loss = 0.11106011271476746
iteration 298, loss = 0.37901532649993896
iteration 299, loss = 0.2991379499435425
iteration 0, loss = 0.548043966293335
iteration 1, loss = 0.23645395040512085
iteration 2, loss = 0.12970711290836334
iteration 3, loss = 0.05487605184316635
iteration 4, loss = 0.17738977074623108
iteration 5, loss = 0.045177776366472244
iteration 6, loss = 0.4036829471588135
iteration 7, loss = 0.11031902581453323
iteration 8, loss = 0.45265135169029236
iteration 9, loss = 0.28242599964141846
iteration 10, loss = 0.1415850818157196
iteration 11, loss = 0.334877610206604
iteration 12, loss = 0.23668111860752106
iteration 13, loss = 0.35459762811660767
iteration 14, loss = 0.26802778244018555
iteration 15, loss = 0.09349366277456284
iteration 16, loss = 0.2679516673088074
iteration 17, loss = 0.16265811026096344
iteration 18, loss = 0.18307659029960632
iteration 19, loss = 0.22969447076320648
iteration 20, loss = 0.36190709471702576
iteration 21, loss = 0.24268151819705963
iteration 22, loss = 0.19502294063568115
iteration 23, loss = 0.17441102862358093
iteration 24, loss = 0.2673097848892212
iteration 25, loss = 0.276387095451355
iteration 26, loss = 0.419009268283844
iteration 27, loss = 0.3134443461894989
iteration 28, loss = 0.24892349541187286
iteration 29, loss = 0.12904371321201324
iteration 30, loss = 0.5068897008895874
iteration 31, loss = 0.3109511733055115
iteration 32, loss = 0.1778922975063324
iteration 33, loss = 0.05248814448714256
iteration 34, loss = 0.3609289824962616
iteration 35, loss = 0.15005835890769958
iteration 36, loss = 0.3141997754573822
iteration 37, loss = 0.40492120385169983
iteration 38, loss = 0.10796689242124557
iteration 39, loss = 0.3936551809310913
iteration 40, loss = 0.3349670469760895
iteration 41, loss = 0.4253508448600769
iteration 42, loss = 0.1834559142589569
iteration 43, loss = 0.4613204300403595
iteration 44, loss = 0.24546559154987335
iteration 45, loss = 0.11564519256353378
iteration 46, loss = 0.25498852133750916
iteration 47, loss = 0.18527469038963318
iteration 48, loss = 0.3809230327606201
iteration 49, loss = 0.4117175042629242
iteration 50, loss = 0.03330609202384949
iteration 51, loss = 0.4971417188644409
iteration 52, loss = 0.20442457497119904
iteration 53, loss = 0.24182741343975067
iteration 54, loss = 0.3244989514350891
iteration 55, loss = 0.2751753032207489
iteration 56, loss = 0.3502930998802185
iteration 57, loss = 0.5268881916999817
iteration 58, loss = 0.3485319912433624
iteration 59, loss = 0.2846938669681549
iteration 60, loss = 0.1190103217959404
iteration 61, loss = 0.15296824276447296
iteration 62, loss = 0.2240014672279358
iteration 63, loss = 0.21501246094703674
iteration 64, loss = 0.43378007411956787
iteration 65, loss = 0.3514670729637146
iteration 66, loss = 0.30378109216690063
iteration 67, loss = 0.17618796229362488
iteration 68, loss = 0.3147706091403961
iteration 69, loss = 0.09830241650342941
iteration 70, loss = 0.15940114855766296
iteration 71, loss = 0.1314818412065506
iteration 72, loss = 0.17857669293880463
iteration 73, loss = 0.3747900128364563
iteration 74, loss = 0.180428147315979
iteration 75, loss = 0.24601805210113525
iteration 76, loss = 0.46587005257606506
iteration 77, loss = 0.13828471302986145
iteration 78, loss = 0.3646831512451172
iteration 79, loss = 0.16595062613487244
iteration 80, loss = 0.09513632208108902
iteration 81, loss = 0.0992211252450943
iteration 82, loss = 0.29591724276542664
iteration 83, loss = 0.2051325887441635
iteration 84, loss = 0.21149960160255432
iteration 85, loss = 0.09465023875236511
iteration 86, loss = 0.3705219626426697
iteration 87, loss = 0.36978229880332947
iteration 88, loss = 0.35229435563087463
iteration 89, loss = 0.4225630462169647
iteration 90, loss = 0.18553975224494934
iteration 91, loss = 0.2665194869041443
iteration 92, loss = 0.06840303540229797
iteration 93, loss = 0.10893397778272629
iteration 94, loss = 0.5567418336868286
iteration 95, loss = 0.1602598875761032
iteration 96, loss = 0.33249637484550476
iteration 97, loss = 0.3373333513736725
iteration 98, loss = 0.26511064171791077
iteration 99, loss = 0.4667036533355713
iteration 100, loss = 0.1679864227771759
iteration 101, loss = 0.1541425585746765
iteration 102, loss = 0.5538464784622192
iteration 103, loss = 0.1580260843038559
iteration 104, loss = 0.26379281282424927
iteration 105, loss = 0.31944724917411804
iteration 106, loss = 0.5258851051330566
iteration 107, loss = 0.29972583055496216
iteration 108, loss = 0.16265474259853363
iteration 109, loss = 0.04057282209396362
iteration 110, loss = 0.19771112501621246
iteration 111, loss = 0.398174524307251
iteration 112, loss = 0.1338542401790619
iteration 113, loss = 0.4166222810745239
iteration 114, loss = 0.15815159678459167
iteration 115, loss = 0.2621593177318573
iteration 116, loss = 0.2908690273761749
iteration 117, loss = 0.5526766777038574
iteration 118, loss = 0.2057616263628006
iteration 119, loss = 0.1401538997888565
iteration 120, loss = 0.2613588273525238
iteration 121, loss = 0.25369566679000854
iteration 122, loss = 0.15097427368164062
iteration 123, loss = 0.1242937296628952
iteration 124, loss = 0.06355838477611542
iteration 125, loss = 0.1035931259393692
iteration 126, loss = 0.12456606328487396
iteration 127, loss = 0.057398486882448196
iteration 128, loss = 0.20960775017738342
iteration 129, loss = 0.11222858726978302
iteration 130, loss = 0.15239475667476654
iteration 131, loss = 0.17344532907009125
iteration 132, loss = 0.098383329808712
iteration 133, loss = 0.5274214744567871
iteration 134, loss = 0.522678017616272
iteration 135, loss = 0.3431626260280609
iteration 136, loss = 0.1324431598186493
iteration 137, loss = 0.4782978296279907
iteration 138, loss = 0.2862207591533661
iteration 139, loss = 0.18332582712173462
iteration 140, loss = 0.2522607743740082
iteration 141, loss = 0.20965932309627533
iteration 142, loss = 0.05822460353374481
iteration 143, loss = 0.0896177887916565
iteration 144, loss = 0.24182170629501343
iteration 145, loss = 0.20696711540222168
iteration 146, loss = 0.3463101387023926
iteration 147, loss = 0.4884270429611206
iteration 148, loss = 0.1355666220188141
iteration 149, loss = 0.05719301104545593
iteration 150, loss = 0.41135627031326294
iteration 151, loss = 0.32382482290267944
iteration 152, loss = 0.29736703634262085
iteration 153, loss = 0.28311848640441895
iteration 154, loss = 0.23649762570858002
iteration 155, loss = 0.14926721155643463
iteration 156, loss = 0.12388143688440323
iteration 157, loss = 0.07573415338993073
iteration 158, loss = 0.32680439949035645
iteration 159, loss = 0.0347711481153965
iteration 160, loss = 0.4478561580181122
iteration 161, loss = 0.5658764839172363
iteration 162, loss = 0.08978639543056488
iteration 163, loss = 0.03654666244983673
iteration 164, loss = 0.22172081470489502
iteration 165, loss = 0.2695624530315399
iteration 166, loss = 0.11054876446723938
iteration 167, loss = 0.3659285604953766
iteration 168, loss = 0.15537381172180176
iteration 169, loss = 0.5819554328918457
iteration 170, loss = 0.44396644830703735
iteration 171, loss = 0.26387912034988403
iteration 172, loss = 0.198444664478302
iteration 173, loss = 0.28816652297973633
iteration 174, loss = 0.6071608662605286
iteration 175, loss = 0.3091539740562439
iteration 176, loss = 0.3538713753223419
iteration 177, loss = 0.2638993561267853
iteration 178, loss = 0.20203332602977753
iteration 179, loss = 0.3977067768573761
iteration 180, loss = 0.246224507689476
iteration 181, loss = 0.4761883318424225
iteration 182, loss = 0.7541345357894897
iteration 183, loss = 0.5312939286231995
iteration 184, loss = 0.5812697410583496
iteration 185, loss = 0.18415674567222595
iteration 186, loss = 0.23224931955337524
iteration 187, loss = 0.19919639825820923
iteration 188, loss = 0.19081224501132965
iteration 189, loss = 0.6012982726097107
iteration 190, loss = 0.6414272785186768
iteration 191, loss = 0.6089260578155518
iteration 192, loss = 0.4313766062259674
iteration 193, loss = 0.5062073469161987
iteration 194, loss = 0.5996830463409424
iteration 195, loss = 0.49432623386383057
iteration 196, loss = 0.20857208967208862
iteration 197, loss = 0.17089590430259705
iteration 198, loss = 0.37689828872680664
iteration 199, loss = 0.23431681096553802
iteration 200, loss = 0.2009468376636505
iteration 201, loss = 0.25302520394325256
iteration 202, loss = 0.3059437572956085
iteration 203, loss = 0.16818121075630188
iteration 204, loss = 0.3212701678276062
iteration 205, loss = 0.39165377616882324
iteration 206, loss = 0.2242528796195984
iteration 207, loss = 0.15852265059947968
iteration 208, loss = 0.3114200234413147
iteration 209, loss = 0.39475277066230774
iteration 210, loss = 0.44583985209465027
iteration 211, loss = 0.39054030179977417
iteration 212, loss = 0.1408112645149231
iteration 213, loss = 0.11667554080486298
iteration 214, loss = 0.3859979212284088
iteration 215, loss = 0.4085307717323303
iteration 216, loss = 0.21845461428165436
iteration 217, loss = 0.11231233179569244
iteration 218, loss = 0.16713014245033264
iteration 219, loss = 0.4474634826183319
iteration 220, loss = 0.30194175243377686
iteration 221, loss = 0.1824159026145935
iteration 222, loss = 0.35730430483818054
iteration 223, loss = 0.19014745950698853
iteration 224, loss = 0.21985122561454773
iteration 225, loss = 0.3319316506385803
iteration 226, loss = 0.2615770101547241
iteration 227, loss = 0.22275230288505554
iteration 228, loss = 0.22351086139678955
iteration 229, loss = 0.16090911626815796
iteration 230, loss = 0.3146519958972931
iteration 231, loss = 0.2623027265071869
iteration 232, loss = 0.2641940414905548
iteration 233, loss = 0.36306676268577576
iteration 234, loss = 0.3413711488246918
iteration 235, loss = 0.25682151317596436
iteration 236, loss = 0.4872749149799347
iteration 237, loss = 0.19319143891334534
iteration 238, loss = 0.11006156355142593
iteration 239, loss = 0.22165323793888092
iteration 240, loss = 0.28869572281837463
iteration 241, loss = 0.2884633541107178
iteration 242, loss = 0.5060595273971558
iteration 243, loss = 0.26430997252464294
iteration 244, loss = 0.10222944617271423
iteration 245, loss = 0.18369928002357483
iteration 246, loss = 0.16696420311927795
iteration 247, loss = 0.21400032937526703
iteration 248, loss = 0.18768571317195892
iteration 249, loss = 0.1092894896864891
iteration 250, loss = 0.2146574854850769
iteration 251, loss = 0.5048277378082275
iteration 252, loss = 0.14686687290668488
iteration 253, loss = 0.12019063532352448
iteration 254, loss = 0.1256093978881836
iteration 255, loss = 0.4548686146736145
iteration 256, loss = 0.16506022214889526
iteration 257, loss = 0.2599099576473236
iteration 258, loss = 0.3801603615283966
iteration 259, loss = 0.3470003008842468
iteration 260, loss = 0.4870384633541107
iteration 261, loss = 0.28801101446151733
iteration 262, loss = 0.19121712446212769
iteration 263, loss = 0.39167797565460205
iteration 264, loss = 0.5133019685745239
iteration 265, loss = 0.1729801893234253
iteration 266, loss = 0.16791033744812012
iteration 267, loss = 0.048245664685964584
iteration 268, loss = 0.05809463933110237
iteration 269, loss = 0.1701865792274475
iteration 270, loss = 0.15951189398765564
iteration 271, loss = 0.36563464999198914
iteration 272, loss = 0.18989790976047516
iteration 273, loss = 0.2841869294643402
iteration 274, loss = 0.3146400451660156
iteration 275, loss = 0.1954052746295929
iteration 276, loss = 0.2547975182533264
iteration 277, loss = 0.1859574317932129
iteration 278, loss = 0.1602439284324646
iteration 279, loss = 0.1854967474937439
iteration 280, loss = 0.3641781806945801
iteration 281, loss = 0.12788982689380646
iteration 282, loss = 0.24836429953575134
iteration 283, loss = 0.12497533857822418
iteration 284, loss = 0.2578609883785248
iteration 285, loss = 0.32590699195861816
iteration 286, loss = 0.09838041663169861
iteration 287, loss = 0.3621238172054291
iteration 288, loss = 0.08042173832654953
iteration 289, loss = 0.15901371836662292
iteration 290, loss = 0.1816394329071045
iteration 291, loss = 0.3248905539512634
iteration 292, loss = 0.054473891854286194
iteration 293, loss = 0.4048047661781311
iteration 294, loss = 0.04806780815124512
iteration 295, loss = 0.05360044911503792
iteration 296, loss = 0.2735723853111267
iteration 297, loss = 0.25900840759277344
iteration 298, loss = 0.3080616295337677
iteration 299, loss = 0.38011065125465393
iteration 0, loss = 0.25859954953193665
iteration 1, loss = 0.17720332741737366
iteration 2, loss = 0.26457637548446655
iteration 3, loss = 0.2009870558977127
iteration 4, loss = 0.2819443345069885
iteration 5, loss = 0.2895103693008423
iteration 6, loss = 0.39054620265960693
iteration 7, loss = 0.22399748861789703
iteration 8, loss = 0.2009187638759613
iteration 9, loss = 0.14606426656246185
iteration 10, loss = 0.2794048488140106
iteration 11, loss = 0.08380290865898132
iteration 12, loss = 0.12888552248477936
iteration 13, loss = 0.13740602135658264
iteration 14, loss = 0.18913400173187256
iteration 15, loss = 0.08413372188806534
iteration 16, loss = 0.24588795006275177
iteration 17, loss = 0.1755448281764984
iteration 18, loss = 0.1922278106212616
iteration 19, loss = 0.18808512389659882
iteration 20, loss = 0.41682541370391846
iteration 21, loss = 0.04979661852121353
iteration 22, loss = 0.20337025821208954
iteration 23, loss = 0.09797732532024384
iteration 24, loss = 0.34066522121429443
iteration 25, loss = 0.07923559099435806
iteration 26, loss = 0.1366344541311264
iteration 27, loss = 0.09046246111392975
iteration 28, loss = 0.08292967826128006
iteration 29, loss = 0.29432588815689087
iteration 30, loss = 0.3818693459033966
iteration 31, loss = 0.15281513333320618
iteration 32, loss = 0.22068777680397034
iteration 33, loss = 0.5104645490646362
iteration 34, loss = 0.3312978744506836
iteration 35, loss = 0.12834322452545166
iteration 36, loss = 0.021871773526072502
iteration 37, loss = 0.2950303256511688
iteration 38, loss = 0.1990450620651245
iteration 39, loss = 0.1672632396221161
iteration 40, loss = 0.2989538311958313
iteration 41, loss = 0.41105419397354126
iteration 42, loss = 0.17804811894893646
iteration 43, loss = 0.2677190601825714
iteration 44, loss = 0.18912366032600403
iteration 45, loss = 0.3531782031059265
iteration 46, loss = 0.15356706082820892
iteration 47, loss = 0.38790684938430786
iteration 48, loss = 0.22879120707511902
iteration 49, loss = 0.2151424139738083
iteration 50, loss = 0.22313553094863892
iteration 51, loss = 0.12887781858444214
iteration 52, loss = 0.3986561894416809
iteration 53, loss = 0.3304128348827362
iteration 54, loss = 0.3445513844490051
iteration 55, loss = 0.10520555078983307
iteration 56, loss = 0.35867321491241455
iteration 57, loss = 0.45820507407188416
iteration 58, loss = 0.07285567373037338
iteration 59, loss = 0.1504470705986023
iteration 60, loss = 0.24505147337913513
iteration 61, loss = 0.5036391615867615
iteration 62, loss = 0.1608043909072876
iteration 63, loss = 0.2580094039440155
iteration 64, loss = 0.17985761165618896
iteration 65, loss = 0.19008681178092957
iteration 66, loss = 0.20026162266731262
iteration 67, loss = 0.347507506608963
iteration 68, loss = 0.20060455799102783
iteration 69, loss = 0.2218669205904007
iteration 70, loss = 0.31958967447280884
iteration 71, loss = 0.211726576089859
iteration 72, loss = 0.1314498782157898
iteration 73, loss = 0.5132747888565063
iteration 74, loss = 0.15990746021270752
iteration 75, loss = 0.2733975350856781
iteration 76, loss = 0.212127685546875
iteration 77, loss = 0.47208383679389954
iteration 78, loss = 0.22727209329605103
iteration 79, loss = 0.2471233308315277
iteration 80, loss = 0.29881107807159424
iteration 81, loss = 0.1272495687007904
iteration 82, loss = 0.2966337203979492
iteration 83, loss = 0.3534271717071533
iteration 84, loss = 0.11148683726787567
iteration 85, loss = 0.11341075599193573
iteration 86, loss = 0.08254694938659668
iteration 87, loss = 0.21062445640563965
iteration 88, loss = 0.4226633310317993
iteration 89, loss = 0.16736501455307007
iteration 90, loss = 0.2020624727010727
iteration 91, loss = 0.21687276661396027
iteration 92, loss = 0.4165578782558441
iteration 93, loss = 0.23283910751342773
iteration 94, loss = 0.23857372999191284
iteration 95, loss = 0.2807121276855469
iteration 96, loss = 0.1419072449207306
iteration 97, loss = 0.1527809351682663
iteration 98, loss = 0.13557347655296326
iteration 99, loss = 0.11484394967556
iteration 100, loss = 0.4475173354148865
iteration 101, loss = 0.28221553564071655
iteration 102, loss = 0.4479057192802429
iteration 103, loss = 0.07926124334335327
iteration 104, loss = 0.29290539026260376
iteration 105, loss = 0.16035854816436768
iteration 106, loss = 0.18324048817157745
iteration 107, loss = 0.3973832428455353
iteration 108, loss = 0.22273367643356323
iteration 109, loss = 0.2711683511734009
iteration 110, loss = 0.2661876082420349
iteration 111, loss = 0.2821701169013977
iteration 112, loss = 0.2943692207336426
iteration 113, loss = 0.3362424373626709
iteration 114, loss = 0.13005998730659485
iteration 115, loss = 0.2036382555961609
iteration 116, loss = 0.33549290895462036
iteration 117, loss = 0.20078390836715698
iteration 118, loss = 0.13503706455230713
iteration 119, loss = 0.27399277687072754
iteration 120, loss = 0.32125380635261536
iteration 121, loss = 0.1968177855014801
iteration 122, loss = 0.2386031150817871
iteration 123, loss = 0.1440727859735489
iteration 124, loss = 0.22624117136001587
iteration 125, loss = 0.3923605680465698
iteration 126, loss = 0.2765583395957947
iteration 127, loss = 0.22232413291931152
iteration 128, loss = 0.31362777948379517
iteration 129, loss = 0.3567820191383362
iteration 130, loss = 0.030444523319602013
iteration 131, loss = 0.17622947692871094
iteration 132, loss = 0.1099097952246666
iteration 133, loss = 0.08377587795257568
iteration 134, loss = 0.23422569036483765
iteration 135, loss = 0.2391299605369568
iteration 136, loss = 0.6065327525138855
iteration 137, loss = 0.2806503176689148
iteration 138, loss = 0.13064859807491302
iteration 139, loss = 0.29347938299179077
iteration 140, loss = 0.19872164726257324
iteration 141, loss = 0.2668568789958954
iteration 142, loss = 0.28565552830696106
iteration 143, loss = 0.22602447867393494
iteration 144, loss = 0.0924626886844635
iteration 145, loss = 0.2644345164299011
iteration 146, loss = 0.219819113612175
iteration 147, loss = 0.3662106692790985
iteration 148, loss = 0.013693161308765411
iteration 149, loss = 0.4423374533653259
iteration 150, loss = 0.37758222222328186
iteration 151, loss = 0.3401266038417816
iteration 152, loss = 0.11440388858318329
iteration 153, loss = 0.32942038774490356
iteration 154, loss = 0.3460618257522583
iteration 155, loss = 0.19636505842208862
iteration 156, loss = 0.28364109992980957
iteration 157, loss = 0.24777072668075562
iteration 158, loss = 0.16784146428108215
iteration 159, loss = 0.2509763538837433
iteration 160, loss = 0.48838669061660767
iteration 161, loss = 0.02846585214138031
iteration 162, loss = 0.1509176343679428
iteration 163, loss = 0.215756356716156
iteration 164, loss = 0.14608915150165558
iteration 165, loss = 0.1266329288482666
iteration 166, loss = 0.05062234029173851
iteration 167, loss = 0.11081553250551224
iteration 168, loss = 0.1870642453432083
iteration 169, loss = 0.38703176379203796
iteration 170, loss = 0.49502187967300415
iteration 171, loss = 0.13155512511730194
iteration 172, loss = 0.3000284433364868
iteration 173, loss = 0.2649814188480377
iteration 174, loss = 0.23478123545646667
iteration 175, loss = 0.28340405225753784
iteration 176, loss = 0.09841173887252808
iteration 177, loss = 0.22777682542800903
iteration 178, loss = 0.10340205579996109
iteration 179, loss = 0.10115809738636017
iteration 180, loss = 0.3064761459827423
iteration 181, loss = 0.2504791021347046
iteration 182, loss = 0.4247913360595703
iteration 183, loss = 0.11743590235710144
iteration 184, loss = 0.2650991380214691
iteration 185, loss = 0.26670265197753906
iteration 186, loss = 0.16212449967861176
iteration 187, loss = 0.09091955423355103
iteration 188, loss = 0.15134991705417633
iteration 189, loss = 0.13858212530612946
iteration 190, loss = 0.17688697576522827
iteration 191, loss = 0.37459123134613037
iteration 192, loss = 0.13043135404586792
iteration 193, loss = 0.2565791606903076
iteration 194, loss = 0.34244197607040405
iteration 195, loss = 0.1692294478416443
iteration 196, loss = 0.1335063874721527
iteration 197, loss = 0.18708094954490662
iteration 198, loss = 0.35363075137138367
iteration 199, loss = 0.13033728301525116
iteration 200, loss = 0.09956921637058258
iteration 201, loss = 0.12243223190307617
iteration 202, loss = 0.15615230798721313
iteration 203, loss = 0.37391629815101624
iteration 204, loss = 0.43840450048446655
iteration 205, loss = 0.5811483860015869
iteration 206, loss = 0.2676895260810852
iteration 207, loss = 0.1287996619939804
iteration 208, loss = 0.283263623714447
iteration 209, loss = 0.3583199977874756
iteration 210, loss = 0.05177091807126999
iteration 211, loss = 0.22313430905342102
iteration 212, loss = 0.2127058207988739
iteration 213, loss = 0.18951432406902313
iteration 214, loss = 0.07738204300403595
iteration 215, loss = 0.46451881527900696
iteration 216, loss = 0.25817355513572693
iteration 217, loss = 0.26297158002853394
iteration 218, loss = 0.22534100711345673
iteration 219, loss = 0.20990297198295593
iteration 220, loss = 0.15397188067436218
iteration 221, loss = 0.34716594219207764
iteration 222, loss = 0.14124628901481628
iteration 223, loss = 0.19363433122634888
iteration 224, loss = 0.1256376951932907
iteration 225, loss = 0.27877578139305115
iteration 226, loss = 0.482021689414978
iteration 227, loss = 0.11562085151672363
iteration 228, loss = 0.05360179394483566
iteration 229, loss = 0.2993527352809906
iteration 230, loss = 0.0725952535867691
iteration 231, loss = 0.09397029131650925
iteration 232, loss = 0.3777253329753876
iteration 233, loss = 0.36438384652137756
iteration 234, loss = 0.2610566318035126
iteration 235, loss = 0.44400691986083984
iteration 236, loss = 0.22774669528007507
iteration 237, loss = 0.10844139754772186
iteration 238, loss = 0.24139827489852905
iteration 239, loss = 0.3333505392074585
iteration 240, loss = 0.2633744776248932
iteration 241, loss = 0.23072406649589539
iteration 242, loss = 0.2526547610759735
iteration 243, loss = 0.14897732436656952
iteration 244, loss = 0.25535571575164795
iteration 245, loss = 0.3073892593383789
iteration 246, loss = 0.2814447283744812
iteration 247, loss = 0.4403712749481201
iteration 248, loss = 0.3893907964229584
iteration 249, loss = 0.41092196106910706
iteration 250, loss = 0.35937103629112244
iteration 251, loss = 0.25398725271224976
iteration 252, loss = 0.2459607720375061
iteration 253, loss = 0.22246608138084412
iteration 254, loss = 0.16069114208221436
iteration 255, loss = 0.13009001314640045
iteration 256, loss = 0.24779215455055237
iteration 257, loss = 0.4864210784435272
iteration 258, loss = 0.5919759273529053
iteration 259, loss = 0.3210519552230835
iteration 260, loss = 0.3336833715438843
iteration 261, loss = 0.27743780612945557
iteration 262, loss = 0.1352590024471283
iteration 263, loss = 0.1666293442249298
iteration 264, loss = 0.20441947877407074
iteration 265, loss = 0.2547324299812317
iteration 266, loss = 0.13778077065944672
iteration 267, loss = 0.4248434901237488
iteration 268, loss = 0.3972102701663971
iteration 269, loss = 0.38923344016075134
iteration 270, loss = 0.1592729687690735
iteration 271, loss = 0.25957611203193665
iteration 272, loss = 0.16632913053035736
iteration 273, loss = 0.23024389147758484
iteration 274, loss = 0.3064402937889099
iteration 275, loss = 0.17122714221477509
iteration 276, loss = 0.36481592059135437
iteration 277, loss = 0.20870983600616455
iteration 278, loss = 0.1602001041173935
iteration 279, loss = 0.178487166762352
iteration 280, loss = 0.338619589805603
iteration 281, loss = 0.13229745626449585
iteration 282, loss = 0.05751771479845047
iteration 283, loss = 0.5218468904495239
iteration 284, loss = 0.27486222982406616
iteration 285, loss = 0.44693610072135925
iteration 286, loss = 0.35963234305381775
iteration 287, loss = 0.08703019469976425
iteration 288, loss = 0.12182280421257019
iteration 289, loss = 0.2582646608352661
iteration 290, loss = 0.31622666120529175
iteration 291, loss = 0.45228826999664307
iteration 292, loss = 0.3198143243789673
iteration 293, loss = 0.21450184285640717
iteration 294, loss = 0.21843871474266052
iteration 295, loss = 0.16563332080841064
iteration 296, loss = 0.27921348810195923
iteration 297, loss = 0.10901409387588501
iteration 298, loss = 0.11557043343782425
iteration 299, loss = 0.07012305408716202
iteration 0, loss = 0.1519375741481781
iteration 1, loss = 0.07212972640991211
iteration 2, loss = 0.29433631896972656
iteration 3, loss = 0.7873725295066833
iteration 4, loss = 0.3394024968147278
iteration 5, loss = 0.13301199674606323
iteration 6, loss = 0.44961947202682495
iteration 7, loss = 0.23200252652168274
iteration 8, loss = 0.0922926738858223
iteration 9, loss = 0.19237807393074036
iteration 10, loss = 0.2631416916847229
iteration 11, loss = 0.38953498005867004
iteration 12, loss = 0.4547726809978485
iteration 13, loss = 0.2421194612979889
iteration 14, loss = 0.18611249327659607
iteration 15, loss = 0.30746525526046753
iteration 16, loss = 0.2337535172700882
iteration 17, loss = 0.08787963539361954
iteration 18, loss = 0.12842650711536407
iteration 19, loss = 0.3416370451450348
iteration 20, loss = 0.2824678421020508
iteration 21, loss = 0.14214329421520233
iteration 22, loss = 0.2814450263977051
iteration 23, loss = 0.2915031313896179
iteration 24, loss = 0.4243420660495758
iteration 25, loss = 0.5830916166305542
iteration 26, loss = 0.36359894275665283
iteration 27, loss = 0.31831881403923035
iteration 28, loss = 0.4134252667427063
iteration 29, loss = 0.2687109410762787
iteration 30, loss = 0.30078768730163574
iteration 31, loss = 0.18187925219535828
iteration 32, loss = 0.25199535489082336
iteration 33, loss = 0.144071564078331
iteration 34, loss = 0.34302768111228943
iteration 35, loss = 0.193311870098114
iteration 36, loss = 0.16847237944602966
iteration 37, loss = 0.18147864937782288
iteration 38, loss = 0.23696719110012054
iteration 39, loss = 0.15568137168884277
iteration 40, loss = 0.32897502183914185
iteration 41, loss = 0.20524129271507263
iteration 42, loss = 0.34813353419303894
iteration 43, loss = 0.4378177225589752
iteration 44, loss = 0.2239595651626587
iteration 45, loss = 0.1883130967617035
iteration 46, loss = 0.18274861574172974
iteration 47, loss = 0.2662191390991211
iteration 48, loss = 0.09154725074768066
iteration 49, loss = 0.15106500685214996
iteration 50, loss = 0.3124236464500427
iteration 51, loss = 0.1339975893497467
iteration 52, loss = 0.20990772545337677
iteration 53, loss = 0.1585826575756073
iteration 54, loss = 0.24887210130691528
iteration 55, loss = 0.45165079832077026
iteration 56, loss = 0.34494662284851074
iteration 57, loss = 0.3941171169281006
iteration 58, loss = 0.22577634453773499
iteration 59, loss = 0.3668074607849121
iteration 60, loss = 0.059659767895936966
iteration 61, loss = 0.025071218609809875
iteration 62, loss = 0.5347944498062134
iteration 63, loss = 0.8017665147781372
iteration 64, loss = 0.49126121401786804
iteration 65, loss = 0.2107577621936798
iteration 66, loss = 0.2587845027446747
iteration 67, loss = 0.1999921053647995
iteration 68, loss = 0.09340804815292358
iteration 69, loss = 0.14068716764450073
iteration 70, loss = 0.3635499179363251
iteration 71, loss = 0.12130522727966309
iteration 72, loss = 0.26924270391464233
iteration 73, loss = 0.184893399477005
iteration 74, loss = 0.3357577919960022
iteration 75, loss = 0.37050357460975647
iteration 76, loss = 0.2331407517194748
iteration 77, loss = 0.05888202413916588
iteration 78, loss = 0.1317797601222992
iteration 79, loss = 0.21390576660633087
iteration 80, loss = 0.4305917024612427
iteration 81, loss = 0.11171898245811462
iteration 82, loss = 0.19179007411003113
iteration 83, loss = 0.13125915825366974
iteration 84, loss = 0.18330377340316772
iteration 85, loss = 0.23028147220611572
iteration 86, loss = 0.20203527808189392
iteration 87, loss = 0.28725284337997437
iteration 88, loss = 0.3491981029510498
iteration 89, loss = 0.25802871584892273
iteration 90, loss = 0.08650180697441101
iteration 91, loss = 0.3535439372062683
iteration 92, loss = 0.186216339468956
iteration 93, loss = 0.13573385775089264
iteration 94, loss = 0.35315027832984924
iteration 95, loss = 0.20835649967193604
iteration 96, loss = 0.28983521461486816
iteration 97, loss = 0.06483396887779236
iteration 98, loss = 0.20914006233215332
iteration 99, loss = 0.24533230066299438
iteration 100, loss = 0.19836962223052979
iteration 101, loss = 0.285250723361969
iteration 102, loss = 0.31108418107032776
iteration 103, loss = 0.31986141204833984
iteration 104, loss = 0.4898360073566437
iteration 105, loss = 0.4173317849636078
iteration 106, loss = 0.3979242146015167
iteration 107, loss = 0.2372015416622162
iteration 108, loss = 0.3631516396999359
iteration 109, loss = 0.1821942776441574
iteration 110, loss = 0.11698475480079651
iteration 111, loss = 0.32638341188430786
iteration 112, loss = 0.28696683049201965
iteration 113, loss = 0.25412991642951965
iteration 114, loss = 0.2607068717479706
iteration 115, loss = 0.21795353293418884
iteration 116, loss = 0.37753650546073914
iteration 117, loss = 0.13069488108158112
iteration 118, loss = 0.2965230345726013
iteration 119, loss = 0.12578541040420532
iteration 120, loss = 0.144605815410614
iteration 121, loss = 0.3241960406303406
iteration 122, loss = 0.25979387760162354
iteration 123, loss = 0.3499935567378998
iteration 124, loss = 0.14480337500572205
iteration 125, loss = 0.2744383215904236
iteration 126, loss = 0.09526476263999939
iteration 127, loss = 0.27786731719970703
iteration 128, loss = 0.2447727620601654
iteration 129, loss = 0.18669261038303375
iteration 130, loss = 0.10685913264751434
iteration 131, loss = 0.21090352535247803
iteration 132, loss = 0.3882617950439453
iteration 133, loss = 0.14627502858638763
iteration 134, loss = 0.15690535306930542
iteration 135, loss = 0.31157752871513367
iteration 136, loss = 0.05468679964542389
iteration 137, loss = 0.11230019479990005
iteration 138, loss = 0.2166387289762497
iteration 139, loss = 0.4515714943408966
iteration 140, loss = 0.23793305456638336
iteration 141, loss = 0.18902000784873962
iteration 142, loss = 0.03767607733607292
iteration 143, loss = 0.3453215956687927
iteration 144, loss = 0.3413657546043396
iteration 145, loss = 0.2308749258518219
iteration 146, loss = 0.17437943816184998
iteration 147, loss = 0.11678019165992737
iteration 148, loss = 0.23860259354114532
iteration 149, loss = 0.24230143427848816
iteration 150, loss = 0.08477173745632172
iteration 151, loss = 0.2409711331129074
iteration 152, loss = 0.3147083818912506
iteration 153, loss = 0.13918469846248627
iteration 154, loss = 0.0730527937412262
iteration 155, loss = 0.12454160302877426
iteration 156, loss = 0.012852264568209648
iteration 157, loss = 0.502463698387146
iteration 158, loss = 0.13463488221168518
iteration 159, loss = 0.21399489045143127
iteration 160, loss = 0.3693760931491852
iteration 161, loss = 0.2913239002227783
iteration 162, loss = 0.331719309091568
iteration 163, loss = 0.06316214054822922
iteration 164, loss = 0.09758181124925613
iteration 165, loss = 0.28828105330467224
iteration 166, loss = 0.07883643358945847
iteration 167, loss = 0.1990509331226349
iteration 168, loss = 0.2824821472167969
iteration 169, loss = 0.02130410261452198
iteration 170, loss = 0.34121111035346985
iteration 171, loss = 0.24711909890174866
iteration 172, loss = 0.35010653734207153
iteration 173, loss = 0.3847106099128723
iteration 174, loss = 0.2835119366645813
iteration 175, loss = 0.06113545969128609
iteration 176, loss = 0.23757924139499664
iteration 177, loss = 0.27174270153045654
iteration 178, loss = 0.14871938526630402
iteration 179, loss = 0.14416661858558655
iteration 180, loss = 0.2871350944042206
iteration 181, loss = 0.23848363757133484
iteration 182, loss = 0.28474754095077515
iteration 183, loss = 0.2891329526901245
iteration 184, loss = 0.19536618888378143
iteration 185, loss = 0.2115340530872345
iteration 186, loss = 0.09598279744386673
iteration 187, loss = 0.0358046256005764
iteration 188, loss = 0.1897849291563034
iteration 189, loss = 0.18460091948509216
iteration 190, loss = 0.09258423000574112
iteration 191, loss = 0.409393310546875
iteration 192, loss = 0.2207164168357849
iteration 193, loss = 0.0749119371175766
iteration 194, loss = 0.0647999495267868
iteration 195, loss = 0.3440442979335785
iteration 196, loss = 0.3128267526626587
iteration 197, loss = 0.3229977786540985
iteration 198, loss = 0.39134541153907776
iteration 199, loss = 0.4356219172477722
iteration 200, loss = 0.291778028011322
iteration 201, loss = 0.16261404752731323
iteration 202, loss = 0.2708112299442291
iteration 203, loss = 0.10394556075334549
iteration 204, loss = 0.43901053071022034
iteration 205, loss = 0.2067396193742752
iteration 206, loss = 0.26542985439300537
iteration 207, loss = 0.2941465973854065
iteration 208, loss = 0.21471282839775085
iteration 209, loss = 0.2816048264503479
iteration 210, loss = 0.16743186116218567
iteration 211, loss = 0.12439528107643127
iteration 212, loss = 0.21562547981739044
iteration 213, loss = 0.13145555555820465
iteration 214, loss = 0.42112985253334045
iteration 215, loss = 0.24885734915733337
iteration 216, loss = 0.08836100995540619
iteration 217, loss = 0.5674165487289429
iteration 218, loss = 0.3805127441883087
iteration 219, loss = 0.42130348086357117
iteration 220, loss = 0.17191219329833984
iteration 221, loss = 0.400691419839859
iteration 222, loss = 0.3381863534450531
iteration 223, loss = 0.35410836338996887
iteration 224, loss = 0.21886557340621948
iteration 225, loss = 0.15480875968933105
iteration 226, loss = 0.2355833351612091
iteration 227, loss = 0.21917670965194702
iteration 228, loss = 0.28313305974006653
iteration 229, loss = 0.3337610065937042
iteration 230, loss = 0.4447356164455414
iteration 231, loss = 0.6664959192276001
iteration 232, loss = 0.44030922651290894
iteration 233, loss = 0.2021622657775879
iteration 234, loss = 0.20429779589176178
iteration 235, loss = 0.36306872963905334
iteration 236, loss = 0.212648406624794
iteration 237, loss = 0.3079227805137634
iteration 238, loss = 0.15657278895378113
iteration 239, loss = 0.1397959291934967
iteration 240, loss = 0.47052401304244995
iteration 241, loss = 0.1688757687807083
iteration 242, loss = 0.6152377724647522
iteration 243, loss = 0.03477923944592476
iteration 244, loss = 0.07736530900001526
iteration 245, loss = 0.11453384160995483
iteration 246, loss = 0.25420916080474854
iteration 247, loss = 0.10766333341598511
iteration 248, loss = 0.40915894508361816
iteration 249, loss = 0.29350125789642334
iteration 250, loss = 0.26875320076942444
iteration 251, loss = 0.21771526336669922
iteration 252, loss = 0.19236710667610168
iteration 253, loss = 0.33477649092674255
iteration 254, loss = 0.17464995384216309
iteration 255, loss = 0.28932732343673706
iteration 256, loss = 0.234236940741539
iteration 257, loss = 0.11439378559589386
iteration 258, loss = 0.05011536180973053
iteration 259, loss = 0.4791272282600403
iteration 260, loss = 0.19000329077243805
iteration 261, loss = 0.17235693335533142
iteration 262, loss = 0.22508451342582703
iteration 263, loss = 0.09553011506795883
iteration 264, loss = 0.03386610373854637
iteration 265, loss = 0.12325777113437653
iteration 266, loss = 0.14569789171218872
iteration 267, loss = 0.10082219541072845
iteration 268, loss = 0.16954335570335388
iteration 269, loss = 0.38554269075393677
iteration 270, loss = 0.214774027466774
iteration 271, loss = 0.32208919525146484
iteration 272, loss = 0.14194928109645844
iteration 273, loss = 0.37004637718200684
iteration 274, loss = 0.2649654448032379
iteration 275, loss = 0.09325676411390305
iteration 276, loss = 0.4353269934654236
iteration 277, loss = 0.24836716055870056
iteration 278, loss = 0.23776665329933167
iteration 279, loss = 0.1373661607503891
iteration 280, loss = 0.10016073286533356
iteration 281, loss = 0.31274479627609253
iteration 282, loss = 0.29528385400772095
iteration 283, loss = 0.37938547134399414
iteration 284, loss = 0.10774596780538559
iteration 285, loss = 0.25934746861457825
iteration 286, loss = 0.4174705147743225
iteration 287, loss = 0.13597898185253143
iteration 288, loss = 0.29575756192207336
iteration 289, loss = 0.10615480691194534
iteration 290, loss = 0.11057592183351517
iteration 291, loss = 0.1592712551355362
iteration 292, loss = 0.30501675605773926
iteration 293, loss = 0.11474990099668503
iteration 294, loss = 0.17412367463111877
iteration 295, loss = 0.3118053674697876
iteration 296, loss = 0.0930081158876419
iteration 297, loss = 0.3088480234146118
iteration 298, loss = 0.1552034616470337
iteration 299, loss = 0.23884791135787964
iteration 0, loss = 0.14774438738822937
iteration 1, loss = 0.2488090693950653
iteration 2, loss = 0.28605544567108154
iteration 3, loss = 0.2729727327823639
iteration 4, loss = 0.11035450547933578
iteration 5, loss = 0.2895069122314453
iteration 6, loss = 0.4358788728713989
iteration 7, loss = 0.17154757678508759
iteration 8, loss = 0.15827390551567078
iteration 9, loss = 0.23421140015125275
iteration 10, loss = 0.22840698063373566
iteration 11, loss = 0.3173817992210388
iteration 12, loss = 0.2520101070404053
iteration 13, loss = 0.06735189259052277
iteration 14, loss = 0.27628830075263977
iteration 15, loss = 0.23499073088169098
iteration 16, loss = 0.3678269386291504
iteration 17, loss = 0.24086782336235046
iteration 18, loss = 0.2809857428073883
iteration 19, loss = 0.17402395606040955
iteration 20, loss = 0.11767588555812836
iteration 21, loss = 0.3078950345516205
iteration 22, loss = 0.16814810037612915
iteration 23, loss = 0.2371366322040558
iteration 24, loss = 0.12539848685264587
iteration 25, loss = 0.17856387794017792
iteration 26, loss = 0.3963441848754883
iteration 27, loss = 0.29063063859939575
iteration 28, loss = 0.2297458052635193
iteration 29, loss = 0.11366582661867142
iteration 30, loss = 0.2549092173576355
iteration 31, loss = 0.279213547706604
iteration 32, loss = 0.19546544551849365
iteration 33, loss = 0.22059208154678345
iteration 34, loss = 0.34538257122039795
iteration 35, loss = 0.4362148642539978
iteration 36, loss = 0.26523637771606445
iteration 37, loss = 0.22763283550739288
iteration 38, loss = 0.3272760510444641
iteration 39, loss = 0.07048088312149048
iteration 40, loss = 0.10119457542896271
iteration 41, loss = 0.18056415021419525
iteration 42, loss = 0.5313949584960938
iteration 43, loss = 0.3497510850429535
iteration 44, loss = 0.04035283252596855
iteration 45, loss = 0.056402143090963364
iteration 46, loss = 0.07027571648359299
iteration 47, loss = 0.221797913312912
iteration 48, loss = 0.13591235876083374
iteration 49, loss = 0.1605650931596756
iteration 50, loss = 0.22830215096473694
iteration 51, loss = 0.30457913875579834
iteration 52, loss = 0.2052951604127884
iteration 53, loss = 0.36336076259613037
iteration 54, loss = 0.3395839035511017
iteration 55, loss = 0.09978487342596054
iteration 56, loss = 0.03482075035572052
iteration 57, loss = 0.4306049048900604
iteration 58, loss = 0.20273074507713318
iteration 59, loss = 0.31803035736083984
iteration 60, loss = 0.13737860321998596
iteration 61, loss = 0.0725548043847084
iteration 62, loss = 0.11239223927259445
iteration 63, loss = 0.16731861233711243
iteration 64, loss = 0.527744472026825
iteration 65, loss = 0.20347893238067627
iteration 66, loss = 0.2693485915660858
iteration 67, loss = 0.3233035206794739
iteration 68, loss = 0.36528635025024414
iteration 69, loss = 0.25018665194511414
iteration 70, loss = 0.1982322335243225
iteration 71, loss = 0.344444215297699
iteration 72, loss = 0.4664612412452698
iteration 73, loss = 0.33705341815948486
iteration 74, loss = 0.20249015092849731
iteration 75, loss = 0.31077978014945984
iteration 76, loss = 0.12835438549518585
iteration 77, loss = 0.2197888195514679
iteration 78, loss = 0.18061763048171997
iteration 79, loss = 0.09936255216598511
iteration 80, loss = 0.3155597746372223
iteration 81, loss = 0.2585344910621643
iteration 82, loss = 0.24034728109836578
iteration 83, loss = 0.2549341917037964
iteration 84, loss = 0.11904577910900116
iteration 85, loss = 0.616463303565979
iteration 86, loss = 0.31381165981292725
iteration 87, loss = 0.23680910468101501
iteration 88, loss = 0.09489203244447708
iteration 89, loss = 0.36774247884750366
iteration 90, loss = 0.2732706069946289
iteration 91, loss = 0.10594935715198517
iteration 92, loss = 0.18441425263881683
iteration 93, loss = 0.16747060418128967
iteration 94, loss = 0.10380593687295914
iteration 95, loss = 0.19861918687820435
iteration 96, loss = 0.22852082550525665
iteration 97, loss = 0.15215864777565002
iteration 98, loss = 0.3131354749202728
iteration 99, loss = 0.41997620463371277
iteration 100, loss = 0.3771408498287201
iteration 101, loss = 0.25218990445137024
iteration 102, loss = 0.26908761262893677
iteration 103, loss = 0.2848106622695923
iteration 104, loss = 0.30696892738342285
iteration 105, loss = 0.1331431269645691
iteration 106, loss = 0.22892722487449646
iteration 107, loss = 0.0960889682173729
iteration 108, loss = 0.08350357413291931
iteration 109, loss = 0.39919623732566833
iteration 110, loss = 0.345963716506958
iteration 111, loss = 0.2665857970714569
iteration 112, loss = 0.3147260844707489
iteration 113, loss = 0.162469744682312
iteration 114, loss = 0.27200454473495483
iteration 115, loss = 0.06556976586580276
iteration 116, loss = 0.10081776976585388
iteration 117, loss = 0.13350819051265717
iteration 118, loss = 0.13866345584392548
iteration 119, loss = 0.35736337304115295
iteration 120, loss = 0.34317344427108765
iteration 121, loss = 0.44047534465789795
iteration 122, loss = 0.26635974645614624
iteration 123, loss = 0.1309608668088913
iteration 124, loss = 0.162550687789917
iteration 125, loss = 0.15934428572654724
iteration 126, loss = 0.5125865340232849
iteration 127, loss = 0.24170039594173431
iteration 128, loss = 0.02670113742351532
iteration 129, loss = 0.27999916672706604
iteration 130, loss = 0.3618060052394867
iteration 131, loss = 0.28952425718307495
iteration 132, loss = 0.2938250005245209
iteration 133, loss = 0.4011223316192627
iteration 134, loss = 0.35379883646965027
iteration 135, loss = 0.2403808981180191
iteration 136, loss = 0.22496117651462555
iteration 137, loss = 0.13957171142101288
iteration 138, loss = 0.17165181040763855
iteration 139, loss = 0.05388420447707176
iteration 140, loss = 0.21225127577781677
iteration 141, loss = 0.3521583676338196
iteration 142, loss = 0.5389708876609802
iteration 143, loss = 0.028023971244692802
iteration 144, loss = 0.4365827441215515
iteration 145, loss = 0.22688892483711243
iteration 146, loss = 0.09567146003246307
iteration 147, loss = 0.42979443073272705
iteration 148, loss = 0.26808252930641174
iteration 149, loss = 0.14733465015888214
iteration 150, loss = 0.047936998307704926
iteration 151, loss = 0.31425145268440247
iteration 152, loss = 0.38785436749458313
iteration 153, loss = 0.2978208661079407
iteration 154, loss = 0.15546387434005737
iteration 155, loss = 0.1444946676492691
iteration 156, loss = 0.23165616393089294
iteration 157, loss = 0.1631828099489212
iteration 158, loss = 0.2488357126712799
iteration 159, loss = 0.127875417470932
iteration 160, loss = 0.14406916499137878
iteration 161, loss = 0.15502041578292847
iteration 162, loss = 0.27617165446281433
iteration 163, loss = 0.1115112155675888
iteration 164, loss = 0.18816764652729034
iteration 165, loss = 0.18353675305843353
iteration 166, loss = 0.1696891486644745
iteration 167, loss = 0.18866127729415894
iteration 168, loss = 0.22549740970134735
iteration 169, loss = 0.1808413863182068
iteration 170, loss = 0.3957292437553406
iteration 171, loss = 0.2496616691350937
iteration 172, loss = 0.22454649209976196
iteration 173, loss = 0.1230759397149086
iteration 174, loss = 0.13532471656799316
iteration 175, loss = 0.12766583263874054
iteration 176, loss = 0.16929715871810913
iteration 177, loss = 0.13172921538352966
iteration 178, loss = 0.22918012738227844
iteration 179, loss = 0.3354838788509369
iteration 180, loss = 0.10674677044153214
iteration 181, loss = 0.41678106784820557
iteration 182, loss = 0.2019573152065277
iteration 183, loss = 0.23222681879997253
iteration 184, loss = 0.2240999937057495
iteration 185, loss = 0.2308998554944992
iteration 186, loss = 0.4633943438529968
iteration 187, loss = 0.24506519734859467
iteration 188, loss = 0.16766484081745148
iteration 189, loss = 0.10655193775892258
iteration 190, loss = 0.03359070420265198
iteration 191, loss = 0.1437223255634308
iteration 192, loss = 0.059650443494319916
iteration 193, loss = 0.1787557601928711
iteration 194, loss = 0.2328358292579651
iteration 195, loss = 0.3266276717185974
iteration 196, loss = 0.22404563426971436
iteration 197, loss = 0.0746031254529953
iteration 198, loss = 0.03212953358888626
iteration 199, loss = 0.3447803854942322
iteration 200, loss = 0.2030496895313263
iteration 201, loss = 0.2658553123474121
iteration 202, loss = 0.41389960050582886
iteration 203, loss = 0.2821075916290283
iteration 204, loss = 0.22013872861862183
iteration 205, loss = 0.16151346266269684
iteration 206, loss = 0.16992154717445374
iteration 207, loss = 0.26019665598869324
iteration 208, loss = 0.2714579403400421
iteration 209, loss = 0.33461254835128784
iteration 210, loss = 0.13183382153511047
iteration 211, loss = 0.2245258092880249
iteration 212, loss = 0.2494535744190216
iteration 213, loss = 0.30240046977996826
iteration 214, loss = 0.22379319369792938
iteration 215, loss = 0.202964186668396
iteration 216, loss = 0.11323021352291107
iteration 217, loss = 0.09366511553525925
iteration 218, loss = 0.4399915635585785
iteration 219, loss = 0.08845895528793335
iteration 220, loss = 0.07938291877508163
iteration 221, loss = 0.2702016830444336
iteration 222, loss = 0.46067890524864197
iteration 223, loss = 0.1781485378742218
iteration 224, loss = 0.21523764729499817
iteration 225, loss = 0.4609425365924835
iteration 226, loss = 0.22595511376857758
iteration 227, loss = 0.17249058187007904
iteration 228, loss = 0.1587727963924408
iteration 229, loss = 0.09414181858301163
iteration 230, loss = 0.3392171561717987
iteration 231, loss = 0.3170626163482666
iteration 232, loss = 0.29069846868515015
iteration 233, loss = 0.2627696990966797
iteration 234, loss = 0.26311981678009033
iteration 235, loss = 0.053297776728868484
iteration 236, loss = 0.3659489154815674
iteration 237, loss = 0.3816075026988983
iteration 238, loss = 0.2768399715423584
iteration 239, loss = 0.15739144384860992
iteration 240, loss = 0.2599228322505951
iteration 241, loss = 0.16688388586044312
iteration 242, loss = 0.11580875515937805
iteration 243, loss = 0.19357280433177948
iteration 244, loss = 0.17038404941558838
iteration 245, loss = 0.08650004863739014
iteration 246, loss = 0.2605956196784973
iteration 247, loss = 0.05911525338888168
iteration 248, loss = 0.8029148578643799
iteration 249, loss = 0.03520766273140907
iteration 250, loss = 0.16811011731624603
iteration 251, loss = 0.23026253283023834
iteration 252, loss = 0.654860258102417
iteration 253, loss = 0.1411333680152893
iteration 254, loss = 0.17441672086715698
iteration 255, loss = 0.42537859082221985
iteration 256, loss = 0.36432352662086487
iteration 257, loss = 0.22018954157829285
iteration 258, loss = 0.3169940710067749
iteration 259, loss = 0.2056918442249298
iteration 260, loss = 0.16663022339344025
iteration 261, loss = 0.28826767206192017
iteration 262, loss = 0.3089940547943115
iteration 263, loss = 0.26557886600494385
iteration 264, loss = 0.12817776203155518
iteration 265, loss = 0.2764207720756531
iteration 266, loss = 0.3056478500366211
iteration 267, loss = 0.2954030930995941
iteration 268, loss = 0.29178136587142944
iteration 269, loss = 0.108586385846138
iteration 270, loss = 0.21123021841049194
iteration 271, loss = 0.2292257398366928
iteration 272, loss = 0.40289920568466187
iteration 273, loss = 0.37108075618743896
iteration 274, loss = 0.1139039471745491
iteration 275, loss = 0.12924635410308838
iteration 276, loss = 0.26263225078582764
iteration 277, loss = 0.2586456537246704
iteration 278, loss = 0.25253039598464966
iteration 279, loss = 0.3938590884208679
iteration 280, loss = 0.2780667245388031
iteration 281, loss = 0.2127867043018341
iteration 282, loss = 0.16935300827026367
iteration 283, loss = 0.15542128682136536
iteration 284, loss = 0.28857263922691345
iteration 285, loss = 0.1708092838525772
iteration 286, loss = 0.38340306282043457
iteration 287, loss = 0.1660986691713333
iteration 288, loss = 0.27610617876052856
iteration 289, loss = 0.10175923258066177
iteration 290, loss = 0.2033563256263733
iteration 291, loss = 0.18423938751220703
iteration 292, loss = 0.4642758369445801
iteration 293, loss = 0.21706834435462952
iteration 294, loss = 0.22263777256011963
iteration 295, loss = 0.2725423574447632
iteration 296, loss = 0.17174428701400757
iteration 297, loss = 0.16777125000953674
iteration 298, loss = 0.45783478021621704
iteration 299, loss = 0.04027632623910904
iteration 0, loss = 0.279587984085083
iteration 1, loss = 0.16389241814613342
iteration 2, loss = 0.11612644046545029
iteration 3, loss = 0.2848539650440216
iteration 4, loss = 0.2700634300708771
iteration 5, loss = 0.45328769087791443
iteration 6, loss = 0.4141825735569
iteration 7, loss = 0.2149033099412918
iteration 8, loss = 0.26327815651893616
iteration 9, loss = 0.644370973110199
iteration 10, loss = 0.2246302366256714
iteration 11, loss = 0.20317813754081726
iteration 12, loss = 0.1714518964290619
iteration 13, loss = 0.3887939453125
iteration 14, loss = 0.2784401476383209
iteration 15, loss = 0.1949499100446701
iteration 16, loss = 0.22521468997001648
iteration 17, loss = 0.23832078278064728
iteration 18, loss = 0.051459427922964096
iteration 19, loss = 0.06392619013786316
iteration 20, loss = 0.11159814149141312
iteration 21, loss = 0.13941527903079987
iteration 22, loss = 0.3947475254535675
iteration 23, loss = 0.2652454674243927
iteration 24, loss = 0.3647462725639343
iteration 25, loss = 0.2612602412700653
iteration 26, loss = 0.20794862508773804
iteration 27, loss = 0.38070130348205566
iteration 28, loss = 0.3491952121257782
iteration 29, loss = 0.16662965714931488
iteration 30, loss = 0.10142606496810913
iteration 31, loss = 0.12414687871932983
iteration 32, loss = 0.2608308792114258
iteration 33, loss = 0.4937680661678314
iteration 34, loss = 0.11896201968193054
iteration 35, loss = 0.0989544540643692
iteration 36, loss = 0.36614859104156494
iteration 37, loss = 0.27904483675956726
iteration 38, loss = 0.11522223800420761
iteration 39, loss = 0.2524245083332062
iteration 40, loss = 0.04347582906484604
iteration 41, loss = 0.21884486079216003
iteration 42, loss = 0.15571947395801544
iteration 43, loss = 0.1584494560956955
iteration 44, loss = 0.256278932094574
iteration 45, loss = 0.3814959228038788
iteration 46, loss = 0.12523114681243896
iteration 47, loss = 0.218404620885849
iteration 48, loss = 0.3352704346179962
iteration 49, loss = 0.10280175507068634
iteration 50, loss = 0.2599920630455017
iteration 51, loss = 0.34939879179000854
iteration 52, loss = 0.30546873807907104
iteration 53, loss = 0.44309332966804504
iteration 54, loss = 0.43382906913757324
iteration 55, loss = 0.2543409466743469
iteration 56, loss = 0.27478042244911194
iteration 57, loss = 0.07994407415390015
iteration 58, loss = 0.1636371612548828
iteration 59, loss = 0.027235087007284164
iteration 60, loss = 0.3192298114299774
iteration 61, loss = 0.3268139362335205
iteration 62, loss = 0.09659517556428909
iteration 63, loss = 0.11997796595096588
iteration 64, loss = 0.08501054346561432
iteration 65, loss = 0.42779651284217834
iteration 66, loss = 0.22252419590950012
iteration 67, loss = 0.24195243418216705
iteration 68, loss = 0.15221573412418365
iteration 69, loss = 0.2902432680130005
iteration 70, loss = 0.25045737624168396
iteration 71, loss = 0.1249605193734169
iteration 72, loss = 0.11426091194152832
iteration 73, loss = 0.2159748673439026
iteration 74, loss = 0.43476423621177673
iteration 75, loss = 0.025320686399936676
iteration 76, loss = 0.21197667717933655
iteration 77, loss = 0.6397637128829956
iteration 78, loss = 0.40811827778816223
iteration 79, loss = 0.4302625060081482
iteration 80, loss = 0.08539649844169617
iteration 81, loss = 0.1270315945148468
iteration 82, loss = 0.27028989791870117
iteration 83, loss = 0.19960439205169678
iteration 84, loss = 0.28048282861709595
iteration 85, loss = 0.31840410828590393
iteration 86, loss = 0.3799072504043579
iteration 87, loss = 0.385451078414917
iteration 88, loss = 0.252469003200531
iteration 89, loss = 0.10537810623645782
iteration 90, loss = 0.1400911659002304
iteration 91, loss = 0.2550796568393707
iteration 92, loss = 0.24156120419502258
iteration 93, loss = 0.19103088974952698
iteration 94, loss = 0.11647992581129074
iteration 95, loss = 0.17926537990570068
iteration 96, loss = 0.08513769507408142
iteration 97, loss = 0.281826376914978
iteration 98, loss = 0.41784900426864624
iteration 99, loss = 0.2084733247756958
iteration 100, loss = 0.13387015461921692
iteration 101, loss = 0.18105480074882507
iteration 102, loss = 0.24215032160282135
iteration 103, loss = 0.08080293238162994
iteration 104, loss = 0.3612634837627411
iteration 105, loss = 0.07804650068283081
iteration 106, loss = 0.14185889065265656
iteration 107, loss = 0.2647486925125122
iteration 108, loss = 0.2693125009536743
iteration 109, loss = 0.2541603446006775
iteration 110, loss = 0.36042091250419617
iteration 111, loss = 0.23538276553153992
iteration 112, loss = 0.35219907760620117
iteration 113, loss = 0.18596364557743073
iteration 114, loss = 0.2621608376502991
iteration 115, loss = 0.1247786432504654
iteration 116, loss = 0.026312123984098434
iteration 117, loss = 0.29969608783721924
iteration 118, loss = 0.17973509430885315
iteration 119, loss = 0.21863384544849396
iteration 120, loss = 0.1826137751340866
iteration 121, loss = 0.2827523648738861
iteration 122, loss = 0.24793215095996857
iteration 123, loss = 0.15846450626850128
iteration 124, loss = 0.3335961103439331
iteration 125, loss = 0.256630003452301
iteration 126, loss = 0.2175527960062027
iteration 127, loss = 0.21962177753448486
iteration 128, loss = 0.4268572926521301
iteration 129, loss = 0.43928760290145874
iteration 130, loss = 0.22315888106822968
iteration 131, loss = 0.20922081172466278
iteration 132, loss = 0.13998465240001678
iteration 133, loss = 0.21653249859809875
iteration 134, loss = 0.3318101465702057
iteration 135, loss = 0.25970011949539185
iteration 136, loss = 0.07365282624959946
iteration 137, loss = 0.29625049233436584
iteration 138, loss = 0.23035168647766113
iteration 139, loss = 0.15762323141098022
iteration 140, loss = 0.6320838332176208
iteration 141, loss = 0.10450246930122375
iteration 142, loss = 0.2867327332496643
iteration 143, loss = 0.2640565037727356
iteration 144, loss = 0.12448857724666595
iteration 145, loss = 0.16502439975738525
iteration 146, loss = 0.13666942715644836
iteration 147, loss = 0.491232305765152
iteration 148, loss = 0.5721573829650879
iteration 149, loss = 0.05505542457103729
iteration 150, loss = 0.15566593408584595
iteration 151, loss = 0.3449441194534302
iteration 152, loss = 0.13499009609222412
iteration 153, loss = 0.24327780306339264
iteration 154, loss = 0.04574153944849968
iteration 155, loss = 0.24237307906150818
iteration 156, loss = 0.14790190756320953
iteration 157, loss = 0.16823381185531616
iteration 158, loss = 0.18702253699302673
iteration 159, loss = 0.24076977372169495
iteration 160, loss = 0.12651531398296356
iteration 161, loss = 0.2183343768119812
iteration 162, loss = 0.13476476073265076
iteration 163, loss = 0.1433258056640625
iteration 164, loss = 0.23044267296791077
iteration 165, loss = 0.261913001537323
iteration 166, loss = 0.38311827182769775
iteration 167, loss = 0.2503625154495239
iteration 168, loss = 0.20269683003425598
iteration 169, loss = 0.07191497087478638
iteration 170, loss = 0.16238753497600555
iteration 171, loss = 0.10306672751903534
iteration 172, loss = 0.12149681150913239
iteration 173, loss = 0.26385730504989624
iteration 174, loss = 0.021528223529458046
iteration 175, loss = 0.5783466100692749
iteration 176, loss = 0.15488716959953308
iteration 177, loss = 0.34062498807907104
iteration 178, loss = 0.2600722014904022
iteration 179, loss = 0.3158508241176605
iteration 180, loss = 0.16776448488235474
iteration 181, loss = 0.1676904261112213
iteration 182, loss = 0.27832287549972534
iteration 183, loss = 0.23692621290683746
iteration 184, loss = 0.3988327085971832
iteration 185, loss = 0.39281415939331055
iteration 186, loss = 0.2996163070201874
iteration 187, loss = 0.08022525161504745
iteration 188, loss = 0.5583019852638245
iteration 189, loss = 0.4719583988189697
iteration 190, loss = 0.4480975568294525
iteration 191, loss = 0.2597558796405792
iteration 192, loss = 0.2553403973579407
iteration 193, loss = 0.0784490555524826
iteration 194, loss = 0.3745768666267395
iteration 195, loss = 0.34245797991752625
iteration 196, loss = 0.31296879053115845
iteration 197, loss = 0.232350692152977
iteration 198, loss = 0.10648427903652191
iteration 199, loss = 0.26068979501724243
iteration 200, loss = 0.20453521609306335
iteration 201, loss = 0.48837968707084656
iteration 202, loss = 0.17346040904521942
iteration 203, loss = 0.35689494013786316
iteration 204, loss = 0.2356356531381607
iteration 205, loss = 0.17166313529014587
iteration 206, loss = 0.18011927604675293
iteration 207, loss = 0.2529592216014862
iteration 208, loss = 0.45550215244293213
iteration 209, loss = 0.23075559735298157
iteration 210, loss = 0.15529878437519073
iteration 211, loss = 0.04967687278985977
iteration 212, loss = 0.11517536640167236
iteration 213, loss = 0.044093601405620575
iteration 214, loss = 0.27798670530319214
iteration 215, loss = 0.18047034740447998
iteration 216, loss = 0.15273529291152954
iteration 217, loss = 0.612769365310669
iteration 218, loss = 0.054421938955783844
iteration 219, loss = 0.0986766666173935
iteration 220, loss = 0.20146001875400543
iteration 221, loss = 0.34006592631340027
iteration 222, loss = 0.39916563034057617
iteration 223, loss = 0.17243041098117828
iteration 224, loss = 0.17158900201320648
iteration 225, loss = 0.22306060791015625
iteration 226, loss = 0.307466596364975
iteration 227, loss = 0.2666670083999634
iteration 228, loss = 0.2214861810207367
iteration 229, loss = 0.30225828289985657
iteration 230, loss = 0.21109522879123688
iteration 231, loss = 0.06720275431871414
iteration 232, loss = 0.25757235288619995
iteration 233, loss = 0.2946261465549469
iteration 234, loss = 0.22250743210315704
iteration 235, loss = 0.2937261462211609
iteration 236, loss = 0.11626836657524109
iteration 237, loss = 0.060120925307273865
iteration 238, loss = 0.21499595046043396
iteration 239, loss = 0.5314049124717712
iteration 240, loss = 0.37340712547302246
iteration 241, loss = 0.06322594732046127
iteration 242, loss = 0.2516591548919678
iteration 243, loss = 0.23804639279842377
iteration 244, loss = 0.3385390639305115
iteration 245, loss = 0.32525181770324707
iteration 246, loss = 0.06745900213718414
iteration 247, loss = 0.24451325833797455
iteration 248, loss = 0.05891899764537811
iteration 249, loss = 0.32330918312072754
iteration 250, loss = 0.2656002640724182
iteration 251, loss = 0.11046252399682999
iteration 252, loss = 0.2371257245540619
iteration 253, loss = 0.1178288534283638
iteration 254, loss = 0.2259375900030136
iteration 255, loss = 0.032429520040750504
iteration 256, loss = 0.2883602976799011
iteration 257, loss = 0.3109477162361145
iteration 258, loss = 0.017218438908457756
iteration 259, loss = 0.16299237310886383
iteration 260, loss = 0.1329747438430786
iteration 261, loss = 0.33918148279190063
iteration 262, loss = 0.14891411364078522
iteration 263, loss = 0.08504822850227356
iteration 264, loss = 0.4275374412536621
iteration 265, loss = 0.3663318455219269
iteration 266, loss = 0.2491483986377716
iteration 267, loss = 0.13418342173099518
iteration 268, loss = 0.11165226250886917
iteration 269, loss = 0.32976555824279785
iteration 270, loss = 0.18542753159999847
iteration 271, loss = 0.24747630953788757
iteration 272, loss = 0.37286698818206787
iteration 273, loss = 0.15816564857959747
iteration 274, loss = 0.034040533006191254
iteration 275, loss = 0.2331428974866867
iteration 276, loss = 0.13203763961791992
iteration 277, loss = 0.33423009514808655
iteration 278, loss = 0.11102322489023209
iteration 279, loss = 0.5050004720687866
iteration 280, loss = 0.2928657829761505
iteration 281, loss = 0.19009603559970856
iteration 282, loss = 0.17405299842357635
iteration 283, loss = 0.12083504348993301
iteration 284, loss = 0.540118396282196
iteration 285, loss = 0.37289875745773315
iteration 286, loss = 0.30825573205947876
iteration 287, loss = 0.20554856956005096
iteration 288, loss = 0.32613351941108704
iteration 289, loss = 0.27348244190216064
iteration 290, loss = 0.3024762272834778
iteration 291, loss = 0.2806655168533325
iteration 292, loss = 0.062801793217659
iteration 293, loss = 0.11074866354465485
iteration 294, loss = 0.2143530249595642
iteration 295, loss = 0.11498267948627472
iteration 296, loss = 0.5382155776023865
iteration 297, loss = 0.48550528287887573
iteration 298, loss = 0.17774388194084167
iteration 299, loss = 0.19100189208984375
iteration 0, loss = 0.12769418954849243
iteration 1, loss = 0.1992240846157074
iteration 2, loss = 0.2459719479084015
iteration 3, loss = 0.2569146752357483
iteration 4, loss = 0.4197540879249573
iteration 5, loss = 0.2365330308675766
iteration 6, loss = 0.21266719698905945
iteration 7, loss = 0.08467170596122742
iteration 8, loss = 0.029133401811122894
iteration 9, loss = 0.20691323280334473
iteration 10, loss = 0.6717914938926697
iteration 11, loss = 0.3858782947063446
iteration 12, loss = 0.29852017760276794
iteration 13, loss = 0.29579195380210876
iteration 14, loss = 0.5605581998825073
iteration 15, loss = 0.4131776690483093
iteration 16, loss = 0.1832408905029297
iteration 17, loss = 0.2796889841556549
iteration 18, loss = 0.09639604389667511
iteration 19, loss = 0.10672903060913086
iteration 20, loss = 0.01764950156211853
iteration 21, loss = 0.4639624357223511
iteration 22, loss = 0.5390002727508545
iteration 23, loss = 0.26082462072372437
iteration 24, loss = 0.20123279094696045
iteration 25, loss = 0.3406258225440979
iteration 26, loss = 0.20087949931621552
iteration 27, loss = 0.15380558371543884
iteration 28, loss = 0.3510398268699646
iteration 29, loss = 0.12298458814620972
iteration 30, loss = 0.05544619262218475
iteration 31, loss = 0.16160187125205994
iteration 32, loss = 0.06249571964144707
iteration 33, loss = 0.24870282411575317
iteration 34, loss = 0.2957591116428375
iteration 35, loss = 0.08915674686431885
iteration 36, loss = 0.11027789860963821
iteration 37, loss = 0.4929511249065399
iteration 38, loss = 0.2776942253112793
iteration 39, loss = 0.17779943346977234
iteration 40, loss = 0.24881874024868011
iteration 41, loss = 0.3141847252845764
iteration 42, loss = 0.24389103055000305
iteration 43, loss = 0.2685023546218872
iteration 44, loss = 0.06553412228822708
iteration 45, loss = 0.1823829561471939
iteration 46, loss = 0.335150808095932
iteration 47, loss = 0.09414331614971161
iteration 48, loss = 0.3820391595363617
iteration 49, loss = 0.14724768698215485
iteration 50, loss = 0.12939149141311646
iteration 51, loss = 0.09259336441755295
iteration 52, loss = 0.2896851897239685
iteration 53, loss = 0.0721428170800209
iteration 54, loss = 0.3473089933395386
iteration 55, loss = 0.23394492268562317
iteration 56, loss = 0.19539111852645874
iteration 57, loss = 0.10829433798789978
iteration 58, loss = 0.274524450302124
iteration 59, loss = 0.36179062724113464
iteration 60, loss = 0.14163602888584137
iteration 61, loss = 0.1951817274093628
iteration 62, loss = 0.38746631145477295
iteration 63, loss = 0.220608189702034
iteration 64, loss = 0.15165124833583832
iteration 65, loss = 0.14263546466827393
iteration 66, loss = 0.1477653533220291
iteration 67, loss = 0.053568534553050995
iteration 68, loss = 0.04998059198260307
iteration 69, loss = 0.058090690523386
iteration 70, loss = 0.2906269133090973
iteration 71, loss = 0.1044078916311264
iteration 72, loss = 0.22949549555778503
iteration 73, loss = 0.5298703908920288
iteration 74, loss = 0.08012297749519348
iteration 75, loss = 0.17088282108306885
iteration 76, loss = 0.25361549854278564
iteration 77, loss = 0.1911381334066391
iteration 78, loss = 0.21400316059589386
iteration 79, loss = 0.1753396838903427
iteration 80, loss = 0.2955701947212219
iteration 81, loss = 0.4509323239326477
iteration 82, loss = 0.5025702714920044
iteration 83, loss = 0.15363673865795135
iteration 84, loss = 0.11769440770149231
iteration 85, loss = 0.11599046736955643
iteration 86, loss = 0.21399810910224915
iteration 87, loss = 0.07057753205299377
iteration 88, loss = 0.21944841742515564
iteration 89, loss = 0.13492408394813538
iteration 90, loss = 0.3947669565677643
iteration 91, loss = 0.06837399303913116
iteration 92, loss = 0.21936693787574768
iteration 93, loss = 0.23163439333438873
iteration 94, loss = 0.032617732882499695
iteration 95, loss = 0.47550541162490845
iteration 96, loss = 0.20279063284397125
iteration 97, loss = 0.08791705220937729
iteration 98, loss = 0.0741729736328125
iteration 99, loss = 0.252951443195343
iteration 100, loss = 0.19305938482284546
iteration 101, loss = 0.15686601400375366
iteration 102, loss = 0.08670888096094131
iteration 103, loss = 0.19802328944206238
iteration 104, loss = 0.04584749788045883
iteration 105, loss = 0.33781930804252625
iteration 106, loss = 0.29114243388175964
iteration 107, loss = 0.29031139612197876
iteration 108, loss = 0.18279610574245453
iteration 109, loss = 0.047545913606882095
iteration 110, loss = 0.4025968611240387
iteration 111, loss = 0.3387283682823181
iteration 112, loss = 0.24463139474391937
iteration 113, loss = 0.2727404236793518
iteration 114, loss = 0.36070582270622253
iteration 115, loss = 0.3767130672931671
iteration 116, loss = 0.22263601422309875
iteration 117, loss = 0.26290738582611084
iteration 118, loss = 0.04293597489595413
iteration 119, loss = 0.057985804975032806
iteration 120, loss = 0.19057446718215942
iteration 121, loss = 0.11262195557355881
iteration 122, loss = 0.15732412040233612
iteration 123, loss = 0.38805192708969116
iteration 124, loss = 0.3840157985687256
iteration 125, loss = 0.14225643873214722
iteration 126, loss = 0.08582576364278793
iteration 127, loss = 0.17684587836265564
iteration 128, loss = 0.2696171998977661
iteration 129, loss = 0.23898698389530182
iteration 130, loss = 0.2857637107372284
iteration 131, loss = 0.14817149937152863
iteration 132, loss = 0.46260789036750793
iteration 133, loss = 0.18762275576591492
iteration 134, loss = 0.29235804080963135
iteration 135, loss = 0.4555348753929138
iteration 136, loss = 0.2450137883424759
iteration 137, loss = 0.4752138555049896
iteration 138, loss = 0.2805188000202179
iteration 139, loss = 0.38017237186431885
iteration 140, loss = 0.37326300144195557
iteration 141, loss = 0.3110935688018799
iteration 142, loss = 0.27893394231796265
iteration 143, loss = 0.3329751789569855
iteration 144, loss = 0.09525039047002792
iteration 145, loss = 0.12835362553596497
iteration 146, loss = 0.10047715902328491
iteration 147, loss = 0.6685451865196228
iteration 148, loss = 0.21430382132530212
iteration 149, loss = 0.578992486000061
iteration 150, loss = 0.33516207337379456
iteration 151, loss = 0.3568233549594879
iteration 152, loss = 0.1753690242767334
iteration 153, loss = 0.18533264100551605
iteration 154, loss = 0.09844429790973663
iteration 155, loss = 0.11637160927057266
iteration 156, loss = 0.33243080973625183
iteration 157, loss = 0.36612290143966675
iteration 158, loss = 0.3514098823070526
iteration 159, loss = 0.21658855676651
iteration 160, loss = 0.3787989020347595
iteration 161, loss = 0.25104111433029175
iteration 162, loss = 0.04738443344831467
iteration 163, loss = 0.3366408050060272
iteration 164, loss = 0.3508382737636566
iteration 165, loss = 0.19336189329624176
iteration 166, loss = 0.07496774196624756
iteration 167, loss = 0.20414911210536957
iteration 168, loss = 0.11386089771986008
iteration 169, loss = 0.1630970537662506
iteration 170, loss = 0.3918466567993164
iteration 171, loss = 0.23158100247383118
iteration 172, loss = 0.26748836040496826
iteration 173, loss = 0.330946683883667
iteration 174, loss = 0.30879342555999756
iteration 175, loss = 0.3683166801929474
iteration 176, loss = 0.1829872578382492
iteration 177, loss = 0.1686771959066391
iteration 178, loss = 0.5983782410621643
iteration 179, loss = 0.12338342517614365
iteration 180, loss = 0.3214540481567383
iteration 181, loss = 0.041661836206912994
iteration 182, loss = 0.3016054034233093
iteration 183, loss = 0.4769805669784546
iteration 184, loss = 0.3100719749927521
iteration 185, loss = 0.1822049915790558
iteration 186, loss = 0.137889102101326
iteration 187, loss = 0.37406739592552185
iteration 188, loss = 0.3737107515335083
iteration 189, loss = 0.15451490879058838
iteration 190, loss = 0.3965202867984772
iteration 191, loss = 0.0914253294467926
iteration 192, loss = 0.35656553506851196
iteration 193, loss = 0.23317719995975494
iteration 194, loss = 0.27240872383117676
iteration 195, loss = 0.27719607949256897
iteration 196, loss = 0.33871155977249146
iteration 197, loss = 0.31057313084602356
iteration 198, loss = 0.25500720739364624
iteration 199, loss = 0.2495993822813034
iteration 200, loss = 0.31071773171424866
iteration 201, loss = 0.1395246833562851
iteration 202, loss = 0.4116344451904297
iteration 203, loss = 0.27661988139152527
iteration 204, loss = 0.41760575771331787
iteration 205, loss = 0.2370847463607788
iteration 206, loss = 0.1843414157629013
iteration 207, loss = 0.4100871682167053
iteration 208, loss = 0.22291262447834015
iteration 209, loss = 0.17897281050682068
iteration 210, loss = 0.169160395860672
iteration 211, loss = 0.21223324537277222
iteration 212, loss = 0.32837557792663574
iteration 213, loss = 0.5655046701431274
iteration 214, loss = 0.07547452300786972
iteration 215, loss = 0.4714322090148926
iteration 216, loss = 0.07482829689979553
iteration 217, loss = 0.19829103350639343
iteration 218, loss = 0.16716256737709045
iteration 219, loss = 0.1600363552570343
iteration 220, loss = 0.15656544268131256
iteration 221, loss = 0.051215413957834244
iteration 222, loss = 0.12435418367385864
iteration 223, loss = 0.11535071581602097
iteration 224, loss = 0.2511409521102905
iteration 225, loss = 0.1582915484905243
iteration 226, loss = 0.15349800884723663
iteration 227, loss = 0.19198580086231232
iteration 228, loss = 0.29259639978408813
iteration 229, loss = 0.1680586338043213
iteration 230, loss = 0.3821752071380615
iteration 231, loss = 0.2641479969024658
iteration 232, loss = 0.12788018584251404
iteration 233, loss = 0.10755251348018646
iteration 234, loss = 0.19864130020141602
iteration 235, loss = 0.2778071165084839
iteration 236, loss = 0.1134844645857811
iteration 237, loss = 0.1411273330450058
iteration 238, loss = 0.33592522144317627
iteration 239, loss = 0.318615198135376
iteration 240, loss = 0.22683504223823547
iteration 241, loss = 0.0934462696313858
iteration 242, loss = 0.45257455110549927
iteration 243, loss = 0.3453109860420227
iteration 244, loss = 0.156358003616333
iteration 245, loss = 0.11823610961437225
iteration 246, loss = 0.274976909160614
iteration 247, loss = 0.16289663314819336
iteration 248, loss = 0.36484861373901367
iteration 249, loss = 0.20805993676185608
iteration 250, loss = 0.1915605068206787
iteration 251, loss = 0.5201123356819153
iteration 252, loss = 0.17255833745002747
iteration 253, loss = 0.27559295296669006
iteration 254, loss = 0.11011017858982086
iteration 255, loss = 0.20480769872665405
iteration 256, loss = 0.23437394201755524
iteration 257, loss = 0.1970934271812439
iteration 258, loss = 0.39272522926330566
iteration 259, loss = 0.2654400169849396
iteration 260, loss = 0.06193975359201431
iteration 261, loss = 0.04657436162233353
iteration 262, loss = 0.11444266140460968
iteration 263, loss = 0.16495148837566376
iteration 264, loss = 0.2702326774597168
iteration 265, loss = 0.1044863685965538
iteration 266, loss = 0.15891100466251373
iteration 267, loss = 0.06076010316610336
iteration 268, loss = 0.16343533992767334
iteration 269, loss = 0.2984561622142792
iteration 270, loss = 0.12482523173093796
iteration 271, loss = 0.1489925980567932
iteration 272, loss = 0.17474523186683655
iteration 273, loss = 0.0763368159532547
iteration 274, loss = 0.21970492601394653
iteration 275, loss = 0.29537248611450195
iteration 276, loss = 0.1783539354801178
iteration 277, loss = 0.03195922076702118
iteration 278, loss = 0.37488335371017456
iteration 279, loss = 0.15975242853164673
iteration 280, loss = 0.27522459626197815
iteration 281, loss = 0.10244132578372955
iteration 282, loss = 0.03993823379278183
iteration 283, loss = 0.20319104194641113
iteration 284, loss = 0.20058467984199524
iteration 285, loss = 0.2006816416978836
iteration 286, loss = 0.32097500562667847
iteration 287, loss = 0.19015562534332275
iteration 288, loss = 0.10915718972682953
iteration 289, loss = 0.19767330586910248
iteration 290, loss = 0.35139548778533936
iteration 291, loss = 0.2054642289876938
iteration 292, loss = 0.29151055216789246
iteration 293, loss = 0.14773549139499664
iteration 294, loss = 0.2553076148033142
iteration 295, loss = 0.17350956797599792
iteration 296, loss = 0.11930667608976364
iteration 297, loss = 0.19745926558971405
iteration 298, loss = 0.1680513620376587
iteration 299, loss = 0.1345376968383789
iteration 0, loss = 0.23130258917808533
iteration 1, loss = 0.16171467304229736
iteration 2, loss = 0.38019299507141113
iteration 3, loss = 0.1549181342124939
iteration 4, loss = 0.20750859379768372
iteration 5, loss = 0.06464217603206635
iteration 6, loss = 0.2823374569416046
iteration 7, loss = 0.5325209498405457
iteration 8, loss = 0.1462697982788086
iteration 9, loss = 0.15154674649238586
iteration 10, loss = 0.16210408508777618
iteration 11, loss = 0.09428967535495758
iteration 12, loss = 0.07348062098026276
iteration 13, loss = 0.4670158922672272
iteration 14, loss = 0.28271591663360596
iteration 15, loss = 0.3183515667915344
iteration 16, loss = 0.17612501978874207
iteration 17, loss = 0.24069714546203613
iteration 18, loss = 0.1815703809261322
iteration 19, loss = 0.34461137652397156
iteration 20, loss = 0.23537151515483856
iteration 21, loss = 0.12067783623933792
iteration 22, loss = 0.1632232666015625
iteration 23, loss = 0.08295035362243652
iteration 24, loss = 0.24260811507701874
iteration 25, loss = 0.6608520746231079
iteration 26, loss = 0.20407520234584808
iteration 27, loss = 0.2299608439207077
iteration 28, loss = 0.047616638243198395
iteration 29, loss = 0.4090464413166046
iteration 30, loss = 0.5201138257980347
iteration 31, loss = 0.26978689432144165
iteration 32, loss = 0.08534380793571472
iteration 33, loss = 0.3931593596935272
iteration 34, loss = 0.19143763184547424
iteration 35, loss = 0.37091803550720215
iteration 36, loss = 0.10974637418985367
iteration 37, loss = 0.3193821907043457
iteration 38, loss = 0.1831607222557068
iteration 39, loss = 0.1801290065050125
iteration 40, loss = 0.11589697003364563
iteration 41, loss = 0.08328863233327866
iteration 42, loss = 0.19980822503566742
iteration 43, loss = 0.2709823548793793
iteration 44, loss = 0.1262296438217163
iteration 45, loss = 0.1317969262599945
iteration 46, loss = 0.10455937683582306
iteration 47, loss = 0.2414264976978302
iteration 48, loss = 0.23788386583328247
iteration 49, loss = 0.08284071087837219
iteration 50, loss = 0.09783661365509033
iteration 51, loss = 0.2071363925933838
iteration 52, loss = 0.2839702367782593
iteration 53, loss = 0.15342065691947937
iteration 54, loss = 0.30587631464004517
iteration 55, loss = 0.19995862245559692
iteration 56, loss = 0.16760069131851196
iteration 57, loss = 0.5617762207984924
iteration 58, loss = 0.22504974901676178
iteration 59, loss = 0.2055104672908783
iteration 60, loss = 0.36039796471595764
iteration 61, loss = 0.0834030956029892
iteration 62, loss = 0.2154606133699417
iteration 63, loss = 0.17464420199394226
iteration 64, loss = 0.46775367856025696
iteration 65, loss = 0.3281591534614563
iteration 66, loss = 0.4009753167629242
iteration 67, loss = 0.35294288396835327
iteration 68, loss = 0.3513141870498657
iteration 69, loss = 0.22746504843235016
iteration 70, loss = 0.1963518261909485
iteration 71, loss = 0.4274020195007324
iteration 72, loss = 0.2096664011478424
iteration 73, loss = 0.3089388906955719
iteration 74, loss = 0.15774241089820862
iteration 75, loss = 0.02608592063188553
iteration 76, loss = 0.023172125220298767
iteration 77, loss = 0.02899901196360588
iteration 78, loss = 0.029242845252156258
iteration 79, loss = 0.5433163642883301
iteration 80, loss = 0.30551832914352417
iteration 81, loss = 0.047073833644390106
iteration 82, loss = 0.18833281099796295
iteration 83, loss = 0.04961875453591347
iteration 84, loss = 0.12405876070261002
iteration 85, loss = 0.4006045460700989
iteration 86, loss = 0.1876102089881897
iteration 87, loss = 0.25824102759361267
iteration 88, loss = 0.3260464668273926
iteration 89, loss = 0.1821804791688919
iteration 90, loss = 0.17357604205608368
iteration 91, loss = 0.3082641363143921
iteration 92, loss = 0.13486656546592712
iteration 93, loss = 0.2511979937553406
iteration 94, loss = 0.28853118419647217
iteration 95, loss = 0.10697707533836365
iteration 96, loss = 0.15392827987670898
iteration 97, loss = 0.25728532671928406
iteration 98, loss = 0.2000841498374939
iteration 99, loss = 0.15629330277442932
iteration 100, loss = 0.2665763795375824
iteration 101, loss = 0.09644963592290878
iteration 102, loss = 0.37995046377182007
iteration 103, loss = 0.03228181600570679
iteration 104, loss = 0.41550523042678833
iteration 105, loss = 0.19372281432151794
iteration 106, loss = 0.10772647708654404
iteration 107, loss = 0.37178122997283936
iteration 108, loss = 0.1691797971725464
iteration 109, loss = 0.1785312443971634
iteration 110, loss = 0.19845613837242126
iteration 111, loss = 0.23661309480667114
iteration 112, loss = 0.327147901058197
iteration 113, loss = 0.35305115580558777
iteration 114, loss = 0.16796118021011353
iteration 115, loss = 0.2420267015695572
iteration 116, loss = 0.07229019701480865
iteration 117, loss = 0.1857229769229889
iteration 118, loss = 0.15960225462913513
iteration 119, loss = 0.19890037178993225
iteration 120, loss = 0.17125587165355682
iteration 121, loss = 0.026921473443508148
iteration 122, loss = 0.27554988861083984
iteration 123, loss = 0.06786374747753143
iteration 124, loss = 0.11214685440063477
iteration 125, loss = 0.12491147965192795
iteration 126, loss = 0.2060905247926712
iteration 127, loss = 0.29440292716026306
iteration 128, loss = 0.32639890909194946
iteration 129, loss = 0.3170601725578308
iteration 130, loss = 0.4764340817928314
iteration 131, loss = 0.16444706916809082
iteration 132, loss = 0.27604034543037415
iteration 133, loss = 0.12129516154527664
iteration 134, loss = 0.23840555548667908
iteration 135, loss = 0.05649852752685547
iteration 136, loss = 0.11019081622362137
iteration 137, loss = 0.5246380567550659
iteration 138, loss = 0.4906369149684906
iteration 139, loss = 0.08523473143577576
iteration 140, loss = 0.19265532493591309
iteration 141, loss = 0.31894397735595703
iteration 142, loss = 0.2179238200187683
iteration 143, loss = 0.03155846148729324
iteration 144, loss = 0.18964636325836182
iteration 145, loss = 0.21933704614639282
iteration 146, loss = 0.24027083814144135
iteration 147, loss = 0.2688746154308319
iteration 148, loss = 0.4435788094997406
iteration 149, loss = 0.12973646819591522
iteration 150, loss = 0.21670149266719818
iteration 151, loss = 0.1452031284570694
iteration 152, loss = 0.1457759141921997
iteration 153, loss = 0.21396955847740173
iteration 154, loss = 0.5748704075813293
iteration 155, loss = 0.33028364181518555
iteration 156, loss = 0.20042313635349274
iteration 157, loss = 0.16386231780052185
iteration 158, loss = 0.12795890867710114
iteration 159, loss = 0.08356030285358429
iteration 160, loss = 0.10919778048992157
iteration 161, loss = 0.1112656220793724
iteration 162, loss = 0.07866815477609634
iteration 163, loss = 0.3237098455429077
iteration 164, loss = 0.19788384437561035
iteration 165, loss = 0.025800669565796852
iteration 166, loss = 0.18553122878074646
iteration 167, loss = 0.1949506253004074
iteration 168, loss = 0.11702780425548553
iteration 169, loss = 0.13705459237098694
iteration 170, loss = 0.04114333540201187
iteration 171, loss = 0.3682849407196045
iteration 172, loss = 0.2583294212818146
iteration 173, loss = 0.25264251232147217
iteration 174, loss = 0.24891409277915955
iteration 175, loss = 0.15325239300727844
iteration 176, loss = 0.15017980337142944
iteration 177, loss = 0.10969656705856323
iteration 178, loss = 0.3813614547252655
iteration 179, loss = 0.3601755201816559
iteration 180, loss = 0.35834962129592896
iteration 181, loss = 0.524060845375061
iteration 182, loss = 0.18025174736976624
iteration 183, loss = 0.18487922847270966
iteration 184, loss = 0.2695768177509308
iteration 185, loss = 0.26114100217819214
iteration 186, loss = 0.3475700914859772
iteration 187, loss = 0.13399188220500946
iteration 188, loss = 0.3528190553188324
iteration 189, loss = 0.048407457768917084
iteration 190, loss = 0.1706242561340332
iteration 191, loss = 0.11598150432109833
iteration 192, loss = 0.265750914812088
iteration 193, loss = 0.1623576432466507
iteration 194, loss = 0.20323573052883148
iteration 195, loss = 0.2214088886976242
iteration 196, loss = 0.03753669932484627
iteration 197, loss = 0.27539321780204773
iteration 198, loss = 0.23507846891880035
iteration 199, loss = 0.477853387594223
iteration 200, loss = 0.14473560452461243
iteration 201, loss = 0.27443304657936096
iteration 202, loss = 0.045649006962776184
iteration 203, loss = 0.23274925351142883
iteration 204, loss = 0.5846172571182251
iteration 205, loss = 0.5184109210968018
iteration 206, loss = 0.24801023304462433
iteration 207, loss = 0.4716816842556
iteration 208, loss = 0.3488788306713104
iteration 209, loss = 0.38540855050086975
iteration 210, loss = 0.021587898954749107
iteration 211, loss = 0.4485020935535431
iteration 212, loss = 0.5083927512168884
iteration 213, loss = 0.0989830493927002
iteration 214, loss = 0.28468310832977295
iteration 215, loss = 0.11946707963943481
iteration 216, loss = 0.29866382479667664
iteration 217, loss = 0.30497056245803833
iteration 218, loss = 0.2815011143684387
iteration 219, loss = 0.27364984154701233
iteration 220, loss = 0.20410090684890747
iteration 221, loss = 0.2834906578063965
iteration 222, loss = 0.3848963677883148
iteration 223, loss = 0.05145417898893356
iteration 224, loss = 0.3013826012611389
iteration 225, loss = 0.2103368639945984
iteration 226, loss = 0.21936781704425812
iteration 227, loss = 0.011783057823777199
iteration 228, loss = 0.0293261855840683
iteration 229, loss = 0.5034491419792175
iteration 230, loss = 0.21540531516075134
iteration 231, loss = 0.08431090414524078
iteration 232, loss = 0.17509062588214874
iteration 233, loss = 0.09605012089014053
iteration 234, loss = 0.15851926803588867
iteration 235, loss = 0.08342388272285461
iteration 236, loss = 0.17838335037231445
iteration 237, loss = 0.11363672465085983
iteration 238, loss = 0.06343354284763336
iteration 239, loss = 0.18180304765701294
iteration 240, loss = 0.4253125786781311
iteration 241, loss = 0.21825972199440002
iteration 242, loss = 0.050636399537324905
iteration 243, loss = 0.09522981196641922
iteration 244, loss = 0.1307138204574585
iteration 245, loss = 0.14544251561164856
iteration 246, loss = 0.09773367643356323
iteration 247, loss = 0.2169826775789261
iteration 248, loss = 0.3242446184158325
iteration 249, loss = 0.036362215876579285
iteration 250, loss = 0.10522125661373138
iteration 251, loss = 0.19849181175231934
iteration 252, loss = 0.33392155170440674
iteration 253, loss = 0.054910242557525635
iteration 254, loss = 0.26183053851127625
iteration 255, loss = 0.4031122922897339
iteration 256, loss = 0.02400279976427555
iteration 257, loss = 0.23003816604614258
iteration 258, loss = 0.3308248817920685
iteration 259, loss = 0.18475064635276794
iteration 260, loss = 0.1553114950656891
iteration 261, loss = 0.05938189849257469
iteration 262, loss = 0.46280550956726074
iteration 263, loss = 0.040694255381822586
iteration 264, loss = 0.2372862696647644
iteration 265, loss = 0.33215922117233276
iteration 266, loss = 0.19680309295654297
iteration 267, loss = 0.21022158861160278
iteration 268, loss = 0.15545077621936798
iteration 269, loss = 0.31850293278694153
iteration 270, loss = 0.17266927659511566
iteration 271, loss = 0.1841801106929779
iteration 272, loss = 0.03166115656495094
iteration 273, loss = 0.2075786292552948
iteration 274, loss = 0.07916470617055893
iteration 275, loss = 0.21078762412071228
iteration 276, loss = 0.20028972625732422
iteration 277, loss = 0.14877751469612122
iteration 278, loss = 0.22541862726211548
iteration 279, loss = 0.23257118463516235
iteration 280, loss = 0.35422801971435547
iteration 281, loss = 0.4708632826805115
iteration 282, loss = 0.38170403242111206
iteration 283, loss = 0.11013446748256683
iteration 284, loss = 0.1806485801935196
iteration 285, loss = 0.26515910029411316
iteration 286, loss = 0.230320543050766
iteration 287, loss = 0.31315547227859497
iteration 288, loss = 0.28923410177230835
iteration 289, loss = 0.2332068681716919
iteration 290, loss = 0.43766170740127563
iteration 291, loss = 0.2611699104309082
iteration 292, loss = 0.38240286707878113
iteration 293, loss = 0.1878524273633957
iteration 294, loss = 0.047214262187480927
iteration 295, loss = 0.4211876094341278
iteration 296, loss = 0.45582225918769836
iteration 297, loss = 0.15458419919013977
iteration 298, loss = 0.30160829424858093
iteration 299, loss = 0.21404877305030823
iteration 0, loss = 0.23231227695941925
iteration 1, loss = 0.22833214700222015
iteration 2, loss = 0.44863858819007874
iteration 3, loss = 0.378337025642395
iteration 4, loss = 0.6250334978103638
iteration 5, loss = 0.3098592758178711
iteration 6, loss = 0.17829114198684692
iteration 7, loss = 0.25159165263175964
iteration 8, loss = 0.17361155152320862
iteration 9, loss = 0.2958739399909973
iteration 10, loss = 0.16975706815719604
iteration 11, loss = 0.2694774568080902
iteration 12, loss = 0.2000613808631897
iteration 13, loss = 0.09648644924163818
iteration 14, loss = 0.1218327134847641
iteration 15, loss = 0.3532961905002594
iteration 16, loss = 0.2698788642883301
iteration 17, loss = 0.07800350338220596
iteration 18, loss = 0.14346058666706085
iteration 19, loss = 0.28044840693473816
iteration 20, loss = 0.2879602909088135
iteration 21, loss = 0.18761010468006134
iteration 22, loss = 0.3283214867115021
iteration 23, loss = 0.09382226318120956
iteration 24, loss = 0.8298777341842651
iteration 25, loss = 0.053982578217983246
iteration 26, loss = 0.26976391673088074
iteration 27, loss = 0.21206462383270264
iteration 28, loss = 0.31829604506492615
iteration 29, loss = 0.10754565894603729
iteration 30, loss = 0.1895940601825714
iteration 31, loss = 0.27764445543289185
iteration 32, loss = 0.2122022956609726
iteration 33, loss = 0.28835928440093994
iteration 34, loss = 0.2214197814464569
iteration 35, loss = 0.08264800906181335
iteration 36, loss = 0.13913007080554962
iteration 37, loss = 0.1401441991329193
iteration 38, loss = 0.10400393605232239
iteration 39, loss = 0.5277155637741089
iteration 40, loss = 0.37891945242881775
iteration 41, loss = 0.17199435830116272
iteration 42, loss = 0.147385373711586
iteration 43, loss = 0.10129544883966446
iteration 44, loss = 0.11397537589073181
iteration 45, loss = 0.10621404647827148
iteration 46, loss = 0.27725470066070557
iteration 47, loss = 0.055014677345752716
iteration 48, loss = 0.1395305097103119
iteration 49, loss = 0.26575154066085815
iteration 50, loss = 0.2860398292541504
iteration 51, loss = 0.08798396587371826
iteration 52, loss = 0.03713894262909889
iteration 53, loss = 0.07134748995304108
iteration 54, loss = 0.14174030721187592
iteration 55, loss = 0.368210107088089
iteration 56, loss = 0.27140021324157715
iteration 57, loss = 0.1756097674369812
iteration 58, loss = 0.08291943371295929
iteration 59, loss = 0.23715075850486755
iteration 60, loss = 0.3542932868003845
iteration 61, loss = 0.21063914895057678
iteration 62, loss = 0.13824987411499023
iteration 63, loss = 0.11355088651180267
iteration 64, loss = 0.07453261315822601
iteration 65, loss = 0.15615436434745789
iteration 66, loss = 0.32879936695098877
iteration 67, loss = 0.3838757574558258
iteration 68, loss = 0.1950078308582306
iteration 69, loss = 0.11130395531654358
iteration 70, loss = 0.04359541833400726
iteration 71, loss = 0.20746973156929016
iteration 72, loss = 0.23535895347595215
iteration 73, loss = 0.24518397450447083
iteration 74, loss = 0.25673553347587585
iteration 75, loss = 0.17984014749526978
iteration 76, loss = 0.166315495967865
iteration 77, loss = 0.2790338695049286
iteration 78, loss = 0.31185731291770935
iteration 79, loss = 0.14879059791564941
iteration 80, loss = 0.317857563495636
iteration 81, loss = 0.11325358599424362
iteration 82, loss = 0.18470659852027893
iteration 83, loss = 0.12419915944337845
iteration 84, loss = 0.54890376329422
iteration 85, loss = 0.17323455214500427
iteration 86, loss = 0.2170340120792389
iteration 87, loss = 0.2630053758621216
iteration 88, loss = 0.33167773485183716
iteration 89, loss = 0.15132710337638855
iteration 90, loss = 0.0868009477853775
iteration 91, loss = 0.15310218930244446
iteration 92, loss = 0.5192829370498657
iteration 93, loss = 0.09126249700784683
iteration 94, loss = 0.08924116939306259
iteration 95, loss = 0.13854491710662842
iteration 96, loss = 0.06154546141624451
iteration 97, loss = 0.2639891505241394
iteration 98, loss = 0.12098870426416397
iteration 99, loss = 0.3337901532649994
iteration 100, loss = 0.15857112407684326
iteration 101, loss = 0.25326281785964966
iteration 102, loss = 0.24631096422672272
iteration 103, loss = 0.1477764993906021
iteration 104, loss = 0.3145174980163574
iteration 105, loss = 0.4028361439704895
iteration 106, loss = 0.38697299361228943
iteration 107, loss = 0.2001500129699707
iteration 108, loss = 0.15094594657421112
iteration 109, loss = 0.23525939881801605
iteration 110, loss = 0.22505779564380646
iteration 111, loss = 0.1899103820323944
iteration 112, loss = 0.13987018167972565
iteration 113, loss = 0.22453388571739197
iteration 114, loss = 0.1442139446735382
iteration 115, loss = 0.18135815858840942
iteration 116, loss = 0.07751660794019699
iteration 117, loss = 0.245257169008255
iteration 118, loss = 0.25847819447517395
iteration 119, loss = 0.3222600519657135
iteration 120, loss = 0.2699146568775177
iteration 121, loss = 0.23161110281944275
iteration 122, loss = 0.3852716088294983
iteration 123, loss = 0.3062645494937897
iteration 124, loss = 0.23816923797130585
iteration 125, loss = 0.10564588010311127
iteration 126, loss = 0.20514382421970367
iteration 127, loss = 0.18474502861499786
iteration 128, loss = 0.13836950063705444
iteration 129, loss = 0.09772542864084244
iteration 130, loss = 0.3094886541366577
iteration 131, loss = 0.18814688920974731
iteration 132, loss = 0.22647157311439514
iteration 133, loss = 0.1586434543132782
iteration 134, loss = 0.16179503500461578
iteration 135, loss = 0.22167663276195526
iteration 136, loss = 0.37494805455207825
iteration 137, loss = 0.09776487946510315
iteration 138, loss = 0.507172703742981
iteration 139, loss = 0.08894772827625275
iteration 140, loss = 0.14222291111946106
iteration 141, loss = 0.022031262516975403
iteration 142, loss = 0.0434856191277504
iteration 143, loss = 0.09885247051715851
iteration 144, loss = 0.1925111711025238
iteration 145, loss = 0.38785865902900696
iteration 146, loss = 0.018373876810073853
iteration 147, loss = 0.2623913288116455
iteration 148, loss = 0.16164258122444153
iteration 149, loss = 0.32981762290000916
iteration 150, loss = 0.03493504971265793
iteration 151, loss = 0.13136252760887146
iteration 152, loss = 0.12163157016038895
iteration 153, loss = 0.37013787031173706
iteration 154, loss = 0.2724461555480957
iteration 155, loss = 0.2201322317123413
iteration 156, loss = 0.2722116708755493
iteration 157, loss = 0.4682287871837616
iteration 158, loss = 0.31288203597068787
iteration 159, loss = 0.25710204243659973
iteration 160, loss = 0.4593753516674042
iteration 161, loss = 0.15029941499233246
iteration 162, loss = 0.0833410769701004
iteration 163, loss = 0.3213444948196411
iteration 164, loss = 0.1319846510887146
iteration 165, loss = 0.07139704376459122
iteration 166, loss = 0.22417569160461426
iteration 167, loss = 0.046082064509391785
iteration 168, loss = 0.1964893490076065
iteration 169, loss = 0.3152047395706177
iteration 170, loss = 0.25806307792663574
iteration 171, loss = 0.03914489597082138
iteration 172, loss = 0.1973116397857666
iteration 173, loss = 0.2924145758152008
iteration 174, loss = 0.19070325791835785
iteration 175, loss = 0.2216944396495819
iteration 176, loss = 0.29114091396331787
iteration 177, loss = 0.24882851541042328
iteration 178, loss = 0.2338038682937622
iteration 179, loss = 0.15798836946487427
iteration 180, loss = 0.1686476171016693
iteration 181, loss = 0.3386511504650116
iteration 182, loss = 0.3073890805244446
iteration 183, loss = 0.13205228745937347
iteration 184, loss = 0.14413496851921082
iteration 185, loss = 0.08405297249555588
iteration 186, loss = 0.22379636764526367
iteration 187, loss = 0.39871060848236084
iteration 188, loss = 0.16070440411567688
iteration 189, loss = 0.26171597838401794
iteration 190, loss = 0.42825645208358765
iteration 191, loss = 0.02644357644021511
iteration 192, loss = 0.3523976504802704
iteration 193, loss = 0.30235397815704346
iteration 194, loss = 0.12516696751117706
iteration 195, loss = 0.30415958166122437
iteration 196, loss = 0.07427382469177246
iteration 197, loss = 0.06756806373596191
iteration 198, loss = 0.3467493951320648
iteration 199, loss = 0.15117403864860535
iteration 200, loss = 0.14394411444664001
iteration 201, loss = 0.24944716691970825
iteration 202, loss = 0.3608166575431824
iteration 203, loss = 0.13552801311016083
iteration 204, loss = 0.14107806980609894
iteration 205, loss = 0.2276243418455124
iteration 206, loss = 0.1338225156068802
iteration 207, loss = 0.44233936071395874
iteration 208, loss = 0.05711419880390167
iteration 209, loss = 0.34961411356925964
iteration 210, loss = 0.3179895877838135
iteration 211, loss = 0.168136864900589
iteration 212, loss = 0.158186137676239
iteration 213, loss = 0.34564539790153503
iteration 214, loss = 0.25645869970321655
iteration 215, loss = 0.16758662462234497
iteration 216, loss = 0.2013639509677887
iteration 217, loss = 0.11081135272979736
iteration 218, loss = 0.12171199917793274
iteration 219, loss = 0.11640103161334991
iteration 220, loss = 0.20403319597244263
iteration 221, loss = 0.15561342239379883
iteration 222, loss = 0.24708303809165955
iteration 223, loss = 0.200099378824234
iteration 224, loss = 0.11627817898988724
iteration 225, loss = 0.3814639151096344
iteration 226, loss = 0.25598087906837463
iteration 227, loss = 0.12250295281410217
iteration 228, loss = 0.3022216260433197
iteration 229, loss = 0.3684665262699127
iteration 230, loss = 0.2672834098339081
iteration 231, loss = 0.2435644268989563
iteration 232, loss = 0.1912360042333603
iteration 233, loss = 0.08894719183444977
iteration 234, loss = 0.23197215795516968
iteration 235, loss = 0.3070255219936371
iteration 236, loss = 0.11232350021600723
iteration 237, loss = 0.1326950341463089
iteration 238, loss = 0.1950601190328598
iteration 239, loss = 0.17541654407978058
iteration 240, loss = 0.17410865426063538
iteration 241, loss = 0.17712365090847015
iteration 242, loss = 0.052396491169929504
iteration 243, loss = 0.25654029846191406
iteration 244, loss = 0.42832979559898376
iteration 245, loss = 0.27176016569137573
iteration 246, loss = 0.22584329545497894
iteration 247, loss = 0.21952958405017853
iteration 248, loss = 0.30848008394241333
iteration 249, loss = 0.1038435772061348
iteration 250, loss = 0.29250696301460266
iteration 251, loss = 0.19901134073734283
iteration 252, loss = 0.22594976425170898
iteration 253, loss = 0.1291111707687378
iteration 254, loss = 0.25425419211387634
iteration 255, loss = 0.13331779837608337
iteration 256, loss = 0.05551382899284363
iteration 257, loss = 0.07348166406154633
iteration 258, loss = 0.16978827118873596
iteration 259, loss = 0.23320037126541138
iteration 260, loss = 0.2392929196357727
iteration 261, loss = 0.08085039258003235
iteration 262, loss = 0.32384324073791504
iteration 263, loss = 0.10600145906209946
iteration 264, loss = 0.22741584479808807
iteration 265, loss = 0.2952086329460144
iteration 266, loss = 0.3147355020046234
iteration 267, loss = 0.20829007029533386
iteration 268, loss = 0.2940337359905243
iteration 269, loss = 0.2500825822353363
iteration 270, loss = 0.17524123191833496
iteration 271, loss = 0.06334787607192993
iteration 272, loss = 0.16037912666797638
iteration 273, loss = 0.3208463490009308
iteration 274, loss = 0.28530749678611755
iteration 275, loss = 0.01663687638938427
iteration 276, loss = 0.3568698763847351
iteration 277, loss = 0.2109849452972412
iteration 278, loss = 0.30114877223968506
iteration 279, loss = 0.06206054240465164
iteration 280, loss = 0.26917681097984314
iteration 281, loss = 0.07238969206809998
iteration 282, loss = 0.2025807797908783
iteration 283, loss = 0.1800975501537323
iteration 284, loss = 0.13951212167739868
iteration 285, loss = 0.04964342713356018
iteration 286, loss = 0.2807035744190216
iteration 287, loss = 0.2714770436286926
iteration 288, loss = 0.0630829781293869
iteration 289, loss = 0.0784517154097557
iteration 290, loss = 0.20394310355186462
iteration 291, loss = 0.062313176691532135
iteration 292, loss = 0.13617192208766937
iteration 293, loss = 0.3549061715602875
iteration 294, loss = 0.09930336475372314
iteration 295, loss = 0.293903648853302
iteration 296, loss = 0.17340095341205597
iteration 297, loss = 0.24168674647808075
iteration 298, loss = 0.16171769797801971
iteration 299, loss = 0.30904167890548706
iteration 0, loss = 0.2704956531524658
iteration 1, loss = 0.2868681848049164
iteration 2, loss = 0.18835881352424622
iteration 3, loss = 0.322790265083313
iteration 4, loss = 0.2576555907726288
iteration 5, loss = 0.22601675987243652
iteration 6, loss = 0.2364557832479477
iteration 7, loss = 0.21717330813407898
iteration 8, loss = 0.18275490403175354
iteration 9, loss = 0.2958148717880249
iteration 10, loss = 0.24884024262428284
iteration 11, loss = 0.016169218346476555
iteration 12, loss = 0.21595613658428192
iteration 13, loss = 0.23713603615760803
iteration 14, loss = 0.21032193303108215
iteration 15, loss = 0.223831906914711
iteration 16, loss = 0.21602998673915863
iteration 17, loss = 0.05981713533401489
iteration 18, loss = 0.23484663665294647
iteration 19, loss = 0.2361188530921936
iteration 20, loss = 0.2548758387565613
iteration 21, loss = 0.40737345814704895
iteration 22, loss = 0.2931339144706726
iteration 23, loss = 0.11639875918626785
iteration 24, loss = 0.0925508588552475
iteration 25, loss = 0.18054437637329102
iteration 26, loss = 0.26833367347717285
iteration 27, loss = 0.17740656435489655
iteration 28, loss = 0.289699912071228
iteration 29, loss = 0.18890979886054993
iteration 30, loss = 0.20276358723640442
iteration 31, loss = 0.30974894762039185
iteration 32, loss = 0.03224588558077812
iteration 33, loss = 0.31304338574409485
iteration 34, loss = 0.1424730122089386
iteration 35, loss = 0.20555159449577332
iteration 36, loss = 0.19700996577739716
iteration 37, loss = 0.20012392103672028
iteration 38, loss = 0.3055448830127716
iteration 39, loss = 0.1693764328956604
iteration 40, loss = 0.17840099334716797
iteration 41, loss = 0.09848673641681671
iteration 42, loss = 0.02444125898182392
iteration 43, loss = 0.6017250418663025
iteration 44, loss = 0.3633749186992645
iteration 45, loss = 0.23421500623226166
iteration 46, loss = 0.11566520482301712
iteration 47, loss = 0.6081224083900452
iteration 48, loss = 0.23691760003566742
iteration 49, loss = 0.44647789001464844
iteration 50, loss = 0.6253529191017151
iteration 51, loss = 0.49273017048835754
iteration 52, loss = 0.28859394788742065
iteration 53, loss = 0.13092318177223206
iteration 54, loss = 0.13439106941223145
iteration 55, loss = 0.007892021909356117
iteration 56, loss = 0.003916767425835133
iteration 57, loss = 0.3415255546569824
iteration 58, loss = 0.6412168145179749
iteration 59, loss = 0.23738592863082886
iteration 60, loss = 0.3085305094718933
iteration 61, loss = 0.24136368930339813
iteration 62, loss = 0.32682332396507263
iteration 63, loss = 0.13394393026828766
iteration 64, loss = 0.18482817709445953
iteration 65, loss = 0.27142223715782166
iteration 66, loss = 0.22848913073539734
iteration 67, loss = 0.1539028286933899
iteration 68, loss = 0.2921496629714966
iteration 69, loss = 0.0749165490269661
iteration 70, loss = 0.19204175472259521
iteration 71, loss = 0.23718133568763733
iteration 72, loss = 0.15452991425991058
iteration 73, loss = 0.2496345043182373
iteration 74, loss = 0.29771602153778076
iteration 75, loss = 0.2975478768348694
iteration 76, loss = 0.1566673368215561
iteration 77, loss = 0.21085748076438904
iteration 78, loss = 0.14205655455589294
iteration 79, loss = 0.07008494436740875
iteration 80, loss = 0.18043780326843262
iteration 81, loss = 0.42472419142723083
iteration 82, loss = 0.1312200427055359
iteration 83, loss = 0.05598662421107292
iteration 84, loss = 0.17984597384929657
iteration 85, loss = 0.22404493391513824
iteration 86, loss = 0.16744807362556458
iteration 87, loss = 0.552254319190979
iteration 88, loss = 0.07159462571144104
iteration 89, loss = 0.42519599199295044
iteration 90, loss = 0.34841105341911316
iteration 91, loss = 0.052280575037002563
iteration 92, loss = 0.33156707882881165
iteration 93, loss = 0.13400399684906006
iteration 94, loss = 0.2548137307167053
iteration 95, loss = 0.3383272886276245
iteration 96, loss = 0.4183090329170227
iteration 97, loss = 0.31039220094680786
iteration 98, loss = 0.1829383820295334
iteration 99, loss = 0.038961783051490784
iteration 100, loss = 0.14538003504276276
iteration 101, loss = 0.1888085901737213
iteration 102, loss = 0.29248011112213135
iteration 103, loss = 0.2600034475326538
iteration 104, loss = 0.22767150402069092
iteration 105, loss = 0.1958073079586029
iteration 106, loss = 0.30874475836753845
iteration 107, loss = 0.2025078982114792
iteration 108, loss = 0.1997401863336563
iteration 109, loss = 0.10466980934143066
iteration 110, loss = 0.1841340959072113
iteration 111, loss = 0.39320510625839233
iteration 112, loss = 0.34883052110671997
iteration 113, loss = 0.1130545511841774
iteration 114, loss = 0.09266670048236847
iteration 115, loss = 0.3314829468727112
iteration 116, loss = 0.5726402401924133
iteration 117, loss = 0.32226428389549255
iteration 118, loss = 0.15194764733314514
iteration 119, loss = 0.26317131519317627
iteration 120, loss = 0.18258073925971985
iteration 121, loss = 0.30002540349960327
iteration 122, loss = 0.1604311317205429
iteration 123, loss = 0.113594651222229
iteration 124, loss = 0.33852434158325195
iteration 125, loss = 0.147882342338562
iteration 126, loss = 0.3128918409347534
iteration 127, loss = 0.19644933938980103
iteration 128, loss = 0.12175549566745758
iteration 129, loss = 0.016906263306736946
iteration 130, loss = 0.034368425607681274
iteration 131, loss = 0.23547744750976562
iteration 132, loss = 0.19420018792152405
iteration 133, loss = 0.08094802498817444
iteration 134, loss = 0.21223488450050354
iteration 135, loss = 0.4830563962459564
iteration 136, loss = 0.15450766682624817
iteration 137, loss = 0.2649158239364624
iteration 138, loss = 0.1870405077934265
iteration 139, loss = 0.34612908959388733
iteration 140, loss = 0.21518003940582275
iteration 141, loss = 0.16161516308784485
iteration 142, loss = 0.2433566451072693
iteration 143, loss = 0.3654710352420807
iteration 144, loss = 0.2129228115081787
iteration 145, loss = 0.1787516474723816
iteration 146, loss = 0.07742826640605927
iteration 147, loss = 0.22925633192062378
iteration 148, loss = 0.2457234263420105
iteration 149, loss = 0.11590465903282166
iteration 150, loss = 0.0792984664440155
iteration 151, loss = 0.1524253487586975
iteration 152, loss = 0.09964262694120407
iteration 153, loss = 0.24867719411849976
iteration 154, loss = 0.24468496441841125
iteration 155, loss = 0.09481014311313629
iteration 156, loss = 0.14183074235916138
iteration 157, loss = 0.17873471975326538
iteration 158, loss = 0.06808198243379593
iteration 159, loss = 0.2407330870628357
iteration 160, loss = 0.20577841997146606
iteration 161, loss = 0.1196010410785675
iteration 162, loss = 0.1887957900762558
iteration 163, loss = 0.1182158812880516
iteration 164, loss = 0.15566697716712952
iteration 165, loss = 0.07472597062587738
iteration 166, loss = 0.16946634650230408
iteration 167, loss = 0.2481059730052948
iteration 168, loss = 0.06755602359771729
iteration 169, loss = 0.2088647484779358
iteration 170, loss = 0.19679445028305054
iteration 171, loss = 0.1503559648990631
iteration 172, loss = 0.22877129912376404
iteration 173, loss = 0.5263859033584595
iteration 174, loss = 0.24794736504554749
iteration 175, loss = 0.361497700214386
iteration 176, loss = 0.15323759615421295
iteration 177, loss = 0.3938452899456024
iteration 178, loss = 0.27183860540390015
iteration 179, loss = 0.15299825370311737
iteration 180, loss = 0.21075257658958435
iteration 181, loss = 0.3024945855140686
iteration 182, loss = 0.05877278000116348
iteration 183, loss = 0.19913801550865173
iteration 184, loss = 0.31553423404693604
iteration 185, loss = 0.5723426938056946
iteration 186, loss = 0.4234071373939514
iteration 187, loss = 0.19064292311668396
iteration 188, loss = 0.4792710542678833
iteration 189, loss = 0.15530432760715485
iteration 190, loss = 0.13553419709205627
iteration 191, loss = 0.1164444237947464
iteration 192, loss = 0.13014240562915802
iteration 193, loss = 0.3461078405380249
iteration 194, loss = 0.2915661633014679
iteration 195, loss = 0.07118827104568481
iteration 196, loss = 0.21575193107128143
iteration 197, loss = 0.2127659171819687
iteration 198, loss = 0.07695385813713074
iteration 199, loss = 0.20787884294986725
iteration 200, loss = 0.23581215739250183
iteration 201, loss = 0.3970490097999573
iteration 202, loss = 0.3956705927848816
iteration 203, loss = 0.6725940108299255
iteration 204, loss = 0.09569275379180908
iteration 205, loss = 0.04953783005475998
iteration 206, loss = 0.057379692792892456
iteration 207, loss = 0.5149072408676147
iteration 208, loss = 0.27364757657051086
iteration 209, loss = 0.32509732246398926
iteration 210, loss = 0.1938832551240921
iteration 211, loss = 0.15147441625595093
iteration 212, loss = 0.16249629855155945
iteration 213, loss = 0.10799404978752136
iteration 214, loss = 0.263592928647995
iteration 215, loss = 0.1287272721529007
iteration 216, loss = 0.0074743288569152355
iteration 217, loss = 0.46200302243232727
iteration 218, loss = 0.26659440994262695
iteration 219, loss = 0.5641443729400635
iteration 220, loss = 0.2596468925476074
iteration 221, loss = 0.31739339232444763
iteration 222, loss = 0.219278484582901
iteration 223, loss = 0.15422312915325165
iteration 224, loss = 0.39075803756713867
iteration 225, loss = 0.3451239764690399
iteration 226, loss = 0.17830419540405273
iteration 227, loss = 0.367971271276474
iteration 228, loss = 0.09008825570344925
iteration 229, loss = 0.09954635798931122
iteration 230, loss = 0.3166772723197937
iteration 231, loss = 0.140192911028862
iteration 232, loss = 0.06513812392950058
iteration 233, loss = 0.32322677969932556
iteration 234, loss = 0.3202293813228607
iteration 235, loss = 0.29731494188308716
iteration 236, loss = 0.1414969265460968
iteration 237, loss = 0.23458753526210785
iteration 238, loss = 0.1874406337738037
iteration 239, loss = 0.13581861555576324
iteration 240, loss = 0.15257614850997925
iteration 241, loss = 0.08644930273294449
iteration 242, loss = 0.39965683221817017
iteration 243, loss = 0.1358819603919983
iteration 244, loss = 0.2808048725128174
iteration 245, loss = 0.20821011066436768
iteration 246, loss = 0.16443952918052673
iteration 247, loss = 0.3110840916633606
iteration 248, loss = 0.19287145137786865
iteration 249, loss = 0.19523438811302185
iteration 250, loss = 0.16169437766075134
iteration 251, loss = 0.14119665324687958
iteration 252, loss = 0.23650816082954407
iteration 253, loss = 0.4776895344257355
iteration 254, loss = 0.4397365152835846
iteration 255, loss = 0.1529228687286377
iteration 256, loss = 0.08676259964704514
iteration 257, loss = 0.24501854181289673
iteration 258, loss = 0.25943300127983093
iteration 259, loss = 0.2080242931842804
iteration 260, loss = 0.08057152479887009
iteration 261, loss = 0.12413819879293442
iteration 262, loss = 0.048488616943359375
iteration 263, loss = 0.2838737666606903
iteration 264, loss = 0.09923708438873291
iteration 265, loss = 0.11619405448436737
iteration 266, loss = 0.41345715522766113
iteration 267, loss = 0.1004701629281044
iteration 268, loss = 0.27718570828437805
iteration 269, loss = 0.15140989422798157
iteration 270, loss = 0.2806105613708496
iteration 271, loss = 0.19270393252372742
iteration 272, loss = 0.15854644775390625
iteration 273, loss = 0.08870746195316315
iteration 274, loss = 0.1504259705543518
iteration 275, loss = 0.40099722146987915
iteration 276, loss = 0.2807466387748718
iteration 277, loss = 0.2851393222808838
iteration 278, loss = 0.11918468028306961
iteration 279, loss = 0.24558445811271667
iteration 280, loss = 0.21712619066238403
iteration 281, loss = 0.20846997201442719
iteration 282, loss = 0.2857407033443451
iteration 283, loss = 0.2970830798149109
iteration 284, loss = 0.19323110580444336
iteration 285, loss = 0.15155285596847534
iteration 286, loss = 0.35776039958000183
iteration 287, loss = 0.18229790031909943
iteration 288, loss = 0.28450414538383484
iteration 289, loss = 0.27098673582077026
iteration 290, loss = 0.07155012339353561
iteration 291, loss = 0.2842923104763031
iteration 292, loss = 0.2698083817958832
iteration 293, loss = 0.2000572681427002
iteration 294, loss = 0.017212126404047012
iteration 295, loss = 0.37212085723876953
iteration 296, loss = 0.38280487060546875
iteration 297, loss = 0.30922529101371765
iteration 298, loss = 0.14021161198616028
iteration 299, loss = 0.36201542615890503
iteration 0, loss = 0.17867368459701538
iteration 1, loss = 0.20654284954071045
iteration 2, loss = 0.22584211826324463
iteration 3, loss = 0.29959413409233093
iteration 4, loss = 0.1408500373363495
iteration 5, loss = 0.1421596109867096
iteration 6, loss = 0.23599812388420105
iteration 7, loss = 0.26270219683647156
iteration 8, loss = 0.191555917263031
iteration 9, loss = 0.1882246434688568
iteration 10, loss = 0.11400562524795532
iteration 11, loss = 0.27202117443084717
iteration 12, loss = 0.093095563352108
iteration 13, loss = 0.19009454548358917
iteration 14, loss = 0.21644903719425201
iteration 15, loss = 0.21709701418876648
iteration 16, loss = 0.14431267976760864
iteration 17, loss = 0.17603501677513123
iteration 18, loss = 0.169147789478302
iteration 19, loss = 0.1711939573287964
iteration 20, loss = 0.08393727988004684
iteration 21, loss = 0.139341801404953
iteration 22, loss = 0.2345329225063324
iteration 23, loss = 0.326715350151062
iteration 24, loss = 0.0575847253203392
iteration 25, loss = 0.044286828488111496
iteration 26, loss = 0.5727766156196594
iteration 27, loss = 0.09825953841209412
iteration 28, loss = 0.43254780769348145
iteration 29, loss = 0.24677586555480957
iteration 30, loss = 0.3205179274082184
iteration 31, loss = 0.33790895342826843
iteration 32, loss = 0.2835279703140259
iteration 33, loss = 0.2704126238822937
iteration 34, loss = 0.31459057331085205
iteration 35, loss = 0.320917546749115
iteration 36, loss = 0.1984807550907135
iteration 37, loss = 0.18153950572013855
iteration 38, loss = 0.2701813280582428
iteration 39, loss = 0.1330009400844574
iteration 40, loss = 0.05831543356180191
iteration 41, loss = 0.5008525848388672
iteration 42, loss = 0.22781556844711304
iteration 43, loss = 0.17863647639751434
iteration 44, loss = 0.10355786979198456
iteration 45, loss = 0.16820336878299713
iteration 46, loss = 0.09913251549005508
iteration 47, loss = 0.08935500681400299
iteration 48, loss = 0.3317795395851135
iteration 49, loss = 0.27151748538017273
iteration 50, loss = 0.18422919511795044
iteration 51, loss = 0.2193356454372406
iteration 52, loss = 0.21610642969608307
iteration 53, loss = 0.3508654236793518
iteration 54, loss = 0.19712844491004944
iteration 55, loss = 0.22981798648834229
iteration 56, loss = 0.06896188110113144
iteration 57, loss = 0.20741449296474457
iteration 58, loss = 0.3069596290588379
iteration 59, loss = 0.5842055678367615
iteration 60, loss = 0.0621667206287384
iteration 61, loss = 0.12237124145030975
iteration 62, loss = 0.15938201546669006
iteration 63, loss = 0.14430227875709534
iteration 64, loss = 0.21632559597492218
iteration 65, loss = 0.1858164221048355
iteration 66, loss = 0.46195536851882935
iteration 67, loss = 0.2054852992296219
iteration 68, loss = 0.08971160650253296
iteration 69, loss = 0.21879486739635468
iteration 70, loss = 0.2624744772911072
iteration 71, loss = 0.30527761578559875
iteration 72, loss = 0.06929299980401993
iteration 73, loss = 0.20702557265758514
iteration 74, loss = 0.19736433029174805
iteration 75, loss = 0.1003875881433487
iteration 76, loss = 0.16629159450531006
iteration 77, loss = 0.0534680150449276
iteration 78, loss = 0.36470723152160645
iteration 79, loss = 0.2192937284708023
iteration 80, loss = 0.4632663130760193
iteration 81, loss = 0.17073173820972443
iteration 82, loss = 0.06666573882102966
iteration 83, loss = 0.19731688499450684
iteration 84, loss = 0.1960059404373169
iteration 85, loss = 0.23344776034355164
iteration 86, loss = 0.10758064687252045
iteration 87, loss = 0.2851700484752655
iteration 88, loss = 0.11451151967048645
iteration 89, loss = 0.17247043550014496
iteration 90, loss = 0.17483413219451904
iteration 91, loss = 0.21218281984329224
iteration 92, loss = 0.06958147883415222
iteration 93, loss = 0.20088151097297668
iteration 94, loss = 0.1679091453552246
iteration 95, loss = 0.17475825548171997
iteration 96, loss = 0.08129727095365524
iteration 97, loss = 0.24899110198020935
iteration 98, loss = 0.21174606680870056
iteration 99, loss = 0.055392514914274216
iteration 100, loss = 0.1394326090812683
iteration 101, loss = 0.16304218769073486
iteration 102, loss = 0.1569269597530365
iteration 103, loss = 0.2881942689418793
iteration 104, loss = 0.08636359870433807
iteration 105, loss = 0.1712290197610855
iteration 106, loss = 0.3786771893501282
iteration 107, loss = 0.1916722059249878
iteration 108, loss = 0.13636089861392975
iteration 109, loss = 0.3825964331626892
iteration 110, loss = 0.24234846234321594
iteration 111, loss = 0.17420074343681335
iteration 112, loss = 0.2632349133491516
iteration 113, loss = 0.3192371129989624
iteration 114, loss = 0.2735185921192169
iteration 115, loss = 0.10204404592514038
iteration 116, loss = 0.2803695797920227
iteration 117, loss = 0.27896609902381897
iteration 118, loss = 0.339643657207489
iteration 119, loss = 0.2437082827091217
iteration 120, loss = 0.2879892885684967
iteration 121, loss = 0.1598530113697052
iteration 122, loss = 0.47583022713661194
iteration 123, loss = 0.4893665611743927
iteration 124, loss = 0.3647400736808777
iteration 125, loss = 0.10357241332530975
iteration 126, loss = 0.05248061940073967
iteration 127, loss = 0.08283039927482605
iteration 128, loss = 0.23773698508739471
iteration 129, loss = 0.10313555598258972
iteration 130, loss = 0.27065417170524597
iteration 131, loss = 0.1410011351108551
iteration 132, loss = 0.16823545098304749
iteration 133, loss = 0.15885590016841888
iteration 134, loss = 0.078481025993824
iteration 135, loss = 0.3316854238510132
iteration 136, loss = 0.3066527545452118
iteration 137, loss = 0.02550956793129444
iteration 138, loss = 0.27820736169815063
iteration 139, loss = 0.4041261076927185
iteration 140, loss = 0.16151706874370575
iteration 141, loss = 0.18181607127189636
iteration 142, loss = 0.1441752314567566
iteration 143, loss = 0.126360684633255
iteration 144, loss = 0.05775086581707001
iteration 145, loss = 0.20940439403057098
iteration 146, loss = 0.0334940031170845
iteration 147, loss = 0.09691693633794785
iteration 148, loss = 0.15380823612213135
iteration 149, loss = 0.43155843019485474
iteration 150, loss = 0.09849303960800171
iteration 151, loss = 0.057609692215919495
iteration 152, loss = 0.12064936757087708
iteration 153, loss = 0.1936042308807373
iteration 154, loss = 0.39005935192108154
iteration 155, loss = 0.18913842737674713
iteration 156, loss = 0.11280374228954315
iteration 157, loss = 0.20651616156101227
iteration 158, loss = 0.24603936076164246
iteration 159, loss = 0.11514323204755783
iteration 160, loss = 0.14603979885578156
iteration 161, loss = 0.5604537725448608
iteration 162, loss = 0.24185743927955627
iteration 163, loss = 0.16058900952339172
iteration 164, loss = 0.1391952931880951
iteration 165, loss = 0.017817756161093712
iteration 166, loss = 0.1563466638326645
iteration 167, loss = 0.09594777971506119
iteration 168, loss = 0.11087293922901154
iteration 169, loss = 0.1507602483034134
iteration 170, loss = 0.21545493602752686
iteration 171, loss = 0.19273541867733002
iteration 172, loss = 0.042086079716682434
iteration 173, loss = 0.30074411630630493
iteration 174, loss = 0.37434521317481995
iteration 175, loss = 0.23408380150794983
iteration 176, loss = 0.07566524296998978
iteration 177, loss = 0.11846919357776642
iteration 178, loss = 0.13589608669281006
iteration 179, loss = 0.2879212498664856
iteration 180, loss = 0.16764990985393524
iteration 181, loss = 0.0728854387998581
iteration 182, loss = 0.26982414722442627
iteration 183, loss = 0.06911656260490417
iteration 184, loss = 0.24288804829120636
iteration 185, loss = 0.1757659912109375
iteration 186, loss = 0.27519917488098145
iteration 187, loss = 0.005265784449875355
iteration 188, loss = 0.1276770830154419
iteration 189, loss = 0.12107671797275543
iteration 190, loss = 0.22065000236034393
iteration 191, loss = 0.13270911574363708
iteration 192, loss = 0.16152988374233246
iteration 193, loss = 0.10876695811748505
iteration 194, loss = 0.09025028347969055
iteration 195, loss = 0.3924122154712677
iteration 196, loss = 0.30248236656188965
iteration 197, loss = 0.13907404243946075
iteration 198, loss = 0.09174811840057373
iteration 199, loss = 0.20500902831554413
iteration 200, loss = 0.24033117294311523
iteration 201, loss = 0.16782718896865845
iteration 202, loss = 0.21120944619178772
iteration 203, loss = 0.16550199687480927
iteration 204, loss = 0.16822205483913422
iteration 205, loss = 0.05330919101834297
iteration 206, loss = 0.0645221620798111
iteration 207, loss = 0.17014651000499725
iteration 208, loss = 0.13411863148212433
iteration 209, loss = 0.09517399966716766
iteration 210, loss = 0.2494562864303589
iteration 211, loss = 0.2170858383178711
iteration 212, loss = 0.13718706369400024
iteration 213, loss = 0.08075332641601562
iteration 214, loss = 0.10676614195108414
iteration 215, loss = 0.023388424888253212
iteration 216, loss = 0.10776887834072113
iteration 217, loss = 0.272210031747818
iteration 218, loss = 0.48224568367004395
iteration 219, loss = 0.4181958734989166
iteration 220, loss = 0.11192332208156586
iteration 221, loss = 0.10434234887361526
iteration 222, loss = 0.4172910749912262
iteration 223, loss = 0.41506654024124146
iteration 224, loss = 0.2569813132286072
iteration 225, loss = 0.15619836747646332
iteration 226, loss = 0.15233246982097626
iteration 227, loss = 0.3311474919319153
iteration 228, loss = 0.1786544919013977
iteration 229, loss = 0.40309202671051025
iteration 230, loss = 0.10517803579568863
iteration 231, loss = 0.06445375829935074
iteration 232, loss = 0.06545180827379227
iteration 233, loss = 0.26835957169532776
iteration 234, loss = 0.12744620442390442
iteration 235, loss = 0.08571431785821915
iteration 236, loss = 0.03337889537215233
iteration 237, loss = 0.07860331237316132
iteration 238, loss = 0.08452481776475906
iteration 239, loss = 0.3540770709514618
iteration 240, loss = 0.01243587490171194
iteration 241, loss = 0.23986592888832092
iteration 242, loss = 0.23363815248012543
iteration 243, loss = 0.07561980187892914
iteration 244, loss = 0.25663208961486816
iteration 245, loss = 0.16790786385536194
iteration 246, loss = 0.18210503458976746
iteration 247, loss = 0.12010443210601807
iteration 248, loss = 0.11026228964328766
iteration 249, loss = 0.13083581626415253
iteration 250, loss = 0.24585023522377014
iteration 251, loss = 0.30194610357284546
iteration 252, loss = 0.11800569295883179
iteration 253, loss = 0.21401427686214447
iteration 254, loss = 0.2487746775150299
iteration 255, loss = 0.25380605459213257
iteration 256, loss = 0.17279157042503357
iteration 257, loss = 0.1778825968503952
iteration 258, loss = 0.11959712952375412
iteration 259, loss = 0.14115677773952484
iteration 260, loss = 0.3149573802947998
iteration 261, loss = 0.38009145855903625
iteration 262, loss = 0.1704915165901184
iteration 263, loss = 0.21759632229804993
iteration 264, loss = 0.17821261286735535
iteration 265, loss = 0.5376147031784058
iteration 266, loss = 0.2450951188802719
iteration 267, loss = 0.12138348817825317
iteration 268, loss = 0.10191083699464798
iteration 269, loss = 0.06225068122148514
iteration 270, loss = 0.19672201573848724
iteration 271, loss = 0.3156183660030365
iteration 272, loss = 0.0674210861325264
iteration 273, loss = 0.5247201919555664
iteration 274, loss = 0.13251832127571106
iteration 275, loss = 0.1715727597475052
iteration 276, loss = 0.3473568856716156
iteration 277, loss = 0.16090044379234314
iteration 278, loss = 0.19544170796871185
iteration 279, loss = 0.08984579890966415
iteration 280, loss = 0.33099842071533203
iteration 281, loss = 0.28666096925735474
iteration 282, loss = 0.3717101514339447
iteration 283, loss = 0.16552987694740295
iteration 284, loss = 0.26872438192367554
iteration 285, loss = 0.2566404938697815
iteration 286, loss = 0.2020464539527893
iteration 287, loss = 0.20504510402679443
iteration 288, loss = 0.09739381074905396
iteration 289, loss = 0.3518548309803009
iteration 290, loss = 0.15376174449920654
iteration 291, loss = 0.44976598024368286
iteration 292, loss = 0.21723903715610504
iteration 293, loss = 0.25744736194610596
iteration 294, loss = 0.16389137506484985
iteration 295, loss = 0.29751691222190857
iteration 296, loss = 0.2179601490497589
iteration 297, loss = 0.25741833448410034
iteration 298, loss = 0.32161468267440796
iteration 299, loss = 0.09742922335863113
iteration 0, loss = 0.0733909010887146
iteration 1, loss = 0.1107541024684906
iteration 2, loss = 0.20790491998195648
iteration 3, loss = 0.1723172962665558
iteration 4, loss = 0.05897599086165428
iteration 5, loss = 0.06929127871990204
iteration 6, loss = 0.2234254777431488
iteration 7, loss = 0.24346107244491577
iteration 8, loss = 0.046343132853507996
iteration 9, loss = 0.2895219624042511
iteration 10, loss = 0.18858517706394196
iteration 11, loss = 0.1841162145137787
iteration 12, loss = 0.10316356271505356
iteration 13, loss = 0.2652086019515991
iteration 14, loss = 0.29701074957847595
iteration 15, loss = 0.30714523792266846
iteration 16, loss = 0.17999044060707092
iteration 17, loss = 0.26619482040405273
iteration 18, loss = 0.07534418255090714
iteration 19, loss = 0.29905959963798523
iteration 20, loss = 0.2110217958688736
iteration 21, loss = 0.1710452437400818
iteration 22, loss = 0.2469455450773239
iteration 23, loss = 0.006758917588740587
iteration 24, loss = 0.1983945369720459
iteration 25, loss = 0.27241265773773193
iteration 26, loss = 0.23950742185115814
iteration 27, loss = 0.5624590516090393
iteration 28, loss = 0.25800231099128723
iteration 29, loss = 0.2039734125137329
iteration 30, loss = 0.1668224185705185
iteration 31, loss = 0.26206907629966736
iteration 32, loss = 0.38646504282951355
iteration 33, loss = 0.3194817304611206
iteration 34, loss = 0.38731515407562256
iteration 35, loss = 0.006683937273919582
iteration 36, loss = 0.39021751284599304
iteration 37, loss = 0.4484662711620331
iteration 38, loss = 0.2187725305557251
iteration 39, loss = 0.18445834517478943
iteration 40, loss = 0.20114164054393768
iteration 41, loss = 0.06418273597955704
iteration 42, loss = 0.11313477158546448
iteration 43, loss = 0.049862660467624664
iteration 44, loss = 0.0945386067032814
iteration 45, loss = 0.13716018199920654
iteration 46, loss = 0.13989245891571045
iteration 47, loss = 0.3145271837711334
iteration 48, loss = 0.3158878684043884
iteration 49, loss = 0.11843772232532501
iteration 50, loss = 0.0877716913819313
iteration 51, loss = 0.20907211303710938
iteration 52, loss = 0.34682291746139526
iteration 53, loss = 0.26778802275657654
iteration 54, loss = 0.25550132989883423
iteration 55, loss = 0.024658793583512306
iteration 56, loss = 0.4434686303138733
iteration 57, loss = 0.09767794609069824
iteration 58, loss = 0.22495627403259277
iteration 59, loss = 0.12095646560192108
iteration 60, loss = 0.157182976603508
iteration 61, loss = 0.2737880051136017
iteration 62, loss = 0.2539331912994385
iteration 63, loss = 0.23387238383293152
iteration 64, loss = 0.17541009187698364
iteration 65, loss = 0.07083698362112045
iteration 66, loss = 0.388578861951828
iteration 67, loss = 0.07758086919784546
iteration 68, loss = 0.2470831722021103
iteration 69, loss = 0.0855565294623375
iteration 70, loss = 0.17864610254764557
iteration 71, loss = 0.1543191373348236
iteration 72, loss = 0.3424885869026184
iteration 73, loss = 0.3028154969215393
iteration 74, loss = 0.3096315860748291
iteration 75, loss = 0.17039260268211365
iteration 76, loss = 0.17084969580173492
iteration 77, loss = 0.06220205873250961
iteration 78, loss = 0.07104028016328812
iteration 79, loss = 0.44960808753967285
iteration 80, loss = 0.2017415165901184
iteration 81, loss = 0.12251962721347809
iteration 82, loss = 0.21886521577835083
iteration 83, loss = 0.07899442315101624
iteration 84, loss = 0.17422685027122498
iteration 85, loss = 0.24331948161125183
iteration 86, loss = 0.09149277210235596
iteration 87, loss = 0.20699016749858856
iteration 88, loss = 0.2811088562011719
iteration 89, loss = 0.20369744300842285
iteration 90, loss = 0.09896662831306458
iteration 91, loss = 0.03452962636947632
iteration 92, loss = 0.022948279976844788
iteration 93, loss = 0.1261586993932724
iteration 94, loss = 0.27387821674346924
iteration 95, loss = 0.3209754526615143
iteration 96, loss = 0.16483087837696075
iteration 97, loss = 0.14877118170261383
iteration 98, loss = 0.3287149667739868
iteration 99, loss = 0.15842115879058838
iteration 100, loss = 0.05769597738981247
iteration 101, loss = 0.07567504048347473
iteration 102, loss = 0.1189442127943039
iteration 103, loss = 0.1876973956823349
iteration 104, loss = 0.23699620366096497
iteration 105, loss = 0.12571609020233154
iteration 106, loss = 0.32900285720825195
iteration 107, loss = 0.25210797786712646
iteration 108, loss = 0.3841928243637085
iteration 109, loss = 0.2962475121021271
iteration 110, loss = 0.3132951557636261
iteration 111, loss = 0.3243183493614197
iteration 112, loss = 0.24339550733566284
iteration 113, loss = 0.050675589591264725
iteration 114, loss = 0.06338318437337875
iteration 115, loss = 0.15994194149971008
iteration 116, loss = 0.31570014357566833
iteration 117, loss = 0.32887670397758484
iteration 118, loss = 0.6671403646469116
iteration 119, loss = 0.23917393386363983
iteration 120, loss = 0.1525423377752304
iteration 121, loss = 0.3227730989456177
iteration 122, loss = 0.40773308277130127
iteration 123, loss = 0.4657410979270935
iteration 124, loss = 0.5176682472229004
iteration 125, loss = 0.2552429437637329
iteration 126, loss = 0.08468964695930481
iteration 127, loss = 0.31112241744995117
iteration 128, loss = 0.5672854781150818
iteration 129, loss = 0.2929786443710327
iteration 130, loss = 0.1845177859067917
iteration 131, loss = 0.3707914352416992
iteration 132, loss = 0.070923812687397
iteration 133, loss = 0.10898181796073914
iteration 134, loss = 0.4244532287120819
iteration 135, loss = 0.4430110454559326
iteration 136, loss = 0.13271626830101013
iteration 137, loss = 0.10671577602624893
iteration 138, loss = 0.217839777469635
iteration 139, loss = 0.17698518931865692
iteration 140, loss = 0.28932881355285645
iteration 141, loss = 0.3248066306114197
iteration 142, loss = 0.08851662278175354
iteration 143, loss = 0.08109962195158005
iteration 144, loss = 0.24499396979808807
iteration 145, loss = 0.2980799376964569
iteration 146, loss = 0.1352965235710144
iteration 147, loss = 0.1960742026567459
iteration 148, loss = 0.15302211046218872
iteration 149, loss = 0.04040161520242691
iteration 150, loss = 0.22975069284439087
iteration 151, loss = 0.20646193623542786
iteration 152, loss = 0.20954611897468567
iteration 153, loss = 0.08989635109901428
iteration 154, loss = 0.28318631649017334
iteration 155, loss = 0.02634943276643753
iteration 156, loss = 0.11301889270544052
iteration 157, loss = 0.07402273267507553
iteration 158, loss = 0.1910654902458191
iteration 159, loss = 0.08721667528152466
iteration 160, loss = 0.3404737710952759
iteration 161, loss = 0.1806890070438385
iteration 162, loss = 0.18035438656806946
iteration 163, loss = 0.10422176122665405
iteration 164, loss = 0.0450289212167263
iteration 165, loss = 0.36999982595443726
iteration 166, loss = 0.10106580704450607
iteration 167, loss = 0.05475546047091484
iteration 168, loss = 0.2691134810447693
iteration 169, loss = 0.16933515667915344
iteration 170, loss = 0.10989256203174591
iteration 171, loss = 0.3096657395362854
iteration 172, loss = 0.0720793604850769
iteration 173, loss = 0.2934528589248657
iteration 174, loss = 0.11173400282859802
iteration 175, loss = 0.242936372756958
iteration 176, loss = 0.25811275839805603
iteration 177, loss = 0.2589932680130005
iteration 178, loss = 0.34410953521728516
iteration 179, loss = 0.07641461491584778
iteration 180, loss = 0.1787695288658142
iteration 181, loss = 0.16391634941101074
iteration 182, loss = 0.3391565978527069
iteration 183, loss = 0.1749678999185562
iteration 184, loss = 0.4427586793899536
iteration 185, loss = 0.06907738745212555
iteration 186, loss = 0.13235050439834595
iteration 187, loss = 0.20272523164749146
iteration 188, loss = 0.21130326390266418
iteration 189, loss = 0.3990505635738373
iteration 190, loss = 0.21095305681228638
iteration 191, loss = 0.3325531482696533
iteration 192, loss = 0.25084778666496277
iteration 193, loss = 0.2940492331981659
iteration 194, loss = 0.12880855798721313
iteration 195, loss = 0.2056373953819275
iteration 196, loss = 0.024819642305374146
iteration 197, loss = 0.20794226229190826
iteration 198, loss = 0.37550997734069824
iteration 199, loss = 0.3224998116493225
iteration 200, loss = 0.20716853439807892
iteration 201, loss = 0.22427581250667572
iteration 202, loss = 0.07847769558429718
iteration 203, loss = 0.3218747675418854
iteration 204, loss = 0.2694734036922455
iteration 205, loss = 0.23817424476146698
iteration 206, loss = 0.31768256425857544
iteration 207, loss = 0.3429650366306305
iteration 208, loss = 0.19218918681144714
iteration 209, loss = 0.18249675631523132
iteration 210, loss = 0.20939700305461884
iteration 211, loss = 0.07425282895565033
iteration 212, loss = 0.25469139218330383
iteration 213, loss = 0.316678524017334
iteration 214, loss = 0.2330974042415619
iteration 215, loss = 0.1684795320034027
iteration 216, loss = 0.240739107131958
iteration 217, loss = 0.12491167336702347
iteration 218, loss = 0.2540386915206909
iteration 219, loss = 0.15623688697814941
iteration 220, loss = 0.2161986231803894
iteration 221, loss = 0.1314951777458191
iteration 222, loss = 0.35742974281311035
iteration 223, loss = 0.16852156817913055
iteration 224, loss = 0.1416272670030594
iteration 225, loss = 0.22399649024009705
iteration 226, loss = 0.13745269179344177
iteration 227, loss = 0.11536088585853577
iteration 228, loss = 0.1163719967007637
iteration 229, loss = 0.5263371467590332
iteration 230, loss = 0.08061383664608002
iteration 231, loss = 0.14497923851013184
iteration 232, loss = 0.26267480850219727
iteration 233, loss = 0.36846110224723816
iteration 234, loss = 0.13407063484191895
iteration 235, loss = 0.08540122210979462
iteration 236, loss = 0.12969642877578735
iteration 237, loss = 0.2125566601753235
iteration 238, loss = 0.23001965880393982
iteration 239, loss = 0.16211755573749542
iteration 240, loss = 0.14967983961105347
iteration 241, loss = 0.28244495391845703
iteration 242, loss = 0.25329530239105225
iteration 243, loss = 0.2569023072719574
iteration 244, loss = 0.22151178121566772
iteration 245, loss = 0.20683260262012482
iteration 246, loss = 0.0371493324637413
iteration 247, loss = 0.08805494010448456
iteration 248, loss = 0.42594239115715027
iteration 249, loss = 0.18661737442016602
iteration 250, loss = 0.2942246198654175
iteration 251, loss = 0.19716031849384308
iteration 252, loss = 0.37330374121665955
iteration 253, loss = 0.16925029456615448
iteration 254, loss = 0.1332615166902542
iteration 255, loss = 0.1949765384197235
iteration 256, loss = 0.0822213813662529
iteration 257, loss = 0.15208546817302704
iteration 258, loss = 0.10682177543640137
iteration 259, loss = 0.26125895977020264
iteration 260, loss = 0.14893074333667755
iteration 261, loss = 0.20433375239372253
iteration 262, loss = 0.19728432595729828
iteration 263, loss = 0.11222604662179947
iteration 264, loss = 0.2026287019252777
iteration 265, loss = 0.39181384444236755
iteration 266, loss = 0.1944972276687622
iteration 267, loss = 0.03699598088860512
iteration 268, loss = 0.19613468647003174
iteration 269, loss = 0.17442184686660767
iteration 270, loss = 0.28399020433425903
iteration 271, loss = 0.11680807173252106
iteration 272, loss = 0.02086404711008072
iteration 273, loss = 0.1205059066414833
iteration 274, loss = 0.2521860599517822
iteration 275, loss = 0.05126102641224861
iteration 276, loss = 0.13233202695846558
iteration 277, loss = 0.40231597423553467
iteration 278, loss = 0.13431097567081451
iteration 279, loss = 0.24078109860420227
iteration 280, loss = 0.15250837802886963
iteration 281, loss = 0.23728883266448975
iteration 282, loss = 0.17677508294582367
iteration 283, loss = 0.19498038291931152
iteration 284, loss = 0.18641451001167297
iteration 285, loss = 0.07440751045942307
iteration 286, loss = 0.22881382703781128
iteration 287, loss = 0.2358996719121933
iteration 288, loss = 0.16762174665927887
iteration 289, loss = 0.28495439887046814
iteration 290, loss = 0.28111255168914795
iteration 291, loss = 0.04143094643950462
iteration 292, loss = 0.17573484778404236
iteration 293, loss = 0.1524938941001892
iteration 294, loss = 0.0453924685716629
iteration 295, loss = 0.3552389442920685
iteration 296, loss = 0.15769433975219727
iteration 297, loss = 0.23439237475395203
iteration 298, loss = 0.08359451591968536
iteration 299, loss = 0.15874648094177246
iteration 0, loss = 0.27665114402770996
iteration 1, loss = 0.26348790526390076
iteration 2, loss = 0.26370587944984436
iteration 3, loss = 0.13148897886276245
iteration 4, loss = 0.16499392688274384
iteration 5, loss = 0.12730416655540466
iteration 6, loss = 0.2281516194343567
iteration 7, loss = 0.2357400357723236
iteration 8, loss = 0.03620430827140808
iteration 9, loss = 0.36359933018684387
iteration 10, loss = 0.21892571449279785
iteration 11, loss = 0.19057530164718628
iteration 12, loss = 0.2602578103542328
iteration 13, loss = 0.13281390070915222
iteration 14, loss = 0.42162519693374634
iteration 15, loss = 0.18433190882205963
iteration 16, loss = 0.1267683207988739
iteration 17, loss = 0.09372715651988983
iteration 18, loss = 0.449857234954834
iteration 19, loss = 0.1883295327425003
iteration 20, loss = 0.131055548787117
iteration 21, loss = 0.18645498156547546
iteration 22, loss = 0.2548152804374695
iteration 23, loss = 0.27657657861709595
iteration 24, loss = 0.2885962724685669
iteration 25, loss = 0.3613753318786621
iteration 26, loss = 0.1416119635105133
iteration 27, loss = 0.2654263377189636
iteration 28, loss = 0.17588573694229126
iteration 29, loss = 0.47866010665893555
iteration 30, loss = 0.0802784338593483
iteration 31, loss = 0.18682143092155457
iteration 32, loss = 0.07894971966743469
iteration 33, loss = 0.09530610591173172
iteration 34, loss = 0.16534528136253357
iteration 35, loss = 0.34479862451553345
iteration 36, loss = 0.19241374731063843
iteration 37, loss = 0.20348693430423737
iteration 38, loss = 0.14551329612731934
iteration 39, loss = 0.30420830845832825
iteration 40, loss = 0.2001367211341858
iteration 41, loss = 0.03808724135160446
iteration 42, loss = 0.2684045433998108
iteration 43, loss = 0.19509674608707428
iteration 44, loss = 0.18227504193782806
iteration 45, loss = 0.2815411686897278
iteration 46, loss = 0.1485893726348877
iteration 47, loss = 0.20494452118873596
iteration 48, loss = 0.17708157002925873
iteration 49, loss = 0.2934061884880066
iteration 50, loss = 0.33461251854896545
iteration 51, loss = 0.15867409110069275
iteration 52, loss = 0.15934967994689941
iteration 53, loss = 0.1703372299671173
iteration 54, loss = 0.07326209545135498
iteration 55, loss = 0.07089756429195404
iteration 56, loss = 0.362888902425766
iteration 57, loss = 0.13770821690559387
iteration 58, loss = 0.20843088626861572
iteration 59, loss = 0.17722994089126587
iteration 60, loss = 0.12342575192451477
iteration 61, loss = 0.13965006172657013
iteration 62, loss = 0.03296090289950371
iteration 63, loss = 0.06155983358621597
iteration 64, loss = 0.06873057782649994
iteration 65, loss = 0.06547457724809647
iteration 66, loss = 0.018086418509483337
iteration 67, loss = 0.17254331707954407
iteration 68, loss = 0.3538413941860199
iteration 69, loss = 0.2754979729652405
iteration 70, loss = 0.20080359280109406
iteration 71, loss = 0.18459481000900269
iteration 72, loss = 0.31449204683303833
iteration 73, loss = 0.24954622983932495
iteration 74, loss = 0.20471705496311188
iteration 75, loss = 0.11351297795772552
iteration 76, loss = 0.10702111572027206
iteration 77, loss = 0.24397757649421692
iteration 78, loss = 0.031672295182943344
iteration 79, loss = 0.10391581803560257
iteration 80, loss = 0.42649173736572266
iteration 81, loss = 0.17294172942638397
iteration 82, loss = 0.1021810919046402
iteration 83, loss = 0.08870811760425568
iteration 84, loss = 0.25648438930511475
iteration 85, loss = 0.37785622477531433
iteration 86, loss = 0.24518786370754242
iteration 87, loss = 0.2674131393432617
iteration 88, loss = 0.33960506319999695
iteration 89, loss = 0.19192323088645935
iteration 90, loss = 0.2889084815979004
iteration 91, loss = 0.022674517706036568
iteration 92, loss = 0.10035552084445953
iteration 93, loss = 0.4542975127696991
iteration 94, loss = 0.3558086156845093
iteration 95, loss = 0.061364151537418365
iteration 96, loss = 0.050394076853990555
iteration 97, loss = 0.25379180908203125
iteration 98, loss = 0.42907559871673584
iteration 99, loss = 0.2942429184913635
iteration 100, loss = 0.27679359912872314
iteration 101, loss = 0.40758416056632996
iteration 102, loss = 0.09502767026424408
iteration 103, loss = 0.017092227935791016
iteration 104, loss = 0.2996756434440613
iteration 105, loss = 0.26723718643188477
iteration 106, loss = 0.04167354851961136
iteration 107, loss = 0.15935254096984863
iteration 108, loss = 0.2040674090385437
iteration 109, loss = 0.06475116312503815
iteration 110, loss = 0.20370134711265564
iteration 111, loss = 0.02067740634083748
iteration 112, loss = 0.05122322961688042
iteration 113, loss = 0.20159518718719482
iteration 114, loss = 0.39784014225006104
iteration 115, loss = 0.17706824839115143
iteration 116, loss = 0.0979074239730835
iteration 117, loss = 0.14102326333522797
iteration 118, loss = 0.12355052679777145
iteration 119, loss = 0.14589843153953552
iteration 120, loss = 0.1602456122636795
iteration 121, loss = 0.14832349121570587
iteration 122, loss = 0.02424583211541176
iteration 123, loss = 0.21675610542297363
iteration 124, loss = 0.2613258957862854
iteration 125, loss = 0.24623416364192963
iteration 126, loss = 0.25020965933799744
iteration 127, loss = 0.2244960218667984
iteration 128, loss = 0.11887121200561523
iteration 129, loss = 0.44490939378738403
iteration 130, loss = 0.30571162700653076
iteration 131, loss = 0.0847792997956276
iteration 132, loss = 0.3660963475704193
iteration 133, loss = 0.12779715657234192
iteration 134, loss = 0.41371622681617737
iteration 135, loss = 0.15596634149551392
iteration 136, loss = 0.08814340829849243
iteration 137, loss = 0.19991067051887512
iteration 138, loss = 0.3067043721675873
iteration 139, loss = 0.09466314315795898
iteration 140, loss = 0.2129509001970291
iteration 141, loss = 0.16405004262924194
iteration 142, loss = 0.345608115196228
iteration 143, loss = 0.1226608157157898
iteration 144, loss = 0.5105787515640259
iteration 145, loss = 0.39046695828437805
iteration 146, loss = 0.3326892852783203
iteration 147, loss = 0.19903388619422913
iteration 148, loss = 0.17852333188056946
iteration 149, loss = 0.2756112515926361
iteration 150, loss = 0.24149401485919952
iteration 151, loss = 0.3884616196155548
iteration 152, loss = 0.0075631579384207726
iteration 153, loss = 0.2503136098384857
iteration 154, loss = 0.17823274433612823
iteration 155, loss = 0.29955410957336426
iteration 156, loss = 0.23481157422065735
iteration 157, loss = 0.1838693618774414
iteration 158, loss = 0.06405188143253326
iteration 159, loss = 0.26203805208206177
iteration 160, loss = 0.32299160957336426
iteration 161, loss = 0.3073006570339203
iteration 162, loss = 0.13525763154029846
iteration 163, loss = 0.10830113291740417
iteration 164, loss = 0.20049743354320526
iteration 165, loss = 0.22292394936084747
iteration 166, loss = 0.2458483725786209
iteration 167, loss = 0.2604069113731384
iteration 168, loss = 0.25931012630462646
iteration 169, loss = 0.08134779334068298
iteration 170, loss = 0.08239918202161789
iteration 171, loss = 0.1164855808019638
iteration 172, loss = 0.2657637298107147
iteration 173, loss = 0.12974032759666443
iteration 174, loss = 0.21529355645179749
iteration 175, loss = 0.06628912687301636
iteration 176, loss = 0.2266451120376587
iteration 177, loss = 0.08609524369239807
iteration 178, loss = 0.4145088195800781
iteration 179, loss = 0.017415236681699753
iteration 180, loss = 0.0904635414481163
iteration 181, loss = 0.13959509134292603
iteration 182, loss = 0.20072054862976074
iteration 183, loss = 0.24137364327907562
iteration 184, loss = 0.20448210835456848
iteration 185, loss = 0.17743781208992004
iteration 186, loss = 0.054331738501787186
iteration 187, loss = 0.07375568151473999
iteration 188, loss = 0.004383700899779797
iteration 189, loss = 0.227813258767128
iteration 190, loss = 0.3304062783718109
iteration 191, loss = 0.17548322677612305
iteration 192, loss = 0.22058214247226715
iteration 193, loss = 0.3387925624847412
iteration 194, loss = 0.18124441802501678
iteration 195, loss = 0.3559257984161377
iteration 196, loss = 0.3263445198535919
iteration 197, loss = 0.2102201133966446
iteration 198, loss = 0.14493627846240997
iteration 199, loss = 0.06721574068069458
iteration 200, loss = 0.3745114207267761
iteration 201, loss = 0.013343694619834423
iteration 202, loss = 0.31533560156822205
iteration 203, loss = 0.01585884392261505
iteration 204, loss = 0.38615381717681885
iteration 205, loss = 0.3403562605381012
iteration 206, loss = 0.046800099313259125
iteration 207, loss = 0.10787694901227951
iteration 208, loss = 0.13427786529064178
iteration 209, loss = 0.16046343743801117
iteration 210, loss = 0.18871460855007172
iteration 211, loss = 0.2356266975402832
iteration 212, loss = 0.2353672832250595
iteration 213, loss = 0.15464867651462555
iteration 214, loss = 0.11736105382442474
iteration 215, loss = 0.04081283509731293
iteration 216, loss = 0.3657720685005188
iteration 217, loss = 0.37988367676734924
iteration 218, loss = 0.336677610874176
iteration 219, loss = 0.3366681933403015
iteration 220, loss = 0.44648993015289307
iteration 221, loss = 0.16263723373413086
iteration 222, loss = 0.4134942889213562
iteration 223, loss = 0.3899228274822235
iteration 224, loss = 0.2945431172847748
iteration 225, loss = 0.5534228086471558
iteration 226, loss = 0.31783831119537354
iteration 227, loss = 0.12507253885269165
iteration 228, loss = 0.2372886836528778
iteration 229, loss = 0.24216949939727783
iteration 230, loss = 0.06759008765220642
iteration 231, loss = 0.5781883001327515
iteration 232, loss = 0.18277651071548462
iteration 233, loss = 0.04987669363617897
iteration 234, loss = 0.2740199565887451
iteration 235, loss = 0.18110936880111694
iteration 236, loss = 0.05706559866666794
iteration 237, loss = 0.14625239372253418
iteration 238, loss = 0.17247608304023743
iteration 239, loss = 0.10438036918640137
iteration 240, loss = 0.18490904569625854
iteration 241, loss = 0.06629502773284912
iteration 242, loss = 0.273046612739563
iteration 243, loss = 0.17480513453483582
iteration 244, loss = 0.19203059375286102
iteration 245, loss = 0.10080593824386597
iteration 246, loss = 0.16271984577178955
iteration 247, loss = 0.3416818082332611
iteration 248, loss = 0.10903866589069366
iteration 249, loss = 0.21252214908599854
iteration 250, loss = 0.08176875859498978
iteration 251, loss = 0.028572270646691322
iteration 252, loss = 0.21642981469631195
iteration 253, loss = 0.41837748885154724
iteration 254, loss = 0.19182629883289337
iteration 255, loss = 0.18835607171058655
iteration 256, loss = 0.046942420303821564
iteration 257, loss = 0.07471344619989395
iteration 258, loss = 0.1739998459815979
iteration 259, loss = 0.1944291740655899
iteration 260, loss = 0.140329971909523
iteration 261, loss = 0.0715085119009018
iteration 262, loss = 0.14762064814567566
iteration 263, loss = 0.21998029947280884
iteration 264, loss = 0.17707103490829468
iteration 265, loss = 0.2922203540802002
iteration 266, loss = 0.27904537320137024
iteration 267, loss = 0.33171579241752625
iteration 268, loss = 0.17463631927967072
iteration 269, loss = 0.18065348267555237
iteration 270, loss = 0.36245661973953247
iteration 271, loss = 0.348985493183136
iteration 272, loss = 0.10687443614006042
iteration 273, loss = 0.27853551506996155
iteration 274, loss = 0.2628943920135498
iteration 275, loss = 0.12422028183937073
iteration 276, loss = 0.21134430170059204
iteration 277, loss = 0.14680984616279602
iteration 278, loss = 0.11743717640638351
iteration 279, loss = 0.18576903641223907
iteration 280, loss = 0.607604444026947
iteration 281, loss = 0.2319018840789795
iteration 282, loss = 0.0771230012178421
iteration 283, loss = 0.2677651047706604
iteration 284, loss = 0.1601092666387558
iteration 285, loss = 0.21585191786289215
iteration 286, loss = 0.19445763528347015
iteration 287, loss = 0.08226354420185089
iteration 288, loss = 0.1016131192445755
iteration 289, loss = 0.12959295511245728
iteration 290, loss = 0.17963272333145142
iteration 291, loss = 0.11300435662269592
iteration 292, loss = 0.21685633063316345
iteration 293, loss = 0.2745456397533417
iteration 294, loss = 0.08719412982463837
iteration 295, loss = 0.36396604776382446
iteration 296, loss = 0.29021212458610535
iteration 297, loss = 0.2803482711315155
iteration 298, loss = 0.13102015852928162
iteration 299, loss = 0.09416491538286209
iteration 0, loss = 0.26572003960609436
iteration 1, loss = 0.4302820563316345
iteration 2, loss = 0.24542178213596344
iteration 3, loss = 0.17739149928092957
iteration 4, loss = 0.19177120923995972
iteration 5, loss = 0.28536614775657654
iteration 6, loss = 0.25620993971824646
iteration 7, loss = 0.31019720435142517
iteration 8, loss = 0.03081589564681053
iteration 9, loss = 0.20481881499290466
iteration 10, loss = 0.14527727663516998
iteration 11, loss = 0.3499816656112671
iteration 12, loss = 0.19197167456150055
iteration 13, loss = 0.2816593647003174
iteration 14, loss = 0.13485394418239594
iteration 15, loss = 0.10830750316381454
iteration 16, loss = 0.04602678865194321
iteration 17, loss = 0.039443038403987885
iteration 18, loss = 0.13337276875972748
iteration 19, loss = 0.08921951055526733
iteration 20, loss = 0.2295980155467987
iteration 21, loss = 0.1848728507757187
iteration 22, loss = 0.0967414379119873
iteration 23, loss = 0.08019379526376724
iteration 24, loss = 0.1720477044582367
iteration 25, loss = 0.40894538164138794
iteration 26, loss = 0.17179439961910248
iteration 27, loss = 0.10908568650484085
iteration 28, loss = 0.19398799538612366
iteration 29, loss = 0.08542298525571823
iteration 30, loss = 0.25489842891693115
iteration 31, loss = 0.19971230626106262
iteration 32, loss = 0.2187591791152954
iteration 33, loss = 0.061796680092811584
iteration 34, loss = 0.07858302444219589
iteration 35, loss = 0.2171298861503601
iteration 36, loss = 0.028489412739872932
iteration 37, loss = 0.08736622333526611
iteration 38, loss = 0.31975826621055603
iteration 39, loss = 0.21591541171073914
iteration 40, loss = 0.09340798854827881
iteration 41, loss = 0.09288837015628815
iteration 42, loss = 0.18507076799869537
iteration 43, loss = 0.12277764081954956
iteration 44, loss = 0.11024336516857147
iteration 45, loss = 0.12876613438129425
iteration 46, loss = 0.24047282338142395
iteration 47, loss = 0.14753715693950653
iteration 48, loss = 0.13327595591545105
iteration 49, loss = 0.10674519836902618
iteration 50, loss = 0.0880274623632431
iteration 51, loss = 0.41334936022758484
iteration 52, loss = 0.1786196529865265
iteration 53, loss = 0.2251487672328949
iteration 54, loss = 0.18252374231815338
iteration 55, loss = 0.11442702263593674
iteration 56, loss = 0.20348982512950897
iteration 57, loss = 0.043950602412223816
iteration 58, loss = 0.24496468901634216
iteration 59, loss = 0.2830437421798706
iteration 60, loss = 0.0978877991437912
iteration 61, loss = 0.04219396784901619
iteration 62, loss = 0.2049267292022705
iteration 63, loss = 0.22791223227977753
iteration 64, loss = 0.13998553156852722
iteration 65, loss = 0.20104794204235077
iteration 66, loss = 0.1472611129283905
iteration 67, loss = 0.25376081466674805
iteration 68, loss = 0.09246674180030823
iteration 69, loss = 0.13252611458301544
iteration 70, loss = 0.1501157581806183
iteration 71, loss = 0.3182108998298645
iteration 72, loss = 0.1013883650302887
iteration 73, loss = 0.15405452251434326
iteration 74, loss = 0.08982588350772858
iteration 75, loss = 0.1579848974943161
iteration 76, loss = 0.17992547154426575
iteration 77, loss = 0.14430156350135803
iteration 78, loss = 0.03720628842711449
iteration 79, loss = 0.1732286661863327
iteration 80, loss = 0.11223006248474121
iteration 81, loss = 0.056178025901317596
iteration 82, loss = 0.24863193929195404
iteration 83, loss = 0.3120717406272888
iteration 84, loss = 0.0782928541302681
iteration 85, loss = 0.1862332671880722
iteration 86, loss = 0.05213208496570587
iteration 87, loss = 0.13553577661514282
iteration 88, loss = 0.2959565818309784
iteration 89, loss = 0.09070717543363571
iteration 90, loss = 0.2669689953327179
iteration 91, loss = 0.061223626136779785
iteration 92, loss = 0.3263391852378845
iteration 93, loss = 0.4024825096130371
iteration 94, loss = 0.04878133535385132
iteration 95, loss = 0.1487017273902893
iteration 96, loss = 0.175077885389328
iteration 97, loss = 0.1374620795249939
iteration 98, loss = 0.1473163664340973
iteration 99, loss = 0.022757289931178093
iteration 100, loss = 0.015638988465070724
iteration 101, loss = 0.09176524728536606
iteration 102, loss = 0.14044351875782013
iteration 103, loss = 0.1099914163351059
iteration 104, loss = 0.28009364008903503
iteration 105, loss = 0.16602620482444763
iteration 106, loss = 0.13989832997322083
iteration 107, loss = 0.2826899588108063
iteration 108, loss = 0.2875770032405853
iteration 109, loss = 0.15867199003696442
iteration 110, loss = 0.12118900567293167
iteration 111, loss = 0.10371457040309906
iteration 112, loss = 0.22457478940486908
iteration 113, loss = 0.08124901354312897
iteration 114, loss = 0.5328505039215088
iteration 115, loss = 0.28064242005348206
iteration 116, loss = 0.27800118923187256
iteration 117, loss = 0.22111207246780396
iteration 118, loss = 0.15416254103183746
iteration 119, loss = 0.06347127258777618
iteration 120, loss = 0.3448959290981293
iteration 121, loss = 0.1558505892753601
iteration 122, loss = 0.13004103302955627
iteration 123, loss = 0.19645561277866364
iteration 124, loss = 0.1426902413368225
iteration 125, loss = 0.07015286386013031
iteration 126, loss = 0.2676575481891632
iteration 127, loss = 0.19116930663585663
iteration 128, loss = 0.05467813089489937
iteration 129, loss = 0.023886261507868767
iteration 130, loss = 0.18801039457321167
iteration 131, loss = 0.44093847274780273
iteration 132, loss = 0.5608289241790771
iteration 133, loss = 0.31672680377960205
iteration 134, loss = 0.29854029417037964
iteration 135, loss = 0.17275658249855042
iteration 136, loss = 0.09239533543586731
iteration 137, loss = 0.09016403555870056
iteration 138, loss = 0.1323118656873703
iteration 139, loss = 0.1844242364168167
iteration 140, loss = 0.17733432352542877
iteration 141, loss = 0.5430468320846558
iteration 142, loss = 0.17485880851745605
iteration 143, loss = 0.01708635687828064
iteration 144, loss = 0.018706679344177246
iteration 145, loss = 0.2645590305328369
iteration 146, loss = 0.43758735060691833
iteration 147, loss = 0.04020307958126068
iteration 148, loss = 0.4898349940776825
iteration 149, loss = 0.0827118530869484
iteration 150, loss = 0.13628362119197845
iteration 151, loss = 0.19217362999916077
iteration 152, loss = 0.1710609346628189
iteration 153, loss = 0.27089741826057434
iteration 154, loss = 0.1782011091709137
iteration 155, loss = 0.10384765267372131
iteration 156, loss = 0.4357263445854187
iteration 157, loss = 0.10407370328903198
iteration 158, loss = 0.39546477794647217
iteration 159, loss = 0.24807824194431305
iteration 160, loss = 0.25100454688072205
iteration 161, loss = 0.1967591792345047
iteration 162, loss = 0.24783137440681458
iteration 163, loss = 0.4005630910396576
iteration 164, loss = 0.2268359363079071
iteration 165, loss = 0.14136482775211334
iteration 166, loss = 0.3258569836616516
iteration 167, loss = 0.28589680790901184
iteration 168, loss = 0.1104469895362854
iteration 169, loss = 0.3257904052734375
iteration 170, loss = 0.05736920237541199
iteration 171, loss = 0.1284223049879074
iteration 172, loss = 0.24431104958057404
iteration 173, loss = 0.19625496864318848
iteration 174, loss = 0.17928265035152435
iteration 175, loss = 0.2178758829832077
iteration 176, loss = 0.14890137314796448
iteration 177, loss = 0.23316757380962372
iteration 178, loss = 0.19980484247207642
iteration 179, loss = 0.05331916734576225
iteration 180, loss = 0.07041965425014496
iteration 181, loss = 0.313331663608551
iteration 182, loss = 0.06747434288263321
iteration 183, loss = 0.14817437529563904
iteration 184, loss = 0.21757084131240845
iteration 185, loss = 0.15467575192451477
iteration 186, loss = 0.22674784064292908
iteration 187, loss = 0.19203251600265503
iteration 188, loss = 0.31875473260879517
iteration 189, loss = 0.11338169872760773
iteration 190, loss = 0.08762602508068085
iteration 191, loss = 0.08424386382102966
iteration 192, loss = 0.16082875430583954
iteration 193, loss = 0.4737165570259094
iteration 194, loss = 0.14386729896068573
iteration 195, loss = 0.07250593602657318
iteration 196, loss = 0.3026190400123596
iteration 197, loss = 0.08962077647447586
iteration 198, loss = 0.22689951956272125
iteration 199, loss = 0.29899728298187256
iteration 200, loss = 0.1192295104265213
iteration 201, loss = 0.05634702369570732
iteration 202, loss = 0.3747812509536743
iteration 203, loss = 0.19042548537254333
iteration 204, loss = 0.23232796788215637
iteration 205, loss = 0.06768573820590973
iteration 206, loss = 0.26490354537963867
iteration 207, loss = 0.1462642103433609
iteration 208, loss = 0.16211146116256714
iteration 209, loss = 0.02752048335969448
iteration 210, loss = 0.12422972917556763
iteration 211, loss = 0.18364253640174866
iteration 212, loss = 0.2772149443626404
iteration 213, loss = 0.1218065693974495
iteration 214, loss = 0.3129865825176239
iteration 215, loss = 0.06309165805578232
iteration 216, loss = 0.13854102790355682
iteration 217, loss = 0.22066816687583923
iteration 218, loss = 0.09616483747959137
iteration 219, loss = 0.08456844091415405
iteration 220, loss = 0.15235435962677002
iteration 221, loss = 0.16095517575740814
iteration 222, loss = 0.4224400520324707
iteration 223, loss = 0.16833658516407013
iteration 224, loss = 0.12214381992816925
iteration 225, loss = 0.279965877532959
iteration 226, loss = 0.3446502387523651
iteration 227, loss = 0.2593134939670563
iteration 228, loss = 0.29338592290878296
iteration 229, loss = 0.030923927202820778
iteration 230, loss = 0.23643483221530914
iteration 231, loss = 0.12751370668411255
iteration 232, loss = 0.7407191395759583
iteration 233, loss = 0.1203952506184578
iteration 234, loss = 0.3193790018558502
iteration 235, loss = 0.2792094945907593
iteration 236, loss = 0.11591225117444992
iteration 237, loss = 0.4473169445991516
iteration 238, loss = 0.34576135873794556
iteration 239, loss = 0.45113229751586914
iteration 240, loss = 0.18205508589744568
iteration 241, loss = 0.11569555103778839
iteration 242, loss = 0.16285209357738495
iteration 243, loss = 0.3733559548854828
iteration 244, loss = 0.26577526330947876
iteration 245, loss = 0.1470736265182495
iteration 246, loss = 0.09222714602947235
iteration 247, loss = 0.12969091534614563
iteration 248, loss = 0.07343241572380066
iteration 249, loss = 0.34431251883506775
iteration 250, loss = 0.1443365514278412
iteration 251, loss = 0.35533013939857483
iteration 252, loss = 0.0740959569811821
iteration 253, loss = 0.07415549457073212
iteration 254, loss = 0.16563639044761658
iteration 255, loss = 0.3136325776576996
iteration 256, loss = 0.15596529841423035
iteration 257, loss = 0.3243929147720337
iteration 258, loss = 0.19784575700759888
iteration 259, loss = 0.06782099604606628
iteration 260, loss = 0.31335821747779846
iteration 261, loss = 0.037554606795310974
iteration 262, loss = 0.031715989112854004
iteration 263, loss = 0.36643654108047485
iteration 264, loss = 0.3175511956214905
iteration 265, loss = 0.2935452461242676
iteration 266, loss = 0.14662212133407593
iteration 267, loss = 0.03175030276179314
iteration 268, loss = 0.227981299161911
iteration 269, loss = 0.19484809041023254
iteration 270, loss = 0.267647385597229
iteration 271, loss = 0.5734853744506836
iteration 272, loss = 0.22504709661006927
iteration 273, loss = 0.2749437689781189
iteration 274, loss = 0.181598961353302
iteration 275, loss = 0.40243276953697205
iteration 276, loss = 0.2719269096851349
iteration 277, loss = 0.10088930279016495
iteration 278, loss = 0.17178329825401306
iteration 279, loss = 0.08293992280960083
iteration 280, loss = 0.21487665176391602
iteration 281, loss = 0.02846076339483261
iteration 282, loss = 0.14559179544448853
iteration 283, loss = 0.028916463255882263
iteration 284, loss = 0.3402198851108551
iteration 285, loss = 0.312661737203598
iteration 286, loss = 0.37947797775268555
iteration 287, loss = 0.2135276198387146
iteration 288, loss = 0.329091876745224
iteration 289, loss = 0.42877450585365295
iteration 290, loss = 0.14392131567001343
iteration 291, loss = 0.1631479114294052
iteration 292, loss = 0.2608375549316406
iteration 293, loss = 0.15221844613552094
iteration 294, loss = 0.2982659339904785
iteration 295, loss = 0.3589365482330322
iteration 296, loss = 0.11290913820266724
iteration 297, loss = 0.05716056749224663
iteration 298, loss = 0.23460549116134644
iteration 299, loss = 0.19191162288188934
iteration 0, loss = 0.16423378884792328
iteration 1, loss = 0.08723054826259613
iteration 2, loss = 0.04926646500825882
iteration 3, loss = 0.22130444645881653
iteration 4, loss = 0.4179835915565491
iteration 5, loss = 0.41319358348846436
iteration 6, loss = 0.33944806456565857
iteration 7, loss = 0.21537534892559052
iteration 8, loss = 0.15266840159893036
iteration 9, loss = 0.2591269016265869
iteration 10, loss = 0.12354376912117004
iteration 11, loss = 0.596795916557312
iteration 12, loss = 0.20051802694797516
iteration 13, loss = 0.225692018866539
iteration 14, loss = 0.10927236080169678
iteration 15, loss = 0.4182925224304199
iteration 16, loss = 0.21916359663009644
iteration 17, loss = 0.1751289665699005
iteration 18, loss = 0.244716614484787
iteration 19, loss = 0.12100814282894135
iteration 20, loss = 0.10879695415496826
iteration 21, loss = 0.3221922814846039
iteration 22, loss = 0.11330373585224152
iteration 23, loss = 0.14511333405971527
iteration 24, loss = 0.1781042069196701
iteration 25, loss = 0.16303272545337677
iteration 26, loss = 0.06394512206315994
iteration 27, loss = 0.06657948344945908
iteration 28, loss = 0.03307308256626129
iteration 29, loss = 0.035476550459861755
iteration 30, loss = 0.203530415892601
iteration 31, loss = 0.11798249185085297
iteration 32, loss = 0.19701924920082092
iteration 33, loss = 0.1255578249692917
iteration 34, loss = 0.1014617532491684
iteration 35, loss = 0.2155556082725525
iteration 36, loss = 0.11556864529848099
iteration 37, loss = 0.12839160859584808
iteration 38, loss = 0.16236644983291626
iteration 39, loss = 0.25318440794944763
iteration 40, loss = 0.3323504626750946
iteration 41, loss = 0.08307349681854248
iteration 42, loss = 0.006453688256442547
iteration 43, loss = 0.18443834781646729
iteration 44, loss = 0.13483160734176636
iteration 45, loss = 0.09796218574047089
iteration 46, loss = 0.1336888074874878
iteration 47, loss = 0.2297034114599228
iteration 48, loss = 0.10740281641483307
iteration 49, loss = 0.2720310389995575
iteration 50, loss = 0.16074612736701965
iteration 51, loss = 0.2427082061767578
iteration 52, loss = 0.051856447011232376
iteration 53, loss = 0.28664708137512207
iteration 54, loss = 0.024709027260541916
iteration 55, loss = 0.09619179368019104
iteration 56, loss = 0.3224872648715973
iteration 57, loss = 0.06626249104738235
iteration 58, loss = 0.4122709035873413
iteration 59, loss = 0.17288412153720856
iteration 60, loss = 0.09617350250482559
iteration 61, loss = 0.03525908291339874
iteration 62, loss = 0.22115620970726013
iteration 63, loss = 0.24706172943115234
iteration 64, loss = 0.3321833610534668
iteration 65, loss = 0.12104244530200958
iteration 66, loss = 0.17203059792518616
iteration 67, loss = 0.1683979481458664
iteration 68, loss = 0.17860275506973267
iteration 69, loss = 0.2465391755104065
iteration 70, loss = 0.2944839596748352
iteration 71, loss = 0.09300220757722855
iteration 72, loss = 0.03587183356285095
iteration 73, loss = 0.07099323719739914
iteration 74, loss = 0.1600549817085266
iteration 75, loss = 0.6868273019790649
iteration 76, loss = 0.005439594853669405
iteration 77, loss = 0.4005400538444519
iteration 78, loss = 0.35188984870910645
iteration 79, loss = 0.23141606152057648
iteration 80, loss = 0.06680358201265335
iteration 81, loss = 0.1872243881225586
iteration 82, loss = 0.20548173785209656
iteration 83, loss = 0.2528136074542999
iteration 84, loss = 0.24797795712947845
iteration 85, loss = 0.03096468187868595
iteration 86, loss = 0.24978484213352203
iteration 87, loss = 0.09845614433288574
iteration 88, loss = 0.2555071711540222
iteration 89, loss = 0.18754498660564423
iteration 90, loss = 0.21072974801063538
iteration 91, loss = 0.07861070334911346
iteration 92, loss = 0.11352865397930145
iteration 93, loss = 0.16410717368125916
iteration 94, loss = 0.15140171349048615
iteration 95, loss = 0.21776527166366577
iteration 96, loss = 0.08208360522985458
iteration 97, loss = 0.06057044118642807
iteration 98, loss = 0.024065520614385605
iteration 99, loss = 0.2952057123184204
iteration 100, loss = 0.13983865082263947
iteration 101, loss = 0.005512284114956856
iteration 102, loss = 0.2425679713487625
iteration 103, loss = 0.14309091866016388
iteration 104, loss = 0.2613787353038788
iteration 105, loss = 0.24083450436592102
iteration 106, loss = 0.1375235915184021
iteration 107, loss = 0.16808703541755676
iteration 108, loss = 0.2595670819282532
iteration 109, loss = 0.29840755462646484
iteration 110, loss = 0.5197871327400208
iteration 111, loss = 0.41763734817504883
iteration 112, loss = 0.1383836567401886
iteration 113, loss = 0.22348950803279877
iteration 114, loss = 0.27205121517181396
iteration 115, loss = 0.19869956374168396
iteration 116, loss = 0.10911695659160614
iteration 117, loss = 0.13500867784023285
iteration 118, loss = 0.05758129432797432
iteration 119, loss = 0.07668593525886536
iteration 120, loss = 0.04768238216638565
iteration 121, loss = 0.17973990738391876
iteration 122, loss = 0.3918173015117645
iteration 123, loss = 0.01763346791267395
iteration 124, loss = 0.41948843002319336
iteration 125, loss = 0.05944715812802315
iteration 126, loss = 0.12897558510303497
iteration 127, loss = 0.2306108921766281
iteration 128, loss = 0.1000269427895546
iteration 129, loss = 0.12287183105945587
iteration 130, loss = 0.25442636013031006
iteration 131, loss = 0.10633810609579086
iteration 132, loss = 0.17120829224586487
iteration 133, loss = 0.23688194155693054
iteration 134, loss = 0.1388576477766037
iteration 135, loss = 0.20799455046653748
iteration 136, loss = 0.4562061131000519
iteration 137, loss = 0.03618411719799042
iteration 138, loss = 0.1574794054031372
iteration 139, loss = 0.20539914071559906
iteration 140, loss = 0.30182451009750366
iteration 141, loss = 0.24500952661037445
iteration 142, loss = 0.4231366515159607
iteration 143, loss = 0.19331856071949005
iteration 144, loss = 0.08998569846153259
iteration 145, loss = 0.06291163712739944
iteration 146, loss = 0.011998654343187809
iteration 147, loss = 0.248266339302063
iteration 148, loss = 0.08594967424869537
iteration 149, loss = 0.44383350014686584
iteration 150, loss = 0.13095007836818695
iteration 151, loss = 0.17324234545230865
iteration 152, loss = 0.1585516631603241
iteration 153, loss = 0.10426159203052521
iteration 154, loss = 0.3137405812740326
iteration 155, loss = 0.06182204559445381
iteration 156, loss = 0.14921554923057556
iteration 157, loss = 0.13026823103427887
iteration 158, loss = 0.13760283589363098
iteration 159, loss = 0.4508166015148163
iteration 160, loss = 0.20205740630626678
iteration 161, loss = 0.14320294559001923
iteration 162, loss = 0.029796065762639046
iteration 163, loss = 0.2622757852077484
iteration 164, loss = 0.2270059883594513
iteration 165, loss = 0.13308948278427124
iteration 166, loss = 0.22793854773044586
iteration 167, loss = 0.3136393427848816
iteration 168, loss = 0.15941092371940613
iteration 169, loss = 0.14506997168064117
iteration 170, loss = 0.03575937822461128
iteration 171, loss = 0.19856390357017517
iteration 172, loss = 0.13005010783672333
iteration 173, loss = 0.038928572088479996
iteration 174, loss = 0.33028432726860046
iteration 175, loss = 0.05889257788658142
iteration 176, loss = 0.07009227573871613
iteration 177, loss = 0.0837981328368187
iteration 178, loss = 0.15978004038333893
iteration 179, loss = 0.18682855367660522
iteration 180, loss = 0.3562433123588562
iteration 181, loss = 0.05529642477631569
iteration 182, loss = 0.15199421346187592
iteration 183, loss = 0.28058186173439026
iteration 184, loss = 0.21294179558753967
iteration 185, loss = 0.06748026609420776
iteration 186, loss = 0.23257410526275635
iteration 187, loss = 0.05916111171245575
iteration 188, loss = 0.12058781087398529
iteration 189, loss = 0.1331673562526703
iteration 190, loss = 0.250455379486084
iteration 191, loss = 0.1717853844165802
iteration 192, loss = 0.17526739835739136
iteration 193, loss = 0.18227620422840118
iteration 194, loss = 0.07835868746042252
iteration 195, loss = 0.2811022698879242
iteration 196, loss = 0.2549051344394684
iteration 197, loss = 0.08223574608564377
iteration 198, loss = 0.10217217355966568
iteration 199, loss = 0.042427435517311096
iteration 200, loss = 0.22818523645401
iteration 201, loss = 0.20026786625385284
iteration 202, loss = 0.209500253200531
iteration 203, loss = 0.19761773943901062
iteration 204, loss = 0.25020483136177063
iteration 205, loss = 0.15955646336078644
iteration 206, loss = 0.2309534102678299
iteration 207, loss = 0.18435262143611908
iteration 208, loss = 0.24335522949695587
iteration 209, loss = 0.11577516049146652
iteration 210, loss = 0.05013469234108925
iteration 211, loss = 0.3600996136665344
iteration 212, loss = 0.23680508136749268
iteration 213, loss = 0.30155816674232483
iteration 214, loss = 0.16875818371772766
iteration 215, loss = 0.3282155990600586
iteration 216, loss = 0.46535420417785645
iteration 217, loss = 0.05728689953684807
iteration 218, loss = 0.01834305189549923
iteration 219, loss = 0.04280226677656174
iteration 220, loss = 0.2989346981048584
iteration 221, loss = 0.25417882204055786
iteration 222, loss = 0.393783301115036
iteration 223, loss = 0.25801077485084534
iteration 224, loss = 0.3803831934928894
iteration 225, loss = 0.03132273256778717
iteration 226, loss = 0.2903890013694763
iteration 227, loss = 0.1295696645975113
iteration 228, loss = 0.15218691527843475
iteration 229, loss = 0.21696436405181885
iteration 230, loss = 0.2658100128173828
iteration 231, loss = 0.11334162205457687
iteration 232, loss = 0.1528705358505249
iteration 233, loss = 0.24472832679748535
iteration 234, loss = 0.2478901445865631
iteration 235, loss = 0.21536913514137268
iteration 236, loss = 0.021108001470565796
iteration 237, loss = 0.11615519970655441
iteration 238, loss = 0.1867978870868683
iteration 239, loss = 0.3757590651512146
iteration 240, loss = 0.1240205317735672
iteration 241, loss = 0.09579712152481079
iteration 242, loss = 0.0715145617723465
iteration 243, loss = 0.1706475168466568
iteration 244, loss = 0.10718993097543716
iteration 245, loss = 0.24451950192451477
iteration 246, loss = 0.20657195150852203
iteration 247, loss = 0.3377019762992859
iteration 248, loss = 0.20552340149879456
iteration 249, loss = 0.13864581286907196
iteration 250, loss = 0.21177546679973602
iteration 251, loss = 0.18459023535251617
iteration 252, loss = 0.18575215339660645
iteration 253, loss = 0.28588631749153137
iteration 254, loss = 0.15407079458236694
iteration 255, loss = 0.26148155331611633
iteration 256, loss = 0.1040290892124176
iteration 257, loss = 0.11876390874385834
iteration 258, loss = 0.12319467216730118
iteration 259, loss = 0.043239232152700424
iteration 260, loss = 0.1584223210811615
iteration 261, loss = 0.009610850363969803
iteration 262, loss = 0.24557620286941528
iteration 263, loss = 0.1649072766304016
iteration 264, loss = 0.027657466009259224
iteration 265, loss = 0.4836188852787018
iteration 266, loss = 0.24683260917663574
iteration 267, loss = 0.23684829473495483
iteration 268, loss = 0.051461443305015564
iteration 269, loss = 0.2545546889305115
iteration 270, loss = 0.33095651865005493
iteration 271, loss = 0.2214803397655487
iteration 272, loss = 0.11269459873437881
iteration 273, loss = 0.07862608134746552
iteration 274, loss = 0.2983401417732239
iteration 275, loss = 0.18380774557590485
iteration 276, loss = 0.2024339735507965
iteration 277, loss = 0.06781961023807526
iteration 278, loss = 0.1501498818397522
iteration 279, loss = 0.32191479206085205
iteration 280, loss = 0.15927086770534515
iteration 281, loss = 0.0626494511961937
iteration 282, loss = 0.07440350204706192
iteration 283, loss = 0.1530224233865738
iteration 284, loss = 0.35964930057525635
iteration 285, loss = 0.04671689495444298
iteration 286, loss = 0.1695277988910675
iteration 287, loss = 0.21017125248908997
iteration 288, loss = 0.17593875527381897
iteration 289, loss = 0.33971869945526123
iteration 290, loss = 0.3629770874977112
iteration 291, loss = 0.2954448163509369
iteration 292, loss = 0.17260202765464783
iteration 293, loss = 0.36805349588394165
iteration 294, loss = 0.21170580387115479
iteration 295, loss = 0.19038665294647217
iteration 296, loss = 0.059460386633872986
iteration 297, loss = 0.06765978038311005
iteration 298, loss = 0.1640649437904358
iteration 299, loss = 0.19888460636138916
iteration 0, loss = 0.07715989649295807
iteration 1, loss = 0.2827025055885315
iteration 2, loss = 0.11474285274744034
iteration 3, loss = 0.22963574528694153
iteration 4, loss = 0.09601495414972305
iteration 5, loss = 0.15731698274612427
iteration 6, loss = 0.3043982982635498
iteration 7, loss = 0.18923813104629517
iteration 8, loss = 0.3999182879924774
iteration 9, loss = 0.23153677582740784
iteration 10, loss = 0.316806435585022
iteration 11, loss = 0.23647277057170868
iteration 12, loss = 0.2560386061668396
iteration 13, loss = 0.22659467160701752
iteration 14, loss = 0.5471504330635071
iteration 15, loss = 0.08279149234294891
iteration 16, loss = 0.09730701893568039
iteration 17, loss = 0.19126920402050018
iteration 18, loss = 0.4220097064971924
iteration 19, loss = 0.2949029505252838
iteration 20, loss = 0.3885241448879242
iteration 21, loss = 0.2843545377254486
iteration 22, loss = 0.33927619457244873
iteration 23, loss = 0.113180011510849
iteration 24, loss = 0.17414094507694244
iteration 25, loss = 0.307395339012146
iteration 26, loss = 0.2529217302799225
iteration 27, loss = 0.3285910487174988
iteration 28, loss = 0.26781317591667175
iteration 29, loss = 0.3462790250778198
iteration 30, loss = 0.22228005528450012
iteration 31, loss = 0.2770456373691559
iteration 32, loss = 0.5141711831092834
iteration 33, loss = 0.05339723080396652
iteration 34, loss = 0.0076356930658221245
iteration 35, loss = 0.1899770051240921
iteration 36, loss = 0.2294234335422516
iteration 37, loss = 0.1374858319759369
iteration 38, loss = 0.24877578020095825
iteration 39, loss = 0.2648984491825104
iteration 40, loss = 0.1661664843559265
iteration 41, loss = 0.03754846379160881
iteration 42, loss = 0.19839368760585785
iteration 43, loss = 0.2048804610967636
iteration 44, loss = 0.13088048994541168
iteration 45, loss = 0.2643864154815674
iteration 46, loss = 0.17918822169303894
iteration 47, loss = 0.28621506690979004
iteration 48, loss = 0.2995944917201996
iteration 49, loss = 0.10831177234649658
iteration 50, loss = 0.3099782168865204
iteration 51, loss = 0.10494402050971985
iteration 52, loss = 0.08473022282123566
iteration 53, loss = 0.0282267015427351
iteration 54, loss = 0.2966391444206238
iteration 55, loss = 0.1964641809463501
iteration 56, loss = 0.333482950925827
iteration 57, loss = 0.05737704783678055
iteration 58, loss = 0.19099530577659607
iteration 59, loss = 0.1750848889350891
iteration 60, loss = 0.34136059880256653
iteration 61, loss = 0.21244537830352783
iteration 62, loss = 0.09613148123025894
iteration 63, loss = 0.25522854924201965
iteration 64, loss = 0.14142727851867676
iteration 65, loss = 0.20094943046569824
iteration 66, loss = 0.08616955578327179
iteration 67, loss = 0.17657089233398438
iteration 68, loss = 0.4080740809440613
iteration 69, loss = 0.12847372889518738
iteration 70, loss = 0.3540974259376526
iteration 71, loss = 0.1880192756652832
iteration 72, loss = 0.5130532383918762
iteration 73, loss = 0.1555761843919754
iteration 74, loss = 0.3233382999897003
iteration 75, loss = 0.1845283955335617
iteration 76, loss = 0.09341908991336823
iteration 77, loss = 0.26178374886512756
iteration 78, loss = 0.3720290958881378
iteration 79, loss = 0.24720335006713867
iteration 80, loss = 0.17519846558570862
iteration 81, loss = 0.28032347559928894
iteration 82, loss = 0.0310429185628891
iteration 83, loss = 0.057079482823610306
iteration 84, loss = 0.1834365278482437
iteration 85, loss = 0.18023288249969482
iteration 86, loss = 0.16362163424491882
iteration 87, loss = 0.24993790686130524
iteration 88, loss = 0.16627731919288635
iteration 89, loss = 0.11298252642154694
iteration 90, loss = 0.276074081659317
iteration 91, loss = 0.10865295678377151
iteration 92, loss = 0.014972826465964317
iteration 93, loss = 0.23147407174110413
iteration 94, loss = 0.28154781460762024
iteration 95, loss = 0.23372170329093933
iteration 96, loss = 0.017090924084186554
iteration 97, loss = 0.10613752901554108
iteration 98, loss = 0.18079745769500732
iteration 99, loss = 0.25389787554740906
iteration 100, loss = 0.2532578706741333
iteration 101, loss = 0.02776413783431053
iteration 102, loss = 0.0798601508140564
iteration 103, loss = 0.15387390553951263
iteration 104, loss = 0.018433894962072372
iteration 105, loss = 0.21803602576255798
iteration 106, loss = 0.07702694088220596
iteration 107, loss = 0.2833552360534668
iteration 108, loss = 0.11006179451942444
iteration 109, loss = 0.11167527735233307
iteration 110, loss = 0.12914109230041504
iteration 111, loss = 0.04871150851249695
iteration 112, loss = 0.10215269774198532
iteration 113, loss = 0.4051574170589447
iteration 114, loss = 0.19855546951293945
iteration 115, loss = 0.07678773254156113
iteration 116, loss = 0.23890382051467896
iteration 117, loss = 0.17711561918258667
iteration 118, loss = 0.14152221381664276
iteration 119, loss = 0.506580114364624
iteration 120, loss = 0.5132070183753967
iteration 121, loss = 0.18273383378982544
iteration 122, loss = 0.03235054761171341
iteration 123, loss = 0.36829668283462524
iteration 124, loss = 0.273937463760376
iteration 125, loss = 0.2961844503879547
iteration 126, loss = 0.22443094849586487
iteration 127, loss = 0.21032923460006714
iteration 128, loss = 0.18559890985488892
iteration 129, loss = 0.24229548871517181
iteration 130, loss = 0.2511003017425537
iteration 131, loss = 0.24075421690940857
iteration 132, loss = 0.23478354513645172
iteration 133, loss = 0.27173200249671936
iteration 134, loss = 0.2883787453174591
iteration 135, loss = 0.3049783706665039
iteration 136, loss = 0.1685967743396759
iteration 137, loss = 0.1373572200536728
iteration 138, loss = 0.12301124632358551
iteration 139, loss = 0.213860422372818
iteration 140, loss = 0.16523918509483337
iteration 141, loss = 0.13660182058811188
iteration 142, loss = 0.19120074808597565
iteration 143, loss = 0.08603531122207642
iteration 144, loss = 0.21375301480293274
iteration 145, loss = 0.28316593170166016
iteration 146, loss = 0.20250394940376282
iteration 147, loss = 0.3173716068267822
iteration 148, loss = 0.3404725193977356
iteration 149, loss = 0.1737300008535385
iteration 150, loss = 0.21479439735412598
iteration 151, loss = 0.2270258069038391
iteration 152, loss = 0.15502479672431946
iteration 153, loss = 0.16162976622581482
iteration 154, loss = 0.09924057126045227
iteration 155, loss = 0.04234061762690544
iteration 156, loss = 0.04603026062250137
iteration 157, loss = 0.08206373453140259
iteration 158, loss = 0.20434537529945374
iteration 159, loss = 0.2093639075756073
iteration 160, loss = 0.1284509301185608
iteration 161, loss = 0.18729451298713684
iteration 162, loss = 0.12561745941638947
iteration 163, loss = 0.17512261867523193
iteration 164, loss = 0.12522132694721222
iteration 165, loss = 0.21388936042785645
iteration 166, loss = 0.2839008569717407
iteration 167, loss = 0.11051861196756363
iteration 168, loss = 0.3237287700176239
iteration 169, loss = 0.1649886667728424
iteration 170, loss = 0.08915964514017105
iteration 171, loss = 0.05674394592642784
iteration 172, loss = 0.055693697184324265
iteration 173, loss = 0.3940979242324829
iteration 174, loss = 0.20578637719154358
iteration 175, loss = 0.24199321866035461
iteration 176, loss = 0.07497823983430862
iteration 177, loss = 0.09521427005529404
iteration 178, loss = 0.16918130218982697
iteration 179, loss = 0.16462139785289764
iteration 180, loss = 0.2468678504228592
iteration 181, loss = 0.3262459337711334
iteration 182, loss = 0.028861256316304207
iteration 183, loss = 0.16326460242271423
iteration 184, loss = 0.39752721786499023
iteration 185, loss = 0.08188770711421967
iteration 186, loss = 0.4170096814632416
iteration 187, loss = 0.20501798391342163
iteration 188, loss = 0.28385433554649353
iteration 189, loss = 0.2729809582233429
iteration 190, loss = 0.6494848728179932
iteration 191, loss = 0.3660026788711548
iteration 192, loss = 0.3118089437484741
iteration 193, loss = 0.09062725305557251
iteration 194, loss = 0.18784809112548828
iteration 195, loss = 0.027273347601294518
iteration 196, loss = 0.17425939440727234
iteration 197, loss = 0.2991407513618469
iteration 198, loss = 0.11334716528654099
iteration 199, loss = 0.2665754556655884
iteration 200, loss = 0.174583300948143
iteration 201, loss = 0.0795346349477768
iteration 202, loss = 0.06917489320039749
iteration 203, loss = 0.043180692940950394
iteration 204, loss = 0.035697903484106064
iteration 205, loss = 0.07969889789819717
iteration 206, loss = 0.3861854076385498
iteration 207, loss = 0.2779805660247803
iteration 208, loss = 0.04067404195666313
iteration 209, loss = 0.13906049728393555
iteration 210, loss = 0.10938689112663269
iteration 211, loss = 0.19120477139949799
iteration 212, loss = 0.1628711223602295
iteration 213, loss = 0.08389818668365479
iteration 214, loss = 0.027445100247859955
iteration 215, loss = 0.36888405680656433
iteration 216, loss = 0.32046496868133545
iteration 217, loss = 0.2194247543811798
iteration 218, loss = 0.0589771568775177
iteration 219, loss = 0.3330138623714447
iteration 220, loss = 0.15442506968975067
iteration 221, loss = 0.10678846389055252
iteration 222, loss = 0.08818137645721436
iteration 223, loss = 0.04700160771608353
iteration 224, loss = 0.1360846310853958
iteration 225, loss = 0.026035167276859283
iteration 226, loss = 0.32208383083343506
iteration 227, loss = 0.3091154396533966
iteration 228, loss = 0.19283567368984222
iteration 229, loss = 0.27799373865127563
iteration 230, loss = 0.27825650572776794
iteration 231, loss = 0.1957237720489502
iteration 232, loss = 0.1701284945011139
iteration 233, loss = 0.33375540375709534
iteration 234, loss = 0.06545575708150864
iteration 235, loss = 0.16986702382564545
iteration 236, loss = 0.12678460776805878
iteration 237, loss = 0.12416248768568039
iteration 238, loss = 0.35752275586128235
iteration 239, loss = 0.020900415256619453
iteration 240, loss = 0.11930013447999954
iteration 241, loss = 0.2207312136888504
iteration 242, loss = 0.1342926323413849
iteration 243, loss = 0.14700663089752197
iteration 244, loss = 0.16008390486240387
iteration 245, loss = 0.34825944900512695
iteration 246, loss = 0.32945898175239563
iteration 247, loss = 0.15655279159545898
iteration 248, loss = 0.12558633089065552
iteration 249, loss = 0.07842410355806351
iteration 250, loss = 0.22618645429611206
iteration 251, loss = 0.15883105993270874
iteration 252, loss = 0.14478954672813416
iteration 253, loss = 0.15221741795539856
iteration 254, loss = 0.20270198583602905
iteration 255, loss = 0.16257286071777344
iteration 256, loss = 0.06470242887735367
iteration 257, loss = 0.2142590880393982
iteration 258, loss = 0.13116200268268585
iteration 259, loss = 0.10860573500394821
iteration 260, loss = 0.15476280450820923
iteration 261, loss = 0.1929827630519867
iteration 262, loss = 0.12619701027870178
iteration 263, loss = 0.1950255185365677
iteration 264, loss = 0.09276402741670609
iteration 265, loss = 0.4084237813949585
iteration 266, loss = 0.09555237740278244
iteration 267, loss = 0.1739889681339264
iteration 268, loss = 0.2827448844909668
iteration 269, loss = 0.26131540536880493
iteration 270, loss = 0.13826431334018707
iteration 271, loss = 0.14664793014526367
iteration 272, loss = 0.3298936188220978
iteration 273, loss = 0.14392216503620148
iteration 274, loss = 0.058690425008535385
iteration 275, loss = 0.10537100583314896
iteration 276, loss = 0.20937742292881012
iteration 277, loss = 0.07657809555530548
iteration 278, loss = 0.18570181727409363
iteration 279, loss = 0.24962487816810608
iteration 280, loss = 0.2263028919696808
iteration 281, loss = 0.19488492608070374
iteration 282, loss = 0.08481734991073608
iteration 283, loss = 0.05594341456890106
iteration 284, loss = 0.27934902906417847
iteration 285, loss = 0.16940709948539734
iteration 286, loss = 0.4511987566947937
iteration 287, loss = 0.1907588094472885
iteration 288, loss = 0.25304198265075684
iteration 289, loss = 0.11627227813005447
iteration 290, loss = 0.14184138178825378
iteration 291, loss = 0.21712765097618103
iteration 292, loss = 0.3596287965774536
iteration 293, loss = 0.029861222952604294
iteration 294, loss = 0.10791006684303284
iteration 295, loss = 0.12131292372941971
iteration 296, loss = 0.21762436628341675
iteration 297, loss = 0.11265923827886581
iteration 298, loss = 0.24937427043914795
iteration 299, loss = 0.06924232840538025
iteration 0, loss = 0.31259143352508545
iteration 1, loss = 0.23378810286521912
iteration 2, loss = 0.14183591306209564
iteration 3, loss = 0.18330132961273193
iteration 4, loss = 0.33625733852386475
iteration 5, loss = 0.5151382684707642
iteration 6, loss = 0.11849060654640198
iteration 7, loss = 0.21282526850700378
iteration 8, loss = 0.2802194356918335
iteration 9, loss = 0.08739472180604935
iteration 10, loss = 0.1089923158288002
iteration 11, loss = 0.552061915397644
iteration 12, loss = 0.31474751234054565
iteration 13, loss = 0.43002793192863464
iteration 14, loss = 0.04937945678830147
iteration 15, loss = 0.057991527020931244
iteration 16, loss = 0.12994609773159027
iteration 17, loss = 0.09845148026943207
iteration 18, loss = 0.10640831291675568
iteration 19, loss = 0.17703792452812195
iteration 20, loss = 0.134298637509346
iteration 21, loss = 0.2839067280292511
iteration 22, loss = 0.14160630106925964
iteration 23, loss = 0.393288254737854
iteration 24, loss = 0.20032022893428802
iteration 25, loss = 0.08683144301176071
iteration 26, loss = 0.09408558905124664
iteration 27, loss = 0.4040100872516632
iteration 28, loss = 0.22393591701984406
iteration 29, loss = 0.16744323074817657
iteration 30, loss = 0.1733173131942749
iteration 31, loss = 0.16417241096496582
iteration 32, loss = 0.2211480438709259
iteration 33, loss = 0.1685660183429718
iteration 34, loss = 0.30766987800598145
iteration 35, loss = 0.14065435528755188
iteration 36, loss = 0.07321792840957642
iteration 37, loss = 0.31101909279823303
iteration 38, loss = 0.4248524308204651
iteration 39, loss = 0.34930890798568726
iteration 40, loss = 0.10479363799095154
iteration 41, loss = 0.17946170270442963
iteration 42, loss = 0.17248813807964325
iteration 43, loss = 0.24090883135795593
iteration 44, loss = 0.03455125167965889
iteration 45, loss = 0.21418190002441406
iteration 46, loss = 0.2652585804462433
iteration 47, loss = 0.21592308580875397
iteration 48, loss = 0.008673062548041344
iteration 49, loss = 0.4103771150112152
iteration 50, loss = 0.042177096009254456
iteration 51, loss = 0.5095699429512024
iteration 52, loss = 0.11417366564273834
iteration 53, loss = 0.06916539371013641
iteration 54, loss = 0.2767280042171478
iteration 55, loss = 0.18441914021968842
iteration 56, loss = 0.2834382653236389
iteration 57, loss = 0.13461261987686157
iteration 58, loss = 0.2515285015106201
iteration 59, loss = 0.34410884976387024
iteration 60, loss = 0.5193041563034058
iteration 61, loss = 0.07125045359134674
iteration 62, loss = 0.1940537989139557
iteration 63, loss = 0.18477435410022736
iteration 64, loss = 0.3572763502597809
iteration 65, loss = 0.06906533241271973
iteration 66, loss = 0.1647167056798935
iteration 67, loss = 0.2406253218650818
iteration 68, loss = 0.0943884402513504
iteration 69, loss = 0.22783415019512177
iteration 70, loss = 0.20447388291358948
iteration 71, loss = 0.16499251127243042
iteration 72, loss = 0.13798098266124725
iteration 73, loss = 0.05784580484032631
iteration 74, loss = 0.17026275396347046
iteration 75, loss = 0.24097171425819397
iteration 76, loss = 0.09486990422010422
iteration 77, loss = 0.20429635047912598
iteration 78, loss = 0.40041887760162354
iteration 79, loss = 0.24482214450836182
iteration 80, loss = 0.2927934229373932
iteration 81, loss = 0.04043631628155708
iteration 82, loss = 0.036537013947963715
iteration 83, loss = 0.169139102101326
iteration 84, loss = 0.2963697910308838
iteration 85, loss = 0.2541009485721588
iteration 86, loss = 0.24771656095981598
iteration 87, loss = 0.32468241453170776
iteration 88, loss = 0.037679947912693024
iteration 89, loss = 0.18128934502601624
iteration 90, loss = 0.23331153392791748
iteration 91, loss = 0.2216903120279312
iteration 92, loss = 0.24247165024280548
iteration 93, loss = 0.3225610554218292
iteration 94, loss = 0.185702845454216
iteration 95, loss = 0.17331546545028687
iteration 96, loss = 0.1983572393655777
iteration 97, loss = 0.21649329364299774
iteration 98, loss = 0.23567365109920502
iteration 99, loss = 0.16825760900974274
iteration 100, loss = 0.25670310854911804
iteration 101, loss = 0.25061970949172974
iteration 102, loss = 0.15811850130558014
iteration 103, loss = 0.13769495487213135
iteration 104, loss = 0.2494167983531952
iteration 105, loss = 0.03746892511844635
iteration 106, loss = 0.4381917715072632
iteration 107, loss = 0.2562972605228424
iteration 108, loss = 0.18226751685142517
iteration 109, loss = 0.24713245034217834
iteration 110, loss = 0.0905802845954895
iteration 111, loss = 0.4126853942871094
iteration 112, loss = 0.1468634307384491
iteration 113, loss = 0.1795952320098877
iteration 114, loss = 0.22376646101474762
iteration 115, loss = 0.2909615635871887
iteration 116, loss = 0.1963590681552887
iteration 117, loss = 0.12184084951877594
iteration 118, loss = 0.22863954305648804
iteration 119, loss = 0.3216719329357147
iteration 120, loss = 0.11493902653455734
iteration 121, loss = 0.12862859666347504
iteration 122, loss = 0.2033950239419937
iteration 123, loss = 0.2591548264026642
iteration 124, loss = 0.25800204277038574
iteration 125, loss = 0.25263360142707825
iteration 126, loss = 0.20443184673786163
iteration 127, loss = 0.18166743218898773
iteration 128, loss = 0.35686400532722473
iteration 129, loss = 0.1288667917251587
iteration 130, loss = 0.022456077858805656
iteration 131, loss = 0.07836389541625977
iteration 132, loss = 0.2291022688150406
iteration 133, loss = 0.18007220327854156
iteration 134, loss = 0.26889118552207947
iteration 135, loss = 0.0916491150856018
iteration 136, loss = 0.28363415598869324
iteration 137, loss = 0.18619650602340698
iteration 138, loss = 0.07985199242830276
iteration 139, loss = 0.2746650278568268
iteration 140, loss = 0.1958959698677063
iteration 141, loss = 0.1900012195110321
iteration 142, loss = 0.1379290074110031
iteration 143, loss = 0.10249211639165878
iteration 144, loss = 0.3699996769428253
iteration 145, loss = 0.29994842410087585
iteration 146, loss = 0.4389815330505371
iteration 147, loss = 0.1330975890159607
iteration 148, loss = 0.07048041373491287
iteration 149, loss = 0.3485061526298523
iteration 150, loss = 0.052656110376119614
iteration 151, loss = 0.08814892172813416
iteration 152, loss = 0.15555669367313385
iteration 153, loss = 0.09887941181659698
iteration 154, loss = 0.12388177216053009
iteration 155, loss = 0.23852984607219696
iteration 156, loss = 0.3127874732017517
iteration 157, loss = 0.2180013656616211
iteration 158, loss = 0.21310056746006012
iteration 159, loss = 0.06878162920475006
iteration 160, loss = 0.4492332339286804
iteration 161, loss = 0.2711489498615265
iteration 162, loss = 0.12273330241441727
iteration 163, loss = 0.053551994264125824
iteration 164, loss = 0.26815032958984375
iteration 165, loss = 0.22092586755752563
iteration 166, loss = 0.17035597562789917
iteration 167, loss = 0.24225257337093353
iteration 168, loss = 0.23316293954849243
iteration 169, loss = 0.1676180362701416
iteration 170, loss = 0.3283904790878296
iteration 171, loss = 0.2079104781150818
iteration 172, loss = 0.18441788852214813
iteration 173, loss = 0.08957697451114655
iteration 174, loss = 0.06776272505521774
iteration 175, loss = 0.2159186601638794
iteration 176, loss = 0.1534384787082672
iteration 177, loss = 0.3356715738773346
iteration 178, loss = 0.28959885239601135
iteration 179, loss = 0.17049187421798706
iteration 180, loss = 0.13378269970417023
iteration 181, loss = 0.10314307361841202
iteration 182, loss = 0.0958804339170456
iteration 183, loss = 0.17121848464012146
iteration 184, loss = 0.3827548027038574
iteration 185, loss = 0.35153430700302124
iteration 186, loss = 0.17532530426979065
iteration 187, loss = 0.12896522879600525
iteration 188, loss = 0.06256350129842758
iteration 189, loss = 0.04557223618030548
iteration 190, loss = 0.06461487710475922
iteration 191, loss = 0.2450680136680603
iteration 192, loss = 0.20151585340499878
iteration 193, loss = 0.19187523424625397
iteration 194, loss = 0.1161208301782608
iteration 195, loss = 0.20618687570095062
iteration 196, loss = 0.27940553426742554
iteration 197, loss = 0.4463433623313904
iteration 198, loss = 0.15133468806743622
iteration 199, loss = 0.12002242356538773
iteration 200, loss = 0.05236600711941719
iteration 201, loss = 0.01531394012272358
iteration 202, loss = 0.23799577355384827
iteration 203, loss = 0.3866240978240967
iteration 204, loss = 0.32479381561279297
iteration 205, loss = 0.13297322392463684
iteration 206, loss = 0.1778971254825592
iteration 207, loss = 0.0957123190164566
iteration 208, loss = 0.2354760617017746
iteration 209, loss = 0.3808271884918213
iteration 210, loss = 0.11037898063659668
iteration 211, loss = 0.16023597121238708
iteration 212, loss = 0.23958712816238403
iteration 213, loss = 0.02917657047510147
iteration 214, loss = 0.03575942665338516
iteration 215, loss = 0.32020699977874756
iteration 216, loss = 0.157174214720726
iteration 217, loss = 0.19796954095363617
iteration 218, loss = 0.0437968373298645
iteration 219, loss = 0.22302009165287018
iteration 220, loss = 0.14770489931106567
iteration 221, loss = 0.25665369629859924
iteration 222, loss = 0.19754965603351593
iteration 223, loss = 0.2660560607910156
iteration 224, loss = 0.1626283973455429
iteration 225, loss = 0.277938574552536
iteration 226, loss = 0.11165104806423187
iteration 227, loss = 0.17206934094429016
iteration 228, loss = 0.030536459758877754
iteration 229, loss = 0.1811390072107315
iteration 230, loss = 0.37201839685440063
iteration 231, loss = 0.007334933150559664
iteration 232, loss = 0.3410794734954834
iteration 233, loss = 0.24951957166194916
iteration 234, loss = 0.265724778175354
iteration 235, loss = 0.20626384019851685
iteration 236, loss = 0.18584677577018738
iteration 237, loss = 0.275416761636734
iteration 238, loss = 0.34819871187210083
iteration 239, loss = 0.22495907545089722
iteration 240, loss = 0.053967926651239395
iteration 241, loss = 0.02167901024222374
iteration 242, loss = 0.1914178729057312
iteration 243, loss = 0.07381342351436615
iteration 244, loss = 0.3002825379371643
iteration 245, loss = 0.2776314318180084
iteration 246, loss = 0.09738601744174957
iteration 247, loss = 0.1394369900226593
iteration 248, loss = 0.34623757004737854
iteration 249, loss = 0.2008230984210968
iteration 250, loss = 0.2905474603176117
iteration 251, loss = 0.23540765047073364
iteration 252, loss = 0.19664010405540466
iteration 253, loss = 0.11405418813228607
iteration 254, loss = 0.2236737310886383
iteration 255, loss = 0.01970185711979866
iteration 256, loss = 0.29049286246299744
iteration 257, loss = 0.11253300309181213
iteration 258, loss = 0.1313663125038147
iteration 259, loss = 0.1979866325855255
iteration 260, loss = 0.2780924439430237
iteration 261, loss = 0.12162259221076965
iteration 262, loss = 0.1597995012998581
iteration 263, loss = 0.027126401662826538
iteration 264, loss = 0.22144906222820282
iteration 265, loss = 0.2273811250925064
iteration 266, loss = 0.3473665416240692
iteration 267, loss = 0.12214791029691696
iteration 268, loss = 0.39243194460868835
iteration 269, loss = 0.2424195408821106
iteration 270, loss = 0.3721373379230499
iteration 271, loss = 0.2518628239631653
iteration 272, loss = 0.10475120693445206
iteration 273, loss = 0.6076427102088928
iteration 274, loss = 0.3680947422981262
iteration 275, loss = 0.18460863828659058
iteration 276, loss = 0.07517872005701065
iteration 277, loss = 0.1134352833032608
iteration 278, loss = 0.47385573387145996
iteration 279, loss = 0.2654237747192383
iteration 280, loss = 0.14770527184009552
iteration 281, loss = 0.2512740194797516
iteration 282, loss = 0.19934040307998657
iteration 283, loss = 0.3141479194164276
iteration 284, loss = 0.28126174211502075
iteration 285, loss = 0.09767031669616699
iteration 286, loss = 0.11320421099662781
iteration 287, loss = 0.15296611189842224
iteration 288, loss = 0.16939185559749603
iteration 289, loss = 0.10571293532848358
iteration 290, loss = 0.1583348512649536
iteration 291, loss = 0.11310047656297684
iteration 292, loss = 0.23407408595085144
iteration 293, loss = 0.09629043936729431
iteration 294, loss = 0.2073274552822113
iteration 295, loss = 0.15484334528446198
iteration 296, loss = 0.03521258756518364
iteration 297, loss = 0.10123292356729507
iteration 298, loss = 0.14781071245670319
iteration 299, loss = 0.09069094061851501
iteration 0, loss = 0.05854131653904915
iteration 1, loss = 0.09622154384851456
iteration 2, loss = 0.028919126838445663
iteration 3, loss = 0.018552526831626892
iteration 4, loss = 0.2506950795650482
iteration 5, loss = 0.3905474543571472
iteration 6, loss = 0.2762523591518402
iteration 7, loss = 0.15467214584350586
iteration 8, loss = 0.07493850588798523
iteration 9, loss = 0.2024126648902893
iteration 10, loss = 0.09743824601173401
iteration 11, loss = 0.12229084223508835
iteration 12, loss = 0.20249676704406738
iteration 13, loss = 0.15445448458194733
iteration 14, loss = 0.45597895979881287
iteration 15, loss = 0.15203027427196503
iteration 16, loss = 0.11754901707172394
iteration 17, loss = 0.14788857102394104
iteration 18, loss = 0.09039442986249924
iteration 19, loss = 0.03794699162244797
iteration 20, loss = 0.22416046261787415
iteration 21, loss = 0.1613890528678894
iteration 22, loss = 0.10465100407600403
iteration 23, loss = 0.09763071686029434
iteration 24, loss = 0.1192597970366478
iteration 25, loss = 0.24167633056640625
iteration 26, loss = 0.199863463640213
iteration 27, loss = 0.17318080365657806
iteration 28, loss = 0.04952232539653778
iteration 29, loss = 0.14496785402297974
iteration 30, loss = 0.10751208662986755
iteration 31, loss = 0.09767479449510574
iteration 32, loss = 0.37185317277908325
iteration 33, loss = 0.019305022433400154
iteration 34, loss = 0.18785728514194489
iteration 35, loss = 0.21921683847904205
iteration 36, loss = 0.02647622488439083
iteration 37, loss = 0.31174415349960327
iteration 38, loss = 0.28858721256256104
iteration 39, loss = 0.23796045780181885
iteration 40, loss = 0.2134397029876709
iteration 41, loss = 0.2133249044418335
iteration 42, loss = 0.2835458219051361
iteration 43, loss = 0.40725988149642944
iteration 44, loss = 0.2466137707233429
iteration 45, loss = 0.29553675651550293
iteration 46, loss = 0.24058255553245544
iteration 47, loss = 0.37032657861709595
iteration 48, loss = 0.2089376300573349
iteration 49, loss = 0.09439884126186371
iteration 50, loss = 0.0595746785402298
iteration 51, loss = 0.24481458961963654
iteration 52, loss = 0.06787770241498947
iteration 53, loss = 0.2521333396434784
iteration 54, loss = 0.3133922219276428
iteration 55, loss = 0.2712232768535614
iteration 56, loss = 0.5509238839149475
iteration 57, loss = 0.061048202216625214
iteration 58, loss = 0.2265879511833191
iteration 59, loss = 0.11992322653532028
iteration 60, loss = 0.3089560568332672
iteration 61, loss = 0.20350134372711182
iteration 62, loss = 0.19173943996429443
iteration 63, loss = 0.2161310911178589
iteration 64, loss = 0.08741194754838943
iteration 65, loss = 0.3110468089580536
iteration 66, loss = 0.14250072836875916
iteration 67, loss = 0.12982197105884552
iteration 68, loss = 0.0245102159678936
iteration 69, loss = 0.19629380106925964
iteration 70, loss = 0.07525703310966492
iteration 71, loss = 0.12216685712337494
iteration 72, loss = 0.23205161094665527
iteration 73, loss = 0.29771900177001953
iteration 74, loss = 0.1483026146888733
iteration 75, loss = 0.023103801533579826
iteration 76, loss = 0.308714359998703
iteration 77, loss = 0.2516021132469177
iteration 78, loss = 0.32397350668907166
iteration 79, loss = 0.19748592376708984
iteration 80, loss = 0.10137783735990524
iteration 81, loss = 0.31064707040786743
iteration 82, loss = 0.17729802429676056
iteration 83, loss = 0.11793766170740128
iteration 84, loss = 0.07747577130794525
iteration 85, loss = 0.22825029492378235
iteration 86, loss = 0.0570407398045063
iteration 87, loss = 0.13623479008674622
iteration 88, loss = 0.26688066124916077
iteration 89, loss = 0.2842079699039459
iteration 90, loss = 0.17077338695526123
iteration 91, loss = 0.03527144342660904
iteration 92, loss = 0.1168307512998581
iteration 93, loss = 0.06624216586351395
iteration 94, loss = 0.07709889858961105
iteration 95, loss = 0.15165069699287415
iteration 96, loss = 0.028196848928928375
iteration 97, loss = 0.18935607373714447
iteration 98, loss = 0.1165698915719986
iteration 99, loss = 0.5232058167457581
iteration 100, loss = 0.18188893795013428
iteration 101, loss = 0.11384184658527374
iteration 102, loss = 0.10960811376571655
iteration 103, loss = 0.2074570655822754
iteration 104, loss = 0.3725680410861969
iteration 105, loss = 0.1083805114030838
iteration 106, loss = 0.13724276423454285
iteration 107, loss = 0.09749065339565277
iteration 108, loss = 0.04421541839838028
iteration 109, loss = 0.19225642085075378
iteration 110, loss = 0.41968610882759094
iteration 111, loss = 0.413665771484375
iteration 112, loss = 0.013714553788304329
iteration 113, loss = 0.07911624014377594
iteration 114, loss = 0.24222034215927124
iteration 115, loss = 0.0975169986486435
iteration 116, loss = 0.2328847348690033
iteration 117, loss = 0.1654449850320816
iteration 118, loss = 0.28932446241378784
iteration 119, loss = 0.16455811262130737
iteration 120, loss = 0.13233605027198792
iteration 121, loss = 0.10752113163471222
iteration 122, loss = 0.0401110015809536
iteration 123, loss = 0.048607729375362396
iteration 124, loss = 0.23996557295322418
iteration 125, loss = 0.10969017446041107
iteration 126, loss = 0.1844015121459961
iteration 127, loss = 0.38389793038368225
iteration 128, loss = 0.19025525450706482
iteration 129, loss = 0.17523042857646942
iteration 130, loss = 0.32395607233047485
iteration 131, loss = 0.358630895614624
iteration 132, loss = 0.3172123432159424
iteration 133, loss = 0.25353506207466125
iteration 134, loss = 0.19366465508937836
iteration 135, loss = 0.23915642499923706
iteration 136, loss = 0.11747755110263824
iteration 137, loss = 0.2036249190568924
iteration 138, loss = 0.01802990771830082
iteration 139, loss = 0.23248162865638733
iteration 140, loss = 0.1281575709581375
iteration 141, loss = 0.3277248740196228
iteration 142, loss = 0.32085859775543213
iteration 143, loss = 0.059525951743125916
iteration 144, loss = 0.25555619597435
iteration 145, loss = 0.5831735134124756
iteration 146, loss = 0.06829007714986801
iteration 147, loss = 0.03913591429591179
iteration 148, loss = 0.342120885848999
iteration 149, loss = 0.11529235541820526
iteration 150, loss = 0.2894684672355652
iteration 151, loss = 0.41304537653923035
iteration 152, loss = 0.32791998982429504
iteration 153, loss = 0.21725791692733765
iteration 154, loss = 0.20777960121631622
iteration 155, loss = 0.2990499436855316
iteration 156, loss = 0.0915135070681572
iteration 157, loss = 0.28025391697883606
iteration 158, loss = 0.1484704315662384
iteration 159, loss = 0.24668344855308533
iteration 160, loss = 0.20227111876010895
iteration 161, loss = 0.10743409395217896
iteration 162, loss = 0.18073609471321106
iteration 163, loss = 0.2953992486000061
iteration 164, loss = 0.2419968545436859
iteration 165, loss = 0.4367421269416809
iteration 166, loss = 0.26177966594696045
iteration 167, loss = 0.311644971370697
iteration 168, loss = 0.30548402667045593
iteration 169, loss = 0.09109970182180405
iteration 170, loss = 0.16073325276374817
iteration 171, loss = 0.1739371418952942
iteration 172, loss = 0.17855484783649445
iteration 173, loss = 0.4009683430194855
iteration 174, loss = 0.1796247363090515
iteration 175, loss = 0.040432266891002655
iteration 176, loss = 0.04863583296537399
iteration 177, loss = 0.14482101798057556
iteration 178, loss = 0.26050955057144165
iteration 179, loss = 0.03412214294075966
iteration 180, loss = 0.24243834614753723
iteration 181, loss = 0.16027355194091797
iteration 182, loss = 0.12330873310565948
iteration 183, loss = 0.13894686102867126
iteration 184, loss = 0.3437589406967163
iteration 185, loss = 0.09417444467544556
iteration 186, loss = 0.03157994523644447
iteration 187, loss = 0.34050312638282776
iteration 188, loss = 0.3110583424568176
iteration 189, loss = 0.023667877539992332
iteration 190, loss = 0.22831910848617554
iteration 191, loss = 0.27418825030326843
iteration 192, loss = 0.09933716058731079
iteration 193, loss = 0.2302844375371933
iteration 194, loss = 0.078978031873703
iteration 195, loss = 0.5065860152244568
iteration 196, loss = 0.2025541514158249
iteration 197, loss = 0.7867298722267151
iteration 198, loss = 0.15888364613056183
iteration 199, loss = 0.23476402461528778
iteration 200, loss = 0.288961261510849
iteration 201, loss = 0.27014827728271484
iteration 202, loss = 0.20196400582790375
iteration 203, loss = 0.653313398361206
iteration 204, loss = 0.23688915371894836
iteration 205, loss = 0.12221098691225052
iteration 206, loss = 0.06720472872257233
iteration 207, loss = 0.014546305872499943
iteration 208, loss = 0.650122880935669
iteration 209, loss = 0.17173010110855103
iteration 210, loss = 0.4574020504951477
iteration 211, loss = 0.289802610874176
iteration 212, loss = 0.2694077491760254
iteration 213, loss = 0.15693874657154083
iteration 214, loss = 0.4033946096897125
iteration 215, loss = 0.17528486251831055
iteration 216, loss = 0.19833889603614807
iteration 217, loss = 0.2604810297489166
iteration 218, loss = 0.10897648334503174
iteration 219, loss = 0.2812517285346985
iteration 220, loss = 0.21385957300662994
iteration 221, loss = 0.11699149012565613
iteration 222, loss = 0.24996285140514374
iteration 223, loss = 0.2335813343524933
iteration 224, loss = 0.1842210292816162
iteration 225, loss = 0.7579984068870544
iteration 226, loss = 0.6409480571746826
iteration 227, loss = 0.23671364784240723
iteration 228, loss = 0.24033573269844055
iteration 229, loss = 0.4801018238067627
iteration 230, loss = 0.14431321620941162
iteration 231, loss = 0.19575127959251404
iteration 232, loss = 0.38945165276527405
iteration 233, loss = 0.10534355044364929
iteration 234, loss = 0.24676702916622162
iteration 235, loss = 0.41936129331588745
iteration 236, loss = 0.08131048828363419
iteration 237, loss = 0.032639872282743454
iteration 238, loss = 0.15276098251342773
iteration 239, loss = 0.243284672498703
iteration 240, loss = 0.04465785250067711
iteration 241, loss = 0.23193348944187164
iteration 242, loss = 0.46391561627388
iteration 243, loss = 0.10463710874319077
iteration 244, loss = 0.05304620414972305
iteration 245, loss = 0.0977770984172821
iteration 246, loss = 0.2132335752248764
iteration 247, loss = 0.10896053910255432
iteration 248, loss = 0.29375845193862915
iteration 249, loss = 0.3762708902359009
iteration 250, loss = 0.2589734196662903
iteration 251, loss = 0.274364173412323
iteration 252, loss = 0.1983785182237625
iteration 253, loss = 0.25401970744132996
iteration 254, loss = 0.22387731075286865
iteration 255, loss = 0.20433370769023895
iteration 256, loss = 0.220455139875412
iteration 257, loss = 0.08820486068725586
iteration 258, loss = 0.06829031556844711
iteration 259, loss = 0.24731260538101196
iteration 260, loss = 0.1393439769744873
iteration 261, loss = 0.09047424048185349
iteration 262, loss = 0.1639336198568344
iteration 263, loss = 0.16583283245563507
iteration 264, loss = 0.12862883508205414
iteration 265, loss = 0.21491393446922302
iteration 266, loss = 0.4591344892978668
iteration 267, loss = 0.1771397739648819
iteration 268, loss = 0.28286975622177124
iteration 269, loss = 0.32467085123062134
iteration 270, loss = 0.15160344541072845
iteration 271, loss = 0.29746225476264954
iteration 272, loss = 0.10478629916906357
iteration 273, loss = 0.1177540197968483
iteration 274, loss = 0.17727790772914886
iteration 275, loss = 0.5015084147453308
iteration 276, loss = 0.033999282866716385
iteration 277, loss = 0.4525277018547058
iteration 278, loss = 0.01935969479382038
iteration 279, loss = 0.16889792680740356
iteration 280, loss = 0.20271071791648865
iteration 281, loss = 0.22306740283966064
iteration 282, loss = 0.31674861907958984
iteration 283, loss = 0.3781818151473999
iteration 284, loss = 0.204970121383667
iteration 285, loss = 0.12943227589130402
iteration 286, loss = 0.11077078431844711
iteration 287, loss = 0.18221108615398407
iteration 288, loss = 0.1833946853876114
iteration 289, loss = 0.011679082177579403
iteration 290, loss = 0.3982332646846771
iteration 291, loss = 0.18095432221889496
iteration 292, loss = 0.03806047886610031
iteration 293, loss = 0.17055833339691162
iteration 294, loss = 0.1217484325170517
iteration 295, loss = 0.19480043649673462
iteration 296, loss = 0.4098062515258789
iteration 297, loss = 0.06497156620025635
iteration 298, loss = 0.22800812125205994
iteration 299, loss = 0.14263880252838135
iteration 0, loss = 0.23161400854587555
iteration 1, loss = 0.186664417386055
iteration 2, loss = 0.22219720482826233
iteration 3, loss = 0.31909406185150146
iteration 4, loss = 0.12338098883628845
iteration 5, loss = 0.12929672002792358
iteration 6, loss = 0.0866527184844017
iteration 7, loss = 0.31563812494277954
iteration 8, loss = 0.16148748993873596
iteration 9, loss = 0.03223158046603203
iteration 10, loss = 0.09621629118919373
iteration 11, loss = 0.16493886709213257
iteration 12, loss = 0.1258116066455841
iteration 13, loss = 0.026463666930794716
iteration 14, loss = 0.04768284782767296
iteration 15, loss = 0.4886031150817871
iteration 16, loss = 0.23322473466396332
iteration 17, loss = 0.11285216361284256
iteration 18, loss = 0.01411679107695818
iteration 19, loss = 0.2548341155052185
iteration 20, loss = 0.18036125600337982
iteration 21, loss = 0.4037620723247528
iteration 22, loss = 0.12446335703134537
iteration 23, loss = 0.0900547206401825
iteration 24, loss = 0.16186094284057617
iteration 25, loss = 0.3752971291542053
iteration 26, loss = 0.2741766571998596
iteration 27, loss = 0.31244727969169617
iteration 28, loss = 0.040387142449617386
iteration 29, loss = 0.3215721547603607
iteration 30, loss = 0.11072270572185516
iteration 31, loss = 0.10866055637598038
iteration 32, loss = 0.18152713775634766
iteration 33, loss = 0.21476569771766663
iteration 34, loss = 0.08271490782499313
iteration 35, loss = 0.2548461854457855
iteration 36, loss = 0.043219633400440216
iteration 37, loss = 0.24060431122779846
iteration 38, loss = 0.009243693202733994
iteration 39, loss = 0.07969368249177933
iteration 40, loss = 0.14286720752716064
iteration 41, loss = 0.07273676246404648
iteration 42, loss = 0.2613968551158905
iteration 43, loss = 0.1846313178539276
iteration 44, loss = 0.017139015719294548
iteration 45, loss = 0.12044138461351395
iteration 46, loss = 0.22887906432151794
iteration 47, loss = 0.20402425527572632
iteration 48, loss = 0.1858942210674286
iteration 49, loss = 0.20285078883171082
iteration 50, loss = 0.20450156927108765
iteration 51, loss = 0.2496219426393509
iteration 52, loss = 0.07595641165971756
iteration 53, loss = 0.15296299755573273
iteration 54, loss = 0.08601748943328857
iteration 55, loss = 0.14464250206947327
iteration 56, loss = 0.17482511699199677
iteration 57, loss = 0.21471159160137177
iteration 58, loss = 0.0677836686372757
iteration 59, loss = 0.31252169609069824
iteration 60, loss = 0.12247194349765778
iteration 61, loss = 0.14364469051361084
iteration 62, loss = 0.14417621493339539
iteration 63, loss = 0.15256962180137634
iteration 64, loss = 0.1776716709136963
iteration 65, loss = 0.12300330400466919
iteration 66, loss = 0.1179027408361435
iteration 67, loss = 0.32737261056900024
iteration 68, loss = 0.25727540254592896
iteration 69, loss = 0.15078812837600708
iteration 70, loss = 0.08475657552480698
iteration 71, loss = 0.16337549686431885
iteration 72, loss = 0.1307118535041809
iteration 73, loss = 0.15067419409751892
iteration 74, loss = 0.15267646312713623
iteration 75, loss = 0.2726759612560272
iteration 76, loss = 0.12047368288040161
iteration 77, loss = 0.21438658237457275
iteration 78, loss = 0.25371676683425903
iteration 79, loss = 0.18667542934417725
iteration 80, loss = 0.11704526096582413
iteration 81, loss = 0.1938662827014923
iteration 82, loss = 0.4513823390007019
iteration 83, loss = 0.20772132277488708
iteration 84, loss = 0.15763995051383972
iteration 85, loss = 0.24118052423000336
iteration 86, loss = 0.16479846835136414
iteration 87, loss = 0.2217072993516922
iteration 88, loss = 0.19526037573814392
iteration 89, loss = 0.21487507224082947
iteration 90, loss = 0.0941455215215683
iteration 91, loss = 0.1756264567375183
iteration 92, loss = 0.025159616023302078
iteration 93, loss = 0.1460305154323578
iteration 94, loss = 0.07669725269079208
iteration 95, loss = 0.13054080307483673
iteration 96, loss = 0.2265857309103012
iteration 97, loss = 0.21539188921451569
iteration 98, loss = 0.09645860642194748
iteration 99, loss = 0.13337716460227966
iteration 100, loss = 0.2161911576986313
iteration 101, loss = 0.127839595079422
iteration 102, loss = 0.24331018328666687
iteration 103, loss = 0.10615640133619308
iteration 104, loss = 0.07530009001493454
iteration 105, loss = 0.02628672495484352
iteration 106, loss = 0.09058941900730133
iteration 107, loss = 0.29018306732177734
iteration 108, loss = 0.010016045533120632
iteration 109, loss = 0.007773632183670998
iteration 110, loss = 0.011770987883210182
iteration 111, loss = 0.4419479966163635
iteration 112, loss = 0.22971448302268982
iteration 113, loss = 0.6340134739875793
iteration 114, loss = 0.16125918924808502
iteration 115, loss = 0.43915045261383057
iteration 116, loss = 0.20959889888763428
iteration 117, loss = 0.18854358792304993
iteration 118, loss = 0.2559923827648163
iteration 119, loss = 0.16829514503479004
iteration 120, loss = 0.13030293583869934
iteration 121, loss = 0.2144254744052887
iteration 122, loss = 0.14731936156749725
iteration 123, loss = 0.16174136102199554
iteration 124, loss = 0.2555510103702545
iteration 125, loss = 0.34752532839775085
iteration 126, loss = 0.12703189253807068
iteration 127, loss = 0.2125582993030548
iteration 128, loss = 0.24534587562084198
iteration 129, loss = 0.1659616231918335
iteration 130, loss = 0.14710311591625214
iteration 131, loss = 0.3432219326496124
iteration 132, loss = 0.39284318685531616
iteration 133, loss = 0.17610496282577515
iteration 134, loss = 0.2070571631193161
iteration 135, loss = 0.11549365520477295
iteration 136, loss = 0.18002845346927643
iteration 137, loss = 0.20904061198234558
iteration 138, loss = 0.14562179148197174
iteration 139, loss = 0.08683983981609344
iteration 140, loss = 0.32320716977119446
iteration 141, loss = 0.4368205964565277
iteration 142, loss = 0.39328861236572266
iteration 143, loss = 0.22695660591125488
iteration 144, loss = 0.19548119604587555
iteration 145, loss = 0.16026775538921356
iteration 146, loss = 0.16115757822990417
iteration 147, loss = 0.3201408386230469
iteration 148, loss = 0.09365029633045197
iteration 149, loss = 0.08318699151277542
iteration 150, loss = 0.015859175473451614
iteration 151, loss = 0.10992724448442459
iteration 152, loss = 0.2227124273777008
iteration 153, loss = 0.3005240261554718
iteration 154, loss = 0.13635455071926117
iteration 155, loss = 0.06935721635818481
iteration 156, loss = 0.135176420211792
iteration 157, loss = 0.2008366733789444
iteration 158, loss = 0.38586658239364624
iteration 159, loss = 0.07172441482543945
iteration 160, loss = 0.383105993270874
iteration 161, loss = 0.10650736093521118
iteration 162, loss = 0.20962703227996826
iteration 163, loss = 0.06019359081983566
iteration 164, loss = 0.034263886511325836
iteration 165, loss = 0.3684820830821991
iteration 166, loss = 0.30879661440849304
iteration 167, loss = 0.2503489553928375
iteration 168, loss = 0.14138486981391907
iteration 169, loss = 0.10367046296596527
iteration 170, loss = 0.19538454711437225
iteration 171, loss = 0.16392315924167633
iteration 172, loss = 0.11667047441005707
iteration 173, loss = 0.13622035086154938
iteration 174, loss = 0.20025189220905304
iteration 175, loss = 0.16353574395179749
iteration 176, loss = 0.3011169135570526
iteration 177, loss = 0.12009045481681824
iteration 178, loss = 0.18538641929626465
iteration 179, loss = 0.23392316699028015
iteration 180, loss = 0.2035856395959854
iteration 181, loss = 0.15389515459537506
iteration 182, loss = 0.3354381322860718
iteration 183, loss = 0.14958396553993225
iteration 184, loss = 0.1397302746772766
iteration 185, loss = 0.07718072831630707
iteration 186, loss = 0.1889251470565796
iteration 187, loss = 0.12106640636920929
iteration 188, loss = 0.08783001452684402
iteration 189, loss = 0.21462517976760864
iteration 190, loss = 0.04826267808675766
iteration 191, loss = 0.27655869722366333
iteration 192, loss = 0.19817164540290833
iteration 193, loss = 0.16236558556556702
iteration 194, loss = 0.04464627802371979
iteration 195, loss = 0.020516276359558105
iteration 196, loss = 0.13616181910037994
iteration 197, loss = 0.1777821183204651
iteration 198, loss = 0.16938075423240662
iteration 199, loss = 0.028513582423329353
iteration 200, loss = 0.35021525621414185
iteration 201, loss = 0.1934599131345749
iteration 202, loss = 0.18799766898155212
iteration 203, loss = 0.19331926107406616
iteration 204, loss = 0.01751451939344406
iteration 205, loss = 0.2215331792831421
iteration 206, loss = 0.20991070568561554
iteration 207, loss = 0.5964573621749878
iteration 208, loss = 0.273433119058609
iteration 209, loss = 0.18314288556575775
iteration 210, loss = 0.10679812729358673
iteration 211, loss = 0.4933125078678131
iteration 212, loss = 0.19181643426418304
iteration 213, loss = 0.10775042325258255
iteration 214, loss = 0.5480489134788513
iteration 215, loss = 0.4971878230571747
iteration 216, loss = 0.19195127487182617
iteration 217, loss = 0.42216649651527405
iteration 218, loss = 0.12494218349456787
iteration 219, loss = 0.06523730605840683
iteration 220, loss = 0.5316326022148132
iteration 221, loss = 0.006336573511362076
iteration 222, loss = 0.3511837422847748
iteration 223, loss = 0.018866555765271187
iteration 224, loss = 0.21321739256381989
iteration 225, loss = 0.4701007008552551
iteration 226, loss = 0.46729058027267456
iteration 227, loss = 0.602022647857666
iteration 228, loss = 0.2533017694950104
iteration 229, loss = 0.2853006422519684
iteration 230, loss = 0.15119576454162598
iteration 231, loss = 0.31838369369506836
iteration 232, loss = 0.4582344889640808
iteration 233, loss = 0.39819929003715515
iteration 234, loss = 0.3137606382369995
iteration 235, loss = 0.27315348386764526
iteration 236, loss = 0.18989244103431702
iteration 237, loss = 0.113101065158844
iteration 238, loss = 0.13854898512363434
iteration 239, loss = 0.10293976962566376
iteration 240, loss = 0.24562767148017883
iteration 241, loss = 0.43898990750312805
iteration 242, loss = 0.2035592496395111
iteration 243, loss = 0.676548182964325
iteration 244, loss = 0.3340936005115509
iteration 245, loss = 0.17806334793567657
iteration 246, loss = 0.10816120356321335
iteration 247, loss = 0.19579271972179413
iteration 248, loss = 0.3938082456588745
iteration 249, loss = 0.5391138792037964
iteration 250, loss = 0.7003672122955322
iteration 251, loss = 0.19673988223075867
iteration 252, loss = 0.2365020215511322
iteration 253, loss = 0.30890196561813354
iteration 254, loss = 0.17530477046966553
iteration 255, loss = 0.26280277967453003
iteration 256, loss = 0.2660914361476898
iteration 257, loss = 0.17080414295196533
iteration 258, loss = 0.04386762157082558
iteration 259, loss = 0.3429398536682129
iteration 260, loss = 0.2638967037200928
iteration 261, loss = 0.16618475317955017
iteration 262, loss = 0.24976855516433716
iteration 263, loss = 0.09827489405870438
iteration 264, loss = 0.2345493882894516
iteration 265, loss = 0.1497480720281601
iteration 266, loss = 0.03737976774573326
iteration 267, loss = 0.20960493385791779
iteration 268, loss = 0.23726798593997955
iteration 269, loss = 0.2042774260044098
iteration 270, loss = 0.12129736691713333
iteration 271, loss = 0.15627002716064453
iteration 272, loss = 0.2728493809700012
iteration 273, loss = 0.0714285746216774
iteration 274, loss = 0.1615704894065857
iteration 275, loss = 0.2603018283843994
iteration 276, loss = 0.22566884756088257
iteration 277, loss = 0.07921944558620453
iteration 278, loss = 0.13800561428070068
iteration 279, loss = 0.11600364744663239
iteration 280, loss = 0.13603778183460236
iteration 281, loss = 0.21372953057289124
iteration 282, loss = 0.24044784903526306
iteration 283, loss = 0.16586577892303467
iteration 284, loss = 0.16498102247714996
iteration 285, loss = 0.05915028601884842
iteration 286, loss = 0.17721670866012573
iteration 287, loss = 0.13575290143489838
iteration 288, loss = 0.15848222374916077
iteration 289, loss = 0.18554656207561493
iteration 290, loss = 0.07504980266094208
iteration 291, loss = 0.1141713559627533
iteration 292, loss = 0.041158247739076614
iteration 293, loss = 0.21337026357650757
iteration 294, loss = 0.1393568217754364
iteration 295, loss = 0.26316189765930176
iteration 296, loss = 0.06947159767150879
iteration 297, loss = 0.43497157096862793
iteration 298, loss = 0.2704651653766632
iteration 299, loss = 0.05470598116517067
iteration 0, loss = 0.13606560230255127
iteration 1, loss = 0.03577546402812004
iteration 2, loss = 0.11847709119319916
iteration 3, loss = 0.12397197633981705
iteration 4, loss = 0.2054574340581894
iteration 5, loss = 0.36936119198799133
iteration 6, loss = 0.208933487534523
iteration 7, loss = 0.2963784635066986
iteration 8, loss = 0.15993759036064148
iteration 9, loss = 0.049838919192552567
iteration 10, loss = 0.23913906514644623
iteration 11, loss = 0.2396456003189087
iteration 12, loss = 0.15507102012634277
iteration 13, loss = 0.32413142919540405
iteration 14, loss = 0.2676137387752533
iteration 15, loss = 0.12907685339450836
iteration 16, loss = 0.18051496148109436
iteration 17, loss = 0.26240482926368713
iteration 18, loss = 0.11134221404790878
iteration 19, loss = 0.01348043605685234
iteration 20, loss = 0.2862732410430908
iteration 21, loss = 0.014745190739631653
iteration 22, loss = 0.12524157762527466
iteration 23, loss = 0.1978449523448944
iteration 24, loss = 0.474743515253067
iteration 25, loss = 0.23297011852264404
iteration 26, loss = 0.25668609142303467
iteration 27, loss = 0.3601713180541992
iteration 28, loss = 0.2085965871810913
iteration 29, loss = 0.15754646062850952
iteration 30, loss = 0.11683370172977448
iteration 31, loss = 0.11106343567371368
iteration 32, loss = 0.17338800430297852
iteration 33, loss = 0.08995169401168823
iteration 34, loss = 0.24407044053077698
iteration 35, loss = 0.21312165260314941
iteration 36, loss = 0.014283285476267338
iteration 37, loss = 0.1490512639284134
iteration 38, loss = 0.26293647289276123
iteration 39, loss = 0.1324392706155777
iteration 40, loss = 0.054293882101774216
iteration 41, loss = 0.3453178107738495
iteration 42, loss = 0.10028883069753647
iteration 43, loss = 0.02501644752919674
iteration 44, loss = 0.2437591701745987
iteration 45, loss = 0.12661759555339813
iteration 46, loss = 0.052069149911403656
iteration 47, loss = 0.11898383498191833
iteration 48, loss = 0.2691912055015564
iteration 49, loss = 0.282166987657547
iteration 50, loss = 0.16493958234786987
iteration 51, loss = 0.1554892659187317
iteration 52, loss = 0.039245519787073135
iteration 53, loss = 0.03076310269534588
iteration 54, loss = 0.17564409971237183
iteration 55, loss = 0.31800559163093567
iteration 56, loss = 0.1195773035287857
iteration 57, loss = 0.5366091132164001
iteration 58, loss = 0.047711122781038284
iteration 59, loss = 0.14282236993312836
iteration 60, loss = 0.23642727732658386
iteration 61, loss = 0.22649282217025757
iteration 62, loss = 0.11236884444952011
iteration 63, loss = 0.20799985527992249
iteration 64, loss = 0.12435140460729599
iteration 65, loss = 0.15294063091278076
iteration 66, loss = 0.3664624094963074
iteration 67, loss = 0.013999625109136105
iteration 68, loss = 0.07226720452308655
iteration 69, loss = 0.046377502381801605
iteration 70, loss = 0.16094917058944702
iteration 71, loss = 0.13744917511940002
iteration 72, loss = 0.2627451717853546
iteration 73, loss = 0.18303528428077698
iteration 74, loss = 0.06636331975460052
iteration 75, loss = 0.23908361792564392
iteration 76, loss = 0.3166719079017639
iteration 77, loss = 0.06629706174135208
iteration 78, loss = 0.19880831241607666
iteration 79, loss = 0.20442286133766174
iteration 80, loss = 0.0408170223236084
iteration 81, loss = 0.188696026802063
iteration 82, loss = 0.07770688086748123
iteration 83, loss = 0.37569162249565125
iteration 84, loss = 0.08949720859527588
iteration 85, loss = 0.13363200426101685
iteration 86, loss = 0.07841656357049942
iteration 87, loss = 0.2634078860282898
iteration 88, loss = 0.13358592987060547
iteration 89, loss = 0.17615026235580444
iteration 90, loss = 0.14447227120399475
iteration 91, loss = 0.058210164308547974
iteration 92, loss = 0.018181178718805313
iteration 93, loss = 0.5685229897499084
iteration 94, loss = 0.4731585681438446
iteration 95, loss = 0.07868096977472305
iteration 96, loss = 0.6745102405548096
iteration 97, loss = 0.015693573281168938
iteration 98, loss = 0.07585033029317856
iteration 99, loss = 0.36649835109710693
iteration 100, loss = 0.2505652606487274
iteration 101, loss = 0.26706546545028687
iteration 102, loss = 0.15565825998783112
iteration 103, loss = 0.27899956703186035
iteration 104, loss = 0.14260342717170715
iteration 105, loss = 0.02453937567770481
iteration 106, loss = 0.0077495276927948
iteration 107, loss = 0.6986398100852966
iteration 108, loss = 0.22223052382469177
iteration 109, loss = 0.589563250541687
iteration 110, loss = 0.33989059925079346
iteration 111, loss = 0.4808827340602875
iteration 112, loss = 0.04938972741365433
iteration 113, loss = 0.12383713573217392
iteration 114, loss = 0.18783316016197205
iteration 115, loss = 0.07608439028263092
iteration 116, loss = 0.14764045178890228
iteration 117, loss = 0.12343436479568481
iteration 118, loss = 0.2067708522081375
iteration 119, loss = 0.23690393567085266
iteration 120, loss = 0.18042033910751343
iteration 121, loss = 0.022824177518486977
iteration 122, loss = 0.1701333224773407
iteration 123, loss = 0.4451708495616913
iteration 124, loss = 0.0649835392832756
iteration 125, loss = 0.3600236773490906
iteration 126, loss = 0.2439166009426117
iteration 127, loss = 0.27917546033859253
iteration 128, loss = 0.2246370166540146
iteration 129, loss = 0.13179408013820648
iteration 130, loss = 0.2506851553916931
iteration 131, loss = 0.24156606197357178
iteration 132, loss = 0.16588595509529114
iteration 133, loss = 0.23481230437755585
iteration 134, loss = 0.22442036867141724
iteration 135, loss = 0.37615275382995605
iteration 136, loss = 0.20688652992248535
iteration 137, loss = 0.3479519486427307
iteration 138, loss = 0.03662627935409546
iteration 139, loss = 0.20289772748947144
iteration 140, loss = 0.12774905562400818
iteration 141, loss = 0.17570632696151733
iteration 142, loss = 0.15204620361328125
iteration 143, loss = 0.18300297856330872
iteration 144, loss = 0.1615176498889923
iteration 145, loss = 0.09713386744260788
iteration 146, loss = 0.08392373472452164
iteration 147, loss = 0.3017350733280182
iteration 148, loss = 0.24520093202590942
iteration 149, loss = 0.21130774915218353
iteration 150, loss = 0.587033212184906
iteration 151, loss = 0.18375156819820404
iteration 152, loss = 0.1047699972987175
iteration 153, loss = 0.23761676251888275
iteration 154, loss = 0.1986410915851593
iteration 155, loss = 0.1955304592847824
iteration 156, loss = 0.19015929102897644
iteration 157, loss = 0.049419887363910675
iteration 158, loss = 0.18711957335472107
iteration 159, loss = 0.02228200063109398
iteration 160, loss = 0.10651195049285889
iteration 161, loss = 0.12268486618995667
iteration 162, loss = 0.2012791633605957
iteration 163, loss = 0.2095799744129181
iteration 164, loss = 0.0892980769276619
iteration 165, loss = 0.1934308558702469
iteration 166, loss = 0.167131245136261
iteration 167, loss = 0.051369957625865936
iteration 168, loss = 0.13440243899822235
iteration 169, loss = 0.09945148229598999
iteration 170, loss = 0.18538343906402588
iteration 171, loss = 0.0732773020863533
iteration 172, loss = 0.254047155380249
iteration 173, loss = 0.06664041429758072
iteration 174, loss = 0.03418770805001259
iteration 175, loss = 0.22743108868598938
iteration 176, loss = 0.23687800765037537
iteration 177, loss = 0.23803594708442688
iteration 178, loss = 0.25133517384529114
iteration 179, loss = 0.2667897939682007
iteration 180, loss = 0.13257311284542084
iteration 181, loss = 0.23610025644302368
iteration 182, loss = 0.23139792680740356
iteration 183, loss = 0.15896880626678467
iteration 184, loss = 0.3577638864517212
iteration 185, loss = 0.16388629376888275
iteration 186, loss = 0.18545839190483093
iteration 187, loss = 0.205728217959404
iteration 188, loss = 0.1304854452610016
iteration 189, loss = 0.3808876872062683
iteration 190, loss = 0.13779576122760773
iteration 191, loss = 0.25826260447502136
iteration 192, loss = 0.210790753364563
iteration 193, loss = 0.02005518227815628
iteration 194, loss = 0.548197329044342
iteration 195, loss = 0.04134068638086319
iteration 196, loss = 0.326236367225647
iteration 197, loss = 0.19692625105381012
iteration 198, loss = 0.0979827344417572
iteration 199, loss = 0.1616530567407608
iteration 200, loss = 0.21536338329315186
iteration 201, loss = 0.15964442491531372
iteration 202, loss = 0.110073983669281
iteration 203, loss = 0.15175333619117737
iteration 204, loss = 0.04149426519870758
iteration 205, loss = 0.10804170370101929
iteration 206, loss = 0.24540595710277557
iteration 207, loss = 0.0887090265750885
iteration 208, loss = 0.36582714319229126
iteration 209, loss = 0.11579765379428864
iteration 210, loss = 0.17465521395206451
iteration 211, loss = 0.22223380208015442
iteration 212, loss = 0.15051284432411194
iteration 213, loss = 0.2076389491558075
iteration 214, loss = 0.32137295603752136
iteration 215, loss = 0.07114765793085098
iteration 216, loss = 0.32171326875686646
iteration 217, loss = 0.0952666699886322
iteration 218, loss = 0.17482726275920868
iteration 219, loss = 0.2543888986110687
iteration 220, loss = 0.23501266539096832
iteration 221, loss = 0.2930578887462616
iteration 222, loss = 0.0995759665966034
iteration 223, loss = 0.11591418832540512
iteration 224, loss = 0.2577252984046936
iteration 225, loss = 0.11852619051933289
iteration 226, loss = 0.15937699377536774
iteration 227, loss = 0.08632171154022217
iteration 228, loss = 0.24885520339012146
iteration 229, loss = 0.1737135499715805
iteration 230, loss = 0.06942854821681976
iteration 231, loss = 0.10338573902845383
iteration 232, loss = 0.2777324914932251
iteration 233, loss = 0.349351704120636
iteration 234, loss = 0.1378820389509201
iteration 235, loss = 0.11783707141876221
iteration 236, loss = 0.1311362087726593
iteration 237, loss = 0.2041810154914856
iteration 238, loss = 0.06952853500843048
iteration 239, loss = 0.2083456665277481
iteration 240, loss = 0.15350839495658875
iteration 241, loss = 0.23312260210514069
iteration 242, loss = 0.14398328959941864
iteration 243, loss = 0.1122426763176918
iteration 244, loss = 0.2584763169288635
iteration 245, loss = 0.14388233423233032
iteration 246, loss = 0.10454025864601135
iteration 247, loss = 0.12438507378101349
iteration 248, loss = 0.14180895686149597
iteration 249, loss = 0.14439181983470917
iteration 250, loss = 0.20662081241607666
iteration 251, loss = 0.2610478103160858
iteration 252, loss = 0.08043818920850754
iteration 253, loss = 0.230707585811615
iteration 254, loss = 0.17177553474903107
iteration 255, loss = 0.11785845458507538
iteration 256, loss = 0.1187649667263031
iteration 257, loss = 0.17026706039905548
iteration 258, loss = 0.07260286062955856
iteration 259, loss = 0.23282983899116516
iteration 260, loss = 0.2293560951948166
iteration 261, loss = 0.01574488915503025
iteration 262, loss = 0.1923278570175171
iteration 263, loss = 0.3411039113998413
iteration 264, loss = 0.21883165836334229
iteration 265, loss = 0.21282508969306946
iteration 266, loss = 0.22442671656608582
iteration 267, loss = 0.27964600920677185
iteration 268, loss = 0.2186591774225235
iteration 269, loss = 0.22709819674491882
iteration 270, loss = 0.12092460691928864
iteration 271, loss = 0.1829548478126526
iteration 272, loss = 0.12283007055521011
iteration 273, loss = 0.1496010571718216
iteration 274, loss = 0.13866083323955536
iteration 275, loss = 0.3757844865322113
iteration 276, loss = 0.200909823179245
iteration 277, loss = 0.1979779601097107
iteration 278, loss = 0.18437466025352478
iteration 279, loss = 0.08860312402248383
iteration 280, loss = 0.18566323816776276
iteration 281, loss = 0.21877740323543549
iteration 282, loss = 0.30651190876960754
iteration 283, loss = 0.23245061933994293
iteration 284, loss = 0.17827308177947998
iteration 285, loss = 0.3514934480190277
iteration 286, loss = 0.1131444126367569
iteration 287, loss = 0.09432458877563477
iteration 288, loss = 0.06742661446332932
iteration 289, loss = 0.12981665134429932
iteration 290, loss = 0.06081564724445343
iteration 291, loss = 0.25294744968414307
iteration 292, loss = 0.11292436718940735
iteration 293, loss = 0.19656595587730408
iteration 294, loss = 0.2248598337173462
iteration 295, loss = 0.2984749972820282
iteration 296, loss = 0.14548790454864502
iteration 297, loss = 0.17514890432357788
iteration 298, loss = 0.23570045828819275
iteration 299, loss = 0.1917463093996048
iteration 0, loss = 0.08739862591028214
iteration 1, loss = 0.10633526742458344
iteration 2, loss = 0.04943918436765671
iteration 3, loss = 0.04198833554983139
iteration 4, loss = 0.1780894547700882
iteration 5, loss = 0.47042030096054077
iteration 6, loss = 0.4115222692489624
iteration 7, loss = 0.0771339163184166
iteration 8, loss = 0.13262581825256348
iteration 9, loss = 0.2306530773639679
iteration 10, loss = 0.03166298568248749
iteration 11, loss = 0.27249273657798767
iteration 12, loss = 0.09780285507440567
iteration 13, loss = 0.1723882406949997
iteration 14, loss = 0.08978726714849472
iteration 15, loss = 0.30455631017684937
iteration 16, loss = 0.11357487738132477
iteration 17, loss = 0.1335955709218979
iteration 18, loss = 0.13516606390476227
iteration 19, loss = 0.31595033407211304
iteration 20, loss = 0.3150586485862732
iteration 21, loss = 0.1133955717086792
iteration 22, loss = 0.1409851312637329
iteration 23, loss = 0.16032257676124573
iteration 24, loss = 0.2445363998413086
iteration 25, loss = 0.12936988472938538
iteration 26, loss = 0.13463982939720154
iteration 27, loss = 0.13039857149124146
iteration 28, loss = 0.15568497776985168
iteration 29, loss = 0.2104155570268631
iteration 30, loss = 0.18109098076820374
iteration 31, loss = 0.0860641673207283
iteration 32, loss = 0.07166042923927307
iteration 33, loss = 0.20259663462638855
iteration 34, loss = 0.24043704569339752
iteration 35, loss = 0.22215956449508667
iteration 36, loss = 0.03973505645990372
iteration 37, loss = 0.14344942569732666
iteration 38, loss = 0.26371270418167114
iteration 39, loss = 0.27850037813186646
iteration 40, loss = 0.11283772438764572
iteration 41, loss = 0.26961374282836914
iteration 42, loss = 0.08462043106555939
iteration 43, loss = 0.10924454033374786
iteration 44, loss = 0.13036777079105377
iteration 45, loss = 0.30939093232154846
iteration 46, loss = 0.1336521953344345
iteration 47, loss = 0.08196008205413818
iteration 48, loss = 0.20258566737174988
iteration 49, loss = 0.1905783712863922
iteration 50, loss = 0.11112571507692337
iteration 51, loss = 0.2301078736782074
iteration 52, loss = 0.1477714478969574
iteration 53, loss = 0.1513713300228119
iteration 54, loss = 0.06640506535768509
iteration 55, loss = 0.07239703834056854
iteration 56, loss = 0.028448689728975296
iteration 57, loss = 0.2693828344345093
iteration 58, loss = 0.19709007441997528
iteration 59, loss = 0.1712491810321808
iteration 60, loss = 0.3667660355567932
iteration 61, loss = 0.0178668312728405
iteration 62, loss = 0.22096411883831024
iteration 63, loss = 0.03733549639582634
iteration 64, loss = 0.11354579776525497
iteration 65, loss = 0.09885655343532562
iteration 66, loss = 0.11667560040950775
iteration 67, loss = 0.07175609469413757
iteration 68, loss = 0.41425102949142456
iteration 69, loss = 0.2712075412273407
iteration 70, loss = 0.1623210459947586
iteration 71, loss = 0.1952478587627411
iteration 72, loss = 0.23489460349082947
iteration 73, loss = 0.19201122224330902
iteration 74, loss = 0.17040947079658508
iteration 75, loss = 0.2854517698287964
iteration 76, loss = 0.1532350331544876
iteration 77, loss = 0.015540407970547676
iteration 78, loss = 0.010092399083077908
iteration 79, loss = 0.655935525894165
iteration 80, loss = 0.3343167006969452
iteration 81, loss = 0.6143699884414673
iteration 82, loss = 0.22317369282245636
iteration 83, loss = 0.5789065957069397
iteration 84, loss = 0.335940957069397
iteration 85, loss = 0.0446346178650856
iteration 86, loss = 0.19293639063835144
iteration 87, loss = 0.10249589383602142
iteration 88, loss = 0.20334112644195557
iteration 89, loss = 0.14554175734519958
iteration 90, loss = 0.14586547017097473
iteration 91, loss = 0.08592116832733154
iteration 92, loss = 0.31950023770332336
iteration 93, loss = 0.2678126096725464
iteration 94, loss = 0.325081467628479
iteration 95, loss = 0.07785630226135254
iteration 96, loss = 0.11746072769165039
iteration 97, loss = 0.05866215378046036
iteration 98, loss = 0.04359009861946106
iteration 99, loss = 0.4083849787712097
iteration 100, loss = 0.04520820081233978
iteration 101, loss = 0.009747334755957127
iteration 102, loss = 0.04526258632540703
iteration 103, loss = 0.2596515119075775
iteration 104, loss = 0.03511707857251167
iteration 105, loss = 0.10927274823188782
iteration 106, loss = 0.2858160138130188
iteration 107, loss = 0.3251298666000366
iteration 108, loss = 0.1431237906217575
iteration 109, loss = 0.29087817668914795
iteration 110, loss = 0.13705343008041382
iteration 111, loss = 0.06761584430932999
iteration 112, loss = 0.1516738384962082
iteration 113, loss = 0.30203601717948914
iteration 114, loss = 0.02364688739180565
iteration 115, loss = 0.0770011693239212
iteration 116, loss = 0.2796594798564911
iteration 117, loss = 0.19252793490886688
iteration 118, loss = 0.17924551665782928
iteration 119, loss = 0.177371546626091
iteration 120, loss = 0.326781690120697
iteration 121, loss = 0.119936503469944
iteration 122, loss = 0.16897085309028625
iteration 123, loss = 0.22439588606357574
iteration 124, loss = 0.13508237898349762
iteration 125, loss = 0.15224041044712067
iteration 126, loss = 0.1924801468849182
iteration 127, loss = 0.021895406767725945
iteration 128, loss = 0.05270322784781456
iteration 129, loss = 0.29184985160827637
iteration 130, loss = 0.32485005259513855
iteration 131, loss = 0.21644386649131775
iteration 132, loss = 0.11554005742073059
iteration 133, loss = 0.06271322071552277
iteration 134, loss = 0.0996314138174057
iteration 135, loss = 0.2646038234233856
iteration 136, loss = 0.5284672379493713
iteration 137, loss = 0.06902272254228592
iteration 138, loss = 0.022379839792847633
iteration 139, loss = 0.08848605304956436
iteration 140, loss = 0.008233512751758099
iteration 141, loss = 0.0607217438519001
iteration 142, loss = 0.2392360270023346
iteration 143, loss = 0.2956662178039551
iteration 144, loss = 0.3934831917285919
iteration 145, loss = 0.21267494559288025
iteration 146, loss = 0.22649551928043365
iteration 147, loss = 0.24359749257564545
iteration 148, loss = 0.03832508996129036
iteration 149, loss = 0.21948663890361786
iteration 150, loss = 0.18646232783794403
iteration 151, loss = 0.22685520350933075
iteration 152, loss = 0.14697104692459106
iteration 153, loss = 0.2777247428894043
iteration 154, loss = 0.1609097272157669
iteration 155, loss = 0.11115682125091553
iteration 156, loss = 0.18210163712501526
iteration 157, loss = 0.16891665756702423
iteration 158, loss = 0.31954947113990784
iteration 159, loss = 0.25001785159111023
iteration 160, loss = 0.08477393537759781
iteration 161, loss = 0.1398370862007141
iteration 162, loss = 0.0232622642070055
iteration 163, loss = 0.5031412839889526
iteration 164, loss = 0.15093755722045898
iteration 165, loss = 0.026012234389781952
iteration 166, loss = 0.003286484396085143
iteration 167, loss = 0.2706023156642914
iteration 168, loss = 0.022616347298026085
iteration 169, loss = 0.17575514316558838
iteration 170, loss = 0.05468461290001869
iteration 171, loss = 0.1148001179099083
iteration 172, loss = 0.157024547457695
iteration 173, loss = 0.1427500993013382
iteration 174, loss = 0.09431853145360947
iteration 175, loss = 0.3255949020385742
iteration 176, loss = 0.07819220423698425
iteration 177, loss = 0.05256299674510956
iteration 178, loss = 0.23281799256801605
iteration 179, loss = 0.25161027908325195
iteration 180, loss = 0.20144113898277283
iteration 181, loss = 0.24542264640331268
iteration 182, loss = 0.318632572889328
iteration 183, loss = 0.07041850686073303
iteration 184, loss = 0.05735059082508087
iteration 185, loss = 0.24929466843605042
iteration 186, loss = 0.09848245978355408
iteration 187, loss = 0.5131189227104187
iteration 188, loss = 0.2057218700647354
iteration 189, loss = 0.10720575600862503
iteration 190, loss = 0.15479053556919098
iteration 191, loss = 0.6304306387901306
iteration 192, loss = 0.5510033369064331
iteration 193, loss = 0.08553099632263184
iteration 194, loss = 0.15732441842556
iteration 195, loss = 0.41789084672927856
iteration 196, loss = 0.444867879152298
iteration 197, loss = 0.11445441097021103
iteration 198, loss = 0.05288653075695038
iteration 199, loss = 0.5005589127540588
iteration 200, loss = 0.21725092828273773
iteration 201, loss = 0.4808950126171112
iteration 202, loss = 0.3090488016605377
iteration 203, loss = 0.22771334648132324
iteration 204, loss = 0.19460313022136688
iteration 205, loss = 0.4387975335121155
iteration 206, loss = 0.017275415360927582
iteration 207, loss = 0.027768118306994438
iteration 208, loss = 0.18285346031188965
iteration 209, loss = 0.2066759467124939
iteration 210, loss = 0.22449909150600433
iteration 211, loss = 0.20190249383449554
iteration 212, loss = 0.14509502053260803
iteration 213, loss = 0.12219765782356262
iteration 214, loss = 0.06555189192295074
iteration 215, loss = 0.1477217972278595
iteration 216, loss = 0.07704644650220871
iteration 217, loss = 0.03414711356163025
iteration 218, loss = 0.25312188267707825
iteration 219, loss = 0.28604042530059814
iteration 220, loss = 0.08552923798561096
iteration 221, loss = 0.1402777135372162
iteration 222, loss = 0.10411743819713593
iteration 223, loss = 0.20870837569236755
iteration 224, loss = 0.18674349784851074
iteration 225, loss = 0.14470182359218597
iteration 226, loss = 0.17460821568965912
iteration 227, loss = 0.15163862705230713
iteration 228, loss = 0.11082462966442108
iteration 229, loss = 0.16941964626312256
iteration 230, loss = 0.14231528341770172
iteration 231, loss = 0.23800939321517944
iteration 232, loss = 0.1801934391260147
iteration 233, loss = 0.3239929676055908
iteration 234, loss = 0.21071986854076385
iteration 235, loss = 0.07832249999046326
iteration 236, loss = 0.27315741777420044
iteration 237, loss = 0.1332445591688156
iteration 238, loss = 0.1528870016336441
iteration 239, loss = 0.14478109776973724
iteration 240, loss = 0.24302980303764343
iteration 241, loss = 0.2501983642578125
iteration 242, loss = 0.09138862043619156
iteration 243, loss = 0.19060856103897095
iteration 244, loss = 0.08758899569511414
iteration 245, loss = 0.1531517505645752
iteration 246, loss = 0.2591099739074707
iteration 247, loss = 0.19458168745040894
iteration 248, loss = 0.14749935269355774
iteration 249, loss = 0.1911323517560959
iteration 250, loss = 0.11649711430072784
iteration 251, loss = 0.296500563621521
iteration 252, loss = 0.1351042091846466
iteration 253, loss = 0.21002256870269775
iteration 254, loss = 0.08632463961839676
iteration 255, loss = 0.021205630153417587
iteration 256, loss = 0.08060405403375626
iteration 257, loss = 0.0740090161561966
iteration 258, loss = 0.21777209639549255
iteration 259, loss = 0.11321644484996796
iteration 260, loss = 0.0800199806690216
iteration 261, loss = 0.16402466595172882
iteration 262, loss = 0.16390806436538696
iteration 263, loss = 0.22930434346199036
iteration 264, loss = 0.16977734863758087
iteration 265, loss = 0.13115739822387695
iteration 266, loss = 0.3109491169452667
iteration 267, loss = 0.09813952445983887
iteration 268, loss = 0.10783436894416809
iteration 269, loss = 0.03692561388015747
iteration 270, loss = 0.20406603813171387
iteration 271, loss = 0.2562277317047119
iteration 272, loss = 0.13089348375797272
iteration 273, loss = 0.14608809351921082
iteration 274, loss = 0.18715131282806396
iteration 275, loss = 0.1495281159877777
iteration 276, loss = 0.1600535809993744
iteration 277, loss = 0.1412399262189865
iteration 278, loss = 0.25106048583984375
iteration 279, loss = 0.09995369613170624
iteration 280, loss = 0.1554846167564392
iteration 281, loss = 0.052926599979400635
iteration 282, loss = 0.06940583884716034
iteration 283, loss = 0.03422461450099945
iteration 284, loss = 0.21902531385421753
iteration 285, loss = 0.21540796756744385
iteration 286, loss = 0.26014244556427
iteration 287, loss = 0.18051378428936005
iteration 288, loss = 0.2210766077041626
iteration 289, loss = 0.1456475406885147
iteration 290, loss = 0.1175164133310318
iteration 291, loss = 0.16805851459503174
iteration 292, loss = 0.12263074517250061
iteration 293, loss = 0.01419825665652752
iteration 294, loss = 0.06518866121768951
iteration 295, loss = 0.13597096502780914
iteration 296, loss = 0.24880766868591309
iteration 297, loss = 0.09389059245586395
iteration 298, loss = 0.176723450422287
iteration 299, loss = 0.18349021673202515
iteration 0, loss = 0.15559223294258118
iteration 1, loss = 0.17010048031806946
iteration 2, loss = 0.11588727682828903
iteration 3, loss = 0.08825404196977615
iteration 4, loss = 0.027176182717084885
iteration 5, loss = 0.12022879719734192
iteration 6, loss = 0.13748782873153687
iteration 7, loss = 0.14247365295886993
iteration 8, loss = 0.1453634649515152
iteration 9, loss = 0.15492868423461914
iteration 10, loss = 0.3945477604866028
iteration 11, loss = 0.17804960906505585
iteration 12, loss = 0.22853946685791016
iteration 13, loss = 0.20353205502033234
iteration 14, loss = 0.296151727437973
iteration 15, loss = 0.11226227879524231
iteration 16, loss = 0.21718314290046692
iteration 17, loss = 0.10004492849111557
iteration 18, loss = 0.14482909440994263
iteration 19, loss = 0.30071595311164856
iteration 20, loss = 0.1190156415104866
iteration 21, loss = 0.16066411137580872
iteration 22, loss = 0.17919617891311646
iteration 23, loss = 0.17272664606571198
iteration 24, loss = 0.28215059638023376
iteration 25, loss = 0.2181638926267624
iteration 26, loss = 0.18548163771629333
iteration 27, loss = 0.2780597507953644
iteration 28, loss = 0.1267346441745758
iteration 29, loss = 0.12876977026462555
iteration 30, loss = 0.08210720866918564
iteration 31, loss = 0.11282944679260254
iteration 32, loss = 0.06815551221370697
iteration 33, loss = 0.09139688313007355
iteration 34, loss = 0.07519251108169556
iteration 35, loss = 0.16117924451828003
iteration 36, loss = 0.21258729696273804
iteration 37, loss = 0.11943171918392181
iteration 38, loss = 0.1738053262233734
iteration 39, loss = 0.2965008020401001
iteration 40, loss = 0.17728738486766815
iteration 41, loss = 0.20757481455802917
iteration 42, loss = 0.1405094563961029
iteration 43, loss = 0.3238580822944641
iteration 44, loss = 0.11587142199277878
iteration 45, loss = 0.06680789589881897
iteration 46, loss = 0.04467476159334183
iteration 47, loss = 0.010900712572038174
iteration 48, loss = 0.29816925525665283
iteration 49, loss = 0.2983786463737488
iteration 50, loss = 0.10744985938072205
iteration 51, loss = 0.16828691959381104
iteration 52, loss = 0.16836316883563995
iteration 53, loss = 0.3135521709918976
iteration 54, loss = 0.21655601263046265
iteration 55, loss = 0.2114270180463791
iteration 56, loss = 0.05416526272892952
iteration 57, loss = 0.16253764927387238
iteration 58, loss = 0.07193693518638611
iteration 59, loss = 0.06595060229301453
iteration 60, loss = 0.08665842562913895
iteration 61, loss = 0.1764499843120575
iteration 62, loss = 0.2262323796749115
iteration 63, loss = 0.08865143358707428
iteration 64, loss = 0.20651191473007202
iteration 65, loss = 0.2158755660057068
iteration 66, loss = 0.06612998247146606
iteration 67, loss = 0.21191754937171936
iteration 68, loss = 0.13993825018405914
iteration 69, loss = 0.21718591451644897
iteration 70, loss = 0.0731816217303276
iteration 71, loss = 0.1925596296787262
iteration 72, loss = 0.09544960409402847
iteration 73, loss = 0.13649579882621765
iteration 74, loss = 0.28365808725357056
iteration 75, loss = 0.22893506288528442
iteration 76, loss = 0.19959773123264313
iteration 77, loss = 0.23830661177635193
iteration 78, loss = 0.29970982670783997
iteration 79, loss = 0.24984809756278992
iteration 80, loss = 0.23073574900627136
iteration 81, loss = 0.2707948386669159
iteration 82, loss = 0.1614346206188202
iteration 83, loss = 0.1692339926958084
iteration 84, loss = 0.3469190001487732
iteration 85, loss = 0.423816978931427
iteration 86, loss = 0.20804689824581146
iteration 87, loss = 0.46810927987098694
iteration 88, loss = 0.3745327591896057
iteration 89, loss = 0.5138139128684998
iteration 90, loss = 0.15431803464889526
iteration 91, loss = 0.24230273067951202
iteration 92, loss = 0.6823418140411377
iteration 93, loss = 0.7243944406509399
iteration 94, loss = 0.17891213297843933
iteration 95, loss = 0.32737869024276733
iteration 96, loss = 0.07980471104383469
iteration 97, loss = 0.03874373808503151
iteration 98, loss = 0.5263937711715698
iteration 99, loss = 0.5422963500022888
iteration 100, loss = 0.32535049319267273
iteration 101, loss = 0.24030178785324097
iteration 102, loss = 0.3317442834377289
iteration 103, loss = 0.4795933961868286
iteration 104, loss = 0.10388153791427612
iteration 105, loss = 0.22605614364147186
iteration 106, loss = 0.47133463621139526
iteration 107, loss = 0.2677094340324402
iteration 108, loss = 0.11549217998981476
iteration 109, loss = 0.0572551004588604
iteration 110, loss = 0.48572126030921936
iteration 111, loss = 0.12481218576431274
iteration 112, loss = 0.30994346737861633
iteration 113, loss = 0.10968195647001266
iteration 114, loss = 0.11040414869785309
iteration 115, loss = 0.16519784927368164
iteration 116, loss = 0.26452845335006714
iteration 117, loss = 0.13822947442531586
iteration 118, loss = 0.07008782029151917
iteration 119, loss = 0.16273465752601624
iteration 120, loss = 0.16929657757282257
iteration 121, loss = 0.11042516678571701
iteration 122, loss = 0.02523421123623848
iteration 123, loss = 0.0335560105741024
iteration 124, loss = 0.0569562092423439
iteration 125, loss = 0.30487334728240967
iteration 126, loss = 0.2339063435792923
iteration 127, loss = 0.03255753219127655
iteration 128, loss = 0.3042081892490387
iteration 129, loss = 0.21539676189422607
iteration 130, loss = 0.2103782594203949
iteration 131, loss = 0.304691344499588
iteration 132, loss = 0.0433565117418766
iteration 133, loss = 0.061763375997543335
iteration 134, loss = 0.29664236307144165
iteration 135, loss = 0.4354443848133087
iteration 136, loss = 0.24563664197921753
iteration 137, loss = 0.3660835325717926
iteration 138, loss = 0.10135406255722046
iteration 139, loss = 0.1620539426803589
iteration 140, loss = 0.07108340412378311
iteration 141, loss = 0.29657870531082153
iteration 142, loss = 0.25158268213272095
iteration 143, loss = 0.3283180594444275
iteration 144, loss = 0.09738510102033615
iteration 145, loss = 0.15862062573432922
iteration 146, loss = 0.11936677247285843
iteration 147, loss = 0.04631357640028
iteration 148, loss = 0.42284783720970154
iteration 149, loss = 0.21054692566394806
iteration 150, loss = 0.23752710223197937
iteration 151, loss = 0.09586624056100845
iteration 152, loss = 0.33510857820510864
iteration 153, loss = 0.29283303022384644
iteration 154, loss = 0.18634723126888275
iteration 155, loss = 0.17675453424453735
iteration 156, loss = 0.2304832935333252
iteration 157, loss = 0.31108734011650085
iteration 158, loss = 0.22355926036834717
iteration 159, loss = 0.2191832959651947
iteration 160, loss = 0.24989663064479828
iteration 161, loss = 0.40970632433891296
iteration 162, loss = 0.189588725566864
iteration 163, loss = 0.11130602657794952
iteration 164, loss = 0.026469485834240913
iteration 165, loss = 0.26291847229003906
iteration 166, loss = 0.17829106748104095
iteration 167, loss = 0.2112177014350891
iteration 168, loss = 0.2572137713432312
iteration 169, loss = 0.1837247759103775
iteration 170, loss = 0.1073697954416275
iteration 171, loss = 0.3216573894023895
iteration 172, loss = 0.2711755931377411
iteration 173, loss = 0.22506988048553467
iteration 174, loss = 0.10795897245407104
iteration 175, loss = 0.20387008786201477
iteration 176, loss = 0.11544688045978546
iteration 177, loss = 0.1868070662021637
iteration 178, loss = 0.10307207703590393
iteration 179, loss = 0.04527299106121063
iteration 180, loss = 0.06311735510826111
iteration 181, loss = 0.1508958339691162
iteration 182, loss = 0.1832713484764099
iteration 183, loss = 0.22986996173858643
iteration 184, loss = 0.05499322712421417
iteration 185, loss = 0.046621616929769516
iteration 186, loss = 0.15294870734214783
iteration 187, loss = 0.38923943042755127
iteration 188, loss = 0.24772053956985474
iteration 189, loss = 0.07486871629953384
iteration 190, loss = 0.13229405879974365
iteration 191, loss = 0.10724954307079315
iteration 192, loss = 0.20691190659999847
iteration 193, loss = 0.17554470896720886
iteration 194, loss = 0.12358498573303223
iteration 195, loss = 0.03077344223856926
iteration 196, loss = 0.1553075909614563
iteration 197, loss = 0.6994316577911377
iteration 198, loss = 0.42124199867248535
iteration 199, loss = 0.24377113580703735
iteration 200, loss = 0.33403146266937256
iteration 201, loss = 0.4738001525402069
iteration 202, loss = 0.5900477766990662
iteration 203, loss = 0.22942231595516205
iteration 204, loss = 0.23240777850151062
iteration 205, loss = 0.23419485986232758
iteration 206, loss = 0.5839306712150574
iteration 207, loss = 0.3525749742984772
iteration 208, loss = 0.12939053773880005
iteration 209, loss = 0.2530471086502075
iteration 210, loss = 0.3795839250087738
iteration 211, loss = 0.19273057579994202
iteration 212, loss = 0.4645460546016693
iteration 213, loss = 0.23763248324394226
iteration 214, loss = 0.11293908208608627
iteration 215, loss = 0.055197905749082565
iteration 216, loss = 0.043849702924489975
iteration 217, loss = 0.06128167733550072
iteration 218, loss = 0.03876078873872757
iteration 219, loss = 0.024647122249007225
iteration 220, loss = 0.17418473958969116
iteration 221, loss = 0.08016924560070038
iteration 222, loss = 0.41549208760261536
iteration 223, loss = 0.13778334856033325
iteration 224, loss = 0.20089875161647797
iteration 225, loss = 0.2022620141506195
iteration 226, loss = 0.21777448058128357
iteration 227, loss = 0.11844269186258316
iteration 228, loss = 0.2251250445842743
iteration 229, loss = 0.10635455697774887
iteration 230, loss = 0.017066551372408867
iteration 231, loss = 0.17609058320522308
iteration 232, loss = 0.29023537039756775
iteration 233, loss = 0.03951270878314972
iteration 234, loss = 0.13690997660160065
iteration 235, loss = 0.1833968460559845
iteration 236, loss = 0.289164274930954
iteration 237, loss = 0.33924952149391174
iteration 238, loss = 0.12202586233615875
iteration 239, loss = 0.16548243165016174
iteration 240, loss = 0.11369342356920242
iteration 241, loss = 0.10179788619279861
iteration 242, loss = 0.1339355707168579
iteration 243, loss = 0.05368250608444214
iteration 244, loss = 0.3931284248828888
iteration 245, loss = 0.1666180044412613
iteration 246, loss = 0.11704903841018677
iteration 247, loss = 0.1714630126953125
iteration 248, loss = 0.21443518996238708
iteration 249, loss = 0.21340122818946838
iteration 250, loss = 0.1591707020998001
iteration 251, loss = 0.22447504103183746
iteration 252, loss = 0.295809268951416
iteration 253, loss = 0.11311470717191696
iteration 254, loss = 0.2941715717315674
iteration 255, loss = 0.12712706625461578
iteration 256, loss = 0.36474859714508057
iteration 257, loss = 0.1415921151638031
iteration 258, loss = 0.2843093276023865
iteration 259, loss = 0.17335018515586853
iteration 260, loss = 0.14217042922973633
iteration 261, loss = 0.14445000886917114
iteration 262, loss = 0.1385749727487564
iteration 263, loss = 0.06554166972637177
iteration 264, loss = 0.13390350341796875
iteration 265, loss = 0.08183974027633667
iteration 266, loss = 0.06852992624044418
iteration 267, loss = 0.10110635310411453
iteration 268, loss = 0.1480170637369156
iteration 269, loss = 0.13376261293888092
iteration 270, loss = 0.16249987483024597
iteration 271, loss = 0.1557648479938507
iteration 272, loss = 0.0740625411272049
iteration 273, loss = 0.2467147409915924
iteration 274, loss = 0.09582716226577759
iteration 275, loss = 0.09421899169683456
iteration 276, loss = 0.21574251353740692
iteration 277, loss = 0.19773425161838531
iteration 278, loss = 0.17900055646896362
iteration 279, loss = 0.11228985339403152
iteration 280, loss = 0.11732041835784912
iteration 281, loss = 0.11097867041826248
iteration 282, loss = 0.11716348677873611
iteration 283, loss = 0.05993901565670967
iteration 284, loss = 0.20537418127059937
iteration 285, loss = 0.13587938249111176
iteration 286, loss = 0.05015875771641731
iteration 287, loss = 0.24039842188358307
iteration 288, loss = 0.16358682513237
iteration 289, loss = 0.16142085194587708
iteration 290, loss = 0.08901985734701157
iteration 291, loss = 0.019347533583641052
iteration 292, loss = 0.42400211095809937
iteration 293, loss = 0.31289058923721313
iteration 294, loss = 0.16179576516151428
iteration 295, loss = 0.27122071385383606
iteration 296, loss = 0.05511241406202316
iteration 297, loss = 0.33902543783187866
iteration 298, loss = 0.2804016172885895
iteration 299, loss = 0.21034736931324005
iteration 0, loss = 0.027299843728542328
iteration 1, loss = 0.48165446519851685
iteration 2, loss = 0.18698656558990479
iteration 3, loss = 0.39485597610473633
iteration 4, loss = 0.25581610202789307
iteration 5, loss = 0.1857350468635559
iteration 6, loss = 0.1648588925600052
iteration 7, loss = 0.19496500492095947
iteration 8, loss = 0.21326954662799835
iteration 9, loss = 0.2657929062843323
iteration 10, loss = 0.17203101515769958
iteration 11, loss = 0.07391331344842911
iteration 12, loss = 0.21374556422233582
iteration 13, loss = 0.28354915976524353
iteration 14, loss = 0.263809472322464
iteration 15, loss = 0.18999066948890686
iteration 16, loss = 0.3212582767009735
iteration 17, loss = 0.11165009438991547
iteration 18, loss = 0.06158407777547836
iteration 19, loss = 0.27235496044158936
iteration 20, loss = 0.2632598578929901
iteration 21, loss = 0.1243232935667038
iteration 22, loss = 0.21365024149417877
iteration 23, loss = 0.11655467748641968
iteration 24, loss = 0.19581827521324158
iteration 25, loss = 0.07389836758375168
iteration 26, loss = 0.1350262463092804
iteration 27, loss = 0.3250837028026581
iteration 28, loss = 0.1297292858362198
iteration 29, loss = 0.22495871782302856
iteration 30, loss = 0.282169908285141
iteration 31, loss = 0.1349724382162094
iteration 32, loss = 0.13095137476921082
iteration 33, loss = 0.06154685840010643
iteration 34, loss = 0.04562096670269966
iteration 35, loss = 0.315787136554718
iteration 36, loss = 0.28437143564224243
iteration 37, loss = 0.0377190038561821
iteration 38, loss = 0.2792176306247711
iteration 39, loss = 0.1092248409986496
iteration 40, loss = 0.22343647480010986
iteration 41, loss = 0.3712800145149231
iteration 42, loss = 0.09571351110935211
iteration 43, loss = 0.06502758711576462
iteration 44, loss = 0.07563109695911407
iteration 45, loss = 0.08876053988933563
iteration 46, loss = 0.5161530375480652
iteration 47, loss = 0.24672797322273254
iteration 48, loss = 0.2474876046180725
iteration 49, loss = 0.41865745186805725
iteration 50, loss = 0.1537741869688034
iteration 51, loss = 0.12811852991580963
iteration 52, loss = 0.1528223603963852
iteration 53, loss = 0.13664886355400085
iteration 54, loss = 0.23937225341796875
iteration 55, loss = 0.13340424001216888
iteration 56, loss = 0.09773612767457962
iteration 57, loss = 0.24394723773002625
iteration 58, loss = 0.2779103219509125
iteration 59, loss = 0.26729437708854675
iteration 60, loss = 0.31591278314590454
iteration 61, loss = 0.07600087672472
iteration 62, loss = 0.4546463191509247
iteration 63, loss = 0.20329633355140686
iteration 64, loss = 0.06801262497901917
iteration 65, loss = 0.18228843808174133
iteration 66, loss = 0.04179107025265694
iteration 67, loss = 0.3413445055484772
iteration 68, loss = 0.16916926205158234
iteration 69, loss = 0.27227091789245605
iteration 70, loss = 0.15360185503959656
iteration 71, loss = 0.020360276103019714
iteration 72, loss = 0.06433665007352829
iteration 73, loss = 0.327841579914093
iteration 74, loss = 0.3480694890022278
iteration 75, loss = 0.17000404000282288
iteration 76, loss = 0.11336041241884232
iteration 77, loss = 0.1435634195804596
iteration 78, loss = 0.13735467195510864
iteration 79, loss = 0.14776532351970673
iteration 80, loss = 0.15933898091316223
iteration 81, loss = 0.078850656747818
iteration 82, loss = 0.10462729632854462
iteration 83, loss = 0.25400310754776
iteration 84, loss = 0.17216986417770386
iteration 85, loss = 0.2544627785682678
iteration 86, loss = 0.2528221905231476
iteration 87, loss = 0.03870506212115288
iteration 88, loss = 0.1631966531276703
iteration 89, loss = 0.14502452313899994
iteration 90, loss = 0.06204837188124657
iteration 91, loss = 0.05596674233675003
iteration 92, loss = 0.1827097237110138
iteration 93, loss = 0.09605671465396881
iteration 94, loss = 0.1692931354045868
iteration 95, loss = 0.24813242256641388
iteration 96, loss = 0.02659466862678528
iteration 97, loss = 0.12798193097114563
iteration 98, loss = 0.1720205396413803
iteration 99, loss = 0.04056040570139885
iteration 100, loss = 0.17551523447036743
iteration 101, loss = 0.18945521116256714
iteration 102, loss = 0.28847789764404297
iteration 103, loss = 0.11943012475967407
iteration 104, loss = 0.10016416758298874
iteration 105, loss = 0.14785197377204895
iteration 106, loss = 0.11453796923160553
iteration 107, loss = 0.4396486282348633
iteration 108, loss = 0.10771626234054565
iteration 109, loss = 0.18927505612373352
iteration 110, loss = 0.4300422966480255
iteration 111, loss = 0.024595504626631737
iteration 112, loss = 0.15495938062667847
iteration 113, loss = 0.056153055280447006
iteration 114, loss = 0.09384079277515411
iteration 115, loss = 0.2857216000556946
iteration 116, loss = 0.19031605124473572
iteration 117, loss = 0.11989931762218475
iteration 118, loss = 0.10432137548923492
iteration 119, loss = 0.0935593992471695
iteration 120, loss = 0.08872398734092712
iteration 121, loss = 0.26269251108169556
iteration 122, loss = 0.0815114974975586
iteration 123, loss = 0.03690453618764877
iteration 124, loss = 0.12112884223461151
iteration 125, loss = 0.10208185762166977
iteration 126, loss = 0.1598837822675705
iteration 127, loss = 0.11728209257125854
iteration 128, loss = 0.11006312072277069
iteration 129, loss = 0.09512397646903992
iteration 130, loss = 0.1538650095462799
iteration 131, loss = 0.19264142215251923
iteration 132, loss = 0.12463043630123138
iteration 133, loss = 0.17779619991779327
iteration 134, loss = 0.22570934891700745
iteration 135, loss = 0.18217512965202332
iteration 136, loss = 0.07543767243623734
iteration 137, loss = 0.14299337565898895
iteration 138, loss = 0.16486310958862305
iteration 139, loss = 0.14239642024040222
iteration 140, loss = 0.24576205015182495
iteration 141, loss = 0.10497698187828064
iteration 142, loss = 0.16099713742733002
iteration 143, loss = 0.14666113257408142
iteration 144, loss = 0.13042137026786804
iteration 145, loss = 0.2903803586959839
iteration 146, loss = 0.1221216544508934
iteration 147, loss = 0.07515041530132294
iteration 148, loss = 0.22066184878349304
iteration 149, loss = 0.17742258310317993
iteration 150, loss = 0.37263423204421997
iteration 151, loss = 0.04551590979099274
iteration 152, loss = 0.024623863399028778
iteration 153, loss = 0.17494326829910278
iteration 154, loss = 0.13960163295269012
iteration 155, loss = 0.13213729858398438
iteration 156, loss = 0.23122423887252808
iteration 157, loss = 0.024549275636672974
iteration 158, loss = 0.02015788108110428
iteration 159, loss = 0.20080314576625824
iteration 160, loss = 0.17871998250484467
iteration 161, loss = 0.1930803656578064
iteration 162, loss = 0.0918782576918602
iteration 163, loss = 0.4840053915977478
iteration 164, loss = 0.004652159288525581
iteration 165, loss = 0.07875233888626099
iteration 166, loss = 0.20144344866275787
iteration 167, loss = 0.19111265242099762
iteration 168, loss = 0.18736815452575684
iteration 169, loss = 0.11854918301105499
iteration 170, loss = 0.17114165425300598
iteration 171, loss = 0.10296879708766937
iteration 172, loss = 0.10546689480543137
iteration 173, loss = 0.09137998521327972
iteration 174, loss = 0.16182832419872284
iteration 175, loss = 0.09712321311235428
iteration 176, loss = 0.23826032876968384
iteration 177, loss = 0.1237662062048912
iteration 178, loss = 0.11476650834083557
iteration 179, loss = 0.1973799169063568
iteration 180, loss = 0.060290735214948654
iteration 181, loss = 0.15126001834869385
iteration 182, loss = 0.20891404151916504
iteration 183, loss = 0.12102804332971573
iteration 184, loss = 0.011113200336694717
iteration 185, loss = 0.19558559358119965
iteration 186, loss = 0.4344422519207001
iteration 187, loss = 0.28713852167129517
iteration 188, loss = 0.25024235248565674
iteration 189, loss = 0.10487866401672363
iteration 190, loss = 0.24945394694805145
iteration 191, loss = 0.15348374843597412
iteration 192, loss = 0.0992540791630745
iteration 193, loss = 0.21095708012580872
iteration 194, loss = 0.2033621072769165
iteration 195, loss = 0.16087009012699127
iteration 196, loss = 0.025411183014512062
iteration 197, loss = 0.03545789048075676
iteration 198, loss = 0.2814052402973175
iteration 199, loss = 0.09411975741386414
iteration 200, loss = 0.03469991683959961
iteration 201, loss = 0.06325170397758484
iteration 202, loss = 0.25449123978614807
iteration 203, loss = 0.19018402695655823
iteration 204, loss = 0.2095632255077362
iteration 205, loss = 0.08877506852149963
iteration 206, loss = 0.04448758065700531
iteration 207, loss = 0.27180105447769165
iteration 208, loss = 0.0801190510392189
iteration 209, loss = 0.037535734474658966
iteration 210, loss = 0.09785729646682739
iteration 211, loss = 0.1503186821937561
iteration 212, loss = 0.21537387371063232
iteration 213, loss = 0.07030234485864639
iteration 214, loss = 0.16016149520874023
iteration 215, loss = 0.08073651790618896
iteration 216, loss = 0.16719023883342743
iteration 217, loss = 0.14235348999500275
iteration 218, loss = 0.1260453760623932
iteration 219, loss = 0.10582101345062256
iteration 220, loss = 0.0639941394329071
iteration 221, loss = 0.07366402447223663
iteration 222, loss = 0.15060490369796753
iteration 223, loss = 0.13483206927776337
iteration 224, loss = 0.0556827187538147
iteration 225, loss = 0.19557148218154907
iteration 226, loss = 0.04899775981903076
iteration 227, loss = 0.3723330795764923
iteration 228, loss = 0.27290841937065125
iteration 229, loss = 0.1599699854850769
iteration 230, loss = 0.051166318356990814
iteration 231, loss = 0.2061845362186432
iteration 232, loss = 0.07308147847652435
iteration 233, loss = 0.19138656556606293
iteration 234, loss = 0.08422354608774185
iteration 235, loss = 0.22531166672706604
iteration 236, loss = 0.16165074706077576
iteration 237, loss = 0.29421719908714294
iteration 238, loss = 0.29178500175476074
iteration 239, loss = 0.1211216002702713
iteration 240, loss = 0.029282741248607635
iteration 241, loss = 0.18264755606651306
iteration 242, loss = 0.23858056962490082
iteration 243, loss = 0.06492854654788971
iteration 244, loss = 0.285772442817688
iteration 245, loss = 0.031103258952498436
iteration 246, loss = 0.11818516999483109
iteration 247, loss = 0.16482841968536377
iteration 248, loss = 0.3487105071544647
iteration 249, loss = 0.21927036345005035
iteration 250, loss = 0.10908785462379456
iteration 251, loss = 0.04821157455444336
iteration 252, loss = 0.13018669188022614
iteration 253, loss = 0.15689703822135925
iteration 254, loss = 0.30579543113708496
iteration 255, loss = 0.11176419258117676
iteration 256, loss = 0.09757321327924728
iteration 257, loss = 0.08681492507457733
iteration 258, loss = 0.13865134119987488
iteration 259, loss = 0.16115228831768036
iteration 260, loss = 0.01806800253689289
iteration 261, loss = 0.03211753070354462
iteration 262, loss = 0.03866869956254959
iteration 263, loss = 0.033461883664131165
iteration 264, loss = 0.3116181492805481
iteration 265, loss = 0.11146117746829987
iteration 266, loss = 0.13701295852661133
iteration 267, loss = 0.2110571563243866
iteration 268, loss = 0.33705437183380127
iteration 269, loss = 0.11162246018648148
iteration 270, loss = 0.1704367846250534
iteration 271, loss = 0.08700325340032578
iteration 272, loss = 0.08341563493013382
iteration 273, loss = 0.034304261207580566
iteration 274, loss = 0.18344782292842865
iteration 275, loss = 0.10353259742259979
iteration 276, loss = 0.13783928751945496
iteration 277, loss = 0.022181985899806023
iteration 278, loss = 0.2781487703323364
iteration 279, loss = 0.10722927749156952
iteration 280, loss = 0.07206219434738159
iteration 281, loss = 0.20897842943668365
iteration 282, loss = 0.17624732851982117
iteration 283, loss = 0.1741565763950348
iteration 284, loss = 0.24533049762248993
iteration 285, loss = 0.17680028080940247
iteration 286, loss = 0.05581796169281006
iteration 287, loss = 0.2547685205936432
iteration 288, loss = 0.05069686844944954
iteration 289, loss = 0.15900620818138123
iteration 290, loss = 0.37157154083251953
iteration 291, loss = 0.12082558125257492
iteration 292, loss = 0.2941119372844696
iteration 293, loss = 0.05447128415107727
iteration 294, loss = 0.17376485466957092
iteration 295, loss = 0.48835036158561707
iteration 296, loss = 0.005402636714279652
iteration 297, loss = 0.2260708212852478
iteration 298, loss = 0.21154248714447021
iteration 299, loss = 0.09216876327991486
iteration 0, loss = 0.08767181634902954
iteration 1, loss = 0.12272869050502777
iteration 2, loss = 0.21849021315574646
iteration 3, loss = 0.09051042795181274
iteration 4, loss = 0.05848391354084015
iteration 5, loss = 0.11918537318706512
iteration 6, loss = 0.29131919145584106
iteration 7, loss = 0.15423263609409332
iteration 8, loss = 0.21696245670318604
iteration 9, loss = 0.12754835188388824
iteration 10, loss = 0.190713569521904
iteration 11, loss = 0.1018436923623085
iteration 12, loss = 0.36339327692985535
iteration 13, loss = 0.22985394299030304
iteration 14, loss = 0.2004840224981308
iteration 15, loss = 0.1560436189174652
iteration 16, loss = 0.03823452815413475
iteration 17, loss = 0.06672167778015137
iteration 18, loss = 0.2920225262641907
iteration 19, loss = 0.1281387358903885
iteration 20, loss = 0.08271477371454239
iteration 21, loss = 0.021160274744033813
iteration 22, loss = 0.12455856800079346
iteration 23, loss = 0.23472686111927032
iteration 24, loss = 0.21466827392578125
iteration 25, loss = 0.19126300513744354
iteration 26, loss = 0.2566210925579071
iteration 27, loss = 0.21203544735908508
iteration 28, loss = 0.057858239859342575
iteration 29, loss = 0.2002931386232376
iteration 30, loss = 0.33727771043777466
iteration 31, loss = 0.32307693362236023
iteration 32, loss = 0.19824115931987762
iteration 33, loss = 0.017796814441680908
iteration 34, loss = 0.3638749420642853
iteration 35, loss = 0.028559884056448936
iteration 36, loss = 0.1292097419500351
iteration 37, loss = 0.20774313807487488
iteration 38, loss = 0.17483112215995789
iteration 39, loss = 0.07559490203857422
iteration 40, loss = 0.19949011504650116
iteration 41, loss = 0.01617896743118763
iteration 42, loss = 0.1622268557548523
iteration 43, loss = 0.22008858621120453
iteration 44, loss = 0.2017757147550583
iteration 45, loss = 0.060500618070364
iteration 46, loss = 0.17402103543281555
iteration 47, loss = 0.19357627630233765
iteration 48, loss = 0.15208978950977325
iteration 49, loss = 0.09823285043239594
iteration 50, loss = 0.14734648168087006
iteration 51, loss = 0.20264053344726562
iteration 52, loss = 0.09658370912075043
iteration 53, loss = 0.15465569496154785
iteration 54, loss = 0.10691577196121216
iteration 55, loss = 0.1967960149049759
iteration 56, loss = 0.10428717732429504
iteration 57, loss = 0.057700756937265396
iteration 58, loss = 0.19219431281089783
iteration 59, loss = 0.055951885879039764
iteration 60, loss = 0.0957687571644783
iteration 61, loss = 0.13576382398605347
iteration 62, loss = 0.14011028409004211
iteration 63, loss = 0.23185621201992035
iteration 64, loss = 0.31710389256477356
iteration 65, loss = 0.08906426280736923
iteration 66, loss = 0.11556447297334671
iteration 67, loss = 0.029351625591516495
iteration 68, loss = 0.021065181121230125
iteration 69, loss = 0.040688466280698776
iteration 70, loss = 0.12783542275428772
iteration 71, loss = 0.3587295114994049
iteration 72, loss = 0.2019672691822052
iteration 73, loss = 0.05784590542316437
iteration 74, loss = 0.1964874267578125
iteration 75, loss = 0.1370186060667038
iteration 76, loss = 0.09549719095230103
iteration 77, loss = 0.18677355349063873
iteration 78, loss = 0.11687695980072021
iteration 79, loss = 0.05197514593601227
iteration 80, loss = 0.33413124084472656
iteration 81, loss = 0.07192601263523102
iteration 82, loss = 0.4793337285518646
iteration 83, loss = 0.09097492694854736
iteration 84, loss = 0.19314977526664734
iteration 85, loss = 0.1537664234638214
iteration 86, loss = 0.0364292673766613
iteration 87, loss = 0.23194773495197296
iteration 88, loss = 0.22786091268062592
iteration 89, loss = 0.0649852529168129
iteration 90, loss = 0.009418992325663567
iteration 91, loss = 0.3966037631034851
iteration 92, loss = 0.5320782661437988
iteration 93, loss = 0.14318600296974182
iteration 94, loss = 0.3333442509174347
iteration 95, loss = 0.17424888908863068
iteration 96, loss = 0.029080774635076523
iteration 97, loss = 0.26161497831344604
iteration 98, loss = 0.1513940691947937
iteration 99, loss = 0.2483498752117157
iteration 100, loss = 0.13316044211387634
iteration 101, loss = 0.054930679500103
iteration 102, loss = 0.09811725467443466
iteration 103, loss = 0.4738360047340393
iteration 104, loss = 0.3827124834060669
iteration 105, loss = 0.6176819205284119
iteration 106, loss = 0.24151869118213654
iteration 107, loss = 0.10434440523386002
iteration 108, loss = 0.2092452049255371
iteration 109, loss = 0.20927631855010986
iteration 110, loss = 0.2769814133644104
iteration 111, loss = 0.1700335443019867
iteration 112, loss = 0.2803540527820587
iteration 113, loss = 0.2717673182487488
iteration 114, loss = 0.06928277760744095
iteration 115, loss = 0.05248996987938881
iteration 116, loss = 0.13059991598129272
iteration 117, loss = 0.329489529132843
iteration 118, loss = 0.0937168225646019
iteration 119, loss = 0.15613529086112976
iteration 120, loss = 0.08603087067604065
iteration 121, loss = 0.19863364100456238
iteration 122, loss = 0.08968614786863327
iteration 123, loss = 0.05096980184316635
iteration 124, loss = 0.16662506759166718
iteration 125, loss = 0.1403466910123825
iteration 126, loss = 0.057987235486507416
iteration 127, loss = 0.2345656454563141
iteration 128, loss = 0.08060901612043381
iteration 129, loss = 0.04081978276371956
iteration 130, loss = 0.15271055698394775
iteration 131, loss = 0.17053596675395966
iteration 132, loss = 0.17557010054588318
iteration 133, loss = 0.23822489380836487
iteration 134, loss = 0.12831127643585205
iteration 135, loss = 0.417432963848114
iteration 136, loss = 0.13247805833816528
iteration 137, loss = 0.16122260689735413
iteration 138, loss = 0.13016721606254578
iteration 139, loss = 0.17381736636161804
iteration 140, loss = 0.11561118811368942
iteration 141, loss = 0.23886644840240479
iteration 142, loss = 0.11198663711547852
iteration 143, loss = 0.09949993342161179
iteration 144, loss = 0.09889674931764603
iteration 145, loss = 0.08528124541044235
iteration 146, loss = 0.38845095038414
iteration 147, loss = 0.2630263864994049
iteration 148, loss = 0.04377764090895653
iteration 149, loss = 0.1305854469537735
iteration 150, loss = 0.3438652753829956
iteration 151, loss = 0.19548892974853516
iteration 152, loss = 0.151123046875
iteration 153, loss = 0.10514780879020691
iteration 154, loss = 0.10504605621099472
iteration 155, loss = 0.008226068690419197
iteration 156, loss = 0.20972609519958496
iteration 157, loss = 0.06822539865970612
iteration 158, loss = 0.1727292686700821
iteration 159, loss = 0.22110149264335632
iteration 160, loss = 0.11572656780481339
iteration 161, loss = 0.09068234264850616
iteration 162, loss = 0.05840134620666504
iteration 163, loss = 0.18295636773109436
iteration 164, loss = 0.2460458129644394
iteration 165, loss = 0.1440374106168747
iteration 166, loss = 0.17466360330581665
iteration 167, loss = 0.08339019864797592
iteration 168, loss = 0.03165189549326897
iteration 169, loss = 0.08606193214654922
iteration 170, loss = 0.2398429960012436
iteration 171, loss = 0.17908406257629395
iteration 172, loss = 0.21327638626098633
iteration 173, loss = 0.213806614279747
iteration 174, loss = 0.1180887222290039
iteration 175, loss = 0.18571250140666962
iteration 176, loss = 0.1073138564825058
iteration 177, loss = 0.11890725791454315
iteration 178, loss = 0.2726837992668152
iteration 179, loss = 0.07414062321186066
iteration 180, loss = 0.07485054433345795
iteration 181, loss = 0.1395617574453354
iteration 182, loss = 0.0383431501686573
iteration 183, loss = 0.089877188205719
iteration 184, loss = 0.09511133283376694
iteration 185, loss = 0.10003781318664551
iteration 186, loss = 0.21797247231006622
iteration 187, loss = 0.11026852577924728
iteration 188, loss = 0.018664799630641937
iteration 189, loss = 0.20301346480846405
iteration 190, loss = 0.11741165071725845
iteration 191, loss = 0.10347924381494522
iteration 192, loss = 0.13015076518058777
iteration 193, loss = 0.2555581033229828
iteration 194, loss = 0.20499363541603088
iteration 195, loss = 0.3743111193180084
iteration 196, loss = 0.42284277081489563
iteration 197, loss = 0.1524675339460373
iteration 198, loss = 0.129152312874794
iteration 199, loss = 0.08464060723781586
iteration 200, loss = 0.10386359691619873
iteration 201, loss = 0.15847423672676086
iteration 202, loss = 0.30233705043792725
iteration 203, loss = 0.09490243345499039
iteration 204, loss = 0.23149177432060242
iteration 205, loss = 0.007520069368183613
iteration 206, loss = 0.059212058782577515
iteration 207, loss = 0.009194228798151016
iteration 208, loss = 0.06128331273794174
iteration 209, loss = 0.08005286753177643
iteration 210, loss = 0.04835127294063568
iteration 211, loss = 0.03868064284324646
iteration 212, loss = 0.07746212184429169
iteration 213, loss = 0.19355328381061554
iteration 214, loss = 0.2906046509742737
iteration 215, loss = 0.22878077626228333
iteration 216, loss = 0.07288981229066849
iteration 217, loss = 0.0767395943403244
iteration 218, loss = 0.18514490127563477
iteration 219, loss = 0.07138390839099884
iteration 220, loss = 0.06747230887413025
iteration 221, loss = 0.320913702249527
iteration 222, loss = 0.0400659441947937
iteration 223, loss = 0.17613935470581055
iteration 224, loss = 0.15311822295188904
iteration 225, loss = 0.21005035936832428
iteration 226, loss = 0.21871504187583923
iteration 227, loss = 0.17173556983470917
iteration 228, loss = 0.07360145449638367
iteration 229, loss = 0.23746418952941895
iteration 230, loss = 0.1471124142408371
iteration 231, loss = 0.10381641983985901
iteration 232, loss = 0.18639537692070007
iteration 233, loss = 0.015079790726304054
iteration 234, loss = 0.2647019028663635
iteration 235, loss = 0.20724916458129883
iteration 236, loss = 0.2769468426704407
iteration 237, loss = 0.13569508492946625
iteration 238, loss = 0.08491620421409607
iteration 239, loss = 0.3465290665626526
iteration 240, loss = 0.16282442212104797
iteration 241, loss = 0.0974002406001091
iteration 242, loss = 0.18962949514389038
iteration 243, loss = 0.018415182828903198
iteration 244, loss = 0.12040948867797852
iteration 245, loss = 0.2320692241191864
iteration 246, loss = 0.14669963717460632
iteration 247, loss = 0.046495258808135986
iteration 248, loss = 0.17473003268241882
iteration 249, loss = 0.11678436398506165
iteration 250, loss = 0.2507631480693817
iteration 251, loss = 0.3652646541595459
iteration 252, loss = 0.33764857053756714
iteration 253, loss = 0.16931962966918945
iteration 254, loss = 0.058553047478199005
iteration 255, loss = 0.037764325737953186
iteration 256, loss = 0.4353371262550354
iteration 257, loss = 0.11105950176715851
iteration 258, loss = 0.008226285688579082
iteration 259, loss = 0.43120840191841125
iteration 260, loss = 0.16788750886917114
iteration 261, loss = 0.12392862886190414
iteration 262, loss = 0.07173822075128555
iteration 263, loss = 0.12897290289402008
iteration 264, loss = 0.10519227385520935
iteration 265, loss = 0.0801776796579361
iteration 266, loss = 0.19876378774642944
iteration 267, loss = 0.2535608112812042
iteration 268, loss = 0.1237807422876358
iteration 269, loss = 0.11921557039022446
iteration 270, loss = 0.21925534307956696
iteration 271, loss = 0.14035466313362122
iteration 272, loss = 0.10279221087694168
iteration 273, loss = 0.025373337790369987
iteration 274, loss = 0.23600813746452332
iteration 275, loss = 0.03485409542918205
iteration 276, loss = 0.1457955241203308
iteration 277, loss = 0.2789649963378906
iteration 278, loss = 0.07036185264587402
iteration 279, loss = 0.153431698679924
iteration 280, loss = 0.0861157476902008
iteration 281, loss = 0.1706850528717041
iteration 282, loss = 0.22752921283245087
iteration 283, loss = 0.044791389256715775
iteration 284, loss = 0.16244938969612122
iteration 285, loss = 0.183026522397995
iteration 286, loss = 0.15727336704730988
iteration 287, loss = 0.20300748944282532
iteration 288, loss = 0.4063875675201416
iteration 289, loss = 0.00834560301154852
iteration 290, loss = 0.12514668703079224
iteration 291, loss = 0.09557068347930908
iteration 292, loss = 0.06842079013586044
iteration 293, loss = 0.23512358963489532
iteration 294, loss = 0.1712416112422943
iteration 295, loss = 0.20992250740528107
iteration 296, loss = 0.07364781200885773
iteration 297, loss = 0.16428163647651672
iteration 298, loss = 0.0952032059431076
iteration 299, loss = 0.009291157126426697
iteration 0, loss = 0.323453426361084
iteration 1, loss = 0.04230380803346634
iteration 2, loss = 0.014321302995085716
iteration 3, loss = 0.3311379849910736
iteration 4, loss = 0.07784086465835571
iteration 5, loss = 0.14123038947582245
iteration 6, loss = 0.12786433100700378
iteration 7, loss = 0.20237986743450165
iteration 8, loss = 0.24450480937957764
iteration 9, loss = 0.3150760233402252
iteration 10, loss = 0.09528008103370667
iteration 11, loss = 0.02463502064347267
iteration 12, loss = 0.03347422927618027
iteration 13, loss = 0.027777167037129402
iteration 14, loss = 0.04265180975198746
iteration 15, loss = 0.0378098227083683
iteration 16, loss = 0.018685216084122658
iteration 17, loss = 0.25177642703056335
iteration 18, loss = 0.3053136467933655
iteration 19, loss = 0.02038983814418316
iteration 20, loss = 0.15374480187892914
iteration 21, loss = 0.066443532705307
iteration 22, loss = 0.09125809371471405
iteration 23, loss = 0.14694549143314362
iteration 24, loss = 0.14585839211940765
iteration 25, loss = 0.21304333209991455
iteration 26, loss = 0.023185906931757927
iteration 27, loss = 0.21736420691013336
iteration 28, loss = 0.026947056874632835
iteration 29, loss = 0.08953415602445602
iteration 30, loss = 0.20284642279148102
iteration 31, loss = 0.2729265093803406
iteration 32, loss = 0.08199675381183624
iteration 33, loss = 0.1810835599899292
iteration 34, loss = 0.023208167403936386
iteration 35, loss = 0.14217080175876617
iteration 36, loss = 0.1408693492412567
iteration 37, loss = 0.2299121767282486
iteration 38, loss = 0.06432415544986725
iteration 39, loss = 0.19617673754692078
iteration 40, loss = 0.03629037365317345
iteration 41, loss = 0.09923352301120758
iteration 42, loss = 0.20449383556842804
iteration 43, loss = 0.10427138954401016
iteration 44, loss = 0.22212821245193481
iteration 45, loss = 0.0749489888548851
iteration 46, loss = 0.06108415126800537
iteration 47, loss = 0.12510466575622559
iteration 48, loss = 0.1700708568096161
iteration 49, loss = 0.11412443220615387
iteration 50, loss = 0.07697761803865433
iteration 51, loss = 0.26091980934143066
iteration 52, loss = 0.4125131070613861
iteration 53, loss = 0.24510423839092255
iteration 54, loss = 0.20787760615348816
iteration 55, loss = 0.2060818374156952
iteration 56, loss = 0.4010503590106964
iteration 57, loss = 0.30160388350486755
iteration 58, loss = 0.15502983331680298
iteration 59, loss = 0.010754939168691635
iteration 60, loss = 0.3323127031326294
iteration 61, loss = 0.17681533098220825
iteration 62, loss = 0.20039363205432892
iteration 63, loss = 0.2847723066806793
iteration 64, loss = 0.18295376002788544
iteration 65, loss = 0.037057723850011826
iteration 66, loss = 0.1668005883693695
iteration 67, loss = 0.06355347484350204
iteration 68, loss = 0.23039540648460388
iteration 69, loss = 0.1431351900100708
iteration 70, loss = 0.2668456733226776
iteration 71, loss = 0.18979181349277496
iteration 72, loss = 0.17296162247657776
iteration 73, loss = 0.03329062834382057
iteration 74, loss = 0.3389313519001007
iteration 75, loss = 0.07687073945999146
iteration 76, loss = 0.16230005025863647
iteration 77, loss = 0.13133065402507782
iteration 78, loss = 0.26305636763572693
iteration 79, loss = 0.22242550551891327
iteration 80, loss = 0.262450248003006
iteration 81, loss = 0.08535139262676239
iteration 82, loss = 0.025087829679250717
iteration 83, loss = 0.0956503376364708
iteration 84, loss = 0.0639987587928772
iteration 85, loss = 0.2530963122844696
iteration 86, loss = 0.060916293412446976
iteration 87, loss = 0.3408089876174927
iteration 88, loss = 0.246275395154953
iteration 89, loss = 0.24034473299980164
iteration 90, loss = 0.22305633127689362
iteration 91, loss = 0.03848297894001007
iteration 92, loss = 0.18118785321712494
iteration 93, loss = 0.2302558273077011
iteration 94, loss = 0.2013457864522934
iteration 95, loss = 0.013386142440140247
iteration 96, loss = 0.07107821851968765
iteration 97, loss = 0.09028786420822144
iteration 98, loss = 0.07296141237020493
iteration 99, loss = 0.03780742362141609
iteration 100, loss = 0.3594810962677002
iteration 101, loss = 0.17286166548728943
iteration 102, loss = 0.0586470328271389
iteration 103, loss = 0.03869273513555527
iteration 104, loss = 0.23094065487384796
iteration 105, loss = 0.21504922211170197
iteration 106, loss = 0.03013291396200657
iteration 107, loss = 0.6118492484092712
iteration 108, loss = 0.42046311497688293
iteration 109, loss = 0.15622904896736145
iteration 110, loss = 0.1552797555923462
iteration 111, loss = 0.19558049738407135
iteration 112, loss = 0.1384788304567337
iteration 113, loss = 0.05939554050564766
iteration 114, loss = 0.15021175146102905
iteration 115, loss = 0.15621671080589294
iteration 116, loss = 0.1698714643716812
iteration 117, loss = 0.1757553070783615
iteration 118, loss = 0.1416158676147461
iteration 119, loss = 0.24351821839809418
iteration 120, loss = 0.03239904344081879
iteration 121, loss = 0.021626591682434082
iteration 122, loss = 0.023502148687839508
iteration 123, loss = 0.26030147075653076
iteration 124, loss = 0.24261952936649323
iteration 125, loss = 0.03280574455857277
iteration 126, loss = 0.05929584801197052
iteration 127, loss = 0.10404149442911148
iteration 128, loss = 0.1184387356042862
iteration 129, loss = 0.2602466940879822
iteration 130, loss = 0.23107053339481354
iteration 131, loss = 0.19354261457920074
iteration 132, loss = 0.07744166254997253
iteration 133, loss = 0.21638493239879608
iteration 134, loss = 0.28085464239120483
iteration 135, loss = 0.23985131084918976
iteration 136, loss = 0.1421632021665573
iteration 137, loss = 0.06171761453151703
iteration 138, loss = 0.2699209451675415
iteration 139, loss = 0.14614950120449066
iteration 140, loss = 0.09069720655679703
iteration 141, loss = 0.22104588150978088
iteration 142, loss = 0.11769527941942215
iteration 143, loss = 0.025436513125896454
iteration 144, loss = 0.1785716563463211
iteration 145, loss = 0.02265201136469841
iteration 146, loss = 0.017087113112211227
iteration 147, loss = 0.41328945755958557
iteration 148, loss = 0.3509967625141144
iteration 149, loss = 0.02516593225300312
iteration 150, loss = 0.28165683150291443
iteration 151, loss = 0.15630172193050385
iteration 152, loss = 0.38248884677886963
iteration 153, loss = 0.11914165318012238
iteration 154, loss = 0.043547123670578
iteration 155, loss = 0.021586012095212936
iteration 156, loss = 0.32247525453567505
iteration 157, loss = 0.3127635419368744
iteration 158, loss = 0.19965463876724243
iteration 159, loss = 0.30250585079193115
iteration 160, loss = 0.23267517983913422
iteration 161, loss = 0.17514082789421082
iteration 162, loss = 0.05039165914058685
iteration 163, loss = 0.4338315427303314
iteration 164, loss = 0.24571442604064941
iteration 165, loss = 0.13551726937294006
iteration 166, loss = 0.10130088031291962
iteration 167, loss = 0.24762019515037537
iteration 168, loss = 0.2775622606277466
iteration 169, loss = 0.40971678495407104
iteration 170, loss = 0.21556350588798523
iteration 171, loss = 0.07747654616832733
iteration 172, loss = 0.4392790198326111
iteration 173, loss = 0.21699635684490204
iteration 174, loss = 0.08815749734640121
iteration 175, loss = 0.32299309968948364
iteration 176, loss = 0.33073940873146057
iteration 177, loss = 0.1831161379814148
iteration 178, loss = 0.04804375767707825
iteration 179, loss = 0.06593012809753418
iteration 180, loss = 0.0489768348634243
iteration 181, loss = 0.02315090410411358
iteration 182, loss = 0.013802526518702507
iteration 183, loss = 0.222604900598526
iteration 184, loss = 0.30705341696739197
iteration 185, loss = 0.43142712116241455
iteration 186, loss = 0.4071003496646881
iteration 187, loss = 0.307746022939682
iteration 188, loss = 0.0992126613855362
iteration 189, loss = 0.2357339859008789
iteration 190, loss = 0.28916531801223755
iteration 191, loss = 0.3299907147884369
iteration 192, loss = 0.13275477290153503
iteration 193, loss = 0.11870266497135162
iteration 194, loss = 0.4509456753730774
iteration 195, loss = 0.06004974618554115
iteration 196, loss = 0.28818774223327637
iteration 197, loss = 0.2559061348438263
iteration 198, loss = 0.16867613792419434
iteration 199, loss = 0.05235478654503822
iteration 200, loss = 0.14973562955856323
iteration 201, loss = 0.1516295224428177
iteration 202, loss = 0.3155759871006012
iteration 203, loss = 0.19629280269145966
iteration 204, loss = 0.1525009274482727
iteration 205, loss = 0.2064049392938614
iteration 206, loss = 0.1799016296863556
iteration 207, loss = 0.10350079089403152
iteration 208, loss = 0.14335839450359344
iteration 209, loss = 0.3357340395450592
iteration 210, loss = 0.2648557424545288
iteration 211, loss = 0.27148640155792236
iteration 212, loss = 0.22158445417881012
iteration 213, loss = 0.12537693977355957
iteration 214, loss = 0.20282228291034698
iteration 215, loss = 0.2993411719799042
iteration 216, loss = 0.22143425047397614
iteration 217, loss = 0.10878871381282806
iteration 218, loss = 0.34574851393699646
iteration 219, loss = 0.3615182638168335
iteration 220, loss = 0.0012526335194706917
iteration 221, loss = 0.19308075308799744
iteration 222, loss = 0.21242982149124146
iteration 223, loss = 0.04746738448739052
iteration 224, loss = 0.1573375165462494
iteration 225, loss = 0.2722669243812561
iteration 226, loss = 0.09732576459646225
iteration 227, loss = 0.3614066243171692
iteration 228, loss = 0.21625368297100067
iteration 229, loss = 0.01865827664732933
iteration 230, loss = 0.41711777448654175
iteration 231, loss = 0.005813953932374716
iteration 232, loss = 0.15362244844436646
iteration 233, loss = 0.5299842953681946
iteration 234, loss = 0.37330958247184753
iteration 235, loss = 0.4847605228424072
iteration 236, loss = 0.174695685505867
iteration 237, loss = 0.06533057987689972
iteration 238, loss = 0.15536412596702576
iteration 239, loss = 0.25447747111320496
iteration 240, loss = 0.13622111082077026
iteration 241, loss = 0.35778480768203735
iteration 242, loss = 0.07804732769727707
iteration 243, loss = 0.24345572292804718
iteration 244, loss = 0.018186423927545547
iteration 245, loss = 0.1537095606327057
iteration 246, loss = 0.22493977844715118
iteration 247, loss = 0.15204696357250214
iteration 248, loss = 0.22361896932125092
iteration 249, loss = 0.07146946340799332
iteration 250, loss = 0.26302438974380493
iteration 251, loss = 0.08685273677110672
iteration 252, loss = 0.16620562970638275
iteration 253, loss = 0.17379683256149292
iteration 254, loss = 0.25111663341522217
iteration 255, loss = 0.2269892394542694
iteration 256, loss = 0.1681319624185562
iteration 257, loss = 0.0344977006316185
iteration 258, loss = 0.00577758252620697
iteration 259, loss = 0.21588855981826782
iteration 260, loss = 0.29204311966896057
iteration 261, loss = 0.024369467049837112
iteration 262, loss = 0.20408578217029572
iteration 263, loss = 0.3798125386238098
iteration 264, loss = 0.37937864661216736
iteration 265, loss = 0.182402104139328
iteration 266, loss = 0.1191447377204895
iteration 267, loss = 0.09432677179574966
iteration 268, loss = 0.1438915878534317
iteration 269, loss = 0.10309065133333206
iteration 270, loss = 0.03005797043442726
iteration 271, loss = 0.06774422526359558
iteration 272, loss = 0.11143188923597336
iteration 273, loss = 0.035342659801244736
iteration 274, loss = 0.16491809487342834
iteration 275, loss = 0.13579592108726501
iteration 276, loss = 0.1149837002158165
iteration 277, loss = 0.2158183753490448
iteration 278, loss = 0.23528535664081573
iteration 279, loss = 0.10000023245811462
iteration 280, loss = 0.16330118477344513
iteration 281, loss = 0.06238599866628647
iteration 282, loss = 0.02045307122170925
iteration 283, loss = 0.43811139464378357
iteration 284, loss = 0.13882094621658325
iteration 285, loss = 0.010381994768977165
iteration 286, loss = 0.20502230525016785
iteration 287, loss = 0.0040426310151815414
iteration 288, loss = 0.2088850736618042
iteration 289, loss = 0.08277911692857742
iteration 290, loss = 0.17871001362800598
iteration 291, loss = 0.29176241159439087
iteration 292, loss = 0.0741184651851654
iteration 293, loss = 0.14426983892917633
iteration 294, loss = 0.19261695444583893
iteration 295, loss = 0.19202890992164612
iteration 296, loss = 0.14547604322433472
iteration 297, loss = 0.03512468934059143
iteration 298, loss = 0.17057274281978607
iteration 299, loss = 0.20942671597003937
iteration 0, loss = 0.22203390300273895
iteration 1, loss = 0.38753819465637207
iteration 2, loss = 0.09920535981655121
iteration 3, loss = 0.1555984914302826
iteration 4, loss = 0.1604432910680771
iteration 5, loss = 0.33628103137016296
iteration 6, loss = 0.11764047294855118
iteration 7, loss = 0.1623181402683258
iteration 8, loss = 0.021249808371067047
iteration 9, loss = 0.0774642825126648
iteration 10, loss = 0.1235530823469162
iteration 11, loss = 0.2921670973300934
iteration 12, loss = 0.00278808968141675
iteration 13, loss = 0.009918500669300556
iteration 14, loss = 0.19322016835212708
iteration 15, loss = 0.27729716897010803
iteration 16, loss = 0.11500627547502518
iteration 17, loss = 0.1359373778104782
iteration 18, loss = 0.20840473473072052
iteration 19, loss = 0.3967307209968567
iteration 20, loss = 0.21961641311645508
iteration 21, loss = 0.0655967965722084
iteration 22, loss = 0.09062407165765762
iteration 23, loss = 0.05424913018941879
iteration 24, loss = 0.0777665302157402
iteration 25, loss = 0.05661890283226967
iteration 26, loss = 0.17413996160030365
iteration 27, loss = 0.16071346402168274
iteration 28, loss = 0.24723561108112335
iteration 29, loss = 0.07146237790584564
iteration 30, loss = 0.23818597197532654
iteration 31, loss = 0.1350790560245514
iteration 32, loss = 0.2357715219259262
iteration 33, loss = 0.16696880757808685
iteration 34, loss = 0.24740073084831238
iteration 35, loss = 0.15221761167049408
iteration 36, loss = 0.025787070393562317
iteration 37, loss = 0.06122785061597824
iteration 38, loss = 0.032630860805511475
iteration 39, loss = 0.07711821049451828
iteration 40, loss = 0.11743639409542084
iteration 41, loss = 0.023166539147496223
iteration 42, loss = 0.18351510167121887
iteration 43, loss = 0.13542547821998596
iteration 44, loss = 0.03479474037885666
iteration 45, loss = 0.18886762857437134
iteration 46, loss = 0.04817643389105797
iteration 47, loss = 0.07005667686462402
iteration 48, loss = 0.12047462165355682
iteration 49, loss = 0.0291752852499485
iteration 50, loss = 0.03223300352692604
iteration 51, loss = 0.006342780310660601
iteration 52, loss = 0.03008773922920227
iteration 53, loss = 0.006121481768786907
iteration 54, loss = 0.5578434467315674
iteration 55, loss = 0.328233927488327
iteration 56, loss = 0.06514032930135727
iteration 57, loss = 0.18393507599830627
iteration 58, loss = 0.5371604561805725
iteration 59, loss = 0.1669873297214508
iteration 60, loss = 0.30538663268089294
iteration 61, loss = 0.16818805038928986
iteration 62, loss = 0.6112046837806702
iteration 63, loss = 0.30537447333335876
iteration 64, loss = 0.1271785944700241
iteration 65, loss = 0.11322245001792908
iteration 66, loss = 0.22838182747364044
iteration 67, loss = 0.48966068029403687
iteration 68, loss = 0.36355459690093994
iteration 69, loss = 0.1225382536649704
iteration 70, loss = 0.27542978525161743
iteration 71, loss = 0.41925036907196045
iteration 72, loss = 0.22075854241847992
iteration 73, loss = 0.07358192652463913
iteration 74, loss = 0.1874142438173294
iteration 75, loss = 0.13616915047168732
iteration 76, loss = 0.6621245741844177
iteration 77, loss = 0.2862936556339264
iteration 78, loss = 0.18346776068210602
iteration 79, loss = 0.18594707548618317
iteration 80, loss = 0.18036840856075287
iteration 81, loss = 0.32415270805358887
iteration 82, loss = 0.15884006023406982
iteration 83, loss = 0.5399717688560486
iteration 84, loss = 0.335229754447937
iteration 85, loss = 0.23679926991462708
iteration 86, loss = 0.3176173269748688
iteration 87, loss = 0.3321928381919861
iteration 88, loss = 0.3168497085571289
iteration 89, loss = 0.5410497188568115
iteration 90, loss = 0.33049920201301575
iteration 91, loss = 0.14562959969043732
iteration 92, loss = 0.08626417815685272
iteration 93, loss = 0.1383516639471054
iteration 94, loss = 0.23635074496269226
iteration 95, loss = 0.44135117530822754
iteration 96, loss = 0.2857629656791687
iteration 97, loss = 0.5015402436256409
iteration 98, loss = 0.3839789628982544
iteration 99, loss = 0.21908529102802277
iteration 100, loss = 0.001119303866289556
iteration 101, loss = 0.30765336751937866
iteration 102, loss = 0.4841836988925934
iteration 103, loss = 0.391136109828949
iteration 104, loss = 0.47638657689094543
iteration 105, loss = 0.720552384853363
iteration 106, loss = 0.42475229501724243
iteration 107, loss = 0.2279786318540573
iteration 108, loss = 0.11459345370531082
iteration 109, loss = 0.19810298085212708
iteration 110, loss = 0.3049941956996918
iteration 111, loss = 0.3903130292892456
iteration 112, loss = 0.06891556084156036
iteration 113, loss = 0.14770416915416718
iteration 114, loss = 0.388868123292923
iteration 115, loss = 0.031031232327222824
iteration 116, loss = 0.013391852378845215
iteration 117, loss = 0.27126818895339966
iteration 118, loss = 0.17048023641109467
iteration 119, loss = 0.3963109850883484
iteration 120, loss = 0.277080237865448
iteration 121, loss = 0.2912827134132385
iteration 122, loss = 0.36787983775138855
iteration 123, loss = 0.5265218615531921
iteration 124, loss = 0.39082688093185425
iteration 125, loss = 0.22193801403045654
iteration 126, loss = 0.29137200117111206
iteration 127, loss = 0.7167321443557739
iteration 128, loss = 0.4728935658931732
iteration 129, loss = 0.12552127242088318
iteration 130, loss = 0.33997613191604614
iteration 131, loss = 0.3026983439922333
iteration 132, loss = 0.05536537989974022
iteration 133, loss = 0.06666600704193115
iteration 134, loss = 0.06601253896951675
iteration 135, loss = 0.05862852558493614
iteration 136, loss = 0.147299125790596
iteration 137, loss = 0.23196880519390106
iteration 138, loss = 0.0631016194820404
iteration 139, loss = 0.1097685918211937
iteration 140, loss = 0.21806655824184418
iteration 141, loss = 0.0484832301735878
iteration 142, loss = 0.13356022536754608
iteration 143, loss = 0.19022244215011597
iteration 144, loss = 0.1444597840309143
iteration 145, loss = 0.04423903673887253
iteration 146, loss = 0.153529554605484
iteration 147, loss = 0.3925951421260834
iteration 148, loss = 0.5050135850906372
iteration 149, loss = 0.23048779368400574
iteration 150, loss = 0.06563132256269455
iteration 151, loss = 0.13518092036247253
iteration 152, loss = 0.3530641198158264
iteration 153, loss = 0.1597471833229065
iteration 154, loss = 0.10250787436962128
iteration 155, loss = 0.14505061507225037
iteration 156, loss = 0.15460577607154846
iteration 157, loss = 0.2420133799314499
iteration 158, loss = 0.22912609577178955
iteration 159, loss = 0.020258471369743347
iteration 160, loss = 0.3258942663669586
iteration 161, loss = 0.21111920475959778
iteration 162, loss = 0.3367169499397278
iteration 163, loss = 0.1514536440372467
iteration 164, loss = 0.08947347104549408
iteration 165, loss = 0.08368536084890366
iteration 166, loss = 0.2824569046497345
iteration 167, loss = 0.2286737859249115
iteration 168, loss = 0.25068870186805725
iteration 169, loss = 0.1040484607219696
iteration 170, loss = 0.07474486529827118
iteration 171, loss = 0.215132936835289
iteration 172, loss = 0.20153461396694183
iteration 173, loss = 0.32669082283973694
iteration 174, loss = 0.004570144694298506
iteration 175, loss = 0.22224193811416626
iteration 176, loss = 0.14140042662620544
iteration 177, loss = 0.11780767142772675
iteration 178, loss = 0.22577990591526031
iteration 179, loss = 0.2348727136850357
iteration 180, loss = 0.2718118727207184
iteration 181, loss = 0.072058767080307
iteration 182, loss = 0.2713901698589325
iteration 183, loss = 0.11816380172967911
iteration 184, loss = 0.2140979915857315
iteration 185, loss = 0.5739089250564575
iteration 186, loss = 0.23292584717273712
iteration 187, loss = 0.19575199484825134
iteration 188, loss = 0.29744455218315125
iteration 189, loss = 0.08671002835035324
iteration 190, loss = 0.04408290982246399
iteration 191, loss = 0.1689033955335617
iteration 192, loss = 0.036608900874853134
iteration 193, loss = 0.27693986892700195
iteration 194, loss = 0.1718142330646515
iteration 195, loss = 0.06387659907341003
iteration 196, loss = 0.12121595442295074
iteration 197, loss = 0.06776664406061172
iteration 198, loss = 0.1937028020620346
iteration 199, loss = 0.2679900527000427
iteration 200, loss = 0.21382489800453186
iteration 201, loss = 0.006152654532343149
iteration 202, loss = 0.15130624175071716
iteration 203, loss = 0.08507601916790009
iteration 204, loss = 0.1663842648267746
iteration 205, loss = 0.1242712140083313
iteration 206, loss = 0.12967632710933685
iteration 207, loss = 0.08308637142181396
iteration 208, loss = 0.19513589143753052
iteration 209, loss = 0.14154991507530212
iteration 210, loss = 0.10669101774692535
iteration 211, loss = 0.18746672570705414
iteration 212, loss = 0.0964985340833664
iteration 213, loss = 0.12230128794908524
iteration 214, loss = 0.2687840163707733
iteration 215, loss = 0.3202560544013977
iteration 216, loss = 0.1455533504486084
iteration 217, loss = 0.1379707008600235
iteration 218, loss = 0.1512961983680725
iteration 219, loss = 0.17738153040409088
iteration 220, loss = 0.0836370438337326
iteration 221, loss = 0.11458061635494232
iteration 222, loss = 0.1558237075805664
iteration 223, loss = 0.016048725694417953
iteration 224, loss = 0.06957454234361649
iteration 225, loss = 0.10739213973283768
iteration 226, loss = 0.04104556888341904
iteration 227, loss = 0.19432216882705688
iteration 228, loss = 0.08934152126312256
iteration 229, loss = 0.04585680365562439
iteration 230, loss = 0.04380812123417854
iteration 231, loss = 0.041513435542583466
iteration 232, loss = 0.10257919877767563
iteration 233, loss = 0.15421469509601593
iteration 234, loss = 0.09396813809871674
iteration 235, loss = 0.20453624427318573
iteration 236, loss = 0.05176454037427902
iteration 237, loss = 0.14545975625514984
iteration 238, loss = 0.10659728944301605
iteration 239, loss = 0.046172551810741425
iteration 240, loss = 0.08354770392179489
iteration 241, loss = 0.1884877234697342
iteration 242, loss = 0.008029432035982609
iteration 243, loss = 0.27715420722961426
iteration 244, loss = 0.06877777725458145
iteration 245, loss = 0.18286272883415222
iteration 246, loss = 0.11736343801021576
iteration 247, loss = 0.24148714542388916
iteration 248, loss = 0.15766361355781555
iteration 249, loss = 0.5139289498329163
iteration 250, loss = 0.4634304940700531
iteration 251, loss = 0.13996092975139618
iteration 252, loss = 0.19835108518600464
iteration 253, loss = 0.22846420109272003
iteration 254, loss = 0.3080718219280243
iteration 255, loss = 0.1642247438430786
iteration 256, loss = 0.13688375055789948
iteration 257, loss = 0.1079588234424591
iteration 258, loss = 0.10076148808002472
iteration 259, loss = 0.22456905245780945
iteration 260, loss = 0.05588742718100548
iteration 261, loss = 0.0333116389811039
iteration 262, loss = 0.10515858232975006
iteration 263, loss = 0.015994135290384293
iteration 264, loss = 0.14138928055763245
iteration 265, loss = 0.05829983204603195
iteration 266, loss = 0.36068084836006165
iteration 267, loss = 0.028893105685710907
iteration 268, loss = 0.1936464011669159
iteration 269, loss = 0.11750882118940353
iteration 270, loss = 0.07453351467847824
iteration 271, loss = 0.06527727097272873
iteration 272, loss = 0.07522965967655182
iteration 273, loss = 0.16235366463661194
iteration 274, loss = 0.1909167468547821
iteration 275, loss = 0.2340449094772339
iteration 276, loss = 0.08932214230298996
iteration 277, loss = 0.09198533743619919
iteration 278, loss = 0.14780710637569427
iteration 279, loss = 0.1575884371995926
iteration 280, loss = 0.15076230466365814
iteration 281, loss = 0.12125349044799805
iteration 282, loss = 0.19521071016788483
iteration 283, loss = 0.16652098298072815
iteration 284, loss = 0.07242883741855621
iteration 285, loss = 0.18149778246879578
iteration 286, loss = 0.20557108521461487
iteration 287, loss = 0.15624848008155823
iteration 288, loss = 0.20947836339473724
iteration 289, loss = 0.06056615710258484
iteration 290, loss = 0.1658596247434616
iteration 291, loss = 0.07318217307329178
iteration 292, loss = 0.04773878678679466
iteration 293, loss = 0.010300504975020885
iteration 294, loss = 0.1931925266981125
iteration 295, loss = 0.02383757196366787
iteration 296, loss = 0.347438782453537
iteration 297, loss = 0.18277709186077118
iteration 298, loss = 0.21619977056980133
iteration 299, loss = 0.053819913417100906
iteration 0, loss = 0.07869020104408264
iteration 1, loss = 0.22921286523342133
iteration 2, loss = 0.060555294156074524
iteration 3, loss = 0.15341325104236603
iteration 4, loss = 0.2568112909793854
iteration 5, loss = 0.06959307938814163
iteration 6, loss = 0.14671437442302704
iteration 7, loss = 0.1852143257856369
iteration 8, loss = 0.053319402039051056
iteration 9, loss = 0.23539160192012787
iteration 10, loss = 0.05593409016728401
iteration 11, loss = 0.13802802562713623
iteration 12, loss = 0.18999743461608887
iteration 13, loss = 0.06728272140026093
iteration 14, loss = 0.13902868330478668
iteration 15, loss = 0.19183792173862457
iteration 16, loss = 0.08851708471775055
iteration 17, loss = 0.18432225286960602
iteration 18, loss = 0.14275439083576202
iteration 19, loss = 0.15568116307258606
iteration 20, loss = 0.05245118588209152
iteration 21, loss = 0.0654665008187294
iteration 22, loss = 0.039005350321531296
iteration 23, loss = 0.23679310083389282
iteration 24, loss = 0.04483781009912491
iteration 25, loss = 0.052460551261901855
iteration 26, loss = 0.11512023210525513
iteration 27, loss = 0.10231031477451324
iteration 28, loss = 0.19534724950790405
iteration 29, loss = 0.2469649612903595
iteration 30, loss = 0.30520084500312805
iteration 31, loss = 0.11632437258958817
iteration 32, loss = 0.21891474723815918
iteration 33, loss = 0.05184745788574219
iteration 34, loss = 0.19623583555221558
iteration 35, loss = 0.3481903374195099
iteration 36, loss = 0.19580048322677612
iteration 37, loss = 0.24952693283557892
iteration 38, loss = 0.10815726965665817
iteration 39, loss = 0.34463897347450256
iteration 40, loss = 0.27542224526405334
iteration 41, loss = 0.05390194058418274
iteration 42, loss = 0.01776830106973648
iteration 43, loss = 0.07270967960357666
iteration 44, loss = 0.02823321893811226
iteration 45, loss = 0.12835808098316193
iteration 46, loss = 0.27499300241470337
iteration 47, loss = 0.0312819667160511
iteration 48, loss = 0.1290462613105774
iteration 49, loss = 0.13968703150749207
iteration 50, loss = 0.12020476162433624
iteration 51, loss = 0.07959393411874771
iteration 52, loss = 0.17369702458381653
iteration 53, loss = 0.033797867596149445
iteration 54, loss = 0.1421186625957489
iteration 55, loss = 0.10809498280286789
iteration 56, loss = 0.10384395718574524
iteration 57, loss = 0.29578283429145813
iteration 58, loss = 0.202315092086792
iteration 59, loss = 0.17611227929592133
iteration 60, loss = 0.3229604959487915
iteration 61, loss = 0.2531830668449402
iteration 62, loss = 0.20126447081565857
iteration 63, loss = 0.08010949194431305
iteration 64, loss = 0.10426945984363556
iteration 65, loss = 0.18566729128360748
iteration 66, loss = 0.33545756340026855
iteration 67, loss = 0.13352477550506592
iteration 68, loss = 0.10495096445083618
iteration 69, loss = 0.15420851111412048
iteration 70, loss = 0.36634302139282227
iteration 71, loss = 0.1336006224155426
iteration 72, loss = 0.06014830619096756
iteration 73, loss = 0.21486373245716095
iteration 74, loss = 0.016506901010870934
iteration 75, loss = 0.08629804104566574
iteration 76, loss = 0.07887622714042664
iteration 77, loss = 0.18948373198509216
iteration 78, loss = 0.16079241037368774
iteration 79, loss = 0.07560078054666519
iteration 80, loss = 0.2433483898639679
iteration 81, loss = 0.17118294537067413
iteration 82, loss = 0.09424388408660889
iteration 83, loss = 0.07120027393102646
iteration 84, loss = 0.009113605134189129
iteration 85, loss = 0.2776292860507965
iteration 86, loss = 0.05103297531604767
iteration 87, loss = 0.2592102587223053
iteration 88, loss = 0.11463285982608795
iteration 89, loss = 0.1630793809890747
iteration 90, loss = 0.15211549401283264
iteration 91, loss = 0.18092387914657593
iteration 92, loss = 0.1412394940853119
iteration 93, loss = 0.13681578636169434
iteration 94, loss = 0.1107027605175972
iteration 95, loss = 0.14060647785663605
iteration 96, loss = 0.19488656520843506
iteration 97, loss = 0.15222591161727905
iteration 98, loss = 0.07783572375774384
iteration 99, loss = 0.20810359716415405
iteration 100, loss = 0.12737281620502472
iteration 101, loss = 0.09962403029203415
iteration 102, loss = 0.15939071774482727
iteration 103, loss = 0.1474667340517044
iteration 104, loss = 0.1922910511493683
iteration 105, loss = 0.08328713476657867
iteration 106, loss = 0.12975628674030304
iteration 107, loss = 0.10497002303600311
iteration 108, loss = 0.14472228288650513
iteration 109, loss = 0.08372215926647186
iteration 110, loss = 0.050698570907115936
iteration 111, loss = 0.09638877958059311
iteration 112, loss = 0.026311930269002914
iteration 113, loss = 0.229233518242836
iteration 114, loss = 0.13914854824543
iteration 115, loss = 0.04388456046581268
iteration 116, loss = 0.13283897936344147
iteration 117, loss = 0.23055313527584076
iteration 118, loss = 0.2312404215335846
iteration 119, loss = 0.33125996589660645
iteration 120, loss = 0.30489522218704224
iteration 121, loss = 0.21857428550720215
iteration 122, loss = 0.10830117762088776
iteration 123, loss = 0.047965772449970245
iteration 124, loss = 0.06097416579723358
iteration 125, loss = 0.04482654854655266
iteration 126, loss = 0.02161436155438423
iteration 127, loss = 0.2000477910041809
iteration 128, loss = 0.012316405773162842
iteration 129, loss = 0.17437908053398132
iteration 130, loss = 0.14773128926753998
iteration 131, loss = 0.21896086633205414
iteration 132, loss = 0.034267548471689224
iteration 133, loss = 0.08751754462718964
iteration 134, loss = 0.14555619657039642
iteration 135, loss = 0.052603818476200104
iteration 136, loss = 0.19505000114440918
iteration 137, loss = 0.1320675015449524
iteration 138, loss = 0.07839736342430115
iteration 139, loss = 0.22878101468086243
iteration 140, loss = 0.2349579781293869
iteration 141, loss = 0.11136583983898163
iteration 142, loss = 0.018154200166463852
iteration 143, loss = 0.21790218353271484
iteration 144, loss = 0.14789199829101562
iteration 145, loss = 0.09392135590314865
iteration 146, loss = 0.09691697359085083
iteration 147, loss = 0.186503067612648
iteration 148, loss = 0.06038921698927879
iteration 149, loss = 0.19684897363185883
iteration 150, loss = 0.06470709294080734
iteration 151, loss = 0.04709465429186821
iteration 152, loss = 0.2641865015029907
iteration 153, loss = 0.05593361333012581
iteration 154, loss = 0.17607173323631287
iteration 155, loss = 0.14920851588249207
iteration 156, loss = 0.12355397641658783
iteration 157, loss = 0.12747368216514587
iteration 158, loss = 0.17257653176784515
iteration 159, loss = 0.0492640882730484
iteration 160, loss = 0.14409378170967102
iteration 161, loss = 0.10678285360336304
iteration 162, loss = 0.24172908067703247
iteration 163, loss = 0.15957464277744293
iteration 164, loss = 0.21846117079257965
iteration 165, loss = 0.16603577136993408
iteration 166, loss = 0.3257604241371155
iteration 167, loss = 0.16725826263427734
iteration 168, loss = 0.01721017248928547
iteration 169, loss = 0.06490589678287506
iteration 170, loss = 0.15779569745063782
iteration 171, loss = 0.18441399931907654
iteration 172, loss = 0.09146493673324585
iteration 173, loss = 0.139251247048378
iteration 174, loss = 0.10565999150276184
iteration 175, loss = 0.10638166964054108
iteration 176, loss = 0.18238994479179382
iteration 177, loss = 0.3133874833583832
iteration 178, loss = 0.10971015691757202
iteration 179, loss = 0.04578079283237457
iteration 180, loss = 0.14316532015800476
iteration 181, loss = 0.24907997250556946
iteration 182, loss = 0.015450267121195793
iteration 183, loss = 0.15486687421798706
iteration 184, loss = 0.12182392925024033
iteration 185, loss = 0.0398981012403965
iteration 186, loss = 0.0134779904037714
iteration 187, loss = 0.1911819726228714
iteration 188, loss = 0.20301352441310883
iteration 189, loss = 0.37142252922058105
iteration 190, loss = 0.2197093963623047
iteration 191, loss = 0.304159015417099
iteration 192, loss = 0.2324756681919098
iteration 193, loss = 0.10658863186836243
iteration 194, loss = 0.09788253903388977
iteration 195, loss = 0.1512031853199005
iteration 196, loss = 0.18571743369102478
iteration 197, loss = 0.14670580625534058
iteration 198, loss = 0.18640165030956268
iteration 199, loss = 0.3897028863430023
iteration 200, loss = 0.29149410128593445
iteration 201, loss = 0.11837246268987656
iteration 202, loss = 0.11503134667873383
iteration 203, loss = 0.1035526916384697
iteration 204, loss = 0.44605952501296997
iteration 205, loss = 0.2167149931192398
iteration 206, loss = 0.07569501549005508
iteration 207, loss = 0.12248633801937103
iteration 208, loss = 0.17665870487689972
iteration 209, loss = 0.3544634282588959
iteration 210, loss = 0.14901202917099
iteration 211, loss = 0.3009895086288452
iteration 212, loss = 0.18583562970161438
iteration 213, loss = 0.17216424643993378
iteration 214, loss = 0.08631961792707443
iteration 215, loss = 0.3006018400192261
iteration 216, loss = 0.1916944533586502
iteration 217, loss = 0.1784450113773346
iteration 218, loss = 0.04045553505420685
iteration 219, loss = 0.06491149961948395
iteration 220, loss = 0.20444245636463165
iteration 221, loss = 0.19044342637062073
iteration 222, loss = 0.09087993949651718
iteration 223, loss = 0.055969007313251495
iteration 224, loss = 0.11770813167095184
iteration 225, loss = 0.20207983255386353
iteration 226, loss = 0.12339051067829132
iteration 227, loss = 0.13960587978363037
iteration 228, loss = 0.11496274918317795
iteration 229, loss = 0.1297823041677475
iteration 230, loss = 0.09639003872871399
iteration 231, loss = 0.17290550470352173
iteration 232, loss = 0.1433185338973999
iteration 233, loss = 0.11469177156686783
iteration 234, loss = 0.07045303285121918
iteration 235, loss = 0.10199189186096191
iteration 236, loss = 0.06529644131660461
iteration 237, loss = 0.03795593976974487
iteration 238, loss = 0.22063621878623962
iteration 239, loss = 0.037574052810668945
iteration 240, loss = 0.23629207909107208
iteration 241, loss = 0.03748755529522896
iteration 242, loss = 0.11985395848751068
iteration 243, loss = 0.10551903396844864
iteration 244, loss = 0.18424440920352936
iteration 245, loss = 0.10891054570674896
iteration 246, loss = 0.14890988171100616
iteration 247, loss = 0.10071767121553421
iteration 248, loss = 0.07586319744586945
iteration 249, loss = 0.1628972589969635
iteration 250, loss = 0.2789730429649353
iteration 251, loss = 0.2848300039768219
iteration 252, loss = 0.13455666601657867
iteration 253, loss = 0.08621549606323242
iteration 254, loss = 0.12140138447284698
iteration 255, loss = 0.2327064722776413
iteration 256, loss = 0.24530191719532013
iteration 257, loss = 0.2729460299015045
iteration 258, loss = 0.10822495073080063
iteration 259, loss = 0.0057210773229599
iteration 260, loss = 0.34163719415664673
iteration 261, loss = 0.2332441359758377
iteration 262, loss = 0.8750210404396057
iteration 263, loss = 0.2324957698583603
iteration 264, loss = 0.6264804005622864
iteration 265, loss = 0.5324797034263611
iteration 266, loss = 0.33732181787490845
iteration 267, loss = 0.07395029067993164
iteration 268, loss = 0.3596428632736206
iteration 269, loss = 0.3120594620704651
iteration 270, loss = 0.4910631477832794
iteration 271, loss = 0.21457916498184204
iteration 272, loss = 0.26982226967811584
iteration 273, loss = 0.11768708378076553
iteration 274, loss = 0.023905184119939804
iteration 275, loss = 0.2974778115749359
iteration 276, loss = 0.2501782476902008
iteration 277, loss = 0.16561251878738403
iteration 278, loss = 0.011676538735628128
iteration 279, loss = 0.11952721327543259
iteration 280, loss = 0.4318879246711731
iteration 281, loss = 0.11925631761550903
iteration 282, loss = 0.30302172899246216
iteration 283, loss = 0.19089867174625397
iteration 284, loss = 0.3882129192352295
iteration 285, loss = 0.15598884224891663
iteration 286, loss = 0.14058609306812286
iteration 287, loss = 0.17178447544574738
iteration 288, loss = 0.0341029167175293
iteration 289, loss = 0.17261582612991333
iteration 290, loss = 0.09252822399139404
iteration 291, loss = 0.47994697093963623
iteration 292, loss = 0.04725226014852524
iteration 293, loss = 0.2063906341791153
iteration 294, loss = 0.15796814858913422
iteration 295, loss = 0.12345083057880402
iteration 296, loss = 0.23113363981246948
iteration 297, loss = 0.19518238306045532
iteration 298, loss = 0.24804428219795227
iteration 299, loss = 0.11953195929527283
iteration 0, loss = 0.11416900902986526
iteration 1, loss = 0.1186932772397995
iteration 2, loss = 0.08309682458639145
iteration 3, loss = 0.05290323495864868
iteration 4, loss = 0.1912965327501297
iteration 5, loss = 0.19226767122745514
iteration 6, loss = 0.13913504779338837
iteration 7, loss = 0.19843897223472595
iteration 8, loss = 0.08441980183124542
iteration 9, loss = 0.2813953161239624
iteration 10, loss = 0.11094601452350616
iteration 11, loss = 0.08859510719776154
iteration 12, loss = 0.08608053624629974
iteration 13, loss = 0.14360806345939636
iteration 14, loss = 0.06279100477695465
iteration 15, loss = 0.0851009264588356
iteration 16, loss = 0.1017618402838707
iteration 17, loss = 0.0885775238275528
iteration 18, loss = 0.013681118376553059
iteration 19, loss = 0.1383042186498642
iteration 20, loss = 0.13513214886188507
iteration 21, loss = 0.12316078692674637
iteration 22, loss = 0.37570273876190186
iteration 23, loss = 0.16600564122200012
iteration 24, loss = 0.1263163536787033
iteration 25, loss = 0.2741023302078247
iteration 26, loss = 0.21204416453838348
iteration 27, loss = 0.03313034772872925
iteration 28, loss = 0.22320759296417236
iteration 29, loss = 0.10564365983009338
iteration 30, loss = 0.1693786233663559
iteration 31, loss = 0.036764685064554214
iteration 32, loss = 0.006915752775967121
iteration 33, loss = 0.17314881086349487
iteration 34, loss = 0.08707204461097717
iteration 35, loss = 0.1256721019744873
iteration 36, loss = 0.32116079330444336
iteration 37, loss = 0.3051368296146393
iteration 38, loss = 0.193708598613739
iteration 39, loss = 0.0866997241973877
iteration 40, loss = 0.26484209299087524
iteration 41, loss = 0.2571128010749817
iteration 42, loss = 0.12867985665798187
iteration 43, loss = 0.2148038148880005
iteration 44, loss = 0.04775884002447128
iteration 45, loss = 0.17097465693950653
iteration 46, loss = 0.27955424785614014
iteration 47, loss = 0.2267352044582367
iteration 48, loss = 0.04555293917655945
iteration 49, loss = 0.04821076616644859
iteration 50, loss = 0.25369104743003845
iteration 51, loss = 0.16975924372673035
iteration 52, loss = 0.11726655066013336
iteration 53, loss = 0.16337016224861145
iteration 54, loss = 0.1953657865524292
iteration 55, loss = 0.1811692863702774
iteration 56, loss = 0.18671928346157074
iteration 57, loss = 0.12958166003227234
iteration 58, loss = 0.2800738215446472
iteration 59, loss = 0.05314946547150612
iteration 60, loss = 0.19424332678318024
iteration 61, loss = 0.1461666077375412
iteration 62, loss = 0.09596046805381775
iteration 63, loss = 0.03805455565452576
iteration 64, loss = 0.07226153463125229
iteration 65, loss = 0.13424137234687805
iteration 66, loss = 0.11440296471118927
iteration 67, loss = 0.2833212912082672
iteration 68, loss = 0.10268089175224304
iteration 69, loss = 0.08881673216819763
iteration 70, loss = 0.07596903294324875
iteration 71, loss = 0.23550139367580414
iteration 72, loss = 0.12161941826343536
iteration 73, loss = 0.08387039601802826
iteration 74, loss = 0.042403511703014374
iteration 75, loss = 0.14018592238426208
iteration 76, loss = 0.42103034257888794
iteration 77, loss = 0.5169731378555298
iteration 78, loss = 0.5945541858673096
iteration 79, loss = 0.003898241091519594
iteration 80, loss = 0.19054542481899261
iteration 81, loss = 0.06636130064725876
iteration 82, loss = 0.28627026081085205
iteration 83, loss = 0.19597500562667847
iteration 84, loss = 0.15854206681251526
iteration 85, loss = 0.14398790895938873
iteration 86, loss = 0.1668395698070526
iteration 87, loss = 0.13710369169712067
iteration 88, loss = 0.13148894906044006
iteration 89, loss = 0.0778362974524498
iteration 90, loss = 0.1640540361404419
iteration 91, loss = 0.1992150992155075
iteration 92, loss = 0.04425607621669769
iteration 93, loss = 0.028026793152093887
iteration 94, loss = 0.05043709650635719
iteration 95, loss = 0.15530580282211304
iteration 96, loss = 0.15130892395973206
iteration 97, loss = 0.09095441550016403
iteration 98, loss = 0.036478593945503235
iteration 99, loss = 0.22050464153289795
iteration 100, loss = 0.17103949189186096
iteration 101, loss = 0.09849752485752106
iteration 102, loss = 0.1621135175228119
iteration 103, loss = 0.21085549890995026
iteration 104, loss = 0.24058231711387634
iteration 105, loss = 0.11799843609333038
iteration 106, loss = 0.045871809124946594
iteration 107, loss = 0.07086262106895447
iteration 108, loss = 0.07623019069433212
iteration 109, loss = 0.25307947397232056
iteration 110, loss = 0.16842685639858246
iteration 111, loss = 0.11125323176383972
iteration 112, loss = 0.12327255308628082
iteration 113, loss = 0.05465637519955635
iteration 114, loss = 0.06039353460073471
iteration 115, loss = 0.1327974647283554
iteration 116, loss = 0.1418089121580124
iteration 117, loss = 0.12485149502754211
iteration 118, loss = 0.06664198637008667
iteration 119, loss = 0.08784022182226181
iteration 120, loss = 0.10562991350889206
iteration 121, loss = 0.06455051153898239
iteration 122, loss = 0.4226263761520386
iteration 123, loss = 0.42144280672073364
iteration 124, loss = 0.24742789566516876
iteration 125, loss = 0.2463497370481491
iteration 126, loss = 0.08317870646715164
iteration 127, loss = 0.18515874445438385
iteration 128, loss = 0.22655443847179413
iteration 129, loss = 0.14226675033569336
iteration 130, loss = 0.06825168430805206
iteration 131, loss = 0.016904789954423904
iteration 132, loss = 0.04711465910077095
iteration 133, loss = 0.368437260389328
iteration 134, loss = 0.5455541014671326
iteration 135, loss = 0.26354750990867615
iteration 136, loss = 0.043879445642232895
iteration 137, loss = 0.15113472938537598
iteration 138, loss = 0.29564622044563293
iteration 139, loss = 0.1880796253681183
iteration 140, loss = 0.1238517090678215
iteration 141, loss = 0.15402348339557648
iteration 142, loss = 0.42537498474121094
iteration 143, loss = 0.5701440572738647
iteration 144, loss = 0.2616059482097626
iteration 145, loss = 0.23631598055362701
iteration 146, loss = 0.04705534875392914
iteration 147, loss = 0.25210142135620117
iteration 148, loss = 0.012713673524558544
iteration 149, loss = 0.060662128031253815
iteration 150, loss = 0.15064144134521484
iteration 151, loss = 0.026901211589574814
iteration 152, loss = 0.039971839636564255
iteration 153, loss = 0.0639423206448555
iteration 154, loss = 0.009877980686724186
iteration 155, loss = 0.09243503957986832
iteration 156, loss = 0.10083404928445816
iteration 157, loss = 0.31192025542259216
iteration 158, loss = 0.16275285184383392
iteration 159, loss = 0.14335203170776367
iteration 160, loss = 0.13817225396633148
iteration 161, loss = 0.4270910918712616
iteration 162, loss = 0.052302539348602295
iteration 163, loss = 0.1656605303287506
iteration 164, loss = 0.2130640149116516
iteration 165, loss = 0.09656994044780731
iteration 166, loss = 0.3378714323043823
iteration 167, loss = 0.13824063539505005
iteration 168, loss = 0.3323477804660797
iteration 169, loss = 0.08688576519489288
iteration 170, loss = 0.025374365970492363
iteration 171, loss = 0.08305563777685165
iteration 172, loss = 0.12339042127132416
iteration 173, loss = 0.25686022639274597
iteration 174, loss = 0.22680936753749847
iteration 175, loss = 0.14626061916351318
iteration 176, loss = 0.15097446739673615
iteration 177, loss = 0.10224782675504684
iteration 178, loss = 0.03431519865989685
iteration 179, loss = 0.32854336500167847
iteration 180, loss = 0.03229626640677452
iteration 181, loss = 0.20466870069503784
iteration 182, loss = 0.19900813698768616
iteration 183, loss = 0.03769064322113991
iteration 184, loss = 0.20228707790374756
iteration 185, loss = 0.1559583842754364
iteration 186, loss = 0.29776981472969055
iteration 187, loss = 0.2512716054916382
iteration 188, loss = 0.08686015754938126
iteration 189, loss = 0.09172270447015762
iteration 190, loss = 0.07069426774978638
iteration 191, loss = 0.12491156160831451
iteration 192, loss = 0.01723623089492321
iteration 193, loss = 0.07189970463514328
iteration 194, loss = 0.24391791224479675
iteration 195, loss = 0.07024148106575012
iteration 196, loss = 0.008909402415156364
iteration 197, loss = 0.05080537497997284
iteration 198, loss = 0.15319697558879852
iteration 199, loss = 0.026145493611693382
iteration 200, loss = 0.18918927013874054
iteration 201, loss = 0.05913008376955986
iteration 202, loss = 0.1943119913339615
iteration 203, loss = 0.18633168935775757
iteration 204, loss = 0.11780082434415817
iteration 205, loss = 0.1325305849313736
iteration 206, loss = 0.30602163076400757
iteration 207, loss = 0.09327676892280579
iteration 208, loss = 0.029267067089676857
iteration 209, loss = 0.0009878508280962706
iteration 210, loss = 0.22757962346076965
iteration 211, loss = 0.2176354080438614
iteration 212, loss = 0.014453331008553505
iteration 213, loss = 0.31820353865623474
iteration 214, loss = 0.15090392529964447
iteration 215, loss = 0.7239718437194824
iteration 216, loss = 0.1437588334083557
iteration 217, loss = 0.11522611975669861
iteration 218, loss = 0.0847570151090622
iteration 219, loss = 0.06280568242073059
iteration 220, loss = 0.06736952811479568
iteration 221, loss = 0.17217670381069183
iteration 222, loss = 0.36704960465431213
iteration 223, loss = 0.1736331731081009
iteration 224, loss = 0.04868140071630478
iteration 225, loss = 0.1303125023841858
iteration 226, loss = 0.32317686080932617
iteration 227, loss = 0.1829855889081955
iteration 228, loss = 0.11480787396430969
iteration 229, loss = 0.4338086247444153
iteration 230, loss = 0.3942973017692566
iteration 231, loss = 0.019382016733288765
iteration 232, loss = 0.009522110223770142
iteration 233, loss = 0.049843888729810715
iteration 234, loss = 0.1788703203201294
iteration 235, loss = 0.14773407578468323
iteration 236, loss = 0.158477321267128
iteration 237, loss = 0.19225774705410004
iteration 238, loss = 0.1974201500415802
iteration 239, loss = 0.1897640973329544
iteration 240, loss = 0.13343074917793274
iteration 241, loss = 0.1714569330215454
iteration 242, loss = 0.0774109959602356
iteration 243, loss = 0.12578308582305908
iteration 244, loss = 0.19264468550682068
iteration 245, loss = 0.00971657782793045
iteration 246, loss = 0.11544857919216156
iteration 247, loss = 0.2861758768558502
iteration 248, loss = 0.45943334698677063
iteration 249, loss = 0.28010448813438416
iteration 250, loss = 0.4120732843875885
iteration 251, loss = 0.28655847907066345
iteration 252, loss = 0.24235989153385162
iteration 253, loss = 0.2840057909488678
iteration 254, loss = 0.08123050630092621
iteration 255, loss = 0.06926155835390091
iteration 256, loss = 0.006717537064105272
iteration 257, loss = 0.2931760549545288
iteration 258, loss = 0.23126095533370972
iteration 259, loss = 0.0011299492325633764
iteration 260, loss = 0.33796581625938416
iteration 261, loss = 0.19595623016357422
iteration 262, loss = 0.03066382184624672
iteration 263, loss = 0.20824603736400604
iteration 264, loss = 0.3068912923336029
iteration 265, loss = 0.24623937904834747
iteration 266, loss = 0.12453867495059967
iteration 267, loss = 0.23702344298362732
iteration 268, loss = 0.11135003715753555
iteration 269, loss = 0.36625248193740845
iteration 270, loss = 0.023558057844638824
iteration 271, loss = 0.10983938723802567
iteration 272, loss = 0.16386082768440247
iteration 273, loss = 0.1260928362607956
iteration 274, loss = 0.014170875772833824
iteration 275, loss = 0.018126824870705605
iteration 276, loss = 0.0066667962819337845
iteration 277, loss = 0.4067896604537964
iteration 278, loss = 0.002133913803845644
iteration 279, loss = 0.03297651559114456
iteration 280, loss = 0.041006918996572495
iteration 281, loss = 0.23746496438980103
iteration 282, loss = 0.08323323726654053
iteration 283, loss = 0.27716100215911865
iteration 284, loss = 0.23663145303726196
iteration 285, loss = 0.1754169464111328
iteration 286, loss = 0.0987033024430275
iteration 287, loss = 0.2505985200405121
iteration 288, loss = 0.037812940776348114
iteration 289, loss = 0.14247596263885498
iteration 290, loss = 0.3313905894756317
iteration 291, loss = 0.05468086525797844
iteration 292, loss = 0.14568063616752625
iteration 293, loss = 0.06954377889633179
iteration 294, loss = 0.12330744415521622
iteration 295, loss = 0.15392783284187317
iteration 296, loss = 0.18033403158187866
iteration 297, loss = 0.3573126494884491
iteration 298, loss = 0.3380149006843567
iteration 299, loss = 0.09368450194597244
iteration 0, loss = 0.07578876614570618
iteration 1, loss = 0.2633264660835266
iteration 2, loss = 0.3184577524662018
iteration 3, loss = 0.03589032590389252
iteration 4, loss = 0.24647831916809082
iteration 5, loss = 0.05983038619160652
iteration 6, loss = 0.04271373897790909
iteration 7, loss = 0.21654552221298218
iteration 8, loss = 0.13728712499141693
iteration 9, loss = 0.08584307134151459
iteration 10, loss = 0.10363618284463882
iteration 11, loss = 0.19013983011245728
iteration 12, loss = 0.1286468654870987
iteration 13, loss = 0.048801690340042114
iteration 14, loss = 0.07694844901561737
iteration 15, loss = 0.07684121280908585
iteration 16, loss = 0.026497099548578262
iteration 17, loss = 0.2614728510379791
iteration 18, loss = 0.19111645221710205
iteration 19, loss = 0.32314714789390564
iteration 20, loss = 0.12590718269348145
iteration 21, loss = 0.10793851315975189
iteration 22, loss = 0.16959406435489655
iteration 23, loss = 0.08104299008846283
iteration 24, loss = 0.21822622418403625
iteration 25, loss = 0.031371407210826874
iteration 26, loss = 0.10919070243835449
iteration 27, loss = 0.47457069158554077
iteration 28, loss = 0.22199152410030365
iteration 29, loss = 0.07328768074512482
iteration 30, loss = 0.18073444068431854
iteration 31, loss = 0.1080632209777832
iteration 32, loss = 0.13776728510856628
iteration 33, loss = 0.1702144891023636
iteration 34, loss = 0.04390057921409607
iteration 35, loss = 0.134402334690094
iteration 36, loss = 0.09240022301673889
iteration 37, loss = 0.18192514777183533
iteration 38, loss = 0.17469891905784607
iteration 39, loss = 0.12730450928211212
iteration 40, loss = 0.17600710690021515
iteration 41, loss = 0.13982784748077393
iteration 42, loss = 0.07747334986925125
iteration 43, loss = 0.1372634321451187
iteration 44, loss = 0.09808499366044998
iteration 45, loss = 0.04434998705983162
iteration 46, loss = 0.07072223722934723
iteration 47, loss = 0.1406784951686859
iteration 48, loss = 0.03626243770122528
iteration 49, loss = 0.14346842467784882
iteration 50, loss = 0.192400261759758
iteration 51, loss = 0.03759923577308655
iteration 52, loss = 0.1072140708565712
iteration 53, loss = 0.18491065502166748
iteration 54, loss = 0.09625815600156784
iteration 55, loss = 0.28987249732017517
iteration 56, loss = 0.1894848793745041
iteration 57, loss = 0.4278489351272583
iteration 58, loss = 0.44225648045539856
iteration 59, loss = 0.016103500500321388
iteration 60, loss = 0.006951183546334505
iteration 61, loss = 0.20952139794826508
iteration 62, loss = 0.171858012676239
iteration 63, loss = 0.10344675183296204
iteration 64, loss = 0.10361362248659134
iteration 65, loss = 0.06790463626384735
iteration 66, loss = 0.05388317257165909
iteration 67, loss = 0.003761115251109004
iteration 68, loss = 0.009922221302986145
iteration 69, loss = 0.13009431958198547
iteration 70, loss = 0.08314372599124908
iteration 71, loss = 0.28296101093292236
iteration 72, loss = 0.11399389803409576
iteration 73, loss = 0.14325957000255585
iteration 74, loss = 0.13672885298728943
iteration 75, loss = 0.020760299637913704
iteration 76, loss = 0.1696258783340454
iteration 77, loss = 0.0829741358757019
iteration 78, loss = 0.030487237498164177
iteration 79, loss = 0.024561360478401184
iteration 80, loss = 0.17062348127365112
iteration 81, loss = 0.4243093430995941
iteration 82, loss = 0.23709683120250702
iteration 83, loss = 0.20180124044418335
iteration 84, loss = 0.2647387683391571
iteration 85, loss = 0.07291257381439209
iteration 86, loss = 0.04206087440252304
iteration 87, loss = 0.196321040391922
iteration 88, loss = 0.08899989724159241
iteration 89, loss = 0.20566652715206146
iteration 90, loss = 0.10834632813930511
iteration 91, loss = 0.2646760940551758
iteration 92, loss = 0.17704340815544128
iteration 93, loss = 0.18373167514801025
iteration 94, loss = 0.039559051394462585
iteration 95, loss = 0.10985684394836426
iteration 96, loss = 0.15541476011276245
iteration 97, loss = 0.12303701788187027
iteration 98, loss = 0.20933325588703156
iteration 99, loss = 0.10995728522539139
iteration 100, loss = 0.12634235620498657
iteration 101, loss = 0.08298756182193756
iteration 102, loss = 0.12707313895225525
iteration 103, loss = 0.04526222497224808
iteration 104, loss = 0.03650280088186264
iteration 105, loss = 0.06365346163511276
iteration 106, loss = 0.20823988318443298
iteration 107, loss = 0.10372969508171082
iteration 108, loss = 0.09284423291683197
iteration 109, loss = 0.17467616498470306
iteration 110, loss = 0.06861080974340439
iteration 111, loss = 0.18469926714897156
iteration 112, loss = 0.07188654690980911
iteration 113, loss = 0.02797100879251957
iteration 114, loss = 0.08758748322725296
iteration 115, loss = 0.1026037186384201
iteration 116, loss = 0.07725632935762405
iteration 117, loss = 0.05706255882978439
iteration 118, loss = 0.03455202281475067
iteration 119, loss = 0.20536689460277557
iteration 120, loss = 0.12173508107662201
iteration 121, loss = 0.2067093849182129
iteration 122, loss = 0.13487660884857178
iteration 123, loss = 0.06217993423342705
iteration 124, loss = 0.03988144174218178
iteration 125, loss = 0.2995106875896454
iteration 126, loss = 0.024047859013080597
iteration 127, loss = 0.18353547155857086
iteration 128, loss = 0.013782291673123837
iteration 129, loss = 0.04137193411588669
iteration 130, loss = 0.31236910820007324
iteration 131, loss = 0.10205715149641037
iteration 132, loss = 0.14652658998966217
iteration 133, loss = 0.10875076055526733
iteration 134, loss = 0.12623953819274902
iteration 135, loss = 0.03111286088824272
iteration 136, loss = 0.06856660544872284
iteration 137, loss = 0.17172877490520477
iteration 138, loss = 0.14629758894443512
iteration 139, loss = 0.13789406418800354
iteration 140, loss = 0.10088791698217392
iteration 141, loss = 0.06594983488321304
iteration 142, loss = 0.2600209712982178
iteration 143, loss = 0.1164027526974678
iteration 144, loss = 0.02230539731681347
iteration 145, loss = 0.07296038419008255
iteration 146, loss = 0.2833743691444397
iteration 147, loss = 0.16949361562728882
iteration 148, loss = 0.16804969310760498
iteration 149, loss = 0.08621811866760254
iteration 150, loss = 0.20385202765464783
iteration 151, loss = 0.22139544785022736
iteration 152, loss = 0.05432035028934479
iteration 153, loss = 0.1265535056591034
iteration 154, loss = 0.012005610391497612
iteration 155, loss = 0.28655943274497986
iteration 156, loss = 0.13758286833763123
iteration 157, loss = 0.1848680078983307
iteration 158, loss = 0.09995162487030029
iteration 159, loss = 0.12613070011138916
iteration 160, loss = 0.20533160865306854
iteration 161, loss = 0.08039869368076324
iteration 162, loss = 0.0975383073091507
iteration 163, loss = 0.1111711636185646
iteration 164, loss = 0.11399009823799133
iteration 165, loss = 0.11445233225822449
iteration 166, loss = 0.07225394248962402
iteration 167, loss = 0.08388708531856537
iteration 168, loss = 0.05514760687947273
iteration 169, loss = 0.03433123603463173
iteration 170, loss = 0.009105213917791843
iteration 171, loss = 0.03852120414376259
iteration 172, loss = 0.10112939774990082
iteration 173, loss = 0.1044284850358963
iteration 174, loss = 0.023990437388420105
iteration 175, loss = 0.07228153198957443
iteration 176, loss = 0.19487810134887695
iteration 177, loss = 0.05766494199633598
iteration 178, loss = 0.08832874894142151
iteration 179, loss = 0.1552554816007614
iteration 180, loss = 0.04819463565945625
iteration 181, loss = 0.23115789890289307
iteration 182, loss = 0.03995262831449509
iteration 183, loss = 0.02912534773349762
iteration 184, loss = 0.02731899544596672
iteration 185, loss = 0.07093493640422821
iteration 186, loss = 0.049647826701402664
iteration 187, loss = 0.04341862350702286
iteration 188, loss = 0.25960221886634827
iteration 189, loss = 0.16367153823375702
iteration 190, loss = 0.13016508519649506
iteration 191, loss = 0.10420648008584976
iteration 192, loss = 0.0795675665140152
iteration 193, loss = 0.15690839290618896
iteration 194, loss = 0.13279253244400024
iteration 195, loss = 0.12586639821529388
iteration 196, loss = 0.04654190316796303
iteration 197, loss = 0.023579373955726624
iteration 198, loss = 0.10636700689792633
iteration 199, loss = 0.21401624381542206
iteration 200, loss = 0.09299328178167343
iteration 201, loss = 0.06520719081163406
iteration 202, loss = 0.18786534667015076
iteration 203, loss = 0.15696175396442413
iteration 204, loss = 0.1639876812696457
iteration 205, loss = 0.2822064161300659
iteration 206, loss = 0.07059267163276672
iteration 207, loss = 0.018913157284259796
iteration 208, loss = 0.2201298624277115
iteration 209, loss = 0.43038010597229004
iteration 210, loss = 0.004134092479944229
iteration 211, loss = 0.04748241603374481
iteration 212, loss = 0.1482136994600296
iteration 213, loss = 0.05945678800344467
iteration 214, loss = 0.07022567838430405
iteration 215, loss = 0.2335980236530304
iteration 216, loss = 0.16309207677841187
iteration 217, loss = 0.026285944506525993
iteration 218, loss = 0.037333499640226364
iteration 219, loss = 0.06250269711017609
iteration 220, loss = 0.04687543958425522
iteration 221, loss = 0.22296176850795746
iteration 222, loss = 0.08923859894275665
iteration 223, loss = 0.024824241176247597
iteration 224, loss = 0.21255411207675934
iteration 225, loss = 0.15556152164936066
iteration 226, loss = 0.10491396486759186
iteration 227, loss = 0.26524055004119873
iteration 228, loss = 0.029077840968966484
iteration 229, loss = 0.36946001648902893
iteration 230, loss = 0.13056188821792603
iteration 231, loss = 0.09457418322563171
iteration 232, loss = 0.18540580570697784
iteration 233, loss = 0.1450323760509491
iteration 234, loss = 0.07499579340219498
iteration 235, loss = 0.1879892349243164
iteration 236, loss = 0.10186019539833069
iteration 237, loss = 0.021530933678150177
iteration 238, loss = 0.15404358506202698
iteration 239, loss = 0.08645199984312057
iteration 240, loss = 0.006149203982204199
iteration 241, loss = 0.14248774945735931
iteration 242, loss = 0.1005893424153328
iteration 243, loss = 0.08185013383626938
iteration 244, loss = 0.15158754587173462
iteration 245, loss = 0.11448842287063599
iteration 246, loss = 0.07006611675024033
iteration 247, loss = 0.08727891743183136
iteration 248, loss = 0.17720021307468414
iteration 249, loss = 0.3460562229156494
iteration 250, loss = 0.09951019287109375
iteration 251, loss = 0.08167286217212677
iteration 252, loss = 0.09261198341846466
iteration 253, loss = 0.2997358739376068
iteration 254, loss = 0.43055260181427
iteration 255, loss = 0.20606842637062073
iteration 256, loss = 0.11046108603477478
iteration 257, loss = 0.1731368452310562
iteration 258, loss = 0.07046017050743103
iteration 259, loss = 0.11173101514577866
iteration 260, loss = 0.1582455039024353
iteration 261, loss = 0.12095272541046143
iteration 262, loss = 0.13014373183250427
iteration 263, loss = 0.0739821344614029
iteration 264, loss = 0.04869008809328079
iteration 265, loss = 0.0368485301733017
iteration 266, loss = 0.13808521628379822
iteration 267, loss = 0.21011272072792053
iteration 268, loss = 0.13120009005069733
iteration 269, loss = 0.3130379021167755
iteration 270, loss = 0.04062644764780998
iteration 271, loss = 0.21360407769680023
iteration 272, loss = 0.1647719144821167
iteration 273, loss = 0.14673052728176117
iteration 274, loss = 0.23114660382270813
iteration 275, loss = 0.06227841228246689
iteration 276, loss = 0.1530955731868744
iteration 277, loss = 0.011746554635465145
iteration 278, loss = 0.3225533664226532
iteration 279, loss = 0.10381583869457245
iteration 280, loss = 0.17478185892105103
iteration 281, loss = 0.047944385558366776
iteration 282, loss = 0.06614096462726593
iteration 283, loss = 0.17518778145313263
iteration 284, loss = 0.05536074936389923
iteration 285, loss = 0.22428832948207855
iteration 286, loss = 0.06859893351793289
iteration 287, loss = 0.10297924280166626
iteration 288, loss = 0.07072553038597107
iteration 289, loss = 0.2867209315299988
iteration 290, loss = 0.1613132506608963
iteration 291, loss = 0.15842030942440033
iteration 292, loss = 0.14223900437355042
iteration 293, loss = 0.11406630277633667
iteration 294, loss = 0.10026691854000092
iteration 295, loss = 0.1624702662229538
iteration 296, loss = 0.03440799191594124
iteration 297, loss = 0.08165565133094788
iteration 298, loss = 0.014535018242895603
iteration 299, loss = 0.004550125449895859
iteration 0, loss = 0.002610870636999607
iteration 1, loss = 0.21726925671100616
iteration 2, loss = 0.31466442346572876
iteration 3, loss = 0.017338750883936882
iteration 4, loss = 0.10459691286087036
iteration 5, loss = 0.19032248854637146
iteration 6, loss = 0.02165825292468071
iteration 7, loss = 0.15540337562561035
iteration 8, loss = 0.2106369286775589
iteration 9, loss = 0.29899877309799194
iteration 10, loss = 0.426033616065979
iteration 11, loss = 0.00833839736878872
iteration 12, loss = 0.2832973897457123
iteration 13, loss = 0.22132498025894165
iteration 14, loss = 0.29437825083732605
iteration 15, loss = 0.14036399126052856
iteration 16, loss = 0.09349504113197327
iteration 17, loss = 0.17465929687023163
iteration 18, loss = 0.3070452809333801
iteration 19, loss = 0.2629079222679138
iteration 20, loss = 0.14685297012329102
iteration 21, loss = 0.16351690888404846
iteration 22, loss = 0.2137746810913086
iteration 23, loss = 0.14995822310447693
iteration 24, loss = 0.07483607530593872
iteration 25, loss = 0.09456020593643188
iteration 26, loss = 0.17125427722930908
iteration 27, loss = 0.09935610741376877
iteration 28, loss = 0.04124561324715614
iteration 29, loss = 0.17637506127357483
iteration 30, loss = 0.07645656913518906
iteration 31, loss = 0.0998026579618454
iteration 32, loss = 0.025899594649672508
iteration 33, loss = 0.14202119410037994
iteration 34, loss = 0.09964536875486374
iteration 35, loss = 0.05887556076049805
iteration 36, loss = 0.06221925839781761
iteration 37, loss = 0.10629980266094208
iteration 38, loss = 0.10651134699583054
iteration 39, loss = 0.09082580357789993
iteration 40, loss = 0.19986975193023682
iteration 41, loss = 0.12131985276937485
iteration 42, loss = 0.0374792143702507
iteration 43, loss = 0.19697806239128113
iteration 44, loss = 0.046454306691884995
iteration 45, loss = 0.3168184459209442
iteration 46, loss = 0.0696132630109787
iteration 47, loss = 0.26410576701164246
iteration 48, loss = 0.09389694780111313
iteration 49, loss = 0.16582302749156952
iteration 50, loss = 0.18498796224594116
iteration 51, loss = 0.11514484882354736
iteration 52, loss = 0.10697932541370392
iteration 53, loss = 0.11696401238441467
iteration 54, loss = 0.023670798167586327
iteration 55, loss = 0.2103816270828247
iteration 56, loss = 0.0071143098175525665
iteration 57, loss = 0.07087457180023193
iteration 58, loss = 0.04559323936700821
iteration 59, loss = 0.030866403132677078
iteration 60, loss = 0.135066419839859
iteration 61, loss = 0.10789332538843155
iteration 62, loss = 0.11165972054004669
iteration 63, loss = 0.20624954998493195
iteration 64, loss = 0.01643448881804943
iteration 65, loss = 0.11201011389493942
iteration 66, loss = 0.1785021722316742
iteration 67, loss = 0.10031140595674515
iteration 68, loss = 0.25936219096183777
iteration 69, loss = 0.06867841631174088
iteration 70, loss = 0.18343128263950348
iteration 71, loss = 0.4517975449562073
iteration 72, loss = 0.0684969574213028
iteration 73, loss = 0.2563505470752716
iteration 74, loss = 0.1148545891046524
iteration 75, loss = 0.07368451356887817
iteration 76, loss = 0.16584216058254242
iteration 77, loss = 0.11362723261117935
iteration 78, loss = 0.17311231791973114
iteration 79, loss = 0.04420257732272148
iteration 80, loss = 0.12038806080818176
iteration 81, loss = 0.11503718048334122
iteration 82, loss = 0.5108199715614319
iteration 83, loss = 0.03327462077140808
iteration 84, loss = 0.046072185039520264
iteration 85, loss = 0.06466087698936462
iteration 86, loss = 0.07632917910814285
iteration 87, loss = 0.14263765513896942
iteration 88, loss = 0.2708192765712738
iteration 89, loss = 0.16562095284461975
iteration 90, loss = 0.13835409283638
iteration 91, loss = 0.07094375789165497
iteration 92, loss = 0.18462064862251282
iteration 93, loss = 0.07867966592311859
iteration 94, loss = 0.05376937612891197
iteration 95, loss = 0.07956656813621521
iteration 96, loss = 0.02573949284851551
iteration 97, loss = 0.1720084249973297
iteration 98, loss = 0.1406998634338379
iteration 99, loss = 0.10644707083702087
iteration 100, loss = 0.19071216881275177
iteration 101, loss = 0.02584002912044525
iteration 102, loss = 0.048802535980939865
iteration 103, loss = 0.09970951080322266
iteration 104, loss = 0.0937345027923584
iteration 105, loss = 0.07306186854839325
iteration 106, loss = 0.14682595431804657
iteration 107, loss = 0.046391431242227554
iteration 108, loss = 0.1004256159067154
iteration 109, loss = 0.10913775861263275
iteration 110, loss = 0.11300282925367355
iteration 111, loss = 0.11386676132678986
iteration 112, loss = 0.08632345497608185
iteration 113, loss = 0.03608855605125427
iteration 114, loss = 0.3465442359447479
iteration 115, loss = 0.21173210442066193
iteration 116, loss = 0.16907227039337158
iteration 117, loss = 0.09075670689344406
iteration 118, loss = 0.1065579429268837
iteration 119, loss = 0.2043992578983307
iteration 120, loss = 0.0543491393327713
iteration 121, loss = 0.1519394814968109
iteration 122, loss = 0.019757898524403572
iteration 123, loss = 0.04624629020690918
iteration 124, loss = 0.03763185441493988
iteration 125, loss = 0.2840205132961273
iteration 126, loss = 0.10231141746044159
iteration 127, loss = 0.03942790627479553
iteration 128, loss = 0.1798854023218155
iteration 129, loss = 0.03487077355384827
iteration 130, loss = 0.04186730831861496
iteration 131, loss = 0.10471007227897644
iteration 132, loss = 0.14476704597473145
iteration 133, loss = 0.062489427626132965
iteration 134, loss = 0.09114545583724976
iteration 135, loss = 0.06578654795885086
iteration 136, loss = 0.09822652488946915
iteration 137, loss = 0.014222998172044754
iteration 138, loss = 0.1969521939754486
iteration 139, loss = 0.11090393364429474
iteration 140, loss = 0.012490479275584221
iteration 141, loss = 0.09733176231384277
iteration 142, loss = 0.12458817660808563
iteration 143, loss = 0.05000876262784004
iteration 144, loss = 0.17606490850448608
iteration 145, loss = 0.09686988592147827
iteration 146, loss = 0.05498012900352478
iteration 147, loss = 0.16228628158569336
iteration 148, loss = 0.0935874879360199
iteration 149, loss = 0.08173324167728424
iteration 150, loss = 0.07109443098306656
iteration 151, loss = 0.10915935039520264
iteration 152, loss = 0.3620052933692932
iteration 153, loss = 0.023317107930779457
iteration 154, loss = 0.009570694528520107
iteration 155, loss = 0.09102393686771393
iteration 156, loss = 0.08341971784830093
iteration 157, loss = 0.09451329708099365
iteration 158, loss = 0.0891156792640686
iteration 159, loss = 0.09873652458190918
iteration 160, loss = 0.16822955012321472
iteration 161, loss = 0.10217330604791641
iteration 162, loss = 0.14118681848049164
iteration 163, loss = 0.147007018327713
iteration 164, loss = 0.05562134087085724
iteration 165, loss = 0.012212981469929218
iteration 166, loss = 0.042940422892570496
iteration 167, loss = 0.012836172245442867
iteration 168, loss = 0.19383373856544495
iteration 169, loss = 0.1689533293247223
iteration 170, loss = 0.2535540759563446
iteration 171, loss = 0.28925621509552
iteration 172, loss = 0.21476911008358002
iteration 173, loss = 0.2003774493932724
iteration 174, loss = 0.13857293128967285
iteration 175, loss = 0.10505044460296631
iteration 176, loss = 0.1124337911605835
iteration 177, loss = 0.005078795365989208
iteration 178, loss = 0.010377858765423298
iteration 179, loss = 0.32676488161087036
iteration 180, loss = 0.15020646154880524
iteration 181, loss = 0.1601072996854782
iteration 182, loss = 0.04694259911775589
iteration 183, loss = 0.025913212448358536
iteration 184, loss = 0.16222690045833588
iteration 185, loss = 0.04587573930621147
iteration 186, loss = 0.11991953104734421
iteration 187, loss = 0.04005483165383339
iteration 188, loss = 0.031195444986224174
iteration 189, loss = 0.4153636395931244
iteration 190, loss = 0.0303351953625679
iteration 191, loss = 0.013984967023134232
iteration 192, loss = 0.06846494227647781
iteration 193, loss = 0.34402668476104736
iteration 194, loss = 0.21275842189788818
iteration 195, loss = 0.10159331560134888
iteration 196, loss = 0.10122554749250412
iteration 197, loss = 0.1690329909324646
iteration 198, loss = 0.1339777708053589
iteration 199, loss = 0.0322476364672184
iteration 200, loss = 0.2318277359008789
iteration 201, loss = 0.21019956469535828
iteration 202, loss = 0.12625673413276672
iteration 203, loss = 0.37802180647850037
iteration 204, loss = 0.22442388534545898
iteration 205, loss = 0.07117826491594315
iteration 206, loss = 0.14835940301418304
iteration 207, loss = 0.24918517470359802
iteration 208, loss = 0.14010456204414368
iteration 209, loss = 0.11165925860404968
iteration 210, loss = 0.12482265383005142
iteration 211, loss = 0.04352482035756111
iteration 212, loss = 0.18829026818275452
iteration 213, loss = 0.1519489586353302
iteration 214, loss = 0.15463198721408844
iteration 215, loss = 0.34298795461654663
iteration 216, loss = 0.1557585448026657
iteration 217, loss = 0.08579964190721512
iteration 218, loss = 0.18923653662204742
iteration 219, loss = 0.19363373517990112
iteration 220, loss = 0.021634142845869064
iteration 221, loss = 0.0312705859541893
iteration 222, loss = 0.208729088306427
iteration 223, loss = 0.2538580298423767
iteration 224, loss = 0.06819266080856323
iteration 225, loss = 0.15574456751346588
iteration 226, loss = 0.11003394424915314
iteration 227, loss = 0.09955236315727234
iteration 228, loss = 0.2399412989616394
iteration 229, loss = 0.05735692381858826
iteration 230, loss = 0.06529924273490906
iteration 231, loss = 0.16752572357654572
iteration 232, loss = 0.32997381687164307
iteration 233, loss = 0.017968211323022842
iteration 234, loss = 0.08181014657020569
iteration 235, loss = 0.2403283417224884
iteration 236, loss = 0.0830421894788742
iteration 237, loss = 0.1177653968334198
iteration 238, loss = 0.13321134448051453
iteration 239, loss = 0.5405582785606384
iteration 240, loss = 0.10285019129514694
iteration 241, loss = 0.13907092809677124
iteration 242, loss = 0.019101789221167564
iteration 243, loss = 0.4440990686416626
iteration 244, loss = 0.2147684097290039
iteration 245, loss = 0.300033837556839
iteration 246, loss = 0.28947964310646057
iteration 247, loss = 0.14991223812103271
iteration 248, loss = 0.5004165172576904
iteration 249, loss = 0.0600445494055748
iteration 250, loss = 0.14856752753257751
iteration 251, loss = 0.15167084336280823
iteration 252, loss = 0.1191888079047203
iteration 253, loss = 0.3408108055591583
iteration 254, loss = 0.4994286596775055
iteration 255, loss = 0.039533887058496475
iteration 256, loss = 0.17476795613765717
iteration 257, loss = 0.13927367329597473
iteration 258, loss = 0.16734790802001953
iteration 259, loss = 0.3432266414165497
iteration 260, loss = 0.2862471342086792
iteration 261, loss = 0.4979718327522278
iteration 262, loss = 0.003754751058295369
iteration 263, loss = 0.23524951934814453
iteration 264, loss = 0.10495313256978989
iteration 265, loss = 0.038907378911972046
iteration 266, loss = 0.13025020062923431
iteration 267, loss = 0.05962340161204338
iteration 268, loss = 0.11926459521055222
iteration 269, loss = 0.12070418894290924
iteration 270, loss = 0.2053379863500595
iteration 271, loss = 0.0983252227306366
iteration 272, loss = 0.006247903220355511
iteration 273, loss = 0.011030066758394241
iteration 274, loss = 0.31776249408721924
iteration 275, loss = 0.15482430160045624
iteration 276, loss = 0.06732682138681412
iteration 277, loss = 0.1360672563314438
iteration 278, loss = 0.1789833903312683
iteration 279, loss = 0.35249707102775574
iteration 280, loss = 0.23527131974697113
iteration 281, loss = 0.07768063247203827
iteration 282, loss = 0.07200528681278229
iteration 283, loss = 0.05685344710946083
iteration 284, loss = 0.15031208097934723
iteration 285, loss = 0.08020877838134766
iteration 286, loss = 0.006902256049215794
iteration 287, loss = 0.0293385349214077
iteration 288, loss = 0.3110724687576294
iteration 289, loss = 0.07898975163698196
iteration 290, loss = 0.18221566081047058
iteration 291, loss = 0.08763884752988815
iteration 292, loss = 0.018886469304561615
iteration 293, loss = 0.266995370388031
iteration 294, loss = 0.36781081557273865
iteration 295, loss = 0.12535518407821655
iteration 296, loss = 0.17463696002960205
iteration 297, loss = 0.20547957718372345
iteration 298, loss = 0.470515638589859
iteration 299, loss = 0.3652975261211395
iteration 0, loss = 0.18015797436237335
iteration 1, loss = 0.1874261200428009
iteration 2, loss = 0.12740115821361542
iteration 3, loss = 0.1462627500295639
iteration 4, loss = 0.10285302996635437
iteration 5, loss = 0.19870881736278534
iteration 6, loss = 0.1953873187303543
iteration 7, loss = 0.1614474356174469
iteration 8, loss = 0.09348465502262115
iteration 9, loss = 0.1522226184606552
iteration 10, loss = 0.10854809731245041
iteration 11, loss = 0.054125770926475525
iteration 12, loss = 0.1833159625530243
iteration 13, loss = 0.05387686565518379
iteration 14, loss = 0.1426033228635788
iteration 15, loss = 0.11821822822093964
iteration 16, loss = 0.14412765204906464
iteration 17, loss = 0.09768141061067581
iteration 18, loss = 0.12165980786085129
iteration 19, loss = 0.03458302840590477
iteration 20, loss = 0.19147798418998718
iteration 21, loss = 0.0895073413848877
iteration 22, loss = 0.2065318375825882
iteration 23, loss = 0.13261747360229492
iteration 24, loss = 0.14729584753513336
iteration 25, loss = 0.2294810712337494
iteration 26, loss = 0.1358029842376709
iteration 27, loss = 0.201851487159729
iteration 28, loss = 0.12845081090927124
iteration 29, loss = 0.0730164498090744
iteration 30, loss = 0.4027518928050995
iteration 31, loss = 0.2421817183494568
iteration 32, loss = 0.3419340252876282
iteration 33, loss = 0.08630811423063278
iteration 34, loss = 0.08958446234464645
iteration 35, loss = 0.21644063293933868
iteration 36, loss = 0.3551737368106842
iteration 37, loss = 0.2401626855134964
iteration 38, loss = 0.1958756297826767
iteration 39, loss = 0.2043929100036621
iteration 40, loss = 0.028699221089482307
iteration 41, loss = 0.007586600258946419
iteration 42, loss = 0.708499014377594
iteration 43, loss = 0.0003869784122798592
iteration 44, loss = 0.6067901849746704
iteration 45, loss = 0.6566548943519592
iteration 46, loss = 0.23480427265167236
iteration 47, loss = 0.2340746372938156
iteration 48, loss = 0.3226233720779419
iteration 49, loss = 0.3700910210609436
iteration 50, loss = 0.10349421203136444
iteration 51, loss = 0.11014263331890106
iteration 52, loss = 0.27441123127937317
iteration 53, loss = 0.4006636440753937
iteration 54, loss = 0.1721755415201187
iteration 55, loss = 0.14332813024520874
iteration 56, loss = 0.1385236531496048
iteration 57, loss = 0.08403581380844116
iteration 58, loss = 0.16831324994564056
iteration 59, loss = 0.004328719340264797
iteration 60, loss = 0.01936239004135132
iteration 61, loss = 0.15015661716461182
iteration 62, loss = 0.035732172429561615
iteration 63, loss = 0.19725973904132843
iteration 64, loss = 0.12496520578861237
iteration 65, loss = 0.016808953136205673
iteration 66, loss = 0.02546181157231331
iteration 67, loss = 0.021174775436520576
iteration 68, loss = 0.15969716012477875
iteration 69, loss = 0.0289556123316288
iteration 70, loss = 0.34536463022232056
iteration 71, loss = 0.03613358363509178
iteration 72, loss = 0.10998440533876419
iteration 73, loss = 0.12008768320083618
iteration 74, loss = 0.050162214785814285
iteration 75, loss = 0.21669833362102509
iteration 76, loss = 0.04768518730998039
iteration 77, loss = 0.0984220877289772
iteration 78, loss = 0.2962801456451416
iteration 79, loss = 0.019183997064828873
iteration 80, loss = 0.12311546504497528
iteration 81, loss = 0.1650693714618683
iteration 82, loss = 0.04148876294493675
iteration 83, loss = 0.06633355468511581
iteration 84, loss = 0.14664208889007568
iteration 85, loss = 0.2014302909374237
iteration 86, loss = 0.08358906209468842
iteration 87, loss = 0.08169576525688171
iteration 88, loss = 0.14637784659862518
iteration 89, loss = 0.1659083217382431
iteration 90, loss = 0.08374740183353424
iteration 91, loss = 0.0768933892250061
iteration 92, loss = 0.2321116179227829
iteration 93, loss = 0.1312003880739212
iteration 94, loss = 0.11773897707462311
iteration 95, loss = 0.340201735496521
iteration 96, loss = 0.07111185044050217
iteration 97, loss = 0.0708668977022171
iteration 98, loss = 0.053054917603731155
iteration 99, loss = 0.0728825256228447
iteration 100, loss = 0.14528346061706543
iteration 101, loss = 0.16280773282051086
iteration 102, loss = 0.12970022857189178
iteration 103, loss = 0.25438448786735535
iteration 104, loss = 0.004059994593262672
iteration 105, loss = 0.0052274977788329124
iteration 106, loss = 0.09861620515584946
iteration 107, loss = 0.009217804297804832
iteration 108, loss = 0.18221639096736908
iteration 109, loss = 0.22382880747318268
iteration 110, loss = 0.23306454718112946
iteration 111, loss = 0.055942829698324203
iteration 112, loss = 0.061335355043411255
iteration 113, loss = 0.10161464661359787
iteration 114, loss = 0.32239246368408203
iteration 115, loss = 0.25122755765914917
iteration 116, loss = 0.19272153079509735
iteration 117, loss = 0.1137280985713005
iteration 118, loss = 0.09886887669563293
iteration 119, loss = 0.14476139843463898
iteration 120, loss = 0.08639682084321976
iteration 121, loss = 0.006985386833548546
iteration 122, loss = 0.11850375682115555
iteration 123, loss = 0.12745268642902374
iteration 124, loss = 0.10055950284004211
iteration 125, loss = 0.1552855223417282
iteration 126, loss = 0.25901859998703003
iteration 127, loss = 0.07567677646875381
iteration 128, loss = 0.2534014880657196
iteration 129, loss = 0.0029300786554813385
iteration 130, loss = 0.347806841135025
iteration 131, loss = 0.5796319246292114
iteration 132, loss = 0.151786208152771
iteration 133, loss = 0.3612658381462097
iteration 134, loss = 0.14319726824760437
iteration 135, loss = 0.11184883117675781
iteration 136, loss = 0.36640501022338867
iteration 137, loss = 0.3500621020793915
iteration 138, loss = 0.1359749138355255
iteration 139, loss = 0.22503000497817993
iteration 140, loss = 0.08718378841876984
iteration 141, loss = 0.15587584674358368
iteration 142, loss = 0.3614453971385956
iteration 143, loss = 0.00456062238663435
iteration 144, loss = 0.014668058604001999
iteration 145, loss = 0.008899439126253128
iteration 146, loss = 0.2975884974002838
iteration 147, loss = 0.22860413789749146
iteration 148, loss = 0.10537607967853546
iteration 149, loss = 0.2980142831802368
iteration 150, loss = 0.22547385096549988
iteration 151, loss = 0.2186654508113861
iteration 152, loss = 0.025748983025550842
iteration 153, loss = 0.08960297703742981
iteration 154, loss = 0.1441144049167633
iteration 155, loss = 0.22541826963424683
iteration 156, loss = 0.03272775560617447
iteration 157, loss = 0.00377669557929039
iteration 158, loss = 0.2927887737751007
iteration 159, loss = 0.13981255888938904
iteration 160, loss = 0.07835724949836731
iteration 161, loss = 0.032410863786935806
iteration 162, loss = 0.16069525480270386
iteration 163, loss = 0.1296117901802063
iteration 164, loss = 0.12137466669082642
iteration 165, loss = 0.17102490365505219
iteration 166, loss = 0.06251220405101776
iteration 167, loss = 0.14500601589679718
iteration 168, loss = 0.30183446407318115
iteration 169, loss = 0.02268214151263237
iteration 170, loss = 0.023524068295955658
iteration 171, loss = 0.13606666028499603
iteration 172, loss = 0.23337212204933167
iteration 173, loss = 0.08742528408765793
iteration 174, loss = 0.28220710158348083
iteration 175, loss = 0.09086316078901291
iteration 176, loss = 0.05855466052889824
iteration 177, loss = 0.1578526794910431
iteration 178, loss = 0.10212547332048416
iteration 179, loss = 0.06231154873967171
iteration 180, loss = 0.07746413350105286
iteration 181, loss = 0.161634624004364
iteration 182, loss = 0.08218488097190857
iteration 183, loss = 0.003107246244326234
iteration 184, loss = 0.3391362130641937
iteration 185, loss = 0.018295077607035637
iteration 186, loss = 0.2947213053703308
iteration 187, loss = 0.0021615789737552404
iteration 188, loss = 0.008145315572619438
iteration 189, loss = 0.006214722990989685
iteration 190, loss = 0.25514286756515503
iteration 191, loss = 0.027593374252319336
iteration 192, loss = 0.051170408725738525
iteration 193, loss = 0.08672770857810974
iteration 194, loss = 0.29248642921447754
iteration 195, loss = 0.03593306243419647
iteration 196, loss = 0.11563976109027863
iteration 197, loss = 0.16760936379432678
iteration 198, loss = 0.23453426361083984
iteration 199, loss = 0.13248471915721893
iteration 200, loss = 0.03493038937449455
iteration 201, loss = 0.04202805832028389
iteration 202, loss = 0.036561835557222366
iteration 203, loss = 0.09740476310253143
iteration 204, loss = 0.08336968719959259
iteration 205, loss = 0.09429343789815903
iteration 206, loss = 0.14374159276485443
iteration 207, loss = 0.20768994092941284
iteration 208, loss = 0.15905633568763733
iteration 209, loss = 0.03168781101703644
iteration 210, loss = 0.06267271935939789
iteration 211, loss = 0.006442388519644737
iteration 212, loss = 0.10826972872018814
iteration 213, loss = 0.13169337809085846
iteration 214, loss = 0.14610505104064941
iteration 215, loss = 0.25158506631851196
iteration 216, loss = 0.03069624863564968
iteration 217, loss = 0.19207780063152313
iteration 218, loss = 0.15180249512195587
iteration 219, loss = 0.29833516478538513
iteration 220, loss = 0.08011789619922638
iteration 221, loss = 0.0013644419377669692
iteration 222, loss = 0.3072488605976105
iteration 223, loss = 0.2263612449169159
iteration 224, loss = 0.1469671130180359
iteration 225, loss = 0.142830953001976
iteration 226, loss = 0.051019858568906784
iteration 227, loss = 0.049274738878011703
iteration 228, loss = 0.11621219664812088
iteration 229, loss = 0.27340731024742126
iteration 230, loss = 0.03992275893688202
iteration 231, loss = 0.043711185455322266
iteration 232, loss = 0.08018326759338379
iteration 233, loss = 0.0019950743298977613
iteration 234, loss = 0.22966019809246063
iteration 235, loss = 0.020460231229662895
iteration 236, loss = 0.29178306460380554
iteration 237, loss = 0.10464022308588028
iteration 238, loss = 0.04313771799206734
iteration 239, loss = 0.2988066077232361
iteration 240, loss = 0.015284628607332706
iteration 241, loss = 0.18463134765625
iteration 242, loss = 0.26319730281829834
iteration 243, loss = 0.12200826406478882
iteration 244, loss = 0.21021729707717896
iteration 245, loss = 0.1014300137758255
iteration 246, loss = 0.035505346953868866
iteration 247, loss = 0.2034159153699875
iteration 248, loss = 0.5812347531318665
iteration 249, loss = 0.5030688047409058
iteration 250, loss = 0.001080091344192624
iteration 251, loss = 0.000992218847386539
iteration 252, loss = 0.2893950939178467
iteration 253, loss = 0.15286648273468018
iteration 254, loss = 0.14769889414310455
iteration 255, loss = 0.027045421302318573
iteration 256, loss = 0.04621974006295204
iteration 257, loss = 0.2530747354030609
iteration 258, loss = 0.30036312341690063
iteration 259, loss = 0.11726164817810059
iteration 260, loss = 0.06202203035354614
iteration 261, loss = 0.16084319353103638
iteration 262, loss = 0.15348266065120697
iteration 263, loss = 0.3503539562225342
iteration 264, loss = 0.05838135629892349
iteration 265, loss = 0.07488638162612915
iteration 266, loss = 0.23910599946975708
iteration 267, loss = 0.2705053985118866
iteration 268, loss = 0.06588464975357056
iteration 269, loss = 0.02633764035999775
iteration 270, loss = 0.07751498371362686
iteration 271, loss = 0.06952403485774994
iteration 272, loss = 0.06511369347572327
iteration 273, loss = 0.1781022548675537
iteration 274, loss = 0.05491609871387482
iteration 275, loss = 0.029365520924329758
iteration 276, loss = 0.0625949576497078
iteration 277, loss = 0.05987800657749176
iteration 278, loss = 0.08525454998016357
iteration 279, loss = 0.24847789108753204
iteration 280, loss = 0.206125408411026
iteration 281, loss = 0.10083754360675812
iteration 282, loss = 0.14575111865997314
iteration 283, loss = 0.11075332015752792
iteration 284, loss = 0.11854800581932068
iteration 285, loss = 0.09098899364471436
iteration 286, loss = 0.09011109918355942
iteration 287, loss = 0.1554332822561264
iteration 288, loss = 0.01427257340401411
iteration 289, loss = 0.07166402786970139
iteration 290, loss = 0.22491149604320526
iteration 291, loss = 0.07630372047424316
iteration 292, loss = 0.010210278443992138
iteration 293, loss = 0.06458627432584763
iteration 294, loss = 0.2086028754711151
iteration 295, loss = 0.19922851026058197
iteration 296, loss = 0.10899101942777634
iteration 297, loss = 0.10870605707168579
iteration 298, loss = 0.11298863589763641
iteration 299, loss = 0.13287627696990967
iteration 0, loss = 0.16136758029460907
iteration 1, loss = 0.1726779043674469
iteration 2, loss = 0.6927132606506348
iteration 3, loss = 0.0526132807135582
iteration 4, loss = 0.2811860740184784
iteration 5, loss = 0.18716895580291748
iteration 6, loss = 0.27201783657073975
iteration 7, loss = 0.14430619776248932
iteration 8, loss = 0.1786288619041443
iteration 9, loss = 0.1799200475215912
iteration 10, loss = 0.14067022502422333
iteration 11, loss = 0.2003561556339264
iteration 12, loss = 0.04465103894472122
iteration 13, loss = 0.1120840385556221
iteration 14, loss = 0.06696052104234695
iteration 15, loss = 0.17530164122581482
iteration 16, loss = 0.21407239139080048
iteration 17, loss = 0.08404184132814407
iteration 18, loss = 0.13853029906749725
iteration 19, loss = 0.05301451310515404
iteration 20, loss = 0.19301995635032654
iteration 21, loss = 0.02205444872379303
iteration 22, loss = 0.049433160573244095
iteration 23, loss = 0.06447476148605347
iteration 24, loss = 0.12185898423194885
iteration 25, loss = 0.052382633090019226
iteration 26, loss = 0.08800484985113144
iteration 27, loss = 0.03272053226828575
iteration 28, loss = 0.2885242700576782
iteration 29, loss = 0.08730801939964294
iteration 30, loss = 0.07033199071884155
iteration 31, loss = 0.1450391709804535
iteration 32, loss = 0.058272939175367355
iteration 33, loss = 0.04825915768742561
iteration 34, loss = 0.08440536260604858
iteration 35, loss = 0.14523695409297943
iteration 36, loss = 0.2973443865776062
iteration 37, loss = 0.24575476348400116
iteration 38, loss = 0.14036941528320312
iteration 39, loss = 0.11474583297967911
iteration 40, loss = 0.011803151108324528
iteration 41, loss = 0.0875488817691803
iteration 42, loss = 0.12610483169555664
iteration 43, loss = 0.13231126964092255
iteration 44, loss = 0.09688284248113632
iteration 45, loss = 0.20765802264213562
iteration 46, loss = 0.08725067228078842
iteration 47, loss = 0.14458027482032776
iteration 48, loss = 0.029766704887151718
iteration 49, loss = 0.2288348227739334
iteration 50, loss = 0.0022362922318279743
iteration 51, loss = 0.21419093012809753
iteration 52, loss = 0.15746581554412842
iteration 53, loss = 0.06254231929779053
iteration 54, loss = 0.12090026587247849
iteration 55, loss = 0.19743695855140686
iteration 56, loss = 0.04059198126196861
iteration 57, loss = 0.05946504324674606
iteration 58, loss = 0.1245085671544075
iteration 59, loss = 0.05810633301734924
iteration 60, loss = 0.12383134663105011
iteration 61, loss = 0.08067633211612701
iteration 62, loss = 0.1384766846895218
iteration 63, loss = 0.06997808814048767
iteration 64, loss = 0.03009766712784767
iteration 65, loss = 0.0990954041481018
iteration 66, loss = 0.08736827969551086
iteration 67, loss = 0.10123555362224579
iteration 68, loss = 0.0527435764670372
iteration 69, loss = 0.1543889194726944
iteration 70, loss = 0.06673170626163483
iteration 71, loss = 0.06348905712366104
iteration 72, loss = 0.08030198514461517
iteration 73, loss = 0.062387481331825256
iteration 74, loss = 0.17213386297225952
iteration 75, loss = 0.14113359153270721
iteration 76, loss = 0.07174871861934662
iteration 77, loss = 0.07027905434370041
iteration 78, loss = 0.05206233635544777
iteration 79, loss = 0.0518590472638607
iteration 80, loss = 0.015950646251440048
iteration 81, loss = 0.04710272327065468
iteration 82, loss = 0.00936977006494999
iteration 83, loss = 0.1083604022860527
iteration 84, loss = 0.2653064429759979
iteration 85, loss = 0.03615163266658783
iteration 86, loss = 0.10052286088466644
iteration 87, loss = 0.1190028116106987
iteration 88, loss = 0.12409324944019318
iteration 89, loss = 0.15094733238220215
iteration 90, loss = 0.10863260179758072
iteration 91, loss = 0.044830888509750366
iteration 92, loss = 0.03244950994849205
iteration 93, loss = 0.0017835700418800116
iteration 94, loss = 0.6425255537033081
iteration 95, loss = 0.158829465508461
iteration 96, loss = 0.06727971136569977
iteration 97, loss = 0.1252441108226776
iteration 98, loss = 0.1424134522676468
iteration 99, loss = 0.3228057026863098
iteration 100, loss = 0.14556877315044403
iteration 101, loss = 0.04275331273674965
iteration 102, loss = 0.40816593170166016
iteration 103, loss = 0.07003194838762283
iteration 104, loss = 0.08788064122200012
iteration 105, loss = 0.2518700957298279
iteration 106, loss = 0.16761565208435059
iteration 107, loss = 0.04212773218750954
iteration 108, loss = 0.2101178765296936
iteration 109, loss = 0.18053698539733887
iteration 110, loss = 0.0880807489156723
iteration 111, loss = 0.18956823647022247
iteration 112, loss = 0.039243850857019424
iteration 113, loss = 0.10346170514822006
iteration 114, loss = 0.20792368054389954
iteration 115, loss = 0.14902237057685852
iteration 116, loss = 0.0116722472012043
iteration 117, loss = 0.008303627371788025
iteration 118, loss = 0.12610046565532684
iteration 119, loss = 0.10212677717208862
iteration 120, loss = 0.1663314700126648
iteration 121, loss = 0.054692987352609634
iteration 122, loss = 0.0360032394528389
iteration 123, loss = 0.49674323201179504
iteration 124, loss = 0.296731561422348
iteration 125, loss = 0.06169608235359192
iteration 126, loss = 0.09000654518604279
iteration 127, loss = 0.06889530271291733
iteration 128, loss = 0.0018120210152119398
iteration 129, loss = 0.42603737115859985
iteration 130, loss = 0.04439515247941017
iteration 131, loss = 0.40047362446784973
iteration 132, loss = 0.40333694219589233
iteration 133, loss = 0.3107568919658661
iteration 134, loss = 0.006570854224264622
iteration 135, loss = 0.0771007388830185
iteration 136, loss = 0.09613069891929626
iteration 137, loss = 0.27365797758102417
iteration 138, loss = 0.2733112573623657
iteration 139, loss = 0.32154741883277893
iteration 140, loss = 0.19600412249565125
iteration 141, loss = 0.215547114610672
iteration 142, loss = 0.28013381361961365
iteration 143, loss = 0.046629805117845535
iteration 144, loss = 0.15022875368595123
iteration 145, loss = 0.11850981414318085
iteration 146, loss = 0.050890546292066574
iteration 147, loss = 0.04042310640215874
iteration 148, loss = 0.061503805220127106
iteration 149, loss = 0.0749039426445961
iteration 150, loss = 0.2602192759513855
iteration 151, loss = 0.23319654166698456
iteration 152, loss = 0.1533232033252716
iteration 153, loss = 0.07791927456855774
iteration 154, loss = 0.22592824697494507
iteration 155, loss = 0.10006051510572433
iteration 156, loss = 0.10473788529634476
iteration 157, loss = 0.11871377378702164
iteration 158, loss = 0.19175854325294495
iteration 159, loss = 0.1465371996164322
iteration 160, loss = 0.06438401341438293
iteration 161, loss = 0.07521557807922363
iteration 162, loss = 0.13197466731071472
iteration 163, loss = 0.10567433387041092
iteration 164, loss = 0.14557135105133057
iteration 165, loss = 0.2073419839143753
iteration 166, loss = 0.042044274508953094
iteration 167, loss = 0.18813692033290863
iteration 168, loss = 0.15738728642463684
iteration 169, loss = 0.07819307595491409
iteration 170, loss = 0.1302165389060974
iteration 171, loss = 0.1382165253162384
iteration 172, loss = 0.24657182395458221
iteration 173, loss = 0.15413716435432434
iteration 174, loss = 0.06166287511587143
iteration 175, loss = 0.0340057834982872
iteration 176, loss = 0.054778050631284714
iteration 177, loss = 0.06200869381427765
iteration 178, loss = 0.07558219134807587
iteration 179, loss = 0.08913068473339081
iteration 180, loss = 0.07141150534152985
iteration 181, loss = 0.22502845525741577
iteration 182, loss = 0.024499155580997467
iteration 183, loss = 0.256515771150589
iteration 184, loss = 0.08349163085222244
iteration 185, loss = 0.26040416955947876
iteration 186, loss = 0.09415078163146973
iteration 187, loss = 0.12034870684146881
iteration 188, loss = 0.07225805521011353
iteration 189, loss = 0.16474029421806335
iteration 190, loss = 0.0587933175265789
iteration 191, loss = 0.005717772524803877
iteration 192, loss = 0.16063621640205383
iteration 193, loss = 0.2654639780521393
iteration 194, loss = 0.2389357089996338
iteration 195, loss = 0.12179297208786011
iteration 196, loss = 0.1131458580493927
iteration 197, loss = 0.14383234083652496
iteration 198, loss = 0.052645549178123474
iteration 199, loss = 0.178121879696846
iteration 200, loss = 0.38251379132270813
iteration 201, loss = 0.03205600380897522
iteration 202, loss = 0.04761908948421478
iteration 203, loss = 0.3448847830295563
iteration 204, loss = 0.4060455560684204
iteration 205, loss = 0.114052414894104
iteration 206, loss = 0.2202325314283371
iteration 207, loss = 0.08217760920524597
iteration 208, loss = 0.15321344137191772
iteration 209, loss = 0.10598214715719223
iteration 210, loss = 0.5165956020355225
iteration 211, loss = 0.19478628039360046
iteration 212, loss = 0.38881638646125793
iteration 213, loss = 0.25772225856781006
iteration 214, loss = 0.17128320038318634
iteration 215, loss = 0.11280173063278198
iteration 216, loss = 0.102747343480587
iteration 217, loss = 0.21857097744941711
iteration 218, loss = 0.2462274730205536
iteration 219, loss = 0.24918273091316223
iteration 220, loss = 0.2145431488752365
iteration 221, loss = 0.07173057645559311
iteration 222, loss = 0.039643749594688416
iteration 223, loss = 0.02606021985411644
iteration 224, loss = 0.22058668732643127
iteration 225, loss = 0.25297272205352783
iteration 226, loss = 0.011430319398641586
iteration 227, loss = 0.4022600054740906
iteration 228, loss = 0.3995336890220642
iteration 229, loss = 0.3066953420639038
iteration 230, loss = 0.08682481199502945
iteration 231, loss = 0.010645187459886074
iteration 232, loss = 0.15703941881656647
iteration 233, loss = 0.024925684556365013
iteration 234, loss = 0.0700254812836647
iteration 235, loss = 0.18387989699840546
iteration 236, loss = 0.39677464962005615
iteration 237, loss = 0.08988639712333679
iteration 238, loss = 0.02911164052784443
iteration 239, loss = 0.016791271045804024
iteration 240, loss = 0.04539119824767113
iteration 241, loss = 0.06853349506855011
iteration 242, loss = 0.1445729285478592
iteration 243, loss = 0.10924166440963745
iteration 244, loss = 0.06712187826633453
iteration 245, loss = 0.0294840969145298
iteration 246, loss = 0.14796163141727448
iteration 247, loss = 0.04146408289670944
iteration 248, loss = 0.1371716856956482
iteration 249, loss = 0.2304408848285675
iteration 250, loss = 0.06689577549695969
iteration 251, loss = 0.007895647548139095
iteration 252, loss = 0.09486648440361023
iteration 253, loss = 0.15001042187213898
iteration 254, loss = 0.09854193031787872
iteration 255, loss = 0.18265412747859955
iteration 256, loss = 0.04151414707303047
iteration 257, loss = 0.29811233282089233
iteration 258, loss = 0.015270381234586239
iteration 259, loss = 0.02334677241742611
iteration 260, loss = 0.13692831993103027
iteration 261, loss = 0.09928672015666962
iteration 262, loss = 0.2274179756641388
iteration 263, loss = 0.09047527611255646
iteration 264, loss = 0.03835449367761612
iteration 265, loss = 0.14179658889770508
iteration 266, loss = 0.07734853774309158
iteration 267, loss = 0.10659320652484894
iteration 268, loss = 0.1984695941209793
iteration 269, loss = 0.12936224043369293
iteration 270, loss = 0.06835769861936569
iteration 271, loss = 0.14599314332008362
iteration 272, loss = 0.0748361349105835
iteration 273, loss = 0.14165140688419342
iteration 274, loss = 0.12238019704818726
iteration 275, loss = 0.13705581426620483
iteration 276, loss = 0.018109062686562538
iteration 277, loss = 0.08717647194862366
iteration 278, loss = 0.012064028531312943
iteration 279, loss = 0.09592004865407944
iteration 280, loss = 0.008192078210413456
iteration 281, loss = 0.3782777190208435
iteration 282, loss = 0.1421106904745102
iteration 283, loss = 0.19592377543449402
iteration 284, loss = 0.09940801560878754
iteration 285, loss = 0.1929464340209961
iteration 286, loss = 0.198752760887146
iteration 287, loss = 0.16547654569149017
iteration 288, loss = 0.05805366113781929
iteration 289, loss = 0.11220789700746536
iteration 290, loss = 0.008212577551603317
iteration 291, loss = 0.15826290845870972
iteration 292, loss = 0.026233280077576637
iteration 293, loss = 0.043977342545986176
iteration 294, loss = 0.017699578776955605
iteration 295, loss = 0.2323710322380066
iteration 296, loss = 0.0640198141336441
iteration 297, loss = 0.12088485062122345
iteration 298, loss = 0.11666961014270782
iteration 299, loss = 0.10226920247077942
iteration 0, loss = 0.16254666447639465
iteration 1, loss = 0.06462781131267548
iteration 2, loss = 0.04242783039808273
iteration 3, loss = 0.2675085663795471
iteration 4, loss = 0.1063326820731163
iteration 5, loss = 0.05136530101299286
iteration 6, loss = 0.07781194150447845
iteration 7, loss = 0.1192023903131485
iteration 8, loss = 0.14807343482971191
iteration 9, loss = 0.03539153188467026
iteration 10, loss = 0.07467950135469437
iteration 11, loss = 0.0929131731390953
iteration 12, loss = 0.112932488322258
iteration 13, loss = 0.07998654991388321
iteration 14, loss = 0.09655437618494034
iteration 15, loss = 0.06802572309970856
iteration 16, loss = 0.041441600769758224
iteration 17, loss = 0.046700213104486465
iteration 18, loss = 0.1669655293226242
iteration 19, loss = 0.0748576745390892
iteration 20, loss = 0.19818396866321564
iteration 21, loss = 0.2039404660463333
iteration 22, loss = 0.10953087359666824
iteration 23, loss = 0.06291142106056213
iteration 24, loss = 0.2843850553035736
iteration 25, loss = 0.20203274488449097
iteration 26, loss = 0.04642678052186966
iteration 27, loss = 0.10697424411773682
iteration 28, loss = 0.043525636196136475
iteration 29, loss = 0.05545105040073395
iteration 30, loss = 0.02980152890086174
iteration 31, loss = 0.21992632746696472
iteration 32, loss = 0.059542760252952576
iteration 33, loss = 0.16125962138175964
iteration 34, loss = 0.1536598801612854
iteration 35, loss = 0.05119726434350014
iteration 36, loss = 0.03269567713141441
iteration 37, loss = 0.3736459016799927
iteration 38, loss = 0.3086552321910858
iteration 39, loss = 0.11200346052646637
iteration 40, loss = 0.028786245733499527
iteration 41, loss = 0.04292729124426842
iteration 42, loss = 0.33369845151901245
iteration 43, loss = 0.22908493876457214
iteration 44, loss = 0.13862787187099457
iteration 45, loss = 0.05080892890691757
iteration 46, loss = 0.13535934686660767
iteration 47, loss = 0.3539399206638336
iteration 48, loss = 0.10091347992420197
iteration 49, loss = 0.180642768740654
iteration 50, loss = 0.013559483923017979
iteration 51, loss = 0.33890122175216675
iteration 52, loss = 0.10940895974636078
iteration 53, loss = 0.11631346493959427
iteration 54, loss = 0.0483468696475029
iteration 55, loss = 0.09385906159877777
iteration 56, loss = 0.1587541699409485
iteration 57, loss = 0.02503165602684021
iteration 58, loss = 0.07426531612873077
iteration 59, loss = 0.08670273423194885
iteration 60, loss = 0.0470728725194931
iteration 61, loss = 0.07002288848161697
iteration 62, loss = 0.08193019777536392
iteration 63, loss = 0.019573049619793892
iteration 64, loss = 0.08998756855726242
iteration 65, loss = 0.11595612019300461
iteration 66, loss = 0.12205477058887482
iteration 67, loss = 0.11316829919815063
iteration 68, loss = 0.06281331926584244
iteration 69, loss = 0.06513877958059311
iteration 70, loss = 0.1473146677017212
iteration 71, loss = 0.11059507727622986
iteration 72, loss = 0.2945413589477539
iteration 73, loss = 0.07031845301389694
iteration 74, loss = 0.0889994353055954
iteration 75, loss = 0.13821563124656677
iteration 76, loss = 0.15326502919197083
iteration 77, loss = 0.15209802985191345
iteration 78, loss = 0.03144443407654762
iteration 79, loss = 0.08054456859827042
iteration 80, loss = 0.08058103919029236
iteration 81, loss = 0.04298452287912369
iteration 82, loss = 0.1051812395453453
iteration 83, loss = 0.225196972489357
iteration 84, loss = 0.12784025073051453
iteration 85, loss = 0.16692374646663666
iteration 86, loss = 0.10832252353429794
iteration 87, loss = 0.11627886444330215
iteration 88, loss = 0.10170045495033264
iteration 89, loss = 0.14857767522335052
iteration 90, loss = 0.2196364402770996
iteration 91, loss = 0.19059182703495026
iteration 92, loss = 0.23344168066978455
iteration 93, loss = 0.1134735718369484
iteration 94, loss = 0.05112920701503754
iteration 95, loss = 0.22514216601848602
iteration 96, loss = 0.10185123980045319
iteration 97, loss = 0.04824225604534149
iteration 98, loss = 0.16189958155155182
iteration 99, loss = 0.20362086594104767
iteration 100, loss = 0.040458451956510544
iteration 101, loss = 0.009944465942680836
iteration 102, loss = 0.13909956812858582
iteration 103, loss = 0.15180224180221558
iteration 104, loss = 0.08636540174484253
iteration 105, loss = 0.08002546429634094
iteration 106, loss = 0.019536355510354042
iteration 107, loss = 0.06640097498893738
iteration 108, loss = 0.047311726957559586
iteration 109, loss = 0.06819120049476624
iteration 110, loss = 0.0812428668141365
iteration 111, loss = 0.00313612655736506
iteration 112, loss = 0.04968978092074394
iteration 113, loss = 0.21285225450992584
iteration 114, loss = 0.19569087028503418
iteration 115, loss = 0.18528372049331665
iteration 116, loss = 0.11482715606689453
iteration 117, loss = 0.04038039967417717
iteration 118, loss = 0.20055055618286133
iteration 119, loss = 0.13566793501377106
iteration 120, loss = 0.012414421886205673
iteration 121, loss = 0.1592535376548767
iteration 122, loss = 0.10228059440851212
iteration 123, loss = 0.27689266204833984
iteration 124, loss = 0.02585718035697937
iteration 125, loss = 0.004972762893885374
iteration 126, loss = 0.15655334293842316
iteration 127, loss = 0.07404860854148865
iteration 128, loss = 0.09243499487638474
iteration 129, loss = 0.13892555236816406
iteration 130, loss = 0.07975506782531738
iteration 131, loss = 0.10734803229570389
iteration 132, loss = 0.18676233291625977
iteration 133, loss = 0.012004604563117027
iteration 134, loss = 0.04508431255817413
iteration 135, loss = 0.35198500752449036
iteration 136, loss = 0.24111154675483704
iteration 137, loss = 0.14762571454048157
iteration 138, loss = 0.010330944322049618
iteration 139, loss = 0.029489098116755486
iteration 140, loss = 0.14299634099006653
iteration 141, loss = 0.08151497691869736
iteration 142, loss = 0.07024574279785156
iteration 143, loss = 0.23197013139724731
iteration 144, loss = 0.2080012857913971
iteration 145, loss = 0.10737758874893188
iteration 146, loss = 0.024157006293535233
iteration 147, loss = 0.14399579167366028
iteration 148, loss = 0.02922772616147995
iteration 149, loss = 0.1507958322763443
iteration 150, loss = 0.5154707431793213
iteration 151, loss = 0.06400442868471146
iteration 152, loss = 0.24393418431282043
iteration 153, loss = 0.23255489766597748
iteration 154, loss = 0.16180916130542755
iteration 155, loss = 0.09075277298688889
iteration 156, loss = 0.14424437284469604
iteration 157, loss = 0.06671588867902756
iteration 158, loss = 0.15960556268692017
iteration 159, loss = 0.37819933891296387
iteration 160, loss = 0.1256874054670334
iteration 161, loss = 0.07576321065425873
iteration 162, loss = 0.15687017142772675
iteration 163, loss = 0.10007794201374054
iteration 164, loss = 0.1058151125907898
iteration 165, loss = 0.14686068892478943
iteration 166, loss = 0.2459883987903595
iteration 167, loss = 0.2531769275665283
iteration 168, loss = 0.02163814567029476
iteration 169, loss = 0.04920640587806702
iteration 170, loss = 0.11235573142766953
iteration 171, loss = 0.09615921974182129
iteration 172, loss = 0.04514284059405327
iteration 173, loss = 0.06395656615495682
iteration 174, loss = 0.07448920607566833
iteration 175, loss = 0.06474793702363968
iteration 176, loss = 0.024991795420646667
iteration 177, loss = 0.03166709095239639
iteration 178, loss = 0.0986565500497818
iteration 179, loss = 0.20634225010871887
iteration 180, loss = 0.1820419728755951
iteration 181, loss = 0.16495341062545776
iteration 182, loss = 0.11110618710517883
iteration 183, loss = 0.14952677488327026
iteration 184, loss = 0.16436845064163208
iteration 185, loss = 0.10755977034568787
iteration 186, loss = 0.024680495262145996
iteration 187, loss = 0.06320644170045853
iteration 188, loss = 0.03357858583331108
iteration 189, loss = 0.050746139138936996
iteration 190, loss = 0.17100295424461365
iteration 191, loss = 0.06543200463056564
iteration 192, loss = 0.0181942917406559
iteration 193, loss = 0.17383195459842682
iteration 194, loss = 0.06937847286462784
iteration 195, loss = 0.0763528123497963
iteration 196, loss = 0.10722877085208893
iteration 197, loss = 0.052643872797489166
iteration 198, loss = 0.16940860450267792
iteration 199, loss = 0.08909529447555542
iteration 200, loss = 0.06789769977331161
iteration 201, loss = 0.09169366955757141
iteration 202, loss = 0.07566099613904953
iteration 203, loss = 0.14857466518878937
iteration 204, loss = 0.03893427178263664
iteration 205, loss = 0.04163923114538193
iteration 206, loss = 0.050480157136917114
iteration 207, loss = 0.271594762802124
iteration 208, loss = 0.026776764541864395
iteration 209, loss = 0.11655443906784058
iteration 210, loss = 0.07393108308315277
iteration 211, loss = 0.2067120522260666
iteration 212, loss = 0.053310152143239975
iteration 213, loss = 0.3816913962364197
iteration 214, loss = 0.02118859440088272
iteration 215, loss = 0.11547799408435822
iteration 216, loss = 0.0356592983007431
iteration 217, loss = 0.179617777466774
iteration 218, loss = 0.05709576606750488
iteration 219, loss = 0.15510153770446777
iteration 220, loss = 0.06640295684337616
iteration 221, loss = 0.07005804032087326
iteration 222, loss = 0.1048152893781662
iteration 223, loss = 0.11157900094985962
iteration 224, loss = 0.12455485761165619
iteration 225, loss = 0.06407744437456131
iteration 226, loss = 0.03844022750854492
iteration 227, loss = 0.006371619645506144
iteration 228, loss = 0.1908392608165741
iteration 229, loss = 0.13159425556659698
iteration 230, loss = 0.041651878505945206
iteration 231, loss = 0.08221720159053802
iteration 232, loss = 0.14019998908042908
iteration 233, loss = 0.08104503899812698
iteration 234, loss = 0.33728882670402527
iteration 235, loss = 0.06238112971186638
iteration 236, loss = 0.05603228509426117
iteration 237, loss = 0.03291573375463486
iteration 238, loss = 0.20294494926929474
iteration 239, loss = 0.014080160297453403
iteration 240, loss = 0.07468311488628387
iteration 241, loss = 0.0582459382712841
iteration 242, loss = 0.03625466302037239
iteration 243, loss = 0.16386732459068298
iteration 244, loss = 0.05042656511068344
iteration 245, loss = 0.008052942343056202
iteration 246, loss = 0.09588361531496048
iteration 247, loss = 0.2950005829334259
iteration 248, loss = 0.17632479965686798
iteration 249, loss = 0.09703908860683441
iteration 250, loss = 0.24367478489875793
iteration 251, loss = 0.14370106160640717
iteration 252, loss = 0.5459010601043701
iteration 253, loss = 0.22169409692287445
iteration 254, loss = 0.23149803280830383
iteration 255, loss = 0.09834322333335876
iteration 256, loss = 0.1989508718252182
iteration 257, loss = 0.14311854541301727
iteration 258, loss = 0.09493731707334518
iteration 259, loss = 0.12203162163496017
iteration 260, loss = 0.40437081456184387
iteration 261, loss = 0.11341346055269241
iteration 262, loss = 0.42105457186698914
iteration 263, loss = 0.15507595241069794
iteration 264, loss = 0.12058509886264801
iteration 265, loss = 0.012442284263670444
iteration 266, loss = 0.1512325406074524
iteration 267, loss = 0.12892431020736694
iteration 268, loss = 0.19642360508441925
iteration 269, loss = 0.0367574580013752
iteration 270, loss = 0.16534090042114258
iteration 271, loss = 0.19681686162948608
iteration 272, loss = 0.1050100326538086
iteration 273, loss = 0.09402769804000854
iteration 274, loss = 0.09166224300861359
iteration 275, loss = 0.06722941249608994
iteration 276, loss = 0.2486186921596527
iteration 277, loss = 0.13910986483097076
iteration 278, loss = 0.0017118698451668024
iteration 279, loss = 0.16700565814971924
iteration 280, loss = 0.0571383461356163
iteration 281, loss = 0.21572482585906982
iteration 282, loss = 0.12855061888694763
iteration 283, loss = 0.15033705532550812
iteration 284, loss = 0.05716093257069588
iteration 285, loss = 0.05560106039047241
iteration 286, loss = 0.19860173761844635
iteration 287, loss = 0.002557970117777586
iteration 288, loss = 0.1770816147327423
iteration 289, loss = 0.02940436638891697
iteration 290, loss = 0.030682606622576714
iteration 291, loss = 0.1337089091539383
iteration 292, loss = 0.2267807126045227
iteration 293, loss = 0.17670807242393494
iteration 294, loss = 0.07480444014072418
iteration 295, loss = 0.13216112554073334
iteration 296, loss = 0.033780425786972046
iteration 297, loss = 0.16071361303329468
iteration 298, loss = 0.011523297056555748
iteration 299, loss = 0.0410979799926281
iteration 0, loss = 0.0928463265299797
iteration 1, loss = 0.11279672384262085
iteration 2, loss = 0.0795593187212944
iteration 3, loss = 0.15602710843086243
iteration 4, loss = 0.2274559736251831
iteration 5, loss = 0.011261782608926296
iteration 6, loss = 0.16697122156620026
iteration 7, loss = 0.12307679653167725
iteration 8, loss = 0.0011857417412102222
iteration 9, loss = 0.2974472641944885
iteration 10, loss = 0.16196580231189728
iteration 11, loss = 0.10545109957456589
iteration 12, loss = 0.11321457475423813
iteration 13, loss = 0.021981948986649513
iteration 14, loss = 0.24789756536483765
iteration 15, loss = 0.10809066891670227
iteration 16, loss = 0.13145115971565247
iteration 17, loss = 0.151658296585083
iteration 18, loss = 0.29675063490867615
iteration 19, loss = 0.017930345609784126
iteration 20, loss = 0.006816391833126545
iteration 21, loss = 0.16401773691177368
iteration 22, loss = 0.3096262812614441
iteration 23, loss = 0.3223884701728821
iteration 24, loss = 0.22868898510932922
iteration 25, loss = 0.239541158080101
iteration 26, loss = 0.02327769622206688
iteration 27, loss = 0.10857348144054413
iteration 28, loss = 0.21801069378852844
iteration 29, loss = 0.11615177243947983
iteration 30, loss = 0.21659274399280548
iteration 31, loss = 0.030646100640296936
iteration 32, loss = 0.23511530458927155
iteration 33, loss = 0.19422928988933563
iteration 34, loss = 0.21784089505672455
iteration 35, loss = 0.0041077760979533195
iteration 36, loss = 0.27167388796806335
iteration 37, loss = 0.11529698222875595
iteration 38, loss = 0.07852432131767273
iteration 39, loss = 0.12270751595497131
iteration 40, loss = 0.24178454279899597
iteration 41, loss = 0.09623675793409348
iteration 42, loss = 0.014715221710503101
iteration 43, loss = 0.10327565670013428
iteration 44, loss = 0.2609216570854187
iteration 45, loss = 0.15724308788776398
iteration 46, loss = 0.20840495824813843
iteration 47, loss = 0.027146728709340096
iteration 48, loss = 0.12841089069843292
iteration 49, loss = 0.05671640485525131
iteration 50, loss = 0.060571685433387756
iteration 51, loss = 0.3019769787788391
iteration 52, loss = 0.0530063733458519
iteration 53, loss = 0.11931309103965759
iteration 54, loss = 0.3371613025665283
iteration 55, loss = 0.012427050620317459
iteration 56, loss = 0.03570641577243805
iteration 57, loss = 0.19930867850780487
iteration 58, loss = 0.3662821054458618
iteration 59, loss = 0.24575020372867584
iteration 60, loss = 0.265972375869751
iteration 61, loss = 0.0014176847180351615
iteration 62, loss = 0.04748046398162842
iteration 63, loss = 0.08993929624557495
iteration 64, loss = 0.16529828310012817
iteration 65, loss = 0.1944255232810974
iteration 66, loss = 0.01578284054994583
iteration 67, loss = 0.2811940610408783
iteration 68, loss = 0.14571744203567505
iteration 69, loss = 0.17584609985351562
iteration 70, loss = 0.1545172929763794
iteration 71, loss = 0.11679847538471222
iteration 72, loss = 0.2155124992132187
iteration 73, loss = 0.09616810083389282
iteration 74, loss = 0.08278745412826538
iteration 75, loss = 0.049779050052165985
iteration 76, loss = 0.11307786405086517
iteration 77, loss = 0.44387638568878174
iteration 78, loss = 0.00632465910166502
iteration 79, loss = 0.2488396167755127
iteration 80, loss = 0.018316272646188736
iteration 81, loss = 0.0832667127251625
iteration 82, loss = 0.08914229273796082
iteration 83, loss = 0.21820726990699768
iteration 84, loss = 0.03162776678800583
iteration 85, loss = 0.11211521923542023
iteration 86, loss = 0.12112560123205185
iteration 87, loss = 0.01934707537293434
iteration 88, loss = 0.19802260398864746
iteration 89, loss = 0.17518937587738037
iteration 90, loss = 0.23711854219436646
iteration 91, loss = 0.09368827939033508
iteration 92, loss = 0.1891375035047531
iteration 93, loss = 0.027456074953079224
iteration 94, loss = 0.021014699712395668
iteration 95, loss = 0.023242635652422905
iteration 96, loss = 0.11427892744541168
iteration 97, loss = 0.36494460701942444
iteration 98, loss = 0.02395293489098549
iteration 99, loss = 0.04304034262895584
iteration 100, loss = 0.12929502129554749
iteration 101, loss = 0.1328396201133728
iteration 102, loss = 0.0120733343064785
iteration 103, loss = 0.16754339635372162
iteration 104, loss = 0.11128249764442444
iteration 105, loss = 0.17127259075641632
iteration 106, loss = 0.038741715252399445
iteration 107, loss = 0.07964742183685303
iteration 108, loss = 0.007799290120601654
iteration 109, loss = 0.3308529257774353
iteration 110, loss = 0.10938549786806107
iteration 111, loss = 0.07071276009082794
iteration 112, loss = 0.13142265379428864
iteration 113, loss = 0.17341206967830658
iteration 114, loss = 0.2382422238588333
iteration 115, loss = 0.06088557839393616
iteration 116, loss = 0.11930141597986221
iteration 117, loss = 0.07653530687093735
iteration 118, loss = 0.10251808911561966
iteration 119, loss = 0.17048723995685577
iteration 120, loss = 0.0807182639837265
iteration 121, loss = 0.1990336775779724
iteration 122, loss = 0.01667940244078636
iteration 123, loss = 0.05199144408106804
iteration 124, loss = 0.06072208285331726
iteration 125, loss = 0.03400246053934097
iteration 126, loss = 0.14108848571777344
iteration 127, loss = 0.45996254682540894
iteration 128, loss = 0.12397108972072601
iteration 129, loss = 0.2620101869106293
iteration 130, loss = 0.06580395996570587
iteration 131, loss = 0.004983752500265837
iteration 132, loss = 0.1541866511106491
iteration 133, loss = 0.3969423770904541
iteration 134, loss = 0.08440762758255005
iteration 135, loss = 0.08661305159330368
iteration 136, loss = 0.19732367992401123
iteration 137, loss = 0.09368613362312317
iteration 138, loss = 0.11404521763324738
iteration 139, loss = 0.06052395701408386
iteration 140, loss = 0.23830312490463257
iteration 141, loss = 0.1492491215467453
iteration 142, loss = 0.1966070979833603
iteration 143, loss = 0.17679841816425323
iteration 144, loss = 0.221322700381279
iteration 145, loss = 0.28936490416526794
iteration 146, loss = 0.031063467264175415
iteration 147, loss = 0.37722060084342957
iteration 148, loss = 0.13307563960552216
iteration 149, loss = 0.06282869726419449
iteration 150, loss = 0.025770820677280426
iteration 151, loss = 0.02599741891026497
iteration 152, loss = 0.3410605490207672
iteration 153, loss = 0.2536693513393402
iteration 154, loss = 0.02085888385772705
iteration 155, loss = 0.14941713213920593
iteration 156, loss = 0.17690037190914154
iteration 157, loss = 0.1602487713098526
iteration 158, loss = 0.07000186294317245
iteration 159, loss = 0.07804856449365616
iteration 160, loss = 0.04813167452812195
iteration 161, loss = 0.08331791311502457
iteration 162, loss = 0.14514918625354767
iteration 163, loss = 0.15056933462619781
iteration 164, loss = 0.06867863982915878
iteration 165, loss = 0.03577253594994545
iteration 166, loss = 0.0559382401406765
iteration 167, loss = 0.08385424315929413
iteration 168, loss = 0.036504101008176804
iteration 169, loss = 0.07931609451770782
iteration 170, loss = 0.15170280635356903
iteration 171, loss = 0.10156647861003876
iteration 172, loss = 0.23981526494026184
iteration 173, loss = 0.1833699345588684
iteration 174, loss = 0.12615275382995605
iteration 175, loss = 0.10698364675045013
iteration 176, loss = 0.035831268876791
iteration 177, loss = 0.08995309472084045
iteration 178, loss = 0.14198003709316254
iteration 179, loss = 0.16890457272529602
iteration 180, loss = 0.19305838644504547
iteration 181, loss = 0.09409983456134796
iteration 182, loss = 0.25591209530830383
iteration 183, loss = 0.19093245267868042
iteration 184, loss = 0.18148213624954224
iteration 185, loss = 0.12671253085136414
iteration 186, loss = 0.13358496129512787
iteration 187, loss = 0.1606886088848114
iteration 188, loss = 0.22996416687965393
iteration 189, loss = 0.13240179419517517
iteration 190, loss = 0.05474946275353432
iteration 191, loss = 0.12122407555580139
iteration 192, loss = 0.17119722068309784
iteration 193, loss = 0.15177901089191437
iteration 194, loss = 0.007734253071248531
iteration 195, loss = 0.153585746884346
iteration 196, loss = 0.24498096108436584
iteration 197, loss = 0.11669301986694336
iteration 198, loss = 0.09865514189004898
iteration 199, loss = 0.1464628130197525
iteration 200, loss = 0.27169135212898254
iteration 201, loss = 0.3258288502693176
iteration 202, loss = 0.15080580115318298
iteration 203, loss = 0.2616995573043823
iteration 204, loss = 0.4800529479980469
iteration 205, loss = 0.32374465465545654
iteration 206, loss = 0.07347479462623596
iteration 207, loss = 0.16109852492809296
iteration 208, loss = 0.19892820715904236
iteration 209, loss = 0.281934916973114
iteration 210, loss = 0.19822244346141815
iteration 211, loss = 0.21699658036231995
iteration 212, loss = 0.20614364743232727
iteration 213, loss = 0.10366236418485641
iteration 214, loss = 0.27705442905426025
iteration 215, loss = 0.20294152200222015
iteration 216, loss = 0.18767571449279785
iteration 217, loss = 0.05736522376537323
iteration 218, loss = 0.35215485095977783
iteration 219, loss = 0.07716814428567886
iteration 220, loss = 0.06219926476478577
iteration 221, loss = 0.08515467494726181
iteration 222, loss = 0.08902819454669952
iteration 223, loss = 0.1121966689825058
iteration 224, loss = 0.2725551128387451
iteration 225, loss = 0.05453063175082207
iteration 226, loss = 0.10094528645277023
iteration 227, loss = 0.16502027213573456
iteration 228, loss = 0.07654714584350586
iteration 229, loss = 0.09248614311218262
iteration 230, loss = 0.07846242189407349
iteration 231, loss = 0.04980720207095146
iteration 232, loss = 0.09949319809675217
iteration 233, loss = 0.1829776018857956
iteration 234, loss = 0.05556328594684601
iteration 235, loss = 0.4357669949531555
iteration 236, loss = 0.12144705653190613
iteration 237, loss = 0.08298641443252563
iteration 238, loss = 0.2131851464509964
iteration 239, loss = 0.08443691581487656
iteration 240, loss = 0.09154551476240158
iteration 241, loss = 0.10704705119132996
iteration 242, loss = 0.04792536795139313
iteration 243, loss = 0.054555971175432205
iteration 244, loss = 0.03640330955386162
iteration 245, loss = 0.024722665548324585
iteration 246, loss = 0.5762569308280945
iteration 247, loss = 0.3940737247467041
iteration 248, loss = 0.19399559497833252
iteration 249, loss = 0.07313034683465958
iteration 250, loss = 0.05196896195411682
iteration 251, loss = 0.1886037439107895
iteration 252, loss = 0.48097488284111023
iteration 253, loss = 0.45667991042137146
iteration 254, loss = 0.18461138010025024
iteration 255, loss = 0.2812093496322632
iteration 256, loss = 0.3953871726989746
iteration 257, loss = 0.0013364457990974188
iteration 258, loss = 0.27155402302742004
iteration 259, loss = 0.647960364818573
iteration 260, loss = 0.356627494096756
iteration 261, loss = 0.4961697459220886
iteration 262, loss = 0.4733668267726898
iteration 263, loss = 0.20333248376846313
iteration 264, loss = 0.025728926062583923
iteration 265, loss = 0.025825204327702522
iteration 266, loss = 0.30543437600135803
iteration 267, loss = 0.27192986011505127
iteration 268, loss = 0.18717308342456818
iteration 269, loss = 0.023842107504606247
iteration 270, loss = 0.37152400612831116
iteration 271, loss = 0.08720644563436508
iteration 272, loss = 0.0128822335973382
iteration 273, loss = 0.03809916973114014
iteration 274, loss = 0.026614271104335785
iteration 275, loss = 0.3116381764411926
iteration 276, loss = 0.08489490300416946
iteration 277, loss = 0.2328607141971588
iteration 278, loss = 0.27765920758247375
iteration 279, loss = 0.17244578897953033
iteration 280, loss = 0.24798837304115295
iteration 281, loss = 0.047708164900541306
iteration 282, loss = 0.09996379911899567
iteration 283, loss = 0.37278860807418823
iteration 284, loss = 0.7517684698104858
iteration 285, loss = 0.06765837967395782
iteration 286, loss = 0.4198382496833801
iteration 287, loss = 0.2654498517513275
iteration 288, loss = 0.02113962359726429
iteration 289, loss = 0.23203140497207642
iteration 290, loss = 0.25456151366233826
iteration 291, loss = 0.3425193130970001
iteration 292, loss = 0.24837979674339294
iteration 293, loss = 0.2577994763851166
iteration 294, loss = 0.19557181000709534
iteration 295, loss = 0.15499810874462128
iteration 296, loss = 0.025697344914078712
iteration 297, loss = 0.09324629604816437
iteration 298, loss = 0.3319602906703949
iteration 299, loss = 0.16356609761714935
iteration 0, loss = 0.1754882037639618
iteration 1, loss = 0.11014334857463837
iteration 2, loss = 0.09575144946575165
iteration 3, loss = 0.184803768992424
iteration 4, loss = 0.27028489112854004
iteration 5, loss = 0.1675509661436081
iteration 6, loss = 0.024749161675572395
iteration 7, loss = 0.08566929399967194
iteration 8, loss = 0.09764993190765381
iteration 9, loss = 0.13256412744522095
iteration 10, loss = 0.16746298968791962
iteration 11, loss = 0.08959406614303589
iteration 12, loss = 0.028577392920851707
iteration 13, loss = 0.008354605175554752
iteration 14, loss = 0.1686834841966629
iteration 15, loss = 0.33687496185302734
iteration 16, loss = 0.2569851279258728
iteration 17, loss = 0.04877946525812149
iteration 18, loss = 0.16365961730480194
iteration 19, loss = 0.19769445061683655
iteration 20, loss = 0.08772331476211548
iteration 21, loss = 0.08709122985601425
iteration 22, loss = 0.16779698431491852
iteration 23, loss = 0.09405321627855301
iteration 24, loss = 0.04355109483003616
iteration 25, loss = 0.09636850655078888
iteration 26, loss = 0.038850583136081696
iteration 27, loss = 0.18310990929603577
iteration 28, loss = 0.17992456257343292
iteration 29, loss = 0.15462835133075714
iteration 30, loss = 0.3157045543193817
iteration 31, loss = 0.11878001689910889
iteration 32, loss = 0.156830295920372
iteration 33, loss = 0.13983756303787231
iteration 34, loss = 0.10368885844945908
iteration 35, loss = 0.0638803020119667
iteration 36, loss = 0.3495139479637146
iteration 37, loss = 0.11566885560750961
iteration 38, loss = 0.17518669366836548
iteration 39, loss = 0.1657327562570572
iteration 40, loss = 0.06558217853307724
iteration 41, loss = 0.056090470403432846
iteration 42, loss = 0.09627142548561096
iteration 43, loss = 0.21579162776470184
iteration 44, loss = 0.23546649515628815
iteration 45, loss = 0.016410019248723984
iteration 46, loss = 0.23638179898262024
iteration 47, loss = 0.059897489845752716
iteration 48, loss = 0.04149267449975014
iteration 49, loss = 0.02818375453352928
iteration 50, loss = 0.022925373166799545
iteration 51, loss = 0.24275769293308258
iteration 52, loss = 0.32592764496803284
iteration 53, loss = 0.1909642219543457
iteration 54, loss = 0.07606492936611176
iteration 55, loss = 0.13365696370601654
iteration 56, loss = 0.11475174129009247
iteration 57, loss = 0.12449485063552856
iteration 58, loss = 0.04393342509865761
iteration 59, loss = 0.05751388892531395
iteration 60, loss = 0.04517549276351929
iteration 61, loss = 0.1994061917066574
iteration 62, loss = 0.0034356089308857918
iteration 63, loss = 0.10655836760997772
iteration 64, loss = 0.18149088323116302
iteration 65, loss = 0.04019162803888321
iteration 66, loss = 0.19969329237937927
iteration 67, loss = 0.06698301434516907
iteration 68, loss = 0.00835523009300232
iteration 69, loss = 0.05849505215883255
iteration 70, loss = 0.13011792302131653
iteration 71, loss = 0.11562326550483704
iteration 72, loss = 0.18895573914051056
iteration 73, loss = 0.07749220728874207
iteration 74, loss = 0.12270339578390121
iteration 75, loss = 0.07271644473075867
iteration 76, loss = 0.1503651738166809
iteration 77, loss = 0.11645615100860596
iteration 78, loss = 0.15561144053936005
iteration 79, loss = 0.08345320075750351
iteration 80, loss = 0.0750681459903717
iteration 81, loss = 0.05727721378207207
iteration 82, loss = 0.12255528569221497
iteration 83, loss = 0.009819377213716507
iteration 84, loss = 0.05937271565198898
iteration 85, loss = 0.13314709067344666
iteration 86, loss = 0.08387594670057297
iteration 87, loss = 0.09697775542736053
iteration 88, loss = 0.09764960408210754
iteration 89, loss = 0.035652343183755875
iteration 90, loss = 0.13395261764526367
iteration 91, loss = 0.06017129495739937
iteration 92, loss = 0.0791139155626297
iteration 93, loss = 0.10203702747821808
iteration 94, loss = 0.07687492668628693
iteration 95, loss = 0.02314012683928013
iteration 96, loss = 0.16005682945251465
iteration 97, loss = 0.18238937854766846
iteration 98, loss = 0.07243069261312485
iteration 99, loss = 0.012942268513143063
iteration 100, loss = 0.014547482132911682
iteration 101, loss = 0.2748836576938629
iteration 102, loss = 0.21893605589866638
iteration 103, loss = 0.053475525230169296
iteration 104, loss = 0.08427031338214874
iteration 105, loss = 0.05203355476260185
iteration 106, loss = 0.12808576226234436
iteration 107, loss = 0.10358209908008575
iteration 108, loss = 0.05321095138788223
iteration 109, loss = 0.13356447219848633
iteration 110, loss = 0.09574328362941742
iteration 111, loss = 0.11773569136857986
iteration 112, loss = 0.00841370690613985
iteration 113, loss = 0.04528524726629257
iteration 114, loss = 0.12446855008602142
iteration 115, loss = 0.04219166561961174
iteration 116, loss = 0.014007097110152245
iteration 117, loss = 0.11249314993619919
iteration 118, loss = 0.04300706833600998
iteration 119, loss = 0.07314242422580719
iteration 120, loss = 0.039093490689992905
iteration 121, loss = 0.14221367239952087
iteration 122, loss = 0.030547315254807472
iteration 123, loss = 0.07510405033826828
iteration 124, loss = 0.11644906550645828
iteration 125, loss = 0.05975475534796715
iteration 126, loss = 0.0715520977973938
iteration 127, loss = 0.05204842984676361
iteration 128, loss = 0.04938989877700806
iteration 129, loss = 0.06663760542869568
iteration 130, loss = 0.09834214299917221
iteration 131, loss = 0.22793182730674744
iteration 132, loss = 0.08436542749404907
iteration 133, loss = 0.0081452252343297
iteration 134, loss = 0.2621409595012665
iteration 135, loss = 0.217027947306633
iteration 136, loss = 0.08197630196809769
iteration 137, loss = 0.23547214269638062
iteration 138, loss = 0.06255731731653214
iteration 139, loss = 0.08271287381649017
iteration 140, loss = 0.1568654477596283
iteration 141, loss = 0.09648925065994263
iteration 142, loss = 0.024450069293379784
iteration 143, loss = 0.03349681571125984
iteration 144, loss = 0.11085041612386703
iteration 145, loss = 0.0979396253824234
iteration 146, loss = 0.09281602501869202
iteration 147, loss = 0.08378957957029343
iteration 148, loss = 0.11257562786340714
iteration 149, loss = 0.014220397919416428
iteration 150, loss = 0.07121400535106659
iteration 151, loss = 0.23153598606586456
iteration 152, loss = 0.022955555468797684
iteration 153, loss = 0.07051398605108261
iteration 154, loss = 0.10181049257516861
iteration 155, loss = 0.05887923389673233
iteration 156, loss = 0.20246678590774536
iteration 157, loss = 0.08954578638076782
iteration 158, loss = 0.038001649081707
iteration 159, loss = 0.06696081161499023
iteration 160, loss = 0.20294322073459625
iteration 161, loss = 0.13779351115226746
iteration 162, loss = 0.09639444202184677
iteration 163, loss = 0.0816773846745491
iteration 164, loss = 0.03372181951999664
iteration 165, loss = 0.12314367294311523
iteration 166, loss = 0.20060071349143982
iteration 167, loss = 0.057738110423088074
iteration 168, loss = 0.0298311784863472
iteration 169, loss = 0.03357260301709175
iteration 170, loss = 0.13378062844276428
iteration 171, loss = 0.2465546578168869
iteration 172, loss = 0.07730595022439957
iteration 173, loss = 0.02252967469394207
iteration 174, loss = 0.12342527508735657
iteration 175, loss = 0.22412654757499695
iteration 176, loss = 0.18910270929336548
iteration 177, loss = 0.06647280603647232
iteration 178, loss = 0.05608850717544556
iteration 179, loss = 0.08726958185434341
iteration 180, loss = 0.15967656672000885
iteration 181, loss = 0.33342018723487854
iteration 182, loss = 0.20190055668354034
iteration 183, loss = 0.11090794950723648
iteration 184, loss = 0.18316204845905304
iteration 185, loss = 0.1399502158164978
iteration 186, loss = 0.10234733670949936
iteration 187, loss = 0.18784081935882568
iteration 188, loss = 0.20090778172016144
iteration 189, loss = 0.11142104864120483
iteration 190, loss = 0.09023450314998627
iteration 191, loss = 0.026936911046504974
iteration 192, loss = 0.04092097282409668
iteration 193, loss = 0.06700784713029861
iteration 194, loss = 0.014239130541682243
iteration 195, loss = 0.14696933329105377
iteration 196, loss = 0.05689786374568939
iteration 197, loss = 0.1569189876317978
iteration 198, loss = 0.10833090543746948
iteration 199, loss = 0.2379341870546341
iteration 200, loss = 0.007215031888335943
iteration 201, loss = 0.13773725926876068
iteration 202, loss = 0.11135851591825485
iteration 203, loss = 0.06620348989963531
iteration 204, loss = 0.2805102467536926
iteration 205, loss = 0.04988301172852516
iteration 206, loss = 0.02158098667860031
iteration 207, loss = 0.26675570011138916
iteration 208, loss = 0.16709476709365845
iteration 209, loss = 0.02520480565726757
iteration 210, loss = 0.055769361555576324
iteration 211, loss = 0.09810179471969604
iteration 212, loss = 0.08573652058839798
iteration 213, loss = 0.10073985904455185
iteration 214, loss = 0.08979378640651703
iteration 215, loss = 0.11323516815900803
iteration 216, loss = 0.01691581681370735
iteration 217, loss = 0.20732897520065308
iteration 218, loss = 0.0781467854976654
iteration 219, loss = 0.23863157629966736
iteration 220, loss = 0.14210033416748047
iteration 221, loss = 0.11314750462770462
iteration 222, loss = 0.02626095525920391
iteration 223, loss = 0.23414693772792816
iteration 224, loss = 0.14247466623783112
iteration 225, loss = 0.03153484687209129
iteration 226, loss = 0.07079118490219116
iteration 227, loss = 0.06800079345703125
iteration 228, loss = 0.0956309586763382
iteration 229, loss = 0.13309027254581451
iteration 230, loss = 0.07363921403884888
iteration 231, loss = 0.13269564509391785
iteration 232, loss = 0.11638213694095612
iteration 233, loss = 0.04021386429667473
iteration 234, loss = 0.12568210065364838
iteration 235, loss = 0.009537546895444393
iteration 236, loss = 0.04884308949112892
iteration 237, loss = 0.04369193688035011
iteration 238, loss = 0.0038731698878109455
iteration 239, loss = 0.19439928233623505
iteration 240, loss = 0.21795040369033813
iteration 241, loss = 0.25116777420043945
iteration 242, loss = 0.03342052549123764
iteration 243, loss = 0.051679614931344986
iteration 244, loss = 0.08684000372886658
iteration 245, loss = 0.03848382830619812
iteration 246, loss = 0.059092625975608826
iteration 247, loss = 0.1448391079902649
iteration 248, loss = 0.08598671853542328
iteration 249, loss = 0.04890253394842148
iteration 250, loss = 0.16580995917320251
iteration 251, loss = 0.06232714280486107
iteration 252, loss = 0.04788174107670784
iteration 253, loss = 0.18786472082138062
iteration 254, loss = 0.14417895674705505
iteration 255, loss = 0.06846724450588226
iteration 256, loss = 0.09647254645824432
iteration 257, loss = 0.25094714760780334
iteration 258, loss = 0.09450607001781464
iteration 259, loss = 0.044611670076847076
iteration 260, loss = 0.0778399109840393
iteration 261, loss = 0.29711079597473145
iteration 262, loss = 0.11345687508583069
iteration 263, loss = 0.1058659702539444
iteration 264, loss = 0.061941005289554596
iteration 265, loss = 0.07883008569478989
iteration 266, loss = 0.0852149948477745
iteration 267, loss = 0.12810245156288147
iteration 268, loss = 0.18310494720935822
iteration 269, loss = 0.011806007474660873
iteration 270, loss = 0.050917088985443115
iteration 271, loss = 0.005673375446349382
iteration 272, loss = 0.33405452966690063
iteration 273, loss = 0.2664911150932312
iteration 274, loss = 0.02457803674042225
iteration 275, loss = 0.05810704454779625
iteration 276, loss = 0.1411323994398117
iteration 277, loss = 0.07929155975580215
iteration 278, loss = 0.16986651718616486
iteration 279, loss = 0.12374240905046463
iteration 280, loss = 0.1450895220041275
iteration 281, loss = 0.01113947294652462
iteration 282, loss = 0.016486994922161102
iteration 283, loss = 0.21174836158752441
iteration 284, loss = 0.1881558895111084
iteration 285, loss = 0.2286733090877533
iteration 286, loss = 0.1633952260017395
iteration 287, loss = 0.2808382213115692
iteration 288, loss = 0.005116573069244623
iteration 289, loss = 0.36793649196624756
iteration 290, loss = 0.291537880897522
iteration 291, loss = 0.11145991086959839
iteration 292, loss = 0.10166235268115997
iteration 293, loss = 0.1588871330022812
iteration 294, loss = 0.1364523470401764
iteration 295, loss = 0.038448598235845566
iteration 296, loss = 0.268816202878952
iteration 297, loss = 0.07760792970657349
iteration 298, loss = 0.18097415566444397
iteration 299, loss = 0.09359776973724365
iteration 0, loss = 0.0367782860994339
iteration 1, loss = 0.17991800606250763
iteration 2, loss = 0.03295459970831871
iteration 3, loss = 0.06771339476108551
iteration 4, loss = 0.02741718478500843
iteration 5, loss = 0.1793166846036911
iteration 6, loss = 0.16052532196044922
iteration 7, loss = 0.11669282615184784
iteration 8, loss = 0.11866654455661774
iteration 9, loss = 0.12880659103393555
iteration 10, loss = 0.16641661524772644
iteration 11, loss = 0.09439460933208466
iteration 12, loss = 0.1457499861717224
iteration 13, loss = 0.08143466711044312
iteration 14, loss = 0.04541420564055443
iteration 15, loss = 0.04746543988585472
iteration 16, loss = 0.061141420155763626
iteration 17, loss = 0.08267756551504135
iteration 18, loss = 0.24391943216323853
iteration 19, loss = 0.10863450169563293
iteration 20, loss = 0.07899888604879379
iteration 21, loss = 0.13877102732658386
iteration 22, loss = 0.07747437804937363
iteration 23, loss = 0.24130646884441376
iteration 24, loss = 0.2719767391681671
iteration 25, loss = 0.018470587208867073
iteration 26, loss = 0.07982585579156876
iteration 27, loss = 0.04149843007326126
iteration 28, loss = 0.03495335951447487
iteration 29, loss = 0.01858711615204811
iteration 30, loss = 0.010510199703276157
iteration 31, loss = 0.2270423322916031
iteration 32, loss = 0.0796932652592659
iteration 33, loss = 0.10479648411273956
iteration 34, loss = 0.16538499295711517
iteration 35, loss = 0.0868426114320755
iteration 36, loss = 0.10910354554653168
iteration 37, loss = 0.1844068169593811
iteration 38, loss = 0.14494356513023376
iteration 39, loss = 0.07558222860097885
iteration 40, loss = 0.07673131674528122
iteration 41, loss = 0.01378268375992775
iteration 42, loss = 0.048065949231386185
iteration 43, loss = 0.17186091840267181
iteration 44, loss = 0.05682096257805824
iteration 45, loss = 0.07900341600179672
iteration 46, loss = 0.02401108853518963
iteration 47, loss = 0.057921890169382095
iteration 48, loss = 0.04233396798372269
iteration 49, loss = 0.07502193748950958
iteration 50, loss = 0.051244404166936874
iteration 51, loss = 0.10097082704305649
iteration 52, loss = 0.11766034364700317
iteration 53, loss = 0.061432238668203354
iteration 54, loss = 0.04639618471264839
iteration 55, loss = 0.14263491332530975
iteration 56, loss = 0.1732189804315567
iteration 57, loss = 0.06492184102535248
iteration 58, loss = 0.23892301321029663
iteration 59, loss = 0.1012226790189743
iteration 60, loss = 0.12655021250247955
iteration 61, loss = 0.07922716438770294
iteration 62, loss = 0.3556470572948456
iteration 63, loss = 0.02467687427997589
iteration 64, loss = 0.1047636941075325
iteration 65, loss = 0.04909144341945648
iteration 66, loss = 0.13710738718509674
iteration 67, loss = 0.057799216359853745
iteration 68, loss = 0.09476859867572784
iteration 69, loss = 0.054799217730760574
iteration 70, loss = 0.03784702345728874
iteration 71, loss = 0.1593845635652542
iteration 72, loss = 0.017473312094807625
iteration 73, loss = 0.09584230929613113
iteration 74, loss = 0.20100954174995422
iteration 75, loss = 0.161626398563385
iteration 76, loss = 0.14324504137039185
iteration 77, loss = 0.27397918701171875
iteration 78, loss = 0.16266688704490662
iteration 79, loss = 0.0715952217578888
iteration 80, loss = 0.013502104207873344
iteration 81, loss = 0.3094606101512909
iteration 82, loss = 0.2784399688243866
iteration 83, loss = 0.006013919599354267
iteration 84, loss = 0.043427955359220505
iteration 85, loss = 0.015180274844169617
iteration 86, loss = 0.08128746598958969
iteration 87, loss = 0.06099480763077736
iteration 88, loss = 0.06113956868648529
iteration 89, loss = 0.025572221726179123
iteration 90, loss = 0.045523516833782196
iteration 91, loss = 0.08997838944196701
iteration 92, loss = 0.09731592237949371
iteration 93, loss = 0.17597728967666626
iteration 94, loss = 0.06099996343255043
iteration 95, loss = 0.062217794358730316
iteration 96, loss = 0.021581854671239853
iteration 97, loss = 0.14904503524303436
iteration 98, loss = 0.062088388949632645
iteration 99, loss = 0.01296903658658266
iteration 100, loss = 0.03833632171154022
iteration 101, loss = 0.02488846704363823
iteration 102, loss = 0.43074852228164673
iteration 103, loss = 0.09187129884958267
iteration 104, loss = 0.05529747158288956
iteration 105, loss = 0.090238057076931
iteration 106, loss = 0.22949203848838806
iteration 107, loss = 0.0854276716709137
iteration 108, loss = 0.1743468940258026
iteration 109, loss = 0.4631086587905884
iteration 110, loss = 0.16183386743068695
iteration 111, loss = 0.3113514482975006
iteration 112, loss = 0.04076046869158745
iteration 113, loss = 0.09783154726028442
iteration 114, loss = 0.06460949033498764
iteration 115, loss = 0.08904001116752625
iteration 116, loss = 0.06851854920387268
iteration 117, loss = 0.17716404795646667
iteration 118, loss = 0.12644456326961517
iteration 119, loss = 0.16857048869132996
iteration 120, loss = 0.18371494114398956
iteration 121, loss = 0.19175197184085846
iteration 122, loss = 0.10108909010887146
iteration 123, loss = 0.225619837641716
iteration 124, loss = 0.021757297217845917
iteration 125, loss = 0.25517377257347107
iteration 126, loss = 0.13142408430576324
iteration 127, loss = 0.20369282364845276
iteration 128, loss = 0.022720035165548325
iteration 129, loss = 0.054243382066488266
iteration 130, loss = 0.061439044773578644
iteration 131, loss = 0.16657879948616028
iteration 132, loss = 0.10193443298339844
iteration 133, loss = 0.06991748511791229
iteration 134, loss = 0.012956034392118454
iteration 135, loss = 0.0619196854531765
iteration 136, loss = 0.039377398788928986
iteration 137, loss = 0.10445930063724518
iteration 138, loss = 0.13244496285915375
iteration 139, loss = 0.009205549955368042
iteration 140, loss = 0.12930861115455627
iteration 141, loss = 0.03629675880074501
iteration 142, loss = 0.03722512349486351
iteration 143, loss = 0.11713320016860962
iteration 144, loss = 0.06436894834041595
iteration 145, loss = 0.011672900058329105
iteration 146, loss = 0.019054926931858063
iteration 147, loss = 0.04488997906446457
iteration 148, loss = 0.17255087196826935
iteration 149, loss = 0.16519613564014435
iteration 150, loss = 0.023106854408979416
iteration 151, loss = 0.10164474695920944
iteration 152, loss = 0.12713725864887238
iteration 153, loss = 0.14629711210727692
iteration 154, loss = 0.06268543004989624
iteration 155, loss = 0.09964630752801895
iteration 156, loss = 0.00381734617985785
iteration 157, loss = 0.16700491309165955
iteration 158, loss = 0.17033109068870544
iteration 159, loss = 0.0005654166452586651
iteration 160, loss = 0.2669936418533325
iteration 161, loss = 0.19726184010505676
iteration 162, loss = 0.0968824177980423
iteration 163, loss = 0.08547043055295944
iteration 164, loss = 0.16824164986610413
iteration 165, loss = 0.1768917739391327
iteration 166, loss = 0.13255468010902405
iteration 167, loss = 0.30797773599624634
iteration 168, loss = 0.13406164944171906
iteration 169, loss = 0.07833510637283325
iteration 170, loss = 0.20618683099746704
iteration 171, loss = 0.13295745849609375
iteration 172, loss = 0.22536244988441467
iteration 173, loss = 0.14273758232593536
iteration 174, loss = 0.16253235936164856
iteration 175, loss = 0.04803961142897606
iteration 176, loss = 0.14757327735424042
iteration 177, loss = 0.031682781875133514
iteration 178, loss = 0.05430703982710838
iteration 179, loss = 0.17915920913219452
iteration 180, loss = 0.07845307886600494
iteration 181, loss = 0.1500842422246933
iteration 182, loss = 0.15286795794963837
iteration 183, loss = 0.11010674387216568
iteration 184, loss = 0.03351988643407822
iteration 185, loss = 0.029085591435432434
iteration 186, loss = 0.3230697214603424
iteration 187, loss = 0.015052525326609612
iteration 188, loss = 0.018644727766513824
iteration 189, loss = 0.057881880551576614
iteration 190, loss = 0.039709750562906265
iteration 191, loss = 0.03680311515927315
iteration 192, loss = 0.04026186466217041
iteration 193, loss = 0.04224356636404991
iteration 194, loss = 0.19822853803634644
iteration 195, loss = 0.07677110284566879
iteration 196, loss = 0.47450780868530273
iteration 197, loss = 0.009722254239022732
iteration 198, loss = 0.2033482939004898
iteration 199, loss = 0.0032195216044783592
iteration 200, loss = 0.08712530136108398
iteration 201, loss = 0.11287164688110352
iteration 202, loss = 0.01938997022807598
iteration 203, loss = 0.12263435870409012
iteration 204, loss = 0.19282519817352295
iteration 205, loss = 0.03549022227525711
iteration 206, loss = 0.08297209441661835
iteration 207, loss = 0.18216216564178467
iteration 208, loss = 0.17732007801532745
iteration 209, loss = 0.31329241394996643
iteration 210, loss = 0.06378806382417679
iteration 211, loss = 0.021844962611794472
iteration 212, loss = 0.07290676981210709
iteration 213, loss = 0.15903064608573914
iteration 214, loss = 0.10841047763824463
iteration 215, loss = 0.13147945702075958
iteration 216, loss = 0.022436296567320824
iteration 217, loss = 0.05892647057771683
iteration 218, loss = 0.17619118094444275
iteration 219, loss = 0.005389956291764975
iteration 220, loss = 0.20072989165782928
iteration 221, loss = 0.23737165331840515
iteration 222, loss = 0.036260101944208145
iteration 223, loss = 0.08098418265581131
iteration 224, loss = 0.15931005775928497
iteration 225, loss = 0.2519756853580475
iteration 226, loss = 0.049414172768592834
iteration 227, loss = 0.011722943745553493
iteration 228, loss = 0.014949768781661987
iteration 229, loss = 0.16115836799144745
iteration 230, loss = 0.22096101939678192
iteration 231, loss = 0.15402910113334656
iteration 232, loss = 0.34154585003852844
iteration 233, loss = 0.14943116903305054
iteration 234, loss = 0.1125100776553154
iteration 235, loss = 0.10959763079881668
iteration 236, loss = 0.11926183849573135
iteration 237, loss = 0.077979676425457
iteration 238, loss = 0.10119319707155228
iteration 239, loss = 0.13709890842437744
iteration 240, loss = 0.018258340656757355
iteration 241, loss = 0.11289533972740173
iteration 242, loss = 0.160764679312706
iteration 243, loss = 0.11947067826986313
iteration 244, loss = 0.014415325596928596
iteration 245, loss = 0.2678675353527069
iteration 246, loss = 0.04517118260264397
iteration 247, loss = 0.15646415948867798
iteration 248, loss = 0.054775938391685486
iteration 249, loss = 0.11421939730644226
iteration 250, loss = 0.09893669188022614
iteration 251, loss = 0.2482239007949829
iteration 252, loss = 0.06075664982199669
iteration 253, loss = 0.06566393375396729
iteration 254, loss = 0.031149785965681076
iteration 255, loss = 0.011005799286067486
iteration 256, loss = 0.024419791996479034
iteration 257, loss = 0.19467492401599884
iteration 258, loss = 0.04978950321674347
iteration 259, loss = 0.16934359073638916
iteration 260, loss = 0.049203112721443176
iteration 261, loss = 0.028166618198156357
iteration 262, loss = 0.0706847757101059
iteration 263, loss = 0.08808999508619308
iteration 264, loss = 0.005532742012292147
iteration 265, loss = 0.14225462079048157
iteration 266, loss = 0.03991757333278656
iteration 267, loss = 0.03558530658483505
iteration 268, loss = 0.07713386416435242
iteration 269, loss = 0.06287037581205368
iteration 270, loss = 0.05913152918219566
iteration 271, loss = 0.04247589781880379
iteration 272, loss = 0.02048819325864315
iteration 273, loss = 0.026750128716230392
iteration 274, loss = 0.0015428571496158838
iteration 275, loss = 0.23080724477767944
iteration 276, loss = 0.22856871783733368
iteration 277, loss = 0.08106472343206406
iteration 278, loss = 0.1031351163983345
iteration 279, loss = 0.017176490277051926
iteration 280, loss = 0.19589605927467346
iteration 281, loss = 0.190940260887146
iteration 282, loss = 0.029719479382038116
iteration 283, loss = 0.00974724069237709
iteration 284, loss = 0.15225020051002502
iteration 285, loss = 0.32336902618408203
iteration 286, loss = 0.0988350659608841
iteration 287, loss = 0.034611061215400696
iteration 288, loss = 0.058981165289878845
iteration 289, loss = 0.07661610096693039
iteration 290, loss = 0.14690452814102173
iteration 291, loss = 0.2005643993616104
iteration 292, loss = 0.13808150589466095
iteration 293, loss = 0.03676193952560425
iteration 294, loss = 0.034435153007507324
iteration 295, loss = 0.03419332206249237
iteration 296, loss = 0.05765677988529205
iteration 297, loss = 0.21892191469669342
iteration 298, loss = 0.10318730771541595
iteration 299, loss = 0.1305549144744873
iteration 0, loss = 0.1797260344028473
iteration 1, loss = 0.1839500069618225
iteration 2, loss = 0.03495560958981514
iteration 3, loss = 0.040151771157979965
iteration 4, loss = 0.4884335994720459
iteration 5, loss = 0.18318259716033936
iteration 6, loss = 0.31037816405296326
iteration 7, loss = 0.09267403930425644
iteration 8, loss = 0.0598657988011837
iteration 9, loss = 0.15045660734176636
iteration 10, loss = 0.19755977392196655
iteration 11, loss = 0.10002228617668152
iteration 12, loss = 0.0446639209985733
iteration 13, loss = 0.07535308599472046
iteration 14, loss = 0.24321988224983215
iteration 15, loss = 0.23744936287403107
iteration 16, loss = 0.13048572838306427
iteration 17, loss = 0.007591212168335915
iteration 18, loss = 0.15686862170696259
iteration 19, loss = 0.23773720860481262
iteration 20, loss = 0.2848942279815674
iteration 21, loss = 0.02696048654615879
iteration 22, loss = 0.128790944814682
iteration 23, loss = 0.10297732055187225
iteration 24, loss = 0.0322507843375206
iteration 25, loss = 0.25093039870262146
iteration 26, loss = 0.16552981734275818
iteration 27, loss = 0.12490525096654892
iteration 28, loss = 0.18061195313930511
iteration 29, loss = 0.1260863095521927
iteration 30, loss = 0.1043231263756752
iteration 31, loss = 0.2633640170097351
iteration 32, loss = 0.05131492391228676
iteration 33, loss = 0.16591399908065796
iteration 34, loss = 0.08839822560548782
iteration 35, loss = 0.2287730574607849
iteration 36, loss = 0.1551486849784851
iteration 37, loss = 0.060257039964199066
iteration 38, loss = 0.013522389344871044
iteration 39, loss = 0.04638795554637909
iteration 40, loss = 0.1450463831424713
iteration 41, loss = 0.17875660955905914
iteration 42, loss = 0.07450582832098007
iteration 43, loss = 0.09611965715885162
iteration 44, loss = 0.1170976385474205
iteration 45, loss = 0.1941639631986618
iteration 46, loss = 0.044094592332839966
iteration 47, loss = 0.14954443275928497
iteration 48, loss = 0.05693921819329262
iteration 49, loss = 0.04024014249444008
iteration 50, loss = 0.07288846373558044
iteration 51, loss = 0.05280886963009834
iteration 52, loss = 0.048102714121341705
iteration 53, loss = 0.2023313045501709
iteration 54, loss = 0.03023059293627739
iteration 55, loss = 0.1409863382577896
iteration 56, loss = 0.20117050409317017
iteration 57, loss = 0.038735680282115936
iteration 58, loss = 0.036292821168899536
iteration 59, loss = 0.1357334554195404
iteration 60, loss = 0.06189212575554848
iteration 61, loss = 0.06203791871666908
iteration 62, loss = 0.014646712690591812
iteration 63, loss = 0.13977234065532684
iteration 64, loss = 0.17208142578601837
iteration 65, loss = 0.04615884646773338
iteration 66, loss = 0.020004281774163246
iteration 67, loss = 0.02799735777080059
iteration 68, loss = 0.011249070055782795
iteration 69, loss = 0.15386532247066498
iteration 70, loss = 0.11866677552461624
iteration 71, loss = 0.011244706809520721
iteration 72, loss = 0.021369673311710358
iteration 73, loss = 0.030190180987119675
iteration 74, loss = 0.02200894057750702
iteration 75, loss = 0.028054917231202126
iteration 76, loss = 0.02001327835023403
iteration 77, loss = 0.05138605833053589
iteration 78, loss = 0.1106235459446907
iteration 79, loss = 0.12573951482772827
iteration 80, loss = 0.073598712682724
iteration 81, loss = 0.08775326609611511
iteration 82, loss = 0.032583918422460556
iteration 83, loss = 0.03920004144310951
iteration 84, loss = 0.06963956356048584
iteration 85, loss = 0.08191180974245071
iteration 86, loss = 0.04731077700853348
iteration 87, loss = 0.2256629765033722
iteration 88, loss = 0.14541363716125488
iteration 89, loss = 0.050121162086725235
iteration 90, loss = 0.016904881224036217
iteration 91, loss = 0.02422904409468174
iteration 92, loss = 0.05581147223711014
iteration 93, loss = 0.028910411521792412
iteration 94, loss = 0.05320628359913826
iteration 95, loss = 0.036129143089056015
iteration 96, loss = 0.0907040610909462
iteration 97, loss = 0.058857277035713196
iteration 98, loss = 0.05963544175028801
iteration 99, loss = 0.08004893362522125
iteration 100, loss = 0.04193662479519844
iteration 101, loss = 0.030655067414045334
iteration 102, loss = 0.04107220843434334
iteration 103, loss = 0.11657287925481796
iteration 104, loss = 0.10627749562263489
iteration 105, loss = 0.12815795838832855
iteration 106, loss = 0.09197596460580826
iteration 107, loss = 0.10588490962982178
iteration 108, loss = 0.050304729491472244
iteration 109, loss = 0.11520948261022568
iteration 110, loss = 0.0920049399137497
iteration 111, loss = 0.08122000098228455
iteration 112, loss = 0.009370923042297363
iteration 113, loss = 0.0981738269329071
iteration 114, loss = 0.04559171944856644
iteration 115, loss = 0.09961234778165817
iteration 116, loss = 0.05251513794064522
iteration 117, loss = 0.12063419073820114
iteration 118, loss = 0.06183609366416931
iteration 119, loss = 0.015812817960977554
iteration 120, loss = 0.04176706448197365
iteration 121, loss = 0.33625224232673645
iteration 122, loss = 0.19041863083839417
iteration 123, loss = 0.1297752410173416
iteration 124, loss = 0.007244681473821402
iteration 125, loss = 0.18355362117290497
iteration 126, loss = 0.08994974195957184
iteration 127, loss = 0.14224134385585785
iteration 128, loss = 0.015873469412326813
iteration 129, loss = 0.15773960947990417
iteration 130, loss = 0.014475783333182335
iteration 131, loss = 0.08306457102298737
iteration 132, loss = 0.1654888093471527
iteration 133, loss = 0.1747228354215622
iteration 134, loss = 0.30844104290008545
iteration 135, loss = 0.2408124953508377
iteration 136, loss = 0.4560326933860779
iteration 137, loss = 0.14694422483444214
iteration 138, loss = 0.008054535835981369
iteration 139, loss = 0.1801978200674057
iteration 140, loss = 0.19864434003829956
iteration 141, loss = 0.2991147041320801
iteration 142, loss = 0.28222858905792236
iteration 143, loss = 0.21536870300769806
iteration 144, loss = 0.03760797530412674
iteration 145, loss = 0.32501500844955444
iteration 146, loss = 0.15735554695129395
iteration 147, loss = 0.1856163889169693
iteration 148, loss = 0.20318087935447693
iteration 149, loss = 0.28247323632240295
iteration 150, loss = 0.19602492451667786
iteration 151, loss = 0.045193273574113846
iteration 152, loss = 0.04240038990974426
iteration 153, loss = 0.04771854355931282
iteration 154, loss = 0.30747586488723755
iteration 155, loss = 0.21517783403396606
iteration 156, loss = 0.2243351936340332
iteration 157, loss = 0.2846815288066864
iteration 158, loss = 0.20144647359848022
iteration 159, loss = 0.11773919314146042
iteration 160, loss = 0.1358516812324524
iteration 161, loss = 0.15718895196914673
iteration 162, loss = 0.011042903177440166
iteration 163, loss = 0.006301775574684143
iteration 164, loss = 0.3211233913898468
iteration 165, loss = 0.26718857884407043
iteration 166, loss = 0.43299487233161926
iteration 167, loss = 0.09559330344200134
iteration 168, loss = 0.1691366732120514
iteration 169, loss = 0.03388459235429764
iteration 170, loss = 0.11603511869907379
iteration 171, loss = 0.20017467439174652
iteration 172, loss = 0.04609999060630798
iteration 173, loss = 0.08048184961080551
iteration 174, loss = 0.3771754801273346
iteration 175, loss = 0.08893610537052155
iteration 176, loss = 0.09568879008293152
iteration 177, loss = 0.21763314306735992
iteration 178, loss = 0.20530809462070465
iteration 179, loss = 0.13482706248760223
iteration 180, loss = 0.275064617395401
iteration 181, loss = 0.06765399873256683
iteration 182, loss = 0.01385838445276022
iteration 183, loss = 0.1649342030286789
iteration 184, loss = 0.20328180491924286
iteration 185, loss = 0.12031514942646027
iteration 186, loss = 0.15016356110572815
iteration 187, loss = 0.14649598300457
iteration 188, loss = 0.18116596341133118
iteration 189, loss = 0.24665097892284393
iteration 190, loss = 0.06762557476758957
iteration 191, loss = 0.251148521900177
iteration 192, loss = 0.12768810987472534
iteration 193, loss = 0.039735857397317886
iteration 194, loss = 0.04130005091428757
iteration 195, loss = 0.0588238462805748
iteration 196, loss = 0.08804832398891449
iteration 197, loss = 0.10099409520626068
iteration 198, loss = 0.0321037620306015
iteration 199, loss = 0.33268922567367554
iteration 200, loss = 0.08866328001022339
iteration 201, loss = 0.027124077081680298
iteration 202, loss = 0.06965198367834091
iteration 203, loss = 0.20931535959243774
iteration 204, loss = 0.06457044184207916
iteration 205, loss = 0.010583455674350262
iteration 206, loss = 0.0922507494688034
iteration 207, loss = 0.12396736443042755
iteration 208, loss = 0.18306055665016174
iteration 209, loss = 0.05347857624292374
iteration 210, loss = 0.09550990164279938
iteration 211, loss = 0.060650087893009186
iteration 212, loss = 0.0925959050655365
iteration 213, loss = 0.11961765587329865
iteration 214, loss = 0.009049370884895325
iteration 215, loss = 0.027609314769506454
iteration 216, loss = 0.06713149696588516
iteration 217, loss = 0.18912315368652344
iteration 218, loss = 0.003049420192837715
iteration 219, loss = 0.2134370356798172
iteration 220, loss = 0.17865830659866333
iteration 221, loss = 0.09162187576293945
iteration 222, loss = 0.09577462822198868
iteration 223, loss = 0.14191555976867676
iteration 224, loss = 0.13549324870109558
iteration 225, loss = 0.05006811022758484
iteration 226, loss = 0.1412872076034546
iteration 227, loss = 0.041085802018642426
iteration 228, loss = 0.040649957954883575
iteration 229, loss = 0.032331373542547226
iteration 230, loss = 0.10630042850971222
iteration 231, loss = 0.05858081579208374
iteration 232, loss = 0.015387480147182941
iteration 233, loss = 0.11014796048402786
iteration 234, loss = 0.01638791710138321
iteration 235, loss = 0.10594566911458969
iteration 236, loss = 0.03728150576353073
iteration 237, loss = 0.107193723320961
iteration 238, loss = 0.0393669418990612
iteration 239, loss = 0.05662981793284416
iteration 240, loss = 0.051182910799980164
iteration 241, loss = 0.01683303341269493
iteration 242, loss = 0.17912715673446655
iteration 243, loss = 0.07842449098825455
iteration 244, loss = 0.054311178624629974
iteration 245, loss = 0.029518328607082367
iteration 246, loss = 0.028181694447994232
iteration 247, loss = 0.02610350400209427
iteration 248, loss = 0.044398583471775055
iteration 249, loss = 0.04514864832162857
iteration 250, loss = 0.05336602032184601
iteration 251, loss = 0.06938589364290237
iteration 252, loss = 0.01738186739385128
iteration 253, loss = 0.03134574368596077
iteration 254, loss = 0.03302467241883278
iteration 255, loss = 0.16978880763053894
iteration 256, loss = 0.004202760756015778
iteration 257, loss = 0.08346118032932281
iteration 258, loss = 0.10705146193504333
iteration 259, loss = 0.04455239698290825
iteration 260, loss = 0.2215799242258072
iteration 261, loss = 0.07530343532562256
iteration 262, loss = 0.10578538477420807
iteration 263, loss = 0.004508081823587418
iteration 264, loss = 0.006670753005892038
iteration 265, loss = 0.27554482221603394
iteration 266, loss = 0.30522531270980835
iteration 267, loss = 0.4328632652759552
iteration 268, loss = 0.10266447067260742
iteration 269, loss = 0.4517974853515625
iteration 270, loss = 0.10523012280464172
iteration 271, loss = 0.04900681599974632
iteration 272, loss = 0.1368671953678131
iteration 273, loss = 0.5150877237319946
iteration 274, loss = 0.486258864402771
iteration 275, loss = 0.18503886461257935
iteration 276, loss = 0.06904499232769012
iteration 277, loss = 0.5725677609443665
iteration 278, loss = 0.10876400768756866
iteration 279, loss = 0.31336385011672974
iteration 280, loss = 0.33466270565986633
iteration 281, loss = 0.45210421085357666
iteration 282, loss = 0.05585823208093643
iteration 283, loss = 0.13267485797405243
iteration 284, loss = 0.21918579936027527
iteration 285, loss = 0.13516351580619812
iteration 286, loss = 0.21449242532253265
iteration 287, loss = 0.019412223249673843
iteration 288, loss = 0.21618257462978363
iteration 289, loss = 0.08577467501163483
iteration 290, loss = 0.020668335258960724
iteration 291, loss = 0.010004639625549316
iteration 292, loss = 0.11752238869667053
iteration 293, loss = 0.26704996824264526
iteration 294, loss = 0.17739026248455048
iteration 295, loss = 0.040170520544052124
iteration 296, loss = 0.08470054715871811
iteration 297, loss = 0.03909291326999664
iteration 298, loss = 0.08593759685754776
iteration 299, loss = 0.0728849247097969
iteration 0, loss = 0.13397085666656494
iteration 1, loss = 0.1808064728975296
iteration 2, loss = 0.041198521852493286
iteration 3, loss = 0.14503829181194305
iteration 4, loss = 0.06820603460073471
iteration 5, loss = 0.10124220699071884
iteration 6, loss = 0.01531874481588602
iteration 7, loss = 0.11412292718887329
iteration 8, loss = 0.024518553167581558
iteration 9, loss = 0.06987889856100082
iteration 10, loss = 0.18022146821022034
iteration 11, loss = 0.05142523720860481
iteration 12, loss = 0.13849306106567383
iteration 13, loss = 0.11058040708303452
iteration 14, loss = 0.07743671536445618
iteration 15, loss = 0.24411369860172272
iteration 16, loss = 0.12525002658367157
iteration 17, loss = 0.14213606715202332
iteration 18, loss = 0.03876331448554993
iteration 19, loss = 0.0947679728269577
iteration 20, loss = 0.09975200891494751
iteration 21, loss = 0.12485339492559433
iteration 22, loss = 0.05479305237531662
iteration 23, loss = 0.258529931306839
iteration 24, loss = 0.017987189814448357
iteration 25, loss = 0.09288567304611206
iteration 26, loss = 0.1093711256980896
iteration 27, loss = 0.1028977707028389
iteration 28, loss = 0.0914602279663086
iteration 29, loss = 0.019198782742023468
iteration 30, loss = 0.19374912977218628
iteration 31, loss = 0.0012500035809352994
iteration 32, loss = 0.030040426179766655
iteration 33, loss = 0.07199516892433167
iteration 34, loss = 0.0815998837351799
iteration 35, loss = 0.14799289405345917
iteration 36, loss = 0.3269793689250946
iteration 37, loss = 0.13459397852420807
iteration 38, loss = 0.08592676371335983
iteration 39, loss = 0.046676069498062134
iteration 40, loss = 0.08398802578449249
iteration 41, loss = 0.15917368233203888
iteration 42, loss = 0.1453695148229599
iteration 43, loss = 0.11152728646993637
iteration 44, loss = 0.0922192633152008
iteration 45, loss = 0.10026202350854874
iteration 46, loss = 0.08597563207149506
iteration 47, loss = 0.2059793472290039
iteration 48, loss = 0.07881105691194534
iteration 49, loss = 0.03358614444732666
iteration 50, loss = 0.1741674244403839
iteration 51, loss = 0.07777833193540573
iteration 52, loss = 0.04757148027420044
iteration 53, loss = 0.044419094920158386
iteration 54, loss = 0.07308713346719742
iteration 55, loss = 0.06784490495920181
iteration 56, loss = 0.060645028948783875
iteration 57, loss = 0.04319797828793526
iteration 58, loss = 0.15298208594322205
iteration 59, loss = 0.08192675560712814
iteration 60, loss = 0.060812946408987045
iteration 61, loss = 0.025854364037513733
iteration 62, loss = 0.00977236870676279
iteration 63, loss = 0.053496696054935455
iteration 64, loss = 0.1321098953485489
iteration 65, loss = 0.20495468378067017
iteration 66, loss = 0.008068333379924297
iteration 67, loss = 0.0587649904191494
iteration 68, loss = 0.020445769652724266
iteration 69, loss = 0.0119975870475173
iteration 70, loss = 0.11017406731843948
iteration 71, loss = 0.07068988680839539
iteration 72, loss = 0.18458016216754913
iteration 73, loss = 0.1926249861717224
iteration 74, loss = 0.04589060693979263
iteration 75, loss = 0.016948532313108444
iteration 76, loss = 0.20562461018562317
iteration 77, loss = 0.11658089607954025
iteration 78, loss = 0.06796933710575104
iteration 79, loss = 0.08238786458969116
iteration 80, loss = 0.021940257400274277
iteration 81, loss = 0.09038149565458298
iteration 82, loss = 0.025613920763134956
iteration 83, loss = 0.01088010799139738
iteration 84, loss = 0.3618689775466919
iteration 85, loss = 0.4136155843734741
iteration 86, loss = 0.3162900507450104
iteration 87, loss = 0.27207526564598083
iteration 88, loss = 0.059027571231126785
iteration 89, loss = 0.37256667017936707
iteration 90, loss = 0.40057116746902466
iteration 91, loss = 0.22730165719985962
iteration 92, loss = 0.23908674716949463
iteration 93, loss = 0.10798463970422745
iteration 94, loss = 0.22283902764320374
iteration 95, loss = 0.4468538463115692
iteration 96, loss = 0.0003370862687006593
iteration 97, loss = 0.14413242042064667
iteration 98, loss = 0.13192594051361084
iteration 99, loss = 0.16441960632801056
iteration 100, loss = 0.09024503082036972
iteration 101, loss = 0.08356591314077377
iteration 102, loss = 0.04130466654896736
iteration 103, loss = 0.04969492927193642
iteration 104, loss = 0.00517256511375308
iteration 105, loss = 0.16797640919685364
iteration 106, loss = 0.10713842511177063
iteration 107, loss = 0.05407695472240448
iteration 108, loss = 0.0727994292974472
iteration 109, loss = 0.15372677147388458
iteration 110, loss = 0.08964809775352478
iteration 111, loss = 0.029360456392169
iteration 112, loss = 0.004403287544846535
iteration 113, loss = 0.0031502339988946915
iteration 114, loss = 0.06993000954389572
iteration 115, loss = 0.45265883207321167
iteration 116, loss = 0.24308432638645172
iteration 117, loss = 0.34159737825393677
iteration 118, loss = 0.011178771033883095
iteration 119, loss = 0.0168305654078722
iteration 120, loss = 0.10612457245588303
iteration 121, loss = 0.1958673596382141
iteration 122, loss = 0.1882212907075882
iteration 123, loss = 0.05442030727863312
iteration 124, loss = 0.3281126022338867
iteration 125, loss = 0.02215794287621975
iteration 126, loss = 0.16829007863998413
iteration 127, loss = 0.054996296763420105
iteration 128, loss = 0.23174944519996643
iteration 129, loss = 0.07764036953449249
iteration 130, loss = 0.03176473081111908
iteration 131, loss = 0.12333333492279053
iteration 132, loss = 0.1814103126525879
iteration 133, loss = 0.003002945566549897
iteration 134, loss = 0.10023337602615356
iteration 135, loss = 0.00324226007796824
iteration 136, loss = 0.26849430799484253
iteration 137, loss = 0.05634324997663498
iteration 138, loss = 0.010958169586956501
iteration 139, loss = 0.1315532624721527
iteration 140, loss = 0.17739224433898926
iteration 141, loss = 0.06822874397039413
iteration 142, loss = 0.03474374860525131
iteration 143, loss = 0.006590279750525951
iteration 144, loss = 0.23033946752548218
iteration 145, loss = 0.17577502131462097
iteration 146, loss = 0.1366397589445114
iteration 147, loss = 0.21019166707992554
iteration 148, loss = 0.1352199912071228
iteration 149, loss = 0.064218670129776
iteration 150, loss = 0.37359169125556946
iteration 151, loss = 0.19385288655757904
iteration 152, loss = 0.1457814872264862
iteration 153, loss = 0.015392588451504707
iteration 154, loss = 0.0960099920630455
iteration 155, loss = 0.0026438795030117035
iteration 156, loss = 0.3614121079444885
iteration 157, loss = 0.14415591955184937
iteration 158, loss = 0.0905078873038292
iteration 159, loss = 0.1773703396320343
iteration 160, loss = 0.07538466155529022
iteration 161, loss = 0.04633674770593643
iteration 162, loss = 0.09000959992408752
iteration 163, loss = 0.14103475213050842
iteration 164, loss = 0.12301303446292877
iteration 165, loss = 0.05773667246103287
iteration 166, loss = 0.0362265519797802
iteration 167, loss = 0.12276101857423782
iteration 168, loss = 0.0011413315078243613
iteration 169, loss = 0.08343192934989929
iteration 170, loss = 0.0035377731546759605
iteration 171, loss = 0.19060704112052917
iteration 172, loss = 0.045848146080970764
iteration 173, loss = 0.03681530803442001
iteration 174, loss = 0.012015148997306824
iteration 175, loss = 0.15344779193401337
iteration 176, loss = 0.12110963463783264
iteration 177, loss = 0.018516171723604202
iteration 178, loss = 0.3770430088043213
iteration 179, loss = 0.18444302678108215
iteration 180, loss = 0.010698237456381321
iteration 181, loss = 0.1757698357105255
iteration 182, loss = 0.014659817330539227
iteration 183, loss = 0.04579828679561615
iteration 184, loss = 0.08413678407669067
iteration 185, loss = 0.04726969078183174
iteration 186, loss = 0.06859724968671799
iteration 187, loss = 0.10275058448314667
iteration 188, loss = 0.17931482195854187
iteration 189, loss = 0.20096369087696075
iteration 190, loss = 0.012809554114937782
iteration 191, loss = 0.1509130895137787
iteration 192, loss = 0.017581472173333168
iteration 193, loss = 0.05078335106372833
iteration 194, loss = 0.21579338610172272
iteration 195, loss = 0.019805165007710457
iteration 196, loss = 0.07889845222234726
iteration 197, loss = 0.06673101335763931
iteration 198, loss = 0.05296805500984192
iteration 199, loss = 0.11613602936267853
iteration 200, loss = 0.03530238941311836
iteration 201, loss = 0.11343447118997574
iteration 202, loss = 0.046976737678050995
iteration 203, loss = 0.008722104132175446
iteration 204, loss = 0.026466626673936844
iteration 205, loss = 0.030042819678783417
iteration 206, loss = 0.18450573086738586
iteration 207, loss = 0.011408774182200432
iteration 208, loss = 0.04659149795770645
iteration 209, loss = 0.045387715101242065
iteration 210, loss = 0.046298980712890625
iteration 211, loss = 0.12073531001806259
iteration 212, loss = 0.05514921247959137
iteration 213, loss = 0.14775487780570984
iteration 214, loss = 0.13976413011550903
iteration 215, loss = 0.2063441425561905
iteration 216, loss = 0.007240196689963341
iteration 217, loss = 0.03435166925191879
iteration 218, loss = 0.013718212023377419
iteration 219, loss = 0.008100708946585655
iteration 220, loss = 0.11139604449272156
iteration 221, loss = 0.04296485334634781
iteration 222, loss = 0.3274995684623718
iteration 223, loss = 0.06473976373672485
iteration 224, loss = 0.18897049129009247
iteration 225, loss = 0.14437884092330933
iteration 226, loss = 0.09639544785022736
iteration 227, loss = 0.029409822076559067
iteration 228, loss = 0.04665602371096611
iteration 229, loss = 0.11565729230642319
iteration 230, loss = 0.021634887903928757
iteration 231, loss = 0.32645928859710693
iteration 232, loss = 0.0029135909862816334
iteration 233, loss = 0.2246951013803482
iteration 234, loss = 0.09822529554367065
iteration 235, loss = 0.13377246260643005
iteration 236, loss = 0.040074434131383896
iteration 237, loss = 0.12197085469961166
iteration 238, loss = 0.030413860455155373
iteration 239, loss = 0.030713865533471107
iteration 240, loss = 0.008042369037866592
iteration 241, loss = 0.16615627706050873
iteration 242, loss = 0.18303751945495605
iteration 243, loss = 0.19093436002731323
iteration 244, loss = 0.010746568441390991
iteration 245, loss = 0.02885468304157257
iteration 246, loss = 0.01598568819463253
iteration 247, loss = 0.24825115501880646
iteration 248, loss = 0.04998006671667099
iteration 249, loss = 0.039948392659425735
iteration 250, loss = 0.045621588826179504
iteration 251, loss = 0.017878100275993347
iteration 252, loss = 0.10004936158657074
iteration 253, loss = 0.16746969521045685
iteration 254, loss = 0.010631228797137737
iteration 255, loss = 0.0356018953025341
iteration 256, loss = 0.0476507730782032
iteration 257, loss = 0.08863577246665955
iteration 258, loss = 0.12838298082351685
iteration 259, loss = 0.07905817776918411
iteration 260, loss = 0.050029102712869644
iteration 261, loss = 0.044674165546894073
iteration 262, loss = 0.1711859107017517
iteration 263, loss = 0.00569018954411149
iteration 264, loss = 0.03125431016087532
iteration 265, loss = 0.037728045135736465
iteration 266, loss = 0.014669888652861118
iteration 267, loss = 0.12732425332069397
iteration 268, loss = 0.08568507432937622
iteration 269, loss = 0.02471458725631237
iteration 270, loss = 0.15540169179439545
iteration 271, loss = 0.04718630015850067
iteration 272, loss = 0.0789794772863388
iteration 273, loss = 0.06224896386265755
iteration 274, loss = 0.009769827127456665
iteration 275, loss = 0.030301429331302643
iteration 276, loss = 0.02975769340991974
iteration 277, loss = 0.18243613839149475
iteration 278, loss = 0.06692328304052353
iteration 279, loss = 0.0029836504254490137
iteration 280, loss = 0.037659723311662674
iteration 281, loss = 0.03811982646584511
iteration 282, loss = 0.007995061576366425
iteration 283, loss = 0.0722600445151329
iteration 284, loss = 0.057879626750946045
iteration 285, loss = 0.12983804941177368
iteration 286, loss = 0.06550123542547226
iteration 287, loss = 0.028054066002368927
iteration 288, loss = 0.004393634386360645
iteration 289, loss = 0.06608694791793823
iteration 290, loss = 0.1009308397769928
iteration 291, loss = 0.12670686841011047
iteration 292, loss = 0.19343245029449463
iteration 293, loss = 0.08859211206436157
iteration 294, loss = 0.11441513895988464
iteration 295, loss = 0.01805809698998928
iteration 296, loss = 0.011369684711098671
iteration 297, loss = 0.08406063169240952
iteration 298, loss = 0.07727333158254623
iteration 299, loss = 0.018947940319776535
iteration 0, loss = 0.12608766555786133
iteration 1, loss = 0.030255788937211037
iteration 2, loss = 0.10403379052877426
iteration 3, loss = 0.051311783492565155
iteration 4, loss = 0.014304637908935547
iteration 5, loss = 0.08311145752668381
iteration 6, loss = 0.06049124896526337
iteration 7, loss = 0.1343529373407364
iteration 8, loss = 0.04679844155907631
iteration 9, loss = 0.03787795454263687
iteration 10, loss = 0.02624819241464138
iteration 11, loss = 0.057168055325746536
iteration 12, loss = 0.026503009721636772
iteration 13, loss = 0.12411725521087646
iteration 14, loss = 0.15925686061382294
iteration 15, loss = 0.048749975860118866
iteration 16, loss = 0.14152923226356506
iteration 17, loss = 0.03832552582025528
iteration 18, loss = 0.09285509586334229
iteration 19, loss = 0.04738112539052963
iteration 20, loss = 0.13997291028499603
iteration 21, loss = 0.12282612174749374
iteration 22, loss = 0.07909774780273438
iteration 23, loss = 0.07100506871938705
iteration 24, loss = 0.1921057254076004
iteration 25, loss = 0.18672437965869904
iteration 26, loss = 0.03910437226295471
iteration 27, loss = 0.187820166349411
iteration 28, loss = 0.09965544939041138
iteration 29, loss = 0.13601161539554596
iteration 30, loss = 0.1326354444026947
iteration 31, loss = 0.49696430563926697
iteration 32, loss = 0.01326717622578144
iteration 33, loss = 0.03976857289671898
iteration 34, loss = 0.17079941928386688
iteration 35, loss = 0.19010624289512634
iteration 36, loss = 0.0430244505405426
iteration 37, loss = 0.029752463102340698
iteration 38, loss = 0.005877952557057142
iteration 39, loss = 0.047806642949581146
iteration 40, loss = 0.010030006989836693
iteration 41, loss = 0.011219003237783909
iteration 42, loss = 0.06235212832689285
iteration 43, loss = 0.07165064662694931
iteration 44, loss = 0.1897473782300949
iteration 45, loss = 0.07421794533729553
iteration 46, loss = 0.14051076769828796
iteration 47, loss = 0.05877393111586571
iteration 48, loss = 0.14433832466602325
iteration 49, loss = 0.00855824165046215
iteration 50, loss = 0.0047068349085748196
iteration 51, loss = 0.20776119828224182
iteration 52, loss = 0.021486839279532433
iteration 53, loss = 0.18058691918849945
iteration 54, loss = 0.8304942846298218
iteration 55, loss = 0.24700631201267242
iteration 56, loss = 0.11017996072769165
iteration 57, loss = 0.26636195182800293
iteration 58, loss = 0.07249442487955093
iteration 59, loss = 0.3465360403060913
iteration 60, loss = 0.5571346282958984
iteration 61, loss = 0.5130252838134766
iteration 62, loss = 0.23437631130218506
iteration 63, loss = 0.04283248260617256
iteration 64, loss = 0.004077862482517958
iteration 65, loss = 0.6193364262580872
iteration 66, loss = 0.4589715301990509
iteration 67, loss = 0.4999275803565979
iteration 68, loss = 0.2347291111946106
iteration 69, loss = 0.2242272049188614
iteration 70, loss = 0.29763826727867126
iteration 71, loss = 0.3038833737373352
iteration 72, loss = 0.029732925817370415
iteration 73, loss = 0.23506173491477966
iteration 74, loss = 0.22106681764125824
iteration 75, loss = 0.31881988048553467
iteration 76, loss = 0.10608193278312683
iteration 77, loss = 0.06714221090078354
iteration 78, loss = 0.40478602051734924
iteration 79, loss = 0.21148519217967987
iteration 80, loss = 0.583716094493866
iteration 81, loss = 0.1500568687915802
iteration 82, loss = 0.014803784899413586
iteration 83, loss = 0.3801644444465637
iteration 84, loss = 0.19073818624019623
iteration 85, loss = 0.2035335898399353
iteration 86, loss = 0.538718044757843
iteration 87, loss = 0.6337182521820068
iteration 88, loss = 0.4732622802257538
iteration 89, loss = 0.43460434675216675
iteration 90, loss = 0.33938780426979065
iteration 91, loss = 0.10893205553293228
iteration 92, loss = 0.1390889585018158
iteration 93, loss = 0.09313225746154785
iteration 94, loss = 0.22367528080940247
iteration 95, loss = 0.38066384196281433
iteration 96, loss = 0.37501609325408936
iteration 97, loss = 0.14137966930866241
iteration 98, loss = 0.06146295368671417
iteration 99, loss = 0.17705541849136353
iteration 100, loss = 0.505807101726532
iteration 101, loss = 0.12894834578037262
iteration 102, loss = 0.27859750390052795
iteration 103, loss = 0.3534534275531769
iteration 104, loss = 0.02132819965481758
iteration 105, loss = 0.015744376927614212
iteration 106, loss = 0.09673324972391129
iteration 107, loss = 0.05776805058121681
iteration 108, loss = 0.15534529089927673
iteration 109, loss = 0.24783432483673096
iteration 110, loss = 0.0715562179684639
iteration 111, loss = 0.041227880865335464
iteration 112, loss = 0.09093162417411804
iteration 113, loss = 0.04824834689497948
iteration 114, loss = 0.39524340629577637
iteration 115, loss = 0.17376042902469635
iteration 116, loss = 0.14355623722076416
iteration 117, loss = 0.1243392825126648
iteration 118, loss = 0.01995854638516903
iteration 119, loss = 0.08271011710166931
iteration 120, loss = 0.2191796600818634
iteration 121, loss = 0.018598036840558052
iteration 122, loss = 0.04762321710586548
iteration 123, loss = 0.016002118587493896
iteration 124, loss = 0.0819232240319252
iteration 125, loss = 0.023636501282453537
iteration 126, loss = 0.004668960347771645
iteration 127, loss = 0.010376590304076672
iteration 128, loss = 0.04265648499131203
iteration 129, loss = 0.39743727445602417
iteration 130, loss = 0.0037105719093233347
iteration 131, loss = 0.1956372708082199
iteration 132, loss = 0.038101255893707275
iteration 133, loss = 0.0303102508187294
iteration 134, loss = 0.24296769499778748
iteration 135, loss = 0.14176952838897705
iteration 136, loss = 0.025889625772833824
iteration 137, loss = 0.1372535675764084
iteration 138, loss = 0.1800220012664795
iteration 139, loss = 0.2578714191913605
iteration 140, loss = 0.006839665584266186
iteration 141, loss = 0.29521968960762024
iteration 142, loss = 0.020273303613066673
iteration 143, loss = 0.0066672726534307
iteration 144, loss = 0.03731969743967056
iteration 145, loss = 0.15129506587982178
iteration 146, loss = 0.05108589679002762
iteration 147, loss = 0.1982494443655014
iteration 148, loss = 0.05421818047761917
iteration 149, loss = 0.055538617074489594
iteration 150, loss = 0.15284468233585358
iteration 151, loss = 0.16639257967472076
iteration 152, loss = 0.11396229267120361
iteration 153, loss = 0.008891069330275059
iteration 154, loss = 0.17450109124183655
iteration 155, loss = 0.11028006672859192
iteration 156, loss = 0.05484694242477417
iteration 157, loss = 0.09235648065805435
iteration 158, loss = 0.026161810383200645
iteration 159, loss = 0.12502075731754303
iteration 160, loss = 0.07362639158964157
iteration 161, loss = 0.013073072768747807
iteration 162, loss = 0.010997448116540909
iteration 163, loss = 0.07629992067813873
iteration 164, loss = 0.09566092491149902
iteration 165, loss = 0.0010105525143444538
iteration 166, loss = 0.1403762847185135
iteration 167, loss = 0.0186239592730999
iteration 168, loss = 0.5006707906723022
iteration 169, loss = 0.09338024258613586
iteration 170, loss = 0.07195673137903214
iteration 171, loss = 0.026028674095869064
iteration 172, loss = 0.2965375781059265
iteration 173, loss = 0.07439646124839783
iteration 174, loss = 0.1886969357728958
iteration 175, loss = 0.33471500873565674
iteration 176, loss = 0.24897146224975586
iteration 177, loss = 0.13563629984855652
iteration 178, loss = 0.1538301259279251
iteration 179, loss = 0.09997502714395523
iteration 180, loss = 0.06240902841091156
iteration 181, loss = 0.1506664752960205
iteration 182, loss = 0.15384292602539062
iteration 183, loss = 0.08074809610843658
iteration 184, loss = 0.017105747014284134
iteration 185, loss = 0.17566807568073273
iteration 186, loss = 0.01522158458828926
iteration 187, loss = 0.015453320927917957
iteration 188, loss = 0.07477760314941406
iteration 189, loss = 0.034309498965740204
iteration 190, loss = 0.03899795189499855
iteration 191, loss = 0.03877308592200279
iteration 192, loss = 0.02727542631328106
iteration 193, loss = 0.15222793817520142
iteration 194, loss = 0.025879131630063057
iteration 195, loss = 0.08776538819074631
iteration 196, loss = 0.029849156737327576
iteration 197, loss = 0.03183041512966156
iteration 198, loss = 0.013755920343101025
iteration 199, loss = 0.0099636260420084
iteration 200, loss = 0.21048033237457275
iteration 201, loss = 0.14165224134922028
iteration 202, loss = 0.15736541152000427
iteration 203, loss = 0.14424729347229004
iteration 204, loss = 0.06964816153049469
iteration 205, loss = 0.41012659668922424
iteration 206, loss = 0.4674014747142792
iteration 207, loss = 0.020297234877943993
iteration 208, loss = 0.01529976911842823
iteration 209, loss = 0.16976197063922882
iteration 210, loss = 0.47549229860305786
iteration 211, loss = 0.1580035537481308
iteration 212, loss = 0.12696847319602966
iteration 213, loss = 0.10026046633720398
iteration 214, loss = 0.07252611219882965
iteration 215, loss = 0.12496978044509888
iteration 216, loss = 0.17801950871944427
iteration 217, loss = 0.1872512400150299
iteration 218, loss = 0.17187713086605072
iteration 219, loss = 0.023605316877365112
iteration 220, loss = 0.34133732318878174
iteration 221, loss = 0.19022051990032196
iteration 222, loss = 0.19386433064937592
iteration 223, loss = 0.007385089062154293
iteration 224, loss = 0.37802302837371826
iteration 225, loss = 0.004728684667497873
iteration 226, loss = 0.026716629043221474
iteration 227, loss = 0.2610294818878174
iteration 228, loss = 0.0683007463812828
iteration 229, loss = 0.2749042212963104
iteration 230, loss = 0.1280445009469986
iteration 231, loss = 0.06743434071540833
iteration 232, loss = 0.007207870949059725
iteration 233, loss = 0.5068103671073914
iteration 234, loss = 0.4785565137863159
iteration 235, loss = 0.0015052113449200988
iteration 236, loss = 0.3944289982318878
iteration 237, loss = 0.4779830574989319
iteration 238, loss = 0.37553882598876953
iteration 239, loss = 0.023221293464303017
iteration 240, loss = 0.39142242074012756
iteration 241, loss = 0.464101642370224
iteration 242, loss = 0.22030816972255707
iteration 243, loss = 0.12853722274303436
iteration 244, loss = 0.6292169094085693
iteration 245, loss = 0.3964518904685974
iteration 246, loss = 0.14197281002998352
iteration 247, loss = 0.14225274324417114
iteration 248, loss = 0.2567015290260315
iteration 249, loss = 0.21499928832054138
iteration 250, loss = 0.1528630405664444
iteration 251, loss = 0.014375487342476845
iteration 252, loss = 0.20989462733268738
iteration 253, loss = 0.13476184010505676
iteration 254, loss = 0.07122965157032013
iteration 255, loss = 0.11139513552188873
iteration 256, loss = 0.0630541741847992
iteration 257, loss = 0.005400928668677807
iteration 258, loss = 0.12522295117378235
iteration 259, loss = 0.08881378173828125
iteration 260, loss = 0.17865107953548431
iteration 261, loss = 0.15482044219970703
iteration 262, loss = 0.1802259385585785
iteration 263, loss = 0.21190516650676727
iteration 264, loss = 0.2707439959049225
iteration 265, loss = 0.1853654980659485
iteration 266, loss = 0.0040918756276369095
iteration 267, loss = 0.06847680360078812
iteration 268, loss = 0.019222091883420944
iteration 269, loss = 0.42050692439079285
iteration 270, loss = 0.02789648063480854
iteration 271, loss = 0.05018603056669235
iteration 272, loss = 0.1731608659029007
iteration 273, loss = 0.04470720514655113
iteration 274, loss = 0.17442265152931213
iteration 275, loss = 0.00671335170045495
iteration 276, loss = 0.0034792579244822264
iteration 277, loss = 0.18023990094661713
iteration 278, loss = 0.07318145036697388
iteration 279, loss = 0.056744370609521866
iteration 280, loss = 0.02700486034154892
iteration 281, loss = 0.2240673303604126
iteration 282, loss = 0.2280227541923523
iteration 283, loss = 0.2517687976360321
iteration 284, loss = 0.19512207806110382
iteration 285, loss = 0.024683184921741486
iteration 286, loss = 0.16648368537425995
iteration 287, loss = 0.08253806829452515
iteration 288, loss = 0.042443156242370605
iteration 289, loss = 0.12275557965040207
iteration 290, loss = 0.10218209028244019
iteration 291, loss = 0.06742924451828003
iteration 292, loss = 0.22914241254329681
iteration 293, loss = 0.06302011758089066
iteration 294, loss = 0.23384208977222443
iteration 295, loss = 0.04365485906600952
iteration 296, loss = 0.005564373917877674
iteration 297, loss = 0.005555940326303244
iteration 298, loss = 0.2059677392244339
iteration 299, loss = 0.44089841842651367
iteration 0, loss = 0.00025179891963489354
iteration 1, loss = 0.5510473847389221
iteration 2, loss = 0.3867165148258209
iteration 3, loss = 0.2750663757324219
iteration 4, loss = 0.6088392734527588
iteration 5, loss = 0.006134591531008482
iteration 6, loss = 0.0062360214069485664
iteration 7, loss = 0.07746315002441406
iteration 8, loss = 0.13182009756565094
iteration 9, loss = 0.10528114438056946
iteration 10, loss = 0.20781177282333374
iteration 11, loss = 0.046784576028585434
iteration 12, loss = 0.10444805026054382
iteration 13, loss = 0.11146091669797897
iteration 14, loss = 0.019196581095457077
iteration 15, loss = 0.22503459453582764
iteration 16, loss = 0.09980537742376328
iteration 17, loss = 0.44963327050209045
iteration 18, loss = 0.017944006249308586
iteration 19, loss = 0.05374107509851456
iteration 20, loss = 0.11885972321033478
iteration 21, loss = 0.0922079086303711
iteration 22, loss = 0.08387593924999237
iteration 23, loss = 0.03939575329422951
iteration 24, loss = 0.020850451663136482
iteration 25, loss = 0.011330296285450459
iteration 26, loss = 0.002111169509589672
iteration 27, loss = 0.0038528279401361942
iteration 28, loss = 0.01107266079634428
iteration 29, loss = 0.0771932452917099
iteration 30, loss = 0.1765519678592682
iteration 31, loss = 0.080698162317276
iteration 32, loss = 0.0905396044254303
iteration 33, loss = 0.128654345870018
iteration 34, loss = 0.21922221779823303
iteration 35, loss = 0.14454923570156097
iteration 36, loss = 0.10476221889257431
iteration 37, loss = 0.09760167449712753
iteration 38, loss = 0.007718504406511784
iteration 39, loss = 0.21512477099895477
iteration 40, loss = 0.29160836338996887
iteration 41, loss = 0.05566985160112381
iteration 42, loss = 0.13984215259552002
iteration 43, loss = 0.17820322513580322
iteration 44, loss = 0.05882598087191582
iteration 45, loss = 0.048056092113256454
iteration 46, loss = 0.10044048726558685
iteration 47, loss = 0.05833820998668671
iteration 48, loss = 0.1305360496044159
iteration 49, loss = 0.0411977618932724
iteration 50, loss = 0.0035424716770648956
iteration 51, loss = 0.07974770665168762
iteration 52, loss = 0.10453888028860092
iteration 53, loss = 0.11967359483242035
iteration 54, loss = 0.10858149081468582
iteration 55, loss = 0.057418908923864365
iteration 56, loss = 0.026304731145501137
iteration 57, loss = 0.01552011538296938
iteration 58, loss = 0.0674908384680748
iteration 59, loss = 0.003992239944636822
iteration 60, loss = 0.04140232503414154
iteration 61, loss = 0.1572754979133606
iteration 62, loss = 0.16184841096401215
iteration 63, loss = 0.088260218501091
iteration 64, loss = 0.14404310286045074
iteration 65, loss = 0.16753916442394257
iteration 66, loss = 0.06833887100219727
iteration 67, loss = 0.1724846363067627
iteration 68, loss = 0.048324547708034515
iteration 69, loss = 0.03930937126278877
iteration 70, loss = 0.0539739653468132
iteration 71, loss = 0.06768905371427536
iteration 72, loss = 0.06420990824699402
iteration 73, loss = 0.02187025360763073
iteration 74, loss = 0.10041260719299316
iteration 75, loss = 0.0885864794254303
iteration 76, loss = 0.0528411865234375
iteration 77, loss = 0.07948888838291168
iteration 78, loss = 0.18645012378692627
iteration 79, loss = 0.10660891234874725
iteration 80, loss = 0.06710661202669144
iteration 81, loss = 0.1009301021695137
iteration 82, loss = 0.020924335345625877
iteration 83, loss = 0.08312508463859558
iteration 84, loss = 0.023072998970746994
iteration 85, loss = 0.17662599682807922
iteration 86, loss = 0.12106744945049286
iteration 87, loss = 0.12222108989953995
iteration 88, loss = 0.10023419559001923
iteration 89, loss = 0.04466243088245392
iteration 90, loss = 0.043495699763298035
iteration 91, loss = 0.11471179872751236
iteration 92, loss = 0.11450476944446564
iteration 93, loss = 0.055580273270606995
iteration 94, loss = 0.18754446506500244
iteration 95, loss = 0.0601731538772583
iteration 96, loss = 0.08846277743577957
iteration 97, loss = 0.1501615345478058
iteration 98, loss = 0.019900621846318245
iteration 99, loss = 0.16194291412830353
iteration 100, loss = 0.0727134719491005
iteration 101, loss = 0.05448537319898605
iteration 102, loss = 0.01846141926944256
iteration 103, loss = 0.09222462028265
iteration 104, loss = 0.009546257555484772
iteration 105, loss = 0.185392364859581
iteration 106, loss = 0.11343321949243546
iteration 107, loss = 0.1959584355354309
iteration 108, loss = 0.22302530705928802
iteration 109, loss = 0.03794001042842865
iteration 110, loss = 0.005731428973376751
iteration 111, loss = 0.0723249614238739
iteration 112, loss = 0.2080436795949936
iteration 113, loss = 0.12002015858888626
iteration 114, loss = 0.05302601307630539
iteration 115, loss = 0.003539272351190448
iteration 116, loss = 0.12586437165737152
iteration 117, loss = 0.019360870122909546
iteration 118, loss = 0.35768386721611023
iteration 119, loss = 0.4086616039276123
iteration 120, loss = 0.28534024953842163
iteration 121, loss = 0.029990920796990395
iteration 122, loss = 0.133455291390419
iteration 123, loss = 0.20277690887451172
iteration 124, loss = 0.237684428691864
iteration 125, loss = 0.012781865894794464
iteration 126, loss = 0.33610406517982483
iteration 127, loss = 0.36183270812034607
iteration 128, loss = 0.03134138882160187
iteration 129, loss = 0.16913114488124847
iteration 130, loss = 0.15524747967720032
iteration 131, loss = 0.1693408042192459
iteration 132, loss = 0.0011292940471321344
iteration 133, loss = 0.039255402982234955
iteration 134, loss = 0.0693567767739296
iteration 135, loss = 0.25293973088264465
iteration 136, loss = 0.1438317745923996
iteration 137, loss = 0.06293494999408722
iteration 138, loss = 0.09162505716085434
iteration 139, loss = 0.03472261130809784
iteration 140, loss = 0.05962587893009186
iteration 141, loss = 0.023997265845537186
iteration 142, loss = 0.1831350177526474
iteration 143, loss = 0.018353808671236038
iteration 144, loss = 0.02474401704967022
iteration 145, loss = 0.025653744116425514
iteration 146, loss = 0.03106275200843811
iteration 147, loss = 0.027863144874572754
iteration 148, loss = 0.0389062874019146
iteration 149, loss = 0.07085280120372772
iteration 150, loss = 0.02879983000457287
iteration 151, loss = 0.12216511368751526
iteration 152, loss = 0.009389557875692844
iteration 153, loss = 0.02578030154109001
iteration 154, loss = 0.016308095306158066
iteration 155, loss = 0.09098789095878601
iteration 156, loss = 0.08883979916572571
iteration 157, loss = 0.06473446637392044
iteration 158, loss = 0.04094184935092926
iteration 159, loss = 0.014423446729779243
iteration 160, loss = 0.03845246508717537
iteration 161, loss = 0.08008082956075668
iteration 162, loss = 0.13349634408950806
iteration 163, loss = 0.04735318943858147
iteration 164, loss = 0.09890557825565338
iteration 165, loss = 0.013104836456477642
iteration 166, loss = 0.04331519454717636
iteration 167, loss = 0.016717825084924698
iteration 168, loss = 0.08896061778068542
iteration 169, loss = 0.00956768449395895
iteration 170, loss = 0.05584751069545746
iteration 171, loss = 0.2552926540374756
iteration 172, loss = 0.020077338442206383
iteration 173, loss = 0.0512814037501812
iteration 174, loss = 0.1523924618959427
iteration 175, loss = 0.09458917379379272
iteration 176, loss = 0.030495937913656235
iteration 177, loss = 0.12322758883237839
iteration 178, loss = 0.31055504083633423
iteration 179, loss = 0.00829241517931223
iteration 180, loss = 0.09240391850471497
iteration 181, loss = 0.3366447389125824
iteration 182, loss = 0.03473446145653725
iteration 183, loss = 0.1183747947216034
iteration 184, loss = 0.12634724378585815
iteration 185, loss = 0.17372915148735046
iteration 186, loss = 0.16605767607688904
iteration 187, loss = 0.25217097997665405
iteration 188, loss = 0.03875259682536125
iteration 189, loss = 0.006959475576877594
iteration 190, loss = 0.07897929847240448
iteration 191, loss = 0.29860690236091614
iteration 192, loss = 0.0031047139782458544
iteration 193, loss = 0.028537452220916748
iteration 194, loss = 0.10461889207363129
iteration 195, loss = 0.05399882420897484
iteration 196, loss = 0.11133088171482086
iteration 197, loss = 0.05467759817838669
iteration 198, loss = 0.06233324110507965
iteration 199, loss = 0.0036252979189157486
iteration 200, loss = 0.16979669034481049
iteration 201, loss = 0.36689549684524536
iteration 202, loss = 0.1802011877298355
iteration 203, loss = 0.029678788036108017
iteration 204, loss = 0.11257563531398773
iteration 205, loss = 0.016822025179862976
iteration 206, loss = 0.061949968338012695
iteration 207, loss = 0.2061334103345871
iteration 208, loss = 0.004818222019821405
iteration 209, loss = 0.026796016842126846
iteration 210, loss = 0.14263665676116943
iteration 211, loss = 0.011447063647210598
iteration 212, loss = 0.0012075027916580439
iteration 213, loss = 0.12061343342065811
iteration 214, loss = 0.07715150713920593
iteration 215, loss = 0.061138417571783066
iteration 216, loss = 0.11064916849136353
iteration 217, loss = 0.17963257431983948
iteration 218, loss = 0.09320325404405594
iteration 219, loss = 0.13574345409870148
iteration 220, loss = 0.045408256351947784
iteration 221, loss = 0.3324718177318573
iteration 222, loss = 0.21306441724300385
iteration 223, loss = 0.00227938499301672
iteration 224, loss = 0.1240997165441513
iteration 225, loss = 0.4016362130641937
iteration 226, loss = 0.23333868384361267
iteration 227, loss = 0.1332220435142517
iteration 228, loss = 0.075098916888237
iteration 229, loss = 0.19500266015529633
iteration 230, loss = 0.29683858156204224
iteration 231, loss = 0.09878464788198471
iteration 232, loss = 0.054100487381219864
iteration 233, loss = 0.15127140283584595
iteration 234, loss = 0.14419792592525482
iteration 235, loss = 0.11277662217617035
iteration 236, loss = 0.1361711621284485
iteration 237, loss = 0.01872280426323414
iteration 238, loss = 0.562811017036438
iteration 239, loss = 0.23361296951770782
iteration 240, loss = 0.02661842666566372
iteration 241, loss = 0.21632611751556396
iteration 242, loss = 0.23585285246372223
iteration 243, loss = 0.4805629849433899
iteration 244, loss = 0.4933835566043854
iteration 245, loss = 0.237667053937912
iteration 246, loss = 0.4828769862651825
iteration 247, loss = 0.5037432312965393
iteration 248, loss = 0.4963333010673523
iteration 249, loss = 0.3017325699329376
iteration 250, loss = 0.7429741024971008
iteration 251, loss = 0.742493748664856
iteration 252, loss = 0.5808482766151428
iteration 253, loss = 0.4772248864173889
iteration 254, loss = 0.23157434165477753
iteration 255, loss = 0.46581369638442993
iteration 256, loss = 0.9482412934303284
iteration 257, loss = 0.48279160261154175
iteration 258, loss = 0.2941204309463501
iteration 259, loss = 0.8196143507957458
iteration 260, loss = 0.19755104184150696
iteration 261, loss = 0.0005862681427970529
iteration 262, loss = 0.008283773437142372
iteration 263, loss = 0.0838940367102623
iteration 264, loss = 0.17674574255943298
iteration 265, loss = 0.01726541854441166
iteration 266, loss = 0.3822857439517975
iteration 267, loss = 0.004509265534579754
iteration 268, loss = 0.023764949291944504
iteration 269, loss = 0.0904201790690422
iteration 270, loss = 0.04900062084197998
iteration 271, loss = 0.12917673587799072
iteration 272, loss = 0.38659659028053284
iteration 273, loss = 0.4410397410392761
iteration 274, loss = 0.014448737725615501
iteration 275, loss = 0.06021817401051521
iteration 276, loss = 0.013060840778052807
iteration 277, loss = 0.002755102002993226
iteration 278, loss = 0.008250085636973381
iteration 279, loss = 0.07656455785036087
iteration 280, loss = 0.1738957166671753
iteration 281, loss = 0.17643824219703674
iteration 282, loss = 0.04514780268073082
iteration 283, loss = 0.08251886814832687
iteration 284, loss = 0.07219972461462021
iteration 285, loss = 0.06840845197439194
iteration 286, loss = 0.10169211030006409
iteration 287, loss = 0.04016614705324173
iteration 288, loss = 0.07500822842121124
iteration 289, loss = 0.06229552626609802
iteration 290, loss = 0.2688657343387604
iteration 291, loss = 0.26742106676101685
iteration 292, loss = 0.16377075016498566
iteration 293, loss = 0.06457897275686264
iteration 294, loss = 0.30819833278656006
iteration 295, loss = 0.07051660120487213
iteration 296, loss = 0.14740072190761566
iteration 297, loss = 0.009820251725614071
iteration 298, loss = 0.0242585651576519
iteration 299, loss = 0.004211544990539551
iteration 0, loss = 0.4289070963859558
iteration 1, loss = 0.0250009223818779
iteration 2, loss = 0.2089206576347351
iteration 3, loss = 0.021534323692321777
iteration 4, loss = 0.030821574851870537
iteration 5, loss = 0.04549289494752884
iteration 6, loss = 0.24983082711696625
iteration 7, loss = 0.13809189200401306
iteration 8, loss = 0.2105570137500763
iteration 9, loss = 0.10060779750347137
iteration 10, loss = 0.09488627314567566
iteration 11, loss = 0.056172579526901245
iteration 12, loss = 0.035164158791303635
iteration 13, loss = 0.12162411212921143
iteration 14, loss = 0.3707665503025055
iteration 15, loss = 0.5028501152992249
iteration 16, loss = 0.06425292044878006
iteration 17, loss = 0.12933357059955597
iteration 18, loss = 0.08100699633359909
iteration 19, loss = 0.12085211277008057
iteration 20, loss = 0.08947431296110153
iteration 21, loss = 0.1885012537240982
iteration 22, loss = 0.15062139928340912
iteration 23, loss = 0.11015268415212631
iteration 24, loss = 0.16161487996578217
iteration 25, loss = 0.11089005321264267
iteration 26, loss = 0.10417802631855011
iteration 27, loss = 0.03181886672973633
iteration 28, loss = 0.1797582358121872
iteration 29, loss = 0.002788769081234932
iteration 30, loss = 0.00486644497141242
iteration 31, loss = 0.15831713378429413
iteration 32, loss = 0.13487288355827332
iteration 33, loss = 0.11811647564172745
iteration 34, loss = 0.0710296481847763
iteration 35, loss = 0.007145685143768787
iteration 36, loss = 0.0013199225068092346
iteration 37, loss = 0.23080207407474518
iteration 38, loss = 0.0004490226274356246
iteration 39, loss = 0.13729897141456604
iteration 40, loss = 0.2193797379732132
iteration 41, loss = 0.40106621384620667
iteration 42, loss = 0.24381646513938904
iteration 43, loss = 0.03689832240343094
iteration 44, loss = 0.23729567229747772
iteration 45, loss = 0.08295184373855591
iteration 46, loss = 0.04490118473768234
iteration 47, loss = 0.16704478859901428
iteration 48, loss = 0.044980913400650024
iteration 49, loss = 0.26803573966026306
iteration 50, loss = 0.2632841169834137
iteration 51, loss = 0.1779254674911499
iteration 52, loss = 0.22666709125041962
iteration 53, loss = 0.1236182227730751
iteration 54, loss = 0.004640375729650259
iteration 55, loss = 0.03924018144607544
iteration 56, loss = 0.0071397097781300545
iteration 57, loss = 0.0029321988113224506
iteration 58, loss = 0.008250120095908642
iteration 59, loss = 0.007467116229236126
iteration 60, loss = 0.017448417842388153
iteration 61, loss = 0.07349257171154022
iteration 62, loss = 0.01087147742509842
iteration 63, loss = 0.043037667870521545
iteration 64, loss = 0.19406180083751678
iteration 65, loss = 0.16210785508155823
iteration 66, loss = 0.08476649224758148
iteration 67, loss = 0.04474262893199921
iteration 68, loss = 0.05251622572541237
iteration 69, loss = 0.05471230298280716
iteration 70, loss = 0.0974029004573822
iteration 71, loss = 0.01719817705452442
iteration 72, loss = 0.0426611602306366
iteration 73, loss = 0.08129220455884933
iteration 74, loss = 0.06268668919801712
iteration 75, loss = 0.02408692240715027
iteration 76, loss = 0.017467722296714783
iteration 77, loss = 0.016963446512818336
iteration 78, loss = 0.19429023563861847
iteration 79, loss = 0.01925344578921795
iteration 80, loss = 0.16539035737514496
iteration 81, loss = 0.018148450180888176
iteration 82, loss = 0.07360082864761353
iteration 83, loss = 0.02206440083682537
iteration 84, loss = 0.17445828020572662
iteration 85, loss = 0.06279422342777252
iteration 86, loss = 0.03525924310088158
iteration 87, loss = 0.000979697797447443
iteration 88, loss = 0.002595966449007392
iteration 89, loss = 0.35368236899375916
iteration 90, loss = 0.28078123927116394
iteration 91, loss = 0.21884749829769135
iteration 92, loss = 0.004303360357880592
iteration 93, loss = 0.11262798309326172
iteration 94, loss = 0.19794271886348724
iteration 95, loss = 0.03258461877703667
iteration 96, loss = 0.03149210661649704
iteration 97, loss = 0.1613319367170334
iteration 98, loss = 0.021946309134364128
iteration 99, loss = 0.014093346893787384
iteration 100, loss = 0.008087019436061382
iteration 101, loss = 0.07769089937210083
iteration 102, loss = 0.15280775725841522
iteration 103, loss = 0.6017120480537415
iteration 104, loss = 0.27712249755859375
iteration 105, loss = 0.14167514443397522
iteration 106, loss = 0.12582442164421082
iteration 107, loss = 0.36130741238594055
iteration 108, loss = 0.5646488070487976
iteration 109, loss = 0.3060249984264374
iteration 110, loss = 0.06648840755224228
iteration 111, loss = 0.11278118193149567
iteration 112, loss = 0.1291191130876541
iteration 113, loss = 0.09924083203077316
iteration 114, loss = 0.0776984766125679
iteration 115, loss = 0.07083093374967575
iteration 116, loss = 0.09421202540397644
iteration 117, loss = 0.29982754588127136
iteration 118, loss = 0.2725679278373718
iteration 119, loss = 0.15994466841220856
iteration 120, loss = 0.1857023388147354
iteration 121, loss = 0.06167725473642349
iteration 122, loss = 0.142302006483078
iteration 123, loss = 0.19358888268470764
iteration 124, loss = 0.11305234581232071
iteration 125, loss = 0.03953292593359947
iteration 126, loss = 0.05572780966758728
iteration 127, loss = 0.06768962740898132
iteration 128, loss = 0.10009537637233734
iteration 129, loss = 0.061074286699295044
iteration 130, loss = 0.034594204276800156
iteration 131, loss = 0.017832372337579727
iteration 132, loss = 0.11765071004629135
iteration 133, loss = 0.006331476848572493
iteration 134, loss = 0.06926070898771286
iteration 135, loss = 0.11261503398418427
iteration 136, loss = 0.03934271261096001
iteration 137, loss = 0.014909295365214348
iteration 138, loss = 0.04717949405312538
iteration 139, loss = 0.11961294710636139
iteration 140, loss = 0.09792909771203995
iteration 141, loss = 0.11509425193071365
iteration 142, loss = 0.10088986903429031
iteration 143, loss = 0.08859774470329285
iteration 144, loss = 0.062409237027168274
iteration 145, loss = 0.24432781338691711
iteration 146, loss = 0.07832421362400055
iteration 147, loss = 0.0523243211209774
iteration 148, loss = 0.0929441973567009
iteration 149, loss = 0.08214687556028366
iteration 150, loss = 0.08050088584423065
iteration 151, loss = 0.20872634649276733
iteration 152, loss = 0.09339821338653564
iteration 153, loss = 0.007353814318776131
iteration 154, loss = 0.00392105570062995
iteration 155, loss = 0.03122764453291893
iteration 156, loss = 0.3957926630973816
iteration 157, loss = 0.14705143868923187
iteration 158, loss = 0.04126748442649841
iteration 159, loss = 0.03590504080057144
iteration 160, loss = 0.17896856367588043
iteration 161, loss = 0.286743700504303
iteration 162, loss = 0.12240186333656311
iteration 163, loss = 0.0661730095744133
iteration 164, loss = 0.01389765739440918
iteration 165, loss = 0.04298543184995651
iteration 166, loss = 0.25719770789146423
iteration 167, loss = 0.11128786206245422
iteration 168, loss = 0.0656280443072319
iteration 169, loss = 0.1271631270647049
iteration 170, loss = 0.2164565920829773
iteration 171, loss = 0.08216102421283722
iteration 172, loss = 0.007370626088231802
iteration 173, loss = 0.2627659738063812
iteration 174, loss = 0.03509173542261124
iteration 175, loss = 0.022473208606243134
iteration 176, loss = 0.23736871778964996
iteration 177, loss = 0.01911541074514389
iteration 178, loss = 0.00459258584305644
iteration 179, loss = 0.00763162225484848
iteration 180, loss = 0.03511691465973854
iteration 181, loss = 0.254336953163147
iteration 182, loss = 0.26640909910202026
iteration 183, loss = 0.011513004079461098
iteration 184, loss = 0.1341249644756317
iteration 185, loss = 0.022398967295885086
iteration 186, loss = 0.0019219747046008706
iteration 187, loss = 0.42219436168670654
iteration 188, loss = 0.0191107876598835
iteration 189, loss = 0.009840523824095726
iteration 190, loss = 0.004076616372913122
iteration 191, loss = 0.18345345556735992
iteration 192, loss = 0.06581715494394302
iteration 193, loss = 0.013099649921059608
iteration 194, loss = 0.06560514122247696
iteration 195, loss = 0.024877287447452545
iteration 196, loss = 0.010512196458876133
iteration 197, loss = 0.10733471810817719
iteration 198, loss = 0.09062972664833069
iteration 199, loss = 0.09693322330713272
iteration 200, loss = 0.02300555258989334
iteration 201, loss = 0.09473367035388947
iteration 202, loss = 0.07945527136325836
iteration 203, loss = 0.02505831979215145
iteration 204, loss = 0.003990446217358112
iteration 205, loss = 0.05700528621673584
iteration 206, loss = 0.14980372786521912
iteration 207, loss = 0.18258389830589294
iteration 208, loss = 0.15195977687835693
iteration 209, loss = 0.02644162066280842
iteration 210, loss = 0.10392271727323532
iteration 211, loss = 0.11061327159404755
iteration 212, loss = 0.15332654118537903
iteration 213, loss = 0.03333944454789162
iteration 214, loss = 0.05015677586197853
iteration 215, loss = 0.0480266772210598
iteration 216, loss = 0.3957279324531555
iteration 217, loss = 0.10828270018100739
iteration 218, loss = 0.007857447490096092
iteration 219, loss = 0.04669110104441643
iteration 220, loss = 0.13157273828983307
iteration 221, loss = 0.07061857730150223
iteration 222, loss = 0.17893943190574646
iteration 223, loss = 0.044032104313373566
iteration 224, loss = 0.031638938933610916
iteration 225, loss = 0.10778415948152542
iteration 226, loss = 0.028555233031511307
iteration 227, loss = 0.09787020087242126
iteration 228, loss = 0.15255886316299438
iteration 229, loss = 0.05791110545396805
iteration 230, loss = 0.05638466402888298
iteration 231, loss = 0.07779098302125931
iteration 232, loss = 0.021021272987127304
iteration 233, loss = 0.04911966994404793
iteration 234, loss = 0.07058872282505035
iteration 235, loss = 0.03054245375096798
iteration 236, loss = 0.04124126583337784
iteration 237, loss = 0.011138559319078922
iteration 238, loss = 0.005364798475056887
iteration 239, loss = 0.18071354925632477
iteration 240, loss = 0.21121695637702942
iteration 241, loss = 0.00047120542149059474
iteration 242, loss = 0.05385999754071236
iteration 243, loss = 0.19783203303813934
iteration 244, loss = 0.2921443581581116
iteration 245, loss = 0.15202099084854126
iteration 246, loss = 0.15579216182231903
iteration 247, loss = 0.021207816898822784
iteration 248, loss = 0.08315244317054749
iteration 249, loss = 0.18285036087036133
iteration 250, loss = 0.30483072996139526
iteration 251, loss = 0.20927098393440247
iteration 252, loss = 0.006738258525729179
iteration 253, loss = 0.1831938773393631
iteration 254, loss = 0.004446340724825859
iteration 255, loss = 0.17747364938259125
iteration 256, loss = 0.05375514179468155
iteration 257, loss = 0.19843275845050812
iteration 258, loss = 0.5146344304084778
iteration 259, loss = 0.3290199339389801
iteration 260, loss = 0.0959746465086937
iteration 261, loss = 0.043870311230421066
iteration 262, loss = 0.08808508515357971
iteration 263, loss = 0.2405151128768921
iteration 264, loss = 0.242261603474617
iteration 265, loss = 0.14484550058841705
iteration 266, loss = 0.09865651279687881
iteration 267, loss = 0.3586583733558655
iteration 268, loss = 0.30340081453323364
iteration 269, loss = 0.10578109323978424
iteration 270, loss = 0.034963663667440414
iteration 271, loss = 0.10415701568126678
iteration 272, loss = 0.03294382244348526
iteration 273, loss = 0.04736657068133354
iteration 274, loss = 0.6309006810188293
iteration 275, loss = 0.06720127910375595
iteration 276, loss = 0.25824886560440063
iteration 277, loss = 0.05313301458954811
iteration 278, loss = 0.06673099845647812
iteration 279, loss = 0.07707153260707855
iteration 280, loss = 0.16270503401756287
iteration 281, loss = 0.37252897024154663
iteration 282, loss = 0.20613305270671844
iteration 283, loss = 0.16093935072422028
iteration 284, loss = 0.010393479838967323
iteration 285, loss = 0.0305420383810997
iteration 286, loss = 0.17624233663082123
iteration 287, loss = 0.0267599206417799
iteration 288, loss = 0.10488392412662506
iteration 289, loss = 0.16308070719242096
iteration 290, loss = 0.08110469579696655
iteration 291, loss = 0.09942034631967545
iteration 292, loss = 0.06175660714507103
iteration 293, loss = 0.07162050157785416
iteration 294, loss = 0.1272037774324417
iteration 295, loss = 0.08240528404712677
iteration 296, loss = 0.018093183636665344
iteration 297, loss = 0.09976693987846375
iteration 298, loss = 0.26233577728271484
iteration 299, loss = 0.03691206872463226
iteration 0, loss = 0.21568945050239563
iteration 1, loss = 0.007862977683544159
iteration 2, loss = 0.04947094991803169
iteration 3, loss = 0.14119412004947662
iteration 4, loss = 0.024768229573965073
iteration 5, loss = 0.015899894759058952
iteration 6, loss = 0.007320841774344444
iteration 7, loss = 0.03873647376894951
iteration 8, loss = 0.10399024933576584
iteration 9, loss = 0.052939217537641525
iteration 10, loss = 0.21515797078609467
iteration 11, loss = 0.09561432898044586
iteration 12, loss = 0.08125116676092148
iteration 13, loss = 0.162918359041214
iteration 14, loss = 0.2547284662723541
iteration 15, loss = 0.026819631457328796
iteration 16, loss = 0.025981388986110687
iteration 17, loss = 0.10977102816104889
iteration 18, loss = 0.032560527324676514
iteration 19, loss = 0.07993406057357788
iteration 20, loss = 0.011374182999134064
iteration 21, loss = 0.029586156830191612
iteration 22, loss = 0.060026753693819046
iteration 23, loss = 0.16567125916481018
iteration 24, loss = 0.17918889224529266
iteration 25, loss = 0.03714892268180847
iteration 26, loss = 0.03646577522158623
iteration 27, loss = 0.051923707127571106
iteration 28, loss = 0.03695133328437805
iteration 29, loss = 0.013873174786567688
iteration 30, loss = 0.015121420845389366
iteration 31, loss = 0.10268080979585648
iteration 32, loss = 0.0035731466487050056
iteration 33, loss = 0.3336018919944763
iteration 34, loss = 0.1854652315378189
iteration 35, loss = 0.024911288172006607
iteration 36, loss = 0.02081640437245369
iteration 37, loss = 0.11149314045906067
iteration 38, loss = 0.19549380242824554
iteration 39, loss = 0.023644227534532547
iteration 40, loss = 0.03335590288043022
iteration 41, loss = 0.027045635506510735
iteration 42, loss = 0.12065885961055756
iteration 43, loss = 0.012355390004813671
iteration 44, loss = 0.37373971939086914
iteration 45, loss = 0.00826029572635889
iteration 46, loss = 0.012194727547466755
iteration 47, loss = 0.16371025145053864
iteration 48, loss = 0.20009538531303406
iteration 49, loss = 0.045603685081005096
iteration 50, loss = 0.04843746870756149
iteration 51, loss = 0.06475421041250229
iteration 52, loss = 0.07828113436698914
iteration 53, loss = 0.021781567484140396
iteration 54, loss = 0.11158281564712524
iteration 55, loss = 0.045336201786994934
iteration 56, loss = 0.005493616685271263
iteration 57, loss = 0.0835045576095581
iteration 58, loss = 0.043242018669843674
iteration 59, loss = 0.08353319764137268
iteration 60, loss = 0.042508840560913086
iteration 61, loss = 0.06015535444021225
iteration 62, loss = 0.036798372864723206
iteration 63, loss = 0.025970442220568657
iteration 64, loss = 0.02818417176604271
iteration 65, loss = 0.12469803541898727
iteration 66, loss = 0.061148401349782944
iteration 67, loss = 0.004801159258931875
iteration 68, loss = 0.04083457589149475
iteration 69, loss = 0.009232156910002232
iteration 70, loss = 0.2809678316116333
iteration 71, loss = 0.0012324382551014423
iteration 72, loss = 0.015922989696264267
iteration 73, loss = 0.010591612197458744
iteration 74, loss = 0.050006844103336334
iteration 75, loss = 0.1147693544626236
iteration 76, loss = 0.02042967826128006
iteration 77, loss = 0.08778784424066544
iteration 78, loss = 0.09493196755647659
iteration 79, loss = 0.06898859143257141
iteration 80, loss = 0.02551339566707611
iteration 81, loss = 0.03028808906674385
iteration 82, loss = 0.013096154667437077
iteration 83, loss = 0.1936686486005783
iteration 84, loss = 0.007928606122732162
iteration 85, loss = 0.023525210097432137
iteration 86, loss = 0.0424763448536396
iteration 87, loss = 0.017369726672768593
iteration 88, loss = 0.06498657166957855
iteration 89, loss = 0.0525607094168663
iteration 90, loss = 0.006030028685927391
iteration 91, loss = 0.06947330385446548
iteration 92, loss = 0.04941471666097641
iteration 93, loss = 0.028294041752815247
iteration 94, loss = 0.08040539175271988
iteration 95, loss = 0.03072076104581356
iteration 96, loss = 0.007535251788794994
iteration 97, loss = 0.07180558890104294
iteration 98, loss = 0.17171059548854828
iteration 99, loss = 0.0664665699005127
iteration 100, loss = 0.013062727637588978
iteration 101, loss = 0.014477565884590149
iteration 102, loss = 0.13888809084892273
iteration 103, loss = 0.02724883146584034
iteration 104, loss = 0.038003288209438324
iteration 105, loss = 0.015035271644592285
iteration 106, loss = 0.0069497376680374146
iteration 107, loss = 0.009888718836009502
iteration 108, loss = 0.06979139894247055
iteration 109, loss = 0.19811788201332092
iteration 110, loss = 0.011376244015991688
iteration 111, loss = 0.03240401670336723
iteration 112, loss = 0.06718418747186661
iteration 113, loss = 0.035911574959754944
iteration 114, loss = 0.035819947719573975
iteration 115, loss = 0.11754658818244934
iteration 116, loss = 0.03287820518016815
iteration 117, loss = 0.09503262490034103
iteration 118, loss = 0.09547417610883713
iteration 119, loss = 0.07527480274438858
iteration 120, loss = 0.0100327143445611
iteration 121, loss = 0.005653135012835264
iteration 122, loss = 0.16579999029636383
iteration 123, loss = 0.0036090523935854435
iteration 124, loss = 0.050721149891614914
iteration 125, loss = 0.18184255063533783
iteration 126, loss = 0.06445435434579849
iteration 127, loss = 0.049661945551633835
iteration 128, loss = 0.023498410359025
iteration 129, loss = 0.0949682965874672
iteration 130, loss = 0.04358943924307823
iteration 131, loss = 0.017748374491930008
iteration 132, loss = 0.11263158917427063
iteration 133, loss = 0.08032464981079102
iteration 134, loss = 0.014374773018062115
iteration 135, loss = 0.08452273905277252
iteration 136, loss = 0.12454762309789658
iteration 137, loss = 0.11057694256305695
iteration 138, loss = 0.03862289711833
iteration 139, loss = 0.026069866493344307
iteration 140, loss = 0.023478278890252113
iteration 141, loss = 0.032461054623126984
iteration 142, loss = 0.024355288594961166
iteration 143, loss = 0.003359031630679965
iteration 144, loss = 0.12205734103918076
iteration 145, loss = 0.0062770661897957325
iteration 146, loss = 0.022668391466140747
iteration 147, loss = 0.062269821763038635
iteration 148, loss = 0.06577210128307343
iteration 149, loss = 0.027230311185121536
iteration 150, loss = 0.023795966058969498
iteration 151, loss = 0.09008477628231049
iteration 152, loss = 0.036951929330825806
iteration 153, loss = 0.018063856288790703
iteration 154, loss = 0.008227690123021603
iteration 155, loss = 0.008132529444992542
iteration 156, loss = 0.03344946727156639
iteration 157, loss = 0.2794196903705597
iteration 158, loss = 0.17875352501869202
iteration 159, loss = 0.03760772570967674
iteration 160, loss = 0.04084920138120651
iteration 161, loss = 0.02707747183740139
iteration 162, loss = 0.03414304926991463
iteration 163, loss = 0.0316927395761013
iteration 164, loss = 0.038091909140348434
iteration 165, loss = 0.07322490960359573
iteration 166, loss = 0.12697865068912506
iteration 167, loss = 0.013727966696023941
iteration 168, loss = 0.05974189192056656
iteration 169, loss = 0.027308331802487373
iteration 170, loss = 0.07726620137691498
iteration 171, loss = 0.08953933417797089
iteration 172, loss = 0.04311741888523102
iteration 173, loss = 0.06208249181509018
iteration 174, loss = 0.04959675297141075
iteration 175, loss = 0.03099841997027397
iteration 176, loss = 0.15825216472148895
iteration 177, loss = 0.06159086152911186
iteration 178, loss = 0.0018290291773155332
iteration 179, loss = 0.11697255820035934
iteration 180, loss = 0.04855376109480858
iteration 181, loss = 0.06261759996414185
iteration 182, loss = 0.039110809564590454
iteration 183, loss = 0.03976268321275711
iteration 184, loss = 0.04347285255789757
iteration 185, loss = 0.07334025949239731
iteration 186, loss = 0.00978628359735012
iteration 187, loss = 0.08475660532712936
iteration 188, loss = 0.03670550137758255
iteration 189, loss = 0.02227114886045456
iteration 190, loss = 0.004611089825630188
iteration 191, loss = 0.12439043819904327
iteration 192, loss = 0.02369724027812481
iteration 193, loss = 0.045851532369852066
iteration 194, loss = 0.0907241702079773
iteration 195, loss = 0.04649689421057701
iteration 196, loss = 0.014733497053384781
iteration 197, loss = 0.024135570973157883
iteration 198, loss = 0.020776400342583656
iteration 199, loss = 0.007956085726618767
iteration 200, loss = 0.027011558413505554
iteration 201, loss = 0.01962999813258648
iteration 202, loss = 0.0036091606598347425
iteration 203, loss = 0.06140535697340965
iteration 204, loss = 0.00953554268926382
iteration 205, loss = 0.0010021260241046548
iteration 206, loss = 0.028410620987415314
iteration 207, loss = 0.0035757170990109444
iteration 208, loss = 0.018464434891939163
iteration 209, loss = 0.009860239923000336
iteration 210, loss = 0.28650614619255066
iteration 211, loss = 0.2117384374141693
iteration 212, loss = 0.2234405279159546
iteration 213, loss = 0.05306163802742958
iteration 214, loss = 0.0006156204035505652
iteration 215, loss = 0.4832398593425751
iteration 216, loss = 0.002289828844368458
iteration 217, loss = 0.17974208295345306
iteration 218, loss = 0.24703016877174377
iteration 219, loss = 0.04591566324234009
iteration 220, loss = 0.20012429356575012
iteration 221, loss = 0.055098000913858414
iteration 222, loss = 0.05497167631983757
iteration 223, loss = 0.023510996252298355
iteration 224, loss = 0.019501561298966408
iteration 225, loss = 0.014196055009961128
iteration 226, loss = 0.12724249064922333
iteration 227, loss = 0.029784779995679855
iteration 228, loss = 0.08110475540161133
iteration 229, loss = 0.2970755100250244
iteration 230, loss = 0.017960714176297188
iteration 231, loss = 0.020915383473038673
iteration 232, loss = 0.005394897423684597
iteration 233, loss = 0.09256114065647125
iteration 234, loss = 0.0298568457365036
iteration 235, loss = 0.0015000860439613461
iteration 236, loss = 0.13801349699497223
iteration 237, loss = 0.0051587484776973724
iteration 238, loss = 0.09286491572856903
iteration 239, loss = 0.008136947639286518
iteration 240, loss = 0.20187199115753174
iteration 241, loss = 0.06793230772018433
iteration 242, loss = 0.009691116400063038
iteration 243, loss = 0.03402664512395859
iteration 244, loss = 0.027441799640655518
iteration 245, loss = 0.013622931204736233
iteration 246, loss = 0.005434093531221151
iteration 247, loss = 0.024230068549513817
iteration 248, loss = 0.016631148755550385
iteration 249, loss = 0.42092472314834595
iteration 250, loss = 0.17856627702713013
iteration 251, loss = 0.018779313191771507
iteration 252, loss = 0.24875114858150482
iteration 253, loss = 0.13916033506393433
iteration 254, loss = 0.008093908429145813
iteration 255, loss = 0.2256932258605957
iteration 256, loss = 0.22992254793643951
iteration 257, loss = 0.20709539949893951
iteration 258, loss = 0.2020326554775238
iteration 259, loss = 0.18849115073680878
iteration 260, loss = 0.0016190387541428208
iteration 261, loss = 0.01940368302166462
iteration 262, loss = 0.0037338859401643276
iteration 263, loss = 0.014345531351864338
iteration 264, loss = 0.041174642741680145
iteration 265, loss = 0.0457683689892292
iteration 266, loss = 0.10716459155082703
iteration 267, loss = 0.023198209702968597
iteration 268, loss = 0.023334704339504242
iteration 269, loss = 0.0414297953248024
iteration 270, loss = 0.1773156076669693
iteration 271, loss = 0.016126997768878937
iteration 272, loss = 0.047452569007873535
iteration 273, loss = 0.12116560339927673
iteration 274, loss = 0.1077059730887413
iteration 275, loss = 0.03073183074593544
iteration 276, loss = 0.07325387001037598
iteration 277, loss = 0.03673526272177696
iteration 278, loss = 0.13371621072292328
iteration 279, loss = 0.07438226044178009
iteration 280, loss = 0.005865367129445076
iteration 281, loss = 0.06283856928348541
iteration 282, loss = 0.008125199005007744
iteration 283, loss = 0.006581242196261883
iteration 284, loss = 0.012950597330927849
iteration 285, loss = 0.04019125923514366
iteration 286, loss = 0.006905561778694391
iteration 287, loss = 0.01573948562145233
iteration 288, loss = 0.05258328095078468
iteration 289, loss = 0.04216516762971878
iteration 290, loss = 0.18350033462047577
iteration 291, loss = 0.038055531680583954
iteration 292, loss = 0.14808404445648193
iteration 293, loss = 0.13529075682163239
iteration 294, loss = 0.12858745455741882
iteration 295, loss = 0.05273282155394554
iteration 296, loss = 0.04619238153100014
iteration 297, loss = 0.14775773882865906
iteration 298, loss = 0.16631875932216644
iteration 299, loss = 0.24994248151779175
iteration 0, loss = 0.11468937247991562
iteration 1, loss = 0.19400262832641602
iteration 2, loss = 0.2344052493572235
iteration 3, loss = 0.06383445113897324
iteration 4, loss = 0.18209640681743622
iteration 5, loss = 0.38308632373809814
iteration 6, loss = 0.2861529588699341
iteration 7, loss = 0.40833672881126404
iteration 8, loss = 0.22757098078727722
iteration 9, loss = 0.04772688075900078
iteration 10, loss = 0.2988802194595337
iteration 11, loss = 0.030395137146115303
iteration 12, loss = 0.24779343605041504
iteration 13, loss = 0.02861316129565239
iteration 14, loss = 0.2013365626335144
iteration 15, loss = 0.20694294571876526
iteration 16, loss = 0.029859168455004692
iteration 17, loss = 0.14072710275650024
iteration 18, loss = 0.1679564118385315
iteration 19, loss = 0.030532484874129295
iteration 20, loss = 0.052068181335926056
iteration 21, loss = 0.07258517295122147
iteration 22, loss = 0.027328936383128166
iteration 23, loss = 0.005443837493658066
iteration 24, loss = 0.21042263507843018
iteration 25, loss = 0.05821584165096283
iteration 26, loss = 0.13131995499134064
iteration 27, loss = 0.22965401411056519
iteration 28, loss = 0.194234237074852
iteration 29, loss = 0.05765173211693764
iteration 30, loss = 0.01609884202480316
iteration 31, loss = 0.005584533791989088
iteration 32, loss = 0.057925157248973846
iteration 33, loss = 0.01855206862092018
iteration 34, loss = 0.09851351380348206
iteration 35, loss = 0.05265708640217781
iteration 36, loss = 0.0056729186326265335
iteration 37, loss = 0.1028396263718605
iteration 38, loss = 0.014705728739500046
iteration 39, loss = 0.15810415148735046
iteration 40, loss = 0.022259818390011787
iteration 41, loss = 0.003567266510799527
iteration 42, loss = 0.07617182284593582
iteration 43, loss = 0.05028577148914337
iteration 44, loss = 0.030452609062194824
iteration 45, loss = 0.03690899908542633
iteration 46, loss = 0.05685693398118019
iteration 47, loss = 0.022425822913646698
iteration 48, loss = 0.025980763137340546
iteration 49, loss = 0.015393083915114403
iteration 50, loss = 0.006659478880465031
iteration 51, loss = 0.2556593418121338
iteration 52, loss = 0.013652949593961239
iteration 53, loss = 0.011097920127213001
iteration 54, loss = 0.011638523079454899
iteration 55, loss = 0.06909213215112686
iteration 56, loss = 0.09108598530292511
iteration 57, loss = 0.03473547846078873
iteration 58, loss = 0.14398671686649323
iteration 59, loss = 0.004673928953707218
iteration 60, loss = 0.2426697313785553
iteration 61, loss = 0.16278815269470215
iteration 62, loss = 0.06671841442584991
iteration 63, loss = 0.05301741510629654
iteration 64, loss = 0.11854272335767746
iteration 65, loss = 0.17504985630512238
iteration 66, loss = 0.08910064399242401
iteration 67, loss = 0.05510687083005905
iteration 68, loss = 0.05980958044528961
iteration 69, loss = 0.029444968327879906
iteration 70, loss = 0.0051559219136834145
iteration 71, loss = 0.007714129984378815
iteration 72, loss = 0.08937088400125504
iteration 73, loss = 0.010495040565729141
iteration 74, loss = 0.0012183038052171469
iteration 75, loss = 0.2270641028881073
iteration 76, loss = 0.14728811383247375
iteration 77, loss = 0.005849752109497786
iteration 78, loss = 0.02493891492486
iteration 79, loss = 0.08324088156223297
iteration 80, loss = 0.09455428272485733
iteration 81, loss = 0.07821757346391678
iteration 82, loss = 0.007214215584099293
iteration 83, loss = 0.0194382481276989
iteration 84, loss = 0.10820860415697098
iteration 85, loss = 0.007873835042119026
iteration 86, loss = 0.10996909439563751
iteration 87, loss = 0.002572562312707305
iteration 88, loss = 0.032679494470357895
iteration 89, loss = 0.12344035506248474
iteration 90, loss = 0.2589118480682373
iteration 91, loss = 0.17690223455429077
iteration 92, loss = 0.035829052329063416
iteration 93, loss = 0.05696646496653557
iteration 94, loss = 0.35541385412216187
iteration 95, loss = 0.0426957905292511
iteration 96, loss = 0.015273883938789368
iteration 97, loss = 0.012892099097371101
iteration 98, loss = 0.004233113955706358
iteration 99, loss = 0.08194098621606827
iteration 100, loss = 0.08603154122829437
iteration 101, loss = 0.06388438493013382
iteration 102, loss = 0.11313088238239288
iteration 103, loss = 0.013052835129201412
iteration 104, loss = 0.026835059747099876
iteration 105, loss = 0.1631803810596466
iteration 106, loss = 0.16525176167488098
iteration 107, loss = 0.018927812576293945
iteration 108, loss = 0.0066377390176057816
iteration 109, loss = 0.16587920486927032
iteration 110, loss = 0.036165159195661545
iteration 111, loss = 0.02539876475930214
iteration 112, loss = 0.2106337994337082
iteration 113, loss = 0.03777487576007843
iteration 114, loss = 0.01899086870253086
iteration 115, loss = 0.08238449692726135
iteration 116, loss = 0.03367242217063904
iteration 117, loss = 0.06097070500254631
iteration 118, loss = 0.07549553364515305
iteration 119, loss = 0.10954948514699936
iteration 120, loss = 0.0143840117380023
iteration 121, loss = 0.02779233269393444
iteration 122, loss = 0.07762035727500916
iteration 123, loss = 0.15646207332611084
iteration 124, loss = 0.04976961016654968
iteration 125, loss = 0.007762370631098747
iteration 126, loss = 0.02168137952685356
iteration 127, loss = 0.1395840048789978
iteration 128, loss = 0.14735101163387299
iteration 129, loss = 0.015694791451096535
iteration 130, loss = 0.12087686359882355
iteration 131, loss = 0.12775249779224396
iteration 132, loss = 0.04360564425587654
iteration 133, loss = 0.13085709512233734
iteration 134, loss = 0.013934423215687275
iteration 135, loss = 0.23815767467021942
iteration 136, loss = 0.12307967245578766
iteration 137, loss = 0.15378056466579437
iteration 138, loss = 0.07275084406137466
iteration 139, loss = 0.0541793517768383
iteration 140, loss = 0.30197054147720337
iteration 141, loss = 0.11229564249515533
iteration 142, loss = 0.0898582711815834
iteration 143, loss = 0.16194899380207062
iteration 144, loss = 0.12672972679138184
iteration 145, loss = 0.008738524280488491
iteration 146, loss = 0.09743935614824295
iteration 147, loss = 0.014705415815114975
iteration 148, loss = 0.21450071036815643
iteration 149, loss = 0.034881651401519775
iteration 150, loss = 0.047919612377882004
iteration 151, loss = 0.06590518355369568
iteration 152, loss = 0.18500791490077972
iteration 153, loss = 0.03371398523449898
iteration 154, loss = 0.24043546617031097
iteration 155, loss = 0.09082445502281189
iteration 156, loss = 0.13506287336349487
iteration 157, loss = 0.20104901492595673
iteration 158, loss = 0.04175560548901558
iteration 159, loss = 0.06475221365690231
iteration 160, loss = 0.014145147986710072
iteration 161, loss = 0.07941130548715591
iteration 162, loss = 0.032864607870578766
iteration 163, loss = 0.037695132195949554
iteration 164, loss = 0.07801898568868637
iteration 165, loss = 0.15781769156455994
iteration 166, loss = 0.1097278892993927
iteration 167, loss = 0.15314331650733948
iteration 168, loss = 0.03997115418314934
iteration 169, loss = 0.02208695001900196
iteration 170, loss = 0.20698262751102448
iteration 171, loss = 0.0016927551478147507
iteration 172, loss = 0.17454105615615845
iteration 173, loss = 0.03352472558617592
iteration 174, loss = 0.03933636471629143
iteration 175, loss = 0.1786104142665863
iteration 176, loss = 0.04275495558977127
iteration 177, loss = 0.1564955860376358
iteration 178, loss = 0.04499921202659607
iteration 179, loss = 0.05858758091926575
iteration 180, loss = 0.022707534953951836
iteration 181, loss = 0.027458155527710915
iteration 182, loss = 0.018222689628601074
iteration 183, loss = 0.15955421328544617
iteration 184, loss = 0.04902788996696472
iteration 185, loss = 0.04701543226838112
iteration 186, loss = 0.08326978981494904
iteration 187, loss = 0.09358551353216171
iteration 188, loss = 0.007500430103391409
iteration 189, loss = 0.06831343472003937
iteration 190, loss = 0.050877273082733154
iteration 191, loss = 0.014582636766135693
iteration 192, loss = 0.05051584169268608
iteration 193, loss = 0.039396680891513824
iteration 194, loss = 0.00842658057808876
iteration 195, loss = 0.022101255133748055
iteration 196, loss = 0.1743514984846115
iteration 197, loss = 0.07358942925930023
iteration 198, loss = 0.22712330520153046
iteration 199, loss = 0.021552352234721184
iteration 200, loss = 0.2467474639415741
iteration 201, loss = 0.1304669976234436
iteration 202, loss = 0.045014653354883194
iteration 203, loss = 0.13682697713375092
iteration 204, loss = 0.04040013253688812
iteration 205, loss = 0.2537021040916443
iteration 206, loss = 0.13863416016101837
iteration 207, loss = 0.023159468546509743
iteration 208, loss = 0.05642934888601303
iteration 209, loss = 0.1136355996131897
iteration 210, loss = 0.10480951517820358
iteration 211, loss = 0.06940166652202606
iteration 212, loss = 0.04016386345028877
iteration 213, loss = 0.10281142592430115
iteration 214, loss = 0.008305791765451431
iteration 215, loss = 0.004482749383896589
iteration 216, loss = 0.004877416417002678
iteration 217, loss = 0.11688601970672607
iteration 218, loss = 0.0012137982994318008
iteration 219, loss = 0.009007194079458714
iteration 220, loss = 0.001423488836735487
iteration 221, loss = 0.050823599100112915
iteration 222, loss = 0.031803205609321594
iteration 223, loss = 0.06952375918626785
iteration 224, loss = 0.040859147906303406
iteration 225, loss = 0.01342296227812767
iteration 226, loss = 0.2234533280134201
iteration 227, loss = 0.2592156231403351
iteration 228, loss = 0.02390385791659355
iteration 229, loss = 0.10076707601547241
iteration 230, loss = 0.10510586202144623
iteration 231, loss = 0.049637965857982635
iteration 232, loss = 0.003513599280267954
iteration 233, loss = 0.2588525116443634
iteration 234, loss = 0.018744315952062607
iteration 235, loss = 0.022564688697457314
iteration 236, loss = 0.06973489373922348
iteration 237, loss = 0.07038456946611404
iteration 238, loss = 0.011052185669541359
iteration 239, loss = 0.1388910412788391
iteration 240, loss = 0.034683916717767715
iteration 241, loss = 0.03581039607524872
iteration 242, loss = 0.07440878450870514
iteration 243, loss = 0.1341606080532074
iteration 244, loss = 0.09079059958457947
iteration 245, loss = 0.08730997890233994
iteration 246, loss = 0.10082002729177475
iteration 247, loss = 0.02099054679274559
iteration 248, loss = 0.058126214891672134
iteration 249, loss = 0.03532303124666214
iteration 250, loss = 0.033179931342601776
iteration 251, loss = 0.03152091056108475
iteration 252, loss = 0.07877658307552338
iteration 253, loss = 0.04849296808242798
iteration 254, loss = 0.015399456024169922
iteration 255, loss = 0.1484251171350479
iteration 256, loss = 0.021063536405563354
iteration 257, loss = 0.006790502462536097
iteration 258, loss = 0.027935322374105453
iteration 259, loss = 0.028166942298412323
iteration 260, loss = 0.01894570328295231
iteration 261, loss = 0.005131985060870647
iteration 262, loss = 0.018321335315704346
iteration 263, loss = 0.019428927451372147
iteration 264, loss = 0.13151073455810547
iteration 265, loss = 0.35593265295028687
iteration 266, loss = 0.006326187867671251
iteration 267, loss = 0.07948490977287292
iteration 268, loss = 0.0730503648519516
iteration 269, loss = 0.04850103706121445
iteration 270, loss = 0.14380332827568054
iteration 271, loss = 0.03694307804107666
iteration 272, loss = 0.018333792686462402
iteration 273, loss = 0.01758640818297863
iteration 274, loss = 0.11543750762939453
iteration 275, loss = 0.12731975317001343
iteration 276, loss = 0.013002272695302963
iteration 277, loss = 0.11209025979042053
iteration 278, loss = 0.14686356484889984
iteration 279, loss = 0.11367163062095642
iteration 280, loss = 0.026378275826573372
iteration 281, loss = 0.029348447918891907
iteration 282, loss = 0.031178053468465805
iteration 283, loss = 0.005873579531908035
iteration 284, loss = 0.007857725955545902
iteration 285, loss = 0.002583134453743696
iteration 286, loss = 0.04633419215679169
iteration 287, loss = 0.1252453476190567
iteration 288, loss = 0.08707303553819656
iteration 289, loss = 0.009224510751664639
iteration 290, loss = 0.021368861198425293
iteration 291, loss = 0.18742479383945465
iteration 292, loss = 0.04784427583217621
iteration 293, loss = 0.08781900256872177
iteration 294, loss = 0.01848546788096428
iteration 295, loss = 0.16749978065490723
iteration 296, loss = 0.006935973651707172
iteration 297, loss = 0.1328480839729309
iteration 298, loss = 0.08896012604236603
iteration 299, loss = 0.04692805930972099
iteration 0, loss = 0.026387469843029976
iteration 1, loss = 0.024240579456090927
iteration 2, loss = 0.14020220935344696
iteration 3, loss = 0.025028688833117485
iteration 4, loss = 0.2666458189487457
iteration 5, loss = 0.045240890234708786
iteration 6, loss = 0.007526420056819916
iteration 7, loss = 0.024550914764404297
iteration 8, loss = 0.19583456218242645
iteration 9, loss = 0.0023901155218482018
iteration 10, loss = 0.013379069976508617
iteration 11, loss = 0.006013737991452217
iteration 12, loss = 0.011801768094301224
iteration 13, loss = 0.14227554202079773
iteration 14, loss = 0.36344316601753235
iteration 15, loss = 0.0873970091342926
iteration 16, loss = 0.3308643400669098
iteration 17, loss = 0.13514919579029083
iteration 18, loss = 0.015399165451526642
iteration 19, loss = 0.05247092619538307
iteration 20, loss = 0.04491318762302399
iteration 21, loss = 0.20599083602428436
iteration 22, loss = 0.016133785247802734
iteration 23, loss = 0.025946669280529022
iteration 24, loss = 0.061533816158771515
iteration 25, loss = 0.07953601330518723
iteration 26, loss = 0.023918524384498596
iteration 27, loss = 0.09183935075998306
iteration 28, loss = 0.043280914425849915
iteration 29, loss = 0.008599289692938328
iteration 30, loss = 0.01484907977283001
iteration 31, loss = 0.003870823420584202
iteration 32, loss = 0.259296178817749
iteration 33, loss = 0.018084751442074776
iteration 34, loss = 0.05050002411007881
iteration 35, loss = 0.0920991376042366
iteration 36, loss = 0.04892798885703087
iteration 37, loss = 0.02127075381577015
iteration 38, loss = 0.32165175676345825
iteration 39, loss = 0.013309495523571968
iteration 40, loss = 0.07129503041505814
iteration 41, loss = 0.31774675846099854
iteration 42, loss = 0.26958778500556946
iteration 43, loss = 0.16428008675575256
iteration 44, loss = 0.08072823286056519
iteration 45, loss = 0.05008884519338608
iteration 46, loss = 0.06316878646612167
iteration 47, loss = 0.027422957122325897
iteration 48, loss = 0.08209450542926788
iteration 49, loss = 0.12663805484771729
iteration 50, loss = 0.18578258156776428
iteration 51, loss = 0.0028345605824142694
iteration 52, loss = 0.05469673499464989
iteration 53, loss = 0.12263216078281403
iteration 54, loss = 0.014807106927037239
iteration 55, loss = 0.007088741287589073
iteration 56, loss = 0.003398576984182
iteration 57, loss = 0.026430130004882812
iteration 58, loss = 0.028586678206920624
iteration 59, loss = 0.09751170873641968
iteration 60, loss = 0.0017591627547517419
iteration 61, loss = 0.03383698686957359
iteration 62, loss = 0.10271774232387543
iteration 63, loss = 0.04438678175210953
iteration 64, loss = 0.06194861978292465
iteration 65, loss = 0.024920694530010223
iteration 66, loss = 0.13134247064590454
iteration 67, loss = 0.062070332467556
iteration 68, loss = 0.013983220793306828
iteration 69, loss = 0.02869231067597866
iteration 70, loss = 0.010835601948201656
iteration 71, loss = 0.11301897466182709
iteration 72, loss = 0.013013082556426525
iteration 73, loss = 0.07398087531328201
iteration 74, loss = 0.04512414336204529
iteration 75, loss = 0.008634882979094982
iteration 76, loss = 0.028157852590084076
iteration 77, loss = 0.03226490318775177
iteration 78, loss = 0.011744031682610512
iteration 79, loss = 0.045338284224271774
iteration 80, loss = 0.007550180424004793
iteration 81, loss = 0.05395103245973587
iteration 82, loss = 0.038815777748823166
iteration 83, loss = 0.01309929694980383
iteration 84, loss = 0.02246898226439953
iteration 85, loss = 0.14781509339809418
iteration 86, loss = 0.10891805589199066
iteration 87, loss = 0.01629219576716423
iteration 88, loss = 0.05106528475880623
iteration 89, loss = 0.05310793220996857
iteration 90, loss = 0.12167911231517792
iteration 91, loss = 0.006601446308195591
iteration 92, loss = 0.024099573493003845
iteration 93, loss = 0.011184781789779663
iteration 94, loss = 0.007342787925153971
iteration 95, loss = 0.004349655471742153
iteration 96, loss = 0.02111707255244255
iteration 97, loss = 0.1305830329656601
iteration 98, loss = 0.1432827264070511
iteration 99, loss = 0.005301106721162796
iteration 100, loss = 0.1527089923620224
iteration 101, loss = 0.05240250006318092
iteration 102, loss = 0.05723421275615692
iteration 103, loss = 0.1406863033771515
iteration 104, loss = 0.025541232898831367
iteration 105, loss = 0.020671885460615158
iteration 106, loss = 0.02438894845545292
iteration 107, loss = 0.09338507801294327
iteration 108, loss = 0.0310419499874115
iteration 109, loss = 0.11940232664346695
iteration 110, loss = 0.019697822630405426
iteration 111, loss = 0.018199114128947258
iteration 112, loss = 0.011060034856200218
iteration 113, loss = 0.01584932580590248
iteration 114, loss = 0.008678758516907692
iteration 115, loss = 0.012572851963341236
iteration 116, loss = 0.038961950689554214
iteration 117, loss = 0.04625766724348068
iteration 118, loss = 0.016160698607563972
iteration 119, loss = 0.02428436651825905
iteration 120, loss = 0.03318266570568085
iteration 121, loss = 0.19459564983844757
iteration 122, loss = 0.18265178799629211
iteration 123, loss = 0.0039004578720778227
iteration 124, loss = 0.03267977386713028
iteration 125, loss = 0.002207366283982992
iteration 126, loss = 0.028988990932703018
iteration 127, loss = 0.019371159374713898
iteration 128, loss = 0.010145659558475018
iteration 129, loss = 0.02809244394302368
iteration 130, loss = 0.02382529154419899
iteration 131, loss = 0.008105142042040825
iteration 132, loss = 0.012920333072543144
iteration 133, loss = 0.04216121509671211
iteration 134, loss = 0.052675869315862656
iteration 135, loss = 0.03269531950354576
iteration 136, loss = 0.030054273083806038
iteration 137, loss = 0.03054957278072834
iteration 138, loss = 0.01684590056538582
iteration 139, loss = 0.002711971290409565
iteration 140, loss = 0.039877887815237045
iteration 141, loss = 0.001288624363951385
iteration 142, loss = 0.05082690715789795
iteration 143, loss = 0.03216329962015152
iteration 144, loss = 0.05448821559548378
iteration 145, loss = 0.02821098081767559
iteration 146, loss = 0.040593378245830536
iteration 147, loss = 0.049530766904354095
iteration 148, loss = 0.019184963777661324
iteration 149, loss = 0.009550743736326694
iteration 150, loss = 0.020025702193379402
iteration 151, loss = 0.00912153348326683
iteration 152, loss = 0.05035241320729256
iteration 153, loss = 0.22646979987621307
iteration 154, loss = 0.2887762188911438
iteration 155, loss = 0.03871079161763191
iteration 156, loss = 0.04593265429139137
iteration 157, loss = 0.03989012911915779
iteration 158, loss = 0.042668137699365616
iteration 159, loss = 0.01845805160701275
iteration 160, loss = 0.09037614613771439
iteration 161, loss = 0.22672951221466064
iteration 162, loss = 0.12270122766494751
iteration 163, loss = 0.024248681962490082
iteration 164, loss = 0.07693584263324738
iteration 165, loss = 0.07735516875982285
iteration 166, loss = 0.029460323974490166
iteration 167, loss = 0.03867904469370842
iteration 168, loss = 0.16051526367664337
iteration 169, loss = 0.15384967625141144
iteration 170, loss = 0.0037261052057147026
iteration 171, loss = 0.09825488179922104
iteration 172, loss = 0.08832737058401108
iteration 173, loss = 0.007923567667603493
iteration 174, loss = 0.1239953339099884
iteration 175, loss = 0.02635110914707184
iteration 176, loss = 0.034388914704322815
iteration 177, loss = 0.044989269226789474
iteration 178, loss = 0.0035041088704019785
iteration 179, loss = 0.06122903898358345
iteration 180, loss = 0.2218729704618454
iteration 181, loss = 0.03372245281934738
iteration 182, loss = 0.004443927202373743
iteration 183, loss = 0.04039320349693298
iteration 184, loss = 0.0316583625972271
iteration 185, loss = 0.07719779014587402
iteration 186, loss = 0.02027646079659462
iteration 187, loss = 0.023506630212068558
iteration 188, loss = 0.07089238613843918
iteration 189, loss = 0.001025475561618805
iteration 190, loss = 0.006476150825619698
iteration 191, loss = 0.010779766365885735
iteration 192, loss = 0.0010529396822676063
iteration 193, loss = 0.008440373465418816
iteration 194, loss = 0.0054863737896084785
iteration 195, loss = 0.00829797051846981
iteration 196, loss = 0.025565505027770996
iteration 197, loss = 0.002901000902056694
iteration 198, loss = 0.10314402729272842
iteration 199, loss = 0.04929548501968384
iteration 200, loss = 0.004536292981356382
iteration 201, loss = 0.010528525337576866
iteration 202, loss = 0.017237208783626556
iteration 203, loss = 0.04198971390724182
iteration 204, loss = 0.002399658551439643
iteration 205, loss = 0.06707441806793213
iteration 206, loss = 0.12410155683755875
iteration 207, loss = 0.0025965915992856026
iteration 208, loss = 0.028454899787902832
iteration 209, loss = 0.03686390444636345
iteration 210, loss = 0.08257600665092468
iteration 211, loss = 0.08203030377626419
iteration 212, loss = 0.04145611450076103
iteration 213, loss = 0.007232495117932558
iteration 214, loss = 0.005019343923777342
iteration 215, loss = 0.11664947867393494
iteration 216, loss = 0.27783170342445374
iteration 217, loss = 0.06018418073654175
iteration 218, loss = 0.04928407073020935
iteration 219, loss = 0.14827682077884674
iteration 220, loss = 0.392875611782074
iteration 221, loss = 0.354997456073761
iteration 222, loss = 0.006906173657625914
iteration 223, loss = 0.1132076159119606
iteration 224, loss = 0.004009800497442484
iteration 225, loss = 0.20287540555000305
iteration 226, loss = 0.0011117774993181229
iteration 227, loss = 0.1883334517478943
iteration 228, loss = 0.22955437004566193
iteration 229, loss = 0.23551183938980103
iteration 230, loss = 0.13105331361293793
iteration 231, loss = 0.22769302129745483
iteration 232, loss = 0.1326301395893097
iteration 233, loss = 0.17009153962135315
iteration 234, loss = 0.003631504252552986
iteration 235, loss = 0.11818326264619827
iteration 236, loss = 0.10742922127246857
iteration 237, loss = 0.04067109152674675
iteration 238, loss = 0.02602158859372139
iteration 239, loss = 0.030985567718744278
iteration 240, loss = 0.03707634657621384
iteration 241, loss = 0.014774191193282604
iteration 242, loss = 0.07354727387428284
iteration 243, loss = 0.09630721062421799
iteration 244, loss = 0.021739428862929344
iteration 245, loss = 0.006223064847290516
iteration 246, loss = 0.024900175631046295
iteration 247, loss = 0.01522698625922203
iteration 248, loss = 0.02154436707496643
iteration 249, loss = 0.053324632346630096
iteration 250, loss = 0.034752052277326584
iteration 251, loss = 0.09243953227996826
iteration 252, loss = 0.026527410373091698
iteration 253, loss = 0.054752275347709656
iteration 254, loss = 0.01139695942401886
iteration 255, loss = 0.09089671075344086
iteration 256, loss = 0.0563950389623642
iteration 257, loss = 0.01802971586585045
iteration 258, loss = 0.051013171672821045
iteration 259, loss = 0.029763970524072647
iteration 260, loss = 0.04342212155461311
iteration 261, loss = 0.21671488881111145
iteration 262, loss = 0.007045777980238199
iteration 263, loss = 0.07088682800531387
iteration 264, loss = 0.14389890432357788
iteration 265, loss = 0.11859776824712753
iteration 266, loss = 0.03992908447980881
iteration 267, loss = 0.13529811799526215
iteration 268, loss = 0.16556690633296967
iteration 269, loss = 0.11329537630081177
iteration 270, loss = 0.03929035738110542
iteration 271, loss = 0.12280073761940002
iteration 272, loss = 0.007403533440083265
iteration 273, loss = 0.16269701719284058
iteration 274, loss = 0.0021464694291353226
iteration 275, loss = 0.006204056553542614
iteration 276, loss = 0.0008101382991299033
iteration 277, loss = 0.11482056230306625
iteration 278, loss = 0.08381788432598114
iteration 279, loss = 0.2503909468650818
iteration 280, loss = 0.07791563868522644
iteration 281, loss = 0.007861590944230556
iteration 282, loss = 0.00408000685274601
iteration 283, loss = 0.060204219073057175
iteration 284, loss = 0.013250655494630337
iteration 285, loss = 0.12638552486896515
iteration 286, loss = 0.037950314581394196
iteration 287, loss = 0.20942315459251404
iteration 288, loss = 0.02783241868019104
iteration 289, loss = 0.0385107696056366
iteration 290, loss = 0.10604184865951538
iteration 291, loss = 0.02922772243618965
iteration 292, loss = 0.01659053936600685
iteration 293, loss = 0.03409196808934212
iteration 294, loss = 0.038011908531188965
iteration 295, loss = 0.010600910522043705
iteration 296, loss = 0.08245261013507843
iteration 297, loss = 0.08012974262237549
iteration 298, loss = 0.15435345470905304
iteration 299, loss = 0.08891838043928146
iteration 0, loss = 0.06918299198150635
iteration 1, loss = 0.09621916711330414
iteration 2, loss = 0.03759600222110748
iteration 3, loss = 0.16555970907211304
iteration 4, loss = 0.07298292964696884
iteration 5, loss = 0.11802319437265396
iteration 6, loss = 0.0008212715038098395
iteration 7, loss = 0.010305041447281837
iteration 8, loss = 0.18050773441791534
iteration 9, loss = 0.04708877578377724
iteration 10, loss = 0.006519346497952938
iteration 11, loss = 0.13289321959018707
iteration 12, loss = 0.024838237091898918
iteration 13, loss = 0.03640462085604668
iteration 14, loss = 0.008894529193639755
iteration 15, loss = 0.03515364229679108
iteration 16, loss = 0.03688676282763481
iteration 17, loss = 0.0013034394942224026
iteration 18, loss = 0.008496680296957493
iteration 19, loss = 0.05493837594985962
iteration 20, loss = 0.010138200595974922
iteration 21, loss = 0.25902605056762695
iteration 22, loss = 0.04567338898777962
iteration 23, loss = 0.060476355254650116
iteration 24, loss = 0.22524946928024292
iteration 25, loss = 0.033984825015068054
iteration 26, loss = 0.10963857918977737
iteration 27, loss = 0.00030328621505759656
iteration 28, loss = 0.017394596710801125
iteration 29, loss = 0.08972616493701935
iteration 30, loss = 0.08557412028312683
iteration 31, loss = 0.12117528915405273
iteration 32, loss = 0.06794162839651108
iteration 33, loss = 0.025562630966305733
iteration 34, loss = 0.13362321257591248
iteration 35, loss = 0.129914328455925
iteration 36, loss = 0.02364923432469368
iteration 37, loss = 0.2475646734237671
iteration 38, loss = 0.028765808790922165
iteration 39, loss = 0.010309177450835705
iteration 40, loss = 0.15474019944667816
iteration 41, loss = 0.02062249928712845
iteration 42, loss = 0.05949125438928604
iteration 43, loss = 0.025701887905597687
iteration 44, loss = 0.06013934314250946
iteration 45, loss = 0.0396876260638237
iteration 46, loss = 0.18802013993263245
iteration 47, loss = 0.022834599018096924
iteration 48, loss = 0.036103490740060806
iteration 49, loss = 0.13138212263584137
iteration 50, loss = 0.05380691587924957
iteration 51, loss = 0.0474519357085228
iteration 52, loss = 0.06338126212358475
iteration 53, loss = 0.1061696857213974
iteration 54, loss = 0.024731386452913284
iteration 55, loss = 0.0942532941699028
iteration 56, loss = 0.053595758974552155
iteration 57, loss = 0.02457188069820404
iteration 58, loss = 0.018925704061985016
iteration 59, loss = 0.12549655139446259
iteration 60, loss = 0.05940080061554909
iteration 61, loss = 0.19808857142925262
iteration 62, loss = 0.006053925026208162
iteration 63, loss = 0.18279577791690826
iteration 64, loss = 0.05571034550666809
iteration 65, loss = 0.3525882661342621
iteration 66, loss = 0.0012969303643330932
iteration 67, loss = 0.006384147331118584
iteration 68, loss = 0.09734861552715302
iteration 69, loss = 0.0020874214824289083
iteration 70, loss = 0.07310725748538971
iteration 71, loss = 0.16169054806232452
iteration 72, loss = 0.2872253954410553
iteration 73, loss = 0.0772082731127739
iteration 74, loss = 0.14659900963306427
iteration 75, loss = 0.07832537591457367
iteration 76, loss = 0.0879177674651146
iteration 77, loss = 0.06556668132543564
iteration 78, loss = 0.3123229742050171
iteration 79, loss = 0.02282937988638878
iteration 80, loss = 0.06502591073513031
iteration 81, loss = 0.043896324932575226
iteration 82, loss = 0.01858234778046608
iteration 83, loss = 0.3382769823074341
iteration 84, loss = 0.36223670840263367
iteration 85, loss = 0.4132469594478607
iteration 86, loss = 0.22136420011520386
iteration 87, loss = 0.026554960757493973
iteration 88, loss = 0.0349808931350708
iteration 89, loss = 0.14017434418201447
iteration 90, loss = 0.6458209156990051
iteration 91, loss = 0.17258401215076447
iteration 92, loss = 0.12349550426006317
iteration 93, loss = 0.11912855505943298
iteration 94, loss = 0.09530685842037201
iteration 95, loss = 0.023099901154637337
iteration 96, loss = 0.00017448414291720837
iteration 97, loss = 0.21829301118850708
iteration 98, loss = 0.002150925574824214
iteration 99, loss = 0.18779589235782623
iteration 100, loss = 0.21297232806682587
iteration 101, loss = 0.09114452451467514
iteration 102, loss = 0.0005335818277671933
iteration 103, loss = 0.004009326919913292
iteration 104, loss = 0.02048817276954651
iteration 105, loss = 0.010906912386417389
iteration 106, loss = 0.0161345936357975
iteration 107, loss = 0.1566515564918518
iteration 108, loss = 0.039480794221162796
iteration 109, loss = 0.030466876924037933
iteration 110, loss = 0.008435828611254692
iteration 111, loss = 0.02620689384639263
iteration 112, loss = 0.015447409823536873
iteration 113, loss = 0.1076851487159729
iteration 114, loss = 0.011994630098342896
iteration 115, loss = 0.1396659016609192
iteration 116, loss = 0.04194485396146774
iteration 117, loss = 0.06321118772029877
iteration 118, loss = 0.03606968745589256
iteration 119, loss = 0.0362338125705719
iteration 120, loss = 0.07667936384677887
iteration 121, loss = 0.01817638799548149
iteration 122, loss = 0.008406253531575203
iteration 123, loss = 0.384598046541214
iteration 124, loss = 0.2209276556968689
iteration 125, loss = 0.0015129477251321077
iteration 126, loss = 0.2054332047700882
iteration 127, loss = 0.2444806843996048
iteration 128, loss = 0.09901177883148193
iteration 129, loss = 0.0807504653930664
iteration 130, loss = 0.6700780987739563
iteration 131, loss = 0.42751550674438477
iteration 132, loss = 0.07484082877635956
iteration 133, loss = 0.1771661639213562
iteration 134, loss = 0.2868008315563202
iteration 135, loss = 0.209930881857872
iteration 136, loss = 0.014274809509515762
iteration 137, loss = 0.20849384367465973
iteration 138, loss = 0.11974234879016876
iteration 139, loss = 0.11790744215250015
iteration 140, loss = 0.04517928883433342
iteration 141, loss = 0.6109083294868469
iteration 142, loss = 0.1728663295507431
iteration 143, loss = 0.09251710027456284
iteration 144, loss = 0.2167782038450241
iteration 145, loss = 0.04266280680894852
iteration 146, loss = 0.0030969721265137196
iteration 147, loss = 0.10277494788169861
iteration 148, loss = 0.005208480171859264
iteration 149, loss = 0.1532323807477951
iteration 150, loss = 0.0033183612395077944
iteration 151, loss = 0.004672439303249121
iteration 152, loss = 0.03762940317392349
iteration 153, loss = 0.015802426263689995
iteration 154, loss = 0.1014154851436615
iteration 155, loss = 0.08555686473846436
iteration 156, loss = 0.04785910248756409
iteration 157, loss = 0.028442369773983955
iteration 158, loss = 0.03542102873325348
iteration 159, loss = 0.14487197995185852
iteration 160, loss = 0.022804129868745804
iteration 161, loss = 0.05571563541889191
iteration 162, loss = 0.0566290020942688
iteration 163, loss = 0.03550722450017929
iteration 164, loss = 0.03674585744738579
iteration 165, loss = 0.06776659935712814
iteration 166, loss = 0.06050765886902809
iteration 167, loss = 0.014291208237409592
iteration 168, loss = 0.03968869522213936
iteration 169, loss = 0.05838453769683838
iteration 170, loss = 0.03459206968545914
iteration 171, loss = 0.018987299874424934
iteration 172, loss = 0.08959835767745972
iteration 173, loss = 0.019686879590153694
iteration 174, loss = 0.10692093521356583
iteration 175, loss = 0.02829437330365181
iteration 176, loss = 0.08719228953123093
iteration 177, loss = 0.082088403403759
iteration 178, loss = 0.03762779012322426
iteration 179, loss = 0.04971514269709587
iteration 180, loss = 0.14852648973464966
iteration 181, loss = 0.03755784034729004
iteration 182, loss = 0.0369015708565712
iteration 183, loss = 0.029538579285144806
iteration 184, loss = 0.03112737089395523
iteration 185, loss = 0.04290616512298584
iteration 186, loss = 0.1606069654226303
iteration 187, loss = 0.02057860605418682
iteration 188, loss = 0.06778670847415924
iteration 189, loss = 0.07543124258518219
iteration 190, loss = 0.020568780601024628
iteration 191, loss = 0.023976992815732956
iteration 192, loss = 0.046620436012744904
iteration 193, loss = 0.05398668348789215
iteration 194, loss = 0.01834714040160179
iteration 195, loss = 0.016177667304873466
iteration 196, loss = 0.07694394886493683
iteration 197, loss = 0.05473591387271881
iteration 198, loss = 0.03307674452662468
iteration 199, loss = 0.10471729189157486
iteration 200, loss = 0.04506311193108559
iteration 201, loss = 0.028697527945041656
iteration 202, loss = 0.012396648526191711
iteration 203, loss = 0.0031694371718913317
iteration 204, loss = 0.011007081717252731
iteration 205, loss = 0.0028140346985310316
iteration 206, loss = 0.0011469889432191849
iteration 207, loss = 0.0018841114360839128
iteration 208, loss = 0.270397812128067
iteration 209, loss = 0.029614875093102455
iteration 210, loss = 0.2270893007516861
iteration 211, loss = 0.0031725121662020683
iteration 212, loss = 0.007193326484411955
iteration 213, loss = 0.14729222655296326
iteration 214, loss = 0.07080040872097015
iteration 215, loss = 0.0032896804623305798
iteration 216, loss = 0.013991459272801876
iteration 217, loss = 0.3824900984764099
iteration 218, loss = 0.009479365311563015
iteration 219, loss = 0.4756357967853546
iteration 220, loss = 0.03392209857702255
iteration 221, loss = 0.09192357957363129
iteration 222, loss = 0.05029560625553131
iteration 223, loss = 0.008422700688242912
iteration 224, loss = 0.19622640311717987
iteration 225, loss = 0.06077864393591881
iteration 226, loss = 0.0018820622935891151
iteration 227, loss = 0.23307755589485168
iteration 228, loss = 0.024335436522960663
iteration 229, loss = 0.022193126380443573
iteration 230, loss = 0.02204998955130577
iteration 231, loss = 0.06302962452173233
iteration 232, loss = 0.052680060267448425
iteration 233, loss = 0.23682254552841187
iteration 234, loss = 0.011543176136910915
iteration 235, loss = 0.047756485641002655
iteration 236, loss = 0.08270484954118729
iteration 237, loss = 0.07559990137815475
iteration 238, loss = 0.16410818696022034
iteration 239, loss = 0.04721792787313461
iteration 240, loss = 0.008368149399757385
iteration 241, loss = 0.015117268078029156
iteration 242, loss = 0.005093838088214397
iteration 243, loss = 0.04702439159154892
iteration 244, loss = 0.17147867381572723
iteration 245, loss = 0.011284342966973782
iteration 246, loss = 0.017031997442245483
iteration 247, loss = 0.017044853419065475
iteration 248, loss = 0.020374475046992302
iteration 249, loss = 0.05406360700726509
iteration 250, loss = 0.19204404950141907
iteration 251, loss = 0.01691647246479988
iteration 252, loss = 0.14303098618984222
iteration 253, loss = 0.03981505334377289
iteration 254, loss = 0.12858404219150543
iteration 255, loss = 0.028118353337049484
iteration 256, loss = 0.13251140713691711
iteration 257, loss = 0.05471000820398331
iteration 258, loss = 0.025539258494973183
iteration 259, loss = 0.010809402912855148
iteration 260, loss = 0.2493123859167099
iteration 261, loss = 0.011705744080245495
iteration 262, loss = 0.008148858323693275
iteration 263, loss = 0.00718260183930397
iteration 264, loss = 0.018215855583548546
iteration 265, loss = 0.0953250452876091
iteration 266, loss = 0.059486664831638336
iteration 267, loss = 0.13571955263614655
iteration 268, loss = 0.16489911079406738
iteration 269, loss = 0.01831425167620182
iteration 270, loss = 0.021884310990571976
iteration 271, loss = 0.1086055263876915
iteration 272, loss = 0.08258137106895447
iteration 273, loss = 0.058240242302417755
iteration 274, loss = 0.058379027992486954
iteration 275, loss = 0.05818915367126465
iteration 276, loss = 0.16990584135055542
iteration 277, loss = 0.023944633081555367
iteration 278, loss = 0.024863336235284805
iteration 279, loss = 0.18649756908416748
iteration 280, loss = 0.2535765767097473
iteration 281, loss = 0.0005889685708098114
iteration 282, loss = 0.005166142247617245
iteration 283, loss = 0.09196789562702179
iteration 284, loss = 0.004595997743308544
iteration 285, loss = 0.08208033442497253
iteration 286, loss = 0.05258568376302719
iteration 287, loss = 0.026616236194968224
iteration 288, loss = 0.04409492015838623
iteration 289, loss = 0.03480972722172737
iteration 290, loss = 0.00131805834826082
iteration 291, loss = 0.17669187486171722
iteration 292, loss = 0.00469087902456522
iteration 293, loss = 0.13654261827468872
iteration 294, loss = 0.16540741920471191
iteration 295, loss = 0.12621283531188965
iteration 296, loss = 0.09596867114305496
iteration 297, loss = 0.06528444588184357
iteration 298, loss = 0.02345937117934227
iteration 299, loss = 0.0712418258190155
iteration 0, loss = 0.10363636910915375
iteration 1, loss = 0.04138512536883354
iteration 2, loss = 0.06234968081116676
iteration 3, loss = 0.10201317071914673
iteration 4, loss = 0.07963664829730988
iteration 5, loss = 0.08578860014677048
iteration 6, loss = 0.06394568830728531
iteration 7, loss = 0.0075252423994243145
iteration 8, loss = 0.2549535036087036
iteration 9, loss = 0.011895470321178436
iteration 10, loss = 0.01222898904234171
iteration 11, loss = 0.09122668951749802
iteration 12, loss = 0.021698571741580963
iteration 13, loss = 0.002069588517770171
iteration 14, loss = 0.03050885908305645
iteration 15, loss = 0.04289545491337776
iteration 16, loss = 0.14398527145385742
iteration 17, loss = 0.0434022955596447
iteration 18, loss = 0.008804284036159515
iteration 19, loss = 0.006280884146690369
iteration 20, loss = 0.009176495485007763
iteration 21, loss = 0.2306404411792755
iteration 22, loss = 0.07927290350198746
iteration 23, loss = 0.0009492987883277237
iteration 24, loss = 0.30464065074920654
iteration 25, loss = 0.00948590599000454
iteration 26, loss = 0.060664545744657516
iteration 27, loss = 0.0625014379620552
iteration 28, loss = 0.11731263250112534
iteration 29, loss = 0.07281829416751862
iteration 30, loss = 0.057034172117710114
iteration 31, loss = 0.03398389369249344
iteration 32, loss = 0.1371646672487259
iteration 33, loss = 0.01533304899930954
iteration 34, loss = 0.16314956545829773
iteration 35, loss = 0.01620488613843918
iteration 36, loss = 0.04192738234996796
iteration 37, loss = 0.10387717932462692
iteration 38, loss = 0.14884299039840698
iteration 39, loss = 0.0018879655981436372
iteration 40, loss = 0.00540960393846035
iteration 41, loss = 0.4835922122001648
iteration 42, loss = 0.4643506109714508
iteration 43, loss = 0.22872309386730194
iteration 44, loss = 0.17494136095046997
iteration 45, loss = 0.4473123848438263
iteration 46, loss = 0.07208071649074554
iteration 47, loss = 0.02013034000992775
iteration 48, loss = 0.2847801148891449
iteration 49, loss = 0.16410788893699646
iteration 50, loss = 0.002908097580075264
iteration 51, loss = 0.2059323936700821
iteration 52, loss = 0.04099123552441597
iteration 53, loss = 0.061121195554733276
iteration 54, loss = 0.09873206168413162
iteration 55, loss = 0.15853285789489746
iteration 56, loss = 0.05687358230352402
iteration 57, loss = 0.11234793066978455
iteration 58, loss = 0.17496618628501892
iteration 59, loss = 0.13876841962337494
iteration 60, loss = 0.14379557967185974
iteration 61, loss = 0.072956383228302
iteration 62, loss = 0.039078667759895325
iteration 63, loss = 0.04033546522259712
iteration 64, loss = 0.03458992764353752
iteration 65, loss = 0.0017915498465299606
iteration 66, loss = 0.14751499891281128
iteration 67, loss = 0.06117287650704384
iteration 68, loss = 0.006450363900512457
iteration 69, loss = 0.13739363849163055
iteration 70, loss = 0.02792041376233101
iteration 71, loss = 0.0636395812034607
iteration 72, loss = 0.06346132606267929
iteration 73, loss = 0.023122606799006462
iteration 74, loss = 0.04023823142051697
iteration 75, loss = 0.09367915242910385
iteration 76, loss = 0.00719771534204483
iteration 77, loss = 0.017485734075307846
iteration 78, loss = 0.015584728680551052
iteration 79, loss = 0.03494329750537872
iteration 80, loss = 0.014759567566215992
iteration 81, loss = 0.030387653037905693
iteration 82, loss = 0.011609639972448349
iteration 83, loss = 0.013135883957147598
iteration 84, loss = 0.007243335712701082
iteration 85, loss = 0.08551950007677078
iteration 86, loss = 0.0119403675198555
iteration 87, loss = 0.0935574546456337
iteration 88, loss = 0.03509256988763809
iteration 89, loss = 0.025758203119039536
iteration 90, loss = 0.09384014457464218
iteration 91, loss = 0.021031416952610016
iteration 92, loss = 0.011218802072107792
iteration 93, loss = 0.19860976934432983
iteration 94, loss = 0.0001927636913023889
iteration 95, loss = 0.2594096064567566
iteration 96, loss = 0.2532971501350403
iteration 97, loss = 0.044180646538734436
iteration 98, loss = 0.07918323576450348
iteration 99, loss = 0.1051471158862114
iteration 100, loss = 0.08922197669744492
iteration 101, loss = 0.039939023554325104
iteration 102, loss = 0.1297558844089508
iteration 103, loss = 0.025525692850351334
iteration 104, loss = 0.31207579374313354
iteration 105, loss = 0.055122166872024536
iteration 106, loss = 0.03341224789619446
iteration 107, loss = 0.01303311251103878
iteration 108, loss = 0.06753700971603394
iteration 109, loss = 0.15583637356758118
iteration 110, loss = 0.02796286530792713
iteration 111, loss = 0.002541369991376996
iteration 112, loss = 0.055816762149333954
iteration 113, loss = 0.11000505834817886
iteration 114, loss = 0.12651647627353668
iteration 115, loss = 0.02024441584944725
iteration 116, loss = 0.08043636381626129
iteration 117, loss = 0.031176680698990822
iteration 118, loss = 0.04191512614488602
iteration 119, loss = 0.0173684973269701
iteration 120, loss = 0.03267105296254158
iteration 121, loss = 0.0362386628985405
iteration 122, loss = 0.10501569509506226
iteration 123, loss = 0.06026461720466614
iteration 124, loss = 0.02750515751540661
iteration 125, loss = 0.018068203702569008
iteration 126, loss = 0.12168905884027481
iteration 127, loss = 0.07167650759220123
iteration 128, loss = 0.06795863062143326
iteration 129, loss = 0.03293303772807121
iteration 130, loss = 0.04651610180735588
iteration 131, loss = 0.045120030641555786
iteration 132, loss = 0.009930664673447609
iteration 133, loss = 0.0188821442425251
iteration 134, loss = 0.00821949727833271
iteration 135, loss = 0.01189953088760376
iteration 136, loss = 0.004667069762945175
iteration 137, loss = 0.002567392773926258
iteration 138, loss = 0.20882870256900787
iteration 139, loss = 0.0017613631207495928
iteration 140, loss = 0.0061620380729436874
iteration 141, loss = 0.004619797226041555
iteration 142, loss = 0.021598001942038536
iteration 143, loss = 0.014956152066588402
iteration 144, loss = 0.018170157447457314
iteration 145, loss = 0.034157220274209976
iteration 146, loss = 0.023738961666822433
iteration 147, loss = 0.007469445001333952
iteration 148, loss = 0.03681990131735802
iteration 149, loss = 0.03871212154626846
iteration 150, loss = 0.015500438399612904
iteration 151, loss = 0.020599069073796272
iteration 152, loss = 0.039744023233652115
iteration 153, loss = 0.011549387127161026
iteration 154, loss = 0.006086071953177452
iteration 155, loss = 0.07802865654230118
iteration 156, loss = 0.0030787214636802673
iteration 157, loss = 0.009231239557266235
iteration 158, loss = 0.07832664996385574
iteration 159, loss = 0.12045889347791672
iteration 160, loss = 0.014001601375639439
iteration 161, loss = 0.08359780162572861
iteration 162, loss = 0.025144239887595177
iteration 163, loss = 0.04765501618385315
iteration 164, loss = 0.04002026841044426
iteration 165, loss = 0.01186230406165123
iteration 166, loss = 0.01525854878127575
iteration 167, loss = 0.006127139087766409
iteration 168, loss = 0.09188412129878998
iteration 169, loss = 0.0217457115650177
iteration 170, loss = 0.06527456641197205
iteration 171, loss = 0.03496844321489334
iteration 172, loss = 0.05525932461023331
iteration 173, loss = 0.026089398190379143
iteration 174, loss = 0.10664962232112885
iteration 175, loss = 0.05763622373342514
iteration 176, loss = 0.007389850448817015
iteration 177, loss = 0.004671430680900812
iteration 178, loss = 0.09704432636499405
iteration 179, loss = 0.17975330352783203
iteration 180, loss = 0.11373453587293625
iteration 181, loss = 0.02197941392660141
iteration 182, loss = 0.08590585738420486
iteration 183, loss = 0.13052089512348175
iteration 184, loss = 0.01189197413623333
iteration 185, loss = 0.028118642047047615
iteration 186, loss = 0.15255357325077057
iteration 187, loss = 0.003877306589856744
iteration 188, loss = 0.0008468691958114505
iteration 189, loss = 0.044948380440473557
iteration 190, loss = 0.014085089787840843
iteration 191, loss = 0.19028718769550323
iteration 192, loss = 0.1954435557126999
iteration 193, loss = 0.12490065395832062
iteration 194, loss = 0.04121894761919975
iteration 195, loss = 0.01633492484688759
iteration 196, loss = 0.15951339900493622
iteration 197, loss = 0.172632098197937
iteration 198, loss = 0.006854949053376913
iteration 199, loss = 0.012277583591639996
iteration 200, loss = 0.0920260101556778
iteration 201, loss = 0.20021429657936096
iteration 202, loss = 0.0016243194695562124
iteration 203, loss = 0.0020558384712785482
iteration 204, loss = 0.36633870005607605
iteration 205, loss = 0.004564776550978422
iteration 206, loss = 0.0028357224073261023
iteration 207, loss = 0.15412892401218414
iteration 208, loss = 0.010655764490365982
iteration 209, loss = 0.1173926368355751
iteration 210, loss = 0.057154785841703415
iteration 211, loss = 0.036007627844810486
iteration 212, loss = 0.15980228781700134
iteration 213, loss = 0.0765899121761322
iteration 214, loss = 0.023228025063872337
iteration 215, loss = 0.009012960828840733
iteration 216, loss = 0.02504202350974083
iteration 217, loss = 0.19584344327449799
iteration 218, loss = 0.05893733352422714
iteration 219, loss = 0.00041702546877786517
iteration 220, loss = 0.0022679781541228294
iteration 221, loss = 0.01065985206514597
iteration 222, loss = 7.795892452122644e-05
iteration 223, loss = 0.00287396227940917
iteration 224, loss = 0.000868968025315553
iteration 225, loss = 0.0242737028747797
iteration 226, loss = 0.012929799035191536
iteration 227, loss = 0.011630011722445488
iteration 228, loss = 0.004005687776952982
iteration 229, loss = 0.10244890302419662
iteration 230, loss = 0.03194611892104149
iteration 231, loss = 0.10935044288635254
iteration 232, loss = 0.02193373069167137
iteration 233, loss = 0.005859793163836002
iteration 234, loss = 0.026369886472821236
iteration 235, loss = 0.0055792005732655525
iteration 236, loss = 0.006694164127111435
iteration 237, loss = 0.21488869190216064
iteration 238, loss = 0.05765771120786667
iteration 239, loss = 0.035289548337459564
iteration 240, loss = 0.060954127460718155
iteration 241, loss = 0.020604332908988
iteration 242, loss = 0.09906197339296341
iteration 243, loss = 0.06993598490953445
iteration 244, loss = 0.010887546464800835
iteration 245, loss = 0.04959908127784729
iteration 246, loss = 0.02703116089105606
iteration 247, loss = 0.3192208409309387
iteration 248, loss = 0.0018796260701492429
iteration 249, loss = 0.011604228988289833
iteration 250, loss = 0.02827920764684677
iteration 251, loss = 0.004747712053358555
iteration 252, loss = 0.12232092022895813
iteration 253, loss = 0.05503571406006813
iteration 254, loss = 0.08426237106323242
iteration 255, loss = 0.004647721536457539
iteration 256, loss = 0.010887469165027142
iteration 257, loss = 0.008379850536584854
iteration 258, loss = 0.1073005348443985
iteration 259, loss = 0.028119105845689774
iteration 260, loss = 0.08655485510826111
iteration 261, loss = 0.024272190406918526
iteration 262, loss = 0.040457479655742645
iteration 263, loss = 0.03995905816555023
iteration 264, loss = 0.03124789148569107
iteration 265, loss = 0.03471532464027405
iteration 266, loss = 0.03418288007378578
iteration 267, loss = 0.0022905145306140184
iteration 268, loss = 0.020334016531705856
iteration 269, loss = 0.003576908726245165
iteration 270, loss = 0.08503811806440353
iteration 271, loss = 0.031945742666721344
iteration 272, loss = 0.10063834488391876
iteration 273, loss = 0.07611031085252762
iteration 274, loss = 0.024731574580073357
iteration 275, loss = 0.007347652222961187
iteration 276, loss = 0.11838575452566147
iteration 277, loss = 0.023716745898127556
iteration 278, loss = 0.09848305583000183
iteration 279, loss = 0.009599810466170311
iteration 280, loss = 0.018550023436546326
iteration 281, loss = 0.04180586338043213
iteration 282, loss = 0.008280969224870205
iteration 283, loss = 0.1163225919008255
iteration 284, loss = 0.01608613319694996
iteration 285, loss = 0.01657514087855816
iteration 286, loss = 0.16453233361244202
iteration 287, loss = 0.06929520517587662
iteration 288, loss = 0.07529051601886749
iteration 289, loss = 0.039924923330545425
iteration 290, loss = 0.07279950380325317
iteration 291, loss = 0.04907222464680672
iteration 292, loss = 0.019320784136652946
iteration 293, loss = 0.016105342656373978
iteration 294, loss = 0.021049875766038895
iteration 295, loss = 0.008554806932806969
iteration 296, loss = 0.03454293683171272
iteration 297, loss = 0.03425300866365433
iteration 298, loss = 0.010588851757347584
iteration 299, loss = 0.03172093629837036

iteration 1, loss = 0.7721426486968994
iteration 2, loss = 0.7744126319885254
iteration 3, loss = 0.7683765292167664
iteration 4, loss = 0.7675374746322632
iteration 5, loss = 0.7740127444267273
iteration 6, loss = 0.7687157392501831
iteration 7, loss = 0.7659201622009277
iteration 8, loss = 0.7527043223381042
iteration 9, loss = 0.7661399841308594
iteration 10, loss = 0.7486461997032166
iteration 11, loss = 0.7431498765945435
iteration 12, loss = 0.7164097428321838
iteration 13, loss = 0.7280012369155884
iteration 14, loss = 0.7348969578742981
iteration 15, loss = 0.7167407274246216
iteration 16, loss = 0.7244489192962646
iteration 17, loss = 0.7097506523132324
iteration 18, loss = 0.7382505536079407
iteration 19, loss = 0.7342721223831177
iteration 20, loss = 0.7022879123687744
iteration 21, loss = 0.7186326384544373
iteration 22, loss = 0.7100307941436768
iteration 23, loss = 0.6760582327842712
iteration 24, loss = 0.709923267364502
iteration 25, loss = 0.676024317741394
iteration 26, loss = 0.6884995698928833
iteration 27, loss = 0.6941236257553101
iteration 28, loss = 0.6811573505401611
iteration 29, loss = 0.6432386636734009
iteration 30, loss = 0.7226179838180542
iteration 31, loss = 0.6926883459091187
iteration 32, loss = 0.6905548572540283
iteration 33, loss = 0.6775141954421997
iteration 34, loss = 0.6858248710632324
iteration 35, loss = 0.6747557520866394
iteration 36, loss = 0.6783071756362915
iteration 37, loss = 0.5941134691238403
iteration 38, loss = 0.6687235832214355
iteration 39, loss = 0.6488840579986572
iteration 40, loss = 0.6855834722518921
iteration 41, loss = 0.6336348652839661
iteration 42, loss = 0.6484783887863159
iteration 43, loss = 0.6435807943344116
iteration 44, loss = 0.6690924763679504
iteration 45, loss = 0.5950790643692017
iteration 46, loss = 0.6007260084152222
iteration 47, loss = 0.6203452348709106
iteration 48, loss = 0.6599522829055786
iteration 49, loss = 0.6140936017036438
iteration 50, loss = 0.6336944103240967
iteration 51, loss = 0.6390403509140015
iteration 52, loss = 0.5824446082115173
iteration 53, loss = 0.6203347444534302
iteration 54, loss = 0.6456459164619446
iteration 55, loss = 0.5800108909606934
iteration 56, loss = 0.6205772161483765
iteration 57, loss = 0.6014106273651123
iteration 58, loss = 0.6394143104553223
iteration 59, loss = 0.5937131643295288
iteration 60, loss = 0.5614230036735535
iteration 61, loss = 0.6746777892112732
iteration 62, loss = 0.6604688167572021
iteration 63, loss = 0.6144734025001526
iteration 64, loss = 0.675484299659729
iteration 65, loss = 0.540921688079834
iteration 66, loss = 0.5658992528915405
iteration 67, loss = 0.5838190317153931
iteration 68, loss = 0.5905200242996216
iteration 69, loss = 0.5976624488830566
iteration 70, loss = 0.5441367626190186
iteration 71, loss = 0.5790095925331116
iteration 72, loss = 0.5485326647758484
iteration 73, loss = 0.5035234689712524
iteration 74, loss = 0.5652201771736145
iteration 75, loss = 0.6257286071777344
iteration 76, loss = 0.5416289567947388
iteration 77, loss = 0.5743404626846313
iteration 78, loss = 0.5528945922851562
iteration 79, loss = 0.619775652885437
iteration 80, loss = 0.6561780571937561
iteration 81, loss = 0.5571202635765076
iteration 82, loss = 0.5898926854133606
iteration 83, loss = 0.5516080260276794
iteration 84, loss = 0.576228678226471
iteration 85, loss = 0.6048086881637573
iteration 86, loss = 0.5496722459793091
iteration 87, loss = 0.6232258677482605
iteration 88, loss = 0.602134108543396
iteration 89, loss = 0.5786775946617126
iteration 90, loss = 0.5904833674430847
iteration 91, loss = 0.5763932466506958
iteration 92, loss = 0.6064691543579102
iteration 93, loss = 0.6230674386024475
iteration 94, loss = 0.5222957134246826
iteration 95, loss = 0.5716963410377502
iteration 96, loss = 0.5672858953475952
iteration 97, loss = 0.6038139462471008
iteration 98, loss = 0.628266453742981
iteration 99, loss = 0.6649458408355713
iteration 100, loss = 0.515674352645874
iteration 101, loss = 0.55892014503479
iteration 102, loss = 0.5579586029052734
iteration 103, loss = 0.5708073377609253
iteration 104, loss = 0.5949839353561401
iteration 105, loss = 0.5371071100234985
iteration 106, loss = 0.4607005715370178
iteration 107, loss = 0.5614815950393677
iteration 108, loss = 0.6402198672294617
iteration 109, loss = 0.48998790979385376
iteration 110, loss = 0.6384657621383667
iteration 111, loss = 0.5785643458366394
iteration 112, loss = 0.5341252088546753
iteration 113, loss = 0.6613654494285583
iteration 114, loss = 0.6185675859451294
iteration 115, loss = 0.48323938250541687
iteration 116, loss = 0.5782016515731812
iteration 117, loss = 0.6093751192092896
iteration 118, loss = 0.45697423815727234
iteration 119, loss = 0.525428295135498
iteration 120, loss = 0.41231098771095276
iteration 121, loss = 0.5634545087814331
iteration 122, loss = 0.5507837533950806
iteration 123, loss = 0.537135124206543
iteration 124, loss = 0.5187271237373352
iteration 125, loss = 0.5257545113563538
iteration 126, loss = 0.5198057889938354
iteration 127, loss = 0.44979071617126465
iteration 128, loss = 0.6129438877105713
iteration 129, loss = 0.7152158617973328
iteration 130, loss = 0.5318564176559448
iteration 131, loss = 0.4811960756778717
iteration 132, loss = 0.550571084022522
iteration 133, loss = 0.4812312424182892
iteration 134, loss = 0.5310806035995483
iteration 135, loss = 0.6652258634567261
iteration 136, loss = 0.5021076202392578
iteration 137, loss = 0.44207897782325745
iteration 138, loss = 0.4538099765777588
iteration 139, loss = 0.6408397555351257
iteration 140, loss = 0.4928773045539856
iteration 141, loss = 0.5384529829025269
iteration 142, loss = 0.49375373125076294
iteration 143, loss = 0.5247066020965576
iteration 144, loss = 0.49375414848327637
iteration 145, loss = 0.5496795773506165
iteration 146, loss = 0.47841060161590576
iteration 147, loss = 0.4030188322067261
iteration 148, loss = 0.6201359033584595
iteration 149, loss = 0.4370883107185364
iteration 150, loss = 0.5531960725784302
iteration 151, loss = 0.4747483730316162
iteration 152, loss = 0.5364601612091064
iteration 153, loss = 0.5480770468711853
iteration 154, loss = 0.498354434967041
iteration 155, loss = 0.4419359564781189
iteration 156, loss = 0.5623549222946167
iteration 157, loss = 0.524712085723877
iteration 158, loss = 0.47620975971221924
iteration 159, loss = 0.442226380109787
iteration 160, loss = 0.5707390904426575
iteration 161, loss = 0.46188151836395264
iteration 162, loss = 0.4181677997112274
iteration 163, loss = 0.5107228755950928
iteration 164, loss = 0.5011850595474243
iteration 165, loss = 0.45278358459472656
iteration 166, loss = 0.4448488652706146
iteration 167, loss = 0.42675602436065674
iteration 168, loss = 0.6719491481781006
iteration 169, loss = 0.4898937940597534
iteration 170, loss = 0.4357335865497589
iteration 171, loss = 0.43234580755233765
iteration 172, loss = 0.6066375970840454
iteration 173, loss = 0.5967575907707214
iteration 174, loss = 0.47328078746795654
iteration 175, loss = 0.5344036221504211
iteration 176, loss = 0.6419286727905273
iteration 177, loss = 0.4627748727798462
iteration 178, loss = 0.5859193801879883
iteration 179, loss = 0.5429197549819946
iteration 180, loss = 0.4716647267341614
iteration 181, loss = 0.5779958963394165
iteration 182, loss = 0.48290473222732544
iteration 183, loss = 0.4161761403083801
iteration 184, loss = 0.3969728350639343
iteration 185, loss = 0.46753907203674316
iteration 186, loss = 0.6100778579711914
iteration 187, loss = 0.5207865834236145
iteration 188, loss = 0.5232486724853516
iteration 189, loss = 0.5413670539855957
iteration 190, loss = 0.5424623489379883
iteration 191, loss = 0.445652574300766
iteration 192, loss = 0.546981692314148
iteration 193, loss = 0.33254316449165344
iteration 194, loss = 0.41899290680885315
iteration 195, loss = 0.4894833564758301
iteration 196, loss = 0.4564853608608246
iteration 197, loss = 0.4652664363384247
iteration 198, loss = 0.5209226012229919
iteration 199, loss = 0.5908676385879517
iteration 200, loss = 0.5600600838661194
iteration 201, loss = 0.41077202558517456
iteration 202, loss = 0.401287317276001
iteration 203, loss = 0.4969802498817444
iteration 204, loss = 0.5736105442047119
iteration 205, loss = 0.4041728377342224
iteration 206, loss = 0.522926926612854
iteration 207, loss = 0.43902280926704407
iteration 208, loss = 0.38252753019332886
iteration 209, loss = 0.44682079553604126
iteration 210, loss = 0.3261655569076538
iteration 211, loss = 0.5113314986228943
iteration 212, loss = 0.4534032642841339
iteration 213, loss = 0.4216368496417999
iteration 214, loss = 0.4487273395061493
iteration 215, loss = 0.36041998863220215
iteration 216, loss = 0.5265336036682129
iteration 217, loss = 0.5013500452041626
iteration 218, loss = 0.42886292934417725
iteration 219, loss = 0.5014275312423706
iteration 220, loss = 0.5146746039390564
iteration 221, loss = 0.563886821269989
iteration 222, loss = 0.4030884802341461
iteration 223, loss = 0.45651039481163025
iteration 224, loss = 0.5497220754623413
iteration 225, loss = 0.5468658804893494
iteration 226, loss = 0.4282311797142029
iteration 227, loss = 0.4423585534095764
iteration 228, loss = 0.39442741870880127
iteration 229, loss = 0.4788210391998291
iteration 230, loss = 0.5478823184967041
iteration 231, loss = 0.427621454000473
iteration 232, loss = 0.40812504291534424
iteration 233, loss = 0.371599406003952
iteration 234, loss = 0.47474586963653564
iteration 235, loss = 0.30046990513801575
iteration 236, loss = 0.47488123178482056
iteration 237, loss = 0.3923195004463196
iteration 238, loss = 0.36708641052246094
iteration 239, loss = 0.4646869897842407
iteration 240, loss = 0.40474915504455566
iteration 241, loss = 0.38982853293418884
iteration 242, loss = 0.35018038749694824
iteration 243, loss = 0.42078185081481934
iteration 244, loss = 0.3465650677680969
iteration 245, loss = 0.4703930616378784
iteration 246, loss = 0.4046449661254883
iteration 247, loss = 0.27396464347839355
iteration 248, loss = 0.43963372707366943
iteration 249, loss = 0.38758987188339233
iteration 250, loss = 0.5858895182609558
iteration 251, loss = 0.46625322103500366
iteration 252, loss = 0.3737720549106598
iteration 253, loss = 0.3776857256889343
iteration 254, loss = 0.45776432752609253
iteration 255, loss = 0.4131786525249481
iteration 256, loss = 0.35477378964424133
iteration 257, loss = 0.27968665957450867
iteration 258, loss = 0.3411198854446411
iteration 259, loss = 0.397513747215271
iteration 260, loss = 0.6212022304534912
iteration 261, loss = 0.401027113199234
iteration 262, loss = 0.4416213035583496
iteration 263, loss = 0.3693528175354004
iteration 264, loss = 0.44164901971817017
iteration 265, loss = 0.6342514753341675
iteration 266, loss = 0.3731454908847809
iteration 267, loss = 0.3475881814956665
iteration 268, loss = 0.3237273097038269
iteration 269, loss = 0.2556944787502289
iteration 270, loss = 0.3724026679992676
iteration 271, loss = 0.623772382736206
iteration 272, loss = 0.26962435245513916
iteration 273, loss = 0.43021100759506226
iteration 274, loss = 0.6403315663337708
iteration 275, loss = 0.4784042537212372
iteration 276, loss = 0.38091176748275757
iteration 277, loss = 0.6168074607849121
iteration 278, loss = 0.5433508157730103
iteration 279, loss = 0.618818998336792
iteration 280, loss = 0.4744868874549866
iteration 281, loss = 0.40573054552078247
iteration 282, loss = 0.3412981629371643
iteration 283, loss = 0.5194586515426636
iteration 284, loss = 0.29428622126579285
iteration 285, loss = 0.2946455478668213
iteration 286, loss = 0.26087719202041626
iteration 287, loss = 0.3214406371116638
iteration 288, loss = 0.3836129903793335
iteration 289, loss = 0.4720552861690521
iteration 290, loss = 0.3490169048309326
iteration 291, loss = 0.46977677941322327
iteration 292, loss = 0.4633217751979828
iteration 293, loss = 0.4687286615371704
iteration 294, loss = 0.4134216606616974
iteration 295, loss = 0.562644362449646
iteration 296, loss = 0.273853600025177
iteration 297, loss = 0.4505937695503235
iteration 298, loss = 0.3969244062900543
iteration 299, loss = 0.4061599373817444
iteration 300, loss = 0.4719456434249878
iteration 1, loss = 0.37384694814682007
iteration 2, loss = 0.40397390723228455
iteration 3, loss = 0.6127039790153503
iteration 4, loss = 0.5408973097801208
iteration 5, loss = 0.2573329210281372
iteration 6, loss = 0.2852171063423157
iteration 7, loss = 0.3833628296852112
iteration 8, loss = 0.27849289774894714
iteration 9, loss = 0.48365840315818787
iteration 10, loss = 0.2358458936214447
iteration 11, loss = 0.5565553307533264
iteration 12, loss = 0.24906134605407715
iteration 13, loss = 0.32932397723197937
iteration 14, loss = 0.29299330711364746
iteration 15, loss = 0.4607129693031311
iteration 16, loss = 0.3727102279663086
iteration 17, loss = 0.34015095233917236
iteration 18, loss = 0.4698561728000641
iteration 19, loss = 0.3523615896701813
iteration 20, loss = 0.3044548034667969
iteration 21, loss = 0.4000560939311981
iteration 22, loss = 0.4263184666633606
iteration 23, loss = 0.31042587757110596
iteration 24, loss = 0.29640674591064453
iteration 25, loss = 0.3005083501338959
iteration 26, loss = 0.3473615050315857
iteration 27, loss = 0.2987510561943054
iteration 28, loss = 0.3392142057418823
iteration 29, loss = 0.23937223851680756
iteration 30, loss = 0.3200133144855499
iteration 31, loss = 0.2955426871776581
iteration 32, loss = 0.35348737239837646
iteration 33, loss = 0.40350839495658875
iteration 34, loss = 0.2914535403251648
iteration 35, loss = 0.29959648847579956
iteration 36, loss = 0.4214804768562317
iteration 37, loss = 0.4533074200153351
iteration 38, loss = 0.43721380829811096
iteration 39, loss = 0.6277058124542236
iteration 40, loss = 0.34695759415626526
iteration 41, loss = 0.24607425928115845
iteration 42, loss = 0.2997158467769623
iteration 43, loss = 0.22292660176753998
iteration 44, loss = 0.3430832028388977
iteration 45, loss = 0.3027954697608948
iteration 46, loss = 0.44673657417297363
iteration 47, loss = 0.37684768438339233
iteration 48, loss = 0.2181473821401596
iteration 49, loss = 0.3412288725376129
iteration 50, loss = 0.15404333174228668
iteration 51, loss = 0.3174706697463989
iteration 52, loss = 0.3920854926109314
iteration 53, loss = 0.3689272403717041
iteration 54, loss = 0.2295631766319275
iteration 55, loss = 0.45429953932762146
iteration 56, loss = 0.3912218511104584
iteration 57, loss = 0.3271259367465973
iteration 58, loss = 0.21391403675079346
iteration 59, loss = 0.5011732578277588
iteration 60, loss = 0.37308090925216675
iteration 61, loss = 0.3819953203201294
iteration 62, loss = 0.3537564277648926
iteration 63, loss = 0.21895506978034973
iteration 64, loss = 0.3638371527194977
iteration 65, loss = 0.18185800313949585
iteration 66, loss = 0.2789872884750366
iteration 67, loss = 0.27951380610466003
iteration 68, loss = 0.21146386861801147
iteration 69, loss = 0.21487295627593994
iteration 70, loss = 0.4276743531227112
iteration 71, loss = 0.6654462814331055
iteration 72, loss = 0.3180731534957886
iteration 73, loss = 0.17215877771377563
iteration 74, loss = 0.32346487045288086
iteration 75, loss = 0.4603680968284607
iteration 76, loss = 0.28666794300079346
iteration 77, loss = 0.28658169507980347
iteration 78, loss = 0.2897859215736389
iteration 79, loss = 0.21449823677539825
iteration 80, loss = 0.2789715528488159
iteration 81, loss = 0.40598800778388977
iteration 82, loss = 0.23625248670578003
iteration 83, loss = 0.12889495491981506
iteration 84, loss = 0.186312735080719
iteration 85, loss = 0.4141429662704468
iteration 86, loss = 0.20301491022109985
iteration 87, loss = 0.16170595586299896
iteration 88, loss = 0.19020800292491913
iteration 89, loss = 0.48185867071151733
iteration 90, loss = 0.22886362671852112
iteration 91, loss = 0.2711000144481659
iteration 92, loss = 0.25682175159454346
iteration 93, loss = 0.20026114583015442
iteration 94, loss = 0.5332947969436646
iteration 95, loss = 0.16655614972114563
iteration 96, loss = 0.4754309058189392
iteration 97, loss = 0.2232232391834259
iteration 98, loss = 0.32190603017807007
iteration 99, loss = 0.24255377054214478
iteration 100, loss = 0.2602277100086212
iteration 101, loss = 0.22329220175743103
iteration 102, loss = 0.33329564332962036
iteration 103, loss = 0.2934611737728119
iteration 104, loss = 0.4197242558002472
iteration 105, loss = 0.5286667346954346
iteration 106, loss = 0.17478889226913452
iteration 107, loss = 0.13220855593681335
iteration 108, loss = 0.1855011284351349
iteration 109, loss = 0.435652494430542
iteration 110, loss = 0.45176398754119873
iteration 111, loss = 0.23141103982925415
iteration 112, loss = 0.3469823896884918
iteration 113, loss = 0.23749059438705444
iteration 114, loss = 0.19211699068546295
iteration 115, loss = 0.1924460530281067
iteration 116, loss = 0.14119814336299896
iteration 117, loss = 0.13255897164344788
iteration 118, loss = 0.20708611607551575
iteration 119, loss = 0.19640567898750305
iteration 120, loss = 0.6885185241699219
iteration 121, loss = 0.16698375344276428
iteration 122, loss = 0.19232843816280365
iteration 123, loss = 0.2569880187511444
iteration 124, loss = 0.4280851483345032
iteration 125, loss = 0.17546841502189636
iteration 126, loss = 0.18266798555850983
iteration 127, loss = 0.2764751613140106
iteration 128, loss = 0.22286629676818848
iteration 129, loss = 0.12761391699314117
iteration 130, loss = 0.3382527828216553
iteration 131, loss = 0.18020638823509216
iteration 132, loss = 0.16728758811950684
iteration 133, loss = 0.26520833373069763
iteration 134, loss = 0.11177361011505127
iteration 135, loss = 0.12793974578380585
iteration 136, loss = 0.3076581358909607
iteration 137, loss = 0.15826676785945892
iteration 138, loss = 0.15616554021835327
iteration 139, loss = 0.15603458881378174
iteration 140, loss = 0.2752617597579956
iteration 141, loss = 0.5786269903182983
iteration 142, loss = 0.20399439334869385
iteration 143, loss = 0.13213378190994263
iteration 144, loss = 0.27281680703163147
iteration 145, loss = 0.1633959859609604
iteration 146, loss = 0.411607563495636
iteration 147, loss = 0.1530752182006836
iteration 148, loss = 0.16747313737869263
iteration 149, loss = 0.25720372796058655
iteration 150, loss = 0.15689189732074738
iteration 151, loss = 0.4072049558162689
iteration 152, loss = 0.1892714500427246
iteration 153, loss = 0.17542827129364014
iteration 154, loss = 0.3449007570743561
iteration 155, loss = 0.1855572760105133
iteration 156, loss = 0.4175967872142792
iteration 157, loss = 0.3814235031604767
iteration 158, loss = 0.3992338478565216
iteration 159, loss = 0.13385532796382904
iteration 160, loss = 0.13382939994335175
iteration 161, loss = 0.2158631980419159
iteration 162, loss = 0.13885702192783356
iteration 163, loss = 0.20827887952327728
iteration 164, loss = 0.1215868666768074
iteration 165, loss = 0.131819948554039
iteration 166, loss = 0.18007753789424896
iteration 167, loss = 0.19679194688796997
iteration 168, loss = 0.46751752495765686
iteration 169, loss = 0.40038323402404785
iteration 170, loss = 0.4525562524795532
iteration 171, loss = 0.11623422056436539
iteration 172, loss = 0.18431785702705383
iteration 173, loss = 0.08279503136873245
iteration 174, loss = 0.19198842346668243
iteration 175, loss = 0.12079215049743652
iteration 176, loss = 0.469746470451355
iteration 177, loss = 0.37546271085739136
iteration 178, loss = 0.15058907866477966
iteration 179, loss = 0.09185805171728134
iteration 180, loss = 0.16605959832668304
iteration 181, loss = 0.4115608334541321
iteration 182, loss = 0.35883834958076477
iteration 183, loss = 0.6933279037475586
iteration 184, loss = 0.12775152921676636
iteration 185, loss = 0.18406489491462708
iteration 186, loss = 0.09626055508852005
iteration 187, loss = 0.08502659201622009
iteration 188, loss = 0.43122759461402893
iteration 189, loss = 0.13230165839195251
iteration 190, loss = 0.18562594056129456
iteration 191, loss = 0.20609506964683533
iteration 192, loss = 0.19865834712982178
iteration 193, loss = 0.44183099269866943
iteration 194, loss = 0.1661258339881897
iteration 195, loss = 0.47101467847824097
iteration 196, loss = 0.14225059747695923
iteration 197, loss = 0.1073046624660492
iteration 198, loss = 0.15869030356407166
iteration 199, loss = 0.37331730127334595
iteration 200, loss = 0.4355210065841675
iteration 201, loss = 0.40702274441719055
iteration 202, loss = 0.20237800478935242
iteration 203, loss = 0.09977258741855621
iteration 204, loss = 0.14513564109802246
iteration 205, loss = 0.18431544303894043
iteration 206, loss = 0.11352819204330444
iteration 207, loss = 0.16182774305343628
iteration 208, loss = 0.14801709353923798
iteration 209, loss = 0.14041352272033691
iteration 210, loss = 0.17218086123466492
iteration 211, loss = 0.12025532126426697
iteration 212, loss = 0.1480596959590912
iteration 213, loss = 0.16007450222969055
iteration 214, loss = 0.18562649190425873
iteration 215, loss = 0.11231957376003265
iteration 216, loss = 0.12613406777381897
iteration 217, loss = 0.06824633479118347
iteration 218, loss = 0.18227699398994446
iteration 219, loss = 0.16503170132637024
iteration 220, loss = 0.1456184834241867
iteration 221, loss = 0.09183049947023392
iteration 222, loss = 0.10359591245651245
iteration 223, loss = 0.10181429982185364
iteration 224, loss = 0.1330694556236267
iteration 225, loss = 0.09338323771953583
iteration 226, loss = 0.14924994111061096
iteration 227, loss = 0.6373839378356934
iteration 228, loss = 0.502784013748169
iteration 229, loss = 0.1053556352853775
iteration 230, loss = 0.17687265574932098
iteration 231, loss = 0.1666763573884964
iteration 232, loss = 0.19632059335708618
iteration 233, loss = 0.09100185334682465
iteration 234, loss = 0.12144187837839127
iteration 235, loss = 0.2014816254377365
iteration 236, loss = 0.3903912305831909
iteration 237, loss = 0.07448189705610275
iteration 238, loss = 0.16177497804164886
iteration 239, loss = 0.36800283193588257
iteration 240, loss = 0.5193076729774475
iteration 241, loss = 0.3564067780971527
iteration 242, loss = 0.11745478957891464
iteration 243, loss = 0.38218435645103455
iteration 244, loss = 0.4336182475090027
iteration 245, loss = 0.1845902055501938
iteration 246, loss = 0.1343046873807907
iteration 247, loss = 0.13755913078784943
iteration 248, loss = 0.15915220975875854
iteration 249, loss = 0.10894160717725754
iteration 250, loss = 0.07058948278427124
iteration 251, loss = 0.23942655324935913
iteration 252, loss = 0.24537356197834015
iteration 253, loss = 0.41920214891433716
iteration 254, loss = 0.14621520042419434
iteration 255, loss = 0.39134806394577026
iteration 256, loss = 0.05647176131606102
iteration 257, loss = 0.7257224321365356
iteration 258, loss = 0.1712331622838974
iteration 259, loss = 0.137253075838089
iteration 260, loss = 0.17524296045303345
iteration 261, loss = 0.10430663824081421
iteration 262, loss = 0.20440635085105896
iteration 263, loss = 0.15843498706817627
iteration 264, loss = 0.10798413306474686
iteration 265, loss = 0.1372966170310974
iteration 266, loss = 0.10506173968315125
iteration 267, loss = 0.07345432043075562
iteration 268, loss = 0.12542766332626343
iteration 269, loss = 0.09444541484117508
iteration 270, loss = 0.1227891594171524
iteration 271, loss = 0.08962582051753998
iteration 272, loss = 0.07392837852239609
iteration 273, loss = 0.1454126387834549
iteration 274, loss = 0.3741220533847809
iteration 275, loss = 0.08158759772777557
iteration 276, loss = 0.15841887891292572
iteration 277, loss = 0.060053225606679916
iteration 278, loss = 0.1138889342546463
iteration 279, loss = 0.44977882504463196
iteration 280, loss = 0.38702109456062317
iteration 281, loss = 0.08719370514154434
iteration 282, loss = 0.06488663703203201
iteration 283, loss = 0.42900779843330383
iteration 284, loss = 0.12901178002357483
iteration 285, loss = 0.3579884469509125
iteration 286, loss = 0.6609714031219482
iteration 287, loss = 0.11138398945331573
iteration 288, loss = 0.12773695588111877
iteration 289, loss = 0.05968988686800003
iteration 290, loss = 0.10359229147434235
iteration 291, loss = 0.047266118228435516
iteration 292, loss = 0.08107328414916992
iteration 293, loss = 0.047172658145427704
iteration 294, loss = 0.3654974102973938
iteration 295, loss = 0.06531552225351334
iteration 296, loss = 0.3728077709674835
iteration 297, loss = 0.06285322457551956
iteration 298, loss = 0.08546404540538788
iteration 299, loss = 0.45865100622177124
iteration 300, loss = 0.06680484861135483
iteration 1, loss = 0.04667973518371582
iteration 2, loss = 0.061593253165483475
iteration 3, loss = 0.13612383604049683
iteration 4, loss = 0.0822443813085556
iteration 5, loss = 0.37187352776527405
iteration 6, loss = 0.09222559630870819
iteration 7, loss = 0.06438903510570526
iteration 8, loss = 0.37304559350013733
iteration 9, loss = 0.3756062984466553
iteration 10, loss = 0.06184548884630203
iteration 11, loss = 0.10031064599752426
iteration 12, loss = 0.10700413584709167
iteration 13, loss = 0.09618858993053436
iteration 14, loss = 0.04725608974695206
iteration 15, loss = 0.41743993759155273
iteration 16, loss = 0.5014735460281372
iteration 17, loss = 0.1105717122554779
iteration 18, loss = 0.09721106290817261
iteration 19, loss = 0.08298851549625397
iteration 20, loss = 0.11409012973308563
iteration 21, loss = 0.08888591825962067
iteration 22, loss = 0.09757426381111145
iteration 23, loss = 0.057995863258838654
iteration 24, loss = 0.07370851933956146
iteration 25, loss = 0.11911799013614655
iteration 26, loss = 0.3847702145576477
iteration 27, loss = 0.3690751791000366
iteration 28, loss = 0.4499484896659851
iteration 29, loss = 0.09429123997688293
iteration 30, loss = 0.35647672414779663
iteration 31, loss = 0.3058409094810486
iteration 32, loss = 0.06249547377228737
iteration 33, loss = 0.3900136351585388
iteration 34, loss = 0.5064425468444824
iteration 35, loss = 0.05552922189235687
iteration 36, loss = 0.047977857291698456
iteration 37, loss = 0.10056407004594803
iteration 38, loss = 0.06117691472172737
iteration 39, loss = 0.07243701815605164
iteration 40, loss = 0.09743204712867737
iteration 41, loss = 0.4026266932487488
iteration 42, loss = 0.08097635209560394
iteration 43, loss = 0.5325213670730591
iteration 44, loss = 0.3759396970272064
iteration 45, loss = 0.07257971167564392
iteration 46, loss = 0.23628391325473785
iteration 47, loss = 0.36996781826019287
iteration 48, loss = 0.07343021035194397
iteration 49, loss = 0.39661720395088196
iteration 50, loss = 0.036112140864133835
iteration 51, loss = 0.10796216875314713
iteration 52, loss = 0.04077964276075363
iteration 53, loss = 0.5037956833839417
iteration 54, loss = 0.035278551280498505
iteration 55, loss = 0.03781682252883911
iteration 56, loss = 0.3701002299785614
iteration 57, loss = 0.4399687647819519
iteration 58, loss = 0.12681539356708527
iteration 59, loss = 0.45292598009109497
iteration 60, loss = 0.05022542178630829
iteration 61, loss = 0.04034551605582237
iteration 62, loss = 0.04430155083537102
iteration 63, loss = 0.41206854581832886
iteration 64, loss = 0.03956536948680878
iteration 65, loss = 0.38703086972236633
iteration 66, loss = 0.10474374145269394
iteration 67, loss = 0.0974714532494545
iteration 68, loss = 0.05692800506949425
iteration 69, loss = 0.11168839782476425
iteration 70, loss = 0.05228825658559799
iteration 71, loss = 0.36781901121139526
iteration 72, loss = 0.11609059572219849
iteration 73, loss = 0.3985863924026489
iteration 74, loss = 0.0503157339990139
iteration 75, loss = 0.4125553071498871
iteration 76, loss = 0.054115984588861465
iteration 77, loss = 0.07550477236509323
iteration 78, loss = 0.030820034444332123
iteration 79, loss = 0.038307953625917435
iteration 80, loss = 0.5483644008636475
iteration 81, loss = 0.05145964026451111
iteration 82, loss = 0.08738239109516144
iteration 83, loss = 0.10820040106773376
iteration 84, loss = 0.09456242620944977
iteration 85, loss = 0.09693923592567444
iteration 86, loss = 0.09567003697156906
iteration 87, loss = 0.09171738475561142
iteration 88, loss = 0.0461927130818367
iteration 89, loss = 0.04328456148505211
iteration 90, loss = 0.06596173346042633
iteration 91, loss = 0.07403003424406052
iteration 92, loss = 0.06286874413490295
iteration 93, loss = 0.028281189501285553
iteration 94, loss = 0.04815775156021118
iteration 95, loss = 0.41247838735580444
iteration 96, loss = 0.045326221734285355
iteration 97, loss = 0.06173296645283699
iteration 98, loss = 0.7249202132225037
iteration 99, loss = 0.10853210091590881
iteration 100, loss = 0.09700741618871689
iteration 101, loss = 0.05330278351902962
iteration 102, loss = 0.17208285629749298
iteration 103, loss = 0.04802553728222847
iteration 104, loss = 0.4548296332359314
iteration 105, loss = 0.3949107527732849
iteration 106, loss = 0.04016128554940224
iteration 107, loss = 0.079880490899086
iteration 108, loss = 0.08642572164535522
iteration 109, loss = 0.11563827097415924
iteration 110, loss = 0.4132607877254486
iteration 111, loss = 0.03441622853279114
iteration 112, loss = 0.37220537662506104
iteration 113, loss = 0.06596876680850983
iteration 114, loss = 0.037599388509988785
iteration 115, loss = 0.3854700028896332
iteration 116, loss = 0.05045890063047409
iteration 117, loss = 0.06074900925159454
iteration 118, loss = 0.3848564624786377
iteration 119, loss = 0.04959910362958908
iteration 120, loss = 0.06813611090183258
iteration 121, loss = 0.05962252616882324
iteration 122, loss = 0.03769815340638161
iteration 123, loss = 0.10022225975990295
iteration 124, loss = 0.4256645441055298
iteration 125, loss = 0.06286358833312988
iteration 126, loss = 0.3811133801937103
iteration 127, loss = 0.4443393349647522
iteration 128, loss = 0.028210751712322235
iteration 129, loss = 0.052664291113615036
iteration 130, loss = 0.3772379457950592
iteration 131, loss = 0.07734845578670502
iteration 132, loss = 0.07285161316394806
iteration 133, loss = 0.3909723162651062
iteration 134, loss = 0.03710116073489189
iteration 135, loss = 0.12474837899208069
iteration 136, loss = 0.05801435559988022
iteration 137, loss = 0.07552918791770935
iteration 138, loss = 0.024812886491417885
iteration 139, loss = 0.07454726099967957
iteration 140, loss = 0.38968437910079956
iteration 141, loss = 0.10393424332141876
iteration 142, loss = 0.5158483386039734
iteration 143, loss = 0.08235867321491241
iteration 144, loss = 0.05398784950375557
iteration 145, loss = 0.03864964842796326
iteration 146, loss = 0.04058127850294113
iteration 147, loss = 0.05471391603350639
iteration 148, loss = 0.4275602698326111
iteration 149, loss = 0.09170156717300415
iteration 150, loss = 0.07769178599119186
iteration 151, loss = 0.4228668510913849
iteration 152, loss = 0.19960598647594452
iteration 153, loss = 0.023824840784072876
iteration 154, loss = 0.05299359932541847
iteration 155, loss = 0.03319040313363075
iteration 156, loss = 0.06260665506124496
iteration 157, loss = 0.0671207532286644
iteration 158, loss = 0.3739938735961914
iteration 159, loss = 0.07042425125837326
iteration 160, loss = 0.052751436829566956
iteration 161, loss = 0.04216168075799942
iteration 162, loss = 0.5360462069511414
iteration 163, loss = 0.03638990595936775
iteration 164, loss = 0.0350373312830925
iteration 165, loss = 0.05728237330913544
iteration 166, loss = 0.042986493557691574
iteration 167, loss = 0.06236691027879715
iteration 168, loss = 0.39438295364379883
iteration 169, loss = 0.040852732956409454
iteration 170, loss = 0.030857570469379425
iteration 171, loss = 0.3741114139556885
iteration 172, loss = 0.03945126384496689
iteration 173, loss = 0.07521200180053711
iteration 174, loss = 0.04748740792274475
iteration 175, loss = 0.1539696305990219
iteration 176, loss = 0.03299623727798462
iteration 177, loss = 0.034125059843063354
iteration 178, loss = 0.030105922371149063
iteration 179, loss = 0.03040061891078949
iteration 180, loss = 0.04651786386966705
iteration 181, loss = 0.08706497400999069
iteration 182, loss = 0.04780331626534462
iteration 183, loss = 0.3838147521018982
iteration 184, loss = 0.047343384474515915
iteration 185, loss = 0.0523214116692543
iteration 186, loss = 0.7432966828346252
iteration 187, loss = 0.0408986359834671
iteration 188, loss = 0.02808089554309845
iteration 189, loss = 0.021409768611192703
iteration 190, loss = 0.041721612215042114
iteration 191, loss = 0.3870300054550171
iteration 192, loss = 0.39647799730300903
iteration 193, loss = 0.43281590938568115
iteration 194, loss = 0.05082244798541069
iteration 195, loss = 0.026601973921060562
iteration 196, loss = 0.029529426246881485
iteration 197, loss = 0.38968366384506226
iteration 198, loss = 0.06318605691194534
iteration 199, loss = 0.4273054599761963
iteration 200, loss = 0.043300047516822815
iteration 201, loss = 0.03404760733246803
iteration 202, loss = 0.7853965759277344
iteration 203, loss = 0.023973021656274796
iteration 204, loss = 0.04880927503108978
iteration 205, loss = 0.05341365933418274
iteration 206, loss = 0.08134715259075165
iteration 207, loss = 0.07067760825157166
iteration 208, loss = 0.029860354959964752
iteration 209, loss = 0.3773660957813263
iteration 210, loss = 0.4681819975376129
iteration 211, loss = 0.051241546869277954
iteration 212, loss = 0.38039684295654297
iteration 213, loss = 0.029346248134970665
iteration 214, loss = 0.050225820392370224
iteration 215, loss = 0.0507945641875267
iteration 216, loss = 0.03335689380764961
iteration 217, loss = 0.039312005043029785
iteration 218, loss = 0.04427165538072586
iteration 219, loss = 0.07042571902275085
iteration 220, loss = 0.04767719283699989
iteration 221, loss = 0.03171341121196747
iteration 222, loss = 0.08228866010904312
iteration 223, loss = 0.04658297076821327
iteration 224, loss = 0.029925186187028885
iteration 225, loss = 0.04147754982113838
iteration 226, loss = 0.02883245050907135
iteration 227, loss = 0.41015756130218506
iteration 228, loss = 0.03092147782444954
iteration 229, loss = 0.05973462760448456
iteration 230, loss = 0.3329838812351227
iteration 231, loss = 0.07333780825138092
iteration 232, loss = 0.028086010366678238
iteration 233, loss = 0.05659911036491394
iteration 234, loss = 0.0190420038998127
iteration 235, loss = 0.03835517168045044
iteration 236, loss = 0.4036335051059723
iteration 237, loss = 0.09627676755189896
iteration 238, loss = 0.04572317749261856
iteration 239, loss = 0.048265110701322556
iteration 240, loss = 0.0342433899641037
iteration 241, loss = 0.15583142638206482
iteration 242, loss = 0.028541434556245804
iteration 243, loss = 0.024412060156464577
iteration 244, loss = 0.7595570087432861
iteration 245, loss = 0.03334415704011917
iteration 246, loss = 0.15851393342018127
iteration 247, loss = 0.04402656853199005
iteration 248, loss = 0.046562906354665756
iteration 249, loss = 0.04054873436689377
iteration 250, loss = 0.4466509222984314
iteration 251, loss = 0.033057160675525665
iteration 252, loss = 0.0412956140935421
iteration 253, loss = 0.7829144597053528
iteration 254, loss = 0.05987885221838951
iteration 255, loss = 0.0452604666352272
iteration 256, loss = 0.03615200147032738
iteration 257, loss = 0.015462864190340042
iteration 258, loss = 0.027427565306425095
iteration 259, loss = 0.39931002259254456
iteration 260, loss = 0.03282468020915985
iteration 261, loss = 0.05101269111037254
iteration 262, loss = 0.4006154537200928
iteration 263, loss = 0.031356047838926315
iteration 264, loss = 0.41552409529685974
iteration 265, loss = 0.04615279659628868
iteration 266, loss = 0.0167595986276865
iteration 267, loss = 0.05147945508360863
iteration 268, loss = 0.045426055788993835
iteration 269, loss = 0.5081691741943359
iteration 270, loss = 0.0165289007127285
iteration 271, loss = 0.01958715170621872
iteration 272, loss = 0.02865460142493248
iteration 273, loss = 0.42289531230926514
iteration 274, loss = 0.02930179424583912
iteration 275, loss = 0.04536443203687668
iteration 276, loss = 0.05546407401561737
iteration 277, loss = 0.038082774728536606
iteration 278, loss = 0.39878377318382263
iteration 279, loss = 0.03235159441828728
iteration 280, loss = 0.04367998614907265
iteration 281, loss = 0.40557408332824707
iteration 282, loss = 0.06538224965333939
iteration 283, loss = 0.06301811337471008
iteration 284, loss = 0.020823579281568527
iteration 285, loss = 0.028758607804775238
iteration 286, loss = 0.06443178653717041
iteration 287, loss = 0.03143663331866264
iteration 288, loss = 0.39466601610183716
iteration 289, loss = 0.4020322263240814
iteration 290, loss = 0.01429753378033638
iteration 291, loss = 0.033887118101119995
iteration 292, loss = 0.3919813632965088
iteration 293, loss = 0.040411777794361115
iteration 294, loss = 0.022355791181325912
iteration 295, loss = 0.3959193825721741
iteration 296, loss = 0.06599849462509155
iteration 297, loss = 0.04843549057841301
iteration 298, loss = 0.3983911871910095
iteration 299, loss = 0.014020801521837711
iteration 300, loss = 0.017960496246814728
iteration 1, loss = 0.41170942783355713
iteration 2, loss = 0.022872325032949448
iteration 3, loss = 0.027774419635534286
iteration 4, loss = 0.027926530689001083
iteration 5, loss = 0.0224230345338583
iteration 6, loss = 0.0709424838423729
iteration 7, loss = 0.023657532408833504
iteration 8, loss = 0.03998538851737976
iteration 9, loss = 0.027115143835544586
iteration 10, loss = 0.02389794960618019
iteration 11, loss = 0.39875540137290955
iteration 12, loss = 0.033288728445768356
iteration 13, loss = 0.040243156254291534
iteration 14, loss = 0.06380213052034378
iteration 15, loss = 0.06009164825081825
iteration 16, loss = 0.39250457286834717
iteration 17, loss = 0.04438988119363785
iteration 18, loss = 0.17275162041187286
iteration 19, loss = 0.4080938696861267
iteration 20, loss = 0.39282846450805664
iteration 21, loss = 0.021978523582220078
iteration 22, loss = 0.019185638055205345
iteration 23, loss = 0.026312950998544693
iteration 24, loss = 0.04183602333068848
iteration 25, loss = 0.46667060256004333
iteration 26, loss = 0.016139741986989975
iteration 27, loss = 0.017641287297010422
iteration 28, loss = 0.03479480370879173
iteration 29, loss = 0.021152136847376823
iteration 30, loss = 0.06011983007192612
iteration 31, loss = 0.4134991765022278
iteration 32, loss = 0.03904365003108978
iteration 33, loss = 0.0202553179115057
iteration 34, loss = 0.032108306884765625
iteration 35, loss = 0.5285006761550903
iteration 36, loss = 0.041734352707862854
iteration 37, loss = 0.0404164120554924
iteration 38, loss = 0.039499785751104355
iteration 39, loss = 0.032756686210632324
iteration 40, loss = 0.03715137392282486
iteration 41, loss = 0.02861432358622551
iteration 42, loss = 0.04265908524394035
iteration 43, loss = 0.052106477320194244
iteration 44, loss = 0.03544338792562485
iteration 45, loss = 0.041157882660627365
iteration 46, loss = 0.02088179811835289
iteration 47, loss = 0.43857541680336
iteration 48, loss = 0.029212987050414085
iteration 49, loss = 0.0230107344686985
iteration 50, loss = 0.4009688198566437
iteration 51, loss = 0.016551295295357704
iteration 52, loss = 0.03617745637893677
iteration 53, loss = 0.05094379186630249
iteration 54, loss = 0.4049665033817291
iteration 55, loss = 0.02284698188304901
iteration 56, loss = 0.03415382280945778
iteration 57, loss = 0.44876521825790405
iteration 58, loss = 0.03156065568327904
iteration 59, loss = 0.031274035573005676
iteration 60, loss = 0.8020349144935608
iteration 61, loss = 0.1537231057882309
iteration 62, loss = 0.025613855570554733
iteration 63, loss = 0.01771882362663746
iteration 64, loss = 0.026012826710939407
iteration 65, loss = 0.020313138142228127
iteration 66, loss = 0.026025598868727684
iteration 67, loss = 0.41436630487442017
iteration 68, loss = 0.023618388921022415
iteration 69, loss = 0.4099928140640259
iteration 70, loss = 0.02886807546019554
iteration 71, loss = 0.021504651755094528
iteration 72, loss = 0.03360450640320778
iteration 73, loss = 0.020180024206638336
iteration 74, loss = 0.03335905820131302
iteration 75, loss = 0.026340628042817116
iteration 76, loss = 0.4076889157295227
iteration 77, loss = 0.39020076394081116
iteration 78, loss = 0.025360535830259323
iteration 79, loss = 0.0426044762134552
iteration 80, loss = 0.018002700060606003
iteration 81, loss = 0.7733060121536255
iteration 82, loss = 0.07552686333656311
iteration 83, loss = 0.02903265692293644
iteration 84, loss = 0.39952218532562256
iteration 85, loss = 0.02264983579516411
iteration 86, loss = 0.048939064145088196
iteration 87, loss = 0.4037960171699524
iteration 88, loss = 0.39041686058044434
iteration 89, loss = 0.04369029402732849
iteration 90, loss = 0.0172863956540823
iteration 91, loss = 0.06266827136278152
iteration 92, loss = 0.027841031551361084
iteration 93, loss = 0.028867237269878387
iteration 94, loss = 0.3956776261329651
iteration 95, loss = 0.025379342958331108
iteration 96, loss = 0.01832282915711403
iteration 97, loss = 0.8114420175552368
iteration 98, loss = 0.013429710641503334
iteration 99, loss = 0.41516542434692383
iteration 100, loss = 0.039171844720840454
iteration 101, loss = 0.027969855815172195
iteration 102, loss = 0.043981097638607025
iteration 103, loss = 0.44588181376457214
iteration 104, loss = 0.017961720004677773
iteration 105, loss = 0.019944606348872185
iteration 106, loss = 0.023687561973929405
iteration 107, loss = 0.027286507189273834
iteration 108, loss = 0.43696361780166626
iteration 109, loss = 0.03069729544222355
iteration 110, loss = 0.03221464529633522
iteration 111, loss = 0.07594890147447586
iteration 112, loss = 0.014286251738667488
iteration 113, loss = 0.048276375979185104
iteration 114, loss = 0.02098901942372322
iteration 115, loss = 0.06035338342189789
iteration 116, loss = 0.03668949380517006
iteration 117, loss = 0.04821156710386276
iteration 118, loss = 0.39330363273620605
iteration 119, loss = 0.02826208621263504
iteration 120, loss = 0.02374889701604843
iteration 121, loss = 0.9061551094055176
iteration 122, loss = 0.40431705117225647
iteration 123, loss = 0.022559933364391327
iteration 124, loss = 0.046852435916662216
iteration 125, loss = 0.0284238550812006
iteration 126, loss = 0.028100429102778435
iteration 127, loss = 0.39340662956237793
iteration 128, loss = 0.012141569517552853
iteration 129, loss = 0.03385024145245552
iteration 130, loss = 0.03294649347662926
iteration 131, loss = 0.02161724679172039
iteration 132, loss = 0.026889173313975334
iteration 133, loss = 0.023242950439453125
iteration 134, loss = 0.7832291126251221
iteration 135, loss = 0.041097354143857956
iteration 136, loss = 0.02735130302608013
iteration 137, loss = 0.02192413993179798
iteration 138, loss = 0.02144758030772209
iteration 139, loss = 0.019268393516540527
iteration 140, loss = 0.02972564473748207
iteration 141, loss = 0.15270206332206726
iteration 142, loss = 0.4020673334598541
iteration 143, loss = 0.4088868498802185
iteration 144, loss = 0.3973475694656372
iteration 145, loss = 0.025003064423799515
iteration 146, loss = 0.027404043823480606
iteration 147, loss = 0.39778029918670654
iteration 148, loss = 0.025153178721666336
iteration 149, loss = 0.047315020114183426
iteration 150, loss = 0.022226465865969658
iteration 151, loss = 0.031218551099300385
iteration 152, loss = 0.01402598712593317
iteration 153, loss = 0.02140086516737938
iteration 154, loss = 0.015756942331790924
iteration 155, loss = 0.016628922894597054
iteration 156, loss = 0.0205526240170002
iteration 157, loss = 0.032555289566516876
iteration 158, loss = 0.022985002025961876
iteration 159, loss = 0.023444121703505516
iteration 160, loss = 0.4072383642196655
iteration 161, loss = 0.07471764087677002
iteration 162, loss = 0.027292050421237946
iteration 163, loss = 0.01679101400077343
iteration 164, loss = 0.4043809771537781
iteration 165, loss = 0.021387621760368347
iteration 166, loss = 0.03279002755880356
iteration 167, loss = 0.40761348605155945
iteration 168, loss = 0.018429556861519814
iteration 169, loss = 0.01799885183572769
iteration 170, loss = 0.04614979401230812
iteration 171, loss = 0.027859410271048546
iteration 172, loss = 0.01587669923901558
iteration 173, loss = 0.016492094844579697
iteration 174, loss = 0.7957946062088013
iteration 175, loss = 0.14893800020217896
iteration 176, loss = 0.01824728585779667
iteration 177, loss = 0.010485706850886345
iteration 178, loss = 0.01603560522198677
iteration 179, loss = 0.013920067809522152
iteration 180, loss = 0.4082302451133728
iteration 181, loss = 0.4064231216907501
iteration 182, loss = 0.02854878082871437
iteration 183, loss = 0.027991652488708496
iteration 184, loss = 0.03143266215920448
iteration 185, loss = 0.3884200155735016
iteration 186, loss = 0.02253901958465576
iteration 187, loss = 0.406873881816864
iteration 188, loss = 0.01582767255604267
iteration 189, loss = 0.016717558726668358
iteration 190, loss = 0.40172576904296875
iteration 191, loss = 0.02548854984343052
iteration 192, loss = 0.3945480287075043
iteration 193, loss = 0.022417962551116943
iteration 194, loss = 0.1595260053873062
iteration 195, loss = 0.011662730947136879
iteration 196, loss = 0.014022528193891048
iteration 197, loss = 0.02901715226471424
iteration 198, loss = 0.020014239475131035
iteration 199, loss = 0.02618420496582985
iteration 200, loss = 0.021240081638097763
iteration 201, loss = 0.4003679156303406
iteration 202, loss = 0.0160694420337677
iteration 203, loss = 0.03640298172831535
iteration 204, loss = 0.030876509845256805
iteration 205, loss = 0.02790483459830284
iteration 206, loss = 0.019070055335760117
iteration 207, loss = 0.034860700368881226
iteration 208, loss = 0.0176093727350235
iteration 209, loss = 0.8078388571739197
iteration 210, loss = 0.017693012952804565
iteration 211, loss = 0.39678728580474854
iteration 212, loss = 0.027657002210617065
iteration 213, loss = 0.020055994391441345
iteration 214, loss = 0.4027408957481384
iteration 215, loss = 0.02038753591477871
iteration 216, loss = 0.020475611090660095
iteration 217, loss = 0.42997926473617554
iteration 218, loss = 0.028720088303089142
iteration 219, loss = 0.03564927726984024
iteration 220, loss = 0.4137156009674072
iteration 221, loss = 0.02560124546289444
iteration 222, loss = 0.012101531960070133
iteration 223, loss = 0.02066338062286377
iteration 224, loss = 0.02893906459212303
iteration 225, loss = 0.39152732491493225
iteration 226, loss = 0.4097382426261902
iteration 227, loss = 0.019348453730344772
iteration 228, loss = 0.14569038152694702
iteration 229, loss = 0.8948081135749817
iteration 230, loss = 0.020115839317440987
iteration 231, loss = 0.3979264795780182
iteration 232, loss = 0.03635624796152115
iteration 233, loss = 0.4016404151916504
iteration 234, loss = 0.4088345468044281
iteration 235, loss = 0.021572478115558624
iteration 236, loss = 0.024446619674563408
iteration 237, loss = 0.019279081374406815
iteration 238, loss = 0.021554261445999146
iteration 239, loss = 0.4206094443798065
iteration 240, loss = 0.3979393541812897
iteration 241, loss = 0.04095444455742836
iteration 242, loss = 0.017846256494522095
iteration 243, loss = 0.3912488520145416
iteration 244, loss = 0.035837750881910324
iteration 245, loss = 0.023763835430145264
iteration 246, loss = 0.023565679788589478
iteration 247, loss = 0.015793608501553535
iteration 248, loss = 0.02043255604803562
iteration 249, loss = 0.07902754098176956
iteration 250, loss = 0.012671624310314655
iteration 251, loss = 0.05113634839653969
iteration 252, loss = 0.5059422254562378
iteration 253, loss = 0.40063467621803284
iteration 254, loss = 0.012333672493696213
iteration 255, loss = 0.020648490637540817
iteration 256, loss = 0.07335956394672394
iteration 257, loss = 0.032889749854803085
iteration 258, loss = 0.026435524225234985
iteration 259, loss = 0.030990229919552803
iteration 260, loss = 0.034970805048942566
iteration 261, loss = 0.034260645508766174
iteration 262, loss = 0.023069918155670166
iteration 263, loss = 0.04495030269026756
iteration 264, loss = 0.4998987019062042
iteration 265, loss = 0.4890038073062897
iteration 266, loss = 0.01535982545465231
iteration 267, loss = 0.4990682601928711
iteration 268, loss = 0.02555912360548973
iteration 269, loss = 0.01923934556543827
iteration 270, loss = 0.025425352156162262
iteration 271, loss = 0.013051630929112434
iteration 272, loss = 0.04030466452240944
iteration 273, loss = 0.4836058020591736
iteration 274, loss = 0.016324380412697792
iteration 275, loss = 0.01972813718020916
iteration 276, loss = 0.77527916431427
iteration 277, loss = 0.018452830612659454
iteration 278, loss = 0.02083546668291092
iteration 279, loss = 0.42469990253448486
iteration 280, loss = 0.025691917166113853
iteration 281, loss = 0.01747378706932068
iteration 282, loss = 0.020150920376181602
iteration 283, loss = 0.02280665561556816
iteration 284, loss = 0.025166528299450874
iteration 285, loss = 0.02915838733315468
iteration 286, loss = 0.01657908409833908
iteration 287, loss = 0.02843870222568512
iteration 288, loss = 0.03681158274412155
iteration 289, loss = 0.028597243130207062
iteration 290, loss = 0.3907368779182434
iteration 291, loss = 0.020327098667621613
iteration 292, loss = 0.018480973318219185
iteration 293, loss = 0.04822055250406265
iteration 294, loss = 0.4102378487586975
iteration 295, loss = 0.01507820375263691
iteration 296, loss = 0.10404467582702637
iteration 297, loss = 0.022748813033103943
iteration 298, loss = 0.04246014729142189
iteration 299, loss = 0.4214639663696289
iteration 300, loss = 0.01668768748641014
iteration 1, loss = 0.022594548761844635
iteration 2, loss = 0.01709005981683731
iteration 3, loss = 0.4000101387500763
iteration 4, loss = 0.055337995290756226
iteration 5, loss = 0.022969074547290802
iteration 6, loss = 0.039143603295087814
iteration 7, loss = 0.02125038392841816
iteration 8, loss = 0.02498321235179901
iteration 9, loss = 0.016155948862433434
iteration 10, loss = 0.39739370346069336
iteration 11, loss = 0.1083739772439003
iteration 12, loss = 0.039344508200883865
iteration 13, loss = 0.019956709817051888
iteration 14, loss = 0.40015119314193726
iteration 15, loss = 0.02397937700152397
iteration 16, loss = 0.04545483738183975
iteration 17, loss = 0.018452884629368782
iteration 18, loss = 0.5132065415382385
iteration 19, loss = 0.3943016231060028
iteration 20, loss = 0.01844952628016472
iteration 21, loss = 0.035702165216207504
iteration 22, loss = 0.02840484119951725
iteration 23, loss = 0.01739567331969738
iteration 24, loss = 0.3945925533771515
iteration 25, loss = 0.022803813219070435
iteration 26, loss = 0.027115173637866974
iteration 27, loss = 0.0367952398955822
iteration 28, loss = 0.056796226650476456
iteration 29, loss = 0.4116215705871582
iteration 30, loss = 0.3952597975730896
iteration 31, loss = 0.033654551953077316
iteration 32, loss = 0.029035866260528564
iteration 33, loss = 0.02080448903143406
iteration 34, loss = 0.016081640496850014
iteration 35, loss = 0.03591756522655487
iteration 36, loss = 0.3994825482368469
iteration 37, loss = 0.021710626780986786
iteration 38, loss = 0.013907024636864662
iteration 39, loss = 0.03784627839922905
iteration 40, loss = 0.039526887238025665
iteration 41, loss = 0.024666430428624153
iteration 42, loss = 0.024790290743112564
iteration 43, loss = 0.41151943802833557
iteration 44, loss = 0.015433237887918949
iteration 45, loss = 0.018022578209638596
iteration 46, loss = 0.3945576846599579
iteration 47, loss = 0.04717475548386574
iteration 48, loss = 0.39836758375167847
iteration 49, loss = 0.01851094514131546
iteration 50, loss = 0.014863504096865654
iteration 51, loss = 0.39683884382247925
iteration 52, loss = 0.020728440955281258
iteration 53, loss = 0.39737898111343384
iteration 54, loss = 0.027138397097587585
iteration 55, loss = 0.017100637778639793
iteration 56, loss = 0.3899242877960205
iteration 57, loss = 0.02239041030406952
iteration 58, loss = 0.020799092948436737
iteration 59, loss = 0.017358629032969475
iteration 60, loss = 0.05029704421758652
iteration 61, loss = 0.768560528755188
iteration 62, loss = 0.015977611765265465
iteration 63, loss = 0.01895526424050331
iteration 64, loss = 0.016507696360349655
iteration 65, loss = 0.02156180515885353
iteration 66, loss = 0.019578374922275543
iteration 67, loss = 0.021539069712162018
iteration 68, loss = 0.02391793578863144
iteration 69, loss = 0.026065446436405182
iteration 70, loss = 0.023749535903334618
iteration 71, loss = 0.3925226032733917
iteration 72, loss = 0.39824366569519043
iteration 73, loss = 0.02023407258093357
iteration 74, loss = 0.018045814707875252
iteration 75, loss = 0.016363607719540596
iteration 76, loss = 0.39525723457336426
iteration 77, loss = 0.4087212085723877
iteration 78, loss = 0.4002876281738281
iteration 79, loss = 0.016232723370194435
iteration 80, loss = 0.024510234594345093
iteration 81, loss = 0.3957410156726837
iteration 82, loss = 0.02886269986629486
iteration 83, loss = 0.0162107702344656
iteration 84, loss = 0.021455971524119377
iteration 85, loss = 0.017676178365945816
iteration 86, loss = 0.14268489181995392
iteration 87, loss = 0.017197025939822197
iteration 88, loss = 0.008923321031033993
iteration 89, loss = 0.3937283754348755
iteration 90, loss = 0.018833713605999947
iteration 91, loss = 0.031917158514261246
iteration 92, loss = 0.015850750729441643
iteration 93, loss = 0.3969174325466156
iteration 94, loss = 0.014400803484022617
iteration 95, loss = 0.01508554257452488
iteration 96, loss = 0.416574090719223
iteration 97, loss = 0.03267502039670944
iteration 98, loss = 0.020300790667533875
iteration 99, loss = 0.13625216484069824
iteration 100, loss = 0.39123621582984924
iteration 101, loss = 0.018477339297533035
iteration 102, loss = 0.024818405508995056
iteration 103, loss = 0.023159468546509743
iteration 104, loss = 0.7984012961387634
iteration 105, loss = 0.026079274713993073
iteration 106, loss = 0.39218342304229736
iteration 107, loss = 0.011434581130743027
iteration 108, loss = 0.513755202293396
iteration 109, loss = 0.018554653972387314
iteration 110, loss = 0.4016309380531311
iteration 111, loss = 0.021899672225117683
iteration 112, loss = 0.026017364114522934
iteration 113, loss = 0.39909717440605164
iteration 114, loss = 0.40740451216697693
iteration 115, loss = 0.024760371074080467
iteration 116, loss = 0.01693943701684475
iteration 117, loss = 0.018783362582325935
iteration 118, loss = 0.01162736490368843
iteration 119, loss = 0.03577202931046486
iteration 120, loss = 0.018537065014243126
iteration 121, loss = 0.026200413703918457
iteration 122, loss = 0.018659284338355064
iteration 123, loss = 0.39654988050460815
iteration 124, loss = 0.017716428264975548
iteration 125, loss = 0.0241825133562088
iteration 126, loss = 0.020167194306850433
iteration 127, loss = 0.017225878313183784
iteration 128, loss = 0.01863808184862137
iteration 129, loss = 0.017772149294614792
iteration 130, loss = 0.017048055306077003
iteration 131, loss = 0.045578137040138245
iteration 132, loss = 0.11528009921312332
iteration 133, loss = 0.0320102795958519
iteration 134, loss = 0.019429266452789307
iteration 135, loss = 0.3993386924266815
iteration 136, loss = 0.02612622082233429
iteration 137, loss = 0.016939304769039154
iteration 138, loss = 0.036945804953575134
iteration 139, loss = 0.7670694589614868
iteration 140, loss = 0.028741750866174698
iteration 141, loss = 0.487692266702652
iteration 142, loss = 0.03553219139575958
iteration 143, loss = 0.039687447249889374
iteration 144, loss = 0.01775188371539116
iteration 145, loss = 0.029347816482186317
iteration 146, loss = 0.7751259207725525
iteration 147, loss = 0.011569829657673836
iteration 148, loss = 0.014279555529356003
iteration 149, loss = 0.028139997273683548
iteration 150, loss = 0.025660334154963493
iteration 151, loss = 0.015791935846209526
iteration 152, loss = 0.01872626505792141
iteration 153, loss = 0.39400458335876465
iteration 154, loss = 0.021142031997442245
iteration 155, loss = 0.02200421318411827
iteration 156, loss = 0.016441509127616882
iteration 157, loss = 0.021588129922747612
iteration 158, loss = 0.3972083628177643
iteration 159, loss = 0.03647223114967346
iteration 160, loss = 0.018682047724723816
iteration 161, loss = 0.764785647392273
iteration 162, loss = 0.4135971665382385
iteration 163, loss = 0.016995377838611603
iteration 164, loss = 0.025302492082118988
iteration 165, loss = 0.40013089776039124
iteration 166, loss = 0.016841372475028038
iteration 167, loss = 0.017263272777199745
iteration 168, loss = 0.044389814138412476
iteration 169, loss = 0.1158415824174881
iteration 170, loss = 0.028206590563058853
iteration 171, loss = 0.41665637493133545
iteration 172, loss = 0.4058387875556946
iteration 173, loss = 0.012303243391215801
iteration 174, loss = 0.02082652598619461
iteration 175, loss = 0.026414144784212112
iteration 176, loss = 0.035184457898139954
iteration 177, loss = 0.022783489897847176
iteration 178, loss = 0.018943846225738525
iteration 179, loss = 0.02262723818421364
iteration 180, loss = 0.03289996087551117
iteration 181, loss = 0.015231017023324966
iteration 182, loss = 0.0188884474337101
iteration 183, loss = 0.024393536150455475
iteration 184, loss = 0.8842697739601135
iteration 185, loss = 0.017645783722400665
iteration 186, loss = 0.03858240321278572
iteration 187, loss = 0.38711947202682495
iteration 188, loss = 0.02156245708465576
iteration 189, loss = 0.02096875198185444
iteration 190, loss = 0.014083530753850937
iteration 191, loss = 0.017514808103442192
iteration 192, loss = 0.015046050772070885
iteration 193, loss = 0.014642046764492989
iteration 194, loss = 0.39622992277145386
iteration 195, loss = 0.028732916340231895
iteration 196, loss = 0.012018370442092419
iteration 197, loss = 0.016302362084388733
iteration 198, loss = 0.39143699407577515
iteration 199, loss = 0.020486941561102867
iteration 200, loss = 0.015228628180921078
iteration 201, loss = 0.022479191422462463
iteration 202, loss = 0.02137087471783161
iteration 203, loss = 0.3878878057003021
iteration 204, loss = 0.11262854933738708
iteration 205, loss = 0.019070977345108986
iteration 206, loss = 0.11254747956991196
iteration 207, loss = 0.4059089422225952
iteration 208, loss = 0.016532836481928825
iteration 209, loss = 0.1076802909374237
iteration 210, loss = 0.40155211091041565
iteration 211, loss = 0.020364487543702126
iteration 212, loss = 0.027350623160600662
iteration 213, loss = 0.023177597671747208
iteration 214, loss = 0.021468352526426315
iteration 215, loss = 0.019272951409220695
iteration 216, loss = 0.02911413088440895
iteration 217, loss = 0.03971582651138306
iteration 218, loss = 0.02100132405757904
iteration 219, loss = 0.027254939079284668
iteration 220, loss = 0.01483161747455597
iteration 221, loss = 0.026737574487924576
iteration 222, loss = 0.39221909642219543
iteration 223, loss = 0.030396247282624245
iteration 224, loss = 0.013729050755500793
iteration 225, loss = 0.09716320037841797
iteration 226, loss = 0.022838691249489784
iteration 227, loss = 0.021576857194304466
iteration 228, loss = 0.04180118814110756
iteration 229, loss = 0.019103877246379852
iteration 230, loss = 0.39040234684944153
iteration 231, loss = 0.3916807770729065
iteration 232, loss = 0.01277622114866972
iteration 233, loss = 0.031048675999045372
iteration 234, loss = 0.020095281302928925
iteration 235, loss = 0.030452460050582886
iteration 236, loss = 0.030434468761086464
iteration 237, loss = 0.3917580246925354
iteration 238, loss = 0.3944225609302521
iteration 239, loss = 0.018112460151314735
iteration 240, loss = 0.03368714451789856
iteration 241, loss = 0.044925082474946976
iteration 242, loss = 0.39783570170402527
iteration 243, loss = 0.027264947071671486
iteration 244, loss = 0.02145181968808174
iteration 245, loss = 0.3955737352371216
iteration 246, loss = 0.019515812397003174
iteration 247, loss = 0.4927821159362793
iteration 248, loss = 0.4878752827644348
iteration 249, loss = 0.029006224125623703
iteration 250, loss = 0.02455076016485691
iteration 251, loss = 0.012338568456470966
iteration 252, loss = 0.4085867404937744
iteration 253, loss = 0.024294324219226837
iteration 254, loss = 0.01288088969886303
iteration 255, loss = 0.02959475666284561
iteration 256, loss = 0.04034263640642166
iteration 257, loss = 0.01331222802400589
iteration 258, loss = 0.4155115485191345
iteration 259, loss = 0.024434752762317657
iteration 260, loss = 0.013667043298482895
iteration 261, loss = 0.014371214434504509
iteration 262, loss = 0.4130399823188782
iteration 263, loss = 0.01625002548098564
iteration 264, loss = 0.013119857758283615
iteration 265, loss = 0.02211899869143963
iteration 266, loss = 0.013329878449440002
iteration 267, loss = 0.015136671252548695
iteration 268, loss = 0.012067830190062523
iteration 269, loss = 0.0329267755150795
iteration 270, loss = 0.3930094540119171
iteration 271, loss = 0.015190215781331062
iteration 272, loss = 0.023514259606599808
iteration 273, loss = 0.017240164801478386
iteration 274, loss = 0.014184593223035336
iteration 275, loss = 0.3986925184726715
iteration 276, loss = 0.02042258344590664
iteration 277, loss = 0.4165155589580536
iteration 278, loss = 0.015182804316282272
iteration 279, loss = 0.014494885690510273
iteration 280, loss = 0.016161197796463966
iteration 281, loss = 0.018961692228913307
iteration 282, loss = 0.011602871119976044
iteration 283, loss = 0.39343756437301636
iteration 284, loss = 0.01757352240383625
iteration 285, loss = 0.024524442851543427
iteration 286, loss = 0.7841821312904358
iteration 287, loss = 0.021223481744527817
iteration 288, loss = 0.03701100870966911
iteration 289, loss = 0.0242354366928339
iteration 290, loss = 0.026412557810544968
iteration 291, loss = 0.01811491884291172
iteration 292, loss = 0.792289674282074
iteration 293, loss = 0.018723055720329285
iteration 294, loss = 0.3929685950279236
iteration 295, loss = 0.01643974520266056
iteration 296, loss = 0.0199824720621109
iteration 297, loss = 0.41805586218833923
iteration 298, loss = 0.012899772264063358
iteration 299, loss = 0.3928607404232025
iteration 300, loss = 0.024241117760539055
iteration 1, loss = 0.017660008743405342
iteration 2, loss = 0.02744258940219879
iteration 3, loss = 0.01734801009297371
iteration 4, loss = 0.026431351900100708
iteration 5, loss = 0.3959324359893799
iteration 6, loss = 0.013567519374191761
iteration 7, loss = 0.010515013709664345
iteration 8, loss = 0.00984154548496008
iteration 9, loss = 0.3974640667438507
iteration 10, loss = 0.8747301697731018
iteration 11, loss = 0.013047671876847744
iteration 12, loss = 0.3931308388710022
iteration 13, loss = 0.38922178745269775
iteration 14, loss = 0.4010547697544098
iteration 15, loss = 0.019769854843616486
iteration 16, loss = 0.018095264211297035
iteration 17, loss = 0.023854725062847137
iteration 18, loss = 0.019580161198973656
iteration 19, loss = 0.1127442866563797
iteration 20, loss = 0.3964326083660126
iteration 21, loss = 0.7656044960021973
iteration 22, loss = 0.013210617005825043
iteration 23, loss = 0.016975704580545425
iteration 24, loss = 0.04656543955206871
iteration 25, loss = 0.021323928609490395
iteration 26, loss = 0.028748858720064163
iteration 27, loss = 0.018973659723997116
iteration 28, loss = 0.3916676342487335
iteration 29, loss = 0.008720237761735916
iteration 30, loss = 0.01755298115313053
iteration 31, loss = 0.01517476886510849
iteration 32, loss = 0.014171544462442398
iteration 33, loss = 0.0123042743653059
iteration 34, loss = 0.010809849947690964
iteration 35, loss = 0.035020116716623306
iteration 36, loss = 0.5159177184104919
iteration 37, loss = 0.019987061619758606
iteration 38, loss = 0.014123465865850449
iteration 39, loss = 0.013214356265962124
iteration 40, loss = 0.012526918202638626
iteration 41, loss = 0.38811835646629333
iteration 42, loss = 0.018478315323591232
iteration 43, loss = 0.011379098519682884
iteration 44, loss = 0.01329021342098713
iteration 45, loss = 0.024182267487049103
iteration 46, loss = 0.39179858565330505
iteration 47, loss = 0.012761741876602173
iteration 48, loss = 0.014263073913753033
iteration 49, loss = 0.019577879458665848
iteration 50, loss = 0.01252691075205803
iteration 51, loss = 0.40058523416519165
iteration 52, loss = 0.12041262537240982
iteration 53, loss = 0.02540435642004013
iteration 54, loss = 0.3879282772541046
iteration 55, loss = 0.023304853588342667
iteration 56, loss = 0.022108715027570724
iteration 57, loss = 0.02093864046037197
iteration 58, loss = 0.3988640010356903
iteration 59, loss = 0.01436700765043497
iteration 60, loss = 0.4244385063648224
iteration 61, loss = 0.024546204134821892
iteration 62, loss = 0.029485825449228287
iteration 63, loss = 0.022752104327082634
iteration 64, loss = 0.01020731683820486
iteration 65, loss = 0.013682061806321144
iteration 66, loss = 0.016726607456803322
iteration 67, loss = 0.04099730774760246
iteration 68, loss = 0.018763717263936996
iteration 69, loss = 0.018589649349451065
iteration 70, loss = 0.020413126796483994
iteration 71, loss = 0.01918206550180912
iteration 72, loss = 0.03710148483514786
iteration 73, loss = 0.39492493867874146
iteration 74, loss = 0.38868552446365356
iteration 75, loss = 0.016150444746017456
iteration 76, loss = 0.013276433572173119
iteration 77, loss = 0.013360467739403248
iteration 78, loss = 0.395264208316803
iteration 79, loss = 0.028628597036004066
iteration 80, loss = 0.012167192995548248
iteration 81, loss = 0.014873364940285683
iteration 82, loss = 0.03899534046649933
iteration 83, loss = 0.01740281842648983
iteration 84, loss = 0.015644822269678116
iteration 85, loss = 0.050740163773298264
iteration 86, loss = 0.017091179266572
iteration 87, loss = 0.028369644656777382
iteration 88, loss = 0.020524058490991592
iteration 89, loss = 0.03325121849775314
iteration 90, loss = 0.4244979918003082
iteration 91, loss = 0.016521092504262924
iteration 92, loss = 0.011917985044419765
iteration 93, loss = 0.3893280327320099
iteration 94, loss = 0.029077254235744476
iteration 95, loss = 0.014845091849565506
iteration 96, loss = 0.008782397024333477
iteration 97, loss = 0.03561665862798691
iteration 98, loss = 0.01838403195142746
iteration 99, loss = 0.03768736496567726
iteration 100, loss = 0.3968340754508972
iteration 101, loss = 0.021315213292837143
iteration 102, loss = 0.020428404211997986
iteration 103, loss = 0.02128903567790985
iteration 104, loss = 0.014496694318950176
iteration 105, loss = 0.4036910831928253
iteration 106, loss = 0.015012861229479313
iteration 107, loss = 0.3947637677192688
iteration 108, loss = 0.3907616138458252
iteration 109, loss = 0.018072575330734253
iteration 110, loss = 0.016759058460593224
iteration 111, loss = 0.3865790367126465
iteration 112, loss = 0.01368858851492405
iteration 113, loss = 0.0130442064255476
iteration 114, loss = 0.02877754345536232
iteration 115, loss = 0.01859782263636589
iteration 116, loss = 0.40245723724365234
iteration 117, loss = 0.3970470130443573
iteration 118, loss = 0.011014866642653942
iteration 119, loss = 0.015571021474897861
iteration 120, loss = 0.02864845097064972
iteration 121, loss = 0.02001567929983139
iteration 122, loss = 0.02187345176935196
iteration 123, loss = 0.4040590524673462
iteration 124, loss = 0.41579416394233704
iteration 125, loss = 0.39396053552627563
iteration 126, loss = 0.02389664202928543
iteration 127, loss = 0.014772565104067326
iteration 128, loss = 0.0188433937728405
iteration 129, loss = 0.018072007223963737
iteration 130, loss = 0.018434463068842888
iteration 131, loss = 0.016404030844569206
iteration 132, loss = 0.02555028721690178
iteration 133, loss = 0.020805764943361282
iteration 134, loss = 0.41511255502700806
iteration 135, loss = 0.0235150083899498
iteration 136, loss = 0.4944913387298584
iteration 137, loss = 0.013162994757294655
iteration 138, loss = 0.017338957637548447
iteration 139, loss = 0.022920003160834312
iteration 140, loss = 0.41110914945602417
iteration 141, loss = 0.016108335927128792
iteration 142, loss = 0.01670331321656704
iteration 143, loss = 0.019421853125095367
iteration 144, loss = 0.013766766525804996
iteration 145, loss = 0.017784414812922478
iteration 146, loss = 0.01585574634373188
iteration 147, loss = 0.013019441626966
iteration 148, loss = 0.013361060991883278
iteration 149, loss = 0.02836631052196026
iteration 150, loss = 0.7664953470230103
iteration 151, loss = 0.017011068761348724
iteration 152, loss = 0.02098231203854084
iteration 153, loss = 0.023023799061775208
iteration 154, loss = 0.027035148814320564
iteration 155, loss = 0.4004618227481842
iteration 156, loss = 0.010460847988724709
iteration 157, loss = 0.3960310220718384
iteration 158, loss = 0.01429092325270176
iteration 159, loss = 0.018379561603069305
iteration 160, loss = 0.4133179485797882
iteration 161, loss = 0.025824815034866333
iteration 162, loss = 0.01988363265991211
iteration 163, loss = 0.014979299157857895
iteration 164, loss = 0.02640666626393795
iteration 165, loss = 0.017696956172585487
iteration 166, loss = 0.01102626882493496
iteration 167, loss = 0.48884090781211853
iteration 168, loss = 0.027625206857919693
iteration 169, loss = 0.0195695161819458
iteration 170, loss = 0.016986839473247528
iteration 171, loss = 0.4078221321105957
iteration 172, loss = 0.11909816414117813
iteration 173, loss = 0.01467471569776535
iteration 174, loss = 0.41809698939323425
iteration 175, loss = 0.018066002056002617
iteration 176, loss = 0.021985739469528198
iteration 177, loss = 0.010535301640629768
iteration 178, loss = 0.014691079035401344
iteration 179, loss = 0.3936672806739807
iteration 180, loss = 0.015217291191220284
iteration 181, loss = 0.028157733380794525
iteration 182, loss = 0.018885718658566475
iteration 183, loss = 0.01673000678420067
iteration 184, loss = 0.011138632893562317
iteration 185, loss = 0.38449883460998535
iteration 186, loss = 0.03156222030520439
iteration 187, loss = 0.02375178411602974
iteration 188, loss = 0.3937317728996277
iteration 189, loss = 0.017341269180178642
iteration 190, loss = 0.027958719059824944
iteration 191, loss = 0.015109766274690628
iteration 192, loss = 0.016096511855721474
iteration 193, loss = 0.02578604966402054
iteration 194, loss = 0.021883517503738403
iteration 195, loss = 0.0207517072558403
iteration 196, loss = 0.3929755687713623
iteration 197, loss = 0.01052122749388218
iteration 198, loss = 0.1303224265575409
iteration 199, loss = 0.012183304876089096
iteration 200, loss = 0.03835827857255936
iteration 201, loss = 0.022764403373003006
iteration 202, loss = 0.025217898190021515
iteration 203, loss = 0.025151968002319336
iteration 204, loss = 0.4891127347946167
iteration 205, loss = 0.028376474976539612
iteration 206, loss = 0.3913760483264923
iteration 207, loss = 0.10865429788827896
iteration 208, loss = 0.014119109138846397
iteration 209, loss = 0.3958328068256378
iteration 210, loss = 0.022556299343705177
iteration 211, loss = 0.02159339189529419
iteration 212, loss = 0.76920485496521
iteration 213, loss = 0.009969913400709629
iteration 214, loss = 0.019012391567230225
iteration 215, loss = 0.39446941018104553
iteration 216, loss = 0.02480405569076538
iteration 217, loss = 0.028965547680854797
iteration 218, loss = 0.015489159151911736
iteration 219, loss = 0.022947371006011963
iteration 220, loss = 0.018600238487124443
iteration 221, loss = 0.02163657732307911
iteration 222, loss = 0.7668800354003906
iteration 223, loss = 0.023287586867809296
iteration 224, loss = 0.023327374830842018
iteration 225, loss = 0.3915676474571228
iteration 226, loss = 0.012260690331459045
iteration 227, loss = 0.0129419956356287
iteration 228, loss = 0.024594254791736603
iteration 229, loss = 0.025576435029506683
iteration 230, loss = 0.018210064619779587
iteration 231, loss = 0.02378189004957676
iteration 232, loss = 0.7725311517715454
iteration 233, loss = 0.013312357477843761
iteration 234, loss = 0.024859588593244553
iteration 235, loss = 0.012150323018431664
iteration 236, loss = 0.396144300699234
iteration 237, loss = 0.016588235273957253
iteration 238, loss = 0.025944899767637253
iteration 239, loss = 0.49235498905181885
iteration 240, loss = 0.014708234928548336
iteration 241, loss = 0.02181261032819748
iteration 242, loss = 0.034558527171611786
iteration 243, loss = 0.02399500086903572
iteration 244, loss = 0.026971999555826187
iteration 245, loss = 0.412090539932251
iteration 246, loss = 0.38780659437179565
iteration 247, loss = 0.3884385824203491
iteration 248, loss = 0.017201142385601997
iteration 249, loss = 0.011368815787136555
iteration 250, loss = 0.015867721289396286
iteration 251, loss = 0.016562869772315025
iteration 252, loss = 0.8627359867095947
iteration 253, loss = 0.019458122551441193
iteration 254, loss = 0.38974329829216003
iteration 255, loss = 0.4227079451084137
iteration 256, loss = 0.022647438570857048
iteration 257, loss = 0.3947470486164093
iteration 258, loss = 0.013250288553535938
iteration 259, loss = 0.4040727913379669
iteration 260, loss = 0.028729017823934555
iteration 261, loss = 0.02177409827709198
iteration 262, loss = 0.11706048250198364
iteration 263, loss = 0.02072455920279026
iteration 264, loss = 0.40381085872650146
iteration 265, loss = 0.015750760212540627
iteration 266, loss = 0.043598320335149765
iteration 267, loss = 0.025802448391914368
iteration 268, loss = 0.3934665024280548
iteration 269, loss = 0.014820780605077744
iteration 270, loss = 0.3885071277618408
iteration 271, loss = 0.01417218241840601
iteration 272, loss = 0.01800197921693325
iteration 273, loss = 0.02533290535211563
iteration 274, loss = 0.39033499360084534
iteration 275, loss = 0.02415967360138893
iteration 276, loss = 0.012295493856072426
iteration 277, loss = 0.391577810049057
iteration 278, loss = 0.011800643056631088
iteration 279, loss = 0.01420606765896082
iteration 280, loss = 0.38964855670928955
iteration 281, loss = 0.02786388248205185
iteration 282, loss = 0.1333395540714264
iteration 283, loss = 0.02303316630423069
iteration 284, loss = 0.01580982469022274
iteration 285, loss = 0.014160887338221073
iteration 286, loss = 0.3904039263725281
iteration 287, loss = 0.01844211481511593
iteration 288, loss = 0.3969889283180237
iteration 289, loss = 0.39524760842323303
iteration 290, loss = 0.01647014170885086
iteration 291, loss = 0.025741834193468094
iteration 292, loss = 0.39741820096969604
iteration 293, loss = 0.01817743293941021
iteration 294, loss = 0.015267272479832172
iteration 295, loss = 0.021527446806430817
iteration 296, loss = 0.02111210860311985
iteration 297, loss = 0.023444529622793198
iteration 298, loss = 0.00902603566646576
iteration 299, loss = 0.10935676097869873
iteration 300, loss = 0.02449679747223854
iteration 1, loss = 0.019381459802389145
iteration 2, loss = 0.019680364057421684
iteration 3, loss = 0.012200720608234406
iteration 4, loss = 0.017013298347592354
iteration 5, loss = 0.01291134487837553
iteration 6, loss = 0.012517409399151802
iteration 7, loss = 0.02147512696683407
iteration 8, loss = 0.018862688913941383
iteration 9, loss = 0.014081343077123165
iteration 10, loss = 0.0184585340321064
iteration 11, loss = 0.018490122631192207
iteration 12, loss = 0.40835049748420715
iteration 13, loss = 0.05023359879851341
iteration 14, loss = 0.02660509943962097
iteration 15, loss = 0.03170234337449074
iteration 16, loss = 0.02110830880701542
iteration 17, loss = 0.4007115662097931
iteration 18, loss = 0.022674409672617912
iteration 19, loss = 0.4005865156650543
iteration 20, loss = 0.024753231555223465
iteration 21, loss = 0.021973775699734688
iteration 22, loss = 0.406642884016037
iteration 23, loss = 0.40660586953163147
iteration 24, loss = 0.3876287043094635
iteration 25, loss = 0.0169239342212677
iteration 26, loss = 0.021304234862327576
iteration 27, loss = 0.01936224475502968
iteration 28, loss = 0.013527315109968185
iteration 29, loss = 0.3946286141872406
iteration 30, loss = 0.13994990289211273
iteration 31, loss = 0.01578252576291561
iteration 32, loss = 0.4042174816131592
iteration 33, loss = 0.01267984975129366
iteration 34, loss = 0.025751158595085144
iteration 35, loss = 0.396297812461853
iteration 36, loss = 0.018373236060142517
iteration 37, loss = 0.013965517282485962
iteration 38, loss = 0.0265194159001112
iteration 39, loss = 0.39317604899406433
iteration 40, loss = 0.3989555537700653
iteration 41, loss = 0.0408172607421875
iteration 42, loss = 0.016308516263961792
iteration 43, loss = 0.11192258447408676
iteration 44, loss = 0.01291933935135603
iteration 45, loss = 0.02038491889834404
iteration 46, loss = 0.488861620426178
iteration 47, loss = 0.40952223539352417
iteration 48, loss = 0.014888309873640537
iteration 49, loss = 0.015820462256669998
iteration 50, loss = 0.020956460386514664
iteration 51, loss = 0.014120183885097504
iteration 52, loss = 0.019744696095585823
iteration 53, loss = 0.11905018240213394
iteration 54, loss = 0.014090740121901035
iteration 55, loss = 0.027626067399978638
iteration 56, loss = 0.013674383983016014
iteration 57, loss = 0.019569559022784233
iteration 58, loss = 0.016533013433218002
iteration 59, loss = 0.3905486762523651
iteration 60, loss = 0.3879101872444153
iteration 61, loss = 0.026531502604484558
iteration 62, loss = 0.014719636179506779
iteration 63, loss = 0.00794467143714428
iteration 64, loss = 0.011635995469987392
iteration 65, loss = 0.0206972137093544
iteration 66, loss = 0.014506770297884941
iteration 67, loss = 0.014790795743465424
iteration 68, loss = 0.39537400007247925
iteration 69, loss = 0.014793883077800274
iteration 70, loss = 0.39500224590301514
iteration 71, loss = 0.015682736411690712
iteration 72, loss = 0.025862032547593117
iteration 73, loss = 0.4116080403327942
iteration 74, loss = 0.016848808154463768
iteration 75, loss = 0.02264084666967392
iteration 76, loss = 0.01748211309313774
iteration 77, loss = 0.017114046961069107
iteration 78, loss = 0.021959152072668076
iteration 79, loss = 0.022054534405469894
iteration 80, loss = 0.018750950694084167
iteration 81, loss = 0.020029179751873016
iteration 82, loss = 0.023802652955055237
iteration 83, loss = 0.017809424549341202
iteration 84, loss = 0.10860592871904373
iteration 85, loss = 0.024148982018232346
iteration 86, loss = 0.38987433910369873
iteration 87, loss = 0.39146533608436584
iteration 88, loss = 0.013281518593430519
iteration 89, loss = 0.3939027786254883
iteration 90, loss = 0.01664808578789234
iteration 91, loss = 0.015606123954057693
iteration 92, loss = 0.3908451199531555
iteration 93, loss = 0.022333184257149696
iteration 94, loss = 0.3885473608970642
iteration 95, loss = 0.027535589411854744
iteration 96, loss = 0.012209762819111347
iteration 97, loss = 0.03866136446595192
iteration 98, loss = 0.01728910766541958
iteration 99, loss = 0.015274411998689175
iteration 100, loss = 0.02685585431754589
iteration 101, loss = 0.02658670023083687
iteration 102, loss = 0.03431522101163864
iteration 103, loss = 0.020051419734954834
iteration 104, loss = 0.7775447368621826
iteration 105, loss = 0.019454579800367355
iteration 106, loss = 0.026078494265675545
iteration 107, loss = 0.015246151946485043
iteration 108, loss = 0.4362456202507019
iteration 109, loss = 0.022822346538305283
iteration 110, loss = 0.02297274023294449
iteration 111, loss = 0.021933602169156075
iteration 112, loss = 0.022800102829933167
iteration 113, loss = 0.01593407429754734
iteration 114, loss = 0.01811271533370018
iteration 115, loss = 0.12229437381029129
iteration 116, loss = 0.010691449977457523
iteration 117, loss = 0.015277155674993992
iteration 118, loss = 0.39114704728126526
iteration 119, loss = 0.020074039697647095
iteration 120, loss = 0.026575226336717606
iteration 121, loss = 0.010981257073581219
iteration 122, loss = 0.020204924046993256
iteration 123, loss = 0.011570178903639317
iteration 124, loss = 0.021693628281354904
iteration 125, loss = 0.3887823522090912
iteration 126, loss = 0.019633758813142776
iteration 127, loss = 0.018011614680290222
iteration 128, loss = 0.40164533257484436
iteration 129, loss = 0.01632877066731453
iteration 130, loss = 0.4839171767234802
iteration 131, loss = 0.026108242571353912
iteration 132, loss = 0.014717767015099525
iteration 133, loss = 0.016703655943274498
iteration 134, loss = 0.02281489223241806
iteration 135, loss = 0.02258455939590931
iteration 136, loss = 0.3964066505432129
iteration 137, loss = 0.037516817450523376
iteration 138, loss = 0.011111397296190262
iteration 139, loss = 0.020252427086234093
iteration 140, loss = 0.010849649086594582
iteration 141, loss = 0.02960948832333088
iteration 142, loss = 0.011000107042491436
iteration 143, loss = 0.3997652530670166
iteration 144, loss = 0.48560982942581177
iteration 145, loss = 0.0095063466578722
iteration 146, loss = 0.013223370537161827
iteration 147, loss = 0.03309088200330734
iteration 148, loss = 0.021280786022543907
iteration 149, loss = 0.01352305244654417
iteration 150, loss = 0.03184199705719948
iteration 151, loss = 0.008144324645400047
iteration 152, loss = 0.013948098756372929
iteration 153, loss = 0.010949898511171341
iteration 154, loss = 0.01717229187488556
iteration 155, loss = 0.38993871212005615
iteration 156, loss = 0.015883350744843483
iteration 157, loss = 0.01184108853340149
iteration 158, loss = 0.39094534516334534
iteration 159, loss = 0.856654703617096
iteration 160, loss = 0.020978957414627075
iteration 161, loss = 0.01269026380032301
iteration 162, loss = 0.02479589357972145
iteration 163, loss = 0.7681026458740234
iteration 164, loss = 0.026730239391326904
iteration 165, loss = 0.016701824963092804
iteration 166, loss = 0.38967016339302063
iteration 167, loss = 0.10946661978960037
iteration 168, loss = 0.027619441971182823
iteration 169, loss = 0.018412085250020027
iteration 170, loss = 0.015372931957244873
iteration 171, loss = 0.020161554217338562
iteration 172, loss = 0.10891061276197433
iteration 173, loss = 0.010504237376153469
iteration 174, loss = 0.02427113614976406
iteration 175, loss = 0.48004665970802307
iteration 176, loss = 0.4051913917064667
iteration 177, loss = 0.015715183690190315
iteration 178, loss = 0.3952164947986603
iteration 179, loss = 0.04084521532058716
iteration 180, loss = 0.017055422067642212
iteration 181, loss = 0.3891873359680176
iteration 182, loss = 0.7684889435768127
iteration 183, loss = 0.39567726850509644
iteration 184, loss = 0.01323320809751749
iteration 185, loss = 0.39858874678611755
iteration 186, loss = 0.7727100253105164
iteration 187, loss = 0.04025252163410187
iteration 188, loss = 0.009824219159781933
iteration 189, loss = 0.019460365176200867
iteration 190, loss = 0.022910578176379204
iteration 191, loss = 0.10700857639312744
iteration 192, loss = 0.39001601934432983
iteration 193, loss = 0.01123574934899807
iteration 194, loss = 0.02537892572581768
iteration 195, loss = 0.7766110301017761
iteration 196, loss = 0.01775776408612728
iteration 197, loss = 0.01656915806233883
iteration 198, loss = 0.015088488347828388
iteration 199, loss = 0.3894040286540985
iteration 200, loss = 0.018173033371567726
iteration 201, loss = 0.40310347080230713
iteration 202, loss = 0.012545355595648289
iteration 203, loss = 0.3885314464569092
iteration 204, loss = 0.03119477815926075
iteration 205, loss = 0.020412812009453773
iteration 206, loss = 0.023210637271404266
iteration 207, loss = 0.016869647428393364
iteration 208, loss = 0.02180175483226776
iteration 209, loss = 0.8468061685562134
iteration 210, loss = 0.02035970613360405
iteration 211, loss = 0.01914922520518303
iteration 212, loss = 0.3879072368144989
iteration 213, loss = 0.40067023038864136
iteration 214, loss = 0.01976068504154682
iteration 215, loss = 0.020597703754901886
iteration 216, loss = 0.019409995526075363
iteration 217, loss = 0.4212898910045624
iteration 218, loss = 0.41352975368499756
iteration 219, loss = 0.020178532227873802
iteration 220, loss = 0.02723325788974762
iteration 221, loss = 0.4031578302383423
iteration 222, loss = 0.3932914733886719
iteration 223, loss = 0.02535650134086609
iteration 224, loss = 0.013544113375246525
iteration 225, loss = 0.4187350571155548
iteration 226, loss = 0.04073723405599594
iteration 227, loss = 0.013113276101648808
iteration 228, loss = 0.03434703126549721
iteration 229, loss = 0.01697658747434616
iteration 230, loss = 0.017759229987859726
iteration 231, loss = 0.3915237784385681
iteration 232, loss = 0.0202679093927145
iteration 233, loss = 0.024610372260212898
iteration 234, loss = 0.028536124154925346
iteration 235, loss = 0.015226521529257298
iteration 236, loss = 0.022102313116192818
iteration 237, loss = 0.7756795883178711
iteration 238, loss = 0.01607619971036911
iteration 239, loss = 0.026254883036017418
iteration 240, loss = 0.018036970868706703
iteration 241, loss = 0.02481961064040661
iteration 242, loss = 0.01777966134250164
iteration 243, loss = 0.017255568876862526
iteration 244, loss = 0.01893926039338112
iteration 245, loss = 0.01639397069811821
iteration 246, loss = 0.02117200940847397
iteration 247, loss = 0.010031427256762981
iteration 248, loss = 0.017454933375120163
iteration 249, loss = 0.015249176882207394
iteration 250, loss = 0.023677313700318336
iteration 251, loss = 0.012917309068143368
iteration 252, loss = 0.01071682758629322
iteration 253, loss = 0.3881303668022156
iteration 254, loss = 0.017994839698076248
iteration 255, loss = 0.390686959028244
iteration 256, loss = 0.022453293204307556
iteration 257, loss = 0.016594933345913887
iteration 258, loss = 0.020745899528265
iteration 259, loss = 0.018157755956053734
iteration 260, loss = 0.016371844336390495
iteration 261, loss = 0.022723454982042313
iteration 262, loss = 0.01153167150914669
iteration 263, loss = 0.3995382785797119
iteration 264, loss = 0.01612510159611702
iteration 265, loss = 0.007146931253373623
iteration 266, loss = 0.018520407378673553
iteration 267, loss = 0.023508574813604355
iteration 268, loss = 0.015045535750687122
iteration 269, loss = 0.010362563654780388
iteration 270, loss = 0.03140327334403992
iteration 271, loss = 0.01651616208255291
iteration 272, loss = 0.42881473898887634
iteration 273, loss = 0.025861894711852074
iteration 274, loss = 0.11124365776777267
iteration 275, loss = 0.011489911936223507
iteration 276, loss = 0.01650218293070793
iteration 277, loss = 0.019198007881641388
iteration 278, loss = 0.3911919593811035
iteration 279, loss = 0.023223843425512314
iteration 280, loss = 0.013222381472587585
iteration 281, loss = 0.04246259480714798
iteration 282, loss = 0.024360939860343933
iteration 283, loss = 0.39603784680366516
iteration 284, loss = 0.39837646484375
iteration 285, loss = 0.016272984445095062
iteration 286, loss = 0.3935808837413788
iteration 287, loss = 0.02719460055232048
iteration 288, loss = 0.02070470340549946
iteration 289, loss = 0.3935241103172302
iteration 290, loss = 0.013074161484837532
iteration 291, loss = 0.0179927758872509
iteration 292, loss = 0.011582097969949245
iteration 293, loss = 0.01225884910672903
iteration 294, loss = 0.3897387683391571
iteration 295, loss = 0.015657270327210426
iteration 296, loss = 0.01726033352315426
iteration 297, loss = 0.4102819859981537
iteration 298, loss = 0.015098366886377335
iteration 299, loss = 0.3890898525714874
iteration 300, loss = 0.031949929893016815
iteration 1, loss = 0.018124345690011978
iteration 2, loss = 0.012652293778955936
iteration 3, loss = 0.01795458421111107
iteration 4, loss = 0.017933247610926628
iteration 5, loss = 0.013211453333497047
iteration 6, loss = 0.02720779925584793
iteration 7, loss = 0.025910284370183945
iteration 8, loss = 0.019768957048654556
iteration 9, loss = 0.01230558194220066
iteration 10, loss = 0.021801425144076347
iteration 11, loss = 0.014639539644122124
iteration 12, loss = 0.02948708087205887
iteration 13, loss = 0.017723411321640015
iteration 14, loss = 0.014986496418714523
iteration 15, loss = 0.3965625464916229
iteration 16, loss = 0.01743149198591709
iteration 17, loss = 0.016736336052417755
iteration 18, loss = 0.40753453969955444
iteration 19, loss = 0.40919211506843567
iteration 20, loss = 0.40750330686569214
iteration 21, loss = 0.1043282225728035
iteration 22, loss = 0.031575534492731094
iteration 23, loss = 0.4774662256240845
iteration 24, loss = 0.021332766860723495
iteration 25, loss = 0.01724374294281006
iteration 26, loss = 0.030926024541258812
iteration 27, loss = 0.019476717337965965
iteration 28, loss = 0.38919535279273987
iteration 29, loss = 0.026838649064302444
iteration 30, loss = 0.024739887565374374
iteration 31, loss = 0.39491814374923706
iteration 32, loss = 0.39228537678718567
iteration 33, loss = 0.019850242882966995
iteration 34, loss = 0.10516422986984253
iteration 35, loss = 0.3888688087463379
iteration 36, loss = 0.013892647810280323
iteration 37, loss = 0.38814640045166016
iteration 38, loss = 0.014438924379646778
iteration 39, loss = 0.018746692687273026
iteration 40, loss = 0.39588630199432373
iteration 41, loss = 0.39680761098861694
iteration 42, loss = 0.014043527655303478
iteration 43, loss = 0.04031944274902344
iteration 44, loss = 0.015638291835784912
iteration 45, loss = 0.017025556415319443
iteration 46, loss = 0.42752066254615784
iteration 47, loss = 0.014555919915437698
iteration 48, loss = 0.031428687274456024
iteration 49, loss = 0.020434467121958733
iteration 50, loss = 0.021685218438506126
iteration 51, loss = 0.01927485689520836
iteration 52, loss = 0.3939095139503479
iteration 53, loss = 0.02447868324816227
iteration 54, loss = 0.4116060137748718
iteration 55, loss = 0.014940949156880379
iteration 56, loss = 0.037627797573804855
iteration 57, loss = 0.01847235858440399
iteration 58, loss = 0.10849017649888992
iteration 59, loss = 0.10632370412349701
iteration 60, loss = 0.013419749215245247
iteration 61, loss = 0.02296469360589981
iteration 62, loss = 0.011233524419367313
iteration 63, loss = 0.02390395849943161
iteration 64, loss = 0.01601935550570488
iteration 65, loss = 0.009466116316616535
iteration 66, loss = 0.020967941731214523
iteration 67, loss = 0.01506507396697998
iteration 68, loss = 0.014566695317626
iteration 69, loss = 0.39348891377449036
iteration 70, loss = 0.03803711384534836
iteration 71, loss = 0.017424041405320168
iteration 72, loss = 0.016999009996652603
iteration 73, loss = 0.020657775923609734
iteration 74, loss = 0.02985423430800438
iteration 75, loss = 0.01439481321722269
iteration 76, loss = 0.026317276060581207
iteration 77, loss = 0.4079558253288269
iteration 78, loss = 0.012182740494608879
iteration 79, loss = 0.01565241441130638
iteration 80, loss = 0.015677090734243393
iteration 81, loss = 0.026648107916116714
iteration 82, loss = 0.4009198546409607
iteration 83, loss = 0.022107884287834167
iteration 84, loss = 0.013720469549298286
iteration 85, loss = 0.10210584849119186
iteration 86, loss = 0.03796238824725151
iteration 87, loss = 0.0181680079549551
iteration 88, loss = 0.39370766282081604
iteration 89, loss = 0.012738829478621483
iteration 90, loss = 0.764690637588501
iteration 91, loss = 0.02941584214568138
iteration 92, loss = 0.01881830208003521
iteration 93, loss = 0.020564772188663483
iteration 94, loss = 0.3892764747142792
iteration 95, loss = 0.017186058685183525
iteration 96, loss = 0.02625410631299019
iteration 97, loss = 0.013892442919313908
iteration 98, loss = 0.39564913511276245
iteration 99, loss = 0.01633426919579506
iteration 100, loss = 0.4134379029273987
iteration 101, loss = 0.4773912727832794
iteration 102, loss = 0.013130240142345428
iteration 103, loss = 0.4078567624092102
iteration 104, loss = 0.025280028581619263
iteration 105, loss = 0.017990313470363617
iteration 106, loss = 0.017438475042581558
iteration 107, loss = 0.01938049867749214
iteration 108, loss = 0.01706520840525627
iteration 109, loss = 0.39579692482948303
iteration 110, loss = 0.01845533214509487
iteration 111, loss = 0.024024881422519684
iteration 112, loss = 0.10456902533769608
iteration 113, loss = 0.3939574658870697
iteration 114, loss = 0.01862236298620701
iteration 115, loss = 0.3956371545791626
iteration 116, loss = 0.3907434940338135
iteration 117, loss = 0.4162277579307556
iteration 118, loss = 0.01410627644509077
iteration 119, loss = 0.013651088811457157
iteration 120, loss = 0.035261593759059906
iteration 121, loss = 0.016297386959195137
iteration 122, loss = 0.01504447590559721
iteration 123, loss = 0.39153099060058594
iteration 124, loss = 0.10793253779411316
iteration 125, loss = 0.020383520051836967
iteration 126, loss = 0.7714654803276062
iteration 127, loss = 0.022341102361679077
iteration 128, loss = 0.3866623342037201
iteration 129, loss = 0.02171102911233902
iteration 130, loss = 0.01872430555522442
iteration 131, loss = 0.009972823783755302
iteration 132, loss = 0.48856380581855774
iteration 133, loss = 0.38658010959625244
iteration 134, loss = 0.019559042528271675
iteration 135, loss = 0.008509260602295399
iteration 136, loss = 0.03614161163568497
iteration 137, loss = 0.4179533123970032
iteration 138, loss = 0.39320555329322815
iteration 139, loss = 0.01824810728430748
iteration 140, loss = 0.39344602823257446
iteration 141, loss = 0.01776658557355404
iteration 142, loss = 0.40086662769317627
iteration 143, loss = 0.012033547274768353
iteration 144, loss = 0.7688637971878052
iteration 145, loss = 0.01940244808793068
iteration 146, loss = 0.011281532235443592
iteration 147, loss = 0.39161598682403564
iteration 148, loss = 0.01798807457089424
iteration 149, loss = 0.01441560685634613
iteration 150, loss = 0.021092163398861885
iteration 151, loss = 0.09700072556734085
iteration 152, loss = 0.017252299934625626
iteration 153, loss = 0.024371858686208725
iteration 154, loss = 0.015552827157080173
iteration 155, loss = 0.024876413866877556
iteration 156, loss = 0.021405033767223358
iteration 157, loss = 0.3972316086292267
iteration 158, loss = 0.030377298593521118
iteration 159, loss = 0.015187868848443031
iteration 160, loss = 0.01904868520796299
iteration 161, loss = 0.0237760990858078
iteration 162, loss = 0.017842890694737434
iteration 163, loss = 0.016424553468823433
iteration 164, loss = 0.01134057529270649
iteration 165, loss = 0.0237283855676651
iteration 166, loss = 0.02433551661670208
iteration 167, loss = 0.38989800214767456
iteration 168, loss = 0.01274458970874548
iteration 169, loss = 0.023570900782942772
iteration 170, loss = 0.3920396566390991
iteration 171, loss = 0.014996116980910301
iteration 172, loss = 0.014505941420793533
iteration 173, loss = 0.018230032175779343
iteration 174, loss = 0.40848344564437866
iteration 175, loss = 0.39790478348731995
iteration 176, loss = 0.014401597902178764
iteration 177, loss = 0.01980443485081196
iteration 178, loss = 0.014873546548187733
iteration 179, loss = 0.021590866148471832
iteration 180, loss = 0.008729935623705387
iteration 181, loss = 0.023515284061431885
iteration 182, loss = 0.017573729157447815
iteration 183, loss = 0.40994173288345337
iteration 184, loss = 0.01552976667881012
iteration 185, loss = 0.3962969183921814
iteration 186, loss = 0.014740628190338612
iteration 187, loss = 0.39352095127105713
iteration 188, loss = 0.3845335841178894
iteration 189, loss = 0.01945628970861435
iteration 190, loss = 0.018163949251174927
iteration 191, loss = 0.10373673588037491
iteration 192, loss = 0.38961344957351685
iteration 193, loss = 0.02118355594575405
iteration 194, loss = 0.023108648136258125
iteration 195, loss = 0.04085134342312813
iteration 196, loss = 0.016724180430173874
iteration 197, loss = 0.013343552127480507
iteration 198, loss = 0.38923022150993347
iteration 199, loss = 0.013165117241442204
iteration 200, loss = 0.01522777322679758
iteration 201, loss = 0.3917015790939331
iteration 202, loss = 0.01523338072001934
iteration 203, loss = 0.025164984166622162
iteration 204, loss = 0.3901379704475403
iteration 205, loss = 0.01684924215078354
iteration 206, loss = 0.022258255630731583
iteration 207, loss = 0.01166643388569355
iteration 208, loss = 0.40280964970588684
iteration 209, loss = 0.01906772516667843
iteration 210, loss = 0.015752095729112625
iteration 211, loss = 0.3879953920841217
iteration 212, loss = 0.021061798557639122
iteration 213, loss = 0.019535409286618233
iteration 214, loss = 0.3913511633872986
iteration 215, loss = 0.02534356713294983
iteration 216, loss = 0.038462307304143906
iteration 217, loss = 0.014100514352321625
iteration 218, loss = 0.009068505838513374
iteration 219, loss = 0.01836632564663887
iteration 220, loss = 0.02390536665916443
iteration 221, loss = 0.015564635396003723
iteration 222, loss = 0.017905045300722122
iteration 223, loss = 0.38827890157699585
iteration 224, loss = 0.014973008073866367
iteration 225, loss = 0.024520402774214745
iteration 226, loss = 0.04110334813594818
iteration 227, loss = 0.014582470059394836
iteration 228, loss = 0.036872945725917816
iteration 229, loss = 0.02193506807088852
iteration 230, loss = 0.016775023192167282
iteration 231, loss = 0.015056317672133446
iteration 232, loss = 0.040508463978767395
iteration 233, loss = 0.39444276690483093
iteration 234, loss = 0.019121887162327766
iteration 235, loss = 0.024845382198691368
iteration 236, loss = 0.47151297330856323
iteration 237, loss = 0.024644695222377777
iteration 238, loss = 0.018503405153751373
iteration 239, loss = 0.016820862889289856
iteration 240, loss = 0.7659314870834351
iteration 241, loss = 0.020678158849477768
iteration 242, loss = 0.024040192365646362
iteration 243, loss = 0.39129945635795593
iteration 244, loss = 0.014290904626250267
iteration 245, loss = 0.4072551727294922
iteration 246, loss = 0.016138955950737
iteration 247, loss = 0.022020980715751648
iteration 248, loss = 0.09945733845233917
iteration 249, loss = 0.041840795427560806
iteration 250, loss = 0.4707423448562622
iteration 251, loss = 0.015188063494861126
iteration 252, loss = 0.02337503433227539
iteration 253, loss = 0.39144521951675415
iteration 254, loss = 0.39474716782569885
iteration 255, loss = 0.4058445394039154
iteration 256, loss = 0.010402546264231205
iteration 257, loss = 0.020988445729017258
iteration 258, loss = 0.3918471336364746
iteration 259, loss = 0.012520167045295238
iteration 260, loss = 0.024607136845588684
iteration 261, loss = 0.023227635771036148
iteration 262, loss = 0.01654953509569168
iteration 263, loss = 0.3922185003757477
iteration 264, loss = 0.3911880552768707
iteration 265, loss = 0.3868080973625183
iteration 266, loss = 0.018185999244451523
iteration 267, loss = 0.012045841664075851
iteration 268, loss = 0.0318874791264534
iteration 269, loss = 0.019157910719513893
iteration 270, loss = 0.011452280916273594
iteration 271, loss = 0.01814696565270424
iteration 272, loss = 0.02472176030278206
iteration 273, loss = 0.021554535254836082
iteration 274, loss = 0.02423899807035923
iteration 275, loss = 0.773594319820404
iteration 276, loss = 0.02721954695880413
iteration 277, loss = 0.3864413797855377
iteration 278, loss = 0.026563294231891632
iteration 279, loss = 0.032532718032598495
iteration 280, loss = 0.3955552279949188
iteration 281, loss = 0.02148285135626793
iteration 282, loss = 0.015818722546100616
iteration 283, loss = 0.016768982633948326
iteration 284, loss = 0.032201848924160004
iteration 285, loss = 0.023077163845300674
iteration 286, loss = 0.01557014137506485
iteration 287, loss = 0.028343474492430687
iteration 288, loss = 0.019427411258220673
iteration 289, loss = 0.025807563215494156
iteration 290, loss = 0.018817957490682602
iteration 291, loss = 0.015889788046479225
iteration 292, loss = 0.4104484021663666
iteration 293, loss = 0.02964330092072487
iteration 294, loss = 0.014654312282800674
iteration 295, loss = 0.018583713099360466
iteration 296, loss = 0.017711946740746498
iteration 297, loss = 0.01509254239499569
iteration 298, loss = 0.017541177570819855
iteration 299, loss = 0.011027815751731396
iteration 300, loss = 0.016266290098428726
iteration 1, loss = 0.016986683011054993
iteration 2, loss = 0.025314979255199432
iteration 3, loss = 0.3921051025390625
iteration 4, loss = 0.011297968216240406
iteration 5, loss = 0.011916589923202991
iteration 6, loss = 0.01718071661889553
iteration 7, loss = 0.389024555683136
iteration 8, loss = 0.026909273117780685
iteration 9, loss = 0.012883679941296577
iteration 10, loss = 0.39266061782836914
iteration 11, loss = 0.015188805758953094
iteration 12, loss = 0.03116053342819214
iteration 13, loss = 0.022284485399723053
iteration 14, loss = 0.024850012734532356
iteration 15, loss = 0.023902527987957
iteration 16, loss = 0.021075686439871788
iteration 17, loss = 0.3867146968841553
iteration 18, loss = 0.03723731264472008
iteration 19, loss = 0.022396545857191086
iteration 20, loss = 0.01925721950829029
iteration 21, loss = 0.014729088172316551
iteration 22, loss = 0.39480847120285034
iteration 23, loss = 0.10723769664764404
iteration 24, loss = 0.014552951790392399
iteration 25, loss = 0.022333726286888123
iteration 26, loss = 0.41053709387779236
iteration 27, loss = 0.027608536183834076
iteration 28, loss = 0.014653624035418034
iteration 29, loss = 0.02535320073366165
iteration 30, loss = 0.022839050740003586
iteration 31, loss = 0.015290757641196251
iteration 32, loss = 0.3959808647632599
iteration 33, loss = 0.38782092928886414
iteration 34, loss = 0.029489342123270035
iteration 35, loss = 0.3983583450317383
iteration 36, loss = 0.034865669906139374
iteration 37, loss = 0.01100753154605627
iteration 38, loss = 0.39439013600349426
iteration 39, loss = 0.034903634339571
iteration 40, loss = 0.39690184593200684
iteration 41, loss = 0.01996552385389805
iteration 42, loss = 0.023969179019331932
iteration 43, loss = 0.024427248165011406
iteration 44, loss = 0.01741672493517399
iteration 45, loss = 0.3897770643234253
iteration 46, loss = 0.01528243999928236
iteration 47, loss = 0.38772210478782654
iteration 48, loss = 0.39102423191070557
iteration 49, loss = 0.02075563371181488
iteration 50, loss = 0.021757634356617928
iteration 51, loss = 0.019783582538366318
iteration 52, loss = 0.012690549716353416
iteration 53, loss = 0.012164856307208538
iteration 54, loss = 0.0176082830876112
iteration 55, loss = 0.3880580961704254
iteration 56, loss = 0.02026188373565674
iteration 57, loss = 0.017961760982871056
iteration 58, loss = 0.04322202131152153
iteration 59, loss = 0.4802740514278412
iteration 60, loss = 0.02293301373720169
iteration 61, loss = 0.02249511331319809
iteration 62, loss = 0.12249889224767685
iteration 63, loss = 0.024430446326732635
iteration 64, loss = 0.0226763766258955
iteration 65, loss = 0.014046150259673595
iteration 66, loss = 0.013325550593435764
iteration 67, loss = 0.48143470287323
iteration 68, loss = 0.40029847621917725
iteration 69, loss = 0.015142373740673065
iteration 70, loss = 0.01523284800350666
iteration 71, loss = 0.014469614252448082
iteration 72, loss = 0.01493581011891365
iteration 73, loss = 0.0292699933052063
iteration 74, loss = 0.021218502894043922
iteration 75, loss = 0.4076887369155884
iteration 76, loss = 0.016524149104952812
iteration 77, loss = 0.018698228523135185
iteration 78, loss = 0.030050672590732574
iteration 79, loss = 0.012681698426604271
iteration 80, loss = 0.016053378582000732
iteration 81, loss = 0.38708052039146423
iteration 82, loss = 0.38913556933403015
iteration 83, loss = 0.024236531928181648
iteration 84, loss = 0.02046838030219078
iteration 85, loss = 0.02008952386677265
iteration 86, loss = 0.028686637058854103
iteration 87, loss = 0.49629539251327515
iteration 88, loss = 0.10671154409646988
iteration 89, loss = 0.013981600292026997
iteration 90, loss = 0.014309427700936794
iteration 91, loss = 0.017887411639094353
iteration 92, loss = 0.3895241916179657
iteration 93, loss = 0.09860943257808685
iteration 94, loss = 0.015299263410270214
iteration 95, loss = 0.7632735967636108
iteration 96, loss = 0.3975794315338135
iteration 97, loss = 0.014910977333784103
iteration 98, loss = 0.022896014153957367
iteration 99, loss = 0.023548826575279236
iteration 100, loss = 0.022552860900759697
iteration 101, loss = 0.47051340341567993
iteration 102, loss = 0.021997328847646713
iteration 103, loss = 0.016268126666545868
iteration 104, loss = 0.018758289515972137
iteration 105, loss = 0.4060063362121582
iteration 106, loss = 0.017038874328136444
iteration 107, loss = 0.03159181773662567
iteration 108, loss = 0.10060057789087296
iteration 109, loss = 0.014112746343016624
iteration 110, loss = 0.3979867994785309
iteration 111, loss = 0.020086627453565598
iteration 112, loss = 0.019868455827236176
iteration 113, loss = 0.01279502548277378
iteration 114, loss = 0.019320743158459663
iteration 115, loss = 0.39715903997421265
iteration 116, loss = 0.40581074357032776
iteration 117, loss = 0.4026980698108673
iteration 118, loss = 0.02268536388874054
iteration 119, loss = 0.02182770147919655
iteration 120, loss = 0.38220253586769104
iteration 121, loss = 0.3912685513496399
iteration 122, loss = 0.016572510823607445
iteration 123, loss = 0.012025685980916023
iteration 124, loss = 0.016503669321537018
iteration 125, loss = 0.03390287235379219
iteration 126, loss = 0.018184876069426537
iteration 127, loss = 0.014487136155366898
iteration 128, loss = 0.4019215404987335
iteration 129, loss = 0.020581934601068497
iteration 130, loss = 0.015074492432177067
iteration 131, loss = 0.02535787969827652
iteration 132, loss = 0.39438629150390625
iteration 133, loss = 0.39211755990982056
iteration 134, loss = 0.028044898062944412
iteration 135, loss = 0.01663069799542427
iteration 136, loss = 0.012491019442677498
iteration 137, loss = 0.01265646331012249
iteration 138, loss = 0.023899033665657043
iteration 139, loss = 0.01853301003575325
iteration 140, loss = 0.025205671787261963
iteration 141, loss = 0.7628182172775269
iteration 142, loss = 0.03276798129081726
iteration 143, loss = 0.01235779095441103
iteration 144, loss = 0.020097555592656136
iteration 145, loss = 0.3906419575214386
iteration 146, loss = 0.01371130533516407
iteration 147, loss = 0.7640928030014038
iteration 148, loss = 0.38849005103111267
iteration 149, loss = 0.03148411214351654
iteration 150, loss = 0.01865343004465103
iteration 151, loss = 0.025149434804916382
iteration 152, loss = 0.017876656726002693
iteration 153, loss = 0.023413289338350296
iteration 154, loss = 0.03700314089655876
iteration 155, loss = 0.01819799654185772
iteration 156, loss = 0.018521809950470924
iteration 157, loss = 0.39681869745254517
iteration 158, loss = 0.020951781421899796
iteration 159, loss = 0.39944222569465637
iteration 160, loss = 0.023369258269667625
iteration 161, loss = 0.10183396190404892
iteration 162, loss = 0.019155731424689293
iteration 163, loss = 0.015610498376190662
iteration 164, loss = 0.3860586881637573
iteration 165, loss = 0.016516126692295074
iteration 166, loss = 0.021166495978832245
iteration 167, loss = 0.02378435619175434
iteration 168, loss = 0.3890584111213684
iteration 169, loss = 0.016850465908646584
iteration 170, loss = 0.0372629277408123
iteration 171, loss = 0.11577629297971725
iteration 172, loss = 0.02407146990299225
iteration 173, loss = 0.38929954171180725
iteration 174, loss = 0.018568115308880806
iteration 175, loss = 0.011830125004053116
iteration 176, loss = 0.02373620867729187
iteration 177, loss = 0.40660521388053894
iteration 178, loss = 0.007707827724516392
iteration 179, loss = 0.017862733453512192
iteration 180, loss = 0.03571980446577072
iteration 181, loss = 0.008963415399193764
iteration 182, loss = 0.014734111726284027
iteration 183, loss = 0.019106315448880196
iteration 184, loss = 0.012181032449007034
iteration 185, loss = 0.47529059648513794
iteration 186, loss = 0.01155975554138422
iteration 187, loss = 0.013225463218986988
iteration 188, loss = 0.38885927200317383
iteration 189, loss = 0.025165650993585587
iteration 190, loss = 0.016619021072983742
iteration 191, loss = 0.02110600844025612
iteration 192, loss = 0.019765738397836685
iteration 193, loss = 0.013244923204183578
iteration 194, loss = 0.3897012770175934
iteration 195, loss = 0.014791423454880714
iteration 196, loss = 0.01549433171749115
iteration 197, loss = 0.00908338651061058
iteration 198, loss = 0.38597044348716736
iteration 199, loss = 0.38711851835250854
iteration 200, loss = 0.3892259895801544
iteration 201, loss = 0.3985898792743683
iteration 202, loss = 0.014920518733561039
iteration 203, loss = 0.3983968198299408
iteration 204, loss = 0.040970511734485626
iteration 205, loss = 0.017962053418159485
iteration 206, loss = 0.030523091554641724
iteration 207, loss = 0.01443684846162796
iteration 208, loss = 0.014751546084880829
iteration 209, loss = 0.02753625437617302
iteration 210, loss = 0.016749203205108643
iteration 211, loss = 0.02185901813209057
iteration 212, loss = 0.012756346724927425
iteration 213, loss = 0.02147025242447853
iteration 214, loss = 0.037269629538059235
iteration 215, loss = 0.01811188831925392
iteration 216, loss = 0.022447802126407623
iteration 217, loss = 0.7693017721176147
iteration 218, loss = 0.3861011564731598
iteration 219, loss = 0.028926702216267586
iteration 220, loss = 0.020923949778079987
iteration 221, loss = 0.01952221617102623
iteration 222, loss = 0.46717557311058044
iteration 223, loss = 0.028736986219882965
iteration 224, loss = 0.01327475905418396
iteration 225, loss = 0.0200941264629364
iteration 226, loss = 0.016355661675333977
iteration 227, loss = 0.027769237756729126
iteration 228, loss = 0.009328484535217285
iteration 229, loss = 0.013195482082664967
iteration 230, loss = 0.01655765064060688
iteration 231, loss = 0.10259345918893814
iteration 232, loss = 0.01876160129904747
iteration 233, loss = 0.020220685750246048
iteration 234, loss = 0.029703784734010696
iteration 235, loss = 0.011282786726951599
iteration 236, loss = 0.3926461637020111
iteration 237, loss = 0.3932107090950012
iteration 238, loss = 0.02698656916618347
iteration 239, loss = 0.39293956756591797
iteration 240, loss = 0.0260547436773777
iteration 241, loss = 0.7719113230705261
iteration 242, loss = 0.02584616094827652
iteration 243, loss = 0.015160311944782734
iteration 244, loss = 0.01730523444712162
iteration 245, loss = 0.012878715991973877
iteration 246, loss = 0.028889399021863937
iteration 247, loss = 0.028842411935329437
iteration 248, loss = 0.40596136450767517
iteration 249, loss = 0.3877689242362976
iteration 250, loss = 0.020457282662391663
iteration 251, loss = 0.01900532841682434
iteration 252, loss = 0.014871611259877682
iteration 253, loss = 0.10780110210180283
iteration 254, loss = 0.021149102598428726
iteration 255, loss = 0.026002980768680573
iteration 256, loss = 0.39446890354156494
iteration 257, loss = 0.4027964174747467
iteration 258, loss = 0.011866439133882523
iteration 259, loss = 0.02418128401041031
iteration 260, loss = 0.01278021838515997
iteration 261, loss = 0.017531979829072952
iteration 262, loss = 0.012936224229633808
iteration 263, loss = 0.02034904435276985
iteration 264, loss = 0.01474100910127163
iteration 265, loss = 0.014564922079443932
iteration 266, loss = 0.016708679497241974
iteration 267, loss = 0.020718224346637726
iteration 268, loss = 0.024456754326820374
iteration 269, loss = 0.013053931295871735
iteration 270, loss = 0.021994493901729584
iteration 271, loss = 0.020436756312847137
iteration 272, loss = 0.016895152628421783
iteration 273, loss = 0.012855098582804203
iteration 274, loss = 0.39205121994018555
iteration 275, loss = 0.016630493104457855
iteration 276, loss = 0.013928176835179329
iteration 277, loss = 0.013296617195010185
iteration 278, loss = 0.018593192100524902
iteration 279, loss = 0.3886328637599945
iteration 280, loss = 0.01792333647608757
iteration 281, loss = 0.3944813013076782
iteration 282, loss = 0.39539796113967896
iteration 283, loss = 0.025327328592538834
iteration 284, loss = 0.38329213857650757
iteration 285, loss = 0.015338157303631306
iteration 286, loss = 0.014131862670183182
iteration 287, loss = 0.3876830041408539
iteration 288, loss = 0.4047866463661194
iteration 289, loss = 0.02617621049284935
iteration 290, loss = 0.016221703961491585
iteration 291, loss = 0.022359924390912056
iteration 292, loss = 1.1731692552566528
iteration 293, loss = 0.027824146673083305
iteration 294, loss = 0.015349138528108597
iteration 295, loss = 0.018982019275426865
iteration 296, loss = 0.015288503840565681
iteration 297, loss = 0.0147651182487607
iteration 298, loss = 0.03653348237276077
iteration 299, loss = 0.03344843536615372
iteration 300, loss = 0.036492448300123215
iteration 1, loss = 0.023436084389686584
iteration 2, loss = 0.03669874742627144
iteration 3, loss = 0.38945332169532776
iteration 4, loss = 0.38927212357521057
iteration 5, loss = 0.017793036997318268
iteration 6, loss = 0.38580775260925293
iteration 7, loss = 0.020960619673132896
iteration 8, loss = 0.015096696093678474
iteration 9, loss = 0.3891507089138031
iteration 10, loss = 0.045461416244506836
iteration 11, loss = 0.027514560148119926
iteration 12, loss = 0.39032140374183655
iteration 13, loss = 0.014497754164040089
iteration 14, loss = 0.7650087475776672
iteration 15, loss = 0.012476332485675812
iteration 16, loss = 0.019879793748259544
iteration 17, loss = 0.027842378243803978
iteration 18, loss = 0.016287624835968018
iteration 19, loss = 0.7635671496391296
iteration 20, loss = 0.0167512446641922
iteration 21, loss = 0.013718540780246258
iteration 22, loss = 0.38482287526130676
iteration 23, loss = 0.015546423383057117
iteration 24, loss = 0.015882426872849464
iteration 25, loss = 0.01821861043572426
iteration 26, loss = 0.017172282561659813
iteration 27, loss = 0.014852152206003666
iteration 28, loss = 0.013503551483154297
iteration 29, loss = 0.020604051649570465
iteration 30, loss = 0.013230512849986553
iteration 31, loss = 0.10633678734302521
iteration 32, loss = 0.025068270042538643
iteration 33, loss = 0.012037649750709534
iteration 34, loss = 0.037612803280353546
iteration 35, loss = 0.39646345376968384
iteration 36, loss = 0.023094728589057922
iteration 37, loss = 0.02787831984460354
iteration 38, loss = 0.023504726588726044
iteration 39, loss = 0.018138961866497993
iteration 40, loss = 0.01735910400748253
iteration 41, loss = 0.02582831308245659
iteration 42, loss = 0.016847824677824974
iteration 43, loss = 0.01960849203169346
iteration 44, loss = 0.02008119225502014
iteration 45, loss = 0.02282990887761116
iteration 46, loss = 0.025581177324056625
iteration 47, loss = 0.3880780339241028
iteration 48, loss = 0.020414188504219055
iteration 49, loss = 0.026043999940156937
iteration 50, loss = 0.016074903309345245
iteration 51, loss = 0.025109855458140373
iteration 52, loss = 0.010283743031322956
iteration 53, loss = 0.015049641951918602
iteration 54, loss = 0.01204533688724041
iteration 55, loss = 0.3816371560096741
iteration 56, loss = 0.017464932054281235
iteration 57, loss = 0.7640818357467651
iteration 58, loss = 0.03475832939147949
iteration 59, loss = 0.01674751378595829
iteration 60, loss = 0.01654132455587387
iteration 61, loss = 0.020002122968435287
iteration 62, loss = 0.013013642281293869
iteration 63, loss = 0.012603024020791054
iteration 64, loss = 0.12066067010164261
iteration 65, loss = 0.021018534898757935
iteration 66, loss = 0.009389165788888931
iteration 67, loss = 0.01717669703066349
iteration 68, loss = 0.01919061504304409
iteration 69, loss = 0.7663954496383667
iteration 70, loss = 0.483900785446167
iteration 71, loss = 0.013844444416463375
iteration 72, loss = 0.3899694085121155
iteration 73, loss = 0.017362331971526146
iteration 74, loss = 0.04023244231939316
iteration 75, loss = 0.01784459501504898
iteration 76, loss = 0.39931046962738037
iteration 77, loss = 0.016748502850532532
iteration 78, loss = 0.017184801399707794
iteration 79, loss = 0.4002149701118469
iteration 80, loss = 0.015629800036549568
iteration 81, loss = 0.009331345558166504
iteration 82, loss = 0.3958043158054352
iteration 83, loss = 0.02216988056898117
iteration 84, loss = 0.018915263935923576
iteration 85, loss = 0.017099453136324883
iteration 86, loss = 0.015921229496598244
iteration 87, loss = 0.3945649266242981
iteration 88, loss = 0.03194066882133484
iteration 89, loss = 0.02072083204984665
iteration 90, loss = 0.014536632224917412
iteration 91, loss = 0.029125627130270004
iteration 92, loss = 0.018580002710223198
iteration 93, loss = 0.008457232266664505
iteration 94, loss = 0.39830753207206726
iteration 95, loss = 0.017326490953564644
iteration 96, loss = 0.3965674042701721
iteration 97, loss = 0.03160076215863228
iteration 98, loss = 0.02466364949941635
iteration 99, loss = 0.01644645258784294
iteration 100, loss = 0.01235039159655571
iteration 101, loss = 0.09941424429416656
iteration 102, loss = 0.027006713673472404
iteration 103, loss = 0.020286783576011658
iteration 104, loss = 0.39088472723960876
iteration 105, loss = 0.01102757640182972
iteration 106, loss = 0.02595634013414383
iteration 107, loss = 0.015039865858852863
iteration 108, loss = 0.01575724221765995
iteration 109, loss = 0.387149840593338
iteration 110, loss = 0.01733611710369587
iteration 111, loss = 0.018844448029994965
iteration 112, loss = 0.3836939334869385
iteration 113, loss = 0.021453555673360825
iteration 114, loss = 0.010216389782726765
iteration 115, loss = 0.4101641774177551
iteration 116, loss = 0.025205066427588463
iteration 117, loss = 0.018997229635715485
iteration 118, loss = 0.01079088356345892
iteration 119, loss = 0.3987618088722229
iteration 120, loss = 0.015439916402101517
iteration 121, loss = 0.021614735946059227
iteration 122, loss = 0.023296523839235306
iteration 123, loss = 0.020494500175118446
iteration 124, loss = 0.009514505974948406
iteration 125, loss = 0.01487744227051735
iteration 126, loss = 0.017980458214879036
iteration 127, loss = 0.010140575468540192
iteration 128, loss = 0.401164174079895
iteration 129, loss = 0.017740925773978233
iteration 130, loss = 0.39966610074043274
iteration 131, loss = 0.015521020628511906
iteration 132, loss = 0.022211233153939247
iteration 133, loss = 0.024271883070468903
iteration 134, loss = 0.01741533726453781
iteration 135, loss = 0.014760461635887623
iteration 136, loss = 0.7786269187927246
iteration 137, loss = 0.10339730978012085
iteration 138, loss = 0.019948810338974
iteration 139, loss = 0.024546995759010315
iteration 140, loss = 0.7603834271430969
iteration 141, loss = 0.013344240374863148
iteration 142, loss = 0.023498322814702988
iteration 143, loss = 0.016743695363402367
iteration 144, loss = 0.01629236526787281
iteration 145, loss = 0.017860254272818565
iteration 146, loss = 0.4065055549144745
iteration 147, loss = 0.3925486207008362
iteration 148, loss = 0.7844424843788147
iteration 149, loss = 0.3924190104007721
iteration 150, loss = 0.3873864710330963
iteration 151, loss = 0.013828184455633163
iteration 152, loss = 0.019836226478219032
iteration 153, loss = 0.01616392470896244
iteration 154, loss = 0.014773965813219547
iteration 155, loss = 0.02309943363070488
iteration 156, loss = 0.026514176279306412
iteration 157, loss = 0.38996896147727966
iteration 158, loss = 0.022439077496528625
iteration 159, loss = 0.42006754875183105
iteration 160, loss = 0.01631847769021988
iteration 161, loss = 0.018836647272109985
iteration 162, loss = 0.3953510522842407
iteration 163, loss = 0.019087716937065125
iteration 164, loss = 0.02008776180446148
iteration 165, loss = 0.017930686473846436
iteration 166, loss = 0.018187087029218674
iteration 167, loss = 0.38602977991104126
iteration 168, loss = 0.026915088295936584
iteration 169, loss = 0.39109206199645996
iteration 170, loss = 0.398031085729599
iteration 171, loss = 0.019676167517900467
iteration 172, loss = 0.016955599188804626
iteration 173, loss = 0.014268177561461926
iteration 174, loss = 0.01998169720172882
iteration 175, loss = 0.3838116526603699
iteration 176, loss = 0.014227310195565224
iteration 177, loss = 0.017218340188264847
iteration 178, loss = 0.39721179008483887
iteration 179, loss = 0.015155036002397537
iteration 180, loss = 0.024882327765226364
iteration 181, loss = 0.02173653431236744
iteration 182, loss = 0.011022903956472874
iteration 183, loss = 0.02237415872514248
iteration 184, loss = 0.018813638016581535
iteration 185, loss = 0.10685931891202927
iteration 186, loss = 0.030469723045825958
iteration 187, loss = 0.018502069637179375
iteration 188, loss = 0.017746150493621826
iteration 189, loss = 0.011802284978330135
iteration 190, loss = 0.40966498851776123
iteration 191, loss = 0.0211016908288002
iteration 192, loss = 0.48626288771629333
iteration 193, loss = 0.01897948421537876
iteration 194, loss = 0.019406693056225777
iteration 195, loss = 0.02383096143603325
iteration 196, loss = 0.021597139537334442
iteration 197, loss = 0.016515444964170456
iteration 198, loss = 0.021983986720442772
iteration 199, loss = 0.019243018701672554
iteration 200, loss = 0.3906145691871643
iteration 201, loss = 0.019320741295814514
iteration 202, loss = 0.9354588389396667
iteration 203, loss = 0.019688474014401436
iteration 204, loss = 0.019240379333496094
iteration 205, loss = 0.015924369916319847
iteration 206, loss = 0.02501094900071621
iteration 207, loss = 0.011870906688272953
iteration 208, loss = 0.014942193403840065
iteration 209, loss = 0.01848500221967697
iteration 210, loss = 0.023267069831490517
iteration 211, loss = 0.40395379066467285
iteration 212, loss = 0.01647905632853508
iteration 213, loss = 0.02379152923822403
iteration 214, loss = 0.023513928055763245
iteration 215, loss = 0.3993595242500305
iteration 216, loss = 0.017641615122556686
iteration 217, loss = 0.01579410769045353
iteration 218, loss = 0.02201922982931137
iteration 219, loss = 0.021963492035865784
iteration 220, loss = 0.01207750290632248
iteration 221, loss = 0.019545577466487885
iteration 222, loss = 0.013220319524407387
iteration 223, loss = 0.3956432044506073
iteration 224, loss = 0.016360033303499222
iteration 225, loss = 0.018355268985033035
iteration 226, loss = 0.771512508392334
iteration 227, loss = 0.012763568200170994
iteration 228, loss = 0.018428537994623184
iteration 229, loss = 0.022987274453043938
iteration 230, loss = 0.02057351917028427
iteration 231, loss = 0.020906291902065277
iteration 232, loss = 0.0143672414124012
iteration 233, loss = 0.3856288194656372
iteration 234, loss = 0.013762366026639938
iteration 235, loss = 0.02249947190284729
iteration 236, loss = 0.03427152708172798
iteration 237, loss = 0.38761842250823975
iteration 238, loss = 0.3994840085506439
iteration 239, loss = 0.41481849551200867
iteration 240, loss = 0.01818876340985298
iteration 241, loss = 0.021411439403891563
iteration 242, loss = 0.49074316024780273
iteration 243, loss = 0.47094735503196716
iteration 244, loss = 0.7571766972541809
iteration 245, loss = 0.014620045199990273
iteration 246, loss = 0.013174131512641907
iteration 247, loss = 0.025243479758501053
iteration 248, loss = 0.02235455811023712
iteration 249, loss = 0.02933848649263382
iteration 250, loss = 0.019442729651927948
iteration 251, loss = 0.030658544972538948
iteration 252, loss = 0.399970144033432
iteration 253, loss = 0.38647589087486267
iteration 254, loss = 0.3853914737701416
iteration 255, loss = 0.026373570784926414
iteration 256, loss = 0.014065379276871681
iteration 257, loss = 0.01380070112645626
iteration 258, loss = 0.014581045135855675
iteration 259, loss = 0.01109339576214552
iteration 260, loss = 0.3898472785949707
iteration 261, loss = 0.01666000857949257
iteration 262, loss = 0.38432005047798157
iteration 263, loss = 0.7569355368614197
iteration 264, loss = 0.38806772232055664
iteration 265, loss = 0.02155117131769657
iteration 266, loss = 0.040119122713804245
iteration 267, loss = 0.39185968041419983
iteration 268, loss = 0.3919747471809387
iteration 269, loss = 0.020179521292448044
iteration 270, loss = 0.01756484992802143
iteration 271, loss = 0.03893514350056648
iteration 272, loss = 0.01842212677001953
iteration 273, loss = 0.10414647310972214
iteration 274, loss = 0.01674775220453739
iteration 275, loss = 0.04323682188987732
iteration 276, loss = 0.02465493604540825
iteration 277, loss = 0.030463090166449547
iteration 278, loss = 0.026505520567297935
iteration 279, loss = 0.01678849384188652
iteration 280, loss = 0.012388293631374836
iteration 281, loss = 0.018480749800801277
iteration 282, loss = 0.4795417785644531
iteration 283, loss = 0.017510665580630302
iteration 284, loss = 0.025067897513508797
iteration 285, loss = 0.033754266798496246
iteration 286, loss = 0.018974771723151207
iteration 287, loss = 0.027195803821086884
iteration 288, loss = 0.48207393288612366
iteration 289, loss = 0.01958758570253849
iteration 290, loss = 0.020911620929837227
iteration 291, loss = 0.021334782242774963
iteration 292, loss = 0.0103566600009799
iteration 293, loss = 0.013038977049291134
iteration 294, loss = 0.01914559118449688
iteration 295, loss = 0.018718106672167778
iteration 296, loss = 0.09300502389669418
iteration 297, loss = 0.022061727941036224
iteration 298, loss = 0.02894982136785984
iteration 299, loss = 0.02507881261408329
iteration 300, loss = 0.02158234640955925
iteration 1, loss = 0.39327704906463623
iteration 2, loss = 0.09598254412412643
iteration 3, loss = 0.021811723709106445
iteration 4, loss = 0.02164839208126068
iteration 5, loss = 0.03699196130037308
iteration 6, loss = 0.4119071960449219
iteration 7, loss = 0.018499083817005157
iteration 8, loss = 0.08899308741092682
iteration 9, loss = 0.03290516138076782
iteration 10, loss = 0.0146406264975667
iteration 11, loss = 0.01773170381784439
iteration 12, loss = 0.018481528386473656
iteration 13, loss = 0.09768091142177582
iteration 14, loss = 0.022521328181028366
iteration 15, loss = 0.012348986230790615
iteration 16, loss = 0.019169658422470093
iteration 17, loss = 0.016925472766160965
iteration 18, loss = 0.01893211528658867
iteration 19, loss = 0.014986054971814156
iteration 20, loss = 0.03349681943655014
iteration 21, loss = 0.01639009267091751
iteration 22, loss = 0.01649252511560917
iteration 23, loss = 0.03021394833922386
iteration 24, loss = 0.03172730281949043
iteration 25, loss = 0.013193891383707523
iteration 26, loss = 0.022916331887245178
iteration 27, loss = 0.02334941178560257
iteration 28, loss = 0.015964604914188385
iteration 29, loss = 0.38868966698646545
iteration 30, loss = 0.013708364218473434
iteration 31, loss = 0.38986286520957947
iteration 32, loss = 0.02087431773543358
iteration 33, loss = 0.02994261309504509
iteration 34, loss = 0.019566727802157402
iteration 35, loss = 0.4680042564868927
iteration 36, loss = 0.39115476608276367
iteration 37, loss = 0.3962044417858124
iteration 38, loss = 0.3866957724094391
iteration 39, loss = 0.09653747826814651
iteration 40, loss = 0.030402090400457382
iteration 41, loss = 0.01617753691971302
iteration 42, loss = 0.016654137521982193
iteration 43, loss = 0.021316077560186386
iteration 44, loss = 0.02229609712958336
iteration 45, loss = 0.4102016091346741
iteration 46, loss = 0.01118504535406828
iteration 47, loss = 0.4113212525844574
iteration 48, loss = 0.016451358795166016
iteration 49, loss = 0.021142475306987762
iteration 50, loss = 0.024738410487771034
iteration 51, loss = 0.031388767063617706
iteration 52, loss = 0.01739725098013878
iteration 53, loss = 0.014330080710351467
iteration 54, loss = 0.018933353945612907
iteration 55, loss = 0.3921450972557068
iteration 56, loss = 0.01654702238738537
iteration 57, loss = 0.019134119153022766
iteration 58, loss = 0.014346776530146599
iteration 59, loss = 0.014614244922995567
iteration 60, loss = 0.01442379504442215
iteration 61, loss = 0.012610900215804577
iteration 62, loss = 0.011336788535118103
iteration 63, loss = 0.39299115538597107
iteration 64, loss = 0.7621418237686157
iteration 65, loss = 0.029379945248365402
iteration 66, loss = 0.016812175512313843
iteration 67, loss = 0.02509273774921894
iteration 68, loss = 0.019963929429650307
iteration 69, loss = 0.01970016211271286
iteration 70, loss = 0.01575392670929432
iteration 71, loss = 0.018492169678211212
iteration 72, loss = 0.012646550312638283
iteration 73, loss = 0.020445451140403748
iteration 74, loss = 0.012071930803358555
iteration 75, loss = 0.017845645546913147
iteration 76, loss = 0.021231237798929214
iteration 77, loss = 0.023072492331266403
iteration 78, loss = 0.016553957015275955
iteration 79, loss = 0.38723787665367126
iteration 80, loss = 0.014193112030625343
iteration 81, loss = 0.014353166334331036
iteration 82, loss = 0.01014561764895916
iteration 83, loss = 0.01107173040509224
iteration 84, loss = 0.020970920100808144
iteration 85, loss = 0.3881978392601013
iteration 86, loss = 0.38970887660980225
iteration 87, loss = 0.03899208456277847
iteration 88, loss = 0.014428576454520226
iteration 89, loss = 0.018505342304706573
iteration 90, loss = 0.016910430043935776
iteration 91, loss = 0.029146552085876465
iteration 92, loss = 0.3901129961013794
iteration 93, loss = 0.016181036829948425
iteration 94, loss = 0.022484172135591507
iteration 95, loss = 0.3960384428501129
iteration 96, loss = 0.3893042504787445
iteration 97, loss = 0.018210560083389282
iteration 98, loss = 0.014029915444552898
iteration 99, loss = 0.023442883044481277
iteration 100, loss = 0.01525317132472992
iteration 101, loss = 0.015922510996460915
iteration 102, loss = 0.01060512475669384
iteration 103, loss = 0.4074488878250122
iteration 104, loss = 0.028432825580239296
iteration 105, loss = 0.019213277846574783
iteration 106, loss = 0.3945353031158447
iteration 107, loss = 0.0276312418282032
iteration 108, loss = 0.009853377938270569
iteration 109, loss = 0.022801510989665985
iteration 110, loss = 0.40020909905433655
iteration 111, loss = 0.38805070519447327
iteration 112, loss = 0.016615793108940125
iteration 113, loss = 0.0287616029381752
iteration 114, loss = 0.39969977736473083
iteration 115, loss = 0.018454836681485176
iteration 116, loss = 0.027439799159765244
iteration 117, loss = 0.023951243609189987
iteration 118, loss = 0.3922262191772461
iteration 119, loss = 0.015081082470715046
iteration 120, loss = 0.019660916179418564
iteration 121, loss = 0.012687244452536106
iteration 122, loss = 0.023673003539443016
iteration 123, loss = 0.7649808526039124
iteration 124, loss = 0.012607802636921406
iteration 125, loss = 0.01538405567407608
iteration 126, loss = 0.39477717876434326
iteration 127, loss = 0.013735106214880943
iteration 128, loss = 0.09969611465930939
iteration 129, loss = 0.023045387119054794
iteration 130, loss = 0.39640817046165466
iteration 131, loss = 0.020734524354338646
iteration 132, loss = 0.019579123705625534
iteration 133, loss = 0.031146250665187836
iteration 134, loss = 0.38427436351776123
iteration 135, loss = 0.38688400387763977
iteration 136, loss = 0.38920727372169495
iteration 137, loss = 0.3912844657897949
iteration 138, loss = 0.012599291279911995
iteration 139, loss = 0.017816869542002678
iteration 140, loss = 0.01309974491596222
iteration 141, loss = 0.7756764888763428
iteration 142, loss = 0.020070938393473625
iteration 143, loss = 0.02653651311993599
iteration 144, loss = 0.016058972105383873
iteration 145, loss = 0.012512772344052792
iteration 146, loss = 0.02251400798559189
iteration 147, loss = 0.0223260298371315
iteration 148, loss = 0.02242528647184372
iteration 149, loss = 0.3904288709163666
iteration 150, loss = 0.017486948519945145
iteration 151, loss = 0.019609903916716576
iteration 152, loss = 0.0230485238134861
iteration 153, loss = 0.01572210155427456
iteration 154, loss = 0.02003365010023117
iteration 155, loss = 0.02524520643055439
iteration 156, loss = 0.016587713733315468
iteration 157, loss = 0.02102438546717167
iteration 158, loss = 0.4697248637676239
iteration 159, loss = 0.7698975801467896
iteration 160, loss = 0.019287995994091034
iteration 161, loss = 0.022261440753936768
iteration 162, loss = 0.3883415162563324
iteration 163, loss = 0.008899819105863571
iteration 164, loss = 0.023110413923859596
iteration 165, loss = 0.018841251730918884
iteration 166, loss = 0.017835333943367004
iteration 167, loss = 0.019019223749637604
iteration 168, loss = 0.030550437048077583
iteration 169, loss = 0.03921671211719513
iteration 170, loss = 0.39327219128608704
iteration 171, loss = 0.47828972339630127
iteration 172, loss = 0.017552616074681282
iteration 173, loss = 0.034184087067842484
iteration 174, loss = 0.01311325654387474
iteration 175, loss = 0.01586165465414524
iteration 176, loss = 0.013016785494983196
iteration 177, loss = 0.0147049929946661
iteration 178, loss = 0.025346269831061363
iteration 179, loss = 0.012488186359405518
iteration 180, loss = 0.38751453161239624
iteration 181, loss = 0.009992157109081745
iteration 182, loss = 0.40372902154922485
iteration 183, loss = 0.3857400417327881
iteration 184, loss = 0.39446768164634705
iteration 185, loss = 0.39935246109962463
iteration 186, loss = 0.40572112798690796
iteration 187, loss = 0.026302872225642204
iteration 188, loss = 0.39069831371307373
iteration 189, loss = 0.019535915926098824
iteration 190, loss = 0.017544548958539963
iteration 191, loss = 0.020089834928512573
iteration 192, loss = 0.10056214034557343
iteration 193, loss = 0.013599807396531105
iteration 194, loss = 0.023881960660219193
iteration 195, loss = 0.01554296724498272
iteration 196, loss = 0.02772386372089386
iteration 197, loss = 0.02146073803305626
iteration 198, loss = 0.017969008535146713
iteration 199, loss = 0.03002454712986946
iteration 200, loss = 0.01867246814072132
iteration 201, loss = 0.09664301574230194
iteration 202, loss = 0.018159491941332817
iteration 203, loss = 0.018628109246492386
iteration 204, loss = 0.39451003074645996
iteration 205, loss = 0.022573385387659073
iteration 206, loss = 0.028695926070213318
iteration 207, loss = 0.025343716144561768
iteration 208, loss = 0.01886344514787197
iteration 209, loss = 0.016045797616243362
iteration 210, loss = 0.017308460548520088
iteration 211, loss = 0.03689771890640259
iteration 212, loss = 0.020863015204668045
iteration 213, loss = 0.3903413712978363
iteration 214, loss = 0.013833053410053253
iteration 215, loss = 0.015603160485625267
iteration 216, loss = 0.018758177757263184
iteration 217, loss = 0.021549642086029053
iteration 218, loss = 0.3826276361942291
iteration 219, loss = 0.11360308527946472
iteration 220, loss = 0.3873511254787445
iteration 221, loss = 0.02301078476011753
iteration 222, loss = 0.018789304420351982
iteration 223, loss = 0.3830088973045349
iteration 224, loss = 0.016604136675596237
iteration 225, loss = 0.018344607204198837
iteration 226, loss = 0.022408440709114075
iteration 227, loss = 0.035022053867578506
iteration 228, loss = 0.030696189031004906
iteration 229, loss = 0.01839640364050865
iteration 230, loss = 0.3958379626274109
iteration 231, loss = 0.018657371401786804
iteration 232, loss = 0.02288373000919819
iteration 233, loss = 0.39903196692466736
iteration 234, loss = 0.020658859983086586
iteration 235, loss = 0.012319657951593399
iteration 236, loss = 0.48850780725479126
iteration 237, loss = 0.7665677666664124
iteration 238, loss = 0.3838328421115875
iteration 239, loss = 0.38957691192626953
iteration 240, loss = 0.01342558953911066
iteration 241, loss = 0.01873878389596939
iteration 242, loss = 0.026546958833932877
iteration 243, loss = 0.020150242373347282
iteration 244, loss = 0.011753994040191174
iteration 245, loss = 0.03302562236785889
iteration 246, loss = 0.4099145829677582
iteration 247, loss = 0.025724060833454132
iteration 248, loss = 0.017597684636712074
iteration 249, loss = 0.0160597525537014
iteration 250, loss = 0.027844155207276344
iteration 251, loss = 0.018071528524160385
iteration 252, loss = 0.39289551973342896
iteration 253, loss = 0.02826368808746338
iteration 254, loss = 0.01157345436513424
iteration 255, loss = 0.386439710855484
iteration 256, loss = 0.4677639603614807
iteration 257, loss = 0.03253389894962311
iteration 258, loss = 0.40193119645118713
iteration 259, loss = 0.021555975079536438
iteration 260, loss = 0.3897267282009125
iteration 261, loss = 0.013420261442661285
iteration 262, loss = 0.023693492636084557
iteration 263, loss = 0.020741336047649384
iteration 264, loss = 0.016385594382882118
iteration 265, loss = 0.38667821884155273
iteration 266, loss = 0.013806729577481747
iteration 267, loss = 0.018771804869174957
iteration 268, loss = 0.013005940243601799
iteration 269, loss = 0.013313621282577515
iteration 270, loss = 0.034411631524562836
iteration 271, loss = 0.01709730178117752
iteration 272, loss = 0.02116742730140686
iteration 273, loss = 0.01891796477138996
iteration 274, loss = 0.01397490594536066
iteration 275, loss = 0.3940950632095337
iteration 276, loss = 0.01447128877043724
iteration 277, loss = 0.014109351672232151
iteration 278, loss = 0.010236111469566822
iteration 279, loss = 0.4061809182167053
iteration 280, loss = 0.017274167388677597
iteration 281, loss = 0.39221182465553284
iteration 282, loss = 0.018519051373004913
iteration 283, loss = 0.012266467325389385
iteration 284, loss = 0.38787439465522766
iteration 285, loss = 0.012421656399965286
iteration 286, loss = 0.013494191691279411
iteration 287, loss = 0.020011331886053085
iteration 288, loss = 0.09862753003835678
iteration 289, loss = 0.394580602645874
iteration 290, loss = 0.3836585581302643
iteration 291, loss = 0.39528536796569824
iteration 292, loss = 0.01898055151104927
iteration 293, loss = 0.40161800384521484
iteration 294, loss = 0.03637317940592766
iteration 295, loss = 0.018825598061084747
iteration 296, loss = 0.3836926519870758
iteration 297, loss = 0.01461300440132618
iteration 298, loss = 0.48129063844680786
iteration 299, loss = 0.0138980932533741
iteration 300, loss = 0.3964841663837433
iteration 1, loss = 0.4007797837257385
iteration 2, loss = 0.018652675673365593
iteration 3, loss = 0.017157718539237976
iteration 4, loss = 0.02551104500889778
iteration 5, loss = 0.3874248266220093
iteration 6, loss = 0.46314239501953125
iteration 7, loss = 0.018953053280711174
iteration 8, loss = 0.01581951044499874
iteration 9, loss = 0.01621912233531475
iteration 10, loss = 0.021809494122862816
iteration 11, loss = 0.39019685983657837
iteration 12, loss = 0.012364900670945644
iteration 13, loss = 0.015690192580223083
iteration 14, loss = 0.38316985964775085
iteration 15, loss = 0.3941434919834137
iteration 16, loss = 0.01502480823546648
iteration 17, loss = 0.018386926501989365
iteration 18, loss = 0.01575709879398346
iteration 19, loss = 0.020264634862542152
iteration 20, loss = 0.3890758454799652
iteration 21, loss = 0.020896492525935173
iteration 22, loss = 0.02822890877723694
iteration 23, loss = 0.009276732802391052
iteration 24, loss = 0.014038155786693096
iteration 25, loss = 0.39526158571243286
iteration 26, loss = 0.39119118452072144
iteration 27, loss = 0.40772750973701477
iteration 28, loss = 0.012219355441629887
iteration 29, loss = 0.029252994805574417
iteration 30, loss = 0.026438921689987183
iteration 31, loss = 0.016905605792999268
iteration 32, loss = 0.012814151123166084
iteration 33, loss = 0.09901713579893112
iteration 34, loss = 0.02920132502913475
iteration 35, loss = 0.013479497283697128
iteration 36, loss = 0.02010061964392662
iteration 37, loss = 0.011973472312092781
iteration 38, loss = 0.024320192635059357
iteration 39, loss = 0.02017251029610634
iteration 40, loss = 0.01676792837679386
iteration 41, loss = 0.02128215879201889
iteration 42, loss = 0.3898259997367859
iteration 43, loss = 0.3935275077819824
iteration 44, loss = 0.39622199535369873
iteration 45, loss = 0.01478542946279049
iteration 46, loss = 0.4127851128578186
iteration 47, loss = 0.015912747010588646
iteration 48, loss = 0.017345426604151726
iteration 49, loss = 0.019642211496829987
iteration 50, loss = 0.015079009346663952
iteration 51, loss = 0.025736885145306587
iteration 52, loss = 0.04218916594982147
iteration 53, loss = 0.017008956521749496
iteration 54, loss = 0.3834380805492401
iteration 55, loss = 0.01224466972053051
iteration 56, loss = 0.40357667207717896
iteration 57, loss = 0.019299186766147614
iteration 58, loss = 0.009315498173236847
iteration 59, loss = 0.022372446954250336
iteration 60, loss = 0.009607549756765366
iteration 61, loss = 0.46613630652427673
iteration 62, loss = 0.014257494360208511
iteration 63, loss = 0.019917378202080727
iteration 64, loss = 0.012943421490490437
iteration 65, loss = 0.018580147996544838
iteration 66, loss = 0.022674119099974632
iteration 67, loss = 0.01828494295477867
iteration 68, loss = 0.011273887939751148
iteration 69, loss = 0.03220229223370552
iteration 70, loss = 0.38607197999954224
iteration 71, loss = 0.3874633014202118
iteration 72, loss = 0.021043287590146065
iteration 73, loss = 0.018410619348287582
iteration 74, loss = 0.03528352081775665
iteration 75, loss = 0.0297432541847229
iteration 76, loss = 0.014077392406761646
iteration 77, loss = 0.014195962809026241
iteration 78, loss = 0.019755400717258453
iteration 79, loss = 0.02463027648627758
iteration 80, loss = 0.030983613803982735
iteration 81, loss = 0.03297413885593414
iteration 82, loss = 0.0230304766446352
iteration 83, loss = 0.08854413777589798
iteration 84, loss = 0.04482339322566986
iteration 85, loss = 0.021691350266337395
iteration 86, loss = 0.01315598376095295
iteration 87, loss = 0.018966730684041977
iteration 88, loss = 0.025112003087997437
iteration 89, loss = 0.01840965449810028
iteration 90, loss = 0.38692936301231384
iteration 91, loss = 0.02622351050376892
iteration 92, loss = 0.7603628635406494
iteration 93, loss = 0.4057653546333313
iteration 94, loss = 0.020069293677806854
iteration 95, loss = 0.04827329143881798
iteration 96, loss = 0.3897327482700348
iteration 97, loss = 0.019793838262557983
iteration 98, loss = 0.7604623436927795
iteration 99, loss = 0.038599275052547455
iteration 100, loss = 0.3893560469150543
iteration 101, loss = 0.39109310507774353
iteration 102, loss = 0.01742795668542385
iteration 103, loss = 0.014356036670506
iteration 104, loss = 0.01745295524597168
iteration 105, loss = 0.01999673992395401
iteration 106, loss = 0.46725666522979736
iteration 107, loss = 0.013916600495576859
iteration 108, loss = 0.010843651369214058
iteration 109, loss = 0.015478557907044888
iteration 110, loss = 0.3845229148864746
iteration 111, loss = 0.3865271210670471
iteration 112, loss = 0.39210641384124756
iteration 113, loss = 0.01513807661831379
iteration 114, loss = 0.02436465211212635
iteration 115, loss = 0.03028332069516182
iteration 116, loss = 0.011862293817102909
iteration 117, loss = 0.018376071006059647
iteration 118, loss = 0.014519773423671722
iteration 119, loss = 0.020791437476873398
iteration 120, loss = 0.02780185081064701
iteration 121, loss = 0.025781717151403427
iteration 122, loss = 0.014749795198440552
iteration 123, loss = 0.3896991014480591
iteration 124, loss = 0.0218715351074934
iteration 125, loss = 0.09003476798534393
iteration 126, loss = 0.7562102675437927
iteration 127, loss = 0.00936819612979889
iteration 128, loss = 0.01825880818068981
iteration 129, loss = 0.011684158816933632
iteration 130, loss = 0.38280975818634033
iteration 131, loss = 1.1282241344451904
iteration 132, loss = 0.38425949215888977
iteration 133, loss = 0.015998121351003647
iteration 134, loss = 0.017010532319545746
iteration 135, loss = 0.030254658311605453
iteration 136, loss = 0.010621996596455574
iteration 137, loss = 0.013619854114949703
iteration 138, loss = 0.013003654778003693
iteration 139, loss = 0.010792204178869724
iteration 140, loss = 0.02267363853752613
iteration 141, loss = 0.013026327826082706
iteration 142, loss = 0.026284413412213326
iteration 143, loss = 0.015450840815901756
iteration 144, loss = 0.029935317113995552
iteration 145, loss = 0.39323270320892334
iteration 146, loss = 0.022105632349848747
iteration 147, loss = 0.01985485851764679
iteration 148, loss = 0.4139515161514282
iteration 149, loss = 0.019749119877815247
iteration 150, loss = 0.016961801797151566
iteration 151, loss = 0.03150782361626625
iteration 152, loss = 0.10656461864709854
iteration 153, loss = 0.020445171743631363
iteration 154, loss = 0.019970450550317764
iteration 155, loss = 0.02344851940870285
iteration 156, loss = 0.38614514470100403
iteration 157, loss = 0.01928432658314705
iteration 158, loss = 0.38274094462394714
iteration 159, loss = 0.392570823431015
iteration 160, loss = 0.39158788323402405
iteration 161, loss = 0.016493424773216248
iteration 162, loss = 0.019772475585341454
iteration 163, loss = 0.01649620197713375
iteration 164, loss = 0.013439086265861988
iteration 165, loss = 0.4727025032043457
iteration 166, loss = 0.01216972153633833
iteration 167, loss = 0.3885524272918701
iteration 168, loss = 0.013691862113773823
iteration 169, loss = 0.01385616697371006
iteration 170, loss = 0.028895314782857895
iteration 171, loss = 0.014524934813380241
iteration 172, loss = 0.38824084401130676
iteration 173, loss = 0.012805569916963577
iteration 174, loss = 0.3871341645717621
iteration 175, loss = 0.014493958093225956
iteration 176, loss = 0.025407763198018074
iteration 177, loss = 0.023803168907761574
iteration 178, loss = 0.018701475113630295
iteration 179, loss = 0.01947134919464588
iteration 180, loss = 0.3899018168449402
iteration 181, loss = 0.034598007798194885
iteration 182, loss = 0.385417640209198
iteration 183, loss = 0.02269602380692959
iteration 184, loss = 0.025547340512275696
iteration 185, loss = 0.016795644536614418
iteration 186, loss = 0.018735013902187347
iteration 187, loss = 0.015474643558263779
iteration 188, loss = 0.01836666278541088
iteration 189, loss = 0.38900327682495117
iteration 190, loss = 0.017853958532214165
iteration 191, loss = 0.015746716409921646
iteration 192, loss = 0.010559234768152237
iteration 193, loss = 0.03706669062376022
iteration 194, loss = 0.015568234957754612
iteration 195, loss = 0.03286563605070114
iteration 196, loss = 0.014175714924931526
iteration 197, loss = 0.014998859725892544
iteration 198, loss = 0.02347356639802456
iteration 199, loss = 0.020451707765460014
iteration 200, loss = 0.7540624141693115
iteration 201, loss = 0.39924532175064087
iteration 202, loss = 0.02014121785759926
iteration 203, loss = 0.027515023946762085
iteration 204, loss = 0.029255423694849014
iteration 205, loss = 0.02854391559958458
iteration 206, loss = 0.01611778326332569
iteration 207, loss = 0.011085941456258297
iteration 208, loss = 0.020636102184653282
iteration 209, loss = 0.39181751012802124
iteration 210, loss = 0.024427317082881927
iteration 211, loss = 0.3832072615623474
iteration 212, loss = 0.02582915872335434
iteration 213, loss = 0.023192139342427254
iteration 214, loss = 0.47455114126205444
iteration 215, loss = 0.012234203517436981
iteration 216, loss = 0.38798391819000244
iteration 217, loss = 0.014514707028865814
iteration 218, loss = 0.023306354880332947
iteration 219, loss = 0.027017395943403244
iteration 220, loss = 0.023313939571380615
iteration 221, loss = 0.015802688896656036
iteration 222, loss = 0.09702569991350174
iteration 223, loss = 0.014371238648891449
iteration 224, loss = 0.3818178176879883
iteration 225, loss = 0.02088700421154499
iteration 226, loss = 0.02378752827644348
iteration 227, loss = 0.021181948482990265
iteration 228, loss = 0.38764187693595886
iteration 229, loss = 0.3908526301383972
iteration 230, loss = 0.39024263620376587
iteration 231, loss = 0.39475521445274353
iteration 232, loss = 0.40888962149620056
iteration 233, loss = 0.018842114135622978
iteration 234, loss = 0.3887020945549011
iteration 235, loss = 0.09438924491405487
iteration 236, loss = 0.022981757298111916
iteration 237, loss = 0.019749918952584267
iteration 238, loss = 0.4684560298919678
iteration 239, loss = 0.024792172014713287
iteration 240, loss = 0.01821189373731613
iteration 241, loss = 0.38523897528648376
iteration 242, loss = 0.017098749056458473
iteration 243, loss = 0.026611782610416412
iteration 244, loss = 0.020501676946878433
iteration 245, loss = 0.017757944762706757
iteration 246, loss = 0.03341468796133995
iteration 247, loss = 0.0327337309718132
iteration 248, loss = 0.015516621060669422
iteration 249, loss = 0.01701873540878296
iteration 250, loss = 0.01204829104244709
iteration 251, loss = 0.029456928372383118
iteration 252, loss = 0.019302494823932648
iteration 253, loss = 0.02057464048266411
iteration 254, loss = 0.013541429303586483
iteration 255, loss = 0.009526016190648079
iteration 256, loss = 0.01962820068001747
iteration 257, loss = 0.02859503962099552
iteration 258, loss = 0.015258497558534145
iteration 259, loss = 0.017466269433498383
iteration 260, loss = 0.3970557749271393
iteration 261, loss = 0.015388249419629574
iteration 262, loss = 0.027387933805584908
iteration 263, loss = 0.09222666919231415
iteration 264, loss = 0.01778228208422661
iteration 265, loss = 0.3900257647037506
iteration 266, loss = 0.01828816719353199
iteration 267, loss = 0.01894332841038704
iteration 268, loss = 0.4081023335456848
iteration 269, loss = 0.011062804609537125
iteration 270, loss = 0.012273767963051796
iteration 271, loss = 0.007782661356031895
iteration 272, loss = 0.04502285644412041
iteration 273, loss = 0.10221857577562332
iteration 274, loss = 0.10574673861265182
iteration 275, loss = 0.03268703445792198
iteration 276, loss = 0.38687214255332947
iteration 277, loss = 0.3954375684261322
iteration 278, loss = 0.03914403170347214
iteration 279, loss = 0.01592842862010002
iteration 280, loss = 0.036593373864889145
iteration 281, loss = 0.41110697388648987
iteration 282, loss = 0.024088803678750992
iteration 283, loss = 0.01692848838865757
iteration 284, loss = 0.020790213719010353
iteration 285, loss = 0.01704435981810093
iteration 286, loss = 0.3883548974990845
iteration 287, loss = 0.39482390880584717
iteration 288, loss = 0.020377952605485916
iteration 289, loss = 0.02052769996225834
iteration 290, loss = 0.019827373325824738
iteration 291, loss = 0.011667894199490547
iteration 292, loss = 0.3959842920303345
iteration 293, loss = 0.3912031650543213
iteration 294, loss = 0.01207361277192831
iteration 295, loss = 0.019545676186680794
iteration 296, loss = 0.015510651282966137
iteration 297, loss = 0.029862351715564728
iteration 298, loss = 0.02170434035360813
iteration 299, loss = 0.38728100061416626
iteration 300, loss = 0.014193766750395298
iteration 1, loss = 0.020603472366929054
iteration 2, loss = 0.39264246821403503
iteration 3, loss = 0.01629604771733284
iteration 4, loss = 0.3915223777294159
iteration 5, loss = 0.02317756786942482
iteration 6, loss = 0.018031466752290726
iteration 7, loss = 0.03325902670621872
iteration 8, loss = 0.026320476084947586
iteration 9, loss = 0.39002525806427
iteration 10, loss = 0.0233603622764349
iteration 11, loss = 0.02012898400425911
iteration 12, loss = 0.030345961451530457
iteration 13, loss = 0.011504638008773327
iteration 14, loss = 0.017532648518681526
iteration 15, loss = 0.09892400354146957
iteration 16, loss = 0.011598154902458191
iteration 17, loss = 0.019103946164250374
iteration 18, loss = 0.016234926879405975
iteration 19, loss = 0.024423103779554367
iteration 20, loss = 0.022914420813322067
iteration 21, loss = 0.019343607127666473
iteration 22, loss = 0.039682842791080475
iteration 23, loss = 0.3952096998691559
iteration 24, loss = 0.04226642847061157
iteration 25, loss = 0.019134702160954475
iteration 26, loss = 0.01657549850642681
iteration 27, loss = 0.38436415791511536
iteration 28, loss = 0.40128037333488464
iteration 29, loss = 0.023163927718997
iteration 30, loss = 0.02505430206656456
iteration 31, loss = 0.02670925483107567
iteration 32, loss = 0.09670078009366989
iteration 33, loss = 0.026282116770744324
iteration 34, loss = 0.01337114255875349
iteration 35, loss = 0.03929898887872696
iteration 36, loss = 0.018962152302265167
iteration 37, loss = 0.7606601715087891
iteration 38, loss = 0.409627765417099
iteration 39, loss = 0.018797138705849648
iteration 40, loss = 0.40596193075180054
iteration 41, loss = 0.014344727620482445
iteration 42, loss = 0.02056719735264778
iteration 43, loss = 0.7526676058769226
iteration 44, loss = 0.3955121338367462
iteration 45, loss = 0.026951774954795837
iteration 46, loss = 0.01897755078971386
iteration 47, loss = 0.019045395776629448
iteration 48, loss = 0.033906105905771255
iteration 49, loss = 0.3828948438167572
iteration 50, loss = 0.01938621513545513
iteration 51, loss = 0.01334429532289505
iteration 52, loss = 0.4001806676387787
iteration 53, loss = 0.02370484732091427
iteration 54, loss = 0.012566888704895973
iteration 55, loss = 0.7843371629714966
iteration 56, loss = 0.39060279726982117
iteration 57, loss = 0.02985396794974804
iteration 58, loss = 0.01589837670326233
iteration 59, loss = 0.021603474393486977
iteration 60, loss = 0.014223247766494751
iteration 61, loss = 0.01353665441274643
iteration 62, loss = 0.013226959854364395
iteration 63, loss = 0.019457291811704636
iteration 64, loss = 0.01355226431041956
iteration 65, loss = 0.018799904733896255
iteration 66, loss = 0.012666135095059872
iteration 67, loss = 0.39053720235824585
iteration 68, loss = 0.38617387413978577
iteration 69, loss = 0.023412829264998436
iteration 70, loss = 0.016726411879062653
iteration 71, loss = 0.011926190927624702
iteration 72, loss = 0.014040963724255562
iteration 73, loss = 0.02078092470765114
iteration 74, loss = 0.10180984437465668
iteration 75, loss = 0.022112946957349777
iteration 76, loss = 0.022859830409288406
iteration 77, loss = 0.3861738443374634
iteration 78, loss = 0.39044952392578125
iteration 79, loss = 0.4732585549354553
iteration 80, loss = 0.09541754424571991
iteration 81, loss = 0.008971348404884338
iteration 82, loss = 0.0166059248149395
iteration 83, loss = 0.3904871642589569
iteration 84, loss = 0.029429208487272263
iteration 85, loss = 0.4636104702949524
iteration 86, loss = 0.012955108657479286
iteration 87, loss = 0.017944736406207085
iteration 88, loss = 0.008488066494464874
iteration 89, loss = 0.39660540223121643
iteration 90, loss = 0.013285619206726551
iteration 91, loss = 0.015729287639260292
iteration 92, loss = 0.017805179581046104
iteration 93, loss = 0.02945210412144661
iteration 94, loss = 0.019630301743745804
iteration 95, loss = 0.019119007512927055
iteration 96, loss = 0.02018018439412117
iteration 97, loss = 0.009375627152621746
iteration 98, loss = 0.3929877281188965
iteration 99, loss = 0.01806579902768135
iteration 100, loss = 0.013056965544819832
iteration 101, loss = 0.0082694748416543
iteration 102, loss = 0.38726627826690674
iteration 103, loss = 0.013543198816478252
iteration 104, loss = 0.017840106040239334
iteration 105, loss = 0.019833000376820564
iteration 106, loss = 0.017107043415308
iteration 107, loss = 0.3853055238723755
iteration 108, loss = 0.01884418912231922
iteration 109, loss = 0.01747027225792408
iteration 110, loss = 0.01316832471638918
iteration 111, loss = 0.02244545705616474
iteration 112, loss = 0.3856695890426636
iteration 113, loss = 0.03352970629930496
iteration 114, loss = 0.02122563309967518
iteration 115, loss = 0.01266741007566452
iteration 116, loss = 0.027770141139626503
iteration 117, loss = 0.019986337050795555
iteration 118, loss = 0.016531310975551605
iteration 119, loss = 0.03342293202877045
iteration 120, loss = 0.09450233727693558
iteration 121, loss = 0.014899813570082188
iteration 122, loss = 0.4044562876224518
iteration 123, loss = 0.03137689828872681
iteration 124, loss = 0.39357972145080566
iteration 125, loss = 0.017580341547727585
iteration 126, loss = 0.012787786312401295
iteration 127, loss = 0.3913591504096985
iteration 128, loss = 0.01983654871582985
iteration 129, loss = 0.023355253040790558
iteration 130, loss = 0.01315726526081562
iteration 131, loss = 0.019137514755129814
iteration 132, loss = 0.39836645126342773
iteration 133, loss = 0.39059585332870483
iteration 134, loss = 0.023299124091863632
iteration 135, loss = 0.3880457282066345
iteration 136, loss = 0.034562479704618454
iteration 137, loss = 0.02587195299565792
iteration 138, loss = 0.03445036709308624
iteration 139, loss = 0.016949698328971863
iteration 140, loss = 0.018341265618801117
iteration 141, loss = 0.009431424550712109
iteration 142, loss = 0.38556793332099915
iteration 143, loss = 0.03714387118816376
iteration 144, loss = 0.011418130248785019
iteration 145, loss = 0.3918774724006653
iteration 146, loss = 0.021599000319838524
iteration 147, loss = 0.01391832996159792
iteration 148, loss = 0.017686035484075546
iteration 149, loss = 0.018846984952688217
iteration 150, loss = 0.020755155012011528
iteration 151, loss = 0.39204952120780945
iteration 152, loss = 0.3823925852775574
iteration 153, loss = 0.015620083548128605
iteration 154, loss = 0.02538909949362278
iteration 155, loss = 0.016042130067944527
iteration 156, loss = 0.7551692724227905
iteration 157, loss = 0.016898302361369133
iteration 158, loss = 0.7671236991882324
iteration 159, loss = 0.39576753973960876
iteration 160, loss = 0.017120515927672386
iteration 161, loss = 0.01406894251704216
iteration 162, loss = 0.021956581622362137
iteration 163, loss = 0.4696063995361328
iteration 164, loss = 0.02301749959588051
iteration 165, loss = 0.022815894335508347
iteration 166, loss = 0.013295210897922516
iteration 167, loss = 0.39051347970962524
iteration 168, loss = 0.02013499289751053
iteration 169, loss = 0.4852451682090759
iteration 170, loss = 0.01467529684305191
iteration 171, loss = 0.01645541563630104
iteration 172, loss = 0.01450397539883852
iteration 173, loss = 0.0123364869505167
iteration 174, loss = 0.38979649543762207
iteration 175, loss = 0.08940411359071732
iteration 176, loss = 0.01732255145907402
iteration 177, loss = 0.09091854840517044
iteration 178, loss = 0.012662272900342941
iteration 179, loss = 0.02426801808178425
iteration 180, loss = 0.01814602129161358
iteration 181, loss = 0.38573339581489563
iteration 182, loss = 0.01900559663772583
iteration 183, loss = 0.02447809837758541
iteration 184, loss = 0.020972374826669693
iteration 185, loss = 0.013435231521725655
iteration 186, loss = 0.01872968301177025
iteration 187, loss = 0.04245695099234581
iteration 188, loss = 0.022075163200497627
iteration 189, loss = 0.013916214928030968
iteration 190, loss = 0.01619306020438671
iteration 191, loss = 0.012342043220996857
iteration 192, loss = 0.021995902061462402
iteration 193, loss = 0.015652582049369812
iteration 194, loss = 0.012479588389396667
iteration 195, loss = 0.027343424037098885
iteration 196, loss = 0.018125366419553757
iteration 197, loss = 0.018164288252592087
iteration 198, loss = 0.4586588144302368
iteration 199, loss = 0.012088307179510593
iteration 200, loss = 0.016191266477108
iteration 201, loss = 0.009439018554985523
iteration 202, loss = 0.021672021597623825
iteration 203, loss = 0.018934663385152817
iteration 204, loss = 0.40323787927627563
iteration 205, loss = 0.01811140589416027
iteration 206, loss = 0.02640206180512905
iteration 207, loss = 0.03034072369337082
iteration 208, loss = 0.016895273700356483
iteration 209, loss = 0.01657511107623577
iteration 210, loss = 0.02065141499042511
iteration 211, loss = 0.39078667759895325
iteration 212, loss = 0.02633589133620262
iteration 213, loss = 0.011764722876250744
iteration 214, loss = 0.020141810178756714
iteration 215, loss = 0.019427718594670296
iteration 216, loss = 0.3871217370033264
iteration 217, loss = 0.013217952102422714
iteration 218, loss = 0.025683794170618057
iteration 219, loss = 0.02027127891778946
iteration 220, loss = 0.010729270055890083
iteration 221, loss = 0.402098685503006
iteration 222, loss = 0.019681090489029884
iteration 223, loss = 0.39309677481651306
iteration 224, loss = 0.031537167727947235
iteration 225, loss = 0.01717548817396164
iteration 226, loss = 0.008552096784114838
iteration 227, loss = 0.020584329962730408
iteration 228, loss = 0.03409113734960556
iteration 229, loss = 0.38503170013427734
iteration 230, loss = 0.021817583590745926
iteration 231, loss = 0.3891568183898926
iteration 232, loss = 0.0183628648519516
iteration 233, loss = 0.37729623913764954
iteration 234, loss = 0.01776452362537384
iteration 235, loss = 0.38838139176368713
iteration 236, loss = 0.3909608721733093
iteration 237, loss = 0.40115657448768616
iteration 238, loss = 0.030849317088723183
iteration 239, loss = 0.027665965259075165
iteration 240, loss = 0.02367055043578148
iteration 241, loss = 0.01568370684981346
iteration 242, loss = 0.014668564312160015
iteration 243, loss = 0.01556647103279829
iteration 244, loss = 0.01997857168316841
iteration 245, loss = 0.026184089481830597
iteration 246, loss = 0.013410926796495914
iteration 247, loss = 0.39021939039230347
iteration 248, loss = 0.7750069499015808
iteration 249, loss = 0.08616585284471512
iteration 250, loss = 0.38797909021377563
iteration 251, loss = 0.014685518108308315
iteration 252, loss = 0.4102035462856293
iteration 253, loss = 0.0109738539904356
iteration 254, loss = 0.017622709274291992
iteration 255, loss = 0.02516663447022438
iteration 256, loss = 0.46960175037384033
iteration 257, loss = 0.026735888794064522
iteration 258, loss = 0.3971189260482788
iteration 259, loss = 0.02073352411389351
iteration 260, loss = 0.02546103298664093
iteration 261, loss = 0.021132634952664375
iteration 262, loss = 0.38102903962135315
iteration 263, loss = 0.39309993386268616
iteration 264, loss = 0.014557105489075184
iteration 265, loss = 0.011955824680626392
iteration 266, loss = 0.38867443799972534
iteration 267, loss = 0.020346861332654953
iteration 268, loss = 0.030174527317285538
iteration 269, loss = 0.017065269872546196
iteration 270, loss = 0.3843480348587036
iteration 271, loss = 0.016613051295280457
iteration 272, loss = 0.018750835210084915
iteration 273, loss = 0.025293057784438133
iteration 274, loss = 0.4081434905529022
iteration 275, loss = 0.38797807693481445
iteration 276, loss = 0.00894259661436081
iteration 277, loss = 0.021034471690654755
iteration 278, loss = 0.021738339215517044
iteration 279, loss = 0.4690609574317932
iteration 280, loss = 0.018484173342585564
iteration 281, loss = 0.021859364584088326
iteration 282, loss = 0.027104374021291733
iteration 283, loss = 0.014097635634243488
iteration 284, loss = 0.013433399610221386
iteration 285, loss = 0.014313054271042347
iteration 286, loss = 0.0256710983812809
iteration 287, loss = 0.015064550563693047
iteration 288, loss = 0.03312605991959572
iteration 289, loss = 0.012230681255459785
iteration 290, loss = 0.011031858623027802
iteration 291, loss = 0.3823511302471161
iteration 292, loss = 0.016586342826485634
iteration 293, loss = 0.025680357590317726
iteration 294, loss = 0.3920762538909912
iteration 295, loss = 0.38366562128067017
iteration 296, loss = 0.01196151040494442
iteration 297, loss = 0.027346061542630196
iteration 298, loss = 0.014901937916874886
iteration 299, loss = 0.3847713768482208
iteration 300, loss = 0.01812169887125492
iteration 1, loss = 0.021773815155029297
iteration 2, loss = 0.014271818101406097
iteration 3, loss = 0.4587121903896332
iteration 4, loss = 0.4610854983329773
iteration 5, loss = 0.025181572884321213
iteration 6, loss = 0.021670615300536156
iteration 7, loss = 0.39192864298820496
iteration 8, loss = 0.3868182301521301
iteration 9, loss = 0.3884597420692444
iteration 10, loss = 0.025964729487895966
iteration 11, loss = 0.3954237997531891
iteration 12, loss = 0.09012343734502792
iteration 13, loss = 0.3994786739349365
iteration 14, loss = 0.021347355097532272
iteration 15, loss = 0.017689071595668793
iteration 16, loss = 0.02903108298778534
iteration 17, loss = 0.023006461560726166
iteration 18, loss = 0.4054107069969177
iteration 19, loss = 0.016627788543701172
iteration 20, loss = 0.02613668702542782
iteration 21, loss = 0.38583970069885254
iteration 22, loss = 0.454955518245697
iteration 23, loss = 0.025918910279870033
iteration 24, loss = 0.019510924816131592
iteration 25, loss = 0.3853320777416229
iteration 26, loss = 0.031043916940689087
iteration 27, loss = 0.028485428541898727
iteration 28, loss = 0.01614881493151188
iteration 29, loss = 0.02014397457242012
iteration 30, loss = 0.024351254105567932
iteration 31, loss = 0.7565841674804688
iteration 32, loss = 0.014801252633333206
iteration 33, loss = 0.3967784643173218
iteration 34, loss = 0.016188539564609528
iteration 35, loss = 0.019896946847438812
iteration 36, loss = 0.014205535873770714
iteration 37, loss = 0.02708081342279911
iteration 38, loss = 0.38315412402153015
iteration 39, loss = 0.3835609555244446
iteration 40, loss = 0.015560087747871876
iteration 41, loss = 0.023202503100037575
iteration 42, loss = 0.011113516986370087
iteration 43, loss = 0.02385096251964569
iteration 44, loss = 0.02286433055996895
iteration 45, loss = 0.02108030579984188
iteration 46, loss = 0.0223561841994524
iteration 47, loss = 0.09260857850313187
iteration 48, loss = 0.022174043580889702
iteration 49, loss = 0.381923109292984
iteration 50, loss = 0.3948379158973694
iteration 51, loss = 0.39159396290779114
iteration 52, loss = 0.020152196288108826
iteration 53, loss = 0.3920063376426697
iteration 54, loss = 0.017439037561416626
iteration 55, loss = 0.3963969647884369
iteration 56, loss = 0.3850957453250885
iteration 57, loss = 0.014297951944172382
iteration 58, loss = 0.38066384196281433
iteration 59, loss = 0.021946066990494728
iteration 60, loss = 0.01504270825535059
iteration 61, loss = 0.023065870627760887
iteration 62, loss = 0.036001332104206085
iteration 63, loss = 0.010403000749647617
iteration 64, loss = 0.3882125914096832
iteration 65, loss = 0.022926701232790947
iteration 66, loss = 0.03060489147901535
iteration 67, loss = 0.01439749076962471
iteration 68, loss = 0.021129611879587173
iteration 69, loss = 0.7545721530914307
iteration 70, loss = 0.026209983974695206
iteration 71, loss = 0.011729913763701916
iteration 72, loss = 0.017238451167941093
iteration 73, loss = 0.09565866738557816
iteration 74, loss = 0.012856903485953808
iteration 75, loss = 0.015376313589513302
iteration 76, loss = 0.45533642172813416
iteration 77, loss = 0.016766784712672234
iteration 78, loss = 0.015166345983743668
iteration 79, loss = 0.014809872955083847
iteration 80, loss = 0.012926885858178139
iteration 81, loss = 0.03641640394926071
iteration 82, loss = 0.3939265310764313
iteration 83, loss = 0.017459388822317123
iteration 84, loss = 0.019171644002199173
iteration 85, loss = 0.02559250220656395
iteration 86, loss = 0.40603193640708923
iteration 87, loss = 0.020300377160310745
iteration 88, loss = 0.4020116627216339
iteration 89, loss = 0.03244930878281593
iteration 90, loss = 0.02416802942752838
iteration 91, loss = 0.3876117169857025
iteration 92, loss = 0.029624715447425842
iteration 93, loss = 0.017431730404496193
iteration 94, loss = 0.01612037420272827
iteration 95, loss = 0.02486445941030979
iteration 96, loss = 0.01811784692108631
iteration 97, loss = 0.027781303972005844
iteration 98, loss = 0.3860509693622589
iteration 99, loss = 0.45843568444252014
iteration 100, loss = 0.38736337423324585
iteration 101, loss = 0.018464447930455208
iteration 102, loss = 0.0161382295191288
iteration 103, loss = 0.3804383873939514
iteration 104, loss = 0.02127768099308014
iteration 105, loss = 0.3834700584411621
iteration 106, loss = 0.01422882080078125
iteration 107, loss = 0.09078521281480789
iteration 108, loss = 0.019822143018245697
iteration 109, loss = 0.012996478006243706
iteration 110, loss = 0.015454220585525036
iteration 111, loss = 0.03797629475593567
iteration 112, loss = 0.017378870397806168
iteration 113, loss = 0.015478464774787426
iteration 114, loss = 0.02216164395213127
iteration 115, loss = 0.013455301523208618
iteration 116, loss = 0.018258988857269287
iteration 117, loss = 0.3910204768180847
iteration 118, loss = 0.46295663714408875
iteration 119, loss = 0.017286201938986778
iteration 120, loss = 0.3883216381072998
iteration 121, loss = 0.00568283349275589
iteration 122, loss = 0.0166939664632082
iteration 123, loss = 0.020050514489412308
iteration 124, loss = 0.023227524012327194
iteration 125, loss = 0.3901578187942505
iteration 126, loss = 0.4560987949371338
iteration 127, loss = 0.08565603196620941
iteration 128, loss = 0.029490485787391663
iteration 129, loss = 0.7624386548995972
iteration 130, loss = 0.01993747055530548
iteration 131, loss = 0.015293962322175503
iteration 132, loss = 0.013841279782354832
iteration 133, loss = 0.7514520287513733
iteration 134, loss = 0.021286964416503906
iteration 135, loss = 0.01968594826757908
iteration 136, loss = 0.030554436147212982
iteration 137, loss = 0.016460726037621498
iteration 138, loss = 0.018917987123131752
iteration 139, loss = 0.012284371070563793
iteration 140, loss = 0.38472235202789307
iteration 141, loss = 0.03223298490047455
iteration 142, loss = 0.014533145353198051
iteration 143, loss = 0.3762367069721222
iteration 144, loss = 0.01731615699827671
iteration 145, loss = 0.026894493028521538
iteration 146, loss = 0.021293047815561295
iteration 147, loss = 0.3994402289390564
iteration 148, loss = 0.7578729391098022
iteration 149, loss = 0.031489305198192596
iteration 150, loss = 0.011412372812628746
iteration 151, loss = 0.015744414180517197
iteration 152, loss = 0.011476881802082062
iteration 153, loss = 0.01902434043586254
iteration 154, loss = 0.027908336371183395
iteration 155, loss = 0.016802145168185234
iteration 156, loss = 0.01297286618500948
iteration 157, loss = 0.019595036283135414
iteration 158, loss = 0.8232118487358093
iteration 159, loss = 0.033021844923496246
iteration 160, loss = 0.4065406322479248
iteration 161, loss = 0.01862408220767975
iteration 162, loss = 0.015363219194114208
iteration 163, loss = 0.38536784052848816
iteration 164, loss = 0.7560582160949707
iteration 165, loss = 0.018407009541988373
iteration 166, loss = 0.02647913247346878
iteration 167, loss = 0.020146064460277557
iteration 168, loss = 0.02781662903726101
iteration 169, loss = 0.016526902094483376
iteration 170, loss = 0.01625613495707512
iteration 171, loss = 0.023143049329519272
iteration 172, loss = 0.014710983261466026
iteration 173, loss = 0.01614258997142315
iteration 174, loss = 0.3983525335788727
iteration 175, loss = 0.01883521117269993
iteration 176, loss = 0.016169141978025436
iteration 177, loss = 0.031083429232239723
iteration 178, loss = 0.02218763157725334
iteration 179, loss = 0.023356009274721146
iteration 180, loss = 0.029028015211224556
iteration 181, loss = 0.00911166425794363
iteration 182, loss = 0.02344456873834133
iteration 183, loss = 0.38630709052085876
iteration 184, loss = 0.016985606402158737
iteration 185, loss = 0.016829900443553925
iteration 186, loss = 0.021303744986653328
iteration 187, loss = 0.38534635305404663
iteration 188, loss = 0.02918325364589691
iteration 189, loss = 0.026651861146092415
iteration 190, loss = 0.014682396315038204
iteration 191, loss = 0.012396911159157753
iteration 192, loss = 0.011330677196383476
iteration 193, loss = 0.020340068265795708
iteration 194, loss = 0.013972277753055096
iteration 195, loss = 0.7721737027168274
iteration 196, loss = 0.40082597732543945
iteration 197, loss = 0.046320732682943344
iteration 198, loss = 0.01693706586956978
iteration 199, loss = 0.015992870554327965
iteration 200, loss = 0.017221756279468536
iteration 201, loss = 0.018071401864290237
iteration 202, loss = 0.3861231207847595
iteration 203, loss = 0.019793257117271423
iteration 204, loss = 0.020732950419187546
iteration 205, loss = 0.030518697574734688
iteration 206, loss = 0.034789666533470154
iteration 207, loss = 0.01679767668247223
iteration 208, loss = 0.037851765751838684
iteration 209, loss = 0.3857637047767639
iteration 210, loss = 0.028950750827789307
iteration 211, loss = 0.02353212982416153
iteration 212, loss = 0.3857974708080292
iteration 213, loss = 0.019736874848604202
iteration 214, loss = 0.39003437757492065
iteration 215, loss = 0.009580369107425213
iteration 216, loss = 0.022017963230609894
iteration 217, loss = 0.03566782549023628
iteration 218, loss = 0.02297120913863182
iteration 219, loss = 0.0208301804959774
iteration 220, loss = 0.02226039208471775
iteration 221, loss = 0.019260568544268608
iteration 222, loss = 0.019612763077020645
iteration 223, loss = 0.024840673431754112
iteration 224, loss = 0.3803962469100952
iteration 225, loss = 0.03441121429204941
iteration 226, loss = 0.020564783364534378
iteration 227, loss = 0.023337464779615402
iteration 228, loss = 0.022841429337859154
iteration 229, loss = 0.01719042845070362
iteration 230, loss = 0.018394025042653084
iteration 231, loss = 0.01533934473991394
iteration 232, loss = 0.01721825823187828
iteration 233, loss = 0.01761862076818943
iteration 234, loss = 0.021879563108086586
iteration 235, loss = 0.019304130226373672
iteration 236, loss = 0.4075831472873688
iteration 237, loss = 0.02602623775601387
iteration 238, loss = 0.024936771020293236
iteration 239, loss = 0.01325719989836216
iteration 240, loss = 0.02037479355931282
iteration 241, loss = 0.022684218361973763
iteration 242, loss = 0.02048485353589058
iteration 243, loss = 0.006611459888517857
iteration 244, loss = 0.014609609730541706
iteration 245, loss = 0.03108080103993416
iteration 246, loss = 0.38203737139701843
iteration 247, loss = 0.019700434058904648
iteration 248, loss = 0.009688700549304485
iteration 249, loss = 0.39014971256256104
iteration 250, loss = 0.011527257040143013
iteration 251, loss = 0.01580834574997425
iteration 252, loss = 0.013017118908464909
iteration 253, loss = 0.02843281254172325
iteration 254, loss = 0.38438451290130615
iteration 255, loss = 0.012638038024306297
iteration 256, loss = 0.0325932614505291
iteration 257, loss = 0.027419287711381912
iteration 258, loss = 0.01236218586564064
iteration 259, loss = 0.3900204002857208
iteration 260, loss = 0.09018617868423462
iteration 261, loss = 0.39960816502571106
iteration 262, loss = 0.024544712156057358
iteration 263, loss = 0.011970669031143188
iteration 264, loss = 0.3853870630264282
iteration 265, loss = 0.38297754526138306
iteration 266, loss = 0.022173836827278137
iteration 267, loss = 0.015985218808054924
iteration 268, loss = 0.018708307296037674
iteration 269, loss = 0.019351273775100708
iteration 270, loss = 0.010540077462792397
iteration 271, loss = 0.012863606214523315
iteration 272, loss = 0.03515135869383812
iteration 273, loss = 0.02298162505030632
iteration 274, loss = 0.03210604563355446
iteration 275, loss = 0.02021489478647709
iteration 276, loss = 0.014593066647648811
iteration 277, loss = 0.40450388193130493
iteration 278, loss = 0.01787545531988144
iteration 279, loss = 0.3854086995124817
iteration 280, loss = 0.09310704469680786
iteration 281, loss = 0.3798750638961792
iteration 282, loss = 0.024114128202199936
iteration 283, loss = 0.02016826532781124
iteration 284, loss = 0.3832392394542694
iteration 285, loss = 0.018494632095098495
iteration 286, loss = 0.02267315983772278
iteration 287, loss = 0.0303824320435524
iteration 288, loss = 0.015550050884485245
iteration 289, loss = 0.018369698897004128
iteration 290, loss = 0.024125494062900543
iteration 291, loss = 0.012713017873466015
iteration 292, loss = 0.013438655063509941
iteration 293, loss = 0.013138131238520145
iteration 294, loss = 0.015354366041719913
iteration 295, loss = 0.021204840391874313
iteration 296, loss = 0.38594353199005127
iteration 297, loss = 0.015713080763816833
iteration 298, loss = 0.02036011777818203
iteration 299, loss = 0.017156146466732025
iteration 300, loss = 0.01844482496380806
iteration 1, loss = 0.022727228701114655
iteration 2, loss = 0.03934429585933685
iteration 3, loss = 0.018328756093978882
iteration 4, loss = 0.018200818449258804
iteration 5, loss = 0.017546359449625015
iteration 6, loss = 0.014380197040736675
iteration 7, loss = 0.028631435707211494
iteration 8, loss = 0.01463527511805296
iteration 9, loss = 0.3924490213394165
iteration 10, loss = 0.013939217664301395
iteration 11, loss = 0.38791000843048096
iteration 12, loss = 0.022673139348626137
iteration 13, loss = 0.01742415502667427
iteration 14, loss = 0.02051762118935585
iteration 15, loss = 0.024270731955766678
iteration 16, loss = 0.013745068572461605
iteration 17, loss = 0.017093554139137268
iteration 18, loss = 0.019302431493997574
iteration 19, loss = 0.027331646531820297
iteration 20, loss = 0.024831203743815422
iteration 21, loss = 0.3851146399974823
iteration 22, loss = 0.021125804632902145
iteration 23, loss = 0.022415131330490112
iteration 24, loss = 0.009724797680974007
iteration 25, loss = 0.021521806716918945
iteration 26, loss = 0.38397228717803955
iteration 27, loss = 0.019708987325429916
iteration 28, loss = 0.016087185591459274
iteration 29, loss = 0.03608663007616997
iteration 30, loss = 0.021532975137233734
iteration 31, loss = 0.016433248296380043
iteration 32, loss = 0.00865241140127182
iteration 33, loss = 0.014733358286321163
iteration 34, loss = 0.015286273322999477
iteration 35, loss = 0.014698944985866547
iteration 36, loss = 0.021794483065605164
iteration 37, loss = 0.024451958015561104
iteration 38, loss = 0.021390806883573532
iteration 39, loss = 0.10340317338705063
iteration 40, loss = 0.7590312957763672
iteration 41, loss = 0.3840300440788269
iteration 42, loss = 0.016535364091396332
iteration 43, loss = 0.3815728724002838
iteration 44, loss = 0.03623751550912857
iteration 45, loss = 0.3953673541545868
iteration 46, loss = 0.014136159792542458
iteration 47, loss = 0.39245378971099854
iteration 48, loss = 0.3866060972213745
iteration 49, loss = 0.3858248293399811
iteration 50, loss = 0.019131727516651154
iteration 51, loss = 0.015558463521301746
iteration 52, loss = 0.7512983083724976
iteration 53, loss = 0.39298295974731445
iteration 54, loss = 0.391619473695755
iteration 55, loss = 0.013960271142423153
iteration 56, loss = 0.017791448161005974
iteration 57, loss = 0.3834395408630371
iteration 58, loss = 0.04340909421443939
iteration 59, loss = 0.019368518143892288
iteration 60, loss = 0.020435398444533348
iteration 61, loss = 0.3938177824020386
iteration 62, loss = 0.018235748633742332
iteration 63, loss = 0.020630013197660446
iteration 64, loss = 0.015798460692167282
iteration 65, loss = 0.3811890482902527
iteration 66, loss = 0.021221010014414787
iteration 67, loss = 0.024382125586271286
iteration 68, loss = 0.47278058528900146
iteration 69, loss = 0.39654532074928284
iteration 70, loss = 0.4723542332649231
iteration 71, loss = 0.018233664333820343
iteration 72, loss = 0.021948736160993576
iteration 73, loss = 0.4100492298603058
iteration 74, loss = 0.03847867250442505
iteration 75, loss = 0.011852291412651539
iteration 76, loss = 0.3997946083545685
iteration 77, loss = 0.021296968683600426
iteration 78, loss = 0.015497526153922081
iteration 79, loss = 0.016996489837765694
iteration 80, loss = 0.017612461000680923
iteration 81, loss = 0.01977742277085781
iteration 82, loss = 0.3810483515262604
iteration 83, loss = 0.015232081525027752
iteration 84, loss = 0.38509875535964966
iteration 85, loss = 0.03025435283780098
iteration 86, loss = 0.021310994401574135
iteration 87, loss = 0.40219876170158386
iteration 88, loss = 0.017959220334887505
iteration 89, loss = 0.014430723153054714
iteration 90, loss = 0.01664469577372074
iteration 91, loss = 0.014979234896600246
iteration 92, loss = 0.012045267038047314
iteration 93, loss = 0.014467531815171242
iteration 94, loss = 0.01439740601927042
iteration 95, loss = 0.013602114282548428
iteration 96, loss = 0.011429296806454659
iteration 97, loss = 0.017680173739790916
iteration 98, loss = 0.0950394943356514
iteration 99, loss = 0.3982042372226715
iteration 100, loss = 0.021322250366210938
iteration 101, loss = 0.015952399000525475
iteration 102, loss = 0.032839108258485794
iteration 103, loss = 0.02370614930987358
iteration 104, loss = 0.38321712613105774
iteration 105, loss = 0.018006356433033943
iteration 106, loss = 0.014343930408358574
iteration 107, loss = 0.017257703468203545
iteration 108, loss = 0.019398560747504234
iteration 109, loss = 0.3902098536491394
iteration 110, loss = 0.02172461710870266
iteration 111, loss = 0.026782473549246788
iteration 112, loss = 0.013669699430465698
iteration 113, loss = 0.024507900699973106
iteration 114, loss = 0.025014083832502365
iteration 115, loss = 0.017805706709623337
iteration 116, loss = 0.014276338741183281
iteration 117, loss = 0.38554519414901733
iteration 118, loss = 0.021809648722410202
iteration 119, loss = 0.01854402758181095
iteration 120, loss = 0.011847627349197865
iteration 121, loss = 0.015978720039129257
iteration 122, loss = 0.020934779196977615
iteration 123, loss = 0.387109637260437
iteration 124, loss = 0.016334477812051773
iteration 125, loss = 0.3840646743774414
iteration 126, loss = 0.017390422523021698
iteration 127, loss = 0.012816684320569038
iteration 128, loss = 0.015024141408503056
iteration 129, loss = 0.09236593544483185
iteration 130, loss = 0.011822331696748734
iteration 131, loss = 0.01458943635225296
iteration 132, loss = 0.39896953105926514
iteration 133, loss = 0.014515403658151627
iteration 134, loss = 0.022957155480980873
iteration 135, loss = 0.012283152900636196
iteration 136, loss = 0.09582021832466125
iteration 137, loss = 0.01129399985074997
iteration 138, loss = 0.0202186182141304
iteration 139, loss = 0.01767507568001747
iteration 140, loss = 0.02022424340248108
iteration 141, loss = 0.3916209042072296
iteration 142, loss = 0.009800459258258343
iteration 143, loss = 0.3900148570537567
iteration 144, loss = 0.3841664493083954
iteration 145, loss = 0.019295966252684593
iteration 146, loss = 0.023255061358213425
iteration 147, loss = 0.012130395509302616
iteration 148, loss = 0.021388711407780647
iteration 149, loss = 0.021587247028946877
iteration 150, loss = 0.3895609676837921
iteration 151, loss = 0.014760919846594334
iteration 152, loss = 0.01389256026595831
iteration 153, loss = 0.017067432403564453
iteration 154, loss = 0.011007904075086117
iteration 155, loss = 0.03508179634809494
iteration 156, loss = 0.022091014310717583
iteration 157, loss = 0.008246504701673985
iteration 158, loss = 0.09564349055290222
iteration 159, loss = 0.016756173223257065
iteration 160, loss = 0.38613319396972656
iteration 161, loss = 0.01982751674950123
iteration 162, loss = 0.38284966349601746
iteration 163, loss = 1.1427865028381348
iteration 164, loss = 0.027379928156733513
iteration 165, loss = 0.3806857764720917
iteration 166, loss = 0.011958836577832699
iteration 167, loss = 0.016017906367778778
iteration 168, loss = 0.016119251027703285
iteration 169, loss = 0.02417694963514805
iteration 170, loss = 0.016329968348145485
iteration 171, loss = 0.015610272996127605
iteration 172, loss = 0.3907640874385834
iteration 173, loss = 0.385137677192688
iteration 174, loss = 0.016240814700722694
iteration 175, loss = 0.02178853563964367
iteration 176, loss = 0.02235296741127968
iteration 177, loss = 0.38083451986312866
iteration 178, loss = 0.014119825325906277
iteration 179, loss = 0.01465113740414381
iteration 180, loss = 0.027261193841695786
iteration 181, loss = 0.01421198807656765
iteration 182, loss = 0.042388468980789185
iteration 183, loss = 0.025365060195326805
iteration 184, loss = 0.014860773459076881
iteration 185, loss = 0.3906838893890381
iteration 186, loss = 0.012761489488184452
iteration 187, loss = 0.019231200218200684
iteration 188, loss = 0.024169843643903732
iteration 189, loss = 0.0261380597949028
iteration 190, loss = 0.46014833450317383
iteration 191, loss = 0.025876877829432487
iteration 192, loss = 0.013005999848246574
iteration 193, loss = 0.013958490453660488
iteration 194, loss = 0.014873223379254341
iteration 195, loss = 0.3951791524887085
iteration 196, loss = 0.01348788384348154
iteration 197, loss = 0.019654253497719765
iteration 198, loss = 0.04862799867987633
iteration 199, loss = 0.01128377951681614
iteration 200, loss = 0.03440990298986435
iteration 201, loss = 0.03561670333147049
iteration 202, loss = 0.017173845320940018
iteration 203, loss = 0.028854724019765854
iteration 204, loss = 0.3969120383262634
iteration 205, loss = 0.017976785078644753
iteration 206, loss = 0.020992349833250046
iteration 207, loss = 0.020618952810764313
iteration 208, loss = 0.016023803502321243
iteration 209, loss = 0.019331563264131546
iteration 210, loss = 0.021382436156272888
iteration 211, loss = 0.016645975410938263
iteration 212, loss = 0.028524231165647507
iteration 213, loss = 0.012539103627204895
iteration 214, loss = 0.022012334316968918
iteration 215, loss = 0.02442910149693489
iteration 216, loss = 0.37885650992393494
iteration 217, loss = 0.02040482871234417
iteration 218, loss = 0.390241414308548
iteration 219, loss = 0.40772438049316406
iteration 220, loss = 0.01592901721596718
iteration 221, loss = 0.017710020765662193
iteration 222, loss = 0.3898310959339142
iteration 223, loss = 0.020985862240195274
iteration 224, loss = 0.01611737534403801
iteration 225, loss = 0.09716441482305527
iteration 226, loss = 0.014830078929662704
iteration 227, loss = 0.09016462415456772
iteration 228, loss = 0.012122281827032566
iteration 229, loss = 0.03985884040594101
iteration 230, loss = 0.02828732691705227
iteration 231, loss = 0.016929449513554573
iteration 232, loss = 0.01136246882379055
iteration 233, loss = 0.012911606580018997
iteration 234, loss = 0.014983120374381542
iteration 235, loss = 0.7594062089920044
iteration 236, loss = 0.4597122371196747
iteration 237, loss = 0.01702442765235901
iteration 238, loss = 0.036003053188323975
iteration 239, loss = 0.022864140570163727
iteration 240, loss = 0.02188001200556755
iteration 241, loss = 0.017859045416116714
iteration 242, loss = 0.3994012176990509
iteration 243, loss = 0.02253103256225586
iteration 244, loss = 0.021411530673503876
iteration 245, loss = 0.025121137499809265
iteration 246, loss = 0.3869380056858063
iteration 247, loss = 0.020333442836999893
iteration 248, loss = 0.46258553862571716
iteration 249, loss = 0.028579285368323326
iteration 250, loss = 0.01882314868271351
iteration 251, loss = 0.7557403445243835
iteration 252, loss = 0.4583675265312195
iteration 253, loss = 0.026198294013738632
iteration 254, loss = 0.01717999204993248
iteration 255, loss = 0.019377032294869423
iteration 256, loss = 0.3907717168331146
iteration 257, loss = 0.0163026861846447
iteration 258, loss = 0.3866662383079529
iteration 259, loss = 0.0110041918233037
iteration 260, loss = 0.012119514867663383
iteration 261, loss = 0.02166050858795643
iteration 262, loss = 0.022808460518717766
iteration 263, loss = 0.021731561049818993
iteration 264, loss = 0.01586923934519291
iteration 265, loss = 0.4095371961593628
iteration 266, loss = 0.012688926421105862
iteration 267, loss = 0.02127481810748577
iteration 268, loss = 0.016538504511117935
iteration 269, loss = 0.3895551860332489
iteration 270, loss = 0.017064571380615234
iteration 271, loss = 0.02576100453734398
iteration 272, loss = 0.021658316254615784
iteration 273, loss = 0.3878500759601593
iteration 274, loss = 0.3844495415687561
iteration 275, loss = 0.014717207290232182
iteration 276, loss = 0.45856043696403503
iteration 277, loss = 0.3834582269191742
iteration 278, loss = 0.018955953419208527
iteration 279, loss = 0.03407029062509537
iteration 280, loss = 0.3981010913848877
iteration 281, loss = 0.3833135962486267
iteration 282, loss = 0.019372478127479553
iteration 283, loss = 0.014954619109630585
iteration 284, loss = 0.3871854245662689
iteration 285, loss = 0.3926747441291809
iteration 286, loss = 0.0269280094653368
iteration 287, loss = 0.01800229772925377
iteration 288, loss = 0.01410845946520567
iteration 289, loss = 0.013864805921912193
iteration 290, loss = 0.01818397082388401
iteration 291, loss = 0.021827034652233124
iteration 292, loss = 0.38608744740486145
iteration 293, loss = 0.3847615122795105
iteration 294, loss = 0.4618973731994629
iteration 295, loss = 0.019431564956903458
iteration 296, loss = 0.026822540909051895
iteration 297, loss = 0.015032375231385231
iteration 298, loss = 0.3818477988243103
iteration 299, loss = 0.01925257220864296
iteration 300, loss = 0.02371220290660858
iteration 1, loss = 0.02113104984164238
iteration 2, loss = 0.013152639381587505
iteration 3, loss = 0.3946148157119751
iteration 4, loss = 0.015078184194862843
iteration 5, loss = 0.017353229224681854
iteration 6, loss = 0.7505821585655212
iteration 7, loss = 0.029927942901849747
iteration 8, loss = 0.01334630697965622
iteration 9, loss = 0.38780757784843445
iteration 10, loss = 0.3858408033847809
iteration 11, loss = 0.017432497814297676
iteration 12, loss = 0.381964772939682
iteration 13, loss = 0.01996229961514473
iteration 14, loss = 0.018872462213039398
iteration 15, loss = 0.025729075074195862
iteration 16, loss = 0.3954484164714813
iteration 17, loss = 0.019245710223913193
iteration 18, loss = 0.017655739560723305
iteration 19, loss = 0.020709749311208725
iteration 20, loss = 0.01908877305686474
iteration 21, loss = 0.014179126359522343
iteration 22, loss = 0.01833636872470379
iteration 23, loss = 0.01608116924762726
iteration 24, loss = 0.0226605162024498
iteration 25, loss = 0.015917930752038956
iteration 26, loss = 0.3878302574157715
iteration 27, loss = 0.010355914011597633
iteration 28, loss = 0.018569760024547577
iteration 29, loss = 0.4619395136833191
iteration 30, loss = 0.3854643702507019
iteration 31, loss = 0.39166995882987976
iteration 32, loss = 0.01884092018008232
iteration 33, loss = 0.020105263218283653
iteration 34, loss = 0.3870249390602112
iteration 35, loss = 0.8249010443687439
iteration 36, loss = 0.39058995246887207
iteration 37, loss = 0.01068023219704628
iteration 38, loss = 0.01671886444091797
iteration 39, loss = 0.014943710528314114
iteration 40, loss = 0.015373957343399525
iteration 41, loss = 0.016855431720614433
iteration 42, loss = 0.01599295809864998
iteration 43, loss = 0.38942745327949524
iteration 44, loss = 0.01657615229487419
iteration 45, loss = 0.014612158760428429
iteration 46, loss = 0.38382551074028015
iteration 47, loss = 0.031031684949994087
iteration 48, loss = 0.01810685358941555
iteration 49, loss = 0.022283194586634636
iteration 50, loss = 0.014322672970592976
iteration 51, loss = 0.09794272482395172
iteration 52, loss = 0.383108913898468
iteration 53, loss = 0.38790738582611084
iteration 54, loss = 0.022279299795627594
iteration 55, loss = 0.03091326542198658
iteration 56, loss = 0.014589251019060612
iteration 57, loss = 0.018075071275234222
iteration 58, loss = 0.756945788860321
iteration 59, loss = 0.0164914820343256
iteration 60, loss = 0.015943096950650215
iteration 61, loss = 0.3822326064109802
iteration 62, loss = 0.01641887240111828
iteration 63, loss = 0.01901339739561081
iteration 64, loss = 0.38202276825904846
iteration 65, loss = 0.018027620390057564
iteration 66, loss = 0.02028219774365425
iteration 67, loss = 0.02284478396177292
iteration 68, loss = 0.38429439067840576
iteration 69, loss = 0.38188835978507996
iteration 70, loss = 0.010250936262309551
iteration 71, loss = 0.3977017402648926
iteration 72, loss = 0.3839000463485718
iteration 73, loss = 0.026743076741695404
iteration 74, loss = 0.013332261703908443
iteration 75, loss = 0.03061710298061371
iteration 76, loss = 0.027533838525414467
iteration 77, loss = 0.017852619290351868
iteration 78, loss = 0.017429398372769356
iteration 79, loss = 0.02614028938114643
iteration 80, loss = 0.39469289779663086
iteration 81, loss = 0.02435467019677162
iteration 82, loss = 0.3864348232746124
iteration 83, loss = 0.033408284187316895
iteration 84, loss = 0.02676810324192047
iteration 85, loss = 0.3838614225387573
iteration 86, loss = 0.017016813158988953
iteration 87, loss = 0.038301434367895126
iteration 88, loss = 0.019016653299331665
iteration 89, loss = 0.38871774077415466
iteration 90, loss = 0.016999460756778717
iteration 91, loss = 0.019973060116171837
iteration 92, loss = 0.02168930135667324
iteration 93, loss = 0.025403345003724098
iteration 94, loss = 0.01738014630973339
iteration 95, loss = 0.0185855645686388
iteration 96, loss = 0.01666364073753357
iteration 97, loss = 0.01150554046034813
iteration 98, loss = 0.38060280680656433
iteration 99, loss = 0.3814365863800049
iteration 100, loss = 0.022407693788409233
iteration 101, loss = 0.022880427539348602
iteration 102, loss = 0.028905821964144707
iteration 103, loss = 0.01707136258482933
iteration 104, loss = 0.015391937457025051
iteration 105, loss = 0.3869175910949707
iteration 106, loss = 0.018434986472129822
iteration 107, loss = 0.01737419329583645
iteration 108, loss = 0.0166720412671566
iteration 109, loss = 0.39622002840042114
iteration 110, loss = 0.4671816825866699
iteration 111, loss = 0.024594685062766075
iteration 112, loss = 0.40284454822540283
iteration 113, loss = 0.019907094538211823
iteration 114, loss = 0.019462157040834427
iteration 115, loss = 0.016682971268892288
iteration 116, loss = 0.38606691360473633
iteration 117, loss = 0.017036445438861847
iteration 118, loss = 0.020254535600543022
iteration 119, loss = 0.02864198386669159
iteration 120, loss = 0.02845565229654312
iteration 121, loss = 0.019484903663396835
iteration 122, loss = 0.023008178919553757
iteration 123, loss = 0.5442783236503601
iteration 124, loss = 0.38510045409202576
iteration 125, loss = 0.01988341100513935
iteration 126, loss = 0.01436813734471798
iteration 127, loss = 0.0194937065243721
iteration 128, loss = 0.3814190924167633
iteration 129, loss = 0.009416939690709114
iteration 130, loss = 0.017481530085206032
iteration 131, loss = 0.015328805893659592
iteration 132, loss = 0.04331875964999199
iteration 133, loss = 0.01685679331421852
iteration 134, loss = 0.024098888039588928
iteration 135, loss = 0.01185529213398695
iteration 136, loss = 0.09388972073793411
iteration 137, loss = 0.015532591380178928
iteration 138, loss = 0.02544044703245163
iteration 139, loss = 0.031394023448228836
iteration 140, loss = 0.015638025477528572
iteration 141, loss = 0.0167549941688776
iteration 142, loss = 0.39707091450691223
iteration 143, loss = 0.019125737249851227
iteration 144, loss = 0.014158288948237896
iteration 145, loss = 0.033270884305238724
iteration 146, loss = 0.015108817256987095
iteration 147, loss = 0.014330810867249966
iteration 148, loss = 0.021817537024617195
iteration 149, loss = 0.025751536712050438
iteration 150, loss = 0.022617045789957047
iteration 151, loss = 0.014362888410687447
iteration 152, loss = 0.022273873910307884
iteration 153, loss = 0.020914949476718903
iteration 154, loss = 0.3829095959663391
iteration 155, loss = 0.02471957355737686
iteration 156, loss = 0.38302081823349
iteration 157, loss = 0.020339682698249817
iteration 158, loss = 0.0137053607031703
iteration 159, loss = 0.015794550999999046
iteration 160, loss = 0.3888539671897888
iteration 161, loss = 0.01031375303864479
iteration 162, loss = 0.018893694505095482
iteration 163, loss = 0.024690959602594376
iteration 164, loss = 0.015368297696113586
iteration 165, loss = 0.013645319268107414
iteration 166, loss = 0.014822345227003098
iteration 167, loss = 0.01438914705067873
iteration 168, loss = 0.009639953263103962
iteration 169, loss = 0.16676495969295502
iteration 170, loss = 0.02228846400976181
iteration 171, loss = 0.022430483251810074
iteration 172, loss = 0.017232745885849
iteration 173, loss = 0.03871284797787666
iteration 174, loss = 0.030713167041540146
iteration 175, loss = 0.09036663174629211
iteration 176, loss = 0.020863477140665054
iteration 177, loss = 0.018416576087474823
iteration 178, loss = 0.38517019152641296
iteration 179, loss = 0.4134346544742584
iteration 180, loss = 0.017995480448007584
iteration 181, loss = 0.01576167345046997
iteration 182, loss = 0.010129774920642376
iteration 183, loss = 0.7508451342582703
iteration 184, loss = 0.7681916356086731
iteration 185, loss = 0.018526960164308548
iteration 186, loss = 0.3812042474746704
iteration 187, loss = 0.022979790344834328
iteration 188, loss = 0.01299662608653307
iteration 189, loss = 0.016145125031471252
iteration 190, loss = 0.014055779203772545
iteration 191, loss = 0.015158252790570259
iteration 192, loss = 0.02500884234905243
iteration 193, loss = 0.39848339557647705
iteration 194, loss = 0.015786543488502502
iteration 195, loss = 0.02063143253326416
iteration 196, loss = 0.02050968073308468
iteration 197, loss = 0.01772601529955864
iteration 198, loss = 0.08947653323411942
iteration 199, loss = 0.3873380124568939
iteration 200, loss = 0.013974569737911224
iteration 201, loss = 0.38709092140197754
iteration 202, loss = 0.38966524600982666
iteration 203, loss = 0.38249850273132324
iteration 204, loss = 0.022759145125746727
iteration 205, loss = 0.017671871930360794
iteration 206, loss = 0.025430751964449883
iteration 207, loss = 0.3947905898094177
iteration 208, loss = 0.014553182758390903
iteration 209, loss = 0.01882784813642502
iteration 210, loss = 0.3980770707130432
iteration 211, loss = 0.033998727798461914
iteration 212, loss = 0.027139533311128616
iteration 213, loss = 0.01825563795864582
iteration 214, loss = 0.018238548189401627
iteration 215, loss = 0.031777478754520416
iteration 216, loss = 0.03620162978768349
iteration 217, loss = 0.014852805063128471
iteration 218, loss = 0.013846593908965588
iteration 219, loss = 0.012610667385160923
iteration 220, loss = 0.6019784808158875
iteration 221, loss = 0.021314114332199097
iteration 222, loss = 0.02164553664624691
iteration 223, loss = 0.026187391951680183
iteration 224, loss = 0.02357817068696022
iteration 225, loss = 0.39082610607147217
iteration 226, loss = 0.0256463885307312
iteration 227, loss = 0.011562779545783997
iteration 228, loss = 0.0306246355175972
iteration 229, loss = 0.7513210773468018
iteration 230, loss = 0.028055787086486816
iteration 231, loss = 0.3855031430721283
iteration 232, loss = 0.02424398995935917
iteration 233, loss = 0.03370705991983414
iteration 234, loss = 0.02064593881368637
iteration 235, loss = 0.0185149684548378
iteration 236, loss = 0.38103920221328735
iteration 237, loss = 0.014839399605989456
iteration 238, loss = 0.02636118233203888
iteration 239, loss = 0.022974982857704163
iteration 240, loss = 0.027521837502717972
iteration 241, loss = 0.018927641212940216
iteration 242, loss = 0.014369465410709381
iteration 243, loss = 0.015563677065074444
iteration 244, loss = 0.011950819753110409
iteration 245, loss = 0.45743486285209656
iteration 246, loss = 0.011065625585615635
iteration 247, loss = 0.020498404279351234
iteration 248, loss = 0.023353174328804016
iteration 249, loss = 0.38930192589759827
iteration 250, loss = 0.024716176092624664
iteration 251, loss = 0.3901270031929016
iteration 252, loss = 0.018751418218016624
iteration 253, loss = 0.38405466079711914
iteration 254, loss = 0.3920861780643463
iteration 255, loss = 0.012454692274332047
iteration 256, loss = 0.02049480378627777
iteration 257, loss = 0.03629467263817787
iteration 258, loss = 0.017078204080462456
iteration 259, loss = 0.027388475835323334
iteration 260, loss = 0.019735511392354965
iteration 261, loss = 0.01315387524664402
iteration 262, loss = 0.018398253247141838
iteration 263, loss = 0.017742309719324112
iteration 264, loss = 0.3890239894390106
iteration 265, loss = 0.012882853858172894
iteration 266, loss = 0.02090378850698471
iteration 267, loss = 0.010706447064876556
iteration 268, loss = 0.015606691129505634
iteration 269, loss = 0.025574207305908203
iteration 270, loss = 0.016549132764339447
iteration 271, loss = 0.044747449457645416
iteration 272, loss = 0.03649846091866493
iteration 273, loss = 0.39054062962532043
iteration 274, loss = 0.013865657150745392
iteration 275, loss = 0.4033445715904236
iteration 276, loss = 0.016126787289977074
iteration 277, loss = 0.017489120364189148
iteration 278, loss = 0.38268589973449707
iteration 279, loss = 0.015174557454884052
iteration 280, loss = 0.01620282605290413
iteration 281, loss = 0.024519437924027443
iteration 282, loss = 0.017514916136860847
iteration 283, loss = 0.02084975317120552
iteration 284, loss = 0.01078708190470934
iteration 285, loss = 0.01513137947767973
iteration 286, loss = 0.0180593840777874
iteration 287, loss = 0.014250273816287518
iteration 288, loss = 0.3800482153892517
iteration 289, loss = 0.3820311427116394
iteration 290, loss = 0.38469627499580383
iteration 291, loss = 0.020229581743478775
iteration 292, loss = 0.017298758029937744
iteration 293, loss = 0.015275433659553528
iteration 294, loss = 0.022633880376815796
iteration 295, loss = 0.017425935715436935
iteration 296, loss = 0.3843585252761841
iteration 297, loss = 0.022820957005023956
iteration 298, loss = 0.3800552189350128
iteration 299, loss = 0.38742661476135254
iteration 300, loss = 0.019217686727643013
iteration 1, loss = 0.016524743288755417
iteration 2, loss = 0.016179509460926056
iteration 3, loss = 0.0158018060028553
iteration 4, loss = 0.020916946232318878
iteration 5, loss = 0.3847724497318268
iteration 6, loss = 0.38599157333374023
iteration 7, loss = 0.016487039625644684
iteration 8, loss = 0.020239468663930893
iteration 9, loss = 0.01580064371228218
iteration 10, loss = 0.029384005814790726
iteration 11, loss = 0.015449597500264645
iteration 12, loss = 0.38745352625846863
iteration 13, loss = 0.020848307758569717
iteration 14, loss = 0.026009270921349525
iteration 15, loss = 0.023562001064419746
iteration 16, loss = 0.01544759701937437
iteration 17, loss = 0.38875260949134827
iteration 18, loss = 0.023578578606247902
iteration 19, loss = 0.09437798708677292
iteration 20, loss = 0.3882196843624115
iteration 21, loss = 0.3847670555114746
iteration 22, loss = 0.02349857985973358
iteration 23, loss = 0.01817180961370468
iteration 24, loss = 0.036931999027729034
iteration 25, loss = 0.010863284580409527
iteration 26, loss = 0.8220272064208984
iteration 27, loss = 0.01678325980901718
iteration 28, loss = 0.0220737773925066
iteration 29, loss = 0.020774871110916138
iteration 30, loss = 0.00913966167718172
iteration 31, loss = 0.01946619153022766
iteration 32, loss = 0.0166657455265522
iteration 33, loss = 0.39094167947769165
iteration 34, loss = 0.4646329879760742
iteration 35, loss = 0.021821903064846992
iteration 36, loss = 0.022791624069213867
iteration 37, loss = 0.01618172600865364
iteration 38, loss = 0.01970362290740013
iteration 39, loss = 0.02482372149825096
iteration 40, loss = 0.38829538226127625
iteration 41, loss = 0.024044496938586235
iteration 42, loss = 0.014788666740059853
iteration 43, loss = 0.017839623615145683
iteration 44, loss = 0.015075398609042168
iteration 45, loss = 0.02134670875966549
iteration 46, loss = 0.023100964725017548
iteration 47, loss = 0.01071401871740818
iteration 48, loss = 0.017353126779198647
iteration 49, loss = 0.8248612880706787
iteration 50, loss = 0.03258238360285759
iteration 51, loss = 0.015122950077056885
iteration 52, loss = 0.37906309962272644
iteration 53, loss = 0.015492326579988003
iteration 54, loss = 0.02116875909268856
iteration 55, loss = 0.012305183336138725
iteration 56, loss = 0.09313399344682693
iteration 57, loss = 0.00944533385336399
iteration 58, loss = 0.011008157394826412
iteration 59, loss = 0.0361713245511055
iteration 60, loss = 0.7531602382659912
iteration 61, loss = 0.3881034255027771
iteration 62, loss = 0.39236387610435486
iteration 63, loss = 0.3979000747203827
iteration 64, loss = 0.045955512672662735
iteration 65, loss = 0.023290634155273438
iteration 66, loss = 0.019706519320607185
iteration 67, loss = 0.023371540009975433
iteration 68, loss = 0.0158393457531929
iteration 69, loss = 0.01819663681089878
iteration 70, loss = 0.015613500960171223
iteration 71, loss = 0.017612099647521973
iteration 72, loss = 0.02065310999751091
iteration 73, loss = 0.02303840033710003
iteration 74, loss = 0.021091528236865997
iteration 75, loss = 0.02502545341849327
iteration 76, loss = 0.01601405069231987
iteration 77, loss = 0.38095539808273315
iteration 78, loss = 0.019003523513674736
iteration 79, loss = 0.024708732962608337
iteration 80, loss = 0.019198814406991005
iteration 81, loss = 0.01600196771323681
iteration 82, loss = 1.1302237510681152
iteration 83, loss = 0.026409167796373367
iteration 84, loss = 0.02423863299190998
iteration 85, loss = 0.37931329011917114
iteration 86, loss = 0.015895796939730644
iteration 87, loss = 0.01568775624036789
iteration 88, loss = 0.016964541748166084
iteration 89, loss = 0.03693503513932228
iteration 90, loss = 0.021380050107836723
iteration 91, loss = 0.016891714185476303
iteration 92, loss = 0.015609774738550186
iteration 93, loss = 0.38088008761405945
iteration 94, loss = 0.035447973757982254
iteration 95, loss = 0.0189206525683403
iteration 96, loss = 0.02314887009561062
iteration 97, loss = 0.021032273769378662
iteration 98, loss = 0.38891762495040894
iteration 99, loss = 0.8467535972595215
iteration 100, loss = 0.03281792998313904
iteration 101, loss = 0.4573953151702881
iteration 102, loss = 0.014247274026274681
iteration 103, loss = 0.016692906618118286
iteration 104, loss = 0.02601867914199829
iteration 105, loss = 0.025700533762574196
iteration 106, loss = 0.3837530016899109
iteration 107, loss = 0.39502087235450745
iteration 108, loss = 0.017284702509641647
iteration 109, loss = 0.02533722296357155
iteration 110, loss = 0.009660936892032623
iteration 111, loss = 0.3807634115219116
iteration 112, loss = 0.38083615899086
iteration 113, loss = 0.014015461318194866
iteration 114, loss = 0.08438999950885773
iteration 115, loss = 0.021259954199194908
iteration 116, loss = 0.021284665912389755
iteration 117, loss = 0.01186522375792265
iteration 118, loss = 0.019804177805781364
iteration 119, loss = 0.022084081545472145
iteration 120, loss = 0.016183482483029366
iteration 121, loss = 0.3873538076877594
iteration 122, loss = 0.3971098065376282
iteration 123, loss = 0.02213379368185997
iteration 124, loss = 0.024991225451231003
iteration 125, loss = 0.011663207784295082
iteration 126, loss = 0.017085252329707146
iteration 127, loss = 0.014440545812249184
iteration 128, loss = 0.01515945140272379
iteration 129, loss = 0.027032503858208656
iteration 130, loss = 0.019024668261408806
iteration 131, loss = 0.37899479269981384
iteration 132, loss = 0.38883426785469055
iteration 133, loss = 0.01574297621846199
iteration 134, loss = 0.01573607698082924
iteration 135, loss = 0.029746731743216515
iteration 136, loss = 0.3822557330131531
iteration 137, loss = 0.3808591067790985
iteration 138, loss = 0.014025790616869926
iteration 139, loss = 0.03236716613173485
iteration 140, loss = 0.40219223499298096
iteration 141, loss = 0.017653249204158783
iteration 142, loss = 0.014232099056243896
iteration 143, loss = 0.38647329807281494
iteration 144, loss = 0.025825127959251404
iteration 145, loss = 0.027127418667078018
iteration 146, loss = 0.018858470022678375
iteration 147, loss = 0.7495563626289368
iteration 148, loss = 0.01583549566566944
iteration 149, loss = 0.08611790835857391
iteration 150, loss = 0.023598261177539825
iteration 151, loss = 0.38166841864585876
iteration 152, loss = 0.014253190718591213
iteration 153, loss = 0.39159876108169556
iteration 154, loss = 0.38713952898979187
iteration 155, loss = 0.013222649693489075
iteration 156, loss = 0.021293621510267258
iteration 157, loss = 0.016113121062517166
iteration 158, loss = 0.017254851758480072
iteration 159, loss = 0.01557282917201519
iteration 160, loss = 0.02712465450167656
iteration 161, loss = 0.022562535479664803
iteration 162, loss = 0.013928704895079136
iteration 163, loss = 0.01821036823093891
iteration 164, loss = 0.02211013436317444
iteration 165, loss = 0.015906840562820435
iteration 166, loss = 0.3942919075489044
iteration 167, loss = 0.01252121850848198
iteration 168, loss = 0.3976360857486725
iteration 169, loss = 0.013327219523489475
iteration 170, loss = 0.024044206365942955
iteration 171, loss = 0.01806887984275818
iteration 172, loss = 0.013689272105693817
iteration 173, loss = 0.02129673771560192
iteration 174, loss = 0.017046021297574043
iteration 175, loss = 0.014159567654132843
iteration 176, loss = 0.3870871365070343
iteration 177, loss = 0.023566586896777153
iteration 178, loss = 0.022182706743478775
iteration 179, loss = 0.38655465841293335
iteration 180, loss = 0.013822482898831367
iteration 181, loss = 0.014995630830526352
iteration 182, loss = 0.3978802561759949
iteration 183, loss = 0.011750342324376106
iteration 184, loss = 0.019520113244652748
iteration 185, loss = 0.014155810698866844
iteration 186, loss = 0.01724664866924286
iteration 187, loss = 0.03746616467833519
iteration 188, loss = 0.38102594017982483
iteration 189, loss = 0.38802093267440796
iteration 190, loss = 0.3858352601528168
iteration 191, loss = 0.018035532906651497
iteration 192, loss = 0.025733938440680504
iteration 193, loss = 0.021897118538618088
iteration 194, loss = 0.02363077737390995
iteration 195, loss = 0.010836644098162651
iteration 196, loss = 0.018274612724781036
iteration 197, loss = 0.014414242468774319
iteration 198, loss = 0.02891283668577671
iteration 199, loss = 0.017956459894776344
iteration 200, loss = 0.39227092266082764
iteration 201, loss = 0.018998468294739723
iteration 202, loss = 0.013887208886444569
iteration 203, loss = 0.3835716247558594
iteration 204, loss = 0.02029459737241268
iteration 205, loss = 0.40118375420570374
iteration 206, loss = 0.014980755746364594
iteration 207, loss = 0.7626703977584839
iteration 208, loss = 0.026081692427396774
iteration 209, loss = 0.01494281180202961
iteration 210, loss = 0.017357200384140015
iteration 211, loss = 0.014829661697149277
iteration 212, loss = 0.017246859148144722
iteration 213, loss = 0.01172645203769207
iteration 214, loss = 0.016581913456320763
iteration 215, loss = 0.4052707850933075
iteration 216, loss = 0.01582404598593712
iteration 217, loss = 0.3804383873939514
iteration 218, loss = 0.02514122799038887
iteration 219, loss = 0.01569092832505703
iteration 220, loss = 0.02013583481311798
iteration 221, loss = 0.016138041391968727
iteration 222, loss = 0.019782627001404762
iteration 223, loss = 0.3832024335861206
iteration 224, loss = 0.38660573959350586
iteration 225, loss = 0.4045896828174591
iteration 226, loss = 0.37844008207321167
iteration 227, loss = 0.017556125298142433
iteration 228, loss = 0.01494612731039524
iteration 229, loss = 0.014940241351723671
iteration 230, loss = 0.03831782191991806
iteration 231, loss = 0.012819762341678143
iteration 232, loss = 0.028948621824383736
iteration 233, loss = 0.029274456202983856
iteration 234, loss = 0.014523766003549099
iteration 235, loss = 0.02025662735104561
iteration 236, loss = 0.016680188477039337
iteration 237, loss = 0.02488727867603302
iteration 238, loss = 0.3826322853565216
iteration 239, loss = 0.3862606883049011
iteration 240, loss = 0.38230976462364197
iteration 241, loss = 0.016156520694494247
iteration 242, loss = 0.014135266654193401
iteration 243, loss = 0.017385954037308693
iteration 244, loss = 0.01499415934085846
iteration 245, loss = 0.017383962869644165
iteration 246, loss = 0.4602116644382477
iteration 247, loss = 0.02169930748641491
iteration 248, loss = 0.021505018696188927
iteration 249, loss = 0.10368122160434723
iteration 250, loss = 0.08554112166166306
iteration 251, loss = 0.030021250247955322
iteration 252, loss = 0.015072541311383247
iteration 253, loss = 0.01841372810304165
iteration 254, loss = 0.45831066370010376
iteration 255, loss = 0.024227065965533257
iteration 256, loss = 0.3927749991416931
iteration 257, loss = 0.01415752898901701
iteration 258, loss = 0.014109086245298386
iteration 259, loss = 0.38371267914772034
iteration 260, loss = 0.017663052305579185
iteration 261, loss = 0.015161493793129921
iteration 262, loss = 0.02402760460972786
iteration 263, loss = 0.39532649517059326
iteration 264, loss = 0.3886663019657135
iteration 265, loss = 0.0282795038074255
iteration 266, loss = 0.020343415439128876
iteration 267, loss = 0.01467039529234171
iteration 268, loss = 0.01174142025411129
iteration 269, loss = 0.013363447971642017
iteration 270, loss = 0.03382715582847595
iteration 271, loss = 0.014641393907368183
iteration 272, loss = 0.018993457779288292
iteration 273, loss = 0.016814030706882477
iteration 274, loss = 0.01050582341849804
iteration 275, loss = 0.02184688113629818
iteration 276, loss = 0.023773401975631714
iteration 277, loss = 0.0222003236413002
iteration 278, loss = 0.38314440846443176
iteration 279, loss = 0.015646640211343765
iteration 280, loss = 0.013502500019967556
iteration 281, loss = 0.03373974189162254
iteration 282, loss = 0.026863083243370056
iteration 283, loss = 0.015653375536203384
iteration 284, loss = 0.453989714384079
iteration 285, loss = 0.40702658891677856
iteration 286, loss = 0.010820271447300911
iteration 287, loss = 0.3943030834197998
iteration 288, loss = 0.38154491782188416
iteration 289, loss = 0.009350004605948925
iteration 290, loss = 0.02146066538989544
iteration 291, loss = 0.018265020102262497
iteration 292, loss = 0.014195621013641357
iteration 293, loss = 0.019566280767321587
iteration 294, loss = 0.01933000236749649
iteration 295, loss = 0.015399913303554058
iteration 296, loss = 0.027373626828193665
iteration 297, loss = 0.01873156987130642
iteration 298, loss = 0.012760376557707787
iteration 299, loss = 0.45296281576156616
iteration 300, loss = 0.015300560742616653
iteration 1, loss = 0.014115081168711185
iteration 2, loss = 0.3807569742202759
iteration 3, loss = 0.04013753682374954
iteration 4, loss = 0.02592410147190094
iteration 5, loss = 0.017872322350740433
iteration 6, loss = 0.08974920958280563
iteration 7, loss = 0.018824871629476547
iteration 8, loss = 0.028431545943021774
iteration 9, loss = 0.026008982211351395
iteration 10, loss = 0.012899728491902351
iteration 11, loss = 0.3971697688102722
iteration 12, loss = 0.390945702791214
iteration 13, loss = 0.016738172620534897
iteration 14, loss = 0.38336560130119324
iteration 15, loss = 0.02449657768011093
iteration 16, loss = 0.012415360659360886
iteration 17, loss = 0.014040212146937847
iteration 18, loss = 0.38247597217559814
iteration 19, loss = 0.014233858324587345
iteration 20, loss = 0.013830924406647682
iteration 21, loss = 0.017477966845035553
iteration 22, loss = 0.07831549644470215
iteration 23, loss = 0.02484128437936306
iteration 24, loss = 0.011319136247038841
iteration 25, loss = 0.02628997340798378
iteration 26, loss = 0.015312987379729748
iteration 27, loss = 0.3762006461620331
iteration 28, loss = 0.01588710956275463
iteration 29, loss = 0.018721314147114754
iteration 30, loss = 0.029619773849844933
iteration 31, loss = 0.027094174176454544
iteration 32, loss = 0.02579616568982601
iteration 33, loss = 0.38092130422592163
iteration 34, loss = 0.8196293711662292
iteration 35, loss = 0.39073285460472107
iteration 36, loss = 0.026726361364126205
iteration 37, loss = 0.009379695169627666
iteration 38, loss = 0.04340951144695282
iteration 39, loss = 0.017295025289058685
iteration 40, loss = 0.3819207549095154
iteration 41, loss = 0.009132039733231068
iteration 42, loss = 0.01495965477079153
iteration 43, loss = 0.08193805813789368
iteration 44, loss = 0.3821142017841339
iteration 45, loss = 0.45212623476982117
iteration 46, loss = 0.7588999271392822
iteration 47, loss = 0.017121223732829094
iteration 48, loss = 0.022217756137251854
iteration 49, loss = 0.015386657789349556
iteration 50, loss = 0.0374298170208931
iteration 51, loss = 0.0487201027572155
iteration 52, loss = 0.03233145922422409
iteration 53, loss = 0.024199357256293297
iteration 54, loss = 0.030738716945052147
iteration 55, loss = 0.02042223885655403
iteration 56, loss = 0.023722073063254356
iteration 57, loss = 0.39410892128944397
iteration 58, loss = 0.02574489638209343
iteration 59, loss = 0.02073073200881481
iteration 60, loss = 0.030477862805128098
iteration 61, loss = 0.037370264530181885
iteration 62, loss = 0.02612258493900299
iteration 63, loss = 0.014031731523573399
iteration 64, loss = 0.02204834297299385
iteration 65, loss = 0.41598260402679443
iteration 66, loss = 0.02434093877673149
iteration 67, loss = 0.3788797855377197
iteration 68, loss = 0.015794305130839348
iteration 69, loss = 0.01989986188709736
iteration 70, loss = 0.01958789862692356
iteration 71, loss = 0.37902867794036865
iteration 72, loss = 0.08145815879106522
iteration 73, loss = 0.018586937338113785
iteration 74, loss = 0.014283495955169201
iteration 75, loss = 0.39476221799850464
iteration 76, loss = 0.3811804950237274
iteration 77, loss = 0.3865653872489929
iteration 78, loss = 0.02398522198200226
iteration 79, loss = 0.01516982913017273
iteration 80, loss = 0.02611481212079525
iteration 81, loss = 0.024470994248986244
iteration 82, loss = 0.4500909447669983
iteration 83, loss = 0.023785477504134178
iteration 84, loss = 0.3979693055152893
iteration 85, loss = 0.020063530653715134
iteration 86, loss = 0.013538094237446785
iteration 87, loss = 0.02941659279167652
iteration 88, loss = 0.01448647677898407
iteration 89, loss = 0.3889976441860199
iteration 90, loss = 0.03527856990695
iteration 91, loss = 0.37862908840179443
iteration 92, loss = 0.3833487927913666
iteration 93, loss = 0.023840704932808876
iteration 94, loss = 0.015860024839639664
iteration 95, loss = 0.015450844541192055
iteration 96, loss = 0.3884788453578949
iteration 97, loss = 0.02643878012895584
iteration 98, loss = 0.3886297047138214
iteration 99, loss = 0.007033852860331535
iteration 100, loss = 0.020934270694851875
iteration 101, loss = 0.014280824922025204
iteration 102, loss = 0.3861885368824005
iteration 103, loss = 0.014830803498625755
iteration 104, loss = 0.011064750142395496
iteration 105, loss = 0.44830912351608276
iteration 106, loss = 0.08647625148296356
iteration 107, loss = 0.02115374431014061
iteration 108, loss = 0.023529937490820885
iteration 109, loss = 0.0135029973462224
iteration 110, loss = 0.008430950343608856
iteration 111, loss = 0.01793920435011387
iteration 112, loss = 0.021433133631944656
iteration 113, loss = 0.026637356728315353
iteration 114, loss = 0.018939554691314697
iteration 115, loss = 0.016323579475283623
iteration 116, loss = 0.02377299591898918
iteration 117, loss = 0.3830927014350891
iteration 118, loss = 0.025976648554205894
iteration 119, loss = 0.3817986249923706
iteration 120, loss = 0.01667562872171402
iteration 121, loss = 0.3869318962097168
iteration 122, loss = 0.03505147993564606
iteration 123, loss = 0.03373599797487259
iteration 124, loss = 0.022123729810118675
iteration 125, loss = 0.018365731462836266
iteration 126, loss = 0.024419080466032028
iteration 127, loss = 0.01623440533876419
iteration 128, loss = 0.019262926653027534
iteration 129, loss = 0.020935965701937675
iteration 130, loss = 0.020790452137589455
iteration 131, loss = 0.02653726190328598
iteration 132, loss = 0.01421718206256628
iteration 133, loss = 0.029612863436341286
iteration 134, loss = 0.015840550884604454
iteration 135, loss = 0.018002493306994438
iteration 136, loss = 0.018629245460033417
iteration 137, loss = 0.02292800135910511
iteration 138, loss = 0.02390378713607788
iteration 139, loss = 0.013800852000713348
iteration 140, loss = 0.0190067570656538
iteration 141, loss = 0.03567788004875183
iteration 142, loss = 0.01909811794757843
iteration 143, loss = 0.014651680365204811
iteration 144, loss = 0.0164371095597744
iteration 145, loss = 0.022048575803637505
iteration 146, loss = 0.02019946463406086
iteration 147, loss = 0.3821321725845337
iteration 148, loss = 0.02096749097108841
iteration 149, loss = 0.7672126889228821
iteration 150, loss = 0.01680745556950569
iteration 151, loss = 0.0200622770935297
iteration 152, loss = 0.01456640474498272
iteration 153, loss = 0.020229535177350044
iteration 154, loss = 0.08866988867521286
iteration 155, loss = 0.04247064143419266
iteration 156, loss = 0.014238906092941761
iteration 157, loss = 0.022039934992790222
iteration 158, loss = 0.44881612062454224
iteration 159, loss = 0.020312700420618057
iteration 160, loss = 0.3795289099216461
iteration 161, loss = 0.02017429657280445
iteration 162, loss = 0.020189372822642326
iteration 163, loss = 0.3854624032974243
iteration 164, loss = 0.015735814347863197
iteration 165, loss = 0.014726310037076473
iteration 166, loss = 0.016089588403701782
iteration 167, loss = 0.014438509941101074
iteration 168, loss = 0.0221747774630785
iteration 169, loss = 0.4528124928474426
iteration 170, loss = 0.3875322639942169
iteration 171, loss = 0.021243900060653687
iteration 172, loss = 0.012072724290192127
iteration 173, loss = 1.1249959468841553
iteration 174, loss = 0.022312559187412262
iteration 175, loss = 0.021307604387402534
iteration 176, loss = 0.01652534492313862
iteration 177, loss = 0.022033600136637688
iteration 178, loss = 0.025755271315574646
iteration 179, loss = 0.38625627756118774
iteration 180, loss = 0.012842481955885887
iteration 181, loss = 0.017651118338108063
iteration 182, loss = 0.019770938903093338
iteration 183, loss = 0.034413911402225494
iteration 184, loss = 0.01516447588801384
iteration 185, loss = 0.015694765374064445
iteration 186, loss = 0.02717057801783085
iteration 187, loss = 0.38722753524780273
iteration 188, loss = 0.3884029686450958
iteration 189, loss = 0.01521481666713953
iteration 190, loss = 0.013771376572549343
iteration 191, loss = 0.02305789291858673
iteration 192, loss = 0.014170479960739613
iteration 193, loss = 0.017017656937241554
iteration 194, loss = 0.01055044773966074
iteration 195, loss = 0.02348930761218071
iteration 196, loss = 0.38112568855285645
iteration 197, loss = 0.38189375400543213
iteration 198, loss = 0.013029947876930237
iteration 199, loss = 0.01546479482203722
iteration 200, loss = 0.015482218936085701
iteration 201, loss = 0.019457008689641953
iteration 202, loss = 0.38079631328582764
iteration 203, loss = 0.017651736736297607
iteration 204, loss = 0.7547603845596313
iteration 205, loss = 0.017297761514782906
iteration 206, loss = 0.014783519320189953
iteration 207, loss = 0.01266776118427515
iteration 208, loss = 0.01745397411286831
iteration 209, loss = 0.38751494884490967
iteration 210, loss = 0.01522340439260006
iteration 211, loss = 0.4561161994934082
iteration 212, loss = 0.012301485054194927
iteration 213, loss = 0.3786662518978119
iteration 214, loss = 0.019213233143091202
iteration 215, loss = 0.3854929208755493
iteration 216, loss = 0.01922914944589138
iteration 217, loss = 0.015690043568611145
iteration 218, loss = 0.01393238827586174
iteration 219, loss = 0.01057711336761713
iteration 220, loss = 0.39150550961494446
iteration 221, loss = 0.3850894272327423
iteration 222, loss = 0.7464703321456909
iteration 223, loss = 0.011961175128817558
iteration 224, loss = 0.013431809842586517
iteration 225, loss = 0.4036007523536682
iteration 226, loss = 0.38202670216560364
iteration 227, loss = 0.39792096614837646
iteration 228, loss = 0.016260169446468353
iteration 229, loss = 0.0197061188519001
iteration 230, loss = 0.02201397903263569
iteration 231, loss = 0.020310457795858383
iteration 232, loss = 0.013170087710022926
iteration 233, loss = 0.01166572142392397
iteration 234, loss = 0.02272840589284897
iteration 235, loss = 0.029610760509967804
iteration 236, loss = 0.016528861597180367
iteration 237, loss = 0.014828928746283054
iteration 238, loss = 0.014564622193574905
iteration 239, loss = 0.38697266578674316
iteration 240, loss = 0.045102041214704514
iteration 241, loss = 0.017356310039758682
iteration 242, loss = 0.029232153668999672
iteration 243, loss = 0.01730046421289444
iteration 244, loss = 0.01687953621149063
iteration 245, loss = 0.02342129312455654
iteration 246, loss = 0.3854626417160034
iteration 247, loss = 0.3877080976963043
iteration 248, loss = 0.38711434602737427
iteration 249, loss = 0.378788560628891
iteration 250, loss = 0.01607421413064003
iteration 251, loss = 0.38619980216026306
iteration 252, loss = 0.01781950518488884
iteration 253, loss = 0.3773553669452667
iteration 254, loss = 0.013999626971781254
iteration 255, loss = 0.022777175530791283
iteration 256, loss = 0.023819614201784134
iteration 257, loss = 0.018924612551927567
iteration 258, loss = 0.020289039239287376
iteration 259, loss = 0.015552137978374958
iteration 260, loss = 0.016847509890794754
iteration 261, loss = 0.02738131210207939
iteration 262, loss = 0.013463137671351433
iteration 263, loss = 0.01900196261703968
iteration 264, loss = 0.7655006051063538
iteration 265, loss = 0.016661712899804115
iteration 266, loss = 0.40046438574790955
iteration 267, loss = 0.3925285339355469
iteration 268, loss = 0.013340643607079983
iteration 269, loss = 0.020923949778079987
iteration 270, loss = 0.015576775185763836
iteration 271, loss = 0.01672947220504284
iteration 272, loss = 0.01037109550088644
iteration 273, loss = 0.02768971398472786
iteration 274, loss = 0.38904067873954773
iteration 275, loss = 0.3813233971595764
iteration 276, loss = 0.3864998519420624
iteration 277, loss = 0.016350995749235153
iteration 278, loss = 0.02064480446279049
iteration 279, loss = 0.011206402443349361
iteration 280, loss = 0.014940444380044937
iteration 281, loss = 0.3827288746833801
iteration 282, loss = 0.014340294525027275
iteration 283, loss = 0.08817548304796219
iteration 284, loss = 0.025446932762861252
iteration 285, loss = 0.017274167388677597
iteration 286, loss = 0.023313818499445915
iteration 287, loss = 0.018095191568136215
iteration 288, loss = 0.019990433007478714
iteration 289, loss = 0.3770429790019989
iteration 290, loss = 0.013661370612680912
iteration 291, loss = 0.019128814339637756
iteration 292, loss = 0.031098851934075356
iteration 293, loss = 0.00961117260158062
iteration 294, loss = 0.09221893548965454
iteration 295, loss = 0.013492953032255173
iteration 296, loss = 0.02490074187517166
iteration 297, loss = 0.01823277771472931
iteration 298, loss = 0.014030260033905506
iteration 299, loss = 0.013725370168685913
iteration 300, loss = 0.024063700810074806
iteration 1, loss = 0.020436126738786697
iteration 2, loss = 0.7546399235725403
iteration 3, loss = 0.01718313619494438
iteration 4, loss = 0.020600540563464165
iteration 5, loss = 0.021174639463424683
iteration 6, loss = 0.014875073917210102
iteration 7, loss = 0.010706151835620403
iteration 8, loss = 0.037595100700855255
iteration 9, loss = 0.03499471768736839
iteration 10, loss = 0.01691078208386898
iteration 11, loss = 0.019742587581276894
iteration 12, loss = 0.7445324063301086
iteration 13, loss = 0.01810813695192337
iteration 14, loss = 0.024501174688339233
iteration 15, loss = 0.3817204236984253
iteration 16, loss = 0.03261082246899605
iteration 17, loss = 0.013281242921948433
iteration 18, loss = 0.022408410906791687
iteration 19, loss = 0.01693594828248024
iteration 20, loss = 0.020215723663568497
iteration 21, loss = 0.3791475296020508
iteration 22, loss = 0.3903256952762604
iteration 23, loss = 0.013136819005012512
iteration 24, loss = 0.010765347629785538
iteration 25, loss = 0.01627027988433838
iteration 26, loss = 0.024958141148090363
iteration 27, loss = 0.3896872401237488
iteration 28, loss = 0.015904845669865608
iteration 29, loss = 0.37683939933776855
iteration 30, loss = 0.016286278143525124
iteration 31, loss = 0.026344923302531242
iteration 32, loss = 0.018819747492671013
iteration 33, loss = 0.38584479689598083
iteration 34, loss = 0.01578480191528797
iteration 35, loss = 0.425141841173172
iteration 36, loss = 0.014086797833442688
iteration 37, loss = 0.022265776991844177
iteration 38, loss = 0.3796093761920929
iteration 39, loss = 0.020404372364282608
iteration 40, loss = 0.01866704225540161
iteration 41, loss = 0.017441455274820328
iteration 42, loss = 0.01953667216002941
iteration 43, loss = 0.016823824495077133
iteration 44, loss = 0.0222952701151371
iteration 45, loss = 0.013759756460785866
iteration 46, loss = 0.016082972288131714
iteration 47, loss = 0.020381366834044456
iteration 48, loss = 0.749205470085144
iteration 49, loss = 0.01475345715880394
iteration 50, loss = 0.09352298825979233
iteration 51, loss = 0.016497595235705376
iteration 52, loss = 0.013859454542398453
iteration 53, loss = 0.021140441298484802
iteration 54, loss = 0.012547670863568783
iteration 55, loss = 0.009518817067146301
iteration 56, loss = 0.01792091131210327
iteration 57, loss = 0.018114544451236725
iteration 58, loss = 0.017389051616191864
iteration 59, loss = 0.014730735681951046
iteration 60, loss = 0.018160449340939522
iteration 61, loss = 0.011355401948094368
iteration 62, loss = 0.010608483105897903
iteration 63, loss = 0.016144156455993652
iteration 64, loss = 0.018360406160354614
iteration 65, loss = 0.3810878396034241
iteration 66, loss = 0.03649692237377167
iteration 67, loss = 0.013780143111944199
iteration 68, loss = 0.38382256031036377
iteration 69, loss = 0.09041030704975128
iteration 70, loss = 0.7532370090484619
iteration 71, loss = 0.010318159125745296
iteration 72, loss = 0.4742925763130188
iteration 73, loss = 0.021763985976576805
iteration 74, loss = 0.38085728883743286
iteration 75, loss = 0.01324481051415205
iteration 76, loss = 0.01562527008354664
iteration 77, loss = 0.3861697316169739
iteration 78, loss = 0.45520952343940735
iteration 79, loss = 0.387723445892334
iteration 80, loss = 0.01653587445616722
iteration 81, loss = 0.017854072153568268
iteration 82, loss = 0.011250035837292671
iteration 83, loss = 0.38641220331192017
iteration 84, loss = 0.014966349117457867
iteration 85, loss = 0.016916893422603607
iteration 86, loss = 0.390608012676239
iteration 87, loss = 0.011858700774610043
iteration 88, loss = 0.014994616620242596
iteration 89, loss = 0.025646967813372612
iteration 90, loss = 0.38239094614982605
iteration 91, loss = 0.01208057627081871
iteration 92, loss = 0.38869351148605347
iteration 93, loss = 0.029049575328826904
iteration 94, loss = 0.01777484267950058
iteration 95, loss = 0.01605297438800335
iteration 96, loss = 0.39421993494033813
iteration 97, loss = 0.017667263746261597
iteration 98, loss = 0.39153918623924255
iteration 99, loss = 0.3784266412258148
iteration 100, loss = 0.014624641276896
iteration 101, loss = 0.02163313888013363
iteration 102, loss = 0.38692840933799744
iteration 103, loss = 0.026493268087506294
iteration 104, loss = 0.01409799326211214
iteration 105, loss = 0.030834132805466652
iteration 106, loss = 0.017127634957432747
iteration 107, loss = 0.026729799807071686
iteration 108, loss = 0.02146247774362564
iteration 109, loss = 0.45297813415527344
iteration 110, loss = 0.019427932798862457
iteration 111, loss = 0.01967499405145645
iteration 112, loss = 0.3869718313217163
iteration 113, loss = 0.3807559609413147
iteration 114, loss = 0.01265663467347622
iteration 115, loss = 0.016237936913967133
iteration 116, loss = 0.3813263475894928
iteration 117, loss = 0.03837548568844795
iteration 118, loss = 0.017472166568040848
iteration 119, loss = 0.3758178651332855
iteration 120, loss = 0.012554842047393322
iteration 121, loss = 0.013097572140395641
iteration 122, loss = 0.013203155249357224
iteration 123, loss = 0.022640954703092575
iteration 124, loss = 0.02821582928299904
iteration 125, loss = 0.03396419435739517
iteration 126, loss = 0.01114629115909338
iteration 127, loss = 0.37918800115585327
iteration 128, loss = 0.3821810185909271
iteration 129, loss = 0.014658642932772636
iteration 130, loss = 0.01462115440517664
iteration 131, loss = 0.018561122938990593
iteration 132, loss = 0.011732063256204128
iteration 133, loss = 0.016798408702015877
iteration 134, loss = 0.40706944465637207
iteration 135, loss = 0.3857133388519287
iteration 136, loss = 0.011015202850103378
iteration 137, loss = 0.01949985697865486
iteration 138, loss = 0.015800930559635162
iteration 139, loss = 0.011690805666148663
iteration 140, loss = 0.013761701993644238
iteration 141, loss = 0.00887240655720234
iteration 142, loss = 0.01890822872519493
iteration 143, loss = 0.02348845824599266
iteration 144, loss = 0.024107828736305237
iteration 145, loss = 0.013637179508805275
iteration 146, loss = 1.1191025972366333
iteration 147, loss = 0.3990672528743744
iteration 148, loss = 0.02229178138077259
iteration 149, loss = 0.01720370538532734
iteration 150, loss = 0.45930492877960205
iteration 151, loss = 0.011411944404244423
iteration 152, loss = 0.014004976488649845
iteration 153, loss = 0.027549371123313904
iteration 154, loss = 0.023262735456228256
iteration 155, loss = 0.018249623477458954
iteration 156, loss = 0.37767574191093445
iteration 157, loss = 0.015325440093874931
iteration 158, loss = 0.016002416610717773
iteration 159, loss = 0.0415787473320961
iteration 160, loss = 0.01094870362430811
iteration 161, loss = 0.02459712140262127
iteration 162, loss = 0.014859639108181
iteration 163, loss = 0.019692206755280495
iteration 164, loss = 0.018482789397239685
iteration 165, loss = 0.014570861123502254
iteration 166, loss = 0.01820450648665428
iteration 167, loss = 0.3862914741039276
iteration 168, loss = 0.0184453334659338
iteration 169, loss = 0.3981940746307373
iteration 170, loss = 0.024417392909526825
iteration 171, loss = 0.022151950746774673
iteration 172, loss = 0.37993136048316956
iteration 173, loss = 0.01648796536028385
iteration 174, loss = 0.01614321395754814
iteration 175, loss = 0.020592065528035164
iteration 176, loss = 0.019929053261876106
iteration 177, loss = 0.37545332312583923
iteration 178, loss = 0.018362348899245262
iteration 179, loss = 0.013935791328549385
iteration 180, loss = 0.37768322229385376
iteration 181, loss = 0.019422583281993866
iteration 182, loss = 0.08874532580375671
iteration 183, loss = 0.01786651462316513
iteration 184, loss = 0.38056954741477966
iteration 185, loss = 0.022905807942152023
iteration 186, loss = 0.023453397676348686
iteration 187, loss = 0.38388392329216003
iteration 188, loss = 0.021959736943244934
iteration 189, loss = 0.7533596158027649
iteration 190, loss = 0.031283389776945114
iteration 191, loss = 0.023285221308469772
iteration 192, loss = 0.008989023976027966
iteration 193, loss = 0.4874453842639923
iteration 194, loss = 0.011539461091160774
iteration 195, loss = 0.017804734408855438
iteration 196, loss = 0.37807273864746094
iteration 197, loss = 0.019409626722335815
iteration 198, loss = 0.03384914621710777
iteration 199, loss = 0.382864773273468
iteration 200, loss = 0.01916794665157795
iteration 201, loss = 0.03510437533259392
iteration 202, loss = 0.3790975511074066
iteration 203, loss = 0.028385240584611893
iteration 204, loss = 0.023971132934093475
iteration 205, loss = 0.014984638430178165
iteration 206, loss = 0.01562746986746788
iteration 207, loss = 0.3822242319583893
iteration 208, loss = 0.017081368714571
iteration 209, loss = 0.010411749593913555
iteration 210, loss = 0.021407796069979668
iteration 211, loss = 0.029222963377833366
iteration 212, loss = 0.019998760893940926
iteration 213, loss = 0.013911102898418903
iteration 214, loss = 0.015614817850291729
iteration 215, loss = 0.040698759257793427
iteration 216, loss = 0.022732578217983246
iteration 217, loss = 0.38692528009414673
iteration 218, loss = 0.02209564670920372
iteration 219, loss = 0.38251861929893494
iteration 220, loss = 0.022948499768972397
iteration 221, loss = 0.018125196918845177
iteration 222, loss = 0.039836689829826355
iteration 223, loss = 0.022810567170381546
iteration 224, loss = 0.016959717497229576
iteration 225, loss = 0.012047573924064636
iteration 226, loss = 0.014463422819972038
iteration 227, loss = 0.036797527223825455
iteration 228, loss = 0.09626179933547974
iteration 229, loss = 0.3792567253112793
iteration 230, loss = 0.38033798336982727
iteration 231, loss = 0.014235795475542545
iteration 232, loss = 0.03774111717939377
iteration 233, loss = 0.017222542315721512
iteration 234, loss = 0.01858673430979252
iteration 235, loss = 0.015799256041646004
iteration 236, loss = 0.39627790451049805
iteration 237, loss = 0.02414044924080372
iteration 238, loss = 0.007522661238908768
iteration 239, loss = 0.45420876145362854
iteration 240, loss = 0.025915103033185005
iteration 241, loss = 0.021975139155983925
iteration 242, loss = 0.009257283993065357
iteration 243, loss = 0.018116425722837448
iteration 244, loss = 0.02508128620684147
iteration 245, loss = 0.015806999057531357
iteration 246, loss = 0.026774030178785324
iteration 247, loss = 0.02052759751677513
iteration 248, loss = 0.011131783947348595
iteration 249, loss = 0.033459439873695374
iteration 250, loss = 0.017115352675318718
iteration 251, loss = 0.3780675232410431
iteration 252, loss = 0.08659922331571579
iteration 253, loss = 0.8164207339286804
iteration 254, loss = 0.01713133230805397
iteration 255, loss = 0.032803915441036224
iteration 256, loss = 0.012694261968135834
iteration 257, loss = 0.01887049898505211
iteration 258, loss = 0.4089263081550598
iteration 259, loss = 0.015200133435428143
iteration 260, loss = 0.01759134978055954
iteration 261, loss = 0.01676829159259796
iteration 262, loss = 0.0243403110653162
iteration 263, loss = 0.020735817030072212
iteration 264, loss = 0.3888666331768036
iteration 265, loss = 0.014340171590447426
iteration 266, loss = 0.019145585596561432
iteration 267, loss = 0.018393775448203087
iteration 268, loss = 0.014840228483080864
iteration 269, loss = 0.018335919827222824
iteration 270, loss = 0.387104332447052
iteration 271, loss = 0.38340893387794495
iteration 272, loss = 0.017142027616500854
iteration 273, loss = 0.024506524205207825
iteration 274, loss = 0.011065902188420296
iteration 275, loss = 0.03343483805656433
iteration 276, loss = 0.013464823365211487
iteration 277, loss = 0.022663943469524384
iteration 278, loss = 0.387742280960083
iteration 279, loss = 0.021026611328125
iteration 280, loss = 0.4505096971988678
iteration 281, loss = 0.012799370102584362
iteration 282, loss = 0.017312483862042427
iteration 283, loss = 0.10463380068540573
iteration 284, loss = 0.013013401068747044
iteration 285, loss = 0.38709762692451477
iteration 286, loss = 0.45370131731033325
iteration 287, loss = 0.04064987599849701
iteration 288, loss = 0.017788711935281754
iteration 289, loss = 0.01905837282538414
iteration 290, loss = 0.39195749163627625
iteration 291, loss = 0.021053697913885117
iteration 292, loss = 0.016906075179576874
iteration 293, loss = 0.02498675137758255
iteration 294, loss = 0.3797294497489929
iteration 295, loss = 0.38575294613838196
iteration 296, loss = 0.014603124000132084
iteration 297, loss = 0.02506418526172638
iteration 298, loss = 0.014087419956922531
iteration 299, loss = 0.011914817616343498
iteration 300, loss = 0.026099134236574173
iteration 1, loss = 0.016103368252515793
iteration 2, loss = 0.020397208631038666
iteration 3, loss = 0.017322272062301636
iteration 4, loss = 0.01666935347020626
iteration 5, loss = 0.017387520521879196
iteration 6, loss = 0.015932049602270126
iteration 7, loss = 0.38226157426834106
iteration 8, loss = 0.017719484865665436
iteration 9, loss = 0.0326383113861084
iteration 10, loss = 0.03822694346308708
iteration 11, loss = 0.018038686364889145
iteration 12, loss = 0.015593400225043297
iteration 13, loss = 0.026089251041412354
iteration 14, loss = 0.015918508172035217
iteration 15, loss = 0.009626246988773346
iteration 16, loss = 0.020111309364438057
iteration 17, loss = 0.3755408227443695
iteration 18, loss = 0.38632693886756897
iteration 19, loss = 0.09232206642627716
iteration 20, loss = 0.027080776169896126
iteration 21, loss = 0.021638020873069763
iteration 22, loss = 0.03209284320473671
iteration 23, loss = 0.3866173326969147
iteration 24, loss = 0.020606713369488716
iteration 25, loss = 0.02810600772500038
iteration 26, loss = 0.025955412536859512
iteration 27, loss = 0.7443205118179321
iteration 28, loss = 0.7449211478233337
iteration 29, loss = 0.37960752844810486
iteration 30, loss = 0.012471849098801613
iteration 31, loss = 0.011133418418467045
iteration 32, loss = 0.011916246265172958
iteration 33, loss = 0.026660829782485962
iteration 34, loss = 0.4496556520462036
iteration 35, loss = 0.3811000883579254
iteration 36, loss = 0.0280148983001709
iteration 37, loss = 0.013436938636004925
iteration 38, loss = 0.01202327013015747
iteration 39, loss = 0.45370233058929443
iteration 40, loss = 0.38075917959213257
iteration 41, loss = 0.02002136968076229
iteration 42, loss = 0.0149152260273695
iteration 43, loss = 0.005710747092962265
iteration 44, loss = 0.01335702184587717
iteration 45, loss = 0.02665189653635025
iteration 46, loss = 0.017640072852373123
iteration 47, loss = 0.014357686042785645
iteration 48, loss = 0.3844873905181885
iteration 49, loss = 0.02563779428601265
iteration 50, loss = 0.013849223963916302
iteration 51, loss = 0.017268724739551544
iteration 52, loss = 0.015562254935503006
iteration 53, loss = 0.01971311680972576
iteration 54, loss = 0.01848127506673336
iteration 55, loss = 0.3773251473903656
iteration 56, loss = 0.7571646571159363
iteration 57, loss = 0.021409083157777786
iteration 58, loss = 0.018185928463935852
iteration 59, loss = 0.015016809105873108
iteration 60, loss = 0.012966470792889595
iteration 61, loss = 0.02094545215368271
iteration 62, loss = 0.019547903910279274
iteration 63, loss = 0.01208475325256586
iteration 64, loss = 0.08969711512327194
iteration 65, loss = 0.02716764435172081
iteration 66, loss = 0.3787909746170044
iteration 67, loss = 0.0191617663949728
iteration 68, loss = 0.02065139263868332
iteration 69, loss = 0.03615882992744446
iteration 70, loss = 0.3856317400932312
iteration 71, loss = 0.3825930058956146
iteration 72, loss = 0.022470250725746155
iteration 73, loss = 0.015041987411677837
iteration 74, loss = 0.017807066440582275
iteration 75, loss = 0.023855505511164665
iteration 76, loss = 0.45057883858680725
iteration 77, loss = 0.7500250935554504
iteration 78, loss = 0.018753625452518463
iteration 79, loss = 0.009717664681375027
iteration 80, loss = 0.032651010900735855
iteration 81, loss = 0.012533927336335182
iteration 82, loss = 0.7419173717498779
iteration 83, loss = 0.3868488073348999
iteration 84, loss = 0.016182905063033104
iteration 85, loss = 0.38454028964042664
iteration 86, loss = 0.031223494559526443
iteration 87, loss = 0.08481481671333313
iteration 88, loss = 0.39300116896629333
iteration 89, loss = 0.01278369128704071
iteration 90, loss = 0.014163974672555923
iteration 91, loss = 0.02006249874830246
iteration 92, loss = 0.010499482043087482
iteration 93, loss = 0.02747456170618534
iteration 94, loss = 0.021823368966579437
iteration 95, loss = 0.014632785692811012
iteration 96, loss = 0.3806679844856262
iteration 97, loss = 0.0166177861392498
iteration 98, loss = 0.01548230741173029
iteration 99, loss = 0.033640362322330475
iteration 100, loss = 0.03200644999742508
iteration 101, loss = 0.39328038692474365
iteration 102, loss = 0.3881802558898926
iteration 103, loss = 0.03670291230082512
iteration 104, loss = 0.08236103504896164
iteration 105, loss = 0.3820847272872925
iteration 106, loss = 0.021064750850200653
iteration 107, loss = 0.01865288056433201
iteration 108, loss = 0.02237122319638729
iteration 109, loss = 0.026969078928232193
iteration 110, loss = 0.7437613010406494
iteration 111, loss = 0.018672704696655273
iteration 112, loss = 0.014786955900490284
iteration 113, loss = 0.010274230502545834
iteration 114, loss = 0.013476279564201832
iteration 115, loss = 0.02705584093928337
iteration 116, loss = 0.4576890170574188
iteration 117, loss = 0.02667887881398201
iteration 118, loss = 0.7426352500915527
iteration 119, loss = 0.015679478645324707
iteration 120, loss = 0.012821977958083153
iteration 121, loss = 0.021860938519239426
iteration 122, loss = 0.023153871297836304
iteration 123, loss = 0.020598720759153366
iteration 124, loss = 0.023433849215507507
iteration 125, loss = 0.02561085671186447
iteration 126, loss = 0.018123598769307137
iteration 127, loss = 0.016374163329601288
iteration 128, loss = 0.39879724383354187
iteration 129, loss = 0.0192735455930233
iteration 130, loss = 0.4033447504043579
iteration 131, loss = 0.3883334994316101
iteration 132, loss = 0.3808307647705078
iteration 133, loss = 0.3800557255744934
iteration 134, loss = 0.37528562545776367
iteration 135, loss = 0.37889617681503296
iteration 136, loss = 0.024002939462661743
iteration 137, loss = 0.37575072050094604
iteration 138, loss = 0.01790335588157177
iteration 139, loss = 0.3781900107860565
iteration 140, loss = 0.022482894361019135
iteration 141, loss = 0.020497402176260948
iteration 142, loss = 0.01821719855070114
iteration 143, loss = 0.022723384201526642
iteration 144, loss = 0.38884252309799194
iteration 145, loss = 0.026877321302890778
iteration 146, loss = 0.023058447986841202
iteration 147, loss = 0.3788539469242096
iteration 148, loss = 0.39463111758232117
iteration 149, loss = 0.01439465768635273
iteration 150, loss = 0.01779337413609028
iteration 151, loss = 0.02161753550171852
iteration 152, loss = 0.3830137252807617
iteration 153, loss = 0.01624891720712185
iteration 154, loss = 0.011691791005432606
iteration 155, loss = 0.017140954732894897
iteration 156, loss = 0.014459087513387203
iteration 157, loss = 0.017198216170072556
iteration 158, loss = 0.016515683382749557
iteration 159, loss = 0.029620220884680748
iteration 160, loss = 0.022563952952623367
iteration 161, loss = 0.017831791192293167
iteration 162, loss = 0.014825130812823772
iteration 163, loss = 0.013584000058472157
iteration 164, loss = 0.025259902700781822
iteration 165, loss = 0.016703767701983452
iteration 166, loss = 0.3898622989654541
iteration 167, loss = 0.0155147984623909
iteration 168, loss = 0.023307912051677704
iteration 169, loss = 0.014604744501411915
iteration 170, loss = 0.018995068967342377
iteration 171, loss = 0.018274758011102676
iteration 172, loss = 0.01584305241703987
iteration 173, loss = 0.02257903292775154
iteration 174, loss = 0.3872571885585785
iteration 175, loss = 0.030419733375310898
iteration 176, loss = 0.38063618540763855
iteration 177, loss = 0.016966350376605988
iteration 178, loss = 0.028247186914086342
iteration 179, loss = 0.7413182854652405
iteration 180, loss = 0.019127562642097473
iteration 181, loss = 0.023803746327757835
iteration 182, loss = 0.028568023815751076
iteration 183, loss = 0.010922095738351345
iteration 184, loss = 0.39489561319351196
iteration 185, loss = 0.01905086636543274
iteration 186, loss = 0.01804697886109352
iteration 187, loss = 0.021721895784139633
iteration 188, loss = 0.016689401119947433
iteration 189, loss = 0.01798645406961441
iteration 190, loss = 0.45701250433921814
iteration 191, loss = 0.7448171377182007
iteration 192, loss = 0.45621728897094727
iteration 193, loss = 0.02333333157002926
iteration 194, loss = 0.0340314581990242
iteration 195, loss = 0.02591594122350216
iteration 196, loss = 0.018692484125494957
iteration 197, loss = 0.4002441465854645
iteration 198, loss = 0.019274530932307243
iteration 199, loss = 0.017243651673197746
iteration 200, loss = 0.3802816867828369
iteration 201, loss = 0.016321204602718353
iteration 202, loss = 0.3794305622577667
iteration 203, loss = 0.03185863792896271
iteration 204, loss = 0.022323312237858772
iteration 205, loss = 0.01928768865764141
iteration 206, loss = 0.026189064607024193
iteration 207, loss = 0.3926793932914734
iteration 208, loss = 0.01539685856550932
iteration 209, loss = 0.01772451214492321
iteration 210, loss = 0.3825832009315491
iteration 211, loss = 0.3759976029396057
iteration 212, loss = 0.3975675702095032
iteration 213, loss = 0.0230195764452219
iteration 214, loss = 0.011355537921190262
iteration 215, loss = 0.020093396306037903
iteration 216, loss = 0.014205871149897575
iteration 217, loss = 0.02300058864057064
iteration 218, loss = 0.02043060213327408
iteration 219, loss = 0.3756479024887085
iteration 220, loss = 0.09122150391340256
iteration 221, loss = 0.03260749205946922
iteration 222, loss = 0.014603042043745518
iteration 223, loss = 0.021044813096523285
iteration 224, loss = 0.018315518274903297
iteration 225, loss = 0.01007037702947855
iteration 226, loss = 0.08617696166038513
iteration 227, loss = 0.016902567818760872
iteration 228, loss = 0.01643710769712925
iteration 229, loss = 0.022538479417562485
iteration 230, loss = 0.018232500180602074
iteration 231, loss = 0.017604704946279526
iteration 232, loss = 0.37514635920524597
iteration 233, loss = 0.013613743707537651
iteration 234, loss = 0.3844088613986969
iteration 235, loss = 0.38510751724243164
iteration 236, loss = 0.02509487420320511
iteration 237, loss = 0.013191433623433113
iteration 238, loss = 0.0138543751090765
iteration 239, loss = 0.02467295527458191
iteration 240, loss = 0.38030946254730225
iteration 241, loss = 0.022474199533462524
iteration 242, loss = 0.033702459186315536
iteration 243, loss = 0.3812218904495239
iteration 244, loss = 0.02200254611670971
iteration 245, loss = 0.014215921983122826
iteration 246, loss = 0.015179399400949478
iteration 247, loss = 0.017982549965381622
iteration 248, loss = 0.02167045883834362
iteration 249, loss = 0.020539840683341026
iteration 250, loss = 0.3813815712928772
iteration 251, loss = 0.01773473620414734
iteration 252, loss = 0.023612983524799347
iteration 253, loss = 0.02558497153222561
iteration 254, loss = 0.3880753517150879
iteration 255, loss = 0.0892048329114914
iteration 256, loss = 0.013281992636620998
iteration 257, loss = 0.016863875091075897
iteration 258, loss = 0.7618484497070312
iteration 259, loss = 0.016877781599760056
iteration 260, loss = 0.01429377868771553
iteration 261, loss = 0.022043488919734955
iteration 262, loss = 0.0163299348205328
iteration 263, loss = 0.014464806765317917
iteration 264, loss = 0.011625193990767002
iteration 265, loss = 0.01113647036254406
iteration 266, loss = 0.016493644565343857
iteration 267, loss = 0.3782288432121277
iteration 268, loss = 0.02661382406949997
iteration 269, loss = 0.018008556216955185
iteration 270, loss = 0.3853316903114319
iteration 271, loss = 0.10399758070707321
iteration 272, loss = 0.017076602205634117
iteration 273, loss = 0.025439104065299034
iteration 274, loss = 0.09539995342493057
iteration 275, loss = 0.033796973526477814
iteration 276, loss = 0.018543526530265808
iteration 277, loss = 0.017839452251791954
iteration 278, loss = 0.011382335796952248
iteration 279, loss = 0.017394110560417175
iteration 280, loss = 0.02159425988793373
iteration 281, loss = 0.02403898350894451
iteration 282, loss = 0.01422763243317604
iteration 283, loss = 0.014794710092246532
iteration 284, loss = 0.015482129529118538
iteration 285, loss = 0.014238887466490269
iteration 286, loss = 0.01703798584640026
iteration 287, loss = 0.02565983310341835
iteration 288, loss = 0.02189723402261734
iteration 289, loss = 0.020179037004709244
iteration 290, loss = 0.021015841513872147
iteration 291, loss = 0.013815250247716904
iteration 292, loss = 0.0203417856246233
iteration 293, loss = 0.021084453910589218
iteration 294, loss = 0.0183471217751503
iteration 295, loss = 0.02262069843709469
iteration 296, loss = 0.012492332607507706
iteration 297, loss = 0.392961710691452
iteration 298, loss = 0.009301451966166496
iteration 299, loss = 0.022287147119641304
iteration 300, loss = 0.026333078742027283

iteration 1, loss = 3.4361157417297363
iteration 2, loss = 6.071143627166748
iteration 3, loss = 5.862336158752441
iteration 4, loss = 4.458922863006592
iteration 5, loss = 1.1049779653549194
iteration 6, loss = 1.429640293121338
iteration 7, loss = 0.647943377494812
iteration 8, loss = 0.41024327278137207
iteration 9, loss = 0.6508992910385132
iteration 10, loss = 0.39307624101638794
iteration 11, loss = 0.5162062644958496
iteration 12, loss = 0.4200858175754547
iteration 13, loss = 0.6001100540161133
iteration 14, loss = 0.32590773701667786
iteration 15, loss = 0.36477363109588623
iteration 16, loss = 0.5302391052246094
iteration 17, loss = 0.39782869815826416
iteration 18, loss = 0.42280733585357666
iteration 19, loss = 0.5154947638511658
iteration 20, loss = 0.4732564091682434
iteration 21, loss = 0.46451452374458313
iteration 22, loss = 0.4520829916000366
iteration 23, loss = 0.4579123258590698
iteration 24, loss = 0.5954511761665344
iteration 25, loss = 0.4269496500492096
iteration 26, loss = 0.4479277431964874
iteration 27, loss = 0.4088398218154907
iteration 28, loss = 0.42094162106513977
iteration 29, loss = 0.4991346597671509
iteration 30, loss = 0.3958900570869446
iteration 31, loss = 0.465434193611145
iteration 32, loss = 0.4947252571582794
iteration 33, loss = 0.4580209255218506
iteration 34, loss = 0.4268171787261963
iteration 35, loss = 0.48839515447616577
iteration 36, loss = 0.4822205901145935
iteration 37, loss = 0.4062882363796234
iteration 38, loss = 0.6487762331962585
iteration 39, loss = 0.5642814636230469
iteration 40, loss = 0.49846354126930237
iteration 41, loss = 0.5310416221618652
iteration 42, loss = 0.43800559639930725
iteration 43, loss = 0.39191073179244995
iteration 44, loss = 0.6319937705993652
iteration 45, loss = 0.5597487092018127
iteration 46, loss = 0.46511110663414
iteration 47, loss = 0.4367738366127014
iteration 48, loss = 0.5180878043174744
iteration 49, loss = 0.4573517441749573
iteration 50, loss = 0.51549232006073
iteration 51, loss = 0.4774879217147827
iteration 52, loss = 0.3921782374382019
iteration 53, loss = 0.4952303469181061
iteration 54, loss = 0.3905273675918579
iteration 55, loss = 0.4680050015449524
iteration 56, loss = 0.49738895893096924
iteration 57, loss = 0.4998730421066284
iteration 58, loss = 0.4051121175289154
iteration 59, loss = 0.440396785736084
iteration 60, loss = 0.38495948910713196
iteration 61, loss = 0.48796218633651733
iteration 62, loss = 0.38263607025146484
iteration 63, loss = 0.5161854028701782
iteration 64, loss = 0.5038180351257324
iteration 65, loss = 0.43330588936805725
iteration 66, loss = 0.4530104994773865
iteration 67, loss = 0.46013206243515015
iteration 68, loss = 0.4308493733406067
iteration 69, loss = 0.484953373670578
iteration 70, loss = 0.46955937147140503
iteration 71, loss = 0.42241886258125305
iteration 72, loss = 0.39757853746414185
iteration 73, loss = 0.4491928815841675
iteration 74, loss = 0.45463550090789795
iteration 75, loss = 0.3663075566291809
iteration 76, loss = 0.4029791057109833
iteration 77, loss = 0.4312613606452942
iteration 78, loss = 0.4124738574028015
iteration 79, loss = 0.4903497099876404
iteration 80, loss = 0.4418773949146271
iteration 81, loss = 0.48704972863197327
iteration 82, loss = 0.43780189752578735
iteration 83, loss = 0.39429333806037903
iteration 84, loss = 0.4922229051589966
iteration 85, loss = 0.43895015120506287
iteration 86, loss = 0.4909162223339081
iteration 87, loss = 0.39367005228996277
iteration 88, loss = 0.509906530380249
iteration 89, loss = 0.502801239490509
iteration 90, loss = 0.4396657943725586
iteration 91, loss = 0.5623734593391418
iteration 92, loss = 0.5066326856613159
iteration 93, loss = 0.4102964401245117
iteration 94, loss = 0.44611606001853943
iteration 95, loss = 0.4081885516643524
iteration 96, loss = 0.47542038559913635
iteration 97, loss = 0.43560805916786194
iteration 98, loss = 0.44812145829200745
iteration 99, loss = 0.39388278126716614
iteration 100, loss = 0.4322463274002075
iteration 101, loss = 0.40343111753463745
iteration 102, loss = 0.3994881510734558
iteration 103, loss = 0.43629711866378784
iteration 104, loss = 0.5129212737083435
iteration 105, loss = 0.4810643196105957
iteration 106, loss = 0.3884392976760864
iteration 107, loss = 0.38301965594291687
iteration 108, loss = 0.4577864110469818
iteration 109, loss = 0.39268970489501953
iteration 110, loss = 0.4525267481803894
iteration 111, loss = 0.38666388392448425
iteration 112, loss = 0.46499142050743103
iteration 113, loss = 0.418570876121521
iteration 114, loss = 0.4818631708621979
iteration 115, loss = 0.3603478968143463
iteration 116, loss = 0.4016311764717102
iteration 117, loss = 0.4833788275718689
iteration 118, loss = 0.3783419132232666
iteration 119, loss = 0.4171488285064697
iteration 120, loss = 0.47760099172592163
iteration 121, loss = 0.40315020084381104
iteration 122, loss = 0.4462434947490692
iteration 123, loss = 0.40814104676246643
iteration 124, loss = 0.541291356086731
iteration 125, loss = 0.42801183462142944
iteration 126, loss = 0.432849258184433
iteration 127, loss = 0.42419949173927307
iteration 128, loss = 0.4320961534976959
iteration 129, loss = 0.44490933418273926
iteration 130, loss = 0.3520866632461548
iteration 131, loss = 0.4039158225059509
iteration 132, loss = 0.44683873653411865
iteration 133, loss = 0.37222665548324585
iteration 134, loss = 0.46491530537605286
iteration 135, loss = 0.39099958539009094
iteration 136, loss = 0.40445348620414734
iteration 137, loss = 0.4405464828014374
iteration 138, loss = 0.44310295581817627
iteration 139, loss = 0.39547592401504517
iteration 140, loss = 0.4729611277580261
iteration 141, loss = 0.38853198289871216
iteration 142, loss = 0.4562258720397949
iteration 143, loss = 0.428091436624527
iteration 144, loss = 0.43398258090019226
iteration 145, loss = 0.3741188645362854
iteration 146, loss = 0.5518878698348999
iteration 147, loss = 0.39663511514663696
iteration 148, loss = 0.42686861753463745
iteration 149, loss = 0.3881208300590515
iteration 150, loss = 0.40147048234939575
iteration 151, loss = 0.443073570728302
iteration 152, loss = 0.4476226270198822
iteration 153, loss = 0.48948997259140015
iteration 154, loss = 0.3904991149902344
iteration 155, loss = 0.4358499050140381
iteration 156, loss = 0.37420859932899475
iteration 157, loss = 0.4357033371925354
iteration 158, loss = 0.4588892459869385
iteration 159, loss = 0.38767504692077637
iteration 160, loss = 0.44067704677581787
iteration 161, loss = 0.46477270126342773
iteration 162, loss = 0.43489155173301697
iteration 163, loss = 0.4522732198238373
iteration 164, loss = 0.47501376271247864
iteration 165, loss = 0.45794469118118286
iteration 166, loss = 0.41864413022994995
iteration 167, loss = 0.40217432379722595
iteration 168, loss = 0.39215371012687683
iteration 169, loss = 0.46249255537986755
iteration 170, loss = 0.42183053493499756
iteration 171, loss = 0.427447646856308
iteration 172, loss = 0.38318943977355957
iteration 173, loss = 0.38945120573043823
iteration 174, loss = 0.3688807785511017
iteration 175, loss = 0.45310837030410767
iteration 176, loss = 0.3409762382507324
iteration 177, loss = 0.4417024850845337
iteration 178, loss = 0.43808868527412415
iteration 179, loss = 0.4586533010005951
iteration 180, loss = 0.46425023674964905
iteration 181, loss = 0.4719613194465637
iteration 182, loss = 0.3631889522075653
iteration 183, loss = 0.34123727679252625
iteration 184, loss = 0.4363551735877991
iteration 185, loss = 0.3424403667449951
iteration 186, loss = 0.39714592695236206
iteration 187, loss = 0.34912723302841187
iteration 188, loss = 0.4312435984611511
iteration 189, loss = 0.39611905813217163
iteration 190, loss = 0.4821106791496277
iteration 191, loss = 0.4458554983139038
iteration 192, loss = 0.4240068197250366
iteration 193, loss = 0.45307856798171997
iteration 194, loss = 0.43817830085754395
iteration 195, loss = 0.44437214732170105
iteration 196, loss = 0.4410274028778076
iteration 197, loss = 0.377402126789093
iteration 198, loss = 0.3663861155509949
iteration 199, loss = 0.4650425314903259
iteration 200, loss = 0.3756912648677826
iteration 201, loss = 0.39762452244758606
iteration 202, loss = 0.38817015290260315
iteration 203, loss = 0.42062678933143616
iteration 204, loss = 0.4156745672225952
iteration 205, loss = 0.41541197896003723
iteration 206, loss = 0.39127951860427856
iteration 207, loss = 0.327874094247818
iteration 208, loss = 0.4070613980293274
iteration 209, loss = 0.43449708819389343
iteration 210, loss = 0.4625415802001953
iteration 211, loss = 0.3698805272579193
iteration 212, loss = 0.4232766628265381
iteration 213, loss = 0.4380606412887573
iteration 214, loss = 0.453848659992218
iteration 215, loss = 0.37969523668289185
iteration 216, loss = 0.44338953495025635
iteration 217, loss = 0.3079126179218292
iteration 218, loss = 0.40706923604011536
iteration 219, loss = 0.4253957271575928
iteration 220, loss = 0.3434833586215973
iteration 221, loss = 0.4233705699443817
iteration 222, loss = 0.40315327048301697
iteration 223, loss = 0.38121479749679565
iteration 224, loss = 0.4042636752128601
iteration 225, loss = 0.31454703211784363
iteration 226, loss = 0.43859851360321045
iteration 227, loss = 0.3594176769256592
iteration 228, loss = 0.32587316632270813
iteration 229, loss = 0.4077804386615753
iteration 230, loss = 0.32328641414642334
iteration 231, loss = 0.41645607352256775
iteration 232, loss = 0.40766462683677673
iteration 233, loss = 0.36274221539497375
iteration 234, loss = 0.48550426959991455
iteration 235, loss = 0.3883734345436096
iteration 236, loss = 0.4120129346847534
iteration 237, loss = 0.3714063763618469
iteration 238, loss = 0.368511438369751
iteration 239, loss = 0.3816644549369812
iteration 240, loss = 0.4128246605396271
iteration 241, loss = 0.36959096789360046
iteration 242, loss = 0.36418771743774414
iteration 243, loss = 0.3963735103607178
iteration 244, loss = 0.3941827118396759
iteration 245, loss = 0.3688850402832031
iteration 246, loss = 0.3831908106803894
iteration 247, loss = 0.3890937268733978
iteration 248, loss = 0.38188326358795166
iteration 249, loss = 0.35289159417152405
iteration 250, loss = 0.3796955943107605
iteration 251, loss = 0.3597109019756317
iteration 252, loss = 0.3916357159614563
iteration 253, loss = 0.47146087884902954
iteration 254, loss = 0.3933994770050049
iteration 255, loss = 0.36314788460731506
iteration 256, loss = 0.4040854871273041
iteration 257, loss = 0.3472699224948883
iteration 258, loss = 0.37391114234924316
iteration 259, loss = 0.3873879015445709
iteration 260, loss = 0.3218005895614624
iteration 261, loss = 0.44257286190986633
iteration 262, loss = 0.3818485736846924
iteration 263, loss = 0.3007771372795105
iteration 264, loss = 0.46538904309272766
iteration 265, loss = 0.35394638776779175
iteration 266, loss = 0.36637207865715027
iteration 267, loss = 0.3703218698501587
iteration 268, loss = 0.33201974630355835
iteration 269, loss = 0.3390161395072937
iteration 270, loss = 0.3649219274520874
iteration 271, loss = 0.4416394829750061
iteration 272, loss = 0.4133499264717102
iteration 273, loss = 0.38540998101234436
iteration 274, loss = 0.3539789617061615
iteration 275, loss = 0.41604501008987427
iteration 276, loss = 0.3156360685825348
iteration 277, loss = 0.38543376326560974
iteration 278, loss = 0.3684770166873932
iteration 279, loss = 0.40719711780548096
iteration 280, loss = 0.3200962543487549
iteration 281, loss = 0.3787216246128082
iteration 282, loss = 0.3062341511249542
iteration 283, loss = 0.3820781707763672
iteration 284, loss = 0.42730847001075745
iteration 285, loss = 0.3861362040042877
iteration 286, loss = 0.4198383688926697
iteration 287, loss = 0.4131242334842682
iteration 288, loss = 0.3353814482688904
iteration 289, loss = 0.46446314454078674
iteration 290, loss = 0.3273281753063202
iteration 291, loss = 0.38628047704696655
iteration 292, loss = 0.34159401059150696
iteration 293, loss = 0.35350102186203003
iteration 294, loss = 0.35849788784980774
iteration 295, loss = 0.3614004850387573
iteration 296, loss = 0.3943004012107849
iteration 297, loss = 0.39252257347106934
iteration 298, loss = 0.4018731713294983
iteration 299, loss = 0.35270845890045166
iteration 300, loss = 0.31964853405952454
iteration 1, loss = 0.3894124925136566
iteration 2, loss = 0.38937145471572876
iteration 3, loss = 0.32832881808280945
iteration 4, loss = 0.3453553318977356
iteration 5, loss = 0.38479819893836975
iteration 6, loss = 0.4619350731372833
iteration 7, loss = 0.413148432970047
iteration 8, loss = 0.416914165019989
iteration 9, loss = 0.39173316955566406
iteration 10, loss = 0.36309489607810974
iteration 11, loss = 0.3700753450393677
iteration 12, loss = 0.3584771156311035
iteration 13, loss = 0.3659762740135193
iteration 14, loss = 0.44592657685279846
iteration 15, loss = 0.34996509552001953
iteration 16, loss = 0.2899738550186157
iteration 17, loss = 0.40315350890159607
iteration 18, loss = 0.41405993700027466
iteration 19, loss = 0.36880093812942505
iteration 20, loss = 0.3546997904777527
iteration 21, loss = 0.3273119628429413
iteration 22, loss = 0.40639615058898926
iteration 23, loss = 0.3956681489944458
iteration 24, loss = 0.37757742404937744
iteration 25, loss = 0.3192579448223114
iteration 26, loss = 0.3397732377052307
iteration 27, loss = 0.34440672397613525
iteration 28, loss = 0.3814629912376404
iteration 29, loss = 0.3289550542831421
iteration 30, loss = 0.376735121011734
iteration 31, loss = 0.35121095180511475
iteration 32, loss = 0.3618457019329071
iteration 33, loss = 0.3891758918762207
iteration 34, loss = 0.40998631715774536
iteration 35, loss = 0.3487321138381958
iteration 36, loss = 0.38661009073257446
iteration 37, loss = 0.3812776505947113
iteration 38, loss = 0.35665959119796753
iteration 39, loss = 0.3569927513599396
iteration 40, loss = 0.37649303674697876
iteration 41, loss = 0.3581013083457947
iteration 42, loss = 0.3806554675102234
iteration 43, loss = 0.3971264064311981
iteration 44, loss = 0.38033023476600647
iteration 45, loss = 0.2886272668838501
iteration 46, loss = 0.36384522914886475
iteration 47, loss = 0.3249533176422119
iteration 48, loss = 0.37999317049980164
iteration 49, loss = 0.33416062593460083
iteration 50, loss = 0.3613708019256592
iteration 51, loss = 0.4109269082546234
iteration 52, loss = 0.34311243891716003
iteration 53, loss = 0.32125595211982727
iteration 54, loss = 0.36517027020454407
iteration 55, loss = 0.3262874186038971
iteration 56, loss = 0.3524612486362457
iteration 57, loss = 0.31980791687965393
iteration 58, loss = 0.32237234711647034
iteration 59, loss = 0.32647252082824707
iteration 60, loss = 0.33898553252220154
iteration 61, loss = 0.33852091431617737
iteration 62, loss = 0.37886375188827515
iteration 63, loss = 0.3749297261238098
iteration 64, loss = 0.36801061034202576
iteration 65, loss = 0.3924604058265686
iteration 66, loss = 0.35607191920280457
iteration 67, loss = 0.2923792898654938
iteration 68, loss = 0.3945334851741791
iteration 69, loss = 0.3541472852230072
iteration 70, loss = 0.40765929222106934
iteration 71, loss = 0.34102141857147217
iteration 72, loss = 0.3474310040473938
iteration 73, loss = 0.33491501212120056
iteration 74, loss = 0.3471348285675049
iteration 75, loss = 0.36988359689712524
iteration 76, loss = 0.4164882004261017
iteration 77, loss = 0.3558308780193329
iteration 78, loss = 0.32964178919792175
iteration 79, loss = 0.3908512592315674
iteration 80, loss = 0.33856743574142456
iteration 81, loss = 0.3317858874797821
iteration 82, loss = 0.34687095880508423
iteration 83, loss = 0.30095356702804565
iteration 84, loss = 0.3476463556289673
iteration 85, loss = 0.3431040346622467
iteration 86, loss = 0.36503487825393677
iteration 87, loss = 0.4143645763397217
iteration 88, loss = 0.3030603528022766
iteration 89, loss = 0.35863372683525085
iteration 90, loss = 0.3642596900463104
iteration 91, loss = 0.3029535412788391
iteration 92, loss = 0.32565000653266907
iteration 93, loss = 0.33510926365852356
iteration 94, loss = 0.29862236976623535
iteration 95, loss = 0.37181219458580017
iteration 96, loss = 0.33819326758384705
iteration 97, loss = 0.39441001415252686
iteration 98, loss = 0.3671889305114746
iteration 99, loss = 0.3431629240512848
iteration 100, loss = 0.3564119040966034
iteration 101, loss = 0.35681572556495667
iteration 102, loss = 0.3112111985683441
iteration 103, loss = 0.31948164105415344
iteration 104, loss = 0.3306308090686798
iteration 105, loss = 0.33036115765571594
iteration 106, loss = 0.3287661671638489
iteration 107, loss = 0.3162185251712799
iteration 108, loss = 0.3128364086151123
iteration 109, loss = 0.34414246678352356
iteration 110, loss = 0.3568941056728363
iteration 111, loss = 0.318842351436615
iteration 112, loss = 0.3583976924419403
iteration 113, loss = 0.3414725661277771
iteration 114, loss = 0.31187212467193604
iteration 115, loss = 0.29412999749183655
iteration 116, loss = 0.3251926004886627
iteration 117, loss = 0.30481624603271484
iteration 118, loss = 0.32788267731666565
iteration 119, loss = 0.30541718006134033
iteration 120, loss = 0.3412328064441681
iteration 121, loss = 0.3379247486591339
iteration 122, loss = 0.29117241501808167
iteration 123, loss = 0.43247130513191223
iteration 124, loss = 0.3260287940502167
iteration 125, loss = 0.3131275177001953
iteration 126, loss = 0.3587469756603241
iteration 127, loss = 0.3210175931453705
iteration 128, loss = 0.2830716073513031
iteration 129, loss = 0.29441845417022705
iteration 130, loss = 0.342105507850647
iteration 131, loss = 0.3612167537212372
iteration 132, loss = 0.3233151435852051
iteration 133, loss = 0.3347286581993103
iteration 134, loss = 0.300628662109375
iteration 135, loss = 0.35120347142219543
iteration 136, loss = 0.3041899502277374
iteration 137, loss = 0.3659846782684326
iteration 138, loss = 0.3856404423713684
iteration 139, loss = 0.32902222871780396
iteration 140, loss = 0.3365369141101837
iteration 141, loss = 0.4329233467578888
iteration 142, loss = 0.3038771152496338
iteration 143, loss = 0.39549070596694946
iteration 144, loss = 0.32879605889320374
iteration 145, loss = 0.3073856830596924
iteration 146, loss = 0.3818591237068176
iteration 147, loss = 0.3130027651786804
iteration 148, loss = 0.3674538731575012
iteration 149, loss = 0.2874425947666168
iteration 150, loss = 0.3535882234573364
iteration 151, loss = 0.3161247968673706
iteration 152, loss = 0.34150609374046326
iteration 153, loss = 0.31647059321403503
iteration 154, loss = 0.31635746359825134
iteration 155, loss = 0.3323397636413574
iteration 156, loss = 0.28108832240104675
iteration 157, loss = 0.3315863609313965
iteration 158, loss = 0.34894829988479614
iteration 159, loss = 0.37637168169021606
iteration 160, loss = 0.3174877464771271
iteration 161, loss = 0.31711244583129883
iteration 162, loss = 0.3088948130607605
iteration 163, loss = 0.30667489767074585
iteration 164, loss = 0.3515845537185669
iteration 165, loss = 0.3081732988357544
iteration 166, loss = 0.2987205386161804
iteration 167, loss = 0.3494347929954529
iteration 168, loss = 0.3137836158275604
iteration 169, loss = 0.30346226692199707
iteration 170, loss = 0.35368478298187256
iteration 171, loss = 0.3390001654624939
iteration 172, loss = 0.2768102288246155
iteration 173, loss = 0.26917117834091187
iteration 174, loss = 0.349747896194458
iteration 175, loss = 0.31144431233406067
iteration 176, loss = 0.3312518000602722
iteration 177, loss = 0.3622822165489197
iteration 178, loss = 0.34634101390838623
iteration 179, loss = 0.32000234723091125
iteration 180, loss = 0.3459959924221039
iteration 181, loss = 0.34651830792427063
iteration 182, loss = 0.29159706830978394
iteration 183, loss = 0.33802902698516846
iteration 184, loss = 0.29734551906585693
iteration 185, loss = 0.3023831248283386
iteration 186, loss = 0.3473321199417114
iteration 187, loss = 0.3570851981639862
iteration 188, loss = 0.3584407567977905
iteration 189, loss = 0.3341991603374481
iteration 190, loss = 0.28729143738746643
iteration 191, loss = 0.3234810531139374
iteration 192, loss = 0.35018855333328247
iteration 193, loss = 0.3133401572704315
iteration 194, loss = 0.29412591457366943
iteration 195, loss = 0.3267103135585785
iteration 196, loss = 0.29623761773109436
iteration 197, loss = 0.29120954871177673
iteration 198, loss = 0.31834593415260315
iteration 199, loss = 0.2985580563545227
iteration 200, loss = 0.3847554624080658
iteration 201, loss = 0.30170321464538574
iteration 202, loss = 0.3574344515800476
iteration 203, loss = 0.3605427145957947
iteration 204, loss = 0.31347864866256714
iteration 205, loss = 0.33762550354003906
iteration 206, loss = 0.2962016761302948
iteration 207, loss = 0.327886164188385
iteration 208, loss = 0.317951500415802
iteration 209, loss = 0.3317692279815674
iteration 210, loss = 0.3507147431373596
iteration 211, loss = 0.2929585874080658
iteration 212, loss = 0.3133368194103241
iteration 213, loss = 0.3068721890449524
iteration 214, loss = 0.35336166620254517
iteration 215, loss = 0.32751700282096863
iteration 216, loss = 0.325933575630188
iteration 217, loss = 0.30589962005615234
iteration 218, loss = 0.29345741868019104
iteration 219, loss = 0.28843367099761963
iteration 220, loss = 0.32026419043540955
iteration 221, loss = 0.2893047332763672
iteration 222, loss = 0.32552528381347656
iteration 223, loss = 0.30025172233581543
iteration 224, loss = 0.33970358967781067
iteration 225, loss = 0.3118188977241516
iteration 226, loss = 0.2968699634075165
iteration 227, loss = 0.3618735373020172
iteration 228, loss = 0.3029509484767914
iteration 229, loss = 0.2893645167350769
iteration 230, loss = 0.27680346369743347
iteration 231, loss = 0.26952382922172546
iteration 232, loss = 0.2800694704055786
iteration 233, loss = 0.299050897359848
iteration 234, loss = 0.331160306930542
iteration 235, loss = 0.3259071409702301
iteration 236, loss = 0.3156159520149231
iteration 237, loss = 0.27235642075538635
iteration 238, loss = 0.3664517104625702
iteration 239, loss = 0.3477695882320404
iteration 240, loss = 0.31658199429512024
iteration 241, loss = 0.2733084261417389
iteration 242, loss = 0.3414028286933899
iteration 243, loss = 0.3418805003166199
iteration 244, loss = 0.3163250684738159
iteration 245, loss = 0.3455502390861511
iteration 246, loss = 0.31346920132637024
iteration 247, loss = 0.4171011447906494
iteration 248, loss = 0.3264811038970947
iteration 249, loss = 0.38888052105903625
iteration 250, loss = 0.4047742486000061
iteration 251, loss = 0.3226804733276367
iteration 252, loss = 0.312309205532074
iteration 253, loss = 0.3085523247718811
iteration 254, loss = 0.36673998832702637
iteration 255, loss = 0.3327280879020691
iteration 256, loss = 0.29549673199653625
iteration 257, loss = 0.29320982098579407
iteration 258, loss = 0.27704739570617676
iteration 259, loss = 0.30501043796539307
iteration 260, loss = 0.298907995223999
iteration 261, loss = 0.31659409403800964
iteration 262, loss = 0.3581971228122711
iteration 263, loss = 0.3384596109390259
iteration 264, loss = 0.3534902036190033
iteration 265, loss = 0.3093526065349579
iteration 266, loss = 0.34444308280944824
iteration 267, loss = 0.2896197736263275
iteration 268, loss = 0.25709375739097595
iteration 269, loss = 0.2874186038970947
iteration 270, loss = 0.33931171894073486
iteration 271, loss = 0.33883386850357056
iteration 272, loss = 0.33542194962501526
iteration 273, loss = 0.2821752727031708
iteration 274, loss = 0.3172261416912079
iteration 275, loss = 0.3473231792449951
iteration 276, loss = 0.31965669989585876
iteration 277, loss = 0.33754825592041016
iteration 278, loss = 0.3571024537086487
iteration 279, loss = 0.30893242359161377
iteration 280, loss = 0.3105815649032593
iteration 281, loss = 0.3158838748931885
iteration 282, loss = 0.2957538962364197
iteration 283, loss = 0.3110053539276123
iteration 284, loss = 0.3254077732563019
iteration 285, loss = 0.3084104359149933
iteration 286, loss = 0.3505418598651886
iteration 287, loss = 0.2946886122226715
iteration 288, loss = 0.27881425619125366
iteration 289, loss = 0.28070342540740967
iteration 290, loss = 0.30453693866729736
iteration 291, loss = 0.2738872766494751
iteration 292, loss = 0.28564634919166565
iteration 293, loss = 0.362346887588501
iteration 294, loss = 0.32522597908973694
iteration 295, loss = 0.35927554965019226
iteration 296, loss = 0.28539568185806274
iteration 297, loss = 0.3216457664966583
iteration 298, loss = 0.26339825987815857
iteration 299, loss = 0.33782273530960083
iteration 300, loss = 0.3199390470981598
iteration 1, loss = 0.29497891664505005
iteration 2, loss = 0.3337588310241699
iteration 3, loss = 0.2982291877269745
iteration 4, loss = 0.2956171929836273
iteration 5, loss = 0.3019757568836212
iteration 6, loss = 0.292089581489563
iteration 7, loss = 0.29984986782073975
iteration 8, loss = 0.36648428440093994
iteration 9, loss = 0.2874986231327057
iteration 10, loss = 0.2925194799900055
iteration 11, loss = 0.26617056131362915
iteration 12, loss = 0.32703274488449097
iteration 13, loss = 0.30791690945625305
iteration 14, loss = 0.28809159994125366
iteration 15, loss = 0.35266584157943726
iteration 16, loss = 0.29207727313041687
iteration 17, loss = 0.2656722068786621
iteration 18, loss = 0.3065856993198395
iteration 19, loss = 0.3467597961425781
iteration 20, loss = 0.2735621929168701
iteration 21, loss = 0.2726156413555145
iteration 22, loss = 0.2759784758090973
iteration 23, loss = 0.2530430555343628
iteration 24, loss = 0.3197769224643707
iteration 25, loss = 0.2610083520412445
iteration 26, loss = 0.3234562575817108
iteration 27, loss = 0.27243664860725403
iteration 28, loss = 0.35427188873291016
iteration 29, loss = 0.2788095474243164
iteration 30, loss = 0.2745929956436157
iteration 31, loss = 0.27805402874946594
iteration 32, loss = 0.2782048285007477
iteration 33, loss = 0.2822156548500061
iteration 34, loss = 0.2870531678199768
iteration 35, loss = 0.27697575092315674
iteration 36, loss = 0.2600879669189453
iteration 37, loss = 0.2954835593700409
iteration 38, loss = 0.33705243468284607
iteration 39, loss = 0.29278603196144104
iteration 40, loss = 0.3166796863079071
iteration 41, loss = 0.30284810066223145
iteration 42, loss = 0.300212562084198
iteration 43, loss = 0.26717084646224976
iteration 44, loss = 0.31256410479545593
iteration 45, loss = 0.26326271891593933
iteration 46, loss = 0.31923311948776245
iteration 47, loss = 0.3018835186958313
iteration 48, loss = 0.34880146384239197
iteration 49, loss = 0.27176058292388916
iteration 50, loss = 0.29246821999549866
iteration 51, loss = 0.2639574110507965
iteration 52, loss = 0.26881659030914307
iteration 53, loss = 0.2990947961807251
iteration 54, loss = 0.26989495754241943
iteration 55, loss = 0.3218068480491638
iteration 56, loss = 0.3049565255641937
iteration 57, loss = 0.29912230372428894
iteration 58, loss = 0.3022947609424591
iteration 59, loss = 0.353420227766037
iteration 60, loss = 0.30604755878448486
iteration 61, loss = 0.27026253938674927
iteration 62, loss = 0.29923132061958313
iteration 63, loss = 0.26545819640159607
iteration 64, loss = 0.2560034692287445
iteration 65, loss = 0.29693880677223206
iteration 66, loss = 0.26099228858947754
iteration 67, loss = 0.2801700532436371
iteration 68, loss = 0.27279287576675415
iteration 69, loss = 0.2915821671485901
iteration 70, loss = 0.2818991541862488
iteration 71, loss = 0.3347773551940918
iteration 72, loss = 0.30072206258773804
iteration 73, loss = 0.25231093168258667
iteration 74, loss = 0.33692196011543274
iteration 75, loss = 0.29541274905204773
iteration 76, loss = 0.29135721921920776
iteration 77, loss = 0.28322628140449524
iteration 78, loss = 0.283211350440979
iteration 79, loss = 0.3082508146762848
iteration 80, loss = 0.3066432476043701
iteration 81, loss = 0.2702544629573822
iteration 82, loss = 0.30075883865356445
iteration 83, loss = 0.2940168082714081
iteration 84, loss = 0.30421391129493713
iteration 85, loss = 0.29117998480796814
iteration 86, loss = 0.27419957518577576
iteration 87, loss = 0.2815609574317932
iteration 88, loss = 0.25136908888816833
iteration 89, loss = 0.30352187156677246
iteration 90, loss = 0.29496872425079346
iteration 91, loss = 0.29506367444992065
iteration 92, loss = 0.2617083191871643
iteration 93, loss = 0.32410329580307007
iteration 94, loss = 0.31438350677490234
iteration 95, loss = 0.2887747287750244
iteration 96, loss = 0.2840259075164795
iteration 97, loss = 0.28316083550453186
iteration 98, loss = 0.3134664297103882
iteration 99, loss = 0.2561399042606354
iteration 100, loss = 0.25942301750183105
iteration 101, loss = 0.31188705563545227
iteration 102, loss = 0.2507230043411255
iteration 103, loss = 0.28308144211769104
iteration 104, loss = 0.2728373408317566
iteration 105, loss = 0.30706068873405457
iteration 106, loss = 0.25896698236465454
iteration 107, loss = 0.3074369728565216
iteration 108, loss = 0.2767791748046875
iteration 109, loss = 0.2707354724407196
iteration 110, loss = 0.2740010619163513
iteration 111, loss = 0.2836343050003052
iteration 112, loss = 0.3054627776145935
iteration 113, loss = 0.2948510944843292
iteration 114, loss = 0.2717660665512085
iteration 115, loss = 0.3142576813697815
iteration 116, loss = 0.34629908204078674
iteration 117, loss = 0.2736891806125641
iteration 118, loss = 0.25500136613845825
iteration 119, loss = 0.2699374556541443
iteration 120, loss = 0.29835081100463867
iteration 121, loss = 0.2956041693687439
iteration 122, loss = 0.3148726224899292
iteration 123, loss = 0.25505638122558594
iteration 124, loss = 0.27150264382362366
iteration 125, loss = 0.2557143270969391
iteration 126, loss = 0.26096922159194946
iteration 127, loss = 0.2857331335544586
iteration 128, loss = 0.2850366234779358
iteration 129, loss = 0.25381264090538025
iteration 130, loss = 0.2709309756755829
iteration 131, loss = 0.3149453103542328
iteration 132, loss = 0.27895811200141907
iteration 133, loss = 0.23596414923667908
iteration 134, loss = 0.27221399545669556
iteration 135, loss = 0.2723061442375183
iteration 136, loss = 0.2800365388393402
iteration 137, loss = 0.3063236176967621
iteration 138, loss = 0.29328733682632446
iteration 139, loss = 0.32964596152305603
iteration 140, loss = 0.2661573588848114
iteration 141, loss = 0.3246792256832123
iteration 142, loss = 0.30596044659614563
iteration 143, loss = 0.27447497844696045
iteration 144, loss = 0.2469715178012848
iteration 145, loss = 0.25779643654823303
iteration 146, loss = 0.26915088295936584
iteration 147, loss = 0.2728387713432312
iteration 148, loss = 0.2748831808567047
iteration 149, loss = 0.2668471038341522
iteration 150, loss = 0.2659849524497986
iteration 151, loss = 0.2570786774158478
iteration 152, loss = 0.2613111734390259
iteration 153, loss = 0.2786773443222046
iteration 154, loss = 0.24956469237804413
iteration 155, loss = 0.27744367718696594
iteration 156, loss = 0.3198910653591156
iteration 157, loss = 0.29333406686782837
iteration 158, loss = 0.26698562502861023
iteration 159, loss = 0.2907133996486664
iteration 160, loss = 0.3284478187561035
iteration 161, loss = 0.2554461658000946
iteration 162, loss = 0.2839256227016449
iteration 163, loss = 0.2583829164505005
iteration 164, loss = 0.2716890573501587
iteration 165, loss = 0.253411203622818
iteration 166, loss = 0.27381622791290283
iteration 167, loss = 0.2745882570743561
iteration 168, loss = 0.2742779552936554
iteration 169, loss = 0.3244684934616089
iteration 170, loss = 0.2671648859977722
iteration 171, loss = 0.27539363503456116
iteration 172, loss = 0.2524139881134033
iteration 173, loss = 0.27613168954849243
iteration 174, loss = 0.24955570697784424
iteration 175, loss = 0.27581119537353516
iteration 176, loss = 0.2932060956954956
iteration 177, loss = 0.2626376450061798
iteration 178, loss = 0.2376212775707245
iteration 179, loss = 0.28501152992248535
iteration 180, loss = 0.24845482409000397
iteration 181, loss = 0.24620702862739563
iteration 182, loss = 0.2783015966415405
iteration 183, loss = 0.2362860143184662
iteration 184, loss = 0.2692279517650604
iteration 185, loss = 0.2628675699234009
iteration 186, loss = 0.2726970911026001
iteration 187, loss = 0.26696765422821045
iteration 188, loss = 0.24815616011619568
iteration 189, loss = 0.27288633584976196
iteration 190, loss = 0.24299311637878418
iteration 191, loss = 0.31566041707992554
iteration 192, loss = 0.2461027354001999
iteration 193, loss = 0.2613975703716278
iteration 194, loss = 0.2736945152282715
iteration 195, loss = 0.2211369276046753
iteration 196, loss = 0.31094181537628174
iteration 197, loss = 0.24391597509384155
iteration 198, loss = 0.24220781028270721
iteration 199, loss = 0.30107298493385315
iteration 200, loss = 0.2728498578071594
iteration 201, loss = 0.3006898760795593
iteration 202, loss = 0.24519391357898712
iteration 203, loss = 0.2500860095024109
iteration 204, loss = 0.24642574787139893
iteration 205, loss = 0.317333847284317
iteration 206, loss = 0.27601268887519836
iteration 207, loss = 0.2632405161857605
iteration 208, loss = 0.24831928312778473
iteration 209, loss = 0.23208534717559814
iteration 210, loss = 0.2582100033760071
iteration 211, loss = 0.24111336469650269
iteration 212, loss = 0.24888041615486145
iteration 213, loss = 0.22534288465976715
iteration 214, loss = 0.2813569903373718
iteration 215, loss = 0.2446335405111313
iteration 216, loss = 0.2824352979660034
iteration 217, loss = 0.2505151033401489
iteration 218, loss = 0.27760523557662964
iteration 219, loss = 0.24263475835323334
iteration 220, loss = 0.25754067301750183
iteration 221, loss = 0.28010183572769165
iteration 222, loss = 0.25264662504196167
iteration 223, loss = 0.2504878640174866
iteration 224, loss = 0.24553409218788147
iteration 225, loss = 0.25776922702789307
iteration 226, loss = 0.23424938321113586
iteration 227, loss = 0.24067120254039764
iteration 228, loss = 0.25645631551742554
iteration 229, loss = 0.24423085153102875
iteration 230, loss = 0.24332579970359802
iteration 231, loss = 0.23754122853279114
iteration 232, loss = 0.2904345691204071
iteration 233, loss = 0.25343772768974304
iteration 234, loss = 0.2712984085083008
iteration 235, loss = 0.25607767701148987
iteration 236, loss = 0.2543971538543701
iteration 237, loss = 0.2545889616012573
iteration 238, loss = 0.24997127056121826
iteration 239, loss = 0.2627265453338623
iteration 240, loss = 0.2490265816450119
iteration 241, loss = 0.2646605968475342
iteration 242, loss = 0.2558506429195404
iteration 243, loss = 0.2418259233236313
iteration 244, loss = 0.24886639416217804
iteration 245, loss = 0.2737465798854828
iteration 246, loss = 0.25178107619285583
iteration 247, loss = 0.26472365856170654
iteration 248, loss = 0.25256985425949097
iteration 249, loss = 0.25323551893234253
iteration 250, loss = 0.22515808045864105
iteration 251, loss = 0.24228300154209137
iteration 252, loss = 0.24005533754825592
iteration 253, loss = 0.2581345736980438
iteration 254, loss = 0.23788541555404663
iteration 255, loss = 0.2568915784358978
iteration 256, loss = 0.2359059751033783
iteration 257, loss = 0.2573845386505127
iteration 258, loss = 0.2800888121128082
iteration 259, loss = 0.24415241181850433
iteration 260, loss = 0.2762469947338104
iteration 261, loss = 0.2362017035484314
iteration 262, loss = 0.23715128004550934
iteration 263, loss = 0.23848983645439148
iteration 264, loss = 0.2737902104854584
iteration 265, loss = 0.24603669345378876
iteration 266, loss = 0.2641288936138153
iteration 267, loss = 0.279091477394104
iteration 268, loss = 0.2665603458881378
iteration 269, loss = 0.25026363134384155
iteration 270, loss = 0.2559862732887268
iteration 271, loss = 0.2605913281440735
iteration 272, loss = 0.24191465973854065
iteration 273, loss = 0.29594314098358154
iteration 274, loss = 0.2547328472137451
iteration 275, loss = 0.23571927845478058
iteration 276, loss = 0.2161179929971695
iteration 277, loss = 0.21968933939933777
iteration 278, loss = 0.23984354734420776
iteration 279, loss = 0.21236877143383026
iteration 280, loss = 0.23820970952510834
iteration 281, loss = 0.22539015114307404
iteration 282, loss = 0.22255854308605194
iteration 283, loss = 0.28904473781585693
iteration 284, loss = 0.2891640067100525
iteration 285, loss = 0.24619640409946442
iteration 286, loss = 0.21445950865745544
iteration 287, loss = 0.24344193935394287
iteration 288, loss = 0.22598473727703094
iteration 289, loss = 0.22755613923072815
iteration 290, loss = 0.2451743185520172
iteration 291, loss = 0.25114819407463074
iteration 292, loss = 0.21130576729774475
iteration 293, loss = 0.2417343705892563
iteration 294, loss = 0.26091399788856506
iteration 295, loss = 0.2582170367240906
iteration 296, loss = 0.21471917629241943
iteration 297, loss = 0.2748960554599762
iteration 298, loss = 0.2522420585155487
iteration 299, loss = 0.26663631200790405
iteration 300, loss = 0.23848716914653778
iteration 1, loss = 0.27805668115615845
iteration 2, loss = 0.2714082896709442
iteration 3, loss = 0.259991854429245
iteration 4, loss = 0.23020781576633453
iteration 5, loss = 0.24054259061813354
iteration 6, loss = 0.2522468566894531
iteration 7, loss = 0.23360475897789001
iteration 8, loss = 0.22159624099731445
iteration 9, loss = 0.2584024667739868
iteration 10, loss = 0.2386220097541809
iteration 11, loss = 0.25449109077453613
iteration 12, loss = 0.2279175966978073
iteration 13, loss = 0.22697508335113525
iteration 14, loss = 0.22983747720718384
iteration 15, loss = 0.24192290008068085
iteration 16, loss = 0.2627439796924591
iteration 17, loss = 0.22089582681655884
iteration 18, loss = 0.23595988750457764
iteration 19, loss = 0.24771687388420105
iteration 20, loss = 0.2364984005689621
iteration 21, loss = 0.21468690037727356
iteration 22, loss = 0.2429996281862259
iteration 23, loss = 0.21498408913612366
iteration 24, loss = 0.27288949489593506
iteration 25, loss = 0.21952050924301147
iteration 26, loss = 0.24287106096744537
iteration 27, loss = 0.23154284060001373
iteration 28, loss = 0.2266872227191925
iteration 29, loss = 0.23005583882331848
iteration 30, loss = 0.29065290093421936
iteration 31, loss = 0.23718778789043427
iteration 32, loss = 0.22806960344314575
iteration 33, loss = 0.21585245430469513
iteration 34, loss = 0.19720591604709625
iteration 35, loss = 0.21958519518375397
iteration 36, loss = 0.2991091310977936
iteration 37, loss = 0.2325184941291809
iteration 38, loss = 0.2206050604581833
iteration 39, loss = 0.250052809715271
iteration 40, loss = 0.24566513299942017
iteration 41, loss = 0.23008409142494202
iteration 42, loss = 0.23169687390327454
iteration 43, loss = 0.25065338611602783
iteration 44, loss = 0.23051661252975464
iteration 45, loss = 0.23021918535232544
iteration 46, loss = 0.23677439987659454
iteration 47, loss = 0.2643967866897583
iteration 48, loss = 0.26338428258895874
iteration 49, loss = 0.220893993973732
iteration 50, loss = 0.2319639027118683
iteration 51, loss = 0.22039398550987244
iteration 52, loss = 0.21456556022167206
iteration 53, loss = 0.25285354256629944
iteration 54, loss = 0.22687368094921112
iteration 55, loss = 0.2306240350008011
iteration 56, loss = 0.2191283404827118
iteration 57, loss = 0.25254273414611816
iteration 58, loss = 0.2107011079788208
iteration 59, loss = 0.2525092661380768
iteration 60, loss = 0.24738451838493347
iteration 61, loss = 0.21397417783737183
iteration 62, loss = 0.24130059778690338
iteration 63, loss = 0.2147926688194275
iteration 64, loss = 0.2147974669933319
iteration 65, loss = 0.22849754989147186
iteration 66, loss = 0.21797136962413788
iteration 67, loss = 0.27784472703933716
iteration 68, loss = 0.20617613196372986
iteration 69, loss = 0.237993523478508
iteration 70, loss = 0.2488481104373932
iteration 71, loss = 0.23722437024116516
iteration 72, loss = 0.24016810953617096
iteration 73, loss = 0.26648420095443726
iteration 74, loss = 0.2810892164707184
iteration 75, loss = 0.2391560971736908
iteration 76, loss = 0.20786575973033905
iteration 77, loss = 0.23217841982841492
iteration 78, loss = 0.22903193533420563
iteration 79, loss = 0.23304101824760437
iteration 80, loss = 0.23505936563014984
iteration 81, loss = 0.2424108237028122
iteration 82, loss = 0.22119595110416412
iteration 83, loss = 0.24485240876674652
iteration 84, loss = 0.224777489900589
iteration 85, loss = 0.22489839792251587
iteration 86, loss = 0.22312483191490173
iteration 87, loss = 0.2185065895318985
iteration 88, loss = 0.22469501197338104
iteration 89, loss = 0.23896048963069916
iteration 90, loss = 0.2246333658695221
iteration 91, loss = 0.23392421007156372
iteration 92, loss = 0.20125964283943176
iteration 93, loss = 0.24271592497825623
iteration 94, loss = 0.25075021386146545
iteration 95, loss = 0.25122538208961487
iteration 96, loss = 0.21971116960048676
iteration 97, loss = 0.21710512042045593
iteration 98, loss = 0.22585636377334595
iteration 99, loss = 0.23279926180839539
iteration 100, loss = 0.23699580132961273
iteration 101, loss = 0.23671215772628784
iteration 102, loss = 0.2055133432149887
iteration 103, loss = 0.2459382563829422
iteration 104, loss = 0.20820803940296173
iteration 105, loss = 0.1935259848833084
iteration 106, loss = 0.21333342790603638
iteration 107, loss = 0.2174641340970993
iteration 108, loss = 0.25459927320480347
iteration 109, loss = 0.2394590526819229
iteration 110, loss = 0.2037908136844635
iteration 111, loss = 0.20464259386062622
iteration 112, loss = 0.21011239290237427
iteration 113, loss = 0.21597176790237427
iteration 114, loss = 0.22237031161785126
iteration 115, loss = 0.18350708484649658
iteration 116, loss = 0.24982210993766785
iteration 117, loss = 0.21451401710510254
iteration 118, loss = 0.21338310837745667
iteration 119, loss = 0.23025652766227722
iteration 120, loss = 0.22534380853176117
iteration 121, loss = 0.2264469563961029
iteration 122, loss = 0.2366095930337906
iteration 123, loss = 0.20415692031383514
iteration 124, loss = 0.21988947689533234
iteration 125, loss = 0.20552216470241547
iteration 126, loss = 0.20051071047782898
iteration 127, loss = 0.2488504946231842
iteration 128, loss = 0.2594922184944153
iteration 129, loss = 0.22788652777671814
iteration 130, loss = 0.21306923031806946
iteration 131, loss = 0.22872892022132874
iteration 132, loss = 0.20358997583389282
iteration 133, loss = 0.2282906174659729
iteration 134, loss = 0.208982914686203
iteration 135, loss = 0.2045891135931015
iteration 136, loss = 0.22151657938957214
iteration 137, loss = 0.22531461715698242
iteration 138, loss = 0.21019935607910156
iteration 139, loss = 0.2040027230978012
iteration 140, loss = 0.21938200294971466
iteration 141, loss = 0.2167818546295166
iteration 142, loss = 0.20993879437446594
iteration 143, loss = 0.232169046998024
iteration 144, loss = 0.22039321064949036
iteration 145, loss = 0.20205052196979523
iteration 146, loss = 0.20956778526306152
iteration 147, loss = 0.22415439784526825
iteration 148, loss = 0.28874486684799194
iteration 149, loss = 0.21821996569633484
iteration 150, loss = 0.21325063705444336
iteration 151, loss = 0.21599557995796204
iteration 152, loss = 0.2639855146408081
iteration 153, loss = 0.19480225443840027
iteration 154, loss = 0.21186117827892303
iteration 155, loss = 0.22299093008041382
iteration 156, loss = 0.24435122311115265
iteration 157, loss = 0.21088621020317078
iteration 158, loss = 0.2029128074645996
iteration 159, loss = 0.18550105392932892
iteration 160, loss = 0.1937464475631714
iteration 161, loss = 0.21714746952056885
iteration 162, loss = 0.17675834894180298
iteration 163, loss = 0.20179639756679535
iteration 164, loss = 0.2080034464597702
iteration 165, loss = 0.259034663438797
iteration 166, loss = 0.18908388912677765
iteration 167, loss = 0.226027250289917
iteration 168, loss = 0.1933049112558365
iteration 169, loss = 0.18405744433403015
iteration 170, loss = 0.2035740613937378
iteration 171, loss = 0.2025887370109558
iteration 172, loss = 0.23789754509925842
iteration 173, loss = 0.20853079855442047
iteration 174, loss = 0.193055659532547
iteration 175, loss = 0.20899607241153717
iteration 176, loss = 0.23676621913909912
iteration 177, loss = 0.2223224639892578
iteration 178, loss = 0.19989562034606934
iteration 179, loss = 0.22960910201072693
iteration 180, loss = 0.22192183136940002
iteration 181, loss = 0.19793014228343964
iteration 182, loss = 0.2162380963563919
iteration 183, loss = 0.23547084629535675
iteration 184, loss = 0.2159484475851059
iteration 185, loss = 0.19566601514816284
iteration 186, loss = 0.19521348178386688
iteration 187, loss = 0.24427247047424316
iteration 188, loss = 0.2025558352470398
iteration 189, loss = 0.18770140409469604
iteration 190, loss = 0.2021833062171936
iteration 191, loss = 0.2041109949350357
iteration 192, loss = 0.19578878581523895
iteration 193, loss = 0.18161509931087494
iteration 194, loss = 0.22341516613960266
iteration 195, loss = 0.18746569752693176
iteration 196, loss = 0.2060752511024475
iteration 197, loss = 0.21600031852722168
iteration 198, loss = 0.26355716586112976
iteration 199, loss = 0.19795449078083038
iteration 200, loss = 0.24452270567417145
iteration 201, loss = 0.16899850964546204
iteration 202, loss = 0.19316993653774261
iteration 203, loss = 0.20866741240024567
iteration 204, loss = 0.1709546595811844
iteration 205, loss = 0.22207070887088776
iteration 206, loss = 0.17103734612464905
iteration 207, loss = 0.17146116495132446
iteration 208, loss = 0.22026044130325317
iteration 209, loss = 0.19910678267478943
iteration 210, loss = 0.19170603156089783
iteration 211, loss = 0.17728866636753082
iteration 212, loss = 0.19605495035648346
iteration 213, loss = 0.2287539541721344
iteration 214, loss = 0.21336352825164795
iteration 215, loss = 0.19750532507896423
iteration 216, loss = 0.19851788878440857
iteration 217, loss = 0.21162591874599457
iteration 218, loss = 0.20304661989212036
iteration 219, loss = 0.2149033546447754
iteration 220, loss = 0.21221566200256348
iteration 221, loss = 0.1709917187690735
iteration 222, loss = 0.1977771371603012
iteration 223, loss = 0.2037704437971115
iteration 224, loss = 0.22232216596603394
iteration 225, loss = 0.2007925808429718
iteration 226, loss = 0.2253047227859497
iteration 227, loss = 0.2232331931591034
iteration 228, loss = 0.22628796100616455
iteration 229, loss = 0.2448752373456955
iteration 230, loss = 0.2098310887813568
iteration 231, loss = 0.20136582851409912
iteration 232, loss = 0.18104660511016846
iteration 233, loss = 0.23162609338760376
iteration 234, loss = 0.17683514952659607
iteration 235, loss = 0.19877594709396362
iteration 236, loss = 0.23624269664287567
iteration 237, loss = 0.19470058381557465
iteration 238, loss = 0.2033623456954956
iteration 239, loss = 0.19882410764694214
iteration 240, loss = 0.20139066874980927
iteration 241, loss = 0.19468942284584045
iteration 242, loss = 0.16330215334892273
iteration 243, loss = 0.18845310807228088
iteration 244, loss = 0.20997868478298187
iteration 245, loss = 0.15813800692558289
iteration 246, loss = 0.22079214453697205
iteration 247, loss = 0.17143385112285614
iteration 248, loss = 0.18873140215873718
iteration 249, loss = 0.1947675347328186
iteration 250, loss = 0.21718421578407288
iteration 251, loss = 0.1922190636396408
iteration 252, loss = 0.18619072437286377
iteration 253, loss = 0.19595539569854736
iteration 254, loss = 0.18311604857444763
iteration 255, loss = 0.21815060079097748
iteration 256, loss = 0.18720339238643646
iteration 257, loss = 0.20563384890556335
iteration 258, loss = 0.1775766909122467
iteration 259, loss = 0.19804102182388306
iteration 260, loss = 0.1953020542860031
iteration 261, loss = 0.21096651256084442
iteration 262, loss = 0.19562195241451263
iteration 263, loss = 0.2592337727546692
iteration 264, loss = 0.2360227257013321
iteration 265, loss = 0.20688799023628235
iteration 266, loss = 0.2165931761264801
iteration 267, loss = 0.1966746300458908
iteration 268, loss = 0.17137809097766876
iteration 269, loss = 0.18112674355506897
iteration 270, loss = 0.20595774054527283
iteration 271, loss = 0.17748971283435822
iteration 272, loss = 0.1811486929655075
iteration 273, loss = 0.19884569942951202
iteration 274, loss = 0.1864582598209381
iteration 275, loss = 0.16081146895885468
iteration 276, loss = 0.20570193231105804
iteration 277, loss = 0.17930085957050323
iteration 278, loss = 0.259848952293396
iteration 279, loss = 0.20472343266010284
iteration 280, loss = 0.19216623902320862
iteration 281, loss = 0.17856858670711517
iteration 282, loss = 0.21358107030391693
iteration 283, loss = 0.2380988746881485
iteration 284, loss = 0.18821066617965698
iteration 285, loss = 0.18125347793102264
iteration 286, loss = 0.22514180839061737
iteration 287, loss = 0.21581251919269562
iteration 288, loss = 0.17272885143756866
iteration 289, loss = 0.17575110495090485
iteration 290, loss = 0.19201569259166718
iteration 291, loss = 0.2088853269815445
iteration 292, loss = 0.23084329068660736
iteration 293, loss = 0.17287079989910126
iteration 294, loss = 0.17767083644866943
iteration 295, loss = 0.18129856884479523
iteration 296, loss = 0.17442382872104645
iteration 297, loss = 0.1964494287967682
iteration 298, loss = 0.19389092922210693
iteration 299, loss = 0.16400887072086334
iteration 300, loss = 0.1871260702610016
iteration 1, loss = 0.18803717195987701
iteration 2, loss = 0.22722932696342468
iteration 3, loss = 0.1924441009759903
iteration 4, loss = 0.1406935155391693
iteration 5, loss = 0.18459901213645935
iteration 6, loss = 0.16846652328968048
iteration 7, loss = 0.16150224208831787
iteration 8, loss = 0.22766536474227905
iteration 9, loss = 0.1950787901878357
iteration 10, loss = 0.2008265256881714
iteration 11, loss = 0.1977936327457428
iteration 12, loss = 0.2220013290643692
iteration 13, loss = 0.15698817372322083
iteration 14, loss = 0.18290524184703827
iteration 15, loss = 0.15883122384548187
iteration 16, loss = 0.20437106490135193
iteration 17, loss = 0.18808294832706451
iteration 18, loss = 0.2031271904706955
iteration 19, loss = 0.19432592391967773
iteration 20, loss = 0.1814366728067398
iteration 21, loss = 0.18258720636367798
iteration 22, loss = 0.16593843698501587
iteration 23, loss = 0.19675259292125702
iteration 24, loss = 0.17537233233451843
iteration 25, loss = 0.20967160165309906
iteration 26, loss = 0.17148320376873016
iteration 27, loss = 0.17710919678211212
iteration 28, loss = 0.17071101069450378
iteration 29, loss = 0.19361548125743866
iteration 30, loss = 0.18134301900863647
iteration 31, loss = 0.18708759546279907
iteration 32, loss = 0.18723194301128387
iteration 33, loss = 0.17039036750793457
iteration 34, loss = 0.16015800833702087
iteration 35, loss = 0.19033172726631165
iteration 36, loss = 0.1779695600271225
iteration 37, loss = 0.19129562377929688
iteration 38, loss = 0.19739320874214172
iteration 39, loss = 0.17913153767585754
iteration 40, loss = 0.19876408576965332
iteration 41, loss = 0.17226751148700714
iteration 42, loss = 0.19920702278614044
iteration 43, loss = 0.22823283076286316
iteration 44, loss = 0.19061161577701569
iteration 45, loss = 0.16267487406730652
iteration 46, loss = 0.16295483708381653
iteration 47, loss = 0.15169039368629456
iteration 48, loss = 0.17392095923423767
iteration 49, loss = 0.19671925902366638
iteration 50, loss = 0.20819967985153198
iteration 51, loss = 0.18300983309745789
iteration 52, loss = 0.15157511830329895
iteration 53, loss = 0.1826745569705963
iteration 54, loss = 0.15953123569488525
iteration 55, loss = 0.18196788430213928
iteration 56, loss = 0.18988992273807526
iteration 57, loss = 0.12930983304977417
iteration 58, loss = 0.15800347924232483
iteration 59, loss = 0.19940543174743652
iteration 60, loss = 0.1841987818479538
iteration 61, loss = 0.26168838143348694
iteration 62, loss = 0.15486927330493927
iteration 63, loss = 0.1478257030248642
iteration 64, loss = 0.19420072436332703
iteration 65, loss = 0.18280047178268433
iteration 66, loss = 0.18895624577999115
iteration 67, loss = 0.16940808296203613
iteration 68, loss = 0.18402883410453796
iteration 69, loss = 0.1569945067167282
iteration 70, loss = 0.20587028563022614
iteration 71, loss = 0.16506914794445038
iteration 72, loss = 0.18292587995529175
iteration 73, loss = 0.16555608808994293
iteration 74, loss = 0.14574113488197327
iteration 75, loss = 0.16601888835430145
iteration 76, loss = 0.15565897524356842
iteration 77, loss = 0.17346321046352386
iteration 78, loss = 0.2165725976228714
iteration 79, loss = 0.1749543845653534
iteration 80, loss = 0.19278232753276825
iteration 81, loss = 0.15722455084323883
iteration 82, loss = 0.15386730432510376
iteration 83, loss = 0.1834356188774109
iteration 84, loss = 0.1569744199514389
iteration 85, loss = 0.16548383235931396
iteration 86, loss = 0.19819992780685425
iteration 87, loss = 0.17807868123054504
iteration 88, loss = 0.2248217910528183
iteration 89, loss = 0.20063965022563934
iteration 90, loss = 0.152325838804245
iteration 91, loss = 0.17964620888233185
iteration 92, loss = 0.19696691632270813
iteration 93, loss = 0.1686972975730896
iteration 94, loss = 0.1481739580631256
iteration 95, loss = 0.17128920555114746
iteration 96, loss = 0.14462517201900482
iteration 97, loss = 0.15961682796478271
iteration 98, loss = 0.1884467452764511
iteration 99, loss = 0.18159614503383636
iteration 100, loss = 0.17198123037815094
iteration 101, loss = 0.16547805070877075
iteration 102, loss = 0.16293729841709137
iteration 103, loss = 0.16355787217617035
iteration 104, loss = 0.15473169088363647
iteration 105, loss = 0.18324923515319824
iteration 106, loss = 0.16135869920253754
iteration 107, loss = 0.15556803345680237
iteration 108, loss = 0.1597565859556198
iteration 109, loss = 0.18219099938869476
iteration 110, loss = 0.19390195608139038
iteration 111, loss = 0.20247310400009155
iteration 112, loss = 0.1596245914697647
iteration 113, loss = 0.1574578583240509
iteration 114, loss = 0.13727664947509766
iteration 115, loss = 0.14873282611370087
iteration 116, loss = 0.13829250633716583
iteration 117, loss = 0.167636439204216
iteration 118, loss = 0.14789791405200958
iteration 119, loss = 0.1791391670703888
iteration 120, loss = 0.18698671460151672
iteration 121, loss = 0.16674548387527466
iteration 122, loss = 0.17625345289707184
iteration 123, loss = 0.1705407351255417
iteration 124, loss = 0.17636795341968536
iteration 125, loss = 0.14616316556930542
iteration 126, loss = 0.1728186458349228
iteration 127, loss = 0.1531686782836914
iteration 128, loss = 0.17981581389904022
iteration 129, loss = 0.1546919345855713
iteration 130, loss = 0.13862061500549316
iteration 131, loss = 0.163496196269989
iteration 132, loss = 0.16338329017162323
iteration 133, loss = 0.2220425307750702
iteration 134, loss = 0.15827029943466187
iteration 135, loss = 0.17731116712093353
iteration 136, loss = 0.1613306850194931
iteration 137, loss = 0.16319003701210022
iteration 138, loss = 0.16836147010326385
iteration 139, loss = 0.1476050764322281
iteration 140, loss = 0.16300024092197418
iteration 141, loss = 0.23932403326034546
iteration 142, loss = 0.1667780578136444
iteration 143, loss = 0.17506664991378784
iteration 144, loss = 0.18169179558753967
iteration 145, loss = 0.17410267889499664
iteration 146, loss = 0.1641760766506195
iteration 147, loss = 0.1519566774368286
iteration 148, loss = 0.17176184058189392
iteration 149, loss = 0.18566162884235382
iteration 150, loss = 0.15932561457157135
iteration 151, loss = 0.16116963326931
iteration 152, loss = 0.13659124076366425
iteration 153, loss = 0.16373923420906067
iteration 154, loss = 0.1710188090801239
iteration 155, loss = 0.1587361842393875
iteration 156, loss = 0.16548490524291992
iteration 157, loss = 0.13121463358402252
iteration 158, loss = 0.16031929850578308
iteration 159, loss = 0.1477447897195816
iteration 160, loss = 0.16184833645820618
iteration 161, loss = 0.16819092631340027
iteration 162, loss = 0.13626211881637573
iteration 163, loss = 0.14429549872875214
iteration 164, loss = 0.15391062200069427
iteration 165, loss = 0.17150737345218658
iteration 166, loss = 0.18604202568531036
iteration 167, loss = 0.16768032312393188
iteration 168, loss = 0.13651670515537262
iteration 169, loss = 0.2025751769542694
iteration 170, loss = 0.17763817310333252
iteration 171, loss = 0.17283950746059418
iteration 172, loss = 0.2305508553981781
iteration 173, loss = 0.177952378988266
iteration 174, loss = 0.21926116943359375
iteration 175, loss = 0.1593305617570877
iteration 176, loss = 0.1458187848329544
iteration 177, loss = 0.15922847390174866
iteration 178, loss = 0.16295413672924042
iteration 179, loss = 0.18357406556606293
iteration 180, loss = 0.12945523858070374
iteration 181, loss = 0.1830294132232666
iteration 182, loss = 0.16651052236557007
iteration 183, loss = 0.16945011913776398
iteration 184, loss = 0.14464670419692993
iteration 185, loss = 0.1849040389060974
iteration 186, loss = 0.14609745144844055
iteration 187, loss = 0.1882539987564087
iteration 188, loss = 0.16198423504829407
iteration 189, loss = 0.15537786483764648
iteration 190, loss = 0.1623258739709854
iteration 191, loss = 0.16359516978263855
iteration 192, loss = 0.17197638750076294
iteration 193, loss = 0.1573193520307541
iteration 194, loss = 0.1514485627412796
iteration 195, loss = 0.14912502467632294
iteration 196, loss = 0.1766805797815323
iteration 197, loss = 0.1533626765012741
iteration 198, loss = 0.158238485455513
iteration 199, loss = 0.16943514347076416
iteration 200, loss = 0.15628302097320557
iteration 201, loss = 0.17421849071979523
iteration 202, loss = 0.12108147144317627
iteration 203, loss = 0.1776261329650879
iteration 204, loss = 0.16062700748443604
iteration 205, loss = 0.1523185670375824
iteration 206, loss = 0.1624060422182083
iteration 207, loss = 0.16081318259239197
iteration 208, loss = 0.16916108131408691
iteration 209, loss = 0.12240367382764816
iteration 210, loss = 0.19210034608840942
iteration 211, loss = 0.18221165239810944
iteration 212, loss = 0.11551286280155182
iteration 213, loss = 0.16917282342910767
iteration 214, loss = 0.14012649655342102
iteration 215, loss = 0.12999774515628815
iteration 216, loss = 0.13843181729316711
iteration 217, loss = 0.16507460176944733
iteration 218, loss = 0.14799822866916656
iteration 219, loss = 0.15145178139209747
iteration 220, loss = 0.15050636231899261
iteration 221, loss = 0.13495463132858276
iteration 222, loss = 0.13602733612060547
iteration 223, loss = 0.15160122513771057
iteration 224, loss = 0.1373206079006195
iteration 225, loss = 0.14831431210041046
iteration 226, loss = 0.1432563215494156
iteration 227, loss = 0.14896553754806519
iteration 228, loss = 0.1255016028881073
iteration 229, loss = 0.15553849935531616
iteration 230, loss = 0.1504809558391571
iteration 231, loss = 0.16363593935966492
iteration 232, loss = 0.14768004417419434
iteration 233, loss = 0.16285625100135803
iteration 234, loss = 0.17070642113685608
iteration 235, loss = 0.16559165716171265
iteration 236, loss = 0.17479315400123596
iteration 237, loss = 0.1618567258119583
iteration 238, loss = 0.13391001522541046
iteration 239, loss = 0.20237869024276733
iteration 240, loss = 0.16980458796024323
iteration 241, loss = 0.16777177155017853
iteration 242, loss = 0.1450064480304718
iteration 243, loss = 0.12498334050178528
iteration 244, loss = 0.14393842220306396
iteration 245, loss = 0.16609953343868256
iteration 246, loss = 0.13442376255989075
iteration 247, loss = 0.150089830160141
iteration 248, loss = 0.16467437148094177
iteration 249, loss = 0.11441953480243683
iteration 250, loss = 0.14440834522247314
iteration 251, loss = 0.19183394312858582
iteration 252, loss = 0.14647312462329865
iteration 253, loss = 0.1462826430797577
iteration 254, loss = 0.14827528595924377
iteration 255, loss = 0.17767126858234406
iteration 256, loss = 0.1689194291830063
iteration 257, loss = 0.17419302463531494
iteration 258, loss = 0.1315663754940033
iteration 259, loss = 0.15535607933998108
iteration 260, loss = 0.14784997701644897
iteration 261, loss = 0.14969751238822937
iteration 262, loss = 0.15238294005393982
iteration 263, loss = 0.14383907616138458
iteration 264, loss = 0.14954452216625214
iteration 265, loss = 0.16787025332450867
iteration 266, loss = 0.1471138596534729
iteration 267, loss = 0.14405080676078796
iteration 268, loss = 0.13231724500656128
iteration 269, loss = 0.11210764944553375
iteration 270, loss = 0.15864846110343933
iteration 271, loss = 0.178888201713562
iteration 272, loss = 0.14325085282325745
iteration 273, loss = 0.13995355367660522
iteration 274, loss = 0.12517501413822174
iteration 275, loss = 0.14212840795516968
iteration 276, loss = 0.1349891573190689
iteration 277, loss = 0.13491378724575043
iteration 278, loss = 0.15650728344917297
iteration 279, loss = 0.18107381463050842
iteration 280, loss = 0.1319868564605713
iteration 281, loss = 0.14807972311973572
iteration 282, loss = 0.12541833519935608
iteration 283, loss = 0.10954534262418747
iteration 284, loss = 0.11508964747190475
iteration 285, loss = 0.18119904398918152
iteration 286, loss = 0.11853770911693573
iteration 287, loss = 0.11070272326469421
iteration 288, loss = 0.14584527909755707
iteration 289, loss = 0.15371575951576233
iteration 290, loss = 0.16249343752861023
iteration 291, loss = 0.1807275265455246
iteration 292, loss = 0.1455167829990387
iteration 293, loss = 0.13409292697906494
iteration 294, loss = 0.16436314582824707
iteration 295, loss = 0.17685924470424652
iteration 296, loss = 0.16845980286598206
iteration 297, loss = 0.14761367440223694
iteration 298, loss = 0.14417420327663422
iteration 299, loss = 0.15272632241249084
iteration 300, loss = 0.16203325986862183
iteration 1, loss = 0.14080245792865753
iteration 2, loss = 0.15433178842067719
iteration 3, loss = 0.12363927066326141
iteration 4, loss = 0.2033763825893402
iteration 5, loss = 0.14299045503139496
iteration 6, loss = 0.126497283577919
iteration 7, loss = 0.13943853974342346
iteration 8, loss = 0.12341383099555969
iteration 9, loss = 0.11619574576616287
iteration 10, loss = 0.14972974359989166
iteration 11, loss = 0.14671917259693146
iteration 12, loss = 0.09779056161642075
iteration 13, loss = 0.1646876484155655
iteration 14, loss = 0.15494808554649353
iteration 15, loss = 0.1325361132621765
iteration 16, loss = 0.13715049624443054
iteration 17, loss = 0.1577090620994568
iteration 18, loss = 0.151201531291008
iteration 19, loss = 0.13015979528427124
iteration 20, loss = 0.13479669392108917
iteration 21, loss = 0.13113993406295776
iteration 22, loss = 0.13895028829574585
iteration 23, loss = 0.12707819044589996
iteration 24, loss = 0.16306209564208984
iteration 25, loss = 0.134286031126976
iteration 26, loss = 0.15293818712234497
iteration 27, loss = 0.10962355136871338
iteration 28, loss = 0.1331770122051239
iteration 29, loss = 0.1536363959312439
iteration 30, loss = 0.15047329664230347
iteration 31, loss = 0.13611000776290894
iteration 32, loss = 0.1081516370177269
iteration 33, loss = 0.14957383275032043
iteration 34, loss = 0.138718381524086
iteration 35, loss = 0.12994831800460815
iteration 36, loss = 0.14614197611808777
iteration 37, loss = 0.14647316932678223
iteration 38, loss = 0.14795437455177307
iteration 39, loss = 0.13472887873649597
iteration 40, loss = 0.1273619830608368
iteration 41, loss = 0.1287553906440735
iteration 42, loss = 0.16782918572425842
iteration 43, loss = 0.1351967751979828
iteration 44, loss = 0.17278458178043365
iteration 45, loss = 0.08009400963783264
iteration 46, loss = 0.13388244807720184
iteration 47, loss = 0.1350187212228775
iteration 48, loss = 0.15335053205490112
iteration 49, loss = 0.12253778427839279
iteration 50, loss = 0.11452389508485794
iteration 51, loss = 0.10308259725570679
iteration 52, loss = 0.144767165184021
iteration 53, loss = 0.12883830070495605
iteration 54, loss = 0.12441268563270569
iteration 55, loss = 0.15182700753211975
iteration 56, loss = 0.11332200467586517
iteration 57, loss = 0.18281173706054688
iteration 58, loss = 0.0957421362400055
iteration 59, loss = 0.17787015438079834
iteration 60, loss = 0.13724014163017273
iteration 61, loss = 0.12969990074634552
iteration 62, loss = 0.15867498517036438
iteration 63, loss = 0.13226766884326935
iteration 64, loss = 0.15494240820407867
iteration 65, loss = 0.15457309782505035
iteration 66, loss = 0.16171687841415405
iteration 67, loss = 0.14381666481494904
iteration 68, loss = 0.11652376502752304
iteration 69, loss = 0.15020276606082916
iteration 70, loss = 0.13168998062610626
iteration 71, loss = 0.1255897432565689
iteration 72, loss = 0.11456046998500824
iteration 73, loss = 0.13835854828357697
iteration 74, loss = 0.1277483105659485
iteration 75, loss = 0.10957517474889755
iteration 76, loss = 0.1413157731294632
iteration 77, loss = 0.1312251091003418
iteration 78, loss = 0.12649863958358765
iteration 79, loss = 0.14820200204849243
iteration 80, loss = 0.15732334554195404
iteration 81, loss = 0.16604235768318176
iteration 82, loss = 0.12519054114818573
iteration 83, loss = 0.1300850510597229
iteration 84, loss = 0.1399058997631073
iteration 85, loss = 0.11275184154510498
iteration 86, loss = 0.14371439814567566
iteration 87, loss = 0.1495378315448761
iteration 88, loss = 0.13138285279273987
iteration 89, loss = 0.11219781637191772
iteration 90, loss = 0.13132187724113464
iteration 91, loss = 0.11867724359035492
iteration 92, loss = 0.11653195321559906
iteration 93, loss = 0.11011852324008942
iteration 94, loss = 0.07665718346834183
iteration 95, loss = 0.19406411051750183
iteration 96, loss = 0.15931980311870575
iteration 97, loss = 0.13063812255859375
iteration 98, loss = 0.13772349059581757
iteration 99, loss = 0.1355867087841034
iteration 100, loss = 0.16468077898025513
iteration 101, loss = 0.11891201138496399
iteration 102, loss = 0.1555875986814499
iteration 103, loss = 0.1424686759710312
iteration 104, loss = 0.16273172199726105
iteration 105, loss = 0.12065211683511734
iteration 106, loss = 0.1436721831560135
iteration 107, loss = 0.14523833990097046
iteration 108, loss = 0.12410353124141693
iteration 109, loss = 0.1426979899406433
iteration 110, loss = 0.1287248581647873
iteration 111, loss = 0.11803106218576431
iteration 112, loss = 0.09607475250959396
iteration 113, loss = 0.08925598114728928
iteration 114, loss = 0.14484846591949463
iteration 115, loss = 0.11682547628879547
iteration 116, loss = 0.0790286436676979
iteration 117, loss = 0.08254225552082062
iteration 118, loss = 0.1215495765209198
iteration 119, loss = 0.1687099039554596
iteration 120, loss = 0.12563243508338928
iteration 121, loss = 0.12573397159576416
iteration 122, loss = 0.09821029007434845
iteration 123, loss = 0.1292371153831482
iteration 124, loss = 0.1179814338684082
iteration 125, loss = 0.0923364907503128
iteration 126, loss = 0.09362483769655228
iteration 127, loss = 0.13430413603782654
iteration 128, loss = 0.1617254912853241
iteration 129, loss = 0.10882502049207687
iteration 130, loss = 0.19662868976593018
iteration 131, loss = 0.09513509273529053
iteration 132, loss = 0.12204347550868988
iteration 133, loss = 0.11740124225616455
iteration 134, loss = 0.13021248579025269
iteration 135, loss = 0.13191108405590057
iteration 136, loss = 0.09567108750343323
iteration 137, loss = 0.15711936354637146
iteration 138, loss = 0.1388542354106903
iteration 139, loss = 0.1348280906677246
iteration 140, loss = 0.12700916826725006
iteration 141, loss = 0.10988585650920868
iteration 142, loss = 0.11600484699010849
iteration 143, loss = 0.14285452663898468
iteration 144, loss = 0.14955362677574158
iteration 145, loss = 0.12339033931493759
iteration 146, loss = 0.13570508360862732
iteration 147, loss = 0.10941353440284729
iteration 148, loss = 0.1270856410264969
iteration 149, loss = 0.17816121876239777
iteration 150, loss = 0.09185776859521866
iteration 151, loss = 0.11820048093795776
iteration 152, loss = 0.11289648711681366
iteration 153, loss = 0.13385328650474548
iteration 154, loss = 0.1461070477962494
iteration 155, loss = 0.12544308602809906
iteration 156, loss = 0.12370281666517258
iteration 157, loss = 0.1580754816532135
iteration 158, loss = 0.1216665655374527
iteration 159, loss = 0.14424191415309906
iteration 160, loss = 0.11220258474349976
iteration 161, loss = 0.0958898589015007
iteration 162, loss = 0.14996977150440216
iteration 163, loss = 0.07673138380050659
iteration 164, loss = 0.11449164152145386
iteration 165, loss = 0.10554063320159912
iteration 166, loss = 0.12908419966697693
iteration 167, loss = 0.11430992931127548
iteration 168, loss = 0.09742891043424606
iteration 169, loss = 0.15893998742103577
iteration 170, loss = 0.14011891186237335
iteration 171, loss = 0.1359182745218277
iteration 172, loss = 0.07619680464267731
iteration 173, loss = 0.10834468901157379
iteration 174, loss = 0.1118713989853859
iteration 175, loss = 0.13570262491703033
iteration 176, loss = 0.09843134135007858
iteration 177, loss = 0.11761178076267242
iteration 178, loss = 0.11338920146226883
iteration 179, loss = 0.1451018750667572
iteration 180, loss = 0.13230358064174652
iteration 181, loss = 0.10683020204305649
iteration 182, loss = 0.14084698259830475
iteration 183, loss = 0.1310736984014511
iteration 184, loss = 0.11629105359315872
iteration 185, loss = 0.09615222364664078
iteration 186, loss = 0.12099120020866394
iteration 187, loss = 0.15205726027488708
iteration 188, loss = 0.09318021684885025
iteration 189, loss = 0.10328298062086105
iteration 190, loss = 0.11294083297252655
iteration 191, loss = 0.08608555793762207
iteration 192, loss = 0.09073204547166824
iteration 193, loss = 0.13500799238681793
iteration 194, loss = 0.11181807518005371
iteration 195, loss = 0.10445353388786316
iteration 196, loss = 0.14335386455059052
iteration 197, loss = 0.12348998337984085
iteration 198, loss = 0.13283509016036987
iteration 199, loss = 0.13084547221660614
iteration 200, loss = 0.06292763352394104
iteration 201, loss = 0.11952561140060425
iteration 202, loss = 0.1272614598274231
iteration 203, loss = 0.11967801302671432
iteration 204, loss = 0.09032471477985382
iteration 205, loss = 0.17576169967651367
iteration 206, loss = 0.15972936153411865
iteration 207, loss = 0.11878456175327301
iteration 208, loss = 0.129008948802948
iteration 209, loss = 0.10428119450807571
iteration 210, loss = 0.09103268384933472
iteration 211, loss = 0.0931069478392601
iteration 212, loss = 0.12437062710523605
iteration 213, loss = 0.10330681502819061
iteration 214, loss = 0.11985336989164352
iteration 215, loss = 0.07987253367900848
iteration 216, loss = 0.12245356291532516
iteration 217, loss = 0.11915908008813858
iteration 218, loss = 0.13611440360546112
iteration 219, loss = 0.11814239621162415
iteration 220, loss = 0.10315272957086563
iteration 221, loss = 0.10527686774730682
iteration 222, loss = 0.10723916441202164
iteration 223, loss = 0.07599826157093048
iteration 224, loss = 0.10067692399024963
iteration 225, loss = 0.11272194981575012
iteration 226, loss = 0.13273577392101288
iteration 227, loss = 0.11360680311918259
iteration 228, loss = 0.0912700966000557
iteration 229, loss = 0.1317564845085144
iteration 230, loss = 0.12854768335819244
iteration 231, loss = 0.09645383805036545
iteration 232, loss = 0.10369241237640381
iteration 233, loss = 0.1061357781291008
iteration 234, loss = 0.10224741697311401
iteration 235, loss = 0.12668953835964203
iteration 236, loss = 0.13330048322677612
iteration 237, loss = 0.12868939340114594
iteration 238, loss = 0.12455104291439056
iteration 239, loss = 0.1328263282775879
iteration 240, loss = 0.09590528905391693
iteration 241, loss = 0.10614099353551865
iteration 242, loss = 0.175320565700531
iteration 243, loss = 0.13720238208770752
iteration 244, loss = 0.10548290610313416
iteration 245, loss = 0.09861115366220474
iteration 246, loss = 0.10320727527141571
iteration 247, loss = 0.15201479196548462
iteration 248, loss = 0.11273718625307083
iteration 249, loss = 0.08001358062028885
iteration 250, loss = 0.11909975111484528
iteration 251, loss = 0.10885635763406754
iteration 252, loss = 0.1390419751405716
iteration 253, loss = 0.0985780730843544
iteration 254, loss = 0.13306401669979095
iteration 255, loss = 0.10906337201595306
iteration 256, loss = 0.1113346517086029
iteration 257, loss = 0.08358784765005112
iteration 258, loss = 0.06996843218803406
iteration 259, loss = 0.10136379301548004
iteration 260, loss = 0.11633401364088058
iteration 261, loss = 0.15999476611614227
iteration 262, loss = 0.12306723743677139
iteration 263, loss = 0.14629781246185303
iteration 264, loss = 0.1347624808549881
iteration 265, loss = 0.10552859306335449
iteration 266, loss = 0.11073765158653259
iteration 267, loss = 0.1068260595202446
iteration 268, loss = 0.14315780997276306
iteration 269, loss = 0.10032446682453156
iteration 270, loss = 0.08912342041730881
iteration 271, loss = 0.1024160161614418
iteration 272, loss = 0.1679891049861908
iteration 273, loss = 0.12267327308654785
iteration 274, loss = 0.12012777477502823
iteration 275, loss = 0.08895032107830048
iteration 276, loss = 0.14078910648822784
iteration 277, loss = 0.0840306431055069
iteration 278, loss = 0.12429754436016083
iteration 279, loss = 0.11366980522871017
iteration 280, loss = 0.11455343663692474
iteration 281, loss = 0.09970562160015106
iteration 282, loss = 0.08993697166442871
iteration 283, loss = 0.12743233144283295
iteration 284, loss = 0.06668607890605927
iteration 285, loss = 0.13351963460445404
iteration 286, loss = 0.09243723005056381
iteration 287, loss = 0.1308075189590454
iteration 288, loss = 0.08943582326173782
iteration 289, loss = 0.1124405637383461
iteration 290, loss = 0.11314652860164642
iteration 291, loss = 0.08807825297117233
iteration 292, loss = 0.08919468522071838
iteration 293, loss = 0.08540184050798416
iteration 294, loss = 0.09701289981603622
iteration 295, loss = 0.10153669118881226
iteration 296, loss = 0.11488347500562668
iteration 297, loss = 0.128663569688797
iteration 298, loss = 0.08400531113147736
iteration 299, loss = 0.07988707721233368
iteration 300, loss = 0.10966165363788605
iteration 1, loss = 0.13637413084506989
iteration 2, loss = 0.09969814121723175
iteration 3, loss = 0.11332868784666061
iteration 4, loss = 0.054216690361499786
iteration 5, loss = 0.10728760808706284
iteration 6, loss = 0.13674207031726837
iteration 7, loss = 0.11549437046051025
iteration 8, loss = 0.1253238469362259
iteration 9, loss = 0.09643728286027908
iteration 10, loss = 0.07666166871786118
iteration 11, loss = 0.12723854184150696
iteration 12, loss = 0.16034957766532898
iteration 13, loss = 0.11737243086099625
iteration 14, loss = 0.08439719676971436
iteration 15, loss = 0.11484363675117493
iteration 16, loss = 0.09877317398786545
iteration 17, loss = 0.1384509801864624
iteration 18, loss = 0.07405776530504227
iteration 19, loss = 0.09216731041669846
iteration 20, loss = 0.11054717749357224
iteration 21, loss = 0.09196841716766357
iteration 22, loss = 0.12106461077928543
iteration 23, loss = 0.10522591322660446
iteration 24, loss = 0.1276077926158905
iteration 25, loss = 0.1008925586938858
iteration 26, loss = 0.06481443345546722
iteration 27, loss = 0.13168050348758698
iteration 28, loss = 0.11637508869171143
iteration 29, loss = 0.10469385236501694
iteration 30, loss = 0.08397852629423141
iteration 31, loss = 0.11187934875488281
iteration 32, loss = 0.08800330013036728
iteration 33, loss = 0.08183286339044571
iteration 34, loss = 0.08694274723529816
iteration 35, loss = 0.08227546513080597
iteration 36, loss = 0.10469143837690353
iteration 37, loss = 0.0941198468208313
iteration 38, loss = 0.10437358170747757
iteration 39, loss = 0.11044039577245712
iteration 40, loss = 0.13172154128551483
iteration 41, loss = 0.12772338092327118
iteration 42, loss = 0.09057875722646713
iteration 43, loss = 0.0795031413435936
iteration 44, loss = 0.15291301906108856
iteration 45, loss = 0.12138378620147705
iteration 46, loss = 0.07993283122777939
iteration 47, loss = 0.1142391636967659
iteration 48, loss = 0.06954941898584366
iteration 49, loss = 0.09980510175228119
iteration 50, loss = 0.0778416246175766
iteration 51, loss = 0.10518648475408554
iteration 52, loss = 0.10860180854797363
iteration 53, loss = 0.14592191576957703
iteration 54, loss = 0.09241443127393723
iteration 55, loss = 0.07436308264732361
iteration 56, loss = 0.1016446202993393
iteration 57, loss = 0.10567249357700348
iteration 58, loss = 0.11393310129642487
iteration 59, loss = 0.07217808067798615
iteration 60, loss = 0.11481881886720657
iteration 61, loss = 0.11142796277999878
iteration 62, loss = 0.09844724088907242
iteration 63, loss = 0.12713780999183655
iteration 64, loss = 0.09727383404970169
iteration 65, loss = 0.1137346625328064
iteration 66, loss = 0.07714517414569855
iteration 67, loss = 0.1107698306441307
iteration 68, loss = 0.12451345473527908
iteration 69, loss = 0.13059253990650177
iteration 70, loss = 0.09755560755729675
iteration 71, loss = 0.10282499343156815
iteration 72, loss = 0.047033898532390594
iteration 73, loss = 0.08713097870349884
iteration 74, loss = 0.11097157746553421
iteration 75, loss = 0.10425965487957001
iteration 76, loss = 0.07443374395370483
iteration 77, loss = 0.11019541323184967
iteration 78, loss = 0.0827351063489914
iteration 79, loss = 0.12054568529129028
iteration 80, loss = 0.14937949180603027
iteration 81, loss = 0.09427977353334427
iteration 82, loss = 0.08720400184392929
iteration 83, loss = 0.06061679497361183
iteration 84, loss = 0.09412441402673721
iteration 85, loss = 0.08011303842067719
iteration 86, loss = 0.11370843648910522
iteration 87, loss = 0.10411912947893143
iteration 88, loss = 0.09464388340711594
iteration 89, loss = 0.10820034891366959
iteration 90, loss = 0.08108732849359512
iteration 91, loss = 0.12055423855781555
iteration 92, loss = 0.09424847364425659
iteration 93, loss = 0.09693728387355804
iteration 94, loss = 0.06405498087406158
iteration 95, loss = 0.1053256019949913
iteration 96, loss = 0.11133231222629547
iteration 97, loss = 0.10169171541929245
iteration 98, loss = 0.0853804349899292
iteration 99, loss = 0.08736645430326462
iteration 100, loss = 0.09626206755638123
iteration 101, loss = 0.09373311698436737
iteration 102, loss = 0.10271528363227844
iteration 103, loss = 0.10226529091596603
iteration 104, loss = 0.10945980995893478
iteration 105, loss = 0.1211824044585228
iteration 106, loss = 0.10144613683223724
iteration 107, loss = 0.07405471801757812
iteration 108, loss = 0.09097882360219955
iteration 109, loss = 0.08969198912382126
iteration 110, loss = 0.09803816676139832
iteration 111, loss = 0.08093536645174026
iteration 112, loss = 0.1228221207857132
iteration 113, loss = 0.09095872193574905
iteration 114, loss = 0.08707884699106216
iteration 115, loss = 0.1339683085680008
iteration 116, loss = 0.11057927459478378
iteration 117, loss = 0.10938598215579987
iteration 118, loss = 0.08550198376178741
iteration 119, loss = 0.08477207273244858
iteration 120, loss = 0.10728041082620621
iteration 121, loss = 0.09759020060300827
iteration 122, loss = 0.1040094792842865
iteration 123, loss = 0.07273396849632263
iteration 124, loss = 0.1182471290230751
iteration 125, loss = 0.058387164026498795
iteration 126, loss = 0.11142855137586594
iteration 127, loss = 0.09537236392498016
iteration 128, loss = 0.08466210216283798
iteration 129, loss = 0.10406091064214706
iteration 130, loss = 0.08891996741294861
iteration 131, loss = 0.11191903054714203
iteration 132, loss = 0.07726860791444778
iteration 133, loss = 0.06874208152294159
iteration 134, loss = 0.06736332178115845
iteration 135, loss = 0.08524373173713684
iteration 136, loss = 0.07336460798978806
iteration 137, loss = 0.09739889204502106
iteration 138, loss = 0.10926835983991623
iteration 139, loss = 0.09423108398914337
iteration 140, loss = 0.13933783769607544
iteration 141, loss = 0.08722024410963058
iteration 142, loss = 0.08187984675168991
iteration 143, loss = 0.08540044724941254
iteration 144, loss = 0.046618618071079254
iteration 145, loss = 0.07336486130952835
iteration 146, loss = 0.08184805512428284
iteration 147, loss = 0.06987843662500381
iteration 148, loss = 0.07867249846458435
iteration 149, loss = 0.11706016957759857
iteration 150, loss = 0.1152472048997879
iteration 151, loss = 0.10537350177764893
iteration 152, loss = 0.07211022078990936
iteration 153, loss = 0.08971555531024933
iteration 154, loss = 0.0912124514579773
iteration 155, loss = 0.15195275843143463
iteration 156, loss = 0.15149663388729095
iteration 157, loss = 0.08152297139167786
iteration 158, loss = 0.13999015092849731
iteration 159, loss = 0.11028176546096802
iteration 160, loss = 0.10896036773920059
iteration 161, loss = 0.10281452536582947
iteration 162, loss = 0.08308754116296768
iteration 163, loss = 0.13172611594200134
iteration 164, loss = 0.09826608002185822
iteration 165, loss = 0.09482815861701965
iteration 166, loss = 0.10590453445911407
iteration 167, loss = 0.06632968783378601
iteration 168, loss = 0.08312895894050598
iteration 169, loss = 0.10874732583761215
iteration 170, loss = 0.08881120383739471
iteration 171, loss = 0.07746511697769165
iteration 172, loss = 0.06348118931055069
iteration 173, loss = 0.07578922808170319
iteration 174, loss = 0.06452576071023941
iteration 175, loss = 0.07065987586975098
iteration 176, loss = 0.0894421935081482
iteration 177, loss = 0.1330418586730957
iteration 178, loss = 0.09522541612386703
iteration 179, loss = 0.12597507238388062
iteration 180, loss = 0.09803742915391922
iteration 181, loss = 0.0806552842259407
iteration 182, loss = 0.09054171293973923
iteration 183, loss = 0.05174758657813072
iteration 184, loss = 0.049541935324668884
iteration 185, loss = 0.07811184227466583
iteration 186, loss = 0.09382957220077515
iteration 187, loss = 0.07371963560581207
iteration 188, loss = 0.07529626041650772
iteration 189, loss = 0.1172073557972908
iteration 190, loss = 0.0636477991938591
iteration 191, loss = 0.09679147601127625
iteration 192, loss = 0.12068231403827667
iteration 193, loss = 0.08850111067295074
iteration 194, loss = 0.06948061287403107
iteration 195, loss = 0.07898607850074768
iteration 196, loss = 0.0976906567811966
iteration 197, loss = 0.08362042903900146
iteration 198, loss = 0.08196711540222168
iteration 199, loss = 0.09176138788461685
iteration 200, loss = 0.06460295617580414
iteration 201, loss = 0.14803089201450348
iteration 202, loss = 0.07037237286567688
iteration 203, loss = 0.06380663812160492
iteration 204, loss = 0.0683203712105751
iteration 205, loss = 0.10113386064767838
iteration 206, loss = 0.09953190386295319
iteration 207, loss = 0.05860905349254608
iteration 208, loss = 0.08901769667863846
iteration 209, loss = 0.09237867593765259
iteration 210, loss = 0.1448533535003662
iteration 211, loss = 0.11983692646026611
iteration 212, loss = 0.08437543362379074
iteration 213, loss = 0.04228447377681732
iteration 214, loss = 0.05763319134712219
iteration 215, loss = 0.11727190017700195
iteration 216, loss = 0.0894736498594284
iteration 217, loss = 0.07144278287887573
iteration 218, loss = 0.08368391543626785
iteration 219, loss = 0.060348693281412125
iteration 220, loss = 0.09286532551050186
iteration 221, loss = 0.11852692812681198
iteration 222, loss = 0.09287188202142715
iteration 223, loss = 0.09217900037765503
iteration 224, loss = 0.12191880494356155
iteration 225, loss = 0.11746123433113098
iteration 226, loss = 0.050846442580223083
iteration 227, loss = 0.07104866206645966
iteration 228, loss = 0.062338314950466156
iteration 229, loss = 0.09726182371377945
iteration 230, loss = 0.09662286192178726
iteration 231, loss = 0.0641600489616394
iteration 232, loss = 0.047076933085918427
iteration 233, loss = 0.05011340230703354
iteration 234, loss = 0.07525867968797684
iteration 235, loss = 0.08210593461990356
iteration 236, loss = 0.047073110938072205
iteration 237, loss = 0.1038556843996048
iteration 238, loss = 0.060975320637226105
iteration 239, loss = 0.10614754259586334
iteration 240, loss = 0.1092635989189148
iteration 241, loss = 0.0732305720448494
iteration 242, loss = 0.0623747892677784
iteration 243, loss = 0.05094091594219208
iteration 244, loss = 0.05700337514281273
iteration 245, loss = 0.10687293112277985
iteration 246, loss = 0.0901964083313942
iteration 247, loss = 0.09209959954023361
iteration 248, loss = 0.09492697566747665
iteration 249, loss = 0.11663322895765305
iteration 250, loss = 0.06443265080451965
iteration 251, loss = 0.07668732106685638
iteration 252, loss = 0.11423289775848389
iteration 253, loss = 0.04022889584302902
iteration 254, loss = 0.06645693629980087
iteration 255, loss = 0.11958643049001694
iteration 256, loss = 0.09074109047651291
iteration 257, loss = 0.08493249863386154
iteration 258, loss = 0.11222296953201294
iteration 259, loss = 0.07517844438552856
iteration 260, loss = 0.07222431153059006
iteration 261, loss = 0.09060600399971008
iteration 262, loss = 0.04581742361187935
iteration 263, loss = 0.052168913185596466
iteration 264, loss = 0.08092394471168518
iteration 265, loss = 0.06687542051076889
iteration 266, loss = 0.04767958074808121
iteration 267, loss = 0.05586269497871399
iteration 268, loss = 0.10134820640087128
iteration 269, loss = 0.06235114485025406
iteration 270, loss = 0.11492770165205002
iteration 271, loss = 0.05716860294342041
iteration 272, loss = 0.07043090462684631
iteration 273, loss = 0.059743575751781464
iteration 274, loss = 0.06280424445867538
iteration 275, loss = 0.172137051820755
iteration 276, loss = 0.0728716254234314
iteration 277, loss = 0.052126675844192505
iteration 278, loss = 0.07801549136638641
iteration 279, loss = 0.0654430240392685
iteration 280, loss = 0.08853185176849365
iteration 281, loss = 0.06558113545179367
iteration 282, loss = 0.08278005570173264
iteration 283, loss = 0.11045371741056442
iteration 284, loss = 0.0746203064918518
iteration 285, loss = 0.03654906153678894
iteration 286, loss = 0.08577392250299454
iteration 287, loss = 0.08802541345357895
iteration 288, loss = 0.07707910984754562
iteration 289, loss = 0.08288681507110596
iteration 290, loss = 0.09610863775014877
iteration 291, loss = 0.11422468721866608
iteration 292, loss = 0.05850054696202278
iteration 293, loss = 0.07077661901712418
iteration 294, loss = 0.10847136378288269
iteration 295, loss = 0.11264395713806152
iteration 296, loss = 0.08057376742362976
iteration 297, loss = 0.09881609678268433
iteration 298, loss = 0.09352109581232071
iteration 299, loss = 0.09769580513238907
iteration 300, loss = 0.07772018015384674
iteration 1, loss = 0.04059579223394394
iteration 2, loss = 0.05298297852277756
iteration 3, loss = 0.07149171084165573
iteration 4, loss = 0.09934740513563156
iteration 5, loss = 0.07640169560909271
iteration 6, loss = 0.0597706139087677
iteration 7, loss = 0.0864957943558693
iteration 8, loss = 0.10279042273759842
iteration 9, loss = 0.07249203324317932
iteration 10, loss = 0.09089171886444092
iteration 11, loss = 0.04341452196240425
iteration 12, loss = 0.08057518303394318
iteration 13, loss = 0.04614073783159256
iteration 14, loss = 0.07294423133134842
iteration 15, loss = 0.061736851930618286
iteration 16, loss = 0.13437022268772125
iteration 17, loss = 0.06244035065174103
iteration 18, loss = 0.06893739104270935
iteration 19, loss = 0.04766351357102394
iteration 20, loss = 0.09725096076726913
iteration 21, loss = 0.08188950270414352
iteration 22, loss = 0.07452115416526794
iteration 23, loss = 0.10967257618904114
iteration 24, loss = 0.06293030083179474
iteration 25, loss = 0.08202047646045685
iteration 26, loss = 0.08749127388000488
iteration 27, loss = 0.08938285708427429
iteration 28, loss = 0.09856699407100677
iteration 29, loss = 0.061613161116838455
iteration 30, loss = 0.061047714203596115
iteration 31, loss = 0.047522224485874176
iteration 32, loss = 0.06361112743616104
iteration 33, loss = 0.09537536650896072
iteration 34, loss = 0.08649923652410507
iteration 35, loss = 0.05174495652318001
iteration 36, loss = 0.10471273213624954
iteration 37, loss = 0.08465642482042313
iteration 38, loss = 0.10335013270378113
iteration 39, loss = 0.07795002311468124
iteration 40, loss = 0.06804563105106354
iteration 41, loss = 0.08349822461605072
iteration 42, loss = 0.09766416251659393
iteration 43, loss = 0.06134500354528427
iteration 44, loss = 0.0897754430770874
iteration 45, loss = 0.05626016482710838
iteration 46, loss = 0.03708890452980995
iteration 47, loss = 0.060976721346378326
iteration 48, loss = 0.05205532908439636
iteration 49, loss = 0.07207201421260834
iteration 50, loss = 0.0759633332490921
iteration 51, loss = 0.060838110744953156
iteration 52, loss = 0.07381464540958405
iteration 53, loss = 0.09191525727510452
iteration 54, loss = 0.09820778667926788
iteration 55, loss = 0.0645076259970665
iteration 56, loss = 0.06892643123865128
iteration 57, loss = 0.10083097219467163
iteration 58, loss = 0.05966847017407417
iteration 59, loss = 0.1015598326921463
iteration 60, loss = 0.03551408275961876
iteration 61, loss = 0.12749211490154266
iteration 62, loss = 0.1000104546546936
iteration 63, loss = 0.08857312798500061
iteration 64, loss = 0.1017889678478241
iteration 65, loss = 0.05118804797530174
iteration 66, loss = 0.040068887174129486
iteration 67, loss = 0.05489485710859299
iteration 68, loss = 0.06322780251502991
iteration 69, loss = 0.07011377811431885
iteration 70, loss = 0.07321449369192123
iteration 71, loss = 0.07575446367263794
iteration 72, loss = 0.055694617331027985
iteration 73, loss = 0.044723108410835266
iteration 74, loss = 0.06590355932712555
iteration 75, loss = 0.06838366389274597
iteration 76, loss = 0.05272038280963898
iteration 77, loss = 0.08585727959871292
iteration 78, loss = 0.07455361634492874
iteration 79, loss = 0.09568734467029572
iteration 80, loss = 0.048055876046419144
iteration 81, loss = 0.06687846034765244
iteration 82, loss = 0.0979909598827362
iteration 83, loss = 0.08704154193401337
iteration 84, loss = 0.07541994750499725
iteration 85, loss = 0.06608223915100098
iteration 86, loss = 0.0774364322423935
iteration 87, loss = 0.05466813966631889
iteration 88, loss = 0.055492497980594635
iteration 89, loss = 0.09822385758161545
iteration 90, loss = 0.10387791693210602
iteration 91, loss = 0.06078826263546944
iteration 92, loss = 0.07605671137571335
iteration 93, loss = 0.07872425019741058
iteration 94, loss = 0.05660592019557953
iteration 95, loss = 0.061095863580703735
iteration 96, loss = 0.08104440569877625
iteration 97, loss = 0.10151077806949615
iteration 98, loss = 0.0823570266366005
iteration 99, loss = 0.06927792727947235
iteration 100, loss = 0.10379932820796967
iteration 101, loss = 0.05561657249927521
iteration 102, loss = 0.09804186969995499
iteration 103, loss = 0.07570955902338028
iteration 104, loss = 0.09878096729516983
iteration 105, loss = 0.05184101313352585
iteration 106, loss = 0.1102260947227478
iteration 107, loss = 0.10065334290266037
iteration 108, loss = 0.10200674086809158
iteration 109, loss = 0.043692562729120255
iteration 110, loss = 0.07350321114063263
iteration 111, loss = 0.042092420160770416
iteration 112, loss = 0.06953099370002747
iteration 113, loss = 0.11838173121213913
iteration 114, loss = 0.04330601915717125
iteration 115, loss = 0.04168540984392166
iteration 116, loss = 0.07377618551254272
iteration 117, loss = 0.07918703556060791
iteration 118, loss = 0.08696652203798294
iteration 119, loss = 0.08053673058748245
iteration 120, loss = 0.03735630214214325
iteration 121, loss = 0.07901646941900253
iteration 122, loss = 0.10324233025312424
iteration 123, loss = 0.09349105507135391
iteration 124, loss = 0.042054515331983566
iteration 125, loss = 0.05958888679742813
iteration 126, loss = 0.05316619202494621
iteration 127, loss = 0.06766829639673233
iteration 128, loss = 0.0721263736486435
iteration 129, loss = 0.05902023985981941
iteration 130, loss = 0.048921748995780945
iteration 131, loss = 0.05555400997400284
iteration 132, loss = 0.0964503064751625
iteration 133, loss = 0.054675646126270294
iteration 134, loss = 0.08354032039642334
iteration 135, loss = 0.07051419466733932
iteration 136, loss = 0.08627690374851227
iteration 137, loss = 0.05733748525381088
iteration 138, loss = 0.054128654301166534
iteration 139, loss = 0.05646711587905884
iteration 140, loss = 0.07333941012620926
iteration 141, loss = 0.0568636991083622
iteration 142, loss = 0.11393015086650848
iteration 143, loss = 0.05473516881465912
iteration 144, loss = 0.05879387632012367
iteration 145, loss = 0.05019234120845795
iteration 146, loss = 0.10461278259754181
iteration 147, loss = 0.0557936355471611
iteration 148, loss = 0.047833189368247986
iteration 149, loss = 0.09469062089920044
iteration 150, loss = 0.04328228160738945
iteration 151, loss = 0.09241662919521332
iteration 152, loss = 0.061893001198768616
iteration 153, loss = 0.048567596822977066
iteration 154, loss = 0.06502602994441986
iteration 155, loss = 0.07546304166316986
iteration 156, loss = 0.06286321580410004
iteration 157, loss = 0.04348558932542801
iteration 158, loss = 0.06031298637390137
iteration 159, loss = 0.06052380055189133
iteration 160, loss = 0.06724578887224197
iteration 161, loss = 0.08243755251169205
iteration 162, loss = 0.04336166009306908
iteration 163, loss = 0.09649980068206787
iteration 164, loss = 0.07379749417304993
iteration 165, loss = 0.10199648141860962
iteration 166, loss = 0.06372712552547455
iteration 167, loss = 0.05872034281492233
iteration 168, loss = 0.0556190200150013
iteration 169, loss = 0.04717424139380455
iteration 170, loss = 0.06625404208898544
iteration 171, loss = 0.13170352578163147
iteration 172, loss = 0.08080572634935379
iteration 173, loss = 0.06401115655899048
iteration 174, loss = 0.08107560127973557
iteration 175, loss = 0.060659851878881454
iteration 176, loss = 0.09395348280668259
iteration 177, loss = 0.11563192307949066
iteration 178, loss = 0.07466264069080353
iteration 179, loss = 0.07660513371229172
iteration 180, loss = 0.08253651857376099
iteration 181, loss = 0.10065574944019318
iteration 182, loss = 0.08154872059822083
iteration 183, loss = 0.049035582691431046
iteration 184, loss = 0.07869761437177658
iteration 185, loss = 0.035254716873168945
iteration 186, loss = 0.053088221698999405
iteration 187, loss = 0.10325140506029129
iteration 188, loss = 0.04962440952658653
iteration 189, loss = 0.06744790822267532
iteration 190, loss = 0.03766843304038048
iteration 191, loss = 0.0669471025466919
iteration 192, loss = 0.05573839694261551
iteration 193, loss = 0.07996857166290283
iteration 194, loss = 0.10689859092235565
iteration 195, loss = 0.047836922109127045
iteration 196, loss = 0.049907442182302475
iteration 197, loss = 0.05710038170218468
iteration 198, loss = 0.07282611727714539
iteration 199, loss = 0.11884992569684982
iteration 200, loss = 0.08745316416025162
iteration 201, loss = 0.029408849775791168
iteration 202, loss = 0.11181455850601196
iteration 203, loss = 0.05248907953500748
iteration 204, loss = 0.07206220924854279
iteration 205, loss = 0.06819025427103043
iteration 206, loss = 0.06459097564220428
iteration 207, loss = 0.08242592215538025
iteration 208, loss = 0.06307455897331238
iteration 209, loss = 0.05996808409690857
iteration 210, loss = 0.07198981940746307
iteration 211, loss = 0.06790069490671158
iteration 212, loss = 0.031230786815285683
iteration 213, loss = 0.04802709445357323
iteration 214, loss = 0.0924120619893074
iteration 215, loss = 0.0532926544547081
iteration 216, loss = 0.06623899191617966
iteration 217, loss = 0.07239394634962082
iteration 218, loss = 0.06760622560977936
iteration 219, loss = 0.09836295247077942
iteration 220, loss = 0.04932951182126999
iteration 221, loss = 0.06803088635206223
iteration 222, loss = 0.047112081199884415
iteration 223, loss = 0.03139287978410721
iteration 224, loss = 0.03676122799515724
iteration 225, loss = 0.046498317271471024
iteration 226, loss = 0.0519593320786953
iteration 227, loss = 0.05263754725456238
iteration 228, loss = 0.06414346396923065
iteration 229, loss = 0.0758759081363678
iteration 230, loss = 0.05914336070418358
iteration 231, loss = 0.055525876581668854
iteration 232, loss = 0.05207096040248871
iteration 233, loss = 0.044943712651729584
iteration 234, loss = 0.06521391868591309
iteration 235, loss = 0.025225449353456497
iteration 236, loss = 0.08714594691991806
iteration 237, loss = 0.0463503934442997
iteration 238, loss = 0.04354001209139824
iteration 239, loss = 0.06311488151550293
iteration 240, loss = 0.063029944896698
iteration 241, loss = 0.03700074553489685
iteration 242, loss = 0.10303793847560883
iteration 243, loss = 0.03273486718535423
iteration 244, loss = 0.08382745087146759
iteration 245, loss = 0.06602752953767776
iteration 246, loss = 0.08096058666706085
iteration 247, loss = 0.05525853857398033
iteration 248, loss = 0.07677209377288818
iteration 249, loss = 0.08321209251880646
iteration 250, loss = 0.0666448250412941
iteration 251, loss = 0.09542619436979294
iteration 252, loss = 0.05967046320438385
iteration 253, loss = 0.0451948381960392
iteration 254, loss = 0.05894797295331955
iteration 255, loss = 0.06161938235163689
iteration 256, loss = 0.0773274302482605
iteration 257, loss = 0.05254171043634415
iteration 258, loss = 0.06361495703458786
iteration 259, loss = 0.05423560366034508
iteration 260, loss = 0.07757939398288727
iteration 261, loss = 0.06107301265001297
iteration 262, loss = 0.03714924678206444
iteration 263, loss = 0.07790658622980118
iteration 264, loss = 0.03951549530029297
iteration 265, loss = 0.08380597829818726
iteration 266, loss = 0.08011346310377121
iteration 267, loss = 0.08376538008451462
iteration 268, loss = 0.059428922832012177
iteration 269, loss = 0.0434064120054245
iteration 270, loss = 0.08887802809476852
iteration 271, loss = 0.03389258310198784
iteration 272, loss = 0.10389385372400284
iteration 273, loss = 0.04615767300128937
iteration 274, loss = 0.04907423257827759
iteration 275, loss = 0.045903753489255905
iteration 276, loss = 0.07464437931776047
iteration 277, loss = 0.15511351823806763
iteration 278, loss = 0.05827484279870987
iteration 279, loss = 0.09681615978479385
iteration 280, loss = 0.07328503578901291
iteration 281, loss = 0.039049938321113586
iteration 282, loss = 0.060506295412778854
iteration 283, loss = 0.0728335827589035
iteration 284, loss = 0.05249219387769699
iteration 285, loss = 0.0781913697719574
iteration 286, loss = 0.0814078077673912
iteration 287, loss = 0.05559583753347397
iteration 288, loss = 0.06744275242090225
iteration 289, loss = 0.0378277562558651
iteration 290, loss = 0.056291911751031876
iteration 291, loss = 0.0606277696788311
iteration 292, loss = 0.07760713994503021
iteration 293, loss = 0.04732193425297737
iteration 294, loss = 0.03058212250471115
iteration 295, loss = 0.05679985508322716
iteration 296, loss = 0.05673171952366829
iteration 297, loss = 0.07283861935138702
iteration 298, loss = 0.09889107942581177
iteration 299, loss = 0.0470159649848938
iteration 300, loss = 0.10050566494464874
iteration 1, loss = 0.07135415077209473
iteration 2, loss = 0.07596956938505173
iteration 3, loss = 0.07407980412244797
iteration 4, loss = 0.05498563498258591
iteration 5, loss = 0.045130953192710876
iteration 6, loss = 0.06153115630149841
iteration 7, loss = 0.022217705845832825
iteration 8, loss = 0.0647522434592247
iteration 9, loss = 0.08132173120975494
iteration 10, loss = 0.059671550989151
iteration 11, loss = 0.0863298624753952
iteration 12, loss = 0.0709177777171135
iteration 13, loss = 0.10493908077478409
iteration 14, loss = 0.07924332469701767
iteration 15, loss = 0.03972633555531502
iteration 16, loss = 0.05212097615003586
iteration 17, loss = 0.05585901439189911
iteration 18, loss = 0.048686347901821136
iteration 19, loss = 0.06367740035057068
iteration 20, loss = 0.05986372008919716
iteration 21, loss = 0.05134088546037674
iteration 22, loss = 0.04960501939058304
iteration 23, loss = 0.021556908264756203
iteration 24, loss = 0.03949376195669174
iteration 25, loss = 0.06224794685840607
iteration 26, loss = 0.07538115978240967
iteration 27, loss = 0.047471098601818085
iteration 28, loss = 0.0625758171081543
iteration 29, loss = 0.09550031274557114
iteration 30, loss = 0.0723462849855423
iteration 31, loss = 0.06476371735334396
iteration 32, loss = 0.08095722645521164
iteration 33, loss = 0.05095033347606659
iteration 34, loss = 0.07414852827787399
iteration 35, loss = 0.02403184212744236
iteration 36, loss = 0.050476204603910446
iteration 37, loss = 0.0400233268737793
iteration 38, loss = 0.05533226951956749
iteration 39, loss = 0.042877860367298126
iteration 40, loss = 0.06756878644227982
iteration 41, loss = 0.04968206584453583
iteration 42, loss = 0.07246515154838562
iteration 43, loss = 0.028910398483276367
iteration 44, loss = 0.06637858599424362
iteration 45, loss = 0.06735917925834656
iteration 46, loss = 0.0662294551730156
iteration 47, loss = 0.1082899421453476
iteration 48, loss = 0.09937624633312225
iteration 49, loss = 0.03982717543840408
iteration 50, loss = 0.06256231665611267
iteration 51, loss = 0.061283472925424576
iteration 52, loss = 0.060592781752347946
iteration 53, loss = 0.04362538829445839
iteration 54, loss = 0.03725156560540199
iteration 55, loss = 0.07866544276475906
iteration 56, loss = 0.04991134628653526
iteration 57, loss = 0.0547872819006443
iteration 58, loss = 0.03593682497739792
iteration 59, loss = 0.06062755361199379
iteration 60, loss = 0.06791291385889053
iteration 61, loss = 0.07253162562847137
iteration 62, loss = 0.05244506895542145
iteration 63, loss = 0.0749032273888588
iteration 64, loss = 0.02420089766383171
iteration 65, loss = 0.06558460742235184
iteration 66, loss = 0.070355124771595
iteration 67, loss = 0.050624553114175797
iteration 68, loss = 0.06551618129014969
iteration 69, loss = 0.0986366719007492
iteration 70, loss = 0.06344105303287506
iteration 71, loss = 0.06499925255775452
iteration 72, loss = 0.06627590954303741
iteration 73, loss = 0.04096761718392372
iteration 74, loss = 0.056796375662088394
iteration 75, loss = 0.07367406785488129
iteration 76, loss = 0.04187789186835289
iteration 77, loss = 0.06592430174350739
iteration 78, loss = 0.045965004712343216
iteration 79, loss = 0.08381228893995285
iteration 80, loss = 0.07998308539390564
iteration 81, loss = 0.05633052811026573
iteration 82, loss = 0.06679558753967285
iteration 83, loss = 0.03821689635515213
iteration 84, loss = 0.09483348578214645
iteration 85, loss = 0.03540431335568428
iteration 86, loss = 0.06312959641218185
iteration 87, loss = 0.08846627175807953
iteration 88, loss = 0.04862170293927193
iteration 89, loss = 0.028246117755770683
iteration 90, loss = 0.03503284603357315
iteration 91, loss = 0.055770598351955414
iteration 92, loss = 0.05098181962966919
iteration 93, loss = 0.06112208962440491
iteration 94, loss = 0.031858835369348526
iteration 95, loss = 0.037841279059648514
iteration 96, loss = 0.05581596493721008
iteration 97, loss = 0.04113473370671272
iteration 98, loss = 0.04187675192952156
iteration 99, loss = 0.08145144581794739
iteration 100, loss = 0.06380689889192581
iteration 101, loss = 0.08890166133642197
iteration 102, loss = 0.04449675604701042
iteration 103, loss = 0.03249645233154297
iteration 104, loss = 0.03903374448418617
iteration 105, loss = 0.06900813430547714
iteration 106, loss = 0.0649915561079979
iteration 107, loss = 0.08834035694599152
iteration 108, loss = 0.06703618913888931
iteration 109, loss = 0.0774691253900528
iteration 110, loss = 0.01692945510149002
iteration 111, loss = 0.04645371809601784
iteration 112, loss = 0.0515349917113781
iteration 113, loss = 0.023250820115208626
iteration 114, loss = 0.03742760792374611
iteration 115, loss = 0.03110591322183609
iteration 116, loss = 0.05849127843976021
iteration 117, loss = 0.038139503449201584
iteration 118, loss = 0.06812720745801926
iteration 119, loss = 0.06375429779291153
iteration 120, loss = 0.07810775190591812
iteration 121, loss = 0.035490814596414566
iteration 122, loss = 0.055180132389068604
iteration 123, loss = 0.06872037798166275
iteration 124, loss = 0.07311378419399261
iteration 125, loss = 0.04412839561700821
iteration 126, loss = 0.04509594663977623
iteration 127, loss = 0.07273288071155548
iteration 128, loss = 0.04600144922733307
iteration 129, loss = 0.03403685241937637
iteration 130, loss = 0.06106867268681526
iteration 131, loss = 0.0739675909280777
iteration 132, loss = 0.03778786212205887
iteration 133, loss = 0.039570294320583344
iteration 134, loss = 0.11799539625644684
iteration 135, loss = 0.07092245668172836
iteration 136, loss = 0.08122450858354568
iteration 137, loss = 0.03163357451558113
iteration 138, loss = 0.022801700979471207
iteration 139, loss = 0.03933590278029442
iteration 140, loss = 0.0788625180721283
iteration 141, loss = 0.06713226437568665
iteration 142, loss = 0.04595591872930527
iteration 143, loss = 0.040902651846408844
iteration 144, loss = 0.017267299816012383
iteration 145, loss = 0.03979457914829254
iteration 146, loss = 0.04433063045144081
iteration 147, loss = 0.05981718748807907
iteration 148, loss = 0.08597885817289352
iteration 149, loss = 0.0326957143843174
iteration 150, loss = 0.03377177566289902
iteration 151, loss = 0.05915224924683571
iteration 152, loss = 0.0693364366889
iteration 153, loss = 0.03550673648715019
iteration 154, loss = 0.07428418844938278
iteration 155, loss = 0.05939207971096039
iteration 156, loss = 0.06424861401319504
iteration 157, loss = 0.021154893562197685
iteration 158, loss = 0.040634311735630035
iteration 159, loss = 0.025752373039722443
iteration 160, loss = 0.037446968257427216
iteration 161, loss = 0.0347091406583786
iteration 162, loss = 0.047529205679893494
iteration 163, loss = 0.06327208131551743
iteration 164, loss = 0.06915873289108276
iteration 165, loss = 0.06413841992616653
iteration 166, loss = 0.02647361531853676
iteration 167, loss = 0.04006723687052727
iteration 168, loss = 0.05366123467683792
iteration 169, loss = 0.06791374832391739
iteration 170, loss = 0.05330996587872505
iteration 171, loss = 0.05330264940857887
iteration 172, loss = 0.022179914638400078
iteration 173, loss = 0.0487089566886425
iteration 174, loss = 0.05498915910720825
iteration 175, loss = 0.05016138032078743
iteration 176, loss = 0.05385153740644455
iteration 177, loss = 0.07881667464971542
iteration 178, loss = 0.07565124332904816
iteration 179, loss = 0.05422259122133255
iteration 180, loss = 0.04487431049346924
iteration 181, loss = 0.05098486691713333
iteration 182, loss = 0.06902439892292023
iteration 183, loss = 0.06850409507751465
iteration 184, loss = 0.03704841434955597
iteration 185, loss = 0.061056993901729584
iteration 186, loss = 0.04611489176750183
iteration 187, loss = 0.05291829630732536
iteration 188, loss = 0.041551623493433
iteration 189, loss = 0.037521883845329285
iteration 190, loss = 0.04815692827105522
iteration 191, loss = 0.040481872856616974
iteration 192, loss = 0.07697657495737076
iteration 193, loss = 0.05575008690357208
iteration 194, loss = 0.03377200663089752
iteration 195, loss = 0.02397306263446808
iteration 196, loss = 0.03400330990552902
iteration 197, loss = 0.07332057505846024
iteration 198, loss = 0.042736537754535675
iteration 199, loss = 0.058555781841278076
iteration 200, loss = 0.09016256034374237
iteration 201, loss = 0.061210837215185165
iteration 202, loss = 0.04439182206988335
iteration 203, loss = 0.03209051489830017
iteration 204, loss = 0.04546085745096207
iteration 205, loss = 0.057746436446905136
iteration 206, loss = 0.027604274451732635
iteration 207, loss = 0.04850247874855995
iteration 208, loss = 0.0949273407459259
iteration 209, loss = 0.02753988653421402
iteration 210, loss = 0.07956238090991974
iteration 211, loss = 0.055451925843954086
iteration 212, loss = 0.06824517995119095
iteration 213, loss = 0.04570291191339493
iteration 214, loss = 0.05607740208506584
iteration 215, loss = 0.05829573795199394
iteration 216, loss = 0.06011461094021797
iteration 217, loss = 0.021914441138505936
iteration 218, loss = 0.0907093733549118
iteration 219, loss = 0.06886972486972809
iteration 220, loss = 0.051151249557733536
iteration 221, loss = 0.06033456698060036
iteration 222, loss = 0.0800933688879013
iteration 223, loss = 0.03286157548427582
iteration 224, loss = 0.06005249544978142
iteration 225, loss = 0.05870126932859421
iteration 226, loss = 0.07755270600318909
iteration 227, loss = 0.05844377353787422
iteration 228, loss = 0.06371170282363892
iteration 229, loss = 0.05221666023135185
iteration 230, loss = 0.022011538967490196
iteration 231, loss = 0.05719122663140297
iteration 232, loss = 0.05958745256066322
iteration 233, loss = 0.022416044026613235
iteration 234, loss = 0.03064190410077572
iteration 235, loss = 0.055025048553943634
iteration 236, loss = 0.044272731989622116
iteration 237, loss = 0.02323286049067974
iteration 238, loss = 0.05875907838344574
iteration 239, loss = 0.06103068217635155
iteration 240, loss = 0.05964513123035431
iteration 241, loss = 0.03518640622496605
iteration 242, loss = 0.037927478551864624
iteration 243, loss = 0.04421388730406761
iteration 244, loss = 0.0441243015229702
iteration 245, loss = 0.0694146677851677
iteration 246, loss = 0.04075661674141884
iteration 247, loss = 0.017537666484713554
iteration 248, loss = 0.04640801250934601
iteration 249, loss = 0.08183407783508301
iteration 250, loss = 0.04715718328952789
iteration 251, loss = 0.029836494475603104
iteration 252, loss = 0.058724530041217804
iteration 253, loss = 0.055825259536504745
iteration 254, loss = 0.04845723509788513
iteration 255, loss = 0.022539976984262466
iteration 256, loss = 0.031303368508815765
iteration 257, loss = 0.055534034967422485
iteration 258, loss = 0.03629982843995094
iteration 259, loss = 0.031138617545366287
iteration 260, loss = 0.07260708510875702
iteration 261, loss = 0.033241525292396545
iteration 262, loss = 0.029861455783247948
iteration 263, loss = 0.06221681088209152
iteration 264, loss = 0.04110787808895111
iteration 265, loss = 0.05074164271354675
iteration 266, loss = 0.06767302751541138
iteration 267, loss = 0.025109756737947464
iteration 268, loss = 0.055474068969488144
iteration 269, loss = 0.05243174731731415
iteration 270, loss = 0.04421838000416756
iteration 271, loss = 0.03807484731078148
iteration 272, loss = 0.04263503476977348
iteration 273, loss = 0.045622568577528
iteration 274, loss = 0.03292771801352501
iteration 275, loss = 0.05027725547552109
iteration 276, loss = 0.04672592133283615
iteration 277, loss = 0.04223691672086716
iteration 278, loss = 0.08621035516262054
iteration 279, loss = 0.058861903846263885
iteration 280, loss = 0.05047207325696945
iteration 281, loss = 0.06303376704454422
iteration 282, loss = 0.016013097018003464
iteration 283, loss = 0.03648001700639725
iteration 284, loss = 0.057070255279541016
iteration 285, loss = 0.0309356190264225
iteration 286, loss = 0.032339029014110565
iteration 287, loss = 0.04243091493844986
iteration 288, loss = 0.03719703480601311
iteration 289, loss = 0.044779762625694275
iteration 290, loss = 0.08195406198501587
iteration 291, loss = 0.025050247088074684
iteration 292, loss = 0.08717933297157288
iteration 293, loss = 0.05381907522678375
iteration 294, loss = 0.042853355407714844
iteration 295, loss = 0.05842424929141998
iteration 296, loss = 0.049528200179338455
iteration 297, loss = 0.03513843193650246
iteration 298, loss = 0.029448464512825012
iteration 299, loss = 0.04419710487127304
iteration 300, loss = 0.07314212620258331
iteration 1, loss = 0.04703810065984726
iteration 2, loss = 0.04373793676495552
iteration 3, loss = 0.0651191920042038
iteration 4, loss = 0.03035596013069153
iteration 5, loss = 0.05494508519768715
iteration 6, loss = 0.08577796071767807
iteration 7, loss = 0.049757443368434906
iteration 8, loss = 0.052391961216926575
iteration 9, loss = 0.05595777556300163
iteration 10, loss = 0.04621108993887901
iteration 11, loss = 0.062449052929878235
iteration 12, loss = 0.044818148016929626
iteration 13, loss = 0.03947984427213669
iteration 14, loss = 0.052498120814561844
iteration 15, loss = 0.062398262321949005
iteration 16, loss = 0.055201057344675064
iteration 17, loss = 0.051657333970069885
iteration 18, loss = 0.04387551173567772
iteration 19, loss = 0.03567524999380112
iteration 20, loss = 0.03977375477552414
iteration 21, loss = 0.0440199188888073
iteration 22, loss = 0.044691313058137894
iteration 23, loss = 0.05501168966293335
iteration 24, loss = 0.06082850694656372
iteration 25, loss = 0.06014856696128845
iteration 26, loss = 0.0390753448009491
iteration 27, loss = 0.039260782301425934
iteration 28, loss = 0.04731679707765579
iteration 29, loss = 0.050837546586990356
iteration 30, loss = 0.05445607006549835
iteration 31, loss = 0.037160176783800125
iteration 32, loss = 0.036075953394174576
iteration 33, loss = 0.025675131008028984
iteration 34, loss = 0.08262550830841064
iteration 35, loss = 0.065622478723526
iteration 36, loss = 0.050804972648620605
iteration 37, loss = 0.04749501869082451
iteration 38, loss = 0.0148647241294384
iteration 39, loss = 0.02391625940799713
iteration 40, loss = 0.04540581256151199
iteration 41, loss = 0.019695378839969635
iteration 42, loss = 0.057503875344991684
iteration 43, loss = 0.049185190349817276
iteration 44, loss = 0.07785934209823608
iteration 45, loss = 0.032581739127635956
iteration 46, loss = 0.05656169354915619
iteration 47, loss = 0.020531821995973587
iteration 48, loss = 0.04527461156249046
iteration 49, loss = 0.04790852963924408
iteration 50, loss = 0.057056620717048645
iteration 51, loss = 0.07863107323646545
iteration 52, loss = 0.0308334119617939
iteration 53, loss = 0.04284052178263664
iteration 54, loss = 0.050685133785009384
iteration 55, loss = 0.036771178245544434
iteration 56, loss = 0.036504048854112625
iteration 57, loss = 0.0481283999979496
iteration 58, loss = 0.06201109290122986
iteration 59, loss = 0.01350541040301323
iteration 60, loss = 0.049637842923402786
iteration 61, loss = 0.0660533681511879
iteration 62, loss = 0.029654642567038536
iteration 63, loss = 0.04712711647152901
iteration 64, loss = 0.07239893078804016
iteration 65, loss = 0.02885451540350914
iteration 66, loss = 0.01606963947415352
iteration 67, loss = 0.05811949819326401
iteration 68, loss = 0.029238933697342873
iteration 69, loss = 0.02310360223054886
iteration 70, loss = 0.0484127514064312
iteration 71, loss = 0.051376134157180786
iteration 72, loss = 0.037452876567840576
iteration 73, loss = 0.04132247343659401
iteration 74, loss = 0.06254210323095322
iteration 75, loss = 0.02992093190550804
iteration 76, loss = 0.06678472459316254
iteration 77, loss = 0.01754697598516941
iteration 78, loss = 0.02332986332476139
iteration 79, loss = 0.025903820991516113
iteration 80, loss = 0.05679820105433464
iteration 81, loss = 0.02816801331937313
iteration 82, loss = 0.06484031677246094
iteration 83, loss = 0.053694434463977814
iteration 84, loss = 0.024323314428329468
iteration 85, loss = 0.05353192612528801
iteration 86, loss = 0.04069703072309494
iteration 87, loss = 0.02238198183476925
iteration 88, loss = 0.08240889012813568
iteration 89, loss = 0.07126142084598541
iteration 90, loss = 0.043276287615299225
iteration 91, loss = 0.049706049263477325
iteration 92, loss = 0.0200925525277853
iteration 93, loss = 0.025348583236336708
iteration 94, loss = 0.05450879782438278
iteration 95, loss = 0.05405651777982712
iteration 96, loss = 0.06407149881124496
iteration 97, loss = 0.028391364961862564
iteration 98, loss = 0.05578746274113655
iteration 99, loss = 0.02691756933927536
iteration 100, loss = 0.043997831642627716
iteration 101, loss = 0.048967596143484116
iteration 102, loss = 0.013293089345097542
iteration 103, loss = 0.035200707614421844
iteration 104, loss = 0.04478522762656212
iteration 105, loss = 0.046798355877399445
iteration 106, loss = 0.029682215303182602
iteration 107, loss = 0.04119053855538368
iteration 108, loss = 0.02458706684410572
iteration 109, loss = 0.05635446310043335
iteration 110, loss = 0.0738927498459816
iteration 111, loss = 0.04301939532160759
iteration 112, loss = 0.060847047716379166
iteration 113, loss = 0.02739444002509117
iteration 114, loss = 0.03620715066790581
iteration 115, loss = 0.06792657822370529
iteration 116, loss = 0.034056514501571655
iteration 117, loss = 0.04101095721125603
iteration 118, loss = 0.05321257561445236
iteration 119, loss = 0.06157558038830757
iteration 120, loss = 0.012373154982924461
iteration 121, loss = 0.0548149012029171
iteration 122, loss = 0.04835335537791252
iteration 123, loss = 0.07171697914600372
iteration 124, loss = 0.023916933685541153
iteration 125, loss = 0.05740412697196007
iteration 126, loss = 0.04169449210166931
iteration 127, loss = 0.0592542439699173
iteration 128, loss = 0.025237897410988808
iteration 129, loss = 0.030860166996717453
iteration 130, loss = 0.12644900381565094
iteration 131, loss = 0.05043666064739227
iteration 132, loss = 0.04588142782449722
iteration 133, loss = 0.028129613026976585
iteration 134, loss = 0.031140414997935295
iteration 135, loss = 0.03277644142508507
iteration 136, loss = 0.0645512193441391
iteration 137, loss = 0.039359159767627716
iteration 138, loss = 0.048739880323410034
iteration 139, loss = 0.08446922898292542
iteration 140, loss = 0.045198626816272736
iteration 141, loss = 0.03885025903582573
iteration 142, loss = 0.02859782986342907
iteration 143, loss = 0.028232283890247345
iteration 144, loss = 0.02499997243285179
iteration 145, loss = 0.03159402310848236
iteration 146, loss = 0.048206083476543427
iteration 147, loss = 0.05084235966205597
iteration 148, loss = 0.04339217022061348
iteration 149, loss = 0.03773346543312073
iteration 150, loss = 0.04792773723602295
iteration 151, loss = 0.047031890600919724
iteration 152, loss = 0.03744887188076973
iteration 153, loss = 0.02094167470932007
iteration 154, loss = 0.03588711470365524
iteration 155, loss = 0.02123096026480198
iteration 156, loss = 0.03693036362528801
iteration 157, loss = 0.03784388676285744
iteration 158, loss = 0.039782263338565826
iteration 159, loss = 0.07034516334533691
iteration 160, loss = 0.032462552189826965
iteration 161, loss = 0.054987646639347076
iteration 162, loss = 0.04689175635576248
iteration 163, loss = 0.023723596706986427
iteration 164, loss = 0.023499801754951477
iteration 165, loss = 0.0746612697839737
iteration 166, loss = 0.038144081830978394
iteration 167, loss = 0.016953062266111374
iteration 168, loss = 0.03221261501312256
iteration 169, loss = 0.044332556426525116
iteration 170, loss = 0.03737344965338707
iteration 171, loss = 0.06695470958948135
iteration 172, loss = 0.027549300342798233
iteration 173, loss = 0.042236506938934326
iteration 174, loss = 0.05548335984349251
iteration 175, loss = 0.049703747034072876
iteration 176, loss = 0.06492026150226593
iteration 177, loss = 0.055882129818201065
iteration 178, loss = 0.052662190049886703
iteration 179, loss = 0.042267292737960815
iteration 180, loss = 0.042912695556879044
iteration 181, loss = 0.02835397981107235
iteration 182, loss = 0.027722030878067017
iteration 183, loss = 0.03093327023088932
iteration 184, loss = 0.03882601484656334
iteration 185, loss = 0.029533889144659042
iteration 186, loss = 0.014839867129921913
iteration 187, loss = 0.03692419081926346
iteration 188, loss = 0.04971841350197792
iteration 189, loss = 0.07053624838590622
iteration 190, loss = 0.03149382397532463
iteration 191, loss = 0.028328407555818558
iteration 192, loss = 0.043599117547273636
iteration 193, loss = 0.027179136872291565
iteration 194, loss = 0.046333976089954376
iteration 195, loss = 0.019997520372271538
iteration 196, loss = 0.13556572794914246
iteration 197, loss = 0.03609950467944145
iteration 198, loss = 0.026623133569955826
iteration 199, loss = 0.033702097833156586
iteration 200, loss = 0.014476335607469082
iteration 201, loss = 0.03595734015107155
iteration 202, loss = 0.07639814913272858
iteration 203, loss = 0.020672382786870003
iteration 204, loss = 0.06786102056503296
iteration 205, loss = 0.031201237812638283
iteration 206, loss = 0.03427247330546379
iteration 207, loss = 0.04385427013039589
iteration 208, loss = 0.048824094235897064
iteration 209, loss = 0.03620915114879608
iteration 210, loss = 0.056381676346063614
iteration 211, loss = 0.03657839447259903
iteration 212, loss = 0.0433955043554306
iteration 213, loss = 0.020797131583094597
iteration 214, loss = 0.03430170565843582
iteration 215, loss = 0.028737381100654602
iteration 216, loss = 0.03944819048047066
iteration 217, loss = 0.05570751428604126
iteration 218, loss = 0.056284282356500626
iteration 219, loss = 0.03428393229842186
iteration 220, loss = 0.05852094665169716
iteration 221, loss = 0.04314520210027695
iteration 222, loss = 0.04354177042841911
iteration 223, loss = 0.054701484739780426
iteration 224, loss = 0.02901540696620941
iteration 225, loss = 0.029691101983189583
iteration 226, loss = 0.031683359295129776
iteration 227, loss = 0.028149127960205078
iteration 228, loss = 0.03189463913440704
iteration 229, loss = 0.04242877662181854
iteration 230, loss = 0.027546130120754242
iteration 231, loss = 0.08232057839632034
iteration 232, loss = 0.040987227112054825
iteration 233, loss = 0.04679504781961441
iteration 234, loss = 0.050572752952575684
iteration 235, loss = 0.0212039053440094
iteration 236, loss = 0.01722395233809948
iteration 237, loss = 0.030935432761907578
iteration 238, loss = 0.05317588150501251
iteration 239, loss = 0.03686665743589401
iteration 240, loss = 0.02586212381720543
iteration 241, loss = 0.02603098191320896
iteration 242, loss = 0.04054604470729828
iteration 243, loss = 0.056480370461940765
iteration 244, loss = 0.02914859913289547
iteration 245, loss = 0.045685380697250366
iteration 246, loss = 0.05126611515879631
iteration 247, loss = 0.02550850249826908
iteration 248, loss = 0.07923291623592377
iteration 249, loss = 0.02907518856227398
iteration 250, loss = 0.032594628632068634
iteration 251, loss = 0.009590478613972664
iteration 252, loss = 0.056997139006853104
iteration 253, loss = 0.046242065727710724
iteration 254, loss = 0.03329652175307274
iteration 255, loss = 0.05556867644190788
iteration 256, loss = 0.02584303729236126
iteration 257, loss = 0.05396744981408119
iteration 258, loss = 0.01594575121998787
iteration 259, loss = 0.05865965038537979
iteration 260, loss = 0.05282590538263321
iteration 261, loss = 0.015961550176143646
iteration 262, loss = 0.028368402272462845
iteration 263, loss = 0.04682214930653572
iteration 264, loss = 0.04234715551137924
iteration 265, loss = 0.025177214294672012
iteration 266, loss = 0.02276954986155033
iteration 267, loss = 0.027799420058727264
iteration 268, loss = 0.03714841604232788
iteration 269, loss = 0.026913505047559738
iteration 270, loss = 0.017677174881100655
iteration 271, loss = 0.03464231640100479
iteration 272, loss = 0.03129119798541069
iteration 273, loss = 0.011487623676657677
iteration 274, loss = 0.04331406205892563
iteration 275, loss = 0.011269614100456238
iteration 276, loss = 0.03595995903015137
iteration 277, loss = 0.034546807408332825
iteration 278, loss = 0.038669899106025696
iteration 279, loss = 0.019953390583395958
iteration 280, loss = 0.04974471405148506
iteration 281, loss = 0.054097920656204224
iteration 282, loss = 0.07010245323181152
iteration 283, loss = 0.061921607702970505
iteration 284, loss = 0.011172670871019363
iteration 285, loss = 0.047477174550294876
iteration 286, loss = 0.061812736093997955
iteration 287, loss = 0.04065448045730591
iteration 288, loss = 0.035859011113643646
iteration 289, loss = 0.04558119177818298
iteration 290, loss = 0.02193954586982727
iteration 291, loss = 0.021006811410188675
iteration 292, loss = 0.0376303568482399
iteration 293, loss = 0.05341741815209389
iteration 294, loss = 0.02897394634783268
iteration 295, loss = 0.04128365218639374
iteration 296, loss = 0.01928163506090641
iteration 297, loss = 0.04441278427839279
iteration 298, loss = 0.01701049879193306
iteration 299, loss = 0.029690872877836227
iteration 300, loss = 0.04984402656555176

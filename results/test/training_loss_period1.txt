iteration 1, loss = 5.754628658294678
iteration 2, loss = 5.72166109085083
iteration 3, loss = 5.539911270141602
iteration 4, loss = 5.355108737945557
iteration 5, loss = 5.079938888549805
iteration 6, loss = 4.945268630981445
iteration 7, loss = 4.665729522705078
iteration 8, loss = 4.349266052246094
iteration 9, loss = 4.137258529663086
iteration 10, loss = 3.905017137527466
iteration 11, loss = 3.606938362121582
iteration 12, loss = 3.2816874980926514
iteration 13, loss = 2.8358676433563232
iteration 14, loss = 2.75897479057312
iteration 15, loss = 2.402752161026001
iteration 16, loss = 2.298832654953003
iteration 17, loss = 2.140068292617798
iteration 18, loss = 1.8140054941177368
iteration 19, loss = 1.5993990898132324
iteration 20, loss = 1.4506337642669678
iteration 21, loss = 1.3862667083740234
iteration 22, loss = 1.287135124206543
iteration 23, loss = 1.331193447113037
iteration 24, loss = 1.0041968822479248
iteration 25, loss = 1.052549123764038
iteration 26, loss = 0.9898443222045898
iteration 27, loss = 0.9213857650756836
iteration 28, loss = 0.9054391384124756
iteration 29, loss = 0.7823925018310547
iteration 30, loss = 0.8236023783683777
iteration 31, loss = 0.7785509824752808
iteration 32, loss = 0.7320036292076111
iteration 33, loss = 0.8012703657150269
iteration 34, loss = 0.7986981868743896
iteration 35, loss = 0.7450689077377319
iteration 36, loss = 0.8189265727996826
iteration 37, loss = 0.7072519063949585
iteration 38, loss = 0.7265909910202026
iteration 39, loss = 0.7251620292663574
iteration 40, loss = 0.6678683757781982
iteration 41, loss = 0.6951469779014587
iteration 42, loss = 0.7086724638938904
iteration 43, loss = 0.7268513441085815
iteration 44, loss = 0.756409227848053
iteration 45, loss = 0.6826242804527283
iteration 46, loss = 0.6964374780654907
iteration 47, loss = 0.7776402235031128
iteration 48, loss = 0.6828102469444275
iteration 49, loss = 0.6776682734489441
iteration 50, loss = 0.7153112292289734
iteration 51, loss = 0.7006634473800659
iteration 52, loss = 0.7261448502540588
iteration 53, loss = 0.6875349283218384
iteration 54, loss = 0.7075343728065491
iteration 55, loss = 0.7626829743385315
iteration 56, loss = 0.6729478240013123
iteration 57, loss = 0.6929880976676941
iteration 58, loss = 0.7008534073829651
iteration 59, loss = 0.662480354309082
iteration 60, loss = 0.6861867904663086
iteration 61, loss = 0.694851815700531
iteration 62, loss = 0.7546797394752502
iteration 63, loss = 0.6788234710693359
iteration 64, loss = 0.7449479699134827
iteration 65, loss = 0.7329707145690918
iteration 66, loss = 0.7258971333503723
iteration 67, loss = 0.7058470845222473
iteration 68, loss = 0.6645069122314453
iteration 69, loss = 0.7143101692199707
iteration 70, loss = 0.6656244993209839
iteration 71, loss = 0.6813696026802063
iteration 72, loss = 0.6542780995368958
iteration 73, loss = 0.6536791324615479
iteration 74, loss = 0.685447096824646
iteration 75, loss = 0.6524420380592346
iteration 76, loss = 0.6620274782180786
iteration 77, loss = 0.6801276206970215
iteration 78, loss = 0.7262403964996338
iteration 79, loss = 0.6822364926338196
iteration 80, loss = 0.7170671224594116
iteration 81, loss = 0.7201148271560669
iteration 82, loss = 0.6715339422225952
iteration 83, loss = 0.680513858795166
iteration 84, loss = 0.6492273807525635
iteration 85, loss = 0.6608039140701294
iteration 86, loss = 0.7010639905929565
iteration 87, loss = 0.696950376033783
iteration 88, loss = 0.6797821521759033
iteration 89, loss = 0.7532099485397339
iteration 90, loss = 0.6843333840370178
iteration 91, loss = 0.6763086915016174
iteration 92, loss = 0.6364485025405884
iteration 93, loss = 0.7064186334609985
iteration 94, loss = 0.6994645595550537
iteration 95, loss = 0.7034177780151367
iteration 96, loss = 0.6866008639335632
iteration 97, loss = 0.6472309231758118
iteration 98, loss = 0.7441859841346741
iteration 99, loss = 0.6432381868362427
iteration 100, loss = 0.6773359775543213
iteration 101, loss = 0.6862092614173889
iteration 102, loss = 0.6442762017250061
iteration 103, loss = 0.7240384221076965
iteration 104, loss = 0.6643784642219543
iteration 105, loss = 0.7190146446228027
iteration 106, loss = 0.6557804942131042
iteration 107, loss = 0.6482487916946411
iteration 108, loss = 0.6965757012367249
iteration 109, loss = 0.6430121064186096
iteration 110, loss = 0.643504798412323
iteration 111, loss = 0.6669143438339233
iteration 112, loss = 0.6590303778648376
iteration 113, loss = 0.620993435382843
iteration 114, loss = 0.656268835067749
iteration 115, loss = 0.6775214076042175
iteration 116, loss = 0.6451179385185242
iteration 117, loss = 0.6986314654350281
iteration 118, loss = 0.6452776789665222
iteration 119, loss = 0.6224107146263123
iteration 120, loss = 0.6247767210006714
iteration 121, loss = 0.6225215792655945
iteration 122, loss = 0.632408082485199
iteration 123, loss = 0.6005573272705078
iteration 124, loss = 0.6541462540626526
iteration 125, loss = 0.6533003449440002
iteration 126, loss = 0.640425443649292
iteration 127, loss = 0.6485212445259094
iteration 128, loss = 0.6829460859298706
iteration 129, loss = 0.6477059721946716
iteration 130, loss = 0.5981094837188721
iteration 131, loss = 0.6404236555099487
iteration 132, loss = 0.679690957069397
iteration 133, loss = 0.6748512983322144
iteration 134, loss = 0.7048457264900208
iteration 135, loss = 0.6287888288497925
iteration 136, loss = 0.625338077545166
iteration 137, loss = 0.6153270602226257
iteration 138, loss = 0.6164638996124268
iteration 139, loss = 0.6977739334106445
iteration 140, loss = 0.6609750986099243
iteration 141, loss = 0.6027266383171082
iteration 142, loss = 0.5905758142471313
iteration 143, loss = 0.6782934665679932
iteration 144, loss = 0.5945543050765991
iteration 145, loss = 0.6539624333381653
iteration 146, loss = 0.602905809879303
iteration 147, loss = 0.615074634552002
iteration 148, loss = 0.6050703525543213
iteration 149, loss = 0.5854625701904297
iteration 150, loss = 0.6145907044410706
iteration 151, loss = 0.5917768478393555
iteration 152, loss = 0.6051115989685059
iteration 153, loss = 0.6561388969421387
iteration 154, loss = 0.6379230618476868
iteration 155, loss = 0.6365743279457092
iteration 156, loss = 0.6313609480857849
iteration 157, loss = 0.6402899026870728
iteration 158, loss = 0.6602920293807983
iteration 159, loss = 0.5991347432136536
iteration 160, loss = 0.6082816123962402
iteration 161, loss = 0.6226251125335693
iteration 162, loss = 0.6150168180465698
iteration 163, loss = 0.6708121299743652
iteration 164, loss = 0.595092236995697
iteration 165, loss = 0.565524697303772
iteration 166, loss = 0.6302902698516846
iteration 167, loss = 0.5622946619987488
iteration 168, loss = 0.6704411506652832
iteration 169, loss = 0.5502907037734985
iteration 170, loss = 0.5501085519790649
iteration 171, loss = 0.6969849467277527
iteration 172, loss = 0.5914919972419739
iteration 173, loss = 0.6178208589553833
iteration 174, loss = 0.6092327833175659
iteration 175, loss = 0.6220265626907349
iteration 176, loss = 0.588672399520874
iteration 177, loss = 0.5960314273834229
iteration 178, loss = 0.5839873552322388
iteration 179, loss = 0.5777665376663208
iteration 180, loss = 0.649782657623291
iteration 181, loss = 0.6524553298950195
iteration 182, loss = 0.5917061567306519
iteration 183, loss = 0.629286527633667
iteration 184, loss = 0.6612471342086792
iteration 185, loss = 0.5674574375152588
iteration 186, loss = 0.5764885544776917
iteration 187, loss = 0.613372802734375
iteration 188, loss = 0.5854395627975464
iteration 189, loss = 0.5427175164222717
iteration 190, loss = 0.6184530258178711
iteration 191, loss = 0.5600165724754333
iteration 192, loss = 0.5351922512054443
iteration 193, loss = 0.5919772386550903
iteration 194, loss = 0.547297477722168
iteration 195, loss = 0.5884144902229309
iteration 196, loss = 0.5615586638450623
iteration 197, loss = 0.5462068915367126
iteration 198, loss = 0.5700523853302002
iteration 199, loss = 0.5516941547393799
iteration 200, loss = 0.538226842880249
iteration 201, loss = 0.5472930073738098
iteration 202, loss = 0.5379695296287537
iteration 203, loss = 0.6331990361213684
iteration 204, loss = 0.5857844352722168
iteration 205, loss = 0.6153744459152222
iteration 206, loss = 0.6039031744003296
iteration 207, loss = 0.5425018668174744
iteration 208, loss = 0.5719067454338074
iteration 209, loss = 0.5997986197471619
iteration 210, loss = 0.6010599732398987
iteration 211, loss = 0.5769689083099365
iteration 212, loss = 0.5542921423912048
iteration 213, loss = 0.5288857221603394
iteration 214, loss = 0.5668594837188721
iteration 215, loss = 0.5328918695449829
iteration 216, loss = 0.5176863670349121
iteration 217, loss = 0.5776770114898682
iteration 218, loss = 0.5580546855926514
iteration 219, loss = 0.5265066027641296
iteration 220, loss = 0.5564868450164795
iteration 221, loss = 0.532796323299408
iteration 222, loss = 0.619898796081543
iteration 223, loss = 0.5403512120246887
iteration 224, loss = 0.5141193270683289
iteration 225, loss = 0.5583025813102722
iteration 226, loss = 0.506641685962677
iteration 227, loss = 0.5457352995872498
iteration 228, loss = 0.6184178590774536
iteration 229, loss = 0.48064059019088745
iteration 230, loss = 0.5761786103248596
iteration 231, loss = 0.5161892175674438
iteration 232, loss = 0.49352121353149414
iteration 233, loss = 0.5373717546463013
iteration 234, loss = 0.5026717185974121
iteration 235, loss = 0.5277799367904663
iteration 236, loss = 0.49598419666290283
iteration 237, loss = 0.5319099426269531
iteration 238, loss = 0.5343995094299316
iteration 239, loss = 0.5509805083274841
iteration 240, loss = 0.5224805474281311
iteration 241, loss = 0.5353923439979553
iteration 242, loss = 0.5400787591934204
iteration 243, loss = 0.570368766784668
iteration 244, loss = 0.5217669606208801
iteration 245, loss = 0.49935200810432434
iteration 246, loss = 0.48963662981987
iteration 247, loss = 0.5636034607887268
iteration 248, loss = 0.5217901468276978
iteration 249, loss = 0.530401349067688
iteration 250, loss = 0.5860697031021118
iteration 251, loss = 0.47717079520225525
iteration 252, loss = 0.541849672794342
iteration 253, loss = 0.47813376784324646
iteration 254, loss = 0.5233180522918701
iteration 255, loss = 0.5042649507522583
iteration 256, loss = 0.510993480682373
iteration 257, loss = 0.49154210090637207
iteration 258, loss = 0.5219231843948364
iteration 259, loss = 0.49885526299476624
iteration 260, loss = 0.6470829844474792
iteration 261, loss = 0.4819789230823517
iteration 262, loss = 0.4996486306190491
iteration 263, loss = 0.44771161675453186
iteration 264, loss = 0.458354115486145
iteration 265, loss = 0.47524774074554443
iteration 266, loss = 0.4697411358356476
iteration 267, loss = 0.48906898498535156
iteration 268, loss = 0.5280959606170654
iteration 269, loss = 0.48261648416519165
iteration 270, loss = 0.49879783391952515
iteration 271, loss = 0.5977358222007751
iteration 272, loss = 0.5777876973152161
iteration 273, loss = 0.4658113420009613
iteration 274, loss = 0.5578295588493347
iteration 275, loss = 0.4483814835548401
iteration 276, loss = 0.4914303123950958
iteration 277, loss = 0.4782302677631378
iteration 278, loss = 0.47584137320518494
iteration 279, loss = 0.5256263613700867
iteration 280, loss = 0.5245171189308167
iteration 281, loss = 0.49271485209465027
iteration 282, loss = 0.45097780227661133
iteration 283, loss = 0.44127148389816284
iteration 284, loss = 0.47788360714912415
iteration 285, loss = 0.5360194444656372
iteration 286, loss = 0.4875226318836212
iteration 287, loss = 0.5314165353775024
iteration 288, loss = 0.5497485995292664
iteration 289, loss = 0.43522220849990845
iteration 290, loss = 0.48578569293022156
iteration 291, loss = 0.4053523540496826
iteration 292, loss = 0.4310401678085327
iteration 293, loss = 0.4293726980686188
iteration 294, loss = 0.42520713806152344
iteration 295, loss = 0.47438594698905945
iteration 296, loss = 0.45918527245521545
iteration 297, loss = 0.4607449173927307
iteration 298, loss = 0.4310155510902405
iteration 299, loss = 0.444551944732666
iteration 300, loss = 0.4595438539981842
iteration 1, loss = 0.455825537443161
iteration 2, loss = 0.4289420247077942
iteration 3, loss = 0.45822572708129883
iteration 4, loss = 0.5272500514984131
iteration 5, loss = 0.45048442482948303
iteration 6, loss = 0.4285919964313507
iteration 7, loss = 0.42023855447769165
iteration 8, loss = 0.4479043185710907
iteration 9, loss = 0.4442310929298401
iteration 10, loss = 0.43209829926490784
iteration 11, loss = 0.40492165088653564
iteration 12, loss = 0.4688671827316284
iteration 13, loss = 0.46581971645355225
iteration 14, loss = 0.3979453444480896
iteration 15, loss = 0.4134269952774048
iteration 16, loss = 0.41420212388038635
iteration 17, loss = 0.44308871030807495
iteration 18, loss = 0.42262715101242065
iteration 19, loss = 0.49082931876182556
iteration 20, loss = 0.46126994490623474
iteration 21, loss = 0.3938733637332916
iteration 22, loss = 0.49172791838645935
iteration 23, loss = 0.4167928993701935
iteration 24, loss = 0.4613083004951477
iteration 25, loss = 0.42018961906433105
iteration 26, loss = 0.41523277759552
iteration 27, loss = 0.3889826536178589
iteration 28, loss = 0.3915824294090271
iteration 29, loss = 0.44084295630455017
iteration 30, loss = 0.4344550669193268
iteration 31, loss = 0.46182435750961304
iteration 32, loss = 0.4163990020751953
iteration 33, loss = 0.3898297846317291
iteration 34, loss = 0.4433823823928833
iteration 35, loss = 0.36836642026901245
iteration 36, loss = 0.3836211860179901
iteration 37, loss = 0.43556150794029236
iteration 38, loss = 0.517683744430542
iteration 39, loss = 0.37574154138565063
iteration 40, loss = 0.3854692876338959
iteration 41, loss = 0.35113686323165894
iteration 42, loss = 0.3632263243198395
iteration 43, loss = 0.3595238924026489
iteration 44, loss = 0.3594283163547516
iteration 45, loss = 0.3828977942466736
iteration 46, loss = 0.5188873410224915
iteration 47, loss = 0.42283886671066284
iteration 48, loss = 0.36083120107650757
iteration 49, loss = 0.35002827644348145
iteration 50, loss = 0.3683009445667267
iteration 51, loss = 0.4306630492210388
iteration 52, loss = 0.3832857310771942
iteration 53, loss = 0.4522455632686615
iteration 54, loss = 0.3340602219104767
iteration 55, loss = 0.48451581597328186
iteration 56, loss = 0.48914363980293274
iteration 57, loss = 0.3546282947063446
iteration 58, loss = 0.564003586769104
iteration 59, loss = 0.45366597175598145
iteration 60, loss = 0.5076960325241089
iteration 61, loss = 0.33234623074531555
iteration 62, loss = 0.40320461988449097
iteration 63, loss = 0.370571106672287
iteration 64, loss = 0.4234084188938141
iteration 65, loss = 0.3380618095397949
iteration 66, loss = 0.38872212171554565
iteration 67, loss = 0.3835456073284149
iteration 68, loss = 0.387824147939682
iteration 69, loss = 0.3448804020881653
iteration 70, loss = 0.3836396634578705
iteration 71, loss = 0.4262542128562927
iteration 72, loss = 0.31475380063056946
iteration 73, loss = 0.43373608589172363
iteration 74, loss = 0.4399702548980713
iteration 75, loss = 0.35322898626327515
iteration 76, loss = 0.35716983675956726
iteration 77, loss = 0.49397343397140503
iteration 78, loss = 0.3359932601451874
iteration 79, loss = 0.3710656762123108
iteration 80, loss = 0.3907219469547272
iteration 81, loss = 0.36640068888664246
iteration 82, loss = 0.40354785323143005
iteration 83, loss = 0.3827323019504547
iteration 84, loss = 0.37464430928230286
iteration 85, loss = 0.4288756847381592
iteration 86, loss = 0.3818800449371338
iteration 87, loss = 0.34573251008987427
iteration 88, loss = 0.30196142196655273
iteration 89, loss = 0.29255440831184387
iteration 90, loss = 0.32807445526123047
iteration 91, loss = 0.3511357605457306
iteration 92, loss = 0.3683965504169464
iteration 93, loss = 0.4271329641342163
iteration 94, loss = 0.3235440254211426
iteration 95, loss = 0.46708518266677856
iteration 96, loss = 0.31883981823921204
iteration 97, loss = 0.32226112484931946
iteration 98, loss = 0.3078102767467499
iteration 99, loss = 0.36041927337646484
iteration 100, loss = 0.3009902834892273
iteration 101, loss = 0.3431016802787781
iteration 102, loss = 0.2901725769042969
iteration 103, loss = 0.29465675354003906
iteration 104, loss = 0.3428415060043335
iteration 105, loss = 0.3131130337715149
iteration 106, loss = 0.312725692987442
iteration 107, loss = 0.3355320394039154
iteration 108, loss = 0.29982876777648926
iteration 109, loss = 0.43728914856910706
iteration 110, loss = 0.34792616963386536
iteration 111, loss = 0.29188114404678345
iteration 112, loss = 0.3345937430858612
iteration 113, loss = 0.28976812958717346
iteration 114, loss = 0.2858734428882599
iteration 115, loss = 0.37699979543685913
iteration 116, loss = 0.3970348536968231
iteration 117, loss = 0.4233045279979706
iteration 118, loss = 0.3824179470539093
iteration 119, loss = 0.3007276952266693
iteration 120, loss = 0.33727702498435974
iteration 121, loss = 0.323885440826416
iteration 122, loss = 0.2921920418739319
iteration 123, loss = 0.3247177004814148
iteration 124, loss = 0.3271167278289795
iteration 125, loss = 0.31671255826950073
iteration 126, loss = 0.2908179759979248
iteration 127, loss = 0.2740047574043274
iteration 128, loss = 0.3082273006439209
iteration 129, loss = 0.2900020182132721
iteration 130, loss = 0.36940479278564453
iteration 131, loss = 0.27904534339904785
iteration 132, loss = 0.30819278955459595
iteration 133, loss = 0.33437442779541016
iteration 134, loss = 0.2545558512210846
iteration 135, loss = 0.2708192467689514
iteration 136, loss = 0.2687106430530548
iteration 137, loss = 0.27664583921432495
iteration 138, loss = 0.30963462591171265
iteration 139, loss = 0.26151251792907715
iteration 140, loss = 0.2792982757091522
iteration 141, loss = 0.304402232170105
iteration 142, loss = 0.3542686104774475
iteration 143, loss = 0.3092559576034546
iteration 144, loss = 0.292660117149353
iteration 145, loss = 0.33208703994750977
iteration 146, loss = 0.3914603590965271
iteration 147, loss = 0.31486913561820984
iteration 148, loss = 0.28383010625839233
iteration 149, loss = 0.3226267397403717
iteration 150, loss = 0.24570059776306152
iteration 151, loss = 0.31981250643730164
iteration 152, loss = 0.31197285652160645
iteration 153, loss = 0.25279709696769714
iteration 154, loss = 0.25109609961509705
iteration 155, loss = 0.2839181423187256
iteration 156, loss = 0.3290291726589203
iteration 157, loss = 0.29625725746154785
iteration 158, loss = 0.2646287977695465
iteration 159, loss = 0.3308615982532501
iteration 160, loss = 0.26370760798454285
iteration 161, loss = 0.24192814528942108
iteration 162, loss = 0.30718106031417847
iteration 163, loss = 0.23616044223308563
iteration 164, loss = 0.3376580476760864
iteration 165, loss = 0.3615354895591736
iteration 166, loss = 0.34608662128448486
iteration 167, loss = 0.2849840819835663
iteration 168, loss = 0.2334531992673874
iteration 169, loss = 0.22911907732486725
iteration 170, loss = 0.2613077163696289
iteration 171, loss = 0.30593398213386536
iteration 172, loss = 0.24339987337589264
iteration 173, loss = 0.3129211962223053
iteration 174, loss = 0.26726099848747253
iteration 175, loss = 0.2639762759208679
iteration 176, loss = 0.28693887591362
iteration 177, loss = 0.27462613582611084
iteration 178, loss = 0.28022778034210205
iteration 179, loss = 0.22615590691566467
iteration 180, loss = 0.261616587638855
iteration 181, loss = 0.24901346862316132
iteration 182, loss = 0.2755179703235626
iteration 183, loss = 0.2914150357246399
iteration 184, loss = 0.2577527165412903
iteration 185, loss = 0.26198822259902954
iteration 186, loss = 0.2808399200439453
iteration 187, loss = 0.3306253254413605
iteration 188, loss = 0.35677748918533325
iteration 189, loss = 0.28994759917259216
iteration 190, loss = 0.2359434962272644
iteration 191, loss = 0.27628660202026367
iteration 192, loss = 0.3134170174598694
iteration 193, loss = 0.22925856709480286
iteration 194, loss = 0.27711281180381775
iteration 195, loss = 0.23522120714187622
iteration 196, loss = 0.21266376972198486
iteration 197, loss = 0.21001304686069489
iteration 198, loss = 0.21680386364459991
iteration 199, loss = 0.3386712670326233
iteration 200, loss = 0.21675485372543335
iteration 201, loss = 0.3047623038291931
iteration 202, loss = 0.2302093505859375
iteration 203, loss = 0.2975276708602905
iteration 204, loss = 0.34372395277023315
iteration 205, loss = 0.251542866230011
iteration 206, loss = 0.23018643260002136
iteration 207, loss = 0.2556024491786957
iteration 208, loss = 0.21443608403205872
iteration 209, loss = 0.22413375973701477
iteration 210, loss = 0.20241491496562958
iteration 211, loss = 0.2482752501964569
iteration 212, loss = 0.20295220613479614
iteration 213, loss = 0.1939087063074112
iteration 214, loss = 0.29552870988845825
iteration 215, loss = 0.2021350860595703
iteration 216, loss = 0.19223617017269135
iteration 217, loss = 0.2365359365940094
iteration 218, loss = 0.21534766256809235
iteration 219, loss = 0.23360906541347504
iteration 220, loss = 0.2623457610607147
iteration 221, loss = 0.2043340504169464
iteration 222, loss = 0.22735241055488586
iteration 223, loss = 0.2226550579071045
iteration 224, loss = 0.19829310476779938
iteration 225, loss = 0.20296797156333923
iteration 226, loss = 0.23213449120521545
iteration 227, loss = 0.19486075639724731
iteration 228, loss = 0.19874367117881775
iteration 229, loss = 0.2565116882324219
iteration 230, loss = 0.19033433496952057
iteration 231, loss = 0.21794185042381287
iteration 232, loss = 0.1863451600074768
iteration 233, loss = 0.24280303716659546
iteration 234, loss = 0.2764229476451874
iteration 235, loss = 0.21834707260131836
iteration 236, loss = 0.19885344803333282
iteration 237, loss = 0.21771380305290222
iteration 238, loss = 0.23738424479961395
iteration 239, loss = 0.1886426955461502
iteration 240, loss = 0.2078784853219986
iteration 241, loss = 0.19719141721725464
iteration 242, loss = 0.20639070868492126
iteration 243, loss = 0.20346608757972717
iteration 244, loss = 0.20398849248886108
iteration 245, loss = 0.20595817267894745
iteration 246, loss = 0.1794281303882599
iteration 247, loss = 0.21070459485054016
iteration 248, loss = 0.2537938952445984
iteration 249, loss = 0.18044322729110718
iteration 250, loss = 0.1893928349018097
iteration 251, loss = 0.18311093747615814
iteration 252, loss = 0.17616085708141327
iteration 253, loss = 0.17877371609210968
iteration 254, loss = 0.25526076555252075
iteration 255, loss = 0.16981510818004608
iteration 256, loss = 0.19489717483520508
iteration 257, loss = 0.17408885061740875
iteration 258, loss = 0.18794295191764832
iteration 259, loss = 0.17437246441841125
iteration 260, loss = 0.16827090084552765
iteration 261, loss = 0.20723268389701843
iteration 262, loss = 0.1792088747024536
iteration 263, loss = 0.17387130856513977
iteration 264, loss = 0.16921772062778473
iteration 265, loss = 0.2165033519268036
iteration 266, loss = 0.18815146386623383
iteration 267, loss = 0.20134632289409637
iteration 268, loss = 0.20586247742176056
iteration 269, loss = 0.16801795363426208
iteration 270, loss = 0.19101984798908234
iteration 271, loss = 0.20194555819034576
iteration 272, loss = 0.20992115139961243
iteration 273, loss = 0.2673197388648987
iteration 274, loss = 0.17789869010448456
iteration 275, loss = 0.1647765338420868
iteration 276, loss = 0.15723952651023865
iteration 277, loss = 0.2158619910478592
iteration 278, loss = 0.19702163338661194
iteration 279, loss = 0.1840004026889801
iteration 280, loss = 0.163310244679451
iteration 281, loss = 0.1920468807220459
iteration 282, loss = 0.20981459319591522
iteration 283, loss = 0.18665318191051483
iteration 284, loss = 0.15729479491710663
iteration 285, loss = 0.1893664002418518
iteration 286, loss = 0.19985216856002808
iteration 287, loss = 0.17081663012504578
iteration 288, loss = 0.16453500092029572
iteration 289, loss = 0.16863366961479187
iteration 290, loss = 0.2548384368419647
iteration 291, loss = 0.1828651875257492
iteration 292, loss = 0.1486997753381729
iteration 293, loss = 0.24052955210208893
iteration 294, loss = 0.21117758750915527
iteration 295, loss = 0.15385764837265015
iteration 296, loss = 0.17214861512184143
iteration 297, loss = 0.24428656697273254
iteration 298, loss = 0.1838769167661667
iteration 299, loss = 0.1557280719280243
iteration 300, loss = 0.14707112312316895
iteration 1, loss = 0.190824493765831
iteration 2, loss = 0.14442187547683716
iteration 3, loss = 0.15042699873447418
iteration 4, loss = 0.16881129145622253
iteration 5, loss = 0.19889120757579803
iteration 6, loss = 0.2238105982542038
iteration 7, loss = 0.14188548922538757
iteration 8, loss = 0.14173677563667297
iteration 9, loss = 0.19965076446533203
iteration 10, loss = 0.1439507007598877
iteration 11, loss = 0.1628572642803192
iteration 12, loss = 0.21344631910324097
iteration 13, loss = 0.1814589947462082
iteration 14, loss = 0.23731522262096405
iteration 15, loss = 0.14989781379699707
iteration 16, loss = 0.145091712474823
iteration 17, loss = 0.15176282823085785
iteration 18, loss = 0.1910262554883957
iteration 19, loss = 0.30236151814460754
iteration 20, loss = 0.15281686186790466
iteration 21, loss = 0.18077664077281952
iteration 22, loss = 0.14361155033111572
iteration 23, loss = 0.18543019890785217
iteration 24, loss = 0.1488906294107437
iteration 25, loss = 0.16418452560901642
iteration 26, loss = 0.18124939501285553
iteration 27, loss = 0.18120114505290985
iteration 28, loss = 0.22824324667453766
iteration 29, loss = 0.17092959582805634
iteration 30, loss = 0.13026532530784607
iteration 31, loss = 0.1561417430639267
iteration 32, loss = 0.13895879685878754
iteration 33, loss = 0.12727965414524078
iteration 34, loss = 0.14352557063102722
iteration 35, loss = 0.14140240848064423
iteration 36, loss = 0.13940222561359406
iteration 37, loss = 0.19649656116962433
iteration 38, loss = 0.12630197405815125
iteration 39, loss = 0.17239700257778168
iteration 40, loss = 0.21393756568431854
iteration 41, loss = 0.14617861807346344
iteration 42, loss = 0.13929039239883423
iteration 43, loss = 0.138397216796875
iteration 44, loss = 0.1393415778875351
iteration 45, loss = 0.19267091155052185
iteration 46, loss = 0.15358255803585052
iteration 47, loss = 0.13522206246852875
iteration 48, loss = 0.12557484209537506
iteration 49, loss = 0.18357020616531372
iteration 50, loss = 0.13338416814804077
iteration 51, loss = 0.12821446359157562
iteration 52, loss = 0.13792434334754944
iteration 53, loss = 0.13773110508918762
iteration 54, loss = 0.14061935245990753
iteration 55, loss = 0.13250002264976501
iteration 56, loss = 0.14779432117938995
iteration 57, loss = 0.1838507503271103
iteration 58, loss = 0.17682546377182007
iteration 59, loss = 0.2091580033302307
iteration 60, loss = 0.21464884281158447
iteration 61, loss = 0.16366179287433624
iteration 62, loss = 0.1308439075946808
iteration 63, loss = 0.12441904842853546
iteration 64, loss = 0.1141267642378807
iteration 65, loss = 0.12336120754480362
iteration 66, loss = 0.11491026729345322
iteration 67, loss = 0.1316925287246704
iteration 68, loss = 0.16403250396251678
iteration 69, loss = 0.16968798637390137
iteration 70, loss = 0.21210096776485443
iteration 71, loss = 0.1389981508255005
iteration 72, loss = 0.154708594083786
iteration 73, loss = 0.11782562732696533
iteration 74, loss = 0.14042620360851288
iteration 75, loss = 0.17680390179157257
iteration 76, loss = 0.14097729325294495
iteration 77, loss = 0.1441364586353302
iteration 78, loss = 0.11645811051130295
iteration 79, loss = 0.1221282035112381
iteration 80, loss = 0.11438334733247757
iteration 81, loss = 0.11970973759889603
iteration 82, loss = 0.12054666131734848
iteration 83, loss = 0.15224160254001617
iteration 84, loss = 0.2704167664051056
iteration 85, loss = 0.15336142480373383
iteration 86, loss = 0.15633690357208252
iteration 87, loss = 0.13577702641487122
iteration 88, loss = 0.12428648769855499
iteration 89, loss = 0.11460902541875839
iteration 90, loss = 0.11362229287624359
iteration 91, loss = 0.11362207680940628
iteration 92, loss = 0.14425207674503326
iteration 93, loss = 0.13514967262744904
iteration 94, loss = 0.17732743918895721
iteration 95, loss = 0.1547115594148636
iteration 96, loss = 0.1291753649711609
iteration 97, loss = 0.1052863746881485
iteration 98, loss = 0.14806850254535675
iteration 99, loss = 0.18225160241127014
iteration 100, loss = 0.11593867838382721
iteration 101, loss = 0.11326281726360321
iteration 102, loss = 0.15536196529865265
iteration 103, loss = 0.11978629231452942
iteration 104, loss = 0.12059548497200012
iteration 105, loss = 0.12871327996253967
iteration 106, loss = 0.13605520129203796
iteration 107, loss = 0.22156302630901337
iteration 108, loss = 0.13099195063114166
iteration 109, loss = 0.11627545207738876
iteration 110, loss = 0.17956387996673584
iteration 111, loss = 0.11228557676076889
iteration 112, loss = 0.12489078938961029
iteration 113, loss = 0.10556244850158691
iteration 114, loss = 0.10847584903240204
iteration 115, loss = 0.12142109870910645
iteration 116, loss = 0.11218805611133575
iteration 117, loss = 0.12439882755279541
iteration 118, loss = 0.1176772266626358
iteration 119, loss = 0.10916824638843536
iteration 120, loss = 0.10930293053388596
iteration 121, loss = 0.09857587516307831
iteration 122, loss = 0.10673362016677856
iteration 123, loss = 0.09861446171998978
iteration 124, loss = 0.11362290382385254
iteration 125, loss = 0.16718395054340363
iteration 126, loss = 0.10977447777986526
iteration 127, loss = 0.09792806208133698
iteration 128, loss = 0.11167967319488525
iteration 129, loss = 0.10030868649482727
iteration 130, loss = 0.10493095219135284
iteration 131, loss = 0.10089567303657532
iteration 132, loss = 0.09855838865041733
iteration 133, loss = 0.11511202901601791
iteration 134, loss = 0.10045592486858368
iteration 135, loss = 0.09567175805568695
iteration 136, loss = 0.09922745823860168
iteration 137, loss = 0.1020299568772316
iteration 138, loss = 0.11862170696258545
iteration 139, loss = 0.10304920375347137
iteration 140, loss = 0.15816378593444824
iteration 141, loss = 0.10762497782707214
iteration 142, loss = 0.09551387280225754
iteration 143, loss = 0.1362018585205078
iteration 144, loss = 0.09649001806974411
iteration 145, loss = 0.10127826035022736
iteration 146, loss = 0.18679432570934296
iteration 147, loss = 0.09362241625785828
iteration 148, loss = 0.12121890485286713
iteration 149, loss = 0.1015649139881134
iteration 150, loss = 0.1016349270939827
iteration 151, loss = 0.0944884642958641
iteration 152, loss = 0.11295606195926666
iteration 153, loss = 0.09532613307237625
iteration 154, loss = 0.09185085445642471
iteration 155, loss = 0.10041559487581253
iteration 156, loss = 0.12714746594429016
iteration 157, loss = 0.10027779638767242
iteration 158, loss = 0.09130494296550751
iteration 159, loss = 0.10550996661186218
iteration 160, loss = 0.12158088386058807
iteration 161, loss = 0.1044318750500679
iteration 162, loss = 0.13596905767917633
iteration 163, loss = 0.09373617172241211
iteration 164, loss = 0.08847551047801971
iteration 165, loss = 0.08814287930727005
iteration 166, loss = 0.11071968823671341
iteration 167, loss = 0.10340140014886856
iteration 168, loss = 0.1437596082687378
iteration 169, loss = 0.10812520980834961
iteration 170, loss = 0.11754117161035538
iteration 171, loss = 0.11020763218402863
iteration 172, loss = 0.1703929603099823
iteration 173, loss = 0.08474975824356079
iteration 174, loss = 0.12992197275161743
iteration 175, loss = 0.09092351794242859
iteration 176, loss = 0.08626975864171982
iteration 177, loss = 0.10038591176271439
iteration 178, loss = 0.0978320986032486
iteration 179, loss = 0.12642544507980347
iteration 180, loss = 0.10196930915117264
iteration 181, loss = 0.11149182915687561
iteration 182, loss = 0.1171138659119606
iteration 183, loss = 0.12740546464920044
iteration 184, loss = 0.09488855302333832
iteration 185, loss = 0.11867578327655792
iteration 186, loss = 0.09742714464664459
iteration 187, loss = 0.10528972744941711
iteration 188, loss = 0.09355887770652771
iteration 189, loss = 0.09945175051689148
iteration 190, loss = 0.13666470348834991
iteration 191, loss = 0.08724497258663177
iteration 192, loss = 0.11335472017526627
iteration 193, loss = 0.12046676874160767
iteration 194, loss = 0.08585704118013382
iteration 195, loss = 0.09155038744211197
iteration 196, loss = 0.07933337241411209
iteration 197, loss = 0.08429700881242752
iteration 198, loss = 0.12947039306163788
iteration 199, loss = 0.12755751609802246
iteration 200, loss = 0.08310344070196152
iteration 201, loss = 0.16235509514808655
iteration 202, loss = 0.07893040776252747
iteration 203, loss = 0.0822601243853569
iteration 204, loss = 0.07640723884105682
iteration 205, loss = 0.10592684894800186
iteration 206, loss = 0.08413779735565186
iteration 207, loss = 0.08527177572250366
iteration 208, loss = 0.11527691781520844
iteration 209, loss = 0.08554902672767639
iteration 210, loss = 0.07910361886024475
iteration 211, loss = 0.08915466070175171
iteration 212, loss = 0.07510507106781006
iteration 213, loss = 0.11636767536401749
iteration 214, loss = 0.1396854817867279
iteration 215, loss = 0.12114303559064865
iteration 216, loss = 0.13189154863357544
iteration 217, loss = 0.105249784886837
iteration 218, loss = 0.11299354583024979
iteration 219, loss = 0.08817250281572342
iteration 220, loss = 0.098540298640728
iteration 221, loss = 0.09902273118495941
iteration 222, loss = 0.07501117885112762
iteration 223, loss = 0.08046135306358337
iteration 224, loss = 0.10725195705890656
iteration 225, loss = 0.09155339002609253
iteration 226, loss = 0.07592920958995819
iteration 227, loss = 0.1267460435628891
iteration 228, loss = 0.07412497699260712
iteration 229, loss = 0.08231613039970398
iteration 230, loss = 0.1039993017911911
iteration 231, loss = 0.11131945252418518
iteration 232, loss = 0.07522187381982803
iteration 233, loss = 0.11804354190826416
iteration 234, loss = 0.07391898334026337
iteration 235, loss = 0.08960720151662827
iteration 236, loss = 0.07794543355703354
iteration 237, loss = 0.07513674348592758
iteration 238, loss = 0.07147032767534256
iteration 239, loss = 0.09177086502313614
iteration 240, loss = 0.07678835839033127
iteration 241, loss = 0.07813944667577744
iteration 242, loss = 0.076512910425663
iteration 243, loss = 0.13344386219978333
iteration 244, loss = 0.06996410340070724
iteration 245, loss = 0.10745929181575775
iteration 246, loss = 0.073896124958992
iteration 247, loss = 0.07151132076978683
iteration 248, loss = 0.08864542096853256
iteration 249, loss = 0.08432181179523468
iteration 250, loss = 0.06861141324043274
iteration 251, loss = 0.07099902629852295
iteration 252, loss = 0.07408730685710907
iteration 253, loss = 0.07500988245010376
iteration 254, loss = 0.0850282832980156
iteration 255, loss = 0.0666016936302185
iteration 256, loss = 0.07173819839954376
iteration 257, loss = 0.06969743967056274
iteration 258, loss = 0.06736283004283905
iteration 259, loss = 0.0795825719833374
iteration 260, loss = 0.07154639065265656
iteration 261, loss = 0.07513315230607986
iteration 262, loss = 0.07996946573257446
iteration 263, loss = 0.07116350531578064
iteration 264, loss = 0.07053446769714355
iteration 265, loss = 0.08915780484676361
iteration 266, loss = 0.12275763601064682
iteration 267, loss = 0.10174720734357834
iteration 268, loss = 0.08689942955970764
iteration 269, loss = 0.07618646323680878
iteration 270, loss = 0.07848548144102097
iteration 271, loss = 0.07027129828929901
iteration 272, loss = 0.07450184226036072
iteration 273, loss = 0.08011709898710251
iteration 274, loss = 0.06896976381540298
iteration 275, loss = 0.06902812421321869
iteration 276, loss = 0.06684232503175735
iteration 277, loss = 0.06961806863546371
iteration 278, loss = 0.06647148728370667
iteration 279, loss = 0.06439737230539322
iteration 280, loss = 0.06222929060459137
iteration 281, loss = 0.11294961720705032
iteration 282, loss = 0.06368623673915863
iteration 283, loss = 0.08587070554494858
iteration 284, loss = 0.06523001194000244
iteration 285, loss = 0.07408100366592407
iteration 286, loss = 0.0640726387500763
iteration 287, loss = 0.09087855368852615
iteration 288, loss = 0.0638652816414833
iteration 289, loss = 0.06756433099508286
iteration 290, loss = 0.06635914742946625
iteration 291, loss = 0.06741677224636078
iteration 292, loss = 0.0654706358909607
iteration 293, loss = 0.08136457204818726
iteration 294, loss = 0.060258347541093826
iteration 295, loss = 0.06670697033405304
iteration 296, loss = 0.0676637589931488
iteration 297, loss = 0.06369289010763168
iteration 298, loss = 0.06711210310459137
iteration 299, loss = 0.0679684653878212
iteration 300, loss = 0.08059531450271606
iteration 1, loss = 0.062347471714019775
iteration 2, loss = 0.06131310760974884
iteration 3, loss = 0.06583917140960693
iteration 4, loss = 0.06713083386421204
iteration 5, loss = 0.06112890690565109
iteration 6, loss = 0.0656493529677391
iteration 7, loss = 0.0661151260137558
iteration 8, loss = 0.06352963298559189
iteration 9, loss = 0.06403112411499023
iteration 10, loss = 0.060996588319540024
iteration 11, loss = 0.058715444058179855
iteration 12, loss = 0.060816507786512375
iteration 13, loss = 0.07017291337251663
iteration 14, loss = 0.05954355373978615
iteration 15, loss = 0.08057597279548645
iteration 16, loss = 0.058943361043930054
iteration 17, loss = 0.11840638518333435
iteration 18, loss = 0.07955741137266159
iteration 19, loss = 0.08887441456317902
iteration 20, loss = 0.11770965903997421
iteration 21, loss = 0.068153977394104
iteration 22, loss = 0.07289263606071472
iteration 23, loss = 0.08577801287174225
iteration 24, loss = 0.0654301717877388
iteration 25, loss = 0.06012681871652603
iteration 26, loss = 0.059376705437898636
iteration 27, loss = 0.07838017493486404
iteration 28, loss = 0.06687983870506287
iteration 29, loss = 0.06712017953395844
iteration 30, loss = 0.07316171377897263
iteration 31, loss = 0.05959102138876915
iteration 32, loss = 0.06546912342309952
iteration 33, loss = 0.06434321403503418
iteration 34, loss = 0.05639687180519104
iteration 35, loss = 0.05853702872991562
iteration 36, loss = 0.060680609196424484
iteration 37, loss = 0.05867086723446846
iteration 38, loss = 0.07299631088972092
iteration 39, loss = 0.05758321285247803
iteration 40, loss = 0.06994923949241638
iteration 41, loss = 0.06715450435876846
iteration 42, loss = 0.09624091535806656
iteration 43, loss = 0.08359548449516296
iteration 44, loss = 0.06297925114631653
iteration 45, loss = 0.05733238160610199
iteration 46, loss = 0.053541649132966995
iteration 47, loss = 0.0567210353910923
iteration 48, loss = 0.05341903120279312
iteration 49, loss = 0.07231311500072479
iteration 50, loss = 0.09473147243261337
iteration 51, loss = 0.08515531569719315
iteration 52, loss = 0.05739806219935417
iteration 53, loss = 0.05584927275776863
iteration 54, loss = 0.07046916335821152
iteration 55, loss = 0.05379891395568848
iteration 56, loss = 0.08099311590194702
iteration 57, loss = 0.05354895070195198
iteration 58, loss = 0.07161090523004532
iteration 59, loss = 0.059987690299749374
iteration 60, loss = 0.06948655843734741
iteration 61, loss = 0.06407897919416428
iteration 62, loss = 0.05200498551130295
iteration 63, loss = 0.05548710748553276
iteration 64, loss = 0.05502987653017044
iteration 65, loss = 0.05581188574433327
iteration 66, loss = 0.059244200587272644
iteration 67, loss = 0.06607568264007568
iteration 68, loss = 0.054205797612667084
iteration 69, loss = 0.0784379094839096
iteration 70, loss = 0.05395074933767319
iteration 71, loss = 0.05223201587796211
iteration 72, loss = 0.06186675280332565
iteration 73, loss = 0.051050253212451935
iteration 74, loss = 0.05348813906311989
iteration 75, loss = 0.07845057547092438
iteration 76, loss = 0.05476633831858635
iteration 77, loss = 0.05054234713315964
iteration 78, loss = 0.055817749351263046
iteration 79, loss = 0.05355099216103554
iteration 80, loss = 0.06736244261264801
iteration 81, loss = 0.050649434328079224
iteration 82, loss = 0.05603721737861633
iteration 83, loss = 0.05073466897010803
iteration 84, loss = 0.05387392267584801
iteration 85, loss = 0.071281798183918
iteration 86, loss = 0.07440047711133957
iteration 87, loss = 0.04813569411635399
iteration 88, loss = 0.05565869063138962
iteration 89, loss = 0.04931521415710449
iteration 90, loss = 0.056146375834941864
iteration 91, loss = 0.08595987409353256
iteration 92, loss = 0.049683719873428345
iteration 93, loss = 0.05469435825943947
iteration 94, loss = 0.05383078381419182
iteration 95, loss = 0.08049523830413818
iteration 96, loss = 0.05014500021934509
iteration 97, loss = 0.05132415518164635
iteration 98, loss = 0.050619497895240784
iteration 99, loss = 0.04924812540411949
iteration 100, loss = 0.09868504852056503
iteration 101, loss = 0.06164943426847458
iteration 102, loss = 0.04785189405083656
iteration 103, loss = 0.06509837508201599
iteration 104, loss = 0.04662070795893669
iteration 105, loss = 0.05379275977611542
iteration 106, loss = 0.0513271726667881
iteration 107, loss = 0.05046750605106354
iteration 108, loss = 0.06604699790477753
iteration 109, loss = 0.07512103021144867
iteration 110, loss = 0.05140862613916397
iteration 111, loss = 0.05326118320226669
iteration 112, loss = 0.05816302448511124
iteration 113, loss = 0.04967194050550461
iteration 114, loss = 0.056129030883312225
iteration 115, loss = 0.06213105469942093
iteration 116, loss = 0.053473975509405136
iteration 117, loss = 0.06687229871749878
iteration 118, loss = 0.0492546521127224
iteration 119, loss = 0.07937946170568466
iteration 120, loss = 0.050964269787073135
iteration 121, loss = 0.08129625767469406
iteration 122, loss = 0.05933711305260658
iteration 123, loss = 0.07109368592500687
iteration 124, loss = 0.0535791777074337
iteration 125, loss = 0.06272001564502716
iteration 126, loss = 0.0748201236128807
iteration 127, loss = 0.046393346041440964
iteration 128, loss = 0.04420860856771469
iteration 129, loss = 0.08427669107913971
iteration 130, loss = 0.0665537416934967
iteration 131, loss = 0.050775423645973206
iteration 132, loss = 0.04885981231927872
iteration 133, loss = 0.043778520077466965
iteration 134, loss = 0.08943638950586319
iteration 135, loss = 0.04963889718055725
iteration 136, loss = 0.06351721286773682
iteration 137, loss = 0.048211678862571716
iteration 138, loss = 0.056030385196208954
iteration 139, loss = 0.044354163110256195
iteration 140, loss = 0.04600432887673378
iteration 141, loss = 0.04751516133546829
iteration 142, loss = 0.05802011489868164
iteration 143, loss = 0.06711211800575256
iteration 144, loss = 0.09294632822275162
iteration 145, loss = 0.04999352619051933
iteration 146, loss = 0.061272431164979935
iteration 147, loss = 0.04876019060611725
iteration 148, loss = 0.045970357954502106
iteration 149, loss = 0.04991231858730316
iteration 150, loss = 0.04705033451318741
iteration 151, loss = 0.045039936900138855
iteration 152, loss = 0.049782805144786835
iteration 153, loss = 0.050551123917102814
iteration 154, loss = 0.04939856380224228
iteration 155, loss = 0.045422717928886414
iteration 156, loss = 0.04476611316204071
iteration 157, loss = 0.049059342592954636
iteration 158, loss = 0.042232152074575424
iteration 159, loss = 0.05655442923307419
iteration 160, loss = 0.05761384218931198
iteration 161, loss = 0.04055924341082573
iteration 162, loss = 0.05080532282590866
iteration 163, loss = 0.042252033948898315
iteration 164, loss = 0.04632749781012535
iteration 165, loss = 0.04691699147224426
iteration 166, loss = 0.08704861998558044
iteration 167, loss = 0.04768998548388481
iteration 168, loss = 0.04188196361064911
iteration 169, loss = 0.04420865699648857
iteration 170, loss = 0.0529295951128006
iteration 171, loss = 0.0641244575381279
iteration 172, loss = 0.03999895602464676
iteration 173, loss = 0.067195363342762
iteration 174, loss = 0.06774908304214478
iteration 175, loss = 0.05546635389328003
iteration 176, loss = 0.04426456615328789
iteration 177, loss = 0.06044885888695717
iteration 178, loss = 0.042624350637197495
iteration 179, loss = 0.04159678891301155
iteration 180, loss = 0.04052186757326126
iteration 181, loss = 0.0415336973965168
iteration 182, loss = 0.041482485830783844
iteration 183, loss = 0.054153963923454285
iteration 184, loss = 0.04742167890071869
iteration 185, loss = 0.04493143782019615
iteration 186, loss = 0.04350632429122925
iteration 187, loss = 0.03988688066601753
iteration 188, loss = 0.04397200420498848
iteration 189, loss = 0.058507148176431656
iteration 190, loss = 0.0569034181535244
iteration 191, loss = 0.06370049715042114
iteration 192, loss = 0.04055411368608475
iteration 193, loss = 0.05983073264360428
iteration 194, loss = 0.05425398051738739
iteration 195, loss = 0.04003641754388809
iteration 196, loss = 0.039532896131277084
iteration 197, loss = 0.090480737388134
iteration 198, loss = 0.0413391999900341
iteration 199, loss = 0.041639544069767
iteration 200, loss = 0.047913771122694016
iteration 201, loss = 0.03997224569320679
iteration 202, loss = 0.05079057067632675
iteration 203, loss = 0.04111840948462486
iteration 204, loss = 0.03829498216509819
iteration 205, loss = 0.0572676919400692
iteration 206, loss = 0.060089875012636185
iteration 207, loss = 0.03627006709575653
iteration 208, loss = 0.03878757730126381
iteration 209, loss = 0.04488641023635864
iteration 210, loss = 0.05073300004005432
iteration 211, loss = 0.03689781576395035
iteration 212, loss = 0.039490457624197006
iteration 213, loss = 0.03894021734595299
iteration 214, loss = 0.03945158049464226
iteration 215, loss = 0.03901822865009308
iteration 216, loss = 0.05572514608502388
iteration 217, loss = 0.04087956249713898
iteration 218, loss = 0.03929945081472397
iteration 219, loss = 0.03953128680586815
iteration 220, loss = 0.03902518004179001
iteration 221, loss = 0.03578818216919899
iteration 222, loss = 0.0429568849503994
iteration 223, loss = 0.06037385016679764
iteration 224, loss = 0.039697252213954926
iteration 225, loss = 0.04684162884950638
iteration 226, loss = 0.05300603806972504
iteration 227, loss = 0.03803325816988945
iteration 228, loss = 0.03996993601322174
iteration 229, loss = 0.03653337433934212
iteration 230, loss = 0.03792569041252136
iteration 231, loss = 0.043389976024627686
iteration 232, loss = 0.04979969561100006
iteration 233, loss = 0.05413069948554039
iteration 234, loss = 0.0397426076233387
iteration 235, loss = 0.038765668869018555
iteration 236, loss = 0.04341236501932144
iteration 237, loss = 0.05972542613744736
iteration 238, loss = 0.040090564638376236
iteration 239, loss = 0.03984804078936577
iteration 240, loss = 0.03574860468506813
iteration 241, loss = 0.061364080756902695
iteration 242, loss = 0.035417962819337845
iteration 243, loss = 0.03438546881079674
iteration 244, loss = 0.052879370748996735
iteration 245, loss = 0.05249566212296486
iteration 246, loss = 0.03934047371149063
iteration 247, loss = 0.0689559280872345
iteration 248, loss = 0.04217944294214249
iteration 249, loss = 0.036466579884290695
iteration 250, loss = 0.0413864329457283
iteration 251, loss = 0.050239503383636475
iteration 252, loss = 0.03895920515060425
iteration 253, loss = 0.03545762598514557
iteration 254, loss = 0.058073490858078
iteration 255, loss = 0.04507245868444443
iteration 256, loss = 0.03949106112122536
iteration 257, loss = 0.03756784647703171
iteration 258, loss = 0.03468458354473114
iteration 259, loss = 0.03407253324985504
iteration 260, loss = 0.048640504479408264
iteration 261, loss = 0.03526116535067558
iteration 262, loss = 0.0387953445315361
iteration 263, loss = 0.04655133932828903
iteration 264, loss = 0.03354659304022789
iteration 265, loss = 0.050806231796741486
iteration 266, loss = 0.03348064795136452
iteration 267, loss = 0.03552333638072014
iteration 268, loss = 0.04687643423676491
iteration 269, loss = 0.038636770099401474
iteration 270, loss = 0.039884984493255615
iteration 271, loss = 0.03527134656906128
iteration 272, loss = 0.03469354659318924
iteration 273, loss = 0.03354187682271004
iteration 274, loss = 0.03576713800430298
iteration 275, loss = 0.035061515867710114
iteration 276, loss = 0.03265174850821495
iteration 277, loss = 0.034437395632267
iteration 278, loss = 0.03850499168038368
iteration 279, loss = 0.034289855509996414
iteration 280, loss = 0.03615816310048103
iteration 281, loss = 0.032699886709451675
iteration 282, loss = 0.053731631487607956
iteration 283, loss = 0.04487098380923271
iteration 284, loss = 0.03562683239579201
iteration 285, loss = 0.03242543712258339
iteration 286, loss = 0.03527500107884407
iteration 287, loss = 0.06455861777067184
iteration 288, loss = 0.05143813043832779
iteration 289, loss = 0.04699365422129631
iteration 290, loss = 0.035393789410591125
iteration 291, loss = 0.03257228061556816
iteration 292, loss = 0.040072251111269
iteration 293, loss = 0.034185782074928284
iteration 294, loss = 0.04398280754685402
iteration 295, loss = 0.04684336110949516
iteration 296, loss = 0.0328216589987278
iteration 297, loss = 0.032478682696819305
iteration 298, loss = 0.05392051115632057
iteration 299, loss = 0.04126899316906929
iteration 300, loss = 0.03548510745167732
iteration 1, loss = 0.046464141458272934
iteration 2, loss = 0.03339069336652756
iteration 3, loss = 0.033855948597192764
iteration 4, loss = 0.032050516456365585
iteration 5, loss = 0.04496127367019653
iteration 6, loss = 0.032308440655469894
iteration 7, loss = 0.03748749941587448
iteration 8, loss = 0.032432518899440765
iteration 9, loss = 0.03490002825856209
iteration 10, loss = 0.048752136528491974
iteration 11, loss = 0.030573762953281403
iteration 12, loss = 0.03672395274043083
iteration 13, loss = 0.030211802572011948
iteration 14, loss = 0.034225944429636
iteration 15, loss = 0.03280625119805336
iteration 16, loss = 0.039608873426914215
iteration 17, loss = 0.03292929381132126
iteration 18, loss = 0.03284956514835358
iteration 19, loss = 0.03352842852473259
iteration 20, loss = 0.0342499203979969
iteration 21, loss = 0.033520568162202835
iteration 22, loss = 0.037281353026628494
iteration 23, loss = 0.03454973176121712
iteration 24, loss = 0.02999943681061268
iteration 25, loss = 0.03239009529352188
iteration 26, loss = 0.031873129308223724
iteration 27, loss = 0.03424551710486412
iteration 28, loss = 0.03775917738676071
iteration 29, loss = 0.03071022965013981
iteration 30, loss = 0.032852284610271454
iteration 31, loss = 0.032435040920972824
iteration 32, loss = 0.03791963309049606
iteration 33, loss = 0.030298592522740364
iteration 34, loss = 0.029547953978180885
iteration 35, loss = 0.032550178468227386
iteration 36, loss = 0.029826516285538673
iteration 37, loss = 0.03164638578891754
iteration 38, loss = 0.047794755548238754
iteration 39, loss = 0.03180002421140671
iteration 40, loss = 0.0317273885011673
iteration 41, loss = 0.04007381573319435
iteration 42, loss = 0.046946458518505096
iteration 43, loss = 0.03898308426141739
iteration 44, loss = 0.033720292150974274
iteration 45, loss = 0.040622588247060776
iteration 46, loss = 0.04311104863882065
iteration 47, loss = 0.0438520610332489
iteration 48, loss = 0.04138218238949776
iteration 49, loss = 0.03214361146092415
iteration 50, loss = 0.028284618631005287
iteration 51, loss = 0.03848358616232872
iteration 52, loss = 0.03412621468305588
iteration 53, loss = 0.032117217779159546
iteration 54, loss = 0.05247629061341286
iteration 55, loss = 0.03626929968595505
iteration 56, loss = 0.030728431418538094
iteration 57, loss = 0.027388274669647217
iteration 58, loss = 0.028237681835889816
iteration 59, loss = 0.030727170407772064
iteration 60, loss = 0.03261037915945053
iteration 61, loss = 0.028405508026480675
iteration 62, loss = 0.035423245280981064
iteration 63, loss = 0.042344555258750916
iteration 64, loss = 0.03299650922417641
iteration 65, loss = 0.028560088947415352
iteration 66, loss = 0.028161004185676575
iteration 67, loss = 0.027424106374382973
iteration 68, loss = 0.03215915709733963
iteration 69, loss = 0.04194708913564682
iteration 70, loss = 0.031584642827510834
iteration 71, loss = 0.02776852808892727
iteration 72, loss = 0.04612905532121658
iteration 73, loss = 0.03974221274256706
iteration 74, loss = 0.033460650593042374
iteration 75, loss = 0.05017435550689697
iteration 76, loss = 0.02744331955909729
iteration 77, loss = 0.0411878302693367
iteration 78, loss = 0.028386207297444344
iteration 79, loss = 0.04854617267847061
iteration 80, loss = 0.04439904913306236
iteration 81, loss = 0.027193287387490273
iteration 82, loss = 0.026802005246281624
iteration 83, loss = 0.04397815093398094
iteration 84, loss = 0.03143162280321121
iteration 85, loss = 0.0404997244477272
iteration 86, loss = 0.029654955491423607
iteration 87, loss = 0.02838202938437462
iteration 88, loss = 0.03148053213953972
iteration 89, loss = 0.030630461871623993
iteration 90, loss = 0.027648096904158592
iteration 91, loss = 0.041699860244989395
iteration 92, loss = 0.02755730412900448
iteration 93, loss = 0.027944184839725494
iteration 94, loss = 0.034655798226594925
iteration 95, loss = 0.027227304875850677
iteration 96, loss = 0.028865791857242584
iteration 97, loss = 0.029924636706709862
iteration 98, loss = 0.033373281359672546
iteration 99, loss = 0.03010643646121025
iteration 100, loss = 0.0281851626932621
iteration 101, loss = 0.02561200223863125
iteration 102, loss = 0.04043188691139221
iteration 103, loss = 0.026443427428603172
iteration 104, loss = 0.02746511623263359
iteration 105, loss = 0.030033646151423454
iteration 106, loss = 0.02977670356631279
iteration 107, loss = 0.034886304289102554
iteration 108, loss = 0.04191317781805992
iteration 109, loss = 0.038707658648490906
iteration 110, loss = 0.025718670338392258
iteration 111, loss = 0.025430820882320404
iteration 112, loss = 0.030601900070905685
iteration 113, loss = 0.04750163108110428
iteration 114, loss = 0.025898663327097893
iteration 115, loss = 0.030428774654865265
iteration 116, loss = 0.027195410802960396
iteration 117, loss = 0.028038686141371727
iteration 118, loss = 0.03651151806116104
iteration 119, loss = 0.04107128083705902
iteration 120, loss = 0.02708590403199196
iteration 121, loss = 0.040112391114234924
iteration 122, loss = 0.02986088953912258
iteration 123, loss = 0.02664290741086006
iteration 124, loss = 0.02747482992708683
iteration 125, loss = 0.02614220604300499
iteration 126, loss = 0.03750571981072426
iteration 127, loss = 0.03148772194981575
iteration 128, loss = 0.0332629419863224
iteration 129, loss = 0.026198945939540863
iteration 130, loss = 0.039915263652801514
iteration 131, loss = 0.02930578961968422
iteration 132, loss = 0.02541816234588623
iteration 133, loss = 0.029737817123532295
iteration 134, loss = 0.02407381869852543
iteration 135, loss = 0.025714918971061707
iteration 136, loss = 0.03921004757285118
iteration 137, loss = 0.025516565889120102
iteration 138, loss = 0.04181469604372978
iteration 139, loss = 0.025866499170660973
iteration 140, loss = 0.02585276961326599
iteration 141, loss = 0.029346304014325142
iteration 142, loss = 0.03345838189125061
iteration 143, loss = 0.024759441614151
iteration 144, loss = 0.026243817061185837
iteration 145, loss = 0.026953259482979774
iteration 146, loss = 0.0243546012789011
iteration 147, loss = 0.03334205970168114
iteration 148, loss = 0.03306130692362785
iteration 149, loss = 0.03665516525506973
iteration 150, loss = 0.02987167239189148
iteration 151, loss = 0.03427112102508545
iteration 152, loss = 0.02960359863936901
iteration 153, loss = 0.027945345267653465
iteration 154, loss = 0.0350445881485939
iteration 155, loss = 0.025829389691352844
iteration 156, loss = 0.026443740352988243
iteration 157, loss = 0.045707281678915024
iteration 158, loss = 0.02486325241625309
iteration 159, loss = 0.03074927069246769
iteration 160, loss = 0.02459372580051422
iteration 161, loss = 0.03586311265826225
iteration 162, loss = 0.023697085678577423
iteration 163, loss = 0.02333119884133339
iteration 164, loss = 0.026322318241000175
iteration 165, loss = 0.028931617736816406
iteration 166, loss = 0.02426740899682045
iteration 167, loss = 0.026794392615556717
iteration 168, loss = 0.026136020198464394
iteration 169, loss = 0.03533148765563965
iteration 170, loss = 0.03087637387216091
iteration 171, loss = 0.026098044589161873
iteration 172, loss = 0.031228909268975258
iteration 173, loss = 0.03261236101388931
iteration 174, loss = 0.030716627836227417
iteration 175, loss = 0.025369733572006226
iteration 176, loss = 0.026472272351384163
iteration 177, loss = 0.0233549065887928
iteration 178, loss = 0.044542986899614334
iteration 179, loss = 0.026665043085813522
iteration 180, loss = 0.024629421532154083
iteration 181, loss = 0.023758085444569588
iteration 182, loss = 0.022970160469412804
iteration 183, loss = 0.024562256410717964
iteration 184, loss = 0.024469492956995964
iteration 185, loss = 0.024578139185905457
iteration 186, loss = 0.022913958877325058
iteration 187, loss = 0.023338980972766876
iteration 188, loss = 0.03407168388366699
iteration 189, loss = 0.02989862859249115
iteration 190, loss = 0.023397283628582954
iteration 191, loss = 0.023280365392565727
iteration 192, loss = 0.022335777059197426
iteration 193, loss = 0.024651363492012024
iteration 194, loss = 0.024822363629937172
iteration 195, loss = 0.02602076530456543
iteration 196, loss = 0.02703145146369934
iteration 197, loss = 0.023642078042030334
iteration 198, loss = 0.02346230484545231
iteration 199, loss = 0.02300790138542652
iteration 200, loss = 0.025748765096068382
iteration 201, loss = 0.022796710953116417
iteration 202, loss = 0.023161858320236206
iteration 203, loss = 0.02197243645787239
iteration 204, loss = 0.024057069793343544
iteration 205, loss = 0.03502693399786949
iteration 206, loss = 0.023328395560383797
iteration 207, loss = 0.03332369029521942
iteration 208, loss = 0.03396417945623398
iteration 209, loss = 0.024717263877391815
iteration 210, loss = 0.025488661602139473
iteration 211, loss = 0.02275674045085907
iteration 212, loss = 0.02213519625365734
iteration 213, loss = 0.029502248391509056
iteration 214, loss = 0.02255549281835556
iteration 215, loss = 0.022055521607398987
iteration 216, loss = 0.021905863657593727
iteration 217, loss = 0.02748784050345421
iteration 218, loss = 0.02424236759543419
iteration 219, loss = 0.022819295525550842
iteration 220, loss = 0.03506089746952057
iteration 221, loss = 0.0341070257127285
iteration 222, loss = 0.02374688908457756
iteration 223, loss = 0.02914375625550747
iteration 224, loss = 0.031525302678346634
iteration 225, loss = 0.023705894127488136
iteration 226, loss = 0.022607795894145966
iteration 227, loss = 0.024027114734053612
iteration 228, loss = 0.022301113232970238
iteration 229, loss = 0.02247449941933155
iteration 230, loss = 0.022031623870134354
iteration 231, loss = 0.021144360303878784
iteration 232, loss = 0.02443677932024002
iteration 233, loss = 0.023982210084795952
iteration 234, loss = 0.027530144900083542
iteration 235, loss = 0.025627940893173218
iteration 236, loss = 0.025022128596901894
iteration 237, loss = 0.022885330021381378
iteration 238, loss = 0.03566393256187439
iteration 239, loss = 0.023132802918553352
iteration 240, loss = 0.022969210520386696
iteration 241, loss = 0.034486498683691025
iteration 242, loss = 0.034824296832084656
iteration 243, loss = 0.03237331658601761
iteration 244, loss = 0.02242550253868103
iteration 245, loss = 0.031425703316926956
iteration 246, loss = 0.020374689251184464
iteration 247, loss = 0.020084291696548462
iteration 248, loss = 0.022124744951725006
iteration 249, loss = 0.022235196083784103
iteration 250, loss = 0.020530156791210175
iteration 251, loss = 0.021880770102143288
iteration 252, loss = 0.020317211747169495
iteration 253, loss = 0.023938234895467758
iteration 254, loss = 0.03133609518408775
iteration 255, loss = 0.03147522732615471
iteration 256, loss = 0.021632159128785133
iteration 257, loss = 0.02209794521331787
iteration 258, loss = 0.02845006436109543
iteration 259, loss = 0.02739296853542328
iteration 260, loss = 0.0212271548807621
iteration 261, loss = 0.021151814609766006
iteration 262, loss = 0.022593028843402863
iteration 263, loss = 0.022155635058879852
iteration 264, loss = 0.03679625689983368
iteration 265, loss = 0.021105889230966568
iteration 266, loss = 0.022009367123246193
iteration 267, loss = 0.04806448519229889
iteration 268, loss = 0.021499881520867348
iteration 269, loss = 0.020325208082795143
iteration 270, loss = 0.02793400175869465
iteration 271, loss = 0.03642549365758896
iteration 272, loss = 0.030182747170329094
iteration 273, loss = 0.030631648376584053
iteration 274, loss = 0.021235378459095955
iteration 275, loss = 0.020213564857840538
iteration 276, loss = 0.020210547372698784
iteration 277, loss = 0.025035487487912178
iteration 278, loss = 0.01904171146452427
iteration 279, loss = 0.020501015707850456
iteration 280, loss = 0.019822392612695694
iteration 281, loss = 0.020868832245469093
iteration 282, loss = 0.03142808377742767
iteration 283, loss = 0.027029989287257195
iteration 284, loss = 0.019750339910387993
iteration 285, loss = 0.03432083874940872
iteration 286, loss = 0.023259807378053665
iteration 287, loss = 0.020899569615721703
iteration 288, loss = 0.026130611076951027
iteration 289, loss = 0.019647130742669106
iteration 290, loss = 0.025174232199788094
iteration 291, loss = 0.02093471959233284
iteration 292, loss = 0.020561182871460915
iteration 293, loss = 0.03840605542063713
iteration 294, loss = 0.021211830899119377
iteration 295, loss = 0.01985413208603859
iteration 296, loss = 0.020186707377433777
iteration 297, loss = 0.026349348947405815
iteration 298, loss = 0.028277238830924034
iteration 299, loss = 0.019251814112067223
iteration 300, loss = 0.022251242771744728
iteration 1, loss = 0.020512981340289116
iteration 2, loss = 0.02012133039534092
iteration 3, loss = 0.01959434151649475
iteration 4, loss = 0.021199431270360947
iteration 5, loss = 0.026344219222664833
iteration 6, loss = 0.019214697182178497
iteration 7, loss = 0.02416728250682354
iteration 8, loss = 0.02091478742659092
iteration 9, loss = 0.01969216950237751
iteration 10, loss = 0.019316477701067924
iteration 11, loss = 0.02734229527413845
iteration 12, loss = 0.03706296533346176
iteration 13, loss = 0.02356182225048542
iteration 14, loss = 0.02736298181116581
iteration 15, loss = 0.021897602826356888
iteration 16, loss = 0.030372638255357742
iteration 17, loss = 0.02144504338502884
iteration 18, loss = 0.024526745080947876
iteration 19, loss = 0.020125363022089005
iteration 20, loss = 0.01948176510632038
iteration 21, loss = 0.018952948972582817
iteration 22, loss = 0.0194598026573658
iteration 23, loss = 0.0207227673381567
iteration 24, loss = 0.019485827535390854
iteration 25, loss = 0.024937409907579422
iteration 26, loss = 0.020346565172076225
iteration 27, loss = 0.018806839361786842
iteration 28, loss = 0.019721614196896553
iteration 29, loss = 0.0197709109634161
iteration 30, loss = 0.02499113604426384
iteration 31, loss = 0.018797924742102623
iteration 32, loss = 0.018963094800710678
iteration 33, loss = 0.022051667794585228
iteration 34, loss = 0.019603166729211807
iteration 35, loss = 0.019173575565218925
iteration 36, loss = 0.02729513868689537
iteration 37, loss = 0.02365666627883911
iteration 38, loss = 0.01844361238181591
iteration 39, loss = 0.020114043727517128
iteration 40, loss = 0.01991957798600197
iteration 41, loss = 0.025507334619760513
iteration 42, loss = 0.0288152527064085
iteration 43, loss = 0.01787414401769638
iteration 44, loss = 0.02538517862558365
iteration 45, loss = 0.021658537909388542
iteration 46, loss = 0.026592519134283066
iteration 47, loss = 0.01984541118144989
iteration 48, loss = 0.018971087411046028
iteration 49, loss = 0.018561357632279396
iteration 50, loss = 0.02703886106610298
iteration 51, loss = 0.02398812212049961
iteration 52, loss = 0.01874455064535141
iteration 53, loss = 0.019080210477113724
iteration 54, loss = 0.025946244597434998
iteration 55, loss = 0.01881193369626999
iteration 56, loss = 0.018793027848005295
iteration 57, loss = 0.01926199533045292
iteration 58, loss = 0.019651401787996292
iteration 59, loss = 0.02656673640012741
iteration 60, loss = 0.02032875269651413
iteration 61, loss = 0.028067030012607574
iteration 62, loss = 0.02024659886956215
iteration 63, loss = 0.02104966528713703
iteration 64, loss = 0.02442474476993084
iteration 65, loss = 0.01861090026795864
iteration 66, loss = 0.019018370658159256
iteration 67, loss = 0.025554705411195755
iteration 68, loss = 0.029078243300318718
iteration 69, loss = 0.020264193415641785
iteration 70, loss = 0.020687373355031013
iteration 71, loss = 0.017874756827950478
iteration 72, loss = 0.018883785232901573
iteration 73, loss = 0.017664361745119095
iteration 74, loss = 0.02575502172112465
iteration 75, loss = 0.021851664409041405
iteration 76, loss = 0.018503235653042793
iteration 77, loss = 0.01883561722934246
iteration 78, loss = 0.01955144666135311
iteration 79, loss = 0.02902895025908947
iteration 80, loss = 0.022331127896904945
iteration 81, loss = 0.017824022099375725
iteration 82, loss = 0.025084124878048897
iteration 83, loss = 0.021267356351017952
iteration 84, loss = 0.0185434278100729
iteration 85, loss = 0.018450917676091194
iteration 86, loss = 0.02256755344569683
iteration 87, loss = 0.02466830052435398
iteration 88, loss = 0.018362436443567276
iteration 89, loss = 0.01714281737804413
iteration 90, loss = 0.017760811373591423
iteration 91, loss = 0.01891820877790451
iteration 92, loss = 0.024157680571079254
iteration 93, loss = 0.025506746023893356
iteration 94, loss = 0.019090553745627403
iteration 95, loss = 0.018071478232741356
iteration 96, loss = 0.019290026277303696
iteration 97, loss = 0.01858699694275856
iteration 98, loss = 0.02104649692773819
iteration 99, loss = 0.018975254148244858
iteration 100, loss = 0.018354861065745354
iteration 101, loss = 0.019248299300670624
iteration 102, loss = 0.018780793994665146
iteration 103, loss = 0.017503371462225914
iteration 104, loss = 0.022196996957063675
iteration 105, loss = 0.01833377406001091
iteration 106, loss = 0.016717132180929184
iteration 107, loss = 0.01909303106367588
iteration 108, loss = 0.018839146941900253
iteration 109, loss = 0.017473429441452026
iteration 110, loss = 0.016949905082583427
iteration 111, loss = 0.024944854900240898
iteration 112, loss = 0.018051650375127792
iteration 113, loss = 0.016620028764009476
iteration 114, loss = 0.02563578262925148
iteration 115, loss = 0.01644178107380867
iteration 116, loss = 0.01596047356724739
iteration 117, loss = 0.023260917514562607
iteration 118, loss = 0.023877214640378952
iteration 119, loss = 0.018036974593997
iteration 120, loss = 0.026508230715990067
iteration 121, loss = 0.01729978621006012
iteration 122, loss = 0.01804276928305626
iteration 123, loss = 0.02326076477766037
iteration 124, loss = 0.01820095255970955
iteration 125, loss = 0.016209226101636887
iteration 126, loss = 0.018402229994535446
iteration 127, loss = 0.017836272716522217
iteration 128, loss = 0.016637442633509636
iteration 129, loss = 0.021509351208806038
iteration 130, loss = 0.017322923988103867
iteration 131, loss = 0.017936740070581436
iteration 132, loss = 0.01629486121237278
iteration 133, loss = 0.022050324827432632
iteration 134, loss = 0.018088342621922493
iteration 135, loss = 0.016642922535538673
iteration 136, loss = 0.01804148405790329
iteration 137, loss = 0.021771153435111046
iteration 138, loss = 0.02069912478327751
iteration 139, loss = 0.016742169857025146
iteration 140, loss = 0.015984447672963142
iteration 141, loss = 0.01630302332341671
iteration 142, loss = 0.019101811572909355
iteration 143, loss = 0.01648573763668537
iteration 144, loss = 0.016861800104379654
iteration 145, loss = 0.017029689624905586
iteration 146, loss = 0.02125648222863674
iteration 147, loss = 0.016654565930366516
iteration 148, loss = 0.01732504740357399
iteration 149, loss = 0.01643470488488674
iteration 150, loss = 0.015565241686999798
iteration 151, loss = 0.01590612158179283
iteration 152, loss = 0.015598058700561523
iteration 153, loss = 0.019719216972589493
iteration 154, loss = 0.0170388575643301
iteration 155, loss = 0.025726379826664925
iteration 156, loss = 0.016694480553269386
iteration 157, loss = 0.01693657785654068
iteration 158, loss = 0.017078708857297897
iteration 159, loss = 0.016122005879878998
iteration 160, loss = 0.0227106511592865
iteration 161, loss = 0.024625519290566444
iteration 162, loss = 0.0166790708899498
iteration 163, loss = 0.017803413793444633
iteration 164, loss = 0.015633003786206245
iteration 165, loss = 0.015493330545723438
iteration 166, loss = 0.01524006761610508
iteration 167, loss = 0.029546154662966728
iteration 168, loss = 0.016485946252942085
iteration 169, loss = 0.017928358167409897
iteration 170, loss = 0.017386790364980698
iteration 171, loss = 0.015354077331721783
iteration 172, loss = 0.015228692442178726
iteration 173, loss = 0.01864941604435444
iteration 174, loss = 0.015895690768957138
iteration 175, loss = 0.016246672719717026
iteration 176, loss = 0.02572294883430004
iteration 177, loss = 0.015073206275701523
iteration 178, loss = 0.016676263883709908
iteration 179, loss = 0.021066708490252495
iteration 180, loss = 0.023006869480013847
iteration 181, loss = 0.02200881391763687
iteration 182, loss = 0.015442471951246262
iteration 183, loss = 0.015858959406614304
iteration 184, loss = 0.015477674081921577
iteration 185, loss = 0.01967257261276245
iteration 186, loss = 0.026521138846874237
iteration 187, loss = 0.018900781869888306
iteration 188, loss = 0.014986547641456127
iteration 189, loss = 0.017449425533413887
iteration 190, loss = 0.016002759337425232
iteration 191, loss = 0.01505227293819189
iteration 192, loss = 0.01737075299024582
iteration 193, loss = 0.01624968647956848
iteration 194, loss = 0.018385184928774834
iteration 195, loss = 0.014176879078149796
iteration 196, loss = 0.01549637783318758
iteration 197, loss = 0.015951605513691902
iteration 198, loss = 0.016218770295381546
iteration 199, loss = 0.014708862639963627
iteration 200, loss = 0.016276098787784576
iteration 201, loss = 0.015007726848125458
iteration 202, loss = 0.015629412606358528
iteration 203, loss = 0.018736034631729126
iteration 204, loss = 0.014664762653410435
iteration 205, loss = 0.014991063624620438
iteration 206, loss = 0.017796166241168976
iteration 207, loss = 0.02306002750992775
iteration 208, loss = 0.021030660718679428
iteration 209, loss = 0.014318601228296757
iteration 210, loss = 0.014776993542909622
iteration 211, loss = 0.014435859397053719
iteration 212, loss = 0.01691211573779583
iteration 213, loss = 0.015082438476383686
iteration 214, loss = 0.016642114147543907
iteration 215, loss = 0.01699434034526348
iteration 216, loss = 0.02666107378900051
iteration 217, loss = 0.01787366345524788
iteration 218, loss = 0.01745600439608097
iteration 219, loss = 0.015550214797258377
iteration 220, loss = 0.015972735360264778
iteration 221, loss = 0.01644744537770748
iteration 222, loss = 0.014614710584282875
iteration 223, loss = 0.020728489384055138
iteration 224, loss = 0.015156467445194721
iteration 225, loss = 0.023237695917487144
iteration 226, loss = 0.02153737284243107
iteration 227, loss = 0.017024267464876175
iteration 228, loss = 0.014439595863223076
iteration 229, loss = 0.02463909052312374
iteration 230, loss = 0.025287261232733727
iteration 231, loss = 0.013953057117760181
iteration 232, loss = 0.01710621267557144
iteration 233, loss = 0.014709236100316048
iteration 234, loss = 0.013988853432238102
iteration 235, loss = 0.015118638053536415
iteration 236, loss = 0.013626963831484318
iteration 237, loss = 0.014473668299615383
iteration 238, loss = 0.013739688321948051
iteration 239, loss = 0.013576286844909191
iteration 240, loss = 0.015043988823890686
iteration 241, loss = 0.0143806803971529
iteration 242, loss = 0.02536303550004959
iteration 243, loss = 0.020041845738887787
iteration 244, loss = 0.01350212749093771
iteration 245, loss = 0.01672181487083435
iteration 246, loss = 0.014508586376905441
iteration 247, loss = 0.015797661617398262
iteration 248, loss = 0.023299463093280792
iteration 249, loss = 0.016162842512130737
iteration 250, loss = 0.014549546875059605
iteration 251, loss = 0.013969054445624352
iteration 252, loss = 0.01335421483963728
iteration 253, loss = 0.01617841050028801
iteration 254, loss = 0.014197644777595997
iteration 255, loss = 0.017335230484604836
iteration 256, loss = 0.014501383528113365
iteration 257, loss = 0.02054525911808014
iteration 258, loss = 0.014868522994220257
iteration 259, loss = 0.015255491249263287
iteration 260, loss = 0.014770645648241043
iteration 261, loss = 0.019907251000404358
iteration 262, loss = 0.017823822796344757
iteration 263, loss = 0.01445845142006874
iteration 264, loss = 0.021716224029660225
iteration 265, loss = 0.016485001891851425
iteration 266, loss = 0.017610762268304825
iteration 267, loss = 0.017589226365089417
iteration 268, loss = 0.02323172241449356
iteration 269, loss = 0.02604691870510578
iteration 270, loss = 0.013176146894693375
iteration 271, loss = 0.014547995291650295
iteration 272, loss = 0.014903835020959377
iteration 273, loss = 0.017628710716962814
iteration 274, loss = 0.014481136575341225
iteration 275, loss = 0.017940513789653778
iteration 276, loss = 0.014959774911403656
iteration 277, loss = 0.015229634009301662
iteration 278, loss = 0.017356041818857193
iteration 279, loss = 0.014438245445489883
iteration 280, loss = 0.02087578736245632
iteration 281, loss = 0.019243484362959862
iteration 282, loss = 0.0141201326623559
iteration 283, loss = 0.014633026905357838
iteration 284, loss = 0.020962942391633987
iteration 285, loss = 0.018894588574767113
iteration 286, loss = 0.01320745050907135
iteration 287, loss = 0.019745081663131714
iteration 288, loss = 0.013632010668516159
iteration 289, loss = 0.014971903525292873
iteration 290, loss = 0.016653578728437424
iteration 291, loss = 0.014357085339725018
iteration 292, loss = 0.028870154172182083
iteration 293, loss = 0.013452558778226376
iteration 294, loss = 0.013535561971366405
iteration 295, loss = 0.013640272431075573
iteration 296, loss = 0.014067424461245537
iteration 297, loss = 0.022567732259631157
iteration 298, loss = 0.014269618317484856
iteration 299, loss = 0.013688236474990845
iteration 300, loss = 0.01959783397614956
iteration 1, loss = 0.019047899171710014
iteration 2, loss = 0.01442431379109621
iteration 3, loss = 0.018237976357340813
iteration 4, loss = 0.013240768574178219
iteration 5, loss = 0.014065515249967575
iteration 6, loss = 0.018461167812347412
iteration 7, loss = 0.01303085871040821
iteration 8, loss = 0.01647034101188183
iteration 9, loss = 0.01497681811451912
iteration 10, loss = 0.014175847172737122
iteration 11, loss = 0.014528696425259113
iteration 12, loss = 0.015575313940644264
iteration 13, loss = 0.013640698976814747
iteration 14, loss = 0.01267859898507595
iteration 15, loss = 0.01721487194299698
iteration 16, loss = 0.014033186249434948
iteration 17, loss = 0.014689714647829533
iteration 18, loss = 0.012393367476761341
iteration 19, loss = 0.018438177183270454
iteration 20, loss = 0.01569235697388649
iteration 21, loss = 0.0133261289447546
iteration 22, loss = 0.01370602659881115
iteration 23, loss = 0.01348188892006874
iteration 24, loss = 0.01274748146533966
iteration 25, loss = 0.013982865959405899
iteration 26, loss = 0.01676022633910179
iteration 27, loss = 0.01330853346735239
iteration 28, loss = 0.012754274532198906
iteration 29, loss = 0.013495637103915215
iteration 30, loss = 0.013091467320919037
iteration 31, loss = 0.013649559579789639
iteration 32, loss = 0.02830256149172783
iteration 33, loss = 0.016383090987801552
iteration 34, loss = 0.014166883192956448
iteration 35, loss = 0.013305739499628544
iteration 36, loss = 0.01830129139125347
iteration 37, loss = 0.013128471560776234
iteration 38, loss = 0.016103865578770638
iteration 39, loss = 0.019182752817869186
iteration 40, loss = 0.014091432094573975
iteration 41, loss = 0.012855568900704384
iteration 42, loss = 0.013633612543344498
iteration 43, loss = 0.013126388192176819
iteration 44, loss = 0.02311357669532299
iteration 45, loss = 0.012581580318510532
iteration 46, loss = 0.013136341236531734
iteration 47, loss = 0.013428939506411552
iteration 48, loss = 0.02068370021879673
iteration 49, loss = 0.021847842261195183
iteration 50, loss = 0.013087643310427666
iteration 51, loss = 0.0174481850117445
iteration 52, loss = 0.01475897803902626
iteration 53, loss = 0.015077287331223488
iteration 54, loss = 0.014262608252465725
iteration 55, loss = 0.012493979185819626
iteration 56, loss = 0.013929834589362144
iteration 57, loss = 0.014674844220280647
iteration 58, loss = 0.013228901661932468
iteration 59, loss = 0.01871282421052456
iteration 60, loss = 0.01264148484915495
iteration 61, loss = 0.012127513997256756
iteration 62, loss = 0.0129435108974576
iteration 63, loss = 0.012906307354569435
iteration 64, loss = 0.012626638635993004
iteration 65, loss = 0.012475167401134968
iteration 66, loss = 0.017952123656868935
iteration 67, loss = 0.012675481848418713
iteration 68, loss = 0.011550582945346832
iteration 69, loss = 0.015434407629072666
iteration 70, loss = 0.01721968688070774
iteration 71, loss = 0.017876923084259033
iteration 72, loss = 0.013181686401367188
iteration 73, loss = 0.015039985068142414
iteration 74, loss = 0.012948501855134964
iteration 75, loss = 0.017133371904492378
iteration 76, loss = 0.012377850711345673
iteration 77, loss = 0.013342819176614285
iteration 78, loss = 0.01754455827176571
iteration 79, loss = 0.0120012816041708
iteration 80, loss = 0.019123749807476997
iteration 81, loss = 0.013791372999548912
iteration 82, loss = 0.01134735718369484
iteration 83, loss = 0.012770308181643486
iteration 84, loss = 0.013334561139345169
iteration 85, loss = 0.012295459397137165
iteration 86, loss = 0.015262886881828308
iteration 87, loss = 0.012529140338301659
iteration 88, loss = 0.01254785805940628
iteration 89, loss = 0.019013818353414536
iteration 90, loss = 0.011881616897881031
iteration 91, loss = 0.0154068972915411
iteration 92, loss = 0.021522430703043938
iteration 93, loss = 0.012903128750622272
iteration 94, loss = 0.01750980131328106
iteration 95, loss = 0.016699442639946938
iteration 96, loss = 0.017700348049402237
iteration 97, loss = 0.012420758605003357
iteration 98, loss = 0.01464136689901352
iteration 99, loss = 0.015359477140009403
iteration 100, loss = 0.017032770439982414
iteration 101, loss = 0.011812017299234867
iteration 102, loss = 0.012655755504965782
iteration 103, loss = 0.012379162944853306
iteration 104, loss = 0.01323382742702961
iteration 105, loss = 0.01132312510162592
iteration 106, loss = 0.012063581496477127
iteration 107, loss = 0.011678957380354404
iteration 108, loss = 0.01231504324823618
iteration 109, loss = 0.012136896140873432
iteration 110, loss = 0.016992587596178055
iteration 111, loss = 0.011516168713569641
iteration 112, loss = 0.011236380785703659
iteration 113, loss = 0.019220532849431038
iteration 114, loss = 0.01352670881897211
iteration 115, loss = 0.011561518535017967
iteration 116, loss = 0.012221337296068668
iteration 117, loss = 0.011624224483966827
iteration 118, loss = 0.011465713381767273
iteration 119, loss = 0.013056091032922268
iteration 120, loss = 0.011734372936189175
iteration 121, loss = 0.01182290818542242
iteration 122, loss = 0.012670813128352165
iteration 123, loss = 0.011692417785525322
iteration 124, loss = 0.011545711196959019
iteration 125, loss = 0.011869631707668304
iteration 126, loss = 0.021230706945061684
iteration 127, loss = 0.011587944813072681
iteration 128, loss = 0.015358948148787022
iteration 129, loss = 0.013304374180734158
iteration 130, loss = 0.011323869228363037
iteration 131, loss = 0.011385024525225163
iteration 132, loss = 0.011962974444031715
iteration 133, loss = 0.012689931318163872
iteration 134, loss = 0.011415764689445496
iteration 135, loss = 0.01169615238904953
iteration 136, loss = 0.012513473629951477
iteration 137, loss = 0.01261734589934349
iteration 138, loss = 0.010848887264728546
iteration 139, loss = 0.011584867723286152
iteration 140, loss = 0.01103266328573227
iteration 141, loss = 0.021536018699407578
iteration 142, loss = 0.011291507631540298
iteration 143, loss = 0.01098446175456047
iteration 144, loss = 0.01155576016753912
iteration 145, loss = 0.012353966012597084
iteration 146, loss = 0.011823628097772598
iteration 147, loss = 0.013207747600972652
iteration 148, loss = 0.013094652444124222
iteration 149, loss = 0.011909536086022854
iteration 150, loss = 0.015148594975471497
iteration 151, loss = 0.017224950715899467
iteration 152, loss = 0.012060612440109253
iteration 153, loss = 0.012386401183903217
iteration 154, loss = 0.011771371588110924
iteration 155, loss = 0.013726433739066124
iteration 156, loss = 0.01146702654659748
iteration 157, loss = 0.015684768557548523
iteration 158, loss = 0.011588354595005512
iteration 159, loss = 0.01588582433760166
iteration 160, loss = 0.012821757234632969
iteration 161, loss = 0.013183082453906536
iteration 162, loss = 0.014669262804090977
iteration 163, loss = 0.015461178496479988
iteration 164, loss = 0.01291518285870552
iteration 165, loss = 0.018438059836626053
iteration 166, loss = 0.011493343859910965
iteration 167, loss = 0.01097462885081768
iteration 168, loss = 0.011406401172280312
iteration 169, loss = 0.015514909289777279
iteration 170, loss = 0.011158453300595284
iteration 171, loss = 0.011903289705514908
iteration 172, loss = 0.011346554383635521
iteration 173, loss = 0.011708197183907032
iteration 174, loss = 0.012566622346639633
iteration 175, loss = 0.01777505688369274
iteration 176, loss = 0.011476777493953705
iteration 177, loss = 0.013156674802303314
iteration 178, loss = 0.010817368514835835
iteration 179, loss = 0.016256457194685936
iteration 180, loss = 0.015679942443966866
iteration 181, loss = 0.011590003967285156
iteration 182, loss = 0.015105395577847958
iteration 183, loss = 0.0117489630356431
iteration 184, loss = 0.013001077808439732
iteration 185, loss = 0.011465384624898434
iteration 186, loss = 0.011013513430953026
iteration 187, loss = 0.01749185472726822
iteration 188, loss = 0.013182646594941616
iteration 189, loss = 0.011899326927959919
iteration 190, loss = 0.01744612120091915
iteration 191, loss = 0.010098891332745552
iteration 192, loss = 0.011854395270347595
iteration 193, loss = 0.01338154822587967
iteration 194, loss = 0.010710302740335464
iteration 195, loss = 0.010913738049566746
iteration 196, loss = 0.011687792837619781
iteration 197, loss = 0.011472495272755623
iteration 198, loss = 0.012678987346589565
iteration 199, loss = 0.017473097890615463
iteration 200, loss = 0.012814425863325596
iteration 201, loss = 0.016359301283955574
iteration 202, loss = 0.013542571105062962
iteration 203, loss = 0.011983349919319153
iteration 204, loss = 0.012388898991048336
iteration 205, loss = 0.01074475422501564
iteration 206, loss = 0.019057298079133034
iteration 207, loss = 0.011990305967628956
iteration 208, loss = 0.011441191658377647
iteration 209, loss = 0.010374273173511028
iteration 210, loss = 0.012412541545927525
iteration 211, loss = 0.013025535270571709
iteration 212, loss = 0.010304952040314674
iteration 213, loss = 0.010691226460039616
iteration 214, loss = 0.019329391419887543
iteration 215, loss = 0.01059593167155981
iteration 216, loss = 0.013882733881473541
iteration 217, loss = 0.010340425185859203
iteration 218, loss = 0.01161191612482071
iteration 219, loss = 0.010689828544855118
iteration 220, loss = 0.014429526403546333
iteration 221, loss = 0.010799681767821312
iteration 222, loss = 0.011433782987296581
iteration 223, loss = 0.011633465066552162
iteration 224, loss = 0.014105952344834805
iteration 225, loss = 0.012705245986580849
iteration 226, loss = 0.012357041239738464
iteration 227, loss = 0.011035426519811153
iteration 228, loss = 0.009961163625121117
iteration 229, loss = 0.010066678747534752
iteration 230, loss = 0.011991593986749649
iteration 231, loss = 0.01109641045331955
iteration 232, loss = 0.014667302370071411
iteration 233, loss = 0.011840678751468658
iteration 234, loss = 0.011867783963680267
iteration 235, loss = 0.012600945308804512
iteration 236, loss = 0.010904635302722454
iteration 237, loss = 0.014260396361351013
iteration 238, loss = 0.009667651727795601
iteration 239, loss = 0.01045792456716299
iteration 240, loss = 0.014630777761340141
iteration 241, loss = 0.010410650633275509
iteration 242, loss = 0.011040964163839817
iteration 243, loss = 0.010717201046645641
iteration 244, loss = 0.011902191676199436
iteration 245, loss = 0.011622020043432713
iteration 246, loss = 0.01144728809595108
iteration 247, loss = 0.014560814946889877
iteration 248, loss = 0.014175744727253914
iteration 249, loss = 0.010673986747860909
iteration 250, loss = 0.01532444916665554
iteration 251, loss = 0.010297097265720367
iteration 252, loss = 0.010010508820414543
iteration 253, loss = 0.010129072703421116
iteration 254, loss = 0.01214107871055603
iteration 255, loss = 0.011532779783010483
iteration 256, loss = 0.009307218715548515
iteration 257, loss = 0.010327446274459362
iteration 258, loss = 0.013229962438344955
iteration 259, loss = 0.010550121776759624
iteration 260, loss = 0.011443239636719227
iteration 261, loss = 0.01639965921640396
iteration 262, loss = 0.010884247720241547
iteration 263, loss = 0.010620775632560253
iteration 264, loss = 0.010828813537955284
iteration 265, loss = 0.009944004006683826
iteration 266, loss = 0.011231653392314911
iteration 267, loss = 0.013859782367944717
iteration 268, loss = 0.010087545961141586
iteration 269, loss = 0.010569827631115913
iteration 270, loss = 0.01054161787033081
iteration 271, loss = 0.009963332675397396
iteration 272, loss = 0.014980338513851166
iteration 273, loss = 0.012431612238287926
iteration 274, loss = 0.011125274933874607
iteration 275, loss = 0.011798042804002762
iteration 276, loss = 0.011005360633134842
iteration 277, loss = 0.011253979057073593
iteration 278, loss = 0.010412400588393211
iteration 279, loss = 0.010420380160212517
iteration 280, loss = 0.009914057329297066
iteration 281, loss = 0.01075081154704094
iteration 282, loss = 0.018665987998247147
iteration 283, loss = 0.009803754277527332
iteration 284, loss = 0.010319933295249939
iteration 285, loss = 0.011728440411388874
iteration 286, loss = 0.010620513930916786
iteration 287, loss = 0.015216732397675514
iteration 288, loss = 0.012686804868280888
iteration 289, loss = 0.015841705724596977
iteration 290, loss = 0.00929767545312643
iteration 291, loss = 0.013262741267681122
iteration 292, loss = 0.014365766197443008
iteration 293, loss = 0.00982976146042347
iteration 294, loss = 0.009623887948691845
iteration 295, loss = 0.013510854914784431
iteration 296, loss = 0.009716597385704517
iteration 297, loss = 0.010993636213243008
iteration 298, loss = 0.009932613000273705
iteration 299, loss = 0.011227572336792946
iteration 300, loss = 0.014443440362811089
iteration 1, loss = 0.009914536029100418
iteration 2, loss = 0.010236183181405067
iteration 3, loss = 0.010397616773843765
iteration 4, loss = 0.011109084822237492
iteration 5, loss = 0.011316481046378613
iteration 6, loss = 0.009695843793451786
iteration 7, loss = 0.009167118929326534
iteration 8, loss = 0.010525177232921124
iteration 9, loss = 0.011768105439841747
iteration 10, loss = 0.010607588104903698
iteration 11, loss = 0.012126749381422997
iteration 12, loss = 0.009144202806055546
iteration 13, loss = 0.012952392920851707
iteration 14, loss = 0.0118651008233428
iteration 15, loss = 0.010807021521031857
iteration 16, loss = 0.015702351927757263
iteration 17, loss = 0.01060257013887167
iteration 18, loss = 0.010343935340642929
iteration 19, loss = 0.009335458278656006
iteration 20, loss = 0.011820239946246147
iteration 21, loss = 0.010563960298895836
iteration 22, loss = 0.015927312895655632
iteration 23, loss = 0.010637176223099232
iteration 24, loss = 0.01343977265059948
iteration 25, loss = 0.009340301156044006
iteration 26, loss = 0.009587290696799755
iteration 27, loss = 0.00940516497939825
iteration 28, loss = 0.010563062503933907
iteration 29, loss = 0.01008253451436758
iteration 30, loss = 0.008783848956227303
iteration 31, loss = 0.010025342926383018
iteration 32, loss = 0.014365783892571926
iteration 33, loss = 0.009361568838357925
iteration 34, loss = 0.009737505577504635
iteration 35, loss = 0.013035457581281662
iteration 36, loss = 0.015432046726346016
iteration 37, loss = 0.010100029408931732
iteration 38, loss = 0.00953083485364914
iteration 39, loss = 0.00998668558895588
iteration 40, loss = 0.010094398632645607
iteration 41, loss = 0.009152517654001713
iteration 42, loss = 0.010395086370408535
iteration 43, loss = 0.009210795164108276
iteration 44, loss = 0.008923135697841644
iteration 45, loss = 0.00933513417840004
iteration 46, loss = 0.009309152141213417
iteration 47, loss = 0.009920391254127026
iteration 48, loss = 0.009842688217759132
iteration 49, loss = 0.009263847023248672
iteration 50, loss = 0.009604128077626228
iteration 51, loss = 0.009633961133658886
iteration 52, loss = 0.014761821366846561
iteration 53, loss = 0.009902069345116615
iteration 54, loss = 0.010588932782411575
iteration 55, loss = 0.009367791004478931
iteration 56, loss = 0.009781007654964924
iteration 57, loss = 0.0099461255595088
iteration 58, loss = 0.009701701812446117
iteration 59, loss = 0.015963375568389893
iteration 60, loss = 0.015514747239649296
iteration 61, loss = 0.009265689179301262
iteration 62, loss = 0.014346236363053322
iteration 63, loss = 0.008882432244718075
iteration 64, loss = 0.01451119128614664
iteration 65, loss = 0.011897863820195198
iteration 66, loss = 0.013578316196799278
iteration 67, loss = 0.009508179500699043
iteration 68, loss = 0.011320860125124454
iteration 69, loss = 0.013558777049183846
iteration 70, loss = 0.010542033240199089
iteration 71, loss = 0.008772165514528751
iteration 72, loss = 0.00968182273209095
iteration 73, loss = 0.009517102502286434
iteration 74, loss = 0.009585225954651833
iteration 75, loss = 0.00928402692079544
iteration 76, loss = 0.010884575545787811
iteration 77, loss = 0.009261425584554672
iteration 78, loss = 0.008975900709629059
iteration 79, loss = 0.00975694227963686
iteration 80, loss = 0.009725049138069153
iteration 81, loss = 0.008653939701616764
iteration 82, loss = 0.008856055326759815
iteration 83, loss = 0.00896577350795269
iteration 84, loss = 0.009667330421507359
iteration 85, loss = 0.012104636058211327
iteration 86, loss = 0.009216085076332092
iteration 87, loss = 0.00878752488642931
iteration 88, loss = 0.00989837758243084
iteration 89, loss = 0.00821173656731844
iteration 90, loss = 0.00917885173112154
iteration 91, loss = 0.00907701626420021
iteration 92, loss = 0.010095362551510334
iteration 93, loss = 0.00915219821035862
iteration 94, loss = 0.010712986811995506
iteration 95, loss = 0.009617502801120281
iteration 96, loss = 0.009627509862184525
iteration 97, loss = 0.012648454867303371
iteration 98, loss = 0.013012837618589401
iteration 99, loss = 0.011357219889760017
iteration 100, loss = 0.00966058112680912
iteration 101, loss = 0.010849542915821075
iteration 102, loss = 0.009139418601989746
iteration 103, loss = 0.00956872757524252
iteration 104, loss = 0.008346997201442719
iteration 105, loss = 0.008896831423044205
iteration 106, loss = 0.011544731445610523
iteration 107, loss = 0.008402821607887745
iteration 108, loss = 0.00861725676804781
iteration 109, loss = 0.009102415293455124
iteration 110, loss = 0.012478869408369064
iteration 111, loss = 0.009847124107182026
iteration 112, loss = 0.008446261286735535
iteration 113, loss = 0.008694586344063282
iteration 114, loss = 0.00878461729735136
iteration 115, loss = 0.009987364523112774
iteration 116, loss = 0.014220720157027245
iteration 117, loss = 0.008995206095278263
iteration 118, loss = 0.012006393633782864
iteration 119, loss = 0.010120488703250885
iteration 120, loss = 0.010254008695483208
iteration 121, loss = 0.010089620016515255
iteration 122, loss = 0.01589345373213291
iteration 123, loss = 0.009051002562046051
iteration 124, loss = 0.009673813357949257
iteration 125, loss = 0.009100314229726791
iteration 126, loss = 0.0089407404884696
iteration 127, loss = 0.008024968206882477
iteration 128, loss = 0.0091090127825737
iteration 129, loss = 0.009466789662837982
iteration 130, loss = 0.009066938422620296
iteration 131, loss = 0.009137626737356186
iteration 132, loss = 0.009166747331619263
iteration 133, loss = 0.009236562065780163
iteration 134, loss = 0.012739931233227253
iteration 135, loss = 0.009569207206368446
iteration 136, loss = 0.011955881491303444
iteration 137, loss = 0.008822453208267689
iteration 138, loss = 0.008789079263806343
iteration 139, loss = 0.008949902839958668
iteration 140, loss = 0.009590519592165947
iteration 141, loss = 0.01098024845123291
iteration 142, loss = 0.008475590497255325
iteration 143, loss = 0.008631878532469273
iteration 144, loss = 0.009582084603607655
iteration 145, loss = 0.01139443926513195
iteration 146, loss = 0.011993587017059326
iteration 147, loss = 0.008802101947367191
iteration 148, loss = 0.01364513486623764
iteration 149, loss = 0.009339314885437489
iteration 150, loss = 0.010420898906886578
iteration 151, loss = 0.01107547152787447
iteration 152, loss = 0.010124808177351952
iteration 153, loss = 0.008893498219549656
iteration 154, loss = 0.011394192464649677
iteration 155, loss = 0.008749713189899921
iteration 156, loss = 0.009500103071331978
iteration 157, loss = 0.01167797390371561
iteration 158, loss = 0.00981217436492443
iteration 159, loss = 0.008929288014769554
iteration 160, loss = 0.011739013716578484
iteration 161, loss = 0.009498566389083862
iteration 162, loss = 0.008836261928081512
iteration 163, loss = 0.009040609933435917
iteration 164, loss = 0.009155536070466042
iteration 165, loss = 0.008414470590651035
iteration 166, loss = 0.010667012073099613
iteration 167, loss = 0.00887809507548809
iteration 168, loss = 0.01203993335366249
iteration 169, loss = 0.00863091554492712
iteration 170, loss = 0.008723449893295765
iteration 171, loss = 0.008520500734448433
iteration 172, loss = 0.010065377689898014
iteration 173, loss = 0.013218209147453308
iteration 174, loss = 0.008941249921917915
iteration 175, loss = 0.008498728275299072
iteration 176, loss = 0.008929934352636337
iteration 177, loss = 0.009325820952653885
iteration 178, loss = 0.010055039077997208
iteration 179, loss = 0.012938744388520718
iteration 180, loss = 0.00833827443420887
iteration 181, loss = 0.00847516767680645
iteration 182, loss = 0.011237721890211105
iteration 183, loss = 0.010736299678683281
iteration 184, loss = 0.008848965167999268
iteration 185, loss = 0.011798130348324776
iteration 186, loss = 0.010300212539732456
iteration 187, loss = 0.00813200231641531
iteration 188, loss = 0.01379415299743414
iteration 189, loss = 0.00962882675230503
iteration 190, loss = 0.008299779146909714
iteration 191, loss = 0.008140647783875465
iteration 192, loss = 0.008482112549245358
iteration 193, loss = 0.016875116154551506
iteration 194, loss = 0.008159596472978592
iteration 195, loss = 0.013704802840948105
iteration 196, loss = 0.012095657177269459
iteration 197, loss = 0.007792487274855375
iteration 198, loss = 0.012333163991570473
iteration 199, loss = 0.008830232545733452
iteration 200, loss = 0.0089653842151165
iteration 201, loss = 0.01206386461853981
iteration 202, loss = 0.008607189171016216
iteration 203, loss = 0.008816278539597988
iteration 204, loss = 0.007809972856193781
iteration 205, loss = 0.008124221116304398
iteration 206, loss = 0.00824806559830904
iteration 207, loss = 0.009020674042403698
iteration 208, loss = 0.010354910977184772
iteration 209, loss = 0.00808581244200468
iteration 210, loss = 0.008738312870264053
iteration 211, loss = 0.007910351268947124
iteration 212, loss = 0.009727813303470612
iteration 213, loss = 0.00839285273104906
iteration 214, loss = 0.00950773898512125
iteration 215, loss = 0.008770479820668697
iteration 216, loss = 0.01045055128633976
iteration 217, loss = 0.008144763298332691
iteration 218, loss = 0.008249778300523758
iteration 219, loss = 0.011056472547352314
iteration 220, loss = 0.008042755536735058
iteration 221, loss = 0.011422192677855492
iteration 222, loss = 0.00768181961029768
iteration 223, loss = 0.008026708848774433
iteration 224, loss = 0.008356079459190369
iteration 225, loss = 0.008966242894530296
iteration 226, loss = 0.00925854779779911
iteration 227, loss = 0.011807508766651154
iteration 228, loss = 0.009969019331037998
iteration 229, loss = 0.008270703256130219
iteration 230, loss = 0.008380265906453133
iteration 231, loss = 0.010551334358751774
iteration 232, loss = 0.010227620601654053
iteration 233, loss = 0.009459706954658031
iteration 234, loss = 0.007883099839091301
iteration 235, loss = 0.009469385258853436
iteration 236, loss = 0.008130708709359169
iteration 237, loss = 0.011113659478724003
iteration 238, loss = 0.00793050043284893
iteration 239, loss = 0.008553730323910713
iteration 240, loss = 0.008471578359603882
iteration 241, loss = 0.008297251537442207
iteration 242, loss = 0.011274243704974651
iteration 243, loss = 0.008065015077590942
iteration 244, loss = 0.009047240950167179
iteration 245, loss = 0.008863424882292747
iteration 246, loss = 0.008500654250383377
iteration 247, loss = 0.007868004962801933
iteration 248, loss = 0.008077346719801426
iteration 249, loss = 0.00962283369153738
iteration 250, loss = 0.008016192354261875
iteration 251, loss = 0.008575654588639736
iteration 252, loss = 0.007575604133307934
iteration 253, loss = 0.009882298298180103
iteration 254, loss = 0.008559834212064743
iteration 255, loss = 0.007999397814273834
iteration 256, loss = 0.010783685371279716
iteration 257, loss = 0.010325007140636444
iteration 258, loss = 0.007669966667890549
iteration 259, loss = 0.008000332862138748
iteration 260, loss = 0.01002688892185688
iteration 261, loss = 0.010059221647679806
iteration 262, loss = 0.007887890562415123
iteration 263, loss = 0.007545816246420145
iteration 264, loss = 0.010838920250535011
iteration 265, loss = 0.007991618476808071
iteration 266, loss = 0.007899309508502483
iteration 267, loss = 0.010796415619552135
iteration 268, loss = 0.007924968376755714
iteration 269, loss = 0.011020605452358723
iteration 270, loss = 0.007939420640468597
iteration 271, loss = 0.007375823799520731
iteration 272, loss = 0.008256726898252964
iteration 273, loss = 0.007319918833673
iteration 274, loss = 0.008678877726197243
iteration 275, loss = 0.008008474484086037
iteration 276, loss = 0.010098624974489212
iteration 277, loss = 0.010099095292389393
iteration 278, loss = 0.008325287140905857
iteration 279, loss = 0.0083619961515069
iteration 280, loss = 0.007343396544456482
iteration 281, loss = 0.009173638187348843
iteration 282, loss = 0.008215028792619705
iteration 283, loss = 0.007741730660200119
iteration 284, loss = 0.01264894101768732
iteration 285, loss = 0.011442586779594421
iteration 286, loss = 0.018873607739806175
iteration 287, loss = 0.013235756196081638
iteration 288, loss = 0.008806917816400528
iteration 289, loss = 0.008914651349186897
iteration 290, loss = 0.007639041170477867
iteration 291, loss = 0.012227248400449753
iteration 292, loss = 0.00813413318246603
iteration 293, loss = 0.007479369640350342
iteration 294, loss = 0.007563484832644463
iteration 295, loss = 0.00782676413655281
iteration 296, loss = 0.012259522452950478
iteration 297, loss = 0.008506947197020054
iteration 298, loss = 0.010665029287338257
iteration 299, loss = 0.012019464746117592
iteration 300, loss = 0.012591008096933365
iteration 1, loss = 0.008587449789047241
iteration 2, loss = 0.010172421112656593
iteration 3, loss = 0.00803334265947342
iteration 4, loss = 0.01217457465827465
iteration 5, loss = 0.007808659225702286
iteration 6, loss = 0.012924849055707455
iteration 7, loss = 0.007739736698567867
iteration 8, loss = 0.008135620504617691
iteration 9, loss = 0.007321517914533615
iteration 10, loss = 0.007385640405118465
iteration 11, loss = 0.008653959259390831
iteration 12, loss = 0.007412202190607786
iteration 13, loss = 0.007260913960635662
iteration 14, loss = 0.0072971368208527565
iteration 15, loss = 0.009098992682993412
iteration 16, loss = 0.010751379653811455
iteration 17, loss = 0.007198965642601252
iteration 18, loss = 0.009438926354050636
iteration 19, loss = 0.00826596561819315
iteration 20, loss = 0.00856852252036333
iteration 21, loss = 0.009424491785466671
iteration 22, loss = 0.007176667917519808
iteration 23, loss = 0.0075480323284864426
iteration 24, loss = 0.009510519914329052
iteration 25, loss = 0.007737491279840469
iteration 26, loss = 0.007780070882290602
iteration 27, loss = 0.007894856855273247
iteration 28, loss = 0.00801161304116249
iteration 29, loss = 0.007877187803387642
iteration 30, loss = 0.006849152967333794
iteration 31, loss = 0.009759427979588509
iteration 32, loss = 0.007311257533729076
iteration 33, loss = 0.007487124763429165
iteration 34, loss = 0.011791576631367207
iteration 35, loss = 0.007518457714468241
iteration 36, loss = 0.008042425848543644
iteration 37, loss = 0.0076414174400269985
iteration 38, loss = 0.00896201841533184
iteration 39, loss = 0.010299017652869225
iteration 40, loss = 0.007897302508354187
iteration 41, loss = 0.008595801889896393
iteration 42, loss = 0.0077441418543457985
iteration 43, loss = 0.010388689115643501
iteration 44, loss = 0.007090161554515362
iteration 45, loss = 0.010400678031146526
iteration 46, loss = 0.007446346338838339
iteration 47, loss = 0.007409034762531519
iteration 48, loss = 0.007543956395238638
iteration 49, loss = 0.007162787020206451
iteration 50, loss = 0.006877110339701176
iteration 51, loss = 0.007307163905352354
iteration 52, loss = 0.007209018804132938
iteration 53, loss = 0.007451503071933985
iteration 54, loss = 0.007491392083466053
iteration 55, loss = 0.009033875539898872
iteration 56, loss = 0.007479521445930004
iteration 57, loss = 0.007080018054693937
iteration 58, loss = 0.007068946026265621
iteration 59, loss = 0.007005930878221989
iteration 60, loss = 0.008117727935314178
iteration 61, loss = 0.0081259086728096
iteration 62, loss = 0.007211657240986824
iteration 63, loss = 0.007557346951216459
iteration 64, loss = 0.007942273281514645
iteration 65, loss = 0.0076687512919306755
iteration 66, loss = 0.0067179505713284016
iteration 67, loss = 0.011751078069210052
iteration 68, loss = 0.009878227487206459
iteration 69, loss = 0.008098374120891094
iteration 70, loss = 0.007161279674619436
iteration 71, loss = 0.007269855588674545
iteration 72, loss = 0.007347869221121073
iteration 73, loss = 0.006810921709984541
iteration 74, loss = 0.0072457026690244675
iteration 75, loss = 0.0071916417218744755
iteration 76, loss = 0.007427370175719261
iteration 77, loss = 0.0071568298153579235
iteration 78, loss = 0.008970453403890133
iteration 79, loss = 0.007260756567120552
iteration 80, loss = 0.007263036444783211
iteration 81, loss = 0.00886759627610445
iteration 82, loss = 0.007427005097270012
iteration 83, loss = 0.007881004363298416
iteration 84, loss = 0.007380194496363401
iteration 85, loss = 0.006619835738092661
iteration 86, loss = 0.006844953168183565
iteration 87, loss = 0.009847967885434628
iteration 88, loss = 0.007227815221995115
iteration 89, loss = 0.007857690565288067
iteration 90, loss = 0.0073811570182442665
iteration 91, loss = 0.007639109622687101
iteration 92, loss = 0.00738158356398344
iteration 93, loss = 0.010309098288416862
iteration 94, loss = 0.006972029805183411
iteration 95, loss = 0.007929292507469654
iteration 96, loss = 0.007208340801298618
iteration 97, loss = 0.009479718282818794
iteration 98, loss = 0.011498033069074154
iteration 99, loss = 0.009942738339304924
iteration 100, loss = 0.008208057843148708
iteration 101, loss = 0.007957734167575836
iteration 102, loss = 0.007194377016276121
iteration 103, loss = 0.007558923214673996
iteration 104, loss = 0.006761347874999046
iteration 105, loss = 0.00920284353196621
iteration 106, loss = 0.011447195895016193
iteration 107, loss = 0.0071866149082779884
iteration 108, loss = 0.007440997287631035
iteration 109, loss = 0.0076533108949661255
iteration 110, loss = 0.008426940068602562
iteration 111, loss = 0.00677125109359622
iteration 112, loss = 0.006479343399405479
iteration 113, loss = 0.007220545317977667
iteration 114, loss = 0.00698800478130579
iteration 115, loss = 0.009684499353170395
iteration 116, loss = 0.007410629186779261
iteration 117, loss = 0.006338729988783598
iteration 118, loss = 0.007276937365531921
iteration 119, loss = 0.00897377822548151
iteration 120, loss = 0.014367563650012016
iteration 121, loss = 0.007227388210594654
iteration 122, loss = 0.006708857603371143
iteration 123, loss = 0.007077212445437908
iteration 124, loss = 0.007873665541410446
iteration 125, loss = 0.009299004450440407
iteration 126, loss = 0.006743143312633038
iteration 127, loss = 0.01077888160943985
iteration 128, loss = 0.006804017815738916
iteration 129, loss = 0.006731562316417694
iteration 130, loss = 0.010081066749989986
iteration 131, loss = 0.007624607067555189
iteration 132, loss = 0.007745054084807634
iteration 133, loss = 0.009308717213571072
iteration 134, loss = 0.007446920499205589
iteration 135, loss = 0.006528932135552168
iteration 136, loss = 0.006704952567815781
iteration 137, loss = 0.006868195254355669
iteration 138, loss = 0.0063920109532773495
iteration 139, loss = 0.007799616549164057
iteration 140, loss = 0.006321573629975319
iteration 141, loss = 0.006885350216180086
iteration 142, loss = 0.0071844677440822124
iteration 143, loss = 0.010693928226828575
iteration 144, loss = 0.008533499203622341
iteration 145, loss = 0.010175108909606934
iteration 146, loss = 0.006961245089769363
iteration 147, loss = 0.007192476652562618
iteration 148, loss = 0.008826433680951595
iteration 149, loss = 0.006487355567514896
iteration 150, loss = 0.0076198335736989975
iteration 151, loss = 0.007161017507314682
iteration 152, loss = 0.00789980310946703
iteration 153, loss = 0.009045258164405823
iteration 154, loss = 0.010765936225652695
iteration 155, loss = 0.007008721120655537
iteration 156, loss = 0.009203970432281494
iteration 157, loss = 0.007363720331341028
iteration 158, loss = 0.006822412833571434
iteration 159, loss = 0.00704856077209115
iteration 160, loss = 0.007725008763372898
iteration 161, loss = 0.006797368638217449
iteration 162, loss = 0.006716595962643623
iteration 163, loss = 0.006734376773238182
iteration 164, loss = 0.008153034374117851
iteration 165, loss = 0.0075463661924004555
iteration 166, loss = 0.006786144804209471
iteration 167, loss = 0.007185232825577259
iteration 168, loss = 0.009250899776816368
iteration 169, loss = 0.006983116734772921
iteration 170, loss = 0.006537448614835739
iteration 171, loss = 0.007314888294786215
iteration 172, loss = 0.00753698730841279
iteration 173, loss = 0.0067796544171869755
iteration 174, loss = 0.007349485997110605
iteration 175, loss = 0.008824391290545464
iteration 176, loss = 0.006327759940177202
iteration 177, loss = 0.007068745791912079
iteration 178, loss = 0.008999181911349297
iteration 179, loss = 0.006536972243338823
iteration 180, loss = 0.0070566958747804165
iteration 181, loss = 0.006400797516107559
iteration 182, loss = 0.007479902822524309
iteration 183, loss = 0.008599335327744484
iteration 184, loss = 0.008890069089829922
iteration 185, loss = 0.00651927012950182
iteration 186, loss = 0.009496118873357773
iteration 187, loss = 0.006389854475855827
iteration 188, loss = 0.006512859370559454
iteration 189, loss = 0.007050776854157448
iteration 190, loss = 0.006545870564877987
iteration 191, loss = 0.0061380621045827866
iteration 192, loss = 0.01299686636775732
iteration 193, loss = 0.010472279973328114
iteration 194, loss = 0.0070478422567248344
iteration 195, loss = 0.006943582557141781
iteration 196, loss = 0.009120685048401356
iteration 197, loss = 0.010661066509783268
iteration 198, loss = 0.010380730964243412
iteration 199, loss = 0.008959157392382622
iteration 200, loss = 0.009495445527136326
iteration 201, loss = 0.006429561413824558
iteration 202, loss = 0.007165791466832161
iteration 203, loss = 0.00740891695022583
iteration 204, loss = 0.008800298906862736
iteration 205, loss = 0.006655659060925245
iteration 206, loss = 0.008840339258313179
iteration 207, loss = 0.010044657625257969
iteration 208, loss = 0.006641514599323273
iteration 209, loss = 0.006514011882245541
iteration 210, loss = 0.006161352153867483
iteration 211, loss = 0.00633920868858695
iteration 212, loss = 0.007429230026900768
iteration 213, loss = 0.008292892947793007
iteration 214, loss = 0.006492724176496267
iteration 215, loss = 0.006179284304380417
iteration 216, loss = 0.006123326253145933
iteration 217, loss = 0.006306391675025225
iteration 218, loss = 0.006725568324327469
iteration 219, loss = 0.006550946272909641
iteration 220, loss = 0.006659649312496185
iteration 221, loss = 0.006393399089574814
iteration 222, loss = 0.006374675780534744
iteration 223, loss = 0.0088296914473176
iteration 224, loss = 0.010476834140717983
iteration 225, loss = 0.0068530552089214325
iteration 226, loss = 0.007428300566971302
iteration 227, loss = 0.007149592041969299
iteration 228, loss = 0.006314227823168039
iteration 229, loss = 0.006241626106202602
iteration 230, loss = 0.007086495868861675
iteration 231, loss = 0.008284470997750759
iteration 232, loss = 0.006994514260441065
iteration 233, loss = 0.007602662779390812
iteration 234, loss = 0.007303584832698107
iteration 235, loss = 0.006504015065729618
iteration 236, loss = 0.006581707391887903
iteration 237, loss = 0.006317694205790758
iteration 238, loss = 0.007494405843317509
iteration 239, loss = 0.007023399695754051
iteration 240, loss = 0.006527133285999298
iteration 241, loss = 0.010361666791141033
iteration 242, loss = 0.006959935184568167
iteration 243, loss = 0.010288110002875328
iteration 244, loss = 0.006976820062845945
iteration 245, loss = 0.007677359972149134
iteration 246, loss = 0.005984310060739517
iteration 247, loss = 0.007940317504107952
iteration 248, loss = 0.006857171189039946
iteration 249, loss = 0.006946564186364412
iteration 250, loss = 0.006895313039422035
iteration 251, loss = 0.009071346372365952
iteration 252, loss = 0.007860137149691582
iteration 253, loss = 0.009144564159214497
iteration 254, loss = 0.010356226935982704
iteration 255, loss = 0.006790199317038059
iteration 256, loss = 0.009487793780863285
iteration 257, loss = 0.006561953574419022
iteration 258, loss = 0.0079147107899189
iteration 259, loss = 0.006020910572260618
iteration 260, loss = 0.006416665390133858
iteration 261, loss = 0.007291942834854126
iteration 262, loss = 0.008077507838606834
iteration 263, loss = 0.006263190880417824
iteration 264, loss = 0.006117367185652256
iteration 265, loss = 0.009469157084822655
iteration 266, loss = 0.006290119141340256
iteration 267, loss = 0.006105681881308556
iteration 268, loss = 0.007516755256801844
iteration 269, loss = 0.007514927536249161
iteration 270, loss = 0.010492103174328804
iteration 271, loss = 0.008998967707157135
iteration 272, loss = 0.012458347715437412
iteration 273, loss = 0.006690391339361668
iteration 274, loss = 0.007509719114750624
iteration 275, loss = 0.005909658968448639
iteration 276, loss = 0.0063458592630922794
iteration 277, loss = 0.006527379155158997
iteration 278, loss = 0.005895807407796383
iteration 279, loss = 0.006927110254764557
iteration 280, loss = 0.00670442683622241
iteration 281, loss = 0.008784836158156395
iteration 282, loss = 0.006117293145507574
iteration 283, loss = 0.007220765575766563
iteration 284, loss = 0.007872440852224827
iteration 285, loss = 0.006100086960941553
iteration 286, loss = 0.006061302963644266
iteration 287, loss = 0.005945576820522547
iteration 288, loss = 0.010925684124231339
iteration 289, loss = 0.007696652784943581
iteration 290, loss = 0.005973527207970619
iteration 291, loss = 0.0064285146072506905
iteration 292, loss = 0.005974620580673218
iteration 293, loss = 0.006450784858316183
iteration 294, loss = 0.006566185504198074
iteration 295, loss = 0.008082524873316288
iteration 296, loss = 0.006034395657479763
iteration 297, loss = 0.006300793960690498
iteration 298, loss = 0.006317353341728449
iteration 299, loss = 0.006214134860783815
iteration 300, loss = 0.006384409032762051
iteration 1, loss = 0.005884948186576366
iteration 2, loss = 0.006209246348589659
iteration 3, loss = 0.0057325828820466995
iteration 4, loss = 0.006701644975692034
iteration 5, loss = 0.0061867074109613895
iteration 6, loss = 0.006731180474162102
iteration 7, loss = 0.009892136789858341
iteration 8, loss = 0.005870305933058262
iteration 9, loss = 0.005870169959962368
iteration 10, loss = 0.00605476088821888
iteration 11, loss = 0.00610104575753212
iteration 12, loss = 0.005661357194185257
iteration 13, loss = 0.008075392805039883
iteration 14, loss = 0.006013625301420689
iteration 15, loss = 0.006327398121356964
iteration 16, loss = 0.0066373394802212715
iteration 17, loss = 0.007336767856031656
iteration 18, loss = 0.006696425843983889
iteration 19, loss = 0.005906853824853897
iteration 20, loss = 0.00915368553251028
iteration 21, loss = 0.007951303385198116
iteration 22, loss = 0.009962361305952072
iteration 23, loss = 0.005960945039987564
iteration 24, loss = 0.006451715249568224
iteration 25, loss = 0.013395186513662338
iteration 26, loss = 0.00801322516053915
iteration 27, loss = 0.006009125616401434
iteration 28, loss = 0.005899486131966114
iteration 29, loss = 0.005779169499874115
iteration 30, loss = 0.006362610962241888
iteration 31, loss = 0.006888176780194044
iteration 32, loss = 0.0057386052794754505
iteration 33, loss = 0.005774837918579578
iteration 34, loss = 0.00769197940826416
iteration 35, loss = 0.006766520440578461
iteration 36, loss = 0.005726650822907686
iteration 37, loss = 0.006722495425492525
iteration 38, loss = 0.00620764447376132
iteration 39, loss = 0.008320879191160202
iteration 40, loss = 0.011427159421145916
iteration 41, loss = 0.0077508073300123215
iteration 42, loss = 0.006545937620103359
iteration 43, loss = 0.005626002326607704
iteration 44, loss = 0.0056829010136425495
iteration 45, loss = 0.005925356410443783
iteration 46, loss = 0.007147102151066065
iteration 47, loss = 0.005340813659131527
iteration 48, loss = 0.006093547213822603
iteration 49, loss = 0.008071772754192352
iteration 50, loss = 0.007571520283818245
iteration 51, loss = 0.006889167241752148
iteration 52, loss = 0.009556313045322895
iteration 53, loss = 0.0057468716986477375
iteration 54, loss = 0.00615728460252285
iteration 55, loss = 0.005615095142275095
iteration 56, loss = 0.005504470784217119
iteration 57, loss = 0.008209913969039917
iteration 58, loss = 0.005767534486949444
iteration 59, loss = 0.006977255921810865
iteration 60, loss = 0.005710049532353878
iteration 61, loss = 0.005915982183068991
iteration 62, loss = 0.005660568363964558
iteration 63, loss = 0.006381834391504526
iteration 64, loss = 0.005775600206106901
iteration 65, loss = 0.008336774073541164
iteration 66, loss = 0.005618172697722912
iteration 67, loss = 0.00901100318878889
iteration 68, loss = 0.0060196081176400185
iteration 69, loss = 0.00690763583406806
iteration 70, loss = 0.005884348414838314
iteration 71, loss = 0.005504250060766935
iteration 72, loss = 0.005987950600683689
iteration 73, loss = 0.005723474081605673
iteration 74, loss = 0.005100702866911888
iteration 75, loss = 0.0077447667717933655
iteration 76, loss = 0.006733810994774103
iteration 77, loss = 0.007765760645270348
iteration 78, loss = 0.00853035505861044
iteration 79, loss = 0.005668118130415678
iteration 80, loss = 0.006237000692635775
iteration 81, loss = 0.006200120784342289
iteration 82, loss = 0.005953636486083269
iteration 83, loss = 0.0068441941402852535
iteration 84, loss = 0.0059349508956074715
iteration 85, loss = 0.007626480422914028
iteration 86, loss = 0.00672285258769989
iteration 87, loss = 0.007932843640446663
iteration 88, loss = 0.005709169432520866
iteration 89, loss = 0.005780485458672047
iteration 90, loss = 0.007611877750605345
iteration 91, loss = 0.00918566808104515
iteration 92, loss = 0.005744860973209143
iteration 93, loss = 0.007681234739720821
iteration 94, loss = 0.005627752747386694
iteration 95, loss = 0.006061380263417959
iteration 96, loss = 0.00559723237529397
iteration 97, loss = 0.00824061781167984
iteration 98, loss = 0.005571802146732807
iteration 99, loss = 0.007435610517859459
iteration 100, loss = 0.005839603953063488
iteration 101, loss = 0.005450395401567221
iteration 102, loss = 0.009547734633088112
iteration 103, loss = 0.007477801758795977
iteration 104, loss = 0.005662425421178341
iteration 105, loss = 0.005556684918701649
iteration 106, loss = 0.006191929802298546
iteration 107, loss = 0.007312838453799486
iteration 108, loss = 0.005475102458149195
iteration 109, loss = 0.00643329368904233
iteration 110, loss = 0.006456214934587479
iteration 111, loss = 0.0067705195397138596
iteration 112, loss = 0.005906146019697189
iteration 113, loss = 0.00577197689563036
iteration 114, loss = 0.006382416933774948
iteration 115, loss = 0.0056780073791742325
iteration 116, loss = 0.006971269845962524
iteration 117, loss = 0.005630900617688894
iteration 118, loss = 0.006397271528840065
iteration 119, loss = 0.0063806683756411076
iteration 120, loss = 0.005638298578560352
iteration 121, loss = 0.005949223414063454
iteration 122, loss = 0.0077655017375946045
iteration 123, loss = 0.007514773868024349
iteration 124, loss = 0.0055609229020774364
iteration 125, loss = 0.005581356585025787
iteration 126, loss = 0.005729736760258675
iteration 127, loss = 0.00614137714728713
iteration 128, loss = 0.006004162132740021
iteration 129, loss = 0.006576097570359707
iteration 130, loss = 0.006133620627224445
iteration 131, loss = 0.005398372653871775
iteration 132, loss = 0.009166840463876724
iteration 133, loss = 0.005264430772513151
iteration 134, loss = 0.005901855416595936
iteration 135, loss = 0.0052796099334955215
iteration 136, loss = 0.005848772823810577
iteration 137, loss = 0.005634751636534929
iteration 138, loss = 0.005348061677068472
iteration 139, loss = 0.005772468168288469
iteration 140, loss = 0.005773351527750492
iteration 141, loss = 0.005631712265312672
iteration 142, loss = 0.005493261385709047
iteration 143, loss = 0.005179837346076965
iteration 144, loss = 0.005569617263972759
iteration 145, loss = 0.007075925823301077
iteration 146, loss = 0.005594363436102867
iteration 147, loss = 0.005747115705162287
iteration 148, loss = 0.00550388777628541
iteration 149, loss = 0.006313704885542393
iteration 150, loss = 0.006679026875644922
iteration 151, loss = 0.00756508344784379
iteration 152, loss = 0.005855289287865162
iteration 153, loss = 0.005952866747975349
iteration 154, loss = 0.006062458269298077
iteration 155, loss = 0.005456128623336554
iteration 156, loss = 0.007774550002068281
iteration 157, loss = 0.005622576922178268
iteration 158, loss = 0.0058105639182031155
iteration 159, loss = 0.006136135198175907
iteration 160, loss = 0.007594432216137648
iteration 161, loss = 0.00806545652449131
iteration 162, loss = 0.006236364133656025
iteration 163, loss = 0.00529047567397356
iteration 164, loss = 0.005500666797161102
iteration 165, loss = 0.00565684400498867
iteration 166, loss = 0.0052650803700089455
iteration 167, loss = 0.0054378267377614975
iteration 168, loss = 0.005739062558859587
iteration 169, loss = 0.006066753529012203
iteration 170, loss = 0.005280787590891123
iteration 171, loss = 0.005449639167636633
iteration 172, loss = 0.005770218558609486
iteration 173, loss = 0.005709507968276739
iteration 174, loss = 0.006112518720328808
iteration 175, loss = 0.005485437344759703
iteration 176, loss = 0.00840449333190918
iteration 177, loss = 0.0069214701652526855
iteration 178, loss = 0.005331097170710564
iteration 179, loss = 0.006272733211517334
iteration 180, loss = 0.008134548552334309
iteration 181, loss = 0.006291033700108528
iteration 182, loss = 0.006209898740053177
iteration 183, loss = 0.00635876227170229
iteration 184, loss = 0.005340597126632929
iteration 185, loss = 0.00508281821385026
iteration 186, loss = 0.005250174086540937
iteration 187, loss = 0.007489505223929882
iteration 188, loss = 0.005184140056371689
iteration 189, loss = 0.005204057786613703
iteration 190, loss = 0.005114058498293161
iteration 191, loss = 0.005219895392656326
iteration 192, loss = 0.0057066986337304115
iteration 193, loss = 0.00579017773270607
iteration 194, loss = 0.005477407481521368
iteration 195, loss = 0.00532678235322237
iteration 196, loss = 0.008888740092515945
iteration 197, loss = 0.005543000064790249
iteration 198, loss = 0.006773300934582949
iteration 199, loss = 0.005305511876940727
iteration 200, loss = 0.0048124706372618675
iteration 201, loss = 0.005039617419242859
iteration 202, loss = 0.0074816299602389336
iteration 203, loss = 0.005433828104287386
iteration 204, loss = 0.005916250869631767
iteration 205, loss = 0.0092850336804986
iteration 206, loss = 0.005492945201694965
iteration 207, loss = 0.005521104671061039
iteration 208, loss = 0.005269411951303482
iteration 209, loss = 0.005287698935717344
iteration 210, loss = 0.006629440002143383
iteration 211, loss = 0.005900759249925613
iteration 212, loss = 0.005185550544410944
iteration 213, loss = 0.005595921538770199
iteration 214, loss = 0.0060073984786868095
iteration 215, loss = 0.005508230999112129
iteration 216, loss = 0.006256555672734976
iteration 217, loss = 0.005383891519159079
iteration 218, loss = 0.005632882472127676
iteration 219, loss = 0.005974413361400366
iteration 220, loss = 0.006708059925585985
iteration 221, loss = 0.005653113592416048
iteration 222, loss = 0.005983658134937286
iteration 223, loss = 0.008482440374791622
iteration 224, loss = 0.008300898596644402
iteration 225, loss = 0.005340160336345434
iteration 226, loss = 0.008207165636122227
iteration 227, loss = 0.006002112757414579
iteration 228, loss = 0.007231764029711485
iteration 229, loss = 0.005988705437630415
iteration 230, loss = 0.010733198374509811
iteration 231, loss = 0.005615915637463331
iteration 232, loss = 0.005348907317966223
iteration 233, loss = 0.005185026675462723
iteration 234, loss = 0.005553800612688065
iteration 235, loss = 0.005610480904579163
iteration 236, loss = 0.005410424899309874
iteration 237, loss = 0.007283700164407492
iteration 238, loss = 0.006988639011979103
iteration 239, loss = 0.0050660246051847935
iteration 240, loss = 0.00630909251049161
iteration 241, loss = 0.005813018884509802
iteration 242, loss = 0.005468510091304779
iteration 243, loss = 0.005326660815626383
iteration 244, loss = 0.008990873582661152
iteration 245, loss = 0.005147940944880247
iteration 246, loss = 0.005311197601258755
iteration 247, loss = 0.004927010741084814
iteration 248, loss = 0.005505754612386227
iteration 249, loss = 0.005009324289858341
iteration 250, loss = 0.007434781640768051
iteration 251, loss = 0.0050440882332623005
iteration 252, loss = 0.005357092712074518
iteration 253, loss = 0.007513954304158688
iteration 254, loss = 0.0054231793619692326
iteration 255, loss = 0.004875742364674807
iteration 256, loss = 0.005764906760305166
iteration 257, loss = 0.004982066806405783
iteration 258, loss = 0.0086371386423707
iteration 259, loss = 0.0066751111298799515
iteration 260, loss = 0.005193257704377174
iteration 261, loss = 0.008622079156339169
iteration 262, loss = 0.005377934314310551
iteration 263, loss = 0.005466064438223839
iteration 264, loss = 0.005312194582074881
iteration 265, loss = 0.005235549993813038
iteration 266, loss = 0.005533824674785137
iteration 267, loss = 0.005404114257544279
iteration 268, loss = 0.007654852699488401
iteration 269, loss = 0.007031704299151897
iteration 270, loss = 0.00769339594990015
iteration 271, loss = 0.0047694239765405655
iteration 272, loss = 0.008524824865162373
iteration 273, loss = 0.004910989664494991
iteration 274, loss = 0.005530632100999355
iteration 275, loss = 0.0067434366792440414
iteration 276, loss = 0.0048211924731731415
iteration 277, loss = 0.005871121771633625
iteration 278, loss = 0.005503898952156305
iteration 279, loss = 0.005217462778091431
iteration 280, loss = 0.006105434149503708
iteration 281, loss = 0.005557685624808073
iteration 282, loss = 0.005397073458880186
iteration 283, loss = 0.007410947233438492
iteration 284, loss = 0.006443975958973169
iteration 285, loss = 0.005173900630325079
iteration 286, loss = 0.0049459789879620075
iteration 287, loss = 0.007190331816673279
iteration 288, loss = 0.004938079975545406
iteration 289, loss = 0.005708471871912479
iteration 290, loss = 0.0053979456424713135
iteration 291, loss = 0.004835934843868017
iteration 292, loss = 0.008352125063538551
iteration 293, loss = 0.004697067197412252
iteration 294, loss = 0.005237712524831295
iteration 295, loss = 0.004675122909247875
iteration 296, loss = 0.0055321622639894485
iteration 297, loss = 0.005719526670873165
iteration 298, loss = 0.006772513967007399
iteration 299, loss = 0.00474255857989192
iteration 300, loss = 0.005040033720433712
iteration 1, loss = 0.005541948601603508
iteration 2, loss = 0.0050770980305969715
iteration 3, loss = 0.004809829872101545
iteration 4, loss = 0.006039568223059177
iteration 5, loss = 0.0081957271322608
iteration 6, loss = 0.00523960217833519
iteration 7, loss = 0.005339927040040493
iteration 8, loss = 0.005003565922379494
iteration 9, loss = 0.005319465883076191
iteration 10, loss = 0.004744166508316994
iteration 11, loss = 0.004579044878482819
iteration 12, loss = 0.007458933629095554
iteration 13, loss = 0.005141485948115587
iteration 14, loss = 0.004871498327702284
iteration 15, loss = 0.005245046690106392
iteration 16, loss = 0.005680715199559927
iteration 17, loss = 0.005112411919981241
iteration 18, loss = 0.004725269507616758
iteration 19, loss = 0.0068300324492156506
iteration 20, loss = 0.004907747730612755
iteration 21, loss = 0.008720936253666878
iteration 22, loss = 0.004872937686741352
iteration 23, loss = 0.006595520302653313
iteration 24, loss = 0.004944671411067247
iteration 25, loss = 0.007046983577311039
iteration 26, loss = 0.0050657023675739765
iteration 27, loss = 0.00484108692035079
iteration 28, loss = 0.005482647567987442
iteration 29, loss = 0.006477032788097858
iteration 30, loss = 0.005230459850281477
iteration 31, loss = 0.004938403610140085
iteration 32, loss = 0.005323814693838358
iteration 33, loss = 0.005335391499102116
iteration 34, loss = 0.007998990826308727
iteration 35, loss = 0.0054826498962938786
iteration 36, loss = 0.005733166821300983
iteration 37, loss = 0.005777273792773485
iteration 38, loss = 0.005222944542765617
iteration 39, loss = 0.005155843682587147
iteration 40, loss = 0.006517333909869194
iteration 41, loss = 0.005354501307010651
iteration 42, loss = 0.006229396443814039
iteration 43, loss = 0.005581867881119251
iteration 44, loss = 0.0048081823624670506
iteration 45, loss = 0.005526347551494837
iteration 46, loss = 0.007116212043911219
iteration 47, loss = 0.008960799314081669
iteration 48, loss = 0.005257066339254379
iteration 49, loss = 0.007508035749197006
iteration 50, loss = 0.004746711812913418
iteration 51, loss = 0.0064752656035125256
iteration 52, loss = 0.0056373244151473045
iteration 53, loss = 0.004819964524358511
iteration 54, loss = 0.005904210265725851
iteration 55, loss = 0.0051490371115505695
iteration 56, loss = 0.0075768968090415
iteration 57, loss = 0.005236883647739887
iteration 58, loss = 0.004598063416779041
iteration 59, loss = 0.004494157154113054
iteration 60, loss = 0.004799300339072943
iteration 61, loss = 0.004896687343716621
iteration 62, loss = 0.00849032960832119
iteration 63, loss = 0.00500686839222908
iteration 64, loss = 0.00552458968013525
iteration 65, loss = 0.004449755884706974
iteration 66, loss = 0.004848320037126541
iteration 67, loss = 0.006016181316226721
iteration 68, loss = 0.005544410087168217
iteration 69, loss = 0.005474244710057974
iteration 70, loss = 0.006773737259209156
iteration 71, loss = 0.007852365262806416
iteration 72, loss = 0.00478415796533227
iteration 73, loss = 0.0068866536021232605
iteration 74, loss = 0.007405921351164579
iteration 75, loss = 0.005429461598396301
iteration 76, loss = 0.006363387685269117
iteration 77, loss = 0.0053169261664152145
iteration 78, loss = 0.006450254935771227
iteration 79, loss = 0.0048484778963029385
iteration 80, loss = 0.004999755881726742
iteration 81, loss = 0.004798488225787878
iteration 82, loss = 0.008262945339083672
iteration 83, loss = 0.004973587114363909
iteration 84, loss = 0.009461171925067902
iteration 85, loss = 0.005051000975072384
iteration 86, loss = 0.004917087499052286
iteration 87, loss = 0.005909166764467955
iteration 88, loss = 0.005791454575955868
iteration 89, loss = 0.005088142119348049
iteration 90, loss = 0.004787952173501253
iteration 91, loss = 0.006971004419028759
iteration 92, loss = 0.005117296706885099
iteration 93, loss = 0.006876967381685972
iteration 94, loss = 0.005384582094848156
iteration 95, loss = 0.005516992881894112
iteration 96, loss = 0.0048714508302509785
iteration 97, loss = 0.005721375811845064
iteration 98, loss = 0.005417523439973593
iteration 99, loss = 0.004507395438849926
iteration 100, loss = 0.004960008431226015
iteration 101, loss = 0.0053390334360301495
iteration 102, loss = 0.004964047111570835
iteration 103, loss = 0.005320217460393906
iteration 104, loss = 0.004762883298099041
iteration 105, loss = 0.007472134195268154
iteration 106, loss = 0.005070786457508802
iteration 107, loss = 0.005089324899017811
iteration 108, loss = 0.0058681536465883255
iteration 109, loss = 0.006852948106825352
iteration 110, loss = 0.005319369491189718
iteration 111, loss = 0.006222207564860582
iteration 112, loss = 0.004453642293810844
iteration 113, loss = 0.006528959143906832
iteration 114, loss = 0.004714291077107191
iteration 115, loss = 0.006668926682323217
iteration 116, loss = 0.005437150597572327
iteration 117, loss = 0.0049616554751992226
iteration 118, loss = 0.006668523885309696
iteration 119, loss = 0.005074719432741404
iteration 120, loss = 0.00837988592684269
iteration 121, loss = 0.005057154223322868
iteration 122, loss = 0.004779148381203413
iteration 123, loss = 0.005138768814504147
iteration 124, loss = 0.00529449200257659
iteration 125, loss = 0.005967858247458935
iteration 126, loss = 0.008690296672284603
iteration 127, loss = 0.007046655751764774
iteration 128, loss = 0.004968848545104265
iteration 129, loss = 0.006339280866086483
iteration 130, loss = 0.005297637544572353
iteration 131, loss = 0.005112225189805031
iteration 132, loss = 0.00547883752733469
iteration 133, loss = 0.006055500824004412
iteration 134, loss = 0.007129795849323273
iteration 135, loss = 0.0046822079457342625
iteration 136, loss = 0.005832355469465256
iteration 137, loss = 0.005013801623135805
iteration 138, loss = 0.005552062764763832
iteration 139, loss = 0.0044759828597307205
iteration 140, loss = 0.004657940473407507
iteration 141, loss = 0.005561075173318386
iteration 142, loss = 0.0053142281249165535
iteration 143, loss = 0.005160830449312925
iteration 144, loss = 0.005393825471401215
iteration 145, loss = 0.005261371377855539
iteration 146, loss = 0.0049054441042244434
iteration 147, loss = 0.005174607038497925
iteration 148, loss = 0.005516877397894859
iteration 149, loss = 0.0050969477742910385
iteration 150, loss = 0.008003913797438145
iteration 151, loss = 0.005142794921994209
iteration 152, loss = 0.005416386760771275
iteration 153, loss = 0.006537667475640774
iteration 154, loss = 0.005132591817528009
iteration 155, loss = 0.0046967086382210255
iteration 156, loss = 0.004968783352524042
iteration 157, loss = 0.006628107745200396
iteration 158, loss = 0.004995767492800951
iteration 159, loss = 0.006476104259490967
iteration 160, loss = 0.005666153971105814
iteration 161, loss = 0.005674534942954779
iteration 162, loss = 0.004854266531765461
iteration 163, loss = 0.004762382246553898
iteration 164, loss = 0.005345347337424755
iteration 165, loss = 0.004748842213302851
iteration 166, loss = 0.004711291752755642
iteration 167, loss = 0.005647415295243263
iteration 168, loss = 0.005208068061619997
iteration 169, loss = 0.006505082827061415
iteration 170, loss = 0.007534611038863659
iteration 171, loss = 0.004760795272886753
iteration 172, loss = 0.005058123730123043
iteration 173, loss = 0.005020221695303917
iteration 174, loss = 0.005323569290339947
iteration 175, loss = 0.005043632350862026
iteration 176, loss = 0.006387408822774887
iteration 177, loss = 0.006533463019877672
iteration 178, loss = 0.005204517859965563
iteration 179, loss = 0.004807718098163605
iteration 180, loss = 0.005901008378714323
iteration 181, loss = 0.009628180414438248
iteration 182, loss = 0.00847618281841278
iteration 183, loss = 0.008207378908991814
iteration 184, loss = 0.006837588269263506
iteration 185, loss = 0.005018720868974924
iteration 186, loss = 0.004656076431274414
iteration 187, loss = 0.004998150747269392
iteration 188, loss = 0.0054366812109947205
iteration 189, loss = 0.0055733611807227135
iteration 190, loss = 0.007154070772230625
iteration 191, loss = 0.005062784068286419
iteration 192, loss = 0.006366649642586708
iteration 193, loss = 0.00523578654974699
iteration 194, loss = 0.00471137510612607
iteration 195, loss = 0.004868808668106794
iteration 196, loss = 0.00681941257789731
iteration 197, loss = 0.008453943766653538
iteration 198, loss = 0.00462785828858614
iteration 199, loss = 0.0049897488206624985
iteration 200, loss = 0.007602266501635313
iteration 201, loss = 0.004898291081190109
iteration 202, loss = 0.004839766304939985
iteration 203, loss = 0.005047634243965149
iteration 204, loss = 0.007031541783362627
iteration 205, loss = 0.005075765307992697
iteration 206, loss = 0.0067103272303938866
iteration 207, loss = 0.00506818201392889
iteration 208, loss = 0.005127560347318649
iteration 209, loss = 0.005544627085328102
iteration 210, loss = 0.0050098528154194355
iteration 211, loss = 0.00499041797593236
iteration 212, loss = 0.005205226130783558
iteration 213, loss = 0.005164103116840124
iteration 214, loss = 0.005118128843605518
iteration 215, loss = 0.004605090711265802
iteration 216, loss = 0.0052892533130943775
iteration 217, loss = 0.00508322985842824
iteration 218, loss = 0.004824819974601269
iteration 219, loss = 0.005311064422130585
iteration 220, loss = 0.004687839653342962
iteration 221, loss = 0.005183265078812838
iteration 222, loss = 0.006135237403213978
iteration 223, loss = 0.006954194512218237
iteration 224, loss = 0.005285236984491348
iteration 225, loss = 0.004786137957125902
iteration 226, loss = 0.004829852841794491
iteration 227, loss = 0.012819381430745125
iteration 228, loss = 0.00470411591231823
iteration 229, loss = 0.004689712077379227
iteration 230, loss = 0.004763027653098106
iteration 231, loss = 0.005785889457911253
iteration 232, loss = 0.005214609671384096
iteration 233, loss = 0.005226524081081152
iteration 234, loss = 0.005498364567756653
iteration 235, loss = 0.0047441148199141026
iteration 236, loss = 0.004746693652123213
iteration 237, loss = 0.005117216147482395
iteration 238, loss = 0.006602434907108545
iteration 239, loss = 0.0051525654271245
iteration 240, loss = 0.004916121251881123
iteration 241, loss = 0.004840118810534477
iteration 242, loss = 0.004996503237634897
iteration 243, loss = 0.0070329699665308
iteration 244, loss = 0.005534566938877106
iteration 245, loss = 0.004843585193157196
iteration 246, loss = 0.005253931507468224
iteration 247, loss = 0.004810878541320562
iteration 248, loss = 0.006155720911920071
iteration 249, loss = 0.005359388887882233
iteration 250, loss = 0.004814821295440197
iteration 251, loss = 0.005193924997001886
iteration 252, loss = 0.007022953126579523
iteration 253, loss = 0.005448904819786549
iteration 254, loss = 0.005108850076794624
iteration 255, loss = 0.005235002841800451
iteration 256, loss = 0.008662521839141846
iteration 257, loss = 0.008171521127223969
iteration 258, loss = 0.006306929048150778
iteration 259, loss = 0.005780249834060669
iteration 260, loss = 0.004755539353936911
iteration 261, loss = 0.004922805353999138
iteration 262, loss = 0.0047849914990365505
iteration 263, loss = 0.005575486458837986
iteration 264, loss = 0.0073208073154091835
iteration 265, loss = 0.0071707735769450665
iteration 266, loss = 0.00616199616342783
iteration 267, loss = 0.0053580994717776775
iteration 268, loss = 0.005857233423739672
iteration 269, loss = 0.005060482304543257
iteration 270, loss = 0.004831110592931509
iteration 271, loss = 0.004944055341184139
iteration 272, loss = 0.005140072200447321
iteration 273, loss = 0.004925737157464027
iteration 274, loss = 0.005533013492822647
iteration 275, loss = 0.0053407661616802216
iteration 276, loss = 0.00476765027269721
iteration 277, loss = 0.005045413970947266
iteration 278, loss = 0.006834335159510374
iteration 279, loss = 0.004766846541315317
iteration 280, loss = 0.005099905654788017
iteration 281, loss = 0.0057942308485507965
iteration 282, loss = 0.005991361103951931
iteration 283, loss = 0.006536656524986029
iteration 284, loss = 0.0054975212551653385
iteration 285, loss = 0.005056522786617279
iteration 286, loss = 0.004868855234235525
iteration 287, loss = 0.005070907529443502
iteration 288, loss = 0.006809631362557411
iteration 289, loss = 0.005671049002557993
iteration 290, loss = 0.005499748513102531
iteration 291, loss = 0.0050629423931241035
iteration 292, loss = 0.005349252372980118
iteration 293, loss = 0.004922900348901749
iteration 294, loss = 0.005285535007715225
iteration 295, loss = 0.004901490174233913
iteration 296, loss = 0.0051015159115195274
iteration 297, loss = 0.005753691308200359
iteration 298, loss = 0.0052247606217861176
iteration 299, loss = 0.005221592262387276
iteration 300, loss = 0.004771203733980656
iteration 1, loss = 0.006535571999847889
iteration 2, loss = 0.005015521310269833
iteration 3, loss = 0.005071211140602827
iteration 4, loss = 0.004745496436953545
iteration 5, loss = 0.005067621357738972
iteration 6, loss = 0.00823352299630642
iteration 7, loss = 0.006655651144683361
iteration 8, loss = 0.0055015613324940205
iteration 9, loss = 0.005006400402635336
iteration 10, loss = 0.005398014560341835
iteration 11, loss = 0.009662441909313202
iteration 12, loss = 0.004800387658178806
iteration 13, loss = 0.005694039631634951
iteration 14, loss = 0.005029384046792984
iteration 15, loss = 0.007180471438914537
iteration 16, loss = 0.0071813794784247875
iteration 17, loss = 0.004657457582652569
iteration 18, loss = 0.00876863393932581
iteration 19, loss = 0.004917726386338472
iteration 20, loss = 0.004933914635330439
iteration 21, loss = 0.006251181475818157
iteration 22, loss = 0.0048466455191373825
iteration 23, loss = 0.0045866891741752625
iteration 24, loss = 0.005033186636865139
iteration 25, loss = 0.006090241018682718
iteration 26, loss = 0.006403167266398668
iteration 27, loss = 0.004801529925316572
iteration 28, loss = 0.0053108627907931805
iteration 29, loss = 0.00472354656085372
iteration 30, loss = 0.005241342820227146
iteration 31, loss = 0.007605500053614378
iteration 32, loss = 0.005093389190733433
iteration 33, loss = 0.004864542745053768
iteration 34, loss = 0.00550443772226572
iteration 35, loss = 0.004471250809729099
iteration 36, loss = 0.007624630816280842
iteration 37, loss = 0.004968784283846617
iteration 38, loss = 0.0052245501428842545
iteration 39, loss = 0.004683844745159149
iteration 40, loss = 0.005041910335421562
iteration 41, loss = 0.004928881768137217
iteration 42, loss = 0.005584637634456158
iteration 43, loss = 0.00514503987506032
iteration 44, loss = 0.005085867363959551
iteration 45, loss = 0.0045998659916222095
iteration 46, loss = 0.0065879616886377335
iteration 47, loss = 0.005258440971374512
iteration 48, loss = 0.008174056187272072
iteration 49, loss = 0.005030269268900156
iteration 50, loss = 0.004750082269310951
iteration 51, loss = 0.005255746655166149
iteration 52, loss = 0.008303634822368622
iteration 53, loss = 0.007834034971892834
iteration 54, loss = 0.005301028490066528
iteration 55, loss = 0.004830561578273773
iteration 56, loss = 0.004857633728533983
iteration 57, loss = 0.005709164310246706
iteration 58, loss = 0.005505254492163658
iteration 59, loss = 0.0064145890064537525
iteration 60, loss = 0.005012203473597765
iteration 61, loss = 0.00824255682528019
iteration 62, loss = 0.004920036997646093
iteration 63, loss = 0.007606050465255976
iteration 64, loss = 0.0050745075568556786
iteration 65, loss = 0.0059099383652210236
iteration 66, loss = 0.005087150726467371
iteration 67, loss = 0.006522039417177439
iteration 68, loss = 0.00484984228387475
iteration 69, loss = 0.004789586644619703
iteration 70, loss = 0.007253584451973438
iteration 71, loss = 0.005281646735966206
iteration 72, loss = 0.0052499547600746155
iteration 73, loss = 0.0046846684999763966
iteration 74, loss = 0.005185750778764486
iteration 75, loss = 0.005437702406197786
iteration 76, loss = 0.006966703105717897
iteration 77, loss = 0.006333197001367807
iteration 78, loss = 0.00737040676176548
iteration 79, loss = 0.0055915918201208115
iteration 80, loss = 0.008634700439870358
iteration 81, loss = 0.004702047910541296
iteration 82, loss = 0.006174252834171057
iteration 83, loss = 0.005145900417119265
iteration 84, loss = 0.005328164901584387
iteration 85, loss = 0.005240913946181536
iteration 86, loss = 0.005358690861612558
iteration 87, loss = 0.006148101296275854
iteration 88, loss = 0.004858956206589937
iteration 89, loss = 0.004632317461073399
iteration 90, loss = 0.005554593168199062
iteration 91, loss = 0.007255431264638901
iteration 92, loss = 0.0084310844540596
iteration 93, loss = 0.005544413812458515
iteration 94, loss = 0.008110054768621922
iteration 95, loss = 0.005063347518444061
iteration 96, loss = 0.0049997540190815926
iteration 97, loss = 0.0062412661500275135
iteration 98, loss = 0.0051999762654304504
iteration 99, loss = 0.0046499352902174
iteration 100, loss = 0.0052954996936023235
iteration 101, loss = 0.006616807542741299
iteration 102, loss = 0.005546740721911192
iteration 103, loss = 0.004836259875446558
iteration 104, loss = 0.005375776439905167
iteration 105, loss = 0.004671684466302395
iteration 106, loss = 0.004870538599789143
iteration 107, loss = 0.004968761466443539
iteration 108, loss = 0.007340109907090664
iteration 109, loss = 0.007837734185159206
iteration 110, loss = 0.005192528013139963
iteration 111, loss = 0.005735824350267649
iteration 112, loss = 0.0051575517281889915
iteration 113, loss = 0.006057235412299633
iteration 114, loss = 0.00476407865062356
iteration 115, loss = 0.005812285467982292
iteration 116, loss = 0.008146842010319233
iteration 117, loss = 0.0048845489509403706
iteration 118, loss = 0.0052015348337590694
iteration 119, loss = 0.005093663465231657
iteration 120, loss = 0.005295621231198311
iteration 121, loss = 0.004805650096386671
iteration 122, loss = 0.005234910175204277
iteration 123, loss = 0.005011172499507666
iteration 124, loss = 0.005528338719159365
iteration 125, loss = 0.005666476208716631
iteration 126, loss = 0.004942234605550766
iteration 127, loss = 0.0059909275732934475
iteration 128, loss = 0.0044136689975857735
iteration 129, loss = 0.005179783795028925
iteration 130, loss = 0.006389984395354986
iteration 131, loss = 0.0070599899627268314
iteration 132, loss = 0.005214492324739695
iteration 133, loss = 0.00493237841874361
iteration 134, loss = 0.005217317491769791
iteration 135, loss = 0.004872133024036884
iteration 136, loss = 0.004902816377580166
iteration 137, loss = 0.0051196422427892685
iteration 138, loss = 0.005310367792844772
iteration 139, loss = 0.004698276985436678
iteration 140, loss = 0.005008871667087078
iteration 141, loss = 0.010393386706709862
iteration 142, loss = 0.004792198538780212
iteration 143, loss = 0.00485738180577755
iteration 144, loss = 0.005018092226237059
iteration 145, loss = 0.005183977074921131
iteration 146, loss = 0.005284409504383802
iteration 147, loss = 0.005444671027362347
iteration 148, loss = 0.005167078692466021
iteration 149, loss = 0.005590054206550121
iteration 150, loss = 0.004879172425717115
iteration 151, loss = 0.004863769747316837
iteration 152, loss = 0.00586715480312705
iteration 153, loss = 0.004953605588525534
iteration 154, loss = 0.005326795857399702
iteration 155, loss = 0.0060018994845449924
iteration 156, loss = 0.005376653280109167
iteration 157, loss = 0.008018450811505318
iteration 158, loss = 0.005766262300312519
iteration 159, loss = 0.0053391526453197
iteration 160, loss = 0.0070058126002550125
iteration 161, loss = 0.006513472180813551
iteration 162, loss = 0.005470932926982641
iteration 163, loss = 0.004553488455712795
iteration 164, loss = 0.005162935703992844
iteration 165, loss = 0.004813469480723143
iteration 166, loss = 0.006414819974452257
iteration 167, loss = 0.004854104481637478
iteration 168, loss = 0.004600289277732372
iteration 169, loss = 0.005560418590903282
iteration 170, loss = 0.004621288739144802
iteration 171, loss = 0.004575241357088089
iteration 172, loss = 0.004818382672965527
iteration 173, loss = 0.004931606352329254
iteration 174, loss = 0.004418851342052221
iteration 175, loss = 0.005553309805691242
iteration 176, loss = 0.005537229124456644
iteration 177, loss = 0.007007178384810686
iteration 178, loss = 0.0047873458825051785
iteration 179, loss = 0.006643298082053661
iteration 180, loss = 0.004970096983015537
iteration 181, loss = 0.006041763816028833
iteration 182, loss = 0.00517257209867239
iteration 183, loss = 0.006155290640890598
iteration 184, loss = 0.004566321615129709
iteration 185, loss = 0.0068913595750927925
iteration 186, loss = 0.004974386654794216
iteration 187, loss = 0.0050196219235658646
iteration 188, loss = 0.004979557357728481
iteration 189, loss = 0.004600475076586008
iteration 190, loss = 0.005401258822530508
iteration 191, loss = 0.004795989487320185
iteration 192, loss = 0.005610757507383823
iteration 193, loss = 0.004876798950135708
iteration 194, loss = 0.0048469905741512775
iteration 195, loss = 0.004839873872697353
iteration 196, loss = 0.0048635113053023815
iteration 197, loss = 0.004505104385316372
iteration 198, loss = 0.004783642012625933
iteration 199, loss = 0.005109080113470554
iteration 200, loss = 0.0049543725326657295
iteration 201, loss = 0.006126902997493744
iteration 202, loss = 0.00483362190425396
iteration 203, loss = 0.005488068796694279
iteration 204, loss = 0.006831468548625708
iteration 205, loss = 0.005435821134597063
iteration 206, loss = 0.004779470153152943
iteration 207, loss = 0.005072572734206915
iteration 208, loss = 0.00557882571592927
iteration 209, loss = 0.0070961350575089455
iteration 210, loss = 0.004954773001372814
iteration 211, loss = 0.0049953400157392025
iteration 212, loss = 0.004387230612337589
iteration 213, loss = 0.004631583578884602
iteration 214, loss = 0.0065865470096468925
iteration 215, loss = 0.005149989388883114
iteration 216, loss = 0.005894159898161888
iteration 217, loss = 0.006654344964772463
iteration 218, loss = 0.005095173139125109
iteration 219, loss = 0.0049971044063568115
iteration 220, loss = 0.005540478974580765
iteration 221, loss = 0.006810562685132027
iteration 222, loss = 0.006635603960603476
iteration 223, loss = 0.005068053957074881
iteration 224, loss = 0.008198452182114124
iteration 225, loss = 0.0048144180327653885
iteration 226, loss = 0.005069136619567871
iteration 227, loss = 0.0077541423961520195
iteration 228, loss = 0.005546269007027149
iteration 229, loss = 0.0052057895809412
iteration 230, loss = 0.006557079963386059
iteration 231, loss = 0.0048788366839289665
iteration 232, loss = 0.005254706833511591
iteration 233, loss = 0.005066001322120428
iteration 234, loss = 0.004780873190611601
iteration 235, loss = 0.005138078238815069
iteration 236, loss = 0.0066581298597157
iteration 237, loss = 0.00465365732088685
iteration 238, loss = 0.006540198810398579
iteration 239, loss = 0.006484692916274071
iteration 240, loss = 0.005122603848576546
iteration 241, loss = 0.004728743340820074
iteration 242, loss = 0.004785003140568733
iteration 243, loss = 0.005044387653470039
iteration 244, loss = 0.004712636582553387
iteration 245, loss = 0.004970754962414503
iteration 246, loss = 0.0053648329339921474
iteration 247, loss = 0.00491156242787838
iteration 248, loss = 0.0050158631056547165
iteration 249, loss = 0.005097278859466314
iteration 250, loss = 0.006858253851532936
iteration 251, loss = 0.005100280977785587
iteration 252, loss = 0.006840420886874199
iteration 253, loss = 0.004797583445906639
iteration 254, loss = 0.008138374425470829
iteration 255, loss = 0.006566628813743591
iteration 256, loss = 0.004794841632246971
iteration 257, loss = 0.0047906613908708096
iteration 258, loss = 0.005158507265150547
iteration 259, loss = 0.007180832792073488
iteration 260, loss = 0.00477024307474494
iteration 261, loss = 0.008364091627299786
iteration 262, loss = 0.004950875882059336
iteration 263, loss = 0.004778746515512466
iteration 264, loss = 0.0052324458956718445
iteration 265, loss = 0.004917683079838753
iteration 266, loss = 0.008327013812959194
iteration 267, loss = 0.0069370451383292675
iteration 268, loss = 0.004660196602344513
iteration 269, loss = 0.005097101908177137
iteration 270, loss = 0.004716795869171619
iteration 271, loss = 0.005456116516143084
iteration 272, loss = 0.004981161095201969
iteration 273, loss = 0.004741255193948746
iteration 274, loss = 0.005240245722234249
iteration 275, loss = 0.005292090121656656
iteration 276, loss = 0.005406850483268499
iteration 277, loss = 0.004819477908313274
iteration 278, loss = 0.004947925452142954
iteration 279, loss = 0.005882441997528076
iteration 280, loss = 0.00502563314512372
iteration 281, loss = 0.00486907921731472
iteration 282, loss = 0.0050601111724972725
iteration 283, loss = 0.004487748723477125
iteration 284, loss = 0.005035819951444864
iteration 285, loss = 0.004406629130244255
iteration 286, loss = 0.005053036846220493
iteration 287, loss = 0.005631850566715002
iteration 288, loss = 0.005562295205891132
iteration 289, loss = 0.004540588241070509
iteration 290, loss = 0.005860069766640663
iteration 291, loss = 0.004628628492355347
iteration 292, loss = 0.005631773732602596
iteration 293, loss = 0.00499920966103673
iteration 294, loss = 0.005418321117758751
iteration 295, loss = 0.005302739329636097
iteration 296, loss = 0.0052623385563492775
iteration 297, loss = 0.0045812842436134815
iteration 298, loss = 0.004769574850797653
iteration 299, loss = 0.004987163469195366
iteration 300, loss = 0.005096341483294964
iteration 1, loss = 0.004923962987959385
iteration 2, loss = 0.005253078415989876
iteration 3, loss = 0.004553230945020914
iteration 4, loss = 0.008119834586977959
iteration 5, loss = 0.005571248009800911
iteration 6, loss = 0.005036824382841587
iteration 7, loss = 0.004964612424373627
iteration 8, loss = 0.004872211720794439
iteration 9, loss = 0.0046550072729587555
iteration 10, loss = 0.004871516488492489
iteration 11, loss = 0.005463684443384409
iteration 12, loss = 0.00541531341150403
iteration 13, loss = 0.006478028371930122
iteration 14, loss = 0.005546011496335268
iteration 15, loss = 0.00506970752030611
iteration 16, loss = 0.0063785468228161335
iteration 17, loss = 0.005763542372733355
iteration 18, loss = 0.005478788632899523
iteration 19, loss = 0.005291366949677467
iteration 20, loss = 0.004927131347358227
iteration 21, loss = 0.0053284913301467896
iteration 22, loss = 0.005720739718526602
iteration 23, loss = 0.004659007303416729
iteration 24, loss = 0.004808304365724325
iteration 25, loss = 0.005677458830177784
iteration 26, loss = 0.0055131856352090836
iteration 27, loss = 0.004856844898313284
iteration 28, loss = 0.005857127718627453
iteration 29, loss = 0.0047014737501740456
iteration 30, loss = 0.005118542816489935
iteration 31, loss = 0.005267803557217121
iteration 32, loss = 0.005520870443433523
iteration 33, loss = 0.009627588093280792
iteration 34, loss = 0.004873388446867466
iteration 35, loss = 0.005133816506713629
iteration 36, loss = 0.006331794895231724
iteration 37, loss = 0.004794977139681578
iteration 38, loss = 0.007737424690276384
iteration 39, loss = 0.007951614446938038
iteration 40, loss = 0.005366744007915258
iteration 41, loss = 0.0048536453396081924
iteration 42, loss = 0.005153585225343704
iteration 43, loss = 0.0061858054250478745
iteration 44, loss = 0.005114572122693062
iteration 45, loss = 0.005164364352822304
iteration 46, loss = 0.006324932910501957
iteration 47, loss = 0.005059015471488237
iteration 48, loss = 0.004719073884189129
iteration 49, loss = 0.006815675646066666
iteration 50, loss = 0.00449650501832366
iteration 51, loss = 0.005158463958650827
iteration 52, loss = 0.004531236831098795
iteration 53, loss = 0.005165667273104191
iteration 54, loss = 0.0048009674064815044
iteration 55, loss = 0.007791443727910519
iteration 56, loss = 0.004903164692223072
iteration 57, loss = 0.0044023809023201466
iteration 58, loss = 0.004888246767222881
iteration 59, loss = 0.005716624669730663
iteration 60, loss = 0.006767061073333025
iteration 61, loss = 0.004439854063093662
iteration 62, loss = 0.005577140487730503
iteration 63, loss = 0.0048296269960701466
iteration 64, loss = 0.005131286568939686
iteration 65, loss = 0.004993843846023083
iteration 66, loss = 0.004747922532260418
iteration 67, loss = 0.004743698053061962
iteration 68, loss = 0.004958451259881258
iteration 69, loss = 0.004696197342127562
iteration 70, loss = 0.00688517140224576
iteration 71, loss = 0.0046494691632688046
iteration 72, loss = 0.004774096887558699
iteration 73, loss = 0.006667786743491888
iteration 74, loss = 0.00690658250823617
iteration 75, loss = 0.005031874869018793
iteration 76, loss = 0.004986794199794531
iteration 77, loss = 0.005098625086247921
iteration 78, loss = 0.006021255627274513
iteration 79, loss = 0.004693090915679932
iteration 80, loss = 0.004387278109788895
iteration 81, loss = 0.004820919129997492
iteration 82, loss = 0.005353294778615236
iteration 83, loss = 0.00466090627014637
iteration 84, loss = 0.005079398397356272
iteration 85, loss = 0.005980080924928188
iteration 86, loss = 0.005230644252151251
iteration 87, loss = 0.005670045502483845
iteration 88, loss = 0.004614466801285744
iteration 89, loss = 0.008449416607618332
iteration 90, loss = 0.00594682339578867
iteration 91, loss = 0.005410608369857073
iteration 92, loss = 0.006937629077583551
iteration 93, loss = 0.004793826024979353
iteration 94, loss = 0.004920609295368195
iteration 95, loss = 0.004876767285168171
iteration 96, loss = 0.0063807228580117226
iteration 97, loss = 0.00471226591616869
iteration 98, loss = 0.004832458682358265
iteration 99, loss = 0.004800548776984215
iteration 100, loss = 0.004788201302289963
iteration 101, loss = 0.004625475965440273
iteration 102, loss = 0.008085938170552254
iteration 103, loss = 0.004709586501121521
iteration 104, loss = 0.005114406812936068
iteration 105, loss = 0.004297136329114437
iteration 106, loss = 0.004840813111513853
iteration 107, loss = 0.005454748868942261
iteration 108, loss = 0.005987504962831736
iteration 109, loss = 0.005352287087589502
iteration 110, loss = 0.005437235347926617
iteration 111, loss = 0.005437003448605537
iteration 112, loss = 0.005117472726851702
iteration 113, loss = 0.0045551215298473835
iteration 114, loss = 0.004823606461286545
iteration 115, loss = 0.008571361191570759
iteration 116, loss = 0.005147890187799931
iteration 117, loss = 0.005290619563311338
iteration 118, loss = 0.008184677921235561
iteration 119, loss = 0.008550405502319336
iteration 120, loss = 0.004618414212018251
iteration 121, loss = 0.004994452930986881
iteration 122, loss = 0.005219456739723682
iteration 123, loss = 0.004970153793692589
iteration 124, loss = 0.006150905974209309
iteration 125, loss = 0.0052651590667665005
iteration 126, loss = 0.005073022563010454
iteration 127, loss = 0.00475558266043663
iteration 128, loss = 0.0055612921714782715
iteration 129, loss = 0.007119693793356419
iteration 130, loss = 0.005509315989911556
iteration 131, loss = 0.00501633994281292
iteration 132, loss = 0.004571926314383745
iteration 133, loss = 0.005436131730675697
iteration 134, loss = 0.006517171859741211
iteration 135, loss = 0.005524476058781147
iteration 136, loss = 0.00459574768319726
iteration 137, loss = 0.004876146093010902
iteration 138, loss = 0.008185635320842266
iteration 139, loss = 0.0063089532777667046
iteration 140, loss = 0.007404517848044634
iteration 141, loss = 0.0045572142116725445
iteration 142, loss = 0.012824214063584805
iteration 143, loss = 0.004868615418672562
iteration 144, loss = 0.00786355696618557
iteration 145, loss = 0.0043617538176476955
iteration 146, loss = 0.0045014615170657635
iteration 147, loss = 0.006826662924140692
iteration 148, loss = 0.004474266432225704
iteration 149, loss = 0.004870000295341015
iteration 150, loss = 0.004731909371912479
iteration 151, loss = 0.006155416835099459
iteration 152, loss = 0.0055065760388970375
iteration 153, loss = 0.0061119319871068
iteration 154, loss = 0.005082869902253151
iteration 155, loss = 0.004930924624204636
iteration 156, loss = 0.00637373561039567
iteration 157, loss = 0.0048029664903879166
iteration 158, loss = 0.004408232402056456
iteration 159, loss = 0.004770768340677023
iteration 160, loss = 0.008549632504582405
iteration 161, loss = 0.006444678176194429
iteration 162, loss = 0.0051595671102404594
iteration 163, loss = 0.0050496323965489864
iteration 164, loss = 0.004912100732326508
iteration 165, loss = 0.005619887262582779
iteration 166, loss = 0.0045770080760121346
iteration 167, loss = 0.004674720112234354
iteration 168, loss = 0.005953249521553516
iteration 169, loss = 0.004840729758143425
iteration 170, loss = 0.004522968549281359
iteration 171, loss = 0.005259193014353514
iteration 172, loss = 0.004604015499353409
iteration 173, loss = 0.004941725172102451
iteration 174, loss = 0.00603565713390708
iteration 175, loss = 0.0048340726643800735
iteration 176, loss = 0.004723190329968929
iteration 177, loss = 0.00493798078969121
iteration 178, loss = 0.008309301920235157
iteration 179, loss = 0.007918679155409336
iteration 180, loss = 0.004742914345115423
iteration 181, loss = 0.004823549184948206
iteration 182, loss = 0.005196808371692896
iteration 183, loss = 0.004853725899010897
iteration 184, loss = 0.005440625362098217
iteration 185, loss = 0.004691351670771837
iteration 186, loss = 0.00502405222505331
iteration 187, loss = 0.004545824136584997
iteration 188, loss = 0.006394022144377232
iteration 189, loss = 0.0071412636898458
iteration 190, loss = 0.005303008481860161
iteration 191, loss = 0.006469094194471836
iteration 192, loss = 0.006250324193388224
iteration 193, loss = 0.004586958326399326
iteration 194, loss = 0.004979895893484354
iteration 195, loss = 0.005236725322902203
iteration 196, loss = 0.005118045024573803
iteration 197, loss = 0.0045142401941120625
iteration 198, loss = 0.004798326175659895
iteration 199, loss = 0.008103628642857075
iteration 200, loss = 0.004762290045619011
iteration 201, loss = 0.006124097388237715
iteration 202, loss = 0.005006419029086828
iteration 203, loss = 0.007581488694995642
iteration 204, loss = 0.00446406751871109
iteration 205, loss = 0.005595962982624769
iteration 206, loss = 0.005408202297985554
iteration 207, loss = 0.004609339404851198
iteration 208, loss = 0.005096303764730692
iteration 209, loss = 0.004784775897860527
iteration 210, loss = 0.004662774503231049
iteration 211, loss = 0.00489811971783638
iteration 212, loss = 0.0055662356317043304
iteration 213, loss = 0.005648593418300152
iteration 214, loss = 0.008645926602184772
iteration 215, loss = 0.00467006815597415
iteration 216, loss = 0.0049099489115178585
iteration 217, loss = 0.004897166509181261
iteration 218, loss = 0.004714474081993103
iteration 219, loss = 0.008570201694965363
iteration 220, loss = 0.00520138768479228
iteration 221, loss = 0.006455727852880955
iteration 222, loss = 0.005190378520637751
iteration 223, loss = 0.004916250705718994
iteration 224, loss = 0.006623154506087303
iteration 225, loss = 0.0048244670033454895
iteration 226, loss = 0.004657890647649765
iteration 227, loss = 0.004887955728918314
iteration 228, loss = 0.006387003231793642
iteration 229, loss = 0.0046850815415382385
iteration 230, loss = 0.00460271630436182
iteration 231, loss = 0.004873787518590689
iteration 232, loss = 0.004530264995992184
iteration 233, loss = 0.005638559348881245
iteration 234, loss = 0.0048286691308021545
iteration 235, loss = 0.0046075861901044846
iteration 236, loss = 0.005346170160919428
iteration 237, loss = 0.004553640726953745
iteration 238, loss = 0.00513315387070179
iteration 239, loss = 0.006822697818279266
iteration 240, loss = 0.006923400331288576
iteration 241, loss = 0.005454071331769228
iteration 242, loss = 0.005910106003284454
iteration 243, loss = 0.005479991901665926
iteration 244, loss = 0.0052781617268919945
iteration 245, loss = 0.0071816593408584595
iteration 246, loss = 0.004800228402018547
iteration 247, loss = 0.005002160556614399
iteration 248, loss = 0.004823069088160992
iteration 249, loss = 0.004846678581088781
iteration 250, loss = 0.005181742832064629
iteration 251, loss = 0.005018673371523619
iteration 252, loss = 0.005010699387639761
iteration 253, loss = 0.005864034406840801
iteration 254, loss = 0.005162438377737999
iteration 255, loss = 0.0048675863072276115
iteration 256, loss = 0.004593782126903534
iteration 257, loss = 0.0052023641765117645
iteration 258, loss = 0.004928910173475742
iteration 259, loss = 0.0049303085543215275
iteration 260, loss = 0.0050330921076238155
iteration 261, loss = 0.004834271967411041
iteration 262, loss = 0.005157446023076773
iteration 263, loss = 0.004896918311715126
iteration 264, loss = 0.005571267567574978
iteration 265, loss = 0.0046597798354923725
iteration 266, loss = 0.005198781378567219
iteration 267, loss = 0.005053387023508549
iteration 268, loss = 0.005351378116756678
iteration 269, loss = 0.005976750049740076
iteration 270, loss = 0.00463464530184865
iteration 271, loss = 0.00973057933151722
iteration 272, loss = 0.004286026582121849
iteration 273, loss = 0.004473153501749039
iteration 274, loss = 0.005113109480589628
iteration 275, loss = 0.004762736614793539
iteration 276, loss = 0.0049079712480306625
iteration 277, loss = 0.00449011055752635
iteration 278, loss = 0.005024249665439129
iteration 279, loss = 0.005102092865854502
iteration 280, loss = 0.0056987046264112
iteration 281, loss = 0.0046267155557870865
iteration 282, loss = 0.005497382953763008
iteration 283, loss = 0.004576687701046467
iteration 284, loss = 0.004686693195253611
iteration 285, loss = 0.004722183104604483
iteration 286, loss = 0.005031255539506674
iteration 287, loss = 0.0052905939519405365
iteration 288, loss = 0.0067807529121637344
iteration 289, loss = 0.00472139660269022
iteration 290, loss = 0.007101384457200766
iteration 291, loss = 0.0047545842826366425
iteration 292, loss = 0.006279428489506245
iteration 293, loss = 0.004527518525719643
iteration 294, loss = 0.0050102174282073975
iteration 295, loss = 0.004990076646208763
iteration 296, loss = 0.006297880318015814
iteration 297, loss = 0.0044859834015369415
iteration 298, loss = 0.0056976210325956345
iteration 299, loss = 0.004912752658128738
iteration 300, loss = 0.00674328813329339
iteration 1, loss = 0.004614762030541897
iteration 2, loss = 0.0046601975336670876
iteration 3, loss = 0.008932682685554028
iteration 4, loss = 0.004862307570874691
iteration 5, loss = 0.004856848157942295
iteration 6, loss = 0.007925854064524174
iteration 7, loss = 0.004884897731244564
iteration 8, loss = 0.004488015081733465
iteration 9, loss = 0.005094695836305618
iteration 10, loss = 0.004608980379998684
iteration 11, loss = 0.004761056043207645
iteration 12, loss = 0.004869280848652124
iteration 13, loss = 0.005086435470730066
iteration 14, loss = 0.0045211524702608585
iteration 15, loss = 0.004978817887604237
iteration 16, loss = 0.005310893524438143
iteration 17, loss = 0.005198207218199968
iteration 18, loss = 0.0050893924199044704
iteration 19, loss = 0.004929214715957642
iteration 20, loss = 0.005349146202206612
iteration 21, loss = 0.004830546677112579
iteration 22, loss = 0.0048218066804111
iteration 23, loss = 0.006646079011261463
iteration 24, loss = 0.006150051020085812
iteration 25, loss = 0.004818395711481571
iteration 26, loss = 0.004734411835670471
iteration 27, loss = 0.0049081421457231045
iteration 28, loss = 0.004796341992914677
iteration 29, loss = 0.005279654636979103
iteration 30, loss = 0.004912762902677059
iteration 31, loss = 0.004559129010885954
iteration 32, loss = 0.004939868580549955
iteration 33, loss = 0.00430843373760581
iteration 34, loss = 0.005613438785076141
iteration 35, loss = 0.004848197102546692
iteration 36, loss = 0.004618713166564703
iteration 37, loss = 0.0047644334845244884
iteration 38, loss = 0.0045148939825594425
iteration 39, loss = 0.005212434101849794
iteration 40, loss = 0.004857775755226612
iteration 41, loss = 0.0048984051682055
iteration 42, loss = 0.004588715732097626
iteration 43, loss = 0.007453537080436945
iteration 44, loss = 0.005442271940410137
iteration 45, loss = 0.0048574237152934074
iteration 46, loss = 0.006677681114524603
iteration 47, loss = 0.0077265179716050625
iteration 48, loss = 0.004858862608671188
iteration 49, loss = 0.004583780653774738
iteration 50, loss = 0.007998639717698097
iteration 51, loss = 0.00509884487837553
iteration 52, loss = 0.004743626341223717
iteration 53, loss = 0.005188466981053352
iteration 54, loss = 0.004529659636318684
iteration 55, loss = 0.004580020438879728
iteration 56, loss = 0.00468178978189826
iteration 57, loss = 0.00483032688498497
iteration 58, loss = 0.004801787436008453
iteration 59, loss = 0.004582760855555534
iteration 60, loss = 0.004793065134435892
iteration 61, loss = 0.005222888197749853
iteration 62, loss = 0.005992961581796408
iteration 63, loss = 0.004499765112996101
iteration 64, loss = 0.007795579731464386
iteration 65, loss = 0.004742748569697142
iteration 66, loss = 0.006962661165744066
iteration 67, loss = 0.0045906612649559975
iteration 68, loss = 0.005204211454838514
iteration 69, loss = 0.004628268536180258
iteration 70, loss = 0.0046461112797260284
iteration 71, loss = 0.004760493524372578
iteration 72, loss = 0.004784987308084965
iteration 73, loss = 0.004495579283684492
iteration 74, loss = 0.005399192217737436
iteration 75, loss = 0.005026576574891806
iteration 76, loss = 0.00821301992982626
iteration 77, loss = 0.006551804486662149
iteration 78, loss = 0.0059178476221859455
iteration 79, loss = 0.004721368197351694
iteration 80, loss = 0.00705010024830699
iteration 81, loss = 0.006012497004121542
iteration 82, loss = 0.005040676798671484
iteration 83, loss = 0.0052263629622757435
iteration 84, loss = 0.004678616765886545
iteration 85, loss = 0.004652899224311113
iteration 86, loss = 0.0045591783709824085
iteration 87, loss = 0.004276446998119354
iteration 88, loss = 0.0051061189733445644
iteration 89, loss = 0.007778023835271597
iteration 90, loss = 0.004565923009067774
iteration 91, loss = 0.00546161038801074
iteration 92, loss = 0.0047370451502501965
iteration 93, loss = 0.00502272043377161
iteration 94, loss = 0.006609736010432243
iteration 95, loss = 0.009218750521540642
iteration 96, loss = 0.006404777057468891
iteration 97, loss = 0.005003586411476135
iteration 98, loss = 0.004923117347061634
iteration 99, loss = 0.004796738736331463
iteration 100, loss = 0.004894747864454985
iteration 101, loss = 0.005427083931863308
iteration 102, loss = 0.0046316771768033504
iteration 103, loss = 0.009481660090386868
iteration 104, loss = 0.007594779133796692
iteration 105, loss = 0.005340837873518467
iteration 106, loss = 0.00793545600026846
iteration 107, loss = 0.004887199029326439
iteration 108, loss = 0.004411703906953335
iteration 109, loss = 0.004871001932770014
iteration 110, loss = 0.004608155693858862
iteration 111, loss = 0.005432779435068369
iteration 112, loss = 0.0046734269708395
iteration 113, loss = 0.005016936454921961
iteration 114, loss = 0.004721906501799822
iteration 115, loss = 0.004172337241470814
iteration 116, loss = 0.005720018409192562
iteration 117, loss = 0.006741050630807877
iteration 118, loss = 0.004967608489096165
iteration 119, loss = 0.0052572148852050304
iteration 120, loss = 0.004741665907204151
iteration 121, loss = 0.004968137945979834
iteration 122, loss = 0.004430993460118771
iteration 123, loss = 0.0069681983441114426
iteration 124, loss = 0.00712734367698431
iteration 125, loss = 0.00575248571112752
iteration 126, loss = 0.008155145682394505
iteration 127, loss = 0.005125659052282572
iteration 128, loss = 0.005453107878565788
iteration 129, loss = 0.007120389025658369
iteration 130, loss = 0.00607153819873929
iteration 131, loss = 0.0046366737224161625
iteration 132, loss = 0.005329265259206295
iteration 133, loss = 0.0042358567006886005
iteration 134, loss = 0.005001396406441927
iteration 135, loss = 0.008089476265013218
iteration 136, loss = 0.006560892332345247
iteration 137, loss = 0.0051183681935071945
iteration 138, loss = 0.005021238699555397
iteration 139, loss = 0.006581517867743969
iteration 140, loss = 0.006538023240864277
iteration 141, loss = 0.005645813886076212
iteration 142, loss = 0.004519923590123653
iteration 143, loss = 0.00482897087931633
iteration 144, loss = 0.00540408119559288
iteration 145, loss = 0.004481958691030741
iteration 146, loss = 0.0062561603263020515
iteration 147, loss = 0.004341428168118
iteration 148, loss = 0.006820774637162685
iteration 149, loss = 0.0068738944828510284
iteration 150, loss = 0.004525133408606052
iteration 151, loss = 0.007300117053091526
iteration 152, loss = 0.005016019102185965
iteration 153, loss = 0.00696416012942791
iteration 154, loss = 0.004658309277147055
iteration 155, loss = 0.006254236213862896
iteration 156, loss = 0.0045288545079529285
iteration 157, loss = 0.004765770863741636
iteration 158, loss = 0.004712335299700499
iteration 159, loss = 0.00517347501590848
iteration 160, loss = 0.004825389012694359
iteration 161, loss = 0.005704168230295181
iteration 162, loss = 0.006428380962461233
iteration 163, loss = 0.007500211242586374
iteration 164, loss = 0.004554884508252144
iteration 165, loss = 0.006501170806586742
iteration 166, loss = 0.004811228718608618
iteration 167, loss = 0.0046129729598760605
iteration 168, loss = 0.00496084988117218
iteration 169, loss = 0.004521915689110756
iteration 170, loss = 0.0058889626525342464
iteration 171, loss = 0.005228026304394007
iteration 172, loss = 0.004462200682610273
iteration 173, loss = 0.006271060556173325
iteration 174, loss = 0.004774828907102346
iteration 175, loss = 0.004922628868371248
iteration 176, loss = 0.0056235347874462605
iteration 177, loss = 0.005075073800981045
iteration 178, loss = 0.005610605701804161
iteration 179, loss = 0.005243557970970869
iteration 180, loss = 0.004689411260187626
iteration 181, loss = 0.007321340497583151
iteration 182, loss = 0.009218120947480202
iteration 183, loss = 0.004516300279647112
iteration 184, loss = 0.004817997105419636
iteration 185, loss = 0.004332599230110645
iteration 186, loss = 0.005019177682697773
iteration 187, loss = 0.0049148742109537125
iteration 188, loss = 0.004908577539026737
iteration 189, loss = 0.008119191974401474
iteration 190, loss = 0.004809330217540264
iteration 191, loss = 0.005054793320596218
iteration 192, loss = 0.004595862701535225
iteration 193, loss = 0.0054372092708945274
iteration 194, loss = 0.004579965956509113
iteration 195, loss = 0.004937881138175726
iteration 196, loss = 0.005342333111912012
iteration 197, loss = 0.004657849203795195
iteration 198, loss = 0.0049822647124528885
iteration 199, loss = 0.0072302548214793205
iteration 200, loss = 0.00787319429218769
iteration 201, loss = 0.005809362046420574
iteration 202, loss = 0.0058011566288769245
iteration 203, loss = 0.0045926677994430065
iteration 204, loss = 0.0046530114486813545
iteration 205, loss = 0.005066863726824522
iteration 206, loss = 0.004451649263501167
iteration 207, loss = 0.006059508770704269
iteration 208, loss = 0.004884815309196711
iteration 209, loss = 0.006686313543468714
iteration 210, loss = 0.004723409190773964
iteration 211, loss = 0.006872599013149738
iteration 212, loss = 0.004627268761396408
iteration 213, loss = 0.004839694127440453
iteration 214, loss = 0.00524089764803648
iteration 215, loss = 0.004431543871760368
iteration 216, loss = 0.006823606789112091
iteration 217, loss = 0.004521270282566547
iteration 218, loss = 0.004418149124830961
iteration 219, loss = 0.00482523487880826
iteration 220, loss = 0.004784254357218742
iteration 221, loss = 0.00553830387070775
iteration 222, loss = 0.004636985249817371
iteration 223, loss = 0.004815392661839724
iteration 224, loss = 0.004770686384290457
iteration 225, loss = 0.004688972141593695
iteration 226, loss = 0.005022602621465921
iteration 227, loss = 0.005333290435373783
iteration 228, loss = 0.006006602663546801
iteration 229, loss = 0.004260662943124771
iteration 230, loss = 0.0047566257417202
iteration 231, loss = 0.005302303470671177
iteration 232, loss = 0.004915784113109112
iteration 233, loss = 0.005038258619606495
iteration 234, loss = 0.006905050948262215
iteration 235, loss = 0.004224197939038277
iteration 236, loss = 0.006459377706050873
iteration 237, loss = 0.004349022172391415
iteration 238, loss = 0.005851294845342636
iteration 239, loss = 0.005253022536635399
iteration 240, loss = 0.004799075424671173
iteration 241, loss = 0.005174948833882809
iteration 242, loss = 0.005059174261987209
iteration 243, loss = 0.0048461128026247025
iteration 244, loss = 0.004955181386321783
iteration 245, loss = 0.004439349751919508
iteration 246, loss = 0.005052602384239435
iteration 247, loss = 0.006561078131198883
iteration 248, loss = 0.004482092335820198
iteration 249, loss = 0.005063369404524565
iteration 250, loss = 0.00574145233258605
iteration 251, loss = 0.00453948276117444
iteration 252, loss = 0.005334571003913879
iteration 253, loss = 0.005038552917540073
iteration 254, loss = 0.0067456187680363655
iteration 255, loss = 0.00455510476604104
iteration 256, loss = 0.0045868223533034325
iteration 257, loss = 0.005296907387673855
iteration 258, loss = 0.0059981876984238625
iteration 259, loss = 0.0047521693632006645
iteration 260, loss = 0.00498306704685092
iteration 261, loss = 0.006082022096961737
iteration 262, loss = 0.004695164505392313
iteration 263, loss = 0.0054596345871686935
iteration 264, loss = 0.004623934160917997
iteration 265, loss = 0.006163932383060455
iteration 266, loss = 0.004341484978795052
iteration 267, loss = 0.004804725293070078
iteration 268, loss = 0.004727921448647976
iteration 269, loss = 0.004750916734337807
iteration 270, loss = 0.005463446490466595
iteration 271, loss = 0.006532702129334211
iteration 272, loss = 0.004734222311526537
iteration 273, loss = 0.00558018172159791
iteration 274, loss = 0.004438148811459541
iteration 275, loss = 0.005527456756681204
iteration 276, loss = 0.005152154713869095
iteration 277, loss = 0.004599934909492731
iteration 278, loss = 0.005457705818116665
iteration 279, loss = 0.006502897012978792
iteration 280, loss = 0.004168733023107052
iteration 281, loss = 0.005221258383244276
iteration 282, loss = 0.005864948965609074
iteration 283, loss = 0.004859992302954197
iteration 284, loss = 0.0047083329409360886
iteration 285, loss = 0.0045215715654194355
iteration 286, loss = 0.006379418540745974
iteration 287, loss = 0.005631756037473679
iteration 288, loss = 0.004652018658816814
iteration 289, loss = 0.007022414822131395
iteration 290, loss = 0.004605370108038187
iteration 291, loss = 0.005063336808234453
iteration 292, loss = 0.004596551414579153
iteration 293, loss = 0.004705481231212616
iteration 294, loss = 0.00825576949864626
iteration 295, loss = 0.0050568548031151295
iteration 296, loss = 0.004655755124986172
iteration 297, loss = 0.005500440951436758
iteration 298, loss = 0.004635601304471493
iteration 299, loss = 0.004945838358253241
iteration 300, loss = 0.0052492073737084866
iteration 1, loss = 0.00531422346830368
iteration 2, loss = 0.006060382351279259
iteration 3, loss = 0.008358919993042946
iteration 4, loss = 0.00477647315710783
iteration 5, loss = 0.004405118990689516
iteration 6, loss = 0.004741733893752098
iteration 7, loss = 0.00506910216063261
iteration 8, loss = 0.006604467518627644
iteration 9, loss = 0.006960361730307341
iteration 10, loss = 0.006599697284400463
iteration 11, loss = 0.0064606438390910625
iteration 12, loss = 0.004585855174809694
iteration 13, loss = 0.005105879157781601
iteration 14, loss = 0.004415783099830151
iteration 15, loss = 0.0048919799737632275
iteration 16, loss = 0.004485160578042269
iteration 17, loss = 0.0050042965449392796
iteration 18, loss = 0.007800906430929899
iteration 19, loss = 0.005244402214884758
iteration 20, loss = 0.004981675650924444
iteration 21, loss = 0.005375353619456291
iteration 22, loss = 0.004407722968608141
iteration 23, loss = 0.004978254437446594
iteration 24, loss = 0.006884525064378977
iteration 25, loss = 0.004537373315542936
iteration 26, loss = 0.005161987617611885
iteration 27, loss = 0.0048409029841423035
iteration 28, loss = 0.006092569790780544
iteration 29, loss = 0.004738318268209696
iteration 30, loss = 0.005143112037330866
iteration 31, loss = 0.005348523613065481
iteration 32, loss = 0.004565285984426737
iteration 33, loss = 0.008014069870114326
iteration 34, loss = 0.006220398470759392
iteration 35, loss = 0.004459038842469454
iteration 36, loss = 0.0054817888885736465
iteration 37, loss = 0.004714406561106443
iteration 38, loss = 0.004551751539111137
iteration 39, loss = 0.007157731801271439
iteration 40, loss = 0.005942625924944878
iteration 41, loss = 0.005885521415621042
iteration 42, loss = 0.004880755208432674
iteration 43, loss = 0.004702321253716946
iteration 44, loss = 0.004507811274379492
iteration 45, loss = 0.0044035459868609905
iteration 46, loss = 0.005243077874183655
iteration 47, loss = 0.005360051523894072
iteration 48, loss = 0.0053391847759485245
iteration 49, loss = 0.004319733940064907
iteration 50, loss = 0.004563130903989077
iteration 51, loss = 0.008411315269768238
iteration 52, loss = 0.007260398473590612
iteration 53, loss = 0.004473515320569277
iteration 54, loss = 0.0041992757469415665
iteration 55, loss = 0.0047615510411560535
iteration 56, loss = 0.004826902877539396
iteration 57, loss = 0.004695110488682985
iteration 58, loss = 0.004542212001979351
iteration 59, loss = 0.006782690063118935
iteration 60, loss = 0.004876934923231602
iteration 61, loss = 0.004793365485966206
iteration 62, loss = 0.004657062701880932
iteration 63, loss = 0.004982317332178354
iteration 64, loss = 0.007091905456036329
iteration 65, loss = 0.004581563640385866
iteration 66, loss = 0.004750004969537258
iteration 67, loss = 0.005583742633461952
iteration 68, loss = 0.004407047294080257
iteration 69, loss = 0.00575541565194726
iteration 70, loss = 0.004866439383476973
iteration 71, loss = 0.006792549509555101
iteration 72, loss = 0.0059791309759020805
iteration 73, loss = 0.004681295715272427
iteration 74, loss = 0.004837918095290661
iteration 75, loss = 0.005501636303961277
iteration 76, loss = 0.005116293206810951
iteration 77, loss = 0.005149486940354109
iteration 78, loss = 0.004975279793143272
iteration 79, loss = 0.00720707094296813
iteration 80, loss = 0.005044628866016865
iteration 81, loss = 0.0044725933112204075
iteration 82, loss = 0.004395475145429373
iteration 83, loss = 0.004972659982740879
iteration 84, loss = 0.004943284671753645
iteration 85, loss = 0.0048438082449138165
iteration 86, loss = 0.004749583080410957
iteration 87, loss = 0.00536838173866272
iteration 88, loss = 0.004724219441413879
iteration 89, loss = 0.0067481244914233685
iteration 90, loss = 0.00468897121027112
iteration 91, loss = 0.007222924847155809
iteration 92, loss = 0.004556393250823021
iteration 93, loss = 0.004494793247431517
iteration 94, loss = 0.005067815538495779
iteration 95, loss = 0.004324995446950197
iteration 96, loss = 0.005660322494804859
iteration 97, loss = 0.00437560398131609
iteration 98, loss = 0.006040547043085098
iteration 99, loss = 0.005264719482511282
iteration 100, loss = 0.005014772526919842
iteration 101, loss = 0.005169210489839315
iteration 102, loss = 0.004798096604645252
iteration 103, loss = 0.005053248256444931
iteration 104, loss = 0.005329411942511797
iteration 105, loss = 0.0059058284386992455
iteration 106, loss = 0.004579220432788134
iteration 107, loss = 0.004217611625790596
iteration 108, loss = 0.007952040061354637
iteration 109, loss = 0.004545563831925392
iteration 110, loss = 0.0043930127285420895
iteration 111, loss = 0.006046556867659092
iteration 112, loss = 0.0054216100834310055
iteration 113, loss = 0.004642542917281389
iteration 114, loss = 0.008453966118395329
iteration 115, loss = 0.005130272824317217
iteration 116, loss = 0.004468971863389015
iteration 117, loss = 0.004663452506065369
iteration 118, loss = 0.006038170773535967
iteration 119, loss = 0.0046478454023599625
iteration 120, loss = 0.005162361077964306
iteration 121, loss = 0.004465329460799694
iteration 122, loss = 0.005104907788336277
iteration 123, loss = 0.004927543457597494
iteration 124, loss = 0.004669881891459227
iteration 125, loss = 0.006648960988968611
iteration 126, loss = 0.005299337673932314
iteration 127, loss = 0.005265769083052874
iteration 128, loss = 0.004768785089254379
iteration 129, loss = 0.005300259217619896
iteration 130, loss = 0.0070180268958210945
iteration 131, loss = 0.004549950361251831
iteration 132, loss = 0.004783554468303919
iteration 133, loss = 0.0061713396571576595
iteration 134, loss = 0.004396722186356783
iteration 135, loss = 0.005113494582474232
iteration 136, loss = 0.004618135746568441
iteration 137, loss = 0.0047222948633134365
iteration 138, loss = 0.004511740058660507
iteration 139, loss = 0.005390960723161697
iteration 140, loss = 0.0048949941992759705
iteration 141, loss = 0.004515547771006823
iteration 142, loss = 0.006496728397905827
iteration 143, loss = 0.004549900535494089
iteration 144, loss = 0.005196425598114729
iteration 145, loss = 0.0046505494974553585
iteration 146, loss = 0.0043678064830601215
iteration 147, loss = 0.004684999585151672
iteration 148, loss = 0.005290808621793985
iteration 149, loss = 0.0045103891752660275
iteration 150, loss = 0.004558118991553783
iteration 151, loss = 0.004781836643815041
iteration 152, loss = 0.004911184310913086
iteration 153, loss = 0.004854472819715738
iteration 154, loss = 0.005396276246756315
iteration 155, loss = 0.004802363459020853
iteration 156, loss = 0.004826962482184172
iteration 157, loss = 0.004659746773540974
iteration 158, loss = 0.004886246751993895
iteration 159, loss = 0.004430037457495928
iteration 160, loss = 0.004422068130224943
iteration 161, loss = 0.00456374604254961
iteration 162, loss = 0.008392038755118847
iteration 163, loss = 0.005493666045367718
iteration 164, loss = 0.005100450478494167
iteration 165, loss = 0.005907872691750526
iteration 166, loss = 0.004624847788363695
iteration 167, loss = 0.004255057778209448
iteration 168, loss = 0.004358027130365372
iteration 169, loss = 0.004583934787660837
iteration 170, loss = 0.004809100646525621
iteration 171, loss = 0.004568730015307665
iteration 172, loss = 0.004819459281861782
iteration 173, loss = 0.00483191991224885
iteration 174, loss = 0.004684424959123135
iteration 175, loss = 0.00512454379349947
iteration 176, loss = 0.007825229316949844
iteration 177, loss = 0.004514831118285656
iteration 178, loss = 0.004503953270614147
iteration 179, loss = 0.004655581898987293
iteration 180, loss = 0.005164093337953091
iteration 181, loss = 0.004843482282012701
iteration 182, loss = 0.004795457702130079
iteration 183, loss = 0.004175594542175531
iteration 184, loss = 0.0046487790532410145
iteration 185, loss = 0.005166590213775635
iteration 186, loss = 0.004931027069687843
iteration 187, loss = 0.004747678525745869
iteration 188, loss = 0.0044856444001197815
iteration 189, loss = 0.004539733752608299
iteration 190, loss = 0.00677930423989892
iteration 191, loss = 0.004658251069486141
iteration 192, loss = 0.005067674443125725
iteration 193, loss = 0.0040392144583165646
iteration 194, loss = 0.005156402941793203
iteration 195, loss = 0.006169519852846861
iteration 196, loss = 0.004626157693564892
iteration 197, loss = 0.004588005132973194
iteration 198, loss = 0.004618948325514793
iteration 199, loss = 0.0047554923221468925
iteration 200, loss = 0.004934781696647406
iteration 201, loss = 0.0075476206839084625
iteration 202, loss = 0.004812971223145723
iteration 203, loss = 0.004638599697500467
iteration 204, loss = 0.009407607838511467
iteration 205, loss = 0.005822597071528435
iteration 206, loss = 0.0042179664596915245
iteration 207, loss = 0.004527709446847439
iteration 208, loss = 0.005386984907090664
iteration 209, loss = 0.004652449861168861
iteration 210, loss = 0.008559766225516796
iteration 211, loss = 0.004693684168159962
iteration 212, loss = 0.004596508573740721
iteration 213, loss = 0.004965596366673708
iteration 214, loss = 0.004603816661983728
iteration 215, loss = 0.006404783111065626
iteration 216, loss = 0.004287877585738897
iteration 217, loss = 0.004952444229274988
iteration 218, loss = 0.004642794374376535
iteration 219, loss = 0.005940579809248447
iteration 220, loss = 0.006683395244181156
iteration 221, loss = 0.005435318220406771
iteration 222, loss = 0.004785493016242981
iteration 223, loss = 0.004669501446187496
iteration 224, loss = 0.006947832182049751
iteration 225, loss = 0.005806536879390478
iteration 226, loss = 0.004560266621410847
iteration 227, loss = 0.006427962798625231
iteration 228, loss = 0.004907827824354172
iteration 229, loss = 0.005002587102353573
iteration 230, loss = 0.004890579730272293
iteration 231, loss = 0.00533526623621583
iteration 232, loss = 0.004896905273199081
iteration 233, loss = 0.004691337700933218
iteration 234, loss = 0.004623825196176767
iteration 235, loss = 0.005444762296974659
iteration 236, loss = 0.00444985693320632
iteration 237, loss = 0.005305883474647999
iteration 238, loss = 0.004425149876624346
iteration 239, loss = 0.004808848258107901
iteration 240, loss = 0.007488585077226162
iteration 241, loss = 0.008142748847603798
iteration 242, loss = 0.004791821353137493
iteration 243, loss = 0.004563570488244295
iteration 244, loss = 0.00465747993439436
iteration 245, loss = 0.0045358724892139435
iteration 246, loss = 0.004697597585618496
iteration 247, loss = 0.004374893382191658
iteration 248, loss = 0.004702236503362656
iteration 249, loss = 0.004866346716880798
iteration 250, loss = 0.005276038311421871
iteration 251, loss = 0.005131481681019068
iteration 252, loss = 0.0047963266260921955
iteration 253, loss = 0.006341463420540094
iteration 254, loss = 0.004359658341854811
iteration 255, loss = 0.004517725668847561
iteration 256, loss = 0.004329199902713299
iteration 257, loss = 0.006996166426688433
iteration 258, loss = 0.005678230430930853
iteration 259, loss = 0.005738571751862764
iteration 260, loss = 0.0045026387088000774
iteration 261, loss = 0.004807261284440756
iteration 262, loss = 0.007189570926129818
iteration 263, loss = 0.00446194875985384
iteration 264, loss = 0.004397165961563587
iteration 265, loss = 0.004476041998714209
iteration 266, loss = 0.0038654417730867863
iteration 267, loss = 0.008177339099347591
iteration 268, loss = 0.008119368925690651
iteration 269, loss = 0.004340946674346924
iteration 270, loss = 0.00467686215415597
iteration 271, loss = 0.0044608949683606625
iteration 272, loss = 0.004657251760363579
iteration 273, loss = 0.004521686118096113
iteration 274, loss = 0.004376439843326807
iteration 275, loss = 0.006300272420048714
iteration 276, loss = 0.007419353350996971
iteration 277, loss = 0.004801919683814049
iteration 278, loss = 0.004259201698005199
iteration 279, loss = 0.005141688510775566
iteration 280, loss = 0.007437166757881641
iteration 281, loss = 0.006316804792732
iteration 282, loss = 0.006075558718293905
iteration 283, loss = 0.004881254397332668
iteration 284, loss = 0.005491476505994797
iteration 285, loss = 0.006192805711179972
iteration 286, loss = 0.005392934661358595
iteration 287, loss = 0.009042157791554928
iteration 288, loss = 0.004626810550689697
iteration 289, loss = 0.005028559826314449
iteration 290, loss = 0.004785986617207527
iteration 291, loss = 0.0043695466592907906
iteration 292, loss = 0.005155722610652447
iteration 293, loss = 0.00616250978782773
iteration 294, loss = 0.006740333512425423
iteration 295, loss = 0.0046548908576369286
iteration 296, loss = 0.005986363161355257
iteration 297, loss = 0.008032410405576229
iteration 298, loss = 0.0049027190543711185
iteration 299, loss = 0.006369887851178646
iteration 300, loss = 0.0044534578919410706
iteration 1, loss = 0.005692312493920326
iteration 2, loss = 0.004674095194786787
iteration 3, loss = 0.004895911552011967
iteration 4, loss = 0.005768623203039169
iteration 5, loss = 0.004770286381244659
iteration 6, loss = 0.004954761825501919
iteration 7, loss = 0.006668418180197477
iteration 8, loss = 0.004495468456298113
iteration 9, loss = 0.00468753557652235
iteration 10, loss = 0.005180754233151674
iteration 11, loss = 0.004770565778017044
iteration 12, loss = 0.0076628862880170345
iteration 13, loss = 0.004559951834380627
iteration 14, loss = 0.0061968169175088406
iteration 15, loss = 0.0045332093723118305
iteration 16, loss = 0.005135956220328808
iteration 17, loss = 0.0045119295828044415
iteration 18, loss = 0.008242168463766575
iteration 19, loss = 0.004792754538357258
iteration 20, loss = 0.004875279031693935
iteration 21, loss = 0.004314843565225601
iteration 22, loss = 0.004994602873921394
iteration 23, loss = 0.007352469954639673
iteration 24, loss = 0.004208675120025873
iteration 25, loss = 0.004703911021351814
iteration 26, loss = 0.004594424273818731
iteration 27, loss = 0.006065009161829948
iteration 28, loss = 0.005308421328663826
iteration 29, loss = 0.004657284822314978
iteration 30, loss = 0.007766562048345804
iteration 31, loss = 0.0049277120269834995
iteration 32, loss = 0.004960082937031984
iteration 33, loss = 0.00496659753844142
iteration 34, loss = 0.004797243047505617
iteration 35, loss = 0.004388808738440275
iteration 36, loss = 0.004676545038819313
iteration 37, loss = 0.00440828176215291
iteration 38, loss = 0.005779195111244917
iteration 39, loss = 0.00477435952052474
iteration 40, loss = 0.004615709185600281
iteration 41, loss = 0.004668414127081633
iteration 42, loss = 0.005359121598303318
iteration 43, loss = 0.004394059535115957
iteration 44, loss = 0.004620615858584642
iteration 45, loss = 0.0042628631927073
iteration 46, loss = 0.006531592924147844
iteration 47, loss = 0.00532513577491045
iteration 48, loss = 0.004431409761309624
iteration 49, loss = 0.008554676547646523
iteration 50, loss = 0.004238284658640623
iteration 51, loss = 0.005106742028146982
iteration 52, loss = 0.004565853159874678
iteration 53, loss = 0.004807943012565374
iteration 54, loss = 0.004948284011334181
iteration 55, loss = 0.004229467827826738
iteration 56, loss = 0.00474135484546423
iteration 57, loss = 0.004907508380711079
iteration 58, loss = 0.004415861796587706
iteration 59, loss = 0.004505547229200602
iteration 60, loss = 0.004706169944256544
iteration 61, loss = 0.0063387672416865826
iteration 62, loss = 0.004286493174731731
iteration 63, loss = 0.006210066378116608
iteration 64, loss = 0.006690502632409334
iteration 65, loss = 0.005107692908495665
iteration 66, loss = 0.0046518901363015175
iteration 67, loss = 0.004775748588144779
iteration 68, loss = 0.0045479885302484035
iteration 69, loss = 0.006336929276585579
iteration 70, loss = 0.004499111790210009
iteration 71, loss = 0.0052755908109247684
iteration 72, loss = 0.006322987377643585
iteration 73, loss = 0.0053613996133208275
iteration 74, loss = 0.0042652711272239685
iteration 75, loss = 0.005982978735119104
iteration 76, loss = 0.004541650880128145
iteration 77, loss = 0.005701759830117226
iteration 78, loss = 0.004724778700619936
iteration 79, loss = 0.0057426756247878075
iteration 80, loss = 0.005207168869674206
iteration 81, loss = 0.00450601102784276
iteration 82, loss = 0.004460046999156475
iteration 83, loss = 0.007581194397062063
iteration 84, loss = 0.00455606821924448
iteration 85, loss = 0.004522426053881645
iteration 86, loss = 0.0049430979415774345
iteration 87, loss = 0.007806703913956881
iteration 88, loss = 0.006460906006395817
iteration 89, loss = 0.004730526357889175
iteration 90, loss = 0.004928553011268377
iteration 91, loss = 0.004286269191652536
iteration 92, loss = 0.0050122616812586784
iteration 93, loss = 0.005122778937220573
iteration 94, loss = 0.0046083880588412285
iteration 95, loss = 0.004398374818265438
iteration 96, loss = 0.00536758778616786
iteration 97, loss = 0.004553354345262051
iteration 98, loss = 0.004280488938093185
iteration 99, loss = 0.004487415309995413
iteration 100, loss = 0.0049385204911231995
iteration 101, loss = 0.005960473325103521
iteration 102, loss = 0.004671481437981129
iteration 103, loss = 0.006679321173578501
iteration 104, loss = 0.004312657751142979
iteration 105, loss = 0.004387508612126112
iteration 106, loss = 0.004907594993710518
iteration 107, loss = 0.005528626032173634
iteration 108, loss = 0.0049207815900444984
iteration 109, loss = 0.004427724983543158
iteration 110, loss = 0.005029730498790741
iteration 111, loss = 0.004623579327017069
iteration 112, loss = 0.00816347450017929
iteration 113, loss = 0.005099662579596043
iteration 114, loss = 0.0046798912808299065
iteration 115, loss = 0.004340155050158501
iteration 116, loss = 0.004302144981920719
iteration 117, loss = 0.005948895122855902
iteration 118, loss = 0.004784007091075182
iteration 119, loss = 0.004860725719481707
iteration 120, loss = 0.006085388362407684
iteration 121, loss = 0.004255430772900581
iteration 122, loss = 0.00459960987791419
iteration 123, loss = 0.004566023126244545
iteration 124, loss = 0.004160877782851458
iteration 125, loss = 0.004352011252194643
iteration 126, loss = 0.004127351567149162
iteration 127, loss = 0.004630202427506447
iteration 128, loss = 0.0066180964931845665
iteration 129, loss = 0.0047880555503070354
iteration 130, loss = 0.006307044066488743
iteration 131, loss = 0.004601401276886463
iteration 132, loss = 0.004848880227655172
iteration 133, loss = 0.00462584663182497
iteration 134, loss = 0.005014028865844011
iteration 135, loss = 0.0045485845766961575
iteration 136, loss = 0.0055414484813809395
iteration 137, loss = 0.004362298641353846
iteration 138, loss = 0.006701446603983641
iteration 139, loss = 0.005571084562689066
iteration 140, loss = 0.00480178277939558
iteration 141, loss = 0.004852876998484135
iteration 142, loss = 0.006507162936031818
iteration 143, loss = 0.006627121940255165
iteration 144, loss = 0.004814006853848696
iteration 145, loss = 0.004261889960616827
iteration 146, loss = 0.004583528731018305
iteration 147, loss = 0.004771308042109013
iteration 148, loss = 0.004846802446991205
iteration 149, loss = 0.00585629278793931
iteration 150, loss = 0.004562631249427795
iteration 151, loss = 0.004778306931257248
iteration 152, loss = 0.004536016378551722
iteration 153, loss = 0.005795633886009455
iteration 154, loss = 0.005033561494201422
iteration 155, loss = 0.004980843979865313
iteration 156, loss = 0.004725729115307331
iteration 157, loss = 0.004676386713981628
iteration 158, loss = 0.008294961415231228
iteration 159, loss = 0.004889545496553183
iteration 160, loss = 0.004956845659762621
iteration 161, loss = 0.004362091887742281
iteration 162, loss = 0.007294064853340387
iteration 163, loss = 0.0045144520699977875
iteration 164, loss = 0.0045906249433755875
iteration 165, loss = 0.004891352728009224
iteration 166, loss = 0.005949816200882196
iteration 167, loss = 0.0051679834723472595
iteration 168, loss = 0.004516668617725372
iteration 169, loss = 0.006761020980775356
iteration 170, loss = 0.004623569082468748
iteration 171, loss = 0.004370278213173151
iteration 172, loss = 0.006570301949977875
iteration 173, loss = 0.005625338293612003
iteration 174, loss = 0.004441052209585905
iteration 175, loss = 0.0061578634195029736
iteration 176, loss = 0.005068778991699219
iteration 177, loss = 0.004962526261806488
iteration 178, loss = 0.004641491919755936
iteration 179, loss = 0.004648115020245314
iteration 180, loss = 0.00475694052875042
iteration 181, loss = 0.005843915976583958
iteration 182, loss = 0.00783480703830719
iteration 183, loss = 0.0050275567919015884
iteration 184, loss = 0.00419983547180891
iteration 185, loss = 0.0046669491566717625
iteration 186, loss = 0.004588333889842033
iteration 187, loss = 0.004563282243907452
iteration 188, loss = 0.008429976180195808
iteration 189, loss = 0.004949772730469704
iteration 190, loss = 0.00706930086016655
iteration 191, loss = 0.00452912412583828
iteration 192, loss = 0.005105930380523205
iteration 193, loss = 0.004760296083986759
iteration 194, loss = 0.004895398393273354
iteration 195, loss = 0.005007143598049879
iteration 196, loss = 0.0046794218942523
iteration 197, loss = 0.0050205569714307785
iteration 198, loss = 0.004826822318136692
iteration 199, loss = 0.004397226497530937
iteration 200, loss = 0.005131324287503958
iteration 201, loss = 0.004619873594492674
iteration 202, loss = 0.004724346566945314
iteration 203, loss = 0.004888370167464018
iteration 204, loss = 0.004323363769799471
iteration 205, loss = 0.004963143263012171
iteration 206, loss = 0.005145881325006485
iteration 207, loss = 0.004487783648073673
iteration 208, loss = 0.0045580780133605
iteration 209, loss = 0.005874487571418285
iteration 210, loss = 0.00529820378869772
iteration 211, loss = 0.004350321367383003
iteration 212, loss = 0.005151703022420406
iteration 213, loss = 0.005260511301457882
iteration 214, loss = 0.006546718999743462
iteration 215, loss = 0.004975293297320604
iteration 216, loss = 0.0044983914121985435
iteration 217, loss = 0.00510528776794672
iteration 218, loss = 0.005808794405311346
iteration 219, loss = 0.00458908686414361
iteration 220, loss = 0.004415477160364389
iteration 221, loss = 0.005116814747452736
iteration 222, loss = 0.007326990831643343
iteration 223, loss = 0.004885098431259394
iteration 224, loss = 0.004274399951100349
iteration 225, loss = 0.005435656290501356
iteration 226, loss = 0.004955390468239784
iteration 227, loss = 0.007808853406459093
iteration 228, loss = 0.004691825248301029
iteration 229, loss = 0.00447903061285615
iteration 230, loss = 0.00815035030245781
iteration 231, loss = 0.006413772702217102
iteration 232, loss = 0.004728918429464102
iteration 233, loss = 0.004557189531624317
iteration 234, loss = 0.004869676660746336
iteration 235, loss = 0.004299954045563936
iteration 236, loss = 0.004199719522148371
iteration 237, loss = 0.005537639372050762
iteration 238, loss = 0.007473938167095184
iteration 239, loss = 0.005313520785421133
iteration 240, loss = 0.007049071602523327
iteration 241, loss = 0.004734303802251816
iteration 242, loss = 0.006927185226231813
iteration 243, loss = 0.004419070668518543
iteration 244, loss = 0.006313352379947901
iteration 245, loss = 0.004482114687561989
iteration 246, loss = 0.0044100238010287285
iteration 247, loss = 0.004964191000908613
iteration 248, loss = 0.0074445041827857494
iteration 249, loss = 0.006820318289101124
iteration 250, loss = 0.004462024196982384
iteration 251, loss = 0.004333564080297947
iteration 252, loss = 0.004898283164948225
iteration 253, loss = 0.004179846495389938
iteration 254, loss = 0.0061523644253611565
iteration 255, loss = 0.004602558445185423
iteration 256, loss = 0.00607979716733098
iteration 257, loss = 0.004334889352321625
iteration 258, loss = 0.004785679280757904
iteration 259, loss = 0.007538292091339827
iteration 260, loss = 0.004750929772853851
iteration 261, loss = 0.0044920966029167175
iteration 262, loss = 0.004601305350661278
iteration 263, loss = 0.004282577894628048
iteration 264, loss = 0.004732043016701937
iteration 265, loss = 0.004314140882343054
iteration 266, loss = 0.004757218528538942
iteration 267, loss = 0.004510553553700447
iteration 268, loss = 0.006067412905395031
iteration 269, loss = 0.004574771039187908
iteration 270, loss = 0.004535433370620012
iteration 271, loss = 0.00464249774813652
iteration 272, loss = 0.006562665104866028
iteration 273, loss = 0.004598863888531923
iteration 274, loss = 0.004934581462293863
iteration 275, loss = 0.0047697932459414005
iteration 276, loss = 0.004374231211841106
iteration 277, loss = 0.005618637427687645
iteration 278, loss = 0.006151419132947922
iteration 279, loss = 0.004746173974126577
iteration 280, loss = 0.004557304549962282
iteration 281, loss = 0.005153044126927853
iteration 282, loss = 0.007637652102857828
iteration 283, loss = 0.005775464698672295
iteration 284, loss = 0.00637566763907671
iteration 285, loss = 0.004361642524600029
iteration 286, loss = 0.006900476291775703
iteration 287, loss = 0.004731498658657074
iteration 288, loss = 0.005181081593036652
iteration 289, loss = 0.005506954621523619
iteration 290, loss = 0.004598207306116819
iteration 291, loss = 0.006475821137428284
iteration 292, loss = 0.004624673631042242
iteration 293, loss = 0.004979555029422045
iteration 294, loss = 0.004355655517429113
iteration 295, loss = 0.0061447592452168465
iteration 296, loss = 0.004377650562673807
iteration 297, loss = 0.004989353008568287
iteration 298, loss = 0.004545859061181545
iteration 299, loss = 0.004425341729074717
iteration 300, loss = 0.004532022401690483
iteration 1, loss = 0.004810651298612356
iteration 2, loss = 0.004842424299567938
iteration 3, loss = 0.0045793731696903706
iteration 4, loss = 0.004387303721159697
iteration 5, loss = 0.004444880876690149
iteration 6, loss = 0.0059400019235908985
iteration 7, loss = 0.0045733326114714146
iteration 8, loss = 0.004485585261136293
iteration 9, loss = 0.005098660476505756
iteration 10, loss = 0.0047648074105381966
iteration 11, loss = 0.004498341586440802
iteration 12, loss = 0.005591931752860546
iteration 13, loss = 0.00858992524445057
iteration 14, loss = 0.006387097295373678
iteration 15, loss = 0.00470541650429368
iteration 16, loss = 0.0044118184596300125
iteration 17, loss = 0.004485801327973604
iteration 18, loss = 0.00476522371172905
iteration 19, loss = 0.00454328628256917
iteration 20, loss = 0.004188441205769777
iteration 21, loss = 0.0056371260434389114
iteration 22, loss = 0.005926555022597313
iteration 23, loss = 0.004346763715147972
iteration 24, loss = 0.004304417874664068
iteration 25, loss = 0.004226816352456808
iteration 26, loss = 0.006162747275084257
iteration 27, loss = 0.004842768423259258
iteration 28, loss = 0.004310670308768749
iteration 29, loss = 0.004378183279186487
iteration 30, loss = 0.0045102862641215324
iteration 31, loss = 0.004609548021107912
iteration 32, loss = 0.004405072424560785
iteration 33, loss = 0.00816989317536354
iteration 34, loss = 0.004442758858203888
iteration 35, loss = 0.005458036437630653
iteration 36, loss = 0.004374374635517597
iteration 37, loss = 0.004565011244267225
iteration 38, loss = 0.00520824920386076
iteration 39, loss = 0.005080800969153643
iteration 40, loss = 0.007466443814337254
iteration 41, loss = 0.004959009122103453
iteration 42, loss = 0.005162844434380531
iteration 43, loss = 0.004456560593098402
iteration 44, loss = 0.004209356382489204
iteration 45, loss = 0.0049833012744784355
iteration 46, loss = 0.005349142476916313
iteration 47, loss = 0.006104723550379276
iteration 48, loss = 0.0055853137746453285
iteration 49, loss = 0.004662391729652882
iteration 50, loss = 0.004955075215548277
iteration 51, loss = 0.004755897913128138
iteration 52, loss = 0.00476960139349103
iteration 53, loss = 0.004502519033849239
iteration 54, loss = 0.0044213272631168365
iteration 55, loss = 0.004489672835916281
iteration 56, loss = 0.004788791295140982
iteration 57, loss = 0.007606600411236286
iteration 58, loss = 0.005820824299007654
iteration 59, loss = 0.0047702970914542675
iteration 60, loss = 0.004538301378488541
iteration 61, loss = 0.004606176633387804
iteration 62, loss = 0.004473744425922632
iteration 63, loss = 0.006454702466726303
iteration 64, loss = 0.004644794389605522
iteration 65, loss = 0.0049320668913424015
iteration 66, loss = 0.004989670589566231
iteration 67, loss = 0.004352369345724583
iteration 68, loss = 0.00502720195800066
iteration 69, loss = 0.004776639398187399
iteration 70, loss = 0.006163861136883497
iteration 71, loss = 0.004500638227909803
iteration 72, loss = 0.0046544745564460754
iteration 73, loss = 0.006280219182372093
iteration 74, loss = 0.0047529046423733234
iteration 75, loss = 0.005054703913629055
iteration 76, loss = 0.005072708707302809
iteration 77, loss = 0.004761397838592529
iteration 78, loss = 0.004512999672442675
iteration 79, loss = 0.006496105808764696
iteration 80, loss = 0.0049078864976763725
iteration 81, loss = 0.005004141945391893
iteration 82, loss = 0.004699829965829849
iteration 83, loss = 0.00446379603818059
iteration 84, loss = 0.004226028919219971
iteration 85, loss = 0.0042969705536961555
iteration 86, loss = 0.00452532060444355
iteration 87, loss = 0.004227650351822376
iteration 88, loss = 0.0046359882690012455
iteration 89, loss = 0.004964001476764679
iteration 90, loss = 0.0043668244034051895
iteration 91, loss = 0.005526668857783079
iteration 92, loss = 0.0040898872539401054
iteration 93, loss = 0.004816041793674231
iteration 94, loss = 0.004435460548847914
iteration 95, loss = 0.00462691206485033
iteration 96, loss = 0.004594787955284119
iteration 97, loss = 0.005843286868184805
iteration 98, loss = 0.004877687431871891
iteration 99, loss = 0.0063817319460213184
iteration 100, loss = 0.005916965659707785
iteration 101, loss = 0.006664815358817577
iteration 102, loss = 0.004929251968860626
iteration 103, loss = 0.007706142961978912
iteration 104, loss = 0.004400765523314476
iteration 105, loss = 0.005102564115077257
iteration 106, loss = 0.0042035141959786415
iteration 107, loss = 0.00422613276168704
iteration 108, loss = 0.004428217653185129
iteration 109, loss = 0.005062820389866829
iteration 110, loss = 0.0050356327556073666
iteration 111, loss = 0.004819134715944529
iteration 112, loss = 0.00475132092833519
iteration 113, loss = 0.004311202093958855
iteration 114, loss = 0.004728838335722685
iteration 115, loss = 0.004227002616971731
iteration 116, loss = 0.0039007726591080427
iteration 117, loss = 0.007714869454503059
iteration 118, loss = 0.00604612659662962
iteration 119, loss = 0.004166827071458101
iteration 120, loss = 0.004124453756958246
iteration 121, loss = 0.0041309683583676815
iteration 122, loss = 0.004682416561990976
iteration 123, loss = 0.004341536667197943
iteration 124, loss = 0.0046243430115282536
iteration 125, loss = 0.004365285858511925
iteration 126, loss = 0.005616369191557169
iteration 127, loss = 0.0053525823168456554
iteration 128, loss = 0.004003587178885937
iteration 129, loss = 0.004458956886082888
iteration 130, loss = 0.004684413317590952
iteration 131, loss = 0.005314691923558712
iteration 132, loss = 0.005877631716430187
iteration 133, loss = 0.003840296296402812
iteration 134, loss = 0.004475264344364405
iteration 135, loss = 0.005637922789901495
iteration 136, loss = 0.004670826252549887
iteration 137, loss = 0.005355885252356529
iteration 138, loss = 0.00505088921636343
iteration 139, loss = 0.005372611805796623
iteration 140, loss = 0.004694837611168623
iteration 141, loss = 0.004303034860640764
iteration 142, loss = 0.004838627763092518
iteration 143, loss = 0.006951815448701382
iteration 144, loss = 0.007720217574387789
iteration 145, loss = 0.00789456907659769
iteration 146, loss = 0.004776876885443926
iteration 147, loss = 0.004489844664931297
iteration 148, loss = 0.005547037348151207
iteration 149, loss = 0.004395947325974703
iteration 150, loss = 0.004090552683919668
iteration 151, loss = 0.006430197041481733
iteration 152, loss = 0.005516528617590666
iteration 153, loss = 0.004348027054220438
iteration 154, loss = 0.005010752473026514
iteration 155, loss = 0.004731664899736643
iteration 156, loss = 0.004764790181070566
iteration 157, loss = 0.004163085948675871
iteration 158, loss = 0.004766260739415884
iteration 159, loss = 0.00489378347992897
iteration 160, loss = 0.005253895185887814
iteration 161, loss = 0.004859147127717733
iteration 162, loss = 0.007893756963312626
iteration 163, loss = 0.004496225155889988
iteration 164, loss = 0.005800269078463316
iteration 165, loss = 0.005219968501478434
iteration 166, loss = 0.004542290233075619
iteration 167, loss = 0.004802773706614971
iteration 168, loss = 0.004363821819424629
iteration 169, loss = 0.007083398289978504
iteration 170, loss = 0.004792550578713417
iteration 171, loss = 0.006898520048707724
iteration 172, loss = 0.004297122359275818
iteration 173, loss = 0.00505373440682888
iteration 174, loss = 0.00640076445415616
iteration 175, loss = 0.004339542239904404
iteration 176, loss = 0.004223773721605539
iteration 177, loss = 0.00418365653604269
iteration 178, loss = 0.005162769928574562
iteration 179, loss = 0.007938935421407223
iteration 180, loss = 0.00433775270357728
iteration 181, loss = 0.0060445996932685375
iteration 182, loss = 0.004709245637059212
iteration 183, loss = 0.0042514316737651825
iteration 184, loss = 0.004943709820508957
iteration 185, loss = 0.004445366095751524
iteration 186, loss = 0.004491892177611589
iteration 187, loss = 0.004526416305452585
iteration 188, loss = 0.004687456414103508
iteration 189, loss = 0.005224546883255243
iteration 190, loss = 0.007087956182658672
iteration 191, loss = 0.0043358877301216125
iteration 192, loss = 0.006868813186883926
iteration 193, loss = 0.006429944187402725
iteration 194, loss = 0.006047537084668875
iteration 195, loss = 0.00444635609164834
iteration 196, loss = 0.00449262373149395
iteration 197, loss = 0.004527839832007885
iteration 198, loss = 0.007424377836287022
iteration 199, loss = 0.007415520027279854
iteration 200, loss = 0.005120964255183935
iteration 201, loss = 0.004869401920586824
iteration 202, loss = 0.004499280825257301
iteration 203, loss = 0.0043001966550946236
iteration 204, loss = 0.005805895663797855
iteration 205, loss = 0.004414774477481842
iteration 206, loss = 0.004540772642940283
iteration 207, loss = 0.004512415267527103
iteration 208, loss = 0.004226737655699253
iteration 209, loss = 0.004776792135089636
iteration 210, loss = 0.004680700600147247
iteration 211, loss = 0.005654015578329563
iteration 212, loss = 0.00443308986723423
iteration 213, loss = 0.00572735583409667
iteration 214, loss = 0.005994078703224659
iteration 215, loss = 0.004515795968472958
iteration 216, loss = 0.004553521983325481
iteration 217, loss = 0.004115738905966282
iteration 218, loss = 0.004278241191059351
iteration 219, loss = 0.006007966585457325
iteration 220, loss = 0.006139079108834267
iteration 221, loss = 0.004472967237234116
iteration 222, loss = 0.004318400286138058
iteration 223, loss = 0.004848015960305929
iteration 224, loss = 0.004776610527187586
iteration 225, loss = 0.007323266938328743
iteration 226, loss = 0.00479454779997468
iteration 227, loss = 0.004105572588741779
iteration 228, loss = 0.004522538743913174
iteration 229, loss = 0.004238177090883255
iteration 230, loss = 0.004765199031680822
iteration 231, loss = 0.004457263275980949
iteration 232, loss = 0.005433287005871534
iteration 233, loss = 0.004230983555316925
iteration 234, loss = 0.006536428816616535
iteration 235, loss = 0.008194519206881523
iteration 236, loss = 0.006709962617605925
iteration 237, loss = 0.005156791303306818
iteration 238, loss = 0.004138517659157515
iteration 239, loss = 0.004759734962135553
iteration 240, loss = 0.0075527941808104515
iteration 241, loss = 0.005203093867748976
iteration 242, loss = 0.004896253347396851
iteration 243, loss = 0.005697783548384905
iteration 244, loss = 0.005658544134348631
iteration 245, loss = 0.004739020951092243
iteration 246, loss = 0.004945925436913967
iteration 247, loss = 0.0052772024646401405
iteration 248, loss = 0.004312986508011818
iteration 249, loss = 0.005455756094306707
iteration 250, loss = 0.004218335263431072
iteration 251, loss = 0.0042924461886286736
iteration 252, loss = 0.006611494813114405
iteration 253, loss = 0.005769375246018171
iteration 254, loss = 0.004336397163569927
iteration 255, loss = 0.004460531286895275
iteration 256, loss = 0.006190447602421045
iteration 257, loss = 0.0054327500984072685
iteration 258, loss = 0.004941416904330254
iteration 259, loss = 0.004263373091816902
iteration 260, loss = 0.004941183142364025
iteration 261, loss = 0.004597752820700407
iteration 262, loss = 0.005693424958735704
iteration 263, loss = 0.004378343932330608
iteration 264, loss = 0.007055537775158882
iteration 265, loss = 0.0044854385778307915
iteration 266, loss = 0.00459735281765461
iteration 267, loss = 0.005611673928797245
iteration 268, loss = 0.004698867443948984
iteration 269, loss = 0.004998307675123215
iteration 270, loss = 0.004878444597125053
iteration 271, loss = 0.006238237023353577
iteration 272, loss = 0.007430211640894413
iteration 273, loss = 0.004684033803641796
iteration 274, loss = 0.004361830186098814
iteration 275, loss = 0.004784613847732544
iteration 276, loss = 0.006083045154809952
iteration 277, loss = 0.004371535498648882
iteration 278, loss = 0.004810755141079426
iteration 279, loss = 0.004332307260483503
iteration 280, loss = 0.004310580436140299
iteration 281, loss = 0.004755993839353323
iteration 282, loss = 0.0054115187376737595
iteration 283, loss = 0.005163726396858692
iteration 284, loss = 0.004535406827926636
iteration 285, loss = 0.0060594044625759125
iteration 286, loss = 0.0046281972900033
iteration 287, loss = 0.00790522899478674
iteration 288, loss = 0.007412528153508902
iteration 289, loss = 0.004579292144626379
iteration 290, loss = 0.004637640435248613
iteration 291, loss = 0.004644555505365133
iteration 292, loss = 0.0049800872802734375
iteration 293, loss = 0.004699746612459421
iteration 294, loss = 0.005663896910846233
iteration 295, loss = 0.00461299205198884
iteration 296, loss = 0.004779006354510784
iteration 297, loss = 0.0045676990412175655
iteration 298, loss = 0.005207360722124577
iteration 299, loss = 0.004454885609447956
iteration 300, loss = 0.004717095289379358
iteration 1, loss = 0.004888113588094711
iteration 2, loss = 0.004426519852131605
iteration 3, loss = 0.004205034580081701
iteration 4, loss = 0.004710384178906679
iteration 5, loss = 0.004297283478081226
iteration 6, loss = 0.004610313568264246
iteration 7, loss = 0.005632559768855572
iteration 8, loss = 0.004482020158320665
iteration 9, loss = 0.00435731653124094
iteration 10, loss = 0.004270378965884447
iteration 11, loss = 0.00437232805415988
iteration 12, loss = 0.006183331366628408
iteration 13, loss = 0.005829732399433851
iteration 14, loss = 0.004212229046970606
iteration 15, loss = 0.0044713132083415985
iteration 16, loss = 0.0045339250937104225
iteration 17, loss = 0.008016335777938366
iteration 18, loss = 0.0044137854129076
iteration 19, loss = 0.00548198726028204
iteration 20, loss = 0.004471419844776392
iteration 21, loss = 0.006069493480026722
iteration 22, loss = 0.004607161041349173
iteration 23, loss = 0.0039751408621668816
iteration 24, loss = 0.0048079462721943855
iteration 25, loss = 0.004951298702508211
iteration 26, loss = 0.005267537664622068
iteration 27, loss = 0.004311522468924522
iteration 28, loss = 0.005515842232853174
iteration 29, loss = 0.0047255102545022964
iteration 30, loss = 0.004507466685026884
iteration 31, loss = 0.00478864461183548
iteration 32, loss = 0.004309722688049078
iteration 33, loss = 0.004453511442989111
iteration 34, loss = 0.005137983243912458
iteration 35, loss = 0.004880217369645834
iteration 36, loss = 0.004551852121949196
iteration 37, loss = 0.004118343815207481
iteration 38, loss = 0.004708008840680122
iteration 39, loss = 0.007426812779158354
iteration 40, loss = 0.006265332456678152
iteration 41, loss = 0.00581444101408124
iteration 42, loss = 0.004984169267117977
iteration 43, loss = 0.004419408738613129
iteration 44, loss = 0.004673069342970848
iteration 45, loss = 0.005110520403832197
iteration 46, loss = 0.004334815312176943
iteration 47, loss = 0.004373255651444197
iteration 48, loss = 0.004536289721727371
iteration 49, loss = 0.004207921214401722
iteration 50, loss = 0.007413807790726423
iteration 51, loss = 0.004932376090437174
iteration 52, loss = 0.004101953469216824
iteration 53, loss = 0.004833047278225422
iteration 54, loss = 0.004926985129714012
iteration 55, loss = 0.004743562079966068
iteration 56, loss = 0.00430353032425046
iteration 57, loss = 0.0042052860371768475
iteration 58, loss = 0.004499559756368399
iteration 59, loss = 0.004150756169110537
iteration 60, loss = 0.004591957665979862
iteration 61, loss = 0.0044209836050868034
iteration 62, loss = 0.004322880879044533
iteration 63, loss = 0.004877711646258831
iteration 64, loss = 0.004590678494423628
iteration 65, loss = 0.004585469141602516
iteration 66, loss = 0.004272190388292074
iteration 67, loss = 0.007394249085336924
iteration 68, loss = 0.004470936954021454
iteration 69, loss = 0.004218199755996466
iteration 70, loss = 0.0045591918751597404
iteration 71, loss = 0.004435000475496054
iteration 72, loss = 0.00451351422816515
iteration 73, loss = 0.008037093095481396
iteration 74, loss = 0.0048605226911604404
iteration 75, loss = 0.005716552957892418
iteration 76, loss = 0.004331742879003286
iteration 77, loss = 0.004901280160993338
iteration 78, loss = 0.0046850405633449554
iteration 79, loss = 0.0044754319824278355
iteration 80, loss = 0.005081589333713055
iteration 81, loss = 0.007528172340244055
iteration 82, loss = 0.004788333084434271
iteration 83, loss = 0.004326946567744017
iteration 84, loss = 0.004903947934508324
iteration 85, loss = 0.006988659035414457
iteration 86, loss = 0.004456571768969297
iteration 87, loss = 0.005290844477713108
iteration 88, loss = 0.0068785953335464
iteration 89, loss = 0.004821726121008396
iteration 90, loss = 0.0045733521692454815
iteration 91, loss = 0.006781283300369978
iteration 92, loss = 0.004535098560154438
iteration 93, loss = 0.0051657408475875854
iteration 94, loss = 0.004806702025234699
iteration 95, loss = 0.004255868960171938
iteration 96, loss = 0.005475468933582306
iteration 97, loss = 0.006660889834165573
iteration 98, loss = 0.004603585693985224
iteration 99, loss = 0.005760790314525366
iteration 100, loss = 0.005688798613846302
iteration 101, loss = 0.004239560104906559
iteration 102, loss = 0.004301872104406357
iteration 103, loss = 0.004236298147588968
iteration 104, loss = 0.004282278008759022
iteration 105, loss = 0.00486123887822032
iteration 106, loss = 0.007253098767250776
iteration 107, loss = 0.004514445085078478
iteration 108, loss = 0.004123349208384752
iteration 109, loss = 0.004426096566021442
iteration 110, loss = 0.0044078282080590725
iteration 111, loss = 0.004493555054068565
iteration 112, loss = 0.00484192930161953
iteration 113, loss = 0.004634351469576359
iteration 114, loss = 0.004378128331154585
iteration 115, loss = 0.004748413804918528
iteration 116, loss = 0.006010064389556646
iteration 117, loss = 0.00441487692296505
iteration 118, loss = 0.004737492650747299
iteration 119, loss = 0.006158371921628714
iteration 120, loss = 0.005097793880850077
iteration 121, loss = 0.00597850326448679
iteration 122, loss = 0.004115618299692869
iteration 123, loss = 0.004405138082802296
iteration 124, loss = 0.004144175909459591
iteration 125, loss = 0.005820459220558405
iteration 126, loss = 0.003997076768428087
iteration 127, loss = 0.004786498378962278
iteration 128, loss = 0.007552696857601404
iteration 129, loss = 0.00812870915979147
iteration 130, loss = 0.006188228726387024
iteration 131, loss = 0.006246648728847504
iteration 132, loss = 0.004852808080613613
iteration 133, loss = 0.006069646216928959
iteration 134, loss = 0.0045609804801642895
iteration 135, loss = 0.004309949930757284
iteration 136, loss = 0.004593067802488804
iteration 137, loss = 0.004667219240218401
iteration 138, loss = 0.004277558531612158
iteration 139, loss = 0.004071921110153198
iteration 140, loss = 0.005545180290937424
iteration 141, loss = 0.006080779246985912
iteration 142, loss = 0.004262528847903013
iteration 143, loss = 0.004530591424554586
iteration 144, loss = 0.00440473947674036
iteration 145, loss = 0.00504685752093792
iteration 146, loss = 0.006683149375021458
iteration 147, loss = 0.007313454989343882
iteration 148, loss = 0.004126958083361387
iteration 149, loss = 0.00433003343641758
iteration 150, loss = 0.004468667786568403
iteration 151, loss = 0.004453754518181086
iteration 152, loss = 0.006089197471737862
iteration 153, loss = 0.007289166562259197
iteration 154, loss = 0.00459862407296896
iteration 155, loss = 0.0042066751047968864
iteration 156, loss = 0.004131182096898556
iteration 157, loss = 0.004219150636345148
iteration 158, loss = 0.004530558828264475
iteration 159, loss = 0.004975521471351385
iteration 160, loss = 0.0047520725056529045
iteration 161, loss = 0.006622680928558111
iteration 162, loss = 0.005142956040799618
iteration 163, loss = 0.007825762033462524
iteration 164, loss = 0.004712652415037155
iteration 165, loss = 0.006288549397140741
iteration 166, loss = 0.005301507189869881
iteration 167, loss = 0.005922216922044754
iteration 168, loss = 0.004490194842219353
iteration 169, loss = 0.004471695516258478
iteration 170, loss = 0.00458303838968277
iteration 171, loss = 0.005745516158640385
iteration 172, loss = 0.004385394975543022
iteration 173, loss = 0.006077460013329983
iteration 174, loss = 0.004402718972414732
iteration 175, loss = 0.00404303427785635
iteration 176, loss = 0.004275346174836159
iteration 177, loss = 0.0041481805965304375
iteration 178, loss = 0.004571835044771433
iteration 179, loss = 0.005583114922046661
iteration 180, loss = 0.0054061016999185085
iteration 181, loss = 0.006042048335075378
iteration 182, loss = 0.004450550768524408
iteration 183, loss = 0.006512991618365049
iteration 184, loss = 0.004950381815433502
iteration 185, loss = 0.006122269667685032
iteration 186, loss = 0.004410249181091785
iteration 187, loss = 0.00670049199834466
iteration 188, loss = 0.004674280062317848
iteration 189, loss = 0.007268848828971386
iteration 190, loss = 0.0046247937716543674
iteration 191, loss = 0.0049831923097372055
iteration 192, loss = 0.004622913897037506
iteration 193, loss = 0.004704256542026997
iteration 194, loss = 0.0042400062084198
iteration 195, loss = 0.00438196025788784
iteration 196, loss = 0.00497419573366642
iteration 197, loss = 0.005965003278106451
iteration 198, loss = 0.004686759319156408
iteration 199, loss = 0.006726711057126522
iteration 200, loss = 0.006328260060399771
iteration 201, loss = 0.006180384196341038
iteration 202, loss = 0.004400458186864853
iteration 203, loss = 0.005010774824768305
iteration 204, loss = 0.0046513923443853855
iteration 205, loss = 0.006206953432410955
iteration 206, loss = 0.004523620009422302
iteration 207, loss = 0.0076825907453894615
iteration 208, loss = 0.0045502944849431515
iteration 209, loss = 0.004378355108201504
iteration 210, loss = 0.004281960893422365
iteration 211, loss = 0.004321963991969824
iteration 212, loss = 0.004226593766361475
iteration 213, loss = 0.004961018450558186
iteration 214, loss = 0.004530655220150948
iteration 215, loss = 0.004591845907270908
iteration 216, loss = 0.004526757635176182
iteration 217, loss = 0.005456421058624983
iteration 218, loss = 0.005597349256277084
iteration 219, loss = 0.007761432323604822
iteration 220, loss = 0.0045094117522239685
iteration 221, loss = 0.00388927198946476
iteration 222, loss = 0.006460072472691536
iteration 223, loss = 0.00592817971482873
iteration 224, loss = 0.004708206281065941
iteration 225, loss = 0.005067538935691118
iteration 226, loss = 0.004355816636234522
iteration 227, loss = 0.005645056255161762
iteration 228, loss = 0.004590699449181557
iteration 229, loss = 0.004812677390873432
iteration 230, loss = 0.006212463602423668
iteration 231, loss = 0.0041626556776463985
iteration 232, loss = 0.0059326463378965855
iteration 233, loss = 0.004167430568486452
iteration 234, loss = 0.0043236021883785725
iteration 235, loss = 0.004505699500441551
iteration 236, loss = 0.004445414524525404
iteration 237, loss = 0.0051564062014222145
iteration 238, loss = 0.004245654214173555
iteration 239, loss = 0.004339470528066158
iteration 240, loss = 0.004217213951051235
iteration 241, loss = 0.005204144865274429
iteration 242, loss = 0.004392081405967474
iteration 243, loss = 0.007217840291559696
iteration 244, loss = 0.0044909389689564705
iteration 245, loss = 0.006528310012072325
iteration 246, loss = 0.0044184485450387
iteration 247, loss = 0.004143285099416971
iteration 248, loss = 0.005885628052055836
iteration 249, loss = 0.005221568513661623
iteration 250, loss = 0.006841117516160011
iteration 251, loss = 0.004453319124877453
iteration 252, loss = 0.004744845908135176
iteration 253, loss = 0.007164745591580868
iteration 254, loss = 0.005628472659736872
iteration 255, loss = 0.004371974617242813
iteration 256, loss = 0.0044631147757172585
iteration 257, loss = 0.004468733910471201
iteration 258, loss = 0.004293829668313265
iteration 259, loss = 0.0042800563387572765
iteration 260, loss = 0.00458374572917819
iteration 261, loss = 0.004606026690453291
iteration 262, loss = 0.004213168751448393
iteration 263, loss = 0.0046182675287127495
iteration 264, loss = 0.005002789199352264
iteration 265, loss = 0.005770811811089516
iteration 266, loss = 0.0045696753077209
iteration 267, loss = 0.004122890532016754
iteration 268, loss = 0.004690030589699745
iteration 269, loss = 0.0048418911173939705
iteration 270, loss = 0.004450664855539799
iteration 271, loss = 0.004816021304577589
iteration 272, loss = 0.0037837019190192223
iteration 273, loss = 0.0040235137566924095
iteration 274, loss = 0.004662173800170422
iteration 275, loss = 0.004370756912976503
iteration 276, loss = 0.0067716496996581554
iteration 277, loss = 0.005037440452724695
iteration 278, loss = 0.004245909862220287
iteration 279, loss = 0.004380240570753813
iteration 280, loss = 0.005827527958899736
iteration 281, loss = 0.004503278993070126
iteration 282, loss = 0.004451156593859196
iteration 283, loss = 0.0044084275141358376
iteration 284, loss = 0.0042678676545619965
iteration 285, loss = 0.004765311721712351
iteration 286, loss = 0.0044206492602825165
iteration 287, loss = 0.004760142881423235
iteration 288, loss = 0.004725088831037283
iteration 289, loss = 0.004493876360356808
iteration 290, loss = 0.0072562056593596935
iteration 291, loss = 0.004139729775488377
iteration 292, loss = 0.005237170495092869
iteration 293, loss = 0.004302409943193197
iteration 294, loss = 0.004492630250751972
iteration 295, loss = 0.004686528816819191
iteration 296, loss = 0.004816517233848572
iteration 297, loss = 0.0043810592032969
iteration 298, loss = 0.004514574073255062
iteration 299, loss = 0.004872811492532492
iteration 300, loss = 0.004544063471257687
iteration 1, loss = 0.004790512844920158
iteration 2, loss = 0.004659443162381649
iteration 3, loss = 0.004556659609079361
iteration 4, loss = 0.0051622698083519936
iteration 5, loss = 0.004633307922631502
iteration 6, loss = 0.007280049845576286
iteration 7, loss = 0.005024118814617395
iteration 8, loss = 0.006084017921239138
iteration 9, loss = 0.006487124599516392
iteration 10, loss = 0.005942170973867178
iteration 11, loss = 0.004708138760179281
iteration 12, loss = 0.0051520331762731075
iteration 13, loss = 0.0048695276491343975
iteration 14, loss = 0.004453219939023256
iteration 15, loss = 0.005913602653890848
iteration 16, loss = 0.005585165228694677
iteration 17, loss = 0.004739317111670971
iteration 18, loss = 0.0043591903522610664
iteration 19, loss = 0.004447564482688904
iteration 20, loss = 0.005525164771825075
iteration 21, loss = 0.004388731438666582
iteration 22, loss = 0.004437102936208248
iteration 23, loss = 0.004460400436073542
iteration 24, loss = 0.004360288847237825
iteration 25, loss = 0.004421029705554247
iteration 26, loss = 0.004312553908675909
iteration 27, loss = 0.004559225402772427
iteration 28, loss = 0.00436426280066371
iteration 29, loss = 0.005919860675930977
iteration 30, loss = 0.006172218360006809
iteration 31, loss = 0.0038026338443160057
iteration 32, loss = 0.004844597075134516
iteration 33, loss = 0.00411858269944787
iteration 34, loss = 0.005735922604799271
iteration 35, loss = 0.00459183007478714
iteration 36, loss = 0.004157814662903547
iteration 37, loss = 0.004122536163777113
iteration 38, loss = 0.004384040366858244
iteration 39, loss = 0.004459440242499113
iteration 40, loss = 0.004092691466212273
iteration 41, loss = 0.007416249252855778
iteration 42, loss = 0.004315622616559267
iteration 43, loss = 0.005964525509625673
iteration 44, loss = 0.005584036465734243
iteration 45, loss = 0.0058028497733175755
iteration 46, loss = 0.004796922672539949
iteration 47, loss = 0.0044025881215929985
iteration 48, loss = 0.003981698304414749
iteration 49, loss = 0.006352593656629324
iteration 50, loss = 0.004158256109803915
iteration 51, loss = 0.00418071961030364
iteration 52, loss = 0.004033786244690418
iteration 53, loss = 0.008573215454816818
iteration 54, loss = 0.004167082253843546
iteration 55, loss = 0.004956338554620743
iteration 56, loss = 0.004567548632621765
iteration 57, loss = 0.004275087267160416
iteration 58, loss = 0.004544100258499384
iteration 59, loss = 0.007147392723709345
iteration 60, loss = 0.0044542280957102776
iteration 61, loss = 0.004282068461179733
iteration 62, loss = 0.005660096183419228
iteration 63, loss = 0.00414250697940588
iteration 64, loss = 0.004418324213474989
iteration 65, loss = 0.004230757709592581
iteration 66, loss = 0.004250919446349144
iteration 67, loss = 0.004519065376371145
iteration 68, loss = 0.007302991114556789
iteration 69, loss = 0.004704856313765049
iteration 70, loss = 0.0059164646081626415
iteration 71, loss = 0.004876152146607637
iteration 72, loss = 0.005357031710445881
iteration 73, loss = 0.004037021659314632
iteration 74, loss = 0.00652637705206871
iteration 75, loss = 0.005079077556729317
iteration 76, loss = 0.004590535070747137
iteration 77, loss = 0.0043394919484853745
iteration 78, loss = 0.006182309705764055
iteration 79, loss = 0.0041332244873046875
iteration 80, loss = 0.004684398416429758
iteration 81, loss = 0.0060338424518704414
iteration 82, loss = 0.005158223677426577
iteration 83, loss = 0.004221681505441666
iteration 84, loss = 0.004965058993548155
iteration 85, loss = 0.004529234487563372
iteration 86, loss = 0.004233463667333126
iteration 87, loss = 0.005816149991005659
iteration 88, loss = 0.006102717947214842
iteration 89, loss = 0.004357956349849701
iteration 90, loss = 0.005798324476927519
iteration 91, loss = 0.004531367216259241
iteration 92, loss = 0.0056607904843986034
iteration 93, loss = 0.00421492476016283
iteration 94, loss = 0.004979193676263094
iteration 95, loss = 0.005078008398413658
iteration 96, loss = 0.006446532905101776
iteration 97, loss = 0.004745939746499062
iteration 98, loss = 0.004305207170546055
iteration 99, loss = 0.004895306192338467
iteration 100, loss = 0.004649449605494738
iteration 101, loss = 0.005330640356987715
iteration 102, loss = 0.004127025604248047
iteration 103, loss = 0.004548246972262859
iteration 104, loss = 0.004242313094437122
iteration 105, loss = 0.004321070853620768
iteration 106, loss = 0.004406005144119263
iteration 107, loss = 0.006085081957280636
iteration 108, loss = 0.00436696782708168
iteration 109, loss = 0.004317169077694416
iteration 110, loss = 0.004449937026947737
iteration 111, loss = 0.005290410481393337
iteration 112, loss = 0.004807164426892996
iteration 113, loss = 0.004634255077689886
iteration 114, loss = 0.005016055889427662
iteration 115, loss = 0.0039412775076925755
iteration 116, loss = 0.004161692224442959
iteration 117, loss = 0.00505074393004179
iteration 118, loss = 0.004167004954069853
iteration 119, loss = 0.005463487468659878
iteration 120, loss = 0.004457538481801748
iteration 121, loss = 0.008440041914582253
iteration 122, loss = 0.004471370950341225
iteration 123, loss = 0.005908091552555561
iteration 124, loss = 0.004300274886190891
iteration 125, loss = 0.004880720283836126
iteration 126, loss = 0.004223886877298355
iteration 127, loss = 0.004538646899163723
iteration 128, loss = 0.006059117615222931
iteration 129, loss = 0.004678074736148119
iteration 130, loss = 0.007609623484313488
iteration 131, loss = 0.004681356251239777
iteration 132, loss = 0.003950247075408697
iteration 133, loss = 0.00557840708643198
iteration 134, loss = 0.00570779200643301
iteration 135, loss = 0.004290875047445297
iteration 136, loss = 0.00473018828779459
iteration 137, loss = 0.006071390118449926
iteration 138, loss = 0.005101443734019995
iteration 139, loss = 0.004399868194013834
iteration 140, loss = 0.0043716756626963615
iteration 141, loss = 0.004046591930091381
iteration 142, loss = 0.0042261723428964615
iteration 143, loss = 0.004356340505182743
iteration 144, loss = 0.004156825598329306
iteration 145, loss = 0.004123304970562458
iteration 146, loss = 0.004138693679124117
iteration 147, loss = 0.004213682841509581
iteration 148, loss = 0.004095426760613918
iteration 149, loss = 0.005474270321428776
iteration 150, loss = 0.006101657636463642
iteration 151, loss = 0.004519075620919466
iteration 152, loss = 0.004144076723605394
iteration 153, loss = 0.00468498095870018
iteration 154, loss = 0.003963687922805548
iteration 155, loss = 0.004561392590403557
iteration 156, loss = 0.004425504244863987
iteration 157, loss = 0.004490654915571213
iteration 158, loss = 0.0043879905715584755
iteration 159, loss = 0.004597801249474287
iteration 160, loss = 0.004392701666802168
iteration 161, loss = 0.004042865242809057
iteration 162, loss = 0.004998972173780203
iteration 163, loss = 0.007644945755600929
iteration 164, loss = 0.007094006985425949
iteration 165, loss = 0.004176784306764603
iteration 166, loss = 0.006314392667263746
iteration 167, loss = 0.0048197065480053425
iteration 168, loss = 0.0042574964463710785
iteration 169, loss = 0.00542430579662323
iteration 170, loss = 0.005060556810349226
iteration 171, loss = 0.004499809350818396
iteration 172, loss = 0.004337538033723831
iteration 173, loss = 0.004165599122643471
iteration 174, loss = 0.008116401731967926
iteration 175, loss = 0.007130627054721117
iteration 176, loss = 0.004276781342923641
iteration 177, loss = 0.004072404466569424
iteration 178, loss = 0.004297721665352583
iteration 179, loss = 0.007940616458654404
iteration 180, loss = 0.00426596449688077
iteration 181, loss = 0.00452769361436367
iteration 182, loss = 0.007106142584234476
iteration 183, loss = 0.004739385563880205
iteration 184, loss = 0.0070260376669466496
iteration 185, loss = 0.0062671322375535965
iteration 186, loss = 0.00413074204698205
iteration 187, loss = 0.004793120082467794
iteration 188, loss = 0.004265369847416878
iteration 189, loss = 0.004254847764968872
iteration 190, loss = 0.005888199433684349
iteration 191, loss = 0.007191587705165148
iteration 192, loss = 0.004167585168033838
iteration 193, loss = 0.004521107766777277
iteration 194, loss = 0.0062286811880767345
iteration 195, loss = 0.0043488070368766785
iteration 196, loss = 0.004529713653028011
iteration 197, loss = 0.0059523265808820724
iteration 198, loss = 0.004453135188668966
iteration 199, loss = 0.005033242516219616
iteration 200, loss = 0.003838595235720277
iteration 201, loss = 0.003960263915359974
iteration 202, loss = 0.005908985156565905
iteration 203, loss = 0.004486291203647852
iteration 204, loss = 0.004521031863987446
iteration 205, loss = 0.004257900174707174
iteration 206, loss = 0.004321180284023285
iteration 207, loss = 0.004717480391263962
iteration 208, loss = 0.004270135425031185
iteration 209, loss = 0.005520046688616276
iteration 210, loss = 0.006881197448819876
iteration 211, loss = 0.004443966317921877
iteration 212, loss = 0.005721254739910364
iteration 213, loss = 0.004559778142720461
iteration 214, loss = 0.00410103565081954
iteration 215, loss = 0.005207369104027748
iteration 216, loss = 0.0045114061795175076
iteration 217, loss = 0.00448210071772337
iteration 218, loss = 0.004394509829580784
iteration 219, loss = 0.004615643993020058
iteration 220, loss = 0.004454299807548523
iteration 221, loss = 0.006840507034212351
iteration 222, loss = 0.004454483278095722
iteration 223, loss = 0.004867962561547756
iteration 224, loss = 0.004290951881557703
iteration 225, loss = 0.004452073015272617
iteration 226, loss = 0.004894908983260393
iteration 227, loss = 0.004872526507824659
iteration 228, loss = 0.004542539827525616
iteration 229, loss = 0.004767626989632845
iteration 230, loss = 0.004766091238707304
iteration 231, loss = 0.0053598289377987385
iteration 232, loss = 0.006368447560817003
iteration 233, loss = 0.004477096255868673
iteration 234, loss = 0.004791088867932558
iteration 235, loss = 0.004039394669234753
iteration 236, loss = 0.00458010658621788
iteration 237, loss = 0.005472950171679258
iteration 238, loss = 0.004377194680273533
iteration 239, loss = 0.004890778101980686
iteration 240, loss = 0.004563989583402872
iteration 241, loss = 0.004577687941491604
iteration 242, loss = 0.004271987825632095
iteration 243, loss = 0.004704105667769909
iteration 244, loss = 0.004198521375656128
iteration 245, loss = 0.004245084710419178
iteration 246, loss = 0.004490337800234556
iteration 247, loss = 0.0041855620220303535
iteration 248, loss = 0.0043886504136025906
iteration 249, loss = 0.005952442064881325
iteration 250, loss = 0.003930998034775257
iteration 251, loss = 0.004933143965899944
iteration 252, loss = 0.004483378957957029
iteration 253, loss = 0.004766714759171009
iteration 254, loss = 0.004560157656669617
iteration 255, loss = 0.007487585302442312
iteration 256, loss = 0.00469099311158061
iteration 257, loss = 0.004418560303747654
iteration 258, loss = 0.0055320532992482185
iteration 259, loss = 0.004549548961222172
iteration 260, loss = 0.004619550425559282
iteration 261, loss = 0.004769926890730858
iteration 262, loss = 0.004303887486457825
iteration 263, loss = 0.0043429480865597725
iteration 264, loss = 0.0062311552464962006
iteration 265, loss = 0.004917903337627649
iteration 266, loss = 0.004300602246075869
iteration 267, loss = 0.00416258629411459
iteration 268, loss = 0.005135190673172474
iteration 269, loss = 0.004549908451735973
iteration 270, loss = 0.004456480033695698
iteration 271, loss = 0.00422869436442852
iteration 272, loss = 0.004344574175775051
iteration 273, loss = 0.004377472680062056
iteration 274, loss = 0.004481627605855465
iteration 275, loss = 0.0045832376927137375
iteration 276, loss = 0.004413534887135029
iteration 277, loss = 0.004313513170927763
iteration 278, loss = 0.004414612893015146
iteration 279, loss = 0.005665627773851156
iteration 280, loss = 0.004219899885356426
iteration 281, loss = 0.008532832376658916
iteration 282, loss = 0.004295903258025646
iteration 283, loss = 0.004397771321237087
iteration 284, loss = 0.005035691428929567
iteration 285, loss = 0.0043110316619277
iteration 286, loss = 0.004681386053562164
iteration 287, loss = 0.004229358863085508
iteration 288, loss = 0.0046672760508954525
iteration 289, loss = 0.008062862791121006
iteration 290, loss = 0.004229484125971794
iteration 291, loss = 0.004389941692352295
iteration 292, loss = 0.0041008880361914635
iteration 293, loss = 0.005920375697314739
iteration 294, loss = 0.006651410833001137
iteration 295, loss = 0.0046253809705376625
iteration 296, loss = 0.004320806358009577
iteration 297, loss = 0.004950214643031359
iteration 298, loss = 0.004398306366056204
iteration 299, loss = 0.004000406246632338
iteration 300, loss = 0.005988671910017729
iteration 1, loss = 0.004748570267111063
iteration 2, loss = 0.004381593782454729
iteration 3, loss = 0.004382000770419836
iteration 4, loss = 0.004285345785319805
iteration 5, loss = 0.004315607249736786
iteration 6, loss = 0.004101221449673176
iteration 7, loss = 0.004812948405742645
iteration 8, loss = 0.0046185883693397045
iteration 9, loss = 0.004231862258166075
iteration 10, loss = 0.00492177763953805
iteration 11, loss = 0.006331755314022303
iteration 12, loss = 0.004950331058353186
iteration 13, loss = 0.004258862230926752
iteration 14, loss = 0.0045149135403335094
iteration 15, loss = 0.004623886663466692
iteration 16, loss = 0.005832314491271973
iteration 17, loss = 0.004213249310851097
iteration 18, loss = 0.00495552271604538
iteration 19, loss = 0.004452744964510202
iteration 20, loss = 0.004206934943795204
iteration 21, loss = 0.004453343339264393
iteration 22, loss = 0.004235204309225082
iteration 23, loss = 0.007256428711116314
iteration 24, loss = 0.004308075178414583
iteration 25, loss = 0.004400357138365507
iteration 26, loss = 0.004175844602286816
iteration 27, loss = 0.004241399932652712
iteration 28, loss = 0.006524702534079552
iteration 29, loss = 0.003902638563886285
iteration 30, loss = 0.004395175725221634
iteration 31, loss = 0.004541249014437199
iteration 32, loss = 0.00475855078548193
iteration 33, loss = 0.004313717596232891
iteration 34, loss = 0.005626731086522341
iteration 35, loss = 0.0051156217232346535
iteration 36, loss = 0.004284501541405916
iteration 37, loss = 0.004638955928385258
iteration 38, loss = 0.004238221328705549
iteration 39, loss = 0.004392655566334724
iteration 40, loss = 0.004341518972069025
iteration 41, loss = 0.0045241122134029865
iteration 42, loss = 0.0063817002810537815
iteration 43, loss = 0.004051943309605122
iteration 44, loss = 0.005599061492830515
iteration 45, loss = 0.00400907639414072
iteration 46, loss = 0.004989376291632652
iteration 47, loss = 0.004669428803026676
iteration 48, loss = 0.0043326206505298615
iteration 49, loss = 0.004337544087320566
iteration 50, loss = 0.0048517389222979546
iteration 51, loss = 0.004094488453119993
iteration 52, loss = 0.0059667727909982204
iteration 53, loss = 0.004064399283379316
iteration 54, loss = 0.00437867920845747
iteration 55, loss = 0.004079725593328476
iteration 56, loss = 0.004746720660477877
iteration 57, loss = 0.005506662651896477
iteration 58, loss = 0.004586114548146725
iteration 59, loss = 0.0044395639561116695
iteration 60, loss = 0.0050389813259243965
iteration 61, loss = 0.004132972564548254
iteration 62, loss = 0.004240952432155609
iteration 63, loss = 0.00429364712908864
iteration 64, loss = 0.004798657260835171
iteration 65, loss = 0.003849772270768881
iteration 66, loss = 0.00425139581784606
iteration 67, loss = 0.005027864594012499
iteration 68, loss = 0.005863016936928034
iteration 69, loss = 0.00418096873909235
iteration 70, loss = 0.004423384554684162
iteration 71, loss = 0.00408780574798584
iteration 72, loss = 0.004322346765547991
iteration 73, loss = 0.004378300625830889
iteration 74, loss = 0.004280911758542061
iteration 75, loss = 0.007092694286257029
iteration 76, loss = 0.004878197330981493
iteration 77, loss = 0.0040900688618421555
iteration 78, loss = 0.00411190977320075
iteration 79, loss = 0.004989613778889179
iteration 80, loss = 0.0053699081763625145
iteration 81, loss = 0.004261311609297991
iteration 82, loss = 0.00560340192168951
iteration 83, loss = 0.0041433158330619335
iteration 84, loss = 0.0041826567612588406
iteration 85, loss = 0.004103570710867643
iteration 86, loss = 0.004779791459441185
iteration 87, loss = 0.004360665567219257
iteration 88, loss = 0.0045808288268744946
iteration 89, loss = 0.006354739889502525
iteration 90, loss = 0.004650617018342018
iteration 91, loss = 0.006034722086042166
iteration 92, loss = 0.004432888235896826
iteration 93, loss = 0.006459599826484919
iteration 94, loss = 0.004566988907754421
iteration 95, loss = 0.004376910626888275
iteration 96, loss = 0.004516183398663998
iteration 97, loss = 0.004218153655529022
iteration 98, loss = 0.006326564121991396
iteration 99, loss = 0.00441282382234931
iteration 100, loss = 0.0046632965095341206
iteration 101, loss = 0.00716753676533699
iteration 102, loss = 0.004413619637489319
iteration 103, loss = 0.004175858572125435
iteration 104, loss = 0.004614945966750383
iteration 105, loss = 0.004841289483010769
iteration 106, loss = 0.004428634885698557
iteration 107, loss = 0.00436836201697588
iteration 108, loss = 0.004310606978833675
iteration 109, loss = 0.004438974894583225
iteration 110, loss = 0.004291771911084652
iteration 111, loss = 0.004592273849993944
iteration 112, loss = 0.004216820001602173
iteration 113, loss = 0.004469091072678566
iteration 114, loss = 0.004639585968106985
iteration 115, loss = 0.006169391795992851
iteration 116, loss = 0.004887045826762915
iteration 117, loss = 0.004735019989311695
iteration 118, loss = 0.0045842076651751995
iteration 119, loss = 0.0056032841093838215
iteration 120, loss = 0.004018893465399742
iteration 121, loss = 0.004160440526902676
iteration 122, loss = 0.007370872888714075
iteration 123, loss = 0.00451552914455533
iteration 124, loss = 0.004113487433642149
iteration 125, loss = 0.006010343786329031
iteration 126, loss = 0.004300970118492842
iteration 127, loss = 0.005970821250230074
iteration 128, loss = 0.0052209580317139626
iteration 129, loss = 0.00387631356716156
iteration 130, loss = 0.004309244453907013
iteration 131, loss = 0.004827515222132206
iteration 132, loss = 0.00412511033937335
iteration 133, loss = 0.004121146164834499
iteration 134, loss = 0.0045719933696091175
iteration 135, loss = 0.00432218424975872
iteration 136, loss = 0.004499000031501055
iteration 137, loss = 0.0077944169752299786
iteration 138, loss = 0.0063708629459142685
iteration 139, loss = 0.00422133831307292
iteration 140, loss = 0.004301284905523062
iteration 141, loss = 0.00509515730664134
iteration 142, loss = 0.004217682871967554
iteration 143, loss = 0.006185930222272873
iteration 144, loss = 0.00435160705819726
iteration 145, loss = 0.006144107319414616
iteration 146, loss = 0.006634218152612448
iteration 147, loss = 0.006541586946696043
iteration 148, loss = 0.00546078197658062
iteration 149, loss = 0.0040976498275995255
iteration 150, loss = 0.004538198933005333
iteration 151, loss = 0.004150588996708393
iteration 152, loss = 0.003958811052143574
iteration 153, loss = 0.005025612656027079
iteration 154, loss = 0.005322401877492666
iteration 155, loss = 0.003925308585166931
iteration 156, loss = 0.0046884710900485516
iteration 157, loss = 0.004383032210171223
iteration 158, loss = 0.006087333895266056
iteration 159, loss = 0.004029981791973114
iteration 160, loss = 0.004606024362146854
iteration 161, loss = 0.0053904554806649685
iteration 162, loss = 0.00614524818956852
iteration 163, loss = 0.004200865048915148
iteration 164, loss = 0.00408421503379941
iteration 165, loss = 0.004500930197536945
iteration 166, loss = 0.004245822783559561
iteration 167, loss = 0.004346269648522139
iteration 168, loss = 0.004180652555078268
iteration 169, loss = 0.005369609221816063
iteration 170, loss = 0.00687004579231143
iteration 171, loss = 0.0088204862549901
iteration 172, loss = 0.005187721457332373
iteration 173, loss = 0.005060020834207535
iteration 174, loss = 0.00554268853738904
iteration 175, loss = 0.0060250647366046906
iteration 176, loss = 0.004414689727127552
iteration 177, loss = 0.004161259159445763
iteration 178, loss = 0.004406176041811705
iteration 179, loss = 0.005287938751280308
iteration 180, loss = 0.004969886038452387
iteration 181, loss = 0.004068827256560326
iteration 182, loss = 0.006957550533115864
iteration 183, loss = 0.0057657621800899506
iteration 184, loss = 0.0043048313818871975
iteration 185, loss = 0.004785493016242981
iteration 186, loss = 0.004140953999012709
iteration 187, loss = 0.004600521642714739
iteration 188, loss = 0.004450036212801933
iteration 189, loss = 0.007045271340757608
iteration 190, loss = 0.004560648929327726
iteration 191, loss = 0.005742030218243599
iteration 192, loss = 0.004630597773939371
iteration 193, loss = 0.007421351503580809
iteration 194, loss = 0.004528045654296875
iteration 195, loss = 0.004517350345849991
iteration 196, loss = 0.007235442753881216
iteration 197, loss = 0.00468885013833642
iteration 198, loss = 0.003939294721931219
iteration 199, loss = 0.00435222452506423
iteration 200, loss = 0.004544117487967014
iteration 201, loss = 0.004831290803849697
iteration 202, loss = 0.00660629291087389
iteration 203, loss = 0.004453388508409262
iteration 204, loss = 0.005338219925761223
iteration 205, loss = 0.0043062735348939896
iteration 206, loss = 0.0039722612127661705
iteration 207, loss = 0.0045939660631120205
iteration 208, loss = 0.00553434481844306
iteration 209, loss = 0.007855514995753765
iteration 210, loss = 0.004512032028287649
iteration 211, loss = 0.009103536605834961
iteration 212, loss = 0.004420510958880186
iteration 213, loss = 0.0043606446124613285
iteration 214, loss = 0.003924728836864233
iteration 215, loss = 0.007529312279075384
iteration 216, loss = 0.004430026281625032
iteration 217, loss = 0.0055433171801269054
iteration 218, loss = 0.004298788961023092
iteration 219, loss = 0.0040015787817537785
iteration 220, loss = 0.004545931238681078
iteration 221, loss = 0.0058860015124082565
iteration 222, loss = 0.004464846104383469
iteration 223, loss = 0.004124623257666826
iteration 224, loss = 0.004854465834796429
iteration 225, loss = 0.0038875648751854897
iteration 226, loss = 0.004473850596696138
iteration 227, loss = 0.00495386216789484
iteration 228, loss = 0.004279321525245905
iteration 229, loss = 0.004533748142421246
iteration 230, loss = 0.004079386126250029
iteration 231, loss = 0.004234838765114546
iteration 232, loss = 0.004647218622267246
iteration 233, loss = 0.003998776897788048
iteration 234, loss = 0.004236273933202028
iteration 235, loss = 0.004784096032381058
iteration 236, loss = 0.003832463175058365
iteration 237, loss = 0.0036255489103496075
iteration 238, loss = 0.004416331648826599
iteration 239, loss = 0.007375064305961132
iteration 240, loss = 0.004169943276792765
iteration 241, loss = 0.003924547228962183
iteration 242, loss = 0.00414632260799408
iteration 243, loss = 0.004556064493954182
iteration 244, loss = 0.004350237548351288
iteration 245, loss = 0.004414712078869343
iteration 246, loss = 0.004428090527653694
iteration 247, loss = 0.004089700523763895
iteration 248, loss = 0.004225038457661867
iteration 249, loss = 0.004120520316064358
iteration 250, loss = 0.004331291187554598
iteration 251, loss = 0.004422532394528389
iteration 252, loss = 0.00751904584467411
iteration 253, loss = 0.0056879096664488316
iteration 254, loss = 0.004389029927551746
iteration 255, loss = 0.008438832126557827
iteration 256, loss = 0.005294987000524998
iteration 257, loss = 0.0040394896641373634
iteration 258, loss = 0.003914754372090101
iteration 259, loss = 0.005838094279170036
iteration 260, loss = 0.0069071645848453045
iteration 261, loss = 0.005747794173657894
iteration 262, loss = 0.0039612725377082825
iteration 263, loss = 0.004348483402282
iteration 264, loss = 0.004413621965795755
iteration 265, loss = 0.003913757856935263
iteration 266, loss = 0.0057253786362707615
iteration 267, loss = 0.005767684895545244
iteration 268, loss = 0.0044324398040771484
iteration 269, loss = 0.0069939508102834225
iteration 270, loss = 0.0048068524338305
iteration 271, loss = 0.003879938740283251
iteration 272, loss = 0.004213270265609026
iteration 273, loss = 0.004338925238698721
iteration 274, loss = 0.004050479270517826
iteration 275, loss = 0.005744564812630415
iteration 276, loss = 0.005033326335251331
iteration 277, loss = 0.00409491965547204
iteration 278, loss = 0.00464644143357873
iteration 279, loss = 0.0042539662681519985
iteration 280, loss = 0.004208832047879696
iteration 281, loss = 0.004035891033709049
iteration 282, loss = 0.004219437949359417
iteration 283, loss = 0.004825688898563385
iteration 284, loss = 0.004675418138504028
iteration 285, loss = 0.004239773843437433
iteration 286, loss = 0.006815553177148104
iteration 287, loss = 0.004240881185978651
iteration 288, loss = 0.004252324346452951
iteration 289, loss = 0.0038472560700029135
iteration 290, loss = 0.005458299070596695
iteration 291, loss = 0.007103851996362209
iteration 292, loss = 0.004978777840733528
iteration 293, loss = 0.004605856724083424
iteration 294, loss = 0.004497685469686985
iteration 295, loss = 0.004452758934348822
iteration 296, loss = 0.004459432791918516
iteration 297, loss = 0.004486665595322847
iteration 298, loss = 0.0053487434051930904
iteration 299, loss = 0.004056673962622881
iteration 300, loss = 0.004346913192421198
iteration 1, loss = 0.004480636678636074
iteration 2, loss = 0.004524836782366037
iteration 3, loss = 0.004314727615565062
iteration 4, loss = 0.005393248051404953
iteration 5, loss = 0.004261689726263285
iteration 6, loss = 0.0053511145524680614
iteration 7, loss = 0.004368300084024668
iteration 8, loss = 0.004315314348787069
iteration 9, loss = 0.004031102638691664
iteration 10, loss = 0.00541032524779439
iteration 11, loss = 0.004268093500286341
iteration 12, loss = 0.004218063782900572
iteration 13, loss = 0.004311223514378071
iteration 14, loss = 0.0039301663637161255
iteration 15, loss = 0.004502222407609224
iteration 16, loss = 0.004195161163806915
iteration 17, loss = 0.004738673567771912
iteration 18, loss = 0.004387756809592247
iteration 19, loss = 0.0043994104489684105
iteration 20, loss = 0.004304020199924707
iteration 21, loss = 0.004006644245237112
iteration 22, loss = 0.003934403415769339
iteration 23, loss = 0.004402246791869402
iteration 24, loss = 0.0054069091565907
iteration 25, loss = 0.004422363825142384
iteration 26, loss = 0.004239494912326336
iteration 27, loss = 0.00424543023109436
iteration 28, loss = 0.0044712042436003685
iteration 29, loss = 0.00492874626070261
iteration 30, loss = 0.004738795571029186
iteration 31, loss = 0.004442622419446707
iteration 32, loss = 0.004188361577689648
iteration 33, loss = 0.004819769877940416
iteration 34, loss = 0.004205206874758005
iteration 35, loss = 0.004387150052934885
iteration 36, loss = 0.0041732266545295715
iteration 37, loss = 0.003949716687202454
iteration 38, loss = 0.004322944208979607
iteration 39, loss = 0.00411270372569561
iteration 40, loss = 0.004417126066982746
iteration 41, loss = 0.00470432685688138
iteration 42, loss = 0.006798009388148785
iteration 43, loss = 0.005888654384762049
iteration 44, loss = 0.004212366882711649
iteration 45, loss = 0.004736988339573145
iteration 46, loss = 0.004050970543175936
iteration 47, loss = 0.005401925649493933
iteration 48, loss = 0.003888526698574424
iteration 49, loss = 0.0040345750749111176
iteration 50, loss = 0.004253337159752846
iteration 51, loss = 0.006429489701986313
iteration 52, loss = 0.004747665021568537
iteration 53, loss = 0.004425588063895702
iteration 54, loss = 0.004244698677212
iteration 55, loss = 0.005047998391091824
iteration 56, loss = 0.005918929353356361
iteration 57, loss = 0.004584498703479767
iteration 58, loss = 0.005232283845543861
iteration 59, loss = 0.005720929242670536
iteration 60, loss = 0.005695187021046877
iteration 61, loss = 0.005971082486212254
iteration 62, loss = 0.007147960364818573
iteration 63, loss = 0.005532150622457266
iteration 64, loss = 0.004164004698395729
iteration 65, loss = 0.007923945784568787
iteration 66, loss = 0.006288506090641022
iteration 67, loss = 0.005980477202683687
iteration 68, loss = 0.004100363235920668
iteration 69, loss = 0.004372531548142433
iteration 70, loss = 0.004479571711272001
iteration 71, loss = 0.0057979486882686615
iteration 72, loss = 0.004846712574362755
iteration 73, loss = 0.006968879606574774
iteration 74, loss = 0.00420914962887764
iteration 75, loss = 0.004148570820689201
iteration 76, loss = 0.005797897465527058
iteration 77, loss = 0.004231824539601803
iteration 78, loss = 0.004921276122331619
iteration 79, loss = 0.0044128322042524815
iteration 80, loss = 0.007155254017561674
iteration 81, loss = 0.004350919742137194
iteration 82, loss = 0.00615606177598238
iteration 83, loss = 0.00390579248778522
iteration 84, loss = 0.004264656454324722
iteration 85, loss = 0.004028621129691601
iteration 86, loss = 0.005304320249706507
iteration 87, loss = 0.007060061674565077
iteration 88, loss = 0.004888225346803665
iteration 89, loss = 0.005035005044192076
iteration 90, loss = 0.004318605177104473
iteration 91, loss = 0.003955291584134102
iteration 92, loss = 0.008577566593885422
iteration 93, loss = 0.004167527891695499
iteration 94, loss = 0.00534110888838768
iteration 95, loss = 0.0042518870905041695
iteration 96, loss = 0.004976104479283094
iteration 97, loss = 0.0041791233234107494
iteration 98, loss = 0.00396463880315423
iteration 99, loss = 0.004948469344526529
iteration 100, loss = 0.004387913271784782
iteration 101, loss = 0.004380711354315281
iteration 102, loss = 0.00421932153403759
iteration 103, loss = 0.003924822434782982
iteration 104, loss = 0.004487727303057909
iteration 105, loss = 0.005834458861500025
iteration 106, loss = 0.004359722603112459
iteration 107, loss = 0.004092840012162924
iteration 108, loss = 0.00462401332333684
iteration 109, loss = 0.004559229593724012
iteration 110, loss = 0.004247316159307957
iteration 111, loss = 0.005945353303104639
iteration 112, loss = 0.004373654723167419
iteration 113, loss = 0.0037607585545629263
iteration 114, loss = 0.004904165863990784
iteration 115, loss = 0.004139265976846218
iteration 116, loss = 0.0044637746177613735
iteration 117, loss = 0.004284976981580257
iteration 118, loss = 0.004283225163817406
iteration 119, loss = 0.0041413381695747375
iteration 120, loss = 0.006359784863889217
iteration 121, loss = 0.0051682027988135815
iteration 122, loss = 0.0040363105945289135
iteration 123, loss = 0.005102014169096947
iteration 124, loss = 0.00431621540337801
iteration 125, loss = 0.0050986651331186295
iteration 126, loss = 0.004769796505570412
iteration 127, loss = 0.004367529880255461
iteration 128, loss = 0.004628942348062992
iteration 129, loss = 0.004268387332558632
iteration 130, loss = 0.004261388443410397
iteration 131, loss = 0.0043044909834861755
iteration 132, loss = 0.004739050753414631
iteration 133, loss = 0.005176156293600798
iteration 134, loss = 0.0046694898046553135
iteration 135, loss = 0.003921528812497854
iteration 136, loss = 0.004023701883852482
iteration 137, loss = 0.004443005658686161
iteration 138, loss = 0.004445347934961319
iteration 139, loss = 0.004502788186073303
iteration 140, loss = 0.004177593160420656
iteration 141, loss = 0.005817340686917305
iteration 142, loss = 0.004812563769519329
iteration 143, loss = 0.004009644966572523
iteration 144, loss = 0.0046156481839716434
iteration 145, loss = 0.004872825928032398
iteration 146, loss = 0.004054605960845947
iteration 147, loss = 0.005363373085856438
iteration 148, loss = 0.003871857188642025
iteration 149, loss = 0.005022432189434767
iteration 150, loss = 0.0068745543248951435
iteration 151, loss = 0.0047337706200778484
iteration 152, loss = 0.004256618209183216
iteration 153, loss = 0.004370597656816244
iteration 154, loss = 0.006291034631431103
iteration 155, loss = 0.00584915466606617
iteration 156, loss = 0.004193092230707407
iteration 157, loss = 0.0050534517504274845
iteration 158, loss = 0.005363949108868837
iteration 159, loss = 0.007054670248180628
iteration 160, loss = 0.005636046640574932
iteration 161, loss = 0.004198973998427391
iteration 162, loss = 0.003713439917191863
iteration 163, loss = 0.004126047715544701
iteration 164, loss = 0.004299077205359936
iteration 165, loss = 0.0060077547095716
iteration 166, loss = 0.004548128228634596
iteration 167, loss = 0.0059910970740020275
iteration 168, loss = 0.00597036350518465
iteration 169, loss = 0.004181274678558111
iteration 170, loss = 0.004352532792836428
iteration 171, loss = 0.004539531655609608
iteration 172, loss = 0.004708911292254925
iteration 173, loss = 0.004359308164566755
iteration 174, loss = 0.004517648369073868
iteration 175, loss = 0.0044375257566571236
iteration 176, loss = 0.0054032341577112675
iteration 177, loss = 0.004023722372949123
iteration 178, loss = 0.003985543269664049
iteration 179, loss = 0.004306158050894737
iteration 180, loss = 0.004132144618779421
iteration 181, loss = 0.005855043884366751
iteration 182, loss = 0.004200365860015154
iteration 183, loss = 0.003992375917732716
iteration 184, loss = 0.007184559479355812
iteration 185, loss = 0.0043184650130569935
iteration 186, loss = 0.004111338406801224
iteration 187, loss = 0.004171114414930344
iteration 188, loss = 0.007268494926393032
iteration 189, loss = 0.007589465007185936
iteration 190, loss = 0.004269029479473829
iteration 191, loss = 0.004498613066971302
iteration 192, loss = 0.004271355923265219
iteration 193, loss = 0.004466481506824493
iteration 194, loss = 0.003957301378250122
iteration 195, loss = 0.004382725339382887
iteration 196, loss = 0.004639407619833946
iteration 197, loss = 0.004447896499186754
iteration 198, loss = 0.007042817305773497
iteration 199, loss = 0.004307369235903025
iteration 200, loss = 0.005526439286768436
iteration 201, loss = 0.004155374132096767
iteration 202, loss = 0.004424038343131542
iteration 203, loss = 0.004456456750631332
iteration 204, loss = 0.004867203999310732
iteration 205, loss = 0.006366557441651821
iteration 206, loss = 0.005463091190904379
iteration 207, loss = 0.005666586570441723
iteration 208, loss = 0.006005423609167337
iteration 209, loss = 0.0044558728113770485
iteration 210, loss = 0.005043914541602135
iteration 211, loss = 0.0043342746794223785
iteration 212, loss = 0.004333585035055876
iteration 213, loss = 0.007544194348156452
iteration 214, loss = 0.004015177022665739
iteration 215, loss = 0.005224458873271942
iteration 216, loss = 0.0043389941565692425
iteration 217, loss = 0.004413099028170109
iteration 218, loss = 0.004515458829700947
iteration 219, loss = 0.004466444719582796
iteration 220, loss = 0.00410459004342556
iteration 221, loss = 0.004684676881879568
iteration 222, loss = 0.006850393954664469
iteration 223, loss = 0.0042130909860134125
iteration 224, loss = 0.004331881646066904
iteration 225, loss = 0.0040757120586931705
iteration 226, loss = 0.004686948377639055
iteration 227, loss = 0.004677499178797007
iteration 228, loss = 0.004158972762525082
iteration 229, loss = 0.004087860696017742
iteration 230, loss = 0.004182201810181141
iteration 231, loss = 0.004539478570222855
iteration 232, loss = 0.0043180594220757484
iteration 233, loss = 0.006964579690247774
iteration 234, loss = 0.005242076702415943
iteration 235, loss = 0.005845761392265558
iteration 236, loss = 0.007572856731712818
iteration 237, loss = 0.004866465460509062
iteration 238, loss = 0.006287011317908764
iteration 239, loss = 0.004047791939228773
iteration 240, loss = 0.0042525650933384895
iteration 241, loss = 0.004659158643335104
iteration 242, loss = 0.004364254884421825
iteration 243, loss = 0.004207400605082512
iteration 244, loss = 0.004181529860943556
iteration 245, loss = 0.003713215934112668
iteration 246, loss = 0.004414365626871586
iteration 247, loss = 0.004383455030620098
iteration 248, loss = 0.004891677759587765
iteration 249, loss = 0.004210061859339476
iteration 250, loss = 0.005094949156045914
iteration 251, loss = 0.005661639850586653
iteration 252, loss = 0.005750811658799648
iteration 253, loss = 0.004576258827000856
iteration 254, loss = 0.004472379572689533
iteration 255, loss = 0.004375905729830265
iteration 256, loss = 0.0043577468022704124
iteration 257, loss = 0.004082347732037306
iteration 258, loss = 0.00583898089826107
iteration 259, loss = 0.006203037686645985
iteration 260, loss = 0.005828720983117819
iteration 261, loss = 0.005703108850866556
iteration 262, loss = 0.0040327380411326885
iteration 263, loss = 0.005444681737571955
iteration 264, loss = 0.0044528888538479805
iteration 265, loss = 0.00458834134042263
iteration 266, loss = 0.00476898904889822
iteration 267, loss = 0.0039545688778162
iteration 268, loss = 0.0041586789302527905
iteration 269, loss = 0.004038746003061533
iteration 270, loss = 0.004469015635550022
iteration 271, loss = 0.004438616335391998
iteration 272, loss = 0.004193900153040886
iteration 273, loss = 0.008038143627345562
iteration 274, loss = 0.004114148672670126
iteration 275, loss = 0.0045965262688696384
iteration 276, loss = 0.0056100343354046345
iteration 277, loss = 0.004289350006729364
iteration 278, loss = 0.004404302220791578
iteration 279, loss = 0.005329451989382505
iteration 280, loss = 0.0043257372453808784
iteration 281, loss = 0.004700212273746729
iteration 282, loss = 0.0064782253466546535
iteration 283, loss = 0.004313270095735788
iteration 284, loss = 0.005972486454993486
iteration 285, loss = 0.003933541942387819
iteration 286, loss = 0.004081706050783396
iteration 287, loss = 0.005134605802595615
iteration 288, loss = 0.005007429979741573
iteration 289, loss = 0.004898588173091412
iteration 290, loss = 0.004159039352089167
iteration 291, loss = 0.005538188386708498
iteration 292, loss = 0.004585688002407551
iteration 293, loss = 0.003973850980401039
iteration 294, loss = 0.004156460985541344
iteration 295, loss = 0.00405893987044692
iteration 296, loss = 0.004004500340670347
iteration 297, loss = 0.00422865804284811
iteration 298, loss = 0.004343079403042793
iteration 299, loss = 0.004652112256735563
iteration 300, loss = 0.00425234530121088
iteration 1, loss = 0.004254024941474199
iteration 2, loss = 0.007412677630782127
iteration 3, loss = 0.005329837091267109
iteration 4, loss = 0.005086059216409922
iteration 5, loss = 0.004408962558954954
iteration 6, loss = 0.004312215838581324
iteration 7, loss = 0.00458244513720274
iteration 8, loss = 0.005977077409625053
iteration 9, loss = 0.004760007839649916
iteration 10, loss = 0.004652527626603842
iteration 11, loss = 0.004317556507885456
iteration 12, loss = 0.004434391390532255
iteration 13, loss = 0.004517837893217802
iteration 14, loss = 0.0060190134681761265
iteration 15, loss = 0.0045692333951592445
iteration 16, loss = 0.004853487480431795
iteration 17, loss = 0.004465462639927864
iteration 18, loss = 0.00448255892843008
iteration 19, loss = 0.0042582787573337555
iteration 20, loss = 0.004037964623421431
iteration 21, loss = 0.004498373717069626
iteration 22, loss = 0.004589412361383438
iteration 23, loss = 0.004138177260756493
iteration 24, loss = 0.004477049224078655
iteration 25, loss = 0.006069857627153397
iteration 26, loss = 0.004063724540174007
iteration 27, loss = 0.004248668905347586
iteration 28, loss = 0.004293561447411776
iteration 29, loss = 0.004349721595644951
iteration 30, loss = 0.0048914640210568905
iteration 31, loss = 0.0041898502968251705
iteration 32, loss = 0.0071020470932126045
iteration 33, loss = 0.0043351841159164906
iteration 34, loss = 0.004613486118614674
iteration 35, loss = 0.004654118791222572
iteration 36, loss = 0.004754112102091312
iteration 37, loss = 0.004613232333213091
iteration 38, loss = 0.004556464496999979
iteration 39, loss = 0.003919760696589947
iteration 40, loss = 0.004361510742455721
iteration 41, loss = 0.004271491430699825
iteration 42, loss = 0.0045612892135977745
iteration 43, loss = 0.004792883992195129
iteration 44, loss = 0.004272650461643934
iteration 45, loss = 0.003874145681038499
iteration 46, loss = 0.004151026718318462
iteration 47, loss = 0.004877969156950712
iteration 48, loss = 0.004002826288342476
iteration 49, loss = 0.004490917548537254
iteration 50, loss = 0.006753204856067896
iteration 51, loss = 0.004390332382172346
iteration 52, loss = 0.004296349361538887
iteration 53, loss = 0.0037882572505623102
iteration 54, loss = 0.004975216928869486
iteration 55, loss = 0.003949714824557304
iteration 56, loss = 0.0043459562584757805
iteration 57, loss = 0.0040320646949112415
iteration 58, loss = 0.00424913177266717
iteration 59, loss = 0.004894931335002184
iteration 60, loss = 0.004887386225163937
iteration 61, loss = 0.006938799284398556
iteration 62, loss = 0.004134468734264374
iteration 63, loss = 0.006739421747624874
iteration 64, loss = 0.004939070902764797
iteration 65, loss = 0.004411338362842798
iteration 66, loss = 0.0056753684766590595
iteration 67, loss = 0.004209497943520546
iteration 68, loss = 0.004565451294183731
iteration 69, loss = 0.004345827270299196
iteration 70, loss = 0.004064660519361496
iteration 71, loss = 0.004182608798146248
iteration 72, loss = 0.004773181397467852
iteration 73, loss = 0.004665563814342022
iteration 74, loss = 0.004404596984386444
iteration 75, loss = 0.004806664772331715
iteration 76, loss = 0.004215381108224392
iteration 77, loss = 0.004985495936125517
iteration 78, loss = 0.004495379514992237
iteration 79, loss = 0.005048186983913183
iteration 80, loss = 0.0076940772123634815
iteration 81, loss = 0.005496248137205839
iteration 82, loss = 0.006999140605330467
iteration 83, loss = 0.005852406844496727
iteration 84, loss = 0.00598611356690526
iteration 85, loss = 0.004232226870954037
iteration 86, loss = 0.00444797333329916
iteration 87, loss = 0.004278750624507666
iteration 88, loss = 0.004313714802265167
iteration 89, loss = 0.004263782408088446
iteration 90, loss = 0.004911117721349001
iteration 91, loss = 0.004286813084036112
iteration 92, loss = 0.004125689156353474
iteration 93, loss = 0.004282533656805754
iteration 94, loss = 0.0055948966182768345
iteration 95, loss = 0.004000140819698572
iteration 96, loss = 0.004410522989928722
iteration 97, loss = 0.004213052336126566
iteration 98, loss = 0.003993760794401169
iteration 99, loss = 0.0062120757065713406
iteration 100, loss = 0.003842440200969577
iteration 101, loss = 0.0046629165299236774
iteration 102, loss = 0.004431229550391436
iteration 103, loss = 0.004081916995346546
iteration 104, loss = 0.004666205495595932
iteration 105, loss = 0.0045011043548583984
iteration 106, loss = 0.004043567460030317
iteration 107, loss = 0.005080329719930887
iteration 108, loss = 0.005625355057418346
iteration 109, loss = 0.004290607292205095
iteration 110, loss = 0.006451568100601435
iteration 111, loss = 0.004239343106746674
iteration 112, loss = 0.005347616970539093
iteration 113, loss = 0.004201488569378853
iteration 114, loss = 0.004486315883696079
iteration 115, loss = 0.0039940630085766315
iteration 116, loss = 0.005814125761389732
iteration 117, loss = 0.004695785231888294
iteration 118, loss = 0.005687248893082142
iteration 119, loss = 0.004088954068720341
iteration 120, loss = 0.004857379011809826
iteration 121, loss = 0.007137691136449575
iteration 122, loss = 0.005821412894874811
iteration 123, loss = 0.004953802563250065
iteration 124, loss = 0.006930433213710785
iteration 125, loss = 0.005553440190851688
iteration 126, loss = 0.004386513959616423
iteration 127, loss = 0.004548848606646061
iteration 128, loss = 0.003766019130125642
iteration 129, loss = 0.004214114509522915
iteration 130, loss = 0.0059197465889155865
iteration 131, loss = 0.004714848008006811
iteration 132, loss = 0.004298689775168896
iteration 133, loss = 0.004405970219522715
iteration 134, loss = 0.004708362743258476
iteration 135, loss = 0.004356971941888332
iteration 136, loss = 0.00706363283097744
iteration 137, loss = 0.004136609844863415
iteration 138, loss = 0.004328472074121237
iteration 139, loss = 0.004779456648975611
iteration 140, loss = 0.004740091506391764
iteration 141, loss = 0.004290148615837097
iteration 142, loss = 0.004741654731333256
iteration 143, loss = 0.004674711264669895
iteration 144, loss = 0.004239227157086134
iteration 145, loss = 0.004693890921771526
iteration 146, loss = 0.004985674750059843
iteration 147, loss = 0.004488853737711906
iteration 148, loss = 0.0035208577755838633
iteration 149, loss = 0.005448418203741312
iteration 150, loss = 0.0046261684037745
iteration 151, loss = 0.004769491963088512
iteration 152, loss = 0.004176822025328875
iteration 153, loss = 0.0044435844756662846
iteration 154, loss = 0.004118042998015881
iteration 155, loss = 0.005485223140567541
iteration 156, loss = 0.0057650282979011536
iteration 157, loss = 0.004605252295732498
iteration 158, loss = 0.004207164514809847
iteration 159, loss = 0.005589466076344252
iteration 160, loss = 0.004232051316648722
iteration 161, loss = 0.004341482650488615
iteration 162, loss = 0.003976683598011732
iteration 163, loss = 0.004684945568442345
iteration 164, loss = 0.004693875089287758
iteration 165, loss = 0.0041795168071985245
iteration 166, loss = 0.004321354907006025
iteration 167, loss = 0.006137790624052286
iteration 168, loss = 0.004502269439399242
iteration 169, loss = 0.004160931333899498
iteration 170, loss = 0.005430435761809349
iteration 171, loss = 0.004291503224521875
iteration 172, loss = 0.0042260331101715565
iteration 173, loss = 0.0045058466494083405
iteration 174, loss = 0.004619661718606949
iteration 175, loss = 0.004153711721301079
iteration 176, loss = 0.007160279434174299
iteration 177, loss = 0.003852838184684515
iteration 178, loss = 0.004909379407763481
iteration 179, loss = 0.003908790647983551
iteration 180, loss = 0.004232327453792095
iteration 181, loss = 0.004450016189366579
iteration 182, loss = 0.004103050567209721
iteration 183, loss = 0.00552270608022809
iteration 184, loss = 0.004714412614703178
iteration 185, loss = 0.00433072866871953
iteration 186, loss = 0.00910595990717411
iteration 187, loss = 0.006475791800767183
iteration 188, loss = 0.006026498042047024
iteration 189, loss = 0.0038370934780687094
iteration 190, loss = 0.004328619223088026
iteration 191, loss = 0.004323818255215883
iteration 192, loss = 0.004304324276745319
iteration 193, loss = 0.004086056258529425
iteration 194, loss = 0.00401263078674674
iteration 195, loss = 0.004508143290877342
iteration 196, loss = 0.006269859150052071
iteration 197, loss = 0.005828004330396652
iteration 198, loss = 0.004332226235419512
iteration 199, loss = 0.0042897434905171394
iteration 200, loss = 0.004127813503146172
iteration 201, loss = 0.003927558194845915
iteration 202, loss = 0.0038534642662853003
iteration 203, loss = 0.004875263199210167
iteration 204, loss = 0.004055540543049574
iteration 205, loss = 0.0044678146950900555
iteration 206, loss = 0.004082499071955681
iteration 207, loss = 0.00492954533547163
iteration 208, loss = 0.004475714173167944
iteration 209, loss = 0.0043243709951639175
iteration 210, loss = 0.005355886649340391
iteration 211, loss = 0.004115132614970207
iteration 212, loss = 0.005799638107419014
iteration 213, loss = 0.004610383417457342
iteration 214, loss = 0.0042439475655555725
iteration 215, loss = 0.004200665280222893
iteration 216, loss = 0.005091818980872631
iteration 217, loss = 0.0040513621643185616
iteration 218, loss = 0.004004572983831167
iteration 219, loss = 0.005794203374534845
iteration 220, loss = 0.003919348120689392
iteration 221, loss = 0.0070379930548369884
iteration 222, loss = 0.004109169822186232
iteration 223, loss = 0.003901517251506448
iteration 224, loss = 0.005988603457808495
iteration 225, loss = 0.0038902321830391884
iteration 226, loss = 0.006231605540961027
iteration 227, loss = 0.006903822533786297
iteration 228, loss = 0.003909459803253412
iteration 229, loss = 0.005722342059016228
iteration 230, loss = 0.004161533433943987
iteration 231, loss = 0.004503002855926752
iteration 232, loss = 0.008927321061491966
iteration 233, loss = 0.0040224166586995125
iteration 234, loss = 0.005311343353241682
iteration 235, loss = 0.0062370686791837215
iteration 236, loss = 0.004359839484095573
iteration 237, loss = 0.0059227366000413895
iteration 238, loss = 0.00427272729575634
iteration 239, loss = 0.00453166663646698
iteration 240, loss = 0.006146285217255354
iteration 241, loss = 0.004440666176378727
iteration 242, loss = 0.0074488394893705845
iteration 243, loss = 0.007052191998809576
iteration 244, loss = 0.00607545580714941
iteration 245, loss = 0.004146140068769455
iteration 246, loss = 0.004312986508011818
iteration 247, loss = 0.004359719809144735
iteration 248, loss = 0.0044385576620697975
iteration 249, loss = 0.00409374525770545
iteration 250, loss = 0.0041372706182301044
iteration 251, loss = 0.00460247416049242
iteration 252, loss = 0.005412101745605469
iteration 253, loss = 0.0041915662586688995
iteration 254, loss = 0.005699905566871166
iteration 255, loss = 0.004372437950223684
iteration 256, loss = 0.004927118308842182
iteration 257, loss = 0.0058702570386230946
iteration 258, loss = 0.0038707188796252012
iteration 259, loss = 0.003975264262408018
iteration 260, loss = 0.004241739399731159
iteration 261, loss = 0.0043549370020627975
iteration 262, loss = 0.00393288629129529
iteration 263, loss = 0.004063707776367664
iteration 264, loss = 0.005799968261271715
iteration 265, loss = 0.004050166811794043
iteration 266, loss = 0.005161514040082693
iteration 267, loss = 0.004079324193298817
iteration 268, loss = 0.00437935022637248
iteration 269, loss = 0.005431652069091797
iteration 270, loss = 0.004049249459058046
iteration 271, loss = 0.003739435924217105
iteration 272, loss = 0.0038990930188447237
iteration 273, loss = 0.005970394238829613
iteration 274, loss = 0.004708143882453442
iteration 275, loss = 0.004388365428894758
iteration 276, loss = 0.0043757278472185135
iteration 277, loss = 0.00502549996599555
iteration 278, loss = 0.0039136833511292934
iteration 279, loss = 0.004225947428494692
iteration 280, loss = 0.004422521218657494
iteration 281, loss = 0.004831236787140369
iteration 282, loss = 0.004372140858322382
iteration 283, loss = 0.00585701409727335
iteration 284, loss = 0.005801748018711805
iteration 285, loss = 0.004719040356576443
iteration 286, loss = 0.007495537865906954
iteration 287, loss = 0.004084029234945774
iteration 288, loss = 0.004549243487417698
iteration 289, loss = 0.006963024381548166
iteration 290, loss = 0.003980398643761873
iteration 291, loss = 0.004242203198373318
iteration 292, loss = 0.004022904206067324
iteration 293, loss = 0.004653832875192165
iteration 294, loss = 0.004669065121561289
iteration 295, loss = 0.0036639038007706404
iteration 296, loss = 0.004235970787703991
iteration 297, loss = 0.005511919967830181
iteration 298, loss = 0.004284227266907692
iteration 299, loss = 0.008037740364670753
iteration 300, loss = 0.004133417271077633
iteration 1, loss = 0.003906616475433111
iteration 2, loss = 0.004736143164336681
iteration 3, loss = 0.009327472187578678
iteration 4, loss = 0.004145908169448376
iteration 5, loss = 0.004586846102029085
iteration 6, loss = 0.00622351560741663
iteration 7, loss = 0.004550857935100794
iteration 8, loss = 0.004340204875916243
iteration 9, loss = 0.0044620889239013195
iteration 10, loss = 0.006307611241936684
iteration 11, loss = 0.004479835741221905
iteration 12, loss = 0.004057344514876604
iteration 13, loss = 0.004377039615064859
iteration 14, loss = 0.004019640386104584
iteration 15, loss = 0.0037187556736171246
iteration 16, loss = 0.00438703503459692
iteration 17, loss = 0.004796912893652916
iteration 18, loss = 0.004512923303991556
iteration 19, loss = 0.0045461333356797695
iteration 20, loss = 0.007331636268645525
iteration 21, loss = 0.004150180146098137
iteration 22, loss = 0.0039701806381344795
iteration 23, loss = 0.0041104573756456375
iteration 24, loss = 0.004129890818148851
iteration 25, loss = 0.00454425485804677
iteration 26, loss = 0.004139572381973267
iteration 27, loss = 0.005682766903191805
iteration 28, loss = 0.004662047140300274
iteration 29, loss = 0.004222906660288572
iteration 30, loss = 0.005786636378616095
iteration 31, loss = 0.004704606253653765
iteration 32, loss = 0.004171610809862614
iteration 33, loss = 0.003978278953582048
iteration 34, loss = 0.004757835064083338
iteration 35, loss = 0.004581735003739595
iteration 36, loss = 0.004460648633539677
iteration 37, loss = 0.004121916368603706
iteration 38, loss = 0.0045503587462008
iteration 39, loss = 0.004228916019201279
iteration 40, loss = 0.007269412279129028
iteration 41, loss = 0.005360020790249109
iteration 42, loss = 0.0046434830874204636
iteration 43, loss = 0.0058508520014584064
iteration 44, loss = 0.004194366745650768
iteration 45, loss = 0.004018969833850861
iteration 46, loss = 0.004603774286806583
iteration 47, loss = 0.0046618543565273285
iteration 48, loss = 0.0047601256519556046
iteration 49, loss = 0.00481100520119071
iteration 50, loss = 0.00422644754871726
iteration 51, loss = 0.004544123075902462
iteration 52, loss = 0.004147280007600784
iteration 53, loss = 0.004207793623209
iteration 54, loss = 0.004268554039299488
iteration 55, loss = 0.004344277549535036
iteration 56, loss = 0.004621708299964666
iteration 57, loss = 0.005585310515016317
iteration 58, loss = 0.004574786871671677
iteration 59, loss = 0.004183597397059202
iteration 60, loss = 0.004705294035375118
iteration 61, loss = 0.004044875502586365
iteration 62, loss = 0.00639901589602232
iteration 63, loss = 0.004431543871760368
iteration 64, loss = 0.006105746608227491
iteration 65, loss = 0.004174765199422836
iteration 66, loss = 0.004063530825078487
iteration 67, loss = 0.005853303242474794
iteration 68, loss = 0.004399351309984922
iteration 69, loss = 0.0042705475352704525
iteration 70, loss = 0.006393950432538986
iteration 71, loss = 0.004177875351160765
iteration 72, loss = 0.004472330678254366
iteration 73, loss = 0.004280152730643749
iteration 74, loss = 0.0043687704019248486
iteration 75, loss = 0.00640362873673439
iteration 76, loss = 0.004649860318750143
iteration 77, loss = 0.004506648518145084
iteration 78, loss = 0.004346321802586317
iteration 79, loss = 0.005534233991056681
iteration 80, loss = 0.003858349286019802
iteration 81, loss = 0.006211477797478437
iteration 82, loss = 0.00448695570230484
iteration 83, loss = 0.007375020533800125
iteration 84, loss = 0.005263835657387972
iteration 85, loss = 0.0047838869504630566
iteration 86, loss = 0.00412459671497345
iteration 87, loss = 0.004433564376085997
iteration 88, loss = 0.0047358861193060875
iteration 89, loss = 0.005213953088968992
iteration 90, loss = 0.005593669600784779
iteration 91, loss = 0.00391346076503396
iteration 92, loss = 0.0041621970012784
iteration 93, loss = 0.0042881048284471035
iteration 94, loss = 0.00529338326305151
iteration 95, loss = 0.004345631692558527
iteration 96, loss = 0.004200474359095097
iteration 97, loss = 0.004751808475703001
iteration 98, loss = 0.0061845071613788605
iteration 99, loss = 0.005732580088078976
iteration 100, loss = 0.004413439892232418
iteration 101, loss = 0.0069719827733933926
iteration 102, loss = 0.004221238661557436
iteration 103, loss = 0.0063120522536337376
iteration 104, loss = 0.004840577021241188
iteration 105, loss = 0.004096099641174078
iteration 106, loss = 0.008624282665550709
iteration 107, loss = 0.0057304962538182735
iteration 108, loss = 0.004091717302799225
iteration 109, loss = 0.004286215174943209
iteration 110, loss = 0.004497147165238857
iteration 111, loss = 0.004084674641489983
iteration 112, loss = 0.0047647845931351185
iteration 113, loss = 0.0038000342901796103
iteration 114, loss = 0.004867827519774437
iteration 115, loss = 0.004411548376083374
iteration 116, loss = 0.004181009717285633
iteration 117, loss = 0.0041656517423689365
iteration 118, loss = 0.004482218995690346
iteration 119, loss = 0.004697796422988176
iteration 120, loss = 0.004373809322714806
iteration 121, loss = 0.005721020977944136
iteration 122, loss = 0.004183135461062193
iteration 123, loss = 0.0041653155349195
iteration 124, loss = 0.004388442728668451
iteration 125, loss = 0.004133778624236584
iteration 126, loss = 0.005892624147236347
iteration 127, loss = 0.004216721281409264
iteration 128, loss = 0.004640358965843916
iteration 129, loss = 0.005751293618232012
iteration 130, loss = 0.004269799217581749
iteration 131, loss = 0.0041059041395783424
iteration 132, loss = 0.00463128974661231
iteration 133, loss = 0.004020937718451023
iteration 134, loss = 0.004262831062078476
iteration 135, loss = 0.005888082552701235
iteration 136, loss = 0.004527281038463116
iteration 137, loss = 0.004054663702845573
iteration 138, loss = 0.004045106004923582
iteration 139, loss = 0.004326680675148964
iteration 140, loss = 0.003971673548221588
iteration 141, loss = 0.004047703929245472
iteration 142, loss = 0.004508705344051123
iteration 143, loss = 0.004394516348838806
iteration 144, loss = 0.0047598546370863914
iteration 145, loss = 0.004481717012822628
iteration 146, loss = 0.004474462941288948
iteration 147, loss = 0.005223048850893974
iteration 148, loss = 0.005506516434252262
iteration 149, loss = 0.0056061940267682076
iteration 150, loss = 0.005555315408855677
iteration 151, loss = 0.007156736683100462
iteration 152, loss = 0.004656663630157709
iteration 153, loss = 0.004279502667486668
iteration 154, loss = 0.0043623787350952625
iteration 155, loss = 0.0056061213836073875
iteration 156, loss = 0.0042106108739972115
iteration 157, loss = 0.006239532493054867
iteration 158, loss = 0.004584060050547123
iteration 159, loss = 0.004560886882245541
iteration 160, loss = 0.004751540720462799
iteration 161, loss = 0.0037835778202861547
iteration 162, loss = 0.006849336437880993
iteration 163, loss = 0.004373443312942982
iteration 164, loss = 0.004007730633020401
iteration 165, loss = 0.004360661376267672
iteration 166, loss = 0.0046566869132220745
iteration 167, loss = 0.004831432830542326
iteration 168, loss = 0.005197974853217602
iteration 169, loss = 0.005323923192918301
iteration 170, loss = 0.004823352210223675
iteration 171, loss = 0.004129350651055574
iteration 172, loss = 0.003933672793209553
iteration 173, loss = 0.004414301365613937
iteration 174, loss = 0.004523049108684063
iteration 175, loss = 0.003990058787167072
iteration 176, loss = 0.007527546491473913
iteration 177, loss = 0.006149631459265947
iteration 178, loss = 0.004388706758618355
iteration 179, loss = 0.006701034028083086
iteration 180, loss = 0.004394665360450745
iteration 181, loss = 0.0046625216491520405
iteration 182, loss = 0.004105827305465937
iteration 183, loss = 0.0060456423088908195
iteration 184, loss = 0.004014526028186083
iteration 185, loss = 0.004001102410256863
iteration 186, loss = 0.004179392941296101
iteration 187, loss = 0.0042504435405135155
iteration 188, loss = 0.004017429426312447
iteration 189, loss = 0.003911846783012152
iteration 190, loss = 0.004053435754030943
iteration 191, loss = 0.004103030078113079
iteration 192, loss = 0.005144172348082066
iteration 193, loss = 0.004220406990498304
iteration 194, loss = 0.00482310401275754
iteration 195, loss = 0.004596279934048653
iteration 196, loss = 0.004132053814828396
iteration 197, loss = 0.004193402361124754
iteration 198, loss = 0.003951354883611202
iteration 199, loss = 0.004480651579797268
iteration 200, loss = 0.004637734964489937
iteration 201, loss = 0.004140782635658979
iteration 202, loss = 0.005067103076726198
iteration 203, loss = 0.005605851300060749
iteration 204, loss = 0.004484965465962887
iteration 205, loss = 0.005017206072807312
iteration 206, loss = 0.004947888199239969
iteration 207, loss = 0.004354424774646759
iteration 208, loss = 0.004495719447731972
iteration 209, loss = 0.004344888962805271
iteration 210, loss = 0.005618393886834383
iteration 211, loss = 0.004771013744175434
iteration 212, loss = 0.004172598011791706
iteration 213, loss = 0.004460816737264395
iteration 214, loss = 0.005442487541586161
iteration 215, loss = 0.004118553828448057
iteration 216, loss = 0.003948093391954899
iteration 217, loss = 0.004393050912767649
iteration 218, loss = 0.00438633281737566
iteration 219, loss = 0.00428715068846941
iteration 220, loss = 0.004448099993169308
iteration 221, loss = 0.006696387194097042
iteration 222, loss = 0.004080813843756914
iteration 223, loss = 0.0040624248795211315
iteration 224, loss = 0.00468034902587533
iteration 225, loss = 0.004474546294659376
iteration 226, loss = 0.004066464025527239
iteration 227, loss = 0.003979962784796953
iteration 228, loss = 0.004266269039362669
iteration 229, loss = 0.005183308385312557
iteration 230, loss = 0.005076555069535971
iteration 231, loss = 0.004279949236661196
iteration 232, loss = 0.004712691996246576
iteration 233, loss = 0.005171301309019327
iteration 234, loss = 0.004630521405488253
iteration 235, loss = 0.004040082450956106
iteration 236, loss = 0.004484503995627165
iteration 237, loss = 0.004128010477870703
iteration 238, loss = 0.005624836776405573
iteration 239, loss = 0.0045905951410532
iteration 240, loss = 0.004311329685151577
iteration 241, loss = 0.004299233201891184
iteration 242, loss = 0.0049094753339886665
iteration 243, loss = 0.00878162682056427
iteration 244, loss = 0.004466495011001825
iteration 245, loss = 0.004466980695724487
iteration 246, loss = 0.004178155679255724
iteration 247, loss = 0.004453226458281279
iteration 248, loss = 0.0042107547633349895
iteration 249, loss = 0.004479669965803623
iteration 250, loss = 0.0038098711520433426
iteration 251, loss = 0.005469413939863443
iteration 252, loss = 0.004202904645353556
iteration 253, loss = 0.005550900939851999
iteration 254, loss = 0.005808796733617783
iteration 255, loss = 0.004016054794192314
iteration 256, loss = 0.005932245869189501
iteration 257, loss = 0.004043567925691605
iteration 258, loss = 0.004796560853719711
iteration 259, loss = 0.0053422232158482075
iteration 260, loss = 0.0062073105946183205
iteration 261, loss = 0.004594332072883844
iteration 262, loss = 0.003998005762696266
iteration 263, loss = 0.005736821331083775
iteration 264, loss = 0.007580460049211979
iteration 265, loss = 0.006907212547957897
iteration 266, loss = 0.005562057252973318
iteration 267, loss = 0.004624124616384506
iteration 268, loss = 0.004095840733498335
iteration 269, loss = 0.004023641813546419
iteration 270, loss = 0.006320134270936251
iteration 271, loss = 0.004160077776759863
iteration 272, loss = 0.00402152119204402
iteration 273, loss = 0.004398643504828215
iteration 274, loss = 0.003926021512597799
iteration 275, loss = 0.004719709511846304
iteration 276, loss = 0.0058707986027002335
iteration 277, loss = 0.0059603238478302956
iteration 278, loss = 0.008943866938352585
iteration 279, loss = 0.0037917853333055973
iteration 280, loss = 0.004503978416323662
iteration 281, loss = 0.004123717080801725
iteration 282, loss = 0.004711252171546221
iteration 283, loss = 0.00485866516828537
iteration 284, loss = 0.004008121322840452
iteration 285, loss = 0.004485412035137415
iteration 286, loss = 0.005999939516186714
iteration 287, loss = 0.004588484764099121
iteration 288, loss = 0.004657360725104809
iteration 289, loss = 0.0036773690953850746
iteration 290, loss = 0.00432405062019825
iteration 291, loss = 0.003965232986956835
iteration 292, loss = 0.004486413206905127
iteration 293, loss = 0.006026038434356451
iteration 294, loss = 0.005799474194645882
iteration 295, loss = 0.004336914047598839
iteration 296, loss = 0.004204940982162952
iteration 297, loss = 0.008726093918085098
iteration 298, loss = 0.0041670226491987705
iteration 299, loss = 0.003955619875341654
iteration 300, loss = 0.003990178927779198
iteration 1, loss = 0.004397244658321142
iteration 2, loss = 0.004634808748960495
iteration 3, loss = 0.005492024123668671
iteration 4, loss = 0.0044877976179122925
iteration 5, loss = 0.004787466488778591
iteration 6, loss = 0.004382140468806028
iteration 7, loss = 0.005079800728708506
iteration 8, loss = 0.006665111985057592
iteration 9, loss = 0.0047179763205349445
iteration 10, loss = 0.004128681030124426
iteration 11, loss = 0.004386078100651503
iteration 12, loss = 0.004787554498761892
iteration 13, loss = 0.004054481629282236
iteration 14, loss = 0.004086453467607498
iteration 15, loss = 0.004088160581886768
iteration 16, loss = 0.004412748385220766
iteration 17, loss = 0.004434723872691393
iteration 18, loss = 0.006531343329697847
iteration 19, loss = 0.007081582676619291
iteration 20, loss = 0.004214242100715637
iteration 21, loss = 0.005987351294606924
iteration 22, loss = 0.004312906414270401
iteration 23, loss = 0.008292554877698421
iteration 24, loss = 0.004169427324086428
iteration 25, loss = 0.0043111154809594154
iteration 26, loss = 0.003627772442996502
iteration 27, loss = 0.005878774914890528
iteration 28, loss = 0.004419129807502031
iteration 29, loss = 0.0043394677340984344
iteration 30, loss = 0.006399588193744421
iteration 31, loss = 0.005809349473565817
iteration 32, loss = 0.004496498964726925
iteration 33, loss = 0.006088078022003174
iteration 34, loss = 0.005047516897320747
iteration 35, loss = 0.004320458974689245
iteration 36, loss = 0.004600672051310539
iteration 37, loss = 0.004267896991223097
iteration 38, loss = 0.004645877983421087
iteration 39, loss = 0.004145475570112467
iteration 40, loss = 0.0044091250747442245
iteration 41, loss = 0.0050130682066082954
iteration 42, loss = 0.0042297374457120895
iteration 43, loss = 0.0049548763781785965
iteration 44, loss = 0.004965079482644796
iteration 45, loss = 0.004507174715399742
iteration 46, loss = 0.0055355168879032135
iteration 47, loss = 0.003904984099790454
iteration 48, loss = 0.0053748637437820435
iteration 49, loss = 0.004523067269474268
iteration 50, loss = 0.005168239586055279
iteration 51, loss = 0.0037170061841607094
iteration 52, loss = 0.005973419640213251
iteration 53, loss = 0.005373157095164061
iteration 54, loss = 0.00424087792634964
iteration 55, loss = 0.00443041417747736
iteration 56, loss = 0.0047568329609930515
iteration 57, loss = 0.004215980879962444
iteration 58, loss = 0.004432227928191423
iteration 59, loss = 0.004309810232371092
iteration 60, loss = 0.004074577242136002
iteration 61, loss = 0.005314030684530735
iteration 62, loss = 0.003937681671231985
iteration 63, loss = 0.004124598577618599
iteration 64, loss = 0.005861327983438969
iteration 65, loss = 0.003922999370843172
iteration 66, loss = 0.004329555667936802
iteration 67, loss = 0.005951486993581057
iteration 68, loss = 0.005581028759479523
iteration 69, loss = 0.004557262174785137
iteration 70, loss = 0.004329541232436895
iteration 71, loss = 0.004435211885720491
iteration 72, loss = 0.0043930779211223125
iteration 73, loss = 0.004370528738945723
iteration 74, loss = 0.005415629595518112
iteration 75, loss = 0.005244788248091936
iteration 76, loss = 0.008598395623266697
iteration 77, loss = 0.004271588288247585
iteration 78, loss = 0.004429216030985117
iteration 79, loss = 0.004223782103508711
iteration 80, loss = 0.004428759682923555
iteration 81, loss = 0.004167692735791206
iteration 82, loss = 0.004870367236435413
iteration 83, loss = 0.004536991938948631
iteration 84, loss = 0.0040420470759272575
iteration 85, loss = 0.00418368773534894
iteration 86, loss = 0.004850910510867834
iteration 87, loss = 0.004586491733789444
iteration 88, loss = 0.006581945810467005
iteration 89, loss = 0.004394518211483955
iteration 90, loss = 0.00467090355232358
iteration 91, loss = 0.004745408892631531
iteration 92, loss = 0.004718603566288948
iteration 93, loss = 0.004206841345876455
iteration 94, loss = 0.004154557827860117
iteration 95, loss = 0.005542335566133261
iteration 96, loss = 0.004892880562692881
iteration 97, loss = 0.0038733549881726503
iteration 98, loss = 0.004372742492705584
iteration 99, loss = 0.004189088474959135
iteration 100, loss = 0.0042794751934707165
iteration 101, loss = 0.0052757179364562035
iteration 102, loss = 0.004585629794746637
iteration 103, loss = 0.0040032328106462955
iteration 104, loss = 0.0050221034325659275
iteration 105, loss = 0.004061120096594095
iteration 106, loss = 0.0077071720734238625
iteration 107, loss = 0.003796321339905262
iteration 108, loss = 0.005838998593389988
iteration 109, loss = 0.004188337363302708
iteration 110, loss = 0.004066416993737221
iteration 111, loss = 0.006797828711569309
iteration 112, loss = 0.004222787916660309
iteration 113, loss = 0.004256495274603367
iteration 114, loss = 0.0043277195654809475
iteration 115, loss = 0.004245718475431204
iteration 116, loss = 0.00485484441742301
iteration 117, loss = 0.00431824941188097
iteration 118, loss = 0.0042458632960915565
iteration 119, loss = 0.004312017001211643
iteration 120, loss = 0.005771607626229525
iteration 121, loss = 0.0041280705481767654
iteration 122, loss = 0.005467735696583986
iteration 123, loss = 0.005334710236638784
iteration 124, loss = 0.004373003728687763
iteration 125, loss = 0.006013199221342802
iteration 126, loss = 0.004073456861078739
iteration 127, loss = 0.004898148588836193
iteration 128, loss = 0.004445819184184074
iteration 129, loss = 0.004458884708583355
iteration 130, loss = 0.005447091534733772
iteration 131, loss = 0.007163006346672773
iteration 132, loss = 0.0039410968311131
iteration 133, loss = 0.004314071498811245
iteration 134, loss = 0.004726068116724491
iteration 135, loss = 0.0039998609572649
iteration 136, loss = 0.004668328911066055
iteration 137, loss = 0.004663391970098019
iteration 138, loss = 0.00454309768974781
iteration 139, loss = 0.004109627567231655
iteration 140, loss = 0.006109294947236776
iteration 141, loss = 0.004126558545976877
iteration 142, loss = 0.004048185423016548
iteration 143, loss = 0.0041170851327478886
iteration 144, loss = 0.005151727236807346
iteration 145, loss = 0.007701574359089136
iteration 146, loss = 0.007712413091212511
iteration 147, loss = 0.006030330900102854
iteration 148, loss = 0.006131348200142384
iteration 149, loss = 0.005518678575754166
iteration 150, loss = 0.007093340158462524
iteration 151, loss = 0.004656338132917881
iteration 152, loss = 0.003939061425626278
iteration 153, loss = 0.004258446395397186
iteration 154, loss = 0.004691064823418856
iteration 155, loss = 0.004270944278687239
iteration 156, loss = 0.004394147545099258
iteration 157, loss = 0.004422683734446764
iteration 158, loss = 0.005964086391031742
iteration 159, loss = 0.004429006949067116
iteration 160, loss = 0.004215891007333994
iteration 161, loss = 0.003951852209866047
iteration 162, loss = 0.007115586660802364
iteration 163, loss = 0.004285599570721388
iteration 164, loss = 0.004054286517202854
iteration 165, loss = 0.004005087539553642
iteration 166, loss = 0.004522486589848995
iteration 167, loss = 0.004033959936350584
iteration 168, loss = 0.004385572858154774
iteration 169, loss = 0.004609564319252968
iteration 170, loss = 0.0058816540986299515
iteration 171, loss = 0.003732013516128063
iteration 172, loss = 0.004131822846829891
iteration 173, loss = 0.00474388524889946
iteration 174, loss = 0.004249759949743748
iteration 175, loss = 0.005818876437842846
iteration 176, loss = 0.0045340475626289845
iteration 177, loss = 0.006630279123783112
iteration 178, loss = 0.004027584567666054
iteration 179, loss = 0.004122518002986908
iteration 180, loss = 0.0077005550265312195
iteration 181, loss = 0.004402344115078449
iteration 182, loss = 0.0047046588733792305
iteration 183, loss = 0.0047832513228058815
iteration 184, loss = 0.004643199499696493
iteration 185, loss = 0.00439139548689127
iteration 186, loss = 0.00405840715393424
iteration 187, loss = 0.004249767400324345
iteration 188, loss = 0.004016813822090626
iteration 189, loss = 0.003942693583667278
iteration 190, loss = 0.004414287395775318
iteration 191, loss = 0.004889918491244316
iteration 192, loss = 0.004415395203977823
iteration 193, loss = 0.0041632805950939655
iteration 194, loss = 0.0042640739120543
iteration 195, loss = 0.005726204719394445
iteration 196, loss = 0.004513321910053492
iteration 197, loss = 0.005241953302174807
iteration 198, loss = 0.007410096470266581
iteration 199, loss = 0.004357733763754368
iteration 200, loss = 0.004144363105297089
iteration 201, loss = 0.007441001944243908
iteration 202, loss = 0.004026099573820829
iteration 203, loss = 0.004061974585056305
iteration 204, loss = 0.0042161657474935055
iteration 205, loss = 0.004151859786361456
iteration 206, loss = 0.004340021405369043
iteration 207, loss = 0.004147297237068415
iteration 208, loss = 0.0064658657647669315
iteration 209, loss = 0.00429224967956543
iteration 210, loss = 0.004685086198151112
iteration 211, loss = 0.007212680298835039
iteration 212, loss = 0.004293352365493774
iteration 213, loss = 0.004096595104783773
iteration 214, loss = 0.004293183796107769
iteration 215, loss = 0.005769400391727686
iteration 216, loss = 0.004190107807517052
iteration 217, loss = 0.0051189325749874115
iteration 218, loss = 0.004568067844957113
iteration 219, loss = 0.0037983062211424112
iteration 220, loss = 0.0047363536432385445
iteration 221, loss = 0.004134790040552616
iteration 222, loss = 0.0043930658139288425
iteration 223, loss = 0.004197219852358103
iteration 224, loss = 0.004086259752511978
iteration 225, loss = 0.006782245822250843
iteration 226, loss = 0.004080804996192455
iteration 227, loss = 0.004003820940852165
iteration 228, loss = 0.003890051506459713
iteration 229, loss = 0.004262404982000589
iteration 230, loss = 0.005552161950618029
iteration 231, loss = 0.004193782806396484
iteration 232, loss = 0.005711881443858147
iteration 233, loss = 0.00426054559648037
iteration 234, loss = 0.003927447367459536
iteration 235, loss = 0.00426456006243825
iteration 236, loss = 0.004638121463358402
iteration 237, loss = 0.004555678926408291
iteration 238, loss = 0.004515849053859711
iteration 239, loss = 0.0040771933272480965
iteration 240, loss = 0.004640115890651941
iteration 241, loss = 0.004386290907859802
iteration 242, loss = 0.006738229189068079
iteration 243, loss = 0.004795524291694164
iteration 244, loss = 0.0042574587278068066
iteration 245, loss = 0.004223714582622051
iteration 246, loss = 0.004227907862514257
iteration 247, loss = 0.004210717044770718
iteration 248, loss = 0.004859955050051212
iteration 249, loss = 0.004471236374229193
iteration 250, loss = 0.004670827649533749
iteration 251, loss = 0.004467733670026064
iteration 252, loss = 0.004527267999947071
iteration 253, loss = 0.004451674409210682
iteration 254, loss = 0.0041081844829022884
iteration 255, loss = 0.006263277493417263
iteration 256, loss = 0.004130041226744652
iteration 257, loss = 0.004671216942369938
iteration 258, loss = 0.004326283931732178
iteration 259, loss = 0.004946655593812466
iteration 260, loss = 0.004282613284885883
iteration 261, loss = 0.004266267642378807
iteration 262, loss = 0.005788517650216818
iteration 263, loss = 0.0044235046952962875
iteration 264, loss = 0.0040553659200668335
iteration 265, loss = 0.004375932738184929
iteration 266, loss = 0.00611583748832345
iteration 267, loss = 0.004464804660528898
iteration 268, loss = 0.0052254218608140945
iteration 269, loss = 0.004218386486172676
iteration 270, loss = 0.003869719337671995
iteration 271, loss = 0.004377676639705896
iteration 272, loss = 0.0038206102326512337
iteration 273, loss = 0.00604436406865716
iteration 274, loss = 0.00541389686986804
iteration 275, loss = 0.006750002503395081
iteration 276, loss = 0.00429679686203599
iteration 277, loss = 0.006036094389855862
iteration 278, loss = 0.004257338587194681
iteration 279, loss = 0.004041856154799461
iteration 280, loss = 0.0058373440988361835
iteration 281, loss = 0.003978435415774584
iteration 282, loss = 0.004213342908769846
iteration 283, loss = 0.006808568723499775
iteration 284, loss = 0.004278227221220732
iteration 285, loss = 0.004371060524135828
iteration 286, loss = 0.004860832821577787
iteration 287, loss = 0.004607629496604204
iteration 288, loss = 0.005624931771308184
iteration 289, loss = 0.00451447069644928
iteration 290, loss = 0.004423695616424084
iteration 291, loss = 0.004350571893155575
iteration 292, loss = 0.004450011532753706
iteration 293, loss = 0.00462295999750495
iteration 294, loss = 0.003999505192041397
iteration 295, loss = 0.004544704686850309
iteration 296, loss = 0.004162181634455919
iteration 297, loss = 0.004833253566175699
iteration 298, loss = 0.0057174526154994965
iteration 299, loss = 0.004118839744478464
iteration 300, loss = 0.004285351373255253
iteration 1, loss = 0.004553286824375391
iteration 2, loss = 0.004164627753198147
iteration 3, loss = 0.004970734938979149
iteration 4, loss = 0.004252998623996973
iteration 5, loss = 0.00432137306779623
iteration 6, loss = 0.004483995493501425
iteration 7, loss = 0.008110917173326015
iteration 8, loss = 0.004817310255020857
iteration 9, loss = 0.004509770777076483
iteration 10, loss = 0.004843057133257389
iteration 11, loss = 0.004789365921169519
iteration 12, loss = 0.003799574915319681
iteration 13, loss = 0.005404728930443525
iteration 14, loss = 0.0040284087881445885
iteration 15, loss = 0.00629811454564333
iteration 16, loss = 0.004748772829771042
iteration 17, loss = 0.004104299936443567
iteration 18, loss = 0.0044084833934903145
iteration 19, loss = 0.0039715091697871685
iteration 20, loss = 0.004084630403667688
iteration 21, loss = 0.006011453922837973
iteration 22, loss = 0.004345424473285675
iteration 23, loss = 0.004750183317810297
iteration 24, loss = 0.003970162943005562
iteration 25, loss = 0.00393122062087059
iteration 26, loss = 0.004226787947118282
iteration 27, loss = 0.00423384178429842
iteration 28, loss = 0.0044605908915400505
iteration 29, loss = 0.004993436858057976
iteration 30, loss = 0.003927483223378658
iteration 31, loss = 0.004520951770246029
iteration 32, loss = 0.003806541906669736
iteration 33, loss = 0.004898440092802048
iteration 34, loss = 0.004023772198706865
iteration 35, loss = 0.004029892385005951
iteration 36, loss = 0.003807272994890809
iteration 37, loss = 0.0062778545543551445
iteration 38, loss = 0.00403609499335289
iteration 39, loss = 0.0038686306215822697
iteration 40, loss = 0.004048963077366352
iteration 41, loss = 0.004795710556209087
iteration 42, loss = 0.005598385352641344
iteration 43, loss = 0.004185882862657309
iteration 44, loss = 0.004400084726512432
iteration 45, loss = 0.00615697493776679
iteration 46, loss = 0.0045983195304870605
iteration 47, loss = 0.0043113562278449535
iteration 48, loss = 0.005470224190503359
iteration 49, loss = 0.004345117602497339
iteration 50, loss = 0.004415702540427446
iteration 51, loss = 0.004473173525184393
iteration 52, loss = 0.00393493939191103
iteration 53, loss = 0.0069971890188753605
iteration 54, loss = 0.004839525558054447
iteration 55, loss = 0.00421339925378561
iteration 56, loss = 0.006009767763316631
iteration 57, loss = 0.004490257706493139
iteration 58, loss = 0.005101640708744526
iteration 59, loss = 0.004475593101233244
iteration 60, loss = 0.006837364751845598
iteration 61, loss = 0.004480686504393816
iteration 62, loss = 0.004243938717991114
iteration 63, loss = 0.004578989464789629
iteration 64, loss = 0.005197478458285332
iteration 65, loss = 0.004301484674215317
iteration 66, loss = 0.004084418993443251
iteration 67, loss = 0.003982573747634888
iteration 68, loss = 0.004186421632766724
iteration 69, loss = 0.004321257583796978
iteration 70, loss = 0.005807702895253897
iteration 71, loss = 0.004759714938700199
iteration 72, loss = 0.00485260458663106
iteration 73, loss = 0.004702188074588776
iteration 74, loss = 0.004202717449516058
iteration 75, loss = 0.0041945939883589745
iteration 76, loss = 0.006153020542114973
iteration 77, loss = 0.00528704933822155
iteration 78, loss = 0.004137531854212284
iteration 79, loss = 0.00439013447612524
iteration 80, loss = 0.004818607121706009
iteration 81, loss = 0.006356855854392052
iteration 82, loss = 0.0042678858153522015
iteration 83, loss = 0.005650640930980444
iteration 84, loss = 0.0044916789047420025
iteration 85, loss = 0.004092961084097624
iteration 86, loss = 0.006752327550202608
iteration 87, loss = 0.005883760284632444
iteration 88, loss = 0.004255090374499559
iteration 89, loss = 0.004906199406832457
iteration 90, loss = 0.004529022146016359
iteration 91, loss = 0.0038299195002764463
iteration 92, loss = 0.004132344387471676
iteration 93, loss = 0.004421250894665718
iteration 94, loss = 0.003940441645681858
iteration 95, loss = 0.005447858944535255
iteration 96, loss = 0.004264664836227894
iteration 97, loss = 0.004006518516689539
iteration 98, loss = 0.004359194077551365
iteration 99, loss = 0.004088032059371471
iteration 100, loss = 0.004278241656720638
iteration 101, loss = 0.004713067784905434
iteration 102, loss = 0.004858325235545635
iteration 103, loss = 0.004275175277143717
iteration 104, loss = 0.004142208956182003
iteration 105, loss = 0.006090135313570499
iteration 106, loss = 0.004853871650993824
iteration 107, loss = 0.004026486538350582
iteration 108, loss = 0.004146921448409557
iteration 109, loss = 0.004841969348490238
iteration 110, loss = 0.0051870327442884445
iteration 111, loss = 0.004125433042645454
iteration 112, loss = 0.004407460335642099
iteration 113, loss = 0.00461152009665966
iteration 114, loss = 0.0043743508867919445
iteration 115, loss = 0.0044975364580750465
iteration 116, loss = 0.004341437015682459
iteration 117, loss = 0.004842670634388924
iteration 118, loss = 0.005806363187730312
iteration 119, loss = 0.0067525822669267654
iteration 120, loss = 0.006248360965400934
iteration 121, loss = 0.006120357662439346
iteration 122, loss = 0.004280462861061096
iteration 123, loss = 0.004486730322241783
iteration 124, loss = 0.005012119188904762
iteration 125, loss = 0.004316824022680521
iteration 126, loss = 0.004408566746860743
iteration 127, loss = 0.0041764480993151665
iteration 128, loss = 0.004236642736941576
iteration 129, loss = 0.007192099932581186
iteration 130, loss = 0.0069816275499761105
iteration 131, loss = 0.004651360679417849
iteration 132, loss = 0.004297725390642881
iteration 133, loss = 0.004089652560651302
iteration 134, loss = 0.004051234573125839
iteration 135, loss = 0.005520478822290897
iteration 136, loss = 0.0042670113034546375
iteration 137, loss = 0.0038374904543161392
iteration 138, loss = 0.003912789281457663
iteration 139, loss = 0.0045235250145196915
iteration 140, loss = 0.0041686780750751495
iteration 141, loss = 0.004834864288568497
iteration 142, loss = 0.004915518686175346
iteration 143, loss = 0.003987382166087627
iteration 144, loss = 0.006332969292998314
iteration 145, loss = 0.004082435742020607
iteration 146, loss = 0.005883350502699614
iteration 147, loss = 0.004352428484708071
iteration 148, loss = 0.00401657959446311
iteration 149, loss = 0.005628805607557297
iteration 150, loss = 0.004590677097439766
iteration 151, loss = 0.004700280260294676
iteration 152, loss = 0.004524126648902893
iteration 153, loss = 0.004067874979227781
iteration 154, loss = 0.003983663860708475
iteration 155, loss = 0.004088725429028273
iteration 156, loss = 0.004138844553381205
iteration 157, loss = 0.003963787574321032
iteration 158, loss = 0.005856115370988846
iteration 159, loss = 0.004615653771907091
iteration 160, loss = 0.004280294757336378
iteration 161, loss = 0.004205641336739063
iteration 162, loss = 0.005680704023689032
iteration 163, loss = 0.004429825581610203
iteration 164, loss = 0.004259092267602682
iteration 165, loss = 0.004050672985613346
iteration 166, loss = 0.003930370789021254
iteration 167, loss = 0.004042331594973803
iteration 168, loss = 0.004192300606518984
iteration 169, loss = 0.004329159390181303
iteration 170, loss = 0.004514636471867561
iteration 171, loss = 0.005340883508324623
iteration 172, loss = 0.005363531410694122
iteration 173, loss = 0.00875865202397108
iteration 174, loss = 0.004144431557506323
iteration 175, loss = 0.004710054025053978
iteration 176, loss = 0.004809129983186722
iteration 177, loss = 0.004748946521431208
iteration 178, loss = 0.004828169010579586
iteration 179, loss = 0.004221935756504536
iteration 180, loss = 0.004102371167391539
iteration 181, loss = 0.004633709788322449
iteration 182, loss = 0.004763276781886816
iteration 183, loss = 0.0041867634281516075
iteration 184, loss = 0.004511835519224405
iteration 185, loss = 0.006008016876876354
iteration 186, loss = 0.004193200264126062
iteration 187, loss = 0.004124307539314032
iteration 188, loss = 0.005799431819468737
iteration 189, loss = 0.004241303075104952
iteration 190, loss = 0.004080208949744701
iteration 191, loss = 0.00468913558870554
iteration 192, loss = 0.004233794752508402
iteration 193, loss = 0.006063379347324371
iteration 194, loss = 0.004157587885856628
iteration 195, loss = 0.004294184502214193
iteration 196, loss = 0.004385083913803101
iteration 197, loss = 0.005048478953540325
iteration 198, loss = 0.003730230964720249
iteration 199, loss = 0.004824516829103231
iteration 200, loss = 0.00597733398899436
iteration 201, loss = 0.007200636435300112
iteration 202, loss = 0.0039039989933371544
iteration 203, loss = 0.0045740846544504166
iteration 204, loss = 0.004845052026212215
iteration 205, loss = 0.003939391113817692
iteration 206, loss = 0.004859475418925285
iteration 207, loss = 0.004350677598267794
iteration 208, loss = 0.005451892968267202
iteration 209, loss = 0.004416983108967543
iteration 210, loss = 0.003943427000194788
iteration 211, loss = 0.006258548237383366
iteration 212, loss = 0.004305646754801273
iteration 213, loss = 0.004735354799777269
iteration 214, loss = 0.008431718684732914
iteration 215, loss = 0.004774667322635651
iteration 216, loss = 0.003896939568221569
iteration 217, loss = 0.005448490381240845
iteration 218, loss = 0.0045891935005784035
iteration 219, loss = 0.004551622550934553
iteration 220, loss = 0.00614919513463974
iteration 221, loss = 0.0038851452991366386
iteration 222, loss = 0.004027483984827995
iteration 223, loss = 0.004431761801242828
iteration 224, loss = 0.0039897882379591465
iteration 225, loss = 0.0042458800598979
iteration 226, loss = 0.008336329832673073
iteration 227, loss = 0.00479919696226716
iteration 228, loss = 0.004865213762968779
iteration 229, loss = 0.004132378380745649
iteration 230, loss = 0.004881132394075394
iteration 231, loss = 0.004661254119127989
iteration 232, loss = 0.004136963281780481
iteration 233, loss = 0.0069647072814404964
iteration 234, loss = 0.004233306273818016
iteration 235, loss = 0.0054496293887495995
iteration 236, loss = 0.006133279297500849
iteration 237, loss = 0.004599503241479397
iteration 238, loss = 0.004236492794007063
iteration 239, loss = 0.0044705625623464584
iteration 240, loss = 0.004498275462538004
iteration 241, loss = 0.004360881634056568
iteration 242, loss = 0.005854335613548756
iteration 243, loss = 0.0042356583289802074
iteration 244, loss = 0.006876444444060326
iteration 245, loss = 0.0040465909987688065
iteration 246, loss = 0.005303931888192892
iteration 247, loss = 0.004404198378324509
iteration 248, loss = 0.003966255579143763
iteration 249, loss = 0.004550707992166281
iteration 250, loss = 0.004184621386229992
iteration 251, loss = 0.004059136379510164
iteration 252, loss = 0.003930897451937199
iteration 253, loss = 0.006914784666150808
iteration 254, loss = 0.0043643866665661335
iteration 255, loss = 0.004171424545347691
iteration 256, loss = 0.004942785948514938
iteration 257, loss = 0.005363599397242069
iteration 258, loss = 0.005748057272285223
iteration 259, loss = 0.004103521350771189
iteration 260, loss = 0.004268825985491276
iteration 261, loss = 0.005767317488789558
iteration 262, loss = 0.00418926402926445
iteration 263, loss = 0.005003758240491152
iteration 264, loss = 0.004243972711265087
iteration 265, loss = 0.004477015696465969
iteration 266, loss = 0.004458189010620117
iteration 267, loss = 0.0050132861360907555
iteration 268, loss = 0.005804828833788633
iteration 269, loss = 0.004374454729259014
iteration 270, loss = 0.007169593125581741
iteration 271, loss = 0.00406228331848979
iteration 272, loss = 0.006542493123561144
iteration 273, loss = 0.003968502394855022
iteration 274, loss = 0.006911730393767357
iteration 275, loss = 0.0060280608013272285
iteration 276, loss = 0.0049421777948737144
iteration 277, loss = 0.004898640792816877
iteration 278, loss = 0.00593269057571888
iteration 279, loss = 0.0040828087367117405
iteration 280, loss = 0.004505910910665989
iteration 281, loss = 0.004571949131786823
iteration 282, loss = 0.004845691844820976
iteration 283, loss = 0.004313825163990259
iteration 284, loss = 0.0038698590360581875
iteration 285, loss = 0.004075654782354832
iteration 286, loss = 0.004018543753772974
iteration 287, loss = 0.004731523338705301
iteration 288, loss = 0.004031458403915167
iteration 289, loss = 0.0041786860674619675
iteration 290, loss = 0.008542599156498909
iteration 291, loss = 0.004364017862826586
iteration 292, loss = 0.004453688859939575
iteration 293, loss = 0.005311056040227413
iteration 294, loss = 0.0039054916705936193
iteration 295, loss = 0.005646997597068548
iteration 296, loss = 0.004754832945764065
iteration 297, loss = 0.004214591346681118
iteration 298, loss = 0.004294637590646744
iteration 299, loss = 0.005508570931851864
iteration 300, loss = 0.004348729737102985
iteration 1, loss = 0.004181392025202513
iteration 2, loss = 0.004044887609779835
iteration 3, loss = 0.004486177582293749
iteration 4, loss = 0.003956110216677189
iteration 5, loss = 0.004158830735832453
iteration 6, loss = 0.0041331034153699875
iteration 7, loss = 0.0044963485561311245
iteration 8, loss = 0.004111060872673988
iteration 9, loss = 0.00442262040451169
iteration 10, loss = 0.0060616242699325085
iteration 11, loss = 0.00421461695805192
iteration 12, loss = 0.005685768090188503
iteration 13, loss = 0.0059709264896810055
iteration 14, loss = 0.005377000197768211
iteration 15, loss = 0.00518654054030776
iteration 16, loss = 0.004746709018945694
iteration 17, loss = 0.004293733276426792
iteration 18, loss = 0.004082066006958485
iteration 19, loss = 0.00440661795437336
iteration 20, loss = 0.004698209930211306
iteration 21, loss = 0.006753175985068083
iteration 22, loss = 0.005707064177840948
iteration 23, loss = 0.007371558807790279
iteration 24, loss = 0.00597975542768836
iteration 25, loss = 0.00414433004334569
iteration 26, loss = 0.004121008329093456
iteration 27, loss = 0.004029277246445417
iteration 28, loss = 0.007064224686473608
iteration 29, loss = 0.006378833204507828
iteration 30, loss = 0.0042157345451414585
iteration 31, loss = 0.004163098521530628
iteration 32, loss = 0.004153461195528507
iteration 33, loss = 0.004292238969355822
iteration 34, loss = 0.0045721628703176975
iteration 35, loss = 0.005066614598035812
iteration 36, loss = 0.005435071419924498
iteration 37, loss = 0.0041604433208703995
iteration 38, loss = 0.003971681464463472
iteration 39, loss = 0.004414450842887163
iteration 40, loss = 0.004743765573948622
iteration 41, loss = 0.0043142386712133884
iteration 42, loss = 0.004236208274960518
iteration 43, loss = 0.008273041807115078
iteration 44, loss = 0.004198543727397919
iteration 45, loss = 0.004770096857100725
iteration 46, loss = 0.00446471618488431
iteration 47, loss = 0.004892704077064991
iteration 48, loss = 0.008296213112771511
iteration 49, loss = 0.0038792279083281755
iteration 50, loss = 0.007512001786381006
iteration 51, loss = 0.004022980108857155
iteration 52, loss = 0.004687839653342962
iteration 53, loss = 0.004473492037504911
iteration 54, loss = 0.004450069274753332
iteration 55, loss = 0.004007835406810045
iteration 56, loss = 0.004011804237961769
iteration 57, loss = 0.003870961256325245
iteration 58, loss = 0.003953126259148121
iteration 59, loss = 0.005276049021631479
iteration 60, loss = 0.00465806107968092
iteration 61, loss = 0.006469180341809988
iteration 62, loss = 0.003915656823664904
iteration 63, loss = 0.004462654702365398
iteration 64, loss = 0.004497212823480368
iteration 65, loss = 0.0054267216473817825
iteration 66, loss = 0.004266473930329084
iteration 67, loss = 0.0043179006315767765
iteration 68, loss = 0.004790735896676779
iteration 69, loss = 0.004016924649477005
iteration 70, loss = 0.0046699452213943005
iteration 71, loss = 0.004110901616513729
iteration 72, loss = 0.0043899598531425
iteration 73, loss = 0.004245218820869923
iteration 74, loss = 0.004402682185173035
iteration 75, loss = 0.003948742989450693
iteration 76, loss = 0.004247759934514761
iteration 77, loss = 0.004238973371684551
iteration 78, loss = 0.003817139193415642
iteration 79, loss = 0.004666759632527828
iteration 80, loss = 0.00719063077121973
iteration 81, loss = 0.005106266122311354
iteration 82, loss = 0.00646743830293417
iteration 83, loss = 0.0043366821482777596
iteration 84, loss = 0.004797244444489479
iteration 85, loss = 0.004148113075643778
iteration 86, loss = 0.003911244682967663
iteration 87, loss = 0.0050204177387058735
iteration 88, loss = 0.005020969547331333
iteration 89, loss = 0.004388370551168919
iteration 90, loss = 0.004138524178415537
iteration 91, loss = 0.004212861880660057
iteration 92, loss = 0.004289967007935047
iteration 93, loss = 0.0042830174788832664
iteration 94, loss = 0.004006473813205957
iteration 95, loss = 0.007893173024058342
iteration 96, loss = 0.00412787776440382
iteration 97, loss = 0.005910202395170927
iteration 98, loss = 0.004726458806544542
iteration 99, loss = 0.005431423895061016
iteration 100, loss = 0.0038459368515759706
iteration 101, loss = 0.004654877819120884
iteration 102, loss = 0.0039480323903262615
iteration 103, loss = 0.00377702247351408
iteration 104, loss = 0.006130815949290991
iteration 105, loss = 0.004188336431980133
iteration 106, loss = 0.0043093315325677395
iteration 107, loss = 0.00418485514819622
iteration 108, loss = 0.004272023215889931
iteration 109, loss = 0.004269806668162346
iteration 110, loss = 0.004904858767986298
iteration 111, loss = 0.0041340915486216545
iteration 112, loss = 0.0038656210526823997
iteration 113, loss = 0.0051971906796097755
iteration 114, loss = 0.004510986153036356
iteration 115, loss = 0.004081272054463625
iteration 116, loss = 0.004482429474592209
iteration 117, loss = 0.004472563974559307
iteration 118, loss = 0.007093693595379591
iteration 119, loss = 0.004311437252908945
iteration 120, loss = 0.004532977007329464
iteration 121, loss = 0.004518238827586174
iteration 122, loss = 0.004751069471240044
iteration 123, loss = 0.0042568654753267765
iteration 124, loss = 0.004304591100662947
iteration 125, loss = 0.004650790244340897
iteration 126, loss = 0.00565384840592742
iteration 127, loss = 0.005798714701086283
iteration 128, loss = 0.006228978745639324
iteration 129, loss = 0.004032266791909933
iteration 130, loss = 0.004119199700653553
iteration 131, loss = 0.004554981365799904
iteration 132, loss = 0.005970788188278675
iteration 133, loss = 0.004783038515597582
iteration 134, loss = 0.004399613477289677
iteration 135, loss = 0.004129840526729822
iteration 136, loss = 0.004882532171905041
iteration 137, loss = 0.00470889825373888
iteration 138, loss = 0.006841265596449375
iteration 139, loss = 0.004559704568237066
iteration 140, loss = 0.004672094713896513
iteration 141, loss = 0.005483058746904135
iteration 142, loss = 0.004024052526801825
iteration 143, loss = 0.006805914919823408
iteration 144, loss = 0.0043833376839756966
iteration 145, loss = 0.006293777376413345
iteration 146, loss = 0.004138370044529438
iteration 147, loss = 0.0042601474560797215
iteration 148, loss = 0.003989644348621368
iteration 149, loss = 0.004166820086538792
iteration 150, loss = 0.004173653200268745
iteration 151, loss = 0.00604734942317009
iteration 152, loss = 0.004449618514627218
iteration 153, loss = 0.007706150412559509
iteration 154, loss = 0.004157570190727711
iteration 155, loss = 0.004056661855429411
iteration 156, loss = 0.004491620697081089
iteration 157, loss = 0.004605954047292471
iteration 158, loss = 0.005394202657043934
iteration 159, loss = 0.004763489589095116
iteration 160, loss = 0.004310121759772301
iteration 161, loss = 0.0041738939471542835
iteration 162, loss = 0.004446934908628464
iteration 163, loss = 0.005657990928739309
iteration 164, loss = 0.004522530362010002
iteration 165, loss = 0.004262360744178295
iteration 166, loss = 0.006685780826956034
iteration 167, loss = 0.004841085523366928
iteration 168, loss = 0.004806811921298504
iteration 169, loss = 0.00477334251627326
iteration 170, loss = 0.004232273902744055
iteration 171, loss = 0.005406545475125313
iteration 172, loss = 0.003572231624275446
iteration 173, loss = 0.004738738294690847
iteration 174, loss = 0.004111114889383316
iteration 175, loss = 0.003986076917499304
iteration 176, loss = 0.004408770706504583
iteration 177, loss = 0.004186886362731457
iteration 178, loss = 0.004298114683479071
iteration 179, loss = 0.004358501173555851
iteration 180, loss = 0.004143575672060251
iteration 181, loss = 0.005529947578907013
iteration 182, loss = 0.004837447311729193
iteration 183, loss = 0.005784059874713421
iteration 184, loss = 0.005242583341896534
iteration 185, loss = 0.004186598584055901
iteration 186, loss = 0.004376890137791634
iteration 187, loss = 0.0041582477279007435
iteration 188, loss = 0.003907961770892143
iteration 189, loss = 0.004691385198384523
iteration 190, loss = 0.004670627880841494
iteration 191, loss = 0.006676047574728727
iteration 192, loss = 0.0054445113055408
iteration 193, loss = 0.004443811718374491
iteration 194, loss = 0.004389013629406691
iteration 195, loss = 0.004552891012281179
iteration 196, loss = 0.006050512194633484
iteration 197, loss = 0.005594945512712002
iteration 198, loss = 0.005630820989608765
iteration 199, loss = 0.004354190081357956
iteration 200, loss = 0.004031173884868622
iteration 201, loss = 0.003962855786085129
iteration 202, loss = 0.004459645599126816
iteration 203, loss = 0.004570089280605316
iteration 204, loss = 0.004192858934402466
iteration 205, loss = 0.0045555466786026955
iteration 206, loss = 0.008164487779140472
iteration 207, loss = 0.0039059172850102186
iteration 208, loss = 0.004325075540691614
iteration 209, loss = 0.004039960913360119
iteration 210, loss = 0.003891400992870331
iteration 211, loss = 0.004721498116850853
iteration 212, loss = 0.005009918473660946
iteration 213, loss = 0.0050141289830207825
iteration 214, loss = 0.004107798915356398
iteration 215, loss = 0.004187325946986675
iteration 216, loss = 0.005204849410802126
iteration 217, loss = 0.004449295345693827
iteration 218, loss = 0.005893941503018141
iteration 219, loss = 0.00898423045873642
iteration 220, loss = 0.004510774742811918
iteration 221, loss = 0.007536507211625576
iteration 222, loss = 0.0053298864513635635
iteration 223, loss = 0.004277798347175121
iteration 224, loss = 0.004199792630970478
iteration 225, loss = 0.0044076028279960155
iteration 226, loss = 0.003993604332208633
iteration 227, loss = 0.0042217266745865345
iteration 228, loss = 0.004142705351114273
iteration 229, loss = 0.004111678805202246
iteration 230, loss = 0.006942173931747675
iteration 231, loss = 0.003888560924679041
iteration 232, loss = 0.004104862455278635
iteration 233, loss = 0.004518468864262104
iteration 234, loss = 0.004314027726650238
iteration 235, loss = 0.004073781426995993
iteration 236, loss = 0.004181070253252983
iteration 237, loss = 0.004251330625265837
iteration 238, loss = 0.004411286674439907
iteration 239, loss = 0.004492711275815964
iteration 240, loss = 0.004775253590196371
iteration 241, loss = 0.0042571816593408585
iteration 242, loss = 0.003992008976638317
iteration 243, loss = 0.003672126680612564
iteration 244, loss = 0.005936840083450079
iteration 245, loss = 0.004546721465885639
iteration 246, loss = 0.004136332310736179
iteration 247, loss = 0.004189320839941502
iteration 248, loss = 0.003988823387771845
iteration 249, loss = 0.00447163125500083
iteration 250, loss = 0.006515296176075935
iteration 251, loss = 0.005973984021693468
iteration 252, loss = 0.004519600421190262
iteration 253, loss = 0.004506611730903387
iteration 254, loss = 0.0036704002413898706
iteration 255, loss = 0.004827545490115881
iteration 256, loss = 0.004554018843919039
iteration 257, loss = 0.004265044815838337
iteration 258, loss = 0.004222970921546221
iteration 259, loss = 0.005739039275795221
iteration 260, loss = 0.004054083954542875
iteration 261, loss = 0.007573348470032215
iteration 262, loss = 0.00465633999556303
iteration 263, loss = 0.0047676945105195045
iteration 264, loss = 0.005016074050217867
iteration 265, loss = 0.00409823888912797
iteration 266, loss = 0.00430844072252512
iteration 267, loss = 0.007216383703052998
iteration 268, loss = 0.003976352512836456
iteration 269, loss = 0.004047248046845198
iteration 270, loss = 0.0047518471255898476
iteration 271, loss = 0.004336717538535595
iteration 272, loss = 0.004674318712204695
iteration 273, loss = 0.004166680853813887
iteration 274, loss = 0.004182125441730022
iteration 275, loss = 0.004096475429832935
iteration 276, loss = 0.007258910685777664
iteration 277, loss = 0.004179527051746845
iteration 278, loss = 0.004546417389065027
iteration 279, loss = 0.003937587141990662
iteration 280, loss = 0.004291446879506111
iteration 281, loss = 0.004474612884223461
iteration 282, loss = 0.004515751264989376
iteration 283, loss = 0.003950439393520355
iteration 284, loss = 0.004832664504647255
iteration 285, loss = 0.004203568212687969
iteration 286, loss = 0.004261983558535576
iteration 287, loss = 0.003889112500473857
iteration 288, loss = 0.004855232313275337
iteration 289, loss = 0.0044750431552529335
iteration 290, loss = 0.004196155350655317
iteration 291, loss = 0.004112530965358019
iteration 292, loss = 0.00466754287481308
iteration 293, loss = 0.004299356136471033
iteration 294, loss = 0.004329585004597902
iteration 295, loss = 0.005861390382051468
iteration 296, loss = 0.003979111555963755
iteration 297, loss = 0.004146857652813196
iteration 298, loss = 0.006120011676102877
iteration 299, loss = 0.00577374454587698
iteration 300, loss = 0.007331928238272667
iteration 1, loss = 0.004239614587277174
iteration 2, loss = 0.004662881605327129
iteration 3, loss = 0.004727475345134735
iteration 4, loss = 0.007818600162863731
iteration 5, loss = 0.005411296151578426
iteration 6, loss = 0.004886401817202568
iteration 7, loss = 0.003963237162679434
iteration 8, loss = 0.004347893875092268
iteration 9, loss = 0.007331221364438534
iteration 10, loss = 0.007702907081693411
iteration 11, loss = 0.003997955936938524
iteration 12, loss = 0.004168563522398472
iteration 13, loss = 0.0035366679076105356
iteration 14, loss = 0.004126891493797302
iteration 15, loss = 0.0040950896218419075
iteration 16, loss = 0.004105434753000736
iteration 17, loss = 0.004605351947247982
iteration 18, loss = 0.0044593168422579765
iteration 19, loss = 0.004245421849191189
iteration 20, loss = 0.00450699869543314
iteration 21, loss = 0.004218301735818386
iteration 22, loss = 0.007072828244417906
iteration 23, loss = 0.005076908506453037
iteration 24, loss = 0.006021957844495773
iteration 25, loss = 0.0042534261010587215
iteration 26, loss = 0.004152203910052776
iteration 27, loss = 0.004202293232083321
iteration 28, loss = 0.0062719304114580154
iteration 29, loss = 0.005642065778374672
iteration 30, loss = 0.006196481641381979
iteration 31, loss = 0.004458182957023382
iteration 32, loss = 0.005135532934218645
iteration 33, loss = 0.004422902129590511
iteration 34, loss = 0.004263961687684059
iteration 35, loss = 0.0042510805651545525
iteration 36, loss = 0.004137455951422453
iteration 37, loss = 0.004354543052613735
iteration 38, loss = 0.004366780631244183
iteration 39, loss = 0.00592541741207242
iteration 40, loss = 0.005770672112703323
iteration 41, loss = 0.004108174704015255
iteration 42, loss = 0.003944353200495243
iteration 43, loss = 0.004043202847242355
iteration 44, loss = 0.004597586579620838
iteration 45, loss = 0.004730559419840574
iteration 46, loss = 0.004144174512475729
iteration 47, loss = 0.003929163794964552
iteration 48, loss = 0.005839299410581589
iteration 49, loss = 0.004194245208054781
iteration 50, loss = 0.005761241540312767
iteration 51, loss = 0.00555424764752388
iteration 52, loss = 0.0041770366951823235
iteration 53, loss = 0.005998522043228149
iteration 54, loss = 0.004164082929491997
iteration 55, loss = 0.005976219195872545
iteration 56, loss = 0.004216049797832966
iteration 57, loss = 0.0057944199070334435
iteration 58, loss = 0.004136830568313599
iteration 59, loss = 0.005419504828751087
iteration 60, loss = 0.0038386606611311436
iteration 61, loss = 0.004051927477121353
iteration 62, loss = 0.007108945865184069
iteration 63, loss = 0.004686607979238033
iteration 64, loss = 0.004182038363069296
iteration 65, loss = 0.005409267731010914
iteration 66, loss = 0.004483604803681374
iteration 67, loss = 0.0047259461134672165
iteration 68, loss = 0.004003279842436314
iteration 69, loss = 0.004631938878446817
iteration 70, loss = 0.007123930379748344
iteration 71, loss = 0.004278135020285845
iteration 72, loss = 0.004440005403012037
iteration 73, loss = 0.004113512579351664
iteration 74, loss = 0.0053520952351391315
iteration 75, loss = 0.005662788636982441
iteration 76, loss = 0.004656825214624405
iteration 77, loss = 0.004079778678715229
iteration 78, loss = 0.003918208181858063
iteration 79, loss = 0.00742247374728322
iteration 80, loss = 0.004000393208116293
iteration 81, loss = 0.004495770670473576
iteration 82, loss = 0.004950592294335365
iteration 83, loss = 0.00413059676066041
iteration 84, loss = 0.006052698008716106
iteration 85, loss = 0.005274120718240738
iteration 86, loss = 0.004630694165825844
iteration 87, loss = 0.004046913702040911
iteration 88, loss = 0.004468476865440607
iteration 89, loss = 0.004481775686144829
iteration 90, loss = 0.005388500634580851
iteration 91, loss = 0.004019603598862886
iteration 92, loss = 0.0039149499498307705
iteration 93, loss = 0.004265851341187954
iteration 94, loss = 0.003956242930144072
iteration 95, loss = 0.004505583550781012
iteration 96, loss = 0.0043687596917152405
iteration 97, loss = 0.004480583127588034
iteration 98, loss = 0.00581781193614006
iteration 99, loss = 0.0038985959254205227
iteration 100, loss = 0.004579583648592234
iteration 101, loss = 0.004596741404384375
iteration 102, loss = 0.00418557645753026
iteration 103, loss = 0.007262900471687317
iteration 104, loss = 0.004292442463338375
iteration 105, loss = 0.006887773517519236
iteration 106, loss = 0.0053564151749014854
iteration 107, loss = 0.005278324708342552
iteration 108, loss = 0.0044054901227355
iteration 109, loss = 0.004068178590387106
iteration 110, loss = 0.00431579677388072
iteration 111, loss = 0.004896785132586956
iteration 112, loss = 0.004128448665142059
iteration 113, loss = 0.004235514439642429
iteration 114, loss = 0.004713688977062702
iteration 115, loss = 0.004780941642820835
iteration 116, loss = 0.003969223238527775
iteration 117, loss = 0.004547555465251207
iteration 118, loss = 0.004114862531423569
iteration 119, loss = 0.004916745238006115
iteration 120, loss = 0.004452690947800875
iteration 121, loss = 0.0046739354729652405
iteration 122, loss = 0.004158317111432552
iteration 123, loss = 0.004010909702628851
iteration 124, loss = 0.005883490201085806
iteration 125, loss = 0.003868485800921917
iteration 126, loss = 0.004818984307348728
iteration 127, loss = 0.005477567203342915
iteration 128, loss = 0.007170144468545914
iteration 129, loss = 0.004283771384507418
iteration 130, loss = 0.004814905114471912
iteration 131, loss = 0.003919925540685654
iteration 132, loss = 0.004150681663304567
iteration 133, loss = 0.00690571591258049
iteration 134, loss = 0.004118920769542456
iteration 135, loss = 0.004319244530051947
iteration 136, loss = 0.00667456304654479
iteration 137, loss = 0.005620714742690325
iteration 138, loss = 0.004289648495614529
iteration 139, loss = 0.004608703777194023
iteration 140, loss = 0.00420774333178997
iteration 141, loss = 0.004127360414713621
iteration 142, loss = 0.004530136473476887
iteration 143, loss = 0.00399630144238472
iteration 144, loss = 0.005344982724636793
iteration 145, loss = 0.004309197887778282
iteration 146, loss = 0.0036725718528032303
iteration 147, loss = 0.004265608265995979
iteration 148, loss = 0.004388580564409494
iteration 149, loss = 0.00462421216070652
iteration 150, loss = 0.004607553593814373
iteration 151, loss = 0.0040872227400541306
iteration 152, loss = 0.004624435678124428
iteration 153, loss = 0.004973183386027813
iteration 154, loss = 0.0035433643497526646
iteration 155, loss = 0.0043814657256007195
iteration 156, loss = 0.005494534503668547
iteration 157, loss = 0.005727419629693031
iteration 158, loss = 0.003980277106165886
iteration 159, loss = 0.0042827073484659195
iteration 160, loss = 0.004264987073838711
iteration 161, loss = 0.004583355505019426
iteration 162, loss = 0.006463932804763317
iteration 163, loss = 0.00469996128231287
iteration 164, loss = 0.003943945746868849
iteration 165, loss = 0.004089992959052324
iteration 166, loss = 0.004210829269140959
iteration 167, loss = 0.0048405504785478115
iteration 168, loss = 0.004425096791237593
iteration 169, loss = 0.004771014675498009
iteration 170, loss = 0.004242351744323969
iteration 171, loss = 0.005758954212069511
iteration 172, loss = 0.0039565106853842735
iteration 173, loss = 0.004546808544546366
iteration 174, loss = 0.003951653838157654
iteration 175, loss = 0.00403076596558094
iteration 176, loss = 0.0042897844687104225
iteration 177, loss = 0.0045270901173353195
iteration 178, loss = 0.004544876515865326
iteration 179, loss = 0.003939677029848099
iteration 180, loss = 0.004228654317557812
iteration 181, loss = 0.003969243727624416
iteration 182, loss = 0.004064080771058798
iteration 183, loss = 0.004015445709228516
iteration 184, loss = 0.004370883572846651
iteration 185, loss = 0.0041702017188072205
iteration 186, loss = 0.004047163296490908
iteration 187, loss = 0.004457421135157347
iteration 188, loss = 0.0043809604831039906
iteration 189, loss = 0.004428631626069546
iteration 190, loss = 0.004338537808507681
iteration 191, loss = 0.004098009318113327
iteration 192, loss = 0.006222694646567106
iteration 193, loss = 0.004482642747461796
iteration 194, loss = 0.004351046867668629
iteration 195, loss = 0.004952602554112673
iteration 196, loss = 0.005371155217289925
iteration 197, loss = 0.005150065291672945
iteration 198, loss = 0.004109840374439955
iteration 199, loss = 0.008995776064693928
iteration 200, loss = 0.0040571363642811775
iteration 201, loss = 0.0044310493394732475
iteration 202, loss = 0.004108952824026346
iteration 203, loss = 0.0041307369247078896
iteration 204, loss = 0.005440299399197102
iteration 205, loss = 0.004261504393070936
iteration 206, loss = 0.0046419790014624596
iteration 207, loss = 0.00530662527307868
iteration 208, loss = 0.004332869779318571
iteration 209, loss = 0.004222039598971605
iteration 210, loss = 0.004129050765186548
iteration 211, loss = 0.004640627186745405
iteration 212, loss = 0.00404741708189249
iteration 213, loss = 0.004183803219348192
iteration 214, loss = 0.0044694868847727776
iteration 215, loss = 0.004838267341256142
iteration 216, loss = 0.005445983726531267
iteration 217, loss = 0.004351516254246235
iteration 218, loss = 0.010576280765235424
iteration 219, loss = 0.004660371225327253
iteration 220, loss = 0.004122455138713121
iteration 221, loss = 0.004559684079140425
iteration 222, loss = 0.004137629177421331
iteration 223, loss = 0.004405748564749956
iteration 224, loss = 0.0043264697305858135
iteration 225, loss = 0.004848657641559839
iteration 226, loss = 0.004218496847897768
iteration 227, loss = 0.005536990240216255
iteration 228, loss = 0.004634141456335783
iteration 229, loss = 0.003915484994649887
iteration 230, loss = 0.004468406550586224
iteration 231, loss = 0.003978653810918331
iteration 232, loss = 0.004232923965901136
iteration 233, loss = 0.00580604188144207
iteration 234, loss = 0.004343710839748383
iteration 235, loss = 0.004505017306655645
iteration 236, loss = 0.006895141676068306
iteration 237, loss = 0.004321893211454153
iteration 238, loss = 0.004451394081115723
iteration 239, loss = 0.004087819717824459
iteration 240, loss = 0.00505070062354207
iteration 241, loss = 0.00414667185395956
iteration 242, loss = 0.004248351790010929
iteration 243, loss = 0.004436202347278595
iteration 244, loss = 0.005718322470784187
iteration 245, loss = 0.007179948966950178
iteration 246, loss = 0.005889636464416981
iteration 247, loss = 0.0042860303074121475
iteration 248, loss = 0.00461273780092597
iteration 249, loss = 0.004864213988184929
iteration 250, loss = 0.005216441582888365
iteration 251, loss = 0.00595584511756897
iteration 252, loss = 0.00445691030472517
iteration 253, loss = 0.0041037448681890965
iteration 254, loss = 0.004237344954162836
iteration 255, loss = 0.004199493210762739
iteration 256, loss = 0.00425297487527132
iteration 257, loss = 0.004231796599924564
iteration 258, loss = 0.004062667489051819
iteration 259, loss = 0.004296666942536831
iteration 260, loss = 0.004342943895608187
iteration 261, loss = 0.00444508483633399
iteration 262, loss = 0.003840933321043849
iteration 263, loss = 0.003918428905308247
iteration 264, loss = 0.0040597980841994286
iteration 265, loss = 0.004037503618746996
iteration 266, loss = 0.004361678846180439
iteration 267, loss = 0.004407932516187429
iteration 268, loss = 0.00948447547852993
iteration 269, loss = 0.005693531129509211
iteration 270, loss = 0.004632832948118448
iteration 271, loss = 0.0037581620272248983
iteration 272, loss = 0.005733002908527851
iteration 273, loss = 0.004408440552651882
iteration 274, loss = 0.005647975020110607
iteration 275, loss = 0.005274484865367413
iteration 276, loss = 0.006865705829113722
iteration 277, loss = 0.0042611053213477135
iteration 278, loss = 0.0043153539299964905
iteration 279, loss = 0.004682688042521477
iteration 280, loss = 0.003792098956182599
iteration 281, loss = 0.004687825683504343
iteration 282, loss = 0.0042075845412909985
iteration 283, loss = 0.004506885539740324
iteration 284, loss = 0.005352122243493795
iteration 285, loss = 0.006214817985892296
iteration 286, loss = 0.007248141802847385
iteration 287, loss = 0.00397912971675396
iteration 288, loss = 0.004051571246236563
iteration 289, loss = 0.00418432941660285
iteration 290, loss = 0.004394080489873886
iteration 291, loss = 0.004202906507998705
iteration 292, loss = 0.004817605018615723
iteration 293, loss = 0.006104883272200823
iteration 294, loss = 0.0038742332253605127
iteration 295, loss = 0.0046531204134225845
iteration 296, loss = 0.0038012005388736725
iteration 297, loss = 0.0070839254185557365
iteration 298, loss = 0.004680994898080826
iteration 299, loss = 0.004508854355663061
iteration 300, loss = 0.004883567336946726
iteration 1, loss = 0.0044487230479717255
iteration 2, loss = 0.004037225618958473
iteration 3, loss = 0.004131403286010027
iteration 4, loss = 0.004707821644842625
iteration 5, loss = 0.005791691597551107
iteration 6, loss = 0.005067370366305113
iteration 7, loss = 0.0049527776427567005
iteration 8, loss = 0.004456645809113979
iteration 9, loss = 0.008170930668711662
iteration 10, loss = 0.006721779238432646
iteration 11, loss = 0.003991386387497187
iteration 12, loss = 0.006374590564519167
iteration 13, loss = 0.003919968847185373
iteration 14, loss = 0.003954196348786354
iteration 15, loss = 0.004116200841963291
iteration 16, loss = 0.00411367230117321
iteration 17, loss = 0.004394552670419216
iteration 18, loss = 0.004292531404644251
iteration 19, loss = 0.005284574348479509
iteration 20, loss = 0.004592106677591801
iteration 21, loss = 0.004245806951075792
iteration 22, loss = 0.00435266550630331
iteration 23, loss = 0.00471026124432683
iteration 24, loss = 0.004186905454844236
iteration 25, loss = 0.004670900292694569
iteration 26, loss = 0.004179318435490131
iteration 27, loss = 0.004460111726075411
iteration 28, loss = 0.0042215753346681595
iteration 29, loss = 0.00415685074403882
iteration 30, loss = 0.004849795252084732
iteration 31, loss = 0.004753300454467535
iteration 32, loss = 0.004462456330657005
iteration 33, loss = 0.004115129355341196
iteration 34, loss = 0.00436273030936718
iteration 35, loss = 0.003983050584793091
iteration 36, loss = 0.004498854745179415
iteration 37, loss = 0.004174746572971344
iteration 38, loss = 0.0045719631016254425
iteration 39, loss = 0.004087514244019985
iteration 40, loss = 0.00435330905020237
iteration 41, loss = 0.004132022615522146
iteration 42, loss = 0.005242441315203905
iteration 43, loss = 0.003996264189481735
iteration 44, loss = 0.004586139228194952
iteration 45, loss = 0.004151571076363325
iteration 46, loss = 0.007248186971992254
iteration 47, loss = 0.0039970953948795795
iteration 48, loss = 0.005592190194875002
iteration 49, loss = 0.004254721105098724
iteration 50, loss = 0.004216793924570084
iteration 51, loss = 0.004531587474048138
iteration 52, loss = 0.004531875252723694
iteration 53, loss = 0.005256257485598326
iteration 54, loss = 0.0042242249473929405
iteration 55, loss = 0.004254963248968124
iteration 56, loss = 0.004041086882352829
iteration 57, loss = 0.007041771896183491
iteration 58, loss = 0.004445014987140894
iteration 59, loss = 0.004945859778672457
iteration 60, loss = 0.004046806134283543
iteration 61, loss = 0.004254637286067009
iteration 62, loss = 0.004062149208039045
iteration 63, loss = 0.005492033436894417
iteration 64, loss = 0.004297868348658085
iteration 65, loss = 0.0038792260456830263
iteration 66, loss = 0.006321871653199196
iteration 67, loss = 0.007397118024528027
iteration 68, loss = 0.004164456389844418
iteration 69, loss = 0.004074275493621826
iteration 70, loss = 0.0036734226159751415
iteration 71, loss = 0.004625131841748953
iteration 72, loss = 0.00454710191115737
iteration 73, loss = 0.004323652479797602
iteration 74, loss = 0.004707216285169125
iteration 75, loss = 0.004459435120224953
iteration 76, loss = 0.004242321942001581
iteration 77, loss = 0.0046622478403151035
iteration 78, loss = 0.00383368949405849
iteration 79, loss = 0.005197177175432444
iteration 80, loss = 0.004355281591415405
iteration 81, loss = 0.004084203392267227
iteration 82, loss = 0.004009325057268143
iteration 83, loss = 0.00404701754450798
iteration 84, loss = 0.0070284949615597725
iteration 85, loss = 0.004611017182469368
iteration 86, loss = 0.004373222589492798
iteration 87, loss = 0.004229624290019274
iteration 88, loss = 0.004459113348275423
iteration 89, loss = 0.004145581275224686
iteration 90, loss = 0.0043221102096140385
iteration 91, loss = 0.003974952269345522
iteration 92, loss = 0.004310074262320995
iteration 93, loss = 0.005760548636317253
iteration 94, loss = 0.004739028867334127
iteration 95, loss = 0.005526050925254822
iteration 96, loss = 0.0038849422708153725
iteration 97, loss = 0.00399040337651968
iteration 98, loss = 0.007032018154859543
iteration 99, loss = 0.003901425749063492
iteration 100, loss = 0.007171696983277798
iteration 101, loss = 0.0042199851013720036
iteration 102, loss = 0.004161196295171976
iteration 103, loss = 0.0045339856296777725
iteration 104, loss = 0.004180034156888723
iteration 105, loss = 0.0041708871722221375
iteration 106, loss = 0.004000857938081026
iteration 107, loss = 0.007525958586484194
iteration 108, loss = 0.004160962533205748
iteration 109, loss = 0.004498204216361046
iteration 110, loss = 0.00426695728674531
iteration 111, loss = 0.0041494485922157764
iteration 112, loss = 0.004381757229566574
iteration 113, loss = 0.004163766745477915
iteration 114, loss = 0.006511430721729994
iteration 115, loss = 0.004277847707271576
iteration 116, loss = 0.004402943886816502
iteration 117, loss = 0.004727646708488464
iteration 118, loss = 0.004644104745239019
iteration 119, loss = 0.005744926631450653
iteration 120, loss = 0.004241871647536755
iteration 121, loss = 0.004190381150692701
iteration 122, loss = 0.00506629841402173
iteration 123, loss = 0.004257609136402607
iteration 124, loss = 0.004228442441672087
iteration 125, loss = 0.004362158477306366
iteration 126, loss = 0.004867370706051588
iteration 127, loss = 0.004702284000813961
iteration 128, loss = 0.003997793886810541
iteration 129, loss = 0.004336476791650057
iteration 130, loss = 0.004086180590093136
iteration 131, loss = 0.005585360340774059
iteration 132, loss = 0.003992134239524603
iteration 133, loss = 0.004264326766133308
iteration 134, loss = 0.00714055635035038
iteration 135, loss = 0.004774473141878843
iteration 136, loss = 0.004269126802682877
iteration 137, loss = 0.004320180509239435
iteration 138, loss = 0.0039047456812113523
iteration 139, loss = 0.004400016739964485
iteration 140, loss = 0.006449388340115547
iteration 141, loss = 0.004080263432115316
iteration 142, loss = 0.004086079075932503
iteration 143, loss = 0.004092853516340256
iteration 144, loss = 0.004400559701025486
iteration 145, loss = 0.004352223128080368
iteration 146, loss = 0.004114395938813686
iteration 147, loss = 0.004025357309728861
iteration 148, loss = 0.0069997962564229965
iteration 149, loss = 0.004201410338282585
iteration 150, loss = 0.004477935843169689
iteration 151, loss = 0.004436501767486334
iteration 152, loss = 0.00406332453712821
iteration 153, loss = 0.004543374292552471
iteration 154, loss = 0.004207676742225885
iteration 155, loss = 0.004435366950929165
iteration 156, loss = 0.0046652862802147865
iteration 157, loss = 0.005563902202993631
iteration 158, loss = 0.004596838261932135
iteration 159, loss = 0.004420045763254166
iteration 160, loss = 0.007532148156315088
iteration 161, loss = 0.00576381292194128
iteration 162, loss = 0.005643627140671015
iteration 163, loss = 0.0042298175394535065
iteration 164, loss = 0.004327366128563881
iteration 165, loss = 0.0041401684284210205
iteration 166, loss = 0.004193681292235851
iteration 167, loss = 0.004637550562620163
iteration 168, loss = 0.005176822654902935
iteration 169, loss = 0.004183024633675814
iteration 170, loss = 0.0041536446660757065
iteration 171, loss = 0.004288044758141041
iteration 172, loss = 0.004415800794959068
iteration 173, loss = 0.004520828370004892
iteration 174, loss = 0.004514531698077917
iteration 175, loss = 0.005226934794336557
iteration 176, loss = 0.0061265695840120316
iteration 177, loss = 0.005820025224238634
iteration 178, loss = 0.004662749823182821
iteration 179, loss = 0.004266362637281418
iteration 180, loss = 0.005698221269994974
iteration 181, loss = 0.004504992626607418
iteration 182, loss = 0.004258302040398121
iteration 183, loss = 0.006612511817365885
iteration 184, loss = 0.005699594505131245
iteration 185, loss = 0.005296339280903339
iteration 186, loss = 0.005602580960839987
iteration 187, loss = 0.005610723048448563
iteration 188, loss = 0.0040922146290540695
iteration 189, loss = 0.004884039517492056
iteration 190, loss = 0.0046332478523254395
iteration 191, loss = 0.004441550932824612
iteration 192, loss = 0.006654847413301468
iteration 193, loss = 0.004255551844835281
iteration 194, loss = 0.0042154258117079735
iteration 195, loss = 0.004179594572633505
iteration 196, loss = 0.003894143272191286
iteration 197, loss = 0.006836215034127235
iteration 198, loss = 0.004224568605422974
iteration 199, loss = 0.0042089009657502174
iteration 200, loss = 0.006698435638099909
iteration 201, loss = 0.005482885055243969
iteration 202, loss = 0.004374359734356403
iteration 203, loss = 0.004372044466435909
iteration 204, loss = 0.0052077388390898705
iteration 205, loss = 0.007353849709033966
iteration 206, loss = 0.004173396620899439
iteration 207, loss = 0.00423435727134347
iteration 208, loss = 0.004401208832859993
iteration 209, loss = 0.004059449303895235
iteration 210, loss = 0.004454377107322216
iteration 211, loss = 0.003957553766667843
iteration 212, loss = 0.0036217428278177977
iteration 213, loss = 0.0064461007714271545
iteration 214, loss = 0.004690401256084442
iteration 215, loss = 0.0052009886130690575
iteration 216, loss = 0.004194709472358227
iteration 217, loss = 0.00690402602776885
iteration 218, loss = 0.004211354069411755
iteration 219, loss = 0.005607180763036013
iteration 220, loss = 0.0038444765377789736
iteration 221, loss = 0.004091730806976557
iteration 222, loss = 0.004620573483407497
iteration 223, loss = 0.0038885739631950855
iteration 224, loss = 0.004459891933947802
iteration 225, loss = 0.00431037088856101
iteration 226, loss = 0.0043108719401061535
iteration 227, loss = 0.004327827133238316
iteration 228, loss = 0.0057447440922260284
iteration 229, loss = 0.00445982813835144
iteration 230, loss = 0.004475878551602364
iteration 231, loss = 0.004682376515120268
iteration 232, loss = 0.00444168271496892
iteration 233, loss = 0.004223449155688286
iteration 234, loss = 0.0038816388696432114
iteration 235, loss = 0.004452481400221586
iteration 236, loss = 0.005096317268908024
iteration 237, loss = 0.003947904333472252
iteration 238, loss = 0.004340847488492727
iteration 239, loss = 0.006752244662493467
iteration 240, loss = 0.004392172209918499
iteration 241, loss = 0.004221803974360228
iteration 242, loss = 0.003890414722263813
iteration 243, loss = 0.0038537895306944847
iteration 244, loss = 0.006148706655949354
iteration 245, loss = 0.004041891545057297
iteration 246, loss = 0.005401508882641792
iteration 247, loss = 0.0045519364066421986
iteration 248, loss = 0.0043654413893818855
iteration 249, loss = 0.004183872137218714
iteration 250, loss = 0.005502298474311829
iteration 251, loss = 0.004511307459324598
iteration 252, loss = 0.0041825962252914906
iteration 253, loss = 0.005622388329356909
iteration 254, loss = 0.0043700397945940495
iteration 255, loss = 0.004430511500686407
iteration 256, loss = 0.00458644051104784
iteration 257, loss = 0.007433561608195305
iteration 258, loss = 0.004716871771961451
iteration 259, loss = 0.00577844912186265
iteration 260, loss = 0.004527088720351458
iteration 261, loss = 0.00466622319072485
iteration 262, loss = 0.004585434682667255
iteration 263, loss = 0.004274157807230949
iteration 264, loss = 0.004471396561712027
iteration 265, loss = 0.005507257767021656
iteration 266, loss = 0.004367985296994448
iteration 267, loss = 0.005154154263436794
iteration 268, loss = 0.004267048556357622
iteration 269, loss = 0.006013199687004089
iteration 270, loss = 0.004433513153344393
iteration 271, loss = 0.004419694654643536
iteration 272, loss = 0.0042250799015164375
iteration 273, loss = 0.004709287546575069
iteration 274, loss = 0.004137828946113586
iteration 275, loss = 0.006177934817969799
iteration 276, loss = 0.0069742766208946705
iteration 277, loss = 0.004172830376774073
iteration 278, loss = 0.004175330046564341
iteration 279, loss = 0.004687155596911907
iteration 280, loss = 0.00405894685536623
iteration 281, loss = 0.005639426410198212
iteration 282, loss = 0.004610398784279823
iteration 283, loss = 0.00496950326487422
iteration 284, loss = 0.004239737056195736
iteration 285, loss = 0.004500719718635082
iteration 286, loss = 0.004125256557017565
iteration 287, loss = 0.009010497480630875
iteration 288, loss = 0.0037788699846714735
iteration 289, loss = 0.006341950502246618
iteration 290, loss = 0.0043789600022137165
iteration 291, loss = 0.00584680400788784
iteration 292, loss = 0.004415248986333609
iteration 293, loss = 0.007296435534954071
iteration 294, loss = 0.005231713410466909
iteration 295, loss = 0.004234824329614639
iteration 296, loss = 0.0038525930140167475
iteration 297, loss = 0.004287978634238243
iteration 298, loss = 0.005789890885353088
iteration 299, loss = 0.0047351219691336155
iteration 300, loss = 0.004188581835478544
iteration 1, loss = 0.0046537453308701515
iteration 2, loss = 0.004004538990557194
iteration 3, loss = 0.0041268328204751015
iteration 4, loss = 0.005403868854045868
iteration 5, loss = 0.007277100346982479
iteration 6, loss = 0.00547340651974082
iteration 7, loss = 0.00487900385633111
iteration 8, loss = 0.006230479571968317
iteration 9, loss = 0.0058843279257416725
iteration 10, loss = 0.004155573435127735
iteration 11, loss = 0.004380303900688887
iteration 12, loss = 0.004156886599957943
iteration 13, loss = 0.004310070537030697
iteration 14, loss = 0.0057678138837218285
iteration 15, loss = 0.00576319033280015
iteration 16, loss = 0.004001254215836525
iteration 17, loss = 0.005199868697673082
iteration 18, loss = 0.004700490739196539
iteration 19, loss = 0.004036486614495516
iteration 20, loss = 0.007364732213318348
iteration 21, loss = 0.0044015273451805115
iteration 22, loss = 0.004418348893523216
iteration 23, loss = 0.004327672533690929
iteration 24, loss = 0.004118523560464382
iteration 25, loss = 0.003947088029235601
iteration 26, loss = 0.004551814869046211
iteration 27, loss = 0.0037862977478653193
iteration 28, loss = 0.004191604908555746
iteration 29, loss = 0.004240105859935284
iteration 30, loss = 0.004117618780583143
iteration 31, loss = 0.004073029384016991
iteration 32, loss = 0.004461084492504597
iteration 33, loss = 0.004040752537548542
iteration 34, loss = 0.004493900574743748
iteration 35, loss = 0.006972399074584246
iteration 36, loss = 0.005991802550852299
iteration 37, loss = 0.004391989670693874
iteration 38, loss = 0.007682546973228455
iteration 39, loss = 0.004203599877655506
iteration 40, loss = 0.004280877765268087
iteration 41, loss = 0.004598223604261875
iteration 42, loss = 0.004400753416121006
iteration 43, loss = 0.004052639473229647
iteration 44, loss = 0.004559928085654974
iteration 45, loss = 0.004272817634046078
iteration 46, loss = 0.004984351806342602
iteration 47, loss = 0.004090433474630117
iteration 48, loss = 0.0050744288600981236
iteration 49, loss = 0.004153857938945293
iteration 50, loss = 0.004329348914325237
iteration 51, loss = 0.004128673113882542
iteration 52, loss = 0.004038267768919468
iteration 53, loss = 0.00515751400962472
iteration 54, loss = 0.0067507619969546795
iteration 55, loss = 0.007254310883581638
iteration 56, loss = 0.004455992020666599
iteration 57, loss = 0.0040936158038675785
iteration 58, loss = 0.0043773530051112175
iteration 59, loss = 0.004135865718126297
iteration 60, loss = 0.004937849473208189
iteration 61, loss = 0.004359598737210035
iteration 62, loss = 0.004717635456472635
iteration 63, loss = 0.0061092684045434
iteration 64, loss = 0.004755458794534206
iteration 65, loss = 0.004035172052681446
iteration 66, loss = 0.004473860375583172
iteration 67, loss = 0.0038138311356306076
iteration 68, loss = 0.004064666107296944
iteration 69, loss = 0.005383486859500408
iteration 70, loss = 0.004700162913650274
iteration 71, loss = 0.004679552279412746
iteration 72, loss = 0.00575614906847477
iteration 73, loss = 0.0046248603612184525
iteration 74, loss = 0.004629047121852636
iteration 75, loss = 0.004142701160162687
iteration 76, loss = 0.004982326179742813
iteration 77, loss = 0.005840543657541275
iteration 78, loss = 0.004349971190094948
iteration 79, loss = 0.00413495535030961
iteration 80, loss = 0.00564192607998848
iteration 81, loss = 0.005200706887990236
iteration 82, loss = 0.004772709682583809
iteration 83, loss = 0.004407425411045551
iteration 84, loss = 0.0050574480555951595
iteration 85, loss = 0.009973423555493355
iteration 86, loss = 0.004531408194452524
iteration 87, loss = 0.004210442770272493
iteration 88, loss = 0.004004272632300854
iteration 89, loss = 0.00405660318210721
iteration 90, loss = 0.005349650979042053
iteration 91, loss = 0.005264689214527607
iteration 92, loss = 0.004885620903223753
iteration 93, loss = 0.0038499836809933186
iteration 94, loss = 0.005924789234995842
iteration 95, loss = 0.005929849576205015
iteration 96, loss = 0.006276533007621765
iteration 97, loss = 0.004280621651560068
iteration 98, loss = 0.004701077938079834
iteration 99, loss = 0.004375689197331667
iteration 100, loss = 0.004109262954443693
iteration 101, loss = 0.004087530542165041
iteration 102, loss = 0.005354238674044609
iteration 103, loss = 0.004686293192207813
iteration 104, loss = 0.0041786073707044125
iteration 105, loss = 0.004183490760624409
iteration 106, loss = 0.0040335627272725105
iteration 107, loss = 0.006054362747818232
iteration 108, loss = 0.004160492215305567
iteration 109, loss = 0.0041962312534451485
iteration 110, loss = 0.005455962382256985
iteration 111, loss = 0.005382280796766281
iteration 112, loss = 0.004540052730590105
iteration 113, loss = 0.004156037699431181
iteration 114, loss = 0.004108385182917118
iteration 115, loss = 0.004108537919819355
iteration 116, loss = 0.0043376474641263485
iteration 117, loss = 0.004708459135144949
iteration 118, loss = 0.004155069589614868
iteration 119, loss = 0.004072877112776041
iteration 120, loss = 0.004204757045954466
iteration 121, loss = 0.0038227159529924393
iteration 122, loss = 0.005890466272830963
iteration 123, loss = 0.00435372069478035
iteration 124, loss = 0.004377309698611498
iteration 125, loss = 0.004487636964768171
iteration 126, loss = 0.0043740663677453995
iteration 127, loss = 0.004398568067699671
iteration 128, loss = 0.007054831832647324
iteration 129, loss = 0.004092479590326548
iteration 130, loss = 0.00453020166605711
iteration 131, loss = 0.00397837907075882
iteration 132, loss = 0.004126380197703838
iteration 133, loss = 0.004385117907077074
iteration 134, loss = 0.004409070126712322
iteration 135, loss = 0.0038756895810365677
iteration 136, loss = 0.004836282227188349
iteration 137, loss = 0.006905869115144014
iteration 138, loss = 0.00533802667632699
iteration 139, loss = 0.003892420092597604
iteration 140, loss = 0.004061560146510601
iteration 141, loss = 0.005021702032536268
iteration 142, loss = 0.004422366619110107
iteration 143, loss = 0.004263320006430149
iteration 144, loss = 0.003764296183362603
iteration 145, loss = 0.0049668410792946815
iteration 146, loss = 0.0043759713880717754
iteration 147, loss = 0.005944164004176855
iteration 148, loss = 0.004453878849744797
iteration 149, loss = 0.004302738234400749
iteration 150, loss = 0.004195838700979948
iteration 151, loss = 0.003905611578375101
iteration 152, loss = 0.004024937748908997
iteration 153, loss = 0.006154379341751337
iteration 154, loss = 0.004554174840450287
iteration 155, loss = 0.00527938362210989
iteration 156, loss = 0.004702121019363403
iteration 157, loss = 0.004183552227914333
iteration 158, loss = 0.004474422428756952
iteration 159, loss = 0.004333074204623699
iteration 160, loss = 0.004944636020809412
iteration 161, loss = 0.004372221417725086
iteration 162, loss = 0.004876530729234219
iteration 163, loss = 0.004481268115341663
iteration 164, loss = 0.006300607696175575
iteration 165, loss = 0.004559393506497145
iteration 166, loss = 0.004582498222589493
iteration 167, loss = 0.005511486902832985
iteration 168, loss = 0.00456975819543004
iteration 169, loss = 0.005940326955169439
iteration 170, loss = 0.005989182274788618
iteration 171, loss = 0.004183527082204819
iteration 172, loss = 0.006257986184209585
iteration 173, loss = 0.004485498182475567
iteration 174, loss = 0.004512092098593712
iteration 175, loss = 0.005020614713430405
iteration 176, loss = 0.004744149278849363
iteration 177, loss = 0.004042281769216061
iteration 178, loss = 0.005755143240094185
iteration 179, loss = 0.004616024438291788
iteration 180, loss = 0.005606318823993206
iteration 181, loss = 0.004405701067298651
iteration 182, loss = 0.0040387921035289764
iteration 183, loss = 0.004089890513569117
iteration 184, loss = 0.004052634816616774
iteration 185, loss = 0.004447295796126127
iteration 186, loss = 0.004733688663691282
iteration 187, loss = 0.003906401805579662
iteration 188, loss = 0.004227176308631897
iteration 189, loss = 0.004098227247595787
iteration 190, loss = 0.004146158695220947
iteration 191, loss = 0.003727302420884371
iteration 192, loss = 0.004234249237924814
iteration 193, loss = 0.004222827963531017
iteration 194, loss = 0.0041582174599170685
iteration 195, loss = 0.004166170954704285
iteration 196, loss = 0.0042045265436172485
iteration 197, loss = 0.007154679391533136
iteration 198, loss = 0.0038674117531627417
iteration 199, loss = 0.004208721686154604
iteration 200, loss = 0.00412038853392005
iteration 201, loss = 0.005891522858291864
iteration 202, loss = 0.00441586272791028
iteration 203, loss = 0.005474023520946503
iteration 204, loss = 0.0059773921966552734
iteration 205, loss = 0.004128580912947655
iteration 206, loss = 0.0047457716427743435
iteration 207, loss = 0.005829407833516598
iteration 208, loss = 0.008208136074244976
iteration 209, loss = 0.004475489258766174
iteration 210, loss = 0.005127275362610817
iteration 211, loss = 0.00398248853161931
iteration 212, loss = 0.004378262907266617
iteration 213, loss = 0.0045139724388718605
iteration 214, loss = 0.004421151243150234
iteration 215, loss = 0.004139556549489498
iteration 216, loss = 0.004107916262000799
iteration 217, loss = 0.005666603334248066
iteration 218, loss = 0.007988004945218563
iteration 219, loss = 0.004276870749890804
iteration 220, loss = 0.0041036056354641914
iteration 221, loss = 0.004367595538496971
iteration 222, loss = 0.005241582170128822
iteration 223, loss = 0.00412840535864234
iteration 224, loss = 0.005727238487452269
iteration 225, loss = 0.004435569979250431
iteration 226, loss = 0.005876064300537109
iteration 227, loss = 0.004209962673485279
iteration 228, loss = 0.004607947077602148
iteration 229, loss = 0.0041534435003995895
iteration 230, loss = 0.00407289108261466
iteration 231, loss = 0.004072761628776789
iteration 232, loss = 0.004050235729664564
iteration 233, loss = 0.0041040382348001
iteration 234, loss = 0.005808458663523197
iteration 235, loss = 0.0056875767186284065
iteration 236, loss = 0.004681416787207127
iteration 237, loss = 0.006701244972646236
iteration 238, loss = 0.004116899333894253
iteration 239, loss = 0.004134675022214651
iteration 240, loss = 0.004362914245575666
iteration 241, loss = 0.00644935667514801
iteration 242, loss = 0.004312288016080856
iteration 243, loss = 0.004076860845088959
iteration 244, loss = 0.00406765379011631
iteration 245, loss = 0.0041014645248651505
iteration 246, loss = 0.004092371091246605
iteration 247, loss = 0.003948939964175224
iteration 248, loss = 0.005379904992878437
iteration 249, loss = 0.004560702480375767
iteration 250, loss = 0.004189819097518921
iteration 251, loss = 0.004365170374512672
iteration 252, loss = 0.006240596063435078
iteration 253, loss = 0.00466581154614687
iteration 254, loss = 0.00685413321480155
iteration 255, loss = 0.004519434180110693
iteration 256, loss = 0.0046421135775744915
iteration 257, loss = 0.004583100322633982
iteration 258, loss = 0.006035651545971632
iteration 259, loss = 0.0076044402085244656
iteration 260, loss = 0.004707080312073231
iteration 261, loss = 0.0060244034975767136
iteration 262, loss = 0.004269933328032494
iteration 263, loss = 0.004288502037525177
iteration 264, loss = 0.004085146822035313
iteration 265, loss = 0.0035010571591556072
iteration 266, loss = 0.006935447454452515
iteration 267, loss = 0.003992294892668724
iteration 268, loss = 0.004442272242158651
iteration 269, loss = 0.004276517312973738
iteration 270, loss = 0.0044813379645347595
iteration 271, loss = 0.004794478882104158
iteration 272, loss = 0.004203691612929106
iteration 273, loss = 0.004427551757544279
iteration 274, loss = 0.004605376627296209
iteration 275, loss = 0.00421276455745101
iteration 276, loss = 0.004179127514362335
iteration 277, loss = 0.0038970629684627056
iteration 278, loss = 0.003659461159259081
iteration 279, loss = 0.004462292417883873
iteration 280, loss = 0.004527373239398003
iteration 281, loss = 0.004482160322368145
iteration 282, loss = 0.004355813842266798
iteration 283, loss = 0.0051514944061636925
iteration 284, loss = 0.00523131899535656
iteration 285, loss = 0.004229108802974224
iteration 286, loss = 0.0043675657361745834
iteration 287, loss = 0.004059528931975365
iteration 288, loss = 0.004124262370169163
iteration 289, loss = 0.006389167625457048
iteration 290, loss = 0.0044586192816495895
iteration 291, loss = 0.004656692501157522
iteration 292, loss = 0.003981233574450016
iteration 293, loss = 0.004267239943146706
iteration 294, loss = 0.004814600571990013
iteration 295, loss = 0.003999884705990553
iteration 296, loss = 0.004328485112637281
iteration 297, loss = 0.0057596927508711815
iteration 298, loss = 0.0037608477286994457
iteration 299, loss = 0.0042843385599553585
iteration 300, loss = 0.004115107469260693
iteration 1, loss = 0.00693636666983366
iteration 2, loss = 0.004149450454860926
iteration 3, loss = 0.006114470772445202
iteration 4, loss = 0.006046051159501076
iteration 5, loss = 0.005339149385690689
iteration 6, loss = 0.004532641731202602
iteration 7, loss = 0.004139226861298084
iteration 8, loss = 0.004534727893769741
iteration 9, loss = 0.004457203671336174
iteration 10, loss = 0.00503222132101655
iteration 11, loss = 0.004552987404167652
iteration 12, loss = 0.004565887618809938
iteration 13, loss = 0.0045267026871442795
iteration 14, loss = 0.008260803297162056
iteration 15, loss = 0.004534400999546051
iteration 16, loss = 0.004073744639754295
iteration 17, loss = 0.003990981727838516
iteration 18, loss = 0.004727155901491642
iteration 19, loss = 0.005172914359718561
iteration 20, loss = 0.004236386623233557
iteration 21, loss = 0.004311881493777037
iteration 22, loss = 0.0043798647820949554
iteration 23, loss = 0.005783825181424618
iteration 24, loss = 0.0035948371514678
iteration 25, loss = 0.005480156280100346
iteration 26, loss = 0.0040629347786307335
iteration 27, loss = 0.0039615207351744175
iteration 28, loss = 0.0041352021507918835
iteration 29, loss = 0.004229567013680935
iteration 30, loss = 0.004300191067159176
iteration 31, loss = 0.0060903821140527725
iteration 32, loss = 0.004236087203025818
iteration 33, loss = 0.004419873468577862
iteration 34, loss = 0.00413660891354084
iteration 35, loss = 0.006253378000110388
iteration 36, loss = 0.003945708274841309
iteration 37, loss = 0.004081502556800842
iteration 38, loss = 0.004130624700337648
iteration 39, loss = 0.004135355353355408
iteration 40, loss = 0.0057543502189219
iteration 41, loss = 0.004178294446319342
iteration 42, loss = 0.004315602593123913
iteration 43, loss = 0.005032800137996674
iteration 44, loss = 0.004160301294177771
iteration 45, loss = 0.004226203076541424
iteration 46, loss = 0.004896985366940498
iteration 47, loss = 0.004442065488547087
iteration 48, loss = 0.004127495922148228
iteration 49, loss = 0.006839653942734003
iteration 50, loss = 0.005708078388124704
iteration 51, loss = 0.0039174435660243034
iteration 52, loss = 0.004345405381172895
iteration 53, loss = 0.004045697394758463
iteration 54, loss = 0.004440648946911097
iteration 55, loss = 0.0043957969173789024
iteration 56, loss = 0.004621087573468685
iteration 57, loss = 0.004109591245651245
iteration 58, loss = 0.004265537019819021
iteration 59, loss = 0.005081469658762217
iteration 60, loss = 0.0066823167726397514
iteration 61, loss = 0.00422186404466629
iteration 62, loss = 0.004450459964573383
iteration 63, loss = 0.004962343722581863
iteration 64, loss = 0.004128114320337772
iteration 65, loss = 0.004880133550614119
iteration 66, loss = 0.005882726516574621
iteration 67, loss = 0.003919913433492184
iteration 68, loss = 0.004325931891798973
iteration 69, loss = 0.006186679005622864
iteration 70, loss = 0.0038646976463496685
iteration 71, loss = 0.0059853969141840935
iteration 72, loss = 0.004551523365080357
iteration 73, loss = 0.0043757325038313866
iteration 74, loss = 0.005503969267010689
iteration 75, loss = 0.0040591382421553135
iteration 76, loss = 0.003994654398411512
iteration 77, loss = 0.006358458660542965
iteration 78, loss = 0.004054977558553219
iteration 79, loss = 0.004666323307901621
iteration 80, loss = 0.003672895720228553
iteration 81, loss = 0.004118938930332661
iteration 82, loss = 0.004587093833833933
iteration 83, loss = 0.004810242913663387
iteration 84, loss = 0.004264451563358307
iteration 85, loss = 0.005855707451701164
iteration 86, loss = 0.0038649418856948614
iteration 87, loss = 0.003953598439693451
iteration 88, loss = 0.0051124547608196735
iteration 89, loss = 0.005044582765549421
iteration 90, loss = 0.0044522713869810104
iteration 91, loss = 0.003888130886480212
iteration 92, loss = 0.004532153252512217
iteration 93, loss = 0.0039997398853302
iteration 94, loss = 0.004583884961903095
iteration 95, loss = 0.004393206909298897
iteration 96, loss = 0.004010126460343599
iteration 97, loss = 0.003825289197266102
iteration 98, loss = 0.00434749573469162
iteration 99, loss = 0.00485586142167449
iteration 100, loss = 0.004689367953687906
iteration 101, loss = 0.004857846535742283
iteration 102, loss = 0.005740983411669731
iteration 103, loss = 0.0045159608125686646
iteration 104, loss = 0.004472469910979271
iteration 105, loss = 0.004140603821724653
iteration 106, loss = 0.004763239994645119
iteration 107, loss = 0.0035391298588365316
iteration 108, loss = 0.004412953741848469
iteration 109, loss = 0.004541393835097551
iteration 110, loss = 0.004853471182286739
iteration 111, loss = 0.0043794517405331135
iteration 112, loss = 0.004000881686806679
iteration 113, loss = 0.005538348574191332
iteration 114, loss = 0.004060474224388599
iteration 115, loss = 0.005942299496382475
iteration 116, loss = 0.005600627977401018
iteration 117, loss = 0.0044601853005588055
iteration 118, loss = 0.004467658698558807
iteration 119, loss = 0.00434952462092042
iteration 120, loss = 0.004639417864382267
iteration 121, loss = 0.005925077944993973
iteration 122, loss = 0.004129167180508375
iteration 123, loss = 0.0043050628155469894
iteration 124, loss = 0.005912510212510824
iteration 125, loss = 0.004204948432743549
iteration 126, loss = 0.003916674759238958
iteration 127, loss = 0.004206808749586344
iteration 128, loss = 0.0044167181476950645
iteration 129, loss = 0.0059927101247012615
iteration 130, loss = 0.005573073402047157
iteration 131, loss = 0.003968338016420603
iteration 132, loss = 0.004691872745752335
iteration 133, loss = 0.004363005515187979
iteration 134, loss = 0.004086996428668499
iteration 135, loss = 0.004036000929772854
iteration 136, loss = 0.004182073753327131
iteration 137, loss = 0.004229675512760878
iteration 138, loss = 0.004362877458333969
iteration 139, loss = 0.005274709314107895
iteration 140, loss = 0.004218508489429951
iteration 141, loss = 0.0060285842046141624
iteration 142, loss = 0.004090501461178064
iteration 143, loss = 0.0058408938348293304
iteration 144, loss = 0.004292807076126337
iteration 145, loss = 0.00381064391694963
iteration 146, loss = 0.006089451722800732
iteration 147, loss = 0.004322105553001165
iteration 148, loss = 0.003915914800018072
iteration 149, loss = 0.006831809878349304
iteration 150, loss = 0.0043941643089056015
iteration 151, loss = 0.003990299999713898
iteration 152, loss = 0.004001271910965443
iteration 153, loss = 0.004154540132731199
iteration 154, loss = 0.004245004151016474
iteration 155, loss = 0.004635788965970278
iteration 156, loss = 0.007290672045201063
iteration 157, loss = 0.0039208740927278996
iteration 158, loss = 0.005701310932636261
iteration 159, loss = 0.0051326267421245575
iteration 160, loss = 0.004908878821879625
iteration 161, loss = 0.0038723554462194443
iteration 162, loss = 0.006898604799062014
iteration 163, loss = 0.004423575010150671
iteration 164, loss = 0.004493502900004387
iteration 165, loss = 0.006881418637931347
iteration 166, loss = 0.004156800452619791
iteration 167, loss = 0.0045233312994241714
iteration 168, loss = 0.004278877750039101
iteration 169, loss = 0.004410030785948038
iteration 170, loss = 0.00501773227006197
iteration 171, loss = 0.005297214724123478
iteration 172, loss = 0.00434116180986166
iteration 173, loss = 0.004429859574884176
iteration 174, loss = 0.005180997308343649
iteration 175, loss = 0.009056290611624718
iteration 176, loss = 0.0063094631768763065
iteration 177, loss = 0.004297120030969381
iteration 178, loss = 0.0056707728654146194
iteration 179, loss = 0.004253467079252005
iteration 180, loss = 0.005382746458053589
iteration 181, loss = 0.003933935426175594
iteration 182, loss = 0.003980659414082766
iteration 183, loss = 0.005712917074561119
iteration 184, loss = 0.004235709086060524
iteration 185, loss = 0.006862719543278217
iteration 186, loss = 0.004034008830785751
iteration 187, loss = 0.004194123670458794
iteration 188, loss = 0.004583091475069523
iteration 189, loss = 0.0049094464629888535
iteration 190, loss = 0.004064181353896856
iteration 191, loss = 0.005582196172326803
iteration 192, loss = 0.004038334358483553
iteration 193, loss = 0.004416691605001688
iteration 194, loss = 0.004503780510276556
iteration 195, loss = 0.00421744491904974
iteration 196, loss = 0.004273586440831423
iteration 197, loss = 0.005821848288178444
iteration 198, loss = 0.0052968342788517475
iteration 199, loss = 0.0042223199270665646
iteration 200, loss = 0.004352171439677477
iteration 201, loss = 0.004584990441799164
iteration 202, loss = 0.003976026549935341
iteration 203, loss = 0.004358459264039993
iteration 204, loss = 0.003966673742979765
iteration 205, loss = 0.003927700221538544
iteration 206, loss = 0.004580385517328978
iteration 207, loss = 0.004035403486341238
iteration 208, loss = 0.006908139679580927
iteration 209, loss = 0.0060308268293738365
iteration 210, loss = 0.004474172368645668
iteration 211, loss = 0.004469152074307203
iteration 212, loss = 0.004478679038584232
iteration 213, loss = 0.004620858933776617
iteration 214, loss = 0.004721649456769228
iteration 215, loss = 0.007335767149925232
iteration 216, loss = 0.004334409721195698
iteration 217, loss = 0.007288346067070961
iteration 218, loss = 0.005317277275025845
iteration 219, loss = 0.004516193643212318
iteration 220, loss = 0.004463016986846924
iteration 221, loss = 0.004218746908009052
iteration 222, loss = 0.003814781317487359
iteration 223, loss = 0.0068299961276352406
iteration 224, loss = 0.004657758865505457
iteration 225, loss = 0.004310161340981722
iteration 226, loss = 0.004430415574461222
iteration 227, loss = 0.004051223862916231
iteration 228, loss = 0.004250951111316681
iteration 229, loss = 0.004155735485255718
iteration 230, loss = 0.005611867178231478
iteration 231, loss = 0.004355128388851881
iteration 232, loss = 0.005109657533466816
iteration 233, loss = 0.003956897649914026
iteration 234, loss = 0.004077928606420755
iteration 235, loss = 0.007132752798497677
iteration 236, loss = 0.003954383544623852
iteration 237, loss = 0.004642977379262447
iteration 238, loss = 0.004776882939040661
iteration 239, loss = 0.00398227060213685
iteration 240, loss = 0.004715815652161837
iteration 241, loss = 0.0038379915058612823
iteration 242, loss = 0.004271282348781824
iteration 243, loss = 0.00521891051903367
iteration 244, loss = 0.004079984035342932
iteration 245, loss = 0.004361470229923725
iteration 246, loss = 0.004475500900298357
iteration 247, loss = 0.0057836552150547504
iteration 248, loss = 0.004665619693696499
iteration 249, loss = 0.0040298448875546455
iteration 250, loss = 0.005972838029265404
iteration 251, loss = 0.004289240576326847
iteration 252, loss = 0.004304165486246347
iteration 253, loss = 0.004363086074590683
iteration 254, loss = 0.004377592820674181
iteration 255, loss = 0.00406287144869566
iteration 256, loss = 0.004315201658755541
iteration 257, loss = 0.004049348644912243
iteration 258, loss = 0.005845673382282257
iteration 259, loss = 0.00463414192199707
iteration 260, loss = 0.004209239035844803
iteration 261, loss = 0.003909099381417036
iteration 262, loss = 0.0059643154963850975
iteration 263, loss = 0.0039605991914868355
iteration 264, loss = 0.0034266668371856213
iteration 265, loss = 0.003945155069231987
iteration 266, loss = 0.004620854277163744
iteration 267, loss = 0.004816168919205666
iteration 268, loss = 0.004347848705947399
iteration 269, loss = 0.0058629573322832584
iteration 270, loss = 0.004014784004539251
iteration 271, loss = 0.004187944810837507
iteration 272, loss = 0.005125250667333603
iteration 273, loss = 0.005197677295655012
iteration 274, loss = 0.004188035614788532
iteration 275, loss = 0.003933179657906294
iteration 276, loss = 0.004368520807474852
iteration 277, loss = 0.004405772313475609
iteration 278, loss = 0.004414783325046301
iteration 279, loss = 0.005576742347329855
iteration 280, loss = 0.0045287711545825005
iteration 281, loss = 0.004531665705144405
iteration 282, loss = 0.006356508936733007
iteration 283, loss = 0.007834809832274914
iteration 284, loss = 0.0038621944840997458
iteration 285, loss = 0.004140008706599474
iteration 286, loss = 0.005312041379511356
iteration 287, loss = 0.006999632343649864
iteration 288, loss = 0.003917729016393423
iteration 289, loss = 0.004646947141736746
iteration 290, loss = 0.004018416628241539
iteration 291, loss = 0.004490538965910673
iteration 292, loss = 0.0045745596289634705
iteration 293, loss = 0.0044258637353777885
iteration 294, loss = 0.006000355817377567
iteration 295, loss = 0.005003709811717272
iteration 296, loss = 0.004405983258038759
iteration 297, loss = 0.005364242009818554
iteration 298, loss = 0.004018348641693592
iteration 299, loss = 0.004519131034612656
iteration 300, loss = 0.005031850188970566
iteration 1, loss = 0.007009429391473532
iteration 2, loss = 0.004461737349629402
iteration 3, loss = 0.0038679465651512146
iteration 4, loss = 0.004256401676684618
iteration 5, loss = 0.004336338024586439
iteration 6, loss = 0.006391569040715694
iteration 7, loss = 0.005351327359676361
iteration 8, loss = 0.005474140867590904
iteration 9, loss = 0.004059403669089079
iteration 10, loss = 0.006006260868161917
iteration 11, loss = 0.005319558084011078
iteration 12, loss = 0.003653596853837371
iteration 13, loss = 0.004471964202821255
iteration 14, loss = 0.004230711609125137
iteration 15, loss = 0.0056326910853385925
iteration 16, loss = 0.0048783146776258945
iteration 17, loss = 0.004076060838997364
iteration 18, loss = 0.005015789996832609
iteration 19, loss = 0.0040889158844947815
iteration 20, loss = 0.003894559107720852
iteration 21, loss = 0.004342906177043915
iteration 22, loss = 0.004071501549333334
iteration 23, loss = 0.004490809515118599
iteration 24, loss = 0.00391286239027977
iteration 25, loss = 0.00450567901134491
iteration 26, loss = 0.004225421696901321
iteration 27, loss = 0.004229272250086069
iteration 28, loss = 0.004215025343000889
iteration 29, loss = 0.0040504843927919865
iteration 30, loss = 0.004297933541238308
iteration 31, loss = 0.004137271083891392
iteration 32, loss = 0.0039989883080124855
iteration 33, loss = 0.004436849150806665
iteration 34, loss = 0.004079157952219248
iteration 35, loss = 0.0041731889359653
iteration 36, loss = 0.004229975864291191
iteration 37, loss = 0.004356347024440765
iteration 38, loss = 0.004572478123009205
iteration 39, loss = 0.004574791993945837
iteration 40, loss = 0.004087231121957302
iteration 41, loss = 0.0051940917037427425
iteration 42, loss = 0.007004592102020979
iteration 43, loss = 0.004568937700241804
iteration 44, loss = 0.005857065320014954
iteration 45, loss = 0.0038374275900423527
iteration 46, loss = 0.004332514014095068
iteration 47, loss = 0.00445055216550827
iteration 48, loss = 0.004548751283437014
iteration 49, loss = 0.004095843061804771
iteration 50, loss = 0.004432212095707655
iteration 51, loss = 0.004159546922892332
iteration 52, loss = 0.004749770741909742
iteration 53, loss = 0.004139374941587448
iteration 54, loss = 0.005891751032322645
iteration 55, loss = 0.005860454402863979
iteration 56, loss = 0.0040303622372448444
iteration 57, loss = 0.0068503934890031815
iteration 58, loss = 0.004315831698477268
iteration 59, loss = 0.004604482091963291
iteration 60, loss = 0.003973058890551329
iteration 61, loss = 0.004420758690685034
iteration 62, loss = 0.006156780291348696
iteration 63, loss = 0.004101725295186043
iteration 64, loss = 0.004252322483807802
iteration 65, loss = 0.00736750615760684
iteration 66, loss = 0.004143637605011463
iteration 67, loss = 0.005189897958189249
iteration 68, loss = 0.004176049027591944
iteration 69, loss = 0.004022266715764999
iteration 70, loss = 0.004885050002485514
iteration 71, loss = 0.004912823438644409
iteration 72, loss = 0.004546227399259806
iteration 73, loss = 0.004365045577287674
iteration 74, loss = 0.010329843498766422
iteration 75, loss = 0.004069829825311899
iteration 76, loss = 0.004986162297427654
iteration 77, loss = 0.0040414039976894855
iteration 78, loss = 0.004614321980625391
iteration 79, loss = 0.0038695489056408405
iteration 80, loss = 0.005393468774855137
iteration 81, loss = 0.004550309851765633
iteration 82, loss = 0.004593563266098499
iteration 83, loss = 0.004329872317612171
iteration 84, loss = 0.004127830266952515
iteration 85, loss = 0.0043369559571146965
iteration 86, loss = 0.005629225634038448
iteration 87, loss = 0.00545109948143363
iteration 88, loss = 0.004564046394079924
iteration 89, loss = 0.0064490847289562225
iteration 90, loss = 0.0039308397099375725
iteration 91, loss = 0.005764787085354328
iteration 92, loss = 0.006011143792420626
iteration 93, loss = 0.004736125934869051
iteration 94, loss = 0.004703615326434374
iteration 95, loss = 0.004838764201849699
iteration 96, loss = 0.00397503050044179
iteration 97, loss = 0.004180064890533686
iteration 98, loss = 0.005360274575650692
iteration 99, loss = 0.0043069603852927685
iteration 100, loss = 0.00988058466464281
iteration 101, loss = 0.003980481997132301
iteration 102, loss = 0.004310752265155315
iteration 103, loss = 0.0063806502148509026
iteration 104, loss = 0.004050374496728182
iteration 105, loss = 0.004349787253886461
iteration 106, loss = 0.004692459478974342
iteration 107, loss = 0.004452803172171116
iteration 108, loss = 0.004213820677250624
iteration 109, loss = 0.004325869493186474
iteration 110, loss = 0.004023339133709669
iteration 111, loss = 0.004046066664159298
iteration 112, loss = 0.004270093981176615
iteration 113, loss = 0.007089462596923113
iteration 114, loss = 0.004751966334879398
iteration 115, loss = 0.004482744261622429
iteration 116, loss = 0.004766974598169327
iteration 117, loss = 0.004483051598072052
iteration 118, loss = 0.004197950940579176
iteration 119, loss = 0.0052544367499649525
iteration 120, loss = 0.004764528013765812
iteration 121, loss = 0.004096049349755049
iteration 122, loss = 0.005106940865516663
iteration 123, loss = 0.004192753229290247
iteration 124, loss = 0.00713167991489172
iteration 125, loss = 0.004175812005996704
iteration 126, loss = 0.0047289347276091576
iteration 127, loss = 0.005041100550442934
iteration 128, loss = 0.00407298794016242
iteration 129, loss = 0.004760657902806997
iteration 130, loss = 0.004097695462405682
iteration 131, loss = 0.004820993170142174
iteration 132, loss = 0.004738129675388336
iteration 133, loss = 0.0037976615130901337
iteration 134, loss = 0.00442232470959425
iteration 135, loss = 0.0042360094375908375
iteration 136, loss = 0.004570902790874243
iteration 137, loss = 0.0038363714702427387
iteration 138, loss = 0.005047920159995556
iteration 139, loss = 0.00400962121784687
iteration 140, loss = 0.003812504466623068
iteration 141, loss = 0.003832103218883276
iteration 142, loss = 0.004230752121657133
iteration 143, loss = 0.004295254126191139
iteration 144, loss = 0.005126151721924543
iteration 145, loss = 0.004491482861340046
iteration 146, loss = 0.005580507218837738
iteration 147, loss = 0.00443925429135561
iteration 148, loss = 0.0035952767357230186
iteration 149, loss = 0.00460434565320611
iteration 150, loss = 0.004084453452378511
iteration 151, loss = 0.0044219521805644035
iteration 152, loss = 0.0041175177320837975
iteration 153, loss = 0.005652669817209244
iteration 154, loss = 0.00552309351041913
iteration 155, loss = 0.003955656662583351
iteration 156, loss = 0.004416503943502903
iteration 157, loss = 0.004573938436806202
iteration 158, loss = 0.004623865243047476
iteration 159, loss = 0.005742577835917473
iteration 160, loss = 0.007246248424053192
iteration 161, loss = 0.004377597011625767
iteration 162, loss = 0.004136000759899616
iteration 163, loss = 0.0049065519124269485
iteration 164, loss = 0.00410762382671237
iteration 165, loss = 0.00454898364841938
iteration 166, loss = 0.00412989966571331
iteration 167, loss = 0.004040923435240984
iteration 168, loss = 0.00460389256477356
iteration 169, loss = 0.006849886383861303
iteration 170, loss = 0.005794598255306482
iteration 171, loss = 0.005918520502746105
iteration 172, loss = 0.01058127824217081
iteration 173, loss = 0.004350773990154266
iteration 174, loss = 0.004109809175133705
iteration 175, loss = 0.0059525975957512856
iteration 176, loss = 0.006096848752349615
iteration 177, loss = 0.007170626427978277
iteration 178, loss = 0.004105741158127785
iteration 179, loss = 0.004510677885264158
iteration 180, loss = 0.003941297065466642
iteration 181, loss = 0.006156536750495434
iteration 182, loss = 0.004196519963443279
iteration 183, loss = 0.00408791983500123
iteration 184, loss = 0.0038612361531704664
iteration 185, loss = 0.005369844380766153
iteration 186, loss = 0.0052780574187636375
iteration 187, loss = 0.004423386882990599
iteration 188, loss = 0.004370367154479027
iteration 189, loss = 0.004849701188504696
iteration 190, loss = 0.003863271791487932
iteration 191, loss = 0.004042410757392645
iteration 192, loss = 0.0042355298064649105
iteration 193, loss = 0.0042590308003127575
iteration 194, loss = 0.004407474771142006
iteration 195, loss = 0.004360617138445377
iteration 196, loss = 0.004073589574545622
iteration 197, loss = 0.004196018911898136
iteration 198, loss = 0.004060330335050821
iteration 199, loss = 0.004062592051923275
iteration 200, loss = 0.004200534429401159
iteration 201, loss = 0.0039009260945022106
iteration 202, loss = 0.00392485735937953
iteration 203, loss = 0.004784581251442432
iteration 204, loss = 0.004255622159689665
iteration 205, loss = 0.004236753564327955
iteration 206, loss = 0.0046489848755300045
iteration 207, loss = 0.004338211379945278
iteration 208, loss = 0.005847265012562275
iteration 209, loss = 0.005099553149193525
iteration 210, loss = 0.004396532196551561
iteration 211, loss = 0.004966203588992357
iteration 212, loss = 0.004340574610978365
iteration 213, loss = 0.004342792555689812
iteration 214, loss = 0.004121793434023857
iteration 215, loss = 0.004052778240293264
iteration 216, loss = 0.004506174009293318
iteration 217, loss = 0.004444531165063381
iteration 218, loss = 0.004354138858616352
iteration 219, loss = 0.004331928677856922
iteration 220, loss = 0.003825585590675473
iteration 221, loss = 0.004176687449216843
iteration 222, loss = 0.004429791122674942
iteration 223, loss = 0.004234766121953726
iteration 224, loss = 0.003529320238158107
iteration 225, loss = 0.00683958362787962
iteration 226, loss = 0.004204567521810532
iteration 227, loss = 0.004240823443979025
iteration 228, loss = 0.004239643458276987
iteration 229, loss = 0.004233528394252062
iteration 230, loss = 0.004829462617635727
iteration 231, loss = 0.0042940182611346245
iteration 232, loss = 0.004107041750103235
iteration 233, loss = 0.004057345446199179
iteration 234, loss = 0.006194779649376869
iteration 235, loss = 0.007055764086544514
iteration 236, loss = 0.00400532828643918
iteration 237, loss = 0.004475361667573452
iteration 238, loss = 0.003968908917158842
iteration 239, loss = 0.0043258219957351685
iteration 240, loss = 0.005686518270522356
iteration 241, loss = 0.005373437888920307
iteration 242, loss = 0.0043432149104774
iteration 243, loss = 0.0045659043826162815
iteration 244, loss = 0.0042226118966937065
iteration 245, loss = 0.0066922209225595
iteration 246, loss = 0.004583149682730436
iteration 247, loss = 0.005439880769699812
iteration 248, loss = 0.003920230083167553
iteration 249, loss = 0.004242326598614454
iteration 250, loss = 0.004154254216700792
iteration 251, loss = 0.004018006380647421
iteration 252, loss = 0.006653392221778631
iteration 253, loss = 0.005515493452548981
iteration 254, loss = 0.004381396807730198
iteration 255, loss = 0.004539402201771736
iteration 256, loss = 0.004330693278461695
iteration 257, loss = 0.004522664938122034
iteration 258, loss = 0.005351470783352852
iteration 259, loss = 0.0041981530375778675
iteration 260, loss = 0.004048129078000784
iteration 261, loss = 0.004524752497673035
iteration 262, loss = 0.004188190214335918
iteration 263, loss = 0.004338411148637533
iteration 264, loss = 0.004281672649085522
iteration 265, loss = 0.004356739576905966
iteration 266, loss = 0.003974113613367081
iteration 267, loss = 0.004054553806781769
iteration 268, loss = 0.005445148330181837
iteration 269, loss = 0.00471000000834465
iteration 270, loss = 0.004116397816687822
iteration 271, loss = 0.004488697275519371
iteration 272, loss = 0.004759836941957474
iteration 273, loss = 0.004240953363478184
iteration 274, loss = 0.0051596094854176044
iteration 275, loss = 0.0061366744339466095
iteration 276, loss = 0.006774254608899355
iteration 277, loss = 0.005361621268093586
iteration 278, loss = 0.004382900893688202
iteration 279, loss = 0.0045464374125003815
iteration 280, loss = 0.004264224320650101
iteration 281, loss = 0.0043222601525485516
iteration 282, loss = 0.006100236438214779
iteration 283, loss = 0.004412195645272732
iteration 284, loss = 0.006264572963118553
iteration 285, loss = 0.004318363033235073
iteration 286, loss = 0.004403137601912022
iteration 287, loss = 0.004197807516902685
iteration 288, loss = 0.004128468222916126
iteration 289, loss = 0.0042437962256371975
iteration 290, loss = 0.0043533192947506905
iteration 291, loss = 0.004096768796443939
iteration 292, loss = 0.006923218257725239
iteration 293, loss = 0.0044853948056697845
iteration 294, loss = 0.006262913811951876
iteration 295, loss = 0.00472784461453557
iteration 296, loss = 0.005835917312651873
iteration 297, loss = 0.004157576709985733
iteration 298, loss = 0.00400781724601984
iteration 299, loss = 0.005097563844174147
iteration 300, loss = 0.004225734155625105
iteration 1, loss = 0.006720587611198425
iteration 2, loss = 0.004037185106426477
iteration 3, loss = 0.004468959756195545
iteration 4, loss = 0.004367689602077007
iteration 5, loss = 0.004165974911302328
iteration 6, loss = 0.003991208970546722
iteration 7, loss = 0.007076882757246494
iteration 8, loss = 0.00443061301484704
iteration 9, loss = 0.006005597300827503
iteration 10, loss = 0.0039678942412137985
iteration 11, loss = 0.004750681575387716
iteration 12, loss = 0.005271979607641697
iteration 13, loss = 0.0038597160018980503
iteration 14, loss = 0.003786565037444234
iteration 15, loss = 0.0040699937380850315
iteration 16, loss = 0.003867998719215393
iteration 17, loss = 0.00565575435757637
iteration 18, loss = 0.004681152291595936
iteration 19, loss = 0.0042198579758405685
iteration 20, loss = 0.0044956086203455925
iteration 21, loss = 0.006062913220375776
iteration 22, loss = 0.004029628820717335
iteration 23, loss = 0.0042800563387572765
iteration 24, loss = 0.005884283222258091
iteration 25, loss = 0.003728723619133234
iteration 26, loss = 0.004713862203061581
iteration 27, loss = 0.005517135374248028
iteration 28, loss = 0.004317035898566246
iteration 29, loss = 0.007190313655883074
iteration 30, loss = 0.004146927502006292
iteration 31, loss = 0.005116371437907219
iteration 32, loss = 0.0038521115202456713
iteration 33, loss = 0.005932920146733522
iteration 34, loss = 0.005807815585285425
iteration 35, loss = 0.003826386295258999
iteration 36, loss = 0.004443995654582977
iteration 37, loss = 0.00500663835555315
iteration 38, loss = 0.006972084753215313
iteration 39, loss = 0.003992625977844
iteration 40, loss = 0.0041654231026768684
iteration 41, loss = 0.004517649300396442
iteration 42, loss = 0.008202607743442059
iteration 43, loss = 0.004804876167327166
iteration 44, loss = 0.006999504752457142
iteration 45, loss = 0.004384331870824099
iteration 46, loss = 0.0040519447065889835
iteration 47, loss = 0.004062569234520197
iteration 48, loss = 0.004294696729630232
iteration 49, loss = 0.005056886468082666
iteration 50, loss = 0.004343431908637285
iteration 51, loss = 0.0038445990066975355
iteration 52, loss = 0.005948938895016909
iteration 53, loss = 0.0037002055905759335
iteration 54, loss = 0.004237984772771597
iteration 55, loss = 0.005715761799365282
iteration 56, loss = 0.004544165451079607
iteration 57, loss = 0.004456979688256979
iteration 58, loss = 0.004059943836182356
iteration 59, loss = 0.003794606076553464
iteration 60, loss = 0.005266257561743259
iteration 61, loss = 0.004291236400604248
iteration 62, loss = 0.004539683926850557
iteration 63, loss = 0.004008380230516195
iteration 64, loss = 0.004593259654939175
iteration 65, loss = 0.005654821638017893
iteration 66, loss = 0.005962252151221037
iteration 67, loss = 0.00408615916967392
iteration 68, loss = 0.0038675465621054173
iteration 69, loss = 0.004438900388777256
iteration 70, loss = 0.004358451347798109
iteration 71, loss = 0.004759589675813913
iteration 72, loss = 0.00562632828950882
iteration 73, loss = 0.006045586429536343
iteration 74, loss = 0.004849956836551428
iteration 75, loss = 0.003973796498030424
iteration 76, loss = 0.004385234322398901
iteration 77, loss = 0.004011655226349831
iteration 78, loss = 0.004631632007658482
iteration 79, loss = 0.00897106435149908
iteration 80, loss = 0.00479038804769516
iteration 81, loss = 0.004114807583391666
iteration 82, loss = 0.0043999627232551575
iteration 83, loss = 0.005300136748701334
iteration 84, loss = 0.006599388550966978
iteration 85, loss = 0.006120153237134218
iteration 86, loss = 0.005412801168859005
iteration 87, loss = 0.006428623106330633
iteration 88, loss = 0.004565660376101732
iteration 89, loss = 0.0040786671452224255
iteration 90, loss = 0.004092958755791187
iteration 91, loss = 0.004046462010592222
iteration 92, loss = 0.005103979725390673
iteration 93, loss = 0.00397729966789484
iteration 94, loss = 0.00439875852316618
iteration 95, loss = 0.005019557196646929
iteration 96, loss = 0.0054076965898275375
iteration 97, loss = 0.005165815353393555
iteration 98, loss = 0.005686861928552389
iteration 99, loss = 0.005075868219137192
iteration 100, loss = 0.004816866479814053
iteration 101, loss = 0.004667024128139019
iteration 102, loss = 0.0038652815856039524
iteration 103, loss = 0.0044591412879526615
iteration 104, loss = 0.004654062911868095
iteration 105, loss = 0.005625414662063122
iteration 106, loss = 0.0062620933167636395
iteration 107, loss = 0.004821639508008957
iteration 108, loss = 0.004359601065516472
iteration 109, loss = 0.0038570312317460775
iteration 110, loss = 0.0040962351486086845
iteration 111, loss = 0.004365336149930954
iteration 112, loss = 0.0070526571944355965
iteration 113, loss = 0.003932265564799309
iteration 114, loss = 0.005993820261210203
iteration 115, loss = 0.0042630648240447044
iteration 116, loss = 0.004326051101088524
iteration 117, loss = 0.004510761704295874
iteration 118, loss = 0.004186561331152916
iteration 119, loss = 0.004392893053591251
iteration 120, loss = 0.004069899208843708
iteration 121, loss = 0.004137760493904352
iteration 122, loss = 0.006487277802079916
iteration 123, loss = 0.004125355742871761
iteration 124, loss = 0.004344424232840538
iteration 125, loss = 0.0037373597733676434
iteration 126, loss = 0.004103833809494972
iteration 127, loss = 0.004372201859951019
iteration 128, loss = 0.00438561150804162
iteration 129, loss = 0.003990646917372942
iteration 130, loss = 0.004105648957192898
iteration 131, loss = 0.003942912444472313
iteration 132, loss = 0.004588309675455093
iteration 133, loss = 0.004305555485188961
iteration 134, loss = 0.004192125052213669
iteration 135, loss = 0.004367412067949772
iteration 136, loss = 0.0040562329813838005
iteration 137, loss = 0.0038921795785427094
iteration 138, loss = 0.00415382906794548
iteration 139, loss = 0.0044395471923053265
iteration 140, loss = 0.00409451499581337
iteration 141, loss = 0.004173691850155592
iteration 142, loss = 0.004209000617265701
iteration 143, loss = 0.004537579603493214
iteration 144, loss = 0.0043401774019002914
iteration 145, loss = 0.005327695515006781
iteration 146, loss = 0.004032307304441929
iteration 147, loss = 0.004390328656882048
iteration 148, loss = 0.004917772486805916
iteration 149, loss = 0.004660682752728462
iteration 150, loss = 0.00415259413421154
iteration 151, loss = 0.00435124896466732
iteration 152, loss = 0.004365080036222935
iteration 153, loss = 0.004001963417977095
iteration 154, loss = 0.005384347401559353
iteration 155, loss = 0.007252449169754982
iteration 156, loss = 0.004882687237113714
iteration 157, loss = 0.004224367439746857
iteration 158, loss = 0.005759896710515022
iteration 159, loss = 0.0046220580115914345
iteration 160, loss = 0.006703647784888744
iteration 161, loss = 0.004338921047747135
iteration 162, loss = 0.006097798235714436
iteration 163, loss = 0.003999113105237484
iteration 164, loss = 0.0059372135438025
iteration 165, loss = 0.004179927986115217
iteration 166, loss = 0.006565974093973637
iteration 167, loss = 0.0036494932137429714
iteration 168, loss = 0.0045362552627921104
iteration 169, loss = 0.003982778638601303
iteration 170, loss = 0.0047279177233576775
iteration 171, loss = 0.004195250570774078
iteration 172, loss = 0.0045480746775865555
iteration 173, loss = 0.004964163061231375
iteration 174, loss = 0.00391793018206954
iteration 175, loss = 0.005244580563157797
iteration 176, loss = 0.004280610475689173
iteration 177, loss = 0.004651587922126055
iteration 178, loss = 0.004522291012108326
iteration 179, loss = 0.0045952219516038895
iteration 180, loss = 0.004216101951897144
iteration 181, loss = 0.004396115895360708
iteration 182, loss = 0.004163019359111786
iteration 183, loss = 0.004329858347773552
iteration 184, loss = 0.003994131460785866
iteration 185, loss = 0.007114543113857508
iteration 186, loss = 0.0044072833843529224
iteration 187, loss = 0.00533759081736207
iteration 188, loss = 0.004390805494040251
iteration 189, loss = 0.004114121664315462
iteration 190, loss = 0.004096671938896179
iteration 191, loss = 0.004420561715960503
iteration 192, loss = 0.004256368614733219
iteration 193, loss = 0.005506868474185467
iteration 194, loss = 0.0045729693956673145
iteration 195, loss = 0.004153802990913391
iteration 196, loss = 0.0039951191283762455
iteration 197, loss = 0.004414201248437166
iteration 198, loss = 0.005668582394719124
iteration 199, loss = 0.0044617727398872375
iteration 200, loss = 0.004477086942642927
iteration 201, loss = 0.004499693866819143
iteration 202, loss = 0.004792783875018358
iteration 203, loss = 0.004748815204948187
iteration 204, loss = 0.006896332837641239
iteration 205, loss = 0.004368803929537535
iteration 206, loss = 0.004226753953844309
iteration 207, loss = 0.004349118564277887
iteration 208, loss = 0.004305941052734852
iteration 209, loss = 0.006793159060180187
iteration 210, loss = 0.0054193418473005295
iteration 211, loss = 0.0046300217509269714
iteration 212, loss = 0.0039990125223994255
iteration 213, loss = 0.004079720936715603
iteration 214, loss = 0.00464944913983345
iteration 215, loss = 0.005287544336169958
iteration 216, loss = 0.004270448815077543
iteration 217, loss = 0.004730864427983761
iteration 218, loss = 0.004966601729393005
iteration 219, loss = 0.004239344969391823
iteration 220, loss = 0.004388666711747646
iteration 221, loss = 0.004274123813956976
iteration 222, loss = 0.003948858007788658
iteration 223, loss = 0.004068369977176189
iteration 224, loss = 0.0043294974602758884
iteration 225, loss = 0.004132299683988094
iteration 226, loss = 0.004085643216967583
iteration 227, loss = 0.004244361538439989
iteration 228, loss = 0.004009338095784187
iteration 229, loss = 0.003979453817009926
iteration 230, loss = 0.0048660836182534695
iteration 231, loss = 0.004484652541577816
iteration 232, loss = 0.004256689455360174
iteration 233, loss = 0.004207190126180649
iteration 234, loss = 0.00403167400509119
iteration 235, loss = 0.0048015667125582695
iteration 236, loss = 0.004576326347887516
iteration 237, loss = 0.004023101646453142
iteration 238, loss = 0.004846469033509493
iteration 239, loss = 0.004385045263916254
iteration 240, loss = 0.004184439312666655
iteration 241, loss = 0.0055459290742874146
iteration 242, loss = 0.004354232922196388
iteration 243, loss = 0.004191389307379723
iteration 244, loss = 0.0054643782787024975
iteration 245, loss = 0.005470611155033112
iteration 246, loss = 0.004154964815825224
iteration 247, loss = 0.0049692317843437195
iteration 248, loss = 0.004126734100282192
iteration 249, loss = 0.003976229578256607
iteration 250, loss = 0.004162880126386881
iteration 251, loss = 0.0042093535885214806
iteration 252, loss = 0.004151246976107359
iteration 253, loss = 0.004790790379047394
iteration 254, loss = 0.0047346437349915504
iteration 255, loss = 0.00603875145316124
iteration 256, loss = 0.00580953061580658
iteration 257, loss = 0.003958233166486025
iteration 258, loss = 0.006786261685192585
iteration 259, loss = 0.004700114950537682
iteration 260, loss = 0.005035091191530228
iteration 261, loss = 0.0043099420145154
iteration 262, loss = 0.004263531882315874
iteration 263, loss = 0.004056872799992561
iteration 264, loss = 0.004040145315229893
iteration 265, loss = 0.004255278967320919
iteration 266, loss = 0.005543089471757412
iteration 267, loss = 0.004133676178753376
iteration 268, loss = 0.004448665305972099
iteration 269, loss = 0.005766051355749369
iteration 270, loss = 0.004485942889004946
iteration 271, loss = 0.005569970700889826
iteration 272, loss = 0.004613916855305433
iteration 273, loss = 0.00433292705565691
iteration 274, loss = 0.004243567120283842
iteration 275, loss = 0.005799528677016497
iteration 276, loss = 0.005283810198307037
iteration 277, loss = 0.005240940488874912
iteration 278, loss = 0.004109770059585571
iteration 279, loss = 0.0054894257336854935
iteration 280, loss = 0.004680855665355921
iteration 281, loss = 0.004867428448051214
iteration 282, loss = 0.005866691004484892
iteration 283, loss = 0.004512026906013489
iteration 284, loss = 0.004502417054027319
iteration 285, loss = 0.00423963088542223
iteration 286, loss = 0.004235531203448772
iteration 287, loss = 0.005931933876127005
iteration 288, loss = 0.005015133880078793
iteration 289, loss = 0.004777616821229458
iteration 290, loss = 0.004204203840345144
iteration 291, loss = 0.004482776392251253
iteration 292, loss = 0.006989768706262112
iteration 293, loss = 0.005239206366240978
iteration 294, loss = 0.004010920412838459
iteration 295, loss = 0.004311859607696533
iteration 296, loss = 0.004487333819270134
iteration 297, loss = 0.004389516543596983
iteration 298, loss = 0.0039573111571371555
iteration 299, loss = 0.0053277346305549145
iteration 300, loss = 0.0038764558266848326
iteration 1, loss = 0.004074858967214823
iteration 2, loss = 0.004530440084636211
iteration 3, loss = 0.004693088121712208
iteration 4, loss = 0.004431677516549826
iteration 5, loss = 0.005264050792902708
iteration 6, loss = 0.004004614893347025
iteration 7, loss = 0.0042595951817929745
iteration 8, loss = 0.003958925139158964
iteration 9, loss = 0.008588653989136219
iteration 10, loss = 0.006198647432029247
iteration 11, loss = 0.0037431162782013416
iteration 12, loss = 0.004742101300507784
iteration 13, loss = 0.005641424562782049
iteration 14, loss = 0.003962748683989048
iteration 15, loss = 0.004555105697363615
iteration 16, loss = 0.00418815016746521
iteration 17, loss = 0.004215440712869167
iteration 18, loss = 0.00400074478238821
iteration 19, loss = 0.004115003161132336
iteration 20, loss = 0.0041120098903775215
iteration 21, loss = 0.004274718929082155
iteration 22, loss = 0.003865271108224988
iteration 23, loss = 0.004322093911468983
iteration 24, loss = 0.004554510582238436
iteration 25, loss = 0.007043494842946529
iteration 26, loss = 0.004124421160668135
iteration 27, loss = 0.004627855494618416
iteration 28, loss = 0.004877825733274221
iteration 29, loss = 0.004186613950878382
iteration 30, loss = 0.0040924567729234695
iteration 31, loss = 0.004137889947742224
iteration 32, loss = 0.004345017019659281
iteration 33, loss = 0.004313298966735601
iteration 34, loss = 0.004236756823956966
iteration 35, loss = 0.004133674781769514
iteration 36, loss = 0.003954907413572073
iteration 37, loss = 0.004892408847808838
iteration 38, loss = 0.005225028842687607
iteration 39, loss = 0.004093989264219999
iteration 40, loss = 0.00440564239397645
iteration 41, loss = 0.004339045844972134
iteration 42, loss = 0.006365536246448755
iteration 43, loss = 0.004247928038239479
iteration 44, loss = 0.004646775312721729
iteration 45, loss = 0.005507559981197119
iteration 46, loss = 0.004376353695988655
iteration 47, loss = 0.004239229019731283
iteration 48, loss = 0.004044775851070881
iteration 49, loss = 0.004099114798009396
iteration 50, loss = 0.0043964325450360775
iteration 51, loss = 0.0043419054709374905
iteration 52, loss = 0.00450939079746604
iteration 53, loss = 0.006920750252902508
iteration 54, loss = 0.006192740984261036
iteration 55, loss = 0.00408387603238225
iteration 56, loss = 0.0038364995270967484
iteration 57, loss = 0.0040520355105400085
iteration 58, loss = 0.004070131573826075
iteration 59, loss = 0.004815696272999048
iteration 60, loss = 0.005166420713067055
iteration 61, loss = 0.004842582158744335
iteration 62, loss = 0.006896516773849726
iteration 63, loss = 0.006587788462638855
iteration 64, loss = 0.004378069657832384
iteration 65, loss = 0.004239374306052923
iteration 66, loss = 0.005084102042019367
iteration 67, loss = 0.004535799380391836
iteration 68, loss = 0.004213989246636629
iteration 69, loss = 0.004118422977626324
iteration 70, loss = 0.004090677946805954
iteration 71, loss = 0.003828790271654725
iteration 72, loss = 0.004397244192659855
iteration 73, loss = 0.004574925638735294
iteration 74, loss = 0.004315740428864956
iteration 75, loss = 0.004114020150154829
iteration 76, loss = 0.004145126324146986
iteration 77, loss = 0.007487263064831495
iteration 78, loss = 0.003674129955470562
iteration 79, loss = 0.005716012790799141
iteration 80, loss = 0.003946185577660799
iteration 81, loss = 0.0041444869711995125
iteration 82, loss = 0.004002344328910112
iteration 83, loss = 0.005291152745485306
iteration 84, loss = 0.006998730823397636
iteration 85, loss = 0.004427651409059763
iteration 86, loss = 0.005646050907671452
iteration 87, loss = 0.0042005483992397785
iteration 88, loss = 0.003968741744756699
iteration 89, loss = 0.0038839925546199083
iteration 90, loss = 0.007074728142470121
iteration 91, loss = 0.004611632786691189
iteration 92, loss = 0.0041152783669531345
iteration 93, loss = 0.004670159425586462
iteration 94, loss = 0.004400113131850958
iteration 95, loss = 0.004490559920668602
iteration 96, loss = 0.0040203589014709
iteration 97, loss = 0.005986717063933611
iteration 98, loss = 0.004094133153557777
iteration 99, loss = 0.00386409112252295
iteration 100, loss = 0.004678551573306322
iteration 101, loss = 0.004535249434411526
iteration 102, loss = 0.004324615001678467
iteration 103, loss = 0.004864626098424196
iteration 104, loss = 0.003912692423909903
iteration 105, loss = 0.004390995018184185
iteration 106, loss = 0.004178440198302269
iteration 107, loss = 0.004276263061910868
iteration 108, loss = 0.004083620849996805
iteration 109, loss = 0.005223535001277924
iteration 110, loss = 0.0038824775256216526
iteration 111, loss = 0.0044120256789028645
iteration 112, loss = 0.004240565001964569
iteration 113, loss = 0.0042085410095751286
iteration 114, loss = 0.004083830397576094
iteration 115, loss = 0.005006320774555206
iteration 116, loss = 0.004600516986101866
iteration 117, loss = 0.003909678664058447
iteration 118, loss = 0.007505299057811499
iteration 119, loss = 0.004103700164705515
iteration 120, loss = 0.004547690507024527
iteration 121, loss = 0.006202861201018095
iteration 122, loss = 0.006007798481732607
iteration 123, loss = 0.004098936915397644
iteration 124, loss = 0.0038919567596167326
iteration 125, loss = 0.005373882129788399
iteration 126, loss = 0.004910370334982872
iteration 127, loss = 0.004515183158218861
iteration 128, loss = 0.004061500541865826
iteration 129, loss = 0.004414810799062252
iteration 130, loss = 0.004090503789484501
iteration 131, loss = 0.005741798784583807
iteration 132, loss = 0.005510462448000908
iteration 133, loss = 0.005366068799048662
iteration 134, loss = 0.005510814022272825
iteration 135, loss = 0.0043722582049667835
iteration 136, loss = 0.003973535727709532
iteration 137, loss = 0.004671972244977951
iteration 138, loss = 0.004387032240629196
iteration 139, loss = 0.0041955909691751
iteration 140, loss = 0.004931715317070484
iteration 141, loss = 0.004246960394084454
iteration 142, loss = 0.0049836840480566025
iteration 143, loss = 0.006050048861652613
iteration 144, loss = 0.004505906254053116
iteration 145, loss = 0.004107822198420763
iteration 146, loss = 0.003861146979033947
iteration 147, loss = 0.005479365587234497
iteration 148, loss = 0.003829407272860408
iteration 149, loss = 0.004527215380221605
iteration 150, loss = 0.004235879052430391
iteration 151, loss = 0.004411129746586084
iteration 152, loss = 0.007091092877089977
iteration 153, loss = 0.004273055121302605
iteration 154, loss = 0.0057288967072963715
iteration 155, loss = 0.004443190060555935
iteration 156, loss = 0.004455632530152798
iteration 157, loss = 0.0064944978803396225
iteration 158, loss = 0.004143663216382265
iteration 159, loss = 0.00500502297654748
iteration 160, loss = 0.004492076113820076
iteration 161, loss = 0.004994483664631844
iteration 162, loss = 0.0041971514001488686
iteration 163, loss = 0.004678515717387199
iteration 164, loss = 0.004104032181203365
iteration 165, loss = 0.0060307178646326065
iteration 166, loss = 0.005387944169342518
iteration 167, loss = 0.0051538972184062
iteration 168, loss = 0.0037409490905702114
iteration 169, loss = 0.003911250736564398
iteration 170, loss = 0.004473920911550522
iteration 171, loss = 0.0037849757354706526
iteration 172, loss = 0.006913735996931791
iteration 173, loss = 0.003948970697820187
iteration 174, loss = 0.006691616959869862
iteration 175, loss = 0.004902736283838749
iteration 176, loss = 0.004017041064798832
iteration 177, loss = 0.004803121089935303
iteration 178, loss = 0.004159514792263508
iteration 179, loss = 0.004358345177024603
iteration 180, loss = 0.005788641981780529
iteration 181, loss = 0.005981227848678827
iteration 182, loss = 0.004884202498942614
iteration 183, loss = 0.004267910495400429
iteration 184, loss = 0.004126310348510742
iteration 185, loss = 0.0038472444284707308
iteration 186, loss = 0.003996073268353939
iteration 187, loss = 0.004030780401080847
iteration 188, loss = 0.0044395579025149345
iteration 189, loss = 0.004091288894414902
iteration 190, loss = 0.004597645718604326
iteration 191, loss = 0.0058537814766168594
iteration 192, loss = 0.005197460763156414
iteration 193, loss = 0.004686332307755947
iteration 194, loss = 0.00552057521417737
iteration 195, loss = 0.004714393988251686
iteration 196, loss = 0.004559071268886328
iteration 197, loss = 0.004030709620565176
iteration 198, loss = 0.004097507800906897
iteration 199, loss = 0.004277799278497696
iteration 200, loss = 0.004746671766042709
iteration 201, loss = 0.0039924089796841145
iteration 202, loss = 0.004231288563460112
iteration 203, loss = 0.004281624220311642
iteration 204, loss = 0.003838198957964778
iteration 205, loss = 0.003950141835957766
iteration 206, loss = 0.0055247205309569836
iteration 207, loss = 0.0051756855100393295
iteration 208, loss = 0.0044578746892511845
iteration 209, loss = 0.003931802231818438
iteration 210, loss = 0.004338222090154886
iteration 211, loss = 0.006121658720076084
iteration 212, loss = 0.004736459814012051
iteration 213, loss = 0.004262454807758331
iteration 214, loss = 0.004794372711330652
iteration 215, loss = 0.0066848024725914
iteration 216, loss = 0.006120678503066301
iteration 217, loss = 0.0054220641031861305
iteration 218, loss = 0.004558633081614971
iteration 219, loss = 0.0054877744987607
iteration 220, loss = 0.004289556760340929
iteration 221, loss = 0.004364446271210909
iteration 222, loss = 0.004718841519206762
iteration 223, loss = 0.004577917046844959
iteration 224, loss = 0.004418161232024431
iteration 225, loss = 0.005657246336340904
iteration 226, loss = 0.0063882144168019295
iteration 227, loss = 0.004039058927446604
iteration 228, loss = 0.006043523084372282
iteration 229, loss = 0.0041751074604690075
iteration 230, loss = 0.006816974841058254
iteration 231, loss = 0.0048783160746097565
iteration 232, loss = 0.006832272745668888
iteration 233, loss = 0.004254795145243406
iteration 234, loss = 0.00395098514854908
iteration 235, loss = 0.004437510389834642
iteration 236, loss = 0.00419369712471962
iteration 237, loss = 0.004765186458826065
iteration 238, loss = 0.004869284573942423
iteration 239, loss = 0.004035452380776405
iteration 240, loss = 0.004640608094632626
iteration 241, loss = 0.004734094254672527
iteration 242, loss = 0.004010973963886499
iteration 243, loss = 0.004182121250778437
iteration 244, loss = 0.003988069947808981
iteration 245, loss = 0.0040916744619607925
iteration 246, loss = 0.004304670728743076
iteration 247, loss = 0.005515340715646744
iteration 248, loss = 0.004070089664310217
iteration 249, loss = 0.004745486192405224
iteration 250, loss = 0.003917116206139326
iteration 251, loss = 0.004192672669887543
iteration 252, loss = 0.0041368575766682625
iteration 253, loss = 0.004525978583842516
iteration 254, loss = 0.004149188753217459
iteration 255, loss = 0.004484528675675392
iteration 256, loss = 0.005246720276772976
iteration 257, loss = 0.004517782479524612
iteration 258, loss = 0.005364574957638979
iteration 259, loss = 0.007460188586264849
iteration 260, loss = 0.0037015376146882772
iteration 261, loss = 0.004764333367347717
iteration 262, loss = 0.0037258039228618145
iteration 263, loss = 0.00438939081504941
iteration 264, loss = 0.0055360449478030205
iteration 265, loss = 0.003928337246179581
iteration 266, loss = 0.00593533692881465
iteration 267, loss = 0.004136608447879553
iteration 268, loss = 0.004624814726412296
iteration 269, loss = 0.00431937538087368
iteration 270, loss = 0.003992815967649221
iteration 271, loss = 0.004539105575531721
iteration 272, loss = 0.004162579774856567
iteration 273, loss = 0.006014851853251457
iteration 274, loss = 0.0038148853927850723
iteration 275, loss = 0.0059822131879627705
iteration 276, loss = 0.005321041215211153
iteration 277, loss = 0.0058386973105371
iteration 278, loss = 0.005812934599816799
iteration 279, loss = 0.006010363809764385
iteration 280, loss = 0.004277687519788742
iteration 281, loss = 0.004731945693492889
iteration 282, loss = 0.004102006554603577
iteration 283, loss = 0.004404660314321518
iteration 284, loss = 0.0059539545327425
iteration 285, loss = 0.00418957881629467
iteration 286, loss = 0.004697985481470823
iteration 287, loss = 0.003749271621927619
iteration 288, loss = 0.00426903460174799
iteration 289, loss = 0.0038740006275475025
iteration 290, loss = 0.004128875210881233
iteration 291, loss = 0.0054518962278962135
iteration 292, loss = 0.003894900204613805
iteration 293, loss = 0.005402137525379658
iteration 294, loss = 0.004715207498520613
iteration 295, loss = 0.006968614179641008
iteration 296, loss = 0.004384269006550312
iteration 297, loss = 0.004232443403452635
iteration 298, loss = 0.004217647016048431
iteration 299, loss = 0.005406377371400595
iteration 300, loss = 0.004409318789839745
iteration 1, loss = 0.00435367738828063
iteration 2, loss = 0.004628188442438841
iteration 3, loss = 0.004355403129011393
iteration 4, loss = 0.007203994318842888
iteration 5, loss = 0.003919544164091349
iteration 6, loss = 0.004493848420679569
iteration 7, loss = 0.004132003057748079
iteration 8, loss = 0.0061957500874996185
iteration 9, loss = 0.006533330772072077
iteration 10, loss = 0.004038653802126646
iteration 11, loss = 0.005367700941860676
iteration 12, loss = 0.004061118233948946
iteration 13, loss = 0.0043031154200434685
iteration 14, loss = 0.004198918584734201
iteration 15, loss = 0.004684291314333677
iteration 16, loss = 0.0041283066384494305
iteration 17, loss = 0.004351747687906027
iteration 18, loss = 0.005473989062011242
iteration 19, loss = 0.004298773594200611
iteration 20, loss = 0.005769834388047457
iteration 21, loss = 0.003894663881510496
iteration 22, loss = 0.0041621471755206585
iteration 23, loss = 0.0039749122224748135
iteration 24, loss = 0.005996610037982464
iteration 25, loss = 0.004647799767553806
iteration 26, loss = 0.004162820056080818
iteration 27, loss = 0.004041298758238554
iteration 28, loss = 0.0042965468019247055
iteration 29, loss = 0.004602951928973198
iteration 30, loss = 0.00480271689593792
iteration 31, loss = 0.005126623436808586
iteration 32, loss = 0.004299155902117491
iteration 33, loss = 0.004280091729015112
iteration 34, loss = 0.003954669460654259
iteration 35, loss = 0.0052372184582054615
iteration 36, loss = 0.005957519169896841
iteration 37, loss = 0.004796117078512907
iteration 38, loss = 0.004894142504781485
iteration 39, loss = 0.00421238224953413
iteration 40, loss = 0.0069697219878435135
iteration 41, loss = 0.006033667828887701
iteration 42, loss = 0.004808269441127777
iteration 43, loss = 0.004314775113016367
iteration 44, loss = 0.003903598990291357
iteration 45, loss = 0.0058227162808179855
iteration 46, loss = 0.00478265481069684
iteration 47, loss = 0.005069828126579523
iteration 48, loss = 0.004035345744341612
iteration 49, loss = 0.006424778141081333
iteration 50, loss = 0.004039924591779709
iteration 51, loss = 0.0061755096539855
iteration 52, loss = 0.0035723356995731592
iteration 53, loss = 0.004153814632445574
iteration 54, loss = 0.004354583099484444
iteration 55, loss = 0.0048592365346848965
iteration 56, loss = 0.004006949719041586
iteration 57, loss = 0.006415062118321657
iteration 58, loss = 0.006687975954264402
iteration 59, loss = 0.004454207606613636
iteration 60, loss = 0.004628217313438654
iteration 61, loss = 0.0051561640575528145
iteration 62, loss = 0.0067304642871022224
iteration 63, loss = 0.004035447724163532
iteration 64, loss = 0.0038069833535701036
iteration 65, loss = 0.00469701224938035
iteration 66, loss = 0.004403652623295784
iteration 67, loss = 0.005579611286520958
iteration 68, loss = 0.004995379131287336
iteration 69, loss = 0.004024602007120848
iteration 70, loss = 0.006194428540766239
iteration 71, loss = 0.004054989665746689
iteration 72, loss = 0.004768861923366785
iteration 73, loss = 0.00427248002961278
iteration 74, loss = 0.004334892611950636
iteration 75, loss = 0.0052474201656877995
iteration 76, loss = 0.004560607019811869
iteration 77, loss = 0.006973522249609232
iteration 78, loss = 0.004140579607337713
iteration 79, loss = 0.004742453806102276
iteration 80, loss = 0.004639594350010157
iteration 81, loss = 0.00388359185308218
iteration 82, loss = 0.005666802637279034
iteration 83, loss = 0.0053179156966507435
iteration 84, loss = 0.004564195871353149
iteration 85, loss = 0.005776288919150829
iteration 86, loss = 0.004896488972008228
iteration 87, loss = 0.004189740866422653
iteration 88, loss = 0.003924407064914703
iteration 89, loss = 0.004758170805871487
iteration 90, loss = 0.004106347914785147
iteration 91, loss = 0.005856523755937815
iteration 92, loss = 0.004138065967708826
iteration 93, loss = 0.0038772285915911198
iteration 94, loss = 0.004464651923626661
iteration 95, loss = 0.005734902340918779
iteration 96, loss = 0.0038756842259317636
iteration 97, loss = 0.00447680102661252
iteration 98, loss = 0.00438104709610343
iteration 99, loss = 0.004727604798972607
iteration 100, loss = 0.0040987650863826275
iteration 101, loss = 0.004183392971754074
iteration 102, loss = 0.003941945731639862
iteration 103, loss = 0.005353163927793503
iteration 104, loss = 0.0067270416766405106
iteration 105, loss = 0.003926996607333422
iteration 106, loss = 0.004428195301443338
iteration 107, loss = 0.004277958534657955
iteration 108, loss = 0.004227190278470516
iteration 109, loss = 0.0041446564719080925
iteration 110, loss = 0.004245194606482983
iteration 111, loss = 0.006382349878549576
iteration 112, loss = 0.003853750415146351
iteration 113, loss = 0.004193258937448263
iteration 114, loss = 0.003799744416028261
iteration 115, loss = 0.004197918809950352
iteration 116, loss = 0.003870252287015319
iteration 117, loss = 0.004125149920582771
iteration 118, loss = 0.004115246701985598
iteration 119, loss = 0.004993999842554331
iteration 120, loss = 0.00443149171769619
iteration 121, loss = 0.005587514955550432
iteration 122, loss = 0.003630739636719227
iteration 123, loss = 0.004753368906676769
iteration 124, loss = 0.0044523002579808235
iteration 125, loss = 0.004325258079916239
iteration 126, loss = 0.005630937404930592
iteration 127, loss = 0.00418156897649169
iteration 128, loss = 0.004258661065250635
iteration 129, loss = 0.005295648705214262
iteration 130, loss = 0.004500543233007193
iteration 131, loss = 0.004111197777092457
iteration 132, loss = 0.004774264991283417
iteration 133, loss = 0.0057716332376003265
iteration 134, loss = 0.004249545279890299
iteration 135, loss = 0.0040588537231087685
iteration 136, loss = 0.004652128089219332
iteration 137, loss = 0.003950093407183886
iteration 138, loss = 0.007542063016444445
iteration 139, loss = 0.004146311432123184
iteration 140, loss = 0.005795780103653669
iteration 141, loss = 0.004283066373318434
iteration 142, loss = 0.004488024394959211
iteration 143, loss = 0.005270163994282484
iteration 144, loss = 0.005500313825905323
iteration 145, loss = 0.0040023354813456535
iteration 146, loss = 0.0060082413256168365
iteration 147, loss = 0.004090894479304552
iteration 148, loss = 0.004146703518927097
iteration 149, loss = 0.003986847121268511
iteration 150, loss = 0.004213638603687286
iteration 151, loss = 0.0057997633703053
iteration 152, loss = 0.00521162198856473
iteration 153, loss = 0.004226976539939642
iteration 154, loss = 0.004439024720340967
iteration 155, loss = 0.0059179323725402355
iteration 156, loss = 0.004313093610107899
iteration 157, loss = 0.003853535745292902
iteration 158, loss = 0.006239678245037794
iteration 159, loss = 0.0038034357130527496
iteration 160, loss = 0.007254221476614475
iteration 161, loss = 0.004178711213171482
iteration 162, loss = 0.004277355503290892
iteration 163, loss = 0.007184578105807304
iteration 164, loss = 0.004243128001689911
iteration 165, loss = 0.004498434718698263
iteration 166, loss = 0.004326066933572292
iteration 167, loss = 0.005846723448485136
iteration 168, loss = 0.004288989119231701
iteration 169, loss = 0.004104401916265488
iteration 170, loss = 0.0045522539876401424
iteration 171, loss = 0.004278056789189577
iteration 172, loss = 0.00514242285862565
iteration 173, loss = 0.006339444313198328
iteration 174, loss = 0.00451037148013711
iteration 175, loss = 0.004187058191746473
iteration 176, loss = 0.004349975846707821
iteration 177, loss = 0.0038470029830932617
iteration 178, loss = 0.004387058317661285
iteration 179, loss = 0.00425597233697772
iteration 180, loss = 0.004402252845466137
iteration 181, loss = 0.004106189589947462
iteration 182, loss = 0.00453804899007082
iteration 183, loss = 0.004683790262788534
iteration 184, loss = 0.004319678992033005
iteration 185, loss = 0.004045626614242792
iteration 186, loss = 0.00554793793708086
iteration 187, loss = 0.004489158745855093
iteration 188, loss = 0.004352971445769072
iteration 189, loss = 0.004704413004219532
iteration 190, loss = 0.004085501190274954
iteration 191, loss = 0.004477630835026503
iteration 192, loss = 0.004632247146219015
iteration 193, loss = 0.00594896636903286
iteration 194, loss = 0.004101019352674484
iteration 195, loss = 0.004009087570011616
iteration 196, loss = 0.003942715469747782
iteration 197, loss = 0.004035208839923143
iteration 198, loss = 0.004067177884280682
iteration 199, loss = 0.003993647173047066
iteration 200, loss = 0.004479236900806427
iteration 201, loss = 0.004301461391150951
iteration 202, loss = 0.0043791113421320915
iteration 203, loss = 0.003925431054085493
iteration 204, loss = 0.003976213745772839
iteration 205, loss = 0.005822648759931326
iteration 206, loss = 0.004222800023853779
iteration 207, loss = 0.005990016274154186
iteration 208, loss = 0.004413120448589325
iteration 209, loss = 0.006944923661649227
iteration 210, loss = 0.0044697728008031845
iteration 211, loss = 0.004653969779610634
iteration 212, loss = 0.004414995666593313
iteration 213, loss = 0.004185796715319157
iteration 214, loss = 0.0051547931507229805
iteration 215, loss = 0.004371107555925846
iteration 216, loss = 0.004054878372699022
iteration 217, loss = 0.007400169037282467
iteration 218, loss = 0.004179045557975769
iteration 219, loss = 0.004105729982256889
iteration 220, loss = 0.0040557002648711205
iteration 221, loss = 0.004413250368088484
iteration 222, loss = 0.005044839344918728
iteration 223, loss = 0.0059973690658807755
iteration 224, loss = 0.004265148192644119
iteration 225, loss = 0.004079554229974747
iteration 226, loss = 0.006781253032386303
iteration 227, loss = 0.004432862624526024
iteration 228, loss = 0.003918718080967665
iteration 229, loss = 0.004926974885165691
iteration 230, loss = 0.004381163977086544
iteration 231, loss = 0.0042686667293310165
iteration 232, loss = 0.007137845735996962
iteration 233, loss = 0.0041914042085409164
iteration 234, loss = 0.003912374377250671
iteration 235, loss = 0.0069072432816028595
iteration 236, loss = 0.004802860785275698
iteration 237, loss = 0.004507932346314192
iteration 238, loss = 0.0037113595753908157
iteration 239, loss = 0.004539485089480877
iteration 240, loss = 0.004022717941552401
iteration 241, loss = 0.007417885586619377
iteration 242, loss = 0.005325559992343187
iteration 243, loss = 0.004291671793907881
iteration 244, loss = 0.00413853395730257
iteration 245, loss = 0.004784281365573406
iteration 246, loss = 0.004246887750923634
iteration 247, loss = 0.004104905296117067
iteration 248, loss = 0.005571309011429548
iteration 249, loss = 0.004628298804163933
iteration 250, loss = 0.004024774767458439
iteration 251, loss = 0.005943606607615948
iteration 252, loss = 0.004619041923433542
iteration 253, loss = 0.004075277596712112
iteration 254, loss = 0.0038810987025499344
iteration 255, loss = 0.005207045469433069
iteration 256, loss = 0.004142948891967535
iteration 257, loss = 0.004391157533973455
iteration 258, loss = 0.005133414641022682
iteration 259, loss = 0.004118798766285181
iteration 260, loss = 0.00419192761182785
iteration 261, loss = 0.004241010174155235
iteration 262, loss = 0.004207913298159838
iteration 263, loss = 0.0045937420800328255
iteration 264, loss = 0.0038750520907342434
iteration 265, loss = 0.004366136156022549
iteration 266, loss = 0.004104761406779289
iteration 267, loss = 0.004445997998118401
iteration 268, loss = 0.00584831740707159
iteration 269, loss = 0.004669728223234415
iteration 270, loss = 0.004277829546481371
iteration 271, loss = 0.0038980310782790184
iteration 272, loss = 0.004273724276572466
iteration 273, loss = 0.0045405784621834755
iteration 274, loss = 0.005031854845583439
iteration 275, loss = 0.00510435551404953
iteration 276, loss = 0.004672359675168991
iteration 277, loss = 0.005483980756253004
iteration 278, loss = 0.005931185558438301
iteration 279, loss = 0.004422471858561039
iteration 280, loss = 0.004619813524186611
iteration 281, loss = 0.00442847516387701
iteration 282, loss = 0.004474744200706482
iteration 283, loss = 0.0037567410618066788
iteration 284, loss = 0.004181124269962311
iteration 285, loss = 0.006106031127274036
iteration 286, loss = 0.0051572853699326515
iteration 287, loss = 0.004095093812793493
iteration 288, loss = 0.004113243892788887
iteration 289, loss = 0.004143201746046543
iteration 290, loss = 0.003927467856556177
iteration 291, loss = 0.006887930445373058
iteration 292, loss = 0.004276752937585115
iteration 293, loss = 0.004087858367711306
iteration 294, loss = 0.004160492680966854
iteration 295, loss = 0.004656512290239334
iteration 296, loss = 0.004164158832281828
iteration 297, loss = 0.00468231737613678
iteration 298, loss = 0.004323153756558895
iteration 299, loss = 0.004205208737403154
iteration 300, loss = 0.004290154203772545
iteration 1, loss = 0.004064976237714291
iteration 2, loss = 0.003992329351603985
iteration 3, loss = 0.004807061515748501
iteration 4, loss = 0.005488686729222536
iteration 5, loss = 0.006535930559039116
iteration 6, loss = 0.00415836600586772
iteration 7, loss = 0.005931555293500423
iteration 8, loss = 0.0057846251875162125
iteration 9, loss = 0.004020660184323788
iteration 10, loss = 0.004616186022758484
iteration 11, loss = 0.006166769191622734
iteration 12, loss = 0.004883942194283009
iteration 13, loss = 0.003997010178864002
iteration 14, loss = 0.004223621916025877
iteration 15, loss = 0.00686271907761693
iteration 16, loss = 0.004168574698269367
iteration 17, loss = 0.007337445858865976
iteration 18, loss = 0.005315099842846394
iteration 19, loss = 0.004087361041456461
iteration 20, loss = 0.003989516757428646
iteration 21, loss = 0.004431251902133226
iteration 22, loss = 0.0047287181951105595
iteration 23, loss = 0.004531397484242916
iteration 24, loss = 0.00396173307672143
iteration 25, loss = 0.003749519120901823
iteration 26, loss = 0.004649388138204813
iteration 27, loss = 0.0038818027824163437
iteration 28, loss = 0.004134894348680973
iteration 29, loss = 0.005142384208738804
iteration 30, loss = 0.004020649939775467
iteration 31, loss = 0.004122589249163866
iteration 32, loss = 0.005163114983588457
iteration 33, loss = 0.004365385975688696
iteration 34, loss = 0.005276770330965519
iteration 35, loss = 0.004886974580585957
iteration 36, loss = 0.0041743190959095955
iteration 37, loss = 0.004375808406621218
iteration 38, loss = 0.004761373624205589
iteration 39, loss = 0.007065125275403261
iteration 40, loss = 0.008587837219238281
iteration 41, loss = 0.004395749419927597
iteration 42, loss = 0.004151961300522089
iteration 43, loss = 0.0043665561825037
iteration 44, loss = 0.003991786856204271
iteration 45, loss = 0.005722714588046074
iteration 46, loss = 0.004092215094715357
iteration 47, loss = 0.004621308762580156
iteration 48, loss = 0.004272728692740202
iteration 49, loss = 0.004086317028850317
iteration 50, loss = 0.005051759537309408
iteration 51, loss = 0.004536861553788185
iteration 52, loss = 0.004428476095199585
iteration 53, loss = 0.003767474787309766
iteration 54, loss = 0.004173189867287874
iteration 55, loss = 0.005739843472838402
iteration 56, loss = 0.003970575984567404
iteration 57, loss = 0.005833950825035572
iteration 58, loss = 0.004504608456045389
iteration 59, loss = 0.005744345486164093
iteration 60, loss = 0.003919763490557671
iteration 61, loss = 0.003937258385121822
iteration 62, loss = 0.003878916148096323
iteration 63, loss = 0.004676021635532379
iteration 64, loss = 0.004093526396900415
iteration 65, loss = 0.003869590349495411
iteration 66, loss = 0.006724718026816845
iteration 67, loss = 0.0040019238367676735
iteration 68, loss = 0.0048254430294036865
iteration 69, loss = 0.0037734168581664562
iteration 70, loss = 0.004209564533084631
iteration 71, loss = 0.004475212190300226
iteration 72, loss = 0.0043504973873496056
iteration 73, loss = 0.004385634325444698
iteration 74, loss = 0.0059938980266451836
iteration 75, loss = 0.00463186576962471
iteration 76, loss = 0.004563859663903713
iteration 77, loss = 0.005295786075294018
iteration 78, loss = 0.005159439519047737
iteration 79, loss = 0.00403692526742816
iteration 80, loss = 0.003907849080860615
iteration 81, loss = 0.0065257675014436245
iteration 82, loss = 0.005831670481711626
iteration 83, loss = 0.005209880881011486
iteration 84, loss = 0.004694967530667782
iteration 85, loss = 0.0034316456876695156
iteration 86, loss = 0.00426592119038105
iteration 87, loss = 0.006640069652348757
iteration 88, loss = 0.005996610037982464
iteration 89, loss = 0.004386437591165304
iteration 90, loss = 0.004178243223577738
iteration 91, loss = 0.004211341962218285
iteration 92, loss = 0.004381771199405193
iteration 93, loss = 0.0061773513443768024
iteration 94, loss = 0.0039028096944093704
iteration 95, loss = 0.003913820255547762
iteration 96, loss = 0.00435602106153965
iteration 97, loss = 0.004076387733221054
iteration 98, loss = 0.004110977053642273
iteration 99, loss = 0.004099828656762838
iteration 100, loss = 0.004149940796196461
iteration 101, loss = 0.00417209230363369
iteration 102, loss = 0.004081218969076872
iteration 103, loss = 0.004533841740339994
iteration 104, loss = 0.0035745652858167887
iteration 105, loss = 0.0056083849631249905
iteration 106, loss = 0.004165190272033215
iteration 107, loss = 0.0043463693000376225
iteration 108, loss = 0.004707507323473692
iteration 109, loss = 0.004265708848834038
iteration 110, loss = 0.004073346499353647
iteration 111, loss = 0.004445342347025871
iteration 112, loss = 0.004637842997908592
iteration 113, loss = 0.005937526002526283
iteration 114, loss = 0.004693495575338602
iteration 115, loss = 0.004149841610342264
iteration 116, loss = 0.004085743799805641
iteration 117, loss = 0.007322876714169979
iteration 118, loss = 0.004581914283335209
iteration 119, loss = 0.00462439376860857
iteration 120, loss = 0.004293116275221109
iteration 121, loss = 0.0038517338689416647
iteration 122, loss = 0.004299292806535959
iteration 123, loss = 0.004749783780425787
iteration 124, loss = 0.0043088155798614025
iteration 125, loss = 0.005505275446921587
iteration 126, loss = 0.004780558869242668
iteration 127, loss = 0.005272501613944769
iteration 128, loss = 0.004198761191219091
iteration 129, loss = 0.004443729761987925
iteration 130, loss = 0.004054432734847069
iteration 131, loss = 0.005394123960286379
iteration 132, loss = 0.004190969280898571
iteration 133, loss = 0.0045163799077272415
iteration 134, loss = 0.005153465550392866
iteration 135, loss = 0.004024975001811981
iteration 136, loss = 0.005242716055363417
iteration 137, loss = 0.0040845321491360664
iteration 138, loss = 0.00437505217269063
iteration 139, loss = 0.004549538251012564
iteration 140, loss = 0.004167170729488134
iteration 141, loss = 0.005308715160936117
iteration 142, loss = 0.004367821849882603
iteration 143, loss = 0.005075058899819851
iteration 144, loss = 0.005497022531926632
iteration 145, loss = 0.006990059278905392
iteration 146, loss = 0.00565534271299839
iteration 147, loss = 0.005663210991770029
iteration 148, loss = 0.004577402025461197
iteration 149, loss = 0.0040540266782045364
iteration 150, loss = 0.00395574513822794
iteration 151, loss = 0.004418371710926294
iteration 152, loss = 0.004475356545299292
iteration 153, loss = 0.00453918008133769
iteration 154, loss = 0.007100950460880995
iteration 155, loss = 0.004026373382657766
iteration 156, loss = 0.004776371642947197
iteration 157, loss = 0.0046074287965893745
iteration 158, loss = 0.004632733762264252
iteration 159, loss = 0.003833238035440445
iteration 160, loss = 0.003948545083403587
iteration 161, loss = 0.0042520021088421345
iteration 162, loss = 0.004448716528713703
iteration 163, loss = 0.007321077398955822
iteration 164, loss = 0.0038447098340839148
iteration 165, loss = 0.004284747876226902
iteration 166, loss = 0.004017199855297804
iteration 167, loss = 0.0044082519598305225
iteration 168, loss = 0.0054253507405519485
iteration 169, loss = 0.0038685775361955166
iteration 170, loss = 0.0041192383505403996
iteration 171, loss = 0.006116124335676432
iteration 172, loss = 0.004506203345954418
iteration 173, loss = 0.004014550242573023
iteration 174, loss = 0.005628562998026609
iteration 175, loss = 0.00407993420958519
iteration 176, loss = 0.006461061537265778
iteration 177, loss = 0.004363276995718479
iteration 178, loss = 0.004294143989682198
iteration 179, loss = 0.005829180590808392
iteration 180, loss = 0.004495497327297926
iteration 181, loss = 0.004134237300604582
iteration 182, loss = 0.004266383592039347
iteration 183, loss = 0.004392808768898249
iteration 184, loss = 0.0042875465005636215
iteration 185, loss = 0.004191553220152855
iteration 186, loss = 0.004377189092338085
iteration 187, loss = 0.003975230269134045
iteration 188, loss = 0.0039801145903766155
iteration 189, loss = 0.004102425184100866
iteration 190, loss = 0.0040727839805185795
iteration 191, loss = 0.004128995351493359
iteration 192, loss = 0.004426124505698681
iteration 193, loss = 0.004078221041709185
iteration 194, loss = 0.004265883006155491
iteration 195, loss = 0.004435074981302023
iteration 196, loss = 0.004553340375423431
iteration 197, loss = 0.004618468694388866
iteration 198, loss = 0.005449153482913971
iteration 199, loss = 0.004236313048750162
iteration 200, loss = 0.004797678906470537
iteration 201, loss = 0.004564686678349972
iteration 202, loss = 0.005055292043834925
iteration 203, loss = 0.003961117472499609
iteration 204, loss = 0.004462012089788914
iteration 205, loss = 0.004264664836227894
iteration 206, loss = 0.004126641433686018
iteration 207, loss = 0.005207466892898083
iteration 208, loss = 0.004166803322732449
iteration 209, loss = 0.004168645013123751
iteration 210, loss = 0.005078165326267481
iteration 211, loss = 0.0042090583592653275
iteration 212, loss = 0.004262236412614584
iteration 213, loss = 0.004441366530954838
iteration 214, loss = 0.005798293277621269
iteration 215, loss = 0.0070582544431090355
iteration 216, loss = 0.004424991551786661
iteration 217, loss = 0.0052232625894248486
iteration 218, loss = 0.0037946426309645176
iteration 219, loss = 0.003990889526903629
iteration 220, loss = 0.0046903337351977825
iteration 221, loss = 0.0037250141613185406
iteration 222, loss = 0.004094894044101238
iteration 223, loss = 0.006190567277371883
iteration 224, loss = 0.00383905996568501
iteration 225, loss = 0.004004221875220537
iteration 226, loss = 0.0042686453089118
iteration 227, loss = 0.005967836361378431
iteration 228, loss = 0.008162575773894787
iteration 229, loss = 0.004403333179652691
iteration 230, loss = 0.0041986675933003426
iteration 231, loss = 0.0065099443309009075
iteration 232, loss = 0.0047093904577195644
iteration 233, loss = 0.005476499907672405
iteration 234, loss = 0.0040102628991007805
iteration 235, loss = 0.004674958996474743
iteration 236, loss = 0.004378634039312601
iteration 237, loss = 0.007221176754683256
iteration 238, loss = 0.0045874579809606075
iteration 239, loss = 0.005099156405776739
iteration 240, loss = 0.0033165698405355215
iteration 241, loss = 0.004029466304928064
iteration 242, loss = 0.0039392816834151745
iteration 243, loss = 0.004500573500990868
iteration 244, loss = 0.0037143649533391
iteration 245, loss = 0.004647810477763414
iteration 246, loss = 0.003840172663331032
iteration 247, loss = 0.0042386422865092754
iteration 248, loss = 0.003774363314732909
iteration 249, loss = 0.0041595203801989555
iteration 250, loss = 0.0044187987223267555
iteration 251, loss = 0.005661886651068926
iteration 252, loss = 0.005149953067302704
iteration 253, loss = 0.004244552459567785
iteration 254, loss = 0.004255326930433512
iteration 255, loss = 0.005917032714933157
iteration 256, loss = 0.00524440873414278
iteration 257, loss = 0.0038680285215377808
iteration 258, loss = 0.0042722467333078384
iteration 259, loss = 0.005455317907035351
iteration 260, loss = 0.005569149740040302
iteration 261, loss = 0.004106547683477402
iteration 262, loss = 0.004292186815291643
iteration 263, loss = 0.0058359731920063496
iteration 264, loss = 0.003822056809440255
iteration 265, loss = 0.004366771783679724
iteration 266, loss = 0.0070004528388381
iteration 267, loss = 0.004238421563059092
iteration 268, loss = 0.004462702665477991
iteration 269, loss = 0.004044526722282171
iteration 270, loss = 0.0074117532931268215
iteration 271, loss = 0.0040706247091293335
iteration 272, loss = 0.0042511350475251675
iteration 273, loss = 0.004646957386285067
iteration 274, loss = 0.005008989945054054
iteration 275, loss = 0.0038473294116556644
iteration 276, loss = 0.0043872264213860035
iteration 277, loss = 0.004189569037407637
iteration 278, loss = 0.004251579754054546
iteration 279, loss = 0.0043164403177797794
iteration 280, loss = 0.004180829040706158
iteration 281, loss = 0.004034765996038914
iteration 282, loss = 0.0046313186176121235
iteration 283, loss = 0.0038886391557753086
iteration 284, loss = 0.003972420934587717
iteration 285, loss = 0.004557472188025713
iteration 286, loss = 0.004350590519607067
iteration 287, loss = 0.007172151003032923
iteration 288, loss = 0.0070767090655863285
iteration 289, loss = 0.004337842110544443
iteration 290, loss = 0.003993639722466469
iteration 291, loss = 0.004775886423885822
iteration 292, loss = 0.004121133591979742
iteration 293, loss = 0.004140202421694994
iteration 294, loss = 0.003888976527377963
iteration 295, loss = 0.006173070520162582
iteration 296, loss = 0.006246643140912056
iteration 297, loss = 0.00477800564840436
iteration 298, loss = 0.0055105420760810375
iteration 299, loss = 0.00533672608435154
iteration 300, loss = 0.004244389943778515
iteration 1, loss = 0.00527906185016036
iteration 2, loss = 0.00468469737097621
iteration 3, loss = 0.0045780763030052185
iteration 4, loss = 0.005663149990141392
iteration 5, loss = 0.006836029700934887
iteration 6, loss = 0.004467403516173363
iteration 7, loss = 0.004271444398909807
iteration 8, loss = 0.003921112045645714
iteration 9, loss = 0.004365502391010523
iteration 10, loss = 0.004506861325353384
iteration 11, loss = 0.003826888743788004
iteration 12, loss = 0.005531717091798782
iteration 13, loss = 0.004739095456898212
iteration 14, loss = 0.004234866239130497
iteration 15, loss = 0.004508135840296745
iteration 16, loss = 0.004126262851059437
iteration 17, loss = 0.0051164934411644936
iteration 18, loss = 0.004580252803862095
iteration 19, loss = 0.004054855555295944
iteration 20, loss = 0.00391637347638607
iteration 21, loss = 0.005061716306954622
iteration 22, loss = 0.004408801905810833
iteration 23, loss = 0.004914099350571632
iteration 24, loss = 0.004216669127345085
iteration 25, loss = 0.004199836868792772
iteration 26, loss = 0.004153750836849213
iteration 27, loss = 0.00444627134129405
iteration 28, loss = 0.005408298224210739
iteration 29, loss = 0.005233014468103647
iteration 30, loss = 0.004383425693958998
iteration 31, loss = 0.006814276799559593
iteration 32, loss = 0.0038842211943119764
iteration 33, loss = 0.004011326469480991
iteration 34, loss = 0.00424490962177515
iteration 35, loss = 0.003834904171526432
iteration 36, loss = 0.004066922701895237
iteration 37, loss = 0.004319767002016306
iteration 38, loss = 0.005143227521330118
iteration 39, loss = 0.003915633074939251
iteration 40, loss = 0.004416216630488634
iteration 41, loss = 0.005518721416592598
iteration 42, loss = 0.004568292293697596
iteration 43, loss = 0.004292995668947697
iteration 44, loss = 0.003451874013990164
iteration 45, loss = 0.004364952445030212
iteration 46, loss = 0.004600257612764835
iteration 47, loss = 0.00593919213861227
iteration 48, loss = 0.004345342516899109
iteration 49, loss = 0.003916169982403517
iteration 50, loss = 0.0053141494281589985
iteration 51, loss = 0.004730460699647665
iteration 52, loss = 0.0059382300823926926
iteration 53, loss = 0.003893517656251788
iteration 54, loss = 0.006001490633934736
iteration 55, loss = 0.00422159256413579
iteration 56, loss = 0.0052780709229409695
iteration 57, loss = 0.004038251005113125
iteration 58, loss = 0.004222291521728039
iteration 59, loss = 0.0038885162211954594
iteration 60, loss = 0.004067244473844767
iteration 61, loss = 0.00829011108726263
iteration 62, loss = 0.005777875892817974
iteration 63, loss = 0.0040801772847771645
iteration 64, loss = 0.005743950139731169
iteration 65, loss = 0.0054107410833239555
iteration 66, loss = 0.003842584090307355
iteration 67, loss = 0.007348275743424892
iteration 68, loss = 0.004410712048411369
iteration 69, loss = 0.004226163495332003
iteration 70, loss = 0.003999106120318174
iteration 71, loss = 0.006698227487504482
iteration 72, loss = 0.004030668642371893
iteration 73, loss = 0.004042449872940779
iteration 74, loss = 0.004030392970889807
iteration 75, loss = 0.0038590626791119576
iteration 76, loss = 0.0038299441803246737
iteration 77, loss = 0.0036321626976132393
iteration 78, loss = 0.007026623468846083
iteration 79, loss = 0.0038299327716231346
iteration 80, loss = 0.006277391221374273
iteration 81, loss = 0.004496855195611715
iteration 82, loss = 0.0046922373585402966
iteration 83, loss = 0.0044411420822143555
iteration 84, loss = 0.004238602239638567
iteration 85, loss = 0.00416587246581912
iteration 86, loss = 0.004158741794526577
iteration 87, loss = 0.0044187637977302074
iteration 88, loss = 0.004483310040086508
iteration 89, loss = 0.003993394318968058
iteration 90, loss = 0.004136672243475914
iteration 91, loss = 0.005226190201938152
iteration 92, loss = 0.003938422072678804
iteration 93, loss = 0.004299138206988573
iteration 94, loss = 0.007205578498542309
iteration 95, loss = 0.004630576819181442
iteration 96, loss = 0.005903802812099457
iteration 97, loss = 0.0045411125756800175
iteration 98, loss = 0.003903987817466259
iteration 99, loss = 0.004257132299244404
iteration 100, loss = 0.004142636898905039
iteration 101, loss = 0.005408802069723606
iteration 102, loss = 0.004139848984777927
iteration 103, loss = 0.0045667653903365135
iteration 104, loss = 0.005490572191774845
iteration 105, loss = 0.004357593599706888
iteration 106, loss = 0.004113664850592613
iteration 107, loss = 0.0045881508849561214
iteration 108, loss = 0.004303023684769869
iteration 109, loss = 0.0037985071539878845
iteration 110, loss = 0.004038722719997168
iteration 111, loss = 0.0050220550037920475
iteration 112, loss = 0.004012912046164274
iteration 113, loss = 0.004344292916357517
iteration 114, loss = 0.00400276156142354
iteration 115, loss = 0.00690749054774642
iteration 116, loss = 0.004384058527648449
iteration 117, loss = 0.004095248878002167
iteration 118, loss = 0.004441382829099894
iteration 119, loss = 0.004020685330033302
iteration 120, loss = 0.0046407850459218025
iteration 121, loss = 0.004374669399112463
iteration 122, loss = 0.0041541289538145065
iteration 123, loss = 0.004746441729366779
iteration 124, loss = 0.00451516080647707
iteration 125, loss = 0.004326275549829006
iteration 126, loss = 0.005675178486853838
iteration 127, loss = 0.003976127598434687
iteration 128, loss = 0.005104281473904848
iteration 129, loss = 0.007475677877664566
iteration 130, loss = 0.0040483009070158005
iteration 131, loss = 0.004062781576067209
iteration 132, loss = 0.004399766214191914
iteration 133, loss = 0.004151735920459032
iteration 134, loss = 0.006062712054699659
iteration 135, loss = 0.005846988409757614
iteration 136, loss = 0.0045059919357299805
iteration 137, loss = 0.003960022237151861
iteration 138, loss = 0.004082900006324053
iteration 139, loss = 0.006873516831547022
iteration 140, loss = 0.005021923687309027
iteration 141, loss = 0.004151993431150913
iteration 142, loss = 0.00647612102329731
iteration 143, loss = 0.0040755607187747955
iteration 144, loss = 0.003928342834115028
iteration 145, loss = 0.007214063312858343
iteration 146, loss = 0.00434717396274209
iteration 147, loss = 0.004418786149471998
iteration 148, loss = 0.0041099172085523605
iteration 149, loss = 0.004752296023070812
iteration 150, loss = 0.004108129069209099
iteration 151, loss = 0.0038154791109263897
iteration 152, loss = 0.004694786388427019
iteration 153, loss = 0.004138541873544455
iteration 154, loss = 0.0077905235812067986
iteration 155, loss = 0.00410636980086565
iteration 156, loss = 0.0069966623559594154
iteration 157, loss = 0.0043903738260269165
iteration 158, loss = 0.004528916906565428
iteration 159, loss = 0.004792909137904644
iteration 160, loss = 0.0037297396920621395
iteration 161, loss = 0.0043156929314136505
iteration 162, loss = 0.00451595988124609
iteration 163, loss = 0.004491213709115982
iteration 164, loss = 0.004480438306927681
iteration 165, loss = 0.003990984987467527
iteration 166, loss = 0.005869325716048479
iteration 167, loss = 0.004522492177784443
iteration 168, loss = 0.004633529111742973
iteration 169, loss = 0.006921611726284027
iteration 170, loss = 0.004576241597533226
iteration 171, loss = 0.0044580670073628426
iteration 172, loss = 0.003944976720958948
iteration 173, loss = 0.005359698086977005
iteration 174, loss = 0.004036226309835911
iteration 175, loss = 0.00385665032081306
iteration 176, loss = 0.004522619768977165
iteration 177, loss = 0.004340839572250843
iteration 178, loss = 0.004213489126414061
iteration 179, loss = 0.004217781592160463
iteration 180, loss = 0.0044183251447975636
iteration 181, loss = 0.003958675544708967
iteration 182, loss = 0.004327468108385801
iteration 183, loss = 0.006060517858713865
iteration 184, loss = 0.004051517229527235
iteration 185, loss = 0.004068947397172451
iteration 186, loss = 0.004031138960272074
iteration 187, loss = 0.006848995573818684
iteration 188, loss = 0.004468261264264584
iteration 189, loss = 0.004491047468036413
iteration 190, loss = 0.004103892482817173
iteration 191, loss = 0.004925218876451254
iteration 192, loss = 0.005113199818879366
iteration 193, loss = 0.004398873541504145
iteration 194, loss = 0.004048550967127085
iteration 195, loss = 0.00686561968177557
iteration 196, loss = 0.0038740888703614473
iteration 197, loss = 0.004193331580609083
iteration 198, loss = 0.0042988574132323265
iteration 199, loss = 0.004243520088493824
iteration 200, loss = 0.006080023944377899
iteration 201, loss = 0.004829267971217632
iteration 202, loss = 0.004215496126562357
iteration 203, loss = 0.004329950548708439
iteration 204, loss = 0.0036539582069963217
iteration 205, loss = 0.005310763604938984
iteration 206, loss = 0.004478440620005131
iteration 207, loss = 0.004666097927838564
iteration 208, loss = 0.0038757305592298508
iteration 209, loss = 0.004593757912516594
iteration 210, loss = 0.0042506312020123005
iteration 211, loss = 0.0044934432953596115
iteration 212, loss = 0.00423036701977253
iteration 213, loss = 0.004153276327997446
iteration 214, loss = 0.0046858773566782475
iteration 215, loss = 0.005211370997130871
iteration 216, loss = 0.007890879176557064
iteration 217, loss = 0.00421329028904438
iteration 218, loss = 0.004268921911716461
iteration 219, loss = 0.0043955715373158455
iteration 220, loss = 0.0043639508076012135
iteration 221, loss = 0.00378236616961658
iteration 222, loss = 0.004467281978577375
iteration 223, loss = 0.004369713831692934
iteration 224, loss = 0.010963594540953636
iteration 225, loss = 0.005487565416842699
iteration 226, loss = 0.00567435659468174
iteration 227, loss = 0.003974412102252245
iteration 228, loss = 0.004188914783298969
iteration 229, loss = 0.004114387556910515
iteration 230, loss = 0.004070781636983156
iteration 231, loss = 0.004683046136051416
iteration 232, loss = 0.005992596503347158
iteration 233, loss = 0.005409736651927233
iteration 234, loss = 0.00449509359896183
iteration 235, loss = 0.005623399745672941
iteration 236, loss = 0.004358021542429924
iteration 237, loss = 0.007006004452705383
iteration 238, loss = 0.003833688795566559
iteration 239, loss = 0.004129837267100811
iteration 240, loss = 0.005615588277578354
iteration 241, loss = 0.004652845207601786
iteration 242, loss = 0.004426734056323767
iteration 243, loss = 0.004403275437653065
iteration 244, loss = 0.003983090631663799
iteration 245, loss = 0.005148828029632568
iteration 246, loss = 0.005905080586671829
iteration 247, loss = 0.005252547562122345
iteration 248, loss = 0.004240791313350201
iteration 249, loss = 0.006849790457636118
iteration 250, loss = 0.005215144250541925
iteration 251, loss = 0.004065464250743389
iteration 252, loss = 0.006428358610719442
iteration 253, loss = 0.0041861766949296
iteration 254, loss = 0.003956117667257786
iteration 255, loss = 0.0041938903741538525
iteration 256, loss = 0.004272241611033678
iteration 257, loss = 0.005202447529882193
iteration 258, loss = 0.004473586101084948
iteration 259, loss = 0.004073638003319502
iteration 260, loss = 0.004588813986629248
iteration 261, loss = 0.00457156915217638
iteration 262, loss = 0.004247443750500679
iteration 263, loss = 0.00401194067671895
iteration 264, loss = 0.004185867495834827
iteration 265, loss = 0.0045802295207977295
iteration 266, loss = 0.004020249005407095
iteration 267, loss = 0.003936640452593565
iteration 268, loss = 0.003979797009378672
iteration 269, loss = 0.0046515208669006824
iteration 270, loss = 0.004051967989653349
iteration 271, loss = 0.005505264736711979
iteration 272, loss = 0.004237462300807238
iteration 273, loss = 0.004246697295457125
iteration 274, loss = 0.0061491564847528934
iteration 275, loss = 0.004542246460914612
iteration 276, loss = 0.0043548536486923695
iteration 277, loss = 0.004038194194436073
iteration 278, loss = 0.004585762042552233
iteration 279, loss = 0.004192817490547895
iteration 280, loss = 0.0042480467818677425
iteration 281, loss = 0.0039820484817028046
iteration 282, loss = 0.0040353103540837765
iteration 283, loss = 0.004358403850346804
iteration 284, loss = 0.004102274309843779
iteration 285, loss = 0.004637022502720356
iteration 286, loss = 0.0040919119492173195
iteration 287, loss = 0.003949154634028673
iteration 288, loss = 0.0039023305289447308
iteration 289, loss = 0.004170693457126617
iteration 290, loss = 0.0062592532485723495
iteration 291, loss = 0.0042488169856369495
iteration 292, loss = 0.003914350178092718
iteration 293, loss = 0.004731600172817707
iteration 294, loss = 0.004305975511670113
iteration 295, loss = 0.0041152555495500565
iteration 296, loss = 0.005752734374254942
iteration 297, loss = 0.005166981834918261
iteration 298, loss = 0.004124049097299576
iteration 299, loss = 0.003995280247181654
iteration 300, loss = 0.005754298064857721
iteration 1, loss = 0.005777399986982346
iteration 2, loss = 0.0036452121566981077
iteration 3, loss = 0.004323911387473345
iteration 4, loss = 0.003931863699108362
iteration 5, loss = 0.003798762336373329
iteration 6, loss = 0.004941638093441725
iteration 7, loss = 0.006020611617714167
iteration 8, loss = 0.00433703139424324
iteration 9, loss = 0.004062436521053314
iteration 10, loss = 0.0038576112128794193
iteration 11, loss = 0.0056364880874753
iteration 12, loss = 0.005387352779507637
iteration 13, loss = 0.0036382321268320084
iteration 14, loss = 0.004502799827605486
iteration 15, loss = 0.004743410274386406
iteration 16, loss = 0.004383436869829893
iteration 17, loss = 0.004588269162923098
iteration 18, loss = 0.0037960035260766745
iteration 19, loss = 0.00518821319565177
iteration 20, loss = 0.005211257841438055
iteration 21, loss = 0.004648126661777496
iteration 22, loss = 0.0037886432837694883
iteration 23, loss = 0.005591005086898804
iteration 24, loss = 0.004536518827080727
iteration 25, loss = 0.003954086918383837
iteration 26, loss = 0.004138672724366188
iteration 27, loss = 0.004366958513855934
iteration 28, loss = 0.005586547777056694
iteration 29, loss = 0.003974112216383219
iteration 30, loss = 0.0042379749938845634
iteration 31, loss = 0.0041194153018295765
iteration 32, loss = 0.004774080589413643
iteration 33, loss = 0.005218984559178352
iteration 34, loss = 0.005883079022169113
iteration 35, loss = 0.004323821514844894
iteration 36, loss = 0.003955726511776447
iteration 37, loss = 0.004068402573466301
iteration 38, loss = 0.003827281529083848
iteration 39, loss = 0.004050414077937603
iteration 40, loss = 0.005447943229228258
iteration 41, loss = 0.0040553854778409
iteration 42, loss = 0.004384785890579224
iteration 43, loss = 0.004156076814979315
iteration 44, loss = 0.007123504765331745
iteration 45, loss = 0.003906016703695059
iteration 46, loss = 0.007229729555547237
iteration 47, loss = 0.004485356621444225
iteration 48, loss = 0.003895397298038006
iteration 49, loss = 0.004105845931917429
iteration 50, loss = 0.004275747574865818
iteration 51, loss = 0.004266582429409027
iteration 52, loss = 0.00431852275505662
iteration 53, loss = 0.0037489994429051876
iteration 54, loss = 0.004099156707525253
iteration 55, loss = 0.005144705064594746
iteration 56, loss = 0.004225221462547779
iteration 57, loss = 0.004018927458673716
iteration 58, loss = 0.0044621797278523445
iteration 59, loss = 0.004136905539780855
iteration 60, loss = 0.004500852897763252
iteration 61, loss = 0.004339748062193394
iteration 62, loss = 0.00678411778062582
iteration 63, loss = 0.004841953981667757
iteration 64, loss = 0.0038981353864073753
iteration 65, loss = 0.004217047709971666
iteration 66, loss = 0.00400661863386631
iteration 67, loss = 0.006779380142688751
iteration 68, loss = 0.005040738731622696
iteration 69, loss = 0.0051574185490608215
iteration 70, loss = 0.0045510041527450085
iteration 71, loss = 0.006021686363965273
iteration 72, loss = 0.006199318449944258
iteration 73, loss = 0.004321790765970945
iteration 74, loss = 0.004601375199854374
iteration 75, loss = 0.004171902779489756
iteration 76, loss = 0.004232215229421854
iteration 77, loss = 0.003963944967836142
iteration 78, loss = 0.004613158758729696
iteration 79, loss = 0.0044649639166891575
iteration 80, loss = 0.004100313875824213
iteration 81, loss = 0.006872855592519045
iteration 82, loss = 0.0037022954784333706
iteration 83, loss = 0.004414550494402647
iteration 84, loss = 0.006530993152409792
iteration 85, loss = 0.004617126192897558
iteration 86, loss = 0.004628603346645832
iteration 87, loss = 0.00468103913590312
iteration 88, loss = 0.004552311263978481
iteration 89, loss = 0.004137924872338772
iteration 90, loss = 0.004483817145228386
iteration 91, loss = 0.0038476488552987576
iteration 92, loss = 0.003960617817938328
iteration 93, loss = 0.004343681037425995
iteration 94, loss = 0.004468049854040146
iteration 95, loss = 0.005349827464669943
iteration 96, loss = 0.006870693992823362
iteration 97, loss = 0.0040900628082454205
iteration 98, loss = 0.0045958529226481915
iteration 99, loss = 0.003998884931206703
iteration 100, loss = 0.004451680462807417
iteration 101, loss = 0.004037029109895229
iteration 102, loss = 0.0041727544739842415
iteration 103, loss = 0.004075950942933559
iteration 104, loss = 0.004182077012956142
iteration 105, loss = 0.004458110313862562
iteration 106, loss = 0.004237063229084015
iteration 107, loss = 0.0040017724968492985
iteration 108, loss = 0.004434139002114534
iteration 109, loss = 0.005131449084728956
iteration 110, loss = 0.0038769536186009645
iteration 111, loss = 0.004033284727483988
iteration 112, loss = 0.004142293240875006
iteration 113, loss = 0.004106416832655668
iteration 114, loss = 0.004304925911128521
iteration 115, loss = 0.004441170021891594
iteration 116, loss = 0.006761695723980665
iteration 117, loss = 0.005177040584385395
iteration 118, loss = 0.00497370446100831
iteration 119, loss = 0.005208361893892288
iteration 120, loss = 0.004040922969579697
iteration 121, loss = 0.00405661016702652
iteration 122, loss = 0.00667263800278306
iteration 123, loss = 0.004760103765875101
iteration 124, loss = 0.004326554946601391
iteration 125, loss = 0.0046529825776815414
iteration 126, loss = 0.004600281827151775
iteration 127, loss = 0.0042504677549004555
iteration 128, loss = 0.0043431795202195644
iteration 129, loss = 0.005942571442574263
iteration 130, loss = 0.004004552960395813
iteration 131, loss = 0.006207868456840515
iteration 132, loss = 0.006693736650049686
iteration 133, loss = 0.004282587207853794
iteration 134, loss = 0.004142049700021744
iteration 135, loss = 0.005590979941189289
iteration 136, loss = 0.0037665448617190123
iteration 137, loss = 0.0040595936588943005
iteration 138, loss = 0.005826150067150593
iteration 139, loss = 0.004126742947846651
iteration 140, loss = 0.004899955354630947
iteration 141, loss = 0.0047670272178947926
iteration 142, loss = 0.004312670789659023
iteration 143, loss = 0.004570023622363806
iteration 144, loss = 0.005929274950176477
iteration 145, loss = 0.005880470387637615
iteration 146, loss = 0.004067725036293268
iteration 147, loss = 0.004507384262979031
iteration 148, loss = 0.0043000467121601105
iteration 149, loss = 0.006980509962886572
iteration 150, loss = 0.004065550398081541
iteration 151, loss = 0.0064618997275829315
iteration 152, loss = 0.0052487519569695
iteration 153, loss = 0.0040275827050209045
iteration 154, loss = 0.004845511168241501
iteration 155, loss = 0.004021201748400927
iteration 156, loss = 0.0059663839638233185
iteration 157, loss = 0.005436847452074289
iteration 158, loss = 0.004427241627126932
iteration 159, loss = 0.004320237785577774
iteration 160, loss = 0.004225822165608406
iteration 161, loss = 0.004905530717223883
iteration 162, loss = 0.004696972668170929
iteration 163, loss = 0.004505551885813475
iteration 164, loss = 0.00400103535503149
iteration 165, loss = 0.005531451664865017
iteration 166, loss = 0.004639495629817247
iteration 167, loss = 0.004040050785988569
iteration 168, loss = 0.003814131021499634
iteration 169, loss = 0.004362837411463261
iteration 170, loss = 0.006181642878800631
iteration 171, loss = 0.004132376983761787
iteration 172, loss = 0.004770445637404919
iteration 173, loss = 0.004213686566799879
iteration 174, loss = 0.0043698642402887344
iteration 175, loss = 0.005417913664132357
iteration 176, loss = 0.004202897660434246
iteration 177, loss = 0.004995076451450586
iteration 178, loss = 0.0045888060703873634
iteration 179, loss = 0.0042137084528803825
iteration 180, loss = 0.004801084287464619
iteration 181, loss = 0.004044618457555771
iteration 182, loss = 0.004050954710692167
iteration 183, loss = 0.005511342082172632
iteration 184, loss = 0.004147663712501526
iteration 185, loss = 0.003940599039196968
iteration 186, loss = 0.004195245914161205
iteration 187, loss = 0.007435789797455072
iteration 188, loss = 0.0040947613306343555
iteration 189, loss = 0.00519221555441618
iteration 190, loss = 0.00409390265122056
iteration 191, loss = 0.004157209303230047
iteration 192, loss = 0.004099566023796797
iteration 193, loss = 0.005945018958300352
iteration 194, loss = 0.0045340294018387794
iteration 195, loss = 0.005306599196046591
iteration 196, loss = 0.003979743458330631
iteration 197, loss = 0.004080954473465681
iteration 198, loss = 0.004560067318379879
iteration 199, loss = 0.005158040672540665
iteration 200, loss = 0.0039869979955255985
iteration 201, loss = 0.004554059822112322
iteration 202, loss = 0.003719072788953781
iteration 203, loss = 0.005898930132389069
iteration 204, loss = 0.006107125896960497
iteration 205, loss = 0.004729869309812784
iteration 206, loss = 0.00452225049957633
iteration 207, loss = 0.00400941027328372
iteration 208, loss = 0.0038300093729048967
iteration 209, loss = 0.004673035815358162
iteration 210, loss = 0.004447356332093477
iteration 211, loss = 0.004019342362880707
iteration 212, loss = 0.004504140000790358
iteration 213, loss = 0.004332003649324179
iteration 214, loss = 0.004440667107701302
iteration 215, loss = 0.0066476911306381226
iteration 216, loss = 0.00543219456449151
iteration 217, loss = 0.00566645385697484
iteration 218, loss = 0.003968037664890289
iteration 219, loss = 0.00528722396120429
iteration 220, loss = 0.00479587446898222
iteration 221, loss = 0.0045294915325939655
iteration 222, loss = 0.0041654156520962715
iteration 223, loss = 0.006020328961312771
iteration 224, loss = 0.005001121666282415
iteration 225, loss = 0.003839437384158373
iteration 226, loss = 0.005286059807986021
iteration 227, loss = 0.004176148679107428
iteration 228, loss = 0.004144306294620037
iteration 229, loss = 0.004308572504669428
iteration 230, loss = 0.004201761446893215
iteration 231, loss = 0.0041656787507236
iteration 232, loss = 0.004721198230981827
iteration 233, loss = 0.00412985822185874
iteration 234, loss = 0.004068446345627308
iteration 235, loss = 0.004045359790325165
iteration 236, loss = 0.0040399422869086266
iteration 237, loss = 0.0037581343203783035
iteration 238, loss = 0.0058665950782597065
iteration 239, loss = 0.0038734828121960163
iteration 240, loss = 0.004340026993304491
iteration 241, loss = 0.004354435484856367
iteration 242, loss = 0.003932831343263388
iteration 243, loss = 0.004308024421334267
iteration 244, loss = 0.004475910682231188
iteration 245, loss = 0.004254105966538191
iteration 246, loss = 0.004111806862056255
iteration 247, loss = 0.004438306204974651
iteration 248, loss = 0.006041143089532852
iteration 249, loss = 0.00433245999738574
iteration 250, loss = 0.005876963958144188
iteration 251, loss = 0.0037307203747332096
iteration 252, loss = 0.005893465131521225
iteration 253, loss = 0.006414859090000391
iteration 254, loss = 0.00412997230887413
iteration 255, loss = 0.00448188791051507
iteration 256, loss = 0.004563219845294952
iteration 257, loss = 0.010545906610786915
iteration 258, loss = 0.003970395307987928
iteration 259, loss = 0.004830085206776857
iteration 260, loss = 0.006359138526022434
iteration 261, loss = 0.0040995595045387745
iteration 262, loss = 0.0073955608531832695
iteration 263, loss = 0.0047044772654771805
iteration 264, loss = 0.004274520557373762
iteration 265, loss = 0.004245331045240164
iteration 266, loss = 0.0044116247445344925
iteration 267, loss = 0.004182047210633755
iteration 268, loss = 0.004230747930705547
iteration 269, loss = 0.005227574147284031
iteration 270, loss = 0.005867587402462959
iteration 271, loss = 0.004356402438133955
iteration 272, loss = 0.003911372274160385
iteration 273, loss = 0.00404304638504982
iteration 274, loss = 0.004412333946675062
iteration 275, loss = 0.0037334412336349487
iteration 276, loss = 0.005983483046293259
iteration 277, loss = 0.0037325823213905096
iteration 278, loss = 0.003907462116330862
iteration 279, loss = 0.007073820568621159
iteration 280, loss = 0.004204262979328632
iteration 281, loss = 0.003934788983315229
iteration 282, loss = 0.004325140733271837
iteration 283, loss = 0.005476620048284531
iteration 284, loss = 0.004245567135512829
iteration 285, loss = 0.004558415152132511
iteration 286, loss = 0.004565850365906954
iteration 287, loss = 0.004558774176985025
iteration 288, loss = 0.004208017606288195
iteration 289, loss = 0.004448098596185446
iteration 290, loss = 0.004595946054905653
iteration 291, loss = 0.004072910640388727
iteration 292, loss = 0.004491616040468216
iteration 293, loss = 0.005583878606557846
iteration 294, loss = 0.004114714916795492
iteration 295, loss = 0.004671797156333923
iteration 296, loss = 0.0036887379828840494
iteration 297, loss = 0.0040497505106031895
iteration 298, loss = 0.00421117153018713
iteration 299, loss = 0.004246431402862072
iteration 300, loss = 0.007539288606494665
iteration 1, loss = 0.004118436016142368
iteration 2, loss = 0.0040859258733689785
iteration 3, loss = 0.0048998999409377575
iteration 4, loss = 0.005509352311491966
iteration 5, loss = 0.005238479934632778
iteration 6, loss = 0.004163950681686401
iteration 7, loss = 0.004050151444971561
iteration 8, loss = 0.004191916435956955
iteration 9, loss = 0.004697020631283522
iteration 10, loss = 0.004372702911496162
iteration 11, loss = 0.004406764637678862
iteration 12, loss = 0.004116450902074575
iteration 13, loss = 0.0042938003316521645
iteration 14, loss = 0.007357148453593254
iteration 15, loss = 0.0043210783042013645
iteration 16, loss = 0.003909111022949219
iteration 17, loss = 0.005444858223199844
iteration 18, loss = 0.00576136214658618
iteration 19, loss = 0.004005585331469774
iteration 20, loss = 0.004660561680793762
iteration 21, loss = 0.004196772351861
iteration 22, loss = 0.0038120949175208807
iteration 23, loss = 0.004069881979376078
iteration 24, loss = 0.004299248103052378
iteration 25, loss = 0.004339830484241247
iteration 26, loss = 0.0037170203868299723
iteration 27, loss = 0.005588982254266739
iteration 28, loss = 0.006158425938338041
iteration 29, loss = 0.004221182782202959
iteration 30, loss = 0.004045011941343546
iteration 31, loss = 0.004052295815199614
iteration 32, loss = 0.004220847971737385
iteration 33, loss = 0.004333318676799536
iteration 34, loss = 0.004204180091619492
iteration 35, loss = 0.0048116156831383705
iteration 36, loss = 0.0046820007264614105
iteration 37, loss = 0.004300189670175314
iteration 38, loss = 0.003935983870178461
iteration 39, loss = 0.004135051742196083
iteration 40, loss = 0.00489489920437336
iteration 41, loss = 0.0037754373624920845
iteration 42, loss = 0.004571904893964529
iteration 43, loss = 0.005700346548110247
iteration 44, loss = 0.00450490228831768
iteration 45, loss = 0.0052474443800747395
iteration 46, loss = 0.003971825819462538
iteration 47, loss = 0.004178710747510195
iteration 48, loss = 0.00484671862795949
iteration 49, loss = 0.006208992563188076
iteration 50, loss = 0.004340642131865025
iteration 51, loss = 0.004346458241343498
iteration 52, loss = 0.005699362605810165
iteration 53, loss = 0.006561518181115389
iteration 54, loss = 0.004532220773398876
iteration 55, loss = 0.004511676263064146
iteration 56, loss = 0.004105409607291222
iteration 57, loss = 0.004251974169164896
iteration 58, loss = 0.0041509345173835754
iteration 59, loss = 0.004291850607842207
iteration 60, loss = 0.004045356996357441
iteration 61, loss = 0.005492310039699078
iteration 62, loss = 0.005724859889596701
iteration 63, loss = 0.004025102127343416
iteration 64, loss = 0.003866138868033886
iteration 65, loss = 0.004321442451328039
iteration 66, loss = 0.006205205339938402
iteration 67, loss = 0.004045611247420311
iteration 68, loss = 0.004806114360690117
iteration 69, loss = 0.004912998527288437
iteration 70, loss = 0.004330119583755732
iteration 71, loss = 0.004330486990511417
iteration 72, loss = 0.004527122713625431
iteration 73, loss = 0.00474809342995286
iteration 74, loss = 0.004644768312573433
iteration 75, loss = 0.004207815509289503
iteration 76, loss = 0.003921049647033215
iteration 77, loss = 0.004119394347071648
iteration 78, loss = 0.004711409565061331
iteration 79, loss = 0.004051072522997856
iteration 80, loss = 0.004402737598866224
iteration 81, loss = 0.0040238648653030396
iteration 82, loss = 0.005799080245196819
iteration 83, loss = 0.006163208745419979
iteration 84, loss = 0.0051234932616353035
iteration 85, loss = 0.00414146576076746
iteration 86, loss = 0.00425858935341239
iteration 87, loss = 0.004545568488538265
iteration 88, loss = 0.004458865150809288
iteration 89, loss = 0.004546807147562504
iteration 90, loss = 0.004180654417723417
iteration 91, loss = 0.004323158413171768
iteration 92, loss = 0.004398843739181757
iteration 93, loss = 0.003686656942591071
iteration 94, loss = 0.004302775952965021
iteration 95, loss = 0.004082807339727879
iteration 96, loss = 0.006925914436578751
iteration 97, loss = 0.006772823631763458
iteration 98, loss = 0.004322864580899477
iteration 99, loss = 0.004035835154354572
iteration 100, loss = 0.0037809102796018124
iteration 101, loss = 0.004770725499838591
iteration 102, loss = 0.004611450247466564
iteration 103, loss = 0.004580022767186165
iteration 104, loss = 0.005191902630031109
iteration 105, loss = 0.005848981905728579
iteration 106, loss = 0.004409787245094776
iteration 107, loss = 0.004007223527878523
iteration 108, loss = 0.006723207421600819
iteration 109, loss = 0.004045533947646618
iteration 110, loss = 0.0039060742128640413
iteration 111, loss = 0.0045439801178872585
iteration 112, loss = 0.004318720195442438
iteration 113, loss = 0.006890171207487583
iteration 114, loss = 0.005712821613997221
iteration 115, loss = 0.004650895483791828
iteration 116, loss = 0.004289546050131321
iteration 117, loss = 0.0038425365928560495
iteration 118, loss = 0.003913079854100943
iteration 119, loss = 0.005912644322961569
iteration 120, loss = 0.005423019174486399
iteration 121, loss = 0.007037317380309105
iteration 122, loss = 0.005157815292477608
iteration 123, loss = 0.004496291279792786
iteration 124, loss = 0.004023814108222723
iteration 125, loss = 0.00395049387589097
iteration 126, loss = 0.0037698931992053986
iteration 127, loss = 0.003978509455919266
iteration 128, loss = 0.005252984818071127
iteration 129, loss = 0.0045860521495342255
iteration 130, loss = 0.0037151616998016834
iteration 131, loss = 0.004378375131636858
iteration 132, loss = 0.005511990282684565
iteration 133, loss = 0.004587812349200249
iteration 134, loss = 0.007578438613563776
iteration 135, loss = 0.004743320867419243
iteration 136, loss = 0.003906627185642719
iteration 137, loss = 0.004371305927634239
iteration 138, loss = 0.006374592427164316
iteration 139, loss = 0.005284010898321867
iteration 140, loss = 0.0043152738362550735
iteration 141, loss = 0.004730588290840387
iteration 142, loss = 0.0038804051000624895
iteration 143, loss = 0.004486257676035166
iteration 144, loss = 0.0038896515034139156
iteration 145, loss = 0.0038645619060844183
iteration 146, loss = 0.00403572665527463
iteration 147, loss = 0.00452684611082077
iteration 148, loss = 0.004381476901471615
iteration 149, loss = 0.00395387364551425
iteration 150, loss = 0.005085133481770754
iteration 151, loss = 0.004344516433775425
iteration 152, loss = 0.00504003232344985
iteration 153, loss = 0.005019090138375759
iteration 154, loss = 0.006026185117661953
iteration 155, loss = 0.005705920048058033
iteration 156, loss = 0.004421660210937262
iteration 157, loss = 0.005466524977236986
iteration 158, loss = 0.0043713003396987915
iteration 159, loss = 0.004269133321940899
iteration 160, loss = 0.004884588066488504
iteration 161, loss = 0.005328139290213585
iteration 162, loss = 0.007897098548710346
iteration 163, loss = 0.004186781123280525
iteration 164, loss = 0.0040336595848202705
iteration 165, loss = 0.004145686514675617
iteration 166, loss = 0.004148188047111034
iteration 167, loss = 0.004420769866555929
iteration 168, loss = 0.00411948561668396
iteration 169, loss = 0.0041916267946362495
iteration 170, loss = 0.004233627580106258
iteration 171, loss = 0.007361204829066992
iteration 172, loss = 0.004180931020528078
iteration 173, loss = 0.004250021185725927
iteration 174, loss = 0.004133207257837057
iteration 175, loss = 0.0037014763802289963
iteration 176, loss = 0.004557023756206036
iteration 177, loss = 0.004128193017095327
iteration 178, loss = 0.006034658756107092
iteration 179, loss = 0.0069396342150866985
iteration 180, loss = 0.004378317855298519
iteration 181, loss = 0.0040097422897815704
iteration 182, loss = 0.004578883294016123
iteration 183, loss = 0.003993124235421419
iteration 184, loss = 0.004859956447035074
iteration 185, loss = 0.005129517987370491
iteration 186, loss = 0.0045205834321677685
iteration 187, loss = 0.004655441734939814
iteration 188, loss = 0.004283770453184843
iteration 189, loss = 0.003890964901074767
iteration 190, loss = 0.004336862824857235
iteration 191, loss = 0.004140020348131657
iteration 192, loss = 0.0041859205812215805
iteration 193, loss = 0.004360891878604889
iteration 194, loss = 0.004180373158305883
iteration 195, loss = 0.004801757168024778
iteration 196, loss = 0.003976273816078901
iteration 197, loss = 0.004191671498119831
iteration 198, loss = 0.004049152135848999
iteration 199, loss = 0.0060495538637042046
iteration 200, loss = 0.0060219112783670425
iteration 201, loss = 0.004144962877035141
iteration 202, loss = 0.00392100540921092
iteration 203, loss = 0.0039053920190781355
iteration 204, loss = 0.00494352774694562
iteration 205, loss = 0.003992101643234491
iteration 206, loss = 0.004306234419345856
iteration 207, loss = 0.004458241164684296
iteration 208, loss = 0.00610714266076684
iteration 209, loss = 0.005403172690421343
iteration 210, loss = 0.004314229357987642
iteration 211, loss = 0.003951480146497488
iteration 212, loss = 0.004686956759542227
iteration 213, loss = 0.004016201011836529
iteration 214, loss = 0.004199535120278597
iteration 215, loss = 0.004712175112217665
iteration 216, loss = 0.004389716777950525
iteration 217, loss = 0.004251172766089439
iteration 218, loss = 0.004534092266112566
iteration 219, loss = 0.004190757405012846
iteration 220, loss = 0.00726375263184309
iteration 221, loss = 0.0039235856384038925
iteration 222, loss = 0.0054399496875703335
iteration 223, loss = 0.003989516757428646
iteration 224, loss = 0.003976659849286079
iteration 225, loss = 0.006988979410380125
iteration 226, loss = 0.0043528126552701
iteration 227, loss = 0.004235288128256798
iteration 228, loss = 0.005729634780436754
iteration 229, loss = 0.004389305133372545
iteration 230, loss = 0.004340101964771748
iteration 231, loss = 0.003834846895188093
iteration 232, loss = 0.004544214811176062
iteration 233, loss = 0.00553991599008441
iteration 234, loss = 0.004165990278124809
iteration 235, loss = 0.004807410761713982
iteration 236, loss = 0.00401374651119113
iteration 237, loss = 0.004966509994119406
iteration 238, loss = 0.0040512788109481335
iteration 239, loss = 0.00411574961617589
iteration 240, loss = 0.004485451616346836
iteration 241, loss = 0.006919210776686668
iteration 242, loss = 0.004509114194661379
iteration 243, loss = 0.005429910961538553
iteration 244, loss = 0.005750973243266344
iteration 245, loss = 0.005581571254879236
iteration 246, loss = 0.00438939593732357
iteration 247, loss = 0.004508812911808491
iteration 248, loss = 0.004524307791143656
iteration 249, loss = 0.0043205986730754375
iteration 250, loss = 0.0039023098070174456
iteration 251, loss = 0.0042970753274858
iteration 252, loss = 0.0044414889998734
iteration 253, loss = 0.004050763789564371
iteration 254, loss = 0.0037543107755482197
iteration 255, loss = 0.005153551697731018
iteration 256, loss = 0.005723173730075359
iteration 257, loss = 0.004032935947179794
iteration 258, loss = 0.005543060600757599
iteration 259, loss = 0.004276864230632782
iteration 260, loss = 0.0038058862555772066
iteration 261, loss = 0.003965197596698999
iteration 262, loss = 0.004076487384736538
iteration 263, loss = 0.0040634796023368835
iteration 264, loss = 0.004831995349377394
iteration 265, loss = 0.004017536994069815
iteration 266, loss = 0.006040634121745825
iteration 267, loss = 0.003863830352202058
iteration 268, loss = 0.003892705077305436
iteration 269, loss = 0.004093131050467491
iteration 270, loss = 0.004280525259673595
iteration 271, loss = 0.0068356432020664215
iteration 272, loss = 0.004366732202470303
iteration 273, loss = 0.0053660389967262745
iteration 274, loss = 0.004920325241982937
iteration 275, loss = 0.004261603578925133
iteration 276, loss = 0.005336823873221874
iteration 277, loss = 0.006065987050533295
iteration 278, loss = 0.0055067227222025394
iteration 279, loss = 0.004478795919567347
iteration 280, loss = 0.004240287933498621
iteration 281, loss = 0.004257419612258673
iteration 282, loss = 0.004040160216391087
iteration 283, loss = 0.006793434731662273
iteration 284, loss = 0.003994728438556194
iteration 285, loss = 0.004208290483802557
iteration 286, loss = 0.006841964554041624
iteration 287, loss = 0.006467824336141348
iteration 288, loss = 0.005053933709859848
iteration 289, loss = 0.0042875055223703384
iteration 290, loss = 0.004574561025947332
iteration 291, loss = 0.006821212824434042
iteration 292, loss = 0.004886247217655182
iteration 293, loss = 0.004221050068736076
iteration 294, loss = 0.004386957734823227
iteration 295, loss = 0.004498166497796774
iteration 296, loss = 0.0038968066219240427
iteration 297, loss = 0.0035782193299382925
iteration 298, loss = 0.0042271688580513
iteration 299, loss = 0.00409685680642724
iteration 300, loss = 0.004203129094094038
iteration 1, loss = 0.0042760358192026615
iteration 2, loss = 0.00415401766076684
iteration 3, loss = 0.004851199220865965
iteration 4, loss = 0.004255990497767925
iteration 5, loss = 0.0056277173571288586
iteration 6, loss = 0.003979841247200966
iteration 7, loss = 0.004019489977508783
iteration 8, loss = 0.004647841211408377
iteration 9, loss = 0.004635276272892952
iteration 10, loss = 0.004299596417695284
iteration 11, loss = 0.0040275901556015015
iteration 12, loss = 0.0038321816828101873
iteration 13, loss = 0.004067428410053253
iteration 14, loss = 0.005418197251856327
iteration 15, loss = 0.0040136054158210754
iteration 16, loss = 0.004307617899030447
iteration 17, loss = 0.0043170880526304245
iteration 18, loss = 0.0045197284780442715
iteration 19, loss = 0.004603707231581211
iteration 20, loss = 0.004060939885675907
iteration 21, loss = 0.003984706476330757
iteration 22, loss = 0.00536070391535759
iteration 23, loss = 0.006024649366736412
iteration 24, loss = 0.0043616448529064655
iteration 25, loss = 0.0039681033231318
iteration 26, loss = 0.003543916391208768
iteration 27, loss = 0.004148345440626144
iteration 28, loss = 0.003885739715769887
iteration 29, loss = 0.006909698247909546
iteration 30, loss = 0.006816773675382137
iteration 31, loss = 0.004199432209134102
iteration 32, loss = 0.003902869299054146
iteration 33, loss = 0.004595978185534477
iteration 34, loss = 0.004127803258597851
iteration 35, loss = 0.004679917823523283
iteration 36, loss = 0.003955129534006119
iteration 37, loss = 0.004254222847521305
iteration 38, loss = 0.004386784974485636
iteration 39, loss = 0.004568193107843399
iteration 40, loss = 0.00725118862465024
iteration 41, loss = 0.004309117794036865
iteration 42, loss = 0.005846534855663776
iteration 43, loss = 0.004204450640827417
iteration 44, loss = 0.007052098400890827
iteration 45, loss = 0.005911080166697502
iteration 46, loss = 0.0041185831651091576
iteration 47, loss = 0.00434143515303731
iteration 48, loss = 0.004083474166691303
iteration 49, loss = 0.0047303736209869385
iteration 50, loss = 0.0040368577465415
iteration 51, loss = 0.007859861478209496
iteration 52, loss = 0.006761872675269842
iteration 53, loss = 0.0044433618895709515
iteration 54, loss = 0.005873189773410559
iteration 55, loss = 0.004132451955229044
iteration 56, loss = 0.00578466709703207
iteration 57, loss = 0.004833740647882223
iteration 58, loss = 0.004800708964467049
iteration 59, loss = 0.006729849614202976
iteration 60, loss = 0.004386022221297026
iteration 61, loss = 0.004014406818896532
iteration 62, loss = 0.005897149909287691
iteration 63, loss = 0.004457218572497368
iteration 64, loss = 0.004107806831598282
iteration 65, loss = 0.0042363922111690044
iteration 66, loss = 0.004357421770691872
iteration 67, loss = 0.0039907181635499
iteration 68, loss = 0.005145553965121508
iteration 69, loss = 0.00392747949808836
iteration 70, loss = 0.004066127352416515
iteration 71, loss = 0.004529319237917662
iteration 72, loss = 0.00423925556242466
iteration 73, loss = 0.004490867257118225
iteration 74, loss = 0.003994157072156668
iteration 75, loss = 0.005762101151049137
iteration 76, loss = 0.005221865139901638
iteration 77, loss = 0.004939782898873091
iteration 78, loss = 0.004852253943681717
iteration 79, loss = 0.004403504077345133
iteration 80, loss = 0.004198497161269188
iteration 81, loss = 0.003966316115111113
iteration 82, loss = 0.003941384609788656
iteration 83, loss = 0.004152454435825348
iteration 84, loss = 0.006112668197602034
iteration 85, loss = 0.005288676358759403
iteration 86, loss = 0.004491487983614206
iteration 87, loss = 0.0042276810854673386
iteration 88, loss = 0.004324277397245169
iteration 89, loss = 0.004365542437881231
iteration 90, loss = 0.005528484471142292
iteration 91, loss = 0.0056022466160357
iteration 92, loss = 0.004668727517127991
iteration 93, loss = 0.006596175022423267
iteration 94, loss = 0.004110353067517281
iteration 95, loss = 0.00474676163867116
iteration 96, loss = 0.004099463112652302
iteration 97, loss = 0.0047234464436769485
iteration 98, loss = 0.0045088352635502815
iteration 99, loss = 0.006890908349305391
iteration 100, loss = 0.004050310235470533
iteration 101, loss = 0.004380679689347744
iteration 102, loss = 0.004176688846200705
iteration 103, loss = 0.004724250175058842
iteration 104, loss = 0.004107045009732246
iteration 105, loss = 0.005973758175969124
iteration 106, loss = 0.003728499636054039
iteration 107, loss = 0.0042456937953829765
iteration 108, loss = 0.00649801641702652
iteration 109, loss = 0.004345486406236887
iteration 110, loss = 0.004288794938474894
iteration 111, loss = 0.004369085654616356
iteration 112, loss = 0.0041267662309110165
iteration 113, loss = 0.0040204767137765884
iteration 114, loss = 0.004407746251672506
iteration 115, loss = 0.004593532532453537
iteration 116, loss = 0.0041839382611215115
iteration 117, loss = 0.004443934187293053
iteration 118, loss = 0.004661325365304947
iteration 119, loss = 0.004306015092879534
iteration 120, loss = 0.004487986676394939
iteration 121, loss = 0.004946004133671522
iteration 122, loss = 0.0052110785618424416
iteration 123, loss = 0.004480269737541676
iteration 124, loss = 0.004521125461906195
iteration 125, loss = 0.003899466013535857
iteration 126, loss = 0.004161045886576176
iteration 127, loss = 0.0052650789730250835
iteration 128, loss = 0.004160476848483086
iteration 129, loss = 0.004452667199075222
iteration 130, loss = 0.003946930170059204
iteration 131, loss = 0.004303252790123224
iteration 132, loss = 0.003832918358966708
iteration 133, loss = 0.005461225286126137
iteration 134, loss = 0.004308308474719524
iteration 135, loss = 0.00410836236551404
iteration 136, loss = 0.003810451366007328
iteration 137, loss = 0.004142744466662407
iteration 138, loss = 0.005734701175242662
iteration 139, loss = 0.00439800787717104
iteration 140, loss = 0.0041852775029838085
iteration 141, loss = 0.003957285080105066
iteration 142, loss = 0.004067901987582445
iteration 143, loss = 0.0046242838725447655
iteration 144, loss = 0.0041706315241754055
iteration 145, loss = 0.004399164579808712
iteration 146, loss = 0.004032106604427099
iteration 147, loss = 0.005158330779522657
iteration 148, loss = 0.004324915818870068
iteration 149, loss = 0.005459565203636885
iteration 150, loss = 0.004067898727953434
iteration 151, loss = 0.004679626319557428
iteration 152, loss = 0.004263375885784626
iteration 153, loss = 0.00430971197783947
iteration 154, loss = 0.004392154980450869
iteration 155, loss = 0.004233327694237232
iteration 156, loss = 0.0038432395085692406
iteration 157, loss = 0.0041708326898515224
iteration 158, loss = 0.004115010146051645
iteration 159, loss = 0.00850746687501669
iteration 160, loss = 0.0038109500892460346
iteration 161, loss = 0.004193521104753017
iteration 162, loss = 0.006365144159644842
iteration 163, loss = 0.00435534818097949
iteration 164, loss = 0.004819453693926334
iteration 165, loss = 0.005113828927278519
iteration 166, loss = 0.004777503199875355
iteration 167, loss = 0.004229453392326832
iteration 168, loss = 0.004746210295706987
iteration 169, loss = 0.005880516953766346
iteration 170, loss = 0.0041593462228775024
iteration 171, loss = 0.003916562534868717
iteration 172, loss = 0.003996733576059341
iteration 173, loss = 0.003937424160540104
iteration 174, loss = 0.0043395389802753925
iteration 175, loss = 0.004243344534188509
iteration 176, loss = 0.004193716216832399
iteration 177, loss = 0.004087130539119244
iteration 178, loss = 0.005991457961499691
iteration 179, loss = 0.00430894922465086
iteration 180, loss = 0.003854285227134824
iteration 181, loss = 0.005859477445483208
iteration 182, loss = 0.0042732227593660355
iteration 183, loss = 0.004029860254377127
iteration 184, loss = 0.003976504318416119
iteration 185, loss = 0.0038449333515018225
iteration 186, loss = 0.005573941394686699
iteration 187, loss = 0.004579578060656786
iteration 188, loss = 0.004783959127962589
iteration 189, loss = 0.004967804066836834
iteration 190, loss = 0.004016955383121967
iteration 191, loss = 0.004174228757619858
iteration 192, loss = 0.004870951175689697
iteration 193, loss = 0.004724904894828796
iteration 194, loss = 0.003942290786653757
iteration 195, loss = 0.004067173693329096
iteration 196, loss = 0.004065066576004028
iteration 197, loss = 0.004274400416761637
iteration 198, loss = 0.008785353973507881
iteration 199, loss = 0.005497888196259737
iteration 200, loss = 0.0041094995103776455
iteration 201, loss = 0.006165627855807543
iteration 202, loss = 0.003995295148342848
iteration 203, loss = 0.004454675130546093
iteration 204, loss = 0.004459569230675697
iteration 205, loss = 0.00408688560128212
iteration 206, loss = 0.0038081540260463953
iteration 207, loss = 0.005544638726860285
iteration 208, loss = 0.0044178529642522335
iteration 209, loss = 0.004258702043443918
iteration 210, loss = 0.006834701634943485
iteration 211, loss = 0.0050575449131429195
iteration 212, loss = 0.005250622518360615
iteration 213, loss = 0.004696875810623169
iteration 214, loss = 0.005109780002385378
iteration 215, loss = 0.004108663182705641
iteration 216, loss = 0.005288087297230959
iteration 217, loss = 0.006252159830182791
iteration 218, loss = 0.003607391146942973
iteration 219, loss = 0.004399622790515423
iteration 220, loss = 0.003761118510738015
iteration 221, loss = 0.004673387855291367
iteration 222, loss = 0.0048837014473974705
iteration 223, loss = 0.004131120163947344
iteration 224, loss = 0.004303013905882835
iteration 225, loss = 0.004289781674742699
iteration 226, loss = 0.003963745664805174
iteration 227, loss = 0.004115113988518715
iteration 228, loss = 0.003933980595320463
iteration 229, loss = 0.004148900043219328
iteration 230, loss = 0.005647460464388132
iteration 231, loss = 0.004106504842638969
iteration 232, loss = 0.003875182708725333
iteration 233, loss = 0.004898798186331987
iteration 234, loss = 0.004297367762774229
iteration 235, loss = 0.004454119596630335
iteration 236, loss = 0.004123721271753311
iteration 237, loss = 0.0042009856551885605
iteration 238, loss = 0.005095266737043858
iteration 239, loss = 0.004284899216145277
iteration 240, loss = 0.005928612779825926
iteration 241, loss = 0.005826498381793499
iteration 242, loss = 0.004611772485077381
iteration 243, loss = 0.004350032191723585
iteration 244, loss = 0.005088722798973322
iteration 245, loss = 0.003759443759918213
iteration 246, loss = 0.005298492033034563
iteration 247, loss = 0.00404263474047184
iteration 248, loss = 0.004006094299256802
iteration 249, loss = 0.004335986450314522
iteration 250, loss = 0.006932550109922886
iteration 251, loss = 0.004286840092390776
iteration 252, loss = 0.004310826305299997
iteration 253, loss = 0.004117916338145733
iteration 254, loss = 0.003750545671209693
iteration 255, loss = 0.0036872008349746466
iteration 256, loss = 0.0041226972825825214
iteration 257, loss = 0.004076481331139803
iteration 258, loss = 0.0041626510210335255
iteration 259, loss = 0.004980291239917278
iteration 260, loss = 0.004759642761200666
iteration 261, loss = 0.004524850286543369
iteration 262, loss = 0.00401289900764823
iteration 263, loss = 0.004192657303065062
iteration 264, loss = 0.003953190986067057
iteration 265, loss = 0.008351285010576248
iteration 266, loss = 0.003980861511081457
iteration 267, loss = 0.00424851244315505
iteration 268, loss = 0.006238806527107954
iteration 269, loss = 0.004217005334794521
iteration 270, loss = 0.005299541633576155
iteration 271, loss = 0.005171569064259529
iteration 272, loss = 0.00469481386244297
iteration 273, loss = 0.0056957039050757885
iteration 274, loss = 0.003810632973909378
iteration 275, loss = 0.004352884367108345
iteration 276, loss = 0.00423385389149189
iteration 277, loss = 0.004274233244359493
iteration 278, loss = 0.0039607766084373
iteration 279, loss = 0.004034238401800394
iteration 280, loss = 0.005078810732811689
iteration 281, loss = 0.004468089900910854
iteration 282, loss = 0.00829667504876852
iteration 283, loss = 0.006029810756444931
iteration 284, loss = 0.004219546914100647
iteration 285, loss = 0.0043032909743487835
iteration 286, loss = 0.006928367540240288
iteration 287, loss = 0.005556588992476463
iteration 288, loss = 0.004005967173725367
iteration 289, loss = 0.0044207884930074215
iteration 290, loss = 0.004136053845286369
iteration 291, loss = 0.004087080713361502
iteration 292, loss = 0.003968776669353247
iteration 293, loss = 0.0051348102279007435
iteration 294, loss = 0.0043196179904043674
iteration 295, loss = 0.004370583221316338
iteration 296, loss = 0.0044766985811293125
iteration 297, loss = 0.004550510086119175
iteration 298, loss = 0.007541045546531677
iteration 299, loss = 0.005515747237950563
iteration 300, loss = 0.004145971033722162
iteration 1, loss = 0.004209061618894339
iteration 2, loss = 0.005723064299672842
iteration 3, loss = 0.0061690909788012505
iteration 4, loss = 0.004261814057826996
iteration 5, loss = 0.004360983148217201
iteration 6, loss = 0.004288648720830679
iteration 7, loss = 0.0039116800762712955
iteration 8, loss = 0.005710306111723185
iteration 9, loss = 0.004272352438420057
iteration 10, loss = 0.0054536666721105576
iteration 11, loss = 0.0039545949548482895
iteration 12, loss = 0.004408256150782108
iteration 13, loss = 0.007126696407794952
iteration 14, loss = 0.003936961758881807
iteration 15, loss = 0.004280433990061283
iteration 16, loss = 0.004069216549396515
iteration 17, loss = 0.00404715770855546
iteration 18, loss = 0.00682855537161231
iteration 19, loss = 0.008436940610408783
iteration 20, loss = 0.004332099575549364
iteration 21, loss = 0.004555247258394957
iteration 22, loss = 0.003992481157183647
iteration 23, loss = 0.0046575237065553665
iteration 24, loss = 0.004510869272053242
iteration 25, loss = 0.004826225806027651
iteration 26, loss = 0.004322548396885395
iteration 27, loss = 0.004249881953001022
iteration 28, loss = 0.00418152054771781
iteration 29, loss = 0.0055590663105249405
iteration 30, loss = 0.006479955278337002
iteration 31, loss = 0.0042611598037183285
iteration 32, loss = 0.0041142795234918594
iteration 33, loss = 0.005450361408293247
iteration 34, loss = 0.005847733002156019
iteration 35, loss = 0.003912196960300207
iteration 36, loss = 0.005878320895135403
iteration 37, loss = 0.004850816912949085
iteration 38, loss = 0.004468646366149187
iteration 39, loss = 0.004444066435098648
iteration 40, loss = 0.004100761841982603
iteration 41, loss = 0.004000001586973667
iteration 42, loss = 0.00410072086378932
iteration 43, loss = 0.004155164118856192
iteration 44, loss = 0.006918382365256548
iteration 45, loss = 0.004087843466550112
iteration 46, loss = 0.00563477398827672
iteration 47, loss = 0.004286068957298994
iteration 48, loss = 0.0046579111367464066
iteration 49, loss = 0.00409435061737895
iteration 50, loss = 0.005334819667041302
iteration 51, loss = 0.006048552691936493
iteration 52, loss = 0.004742499440908432
iteration 53, loss = 0.003879047464579344
iteration 54, loss = 0.004006353672593832
iteration 55, loss = 0.0042879232205450535
iteration 56, loss = 0.0037990359123796225
iteration 57, loss = 0.005672040395438671
iteration 58, loss = 0.004409122280776501
iteration 59, loss = 0.004282278474420309
iteration 60, loss = 0.005684232339262962
iteration 61, loss = 0.004385651089251041
iteration 62, loss = 0.003727593692019582
iteration 63, loss = 0.004442192614078522
iteration 64, loss = 0.004038411192595959
iteration 65, loss = 0.004798102658241987
iteration 66, loss = 0.004101025406271219
iteration 67, loss = 0.004052916541695595
iteration 68, loss = 0.0037817098200321198
iteration 69, loss = 0.004478855058550835
iteration 70, loss = 0.004231092054396868
iteration 71, loss = 0.004513369873166084
iteration 72, loss = 0.004020621068775654
iteration 73, loss = 0.006329147610813379
iteration 74, loss = 0.0038639705162495375
iteration 75, loss = 0.004275655839592218
iteration 76, loss = 0.00468846783041954
iteration 77, loss = 0.004036335274577141
iteration 78, loss = 0.003963808063417673
iteration 79, loss = 0.004347587935626507
iteration 80, loss = 0.004155401140451431
iteration 81, loss = 0.005844030063599348
iteration 82, loss = 0.003972380887717009
iteration 83, loss = 0.004623581189662218
iteration 84, loss = 0.0042096953839063644
iteration 85, loss = 0.005318780429661274
iteration 86, loss = 0.005301989149302244
iteration 87, loss = 0.005638096947222948
iteration 88, loss = 0.0039258371107280254
iteration 89, loss = 0.004434570670127869
iteration 90, loss = 0.007210306823253632
iteration 91, loss = 0.005726668052375317
iteration 92, loss = 0.00744993519037962
iteration 93, loss = 0.005319624207913876
iteration 94, loss = 0.004020241089165211
iteration 95, loss = 0.004467001650482416
iteration 96, loss = 0.004541181027889252
iteration 97, loss = 0.004182609263807535
iteration 98, loss = 0.004534050822257996
iteration 99, loss = 0.00398270133882761
iteration 100, loss = 0.004776685498654842
iteration 101, loss = 0.005579672753810883
iteration 102, loss = 0.0037913774140179157
iteration 103, loss = 0.0042706625536084175
iteration 104, loss = 0.00394945265725255
iteration 105, loss = 0.00738761480897665
iteration 106, loss = 0.004863135516643524
iteration 107, loss = 0.005400179419666529
iteration 108, loss = 0.004160810727626085
iteration 109, loss = 0.004203776363283396
iteration 110, loss = 0.0037879440933465958
iteration 111, loss = 0.004590597469359636
iteration 112, loss = 0.004518060479313135
iteration 113, loss = 0.0037152161821722984
iteration 114, loss = 0.005007697734981775
iteration 115, loss = 0.006471686996519566
iteration 116, loss = 0.004354425705969334
iteration 117, loss = 0.0036438526585698128
iteration 118, loss = 0.004787330515682697
iteration 119, loss = 0.0041632503271102905
iteration 120, loss = 0.003998700529336929
iteration 121, loss = 0.004180106334388256
iteration 122, loss = 0.004308539908379316
iteration 123, loss = 0.00450970558449626
iteration 124, loss = 0.0043894266709685326
iteration 125, loss = 0.005683231167495251
iteration 126, loss = 0.003828352550044656
iteration 127, loss = 0.004359464626759291
iteration 128, loss = 0.0038899716455489397
iteration 129, loss = 0.004251713398844004
iteration 130, loss = 0.0038845790550112724
iteration 131, loss = 0.0037889580707997084
iteration 132, loss = 0.004345658700913191
iteration 133, loss = 0.003796954406425357
iteration 134, loss = 0.0039225551299750805
iteration 135, loss = 0.0042497580870985985
iteration 136, loss = 0.007041952107101679
iteration 137, loss = 0.004145623650401831
iteration 138, loss = 0.00491119222715497
iteration 139, loss = 0.00396450562402606
iteration 140, loss = 0.00401671789586544
iteration 141, loss = 0.0054027666337788105
iteration 142, loss = 0.0049467748031020164
iteration 143, loss = 0.003908790647983551
iteration 144, loss = 0.00402100570499897
iteration 145, loss = 0.005058606155216694
iteration 146, loss = 0.009137013927102089
iteration 147, loss = 0.00460922671481967
iteration 148, loss = 0.0039050185587257147
iteration 149, loss = 0.004527492448687553
iteration 150, loss = 0.00616572005674243
iteration 151, loss = 0.005223393905907869
iteration 152, loss = 0.0044460603967309
iteration 153, loss = 0.0037745751906186342
iteration 154, loss = 0.004346757661551237
iteration 155, loss = 0.0037755367811769247
iteration 156, loss = 0.0042673018760979176
iteration 157, loss = 0.004707486368715763
iteration 158, loss = 0.004155374597758055
iteration 159, loss = 0.004923609551042318
iteration 160, loss = 0.004007887095212936
iteration 161, loss = 0.004014454782009125
iteration 162, loss = 0.0037237871438264847
iteration 163, loss = 0.006305179093033075
iteration 164, loss = 0.005935347639024258
iteration 165, loss = 0.004448631312698126
iteration 166, loss = 0.00459121260792017
iteration 167, loss = 0.005547413602471352
iteration 168, loss = 0.005567341111600399
iteration 169, loss = 0.004407833330333233
iteration 170, loss = 0.005438381340354681
iteration 171, loss = 0.004352543968707323
iteration 172, loss = 0.0042933570221066475
iteration 173, loss = 0.004292962606996298
iteration 174, loss = 0.004108079243451357
iteration 175, loss = 0.006922382861375809
iteration 176, loss = 0.004082279745489359
iteration 177, loss = 0.004016131162643433
iteration 178, loss = 0.004046576097607613
iteration 179, loss = 0.004143266472965479
iteration 180, loss = 0.004474099259823561
iteration 181, loss = 0.004421175457537174
iteration 182, loss = 0.0052218688651919365
iteration 183, loss = 0.0037665516138076782
iteration 184, loss = 0.004334424622356892
iteration 185, loss = 0.0038575641810894012
iteration 186, loss = 0.004386245738714933
iteration 187, loss = 0.0038824512157589197
iteration 188, loss = 0.00675113033503294
iteration 189, loss = 0.00436969893053174
iteration 190, loss = 0.004285762086510658
iteration 191, loss = 0.004107026848942041
iteration 192, loss = 0.00397687591612339
iteration 193, loss = 0.004148644860833883
iteration 194, loss = 0.006941476371139288
iteration 195, loss = 0.0036716200411319733
iteration 196, loss = 0.004416855052113533
iteration 197, loss = 0.0039502717554569244
iteration 198, loss = 0.003820029553025961
iteration 199, loss = 0.005206440109759569
iteration 200, loss = 0.005479221232235432
iteration 201, loss = 0.006228231359273195
iteration 202, loss = 0.004175225738435984
iteration 203, loss = 0.00443299300968647
iteration 204, loss = 0.004071083385497332
iteration 205, loss = 0.0058533125557005405
iteration 206, loss = 0.006781898904591799
iteration 207, loss = 0.004571651108562946
iteration 208, loss = 0.0041099898517131805
iteration 209, loss = 0.0042158388532698154
iteration 210, loss = 0.00476334523409605
iteration 211, loss = 0.00557092297822237
iteration 212, loss = 0.004394978750497103
iteration 213, loss = 0.004432749003171921
iteration 214, loss = 0.004033510107547045
iteration 215, loss = 0.004242160823196173
iteration 216, loss = 0.006316525395959616
iteration 217, loss = 0.004221164155751467
iteration 218, loss = 0.0042587267234921455
iteration 219, loss = 0.004025704227387905
iteration 220, loss = 0.00468597374856472
iteration 221, loss = 0.005443891976028681
iteration 222, loss = 0.003619604045525193
iteration 223, loss = 0.004180435091257095
iteration 224, loss = 0.004204683471471071
iteration 225, loss = 0.004033948760479689
iteration 226, loss = 0.004159781616181135
iteration 227, loss = 0.004323248751461506
iteration 228, loss = 0.004232390318065882
iteration 229, loss = 0.004301578737795353
iteration 230, loss = 0.004175630398094654
iteration 231, loss = 0.004520830232650042
iteration 232, loss = 0.004631347022950649
iteration 233, loss = 0.004661513492465019
iteration 234, loss = 0.004254248924553394
iteration 235, loss = 0.004587763920426369
iteration 236, loss = 0.0042875101789832115
iteration 237, loss = 0.004134410060942173
iteration 238, loss = 0.004089429508894682
iteration 239, loss = 0.004795508924871683
iteration 240, loss = 0.00414895499125123
iteration 241, loss = 0.005774007178843021
iteration 242, loss = 0.004612850025296211
iteration 243, loss = 0.004909433890134096
iteration 244, loss = 0.005904376041144133
iteration 245, loss = 0.003905662801116705
iteration 246, loss = 0.004382508806884289
iteration 247, loss = 0.003925326280295849
iteration 248, loss = 0.004216541536152363
iteration 249, loss = 0.003971224185079336
iteration 250, loss = 0.0041090864688158035
iteration 251, loss = 0.00418555922806263
iteration 252, loss = 0.004796932451426983
iteration 253, loss = 0.004653595853596926
iteration 254, loss = 0.004456645809113979
iteration 255, loss = 0.004127629101276398
iteration 256, loss = 0.005140164867043495
iteration 257, loss = 0.0039014234207570553
iteration 258, loss = 0.0058260285295546055
iteration 259, loss = 0.004107411485165358
iteration 260, loss = 0.005357152782380581
iteration 261, loss = 0.004304299596697092
iteration 262, loss = 0.0039461832493543625
iteration 263, loss = 0.003900407114997506
iteration 264, loss = 0.0070645553059875965
iteration 265, loss = 0.006880651693791151
iteration 266, loss = 0.003976325038820505
iteration 267, loss = 0.0042346748523414135
iteration 268, loss = 0.004896190483123064
iteration 269, loss = 0.003985862713307142
iteration 270, loss = 0.003962208516895771
iteration 271, loss = 0.0045906780287623405
iteration 272, loss = 0.003925264347344637
iteration 273, loss = 0.004092326853424311
iteration 274, loss = 0.004301910754293203
iteration 275, loss = 0.006935882847756147
iteration 276, loss = 0.0038751980755478144
iteration 277, loss = 0.004265894647687674
iteration 278, loss = 0.005383783485740423
iteration 279, loss = 0.0040647187270224094
iteration 280, loss = 0.004338060971349478
iteration 281, loss = 0.0041071646846830845
iteration 282, loss = 0.006043084431439638
iteration 283, loss = 0.0045693968422710896
iteration 284, loss = 0.004903040360659361
iteration 285, loss = 0.0038715749979019165
iteration 286, loss = 0.0038318049628287554
iteration 287, loss = 0.0053465645760297775
iteration 288, loss = 0.0037637343630194664
iteration 289, loss = 0.004345598164945841
iteration 290, loss = 0.004314849618822336
iteration 291, loss = 0.004861673340201378
iteration 292, loss = 0.004245314747095108
iteration 293, loss = 0.005902371369302273
iteration 294, loss = 0.004248511977493763
iteration 295, loss = 0.005183120258152485
iteration 296, loss = 0.0041805291548371315
iteration 297, loss = 0.003941780887544155
iteration 298, loss = 0.004320661071687937
iteration 299, loss = 0.009263187646865845
iteration 300, loss = 0.004270866513252258
iteration 1, loss = 0.006814370397478342
iteration 2, loss = 0.0038541541434824467
iteration 3, loss = 0.003962749149650335
iteration 4, loss = 0.0040565417148172855
iteration 5, loss = 0.005774492397904396
iteration 6, loss = 0.005168038886040449
iteration 7, loss = 0.004641701467335224
iteration 8, loss = 0.004012693651020527
iteration 9, loss = 0.004205521196126938
iteration 10, loss = 0.004243272822350264
iteration 11, loss = 0.004258334636688232
iteration 12, loss = 0.004020889289677143
iteration 13, loss = 0.004080859944224358
iteration 14, loss = 0.006815217901021242
iteration 15, loss = 0.006011705379933119
iteration 16, loss = 0.003878246294334531
iteration 17, loss = 0.00399020453915
iteration 18, loss = 0.005886951927095652
iteration 19, loss = 0.005644173361361027
iteration 20, loss = 0.004309455864131451
iteration 21, loss = 0.005127344746142626
iteration 22, loss = 0.0038400187622755766
iteration 23, loss = 0.004736393690109253
iteration 24, loss = 0.0044168708845973015
iteration 25, loss = 0.004266324453055859
iteration 26, loss = 0.006245922297239304
iteration 27, loss = 0.0065555148757994175
iteration 28, loss = 0.004484622739255428
iteration 29, loss = 0.005272937938570976
iteration 30, loss = 0.004083230625838041
iteration 31, loss = 0.007335039786994457
iteration 32, loss = 0.004478386137634516
iteration 33, loss = 0.006310606375336647
iteration 34, loss = 0.005393066443502903
iteration 35, loss = 0.004700716119259596
iteration 36, loss = 0.004281271249055862
iteration 37, loss = 0.004087434150278568
iteration 38, loss = 0.004412679933011532
iteration 39, loss = 0.006049720570445061
iteration 40, loss = 0.00397136528044939
iteration 41, loss = 0.004009731579571962
iteration 42, loss = 0.0051907701417803764
iteration 43, loss = 0.0049283201806247234
iteration 44, loss = 0.004073361866176128
iteration 45, loss = 0.007016913034021854
iteration 46, loss = 0.0041031185537576675
iteration 47, loss = 0.004045532550662756
iteration 48, loss = 0.00468067079782486
iteration 49, loss = 0.0055082193575799465
iteration 50, loss = 0.004050172865390778
iteration 51, loss = 0.0038541266694664955
iteration 52, loss = 0.004658210556954145
iteration 53, loss = 0.004906164947897196
iteration 54, loss = 0.004171302076429129
iteration 55, loss = 0.004444789607077837
iteration 56, loss = 0.004519921261817217
iteration 57, loss = 0.0042929272167384624
iteration 58, loss = 0.0036509588826447725
iteration 59, loss = 0.004272895399481058
iteration 60, loss = 0.0065018730238080025
iteration 61, loss = 0.004320070147514343
iteration 62, loss = 0.003980283625423908
iteration 63, loss = 0.00563356839120388
iteration 64, loss = 0.005526859313249588
iteration 65, loss = 0.006296439561992884
iteration 66, loss = 0.004118630662560463
iteration 67, loss = 0.004339180886745453
iteration 68, loss = 0.0048136101104319096
iteration 69, loss = 0.006319690961390734
iteration 70, loss = 0.0040311673656105995
iteration 71, loss = 0.003965502139180899
iteration 72, loss = 0.0041131023317575455
iteration 73, loss = 0.004305248614400625
iteration 74, loss = 0.005118836648762226
iteration 75, loss = 0.005375883542001247
iteration 76, loss = 0.004002294968813658
iteration 77, loss = 0.004251712467521429
iteration 78, loss = 0.004257966298609972
iteration 79, loss = 0.004330157767981291
iteration 80, loss = 0.00593077577650547
iteration 81, loss = 0.004084805026650429
iteration 82, loss = 0.004261588677763939
iteration 83, loss = 0.00423427065834403
iteration 84, loss = 0.004070092458277941
iteration 85, loss = 0.004005115944892168
iteration 86, loss = 0.004488023929297924
iteration 87, loss = 0.004335793200880289
iteration 88, loss = 0.0042488607577979565
iteration 89, loss = 0.004150118678808212
iteration 90, loss = 0.004234300460666418
iteration 91, loss = 0.0039867982268333435
iteration 92, loss = 0.00539929885417223
iteration 93, loss = 0.003998120781034231
iteration 94, loss = 0.0036327261477708817
iteration 95, loss = 0.003986524883657694
iteration 96, loss = 0.00448775477707386
iteration 97, loss = 0.003996755927801132
iteration 98, loss = 0.00394830759614706
iteration 99, loss = 0.004127848893404007
iteration 100, loss = 0.004146696999669075
iteration 101, loss = 0.004501059651374817
iteration 102, loss = 0.0041488115675747395
iteration 103, loss = 0.003931923303753138
iteration 104, loss = 0.0041242376901209354
iteration 105, loss = 0.0039617884904146194
iteration 106, loss = 0.004310167860239744
iteration 107, loss = 0.004060810431838036
iteration 108, loss = 0.004427238367497921
iteration 109, loss = 0.006366772577166557
iteration 110, loss = 0.004281974863260984
iteration 111, loss = 0.0061982059851288795
iteration 112, loss = 0.006044202949851751
iteration 113, loss = 0.004683990031480789
iteration 114, loss = 0.004204146098345518
iteration 115, loss = 0.005001543089747429
iteration 116, loss = 0.003987686708569527
iteration 117, loss = 0.005141571629792452
iteration 118, loss = 0.00389847788028419
iteration 119, loss = 0.004127352498471737
iteration 120, loss = 0.00492603424936533
iteration 121, loss = 0.0037844229955226183
iteration 122, loss = 0.00397911062464118
iteration 123, loss = 0.004017602652311325
iteration 124, loss = 0.004536347463726997
iteration 125, loss = 0.003972899634391069
iteration 126, loss = 0.004209229722619057
iteration 127, loss = 0.004125447012484074
iteration 128, loss = 0.004118937999010086
iteration 129, loss = 0.003987730015069246
iteration 130, loss = 0.0041275303810834885
iteration 131, loss = 0.004039195831865072
iteration 132, loss = 0.005918347276747227
iteration 133, loss = 0.004595553502440453
iteration 134, loss = 0.004204259719699621
iteration 135, loss = 0.0044690524227917194
iteration 136, loss = 0.0038743559271097183
iteration 137, loss = 0.004740317817777395
iteration 138, loss = 0.004389284644275904
iteration 139, loss = 0.004526291973888874
iteration 140, loss = 0.0041160001419484615
iteration 141, loss = 0.00413156021386385
iteration 142, loss = 0.005684915464371443
iteration 143, loss = 0.0047530061565339565
iteration 144, loss = 0.004439124837517738
iteration 145, loss = 0.003995770588517189
iteration 146, loss = 0.004139686934649944
iteration 147, loss = 0.005228268448263407
iteration 148, loss = 0.004275305196642876
iteration 149, loss = 0.0040453351102769375
iteration 150, loss = 0.005311379674822092
iteration 151, loss = 0.004081147722899914
iteration 152, loss = 0.004800214897841215
iteration 153, loss = 0.004235439468175173
iteration 154, loss = 0.004060020670294762
iteration 155, loss = 0.005926851183176041
iteration 156, loss = 0.004544085822999477
iteration 157, loss = 0.00643201032653451
iteration 158, loss = 0.004483846016228199
iteration 159, loss = 0.007062052842229605
iteration 160, loss = 0.006151844281703234
iteration 161, loss = 0.005118835251778364
iteration 162, loss = 0.003895712783560157
iteration 163, loss = 0.006946456152945757
iteration 164, loss = 0.0039426302537322044
iteration 165, loss = 0.007834595628082752
iteration 166, loss = 0.004087711684405804
iteration 167, loss = 0.004154100548475981
iteration 168, loss = 0.003912565764039755
iteration 169, loss = 0.0038918715436011553
iteration 170, loss = 0.005409188102930784
iteration 171, loss = 0.004264948423951864
iteration 172, loss = 0.005587317049503326
iteration 173, loss = 0.004197737202048302
iteration 174, loss = 0.004198350943624973
iteration 175, loss = 0.008848430588841438
iteration 176, loss = 0.00429906602948904
iteration 177, loss = 0.004773594904690981
iteration 178, loss = 0.004057625308632851
iteration 179, loss = 0.005899637006223202
iteration 180, loss = 0.004153441172093153
iteration 181, loss = 0.004420948214828968
iteration 182, loss = 0.004376082681119442
iteration 183, loss = 0.004281860776245594
iteration 184, loss = 0.0043024057522416115
iteration 185, loss = 0.004323414992541075
iteration 186, loss = 0.004139515105634928
iteration 187, loss = 0.004175003618001938
iteration 188, loss = 0.0045814295299351215
iteration 189, loss = 0.004993355832993984
iteration 190, loss = 0.007580453064292669
iteration 191, loss = 0.003837928408756852
iteration 192, loss = 0.004174984060227871
iteration 193, loss = 0.004048521164804697
iteration 194, loss = 0.0037962542846798897
iteration 195, loss = 0.0041557480581104755
iteration 196, loss = 0.004492574837058783
iteration 197, loss = 0.004688712768256664
iteration 198, loss = 0.003882668213918805
iteration 199, loss = 0.0047846389934420586
iteration 200, loss = 0.00457372609525919
iteration 201, loss = 0.004579175729304552
iteration 202, loss = 0.004125225357711315
iteration 203, loss = 0.0042295013554394245
iteration 204, loss = 0.004194861277937889
iteration 205, loss = 0.005245103966444731
iteration 206, loss = 0.0062741488218307495
iteration 207, loss = 0.004881228320300579
iteration 208, loss = 0.0039079636335372925
iteration 209, loss = 0.003997163847088814
iteration 210, loss = 0.004066671244800091
iteration 211, loss = 0.00409264862537384
iteration 212, loss = 0.003772722091525793
iteration 213, loss = 0.00394853763282299
iteration 214, loss = 0.004617242142558098
iteration 215, loss = 0.004020368214696646
iteration 216, loss = 0.005245148669928312
iteration 217, loss = 0.004250884521752596
iteration 218, loss = 0.0040877279825508595
iteration 219, loss = 0.005709885153919458
iteration 220, loss = 0.004141418728977442
iteration 221, loss = 0.004428254906088114
iteration 222, loss = 0.0039034737274050713
iteration 223, loss = 0.004514184780418873
iteration 224, loss = 0.005600679665803909
iteration 225, loss = 0.004333376418799162
iteration 226, loss = 0.004022462759166956
iteration 227, loss = 0.0038241834845393896
iteration 228, loss = 0.006971423514187336
iteration 229, loss = 0.008569231256842613
iteration 230, loss = 0.004443205427378416
iteration 231, loss = 0.004722315352410078
iteration 232, loss = 0.004580664448440075
iteration 233, loss = 0.0042910524643957615
iteration 234, loss = 0.004127465654164553
iteration 235, loss = 0.004198223352432251
iteration 236, loss = 0.003960092552006245
iteration 237, loss = 0.003935064189136028
iteration 238, loss = 0.003897971473634243
iteration 239, loss = 0.003962850663810968
iteration 240, loss = 0.003824808169156313
iteration 241, loss = 0.0038289257790893316
iteration 242, loss = 0.0045381467789411545
iteration 243, loss = 0.00453579006716609
iteration 244, loss = 0.004346521571278572
iteration 245, loss = 0.003960601054131985
iteration 246, loss = 0.004067137371748686
iteration 247, loss = 0.004270018078386784
iteration 248, loss = 0.004401122685521841
iteration 249, loss = 0.006961107719689608
iteration 250, loss = 0.003976121079176664
iteration 251, loss = 0.005818135105073452
iteration 252, loss = 0.004277034662663937
iteration 253, loss = 0.004278142936527729
iteration 254, loss = 0.0037707265000790358
iteration 255, loss = 0.006104982458055019
iteration 256, loss = 0.005192226264625788
iteration 257, loss = 0.004306291230022907
iteration 258, loss = 0.004162020981311798
iteration 259, loss = 0.003884748322889209
iteration 260, loss = 0.005782644730061293
iteration 261, loss = 0.004837938118726015
iteration 262, loss = 0.004391185939311981
iteration 263, loss = 0.004936079494655132
iteration 264, loss = 0.004172810819000006
iteration 265, loss = 0.0057920245453715324
iteration 266, loss = 0.0041406662203371525
iteration 267, loss = 0.004206579178571701
iteration 268, loss = 0.0039467476308345795
iteration 269, loss = 0.004070499911904335
iteration 270, loss = 0.004129241220653057
iteration 271, loss = 0.004175084177404642
iteration 272, loss = 0.004610202740877867
iteration 273, loss = 0.004122973419725895
iteration 274, loss = 0.0045417980290949345
iteration 275, loss = 0.004874053411185741
iteration 276, loss = 0.004247248638421297
iteration 277, loss = 0.0065031941048800945
iteration 278, loss = 0.004144985694438219
iteration 279, loss = 0.0074873631820082664
iteration 280, loss = 0.003930391278117895
iteration 281, loss = 0.004114875569939613
iteration 282, loss = 0.004350812640041113
iteration 283, loss = 0.003929782658815384
iteration 284, loss = 0.00511111319065094
iteration 285, loss = 0.004392637871205807
iteration 286, loss = 0.006162127945572138
iteration 287, loss = 0.004638562444597483
iteration 288, loss = 0.00447898730635643
iteration 289, loss = 0.004828712437301874
iteration 290, loss = 0.003864143742248416
iteration 291, loss = 0.0061375731602311134
iteration 292, loss = 0.0042017074301838875
iteration 293, loss = 0.004007734823971987
iteration 294, loss = 0.005582330748438835
iteration 295, loss = 0.004434308968484402
iteration 296, loss = 0.003919939044862986
iteration 297, loss = 0.004596428945660591
iteration 298, loss = 0.008622975088655949
iteration 299, loss = 0.004449782893061638
iteration 300, loss = 0.004668118432164192
iteration 1, loss = 0.003965419251471758
iteration 2, loss = 0.004306351765990257
iteration 3, loss = 0.0039652748964726925
iteration 4, loss = 0.003955299034714699
iteration 5, loss = 0.00616965489462018
iteration 6, loss = 0.004428066313266754
iteration 7, loss = 0.0038303525652736425
iteration 8, loss = 0.004426327534019947
iteration 9, loss = 0.0039571672677993774
iteration 10, loss = 0.003878493793308735
iteration 11, loss = 0.004447038285434246
iteration 12, loss = 0.004016548860818148
iteration 13, loss = 0.004150594584643841
iteration 14, loss = 0.0038309141527861357
iteration 15, loss = 0.0040866415947675705
iteration 16, loss = 0.004273502621799707
iteration 17, loss = 0.0042222109623253345
iteration 18, loss = 0.003820928046479821
iteration 19, loss = 0.004067347384989262
iteration 20, loss = 0.003952360711991787
iteration 21, loss = 0.004868566989898682
iteration 22, loss = 0.00406979862600565
iteration 23, loss = 0.0052032265812158585
iteration 24, loss = 0.004314974881708622
iteration 25, loss = 0.004211499355733395
iteration 26, loss = 0.004800320137292147
iteration 27, loss = 0.004611684940755367
iteration 28, loss = 0.004423979669809341
iteration 29, loss = 0.004649903159588575
iteration 30, loss = 0.0039540547877550125
iteration 31, loss = 0.004250525962561369
iteration 32, loss = 0.005470195785164833
iteration 33, loss = 0.005702136550098658
iteration 34, loss = 0.0039060241542756557
iteration 35, loss = 0.004036815837025642
iteration 36, loss = 0.004782002419233322
iteration 37, loss = 0.004280644003301859
iteration 38, loss = 0.004845381248742342
iteration 39, loss = 0.006819401867687702
iteration 40, loss = 0.005906117148697376
iteration 41, loss = 0.004097484517842531
iteration 42, loss = 0.0053733838722109795
iteration 43, loss = 0.004253259394317865
iteration 44, loss = 0.003677541622892022
iteration 45, loss = 0.004235859960317612
iteration 46, loss = 0.0042075710371136665
iteration 47, loss = 0.005548974499106407
iteration 48, loss = 0.007303859107196331
iteration 49, loss = 0.003885262878611684
iteration 50, loss = 0.004176773596554995
iteration 51, loss = 0.005329469684511423
iteration 52, loss = 0.004258787725120783
iteration 53, loss = 0.004964826162904501
iteration 54, loss = 0.004307358060032129
iteration 55, loss = 0.004296432714909315
iteration 56, loss = 0.004726815968751907
iteration 57, loss = 0.0036867663729935884
iteration 58, loss = 0.005433014594018459
iteration 59, loss = 0.004185657948255539
iteration 60, loss = 0.0045164888724684715
iteration 61, loss = 0.005379633978009224
iteration 62, loss = 0.004056510515511036
iteration 63, loss = 0.003930320497602224
iteration 64, loss = 0.004359705373644829
iteration 65, loss = 0.0037739593535661697
iteration 66, loss = 0.004786327946931124
iteration 67, loss = 0.004087336361408234
iteration 68, loss = 0.004113774746656418
iteration 69, loss = 0.00434513483196497
iteration 70, loss = 0.003716246457770467
iteration 71, loss = 0.00403120880946517
iteration 72, loss = 0.004587661009281874
iteration 73, loss = 0.004316798876971006
iteration 74, loss = 0.006776760797947645
iteration 75, loss = 0.004131004214286804
iteration 76, loss = 0.004367763176560402
iteration 77, loss = 0.004149087239056826
iteration 78, loss = 0.004432896617799997
iteration 79, loss = 0.004066565539687872
iteration 80, loss = 0.004124223254621029
iteration 81, loss = 0.004094666801393032
iteration 82, loss = 0.004091862589120865
iteration 83, loss = 0.0038700224831700325
iteration 84, loss = 0.004615718964487314
iteration 85, loss = 0.007182375993579626
iteration 86, loss = 0.004089555237442255
iteration 87, loss = 0.0037870844826102257
iteration 88, loss = 0.004421125166118145
iteration 89, loss = 0.007492158096283674
iteration 90, loss = 0.004458452109247446
iteration 91, loss = 0.004384090192615986
iteration 92, loss = 0.00404879916459322
iteration 93, loss = 0.004835362080484629
iteration 94, loss = 0.005816187709569931
iteration 95, loss = 0.004618499893695116
iteration 96, loss = 0.003966004587709904
iteration 97, loss = 0.0037882078904658556
iteration 98, loss = 0.004024157300591469
iteration 99, loss = 0.0042715719901025295
iteration 100, loss = 0.003997964318841696
iteration 101, loss = 0.007343619596213102
iteration 102, loss = 0.00550400884822011
iteration 103, loss = 0.004195414483547211
iteration 104, loss = 0.004411596804857254
iteration 105, loss = 0.006886156275868416
iteration 106, loss = 0.004928962327539921
iteration 107, loss = 0.004195977933704853
iteration 108, loss = 0.004044461064040661
iteration 109, loss = 0.004651698283851147
iteration 110, loss = 0.004228224046528339
iteration 111, loss = 0.004050992429256439
iteration 112, loss = 0.004104596562683582
iteration 113, loss = 0.004241569899022579
iteration 114, loss = 0.006230750121176243
iteration 115, loss = 0.005534504074603319
iteration 116, loss = 0.004509912803769112
iteration 117, loss = 0.005340905394405127
iteration 118, loss = 0.004132939036935568
iteration 119, loss = 0.003935919608920813
iteration 120, loss = 0.007231191731989384
iteration 121, loss = 0.005846132058650255
iteration 122, loss = 0.004431986249983311
iteration 123, loss = 0.004410881083458662
iteration 124, loss = 0.004089917987585068
iteration 125, loss = 0.004254262428730726
iteration 126, loss = 0.005973433144390583
iteration 127, loss = 0.004063196014612913
iteration 128, loss = 0.00446128286421299
iteration 129, loss = 0.005418709944933653
iteration 130, loss = 0.003853683825582266
iteration 131, loss = 0.004075116477906704
iteration 132, loss = 0.007156719453632832
iteration 133, loss = 0.004089511465281248
iteration 134, loss = 0.004489060491323471
iteration 135, loss = 0.004204350523650646
iteration 136, loss = 0.004145150072872639
iteration 137, loss = 0.004043031018227339
iteration 138, loss = 0.004355910234153271
iteration 139, loss = 0.004863704554736614
iteration 140, loss = 0.004254176281392574
iteration 141, loss = 0.005036772694438696
iteration 142, loss = 0.004415940493345261
iteration 143, loss = 0.004693773575127125
iteration 144, loss = 0.00473913736641407
iteration 145, loss = 0.004172992426902056
iteration 146, loss = 0.005363114178180695
iteration 147, loss = 0.004046565853059292
iteration 148, loss = 0.004356152843683958
iteration 149, loss = 0.005442048888653517
iteration 150, loss = 0.0044677043333649635
iteration 151, loss = 0.0038061626255512238
iteration 152, loss = 0.003898047609254718
iteration 153, loss = 0.004007220733910799
iteration 154, loss = 0.005086817778646946
iteration 155, loss = 0.004441775847226381
iteration 156, loss = 0.004142968915402889
iteration 157, loss = 0.004020380787551403
iteration 158, loss = 0.006996308919042349
iteration 159, loss = 0.0045374175533652306
iteration 160, loss = 0.007240396924316883
iteration 161, loss = 0.003914491273462772
iteration 162, loss = 0.007341912016272545
iteration 163, loss = 0.003916650079190731
iteration 164, loss = 0.004069906193763018
iteration 165, loss = 0.003762194886803627
iteration 166, loss = 0.005281013436615467
iteration 167, loss = 0.0066501121036708355
iteration 168, loss = 0.0052214572206139565
iteration 169, loss = 0.00502764992415905
iteration 170, loss = 0.004924661014229059
iteration 171, loss = 0.005218271166086197
iteration 172, loss = 0.004477726295590401
iteration 173, loss = 0.004057791084051132
iteration 174, loss = 0.003842460224404931
iteration 175, loss = 0.004306353162974119
iteration 176, loss = 0.004450536798685789
iteration 177, loss = 0.004050672985613346
iteration 178, loss = 0.004218693822622299
iteration 179, loss = 0.004246317315846682
iteration 180, loss = 0.0038387831300497055
iteration 181, loss = 0.00409681024029851
iteration 182, loss = 0.00653825094923377
iteration 183, loss = 0.004194930661469698
iteration 184, loss = 0.004293110221624374
iteration 185, loss = 0.004320651758462191
iteration 186, loss = 0.004085867665708065
iteration 187, loss = 0.004640540108084679
iteration 188, loss = 0.005924603436142206
iteration 189, loss = 0.004101034253835678
iteration 190, loss = 0.0043542515486478806
iteration 191, loss = 0.004312605131417513
iteration 192, loss = 0.004037662874907255
iteration 193, loss = 0.004555952735245228
iteration 194, loss = 0.0038006312679499388
iteration 195, loss = 0.004461762961000204
iteration 196, loss = 0.0068458495661616325
iteration 197, loss = 0.005748266354203224
iteration 198, loss = 0.0047266921028494835
iteration 199, loss = 0.006760061718523502
iteration 200, loss = 0.004410834982991219
iteration 201, loss = 0.0040488517843186855
iteration 202, loss = 0.003943434916436672
iteration 203, loss = 0.006170975975692272
iteration 204, loss = 0.004057121928781271
iteration 205, loss = 0.0035847441758960485
iteration 206, loss = 0.004757249262183905
iteration 207, loss = 0.004960976541042328
iteration 208, loss = 0.004482389893382788
iteration 209, loss = 0.006205311045050621
iteration 210, loss = 0.005127916112542152
iteration 211, loss = 0.004727691877633333
iteration 212, loss = 0.004004966467618942
iteration 213, loss = 0.006064494606107473
iteration 214, loss = 0.004508305341005325
iteration 215, loss = 0.004141814075410366
iteration 216, loss = 0.006664753425866365
iteration 217, loss = 0.004500007256865501
iteration 218, loss = 0.005916954483836889
iteration 219, loss = 0.004167597275227308
iteration 220, loss = 0.003985957242548466
iteration 221, loss = 0.005794811062514782
iteration 222, loss = 0.004138635937124491
iteration 223, loss = 0.004916749894618988
iteration 224, loss = 0.003935250453650951
iteration 225, loss = 0.004134270362555981
iteration 226, loss = 0.005360094830393791
iteration 227, loss = 0.0056657032109797
iteration 228, loss = 0.004416993819177151
iteration 229, loss = 0.0038548805750906467
iteration 230, loss = 0.004194033332169056
iteration 231, loss = 0.006847956217825413
iteration 232, loss = 0.0049262819811701775
iteration 233, loss = 0.004416247364133596
iteration 234, loss = 0.004199474584311247
iteration 235, loss = 0.003675867570564151
iteration 236, loss = 0.004297470208257437
iteration 237, loss = 0.004088451620191336
iteration 238, loss = 0.0036768659483641386
iteration 239, loss = 0.004649304319173098
iteration 240, loss = 0.004030801821500063
iteration 241, loss = 0.004143787082284689
iteration 242, loss = 0.0038712676614522934
iteration 243, loss = 0.003949286881834269
iteration 244, loss = 0.004058714956045151
iteration 245, loss = 0.004304807633161545
iteration 246, loss = 0.003978041000664234
iteration 247, loss = 0.004518982023000717
iteration 248, loss = 0.004185154102742672
iteration 249, loss = 0.004307113122195005
iteration 250, loss = 0.003961218986660242
iteration 251, loss = 0.003999053966253996
iteration 252, loss = 0.0038553974591195583
iteration 253, loss = 0.004013335332274437
iteration 254, loss = 0.0038436120375990868
iteration 255, loss = 0.004095116164535284
iteration 256, loss = 0.005331722088158131
iteration 257, loss = 0.004049054346978664
iteration 258, loss = 0.0038584594149142504
iteration 259, loss = 0.0056701344437897205
iteration 260, loss = 0.006768995895981789
iteration 261, loss = 0.0057511297054588795
iteration 262, loss = 0.00451431330293417
iteration 263, loss = 0.00392510462552309
iteration 264, loss = 0.004338838160037994
iteration 265, loss = 0.007231974974274635
iteration 266, loss = 0.004391718655824661
iteration 267, loss = 0.004188103135675192
iteration 268, loss = 0.005723441019654274
iteration 269, loss = 0.0052834125235676765
iteration 270, loss = 0.004144405480474234
iteration 271, loss = 0.0067079379223287106
iteration 272, loss = 0.0039799269288778305
iteration 273, loss = 0.004129248671233654
iteration 274, loss = 0.004292542580515146
iteration 275, loss = 0.004297623876482248
iteration 276, loss = 0.004231687635183334
iteration 277, loss = 0.0043625179678201675
iteration 278, loss = 0.003906546160578728
iteration 279, loss = 0.003997755236923695
iteration 280, loss = 0.006360631436109543
iteration 281, loss = 0.0070925550535321236
iteration 282, loss = 0.004140859004110098
iteration 283, loss = 0.004366314969956875
iteration 284, loss = 0.005192733835428953
iteration 285, loss = 0.0053680455312132835
iteration 286, loss = 0.0043029035441577435
iteration 287, loss = 0.004205592442303896
iteration 288, loss = 0.004436162766069174
iteration 289, loss = 0.004095711745321751
iteration 290, loss = 0.004714957904070616
iteration 291, loss = 0.003884291974827647
iteration 292, loss = 0.003810158697888255
iteration 293, loss = 0.004980876110494137
iteration 294, loss = 0.006649772636592388
iteration 295, loss = 0.004289807751774788
iteration 296, loss = 0.004012929275631905
iteration 297, loss = 0.005541011691093445
iteration 298, loss = 0.003925587050616741
iteration 299, loss = 0.004109476692974567
iteration 300, loss = 0.00814850814640522
iteration 1, loss = 0.004347928334027529
iteration 2, loss = 0.004071121569722891
iteration 3, loss = 0.005261275917291641
iteration 4, loss = 0.003723881207406521
iteration 5, loss = 0.004563535563647747
iteration 6, loss = 0.004231733735650778
iteration 7, loss = 0.0038307164795696735
iteration 8, loss = 0.004145705606788397
iteration 9, loss = 0.004425715189427137
iteration 10, loss = 0.004025742877274752
iteration 11, loss = 0.006876967381685972
iteration 12, loss = 0.004185407422482967
iteration 13, loss = 0.003929713740944862
iteration 14, loss = 0.004357961006462574
iteration 15, loss = 0.004749606363475323
iteration 16, loss = 0.004074718803167343
iteration 17, loss = 0.004617436323314905
iteration 18, loss = 0.0043230378068983555
iteration 19, loss = 0.004368229769170284
iteration 20, loss = 0.004547901917248964
iteration 21, loss = 0.004238046705722809
iteration 22, loss = 0.004126718733459711
iteration 23, loss = 0.0038741566240787506
iteration 24, loss = 0.007815913297235966
iteration 25, loss = 0.005094163119792938
iteration 26, loss = 0.0058726598508656025
iteration 27, loss = 0.004232447594404221
iteration 28, loss = 0.004312589298933744
iteration 29, loss = 0.003801022656261921
iteration 30, loss = 0.005483335815370083
iteration 31, loss = 0.00452016107738018
iteration 32, loss = 0.003970629535615444
iteration 33, loss = 0.0048049879260361195
iteration 34, loss = 0.004241609014570713
iteration 35, loss = 0.0039442614652216434
iteration 36, loss = 0.003961646463721991
iteration 37, loss = 0.0059560369700193405
iteration 38, loss = 0.006713237147778273
iteration 39, loss = 0.004050093237310648
iteration 40, loss = 0.004444739781320095
iteration 41, loss = 0.0043531679548323154
iteration 42, loss = 0.004839013796299696
iteration 43, loss = 0.004208327271044254
iteration 44, loss = 0.004740733653306961
iteration 45, loss = 0.004135850351303816
iteration 46, loss = 0.004560571629554033
iteration 47, loss = 0.006158157251775265
iteration 48, loss = 0.005959664471447468
iteration 49, loss = 0.006504736840724945
iteration 50, loss = 0.003862582379952073
iteration 51, loss = 0.005547238513827324
iteration 52, loss = 0.004434152040630579
iteration 53, loss = 0.003965307492762804
iteration 54, loss = 0.004417640157043934
iteration 55, loss = 0.006840109825134277
iteration 56, loss = 0.004329887684434652
iteration 57, loss = 0.00422214949503541
iteration 58, loss = 0.004002248868346214
iteration 59, loss = 0.004315133672207594
iteration 60, loss = 0.0041508665308356285
iteration 61, loss = 0.004301462788134813
iteration 62, loss = 0.004271512385457754
iteration 63, loss = 0.004079471342265606
iteration 64, loss = 0.003961339592933655
iteration 65, loss = 0.004764439072459936
iteration 66, loss = 0.005303392186760902
iteration 67, loss = 0.003881854470819235
iteration 68, loss = 0.005526302382349968
iteration 69, loss = 0.004232557490468025
iteration 70, loss = 0.00426004733890295
iteration 71, loss = 0.003919837065041065
iteration 72, loss = 0.0038028229027986526
iteration 73, loss = 0.007064707111567259
iteration 74, loss = 0.0061053382232785225
iteration 75, loss = 0.004003188107162714
iteration 76, loss = 0.003523532534018159
iteration 77, loss = 0.005135032348334789
iteration 78, loss = 0.003914843779057264
iteration 79, loss = 0.0039670951664447784
iteration 80, loss = 0.0040899100713431835
iteration 81, loss = 0.0039583067409694195
iteration 82, loss = 0.00445136334747076
iteration 83, loss = 0.003959805238991976
iteration 84, loss = 0.005871214903891087
iteration 85, loss = 0.004075877834111452
iteration 86, loss = 0.00711078429594636
iteration 87, loss = 0.004088142421096563
iteration 88, loss = 0.005695539992302656
iteration 89, loss = 0.005104215815663338
iteration 90, loss = 0.0071791852824389935
iteration 91, loss = 0.00466409046202898
iteration 92, loss = 0.00679368432611227
iteration 93, loss = 0.0052549708634614944
iteration 94, loss = 0.004244470968842506
iteration 95, loss = 0.006257862783968449
iteration 96, loss = 0.004296302795410156
iteration 97, loss = 0.003842657897621393
iteration 98, loss = 0.005234666168689728
iteration 99, loss = 0.004184197634458542
iteration 100, loss = 0.004294237121939659
iteration 101, loss = 0.004247839096933603
iteration 102, loss = 0.003969228360801935
iteration 103, loss = 0.004274451639503241
iteration 104, loss = 0.004339112900197506
iteration 105, loss = 0.005582289770245552
iteration 106, loss = 0.0045199645683169365
iteration 107, loss = 0.004190852865576744
iteration 108, loss = 0.0051033529452979565
iteration 109, loss = 0.00393340177834034
iteration 110, loss = 0.004487161990255117
iteration 111, loss = 0.003992214798927307
iteration 112, loss = 0.003979770932346582
iteration 113, loss = 0.0043844361789524555
iteration 114, loss = 0.0037071024999022484
iteration 115, loss = 0.004595381207764149
iteration 116, loss = 0.0042765140533447266
iteration 117, loss = 0.0039516412653028965
iteration 118, loss = 0.008190512657165527
iteration 119, loss = 0.004251969512552023
iteration 120, loss = 0.005512146279215813
iteration 121, loss = 0.004278045147657394
iteration 122, loss = 0.005153776612132788
iteration 123, loss = 0.004061778075993061
iteration 124, loss = 0.004117824602872133
iteration 125, loss = 0.006995360367000103
iteration 126, loss = 0.004671871662139893
iteration 127, loss = 0.004548673518002033
iteration 128, loss = 0.004196951165795326
iteration 129, loss = 0.003751258598640561
iteration 130, loss = 0.0075496467761695385
iteration 131, loss = 0.004351162351667881
iteration 132, loss = 0.004340732004493475
iteration 133, loss = 0.004585516173392534
iteration 134, loss = 0.003819999285042286
iteration 135, loss = 0.004205233883112669
iteration 136, loss = 0.005101123824715614
iteration 137, loss = 0.004505560267716646
iteration 138, loss = 0.0038920603692531586
iteration 139, loss = 0.005146922077983618
iteration 140, loss = 0.005356936249881983
iteration 141, loss = 0.0039085703901946545
iteration 142, loss = 0.004023293498903513
iteration 143, loss = 0.005992482416331768
iteration 144, loss = 0.0040642074309289455
iteration 145, loss = 0.005379148293286562
iteration 146, loss = 0.004454037174582481
iteration 147, loss = 0.007626029197126627
iteration 148, loss = 0.004353401716798544
iteration 149, loss = 0.004301371984183788
iteration 150, loss = 0.0056880139745771885
iteration 151, loss = 0.004019824787974358
iteration 152, loss = 0.00423019053414464
iteration 153, loss = 0.005496131256222725
iteration 154, loss = 0.004275713115930557
iteration 155, loss = 0.006176032591611147
iteration 156, loss = 0.005756931845098734
iteration 157, loss = 0.003912604879587889
iteration 158, loss = 0.0041252621449530125
iteration 159, loss = 0.004232717677950859
iteration 160, loss = 0.00462383683770895
iteration 161, loss = 0.004295666236430407
iteration 162, loss = 0.0041428725235164165
iteration 163, loss = 0.0044105942361056805
iteration 164, loss = 0.004092590883374214
iteration 165, loss = 0.0038682883605360985
iteration 166, loss = 0.0042005800642073154
iteration 167, loss = 0.005372644867748022
iteration 168, loss = 0.004205239936709404
iteration 169, loss = 0.003899709554389119
iteration 170, loss = 0.005187941249459982
iteration 171, loss = 0.004960423335433006
iteration 172, loss = 0.004273784346878529
iteration 173, loss = 0.004074752796441317
iteration 174, loss = 0.004610640462487936
iteration 175, loss = 0.004460487049072981
iteration 176, loss = 0.00441202474758029
iteration 177, loss = 0.004093819297850132
iteration 178, loss = 0.003889036364853382
iteration 179, loss = 0.0041696722619235516
iteration 180, loss = 0.0045696222223341465
iteration 181, loss = 0.006303691305220127
iteration 182, loss = 0.004141626413911581
iteration 183, loss = 0.0038633293006569147
iteration 184, loss = 0.004115288145840168
iteration 185, loss = 0.005414812825620174
iteration 186, loss = 0.004674215335398912
iteration 187, loss = 0.0040564099326729774
iteration 188, loss = 0.004429902881383896
iteration 189, loss = 0.004205750301480293
iteration 190, loss = 0.004094226285815239
iteration 191, loss = 0.004650948569178581
iteration 192, loss = 0.004282491281628609
iteration 193, loss = 0.004250044003129005
iteration 194, loss = 0.00426924554631114
iteration 195, loss = 0.006761148106306791
iteration 196, loss = 0.004075133241713047
iteration 197, loss = 0.0045210449025034904
iteration 198, loss = 0.003999638371169567
iteration 199, loss = 0.004232955630868673
iteration 200, loss = 0.004059824161231518
iteration 201, loss = 0.003897694405168295
iteration 202, loss = 0.007586353458464146
iteration 203, loss = 0.004041033331304789
iteration 204, loss = 0.004139856901019812
iteration 205, loss = 0.003929999191313982
iteration 206, loss = 0.004340678453445435
iteration 207, loss = 0.005737188272178173
iteration 208, loss = 0.004428522195667028
iteration 209, loss = 0.005495808552950621
iteration 210, loss = 0.004050427116453648
iteration 211, loss = 0.003971552476286888
iteration 212, loss = 0.004420631565153599
iteration 213, loss = 0.003755544777959585
iteration 214, loss = 0.0040892912074923515
iteration 215, loss = 0.0036810492165386677
iteration 216, loss = 0.004268436226993799
iteration 217, loss = 0.004176413640379906
iteration 218, loss = 0.005591840483248234
iteration 219, loss = 0.00373478839173913
iteration 220, loss = 0.004038170445710421
iteration 221, loss = 0.004338757134974003
iteration 222, loss = 0.004062610678374767
iteration 223, loss = 0.004060715436935425
iteration 224, loss = 0.003961274400353432
iteration 225, loss = 0.0038278717547655106
iteration 226, loss = 0.004427448846399784
iteration 227, loss = 0.0054050772450864315
iteration 228, loss = 0.004055333323776722
iteration 229, loss = 0.004705870524048805
iteration 230, loss = 0.0047298516146838665
iteration 231, loss = 0.004096915479749441
iteration 232, loss = 0.004454232286661863
iteration 233, loss = 0.003968165256083012
iteration 234, loss = 0.004212833940982819
iteration 235, loss = 0.004158017225563526
iteration 236, loss = 0.004918928723782301
iteration 237, loss = 0.0047139511443674564
iteration 238, loss = 0.00396721251308918
iteration 239, loss = 0.003862369805574417
iteration 240, loss = 0.005546608939766884
iteration 241, loss = 0.0041220905259251595
iteration 242, loss = 0.004679964389652014
iteration 243, loss = 0.004189310595393181
iteration 244, loss = 0.004071305971592665
iteration 245, loss = 0.004974058363586664
iteration 246, loss = 0.004556745290756226
iteration 247, loss = 0.004124150145798922
iteration 248, loss = 0.0044723013415932655
iteration 249, loss = 0.003996184561401606
iteration 250, loss = 0.004686116240918636
iteration 251, loss = 0.005498967599123716
iteration 252, loss = 0.006376712117344141
iteration 253, loss = 0.0061133792623877525
iteration 254, loss = 0.003922943025827408
iteration 255, loss = 0.005389681551605463
iteration 256, loss = 0.004692027810961008
iteration 257, loss = 0.0040266141295433044
iteration 258, loss = 0.003983067348599434
iteration 259, loss = 0.004125283565372229
iteration 260, loss = 0.004436822142452002
iteration 261, loss = 0.004037506878376007
iteration 262, loss = 0.004543752875179052
iteration 263, loss = 0.004266464151442051
iteration 264, loss = 0.00520427618175745
iteration 265, loss = 0.004603539127856493
iteration 266, loss = 0.003976617008447647
iteration 267, loss = 0.004574355203658342
iteration 268, loss = 0.00403418717905879
iteration 269, loss = 0.004112104419618845
iteration 270, loss = 0.006659355014562607
iteration 271, loss = 0.004184032790362835
iteration 272, loss = 0.0050509050488471985
iteration 273, loss = 0.006003754213452339
iteration 274, loss = 0.003880613250657916
iteration 275, loss = 0.00434814253821969
iteration 276, loss = 0.0042391084134578705
iteration 277, loss = 0.005819827783852816
iteration 278, loss = 0.005821741186082363
iteration 279, loss = 0.004168338607996702
iteration 280, loss = 0.004731751047074795
iteration 281, loss = 0.0052766078151762486
iteration 282, loss = 0.006702243350446224
iteration 283, loss = 0.004282329231500626
iteration 284, loss = 0.0056419456377625465
iteration 285, loss = 0.004345269873738289
iteration 286, loss = 0.00432075560092926
iteration 287, loss = 0.004290012177079916
iteration 288, loss = 0.0038720625452697277
iteration 289, loss = 0.006844054441899061
iteration 290, loss = 0.004179687704890966
iteration 291, loss = 0.005329202860593796
iteration 292, loss = 0.004110647831112146
iteration 293, loss = 0.004233553074300289
iteration 294, loss = 0.0048272511921823025
iteration 295, loss = 0.004743415396660566
iteration 296, loss = 0.006756856571882963
iteration 297, loss = 0.0044836499728262424
iteration 298, loss = 0.004520237445831299
iteration 299, loss = 0.004668609704822302
iteration 300, loss = 0.004023426212370396
iteration 1, loss = 0.005488444119691849
iteration 2, loss = 0.004151937086135149
iteration 3, loss = 0.004106381442397833
iteration 4, loss = 0.004120418801903725
iteration 5, loss = 0.004170728847384453
iteration 6, loss = 0.0038751952815800905
iteration 7, loss = 0.004269028082489967
iteration 8, loss = 0.0038124271668493748
iteration 9, loss = 0.004356279503554106
iteration 10, loss = 0.004063641652464867
iteration 11, loss = 0.005270150490105152
iteration 12, loss = 0.004657014273107052
iteration 13, loss = 0.003955872729420662
iteration 14, loss = 0.004877755418419838
iteration 15, loss = 0.004206440411508083
iteration 16, loss = 0.005887731444090605
iteration 17, loss = 0.005738368723541498
iteration 18, loss = 0.005578943528234959
iteration 19, loss = 0.004261205904185772
iteration 20, loss = 0.004478687420487404
iteration 21, loss = 0.004369786940515041
iteration 22, loss = 0.005360779352486134
iteration 23, loss = 0.003936056979000568
iteration 24, loss = 0.004112077411264181
iteration 25, loss = 0.0052586705423891544
iteration 26, loss = 0.004051574040204287
iteration 27, loss = 0.003927532583475113
iteration 28, loss = 0.003778808983042836
iteration 29, loss = 0.00430411659181118
iteration 30, loss = 0.00396037707105279
iteration 31, loss = 0.005156598519533873
iteration 32, loss = 0.0036962772719562054
iteration 33, loss = 0.004177751950919628
iteration 34, loss = 0.005792467389255762
iteration 35, loss = 0.004112298134714365
iteration 36, loss = 0.004445185884833336
iteration 37, loss = 0.0039228470996022224
iteration 38, loss = 0.004385997541248798
iteration 39, loss = 0.003877701936289668
iteration 40, loss = 0.0038790463004261255
iteration 41, loss = 0.004126437474042177
iteration 42, loss = 0.005276500713080168
iteration 43, loss = 0.004461678210645914
iteration 44, loss = 0.005325950216501951
iteration 45, loss = 0.0038371686823666096
iteration 46, loss = 0.004081668332219124
iteration 47, loss = 0.004030982963740826
iteration 48, loss = 0.004427873995155096
iteration 49, loss = 0.004127267748117447
iteration 50, loss = 0.003977442160248756
iteration 51, loss = 0.004303078167140484
iteration 52, loss = 0.004043141845613718
iteration 53, loss = 0.004170056898146868
iteration 54, loss = 0.0038702853489667177
iteration 55, loss = 0.004513283260166645
iteration 56, loss = 0.0037315059453248978
iteration 57, loss = 0.007374848239123821
iteration 58, loss = 0.003991744946688414
iteration 59, loss = 0.003955211490392685
iteration 60, loss = 0.00408340385183692
iteration 61, loss = 0.003898633411154151
iteration 62, loss = 0.00546157406643033
iteration 63, loss = 0.004849100485444069
iteration 64, loss = 0.006309258285909891
iteration 65, loss = 0.004330470226705074
iteration 66, loss = 0.004213242791593075
iteration 67, loss = 0.004039437510073185
iteration 68, loss = 0.004260755144059658
iteration 69, loss = 0.007422163616865873
iteration 70, loss = 0.0037430699449032545
iteration 71, loss = 0.004396115429699421
iteration 72, loss = 0.006258501671254635
iteration 73, loss = 0.003991881385445595
iteration 74, loss = 0.004285681061446667
iteration 75, loss = 0.0040444196201860905
iteration 76, loss = 0.0068970127031207085
iteration 77, loss = 0.004137943033128977
iteration 78, loss = 0.00536370649933815
iteration 79, loss = 0.004049542360007763
iteration 80, loss = 0.004157529678195715
iteration 81, loss = 0.00457413587719202
iteration 82, loss = 0.004702514968812466
iteration 83, loss = 0.00564604951068759
iteration 84, loss = 0.005337276495993137
iteration 85, loss = 0.004637084435671568
iteration 86, loss = 0.003967519383877516
iteration 87, loss = 0.003926172852516174
iteration 88, loss = 0.0041506229899823666
iteration 89, loss = 0.0037964126095175743
iteration 90, loss = 0.004819795489311218
iteration 91, loss = 0.003959942609071732
iteration 92, loss = 0.005145250353962183
iteration 93, loss = 0.004716888070106506
iteration 94, loss = 0.006428700406104326
iteration 95, loss = 0.003817430231720209
iteration 96, loss = 0.004058394581079483
iteration 97, loss = 0.006846890319138765
iteration 98, loss = 0.004595674574375153
iteration 99, loss = 0.0038722138851881027
iteration 100, loss = 0.0038468684069812298
iteration 101, loss = 0.00402499083429575
iteration 102, loss = 0.003990708384662867
iteration 103, loss = 0.005215429700911045
iteration 104, loss = 0.00439713429659605
iteration 105, loss = 0.004285186063498259
iteration 106, loss = 0.0042892200872302055
iteration 107, loss = 0.0037881750613451004
iteration 108, loss = 0.003950124140828848
iteration 109, loss = 0.004194580018520355
iteration 110, loss = 0.005126886535435915
iteration 111, loss = 0.0039699194021523
iteration 112, loss = 0.004604943562299013
iteration 113, loss = 0.004070866387337446
iteration 114, loss = 0.003929044120013714
iteration 115, loss = 0.003967039752751589
iteration 116, loss = 0.004581475630402565
iteration 117, loss = 0.006147494073957205
iteration 118, loss = 0.0037987027317285538
iteration 119, loss = 0.004177876748144627
iteration 120, loss = 0.004298114217817783
iteration 121, loss = 0.005095144733786583
iteration 122, loss = 0.0052748676389455795
iteration 123, loss = 0.004044525325298309
iteration 124, loss = 0.007111058570444584
iteration 125, loss = 0.006691226735711098
iteration 126, loss = 0.003885531798005104
iteration 127, loss = 0.003931974060833454
iteration 128, loss = 0.004308538045734167
iteration 129, loss = 0.004098888486623764
iteration 130, loss = 0.004319976083934307
iteration 131, loss = 0.004349778406322002
iteration 132, loss = 0.0038892633747309446
iteration 133, loss = 0.004049694165587425
iteration 134, loss = 0.004061224404722452
iteration 135, loss = 0.004363338463008404
iteration 136, loss = 0.0039397780783474445
iteration 137, loss = 0.006026060786098242
iteration 138, loss = 0.00435561453923583
iteration 139, loss = 0.003972858656197786
iteration 140, loss = 0.004154525697231293
iteration 141, loss = 0.004018863663077354
iteration 142, loss = 0.0045193214900791645
iteration 143, loss = 0.0040805344469845295
iteration 144, loss = 0.007042840123176575
iteration 145, loss = 0.004422498866915703
iteration 146, loss = 0.0050470419228076935
iteration 147, loss = 0.007405068725347519
iteration 148, loss = 0.004231653176248074
iteration 149, loss = 0.005245364271104336
iteration 150, loss = 0.004240398295223713
iteration 151, loss = 0.006669871509075165
iteration 152, loss = 0.00454208767041564
iteration 153, loss = 0.0037829256616532803
iteration 154, loss = 0.004220433998852968
iteration 155, loss = 0.004592908546328545
iteration 156, loss = 0.0036585719790309668
iteration 157, loss = 0.004099012818187475
iteration 158, loss = 0.003773470874875784
iteration 159, loss = 0.005413085222244263
iteration 160, loss = 0.004278161562979221
iteration 161, loss = 0.007006294094026089
iteration 162, loss = 0.004030073527246714
iteration 163, loss = 0.004943637643009424
iteration 164, loss = 0.004217174369841814
iteration 165, loss = 0.0053343698382377625
iteration 166, loss = 0.004071459639817476
iteration 167, loss = 0.003967542666941881
iteration 168, loss = 0.004256417974829674
iteration 169, loss = 0.006062820553779602
iteration 170, loss = 0.006907510571181774
iteration 171, loss = 0.007474150042980909
iteration 172, loss = 0.004203869495540857
iteration 173, loss = 0.004231717437505722
iteration 174, loss = 0.004393385257571936
iteration 175, loss = 0.004968822468072176
iteration 176, loss = 0.004132256843149662
iteration 177, loss = 0.006939437240362167
iteration 178, loss = 0.00356981810182333
iteration 179, loss = 0.004089983645826578
iteration 180, loss = 0.005990267731249332
iteration 181, loss = 0.004064855631440878
iteration 182, loss = 0.0038591017946600914
iteration 183, loss = 0.0046211061999201775
iteration 184, loss = 0.004116441588848829
iteration 185, loss = 0.005207442212849855
iteration 186, loss = 0.004453754518181086
iteration 187, loss = 0.004101506434381008
iteration 188, loss = 0.004308673553168774
iteration 189, loss = 0.0040530916303396225
iteration 190, loss = 0.004297798033803701
iteration 191, loss = 0.004510140512138605
iteration 192, loss = 0.0037192851305007935
iteration 193, loss = 0.004075642675161362
iteration 194, loss = 0.003992453217506409
iteration 195, loss = 0.004171508364379406
iteration 196, loss = 0.00425640819594264
iteration 197, loss = 0.0059159970842301846
iteration 198, loss = 0.0036483483854681253
iteration 199, loss = 0.00466186786070466
iteration 200, loss = 0.0046030557714402676
iteration 201, loss = 0.004916086792945862
iteration 202, loss = 0.0039787814021110535
iteration 203, loss = 0.00537125114351511
iteration 204, loss = 0.006157440133392811
iteration 205, loss = 0.004463658202439547
iteration 206, loss = 0.004615129437297583
iteration 207, loss = 0.004063660744577646
iteration 208, loss = 0.005859928671270609
iteration 209, loss = 0.003945007920265198
iteration 210, loss = 0.004502601455897093
iteration 211, loss = 0.004084156826138496
iteration 212, loss = 0.004337274469435215
iteration 213, loss = 0.006139593664556742
iteration 214, loss = 0.004830174148082733
iteration 215, loss = 0.004461625125259161
iteration 216, loss = 0.004584324546158314
iteration 217, loss = 0.004901765380054712
iteration 218, loss = 0.006047460250556469
iteration 219, loss = 0.004337662365287542
iteration 220, loss = 0.0066059366799890995
iteration 221, loss = 0.004340981133282185
iteration 222, loss = 0.00437450036406517
iteration 223, loss = 0.003931606654077768
iteration 224, loss = 0.004111831076443195
iteration 225, loss = 0.0040908134542405605
iteration 226, loss = 0.004400661215186119
iteration 227, loss = 0.004186058416962624
iteration 228, loss = 0.005113513208925724
iteration 229, loss = 0.005255726166069508
iteration 230, loss = 0.005435571540147066
iteration 231, loss = 0.0042253038845956326
iteration 232, loss = 0.0038616261444985867
iteration 233, loss = 0.004289531148970127
iteration 234, loss = 0.004152616485953331
iteration 235, loss = 0.007112834136933088
iteration 236, loss = 0.0053049055859446526
iteration 237, loss = 0.005543589126318693
iteration 238, loss = 0.00492534413933754
iteration 239, loss = 0.004706390202045441
iteration 240, loss = 0.004036459140479565
iteration 241, loss = 0.004121900536119938
iteration 242, loss = 0.003981136716902256
iteration 243, loss = 0.0038723601028323174
iteration 244, loss = 0.004137618467211723
iteration 245, loss = 0.004300842527300119
iteration 246, loss = 0.0057218968868255615
iteration 247, loss = 0.004117316100746393
iteration 248, loss = 0.003938383422791958
iteration 249, loss = 0.005479149986058474
iteration 250, loss = 0.004471867810934782
iteration 251, loss = 0.003942257259041071
iteration 252, loss = 0.004648376256227493
iteration 253, loss = 0.004687865264713764
iteration 254, loss = 0.004214119631797075
iteration 255, loss = 0.004151254426687956
iteration 256, loss = 0.0037550581619143486
iteration 257, loss = 0.004860364366322756
iteration 258, loss = 0.005077982321381569
iteration 259, loss = 0.0038518798537552357
iteration 260, loss = 0.004352404270321131
iteration 261, loss = 0.003777134697884321
iteration 262, loss = 0.004190960433334112
iteration 263, loss = 0.004312579985707998
iteration 264, loss = 0.004700816702097654
iteration 265, loss = 0.0070835803635418415
iteration 266, loss = 0.005858254618942738
iteration 267, loss = 0.007562941871583462
iteration 268, loss = 0.004198869690299034
iteration 269, loss = 0.00449344702064991
iteration 270, loss = 0.0043385084718465805
iteration 271, loss = 0.004330889321863651
iteration 272, loss = 0.004265254829078913
iteration 273, loss = 0.005851321388036013
iteration 274, loss = 0.004646423738449812
iteration 275, loss = 0.006604522466659546
iteration 276, loss = 0.004411233123391867
iteration 277, loss = 0.005001012235879898
iteration 278, loss = 0.0038600964471697807
iteration 279, loss = 0.006668408866971731
iteration 280, loss = 0.004105305764824152
iteration 281, loss = 0.0050736707635223866
iteration 282, loss = 0.004163258243352175
iteration 283, loss = 0.0041719162836670876
iteration 284, loss = 0.003912429325282574
iteration 285, loss = 0.004784224089235067
iteration 286, loss = 0.004165051504969597
iteration 287, loss = 0.004523802548646927
iteration 288, loss = 0.004628229420632124
iteration 289, loss = 0.005094426684081554
iteration 290, loss = 0.003922155126929283
iteration 291, loss = 0.005729694850742817
iteration 292, loss = 0.004373017232865095
iteration 293, loss = 0.006125107407569885
iteration 294, loss = 0.0056307935155928135
iteration 295, loss = 0.005140624474734068
iteration 296, loss = 0.004212020896375179
iteration 297, loss = 0.005290574859827757
iteration 298, loss = 0.004261444788426161
iteration 299, loss = 0.004334905184805393
iteration 300, loss = 0.004479800350964069
iteration 1, loss = 0.004407196771353483
iteration 2, loss = 0.007076905574649572
iteration 3, loss = 0.004360162653028965
iteration 4, loss = 0.005097550805658102
iteration 5, loss = 0.004365251865237951
iteration 6, loss = 0.007173154968768358
iteration 7, loss = 0.00415680930018425
iteration 8, loss = 0.004343448206782341
iteration 9, loss = 0.004183327779173851
iteration 10, loss = 0.004648804664611816
iteration 11, loss = 0.004411730915307999
iteration 12, loss = 0.005691948346793652
iteration 13, loss = 0.0040067886002361774
iteration 14, loss = 0.004041603300720453
iteration 15, loss = 0.0039385161362588406
iteration 16, loss = 0.003965735901147127
iteration 17, loss = 0.004533581435680389
iteration 18, loss = 0.004159917589277029
iteration 19, loss = 0.00479959836229682
iteration 20, loss = 0.004305470269173384
iteration 21, loss = 0.00448854174464941
iteration 22, loss = 0.00433128047734499
iteration 23, loss = 0.004268655553460121
iteration 24, loss = 0.004275011830031872
iteration 25, loss = 0.0059989215806126595
iteration 26, loss = 0.003960166592150927
iteration 27, loss = 0.004394177347421646
iteration 28, loss = 0.0036288772244006395
iteration 29, loss = 0.004601281136274338
iteration 30, loss = 0.004244011361151934
iteration 31, loss = 0.004171647597104311
iteration 32, loss = 0.004489313345402479
iteration 33, loss = 0.005778694991022348
iteration 34, loss = 0.005617386661469936
iteration 35, loss = 0.005630153231322765
iteration 36, loss = 0.004286735784262419
iteration 37, loss = 0.006942621432244778
iteration 38, loss = 0.0038970550522208214
iteration 39, loss = 0.00438646599650383
iteration 40, loss = 0.004378495272248983
iteration 41, loss = 0.0044135842472314835
iteration 42, loss = 0.004202354699373245
iteration 43, loss = 0.004084132611751556
iteration 44, loss = 0.0044756801798939705
iteration 45, loss = 0.004502869676798582
iteration 46, loss = 0.0044496385380625725
iteration 47, loss = 0.004347567912191153
iteration 48, loss = 0.003957184497267008
iteration 49, loss = 0.003914492204785347
iteration 50, loss = 0.005244315601885319
iteration 51, loss = 0.004477528855204582
iteration 52, loss = 0.004566073417663574
iteration 53, loss = 0.007364580873399973
iteration 54, loss = 0.004032264929264784
iteration 55, loss = 0.0039468789473176
iteration 56, loss = 0.004017770290374756
iteration 57, loss = 0.004180246498435736
iteration 58, loss = 0.004141963552683592
iteration 59, loss = 0.004039209336042404
iteration 60, loss = 0.004071300383657217
iteration 61, loss = 0.003914829809218645
iteration 62, loss = 0.0042968252673745155
iteration 63, loss = 0.004538576118648052
iteration 64, loss = 0.005593808367848396
iteration 65, loss = 0.005074439104646444
iteration 66, loss = 0.003700477071106434
iteration 67, loss = 0.005588771775364876
iteration 68, loss = 0.004570682533085346
iteration 69, loss = 0.004323137458413839
iteration 70, loss = 0.005473710130900145
iteration 71, loss = 0.003985556773841381
iteration 72, loss = 0.004814688581973314
iteration 73, loss = 0.005987209267914295
iteration 74, loss = 0.005250322166830301
iteration 75, loss = 0.005353569518774748
iteration 76, loss = 0.004557042848318815
iteration 77, loss = 0.005854065530002117
iteration 78, loss = 0.005877623334527016
iteration 79, loss = 0.004145920276641846
iteration 80, loss = 0.0038704033941030502
iteration 81, loss = 0.005117332097142935
iteration 82, loss = 0.0038488649297505617
iteration 83, loss = 0.004543956834822893
iteration 84, loss = 0.004062936641275883
iteration 85, loss = 0.007150574121624231
iteration 86, loss = 0.005473157856613398
iteration 87, loss = 0.004133995156735182
iteration 88, loss = 0.00648458581417799
iteration 89, loss = 0.004723687656223774
iteration 90, loss = 0.003906263969838619
iteration 91, loss = 0.003908213693648577
iteration 92, loss = 0.0045400699600577354
iteration 93, loss = 0.003771572606638074
iteration 94, loss = 0.005190636962652206
iteration 95, loss = 0.0038659237325191498
iteration 96, loss = 0.004199388902634382
iteration 97, loss = 0.0038929153233766556
iteration 98, loss = 0.0041179414838552475
iteration 99, loss = 0.0038491138257086277
iteration 100, loss = 0.004432805348187685
iteration 101, loss = 0.0044054607860744
iteration 102, loss = 0.003511852351948619
iteration 103, loss = 0.005676738917827606
iteration 104, loss = 0.004355671815574169
iteration 105, loss = 0.0040617939084768295
iteration 106, loss = 0.003949244040995836
iteration 107, loss = 0.004482642747461796
iteration 108, loss = 0.005488326773047447
iteration 109, loss = 0.004167234059423208
iteration 110, loss = 0.005976709071546793
iteration 111, loss = 0.00601200619712472
iteration 112, loss = 0.005730112548917532
iteration 113, loss = 0.004175739828497171
iteration 114, loss = 0.005429691169410944
iteration 115, loss = 0.004141898825764656
iteration 116, loss = 0.004599204286932945
iteration 117, loss = 0.005253207869827747
iteration 118, loss = 0.004464075900614262
iteration 119, loss = 0.005520581267774105
iteration 120, loss = 0.004109968896955252
iteration 121, loss = 0.004283091984689236
iteration 122, loss = 0.004328517708927393
iteration 123, loss = 0.004035331308841705
iteration 124, loss = 0.007021977100521326
iteration 125, loss = 0.003973246552050114
iteration 126, loss = 0.006848962977528572
iteration 127, loss = 0.004025742877274752
iteration 128, loss = 0.005347537342458963
iteration 129, loss = 0.0045552924275398254
iteration 130, loss = 0.003907844424247742
iteration 131, loss = 0.004058241378515959
iteration 132, loss = 0.004445779602974653
iteration 133, loss = 0.003953587729483843
iteration 134, loss = 0.004473221488296986
iteration 135, loss = 0.0043866741470992565
iteration 136, loss = 0.006915061268955469
iteration 137, loss = 0.005441547371447086
iteration 138, loss = 0.0059468550607562065
iteration 139, loss = 0.004328261595219374
iteration 140, loss = 0.004593991208821535
iteration 141, loss = 0.003916766960173845
iteration 142, loss = 0.004099679179489613
iteration 143, loss = 0.0042220354080200195
iteration 144, loss = 0.004413966089487076
iteration 145, loss = 0.004402140621095896
iteration 146, loss = 0.004322587046772242
iteration 147, loss = 0.005216620862483978
iteration 148, loss = 0.003862215206027031
iteration 149, loss = 0.0044410391710698605
iteration 150, loss = 0.0063293506391346455
iteration 151, loss = 0.004098121542483568
iteration 152, loss = 0.004216175992041826
iteration 153, loss = 0.004441314842551947
iteration 154, loss = 0.005201716907322407
iteration 155, loss = 0.004236523527652025
iteration 156, loss = 0.0038848407566547394
iteration 157, loss = 0.00406569754704833
iteration 158, loss = 0.004830116406083107
iteration 159, loss = 0.004542341455817223
iteration 160, loss = 0.004018714185804129
iteration 161, loss = 0.004008955787867308
iteration 162, loss = 0.004043756518512964
iteration 163, loss = 0.006086154840886593
iteration 164, loss = 0.0052771479822695255
iteration 165, loss = 0.0038320361636579037
iteration 166, loss = 0.003951890394091606
iteration 167, loss = 0.005306054372340441
iteration 168, loss = 0.005798019468784332
iteration 169, loss = 0.005153922364115715
iteration 170, loss = 0.003949059173464775
iteration 171, loss = 0.004194917157292366
iteration 172, loss = 0.0038060820661485195
iteration 173, loss = 0.0043205032125115395
iteration 174, loss = 0.004925009328871965
iteration 175, loss = 0.004247920121997595
iteration 176, loss = 0.0042373319156467915
iteration 177, loss = 0.003873024135828018
iteration 178, loss = 0.003868930274620652
iteration 179, loss = 0.003845320548862219
iteration 180, loss = 0.005233071744441986
iteration 181, loss = 0.003949188627302647
iteration 182, loss = 0.003664752235636115
iteration 183, loss = 0.0036803463008254766
iteration 184, loss = 0.004058342892676592
iteration 185, loss = 0.004293110687285662
iteration 186, loss = 0.004581080749630928
iteration 187, loss = 0.004105855245143175
iteration 188, loss = 0.0039495001547038555
iteration 189, loss = 0.003944917116314173
iteration 190, loss = 0.004088325425982475
iteration 191, loss = 0.00566035695374012
iteration 192, loss = 0.004007237032055855
iteration 193, loss = 0.004420287907123566
iteration 194, loss = 0.004383353516459465
iteration 195, loss = 0.004403685685247183
iteration 196, loss = 0.003979573491960764
iteration 197, loss = 0.005606722552329302
iteration 198, loss = 0.0038228898774832487
iteration 199, loss = 0.005575689021497965
iteration 200, loss = 0.004153230227530003
iteration 201, loss = 0.0037130932323634624
iteration 202, loss = 0.003988496959209442
iteration 203, loss = 0.004212689585983753
iteration 204, loss = 0.0039240531623363495
iteration 205, loss = 0.003904501674696803
iteration 206, loss = 0.005956990644335747
iteration 207, loss = 0.004094912204891443
iteration 208, loss = 0.006971647962927818
iteration 209, loss = 0.006846180651336908
iteration 210, loss = 0.0048200823366642
iteration 211, loss = 0.0040383837185800076
iteration 212, loss = 0.005143860355019569
iteration 213, loss = 0.004088711924850941
iteration 214, loss = 0.0038126353174448013
iteration 215, loss = 0.005630293861031532
iteration 216, loss = 0.004488781560212374
iteration 217, loss = 0.004093749448657036
iteration 218, loss = 0.004619471728801727
iteration 219, loss = 0.007183993700891733
iteration 220, loss = 0.003940821625292301
iteration 221, loss = 0.004007576499134302
iteration 222, loss = 0.004379689693450928
iteration 223, loss = 0.004538204520940781
iteration 224, loss = 0.006804642267525196
iteration 225, loss = 0.004973247647285461
iteration 226, loss = 0.0041670361533761024
iteration 227, loss = 0.004303814377635717
iteration 228, loss = 0.004227758385241032
iteration 229, loss = 0.004302185028791428
iteration 230, loss = 0.004793198313564062
iteration 231, loss = 0.006743674632161856
iteration 232, loss = 0.003955071326345205
iteration 233, loss = 0.004172271117568016
iteration 234, loss = 0.0048578111454844475
iteration 235, loss = 0.0040060607716441154
iteration 236, loss = 0.004029551986604929
iteration 237, loss = 0.005988250020891428
iteration 238, loss = 0.005477425176650286
iteration 239, loss = 0.004603467881679535
iteration 240, loss = 0.0039773061871528625
iteration 241, loss = 0.004098670091480017
iteration 242, loss = 0.004287823103368282
iteration 243, loss = 0.004180691204965115
iteration 244, loss = 0.004458215087652206
iteration 245, loss = 0.003970860969275236
iteration 246, loss = 0.0042450749315321445
iteration 247, loss = 0.004424442071467638
iteration 248, loss = 0.004714007023721933
iteration 249, loss = 0.003843806916847825
iteration 250, loss = 0.004483144264668226
iteration 251, loss = 0.003941558301448822
iteration 252, loss = 0.004101967439055443
iteration 253, loss = 0.004244587849825621
iteration 254, loss = 0.004181820433586836
iteration 255, loss = 0.003996143117547035
iteration 256, loss = 0.004341294523328543
iteration 257, loss = 0.005209425464272499
iteration 258, loss = 0.004600192420184612
iteration 259, loss = 0.004036083817481995
iteration 260, loss = 0.004716468974947929
iteration 261, loss = 0.005359285045415163
iteration 262, loss = 0.004360539373010397
iteration 263, loss = 0.00460414495319128
iteration 264, loss = 0.004201075527817011
iteration 265, loss = 0.00576299661770463
iteration 266, loss = 0.00450257258489728
iteration 267, loss = 0.004527449142187834
iteration 268, loss = 0.00396118126809597
iteration 269, loss = 0.0041132159531116486
iteration 270, loss = 0.0050148856826126575
iteration 271, loss = 0.0038868568371981382
iteration 272, loss = 0.004034779500216246
iteration 273, loss = 0.004160686396062374
iteration 274, loss = 0.006045869551599026
iteration 275, loss = 0.006845145020633936
iteration 276, loss = 0.0051843044348061085
iteration 277, loss = 0.007017819210886955
iteration 278, loss = 0.004632793832570314
iteration 279, loss = 0.00724393967539072
iteration 280, loss = 0.00503266416490078
iteration 281, loss = 0.0046732923947274685
iteration 282, loss = 0.004301872570067644
iteration 283, loss = 0.005853375419974327
iteration 284, loss = 0.003916580229997635
iteration 285, loss = 0.00456734886392951
iteration 286, loss = 0.004014724399894476
iteration 287, loss = 0.004164931830018759
iteration 288, loss = 0.003932169172912836
iteration 289, loss = 0.0039100972935557365
iteration 290, loss = 0.004164119716733694
iteration 291, loss = 0.004127477295696735
iteration 292, loss = 0.005728456191718578
iteration 293, loss = 0.00454180920496583
iteration 294, loss = 0.004356655292212963
iteration 295, loss = 0.004061348736286163
iteration 296, loss = 0.003738163039088249
iteration 297, loss = 0.004212397616356611
iteration 298, loss = 0.007225926034152508
iteration 299, loss = 0.004238853231072426
iteration 300, loss = 0.003975254483520985
iteration 1, loss = 0.004636192694306374
iteration 2, loss = 0.0038434970192611217
iteration 3, loss = 0.0044442168436944485
iteration 4, loss = 0.005300047341734171
iteration 5, loss = 0.0054337275214493275
iteration 6, loss = 0.004034218844026327
iteration 7, loss = 0.004106881096959114
iteration 8, loss = 0.003985239192843437
iteration 9, loss = 0.006006456445902586
iteration 10, loss = 0.0037895929999649525
iteration 11, loss = 0.0046055978164076805
iteration 12, loss = 0.0068227932788431644
iteration 13, loss = 0.0038495506159961224
iteration 14, loss = 0.005260268226265907
iteration 15, loss = 0.004888137802481651
iteration 16, loss = 0.004182733595371246
iteration 17, loss = 0.005695476662367582
iteration 18, loss = 0.00431932182982564
iteration 19, loss = 0.003921411000192165
iteration 20, loss = 0.0038287390489131212
iteration 21, loss = 0.004066915716975927
iteration 22, loss = 0.004480224568396807
iteration 23, loss = 0.004202412907034159
iteration 24, loss = 0.005280255805701017
iteration 25, loss = 0.0042929090559482574
iteration 26, loss = 0.004478964023292065
iteration 27, loss = 0.004658009856939316
iteration 28, loss = 0.004292096011340618
iteration 29, loss = 0.004010003991425037
iteration 30, loss = 0.004266282543540001
iteration 31, loss = 0.004483504220843315
iteration 32, loss = 0.004177935421466827
iteration 33, loss = 0.0037261531688272953
iteration 34, loss = 0.0039053724613040686
iteration 35, loss = 0.005435320548713207
iteration 36, loss = 0.005669259931892157
iteration 37, loss = 0.003811741014942527
iteration 38, loss = 0.00448162667453289
iteration 39, loss = 0.0040633296594023705
iteration 40, loss = 0.003817431628704071
iteration 41, loss = 0.005121483467519283
iteration 42, loss = 0.004204274155199528
iteration 43, loss = 0.005659517832100391
iteration 44, loss = 0.003985720686614513
iteration 45, loss = 0.004208008758723736
iteration 46, loss = 0.003912384156137705
iteration 47, loss = 0.003721832763403654
iteration 48, loss = 0.005113299936056137
iteration 49, loss = 0.004203083459287882
iteration 50, loss = 0.005889481864869595
iteration 51, loss = 0.005537152290344238
iteration 52, loss = 0.006609141826629639
iteration 53, loss = 0.003901422256603837
iteration 54, loss = 0.005431248806416988
iteration 55, loss = 0.0046666827984154224
iteration 56, loss = 0.0045995586551725864
iteration 57, loss = 0.005566111765801907
iteration 58, loss = 0.0038927854038774967
iteration 59, loss = 0.005190116353332996
iteration 60, loss = 0.00569139001891017
iteration 61, loss = 0.005398459732532501
iteration 62, loss = 0.004534280393272638
iteration 63, loss = 0.004161648917943239
iteration 64, loss = 0.00407591974362731
iteration 65, loss = 0.004073792137205601
iteration 66, loss = 0.004913480021059513
iteration 67, loss = 0.003961174748837948
iteration 68, loss = 0.0043171001598238945
iteration 69, loss = 0.0039135850965976715
iteration 70, loss = 0.007864664308726788
iteration 71, loss = 0.005628732033073902
iteration 72, loss = 0.004492176230996847
iteration 73, loss = 0.004084098152816296
iteration 74, loss = 0.0040322719141840935
iteration 75, loss = 0.00392877496778965
iteration 76, loss = 0.004311216529458761
iteration 77, loss = 0.003693354083225131
iteration 78, loss = 0.004866327624768019
iteration 79, loss = 0.004157037939876318
iteration 80, loss = 0.003758334554731846
iteration 81, loss = 0.004011963494122028
iteration 82, loss = 0.005089683923870325
iteration 83, loss = 0.0055496422573924065
iteration 84, loss = 0.005191969219595194
iteration 85, loss = 0.004143226891756058
iteration 86, loss = 0.004443136043846607
iteration 87, loss = 0.004271975252777338
iteration 88, loss = 0.007541330996900797
iteration 89, loss = 0.005963749717921019
iteration 90, loss = 0.004165067337453365
iteration 91, loss = 0.004525948315858841
iteration 92, loss = 0.006839627865701914
iteration 93, loss = 0.003997522871941328
iteration 94, loss = 0.0043735806830227375
iteration 95, loss = 0.004187040962278843
iteration 96, loss = 0.005223846063017845
iteration 97, loss = 0.0038457794580608606
iteration 98, loss = 0.005851849913597107
iteration 99, loss = 0.0041743116453289986
iteration 100, loss = 0.005073504988104105
iteration 101, loss = 0.005183092318475246
iteration 102, loss = 0.004719773307442665
iteration 103, loss = 0.004031405318528414
iteration 104, loss = 0.005658627487719059
iteration 105, loss = 0.009219464845955372
iteration 106, loss = 0.005456462502479553
iteration 107, loss = 0.004409152083098888
iteration 108, loss = 0.0043458594009280205
iteration 109, loss = 0.005119210574775934
iteration 110, loss = 0.004037089645862579
iteration 111, loss = 0.004573633428663015
iteration 112, loss = 0.004316375590860844
iteration 113, loss = 0.0045535145327448845
iteration 114, loss = 0.0037987097166478634
iteration 115, loss = 0.00415037339553237
iteration 116, loss = 0.004265475552529097
iteration 117, loss = 0.003922373987734318
iteration 118, loss = 0.004400265868753195
iteration 119, loss = 0.0069440314546227455
iteration 120, loss = 0.00406552292406559
iteration 121, loss = 0.003939877729862928
iteration 122, loss = 0.0045034270733594894
iteration 123, loss = 0.004195811226963997
iteration 124, loss = 0.003982032649219036
iteration 125, loss = 0.004242494236677885
iteration 126, loss = 0.003930075094103813
iteration 127, loss = 0.00488333310931921
iteration 128, loss = 0.0043535856530070305
iteration 129, loss = 0.004457940347492695
iteration 130, loss = 0.004299283493310213
iteration 131, loss = 0.0042423042468726635
iteration 132, loss = 0.004272635094821453
iteration 133, loss = 0.005058986134827137
iteration 134, loss = 0.004515413660556078
iteration 135, loss = 0.005942835006862879
iteration 136, loss = 0.003939637448638678
iteration 137, loss = 0.004867206327617168
iteration 138, loss = 0.00355862476862967
iteration 139, loss = 0.004015064798295498
iteration 140, loss = 0.004621766973286867
iteration 141, loss = 0.0044231116771698
iteration 142, loss = 0.005971462465822697
iteration 143, loss = 0.004173029214143753
iteration 144, loss = 0.004034241661429405
iteration 145, loss = 0.0040561421774327755
iteration 146, loss = 0.005601462908089161
iteration 147, loss = 0.004261954687535763
iteration 148, loss = 0.003816330572590232
iteration 149, loss = 0.003979693632572889
iteration 150, loss = 0.00402694521471858
iteration 151, loss = 0.00404453044757247
iteration 152, loss = 0.005940067581832409
iteration 153, loss = 0.0057995859533548355
iteration 154, loss = 0.005715374369174242
iteration 155, loss = 0.00391529081389308
iteration 156, loss = 0.0038484104443341494
iteration 157, loss = 0.007153886370360851
iteration 158, loss = 0.004019024316221476
iteration 159, loss = 0.004674973897635937
iteration 160, loss = 0.003957995213568211
iteration 161, loss = 0.0045419735834002495
iteration 162, loss = 0.004349344410002232
iteration 163, loss = 0.004152919165790081
iteration 164, loss = 0.004908204078674316
iteration 165, loss = 0.0038002326618880033
iteration 166, loss = 0.0041152797639369965
iteration 167, loss = 0.004505224525928497
iteration 168, loss = 0.004095780663192272
iteration 169, loss = 0.004143205936998129
iteration 170, loss = 0.0037574991583824158
iteration 171, loss = 0.004166819155216217
iteration 172, loss = 0.004211348947137594
iteration 173, loss = 0.004018821753561497
iteration 174, loss = 0.003998372703790665
iteration 175, loss = 0.004335115198045969
iteration 176, loss = 0.004177452530711889
iteration 177, loss = 0.004370179492980242
iteration 178, loss = 0.003978847060352564
iteration 179, loss = 0.004635230638086796
iteration 180, loss = 0.0039296504110097885
iteration 181, loss = 0.004281305242329836
iteration 182, loss = 0.0041735731065273285
iteration 183, loss = 0.0041994862258434296
iteration 184, loss = 0.005307210609316826
iteration 185, loss = 0.0039803218096494675
iteration 186, loss = 0.004341375548392534
iteration 187, loss = 0.004236286040395498
iteration 188, loss = 0.00463428720831871
iteration 189, loss = 0.0041277590207755566
iteration 190, loss = 0.004316732287406921
iteration 191, loss = 0.005616136360913515
iteration 192, loss = 0.007548070978373289
iteration 193, loss = 0.00448515685275197
iteration 194, loss = 0.004246430471539497
iteration 195, loss = 0.004003902897238731
iteration 196, loss = 0.009372679516673088
iteration 197, loss = 0.00497721042484045
iteration 198, loss = 0.00407072901725769
iteration 199, loss = 0.004377835895866156
iteration 200, loss = 0.004105241969227791
iteration 201, loss = 0.004437872674316168
iteration 202, loss = 0.004470897372812033
iteration 203, loss = 0.004149028565734625
iteration 204, loss = 0.0048624626360833645
iteration 205, loss = 0.003924327902495861
iteration 206, loss = 0.004157216288149357
iteration 207, loss = 0.003781957784667611
iteration 208, loss = 0.0036890795454382896
iteration 209, loss = 0.005704802460968494
iteration 210, loss = 0.0040746754966676235
iteration 211, loss = 0.004035890102386475
iteration 212, loss = 0.004255118314176798
iteration 213, loss = 0.004160548560321331
iteration 214, loss = 0.004361219238489866
iteration 215, loss = 0.0036874650977551937
iteration 216, loss = 0.004516882821917534
iteration 217, loss = 0.00401916541159153
iteration 218, loss = 0.004021366126835346
iteration 219, loss = 0.004161097574979067
iteration 220, loss = 0.004295630380511284
iteration 221, loss = 0.004106809385120869
iteration 222, loss = 0.004206166137009859
iteration 223, loss = 0.0040436116978526115
iteration 224, loss = 0.0067197405733168125
iteration 225, loss = 0.0038358252495527267
iteration 226, loss = 0.004659298341721296
iteration 227, loss = 0.004270592704415321
iteration 228, loss = 0.0043923743069171906
iteration 229, loss = 0.004101990256458521
iteration 230, loss = 0.005577110219746828
iteration 231, loss = 0.004227596800774336
iteration 232, loss = 0.004308481700718403
iteration 233, loss = 0.005961271468549967
iteration 234, loss = 0.003930400125682354
iteration 235, loss = 0.005279863718897104
iteration 236, loss = 0.004428972955793142
iteration 237, loss = 0.00413061399012804
iteration 238, loss = 0.006028775125741959
iteration 239, loss = 0.0042767757549881935
iteration 240, loss = 0.004360742401331663
iteration 241, loss = 0.006812418345361948
iteration 242, loss = 0.005947570316493511
iteration 243, loss = 0.0041860477067530155
iteration 244, loss = 0.0039014697540551424
iteration 245, loss = 0.004442201927304268
iteration 246, loss = 0.003938090521842241
iteration 247, loss = 0.004745086655020714
iteration 248, loss = 0.0061715454794466496
iteration 249, loss = 0.004504631273448467
iteration 250, loss = 0.003932860679924488
iteration 251, loss = 0.003576110117137432
iteration 252, loss = 0.00831926055252552
iteration 253, loss = 0.0049448879435658455
iteration 254, loss = 0.005752041470259428
iteration 255, loss = 0.004115481395274401
iteration 256, loss = 0.003945027478039265
iteration 257, loss = 0.004246399737894535
iteration 258, loss = 0.004536962602287531
iteration 259, loss = 0.005130205303430557
iteration 260, loss = 0.0046846624463796616
iteration 261, loss = 0.004161624703556299
iteration 262, loss = 0.00470562232658267
iteration 263, loss = 0.0039993696846067905
iteration 264, loss = 0.004269290249794722
iteration 265, loss = 0.004113621544092894
iteration 266, loss = 0.004369352012872696
iteration 267, loss = 0.004909572657197714
iteration 268, loss = 0.004697381518781185
iteration 269, loss = 0.0071818153373897076
iteration 270, loss = 0.004048043396323919
iteration 271, loss = 0.00410431670024991
iteration 272, loss = 0.004643074702471495
iteration 273, loss = 0.005339848808944225
iteration 274, loss = 0.004028163850307465
iteration 275, loss = 0.0039800312370061874
iteration 276, loss = 0.0041201249696314335
iteration 277, loss = 0.0045770565047860146
iteration 278, loss = 0.0051590800285339355
iteration 279, loss = 0.003696155035868287
iteration 280, loss = 0.0038653528317809105
iteration 281, loss = 0.0042554400861263275
iteration 282, loss = 0.00449843006208539
iteration 283, loss = 0.00415081437677145
iteration 284, loss = 0.005441548768430948
iteration 285, loss = 0.008878973312675953
iteration 286, loss = 0.007073462009429932
iteration 287, loss = 0.004259544424712658
iteration 288, loss = 0.0048444801941514015
iteration 289, loss = 0.00388327706605196
iteration 290, loss = 0.003769250586628914
iteration 291, loss = 0.00447675259783864
iteration 292, loss = 0.004275016952306032
iteration 293, loss = 0.0040745362639427185
iteration 294, loss = 0.0041441828943789005
iteration 295, loss = 0.003915779758244753
iteration 296, loss = 0.004719320684671402
iteration 297, loss = 0.004181196913123131
iteration 298, loss = 0.004364782012999058
iteration 299, loss = 0.004070121794939041
iteration 300, loss = 0.00519479950889945
iteration 1, loss = 0.008248494006693363
iteration 2, loss = 0.00425219489261508
iteration 3, loss = 0.004294168204069138
iteration 4, loss = 0.004391316790133715
iteration 5, loss = 0.005130604840815067
iteration 6, loss = 0.0043890513479709625
iteration 7, loss = 0.004002689383924007
iteration 8, loss = 0.004107851535081863
iteration 9, loss = 0.005728553514927626
iteration 10, loss = 0.004142346326261759
iteration 11, loss = 0.0038494113832712173
iteration 12, loss = 0.004470378160476685
iteration 13, loss = 0.004118547774851322
iteration 14, loss = 0.004203757271170616
iteration 15, loss = 0.004615278914570808
iteration 16, loss = 0.0039459168910980225
iteration 17, loss = 0.004306026268750429
iteration 18, loss = 0.004147058818489313
iteration 19, loss = 0.004072834737598896
iteration 20, loss = 0.004061379004269838
iteration 21, loss = 0.005182224791496992
iteration 22, loss = 0.006007752846926451
iteration 23, loss = 0.004184744320809841
iteration 24, loss = 0.00422982033342123
iteration 25, loss = 0.00455338042229414
iteration 26, loss = 0.0040821172297000885
iteration 27, loss = 0.003937041386961937
iteration 28, loss = 0.004198544658720493
iteration 29, loss = 0.005384594202041626
iteration 30, loss = 0.004137036856263876
iteration 31, loss = 0.005130067002028227
iteration 32, loss = 0.0037920165341347456
iteration 33, loss = 0.006497794762253761
iteration 34, loss = 0.00450487993657589
iteration 35, loss = 0.004232251551002264
iteration 36, loss = 0.003742186352610588
iteration 37, loss = 0.005244210362434387
iteration 38, loss = 0.0059019941836595535
iteration 39, loss = 0.004316740669310093
iteration 40, loss = 0.0055172317661345005
iteration 41, loss = 0.0056732720695436
iteration 42, loss = 0.004701096564531326
iteration 43, loss = 0.00404713861644268
iteration 44, loss = 0.00409010611474514
iteration 45, loss = 0.004421224817633629
iteration 46, loss = 0.00450161611661315
iteration 47, loss = 0.004580369219183922
iteration 48, loss = 0.005842551123350859
iteration 49, loss = 0.004054517950862646
iteration 50, loss = 0.004376429598778486
iteration 51, loss = 0.0039044900331646204
iteration 52, loss = 0.004264621529728174
iteration 53, loss = 0.005180524196475744
iteration 54, loss = 0.0042430330067873
iteration 55, loss = 0.003949128556996584
iteration 56, loss = 0.004333877004683018
iteration 57, loss = 0.005036959424614906
iteration 58, loss = 0.0040208157151937485
iteration 59, loss = 0.004409607965499163
iteration 60, loss = 0.004045676440000534
iteration 61, loss = 0.004226342309266329
iteration 62, loss = 0.005328509025275707
iteration 63, loss = 0.004139761906117201
iteration 64, loss = 0.004328140523284674
iteration 65, loss = 0.003878386225551367
iteration 66, loss = 0.003779485821723938
iteration 67, loss = 0.007138051558285952
iteration 68, loss = 0.004078462719917297
iteration 69, loss = 0.005446969531476498
iteration 70, loss = 0.004273953847587109
iteration 71, loss = 0.0041880118660628796
iteration 72, loss = 0.005097127519547939
iteration 73, loss = 0.004284526687115431
iteration 74, loss = 0.004225160460919142
iteration 75, loss = 0.004147461149841547
iteration 76, loss = 0.00424389261752367
iteration 77, loss = 0.004282121546566486
iteration 78, loss = 0.003925052471458912
iteration 79, loss = 0.006629942916333675
iteration 80, loss = 0.004050740972161293
iteration 81, loss = 0.004115690942853689
iteration 82, loss = 0.005301888100802898
iteration 83, loss = 0.005088592879474163
iteration 84, loss = 0.008531549945473671
iteration 85, loss = 0.0043862066231667995
iteration 86, loss = 0.004347894340753555
iteration 87, loss = 0.0036817912477999926
iteration 88, loss = 0.004705439321696758
iteration 89, loss = 0.004104123450815678
iteration 90, loss = 0.0037512006238102913
iteration 91, loss = 0.003916165325790644
iteration 92, loss = 0.004026593640446663
iteration 93, loss = 0.005697862710803747
iteration 94, loss = 0.006517644505947828
iteration 95, loss = 0.004466651473194361
iteration 96, loss = 0.004066397435963154
iteration 97, loss = 0.003759281476959586
iteration 98, loss = 0.003939969930797815
iteration 99, loss = 0.004668674897402525
iteration 100, loss = 0.007712482940405607
iteration 101, loss = 0.005474107339978218
iteration 102, loss = 0.004184015095233917
iteration 103, loss = 0.004042987246066332
iteration 104, loss = 0.004002680070698261
iteration 105, loss = 0.00556496437638998
iteration 106, loss = 0.004551929887384176
iteration 107, loss = 0.004026711452752352
iteration 108, loss = 0.004448572173714638
iteration 109, loss = 0.004100401885807514
iteration 110, loss = 0.0042166574858129025
iteration 111, loss = 0.004368836060166359
iteration 112, loss = 0.00466524250805378
iteration 113, loss = 0.004717400297522545
iteration 114, loss = 0.004074788186699152
iteration 115, loss = 0.004287356045097113
iteration 116, loss = 0.0042941272258758545
iteration 117, loss = 0.003921334631741047
iteration 118, loss = 0.004228611476719379
iteration 119, loss = 0.004118654876947403
iteration 120, loss = 0.005955097731202841
iteration 121, loss = 0.004516455810517073
iteration 122, loss = 0.004546444397419691
iteration 123, loss = 0.004058219026774168
iteration 124, loss = 0.004916843492537737
iteration 125, loss = 0.004661873448640108
iteration 126, loss = 0.003992152865976095
iteration 127, loss = 0.0041368817910552025
iteration 128, loss = 0.0040869396179914474
iteration 129, loss = 0.005418360233306885
iteration 130, loss = 0.004114316776394844
iteration 131, loss = 0.0050909933634102345
iteration 132, loss = 0.005829664878547192
iteration 133, loss = 0.004207956604659557
iteration 134, loss = 0.003929541911929846
iteration 135, loss = 0.003713412443175912
iteration 136, loss = 0.004239464178681374
iteration 137, loss = 0.004193497821688652
iteration 138, loss = 0.0037137677427381277
iteration 139, loss = 0.0042372955940663815
iteration 140, loss = 0.005700875073671341
iteration 141, loss = 0.0037465598434209824
iteration 142, loss = 0.005232015624642372
iteration 143, loss = 0.0068267034366726875
iteration 144, loss = 0.004244462586939335
iteration 145, loss = 0.0048765987157821655
iteration 146, loss = 0.005213916301727295
iteration 147, loss = 0.0038887660484761
iteration 148, loss = 0.004532188177108765
iteration 149, loss = 0.00439316825941205
iteration 150, loss = 0.0038803773932158947
iteration 151, loss = 0.008447146974503994
iteration 152, loss = 0.0037837729323655367
iteration 153, loss = 0.0039799027144908905
iteration 154, loss = 0.004518940579146147
iteration 155, loss = 0.003981998655945063
iteration 156, loss = 0.003520996542647481
iteration 157, loss = 0.003968759439885616
iteration 158, loss = 0.003597300499677658
iteration 159, loss = 0.003698399756103754
iteration 160, loss = 0.004380173049867153
iteration 161, loss = 0.0036627755034714937
iteration 162, loss = 0.004054794553667307
iteration 163, loss = 0.0037485468201339245
iteration 164, loss = 0.004174265079200268
iteration 165, loss = 0.005247987806797028
iteration 166, loss = 0.004223328083753586
iteration 167, loss = 0.0072527616284787655
iteration 168, loss = 0.005210239440202713
iteration 169, loss = 0.004045114852488041
iteration 170, loss = 0.004148288629949093
iteration 171, loss = 0.004197051748633385
iteration 172, loss = 0.004985290579497814
iteration 173, loss = 0.00429324759170413
iteration 174, loss = 0.0045202141627669334
iteration 175, loss = 0.003940699156373739
iteration 176, loss = 0.004363713786005974
iteration 177, loss = 0.0037639255169779062
iteration 178, loss = 0.0040203360840678215
iteration 179, loss = 0.004536434076726437
iteration 180, loss = 0.00414816290140152
iteration 181, loss = 0.004269473720341921
iteration 182, loss = 0.004742540419101715
iteration 183, loss = 0.003960156813263893
iteration 184, loss = 0.004183373413980007
iteration 185, loss = 0.004895541816949844
iteration 186, loss = 0.004391527269035578
iteration 187, loss = 0.0046695927157998085
iteration 188, loss = 0.003980517387390137
iteration 189, loss = 0.00520316231995821
iteration 190, loss = 0.0037241880781948566
iteration 191, loss = 0.004551992751657963
iteration 192, loss = 0.004106510896235704
iteration 193, loss = 0.004158474039286375
iteration 194, loss = 0.005540500860661268
iteration 195, loss = 0.004811199381947517
iteration 196, loss = 0.00530315563082695
iteration 197, loss = 0.005216235760599375
iteration 198, loss = 0.00382688594982028
iteration 199, loss = 0.0043620821088552475
iteration 200, loss = 0.003942853771150112
iteration 201, loss = 0.003961360082030296
iteration 202, loss = 0.0038274561520665884
iteration 203, loss = 0.004110193345695734
iteration 204, loss = 0.004316312726587057
iteration 205, loss = 0.007906891405582428
iteration 206, loss = 0.004316904582083225
iteration 207, loss = 0.004245385993272066
iteration 208, loss = 0.004055717494338751
iteration 209, loss = 0.00680504459887743
iteration 210, loss = 0.003991484176367521
iteration 211, loss = 0.0070086135528981686
iteration 212, loss = 0.005274208262562752
iteration 213, loss = 0.0041862232610583305
iteration 214, loss = 0.005157207138836384
iteration 215, loss = 0.0040744999423623085
iteration 216, loss = 0.00432724179700017
iteration 217, loss = 0.007085059769451618
iteration 218, loss = 0.004125009290874004
iteration 219, loss = 0.004141737706959248
iteration 220, loss = 0.00411567185074091
iteration 221, loss = 0.004010979551821947
iteration 222, loss = 0.004207891877740622
iteration 223, loss = 0.004475532565265894
iteration 224, loss = 0.0037678407970815897
iteration 225, loss = 0.004873375874012709
iteration 226, loss = 0.004049759823828936
iteration 227, loss = 0.00396839389577508
iteration 228, loss = 0.004178483504801989
iteration 229, loss = 0.003975199535489082
iteration 230, loss = 0.004217819776386023
iteration 231, loss = 0.004330345895141363
iteration 232, loss = 0.004530252423137426
iteration 233, loss = 0.004416446667164564
iteration 234, loss = 0.003953500185161829
iteration 235, loss = 0.005472488701343536
iteration 236, loss = 0.00398163590580225
iteration 237, loss = 0.006851790007203817
iteration 238, loss = 0.004042689222842455
iteration 239, loss = 0.006801232695579529
iteration 240, loss = 0.004601229913532734
iteration 241, loss = 0.004193196073174477
iteration 242, loss = 0.004775224253535271
iteration 243, loss = 0.00407829973846674
iteration 244, loss = 0.0065099261701107025
iteration 245, loss = 0.0039123171009123325
iteration 246, loss = 0.0059974719770252705
iteration 247, loss = 0.004251936450600624
iteration 248, loss = 0.004008295945823193
iteration 249, loss = 0.0037216756027191877
iteration 250, loss = 0.004060193430632353
iteration 251, loss = 0.004107484593987465
iteration 252, loss = 0.004070628900080919
iteration 253, loss = 0.004990722984075546
iteration 254, loss = 0.005839796271175146
iteration 255, loss = 0.00589205976575613
iteration 256, loss = 0.004206869751214981
iteration 257, loss = 0.004696108866482973
iteration 258, loss = 0.0053492262959480286
iteration 259, loss = 0.004078355152159929
iteration 260, loss = 0.004061882849782705
iteration 261, loss = 0.005759075749665499
iteration 262, loss = 0.0044717974960803986
iteration 263, loss = 0.004152398090809584
iteration 264, loss = 0.0039920625276863575
iteration 265, loss = 0.005680420435965061
iteration 266, loss = 0.005803446285426617
iteration 267, loss = 0.004315790720283985
iteration 268, loss = 0.0038419109769165516
iteration 269, loss = 0.004158142488449812
iteration 270, loss = 0.004482802469283342
iteration 271, loss = 0.004109917674213648
iteration 272, loss = 0.004229277838021517
iteration 273, loss = 0.005101461429148912
iteration 274, loss = 0.004182812757790089
iteration 275, loss = 0.005244892556220293
iteration 276, loss = 0.004051381256431341
iteration 277, loss = 0.004152734763920307
iteration 278, loss = 0.004916544072329998
iteration 279, loss = 0.004441911820322275
iteration 280, loss = 0.004533929284662008
iteration 281, loss = 0.0047387187369167805
iteration 282, loss = 0.005335203837603331
iteration 283, loss = 0.005895947106182575
iteration 284, loss = 0.004022988956421614
iteration 285, loss = 0.0042005558498203754
iteration 286, loss = 0.004961796570569277
iteration 287, loss = 0.009916885755956173
iteration 288, loss = 0.0066921282559633255
iteration 289, loss = 0.003973669372498989
iteration 290, loss = 0.004619970452040434
iteration 291, loss = 0.005333608947694302
iteration 292, loss = 0.00447066081687808
iteration 293, loss = 0.004400039557367563
iteration 294, loss = 0.004275892395526171
iteration 295, loss = 0.004238790832459927
iteration 296, loss = 0.004494722932577133
iteration 297, loss = 0.005704739596694708
iteration 298, loss = 0.003899643663316965
iteration 299, loss = 0.003836255054920912
iteration 300, loss = 0.004789251834154129
iteration 1, loss = 0.005602785386145115
iteration 2, loss = 0.0042686788365244865
iteration 3, loss = 0.0037434189580380917
iteration 4, loss = 0.003932146355509758
iteration 5, loss = 0.005362215917557478
iteration 6, loss = 0.0058625610545277596
iteration 7, loss = 0.0040688179433345795
iteration 8, loss = 0.00548004312440753
iteration 9, loss = 0.0038546614814549685
iteration 10, loss = 0.006951424293220043
iteration 11, loss = 0.0042306846007704735
iteration 12, loss = 0.003944804891943932
iteration 13, loss = 0.004042018670588732
iteration 14, loss = 0.004338839091360569
iteration 15, loss = 0.004723282065242529
iteration 16, loss = 0.003837391035631299
iteration 17, loss = 0.00418200995773077
iteration 18, loss = 0.003993835300207138
iteration 19, loss = 0.004389953799545765
iteration 20, loss = 0.004168638959527016
iteration 21, loss = 0.0044545927084982395
iteration 22, loss = 0.00406646030023694
iteration 23, loss = 0.004000132903456688
iteration 24, loss = 0.0043988218531012535
iteration 25, loss = 0.004035224672406912
iteration 26, loss = 0.004153168294578791
iteration 27, loss = 0.005106736905872822
iteration 28, loss = 0.004317440092563629
iteration 29, loss = 0.004705979488790035
iteration 30, loss = 0.0055849491618573666
iteration 31, loss = 0.004470577463507652
iteration 32, loss = 0.004513878840953112
iteration 33, loss = 0.003897508140653372
iteration 34, loss = 0.003981542307883501
iteration 35, loss = 0.003915200009942055
iteration 36, loss = 0.004512373823672533
iteration 37, loss = 0.0039693391881883144
iteration 38, loss = 0.00376693531870842
iteration 39, loss = 0.0044423420913517475
iteration 40, loss = 0.005031176842749119
iteration 41, loss = 0.004391806665807962
iteration 42, loss = 0.005318209063261747
iteration 43, loss = 0.004380406346172094
iteration 44, loss = 0.0044069611467421055
iteration 45, loss = 0.004260402638465166
iteration 46, loss = 0.003993391525000334
iteration 47, loss = 0.004408376291394234
iteration 48, loss = 0.0038751030806452036
iteration 49, loss = 0.004081537947058678
iteration 50, loss = 0.0042960913851857185
iteration 51, loss = 0.005048149731010199
iteration 52, loss = 0.005520626436918974
iteration 53, loss = 0.004367946647107601
iteration 54, loss = 0.0039407601580023766
iteration 55, loss = 0.004125793464481831
iteration 56, loss = 0.004336549434810877
iteration 57, loss = 0.00405162712559104
iteration 58, loss = 0.003922628238797188
iteration 59, loss = 0.003994608763605356
iteration 60, loss = 0.004294082522392273
iteration 61, loss = 0.0044405157677829266
iteration 62, loss = 0.004552324768155813
iteration 63, loss = 0.003933958243578672
iteration 64, loss = 0.004159363452345133
iteration 65, loss = 0.004627489019185305
iteration 66, loss = 0.00419695395976305
iteration 67, loss = 0.003931550309062004
iteration 68, loss = 0.006620094645768404
iteration 69, loss = 0.0037440455053001642
iteration 70, loss = 0.005875668488442898
iteration 71, loss = 0.005705781746655703
iteration 72, loss = 0.003616724628955126
iteration 73, loss = 0.003923045005649328
iteration 74, loss = 0.0046919239684939384
iteration 75, loss = 0.005171049851924181
iteration 76, loss = 0.007008219137787819
iteration 77, loss = 0.004279079381376505
iteration 78, loss = 0.0042037502862513065
iteration 79, loss = 0.004236944019794464
iteration 80, loss = 0.004190148785710335
iteration 81, loss = 0.006778593175113201
iteration 82, loss = 0.0057228379882872105
iteration 83, loss = 0.0039035528898239136
iteration 84, loss = 0.0039028525352478027
iteration 85, loss = 0.003710951656103134
iteration 86, loss = 0.00530680688098073
iteration 87, loss = 0.004663087427616119
iteration 88, loss = 0.004233645740896463
iteration 89, loss = 0.004003704525530338
iteration 90, loss = 0.005515631753951311
iteration 91, loss = 0.003960495349019766
iteration 92, loss = 0.004880455322563648
iteration 93, loss = 0.0039707389660179615
iteration 94, loss = 0.003970031626522541
iteration 95, loss = 0.004135499242693186
iteration 96, loss = 0.004498141817748547
iteration 97, loss = 0.00576211279258132
iteration 98, loss = 0.004052403382956982
iteration 99, loss = 0.0051440708339214325
iteration 100, loss = 0.007362548727542162
iteration 101, loss = 0.006186223588883877
iteration 102, loss = 0.004143530037254095
iteration 103, loss = 0.004099622368812561
iteration 104, loss = 0.004203152377158403
iteration 105, loss = 0.00432187132537365
iteration 106, loss = 0.004775228910148144
iteration 107, loss = 0.004127801395952702
iteration 108, loss = 0.005448200274258852
iteration 109, loss = 0.006047584116458893
iteration 110, loss = 0.008350668475031853
iteration 111, loss = 0.0040891580283641815
iteration 112, loss = 0.004083126317709684
iteration 113, loss = 0.0038033935707062483
iteration 114, loss = 0.00455896370112896
iteration 115, loss = 0.0039581614546477795
iteration 116, loss = 0.003917391877621412
iteration 117, loss = 0.004262993577867746
iteration 118, loss = 0.006363055668771267
iteration 119, loss = 0.0043091061525046825
iteration 120, loss = 0.004023399204015732
iteration 121, loss = 0.004010435659438372
iteration 122, loss = 0.0050072199665009975
iteration 123, loss = 0.004115317016839981
iteration 124, loss = 0.004170852247625589
iteration 125, loss = 0.004440987482666969
iteration 126, loss = 0.004717560950666666
iteration 127, loss = 0.004272156860679388
iteration 128, loss = 0.004019258078187704
iteration 129, loss = 0.004234218969941139
iteration 130, loss = 0.005679577589035034
iteration 131, loss = 0.003922406118363142
iteration 132, loss = 0.004480506759136915
iteration 133, loss = 0.0043694996275007725
iteration 134, loss = 0.003922025673091412
iteration 135, loss = 0.005535023752599955
iteration 136, loss = 0.005118688568472862
iteration 137, loss = 0.004066809080541134
iteration 138, loss = 0.004068835638463497
iteration 139, loss = 0.005928311962634325
iteration 140, loss = 0.004619904328137636
iteration 141, loss = 0.004256166517734528
iteration 142, loss = 0.005601673386991024
iteration 143, loss = 0.004992087371647358
iteration 144, loss = 0.006315171252936125
iteration 145, loss = 0.004131689202040434
iteration 146, loss = 0.004157409071922302
iteration 147, loss = 0.004254162311553955
iteration 148, loss = 0.003730600466951728
iteration 149, loss = 0.005307475570589304
iteration 150, loss = 0.0044791558757424355
iteration 151, loss = 0.004170231521129608
iteration 152, loss = 0.0038456881884485483
iteration 153, loss = 0.004622103646397591
iteration 154, loss = 0.004098009783774614
iteration 155, loss = 0.005395650397986174
iteration 156, loss = 0.004591796081513166
iteration 157, loss = 0.0034834183752536774
iteration 158, loss = 0.003893714165315032
iteration 159, loss = 0.004562485497444868
iteration 160, loss = 0.0038624857552349567
iteration 161, loss = 0.003930965438485146
iteration 162, loss = 0.004129431210458279
iteration 163, loss = 0.004297151230275631
iteration 164, loss = 0.004324242472648621
iteration 165, loss = 0.004400881472975016
iteration 166, loss = 0.006329051218926907
iteration 167, loss = 0.007491081487387419
iteration 168, loss = 0.004181462340056896
iteration 169, loss = 0.004176808521151543
iteration 170, loss = 0.006554537918418646
iteration 171, loss = 0.004327571019530296
iteration 172, loss = 0.004964001942425966
iteration 173, loss = 0.003968258388340473
iteration 174, loss = 0.006740210577845573
iteration 175, loss = 0.003937245346605778
iteration 176, loss = 0.004441022407263517
iteration 177, loss = 0.003964974079281092
iteration 178, loss = 0.0070187002420425415
iteration 179, loss = 0.0040106694214046
iteration 180, loss = 0.003786689369007945
iteration 181, loss = 0.0037519519682973623
iteration 182, loss = 0.0037888612132519484
iteration 183, loss = 0.006047517526894808
iteration 184, loss = 0.0038308969233185053
iteration 185, loss = 0.005852941889315844
iteration 186, loss = 0.0038794490974396467
iteration 187, loss = 0.005092548672109842
iteration 188, loss = 0.003999981563538313
iteration 189, loss = 0.00385060696862638
iteration 190, loss = 0.00446096807718277
iteration 191, loss = 0.004410059191286564
iteration 192, loss = 0.004527141340076923
iteration 193, loss = 0.004250809550285339
iteration 194, loss = 0.006144691724330187
iteration 195, loss = 0.003817419521510601
iteration 196, loss = 0.004004822578281164
iteration 197, loss = 0.005230131559073925
iteration 198, loss = 0.004317279905080795
iteration 199, loss = 0.004056012723594904
iteration 200, loss = 0.0039713033474981785
iteration 201, loss = 0.004718161653727293
iteration 202, loss = 0.006168091669678688
iteration 203, loss = 0.0045719146728515625
iteration 204, loss = 0.004524748306721449
iteration 205, loss = 0.0038915739860385656
iteration 206, loss = 0.003886563703417778
iteration 207, loss = 0.004101017024368048
iteration 208, loss = 0.006016096565872431
iteration 209, loss = 0.0036432477645576
iteration 210, loss = 0.005824017338454723
iteration 211, loss = 0.00396686140447855
iteration 212, loss = 0.004436802119016647
iteration 213, loss = 0.004251374397426844
iteration 214, loss = 0.00580625282600522
iteration 215, loss = 0.00404702452942729
iteration 216, loss = 0.0038335067220032215
iteration 217, loss = 0.0040537090972065926
iteration 218, loss = 0.004475681111216545
iteration 219, loss = 0.004381388425827026
iteration 220, loss = 0.004081185441464186
iteration 221, loss = 0.004086356610059738
iteration 222, loss = 0.004082824569195509
iteration 223, loss = 0.00444452790543437
iteration 224, loss = 0.003930885344743729
iteration 225, loss = 0.003914438653737307
iteration 226, loss = 0.003959156572818756
iteration 227, loss = 0.006769833620637655
iteration 228, loss = 0.004664337728172541
iteration 229, loss = 0.005941223353147507
iteration 230, loss = 0.004638648126274347
iteration 231, loss = 0.0038549976889044046
iteration 232, loss = 0.004171495325863361
iteration 233, loss = 0.0042601036839187145
iteration 234, loss = 0.004617716185748577
iteration 235, loss = 0.004057982470840216
iteration 236, loss = 0.00677750026807189
iteration 237, loss = 0.003974742256104946
iteration 238, loss = 0.004294159356504679
iteration 239, loss = 0.004550956189632416
iteration 240, loss = 0.0039984080940485
iteration 241, loss = 0.007271353621035814
iteration 242, loss = 0.005727965850383043
iteration 243, loss = 0.00384556595236063
iteration 244, loss = 0.003792139468714595
iteration 245, loss = 0.0041361162438988686
iteration 246, loss = 0.005215524695813656
iteration 247, loss = 0.004875976126641035
iteration 248, loss = 0.003939297050237656
iteration 249, loss = 0.0039600832387804985
iteration 250, loss = 0.003771288087591529
iteration 251, loss = 0.003776065306738019
iteration 252, loss = 0.009242589585483074
iteration 253, loss = 0.004294092766940594
iteration 254, loss = 0.004026248585432768
iteration 255, loss = 0.005397319793701172
iteration 256, loss = 0.006631315685808659
iteration 257, loss = 0.004169390071183443
iteration 258, loss = 0.004196817055344582
iteration 259, loss = 0.004007688723504543
iteration 260, loss = 0.004028245806694031
iteration 261, loss = 0.005350229796022177
iteration 262, loss = 0.0068028331734240055
iteration 263, loss = 0.004057515412569046
iteration 264, loss = 0.0051608034409582615
iteration 265, loss = 0.003468699986115098
iteration 266, loss = 0.006087400484830141
iteration 267, loss = 0.004919865634292364
iteration 268, loss = 0.004428831860423088
iteration 269, loss = 0.004592192359268665
iteration 270, loss = 0.004098196979612112
iteration 271, loss = 0.004165865480899811
iteration 272, loss = 0.003916112706065178
iteration 273, loss = 0.004485724493861198
iteration 274, loss = 0.005141911096870899
iteration 275, loss = 0.0037733756471425295
iteration 276, loss = 0.00404378492385149
iteration 277, loss = 0.003851515706628561
iteration 278, loss = 0.004226408898830414
iteration 279, loss = 0.004288030788302422
iteration 280, loss = 0.005060523748397827
iteration 281, loss = 0.0038272724486887455
iteration 282, loss = 0.004243583884090185
iteration 283, loss = 0.003773860400542617
iteration 284, loss = 0.004063272848725319
iteration 285, loss = 0.005788484588265419
iteration 286, loss = 0.004370744340121746
iteration 287, loss = 0.0038777412846684456
iteration 288, loss = 0.00522885425016284
iteration 289, loss = 0.006672967690974474
iteration 290, loss = 0.004407623782753944
iteration 291, loss = 0.00419651297852397
iteration 292, loss = 0.00796479918062687
iteration 293, loss = 0.005822168197482824
iteration 294, loss = 0.003962962422519922
iteration 295, loss = 0.0036503239534795284
iteration 296, loss = 0.004591589793562889
iteration 297, loss = 0.005440541543066502
iteration 298, loss = 0.004294874612241983
iteration 299, loss = 0.00482413824647665
iteration 300, loss = 0.0042718276381492615
iteration 1, loss = 0.003972236067056656
iteration 2, loss = 0.004153121262788773
iteration 3, loss = 0.004363765940070152
iteration 4, loss = 0.0046142335049808025
iteration 5, loss = 0.0037226032000035048
iteration 6, loss = 0.004338506143540144
iteration 7, loss = 0.004759936593472958
iteration 8, loss = 0.003996680490672588
iteration 9, loss = 0.004616750869899988
iteration 10, loss = 0.0038623670116066933
iteration 11, loss = 0.0046124085783958435
iteration 12, loss = 0.004108559340238571
iteration 13, loss = 0.0051002949476242065
iteration 14, loss = 0.004167594481259584
iteration 15, loss = 0.0040716915391385555
iteration 16, loss = 0.0044191209599375725
iteration 17, loss = 0.0038009004201740026
iteration 18, loss = 0.004334751516580582
iteration 19, loss = 0.004460635595023632
iteration 20, loss = 0.004359318874776363
iteration 21, loss = 0.003781353123486042
iteration 22, loss = 0.00408971356227994
iteration 23, loss = 0.004511123523116112
iteration 24, loss = 0.0057244254276156425
iteration 25, loss = 0.0036974470131099224
iteration 26, loss = 0.004255476873368025
iteration 27, loss = 0.003919569775462151
iteration 28, loss = 0.004075503442436457
iteration 29, loss = 0.0037078028544783592
iteration 30, loss = 0.004855804145336151
iteration 31, loss = 0.004353251773864031
iteration 32, loss = 0.003852750640362501
iteration 33, loss = 0.005087302997708321
iteration 34, loss = 0.004444304388016462
iteration 35, loss = 0.003921834286302328
iteration 36, loss = 0.005232958123087883
iteration 37, loss = 0.006807355210185051
iteration 38, loss = 0.004677420016378164
iteration 39, loss = 0.003952271770685911
iteration 40, loss = 0.0062314788810908794
iteration 41, loss = 0.004013450350612402
iteration 42, loss = 0.0054765562526881695
iteration 43, loss = 0.00506039010360837
iteration 44, loss = 0.006863023620098829
iteration 45, loss = 0.004317366983741522
iteration 46, loss = 0.0043883477337658405
iteration 47, loss = 0.004010284319519997
iteration 48, loss = 0.005350269377231598
iteration 49, loss = 0.00424163555726409
iteration 50, loss = 0.0044469828717410564
iteration 51, loss = 0.006615479942411184
iteration 52, loss = 0.005700820591300726
iteration 53, loss = 0.005509228445589542
iteration 54, loss = 0.004010582808405161
iteration 55, loss = 0.005887719336897135
iteration 56, loss = 0.0038262612652033567
iteration 57, loss = 0.004144024103879929
iteration 58, loss = 0.004717209842056036
iteration 59, loss = 0.004045946057885885
iteration 60, loss = 0.004020872991532087
iteration 61, loss = 0.00543837808072567
iteration 62, loss = 0.003934287931770086
iteration 63, loss = 0.0038752430118620396
iteration 64, loss = 0.0044777896255254745
iteration 65, loss = 0.004254086874425411
iteration 66, loss = 0.006901878397911787
iteration 67, loss = 0.004479676950722933
iteration 68, loss = 0.005999743472784758
iteration 69, loss = 0.005202055908739567
iteration 70, loss = 0.0049546691589057446
iteration 71, loss = 0.00593923032283783
iteration 72, loss = 0.0055546872317790985
iteration 73, loss = 0.006000553723424673
iteration 74, loss = 0.004138203803449869
iteration 75, loss = 0.004169836174696684
iteration 76, loss = 0.006210384424775839
iteration 77, loss = 0.004036240745335817
iteration 78, loss = 0.004166228696703911
iteration 79, loss = 0.007074922323226929
iteration 80, loss = 0.004373759031295776
iteration 81, loss = 0.0038489196449518204
iteration 82, loss = 0.003915105480700731
iteration 83, loss = 0.005419392604380846
iteration 84, loss = 0.004149251617491245
iteration 85, loss = 0.0040796794928610325
iteration 86, loss = 0.004113621078431606
iteration 87, loss = 0.004176985938102007
iteration 88, loss = 0.00392518937587738
iteration 89, loss = 0.004137363284826279
iteration 90, loss = 0.0039190505631268024
iteration 91, loss = 0.004374598152935505
iteration 92, loss = 0.004399375058710575
iteration 93, loss = 0.004430197179317474
iteration 94, loss = 0.006604205351322889
iteration 95, loss = 0.0043934546411037445
iteration 96, loss = 0.005475789308547974
iteration 97, loss = 0.00429138820618391
iteration 98, loss = 0.005573207512497902
iteration 99, loss = 0.004246123135089874
iteration 100, loss = 0.005026726517826319
iteration 101, loss = 0.004900062456727028
iteration 102, loss = 0.007031870540231466
iteration 103, loss = 0.0071524702943861485
iteration 104, loss = 0.006833575200289488
iteration 105, loss = 0.005159496795386076
iteration 106, loss = 0.004563345108181238
iteration 107, loss = 0.005727096926420927
iteration 108, loss = 0.004278207663446665
iteration 109, loss = 0.004478044807910919
iteration 110, loss = 0.003934004809707403
iteration 111, loss = 0.003861574921756983
iteration 112, loss = 0.005053562577813864
iteration 113, loss = 0.004051747731864452
iteration 114, loss = 0.00436374731361866
iteration 115, loss = 0.0040605803951621056
iteration 116, loss = 0.005867126863449812
iteration 117, loss = 0.0044713858515024185
iteration 118, loss = 0.005071382503956556
iteration 119, loss = 0.004241799470037222
iteration 120, loss = 0.0042675514705479145
iteration 121, loss = 0.0037947306409478188
iteration 122, loss = 0.004144828766584396
iteration 123, loss = 0.0041234674863517284
iteration 124, loss = 0.0038851345889270306
iteration 125, loss = 0.004115907009691
iteration 126, loss = 0.004082603380084038
iteration 127, loss = 0.00403281394392252
iteration 128, loss = 0.004476756323128939
iteration 129, loss = 0.0044661895371973515
iteration 130, loss = 0.005241129547357559
iteration 131, loss = 0.006919751409441233
iteration 132, loss = 0.004470426589250565
iteration 133, loss = 0.004239042289555073
iteration 134, loss = 0.005897517781704664
iteration 135, loss = 0.00473741302266717
iteration 136, loss = 0.003824112005531788
iteration 137, loss = 0.0042105684988200665
iteration 138, loss = 0.00428741192445159
iteration 139, loss = 0.005376663524657488
iteration 140, loss = 0.005044141318649054
iteration 141, loss = 0.004024340305477381
iteration 142, loss = 0.005262949503958225
iteration 143, loss = 0.0037735586520284414
iteration 144, loss = 0.00569600984454155
iteration 145, loss = 0.005654366686940193
iteration 146, loss = 0.005146453157067299
iteration 147, loss = 0.0038119240198284388
iteration 148, loss = 0.0043573761358857155
iteration 149, loss = 0.006756891962140799
iteration 150, loss = 0.004066359251737595
iteration 151, loss = 0.003977596759796143
iteration 152, loss = 0.0037900249008089304
iteration 153, loss = 0.006795516237616539
iteration 154, loss = 0.0061200340278446674
iteration 155, loss = 0.0036892846692353487
iteration 156, loss = 0.003991018049418926
iteration 157, loss = 0.003716300707310438
iteration 158, loss = 0.004029972944408655
iteration 159, loss = 0.005317655857652426
iteration 160, loss = 0.004085649736225605
iteration 161, loss = 0.006352253723889589
iteration 162, loss = 0.004827316850423813
iteration 163, loss = 0.0038730057422071695
iteration 164, loss = 0.004000984597951174
iteration 165, loss = 0.007016010582447052
iteration 166, loss = 0.00616312213242054
iteration 167, loss = 0.005116038955748081
iteration 168, loss = 0.006766991689801216
iteration 169, loss = 0.0047530136071145535
iteration 170, loss = 0.003999544773250818
iteration 171, loss = 0.004137586802244186
iteration 172, loss = 0.003819997189566493
iteration 173, loss = 0.004039375577121973
iteration 174, loss = 0.004061414860188961
iteration 175, loss = 0.004034398589283228
iteration 176, loss = 0.004103117622435093
iteration 177, loss = 0.004475459922105074
iteration 178, loss = 0.003743664361536503
iteration 179, loss = 0.004442925099283457
iteration 180, loss = 0.0043224068358540535
iteration 181, loss = 0.0040695867501199245
iteration 182, loss = 0.0040058474987745285
iteration 183, loss = 0.004111188929527998
iteration 184, loss = 0.0040201242081820965
iteration 185, loss = 0.0038240670692175627
iteration 186, loss = 0.006175451911985874
iteration 187, loss = 0.004151399713009596
iteration 188, loss = 0.004270793404430151
iteration 189, loss = 0.003799963742494583
iteration 190, loss = 0.004056215286254883
iteration 191, loss = 0.004293366335332394
iteration 192, loss = 0.004106803797185421
iteration 193, loss = 0.0051081436686217785
iteration 194, loss = 0.00564310047775507
iteration 195, loss = 0.004257446154952049
iteration 196, loss = 0.004360295366495848
iteration 197, loss = 0.004107935819774866
iteration 198, loss = 0.005488691385835409
iteration 199, loss = 0.004148510284721851
iteration 200, loss = 0.003873258363455534
iteration 201, loss = 0.00431080861017108
iteration 202, loss = 0.004529548808932304
iteration 203, loss = 0.004338430240750313
iteration 204, loss = 0.003617950016632676
iteration 205, loss = 0.0037743563298135996
iteration 206, loss = 0.0038723533507436514
iteration 207, loss = 0.00404640007764101
iteration 208, loss = 0.004204473923891783
iteration 209, loss = 0.004559094551950693
iteration 210, loss = 0.005046461708843708
iteration 211, loss = 0.0037088640965521336
iteration 212, loss = 0.004493878223001957
iteration 213, loss = 0.0038867827970534563
iteration 214, loss = 0.006699300371110439
iteration 215, loss = 0.004066199529916048
iteration 216, loss = 0.00553731806576252
iteration 217, loss = 0.005883716978132725
iteration 218, loss = 0.003938032779842615
iteration 219, loss = 0.0038295837584882975
iteration 220, loss = 0.004226114600896835
iteration 221, loss = 0.003897890681400895
iteration 222, loss = 0.00440446101129055
iteration 223, loss = 0.004276637453585863
iteration 224, loss = 0.004170338623225689
iteration 225, loss = 0.003937357105314732
iteration 226, loss = 0.004247448872774839
iteration 227, loss = 0.0058179814368486404
iteration 228, loss = 0.004190478939563036
iteration 229, loss = 0.006759407464414835
iteration 230, loss = 0.004461260046809912
iteration 231, loss = 0.005869096610695124
iteration 232, loss = 0.004474887624382973
iteration 233, loss = 0.003750335657969117
iteration 234, loss = 0.004071342758834362
iteration 235, loss = 0.004375276621431112
iteration 236, loss = 0.003937590401619673
iteration 237, loss = 0.005065456032752991
iteration 238, loss = 0.0039366986602544785
iteration 239, loss = 0.005611763335764408
iteration 240, loss = 0.005781467072665691
iteration 241, loss = 0.00394377950578928
iteration 242, loss = 0.004440859891474247
iteration 243, loss = 0.003984673880040646
iteration 244, loss = 0.00455060787498951
iteration 245, loss = 0.004111445043236017
iteration 246, loss = 0.004821443464607
iteration 247, loss = 0.003978018648922443
iteration 248, loss = 0.004529415629804134
iteration 249, loss = 0.004481298383325338
iteration 250, loss = 0.00740819750353694
iteration 251, loss = 0.004169686697423458
iteration 252, loss = 0.004638027865439653
iteration 253, loss = 0.004067958798259497
iteration 254, loss = 0.0038763065822422504
iteration 255, loss = 0.0041536190547049046
iteration 256, loss = 0.005893997382372618
iteration 257, loss = 0.0040326546877622604
iteration 258, loss = 0.0043710460886359215
iteration 259, loss = 0.003897608257830143
iteration 260, loss = 0.0053021772764623165
iteration 261, loss = 0.0038386289961636066
iteration 262, loss = 0.0049370271153748035
iteration 263, loss = 0.003981537185609341
iteration 264, loss = 0.004069895017892122
iteration 265, loss = 0.0039630308747291565
iteration 266, loss = 0.003892416600137949
iteration 267, loss = 0.004238438326865435
iteration 268, loss = 0.004497310146689415
iteration 269, loss = 0.0038484458345919847
iteration 270, loss = 0.004277092404663563
iteration 271, loss = 0.004353680647909641
iteration 272, loss = 0.0037802192382514477
iteration 273, loss = 0.0040528737008571625
iteration 274, loss = 0.003929566126316786
iteration 275, loss = 0.004718767944723368
iteration 276, loss = 0.004654168151319027
iteration 277, loss = 0.0073835719376802444
iteration 278, loss = 0.0044199014082551
iteration 279, loss = 0.0036445092409849167
iteration 280, loss = 0.00379997119307518
iteration 281, loss = 0.003462133463472128
iteration 282, loss = 0.0040953755378723145
iteration 283, loss = 0.003726435359567404
iteration 284, loss = 0.004314973950386047
iteration 285, loss = 0.004001578316092491
iteration 286, loss = 0.004223058465868235
iteration 287, loss = 0.004822362679988146
iteration 288, loss = 0.006280857603996992
iteration 289, loss = 0.004196949768811464
iteration 290, loss = 0.00455196388065815
iteration 291, loss = 0.004015828017145395
iteration 292, loss = 0.003912513609975576
iteration 293, loss = 0.003893067827448249
iteration 294, loss = 0.00493350625038147
iteration 295, loss = 0.004663325380533934
iteration 296, loss = 0.006042654160410166
iteration 297, loss = 0.004229048732668161
iteration 298, loss = 0.004023508168756962
iteration 299, loss = 0.004527387209236622
iteration 300, loss = 0.004188567865639925
iteration 1, loss = 0.003891127649694681
iteration 2, loss = 0.005647275596857071
iteration 3, loss = 0.004238977562636137
iteration 4, loss = 0.003896879032254219
iteration 5, loss = 0.004181160591542721
iteration 6, loss = 0.0037760031409561634
iteration 7, loss = 0.004087601788341999
iteration 8, loss = 0.0036870886106044054
iteration 9, loss = 0.004323222674429417
iteration 10, loss = 0.004450421780347824
iteration 11, loss = 0.004289599601179361
iteration 12, loss = 0.004955897573381662
iteration 13, loss = 0.0037017324939370155
iteration 14, loss = 0.00567703926935792
iteration 15, loss = 0.005289244465529919
iteration 16, loss = 0.004116108641028404
iteration 17, loss = 0.004560696426779032
iteration 18, loss = 0.005156165454536676
iteration 19, loss = 0.0037055157590657473
iteration 20, loss = 0.005256290081888437
iteration 21, loss = 0.0040625291876494884
iteration 22, loss = 0.004239019006490707
iteration 23, loss = 0.004020360764116049
iteration 24, loss = 0.004918934311717749
iteration 25, loss = 0.004730700049549341
iteration 26, loss = 0.004112943075597286
iteration 27, loss = 0.004537670873105526
iteration 28, loss = 0.006708980072289705
iteration 29, loss = 0.005539149511605501
iteration 30, loss = 0.004566010553389788
iteration 31, loss = 0.0054921056143939495
iteration 32, loss = 0.003999094478785992
iteration 33, loss = 0.004141367040574551
iteration 34, loss = 0.00411040335893631
iteration 35, loss = 0.0041135456413030624
iteration 36, loss = 0.004400444217026234
iteration 37, loss = 0.005635622888803482
iteration 38, loss = 0.003937386907637119
iteration 39, loss = 0.004788269754499197
iteration 40, loss = 0.004290584474802017
iteration 41, loss = 0.0043531726114451885
iteration 42, loss = 0.004122461657971144
iteration 43, loss = 0.004123194143176079
iteration 44, loss = 0.004236957523971796
iteration 45, loss = 0.0043958863243460655
iteration 46, loss = 0.003811855101957917
iteration 47, loss = 0.0041439104825258255
iteration 48, loss = 0.005006251856684685
iteration 49, loss = 0.004165844991803169
iteration 50, loss = 0.007409564219415188
iteration 51, loss = 0.0061334772035479546
iteration 52, loss = 0.005514106713235378
iteration 53, loss = 0.004229642450809479
iteration 54, loss = 0.004480576608330011
iteration 55, loss = 0.006790636107325554
iteration 56, loss = 0.004477926529943943
iteration 57, loss = 0.0042656464502215385
iteration 58, loss = 0.004138270393013954
iteration 59, loss = 0.004376295022666454
iteration 60, loss = 0.004068608861416578
iteration 61, loss = 0.0042761554941535
iteration 62, loss = 0.004604368470609188
iteration 63, loss = 0.004451810847967863
iteration 64, loss = 0.0036249514669179916
iteration 65, loss = 0.004279256332665682
iteration 66, loss = 0.004696916788816452
iteration 67, loss = 0.0059160273522138596
iteration 68, loss = 0.00390068581327796
iteration 69, loss = 0.004117891658097506
iteration 70, loss = 0.004559160675853491
iteration 71, loss = 0.0038827720563858747
iteration 72, loss = 0.0041707525961101055
iteration 73, loss = 0.004397834651172161
iteration 74, loss = 0.00464104488492012
iteration 75, loss = 0.004125304985791445
iteration 76, loss = 0.004133446142077446
iteration 77, loss = 0.0051164221949875355
iteration 78, loss = 0.004024536348879337
iteration 79, loss = 0.003944614436477423
iteration 80, loss = 0.003944083582609892
iteration 81, loss = 0.003950237762182951
iteration 82, loss = 0.003971736878156662
iteration 83, loss = 0.005699440371245146
iteration 84, loss = 0.003794451244175434
iteration 85, loss = 0.0038515287451446056
iteration 86, loss = 0.004040755797177553
iteration 87, loss = 0.004100136924535036
iteration 88, loss = 0.004039721563458443
iteration 89, loss = 0.004210132174193859
iteration 90, loss = 0.0053611076436936855
iteration 91, loss = 0.004018069244921207
iteration 92, loss = 0.0042424993589520454
iteration 93, loss = 0.003950390499085188
iteration 94, loss = 0.00406050868332386
iteration 95, loss = 0.005776823032647371
iteration 96, loss = 0.003841300494968891
iteration 97, loss = 0.004984697792679071
iteration 98, loss = 0.00417209742590785
iteration 99, loss = 0.007915863767266273
iteration 100, loss = 0.004453354049474001
iteration 101, loss = 0.004345593973994255
iteration 102, loss = 0.007560018450021744
iteration 103, loss = 0.004114924930036068
iteration 104, loss = 0.005519210360944271
iteration 105, loss = 0.005240893457084894
iteration 106, loss = 0.006016282830387354
iteration 107, loss = 0.0038716732524335384
iteration 108, loss = 0.004115838557481766
iteration 109, loss = 0.004056147299706936
iteration 110, loss = 0.003908168990164995
iteration 111, loss = 0.004575274884700775
iteration 112, loss = 0.004178659524768591
iteration 113, loss = 0.00542641943320632
iteration 114, loss = 0.0038953295443207026
iteration 115, loss = 0.0044603776186704636
iteration 116, loss = 0.004128945525735617
iteration 117, loss = 0.007879874669015408
iteration 118, loss = 0.0042496235109865665
iteration 119, loss = 0.006527508608996868
iteration 120, loss = 0.0036219933535903692
iteration 121, loss = 0.004342219792306423
iteration 122, loss = 0.004309673793613911
iteration 123, loss = 0.004253298044204712
iteration 124, loss = 0.004861511290073395
iteration 125, loss = 0.0039235916920006275
iteration 126, loss = 0.006468521431088448
iteration 127, loss = 0.0037209487054497004
iteration 128, loss = 0.003979220055043697
iteration 129, loss = 0.004122378304600716
iteration 130, loss = 0.0054939668625593185
iteration 131, loss = 0.004080396611243486
iteration 132, loss = 0.003955462481826544
iteration 133, loss = 0.0038802404887974262
iteration 134, loss = 0.004124362953007221
iteration 135, loss = 0.003742572385817766
iteration 136, loss = 0.0044032130390405655
iteration 137, loss = 0.003594747744500637
iteration 138, loss = 0.0036942462902516127
iteration 139, loss = 0.004130655899643898
iteration 140, loss = 0.004297808278352022
iteration 141, loss = 0.004134232643991709
iteration 142, loss = 0.00412857485935092
iteration 143, loss = 0.00413906667381525
iteration 144, loss = 0.004008396062999964
iteration 145, loss = 0.004528411664068699
iteration 146, loss = 0.006128204520791769
iteration 147, loss = 0.005175058729946613
iteration 148, loss = 0.004160988610237837
iteration 149, loss = 0.004133565817028284
iteration 150, loss = 0.0047861444763839245
iteration 151, loss = 0.004435300827026367
iteration 152, loss = 0.004621452186256647
iteration 153, loss = 0.004470124840736389
iteration 154, loss = 0.004451911896467209
iteration 155, loss = 0.00505370507016778
iteration 156, loss = 0.004366533365100622
iteration 157, loss = 0.0041707283817231655
iteration 158, loss = 0.004355899523943663
iteration 159, loss = 0.006350209470838308
iteration 160, loss = 0.004206202924251556
iteration 161, loss = 0.006225964054465294
iteration 162, loss = 0.004212404601275921
iteration 163, loss = 0.004016190301626921
iteration 164, loss = 0.006744525395333767
iteration 165, loss = 0.007113664411008358
iteration 166, loss = 0.004015452228486538
iteration 167, loss = 0.0056877885945141315
iteration 168, loss = 0.003976493142545223
iteration 169, loss = 0.00394671643152833
iteration 170, loss = 0.004296126775443554
iteration 171, loss = 0.004344344139099121
iteration 172, loss = 0.0056624398566782475
iteration 173, loss = 0.00580767635256052
iteration 174, loss = 0.004586423747241497
iteration 175, loss = 0.005604599602520466
iteration 176, loss = 0.0038716814015060663
iteration 177, loss = 0.004237117245793343
iteration 178, loss = 0.0038883681409060955
iteration 179, loss = 0.0036660756450146437
iteration 180, loss = 0.004174735862761736
iteration 181, loss = 0.0038591837510466576
iteration 182, loss = 0.004864806775003672
iteration 183, loss = 0.004045494366437197
iteration 184, loss = 0.004304561764001846
iteration 185, loss = 0.0038870624266564846
iteration 186, loss = 0.0043958076275885105
iteration 187, loss = 0.004085382912307978
iteration 188, loss = 0.006757880561053753
iteration 189, loss = 0.00522684957832098
iteration 190, loss = 0.004903319291770458
iteration 191, loss = 0.004059574566781521
iteration 192, loss = 0.004188382998108864
iteration 193, loss = 0.0035374765284359455
iteration 194, loss = 0.0037440278101712465
iteration 195, loss = 0.004424729850143194
iteration 196, loss = 0.004165473394095898
iteration 197, loss = 0.004194785840809345
iteration 198, loss = 0.00412863539531827
iteration 199, loss = 0.006703491788357496
iteration 200, loss = 0.003906932659447193
iteration 201, loss = 0.004054563120007515
iteration 202, loss = 0.004129637498408556
iteration 203, loss = 0.004544686526060104
iteration 204, loss = 0.004499837290495634
iteration 205, loss = 0.004849085118621588
iteration 206, loss = 0.004446018021553755
iteration 207, loss = 0.003995403181761503
iteration 208, loss = 0.003987934440374374
iteration 209, loss = 0.004158292431384325
iteration 210, loss = 0.0043908292427659035
iteration 211, loss = 0.003995270002633333
iteration 212, loss = 0.006600281223654747
iteration 213, loss = 0.0040227207355201244
iteration 214, loss = 0.0037781312130391598
iteration 215, loss = 0.004312674980610609
iteration 216, loss = 0.004258116241544485
iteration 217, loss = 0.0039055494125932455
iteration 218, loss = 0.003671338316053152
iteration 219, loss = 0.004208466503769159
iteration 220, loss = 0.007012632209807634
iteration 221, loss = 0.006566984113305807
iteration 222, loss = 0.004019437823444605
iteration 223, loss = 0.005457138177007437
iteration 224, loss = 0.007067670114338398
iteration 225, loss = 0.005224720109254122
iteration 226, loss = 0.004588209558278322
iteration 227, loss = 0.005787653848528862
iteration 228, loss = 0.0044889687560498714
iteration 229, loss = 0.004082303959876299
iteration 230, loss = 0.005825001280754805
iteration 231, loss = 0.004472285974770784
iteration 232, loss = 0.0036849824246019125
iteration 233, loss = 0.004391835071146488
iteration 234, loss = 0.004181417636573315
iteration 235, loss = 0.004224514123052359
iteration 236, loss = 0.003985238261520863
iteration 237, loss = 0.004074444994330406
iteration 238, loss = 0.007265736348927021
iteration 239, loss = 0.00456580426543951
iteration 240, loss = 0.004079439677298069
iteration 241, loss = 0.004184073768556118
iteration 242, loss = 0.004212553612887859
iteration 243, loss = 0.004335982259362936
iteration 244, loss = 0.004875890910625458
iteration 245, loss = 0.004005320370197296
iteration 246, loss = 0.0044788639061152935
iteration 247, loss = 0.005255585070699453
iteration 248, loss = 0.0035685808397829533
iteration 249, loss = 0.006621892098337412
iteration 250, loss = 0.004263694863766432
iteration 251, loss = 0.003933947999030352
iteration 252, loss = 0.003946575801819563
iteration 253, loss = 0.004109703004360199
iteration 254, loss = 0.004070864524692297
iteration 255, loss = 0.004038266371935606
iteration 256, loss = 0.004692009650170803
iteration 257, loss = 0.0038828824181109667
iteration 258, loss = 0.004236950073391199
iteration 259, loss = 0.00408830214291811
iteration 260, loss = 0.0043519469909369946
iteration 261, loss = 0.004440237767994404
iteration 262, loss = 0.004918766673654318
iteration 263, loss = 0.0043553621508181095
iteration 264, loss = 0.004210265353322029
iteration 265, loss = 0.005250667221844196
iteration 266, loss = 0.005950287915766239
iteration 267, loss = 0.004404927603900433
iteration 268, loss = 0.006728779524564743
iteration 269, loss = 0.006643163040280342
iteration 270, loss = 0.00470735365524888
iteration 271, loss = 0.004296245984733105
iteration 272, loss = 0.0041507575660943985
iteration 273, loss = 0.0036530564539134502
iteration 274, loss = 0.0045898729003965855
iteration 275, loss = 0.005148651078343391
iteration 276, loss = 0.004541153088212013
iteration 277, loss = 0.004478061571717262
iteration 278, loss = 0.0038130772300064564
iteration 279, loss = 0.005822868086397648
iteration 280, loss = 0.004341364838182926
iteration 281, loss = 0.0040509975515306
iteration 282, loss = 0.006334283389151096
iteration 283, loss = 0.0068061500787734985
iteration 284, loss = 0.005706337280571461
iteration 285, loss = 0.0056065721437335014
iteration 286, loss = 0.00369109190069139
iteration 287, loss = 0.003765119705349207
iteration 288, loss = 0.0038996629882603884
iteration 289, loss = 0.004204536788165569
iteration 290, loss = 0.005695593077689409
iteration 291, loss = 0.004231301136314869
iteration 292, loss = 0.004438432399183512
iteration 293, loss = 0.004369836300611496
iteration 294, loss = 0.004135956522077322
iteration 295, loss = 0.0037451235111802816
iteration 296, loss = 0.004299779422581196
iteration 297, loss = 0.0056112948805093765
iteration 298, loss = 0.004501974210143089
iteration 299, loss = 0.003917271271348
iteration 300, loss = 0.004045193083584309

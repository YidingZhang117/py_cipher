iteration 1, loss = 1.0911611318588257
iteration 2, loss = 1.0808546543121338
iteration 3, loss = 1.0696077346801758
iteration 4, loss = 1.0661561489105225
iteration 5, loss = 1.0717566013336182
iteration 6, loss = 1.0694674253463745
iteration 7, loss = 1.0762438774108887
iteration 8, loss = 1.070008397102356
iteration 9, loss = 1.1046298742294312
iteration 10, loss = 1.0723685026168823
iteration 11, loss = 1.0804533958435059
iteration 12, loss = 1.0644054412841797
iteration 13, loss = 1.0799012184143066
iteration 14, loss = 1.0565580129623413
iteration 15, loss = 1.0635942220687866
iteration 16, loss = 1.0480542182922363
iteration 17, loss = 1.071384310722351
iteration 18, loss = 1.0693747997283936
iteration 19, loss = 1.06084406375885
iteration 20, loss = 1.0502865314483643
iteration 21, loss = 1.0492768287658691
iteration 22, loss = 1.0649819374084473
iteration 23, loss = 1.0651742219924927
iteration 24, loss = 1.0493541955947876
iteration 25, loss = 1.0828769207000732
iteration 26, loss = 1.0554229021072388
iteration 27, loss = 1.0434962511062622
iteration 28, loss = 1.0703219175338745
iteration 29, loss = 1.0476257801055908
iteration 30, loss = 1.0730599164962769
iteration 31, loss = 1.0463111400604248
iteration 32, loss = 1.0485215187072754
iteration 33, loss = 1.071597933769226
iteration 34, loss = 1.0498000383377075
iteration 35, loss = 1.07851243019104
iteration 36, loss = 1.0508829355239868
iteration 37, loss = 1.0362643003463745
iteration 38, loss = 1.0474587678909302
iteration 39, loss = 1.0407938957214355
iteration 40, loss = 1.0845484733581543
iteration 41, loss = 1.0501480102539062
iteration 42, loss = 1.0523072481155396
iteration 43, loss = 1.047607421875
iteration 44, loss = 1.0426193475723267
iteration 45, loss = 1.039010763168335
iteration 46, loss = 1.037743091583252
iteration 47, loss = 1.051188588142395
iteration 48, loss = 1.0372200012207031
iteration 49, loss = 1.040979027748108
iteration 50, loss = 1.0342628955841064
iteration 51, loss = 1.0411403179168701
iteration 52, loss = 1.0525131225585938
iteration 53, loss = 1.0414597988128662
iteration 54, loss = 1.0507735013961792
iteration 55, loss = 1.030468463897705
iteration 56, loss = 1.0415549278259277
iteration 57, loss = 1.0336174964904785
iteration 58, loss = 1.0497446060180664
iteration 59, loss = 1.0207126140594482
iteration 60, loss = 1.029784083366394
iteration 61, loss = 1.0134668350219727
iteration 62, loss = 1.0195205211639404
iteration 63, loss = 1.0241187810897827
iteration 64, loss = 1.0224727392196655
iteration 65, loss = 1.0262490510940552
iteration 66, loss = 1.0294605493545532
iteration 67, loss = 1.0224003791809082
iteration 68, loss = 1.021655559539795
iteration 69, loss = 1.0456801652908325
iteration 70, loss = 1.0174007415771484
iteration 71, loss = 1.0231796503067017
iteration 72, loss = 1.0251288414001465
iteration 73, loss = 1.036380648612976
iteration 74, loss = 1.01943838596344
iteration 75, loss = 1.0238977670669556
iteration 76, loss = 1.0161335468292236
iteration 77, loss = 1.0304352045059204
iteration 78, loss = 1.0310547351837158
iteration 79, loss = 1.0079782009124756
iteration 80, loss = 1.0026363134384155
iteration 81, loss = 1.0103445053100586
iteration 82, loss = 1.0223681926727295
iteration 83, loss = 1.0109180212020874
iteration 84, loss = 1.0002943277359009
iteration 85, loss = 1.0260051488876343
iteration 86, loss = 1.0031352043151855
iteration 87, loss = 1.0077399015426636
iteration 88, loss = 1.036674976348877
iteration 89, loss = 0.9988355040550232
iteration 90, loss = 1.0236194133758545
iteration 91, loss = 0.9986841082572937
iteration 92, loss = 1.009479284286499
iteration 93, loss = 0.9985479712486267
iteration 94, loss = 1.0082353353500366
iteration 95, loss = 1.0157239437103271
iteration 96, loss = 1.001250982284546
iteration 97, loss = 0.9876956343650818
iteration 98, loss = 1.0153634548187256
iteration 99, loss = 1.0001007318496704
iteration 100, loss = 1.0192208290100098
iteration 101, loss = 1.0161197185516357
iteration 102, loss = 1.0014539957046509
iteration 103, loss = 0.9925427436828613
iteration 104, loss = 0.9822667837142944
iteration 105, loss = 0.9933485388755798
iteration 106, loss = 1.0047742128372192
iteration 107, loss = 1.0018236637115479
iteration 108, loss = 1.0054879188537598
iteration 109, loss = 0.9943599700927734
iteration 110, loss = 0.9974413514137268
iteration 111, loss = 0.9936279058456421
iteration 112, loss = 0.9891922473907471
iteration 113, loss = 0.9888283610343933
iteration 114, loss = 0.9875164031982422
iteration 115, loss = 0.9783002138137817
iteration 116, loss = 0.9682029485702515
iteration 117, loss = 1.0027565956115723
iteration 118, loss = 0.9724915027618408
iteration 119, loss = 0.9895172119140625
iteration 120, loss = 0.9945880174636841
iteration 121, loss = 0.9820082187652588
iteration 122, loss = 0.9859554767608643
iteration 123, loss = 0.9748516082763672
iteration 124, loss = 0.9868800640106201
iteration 125, loss = 0.9900446534156799
iteration 126, loss = 0.974083423614502
iteration 127, loss = 0.9891973733901978
iteration 128, loss = 0.9928370714187622
iteration 129, loss = 0.9767380356788635
iteration 130, loss = 0.9642617106437683
iteration 131, loss = 0.9649524688720703
iteration 132, loss = 0.9765673875808716
iteration 133, loss = 0.9793370962142944
iteration 134, loss = 0.985947847366333
iteration 135, loss = 0.9725315570831299
iteration 136, loss = 0.9769744873046875
iteration 137, loss = 0.9722186923027039
iteration 138, loss = 0.9878059029579163
iteration 139, loss = 0.9773001074790955
iteration 140, loss = 0.9709217548370361
iteration 141, loss = 0.9680695533752441
iteration 142, loss = 0.9780914783477783
iteration 143, loss = 0.9816886186599731
iteration 144, loss = 0.9673339128494263
iteration 145, loss = 0.971809983253479
iteration 146, loss = 0.9744964838027954
iteration 147, loss = 0.9698047637939453
iteration 148, loss = 0.9754276275634766
iteration 149, loss = 0.9593622088432312
iteration 150, loss = 0.9610406160354614
iteration 151, loss = 0.9846962690353394
iteration 152, loss = 0.962490439414978
iteration 153, loss = 0.9630563259124756
iteration 154, loss = 0.9656312465667725
iteration 155, loss = 0.9634571671485901
iteration 156, loss = 0.9713801145553589
iteration 157, loss = 0.955098032951355
iteration 158, loss = 0.9509683847427368
iteration 159, loss = 0.9547950029373169
iteration 160, loss = 0.9649263024330139
iteration 161, loss = 0.9573725461959839
iteration 162, loss = 0.9695253372192383
iteration 163, loss = 0.9627910852432251
iteration 164, loss = 0.9622697830200195
iteration 165, loss = 0.9452588558197021
iteration 166, loss = 0.9657609462738037
iteration 167, loss = 0.9551085233688354
iteration 168, loss = 0.9582172632217407
iteration 169, loss = 0.9544765949249268
iteration 170, loss = 0.9467718601226807
iteration 171, loss = 0.9647613763809204
iteration 172, loss = 0.9488648772239685
iteration 173, loss = 0.9647720456123352
iteration 174, loss = 0.9633978605270386
iteration 175, loss = 0.9345433712005615
iteration 176, loss = 0.9484604597091675
iteration 177, loss = 0.9468798637390137
iteration 178, loss = 0.9519662261009216
iteration 179, loss = 0.9491717219352722
iteration 180, loss = 0.9362030625343323
iteration 181, loss = 0.9578402042388916
iteration 182, loss = 0.9515819549560547
iteration 183, loss = 0.933268129825592
iteration 184, loss = 0.9293155670166016
iteration 185, loss = 0.9373356103897095
iteration 186, loss = 0.9324976205825806
iteration 187, loss = 0.941561222076416
iteration 188, loss = 0.9474961757659912
iteration 189, loss = 0.9405745267868042
iteration 190, loss = 0.9552589654922485
iteration 191, loss = 0.9459517002105713
iteration 192, loss = 0.9351922273635864
iteration 193, loss = 0.9308134913444519
iteration 194, loss = 0.9429714679718018
iteration 195, loss = 0.9232053160667419
iteration 196, loss = 0.9413212537765503
iteration 197, loss = 0.9457398653030396
iteration 198, loss = 0.9278129935264587
iteration 199, loss = 0.9304091930389404
iteration 200, loss = 0.9316186904907227
iteration 201, loss = 0.9260742664337158
iteration 202, loss = 0.9075974225997925
iteration 203, loss = 0.9305123090744019
iteration 204, loss = 0.9158111810684204
iteration 205, loss = 0.9480875730514526
iteration 206, loss = 0.9249044060707092
iteration 207, loss = 0.9218391180038452
iteration 208, loss = 0.921337366104126
iteration 209, loss = 0.9413576126098633
iteration 210, loss = 0.9230308532714844
iteration 211, loss = 0.9287334680557251
iteration 212, loss = 0.9219369888305664
iteration 213, loss = 0.9435046911239624
iteration 214, loss = 0.9325098991394043
iteration 215, loss = 0.9071111083030701
iteration 216, loss = 0.9267588257789612
iteration 217, loss = 0.9182651042938232
iteration 218, loss = 0.9250061511993408
iteration 219, loss = 0.91905677318573
iteration 220, loss = 0.9182291030883789
iteration 221, loss = 0.9124592542648315
iteration 222, loss = 0.9269347190856934
iteration 223, loss = 0.9328563213348389
iteration 224, loss = 0.9181135296821594
iteration 225, loss = 0.9071825742721558
iteration 226, loss = 0.9276115894317627
iteration 227, loss = 0.9298849701881409
iteration 228, loss = 0.9121750593185425
iteration 229, loss = 0.9149686098098755
iteration 230, loss = 0.910521924495697
iteration 231, loss = 0.9179267883300781
iteration 232, loss = 0.9314391613006592
iteration 233, loss = 0.8972316980361938
iteration 234, loss = 0.9113578796386719
iteration 235, loss = 0.9114367961883545
iteration 236, loss = 0.8965412378311157
iteration 237, loss = 0.9212790131568909
iteration 238, loss = 0.8905582427978516
iteration 239, loss = 0.9039176106452942
iteration 240, loss = 0.9057643413543701
iteration 241, loss = 0.9081088900566101
iteration 242, loss = 0.8846694827079773
iteration 243, loss = 0.8828498721122742
iteration 244, loss = 0.9007372856140137
iteration 245, loss = 0.9068981409072876
iteration 246, loss = 0.9090726375579834
iteration 247, loss = 0.8933490514755249
iteration 248, loss = 0.9044665098190308
iteration 249, loss = 0.900856614112854
iteration 250, loss = 0.910077691078186
iteration 251, loss = 0.9206017851829529
iteration 252, loss = 0.9060236215591431
iteration 253, loss = 0.8950653076171875
iteration 254, loss = 0.8877899646759033
iteration 255, loss = 0.9251918196678162
iteration 256, loss = 0.9081206321716309
iteration 257, loss = 0.8944585919380188
iteration 258, loss = 0.9087343811988831
iteration 259, loss = 0.9030343294143677
iteration 260, loss = 0.9142184257507324
iteration 261, loss = 0.9113684892654419
iteration 262, loss = 0.8977552056312561
iteration 263, loss = 0.8923659324645996
iteration 264, loss = 0.88775634765625
iteration 265, loss = 0.8901747465133667
iteration 266, loss = 0.8856208920478821
iteration 267, loss = 0.8810713291168213
iteration 268, loss = 0.8875199556350708
iteration 269, loss = 0.8933345675468445
iteration 270, loss = 0.8730951547622681
iteration 271, loss = 0.8833909630775452
iteration 272, loss = 0.9177980422973633
iteration 273, loss = 0.876923680305481
iteration 274, loss = 0.8979440927505493
iteration 275, loss = 0.8970241546630859
iteration 276, loss = 0.8991153836250305
iteration 277, loss = 0.882278323173523
iteration 278, loss = 0.8928289413452148
iteration 279, loss = 0.8750472068786621
iteration 280, loss = 0.8615302443504333
iteration 281, loss = 0.8761391639709473
iteration 282, loss = 0.8856453895568848
iteration 283, loss = 0.8902832269668579
iteration 284, loss = 0.8832285404205322
iteration 285, loss = 0.8864182233810425
iteration 286, loss = 0.8785454034805298
iteration 287, loss = 0.8852015733718872
iteration 288, loss = 0.8714232444763184
iteration 289, loss = 0.8709979057312012
iteration 290, loss = 0.8664725422859192
iteration 291, loss = 0.8873074054718018
iteration 292, loss = 0.8768373727798462
iteration 293, loss = 0.9073441028594971
iteration 294, loss = 0.8844132423400879
iteration 295, loss = 0.8756353855133057
iteration 296, loss = 0.856629490852356
iteration 297, loss = 0.8542863130569458
iteration 298, loss = 0.861102819442749
iteration 299, loss = 0.8822507858276367
iteration 300, loss = 0.8583958745002747
iteration 1, loss = 0.8895996809005737
iteration 2, loss = 0.8916023969650269
iteration 3, loss = 0.8632653951644897
iteration 4, loss = 0.8783669471740723
iteration 5, loss = 0.8735740184783936
iteration 6, loss = 0.8736021518707275
iteration 7, loss = 0.8766700625419617
iteration 8, loss = 0.8607903718948364
iteration 9, loss = 0.8834227323532104
iteration 10, loss = 0.8540719747543335
iteration 11, loss = 0.87229323387146
iteration 12, loss = 0.8777021169662476
iteration 13, loss = 0.8866150379180908
iteration 14, loss = 0.8726468682289124
iteration 15, loss = 0.84618079662323
iteration 16, loss = 0.8525540232658386
iteration 17, loss = 0.8623190522193909
iteration 18, loss = 0.8653742074966431
iteration 19, loss = 0.859031081199646
iteration 20, loss = 0.8489634990692139
iteration 21, loss = 0.8668774366378784
iteration 22, loss = 0.8485310673713684
iteration 23, loss = 0.8711882829666138
iteration 24, loss = 0.8854345083236694
iteration 25, loss = 0.8460079431533813
iteration 26, loss = 0.8656967878341675
iteration 27, loss = 0.8540663719177246
iteration 28, loss = 0.8396594524383545
iteration 29, loss = 0.8629118204116821
iteration 30, loss = 0.8617508411407471
iteration 31, loss = 0.8712365627288818
iteration 32, loss = 0.8486366271972656
iteration 33, loss = 0.8565647602081299
iteration 34, loss = 0.8418712615966797
iteration 35, loss = 0.8542609214782715
iteration 36, loss = 0.8500200510025024
iteration 37, loss = 0.8575468063354492
iteration 38, loss = 0.8459522724151611
iteration 39, loss = 0.8508754968643188
iteration 40, loss = 0.862326979637146
iteration 41, loss = 0.8688340187072754
iteration 42, loss = 0.8726034760475159
iteration 43, loss = 0.8466578722000122
iteration 44, loss = 0.8363746404647827
iteration 45, loss = 0.8635793328285217
iteration 46, loss = 0.8392802476882935
iteration 47, loss = 0.8544766902923584
iteration 48, loss = 0.833453893661499
iteration 49, loss = 0.8309279680252075
iteration 50, loss = 0.8387159109115601
iteration 51, loss = 0.8346990942955017
iteration 52, loss = 0.8586578369140625
iteration 53, loss = 0.865970253944397
iteration 54, loss = 0.8311827182769775
iteration 55, loss = 0.8484811186790466
iteration 56, loss = 0.8219051361083984
iteration 57, loss = 0.8160161972045898
iteration 58, loss = 0.8439940214157104
iteration 59, loss = 0.8365175724029541
iteration 60, loss = 0.8505504131317139
iteration 61, loss = 0.8442471027374268
iteration 62, loss = 0.8317304849624634
iteration 63, loss = 0.8398125767707825
iteration 64, loss = 0.8339890241622925
iteration 65, loss = 0.8326228260993958
iteration 66, loss = 0.8513000011444092
iteration 67, loss = 0.8344013690948486
iteration 68, loss = 0.8413516283035278
iteration 69, loss = 0.8339159488677979
iteration 70, loss = 0.8355987668037415
iteration 71, loss = 0.8418517112731934
iteration 72, loss = 0.8320934176445007
iteration 73, loss = 0.8293261528015137
iteration 74, loss = 0.8253885507583618
iteration 75, loss = 0.8549360632896423
iteration 76, loss = 0.8301607370376587
iteration 77, loss = 0.8540160059928894
iteration 78, loss = 0.823999285697937
iteration 79, loss = 0.8384159803390503
iteration 80, loss = 0.8329908847808838
iteration 81, loss = 0.8383960127830505
iteration 82, loss = 0.822060227394104
iteration 83, loss = 0.8665923476219177
iteration 84, loss = 0.8330196142196655
iteration 85, loss = 0.8194806575775146
iteration 86, loss = 0.8176735043525696
iteration 87, loss = 0.8182555437088013
iteration 88, loss = 0.8262374401092529
iteration 89, loss = 0.8300229907035828
iteration 90, loss = 0.8405596017837524
iteration 91, loss = 0.856490969657898
iteration 92, loss = 0.8510632514953613
iteration 93, loss = 0.8407566547393799
iteration 94, loss = 0.8231520056724548
iteration 95, loss = 0.8031471967697144
iteration 96, loss = 0.8269265294075012
iteration 97, loss = 0.8165165185928345
iteration 98, loss = 0.833590030670166
iteration 99, loss = 0.8326303958892822
iteration 100, loss = 0.8034586906433105
iteration 101, loss = 0.8325984477996826
iteration 102, loss = 0.8419952988624573
iteration 103, loss = 0.821724534034729
iteration 104, loss = 0.815410852432251
iteration 105, loss = 0.8333436250686646
iteration 106, loss = 0.8139365911483765
iteration 107, loss = 0.8141972422599792
iteration 108, loss = 0.8012641072273254
iteration 109, loss = 0.8055540323257446
iteration 110, loss = 0.8058738708496094
iteration 111, loss = 0.8092373609542847
iteration 112, loss = 0.8221948146820068
iteration 113, loss = 0.8211007118225098
iteration 114, loss = 0.8151096701622009
iteration 115, loss = 0.7994728088378906
iteration 116, loss = 0.8281798362731934
iteration 117, loss = 0.8029358386993408
iteration 118, loss = 0.8227103352546692
iteration 119, loss = 0.8019453287124634
iteration 120, loss = 0.8331102132797241
iteration 121, loss = 0.8045922517776489
iteration 122, loss = 0.7976621389389038
iteration 123, loss = 0.7953860759735107
iteration 124, loss = 0.8047657012939453
iteration 125, loss = 0.8118250370025635
iteration 126, loss = 0.8125925064086914
iteration 127, loss = 0.7986122369766235
iteration 128, loss = 0.8109176158905029
iteration 129, loss = 0.8287070989608765
iteration 130, loss = 0.8150545954704285
iteration 131, loss = 0.7920770049095154
iteration 132, loss = 0.803634762763977
iteration 133, loss = 0.8080694079399109
iteration 134, loss = 0.8134784698486328
iteration 135, loss = 0.7937690019607544
iteration 136, loss = 0.8127377033233643
iteration 137, loss = 0.7918397784233093
iteration 138, loss = 0.7942449450492859
iteration 139, loss = 0.7957998514175415
iteration 140, loss = 0.8318650722503662
iteration 141, loss = 0.7959365844726562
iteration 142, loss = 0.8261277079582214
iteration 143, loss = 0.7867670059204102
iteration 144, loss = 0.7853215336799622
iteration 145, loss = 0.7991313934326172
iteration 146, loss = 0.800330638885498
iteration 147, loss = 0.8152964115142822
iteration 148, loss = 0.807427704334259
iteration 149, loss = 0.8298788070678711
iteration 150, loss = 0.7857508659362793
iteration 151, loss = 0.792242705821991
iteration 152, loss = 0.8007935881614685
iteration 153, loss = 0.822208821773529
iteration 154, loss = 0.7936161756515503
iteration 155, loss = 0.7727604508399963
iteration 156, loss = 0.787886381149292
iteration 157, loss = 0.7904425263404846
iteration 158, loss = 0.8075289726257324
iteration 159, loss = 0.7834369540214539
iteration 160, loss = 0.8491944074630737
iteration 161, loss = 0.7754217982292175
iteration 162, loss = 0.7984354496002197
iteration 163, loss = 0.793804407119751
iteration 164, loss = 0.7891089916229248
iteration 165, loss = 0.7938592433929443
iteration 166, loss = 0.7992989420890808
iteration 167, loss = 0.7759692072868347
iteration 168, loss = 0.7816335558891296
iteration 169, loss = 0.7895725965499878
iteration 170, loss = 0.8059378266334534
iteration 171, loss = 0.7835764288902283
iteration 172, loss = 0.8088747262954712
iteration 173, loss = 0.7861456871032715
iteration 174, loss = 0.7815585732460022
iteration 175, loss = 0.8070846796035767
iteration 176, loss = 0.7939592003822327
iteration 177, loss = 0.8027125597000122
iteration 178, loss = 0.7937174439430237
iteration 179, loss = 0.8003777265548706
iteration 180, loss = 0.8198257684707642
iteration 181, loss = 0.7925719022750854
iteration 182, loss = 0.7978217601776123
iteration 183, loss = 0.7917249202728271
iteration 184, loss = 0.7846090793609619
iteration 185, loss = 0.7824521064758301
iteration 186, loss = 0.7805756330490112
iteration 187, loss = 0.8082517981529236
iteration 188, loss = 0.78827965259552
iteration 189, loss = 0.7830651998519897
iteration 190, loss = 0.7960291504859924
iteration 191, loss = 0.8314714431762695
iteration 192, loss = 0.7872499823570251
iteration 193, loss = 0.7776393294334412
iteration 194, loss = 0.7624822854995728
iteration 195, loss = 0.7564446926116943
iteration 196, loss = 0.8028528690338135
iteration 197, loss = 0.7653049826622009
iteration 198, loss = 0.7731080055236816
iteration 199, loss = 0.7738596796989441
iteration 200, loss = 0.7843016386032104
iteration 201, loss = 0.7682268023490906
iteration 202, loss = 0.7942129373550415
iteration 203, loss = 0.7634385228157043
iteration 204, loss = 0.7910564541816711
iteration 205, loss = 0.7786734104156494
iteration 206, loss = 0.7770615816116333
iteration 207, loss = 0.775334894657135
iteration 208, loss = 0.7881834506988525
iteration 209, loss = 0.7741836309432983
iteration 210, loss = 0.7688848376274109
iteration 211, loss = 0.7843987941741943
iteration 212, loss = 0.7688465714454651
iteration 213, loss = 0.8326830863952637
iteration 214, loss = 0.7825707793235779
iteration 215, loss = 0.7529833316802979
iteration 216, loss = 0.7959873080253601
iteration 217, loss = 0.7729171514511108
iteration 218, loss = 0.7527574300765991
iteration 219, loss = 0.7696390151977539
iteration 220, loss = 0.7806023955345154
iteration 221, loss = 0.789847195148468
iteration 222, loss = 0.797348141670227
iteration 223, loss = 0.7620601654052734
iteration 224, loss = 0.7731481790542603
iteration 225, loss = 0.7970103025436401
iteration 226, loss = 0.7638775110244751
iteration 227, loss = 0.7567325830459595
iteration 228, loss = 0.760873556137085
iteration 229, loss = 0.7839310169219971
iteration 230, loss = 0.7838833928108215
iteration 231, loss = 0.756157636642456
iteration 232, loss = 0.7649152278900146
iteration 233, loss = 0.8259490132331848
iteration 234, loss = 0.7666879892349243
iteration 235, loss = 0.7616065144538879
iteration 236, loss = 0.7451942563056946
iteration 237, loss = 0.7706297636032104
iteration 238, loss = 0.7707761526107788
iteration 239, loss = 0.767296552658081
iteration 240, loss = 0.791234016418457
iteration 241, loss = 0.763857364654541
iteration 242, loss = 0.7692501544952393
iteration 243, loss = 0.7628453969955444
iteration 244, loss = 0.7884480953216553
iteration 245, loss = 0.7676271200180054
iteration 246, loss = 0.7620115280151367
iteration 247, loss = 0.7448172569274902
iteration 248, loss = 0.7584328651428223
iteration 249, loss = 0.7652665376663208
iteration 250, loss = 0.7611634731292725
iteration 251, loss = 0.8107612133026123
iteration 252, loss = 0.7750999331474304
iteration 253, loss = 0.7626892924308777
iteration 254, loss = 0.7615342736244202
iteration 255, loss = 0.7618584632873535
iteration 256, loss = 0.7649363279342651
iteration 257, loss = 0.774855375289917
iteration 258, loss = 0.7521146535873413
iteration 259, loss = 0.7382088899612427
iteration 260, loss = 0.7360279560089111
iteration 261, loss = 0.7594223022460938
iteration 262, loss = 0.7553305625915527
iteration 263, loss = 0.7353368997573853
iteration 264, loss = 0.7500779032707214
iteration 265, loss = 0.7305195331573486
iteration 266, loss = 0.7554370164871216
iteration 267, loss = 0.7631831169128418
iteration 268, loss = 0.7444791793823242
iteration 269, loss = 0.723279595375061
iteration 270, loss = 0.7238320112228394
iteration 271, loss = 0.7726365327835083
iteration 272, loss = 0.7454055547714233
iteration 273, loss = 0.7355943918228149
iteration 274, loss = 0.734124481678009
iteration 275, loss = 0.7567760944366455
iteration 276, loss = 0.720131516456604
iteration 277, loss = 0.734381914138794
iteration 278, loss = 0.7585481405258179
iteration 279, loss = 0.7330765128135681
iteration 280, loss = 0.7584774494171143
iteration 281, loss = 0.7374337315559387
iteration 282, loss = 0.7718039751052856
iteration 283, loss = 0.7645214796066284
iteration 284, loss = 0.7428497076034546
iteration 285, loss = 0.7782277464866638
iteration 286, loss = 0.756201982498169
iteration 287, loss = 0.742949903011322
iteration 288, loss = 0.7630189657211304
iteration 289, loss = 0.7643547058105469
iteration 290, loss = 0.7694079279899597
iteration 291, loss = 0.748272716999054
iteration 292, loss = 0.7297837138175964
iteration 293, loss = 0.714553713798523
iteration 294, loss = 0.7629331350326538
iteration 295, loss = 0.7880820035934448
iteration 296, loss = 0.772955060005188
iteration 297, loss = 0.7398284673690796
iteration 298, loss = 0.7279458045959473
iteration 299, loss = 0.7327962517738342
iteration 300, loss = 0.7393600940704346
iteration 1, loss = 0.7489176988601685
iteration 2, loss = 0.7474325895309448
iteration 3, loss = 0.7366944551467896
iteration 4, loss = 0.745175838470459
iteration 5, loss = 0.7295103073120117
iteration 6, loss = 0.7610111236572266
iteration 7, loss = 0.7477598190307617
iteration 8, loss = 0.7179756164550781
iteration 9, loss = 0.74686199426651
iteration 10, loss = 0.7532947063446045
iteration 11, loss = 0.7314862012863159
iteration 12, loss = 0.7226897478103638
iteration 13, loss = 0.7527426481246948
iteration 14, loss = 0.7252711057662964
iteration 15, loss = 0.7367798089981079
iteration 16, loss = 0.715424656867981
iteration 17, loss = 0.7230744361877441
iteration 18, loss = 0.7310441732406616
iteration 19, loss = 0.7156262993812561
iteration 20, loss = 0.7330166101455688
iteration 21, loss = 0.7455021142959595
iteration 22, loss = 0.7503297924995422
iteration 23, loss = 0.7146218419075012
iteration 24, loss = 0.7204883098602295
iteration 25, loss = 0.7038548588752747
iteration 26, loss = 0.7233589291572571
iteration 27, loss = 0.7329791784286499
iteration 28, loss = 0.7364227771759033
iteration 29, loss = 0.7872580885887146
iteration 30, loss = 0.7703323364257812
iteration 31, loss = 0.758181095123291
iteration 32, loss = 0.7206118106842041
iteration 33, loss = 0.7170774936676025
iteration 34, loss = 0.7530591487884521
iteration 35, loss = 0.7335195541381836
iteration 36, loss = 0.7423990368843079
iteration 37, loss = 0.725326657295227
iteration 38, loss = 0.734207808971405
iteration 39, loss = 0.7365056276321411
iteration 40, loss = 0.7627278566360474
iteration 41, loss = 0.7390713095664978
iteration 42, loss = 0.7567777037620544
iteration 43, loss = 0.7299129366874695
iteration 44, loss = 0.7241883277893066
iteration 45, loss = 0.7169185876846313
iteration 46, loss = 0.7091733813285828
iteration 47, loss = 0.7127338647842407
iteration 48, loss = 0.689346969127655
iteration 49, loss = 0.7478005886077881
iteration 50, loss = 0.7256581783294678
iteration 51, loss = 0.7085238099098206
iteration 52, loss = 0.764955997467041
iteration 53, loss = 0.7653249502182007
iteration 54, loss = 0.727587878704071
iteration 55, loss = 0.7381082773208618
iteration 56, loss = 0.7115098237991333
iteration 57, loss = 0.7783089876174927
iteration 58, loss = 0.7371745705604553
iteration 59, loss = 0.6973443031311035
iteration 60, loss = 0.7160965204238892
iteration 61, loss = 0.7418079376220703
iteration 62, loss = 0.73384690284729
iteration 63, loss = 0.7033087015151978
iteration 64, loss = 0.7609329223632812
iteration 65, loss = 0.7315369844436646
iteration 66, loss = 0.7254064083099365
iteration 67, loss = 0.7115434408187866
iteration 68, loss = 0.7570192813873291
iteration 69, loss = 0.7403255701065063
iteration 70, loss = 0.7417610287666321
iteration 71, loss = 0.7483076453208923
iteration 72, loss = 0.7252719402313232
iteration 73, loss = 0.7286133170127869
iteration 74, loss = 0.7177044153213501
iteration 75, loss = 0.7028220295906067
iteration 76, loss = 0.6939108371734619
iteration 77, loss = 0.7608277201652527
iteration 78, loss = 0.7123107314109802
iteration 79, loss = 0.7020643949508667
iteration 80, loss = 0.7130414247512817
iteration 81, loss = 0.6946670413017273
iteration 82, loss = 0.6903076171875
iteration 83, loss = 0.7294149398803711
iteration 84, loss = 0.6983344554901123
iteration 85, loss = 0.756055474281311
iteration 86, loss = 0.7064259052276611
iteration 87, loss = 0.6985431909561157
iteration 88, loss = 0.6963223218917847
iteration 89, loss = 0.7050586938858032
iteration 90, loss = 0.730004608631134
iteration 91, loss = 0.6849339008331299
iteration 92, loss = 0.7091059684753418
iteration 93, loss = 0.7288244366645813
iteration 94, loss = 0.7045290470123291
iteration 95, loss = 0.703771710395813
iteration 96, loss = 0.7177284955978394
iteration 97, loss = 0.7124565839767456
iteration 98, loss = 0.697342038154602
iteration 99, loss = 0.6999565362930298
iteration 100, loss = 0.7014456987380981
iteration 101, loss = 0.6893254518508911
iteration 102, loss = 0.764743447303772
iteration 103, loss = 0.7011533379554749
iteration 104, loss = 0.7333928942680359
iteration 105, loss = 0.6941291093826294
iteration 106, loss = 0.7421221733093262
iteration 107, loss = 0.6879137754440308
iteration 108, loss = 0.7013115882873535
iteration 109, loss = 0.7307616472244263
iteration 110, loss = 0.7888997793197632
iteration 111, loss = 0.7347066402435303
iteration 112, loss = 0.6933531165122986
iteration 113, loss = 0.709450900554657
iteration 114, loss = 0.738237202167511
iteration 115, loss = 0.7017874121665955
iteration 116, loss = 0.7136099338531494
iteration 117, loss = 0.6893644332885742
iteration 118, loss = 0.704696774482727
iteration 119, loss = 0.7015857696533203
iteration 120, loss = 0.691830039024353
iteration 121, loss = 0.6949206590652466
iteration 122, loss = 0.6887577772140503
iteration 123, loss = 0.6782269477844238
iteration 124, loss = 0.6891515254974365
iteration 125, loss = 0.7422641515731812
iteration 126, loss = 0.6848886013031006
iteration 127, loss = 0.6975304484367371
iteration 128, loss = 0.7023069858551025
iteration 129, loss = 0.6928690075874329
iteration 130, loss = 0.6804126501083374
iteration 131, loss = 0.7179539203643799
iteration 132, loss = 0.7324612140655518
iteration 133, loss = 0.7207098007202148
iteration 134, loss = 0.6841931343078613
iteration 135, loss = 0.7251098155975342
iteration 136, loss = 0.6969380378723145
iteration 137, loss = 0.7098949551582336
iteration 138, loss = 0.6954660415649414
iteration 139, loss = 0.6960539221763611
iteration 140, loss = 0.6811026930809021
iteration 141, loss = 0.6933164000511169
iteration 142, loss = 0.7208300828933716
iteration 143, loss = 0.711787760257721
iteration 144, loss = 0.6795059442520142
iteration 145, loss = 0.6899203062057495
iteration 146, loss = 0.7179822325706482
iteration 147, loss = 0.6935561895370483
iteration 148, loss = 0.6699845790863037
iteration 149, loss = 0.6895670890808105
iteration 150, loss = 0.7132754921913147
iteration 151, loss = 0.7092224359512329
iteration 152, loss = 0.6783970594406128
iteration 153, loss = 0.6707343459129333
iteration 154, loss = 0.694995105266571
iteration 155, loss = 0.6936565637588501
iteration 156, loss = 0.6888880729675293
iteration 157, loss = 0.7155153155326843
iteration 158, loss = 0.7318582534790039
iteration 159, loss = 0.6783109307289124
iteration 160, loss = 0.7598843574523926
iteration 161, loss = 0.7051803469657898
iteration 162, loss = 0.7574038505554199
iteration 163, loss = 0.6882632970809937
iteration 164, loss = 0.6936192512512207
iteration 165, loss = 0.6751714944839478
iteration 166, loss = 0.6891294121742249
iteration 167, loss = 0.7481749653816223
iteration 168, loss = 0.6859058141708374
iteration 169, loss = 0.6735138893127441
iteration 170, loss = 0.6859359741210938
iteration 171, loss = 0.6970008611679077
iteration 172, loss = 0.7059148550033569
iteration 173, loss = 0.6846410036087036
iteration 174, loss = 0.6803064942359924
iteration 175, loss = 0.6742257475852966
iteration 176, loss = 0.7106919288635254
iteration 177, loss = 0.709128737449646
iteration 178, loss = 0.6962142586708069
iteration 179, loss = 0.7261644005775452
iteration 180, loss = 0.7014697194099426
iteration 181, loss = 0.686406135559082
iteration 182, loss = 0.7065812349319458
iteration 183, loss = 0.6783890724182129
iteration 184, loss = 0.672642707824707
iteration 185, loss = 0.6837518215179443
iteration 186, loss = 0.6716128587722778
iteration 187, loss = 0.6644304990768433
iteration 188, loss = 0.6634902954101562
iteration 189, loss = 0.7224524021148682
iteration 190, loss = 0.6882282495498657
iteration 191, loss = 0.6615604758262634
iteration 192, loss = 0.665218710899353
iteration 193, loss = 0.689845085144043
iteration 194, loss = 0.7085590362548828
iteration 195, loss = 0.7043489217758179
iteration 196, loss = 0.6676037907600403
iteration 197, loss = 0.6971902847290039
iteration 198, loss = 0.668493390083313
iteration 199, loss = 0.7525149583816528
iteration 200, loss = 0.7041574716567993
iteration 201, loss = 0.6538326740264893
iteration 202, loss = 0.7455902099609375
iteration 203, loss = 0.6931717991828918
iteration 204, loss = 0.7090305685997009
iteration 205, loss = 0.6782148480415344
iteration 206, loss = 0.7217183709144592
iteration 207, loss = 0.6735272407531738
iteration 208, loss = 0.6849842071533203
iteration 209, loss = 0.661429762840271
iteration 210, loss = 0.6745680570602417
iteration 211, loss = 0.678272008895874
iteration 212, loss = 0.6786855459213257
iteration 213, loss = 0.6954948306083679
iteration 214, loss = 0.6837453842163086
iteration 215, loss = 0.6800376772880554
iteration 216, loss = 0.6645189523696899
iteration 217, loss = 0.6573461294174194
iteration 218, loss = 0.6924156546592712
iteration 219, loss = 0.6832939982414246
iteration 220, loss = 0.6886451840400696
iteration 221, loss = 0.6640064120292664
iteration 222, loss = 0.6721340417861938
iteration 223, loss = 0.6825385689735413
iteration 224, loss = 0.6751425266265869
iteration 225, loss = 0.6947721242904663
iteration 226, loss = 0.6686395406723022
iteration 227, loss = 0.6737522482872009
iteration 228, loss = 0.6826516389846802
iteration 229, loss = 0.6860511898994446
iteration 230, loss = 0.673488974571228
iteration 231, loss = 0.7049005031585693
iteration 232, loss = 0.648730993270874
iteration 233, loss = 0.7842316627502441
iteration 234, loss = 0.6391919255256653
iteration 235, loss = 0.6785944700241089
iteration 236, loss = 0.7261344194412231
iteration 237, loss = 0.6803978681564331
iteration 238, loss = 0.6448715925216675
iteration 239, loss = 0.6740728616714478
iteration 240, loss = 0.6702899932861328
iteration 241, loss = 0.6629682779312134
iteration 242, loss = 0.7057280540466309
iteration 243, loss = 0.6867733001708984
iteration 244, loss = 0.6910649538040161
iteration 245, loss = 0.7027215957641602
iteration 246, loss = 0.6674179434776306
iteration 247, loss = 0.6774649620056152
iteration 248, loss = 0.6640660166740417
iteration 249, loss = 0.6709528565406799
iteration 250, loss = 0.6414644718170166
iteration 251, loss = 0.6504254341125488
iteration 252, loss = 0.66839599609375
iteration 253, loss = 0.6888318061828613
iteration 254, loss = 0.6478227376937866
iteration 255, loss = 0.6608448028564453
iteration 256, loss = 0.675728440284729
iteration 257, loss = 0.657505989074707
iteration 258, loss = 0.6819145679473877
iteration 259, loss = 0.6521841287612915
iteration 260, loss = 0.6439425945281982
iteration 261, loss = 0.6644390821456909
iteration 262, loss = 0.6596028804779053
iteration 263, loss = 0.6510316133499146
iteration 264, loss = 0.681617259979248
iteration 265, loss = 0.6836177706718445
iteration 266, loss = 0.6577715873718262
iteration 267, loss = 0.6295807361602783
iteration 268, loss = 0.6730787754058838
iteration 269, loss = 0.6887844800949097
iteration 270, loss = 0.6516920328140259
iteration 271, loss = 0.6396548748016357
iteration 272, loss = 0.663429856300354
iteration 273, loss = 0.6916482448577881
iteration 274, loss = 0.6492964029312134
iteration 275, loss = 0.6494320034980774
iteration 276, loss = 0.7027567625045776
iteration 277, loss = 0.6860111951828003
iteration 278, loss = 0.6424793004989624
iteration 279, loss = 0.674676775932312
iteration 280, loss = 0.7116773724555969
iteration 281, loss = 0.6461690664291382
iteration 282, loss = 0.663091242313385
iteration 283, loss = 0.6264617443084717
iteration 284, loss = 0.7083166241645813
iteration 285, loss = 0.6623395681381226
iteration 286, loss = 0.6580376625061035
iteration 287, loss = 0.6401376128196716
iteration 288, loss = 0.6637393236160278
iteration 289, loss = 0.6593264937400818
iteration 290, loss = 0.6731887459754944
iteration 291, loss = 0.6438946723937988
iteration 292, loss = 0.6454923152923584
iteration 293, loss = 0.6979695558547974
iteration 294, loss = 0.6655179262161255
iteration 295, loss = 0.6956382393836975
iteration 296, loss = 0.7182807922363281
iteration 297, loss = 0.6538491249084473
iteration 298, loss = 0.6408659219741821
iteration 299, loss = 0.6449304819107056
iteration 300, loss = 0.6584135293960571
iteration 1, loss = 0.6464755535125732
iteration 2, loss = 0.6625534892082214
iteration 3, loss = 0.6282907724380493
iteration 4, loss = 0.6862351894378662
iteration 5, loss = 0.6714392304420471
iteration 6, loss = 0.6631821393966675
iteration 7, loss = 0.6654279232025146
iteration 8, loss = 0.6425853967666626
iteration 9, loss = 0.6476902961730957
iteration 10, loss = 0.658453106880188
iteration 11, loss = 0.6540883779525757
iteration 12, loss = 0.6758100986480713
iteration 13, loss = 0.6380247473716736
iteration 14, loss = 0.6339930295944214
iteration 15, loss = 0.6867494583129883
iteration 16, loss = 0.7077324390411377
iteration 17, loss = 0.6572357416152954
iteration 18, loss = 0.6260478496551514
iteration 19, loss = 0.6336226463317871
iteration 20, loss = 0.7099839448928833
iteration 21, loss = 0.6443989276885986
iteration 22, loss = 0.6684941053390503
iteration 23, loss = 0.6232892870903015
iteration 24, loss = 0.6774144172668457
iteration 25, loss = 0.6576379537582397
iteration 26, loss = 0.6428653597831726
iteration 27, loss = 0.6326358318328857
iteration 28, loss = 0.6722029447555542
iteration 29, loss = 0.6410666108131409
iteration 30, loss = 0.6681896448135376
iteration 31, loss = 0.6824510097503662
iteration 32, loss = 0.6573594808578491
iteration 33, loss = 0.6440455317497253
iteration 34, loss = 0.6444636583328247
iteration 35, loss = 0.7190600633621216
iteration 36, loss = 0.6808894872665405
iteration 37, loss = 0.6989163160324097
iteration 38, loss = 0.6473766565322876
iteration 39, loss = 0.6929341554641724
iteration 40, loss = 0.6447505950927734
iteration 41, loss = 0.648333728313446
iteration 42, loss = 0.7408082485198975
iteration 43, loss = 0.6549319624900818
iteration 44, loss = 0.6101589798927307
iteration 45, loss = 0.6643991470336914
iteration 46, loss = 0.6798392534255981
iteration 47, loss = 0.6313464641571045
iteration 48, loss = 0.666176438331604
iteration 49, loss = 0.6322662830352783
iteration 50, loss = 0.6687958836555481
iteration 51, loss = 0.6563431024551392
iteration 52, loss = 0.6366487145423889
iteration 53, loss = 0.6216439008712769
iteration 54, loss = 0.6455411911010742
iteration 55, loss = 0.6162354946136475
iteration 56, loss = 0.6451090574264526
iteration 57, loss = 0.6433326005935669
iteration 58, loss = 0.6289687156677246
iteration 59, loss = 0.6259502172470093
iteration 60, loss = 0.6390774250030518
iteration 61, loss = 0.6071926355361938
iteration 62, loss = 0.6713494062423706
iteration 63, loss = 0.6613484621047974
iteration 64, loss = 0.657734751701355
iteration 65, loss = 0.6688348054885864
iteration 66, loss = 0.6922156810760498
iteration 67, loss = 0.6412224769592285
iteration 68, loss = 0.62259441614151
iteration 69, loss = 0.6381646990776062
iteration 70, loss = 0.6747663021087646
iteration 71, loss = 0.6237503886222839
iteration 72, loss = 0.6211352348327637
iteration 73, loss = 0.6238046288490295
iteration 74, loss = 0.6275742053985596
iteration 75, loss = 0.6225770115852356
iteration 76, loss = 0.6591765880584717
iteration 77, loss = 0.663646936416626
iteration 78, loss = 0.6307413578033447
iteration 79, loss = 0.6184862852096558
iteration 80, loss = 0.6480547785758972
iteration 81, loss = 0.6494185924530029
iteration 82, loss = 0.6107091307640076
iteration 83, loss = 0.6370505094528198
iteration 84, loss = 0.6555757522583008
iteration 85, loss = 0.6648977994918823
iteration 86, loss = 0.6195926666259766
iteration 87, loss = 0.6777650117874146
iteration 88, loss = 0.6306416988372803
iteration 89, loss = 0.6312785148620605
iteration 90, loss = 0.6356782913208008
iteration 91, loss = 0.7083975076675415
iteration 92, loss = 0.6594506502151489
iteration 93, loss = 0.652187705039978
iteration 94, loss = 0.7033518552780151
iteration 95, loss = 0.6682134866714478
iteration 96, loss = 0.6263154745101929
iteration 97, loss = 0.6368553042411804
iteration 98, loss = 0.6281850337982178
iteration 99, loss = 0.6203458309173584
iteration 100, loss = 0.6467655897140503
iteration 101, loss = 0.6213245391845703
iteration 102, loss = 0.6339890956878662
iteration 103, loss = 0.647109866142273
iteration 104, loss = 0.6308242082595825
iteration 105, loss = 0.6479959487915039
iteration 106, loss = 0.6765458583831787
iteration 107, loss = 0.7005366086959839
iteration 108, loss = 0.706180214881897
iteration 109, loss = 0.6339348554611206
iteration 110, loss = 0.6222284436225891
iteration 111, loss = 0.6391366720199585
iteration 112, loss = 0.6261298656463623
iteration 113, loss = 0.628299355506897
iteration 114, loss = 0.6110092401504517
iteration 115, loss = 0.5921927690505981
iteration 116, loss = 0.6776493787765503
iteration 117, loss = 0.6620102524757385
iteration 118, loss = 0.6615104079246521
iteration 119, loss = 0.6379397511482239
iteration 120, loss = 0.6635969877243042
iteration 121, loss = 0.6514714360237122
iteration 122, loss = 0.6069036722183228
iteration 123, loss = 0.6272681951522827
iteration 124, loss = 0.592949390411377
iteration 125, loss = 0.6370384693145752
iteration 126, loss = 0.6305665969848633
iteration 127, loss = 0.6709606051445007
iteration 128, loss = 0.6269553303718567
iteration 129, loss = 0.617709219455719
iteration 130, loss = 0.7468966245651245
iteration 131, loss = 0.5840812921524048
iteration 132, loss = 0.6277607083320618
iteration 133, loss = 0.664368748664856
iteration 134, loss = 0.641632080078125
iteration 135, loss = 0.6431455612182617
iteration 136, loss = 0.6051145792007446
iteration 137, loss = 0.6421234607696533
iteration 138, loss = 0.6305654048919678
iteration 139, loss = 0.6429741382598877
iteration 140, loss = 0.6132600903511047
iteration 141, loss = 0.5997711420059204
iteration 142, loss = 0.612348198890686
iteration 143, loss = 0.6289600133895874
iteration 144, loss = 0.6632335782051086
iteration 145, loss = 0.6251848340034485
iteration 146, loss = 0.6136623620986938
iteration 147, loss = 0.6103004813194275
iteration 148, loss = 0.6606407165527344
iteration 149, loss = 0.6144890785217285
iteration 150, loss = 0.6272438764572144
iteration 151, loss = 0.6319542527198792
iteration 152, loss = 0.6250386238098145
iteration 153, loss = 0.6666117906570435
iteration 154, loss = 0.7263742685317993
iteration 155, loss = 0.6542266607284546
iteration 156, loss = 0.6616147756576538
iteration 157, loss = 0.6458780169487
iteration 158, loss = 0.6270251870155334
iteration 159, loss = 0.6221798062324524
iteration 160, loss = 0.6160404682159424
iteration 161, loss = 0.6350005865097046
iteration 162, loss = 0.632167637348175
iteration 163, loss = 0.6183292865753174
iteration 164, loss = 0.6187138557434082
iteration 165, loss = 0.5946416854858398
iteration 166, loss = 0.5938832759857178
iteration 167, loss = 0.6027751564979553
iteration 168, loss = 0.6755048036575317
iteration 169, loss = 0.6216763854026794
iteration 170, loss = 0.6237776875495911
iteration 171, loss = 0.5927866697311401
iteration 172, loss = 0.6514213681221008
iteration 173, loss = 0.6039596796035767
iteration 174, loss = 0.6087646484375
iteration 175, loss = 0.5926372408866882
iteration 176, loss = 0.6518757939338684
iteration 177, loss = 0.6236811876296997
iteration 178, loss = 0.6126983165740967
iteration 179, loss = 0.6336894631385803
iteration 180, loss = 0.6413100957870483
iteration 181, loss = 0.6145235300064087
iteration 182, loss = 0.6298108696937561
iteration 183, loss = 0.6458559036254883
iteration 184, loss = 0.6055657863616943
iteration 185, loss = 0.6327842473983765
iteration 186, loss = 0.6181310415267944
iteration 187, loss = 0.6095535159111023
iteration 188, loss = 0.6450295448303223
iteration 189, loss = 0.6336600184440613
iteration 190, loss = 0.6287145614624023
iteration 191, loss = 0.6090703010559082
iteration 192, loss = 0.6074686646461487
iteration 193, loss = 0.5898608565330505
iteration 194, loss = 0.6238819360733032
iteration 195, loss = 0.6095744967460632
iteration 196, loss = 0.6964463591575623
iteration 197, loss = 0.5978268384933472
iteration 198, loss = 0.6399868726730347
iteration 199, loss = 0.6179938316345215
iteration 200, loss = 0.6131889820098877
iteration 201, loss = 0.6380449533462524
iteration 202, loss = 0.586050808429718
iteration 203, loss = 0.6239538192749023
iteration 204, loss = 0.591156005859375
iteration 205, loss = 0.6263841986656189
iteration 206, loss = 0.6269519925117493
iteration 207, loss = 0.6234101057052612
iteration 208, loss = 0.6448379755020142
iteration 209, loss = 0.5935156941413879
iteration 210, loss = 0.6540354490280151
iteration 211, loss = 0.6158575415611267
iteration 212, loss = 0.6046400666236877
iteration 213, loss = 0.5906829237937927
iteration 214, loss = 0.6367183923721313
iteration 215, loss = 0.6327600479125977
iteration 216, loss = 0.6378734111785889
iteration 217, loss = 0.6484678983688354
iteration 218, loss = 0.6130566000938416
iteration 219, loss = 0.6132505536079407
iteration 220, loss = 0.6225647330284119
iteration 221, loss = 0.6411129832267761
iteration 222, loss = 0.6010832190513611
iteration 223, loss = 0.6000093221664429
iteration 224, loss = 0.6196798086166382
iteration 225, loss = 0.5783018469810486
iteration 226, loss = 0.5893076062202454
iteration 227, loss = 0.6674177646636963
iteration 228, loss = 0.618822455406189
iteration 229, loss = 0.6226609349250793
iteration 230, loss = 0.6046638488769531
iteration 231, loss = 0.6343749761581421
iteration 232, loss = 0.5890225768089294
iteration 233, loss = 0.665630578994751
iteration 234, loss = 0.611267626285553
iteration 235, loss = 0.6025769114494324
iteration 236, loss = 0.5975242853164673
iteration 237, loss = 0.5681811571121216
iteration 238, loss = 0.6255420446395874
iteration 239, loss = 0.7035719156265259
iteration 240, loss = 0.5755841732025146
iteration 241, loss = 0.6500493884086609
iteration 242, loss = 0.6110939383506775
iteration 243, loss = 0.6208987236022949
iteration 244, loss = 0.6136033535003662
iteration 245, loss = 0.6908713579177856
iteration 246, loss = 0.6279345154762268
iteration 247, loss = 0.6200832724571228
iteration 248, loss = 0.6275792121887207
iteration 249, loss = 0.6077996492385864
iteration 250, loss = 0.5879495143890381
iteration 251, loss = 0.65673828125
iteration 252, loss = 0.5842962861061096
iteration 253, loss = 0.6211934089660645
iteration 254, loss = 0.6128424406051636
iteration 255, loss = 0.6411469578742981
iteration 256, loss = 0.6044939756393433
iteration 257, loss = 0.6576780080795288
iteration 258, loss = 0.5981432795524597
iteration 259, loss = 0.612591564655304
iteration 260, loss = 0.6044543385505676
iteration 261, loss = 0.6453291177749634
iteration 262, loss = 0.5867778658866882
iteration 263, loss = 0.5806676149368286
iteration 264, loss = 0.5917339324951172
iteration 265, loss = 0.6000513434410095
iteration 266, loss = 0.6314300894737244
iteration 267, loss = 0.6147506833076477
iteration 268, loss = 0.6305542588233948
iteration 269, loss = 0.5969483256340027
iteration 270, loss = 0.6830631494522095
iteration 271, loss = 0.6011675000190735
iteration 272, loss = 0.7393903732299805
iteration 273, loss = 0.5928269624710083
iteration 274, loss = 0.5938520431518555
iteration 275, loss = 0.6297129392623901
iteration 276, loss = 0.6149303317070007
iteration 277, loss = 0.5860298275947571
iteration 278, loss = 0.6112653613090515
iteration 279, loss = 0.6110267639160156
iteration 280, loss = 0.6217316389083862
iteration 281, loss = 0.6305904984474182
iteration 282, loss = 0.6082215309143066
iteration 283, loss = 0.617820680141449
iteration 284, loss = 0.6138399839401245
iteration 285, loss = 0.6306687593460083
iteration 286, loss = 0.6344268321990967
iteration 287, loss = 0.5889925360679626
iteration 288, loss = 0.5720167756080627
iteration 289, loss = 0.6408802270889282
iteration 290, loss = 0.5755254626274109
iteration 291, loss = 0.5991624593734741
iteration 292, loss = 0.5789023637771606
iteration 293, loss = 0.5894206762313843
iteration 294, loss = 0.6430556774139404
iteration 295, loss = 0.5586246252059937
iteration 296, loss = 0.6337490081787109
iteration 297, loss = 0.6118471622467041
iteration 298, loss = 0.5900858640670776
iteration 299, loss = 0.6606281995773315
iteration 300, loss = 0.5852965116500854
iteration 1, loss = 0.6510336399078369
iteration 2, loss = 0.5721825361251831
iteration 3, loss = 0.5781337022781372
iteration 4, loss = 0.597512423992157
iteration 5, loss = 0.6373248100280762
iteration 6, loss = 0.5960079431533813
iteration 7, loss = 0.5767884254455566
iteration 8, loss = 0.650145411491394
iteration 9, loss = 0.6278108358383179
iteration 10, loss = 0.6193249225616455
iteration 11, loss = 0.6052843928337097
iteration 12, loss = 0.6200422644615173
iteration 13, loss = 0.6949663162231445
iteration 14, loss = 0.6090698838233948
iteration 15, loss = 0.5884650349617004
iteration 16, loss = 0.602081298828125
iteration 17, loss = 0.5875942707061768
iteration 18, loss = 0.5963348746299744
iteration 19, loss = 0.6101513504981995
iteration 20, loss = 0.5977576971054077
iteration 21, loss = 0.5856001377105713
iteration 22, loss = 0.5533777475357056
iteration 23, loss = 0.6683900952339172
iteration 24, loss = 0.5942717790603638
iteration 25, loss = 0.6176503896713257
iteration 26, loss = 0.5962368845939636
iteration 27, loss = 0.5995465517044067
iteration 28, loss = 0.5845416784286499
iteration 29, loss = 0.6124072074890137
iteration 30, loss = 0.6071550250053406
iteration 31, loss = 0.6727864742279053
iteration 32, loss = 0.6134306788444519
iteration 33, loss = 0.6084368228912354
iteration 34, loss = 0.5887632966041565
iteration 35, loss = 0.5772103071212769
iteration 36, loss = 0.6606125831604004
iteration 37, loss = 0.6328884959220886
iteration 38, loss = 0.6054319739341736
iteration 39, loss = 0.6013251543045044
iteration 40, loss = 0.6044797897338867
iteration 41, loss = 0.5862510800361633
iteration 42, loss = 0.6005294322967529
iteration 43, loss = 0.5862399339675903
iteration 44, loss = 0.6359248161315918
iteration 45, loss = 0.6418449878692627
iteration 46, loss = 0.6285120248794556
iteration 47, loss = 0.5877506136894226
iteration 48, loss = 0.5678256154060364
iteration 49, loss = 0.6193060874938965
iteration 50, loss = 0.5740057826042175
iteration 51, loss = 0.621101438999176
iteration 52, loss = 0.614670991897583
iteration 53, loss = 0.6135815382003784
iteration 54, loss = 0.580997884273529
iteration 55, loss = 0.5976691842079163
iteration 56, loss = 0.569411039352417
iteration 57, loss = 0.6173436045646667
iteration 58, loss = 0.6041221618652344
iteration 59, loss = 0.5616262555122375
iteration 60, loss = 0.5811992287635803
iteration 61, loss = 0.5927472114562988
iteration 62, loss = 0.5887832045555115
iteration 63, loss = 0.610175609588623
iteration 64, loss = 0.6115503907203674
iteration 65, loss = 0.5737141370773315
iteration 66, loss = 0.6168042421340942
iteration 67, loss = 0.5998597145080566
iteration 68, loss = 0.6361908912658691
iteration 69, loss = 0.6012775301933289
iteration 70, loss = 0.5827269554138184
iteration 71, loss = 0.584538996219635
iteration 72, loss = 0.6002339720726013
iteration 73, loss = 0.6113367080688477
iteration 74, loss = 0.5629637241363525
iteration 75, loss = 0.6216038465499878
iteration 76, loss = 0.5577752590179443
iteration 77, loss = 0.6221575736999512
iteration 78, loss = 0.5907772779464722
iteration 79, loss = 0.5988795757293701
iteration 80, loss = 0.5827419757843018
iteration 81, loss = 0.6306619048118591
iteration 82, loss = 0.6099282503128052
iteration 83, loss = 0.5674717426300049
iteration 84, loss = 0.6043200492858887
iteration 85, loss = 0.5886112451553345
iteration 86, loss = 0.6162122488021851
iteration 87, loss = 0.5757501721382141
iteration 88, loss = 0.615902304649353
iteration 89, loss = 0.6138290166854858
iteration 90, loss = 0.5711803436279297
iteration 91, loss = 0.6120063662528992
iteration 92, loss = 0.5572265386581421
iteration 93, loss = 0.5871595144271851
iteration 94, loss = 0.6177436709403992
iteration 95, loss = 0.5805919170379639
iteration 96, loss = 0.6075348258018494
iteration 97, loss = 0.6068766117095947
iteration 98, loss = 0.6035271286964417
iteration 99, loss = 0.6178064942359924
iteration 100, loss = 0.6065686345100403
iteration 101, loss = 0.583928108215332
iteration 102, loss = 0.6942834854125977
iteration 103, loss = 0.5569461584091187
iteration 104, loss = 0.5787144303321838
iteration 105, loss = 0.6765892505645752
iteration 106, loss = 0.5980569124221802
iteration 107, loss = 0.6029081344604492
iteration 108, loss = 0.6009215712547302
iteration 109, loss = 0.5943748354911804
iteration 110, loss = 0.5801262855529785
iteration 111, loss = 0.5845328569412231
iteration 112, loss = 0.622478187084198
iteration 113, loss = 0.5859951972961426
iteration 114, loss = 0.5758098363876343
iteration 115, loss = 0.5674779415130615
iteration 116, loss = 0.5896207094192505
iteration 117, loss = 0.5751962661743164
iteration 118, loss = 0.5899654030799866
iteration 119, loss = 0.6303985118865967
iteration 120, loss = 0.6023122668266296
iteration 121, loss = 0.5806421041488647
iteration 122, loss = 0.5666879415512085
iteration 123, loss = 0.5726050138473511
iteration 124, loss = 0.5573846101760864
iteration 125, loss = 0.5739908814430237
iteration 126, loss = 0.5600614547729492
iteration 127, loss = 0.5998788475990295
iteration 128, loss = 0.545801043510437
iteration 129, loss = 0.5906140208244324
iteration 130, loss = 0.5699333548545837
iteration 131, loss = 0.604824423789978
iteration 132, loss = 0.5783200263977051
iteration 133, loss = 0.6034127473831177
iteration 134, loss = 0.6167435646057129
iteration 135, loss = 0.5873458385467529
iteration 136, loss = 0.5919123888015747
iteration 137, loss = 0.5872899293899536
iteration 138, loss = 0.5673055648803711
iteration 139, loss = 0.5985098481178284
iteration 140, loss = 0.5724237561225891
iteration 141, loss = 0.5780448913574219
iteration 142, loss = 0.5585317015647888
iteration 143, loss = 0.6965187788009644
iteration 144, loss = 0.5800637006759644
iteration 145, loss = 0.6429437398910522
iteration 146, loss = 0.5974365472793579
iteration 147, loss = 0.5391311049461365
iteration 148, loss = 0.6238360404968262
iteration 149, loss = 0.555001974105835
iteration 150, loss = 0.5669406056404114
iteration 151, loss = 0.5638071894645691
iteration 152, loss = 0.5438282489776611
iteration 153, loss = 0.5839813947677612
iteration 154, loss = 0.5711076259613037
iteration 155, loss = 0.6151825189590454
iteration 156, loss = 0.5524904727935791
iteration 157, loss = 0.5555763840675354
iteration 158, loss = 0.5799272656440735
iteration 159, loss = 0.5548433065414429
iteration 160, loss = 0.5808161497116089
iteration 161, loss = 0.5547807812690735
iteration 162, loss = 0.5518593788146973
iteration 163, loss = 0.5343481302261353
iteration 164, loss = 0.5886326432228088
iteration 165, loss = 0.5664302706718445
iteration 166, loss = 0.5816068053245544
iteration 167, loss = 0.5869015455245972
iteration 168, loss = 0.5946201086044312
iteration 169, loss = 0.5763674974441528
iteration 170, loss = 0.5742136240005493
iteration 171, loss = 0.5529879331588745
iteration 172, loss = 0.6623857021331787
iteration 173, loss = 0.5956889390945435
iteration 174, loss = 0.5580400228500366
iteration 175, loss = 0.5862066745758057
iteration 176, loss = 0.6004350185394287
iteration 177, loss = 0.5404829978942871
iteration 178, loss = 0.5717237591743469
iteration 179, loss = 0.577290415763855
iteration 180, loss = 0.5688539743423462
iteration 181, loss = 0.6211733818054199
iteration 182, loss = 0.5918295383453369
iteration 183, loss = 0.5575202703475952
iteration 184, loss = 0.5646653175354004
iteration 185, loss = 0.5788931846618652
iteration 186, loss = 0.6680363416671753
iteration 187, loss = 0.5832350254058838
iteration 188, loss = 0.5841917991638184
iteration 189, loss = 0.5915403962135315
iteration 190, loss = 0.5830427408218384
iteration 191, loss = 0.5538461804389954
iteration 192, loss = 0.5576320886611938
iteration 193, loss = 0.6107045412063599
iteration 194, loss = 0.5598058104515076
iteration 195, loss = 0.5973227620124817
iteration 196, loss = 0.6362454891204834
iteration 197, loss = 0.5780746936798096
iteration 198, loss = 0.5534207820892334
iteration 199, loss = 0.556809663772583
iteration 200, loss = 0.5799994468688965
iteration 201, loss = 0.5925269722938538
iteration 202, loss = 0.5957905054092407
iteration 203, loss = 0.5987532734870911
iteration 204, loss = 0.6137771606445312
iteration 205, loss = 0.5858120918273926
iteration 206, loss = 0.5913817882537842
iteration 207, loss = 0.5669534206390381
iteration 208, loss = 0.5901023745536804
iteration 209, loss = 0.5768182873725891
iteration 210, loss = 0.5916349291801453
iteration 211, loss = 0.6198053956031799
iteration 212, loss = 0.5597500801086426
iteration 213, loss = 0.5839725732803345
iteration 214, loss = 0.6339570879936218
iteration 215, loss = 0.5610131621360779
iteration 216, loss = 0.55878746509552
iteration 217, loss = 0.5768170356750488
iteration 218, loss = 0.5831018686294556
iteration 219, loss = 0.6671470999717712
iteration 220, loss = 0.5743629932403564
iteration 221, loss = 0.5766458511352539
iteration 222, loss = 0.59306800365448
iteration 223, loss = 0.5775256752967834
iteration 224, loss = 0.6842856407165527
iteration 225, loss = 0.5335813760757446
iteration 226, loss = 0.5584313869476318
iteration 227, loss = 0.5732332468032837
iteration 228, loss = 0.5688128471374512
iteration 229, loss = 0.5673919320106506
iteration 230, loss = 0.6001510620117188
iteration 231, loss = 0.524624228477478
iteration 232, loss = 0.5366426706314087
iteration 233, loss = 0.5932036638259888
iteration 234, loss = 0.5495023727416992
iteration 235, loss = 0.5343170762062073
iteration 236, loss = 0.5830204486846924
iteration 237, loss = 0.5963559150695801
iteration 238, loss = 0.6768271923065186
iteration 239, loss = 0.5816425681114197
iteration 240, loss = 0.5845283269882202
iteration 241, loss = 0.5853961706161499
iteration 242, loss = 0.5559358596801758
iteration 243, loss = 0.5373829007148743
iteration 244, loss = 0.5619701743125916
iteration 245, loss = 0.5451412200927734
iteration 246, loss = 0.5346546173095703
iteration 247, loss = 0.569928765296936
iteration 248, loss = 0.5542632341384888
iteration 249, loss = 0.5779081583023071
iteration 250, loss = 0.5736326575279236
iteration 251, loss = 0.5843085646629333
iteration 252, loss = 0.5329082012176514
iteration 253, loss = 0.5852571725845337
iteration 254, loss = 0.5875858068466187
iteration 255, loss = 0.5607360005378723
iteration 256, loss = 0.5917750597000122
iteration 257, loss = 0.5971652865409851
iteration 258, loss = 0.6666445732116699
iteration 259, loss = 0.5620843768119812
iteration 260, loss = 0.5379441976547241
iteration 261, loss = 0.5923357605934143
iteration 262, loss = 0.5816807150840759
iteration 263, loss = 0.5853111743927002
iteration 264, loss = 0.5790456533432007
iteration 265, loss = 0.6585797667503357
iteration 266, loss = 0.5828879475593567
iteration 267, loss = 0.5805972814559937
iteration 268, loss = 0.5490642189979553
iteration 269, loss = 0.5745877027511597
iteration 270, loss = 0.538853108882904
iteration 271, loss = 0.5539861917495728
iteration 272, loss = 0.5615435838699341
iteration 273, loss = 0.5902365446090698
iteration 274, loss = 0.5390527248382568
iteration 275, loss = 0.5722766518592834
iteration 276, loss = 0.607444167137146
iteration 277, loss = 0.5386837720870972
iteration 278, loss = 0.5649645328521729
iteration 279, loss = 0.6029775142669678
iteration 280, loss = 0.5922307968139648
iteration 281, loss = 0.5513953566551208
iteration 282, loss = 0.5427676439285278
iteration 283, loss = 0.5657814741134644
iteration 284, loss = 0.6879361867904663
iteration 285, loss = 0.583662748336792
iteration 286, loss = 0.5997958183288574
iteration 287, loss = 0.5506249666213989
iteration 288, loss = 0.6949586272239685
iteration 289, loss = 0.5768539905548096
iteration 290, loss = 0.5608757734298706
iteration 291, loss = 0.561576783657074
iteration 292, loss = 0.5554436445236206
iteration 293, loss = 0.5548085570335388
iteration 294, loss = 0.5989518165588379
iteration 295, loss = 0.5843209624290466
iteration 296, loss = 0.5528417825698853
iteration 297, loss = 0.5723140239715576
iteration 298, loss = 0.6010706424713135
iteration 299, loss = 0.5643889307975769
iteration 300, loss = 0.554297685623169
iteration 1, loss = 0.5547083616256714
iteration 2, loss = 0.5721755623817444
iteration 3, loss = 0.5504796504974365
iteration 4, loss = 0.5344775915145874
iteration 5, loss = 0.574588418006897
iteration 6, loss = 0.569167971611023
iteration 7, loss = 0.6252139806747437
iteration 8, loss = 0.558791995048523
iteration 9, loss = 0.5379090309143066
iteration 10, loss = 0.6039241552352905
iteration 11, loss = 0.582455039024353
iteration 12, loss = 0.5579607486724854
iteration 13, loss = 0.5904760956764221
iteration 14, loss = 0.5464217066764832
iteration 15, loss = 0.5459430813789368
iteration 16, loss = 0.5634693503379822
iteration 17, loss = 0.5572470426559448
iteration 18, loss = 0.669632613658905
iteration 19, loss = 0.601874053478241
iteration 20, loss = 0.5819305181503296
iteration 21, loss = 0.5812724828720093
iteration 22, loss = 0.5680235624313354
iteration 23, loss = 0.6101522445678711
iteration 24, loss = 0.5514171123504639
iteration 25, loss = 0.5745847821235657
iteration 26, loss = 0.566483736038208
iteration 27, loss = 0.5738697052001953
iteration 28, loss = 0.5421076416969299
iteration 29, loss = 0.5595090985298157
iteration 30, loss = 0.5519810914993286
iteration 31, loss = 0.5695557594299316
iteration 32, loss = 0.5894507765769958
iteration 33, loss = 0.7098652124404907
iteration 34, loss = 0.5816688537597656
iteration 35, loss = 0.5817991495132446
iteration 36, loss = 0.5760342478752136
iteration 37, loss = 0.5308424830436707
iteration 38, loss = 0.5472978949546814
iteration 39, loss = 0.537606418132782
iteration 40, loss = 0.5382904410362244
iteration 41, loss = 0.5376186370849609
iteration 42, loss = 0.5537352561950684
iteration 43, loss = 0.5678302049636841
iteration 44, loss = 0.5627022981643677
iteration 45, loss = 0.5642876625061035
iteration 46, loss = 0.5815601348876953
iteration 47, loss = 0.55024653673172
iteration 48, loss = 0.5940192341804504
iteration 49, loss = 0.5746599435806274
iteration 50, loss = 0.5885331630706787
iteration 51, loss = 0.5714040994644165
iteration 52, loss = 0.6401047706604004
iteration 53, loss = 0.5807645320892334
iteration 54, loss = 0.5889407396316528
iteration 55, loss = 0.5716816186904907
iteration 56, loss = 0.564064621925354
iteration 57, loss = 0.6114751100540161
iteration 58, loss = 0.5441466569900513
iteration 59, loss = 0.5531923174858093
iteration 60, loss = 0.5800490379333496
iteration 61, loss = 0.593294620513916
iteration 62, loss = 0.5801211595535278
iteration 63, loss = 0.5423529744148254
iteration 64, loss = 0.5704702734947205
iteration 65, loss = 0.6251122951507568
iteration 66, loss = 0.552878201007843
iteration 67, loss = 0.5556164979934692
iteration 68, loss = 0.5431666374206543
iteration 69, loss = 0.5519918203353882
iteration 70, loss = 0.5611056089401245
iteration 71, loss = 0.6596882939338684
iteration 72, loss = 0.5621547102928162
iteration 73, loss = 0.5442245602607727
iteration 74, loss = 0.5894395112991333
iteration 75, loss = 0.6321682929992676
iteration 76, loss = 0.5680854916572571
iteration 77, loss = 0.5596858859062195
iteration 78, loss = 0.5862511992454529
iteration 79, loss = 0.5474326610565186
iteration 80, loss = 0.5309175252914429
iteration 81, loss = 0.5231444239616394
iteration 82, loss = 0.5142723321914673
iteration 83, loss = 0.5320441722869873
iteration 84, loss = 0.567507803440094
iteration 85, loss = 0.5301440358161926
iteration 86, loss = 0.5951760411262512
iteration 87, loss = 0.5377127528190613
iteration 88, loss = 0.5352709293365479
iteration 89, loss = 0.5114105939865112
iteration 90, loss = 0.5573972463607788
iteration 91, loss = 0.5614813566207886
iteration 92, loss = 0.5176284313201904
iteration 93, loss = 0.5552334189414978
iteration 94, loss = 0.5389236807823181
iteration 95, loss = 0.5467193722724915
iteration 96, loss = 0.5210098624229431
iteration 97, loss = 0.5519282221794128
iteration 98, loss = 0.5413656234741211
iteration 99, loss = 0.5604018568992615
iteration 100, loss = 0.5277149081230164
iteration 101, loss = 0.5920504331588745
iteration 102, loss = 0.5583521723747253
iteration 103, loss = 0.5790177583694458
iteration 104, loss = 0.5408562421798706
iteration 105, loss = 0.5414566397666931
iteration 106, loss = 0.59734046459198
iteration 107, loss = 0.5252097845077515
iteration 108, loss = 0.5352797508239746
iteration 109, loss = 0.5687575936317444
iteration 110, loss = 0.5590507984161377
iteration 111, loss = 0.543215274810791
iteration 112, loss = 0.5837237238883972
iteration 113, loss = 0.583665668964386
iteration 114, loss = 0.5354538559913635
iteration 115, loss = 0.5457850694656372
iteration 116, loss = 0.6428910493850708
iteration 117, loss = 0.5425677299499512
iteration 118, loss = 0.5191870927810669
iteration 119, loss = 0.5697887539863586
iteration 120, loss = 0.597846508026123
iteration 121, loss = 0.5699564814567566
iteration 122, loss = 0.5396332740783691
iteration 123, loss = 0.5258897542953491
iteration 124, loss = 0.5749317407608032
iteration 125, loss = 0.5365989804267883
iteration 126, loss = 0.5563693642616272
iteration 127, loss = 0.5648491978645325
iteration 128, loss = 0.5470617413520813
iteration 129, loss = 0.5732290744781494
iteration 130, loss = 0.5816556811332703
iteration 131, loss = 0.5391414165496826
iteration 132, loss = 0.5783246755599976
iteration 133, loss = 0.5341490507125854
iteration 134, loss = 0.543096125125885
iteration 135, loss = 0.5528892278671265
iteration 136, loss = 0.5432343482971191
iteration 137, loss = 0.6239491105079651
iteration 138, loss = 0.5753301382064819
iteration 139, loss = 0.5810655951499939
iteration 140, loss = 0.6540207266807556
iteration 141, loss = 0.512871503829956
iteration 142, loss = 0.570843517780304
iteration 143, loss = 0.5523295998573303
iteration 144, loss = 0.6819792985916138
iteration 145, loss = 0.5521673560142517
iteration 146, loss = 0.5203641057014465
iteration 147, loss = 0.5745184421539307
iteration 148, loss = 0.5627028942108154
iteration 149, loss = 0.512999951839447
iteration 150, loss = 0.5431379675865173
iteration 151, loss = 0.5576969385147095
iteration 152, loss = 0.5176470875740051
iteration 153, loss = 0.5203186869621277
iteration 154, loss = 0.5695494413375854
iteration 155, loss = 0.553327739238739
iteration 156, loss = 0.5408813953399658
iteration 157, loss = 0.5178564190864563
iteration 158, loss = 0.5583115816116333
iteration 159, loss = 0.5718820691108704
iteration 160, loss = 0.5705018043518066
iteration 161, loss = 0.5383641123771667
iteration 162, loss = 0.5756874680519104
iteration 163, loss = 0.5592383742332458
iteration 164, loss = 0.5729764103889465
iteration 165, loss = 0.5296069383621216
iteration 166, loss = 0.5903443694114685
iteration 167, loss = 0.5687363147735596
iteration 168, loss = 0.5356940627098083
iteration 169, loss = 0.5312559604644775
iteration 170, loss = 0.5468255281448364
iteration 171, loss = 0.5551825761795044
iteration 172, loss = 0.528666615486145
iteration 173, loss = 0.5470665693283081
iteration 174, loss = 0.5034736394882202
iteration 175, loss = 0.5140615105628967
iteration 176, loss = 0.5926371812820435
iteration 177, loss = 0.5209912657737732
iteration 178, loss = 0.6166241765022278
iteration 179, loss = 0.569067120552063
iteration 180, loss = 0.5588857531547546
iteration 181, loss = 0.5270179510116577
iteration 182, loss = 0.5423382520675659
iteration 183, loss = 0.5830356478691101
iteration 184, loss = 0.5432414412498474
iteration 185, loss = 0.5424606800079346
iteration 186, loss = 0.5365793704986572
iteration 187, loss = 0.5698672533035278
iteration 188, loss = 0.5100389719009399
iteration 189, loss = 0.5210855603218079
iteration 190, loss = 0.5913370847702026
iteration 191, loss = 0.5209505558013916
iteration 192, loss = 0.517740786075592
iteration 193, loss = 0.5578104257583618
iteration 194, loss = 0.5734789371490479
iteration 195, loss = 0.5493656396865845
iteration 196, loss = 0.5515012741088867
iteration 197, loss = 0.511780858039856
iteration 198, loss = 0.5415430068969727
iteration 199, loss = 0.6340862512588501
iteration 200, loss = 0.5085434317588806
iteration 201, loss = 0.5358057618141174
iteration 202, loss = 0.546779990196228
iteration 203, loss = 0.5841664671897888
iteration 204, loss = 0.5510262250900269
iteration 205, loss = 0.5561432838439941
iteration 206, loss = 0.5406526327133179
iteration 207, loss = 0.5585262775421143
iteration 208, loss = 0.5519411563873291
iteration 209, loss = 0.5205540060997009
iteration 210, loss = 0.5097615718841553
iteration 211, loss = 0.5377103090286255
iteration 212, loss = 0.5214451551437378
iteration 213, loss = 0.55150306224823
iteration 214, loss = 0.6027706861495972
iteration 215, loss = 0.5685724020004272
iteration 216, loss = 0.5325961112976074
iteration 217, loss = 0.5272949934005737
iteration 218, loss = 0.5489752292633057
iteration 219, loss = 0.5485235452651978
iteration 220, loss = 0.5450679063796997
iteration 221, loss = 0.5720101594924927
iteration 222, loss = 0.5404464602470398
iteration 223, loss = 0.5436053276062012
iteration 224, loss = 0.5667130351066589
iteration 225, loss = 0.535614013671875
iteration 226, loss = 0.524937093257904
iteration 227, loss = 0.5251425504684448
iteration 228, loss = 0.5641253590583801
iteration 229, loss = 0.5241658091545105
iteration 230, loss = 0.5323888659477234
iteration 231, loss = 0.5631980895996094
iteration 232, loss = 0.5509903430938721
iteration 233, loss = 0.5771381855010986
iteration 234, loss = 0.7213847041130066
iteration 235, loss = 0.5036342740058899
iteration 236, loss = 0.5400648713111877
iteration 237, loss = 0.5021302700042725
iteration 238, loss = 0.5099276304244995
iteration 239, loss = 0.5574052333831787
iteration 240, loss = 0.558039665222168
iteration 241, loss = 0.5404539108276367
iteration 242, loss = 0.5499237179756165
iteration 243, loss = 0.5073434114456177
iteration 244, loss = 0.5523064136505127
iteration 245, loss = 0.6042076349258423
iteration 246, loss = 0.5092441439628601
iteration 247, loss = 0.5478496551513672
iteration 248, loss = 0.5708186030387878
iteration 249, loss = 0.5456517934799194
iteration 250, loss = 0.5873608589172363
iteration 251, loss = 0.5121372938156128
iteration 252, loss = 0.5633383393287659
iteration 253, loss = 0.5362995862960815
iteration 254, loss = 0.5145284533500671
iteration 255, loss = 0.5422865748405457
iteration 256, loss = 0.5050003528594971
iteration 257, loss = 0.5127261281013489
iteration 258, loss = 0.5360464453697205
iteration 259, loss = 0.5467710494995117
iteration 260, loss = 0.5082345008850098
iteration 261, loss = 0.5470068454742432
iteration 262, loss = 0.5237325429916382
iteration 263, loss = 0.5751923322677612
iteration 264, loss = 0.5600354671478271
iteration 265, loss = 0.56020188331604
iteration 266, loss = 0.5565797090530396
iteration 267, loss = 0.6332399845123291
iteration 268, loss = 0.54310542345047
iteration 269, loss = 0.5296604037284851
iteration 270, loss = 0.5125380754470825
iteration 271, loss = 0.5426146984100342
iteration 272, loss = 0.5164271593093872
iteration 273, loss = 0.5247769951820374
iteration 274, loss = 0.5384703874588013
iteration 275, loss = 0.5656097531318665
iteration 276, loss = 0.5116968154907227
iteration 277, loss = 0.542074978351593
iteration 278, loss = 0.5390011072158813
iteration 279, loss = 0.5332475900650024
iteration 280, loss = 0.5282492637634277
iteration 281, loss = 0.5104410648345947
iteration 282, loss = 0.6646549105644226
iteration 283, loss = 0.5237410664558411
iteration 284, loss = 0.4939896762371063
iteration 285, loss = 0.5808584690093994
iteration 286, loss = 0.5181599259376526
iteration 287, loss = 0.5116022825241089
iteration 288, loss = 0.543149471282959
iteration 289, loss = 0.5224319696426392
iteration 290, loss = 0.581392765045166
iteration 291, loss = 0.5234594345092773
iteration 292, loss = 0.49998944997787476
iteration 293, loss = 0.5285579562187195
iteration 294, loss = 0.5261808037757874
iteration 295, loss = 0.5288171768188477
iteration 296, loss = 0.6176364421844482
iteration 297, loss = 0.5496181845664978
iteration 298, loss = 0.5406957864761353
iteration 299, loss = 0.5339403748512268
iteration 300, loss = 0.5341023206710815
iteration 1, loss = 0.5310516357421875
iteration 2, loss = 0.5480309724807739
iteration 3, loss = 0.5301199555397034
iteration 4, loss = 0.5058090090751648
iteration 5, loss = 0.5098348259925842
iteration 6, loss = 0.5667580366134644
iteration 7, loss = 0.537777841091156
iteration 8, loss = 0.5823644995689392
iteration 9, loss = 0.5104396343231201
iteration 10, loss = 0.548143208026886
iteration 11, loss = 0.5680052042007446
iteration 12, loss = 0.5241715312004089
iteration 13, loss = 0.5594722032546997
iteration 14, loss = 0.5445336103439331
iteration 15, loss = 0.5458246469497681
iteration 16, loss = 0.5776261687278748
iteration 17, loss = 0.5331897139549255
iteration 18, loss = 0.5495800971984863
iteration 19, loss = 0.6016164422035217
iteration 20, loss = 0.5635109543800354
iteration 21, loss = 0.5013835430145264
iteration 22, loss = 0.5145231485366821
iteration 23, loss = 0.555546760559082
iteration 24, loss = 0.5469924211502075
iteration 25, loss = 0.5434646606445312
iteration 26, loss = 0.53950035572052
iteration 27, loss = 0.5810717940330505
iteration 28, loss = 0.5389357209205627
iteration 29, loss = 0.5383657813072205
iteration 30, loss = 0.528779149055481
iteration 31, loss = 0.523749828338623
iteration 32, loss = 0.5365942716598511
iteration 33, loss = 0.6394107341766357
iteration 34, loss = 0.5591108798980713
iteration 35, loss = 0.48595401644706726
iteration 36, loss = 0.5156177282333374
iteration 37, loss = 0.5085421204566956
iteration 38, loss = 0.5092345476150513
iteration 39, loss = 0.5866544842720032
iteration 40, loss = 0.5639477372169495
iteration 41, loss = 0.5413404703140259
iteration 42, loss = 0.5146424770355225
iteration 43, loss = 0.5455289483070374
iteration 44, loss = 0.4930972158908844
iteration 45, loss = 0.47462719678878784
iteration 46, loss = 0.5006495714187622
iteration 47, loss = 0.531597375869751
iteration 48, loss = 0.5332538485527039
iteration 49, loss = 0.5691704750061035
iteration 50, loss = 0.5400477647781372
iteration 51, loss = 0.5101264715194702
iteration 52, loss = 0.5520790815353394
iteration 53, loss = 0.5220865607261658
iteration 54, loss = 0.5413841009140015
iteration 55, loss = 0.5698227286338806
iteration 56, loss = 0.5283217430114746
iteration 57, loss = 0.5507985353469849
iteration 58, loss = 0.5147901177406311
iteration 59, loss = 0.5157955288887024
iteration 60, loss = 0.5268648862838745
iteration 61, loss = 0.5589423179626465
iteration 62, loss = 0.5832436680793762
iteration 63, loss = 0.5477986335754395
iteration 64, loss = 0.4962848126888275
iteration 65, loss = 0.5712399482727051
iteration 66, loss = 0.5273407697677612
iteration 67, loss = 0.5337716341018677
iteration 68, loss = 0.6535736918449402
iteration 69, loss = 0.5167521834373474
iteration 70, loss = 0.5014063119888306
iteration 71, loss = 0.4777607321739197
iteration 72, loss = 0.4944530725479126
iteration 73, loss = 0.5217620730400085
iteration 74, loss = 0.49471402168273926
iteration 75, loss = 0.5029061436653137
iteration 76, loss = 0.5341457724571228
iteration 77, loss = 0.6806430220603943
iteration 78, loss = 0.5241145491600037
iteration 79, loss = 0.5615202188491821
iteration 80, loss = 0.5148459076881409
iteration 81, loss = 0.7921299338340759
iteration 82, loss = 0.5436543822288513
iteration 83, loss = 0.5219831466674805
iteration 84, loss = 0.5160568356513977
iteration 85, loss = 0.5396851301193237
iteration 86, loss = 0.5581079125404358
iteration 87, loss = 0.5291034579277039
iteration 88, loss = 0.5025187730789185
iteration 89, loss = 0.5206727981567383
iteration 90, loss = 0.5440329313278198
iteration 91, loss = 0.48534882068634033
iteration 92, loss = 0.5273973941802979
iteration 93, loss = 0.5199198126792908
iteration 94, loss = 0.5717594623565674
iteration 95, loss = 0.49326610565185547
iteration 96, loss = 0.5329831838607788
iteration 97, loss = 0.49769267439842224
iteration 98, loss = 0.5152416229248047
iteration 99, loss = 0.607859194278717
iteration 100, loss = 0.509501039981842
iteration 101, loss = 0.5562800168991089
iteration 102, loss = 0.493412047624588
iteration 103, loss = 0.5487133264541626
iteration 104, loss = 0.576088547706604
iteration 105, loss = 0.5235929489135742
iteration 106, loss = 0.5185755491256714
iteration 107, loss = 0.5248520970344543
iteration 108, loss = 0.5060614347457886
iteration 109, loss = 0.5506036281585693
iteration 110, loss = 0.5366325378417969
iteration 111, loss = 0.5496045351028442
iteration 112, loss = 0.5061548948287964
iteration 113, loss = 0.5634360313415527
iteration 114, loss = 0.5367988348007202
iteration 115, loss = 0.5578899383544922
iteration 116, loss = 0.5215682983398438
iteration 117, loss = 0.535921573638916
iteration 118, loss = 0.5167189240455627
iteration 119, loss = 0.6209664940834045
iteration 120, loss = 0.5215590000152588
iteration 121, loss = 0.5827169418334961
iteration 122, loss = 0.49487292766571045
iteration 123, loss = 0.5132811069488525
iteration 124, loss = 0.5138245820999146
iteration 125, loss = 0.5031613707542419
iteration 126, loss = 0.5300417542457581
iteration 127, loss = 0.5148627161979675
iteration 128, loss = 0.5648152828216553
iteration 129, loss = 0.511397123336792
iteration 130, loss = 0.5219669342041016
iteration 131, loss = 0.5039469599723816
iteration 132, loss = 0.4724419116973877
iteration 133, loss = 0.5797082781791687
iteration 134, loss = 0.48736101388931274
iteration 135, loss = 0.5100226998329163
iteration 136, loss = 0.601385772228241
iteration 137, loss = 0.49758481979370117
iteration 138, loss = 0.4862934947013855
iteration 139, loss = 0.5488511323928833
iteration 140, loss = 0.511838436126709
iteration 141, loss = 0.511863648891449
iteration 142, loss = 0.5357257127761841
iteration 143, loss = 0.5197886824607849
iteration 144, loss = 0.5163397789001465
iteration 145, loss = 0.5324430465698242
iteration 146, loss = 0.4971272349357605
iteration 147, loss = 0.5329271554946899
iteration 148, loss = 0.5045886635780334
iteration 149, loss = 0.5134934186935425
iteration 150, loss = 0.5114871263504028
iteration 151, loss = 0.5333805084228516
iteration 152, loss = 0.5182313323020935
iteration 153, loss = 0.5176931619644165
iteration 154, loss = 0.4997602701187134
iteration 155, loss = 0.5184680819511414
iteration 156, loss = 0.48144879937171936
iteration 157, loss = 0.4911080002784729
iteration 158, loss = 0.5287136435508728
iteration 159, loss = 0.5016158819198608
iteration 160, loss = 0.5299316644668579
iteration 161, loss = 0.5224974155426025
iteration 162, loss = 0.49316075444221497
iteration 163, loss = 0.5555992126464844
iteration 164, loss = 0.6628059148788452
iteration 165, loss = 0.5737932920455933
iteration 166, loss = 0.5515060424804688
iteration 167, loss = 0.4794390797615051
iteration 168, loss = 0.5143817663192749
iteration 169, loss = 0.48226431012153625
iteration 170, loss = 0.4865797758102417
iteration 171, loss = 0.5094704031944275
iteration 172, loss = 0.5027160048484802
iteration 173, loss = 0.5041782855987549
iteration 174, loss = 0.5063534379005432
iteration 175, loss = 0.5615981817245483
iteration 176, loss = 0.566288411617279
iteration 177, loss = 0.4913686513900757
iteration 178, loss = 0.4911922216415405
iteration 179, loss = 0.5062438249588013
iteration 180, loss = 0.4957376718521118
iteration 181, loss = 0.5130470395088196
iteration 182, loss = 0.5171036124229431
iteration 183, loss = 0.5225973129272461
iteration 184, loss = 0.5181182622909546
iteration 185, loss = 0.5006428360939026
iteration 186, loss = 0.4907132387161255
iteration 187, loss = 0.6286752223968506
iteration 188, loss = 0.5449402928352356
iteration 189, loss = 0.49693578481674194
iteration 190, loss = 0.49893301725387573
iteration 191, loss = 0.520854651927948
iteration 192, loss = 0.5080579519271851
iteration 193, loss = 0.5869888067245483
iteration 194, loss = 0.492855966091156
iteration 195, loss = 0.5276336669921875
iteration 196, loss = 0.4902316927909851
iteration 197, loss = 0.5424261689186096
iteration 198, loss = 0.5253605842590332
iteration 199, loss = 0.5559254884719849
iteration 200, loss = 0.5045312643051147
iteration 201, loss = 0.5305140018463135
iteration 202, loss = 0.5092374086380005
iteration 203, loss = 0.5058853626251221
iteration 204, loss = 0.4827878475189209
iteration 205, loss = 0.5995223522186279
iteration 206, loss = 0.48390471935272217
iteration 207, loss = 0.5040813684463501
iteration 208, loss = 0.49377793073654175
iteration 209, loss = 0.5597968101501465
iteration 210, loss = 0.5063154697418213
iteration 211, loss = 0.49654078483581543
iteration 212, loss = 0.5291383266448975
iteration 213, loss = 0.4969745874404907
iteration 214, loss = 0.6611801385879517
iteration 215, loss = 0.49425315856933594
iteration 216, loss = 0.5268263816833496
iteration 217, loss = 0.47344517707824707
iteration 218, loss = 0.47506222128868103
iteration 219, loss = 0.4959654211997986
iteration 220, loss = 0.47888123989105225
iteration 221, loss = 0.47691959142684937
iteration 222, loss = 0.49636903405189514
iteration 223, loss = 0.5272425413131714
iteration 224, loss = 0.5125452280044556
iteration 225, loss = 0.47240111231803894
iteration 226, loss = 0.5293317437171936
iteration 227, loss = 0.4838685095310211
iteration 228, loss = 0.5094835162162781
iteration 229, loss = 0.4981921911239624
iteration 230, loss = 0.5379558205604553
iteration 231, loss = 0.48644566535949707
iteration 232, loss = 0.5289823412895203
iteration 233, loss = 0.4832761585712433
iteration 234, loss = 0.6192778944969177
iteration 235, loss = 0.5078434348106384
iteration 236, loss = 0.5295913219451904
iteration 237, loss = 0.5892660617828369
iteration 238, loss = 0.5304009318351746
iteration 239, loss = 0.5324842929840088
iteration 240, loss = 0.5105017423629761
iteration 241, loss = 0.4599895477294922
iteration 242, loss = 0.48122307658195496
iteration 243, loss = 0.5165006518363953
iteration 244, loss = 0.5091695785522461
iteration 245, loss = 0.49497827887535095
iteration 246, loss = 0.5168535709381104
iteration 247, loss = 0.49506035447120667
iteration 248, loss = 0.5851359367370605
iteration 249, loss = 0.518234372138977
iteration 250, loss = 0.5259326100349426
iteration 251, loss = 0.5055571794509888
iteration 252, loss = 0.5509098768234253
iteration 253, loss = 0.48725375533103943
iteration 254, loss = 0.5310057997703552
iteration 255, loss = 0.5037055015563965
iteration 256, loss = 0.46224692463874817
iteration 257, loss = 0.5122230052947998
iteration 258, loss = 0.495972603559494
iteration 259, loss = 0.4846138656139374
iteration 260, loss = 0.5211477279663086
iteration 261, loss = 0.474969744682312
iteration 262, loss = 0.5225459933280945
iteration 263, loss = 0.5168615579605103
iteration 264, loss = 0.4992560148239136
iteration 265, loss = 0.4812926948070526
iteration 266, loss = 0.4813010096549988
iteration 267, loss = 0.511509895324707
iteration 268, loss = 0.49273136258125305
iteration 269, loss = 0.5294207334518433
iteration 270, loss = 0.5481194257736206
iteration 271, loss = 0.5611536502838135
iteration 272, loss = 0.49176403880119324
iteration 273, loss = 0.4812704026699066
iteration 274, loss = 0.5166637301445007
iteration 275, loss = 0.5396357178688049
iteration 276, loss = 0.5155028104782104
iteration 277, loss = 0.5238535404205322
iteration 278, loss = 0.5190404653549194
iteration 279, loss = 0.5345509052276611
iteration 280, loss = 0.5738154053688049
iteration 281, loss = 0.5089370012283325
iteration 282, loss = 0.5213866233825684
iteration 283, loss = 0.577494204044342
iteration 284, loss = 0.5028680562973022
iteration 285, loss = 0.49191129207611084
iteration 286, loss = 0.5217088460922241
iteration 287, loss = 0.4706791639328003
iteration 288, loss = 0.5022611618041992
iteration 289, loss = 0.6468746662139893
iteration 290, loss = 0.5011657476425171
iteration 291, loss = 0.5060744881629944
iteration 292, loss = 0.5243408679962158
iteration 293, loss = 0.4925709366798401
iteration 294, loss = 0.6080081462860107
iteration 295, loss = 0.6070902943611145
iteration 296, loss = 0.5664348006248474
iteration 297, loss = 0.47709140181541443
iteration 298, loss = 0.5350252985954285
iteration 299, loss = 0.4824604094028473
iteration 300, loss = 0.461516797542572
iteration 1, loss = 0.5019080638885498
iteration 2, loss = 0.5039989352226257
iteration 3, loss = 0.574203372001648
iteration 4, loss = 0.5725017189979553
iteration 5, loss = 0.4665476083755493
iteration 6, loss = 0.4894241690635681
iteration 7, loss = 0.5005902051925659
iteration 8, loss = 0.48259028792381287
iteration 9, loss = 0.4834355115890503
iteration 10, loss = 0.5062872767448425
iteration 11, loss = 0.48597317934036255
iteration 12, loss = 0.5239884853363037
iteration 13, loss = 0.5370035171508789
iteration 14, loss = 0.4897008538246155
iteration 15, loss = 0.48768770694732666
iteration 16, loss = 0.510309636592865
iteration 17, loss = 0.5212268829345703
iteration 18, loss = 0.6165503263473511
iteration 19, loss = 0.4702293872833252
iteration 20, loss = 0.4815877676010132
iteration 21, loss = 0.6005005836486816
iteration 22, loss = 0.507952868938446
iteration 23, loss = 0.4822351038455963
iteration 24, loss = 0.4962666630744934
iteration 25, loss = 0.5871182084083557
iteration 26, loss = 0.4862734377384186
iteration 27, loss = 0.4980884790420532
iteration 28, loss = 0.5171539187431335
iteration 29, loss = 0.5478283762931824
iteration 30, loss = 0.5480347275733948
iteration 31, loss = 0.5042706727981567
iteration 32, loss = 0.48431092500686646
iteration 33, loss = 0.48438844084739685
iteration 34, loss = 0.5217054486274719
iteration 35, loss = 0.6216035485267639
iteration 36, loss = 0.48017939925193787
iteration 37, loss = 0.542239785194397
iteration 38, loss = 0.5033074617385864
iteration 39, loss = 0.5575816035270691
iteration 40, loss = 0.5012832283973694
iteration 41, loss = 0.4739028215408325
iteration 42, loss = 0.4874984323978424
iteration 43, loss = 0.47770026326179504
iteration 44, loss = 0.4834313988685608
iteration 45, loss = 0.4658409357070923
iteration 46, loss = 0.4731523394584656
iteration 47, loss = 0.5361993908882141
iteration 48, loss = 0.5061599016189575
iteration 49, loss = 0.5020028948783875
iteration 50, loss = 0.4976600408554077
iteration 51, loss = 0.48184734582901
iteration 52, loss = 0.5068076848983765
iteration 53, loss = 0.46022602915763855
iteration 54, loss = 0.45660221576690674
iteration 55, loss = 0.4781934320926666
iteration 56, loss = 0.514253556728363
iteration 57, loss = 0.49944594502449036
iteration 58, loss = 0.4927316904067993
iteration 59, loss = 0.6123490333557129
iteration 60, loss = 0.5100696086883545
iteration 61, loss = 0.47983914613723755
iteration 62, loss = 0.4992588460445404
iteration 63, loss = 0.5428474545478821
iteration 64, loss = 0.4894039034843445
iteration 65, loss = 0.4590595066547394
iteration 66, loss = 0.5296769142150879
iteration 67, loss = 0.5210670232772827
iteration 68, loss = 0.4837011992931366
iteration 69, loss = 0.496797651052475
iteration 70, loss = 0.5269485712051392
iteration 71, loss = 0.5533195734024048
iteration 72, loss = 0.48270487785339355
iteration 73, loss = 0.4799148738384247
iteration 74, loss = 0.5093835592269897
iteration 75, loss = 0.5356863737106323
iteration 76, loss = 0.6145186424255371
iteration 77, loss = 0.6228066682815552
iteration 78, loss = 0.49349433183670044
iteration 79, loss = 0.510577380657196
iteration 80, loss = 0.4831123352050781
iteration 81, loss = 0.4743332266807556
iteration 82, loss = 0.5090073347091675
iteration 83, loss = 0.48287448287010193
iteration 84, loss = 0.4549538791179657
iteration 85, loss = 0.5044664144515991
iteration 86, loss = 0.5669173002243042
iteration 87, loss = 0.46166205406188965
iteration 88, loss = 0.4845031201839447
iteration 89, loss = 0.4973616898059845
iteration 90, loss = 0.5009486079216003
iteration 91, loss = 0.47027212381362915
iteration 92, loss = 0.5020301938056946
iteration 93, loss = 0.5395082235336304
iteration 94, loss = 0.5227549076080322
iteration 95, loss = 0.4822285771369934
iteration 96, loss = 0.4902271032333374
iteration 97, loss = 0.44275301694869995
iteration 98, loss = 0.5353526473045349
iteration 99, loss = 0.46772903203964233
iteration 100, loss = 0.5263599753379822
iteration 101, loss = 0.5877878665924072
iteration 102, loss = 0.48308008909225464
iteration 103, loss = 0.533149003982544
iteration 104, loss = 0.5196946859359741
iteration 105, loss = 0.48117589950561523
iteration 106, loss = 0.5231707096099854
iteration 107, loss = 0.5401684045791626
iteration 108, loss = 0.4514563977718353
iteration 109, loss = 0.46421656012535095
iteration 110, loss = 0.4721822738647461
iteration 111, loss = 0.5385206937789917
iteration 112, loss = 0.48799893260002136
iteration 113, loss = 0.49790385365486145
iteration 114, loss = 0.5052155256271362
iteration 115, loss = 0.47613948583602905
iteration 116, loss = 0.47074228525161743
iteration 117, loss = 0.46537458896636963
iteration 118, loss = 0.5065240263938904
iteration 119, loss = 0.5149951577186584
iteration 120, loss = 0.45994284749031067
iteration 121, loss = 0.49134740233421326
iteration 122, loss = 0.5035549402236938
iteration 123, loss = 0.46014368534088135
iteration 124, loss = 0.47719451785087585
iteration 125, loss = 0.4811171293258667
iteration 126, loss = 0.47553980350494385
iteration 127, loss = 0.5217085480690002
iteration 128, loss = 0.5272331237792969
iteration 129, loss = 0.4595488905906677
iteration 130, loss = 0.4814227819442749
iteration 131, loss = 0.5176755785942078
iteration 132, loss = 0.4719371795654297
iteration 133, loss = 0.5002602338790894
iteration 134, loss = 0.4832875728607178
iteration 135, loss = 0.5021083950996399
iteration 136, loss = 0.5004151463508606
iteration 137, loss = 0.5455888509750366
iteration 138, loss = 0.4927903413772583
iteration 139, loss = 0.5605522394180298
iteration 140, loss = 0.4601922631263733
iteration 141, loss = 0.48925089836120605
iteration 142, loss = 0.4965267777442932
iteration 143, loss = 0.5228420495986938
iteration 144, loss = 0.4849531650543213
iteration 145, loss = 0.5368085503578186
iteration 146, loss = 0.5289710760116577
iteration 147, loss = 0.48742422461509705
iteration 148, loss = 0.5147987008094788
iteration 149, loss = 0.46638619899749756
iteration 150, loss = 0.46990108489990234
iteration 151, loss = 0.48611053824424744
iteration 152, loss = 0.5200227499008179
iteration 153, loss = 0.481360524892807
iteration 154, loss = 0.4969368875026703
iteration 155, loss = 0.5604247450828552
iteration 156, loss = 0.4830666184425354
iteration 157, loss = 0.4919973909854889
iteration 158, loss = 0.5332316160202026
iteration 159, loss = 0.5152255892753601
iteration 160, loss = 0.45096978545188904
iteration 161, loss = 0.4872796833515167
iteration 162, loss = 0.4798320531845093
iteration 163, loss = 0.5834958553314209
iteration 164, loss = 0.4974059462547302
iteration 165, loss = 0.4597165882587433
iteration 166, loss = 0.49395841360092163
iteration 167, loss = 0.553781270980835
iteration 168, loss = 0.4698715806007385
iteration 169, loss = 0.4483206570148468
iteration 170, loss = 0.529309093952179
iteration 171, loss = 0.44612830877304077
iteration 172, loss = 0.5254987478256226
iteration 173, loss = 0.4724385142326355
iteration 174, loss = 0.5182403922080994
iteration 175, loss = 0.47701895236968994
iteration 176, loss = 0.47119131684303284
iteration 177, loss = 0.46949073672294617
iteration 178, loss = 0.457173228263855
iteration 179, loss = 0.4743114113807678
iteration 180, loss = 0.5390188694000244
iteration 181, loss = 0.6483098864555359
iteration 182, loss = 0.4670637249946594
iteration 183, loss = 0.45687711238861084
iteration 184, loss = 0.49377959966659546
iteration 185, loss = 0.4685015082359314
iteration 186, loss = 0.47326087951660156
iteration 187, loss = 0.4948655664920807
iteration 188, loss = 0.4790077209472656
iteration 189, loss = 0.5335570573806763
iteration 190, loss = 0.539979100227356
iteration 191, loss = 0.5710885524749756
iteration 192, loss = 0.48470982909202576
iteration 193, loss = 0.434920996427536
iteration 194, loss = 0.48895347118377686
iteration 195, loss = 0.47253936529159546
iteration 196, loss = 0.5909227132797241
iteration 197, loss = 0.44990652799606323
iteration 198, loss = 0.5118224024772644
iteration 199, loss = 0.47813352942466736
iteration 200, loss = 0.4822232127189636
iteration 201, loss = 0.48839080333709717
iteration 202, loss = 0.43928614258766174
iteration 203, loss = 0.5083839297294617
iteration 204, loss = 0.4830963909626007
iteration 205, loss = 0.49881672859191895
iteration 206, loss = 0.460673063993454
iteration 207, loss = 0.493982195854187
iteration 208, loss = 0.6125302910804749
iteration 209, loss = 0.5064275860786438
iteration 210, loss = 0.47399258613586426
iteration 211, loss = 0.6041044592857361
iteration 212, loss = 0.6248990297317505
iteration 213, loss = 0.5157177448272705
iteration 214, loss = 0.4549599885940552
iteration 215, loss = 0.4674053490161896
iteration 216, loss = 0.551349401473999
iteration 217, loss = 0.5075978636741638
iteration 218, loss = 0.5222082734107971
iteration 219, loss = 0.48044753074645996
iteration 220, loss = 0.5341219902038574
iteration 221, loss = 0.5153460502624512
iteration 222, loss = 0.5049506425857544
iteration 223, loss = 0.4659789204597473
iteration 224, loss = 0.4965965449810028
iteration 225, loss = 0.518635094165802
iteration 226, loss = 0.49884483218193054
iteration 227, loss = 0.48276638984680176
iteration 228, loss = 0.5782742500305176
iteration 229, loss = 0.5057209134101868
iteration 230, loss = 0.4840186536312103
iteration 231, loss = 0.5176482200622559
iteration 232, loss = 0.4686572253704071
iteration 233, loss = 0.475961834192276
iteration 234, loss = 0.5115066170692444
iteration 235, loss = 0.4345240890979767
iteration 236, loss = 0.5295037627220154
iteration 237, loss = 0.4658386707305908
iteration 238, loss = 0.44863542914390564
iteration 239, loss = 0.47657620906829834
iteration 240, loss = 0.45974400639533997
iteration 241, loss = 0.4902385175228119
iteration 242, loss = 0.5314021110534668
iteration 243, loss = 0.47831547260284424
iteration 244, loss = 0.5480117201805115
iteration 245, loss = 0.47742101550102234
iteration 246, loss = 0.4738885164260864
iteration 247, loss = 0.6683382987976074
iteration 248, loss = 0.503377377986908
iteration 249, loss = 0.48685190081596375
iteration 250, loss = 0.5066893696784973
iteration 251, loss = 0.4946979880332947
iteration 252, loss = 0.49710407853126526
iteration 253, loss = 0.44302746653556824
iteration 254, loss = 0.4689404368400574
iteration 255, loss = 0.49409347772598267
iteration 256, loss = 0.4953503906726837
iteration 257, loss = 0.43841320276260376
iteration 258, loss = 0.49019327759742737
iteration 259, loss = 0.5041457414627075
iteration 260, loss = 0.4681362509727478
iteration 261, loss = 0.4783116579055786
iteration 262, loss = 0.4381236433982849
iteration 263, loss = 0.49989643692970276
iteration 264, loss = 0.44053971767425537
iteration 265, loss = 0.531212329864502
iteration 266, loss = 0.4460998773574829
iteration 267, loss = 0.4982485771179199
iteration 268, loss = 0.5169796943664551
iteration 269, loss = 0.49911201000213623
iteration 270, loss = 0.4911365807056427
iteration 271, loss = 0.5115982294082642
iteration 272, loss = 0.5006091594696045
iteration 273, loss = 0.5096678733825684
iteration 274, loss = 0.48865050077438354
iteration 275, loss = 0.47206151485443115
iteration 276, loss = 0.4495646357536316
iteration 277, loss = 0.4610580503940582
iteration 278, loss = 0.44389721751213074
iteration 279, loss = 0.4494137167930603
iteration 280, loss = 0.5009081959724426
iteration 281, loss = 0.4683264493942261
iteration 282, loss = 0.48217758536338806
iteration 283, loss = 0.48701679706573486
iteration 284, loss = 0.44522374868392944
iteration 285, loss = 0.4766867756843567
iteration 286, loss = 0.4624404311180115
iteration 287, loss = 0.4611583352088928
iteration 288, loss = 0.4604758024215698
iteration 289, loss = 0.5011852383613586
iteration 290, loss = 0.4814687967300415
iteration 291, loss = 0.44873547554016113
iteration 292, loss = 0.4440174102783203
iteration 293, loss = 0.4705832600593567
iteration 294, loss = 0.5174776315689087
iteration 295, loss = 0.45559635758399963
iteration 296, loss = 0.5136712789535522
iteration 297, loss = 0.42943593859672546
iteration 298, loss = 0.4418865442276001
iteration 299, loss = 0.4865204691886902
iteration 300, loss = 0.4595452845096588
iteration 1, loss = 0.4886137545108795
iteration 2, loss = 0.4403856098651886
iteration 3, loss = 0.48668789863586426
iteration 4, loss = 0.5037336349487305
iteration 5, loss = 0.49756383895874023
iteration 6, loss = 0.4633117616176605
iteration 7, loss = 0.4864620864391327
iteration 8, loss = 0.49252456426620483
iteration 9, loss = 0.5385757088661194
iteration 10, loss = 0.43975433707237244
iteration 11, loss = 0.44840747117996216
iteration 12, loss = 0.44233253598213196
iteration 13, loss = 0.4927435517311096
iteration 14, loss = 0.4888758659362793
iteration 15, loss = 0.5052281022071838
iteration 16, loss = 0.4832705557346344
iteration 17, loss = 0.45089906454086304
iteration 18, loss = 0.5382837653160095
iteration 19, loss = 0.46426478028297424
iteration 20, loss = 0.4507571756839752
iteration 21, loss = 0.4258091151714325
iteration 22, loss = 0.594525933265686
iteration 23, loss = 0.44929659366607666
iteration 24, loss = 0.4330218434333801
iteration 25, loss = 0.48787617683410645
iteration 26, loss = 0.5971523523330688
iteration 27, loss = 0.45609214901924133
iteration 28, loss = 0.44577524065971375
iteration 29, loss = 0.5062934756278992
iteration 30, loss = 0.48590320348739624
iteration 31, loss = 0.41290000081062317
iteration 32, loss = 0.5681943893432617
iteration 33, loss = 0.44699135422706604
iteration 34, loss = 0.43418413400650024
iteration 35, loss = 0.47933459281921387
iteration 36, loss = 0.43703120946884155
iteration 37, loss = 0.47536447644233704
iteration 38, loss = 0.5205036401748657
iteration 39, loss = 0.4458126723766327
iteration 40, loss = 0.46048426628112793
iteration 41, loss = 0.5189619660377502
iteration 42, loss = 0.5011174082756042
iteration 43, loss = 0.49312281608581543
iteration 44, loss = 0.4732778072357178
iteration 45, loss = 0.4595233201980591
iteration 46, loss = 0.485881507396698
iteration 47, loss = 0.44279977679252625
iteration 48, loss = 0.5077667236328125
iteration 49, loss = 0.4669433832168579
iteration 50, loss = 0.4439333975315094
iteration 51, loss = 0.4760249853134155
iteration 52, loss = 0.4847961664199829
iteration 53, loss = 0.45105960965156555
iteration 54, loss = 0.5929851531982422
iteration 55, loss = 0.4497821033000946
iteration 56, loss = 0.4895550608634949
iteration 57, loss = 0.5043416023254395
iteration 58, loss = 0.5278593897819519
iteration 59, loss = 0.42125123739242554
iteration 60, loss = 0.42766740918159485
iteration 61, loss = 0.4964689016342163
iteration 62, loss = 0.48512139916419983
iteration 63, loss = 0.4857281446456909
iteration 64, loss = 0.428114116191864
iteration 65, loss = 0.4635164737701416
iteration 66, loss = 0.6019918918609619
iteration 67, loss = 0.4612635374069214
iteration 68, loss = 0.45867398381233215
iteration 69, loss = 0.4957166910171509
iteration 70, loss = 0.476759672164917
iteration 71, loss = 0.45245444774627686
iteration 72, loss = 0.5430229902267456
iteration 73, loss = 0.5289176106452942
iteration 74, loss = 0.4856480062007904
iteration 75, loss = 0.470612108707428
iteration 76, loss = 0.4568009078502655
iteration 77, loss = 0.44768694043159485
iteration 78, loss = 0.44503313302993774
iteration 79, loss = 0.6061156392097473
iteration 80, loss = 0.42700114846229553
iteration 81, loss = 0.4201089143753052
iteration 82, loss = 0.45582860708236694
iteration 83, loss = 0.43883219361305237
iteration 84, loss = 0.4270075261592865
iteration 85, loss = 0.46169620752334595
iteration 86, loss = 0.4530472457408905
iteration 87, loss = 0.4813117980957031
iteration 88, loss = 0.4653012156486511
iteration 89, loss = 0.46924176812171936
iteration 90, loss = 0.44420555233955383
iteration 91, loss = 0.4725205898284912
iteration 92, loss = 0.5238071084022522
iteration 93, loss = 0.47305458784103394
iteration 94, loss = 0.44536978006362915
iteration 95, loss = 0.46555420756340027
iteration 96, loss = 0.4712892770767212
iteration 97, loss = 0.4573642313480377
iteration 98, loss = 0.46190500259399414
iteration 99, loss = 0.5989960432052612
iteration 100, loss = 0.4200613498687744
iteration 101, loss = 0.48758602142333984
iteration 102, loss = 0.43011292815208435
iteration 103, loss = 0.4303242266178131
iteration 104, loss = 0.4480368494987488
iteration 105, loss = 0.4790607988834381
iteration 106, loss = 0.4684864282608032
iteration 107, loss = 0.4441949129104614
iteration 108, loss = 0.48506128787994385
iteration 109, loss = 0.4833128750324249
iteration 110, loss = 0.5046936869621277
iteration 111, loss = 0.45782461762428284
iteration 112, loss = 0.4580516219139099
iteration 113, loss = 0.484505295753479
iteration 114, loss = 0.4399731159210205
iteration 115, loss = 0.4779779314994812
iteration 116, loss = 0.4652378559112549
iteration 117, loss = 0.47182124853134155
iteration 118, loss = 0.4538078010082245
iteration 119, loss = 0.46543359756469727
iteration 120, loss = 0.5625518560409546
iteration 121, loss = 0.48414289951324463
iteration 122, loss = 0.4331388473510742
iteration 123, loss = 0.4787105321884155
iteration 124, loss = 0.4604749083518982
iteration 125, loss = 0.4404745101928711
iteration 126, loss = 0.4782727360725403
iteration 127, loss = 0.44636836647987366
iteration 128, loss = 0.4559774398803711
iteration 129, loss = 0.4731402099132538
iteration 130, loss = 0.45471346378326416
iteration 131, loss = 0.5137513875961304
iteration 132, loss = 0.49323055148124695
iteration 133, loss = 0.4365715980529785
iteration 134, loss = 0.4845401346683502
iteration 135, loss = 0.5172865390777588
iteration 136, loss = 0.4260890483856201
iteration 137, loss = 0.4620005190372467
iteration 138, loss = 0.4422982931137085
iteration 139, loss = 0.4643217623233795
iteration 140, loss = 0.5791853070259094
iteration 141, loss = 0.4671528935432434
iteration 142, loss = 0.4441777467727661
iteration 143, loss = 0.4555096924304962
iteration 144, loss = 0.44866305589675903
iteration 145, loss = 0.4698377251625061
iteration 146, loss = 0.4445600211620331
iteration 147, loss = 0.5004457831382751
iteration 148, loss = 0.6232011318206787
iteration 149, loss = 0.45554640889167786
iteration 150, loss = 0.48733359575271606
iteration 151, loss = 0.4472511112689972
iteration 152, loss = 0.4860984683036804
iteration 153, loss = 0.6011491417884827
iteration 154, loss = 0.4528355896472931
iteration 155, loss = 0.4697701632976532
iteration 156, loss = 0.48383352160453796
iteration 157, loss = 0.464570015668869
iteration 158, loss = 0.5101951956748962
iteration 159, loss = 0.4214341342449188
iteration 160, loss = 0.42560529708862305
iteration 161, loss = 0.5218936204910278
iteration 162, loss = 0.42960619926452637
iteration 163, loss = 0.45210564136505127
iteration 164, loss = 0.4236304461956024
iteration 165, loss = 0.48502272367477417
iteration 166, loss = 0.4167443513870239
iteration 167, loss = 0.4516218304634094
iteration 168, loss = 0.549498438835144
iteration 169, loss = 0.4506780803203583
iteration 170, loss = 0.430389404296875
iteration 171, loss = 0.4863976538181305
iteration 172, loss = 0.49755826592445374
iteration 173, loss = 0.4511893093585968
iteration 174, loss = 0.4308204650878906
iteration 175, loss = 0.4639938473701477
iteration 176, loss = 0.43832868337631226
iteration 177, loss = 0.4774799048900604
iteration 178, loss = 0.5072147846221924
iteration 179, loss = 0.43901491165161133
iteration 180, loss = 0.5140406489372253
iteration 181, loss = 0.42599183320999146
iteration 182, loss = 0.42067813873291016
iteration 183, loss = 0.4296530783176422
iteration 184, loss = 0.5291990041732788
iteration 185, loss = 0.6243408918380737
iteration 186, loss = 0.45644140243530273
iteration 187, loss = 0.5444954633712769
iteration 188, loss = 0.4331730306148529
iteration 189, loss = 0.4638054370880127
iteration 190, loss = 0.4527326226234436
iteration 191, loss = 0.5108932852745056
iteration 192, loss = 0.4142302870750427
iteration 193, loss = 0.48016732931137085
iteration 194, loss = 0.4502772092819214
iteration 195, loss = 0.4813435971736908
iteration 196, loss = 0.43745332956314087
iteration 197, loss = 0.4120205044746399
iteration 198, loss = 0.4291402995586395
iteration 199, loss = 0.45334184169769287
iteration 200, loss = 0.49497485160827637
iteration 201, loss = 0.4867897927761078
iteration 202, loss = 0.4720247983932495
iteration 203, loss = 0.4130658507347107
iteration 204, loss = 0.4527944326400757
iteration 205, loss = 0.46477067470550537
iteration 206, loss = 0.4836329519748688
iteration 207, loss = 0.42959901690483093
iteration 208, loss = 0.4814595878124237
iteration 209, loss = 0.5081301927566528
iteration 210, loss = 0.4890978932380676
iteration 211, loss = 0.45281487703323364
iteration 212, loss = 0.49775099754333496
iteration 213, loss = 0.4605548679828644
iteration 214, loss = 0.532944917678833
iteration 215, loss = 0.47561484575271606
iteration 216, loss = 0.4732685983181
iteration 217, loss = 0.5424498319625854
iteration 218, loss = 0.45313596725463867
iteration 219, loss = 0.4729484021663666
iteration 220, loss = 0.5051789283752441
iteration 221, loss = 0.4271872043609619
iteration 222, loss = 0.5323160886764526
iteration 223, loss = 0.4156636893749237
iteration 224, loss = 0.45073598623275757
iteration 225, loss = 0.46561533212661743
iteration 226, loss = 0.4338199198246002
iteration 227, loss = 0.45527127385139465
iteration 228, loss = 0.4177952706813812
iteration 229, loss = 0.42547160387039185
iteration 230, loss = 0.45868316292762756
iteration 231, loss = 0.48181992769241333
iteration 232, loss = 0.4843422472476959
iteration 233, loss = 0.4410839378833771
iteration 234, loss = 0.4266928434371948
iteration 235, loss = 0.40173476934432983
iteration 236, loss = 0.4589211642742157
iteration 237, loss = 0.3926977217197418
iteration 238, loss = 0.44019848108291626
iteration 239, loss = 0.4984862506389618
iteration 240, loss = 0.46574288606643677
iteration 241, loss = 0.43712902069091797
iteration 242, loss = 0.5092693567276001
iteration 243, loss = 0.44557082653045654
iteration 244, loss = 0.48838838934898376
iteration 245, loss = 0.47926977276802063
iteration 246, loss = 0.46830928325653076
iteration 247, loss = 0.4344140887260437
iteration 248, loss = 0.49712681770324707
iteration 249, loss = 0.40237390995025635
iteration 250, loss = 0.3957546353340149
iteration 251, loss = 0.4232950806617737
iteration 252, loss = 0.41211599111557007
iteration 253, loss = 0.5196001529693604
iteration 254, loss = 0.43528997898101807
iteration 255, loss = 0.4436705708503723
iteration 256, loss = 0.4613415002822876
iteration 257, loss = 0.43424466252326965
iteration 258, loss = 0.42405417561531067
iteration 259, loss = 0.4780829846858978
iteration 260, loss = 0.4878869354724884
iteration 261, loss = 0.4928781986236572
iteration 262, loss = 0.491830050945282
iteration 263, loss = 0.4289013743400574
iteration 264, loss = 0.4504178762435913
iteration 265, loss = 0.46633416414260864
iteration 266, loss = 0.5474837422370911
iteration 267, loss = 0.454096257686615
iteration 268, loss = 0.4355763792991638
iteration 269, loss = 0.42823463678359985
iteration 270, loss = 0.49722492694854736
iteration 271, loss = 0.4414634108543396
iteration 272, loss = 0.4165968596935272
iteration 273, loss = 0.44031548500061035
iteration 274, loss = 0.49186134338378906
iteration 275, loss = 0.41184529662132263
iteration 276, loss = 0.465114563703537
iteration 277, loss = 0.4509095549583435
iteration 278, loss = 0.537979245185852
iteration 279, loss = 0.47700703144073486
iteration 280, loss = 0.535068154335022
iteration 281, loss = 0.4391956329345703
iteration 282, loss = 0.4132121503353119
iteration 283, loss = 0.47505009174346924
iteration 284, loss = 0.4363898038864136
iteration 285, loss = 0.47004377841949463
iteration 286, loss = 0.44686686992645264
iteration 287, loss = 0.45901528000831604
iteration 288, loss = 0.4328867197036743
iteration 289, loss = 0.4525997042655945
iteration 290, loss = 0.6041039228439331
iteration 291, loss = 0.5981817245483398
iteration 292, loss = 0.463820219039917
iteration 293, loss = 0.4489402770996094
iteration 294, loss = 0.4208971858024597
iteration 295, loss = 0.4950478672981262
iteration 296, loss = 0.48082464933395386
iteration 297, loss = 0.44891586899757385
iteration 298, loss = 0.4578459858894348
iteration 299, loss = 0.4125095009803772
iteration 300, loss = 0.42724964022636414
iteration 1, loss = 0.48618581891059875
iteration 2, loss = 0.4721166789531708
iteration 3, loss = 0.48166459798812866
iteration 4, loss = 0.5783112645149231
iteration 5, loss = 0.4197581708431244
iteration 6, loss = 0.4239133596420288
iteration 7, loss = 0.4531969428062439
iteration 8, loss = 0.38735610246658325
iteration 9, loss = 0.5357725620269775
iteration 10, loss = 0.46220967173576355
iteration 11, loss = 0.4076361060142517
iteration 12, loss = 0.4081975519657135
iteration 13, loss = 0.4129300117492676
iteration 14, loss = 0.43685203790664673
iteration 15, loss = 0.4407721757888794
iteration 16, loss = 0.4357336163520813
iteration 17, loss = 0.46205270290374756
iteration 18, loss = 0.4380669593811035
iteration 19, loss = 0.4595347046852112
iteration 20, loss = 0.42133772373199463
iteration 21, loss = 0.491800993680954
iteration 22, loss = 0.45256078243255615
iteration 23, loss = 0.431095153093338
iteration 24, loss = 0.4482649266719818
iteration 25, loss = 0.44496405124664307
iteration 26, loss = 0.48706215620040894
iteration 27, loss = 0.4293293058872223
iteration 28, loss = 0.47839871048927307
iteration 29, loss = 0.4091740846633911
iteration 30, loss = 0.447321355342865
iteration 31, loss = 0.4837494492530823
iteration 32, loss = 0.5027419924736023
iteration 33, loss = 0.4231530427932739
iteration 34, loss = 0.580706000328064
iteration 35, loss = 0.42574524879455566
iteration 36, loss = 0.4179822504520416
iteration 37, loss = 0.44668126106262207
iteration 38, loss = 0.41235604882240295
iteration 39, loss = 0.4192166328430176
iteration 40, loss = 0.45626962184906006
iteration 41, loss = 0.4477428197860718
iteration 42, loss = 0.4451640248298645
iteration 43, loss = 0.4519897699356079
iteration 44, loss = 0.3838624060153961
iteration 45, loss = 0.43567731976509094
iteration 46, loss = 0.44934409856796265
iteration 47, loss = 0.4034174978733063
iteration 48, loss = 0.4602247476577759
iteration 49, loss = 0.44202664494514465
iteration 50, loss = 0.45297837257385254
iteration 51, loss = 0.5487192869186401
iteration 52, loss = 0.3909146785736084
iteration 53, loss = 0.43580162525177
iteration 54, loss = 0.49145400524139404
iteration 55, loss = 0.46498093008995056
iteration 56, loss = 0.45923876762390137
iteration 57, loss = 0.4251087009906769
iteration 58, loss = 0.47859251499176025
iteration 59, loss = 0.44058549404144287
iteration 60, loss = 0.40735211968421936
iteration 61, loss = 0.4397072196006775
iteration 62, loss = 0.45374631881713867
iteration 63, loss = 0.4630035161972046
iteration 64, loss = 0.4172195494174957
iteration 65, loss = 0.5206016898155212
iteration 66, loss = 0.4724249839782715
iteration 67, loss = 0.476884126663208
iteration 68, loss = 0.4416637420654297
iteration 69, loss = 0.5098313093185425
iteration 70, loss = 0.3936960995197296
iteration 71, loss = 0.4267311990261078
iteration 72, loss = 0.4720413088798523
iteration 73, loss = 0.43532633781433105
iteration 74, loss = 0.4261191487312317
iteration 75, loss = 0.4991759657859802
iteration 76, loss = 0.44549643993377686
iteration 77, loss = 0.44219452142715454
iteration 78, loss = 0.4451570212841034
iteration 79, loss = 0.46640270948410034
iteration 80, loss = 0.39501452445983887
iteration 81, loss = 0.43886053562164307
iteration 82, loss = 0.4473811388015747
iteration 83, loss = 0.4503101706504822
iteration 84, loss = 0.5051953792572021
iteration 85, loss = 0.3995436131954193
iteration 86, loss = 0.5105255246162415
iteration 87, loss = 0.4494316279888153
iteration 88, loss = 0.4715383052825928
iteration 89, loss = 0.461773157119751
iteration 90, loss = 0.4749069809913635
iteration 91, loss = 0.4386315643787384
iteration 92, loss = 0.4685542583465576
iteration 93, loss = 0.4069298505783081
iteration 94, loss = 0.4174937903881073
iteration 95, loss = 0.41438156366348267
iteration 96, loss = 0.5288395285606384
iteration 97, loss = 0.4454352855682373
iteration 98, loss = 0.518971860408783
iteration 99, loss = 0.4718054533004761
iteration 100, loss = 0.4680839776992798
iteration 101, loss = 0.47216975688934326
iteration 102, loss = 0.5743846893310547
iteration 103, loss = 0.5134987831115723
iteration 104, loss = 0.4217906892299652
iteration 105, loss = 0.4766077995300293
iteration 106, loss = 0.41721561551094055
iteration 107, loss = 0.43175429105758667
iteration 108, loss = 0.5782572627067566
iteration 109, loss = 0.46047377586364746
iteration 110, loss = 0.43584150075912476
iteration 111, loss = 0.41337406635284424
iteration 112, loss = 0.4331183433532715
iteration 113, loss = 0.40924373269081116
iteration 114, loss = 0.5457538366317749
iteration 115, loss = 0.47959741950035095
iteration 116, loss = 0.39946264028549194
iteration 117, loss = 0.3984723687171936
iteration 118, loss = 0.4720919132232666
iteration 119, loss = 0.442579984664917
iteration 120, loss = 0.4683684706687927
iteration 121, loss = 0.4082894027233124
iteration 122, loss = 0.4456979036331177
iteration 123, loss = 0.3973888158798218
iteration 124, loss = 0.45057380199432373
iteration 125, loss = 0.4285159111022949
iteration 126, loss = 0.41926109790802
iteration 127, loss = 0.4356810450553894
iteration 128, loss = 0.5643098950386047
iteration 129, loss = 0.43177953362464905
iteration 130, loss = 0.4217767119407654
iteration 131, loss = 0.4110516905784607
iteration 132, loss = 0.4337608516216278
iteration 133, loss = 0.4159209728240967
iteration 134, loss = 0.4383455514907837
iteration 135, loss = 0.45834359526634216
iteration 136, loss = 0.4063277840614319
iteration 137, loss = 0.4915657639503479
iteration 138, loss = 0.43965911865234375
iteration 139, loss = 0.4852345287799835
iteration 140, loss = 0.45200541615486145
iteration 141, loss = 0.4370347261428833
iteration 142, loss = 0.472212553024292
iteration 143, loss = 0.5456079244613647
iteration 144, loss = 0.44020771980285645
iteration 145, loss = 0.4415946304798126
iteration 146, loss = 0.39422935247421265
iteration 147, loss = 0.4052214026451111
iteration 148, loss = 0.4210476875305176
iteration 149, loss = 0.4219002425670624
iteration 150, loss = 0.42610764503479004
iteration 151, loss = 0.3876115083694458
iteration 152, loss = 0.42933422327041626
iteration 153, loss = 0.4281638562679291
iteration 154, loss = 0.4486151933670044
iteration 155, loss = 0.435099333524704
iteration 156, loss = 0.4267539978027344
iteration 157, loss = 0.44028201699256897
iteration 158, loss = 0.4480171203613281
iteration 159, loss = 0.41386449337005615
iteration 160, loss = 0.5180758237838745
iteration 161, loss = 0.3934768736362457
iteration 162, loss = 0.44548046588897705
iteration 163, loss = 0.40448421239852905
iteration 164, loss = 0.42474958300590515
iteration 165, loss = 0.4239993691444397
iteration 166, loss = 0.45294663310050964
iteration 167, loss = 0.4331883490085602
iteration 168, loss = 0.4184717535972595
iteration 169, loss = 0.4596109986305237
iteration 170, loss = 0.42974936962127686
iteration 171, loss = 0.4424249529838562
iteration 172, loss = 0.39414188265800476
iteration 173, loss = 0.43073365092277527
iteration 174, loss = 0.44265198707580566
iteration 175, loss = 0.40048450231552124
iteration 176, loss = 0.38697242736816406
iteration 177, loss = 0.4783308506011963
iteration 178, loss = 0.36752092838287354
iteration 179, loss = 0.42264631390571594
iteration 180, loss = 0.4056527018547058
iteration 181, loss = 0.5838602185249329
iteration 182, loss = 0.40993136167526245
iteration 183, loss = 0.4402458369731903
iteration 184, loss = 0.4817083775997162
iteration 185, loss = 0.3978314995765686
iteration 186, loss = 0.417428195476532
iteration 187, loss = 0.4260355830192566
iteration 188, loss = 0.39959949254989624
iteration 189, loss = 0.4581696391105652
iteration 190, loss = 0.41558754444122314
iteration 191, loss = 0.39680415391921997
iteration 192, loss = 0.41455531120300293
iteration 193, loss = 0.40916815400123596
iteration 194, loss = 0.4182199239730835
iteration 195, loss = 0.4068610668182373
iteration 196, loss = 0.4195946753025055
iteration 197, loss = 0.4228437840938568
iteration 198, loss = 0.41827401518821716
iteration 199, loss = 0.43647849559783936
iteration 200, loss = 0.41839224100112915
iteration 201, loss = 0.5529696941375732
iteration 202, loss = 0.37833327054977417
iteration 203, loss = 0.4250880479812622
iteration 204, loss = 0.4335968494415283
iteration 205, loss = 0.40255850553512573
iteration 206, loss = 0.414521187543869
iteration 207, loss = 0.41735172271728516
iteration 208, loss = 0.5621381402015686
iteration 209, loss = 0.4044347405433655
iteration 210, loss = 0.39632055163383484
iteration 211, loss = 0.4428193271160126
iteration 212, loss = 0.5115277767181396
iteration 213, loss = 0.4668695628643036
iteration 214, loss = 0.46716415882110596
iteration 215, loss = 0.4417729675769806
iteration 216, loss = 0.43025147914886475
iteration 217, loss = 0.4048222601413727
iteration 218, loss = 0.42503970861434937
iteration 219, loss = 0.6003506183624268
iteration 220, loss = 0.4181167483329773
iteration 221, loss = 0.44606465101242065
iteration 222, loss = 0.43296194076538086
iteration 223, loss = 0.42270663380622864
iteration 224, loss = 0.3883344829082489
iteration 225, loss = 0.4217948615550995
iteration 226, loss = 0.429875910282135
iteration 227, loss = 0.562184751033783
iteration 228, loss = 0.39684686064720154
iteration 229, loss = 0.4425957202911377
iteration 230, loss = 0.39900439977645874
iteration 231, loss = 0.42342546582221985
iteration 232, loss = 0.4500466287136078
iteration 233, loss = 0.4046477675437927
iteration 234, loss = 0.43459293246269226
iteration 235, loss = 0.38264673948287964
iteration 236, loss = 0.4116966426372528
iteration 237, loss = 0.40924757719039917
iteration 238, loss = 0.4276736080646515
iteration 239, loss = 0.40784502029418945
iteration 240, loss = 0.4365538954734802
iteration 241, loss = 0.45550304651260376
iteration 242, loss = 0.40230026841163635
iteration 243, loss = 0.42157045006752014
iteration 244, loss = 0.39446598291397095
iteration 245, loss = 0.40059608221054077
iteration 246, loss = 0.47730448842048645
iteration 247, loss = 0.3903619647026062
iteration 248, loss = 0.433662474155426
iteration 249, loss = 0.41756975650787354
iteration 250, loss = 0.4323906898498535
iteration 251, loss = 0.45360273122787476
iteration 252, loss = 0.5763230323791504
iteration 253, loss = 0.3957774341106415
iteration 254, loss = 0.4705978035926819
iteration 255, loss = 0.3989497423171997
iteration 256, loss = 0.4581056237220764
iteration 257, loss = 0.42795681953430176
iteration 258, loss = 0.4047788381576538
iteration 259, loss = 0.5495767593383789
iteration 260, loss = 0.4040263891220093
iteration 261, loss = 0.39310717582702637
iteration 262, loss = 0.40690502524375916
iteration 263, loss = 0.37132301926612854
iteration 264, loss = 0.4276043772697449
iteration 265, loss = 0.41965746879577637
iteration 266, loss = 0.44374746084213257
iteration 267, loss = 0.396895170211792
iteration 268, loss = 0.41780009865760803
iteration 269, loss = 0.45492255687713623
iteration 270, loss = 0.41812384128570557
iteration 271, loss = 0.40482908487319946
iteration 272, loss = 0.4123992919921875
iteration 273, loss = 0.42183783650398254
iteration 274, loss = 0.43834006786346436
iteration 275, loss = 0.55391925573349
iteration 276, loss = 0.4254496991634369
iteration 277, loss = 0.4224250614643097
iteration 278, loss = 0.4000642001628876
iteration 279, loss = 0.4020118713378906
iteration 280, loss = 0.4241872727870941
iteration 281, loss = 0.41786307096481323
iteration 282, loss = 0.4196966290473938
iteration 283, loss = 0.44017738103866577
iteration 284, loss = 0.41305530071258545
iteration 285, loss = 0.4138537049293518
iteration 286, loss = 0.410530149936676
iteration 287, loss = 0.4178183674812317
iteration 288, loss = 0.41623133420944214
iteration 289, loss = 0.42107605934143066
iteration 290, loss = 0.4839121699333191
iteration 291, loss = 0.38962268829345703
iteration 292, loss = 0.379420667886734
iteration 293, loss = 0.4342133402824402
iteration 294, loss = 0.38844501972198486
iteration 295, loss = 0.41184014081954956
iteration 296, loss = 0.4305815100669861
iteration 297, loss = 0.3897620737552643
iteration 298, loss = 0.3868272602558136
iteration 299, loss = 0.44060879945755005
iteration 300, loss = 0.44850534200668335

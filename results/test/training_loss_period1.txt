iteration 0, loss = 0.5001042485237122
iteration 1, loss = 0.49172788858413696
iteration 2, loss = 0.5002864003181458
iteration 3, loss = 0.4981860816478729
iteration 4, loss = 0.4992794990539551
iteration 5, loss = 0.4961804747581482
iteration 6, loss = 0.4973640441894531
iteration 7, loss = 0.4966520667076111
iteration 8, loss = 0.4981367886066437
iteration 9, loss = 0.49823111295700073
iteration 10, loss = 0.49829551577568054
iteration 11, loss = 0.4930226802825928
iteration 12, loss = 0.49883514642715454
iteration 13, loss = 0.4926709532737732
iteration 14, loss = 0.4944928288459778
iteration 15, loss = 0.4981856942176819
iteration 16, loss = 0.4977623224258423
iteration 17, loss = 0.4976224899291992
iteration 18, loss = 0.4961671233177185
iteration 19, loss = 0.49477964639663696
iteration 20, loss = 0.5003786087036133
iteration 21, loss = 0.4973691999912262
iteration 22, loss = 0.493726909160614
iteration 23, loss = 0.4975350499153137
iteration 24, loss = 0.49984872341156006
iteration 25, loss = 0.5013180375099182
iteration 26, loss = 0.49382612109184265
iteration 27, loss = 0.49773597717285156
iteration 28, loss = 0.49682244658470154
iteration 29, loss = 0.4977495074272156
iteration 30, loss = 0.491998553276062
iteration 31, loss = 0.4953661859035492
iteration 32, loss = 0.4949966073036194
iteration 33, loss = 0.4992411136627197
iteration 34, loss = 0.4937644600868225
iteration 35, loss = 0.4932715594768524
iteration 36, loss = 0.4986204504966736
iteration 37, loss = 0.49642395973205566
iteration 38, loss = 0.4956100583076477
iteration 39, loss = 0.4990101754665375
iteration 40, loss = 0.4968617558479309
iteration 41, loss = 0.49798649549484253
iteration 42, loss = 0.49540430307388306
iteration 43, loss = 0.4958600401878357
iteration 44, loss = 0.4952475428581238
iteration 45, loss = 0.49927401542663574
iteration 46, loss = 0.49425816535949707
iteration 47, loss = 0.49787086248397827
iteration 48, loss = 0.49278023838996887
iteration 49, loss = 0.4931521415710449
iteration 50, loss = 0.4953575134277344
iteration 51, loss = 0.4941670000553131
iteration 52, loss = 0.5003169178962708
iteration 53, loss = 0.49614471197128296
iteration 54, loss = 0.49496322870254517
iteration 55, loss = 0.49369993805885315
iteration 56, loss = 0.49704062938690186
iteration 57, loss = 0.4922339916229248
iteration 58, loss = 0.4959219694137573
iteration 59, loss = 0.4953679144382477
iteration 60, loss = 0.4936845898628235
iteration 61, loss = 0.5006364583969116
iteration 62, loss = 0.4938995838165283
iteration 63, loss = 0.49613726139068604
iteration 64, loss = 0.48961472511291504
iteration 65, loss = 0.4937962591648102
iteration 66, loss = 0.49631768465042114
iteration 67, loss = 0.49494239687919617
iteration 68, loss = 0.4926731586456299
iteration 69, loss = 0.4951498210430145
iteration 70, loss = 0.5008326768875122
iteration 71, loss = 0.4941824972629547
iteration 72, loss = 0.4966830611228943
iteration 73, loss = 0.49433204531669617
iteration 74, loss = 0.49591735005378723
iteration 75, loss = 0.49792104959487915
iteration 76, loss = 0.49345898628234863
iteration 77, loss = 0.49552738666534424
iteration 78, loss = 0.48822295665740967
iteration 79, loss = 0.49860092997550964
iteration 80, loss = 0.49469006061553955
iteration 81, loss = 0.48971930146217346
iteration 82, loss = 0.4928464889526367
iteration 83, loss = 0.4946645498275757
iteration 84, loss = 0.49596887826919556
iteration 85, loss = 0.4935486614704132
iteration 86, loss = 0.49925071001052856
iteration 87, loss = 0.4946303963661194
iteration 88, loss = 0.4965082108974457
iteration 89, loss = 0.4961279034614563
iteration 90, loss = 0.49337923526763916
iteration 91, loss = 0.48808902502059937
iteration 92, loss = 0.49057552218437195
iteration 93, loss = 0.4929318428039551
iteration 94, loss = 0.49618521332740784
iteration 95, loss = 0.4948693811893463
iteration 96, loss = 0.49485594034194946
iteration 97, loss = 0.4933398962020874
iteration 98, loss = 0.4959041476249695
iteration 99, loss = 0.4994240403175354
iteration 100, loss = 0.4950055181980133
iteration 101, loss = 0.4909154176712036
iteration 102, loss = 0.4970206618309021
iteration 103, loss = 0.4939952492713928
iteration 104, loss = 0.4891703128814697
iteration 105, loss = 0.4933995008468628
iteration 106, loss = 0.49628275632858276
iteration 107, loss = 0.49149587750434875
iteration 108, loss = 0.4929342269897461
iteration 109, loss = 0.4978845715522766
iteration 110, loss = 0.4854468107223511
iteration 111, loss = 0.4894741177558899
iteration 112, loss = 0.49289670586586
iteration 113, loss = 0.49117588996887207
iteration 114, loss = 0.4915195107460022
iteration 115, loss = 0.49351516366004944
iteration 116, loss = 0.4988669753074646
iteration 117, loss = 0.4945186376571655
iteration 118, loss = 0.48984000086784363
iteration 119, loss = 0.4955679476261139
iteration 120, loss = 0.49390846490859985
iteration 121, loss = 0.48930004239082336
iteration 122, loss = 0.49324584007263184
iteration 123, loss = 0.4911328852176666
iteration 124, loss = 0.49246668815612793
iteration 125, loss = 0.4950258135795593
iteration 126, loss = 0.4941180646419525
iteration 127, loss = 0.4886589050292969
iteration 128, loss = 0.4944794178009033
iteration 129, loss = 0.49381023645401
iteration 130, loss = 0.49126672744750977
iteration 131, loss = 0.49641311168670654
iteration 132, loss = 0.4943297803401947
iteration 133, loss = 0.4984225630760193
iteration 134, loss = 0.48958492279052734
iteration 135, loss = 0.4913982152938843
iteration 136, loss = 0.49410319328308105
iteration 137, loss = 0.4866456985473633
iteration 138, loss = 0.4948652386665344
iteration 139, loss = 0.49185651540756226
iteration 140, loss = 0.48843395709991455
iteration 141, loss = 0.4913340210914612
iteration 142, loss = 0.49239587783813477
iteration 143, loss = 0.4901122450828552
iteration 144, loss = 0.4913882911205292
iteration 145, loss = 0.4963754415512085
iteration 146, loss = 0.489834189414978
iteration 147, loss = 0.49081701040267944
iteration 148, loss = 0.4901875853538513
iteration 149, loss = 0.49382704496383667
iteration 150, loss = 0.4844226837158203
iteration 151, loss = 0.49008727073669434
iteration 152, loss = 0.4905679523944855
iteration 153, loss = 0.4957383871078491
iteration 154, loss = 0.49126142263412476
iteration 155, loss = 0.4917474389076233
iteration 156, loss = 0.4903522729873657
iteration 157, loss = 0.4923929274082184
iteration 158, loss = 0.4875085949897766
iteration 159, loss = 0.4940733015537262
iteration 160, loss = 0.486684113740921
iteration 161, loss = 0.48865270614624023
iteration 162, loss = 0.49257999658584595
iteration 163, loss = 0.4964955151081085
iteration 164, loss = 0.48982691764831543
iteration 165, loss = 0.48541510105133057
iteration 166, loss = 0.487831711769104
iteration 167, loss = 0.4887966811656952
iteration 168, loss = 0.4918844997882843
iteration 169, loss = 0.4903649091720581
iteration 170, loss = 0.4915425181388855
iteration 171, loss = 0.49241042137145996
iteration 172, loss = 0.49432623386383057
iteration 173, loss = 0.4910450875759125
iteration 174, loss = 0.4923721253871918
iteration 175, loss = 0.4922202229499817
iteration 176, loss = 0.49086010456085205
iteration 177, loss = 0.4928508698940277
iteration 178, loss = 0.48881977796554565
iteration 179, loss = 0.49196958541870117
iteration 180, loss = 0.48552390933036804
iteration 181, loss = 0.4938428997993469
iteration 182, loss = 0.49027860164642334
iteration 183, loss = 0.48742276430130005
iteration 184, loss = 0.4901176691055298
iteration 185, loss = 0.4876745939254761
iteration 186, loss = 0.49113690853118896
iteration 187, loss = 0.4904854893684387
iteration 188, loss = 0.48981773853302
iteration 189, loss = 0.49208924174308777
iteration 190, loss = 0.49132007360458374
iteration 191, loss = 0.49265244603157043
iteration 192, loss = 0.48323702812194824
iteration 193, loss = 0.4940377473831177
iteration 194, loss = 0.486148476600647
iteration 195, loss = 0.48811253905296326
iteration 196, loss = 0.49234452843666077
iteration 197, loss = 0.4911355674266815
iteration 198, loss = 0.48413509130477905
iteration 199, loss = 0.4906034469604492
iteration 200, loss = 0.49225229024887085
iteration 201, loss = 0.48668116331100464
iteration 202, loss = 0.48607873916625977
iteration 203, loss = 0.489646315574646
iteration 204, loss = 0.4886178970336914
iteration 205, loss = 0.4907178282737732
iteration 206, loss = 0.4878087043762207
iteration 207, loss = 0.4926930069923401
iteration 208, loss = 0.4909375309944153
iteration 209, loss = 0.49012622237205505
iteration 210, loss = 0.4839383363723755
iteration 211, loss = 0.48549866676330566
iteration 212, loss = 0.48406732082366943
iteration 213, loss = 0.4912840723991394
iteration 214, loss = 0.4901115596294403
iteration 215, loss = 0.48672324419021606
iteration 216, loss = 0.49130165576934814
iteration 217, loss = 0.4917582869529724
iteration 218, loss = 0.4920579195022583
iteration 219, loss = 0.4909748435020447
iteration 220, loss = 0.4924778342247009
iteration 221, loss = 0.49021750688552856
iteration 222, loss = 0.48712337017059326
iteration 223, loss = 0.49036553502082825
iteration 224, loss = 0.49010974168777466
iteration 225, loss = 0.4918643832206726
iteration 226, loss = 0.4903987944126129
iteration 227, loss = 0.4849521815776825
iteration 228, loss = 0.4872603416442871
iteration 229, loss = 0.48096615076065063
iteration 230, loss = 0.4871909022331238
iteration 231, loss = 0.48085007071495056
iteration 232, loss = 0.48786360025405884
iteration 233, loss = 0.48825860023498535
iteration 234, loss = 0.48040950298309326
iteration 235, loss = 0.4839918315410614
iteration 236, loss = 0.4846838414669037
iteration 237, loss = 0.4843648672103882
iteration 238, loss = 0.48810601234436035
iteration 239, loss = 0.4891016483306885
iteration 240, loss = 0.49063676595687866
iteration 241, loss = 0.4898778796195984
iteration 242, loss = 0.4857107400894165
iteration 243, loss = 0.48021602630615234
iteration 244, loss = 0.4883153736591339
iteration 245, loss = 0.4852796792984009
iteration 246, loss = 0.48521092534065247
iteration 247, loss = 0.49068117141723633
iteration 248, loss = 0.4890129566192627
iteration 249, loss = 0.4866791367530823
iteration 250, loss = 0.4928702116012573
iteration 251, loss = 0.48343250155448914
iteration 252, loss = 0.48988986015319824
iteration 253, loss = 0.4870602488517761
iteration 254, loss = 0.48316967487335205
iteration 255, loss = 0.4823061525821686
iteration 256, loss = 0.49112075567245483
iteration 257, loss = 0.48886948823928833
iteration 258, loss = 0.48412585258483887
iteration 259, loss = 0.4840174913406372
iteration 260, loss = 0.48843854665756226
iteration 261, loss = 0.4830082058906555
iteration 262, loss = 0.4870336055755615
iteration 263, loss = 0.48839104175567627
iteration 264, loss = 0.4822574555873871
iteration 265, loss = 0.48284614086151123
iteration 266, loss = 0.4834684729576111
iteration 267, loss = 0.48407745361328125
iteration 268, loss = 0.4884185492992401
iteration 269, loss = 0.4865131676197052
iteration 270, loss = 0.48544949293136597
iteration 271, loss = 0.484671950340271
iteration 272, loss = 0.48353540897369385
iteration 273, loss = 0.49133506417274475
iteration 274, loss = 0.4882051348686218
iteration 275, loss = 0.4895973205566406
iteration 276, loss = 0.48715394735336304
iteration 277, loss = 0.483894944190979
iteration 278, loss = 0.4844667315483093
iteration 279, loss = 0.485343873500824
iteration 280, loss = 0.4869356155395508
iteration 281, loss = 0.4842512607574463
iteration 282, loss = 0.4827283024787903
iteration 283, loss = 0.48648786544799805
iteration 284, loss = 0.48327285051345825
iteration 285, loss = 0.48750039935112
iteration 286, loss = 0.48714131116867065
iteration 287, loss = 0.4873740077018738
iteration 288, loss = 0.4872702360153198
iteration 289, loss = 0.48462730646133423
iteration 290, loss = 0.48810893297195435
iteration 291, loss = 0.48664718866348267
iteration 292, loss = 0.48787033557891846
iteration 293, loss = 0.48840534687042236
iteration 294, loss = 0.4832533597946167
iteration 295, loss = 0.48382797837257385
iteration 296, loss = 0.4889240860939026
iteration 297, loss = 0.48242369294166565
iteration 298, loss = 0.48373210430145264
iteration 299, loss = 0.4758490025997162
iteration 0, loss = 0.49075695872306824
iteration 1, loss = 0.4839020371437073
iteration 2, loss = 0.481571763753891
iteration 3, loss = 0.48981085419654846
iteration 4, loss = 0.48416638374328613
iteration 5, loss = 0.4825733006000519
iteration 6, loss = 0.48173224925994873
iteration 7, loss = 0.48671603202819824
iteration 8, loss = 0.47536543011665344
iteration 9, loss = 0.4870157241821289
iteration 10, loss = 0.48087385296821594
iteration 11, loss = 0.4834487736225128
iteration 12, loss = 0.48217859864234924
iteration 13, loss = 0.48526468873023987
iteration 14, loss = 0.4810742735862732
iteration 15, loss = 0.48463717103004456
iteration 16, loss = 0.48590216040611267
iteration 17, loss = 0.47691914439201355
iteration 18, loss = 0.4847055673599243
iteration 19, loss = 0.484109103679657
iteration 20, loss = 0.47940123081207275
iteration 21, loss = 0.4864498972892761
iteration 22, loss = 0.4860924482345581
iteration 23, loss = 0.4833313822746277
iteration 24, loss = 0.48352113366127014
iteration 25, loss = 0.4839100241661072
iteration 26, loss = 0.4812065362930298
iteration 27, loss = 0.48412519693374634
iteration 28, loss = 0.48495012521743774
iteration 29, loss = 0.47799769043922424
iteration 30, loss = 0.4913598299026489
iteration 31, loss = 0.485163152217865
iteration 32, loss = 0.4807452857494354
iteration 33, loss = 0.4817713499069214
iteration 34, loss = 0.4825984835624695
iteration 35, loss = 0.48228293657302856
iteration 36, loss = 0.4869634211063385
iteration 37, loss = 0.47628462314605713
iteration 38, loss = 0.4809669256210327
iteration 39, loss = 0.48356863856315613
iteration 40, loss = 0.47842979431152344
iteration 41, loss = 0.4849562346935272
iteration 42, loss = 0.47982627153396606
iteration 43, loss = 0.48665887117385864
iteration 44, loss = 0.48041218519210815
iteration 45, loss = 0.4869017004966736
iteration 46, loss = 0.4807316064834595
iteration 47, loss = 0.48422229290008545
iteration 48, loss = 0.4830089211463928
iteration 49, loss = 0.4839878976345062
iteration 50, loss = 0.4852646589279175
iteration 51, loss = 0.475867360830307
iteration 52, loss = 0.4828770160675049
iteration 53, loss = 0.4861108660697937
iteration 54, loss = 0.47691285610198975
iteration 55, loss = 0.4871698021888733
iteration 56, loss = 0.4817582368850708
iteration 57, loss = 0.4794348478317261
iteration 58, loss = 0.47402453422546387
iteration 59, loss = 0.48421764373779297
iteration 60, loss = 0.48279163241386414
iteration 61, loss = 0.47766149044036865
iteration 62, loss = 0.4802522659301758
iteration 63, loss = 0.48485681414604187
iteration 64, loss = 0.4856795072555542
iteration 65, loss = 0.4894600510597229
iteration 66, loss = 0.47812819480895996
iteration 67, loss = 0.4838615357875824
iteration 68, loss = 0.48045143485069275
iteration 69, loss = 0.4820646643638611
iteration 70, loss = 0.48267555236816406
iteration 71, loss = 0.4813677668571472
iteration 72, loss = 0.48191869258880615
iteration 73, loss = 0.4777717590332031
iteration 74, loss = 0.4860183596611023
iteration 75, loss = 0.47639015316963196
iteration 76, loss = 0.4827520251274109
iteration 77, loss = 0.4805099070072174
iteration 78, loss = 0.47979527711868286
iteration 79, loss = 0.4797038435935974
iteration 80, loss = 0.4806722104549408
iteration 81, loss = 0.4802958369255066
iteration 82, loss = 0.48830360174179077
iteration 83, loss = 0.48101282119750977
iteration 84, loss = 0.48258310556411743
iteration 85, loss = 0.47860169410705566
iteration 86, loss = 0.4778658449649811
iteration 87, loss = 0.4807104170322418
iteration 88, loss = 0.47913533449172974
iteration 89, loss = 0.4792439639568329
iteration 90, loss = 0.477477490901947
iteration 91, loss = 0.47801125049591064
iteration 92, loss = 0.4769909083843231
iteration 93, loss = 0.48100247979164124
iteration 94, loss = 0.484030544757843
iteration 95, loss = 0.48258745670318604
iteration 96, loss = 0.4831576943397522
iteration 97, loss = 0.4853326678276062
iteration 98, loss = 0.4817330241203308
iteration 99, loss = 0.4788271188735962
iteration 100, loss = 0.4816647171974182
iteration 101, loss = 0.4791778326034546
iteration 102, loss = 0.4766770899295807
iteration 103, loss = 0.4762309789657593
iteration 104, loss = 0.4820491075515747
iteration 105, loss = 0.4820566177368164
iteration 106, loss = 0.48129427433013916
iteration 107, loss = 0.46735474467277527
iteration 108, loss = 0.478960245847702
iteration 109, loss = 0.47494325041770935
iteration 110, loss = 0.48374563455581665
iteration 111, loss = 0.4757953882217407
iteration 112, loss = 0.47870004177093506
iteration 113, loss = 0.47993525862693787
iteration 114, loss = 0.4786902666091919
iteration 115, loss = 0.48274409770965576
iteration 116, loss = 0.4774281978607178
iteration 117, loss = 0.479819655418396
iteration 118, loss = 0.4802200198173523
iteration 119, loss = 0.4819698929786682
iteration 120, loss = 0.4791315793991089
iteration 121, loss = 0.48749685287475586
iteration 122, loss = 0.4766872525215149
iteration 123, loss = 0.4750744104385376
iteration 124, loss = 0.4711112678050995
iteration 125, loss = 0.47049885988235474
iteration 126, loss = 0.47707104682922363
iteration 127, loss = 0.4797666668891907
iteration 128, loss = 0.4806320369243622
iteration 129, loss = 0.48156553506851196
iteration 130, loss = 0.4805567264556885
iteration 131, loss = 0.48123884201049805
iteration 132, loss = 0.4817475974559784
iteration 133, loss = 0.48280778527259827
iteration 134, loss = 0.47618263959884644
iteration 135, loss = 0.4750194251537323
iteration 136, loss = 0.4810199737548828
iteration 137, loss = 0.4750291705131531
iteration 138, loss = 0.4695708155632019
iteration 139, loss = 0.48215028643608093
iteration 140, loss = 0.4857352375984192
iteration 141, loss = 0.4773448705673218
iteration 142, loss = 0.4782763719558716
iteration 143, loss = 0.47729215025901794
iteration 144, loss = 0.479428768157959
iteration 145, loss = 0.47555071115493774
iteration 146, loss = 0.4716670513153076
iteration 147, loss = 0.47861599922180176
iteration 148, loss = 0.4782666563987732
iteration 149, loss = 0.475829541683197
iteration 150, loss = 0.4716709554195404
iteration 151, loss = 0.4709065556526184
iteration 152, loss = 0.4776604473590851
iteration 153, loss = 0.4739241600036621
iteration 154, loss = 0.4762720465660095
iteration 155, loss = 0.4725586175918579
iteration 156, loss = 0.4764120280742645
iteration 157, loss = 0.47753751277923584
iteration 158, loss = 0.4732547104358673
iteration 159, loss = 0.47842761874198914
iteration 160, loss = 0.4820755422115326
iteration 161, loss = 0.47631388902664185
iteration 162, loss = 0.482366681098938
iteration 163, loss = 0.4733814597129822
iteration 164, loss = 0.47080177068710327
iteration 165, loss = 0.4672064185142517
iteration 166, loss = 0.4796717166900635
iteration 167, loss = 0.48085740208625793
iteration 168, loss = 0.4654995799064636
iteration 169, loss = 0.47635138034820557
iteration 170, loss = 0.47380495071411133
iteration 171, loss = 0.47577303647994995
iteration 172, loss = 0.4701087772846222
iteration 173, loss = 0.47678518295288086
iteration 174, loss = 0.47544151544570923
iteration 175, loss = 0.4806022346019745
iteration 176, loss = 0.47289174795150757
iteration 177, loss = 0.4822547435760498
iteration 178, loss = 0.47536250948905945
iteration 179, loss = 0.47825944423675537
iteration 180, loss = 0.4704023599624634
iteration 181, loss = 0.4776340126991272
iteration 182, loss = 0.4758104681968689
iteration 183, loss = 0.47638460993766785
iteration 184, loss = 0.4759482741355896
iteration 185, loss = 0.47581541538238525
iteration 186, loss = 0.47061794996261597
iteration 187, loss = 0.47559356689453125
iteration 188, loss = 0.4709838628768921
iteration 189, loss = 0.48066481947898865
iteration 190, loss = 0.47709086537361145
iteration 191, loss = 0.47626903653144836
iteration 192, loss = 0.4757075309753418
iteration 193, loss = 0.4723479747772217
iteration 194, loss = 0.480662077665329
iteration 195, loss = 0.46597450971603394
iteration 196, loss = 0.4726732075214386
iteration 197, loss = 0.4750581383705139
iteration 198, loss = 0.47777971625328064
iteration 199, loss = 0.4752453565597534
iteration 200, loss = 0.4725690484046936
iteration 201, loss = 0.4747784733772278
iteration 202, loss = 0.47639548778533936
iteration 203, loss = 0.473778635263443
iteration 204, loss = 0.47640132904052734
iteration 205, loss = 0.47016778588294983
iteration 206, loss = 0.4734879732131958
iteration 207, loss = 0.4737796187400818
iteration 208, loss = 0.4733199179172516
iteration 209, loss = 0.4739830493927002
iteration 210, loss = 0.47499287128448486
iteration 211, loss = 0.47729402780532837
iteration 212, loss = 0.4772522449493408
iteration 213, loss = 0.46835070848464966
iteration 214, loss = 0.46361488103866577
iteration 215, loss = 0.4770190119743347
iteration 216, loss = 0.47501587867736816
iteration 217, loss = 0.4763253331184387
iteration 218, loss = 0.47521570324897766
iteration 219, loss = 0.47062742710113525
iteration 220, loss = 0.47336286306381226
iteration 221, loss = 0.4798189401626587
iteration 222, loss = 0.47044411301612854
iteration 223, loss = 0.47430798411369324
iteration 224, loss = 0.4686383306980133
iteration 225, loss = 0.4711596369743347
iteration 226, loss = 0.4725297689437866
iteration 227, loss = 0.47159862518310547
iteration 228, loss = 0.4780682325363159
iteration 229, loss = 0.47755753993988037
iteration 230, loss = 0.47943806648254395
iteration 231, loss = 0.4718213677406311
iteration 232, loss = 0.4836158752441406
iteration 233, loss = 0.47503578662872314
iteration 234, loss = 0.4775997996330261
iteration 235, loss = 0.4759906530380249
iteration 236, loss = 0.4635065793991089
iteration 237, loss = 0.482749879360199
iteration 238, loss = 0.4725992679595947
iteration 239, loss = 0.47629404067993164
iteration 240, loss = 0.47314679622650146
iteration 241, loss = 0.4754487872123718
iteration 242, loss = 0.47625601291656494
iteration 243, loss = 0.4711162745952606
iteration 244, loss = 0.4738472104072571
iteration 245, loss = 0.4749199450016022
iteration 246, loss = 0.4782322645187378
iteration 247, loss = 0.4710850715637207
iteration 248, loss = 0.47975844144821167
iteration 249, loss = 0.47142183780670166
iteration 250, loss = 0.4732687473297119
iteration 251, loss = 0.4721047282218933
iteration 252, loss = 0.47120893001556396
iteration 253, loss = 0.4788338243961334
iteration 254, loss = 0.4752812087535858
iteration 255, loss = 0.47434550523757935
iteration 256, loss = 0.46915102005004883
iteration 257, loss = 0.4762685000896454
iteration 258, loss = 0.4673340916633606
iteration 259, loss = 0.4751057028770447
iteration 260, loss = 0.467184454202652
iteration 261, loss = 0.46886730194091797
iteration 262, loss = 0.460150808095932
iteration 263, loss = 0.46565157175064087
iteration 264, loss = 0.469593346118927
iteration 265, loss = 0.47892656922340393
iteration 266, loss = 0.4801422357559204
iteration 267, loss = 0.4659833014011383
iteration 268, loss = 0.4673958420753479
iteration 269, loss = 0.4603376090526581
iteration 270, loss = 0.4747958183288574
iteration 271, loss = 0.4682939648628235
iteration 272, loss = 0.46520981192588806
iteration 273, loss = 0.47432512044906616
iteration 274, loss = 0.46729016304016113
iteration 275, loss = 0.4638859033584595
iteration 276, loss = 0.47909653186798096
iteration 277, loss = 0.4676758050918579
iteration 278, loss = 0.47289329767227173
iteration 279, loss = 0.47331511974334717
iteration 280, loss = 0.4758443236351013
iteration 281, loss = 0.46403175592422485
iteration 282, loss = 0.47072190046310425
iteration 283, loss = 0.47154366970062256
iteration 284, loss = 0.46848544478416443
iteration 285, loss = 0.4755168557167053
iteration 286, loss = 0.47398215532302856
iteration 287, loss = 0.46973085403442383
iteration 288, loss = 0.4720902442932129
iteration 289, loss = 0.4640461802482605
iteration 290, loss = 0.46954435110092163
iteration 291, loss = 0.46711477637290955
iteration 292, loss = 0.4742410182952881
iteration 293, loss = 0.45943063497543335
iteration 294, loss = 0.46852266788482666
iteration 295, loss = 0.4768827557563782
iteration 296, loss = 0.4726176857948303
iteration 297, loss = 0.4732897877693176
iteration 298, loss = 0.4733181297779083
iteration 299, loss = 0.47252994775772095
iteration 0, loss = 0.46492964029312134
iteration 1, loss = 0.46048030257225037
iteration 2, loss = 0.4698765277862549
iteration 3, loss = 0.4798274636268616
iteration 4, loss = 0.47688427567481995
iteration 5, loss = 0.4652239680290222
iteration 6, loss = 0.4752979576587677
iteration 7, loss = 0.4639406204223633
iteration 8, loss = 0.4750915765762329
iteration 9, loss = 0.46836036443710327
iteration 10, loss = 0.45768433809280396
iteration 11, loss = 0.47034040093421936
iteration 12, loss = 0.4709767997264862
iteration 13, loss = 0.4772548973560333
iteration 14, loss = 0.46909141540527344
iteration 15, loss = 0.4669930636882782
iteration 16, loss = 0.46956753730773926
iteration 17, loss = 0.4688897728919983
iteration 18, loss = 0.47325754165649414
iteration 19, loss = 0.4707157015800476
iteration 20, loss = 0.46709609031677246
iteration 21, loss = 0.47003841400146484
iteration 22, loss = 0.4652717709541321
iteration 23, loss = 0.46785402297973633
iteration 24, loss = 0.46747326850891113
iteration 25, loss = 0.4682654142379761
iteration 26, loss = 0.4760344326496124
iteration 27, loss = 0.46986615657806396
iteration 28, loss = 0.45947355031967163
iteration 29, loss = 0.4700521230697632
iteration 30, loss = 0.4722900986671448
iteration 31, loss = 0.46147602796554565
iteration 32, loss = 0.46222633123397827
iteration 33, loss = 0.46864208579063416
iteration 34, loss = 0.4790085554122925
iteration 35, loss = 0.46716463565826416
iteration 36, loss = 0.4680942893028259
iteration 37, loss = 0.47185400128364563
iteration 38, loss = 0.46768805384635925
iteration 39, loss = 0.47158387303352356
iteration 40, loss = 0.45877766609191895
iteration 41, loss = 0.4734669625759125
iteration 42, loss = 0.46951499581336975
iteration 43, loss = 0.4638565182685852
iteration 44, loss = 0.4765605330467224
iteration 45, loss = 0.466582715511322
iteration 46, loss = 0.45926010608673096
iteration 47, loss = 0.4656252861022949
iteration 48, loss = 0.46732646226882935
iteration 49, loss = 0.4694417715072632
iteration 50, loss = 0.46295297145843506
iteration 51, loss = 0.45980989933013916
iteration 52, loss = 0.47203055024147034
iteration 53, loss = 0.47101855278015137
iteration 54, loss = 0.4674489200115204
iteration 55, loss = 0.4664716124534607
iteration 56, loss = 0.466798335313797
iteration 57, loss = 0.459379643201828
iteration 58, loss = 0.47270798683166504
iteration 59, loss = 0.47161003947257996
iteration 60, loss = 0.4657894968986511
iteration 61, loss = 0.46603307127952576
iteration 62, loss = 0.4643627107143402
iteration 63, loss = 0.46636825799942017
iteration 64, loss = 0.467257559299469
iteration 65, loss = 0.4689127206802368
iteration 66, loss = 0.4491521716117859
iteration 67, loss = 0.47001001238822937
iteration 68, loss = 0.4657022953033447
iteration 69, loss = 0.4586018919944763
iteration 70, loss = 0.47234484553337097
iteration 71, loss = 0.46697521209716797
iteration 72, loss = 0.4625340402126312
iteration 73, loss = 0.4637649655342102
iteration 74, loss = 0.4607500433921814
iteration 75, loss = 0.472443163394928
iteration 76, loss = 0.4656585454940796
iteration 77, loss = 0.4740712642669678
iteration 78, loss = 0.4738979935646057
iteration 79, loss = 0.4751538634300232
iteration 80, loss = 0.4566372036933899
iteration 81, loss = 0.4674336612224579
iteration 82, loss = 0.4706280827522278
iteration 83, loss = 0.46222177147865295
iteration 84, loss = 0.46885690093040466
iteration 85, loss = 0.4600013792514801
iteration 86, loss = 0.4660043716430664
iteration 87, loss = 0.46781057119369507
iteration 88, loss = 0.4626913368701935
iteration 89, loss = 0.4679890275001526
iteration 90, loss = 0.45671719312667847
iteration 91, loss = 0.4671158194541931
iteration 92, loss = 0.4612370729446411
iteration 93, loss = 0.46172094345092773
iteration 94, loss = 0.46775075793266296
iteration 95, loss = 0.4634333848953247
iteration 96, loss = 0.4699004888534546
iteration 97, loss = 0.4701066017150879
iteration 98, loss = 0.4713010787963867
iteration 99, loss = 0.4731401801109314
iteration 100, loss = 0.45736968517303467
iteration 101, loss = 0.4581301212310791
iteration 102, loss = 0.46508824825286865
iteration 103, loss = 0.4663406014442444
iteration 104, loss = 0.4600236415863037
iteration 105, loss = 0.4591470956802368
iteration 106, loss = 0.4693378508090973
iteration 107, loss = 0.4629448354244232
iteration 108, loss = 0.4578133821487427
iteration 109, loss = 0.46199601888656616
iteration 110, loss = 0.46187418699264526
iteration 111, loss = 0.4598847031593323
iteration 112, loss = 0.46231144666671753
iteration 113, loss = 0.46400004625320435
iteration 114, loss = 0.4609180688858032
iteration 115, loss = 0.4698704481124878
iteration 116, loss = 0.4625231623649597
iteration 117, loss = 0.4601529836654663
iteration 118, loss = 0.4629637897014618
iteration 119, loss = 0.46169593930244446
iteration 120, loss = 0.468528687953949
iteration 121, loss = 0.46358054876327515
iteration 122, loss = 0.4595335125923157
iteration 123, loss = 0.4615112543106079
iteration 124, loss = 0.46056365966796875
iteration 125, loss = 0.4660537540912628
iteration 126, loss = 0.45602983236312866
iteration 127, loss = 0.4695756733417511
iteration 128, loss = 0.46178513765335083
iteration 129, loss = 0.46488335728645325
iteration 130, loss = 0.4676353931427002
iteration 131, loss = 0.4609222114086151
iteration 132, loss = 0.4691781997680664
iteration 133, loss = 0.46749091148376465
iteration 134, loss = 0.4637607932090759
iteration 135, loss = 0.45360976457595825
iteration 136, loss = 0.47250479459762573
iteration 137, loss = 0.4480220079421997
iteration 138, loss = 0.46238774061203003
iteration 139, loss = 0.4671079218387604
iteration 140, loss = 0.4643929600715637
iteration 141, loss = 0.4613960087299347
iteration 142, loss = 0.4636334776878357
iteration 143, loss = 0.4643297791481018
iteration 144, loss = 0.45744621753692627
iteration 145, loss = 0.46667248010635376
iteration 146, loss = 0.46374180912971497
iteration 147, loss = 0.46197769045829773
iteration 148, loss = 0.4579086899757385
iteration 149, loss = 0.46084293723106384
iteration 150, loss = 0.46243834495544434
iteration 151, loss = 0.46145951747894287
iteration 152, loss = 0.4638761281967163
iteration 153, loss = 0.46622174978256226
iteration 154, loss = 0.45763760805130005
iteration 155, loss = 0.458751916885376
iteration 156, loss = 0.45882904529571533
iteration 157, loss = 0.4660695195198059
iteration 158, loss = 0.4604983329772949
iteration 159, loss = 0.4505593776702881
iteration 160, loss = 0.4649432301521301
iteration 161, loss = 0.46046409010887146
iteration 162, loss = 0.4528005123138428
iteration 163, loss = 0.45948320627212524
iteration 164, loss = 0.46660199761390686
iteration 165, loss = 0.460909903049469
iteration 166, loss = 0.46462154388427734
iteration 167, loss = 0.4575549066066742
iteration 168, loss = 0.46015092730522156
iteration 169, loss = 0.46274250745773315
iteration 170, loss = 0.4546815752983093
iteration 171, loss = 0.45850861072540283
iteration 172, loss = 0.45424365997314453
iteration 173, loss = 0.46500808000564575
iteration 174, loss = 0.46461936831474304
iteration 175, loss = 0.4571426510810852
iteration 176, loss = 0.4562859535217285
iteration 177, loss = 0.4635736346244812
iteration 178, loss = 0.46824735403060913
iteration 179, loss = 0.45204752683639526
iteration 180, loss = 0.4700610637664795
iteration 181, loss = 0.45670783519744873
iteration 182, loss = 0.45659786462783813
iteration 183, loss = 0.4599086344242096
iteration 184, loss = 0.45677462220191956
iteration 185, loss = 0.44826972484588623
iteration 186, loss = 0.4569360315799713
iteration 187, loss = 0.4462272822856903
iteration 188, loss = 0.45094460248947144
iteration 189, loss = 0.45698875188827515
iteration 190, loss = 0.47391197085380554
iteration 191, loss = 0.44809281826019287
iteration 192, loss = 0.4562111794948578
iteration 193, loss = 0.4565538763999939
iteration 194, loss = 0.45066362619400024
iteration 195, loss = 0.46188539266586304
iteration 196, loss = 0.45191341638565063
iteration 197, loss = 0.45507311820983887
iteration 198, loss = 0.468420147895813
iteration 199, loss = 0.4550012946128845
iteration 200, loss = 0.4499092400074005
iteration 201, loss = 0.4640027582645416
iteration 202, loss = 0.454041987657547
iteration 203, loss = 0.44977492094039917
iteration 204, loss = 0.4597969651222229
iteration 205, loss = 0.4601302742958069
iteration 206, loss = 0.4594848155975342
iteration 207, loss = 0.45346200466156006
iteration 208, loss = 0.46229904890060425
iteration 209, loss = 0.464231014251709
iteration 210, loss = 0.4503124952316284
iteration 211, loss = 0.4530308246612549
iteration 212, loss = 0.45871949195861816
iteration 213, loss = 0.45538151264190674
iteration 214, loss = 0.4630272388458252
iteration 215, loss = 0.4444347620010376
iteration 216, loss = 0.46048596501350403
iteration 217, loss = 0.46136319637298584
iteration 218, loss = 0.46265584230422974
iteration 219, loss = 0.4592829942703247
iteration 220, loss = 0.4557958245277405
iteration 221, loss = 0.4592244327068329
iteration 222, loss = 0.45205938816070557
iteration 223, loss = 0.45061635971069336
iteration 224, loss = 0.45804622769355774
iteration 225, loss = 0.45400309562683105
iteration 226, loss = 0.454006165266037
iteration 227, loss = 0.45922738313674927
iteration 228, loss = 0.46398067474365234
iteration 229, loss = 0.45653724670410156
iteration 230, loss = 0.45295441150665283
iteration 231, loss = 0.4560397267341614
iteration 232, loss = 0.44912952184677124
iteration 233, loss = 0.46193820238113403
iteration 234, loss = 0.46500229835510254
iteration 235, loss = 0.4493948519229889
iteration 236, loss = 0.45272159576416016
iteration 237, loss = 0.4521549940109253
iteration 238, loss = 0.44513553380966187
iteration 239, loss = 0.45336204767227173
iteration 240, loss = 0.4556170105934143
iteration 241, loss = 0.46148717403411865
iteration 242, loss = 0.4646758735179901
iteration 243, loss = 0.4565924406051636
iteration 244, loss = 0.45679670572280884
iteration 245, loss = 0.45521456003189087
iteration 246, loss = 0.4508202075958252
iteration 247, loss = 0.44546040892601013
iteration 248, loss = 0.4650907516479492
iteration 249, loss = 0.45905178785324097
iteration 250, loss = 0.4484252333641052
iteration 251, loss = 0.4594051241874695
iteration 252, loss = 0.45275259017944336
iteration 253, loss = 0.4689497947692871
iteration 254, loss = 0.466645210981369
iteration 255, loss = 0.45774316787719727
iteration 256, loss = 0.46084529161453247
iteration 257, loss = 0.4496714174747467
iteration 258, loss = 0.47063058614730835
iteration 259, loss = 0.4587288200855255
iteration 260, loss = 0.4515531063079834
iteration 261, loss = 0.44425952434539795
iteration 262, loss = 0.4607497453689575
iteration 263, loss = 0.4617151916027069
iteration 264, loss = 0.451837956905365
iteration 265, loss = 0.457974374294281
iteration 266, loss = 0.4601859748363495
iteration 267, loss = 0.4654870927333832
iteration 268, loss = 0.45891913771629333
iteration 269, loss = 0.45485568046569824
iteration 270, loss = 0.4509381353855133
iteration 271, loss = 0.4478146433830261
iteration 272, loss = 0.4659637212753296
iteration 273, loss = 0.4550311267375946
iteration 274, loss = 0.4477924406528473
iteration 275, loss = 0.4643205404281616
iteration 276, loss = 0.44578808546066284
iteration 277, loss = 0.454051673412323
iteration 278, loss = 0.45057007670402527
iteration 279, loss = 0.45660334825515747
iteration 280, loss = 0.447393536567688
iteration 281, loss = 0.4468565583229065
iteration 282, loss = 0.4633292555809021
iteration 283, loss = 0.446447491645813
iteration 284, loss = 0.4537351131439209
iteration 285, loss = 0.4562797546386719
iteration 286, loss = 0.4650096893310547
iteration 287, loss = 0.4516390562057495
iteration 288, loss = 0.4439394176006317
iteration 289, loss = 0.4631187319755554
iteration 290, loss = 0.45325160026550293
iteration 291, loss = 0.4546220302581787
iteration 292, loss = 0.455884724855423
iteration 293, loss = 0.4578864574432373
iteration 294, loss = 0.44833821058273315
iteration 295, loss = 0.46313855051994324
iteration 296, loss = 0.4587729573249817
iteration 297, loss = 0.45911747217178345
iteration 298, loss = 0.4577998220920563
iteration 299, loss = 0.45096904039382935
iteration 0, loss = 0.4455603361129761
iteration 1, loss = 0.4466055631637573
iteration 2, loss = 0.4519457221031189
iteration 3, loss = 0.4447239637374878
iteration 4, loss = 0.43496283888816833
iteration 5, loss = 0.45829373598098755
iteration 6, loss = 0.4532829225063324
iteration 7, loss = 0.4517285227775574
iteration 8, loss = 0.4591849446296692
iteration 9, loss = 0.44458499550819397
iteration 10, loss = 0.44323065876960754
iteration 11, loss = 0.4568305015563965
iteration 12, loss = 0.47013819217681885
iteration 13, loss = 0.45385921001434326
iteration 14, loss = 0.45393043756484985
iteration 15, loss = 0.4556807279586792
iteration 16, loss = 0.4471516013145447
iteration 17, loss = 0.44607293605804443
iteration 18, loss = 0.46293723583221436
iteration 19, loss = 0.45548027753829956
iteration 20, loss = 0.4489319920539856
iteration 21, loss = 0.4597264528274536
iteration 22, loss = 0.4513019919395447
iteration 23, loss = 0.44856584072113037
iteration 24, loss = 0.46127596497535706
iteration 25, loss = 0.4467219114303589
iteration 26, loss = 0.4384530782699585
iteration 27, loss = 0.44899263978004456
iteration 28, loss = 0.44626325368881226
iteration 29, loss = 0.44712719321250916
iteration 30, loss = 0.45397257804870605
iteration 31, loss = 0.4393148720264435
iteration 32, loss = 0.4470241665840149
iteration 33, loss = 0.4493597745895386
iteration 34, loss = 0.456437885761261
iteration 35, loss = 0.4435843527317047
iteration 36, loss = 0.45517396926879883
iteration 37, loss = 0.4573543071746826
iteration 38, loss = 0.44711193442344666
iteration 39, loss = 0.4518231749534607
iteration 40, loss = 0.4488945007324219
iteration 41, loss = 0.4436543583869934
iteration 42, loss = 0.4449823796749115
iteration 43, loss = 0.4560990333557129
iteration 44, loss = 0.45243316888809204
iteration 45, loss = 0.44734084606170654
iteration 46, loss = 0.44504132866859436
iteration 47, loss = 0.4575413465499878
iteration 48, loss = 0.45850270986557007
iteration 49, loss = 0.45036882162094116
iteration 50, loss = 0.4644312858581543
iteration 51, loss = 0.45082637667655945
iteration 52, loss = 0.4554343819618225
iteration 53, loss = 0.4568493664264679
iteration 54, loss = 0.4383707642555237
iteration 55, loss = 0.4526336193084717
iteration 56, loss = 0.43998146057128906
iteration 57, loss = 0.44180089235305786
iteration 58, loss = 0.44939085841178894
iteration 59, loss = 0.45627206563949585
iteration 60, loss = 0.4416664242744446
iteration 61, loss = 0.4422733783721924
iteration 62, loss = 0.43776845932006836
iteration 63, loss = 0.4527994990348816
iteration 64, loss = 0.442646861076355
iteration 65, loss = 0.4493461847305298
iteration 66, loss = 0.4507468342781067
iteration 67, loss = 0.45065420866012573
iteration 68, loss = 0.45456141233444214
iteration 69, loss = 0.4325384497642517
iteration 70, loss = 0.4508087635040283
iteration 71, loss = 0.4467623829841614
iteration 72, loss = 0.4511715769767761
iteration 73, loss = 0.4494149684906006
iteration 74, loss = 0.465922474861145
iteration 75, loss = 0.435552179813385
iteration 76, loss = 0.44784411787986755
iteration 77, loss = 0.45367687940597534
iteration 78, loss = 0.4502352476119995
iteration 79, loss = 0.4640331268310547
iteration 80, loss = 0.42960765957832336
iteration 81, loss = 0.4478212594985962
iteration 82, loss = 0.4486859440803528
iteration 83, loss = 0.4614575207233429
iteration 84, loss = 0.45043063163757324
iteration 85, loss = 0.4605928063392639
iteration 86, loss = 0.4529944658279419
iteration 87, loss = 0.45645350217819214
iteration 88, loss = 0.445692241191864
iteration 89, loss = 0.4527038335800171
iteration 90, loss = 0.4401257038116455
iteration 91, loss = 0.45319435000419617
iteration 92, loss = 0.44501563906669617
iteration 93, loss = 0.43874073028564453
iteration 94, loss = 0.4433583617210388
iteration 95, loss = 0.4418712258338928
iteration 96, loss = 0.4451504945755005
iteration 97, loss = 0.44085460901260376
iteration 98, loss = 0.4501549005508423
iteration 99, loss = 0.4350339472293854
iteration 100, loss = 0.45864325761795044
iteration 101, loss = 0.4378446936607361
iteration 102, loss = 0.44110560417175293
iteration 103, loss = 0.42801088094711304
iteration 104, loss = 0.44709908962249756
iteration 105, loss = 0.44732844829559326
iteration 106, loss = 0.4486371874809265
iteration 107, loss = 0.4497968256473541
iteration 108, loss = 0.4421795904636383
iteration 109, loss = 0.45974916219711304
iteration 110, loss = 0.43876874446868896
iteration 111, loss = 0.4372299611568451
iteration 112, loss = 0.4266127347946167
iteration 113, loss = 0.4540320932865143
iteration 114, loss = 0.43880966305732727
iteration 115, loss = 0.4360054135322571
iteration 116, loss = 0.4446396231651306
iteration 117, loss = 0.4334559440612793
iteration 118, loss = 0.44690603017807007
iteration 119, loss = 0.4587497115135193
iteration 120, loss = 0.4488137662410736
iteration 121, loss = 0.4433767795562744
iteration 122, loss = 0.4493303894996643
iteration 123, loss = 0.4540909230709076
iteration 124, loss = 0.44490134716033936
iteration 125, loss = 0.4403415322303772
iteration 126, loss = 0.4372623860836029
iteration 127, loss = 0.4475978910923004
iteration 128, loss = 0.4376637041568756
iteration 129, loss = 0.446948379278183
iteration 130, loss = 0.450935035943985
iteration 131, loss = 0.43981894850730896
iteration 132, loss = 0.43947911262512207
iteration 133, loss = 0.43105071783065796
iteration 134, loss = 0.4454578459262848
iteration 135, loss = 0.4490255117416382
iteration 136, loss = 0.4381977319717407
iteration 137, loss = 0.44045981764793396
iteration 138, loss = 0.4337128698825836
iteration 139, loss = 0.456142783164978
iteration 140, loss = 0.4512001872062683
iteration 141, loss = 0.4505090117454529
iteration 142, loss = 0.43235522508621216
iteration 143, loss = 0.4481985867023468
iteration 144, loss = 0.44059839844703674
iteration 145, loss = 0.44796985387802124
iteration 146, loss = 0.442956805229187
iteration 147, loss = 0.4447748064994812
iteration 148, loss = 0.4429779052734375
iteration 149, loss = 0.43492960929870605
iteration 150, loss = 0.4320564270019531
iteration 151, loss = 0.444806843996048
iteration 152, loss = 0.4422317147254944
iteration 153, loss = 0.44966816902160645
iteration 154, loss = 0.4417778253555298
iteration 155, loss = 0.4401690363883972
iteration 156, loss = 0.44558677077293396
iteration 157, loss = 0.4260052442550659
iteration 158, loss = 0.4535965919494629
iteration 159, loss = 0.43882477283477783
iteration 160, loss = 0.436676025390625
iteration 161, loss = 0.43357783555984497
iteration 162, loss = 0.4332139492034912
iteration 163, loss = 0.4400150179862976
iteration 164, loss = 0.44332417845726013
iteration 165, loss = 0.4216573238372803
iteration 166, loss = 0.45375555753707886
iteration 167, loss = 0.44682931900024414
iteration 168, loss = 0.4476636052131653
iteration 169, loss = 0.43488961458206177
iteration 170, loss = 0.4465489983558655
iteration 171, loss = 0.44283294677734375
iteration 172, loss = 0.44047069549560547
iteration 173, loss = 0.4409424960613251
iteration 174, loss = 0.43797069787979126
iteration 175, loss = 0.43706196546554565
iteration 176, loss = 0.4433901906013489
iteration 177, loss = 0.4458756148815155
iteration 178, loss = 0.43997722864151
iteration 179, loss = 0.44919833540916443
iteration 180, loss = 0.4382319450378418
iteration 181, loss = 0.43672728538513184
iteration 182, loss = 0.43459224700927734
iteration 183, loss = 0.4349880814552307
iteration 184, loss = 0.44140321016311646
iteration 185, loss = 0.45369791984558105
iteration 186, loss = 0.4352526068687439
iteration 187, loss = 0.43677955865859985
iteration 188, loss = 0.4441289007663727
iteration 189, loss = 0.44457024335861206
iteration 190, loss = 0.44482314586639404
iteration 191, loss = 0.45011335611343384
iteration 192, loss = 0.4386938810348511
iteration 193, loss = 0.43705984950065613
iteration 194, loss = 0.4314452111721039
iteration 195, loss = 0.4306698441505432
iteration 196, loss = 0.42786461114883423
iteration 197, loss = 0.4531945586204529
iteration 198, loss = 0.43411603569984436
iteration 199, loss = 0.4329194128513336
iteration 200, loss = 0.4379923939704895
iteration 201, loss = 0.4423588514328003
iteration 202, loss = 0.4445314407348633
iteration 203, loss = 0.42936021089553833
iteration 204, loss = 0.445682168006897
iteration 205, loss = 0.454621285200119
iteration 206, loss = 0.4327893853187561
iteration 207, loss = 0.43823742866516113
iteration 208, loss = 0.43363091349601746
iteration 209, loss = 0.44583994150161743
iteration 210, loss = 0.45337530970573425
iteration 211, loss = 0.4282766282558441
iteration 212, loss = 0.4500614404678345
iteration 213, loss = 0.4538061320781708
iteration 214, loss = 0.4292948842048645
iteration 215, loss = 0.4543072581291199
iteration 216, loss = 0.4394540786743164
iteration 217, loss = 0.43811488151550293
iteration 218, loss = 0.44493550062179565
iteration 219, loss = 0.42611831426620483
iteration 220, loss = 0.4558277130126953
iteration 221, loss = 0.4600933790206909
iteration 222, loss = 0.43795210123062134
iteration 223, loss = 0.4363585412502289
iteration 224, loss = 0.4210166335105896
iteration 225, loss = 0.44385313987731934
iteration 226, loss = 0.4568272829055786
iteration 227, loss = 0.4409518539905548
iteration 228, loss = 0.44361209869384766
iteration 229, loss = 0.4470263123512268
iteration 230, loss = 0.4402816891670227
iteration 231, loss = 0.4443128705024719
iteration 232, loss = 0.42399662733078003
iteration 233, loss = 0.4339703321456909
iteration 234, loss = 0.44091734290122986
iteration 235, loss = 0.43854954838752747
iteration 236, loss = 0.43778306245803833
iteration 237, loss = 0.42172399163246155
iteration 238, loss = 0.44236499071121216
iteration 239, loss = 0.42278534173965454
iteration 240, loss = 0.44266343116760254
iteration 241, loss = 0.4430755078792572
iteration 242, loss = 0.4343792796134949
iteration 243, loss = 0.4281754791736603
iteration 244, loss = 0.4276748299598694
iteration 245, loss = 0.4340248703956604
iteration 246, loss = 0.4433637261390686
iteration 247, loss = 0.4189737141132355
iteration 248, loss = 0.4256640672683716
iteration 249, loss = 0.4196447730064392
iteration 250, loss = 0.4254770278930664
iteration 251, loss = 0.43639373779296875
iteration 252, loss = 0.43920546770095825
iteration 253, loss = 0.43693679571151733
iteration 254, loss = 0.4313544034957886
iteration 255, loss = 0.44239258766174316
iteration 256, loss = 0.44399476051330566
iteration 257, loss = 0.42790132761001587
iteration 258, loss = 0.4243072271347046
iteration 259, loss = 0.4339103102684021
iteration 260, loss = 0.4374374747276306
iteration 261, loss = 0.4401271343231201
iteration 262, loss = 0.42358407378196716
iteration 263, loss = 0.4159433841705322
iteration 264, loss = 0.45946764945983887
iteration 265, loss = 0.4491886496543884
iteration 266, loss = 0.42570456862449646
iteration 267, loss = 0.4434141516685486
iteration 268, loss = 0.43171951174736023
iteration 269, loss = 0.43539953231811523
iteration 270, loss = 0.4348122477531433
iteration 271, loss = 0.42922767996788025
iteration 272, loss = 0.43234020471572876
iteration 273, loss = 0.4184057116508484
iteration 274, loss = 0.4368986189365387
iteration 275, loss = 0.4250878393650055
iteration 276, loss = 0.42726626992225647
iteration 277, loss = 0.43432170152664185
iteration 278, loss = 0.4385201930999756
iteration 279, loss = 0.42315244674682617
iteration 280, loss = 0.4519087076187134
iteration 281, loss = 0.43822234869003296
iteration 282, loss = 0.4329538345336914
iteration 283, loss = 0.4094390273094177
iteration 284, loss = 0.42498964071273804
iteration 285, loss = 0.4431779682636261
iteration 286, loss = 0.43429332971572876
iteration 287, loss = 0.4278382956981659
iteration 288, loss = 0.445062518119812
iteration 289, loss = 0.44002461433410645
iteration 290, loss = 0.4266553521156311
iteration 291, loss = 0.43876564502716064
iteration 292, loss = 0.4208984971046448
iteration 293, loss = 0.4237722158432007
iteration 294, loss = 0.43191587924957275
iteration 295, loss = 0.4064711332321167
iteration 296, loss = 0.41128095984458923
iteration 297, loss = 0.4356902837753296
iteration 298, loss = 0.43770503997802734
iteration 299, loss = 0.4381150007247925
iteration 0, loss = 0.43404459953308105
iteration 1, loss = 0.4380442798137665
iteration 2, loss = 0.44111496210098267
iteration 3, loss = 0.4343396723270416
iteration 4, loss = 0.41634994745254517
iteration 5, loss = 0.44064050912857056
iteration 6, loss = 0.43591123819351196
iteration 7, loss = 0.43010687828063965
iteration 8, loss = 0.4176184833049774
iteration 9, loss = 0.4252908229827881
iteration 10, loss = 0.44736248254776
iteration 11, loss = 0.41669121384620667
iteration 12, loss = 0.44138646125793457
iteration 13, loss = 0.43595778942108154
iteration 14, loss = 0.41589799523353577
iteration 15, loss = 0.4255366921424866
iteration 16, loss = 0.43939727544784546
iteration 17, loss = 0.42678070068359375
iteration 18, loss = 0.42485249042510986
iteration 19, loss = 0.4122937321662903
iteration 20, loss = 0.42518115043640137
iteration 21, loss = 0.4297023117542267
iteration 22, loss = 0.4238020181655884
iteration 23, loss = 0.4226696491241455
iteration 24, loss = 0.4420977532863617
iteration 25, loss = 0.4413987398147583
iteration 26, loss = 0.42842864990234375
iteration 27, loss = 0.43125662207603455
iteration 28, loss = 0.42941778898239136
iteration 29, loss = 0.4305505156517029
iteration 30, loss = 0.42750033736228943
iteration 31, loss = 0.43552568554878235
iteration 32, loss = 0.4360741376876831
iteration 33, loss = 0.43260109424591064
iteration 34, loss = 0.4260011911392212
iteration 35, loss = 0.4277782440185547
iteration 36, loss = 0.4194788932800293
iteration 37, loss = 0.43483227491378784
iteration 38, loss = 0.42367932200431824
iteration 39, loss = 0.41280239820480347
iteration 40, loss = 0.4197756052017212
iteration 41, loss = 0.41142478585243225
iteration 42, loss = 0.4254556894302368
iteration 43, loss = 0.41638708114624023
iteration 44, loss = 0.4225803017616272
iteration 45, loss = 0.4316200017929077
iteration 46, loss = 0.4164935350418091
iteration 47, loss = 0.4255366325378418
iteration 48, loss = 0.44356095790863037
iteration 49, loss = 0.43273288011550903
iteration 50, loss = 0.4196566343307495
iteration 51, loss = 0.4362887740135193
iteration 52, loss = 0.43449196219444275
iteration 53, loss = 0.43115684390068054
iteration 54, loss = 0.42552661895751953
iteration 55, loss = 0.42525842785835266
iteration 56, loss = 0.41774868965148926
iteration 57, loss = 0.43427109718322754
iteration 58, loss = 0.4255421757698059
iteration 59, loss = 0.4285549521446228
iteration 60, loss = 0.43939584493637085
iteration 61, loss = 0.4228053390979767
iteration 62, loss = 0.41664594411849976
iteration 63, loss = 0.41269755363464355
iteration 64, loss = 0.4266311228275299
iteration 65, loss = 0.4243459105491638
iteration 66, loss = 0.43065452575683594
iteration 67, loss = 0.4117429256439209
iteration 68, loss = 0.4236510396003723
iteration 69, loss = 0.4146100878715515
iteration 70, loss = 0.42047691345214844
iteration 71, loss = 0.4271559417247772
iteration 72, loss = 0.43694353103637695
iteration 73, loss = 0.4205698072910309
iteration 74, loss = 0.4514271914958954
iteration 75, loss = 0.4191250801086426
iteration 76, loss = 0.4252484440803528
iteration 77, loss = 0.4283067286014557
iteration 78, loss = 0.4243844151496887
iteration 79, loss = 0.42606252431869507
iteration 80, loss = 0.42192786931991577
iteration 81, loss = 0.40723270177841187
iteration 82, loss = 0.44053468108177185
iteration 83, loss = 0.42423588037490845
iteration 84, loss = 0.41698944568634033
iteration 85, loss = 0.41865840554237366
iteration 86, loss = 0.4363429546356201
iteration 87, loss = 0.42379769682884216
iteration 88, loss = 0.4301365613937378
iteration 89, loss = 0.42013847827911377
iteration 90, loss = 0.4353032112121582
iteration 91, loss = 0.4188315272331238
iteration 92, loss = 0.42705172300338745
iteration 93, loss = 0.42500683665275574
iteration 94, loss = 0.4106629490852356
iteration 95, loss = 0.423722505569458
iteration 96, loss = 0.4182811975479126
iteration 97, loss = 0.4188515841960907
iteration 98, loss = 0.42471450567245483
iteration 99, loss = 0.3981836438179016
iteration 100, loss = 0.423184871673584
iteration 101, loss = 0.4311409294605255
iteration 102, loss = 0.44228190183639526
iteration 103, loss = 0.42856091260910034
iteration 104, loss = 0.44177260994911194
iteration 105, loss = 0.42351070046424866
iteration 106, loss = 0.4296135902404785
iteration 107, loss = 0.40646982192993164
iteration 108, loss = 0.4237250089645386
iteration 109, loss = 0.42720091342926025
iteration 110, loss = 0.42790189385414124
iteration 111, loss = 0.42038649320602417
iteration 112, loss = 0.4084378182888031
iteration 113, loss = 0.42199283838272095
iteration 114, loss = 0.4316410422325134
iteration 115, loss = 0.4296119213104248
iteration 116, loss = 0.4155128002166748
iteration 117, loss = 0.42551907896995544
iteration 118, loss = 0.4144015908241272
iteration 119, loss = 0.4158855974674225
iteration 120, loss = 0.42453697323799133
iteration 121, loss = 0.42352110147476196
iteration 122, loss = 0.41846001148223877
iteration 123, loss = 0.41762715578079224
iteration 124, loss = 0.4170621633529663
iteration 125, loss = 0.43542104959487915
iteration 126, loss = 0.4048686623573303
iteration 127, loss = 0.42556995153427124
iteration 128, loss = 0.4255092740058899
iteration 129, loss = 0.4152207374572754
iteration 130, loss = 0.42276158928871155
iteration 131, loss = 0.4021248519420624
iteration 132, loss = 0.4256620407104492
iteration 133, loss = 0.40940403938293457
iteration 134, loss = 0.43068358302116394
iteration 135, loss = 0.416908860206604
iteration 136, loss = 0.42417922616004944
iteration 137, loss = 0.39165163040161133
iteration 138, loss = 0.44546225666999817
iteration 139, loss = 0.3961259126663208
iteration 140, loss = 0.40893346071243286
iteration 141, loss = 0.41726839542388916
iteration 142, loss = 0.42617255449295044
iteration 143, loss = 0.42170262336730957
iteration 144, loss = 0.4300239086151123
iteration 145, loss = 0.41108375787734985
iteration 146, loss = 0.40275782346725464
iteration 147, loss = 0.43561041355133057
iteration 148, loss = 0.4245561361312866
iteration 149, loss = 0.4208243787288666
iteration 150, loss = 0.4133949279785156
iteration 151, loss = 0.4219050407409668
iteration 152, loss = 0.409771203994751
iteration 153, loss = 0.40629667043685913
iteration 154, loss = 0.4272357225418091
iteration 155, loss = 0.41635197401046753
iteration 156, loss = 0.41223251819610596
iteration 157, loss = 0.4189618229866028
iteration 158, loss = 0.42460739612579346
iteration 159, loss = 0.4216344356536865
iteration 160, loss = 0.41592293977737427
iteration 161, loss = 0.413338303565979
iteration 162, loss = 0.39947599172592163
iteration 163, loss = 0.4081442952156067
iteration 164, loss = 0.424228310585022
iteration 165, loss = 0.43191784620285034
iteration 166, loss = 0.40746769309043884
iteration 167, loss = 0.3997904062271118
iteration 168, loss = 0.41134557127952576
iteration 169, loss = 0.3969113826751709
iteration 170, loss = 0.4311903119087219
iteration 171, loss = 0.419382244348526
iteration 172, loss = 0.42526400089263916
iteration 173, loss = 0.42494848370552063
iteration 174, loss = 0.41848862171173096
iteration 175, loss = 0.40328487753868103
iteration 176, loss = 0.4110789895057678
iteration 177, loss = 0.4092998206615448
iteration 178, loss = 0.394307017326355
iteration 179, loss = 0.4035496711730957
iteration 180, loss = 0.402388334274292
iteration 181, loss = 0.419309139251709
iteration 182, loss = 0.40879470109939575
iteration 183, loss = 0.44753897190093994
iteration 184, loss = 0.41464412212371826
iteration 185, loss = 0.4165980815887451
iteration 186, loss = 0.3917820453643799
iteration 187, loss = 0.4364720582962036
iteration 188, loss = 0.4253004789352417
iteration 189, loss = 0.42355287075042725
iteration 190, loss = 0.42793476581573486
iteration 191, loss = 0.41692018508911133
iteration 192, loss = 0.41938096284866333
iteration 193, loss = 0.4116442799568176
iteration 194, loss = 0.424890398979187
iteration 195, loss = 0.43684810400009155
iteration 196, loss = 0.4146207571029663
iteration 197, loss = 0.4059489369392395
iteration 198, loss = 0.4194958806037903
iteration 199, loss = 0.403958261013031
iteration 200, loss = 0.40648582577705383
iteration 201, loss = 0.40865635871887207
iteration 202, loss = 0.3956955671310425
iteration 203, loss = 0.4221903085708618
iteration 204, loss = 0.41288477182388306
iteration 205, loss = 0.41198742389678955
iteration 206, loss = 0.4125223755836487
iteration 207, loss = 0.4190969467163086
iteration 208, loss = 0.3932294547557831
iteration 209, loss = 0.40790748596191406
iteration 210, loss = 0.42734667658805847
iteration 211, loss = 0.43454521894454956
iteration 212, loss = 0.41747674345970154
iteration 213, loss = 0.414825975894928
iteration 214, loss = 0.4167613983154297
iteration 215, loss = 0.39657968282699585
iteration 216, loss = 0.4135056734085083
iteration 217, loss = 0.41777652502059937
iteration 218, loss = 0.4318872094154358
iteration 219, loss = 0.40190768241882324
iteration 220, loss = 0.43197184801101685
iteration 221, loss = 0.41955333948135376
iteration 222, loss = 0.43669360876083374
iteration 223, loss = 0.4186362624168396
iteration 224, loss = 0.40962469577789307
iteration 225, loss = 0.4100032448768616
iteration 226, loss = 0.40605106949806213
iteration 227, loss = 0.42529040575027466
iteration 228, loss = 0.4244906008243561
iteration 229, loss = 0.41816407442092896
iteration 230, loss = 0.3997252583503723
iteration 231, loss = 0.3999682068824768
iteration 232, loss = 0.3994448781013489
iteration 233, loss = 0.42140650749206543
iteration 234, loss = 0.42040055990219116
iteration 235, loss = 0.4105297923088074
iteration 236, loss = 0.40407562255859375
iteration 237, loss = 0.4133670926094055
iteration 238, loss = 0.42586082220077515
iteration 239, loss = 0.418348491191864
iteration 240, loss = 0.4069496989250183
iteration 241, loss = 0.4031386971473694
iteration 242, loss = 0.40941163897514343
iteration 243, loss = 0.39595863223075867
iteration 244, loss = 0.40862560272216797
iteration 245, loss = 0.4210456609725952
iteration 246, loss = 0.4047582149505615
iteration 247, loss = 0.4322078227996826
iteration 248, loss = 0.42852145433425903
iteration 249, loss = 0.39435672760009766
iteration 250, loss = 0.4131873548030853
iteration 251, loss = 0.4047400951385498
iteration 252, loss = 0.42445048689842224
iteration 253, loss = 0.4008290767669678
iteration 254, loss = 0.40497511625289917
iteration 255, loss = 0.4123249053955078
iteration 256, loss = 0.4371671676635742
iteration 257, loss = 0.3995819091796875
iteration 258, loss = 0.42400020360946655
iteration 259, loss = 0.4084361493587494
iteration 260, loss = 0.39476877450942993
iteration 261, loss = 0.40708816051483154
iteration 262, loss = 0.4203745722770691
iteration 263, loss = 0.4082523584365845
iteration 264, loss = 0.40091943740844727
iteration 265, loss = 0.4014517664909363
iteration 266, loss = 0.39470362663269043
iteration 267, loss = 0.38383299112319946
iteration 268, loss = 0.4214093089103699
iteration 269, loss = 0.3931962847709656
iteration 270, loss = 0.41824424266815186
iteration 271, loss = 0.4218139350414276
iteration 272, loss = 0.3953588008880615
iteration 273, loss = 0.40528327226638794
iteration 274, loss = 0.40245410799980164
iteration 275, loss = 0.4096594750881195
iteration 276, loss = 0.43129968643188477
iteration 277, loss = 0.41702139377593994
iteration 278, loss = 0.4110126495361328
iteration 279, loss = 0.39028793573379517
iteration 280, loss = 0.4027910828590393
iteration 281, loss = 0.40888458490371704
iteration 282, loss = 0.4182846248149872
iteration 283, loss = 0.42671218514442444
iteration 284, loss = 0.39166998863220215
iteration 285, loss = 0.40603601932525635
iteration 286, loss = 0.40518325567245483
iteration 287, loss = 0.39931169152259827
iteration 288, loss = 0.4171332120895386
iteration 289, loss = 0.41895657777786255
iteration 290, loss = 0.3823133707046509
iteration 291, loss = 0.3975725769996643
iteration 292, loss = 0.42176446318626404
iteration 293, loss = 0.39278027415275574
iteration 294, loss = 0.4167466163635254
iteration 295, loss = 0.4163109064102173
iteration 296, loss = 0.42350587248802185
iteration 297, loss = 0.40717118978500366
iteration 298, loss = 0.39708787202835083
iteration 299, loss = 0.4042991101741791

iteration 1, loss = 1.44521164894104
iteration 2, loss = 1.4322640895843506
iteration 3, loss = 1.3709595203399658
iteration 4, loss = 1.4340200424194336
iteration 5, loss = 1.4155389070510864
iteration 6, loss = 1.404056191444397
iteration 7, loss = 1.3581836223602295
iteration 8, loss = 1.3824411630630493
iteration 9, loss = 1.4178861379623413
iteration 10, loss = 1.353853702545166
iteration 11, loss = 1.354891300201416
iteration 12, loss = 1.319298267364502
iteration 13, loss = 1.321535587310791
iteration 14, loss = 1.3167762756347656
iteration 15, loss = 1.3257030248641968
iteration 16, loss = 1.2992942333221436
iteration 17, loss = 1.3258910179138184
iteration 18, loss = 1.3101202249526978
iteration 19, loss = 1.3201065063476562
iteration 20, loss = 1.2719156742095947
iteration 21, loss = 1.178126573562622
iteration 22, loss = 1.2998309135437012
iteration 23, loss = 1.270132303237915
iteration 24, loss = 1.196445345878601
iteration 25, loss = 1.2501907348632812
iteration 26, loss = 1.2240468263626099
iteration 27, loss = 1.2566450834274292
iteration 28, loss = 1.288772463798523
iteration 29, loss = 1.2420623302459717
iteration 30, loss = 1.2427958250045776
iteration 31, loss = 1.192057490348816
iteration 32, loss = 1.2266062498092651
iteration 33, loss = 1.2219198942184448
iteration 34, loss = 1.2134078741073608
iteration 35, loss = 1.1594274044036865
iteration 36, loss = 1.1601474285125732
iteration 37, loss = 1.1405824422836304
iteration 38, loss = 1.1214183568954468
iteration 39, loss = 1.0722557306289673
iteration 40, loss = 1.128868579864502
iteration 41, loss = 0.9579602479934692
iteration 42, loss = 1.1750802993774414
iteration 43, loss = 1.0681631565093994
iteration 44, loss = 1.0221829414367676
iteration 45, loss = 0.96645188331604
iteration 46, loss = 1.113305687904358
iteration 47, loss = 1.060854434967041
iteration 48, loss = 1.0791757106781006
iteration 49, loss = 1.0714010000228882
iteration 50, loss = 1.1443865299224854
iteration 51, loss = 1.0907237529754639
iteration 52, loss = 1.0407947301864624
iteration 53, loss = 0.9441425800323486
iteration 54, loss = 1.1334376335144043
iteration 55, loss = 1.0075492858886719
iteration 56, loss = 0.9355274438858032
iteration 57, loss = 1.0365796089172363
iteration 58, loss = 0.9871460199356079
iteration 59, loss = 1.105130672454834
iteration 60, loss = 0.9960829019546509
iteration 61, loss = 0.9597290754318237
iteration 62, loss = 0.8590190410614014
iteration 63, loss = 1.0875238180160522
iteration 64, loss = 0.9879019856452942
iteration 65, loss = 1.0281572341918945
iteration 66, loss = 1.0006718635559082
iteration 67, loss = 1.033634901046753
iteration 68, loss = 1.0129177570343018
iteration 69, loss = 0.9797056913375854
iteration 70, loss = 1.0208977460861206
iteration 71, loss = 0.9627891778945923
iteration 72, loss = 0.9673070907592773
iteration 73, loss = 0.9370690584182739
iteration 74, loss = 0.8129081726074219
iteration 75, loss = 0.9517227411270142
iteration 76, loss = 0.9573312997817993
iteration 77, loss = 1.0118016004562378
iteration 78, loss = 0.8738147020339966
iteration 79, loss = 0.8725476861000061
iteration 80, loss = 0.7945539355278015
iteration 81, loss = 0.9187185764312744
iteration 82, loss = 0.8426340818405151
iteration 83, loss = 0.839715838432312
iteration 84, loss = 0.7622799873352051
iteration 85, loss = 0.9462385177612305
iteration 86, loss = 0.8387424945831299
iteration 87, loss = 0.8176063895225525
iteration 88, loss = 0.8352466821670532
iteration 89, loss = 0.9640307426452637
iteration 90, loss = 0.9058679342269897
iteration 91, loss = 0.8840248584747314
iteration 92, loss = 0.8032848834991455
iteration 93, loss = 0.8953442573547363
iteration 94, loss = 0.8335207104682922
iteration 95, loss = 0.796237587928772
iteration 96, loss = 0.7167463302612305
iteration 97, loss = 0.7929433584213257
iteration 98, loss = 0.710312008857727
iteration 99, loss = 0.7887960076332092
iteration 100, loss = 0.7610936760902405
iteration 101, loss = 0.8746155500411987
iteration 102, loss = 0.8868470191955566
iteration 103, loss = 0.7845841646194458
iteration 104, loss = 0.7589563131332397
iteration 105, loss = 0.7621517181396484
iteration 106, loss = 0.7215280532836914
iteration 107, loss = 0.6702096462249756
iteration 108, loss = 0.8015884160995483
iteration 109, loss = 0.7799278497695923
iteration 110, loss = 0.8748420476913452
iteration 111, loss = 0.8859474658966064
iteration 112, loss = 0.8789690136909485
iteration 113, loss = 0.8266267776489258
iteration 114, loss = 0.8531183004379272
iteration 115, loss = 0.9024100303649902
iteration 116, loss = 0.7769603729248047
iteration 117, loss = 0.7604867815971375
iteration 118, loss = 0.79219651222229
iteration 119, loss = 0.8138370513916016
iteration 120, loss = 0.7028100490570068
iteration 121, loss = 0.73301100730896
iteration 122, loss = 0.7515361309051514
iteration 123, loss = 0.8212327361106873
iteration 124, loss = 0.8494021892547607
iteration 125, loss = 0.8636783361434937
iteration 126, loss = 0.781438410282135
iteration 127, loss = 0.7829574942588806
iteration 128, loss = 0.6829156279563904
iteration 129, loss = 0.8119786977767944
iteration 130, loss = 0.7825875282287598
iteration 131, loss = 0.84885573387146
iteration 132, loss = 0.7569737434387207
iteration 133, loss = 0.6911519765853882
iteration 134, loss = 0.8054443597793579
iteration 135, loss = 0.6900317668914795
iteration 136, loss = 0.7605860829353333
iteration 137, loss = 0.7777486443519592
iteration 138, loss = 0.7574175596237183
iteration 139, loss = 0.7261835932731628
iteration 140, loss = 0.6539784669876099
iteration 141, loss = 0.7893401384353638
iteration 142, loss = 0.7491147518157959
iteration 143, loss = 0.773821234703064
iteration 144, loss = 0.7263582944869995
iteration 145, loss = 0.8421096801757812
iteration 146, loss = 0.7131556272506714
iteration 147, loss = 0.750959575176239
iteration 148, loss = 0.7299009561538696
iteration 149, loss = 0.6934124231338501
iteration 150, loss = 0.7408654689788818
iteration 151, loss = 0.8368852734565735
iteration 152, loss = 0.6852791905403137
iteration 153, loss = 0.7415667176246643
iteration 154, loss = 0.6758075952529907
iteration 155, loss = 0.6410423517227173
iteration 156, loss = 0.7901736497879028
iteration 157, loss = 0.78749018907547
iteration 158, loss = 0.7304793000221252
iteration 159, loss = 0.6394761800765991
iteration 160, loss = 0.6814570426940918
iteration 161, loss = 0.7331084609031677
iteration 162, loss = 0.804424524307251
iteration 163, loss = 0.6885988712310791
iteration 164, loss = 0.7241981625556946
iteration 165, loss = 0.7813091278076172
iteration 166, loss = 0.7296497821807861
iteration 167, loss = 0.7005106806755066
iteration 168, loss = 0.6998504996299744
iteration 169, loss = 0.7867311239242554
iteration 170, loss = 0.7128194570541382
iteration 171, loss = 0.8145636916160583
iteration 172, loss = 0.8216389417648315
iteration 173, loss = 0.6595330238342285
iteration 174, loss = 0.724524736404419
iteration 175, loss = 0.8137573599815369
iteration 176, loss = 0.7711154818534851
iteration 177, loss = 0.6601141691207886
iteration 178, loss = 0.7578057050704956
iteration 179, loss = 0.6685095429420471
iteration 180, loss = 0.8224620819091797
iteration 181, loss = 0.677236020565033
iteration 182, loss = 0.6573595404624939
iteration 183, loss = 0.653717041015625
iteration 184, loss = 0.7718667984008789
iteration 185, loss = 0.8128819465637207
iteration 186, loss = 0.7293221354484558
iteration 187, loss = 0.7044325470924377
iteration 188, loss = 0.7359885573387146
iteration 189, loss = 0.6325358748435974
iteration 190, loss = 0.6770095229148865
iteration 191, loss = 0.6287605166435242
iteration 192, loss = 0.7286999821662903
iteration 193, loss = 0.7708149552345276
iteration 194, loss = 0.726908802986145
iteration 195, loss = 0.6586844325065613
iteration 196, loss = 0.681143581867218
iteration 197, loss = 0.7043732404708862
iteration 198, loss = 0.6631563305854797
iteration 199, loss = 0.9035462737083435
iteration 200, loss = 0.7384600043296814
iteration 201, loss = 0.747989296913147
iteration 202, loss = 0.7335003614425659
iteration 203, loss = 0.7031286954879761
iteration 204, loss = 0.7750098705291748
iteration 205, loss = 0.7523599863052368
iteration 206, loss = 0.7322227954864502
iteration 207, loss = 0.6585453748703003
iteration 208, loss = 0.7319419384002686
iteration 209, loss = 0.7755216360092163
iteration 210, loss = 0.6528612971305847
iteration 211, loss = 0.6935597062110901
iteration 212, loss = 0.7228497266769409
iteration 213, loss = 0.6335824728012085
iteration 214, loss = 0.5586569309234619
iteration 215, loss = 0.7094480991363525
iteration 216, loss = 0.7394312024116516
iteration 217, loss = 0.6233835220336914
iteration 218, loss = 0.7456001043319702
iteration 219, loss = 0.6563947200775146
iteration 220, loss = 0.7533519864082336
iteration 221, loss = 0.7435013055801392
iteration 222, loss = 0.6463267803192139
iteration 223, loss = 0.6600207090377808
iteration 224, loss = 0.7344356775283813
iteration 225, loss = 0.64983731508255
iteration 226, loss = 0.7202242612838745
iteration 227, loss = 0.6816279292106628
iteration 228, loss = 0.769130289554596
iteration 229, loss = 0.6569445133209229
iteration 230, loss = 0.5862830281257629
iteration 231, loss = 0.589729368686676
iteration 232, loss = 0.7768713235855103
iteration 233, loss = 0.6467195749282837
iteration 234, loss = 0.7185940742492676
iteration 235, loss = 0.6909865140914917
iteration 236, loss = 0.5979131460189819
iteration 237, loss = 0.6952120065689087
iteration 238, loss = 0.6687107086181641
iteration 239, loss = 0.7000396847724915
iteration 240, loss = 0.8308888673782349
iteration 241, loss = 0.7469483613967896
iteration 242, loss = 0.7342945337295532
iteration 243, loss = 0.7555683851242065
iteration 244, loss = 0.6442546844482422
iteration 245, loss = 0.7598004341125488
iteration 246, loss = 0.8224531412124634
iteration 247, loss = 0.6136416792869568
iteration 248, loss = 0.587064266204834
iteration 249, loss = 0.7224118709564209
iteration 250, loss = 0.7680433988571167
iteration 251, loss = 0.7701336145401001
iteration 252, loss = 0.8128598928451538
iteration 253, loss = 0.7454878091812134
iteration 254, loss = 0.613304615020752
iteration 255, loss = 0.6116065979003906
iteration 256, loss = 0.681918203830719
iteration 257, loss = 0.5793133974075317
iteration 258, loss = 0.6708935499191284
iteration 259, loss = 0.7764949202537537
iteration 260, loss = 0.7123439908027649
iteration 261, loss = 0.6892831325531006
iteration 262, loss = 0.7127217054367065
iteration 263, loss = 0.6729061603546143
iteration 264, loss = 0.669138491153717
iteration 265, loss = 0.6334064602851868
iteration 266, loss = 0.6400668621063232
iteration 267, loss = 0.6285219192504883
iteration 268, loss = 0.7961898446083069
iteration 269, loss = 0.7572396993637085
iteration 270, loss = 0.7742001414299011
iteration 271, loss = 0.7376284599304199
iteration 272, loss = 0.7116903066635132
iteration 273, loss = 0.7385596632957458
iteration 274, loss = 0.6975522637367249
iteration 275, loss = 0.64385986328125
iteration 276, loss = 0.6880889534950256
iteration 277, loss = 0.533790647983551
iteration 278, loss = 0.6653170585632324
iteration 279, loss = 0.6952376365661621
iteration 280, loss = 0.7399857044219971
iteration 281, loss = 0.733244776725769
iteration 282, loss = 0.747115433216095
iteration 283, loss = 0.749468207359314
iteration 284, loss = 0.6697776317596436
iteration 285, loss = 0.6880171895027161
iteration 286, loss = 0.672909140586853
iteration 287, loss = 0.7786946892738342
iteration 288, loss = 0.7703077793121338
iteration 289, loss = 0.7870895266532898
iteration 290, loss = 0.7377480268478394
iteration 291, loss = 0.6247153878211975
iteration 292, loss = 0.6545844674110413
iteration 293, loss = 0.6689797043800354
iteration 294, loss = 0.7241343259811401
iteration 295, loss = 0.6556344032287598
iteration 296, loss = 0.6325327157974243
iteration 297, loss = 0.6434957981109619
iteration 298, loss = 0.6176633834838867
iteration 299, loss = 0.716855525970459
iteration 300, loss = 0.6841309070587158
iteration 1, loss = 0.6864668726921082
iteration 2, loss = 0.6827222108840942
iteration 3, loss = 0.6986693143844604
iteration 4, loss = 0.6077038645744324
iteration 5, loss = 0.5877021551132202
iteration 6, loss = 0.6081023216247559
iteration 7, loss = 0.7145813703536987
iteration 8, loss = 0.6714550256729126
iteration 9, loss = 0.6657354235649109
iteration 10, loss = 0.6729495525360107
iteration 11, loss = 0.6472405791282654
iteration 12, loss = 0.7151230573654175
iteration 13, loss = 0.605514645576477
iteration 14, loss = 0.6543365120887756
iteration 15, loss = 0.573238730430603
iteration 16, loss = 0.617435872554779
iteration 17, loss = 0.6232826113700867
iteration 18, loss = 0.6866382956504822
iteration 19, loss = 0.712841272354126
iteration 20, loss = 0.5874029994010925
iteration 21, loss = 0.7265716195106506
iteration 22, loss = 0.7485142350196838
iteration 23, loss = 0.6378480792045593
iteration 24, loss = 0.6497629880905151
iteration 25, loss = 0.6335601210594177
iteration 26, loss = 0.5820980668067932
iteration 27, loss = 0.7822303175926208
iteration 28, loss = 0.7412807941436768
iteration 29, loss = 0.7680730819702148
iteration 30, loss = 0.6205754280090332
iteration 31, loss = 0.6890305876731873
iteration 32, loss = 0.7643334269523621
iteration 33, loss = 0.6223408579826355
iteration 34, loss = 0.615267813205719
iteration 35, loss = 0.628480851650238
iteration 36, loss = 0.6579670310020447
iteration 37, loss = 0.6566451787948608
iteration 38, loss = 0.5856446623802185
iteration 39, loss = 0.667604923248291
iteration 40, loss = 0.7331647872924805
iteration 41, loss = 0.6882559657096863
iteration 42, loss = 0.6255375146865845
iteration 43, loss = 0.667797863483429
iteration 44, loss = 0.7813801765441895
iteration 45, loss = 0.6490921974182129
iteration 46, loss = 0.8586947321891785
iteration 47, loss = 0.6996719837188721
iteration 48, loss = 0.6903476715087891
iteration 49, loss = 0.687917947769165
iteration 50, loss = 0.6285430192947388
iteration 51, loss = 0.6819157600402832
iteration 52, loss = 0.5765085816383362
iteration 53, loss = 0.7198390960693359
iteration 54, loss = 0.5594766139984131
iteration 55, loss = 0.6504592895507812
iteration 56, loss = 0.6471145153045654
iteration 57, loss = 0.5915202498435974
iteration 58, loss = 0.7249426245689392
iteration 59, loss = 0.6756534576416016
iteration 60, loss = 0.6546316742897034
iteration 61, loss = 0.6086607575416565
iteration 62, loss = 0.705949604511261
iteration 63, loss = 0.7044961452484131
iteration 64, loss = 0.6516873836517334
iteration 65, loss = 0.7235118746757507
iteration 66, loss = 0.5577296614646912
iteration 67, loss = 0.550961971282959
iteration 68, loss = 0.5817407965660095
iteration 69, loss = 0.646944522857666
iteration 70, loss = 0.6976546049118042
iteration 71, loss = 0.6565572023391724
iteration 72, loss = 0.6148945093154907
iteration 73, loss = 0.6770597100257874
iteration 74, loss = 0.6638007760047913
iteration 75, loss = 0.6672719717025757
iteration 76, loss = 0.8043466210365295
iteration 77, loss = 0.7505213022232056
iteration 78, loss = 0.7446180582046509
iteration 79, loss = 0.5716180801391602
iteration 80, loss = 0.6413269639015198
iteration 81, loss = 0.7490160465240479
iteration 82, loss = 0.5859819650650024
iteration 83, loss = 0.7199903726577759
iteration 84, loss = 0.6881950497627258
iteration 85, loss = 0.6905564665794373
iteration 86, loss = 0.6979213953018188
iteration 87, loss = 0.6722331047058105
iteration 88, loss = 0.4951772391796112
iteration 89, loss = 0.645732045173645
iteration 90, loss = 0.7298613786697388
iteration 91, loss = 0.7190550565719604
iteration 92, loss = 0.6861972808837891
iteration 93, loss = 0.6672865748405457
iteration 94, loss = 0.6480575799942017
iteration 95, loss = 0.6325741410255432
iteration 96, loss = 0.6598329544067383
iteration 97, loss = 0.6047146320343018
iteration 98, loss = 0.5723743438720703
iteration 99, loss = 0.6129934191703796
iteration 100, loss = 0.6080174446105957
iteration 101, loss = 0.630383312702179
iteration 102, loss = 0.7115405797958374
iteration 103, loss = 0.6662535667419434
iteration 104, loss = 0.6838948130607605
iteration 105, loss = 0.6121034622192383
iteration 106, loss = 0.6249123811721802
iteration 107, loss = 0.790739893913269
iteration 108, loss = 0.6588445901870728
iteration 109, loss = 0.533431351184845
iteration 110, loss = 0.7016359567642212
iteration 111, loss = 0.6350822448730469
iteration 112, loss = 0.6602919101715088
iteration 113, loss = 0.5979959964752197
iteration 114, loss = 0.7088944911956787
iteration 115, loss = 0.5752778649330139
iteration 116, loss = 0.5727735161781311
iteration 117, loss = 0.784969687461853
iteration 118, loss = 0.6655474305152893
iteration 119, loss = 0.7373184561729431
iteration 120, loss = 0.5826807022094727
iteration 121, loss = 0.6559379696846008
iteration 122, loss = 0.66804039478302
iteration 123, loss = 0.57172030210495
iteration 124, loss = 0.7174630165100098
iteration 125, loss = 0.7372283935546875
iteration 126, loss = 0.5851305723190308
iteration 127, loss = 0.6264321804046631
iteration 128, loss = 0.6507594585418701
iteration 129, loss = 0.6675474643707275
iteration 130, loss = 0.639413595199585
iteration 131, loss = 0.5726801753044128
iteration 132, loss = 0.6188914179801941
iteration 133, loss = 0.5521606206893921
iteration 134, loss = 0.5915437936782837
iteration 135, loss = 0.5800521969795227
iteration 136, loss = 0.6251822113990784
iteration 137, loss = 0.5735201239585876
iteration 138, loss = 0.730389416217804
iteration 139, loss = 0.8182007074356079
iteration 140, loss = 0.659822940826416
iteration 141, loss = 0.5159831047058105
iteration 142, loss = 0.6474565863609314
iteration 143, loss = 0.6143373847007751
iteration 144, loss = 0.658165693283081
iteration 145, loss = 0.6389968991279602
iteration 146, loss = 0.6017171740531921
iteration 147, loss = 0.6048717498779297
iteration 148, loss = 0.5919872522354126
iteration 149, loss = 0.6579525470733643
iteration 150, loss = 0.6512553691864014
iteration 151, loss = 0.6224114894866943
iteration 152, loss = 0.5613516569137573
iteration 153, loss = 0.6536459922790527
iteration 154, loss = 0.6774758100509644
iteration 155, loss = 0.6300495266914368
iteration 156, loss = 0.6857492923736572
iteration 157, loss = 0.6514071822166443
iteration 158, loss = 0.6588231921195984
iteration 159, loss = 0.5034703016281128
iteration 160, loss = 0.6552086472511292
iteration 161, loss = 0.5781139731407166
iteration 162, loss = 0.6749674081802368
iteration 163, loss = 0.7076238393783569
iteration 164, loss = 0.608456015586853
iteration 165, loss = 0.6399153470993042
iteration 166, loss = 0.600557267665863
iteration 167, loss = 0.6887792348861694
iteration 168, loss = 0.6297946572303772
iteration 169, loss = 0.7296448945999146
iteration 170, loss = 0.6718089580535889
iteration 171, loss = 0.6405069828033447
iteration 172, loss = 0.6739014387130737
iteration 173, loss = 0.5687928795814514
iteration 174, loss = 0.656670868396759
iteration 175, loss = 0.6871453523635864
iteration 176, loss = 0.6169888973236084
iteration 177, loss = 0.566331684589386
iteration 178, loss = 0.6443119645118713
iteration 179, loss = 0.59388267993927
iteration 180, loss = 0.5894039869308472
iteration 181, loss = 0.6881197690963745
iteration 182, loss = 0.5508220195770264
iteration 183, loss = 0.6108813285827637
iteration 184, loss = 0.6343892812728882
iteration 185, loss = 0.5923348069190979
iteration 186, loss = 0.5414510369300842
iteration 187, loss = 0.6911531686782837
iteration 188, loss = 0.6394317150115967
iteration 189, loss = 0.6495811343193054
iteration 190, loss = 0.6218005418777466
iteration 191, loss = 0.6051763296127319
iteration 192, loss = 0.6425738334655762
iteration 193, loss = 0.6011348962783813
iteration 194, loss = 0.631811261177063
iteration 195, loss = 0.6859989166259766
iteration 196, loss = 0.6461890935897827
iteration 197, loss = 0.6118785738945007
iteration 198, loss = 0.6569015383720398
iteration 199, loss = 0.6391684412956238
iteration 200, loss = 0.6847468614578247
iteration 201, loss = 0.6284782886505127
iteration 202, loss = 0.5914775133132935
iteration 203, loss = 0.65699303150177
iteration 204, loss = 0.6150398254394531
iteration 205, loss = 0.5955129265785217
iteration 206, loss = 0.6832451224327087
iteration 207, loss = 0.579674482345581
iteration 208, loss = 0.6835613250732422
iteration 209, loss = 0.5841602087020874
iteration 210, loss = 0.525266170501709
iteration 211, loss = 0.6027630567550659
iteration 212, loss = 0.572963297367096
iteration 213, loss = 0.6740818023681641
iteration 214, loss = 0.6644516587257385
iteration 215, loss = 0.6445883512496948
iteration 216, loss = 0.6474384665489197
iteration 217, loss = 0.583645761013031
iteration 218, loss = 0.7229316234588623
iteration 219, loss = 0.7464916706085205
iteration 220, loss = 0.5936449766159058
iteration 221, loss = 0.6098968982696533
iteration 222, loss = 0.6509755849838257
iteration 223, loss = 0.6263684630393982
iteration 224, loss = 0.5589146018028259
iteration 225, loss = 0.5915783047676086
iteration 226, loss = 0.557860791683197
iteration 227, loss = 0.5688421726226807
iteration 228, loss = 0.6029011607170105
iteration 229, loss = 0.6221300959587097
iteration 230, loss = 0.6547931432723999
iteration 231, loss = 0.5334843397140503
iteration 232, loss = 0.7324981689453125
iteration 233, loss = 0.6084709763526917
iteration 234, loss = 0.7174467444419861
iteration 235, loss = 0.6607133150100708
iteration 236, loss = 0.6553376317024231
iteration 237, loss = 0.7239581942558289
iteration 238, loss = 0.6095725893974304
iteration 239, loss = 0.6093001961708069
iteration 240, loss = 0.55461186170578
iteration 241, loss = 0.5529772639274597
iteration 242, loss = 0.6471132636070251
iteration 243, loss = 0.6022621393203735
iteration 244, loss = 0.5689737796783447
iteration 245, loss = 0.5330048203468323
iteration 246, loss = 0.636027455329895
iteration 247, loss = 0.4792276620864868
iteration 248, loss = 0.5774818062782288
iteration 249, loss = 0.5707176327705383
iteration 250, loss = 0.6249744892120361
iteration 251, loss = 0.6482442617416382
iteration 252, loss = 0.5946256518363953
iteration 253, loss = 0.556794285774231
iteration 254, loss = 0.6169458031654358
iteration 255, loss = 0.6353946924209595
iteration 256, loss = 0.5816673636436462
iteration 257, loss = 0.6222530007362366
iteration 258, loss = 0.6479855179786682
iteration 259, loss = 0.6387743353843689
iteration 260, loss = 0.6879397630691528
iteration 261, loss = 0.5315893888473511
iteration 262, loss = 0.5083991289138794
iteration 263, loss = 0.5939948558807373
iteration 264, loss = 0.4969506859779358
iteration 265, loss = 0.6389322876930237
iteration 266, loss = 0.5449747443199158
iteration 267, loss = 0.6235238313674927
iteration 268, loss = 0.5173743963241577
iteration 269, loss = 0.5572564601898193
iteration 270, loss = 0.6036730408668518
iteration 271, loss = 0.7587101459503174
iteration 272, loss = 0.49765312671661377
iteration 273, loss = 0.6896056532859802
iteration 274, loss = 0.6796252131462097
iteration 275, loss = 0.6448656320571899
iteration 276, loss = 0.6503176689147949
iteration 277, loss = 0.5787363648414612
iteration 278, loss = 0.6081796884536743
iteration 279, loss = 0.5320051908493042
iteration 280, loss = 0.6903541684150696
iteration 281, loss = 0.49447205662727356
iteration 282, loss = 0.5708928108215332
iteration 283, loss = 0.6012856960296631
iteration 284, loss = 0.7208391427993774
iteration 285, loss = 0.42378294467926025
iteration 286, loss = 0.5637739896774292
iteration 287, loss = 0.6086307764053345
iteration 288, loss = 0.5647361278533936
iteration 289, loss = 0.6563382744789124
iteration 290, loss = 0.5182743668556213
iteration 291, loss = 0.5237518548965454
iteration 292, loss = 0.5880000591278076
iteration 293, loss = 0.7082480192184448
iteration 294, loss = 0.6994534730911255
iteration 295, loss = 0.5882768034934998
iteration 296, loss = 0.7267613410949707
iteration 297, loss = 0.4852508306503296
iteration 298, loss = 0.6323533058166504
iteration 299, loss = 0.6106700301170349
iteration 300, loss = 0.5616225004196167
iteration 1, loss = 0.6030451059341431
iteration 2, loss = 0.6158674955368042
iteration 3, loss = 0.5242107510566711
iteration 4, loss = 0.6474457383155823
iteration 5, loss = 0.5646646022796631
iteration 6, loss = 0.5642046928405762
iteration 7, loss = 0.6069313287734985
iteration 8, loss = 0.6302914619445801
iteration 9, loss = 0.6434445977210999
iteration 10, loss = 0.6709318161010742
iteration 11, loss = 0.5390398502349854
iteration 12, loss = 0.6310555934906006
iteration 13, loss = 0.5053751468658447
iteration 14, loss = 0.6514239311218262
iteration 15, loss = 0.7247068285942078
iteration 16, loss = 0.5945953726768494
iteration 17, loss = 0.507132351398468
iteration 18, loss = 0.4756361246109009
iteration 19, loss = 0.5364429354667664
iteration 20, loss = 0.627805233001709
iteration 21, loss = 0.558266818523407
iteration 22, loss = 0.5517950654029846
iteration 23, loss = 0.5583508014678955
iteration 24, loss = 0.46241793036460876
iteration 25, loss = 0.5490051507949829
iteration 26, loss = 0.5733640193939209
iteration 27, loss = 0.5527200698852539
iteration 28, loss = 0.6460853219032288
iteration 29, loss = 0.5667548179626465
iteration 30, loss = 0.6567614078521729
iteration 31, loss = 0.5991461873054504
iteration 32, loss = 0.5712201595306396
iteration 33, loss = 0.68244469165802
iteration 34, loss = 0.6017212867736816
iteration 35, loss = 0.5956945419311523
iteration 36, loss = 0.6582638621330261
iteration 37, loss = 0.7014695405960083
iteration 38, loss = 0.5095764398574829
iteration 39, loss = 0.4949365258216858
iteration 40, loss = 0.5963631272315979
iteration 41, loss = 0.5351168513298035
iteration 42, loss = 0.49683016538619995
iteration 43, loss = 0.527599573135376
iteration 44, loss = 0.6553707122802734
iteration 45, loss = 0.6016906499862671
iteration 46, loss = 0.5207107067108154
iteration 47, loss = 0.6105012893676758
iteration 48, loss = 0.6053891181945801
iteration 49, loss = 0.5407106876373291
iteration 50, loss = 0.6616795063018799
iteration 51, loss = 0.605000376701355
iteration 52, loss = 0.47177839279174805
iteration 53, loss = 0.5720224976539612
iteration 54, loss = 0.6470819711685181
iteration 55, loss = 0.7082628607749939
iteration 56, loss = 0.5026834011077881
iteration 57, loss = 0.4827805757522583
iteration 58, loss = 0.6037737131118774
iteration 59, loss = 0.5967229008674622
iteration 60, loss = 0.5231172442436218
iteration 61, loss = 0.5415684580802917
iteration 62, loss = 0.6850385665893555
iteration 63, loss = 0.6217508316040039
iteration 64, loss = 0.614813506603241
iteration 65, loss = 0.6759817600250244
iteration 66, loss = 0.6405780911445618
iteration 67, loss = 0.4775053560733795
iteration 68, loss = 0.495482861995697
iteration 69, loss = 0.6464669704437256
iteration 70, loss = 0.5911586284637451
iteration 71, loss = 0.518149733543396
iteration 72, loss = 0.580557107925415
iteration 73, loss = 0.572564423084259
iteration 74, loss = 0.5015192031860352
iteration 75, loss = 0.6077109575271606
iteration 76, loss = 0.4721410572528839
iteration 77, loss = 0.6784875392913818
iteration 78, loss = 0.4647141098976135
iteration 79, loss = 0.5616376399993896
iteration 80, loss = 0.5800819396972656
iteration 81, loss = 0.6560007333755493
iteration 82, loss = 0.6786497235298157
iteration 83, loss = 0.5486485362052917
iteration 84, loss = 0.6425073146820068
iteration 85, loss = 0.5718567371368408
iteration 86, loss = 0.5406794548034668
iteration 87, loss = 0.5287463665008545
iteration 88, loss = 0.5548082590103149
iteration 89, loss = 0.6212391257286072
iteration 90, loss = 0.5500949025154114
iteration 91, loss = 0.4243520498275757
iteration 92, loss = 0.5129731893539429
iteration 93, loss = 0.5647651553153992
iteration 94, loss = 0.5096968412399292
iteration 95, loss = 0.6220970153808594
iteration 96, loss = 0.5130451917648315
iteration 97, loss = 0.561520516872406
iteration 98, loss = 0.505181610584259
iteration 99, loss = 0.779207170009613
iteration 100, loss = 0.6844140291213989
iteration 101, loss = 0.7743368148803711
iteration 102, loss = 0.5236525535583496
iteration 103, loss = 0.4504249095916748
iteration 104, loss = 0.6913881301879883
iteration 105, loss = 0.5903482437133789
iteration 106, loss = 0.5947588682174683
iteration 107, loss = 0.5599032640457153
iteration 108, loss = 0.47802871465682983
iteration 109, loss = 0.6252990961074829
iteration 110, loss = 0.5468824505805969
iteration 111, loss = 0.5525078773498535
iteration 112, loss = 0.4682595431804657
iteration 113, loss = 0.5727876424789429
iteration 114, loss = 0.4868853688240051
iteration 115, loss = 0.6608576774597168
iteration 116, loss = 0.5279507637023926
iteration 117, loss = 0.41125527024269104
iteration 118, loss = 0.5299723148345947
iteration 119, loss = 0.6593027710914612
iteration 120, loss = 0.611737847328186
iteration 121, loss = 0.5584254860877991
iteration 122, loss = 0.4863922595977783
iteration 123, loss = 0.7001981139183044
iteration 124, loss = 0.43602582812309265
iteration 125, loss = 0.5551342964172363
iteration 126, loss = 0.38403627276420593
iteration 127, loss = 0.6716845035552979
iteration 128, loss = 0.5467420816421509
iteration 129, loss = 0.6079880595207214
iteration 130, loss = 0.49711596965789795
iteration 131, loss = 0.4835900068283081
iteration 132, loss = 0.606695294380188
iteration 133, loss = 0.5505783557891846
iteration 134, loss = 0.5157870054244995
iteration 135, loss = 0.5260300040245056
iteration 136, loss = 0.5778962969779968
iteration 137, loss = 0.5439745187759399
iteration 138, loss = 0.4752908945083618
iteration 139, loss = 0.5550339818000793
iteration 140, loss = 0.4681693911552429
iteration 141, loss = 0.48315680027008057
iteration 142, loss = 0.7051275372505188
iteration 143, loss = 0.5317479372024536
iteration 144, loss = 0.46685001254081726
iteration 145, loss = 0.48222091794013977
iteration 146, loss = 0.5539671182632446
iteration 147, loss = 0.5041613578796387
iteration 148, loss = 0.5035974383354187
iteration 149, loss = 0.5052265524864197
iteration 150, loss = 0.6301999092102051
iteration 151, loss = 0.5314916968345642
iteration 152, loss = 0.5019005537033081
iteration 153, loss = 0.4961438775062561
iteration 154, loss = 0.5596354603767395
iteration 155, loss = 0.4706474542617798
iteration 156, loss = 0.59913170337677
iteration 157, loss = 0.6712983846664429
iteration 158, loss = 0.5141966342926025
iteration 159, loss = 0.6568461060523987
iteration 160, loss = 0.5358383655548096
iteration 161, loss = 0.6024916768074036
iteration 162, loss = 0.4415428340435028
iteration 163, loss = 0.4638805389404297
iteration 164, loss = 0.6345348954200745
iteration 165, loss = 0.5713014602661133
iteration 166, loss = 0.6085329651832581
iteration 167, loss = 0.4828186333179474
iteration 168, loss = 0.6361154317855835
iteration 169, loss = 0.5210815072059631
iteration 170, loss = 0.4992189407348633
iteration 171, loss = 0.47547706961631775
iteration 172, loss = 0.46391531825065613
iteration 173, loss = 0.5293951034545898
iteration 174, loss = 0.3720320761203766
iteration 175, loss = 0.5373236536979675
iteration 176, loss = 0.4359505772590637
iteration 177, loss = 0.605711817741394
iteration 178, loss = 0.5356239676475525
iteration 179, loss = 0.7113106846809387
iteration 180, loss = 0.46477848291397095
iteration 181, loss = 0.6367445588111877
iteration 182, loss = 0.6066467761993408
iteration 183, loss = 0.5360806584358215
iteration 184, loss = 0.4829050898551941
iteration 185, loss = 0.4713422954082489
iteration 186, loss = 0.5957499742507935
iteration 187, loss = 0.634965181350708
iteration 188, loss = 0.562375545501709
iteration 189, loss = 0.5930984616279602
iteration 190, loss = 0.608156144618988
iteration 191, loss = 0.5752555727958679
iteration 192, loss = 0.4128608703613281
iteration 193, loss = 0.433563768863678
iteration 194, loss = 0.467183917760849
iteration 195, loss = 0.46218836307525635
iteration 196, loss = 0.4039367139339447
iteration 197, loss = 0.5209763646125793
iteration 198, loss = 0.4757571220397949
iteration 199, loss = 0.6690901517868042
iteration 200, loss = 0.5347742438316345
iteration 201, loss = 0.3997434675693512
iteration 202, loss = 0.4547951817512512
iteration 203, loss = 0.47574836015701294
iteration 204, loss = 0.4988706409931183
iteration 205, loss = 0.5061479210853577
iteration 206, loss = 0.42553508281707764
iteration 207, loss = 0.6595006585121155
iteration 208, loss = 0.600527286529541
iteration 209, loss = 0.4814431369304657
iteration 210, loss = 0.49868276715278625
iteration 211, loss = 0.5808653235435486
iteration 212, loss = 0.5759766697883606
iteration 213, loss = 0.5109843611717224
iteration 214, loss = 0.40485361218452454
iteration 215, loss = 0.5373623371124268
iteration 216, loss = 0.44637593626976013
iteration 217, loss = 0.6278609037399292
iteration 218, loss = 0.4454379975795746
iteration 219, loss = 0.4594288468360901
iteration 220, loss = 0.48784950375556946
iteration 221, loss = 0.534623920917511
iteration 222, loss = 0.5888193845748901
iteration 223, loss = 0.502205491065979
iteration 224, loss = 0.6556858420372009
iteration 225, loss = 0.38829848170280457
iteration 226, loss = 0.6449220180511475
iteration 227, loss = 0.5634143352508545
iteration 228, loss = 0.5392323136329651
iteration 229, loss = 0.5149690508842468
iteration 230, loss = 0.5108121633529663
iteration 231, loss = 0.4634154140949249
iteration 232, loss = 0.5912023782730103
iteration 233, loss = 0.568290650844574
iteration 234, loss = 0.6652887463569641
iteration 235, loss = 0.5175298452377319
iteration 236, loss = 0.43867194652557373
iteration 237, loss = 0.4939642548561096
iteration 238, loss = 0.44930362701416016
iteration 239, loss = 0.5806090831756592
iteration 240, loss = 0.5090171098709106
iteration 241, loss = 0.520429253578186
iteration 242, loss = 0.48079997301101685
iteration 243, loss = 0.6555696725845337
iteration 244, loss = 0.5476182103157043
iteration 245, loss = 0.5658169388771057
iteration 246, loss = 0.6455202102661133
iteration 247, loss = 0.6939466595649719
iteration 248, loss = 0.5765427350997925
iteration 249, loss = 0.48134979605674744
iteration 250, loss = 0.45259860157966614
iteration 251, loss = 0.4128667414188385
iteration 252, loss = 0.6308308839797974
iteration 253, loss = 0.325004905462265
iteration 254, loss = 0.4074496626853943
iteration 255, loss = 0.5440199375152588
iteration 256, loss = 0.5335272550582886
iteration 257, loss = 0.5596527457237244
iteration 258, loss = 0.5741264820098877
iteration 259, loss = 0.5065286755561829
iteration 260, loss = 0.43663954734802246
iteration 261, loss = 0.5682640075683594
iteration 262, loss = 0.47577962279319763
iteration 263, loss = 0.5615299940109253
iteration 264, loss = 0.49663883447647095
iteration 265, loss = 0.5950330495834351
iteration 266, loss = 0.5220744609832764
iteration 267, loss = 0.5494803190231323
iteration 268, loss = 0.3748282790184021
iteration 269, loss = 0.4856729805469513
iteration 270, loss = 0.4365130662918091
iteration 271, loss = 0.5410088300704956
iteration 272, loss = 0.5950692892074585
iteration 273, loss = 0.5546877980232239
iteration 274, loss = 0.4979249835014343
iteration 275, loss = 0.509326696395874
iteration 276, loss = 0.4924857020378113
iteration 277, loss = 0.5255684852600098
iteration 278, loss = 0.7315006256103516
iteration 279, loss = 0.5853906273841858
iteration 280, loss = 0.41744619607925415
iteration 281, loss = 0.4768940508365631
iteration 282, loss = 0.5063301920890808
iteration 283, loss = 0.5809342861175537
iteration 284, loss = 0.3764009475708008
iteration 285, loss = 0.46864449977874756
iteration 286, loss = 0.5055625438690186
iteration 287, loss = 0.39839601516723633
iteration 288, loss = 0.4403986930847168
iteration 289, loss = 0.5913439989089966
iteration 290, loss = 0.5354084372520447
iteration 291, loss = 0.384101539850235
iteration 292, loss = 0.7190183401107788
iteration 293, loss = 0.44154590368270874
iteration 294, loss = 0.5253744721412659
iteration 295, loss = 0.37510502338409424
iteration 296, loss = 0.3857455849647522
iteration 297, loss = 0.6807328462600708
iteration 298, loss = 0.4653884470462799
iteration 299, loss = 0.5430753231048584
iteration 300, loss = 0.33512401580810547
iteration 1, loss = 0.39607226848602295
iteration 2, loss = 0.41067972779273987
iteration 3, loss = 0.5307386517524719
iteration 4, loss = 0.351873517036438
iteration 5, loss = 0.5848361849784851
iteration 6, loss = 0.607563853263855
iteration 7, loss = 0.39595043659210205
iteration 8, loss = 0.5796050429344177
iteration 9, loss = 0.524736225605011
iteration 10, loss = 0.35584381222724915
iteration 11, loss = 0.5106766819953918
iteration 12, loss = 0.4986189603805542
iteration 13, loss = 0.5057922601699829
iteration 14, loss = 0.6588878631591797
iteration 15, loss = 0.4976702928543091
iteration 16, loss = 0.5546608567237854
iteration 17, loss = 0.44942328333854675
iteration 18, loss = 0.38186219334602356
iteration 19, loss = 0.4807198643684387
iteration 20, loss = 0.6131028532981873
iteration 21, loss = 0.5232287049293518
iteration 22, loss = 0.5266750454902649
iteration 23, loss = 0.4625108540058136
iteration 24, loss = 0.5465357899665833
iteration 25, loss = 0.5444764494895935
iteration 26, loss = 0.4222923815250397
iteration 27, loss = 0.5275779366493225
iteration 28, loss = 0.5271455645561218
iteration 29, loss = 0.362618625164032
iteration 30, loss = 0.36691978573799133
iteration 31, loss = 0.37967172265052795
iteration 32, loss = 0.5956261157989502
iteration 33, loss = 0.3808768093585968
iteration 34, loss = 0.4418231248855591
iteration 35, loss = 0.5109723806381226
iteration 36, loss = 0.3907894194126129
iteration 37, loss = 0.4278818964958191
iteration 38, loss = 0.39127659797668457
iteration 39, loss = 0.35854363441467285
iteration 40, loss = 0.38960257172584534
iteration 41, loss = 0.4687373638153076
iteration 42, loss = 0.4985648989677429
iteration 43, loss = 0.4812067747116089
iteration 44, loss = 0.42461034655570984
iteration 45, loss = 0.49306538701057434
iteration 46, loss = 0.43544840812683105
iteration 47, loss = 0.5196631550788879
iteration 48, loss = 0.5453065633773804
iteration 49, loss = 0.3749224543571472
iteration 50, loss = 0.46733859181404114
iteration 51, loss = 0.47245800495147705
iteration 52, loss = 0.5269292593002319
iteration 53, loss = 0.3572508692741394
iteration 54, loss = 0.469830721616745
iteration 55, loss = 0.5640380382537842
iteration 56, loss = 0.420009970664978
iteration 57, loss = 0.5492143630981445
iteration 58, loss = 0.5500470399856567
iteration 59, loss = 0.36416712403297424
iteration 60, loss = 0.4815865755081177
iteration 61, loss = 0.4580147862434387
iteration 62, loss = 0.5920574069023132
iteration 63, loss = 0.45931583642959595
iteration 64, loss = 0.5032472014427185
iteration 65, loss = 0.5697950720787048
iteration 66, loss = 0.4173102080821991
iteration 67, loss = 0.4223962724208832
iteration 68, loss = 0.4794684648513794
iteration 69, loss = 0.4422062635421753
iteration 70, loss = 0.5182706117630005
iteration 71, loss = 0.4478273093700409
iteration 72, loss = 0.44902074337005615
iteration 73, loss = 0.532009482383728
iteration 74, loss = 0.532433807849884
iteration 75, loss = 0.433110773563385
iteration 76, loss = 0.5063720941543579
iteration 77, loss = 0.5723214149475098
iteration 78, loss = 0.48086458444595337
iteration 79, loss = 0.41391298174858093
iteration 80, loss = 0.4534146189689636
iteration 81, loss = 0.3783428966999054
iteration 82, loss = 0.5053526163101196
iteration 83, loss = 0.428597629070282
iteration 84, loss = 0.393398642539978
iteration 85, loss = 0.5525506734848022
iteration 86, loss = 0.42974853515625
iteration 87, loss = 0.39061403274536133
iteration 88, loss = 0.533936619758606
iteration 89, loss = 0.3833877146244049
iteration 90, loss = 0.49131953716278076
iteration 91, loss = 0.6252590417861938
iteration 92, loss = 0.5730504989624023
iteration 93, loss = 0.5223268866539001
iteration 94, loss = 0.41612112522125244
iteration 95, loss = 0.48202335834503174
iteration 96, loss = 0.5581985712051392
iteration 97, loss = 0.41265687346458435
iteration 98, loss = 0.4214446544647217
iteration 99, loss = 0.3599775731563568
iteration 100, loss = 0.5286121368408203
iteration 101, loss = 0.389144629240036
iteration 102, loss = 0.5699262619018555
iteration 103, loss = 0.5309232473373413
iteration 104, loss = 0.39986252784729004
iteration 105, loss = 0.368852436542511
iteration 106, loss = 0.43875789642333984
iteration 107, loss = 0.5040285587310791
iteration 108, loss = 0.4111872613430023
iteration 109, loss = 0.5741807222366333
iteration 110, loss = 0.2628069818019867
iteration 111, loss = 0.515556812286377
iteration 112, loss = 0.5494738817214966
iteration 113, loss = 0.4273925721645355
iteration 114, loss = 0.40350058674812317
iteration 115, loss = 0.3486086130142212
iteration 116, loss = 0.4977802038192749
iteration 117, loss = 0.44280338287353516
iteration 118, loss = 0.5148063898086548
iteration 119, loss = 0.4505935311317444
iteration 120, loss = 0.40217670798301697
iteration 121, loss = 0.406017005443573
iteration 122, loss = 0.37588047981262207
iteration 123, loss = 0.4756849706172943
iteration 124, loss = 0.49094855785369873
iteration 125, loss = 0.5264744162559509
iteration 126, loss = 0.5031946897506714
iteration 127, loss = 0.44162437319755554
iteration 128, loss = 0.49477440118789673
iteration 129, loss = 0.5827759504318237
iteration 130, loss = 0.6253509521484375
iteration 131, loss = 0.41590094566345215
iteration 132, loss = 0.4069225788116455
iteration 133, loss = 0.4173309803009033
iteration 134, loss = 0.3758407235145569
iteration 135, loss = 0.3518344461917877
iteration 136, loss = 0.5161920785903931
iteration 137, loss = 0.39559006690979004
iteration 138, loss = 0.46984344720840454
iteration 139, loss = 0.29899832606315613
iteration 140, loss = 0.49475857615470886
iteration 141, loss = 0.6419939398765564
iteration 142, loss = 0.5517635941505432
iteration 143, loss = 0.5808379650115967
iteration 144, loss = 0.4887612462043762
iteration 145, loss = 0.47832250595092773
iteration 146, loss = 0.4950610101222992
iteration 147, loss = 0.3925847113132477
iteration 148, loss = 0.44149482250213623
iteration 149, loss = 0.4245118200778961
iteration 150, loss = 0.4162515103816986
iteration 151, loss = 0.49568378925323486
iteration 152, loss = 0.5781618356704712
iteration 153, loss = 0.2894234359264374
iteration 154, loss = 0.4117993414402008
iteration 155, loss = 0.40277552604675293
iteration 156, loss = 0.4663006663322449
iteration 157, loss = 0.32001039385795593
iteration 158, loss = 0.47242873907089233
iteration 159, loss = 0.41425445675849915
iteration 160, loss = 0.3927684426307678
iteration 161, loss = 0.5097642540931702
iteration 162, loss = 0.334169864654541
iteration 163, loss = 0.3808431029319763
iteration 164, loss = 0.4221895933151245
iteration 165, loss = 0.47553107142448425
iteration 166, loss = 0.3744964003562927
iteration 167, loss = 0.3345002830028534
iteration 168, loss = 0.500424861907959
iteration 169, loss = 0.7255761623382568
iteration 170, loss = 0.40455901622772217
iteration 171, loss = 0.4342222809791565
iteration 172, loss = 0.4847923219203949
iteration 173, loss = 0.4940226674079895
iteration 174, loss = 0.4719254970550537
iteration 175, loss = 0.4366491138935089
iteration 176, loss = 0.39789828658103943
iteration 177, loss = 0.46709227561950684
iteration 178, loss = 0.5426601767539978
iteration 179, loss = 0.4484744071960449
iteration 180, loss = 0.3316259980201721
iteration 181, loss = 0.37486839294433594
iteration 182, loss = 0.45929351449012756
iteration 183, loss = 0.38420557975769043
iteration 184, loss = 0.3049866855144501
iteration 185, loss = 0.48821672797203064
iteration 186, loss = 0.5367652773857117
iteration 187, loss = 0.2789234519004822
iteration 188, loss = 0.38192084431648254
iteration 189, loss = 0.385844349861145
iteration 190, loss = 0.3168642520904541
iteration 191, loss = 0.32828468084335327
iteration 192, loss = 0.3269549608230591
iteration 193, loss = 0.43791264295578003
iteration 194, loss = 0.35785067081451416
iteration 195, loss = 0.48346632719039917
iteration 196, loss = 0.582109808921814
iteration 197, loss = 0.25458112359046936
iteration 198, loss = 0.415964275598526
iteration 199, loss = 0.4879206717014313
iteration 200, loss = 0.2949511706829071
iteration 201, loss = 0.37801581621170044
iteration 202, loss = 0.37643057107925415
iteration 203, loss = 0.3559974133968353
iteration 204, loss = 0.2778165936470032
iteration 205, loss = 0.41827666759490967
iteration 206, loss = 0.31373855471611023
iteration 207, loss = 0.45242494344711304
iteration 208, loss = 0.43883585929870605
iteration 209, loss = 0.3240131139755249
iteration 210, loss = 0.4505792260169983
iteration 211, loss = 0.44909268617630005
iteration 212, loss = 0.40237823128700256
iteration 213, loss = 0.39045649766921997
iteration 214, loss = 0.6300970315933228
iteration 215, loss = 0.3712018132209778
iteration 216, loss = 0.38732507824897766
iteration 217, loss = 0.3460840582847595
iteration 218, loss = 0.4509793519973755
iteration 219, loss = 0.49133723974227905
iteration 220, loss = 0.4301466643810272
iteration 221, loss = 0.4241962134838104
iteration 222, loss = 0.456002414226532
iteration 223, loss = 0.5671228170394897
iteration 224, loss = 0.4417947232723236
iteration 225, loss = 0.5290831327438354
iteration 226, loss = 0.37299618124961853
iteration 227, loss = 0.5539117455482483
iteration 228, loss = 0.3670805096626282
iteration 229, loss = 0.37795817852020264
iteration 230, loss = 0.4549447298049927
iteration 231, loss = 0.31468603014945984
iteration 232, loss = 0.4612404704093933
iteration 233, loss = 0.38506239652633667
iteration 234, loss = 0.5451712608337402
iteration 235, loss = 0.2724757194519043
iteration 236, loss = 0.4889850318431854
iteration 237, loss = 0.45302391052246094
iteration 238, loss = 0.38579487800598145
iteration 239, loss = 0.4460476040840149
iteration 240, loss = 0.4564228653907776
iteration 241, loss = 0.367878258228302
iteration 242, loss = 0.43704986572265625
iteration 243, loss = 0.4026912450790405
iteration 244, loss = 0.5002644658088684
iteration 245, loss = 0.4302196502685547
iteration 246, loss = 0.257500559091568
iteration 247, loss = 0.47307074069976807
iteration 248, loss = 0.4626270830631256
iteration 249, loss = 0.3857279419898987
iteration 250, loss = 0.30183419585227966
iteration 251, loss = 0.4170251190662384
iteration 252, loss = 0.3067781329154968
iteration 253, loss = 0.2808963656425476
iteration 254, loss = 0.48280131816864014
iteration 255, loss = 0.3050788938999176
iteration 256, loss = 0.4638254642486572
iteration 257, loss = 0.5899880528450012
iteration 258, loss = 0.45005178451538086
iteration 259, loss = 0.4416200518608093
iteration 260, loss = 0.30161619186401367
iteration 261, loss = 0.45611438155174255
iteration 262, loss = 0.3629296123981476
iteration 263, loss = 0.3823888897895813
iteration 264, loss = 0.33879873156547546
iteration 265, loss = 0.3874042332172394
iteration 266, loss = 0.38558387756347656
iteration 267, loss = 0.31251150369644165
iteration 268, loss = 0.6494840979576111
iteration 269, loss = 0.435372531414032
iteration 270, loss = 0.3342283070087433
iteration 271, loss = 0.3730306029319763
iteration 272, loss = 0.31206217408180237
iteration 273, loss = 0.42733412981033325
iteration 274, loss = 0.3138708472251892
iteration 275, loss = 0.5409626960754395
iteration 276, loss = 0.2747950255870819
iteration 277, loss = 0.4190654456615448
iteration 278, loss = 0.38749420642852783
iteration 279, loss = 0.3216918408870697
iteration 280, loss = 0.4291059970855713
iteration 281, loss = 0.3741694688796997
iteration 282, loss = 0.43714141845703125
iteration 283, loss = 0.4183764159679413
iteration 284, loss = 0.47013217210769653
iteration 285, loss = 0.25035178661346436
iteration 286, loss = 0.3440173864364624
iteration 287, loss = 0.27690497040748596
iteration 288, loss = 0.3688512444496155
iteration 289, loss = 0.4624412953853607
iteration 290, loss = 0.29643118381500244
iteration 291, loss = 0.3725227415561676
iteration 292, loss = 0.3826249837875366
iteration 293, loss = 0.34438320994377136
iteration 294, loss = 0.3754129409790039
iteration 295, loss = 0.4815821647644043
iteration 296, loss = 0.4243728518486023
iteration 297, loss = 0.3314482569694519
iteration 298, loss = 0.4073082506656647
iteration 299, loss = 0.46043306589126587
iteration 300, loss = 0.413891077041626
iteration 1, loss = 0.41537249088287354
iteration 2, loss = 0.4624093770980835
iteration 3, loss = 0.35214847326278687
iteration 4, loss = 0.31873443722724915
iteration 5, loss = 0.45719125866889954
iteration 6, loss = 0.5015960931777954
iteration 7, loss = 0.3191699683666229
iteration 8, loss = 0.3230356276035309
iteration 9, loss = 0.3440435826778412
iteration 10, loss = 0.4376521110534668
iteration 11, loss = 0.338042676448822
iteration 12, loss = 0.31715333461761475
iteration 13, loss = 0.4334214925765991
iteration 14, loss = 0.3025127351284027
iteration 15, loss = 0.4290052652359009
iteration 16, loss = 0.32736659049987793
iteration 17, loss = 0.43645939230918884
iteration 18, loss = 0.5344984531402588
iteration 19, loss = 0.34056028723716736
iteration 20, loss = 0.48425906896591187
iteration 21, loss = 0.3329457640647888
iteration 22, loss = 0.41655996441841125
iteration 23, loss = 0.4325469732284546
iteration 24, loss = 0.3892847001552582
iteration 25, loss = 0.3529590368270874
iteration 26, loss = 0.46465569734573364
iteration 27, loss = 0.3033964931964874
iteration 28, loss = 0.40610170364379883
iteration 29, loss = 0.4186854362487793
iteration 30, loss = 0.48199987411499023
iteration 31, loss = 0.40652433037757874
iteration 32, loss = 0.30244046449661255
iteration 33, loss = 0.3510209918022156
iteration 34, loss = 0.3317989706993103
iteration 35, loss = 0.2809734642505646
iteration 36, loss = 0.2994654178619385
iteration 37, loss = 0.36542996764183044
iteration 38, loss = 0.24181781709194183
iteration 39, loss = 0.45639073848724365
iteration 40, loss = 0.4470396041870117
iteration 41, loss = 0.34407907724380493
iteration 42, loss = 0.4343865215778351
iteration 43, loss = 0.35441040992736816
iteration 44, loss = 0.2788298428058624
iteration 45, loss = 0.44063323736190796
iteration 46, loss = 0.3880375325679779
iteration 47, loss = 0.30125436186790466
iteration 48, loss = 0.2916854918003082
iteration 49, loss = 0.37192344665527344
iteration 50, loss = 0.3563012182712555
iteration 51, loss = 0.37115272879600525
iteration 52, loss = 0.32115480303764343
iteration 53, loss = 0.27332907915115356
iteration 54, loss = 0.5766408443450928
iteration 55, loss = 0.3106481432914734
iteration 56, loss = 0.3625459671020508
iteration 57, loss = 0.4324701428413391
iteration 58, loss = 0.3932409882545471
iteration 59, loss = 0.5068447589874268
iteration 60, loss = 0.3861076831817627
iteration 61, loss = 0.2547959089279175
iteration 62, loss = 0.3120570182800293
iteration 63, loss = 0.3302311599254608
iteration 64, loss = 0.29813510179519653
iteration 65, loss = 0.49681341648101807
iteration 66, loss = 0.33581188321113586
iteration 67, loss = 0.4030504524707794
iteration 68, loss = 0.38555556535720825
iteration 69, loss = 0.3868807554244995
iteration 70, loss = 0.24410927295684814
iteration 71, loss = 0.2260998785495758
iteration 72, loss = 0.22647084295749664
iteration 73, loss = 0.3209248185157776
iteration 74, loss = 0.24800564348697662
iteration 75, loss = 0.3447123169898987
iteration 76, loss = 0.3841424584388733
iteration 77, loss = 0.2333115190267563
iteration 78, loss = 0.36534836888313293
iteration 79, loss = 0.3761928379535675
iteration 80, loss = 0.3271118402481079
iteration 81, loss = 0.43813222646713257
iteration 82, loss = 0.2904796302318573
iteration 83, loss = 0.37195107340812683
iteration 84, loss = 0.38867419958114624
iteration 85, loss = 0.3261164426803589
iteration 86, loss = 0.34419623017311096
iteration 87, loss = 0.38877272605895996
iteration 88, loss = 0.3016170263290405
iteration 89, loss = 0.2727476954460144
iteration 90, loss = 0.5034310221672058
iteration 91, loss = 0.35665395855903625
iteration 92, loss = 0.4066534638404846
iteration 93, loss = 0.41956883668899536
iteration 94, loss = 0.39439940452575684
iteration 95, loss = 0.1906767636537552
iteration 96, loss = 0.2971291244029999
iteration 97, loss = 0.3380563259124756
iteration 98, loss = 0.4434375762939453
iteration 99, loss = 0.4148625135421753
iteration 100, loss = 0.31346771121025085
iteration 101, loss = 0.38039395213127136
iteration 102, loss = 0.5953468084335327
iteration 103, loss = 0.6582900285720825
iteration 104, loss = 0.29353243112564087
iteration 105, loss = 0.37606915831565857
iteration 106, loss = 0.39004939794540405
iteration 107, loss = 0.31239399313926697
iteration 108, loss = 0.3531264066696167
iteration 109, loss = 0.33990776538848877
iteration 110, loss = 0.3394598960876465
iteration 111, loss = 0.3729286789894104
iteration 112, loss = 0.2561355233192444
iteration 113, loss = 0.27351677417755127
iteration 114, loss = 0.434444785118103
iteration 115, loss = 0.24666151404380798
iteration 116, loss = 0.2906641364097595
iteration 117, loss = 0.2749929130077362
iteration 118, loss = 0.3966926336288452
iteration 119, loss = 0.43947046995162964
iteration 120, loss = 0.33002328872680664
iteration 121, loss = 0.3157654404640198
iteration 122, loss = 0.43306657671928406
iteration 123, loss = 0.30750685930252075
iteration 124, loss = 0.3339991271495819
iteration 125, loss = 0.29047003388404846
iteration 126, loss = 0.3963087499141693
iteration 127, loss = 0.3883923292160034
iteration 128, loss = 0.3484566807746887
iteration 129, loss = 0.37905341386795044
iteration 130, loss = 0.3086722493171692
iteration 131, loss = 0.2174452245235443
iteration 132, loss = 0.278626024723053
iteration 133, loss = 0.2895745635032654
iteration 134, loss = 0.29345643520355225
iteration 135, loss = 0.28962770104408264
iteration 136, loss = 0.32499000430107117
iteration 137, loss = 0.3357042074203491
iteration 138, loss = 0.2841477394104004
iteration 139, loss = 0.37404143810272217
iteration 140, loss = 0.31437692046165466
iteration 141, loss = 0.3636952042579651
iteration 142, loss = 0.2616462707519531
iteration 143, loss = 0.4293842911720276
iteration 144, loss = 0.4808400273323059
iteration 145, loss = 0.3059701919555664
iteration 146, loss = 0.37993934750556946
iteration 147, loss = 0.3203195631504059
iteration 148, loss = 0.39079928398132324
iteration 149, loss = 0.39096105098724365
iteration 150, loss = 0.2897557318210602
iteration 151, loss = 0.3737837076187134
iteration 152, loss = 0.256592333316803
iteration 153, loss = 0.3081343472003937
iteration 154, loss = 0.25186699628829956
iteration 155, loss = 0.34327730536460876
iteration 156, loss = 0.39784157276153564
iteration 157, loss = 0.2031356394290924
iteration 158, loss = 0.20022642612457275
iteration 159, loss = 0.30255064368247986
iteration 160, loss = 0.29743465781211853
iteration 161, loss = 0.38501453399658203
iteration 162, loss = 0.3008067011833191
iteration 163, loss = 0.3253509998321533
iteration 164, loss = 0.4225323796272278
iteration 165, loss = 0.20248284935951233
iteration 166, loss = 0.41015416383743286
iteration 167, loss = 0.34518957138061523
iteration 168, loss = 0.3756199777126312
iteration 169, loss = 0.4708899259567261
iteration 170, loss = 0.2957376539707184
iteration 171, loss = 0.3262821435928345
iteration 172, loss = 0.2626079320907593
iteration 173, loss = 0.4175669550895691
iteration 174, loss = 0.2557191252708435
iteration 175, loss = 0.16719742119312286
iteration 176, loss = 0.30414387583732605
iteration 177, loss = 0.20584627985954285
iteration 178, loss = 0.24984872341156006
iteration 179, loss = 0.31830286979675293
iteration 180, loss = 0.2660459578037262
iteration 181, loss = 0.4928388297557831
iteration 182, loss = 0.3320150375366211
iteration 183, loss = 0.2722868323326111
iteration 184, loss = 0.31434813141822815
iteration 185, loss = 0.293472021818161
iteration 186, loss = 0.2979912757873535
iteration 187, loss = 0.4805176556110382
iteration 188, loss = 0.240793377161026
iteration 189, loss = 0.39387017488479614
iteration 190, loss = 0.3017685115337372
iteration 191, loss = 0.2599095106124878
iteration 192, loss = 0.39050471782684326
iteration 193, loss = 0.3759108781814575
iteration 194, loss = 0.25037720799446106
iteration 195, loss = 0.23444396257400513
iteration 196, loss = 0.2994050979614258
iteration 197, loss = 0.30410489439964294
iteration 198, loss = 0.25447529554367065
iteration 199, loss = 0.33897531032562256
iteration 200, loss = 0.27770930528640747
iteration 201, loss = 0.3066725730895996
iteration 202, loss = 0.26640528440475464
iteration 203, loss = 0.24380455911159515
iteration 204, loss = 0.3905373215675354
iteration 205, loss = 0.29982417821884155
iteration 206, loss = 0.2948797643184662
iteration 207, loss = 0.5178209543228149
iteration 208, loss = 0.3273182511329651
iteration 209, loss = 0.21840548515319824
iteration 210, loss = 0.3026866614818573
iteration 211, loss = 0.21871252357959747
iteration 212, loss = 0.26163622736930847
iteration 213, loss = 0.3259543776512146
iteration 214, loss = 0.2873814105987549
iteration 215, loss = 0.19949257373809814
iteration 216, loss = 0.2522003948688507
iteration 217, loss = 0.2832825779914856
iteration 218, loss = 0.3744213283061981
iteration 219, loss = 0.3186655044555664
iteration 220, loss = 0.23110975325107574
iteration 221, loss = 0.40353888273239136
iteration 222, loss = 0.18868353962898254
iteration 223, loss = 0.41884708404541016
iteration 224, loss = 0.30291590094566345
iteration 225, loss = 0.3545583486557007
iteration 226, loss = 0.2497178465127945
iteration 227, loss = 0.4166497588157654
iteration 228, loss = 0.29275283217430115
iteration 229, loss = 0.34755098819732666
iteration 230, loss = 0.1956048309803009
iteration 231, loss = 0.37180280685424805
iteration 232, loss = 0.3084242343902588
iteration 233, loss = 0.1963251531124115
iteration 234, loss = 0.2378198206424713
iteration 235, loss = 0.259869247674942
iteration 236, loss = 0.31850019097328186
iteration 237, loss = 0.3252485990524292
iteration 238, loss = 0.26652517914772034
iteration 239, loss = 0.2522217631340027
iteration 240, loss = 0.19049233198165894
iteration 241, loss = 0.2618469297885895
iteration 242, loss = 0.3139703869819641
iteration 243, loss = 0.417338490486145
iteration 244, loss = 0.3290363848209381
iteration 245, loss = 0.21965990960597992
iteration 246, loss = 0.28161346912384033
iteration 247, loss = 0.46358194947242737
iteration 248, loss = 0.3642538785934448
iteration 249, loss = 0.2824477553367615
iteration 250, loss = 0.19854892790317535
iteration 251, loss = 0.2640376091003418
iteration 252, loss = 0.27703505754470825
iteration 253, loss = 0.1662965565919876
iteration 254, loss = 0.3787079155445099
iteration 255, loss = 0.2088189274072647
iteration 256, loss = 0.3018268346786499
iteration 257, loss = 0.30183613300323486
iteration 258, loss = 0.3351629972457886
iteration 259, loss = 0.4641503691673279
iteration 260, loss = 0.2143685668706894
iteration 261, loss = 0.2314835637807846
iteration 262, loss = 0.1845984309911728
iteration 263, loss = 0.21862581372261047
iteration 264, loss = 0.42630696296691895
iteration 265, loss = 0.4491046667098999
iteration 266, loss = 0.4244038462638855
iteration 267, loss = 0.24621537327766418
iteration 268, loss = 0.1632116436958313
iteration 269, loss = 0.16163815557956696
iteration 270, loss = 0.26043498516082764
iteration 271, loss = 0.31349319219589233
iteration 272, loss = 0.2202107459306717
iteration 273, loss = 0.29803869128227234
iteration 274, loss = 0.26764628291130066
iteration 275, loss = 0.2854468822479248
iteration 276, loss = 0.5774829387664795
iteration 277, loss = 0.48241952061653137
iteration 278, loss = 0.22701936960220337
iteration 279, loss = 0.33442074060440063
iteration 280, loss = 0.3768904209136963
iteration 281, loss = 0.35772380232810974
iteration 282, loss = 0.39414161443710327
iteration 283, loss = 0.2509111166000366
iteration 284, loss = 0.25854286551475525
iteration 285, loss = 0.26952800154685974
iteration 286, loss = 0.282551109790802
iteration 287, loss = 0.3965762257575989
iteration 288, loss = 0.20333440601825714
iteration 289, loss = 0.25387322902679443
iteration 290, loss = 0.3080502152442932
iteration 291, loss = 0.3042265772819519
iteration 292, loss = 0.18836233019828796
iteration 293, loss = 0.32831698656082153
iteration 294, loss = 0.32828694581985474
iteration 295, loss = 0.16753678023815155
iteration 296, loss = 0.2517286539077759
iteration 297, loss = 0.22025352716445923
iteration 298, loss = 0.15891122817993164
iteration 299, loss = 0.27048760652542114
iteration 300, loss = 0.19487382471561432
iteration 1, loss = 0.418515145778656
iteration 2, loss = 0.32219740748405457
iteration 3, loss = 0.3668569028377533
iteration 4, loss = 0.2297580987215042
iteration 5, loss = 0.23622757196426392
iteration 6, loss = 0.389299601316452
iteration 7, loss = 0.19187887012958527
iteration 8, loss = 0.33392333984375
iteration 9, loss = 0.29686832427978516
iteration 10, loss = 0.4515596628189087
iteration 11, loss = 0.37791067361831665
iteration 12, loss = 0.2518448829650879
iteration 13, loss = 0.22752949595451355
iteration 14, loss = 0.18262681365013123
iteration 15, loss = 0.26984715461730957
iteration 16, loss = 0.23677854239940643
iteration 17, loss = 0.23937591910362244
iteration 18, loss = 0.4040563106536865
iteration 19, loss = 0.34756287932395935
iteration 20, loss = 0.24666354060173035
iteration 21, loss = 0.17491647601127625
iteration 22, loss = 0.2199038714170456
iteration 23, loss = 0.2817378342151642
iteration 24, loss = 0.24997958540916443
iteration 25, loss = 0.28814223408699036
iteration 26, loss = 0.29098430275917053
iteration 27, loss = 0.2763368785381317
iteration 28, loss = 0.1888379454612732
iteration 29, loss = 0.1947399228811264
iteration 30, loss = 0.1803966909646988
iteration 31, loss = 0.2140693962574005
iteration 32, loss = 0.29807692766189575
iteration 33, loss = 0.3286740779876709
iteration 34, loss = 0.21528245508670807
iteration 35, loss = 0.2789320647716522
iteration 36, loss = 0.20182180404663086
iteration 37, loss = 0.20161621272563934
iteration 38, loss = 0.23531877994537354
iteration 39, loss = 0.301308274269104
iteration 40, loss = 0.25770124793052673
iteration 41, loss = 0.22466033697128296
iteration 42, loss = 0.2935570478439331
iteration 43, loss = 0.3486124277114868
iteration 44, loss = 0.2869284152984619
iteration 45, loss = 0.2383117377758026
iteration 46, loss = 0.16446752846240997
iteration 47, loss = 0.3178703784942627
iteration 48, loss = 0.29263007640838623
iteration 49, loss = 0.35147812962532043
iteration 50, loss = 0.25015437602996826
iteration 51, loss = 0.31159812211990356
iteration 52, loss = 0.2500057816505432
iteration 53, loss = 0.339632511138916
iteration 54, loss = 0.34311240911483765
iteration 55, loss = 0.1537427455186844
iteration 56, loss = 0.15430361032485962
iteration 57, loss = 0.3340783417224884
iteration 58, loss = 0.26580411195755005
iteration 59, loss = 0.22344212234020233
iteration 60, loss = 0.3327098488807678
iteration 61, loss = 0.29721707105636597
iteration 62, loss = 0.22158271074295044
iteration 63, loss = 0.2037619948387146
iteration 64, loss = 0.3890781104564667
iteration 65, loss = 0.20528453588485718
iteration 66, loss = 0.2540149688720703
iteration 67, loss = 0.20300570130348206
iteration 68, loss = 0.25711655616760254
iteration 69, loss = 0.4731563925743103
iteration 70, loss = 0.2469450682401657
iteration 71, loss = 0.17293016612529755
iteration 72, loss = 0.31003910303115845
iteration 73, loss = 0.3432063162326813
iteration 74, loss = 0.3601111173629761
iteration 75, loss = 0.389923095703125
iteration 76, loss = 0.27928251028060913
iteration 77, loss = 0.3477592468261719
iteration 78, loss = 0.2766103148460388
iteration 79, loss = 0.14865532517433167
iteration 80, loss = 0.14618290960788727
iteration 81, loss = 0.25604724884033203
iteration 82, loss = 0.30677101016044617
iteration 83, loss = 0.5550084710121155
iteration 84, loss = 0.23603597283363342
iteration 85, loss = 0.17530208826065063
iteration 86, loss = 0.3322129547595978
iteration 87, loss = 0.28163155913352966
iteration 88, loss = 0.431961327791214
iteration 89, loss = 0.20382684469223022
iteration 90, loss = 0.28625452518463135
iteration 91, loss = 0.22774311900138855
iteration 92, loss = 0.2229667454957962
iteration 93, loss = 0.18569746613502502
iteration 94, loss = 0.29905614256858826
iteration 95, loss = 0.38272619247436523
iteration 96, loss = 0.42619788646698
iteration 97, loss = 0.2940448522567749
iteration 98, loss = 0.27585816383361816
iteration 99, loss = 0.20608867704868317
iteration 100, loss = 0.26114681363105774
iteration 101, loss = 0.25256744027137756
iteration 102, loss = 0.1942460983991623
iteration 103, loss = 0.30703622102737427
iteration 104, loss = 0.32244208455085754
iteration 105, loss = 0.26315078139305115
iteration 106, loss = 0.2765890061855316
iteration 107, loss = 0.19979290664196014
iteration 108, loss = 0.22658534348011017
iteration 109, loss = 0.16913220286369324
iteration 110, loss = 0.1995173692703247
iteration 111, loss = 0.24705448746681213
iteration 112, loss = 0.3026919364929199
iteration 113, loss = 0.2870365083217621
iteration 114, loss = 0.28066733479499817
iteration 115, loss = 0.22462919354438782
iteration 116, loss = 0.25053200125694275
iteration 117, loss = 0.390133261680603
iteration 118, loss = 0.2729823887348175
iteration 119, loss = 0.3952043652534485
iteration 120, loss = 0.12725578248500824
iteration 121, loss = 0.34649163484573364
iteration 122, loss = 0.3478301167488098
iteration 123, loss = 0.20079606771469116
iteration 124, loss = 0.2331569939851761
iteration 125, loss = 0.328889399766922
iteration 126, loss = 0.22598157823085785
iteration 127, loss = 0.23553062975406647
iteration 128, loss = 0.4127245247364044
iteration 129, loss = 0.2162405252456665
iteration 130, loss = 0.17077074944972992
iteration 131, loss = 0.23042500019073486
iteration 132, loss = 0.38263803720474243
iteration 133, loss = 0.2309829294681549
iteration 134, loss = 0.23073211312294006
iteration 135, loss = 0.4084927439689636
iteration 136, loss = 0.36019623279571533
iteration 137, loss = 0.33986765146255493
iteration 138, loss = 0.19994820654392242
iteration 139, loss = 0.39983272552490234
iteration 140, loss = 0.39964550733566284
iteration 141, loss = 0.259884238243103
iteration 142, loss = 0.27040421962738037
iteration 143, loss = 0.2645910978317261
iteration 144, loss = 0.23050519824028015
iteration 145, loss = 0.3397734761238098
iteration 146, loss = 0.18186838924884796
iteration 147, loss = 0.20967288315296173
iteration 148, loss = 0.1661592274904251
iteration 149, loss = 0.41490083932876587
iteration 150, loss = 0.16770724952220917
iteration 151, loss = 0.4031335711479187
iteration 152, loss = 0.16591209173202515
iteration 153, loss = 0.3050225079059601
iteration 154, loss = 0.28724756836891174
iteration 155, loss = 0.21706673502922058
iteration 156, loss = 0.1551518589258194
iteration 157, loss = 0.22205010056495667
iteration 158, loss = 0.3105453848838806
iteration 159, loss = 0.19108805060386658
iteration 160, loss = 0.37113243341445923
iteration 161, loss = 0.37732064723968506
iteration 162, loss = 0.18135374784469604
iteration 163, loss = 0.37699753046035767
iteration 164, loss = 0.23777060210704803
iteration 165, loss = 0.27542781829833984
iteration 166, loss = 0.25760623812675476
iteration 167, loss = 0.4069124758243561
iteration 168, loss = 0.32619547843933105
iteration 169, loss = 0.22970624268054962
iteration 170, loss = 0.17635484039783478
iteration 171, loss = 0.27601248025894165
iteration 172, loss = 0.2894296944141388
iteration 173, loss = 0.17440907657146454
iteration 174, loss = 0.2295905202627182
iteration 175, loss = 0.43156304955482483
iteration 176, loss = 0.21319779753684998
iteration 177, loss = 0.2109495997428894
iteration 178, loss = 0.30970078706741333
iteration 179, loss = 0.2595946192741394
iteration 180, loss = 0.3898624777793884
iteration 181, loss = 0.27115291357040405
iteration 182, loss = 0.19036486744880676
iteration 183, loss = 0.22753697633743286
iteration 184, loss = 0.13685400784015656
iteration 185, loss = 0.3212495744228363
iteration 186, loss = 0.30369260907173157
iteration 187, loss = 0.18126268684864044
iteration 188, loss = 0.1878071129322052
iteration 189, loss = 0.34350478649139404
iteration 190, loss = 0.2559782862663269
iteration 191, loss = 0.2495316118001938
iteration 192, loss = 0.3333264887332916
iteration 193, loss = 0.17437945306301117
iteration 194, loss = 0.4158816337585449
iteration 195, loss = 0.27673566341400146
iteration 196, loss = 0.3773365020751953
iteration 197, loss = 0.31371673941612244
iteration 198, loss = 0.32289955019950867
iteration 199, loss = 0.2409384548664093
iteration 200, loss = 0.35931673645973206
iteration 201, loss = 0.12929682433605194
iteration 202, loss = 0.3691765367984772
iteration 203, loss = 0.2225574553012848
iteration 204, loss = 0.20640087127685547
iteration 205, loss = 0.4592509865760803
iteration 206, loss = 0.2683212161064148
iteration 207, loss = 0.2851235568523407
iteration 208, loss = 0.33420538902282715
iteration 209, loss = 0.18038953840732574
iteration 210, loss = 0.29129067063331604
iteration 211, loss = 0.3198665380477905
iteration 212, loss = 0.18205276131629944
iteration 213, loss = 0.26158976554870605
iteration 214, loss = 0.17038221657276154
iteration 215, loss = 0.2541515827178955
iteration 216, loss = 0.2437494993209839
iteration 217, loss = 0.40862029790878296
iteration 218, loss = 0.38294413685798645
iteration 219, loss = 0.22474068403244019
iteration 220, loss = 0.35572612285614014
iteration 221, loss = 0.18651089072227478
iteration 222, loss = 0.2170473039150238
iteration 223, loss = 0.27264687418937683
iteration 224, loss = 0.27625608444213867
iteration 225, loss = 0.2207936942577362
iteration 226, loss = 0.5591028332710266
iteration 227, loss = 0.4371928870677948
iteration 228, loss = 0.24363505840301514
iteration 229, loss = 0.24845264852046967
iteration 230, loss = 0.22016575932502747
iteration 231, loss = 0.28254055976867676
iteration 232, loss = 0.18040964007377625
iteration 233, loss = 0.33298951387405396
iteration 234, loss = 0.3581863343715668
iteration 235, loss = 0.22659462690353394
iteration 236, loss = 0.25539880990982056
iteration 237, loss = 0.21069103479385376
iteration 238, loss = 0.35459235310554504
iteration 239, loss = 0.40004217624664307
iteration 240, loss = 0.17475004494190216
iteration 241, loss = 0.1581529676914215
iteration 242, loss = 0.3357388377189636
iteration 243, loss = 0.26247164607048035
iteration 244, loss = 0.18040308356285095
iteration 245, loss = 0.2738321125507355
iteration 246, loss = 0.26657599210739136
iteration 247, loss = 0.2021636813879013
iteration 248, loss = 0.13878579437732697
iteration 249, loss = 0.24057507514953613
iteration 250, loss = 0.23760150372982025
iteration 251, loss = 0.19454513490200043
iteration 252, loss = 0.22839660942554474
iteration 253, loss = 0.2276165634393692
iteration 254, loss = 0.28966575860977173
iteration 255, loss = 0.18171335756778717
iteration 256, loss = 0.20378117263317108
iteration 257, loss = 0.3480244576931
iteration 258, loss = 0.3550575375556946
iteration 259, loss = 0.22490692138671875
iteration 260, loss = 0.17965532839298248
iteration 261, loss = 0.15347450971603394
iteration 262, loss = 0.29321688413619995
iteration 263, loss = 0.30173125863075256
iteration 264, loss = 0.2610595226287842
iteration 265, loss = 0.19204729795455933
iteration 266, loss = 0.23205161094665527
iteration 267, loss = 0.2889822721481323
iteration 268, loss = 0.2627575695514679
iteration 269, loss = 0.17258313298225403
iteration 270, loss = 0.3380947709083557
iteration 271, loss = 0.18595361709594727
iteration 272, loss = 0.40833568572998047
iteration 273, loss = 0.21818172931671143
iteration 274, loss = 0.4168136417865753
iteration 275, loss = 0.3406464457511902
iteration 276, loss = 0.4039849042892456
iteration 277, loss = 0.4993930757045746
iteration 278, loss = 0.4072250723838806
iteration 279, loss = 0.15248432755470276
iteration 280, loss = 0.23487535119056702
iteration 281, loss = 0.3552458584308624
iteration 282, loss = 0.19324298202991486
iteration 283, loss = 0.26397281885147095
iteration 284, loss = 0.23350398242473602
iteration 285, loss = 0.29036885499954224
iteration 286, loss = 0.15663734078407288
iteration 287, loss = 0.3532731831073761
iteration 288, loss = 0.2867059111595154
iteration 289, loss = 0.32554301619529724
iteration 290, loss = 0.34236255288124084
iteration 291, loss = 0.3641735911369324
iteration 292, loss = 0.18662309646606445
iteration 293, loss = 0.354069322347641
iteration 294, loss = 0.162904754281044
iteration 295, loss = 0.1723290979862213
iteration 296, loss = 0.3246745467185974
iteration 297, loss = 0.2658942937850952
iteration 298, loss = 0.2775888741016388
iteration 299, loss = 0.2748987078666687
iteration 300, loss = 0.27204254269599915
iteration 1, loss = 0.18696635961532593
iteration 2, loss = 0.19485117495059967
iteration 3, loss = 0.29135358333587646
iteration 4, loss = 0.2533758878707886
iteration 5, loss = 0.3644312024116516
iteration 6, loss = 0.15406274795532227
iteration 7, loss = 0.23675519227981567
iteration 8, loss = 0.2927757203578949
iteration 9, loss = 0.31763342022895813
iteration 10, loss = 0.260259747505188
iteration 11, loss = 0.2474401891231537
iteration 12, loss = 0.37950021028518677
iteration 13, loss = 0.3477698862552643
iteration 14, loss = 0.3077288866043091
iteration 15, loss = 0.22498099505901337
iteration 16, loss = 0.32989031076431274
iteration 17, loss = 0.18515212833881378
iteration 18, loss = 0.2512928247451782
iteration 19, loss = 0.25784963369369507
iteration 20, loss = 0.37811294198036194
iteration 21, loss = 0.24667516350746155
iteration 22, loss = 0.24282880127429962
iteration 23, loss = 0.2707728147506714
iteration 24, loss = 0.16107161343097687
iteration 25, loss = 0.15311208367347717
iteration 26, loss = 0.19506534934043884
iteration 27, loss = 0.16693620383739471
iteration 28, loss = 0.2858995497226715
iteration 29, loss = 0.19392921030521393
iteration 30, loss = 0.3667428493499756
iteration 31, loss = 0.2974814772605896
iteration 32, loss = 0.29995113611221313
iteration 33, loss = 0.21205297112464905
iteration 34, loss = 0.22031375765800476
iteration 35, loss = 0.4232109487056732
iteration 36, loss = 0.2413022667169571
iteration 37, loss = 0.14593492448329926
iteration 38, loss = 0.3470510244369507
iteration 39, loss = 0.25529250502586365
iteration 40, loss = 0.2314586192369461
iteration 41, loss = 0.2633392810821533
iteration 42, loss = 0.2908884882926941
iteration 43, loss = 0.19267538189888
iteration 44, loss = 0.30754441022872925
iteration 45, loss = 0.25564730167388916
iteration 46, loss = 0.2692030072212219
iteration 47, loss = 0.20532023906707764
iteration 48, loss = 0.22963440418243408
iteration 49, loss = 0.4048328399658203
iteration 50, loss = 0.20130319893360138
iteration 51, loss = 0.21009407937526703
iteration 52, loss = 0.1427859663963318
iteration 53, loss = 0.24177514016628265
iteration 54, loss = 0.20929640531539917
iteration 55, loss = 0.3047148585319519
iteration 56, loss = 0.2711673378944397
iteration 57, loss = 0.3367031216621399
iteration 58, loss = 0.2114197313785553
iteration 59, loss = 0.2879941463470459
iteration 60, loss = 0.28465697169303894
iteration 61, loss = 0.24722221493721008
iteration 62, loss = 0.24507269263267517
iteration 63, loss = 0.3339565694332123
iteration 64, loss = 0.3024749159812927
iteration 65, loss = 0.384482204914093
iteration 66, loss = 0.4314950406551361
iteration 67, loss = 0.29597797989845276
iteration 68, loss = 0.5814942121505737
iteration 69, loss = 0.18875505030155182
iteration 70, loss = 0.2995929718017578
iteration 71, loss = 0.2483912706375122
iteration 72, loss = 0.15308736264705658
iteration 73, loss = 0.3287530541419983
iteration 74, loss = 0.2749898135662079
iteration 75, loss = 0.32949650287628174
iteration 76, loss = 0.15650223195552826
iteration 77, loss = 0.13445769250392914
iteration 78, loss = 0.12919102609157562
iteration 79, loss = 0.2446383684873581
iteration 80, loss = 0.1949150264263153
iteration 81, loss = 0.28146088123321533
iteration 82, loss = 0.24116836488246918
iteration 83, loss = 0.23882246017456055
iteration 84, loss = 0.22174298763275146
iteration 85, loss = 0.27141648530960083
iteration 86, loss = 0.18225550651550293
iteration 87, loss = 0.29480862617492676
iteration 88, loss = 0.2442770153284073
iteration 89, loss = 0.2513437569141388
iteration 90, loss = 0.30733606219291687
iteration 91, loss = 0.20069971680641174
iteration 92, loss = 0.45989981293678284
iteration 93, loss = 0.26056793332099915
iteration 94, loss = 0.3584388494491577
iteration 95, loss = 0.11224018782377243
iteration 96, loss = 0.19504699110984802
iteration 97, loss = 0.2512437701225281
iteration 98, loss = 0.31870290637016296
iteration 99, loss = 0.379193514585495
iteration 100, loss = 0.18689855933189392
iteration 101, loss = 0.2743677496910095
iteration 102, loss = 0.17711041867733002
iteration 103, loss = 0.23776081204414368
iteration 104, loss = 0.3192446231842041
iteration 105, loss = 0.24357953667640686
iteration 106, loss = 0.4631098806858063
iteration 107, loss = 0.20488789677619934
iteration 108, loss = 0.45565125346183777
iteration 109, loss = 0.15695367753505707
iteration 110, loss = 0.16708506643772125
iteration 111, loss = 0.33810335397720337
iteration 112, loss = 0.21105775237083435
iteration 113, loss = 0.33519065380096436
iteration 114, loss = 0.16760659217834473
iteration 115, loss = 0.27806752920150757
iteration 116, loss = 0.393386572599411
iteration 117, loss = 0.29477447271347046
iteration 118, loss = 0.2659728527069092
iteration 119, loss = 0.20148570835590363
iteration 120, loss = 0.3714672327041626
iteration 121, loss = 0.5589606761932373
iteration 122, loss = 0.21866561472415924
iteration 123, loss = 0.3017636835575104
iteration 124, loss = 0.40464669466018677
iteration 125, loss = 0.19316694140434265
iteration 126, loss = 0.2719576060771942
iteration 127, loss = 0.1957823783159256
iteration 128, loss = 0.20401151478290558
iteration 129, loss = 0.27634838223457336
iteration 130, loss = 0.38748255372047424
iteration 131, loss = 0.3387516438961029
iteration 132, loss = 0.23063530027866364
iteration 133, loss = 0.2682391107082367
iteration 134, loss = 0.23547208309173584
iteration 135, loss = 0.24881534278392792
iteration 136, loss = 0.26253941655158997
iteration 137, loss = 0.2961176633834839
iteration 138, loss = 0.2898540496826172
iteration 139, loss = 0.1753600537776947
iteration 140, loss = 0.33700862526893616
iteration 141, loss = 0.3486151099205017
iteration 142, loss = 0.4050423204898834
iteration 143, loss = 0.4347347021102905
iteration 144, loss = 0.31394246220588684
iteration 145, loss = 0.24095958471298218
iteration 146, loss = 0.30266180634498596
iteration 147, loss = 0.33679407835006714
iteration 148, loss = 0.2550767660140991
iteration 149, loss = 0.24406449496746063
iteration 150, loss = 0.16952748596668243
iteration 151, loss = 0.27135521173477173
iteration 152, loss = 0.2794998288154602
iteration 153, loss = 0.35789954662323
iteration 154, loss = 0.2960061728954315
iteration 155, loss = 0.21167750656604767
iteration 156, loss = 0.17027905583381653
iteration 157, loss = 0.17989161610603333
iteration 158, loss = 0.19885095953941345
iteration 159, loss = 0.3759695887565613
iteration 160, loss = 0.2937239408493042
iteration 161, loss = 0.24076956510543823
iteration 162, loss = 0.19361992180347443
iteration 163, loss = 0.2780664563179016
iteration 164, loss = 0.13517333567142487
iteration 165, loss = 0.35476255416870117
iteration 166, loss = 0.14308080077171326
iteration 167, loss = 0.47135213017463684
iteration 168, loss = 0.3237189054489136
iteration 169, loss = 0.2316529005765915
iteration 170, loss = 0.22504328191280365
iteration 171, loss = 0.27426987886428833
iteration 172, loss = 0.2583848237991333
iteration 173, loss = 0.2251243144273758
iteration 174, loss = 0.24382871389389038
iteration 175, loss = 0.22865188121795654
iteration 176, loss = 0.3661671280860901
iteration 177, loss = 0.3668995797634125
iteration 178, loss = 0.4462728500366211
iteration 179, loss = 0.2611231803894043
iteration 180, loss = 0.34471264481544495
iteration 181, loss = 0.12578564882278442
iteration 182, loss = 0.21967458724975586
iteration 183, loss = 0.28617972135543823
iteration 184, loss = 0.23931346833705902
iteration 185, loss = 0.32426655292510986
iteration 186, loss = 0.21331118047237396
iteration 187, loss = 0.4146845042705536
iteration 188, loss = 0.12858866155147552
iteration 189, loss = 0.32041803002357483
iteration 190, loss = 0.23452907800674438
iteration 191, loss = 0.20322082936763763
iteration 192, loss = 0.25594455003738403
iteration 193, loss = 0.3398129940032959
iteration 194, loss = 0.3031802773475647
iteration 195, loss = 0.162695974111557
iteration 196, loss = 0.47315406799316406
iteration 197, loss = 0.1472223997116089
iteration 198, loss = 0.24721544981002808
iteration 199, loss = 0.2277880311012268
iteration 200, loss = 0.14065876603126526
iteration 201, loss = 0.29553210735321045
iteration 202, loss = 0.28577882051467896
iteration 203, loss = 0.2987625002861023
iteration 204, loss = 0.17372536659240723
iteration 205, loss = 0.3269278407096863
iteration 206, loss = 0.1747715324163437
iteration 207, loss = 0.3088461458683014
iteration 208, loss = 0.2584371268749237
iteration 209, loss = 0.15279079973697662
iteration 210, loss = 0.36486363410949707
iteration 211, loss = 0.1816248893737793
iteration 212, loss = 0.18665805459022522
iteration 213, loss = 0.29846101999282837
iteration 214, loss = 0.19381599128246307
iteration 215, loss = 0.2791902720928192
iteration 216, loss = 0.31524795293807983
iteration 217, loss = 0.2658595144748688
iteration 218, loss = 0.1235240250825882
iteration 219, loss = 0.26822784543037415
iteration 220, loss = 0.19648900628089905
iteration 221, loss = 0.11733606457710266
iteration 222, loss = 0.16958816349506378
iteration 223, loss = 0.18264038860797882
iteration 224, loss = 0.30859869718551636
iteration 225, loss = 0.1811518371105194
iteration 226, loss = 0.1553545892238617
iteration 227, loss = 0.2374197244644165
iteration 228, loss = 0.3449608087539673
iteration 229, loss = 0.292184442281723
iteration 230, loss = 0.27107739448547363
iteration 231, loss = 0.4463769197463989
iteration 232, loss = 0.23760557174682617
iteration 233, loss = 0.39410749077796936
iteration 234, loss = 0.22098088264465332
iteration 235, loss = 0.1949191689491272
iteration 236, loss = 0.18971148133277893
iteration 237, loss = 0.1241384744644165
iteration 238, loss = 0.29704055190086365
iteration 239, loss = 0.24978040158748627
iteration 240, loss = 0.18845614790916443
iteration 241, loss = 0.1698480248451233
iteration 242, loss = 0.34040018916130066
iteration 243, loss = 0.2494574785232544
iteration 244, loss = 0.31413450837135315
iteration 245, loss = 0.5087931156158447
iteration 246, loss = 0.2451900839805603
iteration 247, loss = 0.20432958006858826
iteration 248, loss = 0.21347880363464355
iteration 249, loss = 0.16365531086921692
iteration 250, loss = 0.3514898419380188
iteration 251, loss = 0.32910507917404175
iteration 252, loss = 0.2914958894252777
iteration 253, loss = 0.27712690830230713
iteration 254, loss = 0.3423117995262146
iteration 255, loss = 0.21936024725437164
iteration 256, loss = 0.16747042536735535
iteration 257, loss = 0.2787635922431946
iteration 258, loss = 0.40219491720199585
iteration 259, loss = 0.33503419160842896
iteration 260, loss = 0.16711804270744324
iteration 261, loss = 0.2530539929866791
iteration 262, loss = 0.35432881116867065
iteration 263, loss = 0.31151729822158813
iteration 264, loss = 0.28185874223709106
iteration 265, loss = 0.24689240753650665
iteration 266, loss = 0.16535711288452148
iteration 267, loss = 0.28193628787994385
iteration 268, loss = 0.14209218323230743
iteration 269, loss = 0.25262370705604553
iteration 270, loss = 0.2455650418996811
iteration 271, loss = 0.3003160059452057
iteration 272, loss = 0.32727065682411194
iteration 273, loss = 0.24812966585159302
iteration 274, loss = 0.4716692268848419
iteration 275, loss = 0.23358562588691711
iteration 276, loss = 0.21827341616153717
iteration 277, loss = 0.3348213732242584
iteration 278, loss = 0.3411736786365509
iteration 279, loss = 0.32776549458503723
iteration 280, loss = 0.17417699098587036
iteration 281, loss = 0.19352243840694427
iteration 282, loss = 0.14147353172302246
iteration 283, loss = 0.3632631301879883
iteration 284, loss = 0.33150169253349304
iteration 285, loss = 0.2173052579164505
iteration 286, loss = 0.23187300562858582
iteration 287, loss = 0.2531691789627075
iteration 288, loss = 0.25157082080841064
iteration 289, loss = 0.17367297410964966
iteration 290, loss = 0.29368647933006287
iteration 291, loss = 0.3257300853729248
iteration 292, loss = 0.22761482000350952
iteration 293, loss = 0.31383299827575684
iteration 294, loss = 0.15432342886924744
iteration 295, loss = 0.29092609882354736
iteration 296, loss = 0.34559741616249084
iteration 297, loss = 0.18073126673698425
iteration 298, loss = 0.14726179838180542
iteration 299, loss = 0.11984160542488098
iteration 300, loss = 0.2740932106971741
iteration 1, loss = 0.1636238545179367
iteration 2, loss = 0.3866215646266937
iteration 3, loss = 0.3115939199924469
iteration 4, loss = 0.2780808210372925
iteration 5, loss = 0.27817681431770325
iteration 6, loss = 0.358157217502594
iteration 7, loss = 0.3145257532596588
iteration 8, loss = 0.2039804309606552
iteration 9, loss = 0.2510235011577606
iteration 10, loss = 0.1324530690908432
iteration 11, loss = 0.5001416206359863
iteration 12, loss = 0.23094595968723297
iteration 13, loss = 0.25013160705566406
iteration 14, loss = 0.4150812029838562
iteration 15, loss = 0.22124767303466797
iteration 16, loss = 0.2521224617958069
iteration 17, loss = 0.2183178961277008
iteration 18, loss = 0.266764372587204
iteration 19, loss = 0.3822270631790161
iteration 20, loss = 0.15466223657131195
iteration 21, loss = 0.38555800914764404
iteration 22, loss = 0.24714355170726776
iteration 23, loss = 0.2317740023136139
iteration 24, loss = 0.251918226480484
iteration 25, loss = 0.3283146023750305
iteration 26, loss = 0.2924998700618744
iteration 27, loss = 0.12544402480125427
iteration 28, loss = 0.19897763431072235
iteration 29, loss = 0.23999767005443573
iteration 30, loss = 0.3207245469093323
iteration 31, loss = 0.1267625242471695
iteration 32, loss = 0.3248208165168762
iteration 33, loss = 0.15377065539360046
iteration 34, loss = 0.34189027547836304
iteration 35, loss = 0.3051588535308838
iteration 36, loss = 0.33925148844718933
iteration 37, loss = 0.19723942875862122
iteration 38, loss = 0.2695119380950928
iteration 39, loss = 0.34954318404197693
iteration 40, loss = 0.15597698092460632
iteration 41, loss = 0.29587578773498535
iteration 42, loss = 0.34648942947387695
iteration 43, loss = 0.3003554046154022
iteration 44, loss = 0.24891135096549988
iteration 45, loss = 0.18211068212985992
iteration 46, loss = 0.25572827458381653
iteration 47, loss = 0.3380807042121887
iteration 48, loss = 0.28960537910461426
iteration 49, loss = 0.11575391888618469
iteration 50, loss = 0.4041827321052551
iteration 51, loss = 0.19083508849143982
iteration 52, loss = 0.38676780462265015
iteration 53, loss = 0.2750071883201599
iteration 54, loss = 0.2664143145084381
iteration 55, loss = 0.15948206186294556
iteration 56, loss = 0.17759650945663452
iteration 57, loss = 0.18455970287322998
iteration 58, loss = 0.32436928153038025
iteration 59, loss = 0.2592375576496124
iteration 60, loss = 0.2308705449104309
iteration 61, loss = 0.27085334062576294
iteration 62, loss = 0.12876255810260773
iteration 63, loss = 0.3450171649456024
iteration 64, loss = 0.23264503479003906
iteration 65, loss = 0.15701201558113098
iteration 66, loss = 0.29563823342323303
iteration 67, loss = 0.2480890452861786
iteration 68, loss = 0.20435334742069244
iteration 69, loss = 0.1834380030632019
iteration 70, loss = 0.3526182472705841
iteration 71, loss = 0.27433010935783386
iteration 72, loss = 0.3111220598220825
iteration 73, loss = 0.17420142889022827
iteration 74, loss = 0.25200355052948
iteration 75, loss = 0.2889516055583954
iteration 76, loss = 0.2836292088031769
iteration 77, loss = 0.24272780120372772
iteration 78, loss = 0.18843290209770203
iteration 79, loss = 0.18070125579833984
iteration 80, loss = 0.2526985704898834
iteration 81, loss = 0.2747289836406708
iteration 82, loss = 0.2474883496761322
iteration 83, loss = 0.16229857504367828
iteration 84, loss = 0.2128380388021469
iteration 85, loss = 0.34421873092651367
iteration 86, loss = 0.22890812158584595
iteration 87, loss = 0.3361421525478363
iteration 88, loss = 0.2915195822715759
iteration 89, loss = 0.28860801458358765
iteration 90, loss = 0.21066802740097046
iteration 91, loss = 0.25673723220825195
iteration 92, loss = 0.21996676921844482
iteration 93, loss = 0.28644317388534546
iteration 94, loss = 0.46127408742904663
iteration 95, loss = 0.2950681149959564
iteration 96, loss = 0.2670685946941376
iteration 97, loss = 0.17169484496116638
iteration 98, loss = 0.23687566816806793
iteration 99, loss = 0.21927422285079956
iteration 100, loss = 0.2521657943725586
iteration 101, loss = 0.3204846978187561
iteration 102, loss = 0.2512211501598358
iteration 103, loss = 0.38916733860969543
iteration 104, loss = 0.32090428471565247
iteration 105, loss = 0.2164406180381775
iteration 106, loss = 0.37787169218063354
iteration 107, loss = 0.2358403205871582
iteration 108, loss = 0.31583142280578613
iteration 109, loss = 0.30002304911613464
iteration 110, loss = 0.4723072052001953
iteration 111, loss = 0.33358436822891235
iteration 112, loss = 0.2610892653465271
iteration 113, loss = 0.2286316454410553
iteration 114, loss = 0.19300130009651184
iteration 115, loss = 0.2720751166343689
iteration 116, loss = 0.2610440254211426
iteration 117, loss = 0.4881647229194641
iteration 118, loss = 0.17685894668102264
iteration 119, loss = 0.1160663366317749
iteration 120, loss = 0.2938644289970398
iteration 121, loss = 0.277640163898468
iteration 122, loss = 0.21288156509399414
iteration 123, loss = 0.30807816982269287
iteration 124, loss = 0.2226649969816208
iteration 125, loss = 0.28817158937454224
iteration 126, loss = 0.13025125861167908
iteration 127, loss = 0.3034069538116455
iteration 128, loss = 0.21457909047603607
iteration 129, loss = 0.33599114418029785
iteration 130, loss = 0.13683563470840454
iteration 131, loss = 0.1743243932723999
iteration 132, loss = 0.2664702534675598
iteration 133, loss = 0.13823163509368896
iteration 134, loss = 0.14756391942501068
iteration 135, loss = 0.18517553806304932
iteration 136, loss = 0.24382555484771729
iteration 137, loss = 0.2879928946495056
iteration 138, loss = 0.16704699397087097
iteration 139, loss = 0.44415080547332764
iteration 140, loss = 0.2661547362804413
iteration 141, loss = 0.3002885580062866
iteration 142, loss = 0.22303274273872375
iteration 143, loss = 0.16152745485305786
iteration 144, loss = 0.13207679986953735
iteration 145, loss = 0.37889617681503296
iteration 146, loss = 0.2651016116142273
iteration 147, loss = 0.29382017254829407
iteration 148, loss = 0.2357192188501358
iteration 149, loss = 0.23365020751953125
iteration 150, loss = 0.18689849972724915
iteration 151, loss = 0.3048233389854431
iteration 152, loss = 0.17953920364379883
iteration 153, loss = 0.20953549444675446
iteration 154, loss = 0.42582204937934875
iteration 155, loss = 0.23155662417411804
iteration 156, loss = 0.22189216315746307
iteration 157, loss = 0.10690470784902573
iteration 158, loss = 0.35084068775177
iteration 159, loss = 0.16284288465976715
iteration 160, loss = 0.2406623214483261
iteration 161, loss = 0.188871368765831
iteration 162, loss = 0.18336550891399384
iteration 163, loss = 0.2994901239871979
iteration 164, loss = 0.20162323117256165
iteration 165, loss = 0.21863606572151184
iteration 166, loss = 0.32072654366493225
iteration 167, loss = 0.3190878629684448
iteration 168, loss = 0.33952516317367554
iteration 169, loss = 0.22250564396381378
iteration 170, loss = 0.16442684829235077
iteration 171, loss = 0.27415937185287476
iteration 172, loss = 0.10686399042606354
iteration 173, loss = 0.13018186390399933
iteration 174, loss = 0.2458943873643875
iteration 175, loss = 0.23083488643169403
iteration 176, loss = 0.25094228982925415
iteration 177, loss = 0.24850255250930786
iteration 178, loss = 0.25676655769348145
iteration 179, loss = 0.346267968416214
iteration 180, loss = 0.23563989996910095
iteration 181, loss = 0.17790848016738892
iteration 182, loss = 0.31736278533935547
iteration 183, loss = 0.4133543372154236
iteration 184, loss = 0.19237291812896729
iteration 185, loss = 0.3073105216026306
iteration 186, loss = 0.20014002919197083
iteration 187, loss = 0.1871356964111328
iteration 188, loss = 0.16846969723701477
iteration 189, loss = 0.21624533832073212
iteration 190, loss = 0.2439701110124588
iteration 191, loss = 0.23163321614265442
iteration 192, loss = 0.2743324637413025
iteration 193, loss = 0.16719666123390198
iteration 194, loss = 0.2971337139606476
iteration 195, loss = 0.16713617742061615
iteration 196, loss = 0.44711583852767944
iteration 197, loss = 0.32233932614326477
iteration 198, loss = 0.14555232226848602
iteration 199, loss = 0.27956703305244446
iteration 200, loss = 0.3675944209098816
iteration 201, loss = 0.31205689907073975
iteration 202, loss = 0.28216344118118286
iteration 203, loss = 0.17658816277980804
iteration 204, loss = 0.1971636712551117
iteration 205, loss = 0.2514779567718506
iteration 206, loss = 0.22804027795791626
iteration 207, loss = 0.2343074381351471
iteration 208, loss = 0.2512575387954712
iteration 209, loss = 0.2689329981803894
iteration 210, loss = 0.25934794545173645
iteration 211, loss = 0.25391942262649536
iteration 212, loss = 0.17474105954170227
iteration 213, loss = 0.19292452931404114
iteration 214, loss = 0.3025272488594055
iteration 215, loss = 0.37380585074424744
iteration 216, loss = 0.28688812255859375
iteration 217, loss = 0.31105977296829224
iteration 218, loss = 0.26972901821136475
iteration 219, loss = 0.2836761474609375
iteration 220, loss = 0.2731200158596039
iteration 221, loss = 0.4102100133895874
iteration 222, loss = 0.1896979659795761
iteration 223, loss = 0.28308242559432983
iteration 224, loss = 0.1800970435142517
iteration 225, loss = 0.09757642447948456
iteration 226, loss = 0.2591593861579895
iteration 227, loss = 0.26306888461112976
iteration 228, loss = 0.1514567732810974
iteration 229, loss = 0.26420947909355164
iteration 230, loss = 0.39548787474632263
iteration 231, loss = 0.2897067964076996
iteration 232, loss = 0.13144290447235107
iteration 233, loss = 0.2571529746055603
iteration 234, loss = 0.21574968099594116
iteration 235, loss = 0.10549422353506088
iteration 236, loss = 0.3749137818813324
iteration 237, loss = 0.27781757712364197
iteration 238, loss = 0.23859158158302307
iteration 239, loss = 0.28076136112213135
iteration 240, loss = 0.19369904696941376
iteration 241, loss = 0.257376104593277
iteration 242, loss = 0.3654406666755676
iteration 243, loss = 0.18137238919734955
iteration 244, loss = 0.33732378482818604
iteration 245, loss = 0.10996760427951813
iteration 246, loss = 0.42418545484542847
iteration 247, loss = 0.1937658041715622
iteration 248, loss = 0.2562673091888428
iteration 249, loss = 0.31329482793807983
iteration 250, loss = 0.44510403275489807
iteration 251, loss = 0.2800392508506775
iteration 252, loss = 0.18640774488449097
iteration 253, loss = 0.42884528636932373
iteration 254, loss = 0.2805439829826355
iteration 255, loss = 0.4071638584136963
iteration 256, loss = 0.14652174711227417
iteration 257, loss = 0.2701069712638855
iteration 258, loss = 0.20591798424720764
iteration 259, loss = 0.20433564484119415
iteration 260, loss = 0.20058314502239227
iteration 261, loss = 0.21174803376197815
iteration 262, loss = 0.2225472629070282
iteration 263, loss = 0.2620866298675537
iteration 264, loss = 0.24359922111034393
iteration 265, loss = 0.13980451226234436
iteration 266, loss = 0.3040575385093689
iteration 267, loss = 0.26070061326026917
iteration 268, loss = 0.2524002194404602
iteration 269, loss = 0.2539013922214508
iteration 270, loss = 0.11254029721021652
iteration 271, loss = 0.21943731606006622
iteration 272, loss = 0.27762067317962646
iteration 273, loss = 0.187623992562294
iteration 274, loss = 0.25167059898376465
iteration 275, loss = 0.23555946350097656
iteration 276, loss = 0.16985562443733215
iteration 277, loss = 0.22086390852928162
iteration 278, loss = 0.3547416925430298
iteration 279, loss = 0.3556492328643799
iteration 280, loss = 0.20699989795684814
iteration 281, loss = 0.3761690855026245
iteration 282, loss = 0.29533183574676514
iteration 283, loss = 0.22073227167129517
iteration 284, loss = 0.16317394375801086
iteration 285, loss = 0.3425062596797943
iteration 286, loss = 0.2512609362602234
iteration 287, loss = 0.2214438021183014
iteration 288, loss = 0.22523453831672668
iteration 289, loss = 0.2232312560081482
iteration 290, loss = 0.3999119699001312
iteration 291, loss = 0.13999813795089722
iteration 292, loss = 0.4101971387863159
iteration 293, loss = 0.2343313992023468
iteration 294, loss = 0.2591756582260132
iteration 295, loss = 0.19235098361968994
iteration 296, loss = 0.20644909143447876
iteration 297, loss = 0.25989919900894165
iteration 298, loss = 0.1942761242389679
iteration 299, loss = 0.23893101513385773
iteration 300, loss = 0.3220025599002838
iteration 1, loss = 0.22758862376213074
iteration 2, loss = 0.35514265298843384
iteration 3, loss = 0.29254770278930664
iteration 4, loss = 0.22923195362091064
iteration 5, loss = 0.2550309896469116
iteration 6, loss = 0.1705361306667328
iteration 7, loss = 0.2964533567428589
iteration 8, loss = 0.25366538763046265
iteration 9, loss = 0.21639767289161682
iteration 10, loss = 0.3542042374610901
iteration 11, loss = 0.13606378436088562
iteration 12, loss = 0.24409617483615875
iteration 13, loss = 0.2427317202091217
iteration 14, loss = 0.32210347056388855
iteration 15, loss = 0.15583504736423492
iteration 16, loss = 0.210202157497406
iteration 17, loss = 0.28713008761405945
iteration 18, loss = 0.37576824426651
iteration 19, loss = 0.15731167793273926
iteration 20, loss = 0.35094118118286133
iteration 21, loss = 0.20795103907585144
iteration 22, loss = 0.3511902093887329
iteration 23, loss = 0.1968076080083847
iteration 24, loss = 0.3077540397644043
iteration 25, loss = 0.236006960272789
iteration 26, loss = 0.30346444249153137
iteration 27, loss = 0.28915780782699585
iteration 28, loss = 0.41547590494155884
iteration 29, loss = 0.23757708072662354
iteration 30, loss = 0.2855793237686157
iteration 31, loss = 0.3038230538368225
iteration 32, loss = 0.14129383862018585
iteration 33, loss = 0.13232694566249847
iteration 34, loss = 0.25955915451049805
iteration 35, loss = 0.36325955390930176
iteration 36, loss = 0.27099016308784485
iteration 37, loss = 0.11338526755571365
iteration 38, loss = 0.287211537361145
iteration 39, loss = 0.13992144167423248
iteration 40, loss = 0.24283406138420105
iteration 41, loss = 0.3654489517211914
iteration 42, loss = 0.23579518496990204
iteration 43, loss = 0.26413771510124207
iteration 44, loss = 0.10565391182899475
iteration 45, loss = 0.2547127306461334
iteration 46, loss = 0.34836018085479736
iteration 47, loss = 0.5167174935340881
iteration 48, loss = 0.12105417251586914
iteration 49, loss = 0.2713357210159302
iteration 50, loss = 0.19600152969360352
iteration 51, loss = 0.2630069851875305
iteration 52, loss = 0.24202224612236023
iteration 53, loss = 0.2180967777967453
iteration 54, loss = 0.2678532600402832
iteration 55, loss = 0.20336662232875824
iteration 56, loss = 0.23626182973384857
iteration 57, loss = 0.15565752983093262
iteration 58, loss = 0.321442186832428
iteration 59, loss = 0.23721109330654144
iteration 60, loss = 0.5153613090515137
iteration 61, loss = 0.23683342337608337
iteration 62, loss = 0.32860228419303894
iteration 63, loss = 0.15202441811561584
iteration 64, loss = 0.2879209518432617
iteration 65, loss = 0.45655113458633423
iteration 66, loss = 0.1401781588792801
iteration 67, loss = 0.33294567465782166
iteration 68, loss = 0.12450379133224487
iteration 69, loss = 0.2918623983860016
iteration 70, loss = 0.13046176731586456
iteration 71, loss = 0.24716199934482574
iteration 72, loss = 0.23201759159564972
iteration 73, loss = 0.36736947298049927
iteration 74, loss = 0.37231603264808655
iteration 75, loss = 0.1455002725124359
iteration 76, loss = 0.23636460304260254
iteration 77, loss = 0.10728432238101959
iteration 78, loss = 0.2929076552391052
iteration 79, loss = 0.23923392593860626
iteration 80, loss = 0.20157526433467865
iteration 81, loss = 0.2574402391910553
iteration 82, loss = 0.3134262263774872
iteration 83, loss = 0.28623318672180176
iteration 84, loss = 0.1665138602256775
iteration 85, loss = 0.28391194343566895
iteration 86, loss = 0.1958647072315216
iteration 87, loss = 0.2134718894958496
iteration 88, loss = 0.19978894293308258
iteration 89, loss = 0.359264075756073
iteration 90, loss = 0.1802772581577301
iteration 91, loss = 0.20272210240364075
iteration 92, loss = 0.30263134837150574
iteration 93, loss = 0.26763835549354553
iteration 94, loss = 0.29398784041404724
iteration 95, loss = 0.30287736654281616
iteration 96, loss = 0.19067084789276123
iteration 97, loss = 0.2866577208042145
iteration 98, loss = 0.22628754377365112
iteration 99, loss = 0.2627906799316406
iteration 100, loss = 0.25252246856689453
iteration 101, loss = 0.22785459458827972
iteration 102, loss = 0.2404252141714096
iteration 103, loss = 0.3017633557319641
iteration 104, loss = 0.34347161650657654
iteration 105, loss = 0.24255993962287903
iteration 106, loss = 0.1714428961277008
iteration 107, loss = 0.23636168241500854
iteration 108, loss = 0.20324213802814484
iteration 109, loss = 0.1881421059370041
iteration 110, loss = 0.2918057441711426
iteration 111, loss = 0.26899850368499756
iteration 112, loss = 0.17978443205356598
iteration 113, loss = 0.21916541457176208
iteration 114, loss = 0.28055012226104736
iteration 115, loss = 0.31803515553474426
iteration 116, loss = 0.24476686120033264
iteration 117, loss = 0.21487870812416077
iteration 118, loss = 0.23123827576637268
iteration 119, loss = 0.287597119808197
iteration 120, loss = 0.17357955873012543
iteration 121, loss = 0.18077710270881653
iteration 122, loss = 0.4291306734085083
iteration 123, loss = 0.14543433487415314
iteration 124, loss = 0.4090535640716553
iteration 125, loss = 0.17744745314121246
iteration 126, loss = 0.39561742544174194
iteration 127, loss = 0.20447756350040436
iteration 128, loss = 0.20692561566829681
iteration 129, loss = 0.19234822690486908
iteration 130, loss = 0.30216068029403687
iteration 131, loss = 0.22025027871131897
iteration 132, loss = 0.343964546918869
iteration 133, loss = 0.17354360222816467
iteration 134, loss = 0.1724744737148285
iteration 135, loss = 0.3878205716609955
iteration 136, loss = 0.17644207179546356
iteration 137, loss = 0.1792730838060379
iteration 138, loss = 0.25726139545440674
iteration 139, loss = 0.20270372927188873
iteration 140, loss = 0.28383147716522217
iteration 141, loss = 0.18332898616790771
iteration 142, loss = 0.12319082021713257
iteration 143, loss = 0.21448542177677155
iteration 144, loss = 0.12377707660198212
iteration 145, loss = 0.23720476031303406
iteration 146, loss = 0.1514878273010254
iteration 147, loss = 0.17086318135261536
iteration 148, loss = 0.1536339521408081
iteration 149, loss = 0.2093924582004547
iteration 150, loss = 0.3039667010307312
iteration 151, loss = 0.17684252560138702
iteration 152, loss = 0.29090216755867004
iteration 153, loss = 0.4032133221626282
iteration 154, loss = 0.2760787904262543
iteration 155, loss = 0.13175085186958313
iteration 156, loss = 0.17893917858600616
iteration 157, loss = 0.3257269263267517
iteration 158, loss = 0.14424923062324524
iteration 159, loss = 0.33508118987083435
iteration 160, loss = 0.2727150321006775
iteration 161, loss = 0.17579740285873413
iteration 162, loss = 0.3406732678413391
iteration 163, loss = 0.34197187423706055
iteration 164, loss = 0.24064458906650543
iteration 165, loss = 0.181923508644104
iteration 166, loss = 0.27927660942077637
iteration 167, loss = 0.2566014528274536
iteration 168, loss = 0.2539321780204773
iteration 169, loss = 0.30949667096138
iteration 170, loss = 0.1264921873807907
iteration 171, loss = 0.2855045199394226
iteration 172, loss = 0.15348930656909943
iteration 173, loss = 0.48508793115615845
iteration 174, loss = 0.2746584713459015
iteration 175, loss = 0.19986608624458313
iteration 176, loss = 0.312684029340744
iteration 177, loss = 0.2546582520008087
iteration 178, loss = 0.19673508405685425
iteration 179, loss = 0.21751224994659424
iteration 180, loss = 0.19711878895759583
iteration 181, loss = 0.19514206051826477
iteration 182, loss = 0.17061084508895874
iteration 183, loss = 0.37426722049713135
iteration 184, loss = 0.14839160442352295
iteration 185, loss = 0.22359704971313477
iteration 186, loss = 0.20300820469856262
iteration 187, loss = 0.4315836429595947
iteration 188, loss = 0.20624089241027832
iteration 189, loss = 0.41241034865379333
iteration 190, loss = 0.22484683990478516
iteration 191, loss = 0.3101544976234436
iteration 192, loss = 0.14252185821533203
iteration 193, loss = 0.20833274722099304
iteration 194, loss = 0.1904037445783615
iteration 195, loss = 0.31744611263275146
iteration 196, loss = 0.16209541261196136
iteration 197, loss = 0.19972650706768036
iteration 198, loss = 0.2316303849220276
iteration 199, loss = 0.19531720876693726
iteration 200, loss = 0.2829320430755615
iteration 201, loss = 0.226671040058136
iteration 202, loss = 0.2917554974555969
iteration 203, loss = 0.31347668170928955
iteration 204, loss = 0.2879052460193634
iteration 205, loss = 0.16122359037399292
iteration 206, loss = 0.1640421450138092
iteration 207, loss = 0.14681997895240784
iteration 208, loss = 0.20916134119033813
iteration 209, loss = 0.2803804874420166
iteration 210, loss = 0.3250167965888977
iteration 211, loss = 0.30369454622268677
iteration 212, loss = 0.21398773789405823
iteration 213, loss = 0.23182153701782227
iteration 214, loss = 0.20993845164775848
iteration 215, loss = 0.2233516424894333
iteration 216, loss = 0.27150166034698486
iteration 217, loss = 0.3282475471496582
iteration 218, loss = 0.4025856554508209
iteration 219, loss = 0.24596789479255676
iteration 220, loss = 0.27535638213157654
iteration 221, loss = 0.23966535925865173
iteration 222, loss = 0.20413422584533691
iteration 223, loss = 0.21786656975746155
iteration 224, loss = 0.33082932233810425
iteration 225, loss = 0.22355999052524567
iteration 226, loss = 0.13309772312641144
iteration 227, loss = 0.1569695770740509
iteration 228, loss = 0.20478123426437378
iteration 229, loss = 0.15890748798847198
iteration 230, loss = 0.2143372893333435
iteration 231, loss = 0.19463977217674255
iteration 232, loss = 0.33674368262290955
iteration 233, loss = 0.19409291446208954
iteration 234, loss = 0.24959136545658112
iteration 235, loss = 0.20445029437541962
iteration 236, loss = 0.27197885513305664
iteration 237, loss = 0.23666246235370636
iteration 238, loss = 0.3754655420780182
iteration 239, loss = 0.29939180612564087
iteration 240, loss = 0.36197951436042786
iteration 241, loss = 0.48150962591171265
iteration 242, loss = 0.2765864133834839
iteration 243, loss = 0.1730736941099167
iteration 244, loss = 0.2831939458847046
iteration 245, loss = 0.18203066289424896
iteration 246, loss = 0.20452111959457397
iteration 247, loss = 0.16814197599887848
iteration 248, loss = 0.2993879020214081
iteration 249, loss = 0.36201465129852295
iteration 250, loss = 0.1852656751871109
iteration 251, loss = 0.16459284722805023
iteration 252, loss = 0.23938792943954468
iteration 253, loss = 0.1779634952545166
iteration 254, loss = 0.1558111160993576
iteration 255, loss = 0.15538601577281952
iteration 256, loss = 0.19121012091636658
iteration 257, loss = 0.16052047908306122
iteration 258, loss = 0.30022698640823364
iteration 259, loss = 0.20370525121688843
iteration 260, loss = 0.4140856862068176
iteration 261, loss = 0.3086785078048706
iteration 262, loss = 0.20927512645721436
iteration 263, loss = 0.24117469787597656
iteration 264, loss = 0.3846128582954407
iteration 265, loss = 0.3490746319293976
iteration 266, loss = 0.3366706371307373
iteration 267, loss = 0.3732033669948578
iteration 268, loss = 0.29105719923973083
iteration 269, loss = 0.3728063106536865
iteration 270, loss = 0.268993616104126
iteration 271, loss = 0.20893323421478271
iteration 272, loss = 0.13639049232006073
iteration 273, loss = 0.17393387854099274
iteration 274, loss = 0.19513235986232758
iteration 275, loss = 0.25006070733070374
iteration 276, loss = 0.18381257355213165
iteration 277, loss = 0.17864683270454407
iteration 278, loss = 0.2076842039823532
iteration 279, loss = 0.28387385606765747
iteration 280, loss = 0.23853200674057007
iteration 281, loss = 0.2618170976638794
iteration 282, loss = 0.17657920718193054
iteration 283, loss = 0.21824997663497925
iteration 284, loss = 0.2777942419052124
iteration 285, loss = 0.49917668104171753
iteration 286, loss = 0.3224184215068817
iteration 287, loss = 0.21145489811897278
iteration 288, loss = 0.2531740665435791
iteration 289, loss = 0.1776355355978012
iteration 290, loss = 0.2265348732471466
iteration 291, loss = 0.15378296375274658
iteration 292, loss = 0.1581791788339615
iteration 293, loss = 0.31577590107917786
iteration 294, loss = 0.3702651858329773
iteration 295, loss = 0.13615943491458893
iteration 296, loss = 0.18476052582263947
iteration 297, loss = 0.17806638777256012
iteration 298, loss = 0.19861161708831787
iteration 299, loss = 0.2531511187553406
iteration 300, loss = 0.15664264559745789
iteration 1, loss = 0.1752217710018158
iteration 2, loss = 0.1589515209197998
iteration 3, loss = 0.24566704034805298
iteration 4, loss = 0.18950006365776062
iteration 5, loss = 0.1591881811618805
iteration 6, loss = 0.2415754795074463
iteration 7, loss = 0.2709639072418213
iteration 8, loss = 0.11913754045963287
iteration 9, loss = 0.4033089876174927
iteration 10, loss = 0.28776174783706665
iteration 11, loss = 0.23824414610862732
iteration 12, loss = 0.15626612305641174
iteration 13, loss = 0.2641306519508362
iteration 14, loss = 0.285102903842926
iteration 15, loss = 0.11993427574634552
iteration 16, loss = 0.24293898046016693
iteration 17, loss = 0.19775377213954926
iteration 18, loss = 0.2089775651693344
iteration 19, loss = 0.18988777697086334
iteration 20, loss = 0.12302186340093613
iteration 21, loss = 0.20776483416557312
iteration 22, loss = 0.298866331577301
iteration 23, loss = 0.37257909774780273
iteration 24, loss = 0.25423943996429443
iteration 25, loss = 0.3588879704475403
iteration 26, loss = 0.2165042608976364
iteration 27, loss = 0.11075130105018616
iteration 28, loss = 0.14641498029232025
iteration 29, loss = 0.2460535317659378
iteration 30, loss = 0.18636223673820496
iteration 31, loss = 0.3102450966835022
iteration 32, loss = 0.24264678359031677
iteration 33, loss = 0.22380036115646362
iteration 34, loss = 0.17445886135101318
iteration 35, loss = 0.23660437762737274
iteration 36, loss = 0.2906056046485901
iteration 37, loss = 0.23706792294979095
iteration 38, loss = 0.15468072891235352
iteration 39, loss = 0.1852731704711914
iteration 40, loss = 0.294933557510376
iteration 41, loss = 0.3105967044830322
iteration 42, loss = 0.16022203862667084
iteration 43, loss = 0.26325783133506775
iteration 44, loss = 0.13494186103343964
iteration 45, loss = 0.12512394785881042
iteration 46, loss = 0.21825984120368958
iteration 47, loss = 0.21846425533294678
iteration 48, loss = 0.1882789433002472
iteration 49, loss = 0.1427917778491974
iteration 50, loss = 0.2867759168148041
iteration 51, loss = 0.14620330929756165
iteration 52, loss = 0.1870386004447937
iteration 53, loss = 0.33175233006477356
iteration 54, loss = 0.29552948474884033
iteration 55, loss = 0.1264382153749466
iteration 56, loss = 0.32170528173446655
iteration 57, loss = 0.36795637011528015
iteration 58, loss = 0.28852641582489014
iteration 59, loss = 0.17761406302452087
iteration 60, loss = 0.2762240767478943
iteration 61, loss = 0.10390932112932205
iteration 62, loss = 0.3151809573173523
iteration 63, loss = 0.34092074632644653
iteration 64, loss = 0.22538596391677856
iteration 65, loss = 0.46960118412971497
iteration 66, loss = 0.22411596775054932
iteration 67, loss = 0.48739510774612427
iteration 68, loss = 0.2872382402420044
iteration 69, loss = 0.2833693325519562
iteration 70, loss = 0.3333802819252014
iteration 71, loss = 0.2498904913663864
iteration 72, loss = 0.3096047341823578
iteration 73, loss = 0.2543809115886688
iteration 74, loss = 0.26293647289276123
iteration 75, loss = 0.22911159694194794
iteration 76, loss = 0.15760907530784607
iteration 77, loss = 0.2544064521789551
iteration 78, loss = 0.23202815651893616
iteration 79, loss = 0.14863130450248718
iteration 80, loss = 0.28730419278144836
iteration 81, loss = 0.2005978375673294
iteration 82, loss = 0.16887031495571136
iteration 83, loss = 0.16558746993541718
iteration 84, loss = 0.22873862087726593
iteration 85, loss = 0.3514493703842163
iteration 86, loss = 0.16442373394966125
iteration 87, loss = 0.24117545783519745
iteration 88, loss = 0.37246161699295044
iteration 89, loss = 0.16819024085998535
iteration 90, loss = 0.1362714320421219
iteration 91, loss = 0.3245137631893158
iteration 92, loss = 0.24517452716827393
iteration 93, loss = 0.16421712934970856
iteration 94, loss = 0.33175358176231384
iteration 95, loss = 0.19002275168895721
iteration 96, loss = 0.1237928718328476
iteration 97, loss = 0.2513978183269501
iteration 98, loss = 0.2689366340637207
iteration 99, loss = 0.2554951608181
iteration 100, loss = 0.3750370144844055
iteration 101, loss = 0.17644651234149933
iteration 102, loss = 0.18717695772647858
iteration 103, loss = 0.1321517676115036
iteration 104, loss = 0.1478963941335678
iteration 105, loss = 0.2806551456451416
iteration 106, loss = 0.29113656282424927
iteration 107, loss = 0.19481690227985382
iteration 108, loss = 0.3200056850910187
iteration 109, loss = 0.2621859312057495
iteration 110, loss = 0.18549193441867828
iteration 111, loss = 0.24569204449653625
iteration 112, loss = 0.3288910686969757
iteration 113, loss = 0.20255687832832336
iteration 114, loss = 0.19464726746082306
iteration 115, loss = 0.32257169485092163
iteration 116, loss = 0.4395749568939209
iteration 117, loss = 0.33979934453964233
iteration 118, loss = 0.2075004130601883
iteration 119, loss = 0.1510479748249054
iteration 120, loss = 0.22929173707962036
iteration 121, loss = 0.23761898279190063
iteration 122, loss = 0.1898295283317566
iteration 123, loss = 0.3057863116264343
iteration 124, loss = 0.1783745288848877
iteration 125, loss = 0.23832039535045624
iteration 126, loss = 0.2671789526939392
iteration 127, loss = 0.36729148030281067
iteration 128, loss = 0.26064667105674744
iteration 129, loss = 0.141322523355484
iteration 130, loss = 0.23841652274131775
iteration 131, loss = 0.15566128492355347
iteration 132, loss = 0.2221928834915161
iteration 133, loss = 0.1722286492586136
iteration 134, loss = 0.15355557203292847
iteration 135, loss = 0.28767961263656616
iteration 136, loss = 0.3034595847129822
iteration 137, loss = 0.3348734378814697
iteration 138, loss = 0.33200570940971375
iteration 139, loss = 0.237374410033226
iteration 140, loss = 0.15537017583847046
iteration 141, loss = 0.09953215718269348
iteration 142, loss = 0.21464546024799347
iteration 143, loss = 0.20179101824760437
iteration 144, loss = 0.2544769048690796
iteration 145, loss = 0.273325651884079
iteration 146, loss = 0.32556456327438354
iteration 147, loss = 0.19925373792648315
iteration 148, loss = 0.2277614027261734
iteration 149, loss = 0.2942732572555542
iteration 150, loss = 0.43663156032562256
iteration 151, loss = 0.2378242015838623
iteration 152, loss = 0.2554296851158142
iteration 153, loss = 0.18025754392147064
iteration 154, loss = 0.21383120119571686
iteration 155, loss = 0.15594954788684845
iteration 156, loss = 0.1389436423778534
iteration 157, loss = 0.16488231718540192
iteration 158, loss = 0.32839322090148926
iteration 159, loss = 0.2058412730693817
iteration 160, loss = 0.3325096368789673
iteration 161, loss = 0.14914986491203308
iteration 162, loss = 0.2046206295490265
iteration 163, loss = 0.3257780075073242
iteration 164, loss = 0.22754214704036713
iteration 165, loss = 0.17774784564971924
iteration 166, loss = 0.2166990041732788
iteration 167, loss = 0.2726418077945709
iteration 168, loss = 0.14136752486228943
iteration 169, loss = 0.31645363569259644
iteration 170, loss = 0.24067623913288116
iteration 171, loss = 0.20814698934555054
iteration 172, loss = 0.2295566201210022
iteration 173, loss = 0.3261551856994629
iteration 174, loss = 0.552773654460907
iteration 175, loss = 0.2312656044960022
iteration 176, loss = 0.19615674018859863
iteration 177, loss = 0.15932756662368774
iteration 178, loss = 0.174933522939682
iteration 179, loss = 0.2027561068534851
iteration 180, loss = 0.33528921008110046
iteration 181, loss = 0.28598812222480774
iteration 182, loss = 0.21982088685035706
iteration 183, loss = 0.1293187141418457
iteration 184, loss = 0.3926655054092407
iteration 185, loss = 0.25113239884376526
iteration 186, loss = 0.45753392577171326
iteration 187, loss = 0.21005794405937195
iteration 188, loss = 0.208297461271286
iteration 189, loss = 0.20780612528324127
iteration 190, loss = 0.2876990735530853
iteration 191, loss = 0.41842836141586304
iteration 192, loss = 0.2045259028673172
iteration 193, loss = 0.23975394666194916
iteration 194, loss = 0.14652813971042633
iteration 195, loss = 0.27526333928108215
iteration 196, loss = 0.24915799498558044
iteration 197, loss = 0.21635200083255768
iteration 198, loss = 0.15569140017032623
iteration 199, loss = 0.29122474789619446
iteration 200, loss = 0.26194852590560913
iteration 201, loss = 0.3062922954559326
iteration 202, loss = 0.19999393820762634
iteration 203, loss = 0.251430481672287
iteration 204, loss = 0.27068281173706055
iteration 205, loss = 0.29645344614982605
iteration 206, loss = 0.2289659082889557
iteration 207, loss = 0.11316268891096115
iteration 208, loss = 0.1258271187543869
iteration 209, loss = 0.30125707387924194
iteration 210, loss = 0.19584080576896667
iteration 211, loss = 0.10011404007673264
iteration 212, loss = 0.28957921266555786
iteration 213, loss = 0.414211243391037
iteration 214, loss = 0.25189775228500366
iteration 215, loss = 0.20117983222007751
iteration 216, loss = 0.22690823674201965
iteration 217, loss = 0.12397433817386627
iteration 218, loss = 0.17355862259864807
iteration 219, loss = 0.1676543653011322
iteration 220, loss = 0.11847623437643051
iteration 221, loss = 0.30688709020614624
iteration 222, loss = 0.14417755603790283
iteration 223, loss = 0.2733044922351837
iteration 224, loss = 0.21813130378723145
iteration 225, loss = 0.16697172820568085
iteration 226, loss = 0.2592982351779938
iteration 227, loss = 0.31788522005081177
iteration 228, loss = 0.1459125131368637
iteration 229, loss = 0.43884795904159546
iteration 230, loss = 0.1687171459197998
iteration 231, loss = 0.17909181118011475
iteration 232, loss = 0.1992005854845047
iteration 233, loss = 0.1630740463733673
iteration 234, loss = 0.3062935173511505
iteration 235, loss = 0.47302573919296265
iteration 236, loss = 0.1689407080411911
iteration 237, loss = 0.4757661521434784
iteration 238, loss = 0.5019314885139465
iteration 239, loss = 0.37530383467674255
iteration 240, loss = 0.28410086035728455
iteration 241, loss = 0.18820440769195557
iteration 242, loss = 0.2164505273103714
iteration 243, loss = 0.2579536437988281
iteration 244, loss = 0.19403788447380066
iteration 245, loss = 0.20997582376003265
iteration 246, loss = 0.16583295166492462
iteration 247, loss = 0.2582564353942871
iteration 248, loss = 0.2711847424507141
iteration 249, loss = 0.3413257896900177
iteration 250, loss = 0.36831143498420715
iteration 251, loss = 0.13364216685295105
iteration 252, loss = 0.21424247324466705
iteration 253, loss = 0.14482249319553375
iteration 254, loss = 0.2166907787322998
iteration 255, loss = 0.17460113763809204
iteration 256, loss = 0.14983171224594116
iteration 257, loss = 0.12420600652694702
iteration 258, loss = 0.3899554908275604
iteration 259, loss = 0.27163171768188477
iteration 260, loss = 0.12410314381122589
iteration 261, loss = 0.22804464399814606
iteration 262, loss = 0.259689599275589
iteration 263, loss = 0.24085573852062225
iteration 264, loss = 0.3166019916534424
iteration 265, loss = 0.2685452103614807
iteration 266, loss = 0.09576926380395889
iteration 267, loss = 0.16808339953422546
iteration 268, loss = 0.23886984586715698
iteration 269, loss = 0.27248653769493103
iteration 270, loss = 0.16502326726913452
iteration 271, loss = 0.15980088710784912
iteration 272, loss = 0.24518698453903198
iteration 273, loss = 0.16683796048164368
iteration 274, loss = 0.2320367991924286
iteration 275, loss = 0.11620433628559113
iteration 276, loss = 0.19784501194953918
iteration 277, loss = 0.2836483120918274
iteration 278, loss = 0.24317872524261475
iteration 279, loss = 0.3083588480949402
iteration 280, loss = 0.21536479890346527
iteration 281, loss = 0.1716950386762619
iteration 282, loss = 0.20367613434791565
iteration 283, loss = 0.2572917640209198
iteration 284, loss = 0.456144243478775
iteration 285, loss = 0.2619415819644928
iteration 286, loss = 0.3580513000488281
iteration 287, loss = 0.2856615483760834
iteration 288, loss = 0.22090090811252594
iteration 289, loss = 0.3708023726940155
iteration 290, loss = 0.18447335064411163
iteration 291, loss = 0.14823885262012482
iteration 292, loss = 0.17627984285354614
iteration 293, loss = 0.2016524225473404
iteration 294, loss = 0.23886005580425262
iteration 295, loss = 0.22720442712306976
iteration 296, loss = 0.3210669755935669
iteration 297, loss = 0.2815393805503845
iteration 298, loss = 0.28617626428604126
iteration 299, loss = 0.18443524837493896
iteration 300, loss = 0.20285868644714355
iteration 1, loss = 0.37530747056007385
iteration 2, loss = 0.23836980760097504
iteration 3, loss = 0.3903453052043915
iteration 4, loss = 0.1462099850177765
iteration 5, loss = 0.24996644258499146
iteration 6, loss = 0.3711399435997009
iteration 7, loss = 0.19770687818527222
iteration 8, loss = 0.1689969301223755
iteration 9, loss = 0.20490163564682007
iteration 10, loss = 0.2034876048564911
iteration 11, loss = 0.3125690221786499
iteration 12, loss = 0.1827867031097412
iteration 13, loss = 0.11689396947622299
iteration 14, loss = 0.1926083266735077
iteration 15, loss = 0.2352449893951416
iteration 16, loss = 0.24504037201404572
iteration 17, loss = 0.1646190732717514
iteration 18, loss = 0.21901534497737885
iteration 19, loss = 0.20921435952186584
iteration 20, loss = 0.21413251757621765
iteration 21, loss = 0.19345702230930328
iteration 22, loss = 0.09457569569349289
iteration 23, loss = 0.23107561469078064
iteration 24, loss = 0.10761357098817825
iteration 25, loss = 0.27721354365348816
iteration 26, loss = 0.11317609250545502
iteration 27, loss = 0.31072455644607544
iteration 28, loss = 0.1437368243932724
iteration 29, loss = 0.09477449208498001
iteration 30, loss = 0.2048608958721161
iteration 31, loss = 0.29760801792144775
iteration 32, loss = 0.29202741384506226
iteration 33, loss = 0.37851351499557495
iteration 34, loss = 0.15597113966941833
iteration 35, loss = 0.350821316242218
iteration 36, loss = 0.16253061592578888
iteration 37, loss = 0.2375677078962326
iteration 38, loss = 0.2896699905395508
iteration 39, loss = 0.31787967681884766
iteration 40, loss = 0.2302185595035553
iteration 41, loss = 0.14379902184009552
iteration 42, loss = 0.17377714812755585
iteration 43, loss = 0.2158917784690857
iteration 44, loss = 0.26724085211753845
iteration 45, loss = 0.22801823914051056
iteration 46, loss = 0.24274525046348572
iteration 47, loss = 0.15252509713172913
iteration 48, loss = 0.18304017186164856
iteration 49, loss = 0.1492902636528015
iteration 50, loss = 0.22100605070590973
iteration 51, loss = 0.2377784550189972
iteration 52, loss = 0.27633410692214966
iteration 53, loss = 0.2841407358646393
iteration 54, loss = 0.11838699877262115
iteration 55, loss = 0.2789395749568939
iteration 56, loss = 0.24359409511089325
iteration 57, loss = 0.2683727741241455
iteration 58, loss = 0.2836497724056244
iteration 59, loss = 0.2712964117527008
iteration 60, loss = 0.2731083929538727
iteration 61, loss = 0.2778438329696655
iteration 62, loss = 0.3648100793361664
iteration 63, loss = 0.2275218665599823
iteration 64, loss = 0.12546323239803314
iteration 65, loss = 0.15973688662052155
iteration 66, loss = 0.2568672299385071
iteration 67, loss = 0.18490208685398102
iteration 68, loss = 0.2077522873878479
iteration 69, loss = 0.22103354334831238
iteration 70, loss = 0.24946211278438568
iteration 71, loss = 0.29381030797958374
iteration 72, loss = 0.3859926462173462
iteration 73, loss = 0.10703380405902863
iteration 74, loss = 0.1560695469379425
iteration 75, loss = 0.12235300987958908
iteration 76, loss = 0.2917396128177643
iteration 77, loss = 0.30933624505996704
iteration 78, loss = 0.1970755159854889
iteration 79, loss = 0.2812579274177551
iteration 80, loss = 0.22796793282032013
iteration 81, loss = 0.22013947367668152
iteration 82, loss = 0.2273082733154297
iteration 83, loss = 0.2198646366596222
iteration 84, loss = 0.19115084409713745
iteration 85, loss = 0.18374484777450562
iteration 86, loss = 0.17406629025936127
iteration 87, loss = 0.2242603302001953
iteration 88, loss = 0.2719995081424713
iteration 89, loss = 0.20070244371891022
iteration 90, loss = 0.2697763741016388
iteration 91, loss = 0.3454544246196747
iteration 92, loss = 0.1464408040046692
iteration 93, loss = 0.39381861686706543
iteration 94, loss = 0.12215124815702438
iteration 95, loss = 0.2665489614009857
iteration 96, loss = 0.3473486304283142
iteration 97, loss = 0.3033454120159149
iteration 98, loss = 0.15150396525859833
iteration 99, loss = 0.13961240649223328
iteration 100, loss = 0.25729209184646606
iteration 101, loss = 0.18474072217941284
iteration 102, loss = 0.27086973190307617
iteration 103, loss = 0.22261744737625122
iteration 104, loss = 0.13996976613998413
iteration 105, loss = 0.25109225511550903
iteration 106, loss = 0.19122040271759033
iteration 107, loss = 0.22107082605361938
iteration 108, loss = 0.19307316839694977
iteration 109, loss = 0.1475251168012619
iteration 110, loss = 0.2802106738090515
iteration 111, loss = 0.2814176380634308
iteration 112, loss = 0.23202446103096008
iteration 113, loss = 0.1827482283115387
iteration 114, loss = 0.20759929716587067
iteration 115, loss = 0.329402357339859
iteration 116, loss = 0.14491315186023712
iteration 117, loss = 0.2217138111591339
iteration 118, loss = 0.16409052908420563
iteration 119, loss = 0.2877195179462433
iteration 120, loss = 0.2436227649450302
iteration 121, loss = 0.21948324143886566
iteration 122, loss = 0.11768998950719833
iteration 123, loss = 0.21530161798000336
iteration 124, loss = 0.24829067289829254
iteration 125, loss = 0.1494230031967163
iteration 126, loss = 0.25207918882369995
iteration 127, loss = 0.10033512115478516
iteration 128, loss = 0.20475909113883972
iteration 129, loss = 0.3440161943435669
iteration 130, loss = 0.28695109486579895
iteration 131, loss = 0.26917344331741333
iteration 132, loss = 0.20964232087135315
iteration 133, loss = 0.2587225139141083
iteration 134, loss = 0.16385036706924438
iteration 135, loss = 0.16515477001667023
iteration 136, loss = 0.120082326233387
iteration 137, loss = 0.31567299365997314
iteration 138, loss = 0.19238334894180298
iteration 139, loss = 0.2715156078338623
iteration 140, loss = 0.25343990325927734
iteration 141, loss = 0.177041158080101
iteration 142, loss = 0.2143496721982956
iteration 143, loss = 0.3259897530078888
iteration 144, loss = 0.20788362622261047
iteration 145, loss = 0.18606027960777283
iteration 146, loss = 0.21256594359874725
iteration 147, loss = 0.236089289188385
iteration 148, loss = 0.2776750326156616
iteration 149, loss = 0.24516832828521729
iteration 150, loss = 0.19736012816429138
iteration 151, loss = 0.11870954930782318
iteration 152, loss = 0.38170772790908813
iteration 153, loss = 0.18719296157360077
iteration 154, loss = 0.11517422646284103
iteration 155, loss = 0.35675036907196045
iteration 156, loss = 0.33898723125457764
iteration 157, loss = 0.18338802456855774
iteration 158, loss = 0.14372004568576813
iteration 159, loss = 0.20172972977161407
iteration 160, loss = 0.2771102488040924
iteration 161, loss = 0.25086432695388794
iteration 162, loss = 0.24133974313735962
iteration 163, loss = 0.2827642858028412
iteration 164, loss = 0.26227280497550964
iteration 165, loss = 0.15902318060398102
iteration 166, loss = 0.19081294536590576
iteration 167, loss = 0.2711403965950012
iteration 168, loss = 0.3507857024669647
iteration 169, loss = 0.30096206068992615
iteration 170, loss = 0.34362953901290894
iteration 171, loss = 0.2776293158531189
iteration 172, loss = 0.23562854528427124
iteration 173, loss = 0.20444314181804657
iteration 174, loss = 0.20512717962265015
iteration 175, loss = 0.24144716560840607
iteration 176, loss = 0.09498677402734756
iteration 177, loss = 0.22866065800189972
iteration 178, loss = 0.27433353662490845
iteration 179, loss = 0.16158613562583923
iteration 180, loss = 0.25717833638191223
iteration 181, loss = 0.1437818557024002
iteration 182, loss = 0.22924453020095825
iteration 183, loss = 0.14662034809589386
iteration 184, loss = 0.32940223813056946
iteration 185, loss = 0.20987318456172943
iteration 186, loss = 0.14134138822555542
iteration 187, loss = 0.35199615359306335
iteration 188, loss = 0.17979007959365845
iteration 189, loss = 0.3646329939365387
iteration 190, loss = 0.18379750847816467
iteration 191, loss = 0.12407233566045761
iteration 192, loss = 0.19553914666175842
iteration 193, loss = 0.2659868896007538
iteration 194, loss = 0.12381039559841156
iteration 195, loss = 0.31719669699668884
iteration 196, loss = 0.24044780433177948
iteration 197, loss = 0.19333750009536743
iteration 198, loss = 0.11966490000486374
iteration 199, loss = 0.08566690981388092
iteration 200, loss = 0.30096060037612915
iteration 201, loss = 0.22468696534633636
iteration 202, loss = 0.18801432847976685
iteration 203, loss = 0.197981595993042
iteration 204, loss = 0.16926604509353638
iteration 205, loss = 0.22776925563812256
iteration 206, loss = 0.3521883487701416
iteration 207, loss = 0.14398907124996185
iteration 208, loss = 0.2263784408569336
iteration 209, loss = 0.30260637402534485
iteration 210, loss = 0.11864146590232849
iteration 211, loss = 0.14425799250602722
iteration 212, loss = 0.14427705109119415
iteration 213, loss = 0.3041602373123169
iteration 214, loss = 0.24065947532653809
iteration 215, loss = 0.20999875664710999
iteration 216, loss = 0.22208258509635925
iteration 217, loss = 0.16721118986606598
iteration 218, loss = 0.14879651367664337
iteration 219, loss = 0.20610716938972473
iteration 220, loss = 0.4012036919593811
iteration 221, loss = 0.18735209107398987
iteration 222, loss = 0.18719011545181274
iteration 223, loss = 0.281131774187088
iteration 224, loss = 0.12096206098794937
iteration 225, loss = 0.26361697912216187
iteration 226, loss = 0.27121502161026
iteration 227, loss = 0.20143963396549225
iteration 228, loss = 0.2742375135421753
iteration 229, loss = 0.2119280844926834
iteration 230, loss = 0.3972054421901703
iteration 231, loss = 0.2600845396518707
iteration 232, loss = 0.2511613368988037
iteration 233, loss = 0.09532169252634048
iteration 234, loss = 0.4250677227973938
iteration 235, loss = 0.3819844126701355
iteration 236, loss = 0.1973305344581604
iteration 237, loss = 0.3061487078666687
iteration 238, loss = 0.22103182971477509
iteration 239, loss = 0.2619283199310303
iteration 240, loss = 0.37146735191345215
iteration 241, loss = 0.21842734515666962
iteration 242, loss = 0.17659932374954224
iteration 243, loss = 0.30471980571746826
iteration 244, loss = 0.2577437162399292
iteration 245, loss = 0.14138518273830414
iteration 246, loss = 0.1298474371433258
iteration 247, loss = 0.31838494539260864
iteration 248, loss = 0.2672659754753113
iteration 249, loss = 0.14504557847976685
iteration 250, loss = 0.2027810513973236
iteration 251, loss = 0.34530627727508545
iteration 252, loss = 0.19614779949188232
iteration 253, loss = 0.24567410349845886
iteration 254, loss = 0.26295027136802673
iteration 255, loss = 0.22220547497272491
iteration 256, loss = 0.3412112891674042
iteration 257, loss = 0.1992669701576233
iteration 258, loss = 0.1969265639781952
iteration 259, loss = 0.25295940041542053
iteration 260, loss = 0.21190999448299408
iteration 261, loss = 0.24246081709861755
iteration 262, loss = 0.27286893129348755
iteration 263, loss = 0.28899645805358887
iteration 264, loss = 0.2736218273639679
iteration 265, loss = 0.2057144045829773
iteration 266, loss = 0.24857719242572784
iteration 267, loss = 0.234480619430542
iteration 268, loss = 0.11258465051651001
iteration 269, loss = 0.38202616572380066
iteration 270, loss = 0.19718830287456512
iteration 271, loss = 0.3921198546886444
iteration 272, loss = 0.14675506949424744
iteration 273, loss = 0.18856416642665863
iteration 274, loss = 0.34203317761421204
iteration 275, loss = 0.19708463549613953
iteration 276, loss = 0.20330440998077393
iteration 277, loss = 0.21181409060955048
iteration 278, loss = 0.22759664058685303
iteration 279, loss = 0.19542238116264343
iteration 280, loss = 0.2315230518579483
iteration 281, loss = 0.2064582109451294
iteration 282, loss = 0.3360356092453003
iteration 283, loss = 0.2593631148338318
iteration 284, loss = 0.41629546880722046
iteration 285, loss = 0.3592720329761505
iteration 286, loss = 0.29412809014320374
iteration 287, loss = 0.24083855748176575
iteration 288, loss = 0.16214126348495483
iteration 289, loss = 0.3787495195865631
iteration 290, loss = 0.13334040343761444
iteration 291, loss = 0.10130356252193451
iteration 292, loss = 0.3411024212837219
iteration 293, loss = 0.32131123542785645
iteration 294, loss = 0.27971988916397095
iteration 295, loss = 0.3292008936405182
iteration 296, loss = 0.2491118162870407
iteration 297, loss = 0.19323641061782837
iteration 298, loss = 0.2464558333158493
iteration 299, loss = 0.251176655292511
iteration 300, loss = 0.18733569979667664
iteration 1, loss = 0.24547231197357178
iteration 2, loss = 0.16374118626117706
iteration 3, loss = 0.45644938945770264
iteration 4, loss = 0.1775846779346466
iteration 5, loss = 0.11594611406326294
iteration 6, loss = 0.17866575717926025
iteration 7, loss = 0.16115686297416687
iteration 8, loss = 0.10304154455661774
iteration 9, loss = 0.18940110504627228
iteration 10, loss = 0.2840287387371063
iteration 11, loss = 0.24669772386550903
iteration 12, loss = 0.13599729537963867
iteration 13, loss = 0.24808146059513092
iteration 14, loss = 0.08562047779560089
iteration 15, loss = 0.17331519722938538
iteration 16, loss = 0.19596484303474426
iteration 17, loss = 0.4073861539363861
iteration 18, loss = 0.45173925161361694
iteration 19, loss = 0.3240104615688324
iteration 20, loss = 0.14919331669807434
iteration 21, loss = 0.10178093612194061
iteration 22, loss = 0.2400961071252823
iteration 23, loss = 0.19237351417541504
iteration 24, loss = 0.21859139204025269
iteration 25, loss = 0.16578732430934906
iteration 26, loss = 0.32167527079582214
iteration 27, loss = 0.0923343300819397
iteration 28, loss = 0.18896475434303284
iteration 29, loss = 0.1302143931388855
iteration 30, loss = 0.23018676042556763
iteration 31, loss = 0.1916637420654297
iteration 32, loss = 0.3356930613517761
iteration 33, loss = 0.16220243275165558
iteration 34, loss = 0.23785923421382904
iteration 35, loss = 0.29606449604034424
iteration 36, loss = 0.34626877307891846
iteration 37, loss = 0.16588813066482544
iteration 38, loss = 0.30986547470092773
iteration 39, loss = 0.28369641304016113
iteration 40, loss = 0.1415066123008728
iteration 41, loss = 0.13655439019203186
iteration 42, loss = 0.29045021533966064
iteration 43, loss = 0.20100156962871552
iteration 44, loss = 0.33642616868019104
iteration 45, loss = 0.20390503108501434
iteration 46, loss = 0.1724393367767334
iteration 47, loss = 0.1833363175392151
iteration 48, loss = 0.12056305259466171
iteration 49, loss = 0.2987290918827057
iteration 50, loss = 0.20966482162475586
iteration 51, loss = 0.26231199502944946
iteration 52, loss = 0.155872642993927
iteration 53, loss = 0.20837315917015076
iteration 54, loss = 0.2427207976579666
iteration 55, loss = 0.08731505274772644
iteration 56, loss = 0.13723275065422058
iteration 57, loss = 0.22220684587955475
iteration 58, loss = 0.27114495635032654
iteration 59, loss = 0.27873021364212036
iteration 60, loss = 0.15724095702171326
iteration 61, loss = 0.301539808511734
iteration 62, loss = 0.15542888641357422
iteration 63, loss = 0.25341081619262695
iteration 64, loss = 0.13184458017349243
iteration 65, loss = 0.2947103977203369
iteration 66, loss = 0.12328187376260757
iteration 67, loss = 0.14084526896476746
iteration 68, loss = 0.28102222084999084
iteration 69, loss = 0.34631240367889404
iteration 70, loss = 0.27212008833885193
iteration 71, loss = 0.23438960313796997
iteration 72, loss = 0.18341459333896637
iteration 73, loss = 0.14852213859558105
iteration 74, loss = 0.3148083984851837
iteration 75, loss = 0.19996637105941772
iteration 76, loss = 0.16713115572929382
iteration 77, loss = 0.20395886898040771
iteration 78, loss = 0.28866612911224365
iteration 79, loss = 0.24510684609413147
iteration 80, loss = 0.18138393759727478
iteration 81, loss = 0.13138791918754578
iteration 82, loss = 0.25116729736328125
iteration 83, loss = 0.16641069948673248
iteration 84, loss = 0.26962271332740784
iteration 85, loss = 0.25125575065612793
iteration 86, loss = 0.283259779214859
iteration 87, loss = 0.17371250689029694
iteration 88, loss = 0.2669975459575653
iteration 89, loss = 0.21732382476329803
iteration 90, loss = 0.17333227396011353
iteration 91, loss = 0.22964660823345184
iteration 92, loss = 0.2682417035102844
iteration 93, loss = 0.23300588130950928
iteration 94, loss = 0.23497644066810608
iteration 95, loss = 0.2938838303089142
iteration 96, loss = 0.24319005012512207
iteration 97, loss = 0.2517257034778595
iteration 98, loss = 0.14168286323547363
iteration 99, loss = 0.18684813380241394
iteration 100, loss = 0.3826914429664612
iteration 101, loss = 0.19737949967384338
iteration 102, loss = 0.24172517657279968
iteration 103, loss = 0.10678044706583023
iteration 104, loss = 0.3013375401496887
iteration 105, loss = 0.33160385489463806
iteration 106, loss = 0.4162667393684387
iteration 107, loss = 0.23676574230194092
iteration 108, loss = 0.3312026262283325
iteration 109, loss = 0.25586432218551636
iteration 110, loss = 0.20069043338298798
iteration 111, loss = 0.24992451071739197
iteration 112, loss = 0.19534967839717865
iteration 113, loss = 0.19579671323299408
iteration 114, loss = 0.13306504487991333
iteration 115, loss = 0.2361135482788086
iteration 116, loss = 0.17122730612754822
iteration 117, loss = 0.2889465093612671
iteration 118, loss = 0.2280825525522232
iteration 119, loss = 0.2347308099269867
iteration 120, loss = 0.3721010982990265
iteration 121, loss = 0.13549621403217316
iteration 122, loss = 0.3081904947757721
iteration 123, loss = 0.185258686542511
iteration 124, loss = 0.19905999302864075
iteration 125, loss = 0.27439600229263306
iteration 126, loss = 0.16529466211795807
iteration 127, loss = 0.1352427899837494
iteration 128, loss = 0.16138805449008942
iteration 129, loss = 0.16964972019195557
iteration 130, loss = 0.23258420825004578
iteration 131, loss = 0.19344282150268555
iteration 132, loss = 0.19235548377037048
iteration 133, loss = 0.2540208697319031
iteration 134, loss = 0.2501221001148224
iteration 135, loss = 0.15862901508808136
iteration 136, loss = 0.30369412899017334
iteration 137, loss = 0.18338795006275177
iteration 138, loss = 0.18963417410850525
iteration 139, loss = 0.17204195261001587
iteration 140, loss = 0.22559377551078796
iteration 141, loss = 0.1597612351179123
iteration 142, loss = 0.18439418077468872
iteration 143, loss = 0.24578744173049927
iteration 144, loss = 0.10403241962194443
iteration 145, loss = 0.2449280321598053
iteration 146, loss = 0.25440698862075806
iteration 147, loss = 0.137338787317276
iteration 148, loss = 0.22840403020381927
iteration 149, loss = 0.11846040189266205
iteration 150, loss = 0.18522006273269653
iteration 151, loss = 0.2849605679512024
iteration 152, loss = 0.1573103815317154
iteration 153, loss = 0.38958436250686646
iteration 154, loss = 0.4674162268638611
iteration 155, loss = 0.23236608505249023
iteration 156, loss = 0.27486538887023926
iteration 157, loss = 0.1693412959575653
iteration 158, loss = 0.25074484944343567
iteration 159, loss = 0.2635470926761627
iteration 160, loss = 0.27526021003723145
iteration 161, loss = 0.1611173152923584
iteration 162, loss = 0.2196999043226242
iteration 163, loss = 0.21089257299900055
iteration 164, loss = 0.3038918077945709
iteration 165, loss = 0.25578728318214417
iteration 166, loss = 0.10745887458324432
iteration 167, loss = 0.160466730594635
iteration 168, loss = 0.18713608384132385
iteration 169, loss = 0.2280023992061615
iteration 170, loss = 0.2196880280971527
iteration 171, loss = 0.13243257999420166
iteration 172, loss = 0.1497781127691269
iteration 173, loss = 0.16348262131214142
iteration 174, loss = 0.14473284780979156
iteration 175, loss = 0.22994360327720642
iteration 176, loss = 0.132333904504776
iteration 177, loss = 0.3288920819759369
iteration 178, loss = 0.24966329336166382
iteration 179, loss = 0.17746838927268982
iteration 180, loss = 0.21617770195007324
iteration 181, loss = 0.23615971207618713
iteration 182, loss = 0.3310936689376831
iteration 183, loss = 0.2252892255783081
iteration 184, loss = 0.19415238499641418
iteration 185, loss = 0.18734021484851837
iteration 186, loss = 0.2433355748653412
iteration 187, loss = 0.21367113292217255
iteration 188, loss = 0.24952034652233124
iteration 189, loss = 0.38784781098365784
iteration 190, loss = 0.18950048089027405
iteration 191, loss = 0.20970848202705383
iteration 192, loss = 0.2296857237815857
iteration 193, loss = 0.18986845016479492
iteration 194, loss = 0.2534967362880707
iteration 195, loss = 0.11099451780319214
iteration 196, loss = 0.14001426100730896
iteration 197, loss = 0.24020014703273773
iteration 198, loss = 0.1458887755870819
iteration 199, loss = 0.12147796154022217
iteration 200, loss = 0.23904559016227722
iteration 201, loss = 0.21210363507270813
iteration 202, loss = 0.2393765151500702
iteration 203, loss = 0.38168609142303467
iteration 204, loss = 0.2182905375957489
iteration 205, loss = 0.3151443600654602
iteration 206, loss = 0.17456117272377014
iteration 207, loss = 0.2213478684425354
iteration 208, loss = 0.30871737003326416
iteration 209, loss = 0.3598994016647339
iteration 210, loss = 0.13060852885246277
iteration 211, loss = 0.2767684757709503
iteration 212, loss = 0.2663753032684326
iteration 213, loss = 0.15644504129886627
iteration 214, loss = 0.25771597027778625
iteration 215, loss = 0.12271421402692795
iteration 216, loss = 0.21038897335529327
iteration 217, loss = 0.26281365752220154
iteration 218, loss = 0.1963302195072174
iteration 219, loss = 0.20122361183166504
iteration 220, loss = 0.13496170938014984
iteration 221, loss = 0.2443547248840332
iteration 222, loss = 0.24823108315467834
iteration 223, loss = 0.33237260580062866
iteration 224, loss = 0.41648489236831665
iteration 225, loss = 0.21151623129844666
iteration 226, loss = 0.3154871463775635
iteration 227, loss = 0.13865908980369568
iteration 228, loss = 0.2661917507648468
iteration 229, loss = 0.2002878487110138
iteration 230, loss = 0.3371700346469879
iteration 231, loss = 0.2050791233778
iteration 232, loss = 0.24588041007518768
iteration 233, loss = 0.18109960854053497
iteration 234, loss = 0.1983298659324646
iteration 235, loss = 0.2668895423412323
iteration 236, loss = 0.19963917136192322
iteration 237, loss = 0.34984681010246277
iteration 238, loss = 0.12045923620462418
iteration 239, loss = 0.25389182567596436
iteration 240, loss = 0.22299028933048248
iteration 241, loss = 0.17451873421669006
iteration 242, loss = 0.225929394364357
iteration 243, loss = 0.3257778286933899
iteration 244, loss = 0.1261812299489975
iteration 245, loss = 0.126922607421875
iteration 246, loss = 0.17691782116889954
iteration 247, loss = 0.16195988655090332
iteration 248, loss = 0.19936710596084595
iteration 249, loss = 0.24023842811584473
iteration 250, loss = 0.2125118523836136
iteration 251, loss = 0.24031637609004974
iteration 252, loss = 0.2550727128982544
iteration 253, loss = 0.2653503119945526
iteration 254, loss = 0.3291841149330139
iteration 255, loss = 0.32596713304519653
iteration 256, loss = 0.20972363650798798
iteration 257, loss = 0.32282519340515137
iteration 258, loss = 0.23770828545093536
iteration 259, loss = 0.1622190624475479
iteration 260, loss = 0.368094801902771
iteration 261, loss = 0.13690704107284546
iteration 262, loss = 0.19390156865119934
iteration 263, loss = 0.13769318163394928
iteration 264, loss = 0.13657215237617493
iteration 265, loss = 0.16772598028182983
iteration 266, loss = 0.22218231856822968
iteration 267, loss = 0.1093570664525032
iteration 268, loss = 0.1312914490699768
iteration 269, loss = 0.26787424087524414
iteration 270, loss = 0.19010844826698303
iteration 271, loss = 0.2285575568675995
iteration 272, loss = 0.10621796548366547
iteration 273, loss = 0.40841910243034363
iteration 274, loss = 0.4091528356075287
iteration 275, loss = 0.19170784950256348
iteration 276, loss = 0.15188656747341156
iteration 277, loss = 0.4518986642360687
iteration 278, loss = 0.2818504869937897
iteration 279, loss = 0.272433340549469
iteration 280, loss = 0.17646706104278564
iteration 281, loss = 0.250029981136322
iteration 282, loss = 0.15249596536159515
iteration 283, loss = 0.2321529984474182
iteration 284, loss = 0.25229573249816895
iteration 285, loss = 0.16253599524497986
iteration 286, loss = 0.11131337285041809
iteration 287, loss = 0.27939969301223755
iteration 288, loss = 0.14514842629432678
iteration 289, loss = 0.12174146622419357
iteration 290, loss = 0.09480681270360947
iteration 291, loss = 0.17629742622375488
iteration 292, loss = 0.47319236397743225
iteration 293, loss = 0.22971992194652557
iteration 294, loss = 0.4260496497154236
iteration 295, loss = 0.2077229619026184
iteration 296, loss = 0.21363630890846252
iteration 297, loss = 0.21258288621902466
iteration 298, loss = 0.15772399306297302
iteration 299, loss = 0.23846623301506042
iteration 300, loss = 0.42835527658462524
iteration 1, loss = 0.17213892936706543
iteration 2, loss = 0.13114555180072784
iteration 3, loss = 0.16588160395622253
iteration 4, loss = 0.12647290527820587
iteration 5, loss = 0.16758620738983154
iteration 6, loss = 0.16658443212509155
iteration 7, loss = 0.1976272165775299
iteration 8, loss = 0.25867217779159546
iteration 9, loss = 0.19950249791145325
iteration 10, loss = 0.1550569236278534
iteration 11, loss = 0.25926661491394043
iteration 12, loss = 0.203596293926239
iteration 13, loss = 0.1780196577310562
iteration 14, loss = 0.21533843874931335
iteration 15, loss = 0.12229539453983307
iteration 16, loss = 0.20932698249816895
iteration 17, loss = 0.17778687179088593
iteration 18, loss = 0.280476838350296
iteration 19, loss = 0.26414892077445984
iteration 20, loss = 0.2988513112068176
iteration 21, loss = 0.0948895812034607
iteration 22, loss = 0.1585099697113037
iteration 23, loss = 0.2470536231994629
iteration 24, loss = 0.20288679003715515
iteration 25, loss = 0.23889866471290588
iteration 26, loss = 0.4121876358985901
iteration 27, loss = 0.2144705057144165
iteration 28, loss = 0.09879682213068008
iteration 29, loss = 0.2964783012866974
iteration 30, loss = 0.20888108015060425
iteration 31, loss = 0.27639397978782654
iteration 32, loss = 0.15505291521549225
iteration 33, loss = 0.3254103660583496
iteration 34, loss = 0.11113647371530533
iteration 35, loss = 0.23837818205356598
iteration 36, loss = 0.3441561162471771
iteration 37, loss = 0.30215927958488464
iteration 38, loss = 0.10572860389947891
iteration 39, loss = 0.10428011417388916
iteration 40, loss = 0.22290915250778198
iteration 41, loss = 0.13932737708091736
iteration 42, loss = 0.37790656089782715
iteration 43, loss = 0.198296457529068
iteration 44, loss = 0.1620626300573349
iteration 45, loss = 0.15182960033416748
iteration 46, loss = 0.3659955859184265
iteration 47, loss = 0.1262577474117279
iteration 48, loss = 0.2371787130832672
iteration 49, loss = 0.25880903005599976
iteration 50, loss = 0.2867963910102844
iteration 51, loss = 0.2590009868144989
iteration 52, loss = 0.17119580507278442
iteration 53, loss = 0.2621574401855469
iteration 54, loss = 0.13912183046340942
iteration 55, loss = 0.19675913453102112
iteration 56, loss = 0.206504225730896
iteration 57, loss = 0.30187705159187317
iteration 58, loss = 0.2461051493883133
iteration 59, loss = 0.3233649432659149
iteration 60, loss = 0.17952723801136017
iteration 61, loss = 0.24662692844867706
iteration 62, loss = 0.3101212680339813
iteration 63, loss = 0.34505796432495117
iteration 64, loss = 0.08921821415424347
iteration 65, loss = 0.22969676554203033
iteration 66, loss = 0.21406927704811096
iteration 67, loss = 0.23742027580738068
iteration 68, loss = 0.25455746054649353
iteration 69, loss = 0.1945304125547409
iteration 70, loss = 0.33907145261764526
iteration 71, loss = 0.1720023900270462
iteration 72, loss = 0.15366700291633606
iteration 73, loss = 0.230936199426651
iteration 74, loss = 0.17004817724227905
iteration 75, loss = 0.20057779550552368
iteration 76, loss = 0.2465876042842865
iteration 77, loss = 0.19553732872009277
iteration 78, loss = 0.36753880977630615
iteration 79, loss = 0.2071807086467743
iteration 80, loss = 0.19455748796463013
iteration 81, loss = 0.448424756526947
iteration 82, loss = 0.27567148208618164
iteration 83, loss = 0.15284830331802368
iteration 84, loss = 0.25758081674575806
iteration 85, loss = 0.33466726541519165
iteration 86, loss = 0.23747602105140686
iteration 87, loss = 0.23865705728530884
iteration 88, loss = 0.15201522409915924
iteration 89, loss = 0.2470526248216629
iteration 90, loss = 0.1161603033542633
iteration 91, loss = 0.2441527545452118
iteration 92, loss = 0.27557075023651123
iteration 93, loss = 0.16987060010433197
iteration 94, loss = 0.14800426363945007
iteration 95, loss = 0.11620031297206879
iteration 96, loss = 0.23360352218151093
iteration 97, loss = 0.2529262900352478
iteration 98, loss = 0.23189567029476166
iteration 99, loss = 0.3786305785179138
iteration 100, loss = 0.15915685892105103
iteration 101, loss = 0.355341374874115
iteration 102, loss = 0.10240662842988968
iteration 103, loss = 0.11720043420791626
iteration 104, loss = 0.1145620197057724
iteration 105, loss = 0.1437627375125885
iteration 106, loss = 0.15484747290611267
iteration 107, loss = 0.19853806495666504
iteration 108, loss = 0.2183382660150528
iteration 109, loss = 0.16887032985687256
iteration 110, loss = 0.18585115671157837
iteration 111, loss = 0.17305558919906616
iteration 112, loss = 0.18727543950080872
iteration 113, loss = 0.1749909222126007
iteration 114, loss = 0.14552058279514313
iteration 115, loss = 0.352535605430603
iteration 116, loss = 0.08775254338979721
iteration 117, loss = 0.08494079113006592
iteration 118, loss = 0.20326775312423706
iteration 119, loss = 0.3505653738975525
iteration 120, loss = 0.15877743065357208
iteration 121, loss = 0.2392989844083786
iteration 122, loss = 0.194144606590271
iteration 123, loss = 0.25564637780189514
iteration 124, loss = 0.263440877199173
iteration 125, loss = 0.12068316340446472
iteration 126, loss = 0.22625897824764252
iteration 127, loss = 0.30087849497795105
iteration 128, loss = 0.24346289038658142
iteration 129, loss = 0.2620293200016022
iteration 130, loss = 0.17897149920463562
iteration 131, loss = 0.19691015779972076
iteration 132, loss = 0.1787409484386444
iteration 133, loss = 0.4210371971130371
iteration 134, loss = 0.29304713010787964
iteration 135, loss = 0.12503668665885925
iteration 136, loss = 0.30292975902557373
iteration 137, loss = 0.17762695252895355
iteration 138, loss = 0.1368425190448761
iteration 139, loss = 0.13668841123580933
iteration 140, loss = 0.31291401386260986
iteration 141, loss = 0.15493084490299225
iteration 142, loss = 0.34381595253944397
iteration 143, loss = 0.1549544632434845
iteration 144, loss = 0.2094920128583908
iteration 145, loss = 0.22937819361686707
iteration 146, loss = 0.13394314050674438
iteration 147, loss = 0.12469138205051422
iteration 148, loss = 0.4777173399925232
iteration 149, loss = 0.19333265721797943
iteration 150, loss = 0.19516974687576294
iteration 151, loss = 0.35249119997024536
iteration 152, loss = 0.2955372929573059
iteration 153, loss = 0.1502079963684082
iteration 154, loss = 0.1380414366722107
iteration 155, loss = 0.11594264209270477
iteration 156, loss = 0.1716097742319107
iteration 157, loss = 0.32899758219718933
iteration 158, loss = 0.18355166912078857
iteration 159, loss = 0.10450360178947449
iteration 160, loss = 0.15824362635612488
iteration 161, loss = 0.2966355085372925
iteration 162, loss = 0.16611678898334503
iteration 163, loss = 0.35972732305526733
iteration 164, loss = 0.23652806878089905
iteration 165, loss = 0.1826937347650528
iteration 166, loss = 0.14962942898273468
iteration 167, loss = 0.18122413754463196
iteration 168, loss = 0.15079358220100403
iteration 169, loss = 0.1619848608970642
iteration 170, loss = 0.3139464855194092
iteration 171, loss = 0.29121100902557373
iteration 172, loss = 0.24083364009857178
iteration 173, loss = 0.28840160369873047
iteration 174, loss = 0.27858680486679077
iteration 175, loss = 0.16959039866924286
iteration 176, loss = 0.11707547307014465
iteration 177, loss = 0.15881909430027008
iteration 178, loss = 0.18154358863830566
iteration 179, loss = 0.27860942482948303
iteration 180, loss = 0.42588186264038086
iteration 181, loss = 0.13911542296409607
iteration 182, loss = 0.12854072451591492
iteration 183, loss = 0.4402935802936554
iteration 184, loss = 0.31654268503189087
iteration 185, loss = 0.17262420058250427
iteration 186, loss = 0.3810224235057831
iteration 187, loss = 0.13634957373142242
iteration 188, loss = 0.1813517063856125
iteration 189, loss = 0.20089024305343628
iteration 190, loss = 0.2411070466041565
iteration 191, loss = 0.1879081428050995
iteration 192, loss = 0.24493956565856934
iteration 193, loss = 0.29558858275413513
iteration 194, loss = 0.12367145717144012
iteration 195, loss = 0.3880678415298462
iteration 196, loss = 0.35439422726631165
iteration 197, loss = 0.32166972756385803
iteration 198, loss = 0.183864563703537
iteration 199, loss = 0.17209197580814362
iteration 200, loss = 0.14214542508125305
iteration 201, loss = 0.23993152379989624
iteration 202, loss = 0.38843899965286255
iteration 203, loss = 0.24905435740947723
iteration 204, loss = 0.21947485208511353
iteration 205, loss = 0.1749468743801117
iteration 206, loss = 0.1375647783279419
iteration 207, loss = 0.1431879848241806
iteration 208, loss = 0.1598709374666214
iteration 209, loss = 0.13458223640918732
iteration 210, loss = 0.25073152780532837
iteration 211, loss = 0.4172603487968445
iteration 212, loss = 0.3042580783367157
iteration 213, loss = 0.13844510912895203
iteration 214, loss = 0.2995750904083252
iteration 215, loss = 0.19639816880226135
iteration 216, loss = 0.24856510758399963
iteration 217, loss = 0.2042454183101654
iteration 218, loss = 0.2559892237186432
iteration 219, loss = 0.1943730115890503
iteration 220, loss = 0.14277073740959167
iteration 221, loss = 0.12335385382175446
iteration 222, loss = 0.17031246423721313
iteration 223, loss = 0.3143688440322876
iteration 224, loss = 0.08172663301229477
iteration 225, loss = 0.20389221608638763
iteration 226, loss = 0.26836085319519043
iteration 227, loss = 0.3254871964454651
iteration 228, loss = 0.3055041432380676
iteration 229, loss = 0.20056405663490295
iteration 230, loss = 0.17364604771137238
iteration 231, loss = 0.1837148815393448
iteration 232, loss = 0.26285094022750854
iteration 233, loss = 0.12777642905712128
iteration 234, loss = 0.13343915343284607
iteration 235, loss = 0.2455447018146515
iteration 236, loss = 0.24123075604438782
iteration 237, loss = 0.23812668025493622
iteration 238, loss = 0.2809887230396271
iteration 239, loss = 0.24515610933303833
iteration 240, loss = 0.12733837962150574
iteration 241, loss = 0.19103087484836578
iteration 242, loss = 0.2617872655391693
iteration 243, loss = 0.1880301535129547
iteration 244, loss = 0.1535671055316925
iteration 245, loss = 0.1506619155406952
iteration 246, loss = 0.12891048192977905
iteration 247, loss = 0.10129740089178085
iteration 248, loss = 0.1373005360364914
iteration 249, loss = 0.2352408766746521
iteration 250, loss = 0.12143517285585403
iteration 251, loss = 0.29291459918022156
iteration 252, loss = 0.2163754552602768
iteration 253, loss = 0.16017279028892517
iteration 254, loss = 0.2963501214981079
iteration 255, loss = 0.17356795072555542
iteration 256, loss = 0.18135981261730194
iteration 257, loss = 0.25930285453796387
iteration 258, loss = 0.21925988793373108
iteration 259, loss = 0.24334114789962769
iteration 260, loss = 0.33833688497543335
iteration 261, loss = 0.141595259308815
iteration 262, loss = 0.2951730191707611
iteration 263, loss = 0.20076747238636017
iteration 264, loss = 0.3464890122413635
iteration 265, loss = 0.1447715014219284
iteration 266, loss = 0.29420095682144165
iteration 267, loss = 0.21400216221809387
iteration 268, loss = 0.335564523935318
iteration 269, loss = 0.13047488033771515
iteration 270, loss = 0.23652975261211395
iteration 271, loss = 0.26178833842277527
iteration 272, loss = 0.20413541793823242
iteration 273, loss = 0.14231257140636444
iteration 274, loss = 0.1430821418762207
iteration 275, loss = 0.14391644299030304
iteration 276, loss = 0.19630053639411926
iteration 277, loss = 0.14157800376415253
iteration 278, loss = 0.21924303472042084
iteration 279, loss = 0.36112460494041443
iteration 280, loss = 0.3424692451953888
iteration 281, loss = 0.08781493455171585
iteration 282, loss = 0.09009774029254913
iteration 283, loss = 0.27055877447128296
iteration 284, loss = 0.26949355006217957
iteration 285, loss = 0.1651446372270584
iteration 286, loss = 0.22794859111309052
iteration 287, loss = 0.1497960090637207
iteration 288, loss = 0.22736988961696625
iteration 289, loss = 0.10079808533191681
iteration 290, loss = 0.11893504858016968
iteration 291, loss = 0.20547257363796234
iteration 292, loss = 0.16944639384746552
iteration 293, loss = 0.298964262008667
iteration 294, loss = 0.11891546100378036
iteration 295, loss = 0.330300509929657
iteration 296, loss = 0.1331382840871811
iteration 297, loss = 0.3282178044319153
iteration 298, loss = 0.28791341185569763
iteration 299, loss = 0.15915045142173767
iteration 300, loss = 0.10621386766433716
iteration 1, loss = 0.20458471775054932
iteration 2, loss = 0.2222287803888321
iteration 3, loss = 0.33689451217651367
iteration 4, loss = 0.2840253710746765
iteration 5, loss = 0.16599412262439728
iteration 6, loss = 0.20021119713783264
iteration 7, loss = 0.15147791802883148
iteration 8, loss = 0.18736578524112701
iteration 9, loss = 0.1207340657711029
iteration 10, loss = 0.20056572556495667
iteration 11, loss = 0.3467710018157959
iteration 12, loss = 0.2508614659309387
iteration 13, loss = 0.2385077178478241
iteration 14, loss = 0.2977217435836792
iteration 15, loss = 0.36701059341430664
iteration 16, loss = 0.28874149918556213
iteration 17, loss = 0.2407936304807663
iteration 18, loss = 0.22185271978378296
iteration 19, loss = 0.2817686200141907
iteration 20, loss = 0.28284192085266113
iteration 21, loss = 0.16063569486141205
iteration 22, loss = 0.20059487223625183
iteration 23, loss = 0.23714600503444672
iteration 24, loss = 0.1856442391872406
iteration 25, loss = 0.16471824049949646
iteration 26, loss = 0.19582250714302063
iteration 27, loss = 0.2308686524629593
iteration 28, loss = 0.12137308716773987
iteration 29, loss = 0.15413768589496613
iteration 30, loss = 0.21697872877120972
iteration 31, loss = 0.18976598978042603
iteration 32, loss = 0.23326513171195984
iteration 33, loss = 0.2159615010023117
iteration 34, loss = 0.19537591934204102
iteration 35, loss = 0.1477997601032257
iteration 36, loss = 0.2534831762313843
iteration 37, loss = 0.3158646821975708
iteration 38, loss = 0.15640747547149658
iteration 39, loss = 0.1705242097377777
iteration 40, loss = 0.08602667599916458
iteration 41, loss = 0.08201497793197632
iteration 42, loss = 0.10936052352190018
iteration 43, loss = 0.2587892711162567
iteration 44, loss = 0.07615366578102112
iteration 45, loss = 0.19439101219177246
iteration 46, loss = 0.2000177800655365
iteration 47, loss = 0.19319799542427063
iteration 48, loss = 0.42506998777389526
iteration 49, loss = 0.21656391024589539
iteration 50, loss = 0.21451109647750854
iteration 51, loss = 0.18726122379302979
iteration 52, loss = 0.27536213397979736
iteration 53, loss = 0.23577453196048737
iteration 54, loss = 0.0889708548784256
iteration 55, loss = 0.2911262810230255
iteration 56, loss = 0.19779258966445923
iteration 57, loss = 0.27101677656173706
iteration 58, loss = 0.31434932351112366
iteration 59, loss = 0.15126003324985504
iteration 60, loss = 0.3153744339942932
iteration 61, loss = 0.29660308361053467
iteration 62, loss = 0.24344417452812195
iteration 63, loss = 0.15298232436180115
iteration 64, loss = 0.237745463848114
iteration 65, loss = 0.17784598469734192
iteration 66, loss = 0.11613675951957703
iteration 67, loss = 0.13937468826770782
iteration 68, loss = 0.25424763560295105
iteration 69, loss = 0.10188346356153488
iteration 70, loss = 0.19438400864601135
iteration 71, loss = 0.23222465813159943
iteration 72, loss = 0.30154502391815186
iteration 73, loss = 0.23573563992977142
iteration 74, loss = 0.21343068778514862
iteration 75, loss = 0.34495750069618225
iteration 76, loss = 0.1560806930065155
iteration 77, loss = 0.2482263147830963
iteration 78, loss = 0.23208031058311462
iteration 79, loss = 0.1290198564529419
iteration 80, loss = 0.22959794104099274
iteration 81, loss = 0.2516287565231323
iteration 82, loss = 0.1559305042028427
iteration 83, loss = 0.2690914273262024
iteration 84, loss = 0.1427466869354248
iteration 85, loss = 0.230964794754982
iteration 86, loss = 0.1484818160533905
iteration 87, loss = 0.08350325375795364
iteration 88, loss = 0.20133712887763977
iteration 89, loss = 0.23013655841350555
iteration 90, loss = 0.33712708950042725
iteration 91, loss = 0.2185533344745636
iteration 92, loss = 0.1622704267501831
iteration 93, loss = 0.2655206322669983
iteration 94, loss = 0.15913209319114685
iteration 95, loss = 0.11902505159378052
iteration 96, loss = 0.2690473794937134
iteration 97, loss = 0.27441728115081787
iteration 98, loss = 0.3408525586128235
iteration 99, loss = 0.45122963190078735
iteration 100, loss = 0.1819804608821869
iteration 101, loss = 0.14207692444324493
iteration 102, loss = 0.20252767205238342
iteration 103, loss = 0.11900341510772705
iteration 104, loss = 0.17914769053459167
iteration 105, loss = 0.12491413205862045
iteration 106, loss = 0.28808972239494324
iteration 107, loss = 0.2000417411327362
iteration 108, loss = 0.13448315858840942
iteration 109, loss = 0.2127234935760498
iteration 110, loss = 0.3187575936317444
iteration 111, loss = 0.15787945687770844
iteration 112, loss = 0.17919018864631653
iteration 113, loss = 0.12027832865715027
iteration 114, loss = 0.08064909279346466
iteration 115, loss = 0.1760837584733963
iteration 116, loss = 0.15746545791625977
iteration 117, loss = 0.2038842737674713
iteration 118, loss = 0.09783559292554855
iteration 119, loss = 0.08677589148283005
iteration 120, loss = 0.146359384059906
iteration 121, loss = 0.23755332827568054
iteration 122, loss = 0.2984044849872589
iteration 123, loss = 0.29457801580429077
iteration 124, loss = 0.36348214745521545
iteration 125, loss = 0.2702404260635376
iteration 126, loss = 0.2235618382692337
iteration 127, loss = 0.2482183426618576
iteration 128, loss = 0.39955976605415344
iteration 129, loss = 0.13113214075565338
iteration 130, loss = 0.18461936712265015
iteration 131, loss = 0.23066136240959167
iteration 132, loss = 0.24436965584754944
iteration 133, loss = 0.18506638705730438
iteration 134, loss = 0.08952289819717407
iteration 135, loss = 0.2772432565689087
iteration 136, loss = 0.10136279463768005
iteration 137, loss = 0.2329961508512497
iteration 138, loss = 0.09994196146726608
iteration 139, loss = 0.2023630440235138
iteration 140, loss = 0.19467693567276
iteration 141, loss = 0.21389782428741455
iteration 142, loss = 0.1998276263475418
iteration 143, loss = 0.37549787759780884
iteration 144, loss = 0.1975681334733963
iteration 145, loss = 0.1359269618988037
iteration 146, loss = 0.32282090187072754
iteration 147, loss = 0.14354270696640015
iteration 148, loss = 0.13498416543006897
iteration 149, loss = 0.19419187307357788
iteration 150, loss = 0.15046533942222595
iteration 151, loss = 0.1439078003168106
iteration 152, loss = 0.17303571105003357
iteration 153, loss = 0.20836885273456573
iteration 154, loss = 0.07984769344329834
iteration 155, loss = 0.09689368307590485
iteration 156, loss = 0.1706218123435974
iteration 157, loss = 0.10372334718704224
iteration 158, loss = 0.096795454621315
iteration 159, loss = 0.2756396532058716
iteration 160, loss = 0.31011727452278137
iteration 161, loss = 0.2497759461402893
iteration 162, loss = 0.2505105137825012
iteration 163, loss = 0.2826704978942871
iteration 164, loss = 0.15824273228645325
iteration 165, loss = 0.1156482845544815
iteration 166, loss = 0.22198916971683502
iteration 167, loss = 0.2187339961528778
iteration 168, loss = 0.18070322275161743
iteration 169, loss = 0.2892928123474121
iteration 170, loss = 0.24937742948532104
iteration 171, loss = 0.08982302993535995
iteration 172, loss = 0.23578554391860962
iteration 173, loss = 0.3181634545326233
iteration 174, loss = 0.17747923731803894
iteration 175, loss = 0.16003179550170898
iteration 176, loss = 0.275387167930603
iteration 177, loss = 0.11768365651369095
iteration 178, loss = 0.4005025029182434
iteration 179, loss = 0.35235071182250977
iteration 180, loss = 0.18163862824440002
iteration 181, loss = 0.3138374090194702
iteration 182, loss = 0.15500637888908386
iteration 183, loss = 0.15788796544075012
iteration 184, loss = 0.14133329689502716
iteration 185, loss = 0.13455873727798462
iteration 186, loss = 0.256522536277771
iteration 187, loss = 0.2748134434223175
iteration 188, loss = 0.1843137890100479
iteration 189, loss = 0.29377102851867676
iteration 190, loss = 0.1203034371137619
iteration 191, loss = 0.20329561829566956
iteration 192, loss = 0.094100221991539
iteration 193, loss = 0.2371489405632019
iteration 194, loss = 0.11680401861667633
iteration 195, loss = 0.2657220661640167
iteration 196, loss = 0.3115499019622803
iteration 197, loss = 0.3657660186290741
iteration 198, loss = 0.14076215028762817
iteration 199, loss = 0.22371430695056915
iteration 200, loss = 0.22036468982696533
iteration 201, loss = 0.1841612309217453
iteration 202, loss = 0.10517866909503937
iteration 203, loss = 0.2994440197944641
iteration 204, loss = 0.1469287872314453
iteration 205, loss = 0.16847798228263855
iteration 206, loss = 0.076966792345047
iteration 207, loss = 0.14128747582435608
iteration 208, loss = 0.26428818702697754
iteration 209, loss = 0.24728664755821228
iteration 210, loss = 0.17234280705451965
iteration 211, loss = 0.1416337639093399
iteration 212, loss = 0.1083618700504303
iteration 213, loss = 0.13321253657341003
iteration 214, loss = 0.10512537509202957
iteration 215, loss = 0.0763220265507698
iteration 216, loss = 0.23190873861312866
iteration 217, loss = 0.1183769702911377
iteration 218, loss = 0.15780240297317505
iteration 219, loss = 0.22313940525054932
iteration 220, loss = 0.2872365117073059
iteration 221, loss = 0.20306627452373505
iteration 222, loss = 0.07424666732549667
iteration 223, loss = 0.2500186860561371
iteration 224, loss = 0.18232879042625427
iteration 225, loss = 0.4536120593547821
iteration 226, loss = 0.2673627436161041
iteration 227, loss = 0.3254755735397339
iteration 228, loss = 0.2941463887691498
iteration 229, loss = 0.30232059955596924
iteration 230, loss = 0.2329288125038147
iteration 231, loss = 0.09431952983140945
iteration 232, loss = 0.40742844343185425
iteration 233, loss = 0.19508589804172516
iteration 234, loss = 0.1343376636505127
iteration 235, loss = 0.14596262574195862
iteration 236, loss = 0.33620864152908325
iteration 237, loss = 0.367876797914505
iteration 238, loss = 0.269438773393631
iteration 239, loss = 0.14132258296012878
iteration 240, loss = 0.1659623086452484
iteration 241, loss = 0.13221104443073273
iteration 242, loss = 0.18484987318515778
iteration 243, loss = 0.26372483372688293
iteration 244, loss = 0.22024071216583252
iteration 245, loss = 0.2248489260673523
iteration 246, loss = 0.18497039377689362
iteration 247, loss = 0.19377198815345764
iteration 248, loss = 0.19445809721946716
iteration 249, loss = 0.19660547375679016
iteration 250, loss = 0.19102519750595093
iteration 251, loss = 0.2390313744544983
iteration 252, loss = 0.2082728147506714
iteration 253, loss = 0.20211061835289001
iteration 254, loss = 0.33281898498535156
iteration 255, loss = 0.18154536187648773
iteration 256, loss = 0.3086036443710327
iteration 257, loss = 0.1808532178401947
iteration 258, loss = 0.38406962156295776
iteration 259, loss = 0.25230589509010315
iteration 260, loss = 0.14323383569717407
iteration 261, loss = 0.2742577791213989
iteration 262, loss = 0.19569727778434753
iteration 263, loss = 0.09443619102239609
iteration 264, loss = 0.30850738286972046
iteration 265, loss = 0.1858108937740326
iteration 266, loss = 0.11606060713529587
iteration 267, loss = 0.3130929172039032
iteration 268, loss = 0.40893787145614624
iteration 269, loss = 0.16216152906417847
iteration 270, loss = 0.25266340374946594
iteration 271, loss = 0.18202853202819824
iteration 272, loss = 0.17592456936836243
iteration 273, loss = 0.38114133477211
iteration 274, loss = 0.15490439534187317
iteration 275, loss = 0.36876949667930603
iteration 276, loss = 0.11907652765512466
iteration 277, loss = 0.20527909696102142
iteration 278, loss = 0.15136776864528656
iteration 279, loss = 0.12299667298793793
iteration 280, loss = 0.12761381268501282
iteration 281, loss = 0.27857285737991333
iteration 282, loss = 0.19543401896953583
iteration 283, loss = 0.3088761866092682
iteration 284, loss = 0.1140919104218483
iteration 285, loss = 0.19520163536071777
iteration 286, loss = 0.269037663936615
iteration 287, loss = 0.28875964879989624
iteration 288, loss = 0.34402281045913696
iteration 289, loss = 0.15705278515815735
iteration 290, loss = 0.13948333263397217
iteration 291, loss = 0.15603007376194
iteration 292, loss = 0.13217540085315704
iteration 293, loss = 0.13040758669376373
iteration 294, loss = 0.24828281998634338
iteration 295, loss = 0.22258500754833221
iteration 296, loss = 0.2721029222011566
iteration 297, loss = 0.07590331137180328
iteration 298, loss = 0.10790887475013733
iteration 299, loss = 0.18523888289928436
iteration 300, loss = 0.2325216829776764
iteration 1, loss = 0.14574143290519714
iteration 2, loss = 0.1539192795753479
iteration 3, loss = 0.2001565396785736
iteration 4, loss = 0.20773687958717346
iteration 5, loss = 0.24705883860588074
iteration 6, loss = 0.24157850444316864
iteration 7, loss = 0.2342020571231842
iteration 8, loss = 0.10546128451824188
iteration 9, loss = 0.22137890756130219
iteration 10, loss = 0.14639617502689362
iteration 11, loss = 0.4163029193878174
iteration 12, loss = 0.10289158672094345
iteration 13, loss = 0.11264921724796295
iteration 14, loss = 0.23142775893211365
iteration 15, loss = 0.12659311294555664
iteration 16, loss = 0.3392113745212555
iteration 17, loss = 0.17380550503730774
iteration 18, loss = 0.40706077218055725
iteration 19, loss = 0.26575130224227905
iteration 20, loss = 0.09872214496135712
iteration 21, loss = 0.2852647304534912
iteration 22, loss = 0.29905611276626587
iteration 23, loss = 0.15396235883235931
iteration 24, loss = 0.3503408432006836
iteration 25, loss = 0.19508682191371918
iteration 26, loss = 0.15693920850753784
iteration 27, loss = 0.17916373908519745
iteration 28, loss = 0.2798088788986206
iteration 29, loss = 0.17433591187000275
iteration 30, loss = 0.36523836851119995
iteration 31, loss = 0.11486631631851196
iteration 32, loss = 0.16573798656463623
iteration 33, loss = 0.1405602991580963
iteration 34, loss = 0.29080650210380554
iteration 35, loss = 0.23680590093135834
iteration 36, loss = 0.19794505834579468
iteration 37, loss = 0.09521950781345367
iteration 38, loss = 0.1862134486436844
iteration 39, loss = 0.14564409852027893
iteration 40, loss = 0.2699856162071228
iteration 41, loss = 0.18544931709766388
iteration 42, loss = 0.19724534451961517
iteration 43, loss = 0.11254990100860596
iteration 44, loss = 0.24386054277420044
iteration 45, loss = 0.22086922824382782
iteration 46, loss = 0.1315491497516632
iteration 47, loss = 0.24461698532104492
iteration 48, loss = 0.17395181953907013
iteration 49, loss = 0.18454968929290771
iteration 50, loss = 0.23129628598690033
iteration 51, loss = 0.13803720474243164
iteration 52, loss = 0.18541985750198364
iteration 53, loss = 0.10133315622806549
iteration 54, loss = 0.32152894139289856
iteration 55, loss = 0.24275650084018707
iteration 56, loss = 0.1016760766506195
iteration 57, loss = 0.15541841089725494
iteration 58, loss = 0.08398936688899994
iteration 59, loss = 0.32926565408706665
iteration 60, loss = 0.14633771777153015
iteration 61, loss = 0.11849585175514221
iteration 62, loss = 0.12776902318000793
iteration 63, loss = 0.4222792387008667
iteration 64, loss = 0.19653016328811646
iteration 65, loss = 0.14398226141929626
iteration 66, loss = 0.22949010133743286
iteration 67, loss = 0.14627794921398163
iteration 68, loss = 0.13482648134231567
iteration 69, loss = 0.16044652462005615
iteration 70, loss = 0.33697688579559326
iteration 71, loss = 0.1616494357585907
iteration 72, loss = 0.26908355951309204
iteration 73, loss = 0.10931308567523956
iteration 74, loss = 0.22031593322753906
iteration 75, loss = 0.18081051111221313
iteration 76, loss = 0.2458133101463318
iteration 77, loss = 0.23999762535095215
iteration 78, loss = 0.12683892250061035
iteration 79, loss = 0.12366954982280731
iteration 80, loss = 0.3231964707374573
iteration 81, loss = 0.367013156414032
iteration 82, loss = 0.24182969331741333
iteration 83, loss = 0.22323015332221985
iteration 84, loss = 0.16279447078704834
iteration 85, loss = 0.16081967949867249
iteration 86, loss = 0.29539093375205994
iteration 87, loss = 0.39492955803871155
iteration 88, loss = 0.29285696148872375
iteration 89, loss = 0.2335568517446518
iteration 90, loss = 0.3237704634666443
iteration 91, loss = 0.33361831307411194
iteration 92, loss = 0.391496479511261
iteration 93, loss = 0.1020592674612999
iteration 94, loss = 0.1269352287054062
iteration 95, loss = 0.23445875942707062
iteration 96, loss = 0.20200559496879578
iteration 97, loss = 0.14962196350097656
iteration 98, loss = 0.09781339019536972
iteration 99, loss = 0.2177330106496811
iteration 100, loss = 0.2793843150138855
iteration 101, loss = 0.11220237612724304
iteration 102, loss = 0.11235729604959488
iteration 103, loss = 0.17262570559978485
iteration 104, loss = 0.21507792174816132
iteration 105, loss = 0.1410239338874817
iteration 106, loss = 0.16581600904464722
iteration 107, loss = 0.1386333853006363
iteration 108, loss = 0.12308097630739212
iteration 109, loss = 0.13664314150810242
iteration 110, loss = 0.18801197409629822
iteration 111, loss = 0.24020281434059143
iteration 112, loss = 0.22217893600463867
iteration 113, loss = 0.07605000585317612
iteration 114, loss = 0.20637840032577515
iteration 115, loss = 0.2520989775657654
iteration 116, loss = 0.21336382627487183
iteration 117, loss = 0.19025465846061707
iteration 118, loss = 0.24254998564720154
iteration 119, loss = 0.08125840127468109
iteration 120, loss = 0.28230562806129456
iteration 121, loss = 0.22486551105976105
iteration 122, loss = 0.12868571281433105
iteration 123, loss = 0.29088059067726135
iteration 124, loss = 0.24452833831310272
iteration 125, loss = 0.14937114715576172
iteration 126, loss = 0.09020930528640747
iteration 127, loss = 0.12844493985176086
iteration 128, loss = 0.2641339898109436
iteration 129, loss = 0.14607661962509155
iteration 130, loss = 0.22458285093307495
iteration 131, loss = 0.16516058146953583
iteration 132, loss = 0.1503133475780487
iteration 133, loss = 0.3361971378326416
iteration 134, loss = 0.25184160470962524
iteration 135, loss = 0.1831720471382141
iteration 136, loss = 0.1574019491672516
iteration 137, loss = 0.30386707186698914
iteration 138, loss = 0.18394912779331207
iteration 139, loss = 0.07703579217195511
iteration 140, loss = 0.14686048030853271
iteration 141, loss = 0.14926184713840485
iteration 142, loss = 0.1659643054008484
iteration 143, loss = 0.2909005880355835
iteration 144, loss = 0.11385674774646759
iteration 145, loss = 0.26762455701828003
iteration 146, loss = 0.15291491150856018
iteration 147, loss = 0.23229418694972992
iteration 148, loss = 0.18885871767997742
iteration 149, loss = 0.2328524887561798
iteration 150, loss = 0.27978670597076416
iteration 151, loss = 0.25566238164901733
iteration 152, loss = 0.1422915905714035
iteration 153, loss = 0.10994507372379303
iteration 154, loss = 0.18674074113368988
iteration 155, loss = 0.3138226270675659
iteration 156, loss = 0.287090927362442
iteration 157, loss = 0.14748626947402954
iteration 158, loss = 0.13620319962501526
iteration 159, loss = 0.24524740874767303
iteration 160, loss = 0.15636666119098663
iteration 161, loss = 0.17364445328712463
iteration 162, loss = 0.22437532246112823
iteration 163, loss = 0.4428435266017914
iteration 164, loss = 0.25109511613845825
iteration 165, loss = 0.12223276495933533
iteration 166, loss = 0.181437149643898
iteration 167, loss = 0.14044299721717834
iteration 168, loss = 0.12334698438644409
iteration 169, loss = 0.19182711839675903
iteration 170, loss = 0.3328329026699066
iteration 171, loss = 0.25464504957199097
iteration 172, loss = 0.24784675240516663
iteration 173, loss = 0.2567017078399658
iteration 174, loss = 0.15445983409881592
iteration 175, loss = 0.0882076770067215
iteration 176, loss = 0.45073482394218445
iteration 177, loss = 0.268135130405426
iteration 178, loss = 0.20716242492198944
iteration 179, loss = 0.13737888634204865
iteration 180, loss = 0.09606590121984482
iteration 181, loss = 0.128948375582695
iteration 182, loss = 0.17035582661628723
iteration 183, loss = 0.27365416288375854
iteration 184, loss = 0.15062224864959717
iteration 185, loss = 0.13584712147712708
iteration 186, loss = 0.30947333574295044
iteration 187, loss = 0.29056793451309204
iteration 188, loss = 0.18883506953716278
iteration 189, loss = 0.17492444813251495
iteration 190, loss = 0.2432994544506073
iteration 191, loss = 0.3003008961677551
iteration 192, loss = 0.1361047625541687
iteration 193, loss = 0.1484259068965912
iteration 194, loss = 0.09665299952030182
iteration 195, loss = 0.17910054326057434
iteration 196, loss = 0.28016120195388794
iteration 197, loss = 0.2441798448562622
iteration 198, loss = 0.211478590965271
iteration 199, loss = 0.138771653175354
iteration 200, loss = 0.06947361677885056
iteration 201, loss = 0.13240589201450348
iteration 202, loss = 0.2580941915512085
iteration 203, loss = 0.17992688715457916
iteration 204, loss = 0.1908857375383377
iteration 205, loss = 0.3085275888442993
iteration 206, loss = 0.224700927734375
iteration 207, loss = 0.2543776035308838
iteration 208, loss = 0.27945661544799805
iteration 209, loss = 0.29671764373779297
iteration 210, loss = 0.2368200123310089
iteration 211, loss = 0.3635203242301941
iteration 212, loss = 0.09236929565668106
iteration 213, loss = 0.1797376573085785
iteration 214, loss = 0.14992597699165344
iteration 215, loss = 0.23365046083927155
iteration 216, loss = 0.23603074252605438
iteration 217, loss = 0.1684410274028778
iteration 218, loss = 0.20544329285621643
iteration 219, loss = 0.42876747250556946
iteration 220, loss = 0.3030672073364258
iteration 221, loss = 0.12248086929321289
iteration 222, loss = 0.10068750381469727
iteration 223, loss = 0.18822473287582397
iteration 224, loss = 0.28000015020370483
iteration 225, loss = 0.21848610043525696
iteration 226, loss = 0.10098893195390701
iteration 227, loss = 0.08458471298217773
iteration 228, loss = 0.1323791742324829
iteration 229, loss = 0.24000990390777588
iteration 230, loss = 0.07755299657583237
iteration 231, loss = 0.0797516256570816
iteration 232, loss = 0.20745238661766052
iteration 233, loss = 0.1905348002910614
iteration 234, loss = 0.3413616418838501
iteration 235, loss = 0.2665497362613678
iteration 236, loss = 0.28428778052330017
iteration 237, loss = 0.16938400268554688
iteration 238, loss = 0.28085649013519287
iteration 239, loss = 0.25267624855041504
iteration 240, loss = 0.1685120165348053
iteration 241, loss = 0.18565809726715088
iteration 242, loss = 0.12357652187347412
iteration 243, loss = 0.11899801343679428
iteration 244, loss = 0.28924471139907837
iteration 245, loss = 0.20268464088439941
iteration 246, loss = 0.2473457008600235
iteration 247, loss = 0.12961015105247498
iteration 248, loss = 0.16604885458946228
iteration 249, loss = 0.19732819497585297
iteration 250, loss = 0.24552471935749054
iteration 251, loss = 0.18761776387691498
iteration 252, loss = 0.09168193489313126
iteration 253, loss = 0.11373933404684067
iteration 254, loss = 0.15112975239753723
iteration 255, loss = 0.15861353278160095
iteration 256, loss = 0.13505756855010986
iteration 257, loss = 0.09258970618247986
iteration 258, loss = 0.23711736500263214
iteration 259, loss = 0.4015209972858429
iteration 260, loss = 0.24172437191009521
iteration 261, loss = 0.251443088054657
iteration 262, loss = 0.19540417194366455
iteration 263, loss = 0.25640854239463806
iteration 264, loss = 0.1687016487121582
iteration 265, loss = 0.31040212512016296
iteration 266, loss = 0.25687509775161743
iteration 267, loss = 0.2714441418647766
iteration 268, loss = 0.19302713871002197
iteration 269, loss = 0.11332045495510101
iteration 270, loss = 0.26914218068122864
iteration 271, loss = 0.24936531484127045
iteration 272, loss = 0.1849481463432312
iteration 273, loss = 0.16443252563476562
iteration 274, loss = 0.1368999034166336
iteration 275, loss = 0.2178446501493454
iteration 276, loss = 0.2526678740978241
iteration 277, loss = 0.23479512333869934
iteration 278, loss = 0.14525800943374634
iteration 279, loss = 0.15004019439220428
iteration 280, loss = 0.3111325800418854
iteration 281, loss = 0.2508540749549866
iteration 282, loss = 0.16122743487358093
iteration 283, loss = 0.15953169763088226
iteration 284, loss = 0.17356246709823608
iteration 285, loss = 0.1928238421678543
iteration 286, loss = 0.11415992677211761
iteration 287, loss = 0.10409548878669739
iteration 288, loss = 0.14653432369232178
iteration 289, loss = 0.16711921989917755
iteration 290, loss = 0.18358232080936432
iteration 291, loss = 0.21678988635540009
iteration 292, loss = 0.19302459061145782
iteration 293, loss = 0.18816177546977997
iteration 294, loss = 0.3159022331237793
iteration 295, loss = 0.2440505474805832
iteration 296, loss = 0.23433849215507507
iteration 297, loss = 0.11024793237447739
iteration 298, loss = 0.2526630461215973
iteration 299, loss = 0.15075810253620148
iteration 300, loss = 0.2683851718902588
iteration 1, loss = 0.12710663676261902
iteration 2, loss = 0.09938247501850128
iteration 3, loss = 0.09950534999370575
iteration 4, loss = 0.12936687469482422
iteration 5, loss = 0.2129257619380951
iteration 6, loss = 0.38538992404937744
iteration 7, loss = 0.37757354974746704
iteration 8, loss = 0.13118501007556915
iteration 9, loss = 0.30744093656539917
iteration 10, loss = 0.19740700721740723
iteration 11, loss = 0.20376013219356537
iteration 12, loss = 0.09256360679864883
iteration 13, loss = 0.13891088962554932
iteration 14, loss = 0.18448534607887268
iteration 15, loss = 0.2706944942474365
iteration 16, loss = 0.21717680990695953
iteration 17, loss = 0.21255064010620117
iteration 18, loss = 0.1186901405453682
iteration 19, loss = 0.29338130354881287
iteration 20, loss = 0.0732213482260704
iteration 21, loss = 0.25993722677230835
iteration 22, loss = 0.19434040784835815
iteration 23, loss = 0.12276014685630798
iteration 24, loss = 0.1924542337656021
iteration 25, loss = 0.28867825865745544
iteration 26, loss = 0.32043859362602234
iteration 27, loss = 0.21899612247943878
iteration 28, loss = 0.13694420456886292
iteration 29, loss = 0.10754233598709106
iteration 30, loss = 0.12411032617092133
iteration 31, loss = 0.30281195044517517
iteration 32, loss = 0.11423797160387039
iteration 33, loss = 0.14865663647651672
iteration 34, loss = 0.24665126204490662
iteration 35, loss = 0.2767476439476013
iteration 36, loss = 0.15155033767223358
iteration 37, loss = 0.16553226113319397
iteration 38, loss = 0.14995436370372772
iteration 39, loss = 0.1398046463727951
iteration 40, loss = 0.18257997930049896
iteration 41, loss = 0.29159826040267944
iteration 42, loss = 0.11875416338443756
iteration 43, loss = 0.11863978952169418
iteration 44, loss = 0.16663892567157745
iteration 45, loss = 0.24390828609466553
iteration 46, loss = 0.22730715572834015
iteration 47, loss = 0.13978073000907898
iteration 48, loss = 0.10290617495775223
iteration 49, loss = 0.2627905607223511
iteration 50, loss = 0.2129305601119995
iteration 51, loss = 0.23756377398967743
iteration 52, loss = 0.15192493796348572
iteration 53, loss = 0.08532066643238068
iteration 54, loss = 0.3220023512840271
iteration 55, loss = 0.20772486925125122
iteration 56, loss = 0.1805415153503418
iteration 57, loss = 0.2163524031639099
iteration 58, loss = 0.43386605381965637
iteration 59, loss = 0.23951610922813416
iteration 60, loss = 0.12757280468940735
iteration 61, loss = 0.29140135645866394
iteration 62, loss = 0.16705915331840515
iteration 63, loss = 0.1432327926158905
iteration 64, loss = 0.1659647524356842
iteration 65, loss = 0.07386363297700882
iteration 66, loss = 0.19439245760440826
iteration 67, loss = 0.17525917291641235
iteration 68, loss = 0.33306485414505005
iteration 69, loss = 0.15597335994243622
iteration 70, loss = 0.24675896763801575
iteration 71, loss = 0.15996447205543518
iteration 72, loss = 0.1603194773197174
iteration 73, loss = 0.33826160430908203
iteration 74, loss = 0.23101100325584412
iteration 75, loss = 0.24866613745689392
iteration 76, loss = 0.13361413776874542
iteration 77, loss = 0.06956213712692261
iteration 78, loss = 0.26855355501174927
iteration 79, loss = 0.24424782395362854
iteration 80, loss = 0.3196832537651062
iteration 81, loss = 0.32080769538879395
iteration 82, loss = 0.1516716182231903
iteration 83, loss = 0.28884536027908325
iteration 84, loss = 0.329378217458725
iteration 85, loss = 0.11943020671606064
iteration 86, loss = 0.2987098693847656
iteration 87, loss = 0.20606312155723572
iteration 88, loss = 0.22495320439338684
iteration 89, loss = 0.2682219445705414
iteration 90, loss = 0.19753403961658478
iteration 91, loss = 0.2872876822948456
iteration 92, loss = 0.14542973041534424
iteration 93, loss = 0.12320747971534729
iteration 94, loss = 0.27300506830215454
iteration 95, loss = 0.22332651913166046
iteration 96, loss = 0.28296494483947754
iteration 97, loss = 0.12816685438156128
iteration 98, loss = 0.21270909905433655
iteration 99, loss = 0.16087231040000916
iteration 100, loss = 0.11234669387340546
iteration 101, loss = 0.18859407305717468
iteration 102, loss = 0.0952015221118927
iteration 103, loss = 0.26125016808509827
iteration 104, loss = 0.2463834583759308
iteration 105, loss = 0.14983832836151123
iteration 106, loss = 0.2794322371482849
iteration 107, loss = 0.12656879425048828
iteration 108, loss = 0.23990842700004578
iteration 109, loss = 0.11339056491851807
iteration 110, loss = 0.14919239282608032
iteration 111, loss = 0.10644164681434631
iteration 112, loss = 0.11903521418571472
iteration 113, loss = 0.20303228497505188
iteration 114, loss = 0.23144713044166565
iteration 115, loss = 0.11544369161128998
iteration 116, loss = 0.1226554811000824
iteration 117, loss = 0.2515716850757599
iteration 118, loss = 0.12213118374347687
iteration 119, loss = 0.15622884035110474
iteration 120, loss = 0.25812605023384094
iteration 121, loss = 0.12015314400196075
iteration 122, loss = 0.1561911404132843
iteration 123, loss = 0.11118565499782562
iteration 124, loss = 0.2453511357307434
iteration 125, loss = 0.2942273020744324
iteration 126, loss = 0.13197138905525208
iteration 127, loss = 0.18610870838165283
iteration 128, loss = 0.23931387066841125
iteration 129, loss = 0.14449776709079742
iteration 130, loss = 0.21672920882701874
iteration 131, loss = 0.13368487358093262
iteration 132, loss = 0.34800663590431213
iteration 133, loss = 0.20293034613132477
iteration 134, loss = 0.22570422291755676
iteration 135, loss = 0.2854706645011902
iteration 136, loss = 0.12696729600429535
iteration 137, loss = 0.3410094678401947
iteration 138, loss = 0.1754397451877594
iteration 139, loss = 0.15528790652751923
iteration 140, loss = 0.07230357825756073
iteration 141, loss = 0.25953754782676697
iteration 142, loss = 0.253173828125
iteration 143, loss = 0.1596250832080841
iteration 144, loss = 0.1521618366241455
iteration 145, loss = 0.10411378741264343
iteration 146, loss = 0.29538434743881226
iteration 147, loss = 0.2910677194595337
iteration 148, loss = 0.1370815634727478
iteration 149, loss = 0.1601395308971405
iteration 150, loss = 0.06845749914646149
iteration 151, loss = 0.22843289375305176
iteration 152, loss = 0.21359753608703613
iteration 153, loss = 0.10127026587724686
iteration 154, loss = 0.234457328915596
iteration 155, loss = 0.0963384285569191
iteration 156, loss = 0.18590767681598663
iteration 157, loss = 0.12226933240890503
iteration 158, loss = 0.25217169523239136
iteration 159, loss = 0.19756388664245605
iteration 160, loss = 0.09794709831476212
iteration 161, loss = 0.1026221439242363
iteration 162, loss = 0.11887499690055847
iteration 163, loss = 0.24117228388786316
iteration 164, loss = 0.13987073302268982
iteration 165, loss = 0.10766486078500748
iteration 166, loss = 0.30641695857048035
iteration 167, loss = 0.08494532108306885
iteration 168, loss = 0.1421041041612625
iteration 169, loss = 0.17125612497329712
iteration 170, loss = 0.29035142064094543
iteration 171, loss = 0.19230210781097412
iteration 172, loss = 0.09697942435741425
iteration 173, loss = 0.23455378413200378
iteration 174, loss = 0.23370543122291565
iteration 175, loss = 0.1609870195388794
iteration 176, loss = 0.44168397784233093
iteration 177, loss = 0.08482344448566437
iteration 178, loss = 0.1995624601840973
iteration 179, loss = 0.3205322325229645
iteration 180, loss = 0.11690317094326019
iteration 181, loss = 0.1547105610370636
iteration 182, loss = 0.22800378501415253
iteration 183, loss = 0.25016456842422485
iteration 184, loss = 0.25582462549209595
iteration 185, loss = 0.3090249300003052
iteration 186, loss = 0.19470566511154175
iteration 187, loss = 0.22394394874572754
iteration 188, loss = 0.22563053667545319
iteration 189, loss = 0.21121272444725037
iteration 190, loss = 0.2767256498336792
iteration 191, loss = 0.18177419900894165
iteration 192, loss = 0.21689775586128235
iteration 193, loss = 0.22475236654281616
iteration 194, loss = 0.1862112283706665
iteration 195, loss = 0.1749018132686615
iteration 196, loss = 0.14686672389507294
iteration 197, loss = 0.26682692766189575
iteration 198, loss = 0.1630784422159195
iteration 199, loss = 0.26764369010925293
iteration 200, loss = 0.20311424136161804
iteration 201, loss = 0.2551317512989044
iteration 202, loss = 0.19151990115642548
iteration 203, loss = 0.09261105209589005
iteration 204, loss = 0.2210107445716858
iteration 205, loss = 0.19879935681819916
iteration 206, loss = 0.29158249497413635
iteration 207, loss = 0.19717484712600708
iteration 208, loss = 0.23138615489006042
iteration 209, loss = 0.19944486021995544
iteration 210, loss = 0.19678297638893127
iteration 211, loss = 0.17815001308918
iteration 212, loss = 0.1798328161239624
iteration 213, loss = 0.2996118664741516
iteration 214, loss = 0.2540789544582367
iteration 215, loss = 0.20455819368362427
iteration 216, loss = 0.18968826532363892
iteration 217, loss = 0.11828865855932236
iteration 218, loss = 0.1033453643321991
iteration 219, loss = 0.10782939940690994
iteration 220, loss = 0.13029320538043976
iteration 221, loss = 0.07888324558734894
iteration 222, loss = 0.21384118497371674
iteration 223, loss = 0.16152901947498322
iteration 224, loss = 0.13537216186523438
iteration 225, loss = 0.2629510164260864
iteration 226, loss = 0.38952213525772095
iteration 227, loss = 0.1523381769657135
iteration 228, loss = 0.15481433272361755
iteration 229, loss = 0.1854027658700943
iteration 230, loss = 0.1313224732875824
iteration 231, loss = 0.15589511394500732
iteration 232, loss = 0.07261788845062256
iteration 233, loss = 0.10616926848888397
iteration 234, loss = 0.11097480356693268
iteration 235, loss = 0.1275719553232193
iteration 236, loss = 0.44402387738227844
iteration 237, loss = 0.10671491920948029
iteration 238, loss = 0.371564656496048
iteration 239, loss = 0.17953211069107056
iteration 240, loss = 0.2465532422065735
iteration 241, loss = 0.12165197730064392
iteration 242, loss = 0.25363677740097046
iteration 243, loss = 0.24799785017967224
iteration 244, loss = 0.17950278520584106
iteration 245, loss = 0.16542035341262817
iteration 246, loss = 0.1297788769006729
iteration 247, loss = 0.14437152445316315
iteration 248, loss = 0.32005321979522705
iteration 249, loss = 0.16368722915649414
iteration 250, loss = 0.1358916461467743
iteration 251, loss = 0.09549969434738159
iteration 252, loss = 0.11663515865802765
iteration 253, loss = 0.2135140746831894
iteration 254, loss = 0.1408199816942215
iteration 255, loss = 0.22668737173080444
iteration 256, loss = 0.1953594982624054
iteration 257, loss = 0.36899709701538086
iteration 258, loss = 0.30415502190589905
iteration 259, loss = 0.0822853296995163
iteration 260, loss = 0.11468635499477386
iteration 261, loss = 0.27634358406066895
iteration 262, loss = 0.15030434727668762
iteration 263, loss = 0.24003002047538757
iteration 264, loss = 0.30437877774238586
iteration 265, loss = 0.33834561705589294
iteration 266, loss = 0.12612438201904297
iteration 267, loss = 0.12718209624290466
iteration 268, loss = 0.11229255795478821
iteration 269, loss = 0.1907634437084198
iteration 270, loss = 0.4793284833431244
iteration 271, loss = 0.2538791000843048
iteration 272, loss = 0.16255053877830505
iteration 273, loss = 0.31152230501174927
iteration 274, loss = 0.18631985783576965
iteration 275, loss = 0.09657429903745651
iteration 276, loss = 0.12719511985778809
iteration 277, loss = 0.1860603392124176
iteration 278, loss = 0.18768015503883362
iteration 279, loss = 0.22540880739688873
iteration 280, loss = 0.3713715076446533
iteration 281, loss = 0.1710612177848816
iteration 282, loss = 0.23456083238124847
iteration 283, loss = 0.23704880475997925
iteration 284, loss = 0.13959193229675293
iteration 285, loss = 0.14879769086837769
iteration 286, loss = 0.12480045109987259
iteration 287, loss = 0.33025044202804565
iteration 288, loss = 0.12210547924041748
iteration 289, loss = 0.17821688950061798
iteration 290, loss = 0.22615239024162292
iteration 291, loss = 0.18417933583259583
iteration 292, loss = 0.22016412019729614
iteration 293, loss = 0.19203034043312073
iteration 294, loss = 0.18143907189369202
iteration 295, loss = 0.16568519175052643
iteration 296, loss = 0.11979727447032928
iteration 297, loss = 0.1391887664794922
iteration 298, loss = 0.3111088275909424
iteration 299, loss = 0.12540842592716217
iteration 300, loss = 0.29793843626976013
iteration 1, loss = 0.236728236079216
iteration 2, loss = 0.14822916686534882
iteration 3, loss = 0.17553992569446564
iteration 4, loss = 0.2316671907901764
iteration 5, loss = 0.19883322715759277
iteration 6, loss = 0.2507265508174896
iteration 7, loss = 0.257949560880661
iteration 8, loss = 0.32380205392837524
iteration 9, loss = 0.37743714451789856
iteration 10, loss = 0.11667707562446594
iteration 11, loss = 0.3823109269142151
iteration 12, loss = 0.2635970711708069
iteration 13, loss = 0.11348125338554382
iteration 14, loss = 0.11206512153148651
iteration 15, loss = 0.14995364844799042
iteration 16, loss = 0.26703187823295593
iteration 17, loss = 0.1557571142911911
iteration 18, loss = 0.12841323018074036
iteration 19, loss = 0.22645694017410278
iteration 20, loss = 0.27950379252433777
iteration 21, loss = 0.1391986459493637
iteration 22, loss = 0.06860513985157013
iteration 23, loss = 0.24042445421218872
iteration 24, loss = 0.18870709836483002
iteration 25, loss = 0.21377167105674744
iteration 26, loss = 0.15800872445106506
iteration 27, loss = 0.12046030163764954
iteration 28, loss = 0.2445540726184845
iteration 29, loss = 0.19347752630710602
iteration 30, loss = 0.1931302696466446
iteration 31, loss = 0.18287162482738495
iteration 32, loss = 0.18899887800216675
iteration 33, loss = 0.2330298274755478
iteration 34, loss = 0.12973859906196594
iteration 35, loss = 0.17070692777633667
iteration 36, loss = 0.23725150525569916
iteration 37, loss = 0.17820928990840912
iteration 38, loss = 0.10691972076892853
iteration 39, loss = 0.12782475352287292
iteration 40, loss = 0.37669187784194946
iteration 41, loss = 0.19329385459423065
iteration 42, loss = 0.3279167413711548
iteration 43, loss = 0.24478751420974731
iteration 44, loss = 0.2934158146381378
iteration 45, loss = 0.24941927194595337
iteration 46, loss = 0.22387415170669556
iteration 47, loss = 0.1892196089029312
iteration 48, loss = 0.1918276846408844
iteration 49, loss = 0.13746805489063263
iteration 50, loss = 0.2409578561782837
iteration 51, loss = 0.09367773681879044
iteration 52, loss = 0.13087040185928345
iteration 53, loss = 0.15376460552215576
iteration 54, loss = 0.33551734685897827
iteration 55, loss = 0.14945173263549805
iteration 56, loss = 0.10493320226669312
iteration 57, loss = 0.1035938635468483
iteration 58, loss = 0.1599627435207367
iteration 59, loss = 0.16698309779167175
iteration 60, loss = 0.1813514232635498
iteration 61, loss = 0.23843622207641602
iteration 62, loss = 0.1683170050382614
iteration 63, loss = 0.1024991124868393
iteration 64, loss = 0.13928954303264618
iteration 65, loss = 0.22180375456809998
iteration 66, loss = 0.3008863925933838
iteration 67, loss = 0.3218608498573303
iteration 68, loss = 0.13546347618103027
iteration 69, loss = 0.3001973330974579
iteration 70, loss = 0.3560963273048401
iteration 71, loss = 0.22694677114486694
iteration 72, loss = 0.2602366805076599
iteration 73, loss = 0.17231124639511108
iteration 74, loss = 0.179419606924057
iteration 75, loss = 0.10334157943725586
iteration 76, loss = 0.10401574522256851
iteration 77, loss = 0.32916438579559326
iteration 78, loss = 0.09881272166967392
iteration 79, loss = 0.2014550119638443
iteration 80, loss = 0.08866642415523529
iteration 81, loss = 0.25458404421806335
iteration 82, loss = 0.13638533651828766
iteration 83, loss = 0.11332754045724869
iteration 84, loss = 0.21452337503433228
iteration 85, loss = 0.11584007740020752
iteration 86, loss = 0.10851437598466873
iteration 87, loss = 0.1571388989686966
iteration 88, loss = 0.14091569185256958
iteration 89, loss = 0.13889923691749573
iteration 90, loss = 0.16623951494693756
iteration 91, loss = 0.1978534758090973
iteration 92, loss = 0.12309488654136658
iteration 93, loss = 0.2671069800853729
iteration 94, loss = 0.08065605163574219
iteration 95, loss = 0.14915236830711365
iteration 96, loss = 0.35791128873825073
iteration 97, loss = 0.1784152090549469
iteration 98, loss = 0.12797147035598755
iteration 99, loss = 0.15962672233581543
iteration 100, loss = 0.19197846949100494
iteration 101, loss = 0.27741187810897827
iteration 102, loss = 0.2930503487586975
iteration 103, loss = 0.23986606299877167
iteration 104, loss = 0.1979278028011322
iteration 105, loss = 0.0740080401301384
iteration 106, loss = 0.17370116710662842
iteration 107, loss = 0.28432127833366394
iteration 108, loss = 0.15544408559799194
iteration 109, loss = 0.057797424495220184
iteration 110, loss = 0.14929282665252686
iteration 111, loss = 0.31764471530914307
iteration 112, loss = 0.08309297263622284
iteration 113, loss = 0.10622383654117584
iteration 114, loss = 0.16703133285045624
iteration 115, loss = 0.1611493080854416
iteration 116, loss = 0.10594348609447479
iteration 117, loss = 0.10750771313905716
iteration 118, loss = 0.22205455601215363
iteration 119, loss = 0.15002137422561646
iteration 120, loss = 0.16411232948303223
iteration 121, loss = 0.22453267872333527
iteration 122, loss = 0.1070181131362915
iteration 123, loss = 0.31797584891319275
iteration 124, loss = 0.0958758071064949
iteration 125, loss = 0.2607266902923584
iteration 126, loss = 0.11914780735969543
iteration 127, loss = 0.2757372260093689
iteration 128, loss = 0.07196424156427383
iteration 129, loss = 0.24616798758506775
iteration 130, loss = 0.2889724373817444
iteration 131, loss = 0.19441023468971252
iteration 132, loss = 0.09086304157972336
iteration 133, loss = 0.2023511379957199
iteration 134, loss = 0.21945235133171082
iteration 135, loss = 0.08874177932739258
iteration 136, loss = 0.1014191135764122
iteration 137, loss = 0.26134514808654785
iteration 138, loss = 0.12353722006082535
iteration 139, loss = 0.06202954426407814
iteration 140, loss = 0.20556898415088654
iteration 141, loss = 0.20404689013957977
iteration 142, loss = 0.23628586530685425
iteration 143, loss = 0.1871887445449829
iteration 144, loss = 0.19156460464000702
iteration 145, loss = 0.19182245433330536
iteration 146, loss = 0.08239398896694183
iteration 147, loss = 0.1314525306224823
iteration 148, loss = 0.17745652794837952
iteration 149, loss = 0.15396057069301605
iteration 150, loss = 0.13987545669078827
iteration 151, loss = 0.12573757767677307
iteration 152, loss = 0.17839771509170532
iteration 153, loss = 0.11409243941307068
iteration 154, loss = 0.23385661840438843
iteration 155, loss = 0.23644687235355377
iteration 156, loss = 0.08593236654996872
iteration 157, loss = 0.4205448627471924
iteration 158, loss = 0.09435535967350006
iteration 159, loss = 0.18257364630699158
iteration 160, loss = 0.2506394684314728
iteration 161, loss = 0.24798965454101562
iteration 162, loss = 0.24993115663528442
iteration 163, loss = 0.09422312676906586
iteration 164, loss = 0.06859175115823746
iteration 165, loss = 0.11460845917463303
iteration 166, loss = 0.13569265604019165
iteration 167, loss = 0.23138947784900665
iteration 168, loss = 0.26298171281814575
iteration 169, loss = 0.2929404377937317
iteration 170, loss = 0.10707375407218933
iteration 171, loss = 0.34487155079841614
iteration 172, loss = 0.07792473584413528
iteration 173, loss = 0.108180470764637
iteration 174, loss = 0.37498754262924194
iteration 175, loss = 0.12281259149312973
iteration 176, loss = 0.25218844413757324
iteration 177, loss = 0.11641836166381836
iteration 178, loss = 0.42144322395324707
iteration 179, loss = 0.22373196482658386
iteration 180, loss = 0.274352490901947
iteration 181, loss = 0.2318636178970337
iteration 182, loss = 0.23746876418590546
iteration 183, loss = 0.1196802631020546
iteration 184, loss = 0.27291446924209595
iteration 185, loss = 0.21490198373794556
iteration 186, loss = 0.2287357747554779
iteration 187, loss = 0.13672137260437012
iteration 188, loss = 0.2343434989452362
iteration 189, loss = 0.1599704772233963
iteration 190, loss = 0.1459401547908783
iteration 191, loss = 0.38860830664634705
iteration 192, loss = 0.2332535684108734
iteration 193, loss = 0.1019444614648819
iteration 194, loss = 0.2205486297607422
iteration 195, loss = 0.20753741264343262
iteration 196, loss = 0.08377831429243088
iteration 197, loss = 0.1257113814353943
iteration 198, loss = 0.22192677855491638
iteration 199, loss = 0.15748678147792816
iteration 200, loss = 0.33038008213043213
iteration 201, loss = 0.20083780586719513
iteration 202, loss = 0.2485547661781311
iteration 203, loss = 0.17275437712669373
iteration 204, loss = 0.24360531568527222
iteration 205, loss = 0.13054782152175903
iteration 206, loss = 0.08014091104269028
iteration 207, loss = 0.11015634983778
iteration 208, loss = 0.38660159707069397
iteration 209, loss = 0.26788780093193054
iteration 210, loss = 0.3076515197753906
iteration 211, loss = 0.2358466237783432
iteration 212, loss = 0.25402724742889404
iteration 213, loss = 0.20244108140468597
iteration 214, loss = 0.0760829821228981
iteration 215, loss = 0.06930294632911682
iteration 216, loss = 0.16994279623031616
iteration 217, loss = 0.4200051426887512
iteration 218, loss = 0.2370430827140808
iteration 219, loss = 0.2746018171310425
iteration 220, loss = 0.12801149487495422
iteration 221, loss = 0.2865417003631592
iteration 222, loss = 0.1595216989517212
iteration 223, loss = 0.1943521350622177
iteration 224, loss = 0.1450541615486145
iteration 225, loss = 0.12681549787521362
iteration 226, loss = 0.14430123567581177
iteration 227, loss = 0.3136831521987915
iteration 228, loss = 0.22385627031326294
iteration 229, loss = 0.10159380733966827
iteration 230, loss = 0.07360327243804932
iteration 231, loss = 0.31342044472694397
iteration 232, loss = 0.15767599642276764
iteration 233, loss = 0.22059568762779236
iteration 234, loss = 0.1786525994539261
iteration 235, loss = 0.2451765090227127
iteration 236, loss = 0.12169118225574493
iteration 237, loss = 0.19277839362621307
iteration 238, loss = 0.19491182267665863
iteration 239, loss = 0.136929452419281
iteration 240, loss = 0.11370208859443665
iteration 241, loss = 0.1983417421579361
iteration 242, loss = 0.11237829178571701
iteration 243, loss = 0.13772514462471008
iteration 244, loss = 0.16964085400104523
iteration 245, loss = 0.10047492384910583
iteration 246, loss = 0.1075986921787262
iteration 247, loss = 0.2807425558567047
iteration 248, loss = 0.1301194429397583
iteration 249, loss = 0.27024751901626587
iteration 250, loss = 0.20097997784614563
iteration 251, loss = 0.1486518234014511
iteration 252, loss = 0.18080087006092072
iteration 253, loss = 0.18174633383750916
iteration 254, loss = 0.12131693959236145
iteration 255, loss = 0.17084798216819763
iteration 256, loss = 0.21732428669929504
iteration 257, loss = 0.15477398037910461
iteration 258, loss = 0.10754963755607605
iteration 259, loss = 0.4240846633911133
iteration 260, loss = 0.1024022251367569
iteration 261, loss = 0.2768159508705139
iteration 262, loss = 0.19129326939582825
iteration 263, loss = 0.21853700280189514
iteration 264, loss = 0.19220878183841705
iteration 265, loss = 0.23235884308815002
iteration 266, loss = 0.1745738834142685
iteration 267, loss = 0.2994289994239807
iteration 268, loss = 0.11337921023368835
iteration 269, loss = 0.15584994852542877
iteration 270, loss = 0.10238228738307953
iteration 271, loss = 0.377252459526062
iteration 272, loss = 0.10450515151023865
iteration 273, loss = 0.17766299843788147
iteration 274, loss = 0.18337726593017578
iteration 275, loss = 0.26557666063308716
iteration 276, loss = 0.3215218186378479
iteration 277, loss = 0.25136831402778625
iteration 278, loss = 0.1760646104812622
iteration 279, loss = 0.21419641375541687
iteration 280, loss = 0.07774152606725693
iteration 281, loss = 0.1825287938117981
iteration 282, loss = 0.17767885327339172
iteration 283, loss = 0.194472536444664
iteration 284, loss = 0.29942652583122253
iteration 285, loss = 0.2552672028541565
iteration 286, loss = 0.16623808443546295
iteration 287, loss = 0.08072418719530106
iteration 288, loss = 0.16448312997817993
iteration 289, loss = 0.09409142285585403
iteration 290, loss = 0.2571127414703369
iteration 291, loss = 0.14567197859287262
iteration 292, loss = 0.19989874958992004
iteration 293, loss = 0.11270146071910858
iteration 294, loss = 0.2658070921897888
iteration 295, loss = 0.16818733513355255
iteration 296, loss = 0.2730281352996826
iteration 297, loss = 0.19471000134944916
iteration 298, loss = 0.10781915485858917
iteration 299, loss = 0.10497412085533142
iteration 300, loss = 0.14748282730579376
iteration 1, loss = 0.11349481344223022
iteration 2, loss = 0.1234939694404602
iteration 3, loss = 0.33505603671073914
iteration 4, loss = 0.3078870475292206
iteration 5, loss = 0.26398956775665283
iteration 6, loss = 0.4017654061317444
iteration 7, loss = 0.22722914814949036
iteration 8, loss = 0.13754957914352417
iteration 9, loss = 0.24323280155658722
iteration 10, loss = 0.13959616422653198
iteration 11, loss = 0.16278110444545746
iteration 12, loss = 0.2513487637042999
iteration 13, loss = 0.13399210572242737
iteration 14, loss = 0.13651137053966522
iteration 15, loss = 0.1114029735326767
iteration 16, loss = 0.19452406466007233
iteration 17, loss = 0.21588899195194244
iteration 18, loss = 0.10203756392002106
iteration 19, loss = 0.26265522837638855
iteration 20, loss = 0.2062283456325531
iteration 21, loss = 0.2531924545764923
iteration 22, loss = 0.24276486039161682
iteration 23, loss = 0.09740282595157623
iteration 24, loss = 0.10530358552932739
iteration 25, loss = 0.15719270706176758
iteration 26, loss = 0.23531025648117065
iteration 27, loss = 0.1085541695356369
iteration 28, loss = 0.22032345831394196
iteration 29, loss = 0.2057611346244812
iteration 30, loss = 0.12046930938959122
iteration 31, loss = 0.11855156719684601
iteration 32, loss = 0.16302835941314697
iteration 33, loss = 0.2181875854730606
iteration 34, loss = 0.18290701508522034
iteration 35, loss = 0.19140861928462982
iteration 36, loss = 0.13942863047122955
iteration 37, loss = 0.17713141441345215
iteration 38, loss = 0.09491567313671112
iteration 39, loss = 0.1553644835948944
iteration 40, loss = 0.13210730254650116
iteration 41, loss = 0.263122022151947
iteration 42, loss = 0.2952384948730469
iteration 43, loss = 0.364627480506897
iteration 44, loss = 0.3290880024433136
iteration 45, loss = 0.18094360828399658
iteration 46, loss = 0.16798537969589233
iteration 47, loss = 0.16342484951019287
iteration 48, loss = 0.34119001030921936
iteration 49, loss = 0.09333276748657227
iteration 50, loss = 0.12241432815790176
iteration 51, loss = 0.2631470263004303
iteration 52, loss = 0.1458394080400467
iteration 53, loss = 0.13569122552871704
iteration 54, loss = 0.2186926156282425
iteration 55, loss = 0.10209062695503235
iteration 56, loss = 0.24758899211883545
iteration 57, loss = 0.2429361343383789
iteration 58, loss = 0.07070841640233994
iteration 59, loss = 0.11242885887622833
iteration 60, loss = 0.08990218490362167
iteration 61, loss = 0.16263046860694885
iteration 62, loss = 0.12895791232585907
iteration 63, loss = 0.14292028546333313
iteration 64, loss = 0.13035213947296143
iteration 65, loss = 0.15407629311084747
iteration 66, loss = 0.5127451419830322
iteration 67, loss = 0.15130101144313812
iteration 68, loss = 0.17720216512680054
iteration 69, loss = 0.36527395248413086
iteration 70, loss = 0.11061221361160278
iteration 71, loss = 0.4259893596172333
iteration 72, loss = 0.3293747305870056
iteration 73, loss = 0.17358547449111938
iteration 74, loss = 0.36530807614326477
iteration 75, loss = 0.24456465244293213
iteration 76, loss = 0.25641930103302
iteration 77, loss = 0.2234354168176651
iteration 78, loss = 0.1315011829137802
iteration 79, loss = 0.18376556038856506
iteration 80, loss = 0.21030068397521973
iteration 81, loss = 0.1423889547586441
iteration 82, loss = 0.12847870588302612
iteration 83, loss = 0.0972379595041275
iteration 84, loss = 0.11791185289621353
iteration 85, loss = 0.16151118278503418
iteration 86, loss = 0.14953231811523438
iteration 87, loss = 0.14234401285648346
iteration 88, loss = 0.15507793426513672
iteration 89, loss = 0.33426332473754883
iteration 90, loss = 0.12255940586328506
iteration 91, loss = 0.10780713707208633
iteration 92, loss = 0.10966220498085022
iteration 93, loss = 0.1271463930606842
iteration 94, loss = 0.17772071063518524
iteration 95, loss = 0.28124141693115234
iteration 96, loss = 0.2732623219490051
iteration 97, loss = 0.1715182214975357
iteration 98, loss = 0.13441774249076843
iteration 99, loss = 0.12348882853984833
iteration 100, loss = 0.12100668251514435
iteration 101, loss = 0.11526554077863693
iteration 102, loss = 0.07397076487541199
iteration 103, loss = 0.2052973061800003
iteration 104, loss = 0.172855406999588
iteration 105, loss = 0.42554423213005066
iteration 106, loss = 0.19946318864822388
iteration 107, loss = 0.14835426211357117
iteration 108, loss = 0.18807373940944672
iteration 109, loss = 0.10654374212026596
iteration 110, loss = 0.14285561442375183
iteration 111, loss = 0.1957443803548813
iteration 112, loss = 0.20114026963710785
iteration 113, loss = 0.285341739654541
iteration 114, loss = 0.20665037631988525
iteration 115, loss = 0.1330844908952713
iteration 116, loss = 0.14692415297031403
iteration 117, loss = 0.13471196591854095
iteration 118, loss = 0.19121316075325012
iteration 119, loss = 0.22923943400382996
iteration 120, loss = 0.17029143869876862
iteration 121, loss = 0.07984640449285507
iteration 122, loss = 0.2574923038482666
iteration 123, loss = 0.25980907678604126
iteration 124, loss = 0.23098401725292206
iteration 125, loss = 0.1665417104959488
iteration 126, loss = 0.2039840817451477
iteration 127, loss = 0.09219230711460114
iteration 128, loss = 0.10462045669555664
iteration 129, loss = 0.28117668628692627
iteration 130, loss = 0.2581290304660797
iteration 131, loss = 0.15424521267414093
iteration 132, loss = 0.1510331928730011
iteration 133, loss = 0.14364594221115112
iteration 134, loss = 0.1809837967157364
iteration 135, loss = 0.10197266936302185
iteration 136, loss = 0.257830411195755
iteration 137, loss = 0.0808521956205368
iteration 138, loss = 0.26836836338043213
iteration 139, loss = 0.26020774245262146
iteration 140, loss = 0.11724330484867096
iteration 141, loss = 0.23987050354480743
iteration 142, loss = 0.2298237681388855
iteration 143, loss = 0.07523998618125916
iteration 144, loss = 0.21231359243392944
iteration 145, loss = 0.19397443532943726
iteration 146, loss = 0.27036386728286743
iteration 147, loss = 0.25902247428894043
iteration 148, loss = 0.2523230314254761
iteration 149, loss = 0.3261522054672241
iteration 150, loss = 0.30406853556632996
iteration 151, loss = 0.21151985228061676
iteration 152, loss = 0.23284779489040375
iteration 153, loss = 0.16146716475486755
iteration 154, loss = 0.1650250107049942
iteration 155, loss = 0.11335340142250061
iteration 156, loss = 0.07103373855352402
iteration 157, loss = 0.14274166524410248
iteration 158, loss = 0.07545752823352814
iteration 159, loss = 0.1414431631565094
iteration 160, loss = 0.13899092376232147
iteration 161, loss = 0.14447593688964844
iteration 162, loss = 0.1815180480480194
iteration 163, loss = 0.2165907770395279
iteration 164, loss = 0.17941290140151978
iteration 165, loss = 0.10342896729707718
iteration 166, loss = 0.3837938904762268
iteration 167, loss = 0.1313323676586151
iteration 168, loss = 0.08825745433568954
iteration 169, loss = 0.12883223593235016
iteration 170, loss = 0.2389429211616516
iteration 171, loss = 0.0900406688451767
iteration 172, loss = 0.2019316852092743
iteration 173, loss = 0.08737330138683319
iteration 174, loss = 0.15055368840694427
iteration 175, loss = 0.1879783570766449
iteration 176, loss = 0.38644173741340637
iteration 177, loss = 0.1814335137605667
iteration 178, loss = 0.07499311119318008
iteration 179, loss = 0.15355950593948364
iteration 180, loss = 0.1382419764995575
iteration 181, loss = 0.15330368280410767
iteration 182, loss = 0.14273908734321594
iteration 183, loss = 0.30562588572502136
iteration 184, loss = 0.2375008463859558
iteration 185, loss = 0.24534893035888672
iteration 186, loss = 0.09330403804779053
iteration 187, loss = 0.2264353185892105
iteration 188, loss = 0.1072673499584198
iteration 189, loss = 0.17652888596057892
iteration 190, loss = 0.15155409276485443
iteration 191, loss = 0.2124147117137909
iteration 192, loss = 0.25125062465667725
iteration 193, loss = 0.12642838060855865
iteration 194, loss = 0.12820526957511902
iteration 195, loss = 0.09450283646583557
iteration 196, loss = 0.19313335418701172
iteration 197, loss = 0.13753943145275116
iteration 198, loss = 0.16511252522468567
iteration 199, loss = 0.31318700313568115
iteration 200, loss = 0.06623663008213043
iteration 201, loss = 0.224004864692688
iteration 202, loss = 0.08101259171962738
iteration 203, loss = 0.20787343382835388
iteration 204, loss = 0.24753838777542114
iteration 205, loss = 0.26907527446746826
iteration 206, loss = 0.20204299688339233
iteration 207, loss = 0.2146792709827423
iteration 208, loss = 0.12322348356246948
iteration 209, loss = 0.08071372658014297
iteration 210, loss = 0.17586180567741394
iteration 211, loss = 0.21195310354232788
iteration 212, loss = 0.17272400856018066
iteration 213, loss = 0.1278664618730545
iteration 214, loss = 0.2636728584766388
iteration 215, loss = 0.11975385248661041
iteration 216, loss = 0.2595035433769226
iteration 217, loss = 0.18649983406066895
iteration 218, loss = 0.19232887029647827
iteration 219, loss = 0.2554338574409485
iteration 220, loss = 0.13520359992980957
iteration 221, loss = 0.0810975432395935
iteration 222, loss = 0.10827267169952393
iteration 223, loss = 0.19419848918914795
iteration 224, loss = 0.22279129922389984
iteration 225, loss = 0.41331759095191956
iteration 226, loss = 0.11225573718547821
iteration 227, loss = 0.11542990803718567
iteration 228, loss = 0.10109526664018631
iteration 229, loss = 0.2733427584171295
iteration 230, loss = 0.1212974339723587
iteration 231, loss = 0.2739328145980835
iteration 232, loss = 0.20218169689178467
iteration 233, loss = 0.2617531716823578
iteration 234, loss = 0.17972354590892792
iteration 235, loss = 0.1886371374130249
iteration 236, loss = 0.11881086230278015
iteration 237, loss = 0.08588864654302597
iteration 238, loss = 0.12295415997505188
iteration 239, loss = 0.1774209439754486
iteration 240, loss = 0.12948355078697205
iteration 241, loss = 0.1601027250289917
iteration 242, loss = 0.20909768342971802
iteration 243, loss = 0.07122484594583511
iteration 244, loss = 0.2312804013490677
iteration 245, loss = 0.1084463894367218
iteration 246, loss = 0.1380237340927124
iteration 247, loss = 0.17687633633613586
iteration 248, loss = 0.22327828407287598
iteration 249, loss = 0.1321553736925125
iteration 250, loss = 0.15291059017181396
iteration 251, loss = 0.1502685844898224
iteration 252, loss = 0.07859943062067032
iteration 253, loss = 0.3260401785373688
iteration 254, loss = 0.31653881072998047
iteration 255, loss = 0.1215682402253151
iteration 256, loss = 0.5406543016433716
iteration 257, loss = 0.10244335234165192
iteration 258, loss = 0.2992948889732361
iteration 259, loss = 0.18989861011505127
iteration 260, loss = 0.11388484388589859
iteration 261, loss = 0.2335793673992157
iteration 262, loss = 0.09792150557041168
iteration 263, loss = 0.1637960970401764
iteration 264, loss = 0.14912216365337372
iteration 265, loss = 0.09509652853012085
iteration 266, loss = 0.12123459577560425
iteration 267, loss = 0.2686958909034729
iteration 268, loss = 0.12351915240287781
iteration 269, loss = 0.2631441354751587
iteration 270, loss = 0.31397104263305664
iteration 271, loss = 0.0994662344455719
iteration 272, loss = 0.4006335139274597
iteration 273, loss = 0.2312605232000351
iteration 274, loss = 0.1011912003159523
iteration 275, loss = 0.09584394842386246
iteration 276, loss = 0.1043868288397789
iteration 277, loss = 0.1952868551015854
iteration 278, loss = 0.1564427763223648
iteration 279, loss = 0.22943317890167236
iteration 280, loss = 0.12664906680583954
iteration 281, loss = 0.22705918550491333
iteration 282, loss = 0.20947198569774628
iteration 283, loss = 0.15354570746421814
iteration 284, loss = 0.10011901706457138
iteration 285, loss = 0.15395981073379517
iteration 286, loss = 0.13619732856750488
iteration 287, loss = 0.10782663524150848
iteration 288, loss = 0.13659819960594177
iteration 289, loss = 0.2080124169588089
iteration 290, loss = 0.12773558497428894
iteration 291, loss = 0.17948687076568604
iteration 292, loss = 0.3294917047023773
iteration 293, loss = 0.1594417691230774
iteration 294, loss = 0.16318096220493317
iteration 295, loss = 0.11546526104211807
iteration 296, loss = 0.17803600430488586
iteration 297, loss = 0.22217729687690735
iteration 298, loss = 0.1810430884361267
iteration 299, loss = 0.3579670488834381
iteration 300, loss = 0.22750473022460938
iteration 1, loss = 0.38912320137023926
iteration 2, loss = 0.30190080404281616
iteration 3, loss = 0.11325062811374664
iteration 4, loss = 0.09309834241867065
iteration 5, loss = 0.1683686226606369
iteration 6, loss = 0.1651538759469986
iteration 7, loss = 0.21524065732955933
iteration 8, loss = 0.12712952494621277
iteration 9, loss = 0.18897885084152222
iteration 10, loss = 0.20179441571235657
iteration 11, loss = 0.09294985234737396
iteration 12, loss = 0.08668896555900574
iteration 13, loss = 0.2270451784133911
iteration 14, loss = 0.2608422040939331
iteration 15, loss = 0.08958008885383606
iteration 16, loss = 0.12043404579162598
iteration 17, loss = 0.29700425267219543
iteration 18, loss = 0.18665900826454163
iteration 19, loss = 0.1923859417438507
iteration 20, loss = 0.19711802899837494
iteration 21, loss = 0.25884464383125305
iteration 22, loss = 0.07755012065172195
iteration 23, loss = 0.13720190525054932
iteration 24, loss = 0.24149107933044434
iteration 25, loss = 0.10332484543323517
iteration 26, loss = 0.13064667582511902
iteration 27, loss = 0.08936601877212524
iteration 28, loss = 0.11602352559566498
iteration 29, loss = 0.20135468244552612
iteration 30, loss = 0.3643672466278076
iteration 31, loss = 0.42808932065963745
iteration 32, loss = 0.3634284436702728
iteration 33, loss = 0.2894558310508728
iteration 34, loss = 0.1458323895931244
iteration 35, loss = 0.2158299684524536
iteration 36, loss = 0.23995675146579742
iteration 37, loss = 0.15264613926410675
iteration 38, loss = 0.24605149030685425
iteration 39, loss = 0.11374756693840027
iteration 40, loss = 0.16760283708572388
iteration 41, loss = 0.348436176776886
iteration 42, loss = 0.3909107446670532
iteration 43, loss = 0.10327450931072235
iteration 44, loss = 0.139757439494133
iteration 45, loss = 0.3393632173538208
iteration 46, loss = 0.09069008380174637
iteration 47, loss = 0.23502397537231445
iteration 48, loss = 0.09808539599180222
iteration 49, loss = 0.08070304989814758
iteration 50, loss = 0.12807507812976837
iteration 51, loss = 0.1047661304473877
iteration 52, loss = 0.39691832661628723
iteration 53, loss = 0.15684281289577484
iteration 54, loss = 0.14106221497058868
iteration 55, loss = 0.24761691689491272
iteration 56, loss = 0.1901901215314865
iteration 57, loss = 0.20303195714950562
iteration 58, loss = 0.14728277921676636
iteration 59, loss = 0.20342382788658142
iteration 60, loss = 0.14134395122528076
iteration 61, loss = 0.2384088933467865
iteration 62, loss = 0.36965975165367126
iteration 63, loss = 0.09421397745609283
iteration 64, loss = 0.31122827529907227
iteration 65, loss = 0.18725337088108063
iteration 66, loss = 0.19830970466136932
iteration 67, loss = 0.20487742125988007
iteration 68, loss = 0.08207109570503235
iteration 69, loss = 0.12678802013397217
iteration 70, loss = 0.13214051723480225
iteration 71, loss = 0.18491172790527344
iteration 72, loss = 0.11229036003351212
iteration 73, loss = 0.3379793167114258
iteration 74, loss = 0.14905041456222534
iteration 75, loss = 0.14677675068378448
iteration 76, loss = 0.1323392242193222
iteration 77, loss = 0.13392053544521332
iteration 78, loss = 0.12331604957580566
iteration 79, loss = 0.131994366645813
iteration 80, loss = 0.10263466089963913
iteration 81, loss = 0.10499271005392075
iteration 82, loss = 0.11388888955116272
iteration 83, loss = 0.22443264722824097
iteration 84, loss = 0.2087281048297882
iteration 85, loss = 0.11948910355567932
iteration 86, loss = 0.1401212364435196
iteration 87, loss = 0.17851287126541138
iteration 88, loss = 0.13331720232963562
iteration 89, loss = 0.11636877804994583
iteration 90, loss = 0.10049505531787872
iteration 91, loss = 0.08738186955451965
iteration 92, loss = 0.07858119904994965
iteration 93, loss = 0.10554755479097366
iteration 94, loss = 0.2082284390926361
iteration 95, loss = 0.18677620589733124
iteration 96, loss = 0.24174995720386505
iteration 97, loss = 0.11311885714530945
iteration 98, loss = 0.2617391049861908
iteration 99, loss = 0.18015077710151672
iteration 100, loss = 0.06411764025688171
iteration 101, loss = 0.19989070296287537
iteration 102, loss = 0.2357649952173233
iteration 103, loss = 0.14156705141067505
iteration 104, loss = 0.2314131259918213
iteration 105, loss = 0.27376672625541687
iteration 106, loss = 0.13460612297058105
iteration 107, loss = 0.22881019115447998
iteration 108, loss = 0.12887075543403625
iteration 109, loss = 0.20604965090751648
iteration 110, loss = 0.18508486449718475
iteration 111, loss = 0.20573195815086365
iteration 112, loss = 0.12025055289268494
iteration 113, loss = 0.11094605922698975
iteration 114, loss = 0.22127574682235718
iteration 115, loss = 0.11818757653236389
iteration 116, loss = 0.2517105042934418
iteration 117, loss = 0.22506015002727509
iteration 118, loss = 0.18366104364395142
iteration 119, loss = 0.13670316338539124
iteration 120, loss = 0.06950146704912186
iteration 121, loss = 0.2726312279701233
iteration 122, loss = 0.21214649081230164
iteration 123, loss = 0.34256306290626526
iteration 124, loss = 0.2662731409072876
iteration 125, loss = 0.1592993140220642
iteration 126, loss = 0.13954980671405792
iteration 127, loss = 0.19397714734077454
iteration 128, loss = 0.1206556111574173
iteration 129, loss = 0.24385513365268707
iteration 130, loss = 0.2657310366630554
iteration 131, loss = 0.1430494636297226
iteration 132, loss = 0.32648205757141113
iteration 133, loss = 0.0842435210943222
iteration 134, loss = 0.14191393554210663
iteration 135, loss = 0.2805079221725464
iteration 136, loss = 0.1391299068927765
iteration 137, loss = 0.12927955389022827
iteration 138, loss = 0.14121906459331512
iteration 139, loss = 0.230417400598526
iteration 140, loss = 0.13001146912574768
iteration 141, loss = 0.12858189642429352
iteration 142, loss = 0.334067165851593
iteration 143, loss = 0.052796777337789536
iteration 144, loss = 0.1970747709274292
iteration 145, loss = 0.15394508838653564
iteration 146, loss = 0.18109957873821259
iteration 147, loss = 0.2160097062587738
iteration 148, loss = 0.11996928602457047
iteration 149, loss = 0.10351379960775375
iteration 150, loss = 0.1312112957239151
iteration 151, loss = 0.20754238963127136
iteration 152, loss = 0.06666218489408493
iteration 153, loss = 0.2344750165939331
iteration 154, loss = 0.27992165088653564
iteration 155, loss = 0.3898707926273346
iteration 156, loss = 0.11283306777477264
iteration 157, loss = 0.17033937573432922
iteration 158, loss = 0.4008655250072479
iteration 159, loss = 0.1230769008398056
iteration 160, loss = 0.22518765926361084
iteration 161, loss = 0.0958249568939209
iteration 162, loss = 0.17424780130386353
iteration 163, loss = 0.1927768886089325
iteration 164, loss = 0.13559652864933014
iteration 165, loss = 0.159787118434906
iteration 166, loss = 0.10468539595603943
iteration 167, loss = 0.3180028796195984
iteration 168, loss = 0.20433303713798523
iteration 169, loss = 0.16919934749603271
iteration 170, loss = 0.19815151393413544
iteration 171, loss = 0.062302906066179276
iteration 172, loss = 0.24103286862373352
iteration 173, loss = 0.2044355273246765
iteration 174, loss = 0.2961125671863556
iteration 175, loss = 0.12650825083255768
iteration 176, loss = 0.1131180077791214
iteration 177, loss = 0.14065630733966827
iteration 178, loss = 0.096213199198246
iteration 179, loss = 0.0996210128068924
iteration 180, loss = 0.15598785877227783
iteration 181, loss = 0.11385323852300644
iteration 182, loss = 0.2653900980949402
iteration 183, loss = 0.09059064090251923
iteration 184, loss = 0.3312109112739563
iteration 185, loss = 0.25283849239349365
iteration 186, loss = 0.07518573105335236
iteration 187, loss = 0.17155605554580688
iteration 188, loss = 0.06554209440946579
iteration 189, loss = 0.21589355170726776
iteration 190, loss = 0.13634885847568512
iteration 191, loss = 0.40626639127731323
iteration 192, loss = 0.09707978367805481
iteration 193, loss = 0.11304772645235062
iteration 194, loss = 0.12918424606323242
iteration 195, loss = 0.334012895822525
iteration 196, loss = 0.41911056637763977
iteration 197, loss = 0.1252889335155487
iteration 198, loss = 0.15685473382472992
iteration 199, loss = 0.28723838925361633
iteration 200, loss = 0.1653146892786026
iteration 201, loss = 0.2673985958099365
iteration 202, loss = 0.10039588809013367
iteration 203, loss = 0.07716462016105652
iteration 204, loss = 0.1385897547006607
iteration 205, loss = 0.25954583287239075
iteration 206, loss = 0.24638360738754272
iteration 207, loss = 0.12219099700450897
iteration 208, loss = 0.11119591444730759
iteration 209, loss = 0.11091088503599167
iteration 210, loss = 0.22573325037956238
iteration 211, loss = 0.24885407090187073
iteration 212, loss = 0.09931335598230362
iteration 213, loss = 0.09876836836338043
iteration 214, loss = 0.10141074657440186
iteration 215, loss = 0.20460093021392822
iteration 216, loss = 0.17146389186382294
iteration 217, loss = 0.11985687911510468
iteration 218, loss = 0.24650830030441284
iteration 219, loss = 0.15402908623218536
iteration 220, loss = 0.12885932624340057
iteration 221, loss = 0.20022770762443542
iteration 222, loss = 0.15472914278507233
iteration 223, loss = 0.13138854503631592
iteration 224, loss = 0.07874542474746704
iteration 225, loss = 0.14546404778957367
iteration 226, loss = 0.23846609890460968
iteration 227, loss = 0.19641974568367004
iteration 228, loss = 0.3490394353866577
iteration 229, loss = 0.14189444482326508
iteration 230, loss = 0.15967503190040588
iteration 231, loss = 0.23555532097816467
iteration 232, loss = 0.1011580228805542
iteration 233, loss = 0.0875387191772461
iteration 234, loss = 0.07425045967102051
iteration 235, loss = 0.16204307973384857
iteration 236, loss = 0.2013738453388214
iteration 237, loss = 0.1507156491279602
iteration 238, loss = 0.19503292441368103
iteration 239, loss = 0.06709848344326019
iteration 240, loss = 0.2693466544151306
iteration 241, loss = 0.060720864683389664
iteration 242, loss = 0.23803827166557312
iteration 243, loss = 0.06808631122112274
iteration 244, loss = 0.05886112526059151
iteration 245, loss = 0.11437512934207916
iteration 246, loss = 0.20822975039482117
iteration 247, loss = 0.06810273975133896
iteration 248, loss = 0.18522009253501892
iteration 249, loss = 0.22267989814281464
iteration 250, loss = 0.2450558841228485
iteration 251, loss = 0.07395381480455399
iteration 252, loss = 0.10444474220275879
iteration 253, loss = 0.08202352374792099
iteration 254, loss = 0.19983293116092682
iteration 255, loss = 0.16454946994781494
iteration 256, loss = 0.16581258177757263
iteration 257, loss = 0.23961980640888214
iteration 258, loss = 0.18476006388664246
iteration 259, loss = 0.15414194762706757
iteration 260, loss = 0.09219971299171448
iteration 261, loss = 0.18605947494506836
iteration 262, loss = 0.1412578970193863
iteration 263, loss = 0.12183549255132675
iteration 264, loss = 0.13250002264976501
iteration 265, loss = 0.39277487993240356
iteration 266, loss = 0.21196404099464417
iteration 267, loss = 0.30610978603363037
iteration 268, loss = 0.32997220754623413
iteration 269, loss = 0.08710519224405289
iteration 270, loss = 0.15844003856182098
iteration 271, loss = 0.13442818820476532
iteration 272, loss = 0.1349671483039856
iteration 273, loss = 0.2543482184410095
iteration 274, loss = 0.09264285117387772
iteration 275, loss = 0.21332460641860962
iteration 276, loss = 0.49390578269958496
iteration 277, loss = 0.15441247820854187
iteration 278, loss = 0.20062395930290222
iteration 279, loss = 0.1211899071931839
iteration 280, loss = 0.11314786970615387
iteration 281, loss = 0.12358024716377258
iteration 282, loss = 0.1548069268465042
iteration 283, loss = 0.16141140460968018
iteration 284, loss = 0.12665536999702454
iteration 285, loss = 0.2428084909915924
iteration 286, loss = 0.20445051789283752
iteration 287, loss = 0.16612300276756287
iteration 288, loss = 0.09388206899166107
iteration 289, loss = 0.281276136636734
iteration 290, loss = 0.09391054511070251
iteration 291, loss = 0.17729032039642334
iteration 292, loss = 0.10804479569196701
iteration 293, loss = 0.2421581745147705
iteration 294, loss = 0.20938542485237122
iteration 295, loss = 0.24474811553955078
iteration 296, loss = 0.22468799352645874
iteration 297, loss = 0.16813571751117706
iteration 298, loss = 0.1164582297205925
iteration 299, loss = 0.12393879890441895
iteration 300, loss = 0.3138028383255005
iteration 1, loss = 0.08131946623325348
iteration 2, loss = 0.2168283462524414
iteration 3, loss = 0.20562727749347687
iteration 4, loss = 0.1301743984222412
iteration 5, loss = 0.12935717403888702
iteration 6, loss = 0.13836060464382172
iteration 7, loss = 0.24799492955207825
iteration 8, loss = 0.25258147716522217
iteration 9, loss = 0.17317381501197815
iteration 10, loss = 0.13850808143615723
iteration 11, loss = 0.2233809530735016
iteration 12, loss = 0.12971042096614838
iteration 13, loss = 0.1853504180908203
iteration 14, loss = 0.05292467772960663
iteration 15, loss = 0.14072301983833313
iteration 16, loss = 0.154699444770813
iteration 17, loss = 0.09420856833457947
iteration 18, loss = 0.1399281620979309
iteration 19, loss = 0.18199166655540466
iteration 20, loss = 0.23193669319152832
iteration 21, loss = 0.2835915982723236
iteration 22, loss = 0.30513274669647217
iteration 23, loss = 0.20002740621566772
iteration 24, loss = 0.3302425742149353
iteration 25, loss = 0.13046640157699585
iteration 26, loss = 0.13435591757297516
iteration 27, loss = 0.11699597537517548
iteration 28, loss = 0.12221496552228928
iteration 29, loss = 0.22507597506046295
iteration 30, loss = 0.1030801385641098
iteration 31, loss = 0.06313589960336685
iteration 32, loss = 0.09224426746368408
iteration 33, loss = 0.2523447573184967
iteration 34, loss = 0.21635407209396362
iteration 35, loss = 0.10948340594768524
iteration 36, loss = 0.27071359753608704
iteration 37, loss = 0.0718386322259903
iteration 38, loss = 0.21079415082931519
iteration 39, loss = 0.09812316298484802
iteration 40, loss = 0.2636663019657135
iteration 41, loss = 0.0823402851819992
iteration 42, loss = 0.08620186150074005
iteration 43, loss = 0.16282901167869568
iteration 44, loss = 0.2511724829673767
iteration 45, loss = 0.2670552432537079
iteration 46, loss = 0.19267567992210388
iteration 47, loss = 0.10921648144721985
iteration 48, loss = 0.14213623106479645
iteration 49, loss = 0.09279605746269226
iteration 50, loss = 0.12266655266284943
iteration 51, loss = 0.0917389914393425
iteration 52, loss = 0.16680285334587097
iteration 53, loss = 0.19504910707473755
iteration 54, loss = 0.10201305150985718
iteration 55, loss = 0.10421492159366608
iteration 56, loss = 0.06447144597768784
iteration 57, loss = 0.23926712572574615
iteration 58, loss = 0.12304627150297165
iteration 59, loss = 0.11399252712726593
iteration 60, loss = 0.14278410375118256
iteration 61, loss = 0.26231926679611206
iteration 62, loss = 0.11706052720546722
iteration 63, loss = 0.17975816130638123
iteration 64, loss = 0.3300930857658386
iteration 65, loss = 0.28963083028793335
iteration 66, loss = 0.1362679898738861
iteration 67, loss = 0.22145722806453705
iteration 68, loss = 0.19861064851284027
iteration 69, loss = 0.2205236852169037
iteration 70, loss = 0.21987928450107574
iteration 71, loss = 0.1944788247346878
iteration 72, loss = 0.1856359839439392
iteration 73, loss = 0.1508418172597885
iteration 74, loss = 0.277997761964798
iteration 75, loss = 0.25690487027168274
iteration 76, loss = 0.11460178345441818
iteration 77, loss = 0.0738009363412857
iteration 78, loss = 0.09567499905824661
iteration 79, loss = 0.0848744735121727
iteration 80, loss = 0.09984271228313446
iteration 81, loss = 0.3688357472419739
iteration 82, loss = 0.11207497864961624
iteration 83, loss = 0.09318126738071442
iteration 84, loss = 0.1266978681087494
iteration 85, loss = 0.22480744123458862
iteration 86, loss = 0.13648957014083862
iteration 87, loss = 0.11091618239879608
iteration 88, loss = 0.4120873510837555
iteration 89, loss = 0.08036398887634277
iteration 90, loss = 0.09691893309354782
iteration 91, loss = 0.19520367681980133
iteration 92, loss = 0.16782109439373016
iteration 93, loss = 0.20008477568626404
iteration 94, loss = 0.14798800647258759
iteration 95, loss = 0.1446327269077301
iteration 96, loss = 0.19250960648059845
iteration 97, loss = 0.22888055443763733
iteration 98, loss = 0.09413562715053558
iteration 99, loss = 0.1292262226343155
iteration 100, loss = 0.19068001210689545
iteration 101, loss = 0.06889240443706512
iteration 102, loss = 0.17026883363723755
iteration 103, loss = 0.14973406493663788
iteration 104, loss = 0.14646361768245697
iteration 105, loss = 0.21937961876392365
iteration 106, loss = 0.19013898074626923
iteration 107, loss = 0.24879196286201477
iteration 108, loss = 0.14000219106674194
iteration 109, loss = 0.07090063393115997
iteration 110, loss = 0.17416852712631226
iteration 111, loss = 0.1052393913269043
iteration 112, loss = 0.11316382884979248
iteration 113, loss = 0.42269834876060486
iteration 114, loss = 0.18679934740066528
iteration 115, loss = 0.08173844963312149
iteration 116, loss = 0.1796063631772995
iteration 117, loss = 0.08911807835102081
iteration 118, loss = 0.16693858802318573
iteration 119, loss = 0.18824173510074615
iteration 120, loss = 0.25872334837913513
iteration 121, loss = 0.16688507795333862
iteration 122, loss = 0.24852652847766876
iteration 123, loss = 0.13300299644470215
iteration 124, loss = 0.32971933484077454
iteration 125, loss = 0.23898884654045105
iteration 126, loss = 0.20743702352046967
iteration 127, loss = 0.1511646807193756
iteration 128, loss = 0.12419354915618896
iteration 129, loss = 0.17781101167201996
iteration 130, loss = 0.2539440393447876
iteration 131, loss = 0.18292266130447388
iteration 132, loss = 0.2990381717681885
iteration 133, loss = 0.21217571198940277
iteration 134, loss = 0.14040839672088623
iteration 135, loss = 0.1287573277950287
iteration 136, loss = 0.10009028017520905
iteration 137, loss = 0.21249350905418396
iteration 138, loss = 0.13404056429862976
iteration 139, loss = 0.26501670479774475
iteration 140, loss = 0.2761496305465698
iteration 141, loss = 0.13735400140285492
iteration 142, loss = 0.07524817436933517
iteration 143, loss = 0.20356614887714386
iteration 144, loss = 0.08647970855236053
iteration 145, loss = 0.06664843112230301
iteration 146, loss = 0.17406469583511353
iteration 147, loss = 0.4448716342449188
iteration 148, loss = 0.09839420765638351
iteration 149, loss = 0.2237992137670517
iteration 150, loss = 0.2504180669784546
iteration 151, loss = 0.112098827958107
iteration 152, loss = 0.2655709385871887
iteration 153, loss = 0.07481895387172699
iteration 154, loss = 0.09332342445850372
iteration 155, loss = 0.3113752007484436
iteration 156, loss = 0.27695658802986145
iteration 157, loss = 0.20378237962722778
iteration 158, loss = 0.08873540163040161
iteration 159, loss = 0.09819537401199341
iteration 160, loss = 0.2593420147895813
iteration 161, loss = 0.10384561121463776
iteration 162, loss = 0.14545299112796783
iteration 163, loss = 0.21404710412025452
iteration 164, loss = 0.13892404735088348
iteration 165, loss = 0.3046663999557495
iteration 166, loss = 0.10895229876041412
iteration 167, loss = 0.17296119034290314
iteration 168, loss = 0.2506255507469177
iteration 169, loss = 0.10738983750343323
iteration 170, loss = 0.08855249732732773
iteration 171, loss = 0.1587003767490387
iteration 172, loss = 0.20387767255306244
iteration 173, loss = 0.1306125372648239
iteration 174, loss = 0.23625658452510834
iteration 175, loss = 0.12257716059684753
iteration 176, loss = 0.37635165452957153
iteration 177, loss = 0.21268847584724426
iteration 178, loss = 0.30509278178215027
iteration 179, loss = 0.09134158492088318
iteration 180, loss = 0.2803133726119995
iteration 181, loss = 0.20368504524230957
iteration 182, loss = 0.2821056544780731
iteration 183, loss = 0.1785685420036316
iteration 184, loss = 0.1884801983833313
iteration 185, loss = 0.07173366099596024
iteration 186, loss = 0.0819229707121849
iteration 187, loss = 0.3455889821052551
iteration 188, loss = 0.15676012635231018
iteration 189, loss = 0.17914527654647827
iteration 190, loss = 0.1552894413471222
iteration 191, loss = 0.21985863149166107
iteration 192, loss = 0.22979560494422913
iteration 193, loss = 0.11929470300674438
iteration 194, loss = 0.1954510658979416
iteration 195, loss = 0.18461821973323822
iteration 196, loss = 0.10892007499933243
iteration 197, loss = 0.179241344332695
iteration 198, loss = 0.2121753692626953
iteration 199, loss = 0.12305888533592224
iteration 200, loss = 0.10924239456653595
iteration 201, loss = 0.10537640750408173
iteration 202, loss = 0.15560992062091827
iteration 203, loss = 0.18692883849143982
iteration 204, loss = 0.12086688727140427
iteration 205, loss = 0.18410205841064453
iteration 206, loss = 0.24872279167175293
iteration 207, loss = 0.17179325222969055
iteration 208, loss = 0.14263248443603516
iteration 209, loss = 0.11566613614559174
iteration 210, loss = 0.20225566625595093
iteration 211, loss = 0.1569480150938034
iteration 212, loss = 0.12304473668336868
iteration 213, loss = 0.2049018293619156
iteration 214, loss = 0.21246512234210968
iteration 215, loss = 0.2326994389295578
iteration 216, loss = 0.13370653986930847
iteration 217, loss = 0.154419407248497
iteration 218, loss = 0.20718401670455933
iteration 219, loss = 0.1668567955493927
iteration 220, loss = 0.4197311997413635
iteration 221, loss = 0.328525185585022
iteration 222, loss = 0.06693176925182343
iteration 223, loss = 0.09497788548469543
iteration 224, loss = 0.1630970984697342
iteration 225, loss = 0.09437107294797897
iteration 226, loss = 0.22469159960746765
iteration 227, loss = 0.40343278646469116
iteration 228, loss = 0.22364327311515808
iteration 229, loss = 0.35660088062286377
iteration 230, loss = 0.10704761743545532
iteration 231, loss = 0.09696269035339355
iteration 232, loss = 0.13538572192192078
iteration 233, loss = 0.23480571806430817
iteration 234, loss = 0.3531109392642975
iteration 235, loss = 0.15394653379917145
iteration 236, loss = 0.25652313232421875
iteration 237, loss = 0.18680910766124725
iteration 238, loss = 0.28546422719955444
iteration 239, loss = 0.18113350868225098
iteration 240, loss = 0.12547121942043304
iteration 241, loss = 0.09230363368988037
iteration 242, loss = 0.2146638184785843
iteration 243, loss = 0.14725038409233093
iteration 244, loss = 0.09568487107753754
iteration 245, loss = 0.10329845547676086
iteration 246, loss = 0.14035387337207794
iteration 247, loss = 0.16256672143936157
iteration 248, loss = 0.24266400933265686
iteration 249, loss = 0.3930084705352783
iteration 250, loss = 0.22673571109771729
iteration 251, loss = 0.20171639323234558
iteration 252, loss = 0.3312539756298065
iteration 253, loss = 0.18923859298229218
iteration 254, loss = 0.19362276792526245
iteration 255, loss = 0.050169773399829865
iteration 256, loss = 0.09573633968830109
iteration 257, loss = 0.12443288415670395
iteration 258, loss = 0.16913488507270813
iteration 259, loss = 0.07924284785985947
iteration 260, loss = 0.21904148161411285
iteration 261, loss = 0.13487891852855682
iteration 262, loss = 0.26086434721946716
iteration 263, loss = 0.08628787100315094
iteration 264, loss = 0.13936705887317657
iteration 265, loss = 0.0750310868024826
iteration 266, loss = 0.07925885170698166
iteration 267, loss = 0.17113065719604492
iteration 268, loss = 0.1152653694152832
iteration 269, loss = 0.14650481939315796
iteration 270, loss = 0.15994954109191895
iteration 271, loss = 0.13497449457645416
iteration 272, loss = 0.2190282940864563
iteration 273, loss = 0.06903450936079025
iteration 274, loss = 0.11490984261035919
iteration 275, loss = 0.1431351602077484
iteration 276, loss = 0.208343505859375
iteration 277, loss = 0.07166972756385803
iteration 278, loss = 0.14762695133686066
iteration 279, loss = 0.18804046511650085
iteration 280, loss = 0.3263635039329529
iteration 281, loss = 0.14139224588871002
iteration 282, loss = 0.33710727095603943
iteration 283, loss = 0.1505735069513321
iteration 284, loss = 0.11124369502067566
iteration 285, loss = 0.16779114305973053
iteration 286, loss = 0.24930158257484436
iteration 287, loss = 0.11360272765159607
iteration 288, loss = 0.16081446409225464
iteration 289, loss = 0.23969832062721252
iteration 290, loss = 0.12390469014644623
iteration 291, loss = 0.061745837330818176
iteration 292, loss = 0.15550461411476135
iteration 293, loss = 0.17186683416366577
iteration 294, loss = 0.05595949664711952
iteration 295, loss = 0.09774655848741531
iteration 296, loss = 0.16663342714309692
iteration 297, loss = 0.08777108788490295
iteration 298, loss = 0.0964120551943779
iteration 299, loss = 0.19991911947727203
iteration 300, loss = 0.11643603444099426

iteration 1, loss = 0.0016229957109317183
iteration 2, loss = 0.0018085609190165997
iteration 3, loss = 0.0019253491191193461
iteration 4, loss = 0.0017986017046496272
iteration 5, loss = 0.0021887836046516895
iteration 6, loss = 0.001927961246110499
iteration 7, loss = 0.0034316182136535645
iteration 8, loss = 0.002785607473924756
iteration 9, loss = 0.0017178865382447839
iteration 10, loss = 0.0018472486408427358
iteration 11, loss = 0.0016496652970090508
iteration 12, loss = 0.0018580000614747405
iteration 13, loss = 0.0022115425672382116
iteration 14, loss = 0.0018291421001777053
iteration 15, loss = 0.0015463563613593578
iteration 16, loss = 0.0020844454411417246
iteration 17, loss = 0.0018388961907476187
iteration 18, loss = 0.002402088139206171
iteration 19, loss = 0.0021168480161577463
iteration 20, loss = 0.001679149572737515
iteration 21, loss = 0.003665306605398655
iteration 22, loss = 0.0019350924994796515
iteration 23, loss = 0.0020763359498232603
iteration 24, loss = 0.0018804974388331175
iteration 25, loss = 0.0021884979214519262
iteration 26, loss = 0.002072284696623683
iteration 27, loss = 0.0029841396026313305
iteration 28, loss = 0.0015120956813916564
iteration 29, loss = 0.004098756704479456
iteration 30, loss = 0.0022204353008419275
iteration 31, loss = 0.0032111292239278555
iteration 32, loss = 0.002097028773277998
iteration 33, loss = 0.0017373953014612198
iteration 34, loss = 0.0018123378977179527
iteration 35, loss = 0.0021099511068314314
iteration 36, loss = 0.001723933033645153
iteration 37, loss = 0.002518553053960204
iteration 38, loss = 0.0022431262768805027
iteration 39, loss = 0.0019650524482131004
iteration 40, loss = 0.001988817472010851
iteration 41, loss = 0.0020233821123838425
iteration 42, loss = 0.002250519348308444
iteration 43, loss = 0.0019709491170942783
iteration 44, loss = 0.0024082644376903772
iteration 45, loss = 0.0021583857014775276
iteration 46, loss = 0.0027117342688143253
iteration 47, loss = 0.002169793238863349
iteration 48, loss = 0.0037059199530631304
iteration 49, loss = 0.0027914512902498245
iteration 50, loss = 0.0017910280730575323
iteration 51, loss = 0.002195283304899931
iteration 52, loss = 0.002569589065387845
iteration 53, loss = 0.0021062628366053104
iteration 54, loss = 0.002125883474946022
iteration 55, loss = 0.0032269274815917015
iteration 56, loss = 0.002052559284493327
iteration 57, loss = 0.0027592401020228863
iteration 58, loss = 0.0018357555381953716
iteration 59, loss = 0.0019251082558184862
iteration 60, loss = 0.002305715810507536
iteration 61, loss = 0.0019786348566412926
iteration 62, loss = 0.002806585980579257
iteration 63, loss = 0.0019816881977021694
iteration 64, loss = 0.0015799646498635411
iteration 65, loss = 0.0023443810641765594
iteration 66, loss = 0.0022026931401342154
iteration 67, loss = 0.0018936267588287592
iteration 68, loss = 0.0018511897651478648
iteration 69, loss = 0.0018918906571343541
iteration 70, loss = 0.003048874204978347
iteration 71, loss = 0.0017349559348076582
iteration 72, loss = 0.0019171616295352578
iteration 73, loss = 0.0018458764534443617
iteration 74, loss = 0.0021245558746159077
iteration 75, loss = 0.0024865837767720222
iteration 76, loss = 0.0019995199982076883
iteration 77, loss = 0.0016507701948285103
iteration 78, loss = 0.002693179529160261
iteration 79, loss = 0.0023960494436323643
iteration 80, loss = 0.002146756276488304
iteration 81, loss = 0.0037405923940241337
iteration 82, loss = 0.0017141149146482348
iteration 83, loss = 0.0023227515630424023
iteration 84, loss = 0.0017944701248779893
iteration 85, loss = 0.0019047068199142814
iteration 86, loss = 0.0019628710579127073
iteration 87, loss = 0.0018740446539595723
iteration 88, loss = 0.002792461309581995
iteration 89, loss = 0.001814856193959713
iteration 90, loss = 0.0021266788244247437
iteration 91, loss = 0.002685526153072715
iteration 92, loss = 0.0028998423367738724
iteration 93, loss = 0.002413051901385188
iteration 94, loss = 0.0017889132723212242
iteration 95, loss = 0.0018529014196246862
iteration 96, loss = 0.001644510542973876
iteration 97, loss = 0.0020200498402118683
iteration 98, loss = 0.002234009327366948
iteration 99, loss = 0.0018714384641498327
iteration 100, loss = 0.001776804099790752
iteration 101, loss = 0.0016712896758690476
iteration 102, loss = 0.0021486750338226557
iteration 103, loss = 0.002212763763964176
iteration 104, loss = 0.0024850585032254457
iteration 105, loss = 0.002205624943599105
iteration 106, loss = 0.0015872626099735498
iteration 107, loss = 0.0018803164130076766
iteration 108, loss = 0.0016767121851444244
iteration 109, loss = 0.0016721704741939902
iteration 110, loss = 0.0016308699268847704
iteration 111, loss = 0.001958283595740795
iteration 112, loss = 0.0019222162663936615
iteration 113, loss = 0.003849681932479143
iteration 114, loss = 0.0018925198819488287
iteration 115, loss = 0.002298142993822694
iteration 116, loss = 0.0018131223041564226
iteration 117, loss = 0.002696434734389186
iteration 118, loss = 0.0015935994451865554
iteration 119, loss = 0.0037196269258856773
iteration 120, loss = 0.003113000886514783
iteration 121, loss = 0.0020712490659207106
iteration 122, loss = 0.0016279552364721894
iteration 123, loss = 0.0025549805723130703
iteration 124, loss = 0.00257087335921824
iteration 125, loss = 0.0016903291689231992
iteration 126, loss = 0.0020855539478361607
iteration 127, loss = 0.0019024786306545138
iteration 128, loss = 0.0029710475355386734
iteration 129, loss = 0.002307743998244405
iteration 130, loss = 0.0018008712213486433
iteration 131, loss = 0.0017215352272614837
iteration 132, loss = 0.0023762607015669346
iteration 133, loss = 0.0020300205796957016
iteration 134, loss = 0.0018110057571902871
iteration 135, loss = 0.0020561087876558304
iteration 136, loss = 0.0019431638065725565
iteration 137, loss = 0.0016125013353303075
iteration 138, loss = 0.00203278218396008
iteration 139, loss = 0.0031631675083190203
iteration 140, loss = 0.002455885987728834
iteration 141, loss = 0.0019030331168323755
iteration 142, loss = 0.0019025185611099005
iteration 143, loss = 0.0018713586032390594
iteration 144, loss = 0.002541050547733903
iteration 145, loss = 0.00272862846031785
iteration 146, loss = 0.0017484399722889066
iteration 147, loss = 0.0019962205551564693
iteration 148, loss = 0.0016468055546283722
iteration 149, loss = 0.0020030757877975702
iteration 150, loss = 0.0026296728756278753
iteration 151, loss = 0.0022950302809476852
iteration 152, loss = 0.0023834474850445986
iteration 153, loss = 0.0020032695028930902
iteration 154, loss = 0.0019729388877749443
iteration 155, loss = 0.0034492481499910355
iteration 156, loss = 0.0036636870354413986
iteration 157, loss = 0.001946664648130536
iteration 158, loss = 0.0018368280725553632
iteration 159, loss = 0.0019583154935389757
iteration 160, loss = 0.0014970290940254927
iteration 161, loss = 0.0024445706512778997
iteration 162, loss = 0.0017923846608027816
iteration 163, loss = 0.0022218646481633186
iteration 164, loss = 0.00214002444408834
iteration 165, loss = 0.001889010309241712
iteration 166, loss = 0.0017859942745417356
iteration 167, loss = 0.0028128777630627155
iteration 168, loss = 0.0022589964792132378
iteration 169, loss = 0.0023714026901870966
iteration 170, loss = 0.002208586083725095
iteration 171, loss = 0.0015107474755495787
iteration 172, loss = 0.001837744377553463
iteration 173, loss = 0.0020266654901206493
iteration 174, loss = 0.002233093837276101
iteration 175, loss = 0.0016547903651371598
iteration 176, loss = 0.0017993778456002474
iteration 177, loss = 0.004190284758806229
iteration 178, loss = 0.0015393368666991591
iteration 179, loss = 0.0036862888373434544
iteration 180, loss = 0.0018393625505268574
iteration 181, loss = 0.001873294240795076
iteration 182, loss = 0.002110182773321867
iteration 183, loss = 0.0016944864764809608
iteration 184, loss = 0.0017181257717311382
iteration 185, loss = 0.0017180850263684988
iteration 186, loss = 0.0020457764621824026
iteration 187, loss = 0.0024378313682973385
iteration 188, loss = 0.0027020557317882776
iteration 189, loss = 0.0016706831520423293
iteration 190, loss = 0.0029727178625762463
iteration 191, loss = 0.0016991934971883893
iteration 192, loss = 0.0025108803529292345
iteration 193, loss = 0.0021059904247522354
iteration 194, loss = 0.0017449076985940337
iteration 195, loss = 0.0017216785345226526
iteration 196, loss = 0.003395689418539405
iteration 197, loss = 0.0017420451622456312
iteration 198, loss = 0.0017809646669775248
iteration 199, loss = 0.0026682938914746046
iteration 200, loss = 0.002059315796941519
iteration 201, loss = 0.002474585548043251
iteration 202, loss = 0.0019545017275959253
iteration 203, loss = 0.0017892789328470826
iteration 204, loss = 0.0017871805466711521
iteration 205, loss = 0.0018039504066109657
iteration 206, loss = 0.0024522175081074238
iteration 207, loss = 0.001992045668885112
iteration 208, loss = 0.0020639789290726185
iteration 209, loss = 0.003527604276314378
iteration 210, loss = 0.002086061052978039
iteration 211, loss = 0.001661102520301938
iteration 212, loss = 0.0017930142348632216
iteration 213, loss = 0.0013880584156140685
iteration 214, loss = 0.0025249095633625984
iteration 215, loss = 0.002634788863360882
iteration 216, loss = 0.0026537722442299128
iteration 217, loss = 0.001867140643298626
iteration 218, loss = 0.0019165304256603122
iteration 219, loss = 0.0019055689917877316
iteration 220, loss = 0.0019584789406508207
iteration 221, loss = 0.0022639792878180742
iteration 222, loss = 0.0024003200232982635
iteration 223, loss = 0.00241406774148345
iteration 224, loss = 0.002612377516925335
iteration 225, loss = 0.0017215609550476074
iteration 226, loss = 0.001570801017805934
iteration 227, loss = 0.002246026648208499
iteration 228, loss = 0.0017539924010634422
iteration 229, loss = 0.0018918570131063461
iteration 230, loss = 0.00172273232601583
iteration 231, loss = 0.0017478433437645435
iteration 232, loss = 0.0022246099542826414
iteration 233, loss = 0.0018332056934013963
iteration 234, loss = 0.003366888267919421
iteration 235, loss = 0.002094842493534088
iteration 236, loss = 0.0035275237169116735
iteration 237, loss = 0.0017873897450044751
iteration 238, loss = 0.0018482996383681893
iteration 239, loss = 0.0036469511687755585
iteration 240, loss = 0.0013586520217359066
iteration 241, loss = 0.0016023666830733418
iteration 242, loss = 0.0018928617937490344
iteration 243, loss = 0.0023923772387206554
iteration 244, loss = 0.0022400489542633295
iteration 245, loss = 0.0015465929172933102
iteration 246, loss = 0.0014753942377865314
iteration 247, loss = 0.0028495131991803646
iteration 248, loss = 0.0023773503489792347
iteration 249, loss = 0.0021802738774567842
iteration 250, loss = 0.003124693175777793
iteration 251, loss = 0.0017511859769001603
iteration 252, loss = 0.002318541519343853
iteration 253, loss = 0.001791778369806707
iteration 254, loss = 0.0025662954431027174
iteration 255, loss = 0.0026155244559049606
iteration 256, loss = 0.0015623252838850021
iteration 257, loss = 0.001555855036713183
iteration 258, loss = 0.0027244361117482185
iteration 259, loss = 0.002030464354902506
iteration 260, loss = 0.002590796910226345
iteration 261, loss = 0.0013228297466412187
iteration 262, loss = 0.0025973308365792036
iteration 263, loss = 0.002513793297111988
iteration 264, loss = 0.0019941874779760838
iteration 265, loss = 0.001570599852129817
iteration 266, loss = 0.003008584724739194
iteration 267, loss = 0.0017632960807532072
iteration 268, loss = 0.0016629970632493496
iteration 269, loss = 0.002366366796195507
iteration 270, loss = 0.0034529592376202345
iteration 271, loss = 0.0017763487994670868
iteration 272, loss = 0.001680046902038157
iteration 273, loss = 0.001527763670310378
iteration 274, loss = 0.003707892494276166
iteration 275, loss = 0.002043244894593954
iteration 276, loss = 0.0028147820848971605
iteration 277, loss = 0.002990483772009611
iteration 278, loss = 0.0017035193741321564
iteration 279, loss = 0.0019096641335636377
iteration 280, loss = 0.003760567167773843
iteration 281, loss = 0.0015761771937832236
iteration 282, loss = 0.0022885489743202925
iteration 283, loss = 0.001776849152520299
iteration 284, loss = 0.0019366504857316613
iteration 285, loss = 0.0017866224516183138
iteration 286, loss = 0.0019389062654227018
iteration 287, loss = 0.001696505700238049
iteration 288, loss = 0.0035517059732228518
iteration 289, loss = 0.0027983637992292643
iteration 290, loss = 0.0015261666849255562
iteration 291, loss = 0.001484616077505052
iteration 292, loss = 0.003162583103403449
iteration 293, loss = 0.002254635328426957
iteration 294, loss = 0.0018413197249174118
iteration 295, loss = 0.0016026031225919724
iteration 296, loss = 0.002514720894396305
iteration 297, loss = 0.0015579387545585632
iteration 298, loss = 0.0014103468274697661
iteration 299, loss = 0.001979637425392866
iteration 300, loss = 0.0018834968795999885
iteration 1, loss = 0.0018773996271193027
iteration 2, loss = 0.0017436917405575514
iteration 3, loss = 0.001699357759207487
iteration 4, loss = 0.00318925897590816
iteration 5, loss = 0.0016750018112361431
iteration 6, loss = 0.001699230633676052
iteration 7, loss = 0.0018762815743684769
iteration 8, loss = 0.0015470876824110746
iteration 9, loss = 0.002078064251691103
iteration 10, loss = 0.001866774633526802
iteration 11, loss = 0.00201201718300581
iteration 12, loss = 0.0024883083533495665
iteration 13, loss = 0.0015275439945980906
iteration 14, loss = 0.0018744450062513351
iteration 15, loss = 0.002029555384069681
iteration 16, loss = 0.0015866531757637858
iteration 17, loss = 0.00195704004727304
iteration 18, loss = 0.0013718472328037024
iteration 19, loss = 0.002368224784731865
iteration 20, loss = 0.0016352110542356968
iteration 21, loss = 0.0023511312901973724
iteration 22, loss = 0.00157008389942348
iteration 23, loss = 0.0013709280174225569
iteration 24, loss = 0.002557719824835658
iteration 25, loss = 0.002632291056215763
iteration 26, loss = 0.001705250353552401
iteration 27, loss = 0.0022508346009999514
iteration 28, loss = 0.0020206726621836424
iteration 29, loss = 0.0014819103525951505
iteration 30, loss = 0.0015258581843227148
iteration 31, loss = 0.0014011452440172434
iteration 32, loss = 0.0017874808982014656
iteration 33, loss = 0.001808714121580124
iteration 34, loss = 0.0014611338265240192
iteration 35, loss = 0.0018660944188013673
iteration 36, loss = 0.0018039161805063486
iteration 37, loss = 0.0018613021820783615
iteration 38, loss = 0.002051570685580373
iteration 39, loss = 0.001764053013175726
iteration 40, loss = 0.0024490708019584417
iteration 41, loss = 0.0016153212636709213
iteration 42, loss = 0.0015953390393406153
iteration 43, loss = 0.0017552315257489681
iteration 44, loss = 0.001827107509598136
iteration 45, loss = 0.0016872084233909845
iteration 46, loss = 0.001817456097342074
iteration 47, loss = 0.003466711612418294
iteration 48, loss = 0.0017936264630407095
iteration 49, loss = 0.0015895708929747343
iteration 50, loss = 0.0019344468601047993
iteration 51, loss = 0.0018671868601813912
iteration 52, loss = 0.001620989991351962
iteration 53, loss = 0.0017721786862239242
iteration 54, loss = 0.002755527850240469
iteration 55, loss = 0.0021216231398284435
iteration 56, loss = 0.0020714187994599342
iteration 57, loss = 0.0024128626100718975
iteration 58, loss = 0.001862223376519978
iteration 59, loss = 0.0019555999897420406
iteration 60, loss = 0.0015603185165673494
iteration 61, loss = 0.0030331271700561047
iteration 62, loss = 0.001869088620878756
iteration 63, loss = 0.0016256043454632163
iteration 64, loss = 0.002843717345967889
iteration 65, loss = 0.004059197381138802
iteration 66, loss = 0.0016597211360931396
iteration 67, loss = 0.0020910559687763453
iteration 68, loss = 0.002202272415161133
iteration 69, loss = 0.0014816110488027334
iteration 70, loss = 0.001850307802669704
iteration 71, loss = 0.002686291467398405
iteration 72, loss = 0.0018952564569190145
iteration 73, loss = 0.0012806006707251072
iteration 74, loss = 0.0017813302110880613
iteration 75, loss = 0.001819835975766182
iteration 76, loss = 0.001623982680030167
iteration 77, loss = 0.001904492499306798
iteration 78, loss = 0.0015574574936181307
iteration 79, loss = 0.0017869708826765418
iteration 80, loss = 0.0016336446860805154
iteration 81, loss = 0.0019209088059142232
iteration 82, loss = 0.00150672800373286
iteration 83, loss = 0.0024127119686454535
iteration 84, loss = 0.0018916807603091002
iteration 85, loss = 0.0016033704159781337
iteration 86, loss = 0.001749318209476769
iteration 87, loss = 0.0014175190590322018
iteration 88, loss = 0.0028587528504431248
iteration 89, loss = 0.002415786264464259
iteration 90, loss = 0.0016873115673661232
iteration 91, loss = 0.0030619443859905005
iteration 92, loss = 0.0013347961939871311
iteration 93, loss = 0.0019321227446198463
iteration 94, loss = 0.0016124250832945108
iteration 95, loss = 0.0017770210979506373
iteration 96, loss = 0.0015804213471710682
iteration 97, loss = 0.002245132112875581
iteration 98, loss = 0.0023636678233742714
iteration 99, loss = 0.0015921613667160273
iteration 100, loss = 0.002900635125115514
iteration 101, loss = 0.0022605224512517452
iteration 102, loss = 0.002075074939057231
iteration 103, loss = 0.0034350682981312275
iteration 104, loss = 0.0015554274432361126
iteration 105, loss = 0.00175832724198699
iteration 106, loss = 0.0025769134517759085
iteration 107, loss = 0.003433400997892022
iteration 108, loss = 0.003487497102469206
iteration 109, loss = 0.0019344495376572013
iteration 110, loss = 0.0017510050674900413
iteration 111, loss = 0.0017493258928880095
iteration 112, loss = 0.0017659463919699192
iteration 113, loss = 0.0026302807964384556
iteration 114, loss = 0.0019921499770134687
iteration 115, loss = 0.0034041916951537132
iteration 116, loss = 0.0019206790020689368
iteration 117, loss = 0.0014601254370063543
iteration 118, loss = 0.0016350036021322012
iteration 119, loss = 0.001607945654541254
iteration 120, loss = 0.002433764049783349
iteration 121, loss = 0.0018298852955922484
iteration 122, loss = 0.0018963866168633103
iteration 123, loss = 0.00343058118596673
iteration 124, loss = 0.0015390838962048292
iteration 125, loss = 0.0017838120693340898
iteration 126, loss = 0.0015510872472077608
iteration 127, loss = 0.0023852945305407047
iteration 128, loss = 0.0015573510900139809
iteration 129, loss = 0.0018890219507738948
iteration 130, loss = 0.0014857602072879672
iteration 131, loss = 0.0031071174889802933
iteration 132, loss = 0.0017171016661450267
iteration 133, loss = 0.0032228289637714624
iteration 134, loss = 0.0014673930127173662
iteration 135, loss = 0.0013572836760431528
iteration 136, loss = 0.0017097990494221449
iteration 137, loss = 0.0022895466536283493
iteration 138, loss = 0.0015058449935168028
iteration 139, loss = 0.0017384267412126064
iteration 140, loss = 0.00167517876252532
iteration 141, loss = 0.002121641067788005
iteration 142, loss = 0.0016676934901624918
iteration 143, loss = 0.0015600386541336775
iteration 144, loss = 0.0018901638686656952
iteration 145, loss = 0.001463262946344912
iteration 146, loss = 0.001629499951377511
iteration 147, loss = 0.002358462195843458
iteration 148, loss = 0.001785188796930015
iteration 149, loss = 0.0022661257535219193
iteration 150, loss = 0.0015964256599545479
iteration 151, loss = 0.002021957654505968
iteration 152, loss = 0.0025270155165344477
iteration 153, loss = 0.002042687265202403
iteration 154, loss = 0.001538789481855929
iteration 155, loss = 0.0018497600685805082
iteration 156, loss = 0.0019173596519976854
iteration 157, loss = 0.0017165200551971793
iteration 158, loss = 0.002145980019122362
iteration 159, loss = 0.0013830730458721519
iteration 160, loss = 0.0017796526663005352
iteration 161, loss = 0.0013455579755827785
iteration 162, loss = 0.001741322805173695
iteration 163, loss = 0.0018179969629272819
iteration 164, loss = 0.001815936528146267
iteration 165, loss = 0.0033454899676144123
iteration 166, loss = 0.003036408219486475
iteration 167, loss = 0.0024663505610078573
iteration 168, loss = 0.0013981361407786608
iteration 169, loss = 0.0018220578785985708
iteration 170, loss = 0.0014312489656731486
iteration 171, loss = 0.0017261833418160677
iteration 172, loss = 0.00147381448186934
iteration 173, loss = 0.001467362279072404
iteration 174, loss = 0.0029700640588998795
iteration 175, loss = 0.004042892251163721
iteration 176, loss = 0.0017674366245046258
iteration 177, loss = 0.0014741054037585855
iteration 178, loss = 0.0018168740207329392
iteration 179, loss = 0.0022267845924943686
iteration 180, loss = 0.0015548468800261617
iteration 181, loss = 0.003099711611866951
iteration 182, loss = 0.0021666837856173515
iteration 183, loss = 0.0029812788125127554
iteration 184, loss = 0.0015360090183094144
iteration 185, loss = 0.0022689944598823786
iteration 186, loss = 0.0013837350998073816
iteration 187, loss = 0.0021946807391941547
iteration 188, loss = 0.0022778655402362347
iteration 189, loss = 0.0022533899173140526
iteration 190, loss = 0.001597920199856162
iteration 191, loss = 0.0024584534112364054
iteration 192, loss = 0.002817298285663128
iteration 193, loss = 0.002221297239884734
iteration 194, loss = 0.0021010437048971653
iteration 195, loss = 0.0014814732130616903
iteration 196, loss = 0.0016687174793332815
iteration 197, loss = 0.0018133216071873903
iteration 198, loss = 0.002285867230966687
iteration 199, loss = 0.0014120186679065228
iteration 200, loss = 0.0018933166284114122
iteration 201, loss = 0.0016130703734233975
iteration 202, loss = 0.0017893207259476185
iteration 203, loss = 0.0018681406509131193
iteration 204, loss = 0.00232321978546679
iteration 205, loss = 0.001545066712424159
iteration 206, loss = 0.0016094825696200132
iteration 207, loss = 0.001577205490320921
iteration 208, loss = 0.0020418930798768997
iteration 209, loss = 0.003144634887576103
iteration 210, loss = 0.0022113043814897537
iteration 211, loss = 0.0015093956608325243
iteration 212, loss = 0.0017945197178050876
iteration 213, loss = 0.0016027940437197685
iteration 214, loss = 0.003338402137160301
iteration 215, loss = 0.0016774903051555157
iteration 216, loss = 0.001605200464837253
iteration 217, loss = 0.0015667291590943933
iteration 218, loss = 0.0024293556343764067
iteration 219, loss = 0.0015850599156692624
iteration 220, loss = 0.002266991650685668
iteration 221, loss = 0.001366042997688055
iteration 222, loss = 0.001630945596843958
iteration 223, loss = 0.0018031352665275335
iteration 224, loss = 0.0016082742949947715
iteration 225, loss = 0.0014428645372390747
iteration 226, loss = 0.0015729404985904694
iteration 227, loss = 0.001742026535794139
iteration 228, loss = 0.0015812928322702646
iteration 229, loss = 0.0018357282970100641
iteration 230, loss = 0.0018076474079862237
iteration 231, loss = 0.0023485217243433
iteration 232, loss = 0.0018593299901112914
iteration 233, loss = 0.002145824721083045
iteration 234, loss = 0.0019574204925447702
iteration 235, loss = 0.0021922539453953505
iteration 236, loss = 0.0016773832030594349
iteration 237, loss = 0.003245759755373001
iteration 238, loss = 0.0019411148969084024
iteration 239, loss = 0.001528644934296608
iteration 240, loss = 0.003195091150701046
iteration 241, loss = 0.0015782119007781148
iteration 242, loss = 0.0015036880504339933
iteration 243, loss = 0.001686733216047287
iteration 244, loss = 0.0024040951393544674
iteration 245, loss = 0.002479804679751396
iteration 246, loss = 0.0016675255028530955
iteration 247, loss = 0.0016692745266482234
iteration 248, loss = 0.001943686860613525
iteration 249, loss = 0.0019049663096666336
iteration 250, loss = 0.0016065937234088778
iteration 251, loss = 0.0016555548645555973
iteration 252, loss = 0.001551078399643302
iteration 253, loss = 0.0031611956655979156
iteration 254, loss = 0.0016781387384980917
iteration 255, loss = 0.002079084748402238
iteration 256, loss = 0.0013571431627497077
iteration 257, loss = 0.0015110583044588566
iteration 258, loss = 0.001634354004636407
iteration 259, loss = 0.0018390794284641743
iteration 260, loss = 0.0016053308499976993
iteration 261, loss = 0.002027682727202773
iteration 262, loss = 0.0015377565287053585
iteration 263, loss = 0.001293693669140339
iteration 264, loss = 0.0025779367424547672
iteration 265, loss = 0.0015568506205454469
iteration 266, loss = 0.0015122487675398588
iteration 267, loss = 0.0013057590695098042
iteration 268, loss = 0.0018635159358382225
iteration 269, loss = 0.0015899052377790213
iteration 270, loss = 0.0017652002861723304
iteration 271, loss = 0.0017386518884450197
iteration 272, loss = 0.002254689810797572
iteration 273, loss = 0.0015984545461833477
iteration 274, loss = 0.0015526695642620325
iteration 275, loss = 0.002191442297771573
iteration 276, loss = 0.001728807226754725
iteration 277, loss = 0.0012394393561407924
iteration 278, loss = 0.0015653586015105247
iteration 279, loss = 0.0014732396230101585
iteration 280, loss = 0.0016300553688779473
iteration 281, loss = 0.0034322577994316816
iteration 282, loss = 0.0017992956563830376
iteration 283, loss = 0.0018518507713451982
iteration 284, loss = 0.0019537752959877253
iteration 285, loss = 0.0016301078721880913
iteration 286, loss = 0.0015824225265532732
iteration 287, loss = 0.0031115522142499685
iteration 288, loss = 0.001725544803775847
iteration 289, loss = 0.0018878283444792032
iteration 290, loss = 0.0031075843144208193
iteration 291, loss = 0.001608885359019041
iteration 292, loss = 0.001692754914984107
iteration 293, loss = 0.003233269089832902
iteration 294, loss = 0.0013746530748903751
iteration 295, loss = 0.003377830144017935
iteration 296, loss = 0.0031305961310863495
iteration 297, loss = 0.0015298235230147839
iteration 298, loss = 0.0014212094247341156
iteration 299, loss = 0.001440134714357555
iteration 300, loss = 0.0028530373238027096
iteration 1, loss = 0.0017553422367200255
iteration 2, loss = 0.0016807139618322253
iteration 3, loss = 0.0015945585910230875
iteration 4, loss = 0.0015345641877502203
iteration 5, loss = 0.0027628736570477486
iteration 6, loss = 0.002167068887501955
iteration 7, loss = 0.0016892057610675693
iteration 8, loss = 0.002909939270466566
iteration 9, loss = 0.0017255347920581698
iteration 10, loss = 0.0014347548130899668
iteration 11, loss = 0.0016284422017633915
iteration 12, loss = 0.002398113254457712
iteration 13, loss = 0.0015315316850319505
iteration 14, loss = 0.001583252684213221
iteration 15, loss = 0.0013259539846330881
iteration 16, loss = 0.001471347757615149
iteration 17, loss = 0.0016737652476876974
iteration 18, loss = 0.001853692578151822
iteration 19, loss = 0.0022141486406326294
iteration 20, loss = 0.0017261960310861468
iteration 21, loss = 0.0024128409568220377
iteration 22, loss = 0.0018977555446326733
iteration 23, loss = 0.0016616370994597673
iteration 24, loss = 0.0015279900981113315
iteration 25, loss = 0.001679303590208292
iteration 26, loss = 0.0016097566112875938
iteration 27, loss = 0.0016154985642060637
iteration 28, loss = 0.0018361841794103384
iteration 29, loss = 0.0014018143992871046
iteration 30, loss = 0.0018858268158510327
iteration 31, loss = 0.0013434867141768336
iteration 32, loss = 0.00193578633479774
iteration 33, loss = 0.001650763675570488
iteration 34, loss = 0.0017012411262840033
iteration 35, loss = 0.0018619999755173922
iteration 36, loss = 0.0016340040601789951
iteration 37, loss = 0.0015260757645592093
iteration 38, loss = 0.0014463678235188127
iteration 39, loss = 0.0017065044958144426
iteration 40, loss = 0.0022184045519679785
iteration 41, loss = 0.0018030022038146853
iteration 42, loss = 0.0016521859215572476
iteration 43, loss = 0.001953140599653125
iteration 44, loss = 0.002103097504004836
iteration 45, loss = 0.002054779091849923
iteration 46, loss = 0.002013696124777198
iteration 47, loss = 0.001834919210523367
iteration 48, loss = 0.0018907218473032117
iteration 49, loss = 0.0017719734460115433
iteration 50, loss = 0.0013351510278880596
iteration 51, loss = 0.0015545157948508859
iteration 52, loss = 0.0011228355579078197
iteration 53, loss = 0.001806285697966814
iteration 54, loss = 0.0014860342489555478
iteration 55, loss = 0.00127103878185153
iteration 56, loss = 0.0014635439729318023
iteration 57, loss = 0.0017966708401218057
iteration 58, loss = 0.0026126636657863855
iteration 59, loss = 0.0013924912782385945
iteration 60, loss = 0.0016612476902082562
iteration 61, loss = 0.0022977765183895826
iteration 62, loss = 0.0018316240748390555
iteration 63, loss = 0.0016944368835538626
iteration 64, loss = 0.0018427784088999033
iteration 65, loss = 0.0017606359906494617
iteration 66, loss = 0.001640108646824956
iteration 67, loss = 0.0020303097553551197
iteration 68, loss = 0.001344785327091813
iteration 69, loss = 0.0019359480356797576
iteration 70, loss = 0.0018589295214042068
iteration 71, loss = 0.001382192363962531
iteration 72, loss = 0.0029473169706761837
iteration 73, loss = 0.0013828118098899722
iteration 74, loss = 0.0019829608500003815
iteration 75, loss = 0.0013555038021877408
iteration 76, loss = 0.001581046381033957
iteration 77, loss = 0.005337356124073267
iteration 78, loss = 0.0016725315945222974
iteration 79, loss = 0.0024473059456795454
iteration 80, loss = 0.0022386545315384865
iteration 81, loss = 0.001590091735124588
iteration 82, loss = 0.0016486376989632845
iteration 83, loss = 0.0011669410159811378
iteration 84, loss = 0.0012662169756367803
iteration 85, loss = 0.001475492725148797
iteration 86, loss = 0.0016039074398577213
iteration 87, loss = 0.001520219724625349
iteration 88, loss = 0.0015009568305686116
iteration 89, loss = 0.001338608330115676
iteration 90, loss = 0.0017610336653888226
iteration 91, loss = 0.001692610327154398
iteration 92, loss = 0.002475279849022627
iteration 93, loss = 0.0018433848163112998
iteration 94, loss = 0.0035442851949483156
iteration 95, loss = 0.0013813284458592534
iteration 96, loss = 0.0013049044646322727
iteration 97, loss = 0.0015903034945949912
iteration 98, loss = 0.0014203641330823302
iteration 99, loss = 0.0016413708217442036
iteration 100, loss = 0.0016921601491048932
iteration 101, loss = 0.0015473654493689537
iteration 102, loss = 0.001462860032916069
iteration 103, loss = 0.001363527262583375
iteration 104, loss = 0.0018707677954807878
iteration 105, loss = 0.0016469836700707674
iteration 106, loss = 0.002951688366010785
iteration 107, loss = 0.0021741879172623158
iteration 108, loss = 0.0015723643591627479
iteration 109, loss = 0.0017603583401069045
iteration 110, loss = 0.0032383317593485117
iteration 111, loss = 0.0018688032869249582
iteration 112, loss = 0.0033418373204767704
iteration 113, loss = 0.0016421782784163952
iteration 114, loss = 0.001654681982472539
iteration 115, loss = 0.001507758628576994
iteration 116, loss = 0.0015990820247679949
iteration 117, loss = 0.0015133342240005732
iteration 118, loss = 0.00198921887204051
iteration 119, loss = 0.0014658496947959065
iteration 120, loss = 0.001527793356217444
iteration 121, loss = 0.0021416740491986275
iteration 122, loss = 0.0015710522420704365
iteration 123, loss = 0.0019601646345108747
iteration 124, loss = 0.0016514703165739775
iteration 125, loss = 0.002293833764269948
iteration 126, loss = 0.0012835257221013308
iteration 127, loss = 0.0014052757760509849
iteration 128, loss = 0.0032027096021920443
iteration 129, loss = 0.0019508120603859425
iteration 130, loss = 0.003054047469049692
iteration 131, loss = 0.001469711191020906
iteration 132, loss = 0.0013345821062102914
iteration 133, loss = 0.0013594991760328412
iteration 134, loss = 0.001236140145920217
iteration 135, loss = 0.002645765198394656
iteration 136, loss = 0.0013771677622571588
iteration 137, loss = 0.001377273933030665
iteration 138, loss = 0.0016039038309827447
iteration 139, loss = 0.00268559530377388
iteration 140, loss = 0.0014776908792555332
iteration 141, loss = 0.0016097312327474356
iteration 142, loss = 0.0019400555174797773
iteration 143, loss = 0.0015587435336783528
iteration 144, loss = 0.0014467077562585473
iteration 145, loss = 0.0020001064985990524
iteration 146, loss = 0.001660422421991825
iteration 147, loss = 0.0014309627003967762
iteration 148, loss = 0.0013976572081446648
iteration 149, loss = 0.0013896781019866467
iteration 150, loss = 0.0014460616512224078
iteration 151, loss = 0.0017972227651625872
iteration 152, loss = 0.0020319661125540733
iteration 153, loss = 0.00134126259945333
iteration 154, loss = 0.0015216590836644173
iteration 155, loss = 0.0018005205783993006
iteration 156, loss = 0.0028995119500905275
iteration 157, loss = 0.002114432631060481
iteration 158, loss = 0.001512069022282958
iteration 159, loss = 0.0018948953365907073
iteration 160, loss = 0.0016300335992127657
iteration 161, loss = 0.0035921211820095778
iteration 162, loss = 0.002571672201156616
iteration 163, loss = 0.002219957299530506
iteration 164, loss = 0.0015151710249483585
iteration 165, loss = 0.001334952306933701
iteration 166, loss = 0.0020657978020608425
iteration 167, loss = 0.0015007124748080969
iteration 168, loss = 0.0014912571059539914
iteration 169, loss = 0.0014892475446686149
iteration 170, loss = 0.002223168034106493
iteration 171, loss = 0.0019114220049232244
iteration 172, loss = 0.0025498413015156984
iteration 173, loss = 0.0013646155130118132
iteration 174, loss = 0.0018501945305615664
iteration 175, loss = 0.0020154137164354324
iteration 176, loss = 0.001424938440322876
iteration 177, loss = 0.0014726754743605852
iteration 178, loss = 0.001957949483767152
iteration 179, loss = 0.0017683437326923013
iteration 180, loss = 0.0016303380252793431
iteration 181, loss = 0.0013544636312872171
iteration 182, loss = 0.0014089543838053942
iteration 183, loss = 0.001569160376675427
iteration 184, loss = 0.0015087345382198691
iteration 185, loss = 0.001492157345637679
iteration 186, loss = 0.0014691604301333427
iteration 187, loss = 0.001927701523527503
iteration 188, loss = 0.0015687718987464905
iteration 189, loss = 0.0014714287826791406
iteration 190, loss = 0.0016892305575311184
iteration 191, loss = 0.0015194591833278537
iteration 192, loss = 0.001303803757764399
iteration 193, loss = 0.0019756173714995384
iteration 194, loss = 0.0016236768569797277
iteration 195, loss = 0.001632695784792304
iteration 196, loss = 0.001585106598213315
iteration 197, loss = 0.0018125791102647781
iteration 198, loss = 0.0025573691818863153
iteration 199, loss = 0.0031279660761356354
iteration 200, loss = 0.0016005297657102346
iteration 201, loss = 0.0015584349166601896
iteration 202, loss = 0.0016178985824808478
iteration 203, loss = 0.0015398229006677866
iteration 204, loss = 0.0021854229271411896
iteration 205, loss = 0.0017935193609446287
iteration 206, loss = 0.0016518663614988327
iteration 207, loss = 0.0013669861946254969
iteration 208, loss = 0.0017319355392828584
iteration 209, loss = 0.003362013027071953
iteration 210, loss = 0.0018071867525577545
iteration 211, loss = 0.0013992005260661244
iteration 212, loss = 0.0014681590255349874
iteration 213, loss = 0.0015235599130392075
iteration 214, loss = 0.0014139269478619099
iteration 215, loss = 0.0013146528508514166
iteration 216, loss = 0.0020375242456793785
iteration 217, loss = 0.001320879440754652
iteration 218, loss = 0.0021348583977669477
iteration 219, loss = 0.0015897295670583844
iteration 220, loss = 0.0020430427975952625
iteration 221, loss = 0.0015091892564669251
iteration 222, loss = 0.001413164078257978
iteration 223, loss = 0.0013321624137461185
iteration 224, loss = 0.0015678541967645288
iteration 225, loss = 0.002004963345825672
iteration 226, loss = 0.00148446811363101
iteration 227, loss = 0.0013070949353277683
iteration 228, loss = 0.0016137546626850963
iteration 229, loss = 0.0013792607933282852
iteration 230, loss = 0.0015658773481845856
iteration 231, loss = 0.00189962238073349
iteration 232, loss = 0.0013828210067003965
iteration 233, loss = 0.0012655615573748946
iteration 234, loss = 0.0014200942823663354
iteration 235, loss = 0.0017517735250294209
iteration 236, loss = 0.0019233992788940668
iteration 237, loss = 0.0016060075722634792
iteration 238, loss = 0.0015425257151946425
iteration 239, loss = 0.0012930502416566014
iteration 240, loss = 0.00263371248729527
iteration 241, loss = 0.002442204626277089
iteration 242, loss = 0.0015666700201109052
iteration 243, loss = 0.001431024051271379
iteration 244, loss = 0.0011484164278954268
iteration 245, loss = 0.002326823538169265
iteration 246, loss = 0.0015997544396668673
iteration 247, loss = 0.001339887734502554
iteration 248, loss = 0.003051666310057044
iteration 249, loss = 0.0014907302102074027
iteration 250, loss = 0.002021823078393936
iteration 251, loss = 0.0020773117430508137
iteration 252, loss = 0.0016464372165501118
iteration 253, loss = 0.002041525673121214
iteration 254, loss = 0.0014980882406234741
iteration 255, loss = 0.0019289293559268117
iteration 256, loss = 0.0012955660931766033
iteration 257, loss = 0.0017200984293594956
iteration 258, loss = 0.001368414843454957
iteration 259, loss = 0.0015304331900551915
iteration 260, loss = 0.0021309424191713333
iteration 261, loss = 0.002858423860743642
iteration 262, loss = 0.0016476981109008193
iteration 263, loss = 0.0022465866059064865
iteration 264, loss = 0.0014736324083060026
iteration 265, loss = 0.0016872695414349437
iteration 266, loss = 0.0030977274291217327
iteration 267, loss = 0.0018120475579053164
iteration 268, loss = 0.0015827793395146728
iteration 269, loss = 0.0034646294079720974
iteration 270, loss = 0.0013643703423440456
iteration 271, loss = 0.0018926929915323853
iteration 272, loss = 0.0013740002177655697
iteration 273, loss = 0.0013149684527888894
iteration 274, loss = 0.001403708360157907
iteration 275, loss = 0.0013863149797543883
iteration 276, loss = 0.0012499066069722176
iteration 277, loss = 0.0014938750537112355
iteration 278, loss = 0.0017028320580720901
iteration 279, loss = 0.001889915787614882
iteration 280, loss = 0.001256308052688837
iteration 281, loss = 0.0017828864511102438
iteration 282, loss = 0.0018636049935594201
iteration 283, loss = 0.0013017812743782997
iteration 284, loss = 0.0025096952449530363
iteration 285, loss = 0.0013501953799277544
iteration 286, loss = 0.0018303841352462769
iteration 287, loss = 0.0020635491237044334
iteration 288, loss = 0.001598744303919375
iteration 289, loss = 0.0027702879160642624
iteration 290, loss = 0.00402631051838398
iteration 291, loss = 0.001223403261974454
iteration 292, loss = 0.0013644476421177387
iteration 293, loss = 0.0018405899172648787
iteration 294, loss = 0.002799033187329769
iteration 295, loss = 0.002979761455208063
iteration 296, loss = 0.0017516202060505748
iteration 297, loss = 0.0014180239522829652
iteration 298, loss = 0.0014837333001196384
iteration 299, loss = 0.0018965904600918293
iteration 300, loss = 0.001521149999462068
iteration 1, loss = 0.0015864202287048101
iteration 2, loss = 0.001362117356620729
iteration 3, loss = 0.001259615644812584
iteration 4, loss = 0.0020870801527053118
iteration 5, loss = 0.0013610519235953689
iteration 6, loss = 0.0014857737114652991
iteration 7, loss = 0.002045414876192808
iteration 8, loss = 0.002485433593392372
iteration 9, loss = 0.001959963236004114
iteration 10, loss = 0.0016406455542892218
iteration 11, loss = 0.0015617560129612684
iteration 12, loss = 0.001658861292526126
iteration 13, loss = 0.001527693122625351
iteration 14, loss = 0.001314531546086073
iteration 15, loss = 0.0017288492526859045
iteration 16, loss = 0.0020947959274053574
iteration 17, loss = 0.0014103249413892627
iteration 18, loss = 0.0017054300988093019
iteration 19, loss = 0.0015353586059063673
iteration 20, loss = 0.0016393802361562848
iteration 21, loss = 0.0014759260229766369
iteration 22, loss = 0.0014379938365891576
iteration 23, loss = 0.001661490648984909
iteration 24, loss = 0.0033794657792896032
iteration 25, loss = 0.0016850755782797933
iteration 26, loss = 0.0013539333594962955
iteration 27, loss = 0.0029122885316610336
iteration 28, loss = 0.001741827349178493
iteration 29, loss = 0.0013677196111530066
iteration 30, loss = 0.0019837773870676756
iteration 31, loss = 0.0014067526208236814
iteration 32, loss = 0.0016185719287022948
iteration 33, loss = 0.0011863186955451965
iteration 34, loss = 0.001398643827997148
iteration 35, loss = 0.0013852979755029082
iteration 36, loss = 0.0018079644069075584
iteration 37, loss = 0.0012450370704755187
iteration 38, loss = 0.0013272130163386464
iteration 39, loss = 0.0012954242993146181
iteration 40, loss = 0.0020940029062330723
iteration 41, loss = 0.0014347336255013943
iteration 42, loss = 0.001984569476917386
iteration 43, loss = 0.0015182376373559237
iteration 44, loss = 0.001530381035991013
iteration 45, loss = 0.0012406038586050272
iteration 46, loss = 0.0016433417331427336
iteration 47, loss = 0.001728930277749896
iteration 48, loss = 0.0013254618970677257
iteration 49, loss = 0.0021885635796934366
iteration 50, loss = 0.0018505988409742713
iteration 51, loss = 0.001397682004608214
iteration 52, loss = 0.0016894342843443155
iteration 53, loss = 0.0015031491639092565
iteration 54, loss = 0.0014212792739272118
iteration 55, loss = 0.0018918156856670976
iteration 56, loss = 0.0025807651691138744
iteration 57, loss = 0.0015839759726077318
iteration 58, loss = 0.0022716219536960125
iteration 59, loss = 0.0021569314412772655
iteration 60, loss = 0.0011360784992575645
iteration 61, loss = 0.0020311898551881313
iteration 62, loss = 0.0019188618753105402
iteration 63, loss = 0.00144759111572057
iteration 64, loss = 0.001275000162422657
iteration 65, loss = 0.0018288751598447561
iteration 66, loss = 0.0018891114741563797
iteration 67, loss = 0.001265586819499731
iteration 68, loss = 0.0013738879933953285
iteration 69, loss = 0.001414142083376646
iteration 70, loss = 0.0011455754283815622
iteration 71, loss = 0.0013949631247669458
iteration 72, loss = 0.0019896174781024456
iteration 73, loss = 0.0016127275303006172
iteration 74, loss = 0.00153416208922863
iteration 75, loss = 0.0016215045470744371
iteration 76, loss = 0.0013872130075469613
iteration 77, loss = 0.0014560178387910128
iteration 78, loss = 0.0014623049646615982
iteration 79, loss = 0.001264323596842587
iteration 80, loss = 0.002014875179156661
iteration 81, loss = 0.0013822889886796474
iteration 82, loss = 0.0021629189141094685
iteration 83, loss = 0.002062656916677952
iteration 84, loss = 0.0012827747268602252
iteration 85, loss = 0.0013053411385044456
iteration 86, loss = 0.0016016752924770117
iteration 87, loss = 0.0015010213246569037
iteration 88, loss = 0.0016671806806698442
iteration 89, loss = 0.0014693301636725664
iteration 90, loss = 0.00125573156401515
iteration 91, loss = 0.0016171938041225076
iteration 92, loss = 0.001335454173386097
iteration 93, loss = 0.001527726766653359
iteration 94, loss = 0.0014275388093665242
iteration 95, loss = 0.0019316081888973713
iteration 96, loss = 0.0015827834140509367
iteration 97, loss = 0.0019302990986034274
iteration 98, loss = 0.0012238502968102694
iteration 99, loss = 0.0014382832450792193
iteration 100, loss = 0.0013775633415207267
iteration 101, loss = 0.0012582014314830303
iteration 102, loss = 0.0026975299697369337
iteration 103, loss = 0.001255288254469633
iteration 104, loss = 0.001301006879657507
iteration 105, loss = 0.0013215801445767283
iteration 106, loss = 0.001595912384800613
iteration 107, loss = 0.0021996591240167618
iteration 108, loss = 0.0016751656075939536
iteration 109, loss = 0.0026538216043263674
iteration 110, loss = 0.0013798614963889122
iteration 111, loss = 0.0028380604926496744
iteration 112, loss = 0.001280840951949358
iteration 113, loss = 0.0028449937235563993
iteration 114, loss = 0.0012110816314816475
iteration 115, loss = 0.00289669306948781
iteration 116, loss = 0.0016409209929406643
iteration 117, loss = 0.0014259435702115297
iteration 118, loss = 0.001511366106569767
iteration 119, loss = 0.0013165405252948403
iteration 120, loss = 0.0014134058728814125
iteration 121, loss = 0.0014190440997481346
iteration 122, loss = 0.0012555638095363975
iteration 123, loss = 0.0012863079318776727
iteration 124, loss = 0.0013499653432518244
iteration 125, loss = 0.001260066987015307
iteration 126, loss = 0.0020559632685035467
iteration 127, loss = 0.0012406441383063793
iteration 128, loss = 0.0021984425839036703
iteration 129, loss = 0.0036802866961807013
iteration 130, loss = 0.0015315858181566
iteration 131, loss = 0.0013541927328333259
iteration 132, loss = 0.0013467407552525401
iteration 133, loss = 0.0014730857219547033
iteration 134, loss = 0.0025307340547442436
iteration 135, loss = 0.00133533775806427
iteration 136, loss = 0.0012939408188685775
iteration 137, loss = 0.0017275898717343807
iteration 138, loss = 0.0011485933791846037
iteration 139, loss = 0.0013405024074018002
iteration 140, loss = 0.0014403973473235965
iteration 141, loss = 0.0013785577611997724
iteration 142, loss = 0.001444936147890985
iteration 143, loss = 0.0018989513628184795
iteration 144, loss = 0.002432178007438779
iteration 145, loss = 0.003296469571068883
iteration 146, loss = 0.002791265258565545
iteration 147, loss = 0.002802656963467598
iteration 148, loss = 0.0023772239219397306
iteration 149, loss = 0.002426446881145239
iteration 150, loss = 0.0020355849992483854
iteration 151, loss = 0.0014837118797004223
iteration 152, loss = 0.0014930337201803923
iteration 153, loss = 0.0015859630657359958
iteration 154, loss = 0.0012177361641079187
iteration 155, loss = 0.0014279450988397002
iteration 156, loss = 0.0018554024863988161
iteration 157, loss = 0.0012283619726076722
iteration 158, loss = 0.0016013180138543248
iteration 159, loss = 0.0019352346425876021
iteration 160, loss = 0.0015842486172914505
iteration 161, loss = 0.0030642282217741013
iteration 162, loss = 0.0020440551452338696
iteration 163, loss = 0.0014418556820601225
iteration 164, loss = 0.0011703319614753127
iteration 165, loss = 0.0012777105439454317
iteration 166, loss = 0.0014734361320734024
iteration 167, loss = 0.0021080828737467527
iteration 168, loss = 0.0016904466319829226
iteration 169, loss = 0.0016414299607276917
iteration 170, loss = 0.0016319877468049526
iteration 171, loss = 0.002233574166893959
iteration 172, loss = 0.0015220708446577191
iteration 173, loss = 0.0017325575463473797
iteration 174, loss = 0.002970184199512005
iteration 175, loss = 0.001382904825732112
iteration 176, loss = 0.0012053840328007936
iteration 177, loss = 0.0012905526673421264
iteration 178, loss = 0.001700195949524641
iteration 179, loss = 0.0015790758188813925
iteration 180, loss = 0.0012300527887418866
iteration 181, loss = 0.0016711705829948187
iteration 182, loss = 0.0011312717106193304
iteration 183, loss = 0.0014551731292158365
iteration 184, loss = 0.0014859923394396901
iteration 185, loss = 0.0026965143624693155
iteration 186, loss = 0.0017686148639768362
iteration 187, loss = 0.0016631778562441468
iteration 188, loss = 0.0013842979678884149
iteration 189, loss = 0.001361067988909781
iteration 190, loss = 0.0014995277160778642
iteration 191, loss = 0.001816732925362885
iteration 192, loss = 0.0016210376052185893
iteration 193, loss = 0.0014051651814952493
iteration 194, loss = 0.0018041656585410237
iteration 195, loss = 0.0012955886777490377
iteration 196, loss = 0.0012589541729539633
iteration 197, loss = 0.0011736716842278838
iteration 198, loss = 0.0019714615773409605
iteration 199, loss = 0.0013572190655395389
iteration 200, loss = 0.0015620390186086297
iteration 201, loss = 0.0014016006607562304
iteration 202, loss = 0.0013965893303975463
iteration 203, loss = 0.0015436168760061264
iteration 204, loss = 0.001299403258599341
iteration 205, loss = 0.001405700109899044
iteration 206, loss = 0.0028234445489943027
iteration 207, loss = 0.001389227225445211
iteration 208, loss = 0.0014244015328586102
iteration 209, loss = 0.0011626724153757095
iteration 210, loss = 0.002020742278546095
iteration 211, loss = 0.0013808199437335134
iteration 212, loss = 0.001341383089311421
iteration 213, loss = 0.0012957712169736624
iteration 214, loss = 0.00194795080460608
iteration 215, loss = 0.0011149313068017364
iteration 216, loss = 0.0012467358028516173
iteration 217, loss = 0.0014806556282564998
iteration 218, loss = 0.001422364148311317
iteration 219, loss = 0.0014159101992845535
iteration 220, loss = 0.00111299694981426
iteration 221, loss = 0.001854612841270864
iteration 222, loss = 0.0016613896004855633
iteration 223, loss = 0.0017854507314041257
iteration 224, loss = 0.001179391285404563
iteration 225, loss = 0.0016832435503602028
iteration 226, loss = 0.0019137327326461673
iteration 227, loss = 0.0014963268768042326
iteration 228, loss = 0.001435641897842288
iteration 229, loss = 0.001648241770453751
iteration 230, loss = 0.0018844847800210118
iteration 231, loss = 0.0010588794248178601
iteration 232, loss = 0.00137117481790483
iteration 233, loss = 0.001550773624330759
iteration 234, loss = 0.001911260886117816
iteration 235, loss = 0.001330516766756773
iteration 236, loss = 0.001954959239810705
iteration 237, loss = 0.0011887457221746445
iteration 238, loss = 0.002566540613770485
iteration 239, loss = 0.002018742263317108
iteration 240, loss = 0.001828886684961617
iteration 241, loss = 0.001487439963966608
iteration 242, loss = 0.0018614690052345395
iteration 243, loss = 0.0014681287575513124
iteration 244, loss = 0.0012309456942602992
iteration 245, loss = 0.0019113841699436307
iteration 246, loss = 0.0027346161659806967
iteration 247, loss = 0.0013474664883688092
iteration 248, loss = 0.0012957979924976826
iteration 249, loss = 0.0012125854846090078
iteration 250, loss = 0.001682814210653305
iteration 251, loss = 0.0014775532763451338
iteration 252, loss = 0.001437490340322256
iteration 253, loss = 0.0012020380236208439
iteration 254, loss = 0.001371206366457045
iteration 255, loss = 0.0011887926375493407
iteration 256, loss = 0.0012388918548822403
iteration 257, loss = 0.0017197559354826808
iteration 258, loss = 0.0016450582770630717
iteration 259, loss = 0.0014582228614017367
iteration 260, loss = 0.0012320940149948
iteration 261, loss = 0.001526702311821282
iteration 262, loss = 0.001224897219799459
iteration 263, loss = 0.002305353293195367
iteration 264, loss = 0.0013717289548367262
iteration 265, loss = 0.0012917113490402699
iteration 266, loss = 0.0013704324373975396
iteration 267, loss = 0.0016288227634504437
iteration 268, loss = 0.0025695855729281902
iteration 269, loss = 0.0010793647961691022
iteration 270, loss = 0.001904032425954938
iteration 271, loss = 0.0016664535505697131
iteration 272, loss = 0.001481044222600758
iteration 273, loss = 0.0017157966503873467
iteration 274, loss = 0.002174793742597103
iteration 275, loss = 0.0018267275299876928
iteration 276, loss = 0.0014296132139861584
iteration 277, loss = 0.0014676495920866728
iteration 278, loss = 0.0013240675907582045
iteration 279, loss = 0.001687301555648446
iteration 280, loss = 0.0015806901501491666
iteration 281, loss = 0.0012989831157028675
iteration 282, loss = 0.0017809274140745401
iteration 283, loss = 0.0019196596695110202
iteration 284, loss = 0.0015623787185177207
iteration 285, loss = 0.0016905218362808228
iteration 286, loss = 0.0015029648784548044
iteration 287, loss = 0.002420434495434165
iteration 288, loss = 0.0017231270903721452
iteration 289, loss = 0.0014735818840563297
iteration 290, loss = 0.002349911257624626
iteration 291, loss = 0.0013840836472809315
iteration 292, loss = 0.0013365267077460885
iteration 293, loss = 0.0013833945849910378
iteration 294, loss = 0.0012901235604658723
iteration 295, loss = 0.001622393960133195
iteration 296, loss = 0.0024468766059726477
iteration 297, loss = 0.0017044682754203677
iteration 298, loss = 0.001292476081289351
iteration 299, loss = 0.0013775528641417623
iteration 300, loss = 0.0015396917006000876
iteration 1, loss = 0.0018073006067425013
iteration 2, loss = 0.0012982391053810716
iteration 3, loss = 0.0014260713942348957
iteration 4, loss = 0.0012359099928289652
iteration 5, loss = 0.0016305430326610804
iteration 6, loss = 0.0019008322851732373
iteration 7, loss = 0.0013051626738160849
iteration 8, loss = 0.0013298257254064083
iteration 9, loss = 0.0014421911910176277
iteration 10, loss = 0.0025869414675980806
iteration 11, loss = 0.0013027879176661372
iteration 12, loss = 0.002149182604625821
iteration 13, loss = 0.0014932510675862432
iteration 14, loss = 0.0013274966040626168
iteration 15, loss = 0.0011428133584558964
iteration 16, loss = 0.0013274515513330698
iteration 17, loss = 0.003624039003625512
iteration 18, loss = 0.0012904026079922915
iteration 19, loss = 0.0015947403153404593
iteration 20, loss = 0.001253744587302208
iteration 21, loss = 0.0025921380147337914
iteration 22, loss = 0.0014362186193466187
iteration 23, loss = 0.0018474254757165909
iteration 24, loss = 0.001384223927743733
iteration 25, loss = 0.0014184215106070042
iteration 26, loss = 0.0015049673384055495
iteration 27, loss = 0.002725632395595312
iteration 28, loss = 0.0014112892094999552
iteration 29, loss = 0.0012290321756154299
iteration 30, loss = 0.0011608583154156804
iteration 31, loss = 0.001618637703359127
iteration 32, loss = 0.0012544359778985381
iteration 33, loss = 0.0014349437551572919
iteration 34, loss = 0.001202533021569252
iteration 35, loss = 0.001376478816382587
iteration 36, loss = 0.0012940968153998256
iteration 37, loss = 0.0014080983819440007
iteration 38, loss = 0.0014228146756067872
iteration 39, loss = 0.001419731299392879
iteration 40, loss = 0.0024113079998642206
iteration 41, loss = 0.0014236059505492449
iteration 42, loss = 0.0011827613925561309
iteration 43, loss = 0.0013569053262472153
iteration 44, loss = 0.0012188174296170473
iteration 45, loss = 0.0014878507936373353
iteration 46, loss = 0.001716047408990562
iteration 47, loss = 0.0012942596804350615
iteration 48, loss = 0.0018859198316931725
iteration 49, loss = 0.001180976047180593
iteration 50, loss = 0.0018179661128669977
iteration 51, loss = 0.001379696768708527
iteration 52, loss = 0.001173205440863967
iteration 53, loss = 0.0016409497475251555
iteration 54, loss = 0.0016525991959497333
iteration 55, loss = 0.0018163223285228014
iteration 56, loss = 0.0013408833183348179
iteration 57, loss = 0.0018777621444314718
iteration 58, loss = 0.001372765633277595
iteration 59, loss = 0.0013826692011207342
iteration 60, loss = 0.0015586555236950517
iteration 61, loss = 0.0017692646943032742
iteration 62, loss = 0.0027229548431932926
iteration 63, loss = 0.0012731862952932715
iteration 64, loss = 0.0019826351199299097
iteration 65, loss = 0.001767394132912159
iteration 66, loss = 0.001436102669686079
iteration 67, loss = 0.0011110124178230762
iteration 68, loss = 0.0012555557768791914
iteration 69, loss = 0.0016742274165153503
iteration 70, loss = 0.0013403781922534108
iteration 71, loss = 0.0015702645760029554
iteration 72, loss = 0.0011735070729628205
iteration 73, loss = 0.001373946899548173
iteration 74, loss = 0.0016433695564046502
iteration 75, loss = 0.0013310604263097048
iteration 76, loss = 0.0013145690318197012
iteration 77, loss = 0.0014281836338341236
iteration 78, loss = 0.0018430978525429964
iteration 79, loss = 0.0016664504073560238
iteration 80, loss = 0.0012709179427474737
iteration 81, loss = 0.002717923140153289
iteration 82, loss = 0.0013074041344225407
iteration 83, loss = 0.0025138931814581156
iteration 84, loss = 0.0009761833352968097
iteration 85, loss = 0.0010969543363898993
iteration 86, loss = 0.002667438006028533
iteration 87, loss = 0.001163342036306858
iteration 88, loss = 0.001986543880775571
iteration 89, loss = 0.0012988619273528457
iteration 90, loss = 0.0023677831050008535
iteration 91, loss = 0.0014222819590941072
iteration 92, loss = 0.0013048428809270263
iteration 93, loss = 0.001309026381932199
iteration 94, loss = 0.0012011824874207377
iteration 95, loss = 0.001458031008951366
iteration 96, loss = 0.0014076486695557833
iteration 97, loss = 0.0014181688893586397
iteration 98, loss = 0.0013072171714156866
iteration 99, loss = 0.001252341433428228
iteration 100, loss = 0.0017417683266103268
iteration 101, loss = 0.0014994600787758827
iteration 102, loss = 0.0012222018558532
iteration 103, loss = 0.0019562740344554186
iteration 104, loss = 0.003426600946113467
iteration 105, loss = 0.001663594855926931
iteration 106, loss = 0.0015125826466828585
iteration 107, loss = 0.0017332311253994703
iteration 108, loss = 0.0011072601191699505
iteration 109, loss = 0.001241051941178739
iteration 110, loss = 0.0018009651685133576
iteration 111, loss = 0.0012978530721738935
iteration 112, loss = 0.0013023336650803685
iteration 113, loss = 0.001361930277198553
iteration 114, loss = 0.001681607449427247
iteration 115, loss = 0.0011469610035419464
iteration 116, loss = 0.0012727186549454927
iteration 117, loss = 0.0011988802580162883
iteration 118, loss = 0.0012679693754762411
iteration 119, loss = 0.0012499478179961443
iteration 120, loss = 0.0011127932230010629
iteration 121, loss = 0.001354091102257371
iteration 122, loss = 0.0012677039485424757
iteration 123, loss = 0.0011791596189141273
iteration 124, loss = 0.0014461568789556623
iteration 125, loss = 0.0014423299580812454
iteration 126, loss = 0.001227794447913766
iteration 127, loss = 0.0018027016194537282
iteration 128, loss = 0.0017430796287953854
iteration 129, loss = 0.0013045559171587229
iteration 130, loss = 0.001571321627125144
iteration 131, loss = 0.00188443751540035
iteration 132, loss = 0.0011981141287833452
iteration 133, loss = 0.001275861170142889
iteration 134, loss = 0.0014097203966230154
iteration 135, loss = 0.0012634395388886333
iteration 136, loss = 0.002631076844409108
iteration 137, loss = 0.0015293265460059047
iteration 138, loss = 0.0019400662276893854
iteration 139, loss = 0.0024509362410753965
iteration 140, loss = 0.0011534022632986307
iteration 141, loss = 0.001276217051781714
iteration 142, loss = 0.0011382645461708307
iteration 143, loss = 0.0012549038510769606
iteration 144, loss = 0.0017726040678098798
iteration 145, loss = 0.0014684548368677497
iteration 146, loss = 0.0015184530057013035
iteration 147, loss = 0.0015524982009083033
iteration 148, loss = 0.002431724453344941
iteration 149, loss = 0.0014750680420547724
iteration 150, loss = 0.0015652392758056521
iteration 151, loss = 0.0014528029132634401
iteration 152, loss = 0.002304967725649476
iteration 153, loss = 0.0011603266466408968
iteration 154, loss = 0.002274571219459176
iteration 155, loss = 0.0012018088018521667
iteration 156, loss = 0.0014782720245420933
iteration 157, loss = 0.0013452902203425765
iteration 158, loss = 0.0014475308125838637
iteration 159, loss = 0.0014857174828648567
iteration 160, loss = 0.0012774346396327019
iteration 161, loss = 0.0012101407628506422
iteration 162, loss = 0.0012083493638783693
iteration 163, loss = 0.0010487945983186364
iteration 164, loss = 0.0011847605928778648
iteration 165, loss = 0.002964256564155221
iteration 166, loss = 0.002065830398350954
iteration 167, loss = 0.0012566152727231383
iteration 168, loss = 0.0012570307590067387
iteration 169, loss = 0.0012469271896407008
iteration 170, loss = 0.0012637219624593854
iteration 171, loss = 0.0014530811458826065
iteration 172, loss = 0.0013603783445432782
iteration 173, loss = 0.001225834246724844
iteration 174, loss = 0.0011379263596609235
iteration 175, loss = 0.0014321054331958294
iteration 176, loss = 0.0013805640628561378
iteration 177, loss = 0.0013481654459610581
iteration 178, loss = 0.00107385846786201
iteration 179, loss = 0.0016344671603292227
iteration 180, loss = 0.001287719001993537
iteration 181, loss = 0.0011620810255408287
iteration 182, loss = 0.0011864886619150639
iteration 183, loss = 0.0017191030783578753
iteration 184, loss = 0.0012555320281535387
iteration 185, loss = 0.001232566894032061
iteration 186, loss = 0.001334668486379087
iteration 187, loss = 0.0010434878058731556
iteration 188, loss = 0.001838551601395011
iteration 189, loss = 0.0016292708460241556
iteration 190, loss = 0.0018569770036265254
iteration 191, loss = 0.0010802170727401972
iteration 192, loss = 0.001034573302604258
iteration 193, loss = 0.0012625915696844459
iteration 194, loss = 0.0011378169292584062
iteration 195, loss = 0.001141146756708622
iteration 196, loss = 0.001133288023993373
iteration 197, loss = 0.0014108398463577032
iteration 198, loss = 0.0014950997428968549
iteration 199, loss = 0.0010413589188829064
iteration 200, loss = 0.0014344742521643639
iteration 201, loss = 0.0021968495566397905
iteration 202, loss = 0.0014236199203878641
iteration 203, loss = 0.0014825805556029081
iteration 204, loss = 0.0011661631288006902
iteration 205, loss = 0.002361504826694727
iteration 206, loss = 0.001961349742487073
iteration 207, loss = 0.0011240238090977073
iteration 208, loss = 0.0012849464546889067
iteration 209, loss = 0.0010539514478296041
iteration 210, loss = 0.0013078503543511033
iteration 211, loss = 0.001385592739097774
iteration 212, loss = 0.0014601335860788822
iteration 213, loss = 0.0011840539518743753
iteration 214, loss = 0.0014526578597724438
iteration 215, loss = 0.0014218379510566592
iteration 216, loss = 0.0015287988353520632
iteration 217, loss = 0.0011038794182240963
iteration 218, loss = 0.001148881739936769
iteration 219, loss = 0.0010907261166721582
iteration 220, loss = 0.0012412478681653738
iteration 221, loss = 0.0016567890997976065
iteration 222, loss = 0.0017777974717319012
iteration 223, loss = 0.0014672128017991781
iteration 224, loss = 0.0014648546930402517
iteration 225, loss = 0.0013220632681623101
iteration 226, loss = 0.0013455018633976579
iteration 227, loss = 0.0012941138120368123
iteration 228, loss = 0.0019505154341459274
iteration 229, loss = 0.002306231763213873
iteration 230, loss = 0.0018376114312559366
iteration 231, loss = 0.0011634601978585124
iteration 232, loss = 0.0011354665039107203
iteration 233, loss = 0.0013478425098583102
iteration 234, loss = 0.0016343796160072088
iteration 235, loss = 0.0016742366133257747
iteration 236, loss = 0.0018340055830776691
iteration 237, loss = 0.0014203691389411688
iteration 238, loss = 0.0010910863056778908
iteration 239, loss = 0.0023891415912657976
iteration 240, loss = 0.0016279690898954868
iteration 241, loss = 0.0011556167155504227
iteration 242, loss = 0.001757267047651112
iteration 243, loss = 0.0018332655308768153
iteration 244, loss = 0.001141717191785574
iteration 245, loss = 0.0016423329943791032
iteration 246, loss = 0.0014841341180726886
iteration 247, loss = 0.0010689909104257822
iteration 248, loss = 0.0013464693911373615
iteration 249, loss = 0.002592936623841524
iteration 250, loss = 0.0010705039603635669
iteration 251, loss = 0.0018420281121507287
iteration 252, loss = 0.001683384063653648
iteration 253, loss = 0.0013430244289338589
iteration 254, loss = 0.0014483279082924128
iteration 255, loss = 0.0010088877752423286
iteration 256, loss = 0.0012593746650964022
iteration 257, loss = 0.0010529112769290805
iteration 258, loss = 0.0013286347966641188
iteration 259, loss = 0.002522585215047002
iteration 260, loss = 0.0021036025136709213
iteration 261, loss = 0.0011310501722618937
iteration 262, loss = 0.0013096944894641638
iteration 263, loss = 0.001174149103462696
iteration 264, loss = 0.0015846778405830264
iteration 265, loss = 0.0013945297105237842
iteration 266, loss = 0.001273344736546278
iteration 267, loss = 0.0015698213828727603
iteration 268, loss = 0.0017080605030059814
iteration 269, loss = 0.0015173221472650766
iteration 270, loss = 0.0013298243284225464
iteration 271, loss = 0.0012202633079141378
iteration 272, loss = 0.0014228887157514691
iteration 273, loss = 0.0021708160638809204
iteration 274, loss = 0.002666300628334284
iteration 275, loss = 0.0010976623743772507
iteration 276, loss = 0.00281182280741632
iteration 277, loss = 0.0012723419349640608
iteration 278, loss = 0.0016701070126146078
iteration 279, loss = 0.0017164688324555755
iteration 280, loss = 0.0010620306711643934
iteration 281, loss = 0.0021873116493225098
iteration 282, loss = 0.001866369741037488
iteration 283, loss = 0.0018171269912272692
iteration 284, loss = 0.0013660816475749016
iteration 285, loss = 0.0013817724538967013
iteration 286, loss = 0.0022163500543683767
iteration 287, loss = 0.001183262444101274
iteration 288, loss = 0.001201450009830296
iteration 289, loss = 0.0011293445713818073
iteration 290, loss = 0.001745033310726285
iteration 291, loss = 0.0012996385339647532
iteration 292, loss = 0.0024811183102428913
iteration 293, loss = 0.0011956089874729514
iteration 294, loss = 0.0012331422185525298
iteration 295, loss = 0.0009890958899632096
iteration 296, loss = 0.0019943288061767817
iteration 297, loss = 0.00141153356526047
iteration 298, loss = 0.0013562031090259552
iteration 299, loss = 0.0013289988273754716
iteration 300, loss = 0.0017518788808956742
iteration 1, loss = 0.001365378499031067
iteration 2, loss = 0.0023878272622823715
iteration 3, loss = 0.0012702917447313666
iteration 4, loss = 0.0011829383438453078
iteration 5, loss = 0.0016430782852694392
iteration 6, loss = 0.001832829206250608
iteration 7, loss = 0.0011526065645739436
iteration 8, loss = 0.0015666390536352992
iteration 9, loss = 0.0017332097049802542
iteration 10, loss = 0.0016367817297577858
iteration 11, loss = 0.00100066012237221
iteration 12, loss = 0.0014009426813572645
iteration 13, loss = 0.0011616344563663006
iteration 14, loss = 0.0013717159163206816
iteration 15, loss = 0.0014449317241087556
iteration 16, loss = 0.001254513394087553
iteration 17, loss = 0.0011204713955521584
iteration 18, loss = 0.0015794653445482254
iteration 19, loss = 0.002274865983054042
iteration 20, loss = 0.0012520035961642861
iteration 21, loss = 0.0014632693491876125
iteration 22, loss = 0.0014511977788060904
iteration 23, loss = 0.001536347670480609
iteration 24, loss = 0.0011326255043968558
iteration 25, loss = 0.0012673716992139816
iteration 26, loss = 0.0009846182074397802
iteration 27, loss = 0.0015342030674219131
iteration 28, loss = 0.0013179853558540344
iteration 29, loss = 0.0025539956986904144
iteration 30, loss = 0.0012614417355507612
iteration 31, loss = 0.0013503262307494879
iteration 32, loss = 0.0013294725213199854
iteration 33, loss = 0.0013759869616478682
iteration 34, loss = 0.0013204521965235472
iteration 35, loss = 0.002626283559948206
iteration 36, loss = 0.00135481683537364
iteration 37, loss = 0.0013933024602010846
iteration 38, loss = 0.0013964975951239467
iteration 39, loss = 0.0016479421174153686
iteration 40, loss = 0.0013377078576013446
iteration 41, loss = 0.0015028601046651602
iteration 42, loss = 0.0012041705194860697
iteration 43, loss = 0.0012903119204565883
iteration 44, loss = 0.0021920837461948395
iteration 45, loss = 0.002053268952295184
iteration 46, loss = 0.001787123503163457
iteration 47, loss = 0.0009045217302627861
iteration 48, loss = 0.0012568689417093992
iteration 49, loss = 0.0011418864596635103
iteration 50, loss = 0.0015802074922248721
iteration 51, loss = 0.0019890754483640194
iteration 52, loss = 0.0014107947936281562
iteration 53, loss = 0.0012864847667515278
iteration 54, loss = 0.0011957617243751884
iteration 55, loss = 0.0010673856595531106
iteration 56, loss = 0.0011834587203338742
iteration 57, loss = 0.001335501903668046
iteration 58, loss = 0.0016473159193992615
iteration 59, loss = 0.0013166898861527443
iteration 60, loss = 0.0010294083040207624
iteration 61, loss = 0.0013228162424638867
iteration 62, loss = 0.0013307657791301608
iteration 63, loss = 0.0017963037826120853
iteration 64, loss = 0.0013552685268223286
iteration 65, loss = 0.001073421910405159
iteration 66, loss = 0.0010587243596091866
iteration 67, loss = 0.0013098460622131824
iteration 68, loss = 0.001646592398174107
iteration 69, loss = 0.001272491062991321
iteration 70, loss = 0.0012024522293359041
iteration 71, loss = 0.0011924051214009523
iteration 72, loss = 0.0010655979858711362
iteration 73, loss = 0.0012416511308401823
iteration 74, loss = 0.001185511122457683
iteration 75, loss = 0.0012829139595851302
iteration 76, loss = 0.0020560366101562977
iteration 77, loss = 0.0011903926497325301
iteration 78, loss = 0.001074425526894629
iteration 79, loss = 0.0010484263766556978
iteration 80, loss = 0.0013675831723958254
iteration 81, loss = 0.0011100397678092122
iteration 82, loss = 0.0021611179690808058
iteration 83, loss = 0.0014054073253646493
iteration 84, loss = 0.0016793805407360196
iteration 85, loss = 0.0029572637286037207
iteration 86, loss = 0.0012564019998535514
iteration 87, loss = 0.0015309244627133012
iteration 88, loss = 0.0015025981701910496
iteration 89, loss = 0.001709209755063057
iteration 90, loss = 0.0013392292894423008
iteration 91, loss = 0.0011094848159700632
iteration 92, loss = 0.0012130248360335827
iteration 93, loss = 0.0010249633342027664
iteration 94, loss = 0.0014107830356806517
iteration 95, loss = 0.001258016680367291
iteration 96, loss = 0.0014132052892819047
iteration 97, loss = 0.0013496109750121832
iteration 98, loss = 0.0013803275069221854
iteration 99, loss = 0.0010923061054199934
iteration 100, loss = 0.0015235673636198044
iteration 101, loss = 0.002303444780409336
iteration 102, loss = 0.00114078086335212
iteration 103, loss = 0.00230838218703866
iteration 104, loss = 0.001152680371887982
iteration 105, loss = 0.0012743776896968484
iteration 106, loss = 0.0016714738449081779
iteration 107, loss = 0.0012546462239697576
iteration 108, loss = 0.0011625050101429224
iteration 109, loss = 0.0013018760364502668
iteration 110, loss = 0.0011982922442257404
iteration 111, loss = 0.0024599034804850817
iteration 112, loss = 0.0016559027135372162
iteration 113, loss = 0.0015115259448066354
iteration 114, loss = 0.0015567205846309662
iteration 115, loss = 0.0012612768914550543
iteration 116, loss = 0.0011354911839589477
iteration 117, loss = 0.001124540576711297
iteration 118, loss = 0.0018918568966910243
iteration 119, loss = 0.0011577940313145518
iteration 120, loss = 0.001618855632841587
iteration 121, loss = 0.0012638145126402378
iteration 122, loss = 0.0010918492916971445
iteration 123, loss = 0.0017308611422777176
iteration 124, loss = 0.001079958281479776
iteration 125, loss = 0.0011017946526408195
iteration 126, loss = 0.001068416517227888
iteration 127, loss = 0.0011720561888068914
iteration 128, loss = 0.0014416510239243507
iteration 129, loss = 0.0010907599935308099
iteration 130, loss = 0.0010039742337539792
iteration 131, loss = 0.0013966459082439542
iteration 132, loss = 0.0014368400443345308
iteration 133, loss = 0.0013843150809407234
iteration 134, loss = 0.0018137512961402535
iteration 135, loss = 0.0011830417206510901
iteration 136, loss = 0.0011403847020119429
iteration 137, loss = 0.0012566022342070937
iteration 138, loss = 0.0016650422476232052
iteration 139, loss = 0.0016067783581092954
iteration 140, loss = 0.0012044064933434129
iteration 141, loss = 0.0012201848439872265
iteration 142, loss = 0.0013819836312904954
iteration 143, loss = 0.001073207939043641
iteration 144, loss = 0.0012691000010818243
iteration 145, loss = 0.0009070925880223513
iteration 146, loss = 0.0032960311509668827
iteration 147, loss = 0.0012064252514392138
iteration 148, loss = 0.0011188341304659843
iteration 149, loss = 0.0015676190378144383
iteration 150, loss = 0.001953323371708393
iteration 151, loss = 0.001025369972921908
iteration 152, loss = 0.001097044674679637
iteration 153, loss = 0.00245608645491302
iteration 154, loss = 0.0021110782399773598
iteration 155, loss = 0.0024118986912071705
iteration 156, loss = 0.001057882560417056
iteration 157, loss = 0.0015283203683793545
iteration 158, loss = 0.0018809788161888719
iteration 159, loss = 0.0014909286983311176
iteration 160, loss = 0.0016688904725015163
iteration 161, loss = 0.0012061239685863256
iteration 162, loss = 0.0012037409469485283
iteration 163, loss = 0.0011460884707048535
iteration 164, loss = 0.0012800374533981085
iteration 165, loss = 0.0012263614917173982
iteration 166, loss = 0.0014146139146760106
iteration 167, loss = 0.0012800164986401796
iteration 168, loss = 0.0010317886481061578
iteration 169, loss = 0.0017724432982504368
iteration 170, loss = 0.0011429545702412724
iteration 171, loss = 0.0015019746497273445
iteration 172, loss = 0.0011915415525436401
iteration 173, loss = 0.0011058623204007745
iteration 174, loss = 0.0014434695476666093
iteration 175, loss = 0.00235865474678576
iteration 176, loss = 0.0014933180063962936
iteration 177, loss = 0.002375642769038677
iteration 178, loss = 0.0013591317692771554
iteration 179, loss = 0.0010596178472042084
iteration 180, loss = 0.0014075851067900658
iteration 181, loss = 0.001143721048720181
iteration 182, loss = 0.0011202606838196516
iteration 183, loss = 0.001688330783508718
iteration 184, loss = 0.0013162586838006973
iteration 185, loss = 0.0011899350211024284
iteration 186, loss = 0.0018116668798029423
iteration 187, loss = 0.002329226117581129
iteration 188, loss = 0.0011179065331816673
iteration 189, loss = 0.0012078183935955167
iteration 190, loss = 0.0024311020970344543
iteration 191, loss = 0.0013740032445639372
iteration 192, loss = 0.0016025191871449351
iteration 193, loss = 0.0010164226405322552
iteration 194, loss = 0.0021215886808931828
iteration 195, loss = 0.0009977592853829265
iteration 196, loss = 0.0009415012900717556
iteration 197, loss = 0.0012976162834092975
iteration 198, loss = 0.0010990842711180449
iteration 199, loss = 0.0015537362778559327
iteration 200, loss = 0.0010543811367824674
iteration 201, loss = 0.0011796692851930857
iteration 202, loss = 0.001325245131738484
iteration 203, loss = 0.001076594926416874
iteration 204, loss = 0.0015044166939333081
iteration 205, loss = 0.0010210579494014382
iteration 206, loss = 0.0010882517090067267
iteration 207, loss = 0.002798524685204029
iteration 208, loss = 0.0017564937006682158
iteration 209, loss = 0.0010862587951123714
iteration 210, loss = 0.001486772671341896
iteration 211, loss = 0.0015780857065692544
iteration 212, loss = 0.0016213154885917902
iteration 213, loss = 0.0012065692571923137
iteration 214, loss = 0.0012549373786896467
iteration 215, loss = 0.001117556937970221
iteration 216, loss = 0.0010902539361268282
iteration 217, loss = 0.0010254619410261512
iteration 218, loss = 0.0011758196633309126
iteration 219, loss = 0.0012574107386171818
iteration 220, loss = 0.0011440602829679847
iteration 221, loss = 0.0010770485969260335
iteration 222, loss = 0.002194129629060626
iteration 223, loss = 0.0011163210729137063
iteration 224, loss = 0.0012416813988238573
iteration 225, loss = 0.0015476078260689974
iteration 226, loss = 0.00160336890257895
iteration 227, loss = 0.0015972129767760634
iteration 228, loss = 0.001229708781465888
iteration 229, loss = 0.0017535904189571738
iteration 230, loss = 0.0010016912128776312
iteration 231, loss = 0.0016951197758316994
iteration 232, loss = 0.0022693874780088663
iteration 233, loss = 0.0014143113512545824
iteration 234, loss = 0.0018829532200470567
iteration 235, loss = 0.0011832519667223096
iteration 236, loss = 0.0015915246913209558
iteration 237, loss = 0.001175922341644764
iteration 238, loss = 0.0010770511580631137
iteration 239, loss = 0.0010586221469566226
iteration 240, loss = 0.001170523464679718
iteration 241, loss = 0.001156242098659277
iteration 242, loss = 0.0010675283847376704
iteration 243, loss = 0.0013158678775653243
iteration 244, loss = 0.001083080773241818
iteration 245, loss = 0.0028779450803995132
iteration 246, loss = 0.002048950642347336
iteration 247, loss = 0.0022785880137234926
iteration 248, loss = 0.0012812286149710417
iteration 249, loss = 0.0010641749249771237
iteration 250, loss = 0.0015773335471749306
iteration 251, loss = 0.0009396469686180353
iteration 252, loss = 0.0024021826684474945
iteration 253, loss = 0.0015215183375403285
iteration 254, loss = 0.0011225523194298148
iteration 255, loss = 0.0012018086854368448
iteration 256, loss = 0.0012126044603064656
iteration 257, loss = 0.0009795944206416607
iteration 258, loss = 0.001375136198475957
iteration 259, loss = 0.0015025560278445482
iteration 260, loss = 0.0010197190567851067
iteration 261, loss = 0.0013302444713190198
iteration 262, loss = 0.0011755054583773017
iteration 263, loss = 0.0017106380546465516
iteration 264, loss = 0.001260001095943153
iteration 265, loss = 0.0012037123087793589
iteration 266, loss = 0.0013079734053462744
iteration 267, loss = 0.0011117543326690793
iteration 268, loss = 0.0011775379534810781
iteration 269, loss = 0.001146780326962471
iteration 270, loss = 0.0010870287660509348
iteration 271, loss = 0.0011617940617725253
iteration 272, loss = 0.001309940591454506
iteration 273, loss = 0.0015566000947728753
iteration 274, loss = 0.0010107086272910237
iteration 275, loss = 0.0013307882472872734
iteration 276, loss = 0.0011459516827017069
iteration 277, loss = 0.0011915970826521516
iteration 278, loss = 0.0013823629124090075
iteration 279, loss = 0.0015173382125794888
iteration 280, loss = 0.0011936344671994448
iteration 281, loss = 0.000982969650067389
iteration 282, loss = 0.00114665855653584
iteration 283, loss = 0.0010267972247675061
iteration 284, loss = 0.0013489016564562917
iteration 285, loss = 0.002113369759172201
iteration 286, loss = 0.0012091761454939842
iteration 287, loss = 0.001311827334575355
iteration 288, loss = 0.001768458285368979
iteration 289, loss = 0.00133365613874048
iteration 290, loss = 0.000975544098764658
iteration 291, loss = 0.00121006288100034
iteration 292, loss = 0.001386187388561666
iteration 293, loss = 0.001067355740815401
iteration 294, loss = 0.0015164762735366821
iteration 295, loss = 0.001106267562136054
iteration 296, loss = 0.0011597690172493458
iteration 297, loss = 0.0009788725292310119
iteration 298, loss = 0.0012890517245978117
iteration 299, loss = 0.0011154240928590298
iteration 300, loss = 0.0010595646454021335
iteration 1, loss = 0.0016260346164926887
iteration 2, loss = 0.0021308748982846737
iteration 3, loss = 0.0009930336382240057
iteration 4, loss = 0.001596742426045239
iteration 5, loss = 0.001059038913808763
iteration 6, loss = 0.0013784033944830298
iteration 7, loss = 0.0013488505501300097
iteration 8, loss = 0.0010929610580205917
iteration 9, loss = 0.002063245978206396
iteration 10, loss = 0.001184712746180594
iteration 11, loss = 0.0013994075125083327
iteration 12, loss = 0.0012335579376667738
iteration 13, loss = 0.002008660463616252
iteration 14, loss = 0.001093496335670352
iteration 15, loss = 0.0011907221050933003
iteration 16, loss = 0.0010753166861832142
iteration 17, loss = 0.0016921013593673706
iteration 18, loss = 0.0012225109385326505
iteration 19, loss = 0.001744381501339376
iteration 20, loss = 0.0011500224936753511
iteration 21, loss = 0.0013594087213277817
iteration 22, loss = 0.0014700554311275482
iteration 23, loss = 0.0009534929413348436
iteration 24, loss = 0.0017142950091511011
iteration 25, loss = 0.0018250348512083292
iteration 26, loss = 0.0010187954176217318
iteration 27, loss = 0.001596182817593217
iteration 28, loss = 0.0013321451842784882
iteration 29, loss = 0.0009870153153315187
iteration 30, loss = 0.0012346114963293076
iteration 31, loss = 0.001238650525920093
iteration 32, loss = 0.0011891693575307727
iteration 33, loss = 0.0016996321501210332
iteration 34, loss = 0.0010665341978892684
iteration 35, loss = 0.0010402294574305415
iteration 36, loss = 0.0010798972798511386
iteration 37, loss = 0.0011361754732206464
iteration 38, loss = 0.001220277394168079
iteration 39, loss = 0.0011414741165935993
iteration 40, loss = 0.0018500798614695668
iteration 41, loss = 0.0018026870675384998
iteration 42, loss = 0.0009985556825995445
iteration 43, loss = 0.0013215381186455488
iteration 44, loss = 0.000909001100808382
iteration 45, loss = 0.0010779325384646654
iteration 46, loss = 0.0015295088523998857
iteration 47, loss = 0.001186060020700097
iteration 48, loss = 0.0011281040497124195
iteration 49, loss = 0.0010596343781799078
iteration 50, loss = 0.0014779465273022652
iteration 51, loss = 0.0019008868839591742
iteration 52, loss = 0.0010279041016474366
iteration 53, loss = 0.0012674032477661967
iteration 54, loss = 0.0012090826639905572
iteration 55, loss = 0.001912584062665701
iteration 56, loss = 0.0011420404771342874
iteration 57, loss = 0.001202854560688138
iteration 58, loss = 0.0009721468086354434
iteration 59, loss = 0.0009461286826990545
iteration 60, loss = 0.0023120935074985027
iteration 61, loss = 0.0009954157285392284
iteration 62, loss = 0.0010925232199952006
iteration 63, loss = 0.0012906830525025725
iteration 64, loss = 0.001613536151126027
iteration 65, loss = 0.0012483996106311679
iteration 66, loss = 0.0011043304111808538
iteration 67, loss = 0.0011380311334505677
iteration 68, loss = 0.0024633335415273905
iteration 69, loss = 0.0011695154244080186
iteration 70, loss = 0.0011503418209031224
iteration 71, loss = 0.0023608452174812555
iteration 72, loss = 0.001222814666107297
iteration 73, loss = 0.001362830982543528
iteration 74, loss = 0.001509944791905582
iteration 75, loss = 0.0011818519560620189
iteration 76, loss = 0.001071579405106604
iteration 77, loss = 0.0014440887607634068
iteration 78, loss = 0.0011036634678021073
iteration 79, loss = 0.0011333139846101403
iteration 80, loss = 0.0029311843682080507
iteration 81, loss = 0.0011841426603496075
iteration 82, loss = 0.0010752605739980936
iteration 83, loss = 0.0018212212016806006
iteration 84, loss = 0.0010459423065185547
iteration 85, loss = 0.0010985024273395538
iteration 86, loss = 0.0011335976887494326
iteration 87, loss = 0.0011058601085096598
iteration 88, loss = 0.002468652790412307
iteration 89, loss = 0.0011099613038823009
iteration 90, loss = 0.0013376770075410604
iteration 91, loss = 0.0016852653352543712
iteration 92, loss = 0.0010800717864185572
iteration 93, loss = 0.0011859516380354762
iteration 94, loss = 0.0011321288766339421
iteration 95, loss = 0.0017798102926462889
iteration 96, loss = 0.001576869748532772
iteration 97, loss = 0.0009290419984608889
iteration 98, loss = 0.0012306966818869114
iteration 99, loss = 0.0016436715377494693
iteration 100, loss = 0.0011031384347006679
iteration 101, loss = 0.0012832634383812547
iteration 102, loss = 0.0012106590438634157
iteration 103, loss = 0.001608214806765318
iteration 104, loss = 0.0014732094714418054
iteration 105, loss = 0.0010767328785732388
iteration 106, loss = 0.0011628123465925455
iteration 107, loss = 0.001406742725521326
iteration 108, loss = 0.001256321556866169
iteration 109, loss = 0.0014600679278373718
iteration 110, loss = 0.0012511115055531263
iteration 111, loss = 0.0010532025480642915
iteration 112, loss = 0.000931944465264678
iteration 113, loss = 0.0011193986283615232
iteration 114, loss = 0.0011503170244395733
iteration 115, loss = 0.0020389691926538944
iteration 116, loss = 0.001118704560212791
iteration 117, loss = 0.000885282875970006
iteration 118, loss = 0.0010401674080640078
iteration 119, loss = 0.0019272996578365564
iteration 120, loss = 0.001295043039135635
iteration 121, loss = 0.0011106885503977537
iteration 122, loss = 0.0011861989041790366
iteration 123, loss = 0.0011837466154247522
iteration 124, loss = 0.0015878663398325443
iteration 125, loss = 0.0023281059693545103
iteration 126, loss = 0.001433448283933103
iteration 127, loss = 0.001022865530103445
iteration 128, loss = 0.0009360141702927649
iteration 129, loss = 0.0010262231808155775
iteration 130, loss = 0.0010714165400713682
iteration 131, loss = 0.0007531086448580027
iteration 132, loss = 0.001595771056599915
iteration 133, loss = 0.0010598041117191315
iteration 134, loss = 0.0011261376785114408
iteration 135, loss = 0.0009872049558907747
iteration 136, loss = 0.0019625392742455006
iteration 137, loss = 0.0011229251977056265
iteration 138, loss = 0.0010887350654229522
iteration 139, loss = 0.0011785388924181461
iteration 140, loss = 0.0010976219782605767
iteration 141, loss = 0.0010805628262460232
iteration 142, loss = 0.001080698100849986
iteration 143, loss = 0.0011345150414854288
iteration 144, loss = 0.0013760009314864874
iteration 145, loss = 0.0015933304093778133
iteration 146, loss = 0.0010832344414666295
iteration 147, loss = 0.0010627629235386848
iteration 148, loss = 0.0015166794182732701
iteration 149, loss = 0.0020007260609418154
iteration 150, loss = 0.0021850077901035547
iteration 151, loss = 0.0011727557284757495
iteration 152, loss = 0.00143623782787472
iteration 153, loss = 0.0008597558480687439
iteration 154, loss = 0.001159369363449514
iteration 155, loss = 0.0010176401119679213
iteration 156, loss = 0.0011550497729331255
iteration 157, loss = 0.0012402336578816175
iteration 158, loss = 0.002334888093173504
iteration 159, loss = 0.001090881647542119
iteration 160, loss = 0.0013454948784783483
iteration 161, loss = 0.0011842688545584679
iteration 162, loss = 0.0013930854620411992
iteration 163, loss = 0.0011602152371779084
iteration 164, loss = 0.0012319415109232068
iteration 165, loss = 0.0010966598056256771
iteration 166, loss = 0.0012388949980959296
iteration 167, loss = 0.0011853647883981466
iteration 168, loss = 0.0010413932614028454
iteration 169, loss = 0.002947521395981312
iteration 170, loss = 0.0014817158225923777
iteration 171, loss = 0.0009167293319478631
iteration 172, loss = 0.0013198364758864045
iteration 173, loss = 0.0008955918601714075
iteration 174, loss = 0.0015650399727746844
iteration 175, loss = 0.0008948083850555122
iteration 176, loss = 0.002158217364922166
iteration 177, loss = 0.0013013766147196293
iteration 178, loss = 0.0010485834209248424
iteration 179, loss = 0.001932217157445848
iteration 180, loss = 0.0010967026464641094
iteration 181, loss = 0.0010419487953186035
iteration 182, loss = 0.0010184691054746509
iteration 183, loss = 0.0010953224264085293
iteration 184, loss = 0.001151867676526308
iteration 185, loss = 0.0010544434189796448
iteration 186, loss = 0.0014013624750077724
iteration 187, loss = 0.001052080886438489
iteration 188, loss = 0.0010296300752088428
iteration 189, loss = 0.0010708665940910578
iteration 190, loss = 0.0017813933081924915
iteration 191, loss = 0.0017963043646886945
iteration 192, loss = 0.00115495384670794
iteration 193, loss = 0.0011514840880408883
iteration 194, loss = 0.0011338464682921767
iteration 195, loss = 0.0014517978997901082
iteration 196, loss = 0.002108170883730054
iteration 197, loss = 0.0011551532661542296
iteration 198, loss = 0.0011375104077160358
iteration 199, loss = 0.0012760338140651584
iteration 200, loss = 0.001691689481958747
iteration 201, loss = 0.0013031066628172994
iteration 202, loss = 0.0012740110978484154
iteration 203, loss = 0.001140000531449914
iteration 204, loss = 0.0010500217322260141
iteration 205, loss = 0.0011658173752948642
iteration 206, loss = 0.000832799298223108
iteration 207, loss = 0.001124574220739305
iteration 208, loss = 0.0015306113054975867
iteration 209, loss = 0.0011775048915296793
iteration 210, loss = 0.0008935990044847131
iteration 211, loss = 0.002010138239711523
iteration 212, loss = 0.000912961084395647
iteration 213, loss = 0.0013038788456469774
iteration 214, loss = 0.0009984885109588504
iteration 215, loss = 0.0012814645888283849
iteration 216, loss = 0.0014747949317097664
iteration 217, loss = 0.0015251091681420803
iteration 218, loss = 0.0011090782936662436
iteration 219, loss = 0.0012161571066826582
iteration 220, loss = 0.0010515504982322454
iteration 221, loss = 0.0015130355022847652
iteration 222, loss = 0.001263791462406516
iteration 223, loss = 0.0020884883124381304
iteration 224, loss = 0.0016057834727689624
iteration 225, loss = 0.001113983686082065
iteration 226, loss = 0.0022213144693523645
iteration 227, loss = 0.0011646162020042539
iteration 228, loss = 0.001010810723528266
iteration 229, loss = 0.0011131422361359
iteration 230, loss = 0.0011482641566544771
iteration 231, loss = 0.0013776440173387527
iteration 232, loss = 0.0008465775172226131
iteration 233, loss = 0.0010884474031627178
iteration 234, loss = 0.0009863745654001832
iteration 235, loss = 0.0010407628724351525
iteration 236, loss = 0.0010138892102986574
iteration 237, loss = 0.001122291898354888
iteration 238, loss = 0.0008765554521232843
iteration 239, loss = 0.0010348608484491706
iteration 240, loss = 0.0009792238706722856
iteration 241, loss = 0.0010541868396103382
iteration 242, loss = 0.0012161892373114824
iteration 243, loss = 0.0013467182870954275
iteration 244, loss = 0.0010843932395800948
iteration 245, loss = 0.0025299065746366978
iteration 246, loss = 0.0011126587633043528
iteration 247, loss = 0.001138409017585218
iteration 248, loss = 0.0019973809830844402
iteration 249, loss = 0.001039455528371036
iteration 250, loss = 0.001039058086462319
iteration 251, loss = 0.00107812590431422
iteration 252, loss = 0.0012971009127795696
iteration 253, loss = 0.0010593854822218418
iteration 254, loss = 0.0010528659913688898
iteration 255, loss = 0.001157370861619711
iteration 256, loss = 0.0011003469116985798
iteration 257, loss = 0.0012272621970623732
iteration 258, loss = 0.001014844048768282
iteration 259, loss = 0.0011949529871344566
iteration 260, loss = 0.0011901336256414652
iteration 261, loss = 0.0010796991409733891
iteration 262, loss = 0.0010229500476270914
iteration 263, loss = 0.0009654070599935949
iteration 264, loss = 0.000916045275516808
iteration 265, loss = 0.0013203986454755068
iteration 266, loss = 0.0025018868036568165
iteration 267, loss = 0.0013118539936840534
iteration 268, loss = 0.0017601633444428444
iteration 269, loss = 0.0013059908524155617
iteration 270, loss = 0.0016702939756214619
iteration 271, loss = 0.0011412821477279067
iteration 272, loss = 0.0014875049237161875
iteration 273, loss = 0.001075057894922793
iteration 274, loss = 0.0010672063799574971
iteration 275, loss = 0.0017301730113103986
iteration 276, loss = 0.002108784392476082
iteration 277, loss = 0.0011211405508220196
iteration 278, loss = 0.0011763708898797631
iteration 279, loss = 0.0010948274284601212
iteration 280, loss = 0.0009191326680593193
iteration 281, loss = 0.0011293870629742742
iteration 282, loss = 0.0012269978178665042
iteration 283, loss = 0.001046146615408361
iteration 284, loss = 0.0011810428695753217
iteration 285, loss = 0.0011491556651890278
iteration 286, loss = 0.001146851573139429
iteration 287, loss = 0.0018440908752381802
iteration 288, loss = 0.0008726742235012352
iteration 289, loss = 0.0015681874938309193
iteration 290, loss = 0.001625844044610858
iteration 291, loss = 0.0009304973063990474
iteration 292, loss = 0.0011131770443171263
iteration 293, loss = 0.0017140005948022008
iteration 294, loss = 0.00100949895568192
iteration 295, loss = 0.002374885370954871
iteration 296, loss = 0.0012628005351871252
iteration 297, loss = 0.000977993244305253
iteration 298, loss = 0.001098641543649137
iteration 299, loss = 0.0015648372936993837
iteration 300, loss = 0.001213326002471149
iteration 1, loss = 0.0009396773530170321
iteration 2, loss = 0.0020357551984488964
iteration 3, loss = 0.0014190817018970847
iteration 4, loss = 0.0010493177687749267
iteration 5, loss = 0.0013622271362692118
iteration 6, loss = 0.0009844343876466155
iteration 7, loss = 0.0009126302902586758
iteration 8, loss = 0.0010413711424916983
iteration 9, loss = 0.0014140572166070342
iteration 10, loss = 0.0009589593391865492
iteration 11, loss = 0.0011751562124118209
iteration 12, loss = 0.001086992910131812
iteration 13, loss = 0.0015337088843807578
iteration 14, loss = 0.0010707549517974257
iteration 15, loss = 0.0010910441633313894
iteration 16, loss = 0.001046612742356956
iteration 17, loss = 0.0011376705951988697
iteration 18, loss = 0.0020525874570012093
iteration 19, loss = 0.0015888686757534742
iteration 20, loss = 0.0011131594656035304
iteration 21, loss = 0.0012011524522677064
iteration 22, loss = 0.0012703844113275409
iteration 23, loss = 0.0015016273828223348
iteration 24, loss = 0.0015182914212346077
iteration 25, loss = 0.0008653156692162156
iteration 26, loss = 0.002033473690971732
iteration 27, loss = 0.0009793629869818687
iteration 28, loss = 0.001228892244398594
iteration 29, loss = 0.0008957494283095002
iteration 30, loss = 0.0012675055768340826
iteration 31, loss = 0.0014898411463946104
iteration 32, loss = 0.001080747926607728
iteration 33, loss = 0.0011383722303435206
iteration 34, loss = 0.0009433415834791958
iteration 35, loss = 0.0019664710853248835
iteration 36, loss = 0.000970947090536356
iteration 37, loss = 0.0010223046410828829
iteration 38, loss = 0.0011605132604017854
iteration 39, loss = 0.000891839386895299
iteration 40, loss = 0.0012633609585464
iteration 41, loss = 0.0009516790742054582
iteration 42, loss = 0.001171513576991856
iteration 43, loss = 0.002655566670000553
iteration 44, loss = 0.0014352191938087344
iteration 45, loss = 0.00216972129419446
iteration 46, loss = 0.0020947237499058247
iteration 47, loss = 0.000981030287221074
iteration 48, loss = 0.0011979560367763042
iteration 49, loss = 0.0010447814129292965
iteration 50, loss = 0.0015104199992492795
iteration 51, loss = 0.002507226774469018
iteration 52, loss = 0.0013330391375347972
iteration 53, loss = 0.000961378391366452
iteration 54, loss = 0.0016115839825943112
iteration 55, loss = 0.00100634281989187
iteration 56, loss = 0.0012802371056750417
iteration 57, loss = 0.0011883676052093506
iteration 58, loss = 0.0010242335265502334
iteration 59, loss = 0.0013725452590733767
iteration 60, loss = 0.0010311505757272243
iteration 61, loss = 0.0011041624238714576
iteration 62, loss = 0.001254216069355607
iteration 63, loss = 0.0010678013786673546
iteration 64, loss = 0.000849953736178577
iteration 65, loss = 0.0011806983966380358
iteration 66, loss = 0.0009155122097581625
iteration 67, loss = 0.0011815144680440426
iteration 68, loss = 0.0010512438602745533
iteration 69, loss = 0.0010464572114869952
iteration 70, loss = 0.0011531031923368573
iteration 71, loss = 0.0010860483162105083
iteration 72, loss = 0.0021938758436590433
iteration 73, loss = 0.00297570344991982
iteration 74, loss = 0.0018925570184364915
iteration 75, loss = 0.0008811745792627335
iteration 76, loss = 0.0009419529233127832
iteration 77, loss = 0.0011572538642212749
iteration 78, loss = 0.0011144332820549607
iteration 79, loss = 0.0011630430817604065
iteration 80, loss = 0.0024266554974019527
iteration 81, loss = 0.001041476964019239
iteration 82, loss = 0.0017286213114857674
iteration 83, loss = 0.0025907086674124002
iteration 84, loss = 0.0013220008695498109
iteration 85, loss = 0.0018155856523662806
iteration 86, loss = 0.0011571806389838457
iteration 87, loss = 0.0012067512143403292
iteration 88, loss = 0.0011322252685204148
iteration 89, loss = 0.0009571870323270559
iteration 90, loss = 0.0018822898855432868
iteration 91, loss = 0.0010989300208166242
iteration 92, loss = 0.0012227228144183755
iteration 93, loss = 0.0020404867827892303
iteration 94, loss = 0.0009543634951114655
iteration 95, loss = 0.0016118156490847468
iteration 96, loss = 0.0012575541622936726
iteration 97, loss = 0.0011999384732916951
iteration 98, loss = 0.0013373543042689562
iteration 99, loss = 0.0009917316492646933
iteration 100, loss = 0.0011316273594275117
iteration 101, loss = 0.0015583049971610308
iteration 102, loss = 0.0010303931776434183
iteration 103, loss = 0.001189346075989306
iteration 104, loss = 0.0012145827058702707
iteration 105, loss = 0.0008987366454675794
iteration 106, loss = 0.0013168195728212595
iteration 107, loss = 0.0009421652648597956
iteration 108, loss = 0.0009854291565716267
iteration 109, loss = 0.0011533783981576562
iteration 110, loss = 0.0014920980902388692
iteration 111, loss = 0.0010924740927293897
iteration 112, loss = 0.0011893351329490542
iteration 113, loss = 0.0010822394397109747
iteration 114, loss = 0.0012913888785988092
iteration 115, loss = 0.0010165746789425611
iteration 116, loss = 0.0010847572702914476
iteration 117, loss = 0.0014441098319366574
iteration 118, loss = 0.0010014735162258148
iteration 119, loss = 0.0012900829315185547
iteration 120, loss = 0.0009017575066536665
iteration 121, loss = 0.0011971824569627643
iteration 122, loss = 0.0009200845961458981
iteration 123, loss = 0.0015105855418369174
iteration 124, loss = 0.0008683418855071068
iteration 125, loss = 0.0014891554601490498
iteration 126, loss = 0.0012999045429751277
iteration 127, loss = 0.0011292033595964313
iteration 128, loss = 0.0012507502688094974
iteration 129, loss = 0.001029696548357606
iteration 130, loss = 0.000922797538805753
iteration 131, loss = 0.0012169990222901106
iteration 132, loss = 0.001029337290674448
iteration 133, loss = 0.0012275626650080085
iteration 134, loss = 0.0019121463410556316
iteration 135, loss = 0.001124233822338283
iteration 136, loss = 0.0009313051705248654
iteration 137, loss = 0.0013006632216274738
iteration 138, loss = 0.0010984048712998629
iteration 139, loss = 0.0011639520525932312
iteration 140, loss = 0.0014009152073413134
iteration 141, loss = 0.0012887615012004972
iteration 142, loss = 0.0009906539926305413
iteration 143, loss = 0.0012335568899288774
iteration 144, loss = 0.0013434955617412925
iteration 145, loss = 0.000944806553889066
iteration 146, loss = 0.0013349292566999793
iteration 147, loss = 0.0010336710838600993
iteration 148, loss = 0.000994745409116149
iteration 149, loss = 0.0017226372146978974
iteration 150, loss = 0.001046587829478085
iteration 151, loss = 0.0009710504673421383
iteration 152, loss = 0.0008962663705460727
iteration 153, loss = 0.0008538526017218828
iteration 154, loss = 0.0009872831869870424
iteration 155, loss = 0.0018716001650318503
iteration 156, loss = 0.001048548030667007
iteration 157, loss = 0.0009323061676695943
iteration 158, loss = 0.0008765093516558409
iteration 159, loss = 0.001024492084980011
iteration 160, loss = 0.0009975394932553172
iteration 161, loss = 0.002067107940092683
iteration 162, loss = 0.0017920145764946938
iteration 163, loss = 0.0011399842333048582
iteration 164, loss = 0.0011581939179450274
iteration 165, loss = 0.0010776703711599112
iteration 166, loss = 0.0011303912615403533
iteration 167, loss = 0.0011374319437891245
iteration 168, loss = 0.001366887241601944
iteration 169, loss = 0.001100296271033585
iteration 170, loss = 0.0008814955945126712
iteration 171, loss = 0.0011566346511244774
iteration 172, loss = 0.0008848137222230434
iteration 173, loss = 0.0010430660331621766
iteration 174, loss = 0.0010654645739123225
iteration 175, loss = 0.0009254854521714151
iteration 176, loss = 0.0009849161142483354
iteration 177, loss = 0.001143388682976365
iteration 178, loss = 0.0009262990788556635
iteration 179, loss = 0.0010275085223838687
iteration 180, loss = 0.0010328968055546284
iteration 181, loss = 0.000824206683319062
iteration 182, loss = 0.0015516320709139109
iteration 183, loss = 0.001213638810440898
iteration 184, loss = 0.0012191866990178823
iteration 185, loss = 0.001515141106210649
iteration 186, loss = 0.0009543850901536644
iteration 187, loss = 0.002051146235316992
iteration 188, loss = 0.0011393630411475897
iteration 189, loss = 0.0011578516568988562
iteration 190, loss = 0.0017077367519959807
iteration 191, loss = 0.0013726108008995652
iteration 192, loss = 0.0012085739290341735
iteration 193, loss = 0.0011043762788176537
iteration 194, loss = 0.0008253415580838919
iteration 195, loss = 0.0008905137074179947
iteration 196, loss = 0.001300375210121274
iteration 197, loss = 0.0010401958134025335
iteration 198, loss = 0.0009318508673459291
iteration 199, loss = 0.0009310216410085559
iteration 200, loss = 0.0011409611906856298
iteration 201, loss = 0.0010402649641036987
iteration 202, loss = 0.0013473677681759
iteration 203, loss = 0.0018869945779442787
iteration 204, loss = 0.0008407265413552523
iteration 205, loss = 0.0009583474602550268
iteration 206, loss = 0.0012017415137961507
iteration 207, loss = 0.0008703334024176002
iteration 208, loss = 0.0013246943708509207
iteration 209, loss = 0.0010594272753223777
iteration 210, loss = 0.000949723063968122
iteration 211, loss = 0.00130263133905828
iteration 212, loss = 0.0010875416919589043
iteration 213, loss = 0.0010536550544202328
iteration 214, loss = 0.0012349651660770178
iteration 215, loss = 0.0013307336485013366
iteration 216, loss = 0.0012655373429879546
iteration 217, loss = 0.001006650272756815
iteration 218, loss = 0.000889415037818253
iteration 219, loss = 0.001791741349734366
iteration 220, loss = 0.0010352444369345903
iteration 221, loss = 0.001234488096088171
iteration 222, loss = 0.0010540230432525277
iteration 223, loss = 0.0008721970370970666
iteration 224, loss = 0.0011206651106476784
iteration 225, loss = 0.0007888254476711154
iteration 226, loss = 0.0011219141306355596
iteration 227, loss = 0.0010840955656021833
iteration 228, loss = 0.0009221249492838979
iteration 229, loss = 0.0010230590123683214
iteration 230, loss = 0.0009977611480280757
iteration 231, loss = 0.00120014906860888
iteration 232, loss = 0.0009387757163494825
iteration 233, loss = 0.001238865777850151
iteration 234, loss = 0.0009578525787219405
iteration 235, loss = 0.00103851780295372
iteration 236, loss = 0.0013546900590881705
iteration 237, loss = 0.0012731156311929226
iteration 238, loss = 0.0012789171887561679
iteration 239, loss = 0.0014042974216863513
iteration 240, loss = 0.0009255711920559406
iteration 241, loss = 0.0009471109369769692
iteration 242, loss = 0.0010519828647375107
iteration 243, loss = 0.0009051183587871492
iteration 244, loss = 0.0019034657161682844
iteration 245, loss = 0.0013729711063206196
iteration 246, loss = 0.0007853514980524778
iteration 247, loss = 0.0008824195829220116
iteration 248, loss = 0.0010173970367759466
iteration 249, loss = 0.0016462039202451706
iteration 250, loss = 0.001278419978916645
iteration 251, loss = 0.0013399346498772502
iteration 252, loss = 0.001084218267351389
iteration 253, loss = 0.0009981156326830387
iteration 254, loss = 0.0011405666591599584
iteration 255, loss = 0.0010155471973121166
iteration 256, loss = 0.0009688441641628742
iteration 257, loss = 0.0008813547319732606
iteration 258, loss = 0.0011946200393140316
iteration 259, loss = 0.0011007471475750208
iteration 260, loss = 0.0016917947214096785
iteration 261, loss = 0.0010527392150834203
iteration 262, loss = 0.0010491556022316217
iteration 263, loss = 0.0009248941205441952
iteration 264, loss = 0.0013221233384683728
iteration 265, loss = 0.0011430350132286549
iteration 266, loss = 0.0010491999564692378
iteration 267, loss = 0.0021839356049895287
iteration 268, loss = 0.002010609023272991
iteration 269, loss = 0.0009290845482610166
iteration 270, loss = 0.0014001713134348392
iteration 271, loss = 0.0011224586050957441
iteration 272, loss = 0.001150088501162827
iteration 273, loss = 0.0019825322087854147
iteration 274, loss = 0.0007557147764600813
iteration 275, loss = 0.00112667097710073
iteration 276, loss = 0.0010788730578497052
iteration 277, loss = 0.0010413542622700334
iteration 278, loss = 0.0011075048241764307
iteration 279, loss = 0.0016174230258911848
iteration 280, loss = 0.0010507559636607766
iteration 281, loss = 0.0009160208865068853
iteration 282, loss = 0.0011240908643230796
iteration 283, loss = 0.0013517587212845683
iteration 284, loss = 0.0007715405081398785
iteration 285, loss = 0.001743451226502657
iteration 286, loss = 0.0013839434832334518
iteration 287, loss = 0.0013173491461202502
iteration 288, loss = 0.001067649107426405
iteration 289, loss = 0.0013767947675660253
iteration 290, loss = 0.0012093025725334883
iteration 291, loss = 0.0011242976179346442
iteration 292, loss = 0.0009976611472666264
iteration 293, loss = 0.0013766571646556258
iteration 294, loss = 0.0010713241063058376
iteration 295, loss = 0.0012158359168097377
iteration 296, loss = 0.0011659623123705387
iteration 297, loss = 0.0009205765672959387
iteration 298, loss = 0.0009325686260126531
iteration 299, loss = 0.002071965020149946
iteration 300, loss = 0.0012574733700603247
iteration 1, loss = 0.001061888411641121
iteration 2, loss = 0.0008024958660826087
iteration 3, loss = 0.001060368143953383
iteration 4, loss = 0.0017603279557079077
iteration 5, loss = 0.0019452504348009825
iteration 6, loss = 0.0013028684770688415
iteration 7, loss = 0.0014536621747538447
iteration 8, loss = 0.0009338040254078805
iteration 9, loss = 0.0014166859909892082
iteration 10, loss = 0.0010589471785351634
iteration 11, loss = 0.0010020018089562654
iteration 12, loss = 0.002050030045211315
iteration 13, loss = 0.0018573662964627147
iteration 14, loss = 0.0011517611565068364
iteration 15, loss = 0.001101355766877532
iteration 16, loss = 0.0009178458712995052
iteration 17, loss = 0.00111681898124516
iteration 18, loss = 0.0007418725290335715
iteration 19, loss = 0.0013542517554014921
iteration 20, loss = 0.0010534354951232672
iteration 21, loss = 0.000963099708314985
iteration 22, loss = 0.000966640654951334
iteration 23, loss = 0.001334612024948001
iteration 24, loss = 0.0013737546978518367
iteration 25, loss = 0.0014691882533952594
iteration 26, loss = 0.0020346420351415873
iteration 27, loss = 0.0012194585287943482
iteration 28, loss = 0.0010766193736344576
iteration 29, loss = 0.001139093074016273
iteration 30, loss = 0.0009448990458622575
iteration 31, loss = 0.001266945619136095
iteration 32, loss = 0.0011986230965703726
iteration 33, loss = 0.0011912386398762465
iteration 34, loss = 0.0018811431946232915
iteration 35, loss = 0.0011181659065186977
iteration 36, loss = 0.0008541873539797962
iteration 37, loss = 0.0021075173281133175
iteration 38, loss = 0.0010008299723267555
iteration 39, loss = 0.0011036957148462534
iteration 40, loss = 0.0012571499682962894
iteration 41, loss = 0.0009070953237824142
iteration 42, loss = 0.0009691198356449604
iteration 43, loss = 0.0008894943748600781
iteration 44, loss = 0.0008936526137404144
iteration 45, loss = 0.0020267111249268055
iteration 46, loss = 0.001257539028301835
iteration 47, loss = 0.00107822404243052
iteration 48, loss = 0.0014135426608845592
iteration 49, loss = 0.0008564828895032406
iteration 50, loss = 0.0010058225598186255
iteration 51, loss = 0.0008771128486841917
iteration 52, loss = 0.0011225072667002678
iteration 53, loss = 0.0013532692100852728
iteration 54, loss = 0.000960039789788425
iteration 55, loss = 0.0011757250176742673
iteration 56, loss = 0.0009808720787987113
iteration 57, loss = 0.0011245800415053964
iteration 58, loss = 0.0010583502007648349
iteration 59, loss = 0.0009743380942381918
iteration 60, loss = 0.0007842360646463931
iteration 61, loss = 0.0010228442260995507
iteration 62, loss = 0.0020173031371086836
iteration 63, loss = 0.0009184449445456266
iteration 64, loss = 0.001050148974172771
iteration 65, loss = 0.0008051551412791014
iteration 66, loss = 0.0009283386752940714
iteration 67, loss = 0.0009230984142050147
iteration 68, loss = 0.0017233267426490784
iteration 69, loss = 0.0009567775996401906
iteration 70, loss = 0.001036234898492694
iteration 71, loss = 0.0009065950289368629
iteration 72, loss = 0.0024101322051137686
iteration 73, loss = 0.0016167665598914027
iteration 74, loss = 0.0009746520081534982
iteration 75, loss = 0.0012370439944788814
iteration 76, loss = 0.0011871951865032315
iteration 77, loss = 0.001043611322529614
iteration 78, loss = 0.0010193142807111144
iteration 79, loss = 0.001292589819058776
iteration 80, loss = 0.001941468333825469
iteration 81, loss = 0.0018006759928539395
iteration 82, loss = 0.0009877311531454325
iteration 83, loss = 0.0008420755621045828
iteration 84, loss = 0.0011508892057463527
iteration 85, loss = 0.001206627581268549
iteration 86, loss = 0.0009255110053345561
iteration 87, loss = 0.0012404246954247355
iteration 88, loss = 0.0007830724352970719
iteration 89, loss = 0.0009561933693476021
iteration 90, loss = 0.0010714855743572116
iteration 91, loss = 0.001534357201308012
iteration 92, loss = 0.0009081476018764079
iteration 93, loss = 0.0010529339779168367
iteration 94, loss = 0.0013501265784725547
iteration 95, loss = 0.0009606766398064792
iteration 96, loss = 0.0012348589953035116
iteration 97, loss = 0.0008315293816849589
iteration 98, loss = 0.0009967823280021548
iteration 99, loss = 0.0009467439376749098
iteration 100, loss = 0.0009713111794553697
iteration 101, loss = 0.0008877087384462357
iteration 102, loss = 0.001867784420028329
iteration 103, loss = 0.00088110490469262
iteration 104, loss = 0.0009598192409612238
iteration 105, loss = 0.0008754308801144361
iteration 106, loss = 0.001063342671841383
iteration 107, loss = 0.001190433045849204
iteration 108, loss = 0.000995196751318872
iteration 109, loss = 0.0012203288497403264
iteration 110, loss = 0.0010994566837325692
iteration 111, loss = 0.0011425118427723646
iteration 112, loss = 0.0020185646135360003
iteration 113, loss = 0.0014874596381559968
iteration 114, loss = 0.0009222078369930387
iteration 115, loss = 0.001007133163511753
iteration 116, loss = 0.0015066376654431224
iteration 117, loss = 0.001280529540963471
iteration 118, loss = 0.001055187894962728
iteration 119, loss = 0.0009705716511234641
iteration 120, loss = 0.0018716068007051945
iteration 121, loss = 0.0013837834121659398
iteration 122, loss = 0.0013328860513865948
iteration 123, loss = 0.0014399788342416286
iteration 124, loss = 0.0009621765348128974
iteration 125, loss = 0.0010287676705047488
iteration 126, loss = 0.000857788894791156
iteration 127, loss = 0.0012199787888675928
iteration 128, loss = 0.0017446361016482115
iteration 129, loss = 0.0009484878974035382
iteration 130, loss = 0.001122020184993744
iteration 131, loss = 0.0011570218484848738
iteration 132, loss = 0.0010313773527741432
iteration 133, loss = 0.0008762709330767393
iteration 134, loss = 0.0012157661840319633
iteration 135, loss = 0.0008618943393230438
iteration 136, loss = 0.0009346514707431197
iteration 137, loss = 0.0013978882925584912
iteration 138, loss = 0.0016810980159789324
iteration 139, loss = 0.0011366535909473896
iteration 140, loss = 0.0009742855909280479
iteration 141, loss = 0.0008926957962103188
iteration 142, loss = 0.0009701246162876487
iteration 143, loss = 0.0009491550736129284
iteration 144, loss = 0.000973806600086391
iteration 145, loss = 0.0009355739457532763
iteration 146, loss = 0.0009729361045174301
iteration 147, loss = 0.0008943530265241861
iteration 148, loss = 0.0015345796709880233
iteration 149, loss = 0.0008929760660976171
iteration 150, loss = 0.0009544932981953025
iteration 151, loss = 0.001443346031010151
iteration 152, loss = 0.0009089352679438889
iteration 153, loss = 0.0012636113679036498
iteration 154, loss = 0.0008573909872211516
iteration 155, loss = 0.0017849975265562534
iteration 156, loss = 0.0011570756323635578
iteration 157, loss = 0.0009576242882758379
iteration 158, loss = 0.001114190905354917
iteration 159, loss = 0.0013570787850767374
iteration 160, loss = 0.001814857590943575
iteration 161, loss = 0.0010985147673636675
iteration 162, loss = 0.0016308361664414406
iteration 163, loss = 0.0013011236442252994
iteration 164, loss = 0.0010276801185682416
iteration 165, loss = 0.0011781749781221151
iteration 166, loss = 0.0009183510555885732
iteration 167, loss = 0.0009110707323998213
iteration 168, loss = 0.0009232207667082548
iteration 169, loss = 0.001779172453097999
iteration 170, loss = 0.0020522079430520535
iteration 171, loss = 0.0012823340948671103
iteration 172, loss = 0.0017359042540192604
iteration 173, loss = 0.0009159574401564896
iteration 174, loss = 0.0011638789437711239
iteration 175, loss = 0.0007404533098451793
iteration 176, loss = 0.001268491381779313
iteration 177, loss = 0.0020172218792140484
iteration 178, loss = 0.0009551466791890562
iteration 179, loss = 0.0011809791903942823
iteration 180, loss = 0.000884423905517906
iteration 181, loss = 0.0009734920458868146
iteration 182, loss = 0.001175923040136695
iteration 183, loss = 0.0013006790541112423
iteration 184, loss = 0.0010507982224225998
iteration 185, loss = 0.001267032464966178
iteration 186, loss = 0.0009622822399251163
iteration 187, loss = 0.0009111224790103734
iteration 188, loss = 0.0012236592592671514
iteration 189, loss = 0.0009723571711219847
iteration 190, loss = 0.0009378955001011491
iteration 191, loss = 0.0008902847184799612
iteration 192, loss = 0.001197722041979432
iteration 193, loss = 0.0009670051513239741
iteration 194, loss = 0.001205225707963109
iteration 195, loss = 0.0009559796890243888
iteration 196, loss = 0.0009605784434825182
iteration 197, loss = 0.0008822281379252672
iteration 198, loss = 0.0012353550409898162
iteration 199, loss = 0.001119173364713788
iteration 200, loss = 0.000897904799785465
iteration 201, loss = 0.0010151969036087394
iteration 202, loss = 0.0009530162205919623
iteration 203, loss = 0.0011623537866398692
iteration 204, loss = 0.0009447040501981974
iteration 205, loss = 0.0013232423225417733
iteration 206, loss = 0.0007805259665474296
iteration 207, loss = 0.0009556965669617057
iteration 208, loss = 0.0010904268128797412
iteration 209, loss = 0.001508776331320405
iteration 210, loss = 0.00127905304543674
iteration 211, loss = 0.000990010448731482
iteration 212, loss = 0.0009356894297525287
iteration 213, loss = 0.001445004832930863
iteration 214, loss = 0.0010713805677369237
iteration 215, loss = 0.0010287059703841805
iteration 216, loss = 0.0011746319942176342
iteration 217, loss = 0.0011769614648073912
iteration 218, loss = 0.0009398665279150009
iteration 219, loss = 0.0011897202348336577
iteration 220, loss = 0.000888621318154037
iteration 221, loss = 0.0009946200298145413
iteration 222, loss = 0.0008782809600234032
iteration 223, loss = 0.0012276183115318418
iteration 224, loss = 0.000945587526075542
iteration 225, loss = 0.0008069811156019568
iteration 226, loss = 0.001001807046122849
iteration 227, loss = 0.0009699530201032758
iteration 228, loss = 0.0009567168308421969
iteration 229, loss = 0.0013988380087539554
iteration 230, loss = 0.001059001311659813
iteration 231, loss = 0.0008765690145082772
iteration 232, loss = 0.0019021296175196767
iteration 233, loss = 0.0010400826577097178
iteration 234, loss = 0.000981114455498755
iteration 235, loss = 0.0012663970701396465
iteration 236, loss = 0.0007911670836620033
iteration 237, loss = 0.0014493016060441732
iteration 238, loss = 0.0008863821276463568
iteration 239, loss = 0.0010201545665040612
iteration 240, loss = 0.0009348161984235048
iteration 241, loss = 0.0008939893450587988
iteration 242, loss = 0.0011070326436311007
iteration 243, loss = 0.0009624146041460335
iteration 244, loss = 0.000893682474270463
iteration 245, loss = 0.0009293348994106054
iteration 246, loss = 0.0009104402270168066
iteration 247, loss = 0.0008716689771972597
iteration 248, loss = 0.0008595312247052789
iteration 249, loss = 0.00192179752048105
iteration 250, loss = 0.0007864125072956085
iteration 251, loss = 0.0008251015096902847
iteration 252, loss = 0.0012032223166897893
iteration 253, loss = 0.0008346328395418823
iteration 254, loss = 0.0009295247145928442
iteration 255, loss = 0.0010778623400256038
iteration 256, loss = 0.0015861421125009656
iteration 257, loss = 0.0008151197689585388
iteration 258, loss = 0.001004114979878068
iteration 259, loss = 0.0014136035460978746
iteration 260, loss = 0.0009992069099098444
iteration 261, loss = 0.00213954271748662
iteration 262, loss = 0.0008478575618937612
iteration 263, loss = 0.0011576545657590032
iteration 264, loss = 0.0010698434198275208
iteration 265, loss = 0.0010458598844707012
iteration 266, loss = 0.0011705338256433606
iteration 267, loss = 0.0009872176451608539
iteration 268, loss = 0.0008751582936383784
iteration 269, loss = 0.0007209778414107859
iteration 270, loss = 0.0016158876242116094
iteration 271, loss = 0.0012208110420033336
iteration 272, loss = 0.0013730135979130864
iteration 273, loss = 0.0010350245283916593
iteration 274, loss = 0.0008140961290337145
iteration 275, loss = 0.0010864553041756153
iteration 276, loss = 0.0018350154859945178
iteration 277, loss = 0.0009352870984002948
iteration 278, loss = 0.0008872405160218477
iteration 279, loss = 0.0018723742105066776
iteration 280, loss = 0.0009277831995859742
iteration 281, loss = 0.0011873862240463495
iteration 282, loss = 0.001027429010719061
iteration 283, loss = 0.0012409428600221872
iteration 284, loss = 0.0010282150469720364
iteration 285, loss = 0.0009575700387358665
iteration 286, loss = 0.0010633199708536267
iteration 287, loss = 0.0009077960858121514
iteration 288, loss = 0.0011500248219817877
iteration 289, loss = 0.0010212039342150092
iteration 290, loss = 0.0009572729468345642
iteration 291, loss = 0.0008755387971177697
iteration 292, loss = 0.0013254353543743491
iteration 293, loss = 0.0009357820963487029
iteration 294, loss = 0.0008509224280714989
iteration 295, loss = 0.000906772562302649
iteration 296, loss = 0.0012127147056162357
iteration 297, loss = 0.0008470840402878821
iteration 298, loss = 0.0012597822351381183
iteration 299, loss = 0.0009668082348071039
iteration 300, loss = 0.0013159021036699414
iteration 1, loss = 0.0016635003266856074
iteration 2, loss = 0.0009038815042003989
iteration 3, loss = 0.0009040270815603435
iteration 4, loss = 0.0011266901856288314
iteration 5, loss = 0.0010526813566684723
iteration 6, loss = 0.0009075861307792366
iteration 7, loss = 0.0008436735952273011
iteration 8, loss = 0.001111507648602128
iteration 9, loss = 0.0011738903122022748
iteration 10, loss = 0.0008278657332994044
iteration 11, loss = 0.0012103731278330088
iteration 12, loss = 0.0009454427054151893
iteration 13, loss = 0.0013286506291478872
iteration 14, loss = 0.0011811600998044014
iteration 15, loss = 0.0008171317749656737
iteration 16, loss = 0.0010860770707949996
iteration 17, loss = 0.0008989075431600213
iteration 18, loss = 0.0007974840118549764
iteration 19, loss = 0.0008827444398775697
iteration 20, loss = 0.0012839592527598143
iteration 21, loss = 0.0010131483431905508
iteration 22, loss = 0.0008686551009304821
iteration 23, loss = 0.0010119949001818895
iteration 24, loss = 0.0012955231359228492
iteration 25, loss = 0.0010212772758677602
iteration 26, loss = 0.001998452004045248
iteration 27, loss = 0.0008033577469177544
iteration 28, loss = 0.0010568995494395494
iteration 29, loss = 0.0010885741794481874
iteration 30, loss = 0.000680998433381319
iteration 31, loss = 0.0008337732870131731
iteration 32, loss = 0.0017733746208250523
iteration 33, loss = 0.0013682636199519038
iteration 34, loss = 0.0009353522909805179
iteration 35, loss = 0.0009589156834408641
iteration 36, loss = 0.0010215225629508495
iteration 37, loss = 0.0008822294767014682
iteration 38, loss = 0.0009921068558469415
iteration 39, loss = 0.0012870365753769875
iteration 40, loss = 0.0010740412399172783
iteration 41, loss = 0.0010250566992908716
iteration 42, loss = 0.0008249611128121614
iteration 43, loss = 0.000804923998657614
iteration 44, loss = 0.0018345422577112913
iteration 45, loss = 0.0009007252519950271
iteration 46, loss = 0.0010691509814932942
iteration 47, loss = 0.001220826175995171
iteration 48, loss = 0.0008783299708738923
iteration 49, loss = 0.001286773127503693
iteration 50, loss = 0.0009471589582972229
iteration 51, loss = 0.0010134902549907565
iteration 52, loss = 0.001495746080763638
iteration 53, loss = 0.0008943132706917822
iteration 54, loss = 0.0009477479034103453
iteration 55, loss = 0.0008231133106164634
iteration 56, loss = 0.0008547710021957755
iteration 57, loss = 0.0016323380405083299
iteration 58, loss = 0.0008964339504018426
iteration 59, loss = 0.0009368101018480957
iteration 60, loss = 0.0009321756078861654
iteration 61, loss = 0.0010959368664771318
iteration 62, loss = 0.0013034025905653834
iteration 63, loss = 0.001371184829622507
iteration 64, loss = 0.0008658183505758643
iteration 65, loss = 0.0009697942296043038
iteration 66, loss = 0.0009811720810830593
iteration 67, loss = 0.0010219071991741657
iteration 68, loss = 0.0017384123057126999
iteration 69, loss = 0.0007440113113261759
iteration 70, loss = 0.0009601367055438459
iteration 71, loss = 0.0012382480781525373
iteration 72, loss = 0.0009330371976830065
iteration 73, loss = 0.0009488686337135732
iteration 74, loss = 0.0008375774486921728
iteration 75, loss = 0.0009045888436958194
iteration 76, loss = 0.0010624630376696587
iteration 77, loss = 0.0012618135660886765
iteration 78, loss = 0.0008727227686904371
iteration 79, loss = 0.001147923176176846
iteration 80, loss = 0.0008987520122900605
iteration 81, loss = 0.0009298741351813078
iteration 82, loss = 0.0017126541351899505
iteration 83, loss = 0.0019353200914338231
iteration 84, loss = 0.0008482062257826328
iteration 85, loss = 0.0010025713127106428
iteration 86, loss = 0.0007415824220515788
iteration 87, loss = 0.001022584387101233
iteration 88, loss = 0.0011997348628938198
iteration 89, loss = 0.0011968982871621847
iteration 90, loss = 0.001941483118571341
iteration 91, loss = 0.000926240929402411
iteration 92, loss = 0.0008574528619647026
iteration 93, loss = 0.0009220110950991511
iteration 94, loss = 0.0007721358560957015
iteration 95, loss = 0.0011125344317406416
iteration 96, loss = 0.0011070121545344591
iteration 97, loss = 0.0007160455570556223
iteration 98, loss = 0.0017900245729833841
iteration 99, loss = 0.0011397874914109707
iteration 100, loss = 0.0013936507748439908
iteration 101, loss = 0.0009513319237157702
iteration 102, loss = 0.0010962359374389052
iteration 103, loss = 0.0010069934651255608
iteration 104, loss = 0.0009136395528912544
iteration 105, loss = 0.0011092906352132559
iteration 106, loss = 0.0011064870050176978
iteration 107, loss = 0.001846226048655808
iteration 108, loss = 0.0009166027884930372
iteration 109, loss = 0.0009513659169897437
iteration 110, loss = 0.0012693321332335472
iteration 111, loss = 0.0008773404988460243
iteration 112, loss = 0.0010078465566039085
iteration 113, loss = 0.0012074188562110066
iteration 114, loss = 0.0009336670045740902
iteration 115, loss = 0.0009706626296974719
iteration 116, loss = 0.0017652781680226326
iteration 117, loss = 0.0008645195048302412
iteration 118, loss = 0.0012553298147395253
iteration 119, loss = 0.0011711919214576483
iteration 120, loss = 0.0017423455137759447
iteration 121, loss = 0.0009830259950831532
iteration 122, loss = 0.0012340276734903455
iteration 123, loss = 0.0013144229305908084
iteration 124, loss = 0.001168271992355585
iteration 125, loss = 0.0009414284722879529
iteration 126, loss = 0.001658259890973568
iteration 127, loss = 0.0009244196116924286
iteration 128, loss = 0.001127173425629735
iteration 129, loss = 0.001341899042017758
iteration 130, loss = 0.0010666766902431846
iteration 131, loss = 0.0009676962508819997
iteration 132, loss = 0.0016107128467410803
iteration 133, loss = 0.0022740717977285385
iteration 134, loss = 0.0008405874250456691
iteration 135, loss = 0.0017016850179061294
iteration 136, loss = 0.0009381420677527785
iteration 137, loss = 0.0009968681260943413
iteration 138, loss = 0.0010965974070131779
iteration 139, loss = 0.0011030760360881686
iteration 140, loss = 0.0010737667325884104
iteration 141, loss = 0.0009156016749329865
iteration 142, loss = 0.0007495606550946832
iteration 143, loss = 0.0014842450618743896
iteration 144, loss = 0.0008767505642026663
iteration 145, loss = 0.0007853001588955522
iteration 146, loss = 0.0011932116467505693
iteration 147, loss = 0.0009107156656682491
iteration 148, loss = 0.0011460070963948965
iteration 149, loss = 0.002485122997313738
iteration 150, loss = 0.0008698970777913928
iteration 151, loss = 0.001015104353427887
iteration 152, loss = 0.0009057650458998978
iteration 153, loss = 0.0008999001001939178
iteration 154, loss = 0.0009490004740655422
iteration 155, loss = 0.0008038877858780324
iteration 156, loss = 0.0013263659784570336
iteration 157, loss = 0.0008429320296272635
iteration 158, loss = 0.0008605436305515468
iteration 159, loss = 0.0009927923092618585
iteration 160, loss = 0.0009413352818228304
iteration 161, loss = 0.000842690235003829
iteration 162, loss = 0.0013575260527431965
iteration 163, loss = 0.0013736974215134978
iteration 164, loss = 0.0008524470031261444
iteration 165, loss = 0.0012677606428042054
iteration 166, loss = 0.0008013314218260348
iteration 167, loss = 0.0016261725686490536
iteration 168, loss = 0.0009291687747463584
iteration 169, loss = 0.000770294398535043
iteration 170, loss = 0.0011756067397072911
iteration 171, loss = 0.0007405137293972075
iteration 172, loss = 0.0008963096770457923
iteration 173, loss = 0.0010759341530501842
iteration 174, loss = 0.0008294165600091219
iteration 175, loss = 0.0008154012029990554
iteration 176, loss = 0.0011627799831330776
iteration 177, loss = 0.0007722362643107772
iteration 178, loss = 0.00105202232953161
iteration 179, loss = 0.0008815988549031317
iteration 180, loss = 0.0011318149045109749
iteration 181, loss = 0.000895253790076822
iteration 182, loss = 0.001104134600609541
iteration 183, loss = 0.0010901157511398196
iteration 184, loss = 0.0016259480034932494
iteration 185, loss = 0.0009103264892473817
iteration 186, loss = 0.0009284989791922271
iteration 187, loss = 0.000764890864957124
iteration 188, loss = 0.0008539579575881362
iteration 189, loss = 0.0009092011023312807
iteration 190, loss = 0.0011915101204067469
iteration 191, loss = 0.0008565583266317844
iteration 192, loss = 0.0009851832874119282
iteration 193, loss = 0.0007275161915458739
iteration 194, loss = 0.0012222559889778495
iteration 195, loss = 0.001005240366794169
iteration 196, loss = 0.0008452414767816663
iteration 197, loss = 0.000892253709025681
iteration 198, loss = 0.0011875577038154006
iteration 199, loss = 0.0010058884508907795
iteration 200, loss = 0.0011079070391133428
iteration 201, loss = 0.0009048690553754568
iteration 202, loss = 0.0010192443151026964
iteration 203, loss = 0.0012569878017529845
iteration 204, loss = 0.0009325037826783955
iteration 205, loss = 0.0011814997997134924
iteration 206, loss = 0.0012662650551646948
iteration 207, loss = 0.0007449829136021435
iteration 208, loss = 0.0016823812620714307
iteration 209, loss = 0.0008043160778470337
iteration 210, loss = 0.0008766654646024108
iteration 211, loss = 0.0009414448286406696
iteration 212, loss = 0.0008596982806921005
iteration 213, loss = 0.001277218572795391
iteration 214, loss = 0.0007236512610688806
iteration 215, loss = 0.000927762477658689
iteration 216, loss = 0.0009277760982513428
iteration 217, loss = 0.0014633035752922297
iteration 218, loss = 0.0007779592415317893
iteration 219, loss = 0.0009444092866033316
iteration 220, loss = 0.0010361308231949806
iteration 221, loss = 0.0010465687373653054
iteration 222, loss = 0.001177270314656198
iteration 223, loss = 0.000740003539249301
iteration 224, loss = 0.000896904559340328
iteration 225, loss = 0.0011692041298374534
iteration 226, loss = 0.0015570262912660837
iteration 227, loss = 0.0017796576721593738
iteration 228, loss = 0.0009004515595734119
iteration 229, loss = 0.0008649075753055513
iteration 230, loss = 0.0010303723393008113
iteration 231, loss = 0.0010534159373492002
iteration 232, loss = 0.0009328442392870784
iteration 233, loss = 0.0007937994669191539
iteration 234, loss = 0.0008541169809177518
iteration 235, loss = 0.0007993627223186195
iteration 236, loss = 0.0012139780446887016
iteration 237, loss = 0.001205709297209978
iteration 238, loss = 0.00099617475643754
iteration 239, loss = 0.000981803284958005
iteration 240, loss = 0.0008881607791408896
iteration 241, loss = 0.0008208609651774168
iteration 242, loss = 0.0007839089375920594
iteration 243, loss = 0.002464028773829341
iteration 244, loss = 0.0007177853840403259
iteration 245, loss = 0.0007002072525210679
iteration 246, loss = 0.0010588883887976408
iteration 247, loss = 0.0017467525321990252
iteration 248, loss = 0.0011645174818113446
iteration 249, loss = 0.0008844293770380318
iteration 250, loss = 0.00081028719432652
iteration 251, loss = 0.0015882907900959253
iteration 252, loss = 0.00121981929987669
iteration 253, loss = 0.0010465785162523389
iteration 254, loss = 0.0011173926759511232
iteration 255, loss = 0.0014018904184922576
iteration 256, loss = 0.0009945615893229842
iteration 257, loss = 0.0007616380462422967
iteration 258, loss = 0.001084687071852386
iteration 259, loss = 0.001553233596496284
iteration 260, loss = 0.0012222258374094963
iteration 261, loss = 0.0008382005617022514
iteration 262, loss = 0.0007772878161631525
iteration 263, loss = 0.0008654403500258923
iteration 264, loss = 0.0009265778353437781
iteration 265, loss = 0.0009368988685309887
iteration 266, loss = 0.001047896919772029
iteration 267, loss = 0.0016894979635253549
iteration 268, loss = 0.0013370332308113575
iteration 269, loss = 0.0009073138353414834
iteration 270, loss = 0.0009012806112878025
iteration 271, loss = 0.0008744777296669781
iteration 272, loss = 0.0010431659175083041
iteration 273, loss = 0.00128650211263448
iteration 274, loss = 0.0008605170296505094
iteration 275, loss = 0.0013156667118892074
iteration 276, loss = 0.0007972004823386669
iteration 277, loss = 0.0007962037343531847
iteration 278, loss = 0.0016589874867349863
iteration 279, loss = 0.0008095967350527644
iteration 280, loss = 0.0013283321168273687
iteration 281, loss = 0.0009117727167904377
iteration 282, loss = 0.0011893755290657282
iteration 283, loss = 0.0008216612040996552
iteration 284, loss = 0.000992264598608017
iteration 285, loss = 0.0014163288287818432
iteration 286, loss = 0.0009459247812628746
iteration 287, loss = 0.0008411487797275186
iteration 288, loss = 0.001229375833645463
iteration 289, loss = 0.0007875658920966089
iteration 290, loss = 0.0012301498791202903
iteration 291, loss = 0.0008911397308111191
iteration 292, loss = 0.0014863319229334593
iteration 293, loss = 0.0012280783848837018
iteration 294, loss = 0.0008504175930283964
iteration 295, loss = 0.0011080887634307146
iteration 296, loss = 0.0008691514376550913
iteration 297, loss = 0.0009357691742479801
iteration 298, loss = 0.0011358429910615087
iteration 299, loss = 0.0008249034290201962
iteration 300, loss = 0.0008798094931989908
iteration 1, loss = 0.0014337748289108276
iteration 2, loss = 0.0007190456963144243
iteration 3, loss = 0.0010415876749902964
iteration 4, loss = 0.0009049960062839091
iteration 5, loss = 0.0012449336936697364
iteration 6, loss = 0.0008750335546210408
iteration 7, loss = 0.0023805941455066204
iteration 8, loss = 0.0009079863666556776
iteration 9, loss = 0.0010404849890619516
iteration 10, loss = 0.0008721347548998892
iteration 11, loss = 0.0012069279327988625
iteration 12, loss = 0.0015462192241102457
iteration 13, loss = 0.0007645671139471233
iteration 14, loss = 0.0007950601866468787
iteration 15, loss = 0.0010807756334543228
iteration 16, loss = 0.0009597429889254272
iteration 17, loss = 0.0007046886021271348
iteration 18, loss = 0.0011152747320011258
iteration 19, loss = 0.0008245911449193954
iteration 20, loss = 0.0009498230065219104
iteration 21, loss = 0.0009515557321719825
iteration 22, loss = 0.0016519465716555715
iteration 23, loss = 0.0008151329820975661
iteration 24, loss = 0.0007592160836793482
iteration 25, loss = 0.0015725124394521117
iteration 26, loss = 0.0010067429393529892
iteration 27, loss = 0.001063669566065073
iteration 28, loss = 0.0008357877377420664
iteration 29, loss = 0.0016984137473627925
iteration 30, loss = 0.0008015274652279913
iteration 31, loss = 0.0017568818293511868
iteration 32, loss = 0.0008348393021151423
iteration 33, loss = 0.0008032568730413914
iteration 34, loss = 0.0013901724014431238
iteration 35, loss = 0.001281386474147439
iteration 36, loss = 0.000846075767185539
iteration 37, loss = 0.0011888612061738968
iteration 38, loss = 0.0019223169656470418
iteration 39, loss = 0.0008162380545400083
iteration 40, loss = 0.0008116470999084413
iteration 41, loss = 0.0007424684008583426
iteration 42, loss = 0.0010319469729438424
iteration 43, loss = 0.0009555439464747906
iteration 44, loss = 0.0015702634118497372
iteration 45, loss = 0.0008415548363700509
iteration 46, loss = 0.0008261268376372755
iteration 47, loss = 0.0010375919518992305
iteration 48, loss = 0.0011616533156484365
iteration 49, loss = 0.0008493497734889388
iteration 50, loss = 0.001023962744511664
iteration 51, loss = 0.0009787939488887787
iteration 52, loss = 0.0009182981448248029
iteration 53, loss = 0.0013725083554163575
iteration 54, loss = 0.0008810135186649859
iteration 55, loss = 0.0008660013554617763
iteration 56, loss = 0.001540584024041891
iteration 57, loss = 0.0008108470938168466
iteration 58, loss = 0.0010853790445253253
iteration 59, loss = 0.0008243938209488988
iteration 60, loss = 0.0007633633795194328
iteration 61, loss = 0.0011179406428709626
iteration 62, loss = 0.0008675243589095771
iteration 63, loss = 0.0007935383473522961
iteration 64, loss = 0.0007519327918998897
iteration 65, loss = 0.0009410034981556237
iteration 66, loss = 0.0008968229521997273
iteration 67, loss = 0.0008591922232881188
iteration 68, loss = 0.0008109807386063039
iteration 69, loss = 0.002165410900488496
iteration 70, loss = 0.0007604309939779341
iteration 71, loss = 0.0012160296319052577
iteration 72, loss = 0.0008763080695644021
iteration 73, loss = 0.0007792306132614613
iteration 74, loss = 0.0012089377269148827
iteration 75, loss = 0.000810963858384639
iteration 76, loss = 0.0014471958857029676
iteration 77, loss = 0.0009770129108801484
iteration 78, loss = 0.0011239683954045177
iteration 79, loss = 0.000985431601293385
iteration 80, loss = 0.000903454958461225
iteration 81, loss = 0.0019290311029180884
iteration 82, loss = 0.0009985369397327304
iteration 83, loss = 0.0011672843247652054
iteration 84, loss = 0.0008738430333323777
iteration 85, loss = 0.0009785996517166495
iteration 86, loss = 0.001621895469725132
iteration 87, loss = 0.0019950212445110083
iteration 88, loss = 0.0009463363094255328
iteration 89, loss = 0.001485037268139422
iteration 90, loss = 0.0011645776685327291
iteration 91, loss = 0.000938256096560508
iteration 92, loss = 0.0006893032696098089
iteration 93, loss = 0.0008922704728320241
iteration 94, loss = 0.0011080008698627353
iteration 95, loss = 0.0007606337312608957
iteration 96, loss = 0.0010962941450998187
iteration 97, loss = 0.0009398971451446414
iteration 98, loss = 0.0008076735539361835
iteration 99, loss = 0.0008564240415580571
iteration 100, loss = 0.0009005340980365872
iteration 101, loss = 0.000805748044513166
iteration 102, loss = 0.0007160637178458273
iteration 103, loss = 0.0009907991625368595
iteration 104, loss = 0.0008749187109060585
iteration 105, loss = 0.0010017845779657364
iteration 106, loss = 0.0007862912025302649
iteration 107, loss = 0.0010756959673017263
iteration 108, loss = 0.0016397974686697125
iteration 109, loss = 0.000919890240766108
iteration 110, loss = 0.0007462867652066052
iteration 111, loss = 0.0010351482778787613
iteration 112, loss = 0.0007568647270090878
iteration 113, loss = 0.0009140464244410396
iteration 114, loss = 0.0009822145802900195
iteration 115, loss = 0.0014353034785017371
iteration 116, loss = 0.0008249792736023664
iteration 117, loss = 0.001075367908924818
iteration 118, loss = 0.0009883864549919963
iteration 119, loss = 0.0008690192480571568
iteration 120, loss = 0.0013071141438558698
iteration 121, loss = 0.0009299056837335229
iteration 122, loss = 0.0007405673968605697
iteration 123, loss = 0.0010008882964029908
iteration 124, loss = 0.0009592068381607533
iteration 125, loss = 0.0008653128170408309
iteration 126, loss = 0.0010696544777601957
iteration 127, loss = 0.0008874433115124702
iteration 128, loss = 0.0009305595885962248
iteration 129, loss = 0.001126915100030601
iteration 130, loss = 0.0009835556847974658
iteration 131, loss = 0.0010685931192710996
iteration 132, loss = 0.0009613874717615545
iteration 133, loss = 0.0010403558844700456
iteration 134, loss = 0.0008943160064518452
iteration 135, loss = 0.0009440772118978202
iteration 136, loss = 0.0007120188674889505
iteration 137, loss = 0.001256904797628522
iteration 138, loss = 0.0011256637517362833
iteration 139, loss = 0.001421371242031455
iteration 140, loss = 0.0008595118415541947
iteration 141, loss = 0.0017379530472680926
iteration 142, loss = 0.001125571085140109
iteration 143, loss = 0.0010695912642404437
iteration 144, loss = 0.0011223664041608572
iteration 145, loss = 0.0007797133293934166
iteration 146, loss = 0.0009492494282312691
iteration 147, loss = 0.0009836958488449454
iteration 148, loss = 0.0017698273295536637
iteration 149, loss = 0.0015036389231681824
iteration 150, loss = 0.0009726106654852629
iteration 151, loss = 0.0009134543361142278
iteration 152, loss = 0.0007867648382671177
iteration 153, loss = 0.001025710254907608
iteration 154, loss = 0.00104701635427773
iteration 155, loss = 0.0008065879228524864
iteration 156, loss = 0.0010380344465374947
iteration 157, loss = 0.0009044735925272107
iteration 158, loss = 0.0008112210198305547
iteration 159, loss = 0.0008374165627174079
iteration 160, loss = 0.0009783926652744412
iteration 161, loss = 0.0009244042448699474
iteration 162, loss = 0.0007723879534751177
iteration 163, loss = 0.0007994903717190027
iteration 164, loss = 0.0008992787916213274
iteration 165, loss = 0.0008754822192713618
iteration 166, loss = 0.0015585569199174643
iteration 167, loss = 0.0009275295888073742
iteration 168, loss = 0.0008442274993285537
iteration 169, loss = 0.0009346003644168377
iteration 170, loss = 0.0008825921104289591
iteration 171, loss = 0.0007152177859097719
iteration 172, loss = 0.0007716896943747997
iteration 173, loss = 0.0008911428740248084
iteration 174, loss = 0.0012658800696954131
iteration 175, loss = 0.0011127492180094123
iteration 176, loss = 0.0008251411491073668
iteration 177, loss = 0.0007759174914099276
iteration 178, loss = 0.0010543371317908168
iteration 179, loss = 0.0008622940513305366
iteration 180, loss = 0.000952844275161624
iteration 181, loss = 0.000882055377587676
iteration 182, loss = 0.0009497221326455474
iteration 183, loss = 0.0014694586861878633
iteration 184, loss = 0.0012359771644696593
iteration 185, loss = 0.00098477303981781
iteration 186, loss = 0.0008135343086905777
iteration 187, loss = 0.0009765673894435167
iteration 188, loss = 0.0011495107319206
iteration 189, loss = 0.0007704767631366849
iteration 190, loss = 0.0008990128990262747
iteration 191, loss = 0.0007724817842245102
iteration 192, loss = 0.0008037408697418869
iteration 193, loss = 0.0011571339564397931
iteration 194, loss = 0.0007421078043989837
iteration 195, loss = 0.0009147758828476071
iteration 196, loss = 0.001481357030570507
iteration 197, loss = 0.0008233386324718595
iteration 198, loss = 0.0008987204637378454
iteration 199, loss = 0.001792454975657165
iteration 200, loss = 0.0009091095416806638
iteration 201, loss = 0.000901858729775995
iteration 202, loss = 0.000915371987503022
iteration 203, loss = 0.0008600299479439855
iteration 204, loss = 0.0007586257415823638
iteration 205, loss = 0.0009480093722231686
iteration 206, loss = 0.001703258021734655
iteration 207, loss = 0.0009946718346327543
iteration 208, loss = 0.0008045400027185678
iteration 209, loss = 0.0013185642892494798
iteration 210, loss = 0.0008467366569675505
iteration 211, loss = 0.001206433866173029
iteration 212, loss = 0.0008563281153328717
iteration 213, loss = 0.0013636172516271472
iteration 214, loss = 0.0008735125884413719
iteration 215, loss = 0.001087220967747271
iteration 216, loss = 0.0008605695329606533
iteration 217, loss = 0.0010907056275755167
iteration 218, loss = 0.0008231145329773426
iteration 219, loss = 0.0008192048990167677
iteration 220, loss = 0.0010912393918260932
iteration 221, loss = 0.0008993251831270754
iteration 222, loss = 0.0012578045716509223
iteration 223, loss = 0.0010596063220873475
iteration 224, loss = 0.0009606681414879858
iteration 225, loss = 0.0007455098675563931
iteration 226, loss = 0.0008406029082834721
iteration 227, loss = 0.0008714983123354614
iteration 228, loss = 0.0008218898437917233
iteration 229, loss = 0.0008014080231077969
iteration 230, loss = 0.0008489405154250562
iteration 231, loss = 0.0018622191855683923
iteration 232, loss = 0.0017906822031363845
iteration 233, loss = 0.0006723297410644591
iteration 234, loss = 0.001005402416922152
iteration 235, loss = 0.0013419976457953453
iteration 236, loss = 0.0008250368409790099
iteration 237, loss = 0.0010174912167713046
iteration 238, loss = 0.0009361943230032921
iteration 239, loss = 0.0009593830909579992
iteration 240, loss = 0.000935047515667975
iteration 241, loss = 0.0010936849284917116
iteration 242, loss = 0.0024854792281985283
iteration 243, loss = 0.0007711960352025926
iteration 244, loss = 0.000704706646502018
iteration 245, loss = 0.0007006193627603352
iteration 246, loss = 0.0009672044543549418
iteration 247, loss = 0.0010259082773700356
iteration 248, loss = 0.0018992067780345678
iteration 249, loss = 0.0009293211624026299
iteration 250, loss = 0.0010412047849968076
iteration 251, loss = 0.0018055700929835439
iteration 252, loss = 0.0008027615840546787
iteration 253, loss = 0.0013029661495238543
iteration 254, loss = 0.0010270128259435296
iteration 255, loss = 0.0009087497601285577
iteration 256, loss = 0.0010231145424768329
iteration 257, loss = 0.0008017639047466218
iteration 258, loss = 0.001765269786119461
iteration 259, loss = 0.0010602001566439867
iteration 260, loss = 0.0008455199422314763
iteration 261, loss = 0.001195921446196735
iteration 262, loss = 0.002095098840072751
iteration 263, loss = 0.0011231055250391364
iteration 264, loss = 0.0008568677585572004
iteration 265, loss = 0.0010173950577154756
iteration 266, loss = 0.0007939268834888935
iteration 267, loss = 0.0008372438023798168
iteration 268, loss = 0.0007935908506624401
iteration 269, loss = 0.0010114938486367464
iteration 270, loss = 0.000984074780717492
iteration 271, loss = 0.0007844045758247375
iteration 272, loss = 0.0014180056750774384
iteration 273, loss = 0.001254900824278593
iteration 274, loss = 0.0008912479970604181
iteration 275, loss = 0.0010857039596885443
iteration 276, loss = 0.0008906502043828368
iteration 277, loss = 0.0009908585343509912
iteration 278, loss = 0.0011206150520592928
iteration 279, loss = 0.0013842342887073755
iteration 280, loss = 0.001102639827877283
iteration 281, loss = 0.0013192729093134403
iteration 282, loss = 0.0009129639947786927
iteration 283, loss = 0.0008066161535680294
iteration 284, loss = 0.0009573274292051792
iteration 285, loss = 0.0008729899418540299
iteration 286, loss = 0.0007727560005150735
iteration 287, loss = 0.0009468072676099837
iteration 288, loss = 0.0009484412148594856
iteration 289, loss = 0.0011757583124563098
iteration 290, loss = 0.0011826080735772848
iteration 291, loss = 0.0008489498286508024
iteration 292, loss = 0.0010968397837132215
iteration 293, loss = 0.0010402401676401496
iteration 294, loss = 0.0007361865718849003
iteration 295, loss = 0.0011241701431572437
iteration 296, loss = 0.0009083328186534345
iteration 297, loss = 0.0008898715022951365
iteration 298, loss = 0.0016778961289674044
iteration 299, loss = 0.001070900820195675
iteration 300, loss = 0.0019016435835510492
iteration 1, loss = 0.002336873672902584
iteration 2, loss = 0.0007643706630915403
iteration 3, loss = 0.0009778131498023868
iteration 4, loss = 0.000926958629861474
iteration 5, loss = 0.0017274413257837296
iteration 6, loss = 0.0010899301851168275
iteration 7, loss = 0.0008358643390238285
iteration 8, loss = 0.0011497379746288061
iteration 9, loss = 0.0018448166083544493
iteration 10, loss = 0.00099758745636791
iteration 11, loss = 0.001236908370628953
iteration 12, loss = 0.00103930348996073
iteration 13, loss = 0.0010869039688259363
iteration 14, loss = 0.0010637355735525489
iteration 15, loss = 0.0007544354302808642
iteration 16, loss = 0.0009779753163456917
iteration 17, loss = 0.0010248410981148481
iteration 18, loss = 0.0008388058631680906
iteration 19, loss = 0.0011849964503198862
iteration 20, loss = 0.0010197404772043228
iteration 21, loss = 0.0009143458446487784
iteration 22, loss = 0.000858870567753911
iteration 23, loss = 0.0014063423732295632
iteration 24, loss = 0.00104897643905133
iteration 25, loss = 0.000764843134675175
iteration 26, loss = 0.0007114870240911841
iteration 27, loss = 0.0008665395434945822
iteration 28, loss = 0.0013612400507554412
iteration 29, loss = 0.0009546976070851088
iteration 30, loss = 0.0009329369058832526
iteration 31, loss = 0.0010898960754275322
iteration 32, loss = 0.0009479564614593983
iteration 33, loss = 0.0011807020055130124
iteration 34, loss = 0.0016003422206267715
iteration 35, loss = 0.0008301946800202131
iteration 36, loss = 0.0009721294045448303
iteration 37, loss = 0.001004362478852272
iteration 38, loss = 0.0009416164830327034
iteration 39, loss = 0.0008109336486086249
iteration 40, loss = 0.0009584230720065534
iteration 41, loss = 0.0010831583058461547
iteration 42, loss = 0.000809884222690016
iteration 43, loss = 0.0009020019788295031
iteration 44, loss = 0.001193829346448183
iteration 45, loss = 0.0011138735571876168
iteration 46, loss = 0.0009711809689179063
iteration 47, loss = 0.0014079828979447484
iteration 48, loss = 0.0008515505469404161
iteration 49, loss = 0.0009535843855701387
iteration 50, loss = 0.0010203388519585133
iteration 51, loss = 0.0012321730609983206
iteration 52, loss = 0.0009673290187492967
iteration 53, loss = 0.0010809025261551142
iteration 54, loss = 0.000705361773725599
iteration 55, loss = 0.0008554643718525767
iteration 56, loss = 0.0009571582195349038
iteration 57, loss = 0.0007524204556830227
iteration 58, loss = 0.0008533915970474482
iteration 59, loss = 0.0008732114220038056
iteration 60, loss = 0.0018307858845219016
iteration 61, loss = 0.0012147343950346112
iteration 62, loss = 0.0010632551275193691
iteration 63, loss = 0.0008263647323474288
iteration 64, loss = 0.0006810202612541616
iteration 65, loss = 0.0007795231067575514
iteration 66, loss = 0.0009436071268282831
iteration 67, loss = 0.0008081138948909938
iteration 68, loss = 0.0011836555786430836
iteration 69, loss = 0.0010172787588089705
iteration 70, loss = 0.001015024958178401
iteration 71, loss = 0.0009140545735135674
iteration 72, loss = 0.0008607421768829226
iteration 73, loss = 0.0017643413739278913
iteration 74, loss = 0.000705474813003093
iteration 75, loss = 0.0017646774649620056
iteration 76, loss = 0.001116839936003089
iteration 77, loss = 0.0009040976874530315
iteration 78, loss = 0.0007239430560730398
iteration 79, loss = 0.0008027969161048532
iteration 80, loss = 0.0012517968425527215
iteration 81, loss = 0.0007047899416647851
iteration 82, loss = 0.0008321100031025708
iteration 83, loss = 0.0008269002428278327
iteration 84, loss = 0.0009776210645213723
iteration 85, loss = 0.0009151378180831671
iteration 86, loss = 0.0013299454003572464
iteration 87, loss = 0.0008421540260314941
iteration 88, loss = 0.0008936875965446234
iteration 89, loss = 0.0008767197141423821
iteration 90, loss = 0.0015987639781087637
iteration 91, loss = 0.0009661500807851553
iteration 92, loss = 0.0007480622152797878
iteration 93, loss = 0.0007758307037875056
iteration 94, loss = 0.0011817286722362041
iteration 95, loss = 0.0010034939041361213
iteration 96, loss = 0.0008030659519135952
iteration 97, loss = 0.0010355152189731598
iteration 98, loss = 0.000985879567451775
iteration 99, loss = 0.0016128781717270613
iteration 100, loss = 0.0008031006436794996
iteration 101, loss = 0.0011839766521006823
iteration 102, loss = 0.0010621198453009129
iteration 103, loss = 0.0007493254961445928
iteration 104, loss = 0.0013066987739875913
iteration 105, loss = 0.0009640834177844226
iteration 106, loss = 0.000790580059401691
iteration 107, loss = 0.000821949855890125
iteration 108, loss = 0.000990177970379591
iteration 109, loss = 0.0009280417580157518
iteration 110, loss = 0.0008738671895116568
iteration 111, loss = 0.0008860298548825085
iteration 112, loss = 0.0012209616834297776
iteration 113, loss = 0.000821075402200222
iteration 114, loss = 0.0008874559425748885
iteration 115, loss = 0.0007667161989957094
iteration 116, loss = 0.0015785901341587305
iteration 117, loss = 0.0008988856570795178
iteration 118, loss = 0.0008400768856517971
iteration 119, loss = 0.0006610159180127084
iteration 120, loss = 0.0008731391280889511
iteration 121, loss = 0.0008558349800296128
iteration 122, loss = 0.0011313245631754398
iteration 123, loss = 0.0010787987848743796
iteration 124, loss = 0.000960423843935132
iteration 125, loss = 0.0008788558188825846
iteration 126, loss = 0.0019418650772422552
iteration 127, loss = 0.0010984980035573244
iteration 128, loss = 0.001383212162181735
iteration 129, loss = 0.0009653844172134995
iteration 130, loss = 0.001127587747760117
iteration 131, loss = 0.0016904808580875397
iteration 132, loss = 0.0009107649675570428
iteration 133, loss = 0.0007231433992274106
iteration 134, loss = 0.0007403035997413099
iteration 135, loss = 0.000869567331392318
iteration 136, loss = 0.001531740534119308
iteration 137, loss = 0.0011137794936075807
iteration 138, loss = 0.0008119115373119712
iteration 139, loss = 0.0009826574241742492
iteration 140, loss = 0.0007172470213845372
iteration 141, loss = 0.0018312219763174653
iteration 142, loss = 0.001223633880726993
iteration 143, loss = 0.0008120849379338324
iteration 144, loss = 0.0009122224291786551
iteration 145, loss = 0.001250498229637742
iteration 146, loss = 0.0016262737335637212
iteration 147, loss = 0.0008677205187268555
iteration 148, loss = 0.0010554310865700245
iteration 149, loss = 0.001092270016670227
iteration 150, loss = 0.0009624509257264435
iteration 151, loss = 0.0012831842759624124
iteration 152, loss = 0.0007679713889956474
iteration 153, loss = 0.0007622962002642453
iteration 154, loss = 0.00072595285018906
iteration 155, loss = 0.001340929651632905
iteration 156, loss = 0.0007334143156185746
iteration 157, loss = 0.0009478389983996749
iteration 158, loss = 0.0009250429575331509
iteration 159, loss = 0.0007995954947546124
iteration 160, loss = 0.0011379496427252889
iteration 161, loss = 0.0008946139132604003
iteration 162, loss = 0.0007811447139829397
iteration 163, loss = 0.0011894508497789502
iteration 164, loss = 0.0009761835681274533
iteration 165, loss = 0.0007754260441288352
iteration 166, loss = 0.0008296705782413483
iteration 167, loss = 0.0007177898660302162
iteration 168, loss = 0.0009879544377326965
iteration 169, loss = 0.0009275475749745965
iteration 170, loss = 0.0009170988341793418
iteration 171, loss = 0.0007769868825562298
iteration 172, loss = 0.0007306333282031119
iteration 173, loss = 0.0012112567201256752
iteration 174, loss = 0.0016540599754080176
iteration 175, loss = 0.0008359590428881347
iteration 176, loss = 0.001026604208163917
iteration 177, loss = 0.0007780729793012142
iteration 178, loss = 0.0017117393435910344
iteration 179, loss = 0.001384412869811058
iteration 180, loss = 0.0012402881402522326
iteration 181, loss = 0.0009511528769508004
iteration 182, loss = 0.0009227659902535379
iteration 183, loss = 0.0007766838534735143
iteration 184, loss = 0.0016853599809110165
iteration 185, loss = 0.000899714941624552
iteration 186, loss = 0.0011225853813812137
iteration 187, loss = 0.0008539651171304286
iteration 188, loss = 0.000721843505743891
iteration 189, loss = 0.001027541235089302
iteration 190, loss = 0.0010119512444362044
iteration 191, loss = 0.0010201138211414218
iteration 192, loss = 0.0011512581259012222
iteration 193, loss = 0.0010029206750914454
iteration 194, loss = 0.0009532287367619574
iteration 195, loss = 0.0010527803096920252
iteration 196, loss = 0.0011476692743599415
iteration 197, loss = 0.001076407264918089
iteration 198, loss = 0.0012665523681789637
iteration 199, loss = 0.0016954790335148573
iteration 200, loss = 0.000791617960203439
iteration 201, loss = 0.0009368772152811289
iteration 202, loss = 0.0011812413576990366
iteration 203, loss = 0.0009861539583653212
iteration 204, loss = 0.0009379997500218451
iteration 205, loss = 0.0011014926712960005
iteration 206, loss = 0.0009136780863627791
iteration 207, loss = 0.0016027925303205848
iteration 208, loss = 0.000904266198631376
iteration 209, loss = 0.000863045861478895
iteration 210, loss = 0.0009580607293173671
iteration 211, loss = 0.0007580936653539538
iteration 212, loss = 0.0012024620082229376
iteration 213, loss = 0.0010970488656312227
iteration 214, loss = 0.0017277809092774987
iteration 215, loss = 0.0008572658989578485
iteration 216, loss = 0.0012563173659145832
iteration 217, loss = 0.0009709309670142829
iteration 218, loss = 0.0007374084088951349
iteration 219, loss = 0.0019385679624974728
iteration 220, loss = 0.0011499060783535242
iteration 221, loss = 0.0008292609709315002
iteration 222, loss = 0.0015010872157290578
iteration 223, loss = 0.000988580984994769
iteration 224, loss = 0.000971171073615551
iteration 225, loss = 0.0011474386556074023
iteration 226, loss = 0.0008688039379194379
iteration 227, loss = 0.001778454054147005
iteration 228, loss = 0.0011269219685345888
iteration 229, loss = 0.0014908905141055584
iteration 230, loss = 0.0009644398232921958
iteration 231, loss = 0.0007713136146776378
iteration 232, loss = 0.0012008377816528082
iteration 233, loss = 0.0007160406094044447
iteration 234, loss = 0.0008264172356575727
iteration 235, loss = 0.0012956359423696995
iteration 236, loss = 0.0010104648536071181
iteration 237, loss = 0.0009152928832918406
iteration 238, loss = 0.000870784220751375
iteration 239, loss = 0.0011709877289831638
iteration 240, loss = 0.0007147984579205513
iteration 241, loss = 0.0013308967463672161
iteration 242, loss = 0.0008804005919955671
iteration 243, loss = 0.0008598900749348104
iteration 244, loss = 0.0007790017407387495
iteration 245, loss = 0.0007928904378786683
iteration 246, loss = 0.0012158771278336644
iteration 247, loss = 0.0008365266839973629
iteration 248, loss = 0.001455830060876906
iteration 249, loss = 0.0008434109622612596
iteration 250, loss = 0.0008587827906012535
iteration 251, loss = 0.000914710690267384
iteration 252, loss = 0.0011747624957934022
iteration 253, loss = 0.0008360800566151738
iteration 254, loss = 0.000952475646045059
iteration 255, loss = 0.0008909600437618792
iteration 256, loss = 0.0008043890120461583
iteration 257, loss = 0.0012098177103325725
iteration 258, loss = 0.0011899282690137625
iteration 259, loss = 0.0008057492668740451
iteration 260, loss = 0.0008890670724213123
iteration 261, loss = 0.0008404316613450646
iteration 262, loss = 0.0007755915867164731
iteration 263, loss = 0.0012515070848166943
iteration 264, loss = 0.0008650160161778331
iteration 265, loss = 0.0008668552036397159
iteration 266, loss = 0.0012680238578468561
iteration 267, loss = 0.001219342928379774
iteration 268, loss = 0.0010319141438230872
iteration 269, loss = 0.0016925649251788855
iteration 270, loss = 0.0008832290768623352
iteration 271, loss = 0.0010488962288945913
iteration 272, loss = 0.0016517848707735538
iteration 273, loss = 0.0008670871611684561
iteration 274, loss = 0.0008008158765733242
iteration 275, loss = 0.0008838222711347044
iteration 276, loss = 0.0011809453135356307
iteration 277, loss = 0.0010334214894101024
iteration 278, loss = 0.0008101745042949915
iteration 279, loss = 0.0008124120649881661
iteration 280, loss = 0.0009098951704800129
iteration 281, loss = 0.0008805138641037047
iteration 282, loss = 0.0008166403276845813
iteration 283, loss = 0.0009039724245667458
iteration 284, loss = 0.0007313648238778114
iteration 285, loss = 0.0010308403288945556
iteration 286, loss = 0.0012321807444095612
iteration 287, loss = 0.001785186119377613
iteration 288, loss = 0.0009494403493590653
iteration 289, loss = 0.0008233831031247973
iteration 290, loss = 0.0008936304366216063
iteration 291, loss = 0.0008710005786269903
iteration 292, loss = 0.0008635343983769417
iteration 293, loss = 0.001197419478558004
iteration 294, loss = 0.0008868630393408239
iteration 295, loss = 0.001921914517879486
iteration 296, loss = 0.001074423547834158
iteration 297, loss = 0.0007926013204269111
iteration 298, loss = 0.0008287687669508159
iteration 299, loss = 0.0015294740442186594
iteration 300, loss = 0.0008581829024478793
iteration 1, loss = 0.0008102472638711333
iteration 2, loss = 0.0008466318831779063
iteration 3, loss = 0.0009604807128198445
iteration 4, loss = 0.0008590201032347977
iteration 5, loss = 0.000761960749514401
iteration 6, loss = 0.0012908170465379953
iteration 7, loss = 0.0008868158329278231
iteration 8, loss = 0.0011875176569446921
iteration 9, loss = 0.0009300491074100137
iteration 10, loss = 0.0009058136492967606
iteration 11, loss = 0.0008046464063227177
iteration 12, loss = 0.0011701929615810513
iteration 13, loss = 0.001407062984071672
iteration 14, loss = 0.0009471288067288697
iteration 15, loss = 0.0007541435770690441
iteration 16, loss = 0.0008266796940006316
iteration 17, loss = 0.0009002237347885966
iteration 18, loss = 0.0011849806178361177
iteration 19, loss = 0.001524294726550579
iteration 20, loss = 0.0009626464452594519
iteration 21, loss = 0.0012771834153681993
iteration 22, loss = 0.002077530138194561
iteration 23, loss = 0.0009222647058777511
iteration 24, loss = 0.001265142229385674
iteration 25, loss = 0.0011155573884025216
iteration 26, loss = 0.0013537376653403044
iteration 27, loss = 0.0008503730641677976
iteration 28, loss = 0.0012218368938192725
iteration 29, loss = 0.0007963356329128146
iteration 30, loss = 0.0006698495708405972
iteration 31, loss = 0.0009706097189337015
iteration 32, loss = 0.0008051284239627421
iteration 33, loss = 0.0011524118017405272
iteration 34, loss = 0.0009612810099497437
iteration 35, loss = 0.0009068258805200458
iteration 36, loss = 0.0007674377411603928
iteration 37, loss = 0.0009736407664604485
iteration 38, loss = 0.0008814353495836258
iteration 39, loss = 0.0020480253733694553
iteration 40, loss = 0.0016163702821359038
iteration 41, loss = 0.0016471811104565859
iteration 42, loss = 0.0008215910056605935
iteration 43, loss = 0.0009960695169866085
iteration 44, loss = 0.0011104225413873792
iteration 45, loss = 0.0007359192240983248
iteration 46, loss = 0.0010047613177448511
iteration 47, loss = 0.0008589697536081076
iteration 48, loss = 0.0011137928813695908
iteration 49, loss = 0.0008664411725476384
iteration 50, loss = 0.0006893638055771589
iteration 51, loss = 0.0008304109796881676
iteration 52, loss = 0.0008600325090810657
iteration 53, loss = 0.0007705363677814603
iteration 54, loss = 0.0008689764072187245
iteration 55, loss = 0.0009719618246890604
iteration 56, loss = 0.0008772462024353445
iteration 57, loss = 0.0011894822819158435
iteration 58, loss = 0.001813802751712501
iteration 59, loss = 0.0008212376851588488
iteration 60, loss = 0.0010496723698452115
iteration 61, loss = 0.0011475543724372983
iteration 62, loss = 0.001999049913138151
iteration 63, loss = 0.0009552219999022782
iteration 64, loss = 0.0016315483953803778
iteration 65, loss = 0.001148924813605845
iteration 66, loss = 0.0008170059882104397
iteration 67, loss = 0.0009146067313849926
iteration 68, loss = 0.0007117273053154349
iteration 69, loss = 0.000746605102904141
iteration 70, loss = 0.001073356717824936
iteration 71, loss = 0.0008458869997411966
iteration 72, loss = 0.0010636012302711606
iteration 73, loss = 0.0008990137139335275
iteration 74, loss = 0.0011279504979029298
iteration 75, loss = 0.0008582038572058082
iteration 76, loss = 0.0011653059627860785
iteration 77, loss = 0.0011847279965877533
iteration 78, loss = 0.0007290511857718229
iteration 79, loss = 0.0009558327728882432
iteration 80, loss = 0.001097487867809832
iteration 81, loss = 0.0011792812729254365
iteration 82, loss = 0.0008685856591910124
iteration 83, loss = 0.0015760465757921338
iteration 84, loss = 0.000626003195066005
iteration 85, loss = 0.0014480382669717073
iteration 86, loss = 0.000984481070190668
iteration 87, loss = 0.001137897022999823
iteration 88, loss = 0.0011982439318671823
iteration 89, loss = 0.001773630385287106
iteration 90, loss = 0.0007724997703917325
iteration 91, loss = 0.0009294968331232667
iteration 92, loss = 0.0009296565549448133
iteration 93, loss = 0.0007263505249284208
iteration 94, loss = 0.001229162560775876
iteration 95, loss = 0.0010240132687613368
iteration 96, loss = 0.0009730505407787859
iteration 97, loss = 0.0009863933082669973
iteration 98, loss = 0.0009268611902371049
iteration 99, loss = 0.0008878835942596197
iteration 100, loss = 0.0008659418090246618
iteration 101, loss = 0.0008004704723134637
iteration 102, loss = 0.0008855866617523134
iteration 103, loss = 0.0015580240869894624
iteration 104, loss = 0.0007378128939308226
iteration 105, loss = 0.001478985184803605
iteration 106, loss = 0.0008141724392771721
iteration 107, loss = 0.0008927772869355977
iteration 108, loss = 0.0011748246615752578
iteration 109, loss = 0.0008066003210842609
iteration 110, loss = 0.0008534092921763659
iteration 111, loss = 0.0009654207970015705
iteration 112, loss = 0.0008665315108373761
iteration 113, loss = 0.0009382808348163962
iteration 114, loss = 0.0010039715562015772
iteration 115, loss = 0.0009719240479171276
iteration 116, loss = 0.0008047462906688452
iteration 117, loss = 0.0008285519434139132
iteration 118, loss = 0.0008252048282884061
iteration 119, loss = 0.0013139224611222744
iteration 120, loss = 0.0007880423218011856
iteration 121, loss = 0.0007033593719825149
iteration 122, loss = 0.0008536175009794533
iteration 123, loss = 0.0010200049728155136
iteration 124, loss = 0.001597876544110477
iteration 125, loss = 0.0009141222108155489
iteration 126, loss = 0.0010711683426052332
iteration 127, loss = 0.0010388318914920092
iteration 128, loss = 0.0011553624644875526
iteration 129, loss = 0.0010261073475703597
iteration 130, loss = 0.0008133260998874903
iteration 131, loss = 0.000814240425825119
iteration 132, loss = 0.0009543626219965518
iteration 133, loss = 0.0009289609733968973
iteration 134, loss = 0.0009085019119083881
iteration 135, loss = 0.0009597648750059307
iteration 136, loss = 0.0008539885166101158
iteration 137, loss = 0.0008342860965058208
iteration 138, loss = 0.001334929489530623
iteration 139, loss = 0.0009717504726722836
iteration 140, loss = 0.0008853671606630087
iteration 141, loss = 0.001198688754811883
iteration 142, loss = 0.0008479544776491821
iteration 143, loss = 0.001035127672366798
iteration 144, loss = 0.0011572655057534575
iteration 145, loss = 0.0009754261118359864
iteration 146, loss = 0.0009608670370653272
iteration 147, loss = 0.0009599344921298325
iteration 148, loss = 0.0008843063842505217
iteration 149, loss = 0.0009269197471439838
iteration 150, loss = 0.0007590247550979257
iteration 151, loss = 0.0012777659576386213
iteration 152, loss = 0.0008074386278167367
iteration 153, loss = 0.0010605212301015854
iteration 154, loss = 0.0012735944474115968
iteration 155, loss = 0.0007727734628133476
iteration 156, loss = 0.0008516391390003264
iteration 157, loss = 0.000780406582634896
iteration 158, loss = 0.0010413031559437513
iteration 159, loss = 0.0007851538830436766
iteration 160, loss = 0.0007386505603790283
iteration 161, loss = 0.0008448819280602038
iteration 162, loss = 0.0013602575054392219
iteration 163, loss = 0.0009989156387746334
iteration 164, loss = 0.0010503014782443643
iteration 165, loss = 0.00079410953912884
iteration 166, loss = 0.0008205021731555462
iteration 167, loss = 0.001126062241382897
iteration 168, loss = 0.002519522560760379
iteration 169, loss = 0.0010177255608141422
iteration 170, loss = 0.0008144386811181903
iteration 171, loss = 0.0012041348963975906
iteration 172, loss = 0.0010519310599192977
iteration 173, loss = 0.0007819075835868716
iteration 174, loss = 0.000798678956925869
iteration 175, loss = 0.0009374163928441703
iteration 176, loss = 0.0009190768469125032
iteration 177, loss = 0.001003012410365045
iteration 178, loss = 0.0009529703529551625
iteration 179, loss = 0.0007832294213585556
iteration 180, loss = 0.0012590731494128704
iteration 181, loss = 0.0006987825036048889
iteration 182, loss = 0.001755644683726132
iteration 183, loss = 0.0011598555138334632
iteration 184, loss = 0.00126751943025738
iteration 185, loss = 0.0008189824875444174
iteration 186, loss = 0.000744329416193068
iteration 187, loss = 0.0009111675317399204
iteration 188, loss = 0.0010732406517490745
iteration 189, loss = 0.0009888462955132127
iteration 190, loss = 0.0008733810391277075
iteration 191, loss = 0.0009800526313483715
iteration 192, loss = 0.0008215817506425083
iteration 193, loss = 0.0011966347228735685
iteration 194, loss = 0.0007125120027922094
iteration 195, loss = 0.0010660655098035932
iteration 196, loss = 0.0007774601108394563
iteration 197, loss = 0.0008258268935605884
iteration 198, loss = 0.001036357250995934
iteration 199, loss = 0.0007694085361436009
iteration 200, loss = 0.001328568090684712
iteration 201, loss = 0.0007639577379450202
iteration 202, loss = 0.0008737302268855274
iteration 203, loss = 0.0009012088412418962
iteration 204, loss = 0.0011684150667861104
iteration 205, loss = 0.0016324849566444755
iteration 206, loss = 0.001660433947108686
iteration 207, loss = 0.0009481817251071334
iteration 208, loss = 0.000685427337884903
iteration 209, loss = 0.0007867265958338976
iteration 210, loss = 0.0008697136072441936
iteration 211, loss = 0.0008536957902833819
iteration 212, loss = 0.0010063009103760123
iteration 213, loss = 0.0018464324530214071
iteration 214, loss = 0.000886176829226315
iteration 215, loss = 0.0008057940285652876
iteration 216, loss = 0.0014603183371946216
iteration 217, loss = 0.0016660714754834771
iteration 218, loss = 0.0008931061020120978
iteration 219, loss = 0.001256580580957234
iteration 220, loss = 0.0013932031579315662
iteration 221, loss = 0.0017706452636048198
iteration 222, loss = 0.000891898525878787
iteration 223, loss = 0.0016767922788858414
iteration 224, loss = 0.0010088045382872224
iteration 225, loss = 0.0018202806822955608
iteration 226, loss = 0.0010976389748975635
iteration 227, loss = 0.0012738810619339347
iteration 228, loss = 0.00112922431435436
iteration 229, loss = 0.0008682549232617021
iteration 230, loss = 0.0008791522704996169
iteration 231, loss = 0.0011165582109242678
iteration 232, loss = 0.0010137702338397503
iteration 233, loss = 0.0008830160368233919
iteration 234, loss = 0.000921566563192755
iteration 235, loss = 0.0011076732771471143
iteration 236, loss = 0.0016442514024674892
iteration 237, loss = 0.0008419587975367904
iteration 238, loss = 0.0008808370912447572
iteration 239, loss = 0.000960257719270885
iteration 240, loss = 0.0008718350436538458
iteration 241, loss = 0.0007640369003638625
iteration 242, loss = 0.0011932855704799294
iteration 243, loss = 0.0011736975284293294
iteration 244, loss = 0.0012148568639531732
iteration 245, loss = 0.001261328812688589
iteration 246, loss = 0.0009838835103437304
iteration 247, loss = 0.0012440728023648262
iteration 248, loss = 0.0009654450695961714
iteration 249, loss = 0.0012560458853840828
iteration 250, loss = 0.0009512586984783411
iteration 251, loss = 0.0011072466149926186
iteration 252, loss = 0.0009039990836754441
iteration 253, loss = 0.0023165021557360888
iteration 254, loss = 0.0007615267531946301
iteration 255, loss = 0.0009308349108323455
iteration 256, loss = 0.0008808271959424019
iteration 257, loss = 0.0017944384599104524
iteration 258, loss = 0.0013651404296979308
iteration 259, loss = 0.001292673172429204
iteration 260, loss = 0.0011088375467807055
iteration 261, loss = 0.000921795261092484
iteration 262, loss = 0.001487822737544775
iteration 263, loss = 0.000812387210316956
iteration 264, loss = 0.0016140571096912026
iteration 265, loss = 0.0007844920619390905
iteration 266, loss = 0.0007017762982286513
iteration 267, loss = 0.0007635332876816392
iteration 268, loss = 0.0007845774525776505
iteration 269, loss = 0.00105753680691123
iteration 270, loss = 0.0011441890383139253
iteration 271, loss = 0.0007875653682276607
iteration 272, loss = 0.0008711160044185817
iteration 273, loss = 0.0011551962234079838
iteration 274, loss = 0.0006700228550471365
iteration 275, loss = 0.0008000084781087935
iteration 276, loss = 0.002064160304144025
iteration 277, loss = 0.0011486352887004614
iteration 278, loss = 0.0007718262495473027
iteration 279, loss = 0.0008372882148250937
iteration 280, loss = 0.0008347805123776197
iteration 281, loss = 0.0008775523165240884
iteration 282, loss = 0.0008378610946238041
iteration 283, loss = 0.0008873397600837052
iteration 284, loss = 0.0007022133795544505
iteration 285, loss = 0.0009943607728928328
iteration 286, loss = 0.0008278338355012238
iteration 287, loss = 0.0011668528895825148
iteration 288, loss = 0.0009967914083972573
iteration 289, loss = 0.0009135677246376872
iteration 290, loss = 0.0007771126693114638
iteration 291, loss = 0.0007182545959949493
iteration 292, loss = 0.0008143630693666637
iteration 293, loss = 0.0008158106356859207
iteration 294, loss = 0.0010007814271375537
iteration 295, loss = 0.0007698155241087079
iteration 296, loss = 0.000771873164921999
iteration 297, loss = 0.0008682345505803823
iteration 298, loss = 0.0008013561018742621
iteration 299, loss = 0.0008571913931518793
iteration 300, loss = 0.0008220046875067055
iteration 1, loss = 0.0007553292089141905
iteration 2, loss = 0.000823147885967046
iteration 3, loss = 0.0008164524915628135
iteration 4, loss = 0.000795944535639137
iteration 5, loss = 0.001132517121732235
iteration 6, loss = 0.0018552500987425447
iteration 7, loss = 0.0007791977841407061
iteration 8, loss = 0.0012094705598428845
iteration 9, loss = 0.0012130580144003034
iteration 10, loss = 0.0009679276845417917
iteration 11, loss = 0.0006759908865205944
iteration 12, loss = 0.0006824996089562774
iteration 13, loss = 0.001986193470656872
iteration 14, loss = 0.0012794327922165394
iteration 15, loss = 0.0011255145072937012
iteration 16, loss = 0.0007943062810227275
iteration 17, loss = 0.0016203827690333128
iteration 18, loss = 0.0007042848155833781
iteration 19, loss = 0.000988848740234971
iteration 20, loss = 0.0009734985651448369
iteration 21, loss = 0.0008535489905625582
iteration 22, loss = 0.0014705364592373371
iteration 23, loss = 0.000844663183670491
iteration 24, loss = 0.0007012420683167875
iteration 25, loss = 0.0008715767180547118
iteration 26, loss = 0.0010364641202613711
iteration 27, loss = 0.0016448735259473324
iteration 28, loss = 0.0008567988406866789
iteration 29, loss = 0.0009190220152959228
iteration 30, loss = 0.0011220492888242006
iteration 31, loss = 0.0008884613635018468
iteration 32, loss = 0.0007404232164844871
iteration 33, loss = 0.0009285684791393578
iteration 34, loss = 0.0009636869071982801
iteration 35, loss = 0.0008800502400845289
iteration 36, loss = 0.0009274391341023147
iteration 37, loss = 0.0008177333511412144
iteration 38, loss = 0.001227294560521841
iteration 39, loss = 0.001091711688786745
iteration 40, loss = 0.0007013294380158186
iteration 41, loss = 0.0009417154360562563
iteration 42, loss = 0.0008961055427789688
iteration 43, loss = 0.0009623516816645861
iteration 44, loss = 0.0008824913529679179
iteration 45, loss = 0.0007921030046418309
iteration 46, loss = 0.0007677351240999997
iteration 47, loss = 0.0007914340822026134
iteration 48, loss = 0.0022513531148433685
iteration 49, loss = 0.0008802666561678052
iteration 50, loss = 0.0011771006975322962
iteration 51, loss = 0.0006635208264924586
iteration 52, loss = 0.0009695599437691271
iteration 53, loss = 0.0010350222000852227
iteration 54, loss = 0.0008734579314477742
iteration 55, loss = 0.000880874169524759
iteration 56, loss = 0.00127484742552042
iteration 57, loss = 0.0007494507008232176
iteration 58, loss = 0.0009449197677895427
iteration 59, loss = 0.0010536741465330124
iteration 60, loss = 0.001054548192769289
iteration 61, loss = 0.0007423516944982111
iteration 62, loss = 0.0008508535102009773
iteration 63, loss = 0.0010471452260389924
iteration 64, loss = 0.0009696828201413155
iteration 65, loss = 0.0008474148926325142
iteration 66, loss = 0.0011319126933813095
iteration 67, loss = 0.001017896574921906
iteration 68, loss = 0.0008135862881317735
iteration 69, loss = 0.0012174039147794247
iteration 70, loss = 0.001699511893093586
iteration 71, loss = 0.0008128694025799632
iteration 72, loss = 0.0011852085590362549
iteration 73, loss = 0.0018376526422798634
iteration 74, loss = 0.0007653537322767079
iteration 75, loss = 0.0008225106867030263
iteration 76, loss = 0.0011717830784618855
iteration 77, loss = 0.0009613058646209538
iteration 78, loss = 0.0008807066478766501
iteration 79, loss = 0.0008856364875100553
iteration 80, loss = 0.001146585331298411
iteration 81, loss = 0.0011215836275368929
iteration 82, loss = 0.000783363007940352
iteration 83, loss = 0.0009309962624683976
iteration 84, loss = 0.0011938615934923291
iteration 85, loss = 0.0009358573006466031
iteration 86, loss = 0.001189507427625358
iteration 87, loss = 0.0008880755049176514
iteration 88, loss = 0.0008979812264442444
iteration 89, loss = 0.0007923246012069285
iteration 90, loss = 0.0008360196952708066
iteration 91, loss = 0.0010401710169389844
iteration 92, loss = 0.0009579627076163888
iteration 93, loss = 0.0008879724773578346
iteration 94, loss = 0.0007753520621918142
iteration 95, loss = 0.0009368659812025726
iteration 96, loss = 0.0008080904372036457
iteration 97, loss = 0.0016542027005925775
iteration 98, loss = 0.0008568924386054277
iteration 99, loss = 0.0019847783260047436
iteration 100, loss = 0.0008803005912341177
iteration 101, loss = 0.001203063759021461
iteration 102, loss = 0.0008133084047585726
iteration 103, loss = 0.0012300093658268452
iteration 104, loss = 0.0009489122312515974
iteration 105, loss = 0.0007967494311742485
iteration 106, loss = 0.0010532698361203074
iteration 107, loss = 0.0007818586891517043
iteration 108, loss = 0.0013659916585311294
iteration 109, loss = 0.0009432868682779372
iteration 110, loss = 0.0013235125225037336
iteration 111, loss = 0.0010224876459687948
iteration 112, loss = 0.001107758260332048
iteration 113, loss = 0.0010754759423434734
iteration 114, loss = 0.0009078522562049329
iteration 115, loss = 0.0009574894793331623
iteration 116, loss = 0.0007370836101472378
iteration 117, loss = 0.0008764556841924787
iteration 118, loss = 0.001071570091880858
iteration 119, loss = 0.0008508198079653084
iteration 120, loss = 0.0007880273624323308
iteration 121, loss = 0.0011442051036283374
iteration 122, loss = 0.0007336195558309555
iteration 123, loss = 0.0009058350697159767
iteration 124, loss = 0.0008250164100900292
iteration 125, loss = 0.0013378659496083856
iteration 126, loss = 0.0009144554496742785
iteration 127, loss = 0.0009489698568359017
iteration 128, loss = 0.0017217525746673346
iteration 129, loss = 0.0011564356973394752
iteration 130, loss = 0.000903816195204854
iteration 131, loss = 0.0008816380286589265
iteration 132, loss = 0.0009435631218366325
iteration 133, loss = 0.0012092329561710358
iteration 134, loss = 0.0016496320022270083
iteration 135, loss = 0.0010650958865880966
iteration 136, loss = 0.0009430802892893553
iteration 137, loss = 0.0009914019610732794
iteration 138, loss = 0.0012134067947044969
iteration 139, loss = 0.0007649291655980051
iteration 140, loss = 0.0009643380180932581
iteration 141, loss = 0.0010330600198358297
iteration 142, loss = 0.001022683922201395
iteration 143, loss = 0.0017637662822380662
iteration 144, loss = 0.000930360984057188
iteration 145, loss = 0.0010906483512371778
iteration 146, loss = 0.0010222805431112647
iteration 147, loss = 0.0009360294206999242
iteration 148, loss = 0.000848382362164557
iteration 149, loss = 0.0008071857737377286
iteration 150, loss = 0.0011019476223737001
iteration 151, loss = 0.0008552210056222975
iteration 152, loss = 0.0008263654308393598
iteration 153, loss = 0.0009248766000382602
iteration 154, loss = 0.0007922450895421207
iteration 155, loss = 0.0007066996186040342
iteration 156, loss = 0.002328246831893921
iteration 157, loss = 0.00090011116117239
iteration 158, loss = 0.0008646172354929149
iteration 159, loss = 0.0010766973719000816
iteration 160, loss = 0.001190969836898148
iteration 161, loss = 0.0008101852727122605
iteration 162, loss = 0.0008139369310811162
iteration 163, loss = 0.0009395607048645616
iteration 164, loss = 0.0016246902523562312
iteration 165, loss = 0.0010444390354678035
iteration 166, loss = 0.00101099768653512
iteration 167, loss = 0.0007025939412415028
iteration 168, loss = 0.0016479456098750234
iteration 169, loss = 0.0007805297500453889
iteration 170, loss = 0.0008954218938015401
iteration 171, loss = 0.0010520166251808405
iteration 172, loss = 0.0010177812073379755
iteration 173, loss = 0.0010756663978099823
iteration 174, loss = 0.0008294496219605207
iteration 175, loss = 0.0009393649525009096
iteration 176, loss = 0.0009296340867877007
iteration 177, loss = 0.0009578823810443282
iteration 178, loss = 0.0009915009140968323
iteration 179, loss = 0.0010734217939898372
iteration 180, loss = 0.001060577924363315
iteration 181, loss = 0.0007944617536850274
iteration 182, loss = 0.001069174613803625
iteration 183, loss = 0.001046717632561922
iteration 184, loss = 0.0010341957677155733
iteration 185, loss = 0.0007748376810923219
iteration 186, loss = 0.0011703346390277147
iteration 187, loss = 0.0008263718336820602
iteration 188, loss = 0.0009267040877602994
iteration 189, loss = 0.0009882630547508597
iteration 190, loss = 0.0009889262728393078
iteration 191, loss = 0.001068234909325838
iteration 192, loss = 0.0008520967676304281
iteration 193, loss = 0.0008292993297800422
iteration 194, loss = 0.0011835830518975854
iteration 195, loss = 0.0007660528062842786
iteration 196, loss = 0.0009128559613600373
iteration 197, loss = 0.00102596590295434
iteration 198, loss = 0.0008630593656562269
iteration 199, loss = 0.0008650473901070654
iteration 200, loss = 0.0007157574291341007
iteration 201, loss = 0.0009267589775845408
iteration 202, loss = 0.0011635563569143414
iteration 203, loss = 0.0015657616313546896
iteration 204, loss = 0.0011179535649716854
iteration 205, loss = 0.0018279587384313345
iteration 206, loss = 0.0012544554192572832
iteration 207, loss = 0.0010294491657987237
iteration 208, loss = 0.0008064797730185091
iteration 209, loss = 0.0016494108131155372
iteration 210, loss = 0.0016257696552202106
iteration 211, loss = 0.0008656827849335968
iteration 212, loss = 0.0018552648834884167
iteration 213, loss = 0.0011965290177613497
iteration 214, loss = 0.0016436129808425903
iteration 215, loss = 0.000860581814777106
iteration 216, loss = 0.0007328774663619697
iteration 217, loss = 0.0007542633102275431
iteration 218, loss = 0.0006744503043591976
iteration 219, loss = 0.0007614728529006243
iteration 220, loss = 0.0009095546556636691
iteration 221, loss = 0.0008972157957032323
iteration 222, loss = 0.0010675671510398388
iteration 223, loss = 0.0007812235271558166
iteration 224, loss = 0.0014184881001710892
iteration 225, loss = 0.0007899888441897929
iteration 226, loss = 0.0010256145615130663
iteration 227, loss = 0.0009619358461350203
iteration 228, loss = 0.0009325136779807508
iteration 229, loss = 0.0011414368636906147
iteration 230, loss = 0.0007390048122033477
iteration 231, loss = 0.0007995875203050673
iteration 232, loss = 0.0008128953631967306
iteration 233, loss = 0.0011719583999365568
iteration 234, loss = 0.0009263462852686644
iteration 235, loss = 0.0009129320387728512
iteration 236, loss = 0.0010825805366039276
iteration 237, loss = 0.001400986686348915
iteration 238, loss = 0.0007661337149329484
iteration 239, loss = 0.0008970893104560673
iteration 240, loss = 0.0007926031248643994
iteration 241, loss = 0.0010847768280655146
iteration 242, loss = 0.0011720515321940184
iteration 243, loss = 0.0009304255363531411
iteration 244, loss = 0.0007249381160363555
iteration 245, loss = 0.0008258025045506656
iteration 246, loss = 0.0009232967859134078
iteration 247, loss = 0.000916239048819989
iteration 248, loss = 0.0009146081283688545
iteration 249, loss = 0.001719171879813075
iteration 250, loss = 0.0007060867501422763
iteration 251, loss = 0.0008169444045051932
iteration 252, loss = 0.0012689507566392422
iteration 253, loss = 0.0016415005084127188
iteration 254, loss = 0.0008969723712652922
iteration 255, loss = 0.001231691101565957
iteration 256, loss = 0.0010123297106474638
iteration 257, loss = 0.0012799387332051992
iteration 258, loss = 0.00075414904858917
iteration 259, loss = 0.000951492867898196
iteration 260, loss = 0.001031428575515747
iteration 261, loss = 0.0008260100148618221
iteration 262, loss = 0.0012611036654561758
iteration 263, loss = 0.0015920985024422407
iteration 264, loss = 0.0007889382541179657
iteration 265, loss = 0.0010522379307076335
iteration 266, loss = 0.000942558515816927
iteration 267, loss = 0.0008643788169138134
iteration 268, loss = 0.0008023990085348487
iteration 269, loss = 0.0009430304053239524
iteration 270, loss = 0.0007948841084726155
iteration 271, loss = 0.0008729805704206228
iteration 272, loss = 0.0014739013276994228
iteration 273, loss = 0.0008452977635897696
iteration 274, loss = 0.0010616278741508722
iteration 275, loss = 0.001194250420667231
iteration 276, loss = 0.0009012097143568099
iteration 277, loss = 0.0014720804756507277
iteration 278, loss = 0.0016369529766961932
iteration 279, loss = 0.0009415221284143627
iteration 280, loss = 0.0007002060301601887
iteration 281, loss = 0.001627243822440505
iteration 282, loss = 0.0008472882327623665
iteration 283, loss = 0.0010231154737994075
iteration 284, loss = 0.0008437294163741171
iteration 285, loss = 0.0009457971900701523
iteration 286, loss = 0.0009678119677118957
iteration 287, loss = 0.0009717633947730064
iteration 288, loss = 0.0019970962312072515
iteration 289, loss = 0.0009518105071038008
iteration 290, loss = 0.0010487091494724154
iteration 291, loss = 0.0007294172537513077
iteration 292, loss = 0.0009215024183504283
iteration 293, loss = 0.0010443604551255703
iteration 294, loss = 0.0008307316456921399
iteration 295, loss = 0.0009429927449673414
iteration 296, loss = 0.0007228509639389813
iteration 297, loss = 0.0008478735107928514
iteration 298, loss = 0.00149125000461936
iteration 299, loss = 0.0007687924080528319
iteration 300, loss = 0.0008820738876238465
iteration 1, loss = 0.0007898616604506969
iteration 2, loss = 0.0009996423032134771
iteration 3, loss = 0.0007301202276721597
iteration 4, loss = 0.0008940651314333081
iteration 5, loss = 0.0009631934808567166
iteration 6, loss = 0.000811615725979209
iteration 7, loss = 0.0011589322239160538
iteration 8, loss = 0.0008366637630388141
iteration 9, loss = 0.0007874792208895087
iteration 10, loss = 0.0008056607330217957
iteration 11, loss = 0.001163987908512354
iteration 12, loss = 0.0007336637936532497
iteration 13, loss = 0.0012007206678390503
iteration 14, loss = 0.001101549481973052
iteration 15, loss = 0.0007315679104067385
iteration 16, loss = 0.0007676758687011898
iteration 17, loss = 0.0008664181223139167
iteration 18, loss = 0.0007458326290361583
iteration 19, loss = 0.0007388287922367454
iteration 20, loss = 0.001024044817313552
iteration 21, loss = 0.0008660526946187019
iteration 22, loss = 0.0015661836368963122
iteration 23, loss = 0.0010350299999117851
iteration 24, loss = 0.0016628361772745848
iteration 25, loss = 0.000657286902423948
iteration 26, loss = 0.0007402511546388268
iteration 27, loss = 0.0009500706801190972
iteration 28, loss = 0.0008379003847949207
iteration 29, loss = 0.0007697403780184686
iteration 30, loss = 0.0015607760287821293
iteration 31, loss = 0.0007731919176876545
iteration 32, loss = 0.0008511633495800197
iteration 33, loss = 0.0008589525241404772
iteration 34, loss = 0.0008769238484092057
iteration 35, loss = 0.0010178564116358757
iteration 36, loss = 0.0008969409391283989
iteration 37, loss = 0.0009464699542149901
iteration 38, loss = 0.0008824277319945395
iteration 39, loss = 0.0015609185211360455
iteration 40, loss = 0.0009356078808195889
iteration 41, loss = 0.0008402966777794063
iteration 42, loss = 0.0008138780249282718
iteration 43, loss = 0.0011301026679575443
iteration 44, loss = 0.0010375423589721322
iteration 45, loss = 0.0007730419747531414
iteration 46, loss = 0.0008477959199808538
iteration 47, loss = 0.0007924445671960711
iteration 48, loss = 0.0011890794849023223
iteration 49, loss = 0.0009188816766254604
iteration 50, loss = 0.0017037928337231278
iteration 51, loss = 0.0008265700889751315
iteration 52, loss = 0.0016461388440802693
iteration 53, loss = 0.0009125657379627228
iteration 54, loss = 0.0017396374605596066
iteration 55, loss = 0.0011282805353403091
iteration 56, loss = 0.000928502413444221
iteration 57, loss = 0.0011806777911260724
iteration 58, loss = 0.0010573198087513447
iteration 59, loss = 0.0008147038170136511
iteration 60, loss = 0.0008747034589760005
iteration 61, loss = 0.0017612820956856012
iteration 62, loss = 0.0007056915201246738
iteration 63, loss = 0.0008431354071944952
iteration 64, loss = 0.0010765171609818935
iteration 65, loss = 0.0007666529272682965
iteration 66, loss = 0.0010575718479231
iteration 67, loss = 0.0010851575061678886
iteration 68, loss = 0.001833721762523055
iteration 69, loss = 0.0014513736823573709
iteration 70, loss = 0.0006541405455209315
iteration 71, loss = 0.0009871101938188076
iteration 72, loss = 0.0009118051966652274
iteration 73, loss = 0.0007409282261505723
iteration 74, loss = 0.0008076203521341085
iteration 75, loss = 0.0009761271066963673
iteration 76, loss = 0.0008554689702577889
iteration 77, loss = 0.0008933872450143099
iteration 78, loss = 0.0016670254990458488
iteration 79, loss = 0.001019165967591107
iteration 80, loss = 0.0007359503069892526
iteration 81, loss = 0.0008938551764003932
iteration 82, loss = 0.0008867229917086661
iteration 83, loss = 0.0009795548394322395
iteration 84, loss = 0.0009149187826551497
iteration 85, loss = 0.000832273333799094
iteration 86, loss = 0.0013632497284561396
iteration 87, loss = 0.0007927638944238424
iteration 88, loss = 0.0008200443699024618
iteration 89, loss = 0.001039865193888545
iteration 90, loss = 0.0010707569308578968
iteration 91, loss = 0.0008344812667928636
iteration 92, loss = 0.0019796795677393675
iteration 93, loss = 0.0009336129296571016
iteration 94, loss = 0.0008884593262337148
iteration 95, loss = 0.0009309694869443774
iteration 96, loss = 0.0015989986713975668
iteration 97, loss = 0.0008951066993176937
iteration 98, loss = 0.000961141602601856
iteration 99, loss = 0.0012187620159238577
iteration 100, loss = 0.0016510023269802332
iteration 101, loss = 0.001039036549627781
iteration 102, loss = 0.0015799307730048895
iteration 103, loss = 0.0010672177886590362
iteration 104, loss = 0.000767028599511832
iteration 105, loss = 0.0012214545859023929
iteration 106, loss = 0.0007712473161518574
iteration 107, loss = 0.001112131867557764
iteration 108, loss = 0.0008101112907752395
iteration 109, loss = 0.0008749633561819792
iteration 110, loss = 0.001223467756062746
iteration 111, loss = 0.0010742482263594866
iteration 112, loss = 0.0009601902565918863
iteration 113, loss = 0.001022069831378758
iteration 114, loss = 0.0011792415753006935
iteration 115, loss = 0.0008736318559385836
iteration 116, loss = 0.0008557735127396882
iteration 117, loss = 0.001062740571796894
iteration 118, loss = 0.0008310074335895479
iteration 119, loss = 0.001887924736365676
iteration 120, loss = 0.0009641351643949747
iteration 121, loss = 0.0011257953010499477
iteration 122, loss = 0.0008480873075313866
iteration 123, loss = 0.0015080022858455777
iteration 124, loss = 0.0008198702125810087
iteration 125, loss = 0.000907364534214139
iteration 126, loss = 0.0014815246686339378
iteration 127, loss = 0.0007558027282357216
iteration 128, loss = 0.0007816491415724158
iteration 129, loss = 0.0010310641955584288
iteration 130, loss = 0.0009371911291964352
iteration 131, loss = 0.001206173445098102
iteration 132, loss = 0.0008948237518779933
iteration 133, loss = 0.002028797985985875
iteration 134, loss = 0.001553814741782844
iteration 135, loss = 0.0008656576392240822
iteration 136, loss = 0.001128333038650453
iteration 137, loss = 0.0008163090096786618
iteration 138, loss = 0.0009121432085521519
iteration 139, loss = 0.0010682478314265609
iteration 140, loss = 0.000746033270843327
iteration 141, loss = 0.0007717346888966858
iteration 142, loss = 0.0010339475702494383
iteration 143, loss = 0.001086328411474824
iteration 144, loss = 0.001011657528579235
iteration 145, loss = 0.001035168650560081
iteration 146, loss = 0.001216262229718268
iteration 147, loss = 0.000783147756010294
iteration 148, loss = 0.000864361587446183
iteration 149, loss = 0.0008963339496403933
iteration 150, loss = 0.0008285880903713405
iteration 151, loss = 0.0010345010086894035
iteration 152, loss = 0.0009287433349527419
iteration 153, loss = 0.00170344696380198
iteration 154, loss = 0.0006500280578620732
iteration 155, loss = 0.0008792696171440184
iteration 156, loss = 0.0008901456603780389
iteration 157, loss = 0.0016437252052128315
iteration 158, loss = 0.0013247152091935277
iteration 159, loss = 0.0008248225203715265
iteration 160, loss = 0.0008335824823006988
iteration 161, loss = 0.0008736035088077188
iteration 162, loss = 0.0010324905160814524
iteration 163, loss = 0.0014909803867340088
iteration 164, loss = 0.0014631985686719418
iteration 165, loss = 0.0008222747128456831
iteration 166, loss = 0.0010614496422931552
iteration 167, loss = 0.0008656585123389959
iteration 168, loss = 0.000992116518318653
iteration 169, loss = 0.0008086728630587459
iteration 170, loss = 0.0007523863459937274
iteration 171, loss = 0.0009571099653840065
iteration 172, loss = 0.001127524534240365
iteration 173, loss = 0.0009298347285948694
iteration 174, loss = 0.0008100885897874832
iteration 175, loss = 0.0016899735201150179
iteration 176, loss = 0.0010376337449997663
iteration 177, loss = 0.0008024648996070027
iteration 178, loss = 0.0011065478902310133
iteration 179, loss = 0.0008865704294294119
iteration 180, loss = 0.0008575848769396544
iteration 181, loss = 0.0013691276544705033
iteration 182, loss = 0.0009875292889773846
iteration 183, loss = 0.0010551346931606531
iteration 184, loss = 0.0015733634354546666
iteration 185, loss = 0.000801446964032948
iteration 186, loss = 0.0007296233088709414
iteration 187, loss = 0.0011676943395286798
iteration 188, loss = 0.0015755250351503491
iteration 189, loss = 0.0007897272007539868
iteration 190, loss = 0.0011413346510380507
iteration 191, loss = 0.0009438277338631451
iteration 192, loss = 0.0012114358833059669
iteration 193, loss = 0.0007583167171105742
iteration 194, loss = 0.0008963920408859849
iteration 195, loss = 0.001371521851979196
iteration 196, loss = 0.0007611209875904024
iteration 197, loss = 0.0008705825312063098
iteration 198, loss = 0.0008079371182247996
iteration 199, loss = 0.0007753613172098994
iteration 200, loss = 0.0010365546913817525
iteration 201, loss = 0.0008423913386650383
iteration 202, loss = 0.0009478221763856709
iteration 203, loss = 0.0009416474495083094
iteration 204, loss = 0.0011176327243447304
iteration 205, loss = 0.0009446461917832494
iteration 206, loss = 0.0007112816674634814
iteration 207, loss = 0.0009595814626663923
iteration 208, loss = 0.0008555894601158798
iteration 209, loss = 0.0006072814576327801
iteration 210, loss = 0.0006356284720823169
iteration 211, loss = 0.001122349058277905
iteration 212, loss = 0.0007850681431591511
iteration 213, loss = 0.0008567082113586366
iteration 214, loss = 0.0007987204589881003
iteration 215, loss = 0.0008882893016561866
iteration 216, loss = 0.0009597127791494131
iteration 217, loss = 0.0009195399470627308
iteration 218, loss = 0.0008631017990410328
iteration 219, loss = 0.001221520360559225
iteration 220, loss = 0.0009439594577997923
iteration 221, loss = 0.0012088478542864323
iteration 222, loss = 0.0016157649224624038
iteration 223, loss = 0.0007561355596408248
iteration 224, loss = 0.0010466251987963915
iteration 225, loss = 0.0008855335181578994
iteration 226, loss = 0.0012982406187802553
iteration 227, loss = 0.0011751445708796382
iteration 228, loss = 0.0007378869340755045
iteration 229, loss = 0.00163197738584131
iteration 230, loss = 0.0008083024295046926
iteration 231, loss = 0.0009045366896316409
iteration 232, loss = 0.0007221866399049759
iteration 233, loss = 0.0009587594540789723
iteration 234, loss = 0.00078958785161376
iteration 235, loss = 0.0008340673521161079
iteration 236, loss = 0.0007494523888453841
iteration 237, loss = 0.0008988537592813373
iteration 238, loss = 0.0013413395499810576
iteration 239, loss = 0.001258632866665721
iteration 240, loss = 0.0009007048211060464
iteration 241, loss = 0.0011841023806482553
iteration 242, loss = 0.0011722142808139324
iteration 243, loss = 0.001015564426779747
iteration 244, loss = 0.000884988228790462
iteration 245, loss = 0.001125225448049605
iteration 246, loss = 0.0008678028243593872
iteration 247, loss = 0.0009586144587956369
iteration 248, loss = 0.0008549048216082156
iteration 249, loss = 0.0008441077079623938
iteration 250, loss = 0.0007906664977781475
iteration 251, loss = 0.0012232753215357661
iteration 252, loss = 0.0008044887217693031
iteration 253, loss = 0.0009657912305556238
iteration 254, loss = 0.002045863773673773
iteration 255, loss = 0.0008988590561784804
iteration 256, loss = 0.0011571847135201097
iteration 257, loss = 0.0011143633164465427
iteration 258, loss = 0.0008705759537406266
iteration 259, loss = 0.0009012037189677358
iteration 260, loss = 0.0008318223990499973
iteration 261, loss = 0.0008581223082728684
iteration 262, loss = 0.0007712123333476484
iteration 263, loss = 0.0009739062516018748
iteration 264, loss = 0.0008794711902737617
iteration 265, loss = 0.0008923432324081659
iteration 266, loss = 0.0008741823257878423
iteration 267, loss = 0.001087275450117886
iteration 268, loss = 0.0009502048487775028
iteration 269, loss = 0.0010280236601829529
iteration 270, loss = 0.0008122625877149403
iteration 271, loss = 0.0008404081454500556
iteration 272, loss = 0.0010400695027783513
iteration 273, loss = 0.001607425045222044
iteration 274, loss = 0.0007613086490891874
iteration 275, loss = 0.0011711374390870333
iteration 276, loss = 0.0009324343409389257
iteration 277, loss = 0.0008914029458537698
iteration 278, loss = 0.0008270304533652961
iteration 279, loss = 0.0012487326748669147
iteration 280, loss = 0.000743456999771297
iteration 281, loss = 0.0008128911140374839
iteration 282, loss = 0.0009119071182794869
iteration 283, loss = 0.0012478022836148739
iteration 284, loss = 0.0009422089206054807
iteration 285, loss = 0.0010093932505697012
iteration 286, loss = 0.0007426236406899989
iteration 287, loss = 0.0009885248728096485
iteration 288, loss = 0.0011633062968030572
iteration 289, loss = 0.0009867469780147076
iteration 290, loss = 0.0017370969289913774
iteration 291, loss = 0.0009868323104456067
iteration 292, loss = 0.0007162519614212215
iteration 293, loss = 0.0009013960952870548
iteration 294, loss = 0.0007718551787547767
iteration 295, loss = 0.0023999549448490143
iteration 296, loss = 0.0016000044997781515
iteration 297, loss = 0.0009415663662366569
iteration 298, loss = 0.0008664277265779674
iteration 299, loss = 0.001327767502516508
iteration 300, loss = 0.0010074813617393374
iteration 1, loss = 0.001490212744101882
iteration 2, loss = 0.001055428059771657
iteration 3, loss = 0.0009481603628955781
iteration 4, loss = 0.0008838300127536058
iteration 5, loss = 0.001099462853744626
iteration 6, loss = 0.0015994156710803509
iteration 7, loss = 0.0007849283283576369
iteration 8, loss = 0.001128826872445643
iteration 9, loss = 0.0005966949975118041
iteration 10, loss = 0.0008370170253328979
iteration 11, loss = 0.0008381789666600525
iteration 12, loss = 0.0008175930706784129
iteration 13, loss = 0.0009073837427422404
iteration 14, loss = 0.0011062795529142022
iteration 15, loss = 0.002141971606761217
iteration 16, loss = 0.0007982093375176191
iteration 17, loss = 0.0016221385449171066
iteration 18, loss = 0.0017445614794269204
iteration 19, loss = 0.0017127537867054343
iteration 20, loss = 0.00086519232718274
iteration 21, loss = 0.0009984626667574048
iteration 22, loss = 0.0010126852430403233
iteration 23, loss = 0.0009136963053606451
iteration 24, loss = 0.0010563546093180776
iteration 25, loss = 0.0008049752213992178
iteration 26, loss = 0.0012255461188033223
iteration 27, loss = 0.00070582190528512
iteration 28, loss = 0.000986384809948504
iteration 29, loss = 0.000977681833319366
iteration 30, loss = 0.0008827010169625282
iteration 31, loss = 0.0007695186650380492
iteration 32, loss = 0.0008615087135694921
iteration 33, loss = 0.0018525002524256706
iteration 34, loss = 0.0008229011436924338
iteration 35, loss = 0.0015245616668835282
iteration 36, loss = 0.0008393881726078689
iteration 37, loss = 0.0011075513903051615
iteration 38, loss = 0.0007992096361704171
iteration 39, loss = 0.0010024997172877192
iteration 40, loss = 0.0006456258706748486
iteration 41, loss = 0.000860451313201338
iteration 42, loss = 0.0013123140670359135
iteration 43, loss = 0.0011278355959802866
iteration 44, loss = 0.0007644169381819665
iteration 45, loss = 0.000677165575325489
iteration 46, loss = 0.0015922851162031293
iteration 47, loss = 0.0017211739905178547
iteration 48, loss = 0.0007525506080128253
iteration 49, loss = 0.0008193093235604465
iteration 50, loss = 0.0009839796693995595
iteration 51, loss = 0.0009391552885062993
iteration 52, loss = 0.0013637933880090714
iteration 53, loss = 0.0007333386456593871
iteration 54, loss = 0.0022474026773124933
iteration 55, loss = 0.0010042705107480288
iteration 56, loss = 0.0008140797144733369
iteration 57, loss = 0.0012854442466050386
iteration 58, loss = 0.0009535509161651134
iteration 59, loss = 0.0007963498937897384
iteration 60, loss = 0.0010851643746718764
iteration 61, loss = 0.0008386030094698071
iteration 62, loss = 0.0010306459153071046
iteration 63, loss = 0.0014071036130189896
iteration 64, loss = 0.0007658156100660563
iteration 65, loss = 0.0009802677668631077
iteration 66, loss = 0.00153249385766685
iteration 67, loss = 0.0010514080058783293
iteration 68, loss = 0.0008125032763928175
iteration 69, loss = 0.0007907745894044638
iteration 70, loss = 0.0012036764528602362
iteration 71, loss = 0.0009303234401158988
iteration 72, loss = 0.000817741034552455
iteration 73, loss = 0.001265165745280683
iteration 74, loss = 0.000720630690921098
iteration 75, loss = 0.0008240406168624759
iteration 76, loss = 0.0009950684616342187
iteration 77, loss = 0.0008426518179476261
iteration 78, loss = 0.0010555831249803305
iteration 79, loss = 0.0008916904334910214
iteration 80, loss = 0.0007149639423005283
iteration 81, loss = 0.0011164010502398014
iteration 82, loss = 0.000733245862647891
iteration 83, loss = 0.0010334949474781752
iteration 84, loss = 0.0008388622663915157
iteration 85, loss = 0.0009279872756451368
iteration 86, loss = 0.00086071970872581
iteration 87, loss = 0.000754046137444675
iteration 88, loss = 0.0007519948994740844
iteration 89, loss = 0.0006375372759066522
iteration 90, loss = 0.001025167410261929
iteration 91, loss = 0.0014598685083910823
iteration 92, loss = 0.0007942605298012495
iteration 93, loss = 0.0015580292092636228
iteration 94, loss = 0.0011935924412682652
iteration 95, loss = 0.0008976479293778539
iteration 96, loss = 0.0019339516293257475
iteration 97, loss = 0.0016476933378726244
iteration 98, loss = 0.0008433501934632659
iteration 99, loss = 0.0008322887006215751
iteration 100, loss = 0.0006918127182871103
iteration 101, loss = 0.0008502643904648721
iteration 102, loss = 0.000801058835349977
iteration 103, loss = 0.0007476142491213977
iteration 104, loss = 0.0008043077541515231
iteration 105, loss = 0.0009415709646418691
iteration 106, loss = 0.0010048139374703169
iteration 107, loss = 0.0007136712083593011
iteration 108, loss = 0.0013289536582306027
iteration 109, loss = 0.0007544367108494043
iteration 110, loss = 0.0009770567994564772
iteration 111, loss = 0.0008797821355983615
iteration 112, loss = 0.0009670614963397384
iteration 113, loss = 0.0009560848702676594
iteration 114, loss = 0.0010707995388656855
iteration 115, loss = 0.0008164558093994856
iteration 116, loss = 0.0011167310876771808
iteration 117, loss = 0.0016859975876286626
iteration 118, loss = 0.001493746996857226
iteration 119, loss = 0.000935516320168972
iteration 120, loss = 0.000890123366843909
iteration 121, loss = 0.0013187070144340396
iteration 122, loss = 0.0011024733539670706
iteration 123, loss = 0.0019137385534122586
iteration 124, loss = 0.0008567607728764415
iteration 125, loss = 0.0009988959645852447
iteration 126, loss = 0.0007881986675783992
iteration 127, loss = 0.0009297641227021813
iteration 128, loss = 0.0009147422388195992
iteration 129, loss = 0.001588856102898717
iteration 130, loss = 0.0012551728868857026
iteration 131, loss = 0.0016102902591228485
iteration 132, loss = 0.0015583343338221312
iteration 133, loss = 0.001003679004497826
iteration 134, loss = 0.0010597421787679195
iteration 135, loss = 0.0009954081615433097
iteration 136, loss = 0.001010020961984992
iteration 137, loss = 0.0008416965138167143
iteration 138, loss = 0.0008038079831749201
iteration 139, loss = 0.0015762516995891929
iteration 140, loss = 0.0009195274324156344
iteration 141, loss = 0.0008571894140914083
iteration 142, loss = 0.0007779384613968432
iteration 143, loss = 0.0007860824116505682
iteration 144, loss = 0.0008236087742261589
iteration 145, loss = 0.0008695291471667588
iteration 146, loss = 0.0014846434351056814
iteration 147, loss = 0.0010219847317785025
iteration 148, loss = 0.0008379389182664454
iteration 149, loss = 0.0009468623320572078
iteration 150, loss = 0.0008196071721613407
iteration 151, loss = 0.001125885872170329
iteration 152, loss = 0.00085258245235309
iteration 153, loss = 0.0010519517818465829
iteration 154, loss = 0.0008061706903390586
iteration 155, loss = 0.0008825786062516272
iteration 156, loss = 0.0008646786445751786
iteration 157, loss = 0.0013074305607005954
iteration 158, loss = 0.0008979251724667847
iteration 159, loss = 0.0009535615099593997
iteration 160, loss = 0.0006568364333361387
iteration 161, loss = 0.0008236424764618278
iteration 162, loss = 0.0008245130302384496
iteration 163, loss = 0.0009857162367552519
iteration 164, loss = 0.0010487708495929837
iteration 165, loss = 0.0009166997624561191
iteration 166, loss = 0.0008085686713457108
iteration 167, loss = 0.001153474091552198
iteration 168, loss = 0.0008814636385068297
iteration 169, loss = 0.0007894104928709567
iteration 170, loss = 0.0009048182400874794
iteration 171, loss = 0.0024853639770299196
iteration 172, loss = 0.0008782831719145179
iteration 173, loss = 0.0015501450980082154
iteration 174, loss = 0.0011547086760401726
iteration 175, loss = 0.0008882859256118536
iteration 176, loss = 0.0010104066459462047
iteration 177, loss = 0.0009696827037259936
iteration 178, loss = 0.0009009221103042364
iteration 179, loss = 0.0008708275854587555
iteration 180, loss = 0.0008708720561116934
iteration 181, loss = 0.0009356257505714893
iteration 182, loss = 0.0017162206349894404
iteration 183, loss = 0.0008843332761898637
iteration 184, loss = 0.00162605126388371
iteration 185, loss = 0.0007429155521094799
iteration 186, loss = 0.0008781820652075112
iteration 187, loss = 0.0011167438933625817
iteration 188, loss = 0.0011973801301792264
iteration 189, loss = 0.000854513724334538
iteration 190, loss = 0.0011983878212049603
iteration 191, loss = 0.0009287744178436697
iteration 192, loss = 0.0008290763944387436
iteration 193, loss = 0.000884413777384907
iteration 194, loss = 0.00120597705245018
iteration 195, loss = 0.0006927225040271878
iteration 196, loss = 0.0009931448148563504
iteration 197, loss = 0.0009097008151002228
iteration 198, loss = 0.0008622654713690281
iteration 199, loss = 0.000996597926132381
iteration 200, loss = 0.0007852129638195038
iteration 201, loss = 0.0006468734936788678
iteration 202, loss = 0.0008262970368377864
iteration 203, loss = 0.0010842857882380486
iteration 204, loss = 0.0008673508418723941
iteration 205, loss = 0.0008255575085058808
iteration 206, loss = 0.0009366378653794527
iteration 207, loss = 0.0014111092314124107
iteration 208, loss = 0.0007812001858837903
iteration 209, loss = 0.0010101801017299294
iteration 210, loss = 0.000946717569604516
iteration 211, loss = 0.00191536545753479
iteration 212, loss = 0.001248219981789589
iteration 213, loss = 0.0007449870463460684
iteration 214, loss = 0.0007222531130537391
iteration 215, loss = 0.0008548643090762198
iteration 216, loss = 0.000859025283716619
iteration 217, loss = 0.0006795531953684986
iteration 218, loss = 0.0008248793892562389
iteration 219, loss = 0.0008442937396466732
iteration 220, loss = 0.0007662763819098473
iteration 221, loss = 0.0006602304056286812
iteration 222, loss = 0.0014049565652385354
iteration 223, loss = 0.0009200815111398697
iteration 224, loss = 0.0011732204584404826
iteration 225, loss = 0.0009615099406801164
iteration 226, loss = 0.0009257920901291072
iteration 227, loss = 0.0010896966559812427
iteration 228, loss = 0.001129525131545961
iteration 229, loss = 0.0010750205256044865
iteration 230, loss = 0.0007524779066443443
iteration 231, loss = 0.0007923169177956879
iteration 232, loss = 0.0008208529325202107
iteration 233, loss = 0.0008646484347991645
iteration 234, loss = 0.0007591526955366135
iteration 235, loss = 0.0009748353622853756
iteration 236, loss = 0.0009100615279749036
iteration 237, loss = 0.0007854559808038175
iteration 238, loss = 0.0012805096339434385
iteration 239, loss = 0.0008726067026145756
iteration 240, loss = 0.0008727188687771559
iteration 241, loss = 0.0007525025284849107
iteration 242, loss = 0.0007616731454618275
iteration 243, loss = 0.0012773729395121336
iteration 244, loss = 0.0012113075936213136
iteration 245, loss = 0.0017619827995076776
iteration 246, loss = 0.0008532613865099847
iteration 247, loss = 0.001394794904626906
iteration 248, loss = 0.0009672172018326819
iteration 249, loss = 0.0007960697985254228
iteration 250, loss = 0.0006050858646631241
iteration 251, loss = 0.0012268265709280968
iteration 252, loss = 0.0008308551623485982
iteration 253, loss = 0.0016862597549334168
iteration 254, loss = 0.0008157899137586355
iteration 255, loss = 0.0009251459850929677
iteration 256, loss = 0.0008019173983484507
iteration 257, loss = 0.0007274163071997464
iteration 258, loss = 0.0008846368291415274
iteration 259, loss = 0.0011700986651703715
iteration 260, loss = 0.0008725023944862187
iteration 261, loss = 0.0009460806613788009
iteration 262, loss = 0.0011828206479549408
iteration 263, loss = 0.0008257633890025318
iteration 264, loss = 0.0012007763143628836
iteration 265, loss = 0.0009865673491731286
iteration 266, loss = 0.0009366861777380109
iteration 267, loss = 0.0011277133598923683
iteration 268, loss = 0.0011102971620857716
iteration 269, loss = 0.0008494025096297264
iteration 270, loss = 0.0009749248274601996
iteration 271, loss = 0.0009781660046428442
iteration 272, loss = 0.0014636849518865347
iteration 273, loss = 0.0008543585427105427
iteration 274, loss = 0.0008624629699625075
iteration 275, loss = 0.0009634427260607481
iteration 276, loss = 0.0007342793978750706
iteration 277, loss = 0.0008916368242353201
iteration 278, loss = 0.0007473768200725317
iteration 279, loss = 0.0008455234346911311
iteration 280, loss = 0.0009030824294313788
iteration 281, loss = 0.0014602578012272716
iteration 282, loss = 0.0010969502618536353
iteration 283, loss = 0.0009359532268717885
iteration 284, loss = 0.0009882798185572028
iteration 285, loss = 0.0008871554746292531
iteration 286, loss = 0.0009332501795142889
iteration 287, loss = 0.0007895280723460019
iteration 288, loss = 0.0006595604354515672
iteration 289, loss = 0.001694207196123898
iteration 290, loss = 0.0010191438486799598
iteration 291, loss = 0.0008713830611668527
iteration 292, loss = 0.000720752403140068
iteration 293, loss = 0.0009743355331011117
iteration 294, loss = 0.0008316424209624529
iteration 295, loss = 0.0009459438733756542
iteration 296, loss = 0.0011227648938074708
iteration 297, loss = 0.0007803102489560843
iteration 298, loss = 0.0009658574126660824
iteration 299, loss = 0.0007861070334911346
iteration 300, loss = 0.0008882558322511613
iteration 1, loss = 0.0008928407914936543
iteration 2, loss = 0.0008599386201240122
iteration 3, loss = 0.0008772284490987659
iteration 4, loss = 0.0009693473111838102
iteration 5, loss = 0.0009250402799807489
iteration 6, loss = 0.0008482239209115505
iteration 7, loss = 0.0009169178083539009
iteration 8, loss = 0.0008330604759976268
iteration 9, loss = 0.0014138920232653618
iteration 10, loss = 0.0009949203813448548
iteration 11, loss = 0.000948647502809763
iteration 12, loss = 0.0009223106317222118
iteration 13, loss = 0.0008616560371592641
iteration 14, loss = 0.0007940573850646615
iteration 15, loss = 0.0012435815297067165
iteration 16, loss = 0.0009109105449169874
iteration 17, loss = 0.0008642658940516412
iteration 18, loss = 0.0010183271951973438
iteration 19, loss = 0.0009275806369259953
iteration 20, loss = 0.0009736429201439023
iteration 21, loss = 0.001016272697597742
iteration 22, loss = 0.0011332533322274685
iteration 23, loss = 0.0008183243917301297
iteration 24, loss = 0.001067464123480022
iteration 25, loss = 0.0017616337863728404
iteration 26, loss = 0.0007680180715397
iteration 27, loss = 0.0007994667394086719
iteration 28, loss = 0.0008033789927139878
iteration 29, loss = 0.0017455938505008817
iteration 30, loss = 0.0006260580266825855
iteration 31, loss = 0.000901936087757349
iteration 32, loss = 0.0009683803073130548
iteration 33, loss = 0.0009788164170458913
iteration 34, loss = 0.0006832238286733627
iteration 35, loss = 0.0009340874385088682
iteration 36, loss = 0.0010001205373555422
iteration 37, loss = 0.002161058597266674
iteration 38, loss = 0.0010500263888388872
iteration 39, loss = 0.0010255883680656552
iteration 40, loss = 0.0009473301470279694
iteration 41, loss = 0.0010319097200408578
iteration 42, loss = 0.0008733446011319757
iteration 43, loss = 0.0011781863868236542
iteration 44, loss = 0.0011522223940119147
iteration 45, loss = 0.0015840231208130717
iteration 46, loss = 0.000801638001576066
iteration 47, loss = 0.0008317689062096179
iteration 48, loss = 0.0009395598899573088
iteration 49, loss = 0.0007441896013915539
iteration 50, loss = 0.0007769321673549712
iteration 51, loss = 0.0015100586460903287
iteration 52, loss = 0.0007894208538345993
iteration 53, loss = 0.000746103294659406
iteration 54, loss = 0.0014967378228902817
iteration 55, loss = 0.0008340238709934056
iteration 56, loss = 0.0007397160516120493
iteration 57, loss = 0.0008276212029159069
iteration 58, loss = 0.0008631512173451483
iteration 59, loss = 0.0008474112837575376
iteration 60, loss = 0.000972231850028038
iteration 61, loss = 0.0011255479184910655
iteration 62, loss = 0.0007096239132806659
iteration 63, loss = 0.0008286246447823942
iteration 64, loss = 0.0010827664518728852
iteration 65, loss = 0.001210397225804627
iteration 66, loss = 0.0008586156764067709
iteration 67, loss = 0.0007058220217004418
iteration 68, loss = 0.0007077840273268521
iteration 69, loss = 0.0008859236259013414
iteration 70, loss = 0.0009922097669914365
iteration 71, loss = 0.0008072715136222541
iteration 72, loss = 0.000809706631116569
iteration 73, loss = 0.0008187943603843451
iteration 74, loss = 0.0010560392402112484
iteration 75, loss = 0.0007503266097046435
iteration 76, loss = 0.002371852518990636
iteration 77, loss = 0.0015574508579447865
iteration 78, loss = 0.0007347407517954707
iteration 79, loss = 0.001529027591459453
iteration 80, loss = 0.0010500632924959064
iteration 81, loss = 0.000811340578366071
iteration 82, loss = 0.0008656000136397779
iteration 83, loss = 0.0008875299827195704
iteration 84, loss = 0.0009714867337606847
iteration 85, loss = 0.0008448332082480192
iteration 86, loss = 0.000890524941496551
iteration 87, loss = 0.0007458067266270518
iteration 88, loss = 0.0008484775898978114
iteration 89, loss = 0.0010814559645950794
iteration 90, loss = 0.0012194873997941613
iteration 91, loss = 0.0012588542886078358
iteration 92, loss = 0.001481402781791985
iteration 93, loss = 0.0012799062533304095
iteration 94, loss = 0.0007704866584390402
iteration 95, loss = 0.0008247292716987431
iteration 96, loss = 0.0010832676198333502
iteration 97, loss = 0.0009166951640509069
iteration 98, loss = 0.0007180255488492548
iteration 99, loss = 0.0007870162953622639
iteration 100, loss = 0.0007761329179629683
iteration 101, loss = 0.0008812900050543249
iteration 102, loss = 0.0008468545856885612
iteration 103, loss = 0.0010202716803178191
iteration 104, loss = 0.0009714696789160371
iteration 105, loss = 0.0008062105625867844
iteration 106, loss = 0.0007436212617903948
iteration 107, loss = 0.0008234965498559177
iteration 108, loss = 0.0010002149501815438
iteration 109, loss = 0.001332937623374164
iteration 110, loss = 0.0010330963414162397
iteration 111, loss = 0.0009187620598822832
iteration 112, loss = 0.0009394881199114025
iteration 113, loss = 0.001128023606725037
iteration 114, loss = 0.0007655328954569995
iteration 115, loss = 0.0009033717215061188
iteration 116, loss = 0.0008383225067518651
iteration 117, loss = 0.0014397467020899057
iteration 118, loss = 0.0009591744164936244
iteration 119, loss = 0.0017817281186580658
iteration 120, loss = 0.0008697367738932371
iteration 121, loss = 0.0017807604745030403
iteration 122, loss = 0.0017712523695081472
iteration 123, loss = 0.0008928983006626368
iteration 124, loss = 0.0013238706160336733
iteration 125, loss = 0.000794088700786233
iteration 126, loss = 0.001015698304399848
iteration 127, loss = 0.0007271174690686166
iteration 128, loss = 0.0008133859955705702
iteration 129, loss = 0.0007988275610841811
iteration 130, loss = 0.0008270767284557223
iteration 131, loss = 0.0007753689424134791
iteration 132, loss = 0.0017734267748892307
iteration 133, loss = 0.0010570829035714269
iteration 134, loss = 0.0013328506611287594
iteration 135, loss = 0.0009702378883957863
iteration 136, loss = 0.0011220830492675304
iteration 137, loss = 0.0007171350880526006
iteration 138, loss = 0.0009506187634542584
iteration 139, loss = 0.000871343829203397
iteration 140, loss = 0.0008495171787217259
iteration 141, loss = 0.0011874237097799778
iteration 142, loss = 0.000855257676448673
iteration 143, loss = 0.0008654215489514172
iteration 144, loss = 0.0007960672373883426
iteration 145, loss = 0.0009162580245174468
iteration 146, loss = 0.0008016114006750286
iteration 147, loss = 0.0013475159648805857
iteration 148, loss = 0.0012805043952539563
iteration 149, loss = 0.0016201972030103207
iteration 150, loss = 0.0007651308551430702
iteration 151, loss = 0.0007133184699341655
iteration 152, loss = 0.0009408125188201666
iteration 153, loss = 0.0009030987275764346
iteration 154, loss = 0.0008412921451963484
iteration 155, loss = 0.0008756508468650281
iteration 156, loss = 0.0009389400947839022
iteration 157, loss = 0.0017292452976107597
iteration 158, loss = 0.0008468097075819969
iteration 159, loss = 0.0009368700557388365
iteration 160, loss = 0.0008823583484627306
iteration 161, loss = 0.0010283326264470816
iteration 162, loss = 0.0007978934445418417
iteration 163, loss = 0.0008240757160820067
iteration 164, loss = 0.0008821548544801772
iteration 165, loss = 0.001093949656933546
iteration 166, loss = 0.000797620159573853
iteration 167, loss = 0.002137593924999237
iteration 168, loss = 0.001476377248764038
iteration 169, loss = 0.001098024658858776
iteration 170, loss = 0.0010388154769316316
iteration 171, loss = 0.0006159021868370473
iteration 172, loss = 0.0007446989184245467
iteration 173, loss = 0.0008330130949616432
iteration 174, loss = 0.0008304242510348558
iteration 175, loss = 0.0009219974745064974
iteration 176, loss = 0.0015246563125401735
iteration 177, loss = 0.0007628159364685416
iteration 178, loss = 0.0015001334249973297
iteration 179, loss = 0.0008431134046986699
iteration 180, loss = 0.0012947760988026857
iteration 181, loss = 0.0010413702111691236
iteration 182, loss = 0.0024416670203208923
iteration 183, loss = 0.0008839552174322307
iteration 184, loss = 0.0007548543508164585
iteration 185, loss = 0.000697408861014992
iteration 186, loss = 0.0011917684460058808
iteration 187, loss = 0.00075686996569857
iteration 188, loss = 0.0007886195671744645
iteration 189, loss = 0.0011546234600245953
iteration 190, loss = 0.0008406796841882169
iteration 191, loss = 0.00121269840747118
iteration 192, loss = 0.0009408614132553339
iteration 193, loss = 0.0008260562317445874
iteration 194, loss = 0.000782532908488065
iteration 195, loss = 0.0011380749056115746
iteration 196, loss = 0.0009912399109452963
iteration 197, loss = 0.001228928565979004
iteration 198, loss = 0.0009493345860391855
iteration 199, loss = 0.0008278219611383975
iteration 200, loss = 0.0007154527702368796
iteration 201, loss = 0.0007577695068903267
iteration 202, loss = 0.0007823614869266748
iteration 203, loss = 0.002160149160772562
iteration 204, loss = 0.0007521695224568248
iteration 205, loss = 0.0008531147614121437
iteration 206, loss = 0.0010995022021234035
iteration 207, loss = 0.000900965416803956
iteration 208, loss = 0.0007137357606552541
iteration 209, loss = 0.0009966283105313778
iteration 210, loss = 0.00108904589433223
iteration 211, loss = 0.000804584939032793
iteration 212, loss = 0.0009412495419383049
iteration 213, loss = 0.0009981158655136824
iteration 214, loss = 0.0007959729409776628
iteration 215, loss = 0.0008680267492309213
iteration 216, loss = 0.0007874263683333993
iteration 217, loss = 0.0007237197714857757
iteration 218, loss = 0.0009485876653343439
iteration 219, loss = 0.0010384079068899155
iteration 220, loss = 0.001039767637848854
iteration 221, loss = 0.0011195249389857054
iteration 222, loss = 0.0007902393699623644
iteration 223, loss = 0.0009763378766365349
iteration 224, loss = 0.0006871927762404084
iteration 225, loss = 0.0007037324830889702
iteration 226, loss = 0.0006743285339325666
iteration 227, loss = 0.0011562774889171124
iteration 228, loss = 0.0008986543398350477
iteration 229, loss = 0.0015498176217079163
iteration 230, loss = 0.000806231691967696
iteration 231, loss = 0.0007932217558845878
iteration 232, loss = 0.000784175586886704
iteration 233, loss = 0.0010960603831335902
iteration 234, loss = 0.0008618744905106723
iteration 235, loss = 0.0008693057461641729
iteration 236, loss = 0.0009040632285177708
iteration 237, loss = 0.0012921758461743593
iteration 238, loss = 0.0010503188241273165
iteration 239, loss = 0.0008153470698744059
iteration 240, loss = 0.0008527052123099566
iteration 241, loss = 0.0007772779790684581
iteration 242, loss = 0.0008909591706469655
iteration 243, loss = 0.0018044314347207546
iteration 244, loss = 0.0008159287390299141
iteration 245, loss = 0.0006600844790227711
iteration 246, loss = 0.0012095883721485734
iteration 247, loss = 0.0007119845249690115
iteration 248, loss = 0.0008560559945181012
iteration 249, loss = 0.001007427228614688
iteration 250, loss = 0.0009042005985975266
iteration 251, loss = 0.00111390370875597
iteration 252, loss = 0.0017696189461275935
iteration 253, loss = 0.0010643030982464552
iteration 254, loss = 0.0006159106269478798
iteration 255, loss = 0.0013695773668587208
iteration 256, loss = 0.0025740053970366716
iteration 257, loss = 0.0008197526331059635
iteration 258, loss = 0.0014578914269804955
iteration 259, loss = 0.0008493231143802404
iteration 260, loss = 0.0012096741702407598
iteration 261, loss = 0.000793685088865459
iteration 262, loss = 0.0008639652514830232
iteration 263, loss = 0.000903544423636049
iteration 264, loss = 0.0008364813984371722
iteration 265, loss = 0.0009285743581131101
iteration 266, loss = 0.0012651894940063357
iteration 267, loss = 0.0011338451877236366
iteration 268, loss = 0.0008277917513623834
iteration 269, loss = 0.0007486857939511538
iteration 270, loss = 0.0010114359902217984
iteration 271, loss = 0.0007712882361374795
iteration 272, loss = 0.0009228494600392878
iteration 273, loss = 0.0007404802599921823
iteration 274, loss = 0.0013148569269105792
iteration 275, loss = 0.0015305514680221677
iteration 276, loss = 0.0011465440038591623
iteration 277, loss = 0.0011759399203583598
iteration 278, loss = 0.0010340620065107942
iteration 279, loss = 0.001056215143762529
iteration 280, loss = 0.0010661949636414647
iteration 281, loss = 0.0013368779327720404
iteration 282, loss = 0.0009058524738065898
iteration 283, loss = 0.0007897306350059807
iteration 284, loss = 0.0009930682135745883
iteration 285, loss = 0.0008822637028060853
iteration 286, loss = 0.0007311406079679728
iteration 287, loss = 0.0010317217092961073
iteration 288, loss = 0.0008695889264345169
iteration 289, loss = 0.0009547319496050477
iteration 290, loss = 0.0014955441001802683
iteration 291, loss = 0.0008447354775853455
iteration 292, loss = 0.0018138266168534756
iteration 293, loss = 0.0008260289905592799
iteration 294, loss = 0.0008439381490461528
iteration 295, loss = 0.0010206771548837423
iteration 296, loss = 0.0010153211187571287
iteration 297, loss = 0.0009022451704367995
iteration 298, loss = 0.0008533214568160474
iteration 299, loss = 0.0008081365376710892
iteration 300, loss = 0.0010962017113342881
iteration 1, loss = 0.0008249740931205451
iteration 2, loss = 0.0011363915400579572
iteration 3, loss = 0.000664385559502989
iteration 4, loss = 0.0008897036896087229
iteration 5, loss = 0.0011727019445970654
iteration 6, loss = 0.0008781122742220759
iteration 7, loss = 0.0011003224644809961
iteration 8, loss = 0.0007499649072997272
iteration 9, loss = 0.001282437820918858
iteration 10, loss = 0.0006251538288779557
iteration 11, loss = 0.0008920467225834727
iteration 12, loss = 0.0008267343509942293
iteration 13, loss = 0.0012652272125706077
iteration 14, loss = 0.0008548561600036919
iteration 15, loss = 0.0012346513103693724
iteration 16, loss = 0.00102056167088449
iteration 17, loss = 0.0008820823859423399
iteration 18, loss = 0.0010849650716409087
iteration 19, loss = 0.0008712774724699557
iteration 20, loss = 0.0008777189068496227
iteration 21, loss = 0.0013438186142593622
iteration 22, loss = 0.0011751566780731082
iteration 23, loss = 0.0015943965408951044
iteration 24, loss = 0.0007698730914853513
iteration 25, loss = 0.0007731746882200241
iteration 26, loss = 0.0010561770759522915
iteration 27, loss = 0.0008477949886582792
iteration 28, loss = 0.0009438468841835856
iteration 29, loss = 0.0009253342868760228
iteration 30, loss = 0.0007590253953821957
iteration 31, loss = 0.0008764289086684585
iteration 32, loss = 0.0006582298083230853
iteration 33, loss = 0.0010182444239035249
iteration 34, loss = 0.0009431443177163601
iteration 35, loss = 0.001507050241343677
iteration 36, loss = 0.00102341384626925
iteration 37, loss = 0.0008688520756550133
iteration 38, loss = 0.0016308857593685389
iteration 39, loss = 0.0013120245421305299
iteration 40, loss = 0.001189216854982078
iteration 41, loss = 0.0007439567707479
iteration 42, loss = 0.000887730740942061
iteration 43, loss = 0.0018870525527745485
iteration 44, loss = 0.0007277812110260129
iteration 45, loss = 0.0012451111106202006
iteration 46, loss = 0.001732183387503028
iteration 47, loss = 0.0008509613107889891
iteration 48, loss = 0.0009647385450080037
iteration 49, loss = 0.0007914697634987533
iteration 50, loss = 0.0013140785740688443
iteration 51, loss = 0.001062877243384719
iteration 52, loss = 0.0007650409825146198
iteration 53, loss = 0.001036318950355053
iteration 54, loss = 0.0008239763556048274
iteration 55, loss = 0.0008052276680245996
iteration 56, loss = 0.0007691264036111534
iteration 57, loss = 0.000807067146524787
iteration 58, loss = 0.000836852181237191
iteration 59, loss = 0.0012918460415676236
iteration 60, loss = 0.001105456962250173
iteration 61, loss = 0.0006887492490932345
iteration 62, loss = 0.0010045001981779933
iteration 63, loss = 0.0009910267544910312
iteration 64, loss = 0.0014360296772792935
iteration 65, loss = 0.0010399428429082036
iteration 66, loss = 0.0010604554554447532
iteration 67, loss = 0.0008757998002693057
iteration 68, loss = 0.0007338485447689891
iteration 69, loss = 0.0009537481237202883
iteration 70, loss = 0.0011577160330489278
iteration 71, loss = 0.001199117279611528
iteration 72, loss = 0.000741638767067343
iteration 73, loss = 0.0008783055818639696
iteration 74, loss = 0.0011378098279237747
iteration 75, loss = 0.0007724663009867072
iteration 76, loss = 0.0009257234632968903
iteration 77, loss = 0.0010299963178113103
iteration 78, loss = 0.0009013927774503827
iteration 79, loss = 0.0016186005668714643
iteration 80, loss = 0.0008842715178616345
iteration 81, loss = 0.0008308805990964174
iteration 82, loss = 0.0015948843210935593
iteration 83, loss = 0.0016079987399280071
iteration 84, loss = 0.0007919088238850236
iteration 85, loss = 0.0012015689862892032
iteration 86, loss = 0.000859768595546484
iteration 87, loss = 0.0007540964870713651
iteration 88, loss = 0.0006467066705226898
iteration 89, loss = 0.001201513223350048
iteration 90, loss = 0.0009271621238440275
iteration 91, loss = 0.0015209891134873033
iteration 92, loss = 0.0007526411209255457
iteration 93, loss = 0.0007537255878560245
iteration 94, loss = 0.0011221806053072214
iteration 95, loss = 0.0008435487397946417
iteration 96, loss = 0.0008902106783352792
iteration 97, loss = 0.0009759223903529346
iteration 98, loss = 0.000747881131246686
iteration 99, loss = 0.0008858045912347734
iteration 100, loss = 0.0009623577352613211
iteration 101, loss = 0.0011688127415254712
iteration 102, loss = 0.001602055854164064
iteration 103, loss = 0.0016995705664157867
iteration 104, loss = 0.0010187398875132203
iteration 105, loss = 0.0012562682386487722
iteration 106, loss = 0.0008257050067186356
iteration 107, loss = 0.0012862160801887512
iteration 108, loss = 0.0007174455095082521
iteration 109, loss = 0.000942917016800493
iteration 110, loss = 0.0009473167010582983
iteration 111, loss = 0.0008218091097660363
iteration 112, loss = 0.0009128500823862851
iteration 113, loss = 0.000737625639885664
iteration 114, loss = 0.0007483987719751894
iteration 115, loss = 0.0014530059415847063
iteration 116, loss = 0.0009337021037936211
iteration 117, loss = 0.0008740422781556845
iteration 118, loss = 0.0009181205532513559
iteration 119, loss = 0.0011652493849396706
iteration 120, loss = 0.0008054154459387064
iteration 121, loss = 0.0017584576271474361
iteration 122, loss = 0.0011271676048636436
iteration 123, loss = 0.0009600393823347986
iteration 124, loss = 0.0006670351722277701
iteration 125, loss = 0.0008459870587103069
iteration 126, loss = 0.0009041068842634559
iteration 127, loss = 0.0007662891293875873
iteration 128, loss = 0.0009876483818516135
iteration 129, loss = 0.0010497735347598791
iteration 130, loss = 0.0007595066563226283
iteration 131, loss = 0.0007173930061981082
iteration 132, loss = 0.000986228697001934
iteration 133, loss = 0.0009501799941062927
iteration 134, loss = 0.0006688961293548346
iteration 135, loss = 0.0008073914796113968
iteration 136, loss = 0.0010241661220788956
iteration 137, loss = 0.000934688956476748
iteration 138, loss = 0.0010971209267154336
iteration 139, loss = 0.001156212412752211
iteration 140, loss = 0.0007648032624274492
iteration 141, loss = 0.0008742196951061487
iteration 142, loss = 0.0011134000960737467
iteration 143, loss = 0.0011715905275195837
iteration 144, loss = 0.0008339044870808721
iteration 145, loss = 0.001385472365655005
iteration 146, loss = 0.0007695187232457101
iteration 147, loss = 0.0007932711741887033
iteration 148, loss = 0.0008567980257794261
iteration 149, loss = 0.000688242434989661
iteration 150, loss = 0.0008240888128057122
iteration 151, loss = 0.0008855791529640555
iteration 152, loss = 0.0008986162720248103
iteration 153, loss = 0.0008459014934487641
iteration 154, loss = 0.0008688296074979007
iteration 155, loss = 0.0009076293208636343
iteration 156, loss = 0.001115973456762731
iteration 157, loss = 0.0012609914410859346
iteration 158, loss = 0.0009754019556567073
iteration 159, loss = 0.0008388119749724865
iteration 160, loss = 0.0008619496366009116
iteration 161, loss = 0.0009278806974180043
iteration 162, loss = 0.0009091781685128808
iteration 163, loss = 0.0009902676101773977
iteration 164, loss = 0.0008000810048542917
iteration 165, loss = 0.0008157831034623086
iteration 166, loss = 0.000734875735361129
iteration 167, loss = 0.000882790656760335
iteration 168, loss = 0.0008240319439209998
iteration 169, loss = 0.0008134620729833841
iteration 170, loss = 0.000978504540398717
iteration 171, loss = 0.0015543671324849129
iteration 172, loss = 0.0007726752082817256
iteration 173, loss = 0.0013320811558514833
iteration 174, loss = 0.0007459689513780177
iteration 175, loss = 0.0009520037565380335
iteration 176, loss = 0.0007754427497275174
iteration 177, loss = 0.0008838230278342962
iteration 178, loss = 0.0011740591144189239
iteration 179, loss = 0.001833934336900711
iteration 180, loss = 0.000995344016700983
iteration 181, loss = 0.001069145742803812
iteration 182, loss = 0.0008867674041539431
iteration 183, loss = 0.0007065086392685771
iteration 184, loss = 0.0006947353831492364
iteration 185, loss = 0.0008170383516699076
iteration 186, loss = 0.0008459369419142604
iteration 187, loss = 0.0008549895719625056
iteration 188, loss = 0.0009467951604165137
iteration 189, loss = 0.0007669781916774809
iteration 190, loss = 0.0010579866357147694
iteration 191, loss = 0.0010700009297579527
iteration 192, loss = 0.0007482666987925768
iteration 193, loss = 0.001447102753445506
iteration 194, loss = 0.0009711933671496809
iteration 195, loss = 0.0019668182358145714
iteration 196, loss = 0.0009222477092407644
iteration 197, loss = 0.0011953320354223251
iteration 198, loss = 0.0007465360686182976
iteration 199, loss = 0.0010699080303311348
iteration 200, loss = 0.0006343463901430368
iteration 201, loss = 0.0007358939037658274
iteration 202, loss = 0.0007626027800142765
iteration 203, loss = 0.0008588114287704229
iteration 204, loss = 0.0016809554072096944
iteration 205, loss = 0.001730051706545055
iteration 206, loss = 0.0010403243359178305
iteration 207, loss = 0.0010831677354872227
iteration 208, loss = 0.0015422863652929664
iteration 209, loss = 0.0007006999803707004
iteration 210, loss = 0.001127696130424738
iteration 211, loss = 0.0009100689203478396
iteration 212, loss = 0.0007565089617855847
iteration 213, loss = 0.0008862557006068528
iteration 214, loss = 0.0017584232846274972
iteration 215, loss = 0.0010291801299899817
iteration 216, loss = 0.0007009364780969918
iteration 217, loss = 0.0008952722419053316
iteration 218, loss = 0.0009572028648108244
iteration 219, loss = 0.0012192016001790762
iteration 220, loss = 0.0009890520013868809
iteration 221, loss = 0.000932095164898783
iteration 222, loss = 0.0011835414916276932
iteration 223, loss = 0.0007955075707286596
iteration 224, loss = 0.0009475427796132863
iteration 225, loss = 0.001459064893424511
iteration 226, loss = 0.0008187300991266966
iteration 227, loss = 0.0009136417647823691
iteration 228, loss = 0.000892111798748374
iteration 229, loss = 0.0008960353443399072
iteration 230, loss = 0.0009243381209671497
iteration 231, loss = 0.0019198479130864143
iteration 232, loss = 0.0007820036844350398
iteration 233, loss = 0.0007626659935340285
iteration 234, loss = 0.0009684270480647683
iteration 235, loss = 0.0016124136745929718
iteration 236, loss = 0.0007719880086369812
iteration 237, loss = 0.0011633681133389473
iteration 238, loss = 0.0008263926138170063
iteration 239, loss = 0.0016158847138285637
iteration 240, loss = 0.0007946440600790083
iteration 241, loss = 0.001040673814713955
iteration 242, loss = 0.0009009274072013795
iteration 243, loss = 0.0009317199583165348
iteration 244, loss = 0.0011672494001686573
iteration 245, loss = 0.000879755592904985
iteration 246, loss = 0.000843196758069098
iteration 247, loss = 0.0007922897348180413
iteration 248, loss = 0.0014156255638226867
iteration 249, loss = 0.00079803925473243
iteration 250, loss = 0.0015600589103996754
iteration 251, loss = 0.0010415459983050823
iteration 252, loss = 0.0010464044753462076
iteration 253, loss = 0.0007265824242495
iteration 254, loss = 0.0009819965343922377
iteration 255, loss = 0.0007357209105975926
iteration 256, loss = 0.001060190610587597
iteration 257, loss = 0.0008716384181752801
iteration 258, loss = 0.0010039698099717498
iteration 259, loss = 0.0008088076720014215
iteration 260, loss = 0.0009458346175961196
iteration 261, loss = 0.0019552106969058514
iteration 262, loss = 0.0010910159908235073
iteration 263, loss = 0.0012171800481155515
iteration 264, loss = 0.001620274386368692
iteration 265, loss = 0.0011616777628660202
iteration 266, loss = 0.0007395037682726979
iteration 267, loss = 0.0007172918412834406
iteration 268, loss = 0.0007767933420836926
iteration 269, loss = 0.0011908498127013445
iteration 270, loss = 0.0009458931162953377
iteration 271, loss = 0.0009249174036085606
iteration 272, loss = 0.0007349341758526862
iteration 273, loss = 0.0010874505387619138
iteration 274, loss = 0.0007621732074767351
iteration 275, loss = 0.000878474093042314
iteration 276, loss = 0.0009334809146821499
iteration 277, loss = 0.0011431495659053326
iteration 278, loss = 0.0008962683496065438
iteration 279, loss = 0.0007854094728827477
iteration 280, loss = 0.0007250439375638962
iteration 281, loss = 0.0016498053446412086
iteration 282, loss = 0.0007627207087352872
iteration 283, loss = 0.001059405505657196
iteration 284, loss = 0.0007890385459177196
iteration 285, loss = 0.0016086339019238949
iteration 286, loss = 0.0008104571024887264
iteration 287, loss = 0.0008996827527880669
iteration 288, loss = 0.0008208393701352179
iteration 289, loss = 0.0008365068933926523
iteration 290, loss = 0.0008637838182039559
iteration 291, loss = 0.0008733991999179125
iteration 292, loss = 0.0008621534798294306
iteration 293, loss = 0.0007575898780487478
iteration 294, loss = 0.0008248673402704298
iteration 295, loss = 0.001085604541003704
iteration 296, loss = 0.0008639016887173057
iteration 297, loss = 0.0008457953808829188
iteration 298, loss = 0.0018265717662870884
iteration 299, loss = 0.0008874586783349514
iteration 300, loss = 0.0009169772383756936
iteration 1, loss = 0.001053149113431573
iteration 2, loss = 0.0007554301409982145
iteration 3, loss = 0.0008180508157238364
iteration 4, loss = 0.0008494359208270907
iteration 5, loss = 0.0009017139673233032
iteration 6, loss = 0.0010837086010724306
iteration 7, loss = 0.0008784605888649821
iteration 8, loss = 0.0011349553242325783
iteration 9, loss = 0.0009391361381858587
iteration 10, loss = 0.0013560194056481123
iteration 11, loss = 0.0008236074354499578
iteration 12, loss = 0.000792813953012228
iteration 13, loss = 0.000897611549589783
iteration 14, loss = 0.0011455211788415909
iteration 15, loss = 0.0009761087130755186
iteration 16, loss = 0.0010927964467555285
iteration 17, loss = 0.0007487783441320062
iteration 18, loss = 0.0009404707234352827
iteration 19, loss = 0.0008699136669747531
iteration 20, loss = 0.0011961222626268864
iteration 21, loss = 0.0011159321293234825
iteration 22, loss = 0.001172254211269319
iteration 23, loss = 0.0009037940762937069
iteration 24, loss = 0.001089349389076233
iteration 25, loss = 0.0018371582264080644
iteration 26, loss = 0.0007629761821590364
iteration 27, loss = 0.0014105228474363685
iteration 28, loss = 0.0008112673531286418
iteration 29, loss = 0.0008554433006793261
iteration 30, loss = 0.0016604067059233785
iteration 31, loss = 0.001050255261361599
iteration 32, loss = 0.0011211441596969962
iteration 33, loss = 0.001097937230952084
iteration 34, loss = 0.0013416685396805406
iteration 35, loss = 0.0007753203390166163
iteration 36, loss = 0.001622705371119082
iteration 37, loss = 0.0007700659334659576
iteration 38, loss = 0.0014043146511539817
iteration 39, loss = 0.0010301172733306885
iteration 40, loss = 0.0009504734189249575
iteration 41, loss = 0.0011022831313312054
iteration 42, loss = 0.0007600663811899722
iteration 43, loss = 0.0009171308483928442
iteration 44, loss = 0.001016605063341558
iteration 45, loss = 0.0008934745565056801
iteration 46, loss = 0.0006989283720031381
iteration 47, loss = 0.001826492603868246
iteration 48, loss = 0.0011888236040249467
iteration 49, loss = 0.0007926495163701475
iteration 50, loss = 0.0007988638244569302
iteration 51, loss = 0.0007562855607829988
iteration 52, loss = 0.0008411450544372201
iteration 53, loss = 0.000922490144148469
iteration 54, loss = 0.0012389757903292775
iteration 55, loss = 0.0008674986311234534
iteration 56, loss = 0.0008624232723377645
iteration 57, loss = 0.0006743397680111229
iteration 58, loss = 0.0008491984335705638
iteration 59, loss = 0.0015662473160773516
iteration 60, loss = 0.0008023984846659005
iteration 61, loss = 0.0007716059917584062
iteration 62, loss = 0.0008136143442243338
iteration 63, loss = 0.0010416493751108646
iteration 64, loss = 0.0009474502876400948
iteration 65, loss = 0.0009734894847497344
iteration 66, loss = 0.0009224353125318885
iteration 67, loss = 0.0009028822532854974
iteration 68, loss = 0.0009914133697748184
iteration 69, loss = 0.0017797279870137572
iteration 70, loss = 0.0007206931477412581
iteration 71, loss = 0.0009349166648462415
iteration 72, loss = 0.0008807911654002964
iteration 73, loss = 0.0008214161498472095
iteration 74, loss = 0.0008561085560359061
iteration 75, loss = 0.0008444953709840775
iteration 76, loss = 0.0007922628428786993
iteration 77, loss = 0.0008788278792053461
iteration 78, loss = 0.0008803001255728304
iteration 79, loss = 0.0013645096914842725
iteration 80, loss = 0.0009055224363692105
iteration 81, loss = 0.0009364424622617662
iteration 82, loss = 0.0009556722361594439
iteration 83, loss = 0.000976779032498598
iteration 84, loss = 0.0009573267889209092
iteration 85, loss = 0.000847585906740278
iteration 86, loss = 0.0008658104343339801
iteration 87, loss = 0.0008556006359867752
iteration 88, loss = 0.0010623108828440309
iteration 89, loss = 0.0007332487148232758
iteration 90, loss = 0.0007866233354434371
iteration 91, loss = 0.0007799637387506664
iteration 92, loss = 0.0011977021349593997
iteration 93, loss = 0.001049280515871942
iteration 94, loss = 0.0008982507279142737
iteration 95, loss = 0.000715414178557694
iteration 96, loss = 0.0012160147307440639
iteration 97, loss = 0.0008594037499278784
iteration 98, loss = 0.001057279878295958
iteration 99, loss = 0.0007759439176879823
iteration 100, loss = 0.0008556779357604682
iteration 101, loss = 0.0007226783782243729
iteration 102, loss = 0.000709091778844595
iteration 103, loss = 0.0014660116285085678
iteration 104, loss = 0.0008225188939832151
iteration 105, loss = 0.0008395438198931515
iteration 106, loss = 0.0015267595881596208
iteration 107, loss = 0.0006352976197376847
iteration 108, loss = 0.0007793457480147481
iteration 109, loss = 0.0007612294866703451
iteration 110, loss = 0.0011977730318903923
iteration 111, loss = 0.0010774945840239525
iteration 112, loss = 0.0008631640230305493
iteration 113, loss = 0.0007666131714358926
iteration 114, loss = 0.0012862789444625378
iteration 115, loss = 0.0023028242867439985
iteration 116, loss = 0.000841377186588943
iteration 117, loss = 0.0010872917482629418
iteration 118, loss = 0.0007392762345261872
iteration 119, loss = 0.0006781417760066688
iteration 120, loss = 0.0016000540927052498
iteration 121, loss = 0.0008288735989481211
iteration 122, loss = 0.0008035104838199914
iteration 123, loss = 0.0021720321383327246
iteration 124, loss = 0.0008472198387607932
iteration 125, loss = 0.001282285200431943
iteration 126, loss = 0.0008665494970045984
iteration 127, loss = 0.0007485972600989044
iteration 128, loss = 0.0008662898908369243
iteration 129, loss = 0.0011108779581263661
iteration 130, loss = 0.0008624392794445157
iteration 131, loss = 0.0008494223584420979
iteration 132, loss = 0.0006431540241464972
iteration 133, loss = 0.0011604001047089696
iteration 134, loss = 0.0010570625308901072
iteration 135, loss = 0.0007549026631750166
iteration 136, loss = 0.0010136067867279053
iteration 137, loss = 0.0008499561809003353
iteration 138, loss = 0.0009034934337250888
iteration 139, loss = 0.0016637735534459352
iteration 140, loss = 0.0007785373600199819
iteration 141, loss = 0.0007825532229617238
iteration 142, loss = 0.0015202598879113793
iteration 143, loss = 0.001198905287310481
iteration 144, loss = 0.0010757655836641788
iteration 145, loss = 0.0008331123390235007
iteration 146, loss = 0.0007757974090054631
iteration 147, loss = 0.0012237612390890718
iteration 148, loss = 0.0008950383053161204
iteration 149, loss = 0.0006660823710262775
iteration 150, loss = 0.0008642428438179195
iteration 151, loss = 0.0009346387814730406
iteration 152, loss = 0.0009611248970031738
iteration 153, loss = 0.0009795266669243574
iteration 154, loss = 0.0008012591279111803
iteration 155, loss = 0.0007907055551186204
iteration 156, loss = 0.001061554648913443
iteration 157, loss = 0.0010463931830599904
iteration 158, loss = 0.0011240042513236403
iteration 159, loss = 0.0009203523513861
iteration 160, loss = 0.001630958984605968
iteration 161, loss = 0.001121597015298903
iteration 162, loss = 0.001048842677846551
iteration 163, loss = 0.0011523760622367263
iteration 164, loss = 0.000914144329726696
iteration 165, loss = 0.0007607019506394863
iteration 166, loss = 0.0008659931481815875
iteration 167, loss = 0.0009494416881352663
iteration 168, loss = 0.0008095948141999543
iteration 169, loss = 0.0008402367820963264
iteration 170, loss = 0.0011617320124059916
iteration 171, loss = 0.0008255187422037125
iteration 172, loss = 0.0009092334075830877
iteration 173, loss = 0.0009514095727354288
iteration 174, loss = 0.001021857955493033
iteration 175, loss = 0.001415287028066814
iteration 176, loss = 0.0008648217772133648
iteration 177, loss = 0.0008326816605404019
iteration 178, loss = 0.0012196567840874195
iteration 179, loss = 0.0017927858280017972
iteration 180, loss = 0.0007999123190529644
iteration 181, loss = 0.0007474212907254696
iteration 182, loss = 0.0009573235874995589
iteration 183, loss = 0.0008843311807140708
iteration 184, loss = 0.0010294400854036212
iteration 185, loss = 0.0011506789596751332
iteration 186, loss = 0.0008146400796249509
iteration 187, loss = 0.0007502373191528022
iteration 188, loss = 0.0007904144586063921
iteration 189, loss = 0.0008474335190840065
iteration 190, loss = 0.0015449586790055037
iteration 191, loss = 0.0009216964826919138
iteration 192, loss = 0.0009489792864769697
iteration 193, loss = 0.0008715682779438794
iteration 194, loss = 0.0010700628627091646
iteration 195, loss = 0.0011937363306060433
iteration 196, loss = 0.000923001382034272
iteration 197, loss = 0.0014916788786649704
iteration 198, loss = 0.0008496809750795364
iteration 199, loss = 0.0011453484185039997
iteration 200, loss = 0.0007866199011914432
iteration 201, loss = 0.0008494328358210623
iteration 202, loss = 0.0007496767211705446
iteration 203, loss = 0.0009694694308564067
iteration 204, loss = 0.0009840037673711777
iteration 205, loss = 0.0006952332332730293
iteration 206, loss = 0.0007909368141554296
iteration 207, loss = 0.0011473479680716991
iteration 208, loss = 0.0020689712837338448
iteration 209, loss = 0.0009642743389122188
iteration 210, loss = 0.0009756283834576607
iteration 211, loss = 0.0009313931223005056
iteration 212, loss = 0.0006286245188675821
iteration 213, loss = 0.0007554796757176518
iteration 214, loss = 0.0006885798647999763
iteration 215, loss = 0.000872243894264102
iteration 216, loss = 0.001464362139813602
iteration 217, loss = 0.0008669859962537885
iteration 218, loss = 0.0008407697314396501
iteration 219, loss = 0.0012757006334140897
iteration 220, loss = 0.0011765126837417483
iteration 221, loss = 0.0009489929652772844
iteration 222, loss = 0.0015066410414874554
iteration 223, loss = 0.0012162522180005908
iteration 224, loss = 0.001177149242721498
iteration 225, loss = 0.0008548089535906911
iteration 226, loss = 0.0007150249439291656
iteration 227, loss = 0.0008714386494830251
iteration 228, loss = 0.0009165207156911492
iteration 229, loss = 0.0009078527800738811
iteration 230, loss = 0.001356179709546268
iteration 231, loss = 0.0008168572094291449
iteration 232, loss = 0.0008906720904633403
iteration 233, loss = 0.0009221280342899263
iteration 234, loss = 0.0011141191935166717
iteration 235, loss = 0.0010689643677324057
iteration 236, loss = 0.0013150249142199755
iteration 237, loss = 0.0016528001287952065
iteration 238, loss = 0.0010840771719813347
iteration 239, loss = 0.000886584515683353
iteration 240, loss = 0.00240901717916131
iteration 241, loss = 0.0009641376091167331
iteration 242, loss = 0.0007405505166389048
iteration 243, loss = 0.0008360244100913405
iteration 244, loss = 0.0008564302115701139
iteration 245, loss = 0.0012282471871003509
iteration 246, loss = 0.0007829717360436916
iteration 247, loss = 0.0007727932534180582
iteration 248, loss = 0.0007819270249456167
iteration 249, loss = 0.0015347349690273404
iteration 250, loss = 0.0007088339189067483
iteration 251, loss = 0.0008503173012286425
iteration 252, loss = 0.0007521786028519273
iteration 253, loss = 0.0008005436393432319
iteration 254, loss = 0.001724538393318653
iteration 255, loss = 0.0007546955021098256
iteration 256, loss = 0.0009159913752228022
iteration 257, loss = 0.0009955521672964096
iteration 258, loss = 0.0011725870426744223
iteration 259, loss = 0.0008344736997969449
iteration 260, loss = 0.0008148547494783998
iteration 261, loss = 0.000766539596952498
iteration 262, loss = 0.0007924963138066232
iteration 263, loss = 0.0008161260630004108
iteration 264, loss = 0.0012560519389808178
iteration 265, loss = 0.0007517155609093606
iteration 266, loss = 0.0011125277960672975
iteration 267, loss = 0.0009677124326117337
iteration 268, loss = 0.0007388077210634947
iteration 269, loss = 0.0008954404038377106
iteration 270, loss = 0.000994967995211482
iteration 271, loss = 0.0007487824186682701
iteration 272, loss = 0.0009002516744658351
iteration 273, loss = 0.000940027239266783
iteration 274, loss = 0.0009323544218204916
iteration 275, loss = 0.0009263628162443638
iteration 276, loss = 0.0011045397259294987
iteration 277, loss = 0.0011336164316162467
iteration 278, loss = 0.0008235087152570486
iteration 279, loss = 0.0010154267074540257
iteration 280, loss = 0.0017698946176096797
iteration 281, loss = 0.0008050061878748238
iteration 282, loss = 0.0006905465270392597
iteration 283, loss = 0.0007553778705187142
iteration 284, loss = 0.0007842855411581695
iteration 285, loss = 0.0008048913441598415
iteration 286, loss = 0.0015182924689725041
iteration 287, loss = 0.0007372847758233547
iteration 288, loss = 0.0007847496308386326
iteration 289, loss = 0.0008169049979187548
iteration 290, loss = 0.0017110708868131042
iteration 291, loss = 0.000785447598900646
iteration 292, loss = 0.0008011639583855867
iteration 293, loss = 0.000872482662089169
iteration 294, loss = 0.0011548586189746857
iteration 295, loss = 0.0007964838296175003
iteration 296, loss = 0.0012415592791512609
iteration 297, loss = 0.000827498733997345
iteration 298, loss = 0.0008210300584323704
iteration 299, loss = 0.0007852640701457858
iteration 300, loss = 0.0010005602380260825
iteration 1, loss = 0.0008113979129120708
iteration 2, loss = 0.0009450952638871968
iteration 3, loss = 0.0015439848648384213
iteration 4, loss = 0.0008261351031251252
iteration 5, loss = 0.001487111672759056
iteration 6, loss = 0.0008882840047590435
iteration 7, loss = 0.0008943132124841213
iteration 8, loss = 0.0008636139100417495
iteration 9, loss = 0.0011118880938738585
iteration 10, loss = 0.0008421544334851205
iteration 11, loss = 0.0010553705506026745
iteration 12, loss = 0.000965943792834878
iteration 13, loss = 0.000909161870367825
iteration 14, loss = 0.0011025015264749527
iteration 15, loss = 0.0007981468224897981
iteration 16, loss = 0.0009727869182825089
iteration 17, loss = 0.0007128326105885208
iteration 18, loss = 0.00190819357521832
iteration 19, loss = 0.0008180732256732881
iteration 20, loss = 0.0007923080120235682
iteration 21, loss = 0.0010376892751082778
iteration 22, loss = 0.0016864510253071785
iteration 23, loss = 0.0007791048265062273
iteration 24, loss = 0.0007646619342267513
iteration 25, loss = 0.0009213258163072169
iteration 26, loss = 0.0008147448534145951
iteration 27, loss = 0.0009624611702747643
iteration 28, loss = 0.0008650344097986817
iteration 29, loss = 0.0008184926118701696
iteration 30, loss = 0.0006943406187929213
iteration 31, loss = 0.0008217822760343552
iteration 32, loss = 0.0007792027899995446
iteration 33, loss = 0.0008115561795420945
iteration 34, loss = 0.0007204408175311983
iteration 35, loss = 0.0008740435005165637
iteration 36, loss = 0.0013561887899413705
iteration 37, loss = 0.0007663274882361293
iteration 38, loss = 0.0008515398949384689
iteration 39, loss = 0.001106121577322483
iteration 40, loss = 0.0011901365360245109
iteration 41, loss = 0.0007343870820477605
iteration 42, loss = 0.0009185486705973744
iteration 43, loss = 0.0011215423000976443
iteration 44, loss = 0.0009179046028293669
iteration 45, loss = 0.0008299258188344538
iteration 46, loss = 0.0008002071408554912
iteration 47, loss = 0.0015138904564082623
iteration 48, loss = 0.0010167134460061789
iteration 49, loss = 0.0007430243422277272
iteration 50, loss = 0.0008277299348264933
iteration 51, loss = 0.0015985630452632904
iteration 52, loss = 0.0011184551985934377
iteration 53, loss = 0.0012290526647120714
iteration 54, loss = 0.0008197654387913644
iteration 55, loss = 0.0011387299746274948
iteration 56, loss = 0.0008061073604039848
iteration 57, loss = 0.0007946058758534491
iteration 58, loss = 0.0008257695590145886
iteration 59, loss = 0.001114714308641851
iteration 60, loss = 0.0009061699965968728
iteration 61, loss = 0.0009698421927168965
iteration 62, loss = 0.0008710741531103849
iteration 63, loss = 0.0008239843882620335
iteration 64, loss = 0.0008610622608102858
iteration 65, loss = 0.0011263363994657993
iteration 66, loss = 0.0008357971673831344
iteration 67, loss = 0.00115211121737957
iteration 68, loss = 0.0009161692578345537
iteration 69, loss = 0.0008015140192583203
iteration 70, loss = 0.0007332215318456292
iteration 71, loss = 0.0008363588713109493
iteration 72, loss = 0.001096237450838089
iteration 73, loss = 0.0007053070585243404
iteration 74, loss = 0.0016117349732667208
iteration 75, loss = 0.0008329951670020819
iteration 76, loss = 0.0011644669575616717
iteration 77, loss = 0.0008383625536225736
iteration 78, loss = 0.0011814497411251068
iteration 79, loss = 0.0010104106040671468
iteration 80, loss = 0.0009348121238872409
iteration 81, loss = 0.0016248597530648112
iteration 82, loss = 0.0007991476450115442
iteration 83, loss = 0.0008090157061815262
iteration 84, loss = 0.001072077895514667
iteration 85, loss = 0.0010898818727582693
iteration 86, loss = 0.0009556658333167434
iteration 87, loss = 0.000973924295976758
iteration 88, loss = 0.0009473553509451449
iteration 89, loss = 0.0008092282223515213
iteration 90, loss = 0.000996106886304915
iteration 91, loss = 0.0009935968555510044
iteration 92, loss = 0.0008463497506454587
iteration 93, loss = 0.001969262957572937
iteration 94, loss = 0.0009707516292110085
iteration 95, loss = 0.0008381832158192992
iteration 96, loss = 0.0007291556103155017
iteration 97, loss = 0.0011214142432436347
iteration 98, loss = 0.0007730017532594502
iteration 99, loss = 0.0014708742965012789
iteration 100, loss = 0.0008970531634986401
iteration 101, loss = 0.0008643188048154116
iteration 102, loss = 0.0008632548851892352
iteration 103, loss = 0.0016154380282387137
iteration 104, loss = 0.0008031221223063767
iteration 105, loss = 0.0008522187708877027
iteration 106, loss = 0.0010047215037047863
iteration 107, loss = 0.0007791072712279856
iteration 108, loss = 0.0007247442845255136
iteration 109, loss = 0.0008830118458718061
iteration 110, loss = 0.0012405283050611615
iteration 111, loss = 0.0011409522267058492
iteration 112, loss = 0.0008124734740704298
iteration 113, loss = 0.0014890818856656551
iteration 114, loss = 0.0007697324617765844
iteration 115, loss = 0.001463918830268085
iteration 116, loss = 0.0008795710746198893
iteration 117, loss = 0.0009544809581711888
iteration 118, loss = 0.0009957446018233895
iteration 119, loss = 0.0010408604284748435
iteration 120, loss = 0.0009883817983791232
iteration 121, loss = 0.0008940124535001814
iteration 122, loss = 0.00174321501981467
iteration 123, loss = 0.0014487297739833593
iteration 124, loss = 0.00079632323468104
iteration 125, loss = 0.0008016572683118284
iteration 126, loss = 0.0009020549478009343
iteration 127, loss = 0.0015787325100973248
iteration 128, loss = 0.0008044761489145458
iteration 129, loss = 0.001135068479925394
iteration 130, loss = 0.0010760106379166245
iteration 131, loss = 0.0009272078168578446
iteration 132, loss = 0.0007698432309553027
iteration 133, loss = 0.000987558625638485
iteration 134, loss = 0.0016135904006659985
iteration 135, loss = 0.0008326750830747187
iteration 136, loss = 0.0012451839866116643
iteration 137, loss = 0.0011012775357812643
iteration 138, loss = 0.0007876958698034286
iteration 139, loss = 0.0009385914891026914
iteration 140, loss = 0.000761510047595948
iteration 141, loss = 0.0014260482275858521
iteration 142, loss = 0.0007808036170899868
iteration 143, loss = 0.0018512593815103173
iteration 144, loss = 0.0008136598626151681
iteration 145, loss = 0.002484341850504279
iteration 146, loss = 0.0014907793374732137
iteration 147, loss = 0.0007895284215919673
iteration 148, loss = 0.0012153013376519084
iteration 149, loss = 0.0009642422664910555
iteration 150, loss = 0.0007518851780332625
iteration 151, loss = 0.0011580666759982705
iteration 152, loss = 0.0008360421634279191
iteration 153, loss = 0.0008091164054349065
iteration 154, loss = 0.0011818320490419865
iteration 155, loss = 0.0010802298784255981
iteration 156, loss = 0.0007355918642133474
iteration 157, loss = 0.0009997561573982239
iteration 158, loss = 0.0012848465703427792
iteration 159, loss = 0.0007853524875827134
iteration 160, loss = 0.0008453601039946079
iteration 161, loss = 0.0008589159697294235
iteration 162, loss = 0.0010677300160750747
iteration 163, loss = 0.0008517768583260477
iteration 164, loss = 0.000929763657040894
iteration 165, loss = 0.0018013357184827328
iteration 166, loss = 0.0008991681970655918
iteration 167, loss = 0.000843475922010839
iteration 168, loss = 0.000893205520696938
iteration 169, loss = 0.0013630626490339637
iteration 170, loss = 0.000880582956597209
iteration 171, loss = 0.0008647798094898462
iteration 172, loss = 0.0008758280309848487
iteration 173, loss = 0.0007863064529374242
iteration 174, loss = 0.00105834542773664
iteration 175, loss = 0.000952630303800106
iteration 176, loss = 0.0011344067752361298
iteration 177, loss = 0.0016129766590893269
iteration 178, loss = 0.000698842981364578
iteration 179, loss = 0.0007450096891261637
iteration 180, loss = 0.0008086033631116152
iteration 181, loss = 0.0008282125345431268
iteration 182, loss = 0.0008636560523882508
iteration 183, loss = 0.0007721127476543188
iteration 184, loss = 0.0007641426636837423
iteration 185, loss = 0.0006689392612315714
iteration 186, loss = 0.0007778473081998527
iteration 187, loss = 0.0007708662305958569
iteration 188, loss = 0.0013214224018156528
iteration 189, loss = 0.0009814894292503595
iteration 190, loss = 0.0009658784256316721
iteration 191, loss = 0.0007755214464850724
iteration 192, loss = 0.0007484324742108583
iteration 193, loss = 0.0012457413831725717
iteration 194, loss = 0.0009713966283015907
iteration 195, loss = 0.0009639658965170383
iteration 196, loss = 0.0008729100227355957
iteration 197, loss = 0.0007628336315974593
iteration 198, loss = 0.0011954584624618292
iteration 199, loss = 0.0008446713327430189
iteration 200, loss = 0.0008179422002285719
iteration 201, loss = 0.001173955388367176
iteration 202, loss = 0.0011210222728550434
iteration 203, loss = 0.0009338437812402844
iteration 204, loss = 0.0009287975262850523
iteration 205, loss = 0.0009327929001301527
iteration 206, loss = 0.001540251774713397
iteration 207, loss = 0.0013996334746479988
iteration 208, loss = 0.0010224388679489493
iteration 209, loss = 0.001573898596689105
iteration 210, loss = 0.0012628225376829505
iteration 211, loss = 0.0008984585292637348
iteration 212, loss = 0.0008021799148991704
iteration 213, loss = 0.0007787283975630999
iteration 214, loss = 0.0006787556339986622
iteration 215, loss = 0.0008981580031104386
iteration 216, loss = 0.0007702367729507387
iteration 217, loss = 0.0008078942191787064
iteration 218, loss = 0.0012834675144404173
iteration 219, loss = 0.0019554789178073406
iteration 220, loss = 0.0009290871093980968
iteration 221, loss = 0.000816781772300601
iteration 222, loss = 0.0009587807580828667
iteration 223, loss = 0.0008608590578660369
iteration 224, loss = 0.001549242064356804
iteration 225, loss = 0.0007878578035160899
iteration 226, loss = 0.0007054632296785712
iteration 227, loss = 0.0008938683895394206
iteration 228, loss = 0.0007638350944034755
iteration 229, loss = 0.000735775800421834
iteration 230, loss = 0.0012972460826858878
iteration 231, loss = 0.0009195130551233888
iteration 232, loss = 0.0009210807038471103
iteration 233, loss = 0.000758133246563375
iteration 234, loss = 0.0008019552333280444
iteration 235, loss = 0.0010102728847414255
iteration 236, loss = 0.0016387802315875888
iteration 237, loss = 0.0010266538010910153
iteration 238, loss = 0.0012363508576527238
iteration 239, loss = 0.0008383997483178973
iteration 240, loss = 0.0007998179644346237
iteration 241, loss = 0.001124212401919067
iteration 242, loss = 0.0008238062728196383
iteration 243, loss = 0.0008616725681349635
iteration 244, loss = 0.0010059314081445336
iteration 245, loss = 0.0008847423596307635
iteration 246, loss = 0.0009094029082916677
iteration 247, loss = 0.0007445875089615583
iteration 248, loss = 0.0007567929569631815
iteration 249, loss = 0.0008376814657822251
iteration 250, loss = 0.0015245119575411081
iteration 251, loss = 0.0008442606776952744
iteration 252, loss = 0.0010128377471119165
iteration 253, loss = 0.0008166796178556979
iteration 254, loss = 0.0011043117847293615
iteration 255, loss = 0.0008013624465093017
iteration 256, loss = 0.0009443379240110517
iteration 257, loss = 0.0008272472769021988
iteration 258, loss = 0.0012493658578023314
iteration 259, loss = 0.0011345933889970183
iteration 260, loss = 0.0008870037854649127
iteration 261, loss = 0.0009553857962600887
iteration 262, loss = 0.0009571962873451412
iteration 263, loss = 0.0009550403337925673
iteration 264, loss = 0.0008111425559036434
iteration 265, loss = 0.0007929053390398622
iteration 266, loss = 0.0008908698800951242
iteration 267, loss = 0.0007822792395018041
iteration 268, loss = 0.0008090529590845108
iteration 269, loss = 0.0008547341567464173
iteration 270, loss = 0.0010653482750058174
iteration 271, loss = 0.0011893323389813304
iteration 272, loss = 0.00079629838000983
iteration 273, loss = 0.0009084541816264391
iteration 274, loss = 0.0009340310352854431
iteration 275, loss = 0.0008710679248906672
iteration 276, loss = 0.001684673479758203
iteration 277, loss = 0.001107561867684126
iteration 278, loss = 0.0008160404395312071
iteration 279, loss = 0.0009486260823905468
iteration 280, loss = 0.0009325065766461194
iteration 281, loss = 0.0012313759652897716
iteration 282, loss = 0.0009702928946353495
iteration 283, loss = 0.001072126324288547
iteration 284, loss = 0.0010272256331518292
iteration 285, loss = 0.0011108387261629105
iteration 286, loss = 0.0007814760319888592
iteration 287, loss = 0.0007998867658898234
iteration 288, loss = 0.0008067816379480064
iteration 289, loss = 0.0008182077435776591
iteration 290, loss = 0.0009647748083807528
iteration 291, loss = 0.0007058482151478529
iteration 292, loss = 0.0007567894062958658
iteration 293, loss = 0.0008546599419787526
iteration 294, loss = 0.0006605684175156057
iteration 295, loss = 0.0007368829683400691
iteration 296, loss = 0.0007937990594655275
iteration 297, loss = 0.0008497795788571239
iteration 298, loss = 0.000782769697252661
iteration 299, loss = 0.0011858257930725813
iteration 300, loss = 0.0008392527815885842
iteration 1, loss = 0.0008216267451643944
iteration 2, loss = 0.0008015887578949332
iteration 3, loss = 0.0010655545629560947
iteration 4, loss = 0.0006498108268715441
iteration 5, loss = 0.0007081223884597421
iteration 6, loss = 0.0008573472150601447
iteration 7, loss = 0.0008475413778796792
iteration 8, loss = 0.0013223413843661547
iteration 9, loss = 0.0007149563170969486
iteration 10, loss = 0.0008254085551016033
iteration 11, loss = 0.0014571987558156252
iteration 12, loss = 0.0007703572046011686
iteration 13, loss = 0.0008035901701077819
iteration 14, loss = 0.0008603769820183516
iteration 15, loss = 0.0009307843283750117
iteration 16, loss = 0.0010470793349668384
iteration 17, loss = 0.0006931400275789201
iteration 18, loss = 0.000903625157661736
iteration 19, loss = 0.0007683848962187767
iteration 20, loss = 0.0009366846643388271
iteration 21, loss = 0.0009076261194422841
iteration 22, loss = 0.0007206231239251792
iteration 23, loss = 0.0008260576287284493
iteration 24, loss = 0.0007919662166386843
iteration 25, loss = 0.0007073106826283038
iteration 26, loss = 0.000875984609592706
iteration 27, loss = 0.0008587377960793674
iteration 28, loss = 0.0009246350382454693
iteration 29, loss = 0.0007772478275001049
iteration 30, loss = 0.000818986794911325
iteration 31, loss = 0.0014416680205613375
iteration 32, loss = 0.0008431895985268056
iteration 33, loss = 0.0011854938929900527
iteration 34, loss = 0.0008434371557086706
iteration 35, loss = 0.0009434564271941781
iteration 36, loss = 0.0010291925864294171
iteration 37, loss = 0.0008618722204118967
iteration 38, loss = 0.0010478164767846465
iteration 39, loss = 0.0008041411638259888
iteration 40, loss = 0.0010148286819458008
iteration 41, loss = 0.0007784286281093955
iteration 42, loss = 0.001135349040850997
iteration 43, loss = 0.001495968783274293
iteration 44, loss = 0.0009441064903512597
iteration 45, loss = 0.0018203294603154063
iteration 46, loss = 0.0008945466834120452
iteration 47, loss = 0.0009591290727257729
iteration 48, loss = 0.001761272200383246
iteration 49, loss = 0.0011558333644643426
iteration 50, loss = 0.0007464159280061722
iteration 51, loss = 0.0010490285931155086
iteration 52, loss = 0.0008983252919279039
iteration 53, loss = 0.0008681442122906446
iteration 54, loss = 0.0012913623359054327
iteration 55, loss = 0.0009621705394238234
iteration 56, loss = 0.0011627650819718838
iteration 57, loss = 0.0008871739264577627
iteration 58, loss = 0.000708847539499402
iteration 59, loss = 0.0009428775520063937
iteration 60, loss = 0.0007990466547198594
iteration 61, loss = 0.001007604063488543
iteration 62, loss = 0.0008560824790038168
iteration 63, loss = 0.001335574546828866
iteration 64, loss = 0.0008596762199886143
iteration 65, loss = 0.0008338794577866793
iteration 66, loss = 0.0008127104956656694
iteration 67, loss = 0.0008047385490499437
iteration 68, loss = 0.0007161233806982636
iteration 69, loss = 0.0008052262710407376
iteration 70, loss = 0.0008546627359464765
iteration 71, loss = 0.0010084019741043448
iteration 72, loss = 0.0007970695151016116
iteration 73, loss = 0.0008566841715946794
iteration 74, loss = 0.001363985938951373
iteration 75, loss = 0.0008229719824157655
iteration 76, loss = 0.0007735573453828692
iteration 77, loss = 0.0014972607605159283
iteration 78, loss = 0.001074770581908524
iteration 79, loss = 0.0008422242826782167
iteration 80, loss = 0.000830901088193059
iteration 81, loss = 0.0008968350011855364
iteration 82, loss = 0.0007086904370225966
iteration 83, loss = 0.0008239686139859259
iteration 84, loss = 0.0012460416182875633
iteration 85, loss = 0.0007644492434337735
iteration 86, loss = 0.001176376361399889
iteration 87, loss = 0.0009407870238646865
iteration 88, loss = 0.0006140306941233575
iteration 89, loss = 0.0008246070938184857
iteration 90, loss = 0.000855178339406848
iteration 91, loss = 0.00084441970102489
iteration 92, loss = 0.0009602419449947774
iteration 93, loss = 0.0007443638169206679
iteration 94, loss = 0.0007452961290255189
iteration 95, loss = 0.0010690069757401943
iteration 96, loss = 0.0008486901642754674
iteration 97, loss = 0.0008549602935090661
iteration 98, loss = 0.0015401458367705345
iteration 99, loss = 0.0014416523044928908
iteration 100, loss = 0.0008514616638422012
iteration 101, loss = 0.0008663905318826437
iteration 102, loss = 0.00123269809409976
iteration 103, loss = 0.0007629893952980638
iteration 104, loss = 0.0007225401350297034
iteration 105, loss = 0.0010181282414123416
iteration 106, loss = 0.0007972848834469914
iteration 107, loss = 0.0010778786381706595
iteration 108, loss = 0.0007770443917252123
iteration 109, loss = 0.0018358249217271805
iteration 110, loss = 0.0007019948679953814
iteration 111, loss = 0.0007901680655777454
iteration 112, loss = 0.0008624351467005908
iteration 113, loss = 0.0008181865559890866
iteration 114, loss = 0.0006916747079230845
iteration 115, loss = 0.0007559944642707705
iteration 116, loss = 0.001463330932892859
iteration 117, loss = 0.000938021345064044
iteration 118, loss = 0.0006866832263767719
iteration 119, loss = 0.0010122611420229077
iteration 120, loss = 0.000977840507403016
iteration 121, loss = 0.0011635427363216877
iteration 122, loss = 0.0011584797175601125
iteration 123, loss = 0.0009282819228246808
iteration 124, loss = 0.0008233874104917049
iteration 125, loss = 0.0008980021812021732
iteration 126, loss = 0.0008441627724096179
iteration 127, loss = 0.0010252950014546514
iteration 128, loss = 0.0009218862978741527
iteration 129, loss = 0.0009595752926543355
iteration 130, loss = 0.0016707672039046884
iteration 131, loss = 0.0014901245012879372
iteration 132, loss = 0.0008881439571268857
iteration 133, loss = 0.0015929641667753458
iteration 134, loss = 0.0016727775800973177
iteration 135, loss = 0.0007116534397937357
iteration 136, loss = 0.0008108955225907266
iteration 137, loss = 0.0008627937640994787
iteration 138, loss = 0.0015195622108876705
iteration 139, loss = 0.001022534561343491
iteration 140, loss = 0.0008355457102879882
iteration 141, loss = 0.0018056515837088227
iteration 142, loss = 0.001566405058838427
iteration 143, loss = 0.0007724678725935519
iteration 144, loss = 0.0015743483090773225
iteration 145, loss = 0.0011503095738589764
iteration 146, loss = 0.0007912808796390891
iteration 147, loss = 0.0007771218661218882
iteration 148, loss = 0.0010022306814789772
iteration 149, loss = 0.0009423319133929908
iteration 150, loss = 0.0008282394264824688
iteration 151, loss = 0.0012186623644083738
iteration 152, loss = 0.0007518561906181276
iteration 153, loss = 0.0010428593959659338
iteration 154, loss = 0.0006799882394261658
iteration 155, loss = 0.0010697435354813933
iteration 156, loss = 0.0010157835204154253
iteration 157, loss = 0.0010009316029027104
iteration 158, loss = 0.0007940097129903734
iteration 159, loss = 0.0016360078006982803
iteration 160, loss = 0.0008702687919139862
iteration 161, loss = 0.0007673442014493048
iteration 162, loss = 0.0008394436445087194
iteration 163, loss = 0.0015696887858211994
iteration 164, loss = 0.0007891162531450391
iteration 165, loss = 0.0008075621444731951
iteration 166, loss = 0.0011324191000312567
iteration 167, loss = 0.0008348830742761493
iteration 168, loss = 0.0014274839777499437
iteration 169, loss = 0.0011810187716037035
iteration 170, loss = 0.0010320371948182583
iteration 171, loss = 0.000970328925177455
iteration 172, loss = 0.0010622593108564615
iteration 173, loss = 0.0008697008015587926
iteration 174, loss = 0.00116158917080611
iteration 175, loss = 0.0009347058949060738
iteration 176, loss = 0.0019190965685993433
iteration 177, loss = 0.0012775672366842628
iteration 178, loss = 0.0014991678763180971
iteration 179, loss = 0.0007010900881141424
iteration 180, loss = 0.0010065692476928234
iteration 181, loss = 0.0008235030109062791
iteration 182, loss = 0.0008031519246287644
iteration 183, loss = 0.0009649478015489876
iteration 184, loss = 0.0008337195031344891
iteration 185, loss = 0.001514363451860845
iteration 186, loss = 0.0008318027830682695
iteration 187, loss = 0.000731836655177176
iteration 188, loss = 0.0007528405403718352
iteration 189, loss = 0.0007588431471958756
iteration 190, loss = 0.0007553996401838958
iteration 191, loss = 0.000905090942978859
iteration 192, loss = 0.0019832258112728596
iteration 193, loss = 0.0008801425574347377
iteration 194, loss = 0.0012043998576700687
iteration 195, loss = 0.0008222111500799656
iteration 196, loss = 0.0008526702877134085
iteration 197, loss = 0.0009761128458194435
iteration 198, loss = 0.0009507033391855657
iteration 199, loss = 0.0007051668944768608
iteration 200, loss = 0.0008936897502280772
iteration 201, loss = 0.0007970210281200707
iteration 202, loss = 0.0010018564062193036
iteration 203, loss = 0.0009005678002722561
iteration 204, loss = 0.0016271553467959166
iteration 205, loss = 0.0008055962971411645
iteration 206, loss = 0.0009096297435462475
iteration 207, loss = 0.0008450186578556895
iteration 208, loss = 0.0008430033922195435
iteration 209, loss = 0.001111524528823793
iteration 210, loss = 0.0011656025890260935
iteration 211, loss = 0.0007396131986752152
iteration 212, loss = 0.00253499299287796
iteration 213, loss = 0.000869502080604434
iteration 214, loss = 0.0009352254564873874
iteration 215, loss = 0.0008652432588860393
iteration 216, loss = 0.001039081602357328
iteration 217, loss = 0.0008441811078228056
iteration 218, loss = 0.0010368191869929433
iteration 219, loss = 0.0008251887047663331
iteration 220, loss = 0.0008388310670852661
iteration 221, loss = 0.0010738895507529378
iteration 222, loss = 0.0010542456293478608
iteration 223, loss = 0.0011308573884889483
iteration 224, loss = 0.0008877660147845745
iteration 225, loss = 0.0007322688470594585
iteration 226, loss = 0.000819629174657166
iteration 227, loss = 0.0007026536623016
iteration 228, loss = 0.0007931996369734406
iteration 229, loss = 0.0007494859164580703
iteration 230, loss = 0.0007368959486484528
iteration 231, loss = 0.001232784939929843
iteration 232, loss = 0.000837945262901485
iteration 233, loss = 0.0010690222261473536
iteration 234, loss = 0.0011174504179507494
iteration 235, loss = 0.0008477199589833617
iteration 236, loss = 0.0007640101248398423
iteration 237, loss = 0.0009072171524167061
iteration 238, loss = 0.0007346075726673007
iteration 239, loss = 0.0012160548940300941
iteration 240, loss = 0.0017578394617885351
iteration 241, loss = 0.0008991918875835836
iteration 242, loss = 0.0008542956784367561
iteration 243, loss = 0.0016383317997679114
iteration 244, loss = 0.0007856223965063691
iteration 245, loss = 0.0007862772326916456
iteration 246, loss = 0.0007673258660361171
iteration 247, loss = 0.0008004893315955997
iteration 248, loss = 0.0008858545334078372
iteration 249, loss = 0.0017203567549586296
iteration 250, loss = 0.0009478547144681215
iteration 251, loss = 0.0009158953907899559
iteration 252, loss = 0.0007811172399669886
iteration 253, loss = 0.0008157553966157138
iteration 254, loss = 0.001129487412981689
iteration 255, loss = 0.0008149183122441173
iteration 256, loss = 0.0008779463241808116
iteration 257, loss = 0.0006997134769335389
iteration 258, loss = 0.0006788590108044446
iteration 259, loss = 0.0008553274092264473
iteration 260, loss = 0.001069330144673586
iteration 261, loss = 0.0011309386463835835
iteration 262, loss = 0.0016499623889103532
iteration 263, loss = 0.000763125077355653
iteration 264, loss = 0.0010707129258662462
iteration 265, loss = 0.0008275656728073955
iteration 266, loss = 0.0016240414697676897
iteration 267, loss = 0.0010981797240674496
iteration 268, loss = 0.0008620725711807609
iteration 269, loss = 0.0013971080770716071
iteration 270, loss = 0.0008456513751298189
iteration 271, loss = 0.0010000073816627264
iteration 272, loss = 0.0008207591017708182
iteration 273, loss = 0.0009089364903047681
iteration 274, loss = 0.0008652732940390706
iteration 275, loss = 0.0008792121661826968
iteration 276, loss = 0.0008242131443694234
iteration 277, loss = 0.001372012891806662
iteration 278, loss = 0.0011932996567338705
iteration 279, loss = 0.0010429058456793427
iteration 280, loss = 0.000921485829167068
iteration 281, loss = 0.000769169011618942
iteration 282, loss = 0.001759725739248097
iteration 283, loss = 0.0009033562382683158
iteration 284, loss = 0.001087899086996913
iteration 285, loss = 0.000943985884077847
iteration 286, loss = 0.0009678907808847725
iteration 287, loss = 0.0014337018365040421
iteration 288, loss = 0.0008097524987533689
iteration 289, loss = 0.0008279463509097695
iteration 290, loss = 0.0008306432282552123
iteration 291, loss = 0.0009853149531409144
iteration 292, loss = 0.000762383861001581
iteration 293, loss = 0.0012078584404662251
iteration 294, loss = 0.0007317884010262787
iteration 295, loss = 0.0008016040083020926
iteration 296, loss = 0.0010641120607033372
iteration 297, loss = 0.0008389553986489773
iteration 298, loss = 0.0007321664597839117
iteration 299, loss = 0.0009436742402613163
iteration 300, loss = 0.0007609514868818223
iteration 1, loss = 0.0007080378709360957
iteration 2, loss = 0.001187872258014977
iteration 3, loss = 0.0008151611546054482
iteration 4, loss = 0.0006999964243732393
iteration 5, loss = 0.0008143030572682619
iteration 6, loss = 0.0009919683216139674
iteration 7, loss = 0.0008017653599381447
iteration 8, loss = 0.0008555711829103529
iteration 9, loss = 0.0007145139970816672
iteration 10, loss = 0.0012050489895045757
iteration 11, loss = 0.0008649397059343755
iteration 12, loss = 0.0013873110292479396
iteration 13, loss = 0.0017836877377703786
iteration 14, loss = 0.0010387701913714409
iteration 15, loss = 0.001349462429061532
iteration 16, loss = 0.0010859854519367218
iteration 17, loss = 0.0011460664682090282
iteration 18, loss = 0.000940910424105823
iteration 19, loss = 0.0007432938436977565
iteration 20, loss = 0.0008801667718216777
iteration 21, loss = 0.0008023031987249851
iteration 22, loss = 0.0009386925376020372
iteration 23, loss = 0.0007708078483119607
iteration 24, loss = 0.0012372611090540886
iteration 25, loss = 0.0007503506494686007
iteration 26, loss = 0.0008415387710556388
iteration 27, loss = 0.0007615822833031416
iteration 28, loss = 0.0009823351865634322
iteration 29, loss = 0.000921121274586767
iteration 30, loss = 0.001049299375154078
iteration 31, loss = 0.000703630328644067
iteration 32, loss = 0.0010671387426555157
iteration 33, loss = 0.000987663515843451
iteration 34, loss = 0.0009116126457229257
iteration 35, loss = 0.0012953621335327625
iteration 36, loss = 0.0014935288345441222
iteration 37, loss = 0.0010530739091336727
iteration 38, loss = 0.001011406653560698
iteration 39, loss = 0.0008359593339264393
iteration 40, loss = 0.0008805745164863765
iteration 41, loss = 0.0010767843341454864
iteration 42, loss = 0.0009094335255213082
iteration 43, loss = 0.000756558554712683
iteration 44, loss = 0.0010224736761301756
iteration 45, loss = 0.0009386670426465571
iteration 46, loss = 0.0018768925219774246
iteration 47, loss = 0.0008517762180417776
iteration 48, loss = 0.0008683142368681729
iteration 49, loss = 0.0009269370348192751
iteration 50, loss = 0.0006645326502621174
iteration 51, loss = 0.0007900823839008808
iteration 52, loss = 0.001702361274510622
iteration 53, loss = 0.0010900204069912434
iteration 54, loss = 0.0006986450753174722
iteration 55, loss = 0.0008170092478394508
iteration 56, loss = 0.0014302604831755161
iteration 57, loss = 0.0010316409170627594
iteration 58, loss = 0.0016229900065809488
iteration 59, loss = 0.0006720166420564055
iteration 60, loss = 0.0011313391150906682
iteration 61, loss = 0.0011667200597003102
iteration 62, loss = 0.0009602490463294089
iteration 63, loss = 0.0008065762231126428
iteration 64, loss = 0.0009637102484703064
iteration 65, loss = 0.0010321192676201463
iteration 66, loss = 0.0008559885318391025
iteration 67, loss = 0.0008349537383764982
iteration 68, loss = 0.0007168737356550992
iteration 69, loss = 0.0007957473862916231
iteration 70, loss = 0.0009065106278285384
iteration 71, loss = 0.0009091895772144198
iteration 72, loss = 0.0007951483712531626
iteration 73, loss = 0.0015763972187414765
iteration 74, loss = 0.000839914835523814
iteration 75, loss = 0.0006902446039021015
iteration 76, loss = 0.0007625439902767539
iteration 77, loss = 0.00087151670595631
iteration 78, loss = 0.0014891976024955511
iteration 79, loss = 0.0009653576416894794
iteration 80, loss = 0.0008278917521238327
iteration 81, loss = 0.0007430041441693902
iteration 82, loss = 0.0008657847647555172
iteration 83, loss = 0.0006622800719924271
iteration 84, loss = 0.0006997174932621419
iteration 85, loss = 0.0007883015787228942
iteration 86, loss = 0.0010791074018925428
iteration 87, loss = 0.0007220759289339185
iteration 88, loss = 0.0018005645833909512
iteration 89, loss = 0.0007654375513084233
iteration 90, loss = 0.0009760507964529097
iteration 91, loss = 0.001114379265345633
iteration 92, loss = 0.001055516884662211
iteration 93, loss = 0.0009107302175834775
iteration 94, loss = 0.0006868449272587895
iteration 95, loss = 0.000750081380829215
iteration 96, loss = 0.0007827732479199767
iteration 97, loss = 0.0008846705313771963
iteration 98, loss = 0.0012821040581911802
iteration 99, loss = 0.0008288318640552461
iteration 100, loss = 0.00078259949805215
iteration 101, loss = 0.0007494479650631547
iteration 102, loss = 0.0015710089355707169
iteration 103, loss = 0.0007979663205333054
iteration 104, loss = 0.0010689739137887955
iteration 105, loss = 0.0009491707314737141
iteration 106, loss = 0.0016615248750895262
iteration 107, loss = 0.0007863591890782118
iteration 108, loss = 0.0010237701935693622
iteration 109, loss = 0.0018825666047632694
iteration 110, loss = 0.001037007663398981
iteration 111, loss = 0.0007906885584816337
iteration 112, loss = 0.0007946357363834977
iteration 113, loss = 0.0008376459591090679
iteration 114, loss = 0.0011684660566970706
iteration 115, loss = 0.0006874853861518204
iteration 116, loss = 0.0009892861125990748
iteration 117, loss = 0.0009505909401923418
iteration 118, loss = 0.0008195272530429065
iteration 119, loss = 0.0011167866177856922
iteration 120, loss = 0.0010667117312550545
iteration 121, loss = 0.0009737133514136076
iteration 122, loss = 0.0006943158805370331
iteration 123, loss = 0.0007217185921035707
iteration 124, loss = 0.001213397947140038
iteration 125, loss = 0.000872328644618392
iteration 126, loss = 0.0009609569679014385
iteration 127, loss = 0.0008254350977949798
iteration 128, loss = 0.0007772500393912196
iteration 129, loss = 0.0009634284651838243
iteration 130, loss = 0.0015045733889564872
iteration 131, loss = 0.0007836670847609639
iteration 132, loss = 0.0009697380010038614
iteration 133, loss = 0.0008678620215505362
iteration 134, loss = 0.0015132729895412922
iteration 135, loss = 0.0011427790159359574
iteration 136, loss = 0.0010331871453672647
iteration 137, loss = 0.0008633994730189443
iteration 138, loss = 0.0010647571180015802
iteration 139, loss = 0.00159951183013618
iteration 140, loss = 0.0011865927372127771
iteration 141, loss = 0.0011349823325872421
iteration 142, loss = 0.000735089706722647
iteration 143, loss = 0.0007280940772034228
iteration 144, loss = 0.0007947911508381367
iteration 145, loss = 0.0008027556468732655
iteration 146, loss = 0.0008946047746576369
iteration 147, loss = 0.0008541112765669823
iteration 148, loss = 0.0009466208866797388
iteration 149, loss = 0.0012535039568319917
iteration 150, loss = 0.0009444388560950756
iteration 151, loss = 0.00109944271389395
iteration 152, loss = 0.0010953323217108846
iteration 153, loss = 0.00085464894073084
iteration 154, loss = 0.000930829148273915
iteration 155, loss = 0.0010325734037905931
iteration 156, loss = 0.000758915557526052
iteration 157, loss = 0.0009878866840153933
iteration 158, loss = 0.0012259641662240028
iteration 159, loss = 0.00106413837056607
iteration 160, loss = 0.0014493344351649284
iteration 161, loss = 0.0010349297663196921
iteration 162, loss = 0.0008250615792348981
iteration 163, loss = 0.001338957459665835
iteration 164, loss = 0.0008917064405977726
iteration 165, loss = 0.0009888976346701384
iteration 166, loss = 0.0009351982735097408
iteration 167, loss = 0.0008017700747586787
iteration 168, loss = 0.0015105910133570433
iteration 169, loss = 0.0010484621161594987
iteration 170, loss = 0.0017056949436664581
iteration 171, loss = 0.0009970454266294837
iteration 172, loss = 0.001470878254622221
iteration 173, loss = 0.0007365452474914491
iteration 174, loss = 0.00069866341073066
iteration 175, loss = 0.0009830112103372812
iteration 176, loss = 0.0014330667909234762
iteration 177, loss = 0.0016012733103707433
iteration 178, loss = 0.0009393825894221663
iteration 179, loss = 0.0007970684091560543
iteration 180, loss = 0.0013948241248726845
iteration 181, loss = 0.0009182917419821024
iteration 182, loss = 0.0007039549527689815
iteration 183, loss = 0.0008498517563566566
iteration 184, loss = 0.0015033703530207276
iteration 185, loss = 0.0009396960958838463
iteration 186, loss = 0.0012197665637359023
iteration 187, loss = 0.0009372089407406747
iteration 188, loss = 0.0007088953861966729
iteration 189, loss = 0.0017261061584576964
iteration 190, loss = 0.0007603519479744136
iteration 191, loss = 0.0010553965112194419
iteration 192, loss = 0.000855910824611783
iteration 193, loss = 0.0016717662801966071
iteration 194, loss = 0.0008177337585948408
iteration 195, loss = 0.0012505499180406332
iteration 196, loss = 0.0011605764739215374
iteration 197, loss = 0.0008394015021622181
iteration 198, loss = 0.0011194322723895311
iteration 199, loss = 0.0012522612232714891
iteration 200, loss = 0.0016275534871965647
iteration 201, loss = 0.0013695735251531005
iteration 202, loss = 0.0008805770776234567
iteration 203, loss = 0.0007869802066124976
iteration 204, loss = 0.0006635040044784546
iteration 205, loss = 0.0010542318923398852
iteration 206, loss = 0.000862404122017324
iteration 207, loss = 0.0011108594480901957
iteration 208, loss = 0.0011363778030499816
iteration 209, loss = 0.0009037223644554615
iteration 210, loss = 0.0008800409268587828
iteration 211, loss = 0.0008054268546402454
iteration 212, loss = 0.0007316286209970713
iteration 213, loss = 0.0007079212227836251
iteration 214, loss = 0.000722407887224108
iteration 215, loss = 0.001607042970135808
iteration 216, loss = 0.0008267693920060992
iteration 217, loss = 0.0008965992019511759
iteration 218, loss = 0.001108985859900713
iteration 219, loss = 0.0009314937051385641
iteration 220, loss = 0.0008733264403417706
iteration 221, loss = 0.0014730391558259726
iteration 222, loss = 0.000916560529731214
iteration 223, loss = 0.0008926043519750237
iteration 224, loss = 0.0006952279363758862
iteration 225, loss = 0.0009740249952301383
iteration 226, loss = 0.0009226001566275954
iteration 227, loss = 0.0008042759727686644
iteration 228, loss = 0.0007476936443708837
iteration 229, loss = 0.000717848539352417
iteration 230, loss = 0.0014534889487549663
iteration 231, loss = 0.0007223379798233509
iteration 232, loss = 0.001523888553492725
iteration 233, loss = 0.0015912834787741303
iteration 234, loss = 0.0012098854640498757
iteration 235, loss = 0.0008579686400480568
iteration 236, loss = 0.0008892110781744123
iteration 237, loss = 0.0007585085113532841
iteration 238, loss = 0.0008547570905648172
iteration 239, loss = 0.0007172739715315402
iteration 240, loss = 0.0009198071784339845
iteration 241, loss = 0.0007607556181028485
iteration 242, loss = 0.000776294618844986
iteration 243, loss = 0.0008335009915754199
iteration 244, loss = 0.001307562692090869
iteration 245, loss = 0.0010953610762953758
iteration 246, loss = 0.0008610381628386676
iteration 247, loss = 0.0008230161038227379
iteration 248, loss = 0.0008192493114620447
iteration 249, loss = 0.0008367166155949235
iteration 250, loss = 0.0007440977497026324
iteration 251, loss = 0.0012890782672911882
iteration 252, loss = 0.0010455516166985035
iteration 253, loss = 0.0009318814263679087
iteration 254, loss = 0.0008599456632509828
iteration 255, loss = 0.0007829476962797344
iteration 256, loss = 0.0009543114574626088
iteration 257, loss = 0.0009337141527794302
iteration 258, loss = 0.0007990417070686817
iteration 259, loss = 0.0007884652586653829
iteration 260, loss = 0.0011118734255433083
iteration 261, loss = 0.0007982976967468858
iteration 262, loss = 0.0007681300630792975
iteration 263, loss = 0.0008356537437066436
iteration 264, loss = 0.001138449413701892
iteration 265, loss = 0.000771553663071245
iteration 266, loss = 0.0007312300149351358
iteration 267, loss = 0.0011091581545770168
iteration 268, loss = 0.0009600752382539213
iteration 269, loss = 0.0009210421703755856
iteration 270, loss = 0.0006779302493669093
iteration 271, loss = 0.0008250527898781002
iteration 272, loss = 0.0008681475301273167
iteration 273, loss = 0.0015005026943981647
iteration 274, loss = 0.0006956300931051373
iteration 275, loss = 0.0008671105606481433
iteration 276, loss = 0.0007752215024083853
iteration 277, loss = 0.0010614718776196241
iteration 278, loss = 0.0008471273467876017
iteration 279, loss = 0.0007282757433131337
iteration 280, loss = 0.0007761149317957461
iteration 281, loss = 0.0007978561334311962
iteration 282, loss = 0.0008175194961950183
iteration 283, loss = 0.0009650301071815193
iteration 284, loss = 0.0009335249196738005
iteration 285, loss = 0.0009212899603880942
iteration 286, loss = 0.0009476991253904998
iteration 287, loss = 0.0010350057855248451
iteration 288, loss = 0.0009104626951739192
iteration 289, loss = 0.0010299196001142263
iteration 290, loss = 0.0010641940170899034
iteration 291, loss = 0.0011816215701401234
iteration 292, loss = 0.000807449861895293
iteration 293, loss = 0.000947247666772455
iteration 294, loss = 0.0010960048530250788
iteration 295, loss = 0.0007737127016298473
iteration 296, loss = 0.000925404136069119
iteration 297, loss = 0.0009764190763235092
iteration 298, loss = 0.0007505175890401006
iteration 299, loss = 0.00121126149315387
iteration 300, loss = 0.0012613468570634723
iteration 1, loss = 0.0008936857338994741
iteration 2, loss = 0.0012519023148342967
iteration 3, loss = 0.0008710442343726754
iteration 4, loss = 0.0007569845183752477
iteration 5, loss = 0.0007036715978756547
iteration 6, loss = 0.0009146351949311793
iteration 7, loss = 0.0009283542167395353
iteration 8, loss = 0.0008604974136687815
iteration 9, loss = 0.0009465791517868638
iteration 10, loss = 0.0013003147905692458
iteration 11, loss = 0.0010465991217643023
iteration 12, loss = 0.0007575711933895946
iteration 13, loss = 0.001753531047143042
iteration 14, loss = 0.0012881647562608123
iteration 15, loss = 0.0008425974519923329
iteration 16, loss = 0.0011229292722418904
iteration 17, loss = 0.0008363815722987056
iteration 18, loss = 0.0007500210194848478
iteration 19, loss = 0.0007617663941346109
iteration 20, loss = 0.0007853492861613631
iteration 21, loss = 0.0008074355428107083
iteration 22, loss = 0.0008573255036026239
iteration 23, loss = 0.0010413535637781024
iteration 24, loss = 0.0013199913082644343
iteration 25, loss = 0.0009238267666660249
iteration 26, loss = 0.0007792969699949026
iteration 27, loss = 0.0009258755017071962
iteration 28, loss = 0.0011765356175601482
iteration 29, loss = 0.0018929506186395884
iteration 30, loss = 0.001642787829041481
iteration 31, loss = 0.0008284118375740945
iteration 32, loss = 0.0009427653858438134
iteration 33, loss = 0.0014527773018926382
iteration 34, loss = 0.0009510882082395256
iteration 35, loss = 0.0009929891675710678
iteration 36, loss = 0.0007699840934947133
iteration 37, loss = 0.0009443381568416953
iteration 38, loss = 0.0008133537485264242
iteration 39, loss = 0.0008784300880506635
iteration 40, loss = 0.0022241377737373114
iteration 41, loss = 0.0008540175622329116
iteration 42, loss = 0.0007307034102268517
iteration 43, loss = 0.0007771662785671651
iteration 44, loss = 0.000849227886646986
iteration 45, loss = 0.0010251800995320082
iteration 46, loss = 0.0007141754031181335
iteration 47, loss = 0.000690365326590836
iteration 48, loss = 0.0008952844073064625
iteration 49, loss = 0.0016158237121999264
iteration 50, loss = 0.0008829486323520541
iteration 51, loss = 0.0007046068785712123
iteration 52, loss = 0.0010462672216817737
iteration 53, loss = 0.0010166647844016552
iteration 54, loss = 0.001908237929455936
iteration 55, loss = 0.0007077875197865069
iteration 56, loss = 0.0010822068434208632
iteration 57, loss = 0.0007838397286832333
iteration 58, loss = 0.0015060813166201115
iteration 59, loss = 0.0007652383064851165
iteration 60, loss = 0.000977129559032619
iteration 61, loss = 0.0007731814403086901
iteration 62, loss = 0.0009735604398883879
iteration 63, loss = 0.0012638892512768507
iteration 64, loss = 0.0007549596484750509
iteration 65, loss = 0.000980223761871457
iteration 66, loss = 0.0007436978630721569
iteration 67, loss = 0.0013957656919956207
iteration 68, loss = 0.0010774607071653008
iteration 69, loss = 0.0007461973582394421
iteration 70, loss = 0.0008337479666806757
iteration 71, loss = 0.0007150932215154171
iteration 72, loss = 0.0007813287666067481
iteration 73, loss = 0.0010660712141543627
iteration 74, loss = 0.0009575308067724109
iteration 75, loss = 0.0009258172358386219
iteration 76, loss = 0.0008825994445942342
iteration 77, loss = 0.0008989819325506687
iteration 78, loss = 0.001108856638893485
iteration 79, loss = 0.000976685550995171
iteration 80, loss = 0.0007634399808011949
iteration 81, loss = 0.0017003938555717468
iteration 82, loss = 0.0012510641245171428
iteration 83, loss = 0.0007626854348927736
iteration 84, loss = 0.0010664444416761398
iteration 85, loss = 0.0008753558504395187
iteration 86, loss = 0.001068787183612585
iteration 87, loss = 0.000790629826951772
iteration 88, loss = 0.0008039628155529499
iteration 89, loss = 0.0008232169784605503
iteration 90, loss = 0.0006655912729911506
iteration 91, loss = 0.0009379872935824096
iteration 92, loss = 0.0009104800992645323
iteration 93, loss = 0.0015638013137504458
iteration 94, loss = 0.0013054476585239172
iteration 95, loss = 0.00078112434130162
iteration 96, loss = 0.0008685009670443833
iteration 97, loss = 0.0016628761077299714
iteration 98, loss = 0.000960425240918994
iteration 99, loss = 0.0007700471323914826
iteration 100, loss = 0.0007736872066743672
iteration 101, loss = 0.001083328272216022
iteration 102, loss = 0.0008084274595603347
iteration 103, loss = 0.00078533252235502
iteration 104, loss = 0.0008755600429140031
iteration 105, loss = 0.00075651949737221
iteration 106, loss = 0.0009367726743221283
iteration 107, loss = 0.0009622530196793377
iteration 108, loss = 0.0014111434575170279
iteration 109, loss = 0.0014077003579586744
iteration 110, loss = 0.0007275635143741965
iteration 111, loss = 0.0008046573493629694
iteration 112, loss = 0.001821262063458562
iteration 113, loss = 0.0006990472902543843
iteration 114, loss = 0.0008167689666152
iteration 115, loss = 0.0009755294304341078
iteration 116, loss = 0.0009200283093377948
iteration 117, loss = 0.0015542153269052505
iteration 118, loss = 0.0010023793438449502
iteration 119, loss = 0.0008747168467380106
iteration 120, loss = 0.0009150021942332387
iteration 121, loss = 0.0011496101506054401
iteration 122, loss = 0.0010961003135889769
iteration 123, loss = 0.0008766315295360982
iteration 124, loss = 0.0008604609174653888
iteration 125, loss = 0.0008520326227881014
iteration 126, loss = 0.0008942964486777782
iteration 127, loss = 0.0008857370121404529
iteration 128, loss = 0.0008747449610382318
iteration 129, loss = 0.0009653818560764194
iteration 130, loss = 0.0007050511776469648
iteration 131, loss = 0.0009179231710731983
iteration 132, loss = 0.0012444633757695556
iteration 133, loss = 0.0006759015377610922
iteration 134, loss = 0.0013772633392363787
iteration 135, loss = 0.0008181509911082685
iteration 136, loss = 0.0006949006346985698
iteration 137, loss = 0.0007999005611054599
iteration 138, loss = 0.0008187443017959595
iteration 139, loss = 0.0006890202639624476
iteration 140, loss = 0.0010662823915481567
iteration 141, loss = 0.0007130062440410256
iteration 142, loss = 0.0008399708312936127
iteration 143, loss = 0.0012635990278795362
iteration 144, loss = 0.000757190806325525
iteration 145, loss = 0.0008002311224117875
iteration 146, loss = 0.0009845779277384281
iteration 147, loss = 0.0008143552695401013
iteration 148, loss = 0.0010469966800883412
iteration 149, loss = 0.0011238135630264878
iteration 150, loss = 0.0008686654618941247
iteration 151, loss = 0.0010099898790940642
iteration 152, loss = 0.0016114719910547137
iteration 153, loss = 0.0008099549449980259
iteration 154, loss = 0.0009384022559970617
iteration 155, loss = 0.0016956712352111936
iteration 156, loss = 0.0012000689748674631
iteration 157, loss = 0.0006883571622893214
iteration 158, loss = 0.0009795100195333362
iteration 159, loss = 0.0009002378792501986
iteration 160, loss = 0.0017702429322525859
iteration 161, loss = 0.0013560502557083964
iteration 162, loss = 0.001083102892152965
iteration 163, loss = 0.0009025941835716367
iteration 164, loss = 0.0008152836235240102
iteration 165, loss = 0.0011687522055581212
iteration 166, loss = 0.0010707286419346929
iteration 167, loss = 0.0010387786896899343
iteration 168, loss = 0.0007153885089792311
iteration 169, loss = 0.0010502778459340334
iteration 170, loss = 0.0007534614414907992
iteration 171, loss = 0.0009059770964086056
iteration 172, loss = 0.0019955227617174387
iteration 173, loss = 0.0007851008558645844
iteration 174, loss = 0.0011027571745216846
iteration 175, loss = 0.000760159338824451
iteration 176, loss = 0.0009396974928677082
iteration 177, loss = 0.0008272078703157604
iteration 178, loss = 0.0011076258961111307
iteration 179, loss = 0.0009279989753849804
iteration 180, loss = 0.0014742794446647167
iteration 181, loss = 0.0017076777294278145
iteration 182, loss = 0.001584322890266776
iteration 183, loss = 0.0008339471532963216
iteration 184, loss = 0.0007411964470520616
iteration 185, loss = 0.001669453689828515
iteration 186, loss = 0.0008275857544504106
iteration 187, loss = 0.0007150511955842376
iteration 188, loss = 0.0010225333971902728
iteration 189, loss = 0.0011982312425971031
iteration 190, loss = 0.0008489919127896428
iteration 191, loss = 0.0007474629674106836
iteration 192, loss = 0.000991634326055646
iteration 193, loss = 0.0008569429628551006
iteration 194, loss = 0.000828823889605701
iteration 195, loss = 0.0008217759896069765
iteration 196, loss = 0.0007979463553056121
iteration 197, loss = 0.0007833717972971499
iteration 198, loss = 0.0014059171080589294
iteration 199, loss = 0.0008121931459754705
iteration 200, loss = 0.0007212473428808153
iteration 201, loss = 0.0009931117529049516
iteration 202, loss = 0.000974625872913748
iteration 203, loss = 0.0008965451852418482
iteration 204, loss = 0.0007233473006635904
iteration 205, loss = 0.0015980965690687299
iteration 206, loss = 0.0008783147786743939
iteration 207, loss = 0.0007836338481865823
iteration 208, loss = 0.0008623399771749973
iteration 209, loss = 0.0009016733383759856
iteration 210, loss = 0.0010543505195528269
iteration 211, loss = 0.0008030141470953822
iteration 212, loss = 0.000874152930919081
iteration 213, loss = 0.0009244912653230131
iteration 214, loss = 0.0008626444614492357
iteration 215, loss = 0.0009714879561215639
iteration 216, loss = 0.0011279885657131672
iteration 217, loss = 0.0008386465488001704
iteration 218, loss = 0.0009451182559132576
iteration 219, loss = 0.0006793319480493665
iteration 220, loss = 0.0012208655243739486
iteration 221, loss = 0.0008456478244625032
iteration 222, loss = 0.0007444178336299956
iteration 223, loss = 0.0007321003940887749
iteration 224, loss = 0.0011004480766132474
iteration 225, loss = 0.0007997509674169123
iteration 226, loss = 0.0007486032554879785
iteration 227, loss = 0.0008463456761091948
iteration 228, loss = 0.0008221538155339658
iteration 229, loss = 0.0006964329513721168
iteration 230, loss = 0.0008548165205866098
iteration 231, loss = 0.0011421047383919358
iteration 232, loss = 0.0015308244619518518
iteration 233, loss = 0.0009458218119107187
iteration 234, loss = 0.0008097872487269342
iteration 235, loss = 0.0009986931690946221
iteration 236, loss = 0.0008777729235589504
iteration 237, loss = 0.0007538953796029091
iteration 238, loss = 0.0008847220451571047
iteration 239, loss = 0.0010328921489417553
iteration 240, loss = 0.0009438010747544467
iteration 241, loss = 0.0008829543949104846
iteration 242, loss = 0.0010566903511062264
iteration 243, loss = 0.0010495163733139634
iteration 244, loss = 0.0014904355630278587
iteration 245, loss = 0.0016661534318700433
iteration 246, loss = 0.0007039834163151681
iteration 247, loss = 0.0008073890930972993
iteration 248, loss = 0.0010173121700063348
iteration 249, loss = 0.0009723648545332253
iteration 250, loss = 0.0007184575079008937
iteration 251, loss = 0.0009519706945866346
iteration 252, loss = 0.0007452316931448877
iteration 253, loss = 0.0007852926501072943
iteration 254, loss = 0.0008916234946809709
iteration 255, loss = 0.0007644257275387645
iteration 256, loss = 0.0008054123027250171
iteration 257, loss = 0.0007559518562629819
iteration 258, loss = 0.0011680363677442074
iteration 259, loss = 0.0010809884406626225
iteration 260, loss = 0.0014409481082111597
iteration 261, loss = 0.0007505218964070082
iteration 262, loss = 0.0008438837248831987
iteration 263, loss = 0.0006754848291166127
iteration 264, loss = 0.0011793707963079214
iteration 265, loss = 0.0007314847316592932
iteration 266, loss = 0.0010758573189377785
iteration 267, loss = 0.0013568911235779524
iteration 268, loss = 0.0011090801563113928
iteration 269, loss = 0.0008363279048353434
iteration 270, loss = 0.0007900943164713681
iteration 271, loss = 0.0010957169579342008
iteration 272, loss = 0.0009230884024873376
iteration 273, loss = 0.0009640522766858339
iteration 274, loss = 0.0008469605818390846
iteration 275, loss = 0.0008442371618002653
iteration 276, loss = 0.0007712458609603345
iteration 277, loss = 0.0024354271590709686
iteration 278, loss = 0.001073184423148632
iteration 279, loss = 0.0010697278194129467
iteration 280, loss = 0.0010651105549186468
iteration 281, loss = 0.0009099738672375679
iteration 282, loss = 0.0009765559225343168
iteration 283, loss = 0.0008695772849023342
iteration 284, loss = 0.0010443044593557715
iteration 285, loss = 0.001039406517520547
iteration 286, loss = 0.0008503060089424253
iteration 287, loss = 0.0006389858317561448
iteration 288, loss = 0.0017664055339992046
iteration 289, loss = 0.0009210169082507491
iteration 290, loss = 0.0009711358579806983
iteration 291, loss = 0.0007535814656876028
iteration 292, loss = 0.0007322825840674341
iteration 293, loss = 0.001092582126148045
iteration 294, loss = 0.000999961281195283
iteration 295, loss = 0.0007922363583929837
iteration 296, loss = 0.0008097871323116124
iteration 297, loss = 0.000854803598485887
iteration 298, loss = 0.0008910385658964515
iteration 299, loss = 0.000962599297054112
iteration 300, loss = 0.0007361799944192171
iteration 1, loss = 0.0008396931807510555
iteration 2, loss = 0.0013839504681527615
iteration 3, loss = 0.0006740793469361961
iteration 4, loss = 0.0007918698829598725
iteration 5, loss = 0.0009947242215275764
iteration 6, loss = 0.0009602656937204301
iteration 7, loss = 0.0009581943158991635
iteration 8, loss = 0.0009404870797879994
iteration 9, loss = 0.0017307724338024855
iteration 10, loss = 0.0009019546560011804
iteration 11, loss = 0.0008802891243249178
iteration 12, loss = 0.0008015842759050429
iteration 13, loss = 0.0009721979149617255
iteration 14, loss = 0.0007516032201237977
iteration 15, loss = 0.0008866742718964815
iteration 16, loss = 0.0008486697915941477
iteration 17, loss = 0.0009702518000267446
iteration 18, loss = 0.0011523012071847916
iteration 19, loss = 0.0007055914611555636
iteration 20, loss = 0.0007930738502182066
iteration 21, loss = 0.0007363609620369971
iteration 22, loss = 0.0007935464382171631
iteration 23, loss = 0.0010987053392454982
iteration 24, loss = 0.0011291777482256293
iteration 25, loss = 0.0011269121896475554
iteration 26, loss = 0.0010271515930071473
iteration 27, loss = 0.0016063717193901539
iteration 28, loss = 0.001731419819407165
iteration 29, loss = 0.001651989878155291
iteration 30, loss = 0.000753304804675281
iteration 31, loss = 0.0010412397095933557
iteration 32, loss = 0.0007826935616321862
iteration 33, loss = 0.0008337779436260462
iteration 34, loss = 0.0008350714342668653
iteration 35, loss = 0.0009023627499118447
iteration 36, loss = 0.001544454018585384
iteration 37, loss = 0.0009015920222736895
iteration 38, loss = 0.000790690362919122
iteration 39, loss = 0.000752895197365433
iteration 40, loss = 0.001241138088516891
iteration 41, loss = 0.0012693323660641909
iteration 42, loss = 0.0009272340685129166
iteration 43, loss = 0.0014586634933948517
iteration 44, loss = 0.000796741631347686
iteration 45, loss = 0.0008505323203280568
iteration 46, loss = 0.0009352274937555194
iteration 47, loss = 0.0008307219832204282
iteration 48, loss = 0.0008936086669564247
iteration 49, loss = 0.0011902367696166039
iteration 50, loss = 0.000759791349992156
iteration 51, loss = 0.0011000181548297405
iteration 52, loss = 0.0007984352414496243
iteration 53, loss = 0.0015911832451820374
iteration 54, loss = 0.001082864124327898
iteration 55, loss = 0.0008205433841794729
iteration 56, loss = 0.0007263926090672612
iteration 57, loss = 0.0008644163608551025
iteration 58, loss = 0.0011340522905811667
iteration 59, loss = 0.0007605422288179398
iteration 60, loss = 0.0007739362772554159
iteration 61, loss = 0.0008662662003189325
iteration 62, loss = 0.0007585362181998789
iteration 63, loss = 0.0009245317196473479
iteration 64, loss = 0.0008796845795586705
iteration 65, loss = 0.0010971990413963795
iteration 66, loss = 0.000816820131149143
iteration 67, loss = 0.0007607046863995492
iteration 68, loss = 0.0012122791958972812
iteration 69, loss = 0.0012941500172019005
iteration 70, loss = 0.0007700729765929282
iteration 71, loss = 0.0008458090596832335
iteration 72, loss = 0.0008894504280760884
iteration 73, loss = 0.0011173245729878545
iteration 74, loss = 0.0006435158429667354
iteration 75, loss = 0.0008603436290286481
iteration 76, loss = 0.0009114383137784898
iteration 77, loss = 0.0007863831706345081
iteration 78, loss = 0.00100544816814363
iteration 79, loss = 0.0008788639679551125
iteration 80, loss = 0.0007609949680045247
iteration 81, loss = 0.0008544940501451492
iteration 82, loss = 0.0009184462251141667
iteration 83, loss = 0.0006863684393465519
iteration 84, loss = 0.000752504332922399
iteration 85, loss = 0.0007548914290964603
iteration 86, loss = 0.0008675111457705498
iteration 87, loss = 0.0007513361051678658
iteration 88, loss = 0.0011467629810795188
iteration 89, loss = 0.0010420596227049828
iteration 90, loss = 0.0007680206326767802
iteration 91, loss = 0.0007725581526756287
iteration 92, loss = 0.0007439012988470495
iteration 93, loss = 0.0009212149307131767
iteration 94, loss = 0.0007905121892690659
iteration 95, loss = 0.0009784097783267498
iteration 96, loss = 0.0009130917605943978
iteration 97, loss = 0.0008803375530987978
iteration 98, loss = 0.0009691528393886983
iteration 99, loss = 0.0006933726253919303
iteration 100, loss = 0.0007966168923303485
iteration 101, loss = 0.0009205590467900038
iteration 102, loss = 0.0009107713121920824
iteration 103, loss = 0.000841971137560904
iteration 104, loss = 0.0008606631890870631
iteration 105, loss = 0.0013196985237300396
iteration 106, loss = 0.0007820550817996264
iteration 107, loss = 0.0008769548730924726
iteration 108, loss = 0.0008502559503540397
iteration 109, loss = 0.0008290523546747863
iteration 110, loss = 0.0016362422611564398
iteration 111, loss = 0.0007081693038344383
iteration 112, loss = 0.000735991692636162
iteration 113, loss = 0.0006643242668360472
iteration 114, loss = 0.000970910768955946
iteration 115, loss = 0.0009305755374953151
iteration 116, loss = 0.0007888686377555132
iteration 117, loss = 0.0010378488805145025
iteration 118, loss = 0.0008857162320055068
iteration 119, loss = 0.000725945399608463
iteration 120, loss = 0.0007996096974238753
iteration 121, loss = 0.0016509428387507796
iteration 122, loss = 0.000830422155559063
iteration 123, loss = 0.0007888979162089527
iteration 124, loss = 0.0010090555297210813
iteration 125, loss = 0.001077743130736053
iteration 126, loss = 0.0017018696526065469
iteration 127, loss = 0.0006945905042812228
iteration 128, loss = 0.000872136268299073
iteration 129, loss = 0.0010169055312871933
iteration 130, loss = 0.000800935784354806
iteration 131, loss = 0.0008505110745318234
iteration 132, loss = 0.0010384601773694158
iteration 133, loss = 0.001002175617031753
iteration 134, loss = 0.0013717981055378914
iteration 135, loss = 0.0007201511180028319
iteration 136, loss = 0.0011742241913452744
iteration 137, loss = 0.0008964374428614974
iteration 138, loss = 0.0013257229002192616
iteration 139, loss = 0.0007511356379836798
iteration 140, loss = 0.0009285404812544584
iteration 141, loss = 0.0007174578495323658
iteration 142, loss = 0.0008563486044295132
iteration 143, loss = 0.0010420337785035372
iteration 144, loss = 0.0007818189333193004
iteration 145, loss = 0.0009687896817922592
iteration 146, loss = 0.0012264869874343276
iteration 147, loss = 0.0007640380645170808
iteration 148, loss = 0.000966097810305655
iteration 149, loss = 0.000699993222951889
iteration 150, loss = 0.0016024373471736908
iteration 151, loss = 0.0008003491093404591
iteration 152, loss = 0.0012290971353650093
iteration 153, loss = 0.0006890770746394992
iteration 154, loss = 0.0019350876100361347
iteration 155, loss = 0.001664225710555911
iteration 156, loss = 0.0010009696707129478
iteration 157, loss = 0.0011633782414719462
iteration 158, loss = 0.000881380692590028
iteration 159, loss = 0.000761276634875685
iteration 160, loss = 0.0007921830983832479
iteration 161, loss = 0.000743148208130151
iteration 162, loss = 0.0008267039665952325
iteration 163, loss = 0.0009985093493014574
iteration 164, loss = 0.001022323383949697
iteration 165, loss = 0.0017064586281776428
iteration 166, loss = 0.0007616259972564876
iteration 167, loss = 0.0009164409711956978
iteration 168, loss = 0.0009440926369279623
iteration 169, loss = 0.0008500801050104201
iteration 170, loss = 0.0013616925571113825
iteration 171, loss = 0.0009187565883621573
iteration 172, loss = 0.0008571604266762733
iteration 173, loss = 0.001259863143786788
iteration 174, loss = 0.000881104962900281
iteration 175, loss = 0.0006916028214618564
iteration 176, loss = 0.0007270857458934188
iteration 177, loss = 0.0011437794892117381
iteration 178, loss = 0.0012367789167910814
iteration 179, loss = 0.0008910073665902019
iteration 180, loss = 0.0008021770045161247
iteration 181, loss = 0.0008860037196427584
iteration 182, loss = 0.0009350969339720905
iteration 183, loss = 0.0007646973826922476
iteration 184, loss = 0.0008047360461205244
iteration 185, loss = 0.0010522428201511502
iteration 186, loss = 0.0007904661470092833
iteration 187, loss = 0.0007825774955563247
iteration 188, loss = 0.0011590664507821202
iteration 189, loss = 0.0016568668652325869
iteration 190, loss = 0.0009414345840923488
iteration 191, loss = 0.0007617624360136688
iteration 192, loss = 0.0008961502462625504
iteration 193, loss = 0.001106779440306127
iteration 194, loss = 0.0009712151950225234
iteration 195, loss = 0.0009114525746554136
iteration 196, loss = 0.0007656282978132367
iteration 197, loss = 0.0009170323610305786
iteration 198, loss = 0.000804378476459533
iteration 199, loss = 0.0008483791025355458
iteration 200, loss = 0.0008612837409600616
iteration 201, loss = 0.0015047105262055993
iteration 202, loss = 0.0010611088946461678
iteration 203, loss = 0.0010159622179344296
iteration 204, loss = 0.0009241883526556194
iteration 205, loss = 0.0008013984770514071
iteration 206, loss = 0.0009937547147274017
iteration 207, loss = 0.001090817735530436
iteration 208, loss = 0.0012071405071765184
iteration 209, loss = 0.0008354486199095845
iteration 210, loss = 0.0014317897148430347
iteration 211, loss = 0.0010491188149899244
iteration 212, loss = 0.0008581776055507362
iteration 213, loss = 0.001450181589461863
iteration 214, loss = 0.0010434520663693547
iteration 215, loss = 0.0008452195907011628
iteration 216, loss = 0.0010615012142807245
iteration 217, loss = 0.0007983653922565281
iteration 218, loss = 0.0010570683516561985
iteration 219, loss = 0.0010849973186850548
iteration 220, loss = 0.0008680028840899467
iteration 221, loss = 0.0008896918152458966
iteration 222, loss = 0.0006916960119269788
iteration 223, loss = 0.0011356365866959095
iteration 224, loss = 0.0008173856767825782
iteration 225, loss = 0.0014463370898738503
iteration 226, loss = 0.000835223647300154
iteration 227, loss = 0.00118244846817106
iteration 228, loss = 0.0007966593257151544
iteration 229, loss = 0.0011066761799156666
iteration 230, loss = 0.0014242037432268262
iteration 231, loss = 0.0006983036291785538
iteration 232, loss = 0.0014409644063562155
iteration 233, loss = 0.0007834871066734195
iteration 234, loss = 0.0010125224944204092
iteration 235, loss = 0.0009627007530070841
iteration 236, loss = 0.00143073417712003
iteration 237, loss = 0.0006697577773593366
iteration 238, loss = 0.0014192264061421156
iteration 239, loss = 0.0010234678629785776
iteration 240, loss = 0.0010500793578103185
iteration 241, loss = 0.0008866404532454908
iteration 242, loss = 0.000913901487365365
iteration 243, loss = 0.0013863419881090522
iteration 244, loss = 0.0014785965904593468
iteration 245, loss = 0.000973688205704093
iteration 246, loss = 0.0008344536763615906
iteration 247, loss = 0.0007795327110216022
iteration 248, loss = 0.0008379638893529773
iteration 249, loss = 0.0008484766585752368
iteration 250, loss = 0.0011070743203163147
iteration 251, loss = 0.0014819131465628743
iteration 252, loss = 0.0006797092501074076
iteration 253, loss = 0.0008956580422818661
iteration 254, loss = 0.0007532257586717606
iteration 255, loss = 0.0008690471877343953
iteration 256, loss = 0.0008113873773254454
iteration 257, loss = 0.0012854208471253514
iteration 258, loss = 0.0007405910291709006
iteration 259, loss = 0.0011135770473629236
iteration 260, loss = 0.001032783417031169
iteration 261, loss = 0.0012309063458815217
iteration 262, loss = 0.0008173425449058414
iteration 263, loss = 0.0007492765435017645
iteration 264, loss = 0.0009248659480363131
iteration 265, loss = 0.0015977310249581933
iteration 266, loss = 0.0008068058523349464
iteration 267, loss = 0.0008120713173411787
iteration 268, loss = 0.0007778477156534791
iteration 269, loss = 0.0006635076133534312
iteration 270, loss = 0.0012081231689080596
iteration 271, loss = 0.0008719575707800686
iteration 272, loss = 0.0007875016308389604
iteration 273, loss = 0.0008222981123253703
iteration 274, loss = 0.001097315107472241
iteration 275, loss = 0.0016609878512099385
iteration 276, loss = 0.001442480948753655
iteration 277, loss = 0.0022866595536470413
iteration 278, loss = 0.0006982259801588953
iteration 279, loss = 0.0008960184641182423
iteration 280, loss = 0.0007048856350593269
iteration 281, loss = 0.0009846477769315243
iteration 282, loss = 0.0006859952700324357
iteration 283, loss = 0.0013907693792134523
iteration 284, loss = 0.0011146941687911749
iteration 285, loss = 0.001038735150359571
iteration 286, loss = 0.0011420603841543198
iteration 287, loss = 0.000709347368683666
iteration 288, loss = 0.0012734754709526896
iteration 289, loss = 0.0008987823966890574
iteration 290, loss = 0.001056175446137786
iteration 291, loss = 0.0007183265988714993
iteration 292, loss = 0.0009331497712992132
iteration 293, loss = 0.0018549810629338026
iteration 294, loss = 0.001199619728140533
iteration 295, loss = 0.0009517129510641098
iteration 296, loss = 0.0006933199474588037
iteration 297, loss = 0.0015354175120592117
iteration 298, loss = 0.000936329597607255
iteration 299, loss = 0.0010086933616548777
iteration 300, loss = 0.0009949745144695044
iteration 1, loss = 0.0009217970655299723
iteration 2, loss = 0.0006812845822423697
iteration 3, loss = 0.0008631142554804683
iteration 4, loss = 0.0013127661077305675
iteration 5, loss = 0.0008448638254776597
iteration 6, loss = 0.0010801565367728472
iteration 7, loss = 0.0012574517168104649
iteration 8, loss = 0.0009439362911507487
iteration 9, loss = 0.001413908088579774
iteration 10, loss = 0.0009973611449822783
iteration 11, loss = 0.0007215237710624933
iteration 12, loss = 0.0007238125544972718
iteration 13, loss = 0.000896114856004715
iteration 14, loss = 0.000767163117416203
iteration 15, loss = 0.0008050830219872296
iteration 16, loss = 0.0009919190779328346
iteration 17, loss = 0.0008557951077818871
iteration 18, loss = 0.0008162727463059127
iteration 19, loss = 0.0011116282548755407
iteration 20, loss = 0.0008021134417504072
iteration 21, loss = 0.0015365747967734933
iteration 22, loss = 0.0007701521972194314
iteration 23, loss = 0.0007162338006310165
iteration 24, loss = 0.0007765319896861911
iteration 25, loss = 0.0008553731604479253
iteration 26, loss = 0.0014418858336284757
iteration 27, loss = 0.0008171306108124554
iteration 28, loss = 0.0008376979967579246
iteration 29, loss = 0.0009563479688949883
iteration 30, loss = 0.0016130520962178707
iteration 31, loss = 0.0009328980813734233
iteration 32, loss = 0.0007112135645002127
iteration 33, loss = 0.0014636882115155458
iteration 34, loss = 0.001088691409677267
iteration 35, loss = 0.0015912165399640799
iteration 36, loss = 0.0007606097497045994
iteration 37, loss = 0.0009030457003973424
iteration 38, loss = 0.0009560426697134972
iteration 39, loss = 0.0006667794077657163
iteration 40, loss = 0.0007470688433386385
iteration 41, loss = 0.0007949416758492589
iteration 42, loss = 0.0008337213657796383
iteration 43, loss = 0.0009955452987924218
iteration 44, loss = 0.0008564320160076022
iteration 45, loss = 0.0008496482623741031
iteration 46, loss = 0.001560916774906218
iteration 47, loss = 0.000730280065909028
iteration 48, loss = 0.00106428109575063
iteration 49, loss = 0.001634868560358882
iteration 50, loss = 0.0007874128641560674
iteration 51, loss = 0.0008339074556715786
iteration 52, loss = 0.0012948373332619667
iteration 53, loss = 0.0008317792089655995
iteration 54, loss = 0.0006884825997985899
iteration 55, loss = 0.0010195113718509674
iteration 56, loss = 0.000826111645437777
iteration 57, loss = 0.0007986429845914245
iteration 58, loss = 0.0007105100667104125
iteration 59, loss = 0.0008078570244833827
iteration 60, loss = 0.0008567031472921371
iteration 61, loss = 0.0012866738252341747
iteration 62, loss = 0.0008274151477962732
iteration 63, loss = 0.0008424042025581002
iteration 64, loss = 0.0008369719726033509
iteration 65, loss = 0.001169833936728537
iteration 66, loss = 0.0011169890640303493
iteration 67, loss = 0.0015031085349619389
iteration 68, loss = 0.0009207555558532476
iteration 69, loss = 0.000806823605671525
iteration 70, loss = 0.0009053570684045553
iteration 71, loss = 0.0010812066029757261
iteration 72, loss = 0.0006833836087025702
iteration 73, loss = 0.0010082372464239597
iteration 74, loss = 0.0008488221210427582
iteration 75, loss = 0.0008601301815360785
iteration 76, loss = 0.0008949514594860375
iteration 77, loss = 0.0009777003433555365
iteration 78, loss = 0.0008537006797268987
iteration 79, loss = 0.0006966542568989098
iteration 80, loss = 0.0009435202809982002
iteration 81, loss = 0.0008413889445364475
iteration 82, loss = 0.0006714680348522961
iteration 83, loss = 0.0008144323946908116
iteration 84, loss = 0.0007592281326651573
iteration 85, loss = 0.0008244706550613046
iteration 86, loss = 0.0006799857947044075
iteration 87, loss = 0.0008804452372714877
iteration 88, loss = 0.0006978496676310897
iteration 89, loss = 0.0011201271554455161
iteration 90, loss = 0.0008874740451574326
iteration 91, loss = 0.0011426634155213833
iteration 92, loss = 0.0008329956908710301
iteration 93, loss = 0.0008213208056986332
iteration 94, loss = 0.0012081362074241042
iteration 95, loss = 0.002063069026917219
iteration 96, loss = 0.0012335871579125524
iteration 97, loss = 0.0013080706121399999
iteration 98, loss = 0.0012401932617649436
iteration 99, loss = 0.0011472604237496853
iteration 100, loss = 0.0007997255306690931
iteration 101, loss = 0.0008596472325734794
iteration 102, loss = 0.0008749293629080057
iteration 103, loss = 0.0007315322291105986
iteration 104, loss = 0.0015354118077084422
iteration 105, loss = 0.0016430944669991732
iteration 106, loss = 0.0007147802389226854
iteration 107, loss = 0.0016300261486321688
iteration 108, loss = 0.0009450940997339785
iteration 109, loss = 0.0007322998717427254
iteration 110, loss = 0.0006451407680287957
iteration 111, loss = 0.000932457740418613
iteration 112, loss = 0.0010334987891837955
iteration 113, loss = 0.000902304716873914
iteration 114, loss = 0.001033699605613947
iteration 115, loss = 0.0016484217485412955
iteration 116, loss = 0.000964112114161253
iteration 117, loss = 0.0009200837230309844
iteration 118, loss = 0.0007737199193798006
iteration 119, loss = 0.0007988674333319068
iteration 120, loss = 0.0009166175732389092
iteration 121, loss = 0.0006920159212313592
iteration 122, loss = 0.0016843971097841859
iteration 123, loss = 0.0008692728588357568
iteration 124, loss = 0.0009801299311220646
iteration 125, loss = 0.0011551502393558621
iteration 126, loss = 0.0011941810371354222
iteration 127, loss = 0.0006858819397166371
iteration 128, loss = 0.0011790578719228506
iteration 129, loss = 0.002141891047358513
iteration 130, loss = 0.0008325725211761892
iteration 131, loss = 0.0010513411834836006
iteration 132, loss = 0.0009228899143636227
iteration 133, loss = 0.0007981026428751647
iteration 134, loss = 0.0009887886699289083
iteration 135, loss = 0.001021660165861249
iteration 136, loss = 0.0007263185689225793
iteration 137, loss = 0.0008855200139805675
iteration 138, loss = 0.001100182649679482
iteration 139, loss = 0.0011566998437047005
iteration 140, loss = 0.0008597533451393247
iteration 141, loss = 0.0008393415482714772
iteration 142, loss = 0.0009597140597179532
iteration 143, loss = 0.0008398880017921329
iteration 144, loss = 0.001567010534927249
iteration 145, loss = 0.000990169239230454
iteration 146, loss = 0.0009030259680002928
iteration 147, loss = 0.0009228531853295863
iteration 148, loss = 0.0011169236386194825
iteration 149, loss = 0.0008419579826295376
iteration 150, loss = 0.0016832239925861359
iteration 151, loss = 0.0009295906638726592
iteration 152, loss = 0.0010294318199157715
iteration 153, loss = 0.0015547366347163916
iteration 154, loss = 0.0008058531093411148
iteration 155, loss = 0.000798112596385181
iteration 156, loss = 0.0008092477801255882
iteration 157, loss = 0.0007975563639774919
iteration 158, loss = 0.0007364219636656344
iteration 159, loss = 0.0008658070582896471
iteration 160, loss = 0.0015355650102719665
iteration 161, loss = 0.0008635675185360014
iteration 162, loss = 0.0010505631798878312
iteration 163, loss = 0.0013510312419384718
iteration 164, loss = 0.0009466848568990827
iteration 165, loss = 0.0009619747870601714
iteration 166, loss = 0.0010324560571461916
iteration 167, loss = 0.0009912902023643255
iteration 168, loss = 0.0009696022607386112
iteration 169, loss = 0.0014974683290347457
iteration 170, loss = 0.0008653554832562804
iteration 171, loss = 0.0017589048948138952
iteration 172, loss = 0.0006851172656752169
iteration 173, loss = 0.0008273230050690472
iteration 174, loss = 0.0007672719075344503
iteration 175, loss = 0.0008573284139856696
iteration 176, loss = 0.000804280280135572
iteration 177, loss = 0.0009248824790120125
iteration 178, loss = 0.0006979307509027421
iteration 179, loss = 0.0009413360967300832
iteration 180, loss = 0.000776000611949712
iteration 181, loss = 0.0008623676840215921
iteration 182, loss = 0.0012023710878565907
iteration 183, loss = 0.0007847812958061695
iteration 184, loss = 0.0015061937738209963
iteration 185, loss = 0.0008643999462947249
iteration 186, loss = 0.0007023366051726043
iteration 187, loss = 0.001113907084800303
iteration 188, loss = 0.0008497473318129778
iteration 189, loss = 0.0012194188311696053
iteration 190, loss = 0.000717621820513159
iteration 191, loss = 0.0008039106614887714
iteration 192, loss = 0.0009689863654784858
iteration 193, loss = 0.0013401497853919864
iteration 194, loss = 0.0007791370153427124
iteration 195, loss = 0.0009825570741668344
iteration 196, loss = 0.000852432451210916
iteration 197, loss = 0.0008041143883019686
iteration 198, loss = 0.0014627232449129224
iteration 199, loss = 0.0011618901044130325
iteration 200, loss = 0.0014048302546143532
iteration 201, loss = 0.0008450991008430719
iteration 202, loss = 0.0008000684319995344
iteration 203, loss = 0.0010864755604416132
iteration 204, loss = 0.001587867853231728
iteration 205, loss = 0.0011172000085934997
iteration 206, loss = 0.0009814605582505465
iteration 207, loss = 0.001521827420219779
iteration 208, loss = 0.0007582387188449502
iteration 209, loss = 0.0008924764115363359
iteration 210, loss = 0.0006965318461880088
iteration 211, loss = 0.0008773174486123025
iteration 212, loss = 0.0008309356053359807
iteration 213, loss = 0.0012216110480949283
iteration 214, loss = 0.0008080218685790896
iteration 215, loss = 0.0009713440667837858
iteration 216, loss = 0.0012858699774369597
iteration 217, loss = 0.0010608882876113057
iteration 218, loss = 0.0007508567068725824
iteration 219, loss = 0.0010659450199455023
iteration 220, loss = 0.0007585295243188739
iteration 221, loss = 0.0014892435865476727
iteration 222, loss = 0.0013823581393808126
iteration 223, loss = 0.0009557451703585684
iteration 224, loss = 0.0007353624678216875
iteration 225, loss = 0.0007475978345610201
iteration 226, loss = 0.0010786204366013408
iteration 227, loss = 0.0007113550091162324
iteration 228, loss = 0.0007174300844781101
iteration 229, loss = 0.0009204194066114724
iteration 230, loss = 0.0010033495491370559
iteration 231, loss = 0.001199499354697764
iteration 232, loss = 0.0006774197681806982
iteration 233, loss = 0.0009628330008126795
iteration 234, loss = 0.0011087340535596013
iteration 235, loss = 0.001303714932873845
iteration 236, loss = 0.0015306791756302118
iteration 237, loss = 0.0010297016706317663
iteration 238, loss = 0.0009502135799266398
iteration 239, loss = 0.0007679217378608882
iteration 240, loss = 0.0007954727043397725
iteration 241, loss = 0.0008077456150203943
iteration 242, loss = 0.0009886579355224967
iteration 243, loss = 0.0008604891481809318
iteration 244, loss = 0.0011203046888113022
iteration 245, loss = 0.0008670384995639324
iteration 246, loss = 0.0006923266919329762
iteration 247, loss = 0.0009106482611969113
iteration 248, loss = 0.001624757656827569
iteration 249, loss = 0.0008523240685462952
iteration 250, loss = 0.0010482043726369739
iteration 251, loss = 0.0015066814376041293
iteration 252, loss = 0.0009368828614242375
iteration 253, loss = 0.0007201836560852826
iteration 254, loss = 0.0011949862819164991
iteration 255, loss = 0.0008475364302285016
iteration 256, loss = 0.0008113610092550516
iteration 257, loss = 0.0008248317753896117
iteration 258, loss = 0.0011600785655900836
iteration 259, loss = 0.0008304411312565207
iteration 260, loss = 0.0009608032996766269
iteration 261, loss = 0.0011000767117366195
iteration 262, loss = 0.0012426248285919428
iteration 263, loss = 0.000959287048317492
iteration 264, loss = 0.0011581603903323412
iteration 265, loss = 0.0008185264887288213
iteration 266, loss = 0.0012223729863762856
iteration 267, loss = 0.0008365721441805363
iteration 268, loss = 0.0008289992692880332
iteration 269, loss = 0.0010131789604201913
iteration 270, loss = 0.0011244360357522964
iteration 271, loss = 0.0008441844838671386
iteration 272, loss = 0.001161242020316422
iteration 273, loss = 0.0009087873040698469
iteration 274, loss = 0.000826922943815589
iteration 275, loss = 0.000855632999446243
iteration 276, loss = 0.000984572572633624
iteration 277, loss = 0.0008812403539195657
iteration 278, loss = 0.0014089879114180803
iteration 279, loss = 0.0008324749651364982
iteration 280, loss = 0.00087341497419402
iteration 281, loss = 0.0007917251205071807
iteration 282, loss = 0.0008118787663988769
iteration 283, loss = 0.0007456925231963396
iteration 284, loss = 0.0009560438338667154
iteration 285, loss = 0.0010161828249692917
iteration 286, loss = 0.0007672908250242472
iteration 287, loss = 0.0007171627366915345
iteration 288, loss = 0.0015402499120682478
iteration 289, loss = 0.0007942420779727399
iteration 290, loss = 0.0008046844741329551
iteration 291, loss = 0.0007348915678448975
iteration 292, loss = 0.001145568792708218
iteration 293, loss = 0.0007569859735667706
iteration 294, loss = 0.0007814044365659356
iteration 295, loss = 0.0007290290086530149
iteration 296, loss = 0.0007700514397583902
iteration 297, loss = 0.0008119707927107811
iteration 298, loss = 0.0009203061927109957
iteration 299, loss = 0.0009518118458800018
iteration 300, loss = 0.001097300322726369
iteration 1, loss = 0.000774113112129271
iteration 2, loss = 0.0006998319877311587
iteration 3, loss = 0.0014604341704398394
iteration 4, loss = 0.0011614259565249085
iteration 5, loss = 0.0008322349167428911
iteration 6, loss = 0.0009208535775542259
iteration 7, loss = 0.0009949803352355957
iteration 8, loss = 0.0011295612202957273
iteration 9, loss = 0.001078128581866622
iteration 10, loss = 0.000782858463935554
iteration 11, loss = 0.0006925769266672432
iteration 12, loss = 0.0008033235790207982
iteration 13, loss = 0.0015278129139915109
iteration 14, loss = 0.000828045594971627
iteration 15, loss = 0.0008823316893540323
iteration 16, loss = 0.0007526554982177913
iteration 17, loss = 0.000967443163972348
iteration 18, loss = 0.0011312179267406464
iteration 19, loss = 0.0007448746473528445
iteration 20, loss = 0.0009695551125332713
iteration 21, loss = 0.0013070943532511592
iteration 22, loss = 0.0006955756689421833
iteration 23, loss = 0.0011738694738596678
iteration 24, loss = 0.0009476676932536066
iteration 25, loss = 0.0011571303475648165
iteration 26, loss = 0.000739672570489347
iteration 27, loss = 0.0007332158857025206
iteration 28, loss = 0.0008969252812676132
iteration 29, loss = 0.000907739857211709
iteration 30, loss = 0.001393597573041916
iteration 31, loss = 0.0008371218573302031
iteration 32, loss = 0.0008644990739412606
iteration 33, loss = 0.0008535989327356219
iteration 34, loss = 0.0009584831423126161
iteration 35, loss = 0.0010499617783352733
iteration 36, loss = 0.0008442711550742388
iteration 37, loss = 0.0017831921577453613
iteration 38, loss = 0.0008982984581962228
iteration 39, loss = 0.0008667286019772291
iteration 40, loss = 0.0009629511623643339
iteration 41, loss = 0.00071927928365767
iteration 42, loss = 0.0017018032958731055
iteration 43, loss = 0.001557239331305027
iteration 44, loss = 0.0009708796860650182
iteration 45, loss = 0.000881870393641293
iteration 46, loss = 0.0007415091968141496
iteration 47, loss = 0.0009791335323825479
iteration 48, loss = 0.000900301500223577
iteration 49, loss = 0.000938858836889267
iteration 50, loss = 0.0008750209235586226
iteration 51, loss = 0.0006805035518482327
iteration 52, loss = 0.0011063354322686791
iteration 53, loss = 0.000783828436397016
iteration 54, loss = 0.0007767672068439424
iteration 55, loss = 0.001146900118328631
iteration 56, loss = 0.0011871774913743138
iteration 57, loss = 0.0009024420287460089
iteration 58, loss = 0.001019761897623539
iteration 59, loss = 0.0010804038029164076
iteration 60, loss = 0.0007323907339014113
iteration 61, loss = 0.0007647676975466311
iteration 62, loss = 0.0009161029011011124
iteration 63, loss = 0.0008332594879902899
iteration 64, loss = 0.0010681884596124291
iteration 65, loss = 0.0006841375725343823
iteration 66, loss = 0.0011846361448988318
iteration 67, loss = 0.001325500081293285
iteration 68, loss = 0.000801024551037699
iteration 69, loss = 0.0010617633815854788
iteration 70, loss = 0.0006500807357951999
iteration 71, loss = 0.0006925256457179785
iteration 72, loss = 0.0011822532396763563
iteration 73, loss = 0.0008938086684793234
iteration 74, loss = 0.0007192674675025046
iteration 75, loss = 0.0009938272414729
iteration 76, loss = 0.0007782758912071586
iteration 77, loss = 0.0009179087355732918
iteration 78, loss = 0.0006536213913932443
iteration 79, loss = 0.0008070546318776906
iteration 80, loss = 0.0011198539286851883
iteration 81, loss = 0.0009486644994467497
iteration 82, loss = 0.0009211050346493721
iteration 83, loss = 0.0008306055096909404
iteration 84, loss = 0.0008616030681878328
iteration 85, loss = 0.001019471907056868
iteration 86, loss = 0.0008203568868339062
iteration 87, loss = 0.0011141891591250896
iteration 88, loss = 0.0008767348481342196
iteration 89, loss = 0.0008578775450587273
iteration 90, loss = 0.0009273749310523272
iteration 91, loss = 0.0007214913493953645
iteration 92, loss = 0.0009008896304294467
iteration 93, loss = 0.0013744033640250564
iteration 94, loss = 0.001439488842152059
iteration 95, loss = 0.0007972473977133632
iteration 96, loss = 0.0006687059067189693
iteration 97, loss = 0.0008223768090829253
iteration 98, loss = 0.0006987061933614314
iteration 99, loss = 0.0015872003277763724
iteration 100, loss = 0.0007887135143391788
iteration 101, loss = 0.0009984112111851573
iteration 102, loss = 0.0008669288363307714
iteration 103, loss = 0.000886482943315059
iteration 104, loss = 0.0011308630928397179
iteration 105, loss = 0.0016156499041244388
iteration 106, loss = 0.000937082280870527
iteration 107, loss = 0.0011642116587609053
iteration 108, loss = 0.0007943310774862766
iteration 109, loss = 0.0015384957659989595
iteration 110, loss = 0.001159350504167378
iteration 111, loss = 0.0006587067618966103
iteration 112, loss = 0.0009034179383888841
iteration 113, loss = 0.0013502248330041766
iteration 114, loss = 0.0008936628000810742
iteration 115, loss = 0.0010156683856621385
iteration 116, loss = 0.002332452218979597
iteration 117, loss = 0.000743538374081254
iteration 118, loss = 0.0013774838298559189
iteration 119, loss = 0.00082957319682464
iteration 120, loss = 0.0010238263057544827
iteration 121, loss = 0.0009231818257831037
iteration 122, loss = 0.0009915835689753294
iteration 123, loss = 0.0008027678122743964
iteration 124, loss = 0.0011878249933943152
iteration 125, loss = 0.0008281416958197951
iteration 126, loss = 0.0012288137804716825
iteration 127, loss = 0.0008072962518781424
iteration 128, loss = 0.0007691152859479189
iteration 129, loss = 0.0009097695583477616
iteration 130, loss = 0.00105828489176929
iteration 131, loss = 0.0009885572362691164
iteration 132, loss = 0.0012700553052127361
iteration 133, loss = 0.0008206127095036209
iteration 134, loss = 0.0008074307115748525
iteration 135, loss = 0.0008170428918674588
iteration 136, loss = 0.0013037101598456502
iteration 137, loss = 0.0008062461856752634
iteration 138, loss = 0.0014001395320519805
iteration 139, loss = 0.001070444006472826
iteration 140, loss = 0.0007250804337672889
iteration 141, loss = 0.0012045912444591522
iteration 142, loss = 0.0008933353237807751
iteration 143, loss = 0.0012354257050901651
iteration 144, loss = 0.0008799543720670044
iteration 145, loss = 0.0014179451391100883
iteration 146, loss = 0.0009464260656386614
iteration 147, loss = 0.0007337025017477572
iteration 148, loss = 0.0007387083023786545
iteration 149, loss = 0.0008230754756368697
iteration 150, loss = 0.0008833752945065498
iteration 151, loss = 0.0008741781930439174
iteration 152, loss = 0.0007195562357082963
iteration 153, loss = 0.0007777437567710876
iteration 154, loss = 0.0018425167072564363
iteration 155, loss = 0.001727339462377131
iteration 156, loss = 0.0007860523764975369
iteration 157, loss = 0.001169312046840787
iteration 158, loss = 0.0009297312353737652
iteration 159, loss = 0.0008669428643770516
iteration 160, loss = 0.0008488562889397144
iteration 161, loss = 0.0007059757481329143
iteration 162, loss = 0.0008199114818125963
iteration 163, loss = 0.001157042570412159
iteration 164, loss = 0.0013622588012367487
iteration 165, loss = 0.0008033399935811758
iteration 166, loss = 0.0015103354817256331
iteration 167, loss = 0.0007505339453928173
iteration 168, loss = 0.0010186205618083477
iteration 169, loss = 0.0009755475912243128
iteration 170, loss = 0.0009672429878264666
iteration 171, loss = 0.0006935919518582523
iteration 172, loss = 0.0017668207874521613
iteration 173, loss = 0.001173456315882504
iteration 174, loss = 0.0010765653569251299
iteration 175, loss = 0.0008134128875099123
iteration 176, loss = 0.0008034725324250758
iteration 177, loss = 0.0008834234904497862
iteration 178, loss = 0.0009630133863538504
iteration 179, loss = 0.0014603635063394904
iteration 180, loss = 0.0008032690966501832
iteration 181, loss = 0.0008934244979172945
iteration 182, loss = 0.0007591877947561443
iteration 183, loss = 0.0007255756063386798
iteration 184, loss = 0.0007618154631927609
iteration 185, loss = 0.0007861607591621578
iteration 186, loss = 0.0006674036849290133
iteration 187, loss = 0.0010510035790503025
iteration 188, loss = 0.0010421022307127714
iteration 189, loss = 0.0007958593196235597
iteration 190, loss = 0.0010436790762469172
iteration 191, loss = 0.0009881219593808055
iteration 192, loss = 0.0006847348413430154
iteration 193, loss = 0.0009355715592391789
iteration 194, loss = 0.000833551399409771
iteration 195, loss = 0.0010538739152252674
iteration 196, loss = 0.0007641349802725017
iteration 197, loss = 0.00082737160846591
iteration 198, loss = 0.0008537827525287867
iteration 199, loss = 0.0007592971669510007
iteration 200, loss = 0.0006640968495048583
iteration 201, loss = 0.0009624296217225492
iteration 202, loss = 0.0007697420660406351
iteration 203, loss = 0.0008693134295754135
iteration 204, loss = 0.0007099687936715782
iteration 205, loss = 0.0009592033457010984
iteration 206, loss = 0.000644534535240382
iteration 207, loss = 0.0008138585253618658
iteration 208, loss = 0.0009309106972068548
iteration 209, loss = 0.000808293407317251
iteration 210, loss = 0.0009804739383980632
iteration 211, loss = 0.0010011607082560658
iteration 212, loss = 0.0014717062003910542
iteration 213, loss = 0.000891139090526849
iteration 214, loss = 0.001203415566124022
iteration 215, loss = 0.0009235010948032141
iteration 216, loss = 0.0021078409627079964
iteration 217, loss = 0.0008791194413788617
iteration 218, loss = 0.0019472872372716665
iteration 219, loss = 0.0008064554422162473
iteration 220, loss = 0.001963459188118577
iteration 221, loss = 0.0009046471095643938
iteration 222, loss = 0.0009388693142682314
iteration 223, loss = 0.0008956105448305607
iteration 224, loss = 0.0008385780965909362
iteration 225, loss = 0.000824387592729181
iteration 226, loss = 0.0008894543279893696
iteration 227, loss = 0.0008493607165291905
iteration 228, loss = 0.0008972812793217599
iteration 229, loss = 0.0009510479867458344
iteration 230, loss = 0.0008368298877030611
iteration 231, loss = 0.0007629088941030204
iteration 232, loss = 0.0007993067265488207
iteration 233, loss = 0.0012002922594547272
iteration 234, loss = 0.00072111637564376
iteration 235, loss = 0.0008100487175397575
iteration 236, loss = 0.0018944662297144532
iteration 237, loss = 0.001090501551516354
iteration 238, loss = 0.0016139696817845106
iteration 239, loss = 0.0017026327550411224
iteration 240, loss = 0.0009930252563208342
iteration 241, loss = 0.0008868931909091771
iteration 242, loss = 0.0009526610374450684
iteration 243, loss = 0.0011249214876443148
iteration 244, loss = 0.0007999761146493256
iteration 245, loss = 0.0008348144474439323
iteration 246, loss = 0.0009051120141521096
iteration 247, loss = 0.0017244801856577396
iteration 248, loss = 0.0014653594698756933
iteration 249, loss = 0.0007948356214910746
iteration 250, loss = 0.001036959933117032
iteration 251, loss = 0.0008758229087106884
iteration 252, loss = 0.0007344266632571816
iteration 253, loss = 0.000924177875276655
iteration 254, loss = 0.0009612884023226798
iteration 255, loss = 0.00081549835158512
iteration 256, loss = 0.0009540088358335197
iteration 257, loss = 0.0008731856942176819
iteration 258, loss = 0.0006437022238969803
iteration 259, loss = 0.0010143513791263103
iteration 260, loss = 0.0008374562603421509
iteration 261, loss = 0.0006506611243821681
iteration 262, loss = 0.0013931056018918753
iteration 263, loss = 0.001073568477295339
iteration 264, loss = 0.0009465129114687443
iteration 265, loss = 0.0013971106382086873
iteration 266, loss = 0.0006963672349229455
iteration 267, loss = 0.0007740795845165849
iteration 268, loss = 0.0009624738595448434
iteration 269, loss = 0.0007426930824294686
iteration 270, loss = 0.0008443602127954364
iteration 271, loss = 0.0008130072965286672
iteration 272, loss = 0.0017840618966147304
iteration 273, loss = 0.0008017841028049588
iteration 274, loss = 0.0008052430930547416
iteration 275, loss = 0.0013305433094501495
iteration 276, loss = 0.0011378891067579389
iteration 277, loss = 0.0009585790685378015
iteration 278, loss = 0.0010907136602327228
iteration 279, loss = 0.0008515487425029278
iteration 280, loss = 0.000987980980426073
iteration 281, loss = 0.0009211204014718533
iteration 282, loss = 0.0008947481401264668
iteration 283, loss = 0.001379413646645844
iteration 284, loss = 0.0007879682816565037
iteration 285, loss = 0.001061401329934597
iteration 286, loss = 0.0009261257946491241
iteration 287, loss = 0.0006770833861082792
iteration 288, loss = 0.00115290901158005
iteration 289, loss = 0.0009437750559300184
iteration 290, loss = 0.0009289806475862861
iteration 291, loss = 0.0009559922036714852
iteration 292, loss = 0.0008749213302507997
iteration 293, loss = 0.0007959851645864546
iteration 294, loss = 0.0008155073737725616
iteration 295, loss = 0.001192749012261629
iteration 296, loss = 0.000813043094240129
iteration 297, loss = 0.0010475136805325747
iteration 298, loss = 0.0010400512255728245
iteration 299, loss = 0.000889491057023406
iteration 300, loss = 0.0008345337118953466
iteration 1, loss = 0.0016597490757703781
iteration 2, loss = 0.0007596781942993402
iteration 3, loss = 0.0007747712661512196
iteration 4, loss = 0.0007128511788323522
iteration 5, loss = 0.0010471682762727141
iteration 6, loss = 0.0011424458352848887
iteration 7, loss = 0.00084064737893641
iteration 8, loss = 0.0007958593778312206
iteration 9, loss = 0.0009891776135191321
iteration 10, loss = 0.0011036223731935024
iteration 11, loss = 0.0007869335822761059
iteration 12, loss = 0.0009040017030201852
iteration 13, loss = 0.0016929287230595946
iteration 14, loss = 0.000826024217531085
iteration 15, loss = 0.0007917812326923013
iteration 16, loss = 0.0008438171353191137
iteration 17, loss = 0.0008985264576040208
iteration 18, loss = 0.000845465692691505
iteration 19, loss = 0.0007162493420764804
iteration 20, loss = 0.000995326554402709
iteration 21, loss = 0.001316496985964477
iteration 22, loss = 0.0007433096179738641
iteration 23, loss = 0.0013971892185509205
iteration 24, loss = 0.0007815465796738863
iteration 25, loss = 0.001892482745461166
iteration 26, loss = 0.0008930694893933833
iteration 27, loss = 0.0010096640326082706
iteration 28, loss = 0.000795122585259378
iteration 29, loss = 0.001599988085217774
iteration 30, loss = 0.0013517665211111307
iteration 31, loss = 0.001073602121323347
iteration 32, loss = 0.0008423449471592903
iteration 33, loss = 0.0006965998909436166
iteration 34, loss = 0.0007983396062627435
iteration 35, loss = 0.0007424418581649661
iteration 36, loss = 0.0008880872628651559
iteration 37, loss = 0.0006766353617422283
iteration 38, loss = 0.0008251601830124855
iteration 39, loss = 0.0008282317430712283
iteration 40, loss = 0.0014245955972000957
iteration 41, loss = 0.0007292421068996191
iteration 42, loss = 0.0008015150087885559
iteration 43, loss = 0.0008187001221813262
iteration 44, loss = 0.0007658503018319607
iteration 45, loss = 0.0009643135708756745
iteration 46, loss = 0.0009005569154396653
iteration 47, loss = 0.0008663444896228611
iteration 48, loss = 0.0016032143030315638
iteration 49, loss = 0.0008805549005046487
iteration 50, loss = 0.0008225605124607682
iteration 51, loss = 0.0009255505283363163
iteration 52, loss = 0.0007468886906281114
iteration 53, loss = 0.000723914650734514
iteration 54, loss = 0.0007541120867244899
iteration 55, loss = 0.0009550619870424271
iteration 56, loss = 0.0008319987682625651
iteration 57, loss = 0.0010418741730973125
iteration 58, loss = 0.0007948859129101038
iteration 59, loss = 0.001004628138616681
iteration 60, loss = 0.0008676759898662567
iteration 61, loss = 0.000856181897688657
iteration 62, loss = 0.0007459968910552561
iteration 63, loss = 0.000773268286138773
iteration 64, loss = 0.0008858382934704423
iteration 65, loss = 0.0008488191524520516
iteration 66, loss = 0.0010265861637890339
iteration 67, loss = 0.000991763430647552
iteration 68, loss = 0.0008406880078837276
iteration 69, loss = 0.001000974327325821
iteration 70, loss = 0.0006845491589047015
iteration 71, loss = 0.0014160394202917814
iteration 72, loss = 0.0009333986090496182
iteration 73, loss = 0.001178508042357862
iteration 74, loss = 0.0010450966656208038
iteration 75, loss = 0.0013514405582100153
iteration 76, loss = 0.0007973220781423151
iteration 77, loss = 0.0010131808230653405
iteration 78, loss = 0.0006681746453978121
iteration 79, loss = 0.0006839873385615647
iteration 80, loss = 0.0007287014741450548
iteration 81, loss = 0.0007654801593162119
iteration 82, loss = 0.0008643788169138134
iteration 83, loss = 0.000848539755679667
iteration 84, loss = 0.0011475522769615054
iteration 85, loss = 0.0008699733298271894
iteration 86, loss = 0.0006807895260863006
iteration 87, loss = 0.001720149302855134
iteration 88, loss = 0.0019294616067782044
iteration 89, loss = 0.0008202832541428506
iteration 90, loss = 0.0007415550644509494
iteration 91, loss = 0.0007066032849252224
iteration 92, loss = 0.0009466761839576066
iteration 93, loss = 0.0008430919260717928
iteration 94, loss = 0.0017599008278921247
iteration 95, loss = 0.0009603225626051426
iteration 96, loss = 0.0009711983148008585
iteration 97, loss = 0.0008274533902294934
iteration 98, loss = 0.0007625052239745855
iteration 99, loss = 0.0009064768673852086
iteration 100, loss = 0.00100969267077744
iteration 101, loss = 0.0010780019219964743
iteration 102, loss = 0.0010204974096268415
iteration 103, loss = 0.0007500247447751462
iteration 104, loss = 0.000766800541896373
iteration 105, loss = 0.0008703897474333644
iteration 106, loss = 0.0010482037905603647
iteration 107, loss = 0.0008350012358278036
iteration 108, loss = 0.0011396293994039297
iteration 109, loss = 0.0007708793855272233
iteration 110, loss = 0.0014229341177269816
iteration 111, loss = 0.0008542962023057044
iteration 112, loss = 0.0008287649834528565
iteration 113, loss = 0.0007773853139951825
iteration 114, loss = 0.0009470880031585693
iteration 115, loss = 0.0009825392626225948
iteration 116, loss = 0.000802434457000345
iteration 117, loss = 0.0011570309288799763
iteration 118, loss = 0.0012204817030578852
iteration 119, loss = 0.001477580051869154
iteration 120, loss = 0.0008739819168113172
iteration 121, loss = 0.0009021578589454293
iteration 122, loss = 0.000899699458386749
iteration 123, loss = 0.0007400023750960827
iteration 124, loss = 0.001069159945473075
iteration 125, loss = 0.0021666784305125475
iteration 126, loss = 0.0007901008357293904
iteration 127, loss = 0.000742963282391429
iteration 128, loss = 0.0009215193567797542
iteration 129, loss = 0.000889752758666873
iteration 130, loss = 0.0008467109291814268
iteration 131, loss = 0.0007979075307957828
iteration 132, loss = 0.0009099783492274582
iteration 133, loss = 0.000882815511431545
iteration 134, loss = 0.0010096898768097162
iteration 135, loss = 0.0009173275320790708
iteration 136, loss = 0.0008151229121722281
iteration 137, loss = 0.0007298632408492267
iteration 138, loss = 0.0019799883011728525
iteration 139, loss = 0.0008123292354866862
iteration 140, loss = 0.001024424098432064
iteration 141, loss = 0.0008970696362666786
iteration 142, loss = 0.0009960492607206106
iteration 143, loss = 0.000792761507909745
iteration 144, loss = 0.0012550272513180971
iteration 145, loss = 0.0009223029483109713
iteration 146, loss = 0.0014931558398529887
iteration 147, loss = 0.0012250255094841123
iteration 148, loss = 0.0008794515742920339
iteration 149, loss = 0.0008185908081941307
iteration 150, loss = 0.001318831229582429
iteration 151, loss = 0.0008526842575520277
iteration 152, loss = 0.0008656583959236741
iteration 153, loss = 0.0006930465460754931
iteration 154, loss = 0.00150243507232517
iteration 155, loss = 0.001458210637792945
iteration 156, loss = 0.0011257630540058017
iteration 157, loss = 0.0010760815348476171
iteration 158, loss = 0.0008910584729164839
iteration 159, loss = 0.00127663544844836
iteration 160, loss = 0.0007950337021611631
iteration 161, loss = 0.0008261846378445625
iteration 162, loss = 0.0007899710908532143
iteration 163, loss = 0.0008408708963543177
iteration 164, loss = 0.0008787190308794379
iteration 165, loss = 0.0008743306389078498
iteration 166, loss = 0.0008331674034707248
iteration 167, loss = 0.0009346470469608903
iteration 168, loss = 0.0016270022606477141
iteration 169, loss = 0.0011738045141100883
iteration 170, loss = 0.0009203474619425833
iteration 171, loss = 0.0009413575753569603
iteration 172, loss = 0.0008578911074437201
iteration 173, loss = 0.0011162463342770934
iteration 174, loss = 0.0008926700102165341
iteration 175, loss = 0.0007838649326004088
iteration 176, loss = 0.0019514935556799173
iteration 177, loss = 0.0009141701739281416
iteration 178, loss = 0.0008183097816072404
iteration 179, loss = 0.000796656880993396
iteration 180, loss = 0.0010052182478830218
iteration 181, loss = 0.0017304979264736176
iteration 182, loss = 0.0007812780677340925
iteration 183, loss = 0.0012239128118380904
iteration 184, loss = 0.0014596291584894061
iteration 185, loss = 0.000748958089388907
iteration 186, loss = 0.0012144776992499828
iteration 187, loss = 0.0007634654175490141
iteration 188, loss = 0.001040410017594695
iteration 189, loss = 0.000849453208502382
iteration 190, loss = 0.0007673652144148946
iteration 191, loss = 0.0007929905550554395
iteration 192, loss = 0.0008037484367378056
iteration 193, loss = 0.00124880310613662
iteration 194, loss = 0.0009261441882699728
iteration 195, loss = 0.0008653023396618664
iteration 196, loss = 0.0010247230529785156
iteration 197, loss = 0.000805586576461792
iteration 198, loss = 0.0008859928348101676
iteration 199, loss = 0.0010556390043348074
iteration 200, loss = 0.0006515491404570639
iteration 201, loss = 0.0008086742600426078
iteration 202, loss = 0.0009819031693041325
iteration 203, loss = 0.0019448241218924522
iteration 204, loss = 0.0008879996603354812
iteration 205, loss = 0.0013594015035778284
iteration 206, loss = 0.0009803608991205692
iteration 207, loss = 0.000755069253500551
iteration 208, loss = 0.0006673369207419455
iteration 209, loss = 0.0008705359068699181
iteration 210, loss = 0.0010164369596168399
iteration 211, loss = 0.000761453527957201
iteration 212, loss = 0.0008327284012921154
iteration 213, loss = 0.0009681814117357135
iteration 214, loss = 0.0009679108625277877
iteration 215, loss = 0.0010852358536794782
iteration 216, loss = 0.000957341690082103
iteration 217, loss = 0.0008047643932513893
iteration 218, loss = 0.0009192423895001411
iteration 219, loss = 0.0009069995721802115
iteration 220, loss = 0.0009892791276797652
iteration 221, loss = 0.0008324499940499663
iteration 222, loss = 0.0013047093525528908
iteration 223, loss = 0.000987247214652598
iteration 224, loss = 0.0010112359886988997
iteration 225, loss = 0.0008219167939387262
iteration 226, loss = 0.0008736038580536842
iteration 227, loss = 0.0006920000305399299
iteration 228, loss = 0.0021941219456493855
iteration 229, loss = 0.000944358529523015
iteration 230, loss = 0.0009164154762402177
iteration 231, loss = 0.0011292357230558991
iteration 232, loss = 0.0008197637507691979
iteration 233, loss = 0.0007786010392010212
iteration 234, loss = 0.0009621462086215615
iteration 235, loss = 0.0007106541306711733
iteration 236, loss = 0.0009304832201451063
iteration 237, loss = 0.0008441912941634655
iteration 238, loss = 0.0007062407676130533
iteration 239, loss = 0.0008376059704460204
iteration 240, loss = 0.0007523546810261905
iteration 241, loss = 0.0008811779553070664
iteration 242, loss = 0.0012592622078955173
iteration 243, loss = 0.0022982691880315542
iteration 244, loss = 0.0007703140727244318
iteration 245, loss = 0.0008034746861085296
iteration 246, loss = 0.0013403194025158882
iteration 247, loss = 0.001036143396049738
iteration 248, loss = 0.0012052265228703618
iteration 249, loss = 0.001096242107450962
iteration 250, loss = 0.0008752996800467372
iteration 251, loss = 0.0010561905801296234
iteration 252, loss = 0.0008575438405387104
iteration 253, loss = 0.001065614284016192
iteration 254, loss = 0.0008967682369984686
iteration 255, loss = 0.0008097225800156593
iteration 256, loss = 0.0009185207891277969
iteration 257, loss = 0.0007951422012411058
iteration 258, loss = 0.0007226834422908723
iteration 259, loss = 0.0007071939180605114
iteration 260, loss = 0.0007819827878847718
iteration 261, loss = 0.000869124080054462
iteration 262, loss = 0.001619953429326415
iteration 263, loss = 0.0008428469300270081
iteration 264, loss = 0.0010376268764957786
iteration 265, loss = 0.0008314335718750954
iteration 266, loss = 0.0008506868616677821
iteration 267, loss = 0.0011185173643752933
iteration 268, loss = 0.0010787223000079393
iteration 269, loss = 0.0009786473819985986
iteration 270, loss = 0.001644182251766324
iteration 271, loss = 0.0008670661482028663
iteration 272, loss = 0.0009581634076312184
iteration 273, loss = 0.0010092383017763495
iteration 274, loss = 0.0007546934066340327
iteration 275, loss = 0.0007545379921793938
iteration 276, loss = 0.0011040930403396487
iteration 277, loss = 0.0009079224546439946
iteration 278, loss = 0.00071265286533162
iteration 279, loss = 0.0009330091997981071
iteration 280, loss = 0.0006858360138721764
iteration 281, loss = 0.0009600342600606382
iteration 282, loss = 0.0008469350286759436
iteration 283, loss = 0.0011595507385209203
iteration 284, loss = 0.0011133988155052066
iteration 285, loss = 0.0015825084410607815
iteration 286, loss = 0.0016196878859773278
iteration 287, loss = 0.0007705389871262014
iteration 288, loss = 0.0010283435694873333
iteration 289, loss = 0.0009111291728913784
iteration 290, loss = 0.0016607979778200388
iteration 291, loss = 0.0009693524916656315
iteration 292, loss = 0.000810032885055989
iteration 293, loss = 0.0007692099316045642
iteration 294, loss = 0.0007221269770525396
iteration 295, loss = 0.0008869020384736359
iteration 296, loss = 0.0008486620499752462
iteration 297, loss = 0.0007687651086598635
iteration 298, loss = 0.0009253997122868896
iteration 299, loss = 0.0016740377759560943
iteration 300, loss = 0.0008378615602850914
iteration 1, loss = 0.0009870920330286026
iteration 2, loss = 0.0007958778878673911
iteration 3, loss = 0.000917341560125351
iteration 4, loss = 0.0008796185720711946
iteration 5, loss = 0.001011991873383522
iteration 6, loss = 0.0009719676454551518
iteration 7, loss = 0.0009639106574468315
iteration 8, loss = 0.0006802267162129283
iteration 9, loss = 0.0010214621433988214
iteration 10, loss = 0.0008266544318757951
iteration 11, loss = 0.0007169769960455596
iteration 12, loss = 0.0018795731011778116
iteration 13, loss = 0.0014839695068076253
iteration 14, loss = 0.0011921963887289166
iteration 15, loss = 0.0016746174078434706
iteration 16, loss = 0.0008656829013489187
iteration 17, loss = 0.0010302450973540545
iteration 18, loss = 0.0015377472154796124
iteration 19, loss = 0.0007721842266619205
iteration 20, loss = 0.0009479798609390855
iteration 21, loss = 0.0007469646516256034
iteration 22, loss = 0.0007426830125041306
iteration 23, loss = 0.0015484458999708295
iteration 24, loss = 0.0009343918645754457
iteration 25, loss = 0.0007962239324115217
iteration 26, loss = 0.0008188208448700607
iteration 27, loss = 0.0012452291557565331
iteration 28, loss = 0.0009275820921175182
iteration 29, loss = 0.0008552361978217959
iteration 30, loss = 0.0017003866378217936
iteration 31, loss = 0.0007313595851883292
iteration 32, loss = 0.0006811055936850607
iteration 33, loss = 0.0009075715206563473
iteration 34, loss = 0.0008681390900164843
iteration 35, loss = 0.0008901954861357808
iteration 36, loss = 0.0022562036756426096
iteration 37, loss = 0.0007012557471171021
iteration 38, loss = 0.0006772708729840815
iteration 39, loss = 0.0010048651602119207
iteration 40, loss = 0.0008223187178373337
iteration 41, loss = 0.0012062222231179476
iteration 42, loss = 0.000868643750436604
iteration 43, loss = 0.0008370143477804959
iteration 44, loss = 0.0009730046149343252
iteration 45, loss = 0.0008835013723000884
iteration 46, loss = 0.0008077942184172571
iteration 47, loss = 0.0011338238837197423
iteration 48, loss = 0.0011659630108624697
iteration 49, loss = 0.0006587064126506448
iteration 50, loss = 0.0017844106769189239
iteration 51, loss = 0.001889638020657003
iteration 52, loss = 0.0008089985931292176
iteration 53, loss = 0.0011173292296007276
iteration 54, loss = 0.0008882252150215209
iteration 55, loss = 0.0008873004117049277
iteration 56, loss = 0.0008913830388337374
iteration 57, loss = 0.0008605408947914839
iteration 58, loss = 0.0009877518750727177
iteration 59, loss = 0.0008704756619408727
iteration 60, loss = 0.0009626495302654803
iteration 61, loss = 0.0010897514875978231
iteration 62, loss = 0.0010698949918150902
iteration 63, loss = 0.0007965206168591976
iteration 64, loss = 0.0007181079708971083
iteration 65, loss = 0.0017015639459714293
iteration 66, loss = 0.0008665057830512524
iteration 67, loss = 0.0012334210332483053
iteration 68, loss = 0.0008279681787826121
iteration 69, loss = 0.0009256212506443262
iteration 70, loss = 0.0010908666299656034
iteration 71, loss = 0.0008246102370321751
iteration 72, loss = 0.001587862498126924
iteration 73, loss = 0.0007174238562583923
iteration 74, loss = 0.0007162817637436092
iteration 75, loss = 0.0008364636451005936
iteration 76, loss = 0.0010884592775255442
iteration 77, loss = 0.001026936573907733
iteration 78, loss = 0.0007524684770032763
iteration 79, loss = 0.0007653164793737233
iteration 80, loss = 0.0012584475334733725
iteration 81, loss = 0.0007434668368659914
iteration 82, loss = 0.000711376778781414
iteration 83, loss = 0.0014491323381662369
iteration 84, loss = 0.0006351436604745686
iteration 85, loss = 0.0012124658096581697
iteration 86, loss = 0.0007522641681134701
iteration 87, loss = 0.001039873342961073
iteration 88, loss = 0.0007559484802186489
iteration 89, loss = 0.001313827815465629
iteration 90, loss = 0.0007089365390129387
iteration 91, loss = 0.0008844477124512196
iteration 92, loss = 0.0008511599735356867
iteration 93, loss = 0.0007407848024740815
iteration 94, loss = 0.0010609974851831794
iteration 95, loss = 0.0009096622234210372
iteration 96, loss = 0.0014634995022788644
iteration 97, loss = 0.000943925348110497
iteration 98, loss = 0.0011674074921756983
iteration 99, loss = 0.001113690435886383
iteration 100, loss = 0.0007006662199273705
iteration 101, loss = 0.0009077831637114286
iteration 102, loss = 0.0011193020036444068
iteration 103, loss = 0.0008632501703687012
iteration 104, loss = 0.0007405087235383689
iteration 105, loss = 0.0016623430419713259
iteration 106, loss = 0.0007800679304637015
iteration 107, loss = 0.0010055596940219402
iteration 108, loss = 0.0011436042841523886
iteration 109, loss = 0.0011293210554867983
iteration 110, loss = 0.0008349896525032818
iteration 111, loss = 0.0025674584321677685
iteration 112, loss = 0.0008804646204225719
iteration 113, loss = 0.0008645338239148259
iteration 114, loss = 0.0008059433312155306
iteration 115, loss = 0.0007909095147624612
iteration 116, loss = 0.0007358945440500975
iteration 117, loss = 0.0008252756670117378
iteration 118, loss = 0.0008476366056129336
iteration 119, loss = 0.0014755501179024577
iteration 120, loss = 0.0008688090601935983
iteration 121, loss = 0.0007641568081453443
iteration 122, loss = 0.0007894934969954193
iteration 123, loss = 0.0014895078493282199
iteration 124, loss = 0.0007976184715516865
iteration 125, loss = 0.0011479348177090287
iteration 126, loss = 0.001063103904016316
iteration 127, loss = 0.0008705273503437638
iteration 128, loss = 0.0007755783153697848
iteration 129, loss = 0.0009096947032958269
iteration 130, loss = 0.0007896607858128846
iteration 131, loss = 0.0007981762173585594
iteration 132, loss = 0.0008735526935197413
iteration 133, loss = 0.0009149494580924511
iteration 134, loss = 0.0007562419632449746
iteration 135, loss = 0.0009481232846155763
iteration 136, loss = 0.0008798417402431369
iteration 137, loss = 0.001360857393592596
iteration 138, loss = 0.0008074780344031751
iteration 139, loss = 0.0008066411828622222
iteration 140, loss = 0.0009300855454057455
iteration 141, loss = 0.0007857175078243017
iteration 142, loss = 0.0009583876235410571
iteration 143, loss = 0.0011734107974916697
iteration 144, loss = 0.001598694478161633
iteration 145, loss = 0.000669431930873543
iteration 146, loss = 0.0008031937759369612
iteration 147, loss = 0.0007102238014340401
iteration 148, loss = 0.0008293879218399525
iteration 149, loss = 0.0007212889613583684
iteration 150, loss = 0.0006659366190433502
iteration 151, loss = 0.0010157371871173382
iteration 152, loss = 0.0008071116171777248
iteration 153, loss = 0.0009131173137575388
iteration 154, loss = 0.0013868971727788448
iteration 155, loss = 0.0009925595950335264
iteration 156, loss = 0.000913631753064692
iteration 157, loss = 0.0008691640687175095
iteration 158, loss = 0.0008322178618982434
iteration 159, loss = 0.0008247701334767044
iteration 160, loss = 0.0011826230911538005
iteration 161, loss = 0.0011578778503462672
iteration 162, loss = 0.0008484424324706197
iteration 163, loss = 0.0013360766461119056
iteration 164, loss = 0.0013111489824950695
iteration 165, loss = 0.000790272606536746
iteration 166, loss = 0.0012323776027187705
iteration 167, loss = 0.001164899906143546
iteration 168, loss = 0.0009321688557974994
iteration 169, loss = 0.0009153045830316842
iteration 170, loss = 0.0010264230659231544
iteration 171, loss = 0.0009118305752053857
iteration 172, loss = 0.0011000098893418908
iteration 173, loss = 0.000793369603343308
iteration 174, loss = 0.0011995534878224134
iteration 175, loss = 0.0007091101724654436
iteration 176, loss = 0.0010340925073251128
iteration 177, loss = 0.0010114198084920645
iteration 178, loss = 0.0009990306571125984
iteration 179, loss = 0.0007327293278649449
iteration 180, loss = 0.000828881748020649
iteration 181, loss = 0.0009632207220420241
iteration 182, loss = 0.0006669238209724426
iteration 183, loss = 0.0009453667444176972
iteration 184, loss = 0.0007505682297050953
iteration 185, loss = 0.0007911911234259605
iteration 186, loss = 0.0012855391250923276
iteration 187, loss = 0.0010336512932553887
iteration 188, loss = 0.0008552534272894263
iteration 189, loss = 0.0011945294681936502
iteration 190, loss = 0.0008759811753407121
iteration 191, loss = 0.0009231510339304805
iteration 192, loss = 0.0013241824926808476
iteration 193, loss = 0.001050629187375307
iteration 194, loss = 0.0009135246509686112
iteration 195, loss = 0.000875924015417695
iteration 196, loss = 0.000883883039932698
iteration 197, loss = 0.0007967930869199336
iteration 198, loss = 0.00096384302014485
iteration 199, loss = 0.0007939863135106862
iteration 200, loss = 0.0008344785892404616
iteration 201, loss = 0.0008495604270137846
iteration 202, loss = 0.0008001200621947646
iteration 203, loss = 0.001049382146447897
iteration 204, loss = 0.0008805616525933146
iteration 205, loss = 0.000945058767683804
iteration 206, loss = 0.0009420473361387849
iteration 207, loss = 0.0008827232522889972
iteration 208, loss = 0.0008673578267917037
iteration 209, loss = 0.0015861729625612497
iteration 210, loss = 0.0007974946056492627
iteration 211, loss = 0.0009523882181383669
iteration 212, loss = 0.0008090328774414957
iteration 213, loss = 0.0018462768057361245
iteration 214, loss = 0.0006881695007905364
iteration 215, loss = 0.0006945310160517693
iteration 216, loss = 0.0015952406683936715
iteration 217, loss = 0.0013938501942902803
iteration 218, loss = 0.0011275922879576683
iteration 219, loss = 0.0018291148589923978
iteration 220, loss = 0.00098459015134722
iteration 221, loss = 0.0010063252411782742
iteration 222, loss = 0.0007054763264022768
iteration 223, loss = 0.000737231457605958
iteration 224, loss = 0.0008485906291753054
iteration 225, loss = 0.0007631551707163453
iteration 226, loss = 0.0008316924795508385
iteration 227, loss = 0.0011146224569529295
iteration 228, loss = 0.000900417915545404
iteration 229, loss = 0.0007519688224419951
iteration 230, loss = 0.0010339966975152493
iteration 231, loss = 0.0011145431781187654
iteration 232, loss = 0.0016009857645258307
iteration 233, loss = 0.0009581884369254112
iteration 234, loss = 0.0009050531079992652
iteration 235, loss = 0.000987321138381958
iteration 236, loss = 0.000896225159522146
iteration 237, loss = 0.0010615806095302105
iteration 238, loss = 0.0009781420230865479
iteration 239, loss = 0.0006266629206947982
iteration 240, loss = 0.0009012801456265152
iteration 241, loss = 0.0007925672689452767
iteration 242, loss = 0.0010612646583467722
iteration 243, loss = 0.0013271012576296926
iteration 244, loss = 0.0010158063378185034
iteration 245, loss = 0.000875979196280241
iteration 246, loss = 0.0009048094507306814
iteration 247, loss = 0.0008084660512395203
iteration 248, loss = 0.0009632083238102496
iteration 249, loss = 0.0009651563595980406
iteration 250, loss = 0.00077087088720873
iteration 251, loss = 0.0007488505216315389
iteration 252, loss = 0.0009020199649967253
iteration 253, loss = 0.0009537702426314354
iteration 254, loss = 0.0008290551486425102
iteration 255, loss = 0.0009339024545624852
iteration 256, loss = 0.0006946647772565484
iteration 257, loss = 0.0012004018062725663
iteration 258, loss = 0.0008106130990199745
iteration 259, loss = 0.0010515021858736873
iteration 260, loss = 0.0011893513146787882
iteration 261, loss = 0.0008566421456634998
iteration 262, loss = 0.0010555920889601111
iteration 263, loss = 0.0015145111829042435
iteration 264, loss = 0.0014326631790027022
iteration 265, loss = 0.0016197854420170188
iteration 266, loss = 0.0014824903337284923
iteration 267, loss = 0.0009666014229878783
iteration 268, loss = 0.0012448341585695744
iteration 269, loss = 0.0009160455665551126
iteration 270, loss = 0.0008233136613853276
iteration 271, loss = 0.000700834731105715
iteration 272, loss = 0.0011874211486428976
iteration 273, loss = 0.0006932443357072771
iteration 274, loss = 0.0016993421595543623
iteration 275, loss = 0.0008953665965236723
iteration 276, loss = 0.0009127706871367991
iteration 277, loss = 0.0010399118764325976
iteration 278, loss = 0.0008400828228332102
iteration 279, loss = 0.0008976802928373218
iteration 280, loss = 0.0012875426327809691
iteration 281, loss = 0.0007702418370172381
iteration 282, loss = 0.0009060148149728775
iteration 283, loss = 0.000918898731470108
iteration 284, loss = 0.0007266118773259223
iteration 285, loss = 0.0006979876779951155
iteration 286, loss = 0.0007338994182646275
iteration 287, loss = 0.0007350192172452807
iteration 288, loss = 0.0009413445368409157
iteration 289, loss = 0.001104320283047855
iteration 290, loss = 0.0010866145603358746
iteration 291, loss = 0.0009254221222363412
iteration 292, loss = 0.0006802616408094764
iteration 293, loss = 0.0006652063457295299
iteration 294, loss = 0.0008242612821049988
iteration 295, loss = 0.0008758631884120405
iteration 296, loss = 0.0008578990236856043
iteration 297, loss = 0.0006523005431517959
iteration 298, loss = 0.0007365705096162856
iteration 299, loss = 0.0007887834217399359
iteration 300, loss = 0.0010290441568940878
iteration 1, loss = 0.001151281758211553
iteration 2, loss = 0.0010747731430456042
iteration 3, loss = 0.0011826989939436316
iteration 4, loss = 0.0009789543692022562
iteration 5, loss = 0.0010741740697994828
iteration 6, loss = 0.0008980032871477306
iteration 7, loss = 0.0009646312682889402
iteration 8, loss = 0.000892194511834532
iteration 9, loss = 0.0010271362261846662
iteration 10, loss = 0.0008674893761053681
iteration 11, loss = 0.0008676331490278244
iteration 12, loss = 0.0009563015773892403
iteration 13, loss = 0.0007665179437026381
iteration 14, loss = 0.0008519430411979556
iteration 15, loss = 0.0007782545289956033
iteration 16, loss = 0.0009372958447784185
iteration 17, loss = 0.0009519376326352358
iteration 18, loss = 0.0011060123797506094
iteration 19, loss = 0.0008835563203319907
iteration 20, loss = 0.0006483811885118484
iteration 21, loss = 0.0007170095341280103
iteration 22, loss = 0.0008015304920263588
iteration 23, loss = 0.0007540431106463075
iteration 24, loss = 0.0014372693840414286
iteration 25, loss = 0.0007388091762550175
iteration 26, loss = 0.0008228351944126189
iteration 27, loss = 0.000800710404291749
iteration 28, loss = 0.0022174844052642584
iteration 29, loss = 0.0007032291614450514
iteration 30, loss = 0.0009579567704349756
iteration 31, loss = 0.0007659309194423258
iteration 32, loss = 0.0009209291310980916
iteration 33, loss = 0.0013812317047268152
iteration 34, loss = 0.0010393497068434954
iteration 35, loss = 0.001113826292566955
iteration 36, loss = 0.0017141603166237473
iteration 37, loss = 0.0007450629491358995
iteration 38, loss = 0.0008138013654388487
iteration 39, loss = 0.0006707415450364351
iteration 40, loss = 0.0009757560328580439
iteration 41, loss = 0.0007634635549038649
iteration 42, loss = 0.0007870057015679777
iteration 43, loss = 0.0008650429081171751
iteration 44, loss = 0.0009114547865465283
iteration 45, loss = 0.0009584541548974812
iteration 46, loss = 0.0010853155981749296
iteration 47, loss = 0.0008546393364667892
iteration 48, loss = 0.0008818951901048422
iteration 49, loss = 0.0009579391917213798
iteration 50, loss = 0.000841720902826637
iteration 51, loss = 0.0007346287020482123
iteration 52, loss = 0.0010282923467457294
iteration 53, loss = 0.0010000515030696988
iteration 54, loss = 0.000972092617303133
iteration 55, loss = 0.0009007990011014044
iteration 56, loss = 0.0009482228779233992
iteration 57, loss = 0.0007835745345801115
iteration 58, loss = 0.0009595962474122643
iteration 59, loss = 0.0008425773121416569
iteration 60, loss = 0.00079491944052279
iteration 61, loss = 0.0008426809217780828
iteration 62, loss = 0.0008783494122326374
iteration 63, loss = 0.0010692167561501265
iteration 64, loss = 0.0009226035908795893
iteration 65, loss = 0.0009297298965975642
iteration 66, loss = 0.0008476391667500138
iteration 67, loss = 0.0009077987633645535
iteration 68, loss = 0.000869378331117332
iteration 69, loss = 0.0006554173305630684
iteration 70, loss = 0.001642115879803896
iteration 71, loss = 0.0009964234195649624
iteration 72, loss = 0.0008114195661619306
iteration 73, loss = 0.0009875815594568849
iteration 74, loss = 0.0009034126996994019
iteration 75, loss = 0.0011584097519516945
iteration 76, loss = 0.0009908248903229833
iteration 77, loss = 0.0006909479852765799
iteration 78, loss = 0.0010263242293149233
iteration 79, loss = 0.0010389955714344978
iteration 80, loss = 0.0006677539204247296
iteration 81, loss = 0.0006785930600017309
iteration 82, loss = 0.0013701984426006675
iteration 83, loss = 0.0016787233762443066
iteration 84, loss = 0.0011462711263448
iteration 85, loss = 0.000745158176869154
iteration 86, loss = 0.0008083287975750864
iteration 87, loss = 0.0007214443758130074
iteration 88, loss = 0.001153077813796699
iteration 89, loss = 0.0011002459796145558
iteration 90, loss = 0.0007519593345932662
iteration 91, loss = 0.0009267734130844474
iteration 92, loss = 0.0009080126765184104
iteration 93, loss = 0.0011963610304519534
iteration 94, loss = 0.0008003618568181992
iteration 95, loss = 0.0008251964463852346
iteration 96, loss = 0.0008140024729073048
iteration 97, loss = 0.0008548784535378218
iteration 98, loss = 0.0008134338422678411
iteration 99, loss = 0.0006906301714479923
iteration 100, loss = 0.000854174664709717
iteration 101, loss = 0.0021359913516789675
iteration 102, loss = 0.0008847873541526496
iteration 103, loss = 0.0010150668676942587
iteration 104, loss = 0.0011575756361708045
iteration 105, loss = 0.0007774595869705081
iteration 106, loss = 0.0008691298426128924
iteration 107, loss = 0.0010012132115662098
iteration 108, loss = 0.0009596997988410294
iteration 109, loss = 0.0009049954824149609
iteration 110, loss = 0.0017201150767505169
iteration 111, loss = 0.0009131164406426251
iteration 112, loss = 0.0007901131175458431
iteration 113, loss = 0.002231890568509698
iteration 114, loss = 0.0008341032080352306
iteration 115, loss = 0.0007273919763974845
iteration 116, loss = 0.0007167428266257048
iteration 117, loss = 0.0008359814528375864
iteration 118, loss = 0.000807049626018852
iteration 119, loss = 0.0008265853975899518
iteration 120, loss = 0.0007675254018977284
iteration 121, loss = 0.0008620205335319042
iteration 122, loss = 0.0012667521368712187
iteration 123, loss = 0.000775438267737627
iteration 124, loss = 0.0008549594203941524
iteration 125, loss = 0.0008797554182820022
iteration 126, loss = 0.0009020742727443576
iteration 127, loss = 0.0008863785769790411
iteration 128, loss = 0.0017047066939994693
iteration 129, loss = 0.0010084647219628096
iteration 130, loss = 0.0007741354056634009
iteration 131, loss = 0.0020980609115213156
iteration 132, loss = 0.001149706426076591
iteration 133, loss = 0.0010801471071317792
iteration 134, loss = 0.0008286144002340734
iteration 135, loss = 0.0007233258220367134
iteration 136, loss = 0.001087165903300047
iteration 137, loss = 0.0006972004775889218
iteration 138, loss = 0.0009468341013416648
iteration 139, loss = 0.0008389039430767298
iteration 140, loss = 0.000812229816801846
iteration 141, loss = 0.0017270814860239625
iteration 142, loss = 0.0006985265645198524
iteration 143, loss = 0.0009528244263492525
iteration 144, loss = 0.0008629966177977622
iteration 145, loss = 0.0007394987042061985
iteration 146, loss = 0.0012728135334327817
iteration 147, loss = 0.0007221505511552095
iteration 148, loss = 0.0015685457037761807
iteration 149, loss = 0.0018167836824432015
iteration 150, loss = 0.0007942482479847968
iteration 151, loss = 0.0007655633380636573
iteration 152, loss = 0.0008419370278716087
iteration 153, loss = 0.000919753743801266
iteration 154, loss = 0.0007268963963724673
iteration 155, loss = 0.002450890140607953
iteration 156, loss = 0.0008733624708838761
iteration 157, loss = 0.000659638550132513
iteration 158, loss = 0.0007840461330488324
iteration 159, loss = 0.0009240868967026472
iteration 160, loss = 0.0011338761541992426
iteration 161, loss = 0.000797128421254456
iteration 162, loss = 0.0017921924591064453
iteration 163, loss = 0.0008015827625058591
iteration 164, loss = 0.0011887054424732924
iteration 165, loss = 0.0005863471305929124
iteration 166, loss = 0.0011578741250559688
iteration 167, loss = 0.0008144528255797923
iteration 168, loss = 0.0006276661297306418
iteration 169, loss = 0.0007905216771177948
iteration 170, loss = 0.0011114596854895353
iteration 171, loss = 0.001540592173114419
iteration 172, loss = 0.0018599779577925801
iteration 173, loss = 0.000986036960966885
iteration 174, loss = 0.0011172929080203176
iteration 175, loss = 0.0022752732038497925
iteration 176, loss = 0.0007607056177221239
iteration 177, loss = 0.0009175307350233197
iteration 178, loss = 0.0011401486117392778
iteration 179, loss = 0.000825617287773639
iteration 180, loss = 0.0007526706322096288
iteration 181, loss = 0.0011646007187664509
iteration 182, loss = 0.0010386377107352018
iteration 183, loss = 0.0007411374826915562
iteration 184, loss = 0.0009066814091056585
iteration 185, loss = 0.0017973071662709117
iteration 186, loss = 0.0006552270497195423
iteration 187, loss = 0.0012695628684014082
iteration 188, loss = 0.000899086007848382
iteration 189, loss = 0.0007434400613419712
iteration 190, loss = 0.0009254615870304406
iteration 191, loss = 0.0009736291249282658
iteration 192, loss = 0.0007869744440540671
iteration 193, loss = 0.0009653629385866225
iteration 194, loss = 0.0011160201393067837
iteration 195, loss = 0.0010297251865267754
iteration 196, loss = 0.0008715653093531728
iteration 197, loss = 0.0012601459166035056
iteration 198, loss = 0.0007955775363370776
iteration 199, loss = 0.0007533719181083143
iteration 200, loss = 0.0008420583908446133
iteration 201, loss = 0.0015851992648094893
iteration 202, loss = 0.0010810288367792964
iteration 203, loss = 0.0008298289030790329
iteration 204, loss = 0.001569376327097416
iteration 205, loss = 0.0007928782724775374
iteration 206, loss = 0.0009643253870308399
iteration 207, loss = 0.0008865819545462728
iteration 208, loss = 0.0016490924172103405
iteration 209, loss = 0.0008331460994668305
iteration 210, loss = 0.000978904776275158
iteration 211, loss = 0.0009208335541188717
iteration 212, loss = 0.0010857366723939776
iteration 213, loss = 0.0008490211330354214
iteration 214, loss = 0.0008399066864512861
iteration 215, loss = 0.0006948590744286776
iteration 216, loss = 0.0009141034679487348
iteration 217, loss = 0.0008542257710359991
iteration 218, loss = 0.0008895537466742098
iteration 219, loss = 0.001018855138681829
iteration 220, loss = 0.0010008936515077949
iteration 221, loss = 0.0012437037657946348
iteration 222, loss = 0.0008285405929200351
iteration 223, loss = 0.0006818989058956504
iteration 224, loss = 0.0007943425443954766
iteration 225, loss = 0.001046287245117128
iteration 226, loss = 0.0007631474873051047
iteration 227, loss = 0.0009877471020445228
iteration 228, loss = 0.0009256082121282816
iteration 229, loss = 0.0008004968985915184
iteration 230, loss = 0.0011902714613825083
iteration 231, loss = 0.0007210650946944952
iteration 232, loss = 0.0008376616169698536
iteration 233, loss = 0.0009256390039809048
iteration 234, loss = 0.000714954687282443
iteration 235, loss = 0.0007533823372796178
iteration 236, loss = 0.0007866400992497802
iteration 237, loss = 0.0006983724306337535
iteration 238, loss = 0.0007247367175295949
iteration 239, loss = 0.0010511684231460094
iteration 240, loss = 0.0014711064286530018
iteration 241, loss = 0.0009737563668750226
iteration 242, loss = 0.0008465739665552974
iteration 243, loss = 0.0008084204164333642
iteration 244, loss = 0.0009505830239504576
iteration 245, loss = 0.0007462173816747963
iteration 246, loss = 0.0011937667150050402
iteration 247, loss = 0.0008278784807771444
iteration 248, loss = 0.0009093642584048212
iteration 249, loss = 0.0008348708506673574
iteration 250, loss = 0.0009664350654929876
iteration 251, loss = 0.0008848684374243021
iteration 252, loss = 0.0008359067142009735
iteration 253, loss = 0.0011590314097702503
iteration 254, loss = 0.0008072102791629732
iteration 255, loss = 0.0013451597187668085
iteration 256, loss = 0.0008132954826578498
iteration 257, loss = 0.000868778326548636
iteration 258, loss = 0.0007426842348650098
iteration 259, loss = 0.000981992343440652
iteration 260, loss = 0.0008318516192957759
iteration 261, loss = 0.0012569604441523552
iteration 262, loss = 0.0008146369364112616
iteration 263, loss = 0.0016704030567780137
iteration 264, loss = 0.0012691353913396597
iteration 265, loss = 0.001078187138773501
iteration 266, loss = 0.0007842652848921716
iteration 267, loss = 0.0008536600507795811
iteration 268, loss = 0.0011020991951227188
iteration 269, loss = 0.001167290611192584
iteration 270, loss = 0.0008572710212320089
iteration 271, loss = 0.0011149836936965585
iteration 272, loss = 0.0008733902359381318
iteration 273, loss = 0.0008100238046608865
iteration 274, loss = 0.001045188051648438
iteration 275, loss = 0.0010409109527245164
iteration 276, loss = 0.0015642230864614248
iteration 277, loss = 0.0008423178805969656
iteration 278, loss = 0.0008336551254615188
iteration 279, loss = 0.0007890118868090212
iteration 280, loss = 0.0007813304546289146
iteration 281, loss = 0.0007954865577630699
iteration 282, loss = 0.0007214678917080164
iteration 283, loss = 0.0010022883070632815
iteration 284, loss = 0.0011870155576616526
iteration 285, loss = 0.0009492101380601525
iteration 286, loss = 0.0012690776493400335
iteration 287, loss = 0.0007079229108057916
iteration 288, loss = 0.0015003987355157733
iteration 289, loss = 0.0013685051817446947
iteration 290, loss = 0.0008006116840988398
iteration 291, loss = 0.0008784871315583587
iteration 292, loss = 0.0008420663070864975
iteration 293, loss = 0.0008943254360929132
iteration 294, loss = 0.0011452677426859736
iteration 295, loss = 0.0008491338230669498
iteration 296, loss = 0.0008335188031196594
iteration 297, loss = 0.000947578577324748
iteration 298, loss = 0.0014530611224472523
iteration 299, loss = 0.0007828442612662911
iteration 300, loss = 0.0007493363227695227
iteration 1, loss = 0.0012040581787005067
iteration 2, loss = 0.0008363783708773553
iteration 3, loss = 0.0007579827797599137
iteration 4, loss = 0.000786905933637172
iteration 5, loss = 0.0015267867129296064
iteration 6, loss = 0.0006773591740056872
iteration 7, loss = 0.0023268682416528463
iteration 8, loss = 0.0015330413589254022
iteration 9, loss = 0.000989790540188551
iteration 10, loss = 0.0008680564351379871
iteration 11, loss = 0.0007295550312846899
iteration 12, loss = 0.000997723313048482
iteration 13, loss = 0.0011737305903807282
iteration 14, loss = 0.0008869928424246609
iteration 15, loss = 0.0012588577810674906
iteration 16, loss = 0.0011460790410637856
iteration 17, loss = 0.0008121929131448269
iteration 18, loss = 0.0012376685626804829
iteration 19, loss = 0.0009144339710474014
iteration 20, loss = 0.0008214499102905393
iteration 21, loss = 0.0009700099471956491
iteration 22, loss = 0.0007583586848340929
iteration 23, loss = 0.0008618669235147536
iteration 24, loss = 0.0010753239039331675
iteration 25, loss = 0.0007859310717321932
iteration 26, loss = 0.0008741439669393003
iteration 27, loss = 0.0010418621823191643
iteration 28, loss = 0.0008271248661912978
iteration 29, loss = 0.0009239943465217948
iteration 30, loss = 0.0007446887902915478
iteration 31, loss = 0.0013312952360138297
iteration 32, loss = 0.00080805120524019
iteration 33, loss = 0.0016559964278712869
iteration 34, loss = 0.0010103329550474882
iteration 35, loss = 0.0006957757868804038
iteration 36, loss = 0.0009419437265023589
iteration 37, loss = 0.0014192541129887104
iteration 38, loss = 0.0012176999589428306
iteration 39, loss = 0.0009325244463980198
iteration 40, loss = 0.0008617372950538993
iteration 41, loss = 0.0007767974748276174
iteration 42, loss = 0.0008627322386018932
iteration 43, loss = 0.0006781925912946463
iteration 44, loss = 0.002002336550503969
iteration 45, loss = 0.0010447168024256825
iteration 46, loss = 0.0008945020381361246
iteration 47, loss = 0.0007130649173632264
iteration 48, loss = 0.0007719123386777937
iteration 49, loss = 0.0010162536054849625
iteration 50, loss = 0.000785530311986804
iteration 51, loss = 0.0017180654685944319
iteration 52, loss = 0.0008548335754312575
iteration 53, loss = 0.000736528541892767
iteration 54, loss = 0.0007181109394878149
iteration 55, loss = 0.0009690981241874397
iteration 56, loss = 0.0007840718026272953
iteration 57, loss = 0.0008720253827050328
iteration 58, loss = 0.000780470494646579
iteration 59, loss = 0.001220385660417378
iteration 60, loss = 0.0009547297959215939
iteration 61, loss = 0.0009211026481352746
iteration 62, loss = 0.000821705732960254
iteration 63, loss = 0.001260750344954431
iteration 64, loss = 0.0009071120293810964
iteration 65, loss = 0.0008982506114989519
iteration 66, loss = 0.0007953702588565648
iteration 67, loss = 0.0017307150410488248
iteration 68, loss = 0.0012800475815311074
iteration 69, loss = 0.0008636332349851727
iteration 70, loss = 0.0008604179602116346
iteration 71, loss = 0.0009143374627456069
iteration 72, loss = 0.0007855930016376078
iteration 73, loss = 0.0007612693589180708
iteration 74, loss = 0.0008071557385846972
iteration 75, loss = 0.0013691142667084932
iteration 76, loss = 0.0007413479033857584
iteration 77, loss = 0.0009980816394090652
iteration 78, loss = 0.0007335777627304196
iteration 79, loss = 0.0009852658258751035
iteration 80, loss = 0.0009116294677369297
iteration 81, loss = 0.0008505649166181684
iteration 82, loss = 0.001618906739167869
iteration 83, loss = 0.000733023218344897
iteration 84, loss = 0.0009027257328853011
iteration 85, loss = 0.0014501985860988498
iteration 86, loss = 0.0016860354226082563
iteration 87, loss = 0.0013151717139407992
iteration 88, loss = 0.0007979411748237908
iteration 89, loss = 0.0011785156093537807
iteration 90, loss = 0.0007930862484499812
iteration 91, loss = 0.0008081500418484211
iteration 92, loss = 0.0014036459615454078
iteration 93, loss = 0.0016578155336901546
iteration 94, loss = 0.001042611664161086
iteration 95, loss = 0.0008979495614767075
iteration 96, loss = 0.000746160454582423
iteration 97, loss = 0.0007618076051585376
iteration 98, loss = 0.0008010197198018432
iteration 99, loss = 0.0007984773837961257
iteration 100, loss = 0.0006974349380470812
iteration 101, loss = 0.0017829680582508445
iteration 102, loss = 0.0011641238816082478
iteration 103, loss = 0.0007658754475414753
iteration 104, loss = 0.0007445046212524176
iteration 105, loss = 0.0008976467652246356
iteration 106, loss = 0.0008043537382036448
iteration 107, loss = 0.0007789868977852166
iteration 108, loss = 0.000759970280341804
iteration 109, loss = 0.000775237800553441
iteration 110, loss = 0.001585529767908156
iteration 111, loss = 0.0009435711544938385
iteration 112, loss = 0.0009354419889859855
iteration 113, loss = 0.0012881294824182987
iteration 114, loss = 0.0017900395905598998
iteration 115, loss = 0.0008568966295570135
iteration 116, loss = 0.001582730794325471
iteration 117, loss = 0.0006797286332584918
iteration 118, loss = 0.0007784583722241223
iteration 119, loss = 0.000809270131867379
iteration 120, loss = 0.0008748320396989584
iteration 121, loss = 0.0010751145891845226
iteration 122, loss = 0.0011047256411984563
iteration 123, loss = 0.0007968349964357913
iteration 124, loss = 0.0008309718105010688
iteration 125, loss = 0.0011495343642309308
iteration 126, loss = 0.0010760336881503463
iteration 127, loss = 0.0007855728617869318
iteration 128, loss = 0.001074956264346838
iteration 129, loss = 0.0007548002176918089
iteration 130, loss = 0.0012632504804059863
iteration 131, loss = 0.0008707598317414522
iteration 132, loss = 0.0006095085409469903
iteration 133, loss = 0.0016990078147500753
iteration 134, loss = 0.0009027081541717052
iteration 135, loss = 0.0008829045109450817
iteration 136, loss = 0.0011282170889899135
iteration 137, loss = 0.0009195253951475024
iteration 138, loss = 0.0006990133551880717
iteration 139, loss = 0.0012352970661595464
iteration 140, loss = 0.0007873409194871783
iteration 141, loss = 0.0008867883589118719
iteration 142, loss = 0.0006997179007157683
iteration 143, loss = 0.0008303902577608824
iteration 144, loss = 0.0008029508753679693
iteration 145, loss = 0.0008754796581342816
iteration 146, loss = 0.0006884828326292336
iteration 147, loss = 0.0008027475560083985
iteration 148, loss = 0.0008844013209454715
iteration 149, loss = 0.0009084437624551356
iteration 150, loss = 0.0009464035974815488
iteration 151, loss = 0.0008070264011621475
iteration 152, loss = 0.0008665671921335161
iteration 153, loss = 0.000849424279294908
iteration 154, loss = 0.0008186585619114339
iteration 155, loss = 0.001078022993169725
iteration 156, loss = 0.000827103154733777
iteration 157, loss = 0.0007320554577745497
iteration 158, loss = 0.0009830700000748038
iteration 159, loss = 0.0007935252506285906
iteration 160, loss = 0.001034139539115131
iteration 161, loss = 0.0009325687424279749
iteration 162, loss = 0.0009942468022927642
iteration 163, loss = 0.0008498290553689003
iteration 164, loss = 0.0008508358150720596
iteration 165, loss = 0.0008075354271568358
iteration 166, loss = 0.0008187362691387534
iteration 167, loss = 0.0008426684653386474
iteration 168, loss = 0.000706195249222219
iteration 169, loss = 0.001084913150407374
iteration 170, loss = 0.0010404811473563313
iteration 171, loss = 0.0007207384915091097
iteration 172, loss = 0.0007746281335130334
iteration 173, loss = 0.0006701993988826871
iteration 174, loss = 0.0013584863627329469
iteration 175, loss = 0.001060948008671403
iteration 176, loss = 0.000767760444432497
iteration 177, loss = 0.0009498496656306088
iteration 178, loss = 0.00116376590449363
iteration 179, loss = 0.0015000582206994295
iteration 180, loss = 0.0015047924825921655
iteration 181, loss = 0.0009561418555676937
iteration 182, loss = 0.0007293644011951983
iteration 183, loss = 0.0008624375332146883
iteration 184, loss = 0.0007378770387731493
iteration 185, loss = 0.0012182266218587756
iteration 186, loss = 0.0017857254715636373
iteration 187, loss = 0.0007560692029073834
iteration 188, loss = 0.0017581143183633685
iteration 189, loss = 0.0015983887715265155
iteration 190, loss = 0.00068286171881482
iteration 191, loss = 0.0011162231676280499
iteration 192, loss = 0.0009291362948715687
iteration 193, loss = 0.0007325946353375912
iteration 194, loss = 0.00080951361451298
iteration 195, loss = 0.0008999578421935439
iteration 196, loss = 0.0007654259097762406
iteration 197, loss = 0.0009595876326784492
iteration 198, loss = 0.0008011891040951014
iteration 199, loss = 0.0008033756166696548
iteration 200, loss = 0.0010690167546272278
iteration 201, loss = 0.0008810654398985207
iteration 202, loss = 0.0007363557233475149
iteration 203, loss = 0.001446769805625081
iteration 204, loss = 0.0007246261811815202
iteration 205, loss = 0.0007290225476026535
iteration 206, loss = 0.0008579525747336447
iteration 207, loss = 0.0009959263261407614
iteration 208, loss = 0.0013888092944398522
iteration 209, loss = 0.0010435915319249034
iteration 210, loss = 0.0008705008076503873
iteration 211, loss = 0.0007971909362822771
iteration 212, loss = 0.0012592849088832736
iteration 213, loss = 0.0006379056721925735
iteration 214, loss = 0.0007666230667382479
iteration 215, loss = 0.0008665261557325721
iteration 216, loss = 0.0007895221933722496
iteration 217, loss = 0.0007541773375123739
iteration 218, loss = 0.0007994859479367733
iteration 219, loss = 0.0008794237510301173
iteration 220, loss = 0.00077308330219239
iteration 221, loss = 0.0010115390177816153
iteration 222, loss = 0.0009046784834936261
iteration 223, loss = 0.0008890180615708232
iteration 224, loss = 0.0011006195563822985
iteration 225, loss = 0.0006809402257204056
iteration 226, loss = 0.0008046998409554362
iteration 227, loss = 0.0010044382652267814
iteration 228, loss = 0.000778191548306495
iteration 229, loss = 0.0010684046428650618
iteration 230, loss = 0.0014589125057682395
iteration 231, loss = 0.0007337124552577734
iteration 232, loss = 0.001491129631176591
iteration 233, loss = 0.0007054393063299358
iteration 234, loss = 0.0007795041892677546
iteration 235, loss = 0.0011679292656481266
iteration 236, loss = 0.000738275412004441
iteration 237, loss = 0.000870396033860743
iteration 238, loss = 0.0008745028753764927
iteration 239, loss = 0.0007961050723679364
iteration 240, loss = 0.0009843888692557812
iteration 241, loss = 0.0009642388322390616
iteration 242, loss = 0.001062446623109281
iteration 243, loss = 0.0007264644373208284
iteration 244, loss = 0.0008518239483237267
iteration 245, loss = 0.0006912130629643798
iteration 246, loss = 0.0007822579937055707
iteration 247, loss = 0.0008518766844645143
iteration 248, loss = 0.000881482264958322
iteration 249, loss = 0.001405794289894402
iteration 250, loss = 0.0007497043698094785
iteration 251, loss = 0.0013415387365967035
iteration 252, loss = 0.0008297270396724343
iteration 253, loss = 0.0008777334587648511
iteration 254, loss = 0.0008177022100426257
iteration 255, loss = 0.0013803145848214626
iteration 256, loss = 0.0009901714511215687
iteration 257, loss = 0.0010169701417908072
iteration 258, loss = 0.0008128038025461137
iteration 259, loss = 0.0007792155374772847
iteration 260, loss = 0.0007001823396421969
iteration 261, loss = 0.0009306760039180517
iteration 262, loss = 0.0008492626948282123
iteration 263, loss = 0.0008987809997051954
iteration 264, loss = 0.0008768248371779919
iteration 265, loss = 0.0008774147136136889
iteration 266, loss = 0.0011445365380495787
iteration 267, loss = 0.0011197050334885716
iteration 268, loss = 0.0013368099462240934
iteration 269, loss = 0.0009044758626259863
iteration 270, loss = 0.0016496243188157678
iteration 271, loss = 0.001021545846015215
iteration 272, loss = 0.0014262329787015915
iteration 273, loss = 0.0007809402886778116
iteration 274, loss = 0.0008609802462160587
iteration 275, loss = 0.0008402244420722127
iteration 276, loss = 0.0011748502729460597
iteration 277, loss = 0.0007926783291622996
iteration 278, loss = 0.000731227220967412
iteration 279, loss = 0.000814903702121228
iteration 280, loss = 0.0009991397382691503
iteration 281, loss = 0.0011461430694907904
iteration 282, loss = 0.0011290616821497679
iteration 283, loss = 0.0008969553164206445
iteration 284, loss = 0.0010808908846229315
iteration 285, loss = 0.0007115061162039638
iteration 286, loss = 0.0009020116995088756
iteration 287, loss = 0.00142991216853261
iteration 288, loss = 0.0014238691655918956
iteration 289, loss = 0.0008312959689646959
iteration 290, loss = 0.0019072913564741611
iteration 291, loss = 0.0009394866647198796
iteration 292, loss = 0.001030320767313242
iteration 293, loss = 0.0007958245114423335
iteration 294, loss = 0.0008295765146613121
iteration 295, loss = 0.0006980395410209894
iteration 296, loss = 0.0011508654570207
iteration 297, loss = 0.0008523861179128289
iteration 298, loss = 0.001589937717653811
iteration 299, loss = 0.0014509100001305342
iteration 300, loss = 0.001183727290481329
iteration 1, loss = 0.0006291510071605444
iteration 2, loss = 0.0009480353910475969
iteration 3, loss = 0.0008740414050407708
iteration 4, loss = 0.0008250882383435965
iteration 5, loss = 0.0007495583849959075
iteration 6, loss = 0.0009674226166680455
iteration 7, loss = 0.0014306074008345604
iteration 8, loss = 0.000997700379230082
iteration 9, loss = 0.0008076124358922243
iteration 10, loss = 0.0009606031235307455
iteration 11, loss = 0.0009504639310762286
iteration 12, loss = 0.0010840857867151499
iteration 13, loss = 0.000895884761121124
iteration 14, loss = 0.0009295839117839932
iteration 15, loss = 0.0010913028381764889
iteration 16, loss = 0.0008410110604017973
iteration 17, loss = 0.0017959971446543932
iteration 18, loss = 0.0008023360278457403
iteration 19, loss = 0.0007628616876900196
iteration 20, loss = 0.0008355551981367171
iteration 21, loss = 0.0010501937940716743
iteration 22, loss = 0.0016399746527895331
iteration 23, loss = 0.0008163160528056324
iteration 24, loss = 0.0008457910735160112
iteration 25, loss = 0.0014246521750465035
iteration 26, loss = 0.0007813918055035174
iteration 27, loss = 0.0007974019972607493
iteration 28, loss = 0.0010478668846189976
iteration 29, loss = 0.0007778337458148599
iteration 30, loss = 0.0011459439992904663
iteration 31, loss = 0.001244233106262982
iteration 32, loss = 0.0011056506773456931
iteration 33, loss = 0.0013535735197365284
iteration 34, loss = 0.0009143431088887155
iteration 35, loss = 0.0015585292130708694
iteration 36, loss = 0.0007563091930933297
iteration 37, loss = 0.000954480143263936
iteration 38, loss = 0.0008655808051116765
iteration 39, loss = 0.00077654147753492
iteration 40, loss = 0.0009098954033106565
iteration 41, loss = 0.0012096813879907131
iteration 42, loss = 0.0007645579753443599
iteration 43, loss = 0.0019403775222599506
iteration 44, loss = 0.0007208309834823012
iteration 45, loss = 0.000900015642400831
iteration 46, loss = 0.0009296938660554588
iteration 47, loss = 0.001201713108457625
iteration 48, loss = 0.0008649090304970741
iteration 49, loss = 0.0008642466855235398
iteration 50, loss = 0.0011046405415982008
iteration 51, loss = 0.0009334220667369664
iteration 52, loss = 0.0007310378714464605
iteration 53, loss = 0.0006366657325997949
iteration 54, loss = 0.0011826208792626858
iteration 55, loss = 0.0008696670993231237
iteration 56, loss = 0.001708626514300704
iteration 57, loss = 0.0008161037694662809
iteration 58, loss = 0.0008982905419543386
iteration 59, loss = 0.000973863119725138
iteration 60, loss = 0.0011423963587731123
iteration 61, loss = 0.0006655115284956992
iteration 62, loss = 0.00071241072146222
iteration 63, loss = 0.0007835612050257623
iteration 64, loss = 0.0009025607723742723
iteration 65, loss = 0.0009612815920263529
iteration 66, loss = 0.0009130273247137666
iteration 67, loss = 0.0008476349175907671
iteration 68, loss = 0.0015817424282431602
iteration 69, loss = 0.0008697583107277751
iteration 70, loss = 0.0007235922967083752
iteration 71, loss = 0.0007819099701009691
iteration 72, loss = 0.000877231010235846
iteration 73, loss = 0.0007339442963711917
iteration 74, loss = 0.0007195895304903388
iteration 75, loss = 0.0008414062904193997
iteration 76, loss = 0.0009995249565690756
iteration 77, loss = 0.0009137946763075888
iteration 78, loss = 0.0008628603536635637
iteration 79, loss = 0.0006539265159517527
iteration 80, loss = 0.0009553101845085621
iteration 81, loss = 0.001058241818100214
iteration 82, loss = 0.0010263859294354916
iteration 83, loss = 0.0008084461442194879
iteration 84, loss = 0.0009687879355624318
iteration 85, loss = 0.0016690983902662992
iteration 86, loss = 0.0008755186572670937
iteration 87, loss = 0.0009335153736174107
iteration 88, loss = 0.0010217847302556038
iteration 89, loss = 0.0008898762753233314
iteration 90, loss = 0.0008696806617081165
iteration 91, loss = 0.0011960119009017944
iteration 92, loss = 0.0013914464507251978
iteration 93, loss = 0.001183678861707449
iteration 94, loss = 0.0009190470445901155
iteration 95, loss = 0.0008941427222453058
iteration 96, loss = 0.0008385564433410764
iteration 97, loss = 0.0007533596944995224
iteration 98, loss = 0.002084029372781515
iteration 99, loss = 0.001152703771367669
iteration 100, loss = 0.0008683658670634031
iteration 101, loss = 0.0007077782065607607
iteration 102, loss = 0.0007740086293779314
iteration 103, loss = 0.0013055101735517383
iteration 104, loss = 0.0010610026074573398
iteration 105, loss = 0.0008549485937692225
iteration 106, loss = 0.000998561386950314
iteration 107, loss = 0.0007353777764365077
iteration 108, loss = 0.0007179424865171313
iteration 109, loss = 0.0010277539258822799
iteration 110, loss = 0.0007988965371623635
iteration 111, loss = 0.0011805949034169316
iteration 112, loss = 0.0007737661944702268
iteration 113, loss = 0.0010856962762773037
iteration 114, loss = 0.0015731463208794594
iteration 115, loss = 0.0010094160679727793
iteration 116, loss = 0.0006703821127302945
iteration 117, loss = 0.0010156704811379313
iteration 118, loss = 0.001074168598279357
iteration 119, loss = 0.0016710025956854224
iteration 120, loss = 0.0008718095486983657
iteration 121, loss = 0.001492125797085464
iteration 122, loss = 0.0014901512768119574
iteration 123, loss = 0.0009422397706657648
iteration 124, loss = 0.0007230220944620669
iteration 125, loss = 0.0014782620128244162
iteration 126, loss = 0.0009345131693407893
iteration 127, loss = 0.000992214772850275
iteration 128, loss = 0.0007641870179213583
iteration 129, loss = 0.0015832276549190283
iteration 130, loss = 0.00116664485540241
iteration 131, loss = 0.0006485913181677461
iteration 132, loss = 0.0009198219631798565
iteration 133, loss = 0.0009072669199667871
iteration 134, loss = 0.0009218102786689997
iteration 135, loss = 0.0008197091519832611
iteration 136, loss = 0.000720379815902561
iteration 137, loss = 0.0013000889448449016
iteration 138, loss = 0.0006432182271964848
iteration 139, loss = 0.0007557542412541807
iteration 140, loss = 0.0008340894710272551
iteration 141, loss = 0.0007080055656842887
iteration 142, loss = 0.0010501131182536483
iteration 143, loss = 0.000705642276443541
iteration 144, loss = 0.0006362786516547203
iteration 145, loss = 0.0007665205048397183
iteration 146, loss = 0.0012657518964260817
iteration 147, loss = 0.0012917960993945599
iteration 148, loss = 0.0011057083029299974
iteration 149, loss = 0.0008789827115833759
iteration 150, loss = 0.0006437968113459647
iteration 151, loss = 0.0010358062572777271
iteration 152, loss = 0.0008739122422412038
iteration 153, loss = 0.0016533387824892998
iteration 154, loss = 0.0011830830480903387
iteration 155, loss = 0.0008037347579374909
iteration 156, loss = 0.000938381243031472
iteration 157, loss = 0.0008420248632319272
iteration 158, loss = 0.0009628173429518938
iteration 159, loss = 0.0008617943967692554
iteration 160, loss = 0.001222863094881177
iteration 161, loss = 0.0007543587125837803
iteration 162, loss = 0.000886649708263576
iteration 163, loss = 0.0007888691616244614
iteration 164, loss = 0.0009146803640760481
iteration 165, loss = 0.0007437561871483922
iteration 166, loss = 0.000806431460659951
iteration 167, loss = 0.0007407504017464817
iteration 168, loss = 0.0008664695778861642
iteration 169, loss = 0.0012205145321786404
iteration 170, loss = 0.0007634434150531888
iteration 171, loss = 0.0010433801217004657
iteration 172, loss = 0.0007934791501611471
iteration 173, loss = 0.0008586192852817476
iteration 174, loss = 0.0006356047233566642
iteration 175, loss = 0.0016415670979768038
iteration 176, loss = 0.0007870980771258473
iteration 177, loss = 0.0009161984780803323
iteration 178, loss = 0.0018444020533934236
iteration 179, loss = 0.0007247574394568801
iteration 180, loss = 0.0008760282071307302
iteration 181, loss = 0.0011746478267014027
iteration 182, loss = 0.0007758596329949796
iteration 183, loss = 0.0007258101250045002
iteration 184, loss = 0.0007771698874421418
iteration 185, loss = 0.0008417665376327932
iteration 186, loss = 0.0008008371805772185
iteration 187, loss = 0.000786325428634882
iteration 188, loss = 0.001029492705129087
iteration 189, loss = 0.0009236411424353719
iteration 190, loss = 0.0007967106648720801
iteration 191, loss = 0.0006946030771359801
iteration 192, loss = 0.0006735039642080665
iteration 193, loss = 0.0012529660016298294
iteration 194, loss = 0.0010279174894094467
iteration 195, loss = 0.00108238123357296
iteration 196, loss = 0.0009687793790362775
iteration 197, loss = 0.0009069199441000819
iteration 198, loss = 0.0008619764121249318
iteration 199, loss = 0.0008316593593917787
iteration 200, loss = 0.0011273104464635253
iteration 201, loss = 0.0009433632949367166
iteration 202, loss = 0.0014101313427090645
iteration 203, loss = 0.0017860815860331059
iteration 204, loss = 0.0010711534414440393
iteration 205, loss = 0.0008758682524785399
iteration 206, loss = 0.0010936062317341566
iteration 207, loss = 0.0007317775161936879
iteration 208, loss = 0.001139266649261117
iteration 209, loss = 0.0008827048586681485
iteration 210, loss = 0.0007841633050702512
iteration 211, loss = 0.0007703897426836193
iteration 212, loss = 0.0009743330883793533
iteration 213, loss = 0.0010404165368527174
iteration 214, loss = 0.0006263083196245134
iteration 215, loss = 0.0008182259043678641
iteration 216, loss = 0.0007619657553732395
iteration 217, loss = 0.0008483588462695479
iteration 218, loss = 0.0007051888387650251
iteration 219, loss = 0.0014686220092698932
iteration 220, loss = 0.0007350643281824887
iteration 221, loss = 0.0008540567941963673
iteration 222, loss = 0.000917248020414263
iteration 223, loss = 0.0015555062564089894
iteration 224, loss = 0.0006927890353836119
iteration 225, loss = 0.001167141366750002
iteration 226, loss = 0.0008131811628118157
iteration 227, loss = 0.0008232241380028427
iteration 228, loss = 0.0008950639166869223
iteration 229, loss = 0.0007993270992301404
iteration 230, loss = 0.0009418587433174253
iteration 231, loss = 0.0007003516075201333
iteration 232, loss = 0.0007466870010830462
iteration 233, loss = 0.0008923491113819182
iteration 234, loss = 0.0013052640715613961
iteration 235, loss = 0.0014321334892883897
iteration 236, loss = 0.0008087002788670361
iteration 237, loss = 0.0013587531866505742
iteration 238, loss = 0.0008119505946524441
iteration 239, loss = 0.0009129531681537628
iteration 240, loss = 0.0008338020415976644
iteration 241, loss = 0.0007238447433337569
iteration 242, loss = 0.0006923242472112179
iteration 243, loss = 0.0008682499756105244
iteration 244, loss = 0.0007963377865962684
iteration 245, loss = 0.0006286277202889323
iteration 246, loss = 0.000737234833650291
iteration 247, loss = 0.0008617819985374808
iteration 248, loss = 0.0007879544864408672
iteration 249, loss = 0.001110425335355103
iteration 250, loss = 0.0008375106262974441
iteration 251, loss = 0.0010801414027810097
iteration 252, loss = 0.0008759205229580402
iteration 253, loss = 0.0008012923644855618
iteration 254, loss = 0.0017128915060311556
iteration 255, loss = 0.0019155580084770918
iteration 256, loss = 0.001080109621398151
iteration 257, loss = 0.0009245057590305805
iteration 258, loss = 0.0010340327862650156
iteration 259, loss = 0.000791143742389977
iteration 260, loss = 0.0008266886579804122
iteration 261, loss = 0.0007542733219452202
iteration 262, loss = 0.0006541297188960016
iteration 263, loss = 0.0014666420174762607
iteration 264, loss = 0.0012059069704264402
iteration 265, loss = 0.0013351021334528923
iteration 266, loss = 0.0007731718942523003
iteration 267, loss = 0.001037526992149651
iteration 268, loss = 0.0009752791374921799
iteration 269, loss = 0.0008159135468304157
iteration 270, loss = 0.0010197408264502883
iteration 271, loss = 0.0018854349618777633
iteration 272, loss = 0.0008186115301214159
iteration 273, loss = 0.0008875419152900577
iteration 274, loss = 0.0007652346394024789
iteration 275, loss = 0.0007710656500421464
iteration 276, loss = 0.0011774091981351376
iteration 277, loss = 0.0010339729487895966
iteration 278, loss = 0.0008454436901956797
iteration 279, loss = 0.0016891821287572384
iteration 280, loss = 0.0008283891947939992
iteration 281, loss = 0.0007701732683926821
iteration 282, loss = 0.0011148880003020167
iteration 283, loss = 0.0008162421872839332
iteration 284, loss = 0.0012751070316880941
iteration 285, loss = 0.0008137286640703678
iteration 286, loss = 0.0007444766815751791
iteration 287, loss = 0.0008672798285260797
iteration 288, loss = 0.0012615281157195568
iteration 289, loss = 0.0009312487673014402
iteration 290, loss = 0.0007339981384575367
iteration 291, loss = 0.000946004525758326
iteration 292, loss = 0.0012105173664167523
iteration 293, loss = 0.0013918527401983738
iteration 294, loss = 0.0012962096370756626
iteration 295, loss = 0.002163022058084607
iteration 296, loss = 0.0008016473148018122
iteration 297, loss = 0.0009933959227055311
iteration 298, loss = 0.0007954948814585805
iteration 299, loss = 0.00131958385463804
iteration 300, loss = 0.0007302443846128881
iteration 1, loss = 0.0008881756220944226
iteration 2, loss = 0.0009279551450163126
iteration 3, loss = 0.0010348442010581493
iteration 4, loss = 0.0010377009166404605
iteration 5, loss = 0.001119163352996111
iteration 6, loss = 0.0011661357712000608
iteration 7, loss = 0.00136135658249259
iteration 8, loss = 0.0009135336731560528
iteration 9, loss = 0.0007296313415281475
iteration 10, loss = 0.0008512199274264276
iteration 11, loss = 0.0016228962922468781
iteration 12, loss = 0.0014626503689214587
iteration 13, loss = 0.001136021688580513
iteration 14, loss = 0.0008901678374968469
iteration 15, loss = 0.0008548723999410868
iteration 16, loss = 0.0016307992627844214
iteration 17, loss = 0.0006959298625588417
iteration 18, loss = 0.0008206454222090542
iteration 19, loss = 0.0008723815553821623
iteration 20, loss = 0.0008048369199968874
iteration 21, loss = 0.0009368974715471268
iteration 22, loss = 0.0010560575174167752
iteration 23, loss = 0.0010367482900619507
iteration 24, loss = 0.0014109153999015689
iteration 25, loss = 0.0016025276854634285
iteration 26, loss = 0.0007844283827580512
iteration 27, loss = 0.0008773431181907654
iteration 28, loss = 0.0008701698970980942
iteration 29, loss = 0.0015807915478944778
iteration 30, loss = 0.0009180225315503776
iteration 31, loss = 0.0009504940244369209
iteration 32, loss = 0.0007654610089957714
iteration 33, loss = 0.0010407394729554653
iteration 34, loss = 0.0012453040108084679
iteration 35, loss = 0.001644869684241712
iteration 36, loss = 0.0007464484078809619
iteration 37, loss = 0.0011201633606106043
iteration 38, loss = 0.0009975554421544075
iteration 39, loss = 0.000754075706936419
iteration 40, loss = 0.0008759453194215894
iteration 41, loss = 0.0007745459442958236
iteration 42, loss = 0.0009863879531621933
iteration 43, loss = 0.0009627591352909803
iteration 44, loss = 0.0015013529919087887
iteration 45, loss = 0.0007031510467641056
iteration 46, loss = 0.0009025390027090907
iteration 47, loss = 0.0008036582148633897
iteration 48, loss = 0.0008227117359638214
iteration 49, loss = 0.001147749600932002
iteration 50, loss = 0.000918705016374588
iteration 51, loss = 0.0009490427910350263
iteration 52, loss = 0.0008244808414019644
iteration 53, loss = 0.001045677112415433
iteration 54, loss = 0.0007519763894379139
iteration 55, loss = 0.001075737178325653
iteration 56, loss = 0.0007926648831926286
iteration 57, loss = 0.0009825985180214047
iteration 58, loss = 0.0007133877370506525
iteration 59, loss = 0.0010235116351395845
iteration 60, loss = 0.000848480558488518
iteration 61, loss = 0.0017982646822929382
iteration 62, loss = 0.0007780696032568812
iteration 63, loss = 0.0007775558042339981
iteration 64, loss = 0.0008089250768534839
iteration 65, loss = 0.0007633542991243303
iteration 66, loss = 0.0008081685518845916
iteration 67, loss = 0.0009623895166441798
iteration 68, loss = 0.0012002384755760431
iteration 69, loss = 0.0008543198346160352
iteration 70, loss = 0.001281989854760468
iteration 71, loss = 0.000922135601285845
iteration 72, loss = 0.0008294021827168763
iteration 73, loss = 0.0008338894695043564
iteration 74, loss = 0.0010353266261518002
iteration 75, loss = 0.0007397497538477182
iteration 76, loss = 0.0007148297736421227
iteration 77, loss = 0.0015769372694194317
iteration 78, loss = 0.0008919462561607361
iteration 79, loss = 0.0008455635979771614
iteration 80, loss = 0.0006990428082644939
iteration 81, loss = 0.0009843736188486218
iteration 82, loss = 0.0006433259113691747
iteration 83, loss = 0.0009569199755787849
iteration 84, loss = 0.0010985640110448003
iteration 85, loss = 0.0008270296966657043
iteration 86, loss = 0.0009101532050408423
iteration 87, loss = 0.0007159587112255394
iteration 88, loss = 0.000839917513076216
iteration 89, loss = 0.0008450754103250802
iteration 90, loss = 0.0006747576408088207
iteration 91, loss = 0.0010989069705829024
iteration 92, loss = 0.0009595844894647598
iteration 93, loss = 0.0010899732587859035
iteration 94, loss = 0.0008485146099701524
iteration 95, loss = 0.0011357481125742197
iteration 96, loss = 0.0009775228099897504
iteration 97, loss = 0.001116794883273542
iteration 98, loss = 0.001079354900866747
iteration 99, loss = 0.0007351608364842832
iteration 100, loss = 0.001168480608612299
iteration 101, loss = 0.0009383843280375004
iteration 102, loss = 0.0009706523269414902
iteration 103, loss = 0.0008012612815946341
iteration 104, loss = 0.0012065240880474448
iteration 105, loss = 0.0008033245103433728
iteration 106, loss = 0.0009046946070156991
iteration 107, loss = 0.0009237367194145918
iteration 108, loss = 0.0008327351533807814
iteration 109, loss = 0.0006940510356798768
iteration 110, loss = 0.001132833887822926
iteration 111, loss = 0.0013031469425186515
iteration 112, loss = 0.0010542962700128555
iteration 113, loss = 0.0008523582364432514
iteration 114, loss = 0.0007372444961220026
iteration 115, loss = 0.0012309395242482424
iteration 116, loss = 0.0008042946574278176
iteration 117, loss = 0.000827422714792192
iteration 118, loss = 0.0008524410077370703
iteration 119, loss = 0.0008138760458678007
iteration 120, loss = 0.0014216916169971228
iteration 121, loss = 0.0008348069386556745
iteration 122, loss = 0.0018720559310168028
iteration 123, loss = 0.0008259728783741593
iteration 124, loss = 0.0012207813560962677
iteration 125, loss = 0.0013722414150834084
iteration 126, loss = 0.0014447066932916641
iteration 127, loss = 0.0007731179357506335
iteration 128, loss = 0.0011968739563599229
iteration 129, loss = 0.0014138718834146857
iteration 130, loss = 0.0007688984624110162
iteration 131, loss = 0.0007017931784503162
iteration 132, loss = 0.0015432089567184448
iteration 133, loss = 0.0011233552359044552
iteration 134, loss = 0.001738044316880405
iteration 135, loss = 0.0007126464042812586
iteration 136, loss = 0.0008307654643431306
iteration 137, loss = 0.0008056099177338183
iteration 138, loss = 0.001125649781897664
iteration 139, loss = 0.0013594570336863399
iteration 140, loss = 0.0007967106648720801
iteration 141, loss = 0.0011048559099435806
iteration 142, loss = 0.0009583507780916989
iteration 143, loss = 0.0019541417714208364
iteration 144, loss = 0.0008939808467403054
iteration 145, loss = 0.0008938796818256378
iteration 146, loss = 0.0008942610584199429
iteration 147, loss = 0.000817236490547657
iteration 148, loss = 0.0012219641357660294
iteration 149, loss = 0.0008376960176974535
iteration 150, loss = 0.0008699189638718963
iteration 151, loss = 0.0011274464195594192
iteration 152, loss = 0.0008677387959323823
iteration 153, loss = 0.0011948333121836185
iteration 154, loss = 0.0014352953294292092
iteration 155, loss = 0.001550230081193149
iteration 156, loss = 0.0008275608997792006
iteration 157, loss = 0.0009055270929820836
iteration 158, loss = 0.0008043599664233625
iteration 159, loss = 0.001510299858637154
iteration 160, loss = 0.0007907667895779014
iteration 161, loss = 0.0014003722462803125
iteration 162, loss = 0.001108103315345943
iteration 163, loss = 0.0007076318725012243
iteration 164, loss = 0.0007190628675743937
iteration 165, loss = 0.000791102007497102
iteration 166, loss = 0.0011338571785017848
iteration 167, loss = 0.0010138459037989378
iteration 168, loss = 0.0007678812253288925
iteration 169, loss = 0.0013084872625768185
iteration 170, loss = 0.0006287590367719531
iteration 171, loss = 0.0007547475397586823
iteration 172, loss = 0.0007280181162059307
iteration 173, loss = 0.0008028154843486845
iteration 174, loss = 0.00155458424706012
iteration 175, loss = 0.0007503517554141581
iteration 176, loss = 0.0008260075119324028
iteration 177, loss = 0.0010259522823616862
iteration 178, loss = 0.0012638072948902845
iteration 179, loss = 0.0008174750255420804
iteration 180, loss = 0.0010103160748258233
iteration 181, loss = 0.0007648526225239038
iteration 182, loss = 0.0015950792003422976
iteration 183, loss = 0.0007292712689377367
iteration 184, loss = 0.0007553473697043955
iteration 185, loss = 0.0008098339312709868
iteration 186, loss = 0.000762208248488605
iteration 187, loss = 0.0008874156046658754
iteration 188, loss = 0.0008426664280705154
iteration 189, loss = 0.000758234818931669
iteration 190, loss = 0.0009129253448918462
iteration 191, loss = 0.0008599687716923654
iteration 192, loss = 0.0008500219555571675
iteration 193, loss = 0.000711564440280199
iteration 194, loss = 0.0008003069087862968
iteration 195, loss = 0.0008375824545510113
iteration 196, loss = 0.0008323498186655343
iteration 197, loss = 0.0007860180921852589
iteration 198, loss = 0.0007676042150706053
iteration 199, loss = 0.0006955802091397345
iteration 200, loss = 0.001073505263775587
iteration 201, loss = 0.0008542736759409308
iteration 202, loss = 0.0007382230833172798
iteration 203, loss = 0.0009084368357434869
iteration 204, loss = 0.0007800524472258985
iteration 205, loss = 0.0013455059379339218
iteration 206, loss = 0.0011812574230134487
iteration 207, loss = 0.000870090676471591
iteration 208, loss = 0.001788117690011859
iteration 209, loss = 0.0008721975609660149
iteration 210, loss = 0.0009285022970288992
iteration 211, loss = 0.0010230899788439274
iteration 212, loss = 0.0009103631018660963
iteration 213, loss = 0.001595590729266405
iteration 214, loss = 0.0014720592880621552
iteration 215, loss = 0.0012525441125035286
iteration 216, loss = 0.0010877884924411774
iteration 217, loss = 0.0009016517433337867
iteration 218, loss = 0.0011147354962304235
iteration 219, loss = 0.0011722700437530875
iteration 220, loss = 0.0009252805612049997
iteration 221, loss = 0.0015791906043887138
iteration 222, loss = 0.001081264577805996
iteration 223, loss = 0.0011768057011067867
iteration 224, loss = 0.0007717364351265132
iteration 225, loss = 0.000987066887319088
iteration 226, loss = 0.0009740966488607228
iteration 227, loss = 0.000810089404694736
iteration 228, loss = 0.0008924708818085492
iteration 229, loss = 0.000804574228823185
iteration 230, loss = 0.0010969380382448435
iteration 231, loss = 0.0010030964622274041
iteration 232, loss = 0.0011368332197889686
iteration 233, loss = 0.001682630623690784
iteration 234, loss = 0.0009209061972796917
iteration 235, loss = 0.0008098611724562943
iteration 236, loss = 0.0008529644692316651
iteration 237, loss = 0.0010805989149957895
iteration 238, loss = 0.0008855107007548213
iteration 239, loss = 0.0006853708764538169
iteration 240, loss = 0.0010307938791811466
iteration 241, loss = 0.0015156761510297656
iteration 242, loss = 0.0010345405898988247
iteration 243, loss = 0.0008737350581213832
iteration 244, loss = 0.0008900280809029937
iteration 245, loss = 0.000666170206386596
iteration 246, loss = 0.0008407191489823163
iteration 247, loss = 0.000865297915879637
iteration 248, loss = 0.0009271131129935384
iteration 249, loss = 0.0007749750511720777
iteration 250, loss = 0.0008641481981612742
iteration 251, loss = 0.000816814077552408
iteration 252, loss = 0.0007403385825455189
iteration 253, loss = 0.0009069119696505368
iteration 254, loss = 0.0018874404486268759
iteration 255, loss = 0.0007812372059561312
iteration 256, loss = 0.000894321477971971
iteration 257, loss = 0.0008451779140159488
iteration 258, loss = 0.001131596276536584
iteration 259, loss = 0.0007632990600541234
iteration 260, loss = 0.0007537916535511613
iteration 261, loss = 0.0010356524726375937
iteration 262, loss = 0.0008718628087081015
iteration 263, loss = 0.0011975481174886227
iteration 264, loss = 0.0008118054247461259
iteration 265, loss = 0.0007456169696524739
iteration 266, loss = 0.0012914303224533796
iteration 267, loss = 0.0008390579023398459
iteration 268, loss = 0.001015472342260182
iteration 269, loss = 0.0009231415460817516
iteration 270, loss = 0.0011002697283402085
iteration 271, loss = 0.0007747418712824583
iteration 272, loss = 0.0008736294694244862
iteration 273, loss = 0.0009481786983087659
iteration 274, loss = 0.0008718657772988081
iteration 275, loss = 0.0008054835489019752
iteration 276, loss = 0.0008475108188576996
iteration 277, loss = 0.000683401245623827
iteration 278, loss = 0.0007629718165844679
iteration 279, loss = 0.000699421449098736
iteration 280, loss = 0.0014523420250043273
iteration 281, loss = 0.000827533658593893
iteration 282, loss = 0.000954149872995913
iteration 283, loss = 0.0007630735635757446
iteration 284, loss = 0.0010535273468121886
iteration 285, loss = 0.0006636269390583038
iteration 286, loss = 0.0007100937655195594
iteration 287, loss = 0.0008395227487199008
iteration 288, loss = 0.0010834884596988559
iteration 289, loss = 0.0008559525012969971
iteration 290, loss = 0.0007562930695712566
iteration 291, loss = 0.0008104813750833273
iteration 292, loss = 0.0007642371347174048
iteration 293, loss = 0.0008863239781931043
iteration 294, loss = 0.0008935058722272515
iteration 295, loss = 0.0010998041834682226
iteration 296, loss = 0.001011460437439382
iteration 297, loss = 0.0010345885530114174
iteration 298, loss = 0.0009884181199595332
iteration 299, loss = 0.0007983067771419883
iteration 300, loss = 0.0008012474863789976
iteration 1, loss = 0.0009084708872251213
iteration 2, loss = 0.0007892936118878424
iteration 3, loss = 0.0010426586959511042
iteration 4, loss = 0.0007418282912112772
iteration 5, loss = 0.0009492351091466844
iteration 6, loss = 0.0007926208199933171
iteration 7, loss = 0.0008056303486227989
iteration 8, loss = 0.0007279989076778293
iteration 9, loss = 0.0009425379685126245
iteration 10, loss = 0.0009319051168859005
iteration 11, loss = 0.0008105307351797819
iteration 12, loss = 0.0008722527418285608
iteration 13, loss = 0.0015739966183900833
iteration 14, loss = 0.001040792092680931
iteration 15, loss = 0.0006960325408726931
iteration 16, loss = 0.0009068058570846915
iteration 17, loss = 0.0015324591659009457
iteration 18, loss = 0.0007660486735403538
iteration 19, loss = 0.0010423220228403807
iteration 20, loss = 0.000978562980890274
iteration 21, loss = 0.0015421713469550014
iteration 22, loss = 0.0013695054221898317
iteration 23, loss = 0.0025012048427015543
iteration 24, loss = 0.0008717990131117404
iteration 25, loss = 0.001133805955760181
iteration 26, loss = 0.0013974873581901193
iteration 27, loss = 0.0009621335193514824
iteration 28, loss = 0.0007919269846752286
iteration 29, loss = 0.0008607745403423905
iteration 30, loss = 0.0006639399798586965
iteration 31, loss = 0.001074671745300293
iteration 32, loss = 0.0009724192786961794
iteration 33, loss = 0.0008139172568917274
iteration 34, loss = 0.0008190678199753165
iteration 35, loss = 0.0008759170887060463
iteration 36, loss = 0.0014722434571012855
iteration 37, loss = 0.0009626983664929867
iteration 38, loss = 0.0010276708053424954
iteration 39, loss = 0.0007996235508471727
iteration 40, loss = 0.0008594376849941909
iteration 41, loss = 0.0010287133045494556
iteration 42, loss = 0.0008120498969219625
iteration 43, loss = 0.0008141974685713649
iteration 44, loss = 0.0010191878536716104
iteration 45, loss = 0.0015303705586120486
iteration 46, loss = 0.0017247048672288656
iteration 47, loss = 0.0007084569660946727
iteration 48, loss = 0.000882841064594686
iteration 49, loss = 0.0010006548836827278
iteration 50, loss = 0.0008416270138695836
iteration 51, loss = 0.0007542556268163025
iteration 52, loss = 0.0008471788605675101
iteration 53, loss = 0.0007658306858502328
iteration 54, loss = 0.0016008408274501562
iteration 55, loss = 0.0007286013569682837
iteration 56, loss = 0.000961555982939899
iteration 57, loss = 0.0008506656740792096
iteration 58, loss = 0.0011905983556061983
iteration 59, loss = 0.001582838362082839
iteration 60, loss = 0.0012318871449679136
iteration 61, loss = 0.0012546973302960396
iteration 62, loss = 0.0016633727354928851
iteration 63, loss = 0.0008284005452878773
iteration 64, loss = 0.0012505610939115286
iteration 65, loss = 0.0008031865581870079
iteration 66, loss = 0.0008585249888710678
iteration 67, loss = 0.0008860555244609714
iteration 68, loss = 0.0009607842657715082
iteration 69, loss = 0.0008199979783967137
iteration 70, loss = 0.0006905461195856333
iteration 71, loss = 0.0016563375247642398
iteration 72, loss = 0.0008346285903826356
iteration 73, loss = 0.0016053661238402128
iteration 74, loss = 0.0008055496145971119
iteration 75, loss = 0.0008157548145391047
iteration 76, loss = 0.0016047602985054255
iteration 77, loss = 0.0007510852883569896
iteration 78, loss = 0.0007979284273460507
iteration 79, loss = 0.0009912695968523622
iteration 80, loss = 0.0008294815779663622
iteration 81, loss = 0.0009547699009999633
iteration 82, loss = 0.000768934260122478
iteration 83, loss = 0.0008621611050330102
iteration 84, loss = 0.0008184909238480031
iteration 85, loss = 0.0008344544330611825
iteration 86, loss = 0.0008199622971005738
iteration 87, loss = 0.0008281706250272691
iteration 88, loss = 0.0010600561508908868
iteration 89, loss = 0.0009142589988186955
iteration 90, loss = 0.0007539800135418773
iteration 91, loss = 0.0008968264446593821
iteration 92, loss = 0.0008926236187107861
iteration 93, loss = 0.0008640622836537659
iteration 94, loss = 0.0008908894378691912
iteration 95, loss = 0.0009366579470224679
iteration 96, loss = 0.0008471215260215104
iteration 97, loss = 0.0018216792959719896
iteration 98, loss = 0.000679895281791687
iteration 99, loss = 0.0011355829192325473
iteration 100, loss = 0.001437339000403881
iteration 101, loss = 0.0008586530457250774
iteration 102, loss = 0.0016303780721500516
iteration 103, loss = 0.0008166140178218484
iteration 104, loss = 0.0007887586834840477
iteration 105, loss = 0.0008022140827961266
iteration 106, loss = 0.001007929677143693
iteration 107, loss = 0.0007942300289869308
iteration 108, loss = 0.0010059241903945804
iteration 109, loss = 0.0007960087386891246
iteration 110, loss = 0.0009867793414741755
iteration 111, loss = 0.0010118219070136547
iteration 112, loss = 0.0008942239219322801
iteration 113, loss = 0.0011861184611916542
iteration 114, loss = 0.0016985505353659391
iteration 115, loss = 0.0007835112628526986
iteration 116, loss = 0.0008947169408202171
iteration 117, loss = 0.0009197709732688963
iteration 118, loss = 0.0012685334077104926
iteration 119, loss = 0.0012153969146311283
iteration 120, loss = 0.0007342264871113002
iteration 121, loss = 0.0007892136345617473
iteration 122, loss = 0.0009052734822034836
iteration 123, loss = 0.0008710527908988297
iteration 124, loss = 0.0011262898333370686
iteration 125, loss = 0.0009135722066275775
iteration 126, loss = 0.0009435573010705411
iteration 127, loss = 0.0009252112358808517
iteration 128, loss = 0.0006858819397166371
iteration 129, loss = 0.0013588836882263422
iteration 130, loss = 0.0006278575165197253
iteration 131, loss = 0.0014483772683888674
iteration 132, loss = 0.0008867789292708039
iteration 133, loss = 0.0007204121211543679
iteration 134, loss = 0.0009056105045601726
iteration 135, loss = 0.0007239672704599798
iteration 136, loss = 0.0009787280578166246
iteration 137, loss = 0.0007650881889276206
iteration 138, loss = 0.00086814328096807
iteration 139, loss = 0.0008462414843961596
iteration 140, loss = 0.00087016262114048
iteration 141, loss = 0.0007967831334099174
iteration 142, loss = 0.0013855575816705823
iteration 143, loss = 0.0006871967343613505
iteration 144, loss = 0.0008588058408349752
iteration 145, loss = 0.0012429115595296025
iteration 146, loss = 0.0007330373045988381
iteration 147, loss = 0.0008671515388414264
iteration 148, loss = 0.0011684864293783903
iteration 149, loss = 0.0008600853616371751
iteration 150, loss = 0.0007956086774356663
iteration 151, loss = 0.0006715429481118917
iteration 152, loss = 0.0009249345166608691
iteration 153, loss = 0.000795902160461992
iteration 154, loss = 0.0009005361935123801
iteration 155, loss = 0.0007498354534618556
iteration 156, loss = 0.0009182150824926794
iteration 157, loss = 0.0006807571626268327
iteration 158, loss = 0.0007967500714585185
iteration 159, loss = 0.0016786305932328105
iteration 160, loss = 0.0011854678159579635
iteration 161, loss = 0.0009254764299839735
iteration 162, loss = 0.0009124011266976595
iteration 163, loss = 0.0007862005732022226
iteration 164, loss = 0.0009926590137183666
iteration 165, loss = 0.0010547293350100517
iteration 166, loss = 0.001373615930788219
iteration 167, loss = 0.0006691297166980803
iteration 168, loss = 0.0008519507246091962
iteration 169, loss = 0.000782054674346
iteration 170, loss = 0.0009993868879973888
iteration 171, loss = 0.0015007517067715526
iteration 172, loss = 0.0015931136440485716
iteration 173, loss = 0.0017722275806590915
iteration 174, loss = 0.0010034898295998573
iteration 175, loss = 0.0010833367705345154
iteration 176, loss = 0.0008025284041650593
iteration 177, loss = 0.0007259714184328914
iteration 178, loss = 0.0010377127910032868
iteration 179, loss = 0.0008497489034198225
iteration 180, loss = 0.0006848727352917194
iteration 181, loss = 0.0010700010461732745
iteration 182, loss = 0.001029893523082137
iteration 183, loss = 0.0010470208944752812
iteration 184, loss = 0.000848313036840409
iteration 185, loss = 0.001408193726092577
iteration 186, loss = 0.001417107880115509
iteration 187, loss = 0.001310195424593985
iteration 188, loss = 0.0008945885929279029
iteration 189, loss = 0.0014986232854425907
iteration 190, loss = 0.000971065426710993
iteration 191, loss = 0.0008599936845712364
iteration 192, loss = 0.0008103805594146252
iteration 193, loss = 0.000897368649020791
iteration 194, loss = 0.0008775829919613898
iteration 195, loss = 0.0007569414447061718
iteration 196, loss = 0.0007126256823539734
iteration 197, loss = 0.0008337654289789498
iteration 198, loss = 0.0009519989835098386
iteration 199, loss = 0.0007914204034022987
iteration 200, loss = 0.0017001773230731487
iteration 201, loss = 0.0010200978722423315
iteration 202, loss = 0.0007979286019690335
iteration 203, loss = 0.0008521974086761475
iteration 204, loss = 0.0007752400124445558
iteration 205, loss = 0.0007897919276729226
iteration 206, loss = 0.0008436606731265783
iteration 207, loss = 0.0009483147878199816
iteration 208, loss = 0.0007284366874955595
iteration 209, loss = 0.000712876848410815
iteration 210, loss = 0.0014967597089707851
iteration 211, loss = 0.0007093427702784538
iteration 212, loss = 0.0011230328818783164
iteration 213, loss = 0.0008444541017524898
iteration 214, loss = 0.0009266457054764032
iteration 215, loss = 0.0008135612588375807
iteration 216, loss = 0.0009217467159032822
iteration 217, loss = 0.0010487164836376905
iteration 218, loss = 0.0009735478088259697
iteration 219, loss = 0.001044242875650525
iteration 220, loss = 0.0008534395601600409
iteration 221, loss = 0.0007575866766273975
iteration 222, loss = 0.0008461612742394209
iteration 223, loss = 0.0007929102866910398
iteration 224, loss = 0.001066546537913382
iteration 225, loss = 0.0009898366406559944
iteration 226, loss = 0.0011470104800537229
iteration 227, loss = 0.000654922507237643
iteration 228, loss = 0.0008794498280622065
iteration 229, loss = 0.001423550653271377
iteration 230, loss = 0.0009771294426172972
iteration 231, loss = 0.0013854600256308913
iteration 232, loss = 0.0010177966905757785
iteration 233, loss = 0.0009680850198492408
iteration 234, loss = 0.0007695691892877221
iteration 235, loss = 0.0007715963874943554
iteration 236, loss = 0.000796967651695013
iteration 237, loss = 0.0011977544054389
iteration 238, loss = 0.0008739040931686759
iteration 239, loss = 0.0011097207898274064
iteration 240, loss = 0.0007427535019814968
iteration 241, loss = 0.0008475706563331187
iteration 242, loss = 0.0008411818416789174
iteration 243, loss = 0.0008926876471377909
iteration 244, loss = 0.0006984943174757063
iteration 245, loss = 0.0007578340009786189
iteration 246, loss = 0.0008014388149604201
iteration 247, loss = 0.0007202533888630569
iteration 248, loss = 0.0012261047959327698
iteration 249, loss = 0.0007204704452306032
iteration 250, loss = 0.0008103153086267412
iteration 251, loss = 0.0008829135331325233
iteration 252, loss = 0.0022493426222354174
iteration 253, loss = 0.0013751831138506532
iteration 254, loss = 0.0014448737492784858
iteration 255, loss = 0.0008402939420193434
iteration 256, loss = 0.0007686362368986011
iteration 257, loss = 0.0007739273132756352
iteration 258, loss = 0.0011585894972085953
iteration 259, loss = 0.0007799157174304128
iteration 260, loss = 0.0013724217424169183
iteration 261, loss = 0.0008984292508102953
iteration 262, loss = 0.0008098179823718965
iteration 263, loss = 0.000792857666965574
iteration 264, loss = 0.0007700455607846379
iteration 265, loss = 0.0011955008376389742
iteration 266, loss = 0.001031290041282773
iteration 267, loss = 0.0009422999573871493
iteration 268, loss = 0.0011688830563798547
iteration 269, loss = 0.0006574364379048347
iteration 270, loss = 0.0017849461873993278
iteration 271, loss = 0.0009713428444229066
iteration 272, loss = 0.0016214060597121716
iteration 273, loss = 0.0008240966126322746
iteration 274, loss = 0.0007921321084722877
iteration 275, loss = 0.0010551156010478735
iteration 276, loss = 0.0007579016964882612
iteration 277, loss = 0.000972084526438266
iteration 278, loss = 0.0008041674154810607
iteration 279, loss = 0.0008800262585282326
iteration 280, loss = 0.001086049829609692
iteration 281, loss = 0.0007368350052274764
iteration 282, loss = 0.0007154424674808979
iteration 283, loss = 0.0008246806683018804
iteration 284, loss = 0.001116350875236094
iteration 285, loss = 0.0006340646068565547
iteration 286, loss = 0.0006216169567778707
iteration 287, loss = 0.00115021166857332
iteration 288, loss = 0.0008267065859399736
iteration 289, loss = 0.001042990479618311
iteration 290, loss = 0.0009607392130419612
iteration 291, loss = 0.0008389275171793997
iteration 292, loss = 0.0008074456127360463
iteration 293, loss = 0.0007333231042139232
iteration 294, loss = 0.0008604249451309443
iteration 295, loss = 0.0007123897084966302
iteration 296, loss = 0.000783362309448421
iteration 297, loss = 0.001541344914585352
iteration 298, loss = 0.0009565491345711052
iteration 299, loss = 0.0010452991118654609
iteration 300, loss = 0.0009797083912417293
iteration 1, loss = 0.0011813538148999214
iteration 2, loss = 0.0007241875282488763
iteration 3, loss = 0.000905045832041651
iteration 4, loss = 0.0016174884513020515
iteration 5, loss = 0.001112028956413269
iteration 6, loss = 0.0007295886753126979
iteration 7, loss = 0.0008551696082577109
iteration 8, loss = 0.0008158385753631592
iteration 9, loss = 0.0009939337614923716
iteration 10, loss = 0.0007607706938870251
iteration 11, loss = 0.0016354554099962115
iteration 12, loss = 0.0008205815101973712
iteration 13, loss = 0.0011678397422656417
iteration 14, loss = 0.0017991643399000168
iteration 15, loss = 0.0009623400401324034
iteration 16, loss = 0.0015961871249601245
iteration 17, loss = 0.0010774364927783608
iteration 18, loss = 0.0008341724751517177
iteration 19, loss = 0.0017446600832045078
iteration 20, loss = 0.0010760618606582284
iteration 21, loss = 0.0009538681479170918
iteration 22, loss = 0.0011304584331810474
iteration 23, loss = 0.0010090740397572517
iteration 24, loss = 0.0011684222845360637
iteration 25, loss = 0.0008768362458795309
iteration 26, loss = 0.000680427358020097
iteration 27, loss = 0.0008584598544985056
iteration 28, loss = 0.0009565014624968171
iteration 29, loss = 0.0009423318551853299
iteration 30, loss = 0.0012077585561200976
iteration 31, loss = 0.0008209623629227281
iteration 32, loss = 0.000817463849671185
iteration 33, loss = 0.0017206526827067137
iteration 34, loss = 0.0008248839294537902
iteration 35, loss = 0.0008234366541728377
iteration 36, loss = 0.0007311893859878182
iteration 37, loss = 0.0010776628041639924
iteration 38, loss = 0.0009370589395985007
iteration 39, loss = 0.0016001455951482058
iteration 40, loss = 0.0008165340404957533
iteration 41, loss = 0.0009695292101241648
iteration 42, loss = 0.0009298145305365324
iteration 43, loss = 0.000736608577426523
iteration 44, loss = 0.0008104139706119895
iteration 45, loss = 0.0008558544795960188
iteration 46, loss = 0.0008255490683950484
iteration 47, loss = 0.0007270031492225826
iteration 48, loss = 0.0011978247202932835
iteration 49, loss = 0.0007791246753185987
iteration 50, loss = 0.0007722618756815791
iteration 51, loss = 0.0008077840902842581
iteration 52, loss = 0.0008002678514458239
iteration 53, loss = 0.0009876054245978594
iteration 54, loss = 0.0007423324859701097
iteration 55, loss = 0.000794919382315129
iteration 56, loss = 0.0007530123693868518
iteration 57, loss = 0.0008635601261630654
iteration 58, loss = 0.0008505528094246984
iteration 59, loss = 0.0010210822802037
iteration 60, loss = 0.0007225514273159206
iteration 61, loss = 0.0009089441737160087
iteration 62, loss = 0.0009929564548656344
iteration 63, loss = 0.0017296839505434036
iteration 64, loss = 0.0008327119285240769
iteration 65, loss = 0.0008431397145614028
iteration 66, loss = 0.0009250183356925845
iteration 67, loss = 0.0009374116780236363
iteration 68, loss = 0.0009525456698611379
iteration 69, loss = 0.0008472589543089271
iteration 70, loss = 0.0008176647243089974
iteration 71, loss = 0.0011118738912045956
iteration 72, loss = 0.0006921040476299822
iteration 73, loss = 0.0009573765564709902
iteration 74, loss = 0.0007640858530066907
iteration 75, loss = 0.0012079996522516012
iteration 76, loss = 0.0009273516479879618
iteration 77, loss = 0.0008741445490159094
iteration 78, loss = 0.0007245044689625502
iteration 79, loss = 0.0008714874857105315
iteration 80, loss = 0.0008258633315563202
iteration 81, loss = 0.0011099041439592838
iteration 82, loss = 0.0008203908219002187
iteration 83, loss = 0.0007108390564098954
iteration 84, loss = 0.0008082174463197589
iteration 85, loss = 0.0007671269122511148
iteration 86, loss = 0.0007661515846848488
iteration 87, loss = 0.0008043639827519655
iteration 88, loss = 0.0008590539218857884
iteration 89, loss = 0.0007854319410398602
iteration 90, loss = 0.0007456287275999784
iteration 91, loss = 0.0009715479682199657
iteration 92, loss = 0.0007791261305101216
iteration 93, loss = 0.0007002385682426393
iteration 94, loss = 0.0012111206306144595
iteration 95, loss = 0.0008061922271735966
iteration 96, loss = 0.0008151260553859174
iteration 97, loss = 0.000772054074332118
iteration 98, loss = 0.001814424991607666
iteration 99, loss = 0.0007767840288579464
iteration 100, loss = 0.0007585676503367722
iteration 101, loss = 0.0007297483971342444
iteration 102, loss = 0.0008980716811493039
iteration 103, loss = 0.001002804609015584
iteration 104, loss = 0.0007392834522761405
iteration 105, loss = 0.0010060705244541168
iteration 106, loss = 0.000826049770694226
iteration 107, loss = 0.0010252692736685276
iteration 108, loss = 0.0009362017153762281
iteration 109, loss = 0.001596219721250236
iteration 110, loss = 0.0007476415485143661
iteration 111, loss = 0.0008824961259961128
iteration 112, loss = 0.0009920359589159489
iteration 113, loss = 0.0010649338364601135
iteration 114, loss = 0.000917017285246402
iteration 115, loss = 0.001062275841832161
iteration 116, loss = 0.001153173972852528
iteration 117, loss = 0.000992802670225501
iteration 118, loss = 0.0011604881146922708
iteration 119, loss = 0.0007497985498048365
iteration 120, loss = 0.000693817506544292
iteration 121, loss = 0.0009175819577649236
iteration 122, loss = 0.0007443592767231166
iteration 123, loss = 0.0010340820299461484
iteration 124, loss = 0.0012530980166047812
iteration 125, loss = 0.000712513574399054
iteration 126, loss = 0.0013023859355598688
iteration 127, loss = 0.0006980113103054464
iteration 128, loss = 0.001065758871845901
iteration 129, loss = 0.0007344875484704971
iteration 130, loss = 0.0008797054761089385
iteration 131, loss = 0.0007918791379779577
iteration 132, loss = 0.0007953587919473648
iteration 133, loss = 0.001247748383320868
iteration 134, loss = 0.0006854133098386228
iteration 135, loss = 0.0010061271023005247
iteration 136, loss = 0.0014241678873077035
iteration 137, loss = 0.0013365843333303928
iteration 138, loss = 0.0009766316507011652
iteration 139, loss = 0.0008367181289941072
iteration 140, loss = 0.0009341854020021856
iteration 141, loss = 0.0008017218788154423
iteration 142, loss = 0.0011567368637770414
iteration 143, loss = 0.0009311054018326104
iteration 144, loss = 0.001409817603416741
iteration 145, loss = 0.0018385566072538495
iteration 146, loss = 0.000907318142708391
iteration 147, loss = 0.0008714560535736382
iteration 148, loss = 0.0008504498982802033
iteration 149, loss = 0.000828550779260695
iteration 150, loss = 0.0009708292782306671
iteration 151, loss = 0.0008055127109400928
iteration 152, loss = 0.0008649426745250821
iteration 153, loss = 0.0007578084478154778
iteration 154, loss = 0.0008537593530490994
iteration 155, loss = 0.0010861658956855536
iteration 156, loss = 0.0008566949400119483
iteration 157, loss = 0.0008119700360111892
iteration 158, loss = 0.0015992511762306094
iteration 159, loss = 0.0009152856655418873
iteration 160, loss = 0.0009377091191709042
iteration 161, loss = 0.0017395294271409512
iteration 162, loss = 0.0006832145154476166
iteration 163, loss = 0.0007599157397635281
iteration 164, loss = 0.0009567897650413215
iteration 165, loss = 0.0008325491799041629
iteration 166, loss = 0.0011169558856636286
iteration 167, loss = 0.0008652909309603274
iteration 168, loss = 0.0007714817766100168
iteration 169, loss = 0.000721401593182236
iteration 170, loss = 0.0006835382664576173
iteration 171, loss = 0.001516630407422781
iteration 172, loss = 0.0007623427081853151
iteration 173, loss = 0.001065842225216329
iteration 174, loss = 0.001302804215811193
iteration 175, loss = 0.0012898477725684643
iteration 176, loss = 0.000753088912460953
iteration 177, loss = 0.0009582209750078619
iteration 178, loss = 0.000792933045886457
iteration 179, loss = 0.0010709988418966532
iteration 180, loss = 0.001669440884143114
iteration 181, loss = 0.0008105697925202549
iteration 182, loss = 0.0011236132122576237
iteration 183, loss = 0.0008494535577483475
iteration 184, loss = 0.0010998847428709269
iteration 185, loss = 0.0009766535367816687
iteration 186, loss = 0.001424799906089902
iteration 187, loss = 0.00132115522865206
iteration 188, loss = 0.0011770430719479918
iteration 189, loss = 0.001246407744474709
iteration 190, loss = 0.0009597012540325522
iteration 191, loss = 0.0009526615613140166
iteration 192, loss = 0.0008621072047390044
iteration 193, loss = 0.0009509763913229108
iteration 194, loss = 0.0008169797947630286
iteration 195, loss = 0.0010070977732539177
iteration 196, loss = 0.0007942185620777309
iteration 197, loss = 0.0011763618094846606
iteration 198, loss = 0.0007074925815686584
iteration 199, loss = 0.000744278309866786
iteration 200, loss = 0.0009572773706167936
iteration 201, loss = 0.0011108122998848557
iteration 202, loss = 0.0007731991936452687
iteration 203, loss = 0.0007307483465410769
iteration 204, loss = 0.0015537070576101542
iteration 205, loss = 0.0008920834516175091
iteration 206, loss = 0.0009397303219884634
iteration 207, loss = 0.0015705942641943693
iteration 208, loss = 0.0008196370908990502
iteration 209, loss = 0.0009100297465920448
iteration 210, loss = 0.0012451789807528257
iteration 211, loss = 0.0008134902454912663
iteration 212, loss = 0.000902537431102246
iteration 213, loss = 0.0007679323316551745
iteration 214, loss = 0.0009294794872403145
iteration 215, loss = 0.0009259229991585016
iteration 216, loss = 0.001404085662215948
iteration 217, loss = 0.0010826017241925001
iteration 218, loss = 0.000847437942866236
iteration 219, loss = 0.0008345281821675599
iteration 220, loss = 0.0015696545597165823
iteration 221, loss = 0.0008542826399207115
iteration 222, loss = 0.0011103276629000902
iteration 223, loss = 0.0009965957142412663
iteration 224, loss = 0.000733663619030267
iteration 225, loss = 0.0008597825653851032
iteration 226, loss = 0.0006519089220091701
iteration 227, loss = 0.0008428413420915604
iteration 228, loss = 0.0008412851602770388
iteration 229, loss = 0.0009032903471961617
iteration 230, loss = 0.000659398443531245
iteration 231, loss = 0.0008558721165172756
iteration 232, loss = 0.000776635657530278
iteration 233, loss = 0.0016594934277236462
iteration 234, loss = 0.000815258885268122
iteration 235, loss = 0.0006789531325921416
iteration 236, loss = 0.0008322351495735347
iteration 237, loss = 0.0016560425283387303
iteration 238, loss = 0.0009810711489990354
iteration 239, loss = 0.0008975435048341751
iteration 240, loss = 0.0008782627992331982
iteration 241, loss = 0.0009475398692302406
iteration 242, loss = 0.0009143500938080251
iteration 243, loss = 0.0008829942671582103
iteration 244, loss = 0.0020207802299410105
iteration 245, loss = 0.0008011754835024476
iteration 246, loss = 0.001615686109289527
iteration 247, loss = 0.0009248527348972857
iteration 248, loss = 0.0008552545914426446
iteration 249, loss = 0.0007964250980876386
iteration 250, loss = 0.000640053884126246
iteration 251, loss = 0.000940095167607069
iteration 252, loss = 0.0006721609970554709
iteration 253, loss = 0.0016096215695142746
iteration 254, loss = 0.0007487713592126966
iteration 255, loss = 0.0011335238814353943
iteration 256, loss = 0.0007922867662273347
iteration 257, loss = 0.001096986117772758
iteration 258, loss = 0.001450417097657919
iteration 259, loss = 0.0009222451481036842
iteration 260, loss = 0.0007925237878225744
iteration 261, loss = 0.0009719911031424999
iteration 262, loss = 0.0008436845964752138
iteration 263, loss = 0.0012126099318265915
iteration 264, loss = 0.0008178515126928687
iteration 265, loss = 0.0016493045259267092
iteration 266, loss = 0.0016438952879980206
iteration 267, loss = 0.0008217028807848692
iteration 268, loss = 0.0008137896074913442
iteration 269, loss = 0.0009568905807100236
iteration 270, loss = 0.0009062569006346166
iteration 271, loss = 0.000767549208831042
iteration 272, loss = 0.000799342873506248
iteration 273, loss = 0.0007639431860297918
iteration 274, loss = 0.0007260879501700401
iteration 275, loss = 0.0010894100414589047
iteration 276, loss = 0.0008740357588976622
iteration 277, loss = 0.0011128244223073125
iteration 278, loss = 0.0010002346243709326
iteration 279, loss = 0.0008766156388446689
iteration 280, loss = 0.0008139676647260785
iteration 281, loss = 0.0013011196861043572
iteration 282, loss = 0.0010490620043128729
iteration 283, loss = 0.0007738275453448296
iteration 284, loss = 0.0006527443183586001
iteration 285, loss = 0.0008835512562654912
iteration 286, loss = 0.0016689264448359609
iteration 287, loss = 0.0010637285886332393
iteration 288, loss = 0.0009153646533377469
iteration 289, loss = 0.0010425345972180367
iteration 290, loss = 0.001588036073371768
iteration 291, loss = 0.0008473974303342402
iteration 292, loss = 0.0007395592983812094
iteration 293, loss = 0.0007862990023568273
iteration 294, loss = 0.0012455201940611005
iteration 295, loss = 0.000877772516105324
iteration 296, loss = 0.0007214451325125992
iteration 297, loss = 0.0012996504083275795
iteration 298, loss = 0.001104842172935605
iteration 299, loss = 0.000761714531108737
iteration 300, loss = 0.001215730793774128
iteration 1, loss = 0.001531388028524816
iteration 2, loss = 0.001978392945602536
iteration 3, loss = 0.0008104730513878167
iteration 4, loss = 0.0008497302769683301
iteration 5, loss = 0.0007894765003584325
iteration 6, loss = 0.0007449447875842452
iteration 7, loss = 0.0009320401004515588
iteration 8, loss = 0.001026711892336607
iteration 9, loss = 0.0007635354995727539
iteration 10, loss = 0.00069761264603585
iteration 11, loss = 0.001081290771253407
iteration 12, loss = 0.0008720211335457861
iteration 13, loss = 0.0010271843057125807
iteration 14, loss = 0.0010130616137757897
iteration 15, loss = 0.000917796220164746
iteration 16, loss = 0.0010722328443080187
iteration 17, loss = 0.000983048346824944
iteration 18, loss = 0.0008108709007501602
iteration 19, loss = 0.0008498512906953692
iteration 20, loss = 0.0008293889695778489
iteration 21, loss = 0.0008161758305504918
iteration 22, loss = 0.0008863328257575631
iteration 23, loss = 0.0009097548318095505
iteration 24, loss = 0.0008764889789745212
iteration 25, loss = 0.0011939832475036383
iteration 26, loss = 0.000796878186520189
iteration 27, loss = 0.0008767395047470927
iteration 28, loss = 0.0006797997048124671
iteration 29, loss = 0.0018842248246073723
iteration 30, loss = 0.0011013170005753636
iteration 31, loss = 0.001075535430572927
iteration 32, loss = 0.0009498050203546882
iteration 33, loss = 0.0009498858125880361
iteration 34, loss = 0.0010197258088737726
iteration 35, loss = 0.0007344469777308404
iteration 36, loss = 0.000717688468284905
iteration 37, loss = 0.0006545702344737947
iteration 38, loss = 0.0011811056174337864
iteration 39, loss = 0.0011466930154711008
iteration 40, loss = 0.0010355963604524732
iteration 41, loss = 0.0008619315922260284
iteration 42, loss = 0.0008576486725360155
iteration 43, loss = 0.0009625471429899335
iteration 44, loss = 0.0010333036771044135
iteration 45, loss = 0.0008455273346044123
iteration 46, loss = 0.0008466921863146126
iteration 47, loss = 0.0010236126836389303
iteration 48, loss = 0.0008276669541373849
iteration 49, loss = 0.001449612551368773
iteration 50, loss = 0.0011097061214968562
iteration 51, loss = 0.0008425669511780143
iteration 52, loss = 0.0008042232948355377
iteration 53, loss = 0.0014475560747087002
iteration 54, loss = 0.0010045168455690145
iteration 55, loss = 0.0007103900425136089
iteration 56, loss = 0.001192901749163866
iteration 57, loss = 0.0009388838079757988
iteration 58, loss = 0.0008732480928301811
iteration 59, loss = 0.0008304347284138203
iteration 60, loss = 0.0007680655689910054
iteration 61, loss = 0.0006522843614220619
iteration 62, loss = 0.0007540480000898242
iteration 63, loss = 0.0014728618552908301
iteration 64, loss = 0.0010530110448598862
iteration 65, loss = 0.0007689620833843946
iteration 66, loss = 0.0007282088627107441
iteration 67, loss = 0.0006774206412956119
iteration 68, loss = 0.0009046329651027918
iteration 69, loss = 0.000866061367560178
iteration 70, loss = 0.000834586622659117
iteration 71, loss = 0.0007754156831651926
iteration 72, loss = 0.0011085181031376123
iteration 73, loss = 0.0014871673192828894
iteration 74, loss = 0.001200870261527598
iteration 75, loss = 0.0007386026554740965
iteration 76, loss = 0.0009622477809898555
iteration 77, loss = 0.0011762625072151423
iteration 78, loss = 0.0012172939023002982
iteration 79, loss = 0.0009586312808096409
iteration 80, loss = 0.001641043578274548
iteration 81, loss = 0.0007831458351574838
iteration 82, loss = 0.0009509038645774126
iteration 83, loss = 0.0007247505709528923
iteration 84, loss = 0.000826326257083565
iteration 85, loss = 0.0014903807314112782
iteration 86, loss = 0.0010572690516710281
iteration 87, loss = 0.0008388673304580152
iteration 88, loss = 0.0017232195241376758
iteration 89, loss = 0.000893818330951035
iteration 90, loss = 0.0009462613379582763
iteration 91, loss = 0.0009791882475838065
iteration 92, loss = 0.0008192856330424547
iteration 93, loss = 0.0008847134304232895
iteration 94, loss = 0.0008503632270731032
iteration 95, loss = 0.0007954718894325197
iteration 96, loss = 0.0009416688117198646
iteration 97, loss = 0.0016234368085861206
iteration 98, loss = 0.001137705985456705
iteration 99, loss = 0.0010953641030937433
iteration 100, loss = 0.0014778062468394637
iteration 101, loss = 0.0008462606929242611
iteration 102, loss = 0.000882122665643692
iteration 103, loss = 0.0009001312428154051
iteration 104, loss = 0.0008125812746584415
iteration 105, loss = 0.0009474755497649312
iteration 106, loss = 0.0012153837596997619
iteration 107, loss = 0.0007954966276884079
iteration 108, loss = 0.0007504438981413841
iteration 109, loss = 0.0012840802082791924
iteration 110, loss = 0.0007293950766324997
iteration 111, loss = 0.0007573348702862859
iteration 112, loss = 0.0006869328208267689
iteration 113, loss = 0.0008703485946170986
iteration 114, loss = 0.0017149026971310377
iteration 115, loss = 0.0008905013091862202
iteration 116, loss = 0.0016697219107300043
iteration 117, loss = 0.0008107699686661363
iteration 118, loss = 0.0009113571140915155
iteration 119, loss = 0.0008739503682591021
iteration 120, loss = 0.0012500039301812649
iteration 121, loss = 0.0010963266249746084
iteration 122, loss = 0.0007991816382855177
iteration 123, loss = 0.0007804019842296839
iteration 124, loss = 0.0008458638330921531
iteration 125, loss = 0.0008134291274473071
iteration 126, loss = 0.0007627725135535002
iteration 127, loss = 0.0017308557871729136
iteration 128, loss = 0.0010149480076506734
iteration 129, loss = 0.0010081784566864371
iteration 130, loss = 0.0008390081347897649
iteration 131, loss = 0.001007587881758809
iteration 132, loss = 0.0007318048737943172
iteration 133, loss = 0.0007749489159323275
iteration 134, loss = 0.0008070198819041252
iteration 135, loss = 0.0008314384031109512
iteration 136, loss = 0.0008184399921447039
iteration 137, loss = 0.0010819024173542857
iteration 138, loss = 0.0016071038553491235
iteration 139, loss = 0.0010991324670612812
iteration 140, loss = 0.000774744083173573
iteration 141, loss = 0.0006954827695153654
iteration 142, loss = 0.0010741031728684902
iteration 143, loss = 0.002133409259840846
iteration 144, loss = 0.0007730508805252612
iteration 145, loss = 0.0009182330104522407
iteration 146, loss = 0.0011141938157379627
iteration 147, loss = 0.0007704625022597611
iteration 148, loss = 0.0014721957268193364
iteration 149, loss = 0.0009447034681215882
iteration 150, loss = 0.0012224335223436356
iteration 151, loss = 0.0009915863629430532
iteration 152, loss = 0.000851630000397563
iteration 153, loss = 0.0009395885281264782
iteration 154, loss = 0.0006788476603105664
iteration 155, loss = 0.0007979560759849846
iteration 156, loss = 0.0010096478508785367
iteration 157, loss = 0.0009260118822567165
iteration 158, loss = 0.0007177962688729167
iteration 159, loss = 0.0007832456612959504
iteration 160, loss = 0.001150802243500948
iteration 161, loss = 0.000644313171505928
iteration 162, loss = 0.0011401697993278503
iteration 163, loss = 0.0010629440657794476
iteration 164, loss = 0.0015943534672260284
iteration 165, loss = 0.0006447548512369394
iteration 166, loss = 0.0015295682242140174
iteration 167, loss = 0.001082706032320857
iteration 168, loss = 0.0011578684207051992
iteration 169, loss = 0.0009229546412825584
iteration 170, loss = 0.0007865119259804487
iteration 171, loss = 0.0010122739477083087
iteration 172, loss = 0.0008621332235634327
iteration 173, loss = 0.0009503677720203996
iteration 174, loss = 0.0009246728150174022
iteration 175, loss = 0.0008271642727777362
iteration 176, loss = 0.001547237508930266
iteration 177, loss = 0.000740945222787559
iteration 178, loss = 0.0007437568274326622
iteration 179, loss = 0.001821586163714528
iteration 180, loss = 0.0007968057761900127
iteration 181, loss = 0.0009727885480970144
iteration 182, loss = 0.0006652618758380413
iteration 183, loss = 0.0008259427268058062
iteration 184, loss = 0.0009671747102402151
iteration 185, loss = 0.000957249547354877
iteration 186, loss = 0.0007745363982394338
iteration 187, loss = 0.0012504968326538801
iteration 188, loss = 0.0007830516551621258
iteration 189, loss = 0.0008492513443343341
iteration 190, loss = 0.0007582203834317625
iteration 191, loss = 0.0009298335644416511
iteration 192, loss = 0.0006392489303834736
iteration 193, loss = 0.000859687221236527
iteration 194, loss = 0.0008571775979362428
iteration 195, loss = 0.0007699559209868312
iteration 196, loss = 0.000983587116934359
iteration 197, loss = 0.0007706963806413114
iteration 198, loss = 0.0009860731661319733
iteration 199, loss = 0.0007135949563235044
iteration 200, loss = 0.0014061153633520007
iteration 201, loss = 0.0009732529288157821
iteration 202, loss = 0.0007649899343959987
iteration 203, loss = 0.000867529190145433
iteration 204, loss = 0.0007171446923166513
iteration 205, loss = 0.0007008995162323117
iteration 206, loss = 0.0009658730123192072
iteration 207, loss = 0.0007594624767079949
iteration 208, loss = 0.0007692037033848464
iteration 209, loss = 0.001261063851416111
iteration 210, loss = 0.0008854103507474065
iteration 211, loss = 0.0023029823787510395
iteration 212, loss = 0.0011521911947056651
iteration 213, loss = 0.0008445181883871555
iteration 214, loss = 0.0007885883096605539
iteration 215, loss = 0.0008894067723304033
iteration 216, loss = 0.0010004343930631876
iteration 217, loss = 0.0007934192544780672
iteration 218, loss = 0.0007270214264281094
iteration 219, loss = 0.0015010712668299675
iteration 220, loss = 0.0008068084716796875
iteration 221, loss = 0.0009889184730127454
iteration 222, loss = 0.0007617942756041884
iteration 223, loss = 0.0012182838981971145
iteration 224, loss = 0.0009076418355107307
iteration 225, loss = 0.002600966952741146
iteration 226, loss = 0.0008771734428592026
iteration 227, loss = 0.0007723365561105311
iteration 228, loss = 0.0013752910308539867
iteration 229, loss = 0.0007125831325538456
iteration 230, loss = 0.0008905526483431458
iteration 231, loss = 0.0008299686596728861
iteration 232, loss = 0.0011893121991306543
iteration 233, loss = 0.0008986127795651555
iteration 234, loss = 0.0007491351570934057
iteration 235, loss = 0.0008395521435886621
iteration 236, loss = 0.0018844169098883867
iteration 237, loss = 0.0010388136142864823
iteration 238, loss = 0.0010648907627910376
iteration 239, loss = 0.0009748880984261632
iteration 240, loss = 0.0007353037944994867
iteration 241, loss = 0.0006646026158705354
iteration 242, loss = 0.0007377932779490948
iteration 243, loss = 0.0014513744972646236
iteration 244, loss = 0.0012606412637978792
iteration 245, loss = 0.0006463498575612903
iteration 246, loss = 0.0010412497213110328
iteration 247, loss = 0.0008726008236408234
iteration 248, loss = 0.00075741350883618
iteration 249, loss = 0.0011444883421063423
iteration 250, loss = 0.0009418284753337502
iteration 251, loss = 0.0009876618860289454
iteration 252, loss = 0.0008060801774263382
iteration 253, loss = 0.0008276482694782317
iteration 254, loss = 0.0009570852853357792
iteration 255, loss = 0.0009800944244489074
iteration 256, loss = 0.001002425909973681
iteration 257, loss = 0.0015142515767365694
iteration 258, loss = 0.001004085410386324
iteration 259, loss = 0.0008278051391243935
iteration 260, loss = 0.00075157405808568
iteration 261, loss = 0.000754607783164829
iteration 262, loss = 0.0009415593231096864
iteration 263, loss = 0.0007242740248329937
iteration 264, loss = 0.0009024817263707519
iteration 265, loss = 0.000905202585272491
iteration 266, loss = 0.0015737236244603992
iteration 267, loss = 0.0007914151065051556
iteration 268, loss = 0.000706975522916764
iteration 269, loss = 0.0009232647716999054
iteration 270, loss = 0.0009653135202825069
iteration 271, loss = 0.0011651618406176567
iteration 272, loss = 0.0010137947974726558
iteration 273, loss = 0.000864679052028805
iteration 274, loss = 0.0011039687087759376
iteration 275, loss = 0.0011312342248857021
iteration 276, loss = 0.0009777462109923363
iteration 277, loss = 0.0009972713887691498
iteration 278, loss = 0.0006855418323539197
iteration 279, loss = 0.0010488920379430056
iteration 280, loss = 0.0011440693633630872
iteration 281, loss = 0.0007586483261547983
iteration 282, loss = 0.0008215989219024777
iteration 283, loss = 0.0008904497954063118
iteration 284, loss = 0.0007004148210398853
iteration 285, loss = 0.0007152805919758976
iteration 286, loss = 0.0009102476178668439
iteration 287, loss = 0.000945058825891465
iteration 288, loss = 0.0010165009880438447
iteration 289, loss = 0.001124463975429535
iteration 290, loss = 0.000759777904022485
iteration 291, loss = 0.001103439717553556
iteration 292, loss = 0.0008780899806879461
iteration 293, loss = 0.0007523357053287327
iteration 294, loss = 0.0009543969063088298
iteration 295, loss = 0.0008338959887623787
iteration 296, loss = 0.0008489980828016996
iteration 297, loss = 0.0007525641703978181
iteration 298, loss = 0.0008764176163822412
iteration 299, loss = 0.0017168261110782623
iteration 300, loss = 0.0009818973485380411
iteration 1, loss = 0.0008957710233516991
iteration 2, loss = 0.0010940685169771314
iteration 3, loss = 0.0011399360373616219
iteration 4, loss = 0.0009142989292740822
iteration 5, loss = 0.0007597854710184038
iteration 6, loss = 0.0007620269316248596
iteration 7, loss = 0.0011846520937979221
iteration 8, loss = 0.0008598858257755637
iteration 9, loss = 0.0008419299847446382
iteration 10, loss = 0.0008146423497237265
iteration 11, loss = 0.0007090838043950498
iteration 12, loss = 0.0007199192186817527
iteration 13, loss = 0.0007062284857966006
iteration 14, loss = 0.0008480268879793584
iteration 15, loss = 0.0007025575614534318
iteration 16, loss = 0.0010193500202149153
iteration 17, loss = 0.0008496220107190311
iteration 18, loss = 0.0011575774988159537
iteration 19, loss = 0.0009076762362383306
iteration 20, loss = 0.0008860827074386179
iteration 21, loss = 0.0014454841148108244
iteration 22, loss = 0.0008126477478072047
iteration 23, loss = 0.0006206220714375377
iteration 24, loss = 0.0008030739845708013
iteration 25, loss = 0.0008169405627995729
iteration 26, loss = 0.0008182736928574741
iteration 27, loss = 0.0007389513193629682
iteration 28, loss = 0.0006772053311578929
iteration 29, loss = 0.0010820821626111865
iteration 30, loss = 0.0006982069462537766
iteration 31, loss = 0.0007399703026749194
iteration 32, loss = 0.0009201291832141578
iteration 33, loss = 0.0007346643833443522
iteration 34, loss = 0.000810550875030458
iteration 35, loss = 0.0007720057037658989
iteration 36, loss = 0.0009453013772144914
iteration 37, loss = 0.0008090937044471502
iteration 38, loss = 0.0015036948025226593
iteration 39, loss = 0.000973077432718128
iteration 40, loss = 0.0009447701741009951
iteration 41, loss = 0.0007820753962732852
iteration 42, loss = 0.0009226927068084478
iteration 43, loss = 0.0011531838681548834
iteration 44, loss = 0.0008172865491360426
iteration 45, loss = 0.0008697886951267719
iteration 46, loss = 0.001181664178147912
iteration 47, loss = 0.0008324047084897757
iteration 48, loss = 0.0010337160201743245
iteration 49, loss = 0.0009415230015292764
iteration 50, loss = 0.0007875484880059958
iteration 51, loss = 0.0006915868143551052
iteration 52, loss = 0.0009226295514963567
iteration 53, loss = 0.000895960140042007
iteration 54, loss = 0.0010609229793772101
iteration 55, loss = 0.0013789336662739515
iteration 56, loss = 0.0015715749468654394
iteration 57, loss = 0.0010749949142336845
iteration 58, loss = 0.0009336837101727724
iteration 59, loss = 0.0008530573104508221
iteration 60, loss = 0.0007222486310638487
iteration 61, loss = 0.0009369284962303936
iteration 62, loss = 0.0016886636149138212
iteration 63, loss = 0.001202801475301385
iteration 64, loss = 0.0010998278157785535
iteration 65, loss = 0.00087423634249717
iteration 66, loss = 0.0007668022881262004
iteration 67, loss = 0.0009031082736328244
iteration 68, loss = 0.0007792297983542085
iteration 69, loss = 0.0008816819754429162
iteration 70, loss = 0.0012321831891313195
iteration 71, loss = 0.0016143778339028358
iteration 72, loss = 0.0007416910957545042
iteration 73, loss = 0.0007005673251114786
iteration 74, loss = 0.0013752772938460112
iteration 75, loss = 0.0013570119626820087
iteration 76, loss = 0.000725322577636689
iteration 77, loss = 0.001782882260158658
iteration 78, loss = 0.0007600773824378848
iteration 79, loss = 0.0009371198248118162
iteration 80, loss = 0.0009941544849425554
iteration 81, loss = 0.0015694068279117346
iteration 82, loss = 0.0008824511896818876
iteration 83, loss = 0.0009286239510402083
iteration 84, loss = 0.0009525956702418625
iteration 85, loss = 0.0007389318197965622
iteration 86, loss = 0.0007734416285529733
iteration 87, loss = 0.0011798060731962323
iteration 88, loss = 0.0008428690489381552
iteration 89, loss = 0.001033714390359819
iteration 90, loss = 0.0007432467537000775
iteration 91, loss = 0.0016539175994694233
iteration 92, loss = 0.0008873397018760443
iteration 93, loss = 0.0006939050508663058
iteration 94, loss = 0.0011489943135529757
iteration 95, loss = 0.0007477034232579172
iteration 96, loss = 0.0016221674159169197
iteration 97, loss = 0.0007608592277392745
iteration 98, loss = 0.0007232452044263482
iteration 99, loss = 0.0007609354797750711
iteration 100, loss = 0.0008903458365239203
iteration 101, loss = 0.0008308707037940621
iteration 102, loss = 0.0008458898519165814
iteration 103, loss = 0.0017401259392499924
iteration 104, loss = 0.0008054658537730575
iteration 105, loss = 0.0008967338362708688
iteration 106, loss = 0.0008736781310290098
iteration 107, loss = 0.0008839573711156845
iteration 108, loss = 0.0007473415462300181
iteration 109, loss = 0.0009852467337623239
iteration 110, loss = 0.0008310792036354542
iteration 111, loss = 0.0008388920687139034
iteration 112, loss = 0.0012764480197802186
iteration 113, loss = 0.00088646070798859
iteration 114, loss = 0.0009302982944063842
iteration 115, loss = 0.0016437165904790163
iteration 116, loss = 0.0009622262441553175
iteration 117, loss = 0.0009688368299975991
iteration 118, loss = 0.0008131426875479519
iteration 119, loss = 0.0012776528019458055
iteration 120, loss = 0.0007911630091257393
iteration 121, loss = 0.0010050319833680987
iteration 122, loss = 0.0007761407759971917
iteration 123, loss = 0.0010694649536162615
iteration 124, loss = 0.0008520290139131248
iteration 125, loss = 0.00078875373583287
iteration 126, loss = 0.0008976133540272713
iteration 127, loss = 0.0007701845024712384
iteration 128, loss = 0.000971987668890506
iteration 129, loss = 0.0012728634756058455
iteration 130, loss = 0.0009937012800946832
iteration 131, loss = 0.0012794225476682186
iteration 132, loss = 0.0012295115739107132
iteration 133, loss = 0.00154959037899971
iteration 134, loss = 0.00096368498634547
iteration 135, loss = 0.0008409546571783721
iteration 136, loss = 0.0007892592111602426
iteration 137, loss = 0.0008556328248232603
iteration 138, loss = 0.0008955350494943559
iteration 139, loss = 0.0009167060488834977
iteration 140, loss = 0.000821291352622211
iteration 141, loss = 0.0008606994524598122
iteration 142, loss = 0.001144916983321309
iteration 143, loss = 0.0007973757456056774
iteration 144, loss = 0.0008125713211484253
iteration 145, loss = 0.0016417637234553695
iteration 146, loss = 0.0009456605766899884
iteration 147, loss = 0.0011140647111460567
iteration 148, loss = 0.0008455351926386356
iteration 149, loss = 0.0013139282818883657
iteration 150, loss = 0.0016643318813294172
iteration 151, loss = 0.0008712803246453404
iteration 152, loss = 0.0008183917962014675
iteration 153, loss = 0.0008205128833651543
iteration 154, loss = 0.0008662420441396534
iteration 155, loss = 0.0015235452447086573
iteration 156, loss = 0.0007072977023199201
iteration 157, loss = 0.0008584475144743919
iteration 158, loss = 0.0009078422444872558
iteration 159, loss = 0.0007538511417806149
iteration 160, loss = 0.0013857381418347359
iteration 161, loss = 0.0018863670993596315
iteration 162, loss = 0.0008288160315714777
iteration 163, loss = 0.0008678167941980064
iteration 164, loss = 0.0010799301089718938
iteration 165, loss = 0.00132485362701118
iteration 166, loss = 0.0009128974634222686
iteration 167, loss = 0.0014530536718666553
iteration 168, loss = 0.0008163155289366841
iteration 169, loss = 0.00115691008977592
iteration 170, loss = 0.0010596815263852477
iteration 171, loss = 0.0007923082448542118
iteration 172, loss = 0.0015921498415991664
iteration 173, loss = 0.0007344964542426169
iteration 174, loss = 0.0008085054578259587
iteration 175, loss = 0.0007981855305843055
iteration 176, loss = 0.0009884684113785625
iteration 177, loss = 0.0007987380959093571
iteration 178, loss = 0.000996662420220673
iteration 179, loss = 0.0008921765838749707
iteration 180, loss = 0.000783812312874943
iteration 181, loss = 0.0008550143102183938
iteration 182, loss = 0.0007545127300545573
iteration 183, loss = 0.0011678180890157819
iteration 184, loss = 0.0010648572351783514
iteration 185, loss = 0.0007570999441668391
iteration 186, loss = 0.0008753734291531146
iteration 187, loss = 0.0008841429953463376
iteration 188, loss = 0.0010746532352641225
iteration 189, loss = 0.0013518949272111058
iteration 190, loss = 0.0013973858440294862
iteration 191, loss = 0.0012071229284629226
iteration 192, loss = 0.0013917024480178952
iteration 193, loss = 0.0009866972686722875
iteration 194, loss = 0.0009711507009342313
iteration 195, loss = 0.0007047278340905905
iteration 196, loss = 0.0016501398058608174
iteration 197, loss = 0.0008427360444329679
iteration 198, loss = 0.0007544670370407403
iteration 199, loss = 0.0008495469810441136
iteration 200, loss = 0.000871655996888876
iteration 201, loss = 0.0008149787317961454
iteration 202, loss = 0.0011881773825734854
iteration 203, loss = 0.0009734897175803781
iteration 204, loss = 0.0011064744321629405
iteration 205, loss = 0.0010414512362331152
iteration 206, loss = 0.0015440955758094788
iteration 207, loss = 0.0008465460268780589
iteration 208, loss = 0.0009949723025783896
iteration 209, loss = 0.0007226967136375606
iteration 210, loss = 0.0007375659188255668
iteration 211, loss = 0.0008956848178058863
iteration 212, loss = 0.0007604791317135096
iteration 213, loss = 0.0006236410117708147
iteration 214, loss = 0.000828135700430721
iteration 215, loss = 0.0009502390748821199
iteration 216, loss = 0.0009669322753325105
iteration 217, loss = 0.0007575228228233755
iteration 218, loss = 0.0011597821721807122
iteration 219, loss = 0.001605343190021813
iteration 220, loss = 0.001013482571579516
iteration 221, loss = 0.0012616636231541634
iteration 222, loss = 0.0009373308857902884
iteration 223, loss = 0.0008743138751015067
iteration 224, loss = 0.0008162225130945444
iteration 225, loss = 0.00102713773958385
iteration 226, loss = 0.001100663561373949
iteration 227, loss = 0.0008631962118670344
iteration 228, loss = 0.0008160690194927156
iteration 229, loss = 0.000984598184004426
iteration 230, loss = 0.0009767732117325068
iteration 231, loss = 0.0008237406727857888
iteration 232, loss = 0.0008149771601893008
iteration 233, loss = 0.0007516885525546968
iteration 234, loss = 0.0010541783412918448
iteration 235, loss = 0.0008223953191190958
iteration 236, loss = 0.0008950454066507518
iteration 237, loss = 0.0007593607879243791
iteration 238, loss = 0.0009800336556509137
iteration 239, loss = 0.0012174044968560338
iteration 240, loss = 0.0017123905709013343
iteration 241, loss = 0.0008554793894290924
iteration 242, loss = 0.000898789323400706
iteration 243, loss = 0.000749658327549696
iteration 244, loss = 0.0008677297737449408
iteration 245, loss = 0.0011247432557865977
iteration 246, loss = 0.0008566128090023994
iteration 247, loss = 0.001033758744597435
iteration 248, loss = 0.0011257511796429753
iteration 249, loss = 0.0007073145243339241
iteration 250, loss = 0.0010974232573062181
iteration 251, loss = 0.0007609822787344456
iteration 252, loss = 0.0006879926077090204
iteration 253, loss = 0.0016851852415129542
iteration 254, loss = 0.0008644879562780261
iteration 255, loss = 0.0013138485373929143
iteration 256, loss = 0.0009072460816241801
iteration 257, loss = 0.0007826400105841458
iteration 258, loss = 0.0009547016816213727
iteration 259, loss = 0.0008118774276226759
iteration 260, loss = 0.0008572514052502811
iteration 261, loss = 0.0008468617452308536
iteration 262, loss = 0.001029490609653294
iteration 263, loss = 0.0008251768886111677
iteration 264, loss = 0.001082415459677577
iteration 265, loss = 0.000988180749118328
iteration 266, loss = 0.0009272681782022119
iteration 267, loss = 0.0011066150618717074
iteration 268, loss = 0.0009523693006485701
iteration 269, loss = 0.0009476528503000736
iteration 270, loss = 0.0014961699489504099
iteration 271, loss = 0.0008751432760618627
iteration 272, loss = 0.0009703966788947582
iteration 273, loss = 0.0010741875739768147
iteration 274, loss = 0.0010478319600224495
iteration 275, loss = 0.000953440205194056
iteration 276, loss = 0.0009032053058035672
iteration 277, loss = 0.0009200014173984528
iteration 278, loss = 0.0008046879665926099
iteration 279, loss = 0.0007773737888783216
iteration 280, loss = 0.0006805344601161778
iteration 281, loss = 0.0008096814854070544
iteration 282, loss = 0.0008486005826853216
iteration 283, loss = 0.0007776978309266269
iteration 284, loss = 0.0009886461775749922
iteration 285, loss = 0.00098207697737962
iteration 286, loss = 0.0010558683425188065
iteration 287, loss = 0.0008522585267201066
iteration 288, loss = 0.0016185196582227945
iteration 289, loss = 0.0010459128534421325
iteration 290, loss = 0.0008485117577947676
iteration 291, loss = 0.0011755824089050293
iteration 292, loss = 0.0008053251076489687
iteration 293, loss = 0.0008514787768945098
iteration 294, loss = 0.0016068480908870697
iteration 295, loss = 0.0015422129072248936
iteration 296, loss = 0.0007114634499885142
iteration 297, loss = 0.0008486825972795486
iteration 298, loss = 0.0009261739323846996
iteration 299, loss = 0.0008757827454246581
iteration 300, loss = 0.0007892978028394282
iteration 1, loss = 0.0009717437205836177
iteration 2, loss = 0.0009125472279265523
iteration 3, loss = 0.0009213027660734951
iteration 4, loss = 0.0009391889325343072
iteration 5, loss = 0.0011611997615545988
iteration 6, loss = 0.0013624659040942788
iteration 7, loss = 0.000998360337689519
iteration 8, loss = 0.0008257861481979489
iteration 9, loss = 0.001976259984076023
iteration 10, loss = 0.0008710862603038549
iteration 11, loss = 0.0008732462883926928
iteration 12, loss = 0.0008906626608222723
iteration 13, loss = 0.0009755256469361484
iteration 14, loss = 0.0007670465274713933
iteration 15, loss = 0.0007735327817499638
iteration 16, loss = 0.000763163436204195
iteration 17, loss = 0.001751971896737814
iteration 18, loss = 0.000954657094553113
iteration 19, loss = 0.0006245702388696373
iteration 20, loss = 0.0009350168402306736
iteration 21, loss = 0.0013073354493826628
iteration 22, loss = 0.0009506497299298644
iteration 23, loss = 0.0012019905261695385
iteration 24, loss = 0.0009397834073752165
iteration 25, loss = 0.001333665451966226
iteration 26, loss = 0.0012984871864318848
iteration 27, loss = 0.0007467610412277281
iteration 28, loss = 0.0009954813867807388
iteration 29, loss = 0.0010652870405465364
iteration 30, loss = 0.0009100846364162862
iteration 31, loss = 0.0008478988311253488
iteration 32, loss = 0.000879560480825603
iteration 33, loss = 0.0009294344927184284
iteration 34, loss = 0.001016578753478825
iteration 35, loss = 0.0007645848672837019
iteration 36, loss = 0.0008239673334173858
iteration 37, loss = 0.001182455220259726
iteration 38, loss = 0.0008009621524251997
iteration 39, loss = 0.001665328280068934
iteration 40, loss = 0.001199317048303783
iteration 41, loss = 0.0007843244238756597
iteration 42, loss = 0.0010890037519857287
iteration 43, loss = 0.0008856744971126318
iteration 44, loss = 0.0006917688297107816
iteration 45, loss = 0.0008482288103550673
iteration 46, loss = 0.0008635708363726735
iteration 47, loss = 0.0015188141260296106
iteration 48, loss = 0.0007729802164249122
iteration 49, loss = 0.0007620680844411254
iteration 50, loss = 0.0010985424742102623
iteration 51, loss = 0.0009437188855372369
iteration 52, loss = 0.0007480069762095809
iteration 53, loss = 0.0009016139665618539
iteration 54, loss = 0.0008768714033067226
iteration 55, loss = 0.001059071160852909
iteration 56, loss = 0.0014431050512939692
iteration 57, loss = 0.0007136489148251712
iteration 58, loss = 0.000830918550491333
iteration 59, loss = 0.0008419443038292229
iteration 60, loss = 0.0016030747210606933
iteration 61, loss = 0.0006992848357185721
iteration 62, loss = 0.0010214108042418957
iteration 63, loss = 0.0016639510868117213
iteration 64, loss = 0.0006148761603981256
iteration 65, loss = 0.000772198080085218
iteration 66, loss = 0.0007925157551653683
iteration 67, loss = 0.0007721055299043655
iteration 68, loss = 0.0016096518374979496
iteration 69, loss = 0.0010518858907744288
iteration 70, loss = 0.0008330523269250989
iteration 71, loss = 0.0009965011849999428
iteration 72, loss = 0.0009287518332712352
iteration 73, loss = 0.0011182361049577594
iteration 74, loss = 0.000838905107229948
iteration 75, loss = 0.000912192917894572
iteration 76, loss = 0.0007327015628106892
iteration 77, loss = 0.0011255532735958695
iteration 78, loss = 0.0007683960720896721
iteration 79, loss = 0.0008639637380838394
iteration 80, loss = 0.0008930428884923458
iteration 81, loss = 0.0007724117604084313
iteration 82, loss = 0.0007301124860532582
iteration 83, loss = 0.0007878323085606098
iteration 84, loss = 0.0007416566950269043
iteration 85, loss = 0.0010585328564047813
iteration 86, loss = 0.0007918043993413448
iteration 87, loss = 0.0007229103939607739
iteration 88, loss = 0.0007457308238372207
iteration 89, loss = 0.0007699706475250423
iteration 90, loss = 0.0009074243134818971
iteration 91, loss = 0.0008043402340263128
iteration 92, loss = 0.0006823858711868525
iteration 93, loss = 0.0010341150918975472
iteration 94, loss = 0.0014013706240803003
iteration 95, loss = 0.0011313495924696326
iteration 96, loss = 0.0018160128965973854
iteration 97, loss = 0.0008200456504710019
iteration 98, loss = 0.000962433114182204
iteration 99, loss = 0.00110674521420151
iteration 100, loss = 0.000734754663426429
iteration 101, loss = 0.0010816850699484348
iteration 102, loss = 0.0009454525425098836
iteration 103, loss = 0.0008154285605996847
iteration 104, loss = 0.0008990895585156977
iteration 105, loss = 0.0019160121446475387
iteration 106, loss = 0.0009825112065300345
iteration 107, loss = 0.0009438272099941969
iteration 108, loss = 0.0007638433598913252
iteration 109, loss = 0.0009184099617414176
iteration 110, loss = 0.0007412730483338237
iteration 111, loss = 0.0008193284156732261
iteration 112, loss = 0.0017052744515240192
iteration 113, loss = 0.000965519342571497
iteration 114, loss = 0.0009501071181148291
iteration 115, loss = 0.0006956679280847311
iteration 116, loss = 0.0007984532276168466
iteration 117, loss = 0.0008643636829219759
iteration 118, loss = 0.0008241105824708939
iteration 119, loss = 0.0007083943928591907
iteration 120, loss = 0.0008764103404246271
iteration 121, loss = 0.00093165721045807
iteration 122, loss = 0.0009288256987929344
iteration 123, loss = 0.0008054510108195245
iteration 124, loss = 0.0008737370953895152
iteration 125, loss = 0.0009573846473358572
iteration 126, loss = 0.0007578929071314633
iteration 127, loss = 0.0009263096726499498
iteration 128, loss = 0.0008657402941025794
iteration 129, loss = 0.0017875914927572012
iteration 130, loss = 0.001027786056511104
iteration 131, loss = 0.0008466976578347385
iteration 132, loss = 0.0010110154980793595
iteration 133, loss = 0.001049240119755268
iteration 134, loss = 0.0006827671313658357
iteration 135, loss = 0.0007815274293534458
iteration 136, loss = 0.0007539594662375748
iteration 137, loss = 0.0007294988608919084
iteration 138, loss = 0.0006442905287258327
iteration 139, loss = 0.0009000092977657914
iteration 140, loss = 0.0008988377521745861
iteration 141, loss = 0.0007518702186644077
iteration 142, loss = 0.001074392581358552
iteration 143, loss = 0.0013278689002618194
iteration 144, loss = 0.0011895040515810251
iteration 145, loss = 0.0010382186155766249
iteration 146, loss = 0.0008505734731443226
iteration 147, loss = 0.0007775356061756611
iteration 148, loss = 0.0011486697476357222
iteration 149, loss = 0.0010653595672920346
iteration 150, loss = 0.0008409970905631781
iteration 151, loss = 0.0007414677529595792
iteration 152, loss = 0.0009374794317409396
iteration 153, loss = 0.000755291897803545
iteration 154, loss = 0.0015907313209027052
iteration 155, loss = 0.001181123428978026
iteration 156, loss = 0.0008439279044978321
iteration 157, loss = 0.0008493611239828169
iteration 158, loss = 0.0016005148645490408
iteration 159, loss = 0.0013368951622396708
iteration 160, loss = 0.0007794933044351637
iteration 161, loss = 0.0014053380582481623
iteration 162, loss = 0.0007542560342699289
iteration 163, loss = 0.0009439982241019607
iteration 164, loss = 0.0012515130219981074
iteration 165, loss = 0.0010018262546509504
iteration 166, loss = 0.0013425202341750264
iteration 167, loss = 0.0008949259063228965
iteration 168, loss = 0.0008376680198125541
iteration 169, loss = 0.0007016235031187534
iteration 170, loss = 0.0009507824433967471
iteration 171, loss = 0.0013071410357952118
iteration 172, loss = 0.0014338381588459015
iteration 173, loss = 0.0009270551381632686
iteration 174, loss = 0.0009024786995723844
iteration 175, loss = 0.0008431700989603996
iteration 176, loss = 0.0009645625832490623
iteration 177, loss = 0.0007126751006580889
iteration 178, loss = 0.001113030593842268
iteration 179, loss = 0.0009029724169522524
iteration 180, loss = 0.0007921238429844379
iteration 181, loss = 0.0007917870534583926
iteration 182, loss = 0.0007622077246196568
iteration 183, loss = 0.0008718111785128713
iteration 184, loss = 0.0008714222931303084
iteration 185, loss = 0.0006821664283052087
iteration 186, loss = 0.0008312510908581316
iteration 187, loss = 0.0008484243880957365
iteration 188, loss = 0.00234911497682333
iteration 189, loss = 0.0008977832621894777
iteration 190, loss = 0.0009513535769656301
iteration 191, loss = 0.000810368568636477
iteration 192, loss = 0.0011637252755463123
iteration 193, loss = 0.0010167048312723637
iteration 194, loss = 0.001715801889076829
iteration 195, loss = 0.0007549707661382854
iteration 196, loss = 0.0015563182532787323
iteration 197, loss = 0.0008200063020922244
iteration 198, loss = 0.0009553342824801803
iteration 199, loss = 0.0008512763306498528
iteration 200, loss = 0.0007708874763920903
iteration 201, loss = 0.000893582939170301
iteration 202, loss = 0.0007544535328634083
iteration 203, loss = 0.0008986600441858172
iteration 204, loss = 0.0007624286226928234
iteration 205, loss = 0.0010599419474601746
iteration 206, loss = 0.0009014872484840453
iteration 207, loss = 0.0008217879221774638
iteration 208, loss = 0.0007391012040898204
iteration 209, loss = 0.0006604117807000875
iteration 210, loss = 0.0007944031385704875
iteration 211, loss = 0.0009560962789691985
iteration 212, loss = 0.0009647822589613497
iteration 213, loss = 0.0008228259393945336
iteration 214, loss = 0.0008887916337698698
iteration 215, loss = 0.001766967703588307
iteration 216, loss = 0.0010869401739910245
iteration 217, loss = 0.0014716789592057467
iteration 218, loss = 0.0011229605879634619
iteration 219, loss = 0.0008385065011680126
iteration 220, loss = 0.0008268357487395406
iteration 221, loss = 0.0006824163137935102
iteration 222, loss = 0.0009565162472426891
iteration 223, loss = 0.0008781402138993144
iteration 224, loss = 0.0007298442651517689
iteration 225, loss = 0.0014805067330598831
iteration 226, loss = 0.0008196469862014055
iteration 227, loss = 0.0015637862961739302
iteration 228, loss = 0.0010774002876132727
iteration 229, loss = 0.0006986582302488387
iteration 230, loss = 0.0010766793275251985
iteration 231, loss = 0.0009479314903728664
iteration 232, loss = 0.0007325606420636177
iteration 233, loss = 0.0008305262308567762
iteration 234, loss = 0.0015466294717043638
iteration 235, loss = 0.0010013044811785221
iteration 236, loss = 0.0008912459015846252
iteration 237, loss = 0.0008981286664493382
iteration 238, loss = 0.0008495304500684142
iteration 239, loss = 0.0013544261455535889
iteration 240, loss = 0.0014598729321733117
iteration 241, loss = 0.0009191258577629924
iteration 242, loss = 0.001013985718600452
iteration 243, loss = 0.0009186046081595123
iteration 244, loss = 0.0008985339081846178
iteration 245, loss = 0.0009518129518255591
iteration 246, loss = 0.0009435137617401779
iteration 247, loss = 0.0012648606207221746
iteration 248, loss = 0.0008581933798268437
iteration 249, loss = 0.0009893005480989814
iteration 250, loss = 0.0008952663047239184
iteration 251, loss = 0.002097844146192074
iteration 252, loss = 0.0007981146918609738
iteration 253, loss = 0.0010055294260382652
iteration 254, loss = 0.0007384376949630678
iteration 255, loss = 0.001494514406658709
iteration 256, loss = 0.000944062601774931
iteration 257, loss = 0.0009533466072753072
iteration 258, loss = 0.0009291662136092782
iteration 259, loss = 0.0007956187473610044
iteration 260, loss = 0.0008657778380438685
iteration 261, loss = 0.0012973763514310122
iteration 262, loss = 0.0007432728889398277
iteration 263, loss = 0.001397451851516962
iteration 264, loss = 0.0009570022812113166
iteration 265, loss = 0.0006541815237142146
iteration 266, loss = 0.0006364321452565491
iteration 267, loss = 0.0006829285412095487
iteration 268, loss = 0.0009508411749266088
iteration 269, loss = 0.0009280155063606799
iteration 270, loss = 0.0010203977581113577
iteration 271, loss = 0.0013797380961477757
iteration 272, loss = 0.0008579874411225319
iteration 273, loss = 0.0009475852712057531
iteration 274, loss = 0.0008868591976352036
iteration 275, loss = 0.0007238663383759558
iteration 276, loss = 0.0007953605963848531
iteration 277, loss = 0.0008472104091197252
iteration 278, loss = 0.0008342177025042474
iteration 279, loss = 0.0008459319942630827
iteration 280, loss = 0.000838138279505074
iteration 281, loss = 0.0008046933799050748
iteration 282, loss = 0.0007879809709265828
iteration 283, loss = 0.0009069957886822522
iteration 284, loss = 0.0008632338722236454
iteration 285, loss = 0.0014574446249753237
iteration 286, loss = 0.0009580716141499579
iteration 287, loss = 0.0007378176669590175
iteration 288, loss = 0.0007685751188546419
iteration 289, loss = 0.000900368089787662
iteration 290, loss = 0.0010406507644802332
iteration 291, loss = 0.00095855921972543
iteration 292, loss = 0.0011233581462875009
iteration 293, loss = 0.0015739270020276308
iteration 294, loss = 0.0008575237588956952
iteration 295, loss = 0.0012712639290839434
iteration 296, loss = 0.0008150410722009838
iteration 297, loss = 0.0009631678694859147
iteration 298, loss = 0.0010243821889162064
iteration 299, loss = 0.0009257430210709572
iteration 300, loss = 0.0007760759326629341
iteration 1, loss = 0.0010702380677685142
iteration 2, loss = 0.0010295440442860126
iteration 3, loss = 0.0013828533701598644
iteration 4, loss = 0.0009224896784871817
iteration 5, loss = 0.0009348337189294398
iteration 6, loss = 0.0012168186949566007
iteration 7, loss = 0.0007490399875678122
iteration 8, loss = 0.0007715500541962683
iteration 9, loss = 0.0019186956342309713
iteration 10, loss = 0.0007412572158500552
iteration 11, loss = 0.0007666708552278578
iteration 12, loss = 0.0007280541467480361
iteration 13, loss = 0.0010292483493685722
iteration 14, loss = 0.000960768957156688
iteration 15, loss = 0.0010032823774963617
iteration 16, loss = 0.0014994682278484106
iteration 17, loss = 0.0018434899393469095
iteration 18, loss = 0.0010719348210841417
iteration 19, loss = 0.0008827713900245726
iteration 20, loss = 0.0010355962440371513
iteration 21, loss = 0.0015293194446712732
iteration 22, loss = 0.0007554934127256274
iteration 23, loss = 0.0007913048611953855
iteration 24, loss = 0.0008793722954578698
iteration 25, loss = 0.001005758298560977
iteration 26, loss = 0.0009017700795084238
iteration 27, loss = 0.0012522183824330568
iteration 28, loss = 0.0008577389526180923
iteration 29, loss = 0.0017837404739111662
iteration 30, loss = 0.0008757678442634642
iteration 31, loss = 0.0013147100107744336
iteration 32, loss = 0.0009699433576315641
iteration 33, loss = 0.0008742714999243617
iteration 34, loss = 0.0007811464020051062
iteration 35, loss = 0.0008955139201134443
iteration 36, loss = 0.0006775851361453533
iteration 37, loss = 0.000778495566919446
iteration 38, loss = 0.0007801904575899243
iteration 39, loss = 0.0007663462893106043
iteration 40, loss = 0.0009798380779102445
iteration 41, loss = 0.0008114628144539893
iteration 42, loss = 0.0009192430297844112
iteration 43, loss = 0.0010469735134392977
iteration 44, loss = 0.0008508460596203804
iteration 45, loss = 0.0007885787053965032
iteration 46, loss = 0.0008321940549649298
iteration 47, loss = 0.00087931560119614
iteration 48, loss = 0.0006425208994187415
iteration 49, loss = 0.0010759644210338593
iteration 50, loss = 0.0012570517137646675
iteration 51, loss = 0.0009543407941237092
iteration 52, loss = 0.0008467598236165941
iteration 53, loss = 0.0007922793738543987
iteration 54, loss = 0.0007545892731286585
iteration 55, loss = 0.0008582642185501754
iteration 56, loss = 0.0011147047625854611
iteration 57, loss = 0.0015508077340200543
iteration 58, loss = 0.0008037741645239294
iteration 59, loss = 0.0007759346626698971
iteration 60, loss = 0.0010099950013682246
iteration 61, loss = 0.0008991803042590618
iteration 62, loss = 0.0007889308035373688
iteration 63, loss = 0.0007660468691028655
iteration 64, loss = 0.0010348960058763623
iteration 65, loss = 0.0009431491489522159
iteration 66, loss = 0.0009335695067420602
iteration 67, loss = 0.000869506096933037
iteration 68, loss = 0.0008837638306431472
iteration 69, loss = 0.0007953365566208959
iteration 70, loss = 0.0009128490346483886
iteration 71, loss = 0.0008259312016889453
iteration 72, loss = 0.0006876789266243577
iteration 73, loss = 0.0013886013766750693
iteration 74, loss = 0.0007599878590553999
iteration 75, loss = 0.0009720015223138034
iteration 76, loss = 0.0009343657875433564
iteration 77, loss = 0.0009659381466917694
iteration 78, loss = 0.0008324344526045024
iteration 79, loss = 0.0008245407952927053
iteration 80, loss = 0.0008759307092987001
iteration 81, loss = 0.0008352029835805297
iteration 82, loss = 0.0008750931010581553
iteration 83, loss = 0.000858639890793711
iteration 84, loss = 0.0009077037102542818
iteration 85, loss = 0.000929497298784554
iteration 86, loss = 0.0010436084121465683
iteration 87, loss = 0.0008756292518228292
iteration 88, loss = 0.0009138910099864006
iteration 89, loss = 0.0009021975565701723
iteration 90, loss = 0.0009189877891913056
iteration 91, loss = 0.0011611116351559758
iteration 92, loss = 0.0012667063856497407
iteration 93, loss = 0.0005604743491858244
iteration 94, loss = 0.0008325622184202075
iteration 95, loss = 0.0012972569093108177
iteration 96, loss = 0.0006691911839880049
iteration 97, loss = 0.0008699102909304202
iteration 98, loss = 0.0008276322623714805
iteration 99, loss = 0.000977582880295813
iteration 100, loss = 0.0008070915355347097
iteration 101, loss = 0.0007138263899832964
iteration 102, loss = 0.0006665909313596785
iteration 103, loss = 0.0013421495677903295
iteration 104, loss = 0.000808375421911478
iteration 105, loss = 0.0007429044926539063
iteration 106, loss = 0.0015939880395308137
iteration 107, loss = 0.0007483857334591448
iteration 108, loss = 0.0009329994209110737
iteration 109, loss = 0.0010889243567362428
iteration 110, loss = 0.0009201847715303302
iteration 111, loss = 0.0013043400831520557
iteration 112, loss = 0.0007018136675469577
iteration 113, loss = 0.001410581753589213
iteration 114, loss = 0.0008612427045591176
iteration 115, loss = 0.0010729084024205804
iteration 116, loss = 0.0008074810029938817
iteration 117, loss = 0.0007548463181592524
iteration 118, loss = 0.0010408582165837288
iteration 119, loss = 0.0006832053186371922
iteration 120, loss = 0.0009433618979528546
iteration 121, loss = 0.000751973653677851
iteration 122, loss = 0.0016833004774525762
iteration 123, loss = 0.0018677804619073868
iteration 124, loss = 0.0007270749774761498
iteration 125, loss = 0.0008343618828803301
iteration 126, loss = 0.0008238089503720403
iteration 127, loss = 0.0006815774831920862
iteration 128, loss = 0.0014375888276845217
iteration 129, loss = 0.0007636115187779069
iteration 130, loss = 0.00085675367154181
iteration 131, loss = 0.0009133866988122463
iteration 132, loss = 0.0008076447993516922
iteration 133, loss = 0.0009156846208497882
iteration 134, loss = 0.0011078912066295743
iteration 135, loss = 0.0007530274451710284
iteration 136, loss = 0.0009174933657050133
iteration 137, loss = 0.001230102265253663
iteration 138, loss = 0.0006320777465589345
iteration 139, loss = 0.0006829792400822043
iteration 140, loss = 0.0010654930956661701
iteration 141, loss = 0.000741815019864589
iteration 142, loss = 0.0008549141930416226
iteration 143, loss = 0.001005705096758902
iteration 144, loss = 0.0007887115934863687
iteration 145, loss = 0.0008638422004878521
iteration 146, loss = 0.0006837216787971556
iteration 147, loss = 0.000978057854808867
iteration 148, loss = 0.0010042426874861121
iteration 149, loss = 0.0007004882209002972
iteration 150, loss = 0.0008786660619080067
iteration 151, loss = 0.0008445005514658988
iteration 152, loss = 0.0015524582704529166
iteration 153, loss = 0.0009227886912412941
iteration 154, loss = 0.001194323762319982
iteration 155, loss = 0.0008220555610023439
iteration 156, loss = 0.0011263340711593628
iteration 157, loss = 0.0010425234213471413
iteration 158, loss = 0.0006197829497978091
iteration 159, loss = 0.0012044955510646105
iteration 160, loss = 0.0010777849238365889
iteration 161, loss = 0.0006307678995653987
iteration 162, loss = 0.0006839382695034146
iteration 163, loss = 0.0007991438033059239
iteration 164, loss = 0.0007907931576482952
iteration 165, loss = 0.0008530453196726739
iteration 166, loss = 0.0007824577623978257
iteration 167, loss = 0.00094603281468153
iteration 168, loss = 0.0010670959018170834
iteration 169, loss = 0.0008668708032928407
iteration 170, loss = 0.000861733453348279
iteration 171, loss = 0.0008139376877807081
iteration 172, loss = 0.0015111536486074328
iteration 173, loss = 0.00119590328540653
iteration 174, loss = 0.001344982418231666
iteration 175, loss = 0.0015866212779656053
iteration 176, loss = 0.0009783339919522405
iteration 177, loss = 0.0008611150551587343
iteration 178, loss = 0.0022819696459919214
iteration 179, loss = 0.0007614310598000884
iteration 180, loss = 0.0015817050589248538
iteration 181, loss = 0.0011625009356066585
iteration 182, loss = 0.0012087262002751231
iteration 183, loss = 0.0010200223186984658
iteration 184, loss = 0.000978596624918282
iteration 185, loss = 0.001035763300023973
iteration 186, loss = 0.0014069253811612725
iteration 187, loss = 0.0009155197767540812
iteration 188, loss = 0.0022605187259614468
iteration 189, loss = 0.0013536162441596389
iteration 190, loss = 0.0009355715592391789
iteration 191, loss = 0.0007364024058915675
iteration 192, loss = 0.0007076011970639229
iteration 193, loss = 0.0008878220687620342
iteration 194, loss = 0.0010229189647361636
iteration 195, loss = 0.0007334497640840709
iteration 196, loss = 0.0009679245413281024
iteration 197, loss = 0.0011816124897450209
iteration 198, loss = 0.0006277054199017584
iteration 199, loss = 0.0009704360272735357
iteration 200, loss = 0.0011266999645158648
iteration 201, loss = 0.0008465391583740711
iteration 202, loss = 0.0008353705052286386
iteration 203, loss = 0.0007866676896810532
iteration 204, loss = 0.0008659183513373137
iteration 205, loss = 0.000971620436757803
iteration 206, loss = 0.0015041822334751487
iteration 207, loss = 0.0016513175796717405
iteration 208, loss = 0.0010282854782417417
iteration 209, loss = 0.0008458654629066586
iteration 210, loss = 0.0008924618596211076
iteration 211, loss = 0.0008534279768355191
iteration 212, loss = 0.0006596384337171912
iteration 213, loss = 0.0009102451149374247
iteration 214, loss = 0.0008910478209145367
iteration 215, loss = 0.0008820325019769371
iteration 216, loss = 0.0007430874975398183
iteration 217, loss = 0.0006838199333287776
iteration 218, loss = 0.0007628489402122796
iteration 219, loss = 0.0008870781748555601
iteration 220, loss = 0.001280719181522727
iteration 221, loss = 0.0007523068343289196
iteration 222, loss = 0.0008582996670156717
iteration 223, loss = 0.0008618710562586784
iteration 224, loss = 0.0007240615086629987
iteration 225, loss = 0.0009999786270782351
iteration 226, loss = 0.0008179814903996885
iteration 227, loss = 0.0008827653946354985
iteration 228, loss = 0.0008046971634030342
iteration 229, loss = 0.000991916749626398
iteration 230, loss = 0.0012949255760759115
iteration 231, loss = 0.000871232827194035
iteration 232, loss = 0.0011380654759705067
iteration 233, loss = 0.0015502652386203408
iteration 234, loss = 0.0006576640880666673
iteration 235, loss = 0.0008041691617108881
iteration 236, loss = 0.0007502716616727412
iteration 237, loss = 0.0010570571757853031
iteration 238, loss = 0.0009205684182234108
iteration 239, loss = 0.0012244072277098894
iteration 240, loss = 0.0007462193607352674
iteration 241, loss = 0.0009694363106973469
iteration 242, loss = 0.0009226569090969861
iteration 243, loss = 0.0007320499280467629
iteration 244, loss = 0.0008631478995084763
iteration 245, loss = 0.0008283656788989902
iteration 246, loss = 0.000743902928661555
iteration 247, loss = 0.0007823342457413673
iteration 248, loss = 0.0007790214149281383
iteration 249, loss = 0.0016644143033772707
iteration 250, loss = 0.001041232026182115
iteration 251, loss = 0.0008040668908506632
iteration 252, loss = 0.0009827937465161085
iteration 253, loss = 0.0009614428272470832
iteration 254, loss = 0.0008601375739090145
iteration 255, loss = 0.0008922668639570475
iteration 256, loss = 0.001539235352538526
iteration 257, loss = 0.0008092864882200956
iteration 258, loss = 0.0008203377365134656
iteration 259, loss = 0.0008204994956031442
iteration 260, loss = 0.0007991713937371969
iteration 261, loss = 0.0010658958926796913
iteration 262, loss = 0.0007140036905184388
iteration 263, loss = 0.0008260335307568312
iteration 264, loss = 0.0009138312307186425
iteration 265, loss = 0.0008645370835438371
iteration 266, loss = 0.000835603685118258
iteration 267, loss = 0.0007556071504950523
iteration 268, loss = 0.0018140171887353063
iteration 269, loss = 0.001010481035336852
iteration 270, loss = 0.0011097737587988377
iteration 271, loss = 0.0006223498494364321
iteration 272, loss = 0.0012867242330685258
iteration 273, loss = 0.0007548292051069438
iteration 274, loss = 0.0008390537113882601
iteration 275, loss = 0.001547384774312377
iteration 276, loss = 0.001531232614070177
iteration 277, loss = 0.0009729592129588127
iteration 278, loss = 0.0008474487694911659
iteration 279, loss = 0.0014670091914013028
iteration 280, loss = 0.0007431517005898058
iteration 281, loss = 0.0015432429499924183
iteration 282, loss = 0.0012448129709810019
iteration 283, loss = 0.0009120831382460892
iteration 284, loss = 0.0009345672442577779
iteration 285, loss = 0.0007382201729342341
iteration 286, loss = 0.0008159653516486287
iteration 287, loss = 0.0007900281343609095
iteration 288, loss = 0.0013787976931780577
iteration 289, loss = 0.0008877159561961889
iteration 290, loss = 0.0009125906508415937
iteration 291, loss = 0.0007201945991255343
iteration 292, loss = 0.0008885852876119316
iteration 293, loss = 0.0009133067796938121
iteration 294, loss = 0.000848255876917392
iteration 295, loss = 0.0008929290343075991
iteration 296, loss = 0.0013778549619019032
iteration 297, loss = 0.0019196367356926203
iteration 298, loss = 0.001158280298113823
iteration 299, loss = 0.001541945617645979
iteration 300, loss = 0.0008236095309257507
iteration 1, loss = 0.0009856316028162837
iteration 2, loss = 0.0008646254427731037
iteration 3, loss = 0.0016900039045140147
iteration 4, loss = 0.0007753880927339196
iteration 5, loss = 0.0007593148038722575
iteration 6, loss = 0.0008060162654146552
iteration 7, loss = 0.0011896813521161675
iteration 8, loss = 0.0008706022636033595
iteration 9, loss = 0.0007585302228108048
iteration 10, loss = 0.0010155385825783014
iteration 11, loss = 0.001186697045341134
iteration 12, loss = 0.000864922534674406
iteration 13, loss = 0.0009179115295410156
iteration 14, loss = 0.0012089780066162348
iteration 15, loss = 0.0007818848825991154
iteration 16, loss = 0.0008860250818543136
iteration 17, loss = 0.0008802435477264225
iteration 18, loss = 0.0011309696128591895
iteration 19, loss = 0.0008127500186674297
iteration 20, loss = 0.0008608118514530361
iteration 21, loss = 0.001091444049961865
iteration 22, loss = 0.0007848708773963153
iteration 23, loss = 0.0008520449046045542
iteration 24, loss = 0.0008258174639195204
iteration 25, loss = 0.001032586907967925
iteration 26, loss = 0.0012516850838437676
iteration 27, loss = 0.001301975455135107
iteration 28, loss = 0.0014211429515853524
iteration 29, loss = 0.0007605982245877385
iteration 30, loss = 0.0010341992601752281
iteration 31, loss = 0.0008802315569482744
iteration 32, loss = 0.000784261676017195
iteration 33, loss = 0.0007888585678301752
iteration 34, loss = 0.0008004580740816891
iteration 35, loss = 0.0007810661918483675
iteration 36, loss = 0.001721417997032404
iteration 37, loss = 0.0009271768503822386
iteration 38, loss = 0.0008842427050694823
iteration 39, loss = 0.0015947127249091864
iteration 40, loss = 0.000872980454005301
iteration 41, loss = 0.001572200795635581
iteration 42, loss = 0.0007119209039956331
iteration 43, loss = 0.0009557869634591043
iteration 44, loss = 0.0010130776790902019
iteration 45, loss = 0.0008011888712644577
iteration 46, loss = 0.0009061762830242515
iteration 47, loss = 0.0007767153438180685
iteration 48, loss = 0.0011955546215176582
iteration 49, loss = 0.000837458879686892
iteration 50, loss = 0.0007950364961288869
iteration 51, loss = 0.0012995334109291434
iteration 52, loss = 0.0016177332727238536
iteration 53, loss = 0.0007945472607389092
iteration 54, loss = 0.0007789062219671905
iteration 55, loss = 0.0007403995259664953
iteration 56, loss = 0.0014954706421121955
iteration 57, loss = 0.0011402179952710867
iteration 58, loss = 0.0008033731719478965
iteration 59, loss = 0.0010721790604293346
iteration 60, loss = 0.0009322711266577244
iteration 61, loss = 0.0009259699145331979
iteration 62, loss = 0.0007566728163510561
iteration 63, loss = 0.0012266465928405523
iteration 64, loss = 0.0010980352526530623
iteration 65, loss = 0.0012109973467886448
iteration 66, loss = 0.0010350458323955536
iteration 67, loss = 0.0007331264787353575
iteration 68, loss = 0.0007854782743379474
iteration 69, loss = 0.0007406669901683927
iteration 70, loss = 0.0010338606080040336
iteration 71, loss = 0.0013101224321871996
iteration 72, loss = 0.000785840442404151
iteration 73, loss = 0.0008551580831408501
iteration 74, loss = 0.0012000763090327382
iteration 75, loss = 0.0009732167236506939
iteration 76, loss = 0.0008542557479813695
iteration 77, loss = 0.0008144908351823688
iteration 78, loss = 0.0009309285087510943
iteration 79, loss = 0.0009747081785462797
iteration 80, loss = 0.0015286465641111135
iteration 81, loss = 0.0007942189113236964
iteration 82, loss = 0.0007149545708671212
iteration 83, loss = 0.0012422981671988964
iteration 84, loss = 0.001752477721311152
iteration 85, loss = 0.0011660181917250156
iteration 86, loss = 0.0007213136996142566
iteration 87, loss = 0.0009041056036949158
iteration 88, loss = 0.0010180023964494467
iteration 89, loss = 0.000768868369050324
iteration 90, loss = 0.0010052936850115657
iteration 91, loss = 0.0008554502855986357
iteration 92, loss = 0.0007374891429208219
iteration 93, loss = 0.0012765309074893594
iteration 94, loss = 0.0008506622398272157
iteration 95, loss = 0.0009964340133592486
iteration 96, loss = 0.0007938133203424513
iteration 97, loss = 0.0006388548645190895
iteration 98, loss = 0.0010387712391093373
iteration 99, loss = 0.0012935156701132655
iteration 100, loss = 0.001526413019746542
iteration 101, loss = 0.0008558579720556736
iteration 102, loss = 0.0006352204363793135
iteration 103, loss = 0.0008405500557273626
iteration 104, loss = 0.0007337815477512777
iteration 105, loss = 0.0009624616359360516
iteration 106, loss = 0.0008998365374282002
iteration 107, loss = 0.0010102714877575636
iteration 108, loss = 0.0007215811056084931
iteration 109, loss = 0.0009391530184075236
iteration 110, loss = 0.000771400285884738
iteration 111, loss = 0.0008645401103422046
iteration 112, loss = 0.0007913702866062522
iteration 113, loss = 0.0008208992658182979
iteration 114, loss = 0.001085934811271727
iteration 115, loss = 0.0007705697207711637
iteration 116, loss = 0.0007711731013841927
iteration 117, loss = 0.0008657428552396595
iteration 118, loss = 0.0007097461493685842
iteration 119, loss = 0.0009260852821171284
iteration 120, loss = 0.0008089242619462311
iteration 121, loss = 0.00095724145649001
iteration 122, loss = 0.0008534211083315313
iteration 123, loss = 0.0007048018742352724
iteration 124, loss = 0.0008904095739126205
iteration 125, loss = 0.0006533956038765609
iteration 126, loss = 0.0006810134509578347
iteration 127, loss = 0.0017707330407574773
iteration 128, loss = 0.0010919430060312152
iteration 129, loss = 0.0008597782580181956
iteration 130, loss = 0.0008434158517047763
iteration 131, loss = 0.0009167406242340803
iteration 132, loss = 0.001079566078260541
iteration 133, loss = 0.001034260611049831
iteration 134, loss = 0.0012849539052695036
iteration 135, loss = 0.0008003704133443534
iteration 136, loss = 0.001097943400964141
iteration 137, loss = 0.0015999317402020097
iteration 138, loss = 0.0009627880062907934
iteration 139, loss = 0.0020191839430481195
iteration 140, loss = 0.0010301802540197968
iteration 141, loss = 0.0008344048983417451
iteration 142, loss = 0.0010603111004456878
iteration 143, loss = 0.0007138319779187441
iteration 144, loss = 0.0008743025828152895
iteration 145, loss = 0.000822363537736237
iteration 146, loss = 0.000981341814622283
iteration 147, loss = 0.0011130014900118113
iteration 148, loss = 0.0009020694997161627
iteration 149, loss = 0.0008026586147025228
iteration 150, loss = 0.001201750710606575
iteration 151, loss = 0.0009132199338637292
iteration 152, loss = 0.000691701308824122
iteration 153, loss = 0.0010888668475672603
iteration 154, loss = 0.0010732031660154462
iteration 155, loss = 0.0007621229160577059
iteration 156, loss = 0.0017030095914378762
iteration 157, loss = 0.0008354400633834302
iteration 158, loss = 0.001600363408215344
iteration 159, loss = 0.0020455005578696728
iteration 160, loss = 0.0014987686881795526
iteration 161, loss = 0.0009810873307287693
iteration 162, loss = 0.0009655505418777466
iteration 163, loss = 0.001452598487958312
iteration 164, loss = 0.0012836805544793606
iteration 165, loss = 0.000973493792116642
iteration 166, loss = 0.0009114776039496064
iteration 167, loss = 0.0007290508947335184
iteration 168, loss = 0.0008122463477775455
iteration 169, loss = 0.0008754350710660219
iteration 170, loss = 0.0008104972657747567
iteration 171, loss = 0.0007075462490320206
iteration 172, loss = 0.0015351499896496534
iteration 173, loss = 0.0017169455531984568
iteration 174, loss = 0.0007834420539438725
iteration 175, loss = 0.0008269569952972233
iteration 176, loss = 0.0008958609541878104
iteration 177, loss = 0.0008421249804086983
iteration 178, loss = 0.0011956054950132966
iteration 179, loss = 0.0007771814707666636
iteration 180, loss = 0.0009066599886864424
iteration 181, loss = 0.0008074542274698615
iteration 182, loss = 0.0009092391701415181
iteration 183, loss = 0.0011911529581993818
iteration 184, loss = 0.001140874344855547
iteration 185, loss = 0.0008606587653048337
iteration 186, loss = 0.0010656813392415643
iteration 187, loss = 0.001273634028621018
iteration 188, loss = 0.0007391221006400883
iteration 189, loss = 0.0008642947068437934
iteration 190, loss = 0.0008709715912118554
iteration 191, loss = 0.0011542373104020953
iteration 192, loss = 0.001034226268529892
iteration 193, loss = 0.0017376687610521913
iteration 194, loss = 0.0007871349225752056
iteration 195, loss = 0.0009211348951794207
iteration 196, loss = 0.0008436680654995143
iteration 197, loss = 0.0009688541176728904
iteration 198, loss = 0.0008051267359405756
iteration 199, loss = 0.0006814552471041679
iteration 200, loss = 0.0007126462878659368
iteration 201, loss = 0.0009384822333231568
iteration 202, loss = 0.0007954579195939004
iteration 203, loss = 0.0008444399572908878
iteration 204, loss = 0.0007512670708820224
iteration 205, loss = 0.0014405450783669949
iteration 206, loss = 0.0014833405148237944
iteration 207, loss = 0.001506571308709681
iteration 208, loss = 0.0009327606530860066
iteration 209, loss = 0.0007567330612801015
iteration 210, loss = 0.0010191656183451414
iteration 211, loss = 0.0007141554960981011
iteration 212, loss = 0.0009266387205570936
iteration 213, loss = 0.000916024437174201
iteration 214, loss = 0.0007499057683162391
iteration 215, loss = 0.0007593279588036239
iteration 216, loss = 0.0010334665421396494
iteration 217, loss = 0.0008389843278564513
iteration 218, loss = 0.0007992435712367296
iteration 219, loss = 0.0008962563006207347
iteration 220, loss = 0.0009607878746464849
iteration 221, loss = 0.0006569219985976815
iteration 222, loss = 0.0007881971541792154
iteration 223, loss = 0.0009647472179494798
iteration 224, loss = 0.0012504713376984
iteration 225, loss = 0.0008339728810824454
iteration 226, loss = 0.0014086310984566808
iteration 227, loss = 0.0007238015532493591
iteration 228, loss = 0.0007957073394209146
iteration 229, loss = 0.001116809668019414
iteration 230, loss = 0.0006969613023102283
iteration 231, loss = 0.0010963045060634613
iteration 232, loss = 0.0008462091791443527
iteration 233, loss = 0.0011091671185567975
iteration 234, loss = 0.0008352524600923061
iteration 235, loss = 0.0009184138616546988
iteration 236, loss = 0.001785788219422102
iteration 237, loss = 0.0008810118306428194
iteration 238, loss = 0.0008133384981192648
iteration 239, loss = 0.001593668246641755
iteration 240, loss = 0.0008137277909554541
iteration 241, loss = 0.0017629442736506462
iteration 242, loss = 0.0015957738505676389
iteration 243, loss = 0.0009825807064771652
iteration 244, loss = 0.0008922746637836099
iteration 245, loss = 0.0007021675119176507
iteration 246, loss = 0.0008125179447233677
iteration 247, loss = 0.0007438376778736711
iteration 248, loss = 0.0012213520240038633
iteration 249, loss = 0.0011179308639839292
iteration 250, loss = 0.0010869773104786873
iteration 251, loss = 0.0007795675774104893
iteration 252, loss = 0.0009188074036501348
iteration 253, loss = 0.0007415565196424723
iteration 254, loss = 0.0006939286249689758
iteration 255, loss = 0.001556634670123458
iteration 256, loss = 0.0006422791630029678
iteration 257, loss = 0.0007978828507475555
iteration 258, loss = 0.0007807980873622
iteration 259, loss = 0.0007769237854517996
iteration 260, loss = 0.0007110084989108145
iteration 261, loss = 0.0008506715530529618
iteration 262, loss = 0.0009450982324779034
iteration 263, loss = 0.0010290775680914521
iteration 264, loss = 0.0007389267557300627
iteration 265, loss = 0.000772314437199384
iteration 266, loss = 0.0007025895756669343
iteration 267, loss = 0.0007334933034144342
iteration 268, loss = 0.0007116172928363085
iteration 269, loss = 0.001346702454611659
iteration 270, loss = 0.0007441497873514891
iteration 271, loss = 0.0009571707341820002
iteration 272, loss = 0.0008669925737194717
iteration 273, loss = 0.0011423075338825583
iteration 274, loss = 0.0008909813477657735
iteration 275, loss = 0.0008130914065986872
iteration 276, loss = 0.0007870247354730964
iteration 277, loss = 0.0010490792337805033
iteration 278, loss = 0.0007081033545546234
iteration 279, loss = 0.0007617830997332931
iteration 280, loss = 0.0009572503622621298
iteration 281, loss = 0.0008695436990819871
iteration 282, loss = 0.000830369652248919
iteration 283, loss = 0.001598055474460125
iteration 284, loss = 0.001453762175515294
iteration 285, loss = 0.000784771516919136
iteration 286, loss = 0.0008046778966672719
iteration 287, loss = 0.0009756038198247552
iteration 288, loss = 0.0009823977015912533
iteration 289, loss = 0.0011643660254776478
iteration 290, loss = 0.0008454973576590419
iteration 291, loss = 0.0009522869950160384
iteration 292, loss = 0.0009628534899093211
iteration 293, loss = 0.0009054872207343578
iteration 294, loss = 0.0007029236876405776
iteration 295, loss = 0.0006640499923378229
iteration 296, loss = 0.0009516343707218766
iteration 297, loss = 0.0009294878109358251
iteration 298, loss = 0.0008497125236317515
iteration 299, loss = 0.001075034262612462
iteration 300, loss = 0.0008103925501927733
iteration 1, loss = 0.0011799272615462542
iteration 2, loss = 0.0012218650663271546
iteration 3, loss = 0.0009926832281053066
iteration 4, loss = 0.0008298782631754875
iteration 5, loss = 0.0007792686810716987
iteration 6, loss = 0.0007354121189564466
iteration 7, loss = 0.0008095729863271117
iteration 8, loss = 0.0006779824034310877
iteration 9, loss = 0.0007429647375829518
iteration 10, loss = 0.0009119098540395498
iteration 11, loss = 0.0008497624075971544
iteration 12, loss = 0.0006792924250476062
iteration 13, loss = 0.0009535235003568232
iteration 14, loss = 0.0008897465304471552
iteration 15, loss = 0.0010345863411203027
iteration 16, loss = 0.0009183447109535336
iteration 17, loss = 0.0007511636940762401
iteration 18, loss = 0.00081167760072276
iteration 19, loss = 0.0009503131150268018
iteration 20, loss = 0.0007348196231760085
iteration 21, loss = 0.000846915296278894
iteration 22, loss = 0.0007290103822015226
iteration 23, loss = 0.0008081916021183133
iteration 24, loss = 0.0008457129588350654
iteration 25, loss = 0.000997041119262576
iteration 26, loss = 0.001435398473404348
iteration 27, loss = 0.0010316821280866861
iteration 28, loss = 0.0007991333259269595
iteration 29, loss = 0.0013894790317863226
iteration 30, loss = 0.0007728429045528173
iteration 31, loss = 0.0008149180794134736
iteration 32, loss = 0.0014542621793225408
iteration 33, loss = 0.0015581605257466435
iteration 34, loss = 0.0007213827921077609
iteration 35, loss = 0.001039093709550798
iteration 36, loss = 0.000636629993095994
iteration 37, loss = 0.0006854113889858127
iteration 38, loss = 0.0023321870248764753
iteration 39, loss = 0.0009133495623245835
iteration 40, loss = 0.0016512901056557894
iteration 41, loss = 0.0008976152166724205
iteration 42, loss = 0.0009551799157634377
iteration 43, loss = 0.0007495573372580111
iteration 44, loss = 0.0009729856974445283
iteration 45, loss = 0.0008995977113954723
iteration 46, loss = 0.0008298045140691102
iteration 47, loss = 0.0007138277869671583
iteration 48, loss = 0.0009514411212876439
iteration 49, loss = 0.000946293817833066
iteration 50, loss = 0.000984200625680387
iteration 51, loss = 0.0008168467902578413
iteration 52, loss = 0.000825931434519589
iteration 53, loss = 0.0007652050699107349
iteration 54, loss = 0.000696070899721235
iteration 55, loss = 0.0008173846872523427
iteration 56, loss = 0.0007467681425623596
iteration 57, loss = 0.0015885650645941496
iteration 58, loss = 0.0015879396814852953
iteration 59, loss = 0.0008550044731236994
iteration 60, loss = 0.0012698783539235592
iteration 61, loss = 0.0007728110067546368
iteration 62, loss = 0.0010080785723403096
iteration 63, loss = 0.0015981468604877591
iteration 64, loss = 0.0007126617128960788
iteration 65, loss = 0.0007700393907725811
iteration 66, loss = 0.0007092636078596115
iteration 67, loss = 0.0012360491091385484
iteration 68, loss = 0.0010586782591417432
iteration 69, loss = 0.0009742900147102773
iteration 70, loss = 0.000718942959792912
iteration 71, loss = 0.0007560238591395319
iteration 72, loss = 0.0010033154394477606
iteration 73, loss = 0.0007028186228126287
iteration 74, loss = 0.0022902858909219503
iteration 75, loss = 0.001092260703444481
iteration 76, loss = 0.0007733788224868476
iteration 77, loss = 0.0008846096461638808
iteration 78, loss = 0.0009611564455553889
iteration 79, loss = 0.0007728481432422996
iteration 80, loss = 0.0008154910756275058
iteration 81, loss = 0.0006683506071567535
iteration 82, loss = 0.0007966889534145594
iteration 83, loss = 0.0008370098657906055
iteration 84, loss = 0.0007169541204348207
iteration 85, loss = 0.0007971277227625251
iteration 86, loss = 0.0013022755738347769
iteration 87, loss = 0.0007804889464750886
iteration 88, loss = 0.0008569988422095776
iteration 89, loss = 0.0008780153002589941
iteration 90, loss = 0.000856358150485903
iteration 91, loss = 0.0013649892061948776
iteration 92, loss = 0.0016763033345341682
iteration 93, loss = 0.0008326574461534619
iteration 94, loss = 0.0007629127940163016
iteration 95, loss = 0.0007119770161807537
iteration 96, loss = 0.0008491997723467648
iteration 97, loss = 0.0010900988709181547
iteration 98, loss = 0.0007802399341017008
iteration 99, loss = 0.0009324990096502006
iteration 100, loss = 0.0008267773664556444
iteration 101, loss = 0.0008504558354616165
iteration 102, loss = 0.0009211346041411161
iteration 103, loss = 0.0010967678390443325
iteration 104, loss = 0.000883024069480598
iteration 105, loss = 0.0007413194398395717
iteration 106, loss = 0.0017933851340785623
iteration 107, loss = 0.0009425660246051848
iteration 108, loss = 0.001821264624595642
iteration 109, loss = 0.0016479096375405788
iteration 110, loss = 0.0011431246530264616
iteration 111, loss = 0.0009521257597953081
iteration 112, loss = 0.0008776183240115643
iteration 113, loss = 0.0007441004272550344
iteration 114, loss = 0.0009172689169645309
iteration 115, loss = 0.0006367965252138674
iteration 116, loss = 0.0008541357819922268
iteration 117, loss = 0.000970733817666769
iteration 118, loss = 0.0010244922013953328
iteration 119, loss = 0.0014789712149649858
iteration 120, loss = 0.0007779403822496533
iteration 121, loss = 0.0008715374860912561
iteration 122, loss = 0.0008494193316437304
iteration 123, loss = 0.0010400356259196997
iteration 124, loss = 0.0015452351653948426
iteration 125, loss = 0.0007916131871752441
iteration 126, loss = 0.0009340574615634978
iteration 127, loss = 0.0008832155144773424
iteration 128, loss = 0.0009392602951265872
iteration 129, loss = 0.0009352352353744209
iteration 130, loss = 0.001569884130731225
iteration 131, loss = 0.0011355357710272074
iteration 132, loss = 0.0009758799569681287
iteration 133, loss = 0.0010113989701494575
iteration 134, loss = 0.0010841983603313565
iteration 135, loss = 0.0012951417593285441
iteration 136, loss = 0.000784542877227068
iteration 137, loss = 0.0009384540608152747
iteration 138, loss = 0.0007675489760003984
iteration 139, loss = 0.0010653011268004775
iteration 140, loss = 0.0009030861547216773
iteration 141, loss = 0.0014297139132395387
iteration 142, loss = 0.000899182166904211
iteration 143, loss = 0.0012388692703098059
iteration 144, loss = 0.0007926143589429557
iteration 145, loss = 0.001172010088339448
iteration 146, loss = 0.0009410538477823138
iteration 147, loss = 0.0017208484932780266
iteration 148, loss = 0.000810865662060678
iteration 149, loss = 0.0009388434700667858
iteration 150, loss = 0.0007715231040492654
iteration 151, loss = 0.0009405814344063401
iteration 152, loss = 0.0008227957296185195
iteration 153, loss = 0.0008390654111281037
iteration 154, loss = 0.0006931294337846339
iteration 155, loss = 0.001940618036314845
iteration 156, loss = 0.0008060608524829149
iteration 157, loss = 0.000898518250323832
iteration 158, loss = 0.0008166510378941894
iteration 159, loss = 0.000753018946852535
iteration 160, loss = 0.0016203417908400297
iteration 161, loss = 0.0008651676471345127
iteration 162, loss = 0.0012732658069580793
iteration 163, loss = 0.0007188782910816371
iteration 164, loss = 0.0014961829874664545
iteration 165, loss = 0.0007749187643639743
iteration 166, loss = 0.0009038947173394263
iteration 167, loss = 0.0008396023185923696
iteration 168, loss = 0.0007686431054025888
iteration 169, loss = 0.000742594653274864
iteration 170, loss = 0.0007601698162034154
iteration 171, loss = 0.0007467437535524368
iteration 172, loss = 0.000741712108720094
iteration 173, loss = 0.0010015241568908095
iteration 174, loss = 0.000832880730740726
iteration 175, loss = 0.0007543233223259449
iteration 176, loss = 0.0008105577435344458
iteration 177, loss = 0.0011180092114955187
iteration 178, loss = 0.0008352884324267507
iteration 179, loss = 0.0008291133563034236
iteration 180, loss = 0.0006629601703025401
iteration 181, loss = 0.0008239930612035096
iteration 182, loss = 0.0007517599151469767
iteration 183, loss = 0.0013153112959116697
iteration 184, loss = 0.000874085002578795
iteration 185, loss = 0.0012467720080167055
iteration 186, loss = 0.0008831768063828349
iteration 187, loss = 0.0007772103417664766
iteration 188, loss = 0.0008346092072315514
iteration 189, loss = 0.0008297981694340706
iteration 190, loss = 0.0010682214051485062
iteration 191, loss = 0.0014649335062131286
iteration 192, loss = 0.000776176224462688
iteration 193, loss = 0.0010514430468901992
iteration 194, loss = 0.0007858714670874178
iteration 195, loss = 0.0008963137515820563
iteration 196, loss = 0.0007243772852234542
iteration 197, loss = 0.0009041090379469097
iteration 198, loss = 0.0016828078078106046
iteration 199, loss = 0.000926446751691401
iteration 200, loss = 0.0007617593510076404
iteration 201, loss = 0.001427585375495255
iteration 202, loss = 0.0007447462994605303
iteration 203, loss = 0.000985618680715561
iteration 204, loss = 0.0006797298556193709
iteration 205, loss = 0.0013508644187822938
iteration 206, loss = 0.0007237495738081634
iteration 207, loss = 0.001004690071567893
iteration 208, loss = 0.0008869432494975626
iteration 209, loss = 0.0011567273177206516
iteration 210, loss = 0.0007861611084081233
iteration 211, loss = 0.0011140848509967327
iteration 212, loss = 0.0009255298646166921
iteration 213, loss = 0.0008425075793638825
iteration 214, loss = 0.0008023045375011861
iteration 215, loss = 0.001146718510426581
iteration 216, loss = 0.0008678764570504427
iteration 217, loss = 0.0007769513176754117
iteration 218, loss = 0.001609453116543591
iteration 219, loss = 0.001158764585852623
iteration 220, loss = 0.0008223088225349784
iteration 221, loss = 0.000888696638867259
iteration 222, loss = 0.0008385028340853751
iteration 223, loss = 0.0011295245494693518
iteration 224, loss = 0.0008597366395406425
iteration 225, loss = 0.0009920097654685378
iteration 226, loss = 0.0011111295316368341
iteration 227, loss = 0.0015450227074325085
iteration 228, loss = 0.000946973217651248
iteration 229, loss = 0.0009815888479351997
iteration 230, loss = 0.0009319650707766414
iteration 231, loss = 0.001484256237745285
iteration 232, loss = 0.0008969547925516963
iteration 233, loss = 0.0008637245045974851
iteration 234, loss = 0.0008715847507119179
iteration 235, loss = 0.0006933909025974572
iteration 236, loss = 0.0011946692829951644
iteration 237, loss = 0.0010516660986468196
iteration 238, loss = 0.0007008599350228906
iteration 239, loss = 0.001512263435870409
iteration 240, loss = 0.000795449479483068
iteration 241, loss = 0.0008016747888177633
iteration 242, loss = 0.0008388219284825027
iteration 243, loss = 0.0008264958159998059
iteration 244, loss = 0.0010968523565679789
iteration 245, loss = 0.0010648630559444427
iteration 246, loss = 0.0011079575633630157
iteration 247, loss = 0.0010492493165656924
iteration 248, loss = 0.0007756671402603388
iteration 249, loss = 0.0008119472186081111
iteration 250, loss = 0.0009613973088562489
iteration 251, loss = 0.0007258764817379415
iteration 252, loss = 0.0010861102491617203
iteration 253, loss = 0.0012055718107149005
iteration 254, loss = 0.0009431213838979602
iteration 255, loss = 0.0011489314492791891
iteration 256, loss = 0.0007858870667405427
iteration 257, loss = 0.0009351155022159219
iteration 258, loss = 0.0016416847938671708
iteration 259, loss = 0.0008037975057959557
iteration 260, loss = 0.0010850656544789672
iteration 261, loss = 0.0009996475419029593
iteration 262, loss = 0.0009423214360140264
iteration 263, loss = 0.0006794305518269539
iteration 264, loss = 0.0008437226642854512
iteration 265, loss = 0.0009853721130639315
iteration 266, loss = 0.0010115352924913168
iteration 267, loss = 0.0011492886114865541
iteration 268, loss = 0.0007609871099703014
iteration 269, loss = 0.0011380749056115746
iteration 270, loss = 0.0016145732952281833
iteration 271, loss = 0.0007151889149099588
iteration 272, loss = 0.0007569182780571282
iteration 273, loss = 0.000789036275818944
iteration 274, loss = 0.0010418304009363055
iteration 275, loss = 0.0008553583174943924
iteration 276, loss = 0.0016829792875796556
iteration 277, loss = 0.001001739059574902
iteration 278, loss = 0.000695535447448492
iteration 279, loss = 0.0007704168674536049
iteration 280, loss = 0.000944092869758606
iteration 281, loss = 0.0014827288687229156
iteration 282, loss = 0.0006465250626206398
iteration 283, loss = 0.0008056483929976821
iteration 284, loss = 0.0009083693148568273
iteration 285, loss = 0.0007955146720632911
iteration 286, loss = 0.0008920669788494706
iteration 287, loss = 0.0008973816293291748
iteration 288, loss = 0.0009508533403277397
iteration 289, loss = 0.000879014958627522
iteration 290, loss = 0.0007319491123780608
iteration 291, loss = 0.0007125631091184914
iteration 292, loss = 0.0008957551326602697
iteration 293, loss = 0.0008659968734718859
iteration 294, loss = 0.0006430007051676512
iteration 295, loss = 0.0007968738209456205
iteration 296, loss = 0.0013793098041787744
iteration 297, loss = 0.0013485848903656006
iteration 298, loss = 0.0013256719103083014
iteration 299, loss = 0.0011643444886431098
iteration 300, loss = 0.0012100593885406852
iteration 1, loss = 0.0016283390577882528
iteration 2, loss = 0.0008161473670043051
iteration 3, loss = 0.0008320243796333671
iteration 4, loss = 0.0006949754897505045
iteration 5, loss = 0.0008133153896778822
iteration 6, loss = 0.0008345444803126156
iteration 7, loss = 0.0009514101548120379
iteration 8, loss = 0.000981487799435854
iteration 9, loss = 0.0010955430334433913
iteration 10, loss = 0.000994757516309619
iteration 11, loss = 0.001720841508358717
iteration 12, loss = 0.0006902015302330256
iteration 13, loss = 0.0009431190555915236
iteration 14, loss = 0.0007516518235206604
iteration 15, loss = 0.0009492603712715209
iteration 16, loss = 0.0007594360504299402
iteration 17, loss = 0.0008262826595455408
iteration 18, loss = 0.0011428892612457275
iteration 19, loss = 0.0007563088438473642
iteration 20, loss = 0.0008549191989004612
iteration 21, loss = 0.0009882693411782384
iteration 22, loss = 0.0011767431860789657
iteration 23, loss = 0.000885991845279932
iteration 24, loss = 0.0015824292786419392
iteration 25, loss = 0.0012216458562761545
iteration 26, loss = 0.0010152290342375636
iteration 27, loss = 0.0007995821069926023
iteration 28, loss = 0.0007443234208039939
iteration 29, loss = 0.0008556118700653315
iteration 30, loss = 0.0008892955956980586
iteration 31, loss = 0.0008481529075652361
iteration 32, loss = 0.000826775620225817
iteration 33, loss = 0.0009228650596924126
iteration 34, loss = 0.0008339390624314547
iteration 35, loss = 0.0008660469902679324
iteration 36, loss = 0.0008095672819763422
iteration 37, loss = 0.0009466018527746201
iteration 38, loss = 0.0008010705350898206
iteration 39, loss = 0.0007855209405533969
iteration 40, loss = 0.0016027063829824328
iteration 41, loss = 0.0006398161058314145
iteration 42, loss = 0.0009254266624338925
iteration 43, loss = 0.0008048023446463048
iteration 44, loss = 0.0007379446178674698
iteration 45, loss = 0.0010043495567515492
iteration 46, loss = 0.0010208877502009273
iteration 47, loss = 0.0006538667366839945
iteration 48, loss = 0.0011168982600793242
iteration 49, loss = 0.0006748357554897666
iteration 50, loss = 0.0015806473093107343
iteration 51, loss = 0.0008947546593844891
iteration 52, loss = 0.0007561434176750481
iteration 53, loss = 0.0009545552311465144
iteration 54, loss = 0.0008867810247465968
iteration 55, loss = 0.0009219882194884121
iteration 56, loss = 0.0006343747372739017
iteration 57, loss = 0.0009321706020273268
iteration 58, loss = 0.0007394092390313745
iteration 59, loss = 0.0007925857789814472
iteration 60, loss = 0.000676484196446836
iteration 61, loss = 0.0006626913673244417
iteration 62, loss = 0.0007779867737554014
iteration 63, loss = 0.0008655445417389274
iteration 64, loss = 0.0006540145841427147
iteration 65, loss = 0.0010466543026268482
iteration 66, loss = 0.0008045310969464481
iteration 67, loss = 0.0007733810343779624
iteration 68, loss = 0.0009180473862215877
iteration 69, loss = 0.0014701626496389508
iteration 70, loss = 0.0012257718481123447
iteration 71, loss = 0.0008414479671046138
iteration 72, loss = 0.0009172132122330368
iteration 73, loss = 0.001001337543129921
iteration 74, loss = 0.0007756620761938393
iteration 75, loss = 0.0007874105358496308
iteration 76, loss = 0.0008515605586580932
iteration 77, loss = 0.0008245971985161304
iteration 78, loss = 0.0012534874258562922
iteration 79, loss = 0.0009776548249647021
iteration 80, loss = 0.0010308594210073352
iteration 81, loss = 0.0010975007899105549
iteration 82, loss = 0.0008951924974098802
iteration 83, loss = 0.0007369990926235914
iteration 84, loss = 0.00125342037063092
iteration 85, loss = 0.0008369191782549024
iteration 86, loss = 0.0012113809352740645
iteration 87, loss = 0.0011184700997546315
iteration 88, loss = 0.001055592787452042
iteration 89, loss = 0.0010263314470648766
iteration 90, loss = 0.0006693949108012021
iteration 91, loss = 0.0008274723077192903
iteration 92, loss = 0.0007052152068354189
iteration 93, loss = 0.0013848908711224794
iteration 94, loss = 0.0009611450368538499
iteration 95, loss = 0.0008731377893127501
iteration 96, loss = 0.0008839494548738003
iteration 97, loss = 0.0008188491337932646
iteration 98, loss = 0.0006804799777455628
iteration 99, loss = 0.000999453361146152
iteration 100, loss = 0.0007423657807521522
iteration 101, loss = 0.0007833446143195033
iteration 102, loss = 0.0012022322043776512
iteration 103, loss = 0.0007460285560227931
iteration 104, loss = 0.0010762899182736874
iteration 105, loss = 0.00082044443115592
iteration 106, loss = 0.001288540312089026
iteration 107, loss = 0.0008084241999313235
iteration 108, loss = 0.00159368384629488
iteration 109, loss = 0.0012154460418969393
iteration 110, loss = 0.000836998806335032
iteration 111, loss = 0.0008300543413497508
iteration 112, loss = 0.0008740421035327017
iteration 113, loss = 0.0017102404963225126
iteration 114, loss = 0.0007464874652214348
iteration 115, loss = 0.0007186909206211567
iteration 116, loss = 0.0007376815192401409
iteration 117, loss = 0.0012091414537280798
iteration 118, loss = 0.0008590046199969947
iteration 119, loss = 0.0008436795906163752
iteration 120, loss = 0.0009956944268196821
iteration 121, loss = 0.000707289669662714
iteration 122, loss = 0.0007540237857028842
iteration 123, loss = 0.0010955664329230785
iteration 124, loss = 0.0007378371083177626
iteration 125, loss = 0.0012189347762614489
iteration 126, loss = 0.000870311283506453
iteration 127, loss = 0.0008119247504509985
iteration 128, loss = 0.0010172211332246661
iteration 129, loss = 0.0006900366861373186
iteration 130, loss = 0.0006984948995523155
iteration 131, loss = 0.0009670069557614625
iteration 132, loss = 0.0007379879243671894
iteration 133, loss = 0.000882000895217061
iteration 134, loss = 0.0007763643516227603
iteration 135, loss = 0.000697463983669877
iteration 136, loss = 0.0008636351558379829
iteration 137, loss = 0.0008711268310435116
iteration 138, loss = 0.0008665417553856969
iteration 139, loss = 0.0008915201760828495
iteration 140, loss = 0.0009244269458577037
iteration 141, loss = 0.0007642327691428363
iteration 142, loss = 0.0007750530494377017
iteration 143, loss = 0.001308086677454412
iteration 144, loss = 0.0007947791018523276
iteration 145, loss = 0.0008474845672026277
iteration 146, loss = 0.0009362439159303904
iteration 147, loss = 0.0010542089585214853
iteration 148, loss = 0.0007902821525931358
iteration 149, loss = 0.0008635580306872725
iteration 150, loss = 0.0018463635351508856
iteration 151, loss = 0.0008848030702210963
iteration 152, loss = 0.0009155147708952427
iteration 153, loss = 0.0007854699506424367
iteration 154, loss = 0.0013183871051296592
iteration 155, loss = 0.0010449271649122238
iteration 156, loss = 0.0008038471569307148
iteration 157, loss = 0.0008788298582658172
iteration 158, loss = 0.0008917894447222352
iteration 159, loss = 0.0007284847088158131
iteration 160, loss = 0.0010607505682855844
iteration 161, loss = 0.0008604798349551857
iteration 162, loss = 0.0007955300970934331
iteration 163, loss = 0.0006905864574946463
iteration 164, loss = 0.0012026206823065877
iteration 165, loss = 0.0008017356740310788
iteration 166, loss = 0.0011701303301379085
iteration 167, loss = 0.0010181943653151393
iteration 168, loss = 0.0009195743477903306
iteration 169, loss = 0.0008506455342285335
iteration 170, loss = 0.0020623300224542618
iteration 171, loss = 0.0009270955924876034
iteration 172, loss = 0.0011392069282010198
iteration 173, loss = 0.0009763073758222163
iteration 174, loss = 0.001225937157869339
iteration 175, loss = 0.0007196952356025577
iteration 176, loss = 0.0008468522573821247
iteration 177, loss = 0.0007378362352028489
iteration 178, loss = 0.0012510435190051794
iteration 179, loss = 0.0009477672865614295
iteration 180, loss = 0.00106112752109766
iteration 181, loss = 0.0008649251540191472
iteration 182, loss = 0.001703673042356968
iteration 183, loss = 0.0009286708664149046
iteration 184, loss = 0.0008494834182783961
iteration 185, loss = 0.0009066102793440223
iteration 186, loss = 0.0007669362239539623
iteration 187, loss = 0.0007167593576014042
iteration 188, loss = 0.0011369846761226654
iteration 189, loss = 0.0018160375766456127
iteration 190, loss = 0.0007955085602588952
iteration 191, loss = 0.0007711597136221826
iteration 192, loss = 0.0008849938167259097
iteration 193, loss = 0.001753764576278627
iteration 194, loss = 0.0006982887862250209
iteration 195, loss = 0.0008739627082832158
iteration 196, loss = 0.0016263346187770367
iteration 197, loss = 0.0015232758596539497
iteration 198, loss = 0.0009598489850759506
iteration 199, loss = 0.0009586259839124978
iteration 200, loss = 0.0009732124744914472
iteration 201, loss = 0.0009131029946729541
iteration 202, loss = 0.0006889078067615628
iteration 203, loss = 0.0011411479208618402
iteration 204, loss = 0.000714023713953793
iteration 205, loss = 0.0012894286774098873
iteration 206, loss = 0.0008095523808151484
iteration 207, loss = 0.00112713233102113
iteration 208, loss = 0.001053081825375557
iteration 209, loss = 0.0010319064604118466
iteration 210, loss = 0.0013725758763030171
iteration 211, loss = 0.0007271283539012074
iteration 212, loss = 0.001150518306531012
iteration 213, loss = 0.0008553011575713754
iteration 214, loss = 0.0015527006471529603
iteration 215, loss = 0.0010684070875868201
iteration 216, loss = 0.0010350566590204835
iteration 217, loss = 0.0008006946300156415
iteration 218, loss = 0.0008268527453765273
iteration 219, loss = 0.0009979034075513482
iteration 220, loss = 0.0008067794260568917
iteration 221, loss = 0.0009054430411197245
iteration 222, loss = 0.0007421180489473045
iteration 223, loss = 0.0016217861557379365
iteration 224, loss = 0.0009292796021327376
iteration 225, loss = 0.001445054542273283
iteration 226, loss = 0.0007751578814350069
iteration 227, loss = 0.0010627933079376817
iteration 228, loss = 0.0010380859021097422
iteration 229, loss = 0.0007135944906622171
iteration 230, loss = 0.0008829325088299811
iteration 231, loss = 0.0012226119870319963
iteration 232, loss = 0.001506455009803176
iteration 233, loss = 0.0008204886689782143
iteration 234, loss = 0.0006413880619220436
iteration 235, loss = 0.0008354769088327885
iteration 236, loss = 0.0007179332314990461
iteration 237, loss = 0.0012003517476841807
iteration 238, loss = 0.0015304323751479387
iteration 239, loss = 0.0007031216518953443
iteration 240, loss = 0.0014086328446865082
iteration 241, loss = 0.0008287280797958374
iteration 242, loss = 0.0009259122889488935
iteration 243, loss = 0.0008031868492253125
iteration 244, loss = 0.0016378770815208554
iteration 245, loss = 0.0007680029375478625
iteration 246, loss = 0.0014446248533204198
iteration 247, loss = 0.0012142545310780406
iteration 248, loss = 0.0015543848276138306
iteration 249, loss = 0.0009516513091512024
iteration 250, loss = 0.0010005413787439466
iteration 251, loss = 0.000881315441802144
iteration 252, loss = 0.0009096789290197194
iteration 253, loss = 0.001598839764483273
iteration 254, loss = 0.000956488074734807
iteration 255, loss = 0.0008359889616258442
iteration 256, loss = 0.0007821228355169296
iteration 257, loss = 0.0009124015923589468
iteration 258, loss = 0.0010144695406779647
iteration 259, loss = 0.0007942792726680636
iteration 260, loss = 0.0007673304644413292
iteration 261, loss = 0.0007316493429243565
iteration 262, loss = 0.0019106118706986308
iteration 263, loss = 0.0012162914499640465
iteration 264, loss = 0.001083020819351077
iteration 265, loss = 0.000807007891125977
iteration 266, loss = 0.0008862169925123453
iteration 267, loss = 0.0007697063847444952
iteration 268, loss = 0.000886423687916249
iteration 269, loss = 0.001169418334029615
iteration 270, loss = 0.0008122414583340287
iteration 271, loss = 0.0013568131253123283
iteration 272, loss = 0.000778768677264452
iteration 273, loss = 0.000875549390912056
iteration 274, loss = 0.0010099473875015974
iteration 275, loss = 0.0006514141568914056
iteration 276, loss = 0.0015662005171179771
iteration 277, loss = 0.0006978964083828032
iteration 278, loss = 0.0010200073011219501
iteration 279, loss = 0.0015350999310612679
iteration 280, loss = 0.0009629405103623867
iteration 281, loss = 0.0008992395014502108
iteration 282, loss = 0.0010659379186108708
iteration 283, loss = 0.001494859461672604
iteration 284, loss = 0.001142037333920598
iteration 285, loss = 0.0007458926411345601
iteration 286, loss = 0.0007623053970746696
iteration 287, loss = 0.0009631635039113462
iteration 288, loss = 0.0008686318760737777
iteration 289, loss = 0.0007479651249013841
iteration 290, loss = 0.001102787209674716
iteration 291, loss = 0.0007256565149873495
iteration 292, loss = 0.0009960330789908767
iteration 293, loss = 0.0015695608453825116
iteration 294, loss = 0.0014705928042531013
iteration 295, loss = 0.001022268203087151
iteration 296, loss = 0.0008487893501296639
iteration 297, loss = 0.0007509576389566064
iteration 298, loss = 0.0009621777571737766
iteration 299, loss = 0.0008228080696426332
iteration 300, loss = 0.0012341744732111692
iteration 1, loss = 0.0008073505014181137
iteration 2, loss = 0.0007142019458115101
iteration 3, loss = 0.0007410676334984601
iteration 4, loss = 0.0008681055041961372
iteration 5, loss = 0.0007589657325297594
iteration 6, loss = 0.0020697275176644325
iteration 7, loss = 0.0008908173767849803
iteration 8, loss = 0.0011154704261571169
iteration 9, loss = 0.0010257730027660728
iteration 10, loss = 0.0009718401706777513
iteration 11, loss = 0.0013994480250403285
iteration 12, loss = 0.0009819951374083757
iteration 13, loss = 0.000825753784738481
iteration 14, loss = 0.0012568305246531963
iteration 15, loss = 0.0012110089883208275
iteration 16, loss = 0.00118056102655828
iteration 17, loss = 0.0008338908664882183
iteration 18, loss = 0.0016026876401156187
iteration 19, loss = 0.0008970370981842279
iteration 20, loss = 0.00209174957126379
iteration 21, loss = 0.0006744208512827754
iteration 22, loss = 0.0013943593949079514
iteration 23, loss = 0.0011110486229881644
iteration 24, loss = 0.0008003131952136755
iteration 25, loss = 0.000902482308447361
iteration 26, loss = 0.0006874718819744885
iteration 27, loss = 0.0009625950478948653
iteration 28, loss = 0.0007361281896010041
iteration 29, loss = 0.0017003492685034871
iteration 30, loss = 0.001436237944290042
iteration 31, loss = 0.0009820647537708282
iteration 32, loss = 0.0016665947623550892
iteration 33, loss = 0.0007049253908917308
iteration 34, loss = 0.0006808285252191126
iteration 35, loss = 0.0008079810650087893
iteration 36, loss = 0.0013178035151213408
iteration 37, loss = 0.0007098648929968476
iteration 38, loss = 0.0009239030769094825
iteration 39, loss = 0.0007700541755184531
iteration 40, loss = 0.0008192303939722478
iteration 41, loss = 0.0006758298841305077
iteration 42, loss = 0.0008630036609247327
iteration 43, loss = 0.001493629883043468
iteration 44, loss = 0.0006851063226349652
iteration 45, loss = 0.0006995120784267783
iteration 46, loss = 0.001006005797535181
iteration 47, loss = 0.0010385867208242416
iteration 48, loss = 0.001520094694569707
iteration 49, loss = 0.0008449352462776005
iteration 50, loss = 0.0007741500157862902
iteration 51, loss = 0.001066600438207388
iteration 52, loss = 0.0008019375964067876
iteration 53, loss = 0.001602826057933271
iteration 54, loss = 0.0008219684241339564
iteration 55, loss = 0.001844872022047639
iteration 56, loss = 0.0009770203614607453
iteration 57, loss = 0.000771871127653867
iteration 58, loss = 0.0008658386650495231
iteration 59, loss = 0.0007687597535550594
iteration 60, loss = 0.0007824352942407131
iteration 61, loss = 0.0007288556080311537
iteration 62, loss = 0.0008080239640548825
iteration 63, loss = 0.0010722505394369364
iteration 64, loss = 0.0007879181066527963
iteration 65, loss = 0.0009487216011621058
iteration 66, loss = 0.0012252137530595064
iteration 67, loss = 0.0008134171366691589
iteration 68, loss = 0.0010146483546122909
iteration 69, loss = 0.0009684525430202484
iteration 70, loss = 0.000769987003877759
iteration 71, loss = 0.0009239345090463758
iteration 72, loss = 0.0008060483378358185
iteration 73, loss = 0.000816855113953352
iteration 74, loss = 0.0008352171862497926
iteration 75, loss = 0.001026428071781993
iteration 76, loss = 0.0011937760282307863
iteration 77, loss = 0.00082592130638659
iteration 78, loss = 0.0010957694612443447
iteration 79, loss = 0.0007152431644499302
iteration 80, loss = 0.0012036884436383843
iteration 81, loss = 0.0008181413868442178
iteration 82, loss = 0.0009895600378513336
iteration 83, loss = 0.0010344203328713775
iteration 84, loss = 0.0007838747696951032
iteration 85, loss = 0.0007890075212344527
iteration 86, loss = 0.0008859552326612175
iteration 87, loss = 0.0007579672383144498
iteration 88, loss = 0.0009214908932335675
iteration 89, loss = 0.0008216371643356979
iteration 90, loss = 0.0007187886512838304
iteration 91, loss = 0.0008499225950799882
iteration 92, loss = 0.0009453764650970697
iteration 93, loss = 0.0010010175174102187
iteration 94, loss = 0.0008230303646996617
iteration 95, loss = 0.0010953432647511363
iteration 96, loss = 0.0008486583246849477
iteration 97, loss = 0.0006982790655456483
iteration 98, loss = 0.0006834488594904542
iteration 99, loss = 0.0008069064933806658
iteration 100, loss = 0.0007624650606885552
iteration 101, loss = 0.0017175396205857396
iteration 102, loss = 0.0009123445488512516
iteration 103, loss = 0.0009656766196712852
iteration 104, loss = 0.000781818525865674
iteration 105, loss = 0.0008729263208806515
iteration 106, loss = 0.0007632831111550331
iteration 107, loss = 0.0008859090739861131
iteration 108, loss = 0.0007939887000247836
iteration 109, loss = 0.0007457254105247557
iteration 110, loss = 0.0006816425011493266
iteration 111, loss = 0.0009604105725884438
iteration 112, loss = 0.0015259033534675837
iteration 113, loss = 0.0011306828819215298
iteration 114, loss = 0.0007968787103891373
iteration 115, loss = 0.0007834676071070135
iteration 116, loss = 0.0010475863236933947
iteration 117, loss = 0.001062861061654985
iteration 118, loss = 0.0009000461432151496
iteration 119, loss = 0.0006677728961221874
iteration 120, loss = 0.0010307691991329193
iteration 121, loss = 0.0010660813422873616
iteration 122, loss = 0.0011893915943801403
iteration 123, loss = 0.0007439328473992646
iteration 124, loss = 0.001372982980683446
iteration 125, loss = 0.0007801835308782756
iteration 126, loss = 0.0008518505492247641
iteration 127, loss = 0.0010791923850774765
iteration 128, loss = 0.0009014151873998344
iteration 129, loss = 0.0010279817506670952
iteration 130, loss = 0.0012352477060630918
iteration 131, loss = 0.0012673313030973077
iteration 132, loss = 0.000992793939076364
iteration 133, loss = 0.001095531159080565
iteration 134, loss = 0.0007289797649718821
iteration 135, loss = 0.0009768216405063868
iteration 136, loss = 0.0008993003866635263
iteration 137, loss = 0.0008573587983846664
iteration 138, loss = 0.0008239318267442286
iteration 139, loss = 0.0008601396111771464
iteration 140, loss = 0.0010053341975435615
iteration 141, loss = 0.0016776786651462317
iteration 142, loss = 0.0011223519686609507
iteration 143, loss = 0.0007695863023400307
iteration 144, loss = 0.0009628627449274063
iteration 145, loss = 0.0007474262383766472
iteration 146, loss = 0.0008288972312584519
iteration 147, loss = 0.0011259936727583408
iteration 148, loss = 0.001232652342878282
iteration 149, loss = 0.0013845053035765886
iteration 150, loss = 0.0007516066543757915
iteration 151, loss = 0.0010391141986474395
iteration 152, loss = 0.0008494970388710499
iteration 153, loss = 0.0008583330782130361
iteration 154, loss = 0.0009313734481111169
iteration 155, loss = 0.0008625187329016626
iteration 156, loss = 0.0009512309916317463
iteration 157, loss = 0.0012381026754155755
iteration 158, loss = 0.0007509427377954125
iteration 159, loss = 0.0007405959768220782
iteration 160, loss = 0.0008347828988917172
iteration 161, loss = 0.0007478934712707996
iteration 162, loss = 0.0011002073297277093
iteration 163, loss = 0.0010278599802404642
iteration 164, loss = 0.0007238543475978076
iteration 165, loss = 0.0010317794512957335
iteration 166, loss = 0.0009769810130819678
iteration 167, loss = 0.002017832826822996
iteration 168, loss = 0.0009505799971520901
iteration 169, loss = 0.0008996798424050212
iteration 170, loss = 0.0015892796218395233
iteration 171, loss = 0.0008998144185170531
iteration 172, loss = 0.0006927851354703307
iteration 173, loss = 0.0012721428647637367
iteration 174, loss = 0.0011251929681748152
iteration 175, loss = 0.0015639791963621974
iteration 176, loss = 0.0008274519932456315
iteration 177, loss = 0.0010500134667381644
iteration 178, loss = 0.001408472191542387
iteration 179, loss = 0.0008274819701910019
iteration 180, loss = 0.0007961015799082816
iteration 181, loss = 0.0006826421013101935
iteration 182, loss = 0.0007677620742470026
iteration 183, loss = 0.0007793210097588599
iteration 184, loss = 0.0008945071022026241
iteration 185, loss = 0.0007467597024515271
iteration 186, loss = 0.000837495259474963
iteration 187, loss = 0.0011218128493055701
iteration 188, loss = 0.0011194563703611493
iteration 189, loss = 0.0006916997954249382
iteration 190, loss = 0.0006937502766959369
iteration 191, loss = 0.0006885104812681675
iteration 192, loss = 0.0007943675154820085
iteration 193, loss = 0.0007323548197746277
iteration 194, loss = 0.0009418549598194659
iteration 195, loss = 0.0007660601986572146
iteration 196, loss = 0.0010502788936719298
iteration 197, loss = 0.0011920677497982979
iteration 198, loss = 0.0010296526597812772
iteration 199, loss = 0.0014530934859067202
iteration 200, loss = 0.0010845514480024576
iteration 201, loss = 0.001096083433367312
iteration 202, loss = 0.001026674872264266
iteration 203, loss = 0.001519458252005279
iteration 204, loss = 0.0007381316390819848
iteration 205, loss = 0.0012852424988523126
iteration 206, loss = 0.0009232200682163239
iteration 207, loss = 0.001011812943033874
iteration 208, loss = 0.0008393450989387929
iteration 209, loss = 0.0007609374006278813
iteration 210, loss = 0.0009801826672628522
iteration 211, loss = 0.001106100040487945
iteration 212, loss = 0.000834207865409553
iteration 213, loss = 0.0006519604939967394
iteration 214, loss = 0.0009639646159484982
iteration 215, loss = 0.0011926692677661777
iteration 216, loss = 0.0009181536152027547
iteration 217, loss = 0.0007831212133169174
iteration 218, loss = 0.0009331476176157594
iteration 219, loss = 0.0009354443172924221
iteration 220, loss = 0.0008504306897521019
iteration 221, loss = 0.0010092717129737139
iteration 222, loss = 0.0010556393535807729
iteration 223, loss = 0.0006655966863036156
iteration 224, loss = 0.0007722284062765539
iteration 225, loss = 0.0010043529327958822
iteration 226, loss = 0.001688629388809204
iteration 227, loss = 0.0006911665550433099
iteration 228, loss = 0.0008893145131878555
iteration 229, loss = 0.0012559532187879086
iteration 230, loss = 0.000768197001889348
iteration 231, loss = 0.0009575089206919074
iteration 232, loss = 0.0008740093326196074
iteration 233, loss = 0.0008567413897253573
iteration 234, loss = 0.0009236828773282468
iteration 235, loss = 0.0008241769974119961
iteration 236, loss = 0.0008457318181172013
iteration 237, loss = 0.0008671758114360273
iteration 238, loss = 0.001437689526937902
iteration 239, loss = 0.0008183039026334882
iteration 240, loss = 0.0015612830175086856
iteration 241, loss = 0.0009273691102862358
iteration 242, loss = 0.0006456591072492301
iteration 243, loss = 0.0008450417080894113
iteration 244, loss = 0.0010990197770297527
iteration 245, loss = 0.0008069938630796969
iteration 246, loss = 0.0008203792385756969
iteration 247, loss = 0.0010623944690451026
iteration 248, loss = 0.0010811759857460856
iteration 249, loss = 0.0008214293629862368
iteration 250, loss = 0.001543213496915996
iteration 251, loss = 0.0008800934883765876
iteration 252, loss = 0.0008017654763534665
iteration 253, loss = 0.0009887642227113247
iteration 254, loss = 0.0006900763837620616
iteration 255, loss = 0.0008268784731626511
iteration 256, loss = 0.0008623992325738072
iteration 257, loss = 0.0010461423080414534
iteration 258, loss = 0.000673915958032012
iteration 259, loss = 0.0008677270961925387
iteration 260, loss = 0.0006882395246066153
iteration 261, loss = 0.0007975283078849316
iteration 262, loss = 0.0010175852803513408
iteration 263, loss = 0.0008803859236650169
iteration 264, loss = 0.0010882634669542313
iteration 265, loss = 0.0007597017684020102
iteration 266, loss = 0.0014644463080912828
iteration 267, loss = 0.0008963419240899384
iteration 268, loss = 0.0010890169069170952
iteration 269, loss = 0.0006702702958136797
iteration 270, loss = 0.0014718807069584727
iteration 271, loss = 0.0011711150873452425
iteration 272, loss = 0.0007189883617684245
iteration 273, loss = 0.001076410640962422
iteration 274, loss = 0.000829668715596199
iteration 275, loss = 0.0007707338081672788
iteration 276, loss = 0.0008754018926993012
iteration 277, loss = 0.0007780543528497219
iteration 278, loss = 0.0008595228428021073
iteration 279, loss = 0.001295223948545754
iteration 280, loss = 0.0009381251293234527
iteration 281, loss = 0.0007544606341980398
iteration 282, loss = 0.0007796873687766492
iteration 283, loss = 0.0008511992054991424
iteration 284, loss = 0.0009508823277428746
iteration 285, loss = 0.0016067672986537218
iteration 286, loss = 0.0008146764594130218
iteration 287, loss = 0.000948411354329437
iteration 288, loss = 0.0007635115180164576
iteration 289, loss = 0.0011472937185317278
iteration 290, loss = 0.0008783983066678047
iteration 291, loss = 0.0010583343682810664
iteration 292, loss = 0.0012444956228137016
iteration 293, loss = 0.0015354923671111465
iteration 294, loss = 0.0010955194011330605
iteration 295, loss = 0.000830451725050807
iteration 296, loss = 0.0007724573952145875
iteration 297, loss = 0.0006649874849244952
iteration 298, loss = 0.0008070629555732012
iteration 299, loss = 0.0009391711791977286
iteration 300, loss = 0.0024553900584578514
iteration 1, loss = 0.0010248267790302634
iteration 2, loss = 0.0016395533457398415
iteration 3, loss = 0.000940280151553452
iteration 4, loss = 0.0014915599022060633
iteration 5, loss = 0.0008151010842993855
iteration 6, loss = 0.0007574997143819928
iteration 7, loss = 0.0008084325236268342
iteration 8, loss = 0.0014910934260115027
iteration 9, loss = 0.0008550210623070598
iteration 10, loss = 0.0010373429395258427
iteration 11, loss = 0.000686075771227479
iteration 12, loss = 0.0015790293691679835
iteration 13, loss = 0.0011338734766468406
iteration 14, loss = 0.0016615414060652256
iteration 15, loss = 0.0007980985101312399
iteration 16, loss = 0.001173000899143517
iteration 17, loss = 0.00078912841854617
iteration 18, loss = 0.0010916093597188592
iteration 19, loss = 0.0008546755416318774
iteration 20, loss = 0.0008572515798732638
iteration 21, loss = 0.0009935131529346108
iteration 22, loss = 0.0008125118911266327
iteration 23, loss = 0.000736543966922909
iteration 24, loss = 0.0008185110054910183
iteration 25, loss = 0.0008470680331811309
iteration 26, loss = 0.0009602431673556566
iteration 27, loss = 0.0006981887272559106
iteration 28, loss = 0.0009301481768488884
iteration 29, loss = 0.0009853512747213244
iteration 30, loss = 0.0008863706607371569
iteration 31, loss = 0.001377012231387198
iteration 32, loss = 0.000812515732832253
iteration 33, loss = 0.001067701610736549
iteration 34, loss = 0.0007382483454421163
iteration 35, loss = 0.0012666070833802223
iteration 36, loss = 0.000720635405741632
iteration 37, loss = 0.0011538522085174918
iteration 38, loss = 0.0010022064670920372
iteration 39, loss = 0.0007482738001272082
iteration 40, loss = 0.0008109874906949699
iteration 41, loss = 0.0008029943564906716
iteration 42, loss = 0.0008833966567181051
iteration 43, loss = 0.0012710251612588763
iteration 44, loss = 0.0007707424811087549
iteration 45, loss = 0.0012015564134344459
iteration 46, loss = 0.001761418767273426
iteration 47, loss = 0.0014817100018262863
iteration 48, loss = 0.0014457731740549207
iteration 49, loss = 0.0015288761351257563
iteration 50, loss = 0.0012866907054558396
iteration 51, loss = 0.0012270869920030236
iteration 52, loss = 0.0006725096609443426
iteration 53, loss = 0.0007706281612627208
iteration 54, loss = 0.000837340543512255
iteration 55, loss = 0.0016507114050909877
iteration 56, loss = 0.0006767993909306824
iteration 57, loss = 0.0007311621448025107
iteration 58, loss = 0.0009822833817452192
iteration 59, loss = 0.0010759691940620542
iteration 60, loss = 0.0011524182045832276
iteration 61, loss = 0.0008565941825509071
iteration 62, loss = 0.0007194910431280732
iteration 63, loss = 0.0007184974965639412
iteration 64, loss = 0.0009573832503519952
iteration 65, loss = 0.0008186428458429873
iteration 66, loss = 0.0008256026776507497
iteration 67, loss = 0.0006662967498414218
iteration 68, loss = 0.001176247838884592
iteration 69, loss = 0.0008965686429291964
iteration 70, loss = 0.0007542847888544202
iteration 71, loss = 0.000997638562694192
iteration 72, loss = 0.0009581084013916552
iteration 73, loss = 0.001263658981770277
iteration 74, loss = 0.0009444469469599426
iteration 75, loss = 0.0014644344337284565
iteration 76, loss = 0.0011683376505970955
iteration 77, loss = 0.0007570539601147175
iteration 78, loss = 0.0009188355761580169
iteration 79, loss = 0.0008275306900031865
iteration 80, loss = 0.0010485367383807898
iteration 81, loss = 0.0007499362691305578
iteration 82, loss = 0.0007779970183037221
iteration 83, loss = 0.0008451871108263731
iteration 84, loss = 0.0011468803277239203
iteration 85, loss = 0.0008000384550541639
iteration 86, loss = 0.0009475905681028962
iteration 87, loss = 0.0016901168273761868
iteration 88, loss = 0.0012001723516732454
iteration 89, loss = 0.001078362693078816
iteration 90, loss = 0.0010214406065642834
iteration 91, loss = 0.0007228002650663257
iteration 92, loss = 0.001253901980817318
iteration 93, loss = 0.0007261614664457738
iteration 94, loss = 0.0015182661591097713
iteration 95, loss = 0.0008488431922160089
iteration 96, loss = 0.0007353753899224102
iteration 97, loss = 0.0007939739734865725
iteration 98, loss = 0.000747992075048387
iteration 99, loss = 0.001586406142450869
iteration 100, loss = 0.0010236117523163557
iteration 101, loss = 0.0008289686520583928
iteration 102, loss = 0.0015236824983730912
iteration 103, loss = 0.0008263754425570369
iteration 104, loss = 0.0007055569440126419
iteration 105, loss = 0.0007188447052612901
iteration 106, loss = 0.0007985251140780747
iteration 107, loss = 0.0008453687187284231
iteration 108, loss = 0.0010511282598599792
iteration 109, loss = 0.0007132590399123728
iteration 110, loss = 0.0007684589363634586
iteration 111, loss = 0.0009428069461137056
iteration 112, loss = 0.0012165072839707136
iteration 113, loss = 0.0008751233690418303
iteration 114, loss = 0.00134954578243196
iteration 115, loss = 0.0010376659920439124
iteration 116, loss = 0.0007789587834849954
iteration 117, loss = 0.0008863488910719752
iteration 118, loss = 0.0007200854015536606
iteration 119, loss = 0.0009025239269249141
iteration 120, loss = 0.0008483898127451539
iteration 121, loss = 0.0011263586347922683
iteration 122, loss = 0.0008240676252171397
iteration 123, loss = 0.001222560298629105
iteration 124, loss = 0.0007234518998302519
iteration 125, loss = 0.0007615220383740962
iteration 126, loss = 0.0010228150058537722
iteration 127, loss = 0.0008879652596078813
iteration 128, loss = 0.0011614086106419563
iteration 129, loss = 0.0007982385577633977
iteration 130, loss = 0.0010344884358346462
iteration 131, loss = 0.0008326568640768528
iteration 132, loss = 0.000765654374845326
iteration 133, loss = 0.0007405946962535381
iteration 134, loss = 0.0008121802820824087
iteration 135, loss = 0.000709639978595078
iteration 136, loss = 0.0007669557235203683
iteration 137, loss = 0.0006216263864189386
iteration 138, loss = 0.0007897354080341756
iteration 139, loss = 0.0006323153502307832
iteration 140, loss = 0.0008720000041648746
iteration 141, loss = 0.0007690888596698642
iteration 142, loss = 0.0007750563672743738
iteration 143, loss = 0.0007459179032593966
iteration 144, loss = 0.001519297598861158
iteration 145, loss = 0.0009109774837270379
iteration 146, loss = 0.001717018662020564
iteration 147, loss = 0.0010499805212020874
iteration 148, loss = 0.001119750551879406
iteration 149, loss = 0.0007644942379556596
iteration 150, loss = 0.0009336585062555969
iteration 151, loss = 0.0008591561345383525
iteration 152, loss = 0.0015602908097207546
iteration 153, loss = 0.0008486522128805518
iteration 154, loss = 0.0006964103085920215
iteration 155, loss = 0.0007508124690502882
iteration 156, loss = 0.0018513804534450173
iteration 157, loss = 0.0009116139262914658
iteration 158, loss = 0.0009409001213498414
iteration 159, loss = 0.0008910446777008474
iteration 160, loss = 0.001158687868155539
iteration 161, loss = 0.0009242059313692153
iteration 162, loss = 0.0010577738285064697
iteration 163, loss = 0.0006600506603717804
iteration 164, loss = 0.0008089217008091509
iteration 165, loss = 0.0009763517882674932
iteration 166, loss = 0.0011343384394422174
iteration 167, loss = 0.0008951345225796103
iteration 168, loss = 0.0016651460900902748
iteration 169, loss = 0.0011810093419626355
iteration 170, loss = 0.001137180021032691
iteration 171, loss = 0.0008421777747571468
iteration 172, loss = 0.0008451060857623816
iteration 173, loss = 0.0007547200075350702
iteration 174, loss = 0.0008428330183960497
iteration 175, loss = 0.0009847836336120963
iteration 176, loss = 0.0009178444161079824
iteration 177, loss = 0.0007467616233043373
iteration 178, loss = 0.0008473074994981289
iteration 179, loss = 0.0008362213266082108
iteration 180, loss = 0.0015067944768816233
iteration 181, loss = 0.0015474599786102772
iteration 182, loss = 0.0008330844575539231
iteration 183, loss = 0.0008561473805457354
iteration 184, loss = 0.0007872109999880195
iteration 185, loss = 0.0006567230448126793
iteration 186, loss = 0.0009807285387068987
iteration 187, loss = 0.0010022206697613
iteration 188, loss = 0.0011344410013407469
iteration 189, loss = 0.0009526759386062622
iteration 190, loss = 0.0007511083385907114
iteration 191, loss = 0.0009642117656767368
iteration 192, loss = 0.0012963726185262203
iteration 193, loss = 0.0009907621424645185
iteration 194, loss = 0.001369331032037735
iteration 195, loss = 0.0008709703688509762
iteration 196, loss = 0.0008011974859982729
iteration 197, loss = 0.0009075304260477424
iteration 198, loss = 0.0007985960110090673
iteration 199, loss = 0.0012376434169709682
iteration 200, loss = 0.0007582915714010596
iteration 201, loss = 0.0008168069762177765
iteration 202, loss = 0.0009518635342828929
iteration 203, loss = 0.0009599759941920638
iteration 204, loss = 0.0009575593867339194
iteration 205, loss = 0.0011772203724831343
iteration 206, loss = 0.0008155541145242751
iteration 207, loss = 0.00070756342029199
iteration 208, loss = 0.0010777446441352367
iteration 209, loss = 0.0011145741445943713
iteration 210, loss = 0.0007528734859079123
iteration 211, loss = 0.0008429671288467944
iteration 212, loss = 0.0007875156006775796
iteration 213, loss = 0.000982710043899715
iteration 214, loss = 0.0015891790390014648
iteration 215, loss = 0.0008837835630401969
iteration 216, loss = 0.0007836730219423771
iteration 217, loss = 0.0010854963911697268
iteration 218, loss = 0.0016324728494510055
iteration 219, loss = 0.0013397503644227982
iteration 220, loss = 0.0007317718118429184
iteration 221, loss = 0.0007812886033207178
iteration 222, loss = 0.0008028025040403008
iteration 223, loss = 0.0010164182167500257
iteration 224, loss = 0.0011553835356608033
iteration 225, loss = 0.0007082957308739424
iteration 226, loss = 0.0006684227264486253
iteration 227, loss = 0.0008879746892489493
iteration 228, loss = 0.000770923332311213
iteration 229, loss = 0.001187794143334031
iteration 230, loss = 0.0008231219835579395
iteration 231, loss = 0.0014470919268205762
iteration 232, loss = 0.0009213110897690058
iteration 233, loss = 0.0016383043257519603
iteration 234, loss = 0.0018308113794773817
iteration 235, loss = 0.0007899255724623799
iteration 236, loss = 0.001064348965883255
iteration 237, loss = 0.0013436645967885852
iteration 238, loss = 0.0009455802501179278
iteration 239, loss = 0.0009000624995678663
iteration 240, loss = 0.0008957655518315732
iteration 241, loss = 0.0016223628772422671
iteration 242, loss = 0.0008635448757559061
iteration 243, loss = 0.0008989181369543076
iteration 244, loss = 0.0007661311537958682
iteration 245, loss = 0.0009494461119174957
iteration 246, loss = 0.001180867780931294
iteration 247, loss = 0.0008137101540341973
iteration 248, loss = 0.0008744224323891103
iteration 249, loss = 0.0008125576423481107
iteration 250, loss = 0.0008336519822478294
iteration 251, loss = 0.0009919509757310152
iteration 252, loss = 0.0010498414048925042
iteration 253, loss = 0.0011034482158720493
iteration 254, loss = 0.0009077804279513657
iteration 255, loss = 0.0011564631713554263
iteration 256, loss = 0.001105701201595366
iteration 257, loss = 0.0008461590623483062
iteration 258, loss = 0.0009457728592678905
iteration 259, loss = 0.0007492070435546339
iteration 260, loss = 0.0009917453862726688
iteration 261, loss = 0.0007330117514356971
iteration 262, loss = 0.0009735178900882602
iteration 263, loss = 0.0007567412685602903
iteration 264, loss = 0.000870442483574152
iteration 265, loss = 0.001064193551428616
iteration 266, loss = 0.0012893307721242309
iteration 267, loss = 0.000776585889980197
iteration 268, loss = 0.0007852215203456581
iteration 269, loss = 0.0008632884128019214
iteration 270, loss = 0.000813228078186512
iteration 271, loss = 0.001195305958390236
iteration 272, loss = 0.0008714699069969356
iteration 273, loss = 0.0006805172306485474
iteration 274, loss = 0.000721708289347589
iteration 275, loss = 0.0015402333810925484
iteration 276, loss = 0.0009079378214664757
iteration 277, loss = 0.0009026892948895693
iteration 278, loss = 0.0011358626652508974
iteration 279, loss = 0.0010487943654879928
iteration 280, loss = 0.00077663833508268
iteration 281, loss = 0.000906153698451817
iteration 282, loss = 0.0010797663126140833
iteration 283, loss = 0.0006622879882343113
iteration 284, loss = 0.0009239662322215736
iteration 285, loss = 0.0008785774116404355
iteration 286, loss = 0.0007846191874705255
iteration 287, loss = 0.0007334663532674313
iteration 288, loss = 0.0007307674386538565
iteration 289, loss = 0.0008239289745688438
iteration 290, loss = 0.0010798875009641051
iteration 291, loss = 0.0011613938258960843
iteration 292, loss = 0.0008253743289969862
iteration 293, loss = 0.0006971062393859029
iteration 294, loss = 0.0010332681704312563
iteration 295, loss = 0.0007426182855851948
iteration 296, loss = 0.0007728800410404801
iteration 297, loss = 0.000780987145844847
iteration 298, loss = 0.0008989655179902911
iteration 299, loss = 0.0013012533308938146
iteration 300, loss = 0.0010404697386547923
iteration 1, loss = 0.0012321923859417439
iteration 2, loss = 0.0009703365503810346
iteration 3, loss = 0.0008044950664043427
iteration 4, loss = 0.0006389769841916859
iteration 5, loss = 0.0007587770232930779
iteration 6, loss = 0.000720963638741523
iteration 7, loss = 0.0008287047385238111
iteration 8, loss = 0.0008867221185937524
iteration 9, loss = 0.000885242479853332
iteration 10, loss = 0.0007384500931948423
iteration 11, loss = 0.0008101626881398261
iteration 12, loss = 0.0007423481438308954
iteration 13, loss = 0.0007739260909147561
iteration 14, loss = 0.0007216960075311363
iteration 15, loss = 0.0011683059856295586
iteration 16, loss = 0.000817439635284245
iteration 17, loss = 0.0014808083651587367
iteration 18, loss = 0.001616692985408008
iteration 19, loss = 0.0008796175825409591
iteration 20, loss = 0.0008827844867482781
iteration 21, loss = 0.0008563703158870339
iteration 22, loss = 0.0009381819400005043
iteration 23, loss = 0.0008573203231208026
iteration 24, loss = 0.0015743615804240108
iteration 25, loss = 0.0010419515892863274
iteration 26, loss = 0.0008899356471374631
iteration 27, loss = 0.0011976148234680295
iteration 28, loss = 0.0014328191755339503
iteration 29, loss = 0.0008108298061415553
iteration 30, loss = 0.001566678169183433
iteration 31, loss = 0.0009668945567682385
iteration 32, loss = 0.001104312133975327
iteration 33, loss = 0.000795767642557621
iteration 34, loss = 0.0015421346761286259
iteration 35, loss = 0.0015247634146362543
iteration 36, loss = 0.0009310998138971627
iteration 37, loss = 0.0008118320256471634
iteration 38, loss = 0.0013994593173265457
iteration 39, loss = 0.0008116918033920228
iteration 40, loss = 0.0008470634929835796
iteration 41, loss = 0.0008903254638426006
iteration 42, loss = 0.0008012045873329043
iteration 43, loss = 0.0014922111295163631
iteration 44, loss = 0.0006920294836163521
iteration 45, loss = 0.0010056088212877512
iteration 46, loss = 0.0009619257180020213
iteration 47, loss = 0.0008288040407933295
iteration 48, loss = 0.0008464513812214136
iteration 49, loss = 0.000795508676674217
iteration 50, loss = 0.0015947911888360977
iteration 51, loss = 0.0011320990743115544
iteration 52, loss = 0.0009237157646566629
iteration 53, loss = 0.0006948733935132623
iteration 54, loss = 0.00074081274215132
iteration 55, loss = 0.0008236159919761121
iteration 56, loss = 0.0009800888365134597
iteration 57, loss = 0.0009280699887312949
iteration 58, loss = 0.0008767193648964167
iteration 59, loss = 0.0008975015953183174
iteration 60, loss = 0.0010154569754377007
iteration 61, loss = 0.0015933113172650337
iteration 62, loss = 0.000779318215791136
iteration 63, loss = 0.000695790397003293
iteration 64, loss = 0.0011985624441877007
iteration 65, loss = 0.0008340957574546337
iteration 66, loss = 0.0006584111833944917
iteration 67, loss = 0.0008284021751023829
iteration 68, loss = 0.0008437313954345882
iteration 69, loss = 0.001727786031551659
iteration 70, loss = 0.0009616203606128693
iteration 71, loss = 0.0013900274643674493
iteration 72, loss = 0.0014781823847442865
iteration 73, loss = 0.0013237043749541044
iteration 74, loss = 0.0008093044743873179
iteration 75, loss = 0.001804308732971549
iteration 76, loss = 0.0008451978210359812
iteration 77, loss = 0.0007865568622946739
iteration 78, loss = 0.0007742131710983813
iteration 79, loss = 0.00103382405359298
iteration 80, loss = 0.0010719873243942857
iteration 81, loss = 0.0007812320254743099
iteration 82, loss = 0.0008499539690092206
iteration 83, loss = 0.0008240288589149714
iteration 84, loss = 0.0008033556514419615
iteration 85, loss = 0.0011628941865637898
iteration 86, loss = 0.0013875269796699286
iteration 87, loss = 0.0011964838486164808
iteration 88, loss = 0.0008483374840579927
iteration 89, loss = 0.0008173626265488565
iteration 90, loss = 0.0007522880332544446
iteration 91, loss = 0.0008481464465148747
iteration 92, loss = 0.0007428586832247674
iteration 93, loss = 0.0006623356021009386
iteration 94, loss = 0.000855427177157253
iteration 95, loss = 0.0006926889764145017
iteration 96, loss = 0.0010419738246127963
iteration 97, loss = 0.0007927861297503114
iteration 98, loss = 0.0007675299420952797
iteration 99, loss = 0.0017441059462726116
iteration 100, loss = 0.0010150077287107706
iteration 101, loss = 0.0008763186633586884
iteration 102, loss = 0.0009002971928566694
iteration 103, loss = 0.0009840073762461543
iteration 104, loss = 0.0016270147170871496
iteration 105, loss = 0.000929543690290302
iteration 106, loss = 0.0008975396049208939
iteration 107, loss = 0.001549313310533762
iteration 108, loss = 0.0007863407954573631
iteration 109, loss = 0.0012219541240483522
iteration 110, loss = 0.0015077756252139807
iteration 111, loss = 0.0008264037896879017
iteration 112, loss = 0.0008716267184354365
iteration 113, loss = 0.0008568253833800554
iteration 114, loss = 0.0008351670112460852
iteration 115, loss = 0.0009311985922977328
iteration 116, loss = 0.0006778399692848325
iteration 117, loss = 0.0008945988374762237
iteration 118, loss = 0.0008830263977870345
iteration 119, loss = 0.0008798708440735936
iteration 120, loss = 0.0009266880224458873
iteration 121, loss = 0.0007290111389011145
iteration 122, loss = 0.0007950466824695468
iteration 123, loss = 0.0009163572685793042
iteration 124, loss = 0.0008777456823736429
iteration 125, loss = 0.0015779286623001099
iteration 126, loss = 0.0014657326973974705
iteration 127, loss = 0.0006717402138747275
iteration 128, loss = 0.0008552994695492089
iteration 129, loss = 0.000982816913165152
iteration 130, loss = 0.0010557014029473066
iteration 131, loss = 0.0008256284636445343
iteration 132, loss = 0.0011608034837991
iteration 133, loss = 0.0007179068634286523
iteration 134, loss = 0.0007898352923803031
iteration 135, loss = 0.000896703393664211
iteration 136, loss = 0.0008374119061045349
iteration 137, loss = 0.002260112902149558
iteration 138, loss = 0.0007829646929167211
iteration 139, loss = 0.0007698622066527605
iteration 140, loss = 0.0009087985963560641
iteration 141, loss = 0.0007253949297592044
iteration 142, loss = 0.0007566832355223596
iteration 143, loss = 0.0007202692213468254
iteration 144, loss = 0.00158224836923182
iteration 145, loss = 0.0007578888325951993
iteration 146, loss = 0.001581569784320891
iteration 147, loss = 0.0008926583104766905
iteration 148, loss = 0.0007513819145970047
iteration 149, loss = 0.0008544474840164185
iteration 150, loss = 0.0008248348021879792
iteration 151, loss = 0.000721586518920958
iteration 152, loss = 0.0007739586289972067
iteration 153, loss = 0.0014522280544042587
iteration 154, loss = 0.000736307178158313
iteration 155, loss = 0.0007979519432410598
iteration 156, loss = 0.000730374944396317
iteration 157, loss = 0.0008268660167232156
iteration 158, loss = 0.0009047893108800054
iteration 159, loss = 0.001229291083291173
iteration 160, loss = 0.0016506962710991502
iteration 161, loss = 0.0008166756015270948
iteration 162, loss = 0.0009801893029361963
iteration 163, loss = 0.0006957349833101034
iteration 164, loss = 0.0015399918192997575
iteration 165, loss = 0.000938493525609374
iteration 166, loss = 0.0008771581342443824
iteration 167, loss = 0.0006839482812210917
iteration 168, loss = 0.0009261658997274935
iteration 169, loss = 0.0007985084084793925
iteration 170, loss = 0.000836015329696238
iteration 171, loss = 0.0013027398381382227
iteration 172, loss = 0.0011744566727429628
iteration 173, loss = 0.0007243538275361061
iteration 174, loss = 0.0014839840587228537
iteration 175, loss = 0.0011553491931408644
iteration 176, loss = 0.000939558376558125
iteration 177, loss = 0.0008084318251349032
iteration 178, loss = 0.0008638767758384347
iteration 179, loss = 0.0020420195069164038
iteration 180, loss = 0.0008721500053070486
iteration 181, loss = 0.0012626979732885957
iteration 182, loss = 0.0007738007698208094
iteration 183, loss = 0.0007762729655951262
iteration 184, loss = 0.0007220073021017015
iteration 185, loss = 0.0007159748929552734
iteration 186, loss = 0.0009436386171728373
iteration 187, loss = 0.0010252862703055143
iteration 188, loss = 0.0006841159774921834
iteration 189, loss = 0.0009626412065699697
iteration 190, loss = 0.000691320514306426
iteration 191, loss = 0.0009463994647376239
iteration 192, loss = 0.0007493799203075469
iteration 193, loss = 0.0007293610251508653
iteration 194, loss = 0.0007706914911977947
iteration 195, loss = 0.0007857244927436113
iteration 196, loss = 0.00119536102283746
iteration 197, loss = 0.0008587477495893836
iteration 198, loss = 0.0007836929871700704
iteration 199, loss = 0.0007425479125231504
iteration 200, loss = 0.0006527220830321312
iteration 201, loss = 0.000700770819094032
iteration 202, loss = 0.0010054722661152482
iteration 203, loss = 0.0008314149454236031
iteration 204, loss = 0.0009039600263349712
iteration 205, loss = 0.001437642378732562
iteration 206, loss = 0.0007667451864108443
iteration 207, loss = 0.0008224218618124723
iteration 208, loss = 0.0009198182378895581
iteration 209, loss = 0.0011063313577324152
iteration 210, loss = 0.00152696599252522
iteration 211, loss = 0.0010641308035701513
iteration 212, loss = 0.0008479096577502787
iteration 213, loss = 0.0017100406112149358
iteration 214, loss = 0.0017999028787016869
iteration 215, loss = 0.0008958214893937111
iteration 216, loss = 0.0009136698208749294
iteration 217, loss = 0.000664469669573009
iteration 218, loss = 0.0007126175914891064
iteration 219, loss = 0.0008027430158108473
iteration 220, loss = 0.0008214045083150268
iteration 221, loss = 0.0008846589480526745
iteration 222, loss = 0.0008002995164133608
iteration 223, loss = 0.0008943250286392868
iteration 224, loss = 0.0006993220886215568
iteration 225, loss = 0.0009198964689858258
iteration 226, loss = 0.0012437172699719667
iteration 227, loss = 0.0007637119269929826
iteration 228, loss = 0.001236257259733975
iteration 229, loss = 0.0006993879796937108
iteration 230, loss = 0.0012211765861138701
iteration 231, loss = 0.0010691892821341753
iteration 232, loss = 0.0008425391279160976
iteration 233, loss = 0.0010187341831624508
iteration 234, loss = 0.0008355695172213018
iteration 235, loss = 0.0012001912109553814
iteration 236, loss = 0.0014816420152783394
iteration 237, loss = 0.000795136031229049
iteration 238, loss = 0.0009225595276802778
iteration 239, loss = 0.0014474567724391818
iteration 240, loss = 0.0010142724495381117
iteration 241, loss = 0.0006939187296666205
iteration 242, loss = 0.0008874835912138224
iteration 243, loss = 0.000972709443885833
iteration 244, loss = 0.0007881837664172053
iteration 245, loss = 0.0010617454536259174
iteration 246, loss = 0.001394458580762148
iteration 247, loss = 0.0007273406954482198
iteration 248, loss = 0.0007766094640828669
iteration 249, loss = 0.001465073088183999
iteration 250, loss = 0.0008521678391844034
iteration 251, loss = 0.000882643973454833
iteration 252, loss = 0.0009191136923618615
iteration 253, loss = 0.0008343446534126997
iteration 254, loss = 0.0013528193812817335
iteration 255, loss = 0.0007334245019592345
iteration 256, loss = 0.0007678278489038348
iteration 257, loss = 0.0010523881064727902
iteration 258, loss = 0.0008456003270111978
iteration 259, loss = 0.000927936693187803
iteration 260, loss = 0.0007362264441326261
iteration 261, loss = 0.001556855859234929
iteration 262, loss = 0.000904835993424058
iteration 263, loss = 0.0007807546644471586
iteration 264, loss = 0.0009587302920408547
iteration 265, loss = 0.000897213292773813
iteration 266, loss = 0.0008863054681569338
iteration 267, loss = 0.0009473961545154452
iteration 268, loss = 0.0008557598339393735
iteration 269, loss = 0.0007510899449698627
iteration 270, loss = 0.0009762014960870147
iteration 271, loss = 0.0012451852671802044
iteration 272, loss = 0.001046447898261249
iteration 273, loss = 0.0008410795708186924
iteration 274, loss = 0.0009813351789489388
iteration 275, loss = 0.0007976631168276072
iteration 276, loss = 0.0008377810008823872
iteration 277, loss = 0.0009041689918376505
iteration 278, loss = 0.0008599473512731493
iteration 279, loss = 0.0006655442412011325
iteration 280, loss = 0.0011745826341211796
iteration 281, loss = 0.0010892025893554091
iteration 282, loss = 0.000712430221028626
iteration 283, loss = 0.001373151782900095
iteration 284, loss = 0.001720069907605648
iteration 285, loss = 0.0009984656935557723
iteration 286, loss = 0.0009044954786077142
iteration 287, loss = 0.000708200444933027
iteration 288, loss = 0.0008244203054346144
iteration 289, loss = 0.0011741762282326818
iteration 290, loss = 0.0009453262318857014
iteration 291, loss = 0.0008558137342333794
iteration 292, loss = 0.0007352100219577551
iteration 293, loss = 0.0008406452834606171
iteration 294, loss = 0.0016794814728200436
iteration 295, loss = 0.0007819468155503273
iteration 296, loss = 0.0008842853712849319
iteration 297, loss = 0.0008768977713771164
iteration 298, loss = 0.000866033835336566
iteration 299, loss = 0.0007803005864843726
iteration 300, loss = 0.0007647567545063794
iteration 1, loss = 0.000738713366445154
iteration 2, loss = 0.000801135553047061
iteration 3, loss = 0.0008606589981354773
iteration 4, loss = 0.0009382100543007255
iteration 5, loss = 0.0007876992458477616
iteration 6, loss = 0.0010455271694809198
iteration 7, loss = 0.0007542615057900548
iteration 8, loss = 0.0011167482007294893
iteration 9, loss = 0.0010069545824080706
iteration 10, loss = 0.0018391191260889173
iteration 11, loss = 0.0007329307845793664
iteration 12, loss = 0.0011866604909300804
iteration 13, loss = 0.0024319570511579514
iteration 14, loss = 0.0009188850526697934
iteration 15, loss = 0.0008163050515577197
iteration 16, loss = 0.0007394733256660402
iteration 17, loss = 0.0008826670236885548
iteration 18, loss = 0.0022265266161412
iteration 19, loss = 0.0008742024656385183
iteration 20, loss = 0.0008759790798649192
iteration 21, loss = 0.0010016098385676742
iteration 22, loss = 0.0008320049964822829
iteration 23, loss = 0.001117319450713694
iteration 24, loss = 0.0008883835980668664
iteration 25, loss = 0.0007012502755969763
iteration 26, loss = 0.000849638192448765
iteration 27, loss = 0.0009545600041747093
iteration 28, loss = 0.0007317092968150973
iteration 29, loss = 0.0009198691695928574
iteration 30, loss = 0.0012157558230683208
iteration 31, loss = 0.0010780340526252985
iteration 32, loss = 0.0008675280259922147
iteration 33, loss = 0.0007977578206919134
iteration 34, loss = 0.0006796668167226017
iteration 35, loss = 0.0009969677776098251
iteration 36, loss = 0.0010348481591790915
iteration 37, loss = 0.001414737431332469
iteration 38, loss = 0.00069336814340204
iteration 39, loss = 0.0008281148038804531
iteration 40, loss = 0.0016118035418912768
iteration 41, loss = 0.0011210785014554858
iteration 42, loss = 0.0009145269868895411
iteration 43, loss = 0.0006860940484330058
iteration 44, loss = 0.0008307035895995796
iteration 45, loss = 0.000888916605617851
iteration 46, loss = 0.0006545241922140121
iteration 47, loss = 0.0008296296000480652
iteration 48, loss = 0.0008435138734057546
iteration 49, loss = 0.0008468467276543379
iteration 50, loss = 0.0007118104840628803
iteration 51, loss = 0.0009622112265788019
iteration 52, loss = 0.0008693226845934987
iteration 53, loss = 0.000911049370188266
iteration 54, loss = 0.0016286780592054129
iteration 55, loss = 0.0012605047086253762
iteration 56, loss = 0.001042433432303369
iteration 57, loss = 0.000914036063477397
iteration 58, loss = 0.0008243256597779691
iteration 59, loss = 0.0007958643836900592
iteration 60, loss = 0.0009622907964512706
iteration 61, loss = 0.0016089841956272721
iteration 62, loss = 0.0011786543764173985
iteration 63, loss = 0.0009641151409596205
iteration 64, loss = 0.000834610138554126
iteration 65, loss = 0.0010281064314767718
iteration 66, loss = 0.000688106520101428
iteration 67, loss = 0.0009435912943445146
iteration 68, loss = 0.0014377136249095201
iteration 69, loss = 0.0008626302587799728
iteration 70, loss = 0.0007489258423447609
iteration 71, loss = 0.001262701116502285
iteration 72, loss = 0.0016187536530196667
iteration 73, loss = 0.0008434156188741326
iteration 74, loss = 0.0011834749020636082
iteration 75, loss = 0.0006927074864506721
iteration 76, loss = 0.0007868928951211274
iteration 77, loss = 0.0006631549913436174
iteration 78, loss = 0.0007650713669136167
iteration 79, loss = 0.0008640477899461985
iteration 80, loss = 0.0011616533156484365
iteration 81, loss = 0.0011290785623714328
iteration 82, loss = 0.0006968098459765315
iteration 83, loss = 0.0011864262633025646
iteration 84, loss = 0.0006503707845695317
iteration 85, loss = 0.0006971373222768307
iteration 86, loss = 0.0015790538163855672
iteration 87, loss = 0.0017385634128004313
iteration 88, loss = 0.0011557723628357053
iteration 89, loss = 0.0007389553356915712
iteration 90, loss = 0.0009654863970354199
iteration 91, loss = 0.0008187975618056953
iteration 92, loss = 0.0010567593853920698
iteration 93, loss = 0.0014876981731504202
iteration 94, loss = 0.0009130331454798579
iteration 95, loss = 0.0009469480137340724
iteration 96, loss = 0.0009493088000454009
iteration 97, loss = 0.0007935562753118575
iteration 98, loss = 0.0010309594217687845
iteration 99, loss = 0.0006638312479481101
iteration 100, loss = 0.001049510668963194
iteration 101, loss = 0.0008344699745066464
iteration 102, loss = 0.0008826408302411437
iteration 103, loss = 0.0008217794820666313
iteration 104, loss = 0.0008580430876463652
iteration 105, loss = 0.0007448708056472242
iteration 106, loss = 0.0008179921424016356
iteration 107, loss = 0.0007762018940411508
iteration 108, loss = 0.000792795792222023
iteration 109, loss = 0.0007131719030439854
iteration 110, loss = 0.0008934710640460253
iteration 111, loss = 0.0012028428027406335
iteration 112, loss = 0.001186520792543888
iteration 113, loss = 0.0010220168624073267
iteration 114, loss = 0.0008867984288372099
iteration 115, loss = 0.000720849318895489
iteration 116, loss = 0.0010198960080742836
iteration 117, loss = 0.0008553651859983802
iteration 118, loss = 0.001584188430570066
iteration 119, loss = 0.0008473247289657593
iteration 120, loss = 0.0008475319482386112
iteration 121, loss = 0.000994555070064962
iteration 122, loss = 0.0010039749322459102
iteration 123, loss = 0.0009095437126234174
iteration 124, loss = 0.0007362491451203823
iteration 125, loss = 0.0009648004779592156
iteration 126, loss = 0.0010938261402770877
iteration 127, loss = 0.0007632482447661459
iteration 128, loss = 0.0009403504664078355
iteration 129, loss = 0.001071823644451797
iteration 130, loss = 0.00094296206953004
iteration 131, loss = 0.0015178389148786664
iteration 132, loss = 0.0007866219384595752
iteration 133, loss = 0.0011188945500180125
iteration 134, loss = 0.0009306831634603441
iteration 135, loss = 0.000690567132551223
iteration 136, loss = 0.0008348617702722549
iteration 137, loss = 0.001138294697739184
iteration 138, loss = 0.0014300676994025707
iteration 139, loss = 0.0006856491090729833
iteration 140, loss = 0.0013631354086101055
iteration 141, loss = 0.0007177283405326307
iteration 142, loss = 0.000787462864536792
iteration 143, loss = 0.0007421033224090934
iteration 144, loss = 0.0008112790528684855
iteration 145, loss = 0.0007932785083539784
iteration 146, loss = 0.0009050206281244755
iteration 147, loss = 0.0007961883675307035
iteration 148, loss = 0.0009302509715780616
iteration 149, loss = 0.000988511135801673
iteration 150, loss = 0.0010032029822468758
iteration 151, loss = 0.0007442740607075393
iteration 152, loss = 0.0007311758236028254
iteration 153, loss = 0.0009274146286770701
iteration 154, loss = 0.0008838212233968079
iteration 155, loss = 0.0008465528953820467
iteration 156, loss = 0.0011216498678550124
iteration 157, loss = 0.0011709870304912329
iteration 158, loss = 0.0015132534317672253
iteration 159, loss = 0.000975709524936974
iteration 160, loss = 0.000879658677149564
iteration 161, loss = 0.0008712062845006585
iteration 162, loss = 0.0007323133177123964
iteration 163, loss = 0.0008233581320382655
iteration 164, loss = 0.0013935955939814448
iteration 165, loss = 0.0008556657703593373
iteration 166, loss = 0.0007904318626970053
iteration 167, loss = 0.0009309939341619611
iteration 168, loss = 0.0010521159274503589
iteration 169, loss = 0.0010092667071148753
iteration 170, loss = 0.0010530250146985054
iteration 171, loss = 0.00081150874029845
iteration 172, loss = 0.0014408796560019255
iteration 173, loss = 0.001891234889626503
iteration 174, loss = 0.0008664480410516262
iteration 175, loss = 0.0013604057021439075
iteration 176, loss = 0.0008905885624699295
iteration 177, loss = 0.0008324272930622101
iteration 178, loss = 0.0008544049924239516
iteration 179, loss = 0.0008841365342959762
iteration 180, loss = 0.0007301799487322569
iteration 181, loss = 0.00084943842375651
iteration 182, loss = 0.0007639215909875929
iteration 183, loss = 0.0007491096621379256
iteration 184, loss = 0.000983050325885415
iteration 185, loss = 0.0008337731705978513
iteration 186, loss = 0.00079106236808002
iteration 187, loss = 0.0010929201962426305
iteration 188, loss = 0.0007497851620428264
iteration 189, loss = 0.0009251845185644925
iteration 190, loss = 0.0015756208449602127
iteration 191, loss = 0.0008842301904223859
iteration 192, loss = 0.001152925775386393
iteration 193, loss = 0.0007684275042265654
iteration 194, loss = 0.0012609658297151327
iteration 195, loss = 0.0007580891833640635
iteration 196, loss = 0.0011193015379831195
iteration 197, loss = 0.000819329870864749
iteration 198, loss = 0.0009720809175632894
iteration 199, loss = 0.0009230189607478678
iteration 200, loss = 0.0007183062843978405
iteration 201, loss = 0.0008059477549977601
iteration 202, loss = 0.0017518759705126286
iteration 203, loss = 0.001090086530894041
iteration 204, loss = 0.001019578194245696
iteration 205, loss = 0.0007290090434253216
iteration 206, loss = 0.0010061361826956272
iteration 207, loss = 0.0009879831923171878
iteration 208, loss = 0.0007513974560424685
iteration 209, loss = 0.0007134629413485527
iteration 210, loss = 0.0009938469156622887
iteration 211, loss = 0.0006050168303772807
iteration 212, loss = 0.0008950113551691175
iteration 213, loss = 0.0008094410295598209
iteration 214, loss = 0.001273551257327199
iteration 215, loss = 0.0009563835337758064
iteration 216, loss = 0.0008839484071359038
iteration 217, loss = 0.0009712347527965903
iteration 218, loss = 0.0013461329508572817
iteration 219, loss = 0.0009093980770558119
iteration 220, loss = 0.0016829409869387746
iteration 221, loss = 0.0009504960617050529
iteration 222, loss = 0.0008085276931524277
iteration 223, loss = 0.0012970768148079515
iteration 224, loss = 0.0008293925784528255
iteration 225, loss = 0.0015378708485513926
iteration 226, loss = 0.0007843270432204008
iteration 227, loss = 0.0014656006824225187
iteration 228, loss = 0.0009343443671241403
iteration 229, loss = 0.0007751039229333401
iteration 230, loss = 0.0014440506929531693
iteration 231, loss = 0.0006659490754827857
iteration 232, loss = 0.0009158909088000655
iteration 233, loss = 0.0007748145144432783
iteration 234, loss = 0.0010390335228294134
iteration 235, loss = 0.0006945198401808739
iteration 236, loss = 0.0012027513002976775
iteration 237, loss = 0.0009183001238852739
iteration 238, loss = 0.000769919075537473
iteration 239, loss = 0.0010845845099538565
iteration 240, loss = 0.0007857261225581169
iteration 241, loss = 0.0009683249518275261
iteration 242, loss = 0.0010884496150538325
iteration 243, loss = 0.0014737442834302783
iteration 244, loss = 0.0016025879886001348
iteration 245, loss = 0.0009441483416594565
iteration 246, loss = 0.0010649124160408974
iteration 247, loss = 0.0007323192548938096
iteration 248, loss = 0.0008834132459014654
iteration 249, loss = 0.000706179067492485
iteration 250, loss = 0.0008206360507756472
iteration 251, loss = 0.0007617790251970291
iteration 252, loss = 0.0008689253591001034
iteration 253, loss = 0.001196142751723528
iteration 254, loss = 0.000691469176672399
iteration 255, loss = 0.001246267114765942
iteration 256, loss = 0.0008114345837384462
iteration 257, loss = 0.0008704822394065559
iteration 258, loss = 0.0009067547507584095
iteration 259, loss = 0.0010130496229976416
iteration 260, loss = 0.0007648746250197291
iteration 261, loss = 0.0007758199353702366
iteration 262, loss = 0.0007516012992709875
iteration 263, loss = 0.001138055115006864
iteration 264, loss = 0.0007950954604893923
iteration 265, loss = 0.0011882421094924212
iteration 266, loss = 0.001055426662787795
iteration 267, loss = 0.00099543621763587
iteration 268, loss = 0.001020905445329845
iteration 269, loss = 0.0008366011315956712
iteration 270, loss = 0.0006824748707003891
iteration 271, loss = 0.0007222694694064558
iteration 272, loss = 0.001709207659587264
iteration 273, loss = 0.0006807026802562177
iteration 274, loss = 0.0007641206029802561
iteration 275, loss = 0.0007762295426800847
iteration 276, loss = 0.000993330730125308
iteration 277, loss = 0.001097209402360022
iteration 278, loss = 0.0009036891278810799
iteration 279, loss = 0.0009259297512471676
iteration 280, loss = 0.0012021398870274425
iteration 281, loss = 0.0008441813988611102
iteration 282, loss = 0.001100364956073463
iteration 283, loss = 0.0007880485500209033
iteration 284, loss = 0.0007920329226180911
iteration 285, loss = 0.0007474764133803546
iteration 286, loss = 0.0008059293031692505
iteration 287, loss = 0.002050260314717889
iteration 288, loss = 0.0008924828143790364
iteration 289, loss = 0.0009241427760571241
iteration 290, loss = 0.0008724404033273458
iteration 291, loss = 0.0008937005768530071
iteration 292, loss = 0.0015614018775522709
iteration 293, loss = 0.0013085325481370091
iteration 294, loss = 0.0007804673514328897
iteration 295, loss = 0.0009383528959006071
iteration 296, loss = 0.0007073623710311949
iteration 297, loss = 0.0008412761380895972
iteration 298, loss = 0.0007793163531459868
iteration 299, loss = 0.0009656723123043776
iteration 300, loss = 0.0007893009460531175
iteration 1, loss = 0.0016272874781861901
iteration 2, loss = 0.0014747497625648975
iteration 3, loss = 0.0010042759822681546
iteration 4, loss = 0.00074822420720011
iteration 5, loss = 0.0009231652948074043
iteration 6, loss = 0.0008552033687010407
iteration 7, loss = 0.0016236414667218924
iteration 8, loss = 0.0008492438355460763
iteration 9, loss = 0.0009790631011128426
iteration 10, loss = 0.0013150465674698353
iteration 11, loss = 0.0010272840736433864
iteration 12, loss = 0.0007372158579528332
iteration 13, loss = 0.0011544950539246202
iteration 14, loss = 0.0011062639532610774
iteration 15, loss = 0.0006859002751298249
iteration 16, loss = 0.0008251719991676509
iteration 17, loss = 0.0008554910309612751
iteration 18, loss = 0.0006441164878197014
iteration 19, loss = 0.0010600289097055793
iteration 20, loss = 0.0008109399932436645
iteration 21, loss = 0.0010651207994669676
iteration 22, loss = 0.0007282998994924128
iteration 23, loss = 0.0008276871521957219
iteration 24, loss = 0.0007833957206457853
iteration 25, loss = 0.000869507493916899
iteration 26, loss = 0.0011240994790568948
iteration 27, loss = 0.001712186960503459
iteration 28, loss = 0.0016033127903938293
iteration 29, loss = 0.0007241768180392683
iteration 30, loss = 0.0008551899809390306
iteration 31, loss = 0.00110950181260705
iteration 32, loss = 0.0008504074066877365
iteration 33, loss = 0.0007516583427786827
iteration 34, loss = 0.00083448167424649
iteration 35, loss = 0.0008516984526067972
iteration 36, loss = 0.0017704558558762074
iteration 37, loss = 0.0008074886864051223
iteration 38, loss = 0.0017038482474163175
iteration 39, loss = 0.000809709366876632
iteration 40, loss = 0.0015679391799494624
iteration 41, loss = 0.0010400076862424612
iteration 42, loss = 0.0011280705220997334
iteration 43, loss = 0.0012601250782608986
iteration 44, loss = 0.0009006720501929522
iteration 45, loss = 0.0009170970879495144
iteration 46, loss = 0.0007353138062171638
iteration 47, loss = 0.0006508118822239339
iteration 48, loss = 0.001242546597495675
iteration 49, loss = 0.0007759627769701183
iteration 50, loss = 0.0009415465756319463
iteration 51, loss = 0.0007936311885714531
iteration 52, loss = 0.0007998045184649527
iteration 53, loss = 0.0008175076218321919
iteration 54, loss = 0.0013626351719722152
iteration 55, loss = 0.0009427323821000755
iteration 56, loss = 0.0009916207054629922
iteration 57, loss = 0.0007090156432241201
iteration 58, loss = 0.001322170952335
iteration 59, loss = 0.000636727490928024
iteration 60, loss = 0.0016389251686632633
iteration 61, loss = 0.0009802549611777067
iteration 62, loss = 0.0010022777132689953
iteration 63, loss = 0.0008461545221507549
iteration 64, loss = 0.001064785523340106
iteration 65, loss = 0.0008717158925719559
iteration 66, loss = 0.0007547270506620407
iteration 67, loss = 0.0012220243224874139
iteration 68, loss = 0.0007758599822409451
iteration 69, loss = 0.0009033152018673718
iteration 70, loss = 0.0011311413254588842
iteration 71, loss = 0.0009009455097839236
iteration 72, loss = 0.0008313016151078045
iteration 73, loss = 0.0008422834798693657
iteration 74, loss = 0.0011017807992175221
iteration 75, loss = 0.0009636497125029564
iteration 76, loss = 0.0007500543724745512
iteration 77, loss = 0.000959036813583225
iteration 78, loss = 0.0007703165174461901
iteration 79, loss = 0.00085538747953251
iteration 80, loss = 0.000957538082730025
iteration 81, loss = 0.0010398634476587176
iteration 82, loss = 0.0008266625227406621
iteration 83, loss = 0.001197625882923603
iteration 84, loss = 0.0013289796188473701
iteration 85, loss = 0.0006737514049746096
iteration 86, loss = 0.0008264239295385778
iteration 87, loss = 0.0008776566246524453
iteration 88, loss = 0.0016511959256604314
iteration 89, loss = 0.0008037310326471925
iteration 90, loss = 0.0009551865514367819
iteration 91, loss = 0.0007454941514879465
iteration 92, loss = 0.000814319122582674
iteration 93, loss = 0.0008542498107999563
iteration 94, loss = 0.0008170199580490589
iteration 95, loss = 0.0008351585711352527
iteration 96, loss = 0.0009254043688997626
iteration 97, loss = 0.000861693057231605
iteration 98, loss = 0.0010902788490056992
iteration 99, loss = 0.0014013939071446657
iteration 100, loss = 0.0015981384785845876
iteration 101, loss = 0.0008759885677136481
iteration 102, loss = 0.0010890911798924208
iteration 103, loss = 0.0009046464692801237
iteration 104, loss = 0.0011424603872001171
iteration 105, loss = 0.0007835957803763449
iteration 106, loss = 0.0008879560045897961
iteration 107, loss = 0.0007034259615465999
iteration 108, loss = 0.0008562238654121757
iteration 109, loss = 0.0008843807736411691
iteration 110, loss = 0.0007554220501333475
iteration 111, loss = 0.0006531156250275671
iteration 112, loss = 0.0010709797497838736
iteration 113, loss = 0.0010954892495647073
iteration 114, loss = 0.0014877544017508626
iteration 115, loss = 0.0008433650946244597
iteration 116, loss = 0.0011009509908035398
iteration 117, loss = 0.0006945566856302321
iteration 118, loss = 0.0015622902428731322
iteration 119, loss = 0.0008850257145240903
iteration 120, loss = 0.0007098166970536113
iteration 121, loss = 0.000761792529374361
iteration 122, loss = 0.001045383047312498
iteration 123, loss = 0.0012863260926678777
iteration 124, loss = 0.0015248153358697891
iteration 125, loss = 0.0018555314745754004
iteration 126, loss = 0.0009343752171844244
iteration 127, loss = 0.0006835730746388435
iteration 128, loss = 0.0008030753233470023
iteration 129, loss = 0.0011103509459644556
iteration 130, loss = 0.0007785508059896529
iteration 131, loss = 0.0008541738498024642
iteration 132, loss = 0.0007476508617401123
iteration 133, loss = 0.0006721477257087827
iteration 134, loss = 0.0008343181107193232
iteration 135, loss = 0.0007277997210621834
iteration 136, loss = 0.0008828559657558799
iteration 137, loss = 0.0008858465007506311
iteration 138, loss = 0.0009370492771267891
iteration 139, loss = 0.000710632186383009
iteration 140, loss = 0.0008750187116675079
iteration 141, loss = 0.0008439891971647739
iteration 142, loss = 0.0009696520282886922
iteration 143, loss = 0.0010135202901437879
iteration 144, loss = 0.0008761308272369206
iteration 145, loss = 0.0010370833333581686
iteration 146, loss = 0.0009199106134474277
iteration 147, loss = 0.0009952352847903967
iteration 148, loss = 0.000783556024543941
iteration 149, loss = 0.0012303577968850732
iteration 150, loss = 0.0008570174686610699
iteration 151, loss = 0.0009260330116376281
iteration 152, loss = 0.0007137872162275016
iteration 153, loss = 0.0008217733120545745
iteration 154, loss = 0.001042505376972258
iteration 155, loss = 0.000801712041720748
iteration 156, loss = 0.0008519414695911109
iteration 157, loss = 0.001110685057938099
iteration 158, loss = 0.0007602671976201236
iteration 159, loss = 0.0010792549001052976
iteration 160, loss = 0.0005312934517860413
iteration 161, loss = 0.0009928893996402621
iteration 162, loss = 0.0009728834847919643
iteration 163, loss = 0.0008660550229251385
iteration 164, loss = 0.0008248237427324057
iteration 165, loss = 0.0009641385404393077
iteration 166, loss = 0.0009710803860798478
iteration 167, loss = 0.0017735407454892993
iteration 168, loss = 0.0014293777057901025
iteration 169, loss = 0.0013219434767961502
iteration 170, loss = 0.0008989717462100089
iteration 171, loss = 0.0007143866969272494
iteration 172, loss = 0.0011134176747873425
iteration 173, loss = 0.0014682041946798563
iteration 174, loss = 0.0008394162869080901
iteration 175, loss = 0.0008379947394132614
iteration 176, loss = 0.0007302458980120718
iteration 177, loss = 0.0008073486387729645
iteration 178, loss = 0.0008516969392076135
iteration 179, loss = 0.0010390951065346599
iteration 180, loss = 0.0009450737852603197
iteration 181, loss = 0.001082960981875658
iteration 182, loss = 0.0008379351929761469
iteration 183, loss = 0.0016006508376449347
iteration 184, loss = 0.0007778187282383442
iteration 185, loss = 0.0007130610756576061
iteration 186, loss = 0.0006903430912643671
iteration 187, loss = 0.0010267454199492931
iteration 188, loss = 0.0010380425956100225
iteration 189, loss = 0.0008849911391735077
iteration 190, loss = 0.0006903805187903345
iteration 191, loss = 0.0006354199722409248
iteration 192, loss = 0.0006695316988043487
iteration 193, loss = 0.0008932336932048202
iteration 194, loss = 0.0010599388042464852
iteration 195, loss = 0.0011316020973026752
iteration 196, loss = 0.0007982763345353305
iteration 197, loss = 0.0008685633656568825
iteration 198, loss = 0.0009324133861809969
iteration 199, loss = 0.0010301085421815515
iteration 200, loss = 0.0008114446536637843
iteration 201, loss = 0.0007278940174728632
iteration 202, loss = 0.001212111208587885
iteration 203, loss = 0.0009292534086853266
iteration 204, loss = 0.0014700262108817697
iteration 205, loss = 0.0008460574317723513
iteration 206, loss = 0.0011834295000880957
iteration 207, loss = 0.0008799371425993741
iteration 208, loss = 0.0008192407549358904
iteration 209, loss = 0.0009347117738798261
iteration 210, loss = 0.0006859002751298249
iteration 211, loss = 0.001239144941791892
iteration 212, loss = 0.0008069637115113437
iteration 213, loss = 0.000819197972305119
iteration 214, loss = 0.0009585374500602484
iteration 215, loss = 0.0009289385052397847
iteration 216, loss = 0.001254168339073658
iteration 217, loss = 0.0009633152512833476
iteration 218, loss = 0.0007331713568419218
iteration 219, loss = 0.0008169464999809861
iteration 220, loss = 0.0007141624810174108
iteration 221, loss = 0.0007539858343079686
iteration 222, loss = 0.0011275835568085313
iteration 223, loss = 0.0008042051340453327
iteration 224, loss = 0.0011042163241654634
iteration 225, loss = 0.0006558078457601368
iteration 226, loss = 0.0008381270454265177
iteration 227, loss = 0.0008413847535848618
iteration 228, loss = 0.0012265819823369384
iteration 229, loss = 0.0014239383162930608
iteration 230, loss = 0.0009092064574360847
iteration 231, loss = 0.0010254594963043928
iteration 232, loss = 0.0009693586034700274
iteration 233, loss = 0.0007747558411210775
iteration 234, loss = 0.0007583996630273759
iteration 235, loss = 0.0008170997025445104
iteration 236, loss = 0.0009270049631595612
iteration 237, loss = 0.0012004049494862556
iteration 238, loss = 0.0008706795051693916
iteration 239, loss = 0.0008067726739682257
iteration 240, loss = 0.0009847626788541675
iteration 241, loss = 0.0008464702405035496
iteration 242, loss = 0.0018866003956645727
iteration 243, loss = 0.0012018322013318539
iteration 244, loss = 0.001104026916436851
iteration 245, loss = 0.0009030487271957099
iteration 246, loss = 0.000798394437879324
iteration 247, loss = 0.0009588701068423688
iteration 248, loss = 0.0007628964958712459
iteration 249, loss = 0.0010603475384414196
iteration 250, loss = 0.0008315831073559821
iteration 251, loss = 0.0007883930811658502
iteration 252, loss = 0.000663538055960089
iteration 253, loss = 0.001794445444829762
iteration 254, loss = 0.0007505909306928515
iteration 255, loss = 0.0007262110011652112
iteration 256, loss = 0.0008870786987245083
iteration 257, loss = 0.0009473332320339978
iteration 258, loss = 0.0010079731000587344
iteration 259, loss = 0.0006970434333197773
iteration 260, loss = 0.0007737698033452034
iteration 261, loss = 0.0008355141035281122
iteration 262, loss = 0.0010099781211465597
iteration 263, loss = 0.0015630022389814258
iteration 264, loss = 0.0008807347621768713
iteration 265, loss = 0.0007314725662581623
iteration 266, loss = 0.0009562837076373398
iteration 267, loss = 0.0010339433792978525
iteration 268, loss = 0.0008140194695442915
iteration 269, loss = 0.0013366249622777104
iteration 270, loss = 0.000829888042062521
iteration 271, loss = 0.001058328663930297
iteration 272, loss = 0.0014324740041047335
iteration 273, loss = 0.0011525368317961693
iteration 274, loss = 0.0015815994702279568
iteration 275, loss = 0.001046597957611084
iteration 276, loss = 0.0007520028739236295
iteration 277, loss = 0.0007550889276899397
iteration 278, loss = 0.0008473574416711926
iteration 279, loss = 0.0007639465038664639
iteration 280, loss = 0.001134973717853427
iteration 281, loss = 0.0009570189286023378
iteration 282, loss = 0.0007577354554086924
iteration 283, loss = 0.0015659520868211985
iteration 284, loss = 0.0006356457015499473
iteration 285, loss = 0.0007895026938058436
iteration 286, loss = 0.001395488390699029
iteration 287, loss = 0.0010756619740277529
iteration 288, loss = 0.0006897230632603168
iteration 289, loss = 0.0016862750053405762
iteration 290, loss = 0.0009725149720907211
iteration 291, loss = 0.0008231765823438764
iteration 292, loss = 0.0007359315641224384
iteration 293, loss = 0.0007418764289468527
iteration 294, loss = 0.0009933877736330032
iteration 295, loss = 0.0008473910856992006
iteration 296, loss = 0.0008228450315073133
iteration 297, loss = 0.0012162410421296954
iteration 298, loss = 0.001592787797562778
iteration 299, loss = 0.001143155386671424
iteration 300, loss = 0.0009014104725793004
iteration 1, loss = 0.0008803242817521095
iteration 2, loss = 0.0008848630823194981
iteration 3, loss = 0.0009068171493709087
iteration 4, loss = 0.0008828105637803674
iteration 5, loss = 0.0008573642116971314
iteration 6, loss = 0.0007734035607427359
iteration 7, loss = 0.0009928522631525993
iteration 8, loss = 0.0008297121385112405
iteration 9, loss = 0.0009599763434380293
iteration 10, loss = 0.0018616631859913468
iteration 11, loss = 0.0017203220631927252
iteration 12, loss = 0.0010513428132981062
iteration 13, loss = 0.001493816263973713
iteration 14, loss = 0.0012026806361973286
iteration 15, loss = 0.0007549171568825841
iteration 16, loss = 0.0009370435145683587
iteration 17, loss = 0.0008873072220012546
iteration 18, loss = 0.0007595552015118301
iteration 19, loss = 0.0007916131871752441
iteration 20, loss = 0.0008444082923233509
iteration 21, loss = 0.0011337953619658947
iteration 22, loss = 0.000671871704980731
iteration 23, loss = 0.000977690564468503
iteration 24, loss = 0.0008421866223216057
iteration 25, loss = 0.0015136508736759424
iteration 26, loss = 0.0009385544108226895
iteration 27, loss = 0.0009775463258847594
iteration 28, loss = 0.0011076421942561865
iteration 29, loss = 0.0007565575651824474
iteration 30, loss = 0.0008165567414835095
iteration 31, loss = 0.0009305613348260522
iteration 32, loss = 0.0007882546633481979
iteration 33, loss = 0.0009734711493365467
iteration 34, loss = 0.0007633080240339041
iteration 35, loss = 0.0009704121621325612
iteration 36, loss = 0.0008437858778052032
iteration 37, loss = 0.0010373775148764253
iteration 38, loss = 0.0007225137087516487
iteration 39, loss = 0.0017058269586414099
iteration 40, loss = 0.0008088221657089889
iteration 41, loss = 0.000887212052475661
iteration 42, loss = 0.0008595822146162391
iteration 43, loss = 0.0009192147408612072
iteration 44, loss = 0.0006963306223042309
iteration 45, loss = 0.0007290214416570961
iteration 46, loss = 0.0006761542172171175
iteration 47, loss = 0.0007405743817798793
iteration 48, loss = 0.0010153960902243853
iteration 49, loss = 0.0006237160414457321
iteration 50, loss = 0.00153355801012367
iteration 51, loss = 0.0007522351224906743
iteration 52, loss = 0.0008863742114044726
iteration 53, loss = 0.00115617283154279
iteration 54, loss = 0.0011615557596087456
iteration 55, loss = 0.00119687314145267
iteration 56, loss = 0.0007421578047797084
iteration 57, loss = 0.001066788099706173
iteration 58, loss = 0.0007876870222389698
iteration 59, loss = 0.001524264574982226
iteration 60, loss = 0.0009292056201957166
iteration 61, loss = 0.0007817636942490935
iteration 62, loss = 0.0008277629967778921
iteration 63, loss = 0.001280721859075129
iteration 64, loss = 0.0008665113127790391
iteration 65, loss = 0.0006723612314090133
iteration 66, loss = 0.0007525777909904718
iteration 67, loss = 0.0008237655274569988
iteration 68, loss = 0.0010838694870471954
iteration 69, loss = 0.000899382634088397
iteration 70, loss = 0.0014330262783914804
iteration 71, loss = 0.0007513295859098434
iteration 72, loss = 0.001171025214716792
iteration 73, loss = 0.0008058627718128264
iteration 74, loss = 0.0007960859220474958
iteration 75, loss = 0.0013359531294554472
iteration 76, loss = 0.0008385917171835899
iteration 77, loss = 0.001209193142130971
iteration 78, loss = 0.0009655153262428939
iteration 79, loss = 0.0009128820965997875
iteration 80, loss = 0.0007329077343456447
iteration 81, loss = 0.0007366054924204946
iteration 82, loss = 0.0009674088796600699
iteration 83, loss = 0.00079328817082569
iteration 84, loss = 0.0007430169498547912
iteration 85, loss = 0.0008043933776207268
iteration 86, loss = 0.000715788803063333
iteration 87, loss = 0.0008109050104394555
iteration 88, loss = 0.0016229469329118729
iteration 89, loss = 0.0008599947905167937
iteration 90, loss = 0.0008749975822865963
iteration 91, loss = 0.0012326533906161785
iteration 92, loss = 0.0022387378849089146
iteration 93, loss = 0.0008651558309793472
iteration 94, loss = 0.0008047886658459902
iteration 95, loss = 0.0008754802402108908
iteration 96, loss = 0.0012850414495915174
iteration 97, loss = 0.000855510588735342
iteration 98, loss = 0.0008448709850199521
iteration 99, loss = 0.0007474605808965862
iteration 100, loss = 0.0017141180578619242
iteration 101, loss = 0.0012267271522432566
iteration 102, loss = 0.0015711858868598938
iteration 103, loss = 0.0014527305029332638
iteration 104, loss = 0.0010484596714377403
iteration 105, loss = 0.0007050909334793687
iteration 106, loss = 0.0009540401515550911
iteration 107, loss = 0.0012092043180018663
iteration 108, loss = 0.0009415945969521999
iteration 109, loss = 0.0008535399683751166
iteration 110, loss = 0.001604486140422523
iteration 111, loss = 0.0010237784590572119
iteration 112, loss = 0.0008497733506374061
iteration 113, loss = 0.0010442951461300254
iteration 114, loss = 0.0008314329315908253
iteration 115, loss = 0.0016993498429656029
iteration 116, loss = 0.0007891992572695017
iteration 117, loss = 0.000990118831396103
iteration 118, loss = 0.000862186832819134
iteration 119, loss = 0.0008289898396469653
iteration 120, loss = 0.0009591067209839821
iteration 121, loss = 0.001012279186397791
iteration 122, loss = 0.001174948993138969
iteration 123, loss = 0.0008199543226510286
iteration 124, loss = 0.0006569313700310886
iteration 125, loss = 0.0009679327486082911
iteration 126, loss = 0.001100232359021902
iteration 127, loss = 0.0008330243872478604
iteration 128, loss = 0.000960413075517863
iteration 129, loss = 0.0008632767130620778
iteration 130, loss = 0.0009457466076128185
iteration 131, loss = 0.000754214939661324
iteration 132, loss = 0.0008951612981036305
iteration 133, loss = 0.0007841442711651325
iteration 134, loss = 0.0008136099204421043
iteration 135, loss = 0.0007424670620821416
iteration 136, loss = 0.0007871099514886737
iteration 137, loss = 0.0006133242277428508
iteration 138, loss = 0.0008185933111235499
iteration 139, loss = 0.0009985272772610188
iteration 140, loss = 0.0008222797187045217
iteration 141, loss = 0.0013783526374027133
iteration 142, loss = 0.0008438823861069977
iteration 143, loss = 0.0008137165568768978
iteration 144, loss = 0.0008907742449082434
iteration 145, loss = 0.0008175731054507196
iteration 146, loss = 0.00107245531398803
iteration 147, loss = 0.00130714219994843
iteration 148, loss = 0.0009510786039754748
iteration 149, loss = 0.0007560845697298646
iteration 150, loss = 0.0008072118507698178
iteration 151, loss = 0.0010166435968130827
iteration 152, loss = 0.0009890490910038352
iteration 153, loss = 0.0008307007374241948
iteration 154, loss = 0.0011088850442320108
iteration 155, loss = 0.0007212045602500439
iteration 156, loss = 0.000875385943800211
iteration 157, loss = 0.000863972119987011
iteration 158, loss = 0.0008919027168303728
iteration 159, loss = 0.0008169497014023364
iteration 160, loss = 0.0006645101821050048
iteration 161, loss = 0.0007536981720477343
iteration 162, loss = 0.0009143973002210259
iteration 163, loss = 0.0008215811103582382
iteration 164, loss = 0.0015293721808120608
iteration 165, loss = 0.000789112236816436
iteration 166, loss = 0.0010819631861522794
iteration 167, loss = 0.0009495512349531054
iteration 168, loss = 0.0009665943798609078
iteration 169, loss = 0.0009973377455025911
iteration 170, loss = 0.0014710422838106751
iteration 171, loss = 0.000888578244484961
iteration 172, loss = 0.0008732604910619557
iteration 173, loss = 0.0015184548683464527
iteration 174, loss = 0.0008461591205559671
iteration 175, loss = 0.0008644778281450272
iteration 176, loss = 0.0008531943894922733
iteration 177, loss = 0.0011434743646532297
iteration 178, loss = 0.0007616293733008206
iteration 179, loss = 0.0011331260902807117
iteration 180, loss = 0.0008030887693166733
iteration 181, loss = 0.0013662747805938125
iteration 182, loss = 0.001505975378677249
iteration 183, loss = 0.0007165087736211717
iteration 184, loss = 0.0007958746864460409
iteration 185, loss = 0.0024882282596081495
iteration 186, loss = 0.0016392441466450691
iteration 187, loss = 0.0008968738839030266
iteration 188, loss = 0.0010886379750445485
iteration 189, loss = 0.0010042344219982624
iteration 190, loss = 0.000789875746704638
iteration 191, loss = 0.0008175945840775967
iteration 192, loss = 0.001036984845995903
iteration 193, loss = 0.0008146245963871479
iteration 194, loss = 0.0007225216832011938
iteration 195, loss = 0.0008912815828807652
iteration 196, loss = 0.0010320290457457304
iteration 197, loss = 0.0010897155152633786
iteration 198, loss = 0.0009672939777374268
iteration 199, loss = 0.0015053022652864456
iteration 200, loss = 0.001138627645559609
iteration 201, loss = 0.0007586238789372146
iteration 202, loss = 0.0006989471148699522
iteration 203, loss = 0.0008031599572859704
iteration 204, loss = 0.0009335459908470511
iteration 205, loss = 0.0016768844798207283
iteration 206, loss = 0.0008367483387701213
iteration 207, loss = 0.0008096132078208029
iteration 208, loss = 0.0010338528081774712
iteration 209, loss = 0.001056264154613018
iteration 210, loss = 0.0010462950449436903
iteration 211, loss = 0.0007529408321715891
iteration 212, loss = 0.0008242576150223613
iteration 213, loss = 0.0010336363920941949
iteration 214, loss = 0.0012669542338699102
iteration 215, loss = 0.0007153120241127908
iteration 216, loss = 0.001292001805268228
iteration 217, loss = 0.0008598605636507273
iteration 218, loss = 0.001093300525099039
iteration 219, loss = 0.0011704667704179883
iteration 220, loss = 0.001298790331929922
iteration 221, loss = 0.0007040379568934441
iteration 222, loss = 0.0008775392780080438
iteration 223, loss = 0.0008810689905658364
iteration 224, loss = 0.0010881231864914298
iteration 225, loss = 0.0007067583501338959
iteration 226, loss = 0.001149783842265606
iteration 227, loss = 0.0008610359509475529
iteration 228, loss = 0.0009600909543223679
iteration 229, loss = 0.0008129124180413783
iteration 230, loss = 0.001675825216807425
iteration 231, loss = 0.0014545888407155871
iteration 232, loss = 0.0010992555180564523
iteration 233, loss = 0.000806304975412786
iteration 234, loss = 0.0014269226230680943
iteration 235, loss = 0.0009337440133094788
iteration 236, loss = 0.0018261417280882597
iteration 237, loss = 0.0007046134560368955
iteration 238, loss = 0.0006821533315815032
iteration 239, loss = 0.0008783579105511308
iteration 240, loss = 0.0008210345404222608
iteration 241, loss = 0.0011200753506273031
iteration 242, loss = 0.0010740928119048476
iteration 243, loss = 0.0008507465245202184
iteration 244, loss = 0.00090548035223037
iteration 245, loss = 0.0008633200777694583
iteration 246, loss = 0.0009755755309015512
iteration 247, loss = 0.001147878123447299
iteration 248, loss = 0.0008018869557417929
iteration 249, loss = 0.0007751066004857421
iteration 250, loss = 0.0014170793583616614
iteration 251, loss = 0.0008073968347162008
iteration 252, loss = 0.0010014709550887346
iteration 253, loss = 0.0008130957721732557
iteration 254, loss = 0.0008695553406141698
iteration 255, loss = 0.0010006874799728394
iteration 256, loss = 0.001265838393010199
iteration 257, loss = 0.0008183238096535206
iteration 258, loss = 0.0010880004847422242
iteration 259, loss = 0.0006681704544462264
iteration 260, loss = 0.0008920830441638827
iteration 261, loss = 0.0010802746983245015
iteration 262, loss = 0.0007247134344652295
iteration 263, loss = 0.0007466438692063093
iteration 264, loss = 0.0010709499474614859
iteration 265, loss = 0.0011225842172279954
iteration 266, loss = 0.0009544127387925982
iteration 267, loss = 0.0007124809199012816
iteration 268, loss = 0.0007536488701589406
iteration 269, loss = 0.0008128142217174172
iteration 270, loss = 0.0007932398002594709
iteration 271, loss = 0.0007382905459962785
iteration 272, loss = 0.0007498188642784953
iteration 273, loss = 0.0011155426036566496
iteration 274, loss = 0.0007810209062881768
iteration 275, loss = 0.0008195309201255441
iteration 276, loss = 0.0007753633544780314
iteration 277, loss = 0.0010083082597702742
iteration 278, loss = 0.0008663716143928468
iteration 279, loss = 0.001033238833770156
iteration 280, loss = 0.0008240243187174201
iteration 281, loss = 0.0005857772193849087
iteration 282, loss = 0.0007483500521630049
iteration 283, loss = 0.0008494676440022886
iteration 284, loss = 0.0007874212460592389
iteration 285, loss = 0.000979400472715497
iteration 286, loss = 0.0010367364156991243
iteration 287, loss = 0.0015060815494507551
iteration 288, loss = 0.0007685390301048756
iteration 289, loss = 0.0008395240874961019
iteration 290, loss = 0.0009273856412619352
iteration 291, loss = 0.0008788916748017073
iteration 292, loss = 0.0012247636914253235
iteration 293, loss = 0.0011999417329207063
iteration 294, loss = 0.0007449011318385601
iteration 295, loss = 0.0010075648315250874
iteration 296, loss = 0.0007276446558535099
iteration 297, loss = 0.0007459213375113904
iteration 298, loss = 0.0009894761024042964
iteration 299, loss = 0.000820881687104702
iteration 300, loss = 0.0007379042217507958
iteration 1, loss = 0.0009506451897323132
iteration 2, loss = 0.0008999000419862568
iteration 3, loss = 0.00088871008483693
iteration 4, loss = 0.0008372941520065069
iteration 5, loss = 0.0008954568766057491
iteration 6, loss = 0.0009586451342329383
iteration 7, loss = 0.0013961769873276353
iteration 8, loss = 0.0009767094161361456
iteration 9, loss = 0.0012638140469789505
iteration 10, loss = 0.0018841527635231614
iteration 11, loss = 0.0008180289878509939
iteration 12, loss = 0.0006911372765898705
iteration 13, loss = 0.0009173014550469816
iteration 14, loss = 0.00071452755946666
iteration 15, loss = 0.0009416284738108516
iteration 16, loss = 0.0008603855967521667
iteration 17, loss = 0.0008031949982978404
iteration 18, loss = 0.000841604545712471
iteration 19, loss = 0.00066427665296942
iteration 20, loss = 0.001072264276444912
iteration 21, loss = 0.0011360053904354572
iteration 22, loss = 0.000969052140135318
iteration 23, loss = 0.000987295643426478
iteration 24, loss = 0.0012432207586243749
iteration 25, loss = 0.001014880952425301
iteration 26, loss = 0.0008022858528420329
iteration 27, loss = 0.0009469756623730063
iteration 28, loss = 0.0008386884583160281
iteration 29, loss = 0.0008673322736285627
iteration 30, loss = 0.0007082981173880398
iteration 31, loss = 0.0011140272254124284
iteration 32, loss = 0.0010474671144038439
iteration 33, loss = 0.00272030639462173
iteration 34, loss = 0.0007975065382197499
iteration 35, loss = 0.0008071598131209612
iteration 36, loss = 0.0009660921641625464
iteration 37, loss = 0.0006993950228206813
iteration 38, loss = 0.0010319676948711276
iteration 39, loss = 0.0007637864910066128
iteration 40, loss = 0.0009615448652766645
iteration 41, loss = 0.0010374403791502118
iteration 42, loss = 0.000876659934874624
iteration 43, loss = 0.0007558659999631345
iteration 44, loss = 0.0020081805996596813
iteration 45, loss = 0.0008489846950396895
iteration 46, loss = 0.0008719988982193172
iteration 47, loss = 0.0007596424547955394
iteration 48, loss = 0.0011967456666752696
iteration 49, loss = 0.0008016021456569433
iteration 50, loss = 0.0014068431919440627
iteration 51, loss = 0.0007261973223648965
iteration 52, loss = 0.0006609692936763167
iteration 53, loss = 0.0007214886718429625
iteration 54, loss = 0.0008162932354025543
iteration 55, loss = 0.0007679639966227114
iteration 56, loss = 0.0008212061948142946
iteration 57, loss = 0.0008277230663225055
iteration 58, loss = 0.0007170055177994072
iteration 59, loss = 0.0008389755384996533
iteration 60, loss = 0.0007132791797630489
iteration 61, loss = 0.001087502925656736
iteration 62, loss = 0.0008250377140939236
iteration 63, loss = 0.0006499116425402462
iteration 64, loss = 0.0009181411587633193
iteration 65, loss = 0.0009012704249471426
iteration 66, loss = 0.0007835578289814293
iteration 67, loss = 0.0008544630836695433
iteration 68, loss = 0.0013625136343762279
iteration 69, loss = 0.0008005669806152582
iteration 70, loss = 0.0015995316207408905
iteration 71, loss = 0.0006677366327494383
iteration 72, loss = 0.0011340364580973983
iteration 73, loss = 0.000721536751370877
iteration 74, loss = 0.0015437279362231493
iteration 75, loss = 0.0008073503850027919
iteration 76, loss = 0.0008079553954303265
iteration 77, loss = 0.0008996721007861197
iteration 78, loss = 0.0009391115163452923
iteration 79, loss = 0.0007601284887641668
iteration 80, loss = 0.0009473257814534009
iteration 81, loss = 0.0012114567216485739
iteration 82, loss = 0.0008014024933800101
iteration 83, loss = 0.0010414518183097243
iteration 84, loss = 0.0006883253809064627
iteration 85, loss = 0.0009690562728792429
iteration 86, loss = 0.0011968687176704407
iteration 87, loss = 0.0008206315687857568
iteration 88, loss = 0.0007191398763097823
iteration 89, loss = 0.0008276539156213403
iteration 90, loss = 0.0009087909129448235
iteration 91, loss = 0.0014025436248630285
iteration 92, loss = 0.0008481742115691304
iteration 93, loss = 0.0008432281319983304
iteration 94, loss = 0.0010386627400293946
iteration 95, loss = 0.0010453442810103297
iteration 96, loss = 0.0009194129961542785
iteration 97, loss = 0.0009053677786141634
iteration 98, loss = 0.0008086027228273451
iteration 99, loss = 0.0007848181412555277
iteration 100, loss = 0.0011996016837656498
iteration 101, loss = 0.000739363837055862
iteration 102, loss = 0.0009868259076029062
iteration 103, loss = 0.0009626339888200164
iteration 104, loss = 0.0020991882774978876
iteration 105, loss = 0.0009930740343406796
iteration 106, loss = 0.00132685003336519
iteration 107, loss = 0.0016180502716451883
iteration 108, loss = 0.0011326216626912355
iteration 109, loss = 0.0012820768170058727
iteration 110, loss = 0.0015706595731899142
iteration 111, loss = 0.0010525808902457356
iteration 112, loss = 0.0007446745876222849
iteration 113, loss = 0.0007402185001410544
iteration 114, loss = 0.0006496028508991003
iteration 115, loss = 0.000814844504930079
iteration 116, loss = 0.0009367167367599905
iteration 117, loss = 0.0007809551316313446
iteration 118, loss = 0.0011539719998836517
iteration 119, loss = 0.0007569231675006449
iteration 120, loss = 0.001476535340771079
iteration 121, loss = 0.0007887278916314244
iteration 122, loss = 0.001330732717178762
iteration 123, loss = 0.0007944622193463147
iteration 124, loss = 0.0007952959276735783
iteration 125, loss = 0.0013395666610449553
iteration 126, loss = 0.0007985366974025965
iteration 127, loss = 0.000705509795807302
iteration 128, loss = 0.000837388914078474
iteration 129, loss = 0.0007621917175129056
iteration 130, loss = 0.0008699281606823206
iteration 131, loss = 0.0007769629592075944
iteration 132, loss = 0.0008194888941943645
iteration 133, loss = 0.0007967238780111074
iteration 134, loss = 0.000795579981058836
iteration 135, loss = 0.001596806338056922
iteration 136, loss = 0.0014424662804231048
iteration 137, loss = 0.0008142810547724366
iteration 138, loss = 0.0011106686433777213
iteration 139, loss = 0.0006724081467837095
iteration 140, loss = 0.0007025573286227882
iteration 141, loss = 0.0010514708701521158
iteration 142, loss = 0.0015654866583645344
iteration 143, loss = 0.0008421277161687613
iteration 144, loss = 0.0010576705681160092
iteration 145, loss = 0.0007534740143455565
iteration 146, loss = 0.0009234821773134172
iteration 147, loss = 0.0010016652522608638
iteration 148, loss = 0.0007735085091553628
iteration 149, loss = 0.0009341794066131115
iteration 150, loss = 0.0009174392325803638
iteration 151, loss = 0.0011026225984096527
iteration 152, loss = 0.0008710005204193294
iteration 153, loss = 0.000850446056574583
iteration 154, loss = 0.0010475838789716363
iteration 155, loss = 0.0010444665094837546
iteration 156, loss = 0.0008006304851733148
iteration 157, loss = 0.0017892264295369387
iteration 158, loss = 0.0009658128255978227
iteration 159, loss = 0.0007422980852425098
iteration 160, loss = 0.001024984405376017
iteration 161, loss = 0.0006911198142915964
iteration 162, loss = 0.0012035787804052234
iteration 163, loss = 0.0006154425209388137
iteration 164, loss = 0.0008505207952111959
iteration 165, loss = 0.0008149362402036786
iteration 166, loss = 0.0008409600122831762
iteration 167, loss = 0.0007546900305896997
iteration 168, loss = 0.0009546717628836632
iteration 169, loss = 0.001581918215379119
iteration 170, loss = 0.0016312369843944907
iteration 171, loss = 0.0008375124307349324
iteration 172, loss = 0.0006056456477381289
iteration 173, loss = 0.0007074057357385755
iteration 174, loss = 0.000716351205483079
iteration 175, loss = 0.0007997668581083417
iteration 176, loss = 0.0009117347653955221
iteration 177, loss = 0.0009738904191181064
iteration 178, loss = 0.0009251543669961393
iteration 179, loss = 0.0006136767915450037
iteration 180, loss = 0.0007564182742498815
iteration 181, loss = 0.000770889047998935
iteration 182, loss = 0.001089095021598041
iteration 183, loss = 0.0016899008769541979
iteration 184, loss = 0.0008956590900197625
iteration 185, loss = 0.000983881182037294
iteration 186, loss = 0.0008952725911512971
iteration 187, loss = 0.0020735368598252535
iteration 188, loss = 0.0012533054687082767
iteration 189, loss = 0.0008655051933601499
iteration 190, loss = 0.00154001300688833
iteration 191, loss = 0.0009721598471514881
iteration 192, loss = 0.0007132590981200337
iteration 193, loss = 0.0007787022041156888
iteration 194, loss = 0.0012122387997806072
iteration 195, loss = 0.0009177004685625434
iteration 196, loss = 0.000885627930983901
iteration 197, loss = 0.0013205399736762047
iteration 198, loss = 0.0010310299694538116
iteration 199, loss = 0.0007526088738813996
iteration 200, loss = 0.0008846026612445712
iteration 201, loss = 0.0008002056274563074
iteration 202, loss = 0.0011539780534803867
iteration 203, loss = 0.0007687325705774128
iteration 204, loss = 0.0008012542966753244
iteration 205, loss = 0.0009354334906674922
iteration 206, loss = 0.001230594702064991
iteration 207, loss = 0.0007381806499324739
iteration 208, loss = 0.0008734959992580116
iteration 209, loss = 0.0008226648205891252
iteration 210, loss = 0.0007968589197844267
iteration 211, loss = 0.0008793816086836159
iteration 212, loss = 0.000886659836396575
iteration 213, loss = 0.001467178575694561
iteration 214, loss = 0.0010686737950891256
iteration 215, loss = 0.0010561393573880196
iteration 216, loss = 0.0007510310970246792
iteration 217, loss = 0.001513094874098897
iteration 218, loss = 0.0008345767855644226
iteration 219, loss = 0.000973863119725138
iteration 220, loss = 0.0007554120384156704
iteration 221, loss = 0.001069842604920268
iteration 222, loss = 0.0008054433856159449
iteration 223, loss = 0.0008020676323212683
iteration 224, loss = 0.0008740061894059181
iteration 225, loss = 0.0007576351054012775
iteration 226, loss = 0.0010590143501758575
iteration 227, loss = 0.0016603871481493115
iteration 228, loss = 0.0008062507258728147
iteration 229, loss = 0.0015006079338490963
iteration 230, loss = 0.0007851215195842087
iteration 231, loss = 0.0006832427461631596
iteration 232, loss = 0.0007648938917554915
iteration 233, loss = 0.0008352386066690087
iteration 234, loss = 0.0019484086660668254
iteration 235, loss = 0.0007275213720276952
iteration 236, loss = 0.0008094565127976239
iteration 237, loss = 0.0009007881162688136
iteration 238, loss = 0.0008537601097486913
iteration 239, loss = 0.0007897563627921045
iteration 240, loss = 0.0008354539750143886
iteration 241, loss = 0.0010705319000408053
iteration 242, loss = 0.0007193117053247988
iteration 243, loss = 0.0008608747739344835
iteration 244, loss = 0.000960869190748781
iteration 245, loss = 0.0016973429592326283
iteration 246, loss = 0.0007419916801154613
iteration 247, loss = 0.0014205710031092167
iteration 248, loss = 0.0010462846839800477
iteration 249, loss = 0.0008420191006734967
iteration 250, loss = 0.0006688900175504386
iteration 251, loss = 0.0013304402818903327
iteration 252, loss = 0.0009623204241506755
iteration 253, loss = 0.0007949833525344729
iteration 254, loss = 0.000731027452275157
iteration 255, loss = 0.0009447003831155598
iteration 256, loss = 0.0008675398421473801
iteration 257, loss = 0.0009641200886107981
iteration 258, loss = 0.0009079094743356109
iteration 259, loss = 0.0009275938500650227
iteration 260, loss = 0.0020419424399733543
iteration 261, loss = 0.0016381556633859873
iteration 262, loss = 0.0009686073753982782
iteration 263, loss = 0.0008208694634959102
iteration 264, loss = 0.0008506514132022858
iteration 265, loss = 0.0008102019783109426
iteration 266, loss = 0.000693487178068608
iteration 267, loss = 0.0006840677233412862
iteration 268, loss = 0.0012498779688030481
iteration 269, loss = 0.0007751452503725886
iteration 270, loss = 0.0010015398729592562
iteration 271, loss = 0.0008974594529718161
iteration 272, loss = 0.0008955691009759903
iteration 273, loss = 0.0006815064698457718
iteration 274, loss = 0.0007980875670909882
iteration 275, loss = 0.0008402245002798736
iteration 276, loss = 0.000799810397438705
iteration 277, loss = 0.000885727466084063
iteration 278, loss = 0.0010792355751618743
iteration 279, loss = 0.0007531497394666076
iteration 280, loss = 0.0009116974542848766
iteration 281, loss = 0.0009884705068543553
iteration 282, loss = 0.0008250029059126973
iteration 283, loss = 0.0009327479056082666
iteration 284, loss = 0.0009031225927174091
iteration 285, loss = 0.0009570703259669244
iteration 286, loss = 0.0008717601886019111
iteration 287, loss = 0.0009024646133184433
iteration 288, loss = 0.0014204096514731646
iteration 289, loss = 0.00090722122695297
iteration 290, loss = 0.000879232247825712
iteration 291, loss = 0.0007715347455814481
iteration 292, loss = 0.0008942763088271022
iteration 293, loss = 0.0010187217267230153
iteration 294, loss = 0.0008656780701130629
iteration 295, loss = 0.0012494476977735758
iteration 296, loss = 0.0007303880411200225
iteration 297, loss = 0.000941301928833127
iteration 298, loss = 0.0010598927037790418
iteration 299, loss = 0.0017676468705758452
iteration 300, loss = 0.0007757219718769193
iteration 1, loss = 0.0006936274585314095
iteration 2, loss = 0.0009099264862015843
iteration 3, loss = 0.0008659062441438437
iteration 4, loss = 0.0006784631405025721
iteration 5, loss = 0.0011808512499555945
iteration 6, loss = 0.0014145926106721163
iteration 7, loss = 0.0008847360732033849
iteration 8, loss = 0.000908808084204793
iteration 9, loss = 0.0009552534902468324
iteration 10, loss = 0.0012874442618340254
iteration 11, loss = 0.001014168024994433
iteration 12, loss = 0.0010736481053754687
iteration 13, loss = 0.0010513048619031906
iteration 14, loss = 0.0010824413038790226
iteration 15, loss = 0.0016543224919587374
iteration 16, loss = 0.0007257984252646565
iteration 17, loss = 0.0009521653992123902
iteration 18, loss = 0.0009577450109645724
iteration 19, loss = 0.0007237462559714913
iteration 20, loss = 0.0016210986068472266
iteration 21, loss = 0.0011434218613430858
iteration 22, loss = 0.0012136962031945586
iteration 23, loss = 0.0007453070720657706
iteration 24, loss = 0.0016722835134714842
iteration 25, loss = 0.0007587214349769056
iteration 26, loss = 0.0009237036574631929
iteration 27, loss = 0.00119865033775568
iteration 28, loss = 0.0007228793110698462
iteration 29, loss = 0.0009805348236113787
iteration 30, loss = 0.0006946014473214746
iteration 31, loss = 0.0008631055243313313
iteration 32, loss = 0.0009509689989499748
iteration 33, loss = 0.0012830023188143969
iteration 34, loss = 0.0007306109182536602
iteration 35, loss = 0.0007409624522551894
iteration 36, loss = 0.001016473164781928
iteration 37, loss = 0.0008792735170572996
iteration 38, loss = 0.0006801440613344312
iteration 39, loss = 0.0007037532632239163
iteration 40, loss = 0.0012244462268427014
iteration 41, loss = 0.0014235421549528837
iteration 42, loss = 0.0009347558370791376
iteration 43, loss = 0.000875097990501672
iteration 44, loss = 0.0012579576577991247
iteration 45, loss = 0.0009291342576034367
iteration 46, loss = 0.0007412225822918117
iteration 47, loss = 0.0008448120206594467
iteration 48, loss = 0.0011123369913548231
iteration 49, loss = 0.0018377701053395867
iteration 50, loss = 0.0008523298311047256
iteration 51, loss = 0.0011112774955108762
iteration 52, loss = 0.0007628786843270063
iteration 53, loss = 0.0007181345135904849
iteration 54, loss = 0.0007932098815217614
iteration 55, loss = 0.0008011900936253369
iteration 56, loss = 0.000950577319599688
iteration 57, loss = 0.0008803284144960344
iteration 58, loss = 0.0011579671408981085
iteration 59, loss = 0.0012398955877870321
iteration 60, loss = 0.0008607602794654667
iteration 61, loss = 0.0009881379082798958
iteration 62, loss = 0.000848976313136518
iteration 63, loss = 0.0007069236016832292
iteration 64, loss = 0.0008370422292500734
iteration 65, loss = 0.001401949324645102
iteration 66, loss = 0.000796965672634542
iteration 67, loss = 0.0009710608865134418
iteration 68, loss = 0.0008159677963703871
iteration 69, loss = 0.001743275672197342
iteration 70, loss = 0.0008612412493675947
iteration 71, loss = 0.0016651398036628962
iteration 72, loss = 0.0006527522928081453
iteration 73, loss = 0.001703884219750762
iteration 74, loss = 0.0008143320446833968
iteration 75, loss = 0.00076538190478459
iteration 76, loss = 0.0008293711580336094
iteration 77, loss = 0.000764128752052784
iteration 78, loss = 0.0013345960760489106
iteration 79, loss = 0.000872588308993727
iteration 80, loss = 0.0015453509986400604
iteration 81, loss = 0.0012498112628236413
iteration 82, loss = 0.0008536409586668015
iteration 83, loss = 0.0008698695455677807
iteration 84, loss = 0.0008897099760361016
iteration 85, loss = 0.0007082899683155119
iteration 86, loss = 0.000854745099786669
iteration 87, loss = 0.0008418947691097856
iteration 88, loss = 0.0006556460866704583
iteration 89, loss = 0.0010766133200377226
iteration 90, loss = 0.0007567799766547978
iteration 91, loss = 0.0019274168880656362
iteration 92, loss = 0.0008066685986705124
iteration 93, loss = 0.0010648721363395452
iteration 94, loss = 0.0007448135293088853
iteration 95, loss = 0.0010272059589624405
iteration 96, loss = 0.0008968740003183484
iteration 97, loss = 0.000984584097750485
iteration 98, loss = 0.0012640443164855242
iteration 99, loss = 0.000996001879684627
iteration 100, loss = 0.0017661760793998837
iteration 101, loss = 0.0009054088150151074
iteration 102, loss = 0.0008502326672896743
iteration 103, loss = 0.0008455856586806476
iteration 104, loss = 0.0008812637533992529
iteration 105, loss = 0.0009877131087705493
iteration 106, loss = 0.0008758283220231533
iteration 107, loss = 0.0011761722853407264
iteration 108, loss = 0.0007840139442123473
iteration 109, loss = 0.0010084137320518494
iteration 110, loss = 0.0007316917181015015
iteration 111, loss = 0.0008554459200240672
iteration 112, loss = 0.0015208539552986622
iteration 113, loss = 0.0006663512904196978
iteration 114, loss = 0.0010456169256940484
iteration 115, loss = 0.0009755634819157422
iteration 116, loss = 0.0006975266733206809
iteration 117, loss = 0.00122591492254287
iteration 118, loss = 0.0013436577282845974
iteration 119, loss = 0.0006793070351704955
iteration 120, loss = 0.0012273491593077779
iteration 121, loss = 0.0008086679154075682
iteration 122, loss = 0.0007982407696545124
iteration 123, loss = 0.0007754454854875803
iteration 124, loss = 0.0013008712558075786
iteration 125, loss = 0.0009099303861148655
iteration 126, loss = 0.0007720210123807192
iteration 127, loss = 0.0009085509809665382
iteration 128, loss = 0.0009525239001959562
iteration 129, loss = 0.0008873526239767671
iteration 130, loss = 0.0014431702438741922
iteration 131, loss = 0.0010684797307476401
iteration 132, loss = 0.0012037951964884996
iteration 133, loss = 0.0008058612002059817
iteration 134, loss = 0.0007195794023573399
iteration 135, loss = 0.000961375655606389
iteration 136, loss = 0.0007514078752137721
iteration 137, loss = 0.0010904030641540885
iteration 138, loss = 0.000788637378718704
iteration 139, loss = 0.0006953894626349211
iteration 140, loss = 0.0007307170308195055
iteration 141, loss = 0.0016324544558301568
iteration 142, loss = 0.0008521275594830513
iteration 143, loss = 0.000724870536942035
iteration 144, loss = 0.0007098689093254507
iteration 145, loss = 0.0009470211225561798
iteration 146, loss = 0.0017928271554410458
iteration 147, loss = 0.0010350632946938276
iteration 148, loss = 0.0007762025925330818
iteration 149, loss = 0.0009315623319707811
iteration 150, loss = 0.0009104891214519739
iteration 151, loss = 0.0007107849232852459
iteration 152, loss = 0.0007729926728643477
iteration 153, loss = 0.0007697126129642129
iteration 154, loss = 0.001087459153495729
iteration 155, loss = 0.0009726001881062984
iteration 156, loss = 0.0009193195146508515
iteration 157, loss = 0.0007401680923067033
iteration 158, loss = 0.0009996837470680475
iteration 159, loss = 0.0017561835702508688
iteration 160, loss = 0.000721271033398807
iteration 161, loss = 0.0007300525903701782
iteration 162, loss = 0.0007226068992167711
iteration 163, loss = 0.001266551436856389
iteration 164, loss = 0.0008850092417560518
iteration 165, loss = 0.0010207887971773744
iteration 166, loss = 0.0010955913458019495
iteration 167, loss = 0.0007563111139461398
iteration 168, loss = 0.0007170249591581523
iteration 169, loss = 0.0007318934076465666
iteration 170, loss = 0.0007109163561835885
iteration 171, loss = 0.0014542273711413145
iteration 172, loss = 0.0010782197350636125
iteration 173, loss = 0.0017649800283834338
iteration 174, loss = 0.0006882596062496305
iteration 175, loss = 0.0009055139380507171
iteration 176, loss = 0.0008999944548122585
iteration 177, loss = 0.0009599777986295521
iteration 178, loss = 0.000833290396258235
iteration 179, loss = 0.0013405566569417715
iteration 180, loss = 0.0011696155415847898
iteration 181, loss = 0.0008487934246659279
iteration 182, loss = 0.0008750914712436497
iteration 183, loss = 0.0008367993868887424
iteration 184, loss = 0.0009567352244630456
iteration 185, loss = 0.0010609892196953297
iteration 186, loss = 0.0011527552269399166
iteration 187, loss = 0.0007103027892298996
iteration 188, loss = 0.0008360942592844367
iteration 189, loss = 0.0010110900038853288
iteration 190, loss = 0.0007426064694300294
iteration 191, loss = 0.0012602625647559762
iteration 192, loss = 0.0009597095777280629
iteration 193, loss = 0.0015715103363618255
iteration 194, loss = 0.0015441479627043009
iteration 195, loss = 0.0008402260718867183
iteration 196, loss = 0.0008044717251323164
iteration 197, loss = 0.0008654007688164711
iteration 198, loss = 0.0014858851209282875
iteration 199, loss = 0.0010142665123566985
iteration 200, loss = 0.0007611711043864489
iteration 201, loss = 0.0007749876822344959
iteration 202, loss = 0.0008170371293090284
iteration 203, loss = 0.0009607848478481174
iteration 204, loss = 0.0009103381307795644
iteration 205, loss = 0.0010582564864307642
iteration 206, loss = 0.001650101039558649
iteration 207, loss = 0.0008640885353088379
iteration 208, loss = 0.0006604804657399654
iteration 209, loss = 0.0008370973519049585
iteration 210, loss = 0.001157729304395616
iteration 211, loss = 0.0008341723587363958
iteration 212, loss = 0.0008847836870700121
iteration 213, loss = 0.0008694452117197216
iteration 214, loss = 0.0007269728230312467
iteration 215, loss = 0.0007984590483829379
iteration 216, loss = 0.0008737229509279132
iteration 217, loss = 0.0008268767269328237
iteration 218, loss = 0.0008788141421973705
iteration 219, loss = 0.0011239766608923674
iteration 220, loss = 0.000766706361901015
iteration 221, loss = 0.0010230167536064982
iteration 222, loss = 0.0008517364040017128
iteration 223, loss = 0.0007479878840968013
iteration 224, loss = 0.0009436460095457733
iteration 225, loss = 0.0012693535536527634
iteration 226, loss = 0.0014901356771588326
iteration 227, loss = 0.001114583807066083
iteration 228, loss = 0.0007998503278940916
iteration 229, loss = 0.0006183793884702027
iteration 230, loss = 0.000888336799107492
iteration 231, loss = 0.0007285378524102271
iteration 232, loss = 0.0005572154186666012
iteration 233, loss = 0.0008712423732504249
iteration 234, loss = 0.0007387490477412939
iteration 235, loss = 0.0007604339625686407
iteration 236, loss = 0.000626583700068295
iteration 237, loss = 0.0012840983690693974
iteration 238, loss = 0.0006974797579459846
iteration 239, loss = 0.0006605510716326535
iteration 240, loss = 0.0009535358403809369
iteration 241, loss = 0.00075323082273826
iteration 242, loss = 0.000817585620097816
iteration 243, loss = 0.0009684221586212516
iteration 244, loss = 0.0010688015026971698
iteration 245, loss = 0.0006851219804957509
iteration 246, loss = 0.001414834288880229
iteration 247, loss = 0.0007141855894587934
iteration 248, loss = 0.00097251171246171
iteration 249, loss = 0.0014791464200243354
iteration 250, loss = 0.0009451993391849101
iteration 251, loss = 0.0007668692851439118
iteration 252, loss = 0.000731228559743613
iteration 253, loss = 0.0009311391040682793
iteration 254, loss = 0.0008419473888352513
iteration 255, loss = 0.0007891582208685577
iteration 256, loss = 0.0008602043380960822
iteration 257, loss = 0.0008282628259621561
iteration 258, loss = 0.0009996240260079503
iteration 259, loss = 0.0006891548400744796
iteration 260, loss = 0.001895165885798633
iteration 261, loss = 0.0008331193239428103
iteration 262, loss = 0.0008761392091400921
iteration 263, loss = 0.0017375564202666283
iteration 264, loss = 0.0009088898077607155
iteration 265, loss = 0.0006582958158105612
iteration 266, loss = 0.000791224418208003
iteration 267, loss = 0.0011609067441895604
iteration 268, loss = 0.001192688592709601
iteration 269, loss = 0.0008144804160110652
iteration 270, loss = 0.001543721416965127
iteration 271, loss = 0.0013643248239532113
iteration 272, loss = 0.0009632505825720727
iteration 273, loss = 0.0008996592951007187
iteration 274, loss = 0.0008644677582196891
iteration 275, loss = 0.0008475229842588305
iteration 276, loss = 0.0007923414232209325
iteration 277, loss = 0.0007685675518587232
iteration 278, loss = 0.000814074941445142
iteration 279, loss = 0.0008685325738042593
iteration 280, loss = 0.0008625341579318047
iteration 281, loss = 0.0007391349063254893
iteration 282, loss = 0.0008916625520214438
iteration 283, loss = 0.000859077728819102
iteration 284, loss = 0.0010181544348597527
iteration 285, loss = 0.0008003305993042886
iteration 286, loss = 0.0007292442605830729
iteration 287, loss = 0.0015472265658900142
iteration 288, loss = 0.0008674197597429156
iteration 289, loss = 0.0012313064653426409
iteration 290, loss = 0.0013987960992380977
iteration 291, loss = 0.0008295247098430991
iteration 292, loss = 0.0010242123389616609
iteration 293, loss = 0.0007945082033984363
iteration 294, loss = 0.0010937204351648688
iteration 295, loss = 0.0008209382649511099
iteration 296, loss = 0.0008497236995026469
iteration 297, loss = 0.0007736982661299407
iteration 298, loss = 0.001001414842903614
iteration 299, loss = 0.0008029351010918617
iteration 300, loss = 0.0010603972477838397
iteration 1, loss = 0.0008230238454416394
iteration 2, loss = 0.0008613855461589992
iteration 3, loss = 0.001089289435185492
iteration 4, loss = 0.0008896500803530216
iteration 5, loss = 0.0008453858899883926
iteration 6, loss = 0.001107292715460062
iteration 7, loss = 0.001187030109576881
iteration 8, loss = 0.0009807634633034468
iteration 9, loss = 0.0007174138445407152
iteration 10, loss = 0.001568100182339549
iteration 11, loss = 0.0007065549143590033
iteration 12, loss = 0.0010441133053973317
iteration 13, loss = 0.000915809185244143
iteration 14, loss = 0.0011482321424409747
iteration 15, loss = 0.0008481813711114228
iteration 16, loss = 0.0007995637715794146
iteration 17, loss = 0.0008941174601204693
iteration 18, loss = 0.0008350616553798318
iteration 19, loss = 0.0007637349772267044
iteration 20, loss = 0.0006358976825140417
iteration 21, loss = 0.000868438626639545
iteration 22, loss = 0.0009207891998812556
iteration 23, loss = 0.0007651672349311411
iteration 24, loss = 0.0006862541777081788
iteration 25, loss = 0.0007905916427262127
iteration 26, loss = 0.0008606383926235139
iteration 27, loss = 0.000868866452947259
iteration 28, loss = 0.0008818423375487328
iteration 29, loss = 0.0008875714265741408
iteration 30, loss = 0.0008602754096500576
iteration 31, loss = 0.0013128072023391724
iteration 32, loss = 0.0012021295260637999
iteration 33, loss = 0.0008900311077013612
iteration 34, loss = 0.0013151984894648194
iteration 35, loss = 0.0007854047580622137
iteration 36, loss = 0.000816978164948523
iteration 37, loss = 0.0009160750196315348
iteration 38, loss = 0.0010611379984766245
iteration 39, loss = 0.0008068863535299897
iteration 40, loss = 0.00079684192314744
iteration 41, loss = 0.0007220958941616118
iteration 42, loss = 0.0011199434520676732
iteration 43, loss = 0.0009300648234784603
iteration 44, loss = 0.00094973313389346
iteration 45, loss = 0.001202086335979402
iteration 46, loss = 0.000778404762968421
iteration 47, loss = 0.0011332440190017223
iteration 48, loss = 0.0009016238036565483
iteration 49, loss = 0.0009708754951134324
iteration 50, loss = 0.0009231353178620338
iteration 51, loss = 0.0012174242874607444
iteration 52, loss = 0.0007174737984314561
iteration 53, loss = 0.0009428730700165033
iteration 54, loss = 0.0008022347465157509
iteration 55, loss = 0.0007983657997101545
iteration 56, loss = 0.0008582225418649614
iteration 57, loss = 0.0008647717186249793
iteration 58, loss = 0.0007486501126550138
iteration 59, loss = 0.00087352265836671
iteration 60, loss = 0.0015376904048025608
iteration 61, loss = 0.0008137078257277608
iteration 62, loss = 0.0006810869672335684
iteration 63, loss = 0.0007753595127724111
iteration 64, loss = 0.0009298518998548388
iteration 65, loss = 0.0012350832112133503
iteration 66, loss = 0.0008592206286266446
iteration 67, loss = 0.0012242787051945925
iteration 68, loss = 0.00160258321557194
iteration 69, loss = 0.0016303567681461573
iteration 70, loss = 0.0008987742476165295
iteration 71, loss = 0.0022410517558455467
iteration 72, loss = 0.0006764984573237598
iteration 73, loss = 0.0012363613350316882
iteration 74, loss = 0.0008361732470802963
iteration 75, loss = 0.0008114813826978207
iteration 76, loss = 0.0007168129086494446
iteration 77, loss = 0.0008887838339433074
iteration 78, loss = 0.0009172722930088639
iteration 79, loss = 0.0011317695025354624
iteration 80, loss = 0.0008264380157925189
iteration 81, loss = 0.0011474261991679668
iteration 82, loss = 0.0017150465864688158
iteration 83, loss = 0.0010244433069601655
iteration 84, loss = 0.0006866625626571476
iteration 85, loss = 0.0007660475675947964
iteration 86, loss = 0.0007315883412957191
iteration 87, loss = 0.001003884244710207
iteration 88, loss = 0.0010533636668697
iteration 89, loss = 0.0006394851952791214
iteration 90, loss = 0.0008762574288994074
iteration 91, loss = 0.0010714194504544139
iteration 92, loss = 0.0007571165915578604
iteration 93, loss = 0.0011784073431044817
iteration 94, loss = 0.0010059102205559611
iteration 95, loss = 0.0013009433168917894
iteration 96, loss = 0.0008586727781221271
iteration 97, loss = 0.0017120102420449257
iteration 98, loss = 0.0006985997315496206
iteration 99, loss = 0.001578294555656612
iteration 100, loss = 0.0007631823536939919
iteration 101, loss = 0.0007190582109615207
iteration 102, loss = 0.0010841557523235679
iteration 103, loss = 0.001157564576715231
iteration 104, loss = 0.0010530397994443774
iteration 105, loss = 0.0008124372689053416
iteration 106, loss = 0.0007980606169439852
iteration 107, loss = 0.0007754389080218971
iteration 108, loss = 0.0008115394739434123
iteration 109, loss = 0.0007879565237089992
iteration 110, loss = 0.0008538608672097325
iteration 111, loss = 0.0009598545730113983
iteration 112, loss = 0.000782971503213048
iteration 113, loss = 0.0013402590993791819
iteration 114, loss = 0.0017009866423904896
iteration 115, loss = 0.0012108298251405358
iteration 116, loss = 0.0016601044917479157
iteration 117, loss = 0.0007335077971220016
iteration 118, loss = 0.0007658116519451141
iteration 119, loss = 0.0017924498533830047
iteration 120, loss = 0.0011229160008952022
iteration 121, loss = 0.0007360150339081883
iteration 122, loss = 0.0008579895365983248
iteration 123, loss = 0.0009070636588148773
iteration 124, loss = 0.0010564454132691026
iteration 125, loss = 0.0008436577045358717
iteration 126, loss = 0.0007498367340303957
iteration 127, loss = 0.0008592548547312617
iteration 128, loss = 0.0008027614094316959
iteration 129, loss = 0.0009576270822435617
iteration 130, loss = 0.0008766487590037286
iteration 131, loss = 0.0008896937943063676
iteration 132, loss = 0.0008317093015648425
iteration 133, loss = 0.0008845838019624352
iteration 134, loss = 0.0014405302936211228
iteration 135, loss = 0.0011190514778718352
iteration 136, loss = 0.0009527449146844447
iteration 137, loss = 0.0007295685354620218
iteration 138, loss = 0.0010472197318449616
iteration 139, loss = 0.0007445714436471462
iteration 140, loss = 0.0008054652134887874
iteration 141, loss = 0.001072653103619814
iteration 142, loss = 0.0008251059334725142
iteration 143, loss = 0.001419505337253213
iteration 144, loss = 0.0007060924544930458
iteration 145, loss = 0.001155327190645039
iteration 146, loss = 0.0006627574912272394
iteration 147, loss = 0.0008391199517063797
iteration 148, loss = 0.0007904007798060775
iteration 149, loss = 0.0009067796636372805
iteration 150, loss = 0.0009939573938027024
iteration 151, loss = 0.0012169760884717107
iteration 152, loss = 0.0006879032589495182
iteration 153, loss = 0.0017367539694532752
iteration 154, loss = 0.0006183087243698537
iteration 155, loss = 0.0008014345075935125
iteration 156, loss = 0.0008399044745601714
iteration 157, loss = 0.0007826330838724971
iteration 158, loss = 0.0006722261314280331
iteration 159, loss = 0.0008632555254735053
iteration 160, loss = 0.0008312933496199548
iteration 161, loss = 0.0008667075890116394
iteration 162, loss = 0.0009282307582907379
iteration 163, loss = 0.0009110471000894904
iteration 164, loss = 0.0012463980820029974
iteration 165, loss = 0.0007501019863411784
iteration 166, loss = 0.0009271664312109351
iteration 167, loss = 0.0013312260853126645
iteration 168, loss = 0.0009130471153184772
iteration 169, loss = 0.0015696033369749784
iteration 170, loss = 0.000624386069830507
iteration 171, loss = 0.0011024160776287317
iteration 172, loss = 0.0006476009148173034
iteration 173, loss = 0.0010916185565292835
iteration 174, loss = 0.0009717332432046533
iteration 175, loss = 0.0007166403229348361
iteration 176, loss = 0.0008496856316924095
iteration 177, loss = 0.0008858564542606473
iteration 178, loss = 0.0008319021435454488
iteration 179, loss = 0.001118903630413115
iteration 180, loss = 0.0009169176919385791
iteration 181, loss = 0.001002054545097053
iteration 182, loss = 0.0007798102451488376
iteration 183, loss = 0.001174293109215796
iteration 184, loss = 0.0017030180897563696
iteration 185, loss = 0.0008721454651094973
iteration 186, loss = 0.0007532350136898458
iteration 187, loss = 0.0007899795891717076
iteration 188, loss = 0.001086762873455882
iteration 189, loss = 0.0008224608027376235
iteration 190, loss = 0.0009754079510457814
iteration 191, loss = 0.0009853503433987498
iteration 192, loss = 0.0008724613580852747
iteration 193, loss = 0.0008391546434722841
iteration 194, loss = 0.001002022298052907
iteration 195, loss = 0.0008315691957250237
iteration 196, loss = 0.0010423423955217004
iteration 197, loss = 0.0018850245978683233
iteration 198, loss = 0.0012680982472375035
iteration 199, loss = 0.000790644611697644
iteration 200, loss = 0.0007859638426452875
iteration 201, loss = 0.0009916478302329779
iteration 202, loss = 0.0007414779975079
iteration 203, loss = 0.001593374996446073
iteration 204, loss = 0.0007516751065850258
iteration 205, loss = 0.0008715214789845049
iteration 206, loss = 0.0007775265257805586
iteration 207, loss = 0.0022787984926253557
iteration 208, loss = 0.0009182654321193695
iteration 209, loss = 0.0009901312878355384
iteration 210, loss = 0.0007175614591687918
iteration 211, loss = 0.0009160179761238396
iteration 212, loss = 0.0009406064637005329
iteration 213, loss = 0.000991221284493804
iteration 214, loss = 0.0018471676157787442
iteration 215, loss = 0.0007360917516052723
iteration 216, loss = 0.000909992610104382
iteration 217, loss = 0.0011080126278102398
iteration 218, loss = 0.0008269853424280882
iteration 219, loss = 0.0007759510772302747
iteration 220, loss = 0.0007995200576260686
iteration 221, loss = 0.0010468221735209227
iteration 222, loss = 0.0008935554651543498
iteration 223, loss = 0.0008630016818642616
iteration 224, loss = 0.001128752832300961
iteration 225, loss = 0.0007189754978753626
iteration 226, loss = 0.0011207531206309795
iteration 227, loss = 0.0008890043245628476
iteration 228, loss = 0.0009298623772338033
iteration 229, loss = 0.0008381720981560647
iteration 230, loss = 0.0007670965860597789
iteration 231, loss = 0.0009318760130554438
iteration 232, loss = 0.0006315320497378707
iteration 233, loss = 0.0008647798094898462
iteration 234, loss = 0.0007214033976197243
iteration 235, loss = 0.0008591982768848538
iteration 236, loss = 0.0008780121570453048
iteration 237, loss = 0.0008301743073388934
iteration 238, loss = 0.0014913013437762856
iteration 239, loss = 0.0006944143096916378
iteration 240, loss = 0.0008461589459329844
iteration 241, loss = 0.0008790323045104742
iteration 242, loss = 0.0012201563222333789
iteration 243, loss = 0.0008192063542082906
iteration 244, loss = 0.0007330112275667489
iteration 245, loss = 0.0006697752978652716
iteration 246, loss = 0.000741290976293385
iteration 247, loss = 0.0010956997284665704
iteration 248, loss = 0.0008450899040326476
iteration 249, loss = 0.000649322581011802
iteration 250, loss = 0.00152312102727592
iteration 251, loss = 0.0019711400382220745
iteration 252, loss = 0.0008259893511421978
iteration 253, loss = 0.0008899985114112496
iteration 254, loss = 0.0011416816851124167
iteration 255, loss = 0.0011110459454357624
iteration 256, loss = 0.0007899162010289729
iteration 257, loss = 0.0015657325275242329
iteration 258, loss = 0.0007139025838114321
iteration 259, loss = 0.0008089776383712888
iteration 260, loss = 0.001509061548858881
iteration 261, loss = 0.0009472186211496592
iteration 262, loss = 0.0006720699020661414
iteration 263, loss = 0.0006933220429345965
iteration 264, loss = 0.0007649213657714427
iteration 265, loss = 0.0011258176527917385
iteration 266, loss = 0.00100791547447443
iteration 267, loss = 0.0008712902781553566
iteration 268, loss = 0.0006342300330288708
iteration 269, loss = 0.0011310349218547344
iteration 270, loss = 0.0009774590143933892
iteration 271, loss = 0.0010909379925578833
iteration 272, loss = 0.0008431493188254535
iteration 273, loss = 0.001418553525581956
iteration 274, loss = 0.0008608993957750499
iteration 275, loss = 0.0015008162008598447
iteration 276, loss = 0.000812025333289057
iteration 277, loss = 0.0006892915116623044
iteration 278, loss = 0.0010007048258557916
iteration 279, loss = 0.000718571012839675
iteration 280, loss = 0.0007660108967684209
iteration 281, loss = 0.0007370221428573132
iteration 282, loss = 0.0010144656989723444
iteration 283, loss = 0.0008820371003821492
iteration 284, loss = 0.0014499721582978964
iteration 285, loss = 0.0009497083956375718
iteration 286, loss = 0.0008061181288212538
iteration 287, loss = 0.0007279475685209036
iteration 288, loss = 0.0013333510141819715
iteration 289, loss = 0.000772748957388103
iteration 290, loss = 0.0008526448509655893
iteration 291, loss = 0.0008901123073883355
iteration 292, loss = 0.001487020286731422
iteration 293, loss = 0.0012435438111424446
iteration 294, loss = 0.0011543771252036095
iteration 295, loss = 0.0011621833546087146
iteration 296, loss = 0.0008379260543733835
iteration 297, loss = 0.000837029074318707
iteration 298, loss = 0.001087549957446754
iteration 299, loss = 0.0007834620773792267
iteration 300, loss = 0.0010807289509102702
iteration 1, loss = 0.001435029087588191
iteration 2, loss = 0.000757941510528326
iteration 3, loss = 0.0011112188221886754
iteration 4, loss = 0.0010347000788897276
iteration 5, loss = 0.0009620538330636919
iteration 6, loss = 0.0007595571805723011
iteration 7, loss = 0.000796846637967974
iteration 8, loss = 0.0008798011695034802
iteration 9, loss = 0.0010385926580056548
iteration 10, loss = 0.0009826755849644542
iteration 11, loss = 0.0008684334461577237
iteration 12, loss = 0.001005709171295166
iteration 13, loss = 0.0007843354833312333
iteration 14, loss = 0.0009280176018364727
iteration 15, loss = 0.0007590138120576739
iteration 16, loss = 0.0014529561158269644
iteration 17, loss = 0.0007480433559976518
iteration 18, loss = 0.0014806011458858848
iteration 19, loss = 0.0007411105325445533
iteration 20, loss = 0.0010708343470469117
iteration 21, loss = 0.0011175096733495593
iteration 22, loss = 0.0007378353038802743
iteration 23, loss = 0.0015634018927812576
iteration 24, loss = 0.000774078827816993
iteration 25, loss = 0.0009738379158079624
iteration 26, loss = 0.0013707199832424521
iteration 27, loss = 0.0007598730153404176
iteration 28, loss = 0.0006704664556309581
iteration 29, loss = 0.0008957944810390472
iteration 30, loss = 0.0008189182262867689
iteration 31, loss = 0.0007125139236450195
iteration 32, loss = 0.000937674951273948
iteration 33, loss = 0.0008900176035240293
iteration 34, loss = 0.0007393323467113078
iteration 35, loss = 0.0007987496792338789
iteration 36, loss = 0.0017263571498915553
iteration 37, loss = 0.0009948365623131394
iteration 38, loss = 0.0012110439129173756
iteration 39, loss = 0.0009400987764820457
iteration 40, loss = 0.0007660901173949242
iteration 41, loss = 0.0013192285550758243
iteration 42, loss = 0.0009290941525250673
iteration 43, loss = 0.0008539819391444325
iteration 44, loss = 0.0008955395314842463
iteration 45, loss = 0.0014985704328864813
iteration 46, loss = 0.0007343877223320305
iteration 47, loss = 0.0007987363496795297
iteration 48, loss = 0.0008254289859905839
iteration 49, loss = 0.0007445588707923889
iteration 50, loss = 0.0007977338973432779
iteration 51, loss = 0.0009585545631125569
iteration 52, loss = 0.0007042316719889641
iteration 53, loss = 0.0008436455973424017
iteration 54, loss = 0.0011910372413694859
iteration 55, loss = 0.0007065216195769608
iteration 56, loss = 0.0008405319531448185
iteration 57, loss = 0.0006976083968766034
iteration 58, loss = 0.0007094718748703599
iteration 59, loss = 0.0008847464341670275
iteration 60, loss = 0.0010369240771979094
iteration 61, loss = 0.0010456504533067346
iteration 62, loss = 0.0008306890958920121
iteration 63, loss = 0.0008559760171920061
iteration 64, loss = 0.000873232667800039
iteration 65, loss = 0.0012453782837837934
iteration 66, loss = 0.0007501970394514501
iteration 67, loss = 0.0007364356424659491
iteration 68, loss = 0.0013978383503854275
iteration 69, loss = 0.0012180553749203682
iteration 70, loss = 0.0009196805767714977
iteration 71, loss = 0.0008223920594900846
iteration 72, loss = 0.0006830766797065735
iteration 73, loss = 0.0012245101388543844
iteration 74, loss = 0.0009039752185344696
iteration 75, loss = 0.0011456223437562585
iteration 76, loss = 0.0008468074956908822
iteration 77, loss = 0.0008298467146232724
iteration 78, loss = 0.0008570997160859406
iteration 79, loss = 0.0018085096962749958
iteration 80, loss = 0.0007263345178216696
iteration 81, loss = 0.000735233654268086
iteration 82, loss = 0.0009329942986369133
iteration 83, loss = 0.0008113958174362779
iteration 84, loss = 0.0007777723949402571
iteration 85, loss = 0.0007801727624610066
iteration 86, loss = 0.0008470534812659025
iteration 87, loss = 0.0010231835767626762
iteration 88, loss = 0.0009969599777832627
iteration 89, loss = 0.0010441472986713052
iteration 90, loss = 0.0007998291403055191
iteration 91, loss = 0.000719891395419836
iteration 92, loss = 0.000996210379526019
iteration 93, loss = 0.0010302107548341155
iteration 94, loss = 0.0006667342968285084
iteration 95, loss = 0.0007757290150038898
iteration 96, loss = 0.0006859522545710206
iteration 97, loss = 0.0010894893202930689
iteration 98, loss = 0.0007651851046830416
iteration 99, loss = 0.000774166255723685
iteration 100, loss = 0.0007978847133927047
iteration 101, loss = 0.0010706253815442324
iteration 102, loss = 0.0007378660375252366
iteration 103, loss = 0.001137310522608459
iteration 104, loss = 0.0007943236269056797
iteration 105, loss = 0.000927435583434999
iteration 106, loss = 0.0008856520871631801
iteration 107, loss = 0.0017328180838376284
iteration 108, loss = 0.0008770367712713778
iteration 109, loss = 0.0008395725162699819
iteration 110, loss = 0.0009473382961004972
iteration 111, loss = 0.0012842488940805197
iteration 112, loss = 0.0014019547961652279
iteration 113, loss = 0.0012084838235750794
iteration 114, loss = 0.0011658772127702832
iteration 115, loss = 0.0007867394015192986
iteration 116, loss = 0.0008237698930315673
iteration 117, loss = 0.0007231048657558858
iteration 118, loss = 0.0006647630361840129
iteration 119, loss = 0.0007402508053928614
iteration 120, loss = 0.0010804588673636317
iteration 121, loss = 0.0011687100632116199
iteration 122, loss = 0.001121722743846476
iteration 123, loss = 0.0008707254892215133
iteration 124, loss = 0.0012561719631776214
iteration 125, loss = 0.001015338464640081
iteration 126, loss = 0.0007665877928957343
iteration 127, loss = 0.0007783577311784029
iteration 128, loss = 0.0007686242461204529
iteration 129, loss = 0.000799154513515532
iteration 130, loss = 0.0009017380070872605
iteration 131, loss = 0.0014039053348824382
iteration 132, loss = 0.0009158708853647113
iteration 133, loss = 0.0007395895081572235
iteration 134, loss = 0.0016575613990426064
iteration 135, loss = 0.0008708258392289281
iteration 136, loss = 0.000757156522013247
iteration 137, loss = 0.0010556348133832216
iteration 138, loss = 0.0007242272840812802
iteration 139, loss = 0.0008152051595970988
iteration 140, loss = 0.00081506313290447
iteration 141, loss = 0.0010051423450931907
iteration 142, loss = 0.0008157562697306275
iteration 143, loss = 0.0007448865217156708
iteration 144, loss = 0.0008240432944148779
iteration 145, loss = 0.0014546613674610853
iteration 146, loss = 0.00178684969432652
iteration 147, loss = 0.000846940791234374
iteration 148, loss = 0.000858719868119806
iteration 149, loss = 0.0007592358160763979
iteration 150, loss = 0.0015686667757108808
iteration 151, loss = 0.0015135490102693439
iteration 152, loss = 0.0008246597135439515
iteration 153, loss = 0.0007910727872513235
iteration 154, loss = 0.0009493480320088565
iteration 155, loss = 0.00089752342319116
iteration 156, loss = 0.0016261425334960222
iteration 157, loss = 0.0009253426687791944
iteration 158, loss = 0.0007652259664610028
iteration 159, loss = 0.0007901451899670064
iteration 160, loss = 0.0007247955072671175
iteration 161, loss = 0.001012434484437108
iteration 162, loss = 0.0009429705096408725
iteration 163, loss = 0.000785363488830626
iteration 164, loss = 0.000892084208317101
iteration 165, loss = 0.000734473520424217
iteration 166, loss = 0.0008501517586410046
iteration 167, loss = 0.000911177892703563
iteration 168, loss = 0.0008185010519810021
iteration 169, loss = 0.0014374683378264308
iteration 170, loss = 0.0009742957772687078
iteration 171, loss = 0.0009996648877859116
iteration 172, loss = 0.001577978953719139
iteration 173, loss = 0.0007215990917757154
iteration 174, loss = 0.0008879072265699506
iteration 175, loss = 0.001046213903464377
iteration 176, loss = 0.0011974181979894638
iteration 177, loss = 0.0015974270645529032
iteration 178, loss = 0.0007026665844023228
iteration 179, loss = 0.0008673620759509504
iteration 180, loss = 0.0008291153353638947
iteration 181, loss = 0.0007164455018937588
iteration 182, loss = 0.0009293978800997138
iteration 183, loss = 0.0007221129490062594
iteration 184, loss = 0.0009666357655078173
iteration 185, loss = 0.0007848493987694383
iteration 186, loss = 0.0007552564493380487
iteration 187, loss = 0.0008373253513127565
iteration 188, loss = 0.0007884648512117565
iteration 189, loss = 0.0014519100077450275
iteration 190, loss = 0.0008198914583772421
iteration 191, loss = 0.0010948177659884095
iteration 192, loss = 0.0009233280434273183
iteration 193, loss = 0.0009975254070013762
iteration 194, loss = 0.0015582104679197073
iteration 195, loss = 0.0012052301317453384
iteration 196, loss = 0.001813442213460803
iteration 197, loss = 0.0011103383731096983
iteration 198, loss = 0.0006704773986712098
iteration 199, loss = 0.000980385928414762
iteration 200, loss = 0.000864140223711729
iteration 201, loss = 0.0009460978908464313
iteration 202, loss = 0.0009373532375320792
iteration 203, loss = 0.000722307653632015
iteration 204, loss = 0.0007888232357800007
iteration 205, loss = 0.000933992734644562
iteration 206, loss = 0.001123133348301053
iteration 207, loss = 0.0008719878969714046
iteration 208, loss = 0.0009489102521911263
iteration 209, loss = 0.0016540478682145476
iteration 210, loss = 0.0009341149125248194
iteration 211, loss = 0.0022054428700357676
iteration 212, loss = 0.0009948051301762462
iteration 213, loss = 0.0011087862076237798
iteration 214, loss = 0.0006776939844712615
iteration 215, loss = 0.0008682927000336349
iteration 216, loss = 0.0009083072072826326
iteration 217, loss = 0.000855335732921958
iteration 218, loss = 0.0016902757342904806
iteration 219, loss = 0.0007923821685835719
iteration 220, loss = 0.000920325459446758
iteration 221, loss = 0.0007371274987235665
iteration 222, loss = 0.0015368268359452486
iteration 223, loss = 0.0009526751236990094
iteration 224, loss = 0.0007727100746706128
iteration 225, loss = 0.0009606307721696794
iteration 226, loss = 0.0014572812942788005
iteration 227, loss = 0.0010316388215869665
iteration 228, loss = 0.0013594168704003096
iteration 229, loss = 0.0006724963895976543
iteration 230, loss = 0.0008858682704158127
iteration 231, loss = 0.00091124651953578
iteration 232, loss = 0.0009170743869617581
iteration 233, loss = 0.0011256532743573189
iteration 234, loss = 0.0010053118458017707
iteration 235, loss = 0.0011365036480128765
iteration 236, loss = 0.0016080186469480395
iteration 237, loss = 0.0006731033208779991
iteration 238, loss = 0.000783583614975214
iteration 239, loss = 0.0007914082380011678
iteration 240, loss = 0.0007891863933764398
iteration 241, loss = 0.0009104983182623982
iteration 242, loss = 0.0014879708178341389
iteration 243, loss = 0.0012076463317498565
iteration 244, loss = 0.0009322351543232799
iteration 245, loss = 0.0007378031150437891
iteration 246, loss = 0.0007374542765319347
iteration 247, loss = 0.001174351666122675
iteration 248, loss = 0.001209478359669447
iteration 249, loss = 0.0007415458676405251
iteration 250, loss = 0.0012215044116601348
iteration 251, loss = 0.0006673607276752591
iteration 252, loss = 0.000945461739320308
iteration 253, loss = 0.0008430360467173159
iteration 254, loss = 0.0008786016842350364
iteration 255, loss = 0.0008075108635239303
iteration 256, loss = 0.00076326570706442
iteration 257, loss = 0.001352958963252604
iteration 258, loss = 0.0009309546439908445
iteration 259, loss = 0.0007785031921230257
iteration 260, loss = 0.0009321483667008579
iteration 261, loss = 0.0007718149572610855
iteration 262, loss = 0.0013696870300918818
iteration 263, loss = 0.0015630769776180387
iteration 264, loss = 0.0008783116936683655
iteration 265, loss = 0.0009338865056633949
iteration 266, loss = 0.0007133216131478548
iteration 267, loss = 0.0009049309883266687
iteration 268, loss = 0.0007300404249690473
iteration 269, loss = 0.0011403316166251898
iteration 270, loss = 0.0008405696717090905
iteration 271, loss = 0.0010072037111967802
iteration 272, loss = 0.0006404257146641612
iteration 273, loss = 0.0008145206957124174
iteration 274, loss = 0.0007203578716143966
iteration 275, loss = 0.0007751432131044567
iteration 276, loss = 0.0008247184450738132
iteration 277, loss = 0.0011558831902220845
iteration 278, loss = 0.0008335986640304327
iteration 279, loss = 0.000846963725052774
iteration 280, loss = 0.000829325697850436
iteration 281, loss = 0.0008071610936895013
iteration 282, loss = 0.0011073227506130934
iteration 283, loss = 0.0010207903105765581
iteration 284, loss = 0.0007982332608662546
iteration 285, loss = 0.000814368948340416
iteration 286, loss = 0.000886009365785867
iteration 287, loss = 0.0015818770043551922
iteration 288, loss = 0.000761477102059871
iteration 289, loss = 0.000827332551125437
iteration 290, loss = 0.000806826283223927
iteration 291, loss = 0.0017259252490475774
iteration 292, loss = 0.0012286541750654578
iteration 293, loss = 0.0007986028213053942
iteration 294, loss = 0.001302392571233213
iteration 295, loss = 0.0012732211034744978
iteration 296, loss = 0.0008477168739773333
iteration 297, loss = 0.0010358750587329268
iteration 298, loss = 0.0006423689774237573
iteration 299, loss = 0.0009309080196544528
iteration 300, loss = 0.0009938778821378946
iteration 1, loss = 0.0007026056409813464
iteration 2, loss = 0.0010072526056319475
iteration 3, loss = 0.0011509781470522285
iteration 4, loss = 0.0007690226775594056
iteration 5, loss = 0.0007579223602078855
iteration 6, loss = 0.0010760582517832518
iteration 7, loss = 0.0008414003532379866
iteration 8, loss = 0.0008294775616377592
iteration 9, loss = 0.0009671636507846415
iteration 10, loss = 0.0007322994060814381
iteration 11, loss = 0.0007512407610192895
iteration 12, loss = 0.0007966000703163445
iteration 13, loss = 0.0008106025634333491
iteration 14, loss = 0.0012700343504548073
iteration 15, loss = 0.0009191284189000726
iteration 16, loss = 0.0010569288861006498
iteration 17, loss = 0.001008711289614439
iteration 18, loss = 0.0014046459691599011
iteration 19, loss = 0.0008659762097522616
iteration 20, loss = 0.0008676546276547015
iteration 21, loss = 0.0009044792386703193
iteration 22, loss = 0.00082299520727247
iteration 23, loss = 0.0006800885312259197
iteration 24, loss = 0.0009141101036220789
iteration 25, loss = 0.0014852142194285989
iteration 26, loss = 0.001640821574255824
iteration 27, loss = 0.0007651238702237606
iteration 28, loss = 0.0012300467351451516
iteration 29, loss = 0.0009632391738705337
iteration 30, loss = 0.000951827154494822
iteration 31, loss = 0.0011572449002414942
iteration 32, loss = 0.0007084098178893328
iteration 33, loss = 0.0008426447166129947
iteration 34, loss = 0.0009360761032439768
iteration 35, loss = 0.0008813898894004524
iteration 36, loss = 0.0012692249147221446
iteration 37, loss = 0.0008186266641132534
iteration 38, loss = 0.001053585554473102
iteration 39, loss = 0.0009738203953020275
iteration 40, loss = 0.0007722649024799466
iteration 41, loss = 0.0006873473757877946
iteration 42, loss = 0.001349995145574212
iteration 43, loss = 0.0007810384850017726
iteration 44, loss = 0.0010139233199879527
iteration 45, loss = 0.0008512873318977654
iteration 46, loss = 0.000708616164047271
iteration 47, loss = 0.001116992556490004
iteration 48, loss = 0.0008757132454775274
iteration 49, loss = 0.0012188887922093272
iteration 50, loss = 0.0008331718272529542
iteration 51, loss = 0.0007957823108881712
iteration 52, loss = 0.0013810851378366351
iteration 53, loss = 0.0006918433937244117
iteration 54, loss = 0.0006978018209338188
iteration 55, loss = 0.0009832853684201837
iteration 56, loss = 0.0009479588479734957
iteration 57, loss = 0.0009699797374196351
iteration 58, loss = 0.0008860003435984254
iteration 59, loss = 0.0010707408655434847
iteration 60, loss = 0.0008051535114645958
iteration 61, loss = 0.0007730231154710054
iteration 62, loss = 0.0006645723478868604
iteration 63, loss = 0.0014377152547240257
iteration 64, loss = 0.0009798950050026178
iteration 65, loss = 0.0008186177001334727
iteration 66, loss = 0.0009062362951226532
iteration 67, loss = 0.0007795265410095453
iteration 68, loss = 0.0009006531909108162
iteration 69, loss = 0.0007694851374253631
iteration 70, loss = 0.0010299108689650893
iteration 71, loss = 0.0007230687187984586
iteration 72, loss = 0.0008366114925593138
iteration 73, loss = 0.0008716555312275887
iteration 74, loss = 0.0009352483903057873
iteration 75, loss = 0.002229145262390375
iteration 76, loss = 0.0009264123509638011
iteration 77, loss = 0.0008514400105923414
iteration 78, loss = 0.0007079506758600473
iteration 79, loss = 0.0011370700085535645
iteration 80, loss = 0.0008781590731814504
iteration 81, loss = 0.0018817478558048606
iteration 82, loss = 0.0008298417087644339
iteration 83, loss = 0.0008698898018337786
iteration 84, loss = 0.0008012114558368921
iteration 85, loss = 0.0007425368530675769
iteration 86, loss = 0.0008058651583269238
iteration 87, loss = 0.0008274371502920985
iteration 88, loss = 0.0017830041470006108
iteration 89, loss = 0.0008035354549065232
iteration 90, loss = 0.0007058584596961737
iteration 91, loss = 0.0011035766219720244
iteration 92, loss = 0.0008135781390592456
iteration 93, loss = 0.0009581787744536996
iteration 94, loss = 0.0007651782361790538
iteration 95, loss = 0.00157124362885952
iteration 96, loss = 0.0008253496489487588
iteration 97, loss = 0.0008782415534369648
iteration 98, loss = 0.0008136223186738789
iteration 99, loss = 0.0009506383794359863
iteration 100, loss = 0.0007112529710866511
iteration 101, loss = 0.0017876914935186505
iteration 102, loss = 0.0008928440511226654
iteration 103, loss = 0.001393144833855331
iteration 104, loss = 0.0009573473362252116
iteration 105, loss = 0.0009736224310472608
iteration 106, loss = 0.00137149088550359
iteration 107, loss = 0.0008650176459923387
iteration 108, loss = 0.0008751770947128534
iteration 109, loss = 0.0007402417832054198
iteration 110, loss = 0.0011195697588846087
iteration 111, loss = 0.0007559566292911768
iteration 112, loss = 0.001153529155999422
iteration 113, loss = 0.0009908895008265972
iteration 114, loss = 0.0016174801858142018
iteration 115, loss = 0.0010073084849864244
iteration 116, loss = 0.0006694990443065763
iteration 117, loss = 0.0009509869269095361
iteration 118, loss = 0.001345635624602437
iteration 119, loss = 0.0008294053841382265
iteration 120, loss = 0.0010203449055552483
iteration 121, loss = 0.001994837773963809
iteration 122, loss = 0.0013612586772069335
iteration 123, loss = 0.0008238983573392034
iteration 124, loss = 0.0011639223666861653
iteration 125, loss = 0.000855579855851829
iteration 126, loss = 0.0007393350824713707
iteration 127, loss = 0.000720345473382622
iteration 128, loss = 0.0013206432340666652
iteration 129, loss = 0.0009108783560805023
iteration 130, loss = 0.001704336842522025
iteration 131, loss = 0.0015209732810035348
iteration 132, loss = 0.001119147171266377
iteration 133, loss = 0.0016009635291993618
iteration 134, loss = 0.0008886950672604144
iteration 135, loss = 0.0007522955420427024
iteration 136, loss = 0.0007657638634555042
iteration 137, loss = 0.0007740009459666908
iteration 138, loss = 0.0008188214269466698
iteration 139, loss = 0.0009296612115576863
iteration 140, loss = 0.0007621404947713017
iteration 141, loss = 0.0012334444327279925
iteration 142, loss = 0.0008414632175117731
iteration 143, loss = 0.0007857537129893899
iteration 144, loss = 0.0009645374375395477
iteration 145, loss = 0.0011257724836468697
iteration 146, loss = 0.0008571146754547954
iteration 147, loss = 0.0007813469273969531
iteration 148, loss = 0.0015235880855470896
iteration 149, loss = 0.0006876192055642605
iteration 150, loss = 0.0008551220525987446
iteration 151, loss = 0.0007277738768607378
iteration 152, loss = 0.000944041064940393
iteration 153, loss = 0.0008879675297066569
iteration 154, loss = 0.001055501401424408
iteration 155, loss = 0.00098802475258708
iteration 156, loss = 0.0008062422275543213
iteration 157, loss = 0.0014731872361153364
iteration 158, loss = 0.0006571800331585109
iteration 159, loss = 0.0007373038679361343
iteration 160, loss = 0.000908737420104444
iteration 161, loss = 0.0011420291848480701
iteration 162, loss = 0.0008639567531645298
iteration 163, loss = 0.000938511046115309
iteration 164, loss = 0.0008482104167342186
iteration 165, loss = 0.0010650517651811242
iteration 166, loss = 0.0016345549374818802
iteration 167, loss = 0.0009751321631483734
iteration 168, loss = 0.0011899862438440323
iteration 169, loss = 0.0008324068039655685
iteration 170, loss = 0.001257541822269559
iteration 171, loss = 0.0006418981938622892
iteration 172, loss = 0.001266396138817072
iteration 173, loss = 0.0006403548177331686
iteration 174, loss = 0.0009152115089818835
iteration 175, loss = 0.0006659989012405276
iteration 176, loss = 0.0014502034755423665
iteration 177, loss = 0.0006794746150262654
iteration 178, loss = 0.0010277655674144626
iteration 179, loss = 0.0011548709589987993
iteration 180, loss = 0.0010695726377889514
iteration 181, loss = 0.0009041403536684811
iteration 182, loss = 0.0012439875863492489
iteration 183, loss = 0.000758790469262749
iteration 184, loss = 0.000830736942589283
iteration 185, loss = 0.0017447613645344973
iteration 186, loss = 0.0007538212230429053
iteration 187, loss = 0.0011128823971375823
iteration 188, loss = 0.0010588716249912977
iteration 189, loss = 0.001255171955563128
iteration 190, loss = 0.0015082306927070022
iteration 191, loss = 0.0008352994336746633
iteration 192, loss = 0.0007938952185213566
iteration 193, loss = 0.0008550258935429156
iteration 194, loss = 0.0008712999988347292
iteration 195, loss = 0.0007845873478800058
iteration 196, loss = 0.001507116132415831
iteration 197, loss = 0.0008172385860234499
iteration 198, loss = 0.00075378647306934
iteration 199, loss = 0.000791687227319926
iteration 200, loss = 0.0009103773045353591
iteration 201, loss = 0.0008708919631317258
iteration 202, loss = 0.0012658750638365746
iteration 203, loss = 0.0007083749515004456
iteration 204, loss = 0.0007187265437096357
iteration 205, loss = 0.0006762101547792554
iteration 206, loss = 0.0014772923896089196
iteration 207, loss = 0.0008468251908197999
iteration 208, loss = 0.0010268378537148237
iteration 209, loss = 0.0014843223616480827
iteration 210, loss = 0.0008906304137781262
iteration 211, loss = 0.0008650401723571122
iteration 212, loss = 0.0008524329750798643
iteration 213, loss = 0.0007188612944446504
iteration 214, loss = 0.0007655664812773466
iteration 215, loss = 0.0006687426939606667
iteration 216, loss = 0.0008347770199179649
iteration 217, loss = 0.0008379326900467277
iteration 218, loss = 0.0007428256794810295
iteration 219, loss = 0.0007829478126950562
iteration 220, loss = 0.000778514426201582
iteration 221, loss = 0.0008542997529730201
iteration 222, loss = 0.0014323393115773797
iteration 223, loss = 0.0007643770077265799
iteration 224, loss = 0.0008636694401502609
iteration 225, loss = 0.0008441908285021782
iteration 226, loss = 0.0007335635600611567
iteration 227, loss = 0.0009901616722345352
iteration 228, loss = 0.0008170583751052618
iteration 229, loss = 0.000918689533136785
iteration 230, loss = 0.0008540964918211102
iteration 231, loss = 0.0007506398833356798
iteration 232, loss = 0.0008769901469349861
iteration 233, loss = 0.0016599864466115832
iteration 234, loss = 0.0008232775726355612
iteration 235, loss = 0.0008996938704513013
iteration 236, loss = 0.0007785808411426842
iteration 237, loss = 0.0007045628735795617
iteration 238, loss = 0.0011212400859221816
iteration 239, loss = 0.0014680320164188743
iteration 240, loss = 0.0009535556891933084
iteration 241, loss = 0.0007785250782035291
iteration 242, loss = 0.0008984189480543137
iteration 243, loss = 0.0007517575868405402
iteration 244, loss = 0.0017699840245768428
iteration 245, loss = 0.0010184987913817167
iteration 246, loss = 0.000834415783174336
iteration 247, loss = 0.0009912019595503807
iteration 248, loss = 0.0008288648095913231
iteration 249, loss = 0.000840376946143806
iteration 250, loss = 0.0008515733061358333
iteration 251, loss = 0.000921405793633312
iteration 252, loss = 0.0009355420479550958
iteration 253, loss = 0.0008779391064308584
iteration 254, loss = 0.0009141619084402919
iteration 255, loss = 0.0014311501290649176
iteration 256, loss = 0.0007302825106307864
iteration 257, loss = 0.0007055668393149972
iteration 258, loss = 0.0008964654407463968
iteration 259, loss = 0.0008294712752103806
iteration 260, loss = 0.0009647028055042028
iteration 261, loss = 0.0008095352095551789
iteration 262, loss = 0.0008411876042373478
iteration 263, loss = 0.0008307236130349338
iteration 264, loss = 0.0010773675749078393
iteration 265, loss = 0.000955606228671968
iteration 266, loss = 0.0007394617423415184
iteration 267, loss = 0.0009375073132105172
iteration 268, loss = 0.000969601096585393
iteration 269, loss = 0.0007395415450446308
iteration 270, loss = 0.0008116030367091298
iteration 271, loss = 0.0008586709736846387
iteration 272, loss = 0.0013296285178512335
iteration 273, loss = 0.0008930076146498322
iteration 274, loss = 0.0009743648115545511
iteration 275, loss = 0.0008311871206387877
iteration 276, loss = 0.000679188349749893
iteration 277, loss = 0.0008997484110295773
iteration 278, loss = 0.0007913983426988125
iteration 279, loss = 0.0009618479525670409
iteration 280, loss = 0.0007545736152678728
iteration 281, loss = 0.0008289578254334629
iteration 282, loss = 0.0020970089826732874
iteration 283, loss = 0.000755924847908318
iteration 284, loss = 0.000587278394959867
iteration 285, loss = 0.0018745650304481387
iteration 286, loss = 0.0008446413557976484
iteration 287, loss = 0.0009571300470270216
iteration 288, loss = 0.000938043522182852
iteration 289, loss = 0.0007955899927765131
iteration 290, loss = 0.0016754713142290711
iteration 291, loss = 0.001095200888812542
iteration 292, loss = 0.0008669791277498007
iteration 293, loss = 0.000823058420792222
iteration 294, loss = 0.000631125527434051
iteration 295, loss = 0.000884055276401341
iteration 296, loss = 0.001173078315332532
iteration 297, loss = 0.0013577404897660017
iteration 298, loss = 0.0007987833814695477
iteration 299, loss = 0.0007000137120485306
iteration 300, loss = 0.0008908901363611221
iteration 1, loss = 0.0009608143009245396
iteration 2, loss = 0.0008045346476137638
iteration 3, loss = 0.001112495083361864
iteration 4, loss = 0.0012476603733375669
iteration 5, loss = 0.0008775857859291136
iteration 6, loss = 0.0008261240436695516
iteration 7, loss = 0.0007040261989459395
iteration 8, loss = 0.0007923132507130504
iteration 9, loss = 0.000913792522624135
iteration 10, loss = 0.0011387703707441688
iteration 11, loss = 0.0009692133753560483
iteration 12, loss = 0.0007588749285787344
iteration 13, loss = 0.0007265738095156848
iteration 14, loss = 0.0006896336562931538
iteration 15, loss = 0.0007272589136846364
iteration 16, loss = 0.0007367654470726848
iteration 17, loss = 0.0008016608189791441
iteration 18, loss = 0.0017561696004122496
iteration 19, loss = 0.0009240072686225176
iteration 20, loss = 0.0006654153694398701
iteration 21, loss = 0.0009771785698831081
iteration 22, loss = 0.001568659907206893
iteration 23, loss = 0.0013290941715240479
iteration 24, loss = 0.0008532661013305187
iteration 25, loss = 0.0008027231087908149
iteration 26, loss = 0.001776089076884091
iteration 27, loss = 0.0016208253800868988
iteration 28, loss = 0.0009175582090392709
iteration 29, loss = 0.0009280344238504767
iteration 30, loss = 0.0008376965997740626
iteration 31, loss = 0.0008487484883517027
iteration 32, loss = 0.0008089275797829032
iteration 33, loss = 0.0010409116512164474
iteration 34, loss = 0.0008317009196616709
iteration 35, loss = 0.000693761685397476
iteration 36, loss = 0.0007514369790442288
iteration 37, loss = 0.000755497079808265
iteration 38, loss = 0.0010316596599295735
iteration 39, loss = 0.0007328020874410868
iteration 40, loss = 0.0009795331861823797
iteration 41, loss = 0.0007479655323550105
iteration 42, loss = 0.0009613673901185393
iteration 43, loss = 0.000769614358432591
iteration 44, loss = 0.001087810262106359
iteration 45, loss = 0.0007505241082981229
iteration 46, loss = 0.0009569041430950165
iteration 47, loss = 0.0008510607876814902
iteration 48, loss = 0.0012769654858857393
iteration 49, loss = 0.0007819443708285689
iteration 50, loss = 0.0010116714984178543
iteration 51, loss = 0.0006710732122883201
iteration 52, loss = 0.0007243188447318971
iteration 53, loss = 0.0015418132534250617
iteration 54, loss = 0.0007614335045218468
iteration 55, loss = 0.0009821978164836764
iteration 56, loss = 0.0008919686661101878
iteration 57, loss = 0.0006967379013076425
iteration 58, loss = 0.0009155590669251978
iteration 59, loss = 0.0007919259951449931
iteration 60, loss = 0.0017508661840111017
iteration 61, loss = 0.0007521920488215983
iteration 62, loss = 0.001051335595548153
iteration 63, loss = 0.0008873510523699224
iteration 64, loss = 0.0014211692614480853
iteration 65, loss = 0.0007219855324365199
iteration 66, loss = 0.0008727563545107841
iteration 67, loss = 0.0008373292512260377
iteration 68, loss = 0.0008886151481419802
iteration 69, loss = 0.0008496715454384685
iteration 70, loss = 0.001271801651455462
iteration 71, loss = 0.0007144187111407518
iteration 72, loss = 0.0008622719324193895
iteration 73, loss = 0.0014604692114517093
iteration 74, loss = 0.000653606781270355
iteration 75, loss = 0.0011512931669130921
iteration 76, loss = 0.0007699716370552778
iteration 77, loss = 0.001048631384037435
iteration 78, loss = 0.0008536570821888745
iteration 79, loss = 0.0008921870030462742
iteration 80, loss = 0.0008231748943217099
iteration 81, loss = 0.000791155151091516
iteration 82, loss = 0.0007628546445630491
iteration 83, loss = 0.00088178098667413
iteration 84, loss = 0.0008075704099610448
iteration 85, loss = 0.0005816682823933661
iteration 86, loss = 0.0006773436907678843
iteration 87, loss = 0.0008035321370698512
iteration 88, loss = 0.0007977085188031197
iteration 89, loss = 0.0007114416803233325
iteration 90, loss = 0.0008955142693594098
iteration 91, loss = 0.0015167114324867725
iteration 92, loss = 0.0006028831121511757
iteration 93, loss = 0.0006977851735427976
iteration 94, loss = 0.0008179110009223223
iteration 95, loss = 0.0007317549316212535
iteration 96, loss = 0.0008148818742483854
iteration 97, loss = 0.0007737938431091607
iteration 98, loss = 0.000928536697756499
iteration 99, loss = 0.0008621093584224582
iteration 100, loss = 0.0008235385175794363
iteration 101, loss = 0.0009986584773287177
iteration 102, loss = 0.0011744210496544838
iteration 103, loss = 0.001273574074730277
iteration 104, loss = 0.0006744363927282393
iteration 105, loss = 0.0006809839978814125
iteration 106, loss = 0.0007824238273315132
iteration 107, loss = 0.0007071621948853135
iteration 108, loss = 0.0007617877563461661
iteration 109, loss = 0.0015062798047438264
iteration 110, loss = 0.0009269457077607512
iteration 111, loss = 0.0008245772332884371
iteration 112, loss = 0.0007769373478367925
iteration 113, loss = 0.0008435526979155838
iteration 114, loss = 0.0010921051725745201
iteration 115, loss = 0.0009787025628611445
iteration 116, loss = 0.000759136164560914
iteration 117, loss = 0.0009084055200219154
iteration 118, loss = 0.0009056851849891245
iteration 119, loss = 0.0011888074222952127
iteration 120, loss = 0.0009670578292571008
iteration 121, loss = 0.000891769421286881
iteration 122, loss = 0.0009037060081027448
iteration 123, loss = 0.0007661923882551491
iteration 124, loss = 0.0007205986767075956
iteration 125, loss = 0.000793841783888638
iteration 126, loss = 0.0008208936778828502
iteration 127, loss = 0.001181596890091896
iteration 128, loss = 0.0006233242456801236
iteration 129, loss = 0.0008156073745340109
iteration 130, loss = 0.0009708361467346549
iteration 131, loss = 0.0009691116865724325
iteration 132, loss = 0.000801366928499192
iteration 133, loss = 0.0011084354482591152
iteration 134, loss = 0.001724293571896851
iteration 135, loss = 0.0009101976174861193
iteration 136, loss = 0.0008786239777691662
iteration 137, loss = 0.001699372660368681
iteration 138, loss = 0.0008743127109482884
iteration 139, loss = 0.0008158123819157481
iteration 140, loss = 0.0008770997519604862
iteration 141, loss = 0.0007246255991049111
iteration 142, loss = 0.0017254813574254513
iteration 143, loss = 0.001500614918768406
iteration 144, loss = 0.0009045085171237588
iteration 145, loss = 0.0007602969417348504
iteration 146, loss = 0.0015790023608133197
iteration 147, loss = 0.0010393464472144842
iteration 148, loss = 0.0008205100311897695
iteration 149, loss = 0.0012627660762518644
iteration 150, loss = 0.0010602747788652778
iteration 151, loss = 0.001538075041025877
iteration 152, loss = 0.0011856911005452275
iteration 153, loss = 0.001131261931732297
iteration 154, loss = 0.0008522446150891483
iteration 155, loss = 0.0010485086822882295
iteration 156, loss = 0.0006765807047486305
iteration 157, loss = 0.0007310643559321761
iteration 158, loss = 0.0014321639901027083
iteration 159, loss = 0.0009826372843235731
iteration 160, loss = 0.0007671269122511148
iteration 161, loss = 0.0007588733569718897
iteration 162, loss = 0.0011069448664784431
iteration 163, loss = 0.0008084484143182635
iteration 164, loss = 0.0007045484380796552
iteration 165, loss = 0.0011082625715062022
iteration 166, loss = 0.001060020294971764
iteration 167, loss = 0.0010011703707277775
iteration 168, loss = 0.0010098065249621868
iteration 169, loss = 0.000705971266143024
iteration 170, loss = 0.0009182504145428538
iteration 171, loss = 0.0007471996359527111
iteration 172, loss = 0.001456019701436162
iteration 173, loss = 0.0012133087730035186
iteration 174, loss = 0.0008176156552508473
iteration 175, loss = 0.0010053038131445646
iteration 176, loss = 0.0017924545099958777
iteration 177, loss = 0.0011708787642419338
iteration 178, loss = 0.0008240450988523662
iteration 179, loss = 0.0008824070682749152
iteration 180, loss = 0.0009755092323757708
iteration 181, loss = 0.0010198896052315831
iteration 182, loss = 0.0008467500447295606
iteration 183, loss = 0.000851161777973175
iteration 184, loss = 0.0014286715304479003
iteration 185, loss = 0.0007399014430120587
iteration 186, loss = 0.000750469509512186
iteration 187, loss = 0.0007291967049241066
iteration 188, loss = 0.000964146398473531
iteration 189, loss = 0.0008284102659672499
iteration 190, loss = 0.0017315719742327929
iteration 191, loss = 0.000775399967096746
iteration 192, loss = 0.0012057741405442357
iteration 193, loss = 0.0007164122071117163
iteration 194, loss = 0.0010766610503196716
iteration 195, loss = 0.0011143642477691174
iteration 196, loss = 0.0006983070052228868
iteration 197, loss = 0.0009361234842799604
iteration 198, loss = 0.0006854240782558918
iteration 199, loss = 0.0010432437993586063
iteration 200, loss = 0.0008111955830827355
iteration 201, loss = 0.0008268693927675486
iteration 202, loss = 0.0007476467289961874
iteration 203, loss = 0.0008951736381277442
iteration 204, loss = 0.000927683140616864
iteration 205, loss = 0.0012865656754001975
iteration 206, loss = 0.0007232977077364922
iteration 207, loss = 0.0007595946080982685
iteration 208, loss = 0.0011507285526022315
iteration 209, loss = 0.0015528969233855605
iteration 210, loss = 0.0010396031429991126
iteration 211, loss = 0.00101655803155154
iteration 212, loss = 0.00104127440135926
iteration 213, loss = 0.0008214632980525494
iteration 214, loss = 0.0009208918199874461
iteration 215, loss = 0.0013096516486257315
iteration 216, loss = 0.000751122017391026
iteration 217, loss = 0.0008321875939145684
iteration 218, loss = 0.0007521130610257387
iteration 219, loss = 0.0006302815745584667
iteration 220, loss = 0.0014685479691252112
iteration 221, loss = 0.0008378849597647786
iteration 222, loss = 0.0007421335903927684
iteration 223, loss = 0.0007948451675474644
iteration 224, loss = 0.0013440133770927787
iteration 225, loss = 0.001022167969495058
iteration 226, loss = 0.0010601792018860579
iteration 227, loss = 0.0008494986686855555
iteration 228, loss = 0.0007957937777973711
iteration 229, loss = 0.0007042370270937681
iteration 230, loss = 0.000869423383846879
iteration 231, loss = 0.0007178359664976597
iteration 232, loss = 0.0008220422314479947
iteration 233, loss = 0.0012944650370627642
iteration 234, loss = 0.0007372151594609022
iteration 235, loss = 0.000842210603877902
iteration 236, loss = 0.0008701903279870749
iteration 237, loss = 0.00080118328332901
iteration 238, loss = 0.0007362099131569266
iteration 239, loss = 0.0008298493339680135
iteration 240, loss = 0.0009156384039670229
iteration 241, loss = 0.0008483754354529083
iteration 242, loss = 0.0007453084690496325
iteration 243, loss = 0.0012570125982165337
iteration 244, loss = 0.0008555972017347813
iteration 245, loss = 0.0007964477990753949
iteration 246, loss = 0.0008755457820370793
iteration 247, loss = 0.000997844384983182
iteration 248, loss = 0.0009085264755412936
iteration 249, loss = 0.0008823212701827288
iteration 250, loss = 0.0010175295174121857
iteration 251, loss = 0.0014409658033400774
iteration 252, loss = 0.0011116781970486045
iteration 253, loss = 0.0020539150573313236
iteration 254, loss = 0.001185365952551365
iteration 255, loss = 0.0010201078839600086
iteration 256, loss = 0.0008199464064091444
iteration 257, loss = 0.0006958399317227304
iteration 258, loss = 0.0014353636652231216
iteration 259, loss = 0.0011352309957146645
iteration 260, loss = 0.000941110251005739
iteration 261, loss = 0.0008442337275482714
iteration 262, loss = 0.0007296092226170003
iteration 263, loss = 0.000664597493596375
iteration 264, loss = 0.0008372064330615103
iteration 265, loss = 0.0011891911271959543
iteration 266, loss = 0.0014977907994762063
iteration 267, loss = 0.000910520029719919
iteration 268, loss = 0.0007243476575240493
iteration 269, loss = 0.000867406080942601
iteration 270, loss = 0.00098699692171067
iteration 271, loss = 0.001954890787601471
iteration 272, loss = 0.0008804736426100135
iteration 273, loss = 0.0007181544206105173
iteration 274, loss = 0.0014435863122344017
iteration 275, loss = 0.0013159599620848894
iteration 276, loss = 0.0015012257499620318
iteration 277, loss = 0.0010991542367264628
iteration 278, loss = 0.0008242814219556749
iteration 279, loss = 0.0008005559793673456
iteration 280, loss = 0.0011049145832657814
iteration 281, loss = 0.0015570290852338076
iteration 282, loss = 0.0009440508438274264
iteration 283, loss = 0.0011605210602283478
iteration 284, loss = 0.0009245059918612242
iteration 285, loss = 0.0008424450643360615
iteration 286, loss = 0.001098934910260141
iteration 287, loss = 0.0011713774874806404
iteration 288, loss = 0.0010393410921096802
iteration 289, loss = 0.0008318126201629639
iteration 290, loss = 0.0008055698126554489
iteration 291, loss = 0.0008230453822761774
iteration 292, loss = 0.0009333945927210152
iteration 293, loss = 0.0015958599979057908
iteration 294, loss = 0.0008289599791169167
iteration 295, loss = 0.001123202731832862
iteration 296, loss = 0.0006987363449297845
iteration 297, loss = 0.0018126309150829911
iteration 298, loss = 0.0011141522554680705
iteration 299, loss = 0.0011992785148322582
iteration 300, loss = 0.001176739577203989
iteration 1, loss = 0.0008131563081406057
iteration 2, loss = 0.0010296219261363149
iteration 3, loss = 0.0012260184157639742
iteration 4, loss = 0.0012357120867818594
iteration 5, loss = 0.001573849469423294
iteration 6, loss = 0.0015635316958650947
iteration 7, loss = 0.0008690669201314449
iteration 8, loss = 0.0006975804571993649
iteration 9, loss = 0.0010704481974244118
iteration 10, loss = 0.0008870625169947743
iteration 11, loss = 0.0008716872544027865
iteration 12, loss = 0.0008694803691469133
iteration 13, loss = 0.0010474438313394785
iteration 14, loss = 0.0009573904681019485
iteration 15, loss = 0.0012346913572400808
iteration 16, loss = 0.001005098456516862
iteration 17, loss = 0.001437854254618287
iteration 18, loss = 0.0007702229195274413
iteration 19, loss = 0.0011526186717674136
iteration 20, loss = 0.0008655186975374818
iteration 21, loss = 0.0009575966978445649
iteration 22, loss = 0.0007932728040032089
iteration 23, loss = 0.000797250191681087
iteration 24, loss = 0.0009942437754943967
iteration 25, loss = 0.0008363378583453596
iteration 26, loss = 0.0009146492229774594
iteration 27, loss = 0.0007799452869221568
iteration 28, loss = 0.0008564043673686683
iteration 29, loss = 0.0008537251851521432
iteration 30, loss = 0.0021628192625939846
iteration 31, loss = 0.0006789047038182616
iteration 32, loss = 0.0017547816969454288
iteration 33, loss = 0.0009706933051347733
iteration 34, loss = 0.001189013826660812
iteration 35, loss = 0.0008540863636881113
iteration 36, loss = 0.0015184030635282397
iteration 37, loss = 0.0008132478105835617
iteration 38, loss = 0.0007609418244101107
iteration 39, loss = 0.0008779910858720541
iteration 40, loss = 0.000790818128734827
iteration 41, loss = 0.001182257430627942
iteration 42, loss = 0.0010376076679676771
iteration 43, loss = 0.0008288264507427812
iteration 44, loss = 0.0007797373691573739
iteration 45, loss = 0.0011575935641303658
iteration 46, loss = 0.0007323092431761324
iteration 47, loss = 0.0009277760982513428
iteration 48, loss = 0.0009042720776051283
iteration 49, loss = 0.0007537897909060121
iteration 50, loss = 0.0009963493794202805
iteration 51, loss = 0.0008449576562270522
iteration 52, loss = 0.0008517848909832537
iteration 53, loss = 0.0009764378773979843
iteration 54, loss = 0.000821444671601057
iteration 55, loss = 0.0008049083407968283
iteration 56, loss = 0.0010881099151447415
iteration 57, loss = 0.0010945152025669813
iteration 58, loss = 0.0012839272385463119
iteration 59, loss = 0.0008471511537209153
iteration 60, loss = 0.0010001820046454668
iteration 61, loss = 0.0007580243400298059
iteration 62, loss = 0.0010857863817363977
iteration 63, loss = 0.0016057532047852874
iteration 64, loss = 0.0007655139779672027
iteration 65, loss = 0.0007141628884710371
iteration 66, loss = 0.0007986794225871563
iteration 67, loss = 0.0009915822884067893
iteration 68, loss = 0.0006239941576495767
iteration 69, loss = 0.000870508432853967
iteration 70, loss = 0.0008995761163532734
iteration 71, loss = 0.0008246159995906055
iteration 72, loss = 0.0012880363501608372
iteration 73, loss = 0.0013394843554124236
iteration 74, loss = 0.001387252239510417
iteration 75, loss = 0.0007743635214865208
iteration 76, loss = 0.0007247391040436924
iteration 77, loss = 0.0007565093692392111
iteration 78, loss = 0.0008902368717826903
iteration 79, loss = 0.0017618362326174974
iteration 80, loss = 0.0011247650254517794
iteration 81, loss = 0.0006577912718057632
iteration 82, loss = 0.0008590796496719122
iteration 83, loss = 0.0009336636867374182
iteration 84, loss = 0.0013176528736948967
iteration 85, loss = 0.0017759072361513972
iteration 86, loss = 0.0010776515118777752
iteration 87, loss = 0.0008752318099141121
iteration 88, loss = 0.0007552590104751289
iteration 89, loss = 0.0008592540980316699
iteration 90, loss = 0.0009380069677717984
iteration 91, loss = 0.0007847845554351807
iteration 92, loss = 0.001126177841797471
iteration 93, loss = 0.000924647378269583
iteration 94, loss = 0.0008215889101848006
iteration 95, loss = 0.0008429554291069508
iteration 96, loss = 0.0011398268397897482
iteration 97, loss = 0.0007777644786983728
iteration 98, loss = 0.0009109360398724675
iteration 99, loss = 0.0011954244691878557
iteration 100, loss = 0.0010460872435942292
iteration 101, loss = 0.0007698341505602002
iteration 102, loss = 0.0008697687881067395
iteration 103, loss = 0.0007232778007164598
iteration 104, loss = 0.0010869300458580256
iteration 105, loss = 0.0011902217520400882
iteration 106, loss = 0.0008518350077793002
iteration 107, loss = 0.000807426287792623
iteration 108, loss = 0.0008933967328630388
iteration 109, loss = 0.0009633272420614958
iteration 110, loss = 0.0009670541621744633
iteration 111, loss = 0.0009748551528900862
iteration 112, loss = 0.0017924344865605235
iteration 113, loss = 0.0007229478796944022
iteration 114, loss = 0.0007862118654884398
iteration 115, loss = 0.001412950805388391
iteration 116, loss = 0.0008111778879538178
iteration 117, loss = 0.0013735913671553135
iteration 118, loss = 0.0007965457625687122
iteration 119, loss = 0.0010003572097048163
iteration 120, loss = 0.0011783386580646038
iteration 121, loss = 0.001611551851965487
iteration 122, loss = 0.000750136561691761
iteration 123, loss = 0.0007242307183332741
iteration 124, loss = 0.0009447954362258315
iteration 125, loss = 0.000790664809755981
iteration 126, loss = 0.0009511567186564207
iteration 127, loss = 0.0008150271605700254
iteration 128, loss = 0.0007416796288453043
iteration 129, loss = 0.0009472980163991451
iteration 130, loss = 0.0008904306450858712
iteration 131, loss = 0.0008371416479349136
iteration 132, loss = 0.000956236501224339
iteration 133, loss = 0.0009837234392762184
iteration 134, loss = 0.0007908328552730381
iteration 135, loss = 0.0011602953309193254
iteration 136, loss = 0.0013007359812036157
iteration 137, loss = 0.0007726216572336853
iteration 138, loss = 0.0012516514398157597
iteration 139, loss = 0.0007967249839566648
iteration 140, loss = 0.0008036175277084112
iteration 141, loss = 0.000617066805716604
iteration 142, loss = 0.001244960119947791
iteration 143, loss = 0.0007305248873308301
iteration 144, loss = 0.000752157240640372
iteration 145, loss = 0.0006899313302710652
iteration 146, loss = 0.000955394352786243
iteration 147, loss = 0.0011445707641541958
iteration 148, loss = 0.0012556519359350204
iteration 149, loss = 0.0008644093759357929
iteration 150, loss = 0.0009131744154728949
iteration 151, loss = 0.0008314723381772637
iteration 152, loss = 0.0007686484605073929
iteration 153, loss = 0.001339752459898591
iteration 154, loss = 0.0007401738548651338
iteration 155, loss = 0.001342214411124587
iteration 156, loss = 0.0008153915987350047
iteration 157, loss = 0.0008167863124981523
iteration 158, loss = 0.0014275179710239172
iteration 159, loss = 0.0008500297553837299
iteration 160, loss = 0.0009257106576114893
iteration 161, loss = 0.00085104675963521
iteration 162, loss = 0.000870651041623205
iteration 163, loss = 0.00182826048694551
iteration 164, loss = 0.0016708901384845376
iteration 165, loss = 0.0008689158130437136
iteration 166, loss = 0.0010419150348752737
iteration 167, loss = 0.0009317223448306322
iteration 168, loss = 0.0007790977251715958
iteration 169, loss = 0.001956284511834383
iteration 170, loss = 0.000978873809799552
iteration 171, loss = 0.0011295669246464968
iteration 172, loss = 0.000656803953461349
iteration 173, loss = 0.0007783849141560495
iteration 174, loss = 0.0006406275206245482
iteration 175, loss = 0.0010488171828910708
iteration 176, loss = 0.000931853661313653
iteration 177, loss = 0.0009170476114377379
iteration 178, loss = 0.0008221816970035434
iteration 179, loss = 0.0015011028153821826
iteration 180, loss = 0.0008326877723447978
iteration 181, loss = 0.0007625406142324209
iteration 182, loss = 0.0008234393317252398
iteration 183, loss = 0.0008105001179501414
iteration 184, loss = 0.0009685477125458419
iteration 185, loss = 0.0017085690051317215
iteration 186, loss = 0.0015458038542419672
iteration 187, loss = 0.0014168921625241637
iteration 188, loss = 0.0007478671614080667
iteration 189, loss = 0.0007451592828147113
iteration 190, loss = 0.0010214834474027157
iteration 191, loss = 0.0006190694984979928
iteration 192, loss = 0.0008429056033492088
iteration 193, loss = 0.0008781932410784066
iteration 194, loss = 0.0008231814717873931
iteration 195, loss = 0.0010381313040852547
iteration 196, loss = 0.0008660841849632561
iteration 197, loss = 0.0007655884837731719
iteration 198, loss = 0.0012877971166744828
iteration 199, loss = 0.0007895679445937276
iteration 200, loss = 0.0008064426365308464
iteration 201, loss = 0.0008454509079456329
iteration 202, loss = 0.001567894360050559
iteration 203, loss = 0.0017382082296535373
iteration 204, loss = 0.0010252713691443205
iteration 205, loss = 0.0009686981211416423
iteration 206, loss = 0.0008276563603430986
iteration 207, loss = 0.0008074946235865355
iteration 208, loss = 0.001704534632153809
iteration 209, loss = 0.0007684968295507133
iteration 210, loss = 0.0009045709739439189
iteration 211, loss = 0.0007645031437277794
iteration 212, loss = 0.0007909152773208916
iteration 213, loss = 0.000805964635219425
iteration 214, loss = 0.0007902503712102771
iteration 215, loss = 0.0009547604713588953
iteration 216, loss = 0.001074973028153181
iteration 217, loss = 0.0009238432976417243
iteration 218, loss = 0.0011551656061783433
iteration 219, loss = 0.0009809882612898946
iteration 220, loss = 0.0009007927146740258
iteration 221, loss = 0.000936377327889204
iteration 222, loss = 0.0008076491649262607
iteration 223, loss = 0.0009051044471561909
iteration 224, loss = 0.0006470120279118419
iteration 225, loss = 0.0012616318417713046
iteration 226, loss = 0.0008803823147900403
iteration 227, loss = 0.0008381115039810538
iteration 228, loss = 0.0011617070995271206
iteration 229, loss = 0.0008990508504211903
iteration 230, loss = 0.0007812026306055486
iteration 231, loss = 0.0006819716072641313
iteration 232, loss = 0.0007046058890409768
iteration 233, loss = 0.0010415095603093505
iteration 234, loss = 0.0007033991860225797
iteration 235, loss = 0.0009386593010276556
iteration 236, loss = 0.0007651090272702277
iteration 237, loss = 0.0009521585307084024
iteration 238, loss = 0.0007045792881399393
iteration 239, loss = 0.0007473176228813827
iteration 240, loss = 0.0008581870933994651
iteration 241, loss = 0.0015648197149857879
iteration 242, loss = 0.0008304643561132252
iteration 243, loss = 0.0007916302420198917
iteration 244, loss = 0.0008904701098799706
iteration 245, loss = 0.0009635512251406908
iteration 246, loss = 0.0007452951394952834
iteration 247, loss = 0.001056439708918333
iteration 248, loss = 0.0013483124785125256
iteration 249, loss = 0.001340062590315938
iteration 250, loss = 0.0007630196632817388
iteration 251, loss = 0.000993324094451964
iteration 252, loss = 0.00119281024672091
iteration 253, loss = 0.0008388235000893474
iteration 254, loss = 0.0007762148743495345
iteration 255, loss = 0.0008813266758807003
iteration 256, loss = 0.0007918750052340329
iteration 257, loss = 0.000810324854683131
iteration 258, loss = 0.0009355841903015971
iteration 259, loss = 0.000774209969677031
iteration 260, loss = 0.0010696981335058808
iteration 261, loss = 0.0009642681106925011
iteration 262, loss = 0.0008493287023156881
iteration 263, loss = 0.0008951142663136125
iteration 264, loss = 0.0006374145741574466
iteration 265, loss = 0.0006817174726165831
iteration 266, loss = 0.0009546364890411496
iteration 267, loss = 0.0011515902588143945
iteration 268, loss = 0.0007874300936236978
iteration 269, loss = 0.0006882140878587961
iteration 270, loss = 0.0009910149965435266
iteration 271, loss = 0.0014061643742024899
iteration 272, loss = 0.0008534413645975292
iteration 273, loss = 0.0006533629493787885
iteration 274, loss = 0.0008148305932991207
iteration 275, loss = 0.001658870023675263
iteration 276, loss = 0.0008410552982240915
iteration 277, loss = 0.0007978036301210523
iteration 278, loss = 0.0007867095991969109
iteration 279, loss = 0.0011229257797822356
iteration 280, loss = 0.000952202535700053
iteration 281, loss = 0.0007323472527787089
iteration 282, loss = 0.0010512737790122628
iteration 283, loss = 0.0009306456195190549
iteration 284, loss = 0.0007691156934015453
iteration 285, loss = 0.0008387012057937682
iteration 286, loss = 0.0011536089004948735
iteration 287, loss = 0.0008023458649404347
iteration 288, loss = 0.0010148611618205905
iteration 289, loss = 0.0007653246284462512
iteration 290, loss = 0.0009837645338848233
iteration 291, loss = 0.0007207621820271015
iteration 292, loss = 0.0008594216196797788
iteration 293, loss = 0.000829485128633678
iteration 294, loss = 0.0008174495887942612
iteration 295, loss = 0.0009136199951171875
iteration 296, loss = 0.0008857566863298416
iteration 297, loss = 0.0009747098665684462
iteration 298, loss = 0.000770379148889333
iteration 299, loss = 0.000789976678788662
iteration 300, loss = 0.0013885778607800603
iteration 1, loss = 0.0008645693305879831
iteration 2, loss = 0.0008698201272636652
iteration 3, loss = 0.0007397921290248632
iteration 4, loss = 0.0006801558774895966
iteration 5, loss = 0.001488728215917945
iteration 6, loss = 0.0007713998202234507
iteration 7, loss = 0.0007221114938147366
iteration 8, loss = 0.0008605456096120179
iteration 9, loss = 0.0008506732992827892
iteration 10, loss = 0.0008393010357394814
iteration 11, loss = 0.0008701778715476394
iteration 12, loss = 0.0007146869320422411
iteration 13, loss = 0.0007003224454820156
iteration 14, loss = 0.0009086528443731368
iteration 15, loss = 0.0011977285612374544
iteration 16, loss = 0.0011589768109843135
iteration 17, loss = 0.0007292067166417837
iteration 18, loss = 0.0010268045589327812
iteration 19, loss = 0.001563684199936688
iteration 20, loss = 0.0007911612628959119
iteration 21, loss = 0.001634235610254109
iteration 22, loss = 0.000768104218877852
iteration 23, loss = 0.0007438735920004547
iteration 24, loss = 0.0008258919697254896
iteration 25, loss = 0.001062840805388987
iteration 26, loss = 0.0007352905813604593
iteration 27, loss = 0.000793597602751106
iteration 28, loss = 0.000885808258317411
iteration 29, loss = 0.0008257475565187633
iteration 30, loss = 0.0007731880759820342
iteration 31, loss = 0.0011646886123344302
iteration 32, loss = 0.0008258328307420015
iteration 33, loss = 0.000694716174621135
iteration 34, loss = 0.0007214903016574681
iteration 35, loss = 0.0009776030201464891
iteration 36, loss = 0.0008447123691439629
iteration 37, loss = 0.0008514780784025788
iteration 38, loss = 0.0011050512548536062
iteration 39, loss = 0.00107233424205333
iteration 40, loss = 0.0012991363182663918
iteration 41, loss = 0.001012994209304452
iteration 42, loss = 0.0010260749841108918
iteration 43, loss = 0.0010817943839356303
iteration 44, loss = 0.0011020798701792955
iteration 45, loss = 0.000850935815833509
iteration 46, loss = 0.0010438605677336454
iteration 47, loss = 0.0016065158415585756
iteration 48, loss = 0.000761247705668211
iteration 49, loss = 0.0010633404599502683
iteration 50, loss = 0.001959856366738677
iteration 51, loss = 0.0015125122154131532
iteration 52, loss = 0.0009088648948818445
iteration 53, loss = 0.0008188858046196401
iteration 54, loss = 0.0012531554093584418
iteration 55, loss = 0.0007662506541237235
iteration 56, loss = 0.0006549874669872224
iteration 57, loss = 0.0010192281333729625
iteration 58, loss = 0.0008484309655614197
iteration 59, loss = 0.0009475096012465656
iteration 60, loss = 0.0011662926990538836
iteration 61, loss = 0.0006043085595592856
iteration 62, loss = 0.0007951683946885169
iteration 63, loss = 0.0008419295190833509
iteration 64, loss = 0.0012526251375675201
iteration 65, loss = 0.0008979497943073511
iteration 66, loss = 0.0009383658180013299
iteration 67, loss = 0.0006492121610790491
iteration 68, loss = 0.0010809424566105008
iteration 69, loss = 0.0007058593910187483
iteration 70, loss = 0.0008216363494284451
iteration 71, loss = 0.0011622136225923896
iteration 72, loss = 0.0009231218136847019
iteration 73, loss = 0.000883801665622741
iteration 74, loss = 0.000687075313180685
iteration 75, loss = 0.0009730846504680812
iteration 76, loss = 0.0009092330583371222
iteration 77, loss = 0.0007401881739497185
iteration 78, loss = 0.000905803928617388
iteration 79, loss = 0.0008449768065474927
iteration 80, loss = 0.0008775466121733189
iteration 81, loss = 0.000728827784769237
iteration 82, loss = 0.0008053715573623776
iteration 83, loss = 0.000782652641646564
iteration 84, loss = 0.0006734417984262109
iteration 85, loss = 0.0009120373870246112
iteration 86, loss = 0.0007713641971349716
iteration 87, loss = 0.0014678286388516426
iteration 88, loss = 0.0012179311597719789
iteration 89, loss = 0.0011899744858965278
iteration 90, loss = 0.00089683459373191
iteration 91, loss = 0.0008758389158174396
iteration 92, loss = 0.0007419325993396342
iteration 93, loss = 0.0007190873147919774
iteration 94, loss = 0.0008470666944049299
iteration 95, loss = 0.0006822844152338803
iteration 96, loss = 0.0010956586338579655
iteration 97, loss = 0.0012705472763627768
iteration 98, loss = 0.0009655613685026765
iteration 99, loss = 0.0008084006840363145
iteration 100, loss = 0.0015464765019714832
iteration 101, loss = 0.0009234058088622987
iteration 102, loss = 0.0007855836884118617
iteration 103, loss = 0.0012535935966297984
iteration 104, loss = 0.0010720740538090467
iteration 105, loss = 0.001046696095727384
iteration 106, loss = 0.0010217028902843595
iteration 107, loss = 0.0007602231344208121
iteration 108, loss = 0.0008615595870651305
iteration 109, loss = 0.0008719598990865052
iteration 110, loss = 0.0009053123067133129
iteration 111, loss = 0.0009574234136380255
iteration 112, loss = 0.0007331920205615461
iteration 113, loss = 0.000655381241813302
iteration 114, loss = 0.0008940614061430097
iteration 115, loss = 0.0011490039760246873
iteration 116, loss = 0.0010925376554951072
iteration 117, loss = 0.000778866873588413
iteration 118, loss = 0.0009103718330152333
iteration 119, loss = 0.0014846668345853686
iteration 120, loss = 0.0009005293832160532
iteration 121, loss = 0.0008608764037489891
iteration 122, loss = 0.0008428632281720638
iteration 123, loss = 0.001012058462947607
iteration 124, loss = 0.0007089567952789366
iteration 125, loss = 0.0012305775890126824
iteration 126, loss = 0.0010386789217591286
iteration 127, loss = 0.0009559018071740866
iteration 128, loss = 0.0012594899162650108
iteration 129, loss = 0.0008255181019194424
iteration 130, loss = 0.000781529990490526
iteration 131, loss = 0.0010978159261867404
iteration 132, loss = 0.0017799083143472672
iteration 133, loss = 0.0016656150110065937
iteration 134, loss = 0.0007545138360001147
iteration 135, loss = 0.0016482101054862142
iteration 136, loss = 0.0008409145521000028
iteration 137, loss = 0.0008366957190446556
iteration 138, loss = 0.0010727581102401018
iteration 139, loss = 0.0014698310988023877
iteration 140, loss = 0.0008956994861364365
iteration 141, loss = 0.0010376807767897844
iteration 142, loss = 0.0012014420935884118
iteration 143, loss = 0.0008774934685789049
iteration 144, loss = 0.0006020860164426267
iteration 145, loss = 0.001486385241150856
iteration 146, loss = 0.00102139450609684
iteration 147, loss = 0.0007342378958128393
iteration 148, loss = 0.0014927561860531569
iteration 149, loss = 0.0008172616362571716
iteration 150, loss = 0.0010816020658239722
iteration 151, loss = 0.0014295625733211637
iteration 152, loss = 0.0010462009813636541
iteration 153, loss = 0.0007146390853449702
iteration 154, loss = 0.0016256513772532344
iteration 155, loss = 0.001130542135797441
iteration 156, loss = 0.00081414426676929
iteration 157, loss = 0.0008547622710466385
iteration 158, loss = 0.0009319105884060264
iteration 159, loss = 0.0010752775706350803
iteration 160, loss = 0.0009141405462287366
iteration 161, loss = 0.001863996498286724
iteration 162, loss = 0.0015423260629177094
iteration 163, loss = 0.0007986311102285981
iteration 164, loss = 0.0009812605567276478
iteration 165, loss = 0.0009633250301703811
iteration 166, loss = 0.0008849796140566468
iteration 167, loss = 0.0013459965121001005
iteration 168, loss = 0.000920223887078464
iteration 169, loss = 0.0014580416027456522
iteration 170, loss = 0.001121583511121571
iteration 171, loss = 0.000677164294756949
iteration 172, loss = 0.0007469914853572845
iteration 173, loss = 0.0006153332651592791
iteration 174, loss = 0.000844227964989841
iteration 175, loss = 0.0007077495683915913
iteration 176, loss = 0.0007256595999933779
iteration 177, loss = 0.000782435410656035
iteration 178, loss = 0.0009755013161338866
iteration 179, loss = 0.0009845036547631025
iteration 180, loss = 0.0008675020071677864
iteration 181, loss = 0.0010051409481093287
iteration 182, loss = 0.0009115863358601928
iteration 183, loss = 0.0009141885093413293
iteration 184, loss = 0.0007580362143926322
iteration 185, loss = 0.000761652656365186
iteration 186, loss = 0.0008555949898436666
iteration 187, loss = 0.0007720637950114906
iteration 188, loss = 0.00088542076991871
iteration 189, loss = 0.0006770042236894369
iteration 190, loss = 0.0008893914055079222
iteration 191, loss = 0.0008506457670591772
iteration 192, loss = 0.0008723820792511106
iteration 193, loss = 0.0007412933045998216
iteration 194, loss = 0.0009961193427443504
iteration 195, loss = 0.0007943382952362299
iteration 196, loss = 0.000784221978392452
iteration 197, loss = 0.0009795303922146559
iteration 198, loss = 0.0008833553292788565
iteration 199, loss = 0.0015907769557088614
iteration 200, loss = 0.0008725855150260031
iteration 201, loss = 0.000885140965692699
iteration 202, loss = 0.000721456075552851
iteration 203, loss = 0.0007404748466797173
iteration 204, loss = 0.0007862059283070266
iteration 205, loss = 0.0015200648922473192
iteration 206, loss = 0.0012551031541079283
iteration 207, loss = 0.0008677420555613935
iteration 208, loss = 0.0008678453741595149
iteration 209, loss = 0.001070731203071773
iteration 210, loss = 0.0007801273604854941
iteration 211, loss = 0.0008638589060865343
iteration 212, loss = 0.0008191922097466886
iteration 213, loss = 0.0007737343548797071
iteration 214, loss = 0.0016518430784344673
iteration 215, loss = 0.0017131168860942125
iteration 216, loss = 0.0011555569944903255
iteration 217, loss = 0.0009880667785182595
iteration 218, loss = 0.0009680663933977485
iteration 219, loss = 0.0008169400971382856
iteration 220, loss = 0.001509740948677063
iteration 221, loss = 0.001593815628439188
iteration 222, loss = 0.0009485539048910141
iteration 223, loss = 0.0009100130409933627
iteration 224, loss = 0.0011025560088455677
iteration 225, loss = 0.0012202459620311856
iteration 226, loss = 0.0008513162611052394
iteration 227, loss = 0.0011113559594377875
iteration 228, loss = 0.0007844317005947232
iteration 229, loss = 0.0007981848320923746
iteration 230, loss = 0.0016881591873243451
iteration 231, loss = 0.0008293307619169354
iteration 232, loss = 0.0006470520747825503
iteration 233, loss = 0.0008625808404758573
iteration 234, loss = 0.0009442194132134318
iteration 235, loss = 0.0008415712509304285
iteration 236, loss = 0.0008026316063478589
iteration 237, loss = 0.0015313181793317199
iteration 238, loss = 0.0009711002930998802
iteration 239, loss = 0.0008728038519620895
iteration 240, loss = 0.001017034170217812
iteration 241, loss = 0.0007199570536613464
iteration 242, loss = 0.0017735270084813237
iteration 243, loss = 0.0008557714754715562
iteration 244, loss = 0.0009502553730271757
iteration 245, loss = 0.0008274164865724742
iteration 246, loss = 0.0007602914120070636
iteration 247, loss = 0.0007591220200993121
iteration 248, loss = 0.0007660415139980614
iteration 249, loss = 0.0008812695159576833
iteration 250, loss = 0.000855455466080457
iteration 251, loss = 0.0010361517779529095
iteration 252, loss = 0.000821392226498574
iteration 253, loss = 0.0006090747774578631
iteration 254, loss = 0.0007982630049809813
iteration 255, loss = 0.0007113727042451501
iteration 256, loss = 0.0009669894352555275
iteration 257, loss = 0.0008699396857991815
iteration 258, loss = 0.000990985776297748
iteration 259, loss = 0.0009884878527373075
iteration 260, loss = 0.0012289624428376555
iteration 261, loss = 0.0015479715075343847
iteration 262, loss = 0.0007240269333124161
iteration 263, loss = 0.0006806799792684615
iteration 264, loss = 0.0009264457621611655
iteration 265, loss = 0.0008994186646305025
iteration 266, loss = 0.0008495498914271593
iteration 267, loss = 0.0008661075262352824
iteration 268, loss = 0.0008181451121345162
iteration 269, loss = 0.0013354563852772117
iteration 270, loss = 0.0008369528222829103
iteration 271, loss = 0.0009184698574244976
iteration 272, loss = 0.0010016108863055706
iteration 273, loss = 0.000815821171272546
iteration 274, loss = 0.0008509297040291131
iteration 275, loss = 0.0008103059371933341
iteration 276, loss = 0.0008665205677971244
iteration 277, loss = 0.0009531204705126584
iteration 278, loss = 0.0008155717514455318
iteration 279, loss = 0.0013887013774365187
iteration 280, loss = 0.00150814070366323
iteration 281, loss = 0.0010689713526517153
iteration 282, loss = 0.000731765350792557
iteration 283, loss = 0.0008622909663245082
iteration 284, loss = 0.0011224433546885848
iteration 285, loss = 0.0008838592912070453
iteration 286, loss = 0.0008681535255163908
iteration 287, loss = 0.0007968486752361059
iteration 288, loss = 0.0012119280872866511
iteration 289, loss = 0.0008569535566493869
iteration 290, loss = 0.0008456481155008078
iteration 291, loss = 0.0007264151936396956
iteration 292, loss = 0.0007205840083770454
iteration 293, loss = 0.00098141108173877
iteration 294, loss = 0.0006808698526583612
iteration 295, loss = 0.001158977160230279
iteration 296, loss = 0.0008879826636984944
iteration 297, loss = 0.001055645989254117
iteration 298, loss = 0.0008602295420132577
iteration 299, loss = 0.0007320753065869212
iteration 300, loss = 0.0014855932677164674
iteration 1, loss = 0.0007223511347547174
iteration 2, loss = 0.0007800352177582681
iteration 3, loss = 0.0012886259937658906
iteration 4, loss = 0.000914377102162689
iteration 5, loss = 0.0008710796246305108
iteration 6, loss = 0.000727956066839397
iteration 7, loss = 0.0008873665356077254
iteration 8, loss = 0.001039697788655758
iteration 9, loss = 0.0007248791516758502
iteration 10, loss = 0.0009592787246219814
iteration 11, loss = 0.0008978659170679748
iteration 12, loss = 0.0007105405093170702
iteration 13, loss = 0.0008436821517534554
iteration 14, loss = 0.0008564528543502092
iteration 15, loss = 0.0008862263057380915
iteration 16, loss = 0.000842180335894227
iteration 17, loss = 0.000782580696977675
iteration 18, loss = 0.0009101820178329945
iteration 19, loss = 0.0008637442952021956
iteration 20, loss = 0.002478738548234105
iteration 21, loss = 0.00087312504183501
iteration 22, loss = 0.0006655509350821376
iteration 23, loss = 0.0011091912165284157
iteration 24, loss = 0.001406914321705699
iteration 25, loss = 0.0015571694821119308
iteration 26, loss = 0.0008291019476018846
iteration 27, loss = 0.000758331036195159
iteration 28, loss = 0.0011253171833232045
iteration 29, loss = 0.001081131398677826
iteration 30, loss = 0.0015654051676392555
iteration 31, loss = 0.0015164177166298032
iteration 32, loss = 0.0007928601698949933
iteration 33, loss = 0.00090793491108343
iteration 34, loss = 0.001061336835846305
iteration 35, loss = 0.0009273726027458906
iteration 36, loss = 0.0016130568692460656
iteration 37, loss = 0.0008969022310338914
iteration 38, loss = 0.001158945495262742
iteration 39, loss = 0.0012944184709340334
iteration 40, loss = 0.0009067601058632135
iteration 41, loss = 0.0008551911450922489
iteration 42, loss = 0.0006730749737471342
iteration 43, loss = 0.0008138010161928833
iteration 44, loss = 0.0006780738476663828
iteration 45, loss = 0.0013814853737130761
iteration 46, loss = 0.0007478102343156934
iteration 47, loss = 0.0007411144324578345
iteration 48, loss = 0.0007108389399945736
iteration 49, loss = 0.0009828930487856269
iteration 50, loss = 0.0007237049867399037
iteration 51, loss = 0.0008646291680634022
iteration 52, loss = 0.0007226055604405701
iteration 53, loss = 0.0007818619487807155
iteration 54, loss = 0.00108257622923702
iteration 55, loss = 0.0009700045338831842
iteration 56, loss = 0.0007760485168546438
iteration 57, loss = 0.0008561853901483119
iteration 58, loss = 0.0012734101619571447
iteration 59, loss = 0.0007285851752385497
iteration 60, loss = 0.0012618566397577524
iteration 61, loss = 0.0007843392086215317
iteration 62, loss = 0.0010983721585944295
iteration 63, loss = 0.0016836697468534112
iteration 64, loss = 0.000758409732952714
iteration 65, loss = 0.001172851538285613
iteration 66, loss = 0.0008968840702436864
iteration 67, loss = 0.0008460747776553035
iteration 68, loss = 0.000805551593657583
iteration 69, loss = 0.0010969022987410426
iteration 70, loss = 0.0007360484451055527
iteration 71, loss = 0.0007762782042846084
iteration 72, loss = 0.0008715549483895302
iteration 73, loss = 0.0013492627767845988
iteration 74, loss = 0.0008827042765915394
iteration 75, loss = 0.000937317730858922
iteration 76, loss = 0.0009650458814576268
iteration 77, loss = 0.0009291845490224659
iteration 78, loss = 0.0013655079528689384
iteration 79, loss = 0.0009069257066585124
iteration 80, loss = 0.0007009009132161736
iteration 81, loss = 0.0007240648847073317
iteration 82, loss = 0.0009911402594298124
iteration 83, loss = 0.0007601180113852024
iteration 84, loss = 0.0009634441812522709
iteration 85, loss = 0.0009123793570324779
iteration 86, loss = 0.0008903670241124928
iteration 87, loss = 0.0008053247001953423
iteration 88, loss = 0.0013154259650036693
iteration 89, loss = 0.0007701197755523026
iteration 90, loss = 0.0009481542510911822
iteration 91, loss = 0.0015869388589635491
iteration 92, loss = 0.0011338868644088507
iteration 93, loss = 0.0007770947413519025
iteration 94, loss = 0.0006651507574133575
iteration 95, loss = 0.0007294696988537908
iteration 96, loss = 0.000734798435587436
iteration 97, loss = 0.0008979273261502385
iteration 98, loss = 0.0007852814742363989
iteration 99, loss = 0.0008858244400471449
iteration 100, loss = 0.0015071064699441195
iteration 101, loss = 0.0010222543496638536
iteration 102, loss = 0.0008838811190798879
iteration 103, loss = 0.0009799400577321649
iteration 104, loss = 0.0009996367152780294
iteration 105, loss = 0.0008585108444094658
iteration 106, loss = 0.0007810763781890273
iteration 107, loss = 0.0010013814317062497
iteration 108, loss = 0.0011727133532986045
iteration 109, loss = 0.0007911764550954103
iteration 110, loss = 0.0010555109474807978
iteration 111, loss = 0.0008291197009384632
iteration 112, loss = 0.0008693931740708649
iteration 113, loss = 0.0007091333973221481
iteration 114, loss = 0.0007149116136133671
iteration 115, loss = 0.0008637910359539092
iteration 116, loss = 0.0008882122347131371
iteration 117, loss = 0.0008545327582396567
iteration 118, loss = 0.0007224901346489787
iteration 119, loss = 0.0012313675833866
iteration 120, loss = 0.0011414329055696726
iteration 121, loss = 0.0007147964206524193
iteration 122, loss = 0.0007357171853072941
iteration 123, loss = 0.0013574655167758465
iteration 124, loss = 0.0007252222276292741
iteration 125, loss = 0.0008193053654395044
iteration 126, loss = 0.000885074317920953
iteration 127, loss = 0.0012043466558679938
iteration 128, loss = 0.001857846975326538
iteration 129, loss = 0.0006495777633972466
iteration 130, loss = 0.0008706648950465024
iteration 131, loss = 0.0006803158903494477
iteration 132, loss = 0.0011175019899383187
iteration 133, loss = 0.000817149062640965
iteration 134, loss = 0.0009201111970469356
iteration 135, loss = 0.0007857118034735322
iteration 136, loss = 0.0008392233867198229
iteration 137, loss = 0.0010653811041265726
iteration 138, loss = 0.0007556579075753689
iteration 139, loss = 0.0011501163244247437
iteration 140, loss = 0.0007801265455782413
iteration 141, loss = 0.0011174277169629931
iteration 142, loss = 0.0008127764449454844
iteration 143, loss = 0.0008158411365002394
iteration 144, loss = 0.000823767448309809
iteration 145, loss = 0.0008479575626552105
iteration 146, loss = 0.0008438747026957572
iteration 147, loss = 0.0007233540527522564
iteration 148, loss = 0.0008110555936582386
iteration 149, loss = 0.0007439122418873012
iteration 150, loss = 0.0008365015964955091
iteration 151, loss = 0.0010913270525634289
iteration 152, loss = 0.0007932370062917471
iteration 153, loss = 0.0011688201921060681
iteration 154, loss = 0.0007351458189077675
iteration 155, loss = 0.0011756332824006677
iteration 156, loss = 0.0007321318844333291
iteration 157, loss = 0.0007684659212827682
iteration 158, loss = 0.0011674966663122177
iteration 159, loss = 0.0006917825667187572
iteration 160, loss = 0.0008171881781890988
iteration 161, loss = 0.0016895709559321404
iteration 162, loss = 0.0008292654529213905
iteration 163, loss = 0.0007484747329726815
iteration 164, loss = 0.0008711803238838911
iteration 165, loss = 0.0009036679985001683
iteration 166, loss = 0.0009910129010677338
iteration 167, loss = 0.0009771762415766716
iteration 168, loss = 0.000857433071359992
iteration 169, loss = 0.0016437069280073047
iteration 170, loss = 0.0008861565729603171
iteration 171, loss = 0.0008707729866728187
iteration 172, loss = 0.0009305872954428196
iteration 173, loss = 0.0008649974479340017
iteration 174, loss = 0.0009233328746631742
iteration 175, loss = 0.0008285264484584332
iteration 176, loss = 0.0008658389560878277
iteration 177, loss = 0.001110393786802888
iteration 178, loss = 0.0007955703185871243
iteration 179, loss = 0.0009486465714871883
iteration 180, loss = 0.0010826040524989367
iteration 181, loss = 0.0011453984770923853
iteration 182, loss = 0.0006914819823578
iteration 183, loss = 0.0007353545515798032
iteration 184, loss = 0.0007129440782591701
iteration 185, loss = 0.0008273296989500523
iteration 186, loss = 0.0019000916508957744
iteration 187, loss = 0.0009639236959628761
iteration 188, loss = 0.0006534210406243801
iteration 189, loss = 0.0010420826729387045
iteration 190, loss = 0.0008907252340577543
iteration 191, loss = 0.0009400589042343199
iteration 192, loss = 0.0019053047290071845
iteration 193, loss = 0.0010021061170846224
iteration 194, loss = 0.0007546843262389302
iteration 195, loss = 0.0008446468855254352
iteration 196, loss = 0.0008203114266507328
iteration 197, loss = 0.0008469084277749062
iteration 198, loss = 0.001294323825277388
iteration 199, loss = 0.0007986410637386143
iteration 200, loss = 0.0013965427642688155
iteration 201, loss = 0.0008268048986792564
iteration 202, loss = 0.0007693579536862671
iteration 203, loss = 0.0009473454556427896
iteration 204, loss = 0.0008447571890428662
iteration 205, loss = 0.0007588829612359405
iteration 206, loss = 0.0008533368236385286
iteration 207, loss = 0.0008879736997187138
iteration 208, loss = 0.0009352437336929142
iteration 209, loss = 0.0009840200655162334
iteration 210, loss = 0.0007879820186644793
iteration 211, loss = 0.0014330948470160365
iteration 212, loss = 0.0016075399471446872
iteration 213, loss = 0.0008698960882611573
iteration 214, loss = 0.0014657628489658237
iteration 215, loss = 0.0006642137304879725
iteration 216, loss = 0.000811473117209971
iteration 217, loss = 0.001463642343878746
iteration 218, loss = 0.0008302583592012525
iteration 219, loss = 0.0009370423504151404
iteration 220, loss = 0.0006669527501799166
iteration 221, loss = 0.0011800358770415187
iteration 222, loss = 0.0011287473607808352
iteration 223, loss = 0.0009250083821825683
iteration 224, loss = 0.0009717336506582797
iteration 225, loss = 0.0007737784762866795
iteration 226, loss = 0.0009170227567665279
iteration 227, loss = 0.0014059290988370776
iteration 228, loss = 0.001578026101924479
iteration 229, loss = 0.0010266827885061502
iteration 230, loss = 0.0008182829478755593
iteration 231, loss = 0.0012308699078857899
iteration 232, loss = 0.00156662636436522
iteration 233, loss = 0.0017551317578181624
iteration 234, loss = 0.0013919707853347063
iteration 235, loss = 0.0015135550638660789
iteration 236, loss = 0.0011847862042486668
iteration 237, loss = 0.0010167601285502315
iteration 238, loss = 0.0010726527543738484
iteration 239, loss = 0.001160325133241713
iteration 240, loss = 0.0015431168721988797
iteration 241, loss = 0.001405657734721899
iteration 242, loss = 0.0009684370597824454
iteration 243, loss = 0.0007977401837706566
iteration 244, loss = 0.0006345683359540999
iteration 245, loss = 0.00098520633764565
iteration 246, loss = 0.001159896608442068
iteration 247, loss = 0.0008340070489794016
iteration 248, loss = 0.0007865173975005746
iteration 249, loss = 0.0010254611261188984
iteration 250, loss = 0.0009959699818864465
iteration 251, loss = 0.0006586987292394042
iteration 252, loss = 0.0008519479306414723
iteration 253, loss = 0.0009864651365205646
iteration 254, loss = 0.0010407106019556522
iteration 255, loss = 0.0009980191243812442
iteration 256, loss = 0.0012318631634116173
iteration 257, loss = 0.0009733248734846711
iteration 258, loss = 0.0010999144287779927
iteration 259, loss = 0.0006011082441546023
iteration 260, loss = 0.0008479662355966866
iteration 261, loss = 0.000737019581720233
iteration 262, loss = 0.0007760419975966215
iteration 263, loss = 0.0008192869718186557
iteration 264, loss = 0.0008065482834354043
iteration 265, loss = 0.0008899819222278893
iteration 266, loss = 0.0007666340097784996
iteration 267, loss = 0.0007637139642611146
iteration 268, loss = 0.000987559207715094
iteration 269, loss = 0.0007479839841835201
iteration 270, loss = 0.0007737392443232238
iteration 271, loss = 0.0007250852650031447
iteration 272, loss = 0.0007907491526566446
iteration 273, loss = 0.0008235689019784331
iteration 274, loss = 0.0009023849852383137
iteration 275, loss = 0.0008876686915755272
iteration 276, loss = 0.0008556952234357595
iteration 277, loss = 0.0008232131949625909
iteration 278, loss = 0.0007644116994924843
iteration 279, loss = 0.0007751649827696383
iteration 280, loss = 0.00094758061459288
iteration 281, loss = 0.0011609536595642567
iteration 282, loss = 0.0011191394878551364
iteration 283, loss = 0.0018141663167625666
iteration 284, loss = 0.0012909446377307177
iteration 285, loss = 0.0016858847811818123
iteration 286, loss = 0.0011224034242331982
iteration 287, loss = 0.0009101205505430698
iteration 288, loss = 0.000934698386117816
iteration 289, loss = 0.0007885222439654171
iteration 290, loss = 0.0008371902513317764
iteration 291, loss = 0.0014804359525442123
iteration 292, loss = 0.0007739743450656533
iteration 293, loss = 0.0007997761713340878
iteration 294, loss = 0.000722559227142483
iteration 295, loss = 0.001075951848179102
iteration 296, loss = 0.0011594241950660944
iteration 297, loss = 0.0008066425798460841
iteration 298, loss = 0.0009400320705026388
iteration 299, loss = 0.00070551986573264
iteration 300, loss = 0.0007503093220293522
iteration 1, loss = 0.0007307668565772474
iteration 2, loss = 0.0007224141736514866
iteration 3, loss = 0.001136187231168151
iteration 4, loss = 0.0007407781085930765
iteration 5, loss = 0.0008650622330605984
iteration 6, loss = 0.0006852811784483492
iteration 7, loss = 0.0009403247968293726
iteration 8, loss = 0.001846654573455453
iteration 9, loss = 0.000787110417149961
iteration 10, loss = 0.0014909411547705531
iteration 11, loss = 0.0009059050935320556
iteration 12, loss = 0.0008547281613573432
iteration 13, loss = 0.0009843396255746484
iteration 14, loss = 0.0008693704730831087
iteration 15, loss = 0.001145645510405302
iteration 16, loss = 0.0008641295717097819
iteration 17, loss = 0.0009628795669414103
iteration 18, loss = 0.000792150676716119
iteration 19, loss = 0.0017151528736576438
iteration 20, loss = 0.0013686759630218148
iteration 21, loss = 0.0007924378151074052
iteration 22, loss = 0.0010960064828395844
iteration 23, loss = 0.0008323019719682634
iteration 24, loss = 0.001209551584906876
iteration 25, loss = 0.0009913265239447355
iteration 26, loss = 0.0006422040751203895
iteration 27, loss = 0.0009830965427681804
iteration 28, loss = 0.0008843423565849662
iteration 29, loss = 0.0007011723937466741
iteration 30, loss = 0.0007415121654048562
iteration 31, loss = 0.0007814053678885102
iteration 32, loss = 0.0009782180422917008
iteration 33, loss = 0.0010592385660856962
iteration 34, loss = 0.0008817662601359189
iteration 35, loss = 0.0009162048809230328
iteration 36, loss = 0.0007664002478122711
iteration 37, loss = 0.0008535268134437501
iteration 38, loss = 0.0008118235855363309
iteration 39, loss = 0.000806501426268369
iteration 40, loss = 0.0007607739535160363
iteration 41, loss = 0.0008681549807079136
iteration 42, loss = 0.0007840335019864142
iteration 43, loss = 0.0008444672566838562
iteration 44, loss = 0.000678872806020081
iteration 45, loss = 0.000822247879114002
iteration 46, loss = 0.0010401332983747125
iteration 47, loss = 0.0007455899612978101
iteration 48, loss = 0.0007366955396719277
iteration 49, loss = 0.0008369323331862688
iteration 50, loss = 0.0012426944449543953
iteration 51, loss = 0.0007961607188917696
iteration 52, loss = 0.0013883375795558095
iteration 53, loss = 0.000933908624574542
iteration 54, loss = 0.0011323493672534823
iteration 55, loss = 0.0011089104227721691
iteration 56, loss = 0.0009088912047445774
iteration 57, loss = 0.0012113223783671856
iteration 58, loss = 0.00091730518033728
iteration 59, loss = 0.0007421820773743093
iteration 60, loss = 0.0009368893806822598
iteration 61, loss = 0.0008593863458372653
iteration 62, loss = 0.0008432320901192725
iteration 63, loss = 0.0007699453271925449
iteration 64, loss = 0.0011647390201687813
iteration 65, loss = 0.0006996897282078862
iteration 66, loss = 0.0009465248440392315
iteration 67, loss = 0.001246343133971095
iteration 68, loss = 0.0006547649390995502
iteration 69, loss = 0.0008546060998924077
iteration 70, loss = 0.001212581992149353
iteration 71, loss = 0.001333331223577261
iteration 72, loss = 0.0008220755844376981
iteration 73, loss = 0.0008850392769090831
iteration 74, loss = 0.001176677644252777
iteration 75, loss = 0.0015309603186324239
iteration 76, loss = 0.0013650301843881607
iteration 77, loss = 0.0009722293470986187
iteration 78, loss = 0.0008945782319642603
iteration 79, loss = 0.0008369009010493755
iteration 80, loss = 0.0009181000059470534
iteration 81, loss = 0.0008508390164934099
iteration 82, loss = 0.0009497025748714805
iteration 83, loss = 0.0008673277916386724
iteration 84, loss = 0.0007432388956658542
iteration 85, loss = 0.0007738459389656782
iteration 86, loss = 0.0007151514873839915
iteration 87, loss = 0.001635104650631547
iteration 88, loss = 0.0009885333711281419
iteration 89, loss = 0.0008159556891769171
iteration 90, loss = 0.0008038845844566822
iteration 91, loss = 0.0009245739784091711
iteration 92, loss = 0.0008320626802742481
iteration 93, loss = 0.0017752919811755419
iteration 94, loss = 0.000740524847060442
iteration 95, loss = 0.0014480090467259288
iteration 96, loss = 0.0010201657423749566
iteration 97, loss = 0.0008698101155459881
iteration 98, loss = 0.0013957343762740493
iteration 99, loss = 0.001433184021152556
iteration 100, loss = 0.001370531041175127
iteration 101, loss = 0.0009036335395649076
iteration 102, loss = 0.0012798564275726676
iteration 103, loss = 0.0007819010643288493
iteration 104, loss = 0.0006799942348152399
iteration 105, loss = 0.0011669807136058807
iteration 106, loss = 0.0013677342794835567
iteration 107, loss = 0.0008716386510059237
iteration 108, loss = 0.0007006461964920163
iteration 109, loss = 0.0008188520441763103
iteration 110, loss = 0.0008101167622953653
iteration 111, loss = 0.001035297173075378
iteration 112, loss = 0.0008262972696684301
iteration 113, loss = 0.0007539722719229758
iteration 114, loss = 0.0007602373952977359
iteration 115, loss = 0.0008920225081965327
iteration 116, loss = 0.0009413845837116241
iteration 117, loss = 0.0010681564453989267
iteration 118, loss = 0.0009165365481749177
iteration 119, loss = 0.0010538632050156593
iteration 120, loss = 0.0015328319277614355
iteration 121, loss = 0.0009287676075473428
iteration 122, loss = 0.0007615333306603134
iteration 123, loss = 0.001039546332322061
iteration 124, loss = 0.0009022338199429214
iteration 125, loss = 0.0014838940696790814
iteration 126, loss = 0.0007155881030485034
iteration 127, loss = 0.001884319819509983
iteration 128, loss = 0.0007693329243920743
iteration 129, loss = 0.0008115617674775422
iteration 130, loss = 0.0010322332382202148
iteration 131, loss = 0.0007238595862872899
iteration 132, loss = 0.0009696486522443593
iteration 133, loss = 0.0007390637183561921
iteration 134, loss = 0.0013659402029588819
iteration 135, loss = 0.000775424181483686
iteration 136, loss = 0.000789928191807121
iteration 137, loss = 0.0008895070641301572
iteration 138, loss = 0.0009163897484540939
iteration 139, loss = 0.000837560452055186
iteration 140, loss = 0.0008047467563301325
iteration 141, loss = 0.000674464157782495
iteration 142, loss = 0.0008598553249612451
iteration 143, loss = 0.0008956457022577524
iteration 144, loss = 0.0007499261992052197
iteration 145, loss = 0.0013247047318145633
iteration 146, loss = 0.000707221741322428
iteration 147, loss = 0.0012449820060282946
iteration 148, loss = 0.001490409136749804
iteration 149, loss = 0.00081097730435431
iteration 150, loss = 0.0008377645863220096
iteration 151, loss = 0.0008743938524276018
iteration 152, loss = 0.0006958264857530594
iteration 153, loss = 0.000971625093370676
iteration 154, loss = 0.0007280883728526533
iteration 155, loss = 0.0010813286062330008
iteration 156, loss = 0.001000473159365356
iteration 157, loss = 0.0008028853917494416
iteration 158, loss = 0.0008403528481721878
iteration 159, loss = 0.0010852717095986009
iteration 160, loss = 0.0011119931004941463
iteration 161, loss = 0.0009061225573532283
iteration 162, loss = 0.0007698747212998569
iteration 163, loss = 0.0009138542227447033
iteration 164, loss = 0.0009071184322237968
iteration 165, loss = 0.0006037563434801996
iteration 166, loss = 0.0010001823538914323
iteration 167, loss = 0.0012123128399252892
iteration 168, loss = 0.0015430423663929105
iteration 169, loss = 0.0013634241186082363
iteration 170, loss = 0.0008493160712532699
iteration 171, loss = 0.0006852197111584246
iteration 172, loss = 0.000758520036470145
iteration 173, loss = 0.000836955092381686
iteration 174, loss = 0.001198907382786274
iteration 175, loss = 0.0011959478724747896
iteration 176, loss = 0.0008823766838759184
iteration 177, loss = 0.0006885300972498953
iteration 178, loss = 0.000772378291003406
iteration 179, loss = 0.0007392984698526561
iteration 180, loss = 0.0007130560697987676
iteration 181, loss = 0.0012102224864065647
iteration 182, loss = 0.0008986741886474192
iteration 183, loss = 0.0008340863278135657
iteration 184, loss = 0.000837869243696332
iteration 185, loss = 0.0008524597506038845
iteration 186, loss = 0.0008514121873304248
iteration 187, loss = 0.0008718598401173949
iteration 188, loss = 0.001112604164518416
iteration 189, loss = 0.0008157800184562802
iteration 190, loss = 0.0009072802495211363
iteration 191, loss = 0.0007074192981235683
iteration 192, loss = 0.0008780534844845533
iteration 193, loss = 0.0008477794472128153
iteration 194, loss = 0.0007569300360046327
iteration 195, loss = 0.0015838785329833627
iteration 196, loss = 0.0007611002074554563
iteration 197, loss = 0.0011624342296272516
iteration 198, loss = 0.000668633496388793
iteration 199, loss = 0.0012115960707888007
iteration 200, loss = 0.0010844629723578691
iteration 201, loss = 0.0007310950313694775
iteration 202, loss = 0.0008936043595895171
iteration 203, loss = 0.0008490891195833683
iteration 204, loss = 0.0013507967814803123
iteration 205, loss = 0.0006728153675794601
iteration 206, loss = 0.0010689559858292341
iteration 207, loss = 0.0009272897150367498
iteration 208, loss = 0.0008584756287746131
iteration 209, loss = 0.0008823808748275042
iteration 210, loss = 0.0009178698528558016
iteration 211, loss = 0.0007759361760690808
iteration 212, loss = 0.0015899783466011286
iteration 213, loss = 0.0007742641028016806
iteration 214, loss = 0.0007260909769684076
iteration 215, loss = 0.0009295605705119669
iteration 216, loss = 0.0007421437767334282
iteration 217, loss = 0.0016421856125816703
iteration 218, loss = 0.0007427117088809609
iteration 219, loss = 0.0010111038573086262
iteration 220, loss = 0.0007534637115895748
iteration 221, loss = 0.0009513137047179043
iteration 222, loss = 0.00073069310747087
iteration 223, loss = 0.0008767204126343131
iteration 224, loss = 0.000916257849894464
iteration 225, loss = 0.0006901348824612796
iteration 226, loss = 0.0010263227159157395
iteration 227, loss = 0.0010756987612694502
iteration 228, loss = 0.0007781057502143085
iteration 229, loss = 0.001084903604350984
iteration 230, loss = 0.0008837497443892062
iteration 231, loss = 0.0007458946784026921
iteration 232, loss = 0.0015377531526610255
iteration 233, loss = 0.0007610235479660332
iteration 234, loss = 0.001137542654760182
iteration 235, loss = 0.0008609969518147409
iteration 236, loss = 0.0013041408965364099
iteration 237, loss = 0.001446328591555357
iteration 238, loss = 0.000988922780379653
iteration 239, loss = 0.001257356838323176
iteration 240, loss = 0.0016771397786214948
iteration 241, loss = 0.00079325120896101
iteration 242, loss = 0.0009962492622435093
iteration 243, loss = 0.0006491694948635995
iteration 244, loss = 0.0014026439748704433
iteration 245, loss = 0.0007924463716335595
iteration 246, loss = 0.000690350541844964
iteration 247, loss = 0.0009507732465863228
iteration 248, loss = 0.0012229899875819683
iteration 249, loss = 0.0012088974472135305
iteration 250, loss = 0.0007860300247557461
iteration 251, loss = 0.0010210521286353469
iteration 252, loss = 0.0018448776099830866
iteration 253, loss = 0.000965149374678731
iteration 254, loss = 0.0007378958980552852
iteration 255, loss = 0.0016111875884234905
iteration 256, loss = 0.0006994510185904801
iteration 257, loss = 0.0009995404398068786
iteration 258, loss = 0.0008099963888525963
iteration 259, loss = 0.0008856090134941041
iteration 260, loss = 0.0007597091025672853
iteration 261, loss = 0.0009321678662672639
iteration 262, loss = 0.0009190425043925643
iteration 263, loss = 0.0008303223294205964
iteration 264, loss = 0.0013253232464194298
iteration 265, loss = 0.000866472371853888
iteration 266, loss = 0.0007392800762318075
iteration 267, loss = 0.001013436121866107
iteration 268, loss = 0.002423118334263563
iteration 269, loss = 0.0008523230208083987
iteration 270, loss = 0.0007834501448087394
iteration 271, loss = 0.0008007417200133204
iteration 272, loss = 0.0008178853895515203
iteration 273, loss = 0.000874954741448164
iteration 274, loss = 0.0016822904581204057
iteration 275, loss = 0.0008041963446885347
iteration 276, loss = 0.0008223919430747628
iteration 277, loss = 0.0010245407465845346
iteration 278, loss = 0.0008265409851446748
iteration 279, loss = 0.0006960137980058789
iteration 280, loss = 0.0017114977817982435
iteration 281, loss = 0.0008604074828326702
iteration 282, loss = 0.0008898425730876625
iteration 283, loss = 0.000633894233033061
iteration 284, loss = 0.0007970493170432746
iteration 285, loss = 0.0007460208144038916
iteration 286, loss = 0.000883932167198509
iteration 287, loss = 0.0011248867958784103
iteration 288, loss = 0.0007751585217192769
iteration 289, loss = 0.0007369231316260993
iteration 290, loss = 0.0009412819053977728
iteration 291, loss = 0.0005545543972402811
iteration 292, loss = 0.001033347682096064
iteration 293, loss = 0.0007241256535053253
iteration 294, loss = 0.0010542613454163074
iteration 295, loss = 0.0014291539555415511
iteration 296, loss = 0.0016659721732139587
iteration 297, loss = 0.001121302368119359
iteration 298, loss = 0.0008406575070694089
iteration 299, loss = 0.0012017970439046621
iteration 300, loss = 0.0007460094057023525
iteration 1, loss = 0.00137056945823133
iteration 2, loss = 0.0007911912398412824
iteration 3, loss = 0.0006992930429987609
iteration 4, loss = 0.0008954473887570202
iteration 5, loss = 0.0009609809494577348
iteration 6, loss = 0.0008817006601020694
iteration 7, loss = 0.0009262044914066792
iteration 8, loss = 0.0009025673498399556
iteration 9, loss = 0.0010151138994842768
iteration 10, loss = 0.000783997995313257
iteration 11, loss = 0.0007368558435700834
iteration 12, loss = 0.0007088374113664031
iteration 13, loss = 0.0007936341571621597
iteration 14, loss = 0.0009332994231954217
iteration 15, loss = 0.0014616511762142181
iteration 16, loss = 0.0008808397105894983
iteration 17, loss = 0.0015796246007084846
iteration 18, loss = 0.0007741118315607309
iteration 19, loss = 0.000957512529566884
iteration 20, loss = 0.00073653255822137
iteration 21, loss = 0.0009763723937794566
iteration 22, loss = 0.001029430772177875
iteration 23, loss = 0.0007545375847257674
iteration 24, loss = 0.0011777393519878387
iteration 25, loss = 0.0006851431098766625
iteration 26, loss = 0.0007525521796196699
iteration 27, loss = 0.0007501404616050422
iteration 28, loss = 0.000808953947853297
iteration 29, loss = 0.0007750975200906396
iteration 30, loss = 0.0008932317141443491
iteration 31, loss = 0.0006116931908763945
iteration 32, loss = 0.0007724071037955582
iteration 33, loss = 0.0009234650060534477
iteration 34, loss = 0.0007198703824542463
iteration 35, loss = 0.0009661746444180608
iteration 36, loss = 0.0007253745570778847
iteration 37, loss = 0.0014856568304821849
iteration 38, loss = 0.00098275241907686
iteration 39, loss = 0.0015653017908334732
iteration 40, loss = 0.0008413732284680009
iteration 41, loss = 0.0009077264694496989
iteration 42, loss = 0.0006556285661645234
iteration 43, loss = 0.0007975454209372401
iteration 44, loss = 0.0007568749715574086
iteration 45, loss = 0.0008637678110972047
iteration 46, loss = 0.000947357330005616
iteration 47, loss = 0.0008844553376547992
iteration 48, loss = 0.0008326824754476547
iteration 49, loss = 0.001368492841720581
iteration 50, loss = 0.0015357783995568752
iteration 51, loss = 0.0008947817259468138
iteration 52, loss = 0.0008385181427001953
iteration 53, loss = 0.0010699382983148098
iteration 54, loss = 0.0008676391444168985
iteration 55, loss = 0.0006784232100471854
iteration 56, loss = 0.001223209430463612
iteration 57, loss = 0.0007119382498785853
iteration 58, loss = 0.0007742418092675507
iteration 59, loss = 0.0011730657424777746
iteration 60, loss = 0.0008877578075043857
iteration 61, loss = 0.0017107067396864295
iteration 62, loss = 0.0007908129482530057
iteration 63, loss = 0.001156597281806171
iteration 64, loss = 0.0008138262201100588
iteration 65, loss = 0.0007408105302602053
iteration 66, loss = 0.0007096669287420809
iteration 67, loss = 0.0006565105868503451
iteration 68, loss = 0.0010301328729838133
iteration 69, loss = 0.000704759091604501
iteration 70, loss = 0.0007878055330365896
iteration 71, loss = 0.000909229158423841
iteration 72, loss = 0.0010726233012974262
iteration 73, loss = 0.0008376825717277825
iteration 74, loss = 0.001022104057483375
iteration 75, loss = 0.0009937159484252334
iteration 76, loss = 0.0008506034500896931
iteration 77, loss = 0.0007450741832144558
iteration 78, loss = 0.0009630466811358929
iteration 79, loss = 0.001035435008816421
iteration 80, loss = 0.0007394427666440606
iteration 81, loss = 0.0015551899559795856
iteration 82, loss = 0.0007802608888596296
iteration 83, loss = 0.001228698412887752
iteration 84, loss = 0.001127743162214756
iteration 85, loss = 0.0014706586953252554
iteration 86, loss = 0.0009970179526135325
iteration 87, loss = 0.001664472627453506
iteration 88, loss = 0.000717554590664804
iteration 89, loss = 0.0008153809467330575
iteration 90, loss = 0.0008724250365048647
iteration 91, loss = 0.0016944361850619316
iteration 92, loss = 0.0007450439734384418
iteration 93, loss = 0.0017028850270435214
iteration 94, loss = 0.0009320570388808846
iteration 95, loss = 0.0013969362480565906
iteration 96, loss = 0.0008226039353758097
iteration 97, loss = 0.0007615929935127497
iteration 98, loss = 0.000880073057487607
iteration 99, loss = 0.0007253260700963438
iteration 100, loss = 0.00081254803808406
iteration 101, loss = 0.001166959060356021
iteration 102, loss = 0.0015915873227640986
iteration 103, loss = 0.001156507758423686
iteration 104, loss = 0.000747568323276937
iteration 105, loss = 0.001542880549095571
iteration 106, loss = 0.0012200232595205307
iteration 107, loss = 0.0013993324246257544
iteration 108, loss = 0.0006712874746881425
iteration 109, loss = 0.0007055142777971923
iteration 110, loss = 0.0008590801153331995
iteration 111, loss = 0.0012742176186293364
iteration 112, loss = 0.0008530341438017786
iteration 113, loss = 0.0008867677533999085
iteration 114, loss = 0.000706266553606838
iteration 115, loss = 0.0008976591052487493
iteration 116, loss = 0.0013425129000097513
iteration 117, loss = 0.0008769242558628321
iteration 118, loss = 0.0007237659301608801
iteration 119, loss = 0.0012425882741808891
iteration 120, loss = 0.0013096838956698775
iteration 121, loss = 0.0008491543703712523
iteration 122, loss = 0.000807512435130775
iteration 123, loss = 0.0009155297884717584
iteration 124, loss = 0.0008009899174794555
iteration 125, loss = 0.0009174359147436917
iteration 126, loss = 0.0011228108778595924
iteration 127, loss = 0.0008374402532353997
iteration 128, loss = 0.0019747738260775805
iteration 129, loss = 0.0008252138504758477
iteration 130, loss = 0.0008938601822592318
iteration 131, loss = 0.0014842789387330413
iteration 132, loss = 0.0011584757594391704
iteration 133, loss = 0.0009219053317792714
iteration 134, loss = 0.0008011491154320538
iteration 135, loss = 0.0009571680566295981
iteration 136, loss = 0.0009570756228640676
iteration 137, loss = 0.0007782371249049902
iteration 138, loss = 0.0007621673867106438
iteration 139, loss = 0.0008058518287725747
iteration 140, loss = 0.000770722224842757
iteration 141, loss = 0.0007517799385823309
iteration 142, loss = 0.0010007488308474422
iteration 143, loss = 0.0010283960727974772
iteration 144, loss = 0.0006474482361227274
iteration 145, loss = 0.0007191823679022491
iteration 146, loss = 0.0006992929847911
iteration 147, loss = 0.0010825855424627662
iteration 148, loss = 0.0008913596393540502
iteration 149, loss = 0.0010976953199133277
iteration 150, loss = 0.0008107522735372186
iteration 151, loss = 0.0008679470629431307
iteration 152, loss = 0.000739407492801547
iteration 153, loss = 0.0007834387943148613
iteration 154, loss = 0.001138999592512846
iteration 155, loss = 0.0008006887510418892
iteration 156, loss = 0.0007981584058143198
iteration 157, loss = 0.0008252890547737479
iteration 158, loss = 0.0009215486934408545
iteration 159, loss = 0.0011550977360457182
iteration 160, loss = 0.0008243973134085536
iteration 161, loss = 0.0011625610059127212
iteration 162, loss = 0.0014968323521316051
iteration 163, loss = 0.0010770505759865046
iteration 164, loss = 0.0007837977027520537
iteration 165, loss = 0.0009194573503918946
iteration 166, loss = 0.0007534143514931202
iteration 167, loss = 0.0007531924638897181
iteration 168, loss = 0.0018390174955129623
iteration 169, loss = 0.0008040877291932702
iteration 170, loss = 0.0009724467527121305
iteration 171, loss = 0.0012639866909012198
iteration 172, loss = 0.0012423672014847398
iteration 173, loss = 0.0008855367195792496
iteration 174, loss = 0.001214694115333259
iteration 175, loss = 0.0010721953585743904
iteration 176, loss = 0.0007704949239268899
iteration 177, loss = 0.0014157393015921116
iteration 178, loss = 0.0010690988274291158
iteration 179, loss = 0.0010424250504001975
iteration 180, loss = 0.0008538367692381144
iteration 181, loss = 0.0007566761923953891
iteration 182, loss = 0.0013345530023798347
iteration 183, loss = 0.0009668678394518793
iteration 184, loss = 0.0008975391974672675
iteration 185, loss = 0.0006943338084965944
iteration 186, loss = 0.0007638201932422817
iteration 187, loss = 0.0023450860753655434
iteration 188, loss = 0.0015397462993860245
iteration 189, loss = 0.0009761889232322574
iteration 190, loss = 0.0007580001838505268
iteration 191, loss = 0.0009592323331162333
iteration 192, loss = 0.0008209804072976112
iteration 193, loss = 0.0011180602014064789
iteration 194, loss = 0.0009474005782976747
iteration 195, loss = 0.0007939706556499004
iteration 196, loss = 0.0008982885046862066
iteration 197, loss = 0.0008756628958508372
iteration 198, loss = 0.0007412263657897711
iteration 199, loss = 0.0008612342062406242
iteration 200, loss = 0.0014891002792865038
iteration 201, loss = 0.0007743767346255481
iteration 202, loss = 0.0013328786008059978
iteration 203, loss = 0.000834895356092602
iteration 204, loss = 0.0007145644631236792
iteration 205, loss = 0.001550746732391417
iteration 206, loss = 0.0010820813477039337
iteration 207, loss = 0.0009622237412258983
iteration 208, loss = 0.0010607317090034485
iteration 209, loss = 0.0011122578289359808
iteration 210, loss = 0.0007383334450423717
iteration 211, loss = 0.0011657607974484563
iteration 212, loss = 0.000719986914191395
iteration 213, loss = 0.0016036391025409102
iteration 214, loss = 0.0009080169256776571
iteration 215, loss = 0.0007641402189619839
iteration 216, loss = 0.0008883947739377618
iteration 217, loss = 0.0007811286486685276
iteration 218, loss = 0.0006593766738660634
iteration 219, loss = 0.0007562935934402049
iteration 220, loss = 0.001396826934069395
iteration 221, loss = 0.0007912294240668416
iteration 222, loss = 0.0011266421061009169
iteration 223, loss = 0.0009566545486450195
iteration 224, loss = 0.0009161236230283976
iteration 225, loss = 0.0008608800126239657
iteration 226, loss = 0.0009225615649484098
iteration 227, loss = 0.0008647255599498749
iteration 228, loss = 0.000958755612373352
iteration 229, loss = 0.000716275186277926
iteration 230, loss = 0.0010435222648084164
iteration 231, loss = 0.0011673754779621959
iteration 232, loss = 0.0007895724847912788
iteration 233, loss = 0.0007841326878406107
iteration 234, loss = 0.0008940711268223822
iteration 235, loss = 0.0010118702193722129
iteration 236, loss = 0.001068645971827209
iteration 237, loss = 0.000776316795963794
iteration 238, loss = 0.0006976378499530256
iteration 239, loss = 0.0009536114521324635
iteration 240, loss = 0.0006477903807535768
iteration 241, loss = 0.0008007482974790037
iteration 242, loss = 0.0010516948532313108
iteration 243, loss = 0.0009552669362165034
iteration 244, loss = 0.0006904635229147971
iteration 245, loss = 0.0011707236990332603
iteration 246, loss = 0.001536053023301065
iteration 247, loss = 0.0007539222715422511
iteration 248, loss = 0.0014007469872012734
iteration 249, loss = 0.0009361938573420048
iteration 250, loss = 0.0017373180016875267
iteration 251, loss = 0.0006857324624434114
iteration 252, loss = 0.0007462499779649079
iteration 253, loss = 0.0008391896262764931
iteration 254, loss = 0.0008106352179311216
iteration 255, loss = 0.0007650528568774462
iteration 256, loss = 0.0011065979488193989
iteration 257, loss = 0.0010418767342343926
iteration 258, loss = 0.0011203481117263436
iteration 259, loss = 0.0009771449258551002
iteration 260, loss = 0.0006535641732625663
iteration 261, loss = 0.0007103645475581288
iteration 262, loss = 0.0008592007216066122
iteration 263, loss = 0.0007453956641256809
iteration 264, loss = 0.0008317172760143876
iteration 265, loss = 0.0008382359519600868
iteration 266, loss = 0.0008792025037109852
iteration 267, loss = 0.000989867839962244
iteration 268, loss = 0.0008947704918682575
iteration 269, loss = 0.0015180560294538736
iteration 270, loss = 0.0008385252440348268
iteration 271, loss = 0.0008012913167476654
iteration 272, loss = 0.0011517631355673075
iteration 273, loss = 0.0010979679645970464
iteration 274, loss = 0.0007284075254574418
iteration 275, loss = 0.0008120512939058244
iteration 276, loss = 0.0009352946653962135
iteration 277, loss = 0.0009084652410820127
iteration 278, loss = 0.000842182373162359
iteration 279, loss = 0.00093509053112939
iteration 280, loss = 0.0018257548799738288
iteration 281, loss = 0.0007738436106592417
iteration 282, loss = 0.0008834761683829129
iteration 283, loss = 0.001455751364119351
iteration 284, loss = 0.0007725932518951595
iteration 285, loss = 0.000945741543546319
iteration 286, loss = 0.0008173021487891674
iteration 287, loss = 0.0008739702752791345
iteration 288, loss = 0.0011809251736849546
iteration 289, loss = 0.000800721172709018
iteration 290, loss = 0.000613550771959126
iteration 291, loss = 0.0014017026405781507
iteration 292, loss = 0.0008607596391811967
iteration 293, loss = 0.0008011694299057126
iteration 294, loss = 0.0007764446781948209
iteration 295, loss = 0.0006857292028144002
iteration 296, loss = 0.0009800875559449196
iteration 297, loss = 0.00127771170809865
iteration 298, loss = 0.0009869046043604612
iteration 299, loss = 0.0006169857224449515
iteration 300, loss = 0.0009195788879878819
iteration 1, loss = 0.0008095780503936112
iteration 2, loss = 0.0009586017695255578
iteration 3, loss = 0.0015807363670319319
iteration 4, loss = 0.0009948816150426865
iteration 5, loss = 0.0017488461453467607
iteration 6, loss = 0.0013140775263309479
iteration 7, loss = 0.0007831788389012218
iteration 8, loss = 0.0009950665989890695
iteration 9, loss = 0.0011123783187940717
iteration 10, loss = 0.0010795783018693328
iteration 11, loss = 0.0007727910997346044
iteration 12, loss = 0.0006591339479200542
iteration 13, loss = 0.0009824925800785422
iteration 14, loss = 0.0011169728823006153
iteration 15, loss = 0.000980971846729517
iteration 16, loss = 0.0011412102030590177
iteration 17, loss = 0.0008621803717687726
iteration 18, loss = 0.0013669736217707396
iteration 19, loss = 0.0010014174040406942
iteration 20, loss = 0.0008125635795295238
iteration 21, loss = 0.0010582308750599623
iteration 22, loss = 0.0015219670021906495
iteration 23, loss = 0.0008900839020498097
iteration 24, loss = 0.001557552837766707
iteration 25, loss = 0.001006998005323112
iteration 26, loss = 0.0017301221378147602
iteration 27, loss = 0.0007778615690767765
iteration 28, loss = 0.000830994569696486
iteration 29, loss = 0.0008242873009294271
iteration 30, loss = 0.0007196353399194777
iteration 31, loss = 0.0008883324335329235
iteration 32, loss = 0.0008608403149992228
iteration 33, loss = 0.0013834403362125158
iteration 34, loss = 0.0011169194476678967
iteration 35, loss = 0.0008112754439935088
iteration 36, loss = 0.000875550031196326
iteration 37, loss = 0.0007780350279062986
iteration 38, loss = 0.0013242855202406645
iteration 39, loss = 0.0007849098765291274
iteration 40, loss = 0.000772005645558238
iteration 41, loss = 0.0008781372453086078
iteration 42, loss = 0.000807526521384716
iteration 43, loss = 0.0008085740264505148
iteration 44, loss = 0.0010303996969014406
iteration 45, loss = 0.0011340031633153558
iteration 46, loss = 0.0008100107661448419
iteration 47, loss = 0.0008608280913904309
iteration 48, loss = 0.0009410183993168175
iteration 49, loss = 0.0010258248075842857
iteration 50, loss = 0.0011136590037494898
iteration 51, loss = 0.0007205123547464609
iteration 52, loss = 0.0009226167458109558
iteration 53, loss = 0.0015561520121991634
iteration 54, loss = 0.0008359555504284799
iteration 55, loss = 0.0010994686745107174
iteration 56, loss = 0.0008017175132408738
iteration 57, loss = 0.0007997680804692209
iteration 58, loss = 0.0007281778380274773
iteration 59, loss = 0.0007252433570101857
iteration 60, loss = 0.0011284082429483533
iteration 61, loss = 0.0008632817189209163
iteration 62, loss = 0.0008387446869164705
iteration 63, loss = 0.0009912400273606181
iteration 64, loss = 0.0007079695933498442
iteration 65, loss = 0.0007422663038596511
iteration 66, loss = 0.0009162711794488132
iteration 67, loss = 0.0011630693916231394
iteration 68, loss = 0.0011589281493797898
iteration 69, loss = 0.0008947697933763266
iteration 70, loss = 0.0007055825553834438
iteration 71, loss = 0.0007679716218262911
iteration 72, loss = 0.0008274433785118163
iteration 73, loss = 0.0011282572522759438
iteration 74, loss = 0.0011195718543604016
iteration 75, loss = 0.0008609546348452568
iteration 76, loss = 0.0010026296367868781
iteration 77, loss = 0.0006703328108415008
iteration 78, loss = 0.0015155277214944363
iteration 79, loss = 0.0019467240199446678
iteration 80, loss = 0.0007577126962132752
iteration 81, loss = 0.0008715979056432843
iteration 82, loss = 0.0011459732195362449
iteration 83, loss = 0.001045136246830225
iteration 84, loss = 0.0008699224563315511
iteration 85, loss = 0.0008776391041465104
iteration 86, loss = 0.0007137882057577372
iteration 87, loss = 0.0013267333852127194
iteration 88, loss = 0.0008601117879152298
iteration 89, loss = 0.0008283247007057071
iteration 90, loss = 0.0008514608489349484
iteration 91, loss = 0.0010095288744196296
iteration 92, loss = 0.0016729597700759768
iteration 93, loss = 0.0013225491857156157
iteration 94, loss = 0.0005972522194497287
iteration 95, loss = 0.0008289130637422204
iteration 96, loss = 0.0015619086334481835
iteration 97, loss = 0.0006671315059065819
iteration 98, loss = 0.0009914611000567675
iteration 99, loss = 0.0008534370572306216
iteration 100, loss = 0.0007467307150363922
iteration 101, loss = 0.0013885218650102615
iteration 102, loss = 0.0007972846506163478
iteration 103, loss = 0.0016499718185514212
iteration 104, loss = 0.0006817496032454073
iteration 105, loss = 0.0008526424644514918
iteration 106, loss = 0.001116406754590571
iteration 107, loss = 0.0008694054558873177
iteration 108, loss = 0.0009484576294198632
iteration 109, loss = 0.0007461352506652474
iteration 110, loss = 0.0007964324322529137
iteration 111, loss = 0.001749172923155129
iteration 112, loss = 0.0007512849988415837
iteration 113, loss = 0.0009709828300401568
iteration 114, loss = 0.0014554958324879408
iteration 115, loss = 0.0010043143993243575
iteration 116, loss = 0.0010959990322589874
iteration 117, loss = 0.0008513568318448961
iteration 118, loss = 0.0007852452690713108
iteration 119, loss = 0.0006947347428649664
iteration 120, loss = 0.0010053181322291493
iteration 121, loss = 0.0010408294619992375
iteration 122, loss = 0.0007388780359178782
iteration 123, loss = 0.0009678436908870935
iteration 124, loss = 0.0015516250859946012
iteration 125, loss = 0.0007037774776108563
iteration 126, loss = 0.0010187578154727817
iteration 127, loss = 0.0009929825318977237
iteration 128, loss = 0.0009739813976921141
iteration 129, loss = 0.0007532583549618721
iteration 130, loss = 0.0008508936734870076
iteration 131, loss = 0.000882763764820993
iteration 132, loss = 0.0010391899850219488
iteration 133, loss = 0.0014862437965348363
iteration 134, loss = 0.0008649572264403105
iteration 135, loss = 0.0010066539980471134
iteration 136, loss = 0.0012146608205512166
iteration 137, loss = 0.000785426760558039
iteration 138, loss = 0.0008196746930480003
iteration 139, loss = 0.0010650197509676218
iteration 140, loss = 0.0008296285523101687
iteration 141, loss = 0.000875145080499351
iteration 142, loss = 0.0009476853301748633
iteration 143, loss = 0.0008652023389004171
iteration 144, loss = 0.0008198738796636462
iteration 145, loss = 0.0007656074594706297
iteration 146, loss = 0.0007361829048022628
iteration 147, loss = 0.0010659300023689866
iteration 148, loss = 0.0009413357474841177
iteration 149, loss = 0.0010384928900748491
iteration 150, loss = 0.0010875854641199112
iteration 151, loss = 0.000760286464355886
iteration 152, loss = 0.0009006846812553704
iteration 153, loss = 0.0012663782108575106
iteration 154, loss = 0.0011579113779589534
iteration 155, loss = 0.0007037806790322065
iteration 156, loss = 0.0010128532303497195
iteration 157, loss = 0.0008001788519322872
iteration 158, loss = 0.0008183976169675589
iteration 159, loss = 0.0006888920324854553
iteration 160, loss = 0.0008665596833452582
iteration 161, loss = 0.0009630130953155458
iteration 162, loss = 0.0012122099287807941
iteration 163, loss = 0.0011981652351096272
iteration 164, loss = 0.0009004841558635235
iteration 165, loss = 0.0009585176594555378
iteration 166, loss = 0.0009327517473138869
iteration 167, loss = 0.000815166044048965
iteration 168, loss = 0.0008254374261014163
iteration 169, loss = 0.0008112761424854398
iteration 170, loss = 0.0009392721694894135
iteration 171, loss = 0.000634601863566786
iteration 172, loss = 0.0008617140701971948
iteration 173, loss = 0.001653660903684795
iteration 174, loss = 0.0008301912457682192
iteration 175, loss = 0.0008412034949287772
iteration 176, loss = 0.0009158183238469064
iteration 177, loss = 0.0009463585447520018
iteration 178, loss = 0.0011837612837553024
iteration 179, loss = 0.0007465716334991157
iteration 180, loss = 0.0007495630416087806
iteration 181, loss = 0.0006987442029640079
iteration 182, loss = 0.0007211626507341862
iteration 183, loss = 0.000676202354952693
iteration 184, loss = 0.0009130188846029341
iteration 185, loss = 0.00075269874650985
iteration 186, loss = 0.0008834126638248563
iteration 187, loss = 0.0006587358075194061
iteration 188, loss = 0.0008207683567889035
iteration 189, loss = 0.0016209586756303906
iteration 190, loss = 0.0011384744429960847
iteration 191, loss = 0.0012508868239820004
iteration 192, loss = 0.001180614228360355
iteration 193, loss = 0.0009489282383583486
iteration 194, loss = 0.0007871168199926615
iteration 195, loss = 0.0008553848601877689
iteration 196, loss = 0.0007632739725522697
iteration 197, loss = 0.00107723253313452
iteration 198, loss = 0.0009376323432661593
iteration 199, loss = 0.0006696519558317959
iteration 200, loss = 0.0009618506883271039
iteration 201, loss = 0.0007638692623004317
iteration 202, loss = 0.0007187765440903604
iteration 203, loss = 0.0010828408412635326
iteration 204, loss = 0.0011778335319831967
iteration 205, loss = 0.0015028392663225532
iteration 206, loss = 0.000746126810554415
iteration 207, loss = 0.0012364547001197934
iteration 208, loss = 0.0007439623004756868
iteration 209, loss = 0.0015560120809823275
iteration 210, loss = 0.001454853336326778
iteration 211, loss = 0.0009972794214263558
iteration 212, loss = 0.0008943084394559264
iteration 213, loss = 0.0010488899424672127
iteration 214, loss = 0.001014077221043408
iteration 215, loss = 0.0006907732458785176
iteration 216, loss = 0.000819583423435688
iteration 217, loss = 0.0009771543554961681
iteration 218, loss = 0.0009006675099954009
iteration 219, loss = 0.0008694246062077582
iteration 220, loss = 0.000810814555734396
iteration 221, loss = 0.0007556013879366219
iteration 222, loss = 0.0008537828107364476
iteration 223, loss = 0.0006757062510587275
iteration 224, loss = 0.0008001186652109027
iteration 225, loss = 0.0013838818995282054
iteration 226, loss = 0.0006241821101866663
iteration 227, loss = 0.001141443382948637
iteration 228, loss = 0.0013308700872585177
iteration 229, loss = 0.0008857682114467025
iteration 230, loss = 0.001067912788130343
iteration 231, loss = 0.0006646767724305391
iteration 232, loss = 0.001215137424878776
iteration 233, loss = 0.0009326449362561107
iteration 234, loss = 0.001189532340504229
iteration 235, loss = 0.0007750156219117343
iteration 236, loss = 0.0017259656451642513
iteration 237, loss = 0.0008515038061887026
iteration 238, loss = 0.0010825535282492638
iteration 239, loss = 0.0006825950695201755
iteration 240, loss = 0.002064115833491087
iteration 241, loss = 0.0007370924577116966
iteration 242, loss = 0.0008991641807369888
iteration 243, loss = 0.001321666408330202
iteration 244, loss = 0.000994521426036954
iteration 245, loss = 0.0009057337883859873
iteration 246, loss = 0.0007200961699709296
iteration 247, loss = 0.000837713829241693
iteration 248, loss = 0.0008168054628185928
iteration 249, loss = 0.0006513336556963623
iteration 250, loss = 0.0008849223959259689
iteration 251, loss = 0.0016498506302013993
iteration 252, loss = 0.000925119617022574
iteration 253, loss = 0.0006739716627635062
iteration 254, loss = 0.0007441273774020374
iteration 255, loss = 0.0008811934967525303
iteration 256, loss = 0.002173341577872634
iteration 257, loss = 0.0008571138605475426
iteration 258, loss = 0.0008128805202431977
iteration 259, loss = 0.0007210992625914514
iteration 260, loss = 0.0007251130882650614
iteration 261, loss = 0.001468271599151194
iteration 262, loss = 0.0008834601612761617
iteration 263, loss = 0.0008140545105561614
iteration 264, loss = 0.0008430496091023088
iteration 265, loss = 0.0006587173556908965
iteration 266, loss = 0.0008569621713832021
iteration 267, loss = 0.0008125653257593513
iteration 268, loss = 0.001095575513318181
iteration 269, loss = 0.0006653819582425058
iteration 270, loss = 0.0008845169795677066
iteration 271, loss = 0.0007011640118435025
iteration 272, loss = 0.001280619646422565
iteration 273, loss = 0.0008056854130700231
iteration 274, loss = 0.0008703645435161889
iteration 275, loss = 0.00081681675510481
iteration 276, loss = 0.0008564373711124063
iteration 277, loss = 0.0008181824232451618
iteration 278, loss = 0.0008760845521464944
iteration 279, loss = 0.0008353695156984031
iteration 280, loss = 0.0011432482860982418
iteration 281, loss = 0.0007788368966430426
iteration 282, loss = 0.0007901875069364905
iteration 283, loss = 0.0007332466193474829
iteration 284, loss = 0.0008311702404171228
iteration 285, loss = 0.0014844740508124232
iteration 286, loss = 0.0006990499095991254
iteration 287, loss = 0.0008443043334409595
iteration 288, loss = 0.0007548760040663183
iteration 289, loss = 0.0006994428695179522
iteration 290, loss = 0.0008711033733561635
iteration 291, loss = 0.000843041343614459
iteration 292, loss = 0.0009002505685202777
iteration 293, loss = 0.001418756553903222
iteration 294, loss = 0.0007941300282254815
iteration 295, loss = 0.0007176075596362352
iteration 296, loss = 0.0010650185868144035
iteration 297, loss = 0.0008653177064843476
iteration 298, loss = 0.0007663436117582023
iteration 299, loss = 0.0008866849238984287
iteration 300, loss = 0.0015601030318066478
iteration 1, loss = 0.000970173510722816
iteration 2, loss = 0.001488434849306941
iteration 3, loss = 0.0007412254344671965
iteration 4, loss = 0.0010063503868877888
iteration 5, loss = 0.0008839865913614631
iteration 6, loss = 0.0008530934574082494
iteration 7, loss = 0.0014189236098900437
iteration 8, loss = 0.0011504384456202388
iteration 9, loss = 0.0007592752808704972
iteration 10, loss = 0.000766823417507112
iteration 11, loss = 0.001122830668464303
iteration 12, loss = 0.0009837131947278976
iteration 13, loss = 0.0009162867208942771
iteration 14, loss = 0.0007487282855436206
iteration 15, loss = 0.0008444514242000878
iteration 16, loss = 0.0008520456030964851
iteration 17, loss = 0.0011455063940957189
iteration 18, loss = 0.0007430124096572399
iteration 19, loss = 0.0010122653329744935
iteration 20, loss = 0.0006388252950273454
iteration 21, loss = 0.0015816993545740843
iteration 22, loss = 0.0007792365504428744
iteration 23, loss = 0.0015319761587306857
iteration 24, loss = 0.001122659188695252
iteration 25, loss = 0.0007562222890555859
iteration 26, loss = 0.0014250867534428835
iteration 27, loss = 0.0007728463970124722
iteration 28, loss = 0.0006683981628157198
iteration 29, loss = 0.0009764800779521465
iteration 30, loss = 0.0011356519535183907
iteration 31, loss = 0.0008403036626987159
iteration 32, loss = 0.0009118770249187946
iteration 33, loss = 0.0010404906934127212
iteration 34, loss = 0.0012629175325855613
iteration 35, loss = 0.0016110544092953205
iteration 36, loss = 0.0007370835519395769
iteration 37, loss = 0.0010235371300950646
iteration 38, loss = 0.001829824410378933
iteration 39, loss = 0.0006280125817283988
iteration 40, loss = 0.001072603976354003
iteration 41, loss = 0.000839646381791681
iteration 42, loss = 0.0008076417725533247
iteration 43, loss = 0.0007116466877050698
iteration 44, loss = 0.0007215963560156524
iteration 45, loss = 0.0014460416277870536
iteration 46, loss = 0.0023775924928486347
iteration 47, loss = 0.0007701237336732447
iteration 48, loss = 0.0006179867195896804
iteration 49, loss = 0.0014098533429205418
iteration 50, loss = 0.0007911316934041679
iteration 51, loss = 0.0010101529769599438
iteration 52, loss = 0.0007527908310294151
iteration 53, loss = 0.0015972228720784187
iteration 54, loss = 0.0008465471328236163
iteration 55, loss = 0.0014453649055212736
iteration 56, loss = 0.0011903566773980856
iteration 57, loss = 0.0007285124156624079
iteration 58, loss = 0.001197412726469338
iteration 59, loss = 0.000844261609017849
iteration 60, loss = 0.0007240917766466737
iteration 61, loss = 0.000818180269561708
iteration 62, loss = 0.0008734885486774147
iteration 63, loss = 0.0007737054256722331
iteration 64, loss = 0.0009083648328669369
iteration 65, loss = 0.0007553440518677235
iteration 66, loss = 0.0009410912171006203
iteration 67, loss = 0.0007850212277844548
iteration 68, loss = 0.0011857501231133938
iteration 69, loss = 0.0007277395343407989
iteration 70, loss = 0.0008142383303493261
iteration 71, loss = 0.0007702655857428908
iteration 72, loss = 0.0014820168726146221
iteration 73, loss = 0.0016459835460409522
iteration 74, loss = 0.0009111624676734209
iteration 75, loss = 0.0007140800007618964
iteration 76, loss = 0.0007746893679723144
iteration 77, loss = 0.0007078787311911583
iteration 78, loss = 0.0007094161701388657
iteration 79, loss = 0.0007847259985283017
iteration 80, loss = 0.001243924256414175
iteration 81, loss = 0.0007739758002571762
iteration 82, loss = 0.0007832420524209738
iteration 83, loss = 0.0008862211252562702
iteration 84, loss = 0.0011198747670277953
iteration 85, loss = 0.0008350551361218095
iteration 86, loss = 0.0007039109477773309
iteration 87, loss = 0.0009403674630448222
iteration 88, loss = 0.001126202754676342
iteration 89, loss = 0.0009196997852995992
iteration 90, loss = 0.0007119839428924024
iteration 91, loss = 0.0006385482265613973
iteration 92, loss = 0.0008305812370963395
iteration 93, loss = 0.0007005746010690928
iteration 94, loss = 0.0007852689013816416
iteration 95, loss = 0.0008064992143772542
iteration 96, loss = 0.0009266724810004234
iteration 97, loss = 0.0010513552697375417
iteration 98, loss = 0.000857752631418407
iteration 99, loss = 0.000928011373616755
iteration 100, loss = 0.0007850031252019107
iteration 101, loss = 0.0010207929881289601
iteration 102, loss = 0.0011463591363281012
iteration 103, loss = 0.0007255507516674697
iteration 104, loss = 0.0010258920956403017
iteration 105, loss = 0.0011535906232893467
iteration 106, loss = 0.0014435277553275228
iteration 107, loss = 0.0008604868780821562
iteration 108, loss = 0.0008838934591040015
iteration 109, loss = 0.0007910134736448526
iteration 110, loss = 0.0009247848065569997
iteration 111, loss = 0.0010139875812456012
iteration 112, loss = 0.0009535221033729613
iteration 113, loss = 0.001628945698030293
iteration 114, loss = 0.0006393571966327727
iteration 115, loss = 0.0009674620814621449
iteration 116, loss = 0.00149635155685246
iteration 117, loss = 0.0007471581920981407
iteration 118, loss = 0.0007354605477303267
iteration 119, loss = 0.0009284268599003553
iteration 120, loss = 0.0008450641762465239
iteration 121, loss = 0.0010386609937995672
iteration 122, loss = 0.0008431447786279023
iteration 123, loss = 0.000871221418492496
iteration 124, loss = 0.0010316648986190557
iteration 125, loss = 0.001054008025676012
iteration 126, loss = 0.0008491527405567467
iteration 127, loss = 0.0011190674267709255
iteration 128, loss = 0.0008114039083011448
iteration 129, loss = 0.0010104038519784808
iteration 130, loss = 0.0008165653562173247
iteration 131, loss = 0.0011798840714618564
iteration 132, loss = 0.0009736703359521925
iteration 133, loss = 0.001084746327251196
iteration 134, loss = 0.0007469768170267344
iteration 135, loss = 0.0009147258242592216
iteration 136, loss = 0.0009317354997619987
iteration 137, loss = 0.0011449563317000866
iteration 138, loss = 0.0008587732445448637
iteration 139, loss = 0.0008202078170143068
iteration 140, loss = 0.0009327687439508736
iteration 141, loss = 0.001128768315538764
iteration 142, loss = 0.0007401596521958709
iteration 143, loss = 0.001900333329103887
iteration 144, loss = 0.0010631559416651726
iteration 145, loss = 0.0008544677402824163
iteration 146, loss = 0.0010584226110950112
iteration 147, loss = 0.0007025600643828511
iteration 148, loss = 0.0015016472898423672
iteration 149, loss = 0.000847267743665725
iteration 150, loss = 0.0010690015042200685
iteration 151, loss = 0.0011456501670181751
iteration 152, loss = 0.0008382288506254554
iteration 153, loss = 0.0007539814105257392
iteration 154, loss = 0.0008185988990589976
iteration 155, loss = 0.000700197706464678
iteration 156, loss = 0.0009360795374959707
iteration 157, loss = 0.0008595813997089863
iteration 158, loss = 0.0008775582537055016
iteration 159, loss = 0.0011538795661181211
iteration 160, loss = 0.0007233377546072006
iteration 161, loss = 0.0018566229846328497
iteration 162, loss = 0.0009918210562318563
iteration 163, loss = 0.0007876966847106814
iteration 164, loss = 0.001047157566063106
iteration 165, loss = 0.0019274071091786027
iteration 166, loss = 0.0009034173563122749
iteration 167, loss = 0.0008689992828294635
iteration 168, loss = 0.0015841879649087787
iteration 169, loss = 0.0008270699763670564
iteration 170, loss = 0.0011124482844024897
iteration 171, loss = 0.0008177408599294722
iteration 172, loss = 0.0009438176639378071
iteration 173, loss = 0.0010393359698355198
iteration 174, loss = 0.0008905878057703376
iteration 175, loss = 0.0007945238030515611
iteration 176, loss = 0.0010354142868891358
iteration 177, loss = 0.0007855872390791774
iteration 178, loss = 0.0011832057498395443
iteration 179, loss = 0.0010505302343517542
iteration 180, loss = 0.0007499861530959606
iteration 181, loss = 0.0007311493391171098
iteration 182, loss = 0.0008101786952465773
iteration 183, loss = 0.0007772958488203585
iteration 184, loss = 0.0008766805403865874
iteration 185, loss = 0.0009177783504128456
iteration 186, loss = 0.0009458169806748629
iteration 187, loss = 0.0006685446132905781
iteration 188, loss = 0.0012391955824568868
iteration 189, loss = 0.000896957004442811
iteration 190, loss = 0.0010222364217042923
iteration 191, loss = 0.0008830219740048051
iteration 192, loss = 0.0008413610048592091
iteration 193, loss = 0.0009703345713205636
iteration 194, loss = 0.0009432714432477951
iteration 195, loss = 0.00127776013687253
iteration 196, loss = 0.0006675339536741376
iteration 197, loss = 0.0007880785851739347
iteration 198, loss = 0.0007061913493089378
iteration 199, loss = 0.0014331882121041417
iteration 200, loss = 0.0008465662831440568
iteration 201, loss = 0.0007703976589255035
iteration 202, loss = 0.0009316650684922934
iteration 203, loss = 0.000976297480519861
iteration 204, loss = 0.0013153572799637914
iteration 205, loss = 0.0009614290902391076
iteration 206, loss = 0.0017284121131524444
iteration 207, loss = 0.0008041049586609006
iteration 208, loss = 0.0007957840571179986
iteration 209, loss = 0.0018057656707242131
iteration 210, loss = 0.001560882548801601
iteration 211, loss = 0.0008734493749216199
iteration 212, loss = 0.0009343153215013444
iteration 213, loss = 0.0008611267548985779
iteration 214, loss = 0.0008789238054305315
iteration 215, loss = 0.0011440422385931015
iteration 216, loss = 0.0008412927854806185
iteration 217, loss = 0.0007791623938828707
iteration 218, loss = 0.0011909672757610679
iteration 219, loss = 0.0009763879352249205
iteration 220, loss = 0.0009318161755800247
iteration 221, loss = 0.0008790199644863605
iteration 222, loss = 0.0008586669573560357
iteration 223, loss = 0.0007906036335043609
iteration 224, loss = 0.0008005351410247386
iteration 225, loss = 0.0008432572358287871
iteration 226, loss = 0.001282084733247757
iteration 227, loss = 0.0007896599126979709
iteration 228, loss = 0.0009012391092255712
iteration 229, loss = 0.0007984480471350253
iteration 230, loss = 0.0005928855971433222
iteration 231, loss = 0.0007604481652379036
iteration 232, loss = 0.0008587471675127745
iteration 233, loss = 0.0011132091749459505
iteration 234, loss = 0.0008372740703634918
iteration 235, loss = 0.000976643292233348
iteration 236, loss = 0.0007952840533107519
iteration 237, loss = 0.000987134873867035
iteration 238, loss = 0.000760915398132056
iteration 239, loss = 0.0007579022785648704
iteration 240, loss = 0.0007340438896790147
iteration 241, loss = 0.0010292880469933152
iteration 242, loss = 0.0010645753936842084
iteration 243, loss = 0.0009301878162659705
iteration 244, loss = 0.0008153798407875001
iteration 245, loss = 0.0006935499259270728
iteration 246, loss = 0.0007463324582204223
iteration 247, loss = 0.001130772870965302
iteration 248, loss = 0.0009470876539126039
iteration 249, loss = 0.0006859174463897943
iteration 250, loss = 0.0007254534284584224
iteration 251, loss = 0.0008092232747003436
iteration 252, loss = 0.0008034631609916687
iteration 253, loss = 0.0011765549425035715
iteration 254, loss = 0.0008502764976583421
iteration 255, loss = 0.001205878215841949
iteration 256, loss = 0.0007117625791579485
iteration 257, loss = 0.0006803171709179878
iteration 258, loss = 0.000733515596948564
iteration 259, loss = 0.0009322320693172514
iteration 260, loss = 0.0013476922176778316
iteration 261, loss = 0.0014804546954110265
iteration 262, loss = 0.0009931105887517333
iteration 263, loss = 0.001042054034769535
iteration 264, loss = 0.0009566037333570421
iteration 265, loss = 0.0010715937241911888
iteration 266, loss = 0.0006671032169833779
iteration 267, loss = 0.0014045349089428782
iteration 268, loss = 0.0008316772291436791
iteration 269, loss = 0.0010585518321022391
iteration 270, loss = 0.0008387475390918553
iteration 271, loss = 0.0014324020594358444
iteration 272, loss = 0.0007418187451548874
iteration 273, loss = 0.0006956436554901302
iteration 274, loss = 0.0010257173562422395
iteration 275, loss = 0.0011541798012331128
iteration 276, loss = 0.0006965479697100818
iteration 277, loss = 0.0011286288499832153
iteration 278, loss = 0.00096247618785128
iteration 279, loss = 0.0007792881806381047
iteration 280, loss = 0.0008817112538963556
iteration 281, loss = 0.001181266037747264
iteration 282, loss = 0.0012718484504148364
iteration 283, loss = 0.001090691308490932
iteration 284, loss = 0.0007393508567474782
iteration 285, loss = 0.0009161746129393578
iteration 286, loss = 0.0015777010703459382
iteration 287, loss = 0.0015494105173274875
iteration 288, loss = 0.0008429358713328838
iteration 289, loss = 0.0010373216355219483
iteration 290, loss = 0.0007255258969962597
iteration 291, loss = 0.0007078754715621471
iteration 292, loss = 0.0008800296345725656
iteration 293, loss = 0.0010952386073768139
iteration 294, loss = 0.0008054801728576422
iteration 295, loss = 0.0007637212402187288
iteration 296, loss = 0.0009453753591515124
iteration 297, loss = 0.0010864645009860396
iteration 298, loss = 0.0006977575831115246
iteration 299, loss = 0.0007391405524685979
iteration 300, loss = 0.000686060986481607
iteration 1, loss = 0.0007538548670709133
iteration 2, loss = 0.0010646273149177432
iteration 3, loss = 0.0010074834572151303
iteration 4, loss = 0.000884941837284714
iteration 5, loss = 0.0007924172678031027
iteration 6, loss = 0.001351290033198893
iteration 7, loss = 0.0008103577420115471
iteration 8, loss = 0.0009614583686925471
iteration 9, loss = 0.0006435427931137383
iteration 10, loss = 0.0007699682610109448
iteration 11, loss = 0.0013519733911380172
iteration 12, loss = 0.0011222314788028598
iteration 13, loss = 0.0008693681447766721
iteration 14, loss = 0.000886095454916358
iteration 15, loss = 0.0018534022383391857
iteration 16, loss = 0.0011510476469993591
iteration 17, loss = 0.000979302218183875
iteration 18, loss = 0.0011041383258998394
iteration 19, loss = 0.0007676169043406844
iteration 20, loss = 0.00086887989891693
iteration 21, loss = 0.0007219703984446824
iteration 22, loss = 0.0008496688678860664
iteration 23, loss = 0.0006446571787819266
iteration 24, loss = 0.0007972018793225288
iteration 25, loss = 0.0007277158438228071
iteration 26, loss = 0.0007153615588322282
iteration 27, loss = 0.0007339700241573155
iteration 28, loss = 0.000730113941244781
iteration 29, loss = 0.0014038959052413702
iteration 30, loss = 0.0013199050445109606
iteration 31, loss = 0.0009510803502053022
iteration 32, loss = 0.0013069744454696774
iteration 33, loss = 0.0008205891936086118
iteration 34, loss = 0.0013665794394910336
iteration 35, loss = 0.0009160161716863513
iteration 36, loss = 0.0012675370089709759
iteration 37, loss = 0.0017584019806236029
iteration 38, loss = 0.000987101811915636
iteration 39, loss = 0.001106632873415947
iteration 40, loss = 0.0007582808611914515
iteration 41, loss = 0.0008210844825953245
iteration 42, loss = 0.0006252736202441156
iteration 43, loss = 0.0008753153379075229
iteration 44, loss = 0.0007839708123356104
iteration 45, loss = 0.0007007456151768565
iteration 46, loss = 0.0006971847615204751
iteration 47, loss = 0.0011518362443894148
iteration 48, loss = 0.0008833110332489014
iteration 49, loss = 0.0010983546962961555
iteration 50, loss = 0.0011650735978037119
iteration 51, loss = 0.0009938199073076248
iteration 52, loss = 0.000700261676684022
iteration 53, loss = 0.0008664141641929746
iteration 54, loss = 0.0008571192156523466
iteration 55, loss = 0.0016540726646780968
iteration 56, loss = 0.0007269073976203799
iteration 57, loss = 0.0007282228907570243
iteration 58, loss = 0.0016573785105720162
iteration 59, loss = 0.0007580199744552374
iteration 60, loss = 0.0007202246342785656
iteration 61, loss = 0.0011644249316304922
iteration 62, loss = 0.0007541377563029528
iteration 63, loss = 0.000698945252224803
iteration 64, loss = 0.0009169671684503555
iteration 65, loss = 0.0007139001972973347
iteration 66, loss = 0.0009571987320668995
iteration 67, loss = 0.0018525774357840419
iteration 68, loss = 0.0009053139947354794
iteration 69, loss = 0.0011671726824715734
iteration 70, loss = 0.0015405096346512437
iteration 71, loss = 0.0008074683137238026
iteration 72, loss = 0.0008701424230821431
iteration 73, loss = 0.0010021281195804477
iteration 74, loss = 0.0008763408404774964
iteration 75, loss = 0.000755941029638052
iteration 76, loss = 0.0011577213881537318
iteration 77, loss = 0.0008221623720601201
iteration 78, loss = 0.0008305765804834664
iteration 79, loss = 0.001297987881116569
iteration 80, loss = 0.0008159137796610594
iteration 81, loss = 0.0008561522699892521
iteration 82, loss = 0.0009129618993028998
iteration 83, loss = 0.0008039686945267022
iteration 84, loss = 0.0007995684863999486
iteration 85, loss = 0.0012336382642388344
iteration 86, loss = 0.0008579522254876792
iteration 87, loss = 0.0006980778998695314
iteration 88, loss = 0.0015014528762549162
iteration 89, loss = 0.0007388422382064164
iteration 90, loss = 0.0009670163271948695
iteration 91, loss = 0.0008349183481186628
iteration 92, loss = 0.0007231207564473152
iteration 93, loss = 0.0011204268084838986
iteration 94, loss = 0.0012225534301251173
iteration 95, loss = 0.0017691553803160787
iteration 96, loss = 0.0010105699766427279
iteration 97, loss = 0.000670079723931849
iteration 98, loss = 0.0008962884894572198
iteration 99, loss = 0.0016734866658225656
iteration 100, loss = 0.000714445486664772
iteration 101, loss = 0.000773282372392714
iteration 102, loss = 0.00156437570694834
iteration 103, loss = 0.0010510890278965235
iteration 104, loss = 0.0010478667682036757
iteration 105, loss = 0.0011412875028327107
iteration 106, loss = 0.0007725676405243576
iteration 107, loss = 0.0007223017164506018
iteration 108, loss = 0.0007535085314884782
iteration 109, loss = 0.0009353432105854154
iteration 110, loss = 0.0008102933061309159
iteration 111, loss = 0.0012266162084415555
iteration 112, loss = 0.0009351002518087626
iteration 113, loss = 0.001609266153536737
iteration 114, loss = 0.0007927027763798833
iteration 115, loss = 0.0009300870005972683
iteration 116, loss = 0.0010801649186760187
iteration 117, loss = 0.000883831235114485
iteration 118, loss = 0.0014891027240082622
iteration 119, loss = 0.0010588362347334623
iteration 120, loss = 0.0007544599357061088
iteration 121, loss = 0.0007725732866674662
iteration 122, loss = 0.0009197692852467299
iteration 123, loss = 0.0008788244449533522
iteration 124, loss = 0.000790343270637095
iteration 125, loss = 0.0009200726053677499
iteration 126, loss = 0.0008352393051609397
iteration 127, loss = 0.0009366027661599219
iteration 128, loss = 0.0006811615894548595
iteration 129, loss = 0.00088866101577878
iteration 130, loss = 0.0007184555288404226
iteration 131, loss = 0.0013869579415768385
iteration 132, loss = 0.0015151072293519974
iteration 133, loss = 0.0010275471722707152
iteration 134, loss = 0.0008722975035198033
iteration 135, loss = 0.0007563515100628138
iteration 136, loss = 0.0011264339555054903
iteration 137, loss = 0.0008163771708495915
iteration 138, loss = 0.0006981766782701015
iteration 139, loss = 0.0010950453579425812
iteration 140, loss = 0.0007406054646708071
iteration 141, loss = 0.0006895377882756293
iteration 142, loss = 0.0010008489480242133
iteration 143, loss = 0.000819653389044106
iteration 144, loss = 0.0008756488095968962
iteration 145, loss = 0.0008206319762393832
iteration 146, loss = 0.0009147922974079847
iteration 147, loss = 0.0007974915206432343
iteration 148, loss = 0.0010179500095546246
iteration 149, loss = 0.0010750257642939687
iteration 150, loss = 0.0013227154267951846
iteration 151, loss = 0.0008600510191172361
iteration 152, loss = 0.0008397633209824562
iteration 153, loss = 0.0009851722279563546
iteration 154, loss = 0.0007107297424226999
iteration 155, loss = 0.0008786419639363885
iteration 156, loss = 0.0008083888096734881
iteration 157, loss = 0.0007836700533516705
iteration 158, loss = 0.000882542401086539
iteration 159, loss = 0.0010968373389914632
iteration 160, loss = 0.0008856880594976246
iteration 161, loss = 0.0011067223967984319
iteration 162, loss = 0.0008914421196095645
iteration 163, loss = 0.0011229028459638357
iteration 164, loss = 0.0007254816591739655
iteration 165, loss = 0.0009517499711364508
iteration 166, loss = 0.001438334584236145
iteration 167, loss = 0.0010023459326475859
iteration 168, loss = 0.0008353882585652173
iteration 169, loss = 0.0008561236900277436
iteration 170, loss = 0.0009577696328051388
iteration 171, loss = 0.0006863905000500381
iteration 172, loss = 0.0008963685249909759
iteration 173, loss = 0.0007419823086820543
iteration 174, loss = 0.0011425752891227603
iteration 175, loss = 0.0010185260325670242
iteration 176, loss = 0.00092118582688272
iteration 177, loss = 0.0014554341323673725
iteration 178, loss = 0.0010926466202363372
iteration 179, loss = 0.0010825740173459053
iteration 180, loss = 0.0007955076289363205
iteration 181, loss = 0.0015382490819320083
iteration 182, loss = 0.0007494407473132014
iteration 183, loss = 0.0007575304480269551
iteration 184, loss = 0.0007830724935047328
iteration 185, loss = 0.0008147273911163211
iteration 186, loss = 0.0008479503449052572
iteration 187, loss = 0.000780300993937999
iteration 188, loss = 0.0013357941061258316
iteration 189, loss = 0.0008376569603569806
iteration 190, loss = 0.000711338478140533
iteration 191, loss = 0.0009876862168312073
iteration 192, loss = 0.0009002549923025072
iteration 193, loss = 0.000989530817605555
iteration 194, loss = 0.0017904724227264524
iteration 195, loss = 0.0009506974020041525
iteration 196, loss = 0.0006830135826021433
iteration 197, loss = 0.0008149244822561741
iteration 198, loss = 0.0009926222264766693
iteration 199, loss = 0.0014405292458832264
iteration 200, loss = 0.0007412860286422074
iteration 201, loss = 0.0010715181706473231
iteration 202, loss = 0.0009282603277824819
iteration 203, loss = 0.0008897114894352853
iteration 204, loss = 0.001046994817443192
iteration 205, loss = 0.0011616891715675592
iteration 206, loss = 0.0007941210060380399
iteration 207, loss = 0.0010528330458328128
iteration 208, loss = 0.0009616578463464975
iteration 209, loss = 0.0009929193183779716
iteration 210, loss = 0.0007217826205305755
iteration 211, loss = 0.000757893081754446
iteration 212, loss = 0.0009240643121302128
iteration 213, loss = 0.0008120593265630305
iteration 214, loss = 0.0014042401453480124
iteration 215, loss = 0.0009909328073263168
iteration 216, loss = 0.000777097069658339
iteration 217, loss = 0.0010923888767138124
iteration 218, loss = 0.0007943888194859028
iteration 219, loss = 0.000805984775070101
iteration 220, loss = 0.0013544561807066202
iteration 221, loss = 0.0006576780579052866
iteration 222, loss = 0.0008049352909438312
iteration 223, loss = 0.001059165340848267
iteration 224, loss = 0.0007424186915159225
iteration 225, loss = 0.0009285867563448846
iteration 226, loss = 0.0007238785037770867
iteration 227, loss = 0.0009114783024415374
iteration 228, loss = 0.0013601841637864709
iteration 229, loss = 0.0008314503938890994
iteration 230, loss = 0.000643103732727468
iteration 231, loss = 0.0009658259223215282
iteration 232, loss = 0.0008293378632515669
iteration 233, loss = 0.0009286621352657676
iteration 234, loss = 0.0010518885683268309
iteration 235, loss = 0.0006042359163984656
iteration 236, loss = 0.0008530987543053925
iteration 237, loss = 0.0007602632977068424
iteration 238, loss = 0.0007250271155498922
iteration 239, loss = 0.0007627269951626658
iteration 240, loss = 0.0006623256485909224
iteration 241, loss = 0.0015063059981912374
iteration 242, loss = 0.0010274971136823297
iteration 243, loss = 0.0010100789368152618
iteration 244, loss = 0.001331806997768581
iteration 245, loss = 0.0007579047814942896
iteration 246, loss = 0.0008055094513110816
iteration 247, loss = 0.0008711810223758221
iteration 248, loss = 0.0007579238736070693
iteration 249, loss = 0.0011762515641748905
iteration 250, loss = 0.000887709145899862
iteration 251, loss = 0.0008333171717822552
iteration 252, loss = 0.0007822688785381615
iteration 253, loss = 0.001599430339410901
iteration 254, loss = 0.0007948125712573528
iteration 255, loss = 0.0007983003742992878
iteration 256, loss = 0.0007493283483199775
iteration 257, loss = 0.0008627810748293996
iteration 258, loss = 0.0010762911988422275
iteration 259, loss = 0.0009253969183191657
iteration 260, loss = 0.0007715696701779962
iteration 261, loss = 0.0006363128195516765
iteration 262, loss = 0.0008884271956048906
iteration 263, loss = 0.0013160011731088161
iteration 264, loss = 0.0015474696410819888
iteration 265, loss = 0.001663512666709721
iteration 266, loss = 0.0008832902531139553
iteration 267, loss = 0.0007607360021211207
iteration 268, loss = 0.0009838234400376678
iteration 269, loss = 0.0006877388223074377
iteration 270, loss = 0.0013734170934185386
iteration 271, loss = 0.0008481330005452037
iteration 272, loss = 0.0007601212128065526
iteration 273, loss = 0.0009257998317480087
iteration 274, loss = 0.0007376951398327947
iteration 275, loss = 0.0009503619512543082
iteration 276, loss = 0.0008457264630123973
iteration 277, loss = 0.0007305248873308301
iteration 278, loss = 0.0008088503964245319
iteration 279, loss = 0.0008568578632548451
iteration 280, loss = 0.000648158835247159
iteration 281, loss = 0.0015457593835890293
iteration 282, loss = 0.0010696338722482324
iteration 283, loss = 0.0011123601580038667
iteration 284, loss = 0.0011919353855773807
iteration 285, loss = 0.001014153123833239
iteration 286, loss = 0.0008016604697331786
iteration 287, loss = 0.0007648171740584075
iteration 288, loss = 0.0009267725399695337
iteration 289, loss = 0.001366366632282734
iteration 290, loss = 0.0013956543989479542
iteration 291, loss = 0.0012529399245977402
iteration 292, loss = 0.0015542162582278252
iteration 293, loss = 0.0013620387762784958
iteration 294, loss = 0.0009102734620682895
iteration 295, loss = 0.0008620275184512138
iteration 296, loss = 0.0010921460343524814
iteration 297, loss = 0.001024338766001165
iteration 298, loss = 0.0008849968435242772
iteration 299, loss = 0.0006979503086768091
iteration 300, loss = 0.000994223984889686
iteration 1, loss = 0.0009221751242876053
iteration 2, loss = 0.0007727895281277597
iteration 3, loss = 0.001077667810022831
iteration 4, loss = 0.0007692386861890554
iteration 5, loss = 0.0007778775761835277
iteration 6, loss = 0.0008118076366372406
iteration 7, loss = 0.0016052935970947146
iteration 8, loss = 0.0008871209574863315
iteration 9, loss = 0.0008041021064855158
iteration 10, loss = 0.0008802358061075211
iteration 11, loss = 0.0010267216712236404
iteration 12, loss = 0.0008151273941621184
iteration 13, loss = 0.0008134585805237293
iteration 14, loss = 0.00103051180485636
iteration 15, loss = 0.0006700295489281416
iteration 16, loss = 0.0008811915176920593
iteration 17, loss = 0.0007912024739198387
iteration 18, loss = 0.0010792353423312306
iteration 19, loss = 0.0009403853910043836
iteration 20, loss = 0.0010601015528663993
iteration 21, loss = 0.0009532573749311268
iteration 22, loss = 0.0008842340903356671
iteration 23, loss = 0.0007220736588351429
iteration 24, loss = 0.0008389324066229165
iteration 25, loss = 0.0009085349156521261
iteration 26, loss = 0.0006774563225917518
iteration 27, loss = 0.0007866937085054815
iteration 28, loss = 0.0007709962083026767
iteration 29, loss = 0.0012233875459060073
iteration 30, loss = 0.0006691892631351948
iteration 31, loss = 0.0007251577917486429
iteration 32, loss = 0.0010397545993328094
iteration 33, loss = 0.0011220776941627264
iteration 34, loss = 0.002252477454021573
iteration 35, loss = 0.0007290662615559995
iteration 36, loss = 0.000877278158441186
iteration 37, loss = 0.0007290481007657945
iteration 38, loss = 0.0009648044942878187
iteration 39, loss = 0.0009697695495560765
iteration 40, loss = 0.000931885908357799
iteration 41, loss = 0.0009684343822300434
iteration 42, loss = 0.0007995448540896177
iteration 43, loss = 0.0006719428347423673
iteration 44, loss = 0.0009178314940072596
iteration 45, loss = 0.0016148170689120889
iteration 46, loss = 0.0007981295348145068
iteration 47, loss = 0.0009605849627405405
iteration 48, loss = 0.0007218769751489162
iteration 49, loss = 0.0007328713545575738
iteration 50, loss = 0.0007110503502190113
iteration 51, loss = 0.0007018364267423749
iteration 52, loss = 0.0006934815901331604
iteration 53, loss = 0.0013929344713687897
iteration 54, loss = 0.0007745407056063414
iteration 55, loss = 0.0006866088951937854
iteration 56, loss = 0.0015657702460885048
iteration 57, loss = 0.0010472566355019808
iteration 58, loss = 0.001872168155387044
iteration 59, loss = 0.0010454872390255332
iteration 60, loss = 0.0007514690514653921
iteration 61, loss = 0.0008631551754660904
iteration 62, loss = 0.001518590492196381
iteration 63, loss = 0.000619030324742198
iteration 64, loss = 0.000752795662265271
iteration 65, loss = 0.0007889186963438988
iteration 66, loss = 0.0008996504475362599
iteration 67, loss = 0.0009293609182350338
iteration 68, loss = 0.0008626377675682306
iteration 69, loss = 0.0015352924820035696
iteration 70, loss = 0.0015403854195028543
iteration 71, loss = 0.0017325268127024174
iteration 72, loss = 0.0009101111209020019
iteration 73, loss = 0.0008207203354686499
iteration 74, loss = 0.0009349780739285052
iteration 75, loss = 0.0008546209428459406
iteration 76, loss = 0.0009455297840759158
iteration 77, loss = 0.0008370851865038276
iteration 78, loss = 0.001090770703740418
iteration 79, loss = 0.0011032577604055405
iteration 80, loss = 0.0012086452916264534
iteration 81, loss = 0.0016116019105538726
iteration 82, loss = 0.0007221942651085556
iteration 83, loss = 0.0009061964810825884
iteration 84, loss = 0.0008469099411740899
iteration 85, loss = 0.0008314181468449533
iteration 86, loss = 0.0011177289998158813
iteration 87, loss = 0.0007275737007148564
iteration 88, loss = 0.0007060063071548939
iteration 89, loss = 0.0008121691062115133
iteration 90, loss = 0.001237153890542686
iteration 91, loss = 0.0009262253879569471
iteration 92, loss = 0.0007876213057897985
iteration 93, loss = 0.0008513040957041085
iteration 94, loss = 0.00109466805588454
iteration 95, loss = 0.00079021614510566
iteration 96, loss = 0.0007189959869720042
iteration 97, loss = 0.0008770549902692437
iteration 98, loss = 0.0015199717599898577
iteration 99, loss = 0.0007133146282285452
iteration 100, loss = 0.0008352099684998393
iteration 101, loss = 0.000698323012329638
iteration 102, loss = 0.0009518231963738799
iteration 103, loss = 0.0008635495323687792
iteration 104, loss = 0.0007793908007442951
iteration 105, loss = 0.0011359242489561439
iteration 106, loss = 0.0010411133989691734
iteration 107, loss = 0.0008025153074413538
iteration 108, loss = 0.0008334248559549451
iteration 109, loss = 0.0009551994153298438
iteration 110, loss = 0.000890132156200707
iteration 111, loss = 0.0008914719219319522
iteration 112, loss = 0.0007138103246688843
iteration 113, loss = 0.001466832123696804
iteration 114, loss = 0.0009228397393599153
iteration 115, loss = 0.0007934132590889931
iteration 116, loss = 0.0007901792996563017
iteration 117, loss = 0.001204917673021555
iteration 118, loss = 0.0013824229827150702
iteration 119, loss = 0.0007282492006197572
iteration 120, loss = 0.0011389657156541944
iteration 121, loss = 0.0008458264055661857
iteration 122, loss = 0.0011144650634378195
iteration 123, loss = 0.0008205142221413553
iteration 124, loss = 0.0007129668956622481
iteration 125, loss = 0.0008832854218780994
iteration 126, loss = 0.0007823330233804882
iteration 127, loss = 0.0008219287265092134
iteration 128, loss = 0.0007120562950149179
iteration 129, loss = 0.0006956793367862701
iteration 130, loss = 0.002354112220928073
iteration 131, loss = 0.0010943689849227667
iteration 132, loss = 0.0008167462074197829
iteration 133, loss = 0.0008508137543685734
iteration 134, loss = 0.0017839621286839247
iteration 135, loss = 0.0008722381899133325
iteration 136, loss = 0.0010613655904307961
iteration 137, loss = 0.0009080682648345828
iteration 138, loss = 0.0007526948465965688
iteration 139, loss = 0.0007475337479263544
iteration 140, loss = 0.0009183160145767033
iteration 141, loss = 0.000738713308237493
iteration 142, loss = 0.0009694688487797976
iteration 143, loss = 0.000923442596103996
iteration 144, loss = 0.0010290952632203698
iteration 145, loss = 0.0007776237325742841
iteration 146, loss = 0.0011553174117580056
iteration 147, loss = 0.000915927579626441
iteration 148, loss = 0.0009710047743283212
iteration 149, loss = 0.0018565121572464705
iteration 150, loss = 0.0007800979074090719
iteration 151, loss = 0.0011574613163247705
iteration 152, loss = 0.0009469467913731933
iteration 153, loss = 0.0011093091452494264
iteration 154, loss = 0.0007964208489283919
iteration 155, loss = 0.0011311954585835338
iteration 156, loss = 0.000939966004807502
iteration 157, loss = 0.0008357969927601516
iteration 158, loss = 0.0009929470252245665
iteration 159, loss = 0.0011375551111996174
iteration 160, loss = 0.0011874499032273889
iteration 161, loss = 0.0009356130613014102
iteration 162, loss = 0.0009418356930837035
iteration 163, loss = 0.0008259797468781471
iteration 164, loss = 0.0008291070116683841
iteration 165, loss = 0.0008110222406685352
iteration 166, loss = 0.0009449718054383993
iteration 167, loss = 0.0008562220609746873
iteration 168, loss = 0.000855307443998754
iteration 169, loss = 0.001376844709739089
iteration 170, loss = 0.0008706897497177124
iteration 171, loss = 0.000983353704214096
iteration 172, loss = 0.0007677972898818552
iteration 173, loss = 0.0016727785114198923
iteration 174, loss = 0.001048641512170434
iteration 175, loss = 0.0008627374190837145
iteration 176, loss = 0.0008011579629965127
iteration 177, loss = 0.0009997382294386625
iteration 178, loss = 0.0020487471483647823
iteration 179, loss = 0.0008634791593067348
iteration 180, loss = 0.00112453056499362
iteration 181, loss = 0.0007684063748456538
iteration 182, loss = 0.0007395240827463567
iteration 183, loss = 0.0011344656813889742
iteration 184, loss = 0.0006245474796742201
iteration 185, loss = 0.0010779805015772581
iteration 186, loss = 0.0021287889685481787
iteration 187, loss = 0.0009834982920438051
iteration 188, loss = 0.0015726630808785558
iteration 189, loss = 0.0008620278676971793
iteration 190, loss = 0.000729549617972225
iteration 191, loss = 0.0008377820486202836
iteration 192, loss = 0.0008392778690904379
iteration 193, loss = 0.0011967511381953955
iteration 194, loss = 0.00099180918186903
iteration 195, loss = 0.0008528344333171844
iteration 196, loss = 0.0010348742362111807
iteration 197, loss = 0.0014048261800780892
iteration 198, loss = 0.0007995886262506247
iteration 199, loss = 0.0007680237758904696
iteration 200, loss = 0.0009339051321148872
iteration 201, loss = 0.0010202792473137379
iteration 202, loss = 0.0007555353222414851
iteration 203, loss = 0.001552848145365715
iteration 204, loss = 0.001635615131817758
iteration 205, loss = 0.0011693862034007907
iteration 206, loss = 0.0011141877621412277
iteration 207, loss = 0.000859713414683938
iteration 208, loss = 0.0008368734852410853
iteration 209, loss = 0.0011809604475274682
iteration 210, loss = 0.0007866082014515996
iteration 211, loss = 0.0010310113430023193
iteration 212, loss = 0.0008110719500109553
iteration 213, loss = 0.0007419704343192279
iteration 214, loss = 0.0007796021527610719
iteration 215, loss = 0.0010120614897459745
iteration 216, loss = 0.0011526489397510886
iteration 217, loss = 0.00087035505566746
iteration 218, loss = 0.0009876122931018472
iteration 219, loss = 0.0007508781854994595
iteration 220, loss = 0.0007560320664197206
iteration 221, loss = 0.0007683828589506447
iteration 222, loss = 0.0011020831298083067
iteration 223, loss = 0.0008824041578918695
iteration 224, loss = 0.0006810378981754184
iteration 225, loss = 0.0007521266816183925
iteration 226, loss = 0.0011328820837661624
iteration 227, loss = 0.0012490706285461783
iteration 228, loss = 0.0021979541052132845
iteration 229, loss = 0.0009734414634294808
iteration 230, loss = 0.000767429475672543
iteration 231, loss = 0.0008908672025427222
iteration 232, loss = 0.0009855908574536443
iteration 233, loss = 0.0008416683995164931
iteration 234, loss = 0.0008092442876659334
iteration 235, loss = 0.0014370864955708385
iteration 236, loss = 0.0006609619595110416
iteration 237, loss = 0.0009999517351388931
iteration 238, loss = 0.0008253393461927772
iteration 239, loss = 0.0007265630993060768
iteration 240, loss = 0.0010059084743261337
iteration 241, loss = 0.0007666790625080466
iteration 242, loss = 0.0007412921986542642
iteration 243, loss = 0.0007537868223153055
iteration 244, loss = 0.0008780941716395319
iteration 245, loss = 0.0007188827148638666
iteration 246, loss = 0.001680193468928337
iteration 247, loss = 0.0008186810300685465
iteration 248, loss = 0.0006758483941666782
iteration 249, loss = 0.0008095358498394489
iteration 250, loss = 0.0010789749212563038
iteration 251, loss = 0.0008812104351818562
iteration 252, loss = 0.0010341163724660873
iteration 253, loss = 0.000831249519251287
iteration 254, loss = 0.0008245804347097874
iteration 255, loss = 0.0007845755899325013
iteration 256, loss = 0.0007550058653578162
iteration 257, loss = 0.0010641387198120356
iteration 258, loss = 0.000780796050094068
iteration 259, loss = 0.001349288271740079
iteration 260, loss = 0.0008323545334860682
iteration 261, loss = 0.0007695799577049911
iteration 262, loss = 0.001026015030220151
iteration 263, loss = 0.0012335599167272449
iteration 264, loss = 0.0007883759681135416
iteration 265, loss = 0.0012303537223488092
iteration 266, loss = 0.0007521121297031641
iteration 267, loss = 0.0007992438622750342
iteration 268, loss = 0.000796132895629853
iteration 269, loss = 0.0012972768163308501
iteration 270, loss = 0.0007331963279284537
iteration 271, loss = 0.0011130885686725378
iteration 272, loss = 0.0008615798433311284
iteration 273, loss = 0.0008293563732877374
iteration 274, loss = 0.0008316959138028324
iteration 275, loss = 0.0008631302043795586
iteration 276, loss = 0.001108446391299367
iteration 277, loss = 0.000941086676903069
iteration 278, loss = 0.001281603006646037
iteration 279, loss = 0.0008383001550100744
iteration 280, loss = 0.0008156960248015821
iteration 281, loss = 0.0011225834023207426
iteration 282, loss = 0.0007755804108455777
iteration 283, loss = 0.0008290753467008471
iteration 284, loss = 0.0007673734799027443
iteration 285, loss = 0.000865051697473973
iteration 286, loss = 0.0009807167807593942
iteration 287, loss = 0.0008707843371666968
iteration 288, loss = 0.000773418287280947
iteration 289, loss = 0.001324263634160161
iteration 290, loss = 0.0007644624565728009
iteration 291, loss = 0.0007417271262966096
iteration 292, loss = 0.0008841818198561668
iteration 293, loss = 0.0015362517442554235
iteration 294, loss = 0.000709189975168556
iteration 295, loss = 0.0009515252313576639
iteration 296, loss = 0.0007468372350558639
iteration 297, loss = 0.0009913535322993994
iteration 298, loss = 0.0007601852994412184
iteration 299, loss = 0.0010125893168151379
iteration 300, loss = 0.0007395003922283649
iteration 1, loss = 0.0009386898018419743
iteration 2, loss = 0.001103667775169015
iteration 3, loss = 0.0008419833029620349
iteration 4, loss = 0.0017207026248797774
iteration 5, loss = 0.0008824166725389659
iteration 6, loss = 0.0009293734328821301
iteration 7, loss = 0.0014318330213427544
iteration 8, loss = 0.0010199095122516155
iteration 9, loss = 0.000746898353099823
iteration 10, loss = 0.0008982038707472384
iteration 11, loss = 0.0007455689483322203
iteration 12, loss = 0.0008316418388858438
iteration 13, loss = 0.0007183110574260354
iteration 14, loss = 0.0012708990834653378
iteration 15, loss = 0.0015492867678403854
iteration 16, loss = 0.0019489608239382505
iteration 17, loss = 0.0008057532832026482
iteration 18, loss = 0.0009385397424921393
iteration 19, loss = 0.0006179627380333841
iteration 20, loss = 0.0006824673037044704
iteration 21, loss = 0.0015183259965851903
iteration 22, loss = 0.0009246250847354531
iteration 23, loss = 0.0009638001210987568
iteration 24, loss = 0.0007897410541772842
iteration 25, loss = 0.0008989665657281876
iteration 26, loss = 0.0007737415726296604
iteration 27, loss = 0.0007373646949417889
iteration 28, loss = 0.0006964514032006264
iteration 29, loss = 0.0008561814902350307
iteration 30, loss = 0.0008965531596913934
iteration 31, loss = 0.0008581186993978918
iteration 32, loss = 0.0007672406500205398
iteration 33, loss = 0.001278384355828166
iteration 34, loss = 0.0008116409881040454
iteration 35, loss = 0.0015007543843239546
iteration 36, loss = 0.0010849281679838896
iteration 37, loss = 0.0007301316945813596
iteration 38, loss = 0.000786885735578835
iteration 39, loss = 0.0009847560431808233
iteration 40, loss = 0.0008360226638615131
iteration 41, loss = 0.0007072927546687424
iteration 42, loss = 0.0007527669076807797
iteration 43, loss = 0.0010442998027428985
iteration 44, loss = 0.0007081562071107328
iteration 45, loss = 0.0006954716518521309
iteration 46, loss = 0.0010714734671637416
iteration 47, loss = 0.0007866494124755263
iteration 48, loss = 0.001409004209563136
iteration 49, loss = 0.0011240235762670636
iteration 50, loss = 0.001175573794171214
iteration 51, loss = 0.0010539990616962314
iteration 52, loss = 0.0006899788277223706
iteration 53, loss = 0.0015561298932880163
iteration 54, loss = 0.0007351841195486486
iteration 55, loss = 0.0008154307724907994
iteration 56, loss = 0.000992360757663846
iteration 57, loss = 0.0014651661040261388
iteration 58, loss = 0.0007934364257380366
iteration 59, loss = 0.0008791622240096331
iteration 60, loss = 0.0007664446020498872
iteration 61, loss = 0.0007244800217449665
iteration 62, loss = 0.0009682726813480258
iteration 63, loss = 0.0009244306129403412
iteration 64, loss = 0.0011624796316027641
iteration 65, loss = 0.0012713457690551877
iteration 66, loss = 0.0008508029859513044
iteration 67, loss = 0.0008278411696664989
iteration 68, loss = 0.0008208926301449537
iteration 69, loss = 0.0009417837718501687
iteration 70, loss = 0.0008492086199112236
iteration 71, loss = 0.0007526247063651681
iteration 72, loss = 0.0007535823970101774
iteration 73, loss = 0.0007938987691886723
iteration 74, loss = 0.0009091095416806638
iteration 75, loss = 0.001253425027243793
iteration 76, loss = 0.0008074107463471591
iteration 77, loss = 0.0008861628011800349
iteration 78, loss = 0.000925734406337142
iteration 79, loss = 0.000762484036386013
iteration 80, loss = 0.0018033933592960238
iteration 81, loss = 0.0009475415572524071
iteration 82, loss = 0.001143117668107152
iteration 83, loss = 0.0009288130095228553
iteration 84, loss = 0.000815677922219038
iteration 85, loss = 0.0006966649671085179
iteration 86, loss = 0.0008638636791147292
iteration 87, loss = 0.001136125880293548
iteration 88, loss = 0.0006358512328006327
iteration 89, loss = 0.0009669782593846321
iteration 90, loss = 0.0010273680090904236
iteration 91, loss = 0.0018303586402907968
iteration 92, loss = 0.0008967857575044036
iteration 93, loss = 0.0011374446330592036
iteration 94, loss = 0.0008177108829841018
iteration 95, loss = 0.0009362065466120839
iteration 96, loss = 0.0008316299645230174
iteration 97, loss = 0.0007602544501423836
iteration 98, loss = 0.0011699497699737549
iteration 99, loss = 0.0007494601886719465
iteration 100, loss = 0.0015644141240045428
iteration 101, loss = 0.0008190127555280924
iteration 102, loss = 0.0009806571761146188
iteration 103, loss = 0.0007094285683706403
iteration 104, loss = 0.0010045478120446205
iteration 105, loss = 0.0008134375093504786
iteration 106, loss = 0.0007703402079641819
iteration 107, loss = 0.0006333152414299548
iteration 108, loss = 0.0015870890347287059
iteration 109, loss = 0.001060260459780693
iteration 110, loss = 0.0009326388826593757
iteration 111, loss = 0.0008445254643447697
iteration 112, loss = 0.0007102064555510879
iteration 113, loss = 0.0007896581082604825
iteration 114, loss = 0.0010497967014089227
iteration 115, loss = 0.0011444267584010959
iteration 116, loss = 0.0007152725011110306
iteration 117, loss = 0.0010768526699393988
iteration 118, loss = 0.0008125533349812031
iteration 119, loss = 0.0008315114537253976
iteration 120, loss = 0.0008495230576954782
iteration 121, loss = 0.0006574601866304874
iteration 122, loss = 0.0007417242741212249
iteration 123, loss = 0.000938776764087379
iteration 124, loss = 0.0007971379673108459
iteration 125, loss = 0.0009831571951508522
iteration 126, loss = 0.0010350404772907495
iteration 127, loss = 0.0016388362273573875
iteration 128, loss = 0.0008270683465525508
iteration 129, loss = 0.000972417532466352
iteration 130, loss = 0.0010807170765474439
iteration 131, loss = 0.000923659885302186
iteration 132, loss = 0.0007293380331248045
iteration 133, loss = 0.0008223705808632076
iteration 134, loss = 0.0006060830783098936
iteration 135, loss = 0.0008817432099021971
iteration 136, loss = 0.000707102648448199
iteration 137, loss = 0.0009412352810613811
iteration 138, loss = 0.0009316218202002347
iteration 139, loss = 0.0008819264476187527
iteration 140, loss = 0.0016467105597257614
iteration 141, loss = 0.0009071409585885704
iteration 142, loss = 0.0011320033809170127
iteration 143, loss = 0.0006963564665056765
iteration 144, loss = 0.0010222898563370109
iteration 145, loss = 0.0006315982900559902
iteration 146, loss = 0.002183240605518222
iteration 147, loss = 0.0017305577639490366
iteration 148, loss = 0.0012428744230419397
iteration 149, loss = 0.000832199992146343
iteration 150, loss = 0.001278662821277976
iteration 151, loss = 0.0008658607257530093
iteration 152, loss = 0.0014617868000641465
iteration 153, loss = 0.0007782868924550712
iteration 154, loss = 0.0008061373373493552
iteration 155, loss = 0.0008373702876269817
iteration 156, loss = 0.0010550280567258596
iteration 157, loss = 0.0011180524015799165
iteration 158, loss = 0.0007466789102181792
iteration 159, loss = 0.0017789313569664955
iteration 160, loss = 0.00100193009711802
iteration 161, loss = 0.0008834563777782023
iteration 162, loss = 0.0009673334425315261
iteration 163, loss = 0.0010409645037725568
iteration 164, loss = 0.0007780775194987655
iteration 165, loss = 0.0008821783703751862
iteration 166, loss = 0.0009097606525756419
iteration 167, loss = 0.0009434905950911343
iteration 168, loss = 0.0008071677875705063
iteration 169, loss = 0.0008896887302398682
iteration 170, loss = 0.0007488885894417763
iteration 171, loss = 0.0007486873073503375
iteration 172, loss = 0.00102732062805444
iteration 173, loss = 0.000854288402479142
iteration 174, loss = 0.0008307937532663345
iteration 175, loss = 0.0008944879518821836
iteration 176, loss = 0.0011110738851130009
iteration 177, loss = 0.0012314410414546728
iteration 178, loss = 0.000874831632245332
iteration 179, loss = 0.0008409707224927843
iteration 180, loss = 0.0008995335665531456
iteration 181, loss = 0.0008994798408821225
iteration 182, loss = 0.0008041393011808395
iteration 183, loss = 0.0007065223762765527
iteration 184, loss = 0.0008069577161222696
iteration 185, loss = 0.0011237047147005796
iteration 186, loss = 0.002058309968560934
iteration 187, loss = 0.0008173492969945073
iteration 188, loss = 0.0007650615880265832
iteration 189, loss = 0.0010878860484808683
iteration 190, loss = 0.0008945222944021225
iteration 191, loss = 0.0007758501451462507
iteration 192, loss = 0.0008018834050744772
iteration 193, loss = 0.0008951982599683106
iteration 194, loss = 0.0013833737466484308
iteration 195, loss = 0.0010982942767441273
iteration 196, loss = 0.0011660659220069647
iteration 197, loss = 0.0008501308620907366
iteration 198, loss = 0.0008169186767190695
iteration 199, loss = 0.0009501393069513142
iteration 200, loss = 0.0007229078328236938
iteration 201, loss = 0.000741915253456682
iteration 202, loss = 0.0007438378524966538
iteration 203, loss = 0.0009562504710629582
iteration 204, loss = 0.0011516849044710398
iteration 205, loss = 0.0007444934453815222
iteration 206, loss = 0.001291103777475655
iteration 207, loss = 0.0008629184449091554
iteration 208, loss = 0.001366691431030631
iteration 209, loss = 0.0010417207377031446
iteration 210, loss = 0.0007951773004606366
iteration 211, loss = 0.0009431427461095154
iteration 212, loss = 0.0007931674481369555
iteration 213, loss = 0.00113021582365036
iteration 214, loss = 0.0014618029817938805
iteration 215, loss = 0.0006924927001819015
iteration 216, loss = 0.0009092096006497741
iteration 217, loss = 0.0006394637748599052
iteration 218, loss = 0.0008322473731823266
iteration 219, loss = 0.000988522544503212
iteration 220, loss = 0.000799572269897908
iteration 221, loss = 0.0010484388330951333
iteration 222, loss = 0.001123082940466702
iteration 223, loss = 0.0008784206002019346
iteration 224, loss = 0.0006263063987717032
iteration 225, loss = 0.0006810689228586853
iteration 226, loss = 0.000944916158914566
iteration 227, loss = 0.001065634423866868
iteration 228, loss = 0.001056763343513012
iteration 229, loss = 0.0008063933346420527
iteration 230, loss = 0.001037806156091392
iteration 231, loss = 0.0008058452513068914
iteration 232, loss = 0.0008375682518817484
iteration 233, loss = 0.0012920488370582461
iteration 234, loss = 0.0014390230644494295
iteration 235, loss = 0.0007676737732253969
iteration 236, loss = 0.000883385248016566
iteration 237, loss = 0.0016235135262832046
iteration 238, loss = 0.0006857570842839777
iteration 239, loss = 0.0014222916215658188
iteration 240, loss = 0.000938323384616524
iteration 241, loss = 0.0010191946057602763
iteration 242, loss = 0.0008205780759453773
iteration 243, loss = 0.0009411495993845165
iteration 244, loss = 0.000878092716448009
iteration 245, loss = 0.0011135416571050882
iteration 246, loss = 0.0008877606596797705
iteration 247, loss = 0.0008463232079520822
iteration 248, loss = 0.0008418008219450712
iteration 249, loss = 0.0008241490577347577
iteration 250, loss = 0.0008047386654652655
iteration 251, loss = 0.0007801116444170475
iteration 252, loss = 0.0006893530953675508
iteration 253, loss = 0.000816214713267982
iteration 254, loss = 0.0009587777312844992
iteration 255, loss = 0.0008864197297953069
iteration 256, loss = 0.0008680774481035769
iteration 257, loss = 0.0008129810448735952
iteration 258, loss = 0.0011657284339889884
iteration 259, loss = 0.001204516738653183
iteration 260, loss = 0.0008846553973853588
iteration 261, loss = 0.0008966365130618215
iteration 262, loss = 0.000958786578848958
iteration 263, loss = 0.0007873093127273023
iteration 264, loss = 0.0007595504866912961
iteration 265, loss = 0.0008014845661818981
iteration 266, loss = 0.000996468821540475
iteration 267, loss = 0.0016451466362923384
iteration 268, loss = 0.0007692977087572217
iteration 269, loss = 0.0014816020848229527
iteration 270, loss = 0.0007729856297373772
iteration 271, loss = 0.0007845840882509947
iteration 272, loss = 0.0007987971184775233
iteration 273, loss = 0.0008050748147070408
iteration 274, loss = 0.0008233575499616563
iteration 275, loss = 0.0007923393277451396
iteration 276, loss = 0.0007903206278569996
iteration 277, loss = 0.0013045236701145768
iteration 278, loss = 0.001181581406854093
iteration 279, loss = 0.0010162702528759837
iteration 280, loss = 0.0007948292186483741
iteration 281, loss = 0.0009029008797369897
iteration 282, loss = 0.0018829221371561289
iteration 283, loss = 0.0014618763234466314
iteration 284, loss = 0.0007469971315003932
iteration 285, loss = 0.0008401898667216301
iteration 286, loss = 0.0008637815481051803
iteration 287, loss = 0.0008373709861189127
iteration 288, loss = 0.0010640567634254694
iteration 289, loss = 0.001197977690026164
iteration 290, loss = 0.0008978154510259628
iteration 291, loss = 0.0016721662832424045
iteration 292, loss = 0.0011821177322417498
iteration 293, loss = 0.0009768448071554303
iteration 294, loss = 0.0007577079813927412
iteration 295, loss = 0.0007173027843236923
iteration 296, loss = 0.0008209560764953494
iteration 297, loss = 0.000834057223983109
iteration 298, loss = 0.0008874570485204458
iteration 299, loss = 0.0008610166842117906
iteration 300, loss = 0.000839570420794189
iteration 1, loss = 0.0007145598065108061
iteration 2, loss = 0.0012044130126014352
iteration 3, loss = 0.0008319964399561286
iteration 4, loss = 0.0007124831317923963
iteration 5, loss = 0.0007131039747036994
iteration 6, loss = 0.001060323789715767
iteration 7, loss = 0.0010433943243697286
iteration 8, loss = 0.0009623503428883851
iteration 9, loss = 0.0007901101489551365
iteration 10, loss = 0.0008603425230830908
iteration 11, loss = 0.0010001909686252475
iteration 12, loss = 0.0007656618254259229
iteration 13, loss = 0.0007722721784375608
iteration 14, loss = 0.001376321422867477
iteration 15, loss = 0.0008749871049076319
iteration 16, loss = 0.0007753208628855646
iteration 17, loss = 0.0007105586701072752
iteration 18, loss = 0.0006697102217003703
iteration 19, loss = 0.000644451763946563
iteration 20, loss = 0.000788554665632546
iteration 21, loss = 0.0010078094201162457
iteration 22, loss = 0.001192022580653429
iteration 23, loss = 0.0016016117297112942
iteration 24, loss = 0.0007677217945456505
iteration 25, loss = 0.001092360122129321
iteration 26, loss = 0.0008147454354912043
iteration 27, loss = 0.0006962137995287776
iteration 28, loss = 0.0008785676909610629
iteration 29, loss = 0.0006774820503778756
iteration 30, loss = 0.0007843813509680331
iteration 31, loss = 0.0007279394776560366
iteration 32, loss = 0.0007487599505111575
iteration 33, loss = 0.0007225520676001906
iteration 34, loss = 0.000779592024628073
iteration 35, loss = 0.0006599380285479128
iteration 36, loss = 0.0007214319775812328
iteration 37, loss = 0.0006947251968085766
iteration 38, loss = 0.0011477535590529442
iteration 39, loss = 0.0010681613348424435
iteration 40, loss = 0.0013121459633111954
iteration 41, loss = 0.000827210140414536
iteration 42, loss = 0.001114075188525021
iteration 43, loss = 0.0008909616735763848
iteration 44, loss = 0.0017317519523203373
iteration 45, loss = 0.0007989968871697783
iteration 46, loss = 0.0010588486911728978
iteration 47, loss = 0.0008038609521463513
iteration 48, loss = 0.0019758003763854504
iteration 49, loss = 0.0008887563599273562
iteration 50, loss = 0.0014484833227470517
iteration 51, loss = 0.0009320419630967081
iteration 52, loss = 0.0009549043606966734
iteration 53, loss = 0.0008863034890964627
iteration 54, loss = 0.0007975245243869722
iteration 55, loss = 0.001714913873001933
iteration 56, loss = 0.0009860447607934475
iteration 57, loss = 0.0008673383854329586
iteration 58, loss = 0.0017827536212280393
iteration 59, loss = 0.0007377506699413061
iteration 60, loss = 0.002043848391622305
iteration 61, loss = 0.0007332435343414545
iteration 62, loss = 0.0007881421479396522
iteration 63, loss = 0.0011874770279973745
iteration 64, loss = 0.0007976856431923807
iteration 65, loss = 0.0008598456042818725
iteration 66, loss = 0.0007411367259919643
iteration 67, loss = 0.0013051177375018597
iteration 68, loss = 0.0008172718225978315
iteration 69, loss = 0.0007573675829917192
iteration 70, loss = 0.0007083669770509005
iteration 71, loss = 0.0008527361205779016
iteration 72, loss = 0.0011551189236342907
iteration 73, loss = 0.0012453309027478099
iteration 74, loss = 0.001475570839829743
iteration 75, loss = 0.0008876562933437526
iteration 76, loss = 0.0010445228544995189
iteration 77, loss = 0.00226985989138484
iteration 78, loss = 0.0007447322714142501
iteration 79, loss = 0.0006899591535329819
iteration 80, loss = 0.0008253435953520238
iteration 81, loss = 0.0007159952074289322
iteration 82, loss = 0.0008566323667764664
iteration 83, loss = 0.001372273312881589
iteration 84, loss = 0.0007253551739268005
iteration 85, loss = 0.0008446122519671917
iteration 86, loss = 0.0007063888479024172
iteration 87, loss = 0.0009976096916943789
iteration 88, loss = 0.0009005815954878926
iteration 89, loss = 0.0007944831741042435
iteration 90, loss = 0.0009463411406613886
iteration 91, loss = 0.0008266009972430766
iteration 92, loss = 0.0008490387117490172
iteration 93, loss = 0.0009440671419724822
iteration 94, loss = 0.0008451855974271894
iteration 95, loss = 0.00086074392311275
iteration 96, loss = 0.0009690751903690398
iteration 97, loss = 0.0014718016609549522
iteration 98, loss = 0.0008705268846824765
iteration 99, loss = 0.0011921377154067159
iteration 100, loss = 0.0008541803108528256
iteration 101, loss = 0.0008005750714801252
iteration 102, loss = 0.0008912628400139511
iteration 103, loss = 0.0009076148271560669
iteration 104, loss = 0.0009327204315923154
iteration 105, loss = 0.0009417944820597768
iteration 106, loss = 0.0013345071347430348
iteration 107, loss = 0.0007771846139803529
iteration 108, loss = 0.0010522010270506144
iteration 109, loss = 0.0006888389470987022
iteration 110, loss = 0.0008377816993743181
iteration 111, loss = 0.000737938389647752
iteration 112, loss = 0.0013540578074753284
iteration 113, loss = 0.0012175588635727763
iteration 114, loss = 0.0008802572265267372
iteration 115, loss = 0.0009975690627470613
iteration 116, loss = 0.0010879406472668052
iteration 117, loss = 0.0008139447309076786
iteration 118, loss = 0.0007989178993739188
iteration 119, loss = 0.0012671365402638912
iteration 120, loss = 0.001081555150449276
iteration 121, loss = 0.0010847712401300669
iteration 122, loss = 0.0015742807881906629
iteration 123, loss = 0.0006795512745156884
iteration 124, loss = 0.0007980024092830718
iteration 125, loss = 0.0007449471740983427
iteration 126, loss = 0.0009788487805053592
iteration 127, loss = 0.0012188778491690755
iteration 128, loss = 0.0007062535150907934
iteration 129, loss = 0.0015098047442734241
iteration 130, loss = 0.0009338767267763615
iteration 131, loss = 0.002070156391710043
iteration 132, loss = 0.0009167833486571908
iteration 133, loss = 0.0007823201012797654
iteration 134, loss = 0.0008254841668531299
iteration 135, loss = 0.0006508612423203886
iteration 136, loss = 0.0007330071530304849
iteration 137, loss = 0.001028594677336514
iteration 138, loss = 0.0009631238644942641
iteration 139, loss = 0.0007662344141863286
iteration 140, loss = 0.0010764807229861617
iteration 141, loss = 0.0008360522333532572
iteration 142, loss = 0.0007923945086076856
iteration 143, loss = 0.0008799484930932522
iteration 144, loss = 0.0012032057857140899
iteration 145, loss = 0.0007014815346337855
iteration 146, loss = 0.0010382095351815224
iteration 147, loss = 0.000758346461225301
iteration 148, loss = 0.0010420801118016243
iteration 149, loss = 0.0009452666854485869
iteration 150, loss = 0.0011817105114459991
iteration 151, loss = 0.0009200978092849255
iteration 152, loss = 0.0008837907225824893
iteration 153, loss = 0.0007764381589367986
iteration 154, loss = 0.0009952811524271965
iteration 155, loss = 0.0008444464765489101
iteration 156, loss = 0.0007483381195925176
iteration 157, loss = 0.0008547299075871706
iteration 158, loss = 0.0007607301231473684
iteration 159, loss = 0.0007689334452152252
iteration 160, loss = 0.0008202709723263979
iteration 161, loss = 0.0006067232461646199
iteration 162, loss = 0.0018584368517622352
iteration 163, loss = 0.000993083929643035
iteration 164, loss = 0.0009168508695438504
iteration 165, loss = 0.0007298171985894442
iteration 166, loss = 0.0012124920031055808
iteration 167, loss = 0.0007782176835462451
iteration 168, loss = 0.000926043139770627
iteration 169, loss = 0.0006722589605487883
iteration 170, loss = 0.0008143078302964568
iteration 171, loss = 0.0010718487901613116
iteration 172, loss = 0.0007809716043993831
iteration 173, loss = 0.0018414836376905441
iteration 174, loss = 0.0008257438894361258
iteration 175, loss = 0.0007112086750566959
iteration 176, loss = 0.0008141369908116758
iteration 177, loss = 0.001303524011746049
iteration 178, loss = 0.0007785739726386964
iteration 179, loss = 0.0011484670685604215
iteration 180, loss = 0.0009346904116682708
iteration 181, loss = 0.0008114386000670493
iteration 182, loss = 0.0008598403655923903
iteration 183, loss = 0.0008346846443600953
iteration 184, loss = 0.0014800716890022159
iteration 185, loss = 0.000723508361261338
iteration 186, loss = 0.0008007458527572453
iteration 187, loss = 0.0010350727243348956
iteration 188, loss = 0.0008984282612800598
iteration 189, loss = 0.0008096836972981691
iteration 190, loss = 0.0007795387646183372
iteration 191, loss = 0.0009337860392406583
iteration 192, loss = 0.0015737609937787056
iteration 193, loss = 0.0008917603408917785
iteration 194, loss = 0.0008219070150516927
iteration 195, loss = 0.0007981746457517147
iteration 196, loss = 0.0006820954149588943
iteration 197, loss = 0.0008280675392597914
iteration 198, loss = 0.0007788017392158508
iteration 199, loss = 0.0012662900844588876
iteration 200, loss = 0.0010687769390642643
iteration 201, loss = 0.0007912453729659319
iteration 202, loss = 0.0008974639931693673
iteration 203, loss = 0.0009079012088477612
iteration 204, loss = 0.000831184268463403
iteration 205, loss = 0.0016139257932081819
iteration 206, loss = 0.0008517673704773188
iteration 207, loss = 0.0010425985092297196
iteration 208, loss = 0.0014591157669201493
iteration 209, loss = 0.00106222671456635
iteration 210, loss = 0.0009311586618423462
iteration 211, loss = 0.0008241535979323089
iteration 212, loss = 0.0009891183581203222
iteration 213, loss = 0.0006458438001573086
iteration 214, loss = 0.0007885062368586659
iteration 215, loss = 0.0006328094750642776
iteration 216, loss = 0.0007419504690915346
iteration 217, loss = 0.0010948609560728073
iteration 218, loss = 0.0007339064031839371
iteration 219, loss = 0.0009813630022108555
iteration 220, loss = 0.0008882244001142681
iteration 221, loss = 0.0007623056299053133
iteration 222, loss = 0.0011150010395795107
iteration 223, loss = 0.0016894612926989794
iteration 224, loss = 0.000906618544831872
iteration 225, loss = 0.0008513733628205955
iteration 226, loss = 0.0009680568473413587
iteration 227, loss = 0.000857153965625912
iteration 228, loss = 0.0008610539953224361
iteration 229, loss = 0.0007855255389586091
iteration 230, loss = 0.000905667373444885
iteration 231, loss = 0.0007574532646685839
iteration 232, loss = 0.0007704221643507481
iteration 233, loss = 0.0008511848282068968
iteration 234, loss = 0.0007884602528065443
iteration 235, loss = 0.0008264192729257047
iteration 236, loss = 0.0007348379585891962
iteration 237, loss = 0.0006060975138098001
iteration 238, loss = 0.0007903720834292471
iteration 239, loss = 0.0007496888283640146
iteration 240, loss = 0.0010126999113708735
iteration 241, loss = 0.0010182042606174946
iteration 242, loss = 0.001208884292282164
iteration 243, loss = 0.002389416564255953
iteration 244, loss = 0.0009635635651648045
iteration 245, loss = 0.0008919685496948659
iteration 246, loss = 0.0010275887325406075
iteration 247, loss = 0.0007701864815317094
iteration 248, loss = 0.001220313599333167
iteration 249, loss = 0.0007152652833610773
iteration 250, loss = 0.0008771445718593895
iteration 251, loss = 0.001052050618454814
iteration 252, loss = 0.0008355252793990076
iteration 253, loss = 0.0019601637031883
iteration 254, loss = 0.0008969207992777228
iteration 255, loss = 0.0012850177008658648
iteration 256, loss = 0.0019998231437057257
iteration 257, loss = 0.0016110532451421022
iteration 258, loss = 0.0019793498795479536
iteration 259, loss = 0.0007425512303598225
iteration 260, loss = 0.0010918175103142858
iteration 261, loss = 0.001038057147525251
iteration 262, loss = 0.0009573270799592137
iteration 263, loss = 0.0009155562147498131
iteration 264, loss = 0.0007557153585366905
iteration 265, loss = 0.0007177742081694305
iteration 266, loss = 0.0009101127507165074
iteration 267, loss = 0.0009146479424089193
iteration 268, loss = 0.0008253388223238289
iteration 269, loss = 0.0007741670124232769
iteration 270, loss = 0.0009641816141083837
iteration 271, loss = 0.0008928538300096989
iteration 272, loss = 0.0009981365874409676
iteration 273, loss = 0.0007913164445199072
iteration 274, loss = 0.0008399293874390423
iteration 275, loss = 0.0007880211342126131
iteration 276, loss = 0.000852513883728534
iteration 277, loss = 0.0007369343074969947
iteration 278, loss = 0.0007923621451482177
iteration 279, loss = 0.0010096421465277672
iteration 280, loss = 0.0008380918297916651
iteration 281, loss = 0.0008077426464296877
iteration 282, loss = 0.0008753774454817176
iteration 283, loss = 0.000796075735706836
iteration 284, loss = 0.0008412115857936442
iteration 285, loss = 0.0008448137086816132
iteration 286, loss = 0.0010681301355361938
iteration 287, loss = 0.0016241701086983085
iteration 288, loss = 0.00087029451970011
iteration 289, loss = 0.0007941462099552155
iteration 290, loss = 0.0009807741735130548
iteration 291, loss = 0.0012755783973261714
iteration 292, loss = 0.0010918221669271588
iteration 293, loss = 0.0009093451662920415
iteration 294, loss = 0.0010848445817828178
iteration 295, loss = 0.00070450350176543
iteration 296, loss = 0.0006833947845734656
iteration 297, loss = 0.0008193879039026797
iteration 298, loss = 0.0009317027288489044
iteration 299, loss = 0.001332513289526105
iteration 300, loss = 0.0008058907696977258
iteration 1, loss = 0.0007788821822032332
iteration 2, loss = 0.0006693830946460366
iteration 3, loss = 0.0016438498860225081
iteration 4, loss = 0.0010739763965830207
iteration 5, loss = 0.0007865696679800749
iteration 6, loss = 0.0010716129327192903
iteration 7, loss = 0.0007035218295641243
iteration 8, loss = 0.0011042222613468766
iteration 9, loss = 0.001090289675630629
iteration 10, loss = 0.0006414257222786546
iteration 11, loss = 0.001094036502763629
iteration 12, loss = 0.0008894656202755868
iteration 13, loss = 0.0008245153585448861
iteration 14, loss = 0.000977776595391333
iteration 15, loss = 0.0011190890800207853
iteration 16, loss = 0.0007027789833955467
iteration 17, loss = 0.0007869323017075658
iteration 18, loss = 0.0010011898120865226
iteration 19, loss = 0.0008182345191016793
iteration 20, loss = 0.000808514014352113
iteration 21, loss = 0.0008191878441721201
iteration 22, loss = 0.000846938812173903
iteration 23, loss = 0.0016999748768284917
iteration 24, loss = 0.000728091923519969
iteration 25, loss = 0.0009500720771029592
iteration 26, loss = 0.0009737704531289637
iteration 27, loss = 0.000828552816528827
iteration 28, loss = 0.0007882661884650588
iteration 29, loss = 0.0011379484785720706
iteration 30, loss = 0.0011236185673624277
iteration 31, loss = 0.0007777480059303343
iteration 32, loss = 0.0008270928519777954
iteration 33, loss = 0.0007792616379447281
iteration 34, loss = 0.0008135407697409391
iteration 35, loss = 0.0015673252055421472
iteration 36, loss = 0.0009922768222168088
iteration 37, loss = 0.0007719749119132757
iteration 38, loss = 0.0007740163127891719
iteration 39, loss = 0.0009921184973791242
iteration 40, loss = 0.000856226310133934
iteration 41, loss = 0.0007607111474499106
iteration 42, loss = 0.0009310078457929194
iteration 43, loss = 0.0010320395231246948
iteration 44, loss = 0.0008088158210739493
iteration 45, loss = 0.0007332043023779988
iteration 46, loss = 0.0012040382716804743
iteration 47, loss = 0.0011598983546718955
iteration 48, loss = 0.0009993374114856124
iteration 49, loss = 0.0011999792186543345
iteration 50, loss = 0.000641602324321866
iteration 51, loss = 0.0007299130083993077
iteration 52, loss = 0.00109111238270998
iteration 53, loss = 0.0007260041311383247
iteration 54, loss = 0.001455523306503892
iteration 55, loss = 0.000844832684379071
iteration 56, loss = 0.0007756880368106067
iteration 57, loss = 0.0007443423965014517
iteration 58, loss = 0.0007094516768120229
iteration 59, loss = 0.0009829418268054724
iteration 60, loss = 0.0011015953496098518
iteration 61, loss = 0.0007295336108654737
iteration 62, loss = 0.0008266825461760163
iteration 63, loss = 0.0010972159216180444
iteration 64, loss = 0.0015009259805083275
iteration 65, loss = 0.000897594727575779
iteration 66, loss = 0.0018199555343016982
iteration 67, loss = 0.0007296836702153087
iteration 68, loss = 0.0016071496065706015
iteration 69, loss = 0.0007600003154948354
iteration 70, loss = 0.0009388148901052773
iteration 71, loss = 0.000918201869353652
iteration 72, loss = 0.0006674622418358922
iteration 73, loss = 0.0009912200039252639
iteration 74, loss = 0.0008024652488529682
iteration 75, loss = 0.000938831246457994
iteration 76, loss = 0.0008772205328568816
iteration 77, loss = 0.001134637393988669
iteration 78, loss = 0.0007801480824127793
iteration 79, loss = 0.0017414066242054105
iteration 80, loss = 0.0010084360837936401
iteration 81, loss = 0.0017522067064419389
iteration 82, loss = 0.0011070368345826864
iteration 83, loss = 0.0016715324018150568
iteration 84, loss = 0.0009086777572520077
iteration 85, loss = 0.0008374783210456371
iteration 86, loss = 0.0006454208050854504
iteration 87, loss = 0.0009068141807802022
iteration 88, loss = 0.0015016492689028382
iteration 89, loss = 0.0014007878489792347
iteration 90, loss = 0.0007785395719110966
iteration 91, loss = 0.0010260558919981122
iteration 92, loss = 0.0009876168332993984
iteration 93, loss = 0.0008830513106659055
iteration 94, loss = 0.0011942703276872635
iteration 95, loss = 0.0008058057865127921
iteration 96, loss = 0.0007588136941194534
iteration 97, loss = 0.001614115433767438
iteration 98, loss = 0.0010732089867815375
iteration 99, loss = 0.0011986683821305633
iteration 100, loss = 0.000902895350009203
iteration 101, loss = 0.0008241632021963596
iteration 102, loss = 0.0008529843180440366
iteration 103, loss = 0.001483266823925078
iteration 104, loss = 0.0007165497518144548
iteration 105, loss = 0.0008003354887478054
iteration 106, loss = 0.0008293536957353354
iteration 107, loss = 0.0010856851004064083
iteration 108, loss = 0.0007625814760103822
iteration 109, loss = 0.0018053242238238454
iteration 110, loss = 0.0007630439358763397
iteration 111, loss = 0.0006787502206861973
iteration 112, loss = 0.0008768225670792162
iteration 113, loss = 0.000718484865501523
iteration 114, loss = 0.0008059287793003023
iteration 115, loss = 0.0009107070509344339
iteration 116, loss = 0.0011208949144929647
iteration 117, loss = 0.0009571212576702237
iteration 118, loss = 0.0009742385009303689
iteration 119, loss = 0.0010121753439307213
iteration 120, loss = 0.0007262990111485124
iteration 121, loss = 0.0009079814772121608
iteration 122, loss = 0.0007244766457006335
iteration 123, loss = 0.001657558255828917
iteration 124, loss = 0.0008508067112416029
iteration 125, loss = 0.0009831041097640991
iteration 126, loss = 0.0008049682364799082
iteration 127, loss = 0.0006808747421018779
iteration 128, loss = 0.0018724917899817228
iteration 129, loss = 0.0011172157246619463
iteration 130, loss = 0.0008642646134831011
iteration 131, loss = 0.0008822386153042316
iteration 132, loss = 0.0015564251225441694
iteration 133, loss = 0.0006875642575323582
iteration 134, loss = 0.0010575050255283713
iteration 135, loss = 0.0007630044128745794
iteration 136, loss = 0.0013990721199661493
iteration 137, loss = 0.0016852235421538353
iteration 138, loss = 0.0012206390965729952
iteration 139, loss = 0.0008687442750670016
iteration 140, loss = 0.0007547838613390923
iteration 141, loss = 0.0008899586391635239
iteration 142, loss = 0.0008769689593464136
iteration 143, loss = 0.0007044915109872818
iteration 144, loss = 0.0012527385260909796
iteration 145, loss = 0.0009061511373147368
iteration 146, loss = 0.00110698735807091
iteration 147, loss = 0.000770377111621201
iteration 148, loss = 0.0008786218240857124
iteration 149, loss = 0.001046271761879325
iteration 150, loss = 0.0007561906240880489
iteration 151, loss = 0.000853219535201788
iteration 152, loss = 0.00122286775149405
iteration 153, loss = 0.000840514141600579
iteration 154, loss = 0.0010039841290563345
iteration 155, loss = 0.0008033086196519434
iteration 156, loss = 0.0006921312306076288
iteration 157, loss = 0.0015143119962885976
iteration 158, loss = 0.0005811368464492261
iteration 159, loss = 0.000945980311371386
iteration 160, loss = 0.0008702065097168088
iteration 161, loss = 0.0007035791059024632
iteration 162, loss = 0.000898521626368165
iteration 163, loss = 0.0006580011686310172
iteration 164, loss = 0.000712296983692795
iteration 165, loss = 0.0015564511995762587
iteration 166, loss = 0.0016466062515974045
iteration 167, loss = 0.001197807607240975
iteration 168, loss = 0.0010695287492126226
iteration 169, loss = 0.0007840501493774354
iteration 170, loss = 0.00119446637108922
iteration 171, loss = 0.0008940607076510787
iteration 172, loss = 0.0009609192493371665
iteration 173, loss = 0.0008247892837971449
iteration 174, loss = 0.0008246157085523009
iteration 175, loss = 0.0010806007776409388
iteration 176, loss = 0.0007248218753375113
iteration 177, loss = 0.0008474214700981975
iteration 178, loss = 0.0008110232884064317
iteration 179, loss = 0.0007162369438447058
iteration 180, loss = 0.0006854899693280458
iteration 181, loss = 0.0008330832934007049
iteration 182, loss = 0.001140796346589923
iteration 183, loss = 0.0007651532650925219
iteration 184, loss = 0.0006667525158263743
iteration 185, loss = 0.0007867256063036621
iteration 186, loss = 0.0007356017013080418
iteration 187, loss = 0.0006793767097406089
iteration 188, loss = 0.0008402738021686673
iteration 189, loss = 0.0008195997797884047
iteration 190, loss = 0.0008521404815837741
iteration 191, loss = 0.0009743034024722874
iteration 192, loss = 0.0007874745642766356
iteration 193, loss = 0.000807934848126024
iteration 194, loss = 0.0007687401375733316
iteration 195, loss = 0.0007796026184223592
iteration 196, loss = 0.0009579937905073166
iteration 197, loss = 0.0006621601642109454
iteration 198, loss = 0.0012141111074015498
iteration 199, loss = 0.000932750990614295
iteration 200, loss = 0.0008724983781576157
iteration 201, loss = 0.0007942937081679702
iteration 202, loss = 0.0010124673135578632
iteration 203, loss = 0.0009213424636982381
iteration 204, loss = 0.0007961516967043281
iteration 205, loss = 0.0008226021309383214
iteration 206, loss = 0.0007931764703243971
iteration 207, loss = 0.0007794410339556634
iteration 208, loss = 0.0008355878526344895
iteration 209, loss = 0.000680403143633157
iteration 210, loss = 0.0007065836107358336
iteration 211, loss = 0.0013162847608327866
iteration 212, loss = 0.0008639537263661623
iteration 213, loss = 0.0010683301370590925
iteration 214, loss = 0.0008569551864638925
iteration 215, loss = 0.0015427382895722985
iteration 216, loss = 0.0008767229155637324
iteration 217, loss = 0.0010895172599703074
iteration 218, loss = 0.0007875572191551328
iteration 219, loss = 0.0007934533059597015
iteration 220, loss = 0.0009281236561946571
iteration 221, loss = 0.0007105798576958477
iteration 222, loss = 0.0009106933721341193
iteration 223, loss = 0.0006708350847475231
iteration 224, loss = 0.001059790956787765
iteration 225, loss = 0.0016970558790490031
iteration 226, loss = 0.0008615897968411446
iteration 227, loss = 0.0010899256449192762
iteration 228, loss = 0.0009494788246229291
iteration 229, loss = 0.0008632008684799075
iteration 230, loss = 0.0008758794283494353
iteration 231, loss = 0.001179622020572424
iteration 232, loss = 0.0009213356534019113
iteration 233, loss = 0.0007875849842093885
iteration 234, loss = 0.0012561975745484233
iteration 235, loss = 0.0008634900441393256
iteration 236, loss = 0.000903522886801511
iteration 237, loss = 0.0007926573161967099
iteration 238, loss = 0.0007103823591023684
iteration 239, loss = 0.0008683268679305911
iteration 240, loss = 0.0009753457270562649
iteration 241, loss = 0.0007936888723634183
iteration 242, loss = 0.000928762776311487
iteration 243, loss = 0.0011574513046070933
iteration 244, loss = 0.0009464543545618653
iteration 245, loss = 0.0012980556348338723
iteration 246, loss = 0.001526793115772307
iteration 247, loss = 0.00083565479144454
iteration 248, loss = 0.0007913938607089221
iteration 249, loss = 0.0007866499945521355
iteration 250, loss = 0.0011687747901305556
iteration 251, loss = 0.001005893456749618
iteration 252, loss = 0.0008184764883480966
iteration 253, loss = 0.0008161966688930988
iteration 254, loss = 0.000909969792701304
iteration 255, loss = 0.0009372202912345529
iteration 256, loss = 0.0010019524488598108
iteration 257, loss = 0.000797838089056313
iteration 258, loss = 0.0006448678905144334
iteration 259, loss = 0.001010323059745133
iteration 260, loss = 0.000849955074954778
iteration 261, loss = 0.0008534746011719108
iteration 262, loss = 0.0008759952033869922
iteration 263, loss = 0.0008969542104750872
iteration 264, loss = 0.0010349205695092678
iteration 265, loss = 0.000716971349902451
iteration 266, loss = 0.001381470705382526
iteration 267, loss = 0.0007386318175122142
iteration 268, loss = 0.001698788721114397
iteration 269, loss = 0.0011749122058972716
iteration 270, loss = 0.0009218449704349041
iteration 271, loss = 0.001048584352247417
iteration 272, loss = 0.0011364798992872238
iteration 273, loss = 0.0006788637838326395
iteration 274, loss = 0.000820221786852926
iteration 275, loss = 0.0010892180725932121
iteration 276, loss = 0.0013046814128756523
iteration 277, loss = 0.0008842397946864367
iteration 278, loss = 0.0007609832100570202
iteration 279, loss = 0.001081658760085702
iteration 280, loss = 0.0007277866243384778
iteration 281, loss = 0.000836690072901547
iteration 282, loss = 0.0007008836255408823
iteration 283, loss = 0.0015351040055975318
iteration 284, loss = 0.0007958415662869811
iteration 285, loss = 0.0007190629839897156
iteration 286, loss = 0.001414095633663237
iteration 287, loss = 0.0014638622524216771
iteration 288, loss = 0.001232505077496171
iteration 289, loss = 0.0007984070107340813
iteration 290, loss = 0.0007167295552790165
iteration 291, loss = 0.0011479441309347749
iteration 292, loss = 0.0007546148262917995
iteration 293, loss = 0.0018010652856901288
iteration 294, loss = 0.0007429186371155083
iteration 295, loss = 0.0015938279684633017
iteration 296, loss = 0.0009694229229353368
iteration 297, loss = 0.0008197721908800304
iteration 298, loss = 0.0008977738907560706
iteration 299, loss = 0.0007966608973219991
iteration 300, loss = 0.0007596368668600917
iteration 1, loss = 0.0008202693425118923
iteration 2, loss = 0.0007574370829388499
iteration 3, loss = 0.0009093820117413998
iteration 4, loss = 0.0007257162942551076
iteration 5, loss = 0.0008095838711597025
iteration 6, loss = 0.001016975031234324
iteration 7, loss = 0.0009359529940411448
iteration 8, loss = 0.0008630804368294775
iteration 9, loss = 0.0008019933011382818
iteration 10, loss = 0.001314082182943821
iteration 11, loss = 0.0008870905730873346
iteration 12, loss = 0.0006859960849396884
iteration 13, loss = 0.001005222206003964
iteration 14, loss = 0.0006518439622595906
iteration 15, loss = 0.001040946925058961
iteration 16, loss = 0.0008641426684334874
iteration 17, loss = 0.0011904669227078557
iteration 18, loss = 0.0009769561002030969
iteration 19, loss = 0.0008519127732142806
iteration 20, loss = 0.000966323830652982
iteration 21, loss = 0.0007831082330085337
iteration 22, loss = 0.0007055113092064857
iteration 23, loss = 0.0008807406993582845
iteration 24, loss = 0.0007367321522906423
iteration 25, loss = 0.000766503217164427
iteration 26, loss = 0.0008849334553815424
iteration 27, loss = 0.001624161028303206
iteration 28, loss = 0.0010141347302123904
iteration 29, loss = 0.0010515577159821987
iteration 30, loss = 0.0008672813419252634
iteration 31, loss = 0.0007611484616063535
iteration 32, loss = 0.0009468402713537216
iteration 33, loss = 0.0008494321373291314
iteration 34, loss = 0.0006991843692958355
iteration 35, loss = 0.001139060826972127
iteration 36, loss = 0.0010636562947183847
iteration 37, loss = 0.0008181392913684249
iteration 38, loss = 0.0007889815606176853
iteration 39, loss = 0.0007490550051443279
iteration 40, loss = 0.0009001306025311351
iteration 41, loss = 0.0009526021894998848
iteration 42, loss = 0.0006963897030800581
iteration 43, loss = 0.0011346200481057167
iteration 44, loss = 0.0010408374946564436
iteration 45, loss = 0.0015566414222121239
iteration 46, loss = 0.0014232889516279101
iteration 47, loss = 0.0007290939684025943
iteration 48, loss = 0.0008531744824722409
iteration 49, loss = 0.0007987861754372716
iteration 50, loss = 0.0009864174062386155
iteration 51, loss = 0.0007808558293618262
iteration 52, loss = 0.001268015126697719
iteration 53, loss = 0.0006703408434987068
iteration 54, loss = 0.000841187487822026
iteration 55, loss = 0.0008956234669312835
iteration 56, loss = 0.0006830560159869492
iteration 57, loss = 0.0006793366628699005
iteration 58, loss = 0.0008883621194399893
iteration 59, loss = 0.0010765065671876073
iteration 60, loss = 0.0009461329318583012
iteration 61, loss = 0.0007842537597753108
iteration 62, loss = 0.0008518623653799295
iteration 63, loss = 0.000746019184589386
iteration 64, loss = 0.0009457291453145444
iteration 65, loss = 0.0009805274894461036
iteration 66, loss = 0.000936996890231967
iteration 67, loss = 0.0014810507418587804
iteration 68, loss = 0.0007784273475408554
iteration 69, loss = 0.0009843617444857955
iteration 70, loss = 0.0010776073904708028
iteration 71, loss = 0.0008383930544368923
iteration 72, loss = 0.0010685727465897799
iteration 73, loss = 0.0007379514281637967
iteration 74, loss = 0.0008947505266405642
iteration 75, loss = 0.0008580352296121418
iteration 76, loss = 0.000965537503361702
iteration 77, loss = 0.0014257174916565418
iteration 78, loss = 0.001026995130814612
iteration 79, loss = 0.0008089818875305355
iteration 80, loss = 0.0009086306090466678
iteration 81, loss = 0.0011288216337561607
iteration 82, loss = 0.0009620270575396717
iteration 83, loss = 0.002432970330119133
iteration 84, loss = 0.000724036421161145
iteration 85, loss = 0.0006972723640501499
iteration 86, loss = 0.0007825993234291673
iteration 87, loss = 0.0009397129178978503
iteration 88, loss = 0.0010104069951921701
iteration 89, loss = 0.0008796845213510096
iteration 90, loss = 0.0008786668768152595
iteration 91, loss = 0.0007950718281790614
iteration 92, loss = 0.0006236688350327313
iteration 93, loss = 0.0008224037592299283
iteration 94, loss = 0.0014390075812116265
iteration 95, loss = 0.0006317225634120405
iteration 96, loss = 0.0007820326136425138
iteration 97, loss = 0.0013465902302414179
iteration 98, loss = 0.0007567896973341703
iteration 99, loss = 0.0008324068621732295
iteration 100, loss = 0.0009088480146601796
iteration 101, loss = 0.0007492656586691737
iteration 102, loss = 0.0008691778057254851
iteration 103, loss = 0.000902217929251492
iteration 104, loss = 0.0009714200277812779
iteration 105, loss = 0.0008392173913307488
iteration 106, loss = 0.00129191973246634
iteration 107, loss = 0.0011919480748474598
iteration 108, loss = 0.000854491489008069
iteration 109, loss = 0.00099773402325809
iteration 110, loss = 0.0009692271705716848
iteration 111, loss = 0.0009551531402394176
iteration 112, loss = 0.0008161286823451519
iteration 113, loss = 0.000749091908801347
iteration 114, loss = 0.0008993533556349576
iteration 115, loss = 0.000888943555764854
iteration 116, loss = 0.002346995286643505
iteration 117, loss = 0.001532669411972165
iteration 118, loss = 0.0007212278433144093
iteration 119, loss = 0.0007523627136833966
iteration 120, loss = 0.0008222858305089176
iteration 121, loss = 0.0013859813334420323
iteration 122, loss = 0.0007815524004399776
iteration 123, loss = 0.0009336151997558773
iteration 124, loss = 0.0007910776766948402
iteration 125, loss = 0.0007511398289352655
iteration 126, loss = 0.0006538913585245609
iteration 127, loss = 0.0008064090507104993
iteration 128, loss = 0.0009172518621198833
iteration 129, loss = 0.0007820767932571471
iteration 130, loss = 0.0006411848007701337
iteration 131, loss = 0.0008836620254442096
iteration 132, loss = 0.0015119501622393727
iteration 133, loss = 0.0015135759022086859
iteration 134, loss = 0.000863095570821315
iteration 135, loss = 0.0008334830636158586
iteration 136, loss = 0.0008861647802405059
iteration 137, loss = 0.0021261845249682665
iteration 138, loss = 0.0010821543401107192
iteration 139, loss = 0.0008126804605126381
iteration 140, loss = 0.000932488648686558
iteration 141, loss = 0.0007541953236795962
iteration 142, loss = 0.0010454088915139437
iteration 143, loss = 0.0012300338130444288
iteration 144, loss = 0.0013127746060490608
iteration 145, loss = 0.0009178085019811988
iteration 146, loss = 0.0008063693530857563
iteration 147, loss = 0.0011304999934509397
iteration 148, loss = 0.0011152485385537148
iteration 149, loss = 0.0007274855161085725
iteration 150, loss = 0.0007935423054732382
iteration 151, loss = 0.0008212889079004526
iteration 152, loss = 0.0008106635068543255
iteration 153, loss = 0.0008963128784671426
iteration 154, loss = 0.0009283620165660977
iteration 155, loss = 0.0011893162736669183
iteration 156, loss = 0.0007650228217244148
iteration 157, loss = 0.0010273411171510816
iteration 158, loss = 0.00076124700717628
iteration 159, loss = 0.0008688715752214193
iteration 160, loss = 0.0007361762691289186
iteration 161, loss = 0.0008922541746869683
iteration 162, loss = 0.0010213165078312159
iteration 163, loss = 0.0007465326925739646
iteration 164, loss = 0.0007127065910026431
iteration 165, loss = 0.0008675124263390899
iteration 166, loss = 0.0009357187664136291
iteration 167, loss = 0.0007833151612430811
iteration 168, loss = 0.0010728850029408932
iteration 169, loss = 0.0012430958449840546
iteration 170, loss = 0.0014872713945806026
iteration 171, loss = 0.0013396111316978931
iteration 172, loss = 0.0010308058699592948
iteration 173, loss = 0.0014556561363860965
iteration 174, loss = 0.0009665577672421932
iteration 175, loss = 0.0008352891891263425
iteration 176, loss = 0.0007182730478234589
iteration 177, loss = 0.0008482980774715543
iteration 178, loss = 0.0016839997842907906
iteration 179, loss = 0.0011538872495293617
iteration 180, loss = 0.0006133687566034496
iteration 181, loss = 0.0008245987119153142
iteration 182, loss = 0.0007109346915967762
iteration 183, loss = 0.0007874298025853932
iteration 184, loss = 0.0014073430793359876
iteration 185, loss = 0.0008929542964324355
iteration 186, loss = 0.000772818922996521
iteration 187, loss = 0.0007939208298921585
iteration 188, loss = 0.000769208709243685
iteration 189, loss = 0.0017871006857603788
iteration 190, loss = 0.0009060297743417323
iteration 191, loss = 0.0007558607030659914
iteration 192, loss = 0.0007972206803970039
iteration 193, loss = 0.000875612604431808
iteration 194, loss = 0.001149721909314394
iteration 195, loss = 0.0010473431320860982
iteration 196, loss = 0.0015475938562303782
iteration 197, loss = 0.001374187646433711
iteration 198, loss = 0.0008498764364048839
iteration 199, loss = 0.001176724792458117
iteration 200, loss = 0.0009782244451344013
iteration 201, loss = 0.0010737467091530561
iteration 202, loss = 0.0021515488624572754
iteration 203, loss = 0.0008369880961254239
iteration 204, loss = 0.00144324847497046
iteration 205, loss = 0.0008326810202561319
iteration 206, loss = 0.0008070520707406104
iteration 207, loss = 0.0007161276880651712
iteration 208, loss = 0.0008175413822755218
iteration 209, loss = 0.001173841068521142
iteration 210, loss = 0.0007620449177920818
iteration 211, loss = 0.0007765645859763026
iteration 212, loss = 0.0008128663757815957
iteration 213, loss = 0.0008243416668847203
iteration 214, loss = 0.0009047566563822329
iteration 215, loss = 0.0019161025993525982
iteration 216, loss = 0.0008709856774657965
iteration 217, loss = 0.0015758125809952617
iteration 218, loss = 0.0009989943355321884
iteration 219, loss = 0.0008069233736023307
iteration 220, loss = 0.0007335774716921151
iteration 221, loss = 0.0008166195475496352
iteration 222, loss = 0.0015084664337337017
iteration 223, loss = 0.0008303929353132844
iteration 224, loss = 0.0009487387142144144
iteration 225, loss = 0.0007603435660712421
iteration 226, loss = 0.0012674827594310045
iteration 227, loss = 0.0008559750858694315
iteration 228, loss = 0.0007417406304739416
iteration 229, loss = 0.001166109461337328
iteration 230, loss = 0.0016097112093120813
iteration 231, loss = 0.0008020708337426186
iteration 232, loss = 0.0007305389735847712
iteration 233, loss = 0.0008653229451738298
iteration 234, loss = 0.0012642423389479518
iteration 235, loss = 0.0006316668586805463
iteration 236, loss = 0.001198644284158945
iteration 237, loss = 0.0007726493058726192
iteration 238, loss = 0.0007505316170863807
iteration 239, loss = 0.00084951042663306
iteration 240, loss = 0.001358674024231732
iteration 241, loss = 0.0007013598224148154
iteration 242, loss = 0.00101585837546736
iteration 243, loss = 0.0009141141781583428
iteration 244, loss = 0.0008275330765172839
iteration 245, loss = 0.0009620667551644146
iteration 246, loss = 0.0007343179895542562
iteration 247, loss = 0.0008625187329016626
iteration 248, loss = 0.0008193884277716279
iteration 249, loss = 0.000802757975179702
iteration 250, loss = 0.0010359497973695397
iteration 251, loss = 0.001403587288223207
iteration 252, loss = 0.0010732989758253098
iteration 253, loss = 0.0008828239515423775
iteration 254, loss = 0.0018548030639067292
iteration 255, loss = 0.0007165747229009867
iteration 256, loss = 0.0017003740649670362
iteration 257, loss = 0.0008909747703000903
iteration 258, loss = 0.0007809312082827091
iteration 259, loss = 0.0008559604175388813
iteration 260, loss = 0.0007725934265181422
iteration 261, loss = 0.0006754278438165784
iteration 262, loss = 0.0008557087276130915
iteration 263, loss = 0.0007237764075398445
iteration 264, loss = 0.0010195920476689935
iteration 265, loss = 0.0006466626655310392
iteration 266, loss = 0.001206171466037631
iteration 267, loss = 0.0007493011653423309
iteration 268, loss = 0.0007301302393898368
iteration 269, loss = 0.0006852008518762887
iteration 270, loss = 0.0015983926132321358
iteration 271, loss = 0.0009234069148078561
iteration 272, loss = 0.0010681868297979236
iteration 273, loss = 0.0008123008301481605
iteration 274, loss = 0.0008230864186771214
iteration 275, loss = 0.000710540683940053
iteration 276, loss = 0.0006944730994291604
iteration 277, loss = 0.0006949005764909089
iteration 278, loss = 0.0007630661712028086
iteration 279, loss = 0.0014147491892799735
iteration 280, loss = 0.00115691137034446
iteration 281, loss = 0.0008061863481998444
iteration 282, loss = 0.0008062890847213566
iteration 283, loss = 0.0006926412461325526
iteration 284, loss = 0.0016026078956201673
iteration 285, loss = 0.001044594799168408
iteration 286, loss = 0.0008157163392752409
iteration 287, loss = 0.0007050985586829484
iteration 288, loss = 0.0008416763739660382
iteration 289, loss = 0.0018865718739107251
iteration 290, loss = 0.001383417984470725
iteration 291, loss = 0.0007506307447329164
iteration 292, loss = 0.0008233276894316077
iteration 293, loss = 0.0008639039006084204
iteration 294, loss = 0.0006828439654782414
iteration 295, loss = 0.0008421606034971774
iteration 296, loss = 0.0010887791868299246
iteration 297, loss = 0.0008711267146281898
iteration 298, loss = 0.0006814358057454228
iteration 299, loss = 0.0007704030722379684
iteration 300, loss = 0.0008117847028188407
iteration 1, loss = 0.0012025302276015282
iteration 2, loss = 0.0009526836220175028
iteration 3, loss = 0.0007283889572136104
iteration 4, loss = 0.0009531539399176836
iteration 5, loss = 0.0006784865981899202
iteration 6, loss = 0.0008664585766382515
iteration 7, loss = 0.0006960302125662565
iteration 8, loss = 0.0011499087559059262
iteration 9, loss = 0.000943400664255023
iteration 10, loss = 0.0014998638071119785
iteration 11, loss = 0.0007913282606750727
iteration 12, loss = 0.0007018898031674325
iteration 13, loss = 0.0014718477614223957
iteration 14, loss = 0.001743445871397853
iteration 15, loss = 0.0011506312293931842
iteration 16, loss = 0.0010145262349396944
iteration 17, loss = 0.001447123708203435
iteration 18, loss = 0.0009998532477766275
iteration 19, loss = 0.0007622124976478517
iteration 20, loss = 0.0008373604505322874
iteration 21, loss = 0.0008724789367988706
iteration 22, loss = 0.000886022811755538
iteration 23, loss = 0.0009383934084326029
iteration 24, loss = 0.001817992771975696
iteration 25, loss = 0.0007611103355884552
iteration 26, loss = 0.0008098527905531228
iteration 27, loss = 0.0007263574516400695
iteration 28, loss = 0.0009293793700635433
iteration 29, loss = 0.0008379733189940453
iteration 30, loss = 0.0006866934127174318
iteration 31, loss = 0.000641330610960722
iteration 32, loss = 0.0008261885959655046
iteration 33, loss = 0.0008053267374634743
iteration 34, loss = 0.0006466312916018069
iteration 35, loss = 0.0007979475194588304
iteration 36, loss = 0.0011243497719988227
iteration 37, loss = 0.0008168153581209481
iteration 38, loss = 0.001591530512087047
iteration 39, loss = 0.0008525075973011553
iteration 40, loss = 0.0008282617200165987
iteration 41, loss = 0.0008170007495209575
iteration 42, loss = 0.0007321411976590753
iteration 43, loss = 0.0007944050594232976
iteration 44, loss = 0.0007634772337041795
iteration 45, loss = 0.0006929174996912479
iteration 46, loss = 0.0013674345100298524
iteration 47, loss = 0.0007085563847795129
iteration 48, loss = 0.0015775522915646434
iteration 49, loss = 0.0010817437432706356
iteration 50, loss = 0.0008868089062161744
iteration 51, loss = 0.000731821870431304
iteration 52, loss = 0.0007937540067359805
iteration 53, loss = 0.0007909471169114113
iteration 54, loss = 0.0009641186916269362
iteration 55, loss = 0.002037745201960206
iteration 56, loss = 0.0008536229142919183
iteration 57, loss = 0.001346467062830925
iteration 58, loss = 0.0006870354409329593
iteration 59, loss = 0.0008001645328477025
iteration 60, loss = 0.0008025635033845901
iteration 61, loss = 0.0017626747721806169
iteration 62, loss = 0.0010848449310287833
iteration 63, loss = 0.0009844934102147818
iteration 64, loss = 0.0010040756314992905
iteration 65, loss = 0.0009248849237337708
iteration 66, loss = 0.0008486626320518553
iteration 67, loss = 0.0006384430453181267
iteration 68, loss = 0.0012026454787701368
iteration 69, loss = 0.0006906469934619963
iteration 70, loss = 0.001459872699342668
iteration 71, loss = 0.0009647734113968909
iteration 72, loss = 0.0008208330837078393
iteration 73, loss = 0.0008074811194092035
iteration 74, loss = 0.0008977364050224423
iteration 75, loss = 0.00092825893079862
iteration 76, loss = 0.0017879069782793522
iteration 77, loss = 0.0010140114463865757
iteration 78, loss = 0.0011097070528194308
iteration 79, loss = 0.0012399180559441447
iteration 80, loss = 0.0007363564218394458
iteration 81, loss = 0.001445723115466535
iteration 82, loss = 0.000980338896624744
iteration 83, loss = 0.0010743644088506699
iteration 84, loss = 0.0009696906199678779
iteration 85, loss = 0.0007884022779762745
iteration 86, loss = 0.0008833829197101295
iteration 87, loss = 0.0008426098502241075
iteration 88, loss = 0.000849262229166925
iteration 89, loss = 0.0016714753583073616
iteration 90, loss = 0.0009195756283588707
iteration 91, loss = 0.0007866984233260155
iteration 92, loss = 0.0008525328012183309
iteration 93, loss = 0.0008617271669209003
iteration 94, loss = 0.0007864015060476959
iteration 95, loss = 0.001267101033590734
iteration 96, loss = 0.000767602410633117
iteration 97, loss = 0.0009015926625579596
iteration 98, loss = 0.0007245037122629583
iteration 99, loss = 0.0007435458246618509
iteration 100, loss = 0.0010928212432190776
iteration 101, loss = 0.0009548812522552907
iteration 102, loss = 0.0008641928434371948
iteration 103, loss = 0.0007764290203340352
iteration 104, loss = 0.0008652291726320982
iteration 105, loss = 0.00094321568030864
iteration 106, loss = 0.0011885510757565498
iteration 107, loss = 0.0007941338699311018
iteration 108, loss = 0.0007758630672469735
iteration 109, loss = 0.000993069144897163
iteration 110, loss = 0.0009118856396526098
iteration 111, loss = 0.0009176247986033559
iteration 112, loss = 0.001066457130946219
iteration 113, loss = 0.0008668848895467818
iteration 114, loss = 0.0010303687304258347
iteration 115, loss = 0.0007466924726031721
iteration 116, loss = 0.0007679031696170568
iteration 117, loss = 0.0008490049513056874
iteration 118, loss = 0.0008117276011034846
iteration 119, loss = 0.0009233340970240533
iteration 120, loss = 0.0007673802319914103
iteration 121, loss = 0.0009267401183024049
iteration 122, loss = 0.0008793395245447755
iteration 123, loss = 0.001360953552648425
iteration 124, loss = 0.0008177297422662377
iteration 125, loss = 0.0007464269874617457
iteration 126, loss = 0.0008194026886485517
iteration 127, loss = 0.0008997080149129033
iteration 128, loss = 0.001143358531408012
iteration 129, loss = 0.0008270334801636636
iteration 130, loss = 0.0009682141244411469
iteration 131, loss = 0.0007596499635837972
iteration 132, loss = 0.0007356886053457856
iteration 133, loss = 0.0012869364582002163
iteration 134, loss = 0.0008247665246017277
iteration 135, loss = 0.0007391353137791157
iteration 136, loss = 0.002264401176944375
iteration 137, loss = 0.0008991765789687634
iteration 138, loss = 0.0007524145767092705
iteration 139, loss = 0.0009523542248643935
iteration 140, loss = 0.0008360854117199779
iteration 141, loss = 0.0009255302138626575
iteration 142, loss = 0.0008642133325338364
iteration 143, loss = 0.0007620345568284392
iteration 144, loss = 0.0007560288067907095
iteration 145, loss = 0.0008403299143537879
iteration 146, loss = 0.0007712303777225316
iteration 147, loss = 0.0006406037718988955
iteration 148, loss = 0.0011674623237922788
iteration 149, loss = 0.0007605673163197935
iteration 150, loss = 0.00105398113373667
iteration 151, loss = 0.0007715653628110886
iteration 152, loss = 0.001155839883722365
iteration 153, loss = 0.0009886393090710044
iteration 154, loss = 0.0009091238607652485
iteration 155, loss = 0.0007597661460749805
iteration 156, loss = 0.0007414506399072707
iteration 157, loss = 0.0006931439856998622
iteration 158, loss = 0.0006440375582315028
iteration 159, loss = 0.0014543511206284165
iteration 160, loss = 0.0014664168702438474
iteration 161, loss = 0.001079660840332508
iteration 162, loss = 0.0020724537316709757
iteration 163, loss = 0.00079371128231287
iteration 164, loss = 0.0006918476428836584
iteration 165, loss = 0.0010450126137584448
iteration 166, loss = 0.0008710581460036337
iteration 167, loss = 0.0015839389525353909
iteration 168, loss = 0.0010237398091703653
iteration 169, loss = 0.0012521244352683425
iteration 170, loss = 0.0008100244449451566
iteration 171, loss = 0.0008915254729799926
iteration 172, loss = 0.0006494263070635498
iteration 173, loss = 0.0012310019228607416
iteration 174, loss = 0.0009957959409803152
iteration 175, loss = 0.0009147365344688296
iteration 176, loss = 0.0009146931115537882
iteration 177, loss = 0.000859106658026576
iteration 178, loss = 0.000983891193754971
iteration 179, loss = 0.0006629993440583348
iteration 180, loss = 0.0011057339143007994
iteration 181, loss = 0.0017756227171048522
iteration 182, loss = 0.0008052924531511962
iteration 183, loss = 0.0009382928255945444
iteration 184, loss = 0.0008139841957017779
iteration 185, loss = 0.0010364619083702564
iteration 186, loss = 0.000979594886302948
iteration 187, loss = 0.0016596851637586951
iteration 188, loss = 0.0007213284261524677
iteration 189, loss = 0.0008433663751929998
iteration 190, loss = 0.0007742060115560889
iteration 191, loss = 0.001391246565617621
iteration 192, loss = 0.000738085713237524
iteration 193, loss = 0.0014241490280255675
iteration 194, loss = 0.0006725413841195405
iteration 195, loss = 0.0010107802227139473
iteration 196, loss = 0.0011193221434950829
iteration 197, loss = 0.0011034042108803988
iteration 198, loss = 0.0009054616093635559
iteration 199, loss = 0.0009862080914899707
iteration 200, loss = 0.0007719336426816881
iteration 201, loss = 0.0014931228943169117
iteration 202, loss = 0.0010426887311041355
iteration 203, loss = 0.000623533851467073
iteration 204, loss = 0.000856489350553602
iteration 205, loss = 0.0012760109966620803
iteration 206, loss = 0.0007409757818095386
iteration 207, loss = 0.0007617692463099957
iteration 208, loss = 0.0008648083312436938
iteration 209, loss = 0.0010314977262169123
iteration 210, loss = 0.0008409607107751071
iteration 211, loss = 0.0007552014430984855
iteration 212, loss = 0.000980093260295689
iteration 213, loss = 0.0009937613504007459
iteration 214, loss = 0.0008184982580132782
iteration 215, loss = 0.0007669521728530526
iteration 216, loss = 0.000766497862059623
iteration 217, loss = 0.0010522914817556739
iteration 218, loss = 0.001134863356128335
iteration 219, loss = 0.000927626620978117
iteration 220, loss = 0.0008232363034039736
iteration 221, loss = 0.001055950066074729
iteration 222, loss = 0.0007758854771964252
iteration 223, loss = 0.0008044120040722191
iteration 224, loss = 0.000717893592081964
iteration 225, loss = 0.0008456290815956891
iteration 226, loss = 0.0010036213789135218
iteration 227, loss = 0.0009476097184233367
iteration 228, loss = 0.0009445471223443747
iteration 229, loss = 0.000990359578281641
iteration 230, loss = 0.0008728471584618092
iteration 231, loss = 0.0010081592481583357
iteration 232, loss = 0.00103213160764426
iteration 233, loss = 0.0009037764393724501
iteration 234, loss = 0.0008347750408574939
iteration 235, loss = 0.0008551658829674125
iteration 236, loss = 0.0013236601371318102
iteration 237, loss = 0.0011295515578240156
iteration 238, loss = 0.0009456018451601267
iteration 239, loss = 0.0009592510759830475
iteration 240, loss = 0.0008697833982296288
iteration 241, loss = 0.0008909334428608418
iteration 242, loss = 0.0010926740942522883
iteration 243, loss = 0.0014280066825449467
iteration 244, loss = 0.0008627866627648473
iteration 245, loss = 0.0010960971703752875
iteration 246, loss = 0.0007129315636120737
iteration 247, loss = 0.0007623370038345456
iteration 248, loss = 0.0006994254072196782
iteration 249, loss = 0.0007617524242959917
iteration 250, loss = 0.0007479171035811305
iteration 251, loss = 0.0007564951665699482
iteration 252, loss = 0.0007640196708962321
iteration 253, loss = 0.0007979101501405239
iteration 254, loss = 0.0018724777037277818
iteration 255, loss = 0.0015472164377570152
iteration 256, loss = 0.0007416291628032923
iteration 257, loss = 0.0006854701787233353
iteration 258, loss = 0.0016287057660520077
iteration 259, loss = 0.0011241105385124683
iteration 260, loss = 0.0007528741843998432
iteration 261, loss = 0.0011383385863155127
iteration 262, loss = 0.0007976254564709961
iteration 263, loss = 0.0011102133430540562
iteration 264, loss = 0.0009225085377693176
iteration 265, loss = 0.0007947006961330771
iteration 266, loss = 0.0015880413120612502
iteration 267, loss = 0.0007880355115048587
iteration 268, loss = 0.0008921403787098825
iteration 269, loss = 0.0008694569696672261
iteration 270, loss = 0.0009895784314721823
iteration 271, loss = 0.0007679092232137918
iteration 272, loss = 0.0007989841978996992
iteration 273, loss = 0.0009412877261638641
iteration 274, loss = 0.000811163627076894
iteration 275, loss = 0.0008535290835425258
iteration 276, loss = 0.0011054989881813526
iteration 277, loss = 0.0007369621307589114
iteration 278, loss = 0.0009089235682040453
iteration 279, loss = 0.0008330241544172168
iteration 280, loss = 0.0007063835510052741
iteration 281, loss = 0.0007983865216374397
iteration 282, loss = 0.0008822672534734011
iteration 283, loss = 0.0008162542362697423
iteration 284, loss = 0.0008256295695900917
iteration 285, loss = 0.0008185743936337531
iteration 286, loss = 0.0006831926293671131
iteration 287, loss = 0.0008020681561902165
iteration 288, loss = 0.001017800415866077
iteration 289, loss = 0.0009319878881797194
iteration 290, loss = 0.0010602392721921206
iteration 291, loss = 0.0016770472284406424
iteration 292, loss = 0.0011379221687093377
iteration 293, loss = 0.0007828008383512497
iteration 294, loss = 0.0014844414545223117
iteration 295, loss = 0.0007308738422580063
iteration 296, loss = 0.0009766696020960808
iteration 297, loss = 0.0007161957328207791
iteration 298, loss = 0.0014104921137914062
iteration 299, loss = 0.0011607715860009193
iteration 300, loss = 0.000914889620617032
iteration 1, loss = 0.0009814114309847355
iteration 2, loss = 0.0010278790723532438
iteration 3, loss = 0.0010651096235960722
iteration 4, loss = 0.0008387890411540866
iteration 5, loss = 0.0008871463360264897
iteration 6, loss = 0.0007417026208713651
iteration 7, loss = 0.0008169218781404197
iteration 8, loss = 0.0017524685245007277
iteration 9, loss = 0.0008229936938732862
iteration 10, loss = 0.0010124833788722754
iteration 11, loss = 0.0014914553612470627
iteration 12, loss = 0.0007308927597478032
iteration 13, loss = 0.0006646970869041979
iteration 14, loss = 0.0008373633027076721
iteration 15, loss = 0.0009947147918865085
iteration 16, loss = 0.000922584265936166
iteration 17, loss = 0.0007463666261173785
iteration 18, loss = 0.0007113622850738466
iteration 19, loss = 0.0008697949815541506
iteration 20, loss = 0.0013949288986623287
iteration 21, loss = 0.000748118560295552
iteration 22, loss = 0.0008093539508990943
iteration 23, loss = 0.000880215666256845
iteration 24, loss = 0.0008005098206922412
iteration 25, loss = 0.0006752118933945894
iteration 26, loss = 0.0010197949595749378
iteration 27, loss = 0.0008299736655317247
iteration 28, loss = 0.0007241349667310715
iteration 29, loss = 0.0007764304173178971
iteration 30, loss = 0.0008908227318897843
iteration 31, loss = 0.0008970096823759377
iteration 32, loss = 0.0009205486276187003
iteration 33, loss = 0.0007777948630973697
iteration 34, loss = 0.0007476319442503154
iteration 35, loss = 0.0017487338045611978
iteration 36, loss = 0.0007494296878576279
iteration 37, loss = 0.0010808720253407955
iteration 38, loss = 0.001130349119193852
iteration 39, loss = 0.0016476598102599382
iteration 40, loss = 0.0008529130136594176
iteration 41, loss = 0.0008220128947868943
iteration 42, loss = 0.0011557210236787796
iteration 43, loss = 0.0008214740082621574
iteration 44, loss = 0.0007968207937665284
iteration 45, loss = 0.0016430222894996405
iteration 46, loss = 0.0009957838337868452
iteration 47, loss = 0.0010080718202516437
iteration 48, loss = 0.001301457523368299
iteration 49, loss = 0.001312539796344936
iteration 50, loss = 0.0007530912989750504
iteration 51, loss = 0.0008229782688431442
iteration 52, loss = 0.0009334985516034067
iteration 53, loss = 0.0008979568956419826
iteration 54, loss = 0.0007675487431697547
iteration 55, loss = 0.0012494277907535434
iteration 56, loss = 0.0007975329644978046
iteration 57, loss = 0.0007999192457646132
iteration 58, loss = 0.0018283798126503825
iteration 59, loss = 0.0007308369968086481
iteration 60, loss = 0.0008159242570400238
iteration 61, loss = 0.0007793067488819361
iteration 62, loss = 0.0010402122279629111
iteration 63, loss = 0.0008647183421999216
iteration 64, loss = 0.0010111901210621
iteration 65, loss = 0.000840140855871141
iteration 66, loss = 0.000840387714561075
iteration 67, loss = 0.0009597361786291003
iteration 68, loss = 0.0008577067055739462
iteration 69, loss = 0.000746075704228133
iteration 70, loss = 0.0009353432105854154
iteration 71, loss = 0.0009213932789862156
iteration 72, loss = 0.0015746753197163343
iteration 73, loss = 0.0008194022811949253
iteration 74, loss = 0.0016558965435251594
iteration 75, loss = 0.0010037478059530258
iteration 76, loss = 0.0008010090095922351
iteration 77, loss = 0.0008904682472348213
iteration 78, loss = 0.0010986842680722475
iteration 79, loss = 0.0012538924347609282
iteration 80, loss = 0.0007343506440520287
iteration 81, loss = 0.0008076095837168396
iteration 82, loss = 0.0010023185750469565
iteration 83, loss = 0.0007886034436523914
iteration 84, loss = 0.0008407955756410956
iteration 85, loss = 0.0009827610338106751
iteration 86, loss = 0.0008979436242952943
iteration 87, loss = 0.0008868908626027405
iteration 88, loss = 0.0008651017560623586
iteration 89, loss = 0.0013270813506096601
iteration 90, loss = 0.000813236169051379
iteration 91, loss = 0.001381381880491972
iteration 92, loss = 0.0016054073348641396
iteration 93, loss = 0.0018415481317788363
iteration 94, loss = 0.0006577144959010184
iteration 95, loss = 0.0008312437566928566
iteration 96, loss = 0.001102070207707584
iteration 97, loss = 0.0009045302285812795
iteration 98, loss = 0.001901199808344245
iteration 99, loss = 0.0007776013226248324
iteration 100, loss = 0.0008184978505596519
iteration 101, loss = 0.0007049915730021894
iteration 102, loss = 0.0007909608539193869
iteration 103, loss = 0.0008383191307075322
iteration 104, loss = 0.000841026718262583
iteration 105, loss = 0.0007700161077082157
iteration 106, loss = 0.0011542497668415308
iteration 107, loss = 0.0015302703250199556
iteration 108, loss = 0.0008446450810879469
iteration 109, loss = 0.0008139221463352442
iteration 110, loss = 0.0007873595459386706
iteration 111, loss = 0.0015920260921120644
iteration 112, loss = 0.000914859352633357
iteration 113, loss = 0.0010699820704758167
iteration 114, loss = 0.0015685136895626783
iteration 115, loss = 0.0009455604013055563
iteration 116, loss = 0.0017473133048042655
iteration 117, loss = 0.0010932155419141054
iteration 118, loss = 0.0010410115355625749
iteration 119, loss = 0.0008146129548549652
iteration 120, loss = 0.000899728387594223
iteration 121, loss = 0.0009059585863724351
iteration 122, loss = 0.0009611154673621058
iteration 123, loss = 0.0008545315940864384
iteration 124, loss = 0.0012732027098536491
iteration 125, loss = 0.0007359866867773235
iteration 126, loss = 0.0013123070821166039
iteration 127, loss = 0.0007595912320539355
iteration 128, loss = 0.0008413759060204029
iteration 129, loss = 0.0007123142131604254
iteration 130, loss = 0.0014807533007115126
iteration 131, loss = 0.0007713164668530226
iteration 132, loss = 0.0011302249040454626
iteration 133, loss = 0.0009991927072405815
iteration 134, loss = 0.0009485313785262406
iteration 135, loss = 0.0009178746258839965
iteration 136, loss = 0.0009707140852697194
iteration 137, loss = 0.0007308622007258236
iteration 138, loss = 0.0007169336313381791
iteration 139, loss = 0.000744290417060256
iteration 140, loss = 0.0014164433814585209
iteration 141, loss = 0.0011180784786120057
iteration 142, loss = 0.0007953255553729832
iteration 143, loss = 0.0008371336734853685
iteration 144, loss = 0.0018557176226750016
iteration 145, loss = 0.0010135348420590162
iteration 146, loss = 0.0008372854208573699
iteration 147, loss = 0.0007414757274091244
iteration 148, loss = 0.000804545299615711
iteration 149, loss = 0.0008539977134205401
iteration 150, loss = 0.0009670414729043841
iteration 151, loss = 0.000724856392480433
iteration 152, loss = 0.0009301468380726874
iteration 153, loss = 0.0008104773005470634
iteration 154, loss = 0.0008577443659305573
iteration 155, loss = 0.0006905986811034381
iteration 156, loss = 0.0010687613394111395
iteration 157, loss = 0.0008173369569703937
iteration 158, loss = 0.0011821513762697577
iteration 159, loss = 0.0010467348620295525
iteration 160, loss = 0.0009334665955975652
iteration 161, loss = 0.0011324322549626231
iteration 162, loss = 0.0008439357625320554
iteration 163, loss = 0.0010031587444245815
iteration 164, loss = 0.001009205006994307
iteration 165, loss = 0.0013254322111606598
iteration 166, loss = 0.0010710636852309108
iteration 167, loss = 0.0007540583028458059
iteration 168, loss = 0.0010721402941271663
iteration 169, loss = 0.0009321222896687686
iteration 170, loss = 0.0007033476722426713
iteration 171, loss = 0.0007770165102556348
iteration 172, loss = 0.0009408605401404202
iteration 173, loss = 0.0008664376218803227
iteration 174, loss = 0.00077391869854182
iteration 175, loss = 0.000738581467885524
iteration 176, loss = 0.0011141366558149457
iteration 177, loss = 0.0011140408460050821
iteration 178, loss = 0.0015140953473746777
iteration 179, loss = 0.000702440447639674
iteration 180, loss = 0.0007265746826305985
iteration 181, loss = 0.0010890288976952434
iteration 182, loss = 0.0007283342420123518
iteration 183, loss = 0.0010594358900561929
iteration 184, loss = 0.0008438585209660232
iteration 185, loss = 0.0010815442074090242
iteration 186, loss = 0.00098499096930027
iteration 187, loss = 0.0008196235285140574
iteration 188, loss = 0.0010579678928479552
iteration 189, loss = 0.0007606359431520104
iteration 190, loss = 0.001158681116066873
iteration 191, loss = 0.0010988845024257898
iteration 192, loss = 0.0006541811162605882
iteration 193, loss = 0.0008000887464731932
iteration 194, loss = 0.0008632525568827987
iteration 195, loss = 0.0008697815937921405
iteration 196, loss = 0.0008777037728577852
iteration 197, loss = 0.0009168957476504147
iteration 198, loss = 0.0008103635627776384
iteration 199, loss = 0.0008359714993275702
iteration 200, loss = 0.0010743384482339025
iteration 201, loss = 0.000898379017598927
iteration 202, loss = 0.0011098472168669105
iteration 203, loss = 0.0006879134452901781
iteration 204, loss = 0.0013846480287611485
iteration 205, loss = 0.001575515023432672
iteration 206, loss = 0.0008687755325809121
iteration 207, loss = 0.0011703015770763159
iteration 208, loss = 0.0008076999802142382
iteration 209, loss = 0.0009641476208344102
iteration 210, loss = 0.0008166781044565141
iteration 211, loss = 0.001158320577815175
iteration 212, loss = 0.0006716644275002182
iteration 213, loss = 0.000934780400712043
iteration 214, loss = 0.0010424789506942034
iteration 215, loss = 0.0007458246545866132
iteration 216, loss = 0.00099178496748209
iteration 217, loss = 0.0006742252735421062
iteration 218, loss = 0.0006620174390263855
iteration 219, loss = 0.0011634466936811805
iteration 220, loss = 0.0007431611302308738
iteration 221, loss = 0.0008390211733058095
iteration 222, loss = 0.0007688787300139666
iteration 223, loss = 0.001717704813927412
iteration 224, loss = 0.0009240328799933195
iteration 225, loss = 0.0010018614120781422
iteration 226, loss = 0.0008384542888961732
iteration 227, loss = 0.0008425196865573525
iteration 228, loss = 0.0007775118574500084
iteration 229, loss = 0.0007838343735784292
iteration 230, loss = 0.0010327319614589214
iteration 231, loss = 0.0007820978644303977
iteration 232, loss = 0.0009681502124294639
iteration 233, loss = 0.0008823840762488544
iteration 234, loss = 0.0006919291336089373
iteration 235, loss = 0.0008602705202065408
iteration 236, loss = 0.0009828355396166444
iteration 237, loss = 0.0007396096480078995
iteration 238, loss = 0.0007506889523938298
iteration 239, loss = 0.001139142899774015
iteration 240, loss = 0.0009211523574776947
iteration 241, loss = 0.0008861328242346644
iteration 242, loss = 0.0008067607413977385
iteration 243, loss = 0.0008993879309855402
iteration 244, loss = 0.0008401963859796524
iteration 245, loss = 0.0007753039244562387
iteration 246, loss = 0.0009855164680629969
iteration 247, loss = 0.0008375053876079619
iteration 248, loss = 0.0007998771034181118
iteration 249, loss = 0.0008665055502206087
iteration 250, loss = 0.000727999082300812
iteration 251, loss = 0.0015011036302894354
iteration 252, loss = 0.002148312982171774
iteration 253, loss = 0.0007515194010920823
iteration 254, loss = 0.0019188447622582316
iteration 255, loss = 0.0007928656996227801
iteration 256, loss = 0.0011514476500451565
iteration 257, loss = 0.0010066437534987926
iteration 258, loss = 0.0007307154592126608
iteration 259, loss = 0.0006817879038862884
iteration 260, loss = 0.0011338458862155676
iteration 261, loss = 0.0007886419189162552
iteration 262, loss = 0.0007316326955333352
iteration 263, loss = 0.0008507957099936903
iteration 264, loss = 0.0007502922089770436
iteration 265, loss = 0.0014502726262435317
iteration 266, loss = 0.0007865267107263207
iteration 267, loss = 0.0008999269921332598
iteration 268, loss = 0.0007396254222840071
iteration 269, loss = 0.0007946583209559321
iteration 270, loss = 0.0008975466480478644
iteration 271, loss = 0.0008274912252090871
iteration 272, loss = 0.0008384700631722808
iteration 273, loss = 0.0009100391180254519
iteration 274, loss = 0.0010278817499056458
iteration 275, loss = 0.0009604368824511766
iteration 276, loss = 0.0007659642724320292
iteration 277, loss = 0.0006593751604668796
iteration 278, loss = 0.0010357783176004887
iteration 279, loss = 0.0009307947475463152
iteration 280, loss = 0.0010179862147197127
iteration 281, loss = 0.0009607785614207387
iteration 282, loss = 0.0008814091561362147
iteration 283, loss = 0.0008847301942296326
iteration 284, loss = 0.0006760230171494186
iteration 285, loss = 0.0007859828765504062
iteration 286, loss = 0.0013081275392323732
iteration 287, loss = 0.0009930082596838474
iteration 288, loss = 0.0007755342521704733
iteration 289, loss = 0.0007503170054405928
iteration 290, loss = 0.0006748345913365483
iteration 291, loss = 0.0008349643321707845
iteration 292, loss = 0.0007083240197971463
iteration 293, loss = 0.0018712449818849564
iteration 294, loss = 0.0008679547463543713
iteration 295, loss = 0.0012381762498989701
iteration 296, loss = 0.0008903949055820704
iteration 297, loss = 0.0007017838070169091
iteration 298, loss = 0.0014108665054664016
iteration 299, loss = 0.0007706155302003026
iteration 300, loss = 0.0008849553996697068
iteration 1, loss = 0.0009577132877893746
iteration 2, loss = 0.000832018384244293
iteration 3, loss = 0.0018385779112577438
iteration 4, loss = 0.0008766890387050807
iteration 5, loss = 0.0007071229047141969
iteration 6, loss = 0.000795731320977211
iteration 7, loss = 0.00088102015433833
iteration 8, loss = 0.0014999362174421549
iteration 9, loss = 0.0009005506872199476
iteration 10, loss = 0.0008400024380534887
iteration 11, loss = 0.0007921943324618042
iteration 12, loss = 0.0009153351420536637
iteration 13, loss = 0.0008870576275512576
iteration 14, loss = 0.0008893515332601964
iteration 15, loss = 0.0010333587415516376
iteration 16, loss = 0.0006982884951867163
iteration 17, loss = 0.0011687404476106167
iteration 18, loss = 0.0015850020572543144
iteration 19, loss = 0.0008037058869376779
iteration 20, loss = 0.0008964888984337449
iteration 21, loss = 0.0009678165661171079
iteration 22, loss = 0.0009003047598525882
iteration 23, loss = 0.0008848316501826048
iteration 24, loss = 0.0015949639491736889
iteration 25, loss = 0.001233025686815381
iteration 26, loss = 0.00150114216376096
iteration 27, loss = 0.0007214376819320023
iteration 28, loss = 0.0009277511271648109
iteration 29, loss = 0.0011596246622502804
iteration 30, loss = 0.0010784079786390066
iteration 31, loss = 0.0008122285944409668
iteration 32, loss = 0.0007050610729493201
iteration 33, loss = 0.0015731249004602432
iteration 34, loss = 0.0008282539783976972
iteration 35, loss = 0.0007910492131486535
iteration 36, loss = 0.0007998126093298197
iteration 37, loss = 0.0008270075777545571
iteration 38, loss = 0.0008594842511229217
iteration 39, loss = 0.00141171517316252
iteration 40, loss = 0.0010130436858162284
iteration 41, loss = 0.0009946317877620459
iteration 42, loss = 0.0018361040856689215
iteration 43, loss = 0.0010602156398817897
iteration 44, loss = 0.0010280234273523092
iteration 45, loss = 0.0009528478258289397
iteration 46, loss = 0.001024364260956645
iteration 47, loss = 0.0016240840777754784
iteration 48, loss = 0.0007713810191489756
iteration 49, loss = 0.0017699161544442177
iteration 50, loss = 0.000740515417419374
iteration 51, loss = 0.0009620042983442545
iteration 52, loss = 0.0007384857744909823
iteration 53, loss = 0.0008914734935387969
iteration 54, loss = 0.0007350507657974958
iteration 55, loss = 0.0008197005372494459
iteration 56, loss = 0.000804414798039943
iteration 57, loss = 0.0008911029435694218
iteration 58, loss = 0.0008987076580524445
iteration 59, loss = 0.001161864842288196
iteration 60, loss = 0.00070049031637609
iteration 61, loss = 0.00191612564958632
iteration 62, loss = 0.0007067114929668605
iteration 63, loss = 0.0012755101779475808
iteration 64, loss = 0.0009946119971573353
iteration 65, loss = 0.0008622607565484941
iteration 66, loss = 0.001072813756763935
iteration 67, loss = 0.0009120618924498558
iteration 68, loss = 0.000874556542839855
iteration 69, loss = 0.0009676484623923898
iteration 70, loss = 0.0007559649529866874
iteration 71, loss = 0.0007540866499766707
iteration 72, loss = 0.0007298869313672185
iteration 73, loss = 0.001180185005068779
iteration 74, loss = 0.0011686523212119937
iteration 75, loss = 0.0009664374520070851
iteration 76, loss = 0.0008033286430872977
iteration 77, loss = 0.00096561957616359
iteration 78, loss = 0.0010230791522189975
iteration 79, loss = 0.0014129218179732561
iteration 80, loss = 0.0007327597704716027
iteration 81, loss = 0.0014573944499716163
iteration 82, loss = 0.0007238449761644006
iteration 83, loss = 0.0007546135457232594
iteration 84, loss = 0.0008714783471077681
iteration 85, loss = 0.001555033028125763
iteration 86, loss = 0.0009198138141073287
iteration 87, loss = 0.0022415185812860727
iteration 88, loss = 0.0008233307162299752
iteration 89, loss = 0.0007780420710332692
iteration 90, loss = 0.0007203564164228737
iteration 91, loss = 0.0008416123455390334
iteration 92, loss = 0.0006353894714266062
iteration 93, loss = 0.0009423613082617521
iteration 94, loss = 0.0012306622229516506
iteration 95, loss = 0.0006507138605229557
iteration 96, loss = 0.001773818745277822
iteration 97, loss = 0.000702656339854002
iteration 98, loss = 0.0008458149968646467
iteration 99, loss = 0.0008602917660027742
iteration 100, loss = 0.0009148583631031215
iteration 101, loss = 0.0007955686887726188
iteration 102, loss = 0.0013777811545878649
iteration 103, loss = 0.0009150430560112
iteration 104, loss = 0.0009076768765226007
iteration 105, loss = 0.0006683821557089686
iteration 106, loss = 0.0008536604582332075
iteration 107, loss = 0.0009519907762296498
iteration 108, loss = 0.0010153979528695345
iteration 109, loss = 0.0014365033712238073
iteration 110, loss = 0.0006740253884345293
iteration 111, loss = 0.0011451150057837367
iteration 112, loss = 0.0012302646646276116
iteration 113, loss = 0.0007388108060695231
iteration 114, loss = 0.0007636254886165261
iteration 115, loss = 0.001034460961818695
iteration 116, loss = 0.0007528705173172057
iteration 117, loss = 0.0007510408759117126
iteration 118, loss = 0.0008605932234786451
iteration 119, loss = 0.0012090668315067887
iteration 120, loss = 0.0006766323349438608
iteration 121, loss = 0.0007027386454865336
iteration 122, loss = 0.0008995839743874967
iteration 123, loss = 0.000680401804856956
iteration 124, loss = 0.0007642559939995408
iteration 125, loss = 0.0011900992831215262
iteration 126, loss = 0.0007606581202708185
iteration 127, loss = 0.0007529442082159221
iteration 128, loss = 0.0008111704373732209
iteration 129, loss = 0.0016393419355154037
iteration 130, loss = 0.0007491830619983375
iteration 131, loss = 0.0013944839593023062
iteration 132, loss = 0.0018348591402173042
iteration 133, loss = 0.0008015323546715081
iteration 134, loss = 0.001157334540039301
iteration 135, loss = 0.0011269602691754699
iteration 136, loss = 0.0007364260382018983
iteration 137, loss = 0.0007500362698920071
iteration 138, loss = 0.000827474519610405
iteration 139, loss = 0.0009269582806155086
iteration 140, loss = 0.0008662436157464981
iteration 141, loss = 0.001241103047505021
iteration 142, loss = 0.000931548944208771
iteration 143, loss = 0.0007453948492184281
iteration 144, loss = 0.0007011588313616812
iteration 145, loss = 0.0010484142694622278
iteration 146, loss = 0.0006979504250921309
iteration 147, loss = 0.001103950897231698
iteration 148, loss = 0.0011951630003750324
iteration 149, loss = 0.0006357832462526858
iteration 150, loss = 0.0009042938472703099
iteration 151, loss = 0.0008106032037176192
iteration 152, loss = 0.0009236009209416807
iteration 153, loss = 0.0008009938756003976
iteration 154, loss = 0.0010252210777252913
iteration 155, loss = 0.0008991201175376773
iteration 156, loss = 0.0010909074917435646
iteration 157, loss = 0.001094394945539534
iteration 158, loss = 0.0016540531069040298
iteration 159, loss = 0.0008160716388374567
iteration 160, loss = 0.0010360653977841139
iteration 161, loss = 0.0008221983443945646
iteration 162, loss = 0.0010395898716524243
iteration 163, loss = 0.0008290279656648636
iteration 164, loss = 0.0010516366455703974
iteration 165, loss = 0.0008792795706540346
iteration 166, loss = 0.0007186710718087852
iteration 167, loss = 0.000680252502206713
iteration 168, loss = 0.0009236918995156884
iteration 169, loss = 0.0012012370862066746
iteration 170, loss = 0.0007307404302991927
iteration 171, loss = 0.0007635801448486745
iteration 172, loss = 0.0011728437384590507
iteration 173, loss = 0.0009477049461565912
iteration 174, loss = 0.000853205390740186
iteration 175, loss = 0.0009329696767963469
iteration 176, loss = 0.0008949862676672637
iteration 177, loss = 0.0008428685832768679
iteration 178, loss = 0.0009718946530483663
iteration 179, loss = 0.0007804614724591374
iteration 180, loss = 0.0007339298026636243
iteration 181, loss = 0.001066308468580246
iteration 182, loss = 0.0007810501847416162
iteration 183, loss = 0.0008799966890364885
iteration 184, loss = 0.0007671994972042739
iteration 185, loss = 0.0006896366248838603
iteration 186, loss = 0.0009317720541730523
iteration 187, loss = 0.0007767818169668317
iteration 188, loss = 0.0009295042254962027
iteration 189, loss = 0.0007531695882789791
iteration 190, loss = 0.0018184072105214
iteration 191, loss = 0.0009154904983006418
iteration 192, loss = 0.0010377522557973862
iteration 193, loss = 0.0012841029092669487
iteration 194, loss = 0.0008513170760124922
iteration 195, loss = 0.0010289647616446018
iteration 196, loss = 0.0008196294656954706
iteration 197, loss = 0.0008513702196069062
iteration 198, loss = 0.0009529598755761981
iteration 199, loss = 0.0007569098379462957
iteration 200, loss = 0.0007969108410179615
iteration 201, loss = 0.0007593540358357131
iteration 202, loss = 0.0006542481132782996
iteration 203, loss = 0.0010045275557786226
iteration 204, loss = 0.0012286753626540303
iteration 205, loss = 0.0008047105511650443
iteration 206, loss = 0.0017033863114193082
iteration 207, loss = 0.0009194966405630112
iteration 208, loss = 0.0008660099701955914
iteration 209, loss = 0.0008486816077493131
iteration 210, loss = 0.000714857189450413
iteration 211, loss = 0.0008881649700924754
iteration 212, loss = 0.0008707912638783455
iteration 213, loss = 0.0008442470571026206
iteration 214, loss = 0.0009620370692573488
iteration 215, loss = 0.0015183670911937952
iteration 216, loss = 0.0007922168588265777
iteration 217, loss = 0.0008431847090832889
iteration 218, loss = 0.0009477391722612083
iteration 219, loss = 0.0007751911180093884
iteration 220, loss = 0.0008941536652855575
iteration 221, loss = 0.0009795038495212793
iteration 222, loss = 0.0007589358137920499
iteration 223, loss = 0.0009304973646067083
iteration 224, loss = 0.0009444376919418573
iteration 225, loss = 0.0007137528737075627
iteration 226, loss = 0.0022265594452619553
iteration 227, loss = 0.0007236974779516459
iteration 228, loss = 0.0008615580736659467
iteration 229, loss = 0.0010601048124954104
iteration 230, loss = 0.0008117735851556063
iteration 231, loss = 0.0008069533505477011
iteration 232, loss = 0.0007794793928042054
iteration 233, loss = 0.0009279008372686803
iteration 234, loss = 0.0009240865474566817
iteration 235, loss = 0.0010233164066448808
iteration 236, loss = 0.001372999045997858
iteration 237, loss = 0.0006977830780670047
iteration 238, loss = 0.0007155277417041361
iteration 239, loss = 0.0008210514788515866
iteration 240, loss = 0.0007351511740125716
iteration 241, loss = 0.0008725136867724359
iteration 242, loss = 0.0017499311361461878
iteration 243, loss = 0.0006499000592157245
iteration 244, loss = 0.001103463931940496
iteration 245, loss = 0.0009801704436540604
iteration 246, loss = 0.0008326705428771675
iteration 247, loss = 0.0009818359976634383
iteration 248, loss = 0.0007478597108274698
iteration 249, loss = 0.0008277187589555979
iteration 250, loss = 0.0007231900235638022
iteration 251, loss = 0.000827341282274574
iteration 252, loss = 0.001452908618375659
iteration 253, loss = 0.0007407939992845058
iteration 254, loss = 0.0011061250697821379
iteration 255, loss = 0.000750018167309463
iteration 256, loss = 0.000661739963106811
iteration 257, loss = 0.001029646024107933
iteration 258, loss = 0.0010330500081181526
iteration 259, loss = 0.0011766882380470634
iteration 260, loss = 0.0006125954096205533
iteration 261, loss = 0.0007795027340762317
iteration 262, loss = 0.0007709653582423925
iteration 263, loss = 0.0006864643073640764
iteration 264, loss = 0.0009011173970066011
iteration 265, loss = 0.0007813889533281326
iteration 266, loss = 0.001105603063479066
iteration 267, loss = 0.001609564176760614
iteration 268, loss = 0.0008510721381753683
iteration 269, loss = 0.0010509726125746965
iteration 270, loss = 0.0009451265214011073
iteration 271, loss = 0.000984050566330552
iteration 272, loss = 0.0008500689873471856
iteration 273, loss = 0.0006738548399880528
iteration 274, loss = 0.0008072389173321426
iteration 275, loss = 0.0010178190423175693
iteration 276, loss = 0.0008360740030184388
iteration 277, loss = 0.001499753911048174
iteration 278, loss = 0.0008481997647322714
iteration 279, loss = 0.0008833825122565031
iteration 280, loss = 0.0008065378060564399
iteration 281, loss = 0.0008492054766975343
iteration 282, loss = 0.0011594269890338182
iteration 283, loss = 0.0011698276503011584
iteration 284, loss = 0.0010314336977899075
iteration 285, loss = 0.0008507590391673148
iteration 286, loss = 0.0009183983784168959
iteration 287, loss = 0.0007390909595414996
iteration 288, loss = 0.001233089016750455
iteration 289, loss = 0.0006938081933185458
iteration 290, loss = 0.0010080677457153797
iteration 291, loss = 0.0011050592875108123
iteration 292, loss = 0.0007682833820581436
iteration 293, loss = 0.0008027136791497469
iteration 294, loss = 0.0007406945805996656
iteration 295, loss = 0.001104511320590973
iteration 296, loss = 0.0008565426105633378
iteration 297, loss = 0.001147763803601265
iteration 298, loss = 0.0011421284871175885
iteration 299, loss = 0.0007576880161650479
iteration 300, loss = 0.0008778093033470213
iteration 1, loss = 0.0008988412446342409
iteration 2, loss = 0.0006901981541886926
iteration 3, loss = 0.0007783017936162651
iteration 4, loss = 0.0009416310349479318
iteration 5, loss = 0.0014550569467246532
iteration 6, loss = 0.000907999521587044
iteration 7, loss = 0.0008637522696517408
iteration 8, loss = 0.0011279972968623042
iteration 9, loss = 0.0009641722426749766
iteration 10, loss = 0.0007390489336103201
iteration 11, loss = 0.0020266324281692505
iteration 12, loss = 0.0009366684826090932
iteration 13, loss = 0.0008504093857482076
iteration 14, loss = 0.0007268652552738786
iteration 15, loss = 0.0007659777766093612
iteration 16, loss = 0.0011347901308909059
iteration 17, loss = 0.0008957969257608056
iteration 18, loss = 0.0008262824267148972
iteration 19, loss = 0.0006725023267790675
iteration 20, loss = 0.0007461478235200047
iteration 21, loss = 0.0013119264040142298
iteration 22, loss = 0.0008825005497783422
iteration 23, loss = 0.0008671320974826813
iteration 24, loss = 0.000780078349635005
iteration 25, loss = 0.0008797656046226621
iteration 26, loss = 0.0009750088211148977
iteration 27, loss = 0.0007509852293878794
iteration 28, loss = 0.0008117661345750093
iteration 29, loss = 0.001023189863190055
iteration 30, loss = 0.0009264205582439899
iteration 31, loss = 0.0015403650468215346
iteration 32, loss = 0.0010141019010916352
iteration 33, loss = 0.0007933923043310642
iteration 34, loss = 0.0007367984508164227
iteration 35, loss = 0.0007053837180137634
iteration 36, loss = 0.0009240532526746392
iteration 37, loss = 0.0008816603804007173
iteration 38, loss = 0.0007433925056830049
iteration 39, loss = 0.0007959631038829684
iteration 40, loss = 0.0007825597422197461
iteration 41, loss = 0.0008953632204793394
iteration 42, loss = 0.0009741351823322475
iteration 43, loss = 0.0013109787832945585
iteration 44, loss = 0.0007650399347767234
iteration 45, loss = 0.001780831255018711
iteration 46, loss = 0.0014265354257076979
iteration 47, loss = 0.0007847548695281148
iteration 48, loss = 0.0008353989687748253
iteration 49, loss = 0.0009628668194636703
iteration 50, loss = 0.0009293725015595555
iteration 51, loss = 0.0010829039383679628
iteration 52, loss = 0.0007709935889579356
iteration 53, loss = 0.0008156879339367151
iteration 54, loss = 0.0008332205470651388
iteration 55, loss = 0.0010866515804082155
iteration 56, loss = 0.0007927939295768738
iteration 57, loss = 0.0008576869731768966
iteration 58, loss = 0.0008019576780498028
iteration 59, loss = 0.000841200933791697
iteration 60, loss = 0.0015214028535410762
iteration 61, loss = 0.0007921459618955851
iteration 62, loss = 0.0014148858608677983
iteration 63, loss = 0.0007312275120057166
iteration 64, loss = 0.000687621533870697
iteration 65, loss = 0.0007978961803019047
iteration 66, loss = 0.0008951175259426236
iteration 67, loss = 0.0007257091347128153
iteration 68, loss = 0.0011640306329354644
iteration 69, loss = 0.0012002813164144754
iteration 70, loss = 0.0008927566814236343
iteration 71, loss = 0.0020721161272376776
iteration 72, loss = 0.0010417072335258126
iteration 73, loss = 0.0008447049767710268
iteration 74, loss = 0.000742164091207087
iteration 75, loss = 0.0009153341525234282
iteration 76, loss = 0.0011507427552714944
iteration 77, loss = 0.0010009085526689887
iteration 78, loss = 0.0012666982365772128
iteration 79, loss = 0.0008313338039442897
iteration 80, loss = 0.0010035809827968478
iteration 81, loss = 0.000894729164429009
iteration 82, loss = 0.0016738844569772482
iteration 83, loss = 0.0007590637542307377
iteration 84, loss = 0.0008894331404007971
iteration 85, loss = 0.0015643868828192353
iteration 86, loss = 0.0014592958614230156
iteration 87, loss = 0.0008785828831605613
iteration 88, loss = 0.001006656908430159
iteration 89, loss = 0.0008260402246378362
iteration 90, loss = 0.0008471946348436177
iteration 91, loss = 0.0014129580231383443
iteration 92, loss = 0.0006362818530760705
iteration 93, loss = 0.0007302622543647885
iteration 94, loss = 0.0009822150459513068
iteration 95, loss = 0.0010820672614499927
iteration 96, loss = 0.0010575928026810288
iteration 97, loss = 0.0008400400984100997
iteration 98, loss = 0.0008347029215656221
iteration 99, loss = 0.0009058198193088174
iteration 100, loss = 0.000730857893358916
iteration 101, loss = 0.0011922456324100494
iteration 102, loss = 0.001273810863494873
iteration 103, loss = 0.0011280651669949293
iteration 104, loss = 0.0008766043465584517
iteration 105, loss = 0.0010332782985642552
iteration 106, loss = 0.0010599822271615267
iteration 107, loss = 0.000866719230543822
iteration 108, loss = 0.0009648837149143219
iteration 109, loss = 0.0006950458628125489
iteration 110, loss = 0.0008418297511525452
iteration 111, loss = 0.0006862697191536427
iteration 112, loss = 0.0011394964531064034
iteration 113, loss = 0.0009366259910166264
iteration 114, loss = 0.000998626695945859
iteration 115, loss = 0.0007697543478570879
iteration 116, loss = 0.0015489080687984824
iteration 117, loss = 0.0008335282327607274
iteration 118, loss = 0.0008582455338910222
iteration 119, loss = 0.0007982305251061916
iteration 120, loss = 0.0007446614326909184
iteration 121, loss = 0.0009288861183449626
iteration 122, loss = 0.0009492220706306398
iteration 123, loss = 0.0008193355170078576
iteration 124, loss = 0.0013063425431028008
iteration 125, loss = 0.0009426840697415173
iteration 126, loss = 0.0014653224498033524
iteration 127, loss = 0.0008418745128437877
iteration 128, loss = 0.0011673171538859606
iteration 129, loss = 0.0009589116089046001
iteration 130, loss = 0.0007591311587020755
iteration 131, loss = 0.0005884315469302237
iteration 132, loss = 0.0008487020968459547
iteration 133, loss = 0.0008324868977069855
iteration 134, loss = 0.0006921516032889485
iteration 135, loss = 0.000734600005671382
iteration 136, loss = 0.000752282387111336
iteration 137, loss = 0.000797504442743957
iteration 138, loss = 0.0011605245526880026
iteration 139, loss = 0.0008373623713850975
iteration 140, loss = 0.000767722143791616
iteration 141, loss = 0.0008052290068008006
iteration 142, loss = 0.0009115443099290133
iteration 143, loss = 0.000831476878374815
iteration 144, loss = 0.0007365476922132075
iteration 145, loss = 0.000988160609267652
iteration 146, loss = 0.0010043509537354112
iteration 147, loss = 0.0010340273147448897
iteration 148, loss = 0.0012048266362398863
iteration 149, loss = 0.0009118036832660437
iteration 150, loss = 0.0014521105913445354
iteration 151, loss = 0.0007776591810397804
iteration 152, loss = 0.0009628483676351607
iteration 153, loss = 0.00112159235868603
iteration 154, loss = 0.0008051192853599787
iteration 155, loss = 0.0009919119765982032
iteration 156, loss = 0.0008258976158685982
iteration 157, loss = 0.000930923386476934
iteration 158, loss = 0.0009238120401278138
iteration 159, loss = 0.0006085845525376499
iteration 160, loss = 0.001053791493177414
iteration 161, loss = 0.000983603298664093
iteration 162, loss = 0.0009525761124677956
iteration 163, loss = 0.0007304445025511086
iteration 164, loss = 0.0008888216107152402
iteration 165, loss = 0.0010887086391448975
iteration 166, loss = 0.0007705134339630604
iteration 167, loss = 0.0007798196747899055
iteration 168, loss = 0.0010452113347128034
iteration 169, loss = 0.0009070883970707655
iteration 170, loss = 0.0007796360878273845
iteration 171, loss = 0.0015625900123268366
iteration 172, loss = 0.0011637896532192826
iteration 173, loss = 0.0011402774835005403
iteration 174, loss = 0.0007280911668203771
iteration 175, loss = 0.0007540889782831073
iteration 176, loss = 0.0008128287736326456
iteration 177, loss = 0.0007604191778227687
iteration 178, loss = 0.0016736595425754786
iteration 179, loss = 0.0008661149768158793
iteration 180, loss = 0.0009798554237931967
iteration 181, loss = 0.0007498016930185258
iteration 182, loss = 0.0006280031520873308
iteration 183, loss = 0.001036352594383061
iteration 184, loss = 0.0009279164369218051
iteration 185, loss = 0.0010377987055107951
iteration 186, loss = 0.001240487676113844
iteration 187, loss = 0.0012873213272541761
iteration 188, loss = 0.0010689364280551672
iteration 189, loss = 0.0025009997189044952
iteration 190, loss = 0.0008266278309747577
iteration 191, loss = 0.0008089067414402962
iteration 192, loss = 0.0008459370583295822
iteration 193, loss = 0.0009171126876026392
iteration 194, loss = 0.0011647228384390473
iteration 195, loss = 0.0007621775148436427
iteration 196, loss = 0.0007816927973181009
iteration 197, loss = 0.0015006198082119226
iteration 198, loss = 0.0013182297116145492
iteration 199, loss = 0.0008046860457397997
iteration 200, loss = 0.0010120343649759889
iteration 201, loss = 0.0014369445852935314
iteration 202, loss = 0.001478066318668425
iteration 203, loss = 0.0008519795956090093
iteration 204, loss = 0.001129946787841618
iteration 205, loss = 0.000923415063880384
iteration 206, loss = 0.0013513527810573578
iteration 207, loss = 0.0006661014631390572
iteration 208, loss = 0.0007791058160364628
iteration 209, loss = 0.0008266319055110216
iteration 210, loss = 0.0009831937495619059
iteration 211, loss = 0.0010347465286031365
iteration 212, loss = 0.000829520111437887
iteration 213, loss = 0.0007070148130878806
iteration 214, loss = 0.0006725509301759303
iteration 215, loss = 0.0009368348401039839
iteration 216, loss = 0.0011350370477885008
iteration 217, loss = 0.001054446678608656
iteration 218, loss = 0.0006650282884947956
iteration 219, loss = 0.0006651956937275827
iteration 220, loss = 0.0008618595893494785
iteration 221, loss = 0.0008881886606104672
iteration 222, loss = 0.0011566313914954662
iteration 223, loss = 0.0013514909660443664
iteration 224, loss = 0.001092453021556139
iteration 225, loss = 0.0006614536396227777
iteration 226, loss = 0.0009331151959486306
iteration 227, loss = 0.0007849829271435738
iteration 228, loss = 0.0011380596552044153
iteration 229, loss = 0.0008843588875606656
iteration 230, loss = 0.0010013720020651817
iteration 231, loss = 0.0007783045875839889
iteration 232, loss = 0.0009174100123345852
iteration 233, loss = 0.0008356536854989827
iteration 234, loss = 0.0007021419005468488
iteration 235, loss = 0.0008233803673647344
iteration 236, loss = 0.000709627871401608
iteration 237, loss = 0.0007849208195693791
iteration 238, loss = 0.0007305452018044889
iteration 239, loss = 0.0006982521736063063
iteration 240, loss = 0.0009957441361621022
iteration 241, loss = 0.000761019648052752
iteration 242, loss = 0.0006485948106274009
iteration 243, loss = 0.000982511555776
iteration 244, loss = 0.0015277479542419314
iteration 245, loss = 0.0008679835009388626
iteration 246, loss = 0.0007427973323501647
iteration 247, loss = 0.0017474195919930935
iteration 248, loss = 0.0007113353349268436
iteration 249, loss = 0.0007943114615045488
iteration 250, loss = 0.0008184935431927443
iteration 251, loss = 0.0007828344823792577
iteration 252, loss = 0.001570655731484294
iteration 253, loss = 0.0009435183019377291
iteration 254, loss = 0.000715568894520402
iteration 255, loss = 0.0010680792620405555
iteration 256, loss = 0.0008172610541805625
iteration 257, loss = 0.0006713498733006418
iteration 258, loss = 0.0008317130850628018
iteration 259, loss = 0.0009463222813792527
iteration 260, loss = 0.0008148880442604423
iteration 261, loss = 0.000819934532046318
iteration 262, loss = 0.000812874233815819
iteration 263, loss = 0.0009805479785427451
iteration 264, loss = 0.0008826252305880189
iteration 265, loss = 0.001412795390933752
iteration 266, loss = 0.0008101158309727907
iteration 267, loss = 0.0008565656607970595
iteration 268, loss = 0.0007506230613216758
iteration 269, loss = 0.0007946653058752418
iteration 270, loss = 0.0007142404792830348
iteration 271, loss = 0.0009341632830910385
iteration 272, loss = 0.0006789583130739629
iteration 273, loss = 0.0013664524303749204
iteration 274, loss = 0.0012481912272050977
iteration 275, loss = 0.0009564931970089674
iteration 276, loss = 0.000762603129260242
iteration 277, loss = 0.0009405112941749394
iteration 278, loss = 0.0006600688793696463
iteration 279, loss = 0.0010281808208674192
iteration 280, loss = 0.000804506242275238
iteration 281, loss = 0.0015641616191715002
iteration 282, loss = 0.00076514040119946
iteration 283, loss = 0.0011055282084271312
iteration 284, loss = 0.0016442673513665795
iteration 285, loss = 0.0007882565842010081
iteration 286, loss = 0.0009636276517994702
iteration 287, loss = 0.0008058301173150539
iteration 288, loss = 0.0008172380621545017
iteration 289, loss = 0.0009626896935515106
iteration 290, loss = 0.0007836750592105091
iteration 291, loss = 0.0013344112085178494
iteration 292, loss = 0.0007263679872266948
iteration 293, loss = 0.0011929123429581523
iteration 294, loss = 0.0008885317947715521
iteration 295, loss = 0.0015653937589377165
iteration 296, loss = 0.0010113746393471956
iteration 297, loss = 0.0010413926793262362
iteration 298, loss = 0.0006988368113525212
iteration 299, loss = 0.0015606359811499715
iteration 300, loss = 0.0008273818530142307
iteration 1, loss = 0.001190435723401606
iteration 2, loss = 0.0009598287288099527
iteration 3, loss = 0.0008908588206395507
iteration 4, loss = 0.000626898487098515
iteration 5, loss = 0.0005703356582671404
iteration 6, loss = 0.0011677686125040054
iteration 7, loss = 0.0010650557233020663
iteration 8, loss = 0.0010122882667928934
iteration 9, loss = 0.0006624458474107087
iteration 10, loss = 0.0007569301524199545
iteration 11, loss = 0.0010729897767305374
iteration 12, loss = 0.0010898658074438572
iteration 13, loss = 0.0007928779814392328
iteration 14, loss = 0.0008582813898101449
iteration 15, loss = 0.0007449631812050939
iteration 16, loss = 0.0007190577453002334
iteration 17, loss = 0.0009614650625735521
iteration 18, loss = 0.000759434828069061
iteration 19, loss = 0.001054302672855556
iteration 20, loss = 0.0008898655651137233
iteration 21, loss = 0.0009220785577781498
iteration 22, loss = 0.0008975684177130461
iteration 23, loss = 0.0010109762661159039
iteration 24, loss = 0.0019058207981288433
iteration 25, loss = 0.0012127760564908385
iteration 26, loss = 0.0015014640521258116
iteration 27, loss = 0.0007211742922663689
iteration 28, loss = 0.000856605707667768
iteration 29, loss = 0.000728011189494282
iteration 30, loss = 0.0008986294851638377
iteration 31, loss = 0.0008272247505374253
iteration 32, loss = 0.0015980026219040155
iteration 33, loss = 0.0011328255059197545
iteration 34, loss = 0.0008423726540058851
iteration 35, loss = 0.0008151759393513203
iteration 36, loss = 0.0007247378816828132
iteration 37, loss = 0.00095028814394027
iteration 38, loss = 0.0008080072002485394
iteration 39, loss = 0.0008006676216609776
iteration 40, loss = 0.0013358360156416893
iteration 41, loss = 0.0007866657106205821
iteration 42, loss = 0.0007482050568796694
iteration 43, loss = 0.0010882773203775287
iteration 44, loss = 0.0010743362363427877
iteration 45, loss = 0.0008027777657844126
iteration 46, loss = 0.0007661624113097787
iteration 47, loss = 0.0009431366343051195
iteration 48, loss = 0.001651228521950543
iteration 49, loss = 0.0009757825755514205
iteration 50, loss = 0.0006839557318016887
iteration 51, loss = 0.0007992401369847357
iteration 52, loss = 0.0011562233557924628
iteration 53, loss = 0.0007583455299027264
iteration 54, loss = 0.0012089104857295752
iteration 55, loss = 0.0015449830098077655
iteration 56, loss = 0.0015988030936568975
iteration 57, loss = 0.0007654927903786302
iteration 58, loss = 0.0007193236378952861
iteration 59, loss = 0.0013519824715331197
iteration 60, loss = 0.0010955750476568937
iteration 61, loss = 0.0008521859999746084
iteration 62, loss = 0.001020119758322835
iteration 63, loss = 0.0007475953898392618
iteration 64, loss = 0.0008914682548493147
iteration 65, loss = 0.0009764654678292572
iteration 66, loss = 0.0007728415657766163
iteration 67, loss = 0.0016289205523207784
iteration 68, loss = 0.0007809156086295843
iteration 69, loss = 0.0007910089916549623
iteration 70, loss = 0.0007652462227270007
iteration 71, loss = 0.000797853572294116
iteration 72, loss = 0.0007847704691812396
iteration 73, loss = 0.0007046477985568345
iteration 74, loss = 0.001000329153612256
iteration 75, loss = 0.0007646593148820102
iteration 76, loss = 0.0007814580458216369
iteration 77, loss = 0.0007706852629780769
iteration 78, loss = 0.0011416791239753366
iteration 79, loss = 0.001764926128089428
iteration 80, loss = 0.0010362231405451894
iteration 81, loss = 0.0008571233483962715
iteration 82, loss = 0.000701352721080184
iteration 83, loss = 0.0008753149304538965
iteration 84, loss = 0.0008180068107321858
iteration 85, loss = 0.0012161413906142116
iteration 86, loss = 0.0010307298507541418
iteration 87, loss = 0.0014932233607396483
iteration 88, loss = 0.0007663979195058346
iteration 89, loss = 0.0015656985342502594
iteration 90, loss = 0.0007227719761431217
iteration 91, loss = 0.0009496032726019621
iteration 92, loss = 0.0007358050206676126
iteration 93, loss = 0.0009295525960624218
iteration 94, loss = 0.0009545519715175033
iteration 95, loss = 0.0014173154486343265
iteration 96, loss = 0.0008898304076865315
iteration 97, loss = 0.0008567619370296597
iteration 98, loss = 0.0007217864622361958
iteration 99, loss = 0.0007550366572104394
iteration 100, loss = 0.000582949083764106
iteration 101, loss = 0.0008118003606796265
iteration 102, loss = 0.000824828923214227
iteration 103, loss = 0.0007339974981732666
iteration 104, loss = 0.0007746357005089521
iteration 105, loss = 0.001337424386292696
iteration 106, loss = 0.0009402570431120694
iteration 107, loss = 0.0009360496187582612
iteration 108, loss = 0.0011709424434229732
iteration 109, loss = 0.0008469457970932126
iteration 110, loss = 0.001105832983739674
iteration 111, loss = 0.0008484785794280469
iteration 112, loss = 0.0009685871773399413
iteration 113, loss = 0.0016575036570429802
iteration 114, loss = 0.0007835795404389501
iteration 115, loss = 0.0009270459995605052
iteration 116, loss = 0.0008074453216977417
iteration 117, loss = 0.001229310524649918
iteration 118, loss = 0.0011119783157482743
iteration 119, loss = 0.0008582534501329064
iteration 120, loss = 0.0007944314857013524
iteration 121, loss = 0.0014135763049125671
iteration 122, loss = 0.0008481068653054535
iteration 123, loss = 0.00066115747904405
iteration 124, loss = 0.0016723086591809988
iteration 125, loss = 0.0014494610950350761
iteration 126, loss = 0.0007229287875816226
iteration 127, loss = 0.0012163248611614108
iteration 128, loss = 0.0008384383982047439
iteration 129, loss = 0.0007404828211292624
iteration 130, loss = 0.0008625262416899204
iteration 131, loss = 0.0007616154034622014
iteration 132, loss = 0.001287597231566906
iteration 133, loss = 0.0015591681003570557
iteration 134, loss = 0.0008759172051213682
iteration 135, loss = 0.0011924877762794495
iteration 136, loss = 0.0007271056529134512
iteration 137, loss = 0.0015228594420477748
iteration 138, loss = 0.000735670211724937
iteration 139, loss = 0.0007132318569347262
iteration 140, loss = 0.0011281819315627217
iteration 141, loss = 0.0008821579394862056
iteration 142, loss = 0.0007444913499057293
iteration 143, loss = 0.0015248190611600876
iteration 144, loss = 0.0010692770592868328
iteration 145, loss = 0.0007361865718849003
iteration 146, loss = 0.0006948492373339832
iteration 147, loss = 0.0008446079446002841
iteration 148, loss = 0.0009436450782231987
iteration 149, loss = 0.0008509317995049059
iteration 150, loss = 0.0008631385280750692
iteration 151, loss = 0.0010288808261975646
iteration 152, loss = 0.0009071421809494495
iteration 153, loss = 0.0007251203642226756
iteration 154, loss = 0.0007390124956145883
iteration 155, loss = 0.0010873244609683752
iteration 156, loss = 0.0010810599196702242
iteration 157, loss = 0.0008101299172267318
iteration 158, loss = 0.0009039978613145649
iteration 159, loss = 0.0009431528160348535
iteration 160, loss = 0.0006298795342445374
iteration 161, loss = 0.0014518508687615395
iteration 162, loss = 0.0008815386681817472
iteration 163, loss = 0.001635490683838725
iteration 164, loss = 0.000864731497131288
iteration 165, loss = 0.0013155381893739104
iteration 166, loss = 0.0008075927617028356
iteration 167, loss = 0.0007287079934030771
iteration 168, loss = 0.001163568696938455
iteration 169, loss = 0.000677441421430558
iteration 170, loss = 0.000674044422339648
iteration 171, loss = 0.0007910168496891856
iteration 172, loss = 0.0008750323322601616
iteration 173, loss = 0.0007597265648655593
iteration 174, loss = 0.0008074772777035832
iteration 175, loss = 0.0010622250847518444
iteration 176, loss = 0.0014534894144162536
iteration 177, loss = 0.0007696555694565177
iteration 178, loss = 0.001402798225171864
iteration 179, loss = 0.0010847736848518252
iteration 180, loss = 0.0007563264225609601
iteration 181, loss = 0.0006822823779657483
iteration 182, loss = 0.0008990292553789914
iteration 183, loss = 0.0008874612976796925
iteration 184, loss = 0.0008038931409828365
iteration 185, loss = 0.0008253721753135324
iteration 186, loss = 0.0008703804924152792
iteration 187, loss = 0.0008382202358916402
iteration 188, loss = 0.0009853887604549527
iteration 189, loss = 0.0007382575422525406
iteration 190, loss = 0.0007216540398076177
iteration 191, loss = 0.0010639614192768931
iteration 192, loss = 0.0016537714982405305
iteration 193, loss = 0.0015338408993557096
iteration 194, loss = 0.0009878086857497692
iteration 195, loss = 0.0007554956246167421
iteration 196, loss = 0.0008929821196943521
iteration 197, loss = 0.0008489923784509301
iteration 198, loss = 0.000638226920273155
iteration 199, loss = 0.0007461479981429875
iteration 200, loss = 0.0014792617876082659
iteration 201, loss = 0.0014356171013787389
iteration 202, loss = 0.0008632328826934099
iteration 203, loss = 0.0009954390116035938
iteration 204, loss = 0.0007852154667489231
iteration 205, loss = 0.0008490673499181867
iteration 206, loss = 0.001060945331119001
iteration 207, loss = 0.0008263310883194208
iteration 208, loss = 0.0008009853772819042
iteration 209, loss = 0.0012361668050289154
iteration 210, loss = 0.001013558590784669
iteration 211, loss = 0.0007217432139441371
iteration 212, loss = 0.000691715395078063
iteration 213, loss = 0.0010098515776917338
iteration 214, loss = 0.0007571449968963861
iteration 215, loss = 0.0008362877415493131
iteration 216, loss = 0.0009242302621714771
iteration 217, loss = 0.0011810415890067816
iteration 218, loss = 0.0014342066133394837
iteration 219, loss = 0.0015655739698559046
iteration 220, loss = 0.0006892539677210152
iteration 221, loss = 0.0009570119436830282
iteration 222, loss = 0.0007709733908995986
iteration 223, loss = 0.0008038380765356123
iteration 224, loss = 0.0009972859406843781
iteration 225, loss = 0.0016103313537314534
iteration 226, loss = 0.0010891043348237872
iteration 227, loss = 0.0012056074338033795
iteration 228, loss = 0.0008749747648835182
iteration 229, loss = 0.0008561919676139951
iteration 230, loss = 0.0007585323182865977
iteration 231, loss = 0.0007386246579699218
iteration 232, loss = 0.0011542479041963816
iteration 233, loss = 0.0008174299728125334
iteration 234, loss = 0.001248242100700736
iteration 235, loss = 0.0009460491128265858
iteration 236, loss = 0.0008477994706481695
iteration 237, loss = 0.0009733104961924255
iteration 238, loss = 0.0008646598435007036
iteration 239, loss = 0.0012060637818649411
iteration 240, loss = 0.0014241788303479552
iteration 241, loss = 0.0008490964537486434
iteration 242, loss = 0.000983897247351706
iteration 243, loss = 0.0007634771754965186
iteration 244, loss = 0.0009954874403774738
iteration 245, loss = 0.0009382141288369894
iteration 246, loss = 0.000731235952116549
iteration 247, loss = 0.0007641016272827983
iteration 248, loss = 0.0013441969640552998
iteration 249, loss = 0.0008041992550715804
iteration 250, loss = 0.0007654505898244679
iteration 251, loss = 0.001570488908328116
iteration 252, loss = 0.0009430117206647992
iteration 253, loss = 0.001093051629140973
iteration 254, loss = 0.0009581345366314054
iteration 255, loss = 0.0007754381513223052
iteration 256, loss = 0.0008464564452879131
iteration 257, loss = 0.0009854455711320043
iteration 258, loss = 0.0009082847973331809
iteration 259, loss = 0.0010144298430532217
iteration 260, loss = 0.0008933893986977637
iteration 261, loss = 0.0009517261059954762
iteration 262, loss = 0.0009182082139886916
iteration 263, loss = 0.0007110768929123878
iteration 264, loss = 0.0013269474729895592
iteration 265, loss = 0.0007048846455290914
iteration 266, loss = 0.0008187179919332266
iteration 267, loss = 0.0009210010175593197
iteration 268, loss = 0.0007662246935069561
iteration 269, loss = 0.0007760145235806704
iteration 270, loss = 0.0008513899520039558
iteration 271, loss = 0.0009059233125299215
iteration 272, loss = 0.0008003100519999862
iteration 273, loss = 0.0007555278134532273
iteration 274, loss = 0.0010303952731192112
iteration 275, loss = 0.0008206085185520351
iteration 276, loss = 0.0010600547539070249
iteration 277, loss = 0.0010015575680881739
iteration 278, loss = 0.000936975993681699
iteration 279, loss = 0.0010050174314528704
iteration 280, loss = 0.0009158048196695745
iteration 281, loss = 0.0008092849748209119
iteration 282, loss = 0.0009107863297685981
iteration 283, loss = 0.0011882893741130829
iteration 284, loss = 0.0008698320598341525
iteration 285, loss = 0.0016349275829270482
iteration 286, loss = 0.0007602631812915206
iteration 287, loss = 0.0007557743228971958
iteration 288, loss = 0.000682114390656352
iteration 289, loss = 0.0009746706928126514
iteration 290, loss = 0.0009707266581244767
iteration 291, loss = 0.001508312881924212
iteration 292, loss = 0.0008378955535590649
iteration 293, loss = 0.000707688566762954
iteration 294, loss = 0.0008020917302928865
iteration 295, loss = 0.0009111028630286455
iteration 296, loss = 0.0008372376323677599
iteration 297, loss = 0.000879791856277734
iteration 298, loss = 0.000990097178146243
iteration 299, loss = 0.0007588426815345883
iteration 300, loss = 0.0007305954932235181
iteration 1, loss = 0.0011496581137180328
iteration 2, loss = 0.0012338187079876661
iteration 3, loss = 0.0008470697212032974
iteration 4, loss = 0.001885066507384181
iteration 5, loss = 0.0007978536887094378
iteration 6, loss = 0.0007962218369357288
iteration 7, loss = 0.0008859935915097594
iteration 8, loss = 0.001112302066758275
iteration 9, loss = 0.0007900935015641153
iteration 10, loss = 0.0009433485101908445
iteration 11, loss = 0.0010290201753377914
iteration 12, loss = 0.000964305188972503
iteration 13, loss = 0.0009210116695612669
iteration 14, loss = 0.0007288882625289261
iteration 15, loss = 0.0015578102320432663
iteration 16, loss = 0.000841418863274157
iteration 17, loss = 0.0008352923905476928
iteration 18, loss = 0.0009177406900562346
iteration 19, loss = 0.0008433185867033899
iteration 20, loss = 0.0008283256902359426
iteration 21, loss = 0.0007987087592482567
iteration 22, loss = 0.0008984990417957306
iteration 23, loss = 0.0014573672087863088
iteration 24, loss = 0.0007917763432487845
iteration 25, loss = 0.0008123689331114292
iteration 26, loss = 0.0007837984012439847
iteration 27, loss = 0.00108171789906919
iteration 28, loss = 0.000812134996522218
iteration 29, loss = 0.0008516752277500927
iteration 30, loss = 0.0012602366041392088
iteration 31, loss = 0.0007106887060217559
iteration 32, loss = 0.0009558911551721394
iteration 33, loss = 0.0008034817292355001
iteration 34, loss = 0.0013966476544737816
iteration 35, loss = 0.0007170870085246861
iteration 36, loss = 0.0006442925659939647
iteration 37, loss = 0.0017849259311333299
iteration 38, loss = 0.0018859445117413998
iteration 39, loss = 0.0008248545927926898
iteration 40, loss = 0.001674691098742187
iteration 41, loss = 0.0007750827353447676
iteration 42, loss = 0.0009342290577478707
iteration 43, loss = 0.0008521024137735367
iteration 44, loss = 0.0008346210233867168
iteration 45, loss = 0.001325551769696176
iteration 46, loss = 0.0008732345886528492
iteration 47, loss = 0.0008015375933609903
iteration 48, loss = 0.0008159783901646733
iteration 49, loss = 0.0008552907384000719
iteration 50, loss = 0.000828480115160346
iteration 51, loss = 0.0007934437599033117
iteration 52, loss = 0.0008190182270482183
iteration 53, loss = 0.0013534759636968374
iteration 54, loss = 0.0007802278269082308
iteration 55, loss = 0.001160179148428142
iteration 56, loss = 0.0008552681538276374
iteration 57, loss = 0.001525174011476338
iteration 58, loss = 0.0007214754587039351
iteration 59, loss = 0.0007096992922015488
iteration 60, loss = 0.0015020041028037667
iteration 61, loss = 0.0008662750478833914
iteration 62, loss = 0.0007451896672137082
iteration 63, loss = 0.001070672646164894
iteration 64, loss = 0.0010488452389836311
iteration 65, loss = 0.0011495706858113408
iteration 66, loss = 0.0008244346245191991
iteration 67, loss = 0.0009538306621834636
iteration 68, loss = 0.0006798404501751065
iteration 69, loss = 0.0009801497217267752
iteration 70, loss = 0.0007093545282259583
iteration 71, loss = 0.0009155415464192629
iteration 72, loss = 0.001095221028663218
iteration 73, loss = 0.0009729984449222684
iteration 74, loss = 0.000728875573258847
iteration 75, loss = 0.0008381283842027187
iteration 76, loss = 0.000904495595023036
iteration 77, loss = 0.0008388865971937776
iteration 78, loss = 0.0008644480258226395
iteration 79, loss = 0.0014048282755538821
iteration 80, loss = 0.0008398577338084579
iteration 81, loss = 0.0016734545351937413
iteration 82, loss = 0.0007684248266741633
iteration 83, loss = 0.0011725000804290175
iteration 84, loss = 0.0008056521182879806
iteration 85, loss = 0.0011721223127096891
iteration 86, loss = 0.0007654146756976843
iteration 87, loss = 0.0008047662558965385
iteration 88, loss = 0.0018392527708783746
iteration 89, loss = 0.001087083132006228
iteration 90, loss = 0.001168707269243896
iteration 91, loss = 0.0007236161618493497
iteration 92, loss = 0.0009238211205229163
iteration 93, loss = 0.0008511989144608378
iteration 94, loss = 0.0010234396904706955
iteration 95, loss = 0.0009802650893107057
iteration 96, loss = 0.0009393741493113339
iteration 97, loss = 0.0008358287159353495
iteration 98, loss = 0.000949287845287472
iteration 99, loss = 0.000835804152302444
iteration 100, loss = 0.0008249260135926306
iteration 101, loss = 0.0015129613457247615
iteration 102, loss = 0.001104916911572218
iteration 103, loss = 0.0012912927195429802
iteration 104, loss = 0.0007250831695273519
iteration 105, loss = 0.0007259081467054784
iteration 106, loss = 0.0008044308051466942
iteration 107, loss = 0.0007008767570368946
iteration 108, loss = 0.0008679620223119855
iteration 109, loss = 0.0008030524477362633
iteration 110, loss = 0.0007898889016360044
iteration 111, loss = 0.0009016431285999715
iteration 112, loss = 0.0008285476360470057
iteration 113, loss = 0.0007175370119512081
iteration 114, loss = 0.000924333231523633
iteration 115, loss = 0.0009222153457812965
iteration 116, loss = 0.0010008919052779675
iteration 117, loss = 0.000820006534922868
iteration 118, loss = 0.0006670727743767202
iteration 119, loss = 0.0010539740324020386
iteration 120, loss = 0.0007661962881684303
iteration 121, loss = 0.0006574405706487596
iteration 122, loss = 0.001215974218212068
iteration 123, loss = 0.000930947542656213
iteration 124, loss = 0.0007665912271477282
iteration 125, loss = 0.0014366349205374718
iteration 126, loss = 0.0009217329788953066
iteration 127, loss = 0.0009971760446205735
iteration 128, loss = 0.0007005830993875861
iteration 129, loss = 0.0008679102757014334
iteration 130, loss = 0.0007006775122135878
iteration 131, loss = 0.0007711582584306598
iteration 132, loss = 0.0008492222987115383
iteration 133, loss = 0.001142370980232954
iteration 134, loss = 0.0015619855839759111
iteration 135, loss = 0.0009411165374331176
iteration 136, loss = 0.0009834946831688285
iteration 137, loss = 0.0007594412309117615
iteration 138, loss = 0.0006554268766194582
iteration 139, loss = 0.0009137146407738328
iteration 140, loss = 0.0009626043029129505
iteration 141, loss = 0.0008583466988056898
iteration 142, loss = 0.0008470468455925584
iteration 143, loss = 0.0009919587755575776
iteration 144, loss = 0.001225443440489471
iteration 145, loss = 0.0012320108944550157
iteration 146, loss = 0.0013458896428346634
iteration 147, loss = 0.0007147078868001699
iteration 148, loss = 0.0010275279637426138
iteration 149, loss = 0.0010764315957203507
iteration 150, loss = 0.0008006623829714954
iteration 151, loss = 0.0007665550801903009
iteration 152, loss = 0.0009900573641061783
iteration 153, loss = 0.0007347579230554402
iteration 154, loss = 0.0008370535797439516
iteration 155, loss = 0.0008570505306124687
iteration 156, loss = 0.0009315492934547365
iteration 157, loss = 0.0006991918198764324
iteration 158, loss = 0.0008577514672651887
iteration 159, loss = 0.0007599275559186935
iteration 160, loss = 0.001837642746977508
iteration 161, loss = 0.0011046645231544971
iteration 162, loss = 0.0006788102327845991
iteration 163, loss = 0.0008670475799590349
iteration 164, loss = 0.0008813352906145155
iteration 165, loss = 0.0013877672608941793
iteration 166, loss = 0.0009406798053532839
iteration 167, loss = 0.0010022668866440654
iteration 168, loss = 0.0014533076900988817
iteration 169, loss = 0.0012559822062030435
iteration 170, loss = 0.0010651159100234509
iteration 171, loss = 0.0007855843286961317
iteration 172, loss = 0.0008862513932399452
iteration 173, loss = 0.0007017228635959327
iteration 174, loss = 0.001272866502404213
iteration 175, loss = 0.0007615499198436737
iteration 176, loss = 0.0008957125246524811
iteration 177, loss = 0.00082506233593449
iteration 178, loss = 0.0008304397924803197
iteration 179, loss = 0.000724411045666784
iteration 180, loss = 0.0015912324888631701
iteration 181, loss = 0.0022764839231967926
iteration 182, loss = 0.0007812193362042308
iteration 183, loss = 0.0007046795799396932
iteration 184, loss = 0.0017242715694010258
iteration 185, loss = 0.001032430911436677
iteration 186, loss = 0.0008759410120546818
iteration 187, loss = 0.0008230545790866017
iteration 188, loss = 0.0006849656929261982
iteration 189, loss = 0.000831078679766506
iteration 190, loss = 0.0012343154521659017
iteration 191, loss = 0.0016534815076738596
iteration 192, loss = 0.0008649294613860548
iteration 193, loss = 0.0011408383725211024
iteration 194, loss = 0.000969642773270607
iteration 195, loss = 0.0016797693679109216
iteration 196, loss = 0.0008720449986867607
iteration 197, loss = 0.0008538292604498565
iteration 198, loss = 0.0006642034277319908
iteration 199, loss = 0.0007607293082401156
iteration 200, loss = 0.0007542147068306804
iteration 201, loss = 0.0009864440653473139
iteration 202, loss = 0.0006864414899609983
iteration 203, loss = 0.0008849343867041171
iteration 204, loss = 0.0008519418188370764
iteration 205, loss = 0.0008375094039365649
iteration 206, loss = 0.0007639111718162894
iteration 207, loss = 0.0007311318768188357
iteration 208, loss = 0.0007513329037465155
iteration 209, loss = 0.0011190143413841724
iteration 210, loss = 0.0008803532691672444
iteration 211, loss = 0.0007575328927487135
iteration 212, loss = 0.00083780603017658
iteration 213, loss = 0.0010247572790831327
iteration 214, loss = 0.0007639367249794304
iteration 215, loss = 0.0007374685374088585
iteration 216, loss = 0.001030163373798132
iteration 217, loss = 0.0008037614170461893
iteration 218, loss = 0.0008517422247678041
iteration 219, loss = 0.0009334696806035936
iteration 220, loss = 0.0010664372239261866
iteration 221, loss = 0.0008840482914820313
iteration 222, loss = 0.0007739777793176472
iteration 223, loss = 0.0009601685451343656
iteration 224, loss = 0.0007000346668064594
iteration 225, loss = 0.0008482286939397454
iteration 226, loss = 0.0013145746197551489
iteration 227, loss = 0.00077752151992172
iteration 228, loss = 0.0009251596638932824
iteration 229, loss = 0.0007241625571623445
iteration 230, loss = 0.000803551112767309
iteration 231, loss = 0.0007736788247711957
iteration 232, loss = 0.0007650789339095354
iteration 233, loss = 0.000775952241383493
iteration 234, loss = 0.001928824814967811
iteration 235, loss = 0.0008494528592564166
iteration 236, loss = 0.001216318691149354
iteration 237, loss = 0.0007375599816441536
iteration 238, loss = 0.0009011111105792224
iteration 239, loss = 0.001195893855765462
iteration 240, loss = 0.0010214619105681777
iteration 241, loss = 0.0008358999039046466
iteration 242, loss = 0.0016354615800082684
iteration 243, loss = 0.0006793820066377521
iteration 244, loss = 0.0007511069998145103
iteration 245, loss = 0.0008673694101162255
iteration 246, loss = 0.0007462059147655964
iteration 247, loss = 0.0007389668608084321
iteration 248, loss = 0.0010366112692281604
iteration 249, loss = 0.0008718850440345705
iteration 250, loss = 0.0008288873359560966
iteration 251, loss = 0.0008336499449796975
iteration 252, loss = 0.0011461444664746523
iteration 253, loss = 0.0007555524935014546
iteration 254, loss = 0.0009303843835368752
iteration 255, loss = 0.0009303204715251923
iteration 256, loss = 0.0009419933776371181
iteration 257, loss = 0.0008461183169856668
iteration 258, loss = 0.0009042281308211386
iteration 259, loss = 0.000844641006551683
iteration 260, loss = 0.0016630992759019136
iteration 261, loss = 0.0008248122176155448
iteration 262, loss = 0.0008039227686822414
iteration 263, loss = 0.000955310941208154
iteration 264, loss = 0.0008743174839764833
iteration 265, loss = 0.0012868449557572603
iteration 266, loss = 0.0009085965575650334
iteration 267, loss = 0.0007993192411959171
iteration 268, loss = 0.0009987176163122058
iteration 269, loss = 0.0007936914335004985
iteration 270, loss = 0.0007973813335411251
iteration 271, loss = 0.000756984984036535
iteration 272, loss = 0.0010414386633783579
iteration 273, loss = 0.0013464131625369191
iteration 274, loss = 0.0009337274823337793
iteration 275, loss = 0.0007226030575111508
iteration 276, loss = 0.0007602921687066555
iteration 277, loss = 0.0007181240362115204
iteration 278, loss = 0.0007641585543751717
iteration 279, loss = 0.0012118861777707934
iteration 280, loss = 0.0008929793257266283
iteration 281, loss = 0.0008796038455329835
iteration 282, loss = 0.001004137797281146
iteration 283, loss = 0.0007619239622727036
iteration 284, loss = 0.0015716315247118473
iteration 285, loss = 0.0014800438657402992
iteration 286, loss = 0.0007993856561370194
iteration 287, loss = 0.0011691609397530556
iteration 288, loss = 0.0007333949324674904
iteration 289, loss = 0.0010056202299892902
iteration 290, loss = 0.0008599607390351593
iteration 291, loss = 0.001422565197572112
iteration 292, loss = 0.0007226585294120014
iteration 293, loss = 0.0008391954470425844
iteration 294, loss = 0.000799061032012105
iteration 295, loss = 0.000959747820161283
iteration 296, loss = 0.0007685984019190073
iteration 297, loss = 0.0007721169968135655
iteration 298, loss = 0.0014449853915721178
iteration 299, loss = 0.0009389225160703063
iteration 300, loss = 0.001015004818327725
iteration 1, loss = 0.001188451424241066
iteration 2, loss = 0.0008120554848574102
iteration 3, loss = 0.0008060385007411242
iteration 4, loss = 0.000940338708460331
iteration 5, loss = 0.0007668930338695645
iteration 6, loss = 0.0009256908087991178
iteration 7, loss = 0.0010579393710941076
iteration 8, loss = 0.0008962227730080485
iteration 9, loss = 0.0006802220013923943
iteration 10, loss = 0.001102189882658422
iteration 11, loss = 0.0007204455323517323
iteration 12, loss = 0.0009699126821942627
iteration 13, loss = 0.0009911819361150265
iteration 14, loss = 0.0009963937336578965
iteration 15, loss = 0.001894174376502633
iteration 16, loss = 0.0007047588005661964
iteration 17, loss = 0.0009543627384118736
iteration 18, loss = 0.0007191632175818086
iteration 19, loss = 0.000769283389672637
iteration 20, loss = 0.001173314405605197
iteration 21, loss = 0.0007403990020975471
iteration 22, loss = 0.0008063100976869464
iteration 23, loss = 0.0009928852086886764
iteration 24, loss = 0.0009038423886522651
iteration 25, loss = 0.0008362701046280563
iteration 26, loss = 0.0009438066044822335
iteration 27, loss = 0.0012069607619196177
iteration 28, loss = 0.0009514240082353354
iteration 29, loss = 0.0010082786902785301
iteration 30, loss = 0.0009960888419300318
iteration 31, loss = 0.0007368975202552974
iteration 32, loss = 0.0009792331838980317
iteration 33, loss = 0.0008348938426934183
iteration 34, loss = 0.0018110002856701612
iteration 35, loss = 0.0009846690809354186
iteration 36, loss = 0.0008201517630368471
iteration 37, loss = 0.0007262813742272556
iteration 38, loss = 0.0007279891869984567
iteration 39, loss = 0.0013965314719825983
iteration 40, loss = 0.0010948677081614733
iteration 41, loss = 0.0008657955331727862
iteration 42, loss = 0.0007487463881261647
iteration 43, loss = 0.0007996813510544598
iteration 44, loss = 0.0008083020220510662
iteration 45, loss = 0.0008095700177364051
iteration 46, loss = 0.000859453110024333
iteration 47, loss = 0.0019128033891320229
iteration 48, loss = 0.0008603574824519455
iteration 49, loss = 0.0007686924654990435
iteration 50, loss = 0.0012685878900811076
iteration 51, loss = 0.0009923004545271397
iteration 52, loss = 0.0007155353669077158
iteration 53, loss = 0.0009346669539809227
iteration 54, loss = 0.0008247300866059959
iteration 55, loss = 0.0007515155011788011
iteration 56, loss = 0.0007543425308540463
iteration 57, loss = 0.0008128981571644545
iteration 58, loss = 0.000711341854184866
iteration 59, loss = 0.0007022252539172769
iteration 60, loss = 0.000867116148583591
iteration 61, loss = 0.0007423727656714618
iteration 62, loss = 0.0007165875867940485
iteration 63, loss = 0.0010544309625402093
iteration 64, loss = 0.0008605713956058025
iteration 65, loss = 0.000777391076553613
iteration 66, loss = 0.000832878693472594
iteration 67, loss = 0.0007627326995134354
iteration 68, loss = 0.0008518436225131154
iteration 69, loss = 0.0006823483854532242
iteration 70, loss = 0.0008014977211132646
iteration 71, loss = 0.0009213543380610645
iteration 72, loss = 0.0007607192965224385
iteration 73, loss = 0.0010337682906538248
iteration 74, loss = 0.0009923938196152449
iteration 75, loss = 0.001083193114027381
iteration 76, loss = 0.0008183856843970716
iteration 77, loss = 0.0006519567687064409
iteration 78, loss = 0.0007333396933972836
iteration 79, loss = 0.0009778387611731887
iteration 80, loss = 0.0007338984287343919
iteration 81, loss = 0.0010022842325270176
iteration 82, loss = 0.0008269704994745553
iteration 83, loss = 0.0008555981912650168
iteration 84, loss = 0.0009234559256583452
iteration 85, loss = 0.000937984383199364
iteration 86, loss = 0.0011452879989519715
iteration 87, loss = 0.0014399662613868713
iteration 88, loss = 0.0007500189822167158
iteration 89, loss = 0.0008520805859006941
iteration 90, loss = 0.0008846988785080612
iteration 91, loss = 0.0008509161416441202
iteration 92, loss = 0.0010126428678631783
iteration 93, loss = 0.0009675116743892431
iteration 94, loss = 0.0009098097216337919
iteration 95, loss = 0.0009803061839193106
iteration 96, loss = 0.0016687118913978338
iteration 97, loss = 0.0007134407060220838
iteration 98, loss = 0.001640145666897297
iteration 99, loss = 0.0009666401892900467
iteration 100, loss = 0.0008382489904761314
iteration 101, loss = 0.0015702623641118407
iteration 102, loss = 0.0008330622804351151
iteration 103, loss = 0.0006811662460677326
iteration 104, loss = 0.0008730540284886956
iteration 105, loss = 0.001115562510676682
iteration 106, loss = 0.0012775079812854528
iteration 107, loss = 0.0015272303717210889
iteration 108, loss = 0.001190038863569498
iteration 109, loss = 0.0011049271561205387
iteration 110, loss = 0.0012226601829752326
iteration 111, loss = 0.0008460443932563066
iteration 112, loss = 0.002181013347581029
iteration 113, loss = 0.0008140421123243868
iteration 114, loss = 0.0008029629825614393
iteration 115, loss = 0.0012445910833775997
iteration 116, loss = 0.0009798158425837755
iteration 117, loss = 0.000640275829937309
iteration 118, loss = 0.0008263006457127631
iteration 119, loss = 0.0009560204925946891
iteration 120, loss = 0.0010094402823597193
iteration 121, loss = 0.0008457100484520197
iteration 122, loss = 0.0007528343121521175
iteration 123, loss = 0.0008476460352540016
iteration 124, loss = 0.0010663637658581138
iteration 125, loss = 0.0008470957400277257
iteration 126, loss = 0.0006842130678705871
iteration 127, loss = 0.0008775511523708701
iteration 128, loss = 0.000803509377874434
iteration 129, loss = 0.000743139476981014
iteration 130, loss = 0.0007697618566453457
iteration 131, loss = 0.0008606715127825737
iteration 132, loss = 0.001544720958918333
iteration 133, loss = 0.0009170004050247371
iteration 134, loss = 0.0008095749653875828
iteration 135, loss = 0.0007729202043265104
iteration 136, loss = 0.001592468237504363
iteration 137, loss = 0.0009785755537450314
iteration 138, loss = 0.000751396466512233
iteration 139, loss = 0.0016096584731712937
iteration 140, loss = 0.0008082923013716936
iteration 141, loss = 0.0011442002141848207
iteration 142, loss = 0.000770358950830996
iteration 143, loss = 0.0007507982081733644
iteration 144, loss = 0.0008778863120824099
iteration 145, loss = 0.0008231345564126968
iteration 146, loss = 0.001476041041314602
iteration 147, loss = 0.0009110051323659718
iteration 148, loss = 0.0010438021272420883
iteration 149, loss = 0.0007453817524947226
iteration 150, loss = 0.0008519607945345342
iteration 151, loss = 0.0008047278970479965
iteration 152, loss = 0.0007759089930914342
iteration 153, loss = 0.0012969947420060635
iteration 154, loss = 0.0011118914699181914
iteration 155, loss = 0.00109311961568892
iteration 156, loss = 0.0007137321517802775
iteration 157, loss = 0.0009401896968483925
iteration 158, loss = 0.0011457684449851513
iteration 159, loss = 0.0008569240453653038
iteration 160, loss = 0.0015667484840378165
iteration 161, loss = 0.0009907614439725876
iteration 162, loss = 0.0009131106198765337
iteration 163, loss = 0.0007133380277082324
iteration 164, loss = 0.0011820131912827492
iteration 165, loss = 0.0009437145199626684
iteration 166, loss = 0.0006955854478292167
iteration 167, loss = 0.0007221315754577518
iteration 168, loss = 0.0009684369433671236
iteration 169, loss = 0.001631106249988079
iteration 170, loss = 0.000962744583375752
iteration 171, loss = 0.0008933153003454208
iteration 172, loss = 0.0008846378186717629
iteration 173, loss = 0.0006616211612708867
iteration 174, loss = 0.0015761523973196745
iteration 175, loss = 0.0011224733898416162
iteration 176, loss = 0.0008916129590943456
iteration 177, loss = 0.0008721548365429044
iteration 178, loss = 0.0007251657661981881
iteration 179, loss = 0.0008146416512317955
iteration 180, loss = 0.0007419305620715022
iteration 181, loss = 0.0008601762820035219
iteration 182, loss = 0.0007019016775302589
iteration 183, loss = 0.0007805352215655148
iteration 184, loss = 0.0012330603785812855
iteration 185, loss = 0.000894215248990804
iteration 186, loss = 0.0007298626587726176
iteration 187, loss = 0.0007618405506946146
iteration 188, loss = 0.0007650846964679658
iteration 189, loss = 0.0012325800489634275
iteration 190, loss = 0.0007689361809752882
iteration 191, loss = 0.0007297588745132089
iteration 192, loss = 0.0011519544059410691
iteration 193, loss = 0.0007382521289400756
iteration 194, loss = 0.0014498160453513265
iteration 195, loss = 0.0014739419566467404
iteration 196, loss = 0.0010503208031877875
iteration 197, loss = 0.0007083544041961432
iteration 198, loss = 0.0009351905900985003
iteration 199, loss = 0.0013478328473865986
iteration 200, loss = 0.0007241890416480601
iteration 201, loss = 0.0014596742112189531
iteration 202, loss = 0.0011653404217213392
iteration 203, loss = 0.0008818081696517766
iteration 204, loss = 0.0008648873772472143
iteration 205, loss = 0.0016045166412368417
iteration 206, loss = 0.0007615739013999701
iteration 207, loss = 0.0009744275594130158
iteration 208, loss = 0.0010198121890425682
iteration 209, loss = 0.0008278288878500462
iteration 210, loss = 0.0015933467075228691
iteration 211, loss = 0.0008188820793293417
iteration 212, loss = 0.0013966234400868416
iteration 213, loss = 0.0008852758328430355
iteration 214, loss = 0.0008249831153079867
iteration 215, loss = 0.0010654557263478637
iteration 216, loss = 0.0007994004990905523
iteration 217, loss = 0.0010818545706570148
iteration 218, loss = 0.0009705460397526622
iteration 219, loss = 0.0007892950670793653
iteration 220, loss = 0.0012576085282489657
iteration 221, loss = 0.0006938413716852665
iteration 222, loss = 0.0015158302849158645
iteration 223, loss = 0.0007765321061015129
iteration 224, loss = 0.0010005055228248239
iteration 225, loss = 0.0010011246195062995
iteration 226, loss = 0.001439079875126481
iteration 227, loss = 0.0008986291359178722
iteration 228, loss = 0.0007434054277837276
iteration 229, loss = 0.0007932948647066951
iteration 230, loss = 0.000765548727940768
iteration 231, loss = 0.001045690500177443
iteration 232, loss = 0.0009033529204316437
iteration 233, loss = 0.0008725360967218876
iteration 234, loss = 0.0019562679808586836
iteration 235, loss = 0.0007900436175987124
iteration 236, loss = 0.0012397406389936805
iteration 237, loss = 0.0006527083460241556
iteration 238, loss = 0.0008582330192439258
iteration 239, loss = 0.0008606477058492601
iteration 240, loss = 0.0009263843530789018
iteration 241, loss = 0.0008190928492695093
iteration 242, loss = 0.0009181576315313578
iteration 243, loss = 0.0015238666674122214
iteration 244, loss = 0.0010853427229449153
iteration 245, loss = 0.0007055959431454539
iteration 246, loss = 0.0009537332807667553
iteration 247, loss = 0.0008542558643966913
iteration 248, loss = 0.0008529427577741444
iteration 249, loss = 0.0012217106996104121
iteration 250, loss = 0.0009523213375359774
iteration 251, loss = 0.0008781447540968657
iteration 252, loss = 0.0008228576043620706
iteration 253, loss = 0.0007533621974289417
iteration 254, loss = 0.0009142767521552742
iteration 255, loss = 0.0008985211607068777
iteration 256, loss = 0.000988046987913549
iteration 257, loss = 0.001436760532669723
iteration 258, loss = 0.001058158348314464
iteration 259, loss = 0.0008427167776972055
iteration 260, loss = 0.0010786264901980758
iteration 261, loss = 0.000728091225028038
iteration 262, loss = 0.001352223800495267
iteration 263, loss = 0.0009554060525260866
iteration 264, loss = 0.0008400476654060185
iteration 265, loss = 0.0007954119355417788
iteration 266, loss = 0.0006457737763412297
iteration 267, loss = 0.0015063771279528737
iteration 268, loss = 0.0011455276980996132
iteration 269, loss = 0.0008297563181258738
iteration 270, loss = 0.0006802743882872164
iteration 271, loss = 0.0008909088210202754
iteration 272, loss = 0.0007022462668828666
iteration 273, loss = 0.001072185579687357
iteration 274, loss = 0.0008253480773419142
iteration 275, loss = 0.0008140348945744336
iteration 276, loss = 0.0009428587509319186
iteration 277, loss = 0.0006719136144965887
iteration 278, loss = 0.0007270455243997276
iteration 279, loss = 0.0009879489662125707
iteration 280, loss = 0.0008186350460164249
iteration 281, loss = 0.0012839895207434893
iteration 282, loss = 0.0008031618781387806
iteration 283, loss = 0.0007563998224213719
iteration 284, loss = 0.001126208808273077
iteration 285, loss = 0.0012462971499189734
iteration 286, loss = 0.0011366477701812983
iteration 287, loss = 0.0009656338370405138
iteration 288, loss = 0.0008614044636487961
iteration 289, loss = 0.0009247423731721938
iteration 290, loss = 0.0008266286458820105
iteration 291, loss = 0.0011576339602470398
iteration 292, loss = 0.0008141344878822565
iteration 293, loss = 0.0007670712657272816
iteration 294, loss = 0.0008446929277852178
iteration 295, loss = 0.0008383946260437369
iteration 296, loss = 0.0009649839485064149
iteration 297, loss = 0.0007856080774217844
iteration 298, loss = 0.0008793782908469439
iteration 299, loss = 0.0010680996347218752
iteration 300, loss = 0.0008284702780656517
iteration 1, loss = 0.0008696640143170953
iteration 2, loss = 0.0015503333415836096
iteration 3, loss = 0.0008162885205820203
iteration 4, loss = 0.001146468217484653
iteration 5, loss = 0.0008753229049034417
iteration 6, loss = 0.0008675894350744784
iteration 7, loss = 0.0008174711838364601
iteration 8, loss = 0.0010955422185361385
iteration 9, loss = 0.0006639628554694355
iteration 10, loss = 0.0006685169646516442
iteration 11, loss = 0.0007939872448332608
iteration 12, loss = 0.0008534433436580002
iteration 13, loss = 0.0008007052820175886
iteration 14, loss = 0.00120604841504246
iteration 15, loss = 0.0012839416740462184
iteration 16, loss = 0.0009257768979296088
iteration 17, loss = 0.0007238037069328129
iteration 18, loss = 0.0007791387033648789
iteration 19, loss = 0.0006612369907088578
iteration 20, loss = 0.0015502349706366658
iteration 21, loss = 0.0008518286631442606
iteration 22, loss = 0.0011175235267728567
iteration 23, loss = 0.0008798943599686027
iteration 24, loss = 0.0009924734476953745
iteration 25, loss = 0.0008651823154650629
iteration 26, loss = 0.0008594709215685725
iteration 27, loss = 0.000923474202863872
iteration 28, loss = 0.0006550032994709909
iteration 29, loss = 0.0011695502325892448
iteration 30, loss = 0.0016117445193231106
iteration 31, loss = 0.0009449860663153231
iteration 32, loss = 0.0011970256455242634
iteration 33, loss = 0.0009874941315501928
iteration 34, loss = 0.0006755069480277598
iteration 35, loss = 0.0014875024789944291
iteration 36, loss = 0.0015923845348879695
iteration 37, loss = 0.00177595519926399
iteration 38, loss = 0.0007360157906077802
iteration 39, loss = 0.0009148938115686178
iteration 40, loss = 0.0007532170857302845
iteration 41, loss = 0.0010048823896795511
iteration 42, loss = 0.0007876069867052138
iteration 43, loss = 0.0008972699870355427
iteration 44, loss = 0.0012292314786463976
iteration 45, loss = 0.0008525910088792443
iteration 46, loss = 0.001324218581430614
iteration 47, loss = 0.0009741803514771163
iteration 48, loss = 0.0006045165355317295
iteration 49, loss = 0.000855867110658437
iteration 50, loss = 0.0008559594280086458
iteration 51, loss = 0.0012073516845703125
iteration 52, loss = 0.0006670989096164703
iteration 53, loss = 0.0008062598062679172
iteration 54, loss = 0.0008745528175495565
iteration 55, loss = 0.0009369912440888584
iteration 56, loss = 0.0010672124335542321
iteration 57, loss = 0.0007503816159442067
iteration 58, loss = 0.0010528116254135966
iteration 59, loss = 0.0015928256325423717
iteration 60, loss = 0.0010214961366727948
iteration 61, loss = 0.0007179537788033485
iteration 62, loss = 0.0007952371379360557
iteration 63, loss = 0.0007204009452834725
iteration 64, loss = 0.0008214728441089392
iteration 65, loss = 0.0008443713304586709
iteration 66, loss = 0.0008288274984806776
iteration 67, loss = 0.0007929418934509158
iteration 68, loss = 0.001765973400324583
iteration 69, loss = 0.0008731165435165167
iteration 70, loss = 0.0007134796469472349
iteration 71, loss = 0.0013431364204734564
iteration 72, loss = 0.0009558656020089984
iteration 73, loss = 0.0007447176612913609
iteration 74, loss = 0.0014226578641682863
iteration 75, loss = 0.0011302870698273182
iteration 76, loss = 0.000879709143191576
iteration 77, loss = 0.0008792078588157892
iteration 78, loss = 0.0007659458788111806
iteration 79, loss = 0.0006725335260853171
iteration 80, loss = 0.001596558024175465
iteration 81, loss = 0.0009997066808864474
iteration 82, loss = 0.0009441897855140269
iteration 83, loss = 0.0008489097817800939
iteration 84, loss = 0.0016488616820424795
iteration 85, loss = 0.001541592413559556
iteration 86, loss = 0.0007790325907990336
iteration 87, loss = 0.0008519794791936874
iteration 88, loss = 0.0009665777906775475
iteration 89, loss = 0.0009303624392487109
iteration 90, loss = 0.0007624575519002974
iteration 91, loss = 0.0008790873689576983
iteration 92, loss = 0.0008329199627041817
iteration 93, loss = 0.0006810703198425472
iteration 94, loss = 0.0010640670079737902
iteration 95, loss = 0.0011881000827997923
iteration 96, loss = 0.0008108927868306637
iteration 97, loss = 0.0008198292343877256
iteration 98, loss = 0.0006636327598243952
iteration 99, loss = 0.0008526373421773314
iteration 100, loss = 0.0008422865066677332
iteration 101, loss = 0.000838525767903775
iteration 102, loss = 0.0008008201839402318
iteration 103, loss = 0.0006842686561867595
iteration 104, loss = 0.0014437433565035462
iteration 105, loss = 0.0008219313458539546
iteration 106, loss = 0.0008606218616478145
iteration 107, loss = 0.0008434950723312795
iteration 108, loss = 0.0007198104867711663
iteration 109, loss = 0.0007915153983049095
iteration 110, loss = 0.0008046742295846343
iteration 111, loss = 0.0007881341734901071
iteration 112, loss = 0.0007319588912650943
iteration 113, loss = 0.0006380185368470848
iteration 114, loss = 0.001167652546428144
iteration 115, loss = 0.001084679737687111
iteration 116, loss = 0.0007312974194064736
iteration 117, loss = 0.0015211772406473756
iteration 118, loss = 0.0010188943706452847
iteration 119, loss = 0.0008238278096541762
iteration 120, loss = 0.0007746064802631736
iteration 121, loss = 0.0007475746097043157
iteration 122, loss = 0.0009226292604580522
iteration 123, loss = 0.0007968684658408165
iteration 124, loss = 0.00185067905113101
iteration 125, loss = 0.0007847478846088052
iteration 126, loss = 0.0008523433352820575
iteration 127, loss = 0.0007902230136096478
iteration 128, loss = 0.0008003999246284366
iteration 129, loss = 0.0011423274409025908
iteration 130, loss = 0.0009938674047589302
iteration 131, loss = 0.0014137009857222438
iteration 132, loss = 0.0007847136002965271
iteration 133, loss = 0.0010680046398192644
iteration 134, loss = 0.0007591406465508044
iteration 135, loss = 0.0015560187166556716
iteration 136, loss = 0.0009306131396442652
iteration 137, loss = 0.0015643801307305694
iteration 138, loss = 0.000916613731533289
iteration 139, loss = 0.0014826643746346235
iteration 140, loss = 0.0016799250151962042
iteration 141, loss = 0.0010273335501551628
iteration 142, loss = 0.0007335382397286594
iteration 143, loss = 0.0013429975369945168
iteration 144, loss = 0.0007954866741783917
iteration 145, loss = 0.0012019427958875895
iteration 146, loss = 0.001515062409453094
iteration 147, loss = 0.0009262930834665895
iteration 148, loss = 0.0016907390672713518
iteration 149, loss = 0.0008061129483394325
iteration 150, loss = 0.0007227875757962465
iteration 151, loss = 0.000941804435569793
iteration 152, loss = 0.0009665702818892896
iteration 153, loss = 0.0009066942147910595
iteration 154, loss = 0.0010125881526619196
iteration 155, loss = 0.000686108716763556
iteration 156, loss = 0.0011647563660517335
iteration 157, loss = 0.0007243271102197468
iteration 158, loss = 0.00084992143092677
iteration 159, loss = 0.0008650145609863102
iteration 160, loss = 0.0007214296492747962
iteration 161, loss = 0.0007558049401268363
iteration 162, loss = 0.0009025636245496571
iteration 163, loss = 0.0012515683192759752
iteration 164, loss = 0.0011228614021092653
iteration 165, loss = 0.0007820749306119978
iteration 166, loss = 0.0007940495852380991
iteration 167, loss = 0.0009488813811913133
iteration 168, loss = 0.0014478254597634077
iteration 169, loss = 0.00096670794300735
iteration 170, loss = 0.0010817490983754396
iteration 171, loss = 0.0011170414509251714
iteration 172, loss = 0.0007094460306689143
iteration 173, loss = 0.0010047784307971597
iteration 174, loss = 0.0007566831191070378
iteration 175, loss = 0.0008738205651752651
iteration 176, loss = 0.0007973838364705443
iteration 177, loss = 0.0009604638325981796
iteration 178, loss = 0.0013336476404219866
iteration 179, loss = 0.0008184139151126146
iteration 180, loss = 0.0008736046147532761
iteration 181, loss = 0.0008464590064249933
iteration 182, loss = 0.001218035933561623
iteration 183, loss = 0.0007174669299274683
iteration 184, loss = 0.0006805873126722872
iteration 185, loss = 0.0007713547674939036
iteration 186, loss = 0.0010571113089099526
iteration 187, loss = 0.0012540096649900079
iteration 188, loss = 0.0013793166726827621
iteration 189, loss = 0.0008571195648983121
iteration 190, loss = 0.0007532743038609624
iteration 191, loss = 0.0006512804538942873
iteration 192, loss = 0.0012243156088516116
iteration 193, loss = 0.000757041503675282
iteration 194, loss = 0.0012719834921881557
iteration 195, loss = 0.000857391394674778
iteration 196, loss = 0.0007943006930872798
iteration 197, loss = 0.0007098193163983524
iteration 198, loss = 0.0009744158596731722
iteration 199, loss = 0.0007357590948231518
iteration 200, loss = 0.0009284975240007043
iteration 201, loss = 0.0009075153502635658
iteration 202, loss = 0.0015661161160096526
iteration 203, loss = 0.000870133051648736
iteration 204, loss = 0.0008818692294880748
iteration 205, loss = 0.0007665837183594704
iteration 206, loss = 0.0008144081803038716
iteration 207, loss = 0.0006810494232922792
iteration 208, loss = 0.0007472879369743168
iteration 209, loss = 0.0009972858242690563
iteration 210, loss = 0.0011065955040976405
iteration 211, loss = 0.0010182609548792243
iteration 212, loss = 0.0008750313427299261
iteration 213, loss = 0.0007715530227869749
iteration 214, loss = 0.000818618864286691
iteration 215, loss = 0.000819979642983526
iteration 216, loss = 0.0007092305459082127
iteration 217, loss = 0.0009450312936678529
iteration 218, loss = 0.0008479241514578462
iteration 219, loss = 0.001040445757098496
iteration 220, loss = 0.0008086822926998138
iteration 221, loss = 0.0007359579904004931
iteration 222, loss = 0.0011938470415771008
iteration 223, loss = 0.0009210550342686474
iteration 224, loss = 0.0007403800264000893
iteration 225, loss = 0.0014485626015812159
iteration 226, loss = 0.0009802485583350062
iteration 227, loss = 0.0007964265532791615
iteration 228, loss = 0.0010222723940387368
iteration 229, loss = 0.0006532051484100521
iteration 230, loss = 0.001018364680930972
iteration 231, loss = 0.0008170445798896253
iteration 232, loss = 0.0008673649281263351
iteration 233, loss = 0.0010557483183220029
iteration 234, loss = 0.0014584314776584506
iteration 235, loss = 0.0006900254520587623
iteration 236, loss = 0.0017291945405304432
iteration 237, loss = 0.0007481539505533874
iteration 238, loss = 0.0008975105010904372
iteration 239, loss = 0.0009482341702096164
iteration 240, loss = 0.0008136623655445874
iteration 241, loss = 0.0012786320876330137
iteration 242, loss = 0.0008234223932959139
iteration 243, loss = 0.0008742318022996187
iteration 244, loss = 0.0008082921267487109
iteration 245, loss = 0.0007645292207598686
iteration 246, loss = 0.0008826780249364674
iteration 247, loss = 0.0009951534448191524
iteration 248, loss = 0.000688140164129436
iteration 249, loss = 0.0007302130688913167
iteration 250, loss = 0.0011521923588588834
iteration 251, loss = 0.0011577884433791041
iteration 252, loss = 0.0010543668176978827
iteration 253, loss = 0.0009799683466553688
iteration 254, loss = 0.0008266754448413849
iteration 255, loss = 0.0008909306488931179
iteration 256, loss = 0.0009937990689650178
iteration 257, loss = 0.00108442478813231
iteration 258, loss = 0.0013692048378288746
iteration 259, loss = 0.0009011684451252222
iteration 260, loss = 0.0008549581980332732
iteration 261, loss = 0.000768084020819515
iteration 262, loss = 0.000846875598654151
iteration 263, loss = 0.0008527992758899927
iteration 264, loss = 0.0010189348831772804
iteration 265, loss = 0.0009104085038416088
iteration 266, loss = 0.0009305701241828501
iteration 267, loss = 0.000783346826210618
iteration 268, loss = 0.0008733426220715046
iteration 269, loss = 0.0008837690111249685
iteration 270, loss = 0.0008817010093480349
iteration 271, loss = 0.0008726920932531357
iteration 272, loss = 0.001760207349434495
iteration 273, loss = 0.0006677182391285896
iteration 274, loss = 0.000731482810806483
iteration 275, loss = 0.0007851562695577741
iteration 276, loss = 0.0008258724119514227
iteration 277, loss = 0.0007610911270603538
iteration 278, loss = 0.0009224180248565972
iteration 279, loss = 0.0011419412912800908
iteration 280, loss = 0.001195136341266334
iteration 281, loss = 0.0009774621576070786
iteration 282, loss = 0.0010237449314445257
iteration 283, loss = 0.0010276949033141136
iteration 284, loss = 0.0009815833764150739
iteration 285, loss = 0.001417796709574759
iteration 286, loss = 0.0007753403624519706
iteration 287, loss = 0.0006949029047973454
iteration 288, loss = 0.0015972236869856715
iteration 289, loss = 0.0008386991685256362
iteration 290, loss = 0.0007702779839746654
iteration 291, loss = 0.0008526286692358553
iteration 292, loss = 0.000756356050260365
iteration 293, loss = 0.0015339086530730128
iteration 294, loss = 0.0007965241093188524
iteration 295, loss = 0.0007646642625331879
iteration 296, loss = 0.0006926266942173243
iteration 297, loss = 0.0007126889540813863
iteration 298, loss = 0.0009411831852048635
iteration 299, loss = 0.0007897207979112864
iteration 300, loss = 0.0009818961843848228
iteration 1, loss = 0.0007054702728055418
iteration 2, loss = 0.0010297191329300404
iteration 3, loss = 0.0015883803134784102
iteration 4, loss = 0.0011117382673546672
iteration 5, loss = 0.0008093810174614191
iteration 6, loss = 0.000935081101488322
iteration 7, loss = 0.0016520838253200054
iteration 8, loss = 0.0006753950729034841
iteration 9, loss = 0.0006649817805737257
iteration 10, loss = 0.001268969033844769
iteration 11, loss = 0.0006997259333729744
iteration 12, loss = 0.0007765196496620774
iteration 13, loss = 0.0011210535885766149
iteration 14, loss = 0.0013338893186300993
iteration 15, loss = 0.0007765335030853748
iteration 16, loss = 0.0007854572613723576
iteration 17, loss = 0.0007981519447639585
iteration 18, loss = 0.0006823263829573989
iteration 19, loss = 0.0009341024560853839
iteration 20, loss = 0.0008677743026055396
iteration 21, loss = 0.0007404725765809417
iteration 22, loss = 0.0008452392066828907
iteration 23, loss = 0.0007847641245462
iteration 24, loss = 0.0008243431802839041
iteration 25, loss = 0.0007937983609735966
iteration 26, loss = 0.0008691382245160639
iteration 27, loss = 0.0014710893156006932
iteration 28, loss = 0.0009482346940785646
iteration 29, loss = 0.0008536194218322635
iteration 30, loss = 0.0007215458899736404
iteration 31, loss = 0.0009066663915291429
iteration 32, loss = 0.0007079339120537043
iteration 33, loss = 0.0008074030629359186
iteration 34, loss = 0.0010802862234413624
iteration 35, loss = 0.0008740043267607689
iteration 36, loss = 0.0007070007850416005
iteration 37, loss = 0.0010217612143605947
iteration 38, loss = 0.000843901070766151
iteration 39, loss = 0.0015229970449581742
iteration 40, loss = 0.00077351916115731
iteration 41, loss = 0.0008916010265238583
iteration 42, loss = 0.0007042629877105355
iteration 43, loss = 0.00079031300265342
iteration 44, loss = 0.0009026320185512304
iteration 45, loss = 0.0012338233646005392
iteration 46, loss = 0.0007562061073258519
iteration 47, loss = 0.0008273700368590653
iteration 48, loss = 0.0008747558458708227
iteration 49, loss = 0.0009787227027118206
iteration 50, loss = 0.001135523896664381
iteration 51, loss = 0.0011640964075922966
iteration 52, loss = 0.0008561919676139951
iteration 53, loss = 0.0009028438944369555
iteration 54, loss = 0.0008786115795373917
iteration 55, loss = 0.001157736056484282
iteration 56, loss = 0.0008058144012466073
iteration 57, loss = 0.0009390230989083648
iteration 58, loss = 0.0007994397892616689
iteration 59, loss = 0.0014133945805951953
iteration 60, loss = 0.001075391424819827
iteration 61, loss = 0.0006411742069758475
iteration 62, loss = 0.0007477574981749058
iteration 63, loss = 0.0012586070224642754
iteration 64, loss = 0.00069058733060956
iteration 65, loss = 0.0018133267294615507
iteration 66, loss = 0.0008222372271120548
iteration 67, loss = 0.0010998586658388376
iteration 68, loss = 0.0011691632680594921
iteration 69, loss = 0.0009651893633417785
iteration 70, loss = 0.000820236629806459
iteration 71, loss = 0.0010330160148441792
iteration 72, loss = 0.0008481313707306981
iteration 73, loss = 0.0014058469096198678
iteration 74, loss = 0.0008110450580716133
iteration 75, loss = 0.0008070089388638735
iteration 76, loss = 0.0011000381782650948
iteration 77, loss = 0.0007515610195696354
iteration 78, loss = 0.0008172303787432611
iteration 79, loss = 0.0015614264411851764
iteration 80, loss = 0.001126598916016519
iteration 81, loss = 0.001458035665564239
iteration 82, loss = 0.0016172279138118029
iteration 83, loss = 0.0011135147651657462
iteration 84, loss = 0.0015537224244326353
iteration 85, loss = 0.000806608994025737
iteration 86, loss = 0.0007465067901648581
iteration 87, loss = 0.0008444810518994927
iteration 88, loss = 0.0012191993882879615
iteration 89, loss = 0.0010220464318990707
iteration 90, loss = 0.0009732741746120155
iteration 91, loss = 0.0007101125665940344
iteration 92, loss = 0.0015479997964575887
iteration 93, loss = 0.0016569803701713681
iteration 94, loss = 0.000958654738496989
iteration 95, loss = 0.0007465251255780458
iteration 96, loss = 0.00069092505145818
iteration 97, loss = 0.0008388386340811849
iteration 98, loss = 0.000767712714150548
iteration 99, loss = 0.0008480757242068648
iteration 100, loss = 0.0010782290482893586
iteration 101, loss = 0.000836740480735898
iteration 102, loss = 0.001429530675522983
iteration 103, loss = 0.0008968175388872623
iteration 104, loss = 0.0014931352343410254
iteration 105, loss = 0.000959196244366467
iteration 106, loss = 0.0008870403980836272
iteration 107, loss = 0.0011366037651896477
iteration 108, loss = 0.0007309028878808022
iteration 109, loss = 0.000822019821498543
iteration 110, loss = 0.0009316812502220273
iteration 111, loss = 0.001242556725628674
iteration 112, loss = 0.0006738195661455393
iteration 113, loss = 0.0007881889468990266
iteration 114, loss = 0.0007651006453670561
iteration 115, loss = 0.0008997738477773964
iteration 116, loss = 0.000814284139778465
iteration 117, loss = 0.0008260906906798482
iteration 118, loss = 0.0009224380482919514
iteration 119, loss = 0.0008858332876116037
iteration 120, loss = 0.0010441155172884464
iteration 121, loss = 0.0009955046698451042
iteration 122, loss = 0.0007700403220951557
iteration 123, loss = 0.001822227262891829
iteration 124, loss = 0.0007309127831831574
iteration 125, loss = 0.0006936942809261382
iteration 126, loss = 0.0006865587783977389
iteration 127, loss = 0.0007550878217443824
iteration 128, loss = 0.0008135424577631056
iteration 129, loss = 0.0009200811618939042
iteration 130, loss = 0.0007352137472480536
iteration 131, loss = 0.000764104537665844
iteration 132, loss = 0.0009818031685426831
iteration 133, loss = 0.0007357719587162137
iteration 134, loss = 0.0007671992061659694
iteration 135, loss = 0.0007591251051053405
iteration 136, loss = 0.0007312163943424821
iteration 137, loss = 0.00104682263918221
iteration 138, loss = 0.0008613390382379293
iteration 139, loss = 0.0016029855469241738
iteration 140, loss = 0.0007646821322850883
iteration 141, loss = 0.0008076506783254445
iteration 142, loss = 0.001313106738962233
iteration 143, loss = 0.0010500793578103185
iteration 144, loss = 0.0011900780955329537
iteration 145, loss = 0.0008439961238764226
iteration 146, loss = 0.0007354696863330901
iteration 147, loss = 0.0008247166988439858
iteration 148, loss = 0.0008612044039182365
iteration 149, loss = 0.0009572332492098212
iteration 150, loss = 0.0008630767115391791
iteration 151, loss = 0.0008061308180913329
iteration 152, loss = 0.0007601023535244167
iteration 153, loss = 0.0007559905061498284
iteration 154, loss = 0.0009053564281202853
iteration 155, loss = 0.0010010739788413048
iteration 156, loss = 0.0009304978884756565
iteration 157, loss = 0.0007981017115525901
iteration 158, loss = 0.0007865963852964342
iteration 159, loss = 0.001509070978499949
iteration 160, loss = 0.0015555581776425242
iteration 161, loss = 0.0007166591822169721
iteration 162, loss = 0.0007077085319906473
iteration 163, loss = 0.0007729651988483965
iteration 164, loss = 0.0008218124858103693
iteration 165, loss = 0.001059891888871789
iteration 166, loss = 0.000997575931251049
iteration 167, loss = 0.0010471455752849579
iteration 168, loss = 0.0011820204090327024
iteration 169, loss = 0.0008600890869274735
iteration 170, loss = 0.0007607285515405238
iteration 171, loss = 0.0006835494423285127
iteration 172, loss = 0.000779457448516041
iteration 173, loss = 0.0007961001829244196
iteration 174, loss = 0.0008129638154059649
iteration 175, loss = 0.0014101058477535844
iteration 176, loss = 0.0006492190295830369
iteration 177, loss = 0.0008086360758170485
iteration 178, loss = 0.0007437666645273566
iteration 179, loss = 0.0008322304929606616
iteration 180, loss = 0.0005465415888465941
iteration 181, loss = 0.0008813629392534494
iteration 182, loss = 0.0008268528617918491
iteration 183, loss = 0.001001363736577332
iteration 184, loss = 0.001105045317672193
iteration 185, loss = 0.0019873743876814842
iteration 186, loss = 0.0014947272138670087
iteration 187, loss = 0.0007393686100840569
iteration 188, loss = 0.0008478612871840596
iteration 189, loss = 0.0007548598805442452
iteration 190, loss = 0.0016587910940870643
iteration 191, loss = 0.0007877624593675137
iteration 192, loss = 0.0009813812794163823
iteration 193, loss = 0.001133534824475646
iteration 194, loss = 0.0017354588489979506
iteration 195, loss = 0.0012747665168717504
iteration 196, loss = 0.0010268855839967728
iteration 197, loss = 0.0008582895970903337
iteration 198, loss = 0.0009017958655022085
iteration 199, loss = 0.0006143730133771896
iteration 200, loss = 0.0011230214731767774
iteration 201, loss = 0.0016883196076378226
iteration 202, loss = 0.0008077939855866134
iteration 203, loss = 0.0009173875441774726
iteration 204, loss = 0.0009259990183636546
iteration 205, loss = 0.0013005513465031981
iteration 206, loss = 0.0008184917969629169
iteration 207, loss = 0.0017459120135754347
iteration 208, loss = 0.000773461302742362
iteration 209, loss = 0.001568526029586792
iteration 210, loss = 0.0010571398306638002
iteration 211, loss = 0.001014881650917232
iteration 212, loss = 0.0007407949306070805
iteration 213, loss = 0.000661606842186302
iteration 214, loss = 0.0006303151021711528
iteration 215, loss = 0.0013100244104862213
iteration 216, loss = 0.0006812980864197016
iteration 217, loss = 0.0007616791990585625
iteration 218, loss = 0.0008288375101983547
iteration 219, loss = 0.0007772851386107504
iteration 220, loss = 0.0007715574465692043
iteration 221, loss = 0.0012301959795877337
iteration 222, loss = 0.0010463772341609001
iteration 223, loss = 0.0011784119997173548
iteration 224, loss = 0.0008699542959220707
iteration 225, loss = 0.0006052129901945591
iteration 226, loss = 0.0008519437396898866
iteration 227, loss = 0.0007996018975973129
iteration 228, loss = 0.0010287787299603224
iteration 229, loss = 0.0007651550113223493
iteration 230, loss = 0.0009240876534022391
iteration 231, loss = 0.0009757672087289393
iteration 232, loss = 0.0010650181211531162
iteration 233, loss = 0.0011619237484410405
iteration 234, loss = 0.0009321844554506242
iteration 235, loss = 0.0006940898019820452
iteration 236, loss = 0.0006881326553411782
iteration 237, loss = 0.0008110746275633574
iteration 238, loss = 0.0012123339110985398
iteration 239, loss = 0.0008981425780802965
iteration 240, loss = 0.000881127140019089
iteration 241, loss = 0.0008709522662684321
iteration 242, loss = 0.0010968957794830203
iteration 243, loss = 0.002472264226526022
iteration 244, loss = 0.0009544562781229615
iteration 245, loss = 0.0007565120467916131
iteration 246, loss = 0.0010152139002457261
iteration 247, loss = 0.0010359659790992737
iteration 248, loss = 0.0011091148480772972
iteration 249, loss = 0.0009445744217373431
iteration 250, loss = 0.000801496731583029
iteration 251, loss = 0.0009543125052005053
iteration 252, loss = 0.0008061108528636396
iteration 253, loss = 0.0014336053282022476
iteration 254, loss = 0.0007714956882409751
iteration 255, loss = 0.0006770800100639462
iteration 256, loss = 0.0009950448293238878
iteration 257, loss = 0.0008082330459728837
iteration 258, loss = 0.0010152484755963087
iteration 259, loss = 0.0008568977937102318
iteration 260, loss = 0.0014235387789085507
iteration 261, loss = 0.0009865787578746676
iteration 262, loss = 0.0012166821397840977
iteration 263, loss = 0.0007819171296432614
iteration 264, loss = 0.000774245010688901
iteration 265, loss = 0.0009621442877687514
iteration 266, loss = 0.0009893508395180106
iteration 267, loss = 0.0011299363104626536
iteration 268, loss = 0.0007221493870019913
iteration 269, loss = 0.0010204437421634793
iteration 270, loss = 0.0007924585952423513
iteration 271, loss = 0.0008501912234351039
iteration 272, loss = 0.0008216831483878195
iteration 273, loss = 0.0008715251460671425
iteration 274, loss = 0.0006616725586354733
iteration 275, loss = 0.0008971996139734983
iteration 276, loss = 0.0008367044501937926
iteration 277, loss = 0.001121199456974864
iteration 278, loss = 0.0008535485249012709
iteration 279, loss = 0.0007094909087754786
iteration 280, loss = 0.0013789980439469218
iteration 281, loss = 0.0007878484320826828
iteration 282, loss = 0.0007624772260896862
iteration 283, loss = 0.0007016911986283958
iteration 284, loss = 0.0008139473502524197
iteration 285, loss = 0.0007239815895445645
iteration 286, loss = 0.0008413376635871828
iteration 287, loss = 0.0007902617799118161
iteration 288, loss = 0.0008874949999153614
iteration 289, loss = 0.0009459473076276481
iteration 290, loss = 0.0008593889069743454
iteration 291, loss = 0.0007731512887403369
iteration 292, loss = 0.000863279215991497
iteration 293, loss = 0.0009878185810521245
iteration 294, loss = 0.0018635577289387584
iteration 295, loss = 0.000840230961330235
iteration 296, loss = 0.0008412142051383853
iteration 297, loss = 0.0011388568673282862
iteration 298, loss = 0.0007191504701040685
iteration 299, loss = 0.001238456810824573
iteration 300, loss = 0.0006330069154500961
iteration 1, loss = 0.0006792109343223274
iteration 2, loss = 0.0017519958782941103
iteration 3, loss = 0.0007471322896890342
iteration 4, loss = 0.0015580912586301565
iteration 5, loss = 0.0006338986568152905
iteration 6, loss = 0.0007516607875004411
iteration 7, loss = 0.0008913526544347405
iteration 8, loss = 0.0010802270844578743
iteration 9, loss = 0.001072813756763935
iteration 10, loss = 0.0009023301536217332
iteration 11, loss = 0.000698338495567441
iteration 12, loss = 0.0009044555481523275
iteration 13, loss = 0.0008167832274921238
iteration 14, loss = 0.0015062701422721148
iteration 15, loss = 0.0009544483618810773
iteration 16, loss = 0.0009078867151401937
iteration 17, loss = 0.0008265253272838891
iteration 18, loss = 0.0009314081398770213
iteration 19, loss = 0.0009149820543825626
iteration 20, loss = 0.0007728392374701798
iteration 21, loss = 0.0007295523537322879
iteration 22, loss = 0.0007635034271515906
iteration 23, loss = 0.001125409733504057
iteration 24, loss = 0.0008270920952782035
iteration 25, loss = 0.0008199012954719365
iteration 26, loss = 0.001089494675397873
iteration 27, loss = 0.0007442730711773038
iteration 28, loss = 0.001094072125852108
iteration 29, loss = 0.0018923450261354446
iteration 30, loss = 0.0011458771768957376
iteration 31, loss = 0.0008168691419996321
iteration 32, loss = 0.0010444528888911009
iteration 33, loss = 0.0009203054942190647
iteration 34, loss = 0.0007443121867254376
iteration 35, loss = 0.0007911878637969494
iteration 36, loss = 0.0012685738038271666
iteration 37, loss = 0.0009199175983667374
iteration 38, loss = 0.0009567115921527147
iteration 39, loss = 0.0008319832850247622
iteration 40, loss = 0.0006950525566935539
iteration 41, loss = 0.001044049160555005
iteration 42, loss = 0.0007476066239178181
iteration 43, loss = 0.001632707891985774
iteration 44, loss = 0.000773084640968591
iteration 45, loss = 0.0008301894413307309
iteration 46, loss = 0.0008278733002953231
iteration 47, loss = 0.0009047557832673192
iteration 48, loss = 0.0007793735130690038
iteration 49, loss = 0.0007600210956297815
iteration 50, loss = 0.0007566119893454015
iteration 51, loss = 0.0010254980297759175
iteration 52, loss = 0.0008646746864542365
iteration 53, loss = 0.0007205529254861176
iteration 54, loss = 0.0008462184341624379
iteration 55, loss = 0.0008338519837707281
iteration 56, loss = 0.0008169776992872357
iteration 57, loss = 0.001349385129287839
iteration 58, loss = 0.0009633206645958126
iteration 59, loss = 0.0013032053830102086
iteration 60, loss = 0.0008614292019046843
iteration 61, loss = 0.0009196102037094533
iteration 62, loss = 0.0008869598968885839
iteration 63, loss = 0.0007260215352289379
iteration 64, loss = 0.00083865353371948
iteration 65, loss = 0.0011656542774289846
iteration 66, loss = 0.0007710565696470439
iteration 67, loss = 0.0008515285444445908
iteration 68, loss = 0.0007797623984515667
iteration 69, loss = 0.0008968852343969047
iteration 70, loss = 0.0008212982211261988
iteration 71, loss = 0.0008408027933910489
iteration 72, loss = 0.0011691328836604953
iteration 73, loss = 0.0009929111693054438
iteration 74, loss = 0.0007516668410971761
iteration 75, loss = 0.0009755950304679573
iteration 76, loss = 0.0009138030000030994
iteration 77, loss = 0.0007806723588146269
iteration 78, loss = 0.0008409535512328148
iteration 79, loss = 0.0007529412396252155
iteration 80, loss = 0.0013539462815970182
iteration 81, loss = 0.0010203748242929578
iteration 82, loss = 0.0009421773720532656
iteration 83, loss = 0.0006442366284318268
iteration 84, loss = 0.0007603386184200644
iteration 85, loss = 0.0008368496201001108
iteration 86, loss = 0.0008410613518208265
iteration 87, loss = 0.0023821445647627115
iteration 88, loss = 0.0008662461186759174
iteration 89, loss = 0.0008661207975819707
iteration 90, loss = 0.0010013298597186804
iteration 91, loss = 0.001679671579040587
iteration 92, loss = 0.0008114599040709436
iteration 93, loss = 0.0010492713190615177
iteration 94, loss = 0.0006940088933333755
iteration 95, loss = 0.0006714994087815285
iteration 96, loss = 0.002056468976661563
iteration 97, loss = 0.0007874288712628186
iteration 98, loss = 0.0007897738250903785
iteration 99, loss = 0.0008283829665742815
iteration 100, loss = 0.0010589160956442356
iteration 101, loss = 0.0013624699786305428
iteration 102, loss = 0.0007876551244407892
iteration 103, loss = 0.0007853548740968108
iteration 104, loss = 0.0008109699701890349
iteration 105, loss = 0.0007313060923479497
iteration 106, loss = 0.001313300454057753
iteration 107, loss = 0.0008203021716326475
iteration 108, loss = 0.0010846529621630907
iteration 109, loss = 0.0008264541393145919
iteration 110, loss = 0.000748045917134732
iteration 111, loss = 0.001087860087864101
iteration 112, loss = 0.0013168556615710258
iteration 113, loss = 0.0009177895844914019
iteration 114, loss = 0.0007862319471314549
iteration 115, loss = 0.0008169436478056014
iteration 116, loss = 0.0007240217528305948
iteration 117, loss = 0.0008400552324019372
iteration 118, loss = 0.0005759344203397632
iteration 119, loss = 0.000974895607214421
iteration 120, loss = 0.0007445158553309739
iteration 121, loss = 0.0010449470719322562
iteration 122, loss = 0.000718749244697392
iteration 123, loss = 0.0012269830331206322
iteration 124, loss = 0.0009926604107022285
iteration 125, loss = 0.0006717157666571438
iteration 126, loss = 0.0007501478539779782
iteration 127, loss = 0.001186924404464662
iteration 128, loss = 0.0007931424770504236
iteration 129, loss = 0.0015057619893923402
iteration 130, loss = 0.0007797769503667951
iteration 131, loss = 0.0011808732524514198
iteration 132, loss = 0.0005702604539692402
iteration 133, loss = 0.0010595377534627914
iteration 134, loss = 0.0006718622171320021
iteration 135, loss = 0.0007668811595067382
iteration 136, loss = 0.0009160683839581907
iteration 137, loss = 0.0008656749851070344
iteration 138, loss = 0.0006250666920095682
iteration 139, loss = 0.0016195172211155295
iteration 140, loss = 0.0011240923777222633
iteration 141, loss = 0.0010276498505845666
iteration 142, loss = 0.0006368663744069636
iteration 143, loss = 0.0008336401078850031
iteration 144, loss = 0.0008082152926363051
iteration 145, loss = 0.0007771371165290475
iteration 146, loss = 0.0014892969047650695
iteration 147, loss = 0.0007633752538822591
iteration 148, loss = 0.001613776432350278
iteration 149, loss = 0.0008755457820370793
iteration 150, loss = 0.0006077587022446096
iteration 151, loss = 0.0008772692526690662
iteration 152, loss = 0.0008144706371240318
iteration 153, loss = 0.0007955438923090696
iteration 154, loss = 0.001049163518473506
iteration 155, loss = 0.000989314983598888
iteration 156, loss = 0.0008459816453978419
iteration 157, loss = 0.0009239270584657788
iteration 158, loss = 0.0007583708502352238
iteration 159, loss = 0.0008663315093144774
iteration 160, loss = 0.0008565860916860402
iteration 161, loss = 0.0007987178396433592
iteration 162, loss = 0.0008406942361034453
iteration 163, loss = 0.001484497799538076
iteration 164, loss = 0.0008852717000991106
iteration 165, loss = 0.0008757154573686421
iteration 166, loss = 0.0008833238389343023
iteration 167, loss = 0.0008589192293584347
iteration 168, loss = 0.0010217995150014758
iteration 169, loss = 0.0008336592582054436
iteration 170, loss = 0.0011567088076844811
iteration 171, loss = 0.0008061177795752883
iteration 172, loss = 0.000685756909660995
iteration 173, loss = 0.0010150885209441185
iteration 174, loss = 0.000807973206974566
iteration 175, loss = 0.001020451309159398
iteration 176, loss = 0.0008788793347775936
iteration 177, loss = 0.0009155044099316001
iteration 178, loss = 0.0007872456917539239
iteration 179, loss = 0.0007654462824575603
iteration 180, loss = 0.0008100965642370284
iteration 181, loss = 0.0015953723341226578
iteration 182, loss = 0.0008144032908603549
iteration 183, loss = 0.0008690126705914736
iteration 184, loss = 0.0009927183855324984
iteration 185, loss = 0.000871557742357254
iteration 186, loss = 0.0014720623148605227
iteration 187, loss = 0.0010432087583467364
iteration 188, loss = 0.001282860292121768
iteration 189, loss = 0.000962526595685631
iteration 190, loss = 0.0008804241078905761
iteration 191, loss = 0.000928800436668098
iteration 192, loss = 0.0010461300844326615
iteration 193, loss = 0.0010273503139615059
iteration 194, loss = 0.0007520020008087158
iteration 195, loss = 0.0009231938747689128
iteration 196, loss = 0.0007941473741084337
iteration 197, loss = 0.0011699926108121872
iteration 198, loss = 0.0007007684325799346
iteration 199, loss = 0.0017079489771276712
iteration 200, loss = 0.0009009898640215397
iteration 201, loss = 0.000691588968038559
iteration 202, loss = 0.0009990893304347992
iteration 203, loss = 0.0006951193208806217
iteration 204, loss = 0.000794679915998131
iteration 205, loss = 0.0007585761486552656
iteration 206, loss = 0.0009961273754015565
iteration 207, loss = 0.0016167802968993783
iteration 208, loss = 0.000735610316041857
iteration 209, loss = 0.0010293737286701798
iteration 210, loss = 0.0015406309394165874
iteration 211, loss = 0.0015931386733427644
iteration 212, loss = 0.0010175327770411968
iteration 213, loss = 0.0007822844781912863
iteration 214, loss = 0.0008302684291265905
iteration 215, loss = 0.000677643867675215
iteration 216, loss = 0.0009371583582833409
iteration 217, loss = 0.0008259020978584886
iteration 218, loss = 0.0010353968245908618
iteration 219, loss = 0.001274488284252584
iteration 220, loss = 0.0007922925287857652
iteration 221, loss = 0.0007207698072306812
iteration 222, loss = 0.0007222386193461716
iteration 223, loss = 0.0006904492620378733
iteration 224, loss = 0.0007273487863130867
iteration 225, loss = 0.0007491387077607214
iteration 226, loss = 0.0007822301704436541
iteration 227, loss = 0.0006839865236543119
iteration 228, loss = 0.0008934513898566365
iteration 229, loss = 0.0006381862913258374
iteration 230, loss = 0.000709843123331666
iteration 231, loss = 0.0007051110733300447
iteration 232, loss = 0.0006743489648215473
iteration 233, loss = 0.0012583555653691292
iteration 234, loss = 0.0010758669814094901
iteration 235, loss = 0.0011823429958894849
iteration 236, loss = 0.0008483573328703642
iteration 237, loss = 0.0008576017571613193
iteration 238, loss = 0.0008626797934994102
iteration 239, loss = 0.0009248437127098441
iteration 240, loss = 0.0012022837763652205
iteration 241, loss = 0.001866247272118926
iteration 242, loss = 0.0012860982678830624
iteration 243, loss = 0.0008329339325428009
iteration 244, loss = 0.0006649965071119368
iteration 245, loss = 0.0013490468263626099
iteration 246, loss = 0.0008047541487030685
iteration 247, loss = 0.0010421543847769499
iteration 248, loss = 0.0008453060290776193
iteration 249, loss = 0.0006966709042899311
iteration 250, loss = 0.0013898604083806276
iteration 251, loss = 0.0010216834489256144
iteration 252, loss = 0.000652888382319361
iteration 253, loss = 0.001045502838678658
iteration 254, loss = 0.000911105191335082
iteration 255, loss = 0.0007903562509454787
iteration 256, loss = 0.0006239445065148175
iteration 257, loss = 0.0009894543327391148
iteration 258, loss = 0.001009932835586369
iteration 259, loss = 0.0008425258565694094
iteration 260, loss = 0.001573040266521275
iteration 261, loss = 0.0012715656775981188
iteration 262, loss = 0.0008527904283255339
iteration 263, loss = 0.0011564897140488029
iteration 264, loss = 0.0008324818918481469
iteration 265, loss = 0.001105588860809803
iteration 266, loss = 0.0009251406881958246
iteration 267, loss = 0.0007650689221918583
iteration 268, loss = 0.0015914682298898697
iteration 269, loss = 0.0008778230403549969
iteration 270, loss = 0.0006968710804358125
iteration 271, loss = 0.0008963207365013659
iteration 272, loss = 0.0014544370351359248
iteration 273, loss = 0.0009812706848606467
iteration 274, loss = 0.0007304243627004325
iteration 275, loss = 0.000949379347730428
iteration 276, loss = 0.000910705653950572
iteration 277, loss = 0.0009501032764092088
iteration 278, loss = 0.0007766922935843468
iteration 279, loss = 0.0007416436565108597
iteration 280, loss = 0.0007091269944794476
iteration 281, loss = 0.00135459890589118
iteration 282, loss = 0.0014488240703940392
iteration 283, loss = 0.000767305085901171
iteration 284, loss = 0.0009792237542569637
iteration 285, loss = 0.0010591717436909676
iteration 286, loss = 0.0009035593247972429
iteration 287, loss = 0.0009500155574642122
iteration 288, loss = 0.00205571623519063
iteration 289, loss = 0.0012414433294907212
iteration 290, loss = 0.0006792709464207292
iteration 291, loss = 0.0008800206705927849
iteration 292, loss = 0.0006465620244853199
iteration 293, loss = 0.0007722218870185316
iteration 294, loss = 0.0008568172343075275
iteration 295, loss = 0.0015595097793266177
iteration 296, loss = 0.0011115928646177053
iteration 297, loss = 0.0011682476615533233
iteration 298, loss = 0.0010341943707317114
iteration 299, loss = 0.0012837680988013744
iteration 300, loss = 0.001561017008498311
iteration 1, loss = 0.0007761196466162801
iteration 2, loss = 0.000736432324629277
iteration 3, loss = 0.0014626375632360578
iteration 4, loss = 0.0007705750758759677
iteration 5, loss = 0.0008335463935509324
iteration 6, loss = 0.000809809600468725
iteration 7, loss = 0.00068821688182652
iteration 8, loss = 0.001128796604461968
iteration 9, loss = 0.0014789141714572906
iteration 10, loss = 0.0009118632297031581
iteration 11, loss = 0.0017588769551366568
iteration 12, loss = 0.001132434350438416
iteration 13, loss = 0.0007756659178994596
iteration 14, loss = 0.0011720354668796062
iteration 15, loss = 0.0012115142308175564
iteration 16, loss = 0.0015895082615315914
iteration 17, loss = 0.0011258918093517423
iteration 18, loss = 0.0014283399796113372
iteration 19, loss = 0.0013881124323233962
iteration 20, loss = 0.0011112848296761513
iteration 21, loss = 0.0007545507978647947
iteration 22, loss = 0.001478118821978569
iteration 23, loss = 0.0008568816701881588
iteration 24, loss = 0.0008566672331653535
iteration 25, loss = 0.00139553751796484
iteration 26, loss = 0.0007475230959244072
iteration 27, loss = 0.001006603823043406
iteration 28, loss = 0.0007735432591289282
iteration 29, loss = 0.0008660201565362513
iteration 30, loss = 0.0007897667237557471
iteration 31, loss = 0.0009818455437198281
iteration 32, loss = 0.0009202741202898324
iteration 33, loss = 0.0008891996694728732
iteration 34, loss = 0.0010892063146457076
iteration 35, loss = 0.0012334291823208332
iteration 36, loss = 0.0007692557410337031
iteration 37, loss = 0.0007601589313708246
iteration 38, loss = 0.0009072478860616684
iteration 39, loss = 0.0010620204266160727
iteration 40, loss = 0.0008553515654057264
iteration 41, loss = 0.0006862346199341118
iteration 42, loss = 0.0009689007420092821
iteration 43, loss = 0.000996691407635808
iteration 44, loss = 0.0006367915775626898
iteration 45, loss = 0.0011065956205129623
iteration 46, loss = 0.0008637639693915844
iteration 47, loss = 0.0008979576523415744
iteration 48, loss = 0.0006531095132231712
iteration 49, loss = 0.0007868192042224109
iteration 50, loss = 0.0008909813477657735
iteration 51, loss = 0.00098078150767833
iteration 52, loss = 0.000979853211902082
iteration 53, loss = 0.0008598398417234421
iteration 54, loss = 0.0007543354295194149
iteration 55, loss = 0.0008126060711219907
iteration 56, loss = 0.0010527389822527766
iteration 57, loss = 0.0009134672000072896
iteration 58, loss = 0.0006685606786049902
iteration 59, loss = 0.0013288760092109442
iteration 60, loss = 0.0011865950655192137
iteration 61, loss = 0.0016156297642737627
iteration 62, loss = 0.000954942312091589
iteration 63, loss = 0.0007413381244987249
iteration 64, loss = 0.0009432830265723169
iteration 65, loss = 0.000772412633523345
iteration 66, loss = 0.0010261256247758865
iteration 67, loss = 0.0008864745614118874
iteration 68, loss = 0.0009373629582114518
iteration 69, loss = 0.0009739947272464633
iteration 70, loss = 0.000831650453619659
iteration 71, loss = 0.0009691692539490759
iteration 72, loss = 0.000993897090665996
iteration 73, loss = 0.0015670773573219776
iteration 74, loss = 0.0011783691588789225
iteration 75, loss = 0.001109852222725749
iteration 76, loss = 0.000702095334418118
iteration 77, loss = 0.0007228599861264229
iteration 78, loss = 0.0011545791057869792
iteration 79, loss = 0.0008962347637861967
iteration 80, loss = 0.0009973245905712247
iteration 81, loss = 0.0009052663226611912
iteration 82, loss = 0.0007873471477068961
iteration 83, loss = 0.0007290315115824342
iteration 84, loss = 0.0008692286210134625
iteration 85, loss = 0.0007858973112888634
iteration 86, loss = 0.0008517486276105046
iteration 87, loss = 0.0008421522798016667
iteration 88, loss = 0.000746289559174329
iteration 89, loss = 0.0009352262713946402
iteration 90, loss = 0.0009750201134011149
iteration 91, loss = 0.0008164698956534266
iteration 92, loss = 0.0007785351481288671
iteration 93, loss = 0.0010649163741618395
iteration 94, loss = 0.0014836848713457584
iteration 95, loss = 0.0017746019875630736
iteration 96, loss = 0.0017163490410894156
iteration 97, loss = 0.0017296340083703399
iteration 98, loss = 0.00137830781750381
iteration 99, loss = 0.0008677808218635619
iteration 100, loss = 0.0006962742190808058
iteration 101, loss = 0.0008818471105769277
iteration 102, loss = 0.0007434668950736523
iteration 103, loss = 0.0006752925692126155
iteration 104, loss = 0.0010154476622119546
iteration 105, loss = 0.0021270872093737125
iteration 106, loss = 0.0010516941547393799
iteration 107, loss = 0.0009176284656859934
iteration 108, loss = 0.001155084348283708
iteration 109, loss = 0.0008202502503991127
iteration 110, loss = 0.0014420640654861927
iteration 111, loss = 0.0016189186135306954
iteration 112, loss = 0.0008663312182761729
iteration 113, loss = 0.0006892949459142983
iteration 114, loss = 0.0015387440798804164
iteration 115, loss = 0.0011190795339643955
iteration 116, loss = 0.0007628691964782774
iteration 117, loss = 0.0009869950590655208
iteration 118, loss = 0.0008495067595504224
iteration 119, loss = 0.0008318339241668582
iteration 120, loss = 0.0009881936712190509
iteration 121, loss = 0.000728721497580409
iteration 122, loss = 0.0008443397236987948
iteration 123, loss = 0.001183134620077908
iteration 124, loss = 0.0010876754531636834
iteration 125, loss = 0.0008338438929058611
iteration 126, loss = 0.001556508825160563
iteration 127, loss = 0.0007010542904026806
iteration 128, loss = 0.0009099930175580084
iteration 129, loss = 0.0010866994271054864
iteration 130, loss = 0.0007015285664238036
iteration 131, loss = 0.0008876181673258543
iteration 132, loss = 0.0009093396947719157
iteration 133, loss = 0.001032619271427393
iteration 134, loss = 0.0011311491252854466
iteration 135, loss = 0.0007573123439215124
iteration 136, loss = 0.0007033279398456216
iteration 137, loss = 0.0008643994806334376
iteration 138, loss = 0.0012017018161714077
iteration 139, loss = 0.0007083295495249331
iteration 140, loss = 0.0010783460456877947
iteration 141, loss = 0.0009788053575903177
iteration 142, loss = 0.0011660048039630055
iteration 143, loss = 0.0008671365212649107
iteration 144, loss = 0.0008222925243899226
iteration 145, loss = 0.0008404773543588817
iteration 146, loss = 0.0008517910027876496
iteration 147, loss = 0.0011031196918338537
iteration 148, loss = 0.0007084575481712818
iteration 149, loss = 0.0009601081255823374
iteration 150, loss = 0.0007247051689773798
iteration 151, loss = 0.0010242832358926535
iteration 152, loss = 0.0008601531153544784
iteration 153, loss = 0.000720984535291791
iteration 154, loss = 0.0007458159234374762
iteration 155, loss = 0.0007818897720426321
iteration 156, loss = 0.0013363879406824708
iteration 157, loss = 0.0007942562806420028
iteration 158, loss = 0.0007300482247956097
iteration 159, loss = 0.0006519865710288286
iteration 160, loss = 0.0007578998920507729
iteration 161, loss = 0.0015501037705689669
iteration 162, loss = 0.00086116639431566
iteration 163, loss = 0.0007966557750478387
iteration 164, loss = 0.0007802275358699262
iteration 165, loss = 0.0009123949566856027
iteration 166, loss = 0.0008868732256814837
iteration 167, loss = 0.0008339674677699804
iteration 168, loss = 0.0009454280370846391
iteration 169, loss = 0.0007563167600892484
iteration 170, loss = 0.0008361763320863247
iteration 171, loss = 0.001104909460991621
iteration 172, loss = 0.000792691542301327
iteration 173, loss = 0.001519984332844615
iteration 174, loss = 0.0008652188698761165
iteration 175, loss = 0.0008446003776043653
iteration 176, loss = 0.001497133169323206
iteration 177, loss = 0.0008218279108405113
iteration 178, loss = 0.0011130879865959287
iteration 179, loss = 0.0008786692633293569
iteration 180, loss = 0.000742775562684983
iteration 181, loss = 0.0007359868031926453
iteration 182, loss = 0.0007429424440488219
iteration 183, loss = 0.0008904332644306123
iteration 184, loss = 0.0010319480206817389
iteration 185, loss = 0.0012080572778359056
iteration 186, loss = 0.001483009080402553
iteration 187, loss = 0.0008137782569974661
iteration 188, loss = 0.0008920873515307903
iteration 189, loss = 0.0007466918905265629
iteration 190, loss = 0.0006831806385889649
iteration 191, loss = 0.0014810144202783704
iteration 192, loss = 0.0010778105352073908
iteration 193, loss = 0.0008194141555577517
iteration 194, loss = 0.0006993378046900034
iteration 195, loss = 0.0016150915762409568
iteration 196, loss = 0.000814034603536129
iteration 197, loss = 0.0009900176664814353
iteration 198, loss = 0.0008058840758167207
iteration 199, loss = 0.0010597349610179663
iteration 200, loss = 0.0011169702047482133
iteration 201, loss = 0.0008432515896856785
iteration 202, loss = 0.0009617390460334718
iteration 203, loss = 0.0007226580055430532
iteration 204, loss = 0.0007584926788695157
iteration 205, loss = 0.0008011501049622893
iteration 206, loss = 0.0007574567571282387
iteration 207, loss = 0.0007828075904399157
iteration 208, loss = 0.001365642063319683
iteration 209, loss = 0.001062719151377678
iteration 210, loss = 0.0008242142503149807
iteration 211, loss = 0.0007219789549708366
iteration 212, loss = 0.00113503762986511
iteration 213, loss = 0.001386161195114255
iteration 214, loss = 0.0008563186274841428
iteration 215, loss = 0.0008682821644470096
iteration 216, loss = 0.0009243434178642929
iteration 217, loss = 0.0008019538363441825
iteration 218, loss = 0.0012075393460690975
iteration 219, loss = 0.0007656302768737078
iteration 220, loss = 0.0010560627561062574
iteration 221, loss = 0.0014609138015657663
iteration 222, loss = 0.0006983170169405639
iteration 223, loss = 0.0009824552107602358
iteration 224, loss = 0.0010811156826093793
iteration 225, loss = 0.0008429650915786624
iteration 226, loss = 0.0009118169546127319
iteration 227, loss = 0.0007848425302654505
iteration 228, loss = 0.0008549741469323635
iteration 229, loss = 0.0011351396096870303
iteration 230, loss = 0.000692785601131618
iteration 231, loss = 0.0008759038173593581
iteration 232, loss = 0.0008859885856509209
iteration 233, loss = 0.001267403014935553
iteration 234, loss = 0.0015069999499246478
iteration 235, loss = 0.0009819765109568834
iteration 236, loss = 0.0015494705876335502
iteration 237, loss = 0.000726439175195992
iteration 238, loss = 0.001181930536404252
iteration 239, loss = 0.000858520099427551
iteration 240, loss = 0.000988540006801486
iteration 241, loss = 0.0007505160174332559
iteration 242, loss = 0.0007328949286602437
iteration 243, loss = 0.0007129780715331435
iteration 244, loss = 0.0010064635425806046
iteration 245, loss = 0.0010368179064244032
iteration 246, loss = 0.0006685527041554451
iteration 247, loss = 0.000991361797787249
iteration 248, loss = 0.0010544187389314175
iteration 249, loss = 0.0006981129408814013
iteration 250, loss = 0.0007542473613284528
iteration 251, loss = 0.0008941027917899191
iteration 252, loss = 0.000840385677292943
iteration 253, loss = 0.000772609084378928
iteration 254, loss = 0.0007253155345097184
iteration 255, loss = 0.0011597530683502555
iteration 256, loss = 0.0007188397576101124
iteration 257, loss = 0.0007326633203774691
iteration 258, loss = 0.0011156597174704075
iteration 259, loss = 0.0009190468699671328
iteration 260, loss = 0.0007747248164378107
iteration 261, loss = 0.0006756961811333895
iteration 262, loss = 0.0007518535130657256
iteration 263, loss = 0.0009191368008032441
iteration 264, loss = 0.0011630969820544124
iteration 265, loss = 0.0007016243180260062
iteration 266, loss = 0.0009423262672498822
iteration 267, loss = 0.0008889113087207079
iteration 268, loss = 0.0009691285085864365
iteration 269, loss = 0.0008373091113753617
iteration 270, loss = 0.0010792692191898823
iteration 271, loss = 0.0008074461366049945
iteration 272, loss = 0.0007779152365401387
iteration 273, loss = 0.0012049702927470207
iteration 274, loss = 0.0010064110392704606
iteration 275, loss = 0.0007058731280267239
iteration 276, loss = 0.0007827244699001312
iteration 277, loss = 0.0006758885574527085
iteration 278, loss = 0.0006385738379321992
iteration 279, loss = 0.0006791657069697976
iteration 280, loss = 0.0008413543691858649
iteration 281, loss = 0.0008858966175466776
iteration 282, loss = 0.001614203560166061
iteration 283, loss = 0.0008345453534275293
iteration 284, loss = 0.000989599502645433
iteration 285, loss = 0.0006897718994878232
iteration 286, loss = 0.0009315284551121294
iteration 287, loss = 0.000995333888567984
iteration 288, loss = 0.0011759012704715133
iteration 289, loss = 0.0007884697988629341
iteration 290, loss = 0.0007711679209023714
iteration 291, loss = 0.0007270178757607937
iteration 292, loss = 0.0014857114292681217
iteration 293, loss = 0.0007997391512617469
iteration 294, loss = 0.0007890464039519429
iteration 295, loss = 0.0007874282891862094
iteration 296, loss = 0.0009963677730411291
iteration 297, loss = 0.0007442219648510218
iteration 298, loss = 0.0007423245115205646
iteration 299, loss = 0.001218281453475356
iteration 300, loss = 0.0006799701368436217
iteration 1, loss = 0.0007906961836852133
iteration 2, loss = 0.0008254431304521859
iteration 3, loss = 0.0007280281861312687
iteration 4, loss = 0.0015840294072404504
iteration 5, loss = 0.0008962751016952097
iteration 6, loss = 0.0016528534470126033
iteration 7, loss = 0.000893001037184149
iteration 8, loss = 0.00105601130053401
iteration 9, loss = 0.0007792974356561899
iteration 10, loss = 0.0012905902694910765
iteration 11, loss = 0.0008070094045251608
iteration 12, loss = 0.0008864247938618064
iteration 13, loss = 0.0008009455050341785
iteration 14, loss = 0.0009463944006711245
iteration 15, loss = 0.0007882206118665636
iteration 16, loss = 0.0011246941285207868
iteration 17, loss = 0.0008696274599060416
iteration 18, loss = 0.0015531626995652914
iteration 19, loss = 0.000720214331522584
iteration 20, loss = 0.0008721904596313834
iteration 21, loss = 0.0010693491203710437
iteration 22, loss = 0.0009331452893093228
iteration 23, loss = 0.0006838112603873014
iteration 24, loss = 0.0008549160556867719
iteration 25, loss = 0.001531723770312965
iteration 26, loss = 0.0007235528319142759
iteration 27, loss = 0.0006270579760894179
iteration 28, loss = 0.0009064374607987702
iteration 29, loss = 0.0007277053082361817
iteration 30, loss = 0.0008803745731711388
iteration 31, loss = 0.0009133520070463419
iteration 32, loss = 0.0007351234089583158
iteration 33, loss = 0.0007141351234167814
iteration 34, loss = 0.0009420821443200111
iteration 35, loss = 0.0011207177303731441
iteration 36, loss = 0.0013000002363696694
iteration 37, loss = 0.000994508620351553
iteration 38, loss = 0.0008182094898074865
iteration 39, loss = 0.0016282934229820967
iteration 40, loss = 0.0006916517741046846
iteration 41, loss = 0.0007826481014490128
iteration 42, loss = 0.0009012252558022738
iteration 43, loss = 0.0007615576032549143
iteration 44, loss = 0.0008165218168869615
iteration 45, loss = 0.0007738273707218468
iteration 46, loss = 0.0010197634110227227
iteration 47, loss = 0.0006422621081583202
iteration 48, loss = 0.0018227521795779467
iteration 49, loss = 0.0018972058314830065
iteration 50, loss = 0.0010153843322768807
iteration 51, loss = 0.0006582162459380925
iteration 52, loss = 0.0015266400296241045
iteration 53, loss = 0.0007796913851052523
iteration 54, loss = 0.000760251481551677
iteration 55, loss = 0.001023022341541946
iteration 56, loss = 0.001278862589970231
iteration 57, loss = 0.0006577003514394164
iteration 58, loss = 0.0008532342035323381
iteration 59, loss = 0.0007338149007409811
iteration 60, loss = 0.0008493909845128655
iteration 61, loss = 0.0008612041128799319
iteration 62, loss = 0.0009484713664278388
iteration 63, loss = 0.0014249146915972233
iteration 64, loss = 0.0007769264630042017
iteration 65, loss = 0.000715423549991101
iteration 66, loss = 0.0009784259600564837
iteration 67, loss = 0.000725488702300936
iteration 68, loss = 0.000912496296223253
iteration 69, loss = 0.000727848382666707
iteration 70, loss = 0.0007329451618716121
iteration 71, loss = 0.001302557997405529
iteration 72, loss = 0.001016964204609394
iteration 73, loss = 0.000778060988523066
iteration 74, loss = 0.001636222586967051
iteration 75, loss = 0.0008581246947869658
iteration 76, loss = 0.001061242655850947
iteration 77, loss = 0.0008572438964620233
iteration 78, loss = 0.0020653337705880404
iteration 79, loss = 0.0006851335638202727
iteration 80, loss = 0.0008085593581199646
iteration 81, loss = 0.0015632613794878125
iteration 82, loss = 0.0006949188536964357
iteration 83, loss = 0.0009109754464589059
iteration 84, loss = 0.000857496284879744
iteration 85, loss = 0.000713727786205709
iteration 86, loss = 0.0009071112726815045
iteration 87, loss = 0.0008473271736875176
iteration 88, loss = 0.001194125972688198
iteration 89, loss = 0.0008747595711611211
iteration 90, loss = 0.0009664758108556271
iteration 91, loss = 0.0008110574563033879
iteration 92, loss = 0.0012306220596656203
iteration 93, loss = 0.0010251447092741728
iteration 94, loss = 0.0010639338288456202
iteration 95, loss = 0.0011566075263544917
iteration 96, loss = 0.0012258250499144197
iteration 97, loss = 0.0014003084506839514
iteration 98, loss = 0.0007822986226528883
iteration 99, loss = 0.0006890557124279439
iteration 100, loss = 0.001555285300128162
iteration 101, loss = 0.0007365394267253578
iteration 102, loss = 0.0008085343288257718
iteration 103, loss = 0.001024328637868166
iteration 104, loss = 0.0013231030898168683
iteration 105, loss = 0.00115305301733315
iteration 106, loss = 0.0007713602390140295
iteration 107, loss = 0.0010447874665260315
iteration 108, loss = 0.0010321077425032854
iteration 109, loss = 0.001630868180654943
iteration 110, loss = 0.0008663283661007881
iteration 111, loss = 0.0008040404645726085
iteration 112, loss = 0.0011435422347858548
iteration 113, loss = 0.0008590339566580951
iteration 114, loss = 0.0009182420908473432
iteration 115, loss = 0.0012438754783943295
iteration 116, loss = 0.001451552496291697
iteration 117, loss = 0.0010740029392763972
iteration 118, loss = 0.0007263438892550766
iteration 119, loss = 0.0009397421381436288
iteration 120, loss = 0.0009704389958642423
iteration 121, loss = 0.0007414104184135795
iteration 122, loss = 0.0008410267764702439
iteration 123, loss = 0.0017790109850466251
iteration 124, loss = 0.0007794080302119255
iteration 125, loss = 0.0008059993851929903
iteration 126, loss = 0.0010810859967023134
iteration 127, loss = 0.0008818736532703042
iteration 128, loss = 0.0006988412933424115
iteration 129, loss = 0.0007463098154403269
iteration 130, loss = 0.0007581039099022746
iteration 131, loss = 0.0007714751409366727
iteration 132, loss = 0.0009279372170567513
iteration 133, loss = 0.0008691344410181046
iteration 134, loss = 0.0008827902493067086
iteration 135, loss = 0.0009573565912432969
iteration 136, loss = 0.0014441328821703792
iteration 137, loss = 0.0007769186631776392
iteration 138, loss = 0.0009764484711922705
iteration 139, loss = 0.0006539738387800753
iteration 140, loss = 0.0007535394397564232
iteration 141, loss = 0.0007147001451812685
iteration 142, loss = 0.0014104378642514348
iteration 143, loss = 0.0009997088927775621
iteration 144, loss = 0.00113837665412575
iteration 145, loss = 0.0007687715697102249
iteration 146, loss = 0.0010043660877272487
iteration 147, loss = 0.0007691881037317216
iteration 148, loss = 0.0008448179578408599
iteration 149, loss = 0.000726598605979234
iteration 150, loss = 0.0010989367729052901
iteration 151, loss = 0.0014776481548324227
iteration 152, loss = 0.0009368476457893848
iteration 153, loss = 0.0014303689822554588
iteration 154, loss = 0.0007942734519019723
iteration 155, loss = 0.0007579303928650916
iteration 156, loss = 0.0007520552026107907
iteration 157, loss = 0.0006684543914161623
iteration 158, loss = 0.0015125458594411612
iteration 159, loss = 0.0010449792025610805
iteration 160, loss = 0.0007618428790010512
iteration 161, loss = 0.0007883785874582827
iteration 162, loss = 0.0011255386052653193
iteration 163, loss = 0.0008246655925177038
iteration 164, loss = 0.0009307961445301771
iteration 165, loss = 0.001025410951115191
iteration 166, loss = 0.000869283452630043
iteration 167, loss = 0.0008959068218246102
iteration 168, loss = 0.002177511341869831
iteration 169, loss = 0.0008024614071473479
iteration 170, loss = 0.0006763142882846296
iteration 171, loss = 0.0007799124578014016
iteration 172, loss = 0.0006458104471676052
iteration 173, loss = 0.0007331294473260641
iteration 174, loss = 0.000852182216476649
iteration 175, loss = 0.0011364867677912116
iteration 176, loss = 0.0007611813489347696
iteration 177, loss = 0.0007225474109873176
iteration 178, loss = 0.0006303478148765862
iteration 179, loss = 0.0016733028460294008
iteration 180, loss = 0.0008837327477522194
iteration 181, loss = 0.0009659964125603437
iteration 182, loss = 0.0008958990219980478
iteration 183, loss = 0.0007645466248504817
iteration 184, loss = 0.0009297059150412679
iteration 185, loss = 0.0008108434267342091
iteration 186, loss = 0.0011210086522623897
iteration 187, loss = 0.000705164740793407
iteration 188, loss = 0.0007923615630716085
iteration 189, loss = 0.0010202800622209907
iteration 190, loss = 0.0008706923108547926
iteration 191, loss = 0.0008956404635682702
iteration 192, loss = 0.0008662046166136861
iteration 193, loss = 0.0009127728990279138
iteration 194, loss = 0.0007378068403340876
iteration 195, loss = 0.0007753120735287666
iteration 196, loss = 0.0023051670286804438
iteration 197, loss = 0.0007855621515773237
iteration 198, loss = 0.0007066673715598881
iteration 199, loss = 0.0019374476978555322
iteration 200, loss = 0.0007195472717285156
iteration 201, loss = 0.0016201192047446966
iteration 202, loss = 0.0007615999202243984
iteration 203, loss = 0.0008492298074997962
iteration 204, loss = 0.0008869569282978773
iteration 205, loss = 0.0008454066701233387
iteration 206, loss = 0.0006390339112840593
iteration 207, loss = 0.0008417221251875162
iteration 208, loss = 0.0007576597272418439
iteration 209, loss = 0.0008273085113614798
iteration 210, loss = 0.0007446593954227865
iteration 211, loss = 0.000882025808095932
iteration 212, loss = 0.002035098383203149
iteration 213, loss = 0.0010485476814210415
iteration 214, loss = 0.0009240772342309356
iteration 215, loss = 0.0008550487691536546
iteration 216, loss = 0.0007781730382703245
iteration 217, loss = 0.0006930830422788858
iteration 218, loss = 0.0007249165792018175
iteration 219, loss = 0.001047438709065318
iteration 220, loss = 0.0008537992252968252
iteration 221, loss = 0.000956704665441066
iteration 222, loss = 0.0012877919944003224
iteration 223, loss = 0.0008289892575703561
iteration 224, loss = 0.0009601551573723555
iteration 225, loss = 0.0007174324709922075
iteration 226, loss = 0.0007907796534709632
iteration 227, loss = 0.000800758192781359
iteration 228, loss = 0.0008521439740434289
iteration 229, loss = 0.0006737817311659455
iteration 230, loss = 0.000782468356192112
iteration 231, loss = 0.0007993244798853993
iteration 232, loss = 0.0007691874634474516
iteration 233, loss = 0.0008471210021525621
iteration 234, loss = 0.0007798625156283379
iteration 235, loss = 0.0009740779059939086
iteration 236, loss = 0.000850684242323041
iteration 237, loss = 0.0009208899573422968
iteration 238, loss = 0.001140917418524623
iteration 239, loss = 0.001032440923154354
iteration 240, loss = 0.001111924764700234
iteration 241, loss = 0.0009938073344528675
iteration 242, loss = 0.0012031022924929857
iteration 243, loss = 0.0009185250964947045
iteration 244, loss = 0.0013787078205496073
iteration 245, loss = 0.0009901259327307343
iteration 246, loss = 0.0008930074982345104
iteration 247, loss = 0.0008701488841325045
iteration 248, loss = 0.0009411067003384233
iteration 249, loss = 0.0012658750638365746
iteration 250, loss = 0.0006307566654868424
iteration 251, loss = 0.0014783692313358188
iteration 252, loss = 0.0009028278291225433
iteration 253, loss = 0.0009318757802248001
iteration 254, loss = 0.0007159403758123517
iteration 255, loss = 0.001230585970915854
iteration 256, loss = 0.0010443570790812373
iteration 257, loss = 0.000966639956459403
iteration 258, loss = 0.0007586950669065118
iteration 259, loss = 0.0010617967927828431
iteration 260, loss = 0.0010199669050052762
iteration 261, loss = 0.0008595668477937579
iteration 262, loss = 0.0008480348624289036
iteration 263, loss = 0.0012793142814189196
iteration 264, loss = 0.0007947057019919157
iteration 265, loss = 0.001294440939091146
iteration 266, loss = 0.0008196730632334948
iteration 267, loss = 0.0006332231569103897
iteration 268, loss = 0.0008102137944661081
iteration 269, loss = 0.000731628155335784
iteration 270, loss = 0.000883187516592443
iteration 271, loss = 0.0009502702159807086
iteration 272, loss = 0.0007550213485956192
iteration 273, loss = 0.000798790599219501
iteration 274, loss = 0.0009362844284623861
iteration 275, loss = 0.0008618664578534663
iteration 276, loss = 0.0007482686196453869
iteration 277, loss = 0.0011746181407943368
iteration 278, loss = 0.001105124014429748
iteration 279, loss = 0.0007026939420029521
iteration 280, loss = 0.000754284206777811
iteration 281, loss = 0.0006953898700885475
iteration 282, loss = 0.0008231619722209871
iteration 283, loss = 0.0012352169724181294
iteration 284, loss = 0.0011781019857153296
iteration 285, loss = 0.0008369184797629714
iteration 286, loss = 0.0007058975170366466
iteration 287, loss = 0.0008960299892351031
iteration 288, loss = 0.0007986829150468111
iteration 289, loss = 0.0008423096733167768
iteration 290, loss = 0.0008214862318709493
iteration 291, loss = 0.0008004623814485967
iteration 292, loss = 0.0006570920231752098
iteration 293, loss = 0.0008432838949374855
iteration 294, loss = 0.0008765838574618101
iteration 295, loss = 0.0012565028155222535
iteration 296, loss = 0.0011749606346711516
iteration 297, loss = 0.0010824842611327767
iteration 298, loss = 0.0007461787317879498
iteration 299, loss = 0.0009372943313792348
iteration 300, loss = 0.0010562710231170058
iteration 1, loss = 0.0008098911494016647
iteration 2, loss = 0.0007291326182894409
iteration 3, loss = 0.0010148851433768868
iteration 4, loss = 0.0006630966672673821
iteration 5, loss = 0.001599184121005237
iteration 6, loss = 0.0006593077559955418
iteration 7, loss = 0.0010187337175011635
iteration 8, loss = 0.0007125584525056183
iteration 9, loss = 0.0015179485781118274
iteration 10, loss = 0.000835260609164834
iteration 11, loss = 0.0008188073989003897
iteration 12, loss = 0.0008396083721891046
iteration 13, loss = 0.0018519259756430984
iteration 14, loss = 0.001026390353217721
iteration 15, loss = 0.0008200404117815197
iteration 16, loss = 0.0009463168680667877
iteration 17, loss = 0.0011639805743470788
iteration 18, loss = 0.0013884952059015632
iteration 19, loss = 0.0008936263038776815
iteration 20, loss = 0.00069345289375633
iteration 21, loss = 0.0009414070518687367
iteration 22, loss = 0.000865811132825911
iteration 23, loss = 0.0007823877967894077
iteration 24, loss = 0.0008433866314589977
iteration 25, loss = 0.000784682750236243
iteration 26, loss = 0.0007777906721457839
iteration 27, loss = 0.0007785669877193868
iteration 28, loss = 0.0007669894839636981
iteration 29, loss = 0.0008097458630800247
iteration 30, loss = 0.0007933586020953953
iteration 31, loss = 0.001152220880612731
iteration 32, loss = 0.0007634401554241776
iteration 33, loss = 0.0011917384108528495
iteration 34, loss = 0.0008885223069228232
iteration 35, loss = 0.0009460407309234142
iteration 36, loss = 0.0015983398770913482
iteration 37, loss = 0.0014384150272235274
iteration 38, loss = 0.0009005004540085793
iteration 39, loss = 0.0008207005448639393
iteration 40, loss = 0.0008436652715317905
iteration 41, loss = 0.0007430815603584051
iteration 42, loss = 0.0009833198273554444
iteration 43, loss = 0.0008405850967392325
iteration 44, loss = 0.0007784168119542301
iteration 45, loss = 0.0010112992022186518
iteration 46, loss = 0.0010775593109428883
iteration 47, loss = 0.0006525336648337543
iteration 48, loss = 0.0012282916577532887
iteration 49, loss = 0.0014903786359354854
iteration 50, loss = 0.0008369706338271499
iteration 51, loss = 0.0009069818770512938
iteration 52, loss = 0.0007517202757298946
iteration 53, loss = 0.00082601816393435
iteration 54, loss = 0.0013213979545980692
iteration 55, loss = 0.0008755745366215706
iteration 56, loss = 0.0008354486199095845
iteration 57, loss = 0.0008376005571335554
iteration 58, loss = 0.0013597736833617091
iteration 59, loss = 0.0007563928375020623
iteration 60, loss = 0.0011385348625481129
iteration 61, loss = 0.0008674593991599977
iteration 62, loss = 0.0009307949803769588
iteration 63, loss = 0.0011869075242429972
iteration 64, loss = 0.001050902996212244
iteration 65, loss = 0.0008278057794086635
iteration 66, loss = 0.0014024197589606047
iteration 67, loss = 0.0007477913168258965
iteration 68, loss = 0.0009931338718160987
iteration 69, loss = 0.0007151344907470047
iteration 70, loss = 0.0007760861190035939
iteration 71, loss = 0.001011683139950037
iteration 72, loss = 0.0007277296390384436
iteration 73, loss = 0.001171298325061798
iteration 74, loss = 0.000814968952909112
iteration 75, loss = 0.0009580590412952006
iteration 76, loss = 0.001038951799273491
iteration 77, loss = 0.0007874829461798072
iteration 78, loss = 0.0008347171824425459
iteration 79, loss = 0.001005697064101696
iteration 80, loss = 0.0008290274417959154
iteration 81, loss = 0.000819800712633878
iteration 82, loss = 0.001081752241589129
iteration 83, loss = 0.0014933799393475056
iteration 84, loss = 0.0012742934050038457
iteration 85, loss = 0.0008475584327243268
iteration 86, loss = 0.0010299935238435864
iteration 87, loss = 0.0008692733244970441
iteration 88, loss = 0.0009218433406203985
iteration 89, loss = 0.0008048665476962924
iteration 90, loss = 0.000863723224028945
iteration 91, loss = 0.0008304110961034894
iteration 92, loss = 0.001164410263299942
iteration 93, loss = 0.0016653495840728283
iteration 94, loss = 0.001354389125481248
iteration 95, loss = 0.0006799561670050025
iteration 96, loss = 0.0006725827697664499
iteration 97, loss = 0.0008477585506625473
iteration 98, loss = 0.0006819804548285902
iteration 99, loss = 0.0013889222173020244
iteration 100, loss = 0.0006922287866473198
iteration 101, loss = 0.0008095052908174694
iteration 102, loss = 0.0012796082301065326
iteration 103, loss = 0.0009370651678182185
iteration 104, loss = 0.001764386659488082
iteration 105, loss = 0.0006718977820128202
iteration 106, loss = 0.0009033480891957879
iteration 107, loss = 0.0008328447584062815
iteration 108, loss = 0.000629133137408644
iteration 109, loss = 0.001148190931417048
iteration 110, loss = 0.0008264694479294121
iteration 111, loss = 0.0012952268589287996
iteration 112, loss = 0.0008633941179141402
iteration 113, loss = 0.000839597312733531
iteration 114, loss = 0.0008574641542509198
iteration 115, loss = 0.0009178371401503682
iteration 116, loss = 0.0012085353955626488
iteration 117, loss = 0.0015524770133197308
iteration 118, loss = 0.0007112862076610327
iteration 119, loss = 0.0009457741980440915
iteration 120, loss = 0.00145509815774858
iteration 121, loss = 0.0008381247753277421
iteration 122, loss = 0.000930125592276454
iteration 123, loss = 0.0008765796665102243
iteration 124, loss = 0.0009184755617752671
iteration 125, loss = 0.0006649959250353277
iteration 126, loss = 0.0008023424888961017
iteration 127, loss = 0.0009144855430349708
iteration 128, loss = 0.0009570370311848819
iteration 129, loss = 0.0007820046739652753
iteration 130, loss = 0.001092136255465448
iteration 131, loss = 0.0007470552227459848
iteration 132, loss = 0.0007940070936456323
iteration 133, loss = 0.001509853987954557
iteration 134, loss = 0.0007917993934825063
iteration 135, loss = 0.000727166305296123
iteration 136, loss = 0.0015613703290000558
iteration 137, loss = 0.0009572795243002474
iteration 138, loss = 0.0010359800653532147
iteration 139, loss = 0.0007959752110764384
iteration 140, loss = 0.0009119766182266176
iteration 141, loss = 0.0007338247378356755
iteration 142, loss = 0.0011012610048055649
iteration 143, loss = 0.0008325574453920126
iteration 144, loss = 0.0007859015022404492
iteration 145, loss = 0.0008618324645794928
iteration 146, loss = 0.0007711093639954925
iteration 147, loss = 0.0009579870966263115
iteration 148, loss = 0.0008323793299496174
iteration 149, loss = 0.00078094273339957
iteration 150, loss = 0.0008645967463962734
iteration 151, loss = 0.0010185886640101671
iteration 152, loss = 0.0008384524844586849
iteration 153, loss = 0.0010517111513763666
iteration 154, loss = 0.0007642852724529803
iteration 155, loss = 0.001099088229238987
iteration 156, loss = 0.0008422886021435261
iteration 157, loss = 0.0009498711442574859
iteration 158, loss = 0.0009123517666012049
iteration 159, loss = 0.0008952741627581418
iteration 160, loss = 0.0009606752428226173
iteration 161, loss = 0.0009393568034283817
iteration 162, loss = 0.0008693074341863394
iteration 163, loss = 0.0010981244267895818
iteration 164, loss = 0.0015369056491181254
iteration 165, loss = 0.001376976491883397
iteration 166, loss = 0.0010029594413936138
iteration 167, loss = 0.0017257648287340999
iteration 168, loss = 0.0009206724353134632
iteration 169, loss = 0.0008262363844551146
iteration 170, loss = 0.0009576448937878013
iteration 171, loss = 0.001290168147534132
iteration 172, loss = 0.0009325517457909882
iteration 173, loss = 0.0008381489315070212
iteration 174, loss = 0.0008061978733167052
iteration 175, loss = 0.0006840156856924295
iteration 176, loss = 0.0008584451279602945
iteration 177, loss = 0.0008509154431521893
iteration 178, loss = 0.0009249575086869299
iteration 179, loss = 0.0006589594413526356
iteration 180, loss = 0.0008016822393983603
iteration 181, loss = 0.001118606305681169
iteration 182, loss = 0.0009740825626067817
iteration 183, loss = 0.0009386142482981086
iteration 184, loss = 0.0008298392058350146
iteration 185, loss = 0.000776509812567383
iteration 186, loss = 0.0007575007039122283
iteration 187, loss = 0.0014017635257914662
iteration 188, loss = 0.001041614799760282
iteration 189, loss = 0.0006694672047160566
iteration 190, loss = 0.0006147829117253423
iteration 191, loss = 0.00160669453907758
iteration 192, loss = 0.0008469422464258969
iteration 193, loss = 0.0008092198986560106
iteration 194, loss = 0.0008180218865163624
iteration 195, loss = 0.0010890897829085588
iteration 196, loss = 0.0006647456903010607
iteration 197, loss = 0.0008343704976141453
iteration 198, loss = 0.0006917024729773402
iteration 199, loss = 0.0008948046015575528
iteration 200, loss = 0.00078258739085868
iteration 201, loss = 0.0006528838421218097
iteration 202, loss = 0.001434890553355217
iteration 203, loss = 0.0008412704337388277
iteration 204, loss = 0.0012511486420407891
iteration 205, loss = 0.0007300540455617011
iteration 206, loss = 0.0011306724045425653
iteration 207, loss = 0.0011615611147135496
iteration 208, loss = 0.0009053418762050569
iteration 209, loss = 0.0006613883888348937
iteration 210, loss = 0.0007624182035215199
iteration 211, loss = 0.0007753928657621145
iteration 212, loss = 0.0008373415912501514
iteration 213, loss = 0.0008243616321124136
iteration 214, loss = 0.000817806925624609
iteration 215, loss = 0.0007052560104057193
iteration 216, loss = 0.0010719132842496037
iteration 217, loss = 0.0007571777678094804
iteration 218, loss = 0.0011493737110868096
iteration 219, loss = 0.0007744616596028209
iteration 220, loss = 0.0010451015550643206
iteration 221, loss = 0.0009158028988167644
iteration 222, loss = 0.001196719822473824
iteration 223, loss = 0.0010532611049711704
iteration 224, loss = 0.0007224027649499476
iteration 225, loss = 0.0009380106930620968
iteration 226, loss = 0.0009957526344805956
iteration 227, loss = 0.0007683519506826997
iteration 228, loss = 0.0007736417464911938
iteration 229, loss = 0.0011762254871428013
iteration 230, loss = 0.0006780281546525657
iteration 231, loss = 0.001107949297875166
iteration 232, loss = 0.0010019512847065926
iteration 233, loss = 0.0008335270686075091
iteration 234, loss = 0.000991316046565771
iteration 235, loss = 0.0009664251701906323
iteration 236, loss = 0.0007787444046698511
iteration 237, loss = 0.0008655745768919587
iteration 238, loss = 0.0009201333159580827
iteration 239, loss = 0.0007381399045698345
iteration 240, loss = 0.0010684363078325987
iteration 241, loss = 0.0009975795401260257
iteration 242, loss = 0.0007118235807865858
iteration 243, loss = 0.0008708292152732611
iteration 244, loss = 0.0008732901187613606
iteration 245, loss = 0.0009807859314605594
iteration 246, loss = 0.0008528844336979091
iteration 247, loss = 0.0008344701491296291
iteration 248, loss = 0.0014148937771096826
iteration 249, loss = 0.0007542968960478902
iteration 250, loss = 0.0017579835839569569
iteration 251, loss = 0.0010812015971168876
iteration 252, loss = 0.0008344544330611825
iteration 253, loss = 0.0008504338329657912
iteration 254, loss = 0.0006997838499955833
iteration 255, loss = 0.0008191021624952555
iteration 256, loss = 0.0014423298416659236
iteration 257, loss = 0.0008081406122073531
iteration 258, loss = 0.0008387446869164705
iteration 259, loss = 0.0016321901930496097
iteration 260, loss = 0.0007943413220345974
iteration 261, loss = 0.0008906630100682378
iteration 262, loss = 0.0016069679986685514
iteration 263, loss = 0.000860063184518367
iteration 264, loss = 0.0010295126121491194
iteration 265, loss = 0.0010814010165631771
iteration 266, loss = 0.0009036326082423329
iteration 267, loss = 0.0005990703939460218
iteration 268, loss = 0.0010478692129254341
iteration 269, loss = 0.0015170213300734758
iteration 270, loss = 0.0007152851903811097
iteration 271, loss = 0.0009332537301816046
iteration 272, loss = 0.0010967676062136889
iteration 273, loss = 0.000856974977068603
iteration 274, loss = 0.0008683332125656307
iteration 275, loss = 0.000859573483467102
iteration 276, loss = 0.0010013118153437972
iteration 277, loss = 0.0016394348349422216
iteration 278, loss = 0.0011990931816399097
iteration 279, loss = 0.000824696384370327
iteration 280, loss = 0.0007776161655783653
iteration 281, loss = 0.0007101174560375512
iteration 282, loss = 0.0009039712604135275
iteration 283, loss = 0.0008291099220514297
iteration 284, loss = 0.0008479180396534503
iteration 285, loss = 0.0008086556917987764
iteration 286, loss = 0.0009453367674723268
iteration 287, loss = 0.0006659787031821907
iteration 288, loss = 0.002425850136205554
iteration 289, loss = 0.0007368081714957952
iteration 290, loss = 0.001134218298830092
iteration 291, loss = 0.001212707138620317
iteration 292, loss = 0.0007613415364176035
iteration 293, loss = 0.0009718456421978772
iteration 294, loss = 0.001061791437678039
iteration 295, loss = 0.0010000645415857434
iteration 296, loss = 0.00101053761318326
iteration 297, loss = 0.000987025909125805
iteration 298, loss = 0.0007634454523213208
iteration 299, loss = 0.00073985819472
iteration 300, loss = 0.000797573768068105
iteration 1, loss = 0.0006960492464713752
iteration 2, loss = 0.000901344115845859
iteration 3, loss = 0.0008129687048494816
iteration 4, loss = 0.0008432417525909841
iteration 5, loss = 0.0011000364320352674
iteration 6, loss = 0.000651672831736505
iteration 7, loss = 0.001027423539198935
iteration 8, loss = 0.0008119889535009861
iteration 9, loss = 0.0007832811097614467
iteration 10, loss = 0.0014003479154780507
iteration 11, loss = 0.0007772970711812377
iteration 12, loss = 0.0007013360736891627
iteration 13, loss = 0.000982789322733879
iteration 14, loss = 0.0008106731693260372
iteration 15, loss = 0.0008628791547380388
iteration 16, loss = 0.0008820862276479602
iteration 17, loss = 0.0007631344487890601
iteration 18, loss = 0.0008772398578003049
iteration 19, loss = 0.0007423590868711472
iteration 20, loss = 0.0009047735948115587
iteration 21, loss = 0.0006048856303095818
iteration 22, loss = 0.000993196154013276
iteration 23, loss = 0.0014412408927455544
iteration 24, loss = 0.0008344504749402404
iteration 25, loss = 0.001016830326989293
iteration 26, loss = 0.0008792470907792449
iteration 27, loss = 0.000813630351331085
iteration 28, loss = 0.0019294992089271545
iteration 29, loss = 0.0007605614955537021
iteration 30, loss = 0.001160135492682457
iteration 31, loss = 0.0008848609868437052
iteration 32, loss = 0.0008035592618398368
iteration 33, loss = 0.0007568303262814879
iteration 34, loss = 0.0007823489722795784
iteration 35, loss = 0.0007670741761103272
iteration 36, loss = 0.0007057462353259325
iteration 37, loss = 0.0012412574142217636
iteration 38, loss = 0.0009282089304178953
iteration 39, loss = 0.0006839673151262105
iteration 40, loss = 0.0016264517325907946
iteration 41, loss = 0.0009229183197021484
iteration 42, loss = 0.0008640541927888989
iteration 43, loss = 0.0006178209441713989
iteration 44, loss = 0.000717654824256897
iteration 45, loss = 0.0011727962410077453
iteration 46, loss = 0.0009118154994212091
iteration 47, loss = 0.001220812788233161
iteration 48, loss = 0.0008879988454282284
iteration 49, loss = 0.0007891698041930795
iteration 50, loss = 0.0010962500236928463
iteration 51, loss = 0.0007358653238043189
iteration 52, loss = 0.0008068145252764225
iteration 53, loss = 0.0007921938085928559
iteration 54, loss = 0.0010049749398604035
iteration 55, loss = 0.0009121694602072239
iteration 56, loss = 0.0008786490652710199
iteration 57, loss = 0.0006748284213244915
iteration 58, loss = 0.001030831946991384
iteration 59, loss = 0.0008221854805015028
iteration 60, loss = 0.000790113233961165
iteration 61, loss = 0.0007985317497514188
iteration 62, loss = 0.0011058681411668658
iteration 63, loss = 0.0007364304037764668
iteration 64, loss = 0.0008324261289089918
iteration 65, loss = 0.0006684535765089095
iteration 66, loss = 0.0007012439891695976
iteration 67, loss = 0.0009139431640505791
iteration 68, loss = 0.001577150309458375
iteration 69, loss = 0.0010499197524040937
iteration 70, loss = 0.0013869627146050334
iteration 71, loss = 0.0014382422668859363
iteration 72, loss = 0.0008504393044859171
iteration 73, loss = 0.0008928280440159142
iteration 74, loss = 0.0014556099195033312
iteration 75, loss = 0.0006836188840679824
iteration 76, loss = 0.0010501522338017821
iteration 77, loss = 0.0014770843554288149
iteration 78, loss = 0.0006121982005424798
iteration 79, loss = 0.0008647033246234059
iteration 80, loss = 0.0008013726910576224
iteration 81, loss = 0.000744418241083622
iteration 82, loss = 0.0010150601156055927
iteration 83, loss = 0.0009988534729927778
iteration 84, loss = 0.0014519511023536325
iteration 85, loss = 0.0011491100303828716
iteration 86, loss = 0.0008614431717433035
iteration 87, loss = 0.0007579546654596925
iteration 88, loss = 0.0007192608900368214
iteration 89, loss = 0.0007137738866731524
iteration 90, loss = 0.00083976547466591
iteration 91, loss = 0.000918439996894449
iteration 92, loss = 0.0008233081316575408
iteration 93, loss = 0.0008908878080546856
iteration 94, loss = 0.001045094570145011
iteration 95, loss = 0.0011826609261333942
iteration 96, loss = 0.0009737181244418025
iteration 97, loss = 0.0007971564773470163
iteration 98, loss = 0.0007930533029139042
iteration 99, loss = 0.0006141132907941937
iteration 100, loss = 0.0008533367654308677
iteration 101, loss = 0.0007039768970571458
iteration 102, loss = 0.000976458250079304
iteration 103, loss = 0.0012231641449034214
iteration 104, loss = 0.0008610795484855771
iteration 105, loss = 0.0016046573873609304
iteration 106, loss = 0.0008247136720456183
iteration 107, loss = 0.0012896871194243431
iteration 108, loss = 0.0011013024486601353
iteration 109, loss = 0.0011158273555338383
iteration 110, loss = 0.0014898149529471993
iteration 111, loss = 0.0007604730781167746
iteration 112, loss = 0.0009027176420204341
iteration 113, loss = 0.0008893525809980929
iteration 114, loss = 0.0008072729106061161
iteration 115, loss = 0.0008300765184685588
iteration 116, loss = 0.0009430942591279745
iteration 117, loss = 0.0009797537932172418
iteration 118, loss = 0.0008576273685321212
iteration 119, loss = 0.00115704454947263
iteration 120, loss = 0.0013296405086293817
iteration 121, loss = 0.0006502839969471097
iteration 122, loss = 0.0007539405487477779
iteration 123, loss = 0.0016547690611332655
iteration 124, loss = 0.0008165334584191442
iteration 125, loss = 0.0009908920619636774
iteration 126, loss = 0.0007689386839047074
iteration 127, loss = 0.0007979417569003999
iteration 128, loss = 0.001183356624096632
iteration 129, loss = 0.0009098054724745452
iteration 130, loss = 0.000802690745331347
iteration 131, loss = 0.0008028856245800853
iteration 132, loss = 0.001171796815469861
iteration 133, loss = 0.0010561187518760562
iteration 134, loss = 0.0011618806747719646
iteration 135, loss = 0.0009716983186081052
iteration 136, loss = 0.0008977415272966027
iteration 137, loss = 0.0010675404919311404
iteration 138, loss = 0.0007779387524351478
iteration 139, loss = 0.0008623431203886867
iteration 140, loss = 0.0007728812051936984
iteration 141, loss = 0.0007954750908538699
iteration 142, loss = 0.0009284949628636241
iteration 143, loss = 0.0009811772033572197
iteration 144, loss = 0.0006672177114523947
iteration 145, loss = 0.0014618929708376527
iteration 146, loss = 0.0008187406929209828
iteration 147, loss = 0.001531193614937365
iteration 148, loss = 0.0012191168498247862
iteration 149, loss = 0.0009389488841407001
iteration 150, loss = 0.0008525308803655207
iteration 151, loss = 0.0011855155462399125
iteration 152, loss = 0.0007924304809421301
iteration 153, loss = 0.0007732727681286633
iteration 154, loss = 0.0007562275277450681
iteration 155, loss = 0.0007616186630912125
iteration 156, loss = 0.001088635646738112
iteration 157, loss = 0.0008308638352900743
iteration 158, loss = 0.0007678100955672562
iteration 159, loss = 0.001184303779155016
iteration 160, loss = 0.0008490019245073199
iteration 161, loss = 0.000899203703738749
iteration 162, loss = 0.0008469126769341528
iteration 163, loss = 0.0008854074403643608
iteration 164, loss = 0.0010314923711121082
iteration 165, loss = 0.0012118869926780462
iteration 166, loss = 0.000699346826877445
iteration 167, loss = 0.0010254939552396536
iteration 168, loss = 0.0007238665712065995
iteration 169, loss = 0.0007368221413344145
iteration 170, loss = 0.0007477726903744042
iteration 171, loss = 0.0015452185180038214
iteration 172, loss = 0.0008486431324854493
iteration 173, loss = 0.0008360047359019518
iteration 174, loss = 0.0018592592095956206
iteration 175, loss = 0.000812876911368221
iteration 176, loss = 0.0006936816498637199
iteration 177, loss = 0.0007115320768207312
iteration 178, loss = 0.0007135120104067028
iteration 179, loss = 0.0007758172578178346
iteration 180, loss = 0.0010998435318470001
iteration 181, loss = 0.0008781414944678545
iteration 182, loss = 0.000978166121058166
iteration 183, loss = 0.0008605148177593946
iteration 184, loss = 0.0009438458364456892
iteration 185, loss = 0.0008369839051738381
iteration 186, loss = 0.0019485482480376959
iteration 187, loss = 0.0014840037329122424
iteration 188, loss = 0.0010829382808879018
iteration 189, loss = 0.001528523862361908
iteration 190, loss = 0.0006695630145259202
iteration 191, loss = 0.0008770748972892761
iteration 192, loss = 0.001233808696269989
iteration 193, loss = 0.0005803045351058245
iteration 194, loss = 0.0016350361984223127
iteration 195, loss = 0.0010515531757846475
iteration 196, loss = 0.000933422998059541
iteration 197, loss = 0.0008334998274222016
iteration 198, loss = 0.0006255766493268311
iteration 199, loss = 0.0023128995671868324
iteration 200, loss = 0.0008440501987934113
iteration 201, loss = 0.0007680697017349303
iteration 202, loss = 0.000782874645665288
iteration 203, loss = 0.0006667550187557936
iteration 204, loss = 0.0009889580542221665
iteration 205, loss = 0.0008747302345000207
iteration 206, loss = 0.0007036564056761563
iteration 207, loss = 0.0008132055518217385
iteration 208, loss = 0.0007247296744026244
iteration 209, loss = 0.001518897246569395
iteration 210, loss = 0.0008618331048637629
iteration 211, loss = 0.0010033553699031472
iteration 212, loss = 0.0013620418030768633
iteration 213, loss = 0.0008055328507907689
iteration 214, loss = 0.0008001616224646568
iteration 215, loss = 0.0008100064005702734
iteration 216, loss = 0.0008599113207310438
iteration 217, loss = 0.0009096268331632018
iteration 218, loss = 0.0007338006398640573
iteration 219, loss = 0.000764171767514199
iteration 220, loss = 0.0009084049961529672
iteration 221, loss = 0.0007314105168916285
iteration 222, loss = 0.0008831466548144817
iteration 223, loss = 0.0006943740881979465
iteration 224, loss = 0.0007435635197907686
iteration 225, loss = 0.0008168882341124117
iteration 226, loss = 0.0007973987958393991
iteration 227, loss = 0.0008813529275357723
iteration 228, loss = 0.0018725200789049268
iteration 229, loss = 0.0008475497597828507
iteration 230, loss = 0.0011760820634663105
iteration 231, loss = 0.0009351515327580273
iteration 232, loss = 0.000964897742960602
iteration 233, loss = 0.0011373392771929502
iteration 234, loss = 0.0006839616689831018
iteration 235, loss = 0.0008649770170450211
iteration 236, loss = 0.000698660674970597
iteration 237, loss = 0.0007378553855232894
iteration 238, loss = 0.000719692965503782
iteration 239, loss = 0.0010663594584912062
iteration 240, loss = 0.0008583064773119986
iteration 241, loss = 0.0015712508466094732
iteration 242, loss = 0.0013102671364322305
iteration 243, loss = 0.0006652831798419356
iteration 244, loss = 0.0009742886177264154
iteration 245, loss = 0.0009444671450182796
iteration 246, loss = 0.000744491524528712
iteration 247, loss = 0.0008086007437668741
iteration 248, loss = 0.0011046293657273054
iteration 249, loss = 0.0013634851202368736
iteration 250, loss = 0.001630110782571137
iteration 251, loss = 0.0008780568023212254
iteration 252, loss = 0.0007565085543319583
iteration 253, loss = 0.0007574143819510937
iteration 254, loss = 0.0008914995705708861
iteration 255, loss = 0.0016112332232296467
iteration 256, loss = 0.0008265291107818484
iteration 257, loss = 0.0013009256217628717
iteration 258, loss = 0.0006188942934386432
iteration 259, loss = 0.0007404129719361663
iteration 260, loss = 0.0009148010285571218
iteration 261, loss = 0.0016408292576670647
iteration 262, loss = 0.001650063437409699
iteration 263, loss = 0.0009128079982474446
iteration 264, loss = 0.0006897196872159839
iteration 265, loss = 0.0011042895494028926
iteration 266, loss = 0.0008057028753682971
iteration 267, loss = 0.000914353528060019
iteration 268, loss = 0.0008706588996574283
iteration 269, loss = 0.0011262737680226564
iteration 270, loss = 0.0007419565226882696
iteration 271, loss = 0.0013615131611004472
iteration 272, loss = 0.0009231219301000237
iteration 273, loss = 0.0010321749141439795
iteration 274, loss = 0.000889239483512938
iteration 275, loss = 0.0010681241983547807
iteration 276, loss = 0.0008612406672909856
iteration 277, loss = 0.000645114341750741
iteration 278, loss = 0.0008278990280814469
iteration 279, loss = 0.0011588191846385598
iteration 280, loss = 0.0010385499335825443
iteration 281, loss = 0.0011685079662129283
iteration 282, loss = 0.0016181350219994783
iteration 283, loss = 0.0017916138749569654
iteration 284, loss = 0.0007867639069445431
iteration 285, loss = 0.0006659899954684079
iteration 286, loss = 0.0008953362703323364
iteration 287, loss = 0.0006421508733183146
iteration 288, loss = 0.0008690575486980379
iteration 289, loss = 0.0007923014927655458
iteration 290, loss = 0.00094453408382833
iteration 291, loss = 0.0007892192807048559
iteration 292, loss = 0.0008108186302706599
iteration 293, loss = 0.0007642894051969051
iteration 294, loss = 0.000879950646776706
iteration 295, loss = 0.0007192908087745309
iteration 296, loss = 0.0005878055235370994
iteration 297, loss = 0.0015649066772311926
iteration 298, loss = 0.001020172843709588
iteration 299, loss = 0.0009108426165767014
iteration 300, loss = 0.0008387431153096259
iteration 1, loss = 0.0016299347626045346
iteration 2, loss = 0.0015361564001068473
iteration 3, loss = 0.001027003861963749
iteration 4, loss = 0.00080141267972067
iteration 5, loss = 0.0010873919818550348
iteration 6, loss = 0.0008534507942385972
iteration 7, loss = 0.0011135699460282922
iteration 8, loss = 0.0008173920214176178
iteration 9, loss = 0.0010304802563041449
iteration 10, loss = 0.000942066079005599
iteration 11, loss = 0.000839298008941114
iteration 12, loss = 0.0007370819803327322
iteration 13, loss = 0.0006657904596067965
iteration 14, loss = 0.0008578679990023375
iteration 15, loss = 0.0008801663061603904
iteration 16, loss = 0.0007151850732043386
iteration 17, loss = 0.0008553085499443114
iteration 18, loss = 0.0008687766385264695
iteration 19, loss = 0.0008415949996560812
iteration 20, loss = 0.0011491173645481467
iteration 21, loss = 0.0007473993464373052
iteration 22, loss = 0.0007743362803012133
iteration 23, loss = 0.0008537141839042306
iteration 24, loss = 0.0009921400342136621
iteration 25, loss = 0.0007669713231734931
iteration 26, loss = 0.0007565205451101065
iteration 27, loss = 0.0007925154059194028
iteration 28, loss = 0.001141633721999824
iteration 29, loss = 0.000997529597952962
iteration 30, loss = 0.000730960164219141
iteration 31, loss = 0.0008074418874457479
iteration 32, loss = 0.0007804443594068289
iteration 33, loss = 0.0009932777611538768
iteration 34, loss = 0.0010306762997061014
iteration 35, loss = 0.0007639373070560396
iteration 36, loss = 0.0009751161560416222
iteration 37, loss = 0.0010783650213852525
iteration 38, loss = 0.0007686318713240325
iteration 39, loss = 0.0007308620843105018
iteration 40, loss = 0.0008565459284000099
iteration 41, loss = 0.0009302826365455985
iteration 42, loss = 0.0017742151394486427
iteration 43, loss = 0.0008026426075957716
iteration 44, loss = 0.000995944021269679
iteration 45, loss = 0.0009112944826483727
iteration 46, loss = 0.001019750372506678
iteration 47, loss = 0.0007990369340404868
iteration 48, loss = 0.0008054068894125521
iteration 49, loss = 0.0007875110022723675
iteration 50, loss = 0.0010743237799033523
iteration 51, loss = 0.0008560707792639732
iteration 52, loss = 0.0010495667811483145
iteration 53, loss = 0.000805083429440856
iteration 54, loss = 0.0013618305092677474
iteration 55, loss = 0.0009647797560319304
iteration 56, loss = 0.0013089064741507173
iteration 57, loss = 0.0008492119959555566
iteration 58, loss = 0.0014989787014201283
iteration 59, loss = 0.0007382945041172206
iteration 60, loss = 0.0011502300621941686
iteration 61, loss = 0.0007457685424014926
iteration 62, loss = 0.0007694989326409996
iteration 63, loss = 0.0007378088776022196
iteration 64, loss = 0.0006670589791610837
iteration 65, loss = 0.0008105807355605066
iteration 66, loss = 0.0012703025713562965
iteration 67, loss = 0.0008533186628483236
iteration 68, loss = 0.0007419575704261661
iteration 69, loss = 0.0008510856423527002
iteration 70, loss = 0.0009359483374282718
iteration 71, loss = 0.0008365748217329383
iteration 72, loss = 0.0014993390068411827
iteration 73, loss = 0.0011109006591141224
iteration 74, loss = 0.0009210039861500263
iteration 75, loss = 0.0006900008884258568
iteration 76, loss = 0.0010578620713204145
iteration 77, loss = 0.0009454366518184543
iteration 78, loss = 0.0008667513029649854
iteration 79, loss = 0.0007300010765902698
iteration 80, loss = 0.0008531644125469029
iteration 81, loss = 0.0008021745597943664
iteration 82, loss = 0.0007962633389979601
iteration 83, loss = 0.0007674064254388213
iteration 84, loss = 0.0006980643956921995
iteration 85, loss = 0.001036309520713985
iteration 86, loss = 0.0007499924977310002
iteration 87, loss = 0.0007664996664971113
iteration 88, loss = 0.0006516170687973499
iteration 89, loss = 0.0007978290086612105
iteration 90, loss = 0.0007928208215162158
iteration 91, loss = 0.0007718170527368784
iteration 92, loss = 0.00114214897621423
iteration 93, loss = 0.0013606001157313585
iteration 94, loss = 0.0010816998546943069
iteration 95, loss = 0.0007064931560307741
iteration 96, loss = 0.0007705194293521345
iteration 97, loss = 0.0008031739853322506
iteration 98, loss = 0.0008442674879916012
iteration 99, loss = 0.0008515334920957685
iteration 100, loss = 0.001074720872566104
iteration 101, loss = 0.0007854472496546805
iteration 102, loss = 0.0009506585192866623
iteration 103, loss = 0.0008632916724309325
iteration 104, loss = 0.0014084663707762957
iteration 105, loss = 0.0007272625807672739
iteration 106, loss = 0.0007939646020531654
iteration 107, loss = 0.0008738832548260689
iteration 108, loss = 0.0008049919269979
iteration 109, loss = 0.000819656765088439
iteration 110, loss = 0.0011564017040655017
iteration 111, loss = 0.0015284817200154066
iteration 112, loss = 0.0008254162385128438
iteration 113, loss = 0.0008310488192364573
iteration 114, loss = 0.0008078297832980752
iteration 115, loss = 0.0012052543461322784
iteration 116, loss = 0.0010799396550282836
iteration 117, loss = 0.0007661592098884284
iteration 118, loss = 0.0010241428390145302
iteration 119, loss = 0.0007851255359128118
iteration 120, loss = 0.0008063978748396039
iteration 121, loss = 0.0007936153560876846
iteration 122, loss = 0.000870775431394577
iteration 123, loss = 0.001667507691308856
iteration 124, loss = 0.0007419182802550495
iteration 125, loss = 0.0007293617236427963
iteration 126, loss = 0.0007656418019905686
iteration 127, loss = 0.0016750369686633348
iteration 128, loss = 0.0009869220666587353
iteration 129, loss = 0.0012976998696103692
iteration 130, loss = 0.00102376495487988
iteration 131, loss = 0.0009546864312142134
iteration 132, loss = 0.0007341256132349372
iteration 133, loss = 0.000794664490967989
iteration 134, loss = 0.0008399472571909428
iteration 135, loss = 0.0008131993818096817
iteration 136, loss = 0.0008903869893401861
iteration 137, loss = 0.0008141500875353813
iteration 138, loss = 0.0012513805413618684
iteration 139, loss = 0.00110655149910599
iteration 140, loss = 0.000924602325540036
iteration 141, loss = 0.0007904432713985443
iteration 142, loss = 0.0009338069939985871
iteration 143, loss = 0.0008681107428856194
iteration 144, loss = 0.0008723132777959108
iteration 145, loss = 0.000847663963213563
iteration 146, loss = 0.0007552159950137138
iteration 147, loss = 0.000738444272428751
iteration 148, loss = 0.0009976403089240193
iteration 149, loss = 0.0016488784458488226
iteration 150, loss = 0.0016591434832662344
iteration 151, loss = 0.0008875590283423662
iteration 152, loss = 0.000732705753762275
iteration 153, loss = 0.0008637182181701064
iteration 154, loss = 0.00104209640994668
iteration 155, loss = 0.0007622367120347917
iteration 156, loss = 0.001245309947989881
iteration 157, loss = 0.0015593370189890265
iteration 158, loss = 0.000710860185790807
iteration 159, loss = 0.0011354809394106269
iteration 160, loss = 0.0009485248010605574
iteration 161, loss = 0.0013897561002522707
iteration 162, loss = 0.0010201869299635291
iteration 163, loss = 0.0007407693192362785
iteration 164, loss = 0.0009468703647144139
iteration 165, loss = 0.0008659925078973174
iteration 166, loss = 0.000947945867665112
iteration 167, loss = 0.0008797245100140572
iteration 168, loss = 0.0010346972849220037
iteration 169, loss = 0.0011308521497994661
iteration 170, loss = 0.001026939949952066
iteration 171, loss = 0.0009443577146157622
iteration 172, loss = 0.0008320634951815009
iteration 173, loss = 0.0008195274276658893
iteration 174, loss = 0.0005628224462270737
iteration 175, loss = 0.0007602221448905766
iteration 176, loss = 0.0009397404501214623
iteration 177, loss = 0.0006978875026106834
iteration 178, loss = 0.0006929821101948619
iteration 179, loss = 0.0009190826676785946
iteration 180, loss = 0.0014572080690413713
iteration 181, loss = 0.0010903436923399568
iteration 182, loss = 0.0008881622925400734
iteration 183, loss = 0.0013891862472519279
iteration 184, loss = 0.0007133788894861937
iteration 185, loss = 0.0007243830477818847
iteration 186, loss = 0.001095852698199451
iteration 187, loss = 0.0015716853085905313
iteration 188, loss = 0.0008906536968424916
iteration 189, loss = 0.0007903064833953977
iteration 190, loss = 0.0007050929125398397
iteration 191, loss = 0.0008253928972408175
iteration 192, loss = 0.0009663505479693413
iteration 193, loss = 0.0008374048629775643
iteration 194, loss = 0.0007286812760867178
iteration 195, loss = 0.0008344731759279966
iteration 196, loss = 0.0008911306504160166
iteration 197, loss = 0.0008274393039755523
iteration 198, loss = 0.0010330963414162397
iteration 199, loss = 0.000719043891876936
iteration 200, loss = 0.002301587490364909
iteration 201, loss = 0.0005916015361435711
iteration 202, loss = 0.0007504948880523443
iteration 203, loss = 0.0010479445336386561
iteration 204, loss = 0.0008004650007933378
iteration 205, loss = 0.0008206130587495863
iteration 206, loss = 0.000848839757964015
iteration 207, loss = 0.0006832266808487475
iteration 208, loss = 0.000738343340344727
iteration 209, loss = 0.0006830744096077979
iteration 210, loss = 0.000747042999137193
iteration 211, loss = 0.0013174178311601281
iteration 212, loss = 0.0007564196130260825
iteration 213, loss = 0.0008608703501522541
iteration 214, loss = 0.0007320753647945821
iteration 215, loss = 0.0009488379582762718
iteration 216, loss = 0.000972380512394011
iteration 217, loss = 0.0014779125340282917
iteration 218, loss = 0.0007496176403947175
iteration 219, loss = 0.0010837337467819452
iteration 220, loss = 0.0008085392182692885
iteration 221, loss = 0.0014127613976597786
iteration 222, loss = 0.0007882644422352314
iteration 223, loss = 0.0008011630270630121
iteration 224, loss = 0.0010963249951601028
iteration 225, loss = 0.00079302117228508
iteration 226, loss = 0.0009285692940466106
iteration 227, loss = 0.0008989024208858609
iteration 228, loss = 0.0010377921862527728
iteration 229, loss = 0.0016167876310646534
iteration 230, loss = 0.0009349681786261499
iteration 231, loss = 0.0006764296558685601
iteration 232, loss = 0.0009706682758405805
iteration 233, loss = 0.000997251132503152
iteration 234, loss = 0.0007745065377093852
iteration 235, loss = 0.0009562878403812647
iteration 236, loss = 0.0012851539067924023
iteration 237, loss = 0.001654338208027184
iteration 238, loss = 0.0013020773185417056
iteration 239, loss = 0.0009236340993084013
iteration 240, loss = 0.0006395731470547616
iteration 241, loss = 0.0009191497229039669
iteration 242, loss = 0.0007299778517335653
iteration 243, loss = 0.0007510764990001917
iteration 244, loss = 0.0008465467835776508
iteration 245, loss = 0.0008945793961174786
iteration 246, loss = 0.0008121048449538648
iteration 247, loss = 0.0012312077451497316
iteration 248, loss = 0.0016197903314605355
iteration 249, loss = 0.001741431769914925
iteration 250, loss = 0.0008001727983355522
iteration 251, loss = 0.0010469505796208978
iteration 252, loss = 0.0008792128646746278
iteration 253, loss = 0.0009048341307789087
iteration 254, loss = 0.0015636279713362455
iteration 255, loss = 0.0011660356540232897
iteration 256, loss = 0.0010089428396895528
iteration 257, loss = 0.0006941728643141687
iteration 258, loss = 0.0008340089116245508
iteration 259, loss = 0.0009866701439023018
iteration 260, loss = 0.0010041468776762486
iteration 261, loss = 0.0010386937065050006
iteration 262, loss = 0.0012754258932545781
iteration 263, loss = 0.0009530743700452149
iteration 264, loss = 0.0008008801960386336
iteration 265, loss = 0.0013396019348874688
iteration 266, loss = 0.0007507617119699717
iteration 267, loss = 0.0008898643427528441
iteration 268, loss = 0.0009201032225973904
iteration 269, loss = 0.0010620142566040158
iteration 270, loss = 0.0007145137642510235
iteration 271, loss = 0.0006768804742023349
iteration 272, loss = 0.000890409923158586
iteration 273, loss = 0.0008230379316955805
iteration 274, loss = 0.0007030678098089993
iteration 275, loss = 0.0008315456216223538
iteration 276, loss = 0.0008261753828264773
iteration 277, loss = 0.0011054547503590584
iteration 278, loss = 0.0010283379815518856
iteration 279, loss = 0.0007056468166410923
iteration 280, loss = 0.0007779818843118846
iteration 281, loss = 0.0015322503168135881
iteration 282, loss = 0.000654300267342478
iteration 283, loss = 0.0011959532275795937
iteration 284, loss = 0.0008984380401670933
iteration 285, loss = 0.0010344706242904067
iteration 286, loss = 0.0012803564313799143
iteration 287, loss = 0.0007773805991746485
iteration 288, loss = 0.0009543794440105557
iteration 289, loss = 0.0012822241988033056
iteration 290, loss = 0.0008059805841185153
iteration 291, loss = 0.000759495422244072
iteration 292, loss = 0.0011580800637602806
iteration 293, loss = 0.001438161008991301
iteration 294, loss = 0.0008788709528744221
iteration 295, loss = 0.0008595280814915895
iteration 296, loss = 0.0009126575314439833
iteration 297, loss = 0.0014428398571908474
iteration 298, loss = 0.0011386142577975988
iteration 299, loss = 0.0007492593140341341
iteration 300, loss = 0.0021610925905406475
iteration 1, loss = 0.0008529942133463919
iteration 2, loss = 0.0007024163496680558
iteration 3, loss = 0.0013408783124759793
iteration 4, loss = 0.001136749517172575
iteration 5, loss = 0.0009262915118597448
iteration 6, loss = 0.0007465913076885045
iteration 7, loss = 0.0009172986028715968
iteration 8, loss = 0.0007363546756096184
iteration 9, loss = 0.0007807117071934044
iteration 10, loss = 0.0006947083165869117
iteration 11, loss = 0.001052746782079339
iteration 12, loss = 0.0008697478333488107
iteration 13, loss = 0.000798374239820987
iteration 14, loss = 0.000709026528056711
iteration 15, loss = 0.0007454208098351955
iteration 16, loss = 0.0013673657085746527
iteration 17, loss = 0.0011548753827810287
iteration 18, loss = 0.0007826397195458412
iteration 19, loss = 0.0007668749312870204
iteration 20, loss = 0.0008026348659768701
iteration 21, loss = 0.0010242965072393417
iteration 22, loss = 0.0007346777711063623
iteration 23, loss = 0.0006608470575883985
iteration 24, loss = 0.0011605075560510159
iteration 25, loss = 0.0011775813763961196
iteration 26, loss = 0.0007887982646934688
iteration 27, loss = 0.0012416011886671185
iteration 28, loss = 0.0015057669952511787
iteration 29, loss = 0.000752490886952728
iteration 30, loss = 0.0008149486384354532
iteration 31, loss = 0.0008033979102037847
iteration 32, loss = 0.000841805711388588
iteration 33, loss = 0.0009959971066564322
iteration 34, loss = 0.0009770450415089726
iteration 35, loss = 0.0007339305593632162
iteration 36, loss = 0.0008407577406615019
iteration 37, loss = 0.0012923784088343382
iteration 38, loss = 0.0009411373175680637
iteration 39, loss = 0.0018205592641606927
iteration 40, loss = 0.0014883860712870955
iteration 41, loss = 0.001147955539636314
iteration 42, loss = 0.0010192359331995249
iteration 43, loss = 0.0008470460306853056
iteration 44, loss = 0.0011050696484744549
iteration 45, loss = 0.0007060710340738297
iteration 46, loss = 0.0009846899192780256
iteration 47, loss = 0.0017497232183814049
iteration 48, loss = 0.001708364230580628
iteration 49, loss = 0.001997960265725851
iteration 50, loss = 0.0008164587197825313
iteration 51, loss = 0.0018231087597087026
iteration 52, loss = 0.0008863424300216138
iteration 53, loss = 0.000845734728500247
iteration 54, loss = 0.0009927998762577772
iteration 55, loss = 0.0008029589080251753
iteration 56, loss = 0.0007945255492813885
iteration 57, loss = 0.0008905322756618261
iteration 58, loss = 0.0009136538137681782
iteration 59, loss = 0.0008568242192268372
iteration 60, loss = 0.0006612601573579013
iteration 61, loss = 0.0007664449512958527
iteration 62, loss = 0.0009689237922430038
iteration 63, loss = 0.0017083528218790889
iteration 64, loss = 0.0007617828086949885
iteration 65, loss = 0.0007610387401655316
iteration 66, loss = 0.0009662958327680826
iteration 67, loss = 0.0010295138927176595
iteration 68, loss = 0.0008608947973698378
iteration 69, loss = 0.0009837629040703177
iteration 70, loss = 0.0008969412883743644
iteration 71, loss = 0.0007915118476375937
iteration 72, loss = 0.0008052555494941771
iteration 73, loss = 0.0009131046826951206
iteration 74, loss = 0.000845032453071326
iteration 75, loss = 0.0007445564842782915
iteration 76, loss = 0.0008184178732335567
iteration 77, loss = 0.0008066512527875602
iteration 78, loss = 0.0008920668624341488
iteration 79, loss = 0.0006839970592409372
iteration 80, loss = 0.0008154809474945068
iteration 81, loss = 0.0009608047548681498
iteration 82, loss = 0.0006905430927872658
iteration 83, loss = 0.0010241976706311107
iteration 84, loss = 0.0008114614174701273
iteration 85, loss = 0.0007122759707272053
iteration 86, loss = 0.0008497710223309696
iteration 87, loss = 0.0007750866934657097
iteration 88, loss = 0.0007913151639513671
iteration 89, loss = 0.000694751855917275
iteration 90, loss = 0.00086215790361166
iteration 91, loss = 0.0008868370787240565
iteration 92, loss = 0.0007963817333802581
iteration 93, loss = 0.0007144731353037059
iteration 94, loss = 0.0009549687383696437
iteration 95, loss = 0.0009430084610357881
iteration 96, loss = 0.0008877450018189847
iteration 97, loss = 0.0007745089242234826
iteration 98, loss = 0.0009304662817157805
iteration 99, loss = 0.0008976154495030642
iteration 100, loss = 0.0008466701256111264
iteration 101, loss = 0.0007250901544466615
iteration 102, loss = 0.0017620273865759373
iteration 103, loss = 0.0007735956460237503
iteration 104, loss = 0.0009105807985179126
iteration 105, loss = 0.0008846667478792369
iteration 106, loss = 0.0008553045336157084
iteration 107, loss = 0.0009107645018957555
iteration 108, loss = 0.001592522137798369
iteration 109, loss = 0.0010201565455645323
iteration 110, loss = 0.0010593822225928307
iteration 111, loss = 0.0008704650681465864
iteration 112, loss = 0.0013217385858297348
iteration 113, loss = 0.0007189987227320671
iteration 114, loss = 0.0008484497666358948
iteration 115, loss = 0.0009591951966285706
iteration 116, loss = 0.0007946898695081472
iteration 117, loss = 0.000905002816580236
iteration 118, loss = 0.0008616691920906305
iteration 119, loss = 0.0013160423841327429
iteration 120, loss = 0.0009647550759837031
iteration 121, loss = 0.0008331758435815573
iteration 122, loss = 0.0008366098627448082
iteration 123, loss = 0.0008286959491670132
iteration 124, loss = 0.0006771578919142485
iteration 125, loss = 0.000991655862890184
iteration 126, loss = 0.0009741275571286678
iteration 127, loss = 0.0008744323858991265
iteration 128, loss = 0.0008113195654004812
iteration 129, loss = 0.0013986454578116536
iteration 130, loss = 0.0008008172153495252
iteration 131, loss = 0.000814453000202775
iteration 132, loss = 0.0008575710235163569
iteration 133, loss = 0.0015611826675012708
iteration 134, loss = 0.0010151463793590665
iteration 135, loss = 0.0010530834551900625
iteration 136, loss = 0.0016473913565278053
iteration 137, loss = 0.0016956033650785685
iteration 138, loss = 0.0007325906190089881
iteration 139, loss = 0.0010732855880632997
iteration 140, loss = 0.0007053327281028032
iteration 141, loss = 0.0008455543429590762
iteration 142, loss = 0.0012045421171933413
iteration 143, loss = 0.0010971830924972892
iteration 144, loss = 0.00104898726567626
iteration 145, loss = 0.0008184172911569476
iteration 146, loss = 0.0014325815718621016
iteration 147, loss = 0.0009970589308068156
iteration 148, loss = 0.0014915568754076958
iteration 149, loss = 0.0007259557605721056
iteration 150, loss = 0.0008090962655842304
iteration 151, loss = 0.0008054873906075954
iteration 152, loss = 0.0008918714011088014
iteration 153, loss = 0.001102240290492773
iteration 154, loss = 0.0007850754191167653
iteration 155, loss = 0.0007212500786408782
iteration 156, loss = 0.0007334726396948099
iteration 157, loss = 0.0009201249340549111
iteration 158, loss = 0.0007354617700912058
iteration 159, loss = 0.0006929183145985007
iteration 160, loss = 0.0008026891737245023
iteration 161, loss = 0.0008022937690839171
iteration 162, loss = 0.001015401678159833
iteration 163, loss = 0.0006887204363010824
iteration 164, loss = 0.0009080295567400753
iteration 165, loss = 0.0015685316175222397
iteration 166, loss = 0.001366791664622724
iteration 167, loss = 0.0008199262665584683
iteration 168, loss = 0.0008520393166691065
iteration 169, loss = 0.0014854006003588438
iteration 170, loss = 0.0013148190919309855
iteration 171, loss = 0.0010398020967841148
iteration 172, loss = 0.0006000621942803264
iteration 173, loss = 0.0009881085716187954
iteration 174, loss = 0.0009655761532485485
iteration 175, loss = 0.0007332760724239051
iteration 176, loss = 0.0008058731909841299
iteration 177, loss = 0.0008724819053895772
iteration 178, loss = 0.0007138115470297635
iteration 179, loss = 0.0010623842244967818
iteration 180, loss = 0.0014075246872380376
iteration 181, loss = 0.001049629645422101
iteration 182, loss = 0.0009263622341677547
iteration 183, loss = 0.0007941488875076175
iteration 184, loss = 0.0008680432802066207
iteration 185, loss = 0.0015164323849603534
iteration 186, loss = 0.0009896198753267527
iteration 187, loss = 0.0007773282704874873
iteration 188, loss = 0.0009276191121898592
iteration 189, loss = 0.0008314815349876881
iteration 190, loss = 0.0013199076056480408
iteration 191, loss = 0.0007780513260513544
iteration 192, loss = 0.0007854909636080265
iteration 193, loss = 0.0008416122291237116
iteration 194, loss = 0.0011101667769253254
iteration 195, loss = 0.0008660960011184216
iteration 196, loss = 0.0010506678372621536
iteration 197, loss = 0.0007469330448657274
iteration 198, loss = 0.0007240090635605156
iteration 199, loss = 0.0007037805044092238
iteration 200, loss = 0.0007482132059521973
iteration 201, loss = 0.0007713493541814387
iteration 202, loss = 0.0006217676564119756
iteration 203, loss = 0.0009157821186818182
iteration 204, loss = 0.0011622488964349031
iteration 205, loss = 0.000799017958343029
iteration 206, loss = 0.0006311418837867677
iteration 207, loss = 0.0012027837801724672
iteration 208, loss = 0.0013214352075010538
iteration 209, loss = 0.0010201360564678907
iteration 210, loss = 0.0008647993090562522
iteration 211, loss = 0.0008184491307474673
iteration 212, loss = 0.0007742014713585377
iteration 213, loss = 0.0006483229226432741
iteration 214, loss = 0.0009105276549234986
iteration 215, loss = 0.0007880993653088808
iteration 216, loss = 0.0010955330217257142
iteration 217, loss = 0.0007089790306054056
iteration 218, loss = 0.0008315043523907661
iteration 219, loss = 0.0007804312626831234
iteration 220, loss = 0.0008271497208625078
iteration 221, loss = 0.0009672679007053375
iteration 222, loss = 0.0007605423452332616
iteration 223, loss = 0.0024233334697782993
iteration 224, loss = 0.0008102009305730462
iteration 225, loss = 0.0009111581020988524
iteration 226, loss = 0.0009160976042039692
iteration 227, loss = 0.0007173760095611215
iteration 228, loss = 0.0008731172420084476
iteration 229, loss = 0.000828225864097476
iteration 230, loss = 0.0007338427822105587
iteration 231, loss = 0.0010804972844198346
iteration 232, loss = 0.0010458333417773247
iteration 233, loss = 0.0009471045923419297
iteration 234, loss = 0.0007108087302185595
iteration 235, loss = 0.0008443373953923583
iteration 236, loss = 0.0006537596927955747
iteration 237, loss = 0.0006963961641304195
iteration 238, loss = 0.0012877859408035874
iteration 239, loss = 0.0007509991992264986
iteration 240, loss = 0.0009084967314265668
iteration 241, loss = 0.0008689808892086148
iteration 242, loss = 0.0007793938857503235
iteration 243, loss = 0.0010941913351416588
iteration 244, loss = 0.0014613332459703088
iteration 245, loss = 0.0011221852619200945
iteration 246, loss = 0.0008911605691537261
iteration 247, loss = 0.0007159172091633081
iteration 248, loss = 0.0014302815543487668
iteration 249, loss = 0.0015490150544792414
iteration 250, loss = 0.0010299457935616374
iteration 251, loss = 0.0006922475295141339
iteration 252, loss = 0.0008452644106000662
iteration 253, loss = 0.001805752981454134
iteration 254, loss = 0.0006966195651330054
iteration 255, loss = 0.0008044101996347308
iteration 256, loss = 0.0009679929353296757
iteration 257, loss = 0.0013750528451055288
iteration 258, loss = 0.0017475162167102098
iteration 259, loss = 0.0006271739257499576
iteration 260, loss = 0.0009074381086975336
iteration 261, loss = 0.0006433931994251907
iteration 262, loss = 0.0007287118351086974
iteration 263, loss = 0.0007754125981591642
iteration 264, loss = 0.0010035112500190735
iteration 265, loss = 0.0008999122073873878
iteration 266, loss = 0.0007237031823024154
iteration 267, loss = 0.0009485939517617226
iteration 268, loss = 0.0009188212570734322
iteration 269, loss = 0.001194686396047473
iteration 270, loss = 0.0009742247639223933
iteration 271, loss = 0.0008266940712928772
iteration 272, loss = 0.001456028432585299
iteration 273, loss = 0.0015321848914027214
iteration 274, loss = 0.0011438826331868768
iteration 275, loss = 0.0009117258014157414
iteration 276, loss = 0.0007001952617429197
iteration 277, loss = 0.0010229047620669007
iteration 278, loss = 0.0007977582281455398
iteration 279, loss = 0.0007691896753385663
iteration 280, loss = 0.0009744656272232533
iteration 281, loss = 0.0008918037638068199
iteration 282, loss = 0.0015611323760822415
iteration 283, loss = 0.0009623183286748827
iteration 284, loss = 0.0007631655898876488
iteration 285, loss = 0.0006604186492040753
iteration 286, loss = 0.0008037250954657793
iteration 287, loss = 0.0008484977879561484
iteration 288, loss = 0.0008960283594205976
iteration 289, loss = 0.0008080543484538794
iteration 290, loss = 0.0009088539518415928
iteration 291, loss = 0.0006846780306659639
iteration 292, loss = 0.0008519611437804997
iteration 293, loss = 0.0011509793112054467
iteration 294, loss = 0.0009836229728534818
iteration 295, loss = 0.0010858208406716585
iteration 296, loss = 0.000762919255066663
iteration 297, loss = 0.0015482392627745867
iteration 298, loss = 0.0008278667228296399
iteration 299, loss = 0.0007493523880839348
iteration 300, loss = 0.0010260872077196836
iteration 1, loss = 0.0006527072400785983
iteration 2, loss = 0.0010018493048846722
iteration 3, loss = 0.000861434091348201
iteration 4, loss = 0.0011514435755088925
iteration 5, loss = 0.0007974182954058051
iteration 6, loss = 0.0007975141052156687
iteration 7, loss = 0.0007738379063084722
iteration 8, loss = 0.0010567428544163704
iteration 9, loss = 0.0007376311114057899
iteration 10, loss = 0.0009597317548468709
iteration 11, loss = 0.0006984018255025148
iteration 12, loss = 0.0008417219505645335
iteration 13, loss = 0.0009043840691447258
iteration 14, loss = 0.000918572535738349
iteration 15, loss = 0.001187642803415656
iteration 16, loss = 0.0007464431691914797
iteration 17, loss = 0.001369424513541162
iteration 18, loss = 0.0008987808832898736
iteration 19, loss = 0.0007431783596985042
iteration 20, loss = 0.0008585468167439103
iteration 21, loss = 0.0007187608280219138
iteration 22, loss = 0.000803648610599339
iteration 23, loss = 0.0008351597934961319
iteration 24, loss = 0.0013274747179821134
iteration 25, loss = 0.0009267755667679012
iteration 26, loss = 0.00099744217004627
iteration 27, loss = 0.0008065907168202102
iteration 28, loss = 0.0006995337316766381
iteration 29, loss = 0.0008939190884120762
iteration 30, loss = 0.0007551814196631312
iteration 31, loss = 0.0008254031999967992
iteration 32, loss = 0.0008881797548383474
iteration 33, loss = 0.0025181847158819437
iteration 34, loss = 0.001016686437651515
iteration 35, loss = 0.0007345456979237497
iteration 36, loss = 0.0008736634626984596
iteration 37, loss = 0.0007565455161966383
iteration 38, loss = 0.0009674086468294263
iteration 39, loss = 0.0006916022975929081
iteration 40, loss = 0.0008957079844549298
iteration 41, loss = 0.000887671485543251
iteration 42, loss = 0.0015931540401652455
iteration 43, loss = 0.0009612275753170252
iteration 44, loss = 0.0006761389086022973
iteration 45, loss = 0.0008605655748397112
iteration 46, loss = 0.0006820868002250791
iteration 47, loss = 0.0009290234884247184
iteration 48, loss = 0.0025016339495778084
iteration 49, loss = 0.0014930268516764045
iteration 50, loss = 0.000804698618594557
iteration 51, loss = 0.0008503813296556473
iteration 52, loss = 0.0007888512336649001
iteration 53, loss = 0.0012097827857360244
iteration 54, loss = 0.0007865034858696163
iteration 55, loss = 0.0007811412215232849
iteration 56, loss = 0.0006962742190808058
iteration 57, loss = 0.000728362996596843
iteration 58, loss = 0.0010097053600475192
iteration 59, loss = 0.0020664341282099485
iteration 60, loss = 0.0014881021343171597
iteration 61, loss = 0.0009007799671962857
iteration 62, loss = 0.0007573209004476666
iteration 63, loss = 0.0012932182289659977
iteration 64, loss = 0.0010969187133014202
iteration 65, loss = 0.0007410199614241719
iteration 66, loss = 0.0009857937693595886
iteration 67, loss = 0.0007589728338643909
iteration 68, loss = 0.001168325892649591
iteration 69, loss = 0.0007615684298798442
iteration 70, loss = 0.0007917312323115766
iteration 71, loss = 0.0014294912107288837
iteration 72, loss = 0.0007715964457020164
iteration 73, loss = 0.0007963665411807597
iteration 74, loss = 0.0014600150752812624
iteration 75, loss = 0.0007257550023496151
iteration 76, loss = 0.0010028050746768713
iteration 77, loss = 0.0008489128667861223
iteration 78, loss = 0.001939471927471459
iteration 79, loss = 0.0011060917750000954
iteration 80, loss = 0.001374933053739369
iteration 81, loss = 0.0006480685551650822
iteration 82, loss = 0.000824854476377368
iteration 83, loss = 0.0008665581699460745
iteration 84, loss = 0.0008429401204921305
iteration 85, loss = 0.0009530996321700513
iteration 86, loss = 0.001030144514515996
iteration 87, loss = 0.0007513929158449173
iteration 88, loss = 0.0008690666290931404
iteration 89, loss = 0.000803087605163455
iteration 90, loss = 0.0008050234755501151
iteration 91, loss = 0.000994403031654656
iteration 92, loss = 0.0008377754711546004
iteration 93, loss = 0.0008196027483791113
iteration 94, loss = 0.0006887720664963126
iteration 95, loss = 0.0017613883828744292
iteration 96, loss = 0.0007524638785980642
iteration 97, loss = 0.0008552455110475421
iteration 98, loss = 0.0007485259557142854
iteration 99, loss = 0.0008806877303868532
iteration 100, loss = 0.0007548253051936626
iteration 101, loss = 0.0008573701488785446
iteration 102, loss = 0.0010855605360120535
iteration 103, loss = 0.000761187169700861
iteration 104, loss = 0.0007882605423219502
iteration 105, loss = 0.0006753627676516771
iteration 106, loss = 0.0009859632700681686
iteration 107, loss = 0.0006906540365889668
iteration 108, loss = 0.0010295312386006117
iteration 109, loss = 0.0015074695693328977
iteration 110, loss = 0.0011173480888828635
iteration 111, loss = 0.0009209608542732894
iteration 112, loss = 0.0008956458768807352
iteration 113, loss = 0.0012949698138982058
iteration 114, loss = 0.001003272132948041
iteration 115, loss = 0.0009211820433847606
iteration 116, loss = 0.000913986295927316
iteration 117, loss = 0.0009737703949213028
iteration 118, loss = 0.0016881170449778438
iteration 119, loss = 0.0009739567758515477
iteration 120, loss = 0.0008910613250918686
iteration 121, loss = 0.0008885227725841105
iteration 122, loss = 0.0012327649164944887
iteration 123, loss = 0.0008279688190668821
iteration 124, loss = 0.0007123731775209308
iteration 125, loss = 0.0015563800698146224
iteration 126, loss = 0.0011566558387130499
iteration 127, loss = 0.0007990272133611143
iteration 128, loss = 0.0006521281320601702
iteration 129, loss = 0.001053091138601303
iteration 130, loss = 0.0007981167291291058
iteration 131, loss = 0.0008549396879971027
iteration 132, loss = 0.0008547408506274223
iteration 133, loss = 0.0008613315876573324
iteration 134, loss = 0.0007622800185345113
iteration 135, loss = 0.0008679932216182351
iteration 136, loss = 0.0007393687264993787
iteration 137, loss = 0.0008485991274937987
iteration 138, loss = 0.0008872814942151308
iteration 139, loss = 0.0010272975778207183
iteration 140, loss = 0.001083464827388525
iteration 141, loss = 0.0012098560109734535
iteration 142, loss = 0.0009023280581459403
iteration 143, loss = 0.0012004994787275791
iteration 144, loss = 0.0007820444297976792
iteration 145, loss = 0.0008148206397891045
iteration 146, loss = 0.0010004056384786963
iteration 147, loss = 0.001101732486858964
iteration 148, loss = 0.0008184579201042652
iteration 149, loss = 0.0008170540677383542
iteration 150, loss = 0.0011044887360185385
iteration 151, loss = 0.0009528475347906351
iteration 152, loss = 0.0008790952851995826
iteration 153, loss = 0.0007953943568281829
iteration 154, loss = 0.0007502764347009361
iteration 155, loss = 0.0007042125216685236
iteration 156, loss = 0.0007678488036617637
iteration 157, loss = 0.0008252846891991794
iteration 158, loss = 0.0008226633653976023
iteration 159, loss = 0.0006834340165369213
iteration 160, loss = 0.0007617415394634008
iteration 161, loss = 0.0008780413772910833
iteration 162, loss = 0.0012619061162695289
iteration 163, loss = 0.0023763831704854965
iteration 164, loss = 0.0008162062731571496
iteration 165, loss = 0.0008119498961605132
iteration 166, loss = 0.0007646156009286642
iteration 167, loss = 0.0009061244782060385
iteration 168, loss = 0.0007807741058059037
iteration 169, loss = 0.0008219873416237533
iteration 170, loss = 0.0008283189963549376
iteration 171, loss = 0.000819975626654923
iteration 172, loss = 0.0008531227940693498
iteration 173, loss = 0.0006005759350955486
iteration 174, loss = 0.0008650097297504544
iteration 175, loss = 0.0007454477017745376
iteration 176, loss = 0.0009538612794131041
iteration 177, loss = 0.0009711570455692708
iteration 178, loss = 0.0015435689128935337
iteration 179, loss = 0.0017538906540721655
iteration 180, loss = 0.0007282577571459115
iteration 181, loss = 0.0011668496299535036
iteration 182, loss = 0.0008605954935774207
iteration 183, loss = 0.000998481991700828
iteration 184, loss = 0.0008215897250920534
iteration 185, loss = 0.0007436497253365815
iteration 186, loss = 0.0015138644957914948
iteration 187, loss = 0.0008716626325622201
iteration 188, loss = 0.0008610592922195792
iteration 189, loss = 0.0010081427171826363
iteration 190, loss = 0.0008311321143992245
iteration 191, loss = 0.0007170353201217949
iteration 192, loss = 0.0012802551500499249
iteration 193, loss = 0.0007289443747140467
iteration 194, loss = 0.000723015982657671
iteration 195, loss = 0.000660037447232753
iteration 196, loss = 0.0009978740708902478
iteration 197, loss = 0.0014775801682844758
iteration 198, loss = 0.0007653863867744803
iteration 199, loss = 0.0010082037188112736
iteration 200, loss = 0.0010272730141878128
iteration 201, loss = 0.0007778147701174021
iteration 202, loss = 0.0011843206593766809
iteration 203, loss = 0.0009259841172024608
iteration 204, loss = 0.0006238922942429781
iteration 205, loss = 0.0008697574958205223
iteration 206, loss = 0.0008557824185118079
iteration 207, loss = 0.0009967971127480268
iteration 208, loss = 0.0008860119851306081
iteration 209, loss = 0.0007424065261147916
iteration 210, loss = 0.0011075371876358986
iteration 211, loss = 0.0009883858729153872
iteration 212, loss = 0.0009594978182576597
iteration 213, loss = 0.0009659435017965734
iteration 214, loss = 0.0015193962026387453
iteration 215, loss = 0.000858495885040611
iteration 216, loss = 0.0009311771718785167
iteration 217, loss = 0.0007255883538164198
iteration 218, loss = 0.001009344938211143
iteration 219, loss = 0.0010208982275798917
iteration 220, loss = 0.0007838441524654627
iteration 221, loss = 0.0009412058861926198
iteration 222, loss = 0.0013260309351608157
iteration 223, loss = 0.0011304727522656322
iteration 224, loss = 0.0014227156061679125
iteration 225, loss = 0.0008734430884942412
iteration 226, loss = 0.0008461695397272706
iteration 227, loss = 0.0008544820011593401
iteration 228, loss = 0.0006821761489845812
iteration 229, loss = 0.00149579718708992
iteration 230, loss = 0.0013697647955268621
iteration 231, loss = 0.0008168644271790981
iteration 232, loss = 0.000942000828217715
iteration 233, loss = 0.0011655351845547557
iteration 234, loss = 0.0008407514542341232
iteration 235, loss = 0.0010439666220918298
iteration 236, loss = 0.0007746476330794394
iteration 237, loss = 0.0009537130827084184
iteration 238, loss = 0.0010655713267624378
iteration 239, loss = 0.0015286100097000599
iteration 240, loss = 0.0007833835552446544
iteration 241, loss = 0.0006693065515719354
iteration 242, loss = 0.0007539840880781412
iteration 243, loss = 0.0008636280545033514
iteration 244, loss = 0.000750657869502902
iteration 245, loss = 0.0006789920735172927
iteration 246, loss = 0.0010889241239055991
iteration 247, loss = 0.0007804419146850705
iteration 248, loss = 0.001516756252385676
iteration 249, loss = 0.0008399925427511334
iteration 250, loss = 0.0009010907378979027
iteration 251, loss = 0.0007355902343988419
iteration 252, loss = 0.0006919810548424721
iteration 253, loss = 0.0011324292281642556
iteration 254, loss = 0.0016023456119000912
iteration 255, loss = 0.0007871772395446897
iteration 256, loss = 0.000810253550298512
iteration 257, loss = 0.0015771167818456888
iteration 258, loss = 0.0007963590906001627
iteration 259, loss = 0.0009236651821993291
iteration 260, loss = 0.0007830982794985175
iteration 261, loss = 0.0008151959627866745
iteration 262, loss = 0.0008891604957170784
iteration 263, loss = 0.0007814785931259394
iteration 264, loss = 0.0011035618372261524
iteration 265, loss = 0.0009386716410517693
iteration 266, loss = 0.0012167723616585135
iteration 267, loss = 0.0012149371905252337
iteration 268, loss = 0.00106084777507931
iteration 269, loss = 0.0008312738500535488
iteration 270, loss = 0.0008721236954443157
iteration 271, loss = 0.0007333679241128266
iteration 272, loss = 0.0007862772908993065
iteration 273, loss = 0.0015118989394977689
iteration 274, loss = 0.0008229660452343524
iteration 275, loss = 0.0010444269282743335
iteration 276, loss = 0.0007053235312923789
iteration 277, loss = 0.000731569598428905
iteration 278, loss = 0.0007624446880072355
iteration 279, loss = 0.0008835965418256819
iteration 280, loss = 0.000999664654955268
iteration 281, loss = 0.0008557264227420092
iteration 282, loss = 0.0009115476277656853
iteration 283, loss = 0.0016081914072856307
iteration 284, loss = 0.000728607177734375
iteration 285, loss = 0.0011579162674024701
iteration 286, loss = 0.0009507792419753969
iteration 287, loss = 0.0008849520818330348
iteration 288, loss = 0.0007943552918732166
iteration 289, loss = 0.0006704248953610659
iteration 290, loss = 0.0008613491663709283
iteration 291, loss = 0.0007518017664551735
iteration 292, loss = 0.0010143149411305785
iteration 293, loss = 0.0007723231101408601
iteration 294, loss = 0.0009314734488725662
iteration 295, loss = 0.000858938496094197
iteration 296, loss = 0.0008712059934623539
iteration 297, loss = 0.0009038466378115118
iteration 298, loss = 0.0009946288773790002
iteration 299, loss = 0.0008847130229696631
iteration 300, loss = 0.0007301743025891483
iteration 1, loss = 0.0008045510621741414
iteration 2, loss = 0.0007928853738121688
iteration 3, loss = 0.0008612207602709532
iteration 4, loss = 0.0007667455356568098
iteration 5, loss = 0.0008088680333457887
iteration 6, loss = 0.0009198233601637185
iteration 7, loss = 0.001476890523917973
iteration 8, loss = 0.000779107678681612
iteration 9, loss = 0.0007644712459295988
iteration 10, loss = 0.0008600808214396238
iteration 11, loss = 0.0007403319468721747
iteration 12, loss = 0.0013704110169783235
iteration 13, loss = 0.0008787845144979656
iteration 14, loss = 0.0005831450107507408
iteration 15, loss = 0.0010690419003367424
iteration 16, loss = 0.0012781278928741813
iteration 17, loss = 0.0014451645547524095
iteration 18, loss = 0.0007199104875326157
iteration 19, loss = 0.0009155325824394822
iteration 20, loss = 0.000821068708319217
iteration 21, loss = 0.0008141367579810321
iteration 22, loss = 0.0007899993797764182
iteration 23, loss = 0.0008863632101565599
iteration 24, loss = 0.0010610141325742006
iteration 25, loss = 0.0008023460395634174
iteration 26, loss = 0.0009106172365136445
iteration 27, loss = 0.000963149475865066
iteration 28, loss = 0.00104418711271137
iteration 29, loss = 0.001111946301534772
iteration 30, loss = 0.0008017780492082238
iteration 31, loss = 0.0007735216640867293
iteration 32, loss = 0.0006748478044755757
iteration 33, loss = 0.0008319871849380434
iteration 34, loss = 0.0007958329515531659
iteration 35, loss = 0.0007099795038811862
iteration 36, loss = 0.0006826351745985448
iteration 37, loss = 0.0008396339253522456
iteration 38, loss = 0.0007587745785713196
iteration 39, loss = 0.0009760878747329116
iteration 40, loss = 0.0015099582960829139
iteration 41, loss = 0.0007743365131318569
iteration 42, loss = 0.0007325544138439
iteration 43, loss = 0.0008677929872646928
iteration 44, loss = 0.0015819657128304243
iteration 45, loss = 0.0009341322002001107
iteration 46, loss = 0.00125367590226233
iteration 47, loss = 0.0015768767334520817
iteration 48, loss = 0.0010469157714396715
iteration 49, loss = 0.00157223641872406
iteration 50, loss = 0.0011363615049049258
iteration 51, loss = 0.001454461133107543
iteration 52, loss = 0.0008096467354334891
iteration 53, loss = 0.000725521647837013
iteration 54, loss = 0.0007904738304205239
iteration 55, loss = 0.0011793995508924127
iteration 56, loss = 0.000926547625567764
iteration 57, loss = 0.0007528482237830758
iteration 58, loss = 0.0011510985204949975
iteration 59, loss = 0.0009693655883893371
iteration 60, loss = 0.0006614872254431248
iteration 61, loss = 0.0008679168531671166
iteration 62, loss = 0.0015131167601794004
iteration 63, loss = 0.0008436630014330149
iteration 64, loss = 0.000919206184335053
iteration 65, loss = 0.0007892202702350914
iteration 66, loss = 0.0013428881065919995
iteration 67, loss = 0.0008173623355105519
iteration 68, loss = 0.0008511841879226267
iteration 69, loss = 0.0009649074054323137
iteration 70, loss = 0.0007092977175489068
iteration 71, loss = 0.0009002285660244524
iteration 72, loss = 0.0011980487033724785
iteration 73, loss = 0.0009405076270923018
iteration 74, loss = 0.0008663576445542276
iteration 75, loss = 0.0007553454488515854
iteration 76, loss = 0.0015912560047581792
iteration 77, loss = 0.0010778437135741115
iteration 78, loss = 0.0011808992130681872
iteration 79, loss = 0.0006652944139204919
iteration 80, loss = 0.0008476426592096686
iteration 81, loss = 0.0007876707823015749
iteration 82, loss = 0.0014044797280803323
iteration 83, loss = 0.0019916901364922523
iteration 84, loss = 0.0007982755196280777
iteration 85, loss = 0.0005990461213514209
iteration 86, loss = 0.001399586210027337
iteration 87, loss = 0.0008378421189263463
iteration 88, loss = 0.0011086729355156422
iteration 89, loss = 0.000794316059909761
iteration 90, loss = 0.0014973994111642241
iteration 91, loss = 0.0008177518611773849
iteration 92, loss = 0.0012251860462129116
iteration 93, loss = 0.000759629940148443
iteration 94, loss = 0.0009267718414776027
iteration 95, loss = 0.0007208400638774037
iteration 96, loss = 0.0010051206918433309
iteration 97, loss = 0.0007525415858253837
iteration 98, loss = 0.0009487413335591555
iteration 99, loss = 0.0007883190992288291
iteration 100, loss = 0.0012911594239994884
iteration 101, loss = 0.0011979332193732262
iteration 102, loss = 0.0011990426573902369
iteration 103, loss = 0.0014124821173027158
iteration 104, loss = 0.0008231988176703453
iteration 105, loss = 0.0009575646836310625
iteration 106, loss = 0.0008489081519655883
iteration 107, loss = 0.0009913814719766378
iteration 108, loss = 0.0008760576019994915
iteration 109, loss = 0.0009696040069684386
iteration 110, loss = 0.0007914319867268205
iteration 111, loss = 0.0010983444517478347
iteration 112, loss = 0.0007883235812187195
iteration 113, loss = 0.0009216488106176257
iteration 114, loss = 0.0007336538983508945
iteration 115, loss = 0.0013817299623042345
iteration 116, loss = 0.0009475327096879482
iteration 117, loss = 0.0008057162631303072
iteration 118, loss = 0.0011472353944554925
iteration 119, loss = 0.0006609601550735533
iteration 120, loss = 0.0008084990549832582
iteration 121, loss = 0.0007904749363660812
iteration 122, loss = 0.001255952985957265
iteration 123, loss = 0.0007664331933483481
iteration 124, loss = 0.0006601492059417069
iteration 125, loss = 0.0007650539628230035
iteration 126, loss = 0.0007587677100673318
iteration 127, loss = 0.0006464518955908716
iteration 128, loss = 0.000895461649633944
iteration 129, loss = 0.0007658891845494509
iteration 130, loss = 0.000782300136052072
iteration 131, loss = 0.001902374206110835
iteration 132, loss = 0.0006516503635793924
iteration 133, loss = 0.0007352659595198929
iteration 134, loss = 0.0008200198644772172
iteration 135, loss = 0.0015018234262242913
iteration 136, loss = 0.000927027955185622
iteration 137, loss = 0.0008163107559084892
iteration 138, loss = 0.0015948322834447026
iteration 139, loss = 0.0007638252573087811
iteration 140, loss = 0.0009880311554297805
iteration 141, loss = 0.0007161076064221561
iteration 142, loss = 0.0007974933832883835
iteration 143, loss = 0.0012917929561808705
iteration 144, loss = 0.001511545036919415
iteration 145, loss = 0.0007416413282044232
iteration 146, loss = 0.0009121443144977093
iteration 147, loss = 0.0010258870897814631
iteration 148, loss = 0.0008836378110572696
iteration 149, loss = 0.0015445814933627844
iteration 150, loss = 0.0007831038092263043
iteration 151, loss = 0.0009114255663007498
iteration 152, loss = 0.0009109973907470703
iteration 153, loss = 0.0009032934322021902
iteration 154, loss = 0.0008993505616672337
iteration 155, loss = 0.0009126244112849236
iteration 156, loss = 0.0008339804480783641
iteration 157, loss = 0.0009211982251144946
iteration 158, loss = 0.00075668899808079
iteration 159, loss = 0.0008230631938204169
iteration 160, loss = 0.0007890132837928832
iteration 161, loss = 0.0011273074196651578
iteration 162, loss = 0.0010640614200383425
iteration 163, loss = 0.0008166448678821325
iteration 164, loss = 0.0010038423351943493
iteration 165, loss = 0.0007829122478142381
iteration 166, loss = 0.0007812845287844539
iteration 167, loss = 0.0008953906944952905
iteration 168, loss = 0.0007153397891670465
iteration 169, loss = 0.0009635284659452736
iteration 170, loss = 0.0007713580853305757
iteration 171, loss = 0.0012315529165789485
iteration 172, loss = 0.0009288900764659047
iteration 173, loss = 0.0010773091344162822
iteration 174, loss = 0.001057417131960392
iteration 175, loss = 0.0007258827099576592
iteration 176, loss = 0.0011467732256278396
iteration 177, loss = 0.0009465994080528617
iteration 178, loss = 0.0009162379428744316
iteration 179, loss = 0.0017665731720626354
iteration 180, loss = 0.0008381776860915124
iteration 181, loss = 0.0008692780393175781
iteration 182, loss = 0.0007206168374978006
iteration 183, loss = 0.0007206678856164217
iteration 184, loss = 0.0013814782723784447
iteration 185, loss = 0.0008472373592667282
iteration 186, loss = 0.0009153370629064739
iteration 187, loss = 0.0007853121496737003
iteration 188, loss = 0.0007006721571087837
iteration 189, loss = 0.000808536249678582
iteration 190, loss = 0.0010140950325876474
iteration 191, loss = 0.0007606158033013344
iteration 192, loss = 0.0007710512727499008
iteration 193, loss = 0.0008854710613377392
iteration 194, loss = 0.001518147881142795
iteration 195, loss = 0.0010821515461429954
iteration 196, loss = 0.0007003032369539142
iteration 197, loss = 0.000787775672506541
iteration 198, loss = 0.000920412945561111
iteration 199, loss = 0.0008536195382475853
iteration 200, loss = 0.0009454162791371346
iteration 201, loss = 0.0006971686379984021
iteration 202, loss = 0.0006785985897295177
iteration 203, loss = 0.001107318908907473
iteration 204, loss = 0.0008587046177126467
iteration 205, loss = 0.0012036038096994162
iteration 206, loss = 0.0009230266441591084
iteration 207, loss = 0.0007625070284120739
iteration 208, loss = 0.0009339383104816079
iteration 209, loss = 0.0010739973513409495
iteration 210, loss = 0.0008156190160661936
iteration 211, loss = 0.0007881531491875648
iteration 212, loss = 0.0008600573055446148
iteration 213, loss = 0.0007886418607085943
iteration 214, loss = 0.0006650727009400725
iteration 215, loss = 0.000826116418465972
iteration 216, loss = 0.000814635306596756
iteration 217, loss = 0.001072246115654707
iteration 218, loss = 0.0017462626565247774
iteration 219, loss = 0.0014486549189314246
iteration 220, loss = 0.001545633072964847
iteration 221, loss = 0.0008036330109462142
iteration 222, loss = 0.001303480239585042
iteration 223, loss = 0.0009071145323105156
iteration 224, loss = 0.0006466030026786029
iteration 225, loss = 0.0007387827499769628
iteration 226, loss = 0.000883963075466454
iteration 227, loss = 0.0008484392310492694
iteration 228, loss = 0.000829591415822506
iteration 229, loss = 0.001105761039070785
iteration 230, loss = 0.0007876759045757353
iteration 231, loss = 0.0008010565070435405
iteration 232, loss = 0.0008316607563756406
iteration 233, loss = 0.0009132253471761942
iteration 234, loss = 0.0014198708813637495
iteration 235, loss = 0.0009141449700109661
iteration 236, loss = 0.0008493298664689064
iteration 237, loss = 0.0008315151208080351
iteration 238, loss = 0.0011203698813915253
iteration 239, loss = 0.0017968190368264914
iteration 240, loss = 0.0009956152644008398
iteration 241, loss = 0.0010080678621307015
iteration 242, loss = 0.0008245003409683704
iteration 243, loss = 0.0008396116900257766
iteration 244, loss = 0.0006717498181387782
iteration 245, loss = 0.0010835070861503482
iteration 246, loss = 0.0009602347854524851
iteration 247, loss = 0.0007517540943808854
iteration 248, loss = 0.0007627742597833276
iteration 249, loss = 0.0009315479546785355
iteration 250, loss = 0.0010917548788711429
iteration 251, loss = 0.0009950809180736542
iteration 252, loss = 0.0008111178176477551
iteration 253, loss = 0.0015045409090816975
iteration 254, loss = 0.0010285374009981751
iteration 255, loss = 0.0008608170319348574
iteration 256, loss = 0.0008550735656172037
iteration 257, loss = 0.0010623231064528227
iteration 258, loss = 0.0008261024486273527
iteration 259, loss = 0.0009692348539829254
iteration 260, loss = 0.001338882721029222
iteration 261, loss = 0.0011182848829776049
iteration 262, loss = 0.0008251286926679313
iteration 263, loss = 0.0009073301916942
iteration 264, loss = 0.000845761620439589
iteration 265, loss = 0.0007184548885561526
iteration 266, loss = 0.0012015869142487645
iteration 267, loss = 0.0007092119776643813
iteration 268, loss = 0.0014752428978681564
iteration 269, loss = 0.000803617003839463
iteration 270, loss = 0.0008575068786740303
iteration 271, loss = 0.0015739346854388714
iteration 272, loss = 0.0009806568268686533
iteration 273, loss = 0.0008065337315201759
iteration 274, loss = 0.0006792076164856553
iteration 275, loss = 0.0007851351401768625
iteration 276, loss = 0.0006957012228667736
iteration 277, loss = 0.0009989942191168666
iteration 278, loss = 0.0009186551906168461
iteration 279, loss = 0.0008423037361353636
iteration 280, loss = 0.0009107293444685638
iteration 281, loss = 0.001747481757774949
iteration 282, loss = 0.0006530262180604041
iteration 283, loss = 0.0010185807477682829
iteration 284, loss = 0.0007973873871378601
iteration 285, loss = 0.0008806194528006017
iteration 286, loss = 0.0007131900056265295
iteration 287, loss = 0.0008926914888434112
iteration 288, loss = 0.000709090381860733
iteration 289, loss = 0.0007176300277933478
iteration 290, loss = 0.0009190461132675409
iteration 291, loss = 0.0010369796073064208
iteration 292, loss = 0.0009329507010988891
iteration 293, loss = 0.0007884684600867331
iteration 294, loss = 0.000803819508291781
iteration 295, loss = 0.000938948942348361
iteration 296, loss = 0.0010589633602648973
iteration 297, loss = 0.000824625778477639
iteration 298, loss = 0.0011883813422173262
iteration 299, loss = 0.0010412014089524746
iteration 300, loss = 0.0007538175559602678
iteration 1, loss = 0.0008912400808185339
iteration 2, loss = 0.000825820374302566
iteration 3, loss = 0.001489426358602941
iteration 4, loss = 0.0012262914096936584
iteration 5, loss = 0.001222891383804381
iteration 6, loss = 0.0010656702797859907
iteration 7, loss = 0.001021864009089768
iteration 8, loss = 0.0012996195582672954
iteration 9, loss = 0.000915215932764113
iteration 10, loss = 0.0010512269800528884
iteration 11, loss = 0.0006774034700356424
iteration 12, loss = 0.0016656622756272554
iteration 13, loss = 0.0008573060040362179
iteration 14, loss = 0.0008896032231859863
iteration 15, loss = 0.0007847428205423057
iteration 16, loss = 0.0009518091683275998
iteration 17, loss = 0.0007954905740916729
iteration 18, loss = 0.0008438585791736841
iteration 19, loss = 0.0007449163822457194
iteration 20, loss = 0.0007994493935257196
iteration 21, loss = 0.0008104725275188684
iteration 22, loss = 0.0007340522715821862
iteration 23, loss = 0.0011112422216683626
iteration 24, loss = 0.0007849620305933058
iteration 25, loss = 0.0009977284353226423
iteration 26, loss = 0.0009442232549190521
iteration 27, loss = 0.0007949657156132162
iteration 28, loss = 0.0011720480397343636
iteration 29, loss = 0.0007864037761464715
iteration 30, loss = 0.0008031224715523422
iteration 31, loss = 0.0007462085923179984
iteration 32, loss = 0.0008735158480703831
iteration 33, loss = 0.0007500313804484904
iteration 34, loss = 0.0011938675306737423
iteration 35, loss = 0.0007238858961500227
iteration 36, loss = 0.0007295387331396341
iteration 37, loss = 0.0007053633453324437
iteration 38, loss = 0.0008683502092026174
iteration 39, loss = 0.0007946781697683036
iteration 40, loss = 0.0007988869328983128
iteration 41, loss = 0.0008197426795959473
iteration 42, loss = 0.0008059334941208363
iteration 43, loss = 0.001121592940762639
iteration 44, loss = 0.0009032668895088136
iteration 45, loss = 0.0007261111168190837
iteration 46, loss = 0.0011593321105465293
iteration 47, loss = 0.0009568738751113415
iteration 48, loss = 0.001385014969855547
iteration 49, loss = 0.001218931283801794
iteration 50, loss = 0.0008054371573962271
iteration 51, loss = 0.0009774698410183191
iteration 52, loss = 0.0008771775756031275
iteration 53, loss = 0.0008596131228841841
iteration 54, loss = 0.0008606191840954125
iteration 55, loss = 0.0016445631626993418
iteration 56, loss = 0.0007594150956720114
iteration 57, loss = 0.0007467418326996267
iteration 58, loss = 0.0007724154856987298
iteration 59, loss = 0.0009054355323314667
iteration 60, loss = 0.0006952540134079754
iteration 61, loss = 0.0009167219395749271
iteration 62, loss = 0.0008588085183873773
iteration 63, loss = 0.001228930545039475
iteration 64, loss = 0.0010994988260790706
iteration 65, loss = 0.0015508260112255812
iteration 66, loss = 0.001674691098742187
iteration 67, loss = 0.0007665635785087943
iteration 68, loss = 0.0009165275259874761
iteration 69, loss = 0.0007707144250161946
iteration 70, loss = 0.0014245477505028248
iteration 71, loss = 0.0007357490248978138
iteration 72, loss = 0.0008179456926882267
iteration 73, loss = 0.0011927214218303561
iteration 74, loss = 0.0015172363491728902
iteration 75, loss = 0.0009572669514454901
iteration 76, loss = 0.0007398757734335959
iteration 77, loss = 0.0009050174267031252
iteration 78, loss = 0.001543671591207385
iteration 79, loss = 0.0008409465081058443
iteration 80, loss = 0.001112376805394888
iteration 81, loss = 0.0007508512353524566
iteration 82, loss = 0.0010499032214283943
iteration 83, loss = 0.001201309380121529
iteration 84, loss = 0.0007667889585718513
iteration 85, loss = 0.001747454283758998
iteration 86, loss = 0.0008229633676819503
iteration 87, loss = 0.0007571433670818806
iteration 88, loss = 0.0009668123093433678
iteration 89, loss = 0.0011029094457626343
iteration 90, loss = 0.0008963759755715728
iteration 91, loss = 0.0007697390392422676
iteration 92, loss = 0.0010891983984038234
iteration 93, loss = 0.0008606510236859322
iteration 94, loss = 0.001293401699513197
iteration 95, loss = 0.0011004131520166993
iteration 96, loss = 0.0010849314276129007
iteration 97, loss = 0.0007664862205274403
iteration 98, loss = 0.0008001781534403563
iteration 99, loss = 0.0008818170172162354
iteration 100, loss = 0.0007456554449163377
iteration 101, loss = 0.0007230768678709865
iteration 102, loss = 0.0016164126573130488
iteration 103, loss = 0.0006834694067947567
iteration 104, loss = 0.0007298609125427902
iteration 105, loss = 0.0012567042140290141
iteration 106, loss = 0.0008166777552105486
iteration 107, loss = 0.001103913295082748
iteration 108, loss = 0.000875321973580867
iteration 109, loss = 0.0007896450697444379
iteration 110, loss = 0.0008008917211554945
iteration 111, loss = 0.000783305789809674
iteration 112, loss = 0.0007880089106038213
iteration 113, loss = 0.001011627377010882
iteration 114, loss = 0.0009745649877004325
iteration 115, loss = 0.0005495186778716743
iteration 116, loss = 0.000770476704929024
iteration 117, loss = 0.0007183603011071682
iteration 118, loss = 0.0009760037064552307
iteration 119, loss = 0.0010856896406039596
iteration 120, loss = 0.001425578841008246
iteration 121, loss = 0.0007493994780816138
iteration 122, loss = 0.0008947028545662761
iteration 123, loss = 0.0007886519306339324
iteration 124, loss = 0.0010989975417032838
iteration 125, loss = 0.0011140734422951937
iteration 126, loss = 0.0010048320982605219
iteration 127, loss = 0.0007402640767395496
iteration 128, loss = 0.0012096930295228958
iteration 129, loss = 0.0008290896657854319
iteration 130, loss = 0.0006704670959152281
iteration 131, loss = 0.000908631191123277
iteration 132, loss = 0.0009011360816657543
iteration 133, loss = 0.0009619013289920986
iteration 134, loss = 0.0011960481060668826
iteration 135, loss = 0.000856270082294941
iteration 136, loss = 0.0011771649587899446
iteration 137, loss = 0.0007654781802557409
iteration 138, loss = 0.0006316598155535758
iteration 139, loss = 0.0009216659236699343
iteration 140, loss = 0.0008709248504601419
iteration 141, loss = 0.0008349810377694666
iteration 142, loss = 0.0017135539092123508
iteration 143, loss = 0.0010193967027589679
iteration 144, loss = 0.0008272965787909925
iteration 145, loss = 0.0008774405578151345
iteration 146, loss = 0.0011931657791137695
iteration 147, loss = 0.0007554406183771789
iteration 148, loss = 0.0007693787920288742
iteration 149, loss = 0.0008599237771704793
iteration 150, loss = 0.0008217802387662232
iteration 151, loss = 0.0008669551461935043
iteration 152, loss = 0.001179301179945469
iteration 153, loss = 0.00154585181735456
iteration 154, loss = 0.0011127545731142163
iteration 155, loss = 0.0007876593736000359
iteration 156, loss = 0.0010111326118931174
iteration 157, loss = 0.0008056029910221696
iteration 158, loss = 0.001438383013010025
iteration 159, loss = 0.0007486477843485773
iteration 160, loss = 0.0008659535087645054
iteration 161, loss = 0.0010199707467108965
iteration 162, loss = 0.0009338991367258132
iteration 163, loss = 0.000880389183294028
iteration 164, loss = 0.0018702123779803514
iteration 165, loss = 0.0007275279494933784
iteration 166, loss = 0.0007655941881239414
iteration 167, loss = 0.000841179396957159
iteration 168, loss = 0.0012074975529685616
iteration 169, loss = 0.0009607785614207387
iteration 170, loss = 0.0011957210954278708
iteration 171, loss = 0.0009992837440222502
iteration 172, loss = 0.0008740060729905963
iteration 173, loss = 0.0008206124184653163
iteration 174, loss = 0.0013920199126005173
iteration 175, loss = 0.0013092332519590855
iteration 176, loss = 0.0008428281871601939
iteration 177, loss = 0.0008055728976614773
iteration 178, loss = 0.0006897225393913686
iteration 179, loss = 0.0007666024030186236
iteration 180, loss = 0.0014018062502145767
iteration 181, loss = 0.0007213120115920901
iteration 182, loss = 0.0008608351345174015
iteration 183, loss = 0.0009719284134916961
iteration 184, loss = 0.0010207041632384062
iteration 185, loss = 0.0007459576008841395
iteration 186, loss = 0.0008338217157870531
iteration 187, loss = 0.0008445743005722761
iteration 188, loss = 0.0008461970719508827
iteration 189, loss = 0.000772099185269326
iteration 190, loss = 0.0007309437496587634
iteration 191, loss = 0.0007825489738024771
iteration 192, loss = 0.0006918067811056972
iteration 193, loss = 0.0007958034402690828
iteration 194, loss = 0.0015926038613542914
iteration 195, loss = 0.0008926596492528915
iteration 196, loss = 0.0007226632442325354
iteration 197, loss = 0.0011836208868771791
iteration 198, loss = 0.0016966925468295813
iteration 199, loss = 0.0007751845987513661
iteration 200, loss = 0.0009277614299207926
iteration 201, loss = 0.001106233918108046
iteration 202, loss = 0.001045195502229035
iteration 203, loss = 0.0008855508640408516
iteration 204, loss = 0.0007574917981401086
iteration 205, loss = 0.0007531829178333282
iteration 206, loss = 0.000852869066875428
iteration 207, loss = 0.0008892412297427654
iteration 208, loss = 0.0007876441231928766
iteration 209, loss = 0.0009328391752205789
iteration 210, loss = 0.0008688044035807252
iteration 211, loss = 0.0009051942033693194
iteration 212, loss = 0.0014556810492649674
iteration 213, loss = 0.0008078687242232263
iteration 214, loss = 0.0009928791550919414
iteration 215, loss = 0.0008118127007037401
iteration 216, loss = 0.0007956443587318063
iteration 217, loss = 0.0007902554934844375
iteration 218, loss = 0.0007720781723037362
iteration 219, loss = 0.0007685354794375598
iteration 220, loss = 0.0011642761528491974
iteration 221, loss = 0.00071496213786304
iteration 222, loss = 0.000885333982296288
iteration 223, loss = 0.0007844453793950379
iteration 224, loss = 0.0008402304956689477
iteration 225, loss = 0.0007737785927020013
iteration 226, loss = 0.0007653998909518123
iteration 227, loss = 0.000962896563578397
iteration 228, loss = 0.0008880804525688291
iteration 229, loss = 0.00107700249645859
iteration 230, loss = 0.0012666184920817614
iteration 231, loss = 0.0007310325745493174
iteration 232, loss = 0.0007528856513090432
iteration 233, loss = 0.0015432987129315734
iteration 234, loss = 0.0006497177528217435
iteration 235, loss = 0.002000742359086871
iteration 236, loss = 0.0011030189925804734
iteration 237, loss = 0.0007513918098993599
iteration 238, loss = 0.0006407321197912097
iteration 239, loss = 0.0008150813519023359
iteration 240, loss = 0.0010271180653944612
iteration 241, loss = 0.0010323013411834836
iteration 242, loss = 0.0009374834480695426
iteration 243, loss = 0.0010224536526948214
iteration 244, loss = 0.0007664510631002486
iteration 245, loss = 0.000824444112367928
iteration 246, loss = 0.000829675467684865
iteration 247, loss = 0.0009851654758676887
iteration 248, loss = 0.0016481205821037292
iteration 249, loss = 0.0009938044240698218
iteration 250, loss = 0.0009094888810068369
iteration 251, loss = 0.0011814924655482173
iteration 252, loss = 0.0007327928906306624
iteration 253, loss = 0.0008010659366846085
iteration 254, loss = 0.0016091824509203434
iteration 255, loss = 0.0008293543360196054
iteration 256, loss = 0.0008997444529086351
iteration 257, loss = 0.0009755351347848773
iteration 258, loss = 0.0008555302629247308
iteration 259, loss = 0.000806909694802016
iteration 260, loss = 0.001036961330100894
iteration 261, loss = 0.0009208029368892312
iteration 262, loss = 0.0007431460544466972
iteration 263, loss = 0.0006747518200427294
iteration 264, loss = 0.0008879981469362974
iteration 265, loss = 0.0010458097094669938
iteration 266, loss = 0.0009944650810211897
iteration 267, loss = 0.000809257326181978
iteration 268, loss = 0.0007591821486130357
iteration 269, loss = 0.0008144315215758979
iteration 270, loss = 0.0007403044728562236
iteration 271, loss = 0.0009714774205349386
iteration 272, loss = 0.0008356444886885583
iteration 273, loss = 0.0007413510465994477
iteration 274, loss = 0.0011319336481392384
iteration 275, loss = 0.000891948991920799
iteration 276, loss = 0.0009156246669590473
iteration 277, loss = 0.0008583125891163945
iteration 278, loss = 0.0007548567955382168
iteration 279, loss = 0.0008038878440856934
iteration 280, loss = 0.0007701891590841115
iteration 281, loss = 0.0009538204758428037
iteration 282, loss = 0.0006076902500353754
iteration 283, loss = 0.0014002560637891293
iteration 284, loss = 0.0008584746974520385
iteration 285, loss = 0.0007566334097646177
iteration 286, loss = 0.0007191382464952767
iteration 287, loss = 0.0011237364960834384
iteration 288, loss = 0.0011131558567285538
iteration 289, loss = 0.0009106978541240096
iteration 290, loss = 0.0008705641375854611
iteration 291, loss = 0.0008628159412182868
iteration 292, loss = 0.002411931985989213
iteration 293, loss = 0.0008693159325048327
iteration 294, loss = 0.0009253718890249729
iteration 295, loss = 0.0009705110569484532
iteration 296, loss = 0.0011160381836816669
iteration 297, loss = 0.001615661894902587
iteration 298, loss = 0.0007233188953250647
iteration 299, loss = 0.0007293648668564856
iteration 300, loss = 0.0009160276968032122
iteration 1, loss = 0.0007676305249333382
iteration 2, loss = 0.0007298535201698542
iteration 3, loss = 0.0009672365267761052
iteration 4, loss = 0.0008004220435395837
iteration 5, loss = 0.000807522505056113
iteration 6, loss = 0.0010006321826949716
iteration 7, loss = 0.0007624628487974405
iteration 8, loss = 0.0008489685133099556
iteration 9, loss = 0.0007953507010824978
iteration 10, loss = 0.0008657659054733813
iteration 11, loss = 0.0009958789451047778
iteration 12, loss = 0.0009713642066344619
iteration 13, loss = 0.0007957101333886385
iteration 14, loss = 0.0007245238521136343
iteration 15, loss = 0.0007910137064754963
iteration 16, loss = 0.0011991895735263824
iteration 17, loss = 0.000825422175694257
iteration 18, loss = 0.0008302167989313602
iteration 19, loss = 0.0008023021509870887
iteration 20, loss = 0.000831252196803689
iteration 21, loss = 0.0010038139298558235
iteration 22, loss = 0.0007007571402937174
iteration 23, loss = 0.0010045758681371808
iteration 24, loss = 0.0009256419725716114
iteration 25, loss = 0.0007903624209575355
iteration 26, loss = 0.0007882077479735017
iteration 27, loss = 0.0008053116034716368
iteration 28, loss = 0.0007324532489292324
iteration 29, loss = 0.0007102092495188117
iteration 30, loss = 0.0007314833346754313
iteration 31, loss = 0.0008384015527553856
iteration 32, loss = 0.0009946348145604134
iteration 33, loss = 0.0007167214062064886
iteration 34, loss = 0.0008066731388680637
iteration 35, loss = 0.0007223124266602099
iteration 36, loss = 0.0008496258524246514
iteration 37, loss = 0.0016019846079871058
iteration 38, loss = 0.0007989873411133885
iteration 39, loss = 0.0009881587466225028
iteration 40, loss = 0.0007772839744575322
iteration 41, loss = 0.0008278044988401234
iteration 42, loss = 0.0008315167506225407
iteration 43, loss = 0.0009817664977163076
iteration 44, loss = 0.0007820395985618234
iteration 45, loss = 0.0007963423267938197
iteration 46, loss = 0.0010335203260183334
iteration 47, loss = 0.000821180350612849
iteration 48, loss = 0.0011159060522913933
iteration 49, loss = 0.0016427382361143827
iteration 50, loss = 0.0009281774400733411
iteration 51, loss = 0.0007509692222811282
iteration 52, loss = 0.0010024867951869965
iteration 53, loss = 0.0008317734464071691
iteration 54, loss = 0.0016463473439216614
iteration 55, loss = 0.0007500371430069208
iteration 56, loss = 0.0007533907191827893
iteration 57, loss = 0.0016560452058911324
iteration 58, loss = 0.0008752822177484632
iteration 59, loss = 0.0008018483640626073
iteration 60, loss = 0.0007218140526674688
iteration 61, loss = 0.0015456471592187881
iteration 62, loss = 0.0008029375458136201
iteration 63, loss = 0.00145138509105891
iteration 64, loss = 0.0016809238586574793
iteration 65, loss = 0.0013618365628644824
iteration 66, loss = 0.001435962156392634
iteration 67, loss = 0.0008906163275241852
iteration 68, loss = 0.00090053491294384
iteration 69, loss = 0.0010192211484536529
iteration 70, loss = 0.0010953895980492234
iteration 71, loss = 0.0006851166253909469
iteration 72, loss = 0.0018741313833743334
iteration 73, loss = 0.0008800295181572437
iteration 74, loss = 0.0007871402194723487
iteration 75, loss = 0.001173381693661213
iteration 76, loss = 0.0010661922860890627
iteration 77, loss = 0.000871314259711653
iteration 78, loss = 0.0007455152226611972
iteration 79, loss = 0.000805589952506125
iteration 80, loss = 0.001066856668330729
iteration 81, loss = 0.0008431809837929904
iteration 82, loss = 0.0009284807019867003
iteration 83, loss = 0.0009796934900805354
iteration 84, loss = 0.0008985892636701465
iteration 85, loss = 0.0007582537364214659
iteration 86, loss = 0.0009715521591715515
iteration 87, loss = 0.0007118940120562911
iteration 88, loss = 0.000713949091732502
iteration 89, loss = 0.001040747156366706
iteration 90, loss = 0.0011672858381643891
iteration 91, loss = 0.0009377103997394443
iteration 92, loss = 0.0008145131869241595
iteration 93, loss = 0.000927941408008337
iteration 94, loss = 0.0008752175490371883
iteration 95, loss = 0.0007959994836710393
iteration 96, loss = 0.0015601066406816244
iteration 97, loss = 0.000831097480840981
iteration 98, loss = 0.000866285408847034
iteration 99, loss = 0.0009611546411179006
iteration 100, loss = 0.0010336965788155794
iteration 101, loss = 0.0008778625633567572
iteration 102, loss = 0.00089149025734514
iteration 103, loss = 0.0010635263752192259
iteration 104, loss = 0.0008042917470447719
iteration 105, loss = 0.0009072746033780277
iteration 106, loss = 0.0007180103566497564
iteration 107, loss = 0.0010013517457991838
iteration 108, loss = 0.001248844782821834
iteration 109, loss = 0.0008372055017389357
iteration 110, loss = 0.0008347404072992504
iteration 111, loss = 0.000829635071568191
iteration 112, loss = 0.0008963581640273333
iteration 113, loss = 0.0015496881678700447
iteration 114, loss = 0.0008485578582622111
iteration 115, loss = 0.0008695871802046895
iteration 116, loss = 0.0013996220659464598
iteration 117, loss = 0.0008743841317482293
iteration 118, loss = 0.0008361554355360568
iteration 119, loss = 0.000799698056653142
iteration 120, loss = 0.0012858668342232704
iteration 121, loss = 0.0007858952158130705
iteration 122, loss = 0.00115486781578511
iteration 123, loss = 0.0007425813237205148
iteration 124, loss = 0.000802363152615726
iteration 125, loss = 0.0008685393258929253
iteration 126, loss = 0.0012019211426377296
iteration 127, loss = 0.0009672826272435486
iteration 128, loss = 0.000981091638095677
iteration 129, loss = 0.000743886805139482
iteration 130, loss = 0.0007690904312767088
iteration 131, loss = 0.0007927211700007319
iteration 132, loss = 0.0009004261810332537
iteration 133, loss = 0.0010568549623712897
iteration 134, loss = 0.0008799497736617923
iteration 135, loss = 0.0008115481468848884
iteration 136, loss = 0.0013853339478373528
iteration 137, loss = 0.0012322465190663934
iteration 138, loss = 0.0006289732409641147
iteration 139, loss = 0.0016052325954660773
iteration 140, loss = 0.0007620721007697284
iteration 141, loss = 0.0007926705875433981
iteration 142, loss = 0.000892270531039685
iteration 143, loss = 0.001076763728633523
iteration 144, loss = 0.0012057245476171374
iteration 145, loss = 0.0008149287314154208
iteration 146, loss = 0.0008540222188457847
iteration 147, loss = 0.0008047463488765061
iteration 148, loss = 0.0008185521583072841
iteration 149, loss = 0.0008634080877527595
iteration 150, loss = 0.0006943843909539282
iteration 151, loss = 0.00100298214238137
iteration 152, loss = 0.0007542752427980304
iteration 153, loss = 0.0008569593774154782
iteration 154, loss = 0.0007220220286399126
iteration 155, loss = 0.0009134652791544795
iteration 156, loss = 0.0014042864786460996
iteration 157, loss = 0.0007600741810165346
iteration 158, loss = 0.002068637404590845
iteration 159, loss = 0.0006181858479976654
iteration 160, loss = 0.0007267718319781125
iteration 161, loss = 0.0008254850981757045
iteration 162, loss = 0.0011148019693791866
iteration 163, loss = 0.0009305858402512968
iteration 164, loss = 0.0007310554501600564
iteration 165, loss = 0.0009543325286358595
iteration 166, loss = 0.000988663756288588
iteration 167, loss = 0.0006585725932382047
iteration 168, loss = 0.0009839428821578622
iteration 169, loss = 0.0007403724594041705
iteration 170, loss = 0.001033214502967894
iteration 171, loss = 0.001090889098122716
iteration 172, loss = 0.0008425862179137766
iteration 173, loss = 0.0009829328628256917
iteration 174, loss = 0.0006917924038134515
iteration 175, loss = 0.0010969073045998812
iteration 176, loss = 0.000672805355861783
iteration 177, loss = 0.0013901047641411424
iteration 178, loss = 0.0007524527027271688
iteration 179, loss = 0.0009466388146393001
iteration 180, loss = 0.0009675765177235007
iteration 181, loss = 0.0007271059439517558
iteration 182, loss = 0.0016492900904268026
iteration 183, loss = 0.000851151708047837
iteration 184, loss = 0.0012508606305345893
iteration 185, loss = 0.0007981181261129677
iteration 186, loss = 0.0007921256474219263
iteration 187, loss = 0.0007966484990902245
iteration 188, loss = 0.0010260066483169794
iteration 189, loss = 0.0008108594338409603
iteration 190, loss = 0.0007848064415156841
iteration 191, loss = 0.0008360942592844367
iteration 192, loss = 0.0010627331212162971
iteration 193, loss = 0.0015271016163751483
iteration 194, loss = 0.0007755271508358419
iteration 195, loss = 0.0008011070895008743
iteration 196, loss = 0.0008517478127032518
iteration 197, loss = 0.0009801473934203386
iteration 198, loss = 0.0014612554805353284
iteration 199, loss = 0.0008973710355348885
iteration 200, loss = 0.0007719274144619703
iteration 201, loss = 0.0007912157452665269
iteration 202, loss = 0.001048040809109807
iteration 203, loss = 0.0007919625495560467
iteration 204, loss = 0.0008565789321437478
iteration 205, loss = 0.0008076660451479256
iteration 206, loss = 0.0008074046345427632
iteration 207, loss = 0.0012892886297777295
iteration 208, loss = 0.0008716649608686566
iteration 209, loss = 0.0009382315911352634
iteration 210, loss = 0.0008596669649705291
iteration 211, loss = 0.0008538607507944107
iteration 212, loss = 0.001533359638415277
iteration 213, loss = 0.0007440250483341515
iteration 214, loss = 0.0007253804942592978
iteration 215, loss = 0.001200356287881732
iteration 216, loss = 0.0012864558957517147
iteration 217, loss = 0.0009539943421259522
iteration 218, loss = 0.001443854533135891
iteration 219, loss = 0.001114687300287187
iteration 220, loss = 0.001823508064262569
iteration 221, loss = 0.0008519908296875656
iteration 222, loss = 0.0007698084227740765
iteration 223, loss = 0.0006543243071064353
iteration 224, loss = 0.0006630112184211612
iteration 225, loss = 0.0013752738013863564
iteration 226, loss = 0.0006664887769147754
iteration 227, loss = 0.0019000183092430234
iteration 228, loss = 0.0007513028685934842
iteration 229, loss = 0.0008386484696529806
iteration 230, loss = 0.0008113663643598557
iteration 231, loss = 0.0008143216837197542
iteration 232, loss = 0.0013697759713977575
iteration 233, loss = 0.0010884810471907258
iteration 234, loss = 0.0008019374217838049
iteration 235, loss = 0.0007186873117461801
iteration 236, loss = 0.0008559608832001686
iteration 237, loss = 0.0009007533080875874
iteration 238, loss = 0.001148167415522039
iteration 239, loss = 0.002361560007557273
iteration 240, loss = 0.0008018693188205361
iteration 241, loss = 0.0007526272675022483
iteration 242, loss = 0.0007507237605750561
iteration 243, loss = 0.001193133182823658
iteration 244, loss = 0.0008415416814386845
iteration 245, loss = 0.000732362037524581
iteration 246, loss = 0.0008038529194891453
iteration 247, loss = 0.0008687837398611009
iteration 248, loss = 0.001847021863795817
iteration 249, loss = 0.000866758986376226
iteration 250, loss = 0.0012886356562376022
iteration 251, loss = 0.0005775995668955147
iteration 252, loss = 0.0007425514049828053
iteration 253, loss = 0.0010599825764074922
iteration 254, loss = 0.0008244139607995749
iteration 255, loss = 0.0008339757914654911
iteration 256, loss = 0.0009354457724839449
iteration 257, loss = 0.0015488347271457314
iteration 258, loss = 0.0010490847053006291
iteration 259, loss = 0.000970345688983798
iteration 260, loss = 0.0007858653552830219
iteration 261, loss = 0.0005944623262621462
iteration 262, loss = 0.001082590315490961
iteration 263, loss = 0.0007524098036810756
iteration 264, loss = 0.001440244959667325
iteration 265, loss = 0.0011365605751052499
iteration 266, loss = 0.0007326783379539847
iteration 267, loss = 0.0012098046718165278
iteration 268, loss = 0.0007562004611827433
iteration 269, loss = 0.0008167418418452144
iteration 270, loss = 0.0008056864608079195
iteration 271, loss = 0.0007973553147166967
iteration 272, loss = 0.0008125578169710934
iteration 273, loss = 0.0007319033029489219
iteration 274, loss = 0.0008336194441653788
iteration 275, loss = 0.0006954700802452862
iteration 276, loss = 0.0007507476257160306
iteration 277, loss = 0.0010726186446845531
iteration 278, loss = 0.0008982920553535223
iteration 279, loss = 0.0008582265581935644
iteration 280, loss = 0.001123214839026332
iteration 281, loss = 0.0010916052851825953
iteration 282, loss = 0.0007909550331532955
iteration 283, loss = 0.0008340842323377728
iteration 284, loss = 0.0007074596942402422
iteration 285, loss = 0.0013621937250718474
iteration 286, loss = 0.0007722739828750491
iteration 287, loss = 0.0009741253452375531
iteration 288, loss = 0.0008428905857726932
iteration 289, loss = 0.0018069917568936944
iteration 290, loss = 0.0013285332825034857
iteration 291, loss = 0.0007164240232668817
iteration 292, loss = 0.000739486888051033
iteration 293, loss = 0.0007022800273261964
iteration 294, loss = 0.0010096197947859764
iteration 295, loss = 0.0010344128822907805
iteration 296, loss = 0.0008999652927741408
iteration 297, loss = 0.0008559161215089262
iteration 298, loss = 0.000828005897346884
iteration 299, loss = 0.0007949805585667491
iteration 300, loss = 0.001065965392626822
iteration 1, loss = 0.001616782508790493
iteration 2, loss = 0.000900090963114053
iteration 3, loss = 0.0006625116802752018
iteration 4, loss = 0.001448669470846653
iteration 5, loss = 0.0008511217311024666
iteration 6, loss = 0.0008308234391734004
iteration 7, loss = 0.0007450171979144216
iteration 8, loss = 0.0009857628028839827
iteration 9, loss = 0.0008108381880447268
iteration 10, loss = 0.0009216348407790065
iteration 11, loss = 0.0019186431309208274
iteration 12, loss = 0.0007537727360613644
iteration 13, loss = 0.0007092440500855446
iteration 14, loss = 0.0009164290386252105
iteration 15, loss = 0.0008964863372966647
iteration 16, loss = 0.0007705467287451029
iteration 17, loss = 0.0008452368201687932
iteration 18, loss = 0.0007127590361051261
iteration 19, loss = 0.000920590478926897
iteration 20, loss = 0.0008018538937903941
iteration 21, loss = 0.0008476214716210961
iteration 22, loss = 0.0007558967918157578
iteration 23, loss = 0.0007198823150247335
iteration 24, loss = 0.0006227808771654963
iteration 25, loss = 0.0016569800209254026
iteration 26, loss = 0.000678232463542372
iteration 27, loss = 0.0008302630158141255
iteration 28, loss = 0.0008637820719741285
iteration 29, loss = 0.0006432594382204115
iteration 30, loss = 0.0006861698348075151
iteration 31, loss = 0.0008877661311998963
iteration 32, loss = 0.0007277262629941106
iteration 33, loss = 0.0014610863290727139
iteration 34, loss = 0.0016493959119543433
iteration 35, loss = 0.0013880045153200626
iteration 36, loss = 0.0008166668121702969
iteration 37, loss = 0.0007726230542175472
iteration 38, loss = 0.001429842901416123
iteration 39, loss = 0.0008769265841692686
iteration 40, loss = 0.0009168123942799866
iteration 41, loss = 0.0009714281186461449
iteration 42, loss = 0.0010224913712590933
iteration 43, loss = 0.0008406713022850454
iteration 44, loss = 0.0008288713870570064
iteration 45, loss = 0.0008973098592832685
iteration 46, loss = 0.0007319143041968346
iteration 47, loss = 0.0017078886739909649
iteration 48, loss = 0.0013674143701791763
iteration 49, loss = 0.0006843949668109417
iteration 50, loss = 0.0016014187131077051
iteration 51, loss = 0.000786909309681505
iteration 52, loss = 0.0009484457550570369
iteration 53, loss = 0.0008052514167502522
iteration 54, loss = 0.000776824657805264
iteration 55, loss = 0.0009036926203407347
iteration 56, loss = 0.000753678847104311
iteration 57, loss = 0.0013539778301492333
iteration 58, loss = 0.0009879092685878277
iteration 59, loss = 0.0008062211563810706
iteration 60, loss = 0.0008188212523236871
iteration 61, loss = 0.0009218421764671803
iteration 62, loss = 0.0009099466260522604
iteration 63, loss = 0.0007844913052394986
iteration 64, loss = 0.0013689533807337284
iteration 65, loss = 0.0007752748788334429
iteration 66, loss = 0.001010113744996488
iteration 67, loss = 0.0008192872628569603
iteration 68, loss = 0.0008469460881315172
iteration 69, loss = 0.0009619995253160596
iteration 70, loss = 0.0010744152823463082
iteration 71, loss = 0.0011169613571837544
iteration 72, loss = 0.000947703723795712
iteration 73, loss = 0.0008201915770769119
iteration 74, loss = 0.000970414956100285
iteration 75, loss = 0.0007823698688298464
iteration 76, loss = 0.0011161785805597901
iteration 77, loss = 0.0010870127007365227
iteration 78, loss = 0.0007099161157384515
iteration 79, loss = 0.0008616441045887768
iteration 80, loss = 0.001073565217666328
iteration 81, loss = 0.0006744663696736097
iteration 82, loss = 0.0008020860841497779
iteration 83, loss = 0.0007512345910072327
iteration 84, loss = 0.0007539272191934288
iteration 85, loss = 0.0010178847005590796
iteration 86, loss = 0.0011618899879977107
iteration 87, loss = 0.0008681353647261858
iteration 88, loss = 0.0007981884991750121
iteration 89, loss = 0.0008450138266198337
iteration 90, loss = 0.0007971340091899037
iteration 91, loss = 0.0007112526218406856
iteration 92, loss = 0.0010511127766221762
iteration 93, loss = 0.0008190377848222852
iteration 94, loss = 0.0007755208644084632
iteration 95, loss = 0.0008356838952749968
iteration 96, loss = 0.00103483023121953
iteration 97, loss = 0.001458470826037228
iteration 98, loss = 0.0011913941707462072
iteration 99, loss = 0.0005777627811767161
iteration 100, loss = 0.0012219332857057452
iteration 101, loss = 0.0015090665547177196
iteration 102, loss = 0.0011567439651116729
iteration 103, loss = 0.0012091418029740453
iteration 104, loss = 0.0007193052442744374
iteration 105, loss = 0.001140222535468638
iteration 106, loss = 0.0008309872937388718
iteration 107, loss = 0.0010414490243420005
iteration 108, loss = 0.001424324233084917
iteration 109, loss = 0.0008113557705655694
iteration 110, loss = 0.0007668196340091527
iteration 111, loss = 0.0009813470533117652
iteration 112, loss = 0.000799722271040082
iteration 113, loss = 0.0007201567059382796
iteration 114, loss = 0.0010060896165668964
iteration 115, loss = 0.0009999271715059876
iteration 116, loss = 0.001600485178641975
iteration 117, loss = 0.0008892604382708669
iteration 118, loss = 0.0006903673056513071
iteration 119, loss = 0.0006027902709320188
iteration 120, loss = 0.0013501369394361973
iteration 121, loss = 0.0009646167163737118
iteration 122, loss = 0.0007712076185271144
iteration 123, loss = 0.0017060511745512486
iteration 124, loss = 0.0007713551167398691
iteration 125, loss = 0.0008729229448363185
iteration 126, loss = 0.0008328748517669737
iteration 127, loss = 0.0008186654304154217
iteration 128, loss = 0.0015069071669131517
iteration 129, loss = 0.0008054417558014393
iteration 130, loss = 0.0008633937686681747
iteration 131, loss = 0.0009521486354060471
iteration 132, loss = 0.0008401461527682841
iteration 133, loss = 0.001433641999028623
iteration 134, loss = 0.0010480110067874193
iteration 135, loss = 0.0008984728483483195
iteration 136, loss = 0.0009339654934592545
iteration 137, loss = 0.0010328652570024133
iteration 138, loss = 0.0011954109650105238
iteration 139, loss = 0.00069479423109442
iteration 140, loss = 0.001153333461843431
iteration 141, loss = 0.0008045436698012054
iteration 142, loss = 0.0008827780839055777
iteration 143, loss = 0.0010493542067706585
iteration 144, loss = 0.000801278103608638
iteration 145, loss = 0.0007946116384118795
iteration 146, loss = 0.0007833167910575867
iteration 147, loss = 0.0008150091161951423
iteration 148, loss = 0.0011336992029100657
iteration 149, loss = 0.0007732711965218186
iteration 150, loss = 0.001039467635564506
iteration 151, loss = 0.0007143301190808415
iteration 152, loss = 0.0007600748795084655
iteration 153, loss = 0.0007871623965911567
iteration 154, loss = 0.0007199665997177362
iteration 155, loss = 0.0011316200252622366
iteration 156, loss = 0.0010324763134121895
iteration 157, loss = 0.0007674147491343319
iteration 158, loss = 0.0007365529309026897
iteration 159, loss = 0.0010541498195379972
iteration 160, loss = 0.0007395233842544258
iteration 161, loss = 0.0009240456274710596
iteration 162, loss = 0.0008135944954119623
iteration 163, loss = 0.0009480437147431076
iteration 164, loss = 0.0010885830270126462
iteration 165, loss = 0.0011726536322385073
iteration 166, loss = 0.0007411020924337208
iteration 167, loss = 0.0009162399219349027
iteration 168, loss = 0.0008529384504072368
iteration 169, loss = 0.0013172030448913574
iteration 170, loss = 0.0009275357006117702
iteration 171, loss = 0.0008098940015770495
iteration 172, loss = 0.0014866198180243373
iteration 173, loss = 0.0007800383027642965
iteration 174, loss = 0.0018418554682284594
iteration 175, loss = 0.0011565849417820573
iteration 176, loss = 0.0014104322763159871
iteration 177, loss = 0.000964945531450212
iteration 178, loss = 0.0008570401114411652
iteration 179, loss = 0.0007873977883718908
iteration 180, loss = 0.0009439019486308098
iteration 181, loss = 0.0006915948470123112
iteration 182, loss = 0.0008408451685681939
iteration 183, loss = 0.0006630156422033906
iteration 184, loss = 0.0010679883416742086
iteration 185, loss = 0.0010260925628244877
iteration 186, loss = 0.0009074182598851621
iteration 187, loss = 0.0010205775033682585
iteration 188, loss = 0.0009897826239466667
iteration 189, loss = 0.0010718791745603085
iteration 190, loss = 0.0007776165730319917
iteration 191, loss = 0.0009420901187695563
iteration 192, loss = 0.0009425149182789028
iteration 193, loss = 0.0009566346416249871
iteration 194, loss = 0.000724517391063273
iteration 195, loss = 0.001001496915705502
iteration 196, loss = 0.0008147322223521769
iteration 197, loss = 0.0011045762803405523
iteration 198, loss = 0.0014427239075303078
iteration 199, loss = 0.0007977628847584128
iteration 200, loss = 0.001719756401143968
iteration 201, loss = 0.0014889843296259642
iteration 202, loss = 0.0009085234487429261
iteration 203, loss = 0.0008127214387059212
iteration 204, loss = 0.0007874975563026965
iteration 205, loss = 0.000876931706443429
iteration 206, loss = 0.0009735385538078845
iteration 207, loss = 0.0008078468963503838
iteration 208, loss = 0.0007031240384094417
iteration 209, loss = 0.0014615074032917619
iteration 210, loss = 0.0015393981011584401
iteration 211, loss = 0.00084821687778458
iteration 212, loss = 0.0009573533316142857
iteration 213, loss = 0.0009008190827444196
iteration 214, loss = 0.000948114029597491
iteration 215, loss = 0.0007665797602385283
iteration 216, loss = 0.0013372714165598154
iteration 217, loss = 0.0008133369847200811
iteration 218, loss = 0.0008564002346247435
iteration 219, loss = 0.0007856835727579892
iteration 220, loss = 0.0008564066374674439
iteration 221, loss = 0.0007996640051715076
iteration 222, loss = 0.0011914544738829136
iteration 223, loss = 0.0008340509957633913
iteration 224, loss = 0.0010702728759497404
iteration 225, loss = 0.0007220492698252201
iteration 226, loss = 0.0008191990200430155
iteration 227, loss = 0.0015975459245964885
iteration 228, loss = 0.0007177288644015789
iteration 229, loss = 0.000759778602514416
iteration 230, loss = 0.0007586074061691761
iteration 231, loss = 0.0010085203684866428
iteration 232, loss = 0.0008528240141458809
iteration 233, loss = 0.0008421044331043959
iteration 234, loss = 0.0007419281173497438
iteration 235, loss = 0.0008662918116897345
iteration 236, loss = 0.0009931466775014997
iteration 237, loss = 0.0008500125259160995
iteration 238, loss = 0.0010924858506768942
iteration 239, loss = 0.0010649649193510413
iteration 240, loss = 0.0012466335901990533
iteration 241, loss = 0.0008266878430731595
iteration 242, loss = 0.0008288109675049782
iteration 243, loss = 0.0007895649177953601
iteration 244, loss = 0.000810672587249428
iteration 245, loss = 0.0008048363961279392
iteration 246, loss = 0.0007612020126543939
iteration 247, loss = 0.0015299057122319937
iteration 248, loss = 0.0010734065435826778
iteration 249, loss = 0.0010448643006384373
iteration 250, loss = 0.0007315045804716647
iteration 251, loss = 0.0010042611975222826
iteration 252, loss = 0.000890352122951299
iteration 253, loss = 0.0006595730665139854
iteration 254, loss = 0.0015928599750623107
iteration 255, loss = 0.0009774438804015517
iteration 256, loss = 0.0006606627139262855
iteration 257, loss = 0.0009357394883409142
iteration 258, loss = 0.0007602647528983653
iteration 259, loss = 0.0009851744398474693
iteration 260, loss = 0.0007908140541985631
iteration 261, loss = 0.0012553068809211254
iteration 262, loss = 0.0007736975094303489
iteration 263, loss = 0.0008231501560658216
iteration 264, loss = 0.0007456734892912209
iteration 265, loss = 0.0008893648628145456
iteration 266, loss = 0.000813641119748354
iteration 267, loss = 0.0008025912684388459
iteration 268, loss = 0.0008147972403094172
iteration 269, loss = 0.0009681318188086152
iteration 270, loss = 0.0011580706341192126
iteration 271, loss = 0.0009453529492020607
iteration 272, loss = 0.0008211436797864735
iteration 273, loss = 0.000873183598741889
iteration 274, loss = 0.0006232484010979533
iteration 275, loss = 0.0008566014003008604
iteration 276, loss = 0.0009300441015511751
iteration 277, loss = 0.0013560083461925387
iteration 278, loss = 0.0010243740398436785
iteration 279, loss = 0.0008153239614330232
iteration 280, loss = 0.0008621916058473289
iteration 281, loss = 0.0009650418651290238
iteration 282, loss = 0.0009508798248134553
iteration 283, loss = 0.0006980759208090603
iteration 284, loss = 0.000892253709025681
iteration 285, loss = 0.0022875003051012754
iteration 286, loss = 0.0008305492228828371
iteration 287, loss = 0.0007666880846954882
iteration 288, loss = 0.0010248766047880054
iteration 289, loss = 0.0007806476205587387
iteration 290, loss = 0.0007981695234775543
iteration 291, loss = 0.0008589151548221707
iteration 292, loss = 0.0008529347833245993
iteration 293, loss = 0.0008740018238313496
iteration 294, loss = 0.0008138625416904688
iteration 295, loss = 0.0008217365830205381
iteration 296, loss = 0.0008101221756078303
iteration 297, loss = 0.0007791062816977501
iteration 298, loss = 0.0011794108431786299
iteration 299, loss = 0.0012063542380928993
iteration 300, loss = 0.0008046734728850424
iteration 1, loss = 0.0008285675430670381
iteration 2, loss = 0.001237345626577735
iteration 3, loss = 0.0011140143033117056
iteration 4, loss = 0.0008152834489010274
iteration 5, loss = 0.0006979392492212355
iteration 6, loss = 0.001072240760549903
iteration 7, loss = 0.0009395506349392235
iteration 8, loss = 0.000851634715218097
iteration 9, loss = 0.0011666544014587998
iteration 10, loss = 0.0007648702012374997
iteration 11, loss = 0.0007851845002733171
iteration 12, loss = 0.0015234590973705053
iteration 13, loss = 0.0010035556042566895
iteration 14, loss = 0.001310286927036941
iteration 15, loss = 0.0009034823160618544
iteration 16, loss = 0.001004107529297471
iteration 17, loss = 0.0007291479269042611
iteration 18, loss = 0.0009090452222153544
iteration 19, loss = 0.0008408610010519624
iteration 20, loss = 0.0008147619664669037
iteration 21, loss = 0.0007345852791331708
iteration 22, loss = 0.0007882514037191868
iteration 23, loss = 0.0016592001775279641
iteration 24, loss = 0.0008062581182457507
iteration 25, loss = 0.0012802014825865626
iteration 26, loss = 0.0008726130472496152
iteration 27, loss = 0.0007642775890417397
iteration 28, loss = 0.0008589725475758314
iteration 29, loss = 0.0014901862014085054
iteration 30, loss = 0.0010097742779180408
iteration 31, loss = 0.001616526278667152
iteration 32, loss = 0.0007484215893782675
iteration 33, loss = 0.0012367010349407792
iteration 34, loss = 0.0010205869330093265
iteration 35, loss = 0.002233903855085373
iteration 36, loss = 0.0013760622823610902
iteration 37, loss = 0.0014756701420992613
iteration 38, loss = 0.0014333203434944153
iteration 39, loss = 0.0008711196715012193
iteration 40, loss = 0.001173086347989738
iteration 41, loss = 0.0007561963866464794
iteration 42, loss = 0.0009681766387075186
iteration 43, loss = 0.0010955864563584328
iteration 44, loss = 0.0007652710191905499
iteration 45, loss = 0.001138413674198091
iteration 46, loss = 0.0007465151138603687
iteration 47, loss = 0.000833928061183542
iteration 48, loss = 0.0009613075526431203
iteration 49, loss = 0.001306748017668724
iteration 50, loss = 0.000800782348960638
iteration 51, loss = 0.0009460276341997087
iteration 52, loss = 0.0010031777201220393
iteration 53, loss = 0.0008186956401914358
iteration 54, loss = 0.0008356428006663918
iteration 55, loss = 0.0007109555881470442
iteration 56, loss = 0.0009448626660741866
iteration 57, loss = 0.0008000389207154512
iteration 58, loss = 0.0007324351463466883
iteration 59, loss = 0.0007846484077163041
iteration 60, loss = 0.000772169092670083
iteration 61, loss = 0.0008457418298348784
iteration 62, loss = 0.0010523231467232108
iteration 63, loss = 0.0011537927202880383
iteration 64, loss = 0.0009368049213662744
iteration 65, loss = 0.0008036852814257145
iteration 66, loss = 0.0007939435890875757
iteration 67, loss = 0.0008732246933504939
iteration 68, loss = 0.0008507551392540336
iteration 69, loss = 0.0007554900366812944
iteration 70, loss = 0.0007869375986047089
iteration 71, loss = 0.0006710522575303912
iteration 72, loss = 0.0006666396511718631
iteration 73, loss = 0.0009068556828424335
iteration 74, loss = 0.0007839630707167089
iteration 75, loss = 0.0009847276378422976
iteration 76, loss = 0.0007818356389179826
iteration 77, loss = 0.0010774699039757252
iteration 78, loss = 0.0009569533867761493
iteration 79, loss = 0.0009357267990708351
iteration 80, loss = 0.0011147450422868133
iteration 81, loss = 0.0007632059860043228
iteration 82, loss = 0.0007113727042451501
iteration 83, loss = 0.0007437276071868837
iteration 84, loss = 0.0009905147599056363
iteration 85, loss = 0.0009545666980557144
iteration 86, loss = 0.0008370995055884123
iteration 87, loss = 0.0007572685717605054
iteration 88, loss = 0.0008236561552621424
iteration 89, loss = 0.0014471488539129496
iteration 90, loss = 0.0007524791872128844
iteration 91, loss = 0.0008686921210028231
iteration 92, loss = 0.0008280372712761164
iteration 93, loss = 0.0007164156413637102
iteration 94, loss = 0.0012252375017851591
iteration 95, loss = 0.0009979153983294964
iteration 96, loss = 0.0010073945159092546
iteration 97, loss = 0.0012080104788765311
iteration 98, loss = 0.0008987235487438738
iteration 99, loss = 0.0007575240451842546
iteration 100, loss = 0.0008204275509342551
iteration 101, loss = 0.0007765250047668815
iteration 102, loss = 0.0017381334910169244
iteration 103, loss = 0.0011059995740652084
iteration 104, loss = 0.0008542893338017166
iteration 105, loss = 0.0015052061062306166
iteration 106, loss = 0.00076822389382869
iteration 107, loss = 0.0008085248991847038
iteration 108, loss = 0.0007552094175480306
iteration 109, loss = 0.0006847807089798152
iteration 110, loss = 0.0009789313189685345
iteration 111, loss = 0.0008367158006876707
iteration 112, loss = 0.0014993571676313877
iteration 113, loss = 0.0011842724634334445
iteration 114, loss = 0.0011148402700200677
iteration 115, loss = 0.0010436174925416708
iteration 116, loss = 0.0011314654257148504
iteration 117, loss = 0.000811003556009382
iteration 118, loss = 0.0007365862838923931
iteration 119, loss = 0.0009251823066733778
iteration 120, loss = 0.0009618019685149193
iteration 121, loss = 0.0006411039503291249
iteration 122, loss = 0.001126231043599546
iteration 123, loss = 0.0016761000733822584
iteration 124, loss = 0.000844322144985199
iteration 125, loss = 0.0007218774990178645
iteration 126, loss = 0.0011055355425924063
iteration 127, loss = 0.0008908950840122998
iteration 128, loss = 0.0006550666294060647
iteration 129, loss = 0.0008713818388059735
iteration 130, loss = 0.0008724002982489765
iteration 131, loss = 0.0008541778661310673
iteration 132, loss = 0.0017468275036662817
iteration 133, loss = 0.0006496542482636869
iteration 134, loss = 0.000841800298076123
iteration 135, loss = 0.000806832336820662
iteration 136, loss = 0.0009295925847254694
iteration 137, loss = 0.0007725649047642946
iteration 138, loss = 0.0010324324248358607
iteration 139, loss = 0.0006710368325002491
iteration 140, loss = 0.0011136148823425174
iteration 141, loss = 0.0008519950788468122
iteration 142, loss = 0.001649942947551608
iteration 143, loss = 0.001629263861104846
iteration 144, loss = 0.001498316414654255
iteration 145, loss = 0.0011092736385762691
iteration 146, loss = 0.0007932218722999096
iteration 147, loss = 0.0008740765624679625
iteration 148, loss = 0.00066425243858248
iteration 149, loss = 0.0007236299570649862
iteration 150, loss = 0.000821388850454241
iteration 151, loss = 0.001781346625648439
iteration 152, loss = 0.0009303477127104998
iteration 153, loss = 0.000822089787106961
iteration 154, loss = 0.0011468016309663653
iteration 155, loss = 0.0008500267867930233
iteration 156, loss = 0.0009505694033578038
iteration 157, loss = 0.0015458272537216544
iteration 158, loss = 0.0015179796610027552
iteration 159, loss = 0.0007526797708123922
iteration 160, loss = 0.0007444341317750514
iteration 161, loss = 0.0007782400934956968
iteration 162, loss = 0.0009239251958206296
iteration 163, loss = 0.0006788434693589807
iteration 164, loss = 0.001509831054136157
iteration 165, loss = 0.0006969994865357876
iteration 166, loss = 0.0007466073730029166
iteration 167, loss = 0.0006510005332529545
iteration 168, loss = 0.0008982835570350289
iteration 169, loss = 0.0010064349044114351
iteration 170, loss = 0.0006712463218718767
iteration 171, loss = 0.0008525470038875937
iteration 172, loss = 0.000969821005128324
iteration 173, loss = 0.0010067428229376674
iteration 174, loss = 0.0008935554069466889
iteration 175, loss = 0.0007453460711985826
iteration 176, loss = 0.0007799301529303193
iteration 177, loss = 0.000877842481713742
iteration 178, loss = 0.0006137496675364673
iteration 179, loss = 0.0007653865031898022
iteration 180, loss = 0.001525079132989049
iteration 181, loss = 0.0008121503051370382
iteration 182, loss = 0.0009090030216611922
iteration 183, loss = 0.0008039282402023673
iteration 184, loss = 0.0007569057634100318
iteration 185, loss = 0.0007434486760757864
iteration 186, loss = 0.0009288931032642722
iteration 187, loss = 0.0009004874154925346
iteration 188, loss = 0.001520599122159183
iteration 189, loss = 0.0009122276096604764
iteration 190, loss = 0.0016928313998505473
iteration 191, loss = 0.0007525052642449737
iteration 192, loss = 0.0009735290077514946
iteration 193, loss = 0.0006965788779780269
iteration 194, loss = 0.000670642068143934
iteration 195, loss = 0.0006996229058131576
iteration 196, loss = 0.0008858254877850413
iteration 197, loss = 0.0008644010522402823
iteration 198, loss = 0.000945854582823813
iteration 199, loss = 0.0008518953109160066
iteration 200, loss = 0.0008280549081973732
iteration 201, loss = 0.0008488716557621956
iteration 202, loss = 0.0008210329688154161
iteration 203, loss = 0.0007477374165318906
iteration 204, loss = 0.0007010262925177813
iteration 205, loss = 0.0007722292793914676
iteration 206, loss = 0.0013321038568392396
iteration 207, loss = 0.0008980592829175293
iteration 208, loss = 0.001450871117413044
iteration 209, loss = 0.0008953555952757597
iteration 210, loss = 0.0008339178748428822
iteration 211, loss = 0.0009844579035416245
iteration 212, loss = 0.0008026451105251908
iteration 213, loss = 0.0007137901266105473
iteration 214, loss = 0.000723973207641393
iteration 215, loss = 0.000837639148812741
iteration 216, loss = 0.0007972320890985429
iteration 217, loss = 0.0009212503209710121
iteration 218, loss = 0.0008915979415178299
iteration 219, loss = 0.001568180974572897
iteration 220, loss = 0.0009829298360273242
iteration 221, loss = 0.0008213496766984463
iteration 222, loss = 0.0007783881155773997
iteration 223, loss = 0.0008056584629230201
iteration 224, loss = 0.0008130295318551362
iteration 225, loss = 0.0008377786725759506
iteration 226, loss = 0.0008585341856814921
iteration 227, loss = 0.0007738779531791806
iteration 228, loss = 0.0011429708683863282
iteration 229, loss = 0.0008762740762904286
iteration 230, loss = 0.0008111855131573975
iteration 231, loss = 0.0007075543981045485
iteration 232, loss = 0.0006583883077837527
iteration 233, loss = 0.0007633046479895711
iteration 234, loss = 0.0007616938091814518
iteration 235, loss = 0.0008294882718473673
iteration 236, loss = 0.0008292788988910615
iteration 237, loss = 0.0009183719521388412
iteration 238, loss = 0.0007873150752857327
iteration 239, loss = 0.001287446590140462
iteration 240, loss = 0.0008170415530912578
iteration 241, loss = 0.0010169614106416702
iteration 242, loss = 0.0008355813333764672
iteration 243, loss = 0.0018959309672936797
iteration 244, loss = 0.0007054713205434382
iteration 245, loss = 0.0007523660897277296
iteration 246, loss = 0.0013043276267126203
iteration 247, loss = 0.0010086925467476249
iteration 248, loss = 0.0008256916189566255
iteration 249, loss = 0.0007012527785263956
iteration 250, loss = 0.001081347349099815
iteration 251, loss = 0.0013951454311609268
iteration 252, loss = 0.0008786776452325284
iteration 253, loss = 0.0007722446462139487
iteration 254, loss = 0.0009193582227453589
iteration 255, loss = 0.0011702614137902856
iteration 256, loss = 0.0009210548596456647
iteration 257, loss = 0.0007848505629226565
iteration 258, loss = 0.0008523459546267986
iteration 259, loss = 0.0008524560835212469
iteration 260, loss = 0.001154446043074131
iteration 261, loss = 0.0008228069636970758
iteration 262, loss = 0.0008757308823987842
iteration 263, loss = 0.0012277818750590086
iteration 264, loss = 0.0007519560749642551
iteration 265, loss = 0.0009263242245651782
iteration 266, loss = 0.0007725415634922683
iteration 267, loss = 0.0007628455059602857
iteration 268, loss = 0.0008456318755634129
iteration 269, loss = 0.0014432778116315603
iteration 270, loss = 0.000666229403577745
iteration 271, loss = 0.0005913876229897141
iteration 272, loss = 0.0008069228497333825
iteration 273, loss = 0.0014072650810703635
iteration 274, loss = 0.001552750007249415
iteration 275, loss = 0.0007395471329800785
iteration 276, loss = 0.0015012164367362857
iteration 277, loss = 0.0010744642931967974
iteration 278, loss = 0.0012518847361207008
iteration 279, loss = 0.0010650143958628178
iteration 280, loss = 0.0007834599819034338
iteration 281, loss = 0.0007246260065585375
iteration 282, loss = 0.0008431805181317031
iteration 283, loss = 0.0016903607174754143
iteration 284, loss = 0.0008348679984919727
iteration 285, loss = 0.0008129389607347548
iteration 286, loss = 0.0009503031033091247
iteration 287, loss = 0.0011018801014870405
iteration 288, loss = 0.0008669827948324382
iteration 289, loss = 0.0007558605866506696
iteration 290, loss = 0.0009197566541843116
iteration 291, loss = 0.0007039061165414751
iteration 292, loss = 0.0006973313284106553
iteration 293, loss = 0.0015517689753323793
iteration 294, loss = 0.000909490161575377
iteration 295, loss = 0.000916477176360786
iteration 296, loss = 0.0009417922701686621
iteration 297, loss = 0.0009407486068084836
iteration 298, loss = 0.0009853484807536006
iteration 299, loss = 0.0008260291651822627
iteration 300, loss = 0.0007484027300961316
iteration 1, loss = 0.0009746741270646453
iteration 2, loss = 0.0008317361352965236
iteration 3, loss = 0.0008934920188039541
iteration 4, loss = 0.0008713824208825827
iteration 5, loss = 0.0009453610982745886
iteration 6, loss = 0.0012797701638191938
iteration 7, loss = 0.0010194402420893312
iteration 8, loss = 0.0015622187638655305
iteration 9, loss = 0.000647765991743654
iteration 10, loss = 0.0007376898429356515
iteration 11, loss = 0.0009129306999966502
iteration 12, loss = 0.0006773319910280406
iteration 13, loss = 0.0007458935142494738
iteration 14, loss = 0.0007203762652352452
iteration 15, loss = 0.0015576366567984223
iteration 16, loss = 0.0012566967634484172
iteration 17, loss = 0.0009087597136385739
iteration 18, loss = 0.0007405005744658411
iteration 19, loss = 0.0008169601787813008
iteration 20, loss = 0.0011546987807378173
iteration 21, loss = 0.0008053859928622842
iteration 22, loss = 0.000782749499194324
iteration 23, loss = 0.0007833701674826443
iteration 24, loss = 0.0008321459754370153
iteration 25, loss = 0.0007203005952760577
iteration 26, loss = 0.0007054511224851012
iteration 27, loss = 0.0007809428498148918
iteration 28, loss = 0.0010635203216224909
iteration 29, loss = 0.0013937370385974646
iteration 30, loss = 0.0009523035841993988
iteration 31, loss = 0.0008648705552332103
iteration 32, loss = 0.0006885258480906487
iteration 33, loss = 0.000971432076767087
iteration 34, loss = 0.0009457373525947332
iteration 35, loss = 0.0008050855831243098
iteration 36, loss = 0.0007963228854350746
iteration 37, loss = 0.0008015597122721374
iteration 38, loss = 0.0012373231584206223
iteration 39, loss = 0.0012826024321839213
iteration 40, loss = 0.0010290095815435052
iteration 41, loss = 0.0013770496007055044
iteration 42, loss = 0.000760342925786972
iteration 43, loss = 0.0008058619569055736
iteration 44, loss = 0.0007845294312573969
iteration 45, loss = 0.0007701856084167957
iteration 46, loss = 0.0007285214960575104
iteration 47, loss = 0.0007599060772918165
iteration 48, loss = 0.0007620203541591763
iteration 49, loss = 0.00145000871270895
iteration 50, loss = 0.0008706226362846792
iteration 51, loss = 0.0008499243413098156
iteration 52, loss = 0.0010289519559592009
iteration 53, loss = 0.0006696960190311074
iteration 54, loss = 0.0008476126240566373
iteration 55, loss = 0.0009242526721209288
iteration 56, loss = 0.0009461334557272494
iteration 57, loss = 0.0011457905638962984
iteration 58, loss = 0.0011598649434745312
iteration 59, loss = 0.0015049157664179802
iteration 60, loss = 0.0012466306798160076
iteration 61, loss = 0.0009546045330353081
iteration 62, loss = 0.0008323572692461312
iteration 63, loss = 0.000823701499029994
iteration 64, loss = 0.0008081370033323765
iteration 65, loss = 0.001050911727361381
iteration 66, loss = 0.0009427509503439069
iteration 67, loss = 0.0008991744252853096
iteration 68, loss = 0.0010429840767756104
iteration 69, loss = 0.0008231987594626844
iteration 70, loss = 0.0010900936322286725
iteration 71, loss = 0.0007499890634790063
iteration 72, loss = 0.000835374987218529
iteration 73, loss = 0.000796570791862905
iteration 74, loss = 0.0008413586765527725
iteration 75, loss = 0.0012325942516326904
iteration 76, loss = 0.0010881047928705812
iteration 77, loss = 0.001095968415029347
iteration 78, loss = 0.0008404256659559906
iteration 79, loss = 0.0008661707397550344
iteration 80, loss = 0.0008203540346585214
iteration 81, loss = 0.0008182764868251979
iteration 82, loss = 0.0010315213585272431
iteration 83, loss = 0.0008474463247694075
iteration 84, loss = 0.0012862237635999918
iteration 85, loss = 0.0011206892086192966
iteration 86, loss = 0.0008070347830653191
iteration 87, loss = 0.0008468126179650426
iteration 88, loss = 0.0007214855868369341
iteration 89, loss = 0.0015519262524321675
iteration 90, loss = 0.0009189625270664692
iteration 91, loss = 0.0009804610162973404
iteration 92, loss = 0.0012487495550885797
iteration 93, loss = 0.0010099371429532766
iteration 94, loss = 0.0011930891778320074
iteration 95, loss = 0.0006435359246097505
iteration 96, loss = 0.0006227704579941928
iteration 97, loss = 0.0008508976898156106
iteration 98, loss = 0.0009366004960611463
iteration 99, loss = 0.0008875492494553328
iteration 100, loss = 0.0008415536722168326
iteration 101, loss = 0.0008561364957131445
iteration 102, loss = 0.0008353178855031729
iteration 103, loss = 0.0008111752104014158
iteration 104, loss = 0.0008415340562351048
iteration 105, loss = 0.0011526080779731274
iteration 106, loss = 0.0012923983158543706
iteration 107, loss = 0.0015249063726514578
iteration 108, loss = 0.0006520914612337947
iteration 109, loss = 0.0008073705248534679
iteration 110, loss = 0.0009463850292377174
iteration 111, loss = 0.0011296175653114915
iteration 112, loss = 0.0008248615195043385
iteration 113, loss = 0.001563686178997159
iteration 114, loss = 0.0008479059324599802
iteration 115, loss = 0.0011454287450760603
iteration 116, loss = 0.001171695883385837
iteration 117, loss = 0.001962025184184313
iteration 118, loss = 0.0006727202562615275
iteration 119, loss = 0.0007886741659604013
iteration 120, loss = 0.0006963490159250796
iteration 121, loss = 0.0008039658423513174
iteration 122, loss = 0.0013966794358566403
iteration 123, loss = 0.0008600438013672829
iteration 124, loss = 0.0008528553880751133
iteration 125, loss = 0.0007342505268752575
iteration 126, loss = 0.0009937152499333024
iteration 127, loss = 0.0008955041994340718
iteration 128, loss = 0.0008152351365424693
iteration 129, loss = 0.0010055017191916704
iteration 130, loss = 0.0011448180302977562
iteration 131, loss = 0.0009564458159729838
iteration 132, loss = 0.0007897695759311318
iteration 133, loss = 0.0007102283998392522
iteration 134, loss = 0.0007232263451442122
iteration 135, loss = 0.0011521321721374989
iteration 136, loss = 0.0007848594104871154
iteration 137, loss = 0.0009286762215197086
iteration 138, loss = 0.0007823801133781672
iteration 139, loss = 0.0012097166618332267
iteration 140, loss = 0.001411483739502728
iteration 141, loss = 0.0010238662362098694
iteration 142, loss = 0.0008762187790125608
iteration 143, loss = 0.0015132399275898933
iteration 144, loss = 0.001343944575637579
iteration 145, loss = 0.0008556128013879061
iteration 146, loss = 0.0008522567222826183
iteration 147, loss = 0.0007829945534467697
iteration 148, loss = 0.0008010883466340601
iteration 149, loss = 0.0017400251235812902
iteration 150, loss = 0.0009034600807353854
iteration 151, loss = 0.0009195395396091044
iteration 152, loss = 0.0009070214000530541
iteration 153, loss = 0.0010450578993186355
iteration 154, loss = 0.0007080623763613403
iteration 155, loss = 0.0009324728744104505
iteration 156, loss = 0.0007732330122962594
iteration 157, loss = 0.0010490519925951958
iteration 158, loss = 0.000827242445666343
iteration 159, loss = 0.0011583856539800763
iteration 160, loss = 0.0006753962952643633
iteration 161, loss = 0.0008359569474123418
iteration 162, loss = 0.0007790821837261319
iteration 163, loss = 0.0010652479249984026
iteration 164, loss = 0.000818101572804153
iteration 165, loss = 0.0011420159135013819
iteration 166, loss = 0.0007395545835606754
iteration 167, loss = 0.0011255597928538918
iteration 168, loss = 0.0010580753441900015
iteration 169, loss = 0.0008332539582625031
iteration 170, loss = 0.0016035186126828194
iteration 171, loss = 0.0014345085946843028
iteration 172, loss = 0.0017563006840646267
iteration 173, loss = 0.0007611414766870439
iteration 174, loss = 0.0008832060266286135
iteration 175, loss = 0.0008262813789770007
iteration 176, loss = 0.0006260104128159583
iteration 177, loss = 0.0007811386603862047
iteration 178, loss = 0.0006825567688792944
iteration 179, loss = 0.0008746164385229349
iteration 180, loss = 0.0007721458096057177
iteration 181, loss = 0.000918672070838511
iteration 182, loss = 0.000832456920761615
iteration 183, loss = 0.000746874778997153
iteration 184, loss = 0.0008015775820240378
iteration 185, loss = 0.0007097735651768744
iteration 186, loss = 0.0008503522258251905
iteration 187, loss = 0.000936924247071147
iteration 188, loss = 0.000985621358267963
iteration 189, loss = 0.0006174517329782248
iteration 190, loss = 0.0010888947872444987
iteration 191, loss = 0.0013793130638077855
iteration 192, loss = 0.0009248623391613364
iteration 193, loss = 0.0010237491223961115
iteration 194, loss = 0.0013904816005378962
iteration 195, loss = 0.0014340105699375272
iteration 196, loss = 0.0012018752750009298
iteration 197, loss = 0.0015186112141236663
iteration 198, loss = 0.0009034211980178952
iteration 199, loss = 0.0008999028359539807
iteration 200, loss = 0.0007650013430975378
iteration 201, loss = 0.0015348754823207855
iteration 202, loss = 0.001152968849055469
iteration 203, loss = 0.0011196652194485068
iteration 204, loss = 0.0006968838861212134
iteration 205, loss = 0.0011849487200379372
iteration 206, loss = 0.0007451896090060472
iteration 207, loss = 0.0012008806224912405
iteration 208, loss = 0.0009382666321471334
iteration 209, loss = 0.0007478113984689116
iteration 210, loss = 0.0015069962246343493
iteration 211, loss = 0.0009246380068361759
iteration 212, loss = 0.0007197697414085269
iteration 213, loss = 0.0013211587211117148
iteration 214, loss = 0.0008802769589237869
iteration 215, loss = 0.0006046179914847016
iteration 216, loss = 0.0007891681743785739
iteration 217, loss = 0.0008809925056993961
iteration 218, loss = 0.0016486799577251077
iteration 219, loss = 0.0015230010030791163
iteration 220, loss = 0.000863222754560411
iteration 221, loss = 0.0007725073955953121
iteration 222, loss = 0.001003710669465363
iteration 223, loss = 0.0008653925033286214
iteration 224, loss = 0.0008010840392671525
iteration 225, loss = 0.0010620631510391831
iteration 226, loss = 0.0007932931766845286
iteration 227, loss = 0.0006150282570160925
iteration 228, loss = 0.0006708614528179169
iteration 229, loss = 0.0007103164098225534
iteration 230, loss = 0.001020136522129178
iteration 231, loss = 0.0013788539217785
iteration 232, loss = 0.0010799256851896644
iteration 233, loss = 0.0012739119119942188
iteration 234, loss = 0.0006782826385460794
iteration 235, loss = 0.0007953912136144936
iteration 236, loss = 0.0009437116095796227
iteration 237, loss = 0.0009866032050922513
iteration 238, loss = 0.0019208511803299189
iteration 239, loss = 0.0010072853183373809
iteration 240, loss = 0.0008137400145642459
iteration 241, loss = 0.0009761583060026169
iteration 242, loss = 0.0009351462940685451
iteration 243, loss = 0.0007435490260832012
iteration 244, loss = 0.0010716929100453854
iteration 245, loss = 0.0006482928292825818
iteration 246, loss = 0.0006300605600699782
iteration 247, loss = 0.000763994175940752
iteration 248, loss = 0.0008465197752229869
iteration 249, loss = 0.0007090230355970562
iteration 250, loss = 0.0008833924657665193
iteration 251, loss = 0.0006823165458627045
iteration 252, loss = 0.0011912196641787887
iteration 253, loss = 0.0008663799962960184
iteration 254, loss = 0.0010178093798458576
iteration 255, loss = 0.0006900936714373529
iteration 256, loss = 0.0008924392168410122
iteration 257, loss = 0.000947357330005616
iteration 258, loss = 0.0009205856476910412
iteration 259, loss = 0.0007098201313056052
iteration 260, loss = 0.0007232455536723137
iteration 261, loss = 0.0007329809013754129
iteration 262, loss = 0.0008115462842397392
iteration 263, loss = 0.0008628643117845058
iteration 264, loss = 0.0008912446792237461
iteration 265, loss = 0.0006722835823893547
iteration 266, loss = 0.0007186525617726147
iteration 267, loss = 0.001627799472771585
iteration 268, loss = 0.0009551208931952715
iteration 269, loss = 0.0009668241254985332
iteration 270, loss = 0.0009064020123332739
iteration 271, loss = 0.0009762506233528256
iteration 272, loss = 0.0007328944047912955
iteration 273, loss = 0.000786036136560142
iteration 274, loss = 0.0008805060060694814
iteration 275, loss = 0.0008501553675159812
iteration 276, loss = 0.0010099309729412198
iteration 277, loss = 0.0008342647342942655
iteration 278, loss = 0.0007277323747985065
iteration 279, loss = 0.0008601723238825798
iteration 280, loss = 0.0014355939347296953
iteration 281, loss = 0.0010104298125952482
iteration 282, loss = 0.0015779308741912246
iteration 283, loss = 0.000778206333052367
iteration 284, loss = 0.0009501411695964634
iteration 285, loss = 0.0006102373008616269
iteration 286, loss = 0.0008120476268231869
iteration 287, loss = 0.0007727380725555122
iteration 288, loss = 0.0007413707207888365
iteration 289, loss = 0.0014492522459477186
iteration 290, loss = 0.0011574184754863381
iteration 291, loss = 0.001039460999891162
iteration 292, loss = 0.0007927018450573087
iteration 293, loss = 0.0008736505988053977
iteration 294, loss = 0.0008784444071352482
iteration 295, loss = 0.0007076344918459654
iteration 296, loss = 0.0008412748575210571
iteration 297, loss = 0.0008999591809697449
iteration 298, loss = 0.0007652866770513356
iteration 299, loss = 0.0007764474721625447
iteration 300, loss = 0.0007722618756815791
iteration 1, loss = 0.0008801239309832454
iteration 2, loss = 0.0007025221129879355
iteration 3, loss = 0.0014962261775508523
iteration 4, loss = 0.001583607867360115
iteration 5, loss = 0.0009133017738349736
iteration 6, loss = 0.0009687429992482066
iteration 7, loss = 0.0007915677269920707
iteration 8, loss = 0.0009834510274231434
iteration 9, loss = 0.0007538796635344625
iteration 10, loss = 0.000751478539314121
iteration 11, loss = 0.0008390946313738823
iteration 12, loss = 0.0008384788525290787
iteration 13, loss = 0.0008892868645489216
iteration 14, loss = 0.0009293462499044836
iteration 15, loss = 0.0008972575888037682
iteration 16, loss = 0.0015601329505443573
iteration 17, loss = 0.0008817173074930906
iteration 18, loss = 0.0007656035595573485
iteration 19, loss = 0.0007732452941127121
iteration 20, loss = 0.0008509701583534479
iteration 21, loss = 0.001637786626815796
iteration 22, loss = 0.0007167994044721127
iteration 23, loss = 0.0012808130122721195
iteration 24, loss = 0.0006803919677622616
iteration 25, loss = 0.0008210767991840839
iteration 26, loss = 0.0007956915069371462
iteration 27, loss = 0.0008750429260544479
iteration 28, loss = 0.0015042955055832863
iteration 29, loss = 0.0010271694045513868
iteration 30, loss = 0.0010343060130253434
iteration 31, loss = 0.0007151677273213863
iteration 32, loss = 0.000787195807788521
iteration 33, loss = 0.0007626379374414682
iteration 34, loss = 0.001089553814381361
iteration 35, loss = 0.0007073135930113494
iteration 36, loss = 0.0008930314215831459
iteration 37, loss = 0.0009652802255004644
iteration 38, loss = 0.0009941828902810812
iteration 39, loss = 0.0007986458949744701
iteration 40, loss = 0.000972204958088696
iteration 41, loss = 0.0007285857573151588
iteration 42, loss = 0.001505894586443901
iteration 43, loss = 0.0008137936238199472
iteration 44, loss = 0.0007309129578061402
iteration 45, loss = 0.0007475998718291521
iteration 46, loss = 0.000825964380055666
iteration 47, loss = 0.0008852887549437582
iteration 48, loss = 0.0008180928998626769
iteration 49, loss = 0.0009336106013506651
iteration 50, loss = 0.0007064714445732534
iteration 51, loss = 0.0007901890203356743
iteration 52, loss = 0.001480311737395823
iteration 53, loss = 0.00070411671185866
iteration 54, loss = 0.001522464444860816
iteration 55, loss = 0.0007488008704967797
iteration 56, loss = 0.0007865703664720058
iteration 57, loss = 0.0010316786356270313
iteration 58, loss = 0.0009085046476684511
iteration 59, loss = 0.0009025702602230012
iteration 60, loss = 0.0010379308369010687
iteration 61, loss = 0.0014024506090208888
iteration 62, loss = 0.0015268534189090133
iteration 63, loss = 0.0009504485060460865
iteration 64, loss = 0.000698128598742187
iteration 65, loss = 0.0012069843942299485
iteration 66, loss = 0.0005877236253581941
iteration 67, loss = 0.0006566191441379488
iteration 68, loss = 0.0009064704645425081
iteration 69, loss = 0.0006373969954438508
iteration 70, loss = 0.0007893427973613143
iteration 71, loss = 0.0008689489914104342
iteration 72, loss = 0.0007465858361683786
iteration 73, loss = 0.0011372471926733851
iteration 74, loss = 0.0009200366912409663
iteration 75, loss = 0.000927615212276578
iteration 76, loss = 0.0008694446878507733
iteration 77, loss = 0.0012575261062011123
iteration 78, loss = 0.000626256107352674
iteration 79, loss = 0.0009222450316883624
iteration 80, loss = 0.000751431449316442
iteration 81, loss = 0.0008753437432460487
iteration 82, loss = 0.0007297315751202404
iteration 83, loss = 0.0006228039273992181
iteration 84, loss = 0.0010134822223335505
iteration 85, loss = 0.0008318218169733882
iteration 86, loss = 0.0009297810611315072
iteration 87, loss = 0.001064895885065198
iteration 88, loss = 0.000718104827683419
iteration 89, loss = 0.0008749099797569215
iteration 90, loss = 0.0009517270373180509
iteration 91, loss = 0.00108981947414577
iteration 92, loss = 0.0007771586533635855
iteration 93, loss = 0.0013488191179931164
iteration 94, loss = 0.0011893470073118806
iteration 95, loss = 0.0009424404124729335
iteration 96, loss = 0.0009015483665280044
iteration 97, loss = 0.0009211301803588867
iteration 98, loss = 0.0008999728597700596
iteration 99, loss = 0.0007414147257804871
iteration 100, loss = 0.0013543126406148076
iteration 101, loss = 0.0008311022538691759
iteration 102, loss = 0.0011006362037733197
iteration 103, loss = 0.0009508669027127326
iteration 104, loss = 0.0007183786947280169
iteration 105, loss = 0.0009235290344804525
iteration 106, loss = 0.0012760108802467585
iteration 107, loss = 0.000656085554510355
iteration 108, loss = 0.0009054877446033061
iteration 109, loss = 0.0008504496072418988
iteration 110, loss = 0.0007863553473725915
iteration 111, loss = 0.0010646505979821086
iteration 112, loss = 0.0009531319374218583
iteration 113, loss = 0.0014027819270268083
iteration 114, loss = 0.0008037182269617915
iteration 115, loss = 0.0007457656320184469
iteration 116, loss = 0.0015503307804465294
iteration 117, loss = 0.0007336486596614122
iteration 118, loss = 0.0017525249859318137
iteration 119, loss = 0.0008595019462518394
iteration 120, loss = 0.0013955460162833333
iteration 121, loss = 0.0008696026052348316
iteration 122, loss = 0.0008746658568270504
iteration 123, loss = 0.001483453786931932
iteration 124, loss = 0.0008451006724499166
iteration 125, loss = 0.0010669452603906393
iteration 126, loss = 0.0010113600874319673
iteration 127, loss = 0.0009899192955344915
iteration 128, loss = 0.0010092314332723618
iteration 129, loss = 0.0007650920888409019
iteration 130, loss = 0.0008200271404348314
iteration 131, loss = 0.0007625773432664573
iteration 132, loss = 0.0010058763436973095
iteration 133, loss = 0.0007929803105071187
iteration 134, loss = 0.0009979910682886839
iteration 135, loss = 0.0009394275257363915
iteration 136, loss = 0.0008039462263695896
iteration 137, loss = 0.0007340422598645091
iteration 138, loss = 0.0011370129650458694
iteration 139, loss = 0.0008777687326073647
iteration 140, loss = 0.0008460027165710926
iteration 141, loss = 0.0011186670744791627
iteration 142, loss = 0.0007694390951655805
iteration 143, loss = 0.00099461292847991
iteration 144, loss = 0.001287679304368794
iteration 145, loss = 0.0009302834514528513
iteration 146, loss = 0.000628130161203444
iteration 147, loss = 0.0007345426711253822
iteration 148, loss = 0.0016255928203463554
iteration 149, loss = 0.0006603663205169141
iteration 150, loss = 0.0006479590083472431
iteration 151, loss = 0.0008401303784921765
iteration 152, loss = 0.0013013504212722182
iteration 153, loss = 0.001572494744323194
iteration 154, loss = 0.0009581177728250623
iteration 155, loss = 0.0008131403010338545
iteration 156, loss = 0.00083518261089921
iteration 157, loss = 0.0008760362397879362
iteration 158, loss = 0.000770555401686579
iteration 159, loss = 0.0007557505159638822
iteration 160, loss = 0.0008351270807906985
iteration 161, loss = 0.0008088233880698681
iteration 162, loss = 0.0013582961400970817
iteration 163, loss = 0.0010968492133542895
iteration 164, loss = 0.0006477386923506856
iteration 165, loss = 0.0010002200724557042
iteration 166, loss = 0.0011174215469509363
iteration 167, loss = 0.0007015111623331904
iteration 168, loss = 0.0008046053117141128
iteration 169, loss = 0.0006840216228738427
iteration 170, loss = 0.001202937914058566
iteration 171, loss = 0.0014985169982537627
iteration 172, loss = 0.0007594629423692822
iteration 173, loss = 0.0007658363319933414
iteration 174, loss = 0.0015205545350909233
iteration 175, loss = 0.0008866160060279071
iteration 176, loss = 0.0008700833423063159
iteration 177, loss = 0.0007957919151522219
iteration 178, loss = 0.0011285919463261962
iteration 179, loss = 0.0008097229874692857
iteration 180, loss = 0.0006829714402556419
iteration 181, loss = 0.0010876135202124715
iteration 182, loss = 0.0007586423889733851
iteration 183, loss = 0.0009173809085041285
iteration 184, loss = 0.0010175973875448108
iteration 185, loss = 0.0007329239160753787
iteration 186, loss = 0.0007320244912989438
iteration 187, loss = 0.0014776875032112002
iteration 188, loss = 0.0008263111812993884
iteration 189, loss = 0.0009320847457274795
iteration 190, loss = 0.0007365386700257659
iteration 191, loss = 0.0010760390432551503
iteration 192, loss = 0.000736531219445169
iteration 193, loss = 0.0009701810195110738
iteration 194, loss = 0.0009559591999277472
iteration 195, loss = 0.0006749792955815792
iteration 196, loss = 0.0008543797302991152
iteration 197, loss = 0.0008456059149466455
iteration 198, loss = 0.0018219888443127275
iteration 199, loss = 0.0008330001146532595
iteration 200, loss = 0.0009361153934150934
iteration 201, loss = 0.0007483718800358474
iteration 202, loss = 0.0008260328322649002
iteration 203, loss = 0.0007943535456433892
iteration 204, loss = 0.0008814040920697153
iteration 205, loss = 0.0010403580963611603
iteration 206, loss = 0.0010974790202453732
iteration 207, loss = 0.0007271202630363405
iteration 208, loss = 0.0010849602986127138
iteration 209, loss = 0.0008878864464350045
iteration 210, loss = 0.001017857575789094
iteration 211, loss = 0.0007685312302783132
iteration 212, loss = 0.0007951132720336318
iteration 213, loss = 0.0007002638303674757
iteration 214, loss = 0.0008930186158977449
iteration 215, loss = 0.000715368427336216
iteration 216, loss = 0.0008100675768218935
iteration 217, loss = 0.0010548398131504655
iteration 218, loss = 0.0009579089237377048
iteration 219, loss = 0.000961350160650909
iteration 220, loss = 0.0007175231003202498
iteration 221, loss = 0.0009104605414904654
iteration 222, loss = 0.0014492846094071865
iteration 223, loss = 0.0009800454135984182
iteration 224, loss = 0.0007475496386177838
iteration 225, loss = 0.0007663021679036319
iteration 226, loss = 0.0008277942542918026
iteration 227, loss = 0.0008493430796079338
iteration 228, loss = 0.0016489187255501747
iteration 229, loss = 0.0008280083420686424
iteration 230, loss = 0.0015262806555256248
iteration 231, loss = 0.0006968203815631568
iteration 232, loss = 0.0007191808545030653
iteration 233, loss = 0.0007699966081418097
iteration 234, loss = 0.0009661783115006983
iteration 235, loss = 0.0008250594837591052
iteration 236, loss = 0.000906104629393667
iteration 237, loss = 0.0007208685856312513
iteration 238, loss = 0.0007041037315502763
iteration 239, loss = 0.000876304809935391
iteration 240, loss = 0.001138131832703948
iteration 241, loss = 0.0014040322275832295
iteration 242, loss = 0.0007913445006124675
iteration 243, loss = 0.00192998256534338
iteration 244, loss = 0.001234244555234909
iteration 245, loss = 0.001794435316696763
iteration 246, loss = 0.0006832684157416224
iteration 247, loss = 0.0007275722455233335
iteration 248, loss = 0.0015115635469555855
iteration 249, loss = 0.0013151632156223059
iteration 250, loss = 0.0007180479005910456
iteration 251, loss = 0.0008715258445590734
iteration 252, loss = 0.0015451585641130805
iteration 253, loss = 0.001368790864944458
iteration 254, loss = 0.0007982571260072291
iteration 255, loss = 0.0009545927750878036
iteration 256, loss = 0.0008802806842140853
iteration 257, loss = 0.0007718405686318874
iteration 258, loss = 0.0008242746698670089
iteration 259, loss = 0.0009188166004605591
iteration 260, loss = 0.0007679456612095237
iteration 261, loss = 0.0007475604652427137
iteration 262, loss = 0.0010385655332356691
iteration 263, loss = 0.0008264294592663646
iteration 264, loss = 0.0010060303611680865
iteration 265, loss = 0.0008644748595543206
iteration 266, loss = 0.0009786552982404828
iteration 267, loss = 0.001377043197862804
iteration 268, loss = 0.0009098975569941103
iteration 269, loss = 0.0006907649803906679
iteration 270, loss = 0.0011662807082757354
iteration 271, loss = 0.0010290015488862991
iteration 272, loss = 0.0007866453379392624
iteration 273, loss = 0.0008654244593344629
iteration 274, loss = 0.000991434557363391
iteration 275, loss = 0.0008605296025052667
iteration 276, loss = 0.0008630295633338392
iteration 277, loss = 0.0015787703450769186
iteration 278, loss = 0.000810128403827548
iteration 279, loss = 0.0011092127533629537
iteration 280, loss = 0.0009232793236151338
iteration 281, loss = 0.0007927726837806404
iteration 282, loss = 0.0011456748470664024
iteration 283, loss = 0.0006162885692901909
iteration 284, loss = 0.0008310689590871334
iteration 285, loss = 0.0008763684891164303
iteration 286, loss = 0.0011097098467871547
iteration 287, loss = 0.0008891267352737486
iteration 288, loss = 0.0012122513726353645
iteration 289, loss = 0.0008535666856914759
iteration 290, loss = 0.0007768881041556597
iteration 291, loss = 0.0007067566621117294
iteration 292, loss = 0.0010452885180711746
iteration 293, loss = 0.0008822577074170113
iteration 294, loss = 0.0011754273436963558
iteration 295, loss = 0.001011196873150766
iteration 296, loss = 0.0009638597839511931
iteration 297, loss = 0.0007939145434647799
iteration 298, loss = 0.0010150638408958912
iteration 299, loss = 0.0015212357975542545
iteration 300, loss = 0.0007280686404556036
iteration 1, loss = 0.0009335578652098775
iteration 2, loss = 0.0008717247401364148
iteration 3, loss = 0.0007663372671231627
iteration 4, loss = 0.0009991124970838428
iteration 5, loss = 0.00088939891429618
iteration 6, loss = 0.0013341627782210708
iteration 7, loss = 0.000792801845818758
iteration 8, loss = 0.0007758639403618872
iteration 9, loss = 0.0007831882685422897
iteration 10, loss = 0.0010080860229209065
iteration 11, loss = 0.0007372788968496025
iteration 12, loss = 0.0010628140298649669
iteration 13, loss = 0.000989638501778245
iteration 14, loss = 0.0009068347280845046
iteration 15, loss = 0.0007511323783546686
iteration 16, loss = 0.0008049834868870676
iteration 17, loss = 0.0008747325045987964
iteration 18, loss = 0.0007452048012055457
iteration 19, loss = 0.0014605510514229536
iteration 20, loss = 0.0007430227124132216
iteration 21, loss = 0.0008094048826023936
iteration 22, loss = 0.0009175018640235066
iteration 23, loss = 0.0005495305522345006
iteration 24, loss = 0.0007519526989199221
iteration 25, loss = 0.0008372677257284522
iteration 26, loss = 0.0008803068776614964
iteration 27, loss = 0.0006972872652113438
iteration 28, loss = 0.0008616015547886491
iteration 29, loss = 0.0007805286440998316
iteration 30, loss = 0.0008878399967215955
iteration 31, loss = 0.0007798001752234995
iteration 32, loss = 0.0010241662384942174
iteration 33, loss = 0.0008752187713980675
iteration 34, loss = 0.0012109432136639953
iteration 35, loss = 0.0008436478674411774
iteration 36, loss = 0.001105872797779739
iteration 37, loss = 0.00104803079739213
iteration 38, loss = 0.0007983978721313179
iteration 39, loss = 0.0009315034840255976
iteration 40, loss = 0.0007816504803486168
iteration 41, loss = 0.0015318465884774923
iteration 42, loss = 0.0009735398925840855
iteration 43, loss = 0.0009698131470941007
iteration 44, loss = 0.0008627079660072923
iteration 45, loss = 0.000797296641394496
iteration 46, loss = 0.0008724805666133761
iteration 47, loss = 0.001654954394325614
iteration 48, loss = 0.0006524953059852123
iteration 49, loss = 0.0011247812071815133
iteration 50, loss = 0.0009897015988826752
iteration 51, loss = 0.0012782230041921139
iteration 52, loss = 0.0008668539230711758
iteration 53, loss = 0.0013651051558554173
iteration 54, loss = 0.0008548389887437224
iteration 55, loss = 0.0010283642914146185
iteration 56, loss = 0.001504051499068737
iteration 57, loss = 0.0009590478148311377
iteration 58, loss = 0.00063108786707744
iteration 59, loss = 0.0010634091449901462
iteration 60, loss = 0.0010485963430255651
iteration 61, loss = 0.001003936748020351
iteration 62, loss = 0.0011914769420400262
iteration 63, loss = 0.0010946524562314153
iteration 64, loss = 0.000860884552821517
iteration 65, loss = 0.0016023718053475022
iteration 66, loss = 0.0006992897251620889
iteration 67, loss = 0.0006364748696796596
iteration 68, loss = 0.0007550170994363725
iteration 69, loss = 0.0009506472270004451
iteration 70, loss = 0.0008815855835564435
iteration 71, loss = 0.0012066853232681751
iteration 72, loss = 0.0015767163131386042
iteration 73, loss = 0.0008844643016345799
iteration 74, loss = 0.000648464250843972
iteration 75, loss = 0.0009546109940856695
iteration 76, loss = 0.0007054271409288049
iteration 77, loss = 0.000836974591948092
iteration 78, loss = 0.0008321747300215065
iteration 79, loss = 0.0006868748459964991
iteration 80, loss = 0.0007753498503006995
iteration 81, loss = 0.0008090725750662386
iteration 82, loss = 0.0006826549069955945
iteration 83, loss = 0.0012023616582155228
iteration 84, loss = 0.0009472427191212773
iteration 85, loss = 0.0008831905433908105
iteration 86, loss = 0.0011024869745597243
iteration 87, loss = 0.0008622879395261407
iteration 88, loss = 0.00138069165404886
iteration 89, loss = 0.0011480416869744658
iteration 90, loss = 0.0010994856711477041
iteration 91, loss = 0.0007518280763179064
iteration 92, loss = 0.0007911324501037598
iteration 93, loss = 0.0009975469438359141
iteration 94, loss = 0.0008687148801982403
iteration 95, loss = 0.0008783901575952768
iteration 96, loss = 0.0006730476743541658
iteration 97, loss = 0.0008684073109179735
iteration 98, loss = 0.0010072359582409263
iteration 99, loss = 0.0007303981692530215
iteration 100, loss = 0.000693913025315851
iteration 101, loss = 0.0007523806416429579
iteration 102, loss = 0.000887532951310277
iteration 103, loss = 0.0011111838975921273
iteration 104, loss = 0.0008078847313299775
iteration 105, loss = 0.0006296513602137566
iteration 106, loss = 0.0012174852890893817
iteration 107, loss = 0.0006027328199706972
iteration 108, loss = 0.0012044613249599934
iteration 109, loss = 0.0009589574765414
iteration 110, loss = 0.0008059925748966634
iteration 111, loss = 0.0008894200436770916
iteration 112, loss = 0.0013395025162026286
iteration 113, loss = 0.000900106446351856
iteration 114, loss = 0.0006440157303586602
iteration 115, loss = 0.0007177951629273593
iteration 116, loss = 0.000960085541009903
iteration 117, loss = 0.0008566766628064215
iteration 118, loss = 0.0014473869232460856
iteration 119, loss = 0.0008516105590388179
iteration 120, loss = 0.0007492703152820468
iteration 121, loss = 0.0024124886840581894
iteration 122, loss = 0.0009029450593516231
iteration 123, loss = 0.0008482479024678469
iteration 124, loss = 0.0008625311311334372
iteration 125, loss = 0.0008121496066451073
iteration 126, loss = 0.0007303366437554359
iteration 127, loss = 0.000592568248976022
iteration 128, loss = 0.0007560582598671317
iteration 129, loss = 0.0008564920863136649
iteration 130, loss = 0.0017265231581404805
iteration 131, loss = 0.0011072901543229818
iteration 132, loss = 0.0008012680336833
iteration 133, loss = 0.001038275077007711
iteration 134, loss = 0.000899949693121016
iteration 135, loss = 0.0011608748463913798
iteration 136, loss = 0.0009445315808989108
iteration 137, loss = 0.0007816311554051936
iteration 138, loss = 0.0007297878037206829
iteration 139, loss = 0.0010112174786627293
iteration 140, loss = 0.0008407723507843912
iteration 141, loss = 0.0009014144307002425
iteration 142, loss = 0.001532203983515501
iteration 143, loss = 0.0007834833231754601
iteration 144, loss = 0.0007059915224090219
iteration 145, loss = 0.0008116904646158218
iteration 146, loss = 0.0008374176686629653
iteration 147, loss = 0.0018182628555223346
iteration 148, loss = 0.000819663458969444
iteration 149, loss = 0.0007641608826816082
iteration 150, loss = 0.0007523145177401602
iteration 151, loss = 0.000687655818182975
iteration 152, loss = 0.0010759164579212666
iteration 153, loss = 0.0007083926466293633
iteration 154, loss = 0.0008509679464623332
iteration 155, loss = 0.0006670992588624358
iteration 156, loss = 0.0019166915444657207
iteration 157, loss = 0.0015531396493315697
iteration 158, loss = 0.0008616825798526406
iteration 159, loss = 0.0006898986757732928
iteration 160, loss = 0.0008445260464213789
iteration 161, loss = 0.0006975436117500067
iteration 162, loss = 0.0009271444869227707
iteration 163, loss = 0.0007542534731328487
iteration 164, loss = 0.0009017245611175895
iteration 165, loss = 0.001113522914238274
iteration 166, loss = 0.0010159420780837536
iteration 167, loss = 0.0011980313574895263
iteration 168, loss = 0.001649581710807979
iteration 169, loss = 0.0008753563743084669
iteration 170, loss = 0.0007618365343660116
iteration 171, loss = 0.0007056851172819734
iteration 172, loss = 0.0009132440318353474
iteration 173, loss = 0.0008663622429594398
iteration 174, loss = 0.0010248890612274408
iteration 175, loss = 0.0007129392470233142
iteration 176, loss = 0.0007705854950472713
iteration 177, loss = 0.0009469967917539179
iteration 178, loss = 0.000842978130094707
iteration 179, loss = 0.0006732058827765286
iteration 180, loss = 0.0007253668736666441
iteration 181, loss = 0.0009511857060715556
iteration 182, loss = 0.0007956034387461841
iteration 183, loss = 0.001449071103706956
iteration 184, loss = 0.000762921990826726
iteration 185, loss = 0.0008877537911757827
iteration 186, loss = 0.0008663762710057199
iteration 187, loss = 0.0008287262171506882
iteration 188, loss = 0.0007237503887154162
iteration 189, loss = 0.0014086647424846888
iteration 190, loss = 0.0008675563149154186
iteration 191, loss = 0.0008108578622341156
iteration 192, loss = 0.0007709419587627053
iteration 193, loss = 0.0009460726869292557
iteration 194, loss = 0.0014068650780245662
iteration 195, loss = 0.0007693389779888093
iteration 196, loss = 0.001324047101661563
iteration 197, loss = 0.0007873484282754362
iteration 198, loss = 0.0007207593880593777
iteration 199, loss = 0.0015676644397899508
iteration 200, loss = 0.0008796458714641631
iteration 201, loss = 0.0009032012894749641
iteration 202, loss = 0.0010001490591093898
iteration 203, loss = 0.0007062545628286898
iteration 204, loss = 0.00080909114331007
iteration 205, loss = 0.0016714895609766245
iteration 206, loss = 0.0007974563632160425
iteration 207, loss = 0.0007798057049512863
iteration 208, loss = 0.0012650300050154328
iteration 209, loss = 0.0008711397531442344
iteration 210, loss = 0.001387082738801837
iteration 211, loss = 0.0014637649292126298
iteration 212, loss = 0.0011515915393829346
iteration 213, loss = 0.0009769350290298462
iteration 214, loss = 0.0014979965053498745
iteration 215, loss = 0.0008258669986389577
iteration 216, loss = 0.0017354025039821863
iteration 217, loss = 0.0007409863756038249
iteration 218, loss = 0.0010124312248080969
iteration 219, loss = 0.0008534573717042804
iteration 220, loss = 0.0009253083844669163
iteration 221, loss = 0.0012467496562749147
iteration 222, loss = 0.0009918465511873364
iteration 223, loss = 0.0011778903426602483
iteration 224, loss = 0.000986216589808464
iteration 225, loss = 0.0007861048216000199
iteration 226, loss = 0.0007529061404056847
iteration 227, loss = 0.0008013789774850011
iteration 228, loss = 0.0007911259308457375
iteration 229, loss = 0.0007431035046465695
iteration 230, loss = 0.0008327882969751954
iteration 231, loss = 0.0014863909455016255
iteration 232, loss = 0.0007231768686324358
iteration 233, loss = 0.0011885791318491101
iteration 234, loss = 0.0007572223548777401
iteration 235, loss = 0.0013665746664628386
iteration 236, loss = 0.0008164807804860175
iteration 237, loss = 0.0016764071770012379
iteration 238, loss = 0.0008186658378690481
iteration 239, loss = 0.0007469865377061069
iteration 240, loss = 0.0008807328995317221
iteration 241, loss = 0.0011408249847590923
iteration 242, loss = 0.0007493816665373743
iteration 243, loss = 0.0009530276875011623
iteration 244, loss = 0.0015946727944537997
iteration 245, loss = 0.0008987645269371569
iteration 246, loss = 0.0008211002568714321
iteration 247, loss = 0.0008427132852375507
iteration 248, loss = 0.00094111158978194
iteration 249, loss = 0.0009418799309059978
iteration 250, loss = 0.0008272702107205987
iteration 251, loss = 0.0007145715644583106
iteration 252, loss = 0.0007109245052561164
iteration 253, loss = 0.0009486114722676575
iteration 254, loss = 0.0008554085507057607
iteration 255, loss = 0.00111871596891433
iteration 256, loss = 0.0011001507518813014
iteration 257, loss = 0.0007678490364924073
iteration 258, loss = 0.000928941648453474
iteration 259, loss = 0.0010777614079415798
iteration 260, loss = 0.0008696667500771582
iteration 261, loss = 0.0012117327423766255
iteration 262, loss = 0.0016479623736813664
iteration 263, loss = 0.0006635280442424119
iteration 264, loss = 0.0007757586427032948
iteration 265, loss = 0.0010642929701134562
iteration 266, loss = 0.000653459457680583
iteration 267, loss = 0.0006505331839434803
iteration 268, loss = 0.0009566519875079393
iteration 269, loss = 0.0006826616590842605
iteration 270, loss = 0.0007986772689037025
iteration 271, loss = 0.0008706045337021351
iteration 272, loss = 0.000663022103253752
iteration 273, loss = 0.001056587789207697
iteration 274, loss = 0.000868613482452929
iteration 275, loss = 0.0007626047008670866
iteration 276, loss = 0.0007882484933361411
iteration 277, loss = 0.0008340773638337851
iteration 278, loss = 0.0007338364375755191
iteration 279, loss = 0.0008700053440406919
iteration 280, loss = 0.0012370683252811432
iteration 281, loss = 0.0010065619135275483
iteration 282, loss = 0.0009121563052758574
iteration 283, loss = 0.0007737840642221272
iteration 284, loss = 0.0009318010415881872
iteration 285, loss = 0.0013720339629799128
iteration 286, loss = 0.0015838714316487312
iteration 287, loss = 0.0016143422108143568
iteration 288, loss = 0.0009796986123546958
iteration 289, loss = 0.0009350584587082267
iteration 290, loss = 0.0008747015381231904
iteration 291, loss = 0.000807102769613266
iteration 292, loss = 0.0008257407462224364
iteration 293, loss = 0.0008928055176511407
iteration 294, loss = 0.0007140774396248162
iteration 295, loss = 0.0008036291692405939
iteration 296, loss = 0.0009459247812628746
iteration 297, loss = 0.0008579834247939289
iteration 298, loss = 0.0007664342410862446
iteration 299, loss = 0.0008002633694559336
iteration 300, loss = 0.0011142697185277939
iteration 1, loss = 0.0007900550263002515
iteration 2, loss = 0.0007447785465046763
iteration 3, loss = 0.0012972455006092787
iteration 4, loss = 0.0008435692870989442
iteration 5, loss = 0.0009189089760184288
iteration 6, loss = 0.0008571486105211079
iteration 7, loss = 0.0019241351401433349
iteration 8, loss = 0.0008688561501912773
iteration 9, loss = 0.0007158335065469146
iteration 10, loss = 0.000932917115278542
iteration 11, loss = 0.0015320421662181616
iteration 12, loss = 0.001036488451063633
iteration 13, loss = 0.0007995318155735731
iteration 14, loss = 0.0011968780308961868
iteration 15, loss = 0.0009139169123955071
iteration 16, loss = 0.0006379567785188556
iteration 17, loss = 0.0008602212183177471
iteration 18, loss = 0.0007320052245631814
iteration 19, loss = 0.0009653634624555707
iteration 20, loss = 0.0008023757836781442
iteration 21, loss = 0.0008217048598453403
iteration 22, loss = 0.0011726005468517542
iteration 23, loss = 0.0010581276146695018
iteration 24, loss = 0.0008562608272768557
iteration 25, loss = 0.0007442118367180228
iteration 26, loss = 0.0009229412535205483
iteration 27, loss = 0.0006344453431665897
iteration 28, loss = 0.0008197777206078172
iteration 29, loss = 0.0007973317988216877
iteration 30, loss = 0.0007402647752314806
iteration 31, loss = 0.0008014456834644079
iteration 32, loss = 0.0011905621504411101
iteration 33, loss = 0.0008422036189585924
iteration 34, loss = 0.0008383315289393067
iteration 35, loss = 0.0011148087214678526
iteration 36, loss = 0.0007958341739140451
iteration 37, loss = 0.0008212776738218963
iteration 38, loss = 0.0013497932814061642
iteration 39, loss = 0.0008993648225441575
iteration 40, loss = 0.0007921981159597635
iteration 41, loss = 0.0007348381914198399
iteration 42, loss = 0.0009684457909315825
iteration 43, loss = 0.0009061910677701235
iteration 44, loss = 0.000885125482454896
iteration 45, loss = 0.0010511939181014895
iteration 46, loss = 0.0006804954609833658
iteration 47, loss = 0.0009884481551125646
iteration 48, loss = 0.0007971883751451969
iteration 49, loss = 0.0008070843759924173
iteration 50, loss = 0.0009111183462664485
iteration 51, loss = 0.0008003403199836612
iteration 52, loss = 0.0009143962524831295
iteration 53, loss = 0.0007382857729680836
iteration 54, loss = 0.0017270792741328478
iteration 55, loss = 0.0007068197010084987
iteration 56, loss = 0.0008085857261903584
iteration 57, loss = 0.00088126165792346
iteration 58, loss = 0.0012617037864401937
iteration 59, loss = 0.0007180785178206861
iteration 60, loss = 0.0007157588843256235
iteration 61, loss = 0.0008586718467995524
iteration 62, loss = 0.0013360882876440883
iteration 63, loss = 0.0008783279336057603
iteration 64, loss = 0.0006977354059927166
iteration 65, loss = 0.0007520978106185794
iteration 66, loss = 0.0008014834602363408
iteration 67, loss = 0.0007992415921762586
iteration 68, loss = 0.0007153118494898081
iteration 69, loss = 0.0007612332701683044
iteration 70, loss = 0.0008216917631216347
iteration 71, loss = 0.0007810242823325098
iteration 72, loss = 0.0008277033921331167
iteration 73, loss = 0.0007721297442913055
iteration 74, loss = 0.0009959342423826456
iteration 75, loss = 0.0011359874624758959
iteration 76, loss = 0.0007243601721711457
iteration 77, loss = 0.0008939885883592069
iteration 78, loss = 0.001082975184544921
iteration 79, loss = 0.00076993991388008
iteration 80, loss = 0.0009909591171890497
iteration 81, loss = 0.0008404777618125081
iteration 82, loss = 0.0007452782010659575
iteration 83, loss = 0.000784795789513737
iteration 84, loss = 0.0007460160413756967
iteration 85, loss = 0.0010224372381344438
iteration 86, loss = 0.0009341617696918547
iteration 87, loss = 0.001288244384340942
iteration 88, loss = 0.0008673203992657363
iteration 89, loss = 0.001342708826996386
iteration 90, loss = 0.0008117521647363901
iteration 91, loss = 0.0009835705859586596
iteration 92, loss = 0.0017145569436252117
iteration 93, loss = 0.0006879704305902123
iteration 94, loss = 0.0007383663905784488
iteration 95, loss = 0.0007526728441007435
iteration 96, loss = 0.000720590353012085
iteration 97, loss = 0.001070155412890017
iteration 98, loss = 0.0007681937422603369
iteration 99, loss = 0.0008768484694883227
iteration 100, loss = 0.0008703694329597056
iteration 101, loss = 0.0009332726476714015
iteration 102, loss = 0.0007693255902267992
iteration 103, loss = 0.0007859847974032164
iteration 104, loss = 0.0007105446420609951
iteration 105, loss = 0.001072556828148663
iteration 106, loss = 0.0006770286126993597
iteration 107, loss = 0.0006926011992618442
iteration 108, loss = 0.001419339212588966
iteration 109, loss = 0.0009242181549780071
iteration 110, loss = 0.0008136697579175234
iteration 111, loss = 0.0009257659548893571
iteration 112, loss = 0.0007154411869123578
iteration 113, loss = 0.0011055665090680122
iteration 114, loss = 0.0006991028785705566
iteration 115, loss = 0.0007445780793204904
iteration 116, loss = 0.0007221659761853516
iteration 117, loss = 0.0008003925322555006
iteration 118, loss = 0.001598962815478444
iteration 119, loss = 0.0007178016239777207
iteration 120, loss = 0.0007347583305090666
iteration 121, loss = 0.000746271398384124
iteration 122, loss = 0.0008602330926805735
iteration 123, loss = 0.0007744212634861469
iteration 124, loss = 0.0006656207842752337
iteration 125, loss = 0.0009297400247305632
iteration 126, loss = 0.0007418437162414193
iteration 127, loss = 0.000747005280572921
iteration 128, loss = 0.0009610513807274401
iteration 129, loss = 0.0011501621920615435
iteration 130, loss = 0.0007942309021018445
iteration 131, loss = 0.001154530793428421
iteration 132, loss = 0.0009556463919579983
iteration 133, loss = 0.0009383679716847837
iteration 134, loss = 0.0010917149484157562
iteration 135, loss = 0.0006321796681731939
iteration 136, loss = 0.0008203972247429192
iteration 137, loss = 0.0014652502723038197
iteration 138, loss = 0.0010910306591540575
iteration 139, loss = 0.0010772878304123878
iteration 140, loss = 0.0007956321351230145
iteration 141, loss = 0.0012501080054789782
iteration 142, loss = 0.0011682952754199505
iteration 143, loss = 0.0015983444172888994
iteration 144, loss = 0.0008009862503968179
iteration 145, loss = 0.000853682984597981
iteration 146, loss = 0.0012452744413167238
iteration 147, loss = 0.0019473719876259565
iteration 148, loss = 0.001577307702973485
iteration 149, loss = 0.0009010032517835498
iteration 150, loss = 0.0006789973704144359
iteration 151, loss = 0.0008017253712750971
iteration 152, loss = 0.002064736792817712
iteration 153, loss = 0.0007476599421352148
iteration 154, loss = 0.0008353794692084193
iteration 155, loss = 0.0006890755612403154
iteration 156, loss = 0.0007102596573531628
iteration 157, loss = 0.0007083478849381208
iteration 158, loss = 0.0011339683551341295
iteration 159, loss = 0.0014204006874933839
iteration 160, loss = 0.0008005895069800317
iteration 161, loss = 0.0008597291307523847
iteration 162, loss = 0.001019356306642294
iteration 163, loss = 0.001273428788408637
iteration 164, loss = 0.0007571026799269021
iteration 165, loss = 0.0008222386823035777
iteration 166, loss = 0.0007395554566755891
iteration 167, loss = 0.00141793058719486
iteration 168, loss = 0.0008686359506100416
iteration 169, loss = 0.0006868031341582537
iteration 170, loss = 0.0008332374854944646
iteration 171, loss = 0.0010631063487380743
iteration 172, loss = 0.0007188796298578382
iteration 173, loss = 0.0014354432933032513
iteration 174, loss = 0.0018425589660182595
iteration 175, loss = 0.0006663186941295862
iteration 176, loss = 0.0010793650289997458
iteration 177, loss = 0.0007311207009479403
iteration 178, loss = 0.0009436238906346262
iteration 179, loss = 0.0006590746925212443
iteration 180, loss = 0.0009136532316915691
iteration 181, loss = 0.0009007118060253561
iteration 182, loss = 0.0008919246611185372
iteration 183, loss = 0.0007931854925118387
iteration 184, loss = 0.0008735197479836643
iteration 185, loss = 0.0009649243438616395
iteration 186, loss = 0.0009122613118961453
iteration 187, loss = 0.001135740545578301
iteration 188, loss = 0.0008459117962047458
iteration 189, loss = 0.0008356973994523287
iteration 190, loss = 0.000702373799867928
iteration 191, loss = 0.0014993875520303845
iteration 192, loss = 0.001013029832392931
iteration 193, loss = 0.0006659214850515127
iteration 194, loss = 0.000729236111510545
iteration 195, loss = 0.0007111785234883428
iteration 196, loss = 0.0013855890138074756
iteration 197, loss = 0.0010833056876435876
iteration 198, loss = 0.0008745352388359606
iteration 199, loss = 0.0010417151497676969
iteration 200, loss = 0.0008471781620755792
iteration 201, loss = 0.0009424826130270958
iteration 202, loss = 0.0007263793959282339
iteration 203, loss = 0.0007587936706840992
iteration 204, loss = 0.0009579660836607218
iteration 205, loss = 0.0008203817415051162
iteration 206, loss = 0.0009668818092904985
iteration 207, loss = 0.0008576411637477577
iteration 208, loss = 0.0008291675476357341
iteration 209, loss = 0.0007916311733424664
iteration 210, loss = 0.0007451979909092188
iteration 211, loss = 0.0006842910661362112
iteration 212, loss = 0.000990640022791922
iteration 213, loss = 0.000874750898219645
iteration 214, loss = 0.0008192439563572407
iteration 215, loss = 0.0014569106278941035
iteration 216, loss = 0.0016456085722893476
iteration 217, loss = 0.0014895317144691944
iteration 218, loss = 0.0009385356097482145
iteration 219, loss = 0.000782532268203795
iteration 220, loss = 0.0016235032817348838
iteration 221, loss = 0.0007779372972436249
iteration 222, loss = 0.0015805913135409355
iteration 223, loss = 0.0008930471376515925
iteration 224, loss = 0.0007332360255531967
iteration 225, loss = 0.0007756268605589867
iteration 226, loss = 0.0020395356696099043
iteration 227, loss = 0.0007692592917010188
iteration 228, loss = 0.0008698058081790805
iteration 229, loss = 0.0015115225687623024
iteration 230, loss = 0.0008003545226529241
iteration 231, loss = 0.0009957782458513975
iteration 232, loss = 0.0010586875723674893
iteration 233, loss = 0.0007635182118974626
iteration 234, loss = 0.0008601945592090487
iteration 235, loss = 0.001014519715681672
iteration 236, loss = 0.0010057068429887295
iteration 237, loss = 0.0008170310757122934
iteration 238, loss = 0.0008095024968497455
iteration 239, loss = 0.000678889686241746
iteration 240, loss = 0.0008851599413901567
iteration 241, loss = 0.0007521372754126787
iteration 242, loss = 0.0008438561926595867
iteration 243, loss = 0.0007512529846280813
iteration 244, loss = 0.0008590014767833054
iteration 245, loss = 0.0015909767244011164
iteration 246, loss = 0.0009535560384392738
iteration 247, loss = 0.0008215054403990507
iteration 248, loss = 0.0015715702902525663
iteration 249, loss = 0.0007021815399639308
iteration 250, loss = 0.0007128622382879257
iteration 251, loss = 0.0008615638362243772
iteration 252, loss = 0.0008686028304509819
iteration 253, loss = 0.0007097555790096521
iteration 254, loss = 0.0009549203678034246
iteration 255, loss = 0.0008023773552849889
iteration 256, loss = 0.0012823027791455388
iteration 257, loss = 0.0010779999429360032
iteration 258, loss = 0.0007358969305641949
iteration 259, loss = 0.0015110565582290292
iteration 260, loss = 0.0015729876467958093
iteration 261, loss = 0.0009362162090837955
iteration 262, loss = 0.0007579999510198832
iteration 263, loss = 0.001444704714231193
iteration 264, loss = 0.001413131831213832
iteration 265, loss = 0.0006944026099517941
iteration 266, loss = 0.0009754217462614179
iteration 267, loss = 0.000760574359446764
iteration 268, loss = 0.0008507050224579871
iteration 269, loss = 0.0007643582648597658
iteration 270, loss = 0.0008609947399236262
iteration 271, loss = 0.0016448117094114423
iteration 272, loss = 0.0012234970927238464
iteration 273, loss = 0.0006872682715766132
iteration 274, loss = 0.0007713569211773574
iteration 275, loss = 0.001008271356113255
iteration 276, loss = 0.0018714906182140112
iteration 277, loss = 0.0008644840563647449
iteration 278, loss = 0.001037697889842093
iteration 279, loss = 0.0015701716765761375
iteration 280, loss = 0.000828390649985522
iteration 281, loss = 0.000821772962808609
iteration 282, loss = 0.0007938421913422644
iteration 283, loss = 0.0009615621529519558
iteration 284, loss = 0.0011168651981279254
iteration 285, loss = 0.000871088937856257
iteration 286, loss = 0.000994979403913021
iteration 287, loss = 0.0008886458235792816
iteration 288, loss = 0.0008711369591765106
iteration 289, loss = 0.0007584172999486327
iteration 290, loss = 0.000711065367795527
iteration 291, loss = 0.0011207453208044171
iteration 292, loss = 0.0005770182469859719
iteration 293, loss = 0.0008750330889597535
iteration 294, loss = 0.0007953516906127334
iteration 295, loss = 0.0009696256602182984
iteration 296, loss = 0.0007598007214255631
iteration 297, loss = 0.0015208625700324774
iteration 298, loss = 0.0015480370493605733
iteration 299, loss = 0.0008513480424880981
iteration 300, loss = 0.0007146048592403531
iteration 1, loss = 0.001421656459569931
iteration 2, loss = 0.0010119850048795342
iteration 3, loss = 0.000712529115844518
iteration 4, loss = 0.0018269778229296207
iteration 5, loss = 0.0015373372007161379
iteration 6, loss = 0.00093709712382406
iteration 7, loss = 0.0010304704774171114
iteration 8, loss = 0.0013019390171393752
iteration 9, loss = 0.0007821505423635244
iteration 10, loss = 0.0007680595153942704
iteration 11, loss = 0.0012278595240786672
iteration 12, loss = 0.0008421408128924668
iteration 13, loss = 0.0008867905125953257
iteration 14, loss = 0.0008580427384003997
iteration 15, loss = 0.001049537560902536
iteration 16, loss = 0.0007802911568433046
iteration 17, loss = 0.0011618054704740644
iteration 18, loss = 0.0007450181874446571
iteration 19, loss = 0.0007818526355549693
iteration 20, loss = 0.0009712753817439079
iteration 21, loss = 0.0008670247043482959
iteration 22, loss = 0.0008344342350028455
iteration 23, loss = 0.0013884843792766333
iteration 24, loss = 0.001489711576141417
iteration 25, loss = 0.000875602476298809
iteration 26, loss = 0.0006914681289345026
iteration 27, loss = 0.000765150529332459
iteration 28, loss = 0.0008010226883925498
iteration 29, loss = 0.0006284945411607623
iteration 30, loss = 0.000968533509876579
iteration 31, loss = 0.0009349770843982697
iteration 32, loss = 0.0008052772609516978
iteration 33, loss = 0.001169838011264801
iteration 34, loss = 0.0015313721960410476
iteration 35, loss = 0.0006686739507131279
iteration 36, loss = 0.000977543881163001
iteration 37, loss = 0.000813718477729708
iteration 38, loss = 0.0007983766263350844
iteration 39, loss = 0.0007172884652391076
iteration 40, loss = 0.0009270677110180259
iteration 41, loss = 0.000860546890180558
iteration 42, loss = 0.0008478113450109959
iteration 43, loss = 0.000843928602989763
iteration 44, loss = 0.001979128224775195
iteration 45, loss = 0.0010338755091652274
iteration 46, loss = 0.000702687306329608
iteration 47, loss = 0.0014988502953201532
iteration 48, loss = 0.0008323100628331304
iteration 49, loss = 0.0009126003133133054
iteration 50, loss = 0.0010734270326793194
iteration 51, loss = 0.0011397828347980976
iteration 52, loss = 0.001052746083587408
iteration 53, loss = 0.0008690597023814917
iteration 54, loss = 0.000748195219784975
iteration 55, loss = 0.0006842625443823636
iteration 56, loss = 0.0009302220423705876
iteration 57, loss = 0.0007109480793587863
iteration 58, loss = 0.0008272996637970209
iteration 59, loss = 0.0006350350449793041
iteration 60, loss = 0.0008577365661039948
iteration 61, loss = 0.0010334589751437306
iteration 62, loss = 0.000905964057892561
iteration 63, loss = 0.0007439490291289985
iteration 64, loss = 0.0008450251189060509
iteration 65, loss = 0.0008486706647090614
iteration 66, loss = 0.0008172721136361361
iteration 67, loss = 0.0007793197291903198
iteration 68, loss = 0.0008165574399754405
iteration 69, loss = 0.000781881797593087
iteration 70, loss = 0.0007542367093265057
iteration 71, loss = 0.0009454686078242958
iteration 72, loss = 0.0007811795803718269
iteration 73, loss = 0.0007994630723260343
iteration 74, loss = 0.0009486504714004695
iteration 75, loss = 0.0008712464477866888
iteration 76, loss = 0.0006830105558037758
iteration 77, loss = 0.0010396621655672789
iteration 78, loss = 0.0008148879860527813
iteration 79, loss = 0.000807207019533962
iteration 80, loss = 0.0009456337429583073
iteration 81, loss = 0.0007262906292453408
iteration 82, loss = 0.0008319600601680577
iteration 83, loss = 0.0009945097845047712
iteration 84, loss = 0.0011662126053124666
iteration 85, loss = 0.0007084420649334788
iteration 86, loss = 0.0008985204622149467
iteration 87, loss = 0.0008718777680769563
iteration 88, loss = 0.0013141287490725517
iteration 89, loss = 0.0010663644643500447
iteration 90, loss = 0.0008177699637599289
iteration 91, loss = 0.000972334761172533
iteration 92, loss = 0.0009720244561322033
iteration 93, loss = 0.0008099273545667529
iteration 94, loss = 0.0008398770587518811
iteration 95, loss = 0.0011256749276071787
iteration 96, loss = 0.0006658981437794864
iteration 97, loss = 0.0010311879450455308
iteration 98, loss = 0.0007202393608167768
iteration 99, loss = 0.0013181406538933516
iteration 100, loss = 0.0010853451676666737
iteration 101, loss = 0.0008979435660876334
iteration 102, loss = 0.0010275887325406075
iteration 103, loss = 0.0007354873232543468
iteration 104, loss = 0.0007281390717253089
iteration 105, loss = 0.0007691236678510904
iteration 106, loss = 0.000812392623629421
iteration 107, loss = 0.0010054792510345578
iteration 108, loss = 0.0008840057998895645
iteration 109, loss = 0.0008219915325753391
iteration 110, loss = 0.001055450877174735
iteration 111, loss = 0.0009839503327384591
iteration 112, loss = 0.000863938475959003
iteration 113, loss = 0.0009536404977552593
iteration 114, loss = 0.001583412871696055
iteration 115, loss = 0.0007147560245357454
iteration 116, loss = 0.0007770220981910825
iteration 117, loss = 0.0007212787168100476
iteration 118, loss = 0.0013388904044404626
iteration 119, loss = 0.0007418335881084204
iteration 120, loss = 0.000873558921739459
iteration 121, loss = 0.0010803337208926678
iteration 122, loss = 0.0007363334880210459
iteration 123, loss = 0.0013037037570029497
iteration 124, loss = 0.0010787001810967922
iteration 125, loss = 0.0007521642837673426
iteration 126, loss = 0.001371891936287284
iteration 127, loss = 0.0008853813633322716
iteration 128, loss = 0.000725586258340627
iteration 129, loss = 0.0008707797387614846
iteration 130, loss = 0.0007890219567343593
iteration 131, loss = 0.0007873574504628778
iteration 132, loss = 0.000869962212163955
iteration 133, loss = 0.0008460889803245664
iteration 134, loss = 0.0008239572634920478
iteration 135, loss = 0.0007142736576497555
iteration 136, loss = 0.0008527991594746709
iteration 137, loss = 0.0010620526736602187
iteration 138, loss = 0.0008917369996197522
iteration 139, loss = 0.0007919004419818521
iteration 140, loss = 0.0009847406763583422
iteration 141, loss = 0.0008715404546819627
iteration 142, loss = 0.0008490066975355148
iteration 143, loss = 0.0009162406204268336
iteration 144, loss = 0.001216726959683001
iteration 145, loss = 0.000941961188800633
iteration 146, loss = 0.0007413215353153646
iteration 147, loss = 0.0007919096969999373
iteration 148, loss = 0.0009143076022155583
iteration 149, loss = 0.0006798141403123736
iteration 150, loss = 0.000871614902280271
iteration 151, loss = 0.0009296877542510629
iteration 152, loss = 0.001008979044854641
iteration 153, loss = 0.000857513106893748
iteration 154, loss = 0.000748866586945951
iteration 155, loss = 0.0013326286571100354
iteration 156, loss = 0.0007728743366897106
iteration 157, loss = 0.0009283573017455637
iteration 158, loss = 0.0022209356538951397
iteration 159, loss = 0.0008541967836208642
iteration 160, loss = 0.0006868359632790089
iteration 161, loss = 0.0015736829955130816
iteration 162, loss = 0.0009830633644014597
iteration 163, loss = 0.0007727755582891405
iteration 164, loss = 0.0008375005563721061
iteration 165, loss = 0.0007643522112630308
iteration 166, loss = 0.0008620795561000705
iteration 167, loss = 0.0008696659933775663
iteration 168, loss = 0.0008523136493749917
iteration 169, loss = 0.0007969391299411654
iteration 170, loss = 0.001139510190114379
iteration 171, loss = 0.0008772988803684711
iteration 172, loss = 0.0015254239551723003
iteration 173, loss = 0.0007666100864298642
iteration 174, loss = 0.0009467446943745017
iteration 175, loss = 0.0016034438740462065
iteration 176, loss = 0.001408996875397861
iteration 177, loss = 0.0006845368188805878
iteration 178, loss = 0.0009003255399875343
iteration 179, loss = 0.0009443445596843958
iteration 180, loss = 0.0007271739304997027
iteration 181, loss = 0.0009072637185454369
iteration 182, loss = 0.0007768601644784212
iteration 183, loss = 0.0007419646717607975
iteration 184, loss = 0.0008605738403275609
iteration 185, loss = 0.0009119386086240411
iteration 186, loss = 0.0008020095992833376
iteration 187, loss = 0.000837844330817461
iteration 188, loss = 0.0007848487584851682
iteration 189, loss = 0.0007792917313054204
iteration 190, loss = 0.0009253715979866683
iteration 191, loss = 0.0008305365336127579
iteration 192, loss = 0.0008277174783870578
iteration 193, loss = 0.0007040089694783092
iteration 194, loss = 0.0007581935497000813
iteration 195, loss = 0.0015479513676837087
iteration 196, loss = 0.0007570208981633186
iteration 197, loss = 0.001483613857999444
iteration 198, loss = 0.0008348218398168683
iteration 199, loss = 0.002336186822503805
iteration 200, loss = 0.0010181376710534096
iteration 201, loss = 0.0008835868793539703
iteration 202, loss = 0.0014042628463357687
iteration 203, loss = 0.0015336911892518401
iteration 204, loss = 0.0011626397026702762
iteration 205, loss = 0.0009264202672056854
iteration 206, loss = 0.0007585465209558606
iteration 207, loss = 0.0007021803176030517
iteration 208, loss = 0.0011100866831839085
iteration 209, loss = 0.0012445690808817744
iteration 210, loss = 0.0007978124194778502
iteration 211, loss = 0.0007904610829427838
iteration 212, loss = 0.0008468846790492535
iteration 213, loss = 0.0010873251594603062
iteration 214, loss = 0.0011055320501327515
iteration 215, loss = 0.0014654670376330614
iteration 216, loss = 0.00077177828643471
iteration 217, loss = 0.0009699887013994157
iteration 218, loss = 0.0006549465470016003
iteration 219, loss = 0.0013908749679103494
iteration 220, loss = 0.000745175639167428
iteration 221, loss = 0.0012305991258472204
iteration 222, loss = 0.001139705185778439
iteration 223, loss = 0.0010555102489888668
iteration 224, loss = 0.0007153695332817733
iteration 225, loss = 0.001554052927531302
iteration 226, loss = 0.0008734631701372564
iteration 227, loss = 0.0009331756155006588
iteration 228, loss = 0.0007194207282736897
iteration 229, loss = 0.0010321849258616567
iteration 230, loss = 0.001324223936535418
iteration 231, loss = 0.0008346554241143167
iteration 232, loss = 0.0010521188378334045
iteration 233, loss = 0.0008480004034936428
iteration 234, loss = 0.000806894269771874
iteration 235, loss = 0.0012972446857020259
iteration 236, loss = 0.000749197555705905
iteration 237, loss = 0.0006851752405054867
iteration 238, loss = 0.000801152375061065
iteration 239, loss = 0.0008967575849965215
iteration 240, loss = 0.0008473240886814892
iteration 241, loss = 0.0009203182999044657
iteration 242, loss = 0.0007744342437945306
iteration 243, loss = 0.0008639083825983107
iteration 244, loss = 0.0008613249519839883
iteration 245, loss = 0.0011266525834798813
iteration 246, loss = 0.0008658680599182844
iteration 247, loss = 0.0007451252895407379
iteration 248, loss = 0.0010610366007313132
iteration 249, loss = 0.0007591829053126276
iteration 250, loss = 0.0008019765955395997
iteration 251, loss = 0.0008957718382589519
iteration 252, loss = 0.001275267917662859
iteration 253, loss = 0.000797790358774364
iteration 254, loss = 0.0008742333739064634
iteration 255, loss = 0.0009000311838462949
iteration 256, loss = 0.0007893512956798077
iteration 257, loss = 0.0006410915520973504
iteration 258, loss = 0.0018074141116812825
iteration 259, loss = 0.0008765967795625329
iteration 260, loss = 0.000625205400865525
iteration 261, loss = 0.0007525340188294649
iteration 262, loss = 0.0009198875050060451
iteration 263, loss = 0.0007563693216070533
iteration 264, loss = 0.0008015182102099061
iteration 265, loss = 0.0009808044414967299
iteration 266, loss = 0.0006957812584005296
iteration 267, loss = 0.0008458535885438323
iteration 268, loss = 0.0006713987095281482
iteration 269, loss = 0.000625853193923831
iteration 270, loss = 0.0007493287557736039
iteration 271, loss = 0.0007827373337931931
iteration 272, loss = 0.0009605177328921854
iteration 273, loss = 0.001285844249650836
iteration 274, loss = 0.0008237608708441257
iteration 275, loss = 0.0009810252813622355
iteration 276, loss = 0.0014203533064574003
iteration 277, loss = 0.0006745086284354329
iteration 278, loss = 0.0016641505062580109
iteration 279, loss = 0.0006451835506595671
iteration 280, loss = 0.0007638521492481232
iteration 281, loss = 0.0011708308011293411
iteration 282, loss = 0.0011092936620116234
iteration 283, loss = 0.0008030864410102367
iteration 284, loss = 0.0006814069347456098
iteration 285, loss = 0.0016423370689153671
iteration 286, loss = 0.0010224981233477592
iteration 287, loss = 0.0008868568693287671
iteration 288, loss = 0.002034194068983197
iteration 289, loss = 0.001112723839469254
iteration 290, loss = 0.000914917269255966
iteration 291, loss = 0.000656226126011461
iteration 292, loss = 0.0012830907944589853
iteration 293, loss = 0.000787328346632421
iteration 294, loss = 0.0008155980031006038
iteration 295, loss = 0.0010713072260841727
iteration 296, loss = 0.0008368520066142082
iteration 297, loss = 0.0009121919283643365
iteration 298, loss = 0.0010769808432087302
iteration 299, loss = 0.0014372352743521333
iteration 300, loss = 0.0007036665920168161
iteration 1, loss = 0.0008072939235717058
iteration 2, loss = 0.0008003461407497525
iteration 3, loss = 0.0008191008819267154
iteration 4, loss = 0.0015346030704677105
iteration 5, loss = 0.0008792257867753506
iteration 6, loss = 0.0008238629088737071
iteration 7, loss = 0.0010204147547483444
iteration 8, loss = 0.0009581986232660711
iteration 9, loss = 0.0007558526122011244
iteration 10, loss = 0.0013692229986190796
iteration 11, loss = 0.0013947590487077832
iteration 12, loss = 0.000877268030308187
iteration 13, loss = 0.0009033383103087544
iteration 14, loss = 0.0006604401860386133
iteration 15, loss = 0.0006840027053840458
iteration 16, loss = 0.0011515305377542973
iteration 17, loss = 0.0007887665997259319
iteration 18, loss = 0.0008205859921872616
iteration 19, loss = 0.0007742465240880847
iteration 20, loss = 0.000833776022773236
iteration 21, loss = 0.0007377541624009609
iteration 22, loss = 0.0007283486193045974
iteration 23, loss = 0.0006968380766920745
iteration 24, loss = 0.0007566243875771761
iteration 25, loss = 0.001150705385953188
iteration 26, loss = 0.0007634153007529676
iteration 27, loss = 0.0008893713820725679
iteration 28, loss = 0.001126713352277875
iteration 29, loss = 0.0009565589134581387
iteration 30, loss = 0.0007207437884062529
iteration 31, loss = 0.0007337884744629264
iteration 32, loss = 0.0006338413222692907
iteration 33, loss = 0.000852708239108324
iteration 34, loss = 0.0007763345493003726
iteration 35, loss = 0.0007182529661804438
iteration 36, loss = 0.0007895510061644018
iteration 37, loss = 0.0008697551675140858
iteration 38, loss = 0.0008091041818261147
iteration 39, loss = 0.0008206472848542035
iteration 40, loss = 0.0008312799036502838
iteration 41, loss = 0.0008066116715781391
iteration 42, loss = 0.001297797542065382
iteration 43, loss = 0.000877914484590292
iteration 44, loss = 0.000899891834706068
iteration 45, loss = 0.0007974305190145969
iteration 46, loss = 0.0010903135407716036
iteration 47, loss = 0.0015836885431781411
iteration 48, loss = 0.0008108640904538333
iteration 49, loss = 0.0009697540663182735
iteration 50, loss = 0.0011089976178482175
iteration 51, loss = 0.0009226942202076316
iteration 52, loss = 0.0015205786330625415
iteration 53, loss = 0.0008491018670611084
iteration 54, loss = 0.001041289884597063
iteration 55, loss = 0.0009853402152657509
iteration 56, loss = 0.0007436256273649633
iteration 57, loss = 0.00083988590631634
iteration 58, loss = 0.0008341496577486396
iteration 59, loss = 0.0008025408023968339
iteration 60, loss = 0.0006017637206241488
iteration 61, loss = 0.0015215004095807672
iteration 62, loss = 0.0007974050240591168
iteration 63, loss = 0.0011220112210139632
iteration 64, loss = 0.0007620788528583944
iteration 65, loss = 0.0008333932491950691
iteration 66, loss = 0.0007455063168890774
iteration 67, loss = 0.0010895286686718464
iteration 68, loss = 0.0008059574756771326
iteration 69, loss = 0.0009525557397864759
iteration 70, loss = 0.0008315194863826036
iteration 71, loss = 0.0009418729459866881
iteration 72, loss = 0.0006855989922769368
iteration 73, loss = 0.0008615973056294024
iteration 74, loss = 0.0007384366472251713
iteration 75, loss = 0.0006668796995654702
iteration 76, loss = 0.0009156884043477476
iteration 77, loss = 0.0009075845591723919
iteration 78, loss = 0.0015786712756380439
iteration 79, loss = 0.0009270423324778676
iteration 80, loss = 0.0007841064943931997
iteration 81, loss = 0.0007455027080141008
iteration 82, loss = 0.0010182659607380629
iteration 83, loss = 0.0011836137855425477
iteration 84, loss = 0.00083264330169186
iteration 85, loss = 0.0007704433519393206
iteration 86, loss = 0.000681958335917443
iteration 87, loss = 0.0008177582640200853
iteration 88, loss = 0.0012007547775283456
iteration 89, loss = 0.000930780777707696
iteration 90, loss = 0.001004858990199864
iteration 91, loss = 0.0008228860097005963
iteration 92, loss = 0.0008737940224818885
iteration 93, loss = 0.0008132271468639374
iteration 94, loss = 0.0014381823129951954
iteration 95, loss = 0.0010457627940922976
iteration 96, loss = 0.0008265060605481267
iteration 97, loss = 0.0007671655621379614
iteration 98, loss = 0.0008913202909752727
iteration 99, loss = 0.001023273915052414
iteration 100, loss = 0.0009807305177673697
iteration 101, loss = 0.0011416388442739844
iteration 102, loss = 0.0008397437632083893
iteration 103, loss = 0.001526937703602016
iteration 104, loss = 0.0007966647390276194
iteration 105, loss = 0.0009934806730598211
iteration 106, loss = 0.001007731887511909
iteration 107, loss = 0.001372191938571632
iteration 108, loss = 0.0008356879698112607
iteration 109, loss = 0.0009599309414625168
iteration 110, loss = 0.0009199764463119209
iteration 111, loss = 0.001266948413103819
iteration 112, loss = 0.0008705880609340966
iteration 113, loss = 0.0006729723536409438
iteration 114, loss = 0.0018897239351645112
iteration 115, loss = 0.0009042465826496482
iteration 116, loss = 0.0007671263301745057
iteration 117, loss = 0.0008470236789435148
iteration 118, loss = 0.0007029046537354589
iteration 119, loss = 0.0014483619015663862
iteration 120, loss = 0.0007866578525863588
iteration 121, loss = 0.0007807291112840176
iteration 122, loss = 0.001081501366570592
iteration 123, loss = 0.0009029777138493955
iteration 124, loss = 0.0012620976194739342
iteration 125, loss = 0.0005801004590466619
iteration 126, loss = 0.0006719041266478598
iteration 127, loss = 0.0009550757240504026
iteration 128, loss = 0.0007426735246554017
iteration 129, loss = 0.0006941819447092712
iteration 130, loss = 0.0006649759015999734
iteration 131, loss = 0.0008291309932246804
iteration 132, loss = 0.0007372681866399944
iteration 133, loss = 0.0012189765693619847
iteration 134, loss = 0.0008572197402827442
iteration 135, loss = 0.0008173926617018878
iteration 136, loss = 0.0006541027105413377
iteration 137, loss = 0.0008150998619385064
iteration 138, loss = 0.0007828898960724473
iteration 139, loss = 0.0007296882104128599
iteration 140, loss = 0.000860308762639761
iteration 141, loss = 0.0007316411356441677
iteration 142, loss = 0.0009783387649804354
iteration 143, loss = 0.0007526139379478991
iteration 144, loss = 0.0008368054404854774
iteration 145, loss = 0.0008585464092902839
iteration 146, loss = 0.0012373689096421003
iteration 147, loss = 0.0008733341237530112
iteration 148, loss = 0.0015719486400485039
iteration 149, loss = 0.0008756605675444007
iteration 150, loss = 0.001391948782838881
iteration 151, loss = 0.0016773594543337822
iteration 152, loss = 0.0021844012662768364
iteration 153, loss = 0.0006742421537637711
iteration 154, loss = 0.000976412498857826
iteration 155, loss = 0.0008964050794020295
iteration 156, loss = 0.0009813288925215602
iteration 157, loss = 0.000752028136048466
iteration 158, loss = 0.0007315737893804908
iteration 159, loss = 0.0009326033759862185
iteration 160, loss = 0.0007700750720687211
iteration 161, loss = 0.0013744552852585912
iteration 162, loss = 0.0012701046653091908
iteration 163, loss = 0.001156846061348915
iteration 164, loss = 0.0007346236379817128
iteration 165, loss = 0.0006557504530064762
iteration 166, loss = 0.0009149291436187923
iteration 167, loss = 0.0008778649498708546
iteration 168, loss = 0.0010191656183451414
iteration 169, loss = 0.0007038069888949394
iteration 170, loss = 0.0011692963307723403
iteration 171, loss = 0.0009745502611622214
iteration 172, loss = 0.0007608749438077211
iteration 173, loss = 0.0009514698758721352
iteration 174, loss = 0.000743818178307265
iteration 175, loss = 0.0007658905233256519
iteration 176, loss = 0.0009187429095618427
iteration 177, loss = 0.000832786550745368
iteration 178, loss = 0.0008556752582080662
iteration 179, loss = 0.0009692389867268503
iteration 180, loss = 0.0016698233084753156
iteration 181, loss = 0.0013177132932469249
iteration 182, loss = 0.0007997805369086564
iteration 183, loss = 0.0008349331328645349
iteration 184, loss = 0.0008829400176182389
iteration 185, loss = 0.0007141113164834678
iteration 186, loss = 0.00099086610134691
iteration 187, loss = 0.0008230388048104942
iteration 188, loss = 0.0011327621759846807
iteration 189, loss = 0.0011432694736868143
iteration 190, loss = 0.0007538069621659815
iteration 191, loss = 0.0015175571897998452
iteration 192, loss = 0.0008415105403400958
iteration 193, loss = 0.0007882178761065006
iteration 194, loss = 0.0008597978739999235
iteration 195, loss = 0.0007879783515818417
iteration 196, loss = 0.0010297016706317663
iteration 197, loss = 0.0012011073995381594
iteration 198, loss = 0.0018059347057715058
iteration 199, loss = 0.0008981986902654171
iteration 200, loss = 0.000854850688483566
iteration 201, loss = 0.0014098146930336952
iteration 202, loss = 0.0007295821560546756
iteration 203, loss = 0.0008453486952930689
iteration 204, loss = 0.0007814105483703315
iteration 205, loss = 0.0008447555010206997
iteration 206, loss = 0.0008331927238032222
iteration 207, loss = 0.0016892221756279469
iteration 208, loss = 0.0011884496780112386
iteration 209, loss = 0.001419977517798543
iteration 210, loss = 0.0013345322804525495
iteration 211, loss = 0.0008061658008955419
iteration 212, loss = 0.0009466894553042948
iteration 213, loss = 0.0008260873146355152
iteration 214, loss = 0.0015627563698217273
iteration 215, loss = 0.0007509738206863403
iteration 216, loss = 0.0007912765722721815
iteration 217, loss = 0.0008447521249763668
iteration 218, loss = 0.0009567331289872527
iteration 219, loss = 0.001507211709395051
iteration 220, loss = 0.000913681578822434
iteration 221, loss = 0.0007898567127995193
iteration 222, loss = 0.0007765028276480734
iteration 223, loss = 0.000780741625931114
iteration 224, loss = 0.0007664930308237672
iteration 225, loss = 0.0008800823125056922
iteration 226, loss = 0.001097605680115521
iteration 227, loss = 0.0008028962765820324
iteration 228, loss = 0.0006885398179292679
iteration 229, loss = 0.000705599260982126
iteration 230, loss = 0.0009676286717876792
iteration 231, loss = 0.0009344889549538493
iteration 232, loss = 0.0009658659109845757
iteration 233, loss = 0.0010402656625956297
iteration 234, loss = 0.0007705036550760269
iteration 235, loss = 0.0008588705095462501
iteration 236, loss = 0.0008934442303143442
iteration 237, loss = 0.0009602054487913847
iteration 238, loss = 0.0014114126097410917
iteration 239, loss = 0.0008712475537322462
iteration 240, loss = 0.0009895922848954797
iteration 241, loss = 0.0014514580834656954
iteration 242, loss = 0.0008865540148690343
iteration 243, loss = 0.000988225918263197
iteration 244, loss = 0.0008476116345264018
iteration 245, loss = 0.0009236166952177882
iteration 246, loss = 0.0013890544651076198
iteration 247, loss = 0.0008824274991638958
iteration 248, loss = 0.0007124558323994279
iteration 249, loss = 0.0011617112904787064
iteration 250, loss = 0.0010269413469359279
iteration 251, loss = 0.0008117847610265017
iteration 252, loss = 0.0007271554786711931
iteration 253, loss = 0.0008796327747404575
iteration 254, loss = 0.0011566593311727047
iteration 255, loss = 0.0009519938612356782
iteration 256, loss = 0.0010961312800645828
iteration 257, loss = 0.0015815366059541702
iteration 258, loss = 0.0007045970414765179
iteration 259, loss = 0.0009296124335378408
iteration 260, loss = 0.0008011004538275301
iteration 261, loss = 0.0011179436696693301
iteration 262, loss = 0.0011294875293970108
iteration 263, loss = 0.0008238592417910695
iteration 264, loss = 0.0010301900329068303
iteration 265, loss = 0.0009400606504641473
iteration 266, loss = 0.000925969798117876
iteration 267, loss = 0.000870140444021672
iteration 268, loss = 0.0009343991405330598
iteration 269, loss = 0.0014769945992156863
iteration 270, loss = 0.0007985042757354677
iteration 271, loss = 0.0008280024048872292
iteration 272, loss = 0.000758269103243947
iteration 273, loss = 0.0008426641579717398
iteration 274, loss = 0.0015322176041081548
iteration 275, loss = 0.0008434515912085772
iteration 276, loss = 0.0007026549428701401
iteration 277, loss = 0.0006971422699280083
iteration 278, loss = 0.0008172639063559473
iteration 279, loss = 0.0010993435280397534
iteration 280, loss = 0.0009678988717496395
iteration 281, loss = 0.0008529365295544267
iteration 282, loss = 0.0010331653757020831
iteration 283, loss = 0.0010066055692732334
iteration 284, loss = 0.0006893021636642516
iteration 285, loss = 0.0008670362876728177
iteration 286, loss = 0.0009246434783563018
iteration 287, loss = 0.0007437919266521931
iteration 288, loss = 0.0008030847529880702
iteration 289, loss = 0.0017708346713334322
iteration 290, loss = 0.0010611078469082713
iteration 291, loss = 0.0008024969720281661
iteration 292, loss = 0.0007195306243374944
iteration 293, loss = 0.001541600446216762
iteration 294, loss = 0.0007608915911987424
iteration 295, loss = 0.0007061180076561868
iteration 296, loss = 0.0010061397915706038
iteration 297, loss = 0.0013015143340453506
iteration 298, loss = 0.0009274094481952488
iteration 299, loss = 0.0007821293547749519
iteration 300, loss = 0.0007754077087156475
iteration 1, loss = 0.0006054473342373967
iteration 2, loss = 0.0009044563630595803
iteration 3, loss = 0.0007987295975908637
iteration 4, loss = 0.0006494545377790928
iteration 5, loss = 0.0010032018180936575
iteration 6, loss = 0.0010731866350397468
iteration 7, loss = 0.0008325885282829404
iteration 8, loss = 0.0007818058365955949
iteration 9, loss = 0.0007180676911957562
iteration 10, loss = 0.0009397699031978846
iteration 11, loss = 0.0008579587447457016
iteration 12, loss = 0.001381859416142106
iteration 13, loss = 0.0010491450084373355
iteration 14, loss = 0.0009793099015951157
iteration 15, loss = 0.0008115213131532073
iteration 16, loss = 0.0009822538122534752
iteration 17, loss = 0.001054797088727355
iteration 18, loss = 0.0009266870911233127
iteration 19, loss = 0.0007235046941787004
iteration 20, loss = 0.0009875188115984201
iteration 21, loss = 0.0010800667805597186
iteration 22, loss = 0.0007970472215674818
iteration 23, loss = 0.0007833498530089855
iteration 24, loss = 0.0008254292188212276
iteration 25, loss = 0.0012862170115113258
iteration 26, loss = 0.0008752245339564979
iteration 27, loss = 0.0011674908455461264
iteration 28, loss = 0.0008063459536060691
iteration 29, loss = 0.0006903691682964563
iteration 30, loss = 0.0013671397464349866
iteration 31, loss = 0.0011432559695094824
iteration 32, loss = 0.0008407107088714838
iteration 33, loss = 0.0009796132799237967
iteration 34, loss = 0.0008536619134247303
iteration 35, loss = 0.0010695208329707384
iteration 36, loss = 0.0007940809591673315
iteration 37, loss = 0.0011103153228759766
iteration 38, loss = 0.0008857515640556812
iteration 39, loss = 0.0007851801929064095
iteration 40, loss = 0.0017474560299888253
iteration 41, loss = 0.0008521431591361761
iteration 42, loss = 0.0006534341955557466
iteration 43, loss = 0.0007036503520794213
iteration 44, loss = 0.001195818418636918
iteration 45, loss = 0.0006704662810079753
iteration 46, loss = 0.0011108614271506667
iteration 47, loss = 0.0007770585361868143
iteration 48, loss = 0.0009729023440741003
iteration 49, loss = 0.001384129747748375
iteration 50, loss = 0.0007793703116476536
iteration 51, loss = 0.0006261553498916328
iteration 52, loss = 0.0007577731157653034
iteration 53, loss = 0.0009661231888458133
iteration 54, loss = 0.0007342441822402179
iteration 55, loss = 0.0009053690009750426
iteration 56, loss = 0.000821551016997546
iteration 57, loss = 0.0011271833209320903
iteration 58, loss = 0.000699284253641963
iteration 59, loss = 0.0007573458715341985
iteration 60, loss = 0.0008081041742116213
iteration 61, loss = 0.000996953109279275
iteration 62, loss = 0.0007550960872322321
iteration 63, loss = 0.0009409569902345538
iteration 64, loss = 0.0009551726398058236
iteration 65, loss = 0.000837845669593662
iteration 66, loss = 0.0008084637811407447
iteration 67, loss = 0.0009981512557715178
iteration 68, loss = 0.0006671294686384499
iteration 69, loss = 0.000999214593321085
iteration 70, loss = 0.0012799364048987627
iteration 71, loss = 0.001002721255645156
iteration 72, loss = 0.0008521286654286087
iteration 73, loss = 0.0008817568887025118
iteration 74, loss = 0.0010297191329300404
iteration 75, loss = 0.0015005329623818398
iteration 76, loss = 0.0009347329614683986
iteration 77, loss = 0.000785374897532165
iteration 78, loss = 0.0007219779072329402
iteration 79, loss = 0.0006648984272032976
iteration 80, loss = 0.0013856770237907767
iteration 81, loss = 0.0007925281533971429
iteration 82, loss = 0.0008507604361511767
iteration 83, loss = 0.0010074819438159466
iteration 84, loss = 0.0010813558474183083
iteration 85, loss = 0.0008027611183933914
iteration 86, loss = 0.0007207082817330956
iteration 87, loss = 0.001010397681966424
iteration 88, loss = 0.000983978621661663
iteration 89, loss = 0.0008796964539214969
iteration 90, loss = 0.0007399679743684828
iteration 91, loss = 0.000689842680003494
iteration 92, loss = 0.0010079388739541173
iteration 93, loss = 0.0007377032889053226
iteration 94, loss = 0.0014032010221853852
iteration 95, loss = 0.000818313448689878
iteration 96, loss = 0.0007330043590627611
iteration 97, loss = 0.0008568221237510443
iteration 98, loss = 0.0014169225469231606
iteration 99, loss = 0.0010497012408450246
iteration 100, loss = 0.0009613976581022143
iteration 101, loss = 0.000951808353420347
iteration 102, loss = 0.0007512254524044693
iteration 103, loss = 0.0009283903054893017
iteration 104, loss = 0.0007753242971375585
iteration 105, loss = 0.0008396910852752626
iteration 106, loss = 0.0010153638431802392
iteration 107, loss = 0.0009201683569699526
iteration 108, loss = 0.0011383191449567676
iteration 109, loss = 0.0006415889947675169
iteration 110, loss = 0.0008595476392656565
iteration 111, loss = 0.0014177380362525582
iteration 112, loss = 0.0008409877773374319
iteration 113, loss = 0.0008038005908019841
iteration 114, loss = 0.0007129996665753424
iteration 115, loss = 0.0009176725870929658
iteration 116, loss = 0.001011911896057427
iteration 117, loss = 0.0009564072242937982
iteration 118, loss = 0.0008708797977305949
iteration 119, loss = 0.0014042436378076673
iteration 120, loss = 0.0008175502298399806
iteration 121, loss = 0.0006812368519604206
iteration 122, loss = 0.0007394690765067935
iteration 123, loss = 0.0007564336410723627
iteration 124, loss = 0.0008159787976182997
iteration 125, loss = 0.0012471020454540849
iteration 126, loss = 0.0010640769032761455
iteration 127, loss = 0.000716645154170692
iteration 128, loss = 0.000881792395375669
iteration 129, loss = 0.0008160314755514264
iteration 130, loss = 0.0008346896502189338
iteration 131, loss = 0.0008749180706217885
iteration 132, loss = 0.0009949026862159371
iteration 133, loss = 0.002208143938332796
iteration 134, loss = 0.0008087542373687029
iteration 135, loss = 0.0006966047803871334
iteration 136, loss = 0.000840995809994638
iteration 137, loss = 0.0010844606440514326
iteration 138, loss = 0.0010838140733540058
iteration 139, loss = 0.0007213799399323761
iteration 140, loss = 0.0008785687386989594
iteration 141, loss = 0.0008100708946585655
iteration 142, loss = 0.0012094711419194937
iteration 143, loss = 0.0009410208440385759
iteration 144, loss = 0.001086830161511898
iteration 145, loss = 0.0014071031473577023
iteration 146, loss = 0.0006887195631861687
iteration 147, loss = 0.0008130736532621086
iteration 148, loss = 0.0008031856850720942
iteration 149, loss = 0.0016349208308383822
iteration 150, loss = 0.0008394633186981082
iteration 151, loss = 0.0007538969512097538
iteration 152, loss = 0.0012061384040862322
iteration 153, loss = 0.0007150387391448021
iteration 154, loss = 0.0011543304426595569
iteration 155, loss = 0.0008189380750991404
iteration 156, loss = 0.0009724535630084574
iteration 157, loss = 0.0012589230900630355
iteration 158, loss = 0.0009799094405025244
iteration 159, loss = 0.0015975682763382792
iteration 160, loss = 0.0008022662368603051
iteration 161, loss = 0.0009738720254972577
iteration 162, loss = 0.0008759939228184521
iteration 163, loss = 0.000838247244246304
iteration 164, loss = 0.002526070922613144
iteration 165, loss = 0.0007970088627189398
iteration 166, loss = 0.0007393918931484222
iteration 167, loss = 0.0014056067448109388
iteration 168, loss = 0.0008594420505687594
iteration 169, loss = 0.000768091413192451
iteration 170, loss = 0.0009162285714410245
iteration 171, loss = 0.0007676618406549096
iteration 172, loss = 0.0007832584669813514
iteration 173, loss = 0.0006989285466261208
iteration 174, loss = 0.0010335836559534073
iteration 175, loss = 0.00064614147413522
iteration 176, loss = 0.0008769526612013578
iteration 177, loss = 0.0009952664840966463
iteration 178, loss = 0.001542304758913815
iteration 179, loss = 0.000831147248391062
iteration 180, loss = 0.0012791510671377182
iteration 181, loss = 0.00086624437244609
iteration 182, loss = 0.0006873338134028018
iteration 183, loss = 0.0007423252100124955
iteration 184, loss = 0.0007895496673882008
iteration 185, loss = 0.0007207765011116862
iteration 186, loss = 0.0007780360174365342
iteration 187, loss = 0.0006998522439971566
iteration 188, loss = 0.0009389393962919712
iteration 189, loss = 0.0006888324860483408
iteration 190, loss = 0.001054476830177009
iteration 191, loss = 0.0006400846759788692
iteration 192, loss = 0.00072914466727525
iteration 193, loss = 0.0007120772497728467
iteration 194, loss = 0.0008528819889761508
iteration 195, loss = 0.0015679319621995091
iteration 196, loss = 0.0008575108367949724
iteration 197, loss = 0.0017269490053877234
iteration 198, loss = 0.0007676534005440772
iteration 199, loss = 0.001829010434448719
iteration 200, loss = 0.0011148665798828006
iteration 201, loss = 0.000703084806445986
iteration 202, loss = 0.0007337996503338218
iteration 203, loss = 0.0008699797326698899
iteration 204, loss = 0.0009207598632201552
iteration 205, loss = 0.0007702622679062188
iteration 206, loss = 0.0008634148398414254
iteration 207, loss = 0.0008436606731265783
iteration 208, loss = 0.0007905285456217825
iteration 209, loss = 0.0007213607896119356
iteration 210, loss = 0.0006320629618130624
iteration 211, loss = 0.0009151270496658981
iteration 212, loss = 0.0009737818036228418
iteration 213, loss = 0.0011580382706597447
iteration 214, loss = 0.0007178853265941143
iteration 215, loss = 0.0009508326184004545
iteration 216, loss = 0.001020906143821776
iteration 217, loss = 0.0008451198227703571
iteration 218, loss = 0.0008315207669511437
iteration 219, loss = 0.0008038145606406033
iteration 220, loss = 0.0015824445290490985
iteration 221, loss = 0.0017065810970962048
iteration 222, loss = 0.0009159579640254378
iteration 223, loss = 0.0007164432900026441
iteration 224, loss = 0.0017594652017578483
iteration 225, loss = 0.0011328181717544794
iteration 226, loss = 0.0009392961510457098
iteration 227, loss = 0.0006719630328007042
iteration 228, loss = 0.0007612745393998921
iteration 229, loss = 0.0006915121921338141
iteration 230, loss = 0.0009190397104248405
iteration 231, loss = 0.0008538412512280047
iteration 232, loss = 0.0007734365062788129
iteration 233, loss = 0.0007236148812808096
iteration 234, loss = 0.0009333050693385303
iteration 235, loss = 0.0013219666434451938
iteration 236, loss = 0.0009108699159696698
iteration 237, loss = 0.0010246378369629383
iteration 238, loss = 0.0015390017069876194
iteration 239, loss = 0.0007535419426858425
iteration 240, loss = 0.0010098632192239165
iteration 241, loss = 0.000735504028853029
iteration 242, loss = 0.00125805102288723
iteration 243, loss = 0.0007589926826767623
iteration 244, loss = 0.0006906432681716979
iteration 245, loss = 0.0009519708110019565
iteration 246, loss = 0.0008296400774270296
iteration 247, loss = 0.0009195812162943184
iteration 248, loss = 0.0007680717972107232
iteration 249, loss = 0.0012436341494321823
iteration 250, loss = 0.0007694970117881894
iteration 251, loss = 0.001157328486442566
iteration 252, loss = 0.0008791809668764472
iteration 253, loss = 0.0009541811305098236
iteration 254, loss = 0.0010305900359526277
iteration 255, loss = 0.0007985559641383588
iteration 256, loss = 0.0009965738281607628
iteration 257, loss = 0.0008286350057460368
iteration 258, loss = 0.0007949377177283168
iteration 259, loss = 0.0007360687013715506
iteration 260, loss = 0.0008831744780763984
iteration 261, loss = 0.0008411227026954293
iteration 262, loss = 0.0007364372722804546
iteration 263, loss = 0.0008651586249470711
iteration 264, loss = 0.0008194804540835321
iteration 265, loss = 0.0008774442248977721
iteration 266, loss = 0.0008378103375434875
iteration 267, loss = 0.001123456982895732
iteration 268, loss = 0.0007990205194801092
iteration 269, loss = 0.00193480821326375
iteration 270, loss = 0.0009115150896832347
iteration 271, loss = 0.0006667070556432009
iteration 272, loss = 0.0016210507601499557
iteration 273, loss = 0.0008406133274547756
iteration 274, loss = 0.0008805313846096396
iteration 275, loss = 0.0009711477905511856
iteration 276, loss = 0.0015730715822428465
iteration 277, loss = 0.0006465225014835596
iteration 278, loss = 0.0008402396342717111
iteration 279, loss = 0.0009702134411782026
iteration 280, loss = 0.0010427141096442938
iteration 281, loss = 0.0012497389689087868
iteration 282, loss = 0.0010418201563879848
iteration 283, loss = 0.0006804659496992826
iteration 284, loss = 0.0014759666519239545
iteration 285, loss = 0.0007525234250351787
iteration 286, loss = 0.0019413549453020096
iteration 287, loss = 0.0006316557992249727
iteration 288, loss = 0.0009259290527552366
iteration 289, loss = 0.001831410569138825
iteration 290, loss = 0.0007995454361662269
iteration 291, loss = 0.000839458720292896
iteration 292, loss = 0.0007611648179590702
iteration 293, loss = 0.0007446013041771948
iteration 294, loss = 0.0011705270735546947
iteration 295, loss = 0.0006690064910799265
iteration 296, loss = 0.000876262376550585
iteration 297, loss = 0.0007637428934685886
iteration 298, loss = 0.0011418326757848263
iteration 299, loss = 0.0013740239664912224
iteration 300, loss = 0.0009430464706383646
iteration 1, loss = 0.0008281473419629037
iteration 2, loss = 0.0010682085994631052
iteration 3, loss = 0.0008693884592503309
iteration 4, loss = 0.001495912903919816
iteration 5, loss = 0.0009088421356864274
iteration 6, loss = 0.0009497745195403695
iteration 7, loss = 0.0011484912829473615
iteration 8, loss = 0.0011903248960152268
iteration 9, loss = 0.0008348197443410754
iteration 10, loss = 0.0007451103301718831
iteration 11, loss = 0.0008803536766208708
iteration 12, loss = 0.0007362504838965833
iteration 13, loss = 0.0006741754477843642
iteration 14, loss = 0.0010211281478404999
iteration 15, loss = 0.0008027713047340512
iteration 16, loss = 0.0008149482309818268
iteration 17, loss = 0.0013343130704015493
iteration 18, loss = 0.000912596529815346
iteration 19, loss = 0.0014083170099183917
iteration 20, loss = 0.0006954205455258489
iteration 21, loss = 0.0007253422518260777
iteration 22, loss = 0.0007959194481372833
iteration 23, loss = 0.0008842621464282274
iteration 24, loss = 0.0007569438312202692
iteration 25, loss = 0.0010454620933160186
iteration 26, loss = 0.001025947742164135
iteration 27, loss = 0.0008061801781877875
iteration 28, loss = 0.000792493112385273
iteration 29, loss = 0.0008856626227498055
iteration 30, loss = 0.0007432710262946784
iteration 31, loss = 0.0007708570919930935
iteration 32, loss = 0.0008004684932529926
iteration 33, loss = 0.001388268661685288
iteration 34, loss = 0.0007517726044170558
iteration 35, loss = 0.001268230495043099
iteration 36, loss = 0.0010211494518443942
iteration 37, loss = 0.00112058874219656
iteration 38, loss = 0.0007934719324111938
iteration 39, loss = 0.0011519257677718997
iteration 40, loss = 0.0009118006564676762
iteration 41, loss = 0.0010864336509257555
iteration 42, loss = 0.001349920523352921
iteration 43, loss = 0.00077266019070521
iteration 44, loss = 0.001656003063544631
iteration 45, loss = 0.0006992085254751146
iteration 46, loss = 0.0007016602903604507
iteration 47, loss = 0.0008580993162468076
iteration 48, loss = 0.0011678575538098812
iteration 49, loss = 0.000729512597899884
iteration 50, loss = 0.0016086504328995943
iteration 51, loss = 0.0006881639710627496
iteration 52, loss = 0.0008768177940510213
iteration 53, loss = 0.0011201717425137758
iteration 54, loss = 0.0008049851167015731
iteration 55, loss = 0.0008261483162641525
iteration 56, loss = 0.0008283824427053332
iteration 57, loss = 0.0021878345869481564
iteration 58, loss = 0.0006673309835605323
iteration 59, loss = 0.0006496862042695284
iteration 60, loss = 0.0017970402259379625
iteration 61, loss = 0.0008532070787623525
iteration 62, loss = 0.0015007311012595892
iteration 63, loss = 0.00133886921685189
iteration 64, loss = 0.0008269111858680844
iteration 65, loss = 0.0013989376602694392
iteration 66, loss = 0.0007764699403196573
iteration 67, loss = 0.0007275996031239629
iteration 68, loss = 0.0007838066667318344
iteration 69, loss = 0.0008616151753813028
iteration 70, loss = 0.0007762804161757231
iteration 71, loss = 0.00085233966819942
iteration 72, loss = 0.0006194483721628785
iteration 73, loss = 0.0008109378395602107
iteration 74, loss = 0.0008904098649509251
iteration 75, loss = 0.0015828191535547376
iteration 76, loss = 0.0007234009099192917
iteration 77, loss = 0.0013901189668104053
iteration 78, loss = 0.0008014952763915062
iteration 79, loss = 0.0006787588936276734
iteration 80, loss = 0.0008160177967511117
iteration 81, loss = 0.0007940463838167489
iteration 82, loss = 0.000869736191816628
iteration 83, loss = 0.0008362236549146473
iteration 84, loss = 0.0007714629755355418
iteration 85, loss = 0.000834418460726738
iteration 86, loss = 0.0013343424070626497
iteration 87, loss = 0.0007795640267431736
iteration 88, loss = 0.0012473473325371742
iteration 89, loss = 0.0009026137413457036
iteration 90, loss = 0.0008381493971683085
iteration 91, loss = 0.0010304959723725915
iteration 92, loss = 0.0009745894349180162
iteration 93, loss = 0.0009246810805052519
iteration 94, loss = 0.0011246383655816317
iteration 95, loss = 0.0008778953342698514
iteration 96, loss = 0.0008597385603934526
iteration 97, loss = 0.0011031113099306822
iteration 98, loss = 0.0012852996587753296
iteration 99, loss = 0.0008126156171783805
iteration 100, loss = 0.0008011969039216638
iteration 101, loss = 0.0008034371421672404
iteration 102, loss = 0.0011716013541445136
iteration 103, loss = 0.0008763829246163368
iteration 104, loss = 0.0006841275608167052
iteration 105, loss = 0.0007511526346206665
iteration 106, loss = 0.001073980238288641
iteration 107, loss = 0.0010687001049518585
iteration 108, loss = 0.0007838563178665936
iteration 109, loss = 0.0007482115179300308
iteration 110, loss = 0.0006399646517820656
iteration 111, loss = 0.0008546743192709982
iteration 112, loss = 0.0007660784176550806
iteration 113, loss = 0.0009632115834392607
iteration 114, loss = 0.0007881560595706105
iteration 115, loss = 0.0008295848965644836
iteration 116, loss = 0.0009354704525321722
iteration 117, loss = 0.0008063799468800426
iteration 118, loss = 0.0014375053578987718
iteration 119, loss = 0.0012414628872647882
iteration 120, loss = 0.0007835684227757156
iteration 121, loss = 0.0010842764750123024
iteration 122, loss = 0.0008551538921892643
iteration 123, loss = 0.0006056412239558995
iteration 124, loss = 0.0007814767304807901
iteration 125, loss = 0.0008694055723026395
iteration 126, loss = 0.0016646228032186627
iteration 127, loss = 0.0009458131389692426
iteration 128, loss = 0.0006932279793545604
iteration 129, loss = 0.0009286171407438815
iteration 130, loss = 0.0009554636781103909
iteration 131, loss = 0.0011481372639536858
iteration 132, loss = 0.0010453538270667195
iteration 133, loss = 0.0007177669322118163
iteration 134, loss = 0.0013191169127821922
iteration 135, loss = 0.0011335390154272318
iteration 136, loss = 0.001031518797390163
iteration 137, loss = 0.0008229576633311808
iteration 138, loss = 0.0008548840996809304
iteration 139, loss = 0.0010841406183317304
iteration 140, loss = 0.0010199612006545067
iteration 141, loss = 0.0011103750439360738
iteration 142, loss = 0.0009602766367606819
iteration 143, loss = 0.0008942816057242453
iteration 144, loss = 0.0010395037243142724
iteration 145, loss = 0.0011123758740723133
iteration 146, loss = 0.001065572490915656
iteration 147, loss = 0.0009016551775857806
iteration 148, loss = 0.000793105864431709
iteration 149, loss = 0.0007887139217928052
iteration 150, loss = 0.0008681223844178021
iteration 151, loss = 0.0008713098359294236
iteration 152, loss = 0.0008998135454021394
iteration 153, loss = 0.0017346773529425263
iteration 154, loss = 0.0011321034980937839
iteration 155, loss = 0.0008997284458018839
iteration 156, loss = 0.0008594231330789626
iteration 157, loss = 0.0008409306174144149
iteration 158, loss = 0.0008571409853175282
iteration 159, loss = 0.0007985604461282492
iteration 160, loss = 0.001128652598708868
iteration 161, loss = 0.0011376304319128394
iteration 162, loss = 0.0010116664925590158
iteration 163, loss = 0.0010267645120620728
iteration 164, loss = 0.0007584701525047421
iteration 165, loss = 0.0009542439947836101
iteration 166, loss = 0.0010506917024031281
iteration 167, loss = 0.0007438694010488689
iteration 168, loss = 0.0009293650509789586
iteration 169, loss = 0.0011330186389386654
iteration 170, loss = 0.0010760738514363766
iteration 171, loss = 0.0007635889342054725
iteration 172, loss = 0.0011344356462359428
iteration 173, loss = 0.0008469271706417203
iteration 174, loss = 0.0008676315774209797
iteration 175, loss = 0.0015862449072301388
iteration 176, loss = 0.0010317807318642735
iteration 177, loss = 0.0007563599501736462
iteration 178, loss = 0.0010729215573519468
iteration 179, loss = 0.0009084193734452128
iteration 180, loss = 0.0011176395928487182
iteration 181, loss = 0.0008502161363139749
iteration 182, loss = 0.0009725653217174113
iteration 183, loss = 0.0008848949219100177
iteration 184, loss = 0.0010384777560830116
iteration 185, loss = 0.0007828273228369653
iteration 186, loss = 0.0007769944495521486
iteration 187, loss = 0.0008136054966598749
iteration 188, loss = 0.0007382497424259782
iteration 189, loss = 0.0006225458928383887
iteration 190, loss = 0.0008887213189154863
iteration 191, loss = 0.0008049376192502677
iteration 192, loss = 0.0008125550812110305
iteration 193, loss = 0.0006605249363929033
iteration 194, loss = 0.0010533931199461222
iteration 195, loss = 0.0007779754814691842
iteration 196, loss = 0.000974565336946398
iteration 197, loss = 0.000958173768594861
iteration 198, loss = 0.0008056871010921896
iteration 199, loss = 0.0007863526698201895
iteration 200, loss = 0.0010195127688348293
iteration 201, loss = 0.0007579615339636803
iteration 202, loss = 0.000864675675984472
iteration 203, loss = 0.0009596230229362845
iteration 204, loss = 0.0010958104394376278
iteration 205, loss = 0.000662678387016058
iteration 206, loss = 0.0008195416885428131
iteration 207, loss = 0.0014123644214123487
iteration 208, loss = 0.0009832618525251746
iteration 209, loss = 0.0008179998840205371
iteration 210, loss = 0.0009140935144387186
iteration 211, loss = 0.0008617258281446993
iteration 212, loss = 0.0007541027152910829
iteration 213, loss = 0.0008459436940029263
iteration 214, loss = 0.001067760051228106
iteration 215, loss = 0.0009988403180614114
iteration 216, loss = 0.0016096346080303192
iteration 217, loss = 0.0014341658679768443
iteration 218, loss = 0.0015827843453735113
iteration 219, loss = 0.0009526595822535455
iteration 220, loss = 0.0009247962734661996
iteration 221, loss = 0.0015083737671375275
iteration 222, loss = 0.0008152337977662683
iteration 223, loss = 0.0007507064728997648
iteration 224, loss = 0.0010919062187895179
iteration 225, loss = 0.0008285493240691721
iteration 226, loss = 0.0007178944651968777
iteration 227, loss = 0.0007133176550269127
iteration 228, loss = 0.0008925892179831862
iteration 229, loss = 0.0009106573415920138
iteration 230, loss = 0.0012077133869752288
iteration 231, loss = 0.0010008239187300205
iteration 232, loss = 0.001154051860794425
iteration 233, loss = 0.0006568852113559842
iteration 234, loss = 0.0012224038364365697
iteration 235, loss = 0.0009997488232329488
iteration 236, loss = 0.0008300021290779114
iteration 237, loss = 0.0009354791254736483
iteration 238, loss = 0.0008835943299345672
iteration 239, loss = 0.0010005957446992397
iteration 240, loss = 0.0007367398357018828
iteration 241, loss = 0.0014377993065863848
iteration 242, loss = 0.00158505083527416
iteration 243, loss = 0.0007591582252644002
iteration 244, loss = 0.0008310042903758585
iteration 245, loss = 0.0007301827426999807
iteration 246, loss = 0.0007496093166992068
iteration 247, loss = 0.001017264905385673
iteration 248, loss = 0.0008636155398562551
iteration 249, loss = 0.0008868153672665358
iteration 250, loss = 0.0008662024629302323
iteration 251, loss = 0.0016010516555979848
iteration 252, loss = 0.0007446518284268677
iteration 253, loss = 0.0009285649284720421
iteration 254, loss = 0.0007413291605189443
iteration 255, loss = 0.0009316399809904397
iteration 256, loss = 0.0008794079185463488
iteration 257, loss = 0.0009424318559467793
iteration 258, loss = 0.0006743160774931312
iteration 259, loss = 0.0008616915438324213
iteration 260, loss = 0.0010076446924358606
iteration 261, loss = 0.0008214628323912621
iteration 262, loss = 0.0014936269726604223
iteration 263, loss = 0.0009158519096672535
iteration 264, loss = 0.0009252220625057817
iteration 265, loss = 0.0008437926298938692
iteration 266, loss = 0.0008841222734190524
iteration 267, loss = 0.000823626818601042
iteration 268, loss = 0.0007748355274088681
iteration 269, loss = 0.0007566724088974297
iteration 270, loss = 0.0007475322345271707
iteration 271, loss = 0.0007844142382964492
iteration 272, loss = 0.0008228312944993377
iteration 273, loss = 0.0007025062805041671
iteration 274, loss = 0.0006979363388381898
iteration 275, loss = 0.0007992889732122421
iteration 276, loss = 0.0015727468999102712
iteration 277, loss = 0.0007147005526348948
iteration 278, loss = 0.0008443181868642569
iteration 279, loss = 0.0008219447336159647
iteration 280, loss = 0.0007993343751877546
iteration 281, loss = 0.000905091583263129
iteration 282, loss = 0.0008318526670336723
iteration 283, loss = 0.0012744697742164135
iteration 284, loss = 0.0008534182561561465
iteration 285, loss = 0.0007549047004431486
iteration 286, loss = 0.0015677971532568336
iteration 287, loss = 0.0007445907103829086
iteration 288, loss = 0.001014235196635127
iteration 289, loss = 0.0007051018183119595
iteration 290, loss = 0.0006539853638969362
iteration 291, loss = 0.0017583563458174467
iteration 292, loss = 0.0007265937165357172
iteration 293, loss = 0.0009775653015822172
iteration 294, loss = 0.0006758146337233484
iteration 295, loss = 0.0006688150460831821
iteration 296, loss = 0.000763559015467763
iteration 297, loss = 0.0013283833395689726
iteration 298, loss = 0.0009481427259743214
iteration 299, loss = 0.0007493656594306231
iteration 300, loss = 0.0009283240651711822
iteration 1, loss = 0.001042793970555067
iteration 2, loss = 0.0011138657573610544
iteration 3, loss = 0.0010756837436929345
iteration 4, loss = 0.0009352805791422725
iteration 5, loss = 0.0008945035515353084
iteration 6, loss = 0.0008207762730307877
iteration 7, loss = 0.0010999070946127176
iteration 8, loss = 0.001077938824892044
iteration 9, loss = 0.0014050410827621818
iteration 10, loss = 0.0007191994809545577
iteration 11, loss = 0.0009442558512091637
iteration 12, loss = 0.0013905814848840237
iteration 13, loss = 0.0015768681187182665
iteration 14, loss = 0.0014635330298915505
iteration 15, loss = 0.0008004637784324586
iteration 16, loss = 0.001389837940223515
iteration 17, loss = 0.0009021431906148791
iteration 18, loss = 0.0011519904946908355
iteration 19, loss = 0.0008232588297687471
iteration 20, loss = 0.0015145258512347937
iteration 21, loss = 0.0009183958172798157
iteration 22, loss = 0.0024535234551876783
iteration 23, loss = 0.0008058537496253848
iteration 24, loss = 0.0006892468663863838
iteration 25, loss = 0.000948212924413383
iteration 26, loss = 0.0008770072017796338
iteration 27, loss = 0.0011957860551774502
iteration 28, loss = 0.0012378798564895988
iteration 29, loss = 0.0007734449463896453
iteration 30, loss = 0.00129748007748276
iteration 31, loss = 0.000856970262248069
iteration 32, loss = 0.0007276713731698692
iteration 33, loss = 0.0007316230912692845
iteration 34, loss = 0.0007800051243975759
iteration 35, loss = 0.0013643117854371667
iteration 36, loss = 0.0009160526096820831
iteration 37, loss = 0.0010050188284367323
iteration 38, loss = 0.0008139638230204582
iteration 39, loss = 0.0007281717844307423
iteration 40, loss = 0.0017941948026418686
iteration 41, loss = 0.0008132415823638439
iteration 42, loss = 0.0008316317689605057
iteration 43, loss = 0.0009972505504265428
iteration 44, loss = 0.0008952193311415613
iteration 45, loss = 0.0010237116366624832
iteration 46, loss = 0.0009827280882745981
iteration 47, loss = 0.000767159799579531
iteration 48, loss = 0.001294995890930295
iteration 49, loss = 0.0010042341891676188
iteration 50, loss = 0.0009041387820616364
iteration 51, loss = 0.0008445227867923677
iteration 52, loss = 0.001207237713970244
iteration 53, loss = 0.0007814868586137891
iteration 54, loss = 0.0007174190832301974
iteration 55, loss = 0.000885845918674022
iteration 56, loss = 0.0007443471695296466
iteration 57, loss = 0.001092835096642375
iteration 58, loss = 0.0009525134228169918
iteration 59, loss = 0.0011441025417298079
iteration 60, loss = 0.0011283298954367638
iteration 61, loss = 0.0007644451688975096
iteration 62, loss = 0.0007448149262927473
iteration 63, loss = 0.0008594637620262802
iteration 64, loss = 0.001575653674080968
iteration 65, loss = 0.0009780069813132286
iteration 66, loss = 0.0006985837826505303
iteration 67, loss = 0.0008107126923277974
iteration 68, loss = 0.0008604912436567247
iteration 69, loss = 0.0007536925259046257
iteration 70, loss = 0.0008680890314280987
iteration 71, loss = 0.000719305535312742
iteration 72, loss = 0.0008807377307675779
iteration 73, loss = 0.0008747553802095354
iteration 74, loss = 0.0016996022313833237
iteration 75, loss = 0.0009099769522435963
iteration 76, loss = 0.000738196074962616
iteration 77, loss = 0.0008684484055265784
iteration 78, loss = 0.0008396405028179288
iteration 79, loss = 0.0010186415165662766
iteration 80, loss = 0.000806943338830024
iteration 81, loss = 0.0008522205171175301
iteration 82, loss = 0.0007852815324440598
iteration 83, loss = 0.0008208173676393926
iteration 84, loss = 0.0008334378362633288
iteration 85, loss = 0.0014352056896314025
iteration 86, loss = 0.0008016175124794245
iteration 87, loss = 0.0007818726589903235
iteration 88, loss = 0.0008522892603650689
iteration 89, loss = 0.0006597400060854852
iteration 90, loss = 0.0011411631712689996
iteration 91, loss = 0.0010481657227501273
iteration 92, loss = 0.0016194939380511642
iteration 93, loss = 0.0008785516256466508
iteration 94, loss = 0.000609865237493068
iteration 95, loss = 0.0007951812003739178
iteration 96, loss = 0.0007906281389296055
iteration 97, loss = 0.0007238970138132572
iteration 98, loss = 0.0009462535963393748
iteration 99, loss = 0.0008036615327000618
iteration 100, loss = 0.0007581101963296533
iteration 101, loss = 0.0008246478973887861
iteration 102, loss = 0.0009956291178241372
iteration 103, loss = 0.0008035088540054858
iteration 104, loss = 0.0007689633639529347
iteration 105, loss = 0.0008689226815477014
iteration 106, loss = 0.0011414189357310534
iteration 107, loss = 0.0007543949177488685
iteration 108, loss = 0.0007129377918317914
iteration 109, loss = 0.0007403628551401198
iteration 110, loss = 0.001068837707862258
iteration 111, loss = 0.000885804183781147
iteration 112, loss = 0.0012514792615547776
iteration 113, loss = 0.0010995324701070786
iteration 114, loss = 0.0008625774062238634
iteration 115, loss = 0.0008942742715589702
iteration 116, loss = 0.0006831064238213003
iteration 117, loss = 0.001092011807486415
iteration 118, loss = 0.0009281793609261513
iteration 119, loss = 0.0010532200103625655
iteration 120, loss = 0.0008262181072495878
iteration 121, loss = 0.0012948703952133656
iteration 122, loss = 0.0011026045540347695
iteration 123, loss = 0.0008052043849602342
iteration 124, loss = 0.001017297967337072
iteration 125, loss = 0.0008926090085878968
iteration 126, loss = 0.0008557713008485734
iteration 127, loss = 0.0007341349264606833
iteration 128, loss = 0.0008724163635633886
iteration 129, loss = 0.0008615453843958676
iteration 130, loss = 0.0013852170668542385
iteration 131, loss = 0.0007729049539193511
iteration 132, loss = 0.0007817193400114775
iteration 133, loss = 0.0009313386981375515
iteration 134, loss = 0.0014782851794734597
iteration 135, loss = 0.0009511965909041464
iteration 136, loss = 0.0010653460631147027
iteration 137, loss = 0.0008084374130703509
iteration 138, loss = 0.0007843922940082848
iteration 139, loss = 0.0009253125172108412
iteration 140, loss = 0.0007834337884560227
iteration 141, loss = 0.000870955758728087
iteration 142, loss = 0.0017831448931246996
iteration 143, loss = 0.0007555881165899336
iteration 144, loss = 0.0008182581514120102
iteration 145, loss = 0.001002211356535554
iteration 146, loss = 0.0016394146950915456
iteration 147, loss = 0.0016143685206770897
iteration 148, loss = 0.0010559353977441788
iteration 149, loss = 0.0009565912187099457
iteration 150, loss = 0.0008093635551631451
iteration 151, loss = 0.0008340534986928105
iteration 152, loss = 0.0006763405399397016
iteration 153, loss = 0.000767428835388273
iteration 154, loss = 0.0013763993047177792
iteration 155, loss = 0.0007615122012794018
iteration 156, loss = 0.0008051818003877997
iteration 157, loss = 0.0007310483488254249
iteration 158, loss = 0.0007827938534319401
iteration 159, loss = 0.0007966576376929879
iteration 160, loss = 0.0015913039678707719
iteration 161, loss = 0.0013668154133483768
iteration 162, loss = 0.0009559256723150611
iteration 163, loss = 0.0011287964880466461
iteration 164, loss = 0.0010641426779329777
iteration 165, loss = 0.0006871473742648959
iteration 166, loss = 0.0017008581198751926
iteration 167, loss = 0.0009387601166963577
iteration 168, loss = 0.0008747999090701342
iteration 169, loss = 0.000946486834436655
iteration 170, loss = 0.000956611183937639
iteration 171, loss = 0.0006918478757143021
iteration 172, loss = 0.0010107157286256552
iteration 173, loss = 0.0008200778975151479
iteration 174, loss = 0.0013608557637780905
iteration 175, loss = 0.0009442883892916143
iteration 176, loss = 0.0007256423705257475
iteration 177, loss = 0.0007445169030688703
iteration 178, loss = 0.0008358687628060579
iteration 179, loss = 0.0007171669276431203
iteration 180, loss = 0.0007347280625253916
iteration 181, loss = 0.0005930039333179593
iteration 182, loss = 0.0007658166578039527
iteration 183, loss = 0.0007471748976968229
iteration 184, loss = 0.0009799328399822116
iteration 185, loss = 0.0008235975401476026
iteration 186, loss = 0.0006588990218006074
iteration 187, loss = 0.0007383059128187597
iteration 188, loss = 0.0015083423350006342
iteration 189, loss = 0.0007589111919514835
iteration 190, loss = 0.0008841065573506057
iteration 191, loss = 0.0008003113325685263
iteration 192, loss = 0.0015763973351567984
iteration 193, loss = 0.0008003709372133017
iteration 194, loss = 0.0009674851899035275
iteration 195, loss = 0.0009483571629971266
iteration 196, loss = 0.0009409355116076767
iteration 197, loss = 0.0007318775169551373
iteration 198, loss = 0.0007442630594596267
iteration 199, loss = 0.0007583056576550007
iteration 200, loss = 0.0009841154096648097
iteration 201, loss = 0.000831651734188199
iteration 202, loss = 0.0011848837602883577
iteration 203, loss = 0.0007801755564287305
iteration 204, loss = 0.0007916265167295933
iteration 205, loss = 0.0012768986634910107
iteration 206, loss = 0.000912825227715075
iteration 207, loss = 0.0008090846240520477
iteration 208, loss = 0.0014587127370759845
iteration 209, loss = 0.0007161215180531144
iteration 210, loss = 0.0011009573936462402
iteration 211, loss = 0.0008344501839019358
iteration 212, loss = 0.0009848559275269508
iteration 213, loss = 0.00078062858665362
iteration 214, loss = 0.0011666994541883469
iteration 215, loss = 0.0007401487091556191
iteration 216, loss = 0.0009450325160287321
iteration 217, loss = 0.0007729977369308472
iteration 218, loss = 0.0007767417700961232
iteration 219, loss = 0.001510241418145597
iteration 220, loss = 0.000869445619173348
iteration 221, loss = 0.0008159590070135891
iteration 222, loss = 0.0007937982445582747
iteration 223, loss = 0.0008452075999230146
iteration 224, loss = 0.0008232152904383838
iteration 225, loss = 0.0010755278635770082
iteration 226, loss = 0.0007273633382283151
iteration 227, loss = 0.0011797456536442041
iteration 228, loss = 0.0006919339066371322
iteration 229, loss = 0.0009750304161570966
iteration 230, loss = 0.0007359447772614658
iteration 231, loss = 0.0009845762979239225
iteration 232, loss = 0.0007154122577048838
iteration 233, loss = 0.0006932970136404037
iteration 234, loss = 0.0008047670125961304
iteration 235, loss = 0.0012617663014680147
iteration 236, loss = 0.0006994071882218122
iteration 237, loss = 0.0009975074790418148
iteration 238, loss = 0.0007475464954040945
iteration 239, loss = 0.0008573518134653568
iteration 240, loss = 0.0011073033092543483
iteration 241, loss = 0.0014771074056625366
iteration 242, loss = 0.0006694410694763064
iteration 243, loss = 0.0015892153605818748
iteration 244, loss = 0.0006626876420341432
iteration 245, loss = 0.000835395825561136
iteration 246, loss = 0.0008795494213700294
iteration 247, loss = 0.0007863994687795639
iteration 248, loss = 0.0007950469153001904
iteration 249, loss = 0.0014294324209913611
iteration 250, loss = 0.0008658549631945789
iteration 251, loss = 0.0014896397478878498
iteration 252, loss = 0.0009984408970922232
iteration 253, loss = 0.0008071063202805817
iteration 254, loss = 0.0007697995752096176
iteration 255, loss = 0.0008694283314980567
iteration 256, loss = 0.0007746224873699248
iteration 257, loss = 0.0009280199883505702
iteration 258, loss = 0.0007068545673973858
iteration 259, loss = 0.0007488162955269217
iteration 260, loss = 0.0019431667169556022
iteration 261, loss = 0.0007540269871242344
iteration 262, loss = 0.0007069975254125893
iteration 263, loss = 0.0011095989029854536
iteration 264, loss = 0.0007601195247843862
iteration 265, loss = 0.0012062827590852976
iteration 266, loss = 0.0009956919820979238
iteration 267, loss = 0.000912400137167424
iteration 268, loss = 0.0009117042063735425
iteration 269, loss = 0.0008921885164454579
iteration 270, loss = 0.0010176042560487986
iteration 271, loss = 0.0008020382956601679
iteration 272, loss = 0.0014087283052504063
iteration 273, loss = 0.0008615972474217415
iteration 274, loss = 0.0007066791877150536
iteration 275, loss = 0.0009923817124217749
iteration 276, loss = 0.000701801385730505
iteration 277, loss = 0.000997841591015458
iteration 278, loss = 0.0008048706222325563
iteration 279, loss = 0.0007820339524187148
iteration 280, loss = 0.0008934895158745348
iteration 281, loss = 0.0006150105618871748
iteration 282, loss = 0.0007300798315554857
iteration 283, loss = 0.0007447528187185526
iteration 284, loss = 0.0006653168238699436
iteration 285, loss = 0.0006559196044690907
iteration 286, loss = 0.0009117557783611119
iteration 287, loss = 0.0009186507668346167
iteration 288, loss = 0.0011710361577570438
iteration 289, loss = 0.0007403710624203086
iteration 290, loss = 0.0009514638222754002
iteration 291, loss = 0.0007081359508447349
iteration 292, loss = 0.000937451608479023
iteration 293, loss = 0.0009013763628900051
iteration 294, loss = 0.0015106933424249291
iteration 295, loss = 0.0009634182206355035
iteration 296, loss = 0.0009993838611990213
iteration 297, loss = 0.0007806717767380178
iteration 298, loss = 0.0008651301031932235
iteration 299, loss = 0.0008285702206194401
iteration 300, loss = 0.0007579077500849962
iteration 1, loss = 0.0006581823690794408
iteration 2, loss = 0.0007327506318688393
iteration 3, loss = 0.0007119489600881934
iteration 4, loss = 0.0013241417473182082
iteration 5, loss = 0.0007033933652564883
iteration 6, loss = 0.0008739615441299975
iteration 7, loss = 0.0008562158327549696
iteration 8, loss = 0.0007214797660708427
iteration 9, loss = 0.000716398935765028
iteration 10, loss = 0.0007721784058958292
iteration 11, loss = 0.0008194417459890246
iteration 12, loss = 0.0008083686116151512
iteration 13, loss = 0.0009099845192395151
iteration 14, loss = 0.0013925209641456604
iteration 15, loss = 0.0007354258559644222
iteration 16, loss = 0.0007918583578430116
iteration 17, loss = 0.0006884714821353555
iteration 18, loss = 0.0008366855327039957
iteration 19, loss = 0.0008586541516706347
iteration 20, loss = 0.0009757803054526448
iteration 21, loss = 0.0014887236757203937
iteration 22, loss = 0.0006479124422185123
iteration 23, loss = 0.0018722969107329845
iteration 24, loss = 0.0007137684733606875
iteration 25, loss = 0.0008593124803155661
iteration 26, loss = 0.0007756978739053011
iteration 27, loss = 0.0009724284755066037
iteration 28, loss = 0.0010002891067415476
iteration 29, loss = 0.0008299150504171848
iteration 30, loss = 0.0010164511622861028
iteration 31, loss = 0.0014102412387728691
iteration 32, loss = 0.0007019328768365085
iteration 33, loss = 0.0007683445583097637
iteration 34, loss = 0.0008699400350451469
iteration 35, loss = 0.0011549382470548153
iteration 36, loss = 0.0007756327395327389
iteration 37, loss = 0.0008067057351581752
iteration 38, loss = 0.000854037469252944
iteration 39, loss = 0.0007326180348172784
iteration 40, loss = 0.0012894350802525878
iteration 41, loss = 0.0007432484999299049
iteration 42, loss = 0.0008938406244851649
iteration 43, loss = 0.0011403789976611733
iteration 44, loss = 0.001312360865995288
iteration 45, loss = 0.001764084561727941
iteration 46, loss = 0.0007623631972819567
iteration 47, loss = 0.001484694192185998
iteration 48, loss = 0.0007560413796454668
iteration 49, loss = 0.0010387906804680824
iteration 50, loss = 0.0013831015676259995
iteration 51, loss = 0.00074190046871081
iteration 52, loss = 0.0010146994609385729
iteration 53, loss = 0.0013109920546412468
iteration 54, loss = 0.0015945070190355182
iteration 55, loss = 0.0008185096085071564
iteration 56, loss = 0.0011062246048823
iteration 57, loss = 0.0013351788511499763
iteration 58, loss = 0.0007586138090118766
iteration 59, loss = 0.0007857948075979948
iteration 60, loss = 0.0007782774628140032
iteration 61, loss = 0.0009698795038275421
iteration 62, loss = 0.000965310086030513
iteration 63, loss = 0.0009060833835974336
iteration 64, loss = 0.0013097634073346853
iteration 65, loss = 0.0008056252263486385
iteration 66, loss = 0.0008316841558553278
iteration 67, loss = 0.0007420029724016786
iteration 68, loss = 0.0009547926019877195
iteration 69, loss = 0.0012399202678352594
iteration 70, loss = 0.0008768439292907715
iteration 71, loss = 0.0009610645938664675
iteration 72, loss = 0.0012952571269124746
iteration 73, loss = 0.0009919028962031007
iteration 74, loss = 0.0007435186416842043
iteration 75, loss = 0.0007504779496230185
iteration 76, loss = 0.0010127900168299675
iteration 77, loss = 0.0009809063049033284
iteration 78, loss = 0.0008835802436806262
iteration 79, loss = 0.0008384764078073204
iteration 80, loss = 0.0008555488893762231
iteration 81, loss = 0.0008087793830782175
iteration 82, loss = 0.0007634913781657815
iteration 83, loss = 0.0006344542489387095
iteration 84, loss = 0.0007184010464698076
iteration 85, loss = 0.0008176142000593245
iteration 86, loss = 0.0008081152918748558
iteration 87, loss = 0.0009239162900485098
iteration 88, loss = 0.0014684910420328379
iteration 89, loss = 0.0008762376965023577
iteration 90, loss = 0.001144587411545217
iteration 91, loss = 0.00102879642508924
iteration 92, loss = 0.0010875808075070381
iteration 93, loss = 0.0008279658504761755
iteration 94, loss = 0.0008144300081767142
iteration 95, loss = 0.0008983880397863686
iteration 96, loss = 0.0007130943704396486
iteration 97, loss = 0.0008472457411698997
iteration 98, loss = 0.000923884566873312
iteration 99, loss = 0.0007086398545652628
iteration 100, loss = 0.00064066395862028
iteration 101, loss = 0.0009017688571475446
iteration 102, loss = 0.0011104745790362358
iteration 103, loss = 0.0007186869625002146
iteration 104, loss = 0.0008688727975822985
iteration 105, loss = 0.000853366800583899
iteration 106, loss = 0.0013771647354587913
iteration 107, loss = 0.0010479700285941362
iteration 108, loss = 0.0009068864164873958
iteration 109, loss = 0.001408647047355771
iteration 110, loss = 0.0009219801868312061
iteration 111, loss = 0.0009855115786194801
iteration 112, loss = 0.0008396405610255897
iteration 113, loss = 0.0008095877128653228
iteration 114, loss = 0.0007145639392547309
iteration 115, loss = 0.0007291165529750288
iteration 116, loss = 0.0009400537819601595
iteration 117, loss = 0.000746222329325974
iteration 118, loss = 0.0007976662600412965
iteration 119, loss = 0.0007469786214642227
iteration 120, loss = 0.0008010212914086878
iteration 121, loss = 0.0009165202500298619
iteration 122, loss = 0.0014768330147489905
iteration 123, loss = 0.0008315315935760736
iteration 124, loss = 0.0008236519643105567
iteration 125, loss = 0.0010184785351157188
iteration 126, loss = 0.0007366164354607463
iteration 127, loss = 0.0008865391719155014
iteration 128, loss = 0.0007701510330662131
iteration 129, loss = 0.0007610851898789406
iteration 130, loss = 0.0008921336848288774
iteration 131, loss = 0.0011449124431237578
iteration 132, loss = 0.0008874604245647788
iteration 133, loss = 0.0008389847353100777
iteration 134, loss = 0.0016308218473568559
iteration 135, loss = 0.0017221452435478568
iteration 136, loss = 0.0008179497672244906
iteration 137, loss = 0.0006765392026863992
iteration 138, loss = 0.0008035351056605577
iteration 139, loss = 0.001080211834050715
iteration 140, loss = 0.0007997625507414341
iteration 141, loss = 0.0006220960058271885
iteration 142, loss = 0.0009022968588396907
iteration 143, loss = 0.0006793924258090556
iteration 144, loss = 0.0007448409451171756
iteration 145, loss = 0.0015146216610446572
iteration 146, loss = 0.0016185324639081955
iteration 147, loss = 0.000802172755356878
iteration 148, loss = 0.0008693376439623535
iteration 149, loss = 0.0010461843339726329
iteration 150, loss = 0.0013707652688026428
iteration 151, loss = 0.0008585505420342088
iteration 152, loss = 0.0008736700983718038
iteration 153, loss = 0.0007663654978387058
iteration 154, loss = 0.0009449930512346327
iteration 155, loss = 0.001101764151826501
iteration 156, loss = 0.0007839329191483557
iteration 157, loss = 0.0011345529928803444
iteration 158, loss = 0.0008586003677919507
iteration 159, loss = 0.001107473741285503
iteration 160, loss = 0.0009011835209093988
iteration 161, loss = 0.000783488794695586
iteration 162, loss = 0.0007239367114380002
iteration 163, loss = 0.0011240898165851831
iteration 164, loss = 0.0011623585596680641
iteration 165, loss = 0.0009737869841046631
iteration 166, loss = 0.0009044782491400838
iteration 167, loss = 0.0008894472848623991
iteration 168, loss = 0.0009474309044890106
iteration 169, loss = 0.000790819525718689
iteration 170, loss = 0.0007832511328160763
iteration 171, loss = 0.0008221407188102603
iteration 172, loss = 0.0013139307266101241
iteration 173, loss = 0.0015376437222585082
iteration 174, loss = 0.0007484552916139364
iteration 175, loss = 0.0006867137853987515
iteration 176, loss = 0.0007851964910514653
iteration 177, loss = 0.000738256610929966
iteration 178, loss = 0.0008055431535467505
iteration 179, loss = 0.0008740515331737697
iteration 180, loss = 0.000918283243663609
iteration 181, loss = 0.0014978322433307767
iteration 182, loss = 0.000789220561273396
iteration 183, loss = 0.000737512658815831
iteration 184, loss = 0.001570977154187858
iteration 185, loss = 0.0012984646018594503
iteration 186, loss = 0.0009468902717344463
iteration 187, loss = 0.0008649295195937157
iteration 188, loss = 0.0008938565733842552
iteration 189, loss = 0.0009695739136077464
iteration 190, loss = 0.000790910969953984
iteration 191, loss = 0.001094849081709981
iteration 192, loss = 0.0012458749115467072
iteration 193, loss = 0.002139018615707755
iteration 194, loss = 0.00087898806668818
iteration 195, loss = 0.0008698226884007454
iteration 196, loss = 0.0010771119268611073
iteration 197, loss = 0.0007856579613871872
iteration 198, loss = 0.0012284028343856335
iteration 199, loss = 0.0009126850636675954
iteration 200, loss = 0.0006521951290778816
iteration 201, loss = 0.0008175440598279238
iteration 202, loss = 0.0008441497921012342
iteration 203, loss = 0.0008007933502085507
iteration 204, loss = 0.0007364736520685256
iteration 205, loss = 0.000927066255826503
iteration 206, loss = 0.0010177170624956489
iteration 207, loss = 0.0008534103399142623
iteration 208, loss = 0.0009109506499953568
iteration 209, loss = 0.0009444501483812928
iteration 210, loss = 0.0018432203214615583
iteration 211, loss = 0.000867328024469316
iteration 212, loss = 0.0007482729852199554
iteration 213, loss = 0.001051531289704144
iteration 214, loss = 0.0007265695603564382
iteration 215, loss = 0.000817482708953321
iteration 216, loss = 0.001098332810215652
iteration 217, loss = 0.0009286388522014022
iteration 218, loss = 0.001353205181658268
iteration 219, loss = 0.0012161744525656104
iteration 220, loss = 0.0007049726555123925
iteration 221, loss = 0.0008453264599665999
iteration 222, loss = 0.0007570525631308556
iteration 223, loss = 0.0007479480700567365
iteration 224, loss = 0.0014587815385311842
iteration 225, loss = 0.0007874258444644511
iteration 226, loss = 0.0008198052528314292
iteration 227, loss = 0.0008405063417740166
iteration 228, loss = 0.0007475892198272049
iteration 229, loss = 0.0007641055854037404
iteration 230, loss = 0.0008967869798652828
iteration 231, loss = 0.0008934325305745006
iteration 232, loss = 0.0009502739412710071
iteration 233, loss = 0.0007331040687859058
iteration 234, loss = 0.0008944663568399847
iteration 235, loss = 0.0013404192868620157
iteration 236, loss = 0.0007221781997941434
iteration 237, loss = 0.0013380274176597595
iteration 238, loss = 0.0009762958507053554
iteration 239, loss = 0.0009990658145397902
iteration 240, loss = 0.0010823705233633518
iteration 241, loss = 0.000800871872343123
iteration 242, loss = 0.0009982023620977998
iteration 243, loss = 0.0008839156362228096
iteration 244, loss = 0.0006895934930071235
iteration 245, loss = 0.0009155317675322294
iteration 246, loss = 0.0015210369601845741
iteration 247, loss = 0.0009248248534277081
iteration 248, loss = 0.0006670090951956809
iteration 249, loss = 0.0007126140408217907
iteration 250, loss = 0.0006506973877549171
iteration 251, loss = 0.0007580836536362767
iteration 252, loss = 0.0008334686863236129
iteration 253, loss = 0.0014108011964708567
iteration 254, loss = 0.0007800231105647981
iteration 255, loss = 0.0008823753450997174
iteration 256, loss = 0.0010670563206076622
iteration 257, loss = 0.0009259216603823006
iteration 258, loss = 0.0008454836788587272
iteration 259, loss = 0.0007273682858794928
iteration 260, loss = 0.0006642322987318039
iteration 261, loss = 0.0009905715705826879
iteration 262, loss = 0.0006956757279112935
iteration 263, loss = 0.0013379832962527871
iteration 264, loss = 0.0007684596348553896
iteration 265, loss = 0.0018062220187857747
iteration 266, loss = 0.0007297911797650158
iteration 267, loss = 0.0007809759117662907
iteration 268, loss = 0.0007970974547788501
iteration 269, loss = 0.0007460754131898284
iteration 270, loss = 0.0010597523069009185
iteration 271, loss = 0.0010764336911961436
iteration 272, loss = 0.0007165152346715331
iteration 273, loss = 0.0009522199397906661
iteration 274, loss = 0.0014795612078160048
iteration 275, loss = 0.0010045993840321898
iteration 276, loss = 0.0009072793181985617
iteration 277, loss = 0.0009829148184508085
iteration 278, loss = 0.0008783445227891207
iteration 279, loss = 0.0009180842898786068
iteration 280, loss = 0.0007540011429227889
iteration 281, loss = 0.0006110242684371769
iteration 282, loss = 0.0008150110952556133
iteration 283, loss = 0.0009963782504200935
iteration 284, loss = 0.0008391370065510273
iteration 285, loss = 0.0007351309177465737
iteration 286, loss = 0.0006839839043095708
iteration 287, loss = 0.0009112234693020582
iteration 288, loss = 0.0007616144139319658
iteration 289, loss = 0.001844119280576706
iteration 290, loss = 0.0008425363339483738
iteration 291, loss = 0.001188137917779386
iteration 292, loss = 0.0009068084182217717
iteration 293, loss = 0.001068577286787331
iteration 294, loss = 0.0007727998890914023
iteration 295, loss = 0.0010190538596361876
iteration 296, loss = 0.0015061987796798348
iteration 297, loss = 0.000777341949287802
iteration 298, loss = 0.000865551526658237
iteration 299, loss = 0.001313312677666545
iteration 300, loss = 0.0007989847217686474
iteration 1, loss = 0.0009341222466900945
iteration 2, loss = 0.0013522289227694273
iteration 3, loss = 0.0011721275513991714
iteration 4, loss = 0.0008834580658003688
iteration 5, loss = 0.0007213398930616677
iteration 6, loss = 0.0014370499411597848
iteration 7, loss = 0.0008244820637628436
iteration 8, loss = 0.0008627945207990706
iteration 9, loss = 0.0009304953273385763
iteration 10, loss = 0.0007926300750114024
iteration 11, loss = 0.0008436919888481498
iteration 12, loss = 0.0008227939251810312
iteration 13, loss = 0.0011327804531902075
iteration 14, loss = 0.0007459914777427912
iteration 15, loss = 0.0008382177911698818
iteration 16, loss = 0.0013164763804525137
iteration 17, loss = 0.0008145243627950549
iteration 18, loss = 0.0007031845161691308
iteration 19, loss = 0.0009284091647714376
iteration 20, loss = 0.0007540476508438587
iteration 21, loss = 0.001067714300006628
iteration 22, loss = 0.0008417441858910024
iteration 23, loss = 0.000988923478871584
iteration 24, loss = 0.0010637858649715781
iteration 25, loss = 0.0016428501112386584
iteration 26, loss = 0.0008874677587300539
iteration 27, loss = 0.000854285026434809
iteration 28, loss = 0.0007379562011919916
iteration 29, loss = 0.0008801115909591317
iteration 30, loss = 0.0010494949528947473
iteration 31, loss = 0.0007164326962083578
iteration 32, loss = 0.00164892990142107
iteration 33, loss = 0.0019327466143295169
iteration 34, loss = 0.000780514208599925
iteration 35, loss = 0.000979167758487165
iteration 36, loss = 0.001507699373178184
iteration 37, loss = 0.0008005921263247728
iteration 38, loss = 0.0008601178997196257
iteration 39, loss = 0.0009200181229971349
iteration 40, loss = 0.000884953944478184
iteration 41, loss = 0.0015566819347441196
iteration 42, loss = 0.0007412399863824248
iteration 43, loss = 0.0010362830944359303
iteration 44, loss = 0.001096361200325191
iteration 45, loss = 0.0006768631865270436
iteration 46, loss = 0.001058558002114296
iteration 47, loss = 0.0007102877134457231
iteration 48, loss = 0.0008545703603886068
iteration 49, loss = 0.0022852446418255568
iteration 50, loss = 0.0007172636687755585
iteration 51, loss = 0.0006689358269795775
iteration 52, loss = 0.0010481178760528564
iteration 53, loss = 0.0010230636689811945
iteration 54, loss = 0.0013085940154269338
iteration 55, loss = 0.0007792952819727361
iteration 56, loss = 0.0007272121729329228
iteration 57, loss = 0.0008008756558410823
iteration 58, loss = 0.0009077356080524623
iteration 59, loss = 0.0008321931818500161
iteration 60, loss = 0.000901566818356514
iteration 61, loss = 0.0006212809821590781
iteration 62, loss = 0.0009788463357836008
iteration 63, loss = 0.000644148385617882
iteration 64, loss = 0.0009169133845716715
iteration 65, loss = 0.0008373156888410449
iteration 66, loss = 0.0007852796697989106
iteration 67, loss = 0.001296065398491919
iteration 68, loss = 0.0009342259145341814
iteration 69, loss = 0.0008787608821876347
iteration 70, loss = 0.0007543568499386311
iteration 71, loss = 0.0010383521439507604
iteration 72, loss = 0.001056614681147039
iteration 73, loss = 0.0008417879580520093
iteration 74, loss = 0.0019305766327306628
iteration 75, loss = 0.0008596223196946084
iteration 76, loss = 0.0008719115285202861
iteration 77, loss = 0.0007227496826089919
iteration 78, loss = 0.0008510423358529806
iteration 79, loss = 0.0006958794547244906
iteration 80, loss = 0.0007325774640776217
iteration 81, loss = 0.0016698746476322412
iteration 82, loss = 0.0010274883825331926
iteration 83, loss = 0.001100613852031529
iteration 84, loss = 0.000725508201867342
iteration 85, loss = 0.0008637385908514261
iteration 86, loss = 0.0009633308509364724
iteration 87, loss = 0.000896754558198154
iteration 88, loss = 0.001520964433439076
iteration 89, loss = 0.0015683096135035157
iteration 90, loss = 0.0011864190455526114
iteration 91, loss = 0.0010778220603242517
iteration 92, loss = 0.0014657711144536734
iteration 93, loss = 0.0011089135659858584
iteration 94, loss = 0.0008130114292725921
iteration 95, loss = 0.0010589207522571087
iteration 96, loss = 0.0010021449998021126
iteration 97, loss = 0.001115941908210516
iteration 98, loss = 0.0008855374762788415
iteration 99, loss = 0.0006832886720076203
iteration 100, loss = 0.0010767846833914518
iteration 101, loss = 0.0007565281703136861
iteration 102, loss = 0.0010972507297992706
iteration 103, loss = 0.000812469283118844
iteration 104, loss = 0.0010687779868021607
iteration 105, loss = 0.0008423436083830893
iteration 106, loss = 0.0008525255834683776
iteration 107, loss = 0.0009648689301684499
iteration 108, loss = 0.0007792550604790449
iteration 109, loss = 0.0007004132494330406
iteration 110, loss = 0.0008623801986686885
iteration 111, loss = 0.0007392421830445528
iteration 112, loss = 0.0006865801988169551
iteration 113, loss = 0.00081129145110026
iteration 114, loss = 0.0012517728609964252
iteration 115, loss = 0.0007356316200457513
iteration 116, loss = 0.001102322363294661
iteration 117, loss = 0.0008474148344248533
iteration 118, loss = 0.000999203883111477
iteration 119, loss = 0.0006759834359399974
iteration 120, loss = 0.0007895393646322191
iteration 121, loss = 0.0008731554844416678
iteration 122, loss = 0.0008994148229248822
iteration 123, loss = 0.0010134695330634713
iteration 124, loss = 0.001263811718672514
iteration 125, loss = 0.0008229016675613821
iteration 126, loss = 0.000699390540830791
iteration 127, loss = 0.0007430953555740416
iteration 128, loss = 0.0007564250263385475
iteration 129, loss = 0.001723854336887598
iteration 130, loss = 0.0007780133746564388
iteration 131, loss = 0.0008112279465422034
iteration 132, loss = 0.0007836430449970067
iteration 133, loss = 0.0008063423447310925
iteration 134, loss = 0.000968487118370831
iteration 135, loss = 0.0007675190572626889
iteration 136, loss = 0.000844283145852387
iteration 137, loss = 0.0010735872201621532
iteration 138, loss = 0.0008523792494088411
iteration 139, loss = 0.0010754989925771952
iteration 140, loss = 0.000737145310267806
iteration 141, loss = 0.0008104004664346576
iteration 142, loss = 0.0007091701263561845
iteration 143, loss = 0.0009192492580041289
iteration 144, loss = 0.0009601066121831536
iteration 145, loss = 0.0011564078740775585
iteration 146, loss = 0.0009398268302902579
iteration 147, loss = 0.0008172851521521807
iteration 148, loss = 0.0008367886184714735
iteration 149, loss = 0.0008274688152596354
iteration 150, loss = 0.0007666363962925971
iteration 151, loss = 0.0008719704928807914
iteration 152, loss = 0.0014947609743103385
iteration 153, loss = 0.0011200527660548687
iteration 154, loss = 0.0007812214316800237
iteration 155, loss = 0.0008597078849561512
iteration 156, loss = 0.000927685119677335
iteration 157, loss = 0.0011586672626435757
iteration 158, loss = 0.000995595008134842
iteration 159, loss = 0.0006661332445219159
iteration 160, loss = 0.0007940116920508444
iteration 161, loss = 0.0009309794986620545
iteration 162, loss = 0.0010326039046049118
iteration 163, loss = 0.0007504126988351345
iteration 164, loss = 0.0010250064078718424
iteration 165, loss = 0.0010176940122619271
iteration 166, loss = 0.0008432206814177334
iteration 167, loss = 0.0008629930671304464
iteration 168, loss = 0.0009425274911336601
iteration 169, loss = 0.0013599538942798972
iteration 170, loss = 0.0010676367674022913
iteration 171, loss = 0.000839890621136874
iteration 172, loss = 0.0015305792912840843
iteration 173, loss = 0.0008385070832446218
iteration 174, loss = 0.000732411106582731
iteration 175, loss = 0.001036185771226883
iteration 176, loss = 0.0017003065440803766
iteration 177, loss = 0.0010574661428108811
iteration 178, loss = 0.0007651346968486905
iteration 179, loss = 0.0008370448485948145
iteration 180, loss = 0.0009745965362526476
iteration 181, loss = 0.0008715548319742084
iteration 182, loss = 0.0006544190691784024
iteration 183, loss = 0.0008867888245731592
iteration 184, loss = 0.0008375231409445405
iteration 185, loss = 0.0008300863555632532
iteration 186, loss = 0.0009205751703120768
iteration 187, loss = 0.0008439950179308653
iteration 188, loss = 0.0007320826407521963
iteration 189, loss = 0.000773786217905581
iteration 190, loss = 0.0011074182111769915
iteration 191, loss = 0.0007473736186511815
iteration 192, loss = 0.000841593777295202
iteration 193, loss = 0.0008063351269811392
iteration 194, loss = 0.000761675531975925
iteration 195, loss = 0.0007096125045791268
iteration 196, loss = 0.0008180198492482305
iteration 197, loss = 0.000816854415461421
iteration 198, loss = 0.0014455305645242333
iteration 199, loss = 0.001082889037206769
iteration 200, loss = 0.0007762748864479363
iteration 201, loss = 0.0019276916282251477
iteration 202, loss = 0.0009330397588200867
iteration 203, loss = 0.0007783595938235521
iteration 204, loss = 0.0010649461764842272
iteration 205, loss = 0.0009547070949338377
iteration 206, loss = 0.0011063539423048496
iteration 207, loss = 0.0014960126718506217
iteration 208, loss = 0.0009598022443242371
iteration 209, loss = 0.0011142895091325045
iteration 210, loss = 0.0008903812849894166
iteration 211, loss = 0.0006551656988449395
iteration 212, loss = 0.000701269309502095
iteration 213, loss = 0.0008058155653998256
iteration 214, loss = 0.0009682629606686532
iteration 215, loss = 0.0006574357394129038
iteration 216, loss = 0.0009042304009199142
iteration 217, loss = 0.0008791724103502929
iteration 218, loss = 0.0008450018358416855
iteration 219, loss = 0.0008002917747944593
iteration 220, loss = 0.001181647414341569
iteration 221, loss = 0.0009180248016491532
iteration 222, loss = 0.0008078516693785787
iteration 223, loss = 0.0008730822592042387
iteration 224, loss = 0.000991280423477292
iteration 225, loss = 0.0008013880578801036
iteration 226, loss = 0.0007375131244771183
iteration 227, loss = 0.0007496965117752552
iteration 228, loss = 0.001119260094128549
iteration 229, loss = 0.0007511439034715295
iteration 230, loss = 0.0010226689046248794
iteration 231, loss = 0.0009830454364418983
iteration 232, loss = 0.0008185086189769208
iteration 233, loss = 0.0008589305216446519
iteration 234, loss = 0.001004288555122912
iteration 235, loss = 0.0014144369633868337
iteration 236, loss = 0.0009119602618739009
iteration 237, loss = 0.0006507383077405393
iteration 238, loss = 0.0017058284720405936
iteration 239, loss = 0.0013095359317958355
iteration 240, loss = 0.0015113716945052147
iteration 241, loss = 0.0010028857504948974
iteration 242, loss = 0.0007007300155237317
iteration 243, loss = 0.0008649968658573925
iteration 244, loss = 0.0006962420302443206
iteration 245, loss = 0.000827712588943541
iteration 246, loss = 0.0008761748904362321
iteration 247, loss = 0.0009693408501334488
iteration 248, loss = 0.0007442483911290765
iteration 249, loss = 0.0008843114483170211
iteration 250, loss = 0.0007242112187668681
iteration 251, loss = 0.0007734415121376514
iteration 252, loss = 0.0008669677190482616
iteration 253, loss = 0.0014299243921414018
iteration 254, loss = 0.0007856415468268096
iteration 255, loss = 0.0012751641916111112
iteration 256, loss = 0.001539370627142489
iteration 257, loss = 0.000842552981339395
iteration 258, loss = 0.0007025394588708878
iteration 259, loss = 0.0009479942382313311
iteration 260, loss = 0.0010245849844068289
iteration 261, loss = 0.0006563563365489244
iteration 262, loss = 0.001008090446703136
iteration 263, loss = 0.0007086736732162535
iteration 264, loss = 0.0009202658548019826
iteration 265, loss = 0.0009116323781199753
iteration 266, loss = 0.0007760062580928206
iteration 267, loss = 0.0010125958360731602
iteration 268, loss = 0.0007968568243086338
iteration 269, loss = 0.000929643923882395
iteration 270, loss = 0.0011437166249379516
iteration 271, loss = 0.0007277638651430607
iteration 272, loss = 0.0010538577334955335
iteration 273, loss = 0.0007478599436581135
iteration 274, loss = 0.0008478585514239967
iteration 275, loss = 0.0007628556340932846
iteration 276, loss = 0.0008191567030735314
iteration 277, loss = 0.000713717716280371
iteration 278, loss = 0.0006697708158753812
iteration 279, loss = 0.0008920426480472088
iteration 280, loss = 0.0010674558579921722
iteration 281, loss = 0.0007491099531762302
iteration 282, loss = 0.000873887212947011
iteration 283, loss = 0.000719514035154134
iteration 284, loss = 0.0006715452182106674
iteration 285, loss = 0.0007167747826315463
iteration 286, loss = 0.0008113154908642173
iteration 287, loss = 0.0007976634660735726
iteration 288, loss = 0.0007888401160016656
iteration 289, loss = 0.0015799672110006213
iteration 290, loss = 0.0010208305902779102
iteration 291, loss = 0.000744550721719861
iteration 292, loss = 0.0008297395543195307
iteration 293, loss = 0.0009736759820953012
iteration 294, loss = 0.0010843169875442982
iteration 295, loss = 0.0008040987886488438
iteration 296, loss = 0.0008494220091961324
iteration 297, loss = 0.001173986354842782
iteration 298, loss = 0.0018509782385081053
iteration 299, loss = 0.00093605782603845
iteration 300, loss = 0.0007647890597581863
iteration 1, loss = 0.0007219602121040225
iteration 2, loss = 0.0008330617565661669
iteration 3, loss = 0.0006983264465816319
iteration 4, loss = 0.0014133541844785213
iteration 5, loss = 0.0011818113271147013
iteration 6, loss = 0.0010636268416419625
iteration 7, loss = 0.0007839996251277626
iteration 8, loss = 0.000868441304191947
iteration 9, loss = 0.0009281135280616581
iteration 10, loss = 0.0018998185405507684
iteration 11, loss = 0.0008471732726320624
iteration 12, loss = 0.0011620456352829933
iteration 13, loss = 0.0008017762447707355
iteration 14, loss = 0.0008840119699016213
iteration 15, loss = 0.0011027370346710086
iteration 16, loss = 0.0009556902223266661
iteration 17, loss = 0.0015913299284875393
iteration 18, loss = 0.0013978645438328385
iteration 19, loss = 0.0009767247829586267
iteration 20, loss = 0.0008550467900931835
iteration 21, loss = 0.0006554223946295679
iteration 22, loss = 0.000709405227098614
iteration 23, loss = 0.0007280984427779913
iteration 24, loss = 0.0007652899948880076
iteration 25, loss = 0.000845140777528286
iteration 26, loss = 0.0007766089402139187
iteration 27, loss = 0.0007286812178790569
iteration 28, loss = 0.0006999303586781025
iteration 29, loss = 0.0007682510185986757
iteration 30, loss = 0.0008144723251461983
iteration 31, loss = 0.001135998871177435
iteration 32, loss = 0.0010880309855565429
iteration 33, loss = 0.0008510959451086819
iteration 34, loss = 0.0011239200830459595
iteration 35, loss = 0.0008104194421321154
iteration 36, loss = 0.0007654156652279198
iteration 37, loss = 0.0007757866405881941
iteration 38, loss = 0.0007909976993687451
iteration 39, loss = 0.0007839973550289869
iteration 40, loss = 0.0007456835592165589
iteration 41, loss = 0.0009670211002230644
iteration 42, loss = 0.0006825156160630286
iteration 43, loss = 0.0007535316981375217
iteration 44, loss = 0.0008417584467679262
iteration 45, loss = 0.0010342979803681374
iteration 46, loss = 0.0008890244644135237
iteration 47, loss = 0.0009613843285478652
iteration 48, loss = 0.0007824384374544024
iteration 49, loss = 0.000798611028585583
iteration 50, loss = 0.0009038134012371302
iteration 51, loss = 0.0009145038784481585
iteration 52, loss = 0.0009082794422283769
iteration 53, loss = 0.0009680538787506521
iteration 54, loss = 0.0011387488339096308
iteration 55, loss = 0.0006968548987060785
iteration 56, loss = 0.0009041465818881989
iteration 57, loss = 0.000809201505035162
iteration 58, loss = 0.0010981288505718112
iteration 59, loss = 0.0008250953978858888
iteration 60, loss = 0.0006910261581651866
iteration 61, loss = 0.0007668349426239729
iteration 62, loss = 0.0007474903832189739
iteration 63, loss = 0.0008813421009108424
iteration 64, loss = 0.0012053953250870109
iteration 65, loss = 0.0007436275482177734
iteration 66, loss = 0.0007775394478812814
iteration 67, loss = 0.001161644933745265
iteration 68, loss = 0.0009597097523510456
iteration 69, loss = 0.0008488500025123358
iteration 70, loss = 0.0008660989697091281
iteration 71, loss = 0.0007150690071284771
iteration 72, loss = 0.0008961524581536651
iteration 73, loss = 0.0015467972261831164
iteration 74, loss = 0.0011199148138985038
iteration 75, loss = 0.0012296981876716018
iteration 76, loss = 0.0008042482659220695
iteration 77, loss = 0.0009679129580035806
iteration 78, loss = 0.0022320058196783066
iteration 79, loss = 0.0006698744837194681
iteration 80, loss = 0.0008973759831860662
iteration 81, loss = 0.0007226330926641822
iteration 82, loss = 0.0009467457421123981
iteration 83, loss = 0.0008873248589225113
iteration 84, loss = 0.001615495653823018
iteration 85, loss = 0.0013839289313182235
iteration 86, loss = 0.0009419865673407912
iteration 87, loss = 0.0007060395437292755
iteration 88, loss = 0.0008433347102254629
iteration 89, loss = 0.0013777489075437188
iteration 90, loss = 0.0011304498184472322
iteration 91, loss = 0.0011793430894613266
iteration 92, loss = 0.0008250810205936432
iteration 93, loss = 0.0010401421459391713
iteration 94, loss = 0.0006602407665923238
iteration 95, loss = 0.001053547952324152
iteration 96, loss = 0.0007823603809811175
iteration 97, loss = 0.0007811340037733316
iteration 98, loss = 0.0008583097951486707
iteration 99, loss = 0.0010058655170723796
iteration 100, loss = 0.0006825858145020902
iteration 101, loss = 0.0013781473971903324
iteration 102, loss = 0.0008047052542679012
iteration 103, loss = 0.0008688541711308062
iteration 104, loss = 0.0010361123131588101
iteration 105, loss = 0.0009112897096201777
iteration 106, loss = 0.0012648686533793807
iteration 107, loss = 0.0008306748932227492
iteration 108, loss = 0.0009035875555127859
iteration 109, loss = 0.00080188421998173
iteration 110, loss = 0.001584302051924169
iteration 111, loss = 0.0010028776014223695
iteration 112, loss = 0.0008075056248344481
iteration 113, loss = 0.0010834132554009557
iteration 114, loss = 0.0007054256275296211
iteration 115, loss = 0.0006715571507811546
iteration 116, loss = 0.0014438291545957327
iteration 117, loss = 0.0006568675162270665
iteration 118, loss = 0.0009006211184896529
iteration 119, loss = 0.000774990941863507
iteration 120, loss = 0.0007009912515059114
iteration 121, loss = 0.0008096868405118585
iteration 122, loss = 0.0008042752742767334
iteration 123, loss = 0.0008218147559091449
iteration 124, loss = 0.000973653222899884
iteration 125, loss = 0.0007953564636409283
iteration 126, loss = 0.0008941824780777097
iteration 127, loss = 0.0008035735809244215
iteration 128, loss = 0.0009210027055814862
iteration 129, loss = 0.0015898338751867414
iteration 130, loss = 0.0007586278370581567
iteration 131, loss = 0.0008638327126391232
iteration 132, loss = 0.0007993746548891068
iteration 133, loss = 0.0007734067039564252
iteration 134, loss = 0.0012386473827064037
iteration 135, loss = 0.0006276547792367637
iteration 136, loss = 0.0007528841961175203
iteration 137, loss = 0.0009244467946700752
iteration 138, loss = 0.0014573768712580204
iteration 139, loss = 0.0011696440633386374
iteration 140, loss = 0.0009724728297442198
iteration 141, loss = 0.0006752615445293486
iteration 142, loss = 0.0006900083972141147
iteration 143, loss = 0.0007514008902944624
iteration 144, loss = 0.0007775574340485036
iteration 145, loss = 0.0008057818631641567
iteration 146, loss = 0.0008497840026393533
iteration 147, loss = 0.0008975247037597001
iteration 148, loss = 0.0009580092737451196
iteration 149, loss = 0.0015549410600215197
iteration 150, loss = 0.0009446351323276758
iteration 151, loss = 0.001081319060176611
iteration 152, loss = 0.0012202601647004485
iteration 153, loss = 0.000931605405639857
iteration 154, loss = 0.001467389753088355
iteration 155, loss = 0.002335721394047141
iteration 156, loss = 0.000836036866530776
iteration 157, loss = 0.0010810794774442911
iteration 158, loss = 0.0007866157684475183
iteration 159, loss = 0.0009161991765722632
iteration 160, loss = 0.0007688305922783911
iteration 161, loss = 0.0010278938570991158
iteration 162, loss = 0.0008767042309045792
iteration 163, loss = 0.0011143396841362119
iteration 164, loss = 0.0007711456855759025
iteration 165, loss = 0.000697215786203742
iteration 166, loss = 0.0010984299005940557
iteration 167, loss = 0.0010365830967202783
iteration 168, loss = 0.0007609009626321495
iteration 169, loss = 0.000961108657065779
iteration 170, loss = 0.0010657405946403742
iteration 171, loss = 0.001552150584757328
iteration 172, loss = 0.0006575704319402575
iteration 173, loss = 0.0006948604132048786
iteration 174, loss = 0.000855955935548991
iteration 175, loss = 0.0011466504074633121
iteration 176, loss = 0.0008987420587800443
iteration 177, loss = 0.0008979644044302404
iteration 178, loss = 0.0009180732304230332
iteration 179, loss = 0.0009056814014911652
iteration 180, loss = 0.0006682017701677978
iteration 181, loss = 0.0007534848991781473
iteration 182, loss = 0.0009629509295336902
iteration 183, loss = 0.00110052980016917
iteration 184, loss = 0.001647735945880413
iteration 185, loss = 0.0009497532737441361
iteration 186, loss = 0.0010006409138441086
iteration 187, loss = 0.0010425584623590112
iteration 188, loss = 0.0007599241798743606
iteration 189, loss = 0.0016576461493968964
iteration 190, loss = 0.0009978924645110965
iteration 191, loss = 0.0008488846942782402
iteration 192, loss = 0.0009574322029948235
iteration 193, loss = 0.000823476817458868
iteration 194, loss = 0.001054624910466373
iteration 195, loss = 0.0007226503803394735
iteration 196, loss = 0.0009620707714930177
iteration 197, loss = 0.0008932917262427509
iteration 198, loss = 0.0007094423053786159
iteration 199, loss = 0.0011804401874542236
iteration 200, loss = 0.0015284650726243854
iteration 201, loss = 0.000918491103220731
iteration 202, loss = 0.0008598213316872716
iteration 203, loss = 0.0012889544013887644
iteration 204, loss = 0.0008045847062021494
iteration 205, loss = 0.0008496457012370229
iteration 206, loss = 0.0007187563460320234
iteration 207, loss = 0.0007210947223939002
iteration 208, loss = 0.001698276842944324
iteration 209, loss = 0.0007392448605969548
iteration 210, loss = 0.001179674407467246
iteration 211, loss = 0.0011444379342719913
iteration 212, loss = 0.0009106633951887488
iteration 213, loss = 0.0008091487106867135
iteration 214, loss = 0.0014033876359462738
iteration 215, loss = 0.0008392203017137945
iteration 216, loss = 0.0008016040665097535
iteration 217, loss = 0.0015782504342496395
iteration 218, loss = 0.0006600102642551064
iteration 219, loss = 0.0006861752481199801
iteration 220, loss = 0.0007730831857770681
iteration 221, loss = 0.001332129817456007
iteration 222, loss = 0.0010856927838176489
iteration 223, loss = 0.000899860926438123
iteration 224, loss = 0.0009808411123231053
iteration 225, loss = 0.0007095332257449627
iteration 226, loss = 0.0007696928805671632
iteration 227, loss = 0.0007746474002487957
iteration 228, loss = 0.0015344882849603891
iteration 229, loss = 0.0012028309283778071
iteration 230, loss = 0.0008599846041761339
iteration 231, loss = 0.0008229115628637373
iteration 232, loss = 0.0009186802199110389
iteration 233, loss = 0.0009593284339644015
iteration 234, loss = 0.0008336011087521911
iteration 235, loss = 0.0012239868519827724
iteration 236, loss = 0.0007246817112900317
iteration 237, loss = 0.0007651092018932104
iteration 238, loss = 0.000835807528346777
iteration 239, loss = 0.0007927436381578445
iteration 240, loss = 0.0007450361154042184
iteration 241, loss = 0.0015609062975272536
iteration 242, loss = 0.0011145616881549358
iteration 243, loss = 0.0007118518115021288
iteration 244, loss = 0.0007895855233073235
iteration 245, loss = 0.0007377068977802992
iteration 246, loss = 0.0007799625163897872
iteration 247, loss = 0.0011475511128082871
iteration 248, loss = 0.0007527993293479085
iteration 249, loss = 0.0007114779436960816
iteration 250, loss = 0.0007738856365904212
iteration 251, loss = 0.0009033677051775157
iteration 252, loss = 0.000768556259572506
iteration 253, loss = 0.0009283588733524084
iteration 254, loss = 0.0009763317066244781
iteration 255, loss = 0.00101357267703861
iteration 256, loss = 0.0009022887679748237
iteration 257, loss = 0.0007844389765523374
iteration 258, loss = 0.0017108607571572065
iteration 259, loss = 0.000680208730045706
iteration 260, loss = 0.0007783307810314
iteration 261, loss = 0.0007812991389073431
iteration 262, loss = 0.0007989026489667594
iteration 263, loss = 0.0009501391905359924
iteration 264, loss = 0.0007874284638091922
iteration 265, loss = 0.001040216418914497
iteration 266, loss = 0.0014190784422680736
iteration 267, loss = 0.0007735380786471069
iteration 268, loss = 0.0008025692659430206
iteration 269, loss = 0.0008956780657172203
iteration 270, loss = 0.0007914674933999777
iteration 271, loss = 0.000802950351499021
iteration 272, loss = 0.0006987123051658273
iteration 273, loss = 0.0008919889805838466
iteration 274, loss = 0.0007202738779596984
iteration 275, loss = 0.0008494707872159779
iteration 276, loss = 0.0010843293275684118
iteration 277, loss = 0.0007514337194152176
iteration 278, loss = 0.0008342015207745135
iteration 279, loss = 0.0006505160126835108
iteration 280, loss = 0.000985747086815536
iteration 281, loss = 0.000829220050945878
iteration 282, loss = 0.0007209835457615554
iteration 283, loss = 0.0008754779119044542
iteration 284, loss = 0.0006566533702425659
iteration 285, loss = 0.0010106367990374565
iteration 286, loss = 0.0007294697570614517
iteration 287, loss = 0.0007567564607597888
iteration 288, loss = 0.0026057036593556404
iteration 289, loss = 0.0008339917403645813
iteration 290, loss = 0.0014832834713160992
iteration 291, loss = 0.0009110438986681402
iteration 292, loss = 0.0007177761872299016
iteration 293, loss = 0.0007555802003480494
iteration 294, loss = 0.001183387590572238
iteration 295, loss = 0.0007626994047313929
iteration 296, loss = 0.001062036957591772
iteration 297, loss = 0.0008307892130687833
iteration 298, loss = 0.0007445266819559038
iteration 299, loss = 0.0008641935419291258
iteration 300, loss = 0.0008930181502364576

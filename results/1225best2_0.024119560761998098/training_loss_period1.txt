iteration 0, loss = 0.7143955230712891
iteration 1, loss = 0.7133058309555054
iteration 2, loss = 0.7043386697769165
iteration 3, loss = 0.6948329210281372
iteration 4, loss = 0.714698851108551
iteration 5, loss = 0.7102617025375366
iteration 6, loss = 0.7024285793304443
iteration 7, loss = 0.7017989158630371
iteration 8, loss = 0.6957933902740479
iteration 9, loss = 0.6967529058456421
iteration 10, loss = 0.7045751810073853
iteration 11, loss = 0.698701024055481
iteration 12, loss = 0.7063595056533813
iteration 13, loss = 0.7014811635017395
iteration 14, loss = 0.6831822395324707
iteration 15, loss = 0.7033809423446655
iteration 16, loss = 0.6862038373947144
iteration 17, loss = 0.6853559017181396
iteration 18, loss = 0.6935685276985168
iteration 19, loss = 0.6952429413795471
iteration 20, loss = 0.6871333122253418
iteration 21, loss = 0.6844121813774109
iteration 22, loss = 0.6933689117431641
iteration 23, loss = 0.6924974322319031
iteration 24, loss = 0.6723462343215942
iteration 25, loss = 0.6808421611785889
iteration 26, loss = 0.6758553385734558
iteration 27, loss = 0.6753462553024292
iteration 28, loss = 0.6769520044326782
iteration 29, loss = 0.6587710380554199
iteration 30, loss = 0.6645898818969727
iteration 31, loss = 0.6527446508407593
iteration 32, loss = 0.6740578413009644
iteration 33, loss = 0.6530457735061646
iteration 34, loss = 0.6510906219482422
iteration 35, loss = 0.6537289619445801
iteration 36, loss = 0.6595200300216675
iteration 37, loss = 0.6673033237457275
iteration 38, loss = 0.6445889472961426
iteration 39, loss = 0.6610006093978882
iteration 40, loss = 0.6602208018302917
iteration 41, loss = 0.6593571901321411
iteration 42, loss = 0.6606639623641968
iteration 43, loss = 0.6549855470657349
iteration 44, loss = 0.6500632762908936
iteration 45, loss = 0.642265796661377
iteration 46, loss = 0.6479468941688538
iteration 47, loss = 0.6520029902458191
iteration 48, loss = 0.6474115252494812
iteration 49, loss = 0.6525207757949829
iteration 50, loss = 0.6321039199829102
iteration 51, loss = 0.636676013469696
iteration 52, loss = 0.6297361850738525
iteration 53, loss = 0.6301860213279724
iteration 54, loss = 0.6237397193908691
iteration 55, loss = 0.6123768091201782
iteration 56, loss = 0.6717015504837036
iteration 57, loss = 0.6684248447418213
iteration 58, loss = 0.6441601514816284
iteration 59, loss = 0.6377903819084167
iteration 60, loss = 0.6233860850334167
iteration 61, loss = 0.6177346706390381
iteration 62, loss = 0.6463683843612671
iteration 63, loss = 0.6559472680091858
iteration 64, loss = 0.6132543087005615
iteration 65, loss = 0.6161507964134216
iteration 66, loss = 0.5995745658874512
iteration 67, loss = 0.6201064586639404
iteration 68, loss = 0.6365350484848022
iteration 69, loss = 0.6369365453720093
iteration 70, loss = 0.6285818815231323
iteration 71, loss = 0.6340147256851196
iteration 72, loss = 0.5960227251052856
iteration 73, loss = 0.6184508204460144
iteration 74, loss = 0.628728985786438
iteration 75, loss = 0.5934242010116577
iteration 76, loss = 0.6523271203041077
iteration 77, loss = 0.612627387046814
iteration 78, loss = 0.6097626686096191
iteration 79, loss = 0.6319386959075928
iteration 80, loss = 0.6035831570625305
iteration 81, loss = 0.6221358776092529
iteration 82, loss = 0.5950425267219543
iteration 83, loss = 0.6280100345611572
iteration 84, loss = 0.6116865873336792
iteration 85, loss = 0.6026276350021362
iteration 86, loss = 0.6025400757789612
iteration 87, loss = 0.5955562591552734
iteration 88, loss = 0.5953232049942017
iteration 89, loss = 0.5741890668869019
iteration 90, loss = 0.6075843572616577
iteration 91, loss = 0.5702400803565979
iteration 92, loss = 0.5853805541992188
iteration 93, loss = 0.5745450258255005
iteration 94, loss = 0.6008570194244385
iteration 95, loss = 0.5919995307922363
iteration 96, loss = 0.6113274097442627
iteration 97, loss = 0.6047459840774536
iteration 98, loss = 0.5879241824150085
iteration 99, loss = 0.5842732787132263
iteration 100, loss = 0.6422123908996582
iteration 101, loss = 0.5662482976913452
iteration 102, loss = 0.5921670198440552
iteration 103, loss = 0.5761374831199646
iteration 104, loss = 0.5614005327224731
iteration 105, loss = 0.5666314363479614
iteration 106, loss = 0.5920084714889526
iteration 107, loss = 0.5635531544685364
iteration 108, loss = 0.6036021709442139
iteration 109, loss = 0.6075356006622314
iteration 110, loss = 0.6323909759521484
iteration 111, loss = 0.5832973718643188
iteration 112, loss = 0.6426416635513306
iteration 113, loss = 0.5859516263008118
iteration 114, loss = 0.5828198790550232
iteration 115, loss = 0.5617493987083435
iteration 116, loss = 0.6023402214050293
iteration 117, loss = 0.566218376159668
iteration 118, loss = 0.5649889707565308
iteration 119, loss = 0.570550799369812
iteration 120, loss = 0.5780670642852783
iteration 121, loss = 0.5533596277236938
iteration 122, loss = 0.5779199600219727
iteration 123, loss = 0.5942988395690918
iteration 124, loss = 0.5812172293663025
iteration 125, loss = 0.5482913851737976
iteration 126, loss = 0.5527247786521912
iteration 127, loss = 0.5831185579299927
iteration 128, loss = 0.6045031547546387
iteration 129, loss = 0.5600801110267639
iteration 130, loss = 0.5916290283203125
iteration 131, loss = 0.548346996307373
iteration 132, loss = 0.5746918320655823
iteration 133, loss = 0.5704041719436646
iteration 134, loss = 0.5460251569747925
iteration 135, loss = 0.5909101963043213
iteration 136, loss = 0.55671626329422
iteration 137, loss = 0.556503415107727
iteration 138, loss = 0.5310649871826172
iteration 139, loss = 0.5816916227340698
iteration 140, loss = 0.5473213195800781
iteration 141, loss = 0.5647733211517334
iteration 142, loss = 0.5502707362174988
iteration 143, loss = 0.5558272004127502
iteration 144, loss = 0.5592248439788818
iteration 145, loss = 0.5567713975906372
iteration 146, loss = 0.5491042137145996
iteration 147, loss = 0.5762757062911987
iteration 148, loss = 0.5369842052459717
iteration 149, loss = 0.5292226076126099
iteration 150, loss = 0.5210996866226196
iteration 151, loss = 0.5599608421325684
iteration 152, loss = 0.5271695256233215
iteration 153, loss = 0.5383440256118774
iteration 154, loss = 0.5571727752685547
iteration 155, loss = 0.5298092365264893
iteration 156, loss = 0.5185359716415405
iteration 157, loss = 0.5462533235549927
iteration 158, loss = 0.5186159014701843
iteration 159, loss = 0.5539637804031372
iteration 160, loss = 0.5043043494224548
iteration 161, loss = 0.5216494798660278
iteration 162, loss = 0.5322673916816711
iteration 163, loss = 0.527958869934082
iteration 164, loss = 0.5377942323684692
iteration 165, loss = 0.5690007209777832
iteration 166, loss = 0.5144413113594055
iteration 167, loss = 0.5548050403594971
iteration 168, loss = 0.5714308023452759
iteration 169, loss = 0.5151645541191101
iteration 170, loss = 0.511603832244873
iteration 171, loss = 0.5171119570732117
iteration 172, loss = 0.5875072479248047
iteration 173, loss = 0.541776180267334
iteration 174, loss = 0.5161255598068237
iteration 175, loss = 0.5400128364562988
iteration 176, loss = 0.5514525175094604
iteration 177, loss = 0.5154393911361694
iteration 178, loss = 0.48581284284591675
iteration 179, loss = 0.5615761280059814
iteration 180, loss = 0.5471380949020386
iteration 181, loss = 0.5474072694778442
iteration 182, loss = 0.5000360608100891
iteration 183, loss = 0.5894045829772949
iteration 184, loss = 0.5310624837875366
iteration 185, loss = 0.5022737979888916
iteration 186, loss = 0.5271279811859131
iteration 187, loss = 0.519327700138092
iteration 188, loss = 0.5288317203521729
iteration 189, loss = 0.5236945748329163
iteration 190, loss = 0.5182140469551086
iteration 191, loss = 0.5455919504165649
iteration 192, loss = 0.49341750144958496
iteration 193, loss = 0.5061661005020142
iteration 194, loss = 0.504147469997406
iteration 195, loss = 0.5157444477081299
iteration 196, loss = 0.5133633613586426
iteration 197, loss = 0.508588969707489
iteration 198, loss = 0.5177438855171204
iteration 199, loss = 0.5363202095031738
iteration 200, loss = 0.5311106443405151
iteration 201, loss = 0.5697561502456665
iteration 202, loss = 0.5385532975196838
iteration 203, loss = 0.5410232543945312
iteration 204, loss = 0.4918220341205597
iteration 205, loss = 0.4939536452293396
iteration 206, loss = 0.5073901414871216
iteration 207, loss = 0.5133112668991089
iteration 208, loss = 0.5635489225387573
iteration 209, loss = 0.4945235848426819
iteration 210, loss = 0.513264536857605
iteration 211, loss = 0.521772563457489
iteration 212, loss = 0.5192654132843018
iteration 213, loss = 0.4974929690361023
iteration 214, loss = 0.50201815366745
iteration 215, loss = 0.4834026098251343
iteration 216, loss = 0.4659986197948456
iteration 217, loss = 0.5103553533554077
iteration 218, loss = 0.524127185344696
iteration 219, loss = 0.48076725006103516
iteration 220, loss = 0.5387349724769592
iteration 221, loss = 0.5093349814414978
iteration 222, loss = 0.5545324683189392
iteration 223, loss = 0.4792592525482178
iteration 224, loss = 0.5210616588592529
iteration 225, loss = 0.5255682468414307
iteration 226, loss = 0.5520482063293457
iteration 227, loss = 0.5059173703193665
iteration 228, loss = 0.4716794490814209
iteration 229, loss = 0.49768245220184326
iteration 230, loss = 0.509642481803894
iteration 231, loss = 0.4985021948814392
iteration 232, loss = 0.4773622453212738
iteration 233, loss = 0.45920804142951965
iteration 234, loss = 0.5298411250114441
iteration 235, loss = 0.4611986577510834
iteration 236, loss = 0.49992573261260986
iteration 237, loss = 0.478704571723938
iteration 238, loss = 0.5092290639877319
iteration 239, loss = 0.5117472410202026
iteration 240, loss = 0.4724578857421875
iteration 241, loss = 0.5122007131576538
iteration 242, loss = 0.5189664363861084
iteration 243, loss = 0.5113252401351929
iteration 244, loss = 0.5464619994163513
iteration 245, loss = 0.47265565395355225
iteration 246, loss = 0.4743783473968506
iteration 247, loss = 0.4603850841522217
iteration 248, loss = 0.49043864011764526
iteration 249, loss = 0.4467121362686157
iteration 250, loss = 0.5391096472740173
iteration 251, loss = 0.5226277112960815
iteration 252, loss = 0.4358687996864319
iteration 253, loss = 0.46068963408470154
iteration 254, loss = 0.4942389726638794
iteration 255, loss = 0.47499537467956543
iteration 256, loss = 0.49558892846107483
iteration 257, loss = 0.4356412887573242
iteration 258, loss = 0.453266441822052
iteration 259, loss = 0.47736692428588867
iteration 260, loss = 0.474789023399353
iteration 261, loss = 0.4745446443557739
iteration 262, loss = 0.4831259250640869
iteration 263, loss = 0.5093615651130676
iteration 264, loss = 0.5326781868934631
iteration 265, loss = 0.45456549525260925
iteration 266, loss = 0.490286648273468
iteration 267, loss = 0.5115984678268433
iteration 268, loss = 0.42450159788131714
iteration 269, loss = 0.4220474064350128
iteration 270, loss = 0.5282836556434631
iteration 271, loss = 0.4626959264278412
iteration 272, loss = 0.45294973254203796
iteration 273, loss = 0.45612937211990356
iteration 274, loss = 0.4280451536178589
iteration 275, loss = 0.4821769595146179
iteration 276, loss = 0.5110061168670654
iteration 277, loss = 0.4659511148929596
iteration 278, loss = 0.4617665410041809
iteration 279, loss = 0.48521608114242554
iteration 280, loss = 0.5111306309700012
iteration 281, loss = 0.4486783444881439
iteration 282, loss = 0.47991669178009033
iteration 283, loss = 0.47100764513015747
iteration 284, loss = 0.4519825577735901
iteration 285, loss = 0.4593716263771057
iteration 286, loss = 0.4331517517566681
iteration 287, loss = 0.4690076410770416
iteration 288, loss = 0.4448816180229187
iteration 289, loss = 0.4796941578388214
iteration 290, loss = 0.4501859247684479
iteration 291, loss = 0.4460645318031311
iteration 292, loss = 0.4197254776954651
iteration 293, loss = 0.474910706281662
iteration 294, loss = 0.4355400502681732
iteration 295, loss = 0.5016573667526245
iteration 296, loss = 0.4763922095298767
iteration 297, loss = 0.46614617109298706
iteration 298, loss = 0.45645129680633545
iteration 299, loss = 0.4135698974132538
iteration 0, loss = 0.4352249205112457
iteration 1, loss = 0.4261080026626587
iteration 2, loss = 0.4561915695667267
iteration 3, loss = 0.48498594760894775
iteration 4, loss = 0.4574534296989441
iteration 5, loss = 0.4167497456073761
iteration 6, loss = 0.4127182960510254
iteration 7, loss = 0.40169963240623474
iteration 8, loss = 0.4392399489879608
iteration 9, loss = 0.45760345458984375
iteration 10, loss = 0.43893539905548096
iteration 11, loss = 0.4429663419723511
iteration 12, loss = 0.4683822989463806
iteration 13, loss = 0.5046136975288391
iteration 14, loss = 0.4271659255027771
iteration 15, loss = 0.4715753495693207
iteration 16, loss = 0.47686052322387695
iteration 17, loss = 0.4455440640449524
iteration 18, loss = 0.4884849488735199
iteration 19, loss = 0.46296197175979614
iteration 20, loss = 0.40134432911872864
iteration 21, loss = 0.45615896582603455
iteration 22, loss = 0.433788001537323
iteration 23, loss = 0.4981062114238739
iteration 24, loss = 0.43981271982192993
iteration 25, loss = 0.4635770916938782
iteration 26, loss = 0.4121819734573364
iteration 27, loss = 0.42290931940078735
iteration 28, loss = 0.3820863962173462
iteration 29, loss = 0.3960680067539215
iteration 30, loss = 0.4409039616584778
iteration 31, loss = 0.42595136165618896
iteration 32, loss = 0.5046054124832153
iteration 33, loss = 0.44688886404037476
iteration 34, loss = 0.41771945357322693
iteration 35, loss = 0.42288991808891296
iteration 36, loss = 0.40067625045776367
iteration 37, loss = 0.47522634267807007
iteration 38, loss = 0.48501336574554443
iteration 39, loss = 0.4368613660335541
iteration 40, loss = 0.4102563261985779
iteration 41, loss = 0.4777070879936218
iteration 42, loss = 0.3907574415206909
iteration 43, loss = 0.4066910743713379
iteration 44, loss = 0.46782881021499634
iteration 45, loss = 0.36669251322746277
iteration 46, loss = 0.4366138279438019
iteration 47, loss = 0.4143756926059723
iteration 48, loss = 0.45891931653022766
iteration 49, loss = 0.40968024730682373
iteration 50, loss = 0.458251953125
iteration 51, loss = 0.44100749492645264
iteration 52, loss = 0.42589911818504333
iteration 53, loss = 0.3968193829059601
iteration 54, loss = 0.3864704966545105
iteration 55, loss = 0.39372187852859497
iteration 56, loss = 0.431380957365036
iteration 57, loss = 0.44093576073646545
iteration 58, loss = 0.4130469560623169
iteration 59, loss = 0.4633699059486389
iteration 60, loss = 0.40983253717422485
iteration 61, loss = 0.3874812424182892
iteration 62, loss = 0.3827318251132965
iteration 63, loss = 0.4454491436481476
iteration 64, loss = 0.39727169275283813
iteration 65, loss = 0.40988534688949585
iteration 66, loss = 0.42591631412506104
iteration 67, loss = 0.4579438269138336
iteration 68, loss = 0.3926330506801605
iteration 69, loss = 0.37682151794433594
iteration 70, loss = 0.41916948556900024
iteration 71, loss = 0.3920097351074219
iteration 72, loss = 0.448485791683197
iteration 73, loss = 0.43500322103500366
iteration 74, loss = 0.3757244944572449
iteration 75, loss = 0.3941708505153656
iteration 76, loss = 0.45257726311683655
iteration 77, loss = 0.3965458869934082
iteration 78, loss = 0.4728735089302063
iteration 79, loss = 0.38260236382484436
iteration 80, loss = 0.434826135635376
iteration 81, loss = 0.42000508308410645
iteration 82, loss = 0.41451239585876465
iteration 83, loss = 0.3561471700668335
iteration 84, loss = 0.37812119722366333
iteration 85, loss = 0.3612551689147949
iteration 86, loss = 0.3945048451423645
iteration 87, loss = 0.37059924006462097
iteration 88, loss = 0.41029128432273865
iteration 89, loss = 0.4269629120826721
iteration 90, loss = 0.4214490056037903
iteration 91, loss = 0.3828991651535034
iteration 92, loss = 0.36009976267814636
iteration 93, loss = 0.3938677906990051
iteration 94, loss = 0.44789645075798035
iteration 95, loss = 0.40414243936538696
iteration 96, loss = 0.3376905918121338
iteration 97, loss = 0.37247201800346375
iteration 98, loss = 0.4016724228858948
iteration 99, loss = 0.38394036889076233
iteration 100, loss = 0.4106121063232422
iteration 101, loss = 0.38483425974845886
iteration 102, loss = 0.3873707950115204
iteration 103, loss = 0.38452982902526855
iteration 104, loss = 0.3771210312843323
iteration 105, loss = 0.3614081144332886
iteration 106, loss = 0.4047727882862091
iteration 107, loss = 0.46369147300720215
iteration 108, loss = 0.402407705783844
iteration 109, loss = 0.36986956000328064
iteration 110, loss = 0.34520137310028076
iteration 111, loss = 0.34787610173225403
iteration 112, loss = 0.4217848479747772
iteration 113, loss = 0.3618794083595276
iteration 114, loss = 0.37650737166404724
iteration 115, loss = 0.39396387338638306
iteration 116, loss = 0.3452568054199219
iteration 117, loss = 0.3496330976486206
iteration 118, loss = 0.40155673027038574
iteration 119, loss = 0.38700345158576965
iteration 120, loss = 0.3430562913417816
iteration 121, loss = 0.34264007210731506
iteration 122, loss = 0.3727262318134308
iteration 123, loss = 0.42413419485092163
iteration 124, loss = 0.39774757623672485
iteration 125, loss = 0.36492112278938293
iteration 126, loss = 0.3622211515903473
iteration 127, loss = 0.37570616602897644
iteration 128, loss = 0.33085060119628906
iteration 129, loss = 0.3945807218551636
iteration 130, loss = 0.3653564155101776
iteration 131, loss = 0.3499159812927246
iteration 132, loss = 0.36919790506362915
iteration 133, loss = 0.35276028513908386
iteration 134, loss = 0.33184748888015747
iteration 135, loss = 0.3722313344478607
iteration 136, loss = 0.3014994263648987
iteration 137, loss = 0.3851882219314575
iteration 138, loss = 0.31769058108329773
iteration 139, loss = 0.29915547370910645
iteration 140, loss = 0.37607118487358093
iteration 141, loss = 0.3358566164970398
iteration 142, loss = 0.314495712518692
iteration 143, loss = 0.33618539571762085
iteration 144, loss = 0.34842902421951294
iteration 145, loss = 0.3697550892829895
iteration 146, loss = 0.36772793531417847
iteration 147, loss = 0.37071800231933594
iteration 148, loss = 0.37200701236724854
iteration 149, loss = 0.36543741822242737
iteration 150, loss = 0.3309054970741272
iteration 151, loss = 0.3887147903442383
iteration 152, loss = 0.362237811088562
iteration 153, loss = 0.30510175228118896
iteration 154, loss = 0.34713223576545715
iteration 155, loss = 0.3135949969291687
iteration 156, loss = 0.30221831798553467
iteration 157, loss = 0.3145572245121002
iteration 158, loss = 0.3307476043701172
iteration 159, loss = 0.32825449109077454
iteration 160, loss = 0.3594260513782501
iteration 161, loss = 0.41238030791282654
iteration 162, loss = 0.3443617820739746
iteration 163, loss = 0.3231840431690216
iteration 164, loss = 0.31527450680732727
iteration 165, loss = 0.34439268708229065
iteration 166, loss = 0.35294854640960693
iteration 167, loss = 0.3749638795852661
iteration 168, loss = 0.352924644947052
iteration 169, loss = 0.3223034739494324
iteration 170, loss = 0.32060351967811584
iteration 171, loss = 0.43799999356269836
iteration 172, loss = 0.3664289712905884
iteration 173, loss = 0.36470651626586914
iteration 174, loss = 0.3322199285030365
iteration 175, loss = 0.30658838152885437
iteration 176, loss = 0.3378470838069916
iteration 177, loss = 0.3271663784980774
iteration 178, loss = 0.3449765741825104
iteration 179, loss = 0.31808850169181824
iteration 180, loss = 0.29771915078163147
iteration 181, loss = 0.353924036026001
iteration 182, loss = 0.3365746736526489
iteration 183, loss = 0.308307021856308
iteration 184, loss = 0.31272608041763306
iteration 185, loss = 0.31461024284362793
iteration 186, loss = 0.36834800243377686
iteration 187, loss = 0.3575354218482971
iteration 188, loss = 0.3162684142589569
iteration 189, loss = 0.326833039522171
iteration 190, loss = 0.28144603967666626
iteration 191, loss = 0.3103242516517639
iteration 192, loss = 0.3002055287361145
iteration 193, loss = 0.2990511953830719
iteration 194, loss = 0.3322140872478485
iteration 195, loss = 0.362017422914505
iteration 196, loss = 0.3547957241535187
iteration 197, loss = 0.32583680748939514
iteration 198, loss = 0.2945784330368042
iteration 199, loss = 0.3178855776786804
iteration 200, loss = 0.3097703456878662
iteration 201, loss = 0.36796054244041443
iteration 202, loss = 0.31546610593795776
iteration 203, loss = 0.306407630443573
iteration 204, loss = 0.32126420736312866
iteration 205, loss = 0.35073453187942505
iteration 206, loss = 0.2984640300273895
iteration 207, loss = 0.2693502902984619
iteration 208, loss = 0.29541119933128357
iteration 209, loss = 0.26902249455451965
iteration 210, loss = 0.3410474956035614
iteration 211, loss = 0.2888701856136322
iteration 212, loss = 0.3116413950920105
iteration 213, loss = 0.3748535215854645
iteration 214, loss = 0.29343271255493164
iteration 215, loss = 0.27290692925453186
iteration 216, loss = 0.2724887430667877
iteration 217, loss = 0.25540032982826233
iteration 218, loss = 0.32356369495391846
iteration 219, loss = 0.3196166753768921
iteration 220, loss = 0.3119099736213684
iteration 221, loss = 0.2707901895046234
iteration 222, loss = 0.2893061935901642
iteration 223, loss = 0.27404260635375977
iteration 224, loss = 0.2628805935382843
iteration 225, loss = 0.26796647906303406
iteration 226, loss = 0.2416081428527832
iteration 227, loss = 0.2828833758831024
iteration 228, loss = 0.2972375750541687
iteration 229, loss = 0.31063729524612427
iteration 230, loss = 0.38098394870758057
iteration 231, loss = 0.33007094264030457
iteration 232, loss = 0.3305406868457794
iteration 233, loss = 0.24074333906173706
iteration 234, loss = 0.28413331508636475
iteration 235, loss = 0.29252317547798157
iteration 236, loss = 0.3026418089866638
iteration 237, loss = 0.3273981213569641
iteration 238, loss = 0.28482091426849365
iteration 239, loss = 0.3130570352077484
iteration 240, loss = 0.2976362705230713
iteration 241, loss = 0.3228004276752472
iteration 242, loss = 0.2970203161239624
iteration 243, loss = 0.35614416003227234
iteration 244, loss = 0.2545285224914551
iteration 245, loss = 0.24842655658721924
iteration 246, loss = 0.335573673248291
iteration 247, loss = 0.30902981758117676
iteration 248, loss = 0.28786423802375793
iteration 249, loss = 0.3174397349357605
iteration 250, loss = 0.3346092998981476
iteration 251, loss = 0.24549418687820435
iteration 252, loss = 0.27798494696617126
iteration 253, loss = 0.2781936228275299
iteration 254, loss = 0.33377858996391296
iteration 255, loss = 0.37862637639045715
iteration 256, loss = 0.23605605959892273
iteration 257, loss = 0.3430224657058716
iteration 258, loss = 0.3135729134082794
iteration 259, loss = 0.23151852190494537
iteration 260, loss = 0.25813499093055725
iteration 261, loss = 0.26684847474098206
iteration 262, loss = 0.2628531754016876
iteration 263, loss = 0.3258039951324463
iteration 264, loss = 0.2404399812221527
iteration 265, loss = 0.3186297118663788
iteration 266, loss = 0.28120967745780945
iteration 267, loss = 0.27933090925216675
iteration 268, loss = 0.2834649682044983
iteration 269, loss = 0.3561933636665344
iteration 270, loss = 0.310764342546463
iteration 271, loss = 0.24165447056293488
iteration 272, loss = 0.21709860861301422
iteration 273, loss = 0.28497982025146484
iteration 274, loss = 0.24301674962043762
iteration 275, loss = 0.229718416929245
iteration 276, loss = 0.2812184989452362
iteration 277, loss = 0.2615886330604553
iteration 278, loss = 0.21848714351654053
iteration 279, loss = 0.31573712825775146
iteration 280, loss = 0.2851468324661255
iteration 281, loss = 0.25015509128570557
iteration 282, loss = 0.23746301233768463
iteration 283, loss = 0.2377263605594635
iteration 284, loss = 0.22974829375743866
iteration 285, loss = 0.24870413541793823
iteration 286, loss = 0.2532375454902649
iteration 287, loss = 0.2711026966571808
iteration 288, loss = 0.2059965431690216
iteration 289, loss = 0.31371885538101196
iteration 290, loss = 0.30572009086608887
iteration 291, loss = 0.25580117106437683
iteration 292, loss = 0.2545657753944397
iteration 293, loss = 0.21448010206222534
iteration 294, loss = 0.2652527987957001
iteration 295, loss = 0.23920270800590515
iteration 296, loss = 0.24518881738185883
iteration 297, loss = 0.23756523430347443
iteration 298, loss = 0.24615412950515747
iteration 299, loss = 0.22140255570411682
iteration 0, loss = 0.19617730379104614
iteration 1, loss = 0.28655725717544556
iteration 2, loss = 0.27067649364471436
iteration 3, loss = 0.24525025486946106
iteration 4, loss = 0.27562686800956726
iteration 5, loss = 0.22580820322036743
iteration 6, loss = 0.36487770080566406
iteration 7, loss = 0.2334948033094406
iteration 8, loss = 0.20587024092674255
iteration 9, loss = 0.2773723602294922
iteration 10, loss = 0.2337501049041748
iteration 11, loss = 0.276497483253479
iteration 12, loss = 0.20609644055366516
iteration 13, loss = 0.22889378666877747
iteration 14, loss = 0.20762518048286438
iteration 15, loss = 0.2875013053417206
iteration 16, loss = 0.2295137345790863
iteration 17, loss = 0.19980598986148834
iteration 18, loss = 0.24701011180877686
iteration 19, loss = 0.21128502488136292
iteration 20, loss = 0.2279219627380371
iteration 21, loss = 0.19121010601520538
iteration 22, loss = 0.2270810902118683
iteration 23, loss = 0.20383623242378235
iteration 24, loss = 0.21375258266925812
iteration 25, loss = 0.2849218547344208
iteration 26, loss = 0.26113957166671753
iteration 27, loss = 0.23537573218345642
iteration 28, loss = 0.24641035497188568
iteration 29, loss = 0.2862684726715088
iteration 30, loss = 0.21142467856407166
iteration 31, loss = 0.20975147187709808
iteration 32, loss = 0.2605236768722534
iteration 33, loss = 0.2030506581068039
iteration 34, loss = 0.20937305688858032
iteration 35, loss = 0.19475986063480377
iteration 36, loss = 0.19847159087657928
iteration 37, loss = 0.25389915704727173
iteration 38, loss = 0.2334243804216385
iteration 39, loss = 0.26164501905441284
iteration 40, loss = 0.22385050356388092
iteration 41, loss = 0.25081872940063477
iteration 42, loss = 0.22039416432380676
iteration 43, loss = 0.21377058327198029
iteration 44, loss = 0.20560283958911896
iteration 45, loss = 0.1851750910282135
iteration 46, loss = 0.20973283052444458
iteration 47, loss = 0.24479681253433228
iteration 48, loss = 0.19594630599021912
iteration 49, loss = 0.21880832314491272
iteration 50, loss = 0.21751835942268372
iteration 51, loss = 0.2259628176689148
iteration 52, loss = 0.24271340668201447
iteration 53, loss = 0.21254916489124298
iteration 54, loss = 0.24816493690013885
iteration 55, loss = 0.22434119880199432
iteration 56, loss = 0.2287832796573639
iteration 57, loss = 0.20949970185756683
iteration 58, loss = 0.2832295894622803
iteration 59, loss = 0.27410972118377686
iteration 60, loss = 0.2071034461259842
iteration 61, loss = 0.179026260972023
iteration 62, loss = 0.17019686102867126
iteration 63, loss = 0.20171186327934265
iteration 64, loss = 0.16862605512142181
iteration 65, loss = 0.25840622186660767
iteration 66, loss = 0.18999361991882324
iteration 67, loss = 0.21236558258533478
iteration 68, loss = 0.19422392547130585
iteration 69, loss = 0.21031928062438965
iteration 70, loss = 0.1977897733449936
iteration 71, loss = 0.23066391050815582
iteration 72, loss = 0.1768958419561386
iteration 73, loss = 0.26227667927742004
iteration 74, loss = 0.23390305042266846
iteration 75, loss = 0.17000418901443481
iteration 76, loss = 0.21397754549980164
iteration 77, loss = 0.20437736809253693
iteration 78, loss = 0.1984444409608841
iteration 79, loss = 0.18197055160999298
iteration 80, loss = 0.217693492770195
iteration 81, loss = 0.19042624533176422
iteration 82, loss = 0.18790069222450256
iteration 83, loss = 0.19805306196212769
iteration 84, loss = 0.20128050446510315
iteration 85, loss = 0.18834750354290009
iteration 86, loss = 0.17100214958190918
iteration 87, loss = 0.1778741478919983
iteration 88, loss = 0.17973476648330688
iteration 89, loss = 0.23724693059921265
iteration 90, loss = 0.2145184576511383
iteration 91, loss = 0.21988949179649353
iteration 92, loss = 0.19297783076763153
iteration 93, loss = 0.196671724319458
iteration 94, loss = 0.1770246922969818
iteration 95, loss = 0.18019497394561768
iteration 96, loss = 0.2027217149734497
iteration 97, loss = 0.17432227730751038
iteration 98, loss = 0.2846119701862335
iteration 99, loss = 0.1924845576286316
iteration 100, loss = 0.15274114906787872
iteration 101, loss = 0.1594323217868805
iteration 102, loss = 0.22488734126091003
iteration 103, loss = 0.18352888524532318
iteration 104, loss = 0.1820807307958603
iteration 105, loss = 0.16972552239894867
iteration 106, loss = 0.2007894665002823
iteration 107, loss = 0.2132611721754074
iteration 108, loss = 0.17652469873428345
iteration 109, loss = 0.1543758660554886
iteration 110, loss = 0.1640477031469345
iteration 111, loss = 0.21828168630599976
iteration 112, loss = 0.18328920006752014
iteration 113, loss = 0.18051758408546448
iteration 114, loss = 0.18703848123550415
iteration 115, loss = 0.22379127144813538
iteration 116, loss = 0.14964637160301208
iteration 117, loss = 0.1655043363571167
iteration 118, loss = 0.15887221693992615
iteration 119, loss = 0.2269423007965088
iteration 120, loss = 0.2475401610136032
iteration 121, loss = 0.23849725723266602
iteration 122, loss = 0.22983089089393616
iteration 123, loss = 0.2037360817193985
iteration 124, loss = 0.1632044017314911
iteration 125, loss = 0.170647531747818
iteration 126, loss = 0.19184227287769318
iteration 127, loss = 0.17327935993671417
iteration 128, loss = 0.18632927536964417
iteration 129, loss = 0.15132078528404236
iteration 130, loss = 0.18031752109527588
iteration 131, loss = 0.20007236301898956
iteration 132, loss = 0.15426558256149292
iteration 133, loss = 0.1984298974275589
iteration 134, loss = 0.18903760612010956
iteration 135, loss = 0.24695251882076263
iteration 136, loss = 0.18327927589416504
iteration 137, loss = 0.2055492103099823
iteration 138, loss = 0.13482387363910675
iteration 139, loss = 0.13430869579315186
iteration 140, loss = 0.16051971912384033
iteration 141, loss = 0.2254003882408142
iteration 142, loss = 0.19154079258441925
iteration 143, loss = 0.1684623807668686
iteration 144, loss = 0.1745060384273529
iteration 145, loss = 0.1485029011964798
iteration 146, loss = 0.15613262355327606
iteration 147, loss = 0.18651995062828064
iteration 148, loss = 0.14578983187675476
iteration 149, loss = 0.15458056330680847
iteration 150, loss = 0.14227499067783356
iteration 151, loss = 0.14010776579380035
iteration 152, loss = 0.13468171656131744
iteration 153, loss = 0.15769018232822418
iteration 154, loss = 0.20233853161334991
iteration 155, loss = 0.14133459329605103
iteration 156, loss = 0.19029678404331207
iteration 157, loss = 0.1670127958059311
iteration 158, loss = 0.16110612452030182
iteration 159, loss = 0.12033557146787643
iteration 160, loss = 0.1835203766822815
iteration 161, loss = 0.17720487713813782
iteration 162, loss = 0.15602800250053406
iteration 163, loss = 0.1758379340171814
iteration 164, loss = 0.19160602986812592
iteration 165, loss = 0.12923140823841095
iteration 166, loss = 0.1809288114309311
iteration 167, loss = 0.18401825428009033
iteration 168, loss = 0.1426331102848053
iteration 169, loss = 0.1641453504562378
iteration 170, loss = 0.1562923938035965
iteration 171, loss = 0.15174998342990875
iteration 172, loss = 0.1521613597869873
iteration 173, loss = 0.17045004665851593
iteration 174, loss = 0.1445161998271942
iteration 175, loss = 0.17056779563426971
iteration 176, loss = 0.11916903406381607
iteration 177, loss = 0.18338090181350708
iteration 178, loss = 0.14239731431007385
iteration 179, loss = 0.19441808760166168
iteration 180, loss = 0.22469139099121094
iteration 181, loss = 0.1370352804660797
iteration 182, loss = 0.1437184363603592
iteration 183, loss = 0.14935308694839478
iteration 184, loss = 0.15407754480838776
iteration 185, loss = 0.12587156891822815
iteration 186, loss = 0.13635751605033875
iteration 187, loss = 0.13673369586467743
iteration 188, loss = 0.13698332011699677
iteration 189, loss = 0.14904530346393585
iteration 190, loss = 0.16293865442276
iteration 191, loss = 0.15085378289222717
iteration 192, loss = 0.20542286336421967
iteration 193, loss = 0.18755026161670685
iteration 194, loss = 0.1498878300189972
iteration 195, loss = 0.16001304984092712
iteration 196, loss = 0.121554434299469
iteration 197, loss = 0.14364443719387054
iteration 198, loss = 0.1103486716747284
iteration 199, loss = 0.15693266689777374
iteration 200, loss = 0.14756377041339874
iteration 201, loss = 0.15792471170425415
iteration 202, loss = 0.1817047894001007
iteration 203, loss = 0.13882799446582794
iteration 204, loss = 0.12607021629810333
iteration 205, loss = 0.1469379961490631
iteration 206, loss = 0.13649941980838776
iteration 207, loss = 0.12273786216974258
iteration 208, loss = 0.1709020882844925
iteration 209, loss = 0.12717050313949585
iteration 210, loss = 0.1698034405708313
iteration 211, loss = 0.13516853749752045
iteration 212, loss = 0.13066533207893372
iteration 213, loss = 0.13513334095478058
iteration 214, loss = 0.1167760044336319
iteration 215, loss = 0.11571083217859268
iteration 216, loss = 0.11969634890556335
iteration 217, loss = 0.1504262387752533
iteration 218, loss = 0.15092533826828003
iteration 219, loss = 0.14578457176685333
iteration 220, loss = 0.13900338113307953
iteration 221, loss = 0.1333625316619873
iteration 222, loss = 0.16313856840133667
iteration 223, loss = 0.1350625604391098
iteration 224, loss = 0.12809111177921295
iteration 225, loss = 0.11251702904701233
iteration 226, loss = 0.1178179681301117
iteration 227, loss = 0.14903172850608826
iteration 228, loss = 0.1149507686495781
iteration 229, loss = 0.11430156230926514
iteration 230, loss = 0.13272181153297424
iteration 231, loss = 0.12496115267276764
iteration 232, loss = 0.13298916816711426
iteration 233, loss = 0.21738900244235992
iteration 234, loss = 0.10544565320014954
iteration 235, loss = 0.11726638674736023
iteration 236, loss = 0.11010679602622986
iteration 237, loss = 0.10246580839157104
iteration 238, loss = 0.19504843652248383
iteration 239, loss = 0.11177264899015427
iteration 240, loss = 0.13137120008468628
iteration 241, loss = 0.1649959683418274
iteration 242, loss = 0.10531353205442429
iteration 243, loss = 0.11738013476133347
iteration 244, loss = 0.12704400718212128
iteration 245, loss = 0.13168232142925262
iteration 246, loss = 0.11733533442020416
iteration 247, loss = 0.11838102340698242
iteration 248, loss = 0.1557629108428955
iteration 249, loss = 0.17434360086917877
iteration 250, loss = 0.11932005733251572
iteration 251, loss = 0.17044487595558167
iteration 252, loss = 0.11477747559547424
iteration 253, loss = 0.13479091227054596
iteration 254, loss = 0.15129771828651428
iteration 255, loss = 0.10487165302038193
iteration 256, loss = 0.09783615171909332
iteration 257, loss = 0.13950124382972717
iteration 258, loss = 0.13970117270946503
iteration 259, loss = 0.12752149999141693
iteration 260, loss = 0.11930719017982483
iteration 261, loss = 0.10554246604442596
iteration 262, loss = 0.1251770555973053
iteration 263, loss = 0.1329859495162964
iteration 264, loss = 0.11280415952205658
iteration 265, loss = 0.14320269227027893
iteration 266, loss = 0.11309637874364853
iteration 267, loss = 0.09924808144569397
iteration 268, loss = 0.11886967718601227
iteration 269, loss = 0.12595859169960022
iteration 270, loss = 0.09756852686405182
iteration 271, loss = 0.12506091594696045
iteration 272, loss = 0.1474064439535141
iteration 273, loss = 0.09383396804332733
iteration 274, loss = 0.1396646499633789
iteration 275, loss = 0.16846027970314026
iteration 276, loss = 0.09270084649324417
iteration 277, loss = 0.1242230162024498
iteration 278, loss = 0.09831423312425613
iteration 279, loss = 0.12873780727386475
iteration 280, loss = 0.1022358164191246
iteration 281, loss = 0.11138132214546204
iteration 282, loss = 0.13886745274066925
iteration 283, loss = 0.15017277002334595
iteration 284, loss = 0.09103944897651672
iteration 285, loss = 0.09442579746246338
iteration 286, loss = 0.09603666514158249
iteration 287, loss = 0.1260807067155838
iteration 288, loss = 0.10601840168237686
iteration 289, loss = 0.09400157630443573
iteration 290, loss = 0.10120409727096558
iteration 291, loss = 0.10606367141008377
iteration 292, loss = 0.11232700198888779
iteration 293, loss = 0.11207447946071625
iteration 294, loss = 0.09028920531272888
iteration 295, loss = 0.10271310061216354
iteration 296, loss = 0.11794406175613403
iteration 297, loss = 0.11053285002708435
iteration 298, loss = 0.09294463694095612
iteration 299, loss = 0.08592692017555237
iteration 0, loss = 0.1005534753203392
iteration 1, loss = 0.10148315131664276
iteration 2, loss = 0.11374043673276901
iteration 3, loss = 0.11320605129003525
iteration 4, loss = 0.11949831992387772
iteration 5, loss = 0.1380106657743454
iteration 6, loss = 0.10098189860582352
iteration 7, loss = 0.10987816751003265
iteration 8, loss = 0.11580689251422882
iteration 9, loss = 0.08736386895179749
iteration 10, loss = 0.11459200829267502
iteration 11, loss = 0.09129606187343597
iteration 12, loss = 0.12299798429012299
iteration 13, loss = 0.09016763418912888
iteration 14, loss = 0.16880907118320465
iteration 15, loss = 0.10322995483875275
iteration 16, loss = 0.10272911190986633
iteration 17, loss = 0.08441144227981567
iteration 18, loss = 0.11767349392175674
iteration 19, loss = 0.07925771921873093
iteration 20, loss = 0.09901731461286545
iteration 21, loss = 0.12427127361297607
iteration 22, loss = 0.10319755971431732
iteration 23, loss = 0.10524554550647736
iteration 24, loss = 0.13309982419013977
iteration 25, loss = 0.11663925647735596
iteration 26, loss = 0.10825692117214203
iteration 27, loss = 0.08577422052621841
iteration 28, loss = 0.08719256520271301
iteration 29, loss = 0.09266319870948792
iteration 30, loss = 0.1004813089966774
iteration 31, loss = 0.11117290705442429
iteration 32, loss = 0.10051443427801132
iteration 33, loss = 0.1006089523434639
iteration 34, loss = 0.12307155132293701
iteration 35, loss = 0.09044060856103897
iteration 36, loss = 0.09384869784116745
iteration 37, loss = 0.11593308299779892
iteration 38, loss = 0.09541662037372589
iteration 39, loss = 0.09780704975128174
iteration 40, loss = 0.11421964317560196
iteration 41, loss = 0.10813214629888535
iteration 42, loss = 0.0788654088973999
iteration 43, loss = 0.13553310930728912
iteration 44, loss = 0.08208170533180237
iteration 45, loss = 0.13530994951725006
iteration 46, loss = 0.09059121459722519
iteration 47, loss = 0.07709842175245285
iteration 48, loss = 0.08549613505601883
iteration 49, loss = 0.07827939838171005
iteration 50, loss = 0.10583178699016571
iteration 51, loss = 0.09790012985467911
iteration 52, loss = 0.101334348320961
iteration 53, loss = 0.07706677168607712
iteration 54, loss = 0.13464197516441345
iteration 55, loss = 0.08391234278678894
iteration 56, loss = 0.1048436388373375
iteration 57, loss = 0.0911337211728096
iteration 58, loss = 0.07504365593194962
iteration 59, loss = 0.082964688539505
iteration 60, loss = 0.12200386077165604
iteration 61, loss = 0.10945454239845276
iteration 62, loss = 0.09523411095142365
iteration 63, loss = 0.07852251827716827
iteration 64, loss = 0.14426074922084808
iteration 65, loss = 0.1038534939289093
iteration 66, loss = 0.09226226806640625
iteration 67, loss = 0.09357671439647675
iteration 68, loss = 0.0976656973361969
iteration 69, loss = 0.08208131045103073
iteration 70, loss = 0.0900460034608841
iteration 71, loss = 0.08684001863002777
iteration 72, loss = 0.09629932790994644
iteration 73, loss = 0.12362056225538254
iteration 74, loss = 0.10220583528280258
iteration 75, loss = 0.09411845356225967
iteration 76, loss = 0.08413074910640717
iteration 77, loss = 0.11094298958778381
iteration 78, loss = 0.0862501859664917
iteration 79, loss = 0.07262122631072998
iteration 80, loss = 0.09841175377368927
iteration 81, loss = 0.0836150050163269
iteration 82, loss = 0.07316512614488602
iteration 83, loss = 0.08142641186714172
iteration 84, loss = 0.07564954459667206
iteration 85, loss = 0.12167094647884369
iteration 86, loss = 0.08565971255302429
iteration 87, loss = 0.08578699827194214
iteration 88, loss = 0.07432219386100769
iteration 89, loss = 0.0743439644575119
iteration 90, loss = 0.13108468055725098
iteration 91, loss = 0.0804145336151123
iteration 92, loss = 0.09286152571439743
iteration 93, loss = 0.0873655304312706
iteration 94, loss = 0.11298377811908722
iteration 95, loss = 0.10959714651107788
iteration 96, loss = 0.077852763235569
iteration 97, loss = 0.09058275818824768
iteration 98, loss = 0.08470535278320312
iteration 99, loss = 0.08933112025260925
iteration 100, loss = 0.07937605679035187
iteration 101, loss = 0.16357900202274323
iteration 102, loss = 0.0750693827867508
iteration 103, loss = 0.08484024554491043
iteration 104, loss = 0.08939504623413086
iteration 105, loss = 0.07765421271324158
iteration 106, loss = 0.07769796997308731
iteration 107, loss = 0.06477196514606476
iteration 108, loss = 0.09708710759878159
iteration 109, loss = 0.07993681728839874
iteration 110, loss = 0.09682229161262512
iteration 111, loss = 0.07199646532535553
iteration 112, loss = 0.09356217831373215
iteration 113, loss = 0.11810709536075592
iteration 114, loss = 0.07762779295444489
iteration 115, loss = 0.07617464661598206
iteration 116, loss = 0.08673035353422165
iteration 117, loss = 0.07280965894460678
iteration 118, loss = 0.09112116694450378
iteration 119, loss = 0.08694028854370117
iteration 120, loss = 0.06258311122655869
iteration 121, loss = 0.09196354448795319
iteration 122, loss = 0.08088135719299316
iteration 123, loss = 0.11257779598236084
iteration 124, loss = 0.06764350086450577
iteration 125, loss = 0.0852474570274353
iteration 126, loss = 0.07175249606370926
iteration 127, loss = 0.10117921233177185
iteration 128, loss = 0.07417584210634232
iteration 129, loss = 0.08035657554864883
iteration 130, loss = 0.10602109879255295
iteration 131, loss = 0.11059539765119553
iteration 132, loss = 0.08563736081123352
iteration 133, loss = 0.07008276134729385
iteration 134, loss = 0.07232274115085602
iteration 135, loss = 0.07827439904212952
iteration 136, loss = 0.06357844918966293
iteration 137, loss = 0.07600509375333786
iteration 138, loss = 0.06867581605911255
iteration 139, loss = 0.08907567709684372
iteration 140, loss = 0.07158458977937698
iteration 141, loss = 0.07436531782150269
iteration 142, loss = 0.07847362756729126
iteration 143, loss = 0.07649239897727966
iteration 144, loss = 0.10683728009462357
iteration 145, loss = 0.1067303940653801
iteration 146, loss = 0.07608428597450256
iteration 147, loss = 0.07740660011768341
iteration 148, loss = 0.06212814897298813
iteration 149, loss = 0.06085361912846565
iteration 150, loss = 0.13477277755737305
iteration 151, loss = 0.06831438094377518
iteration 152, loss = 0.08143746852874756
iteration 153, loss = 0.066645547747612
iteration 154, loss = 0.07727569341659546
iteration 155, loss = 0.07082823663949966
iteration 156, loss = 0.05988208204507828
iteration 157, loss = 0.10286727547645569
iteration 158, loss = 0.10769221186637878
iteration 159, loss = 0.07846683263778687
iteration 160, loss = 0.08073762059211731
iteration 161, loss = 0.0746859759092331
iteration 162, loss = 0.06271781027317047
iteration 163, loss = 0.07127420604228973
iteration 164, loss = 0.07772888988256454
iteration 165, loss = 0.0660175159573555
iteration 166, loss = 0.10283049941062927
iteration 167, loss = 0.07176550477743149
iteration 168, loss = 0.06946932524442673
iteration 169, loss = 0.07792661339044571
iteration 170, loss = 0.06252651661634445
iteration 171, loss = 0.060348786413669586
iteration 172, loss = 0.061389029026031494
iteration 173, loss = 0.07063627243041992
iteration 174, loss = 0.06057815998792648
iteration 175, loss = 0.06936533004045486
iteration 176, loss = 0.05526740103960037
iteration 177, loss = 0.08883554488420486
iteration 178, loss = 0.08070353418588638
iteration 179, loss = 0.09780646860599518
iteration 180, loss = 0.06342813372612
iteration 181, loss = 0.13043509423732758
iteration 182, loss = 0.06440557539463043
iteration 183, loss = 0.057948220521211624
iteration 184, loss = 0.05793144926428795
iteration 185, loss = 0.0557788722217083
iteration 186, loss = 0.09176971018314362
iteration 187, loss = 0.06891164928674698
iteration 188, loss = 0.06825507432222366
iteration 189, loss = 0.05847957730293274
iteration 190, loss = 0.06619364768266678
iteration 191, loss = 0.05423586443066597
iteration 192, loss = 0.06590818613767624
iteration 193, loss = 0.06081055849790573
iteration 194, loss = 0.060021478682756424
iteration 195, loss = 0.06598056852817535
iteration 196, loss = 0.07117794454097748
iteration 197, loss = 0.06039319187402725
iteration 198, loss = 0.058380600064992905
iteration 199, loss = 0.07415547221899033
iteration 200, loss = 0.11292506009340286
iteration 201, loss = 0.05220181122422218
iteration 202, loss = 0.05846197530627251
iteration 203, loss = 0.06322994828224182
iteration 204, loss = 0.06017337739467621
iteration 205, loss = 0.07494496554136276
iteration 206, loss = 0.05161178857088089
iteration 207, loss = 0.07275522500276566
iteration 208, loss = 0.09819406270980835
iteration 209, loss = 0.09360720217227936
iteration 210, loss = 0.059385985136032104
iteration 211, loss = 0.06559060513973236
iteration 212, loss = 0.07259009033441544
iteration 213, loss = 0.058653414249420166
iteration 214, loss = 0.07334648817777634
iteration 215, loss = 0.05655495822429657
iteration 216, loss = 0.08086981624364853
iteration 217, loss = 0.059639740735292435
iteration 218, loss = 0.07937339693307877
iteration 219, loss = 0.07716454565525055
iteration 220, loss = 0.06343524158000946
iteration 221, loss = 0.09431533515453339
iteration 222, loss = 0.06220366060733795
iteration 223, loss = 0.05405296012759209
iteration 224, loss = 0.06475791335105896
iteration 225, loss = 0.055680353194475174
iteration 226, loss = 0.07724077999591827
iteration 227, loss = 0.0643947646021843
iteration 228, loss = 0.09061124920845032
iteration 229, loss = 0.05617884546518326
iteration 230, loss = 0.06009560078382492
iteration 231, loss = 0.05572342872619629
iteration 232, loss = 0.07122372835874557
iteration 233, loss = 0.05660845339298248
iteration 234, loss = 0.057763807475566864
iteration 235, loss = 0.058927956968545914
iteration 236, loss = 0.07864305377006531
iteration 237, loss = 0.06905481219291687
iteration 238, loss = 0.05308908224105835
iteration 239, loss = 0.056743234395980835
iteration 240, loss = 0.05920810624957085
iteration 241, loss = 0.06222478300333023
iteration 242, loss = 0.05401751771569252
iteration 243, loss = 0.06755181401968002
iteration 244, loss = 0.061038076877593994
iteration 245, loss = 0.05131204053759575
iteration 246, loss = 0.054489508271217346
iteration 247, loss = 0.05759894475340843
iteration 248, loss = 0.06322326511144638
iteration 249, loss = 0.06437284499406815
iteration 250, loss = 0.060441579669713974
iteration 251, loss = 0.11723306030035019
iteration 252, loss = 0.06386926025152206
iteration 253, loss = 0.05295221880078316
iteration 254, loss = 0.05674683302640915
iteration 255, loss = 0.061467599123716354
iteration 256, loss = 0.06572363525629044
iteration 257, loss = 0.059400126338005066
iteration 258, loss = 0.05634760484099388
iteration 259, loss = 0.05029940977692604
iteration 260, loss = 0.05791483819484711
iteration 261, loss = 0.05224379152059555
iteration 262, loss = 0.048256710171699524
iteration 263, loss = 0.05988818407058716
iteration 264, loss = 0.05820014327764511
iteration 265, loss = 0.08171061426401138
iteration 266, loss = 0.059677816927433014
iteration 267, loss = 0.059473760426044464
iteration 268, loss = 0.05167132616043091
iteration 269, loss = 0.05716796591877937
iteration 270, loss = 0.07099348306655884
iteration 271, loss = 0.08312413841485977
iteration 272, loss = 0.06006496399641037
iteration 273, loss = 0.049117449671030045
iteration 274, loss = 0.0627295970916748
iteration 275, loss = 0.050511959940195084
iteration 276, loss = 0.050325777381658554
iteration 277, loss = 0.05166814476251602
iteration 278, loss = 0.0552193745970726
iteration 279, loss = 0.055290527641773224
iteration 280, loss = 0.051400139927864075
iteration 281, loss = 0.048966169357299805
iteration 282, loss = 0.05663006007671356
iteration 283, loss = 0.066073477268219
iteration 284, loss = 0.04364341124892235
iteration 285, loss = 0.04491569474339485
iteration 286, loss = 0.05646020919084549
iteration 287, loss = 0.05162933096289635
iteration 288, loss = 0.045462507754564285
iteration 289, loss = 0.04716745764017105
iteration 290, loss = 0.08227419853210449
iteration 291, loss = 0.062168315052986145
iteration 292, loss = 0.04872296378016472
iteration 293, loss = 0.07380170375108719
iteration 294, loss = 0.06045371666550636
iteration 295, loss = 0.047455403953790665
iteration 296, loss = 0.05221516638994217
iteration 297, loss = 0.05027276650071144
iteration 298, loss = 0.04852214455604553
iteration 299, loss = 0.05620814487338066
iteration 0, loss = 0.04655887931585312
iteration 1, loss = 0.047346096485853195
iteration 2, loss = 0.05352022498846054
iteration 3, loss = 0.050890736281871796
iteration 4, loss = 0.04451572149991989
iteration 5, loss = 0.045624107122421265
iteration 6, loss = 0.04459979757666588
iteration 7, loss = 0.05881365388631821
iteration 8, loss = 0.05271501839160919
iteration 9, loss = 0.04917079955339432
iteration 10, loss = 0.058288708329200745
iteration 11, loss = 0.05264698341488838
iteration 12, loss = 0.08036330342292786
iteration 13, loss = 0.04277224466204643
iteration 14, loss = 0.04529716819524765
iteration 15, loss = 0.052284643054008484
iteration 16, loss = 0.054608047008514404
iteration 17, loss = 0.06052570790052414
iteration 18, loss = 0.046547453850507736
iteration 19, loss = 0.04853884130716324
iteration 20, loss = 0.047048695385456085
iteration 21, loss = 0.04760292172431946
iteration 22, loss = 0.04791760444641113
iteration 23, loss = 0.044597361236810684
iteration 24, loss = 0.08296719938516617
iteration 25, loss = 0.06717696785926819
iteration 26, loss = 0.07437889277935028
iteration 27, loss = 0.0807073712348938
iteration 28, loss = 0.09658980369567871
iteration 29, loss = 0.05221957713365555
iteration 30, loss = 0.049583226442337036
iteration 31, loss = 0.06292533874511719
iteration 32, loss = 0.049270499497652054
iteration 33, loss = 0.04248068481683731
iteration 34, loss = 0.05199425667524338
iteration 35, loss = 0.052219681441783905
iteration 36, loss = 0.04820984974503517
iteration 37, loss = 0.048175521194934845
iteration 38, loss = 0.04722120240330696
iteration 39, loss = 0.04867129027843475
iteration 40, loss = 0.050211288034915924
iteration 41, loss = 0.048100546002388
iteration 42, loss = 0.07750549912452698
iteration 43, loss = 0.04880771040916443
iteration 44, loss = 0.04313215985894203
iteration 45, loss = 0.061502449214458466
iteration 46, loss = 0.047774676233530045
iteration 47, loss = 0.04130115732550621
iteration 48, loss = 0.045401882380247116
iteration 49, loss = 0.05010677129030228
iteration 50, loss = 0.043480537831783295
iteration 51, loss = 0.048676446080207825
iteration 52, loss = 0.046029821038246155
iteration 53, loss = 0.04133975878357887
iteration 54, loss = 0.05335300415754318
iteration 55, loss = 0.04479105398058891
iteration 56, loss = 0.04871558025479317
iteration 57, loss = 0.056752562522888184
iteration 58, loss = 0.052170563489198685
iteration 59, loss = 0.07330605387687683
iteration 60, loss = 0.0408172681927681
iteration 61, loss = 0.04935314506292343
iteration 62, loss = 0.044064585119485855
iteration 63, loss = 0.07542334496974945
iteration 64, loss = 0.03853438422083855
iteration 65, loss = 0.05222269892692566
iteration 66, loss = 0.04768120497465134
iteration 67, loss = 0.057801105082035065
iteration 68, loss = 0.040273673832416534
iteration 69, loss = 0.03909870237112045
iteration 70, loss = 0.04030545428395271
iteration 71, loss = 0.0434185191988945
iteration 72, loss = 0.04428904503583908
iteration 73, loss = 0.044036395847797394
iteration 74, loss = 0.042003363370895386
iteration 75, loss = 0.040103729814291
iteration 76, loss = 0.046532947570085526
iteration 77, loss = 0.05016811564564705
iteration 78, loss = 0.07829563319683075
iteration 79, loss = 0.07391763478517532
iteration 80, loss = 0.04025331139564514
iteration 81, loss = 0.07077734172344208
iteration 82, loss = 0.06387575715780258
iteration 83, loss = 0.03786713629961014
iteration 84, loss = 0.04271663352847099
iteration 85, loss = 0.0408116951584816
iteration 86, loss = 0.07131470739841461
iteration 87, loss = 0.040100567042827606
iteration 88, loss = 0.048750270158052444
iteration 89, loss = 0.04585503786802292
iteration 90, loss = 0.05355522409081459
iteration 91, loss = 0.04424414411187172
iteration 92, loss = 0.03938574343919754
iteration 93, loss = 0.06818199157714844
iteration 94, loss = 0.05143725872039795
iteration 95, loss = 0.03901570290327072
iteration 96, loss = 0.047991834580898285
iteration 97, loss = 0.0445004440844059
iteration 98, loss = 0.03842166066169739
iteration 99, loss = 0.04982459545135498
iteration 100, loss = 0.040505945682525635
iteration 101, loss = 0.04055482894182205
iteration 102, loss = 0.04396422207355499
iteration 103, loss = 0.04621508717536926
iteration 104, loss = 0.04027058556675911
iteration 105, loss = 0.044355690479278564
iteration 106, loss = 0.04167066887021065
iteration 107, loss = 0.04645445570349693
iteration 108, loss = 0.03670617192983627
iteration 109, loss = 0.038988590240478516
iteration 110, loss = 0.052810974419116974
iteration 111, loss = 0.0397469624876976
iteration 112, loss = 0.04105325788259506
iteration 113, loss = 0.042916759848594666
iteration 114, loss = 0.042307298630476
iteration 115, loss = 0.04230448603630066
iteration 116, loss = 0.03832288086414337
iteration 117, loss = 0.06565060466527939
iteration 118, loss = 0.046102266758680344
iteration 119, loss = 0.041171953082084656
iteration 120, loss = 0.03544621542096138
iteration 121, loss = 0.0389929935336113
iteration 122, loss = 0.03625086694955826
iteration 123, loss = 0.04339691996574402
iteration 124, loss = 0.04102209210395813
iteration 125, loss = 0.05550418049097061
iteration 126, loss = 0.04222872853279114
iteration 127, loss = 0.05074627324938774
iteration 128, loss = 0.05080674961209297
iteration 129, loss = 0.03529626876115799
iteration 130, loss = 0.03778296709060669
iteration 131, loss = 0.04425933584570885
iteration 132, loss = 0.039576370269060135
iteration 133, loss = 0.06730759888887405
iteration 134, loss = 0.04690665006637573
iteration 135, loss = 0.03502676635980606
iteration 136, loss = 0.037554770708084106
iteration 137, loss = 0.03888546675443649
iteration 138, loss = 0.037619635462760925
iteration 139, loss = 0.04013275355100632
iteration 140, loss = 0.045554034411907196
iteration 141, loss = 0.04061930626630783
iteration 142, loss = 0.0372341126203537
iteration 143, loss = 0.043358929455280304
iteration 144, loss = 0.04936736822128296
iteration 145, loss = 0.04085994511842728
iteration 146, loss = 0.04425012320280075
iteration 147, loss = 0.03466249257326126
iteration 148, loss = 0.040634606033563614
iteration 149, loss = 0.03874509409070015
iteration 150, loss = 0.0619458444416523
iteration 151, loss = 0.035834427922964096
iteration 152, loss = 0.03662954270839691
iteration 153, loss = 0.040615979582071304
iteration 154, loss = 0.04069504514336586
iteration 155, loss = 0.04133901745080948
iteration 156, loss = 0.04011101275682449
iteration 157, loss = 0.06270116567611694
iteration 158, loss = 0.0409473292529583
iteration 159, loss = 0.043010469526052475
iteration 160, loss = 0.03389974310994148
iteration 161, loss = 0.03818931430578232
iteration 162, loss = 0.040960077196359634
iteration 163, loss = 0.03526461124420166
iteration 164, loss = 0.03897840902209282
iteration 165, loss = 0.03914480283856392
iteration 166, loss = 0.04018482565879822
iteration 167, loss = 0.06168809533119202
iteration 168, loss = 0.036724355071783066
iteration 169, loss = 0.03503699228167534
iteration 170, loss = 0.04155634343624115
iteration 171, loss = 0.054953090846538544
iteration 172, loss = 0.06640879809856415
iteration 173, loss = 0.05882837250828743
iteration 174, loss = 0.044614002108573914
iteration 175, loss = 0.03615536913275719
iteration 176, loss = 0.03405659645795822
iteration 177, loss = 0.03253724426031113
iteration 178, loss = 0.03371840715408325
iteration 179, loss = 0.04156454652547836
iteration 180, loss = 0.039054986089468
iteration 181, loss = 0.03841607645153999
iteration 182, loss = 0.05489588901400566
iteration 183, loss = 0.042532775551080704
iteration 184, loss = 0.03948166221380234
iteration 185, loss = 0.03308659791946411
iteration 186, loss = 0.032598406076431274
iteration 187, loss = 0.034650515764951706
iteration 188, loss = 0.03666285425424576
iteration 189, loss = 0.033579979091882706
iteration 190, loss = 0.043077945709228516
iteration 191, loss = 0.04030884802341461
iteration 192, loss = 0.06436006724834442
iteration 193, loss = 0.06749606132507324
iteration 194, loss = 0.04236985743045807
iteration 195, loss = 0.04233295097947121
iteration 196, loss = 0.0347888208925724
iteration 197, loss = 0.06193061172962189
iteration 198, loss = 0.036374226212501526
iteration 199, loss = 0.03652278333902359
iteration 200, loss = 0.03584350273013115
iteration 201, loss = 0.04191521927714348
iteration 202, loss = 0.036663997918367386
iteration 203, loss = 0.0350881963968277
iteration 204, loss = 0.05884428322315216
iteration 205, loss = 0.03383245691657066
iteration 206, loss = 0.060609012842178345
iteration 207, loss = 0.03966321423649788
iteration 208, loss = 0.04108569771051407
iteration 209, loss = 0.03082757070660591
iteration 210, loss = 0.07704266160726547
iteration 211, loss = 0.031126441434025764
iteration 212, loss = 0.03424764424562454
iteration 213, loss = 0.03578142076730728
iteration 214, loss = 0.03206149861216545
iteration 215, loss = 0.03377949818968773
iteration 216, loss = 0.0353909395635128
iteration 217, loss = 0.03044508770108223
iteration 218, loss = 0.029695257544517517
iteration 219, loss = 0.056290727108716965
iteration 220, loss = 0.03837791830301285
iteration 221, loss = 0.04303200915455818
iteration 222, loss = 0.04043196141719818
iteration 223, loss = 0.03515483811497688
iteration 224, loss = 0.03719989210367203
iteration 225, loss = 0.037665050476789474
iteration 226, loss = 0.030729545280337334
iteration 227, loss = 0.06053934618830681
iteration 228, loss = 0.030248524621129036
iteration 229, loss = 0.06766801327466965
iteration 230, loss = 0.03467637673020363
iteration 231, loss = 0.0316135473549366
iteration 232, loss = 0.046851444989442825
iteration 233, loss = 0.034973062574863434
iteration 234, loss = 0.030221622437238693
iteration 235, loss = 0.04746430739760399
iteration 236, loss = 0.02995448000729084
iteration 237, loss = 0.03045281581580639
iteration 238, loss = 0.03456525504589081
iteration 239, loss = 0.0424409918487072
iteration 240, loss = 0.03242407739162445
iteration 241, loss = 0.03454233333468437
iteration 242, loss = 0.031534694135189056
iteration 243, loss = 0.03206625580787659
iteration 244, loss = 0.035337794572114944
iteration 245, loss = 0.03800441324710846
iteration 246, loss = 0.03580022230744362
iteration 247, loss = 0.02875160612165928
iteration 248, loss = 0.03323262557387352
iteration 249, loss = 0.03622327744960785
iteration 250, loss = 0.03300469368696213
iteration 251, loss = 0.03077535517513752
iteration 252, loss = 0.03077373467385769
iteration 253, loss = 0.0303233303129673
iteration 254, loss = 0.03149120882153511
iteration 255, loss = 0.0331466943025589
iteration 256, loss = 0.05243733897805214
iteration 257, loss = 0.031059077009558678
iteration 258, loss = 0.04839293658733368
iteration 259, loss = 0.036731693893671036
iteration 260, loss = 0.03892691805958748
iteration 261, loss = 0.028104165568947792
iteration 262, loss = 0.02909894287586212
iteration 263, loss = 0.03292505443096161
iteration 264, loss = 0.03491320088505745
iteration 265, loss = 0.027826229110360146
iteration 266, loss = 0.043607208877801895
iteration 267, loss = 0.0331135094165802
iteration 268, loss = 0.03718412667512894
iteration 269, loss = 0.033791378140449524
iteration 270, loss = 0.02818046510219574
iteration 271, loss = 0.03524554520845413
iteration 272, loss = 0.03933707997202873
iteration 273, loss = 0.032098934054374695
iteration 274, loss = 0.057791344821453094
iteration 275, loss = 0.04954076185822487
iteration 276, loss = 0.029622143134474754
iteration 277, loss = 0.026770785450935364
iteration 278, loss = 0.03207622095942497
iteration 279, loss = 0.042910609394311905
iteration 280, loss = 0.03172637149691582
iteration 281, loss = 0.03286804258823395
iteration 282, loss = 0.033283837139606476
iteration 283, loss = 0.029348792508244514
iteration 284, loss = 0.027995727956295013
iteration 285, loss = 0.029386695474386215
iteration 286, loss = 0.031000414863228798
iteration 287, loss = 0.02972586266696453
iteration 288, loss = 0.052167732268571854
iteration 289, loss = 0.03630215674638748
iteration 290, loss = 0.03285690397024155
iteration 291, loss = 0.026497486978769302
iteration 292, loss = 0.029150474816560745
iteration 293, loss = 0.031440895050764084
iteration 294, loss = 0.04008603096008301
iteration 295, loss = 0.029057571664452553
iteration 296, loss = 0.029642893001437187
iteration 297, loss = 0.027433348819613457
iteration 298, loss = 0.026556797325611115
iteration 299, loss = 0.034776534885168076
iteration 0, loss = 0.034835055470466614
iteration 1, loss = 0.029789980500936508
iteration 2, loss = 0.03031356818974018
iteration 3, loss = 0.029624000191688538
iteration 4, loss = 0.028488535434007645
iteration 5, loss = 0.04241943359375
iteration 6, loss = 0.03095938451588154
iteration 7, loss = 0.04013236612081528
iteration 8, loss = 0.035850949585437775
iteration 9, loss = 0.026089441031217575
iteration 10, loss = 0.03090791404247284
iteration 11, loss = 0.04271050915122032
iteration 12, loss = 0.05695444345474243
iteration 13, loss = 0.030125891789793968
iteration 14, loss = 0.04914176091551781
iteration 15, loss = 0.030337167903780937
iteration 16, loss = 0.052843138575553894
iteration 17, loss = 0.05158234387636185
iteration 18, loss = 0.0246349535882473
iteration 19, loss = 0.03380852937698364
iteration 20, loss = 0.027742931619286537
iteration 21, loss = 0.02868996001780033
iteration 22, loss = 0.030155951157212257
iteration 23, loss = 0.028518516570329666
iteration 24, loss = 0.030083773657679558
iteration 25, loss = 0.05100955814123154
iteration 26, loss = 0.03112625889480114
iteration 27, loss = 0.02603408880531788
iteration 28, loss = 0.025079555809497833
iteration 29, loss = 0.056874409317970276
iteration 30, loss = 0.026855643838644028
iteration 31, loss = 0.02562914788722992
iteration 32, loss = 0.02466540038585663
iteration 33, loss = 0.026066116988658905
iteration 34, loss = 0.02735704369843006
iteration 35, loss = 0.026455312967300415
iteration 36, loss = 0.030320504680275917
iteration 37, loss = 0.053787123411893845
iteration 38, loss = 0.02895818091928959
iteration 39, loss = 0.031085167080163956
iteration 40, loss = 0.0289066880941391
iteration 41, loss = 0.030855707824230194
iteration 42, loss = 0.02764950878918171
iteration 43, loss = 0.030749445781111717
iteration 44, loss = 0.03048967383801937
iteration 45, loss = 0.02629784122109413
iteration 46, loss = 0.03165948763489723
iteration 47, loss = 0.024861779063940048
iteration 48, loss = 0.04083879292011261
iteration 49, loss = 0.025483030825853348
iteration 50, loss = 0.031967803835868835
iteration 51, loss = 0.03028077632188797
iteration 52, loss = 0.0301828533411026
iteration 53, loss = 0.026361875236034393
iteration 54, loss = 0.026429949328303337
iteration 55, loss = 0.026650261133909225
iteration 56, loss = 0.05130596458911896
iteration 57, loss = 0.032006632536649704
iteration 58, loss = 0.0302040446549654
iteration 59, loss = 0.025500578805804253
iteration 60, loss = 0.027259761467576027
iteration 61, loss = 0.027746379375457764
iteration 62, loss = 0.027777306735515594
iteration 63, loss = 0.028799142688512802
iteration 64, loss = 0.03121732920408249
iteration 65, loss = 0.025124702602624893
iteration 66, loss = 0.02635779418051243
iteration 67, loss = 0.0473192036151886
iteration 68, loss = 0.03041631169617176
iteration 69, loss = 0.04363095015287399
iteration 70, loss = 0.02454761043190956
iteration 71, loss = 0.027593275532126427
iteration 72, loss = 0.023938555270433426
iteration 73, loss = 0.05519277602434158
iteration 74, loss = 0.05575733259320259
iteration 75, loss = 0.032060667872428894
iteration 76, loss = 0.024824485182762146
iteration 77, loss = 0.026013242080807686
iteration 78, loss = 0.029652103781700134
iteration 79, loss = 0.02801673486828804
iteration 80, loss = 0.025350555777549744
iteration 81, loss = 0.02671295776963234
iteration 82, loss = 0.024938398972153664
iteration 83, loss = 0.02857431210577488
iteration 84, loss = 0.024413224309682846
iteration 85, loss = 0.05061832070350647
iteration 86, loss = 0.025639303028583527
iteration 87, loss = 0.02763608656823635
iteration 88, loss = 0.02874705381691456
iteration 89, loss = 0.03955107554793358
iteration 90, loss = 0.027846986427903175
iteration 91, loss = 0.028704172000288963
iteration 92, loss = 0.02726079151034355
iteration 93, loss = 0.027821853756904602
iteration 94, loss = 0.03811390697956085
iteration 95, loss = 0.026799846440553665
iteration 96, loss = 0.03125271201133728
iteration 97, loss = 0.027460314333438873
iteration 98, loss = 0.02470465749502182
iteration 99, loss = 0.02571786195039749
iteration 100, loss = 0.024920256808400154
iteration 101, loss = 0.025515470653772354
iteration 102, loss = 0.02527172863483429
iteration 103, loss = 0.02663729153573513
iteration 104, loss = 0.022969968616962433
iteration 105, loss = 0.027974849566817284
iteration 106, loss = 0.027590906247496605
iteration 107, loss = 0.03052675351500511
iteration 108, loss = 0.029308408498764038
iteration 109, loss = 0.04585263133049011
iteration 110, loss = 0.027902336791157722
iteration 111, loss = 0.030599990859627724
iteration 112, loss = 0.024893010035157204
iteration 113, loss = 0.024065658450126648
iteration 114, loss = 0.024905016645789146
iteration 115, loss = 0.024065334349870682
iteration 116, loss = 0.023080233484506607
iteration 117, loss = 0.026029512286186218
iteration 118, loss = 0.02496800385415554
iteration 119, loss = 0.02166491001844406
iteration 120, loss = 0.028007403016090393
iteration 121, loss = 0.024454690515995026
iteration 122, loss = 0.025534942746162415
iteration 123, loss = 0.028224904090166092
iteration 124, loss = 0.02470712549984455
iteration 125, loss = 0.025920918211340904
iteration 126, loss = 0.025854341685771942
iteration 127, loss = 0.029427550733089447
iteration 128, loss = 0.0376109704375267
iteration 129, loss = 0.02526840940117836
iteration 130, loss = 0.027430443093180656
iteration 131, loss = 0.02724381349980831
iteration 132, loss = 0.028090674430131912
iteration 133, loss = 0.03222465515136719
iteration 134, loss = 0.026583168655633926
iteration 135, loss = 0.023294346407055855
iteration 136, loss = 0.022732378914952278
iteration 137, loss = 0.02779501862823963
iteration 138, loss = 0.021354923024773598
iteration 139, loss = 0.021793881431221962
iteration 140, loss = 0.02327527105808258
iteration 141, loss = 0.02582087554037571
iteration 142, loss = 0.022982284426689148
iteration 143, loss = 0.02208402194082737
iteration 144, loss = 0.022809898480772972
iteration 145, loss = 0.03629564866423607
iteration 146, loss = 0.022731123492121696
iteration 147, loss = 0.02707253396511078
iteration 148, loss = 0.025741787627339363
iteration 149, loss = 0.022972218692302704
iteration 150, loss = 0.023953868076205254
iteration 151, loss = 0.026502683758735657
iteration 152, loss = 0.026289168745279312
iteration 153, loss = 0.030103541910648346
iteration 154, loss = 0.02401714213192463
iteration 155, loss = 0.04512368142604828
iteration 156, loss = 0.022400353103876114
iteration 157, loss = 0.04721223562955856
iteration 158, loss = 0.038130369037389755
iteration 159, loss = 0.036494094878435135
iteration 160, loss = 0.03537755832076073
iteration 161, loss = 0.027507081627845764
iteration 162, loss = 0.024080218747258186
iteration 163, loss = 0.027466146275401115
iteration 164, loss = 0.026592684909701347
iteration 165, loss = 0.024284427985548973
iteration 166, loss = 0.025559045374393463
iteration 167, loss = 0.0297298114746809
iteration 168, loss = 0.023345407098531723
iteration 169, loss = 0.035882823169231415
iteration 170, loss = 0.0347587876021862
iteration 171, loss = 0.021608587354421616
iteration 172, loss = 0.026859775185585022
iteration 173, loss = 0.03151596337556839
iteration 174, loss = 0.03229488804936409
iteration 175, loss = 0.024516617879271507
iteration 176, loss = 0.02170175313949585
iteration 177, loss = 0.021824797615408897
iteration 178, loss = 0.024151980876922607
iteration 179, loss = 0.022447114810347557
iteration 180, loss = 0.02340850792825222
iteration 181, loss = 0.025546114891767502
iteration 182, loss = 0.027496784925460815
iteration 183, loss = 0.025090519338846207
iteration 184, loss = 0.02361113391816616
iteration 185, loss = 0.028571993112564087
iteration 186, loss = 0.020883573219180107
iteration 187, loss = 0.023854801431298256
iteration 188, loss = 0.022908173501491547
iteration 189, loss = 0.04258177429437637
iteration 190, loss = 0.026624996215105057
iteration 191, loss = 0.02182011492550373
iteration 192, loss = 0.020269205793738365
iteration 193, loss = 0.021676771342754364
iteration 194, loss = 0.0215364471077919
iteration 195, loss = 0.0237722210586071
iteration 196, loss = 0.02313276194036007
iteration 197, loss = 0.021100929006934166
iteration 198, loss = 0.020900392904877663
iteration 199, loss = 0.022574827075004578
iteration 200, loss = 0.02365492656826973
iteration 201, loss = 0.02142038196325302
iteration 202, loss = 0.02171078510582447
iteration 203, loss = 0.03312649950385094
iteration 204, loss = 0.02247386984527111
iteration 205, loss = 0.024163026362657547
iteration 206, loss = 0.021644003689289093
iteration 207, loss = 0.025723882019519806
iteration 208, loss = 0.023922540247440338
iteration 209, loss = 0.019360458478331566
iteration 210, loss = 0.04394683614373207
iteration 211, loss = 0.021115323528647423
iteration 212, loss = 0.02685842476785183
iteration 213, loss = 0.03529172018170357
iteration 214, loss = 0.019706346094608307
iteration 215, loss = 0.024228768423199654
iteration 216, loss = 0.027430657297372818
iteration 217, loss = 0.020175324752926826
iteration 218, loss = 0.026389844715595245
iteration 219, loss = 0.020916298031806946
iteration 220, loss = 0.020686643198132515
iteration 221, loss = 0.034643739461898804
iteration 222, loss = 0.019366422668099403
iteration 223, loss = 0.03273823484778404
iteration 224, loss = 0.02326200343668461
iteration 225, loss = 0.023010365664958954
iteration 226, loss = 0.025205010548233986
iteration 227, loss = 0.02206018753349781
iteration 228, loss = 0.019532987847924232
iteration 229, loss = 0.020694183185696602
iteration 230, loss = 0.020769892260432243
iteration 231, loss = 0.020585237070918083
iteration 232, loss = 0.02086973935365677
iteration 233, loss = 0.021628636866807938
iteration 234, loss = 0.022512681782245636
iteration 235, loss = 0.02244296483695507
iteration 236, loss = 0.019284475594758987
iteration 237, loss = 0.02656397968530655
iteration 238, loss = 0.019182609394192696
iteration 239, loss = 0.020755084231495857
iteration 240, loss = 0.018387073650956154
iteration 241, loss = 0.03133130073547363
iteration 242, loss = 0.019892876967787743
iteration 243, loss = 0.025486327707767487
iteration 244, loss = 0.01951601356267929
iteration 245, loss = 0.019977593794465065
iteration 246, loss = 0.019956063479185104
iteration 247, loss = 0.030358681455254555
iteration 248, loss = 0.03099917061626911
iteration 249, loss = 0.021596170961856842
iteration 250, loss = 0.026178931817412376
iteration 251, loss = 0.030091874301433563
iteration 252, loss = 0.02291247807443142
iteration 253, loss = 0.025931619107723236
iteration 254, loss = 0.020062265917658806
iteration 255, loss = 0.02351353131234646
iteration 256, loss = 0.031402163207530975
iteration 257, loss = 0.02150062471628189
iteration 258, loss = 0.02221772074699402
iteration 259, loss = 0.019714675843715668
iteration 260, loss = 0.02217700146138668
iteration 261, loss = 0.020757900550961494
iteration 262, loss = 0.02074285037815571
iteration 263, loss = 0.02172861061990261
iteration 264, loss = 0.02097102254629135
iteration 265, loss = 0.019267462193965912
iteration 266, loss = 0.020010678097605705
iteration 267, loss = 0.022218456491827965
iteration 268, loss = 0.019086629152297974
iteration 269, loss = 0.01781664788722992
iteration 270, loss = 0.02200610376894474
iteration 271, loss = 0.018837930634617805
iteration 272, loss = 0.018924729898571968
iteration 273, loss = 0.02080571837723255
iteration 274, loss = 0.021350421011447906
iteration 275, loss = 0.020742041990160942
iteration 276, loss = 0.018565788865089417
iteration 277, loss = 0.020262204110622406
iteration 278, loss = 0.02002338506281376
iteration 279, loss = 0.021862858906388283
iteration 280, loss = 0.02468692511320114
iteration 281, loss = 0.024198731407523155
iteration 282, loss = 0.022828292101621628
iteration 283, loss = 0.019590085372328758
iteration 284, loss = 0.02176138013601303
iteration 285, loss = 0.041200194507837296
iteration 286, loss = 0.02014184556901455
iteration 287, loss = 0.022377027198672295
iteration 288, loss = 0.01887107640504837
iteration 289, loss = 0.019693708047270775
iteration 290, loss = 0.019025446847081184
iteration 291, loss = 0.04196147248148918
iteration 292, loss = 0.022538429126143456
iteration 293, loss = 0.020485153421759605
iteration 294, loss = 0.023770896717905998
iteration 295, loss = 0.02164352685213089
iteration 296, loss = 0.020219048485159874
iteration 297, loss = 0.02232138253748417
iteration 298, loss = 0.02157803811132908
iteration 299, loss = 0.021904297173023224
iteration 0, loss = 0.028020834550261497
iteration 1, loss = 0.01956062763929367
iteration 2, loss = 0.018113737925887108
iteration 3, loss = 0.019296826794743538
iteration 4, loss = 0.017402194440364838
iteration 5, loss = 0.017597880214452744
iteration 6, loss = 0.018071502447128296
iteration 7, loss = 0.02114606834948063
iteration 8, loss = 0.018385784700512886
iteration 9, loss = 0.025067582726478577
iteration 10, loss = 0.01918184943497181
iteration 11, loss = 0.018811805173754692
iteration 12, loss = 0.018276315182447433
iteration 13, loss = 0.018360542133450508
iteration 14, loss = 0.020519988611340523
iteration 15, loss = 0.018219899386167526
iteration 16, loss = 0.018289998173713684
iteration 17, loss = 0.04176851734519005
iteration 18, loss = 0.018946535885334015
iteration 19, loss = 0.019992664456367493
iteration 20, loss = 0.02363746240735054
iteration 21, loss = 0.019884999841451645
iteration 22, loss = 0.027422063052654266
iteration 23, loss = 0.023111140355467796
iteration 24, loss = 0.01693802699446678
iteration 25, loss = 0.020226847380399704
iteration 26, loss = 0.018642926588654518
iteration 27, loss = 0.0225303303450346
iteration 28, loss = 0.020890312269330025
iteration 29, loss = 0.01860407181084156
iteration 30, loss = 0.019856473430991173
iteration 31, loss = 0.01981712505221367
iteration 32, loss = 0.028100861236453056
iteration 33, loss = 0.01882794126868248
iteration 34, loss = 0.01730974018573761
iteration 35, loss = 0.017717920243740082
iteration 36, loss = 0.01799047365784645
iteration 37, loss = 0.01879795826971531
iteration 38, loss = 0.025883737951517105
iteration 39, loss = 0.01933368481695652
iteration 40, loss = 0.018460771068930626
iteration 41, loss = 0.022941697388887405
iteration 42, loss = 0.019241683185100555
iteration 43, loss = 0.027631832286715508
iteration 44, loss = 0.02413111925125122
iteration 45, loss = 0.01871512085199356
iteration 46, loss = 0.017345912754535675
iteration 47, loss = 0.032681096345186234
iteration 48, loss = 0.01678299903869629
iteration 49, loss = 0.020188840106129646
iteration 50, loss = 0.018819134682416916
iteration 51, loss = 0.01794460043311119
iteration 52, loss = 0.019991161301732063
iteration 53, loss = 0.018241140991449356
iteration 54, loss = 0.016364965587854385
iteration 55, loss = 0.018364086747169495
iteration 56, loss = 0.026660801842808723
iteration 57, loss = 0.016364403069019318
iteration 58, loss = 0.021973920986056328
iteration 59, loss = 0.017829805612564087
iteration 60, loss = 0.016575071960687637
iteration 61, loss = 0.01934180036187172
iteration 62, loss = 0.024854080751538277
iteration 63, loss = 0.020565280690789223
iteration 64, loss = 0.01791466400027275
iteration 65, loss = 0.017775945365428925
iteration 66, loss = 0.017177049070596695
iteration 67, loss = 0.017089232802391052
iteration 68, loss = 0.0190530214458704
iteration 69, loss = 0.019667446613311768
iteration 70, loss = 0.019100701436400414
iteration 71, loss = 0.018970074132084846
iteration 72, loss = 0.01654711365699768
iteration 73, loss = 0.020417943596839905
iteration 74, loss = 0.017421815544366837
iteration 75, loss = 0.01708042062819004
iteration 76, loss = 0.016033627092838287
iteration 77, loss = 0.03889364376664162
iteration 78, loss = 0.019883930683135986
iteration 79, loss = 0.017544709146022797
iteration 80, loss = 0.015886215493083
iteration 81, loss = 0.0176194217056036
iteration 82, loss = 0.018586773425340652
iteration 83, loss = 0.024491211399435997
iteration 84, loss = 0.025962211191654205
iteration 85, loss = 0.03772576525807381
iteration 86, loss = 0.020545590668916702
iteration 87, loss = 0.019144557416439056
iteration 88, loss = 0.02270524762570858
iteration 89, loss = 0.05475936457514763
iteration 90, loss = 0.018630294129252434
iteration 91, loss = 0.01816796138882637
iteration 92, loss = 0.025801481679081917
iteration 93, loss = 0.01641070656478405
iteration 94, loss = 0.015443728305399418
iteration 95, loss = 0.01527257077395916
iteration 96, loss = 0.017264576628804207
iteration 97, loss = 0.016595669090747833
iteration 98, loss = 0.018061714246869087
iteration 99, loss = 0.017624858766794205
iteration 100, loss = 0.01977374777197838
iteration 101, loss = 0.019741596654057503
iteration 102, loss = 0.03506991267204285
iteration 103, loss = 0.01691724732518196
iteration 104, loss = 0.020633719861507416
iteration 105, loss = 0.02137882448732853
iteration 106, loss = 0.01892746239900589
iteration 107, loss = 0.0260700024664402
iteration 108, loss = 0.017834600061178207
iteration 109, loss = 0.020297789946198463
iteration 110, loss = 0.015698267146945
iteration 111, loss = 0.015480156987905502
iteration 112, loss = 0.015760989859700203
iteration 113, loss = 0.01690070703625679
iteration 114, loss = 0.017356639727950096
iteration 115, loss = 0.019486000761389732
iteration 116, loss = 0.038190748542547226
iteration 117, loss = 0.01975024864077568
iteration 118, loss = 0.015681177377700806
iteration 119, loss = 0.016482871025800705
iteration 120, loss = 0.018618406727910042
iteration 121, loss = 0.027147920802235603
iteration 122, loss = 0.01733909733593464
iteration 123, loss = 0.01757151447236538
iteration 124, loss = 0.015628565102815628
iteration 125, loss = 0.017726801335811615
iteration 126, loss = 0.027569258585572243
iteration 127, loss = 0.014963575638830662
iteration 128, loss = 0.03371608629822731
iteration 129, loss = 0.017010115087032318
iteration 130, loss = 0.01679353415966034
iteration 131, loss = 0.016517113894224167
iteration 132, loss = 0.0170755498111248
iteration 133, loss = 0.017015229910612106
iteration 134, loss = 0.02068924903869629
iteration 135, loss = 0.01818700134754181
iteration 136, loss = 0.015501889400184155
iteration 137, loss = 0.016476945951581
iteration 138, loss = 0.018437087535858154
iteration 139, loss = 0.016409708186984062
iteration 140, loss = 0.017283186316490173
iteration 141, loss = 0.020189473405480385
iteration 142, loss = 0.02531895786523819
iteration 143, loss = 0.014818851836025715
iteration 144, loss = 0.0165462214499712
iteration 145, loss = 0.020154116675257683
iteration 146, loss = 0.032871145755052567
iteration 147, loss = 0.01642094925045967
iteration 148, loss = 0.015065211802721024
iteration 149, loss = 0.01586262881755829
iteration 150, loss = 0.01500860694795847
iteration 151, loss = 0.01754526048898697
iteration 152, loss = 0.018688682466745377
iteration 153, loss = 0.0171253252774477
iteration 154, loss = 0.01854965277016163
iteration 155, loss = 0.015881771221756935
iteration 156, loss = 0.018034273758530617
iteration 157, loss = 0.014403469860553741
iteration 158, loss = 0.01681167259812355
iteration 159, loss = 0.014911690726876259
iteration 160, loss = 0.016499478369951248
iteration 161, loss = 0.023489106446504593
iteration 162, loss = 0.017473194748163223
iteration 163, loss = 0.018946127966046333
iteration 164, loss = 0.017284192144870758
iteration 165, loss = 0.015568709932267666
iteration 166, loss = 0.03307843953371048
iteration 167, loss = 0.0238503348082304
iteration 168, loss = 0.019148383289575577
iteration 169, loss = 0.016556240618228912
iteration 170, loss = 0.018619921058416367
iteration 171, loss = 0.01895556040108204
iteration 172, loss = 0.01454043947160244
iteration 173, loss = 0.017544269561767578
iteration 174, loss = 0.020962433889508247
iteration 175, loss = 0.015750469639897346
iteration 176, loss = 0.01595606654882431
iteration 177, loss = 0.0157784316688776
iteration 178, loss = 0.018321974202990532
iteration 179, loss = 0.014702828601002693
iteration 180, loss = 0.01770426519215107
iteration 181, loss = 0.014544383622705936
iteration 182, loss = 0.015560847707092762
iteration 183, loss = 0.014858465641736984
iteration 184, loss = 0.01447737030684948
iteration 185, loss = 0.01429192814975977
iteration 186, loss = 0.01711256429553032
iteration 187, loss = 0.03231452777981758
iteration 188, loss = 0.018560189753770828
iteration 189, loss = 0.03395066037774086
iteration 190, loss = 0.01838594116270542
iteration 191, loss = 0.03186388686299324
iteration 192, loss = 0.016066769137978554
iteration 193, loss = 0.017226524651050568
iteration 194, loss = 0.021472278982400894
iteration 195, loss = 0.0183232631534338
iteration 196, loss = 0.01969921588897705
iteration 197, loss = 0.01544877327978611
iteration 198, loss = 0.01456564012914896
iteration 199, loss = 0.014615737833082676
iteration 200, loss = 0.015959065407514572
iteration 201, loss = 0.014243393205106258
iteration 202, loss = 0.014977650716900826
iteration 203, loss = 0.01592405140399933
iteration 204, loss = 0.015429536812007427
iteration 205, loss = 0.0168747715651989
iteration 206, loss = 0.015188931487500668
iteration 207, loss = 0.02584252692759037
iteration 208, loss = 0.013464598916471004
iteration 209, loss = 0.013884068466722965
iteration 210, loss = 0.015422062948346138
iteration 211, loss = 0.01748901978135109
iteration 212, loss = 0.01705709658563137
iteration 213, loss = 0.015559674240648746
iteration 214, loss = 0.023490125313401222
iteration 215, loss = 0.01428915187716484
iteration 216, loss = 0.01595851220190525
iteration 217, loss = 0.01722433790564537
iteration 218, loss = 0.014518436044454575
iteration 219, loss = 0.013838384300470352
iteration 220, loss = 0.017538970336318016
iteration 221, loss = 0.01754329353570938
iteration 222, loss = 0.01831463724374771
iteration 223, loss = 0.013862622901797295
iteration 224, loss = 0.021547995507717133
iteration 225, loss = 0.015664607286453247
iteration 226, loss = 0.022641858085989952
iteration 227, loss = 0.023718347772955894
iteration 228, loss = 0.015171625651419163
iteration 229, loss = 0.016881119459867477
iteration 230, loss = 0.015057244338095188
iteration 231, loss = 0.013288456946611404
iteration 232, loss = 0.014828961342573166
iteration 233, loss = 0.025034751743078232
iteration 234, loss = 0.015297356992959976
iteration 235, loss = 0.016921663656830788
iteration 236, loss = 0.015897197648882866
iteration 237, loss = 0.014227958396077156
iteration 238, loss = 0.014027886092662811
iteration 239, loss = 0.014478344470262527
iteration 240, loss = 0.014041850343346596
iteration 241, loss = 0.03287527337670326
iteration 242, loss = 0.013600699603557587
iteration 243, loss = 0.02103300578892231
iteration 244, loss = 0.014636356383562088
iteration 245, loss = 0.015482503920793533
iteration 246, loss = 0.013487903401255608
iteration 247, loss = 0.014621161855757236
iteration 248, loss = 0.018037667497992516
iteration 249, loss = 0.015343431383371353
iteration 250, loss = 0.014040752314031124
iteration 251, loss = 0.015162361785769463
iteration 252, loss = 0.01491828914731741
iteration 253, loss = 0.022933367639780045
iteration 254, loss = 0.01540384627878666
iteration 255, loss = 0.02189091220498085
iteration 256, loss = 0.022216573357582092
iteration 257, loss = 0.013747184537351131
iteration 258, loss = 0.015187281183898449
iteration 259, loss = 0.018720462918281555
iteration 260, loss = 0.015285269357264042
iteration 261, loss = 0.017757482826709747
iteration 262, loss = 0.015905655920505524
iteration 263, loss = 0.014779027551412582
iteration 264, loss = 0.020731842145323753
iteration 265, loss = 0.013747353106737137
iteration 266, loss = 0.019419360905885696
iteration 267, loss = 0.015399777330458164
iteration 268, loss = 0.03102939762175083
iteration 269, loss = 0.015088705345988274
iteration 270, loss = 0.015239092521369457
iteration 271, loss = 0.01651757024228573
iteration 272, loss = 0.016674065962433815
iteration 273, loss = 0.012782209552824497
iteration 274, loss = 0.015184379182755947
iteration 275, loss = 0.013312457129359245
iteration 276, loss = 0.014979226514697075
iteration 277, loss = 0.02881924994289875
iteration 278, loss = 0.015838004648685455
iteration 279, loss = 0.014558810740709305
iteration 280, loss = 0.0253792442381382
iteration 281, loss = 0.013154960237443447
iteration 282, loss = 0.01535362284630537
iteration 283, loss = 0.015366693958640099
iteration 284, loss = 0.015070667490363121
iteration 285, loss = 0.018083663657307625
iteration 286, loss = 0.013193606398999691
iteration 287, loss = 0.022019119933247566
iteration 288, loss = 0.013553356751799583
iteration 289, loss = 0.019344978034496307
iteration 290, loss = 0.012635537423193455
iteration 291, loss = 0.012568649835884571
iteration 292, loss = 0.01816471666097641
iteration 293, loss = 0.01423950120806694
iteration 294, loss = 0.020564835518598557
iteration 295, loss = 0.016009002923965454
iteration 296, loss = 0.016055064275860786
iteration 297, loss = 0.015004957094788551
iteration 298, loss = 0.014861425384879112
iteration 299, loss = 0.012649831362068653
iteration 0, loss = 0.014182765036821365
iteration 1, loss = 0.01436000969260931
iteration 2, loss = 0.014325183816254139
iteration 3, loss = 0.015205486677587032
iteration 4, loss = 0.013935165479779243
iteration 5, loss = 0.015453215688467026
iteration 6, loss = 0.020084945484995842
iteration 7, loss = 0.012799855321645737
iteration 8, loss = 0.03154914826154709
iteration 9, loss = 0.013291360810399055
iteration 10, loss = 0.013956555165350437
iteration 11, loss = 0.01856943778693676
iteration 12, loss = 0.02004738338291645
iteration 13, loss = 0.014547808095812798
iteration 14, loss = 0.013727668672800064
iteration 15, loss = 0.0142807736992836
iteration 16, loss = 0.0168791264295578
iteration 17, loss = 0.01412861980497837
iteration 18, loss = 0.014657207764685154
iteration 19, loss = 0.012948749586939812
iteration 20, loss = 0.01332599762827158
iteration 21, loss = 0.01331692561507225
iteration 22, loss = 0.013287357985973358
iteration 23, loss = 0.020976997911930084
iteration 24, loss = 0.014708168804645538
iteration 25, loss = 0.013128549791872501
iteration 26, loss = 0.01642652601003647
iteration 27, loss = 0.014818225987255573
iteration 28, loss = 0.013593309558928013
iteration 29, loss = 0.01689147762954235
iteration 30, loss = 0.030368199571967125
iteration 31, loss = 0.02232397347688675
iteration 32, loss = 0.012035930529236794
iteration 33, loss = 0.013627903535962105
iteration 34, loss = 0.015004320070147514
iteration 35, loss = 0.013286758214235306
iteration 36, loss = 0.021496964618563652
iteration 37, loss = 0.013849344104528427
iteration 38, loss = 0.01279263012111187
iteration 39, loss = 0.012726892717182636
iteration 40, loss = 0.013333886861801147
iteration 41, loss = 0.01475586649030447
iteration 42, loss = 0.013903873041272163
iteration 43, loss = 0.011883269995450974
iteration 44, loss = 0.013913464732468128
iteration 45, loss = 0.013033206574618816
iteration 46, loss = 0.012172708287835121
iteration 47, loss = 0.014607250690460205
iteration 48, loss = 0.013702034950256348
iteration 49, loss = 0.02808999828994274
iteration 50, loss = 0.03740205615758896
iteration 51, loss = 0.015696266666054726
iteration 52, loss = 0.012049773707985878
iteration 53, loss = 0.012681454420089722
iteration 54, loss = 0.012548989616334438
iteration 55, loss = 0.017727695405483246
iteration 56, loss = 0.027836952358484268
iteration 57, loss = 0.013879569247364998
iteration 58, loss = 0.017632298171520233
iteration 59, loss = 0.01187223196029663
iteration 60, loss = 0.013386053033173084
iteration 61, loss = 0.020687013864517212
iteration 62, loss = 0.014060885645449162
iteration 63, loss = 0.02404533512890339
iteration 64, loss = 0.014429355040192604
iteration 65, loss = 0.024035412818193436
iteration 66, loss = 0.012173742055892944
iteration 67, loss = 0.013298938050866127
iteration 68, loss = 0.012038815766572952
iteration 69, loss = 0.013787318021059036
iteration 70, loss = 0.012536397203803062
iteration 71, loss = 0.020970100536942482
iteration 72, loss = 0.028116073459386826
iteration 73, loss = 0.012299785390496254
iteration 74, loss = 0.012224278412759304
iteration 75, loss = 0.01299126073718071
iteration 76, loss = 0.013559723272919655
iteration 77, loss = 0.01500184740871191
iteration 78, loss = 0.011923479847609997
iteration 79, loss = 0.012137820944190025
iteration 80, loss = 0.014111972413957119
iteration 81, loss = 0.01438495609909296
iteration 82, loss = 0.013795810751616955
iteration 83, loss = 0.015264138579368591
iteration 84, loss = 0.012647854164242744
iteration 85, loss = 0.01444120705127716
iteration 86, loss = 0.019323009997606277
iteration 87, loss = 0.01327595952898264
iteration 88, loss = 0.01100006140768528
iteration 89, loss = 0.012810966931283474
iteration 90, loss = 0.014594526030123234
iteration 91, loss = 0.013128425925970078
iteration 92, loss = 0.028559375554323196
iteration 93, loss = 0.012688497081398964
iteration 94, loss = 0.0127366678789258
iteration 95, loss = 0.016884449869394302
iteration 96, loss = 0.011435640044510365
iteration 97, loss = 0.02891506254673004
iteration 98, loss = 0.012059308588504791
iteration 99, loss = 0.015545126050710678
iteration 100, loss = 0.02686186693608761
iteration 101, loss = 0.01267123594880104
iteration 102, loss = 0.011660369113087654
iteration 103, loss = 0.013155795633792877
iteration 104, loss = 0.011415147222578526
iteration 105, loss = 0.012618400156497955
iteration 106, loss = 0.012909946963191032
iteration 107, loss = 0.012439161539077759
iteration 108, loss = 0.017646532505750656
iteration 109, loss = 0.013661670498549938
iteration 110, loss = 0.014046495780348778
iteration 111, loss = 0.013650589622557163
iteration 112, loss = 0.01623588055372238
iteration 113, loss = 0.027146073058247566
iteration 114, loss = 0.011838497593998909
iteration 115, loss = 0.011437665671110153
iteration 116, loss = 0.01278966385871172
iteration 117, loss = 0.01583999954164028
iteration 118, loss = 0.011494144797325134
iteration 119, loss = 0.01251240074634552
iteration 120, loss = 0.012213234789669514
iteration 121, loss = 0.012105101719498634
iteration 122, loss = 0.013751259073615074
iteration 123, loss = 0.011964723467826843
iteration 124, loss = 0.012666724622249603
iteration 125, loss = 0.012544116005301476
iteration 126, loss = 0.011898928321897984
iteration 127, loss = 0.020389948040246964
iteration 128, loss = 0.012634131126105785
iteration 129, loss = 0.01173803023993969
iteration 130, loss = 0.014142228290438652
iteration 131, loss = 0.012917394749820232
iteration 132, loss = 0.011978263966739178
iteration 133, loss = 0.011745677329599857
iteration 134, loss = 0.012552333064377308
iteration 135, loss = 0.013708403334021568
iteration 136, loss = 0.013959011062979698
iteration 137, loss = 0.011696560308337212
iteration 138, loss = 0.013746656477451324
iteration 139, loss = 0.01220282819122076
iteration 140, loss = 0.018444353714585304
iteration 141, loss = 0.012319381348788738
iteration 142, loss = 0.013418220914900303
iteration 143, loss = 0.013769702985882759
iteration 144, loss = 0.012086555361747742
iteration 145, loss = 0.0115894116461277
iteration 146, loss = 0.025662150233983994
iteration 147, loss = 0.014070897363126278
iteration 148, loss = 0.017297642305493355
iteration 149, loss = 0.011129265651106834
iteration 150, loss = 0.014892403036355972
iteration 151, loss = 0.012342048808932304
iteration 152, loss = 0.010819236747920513
iteration 153, loss = 0.012877698056399822
iteration 154, loss = 0.010985685512423515
iteration 155, loss = 0.01219509169459343
iteration 156, loss = 0.012506233528256416
iteration 157, loss = 0.012194070033729076
iteration 158, loss = 0.01252210233360529
iteration 159, loss = 0.014995662495493889
iteration 160, loss = 0.013946909457445145
iteration 161, loss = 0.025010230019688606
iteration 162, loss = 0.011248008348047733
iteration 163, loss = 0.012279504910111427
iteration 164, loss = 0.011274615302681923
iteration 165, loss = 0.023171260952949524
iteration 166, loss = 0.012911075726151466
iteration 167, loss = 0.010977379977703094
iteration 168, loss = 0.012303197756409645
iteration 169, loss = 0.012686804868280888
iteration 170, loss = 0.012315835803747177
iteration 171, loss = 0.0190765168517828
iteration 172, loss = 0.012816118076443672
iteration 173, loss = 0.011152158491313457
iteration 174, loss = 0.016845207661390305
iteration 175, loss = 0.014559943228960037
iteration 176, loss = 0.013180532492697239
iteration 177, loss = 0.01078400295227766
iteration 178, loss = 0.011749964207410812
iteration 179, loss = 0.012862750329077244
iteration 180, loss = 0.013405180536210537
iteration 181, loss = 0.0123285548761487
iteration 182, loss = 0.010918883606791496
iteration 183, loss = 0.011976906098425388
iteration 184, loss = 0.01261527556926012
iteration 185, loss = 0.012187615968286991
iteration 186, loss = 0.017513776198029518
iteration 187, loss = 0.011979024857282639
iteration 188, loss = 0.011453136801719666
iteration 189, loss = 0.011461012065410614
iteration 190, loss = 0.010743247345089912
iteration 191, loss = 0.015071085654199123
iteration 192, loss = 0.010631377808749676
iteration 193, loss = 0.012061689980328083
iteration 194, loss = 0.012855058535933495
iteration 195, loss = 0.010508416220545769
iteration 196, loss = 0.012263165786862373
iteration 197, loss = 0.011631966568529606
iteration 198, loss = 0.011395594105124474
iteration 199, loss = 0.010520401410758495
iteration 200, loss = 0.011865668930113316
iteration 201, loss = 0.011469983495771885
iteration 202, loss = 0.015076146461069584
iteration 203, loss = 0.012263255193829536
iteration 204, loss = 0.015223117545247078
iteration 205, loss = 0.013012788258492947
iteration 206, loss = 0.01060186792165041
iteration 207, loss = 0.0187050960958004
iteration 208, loss = 0.011152935214340687
iteration 209, loss = 0.015173587948083878
iteration 210, loss = 0.010731932707130909
iteration 211, loss = 0.011586827225983143
iteration 212, loss = 0.01318224798887968
iteration 213, loss = 0.01264120638370514
iteration 214, loss = 0.011755049228668213
iteration 215, loss = 0.010714671574532986
iteration 216, loss = 0.011216786690056324
iteration 217, loss = 0.010614585131406784
iteration 218, loss = 0.01100252941250801
iteration 219, loss = 0.011083533987402916
iteration 220, loss = 0.01207703910768032
iteration 221, loss = 0.011513959616422653
iteration 222, loss = 0.012730917893350124
iteration 223, loss = 0.01028391718864441
iteration 224, loss = 0.010110127739608288
iteration 225, loss = 0.011591963469982147
iteration 226, loss = 0.010682997293770313
iteration 227, loss = 0.019330063834786415
iteration 228, loss = 0.011202641762793064
iteration 229, loss = 0.010166143998503685
iteration 230, loss = 0.012375649996101856
iteration 231, loss = 0.012553117237985134
iteration 232, loss = 0.011047892272472382
iteration 233, loss = 0.01202019490301609
iteration 234, loss = 0.011422675102949142
iteration 235, loss = 0.01697506383061409
iteration 236, loss = 0.010113737545907497
iteration 237, loss = 0.010667758993804455
iteration 238, loss = 0.024437401443719864
iteration 239, loss = 0.011061620898544788
iteration 240, loss = 0.010203923098742962
iteration 241, loss = 0.012004708871245384
iteration 242, loss = 0.012571338564157486
iteration 243, loss = 0.01247918140143156
iteration 244, loss = 0.017228372395038605
iteration 245, loss = 0.011864550411701202
iteration 246, loss = 0.009983401745557785
iteration 247, loss = 0.011890849098563194
iteration 248, loss = 0.010278398171067238
iteration 249, loss = 0.010603061877191067
iteration 250, loss = 0.011445483192801476
iteration 251, loss = 0.011935950256884098
iteration 252, loss = 0.013146660290658474
iteration 253, loss = 0.00990073662251234
iteration 254, loss = 0.010015149600803852
iteration 255, loss = 0.010853162966668606
iteration 256, loss = 0.01765616051852703
iteration 257, loss = 0.01233622059226036
iteration 258, loss = 0.011680823750793934
iteration 259, loss = 0.010255671106278896
iteration 260, loss = 0.012203153222799301
iteration 261, loss = 0.011678488925099373
iteration 262, loss = 0.010762089863419533
iteration 263, loss = 0.011883941479027271
iteration 264, loss = 0.010338202118873596
iteration 265, loss = 0.011271030642092228
iteration 266, loss = 0.02462339960038662
iteration 267, loss = 0.013668379746377468
iteration 268, loss = 0.01067540142685175
iteration 269, loss = 0.023653484880924225
iteration 270, loss = 0.010605677962303162
iteration 271, loss = 0.01658714935183525
iteration 272, loss = 0.010728619992733002
iteration 273, loss = 0.012639529071748257
iteration 274, loss = 0.009777656756341457
iteration 275, loss = 0.01052312646061182
iteration 276, loss = 0.012441050261259079
iteration 277, loss = 0.011844044551253319
iteration 278, loss = 0.010537229478359222
iteration 279, loss = 0.01159369945526123
iteration 280, loss = 0.009918520227074623
iteration 281, loss = 0.011120266281068325
iteration 282, loss = 0.010804719291627407
iteration 283, loss = 0.009460420347750187
iteration 284, loss = 0.010508386418223381
iteration 285, loss = 0.010203083045780659
iteration 286, loss = 0.010539551265537739
iteration 287, loss = 0.009531158022582531
iteration 288, loss = 0.01669307053089142
iteration 289, loss = 0.010816199705004692
iteration 290, loss = 0.010138308629393578
iteration 291, loss = 0.01263023167848587
iteration 292, loss = 0.010492819361388683
iteration 293, loss = 0.011730330064892769
iteration 294, loss = 0.012272458523511887
iteration 295, loss = 0.01302625797688961
iteration 296, loss = 0.010361658409237862
iteration 297, loss = 0.01090673916041851
iteration 298, loss = 0.016917256638407707
iteration 299, loss = 0.010348454117774963
iteration 0, loss = 0.009534732438623905
iteration 1, loss = 0.01259111613035202
iteration 2, loss = 0.009670178405940533
iteration 3, loss = 0.011324879713356495
iteration 4, loss = 0.009494323283433914
iteration 5, loss = 0.010219637304544449
iteration 6, loss = 0.010843522846698761
iteration 7, loss = 0.011273788288235664
iteration 8, loss = 0.010406501591205597
iteration 9, loss = 0.009745251387357712
iteration 10, loss = 0.009788073599338531
iteration 11, loss = 0.01714978739619255
iteration 12, loss = 0.010874941945075989
iteration 13, loss = 0.01129811443388462
iteration 14, loss = 0.024823157116770744
iteration 15, loss = 0.010202258825302124
iteration 16, loss = 0.009890967048704624
iteration 17, loss = 0.012387795373797417
iteration 18, loss = 0.009886445477604866
iteration 19, loss = 0.011649983003735542
iteration 20, loss = 0.011503789573907852
iteration 21, loss = 0.009760282933712006
iteration 22, loss = 0.01095345988869667
iteration 23, loss = 0.015184267424046993
iteration 24, loss = 0.009221122600138187
iteration 25, loss = 0.011695914901793003
iteration 26, loss = 0.010061253793537617
iteration 27, loss = 0.01137609127908945
iteration 28, loss = 0.009585936553776264
iteration 29, loss = 0.010127553716301918
iteration 30, loss = 0.010029108263552189
iteration 31, loss = 0.009911108762025833
iteration 32, loss = 0.018459469079971313
iteration 33, loss = 0.014476621523499489
iteration 34, loss = 0.009767194278538227
iteration 35, loss = 0.017585845664143562
iteration 36, loss = 0.011119886301457882
iteration 37, loss = 0.009354147128760815
iteration 38, loss = 0.00959610752761364
iteration 39, loss = 0.010094166733324528
iteration 40, loss = 0.011288270354270935
iteration 41, loss = 0.016437144950032234
iteration 42, loss = 0.009879286400973797
iteration 43, loss = 0.02376464195549488
iteration 44, loss = 0.00970129482448101
iteration 45, loss = 0.010812880471348763
iteration 46, loss = 0.010951098054647446
iteration 47, loss = 0.010121610946953297
iteration 48, loss = 0.015032880008220673
iteration 49, loss = 0.0101275984197855
iteration 50, loss = 0.016143105924129486
iteration 51, loss = 0.012064394541084766
iteration 52, loss = 0.01060210820287466
iteration 53, loss = 0.01083393581211567
iteration 54, loss = 0.010067563503980637
iteration 55, loss = 0.009019618853926659
iteration 56, loss = 0.009286184795200825
iteration 57, loss = 0.010195326060056686
iteration 58, loss = 0.01027598325163126
iteration 59, loss = 0.009618150070309639
iteration 60, loss = 0.009792921133339405
iteration 61, loss = 0.009502314031124115
iteration 62, loss = 0.011560539714992046
iteration 63, loss = 0.010143958032131195
iteration 64, loss = 0.009739414788782597
iteration 65, loss = 0.009163246490061283
iteration 66, loss = 0.008891569450497627
iteration 67, loss = 0.010733389295637608
iteration 68, loss = 0.009550253860652447
iteration 69, loss = 0.010161378420889378
iteration 70, loss = 0.010021647438406944
iteration 71, loss = 0.009448070079088211
iteration 72, loss = 0.009633896872401237
iteration 73, loss = 0.011170131154358387
iteration 74, loss = 0.010320347733795643
iteration 75, loss = 0.010608824901282787
iteration 76, loss = 0.011533543467521667
iteration 77, loss = 0.010916178114712238
iteration 78, loss = 0.009265110827982426
iteration 79, loss = 0.0094167934730649
iteration 80, loss = 0.009317122399806976
iteration 81, loss = 0.010551024228334427
iteration 82, loss = 0.009685559198260307
iteration 83, loss = 0.015316776931285858
iteration 84, loss = 0.009344509802758694
iteration 85, loss = 0.010629134252667427
iteration 86, loss = 0.022706041112542152
iteration 87, loss = 0.010700357146561146
iteration 88, loss = 0.008961939252912998
iteration 89, loss = 0.01225203089416027
iteration 90, loss = 0.010211924090981483
iteration 91, loss = 0.016564633697271347
iteration 92, loss = 0.00942309945821762
iteration 93, loss = 0.013064508326351643
iteration 94, loss = 0.009962922893464565
iteration 95, loss = 0.00922947097569704
iteration 96, loss = 0.01006405521184206
iteration 97, loss = 0.011030580848455429
iteration 98, loss = 0.011352194473147392
iteration 99, loss = 0.00974698830395937
iteration 100, loss = 0.009422273375093937
iteration 101, loss = 0.014724565669894218
iteration 102, loss = 0.010840777307748795
iteration 103, loss = 0.010869781486690044
iteration 104, loss = 0.013749860227108002
iteration 105, loss = 0.009191812016069889
iteration 106, loss = 0.009058011695742607
iteration 107, loss = 0.009541005827486515
iteration 108, loss = 0.011535857804119587
iteration 109, loss = 0.00953279621899128
iteration 110, loss = 0.00909312255680561
iteration 111, loss = 0.013222002424299717
iteration 112, loss = 0.01484411396086216
iteration 113, loss = 0.00905093178153038
iteration 114, loss = 0.009266847744584084
iteration 115, loss = 0.009751228615641594
iteration 116, loss = 0.009768467396497726
iteration 117, loss = 0.00991190504282713
iteration 118, loss = 0.02408231981098652
iteration 119, loss = 0.010429426096379757
iteration 120, loss = 0.011096161790192127
iteration 121, loss = 0.008461298421025276
iteration 122, loss = 0.008737177588045597
iteration 123, loss = 0.009685865603387356
iteration 124, loss = 0.01677178032696247
iteration 125, loss = 0.010239616967737675
iteration 126, loss = 0.009247204288840294
iteration 127, loss = 0.013800770044326782
iteration 128, loss = 0.010286309756338596
iteration 129, loss = 0.010555120185017586
iteration 130, loss = 0.010067827068269253
iteration 131, loss = 0.009062198922038078
iteration 132, loss = 0.00881702359765768
iteration 133, loss = 0.013128459453582764
iteration 134, loss = 0.010273350402712822
iteration 135, loss = 0.010338017717003822
iteration 136, loss = 0.011857641860842705
iteration 137, loss = 0.009946487843990326
iteration 138, loss = 0.008424418047070503
iteration 139, loss = 0.008886477909982204
iteration 140, loss = 0.009657121263444424
iteration 141, loss = 0.009664485231041908
iteration 142, loss = 0.008730385452508926
iteration 143, loss = 0.010083167813718319
iteration 144, loss = 0.009185497649013996
iteration 145, loss = 0.009487523697316647
iteration 146, loss = 0.00890939962118864
iteration 147, loss = 0.010484233498573303
iteration 148, loss = 0.008350341580808163
iteration 149, loss = 0.009204238653182983
iteration 150, loss = 0.010478679090738297
iteration 151, loss = 0.009778422303497791
iteration 152, loss = 0.010623936541378498
iteration 153, loss = 0.009831482544541359
iteration 154, loss = 0.009076442569494247
iteration 155, loss = 0.008775663562119007
iteration 156, loss = 0.01142956968396902
iteration 157, loss = 0.012639765627682209
iteration 158, loss = 0.00866156630218029
iteration 159, loss = 0.007976601831614971
iteration 160, loss = 0.010568443685770035
iteration 161, loss = 0.010560473427176476
iteration 162, loss = 0.008638568222522736
iteration 163, loss = 0.00866544060409069
iteration 164, loss = 0.009562825784087181
iteration 165, loss = 0.010855136439204216
iteration 166, loss = 0.011831485666334629
iteration 167, loss = 0.009699028916656971
iteration 168, loss = 0.008098412305116653
iteration 169, loss = 0.009625829756259918
iteration 170, loss = 0.008829870261251926
iteration 171, loss = 0.01112100388854742
iteration 172, loss = 0.01027214340865612
iteration 173, loss = 0.008966813795268536
iteration 174, loss = 0.012004243209958076
iteration 175, loss = 0.012057945132255554
iteration 176, loss = 0.01460216660052538
iteration 177, loss = 0.00871637649834156
iteration 178, loss = 0.008808363229036331
iteration 179, loss = 0.01052193995565176
iteration 180, loss = 0.010173313319683075
iteration 181, loss = 0.009541752748191357
iteration 182, loss = 0.009115813300013542
iteration 183, loss = 0.008345196023583412
iteration 184, loss = 0.013314600102603436
iteration 185, loss = 0.01087623555213213
iteration 186, loss = 0.009632496163249016
iteration 187, loss = 0.01089559681713581
iteration 188, loss = 0.009622786194086075
iteration 189, loss = 0.009843441657721996
iteration 190, loss = 0.0082105603069067
iteration 191, loss = 0.00952460989356041
iteration 192, loss = 0.009694430977106094
iteration 193, loss = 0.009663775563240051
iteration 194, loss = 0.00937171932309866
iteration 195, loss = 0.009108975529670715
iteration 196, loss = 0.008316254243254662
iteration 197, loss = 0.008591401390731335
iteration 198, loss = 0.008687354624271393
iteration 199, loss = 0.00868746917694807
iteration 200, loss = 0.009245365858078003
iteration 201, loss = 0.009075581096112728
iteration 202, loss = 0.01279506366699934
iteration 203, loss = 0.008221047930419445
iteration 204, loss = 0.00905687641352415
iteration 205, loss = 0.013077991083264351
iteration 206, loss = 0.008512834087014198
iteration 207, loss = 0.009343162178993225
iteration 208, loss = 0.008416839875280857
iteration 209, loss = 0.009342526085674763
iteration 210, loss = 0.009233853779733181
iteration 211, loss = 0.008949244394898415
iteration 212, loss = 0.008016274310648441
iteration 213, loss = 0.021118931472301483
iteration 214, loss = 0.008855377323925495
iteration 215, loss = 0.008339153602719307
iteration 216, loss = 0.008321162313222885
iteration 217, loss = 0.009747815318405628
iteration 218, loss = 0.010396011173725128
iteration 219, loss = 0.007842294871807098
iteration 220, loss = 0.02065751701593399
iteration 221, loss = 0.008387682028114796
iteration 222, loss = 0.010275010019540787
iteration 223, loss = 0.008704333566129208
iteration 224, loss = 0.008811136707663536
iteration 225, loss = 0.021150236949324608
iteration 226, loss = 0.00847446359694004
iteration 227, loss = 0.010472903959453106
iteration 228, loss = 0.008019508793950081
iteration 229, loss = 0.009676091372966766
iteration 230, loss = 0.009026288986206055
iteration 231, loss = 0.009078857488930225
iteration 232, loss = 0.00902987364679575
iteration 233, loss = 0.02064618468284607
iteration 234, loss = 0.008520564064383507
iteration 235, loss = 0.008431456983089447
iteration 236, loss = 0.01450616680085659
iteration 237, loss = 0.020098308101296425
iteration 238, loss = 0.00918091181665659
iteration 239, loss = 0.008118377067148685
iteration 240, loss = 0.009599578566849232
iteration 241, loss = 0.021121852099895477
iteration 242, loss = 0.02034139633178711
iteration 243, loss = 0.008534930646419525
iteration 244, loss = 0.011730383150279522
iteration 245, loss = 0.009210090152919292
iteration 246, loss = 0.008648793213069439
iteration 247, loss = 0.012250243686139584
iteration 248, loss = 0.008594108745455742
iteration 249, loss = 0.009744375012814999
iteration 250, loss = 0.008184417150914669
iteration 251, loss = 0.01844637840986252
iteration 252, loss = 0.008701319806277752
iteration 253, loss = 0.009858572855591774
iteration 254, loss = 0.008573822677135468
iteration 255, loss = 0.00771818682551384
iteration 256, loss = 0.008072764612734318
iteration 257, loss = 0.008808517828583717
iteration 258, loss = 0.0087394118309021
iteration 259, loss = 0.007971814833581448
iteration 260, loss = 0.008552066050469875
iteration 261, loss = 0.00959613174200058
iteration 262, loss = 0.007753036450594664
iteration 263, loss = 0.0085352947935462
iteration 264, loss = 0.00805053859949112
iteration 265, loss = 0.012562323361635208
iteration 266, loss = 0.00989027600735426
iteration 267, loss = 0.008438759483397007
iteration 268, loss = 0.007763819303363562
iteration 269, loss = 0.008367522619664669
iteration 270, loss = 0.010818994604051113
iteration 271, loss = 0.008412385359406471
iteration 272, loss = 0.00856899842619896
iteration 273, loss = 0.01986459456384182
iteration 274, loss = 0.008115315809845924
iteration 275, loss = 0.007828406989574432
iteration 276, loss = 0.007674147840589285
iteration 277, loss = 0.009864077903330326
iteration 278, loss = 0.008521483279764652
iteration 279, loss = 0.01365562155842781
iteration 280, loss = 0.009552909061312675
iteration 281, loss = 0.012953322380781174
iteration 282, loss = 0.008789016865193844
iteration 283, loss = 0.008184336125850677
iteration 284, loss = 0.008829296566545963
iteration 285, loss = 0.013960637152194977
iteration 286, loss = 0.008896617218852043
iteration 287, loss = 0.020042581483721733
iteration 288, loss = 0.00787606742233038
iteration 289, loss = 0.009134483523666859
iteration 290, loss = 0.0104684391990304
iteration 291, loss = 0.008825571276247501
iteration 292, loss = 0.008243408054113388
iteration 293, loss = 0.013749540783464909
iteration 294, loss = 0.022043395787477493
iteration 295, loss = 0.019445331767201424
iteration 296, loss = 0.009609004482626915
iteration 297, loss = 0.010780822485685349
iteration 298, loss = 0.008486931212246418
iteration 299, loss = 0.008543412201106548
iteration 0, loss = 0.008096461184322834
iteration 1, loss = 0.007804180029779673
iteration 2, loss = 0.008990741334855556
iteration 3, loss = 0.007802802603691816
iteration 4, loss = 0.008055048063397408
iteration 5, loss = 0.020798929035663605
iteration 6, loss = 0.009174969047307968
iteration 7, loss = 0.009308861568570137
iteration 8, loss = 0.008578839711844921
iteration 9, loss = 0.01246865838766098
iteration 10, loss = 0.013364391401410103
iteration 11, loss = 0.007482261396944523
iteration 12, loss = 0.008967453613877296
iteration 13, loss = 0.007847161032259464
iteration 14, loss = 0.008137620985507965
iteration 15, loss = 0.007192378863692284
iteration 16, loss = 0.008384018205106258
iteration 17, loss = 0.008477073162794113
iteration 18, loss = 0.008059828542172909
iteration 19, loss = 0.008630306459963322
iteration 20, loss = 0.008090006187558174
iteration 21, loss = 0.008067489601671696
iteration 22, loss = 0.007674865424633026
iteration 23, loss = 0.009825782850384712
iteration 24, loss = 0.007639585994184017
iteration 25, loss = 0.009049467742443085
iteration 26, loss = 0.01807042583823204
iteration 27, loss = 0.00782442931085825
iteration 28, loss = 0.01270300056785345
iteration 29, loss = 0.008849943056702614
iteration 30, loss = 0.0089363232254982
iteration 31, loss = 0.011167502030730247
iteration 32, loss = 0.007907191291451454
iteration 33, loss = 0.00814201682806015
iteration 34, loss = 0.008401918224990368
iteration 35, loss = 0.020277239382267
iteration 36, loss = 0.018275313079357147
iteration 37, loss = 0.008400969207286835
iteration 38, loss = 0.007869277149438858
iteration 39, loss = 0.008134965784847736
iteration 40, loss = 0.008906182833015919
iteration 41, loss = 0.009467678144574165
iteration 42, loss = 0.008745537139475346
iteration 43, loss = 0.011784873902797699
iteration 44, loss = 0.007516748737543821
iteration 45, loss = 0.007595642935484648
iteration 46, loss = 0.007823665626347065
iteration 47, loss = 0.007782493252307177
iteration 48, loss = 0.007713118102401495
iteration 49, loss = 0.017946135252714157
iteration 50, loss = 0.007621040567755699
iteration 51, loss = 0.007452951278537512
iteration 52, loss = 0.010757328942418098
iteration 53, loss = 0.00809001736342907
iteration 54, loss = 0.00849029142409563
iteration 55, loss = 0.00742519972845912
iteration 56, loss = 0.008764415048062801
iteration 57, loss = 0.008385482244193554
iteration 58, loss = 0.007627962622791529
iteration 59, loss = 0.008375972509384155
iteration 60, loss = 0.009648210369050503
iteration 61, loss = 0.007752286735922098
iteration 62, loss = 0.008928196504712105
iteration 63, loss = 0.007150944788008928
iteration 64, loss = 0.008415156975388527
iteration 65, loss = 0.009396214969456196
iteration 66, loss = 0.014158225618302822
iteration 67, loss = 0.010445743799209595
iteration 68, loss = 0.008907990530133247
iteration 69, loss = 0.007719401735812426
iteration 70, loss = 0.008398051373660564
iteration 71, loss = 0.009196224622428417
iteration 72, loss = 0.007382100448012352
iteration 73, loss = 0.01087259128689766
iteration 74, loss = 0.008464016951620579
iteration 75, loss = 0.008554649539291859
iteration 76, loss = 0.006993745919317007
iteration 77, loss = 0.008802617900073528
iteration 78, loss = 0.01225653663277626
iteration 79, loss = 0.00788634829223156
iteration 80, loss = 0.008043019101023674
iteration 81, loss = 0.0073709795251488686
iteration 82, loss = 0.007703379727900028
iteration 83, loss = 0.006756951101124287
iteration 84, loss = 0.008836033754050732
iteration 85, loss = 0.008178813382983208
iteration 86, loss = 0.008542731404304504
iteration 87, loss = 0.007295818068087101
iteration 88, loss = 0.007574144285172224
iteration 89, loss = 0.017682993784546852
iteration 90, loss = 0.007557277102023363
iteration 91, loss = 0.016177654266357422
iteration 92, loss = 0.007507017347961664
iteration 93, loss = 0.0067999716848134995
iteration 94, loss = 0.007567303720861673
iteration 95, loss = 0.007854659110307693
iteration 96, loss = 0.007667943369597197
iteration 97, loss = 0.007541281171143055
iteration 98, loss = 0.009225757792592049
iteration 99, loss = 0.013084766454994678
iteration 100, loss = 0.007096312008798122
iteration 101, loss = 0.00790887139737606
iteration 102, loss = 0.007260771002620459
iteration 103, loss = 0.00756538612768054
iteration 104, loss = 0.008099651895463467
iteration 105, loss = 0.009341872297227383
iteration 106, loss = 0.008558347821235657
iteration 107, loss = 0.008238591253757477
iteration 108, loss = 0.007491589058190584
iteration 109, loss = 0.00742608355358243
iteration 110, loss = 0.008262161165475845
iteration 111, loss = 0.007859255187213421
iteration 112, loss = 0.010334858670830727
iteration 113, loss = 0.008253426291048527
iteration 114, loss = 0.007034276612102985
iteration 115, loss = 0.0078070140443742275
iteration 116, loss = 0.007386647164821625
iteration 117, loss = 0.008320257067680359
iteration 118, loss = 0.017525697126984596
iteration 119, loss = 0.007866712287068367
iteration 120, loss = 0.0069157425314188
iteration 121, loss = 0.007599855773150921
iteration 122, loss = 0.007376058027148247
iteration 123, loss = 0.007756403181701899
iteration 124, loss = 0.008074989542365074
iteration 125, loss = 0.007929319515824318
iteration 126, loss = 0.011645282618701458
iteration 127, loss = 0.007629226893186569
iteration 128, loss = 0.0072723086923360825
iteration 129, loss = 0.009501826949417591
iteration 130, loss = 0.008709201589226723
iteration 131, loss = 0.008869174867868423
iteration 132, loss = 0.008070549927651882
iteration 133, loss = 0.007797160651534796
iteration 134, loss = 0.006837225519120693
iteration 135, loss = 0.007794354110956192
iteration 136, loss = 0.00822706799954176
iteration 137, loss = 0.0077201686799526215
iteration 138, loss = 0.007819830439984798
iteration 139, loss = 0.007439800538122654
iteration 140, loss = 0.008115970529615879
iteration 141, loss = 0.017386944964528084
iteration 142, loss = 0.007921532727777958
iteration 143, loss = 0.006635500583797693
iteration 144, loss = 0.007750520948320627
iteration 145, loss = 0.008615147322416306
iteration 146, loss = 0.008183411322534084
iteration 147, loss = 0.007404971867799759
iteration 148, loss = 0.007962079718708992
iteration 149, loss = 0.006974341347813606
iteration 150, loss = 0.006949699949473143
iteration 151, loss = 0.02067362517118454
iteration 152, loss = 0.00811377726495266
iteration 153, loss = 0.006978207733482122
iteration 154, loss = 0.007041263394057751
iteration 155, loss = 0.01731085032224655
iteration 156, loss = 0.011078773997724056
iteration 157, loss = 0.011566375382244587
iteration 158, loss = 0.007283876184374094
iteration 159, loss = 0.006940733175724745
iteration 160, loss = 0.008082970976829529
iteration 161, loss = 0.011414814740419388
iteration 162, loss = 0.007646438665688038
iteration 163, loss = 0.007247092202305794
iteration 164, loss = 0.008584575727581978
iteration 165, loss = 0.01163499429821968
iteration 166, loss = 0.011682079173624516
iteration 167, loss = 0.007496492005884647
iteration 168, loss = 0.007460287306457758
iteration 169, loss = 0.008361783809959888
iteration 170, loss = 0.0076188878156244755
iteration 171, loss = 0.00652112253010273
iteration 172, loss = 0.0077357059344649315
iteration 173, loss = 0.007155292667448521
iteration 174, loss = 0.010202709585428238
iteration 175, loss = 0.010809003375470638
iteration 176, loss = 0.008147741667926311
iteration 177, loss = 0.010560331866145134
iteration 178, loss = 0.0070696305483579636
iteration 179, loss = 0.006661298684775829
iteration 180, loss = 0.006604818161576986
iteration 181, loss = 0.017398755997419357
iteration 182, loss = 0.0080240648239851
iteration 183, loss = 0.007106445264071226
iteration 184, loss = 0.007000140380114317
iteration 185, loss = 0.007065766956657171
iteration 186, loss = 0.006680463906377554
iteration 187, loss = 0.006951741874217987
iteration 188, loss = 0.008647393435239792
iteration 189, loss = 0.00706834439188242
iteration 190, loss = 0.007571180816739798
iteration 191, loss = 0.016910718753933907
iteration 192, loss = 0.0076425690203905106
iteration 193, loss = 0.007346292957663536
iteration 194, loss = 0.00676580099388957
iteration 195, loss = 0.007352171931415796
iteration 196, loss = 0.00890759751200676
iteration 197, loss = 0.009714584797620773
iteration 198, loss = 0.017825882881879807
iteration 199, loss = 0.007243979722261429
iteration 200, loss = 0.006555414758622646
iteration 201, loss = 0.006382557563483715
iteration 202, loss = 0.006507160607725382
iteration 203, loss = 0.007015646435320377
iteration 204, loss = 0.007335766218602657
iteration 205, loss = 0.008727115578949451
iteration 206, loss = 0.007396898698061705
iteration 207, loss = 0.008455799892544746
iteration 208, loss = 0.007994299754500389
iteration 209, loss = 0.006506249774247408
iteration 210, loss = 0.0076918825507164
iteration 211, loss = 0.007231762167066336
iteration 212, loss = 0.006748784799128771
iteration 213, loss = 0.008309922181069851
iteration 214, loss = 0.00680510001257062
iteration 215, loss = 0.016309408470988274
iteration 216, loss = 0.011733011342585087
iteration 217, loss = 0.006855701096355915
iteration 218, loss = 0.007075035013258457
iteration 219, loss = 0.007716877851635218
iteration 220, loss = 0.006888984702527523
iteration 221, loss = 0.006644998677074909
iteration 222, loss = 0.007940861396491528
iteration 223, loss = 0.0069027552381157875
iteration 224, loss = 0.006697658449411392
iteration 225, loss = 0.007656087167561054
iteration 226, loss = 0.006952404510229826
iteration 227, loss = 0.010416669771075249
iteration 228, loss = 0.011056935414671898
iteration 229, loss = 0.008249111473560333
iteration 230, loss = 0.006830680649727583
iteration 231, loss = 0.010959867388010025
iteration 232, loss = 0.010999838821589947
iteration 233, loss = 0.0070809186436235905
iteration 234, loss = 0.008872042410075665
iteration 235, loss = 0.0070676496252417564
iteration 236, loss = 0.007479560095816851
iteration 237, loss = 0.007108716759830713
iteration 238, loss = 0.008531259372830391
iteration 239, loss = 0.006611343938857317
iteration 240, loss = 0.006954043172299862
iteration 241, loss = 0.008514163084328175
iteration 242, loss = 0.006719690747559071
iteration 243, loss = 0.006866251118481159
iteration 244, loss = 0.006592425052076578
iteration 245, loss = 0.007025560829788446
iteration 246, loss = 0.008564790710806847
iteration 247, loss = 0.0068750218488276005
iteration 248, loss = 0.007878793403506279
iteration 249, loss = 0.007196439895778894
iteration 250, loss = 0.007299167104065418
iteration 251, loss = 0.0074034747667610645
iteration 252, loss = 0.006642651278525591
iteration 253, loss = 0.009400504641234875
iteration 254, loss = 0.00740412762388587
iteration 255, loss = 0.006533102132380009
iteration 256, loss = 0.006962365936487913
iteration 257, loss = 0.007026375271379948
iteration 258, loss = 0.006680035497993231
iteration 259, loss = 0.009502044878900051
iteration 260, loss = 0.009397896006703377
iteration 261, loss = 0.00824691355228424
iteration 262, loss = 0.007119500078260899
iteration 263, loss = 0.008837126195430756
iteration 264, loss = 0.006904256995767355
iteration 265, loss = 0.006537150591611862
iteration 266, loss = 0.006571134086698294
iteration 267, loss = 0.006684491876512766
iteration 268, loss = 0.006234663538634777
iteration 269, loss = 0.006487594451755285
iteration 270, loss = 0.010652076452970505
iteration 271, loss = 0.00719611719250679
iteration 272, loss = 0.008257775567471981
iteration 273, loss = 0.009662114083766937
iteration 274, loss = 0.006958561949431896
iteration 275, loss = 0.007276071235537529
iteration 276, loss = 0.006929720286279917
iteration 277, loss = 0.007970261387526989
iteration 278, loss = 0.0066944812424480915
iteration 279, loss = 0.006986425723880529
iteration 280, loss = 0.007816874422132969
iteration 281, loss = 0.006697111763060093
iteration 282, loss = 0.0068646683357656
iteration 283, loss = 0.010071435943245888
iteration 284, loss = 0.007227086927741766
iteration 285, loss = 0.0067141251638531685
iteration 286, loss = 0.006390833295881748
iteration 287, loss = 0.007062164600938559
iteration 288, loss = 0.0064056008122861385
iteration 289, loss = 0.007008546497672796
iteration 290, loss = 0.007154049817472696
iteration 291, loss = 0.011123272590339184
iteration 292, loss = 0.015827205032110214
iteration 293, loss = 0.006249195896089077
iteration 294, loss = 0.00710197351872921
iteration 295, loss = 0.007449685130268335
iteration 296, loss = 0.007942168973386288
iteration 297, loss = 0.006044567562639713
iteration 298, loss = 0.007220906671136618
iteration 299, loss = 0.008438128978013992
iteration 0, loss = 0.012788609601557255
iteration 1, loss = 0.006501043681055307
iteration 2, loss = 0.010361586697399616
iteration 3, loss = 0.006342526525259018
iteration 4, loss = 0.006678357254713774
iteration 5, loss = 0.006657643709331751
iteration 6, loss = 0.006880262866616249
iteration 7, loss = 0.006094133947044611
iteration 8, loss = 0.007028444670140743
iteration 9, loss = 0.00921710766851902
iteration 10, loss = 0.007280205842107534
iteration 11, loss = 0.006939420010894537
iteration 12, loss = 0.0069342851638793945
iteration 13, loss = 0.010255692526698112
iteration 14, loss = 0.009708642028272152
iteration 15, loss = 0.00890500470995903
iteration 16, loss = 0.0073951054364442825
iteration 17, loss = 0.006299779284745455
iteration 18, loss = 0.0064786761067807674
iteration 19, loss = 0.006805332377552986
iteration 20, loss = 0.00733517948538065
iteration 21, loss = 0.007579603232443333
iteration 22, loss = 0.006330396048724651
iteration 23, loss = 0.007393610663712025
iteration 24, loss = 0.006864633876830339
iteration 25, loss = 0.007117189466953278
iteration 26, loss = 0.006547824013978243
iteration 27, loss = 0.006437903735786676
iteration 28, loss = 0.0071651507169008255
iteration 29, loss = 0.00697139510884881
iteration 30, loss = 0.0062361424788832664
iteration 31, loss = 0.007483575027436018
iteration 32, loss = 0.010533723048865795
iteration 33, loss = 0.006330392323434353
iteration 34, loss = 0.007204902358353138
iteration 35, loss = 0.006766867823898792
iteration 36, loss = 0.006257143337279558
iteration 37, loss = 0.006606527604162693
iteration 38, loss = 0.010361450724303722
iteration 39, loss = 0.007588779553771019
iteration 40, loss = 0.018227469176054
iteration 41, loss = 0.007935475558042526
iteration 42, loss = 0.006128652021288872
iteration 43, loss = 0.00703380536288023
iteration 44, loss = 0.015702733770012856
iteration 45, loss = 0.0066763246431946754
iteration 46, loss = 0.006079891696572304
iteration 47, loss = 0.006605216301977634
iteration 48, loss = 0.006054254248738289
iteration 49, loss = 0.007254339754581451
iteration 50, loss = 0.00610794872045517
iteration 51, loss = 0.006366401445120573
iteration 52, loss = 0.006826878525316715
iteration 53, loss = 0.006927181500941515
iteration 54, loss = 0.006699919234961271
iteration 55, loss = 0.006971205119043589
iteration 56, loss = 0.008344558998942375
iteration 57, loss = 0.0060962834395468235
iteration 58, loss = 0.00616973964497447
iteration 59, loss = 0.0064804996363818645
iteration 60, loss = 0.00695706345140934
iteration 61, loss = 0.006109586916863918
iteration 62, loss = 0.007678789086639881
iteration 63, loss = 0.006096593104302883
iteration 64, loss = 0.00697830505669117
iteration 65, loss = 0.005973774939775467
iteration 66, loss = 0.006260694935917854
iteration 67, loss = 0.006445550359785557
iteration 68, loss = 0.00572722963988781
iteration 69, loss = 0.006620763801038265
iteration 70, loss = 0.015212960541248322
iteration 71, loss = 0.006266256328672171
iteration 72, loss = 0.005857339594513178
iteration 73, loss = 0.007072835229337215
iteration 74, loss = 0.00610346719622612
iteration 75, loss = 0.006121969316154718
iteration 76, loss = 0.006254732608795166
iteration 77, loss = 0.006025010719895363
iteration 78, loss = 0.0070858183316886425
iteration 79, loss = 0.008509453386068344
iteration 80, loss = 0.006223422009497881
iteration 81, loss = 0.007139333989471197
iteration 82, loss = 0.009202040731906891
iteration 83, loss = 0.006740427576005459
iteration 84, loss = 0.015171988867223263
iteration 85, loss = 0.00655721640214324
iteration 86, loss = 0.006270475219935179
iteration 87, loss = 0.00677268672734499
iteration 88, loss = 0.006485226564109325
iteration 89, loss = 0.015084202401340008
iteration 90, loss = 0.0059809559024870396
iteration 91, loss = 0.010406755842268467
iteration 92, loss = 0.006196414120495319
iteration 93, loss = 0.006071950774639845
iteration 94, loss = 0.0064460840076208115
iteration 95, loss = 0.006022781599313021
iteration 96, loss = 0.005994514562189579
iteration 97, loss = 0.006154519040137529
iteration 98, loss = 0.0075576091185212135
iteration 99, loss = 0.006005290895700455
iteration 100, loss = 0.005919183604419231
iteration 101, loss = 0.005733520723879337
iteration 102, loss = 0.007008237764239311
iteration 103, loss = 0.006682060658931732
iteration 104, loss = 0.00598764605820179
iteration 105, loss = 0.0058787185698747635
iteration 106, loss = 0.006354464218020439
iteration 107, loss = 0.006825686898082495
iteration 108, loss = 0.010176615789532661
iteration 109, loss = 0.006979477591812611
iteration 110, loss = 0.006620305124670267
iteration 111, loss = 0.005903571378439665
iteration 112, loss = 0.0104460334405303
iteration 113, loss = 0.006555065978318453
iteration 114, loss = 0.006196719594299793
iteration 115, loss = 0.007544812746345997
iteration 116, loss = 0.006270269863307476
iteration 117, loss = 0.006547884549945593
iteration 118, loss = 0.00951986201107502
iteration 119, loss = 0.00703456811606884
iteration 120, loss = 0.009324004873633385
iteration 121, loss = 0.008317222818732262
iteration 122, loss = 0.005701772402971983
iteration 123, loss = 0.006339229643344879
iteration 124, loss = 0.0063660042360424995
iteration 125, loss = 0.00616168137639761
iteration 126, loss = 0.006432182155549526
iteration 127, loss = 0.00582638755440712
iteration 128, loss = 0.0068219383247196674
iteration 129, loss = 0.0056713297963142395
iteration 130, loss = 0.0061236838810145855
iteration 131, loss = 0.006941743660718203
iteration 132, loss = 0.0065109701827168465
iteration 133, loss = 0.0069743734784424305
iteration 134, loss = 0.014866683632135391
iteration 135, loss = 0.009917881339788437
iteration 136, loss = 0.007049298845231533
iteration 137, loss = 0.014847638085484505
iteration 138, loss = 0.006688116118311882
iteration 139, loss = 0.00938696414232254
iteration 140, loss = 0.009328127838671207
iteration 141, loss = 0.005753471050411463
iteration 142, loss = 0.00649963179603219
iteration 143, loss = 0.0059345015324652195
iteration 144, loss = 0.005772516597062349
iteration 145, loss = 0.006172562018036842
iteration 146, loss = 0.005865676328539848
iteration 147, loss = 0.0069007910788059235
iteration 148, loss = 0.010082244873046875
iteration 149, loss = 0.009343788027763367
iteration 150, loss = 0.0067275576293468475
iteration 151, loss = 0.006447014398872852
iteration 152, loss = 0.005692357663065195
iteration 153, loss = 0.00783616490662098
iteration 154, loss = 0.006240379065275192
iteration 155, loss = 0.005990683101117611
iteration 156, loss = 0.014454415068030357
iteration 157, loss = 0.006677940953522921
iteration 158, loss = 0.006290670018643141
iteration 159, loss = 0.0057921516709029675
iteration 160, loss = 0.005395445041358471
iteration 161, loss = 0.0064759948290884495
iteration 162, loss = 0.0062291887588799
iteration 163, loss = 0.006570945493876934
iteration 164, loss = 0.0059168399311602116
iteration 165, loss = 0.006371659692376852
iteration 166, loss = 0.0053510903380811214
iteration 167, loss = 0.006345001049339771
iteration 168, loss = 0.00617829617112875
iteration 169, loss = 0.005606403574347496
iteration 170, loss = 0.005701952613890171
iteration 171, loss = 0.014373254962265491
iteration 172, loss = 0.006318427622318268
iteration 173, loss = 0.00576872518286109
iteration 174, loss = 0.006439371034502983
iteration 175, loss = 0.006971194874495268
iteration 176, loss = 0.00725919846445322
iteration 177, loss = 0.006719788536429405
iteration 178, loss = 0.014457521960139275
iteration 179, loss = 0.006781263276934624
iteration 180, loss = 0.005717065185308456
iteration 181, loss = 0.009622432291507721
iteration 182, loss = 0.006726720370352268
iteration 183, loss = 0.005866987630724907
iteration 184, loss = 0.00604845630005002
iteration 185, loss = 0.006183560937643051
iteration 186, loss = 0.00568516505882144
iteration 187, loss = 0.006225363351404667
iteration 188, loss = 0.016584448516368866
iteration 189, loss = 0.0066087506711483
iteration 190, loss = 0.005813299212604761
iteration 191, loss = 0.006986917927861214
iteration 192, loss = 0.005541688296943903
iteration 193, loss = 0.0064202360808849335
iteration 194, loss = 0.005705320276319981
iteration 195, loss = 0.00867934338748455
iteration 196, loss = 0.006470711901783943
iteration 197, loss = 0.005685599986463785
iteration 198, loss = 0.00585376750677824
iteration 199, loss = 0.005420186091214418
iteration 200, loss = 0.005913969594985247
iteration 201, loss = 0.00613800436258316
iteration 202, loss = 0.005976143758744001
iteration 203, loss = 0.0062383124604821205
iteration 204, loss = 0.00581715302541852
iteration 205, loss = 0.006534337531775236
iteration 206, loss = 0.006088351830840111
iteration 207, loss = 0.014038678258657455
iteration 208, loss = 0.006777319125831127
iteration 209, loss = 0.00663372129201889
iteration 210, loss = 0.014652349054813385
iteration 211, loss = 0.00584114296361804
iteration 212, loss = 0.005870417226105928
iteration 213, loss = 0.005621755495667458
iteration 214, loss = 0.006884564645588398
iteration 215, loss = 0.0056870561093091965
iteration 216, loss = 0.005987516138702631
iteration 217, loss = 0.006013752426952124
iteration 218, loss = 0.014887263998389244
iteration 219, loss = 0.005888098850846291
iteration 220, loss = 0.008029593154788017
iteration 221, loss = 0.00601169653236866
iteration 222, loss = 0.006346864625811577
iteration 223, loss = 0.006350093055516481
iteration 224, loss = 0.005197387654334307
iteration 225, loss = 0.006495104171335697
iteration 226, loss = 0.006673782132565975
iteration 227, loss = 0.006907072849571705
iteration 228, loss = 0.005820718128234148
iteration 229, loss = 0.005886011756956577
iteration 230, loss = 0.005818965844810009
iteration 231, loss = 0.00697094202041626
iteration 232, loss = 0.0082117710262537
iteration 233, loss = 0.006979369558393955
iteration 234, loss = 0.005977836903184652
iteration 235, loss = 0.007934815250337124
iteration 236, loss = 0.007359880022704601
iteration 237, loss = 0.0053533217869699
iteration 238, loss = 0.005709340795874596
iteration 239, loss = 0.005829834379255772
iteration 240, loss = 0.005762827582657337
iteration 241, loss = 0.007831035181879997
iteration 242, loss = 0.00592327956110239
iteration 243, loss = 0.00555527675896883
iteration 244, loss = 0.01711721532046795
iteration 245, loss = 0.006771680433303118
iteration 246, loss = 0.005448161158710718
iteration 247, loss = 0.006547945085912943
iteration 248, loss = 0.006652534008026123
iteration 249, loss = 0.005518140271306038
iteration 250, loss = 0.005777532234787941
iteration 251, loss = 0.006872690282762051
iteration 252, loss = 0.006548155099153519
iteration 253, loss = 0.005607290659099817
iteration 254, loss = 0.0065741166472435
iteration 255, loss = 0.0056852661073207855
iteration 256, loss = 0.005463809240609407
iteration 257, loss = 0.006850528530776501
iteration 258, loss = 0.005268014967441559
iteration 259, loss = 0.005556321237236261
iteration 260, loss = 0.005921749398112297
iteration 261, loss = 0.010847094468772411
iteration 262, loss = 0.005839092191308737
iteration 263, loss = 0.005952043458819389
iteration 264, loss = 0.01014140248298645
iteration 265, loss = 0.006353065371513367
iteration 266, loss = 0.0052542779594659805
iteration 267, loss = 0.006821140181273222
iteration 268, loss = 0.00643105898052454
iteration 269, loss = 0.0056337653659284115
iteration 270, loss = 0.008376139216125011
iteration 271, loss = 0.0072553399950265884
iteration 272, loss = 0.0057337540201842785
iteration 273, loss = 0.005827804561704397
iteration 274, loss = 0.0054015954956412315
iteration 275, loss = 0.005407081916928291
iteration 276, loss = 0.005782673601061106
iteration 277, loss = 0.005798712372779846
iteration 278, loss = 0.008771931752562523
iteration 279, loss = 0.005360754672437906
iteration 280, loss = 0.0058702267706394196
iteration 281, loss = 0.005515457596629858
iteration 282, loss = 0.005089809186756611
iteration 283, loss = 0.0055104694329202175
iteration 284, loss = 0.0066357627511024475
iteration 285, loss = 0.005597392562776804
iteration 286, loss = 0.005035217385739088
iteration 287, loss = 0.005434674210846424
iteration 288, loss = 0.005993093829602003
iteration 289, loss = 0.008159366436302662
iteration 290, loss = 0.00547811109572649
iteration 291, loss = 0.0053322287276387215
iteration 292, loss = 0.005809578113257885
iteration 293, loss = 0.005247511900961399
iteration 294, loss = 0.00528712896630168
iteration 295, loss = 0.007495197467505932
iteration 296, loss = 0.005337328650057316
iteration 297, loss = 0.005841424223035574
iteration 298, loss = 0.005634842906147242
iteration 299, loss = 0.005521742627024651
iteration 0, loss = 0.0051092044450342655
iteration 1, loss = 0.005312713328748941
iteration 2, loss = 0.005608316510915756
iteration 3, loss = 0.005801754537969828
iteration 4, loss = 0.006178641226142645
iteration 5, loss = 0.005477556027472019
iteration 6, loss = 0.00547137251123786
iteration 7, loss = 0.006505715195089579
iteration 8, loss = 0.004896760918200016
iteration 9, loss = 0.008724929764866829
iteration 10, loss = 0.009145165793597698
iteration 11, loss = 0.0050240131095051765
iteration 12, loss = 0.005357001908123493
iteration 13, loss = 0.0057701170444488525
iteration 14, loss = 0.013419835828244686
iteration 15, loss = 0.005723258480429649
iteration 16, loss = 0.007473462261259556
iteration 17, loss = 0.005663644056767225
iteration 18, loss = 0.005296044051647186
iteration 19, loss = 0.006181039847433567
iteration 20, loss = 0.00522790988907218
iteration 21, loss = 0.005579966586083174
iteration 22, loss = 0.005014222115278244
iteration 23, loss = 0.013294988311827183
iteration 24, loss = 0.0058509586378932
iteration 25, loss = 0.0050783357582986355
iteration 26, loss = 0.005048165563493967
iteration 27, loss = 0.0058210198767483234
iteration 28, loss = 0.015043230727314949
iteration 29, loss = 0.006588341668248177
iteration 30, loss = 0.005425638519227505
iteration 31, loss = 0.005920275580137968
iteration 32, loss = 0.005142355803400278
iteration 33, loss = 0.004974689334630966
iteration 34, loss = 0.004849130753427744
iteration 35, loss = 0.005448352079838514
iteration 36, loss = 0.006099648308008909
iteration 37, loss = 0.006297482177615166
iteration 38, loss = 0.005646141245961189
iteration 39, loss = 0.005971499253064394
iteration 40, loss = 0.0053110262379050255
iteration 41, loss = 0.005886211525648832
iteration 42, loss = 0.006384780630469322
iteration 43, loss = 0.0058156331069767475
iteration 44, loss = 0.005378627218306065
iteration 45, loss = 0.005274628754705191
iteration 46, loss = 0.004991847090423107
iteration 47, loss = 0.004770021885633469
iteration 48, loss = 0.005502811633050442
iteration 49, loss = 0.006260215770453215
iteration 50, loss = 0.013901390135288239
iteration 51, loss = 0.008180297911167145
iteration 52, loss = 0.00561444740742445
iteration 53, loss = 0.005746469832956791
iteration 54, loss = 0.0060882167890667915
iteration 55, loss = 0.005864568054676056
iteration 56, loss = 0.005589053966104984
iteration 57, loss = 0.005551654379814863
iteration 58, loss = 0.007886355742812157
iteration 59, loss = 0.004917537793517113
iteration 60, loss = 0.004951667971909046
iteration 61, loss = 0.005221046507358551
iteration 62, loss = 0.004842615686357021
iteration 63, loss = 0.008468004874885082
iteration 64, loss = 0.005750234704464674
iteration 65, loss = 0.005687598139047623
iteration 66, loss = 0.006915962789207697
iteration 67, loss = 0.0050804950296878815
iteration 68, loss = 0.00516138132661581
iteration 69, loss = 0.0057726772502064705
iteration 70, loss = 0.007646144367754459
iteration 71, loss = 0.006039898842573166
iteration 72, loss = 0.005994359962642193
iteration 73, loss = 0.005932134576141834
iteration 74, loss = 0.005688668228685856
iteration 75, loss = 0.0059393043629825115
iteration 76, loss = 0.00594225712120533
iteration 77, loss = 0.006174540612846613
iteration 78, loss = 0.013696209527552128
iteration 79, loss = 0.005214292090386152
iteration 80, loss = 0.004946933127939701
iteration 81, loss = 0.005357815884053707
iteration 82, loss = 0.005486868787556887
iteration 83, loss = 0.00588525366038084
iteration 84, loss = 0.005094490479677916
iteration 85, loss = 0.004812016151845455
iteration 86, loss = 0.012933490797877312
iteration 87, loss = 0.005940739996731281
iteration 88, loss = 0.006701881065964699
iteration 89, loss = 0.005175390746444464
iteration 90, loss = 0.004959152080118656
iteration 91, loss = 0.006974366027861834
iteration 92, loss = 0.005488008260726929
iteration 93, loss = 0.006292289588600397
iteration 94, loss = 0.005395727232098579
iteration 95, loss = 0.0057739573530852795
iteration 96, loss = 0.004970789887011051
iteration 97, loss = 0.00513111986219883
iteration 98, loss = 0.00516725517809391
iteration 99, loss = 0.005926379468291998
iteration 100, loss = 0.005852396134287119
iteration 101, loss = 0.00525697274133563
iteration 102, loss = 0.005329480394721031
iteration 103, loss = 0.005804999731481075
iteration 104, loss = 0.005439838860183954
iteration 105, loss = 0.007856038399040699
iteration 106, loss = 0.007293174043297768
iteration 107, loss = 0.004981370642781258
iteration 108, loss = 0.005074605345726013
iteration 109, loss = 0.004963782150298357
iteration 110, loss = 0.006376032717525959
iteration 111, loss = 0.005143322981894016
iteration 112, loss = 0.005079030524939299
iteration 113, loss = 0.004934819880872965
iteration 114, loss = 0.00584787642583251
iteration 115, loss = 0.005512160249054432
iteration 116, loss = 0.004694451577961445
iteration 117, loss = 0.004850954283028841
iteration 118, loss = 0.005167048424482346
iteration 119, loss = 0.0053306943736970425
iteration 120, loss = 0.004798175301402807
iteration 121, loss = 0.0053681181743741035
iteration 122, loss = 0.005075163207948208
iteration 123, loss = 0.00530330091714859
iteration 124, loss = 0.005166098475456238
iteration 125, loss = 0.004969884641468525
iteration 126, loss = 0.0053482926450669765
iteration 127, loss = 0.005060229450464249
iteration 128, loss = 0.00519279669970274
iteration 129, loss = 0.005181793123483658
iteration 130, loss = 0.0069462270475924015
iteration 131, loss = 0.005055438261479139
iteration 132, loss = 0.005972755141556263
iteration 133, loss = 0.006924529559910297
iteration 134, loss = 0.004681434947997332
iteration 135, loss = 0.004710223525762558
iteration 136, loss = 0.004923633765429258
iteration 137, loss = 0.004991511348634958
iteration 138, loss = 0.004958602134138346
iteration 139, loss = 0.005581884644925594
iteration 140, loss = 0.005549178458750248
iteration 141, loss = 0.005060015246272087
iteration 142, loss = 0.005480114836245775
iteration 143, loss = 0.00572411622852087
iteration 144, loss = 0.00563338166102767
iteration 145, loss = 0.005656386725604534
iteration 146, loss = 0.00553461629897356
iteration 147, loss = 0.004943853244185448
iteration 148, loss = 0.00533369742333889
iteration 149, loss = 0.005150070879608393
iteration 150, loss = 0.013176818378269672
iteration 151, loss = 0.004863639362156391
iteration 152, loss = 0.006544245406985283
iteration 153, loss = 0.005047148559242487
iteration 154, loss = 0.006327940616756678
iteration 155, loss = 0.005722524598240852
iteration 156, loss = 0.006947301793843508
iteration 157, loss = 0.005463289562612772
iteration 158, loss = 0.005012359004467726
iteration 159, loss = 0.0048539224080741405
iteration 160, loss = 0.004771379753947258
iteration 161, loss = 0.004995525814592838
iteration 162, loss = 0.012854213826358318
iteration 163, loss = 0.005048351362347603
iteration 164, loss = 0.006572571583092213
iteration 165, loss = 0.006013768259435892
iteration 166, loss = 0.006063570734113455
iteration 167, loss = 0.00618019700050354
iteration 168, loss = 0.004776403307914734
iteration 169, loss = 0.0054268310777843
iteration 170, loss = 0.004993835464119911
iteration 171, loss = 0.0057991668581962585
iteration 172, loss = 0.005352240987122059
iteration 173, loss = 0.004493594169616699
iteration 174, loss = 0.005093701649457216
iteration 175, loss = 0.005518692079931498
iteration 176, loss = 0.004982527811080217
iteration 177, loss = 0.0051759579218924046
iteration 178, loss = 0.008148645982146263
iteration 179, loss = 0.0069316476583480835
iteration 180, loss = 0.004890051204711199
iteration 181, loss = 0.005207214038819075
iteration 182, loss = 0.004910422954708338
iteration 183, loss = 0.0052142394706606865
iteration 184, loss = 0.005324170924723148
iteration 185, loss = 0.004715289454907179
iteration 186, loss = 0.005199645645916462
iteration 187, loss = 0.006686796434223652
iteration 188, loss = 0.005706102587282658
iteration 189, loss = 0.005366686265915632
iteration 190, loss = 0.004903225693851709
iteration 191, loss = 0.005308428313583136
iteration 192, loss = 0.005495247896760702
iteration 193, loss = 0.0048860590904951096
iteration 194, loss = 0.004767737817019224
iteration 195, loss = 0.005763085559010506
iteration 196, loss = 0.005940079223364592
iteration 197, loss = 0.006924821063876152
iteration 198, loss = 0.005435829982161522
iteration 199, loss = 0.004819868132472038
iteration 200, loss = 0.005158813204616308
iteration 201, loss = 0.005733560770750046
iteration 202, loss = 0.004869628697633743
iteration 203, loss = 0.007799162063747644
iteration 204, loss = 0.004601649474352598
iteration 205, loss = 0.00486996304243803
iteration 206, loss = 0.00769741740077734
iteration 207, loss = 0.004838964436203241
iteration 208, loss = 0.004465150181204081
iteration 209, loss = 0.005611753091216087
iteration 210, loss = 0.007903999648988247
iteration 211, loss = 0.005249560810625553
iteration 212, loss = 0.005118758417665958
iteration 213, loss = 0.0063581569120287895
iteration 214, loss = 0.005364160984754562
iteration 215, loss = 0.004990092944353819
iteration 216, loss = 0.005045403726398945
iteration 217, loss = 0.004489769693464041
iteration 218, loss = 0.004544674418866634
iteration 219, loss = 0.012441864237189293
iteration 220, loss = 0.007788695394992828
iteration 221, loss = 0.004668857902288437
iteration 222, loss = 0.014656208455562592
iteration 223, loss = 0.007645299192517996
iteration 224, loss = 0.01264831330627203
iteration 225, loss = 0.005056464113295078
iteration 226, loss = 0.005652843974530697
iteration 227, loss = 0.013202274218201637
iteration 228, loss = 0.012928120791912079
iteration 229, loss = 0.004564215429127216
iteration 230, loss = 0.0052844756282866
iteration 231, loss = 0.006575544364750385
iteration 232, loss = 0.004904031753540039
iteration 233, loss = 0.005275784060359001
iteration 234, loss = 0.004652884788811207
iteration 235, loss = 0.004396672826260328
iteration 236, loss = 0.008067225106060505
iteration 237, loss = 0.004673395771533251
iteration 238, loss = 0.007121934089809656
iteration 239, loss = 0.00468089384958148
iteration 240, loss = 0.004568059463053942
iteration 241, loss = 0.004998304415494204
iteration 242, loss = 0.0047419723123312
iteration 243, loss = 0.005359289236366749
iteration 244, loss = 0.006111917085945606
iteration 245, loss = 0.004950636997818947
iteration 246, loss = 0.004885462112724781
iteration 247, loss = 0.005533622112125158
iteration 248, loss = 0.004552057944238186
iteration 249, loss = 0.004808203317224979
iteration 250, loss = 0.005200064275413752
iteration 251, loss = 0.005111386999487877
iteration 252, loss = 0.004993080161511898
iteration 253, loss = 0.0055600786581635475
iteration 254, loss = 0.005160067230463028
iteration 255, loss = 0.012025841511785984
iteration 256, loss = 0.004313419573009014
iteration 257, loss = 0.004683834966272116
iteration 258, loss = 0.0044525000266730785
iteration 259, loss = 0.0063546448945999146
iteration 260, loss = 0.00466314610093832
iteration 261, loss = 0.004566997289657593
iteration 262, loss = 0.004579911008477211
iteration 263, loss = 0.007674796041101217
iteration 264, loss = 0.005322831217199564
iteration 265, loss = 0.004466963931918144
iteration 266, loss = 0.004763099830597639
iteration 267, loss = 0.014121362008154392
iteration 268, loss = 0.005004876758903265
iteration 269, loss = 0.007183315232396126
iteration 270, loss = 0.0042114416137337685
iteration 271, loss = 0.005586497485637665
iteration 272, loss = 0.004644168075174093
iteration 273, loss = 0.005225988104939461
iteration 274, loss = 0.0069778007455170155
iteration 275, loss = 0.008480332791805267
iteration 276, loss = 0.005070969462394714
iteration 277, loss = 0.007832838222384453
iteration 278, loss = 0.005098465830087662
iteration 279, loss = 0.004793534986674786
iteration 280, loss = 0.004740134347230196
iteration 281, loss = 0.00455357925966382
iteration 282, loss = 0.006878022104501724
iteration 283, loss = 0.007445966359227896
iteration 284, loss = 0.005790078546851873
iteration 285, loss = 0.004294821061193943
iteration 286, loss = 0.007449625991284847
iteration 287, loss = 0.004880670458078384
iteration 288, loss = 0.004783706273883581
iteration 289, loss = 0.007817701436579227
iteration 290, loss = 0.004496572073549032
iteration 291, loss = 0.005067911930382252
iteration 292, loss = 0.00458777230232954
iteration 293, loss = 0.004769482649862766
iteration 294, loss = 0.005327160935848951
iteration 295, loss = 0.007675900589674711
iteration 296, loss = 0.005269388202577829
iteration 297, loss = 0.005549202673137188
iteration 298, loss = 0.004984609317034483
iteration 299, loss = 0.005644121672958136
iteration 0, loss = 0.005451302044093609
iteration 1, loss = 0.004693605471402407
iteration 2, loss = 0.005045853555202484
iteration 3, loss = 0.004822420421987772
iteration 4, loss = 0.004737453069537878
iteration 5, loss = 0.007116446737200022
iteration 6, loss = 0.004276168532669544
iteration 7, loss = 0.005000260192900896
iteration 8, loss = 0.005261508747935295
iteration 9, loss = 0.00444839708507061
iteration 10, loss = 0.004956547636538744
iteration 11, loss = 0.005180578213185072
iteration 12, loss = 0.004518880508840084
iteration 13, loss = 0.004944422282278538
iteration 14, loss = 0.004867035895586014
iteration 15, loss = 0.005025154445320368
iteration 16, loss = 0.004644019529223442
iteration 17, loss = 0.004744304344058037
iteration 18, loss = 0.0041299257427453995
iteration 19, loss = 0.006094187498092651
iteration 20, loss = 0.005206495523452759
iteration 21, loss = 0.004386991262435913
iteration 22, loss = 0.004681792110204697
iteration 23, loss = 0.0047504594549536705
iteration 24, loss = 0.004837528802454472
iteration 25, loss = 0.004506414756178856
iteration 26, loss = 0.004999872297048569
iteration 27, loss = 0.006781435571610928
iteration 28, loss = 0.011822509579360485
iteration 29, loss = 0.0044302791357040405
iteration 30, loss = 0.007789896801114082
iteration 31, loss = 0.004804139956831932
iteration 32, loss = 0.005481755826622248
iteration 33, loss = 0.004488754086196423
iteration 34, loss = 0.004375893622636795
iteration 35, loss = 0.0047683315351605415
iteration 36, loss = 0.004901582840830088
iteration 37, loss = 0.005066785961389542
iteration 38, loss = 0.0042554838582873344
iteration 39, loss = 0.012035318650305271
iteration 40, loss = 0.004868099465966225
iteration 41, loss = 0.004959348123520613
iteration 42, loss = 0.004343186505138874
iteration 43, loss = 0.004776280373334885
iteration 44, loss = 0.0073880599811673164
iteration 45, loss = 0.006175254005938768
iteration 46, loss = 0.004537478554993868
iteration 47, loss = 0.004108751658350229
iteration 48, loss = 0.004757355432957411
iteration 49, loss = 0.005350961349904537
iteration 50, loss = 0.004528264049440622
iteration 51, loss = 0.005739040207117796
iteration 52, loss = 0.00456772418692708
iteration 53, loss = 0.005636018235236406
iteration 54, loss = 0.004317492712289095
iteration 55, loss = 0.0045038615353405476
iteration 56, loss = 0.007631596643477678
iteration 57, loss = 0.005407959222793579
iteration 58, loss = 0.004476236179471016
iteration 59, loss = 0.0046483613550662994
iteration 60, loss = 0.004554202314466238
iteration 61, loss = 0.011572535149753094
iteration 62, loss = 0.0059444112703204155
iteration 63, loss = 0.004380138125270605
iteration 64, loss = 0.007021734956651926
iteration 65, loss = 0.005553385242819786
iteration 66, loss = 0.004573096986860037
iteration 67, loss = 0.005359908565878868
iteration 68, loss = 0.004508623853325844
iteration 69, loss = 0.00471185427159071
iteration 70, loss = 0.004690262023359537
iteration 71, loss = 0.011554410681128502
iteration 72, loss = 0.011362742632627487
iteration 73, loss = 0.004388128407299519
iteration 74, loss = 0.004703835118561983
iteration 75, loss = 0.007102914620190859
iteration 76, loss = 0.004192898515611887
iteration 77, loss = 0.004688096698373556
iteration 78, loss = 0.004895741585642099
iteration 79, loss = 0.004269320517778397
iteration 80, loss = 0.004209253471344709
iteration 81, loss = 0.011181537993252277
iteration 82, loss = 0.004648694768548012
iteration 83, loss = 0.0067126075737178326
iteration 84, loss = 0.004396037198603153
iteration 85, loss = 0.004987752065062523
iteration 86, loss = 0.004708138294517994
iteration 87, loss = 0.007389358710497618
iteration 88, loss = 0.004993067122995853
iteration 89, loss = 0.004113260190933943
iteration 90, loss = 0.004463825840502977
iteration 91, loss = 0.00495396601036191
iteration 92, loss = 0.004142942372709513
iteration 93, loss = 0.004213433712720871
iteration 94, loss = 0.004634927026927471
iteration 95, loss = 0.004067691508680582
iteration 96, loss = 0.004837198182940483
iteration 97, loss = 0.004225823096930981
iteration 98, loss = 0.006187081802636385
iteration 99, loss = 0.004552793689072132
iteration 100, loss = 0.00759517727419734
iteration 101, loss = 0.004512914456427097
iteration 102, loss = 0.00449225353077054
iteration 103, loss = 0.004766806494444609
iteration 104, loss = 0.0045966655015945435
iteration 105, loss = 0.004640238359570503
iteration 106, loss = 0.005005771294236183
iteration 107, loss = 0.00438739312812686
iteration 108, loss = 0.004832483362406492
iteration 109, loss = 0.0064558410085737705
iteration 110, loss = 0.005408043973147869
iteration 111, loss = 0.0055078486911952496
iteration 112, loss = 0.004874412901699543
iteration 113, loss = 0.004502348136156797
iteration 114, loss = 0.004500410985201597
iteration 115, loss = 0.0042047579772770405
iteration 116, loss = 0.00431651109829545
iteration 117, loss = 0.012569687329232693
iteration 118, loss = 0.004246670752763748
iteration 119, loss = 0.008354916237294674
iteration 120, loss = 0.005981653928756714
iteration 121, loss = 0.004260123707354069
iteration 122, loss = 0.0044014244340360165
iteration 123, loss = 0.00430699810385704
iteration 124, loss = 0.0042222486808896065
iteration 125, loss = 0.004404979757964611
iteration 126, loss = 0.006547930184751749
iteration 127, loss = 0.004619172308593988
iteration 128, loss = 0.004729629959911108
iteration 129, loss = 0.005085508339107037
iteration 130, loss = 0.004385450854897499
iteration 131, loss = 0.005366752855479717
iteration 132, loss = 0.0041741374880075455
iteration 133, loss = 0.005712683312594891
iteration 134, loss = 0.0046568363904953
iteration 135, loss = 0.004706703592091799
iteration 136, loss = 0.00442124018445611
iteration 137, loss = 0.006684300024062395
iteration 138, loss = 0.004623784217983484
iteration 139, loss = 0.004433877300471067
iteration 140, loss = 0.004703951999545097
iteration 141, loss = 0.0045122127048671246
iteration 142, loss = 0.004250244703143835
iteration 143, loss = 0.004705111030489206
iteration 144, loss = 0.004781623836606741
iteration 145, loss = 0.004448014311492443
iteration 146, loss = 0.0046191029250621796
iteration 147, loss = 0.00415966659784317
iteration 148, loss = 0.004660594277083874
iteration 149, loss = 0.004140528384596109
iteration 150, loss = 0.010882716625928879
iteration 151, loss = 0.009866995736956596
iteration 152, loss = 0.0072695729322731495
iteration 153, loss = 0.004511568229645491
iteration 154, loss = 0.004474435932934284
iteration 155, loss = 0.006599431857466698
iteration 156, loss = 0.0070238737389445305
iteration 157, loss = 0.011095947585999966
iteration 158, loss = 0.0053010000847280025
iteration 159, loss = 0.004159749951213598
iteration 160, loss = 0.011219292879104614
iteration 161, loss = 0.003952146042138338
iteration 162, loss = 0.004145574290305376
iteration 163, loss = 0.004512506537139416
iteration 164, loss = 0.0047495001927018166
iteration 165, loss = 0.003915741574019194
iteration 166, loss = 0.0042589888907969
iteration 167, loss = 0.003920180723071098
iteration 168, loss = 0.011284549720585346
iteration 169, loss = 0.004621410742402077
iteration 170, loss = 0.007053273729979992
iteration 171, loss = 0.004586863797158003
iteration 172, loss = 0.005119292996823788
iteration 173, loss = 0.004401577636599541
iteration 174, loss = 0.003998930100351572
iteration 175, loss = 0.006611191667616367
iteration 176, loss = 0.004177742637693882
iteration 177, loss = 0.004133624956011772
iteration 178, loss = 0.004255772102624178
iteration 179, loss = 0.004560952074825764
iteration 180, loss = 0.0045047421008348465
iteration 181, loss = 0.004491884261369705
iteration 182, loss = 0.004112857393920422
iteration 183, loss = 0.004632020369172096
iteration 184, loss = 0.004785496275871992
iteration 185, loss = 0.003939553163945675
iteration 186, loss = 0.004170146305114031
iteration 187, loss = 0.004362073726952076
iteration 188, loss = 0.004428524058312178
iteration 189, loss = 0.004967382177710533
iteration 190, loss = 0.004547610878944397
iteration 191, loss = 0.005723994225263596
iteration 192, loss = 0.0047156489454209805
iteration 193, loss = 0.004827144555747509
iteration 194, loss = 0.003967250697314739
iteration 195, loss = 0.004638561047613621
iteration 196, loss = 0.004756369162350893
iteration 197, loss = 0.005793892778456211
iteration 198, loss = 0.004131468944251537
iteration 199, loss = 0.004163774661719799
iteration 200, loss = 0.004798779264092445
iteration 201, loss = 0.005020454525947571
iteration 202, loss = 0.006640888750553131
iteration 203, loss = 0.003974983934313059
iteration 204, loss = 0.0040515088476240635
iteration 205, loss = 0.004104977939277887
iteration 206, loss = 0.0041801948100328445
iteration 207, loss = 0.006146457511931658
iteration 208, loss = 0.004292243625968695
iteration 209, loss = 0.006493973080068827
iteration 210, loss = 0.004009515047073364
iteration 211, loss = 0.004363582469522953
iteration 212, loss = 0.0040455469861626625
iteration 213, loss = 0.0037618272472172976
iteration 214, loss = 0.005897739436477423
iteration 215, loss = 0.0043089971877634525
iteration 216, loss = 0.004898647777736187
iteration 217, loss = 0.004415085073560476
iteration 218, loss = 0.004990525543689728
iteration 219, loss = 0.004461405333131552
iteration 220, loss = 0.004498510155826807
iteration 221, loss = 0.003955750726163387
iteration 222, loss = 0.004515814129263163
iteration 223, loss = 0.004128882195800543
iteration 224, loss = 0.00473626796156168
iteration 225, loss = 0.004267720505595207
iteration 226, loss = 0.004116815514862537
iteration 227, loss = 0.004006584174931049
iteration 228, loss = 0.004715897608548403
iteration 229, loss = 0.007713706698268652
iteration 230, loss = 0.0039574941620230675
iteration 231, loss = 0.004395509138703346
iteration 232, loss = 0.004069216083735228
iteration 233, loss = 0.004442734643816948
iteration 234, loss = 0.004812338389456272
iteration 235, loss = 0.0043667275458574295
iteration 236, loss = 0.004586081486195326
iteration 237, loss = 0.004205315373837948
iteration 238, loss = 0.00497780553996563
iteration 239, loss = 0.003895578673109412
iteration 240, loss = 0.004793547093868256
iteration 241, loss = 0.0045649949461221695
iteration 242, loss = 0.004799875896424055
iteration 243, loss = 0.004309636540710926
iteration 244, loss = 0.004639270715415478
iteration 245, loss = 0.00522375525906682
iteration 246, loss = 0.0047812978737056255
iteration 247, loss = 0.010640800930559635
iteration 248, loss = 0.0042412965558469296
iteration 249, loss = 0.0037894498091191053
iteration 250, loss = 0.0043113501742482185
iteration 251, loss = 0.004003242123872042
iteration 252, loss = 0.003923841752111912
iteration 253, loss = 0.004584021400660276
iteration 254, loss = 0.0036691995337605476
iteration 255, loss = 0.0037465193308889866
iteration 256, loss = 0.004054456949234009
iteration 257, loss = 0.003859101328998804
iteration 258, loss = 0.004427653271704912
iteration 259, loss = 0.007273286581039429
iteration 260, loss = 0.004189980216324329
iteration 261, loss = 0.0040998258627951145
iteration 262, loss = 0.004439210519194603
iteration 263, loss = 0.005890363827347755
iteration 264, loss = 0.004111265763640404
iteration 265, loss = 0.003875887952744961
iteration 266, loss = 0.004690507426857948
iteration 267, loss = 0.0038791843689978123
iteration 268, loss = 0.004055500961840153
iteration 269, loss = 0.00488278130069375
iteration 270, loss = 0.00408150814473629
iteration 271, loss = 0.0037214928306639194
iteration 272, loss = 0.004227340687066317
iteration 273, loss = 0.004034130834043026
iteration 274, loss = 0.0038100164383649826
iteration 275, loss = 0.004171113483607769
iteration 276, loss = 0.003943596035242081
iteration 277, loss = 0.00392540218308568
iteration 278, loss = 0.005019203294068575
iteration 279, loss = 0.01022222451865673
iteration 280, loss = 0.004244409501552582
iteration 281, loss = 0.0043437546119093895
iteration 282, loss = 0.010812454856932163
iteration 283, loss = 0.006256686523556709
iteration 284, loss = 0.0044747949577867985
iteration 285, loss = 0.004274384118616581
iteration 286, loss = 0.003780982457101345
iteration 287, loss = 0.004573932848870754
iteration 288, loss = 0.004946295637637377
iteration 289, loss = 0.004035564139485359
iteration 290, loss = 0.0037476280704140663
iteration 291, loss = 0.003976945765316486
iteration 292, loss = 0.010265556164085865
iteration 293, loss = 0.00423012999817729
iteration 294, loss = 0.004084675572812557
iteration 295, loss = 0.0056183068081736565
iteration 296, loss = 0.0049639977514743805
iteration 297, loss = 0.003958920016884804
iteration 298, loss = 0.003726340364664793
iteration 299, loss = 0.003782113781198859
iteration 0, loss = 0.004363224841654301
iteration 1, loss = 0.00520816957578063
iteration 2, loss = 0.003929845057427883
iteration 3, loss = 0.004233270417898893
iteration 4, loss = 0.005869497545063496
iteration 5, loss = 0.004041609354317188
iteration 6, loss = 0.0039035091176629066
iteration 7, loss = 0.003670811653137207
iteration 8, loss = 0.004109821282327175
iteration 9, loss = 0.004458459094166756
iteration 10, loss = 0.004473420325666666
iteration 11, loss = 0.00393801461905241
iteration 12, loss = 0.006136002484709024
iteration 13, loss = 0.004005232825875282
iteration 14, loss = 0.004340410232543945
iteration 15, loss = 0.0037518818862736225
iteration 16, loss = 0.003962948452681303
iteration 17, loss = 0.004288854543119669
iteration 18, loss = 0.004189915023744106
iteration 19, loss = 0.003761799307540059
iteration 20, loss = 0.004202043171972036
iteration 21, loss = 0.003981566987931728
iteration 22, loss = 0.0048979599960148335
iteration 23, loss = 0.004816325381398201
iteration 24, loss = 0.006255353335291147
iteration 25, loss = 0.004545105155557394
iteration 26, loss = 0.004273189231753349
iteration 27, loss = 0.006651705130934715
iteration 28, loss = 0.004571215249598026
iteration 29, loss = 0.003738978412002325
iteration 30, loss = 0.004223750904202461
iteration 31, loss = 0.004021940752863884
iteration 32, loss = 0.003807511180639267
iteration 33, loss = 0.006411722395569086
iteration 34, loss = 0.0037096478044986725
iteration 35, loss = 0.0040871761739254
iteration 36, loss = 0.0037981318309903145
iteration 37, loss = 0.004105923697352409
iteration 38, loss = 0.003808015724644065
iteration 39, loss = 0.0038522316608577967
iteration 40, loss = 0.003827902488410473
iteration 41, loss = 0.0038585267029702663
iteration 42, loss = 0.004104504361748695
iteration 43, loss = 0.006614234764128923
iteration 44, loss = 0.0036510031204670668
iteration 45, loss = 0.0038712334353476763
iteration 46, loss = 0.003921563271433115
iteration 47, loss = 0.004183839540928602
iteration 48, loss = 0.0037803021259605885
iteration 49, loss = 0.003998680971562862
iteration 50, loss = 0.003957456909120083
iteration 51, loss = 0.005680037196725607
iteration 52, loss = 0.003942726645618677
iteration 53, loss = 0.0036847349256277084
iteration 54, loss = 0.004312098026275635
iteration 55, loss = 0.005476550664752722
iteration 56, loss = 0.003902606200426817
iteration 57, loss = 0.004020803142338991
iteration 58, loss = 0.0037807156331837177
iteration 59, loss = 0.0038831543643027544
iteration 60, loss = 0.004045587033033371
iteration 61, loss = 0.004367370158433914
iteration 62, loss = 0.008267300203442574
iteration 63, loss = 0.004575680010020733
iteration 64, loss = 0.003614572109654546
iteration 65, loss = 0.005020546726882458
iteration 66, loss = 0.003916493151336908
iteration 67, loss = 0.004301945213228464
iteration 68, loss = 0.003761962288990617
iteration 69, loss = 0.0034939453471451998
iteration 70, loss = 0.004154684022068977
iteration 71, loss = 0.004585822578519583
iteration 72, loss = 0.0037654826883226633
iteration 73, loss = 0.005451107397675514
iteration 74, loss = 0.005061064846813679
iteration 75, loss = 0.004727963823825121
iteration 76, loss = 0.004799489863216877
iteration 77, loss = 0.006119086407124996
iteration 78, loss = 0.0037725267466157675
iteration 79, loss = 0.003700155531987548
iteration 80, loss = 0.003630378982052207
iteration 81, loss = 0.0052178362384438515
iteration 82, loss = 0.003956649452447891
iteration 83, loss = 0.00412414176389575
iteration 84, loss = 0.0036268995609134436
iteration 85, loss = 0.004782164003700018
iteration 86, loss = 0.004940822720527649
iteration 87, loss = 0.004238935187458992
iteration 88, loss = 0.004567726515233517
iteration 89, loss = 0.0038312526885420084
iteration 90, loss = 0.004168116487562656
iteration 91, loss = 0.003456349717453122
iteration 92, loss = 0.00365018704906106
iteration 93, loss = 0.004086396191269159
iteration 94, loss = 0.004182280041277409
iteration 95, loss = 0.003749414347112179
iteration 96, loss = 0.00704749533906579
iteration 97, loss = 0.003968548029661179
iteration 98, loss = 0.0043864017352461815
iteration 99, loss = 0.0037950873374938965
iteration 100, loss = 0.003664677496999502
iteration 101, loss = 0.004002823960036039
iteration 102, loss = 0.004064702894538641
iteration 103, loss = 0.011358384974300861
iteration 104, loss = 0.0035079186782240868
iteration 105, loss = 0.0036223812494426966
iteration 106, loss = 0.004425313789397478
iteration 107, loss = 0.0042619649320840836
iteration 108, loss = 0.004151638597249985
iteration 109, loss = 0.004289555829018354
iteration 110, loss = 0.004364717751741409
iteration 111, loss = 0.003932211548089981
iteration 112, loss = 0.0061141750775277615
iteration 113, loss = 0.0038630168419331312
iteration 114, loss = 0.0036217868328094482
iteration 115, loss = 0.004251972306519747
iteration 116, loss = 0.0039971619844436646
iteration 117, loss = 0.00355911860242486
iteration 118, loss = 0.00363543420098722
iteration 119, loss = 0.00519728846848011
iteration 120, loss = 0.004224407020956278
iteration 121, loss = 0.0036488377954810858
iteration 122, loss = 0.0034757547546178102
iteration 123, loss = 0.004324485547840595
iteration 124, loss = 0.003707651514559984
iteration 125, loss = 0.004197199363261461
iteration 126, loss = 0.003439018502831459
iteration 127, loss = 0.004366790410131216
iteration 128, loss = 0.004212606698274612
iteration 129, loss = 0.003617088543251157
iteration 130, loss = 0.004250236786901951
iteration 131, loss = 0.003583053592592478
iteration 132, loss = 0.004899146966636181
iteration 133, loss = 0.0038791741244494915
iteration 134, loss = 0.003983152564615011
iteration 135, loss = 0.004371065646409988
iteration 136, loss = 0.004034341778606176
iteration 137, loss = 0.003418695880100131
iteration 138, loss = 0.01224544458091259
iteration 139, loss = 0.004416113719344139
iteration 140, loss = 0.004162840079516172
iteration 141, loss = 0.00351973925717175
iteration 142, loss = 0.005812329240143299
iteration 143, loss = 0.004615516867488623
iteration 144, loss = 0.004974233452230692
iteration 145, loss = 0.0045638964511454105
iteration 146, loss = 0.0037213528994470835
iteration 147, loss = 0.004274911247193813
iteration 148, loss = 0.00488373963162303
iteration 149, loss = 0.003986048512160778
iteration 150, loss = 0.0037903052289038897
iteration 151, loss = 0.003590758191421628
iteration 152, loss = 0.0040529100224375725
iteration 153, loss = 0.004005851689726114
iteration 154, loss = 0.0037876858841627836
iteration 155, loss = 0.0036480866838246584
iteration 156, loss = 0.009985788725316525
iteration 157, loss = 0.003540630917996168
iteration 158, loss = 0.003640672191977501
iteration 159, loss = 0.0036476259119808674
iteration 160, loss = 0.0036214350257068872
iteration 161, loss = 0.003586075035855174
iteration 162, loss = 0.004683257546275854
iteration 163, loss = 0.004166209604591131
iteration 164, loss = 0.005961481016129255
iteration 165, loss = 0.003433375619351864
iteration 166, loss = 0.0036634081043303013
iteration 167, loss = 0.010165056213736534
iteration 168, loss = 0.00370571156963706
iteration 169, loss = 0.004184760618954897
iteration 170, loss = 0.004093134310096502
iteration 171, loss = 0.005175180267542601
iteration 172, loss = 0.003978922497481108
iteration 173, loss = 0.005237560253590345
iteration 174, loss = 0.00429275119677186
iteration 175, loss = 0.0033275086898356676
iteration 176, loss = 0.003919981885701418
iteration 177, loss = 0.005683509167283773
iteration 178, loss = 0.0041695875115692616
iteration 179, loss = 0.0035637083929032087
iteration 180, loss = 0.004253046587109566
iteration 181, loss = 0.0038306345231831074
iteration 182, loss = 0.0036807842552661896
iteration 183, loss = 0.005100263748317957
iteration 184, loss = 0.0037810003850609064
iteration 185, loss = 0.004180353134870529
iteration 186, loss = 0.01600473001599312
iteration 187, loss = 0.003397210268303752
iteration 188, loss = 0.0037576823960989714
iteration 189, loss = 0.004091493785381317
iteration 190, loss = 0.0035744134802371264
iteration 191, loss = 0.0039714183658361435
iteration 192, loss = 0.0036002600099891424
iteration 193, loss = 0.0065700095146894455
iteration 194, loss = 0.003549026558175683
iteration 195, loss = 0.004245314747095108
iteration 196, loss = 0.005912933498620987
iteration 197, loss = 0.0036297279875725508
iteration 198, loss = 0.003391323843970895
iteration 199, loss = 0.009758378379046917
iteration 200, loss = 0.004793411120772362
iteration 201, loss = 0.0035104304552078247
iteration 202, loss = 0.0035681119188666344
iteration 203, loss = 0.003909270279109478
iteration 204, loss = 0.004517965484410524
iteration 205, loss = 0.003656746819615364
iteration 206, loss = 0.0038838812615722418
iteration 207, loss = 0.003921261988580227
iteration 208, loss = 0.003932199440896511
iteration 209, loss = 0.0040559545159339905
iteration 210, loss = 0.005818253383040428
iteration 211, loss = 0.003588056191802025
iteration 212, loss = 0.004290730692446232
iteration 213, loss = 0.0034698552917689085
iteration 214, loss = 0.003677587490528822
iteration 215, loss = 0.0037030340172350407
iteration 216, loss = 0.003777919104322791
iteration 217, loss = 0.00400218740105629
iteration 218, loss = 0.003612685948610306
iteration 219, loss = 0.006426640786230564
iteration 220, loss = 0.0035663540475070477
iteration 221, loss = 0.0035174053627997637
iteration 222, loss = 0.0037787146866321564
iteration 223, loss = 0.0038112110923975706
iteration 224, loss = 0.0034677565563470125
iteration 225, loss = 0.003988421056419611
iteration 226, loss = 0.0036022006534039974
iteration 227, loss = 0.003534683957695961
iteration 228, loss = 0.003896094625815749
iteration 229, loss = 0.00928631704300642
iteration 230, loss = 0.003604982513934374
iteration 231, loss = 0.005571104120463133
iteration 232, loss = 0.009768166579306126
iteration 233, loss = 0.004188894759863615
iteration 234, loss = 0.004979370161890984
iteration 235, loss = 0.003500419668853283
iteration 236, loss = 0.003781413659453392
iteration 237, loss = 0.003720400622114539
iteration 238, loss = 0.006129666697233915
iteration 239, loss = 0.0035362481139600277
iteration 240, loss = 0.005403484217822552
iteration 241, loss = 0.0042716627940535545
iteration 242, loss = 0.0042616999708116055
iteration 243, loss = 0.0039628902450203896
iteration 244, loss = 0.0037435349076986313
iteration 245, loss = 0.0035234822425991297
iteration 246, loss = 0.003950165119022131
iteration 247, loss = 0.003756317775696516
iteration 248, loss = 0.003518575569614768
iteration 249, loss = 0.0035371289122849703
iteration 250, loss = 0.0037693448830395937
iteration 251, loss = 0.00407444266602397
iteration 252, loss = 0.003936039283871651
iteration 253, loss = 0.003739316016435623
iteration 254, loss = 0.0039165872149169445
iteration 255, loss = 0.003965626936405897
iteration 256, loss = 0.003739266889169812
iteration 257, loss = 0.003483225591480732
iteration 258, loss = 0.0036015796940773726
iteration 259, loss = 0.0037200599908828735
iteration 260, loss = 0.004007437266409397
iteration 261, loss = 0.0037658053915947676
iteration 262, loss = 0.009229465387761593
iteration 263, loss = 0.005145497620105743
iteration 264, loss = 0.0043127043172717094
iteration 265, loss = 0.005436254199594259
iteration 266, loss = 0.0034768583718687296
iteration 267, loss = 0.01523568294942379
iteration 268, loss = 0.003531019203364849
iteration 269, loss = 0.009366358630359173
iteration 270, loss = 0.003638070309534669
iteration 271, loss = 0.003527834080159664
iteration 272, loss = 0.003795338561758399
iteration 273, loss = 0.004031343385577202
iteration 274, loss = 0.0033849531318992376
iteration 275, loss = 0.009823694825172424
iteration 276, loss = 0.0036688409745693207
iteration 277, loss = 0.0035254815593361855
iteration 278, loss = 0.0034558544866740704
iteration 279, loss = 0.005906084086745977
iteration 280, loss = 0.004040393978357315
iteration 281, loss = 0.0036583514884114265
iteration 282, loss = 0.00363901793025434
iteration 283, loss = 0.0037764243315905333
iteration 284, loss = 0.003957921173423529
iteration 285, loss = 0.0034042787738144398
iteration 286, loss = 0.004021089058369398
iteration 287, loss = 0.0038169624749571085
iteration 288, loss = 0.0032090593595057726
iteration 289, loss = 0.0036034712102264166
iteration 290, loss = 0.003206280991435051
iteration 291, loss = 0.00329623743891716
iteration 292, loss = 0.003573112189769745
iteration 293, loss = 0.003846951061859727
iteration 294, loss = 0.0032174086663872004
iteration 295, loss = 0.0036052418872714043
iteration 296, loss = 0.010841812938451767
iteration 297, loss = 0.0031625027768313885
iteration 298, loss = 0.003556764917448163
iteration 299, loss = 0.00349363312125206
iteration 0, loss = 0.003373201936483383
iteration 1, loss = 0.005007514730095863
iteration 2, loss = 0.003464682260528207
iteration 3, loss = 0.003394675673916936
iteration 4, loss = 0.0036052903160452843
iteration 5, loss = 0.003945894539356232
iteration 6, loss = 0.003534684656187892
iteration 7, loss = 0.003385834861546755
iteration 8, loss = 0.0056147282011806965
iteration 9, loss = 0.003377300687134266
iteration 10, loss = 0.0036861877888441086
iteration 11, loss = 0.0035380127374082804
iteration 12, loss = 0.0033945185132324696
iteration 13, loss = 0.003374418942257762
iteration 14, loss = 0.003628009231761098
iteration 15, loss = 0.004404844716191292
iteration 16, loss = 0.009175869636237621
iteration 17, loss = 0.005987988784909248
iteration 18, loss = 0.005853528156876564
iteration 19, loss = 0.003192700445652008
iteration 20, loss = 0.0033864807337522507
iteration 21, loss = 0.0058780754916369915
iteration 22, loss = 0.009358346462249756
iteration 23, loss = 0.0034086573868989944
iteration 24, loss = 0.006353558506816626
iteration 25, loss = 0.004889912903308868
iteration 26, loss = 0.003720649750903249
iteration 27, loss = 0.0034251855686306953
iteration 28, loss = 0.003274971153587103
iteration 29, loss = 0.003423016518354416
iteration 30, loss = 0.004386599641293287
iteration 31, loss = 0.0033277757465839386
iteration 32, loss = 0.004494331311434507
iteration 33, loss = 0.0034075984731316566
iteration 34, loss = 0.003565853228792548
iteration 35, loss = 0.003734963946044445
iteration 36, loss = 0.0041478583589196205
iteration 37, loss = 0.003601125441491604
iteration 38, loss = 0.004172260873019695
iteration 39, loss = 0.0033159980084747076
iteration 40, loss = 0.004063372500240803
iteration 41, loss = 0.0037423213943839073
iteration 42, loss = 0.0032559800893068314
iteration 43, loss = 0.0036602281033992767
iteration 44, loss = 0.004216322675347328
iteration 45, loss = 0.003955715335905552
iteration 46, loss = 0.0037755470257252455
iteration 47, loss = 0.003070338163524866
iteration 48, loss = 0.004909058101475239
iteration 49, loss = 0.0038461661897599697
iteration 50, loss = 0.003584417514503002
iteration 51, loss = 0.00404489366337657
iteration 52, loss = 0.0035717796999961138
iteration 53, loss = 0.003479515668004751
iteration 54, loss = 0.0033166315406560898
iteration 55, loss = 0.003574273083359003
iteration 56, loss = 0.0035735787823796272
iteration 57, loss = 0.00334303081035614
iteration 58, loss = 0.003236368764191866
iteration 59, loss = 0.0032468182034790516
iteration 60, loss = 0.00880675483494997
iteration 61, loss = 0.0037548148538917303
iteration 62, loss = 0.005393689498305321
iteration 63, loss = 0.003307117149233818
iteration 64, loss = 0.0034676771610975266
iteration 65, loss = 0.003604658879339695
iteration 66, loss = 0.003906828351318836
iteration 67, loss = 0.0039051026105880737
iteration 68, loss = 0.0033111681696027517
iteration 69, loss = 0.0036882050335407257
iteration 70, loss = 0.0032930776942521334
iteration 71, loss = 0.0034072143025696278
iteration 72, loss = 0.003291886765509844
iteration 73, loss = 0.003309373278170824
iteration 74, loss = 0.0033288169652223587
iteration 75, loss = 0.005413524340838194
iteration 76, loss = 0.003448733827099204
iteration 77, loss = 0.003403914626687765
iteration 78, loss = 0.003580880118533969
iteration 79, loss = 0.004234162159264088
iteration 80, loss = 0.004304688423871994
iteration 81, loss = 0.0034931947011500597
iteration 82, loss = 0.004972982686012983
iteration 83, loss = 0.0036850653123110533
iteration 84, loss = 0.008761494420468807
iteration 85, loss = 0.003281963989138603
iteration 86, loss = 0.0032825011294335127
iteration 87, loss = 0.0030496101826429367
iteration 88, loss = 0.003441247157752514
iteration 89, loss = 0.003988486714661121
iteration 90, loss = 0.008707646280527115
iteration 91, loss = 0.003713081358000636
iteration 92, loss = 0.00372287817299366
iteration 93, loss = 0.0036314292810857296
iteration 94, loss = 0.003566924948245287
iteration 95, loss = 0.0031409759540110826
iteration 96, loss = 0.0030298607889562845
iteration 97, loss = 0.0037003401666879654
iteration 98, loss = 0.0034082354977726936
iteration 99, loss = 0.0031876307912170887
iteration 100, loss = 0.003707903204485774
iteration 101, loss = 0.0033511423971503973
iteration 102, loss = 0.0034673823975026608
iteration 103, loss = 0.0036157979629933834
iteration 104, loss = 0.003210053313523531
iteration 105, loss = 0.00395224429666996
iteration 106, loss = 0.003430259646847844
iteration 107, loss = 0.003506750799715519
iteration 108, loss = 0.004167975392192602
iteration 109, loss = 0.0036901826970279217
iteration 110, loss = 0.0032181725837290287
iteration 111, loss = 0.003323630429804325
iteration 112, loss = 0.003741104854270816
iteration 113, loss = 0.003997869789600372
iteration 114, loss = 0.0045995404943823814
iteration 115, loss = 0.0031197285279631615
iteration 116, loss = 0.008806119672954082
iteration 117, loss = 0.008634855970740318
iteration 118, loss = 0.0034900063183158636
iteration 119, loss = 0.0031373286619782448
iteration 120, loss = 0.0031395764090120792
iteration 121, loss = 0.003761033061891794
iteration 122, loss = 0.0032592008356004953
iteration 123, loss = 0.0032967831939458847
iteration 124, loss = 0.005467704497277737
iteration 125, loss = 0.005568626336753368
iteration 126, loss = 0.003426742972806096
iteration 127, loss = 0.0036450938787311316
iteration 128, loss = 0.0038286687340587378
iteration 129, loss = 0.003439107909798622
iteration 130, loss = 0.003119654720649123
iteration 131, loss = 0.008479654788970947
iteration 132, loss = 0.00863330252468586
iteration 133, loss = 0.003636114066466689
iteration 134, loss = 0.0033522325102239847
iteration 135, loss = 0.0054536182433366776
iteration 136, loss = 0.0032582369167357683
iteration 137, loss = 0.0036218471359461546
iteration 138, loss = 0.0031528309918940067
iteration 139, loss = 0.0031347002368420362
iteration 140, loss = 0.003738257335498929
iteration 141, loss = 0.005105092190206051
iteration 142, loss = 0.004938211292028427
iteration 143, loss = 0.004669894929975271
iteration 144, loss = 0.0034044228959828615
iteration 145, loss = 0.0033423679415136576
iteration 146, loss = 0.004612477961927652
iteration 147, loss = 0.00337146851234138
iteration 148, loss = 0.0034403768368065357
iteration 149, loss = 0.0032460808288306
iteration 150, loss = 0.0030754508916288614
iteration 151, loss = 0.003279553959146142
iteration 152, loss = 0.003686363808810711
iteration 153, loss = 0.0032311510294675827
iteration 154, loss = 0.0034817014820873737
iteration 155, loss = 0.0032247519120573997
iteration 156, loss = 0.0034245499409735203
iteration 157, loss = 0.003099986119195819
iteration 158, loss = 0.003483494045212865
iteration 159, loss = 0.003434254787862301
iteration 160, loss = 0.003782933810725808
iteration 161, loss = 0.003097573295235634
iteration 162, loss = 0.00301571493037045
iteration 163, loss = 0.003271879628300667
iteration 164, loss = 0.0037258912343531847
iteration 165, loss = 0.008567972108721733
iteration 166, loss = 0.0031751682981848717
iteration 167, loss = 0.0035080963280051947
iteration 168, loss = 0.003114094492048025
iteration 169, loss = 0.0034036997240036726
iteration 170, loss = 0.003458829130977392
iteration 171, loss = 0.003003411926329136
iteration 172, loss = 0.0031792062800377607
iteration 173, loss = 0.0037877727299928665
iteration 174, loss = 0.003324700053781271
iteration 175, loss = 0.003179455641657114
iteration 176, loss = 0.0034963241778314114
iteration 177, loss = 0.003522339276969433
iteration 178, loss = 0.00337228667922318
iteration 179, loss = 0.00352679705247283
iteration 180, loss = 0.004258003551512957
iteration 181, loss = 0.0043763648718595505
iteration 182, loss = 0.008546678349375725
iteration 183, loss = 0.003228127956390381
iteration 184, loss = 0.0031498121097683907
iteration 185, loss = 0.003183088032528758
iteration 186, loss = 0.004428142216056585
iteration 187, loss = 0.003215847536921501
iteration 188, loss = 0.004282057285308838
iteration 189, loss = 0.0036444358993321657
iteration 190, loss = 0.008400849997997284
iteration 191, loss = 0.005353641230612993
iteration 192, loss = 0.0031424241606146097
iteration 193, loss = 0.0034917639568448067
iteration 194, loss = 0.0036729725543409586
iteration 195, loss = 0.003157757455483079
iteration 196, loss = 0.0031420541927218437
iteration 197, loss = 0.003761168336495757
iteration 198, loss = 0.003136960556730628
iteration 199, loss = 0.0034960145130753517
iteration 200, loss = 0.003165215253829956
iteration 201, loss = 0.005096425302326679
iteration 202, loss = 0.003007009858265519
iteration 203, loss = 0.005318653304129839
iteration 204, loss = 0.003448947099968791
iteration 205, loss = 0.004603577312082052
iteration 206, loss = 0.0030747773125767708
iteration 207, loss = 0.003209706163033843
iteration 208, loss = 0.0031390576623380184
iteration 209, loss = 0.003092002822086215
iteration 210, loss = 0.008305839262902737
iteration 211, loss = 0.0029041990637779236
iteration 212, loss = 0.0034802365116775036
iteration 213, loss = 0.003148401156067848
iteration 214, loss = 0.003426537150517106
iteration 215, loss = 0.003830802859738469
iteration 216, loss = 0.00310332840308547
iteration 217, loss = 0.004055467899888754
iteration 218, loss = 0.0035005942918360233
iteration 219, loss = 0.0030876800883561373
iteration 220, loss = 0.0043497332371771336
iteration 221, loss = 0.0034207438584417105
iteration 222, loss = 0.002983362413942814
iteration 223, loss = 0.004344598855823278
iteration 224, loss = 0.0034306347370147705
iteration 225, loss = 0.003584357211366296
iteration 226, loss = 0.003487259615212679
iteration 227, loss = 0.0032110470347106457
iteration 228, loss = 0.004018239211291075
iteration 229, loss = 0.0031646685674786568
iteration 230, loss = 0.0029366768430918455
iteration 231, loss = 0.0033043755684047937
iteration 232, loss = 0.004786630626767874
iteration 233, loss = 0.0032114090863615274
iteration 234, loss = 0.0031163236126303673
iteration 235, loss = 0.0030525941401720047
iteration 236, loss = 0.0030366801656782627
iteration 237, loss = 0.003097602864727378
iteration 238, loss = 0.003347216872498393
iteration 239, loss = 0.008469168096780777
iteration 240, loss = 0.004509557969868183
iteration 241, loss = 0.0031474875286221504
iteration 242, loss = 0.0034063642378896475
iteration 243, loss = 0.0037614000029861927
iteration 244, loss = 0.0038653300143778324
iteration 245, loss = 0.003007226623594761
iteration 246, loss = 0.0032657627016305923
iteration 247, loss = 0.004274744540452957
iteration 248, loss = 0.004477262031286955
iteration 249, loss = 0.0032742067705839872
iteration 250, loss = 0.003855905495584011
iteration 251, loss = 0.005094252992421389
iteration 252, loss = 0.0030773046892136335
iteration 253, loss = 0.009015069343149662
iteration 254, loss = 0.0035783452913165092
iteration 255, loss = 0.003982660826295614
iteration 256, loss = 0.004120535682886839
iteration 257, loss = 0.0029738370794802904
iteration 258, loss = 0.00338974641636014
iteration 259, loss = 0.003290430875495076
iteration 260, loss = 0.0030426534358412027
iteration 261, loss = 0.00506874592974782
iteration 262, loss = 0.0029392861761152744
iteration 263, loss = 0.0036103602033108473
iteration 264, loss = 0.003414396895095706
iteration 265, loss = 0.003495745826512575
iteration 266, loss = 0.0036488017067313194
iteration 267, loss = 0.0028648078441619873
iteration 268, loss = 0.002940366044640541
iteration 269, loss = 0.0034004184417426586
iteration 270, loss = 0.0036097061820328236
iteration 271, loss = 0.0030741191003471613
iteration 272, loss = 0.0036568385548889637
iteration 273, loss = 0.004327230155467987
iteration 274, loss = 0.003346390090882778
iteration 275, loss = 0.005101208575069904
iteration 276, loss = 0.003313700668513775
iteration 277, loss = 0.0034137440379709005
iteration 278, loss = 0.006290605757385492
iteration 279, loss = 0.002894908655434847
iteration 280, loss = 0.003719551023095846
iteration 281, loss = 0.003078964538872242
iteration 282, loss = 0.0032236636616289616
iteration 283, loss = 0.0033157169818878174
iteration 284, loss = 0.0031281013507395983
iteration 285, loss = 0.0050687226466834545
iteration 286, loss = 0.004124213941395283
iteration 287, loss = 0.0032500007655471563
iteration 288, loss = 0.002881387947127223
iteration 289, loss = 0.0030160227324813604
iteration 290, loss = 0.0036515516694635153
iteration 291, loss = 0.004133203998208046
iteration 292, loss = 0.003919645678251982
iteration 293, loss = 0.002831787336617708
iteration 294, loss = 0.0032757094595581293
iteration 295, loss = 0.003048934042453766
iteration 296, loss = 0.003940390422940254
iteration 297, loss = 0.003278806572780013
iteration 298, loss = 0.0032211514189839363
iteration 299, loss = 0.003050633240491152
iteration 0, loss = 0.002887832000851631
iteration 1, loss = 0.0029903335962444544
iteration 2, loss = 0.0032362190540879965
iteration 3, loss = 0.0033607780933380127
iteration 4, loss = 0.005185988731682301
iteration 5, loss = 0.003122134367004037
iteration 6, loss = 0.003078293288126588
iteration 7, loss = 0.003360035829246044
iteration 8, loss = 0.0030844416469335556
iteration 9, loss = 0.004457964561879635
iteration 10, loss = 0.003389365505427122
iteration 11, loss = 0.0034755293745547533
iteration 12, loss = 0.004198060836642981
iteration 13, loss = 0.008222485892474651
iteration 14, loss = 0.0029681941960006952
iteration 15, loss = 0.002841750392690301
iteration 16, loss = 0.004724464379251003
iteration 17, loss = 0.0033298120833933353
iteration 18, loss = 0.0032481993548572063
iteration 19, loss = 0.0032490547746419907
iteration 20, loss = 0.0035201332066208124
iteration 21, loss = 0.004908010829240084
iteration 22, loss = 0.0029473674949258566
iteration 23, loss = 0.003618089947849512
iteration 24, loss = 0.002941126236692071
iteration 25, loss = 0.0029858797788619995
iteration 26, loss = 0.003212045645341277
iteration 27, loss = 0.0031612564343959093
iteration 28, loss = 0.0029353718273341656
iteration 29, loss = 0.005727474577724934
iteration 30, loss = 0.003330579260364175
iteration 31, loss = 0.0033548197243362665
iteration 32, loss = 0.004789768718183041
iteration 33, loss = 0.004262157250195742
iteration 34, loss = 0.0031329665798693895
iteration 35, loss = 0.004785891156643629
iteration 36, loss = 0.0035368020180612803
iteration 37, loss = 0.0032066793646663427
iteration 38, loss = 0.003749675350263715
iteration 39, loss = 0.003847918938845396
iteration 40, loss = 0.0032933081965893507
iteration 41, loss = 0.00303017720580101
iteration 42, loss = 0.0029200047720223665
iteration 43, loss = 0.0036369874142110348
iteration 44, loss = 0.00537788774818182
iteration 45, loss = 0.003171004820615053
iteration 46, loss = 0.003472710959613323
iteration 47, loss = 0.003287050873041153
iteration 48, loss = 0.003009989857673645
iteration 49, loss = 0.003214059630408883
iteration 50, loss = 0.0029620917048305273
iteration 51, loss = 0.0034180437214672565
iteration 52, loss = 0.0027871718630194664
iteration 53, loss = 0.003508364548906684
iteration 54, loss = 0.00294169457629323
iteration 55, loss = 0.002948360051959753
iteration 56, loss = 0.0033190129324793816
iteration 57, loss = 0.002938468474894762
iteration 58, loss = 0.004131827503442764
iteration 59, loss = 0.0030943523161113262
iteration 60, loss = 0.0032159173861145973
iteration 61, loss = 0.0030096631962805986
iteration 62, loss = 0.002809614408761263
iteration 63, loss = 0.007897844538092613
iteration 64, loss = 0.003425172995775938
iteration 65, loss = 0.003012035507708788
iteration 66, loss = 0.004129774868488312
iteration 67, loss = 0.004757151007652283
iteration 68, loss = 0.003228274406865239
iteration 69, loss = 0.002836637431755662
iteration 70, loss = 0.0036713972222059965
iteration 71, loss = 0.0028201998211443424
iteration 72, loss = 0.004772561602294445
iteration 73, loss = 0.0036711962893605232
iteration 74, loss = 0.0042311204597353935
iteration 75, loss = 0.003558530006557703
iteration 76, loss = 0.00807979330420494
iteration 77, loss = 0.004400416277348995
iteration 78, loss = 0.002971139270812273
iteration 79, loss = 0.00300600822083652
iteration 80, loss = 0.0032835009042173624
iteration 81, loss = 0.008018750697374344
iteration 82, loss = 0.002944732317700982
iteration 83, loss = 0.0029881016816943884
iteration 84, loss = 0.0036215968430042267
iteration 85, loss = 0.003737360704690218
iteration 86, loss = 0.0030802979599684477
iteration 87, loss = 0.0028796170372515917
iteration 88, loss = 0.0035435424651950598
iteration 89, loss = 0.002980833640322089
iteration 90, loss = 0.0034591483417898417
iteration 91, loss = 0.002821254776790738
iteration 92, loss = 0.0036395052447915077
iteration 93, loss = 0.003423725487664342
iteration 94, loss = 0.0032923296093940735
iteration 95, loss = 0.004206627607345581
iteration 96, loss = 0.0037731004413217306
iteration 97, loss = 0.0027985558845102787
iteration 98, loss = 0.0030184139031916857
iteration 99, loss = 0.002815593034029007
iteration 100, loss = 0.003394694998860359
iteration 101, loss = 0.003408571472391486
iteration 102, loss = 0.0028047722298651934
iteration 103, loss = 0.0028167287819087505
iteration 104, loss = 0.00404375558719039
iteration 105, loss = 0.004375079646706581
iteration 106, loss = 0.0028065978549420834
iteration 107, loss = 0.002945758868008852
iteration 108, loss = 0.0033743358217179775
iteration 109, loss = 0.002937533427029848
iteration 110, loss = 0.0032763078343123198
iteration 111, loss = 0.0038794022984802723
iteration 112, loss = 0.0030801843386143446
iteration 113, loss = 0.004574790596961975
iteration 114, loss = 0.0036579493898898363
iteration 115, loss = 0.0028305891901254654
iteration 116, loss = 0.0035225844476372004
iteration 117, loss = 0.0029662579763680696
iteration 118, loss = 0.007860872894525528
iteration 119, loss = 0.003302711993455887
iteration 120, loss = 0.004301423206925392
iteration 121, loss = 0.003281425219029188
iteration 122, loss = 0.002992775058373809
iteration 123, loss = 0.0028510745614767075
iteration 124, loss = 0.003067860845476389
iteration 125, loss = 0.003952446393668652
iteration 126, loss = 0.002770179882645607
iteration 127, loss = 0.004855774343013763
iteration 128, loss = 0.0029089725576341152
iteration 129, loss = 0.002643256215378642
iteration 130, loss = 0.004591474775224924
iteration 131, loss = 0.0037502089980989695
iteration 132, loss = 0.003166656708344817
iteration 133, loss = 0.0030565091874450445
iteration 134, loss = 0.002843134803697467
iteration 135, loss = 0.002699390286579728
iteration 136, loss = 0.0035051763989031315
iteration 137, loss = 0.0026563284918665886
iteration 138, loss = 0.003196499077603221
iteration 139, loss = 0.0027963523752987385
iteration 140, loss = 0.0030665446538478136
iteration 141, loss = 0.004463936202228069
iteration 142, loss = 0.002536341082304716
iteration 143, loss = 0.002777545480057597
iteration 144, loss = 0.0030538274440914392
iteration 145, loss = 0.003105847630649805
iteration 146, loss = 0.0034120678901672363
iteration 147, loss = 0.0028806724585592747
iteration 148, loss = 0.0046090297400951385
iteration 149, loss = 0.00773591548204422
iteration 150, loss = 0.0030673814471811056
iteration 151, loss = 0.0027002247516065836
iteration 152, loss = 0.007580260746181011
iteration 153, loss = 0.0032504627015441656
iteration 154, loss = 0.0029175959061831236
iteration 155, loss = 0.003231095615774393
iteration 156, loss = 0.003127762582153082
iteration 157, loss = 0.0031262326519936323
iteration 158, loss = 0.003404837567359209
iteration 159, loss = 0.003407021053135395
iteration 160, loss = 0.003171152900904417
iteration 161, loss = 0.0027046017348766327
iteration 162, loss = 0.0030945809558033943
iteration 163, loss = 0.002664882456883788
iteration 164, loss = 0.003181920852512121
iteration 165, loss = 0.002815346233546734
iteration 166, loss = 0.0031775259412825108
iteration 167, loss = 0.0026435269974172115
iteration 168, loss = 0.003050421830266714
iteration 169, loss = 0.002968419110402465
iteration 170, loss = 0.007685057353228331
iteration 171, loss = 0.003259029472246766
iteration 172, loss = 0.002752195578068495
iteration 173, loss = 0.002750530606135726
iteration 174, loss = 0.0029624439775943756
iteration 175, loss = 0.0026538027450442314
iteration 176, loss = 0.003220311366021633
iteration 177, loss = 0.004542385693639517
iteration 178, loss = 0.0030138837173581123
iteration 179, loss = 0.0032351266127079725
iteration 180, loss = 0.00267286179587245
iteration 181, loss = 0.003014293732121587
iteration 182, loss = 0.003053640481084585
iteration 183, loss = 0.002949911868199706
iteration 184, loss = 0.003324315184727311
iteration 185, loss = 0.003096839878708124
iteration 186, loss = 0.0028165492694824934
iteration 187, loss = 0.0032212520018219948
iteration 188, loss = 0.0029355757869780064
iteration 189, loss = 0.003101389156654477
iteration 190, loss = 0.0027293937746435404
iteration 191, loss = 0.0029049632139503956
iteration 192, loss = 0.003831665264442563
iteration 193, loss = 0.002864386420696974
iteration 194, loss = 0.004586333408951759
iteration 195, loss = 0.0031085715163499117
iteration 196, loss = 0.0027815597131848335
iteration 197, loss = 0.003524286672472954
iteration 198, loss = 0.0027963966131210327
iteration 199, loss = 0.0028165914118289948
iteration 200, loss = 0.0028732619248330593
iteration 201, loss = 0.0032930541783571243
iteration 202, loss = 0.0031712085474282503
iteration 203, loss = 0.0029313007835298777
iteration 204, loss = 0.002795642241835594
iteration 205, loss = 0.008069602772593498
iteration 206, loss = 0.0026774343568831682
iteration 207, loss = 0.0028190878219902515
iteration 208, loss = 0.0031470234971493483
iteration 209, loss = 0.003126650582998991
iteration 210, loss = 0.003027180675417185
iteration 211, loss = 0.003168542170897126
iteration 212, loss = 0.0033304698299616575
iteration 213, loss = 0.003177361097186804
iteration 214, loss = 0.003210993018001318
iteration 215, loss = 0.007468249183148146
iteration 216, loss = 0.007678363937884569
iteration 217, loss = 0.0027880899142473936
iteration 218, loss = 0.002716153860092163
iteration 219, loss = 0.0028811481315642595
iteration 220, loss = 0.0032794377766549587
iteration 221, loss = 0.0027347325813025236
iteration 222, loss = 0.002867773175239563
iteration 223, loss = 0.0031799976713955402
iteration 224, loss = 0.0029215810354799032
iteration 225, loss = 0.00275389663875103
iteration 226, loss = 0.002949201501905918
iteration 227, loss = 0.00503758667036891
iteration 228, loss = 0.003144372720271349
iteration 229, loss = 0.003033372573554516
iteration 230, loss = 0.0031249113380908966
iteration 231, loss = 0.0026909916196018457
iteration 232, loss = 0.007872330956161022
iteration 233, loss = 0.0029168883338570595
iteration 234, loss = 0.0031279544346034527
iteration 235, loss = 0.0026565040461719036
iteration 236, loss = 0.002734675072133541
iteration 237, loss = 0.0030855603981763124
iteration 238, loss = 0.003805895335972309
iteration 239, loss = 0.0027747347485274076
iteration 240, loss = 0.00421747425571084
iteration 241, loss = 0.00331894145347178
iteration 242, loss = 0.0027702199295163155
iteration 243, loss = 0.002698594471439719
iteration 244, loss = 0.0028118330519646406
iteration 245, loss = 0.003528024535626173
iteration 246, loss = 0.00807200651615858
iteration 247, loss = 0.002962901722639799
iteration 248, loss = 0.002836621832102537
iteration 249, loss = 0.0030141547322273254
iteration 250, loss = 0.0027030131313949823
iteration 251, loss = 0.0028328218031674623
iteration 252, loss = 0.002636261750012636
iteration 253, loss = 0.003005733247846365
iteration 254, loss = 0.002706906059756875
iteration 255, loss = 0.0032205190509557724
iteration 256, loss = 0.0039051419589668512
iteration 257, loss = 0.003115113126114011
iteration 258, loss = 0.003526056418195367
iteration 259, loss = 0.00368690793402493
iteration 260, loss = 0.0040921401232481
iteration 261, loss = 0.00305555434897542
iteration 262, loss = 0.004346062429249287
iteration 263, loss = 0.0027158036828041077
iteration 264, loss = 0.002752393251284957
iteration 265, loss = 0.002709092339500785
iteration 266, loss = 0.003011389635503292
iteration 267, loss = 0.0026544316206127405
iteration 268, loss = 0.002738998970016837
iteration 269, loss = 0.007840625941753387
iteration 270, loss = 0.003052802523598075
iteration 271, loss = 0.0027022480498999357
iteration 272, loss = 0.0032010513823479414
iteration 273, loss = 0.00459707248955965
iteration 274, loss = 0.0025989804416894913
iteration 275, loss = 0.002795320237055421
iteration 276, loss = 0.0030791317112743855
iteration 277, loss = 0.0026873985771089792
iteration 278, loss = 0.0027678480837494135
iteration 279, loss = 0.0028370684012770653
iteration 280, loss = 0.002681958954781294
iteration 281, loss = 0.002933911979198456
iteration 282, loss = 0.002962762489914894
iteration 283, loss = 0.00326702743768692
iteration 284, loss = 0.003776038996875286
iteration 285, loss = 0.0025732696522027254
iteration 286, loss = 0.0027731037698686123
iteration 287, loss = 0.003102492308244109
iteration 288, loss = 0.002790160244330764
iteration 289, loss = 0.0030377786606550217
iteration 290, loss = 0.003717015730217099
iteration 291, loss = 0.0029083003755658865
iteration 292, loss = 0.003753697732463479
iteration 293, loss = 0.007149709388613701
iteration 294, loss = 0.002919563790783286
iteration 295, loss = 0.0027838076930493116
iteration 296, loss = 0.002647161250934005
iteration 297, loss = 0.0038699847646057606
iteration 298, loss = 0.0027376413345336914
iteration 299, loss = 0.00280487397685647
iteration 0, loss = 0.002762698568403721
iteration 1, loss = 0.0043532527051866055
iteration 2, loss = 0.00266883778385818
iteration 3, loss = 0.0033471756614744663
iteration 4, loss = 0.0029649175703525543
iteration 5, loss = 0.0029369695112109184
iteration 6, loss = 0.00573754170909524
iteration 7, loss = 0.0030335586052387953
iteration 8, loss = 0.0036106184124946594
iteration 9, loss = 0.0034094289876520634
iteration 10, loss = 0.003058983478695154
iteration 11, loss = 0.0025314337108284235
iteration 12, loss = 0.002678417367860675
iteration 13, loss = 0.0025981408543884754
iteration 14, loss = 0.003532737260684371
iteration 15, loss = 0.002716373885050416
iteration 16, loss = 0.002785835415124893
iteration 17, loss = 0.003236448857933283
iteration 18, loss = 0.0027652231510728598
iteration 19, loss = 0.003170579206198454
iteration 20, loss = 0.004780120216310024
iteration 21, loss = 0.0028891735710203648
iteration 22, loss = 0.002890172880142927
iteration 23, loss = 0.0025762205477803946
iteration 24, loss = 0.002633826807141304
iteration 25, loss = 0.00268575013615191
iteration 26, loss = 0.0026477910578250885
iteration 27, loss = 0.0031078343745321035
iteration 28, loss = 0.003335363697260618
iteration 29, loss = 0.002696059877052903
iteration 30, loss = 0.003393998369574547
iteration 31, loss = 0.0026101700495928526
iteration 32, loss = 0.007288296706974506
iteration 33, loss = 0.002817180473357439
iteration 34, loss = 0.0029149840120226145
iteration 35, loss = 0.0038629171904176474
iteration 36, loss = 0.0030572577379643917
iteration 37, loss = 0.0027959065046161413
iteration 38, loss = 0.002901107771322131
iteration 39, loss = 0.002665916457772255
iteration 40, loss = 0.0025667655281722546
iteration 41, loss = 0.002643792424350977
iteration 42, loss = 0.0026236046105623245
iteration 43, loss = 0.003147340379655361
iteration 44, loss = 0.0028283363208174706
iteration 45, loss = 0.002629256807267666
iteration 46, loss = 0.002868060953915119
iteration 47, loss = 0.00317003414966166
iteration 48, loss = 0.0026309865061193705
iteration 49, loss = 0.0074816434644162655
iteration 50, loss = 0.002954388502985239
iteration 51, loss = 0.002858386840671301
iteration 52, loss = 0.002751735271885991
iteration 53, loss = 0.0025705653242766857
iteration 54, loss = 0.0024066269397735596
iteration 55, loss = 0.0034050066024065018
iteration 56, loss = 0.002759062685072422
iteration 57, loss = 0.002767544472590089
iteration 58, loss = 0.002779964590445161
iteration 59, loss = 0.002606729744002223
iteration 60, loss = 0.0029628630727529526
iteration 61, loss = 0.00290493480861187
iteration 62, loss = 0.002636674325913191
iteration 63, loss = 0.003145375521853566
iteration 64, loss = 0.006978385150432587
iteration 65, loss = 0.003532069269567728
iteration 66, loss = 0.0025351408403366804
iteration 67, loss = 0.003508614143356681
iteration 68, loss = 0.004030644427984953
iteration 69, loss = 0.002659010235220194
iteration 70, loss = 0.0027144637424498796
iteration 71, loss = 0.0036990479566156864
iteration 72, loss = 0.0029361150227487087
iteration 73, loss = 0.0027366578578948975
iteration 74, loss = 0.002769235987216234
iteration 75, loss = 0.0043511828407645226
iteration 76, loss = 0.002860375214368105
iteration 77, loss = 0.0026007157284766436
iteration 78, loss = 0.0025376826524734497
iteration 79, loss = 0.0027624652720987797
iteration 80, loss = 0.007316055241972208
iteration 81, loss = 0.0024468053597956896
iteration 82, loss = 0.002588830888271332
iteration 83, loss = 0.002879332285374403
iteration 84, loss = 0.002896328456699848
iteration 85, loss = 0.004322566092014313
iteration 86, loss = 0.003134776372462511
iteration 87, loss = 0.003666619537398219
iteration 88, loss = 0.0025481118354946375
iteration 89, loss = 0.003013210603967309
iteration 90, loss = 0.002511911327019334
iteration 91, loss = 0.002574810292571783
iteration 92, loss = 0.0027145594358444214
iteration 93, loss = 0.002952780807390809
iteration 94, loss = 0.0029729390516877174
iteration 95, loss = 0.0026692349929362535
iteration 96, loss = 0.0029488569125533104
iteration 97, loss = 0.0029252811800688505
iteration 98, loss = 0.004946297500282526
iteration 99, loss = 0.0031126849353313446
iteration 100, loss = 0.004286349751055241
iteration 101, loss = 0.0027162479236721992
iteration 102, loss = 0.0028680176474153996
iteration 103, loss = 0.00253675295971334
iteration 104, loss = 0.0030649485997855663
iteration 105, loss = 0.004730483517050743
iteration 106, loss = 0.0026431893929839134
iteration 107, loss = 0.0024733473546802998
iteration 108, loss = 0.0027643784414976835
iteration 109, loss = 0.002758230548352003
iteration 110, loss = 0.0037548362743109465
iteration 111, loss = 0.0027882156427949667
iteration 112, loss = 0.0028319470584392548
iteration 113, loss = 0.002538711531087756
iteration 114, loss = 0.0030928628984838724
iteration 115, loss = 0.0024576219730079174
iteration 116, loss = 0.002426776336506009
iteration 117, loss = 0.002647246466949582
iteration 118, loss = 0.002805486787110567
iteration 119, loss = 0.0029666919726878405
iteration 120, loss = 0.0025515411980450153
iteration 121, loss = 0.003740111365914345
iteration 122, loss = 0.0035431240685284138
iteration 123, loss = 0.0025587568525224924
iteration 124, loss = 0.003079804591834545
iteration 125, loss = 0.002577662467956543
iteration 126, loss = 0.0026140769477933645
iteration 127, loss = 0.0025524774100631475
iteration 128, loss = 0.007031342480331659
iteration 129, loss = 0.002637708093971014
iteration 130, loss = 0.007123537827283144
iteration 131, loss = 0.0030978189315646887
iteration 132, loss = 0.004304985050112009
iteration 133, loss = 0.0033160836901515722
iteration 134, loss = 0.002794826403260231
iteration 135, loss = 0.0026897042989730835
iteration 136, loss = 0.002534447703510523
iteration 137, loss = 0.007431964389979839
iteration 138, loss = 0.004563395399600267
iteration 139, loss = 0.002789345569908619
iteration 140, loss = 0.0030052976217120886
iteration 141, loss = 0.003102073911577463
iteration 142, loss = 0.0029516732320189476
iteration 143, loss = 0.004175378009676933
iteration 144, loss = 0.0026424622628837824
iteration 145, loss = 0.003470580093562603
iteration 146, loss = 0.00690836226567626
iteration 147, loss = 0.0029737656004726887
iteration 148, loss = 0.0024323223624378443
iteration 149, loss = 0.003735913196578622
iteration 150, loss = 0.0028581605292856693
iteration 151, loss = 0.003908383660018444
iteration 152, loss = 0.002618008991703391
iteration 153, loss = 0.0029429642017930746
iteration 154, loss = 0.0034609034191817045
iteration 155, loss = 0.0024883756414055824
iteration 156, loss = 0.0028725711163133383
iteration 157, loss = 0.0037574677262455225
iteration 158, loss = 0.0026426450349390507
iteration 159, loss = 0.0027197333984076977
iteration 160, loss = 0.0024776190984994173
iteration 161, loss = 0.0027525743935257196
iteration 162, loss = 0.0030609096866101027
iteration 163, loss = 0.0071535902097821236
iteration 164, loss = 0.0028247402515262365
iteration 165, loss = 0.0032838908955454826
iteration 166, loss = 0.002547456417232752
iteration 167, loss = 0.0028371205553412437
iteration 168, loss = 0.0026144557632505894
iteration 169, loss = 0.002830969402566552
iteration 170, loss = 0.0025304495356976986
iteration 171, loss = 0.00247072521597147
iteration 172, loss = 0.00680578825995326
iteration 173, loss = 0.0037625576369464397
iteration 174, loss = 0.004314212594181299
iteration 175, loss = 0.002756695030257106
iteration 176, loss = 0.0032149283215403557
iteration 177, loss = 0.003121813992038369
iteration 178, loss = 0.004239896312355995
iteration 179, loss = 0.0026266067288815975
iteration 180, loss = 0.0026469542644917965
iteration 181, loss = 0.0030070801731199026
iteration 182, loss = 0.0026434874162077904
iteration 183, loss = 0.0027506728656589985
iteration 184, loss = 0.002418101066723466
iteration 185, loss = 0.002490059472620487
iteration 186, loss = 0.002988021820783615
iteration 187, loss = 0.0025263014249503613
iteration 188, loss = 0.002982177073135972
iteration 189, loss = 0.0026607001200318336
iteration 190, loss = 0.0024770768359303474
iteration 191, loss = 0.00252329558134079
iteration 192, loss = 0.003533064853399992
iteration 193, loss = 0.0026942333206534386
iteration 194, loss = 0.0029770329128950834
iteration 195, loss = 0.0025935161393135786
iteration 196, loss = 0.0029572767671197653
iteration 197, loss = 0.002439274685457349
iteration 198, loss = 0.0032652101945132017
iteration 199, loss = 0.0035822014324367046
iteration 200, loss = 0.002841913839802146
iteration 201, loss = 0.006764535792171955
iteration 202, loss = 0.003815438598394394
iteration 203, loss = 0.0026070440653711557
iteration 204, loss = 0.0031222673133015633
iteration 205, loss = 0.0024414376821368933
iteration 206, loss = 0.002481853822246194
iteration 207, loss = 0.0024994569830596447
iteration 208, loss = 0.004312356002628803
iteration 209, loss = 0.002396981231868267
iteration 210, loss = 0.0024739974178373814
iteration 211, loss = 0.0024286939296871424
iteration 212, loss = 0.0029170713387429714
iteration 213, loss = 0.0024588345550000668
iteration 214, loss = 0.0031520547345280647
iteration 215, loss = 0.0028173942118883133
iteration 216, loss = 0.00287130125798285
iteration 217, loss = 0.002720101270824671
iteration 218, loss = 0.00297059235163033
iteration 219, loss = 0.0026917709037661552
iteration 220, loss = 0.0026225841138511896
iteration 221, loss = 0.0028865106869488955
iteration 222, loss = 0.0027819378301501274
iteration 223, loss = 0.002625816036015749
iteration 224, loss = 0.0025093138683587313
iteration 225, loss = 0.0024542445316910744
iteration 226, loss = 0.0027267555706202984
iteration 227, loss = 0.0025964390952140093
iteration 228, loss = 0.0026378785260021687
iteration 229, loss = 0.002455641981214285
iteration 230, loss = 0.0024909742642194033
iteration 231, loss = 0.0024913353845477104
iteration 232, loss = 0.006611588876694441
iteration 233, loss = 0.0027124069165438414
iteration 234, loss = 0.0022920393384993076
iteration 235, loss = 0.002512599341571331
iteration 236, loss = 0.00295893638394773
iteration 237, loss = 0.0026135435327887535
iteration 238, loss = 0.002551177516579628
iteration 239, loss = 0.0028598057106137276
iteration 240, loss = 0.002660251222550869
iteration 241, loss = 0.0025167723651975393
iteration 242, loss = 0.002483674790710211
iteration 243, loss = 0.006771788466721773
iteration 244, loss = 0.0028458985034376383
iteration 245, loss = 0.00272224354557693
iteration 246, loss = 0.0026849813293665648
iteration 247, loss = 0.00258996500633657
iteration 248, loss = 0.002606476191431284
iteration 249, loss = 0.0029638248961418867
iteration 250, loss = 0.002651506569236517
iteration 251, loss = 0.002956029260531068
iteration 252, loss = 0.006757767871022224
iteration 253, loss = 0.002386285923421383
iteration 254, loss = 0.0027337733190506697
iteration 255, loss = 0.002286887029185891
iteration 256, loss = 0.0027391130570322275
iteration 257, loss = 0.0035279260482639074
iteration 258, loss = 0.0025241768453270197
iteration 259, loss = 0.0028703012503683567
iteration 260, loss = 0.002879103645682335
iteration 261, loss = 0.0029248930513858795
iteration 262, loss = 0.002587020630016923
iteration 263, loss = 0.002443879609927535
iteration 264, loss = 0.0028345577884465456
iteration 265, loss = 0.002376966178417206
iteration 266, loss = 0.002618888858705759
iteration 267, loss = 0.0024137599393725395
iteration 268, loss = 0.0030036959797143936
iteration 269, loss = 0.002596798585727811
iteration 270, loss = 0.0028182549867779016
iteration 271, loss = 0.00399826979264617
iteration 272, loss = 0.0026543003041297197
iteration 273, loss = 0.0027150847017765045
iteration 274, loss = 0.0024331663735210896
iteration 275, loss = 0.002360833575949073
iteration 276, loss = 0.002295680111274123
iteration 277, loss = 0.0024086711928248405
iteration 278, loss = 0.0027204870712012053
iteration 279, loss = 0.0023919781669974327
iteration 280, loss = 0.0024223083164542913
iteration 281, loss = 0.0026572709903120995
iteration 282, loss = 0.006726846564561129
iteration 283, loss = 0.002482743700966239
iteration 284, loss = 0.00397103326395154
iteration 285, loss = 0.0025952099822461605
iteration 286, loss = 0.003864555386826396
iteration 287, loss = 0.002983256010338664
iteration 288, loss = 0.0030070736538618803
iteration 289, loss = 0.0026695597916841507
iteration 290, loss = 0.004111252725124359
iteration 291, loss = 0.0025162610691040754
iteration 292, loss = 0.0026801491621881723
iteration 293, loss = 0.003929510712623596
iteration 294, loss = 0.0026738145388662815
iteration 295, loss = 0.0024047535844147205
iteration 296, loss = 0.0027263383381068707
iteration 297, loss = 0.002350169699639082
iteration 298, loss = 0.0027140656020492315
iteration 299, loss = 0.0022818418219685555
iteration 0, loss = 0.002484952798113227
iteration 1, loss = 0.0026421421207487583
iteration 2, loss = 0.002692585112527013
iteration 3, loss = 0.0033906931057572365
iteration 4, loss = 0.002384187187999487
iteration 5, loss = 0.002403165912255645
iteration 6, loss = 0.002616990590468049
iteration 7, loss = 0.003525503445416689
iteration 8, loss = 0.0027232090942561626
iteration 9, loss = 0.006478109862655401
iteration 10, loss = 0.0024096036795526743
iteration 11, loss = 0.006814957596361637
iteration 12, loss = 0.002909274073317647
iteration 13, loss = 0.0023748083040118217
iteration 14, loss = 0.0033435013610869646
iteration 15, loss = 0.0027966671623289585
iteration 16, loss = 0.0025275342632085085
iteration 17, loss = 0.0027736315969377756
iteration 18, loss = 0.0024355980567634106
iteration 19, loss = 0.0023789149709045887
iteration 20, loss = 0.0023157834075391293
iteration 21, loss = 0.002329784445464611
iteration 22, loss = 0.0029139083344489336
iteration 23, loss = 0.0023898957297205925
iteration 24, loss = 0.002616220386698842
iteration 25, loss = 0.002422490157186985
iteration 26, loss = 0.0023565341252833605
iteration 27, loss = 0.00244501861743629
iteration 28, loss = 0.0031134237069636583
iteration 29, loss = 0.0029544970020651817
iteration 30, loss = 0.002553868107497692
iteration 31, loss = 0.00248170574195683
iteration 32, loss = 0.0037963183131068945
iteration 33, loss = 0.002308380324393511
iteration 34, loss = 0.0025961073115468025
iteration 35, loss = 0.002474021166563034
iteration 36, loss = 0.0026496578939259052
iteration 37, loss = 0.002413026988506317
iteration 38, loss = 0.003568529849871993
iteration 39, loss = 0.002728784689679742
iteration 40, loss = 0.0024400672409683466
iteration 41, loss = 0.0023742953781038523
iteration 42, loss = 0.0023104080464690924
iteration 43, loss = 0.003939182963222265
iteration 44, loss = 0.0028323468286544085
iteration 45, loss = 0.004708416759967804
iteration 46, loss = 0.0023513492196798325
iteration 47, loss = 0.0027699037455022335
iteration 48, loss = 0.002416150411590934
iteration 49, loss = 0.0023267772048711777
iteration 50, loss = 0.002494701649993658
iteration 51, loss = 0.006635398603975773
iteration 52, loss = 0.002415972761809826
iteration 53, loss = 0.002307696733623743
iteration 54, loss = 0.0036175220739096403
iteration 55, loss = 0.0023262989707291126
iteration 56, loss = 0.0036780706141144037
iteration 57, loss = 0.0023095847573131323
iteration 58, loss = 0.002222590148448944
iteration 59, loss = 0.0023475296329706907
iteration 60, loss = 0.00234760669991374
iteration 61, loss = 0.006675329990684986
iteration 62, loss = 0.0025756550021469593
iteration 63, loss = 0.002745064441114664
iteration 64, loss = 0.0025603522080928087
iteration 65, loss = 0.0026424513198435307
iteration 66, loss = 0.0032856373582035303
iteration 67, loss = 0.0025315312668681145
iteration 68, loss = 0.0033267883118242025
iteration 69, loss = 0.0031711971387267113
iteration 70, loss = 0.0024260079953819513
iteration 71, loss = 0.0035036159679293633
iteration 72, loss = 0.0024580038152635098
iteration 73, loss = 0.0026178404223173857
iteration 74, loss = 0.002318550832569599
iteration 75, loss = 0.0024312748573720455
iteration 76, loss = 0.004847058095037937
iteration 77, loss = 0.007358235772699118
iteration 78, loss = 0.002352736424654722
iteration 79, loss = 0.00249671027995646
iteration 80, loss = 0.003146961098536849
iteration 81, loss = 0.0023263010662049055
iteration 82, loss = 0.0024878496769815683
iteration 83, loss = 0.004121382255107164
iteration 84, loss = 0.0028011249378323555
iteration 85, loss = 0.002637566765770316
iteration 86, loss = 0.0024440637789666653
iteration 87, loss = 0.002471490763127804
iteration 88, loss = 0.0022199342492967844
iteration 89, loss = 0.0023821787908673286
iteration 90, loss = 0.0037125861272215843
iteration 91, loss = 0.0024406646843999624
iteration 92, loss = 0.0026278647128492594
iteration 93, loss = 0.0031758337281644344
iteration 94, loss = 0.002364126965403557
iteration 95, loss = 0.002414388582110405
iteration 96, loss = 0.002491721650585532
iteration 97, loss = 0.002257111482322216
iteration 98, loss = 0.0024186864029616117
iteration 99, loss = 0.0023820193018764257
iteration 100, loss = 0.002682230668142438
iteration 101, loss = 0.0024500044528394938
iteration 102, loss = 0.0024516077246516943
iteration 103, loss = 0.002448782091960311
iteration 104, loss = 0.0025597549974918365
iteration 105, loss = 0.002367118839174509
iteration 106, loss = 0.002659627702087164
iteration 107, loss = 0.0025391739327460527
iteration 108, loss = 0.0029521677643060684
iteration 109, loss = 0.00278413575142622
iteration 110, loss = 0.0026264952030032873
iteration 111, loss = 0.0035463126841932535
iteration 112, loss = 0.002633847063407302
iteration 113, loss = 0.002579319989308715
iteration 114, loss = 0.004145692102611065
iteration 115, loss = 0.002291297074407339
iteration 116, loss = 0.0025158149655908346
iteration 117, loss = 0.002578646643087268
iteration 118, loss = 0.0036141453310847282
iteration 119, loss = 0.0024013440124690533
iteration 120, loss = 0.0028132693842053413
iteration 121, loss = 0.0023171729408204556
iteration 122, loss = 0.003808572655543685
iteration 123, loss = 0.0025615806225687265
iteration 124, loss = 0.0027667866088449955
iteration 125, loss = 0.0020533427596092224
iteration 126, loss = 0.0037625981494784355
iteration 127, loss = 0.002594245132058859
iteration 128, loss = 0.003663830691948533
iteration 129, loss = 0.0026095574721693993
iteration 130, loss = 0.0023529825266450644
iteration 131, loss = 0.002380496123805642
iteration 132, loss = 0.003756449092179537
iteration 133, loss = 0.002698675263673067
iteration 134, loss = 0.002880280604586005
iteration 135, loss = 0.0037659555673599243
iteration 136, loss = 0.003308392595499754
iteration 137, loss = 0.0023262863978743553
iteration 138, loss = 0.002751074731349945
iteration 139, loss = 0.0023427533451467752
iteration 140, loss = 0.0038510451558977365
iteration 141, loss = 0.002383189043030143
iteration 142, loss = 0.0023468658328056335
iteration 143, loss = 0.0037031047977507114
iteration 144, loss = 0.002503276104107499
iteration 145, loss = 0.006228398531675339
iteration 146, loss = 0.0023198712151497602
iteration 147, loss = 0.0022193696349859238
iteration 148, loss = 0.002617435296997428
iteration 149, loss = 0.002342168241739273
iteration 150, loss = 0.002198169007897377
iteration 151, loss = 0.002421066863462329
iteration 152, loss = 0.003793291514739394
iteration 153, loss = 0.0027574310079216957
iteration 154, loss = 0.00230522477068007
iteration 155, loss = 0.002261494053527713
iteration 156, loss = 0.0063595641404390335
iteration 157, loss = 0.0023484386038035154
iteration 158, loss = 0.0023099230602383614
iteration 159, loss = 0.004149335902184248
iteration 160, loss = 0.0023573879152536392
iteration 161, loss = 0.0035208824556320906
iteration 162, loss = 0.0025254872161895037
iteration 163, loss = 0.002419174648821354
iteration 164, loss = 0.002494843676686287
iteration 165, loss = 0.0025175900664180517
iteration 166, loss = 0.0028987242840230465
iteration 167, loss = 0.0026398664340376854
iteration 168, loss = 0.0021200841292738914
iteration 169, loss = 0.002817833563312888
iteration 170, loss = 0.006462853867560625
iteration 171, loss = 0.0023712096735835075
iteration 172, loss = 0.0022481039632111788
iteration 173, loss = 0.0022602053359150887
iteration 174, loss = 0.003980365116149187
iteration 175, loss = 0.002150272950530052
iteration 176, loss = 0.0022962051443755627
iteration 177, loss = 0.0025399380829185247
iteration 178, loss = 0.0028765902388840914
iteration 179, loss = 0.006307539530098438
iteration 180, loss = 0.002435665111988783
iteration 181, loss = 0.0023505715653300285
iteration 182, loss = 0.00230435561388731
iteration 183, loss = 0.002254004357382655
iteration 184, loss = 0.0026427009142935276
iteration 185, loss = 0.0021455136593431234
iteration 186, loss = 0.00352800777181983
iteration 187, loss = 0.002529401332139969
iteration 188, loss = 0.0022793756797909737
iteration 189, loss = 0.002285888884216547
iteration 190, loss = 0.002538371365517378
iteration 191, loss = 0.0025832680985331535
iteration 192, loss = 0.0023408837150782347
iteration 193, loss = 0.00232762866653502
iteration 194, loss = 0.0026331020053476095
iteration 195, loss = 0.002296869410201907
iteration 196, loss = 0.002279816195368767
iteration 197, loss = 0.0032259412109851837
iteration 198, loss = 0.0036358253564685583
iteration 199, loss = 0.0030428776517510414
iteration 200, loss = 0.002249276265501976
iteration 201, loss = 0.003070211037993431
iteration 202, loss = 0.0022473714780062437
iteration 203, loss = 0.0024833972565829754
iteration 204, loss = 0.00270360941067338
iteration 205, loss = 0.002572674537077546
iteration 206, loss = 0.0024133394472301006
iteration 207, loss = 0.0030352603644132614
iteration 208, loss = 0.0023844882380217314
iteration 209, loss = 0.002498231129720807
iteration 210, loss = 0.006196493748575449
iteration 211, loss = 0.0023847913835197687
iteration 212, loss = 0.00248483894392848
iteration 213, loss = 0.003341415897011757
iteration 214, loss = 0.0021818282548338175
iteration 215, loss = 0.002376890741288662
iteration 216, loss = 0.002521451795473695
iteration 217, loss = 0.002244421048089862
iteration 218, loss = 0.0035662315785884857
iteration 219, loss = 0.0023873692844063044
iteration 220, loss = 0.0024120775051414967
iteration 221, loss = 0.0023073945194482803
iteration 222, loss = 0.006839526351541281
iteration 223, loss = 0.0022490089759230614
iteration 224, loss = 0.002395827090367675
iteration 225, loss = 0.002735942369326949
iteration 226, loss = 0.006145828869193792
iteration 227, loss = 0.002642988692969084
iteration 228, loss = 0.0023891814053058624
iteration 229, loss = 0.0026523792184889317
iteration 230, loss = 0.002520752139389515
iteration 231, loss = 0.002964516868814826
iteration 232, loss = 0.002357684075832367
iteration 233, loss = 0.002174660563468933
iteration 234, loss = 0.002909444272518158
iteration 235, loss = 0.0023584135342389345
iteration 236, loss = 0.0022353515960276127
iteration 237, loss = 0.00248670089058578
iteration 238, loss = 0.0023505091667175293
iteration 239, loss = 0.002476353431120515
iteration 240, loss = 0.0029171514324843884
iteration 241, loss = 0.0023441773373633623
iteration 242, loss = 0.0021728146821260452
iteration 243, loss = 0.002487284829840064
iteration 244, loss = 0.002839810913428664
iteration 245, loss = 0.0022184941917657852
iteration 246, loss = 0.00233058980666101
iteration 247, loss = 0.0026873552706092596
iteration 248, loss = 0.0023294228594750166
iteration 249, loss = 0.0022801598533988
iteration 250, loss = 0.002271682722494006
iteration 251, loss = 0.0028188477735966444
iteration 252, loss = 0.0022406307980418205
iteration 253, loss = 0.0029094056226313114
iteration 254, loss = 0.002127715852111578
iteration 255, loss = 0.0024836119264364243
iteration 256, loss = 0.0022586381528526545
iteration 257, loss = 0.002625528024509549
iteration 258, loss = 0.0025000092573463917
iteration 259, loss = 0.0026563904248178005
iteration 260, loss = 0.0024481681175529957
iteration 261, loss = 0.0025730011984705925
iteration 262, loss = 0.0021498396527022123
iteration 263, loss = 0.002482241252437234
iteration 264, loss = 0.002305954694747925
iteration 265, loss = 0.00267018168233335
iteration 266, loss = 0.006240574177354574
iteration 267, loss = 0.0024608676321804523
iteration 268, loss = 0.0026669565122574568
iteration 269, loss = 0.0021771728061139584
iteration 270, loss = 0.006174240726977587
iteration 271, loss = 0.0024371228646486998
iteration 272, loss = 0.002106807194650173
iteration 273, loss = 0.006008801516145468
iteration 274, loss = 0.0023734294809401035
iteration 275, loss = 0.002513327170163393
iteration 276, loss = 0.0025306828320026398
iteration 277, loss = 0.0023181522265076637
iteration 278, loss = 0.0022353208623826504
iteration 279, loss = 0.0021967142820358276
iteration 280, loss = 0.0022483905777335167
iteration 281, loss = 0.002278408734127879
iteration 282, loss = 0.0021996202412992716
iteration 283, loss = 0.0021379718091338873
iteration 284, loss = 0.003401927649974823
iteration 285, loss = 0.002461022697389126
iteration 286, loss = 0.002335258759558201
iteration 287, loss = 0.002172103151679039
iteration 288, loss = 0.0023489603772759438
iteration 289, loss = 0.0021671182475984097
iteration 290, loss = 0.0025008576922118664
iteration 291, loss = 0.0025327391922473907
iteration 292, loss = 0.0020856158807873726
iteration 293, loss = 0.0024539318401366472
iteration 294, loss = 0.002180669456720352
iteration 295, loss = 0.0023756057489663363
iteration 296, loss = 0.0021326204296201468
iteration 297, loss = 0.0020966590382158756
iteration 298, loss = 0.002453763969242573
iteration 299, loss = 0.0028971233405172825
iteration 0, loss = 0.0021833758801221848
iteration 1, loss = 0.003084513358771801
iteration 2, loss = 0.003290662309154868
iteration 3, loss = 0.002458154922351241
iteration 4, loss = 0.003090046579018235
iteration 5, loss = 0.002067327033728361
iteration 6, loss = 0.0028661866672337055
iteration 7, loss = 0.006087347399443388
iteration 8, loss = 0.002422067103907466
iteration 9, loss = 0.00247005932033062
iteration 10, loss = 0.002456831745803356
iteration 11, loss = 0.0025912094861268997
iteration 12, loss = 0.002817238215357065
iteration 13, loss = 0.002342017600312829
iteration 14, loss = 0.002198716625571251
iteration 15, loss = 0.0022224956192076206
iteration 16, loss = 0.0026075514033436775
iteration 17, loss = 0.0021799486130476
iteration 18, loss = 0.002134891925379634
iteration 19, loss = 0.0023997570388019085
iteration 20, loss = 0.0022068030666559935
iteration 21, loss = 0.0023242218885570765
iteration 22, loss = 0.002724414225667715
iteration 23, loss = 0.0028181420639157295
iteration 24, loss = 0.0023382671643048525
iteration 25, loss = 0.002584900241345167
iteration 26, loss = 0.002216485096141696
iteration 27, loss = 0.0034118490293622017
iteration 28, loss = 0.00375649007037282
iteration 29, loss = 0.0023867939598858356
iteration 30, loss = 0.0024008220061659813
iteration 31, loss = 0.0021498934365808964
iteration 32, loss = 0.0030029763001948595
iteration 33, loss = 0.002354187425225973
iteration 34, loss = 0.002417881041765213
iteration 35, loss = 0.0023408797569572926
iteration 36, loss = 0.002148752799257636
iteration 37, loss = 0.002417802345007658
iteration 38, loss = 0.002556377090513706
iteration 39, loss = 0.0022350603248924017
iteration 40, loss = 0.0022852320689707994
iteration 41, loss = 0.002086278283968568
iteration 42, loss = 0.0039464146830141544
iteration 43, loss = 0.002404589671641588
iteration 44, loss = 0.002349080517888069
iteration 45, loss = 0.0022307918407022953
iteration 46, loss = 0.0059493109583854675
iteration 47, loss = 0.0022149598225951195
iteration 48, loss = 0.005981002002954483
iteration 49, loss = 0.0024654120206832886
iteration 50, loss = 0.0029810299165546894
iteration 51, loss = 0.0021985992789268494
iteration 52, loss = 0.002124861115589738
iteration 53, loss = 0.0021128987427800894
iteration 54, loss = 0.003002899931743741
iteration 55, loss = 0.002655403921380639
iteration 56, loss = 0.0020763252396136522
iteration 57, loss = 0.0021915906108915806
iteration 58, loss = 0.0022503514774143696
iteration 59, loss = 0.002401095349341631
iteration 60, loss = 0.001979995984584093
iteration 61, loss = 0.002121459925547242
iteration 62, loss = 0.00223422865383327
iteration 63, loss = 0.002159444149583578
iteration 64, loss = 0.0024674669839441776
iteration 65, loss = 0.0021399727556854486
iteration 66, loss = 0.0023317865561693907
iteration 67, loss = 0.0022020824253559113
iteration 68, loss = 0.0020835120230913162
iteration 69, loss = 0.002702097874134779
iteration 70, loss = 0.00236674421466887
iteration 71, loss = 0.0023076843935996294
iteration 72, loss = 0.002701439894735813
iteration 73, loss = 0.002056759549304843
iteration 74, loss = 0.0024167178198695183
iteration 75, loss = 0.0021535519044846296
iteration 76, loss = 0.00221719266846776
iteration 77, loss = 0.0023165508173406124
iteration 78, loss = 0.0020747261587530375
iteration 79, loss = 0.002175628673285246
iteration 80, loss = 0.0035109801683574915
iteration 81, loss = 0.002217083005234599
iteration 82, loss = 0.002221790375187993
iteration 83, loss = 0.002354453084990382
iteration 84, loss = 0.002119894605129957
iteration 85, loss = 0.002414384623989463
iteration 86, loss = 0.0021899808198213577
iteration 87, loss = 0.002275437116622925
iteration 88, loss = 0.002111115027219057
iteration 89, loss = 0.0021197297610342503
iteration 90, loss = 0.002207445213571191
iteration 91, loss = 0.002572922967374325
iteration 92, loss = 0.0021278434433043003
iteration 93, loss = 0.0019205519929528236
iteration 94, loss = 0.0035627447068691254
iteration 95, loss = 0.0035603519063442945
iteration 96, loss = 0.002311080927029252
iteration 97, loss = 0.002958493772894144
iteration 98, loss = 0.002266572555527091
iteration 99, loss = 0.0021257640328258276
iteration 100, loss = 0.0021476030815392733
iteration 101, loss = 0.002238320652395487
iteration 102, loss = 0.0022480597253888845
iteration 103, loss = 0.002187115838751197
iteration 104, loss = 0.0022861468605697155
iteration 105, loss = 0.0022112366277724504
iteration 106, loss = 0.0023544789291918278
iteration 107, loss = 0.0035186109598726034
iteration 108, loss = 0.002408270724117756
iteration 109, loss = 0.0021122673060745
iteration 110, loss = 0.002219180343672633
iteration 111, loss = 0.0021721317898482084
iteration 112, loss = 0.0023372385185211897
iteration 113, loss = 0.0023631115909665823
iteration 114, loss = 0.002324241679161787
iteration 115, loss = 0.002583166351541877
iteration 116, loss = 0.0024632145650684834
iteration 117, loss = 0.0021552203688770533
iteration 118, loss = 0.006095790769904852
iteration 119, loss = 0.0030540914740413427
iteration 120, loss = 0.0034885313361883163
iteration 121, loss = 0.0020588731858879328
iteration 122, loss = 0.0021128496155142784
iteration 123, loss = 0.002121147932484746
iteration 124, loss = 0.0022588958963751793
iteration 125, loss = 0.0022778736893087626
iteration 126, loss = 0.002619538689032197
iteration 127, loss = 0.002374958945438266
iteration 128, loss = 0.002227674936875701
iteration 129, loss = 0.002050497103482485
iteration 130, loss = 0.0022037746384739876
iteration 131, loss = 0.001921440358273685
iteration 132, loss = 0.002439121250063181
iteration 133, loss = 0.0022853482514619827
iteration 134, loss = 0.0037892002146691084
iteration 135, loss = 0.0059075322933495045
iteration 136, loss = 0.002213846193626523
iteration 137, loss = 0.0022794962860643864
iteration 138, loss = 0.0024645482189953327
iteration 139, loss = 0.005967576988041401
iteration 140, loss = 0.0020505404099822044
iteration 141, loss = 0.0020879663061350584
iteration 142, loss = 0.0020869749132543802
iteration 143, loss = 0.003155579324811697
iteration 144, loss = 0.005976193118840456
iteration 145, loss = 0.0020328634418547153
iteration 146, loss = 0.0025767681654542685
iteration 147, loss = 0.0021231553982943296
iteration 148, loss = 0.0023769009858369827
iteration 149, loss = 0.0022329206112772226
iteration 150, loss = 0.0027517881244421005
iteration 151, loss = 0.0020794773008674383
iteration 152, loss = 0.0023058345541357994
iteration 153, loss = 0.0024802503176033497
iteration 154, loss = 0.0023168267216533422
iteration 155, loss = 0.002225151751190424
iteration 156, loss = 0.002407296560704708
iteration 157, loss = 0.0021617384627461433
iteration 158, loss = 0.002055100165307522
iteration 159, loss = 0.0023286165669560432
iteration 160, loss = 0.0023878435604274273
iteration 161, loss = 0.0021634071599692106
iteration 162, loss = 0.002272607060149312
iteration 163, loss = 0.005757496226578951
iteration 164, loss = 0.0020818233024328947
iteration 165, loss = 0.0020142379216849804
iteration 166, loss = 0.002190070226788521
iteration 167, loss = 0.0023423265665769577
iteration 168, loss = 0.0021297435741871595
iteration 169, loss = 0.002319585531949997
iteration 170, loss = 0.0022109802812337875
iteration 171, loss = 0.0031698630191385746
iteration 172, loss = 0.0023284610360860825
iteration 173, loss = 0.0020314878784120083
iteration 174, loss = 0.002118390519171953
iteration 175, loss = 0.00248128199018538
iteration 176, loss = 0.0022881331387907267
iteration 177, loss = 0.002337018260732293
iteration 178, loss = 0.0057395524345338345
iteration 179, loss = 0.0020779315382242203
iteration 180, loss = 0.005857367068529129
iteration 181, loss = 0.0022054717410355806
iteration 182, loss = 0.00200175354257226
iteration 183, loss = 0.003385628806427121
iteration 184, loss = 0.0023965612053871155
iteration 185, loss = 0.0022676594089716673
iteration 186, loss = 0.002021773951128125
iteration 187, loss = 0.0022263971623033285
iteration 188, loss = 0.0024836373049765825
iteration 189, loss = 0.0022954042069613934
iteration 190, loss = 0.0019197348738089204
iteration 191, loss = 0.0020346613600850105
iteration 192, loss = 0.0022036065347492695
iteration 193, loss = 0.0058469269424676895
iteration 194, loss = 0.0025525272358208895
iteration 195, loss = 0.002189029473811388
iteration 196, loss = 0.0022104657255113125
iteration 197, loss = 0.0020102120470255613
iteration 198, loss = 0.003580617718398571
iteration 199, loss = 0.002080170903354883
iteration 200, loss = 0.002877539489418268
iteration 201, loss = 0.0020976911764591932
iteration 202, loss = 0.0019857489969581366
iteration 203, loss = 0.0020379687193781137
iteration 204, loss = 0.0020729973912239075
iteration 205, loss = 0.002142351819202304
iteration 206, loss = 0.002190560568124056
iteration 207, loss = 0.0027022496797144413
iteration 208, loss = 0.002280013170093298
iteration 209, loss = 0.002307929564267397
iteration 210, loss = 0.0027043409645557404
iteration 211, loss = 0.001981091219931841
iteration 212, loss = 0.003120678709819913
iteration 213, loss = 0.0022423448972404003
iteration 214, loss = 0.002263457980006933
iteration 215, loss = 0.0023196684196591377
iteration 216, loss = 0.0021512166131287813
iteration 217, loss = 0.0022672535851597786
iteration 218, loss = 0.0021193518768996
iteration 219, loss = 0.005852147471159697
iteration 220, loss = 0.002222228329628706
iteration 221, loss = 0.0021296374034136534
iteration 222, loss = 0.002003937726840377
iteration 223, loss = 0.0021654609590768814
iteration 224, loss = 0.001964925555512309
iteration 225, loss = 0.002117318566888571
iteration 226, loss = 0.0025334549136459827
iteration 227, loss = 0.003857945092022419
iteration 228, loss = 0.0021752500906586647
iteration 229, loss = 0.0020845262333750725
iteration 230, loss = 0.002055019373074174
iteration 231, loss = 0.002192169427871704
iteration 232, loss = 0.003000024938955903
iteration 233, loss = 0.002368224784731865
iteration 234, loss = 0.007244669832289219
iteration 235, loss = 0.002055154647678137
iteration 236, loss = 0.00289172469638288
iteration 237, loss = 0.0024421587586402893
iteration 238, loss = 0.002314731478691101
iteration 239, loss = 0.001956692198291421
iteration 240, loss = 0.0020383582450449467
iteration 241, loss = 0.0022645730059593916
iteration 242, loss = 0.002457295311614871
iteration 243, loss = 0.0019947888795286417
iteration 244, loss = 0.002292066812515259
iteration 245, loss = 0.0024710139259696007
iteration 246, loss = 0.002096626441925764
iteration 247, loss = 0.002173644956201315
iteration 248, loss = 0.0019759663846343756
iteration 249, loss = 0.002001129323616624
iteration 250, loss = 0.0021182603668421507
iteration 251, loss = 0.002201439579948783
iteration 252, loss = 0.0019765428733080626
iteration 253, loss = 0.002716702874749899
iteration 254, loss = 0.0021504173055291176
iteration 255, loss = 0.0030062405858188868
iteration 256, loss = 0.002040520077571273
iteration 257, loss = 0.0020302378106862307
iteration 258, loss = 0.003401069436222315
iteration 259, loss = 0.0022792527452111244
iteration 260, loss = 0.002095182193443179
iteration 261, loss = 0.0021951969247311354
iteration 262, loss = 0.002089545363560319
iteration 263, loss = 0.002547035226598382
iteration 264, loss = 0.002195653971284628
iteration 265, loss = 0.002010725438594818
iteration 266, loss = 0.0029911405872553587
iteration 267, loss = 0.0022631040774285793
iteration 268, loss = 0.0021163057535886765
iteration 269, loss = 0.0027292179875075817
iteration 270, loss = 0.0021834082435816526
iteration 271, loss = 0.0020105778239667416
iteration 272, loss = 0.0030513431411236525
iteration 273, loss = 0.00325951911509037
iteration 274, loss = 0.009101628325879574
iteration 275, loss = 0.003583665471524
iteration 276, loss = 0.0033958787098526955
iteration 277, loss = 0.0020403838716447353
iteration 278, loss = 0.0033635881263762712
iteration 279, loss = 0.00207012752071023
iteration 280, loss = 0.0028732242062687874
iteration 281, loss = 0.0021147592924535275
iteration 282, loss = 0.002139131538569927
iteration 283, loss = 0.002377364318817854
iteration 284, loss = 0.002007094444707036
iteration 285, loss = 0.0017820065841078758
iteration 286, loss = 0.002192182932049036
iteration 287, loss = 0.0020088599994778633
iteration 288, loss = 0.0022347134072333574
iteration 289, loss = 0.002285448834300041
iteration 290, loss = 0.003990768454968929
iteration 291, loss = 0.0023532728664577007
iteration 292, loss = 0.0027935216203331947
iteration 293, loss = 0.0022695036605000496
iteration 294, loss = 0.0033025327138602734
iteration 295, loss = 0.002003547502681613
iteration 296, loss = 0.0020645831245929003
iteration 297, loss = 0.0031512139830738306
iteration 298, loss = 0.002477057743817568
iteration 299, loss = 0.0020494742784649134
iteration 0, loss = 0.0018032180378213525
iteration 1, loss = 0.002221147995442152
iteration 2, loss = 0.002008284442126751
iteration 3, loss = 0.0022151563316583633
iteration 4, loss = 0.0032508191652595997
iteration 5, loss = 0.0020517476368695498
iteration 6, loss = 0.002325736917555332
iteration 7, loss = 0.002361241029575467
iteration 8, loss = 0.00206397520378232
iteration 9, loss = 0.0019713991787284613
iteration 10, loss = 0.003238420933485031
iteration 11, loss = 0.0019603916443884373
iteration 12, loss = 0.0020204000174999237
iteration 13, loss = 0.0019552973099052906
iteration 14, loss = 0.002296754391863942
iteration 15, loss = 0.002028296235948801
iteration 16, loss = 0.002340848557651043
iteration 17, loss = 0.0019450896652415395
iteration 18, loss = 0.002130520762875676
iteration 19, loss = 0.0020819653291255236
iteration 20, loss = 0.0020062464755028486
iteration 21, loss = 0.0021649617701768875
iteration 22, loss = 0.002510731341317296
iteration 23, loss = 0.009167597629129887
iteration 24, loss = 0.004468489903956652
iteration 25, loss = 0.0021845099981874228
iteration 26, loss = 0.0021761036477983
iteration 27, loss = 0.0020106849260628223
iteration 28, loss = 0.0021015324164181948
iteration 29, loss = 0.00210885819979012
iteration 30, loss = 0.001968399388715625
iteration 31, loss = 0.0019540702924132347
iteration 32, loss = 0.002112767891958356
iteration 33, loss = 0.0024077098350971937
iteration 34, loss = 0.002093052491545677
iteration 35, loss = 0.0024221991188824177
iteration 36, loss = 0.0019339548889547586
iteration 37, loss = 0.0023008978459984064
iteration 38, loss = 0.003176868660375476
iteration 39, loss = 0.002418690826743841
iteration 40, loss = 0.002329603536054492
iteration 41, loss = 0.005525074899196625
iteration 42, loss = 0.0019673462957143784
iteration 43, loss = 0.001933925785124302
iteration 44, loss = 0.0023867329582571983
iteration 45, loss = 0.0026674591936171055
iteration 46, loss = 0.002099689096212387
iteration 47, loss = 0.0019349876092746854
iteration 48, loss = 0.0038308158982545137
iteration 49, loss = 0.002172989072278142
iteration 50, loss = 0.0021884588059037924
iteration 51, loss = 0.0017957978416234255
iteration 52, loss = 0.002190455561503768
iteration 53, loss = 0.002805503085255623
iteration 54, loss = 0.001910497434437275
iteration 55, loss = 0.002004605019465089
iteration 56, loss = 0.002136619295924902
iteration 57, loss = 0.0019318744307383895
iteration 58, loss = 0.0020167939364910126
iteration 59, loss = 0.001958535285666585
iteration 60, loss = 0.0020000478252768517
iteration 61, loss = 0.002067502588033676
iteration 62, loss = 0.001982275163754821
iteration 63, loss = 0.0020114686340093613
iteration 64, loss = 0.0030046114698052406
iteration 65, loss = 0.002920027356594801
iteration 66, loss = 0.005617035087198019
iteration 67, loss = 0.00194640108384192
iteration 68, loss = 0.0019414722919464111
iteration 69, loss = 0.0028206603601574898
iteration 70, loss = 0.0024095855187624693
iteration 71, loss = 0.001865072175860405
iteration 72, loss = 0.0019802863243967295
iteration 73, loss = 0.00326350936666131
iteration 74, loss = 0.002262741792947054
iteration 75, loss = 0.00624396838247776
iteration 76, loss = 0.002313234144821763
iteration 77, loss = 0.0025778640992939472
iteration 78, loss = 0.00230983505025506
iteration 79, loss = 0.0018985276110470295
iteration 80, loss = 0.00194069417193532
iteration 81, loss = 0.001887909951619804
iteration 82, loss = 0.0018913159146904945
iteration 83, loss = 0.0021710707806050777
iteration 84, loss = 0.001978167798370123
iteration 85, loss = 0.001958417473360896
iteration 86, loss = 0.0020344608929008245
iteration 87, loss = 0.002474543172866106
iteration 88, loss = 0.0018557802541181445
iteration 89, loss = 0.0022167509887367487
iteration 90, loss = 0.005405401811003685
iteration 91, loss = 0.002019257750362158
iteration 92, loss = 0.001901953830383718
iteration 93, loss = 0.0018663518130779266
iteration 94, loss = 0.002135358052328229
iteration 95, loss = 0.002067569177597761
iteration 96, loss = 0.002993576694279909
iteration 97, loss = 0.002147435210645199
iteration 98, loss = 0.003505571046844125
iteration 99, loss = 0.0021353615447878838
iteration 100, loss = 0.0018322940450161695
iteration 101, loss = 0.0021975936833769083
iteration 102, loss = 0.0019706441089510918
iteration 103, loss = 0.0018284042598679662
iteration 104, loss = 0.002195844892412424
iteration 105, loss = 0.0030275185126811266
iteration 106, loss = 0.0019850239623337984
iteration 107, loss = 0.0019973968155682087
iteration 108, loss = 0.0029744526837021112
iteration 109, loss = 0.0019923620857298374
iteration 110, loss = 0.003668667981401086
iteration 111, loss = 0.0018982000183314085
iteration 112, loss = 0.0021192929707467556
iteration 113, loss = 0.0021896183025091887
iteration 114, loss = 0.002091629896312952
iteration 115, loss = 0.0020958208478987217
iteration 116, loss = 0.0021960376761853695
iteration 117, loss = 0.002398565411567688
iteration 118, loss = 0.0021868215408176184
iteration 119, loss = 0.0020304685458540916
iteration 120, loss = 0.0023547725286334753
iteration 121, loss = 0.005305994302034378
iteration 122, loss = 0.002182828029617667
iteration 123, loss = 0.0018329937011003494
iteration 124, loss = 0.002173492219299078
iteration 125, loss = 0.002025868510827422
iteration 126, loss = 0.0019568209536373615
iteration 127, loss = 0.001906115561723709
iteration 128, loss = 0.0024532503448426723
iteration 129, loss = 0.0019915730226784945
iteration 130, loss = 0.002947064582258463
iteration 131, loss = 0.001994919963181019
iteration 132, loss = 0.0018857779214158654
iteration 133, loss = 0.0021426149178296328
iteration 134, loss = 0.0020001367665827274
iteration 135, loss = 0.0019119798671454191
iteration 136, loss = 0.0018129287054762244
iteration 137, loss = 0.002226342912763357
iteration 138, loss = 0.0019106362015008926
iteration 139, loss = 0.0021372593473643064
iteration 140, loss = 0.002601027488708496
iteration 141, loss = 0.0020098374225199223
iteration 142, loss = 0.0020171336364001036
iteration 143, loss = 0.0019980010110884905
iteration 144, loss = 0.002200701739639044
iteration 145, loss = 0.002061756793409586
iteration 146, loss = 0.0023181713186204433
iteration 147, loss = 0.0022866216022521257
iteration 148, loss = 0.002123890444636345
iteration 149, loss = 0.001940011978149414
iteration 150, loss = 0.0018763289554044604
iteration 151, loss = 0.0031306766904890537
iteration 152, loss = 0.0019882405176758766
iteration 153, loss = 0.0019019654719159007
iteration 154, loss = 0.0020834810566157103
iteration 155, loss = 0.003145358758047223
iteration 156, loss = 0.00208578584715724
iteration 157, loss = 0.002281890017911792
iteration 158, loss = 0.0025048411916941404
iteration 159, loss = 0.0021198661997914314
iteration 160, loss = 0.0019633276388049126
iteration 161, loss = 0.003197490470483899
iteration 162, loss = 0.0026787854731082916
iteration 163, loss = 0.001962458249181509
iteration 164, loss = 0.0022258409298956394
iteration 165, loss = 0.0022121649235486984
iteration 166, loss = 0.002026713453233242
iteration 167, loss = 0.0020142460707575083
iteration 168, loss = 0.002017336431890726
iteration 169, loss = 0.0019340496510267258
iteration 170, loss = 0.003499805461615324
iteration 171, loss = 0.002327726688235998
iteration 172, loss = 0.002470653969794512
iteration 173, loss = 0.00255324924364686
iteration 174, loss = 0.006456165574491024
iteration 175, loss = 0.0020240317098796368
iteration 176, loss = 0.0018536970019340515
iteration 177, loss = 0.0018765938002616167
iteration 178, loss = 0.002281403634697199
iteration 179, loss = 0.0021077271085232496
iteration 180, loss = 0.0018614022992551327
iteration 181, loss = 0.002368235494941473
iteration 182, loss = 0.001991613768041134
iteration 183, loss = 0.0019504715455695987
iteration 184, loss = 0.00187843875028193
iteration 185, loss = 0.00213455269113183
iteration 186, loss = 0.0020111529156565666
iteration 187, loss = 0.0020866109989583492
iteration 188, loss = 0.0019419491291046143
iteration 189, loss = 0.002078188117593527
iteration 190, loss = 0.0017376989126205444
iteration 191, loss = 0.001899567199870944
iteration 192, loss = 0.0020754276774823666
iteration 193, loss = 0.002094466472044587
iteration 194, loss = 0.0019640731625258923
iteration 195, loss = 0.001850219676271081
iteration 196, loss = 0.0018948102369904518
iteration 197, loss = 0.0021039173007011414
iteration 198, loss = 0.002797748427838087
iteration 199, loss = 0.0027047900948673487
iteration 200, loss = 0.001994989113882184
iteration 201, loss = 0.0018802722916007042
iteration 202, loss = 0.0018657126929610968
iteration 203, loss = 0.001925658667460084
iteration 204, loss = 0.003089207224547863
iteration 205, loss = 0.001925829448737204
iteration 206, loss = 0.0027107386849820614
iteration 207, loss = 0.002035545650869608
iteration 208, loss = 0.002089941408485174
iteration 209, loss = 0.00534364627674222
iteration 210, loss = 0.0022483805660158396
iteration 211, loss = 0.001962465001270175
iteration 212, loss = 0.00198475643992424
iteration 213, loss = 0.001806215150281787
iteration 214, loss = 0.002007213653996587
iteration 215, loss = 0.0031071677803993225
iteration 216, loss = 0.0020945672877132893
iteration 217, loss = 0.002054481068626046
iteration 218, loss = 0.003075237385928631
iteration 219, loss = 0.0027782979886978865
iteration 220, loss = 0.002260648412629962
iteration 221, loss = 0.0020649293437600136
iteration 222, loss = 0.002089916029945016
iteration 223, loss = 0.0022113530430942774
iteration 224, loss = 0.005379137117415667
iteration 225, loss = 0.001863508834503591
iteration 226, loss = 0.0017755014123395085
iteration 227, loss = 0.0020171066280454397
iteration 228, loss = 0.0022688754834234715
iteration 229, loss = 0.0020126502495259047
iteration 230, loss = 0.0018570186803117394
iteration 231, loss = 0.0018646043026819825
iteration 232, loss = 0.0018296450143679976
iteration 233, loss = 0.0017536610830575228
iteration 234, loss = 0.001810974907130003
iteration 235, loss = 0.002003638306632638
iteration 236, loss = 0.00199115090072155
iteration 237, loss = 0.002882710425183177
iteration 238, loss = 0.005638458766043186
iteration 239, loss = 0.00183187797665596
iteration 240, loss = 0.001929564168676734
iteration 241, loss = 0.0030447489116340876
iteration 242, loss = 0.0019169787410646677
iteration 243, loss = 0.002948927925899625
iteration 244, loss = 0.0019448420498520136
iteration 245, loss = 0.002220647409558296
iteration 246, loss = 0.002070685150101781
iteration 247, loss = 0.002026683185249567
iteration 248, loss = 0.0019173453329131007
iteration 249, loss = 0.0021311852615326643
iteration 250, loss = 0.002165363635867834
iteration 251, loss = 0.0019392867106944323
iteration 252, loss = 0.001977979438379407
iteration 253, loss = 0.0021292902529239655
iteration 254, loss = 0.0018382814014330506
iteration 255, loss = 0.0020526619628071785
iteration 256, loss = 0.0017848778516054153
iteration 257, loss = 0.0019044032087549567
iteration 258, loss = 0.002054603537544608
iteration 259, loss = 0.0019632887560874224
iteration 260, loss = 0.0019121908117085695
iteration 261, loss = 0.008694306947290897
iteration 262, loss = 0.0020512917544692755
iteration 263, loss = 0.0018229788402095437
iteration 264, loss = 0.0021279396023601294
iteration 265, loss = 0.001804980682209134
iteration 266, loss = 0.0024879060219973326
iteration 267, loss = 0.001924868207424879
iteration 268, loss = 0.0017114451620727777
iteration 269, loss = 0.0021304176189005375
iteration 270, loss = 0.001762020867317915
iteration 271, loss = 0.0018087233183905482
iteration 272, loss = 0.001931460341438651
iteration 273, loss = 0.0018903289455920458
iteration 274, loss = 0.0020535539370030165
iteration 275, loss = 0.00200990354642272
iteration 276, loss = 0.001826935331337154
iteration 277, loss = 0.002656421158462763
iteration 278, loss = 0.0017800257774069905
iteration 279, loss = 0.005085861310362816
iteration 280, loss = 0.005107605364173651
iteration 281, loss = 0.0019086779793724418
iteration 282, loss = 0.0017850177828222513
iteration 283, loss = 0.002126137726008892
iteration 284, loss = 0.0017870295559987426
iteration 285, loss = 0.0017761776689440012
iteration 286, loss = 0.0023059099912643433
iteration 287, loss = 0.0031231469474732876
iteration 288, loss = 0.0021528645884245634
iteration 289, loss = 0.0021916013211011887
iteration 290, loss = 0.00292479177005589
iteration 291, loss = 0.0020119508262723684
iteration 292, loss = 0.002017199294641614
iteration 293, loss = 0.003160718595609069
iteration 294, loss = 0.0018665347015485168
iteration 295, loss = 0.0022997218184173107
iteration 296, loss = 0.0024969764053821564
iteration 297, loss = 0.002534728031605482
iteration 298, loss = 0.0017645182088017464
iteration 299, loss = 0.001775674638338387
iteration 0, loss = 0.005410661920905113
iteration 1, loss = 0.0019653066992759705
iteration 2, loss = 0.0019074797164648771
iteration 3, loss = 0.0018030264182016253
iteration 4, loss = 0.0020600156858563423
iteration 5, loss = 0.0017256640130653977
iteration 6, loss = 0.0029751078691333532
iteration 7, loss = 0.002035698154941201
iteration 8, loss = 0.0018872027285397053
iteration 9, loss = 0.0019536898471415043
iteration 10, loss = 0.001838763477280736
iteration 11, loss = 0.002087936969473958
iteration 12, loss = 0.0019962135702371597
iteration 13, loss = 0.0018700892105698586
iteration 14, loss = 0.002069883281365037
iteration 15, loss = 0.0019814730621874332
iteration 16, loss = 0.0020847227424383163
iteration 17, loss = 0.005168225150555372
iteration 18, loss = 0.0018838753458112478
iteration 19, loss = 0.0017953315982595086
iteration 20, loss = 0.005108117125928402
iteration 21, loss = 0.0018415848026052117
iteration 22, loss = 0.0023950417526066303
iteration 23, loss = 0.005047853570431471
iteration 24, loss = 0.0023141910787671804
iteration 25, loss = 0.002870825119316578
iteration 26, loss = 0.0019278863910585642
iteration 27, loss = 0.001979903783649206
iteration 28, loss = 0.001770870410837233
iteration 29, loss = 0.0024751899763941765
iteration 30, loss = 0.0019646743312478065
iteration 31, loss = 0.0017391758738085628
iteration 32, loss = 0.002146662911400199
iteration 33, loss = 0.0021207102108746767
iteration 34, loss = 0.0020218726713210344
iteration 35, loss = 0.0017818498890846968
iteration 36, loss = 0.0018512345850467682
iteration 37, loss = 0.0024166374932974577
iteration 38, loss = 0.0017409694846719503
iteration 39, loss = 0.0033555803820490837
iteration 40, loss = 0.0020642527379095554
iteration 41, loss = 0.0018335191998630762
iteration 42, loss = 0.0018985458882525563
iteration 43, loss = 0.001886841026134789
iteration 44, loss = 0.0020303346682339907
iteration 45, loss = 0.0018472576048225164
iteration 46, loss = 0.0020165867172181606
iteration 47, loss = 0.002059498568996787
iteration 48, loss = 0.0022301930002868176
iteration 49, loss = 0.0018496840493753552
iteration 50, loss = 0.0018748999573290348
iteration 51, loss = 0.004987763240933418
iteration 52, loss = 0.0018555476563051343
iteration 53, loss = 0.0020177364349365234
iteration 54, loss = 0.0018021829891949892
iteration 55, loss = 0.0020262289326637983
iteration 56, loss = 0.0017808900447562337
iteration 57, loss = 0.002141023986041546
iteration 58, loss = 0.0020095007494091988
iteration 59, loss = 0.0018854632508009672
iteration 60, loss = 0.0017728779930621386
iteration 61, loss = 0.0017747406382113695
iteration 62, loss = 0.002121726516634226
iteration 63, loss = 0.0017888499423861504
iteration 64, loss = 0.0019259745022282004
iteration 65, loss = 0.0027488761115819216
iteration 66, loss = 0.0021012548822909594
iteration 67, loss = 0.0020759126637130976
iteration 68, loss = 0.0020740581676363945
iteration 69, loss = 0.0019878640305250883
iteration 70, loss = 0.0018570325337350368
iteration 71, loss = 0.0019318379927426577
iteration 72, loss = 0.00233464571647346
iteration 73, loss = 0.002963652368634939
iteration 74, loss = 0.0017842812230810523
iteration 75, loss = 0.0020534140057861805
iteration 76, loss = 0.0018874313682317734
iteration 77, loss = 0.0016382874455302954
iteration 78, loss = 0.001961903180927038
iteration 79, loss = 0.0017988220788538456
iteration 80, loss = 0.0020357330795377493
iteration 81, loss = 0.001966251293197274
iteration 82, loss = 0.0026628850027918816
iteration 83, loss = 0.0022844031918793917
iteration 84, loss = 0.0017848941497504711
iteration 85, loss = 0.002960763406008482
iteration 86, loss = 0.0018877256661653519
iteration 87, loss = 0.0020049484446644783
iteration 88, loss = 0.0016775315161794424
iteration 89, loss = 0.0026286656502634287
iteration 90, loss = 0.0018028710037469864
iteration 91, loss = 0.002375295851379633
iteration 92, loss = 0.005148072261363268
iteration 93, loss = 0.00186013407073915
iteration 94, loss = 0.00212099589407444
iteration 95, loss = 0.0019243399146944284
iteration 96, loss = 0.0023974799551069736
iteration 97, loss = 0.0020717226434499025
iteration 98, loss = 0.0018654143204912543
iteration 99, loss = 0.0018191668204963207
iteration 100, loss = 0.0017431725282222033
iteration 101, loss = 0.0029359592590481043
iteration 102, loss = 0.0030827485024929047
iteration 103, loss = 0.0029069872107356787
iteration 104, loss = 0.0020044376142323017
iteration 105, loss = 0.0019666508305817842
iteration 106, loss = 0.002137985313311219
iteration 107, loss = 0.0030871951021254063
iteration 108, loss = 0.002969382330775261
iteration 109, loss = 0.00490163778886199
iteration 110, loss = 0.0017486639553681016
iteration 111, loss = 0.0020810654386878014
iteration 112, loss = 0.0019613290205597878
iteration 113, loss = 0.0018847713945433497
iteration 114, loss = 0.002109152963384986
iteration 115, loss = 0.001794158131815493
iteration 116, loss = 0.0049889590591192245
iteration 117, loss = 0.0017973624635487795
iteration 118, loss = 0.002151878084987402
iteration 119, loss = 0.0018213405273854733
iteration 120, loss = 0.0017003595130518079
iteration 121, loss = 0.001818304997868836
iteration 122, loss = 0.0019693272188305855
iteration 123, loss = 0.002104287501424551
iteration 124, loss = 0.002147451275959611
iteration 125, loss = 0.0019986829720437527
iteration 126, loss = 0.001964726485311985
iteration 127, loss = 0.0016819717129692435
iteration 128, loss = 0.002190676983445883
iteration 129, loss = 0.0018403944559395313
iteration 130, loss = 0.0021238040644675493
iteration 131, loss = 0.0017056503565981984
iteration 132, loss = 0.0019613541662693024
iteration 133, loss = 0.002205255441367626
iteration 134, loss = 0.001742372871376574
iteration 135, loss = 0.0017467755824327469
iteration 136, loss = 0.002982406411319971
iteration 137, loss = 0.0017867770511657
iteration 138, loss = 0.0051049236208200455
iteration 139, loss = 0.0017489327583462
iteration 140, loss = 0.004886845126748085
iteration 141, loss = 0.0020067463628947735
iteration 142, loss = 0.004945447668433189
iteration 143, loss = 0.002911396324634552
iteration 144, loss = 0.0020553981885313988
iteration 145, loss = 0.002892459509894252
iteration 146, loss = 0.00211704196408391
iteration 147, loss = 0.001917499816045165
iteration 148, loss = 0.001969946315512061
iteration 149, loss = 0.0017753647407516837
iteration 150, loss = 0.001711036078631878
iteration 151, loss = 0.002100250218063593
iteration 152, loss = 0.0021539051085710526
iteration 153, loss = 0.0018775612115859985
iteration 154, loss = 0.0026470511220395565
iteration 155, loss = 0.0020364876836538315
iteration 156, loss = 0.0037232008762657642
iteration 157, loss = 0.0018747157882899046
iteration 158, loss = 0.001656640088185668
iteration 159, loss = 0.0032828920520842075
iteration 160, loss = 0.002158271847292781
iteration 161, loss = 0.0021347724832594395
iteration 162, loss = 0.001905000419355929
iteration 163, loss = 0.0020100981928408146
iteration 164, loss = 0.0016902163624763489
iteration 165, loss = 0.0017059085657820106
iteration 166, loss = 0.0020427764393389225
iteration 167, loss = 0.0026072256732732058
iteration 168, loss = 0.0024982301983982325
iteration 169, loss = 0.002719938289374113
iteration 170, loss = 0.0028704828582704067
iteration 171, loss = 0.001983035821467638
iteration 172, loss = 0.0019519886700436473
iteration 173, loss = 0.0024094830732792616
iteration 174, loss = 0.0018279273062944412
iteration 175, loss = 0.00189207773655653
iteration 176, loss = 0.0017620747676119208
iteration 177, loss = 0.0022170785814523697
iteration 178, loss = 0.001731237513013184
iteration 179, loss = 0.0019581110682338476
iteration 180, loss = 0.0017157422844320536
iteration 181, loss = 0.002010627882555127
iteration 182, loss = 0.001935648499056697
iteration 183, loss = 0.0018597189337015152
iteration 184, loss = 0.0017702907789498568
iteration 185, loss = 0.00199356721714139
iteration 186, loss = 0.0018228362314403057
iteration 187, loss = 0.0017770222621038556
iteration 188, loss = 0.00504707545042038
iteration 189, loss = 0.0017180519644171
iteration 190, loss = 0.0019040819024667144
iteration 191, loss = 0.0019007213413715363
iteration 192, loss = 0.0049488963559269905
iteration 193, loss = 0.001894736080430448
iteration 194, loss = 0.0018064165487885475
iteration 195, loss = 0.0017136014066636562
iteration 196, loss = 0.0019543275702744722
iteration 197, loss = 0.002125515602529049
iteration 198, loss = 0.0018638877663761377
iteration 199, loss = 0.0020486696157604456
iteration 200, loss = 0.0017245972994714975
iteration 201, loss = 0.0019462249474599957
iteration 202, loss = 0.001817567739635706
iteration 203, loss = 0.004923100117594004
iteration 204, loss = 0.002497856505215168
iteration 205, loss = 0.0018012203508988023
iteration 206, loss = 0.0025051147677004337
iteration 207, loss = 0.001826299587264657
iteration 208, loss = 0.001832850743085146
iteration 209, loss = 0.001888515311293304
iteration 210, loss = 0.001885752659291029
iteration 211, loss = 0.0017705655191093683
iteration 212, loss = 0.001662731054238975
iteration 213, loss = 0.0015101274475455284
iteration 214, loss = 0.001746990135870874
iteration 215, loss = 0.0017812862060964108
iteration 216, loss = 0.002407049760222435
iteration 217, loss = 0.002054157666862011
iteration 218, loss = 0.0017478402005508542
iteration 219, loss = 0.0016575604677200317
iteration 220, loss = 0.0017108991742134094
iteration 221, loss = 0.0018705250695347786
iteration 222, loss = 0.0022443835623562336
iteration 223, loss = 0.0016277855029329658
iteration 224, loss = 0.0019397505093365908
iteration 225, loss = 0.0017117673996835947
iteration 226, loss = 0.0019157823408022523
iteration 227, loss = 0.0018970140954479575
iteration 228, loss = 0.0017462671967223287
iteration 229, loss = 0.002144323894754052
iteration 230, loss = 0.0016232467023655772
iteration 231, loss = 0.0017349489498883486
iteration 232, loss = 0.0017512289341539145
iteration 233, loss = 0.0019396250136196613
iteration 234, loss = 0.00293007236905396
iteration 235, loss = 0.002160213189199567
iteration 236, loss = 0.001646405435167253
iteration 237, loss = 0.002984567079693079
iteration 238, loss = 0.0017253842670470476
iteration 239, loss = 0.0018837155075743794
iteration 240, loss = 0.002249012468382716
iteration 241, loss = 0.0016707134200260043
iteration 242, loss = 0.0018394081853330135
iteration 243, loss = 0.0018105142517015338
iteration 244, loss = 0.0016657585510984063
iteration 245, loss = 0.0016384629998356104
iteration 246, loss = 0.0018639101181179285
iteration 247, loss = 0.0022345660254359245
iteration 248, loss = 0.0022737570106983185
iteration 249, loss = 0.0017594177043065429
iteration 250, loss = 0.001870919018983841
iteration 251, loss = 0.0017825397662818432
iteration 252, loss = 0.0017659428995102644
iteration 253, loss = 0.0016709336778149009
iteration 254, loss = 0.001841240911744535
iteration 255, loss = 0.001966583076864481
iteration 256, loss = 0.0019177801441401243
iteration 257, loss = 0.0016629700548946857
iteration 258, loss = 0.001846954575739801
iteration 259, loss = 0.002785830292850733
iteration 260, loss = 0.0017979713156819344
iteration 261, loss = 0.002394456183537841
iteration 262, loss = 0.0016908547841012478
iteration 263, loss = 0.001844216836616397
iteration 264, loss = 0.0017017563804984093
iteration 265, loss = 0.0019646487198770046
iteration 266, loss = 0.0016190455062314868
iteration 267, loss = 0.0016497106989845634
iteration 268, loss = 0.0017964229919016361
iteration 269, loss = 0.0015574817080050707
iteration 270, loss = 0.003003425896167755
iteration 271, loss = 0.0021997038275003433
iteration 272, loss = 0.0018736899364739656
iteration 273, loss = 0.0015906080370768905
iteration 274, loss = 0.0017820921493694186
iteration 275, loss = 0.0018516788259148598
iteration 276, loss = 0.0019215424545109272
iteration 277, loss = 0.0018688008422031999
iteration 278, loss = 0.0017721409676596522
iteration 279, loss = 0.0016764660831540823
iteration 280, loss = 0.0020276482682675123
iteration 281, loss = 0.0026061267126351595
iteration 282, loss = 0.0017717398004606366
iteration 283, loss = 0.001793733099475503
iteration 284, loss = 0.0017579342238605022
iteration 285, loss = 0.002254323335364461
iteration 286, loss = 0.0015908523928374052
iteration 287, loss = 0.0017888301517814398
iteration 288, loss = 0.0024610937107354403
iteration 289, loss = 0.0016507788095623255
iteration 290, loss = 0.0017292756820097566
iteration 291, loss = 0.0018300381489098072
iteration 292, loss = 0.004884141962975264
iteration 293, loss = 0.0016528654377907515
iteration 294, loss = 0.002759266644716263
iteration 295, loss = 0.0019359113648533821
iteration 296, loss = 0.0015756422653794289
iteration 297, loss = 0.001724316505715251
iteration 298, loss = 0.0017760110786184669
iteration 299, loss = 0.001701359753496945
iteration 0, loss = 0.0036603028420358896
iteration 1, loss = 0.0020938522648066282
iteration 2, loss = 0.0029045811388641596
iteration 3, loss = 0.003458768595010042
iteration 4, loss = 0.0016923602670431137
iteration 5, loss = 0.0019844952039420605
iteration 6, loss = 0.002541242865845561
iteration 7, loss = 0.0017413008026778698
iteration 8, loss = 0.001684107119217515
iteration 9, loss = 0.0020898664370179176
iteration 10, loss = 0.0017882593674585223
iteration 11, loss = 0.0029418589547276497
iteration 12, loss = 0.0018088833894580603
iteration 13, loss = 0.002909436821937561
iteration 14, loss = 0.0016484365332871675
iteration 15, loss = 0.0018128330120816827
iteration 16, loss = 0.0015908185159787536
iteration 17, loss = 0.004716087598353624
iteration 18, loss = 0.0017049915622919798
iteration 19, loss = 0.0019758702255785465
iteration 20, loss = 0.001889277482405305
iteration 21, loss = 0.0017653201939538121
iteration 22, loss = 0.0016913209110498428
iteration 23, loss = 0.0017676691059023142
iteration 24, loss = 0.001839487929828465
iteration 25, loss = 0.0025456398725509644
iteration 26, loss = 0.0020422094967216253
iteration 27, loss = 0.004657115787267685
iteration 28, loss = 0.0018262836383655667
iteration 29, loss = 0.0032393017318099737
iteration 30, loss = 0.0046997214667499065
iteration 31, loss = 0.004824191331863403
iteration 32, loss = 0.001641365815885365
iteration 33, loss = 0.0016982925590127707
iteration 34, loss = 0.0020875767804682255
iteration 35, loss = 0.0024767753202468157
iteration 36, loss = 0.0017873989418148994
iteration 37, loss = 0.0019677395466715097
iteration 38, loss = 0.0015897314297035336
iteration 39, loss = 0.0020541136618703604
iteration 40, loss = 0.0018053532112389803
iteration 41, loss = 0.0017240961315110326
iteration 42, loss = 0.002523536793887615
iteration 43, loss = 0.0015983021585270762
iteration 44, loss = 0.004694873932749033
iteration 45, loss = 0.0046629151329398155
iteration 46, loss = 0.002254695864394307
iteration 47, loss = 0.0018146863440051675
iteration 48, loss = 0.0016528544947504997
iteration 49, loss = 0.0017780460184440017
iteration 50, loss = 0.001858419505879283
iteration 51, loss = 0.0017926340224221349
iteration 52, loss = 0.0023871527519077063
iteration 53, loss = 0.0016756830736994743
iteration 54, loss = 0.00192619941662997
iteration 55, loss = 0.0018487898632884026
iteration 56, loss = 0.001686964649707079
iteration 57, loss = 0.0016772127710282803
iteration 58, loss = 0.0020142861176282167
iteration 59, loss = 0.0019081871723756194
iteration 60, loss = 0.0020356138702481985
iteration 61, loss = 0.001852291519753635
iteration 62, loss = 0.0016150432638823986
iteration 63, loss = 0.0018148919334635139
iteration 64, loss = 0.001627924619242549
iteration 65, loss = 0.004889805801212788
iteration 66, loss = 0.0017264907946810126
iteration 67, loss = 0.0027371894102543592
iteration 68, loss = 0.0019228860037401319
iteration 69, loss = 0.001713755540549755
iteration 70, loss = 0.001572625245898962
iteration 71, loss = 0.0019072818104177713
iteration 72, loss = 0.0017486499855294824
iteration 73, loss = 0.0017434513429179788
iteration 74, loss = 0.005713984835892916
iteration 75, loss = 0.002862524939700961
iteration 76, loss = 0.001851038308814168
iteration 77, loss = 0.0023825671523809433
iteration 78, loss = 0.0027175431605428457
iteration 79, loss = 0.002531870035454631
iteration 80, loss = 0.001625536591745913
iteration 81, loss = 0.0015893245581537485
iteration 82, loss = 0.0016228094464167953
iteration 83, loss = 0.0018028143094852567
iteration 84, loss = 0.0023832388687878847
iteration 85, loss = 0.001864867634139955
iteration 86, loss = 0.0017920637037605047
iteration 87, loss = 0.0016142004169523716
iteration 88, loss = 0.002190314931795001
iteration 89, loss = 0.0018920236034318805
iteration 90, loss = 0.0018834228394553065
iteration 91, loss = 0.001584744080901146
iteration 92, loss = 0.0023516430519521236
iteration 93, loss = 0.00252917199395597
iteration 94, loss = 0.0023251378443092108
iteration 95, loss = 0.0017984770238399506
iteration 96, loss = 0.0017310541588813066
iteration 97, loss = 0.0018069823272526264
iteration 98, loss = 0.0018466768087819219
iteration 99, loss = 0.0018147792434319854
iteration 100, loss = 0.0016531520523130894
iteration 101, loss = 0.0019282839493826032
iteration 102, loss = 0.0016396293649449944
iteration 103, loss = 0.0016259746626019478
iteration 104, loss = 0.0027539485599845648
iteration 105, loss = 0.0016847681254148483
iteration 106, loss = 0.0021844650618731976
iteration 107, loss = 0.001790763228200376
iteration 108, loss = 0.0015790779143571854
iteration 109, loss = 0.001787827117368579
iteration 110, loss = 0.0026006042025983334
iteration 111, loss = 0.0016137738712131977
iteration 112, loss = 0.0016113299643620849
iteration 113, loss = 0.001655795145779848
iteration 114, loss = 0.001872035558335483
iteration 115, loss = 0.0015447826590389013
iteration 116, loss = 0.0017990149790421128
iteration 117, loss = 0.0018418484833091497
iteration 118, loss = 0.0021294960752129555
iteration 119, loss = 0.0015838267281651497
iteration 120, loss = 0.001636275788769126
iteration 121, loss = 0.001787657500244677
iteration 122, loss = 0.0018186490051448345
iteration 123, loss = 0.00472464133054018
iteration 124, loss = 0.004740661941468716
iteration 125, loss = 0.0016515274764969945
iteration 126, loss = 0.001880960538983345
iteration 127, loss = 0.0018469315255060792
iteration 128, loss = 0.00169293989893049
iteration 129, loss = 0.001635326654650271
iteration 130, loss = 0.0023522537667304277
iteration 131, loss = 0.001681244233623147
iteration 132, loss = 0.0016493378207087517
iteration 133, loss = 0.001666574738919735
iteration 134, loss = 0.0016000228933990002
iteration 135, loss = 0.0017522766720503569
iteration 136, loss = 0.0016502593643963337
iteration 137, loss = 0.0017332449788227677
iteration 138, loss = 0.001648005680181086
iteration 139, loss = 0.0018266614060848951
iteration 140, loss = 0.0016065869713202119
iteration 141, loss = 0.0019650175236165524
iteration 142, loss = 0.0016766266198828816
iteration 143, loss = 0.0018035366665571928
iteration 144, loss = 0.0017952644266188145
iteration 145, loss = 0.0018224666127935052
iteration 146, loss = 0.0017482807161286473
iteration 147, loss = 0.0018203557701781392
iteration 148, loss = 0.0019210632890462875
iteration 149, loss = 0.0018195156008005142
iteration 150, loss = 0.0017358841141685843
iteration 151, loss = 0.0019869673997163773
iteration 152, loss = 0.0016938865883275867
iteration 153, loss = 0.0016776907723397017
iteration 154, loss = 0.0016396458959206939
iteration 155, loss = 0.001783324871212244
iteration 156, loss = 0.0016936528263613582
iteration 157, loss = 0.0020658804569393396
iteration 158, loss = 0.0018861581338569522
iteration 159, loss = 0.0016606723656877875
iteration 160, loss = 0.001576277893036604
iteration 161, loss = 0.0017861051019281149
iteration 162, loss = 0.0019762725569307804
iteration 163, loss = 0.0019429574022069573
iteration 164, loss = 0.00174473668448627
iteration 165, loss = 0.0016429766546934843
iteration 166, loss = 0.0021474305540323257
iteration 167, loss = 0.0016466202214360237
iteration 168, loss = 0.0016470137052237988
iteration 169, loss = 0.0019302560249343514
iteration 170, loss = 0.0016204166458919644
iteration 171, loss = 0.0017988645704463124
iteration 172, loss = 0.0016624749405309558
iteration 173, loss = 0.0016539418138563633
iteration 174, loss = 0.00168766884598881
iteration 175, loss = 0.0015337669756263494
iteration 176, loss = 0.0018738966900855303
iteration 177, loss = 0.0018655367894098163
iteration 178, loss = 0.0015545061323791742
iteration 179, loss = 0.004606199916452169
iteration 180, loss = 0.0018877532565966249
iteration 181, loss = 0.0015905374893918633
iteration 182, loss = 0.001589819323271513
iteration 183, loss = 0.001718894811347127
iteration 184, loss = 0.002259193453937769
iteration 185, loss = 0.001604136312380433
iteration 186, loss = 0.0020364741794764996
iteration 187, loss = 0.0017274628626182675
iteration 188, loss = 0.001686463481746614
iteration 189, loss = 0.0017094158101826906
iteration 190, loss = 0.0017028393922373652
iteration 191, loss = 0.0018841363489627838
iteration 192, loss = 0.0016204193234443665
iteration 193, loss = 0.0015560403699055314
iteration 194, loss = 0.001782826497219503
iteration 195, loss = 0.001632041996344924
iteration 196, loss = 0.0016701759304851294
iteration 197, loss = 0.002438133582472801
iteration 198, loss = 0.0016866946825757623
iteration 199, loss = 0.0017475339118391275
iteration 200, loss = 0.001604200340807438
iteration 201, loss = 0.0018260303186252713
iteration 202, loss = 0.001810454879887402
iteration 203, loss = 0.0016983167733997107
iteration 204, loss = 0.002281994791701436
iteration 205, loss = 0.0018387092277407646
iteration 206, loss = 0.0017180481227114797
iteration 207, loss = 0.001989081036299467
iteration 208, loss = 0.0017015484627336264
iteration 209, loss = 0.00177324831020087
iteration 210, loss = 0.0028034457936882973
iteration 211, loss = 0.0018833447247743607
iteration 212, loss = 0.001682319096289575
iteration 213, loss = 0.0015877197729423642
iteration 214, loss = 0.0017007588176056743
iteration 215, loss = 0.0017860380467027426
iteration 216, loss = 0.0018902828451246023
iteration 217, loss = 0.0016391188837587833
iteration 218, loss = 0.0016882296185940504
iteration 219, loss = 0.0015813170466572046
iteration 220, loss = 0.0016442062333226204
iteration 221, loss = 0.0017255069687962532
iteration 222, loss = 0.0015990646788850427
iteration 223, loss = 0.0019568789284676313
iteration 224, loss = 0.001599007286131382
iteration 225, loss = 0.0019279374973848462
iteration 226, loss = 0.0018663647351786494
iteration 227, loss = 0.0017526025185361505
iteration 228, loss = 0.0015472806990146637
iteration 229, loss = 0.0015778257511556149
iteration 230, loss = 0.0015803049318492413
iteration 231, loss = 0.0019635779317468405
iteration 232, loss = 0.0016743340529501438
iteration 233, loss = 0.0016507195541635156
iteration 234, loss = 0.0016841028118506074
iteration 235, loss = 0.0016254453221336007
iteration 236, loss = 0.0019507644465193152
iteration 237, loss = 0.0020859718788415194
iteration 238, loss = 0.0016274206573143601
iteration 239, loss = 0.0017821778310462832
iteration 240, loss = 0.0017330462578684092
iteration 241, loss = 0.0015863885637372732
iteration 242, loss = 0.0025169013533741236
iteration 243, loss = 0.0015507341595366597
iteration 244, loss = 0.0017678206786513329
iteration 245, loss = 0.0016384817427024245
iteration 246, loss = 0.0015908839413896203
iteration 247, loss = 0.0017335193697363138
iteration 248, loss = 0.0016153223114088178
iteration 249, loss = 0.00154921505600214
iteration 250, loss = 0.0075509315356612206
iteration 251, loss = 0.0015533417463302612
iteration 252, loss = 0.0016689144540578127
iteration 253, loss = 0.0016195192001760006
iteration 254, loss = 0.0017343491781502962
iteration 255, loss = 0.005616888403892517
iteration 256, loss = 0.0019105882383883
iteration 257, loss = 0.002299653133377433
iteration 258, loss = 0.0018206927925348282
iteration 259, loss = 0.0018624602816998959
iteration 260, loss = 0.002195814624428749
iteration 261, loss = 0.0017162758158519864
iteration 262, loss = 0.001774446340277791
iteration 263, loss = 0.001576534123159945
iteration 264, loss = 0.0016676608938723803
iteration 265, loss = 0.00234152446500957
iteration 266, loss = 0.002151309512555599
iteration 267, loss = 0.0029235512483865023
iteration 268, loss = 0.0017231650417670608
iteration 269, loss = 0.0017755732405930758
iteration 270, loss = 0.0017050399910658598
iteration 271, loss = 0.0015641365898773074
iteration 272, loss = 0.0026421411894261837
iteration 273, loss = 0.0014876991044729948
iteration 274, loss = 0.0015424733282998204
iteration 275, loss = 0.001552717643789947
iteration 276, loss = 0.0016735022654756904
iteration 277, loss = 0.0018775473581627011
iteration 278, loss = 0.0016238249372690916
iteration 279, loss = 0.0016029721591621637
iteration 280, loss = 0.0015900079160928726
iteration 281, loss = 0.0015596045413985848
iteration 282, loss = 0.0015565495705232024
iteration 283, loss = 0.001639566384255886
iteration 284, loss = 0.0015960917808115482
iteration 285, loss = 0.0018202777719125152
iteration 286, loss = 0.0016705230809748173
iteration 287, loss = 0.004482743795961142
iteration 288, loss = 0.002513539744541049
iteration 289, loss = 0.0016839588060975075
iteration 290, loss = 0.0014613126404583454
iteration 291, loss = 0.0016112872399389744
iteration 292, loss = 0.0016310455976054072
iteration 293, loss = 0.0015979697927832603
iteration 294, loss = 0.0014421064406633377
iteration 295, loss = 0.0017459909431636333
iteration 296, loss = 0.0019570961594581604
iteration 297, loss = 0.0015125336358323693
iteration 298, loss = 0.002526717958971858
iteration 299, loss = 0.0015068885404616594
iteration 0, loss = 0.0015853444347158074
iteration 1, loss = 0.0015424248995259404
iteration 2, loss = 0.0016718875849619508
iteration 3, loss = 0.0016442437190562487
iteration 4, loss = 0.0045490083284676075
iteration 5, loss = 0.0017936477670446038
iteration 6, loss = 0.0020905358251184225
iteration 7, loss = 0.0016329193022102118
iteration 8, loss = 0.001642225426621735
iteration 9, loss = 0.0017788088880479336
iteration 10, loss = 0.0017281160689890385
iteration 11, loss = 0.002528152195736766
iteration 12, loss = 0.001517490716651082
iteration 13, loss = 0.0017065927386283875
iteration 14, loss = 0.0017807487165555358
iteration 15, loss = 0.0017001456581056118
iteration 16, loss = 0.002571455668658018
iteration 17, loss = 0.0016610509483143687
iteration 18, loss = 0.0026649695355445147
iteration 19, loss = 0.001593013177625835
iteration 20, loss = 0.0016571155283600092
iteration 21, loss = 0.0018453047377988696
iteration 22, loss = 0.0015041467268019915
iteration 23, loss = 0.001591379870660603
iteration 24, loss = 0.0015788782620802522
iteration 25, loss = 0.0022631653118878603
iteration 26, loss = 0.001787985791452229
iteration 27, loss = 0.001519494573585689
iteration 28, loss = 0.0020727866794914007
iteration 29, loss = 0.001799924299120903
iteration 30, loss = 0.0015281118685379624
iteration 31, loss = 0.0016098505584523082
iteration 32, loss = 0.0020900771487504244
iteration 33, loss = 0.0016504754312336445
iteration 34, loss = 0.0016614842461422086
iteration 35, loss = 0.002589388517662883
iteration 36, loss = 0.0022753437515348196
iteration 37, loss = 0.0025816087145358324
iteration 38, loss = 0.0016737746773287654
iteration 39, loss = 0.00159862928558141
iteration 40, loss = 0.0016077135223895311
iteration 41, loss = 0.001716474536806345
iteration 42, loss = 0.001794569194316864
iteration 43, loss = 0.0015653519658371806
iteration 44, loss = 0.004336823709309101
iteration 45, loss = 0.001540578668937087
iteration 46, loss = 0.0015104528283700347
iteration 47, loss = 0.001553259207867086
iteration 48, loss = 0.0015558942686766386
iteration 49, loss = 0.0017595964018255472
iteration 50, loss = 0.0019596507772803307
iteration 51, loss = 0.001695940736681223
iteration 52, loss = 0.004306145943701267
iteration 53, loss = 0.004471560940146446
iteration 54, loss = 0.001985637005418539
iteration 55, loss = 0.0016298138070851564
iteration 56, loss = 0.0015921532176434994
iteration 57, loss = 0.0016424146015197039
iteration 58, loss = 0.00151335378177464
iteration 59, loss = 0.00268548633903265
iteration 60, loss = 0.0014930700417608023
iteration 61, loss = 0.001560433185659349
iteration 62, loss = 0.0015280323568731546
iteration 63, loss = 0.001537435338832438
iteration 64, loss = 0.0017660250887274742
iteration 65, loss = 0.0015593739226460457
iteration 66, loss = 0.0016834076959639788
iteration 67, loss = 0.0015769428573548794
iteration 68, loss = 0.002012792741879821
iteration 69, loss = 0.0016615090426057577
iteration 70, loss = 0.0015433006919920444
iteration 71, loss = 0.0017462141113355756
iteration 72, loss = 0.001498839701525867
iteration 73, loss = 0.001431862125173211
iteration 74, loss = 0.0022153561003506184
iteration 75, loss = 0.002453932538628578
iteration 76, loss = 0.001611719955690205
iteration 77, loss = 0.0016623811097815633
iteration 78, loss = 0.0015656702453270555
iteration 79, loss = 0.001579040545038879
iteration 80, loss = 0.0016818479634821415
iteration 81, loss = 0.0018348782323300838
iteration 82, loss = 0.0028628897853195667
iteration 83, loss = 0.0016513274749740958
iteration 84, loss = 0.001578997471369803
iteration 85, loss = 0.001497170771472156
iteration 86, loss = 0.001641965238377452
iteration 87, loss = 0.0015289924340322614
iteration 88, loss = 0.0015957859577611089
iteration 89, loss = 0.001523601240478456
iteration 90, loss = 0.0044120908714830875
iteration 91, loss = 0.0014923634007573128
iteration 92, loss = 0.0018400062108412385
iteration 93, loss = 0.0016217721858993173
iteration 94, loss = 0.0014635365223512053
iteration 95, loss = 0.0015223721275106072
iteration 96, loss = 0.0016490540001541376
iteration 97, loss = 0.0016802630852907896
iteration 98, loss = 0.0018492366652935743
iteration 99, loss = 0.0021642872598022223
iteration 100, loss = 0.001551913213916123
iteration 101, loss = 0.0016525802202522755
iteration 102, loss = 0.0016355370171368122
iteration 103, loss = 0.0018164683133363724
iteration 104, loss = 0.001713750185444951
iteration 105, loss = 0.0016541966469958425
iteration 106, loss = 0.0018871370702981949
iteration 107, loss = 0.0014985797461122274
iteration 108, loss = 0.0014922230038791895
iteration 109, loss = 0.002184669952839613
iteration 110, loss = 0.0015362061094492674
iteration 111, loss = 0.0016830827808007598
iteration 112, loss = 0.0018478515557944775
iteration 113, loss = 0.0015324309933930635
iteration 114, loss = 0.0017368498956784606
iteration 115, loss = 0.0019628091249614954
iteration 116, loss = 0.004284930415451527
iteration 117, loss = 0.0021631147246807814
iteration 118, loss = 0.0015194460283964872
iteration 119, loss = 0.0018690096912905574
iteration 120, loss = 0.0017706657527014613
iteration 121, loss = 0.0015470831422135234
iteration 122, loss = 0.001682679750956595
iteration 123, loss = 0.00160941481590271
iteration 124, loss = 0.0017381156794726849
iteration 125, loss = 0.001509814290329814
iteration 126, loss = 0.004447022452950478
iteration 127, loss = 0.0018242117948830128
iteration 128, loss = 0.002456596354022622
iteration 129, loss = 0.0017563443398103118
iteration 130, loss = 0.002195121720433235
iteration 131, loss = 0.0015688247513026
iteration 132, loss = 0.0043906900100409985
iteration 133, loss = 0.001644765492528677
iteration 134, loss = 0.0014552688226103783
iteration 135, loss = 0.0019721307326108217
iteration 136, loss = 0.0017069223104044795
iteration 137, loss = 0.0014629598008468747
iteration 138, loss = 0.0015682050725445151
iteration 139, loss = 0.0014680359745398164
iteration 140, loss = 0.0018822518177330494
iteration 141, loss = 0.0015961958561092615
iteration 142, loss = 0.0015110505046322942
iteration 143, loss = 0.001540245022624731
iteration 144, loss = 0.0015463253948837519
iteration 145, loss = 0.0018986386712640524
iteration 146, loss = 0.0016025544609874487
iteration 147, loss = 0.0016327885678038
iteration 148, loss = 0.0015913737006485462
iteration 149, loss = 0.0016096881590783596
iteration 150, loss = 0.0016118253115564585
iteration 151, loss = 0.0015658342745155096
iteration 152, loss = 0.0015114230336621404
iteration 153, loss = 0.0014370015123859048
iteration 154, loss = 0.0019066205713897943
iteration 155, loss = 0.0014335596933960915
iteration 156, loss = 0.001717585721053183
iteration 157, loss = 0.002282256493344903
iteration 158, loss = 0.004250465892255306
iteration 159, loss = 0.001489237998612225
iteration 160, loss = 0.001789512112736702
iteration 161, loss = 0.002666743705049157
iteration 162, loss = 0.0020636713597923517
iteration 163, loss = 0.002662490587681532
iteration 164, loss = 0.001499349600635469
iteration 165, loss = 0.0014883463736623526
iteration 166, loss = 0.0014738041209056973
iteration 167, loss = 0.0015044377651065588
iteration 168, loss = 0.0018518324941396713
iteration 169, loss = 0.002250493271276355
iteration 170, loss = 0.00524779362604022
iteration 171, loss = 0.0016689291223883629
iteration 172, loss = 0.0017917202785611153
iteration 173, loss = 0.001502273604273796
iteration 174, loss = 0.0015278080245479941
iteration 175, loss = 0.0016173992771655321
iteration 176, loss = 0.0014734782744199038
iteration 177, loss = 0.00156165671069175
iteration 178, loss = 0.0015871592331677675
iteration 179, loss = 0.0016454635187983513
iteration 180, loss = 0.0014554373919963837
iteration 181, loss = 0.0022856644354760647
iteration 182, loss = 0.0016066085081547499
iteration 183, loss = 0.0014698579907417297
iteration 184, loss = 0.0017439662478864193
iteration 185, loss = 0.0019325383473187685
iteration 186, loss = 0.001914985361509025
iteration 187, loss = 0.001510482281446457
iteration 188, loss = 0.0017801513895392418
iteration 189, loss = 0.0014846909325569868
iteration 190, loss = 0.0015869933413341641
iteration 191, loss = 0.0016138311475515366
iteration 192, loss = 0.0014378110645338893
iteration 193, loss = 0.0014277728041633964
iteration 194, loss = 0.001710372045636177
iteration 195, loss = 0.0015681057702749968
iteration 196, loss = 0.001691320794634521
iteration 197, loss = 0.0017141038551926613
iteration 198, loss = 0.002882190514355898
iteration 199, loss = 0.0017054997151717544
iteration 200, loss = 0.001743584405630827
iteration 201, loss = 0.0017048718873411417
iteration 202, loss = 0.0028879696037620306
iteration 203, loss = 0.0016323707532137632
iteration 204, loss = 0.0016294372035190463
iteration 205, loss = 0.0015055445255711675
iteration 206, loss = 0.0015997050795704126
iteration 207, loss = 0.001412064186297357
iteration 208, loss = 0.0015694282483309507
iteration 209, loss = 0.0015471121296286583
iteration 210, loss = 0.0016072341240942478
iteration 211, loss = 0.0016186453867703676
iteration 212, loss = 0.0015517560532316566
iteration 213, loss = 0.0021343226544559
iteration 214, loss = 0.0016573142493143678
iteration 215, loss = 0.004166027996689081
iteration 216, loss = 0.004338855855166912
iteration 217, loss = 0.0017443601973354816
iteration 218, loss = 0.001558696269057691
iteration 219, loss = 0.001574825611896813
iteration 220, loss = 0.001811609836295247
iteration 221, loss = 0.0017577679827809334
iteration 222, loss = 0.0014801917131990194
iteration 223, loss = 0.0015060588484629989
iteration 224, loss = 0.0050873286090791225
iteration 225, loss = 0.0017426019767299294
iteration 226, loss = 0.001565741142258048
iteration 227, loss = 0.0016468012472614646
iteration 228, loss = 0.0025169379077851772
iteration 229, loss = 0.0016467757523059845
iteration 230, loss = 0.001986530376598239
iteration 231, loss = 0.0015924656763672829
iteration 232, loss = 0.0041814385913312435
iteration 233, loss = 0.0015182752395048738
iteration 234, loss = 0.0015740602975711226
iteration 235, loss = 0.001637508743442595
iteration 236, loss = 0.0014935522340238094
iteration 237, loss = 0.0014744929503649473
iteration 238, loss = 0.0014642849564552307
iteration 239, loss = 0.002373417606577277
iteration 240, loss = 0.0015541743487119675
iteration 241, loss = 0.0014699107268825173
iteration 242, loss = 0.0016676434315741062
iteration 243, loss = 0.0015633285511285067
iteration 244, loss = 0.0015113700646907091
iteration 245, loss = 0.0018142098560929298
iteration 246, loss = 0.0013765365583822131
iteration 247, loss = 0.00246070371940732
iteration 248, loss = 0.0014449972659349442
iteration 249, loss = 0.0016587062273174524
iteration 250, loss = 0.0016502285143360496
iteration 251, loss = 0.0014997825492173433
iteration 252, loss = 0.001549517852254212
iteration 253, loss = 0.0014241439057514071
iteration 254, loss = 0.0016344290925189853
iteration 255, loss = 0.001533576869405806
iteration 256, loss = 0.0014084905851632357
iteration 257, loss = 0.0017720512114465237
iteration 258, loss = 0.0016100715147331357
iteration 259, loss = 0.002385626779869199
iteration 260, loss = 0.0016639423556625843
iteration 261, loss = 0.0016355797415599227
iteration 262, loss = 0.0016851226100698113
iteration 263, loss = 0.0016402285546064377
iteration 264, loss = 0.0015281448140740395
iteration 265, loss = 0.0017045174026861787
iteration 266, loss = 0.0015800732653588057
iteration 267, loss = 0.0016237730160355568
iteration 268, loss = 0.0016656750813126564
iteration 269, loss = 0.001706330687738955
iteration 270, loss = 0.0014451496535912156
iteration 271, loss = 0.0014662243193015456
iteration 272, loss = 0.0025070812553167343
iteration 273, loss = 0.002761853625997901
iteration 274, loss = 0.0014915128704160452
iteration 275, loss = 0.002088007749989629
iteration 276, loss = 0.001413924153894186
iteration 277, loss = 0.0016714000375941396
iteration 278, loss = 0.001419906853698194
iteration 279, loss = 0.00145439722109586
iteration 280, loss = 0.0018066669581457973
iteration 281, loss = 0.0017442865064367652
iteration 282, loss = 0.001612870953977108
iteration 283, loss = 0.0018873604713007808
iteration 284, loss = 0.0014640992740169168
iteration 285, loss = 0.0014531147899106145
iteration 286, loss = 0.0014857540372759104
iteration 287, loss = 0.002417630283161998
iteration 288, loss = 0.0014473464107140899
iteration 289, loss = 0.0014876334462314844
iteration 290, loss = 0.0041808015666902065
iteration 291, loss = 0.0014147671172395349
iteration 292, loss = 0.001499450416304171
iteration 293, loss = 0.001648555975407362
iteration 294, loss = 0.0015956740826368332
iteration 295, loss = 0.002451767213642597
iteration 296, loss = 0.0017188619822263718
iteration 297, loss = 0.0015036646509543061
iteration 298, loss = 0.0014659164007753134
iteration 299, loss = 0.0016787918284535408
iteration 0, loss = 0.002238870831206441
iteration 1, loss = 0.0016624900745227933
iteration 2, loss = 0.0013492421712726355
iteration 3, loss = 0.0016738896956667304
iteration 4, loss = 0.00147428666241467
iteration 5, loss = 0.002192710991948843
iteration 6, loss = 0.002329103881493211
iteration 7, loss = 0.001630918006412685
iteration 8, loss = 0.0014445919077843428
iteration 9, loss = 0.0018589763203635812
iteration 10, loss = 0.0016154812183231115
iteration 11, loss = 0.0016177284996956587
iteration 12, loss = 0.0014017117209732533
iteration 13, loss = 0.0016794392140582204
iteration 14, loss = 0.0021793367341160774
iteration 15, loss = 0.0015093924012035131
iteration 16, loss = 0.0019260412082076073
iteration 17, loss = 0.0015111329266801476
iteration 18, loss = 0.0014649725053459406
iteration 19, loss = 0.002026232657954097
iteration 20, loss = 0.0015144277131184936
iteration 21, loss = 0.0016207866137847304
iteration 22, loss = 0.0016159741207957268
iteration 23, loss = 0.0016325496835634112
iteration 24, loss = 0.004144343081861734
iteration 25, loss = 0.0017478372901678085
iteration 26, loss = 0.004365390166640282
iteration 27, loss = 0.0015418381663039327
iteration 28, loss = 0.004744627978652716
iteration 29, loss = 0.0016985968686640263
iteration 30, loss = 0.001467253197915852
iteration 31, loss = 0.0015731611056253314
iteration 32, loss = 0.0015035243704915047
iteration 33, loss = 0.0015448275953531265
iteration 34, loss = 0.0014557032845914364
iteration 35, loss = 0.001709122327156365
iteration 36, loss = 0.0014827889390289783
iteration 37, loss = 0.001979290274903178
iteration 38, loss = 0.0014624379109591246
iteration 39, loss = 0.001538064330816269
iteration 40, loss = 0.0016838895389810205
iteration 41, loss = 0.001440458232536912
iteration 42, loss = 0.001743155182339251
iteration 43, loss = 0.0015344253042712808
iteration 44, loss = 0.0015311792958527803
iteration 45, loss = 0.001403211965225637
iteration 46, loss = 0.0014841780066490173
iteration 47, loss = 0.0016443334752693772
iteration 48, loss = 0.0015965593047440052
iteration 49, loss = 0.0014110960764810443
iteration 50, loss = 0.0015249063726514578
iteration 51, loss = 0.0016218596138060093
iteration 52, loss = 0.0015661255456507206
iteration 53, loss = 0.0016986113041639328
iteration 54, loss = 0.0013979736249893904
iteration 55, loss = 0.0026566998567432165
iteration 56, loss = 0.0017557907849550247
iteration 57, loss = 0.0013769279466941953
iteration 58, loss = 0.0016018389724195004
iteration 59, loss = 0.0013776698615401983
iteration 60, loss = 0.0013708542101085186
iteration 61, loss = 0.0016969057032838464
iteration 62, loss = 0.0039972830563783646
iteration 63, loss = 0.0013933039736002684
iteration 64, loss = 0.001456935191527009
iteration 65, loss = 0.0015376206720247865
iteration 66, loss = 0.0016063291113823652
iteration 67, loss = 0.0015242345398291945
iteration 68, loss = 0.0018791045295074582
iteration 69, loss = 0.002811606042087078
iteration 70, loss = 0.0013942193472757936
iteration 71, loss = 0.0014541142154484987
iteration 72, loss = 0.0015017013065516949
iteration 73, loss = 0.0013933514710515738
iteration 74, loss = 0.0023881825618445873
iteration 75, loss = 0.0014515684451907873
iteration 76, loss = 0.0019694133661687374
iteration 77, loss = 0.0016452004201710224
iteration 78, loss = 0.001546019921079278
iteration 79, loss = 0.0014254353009164333
iteration 80, loss = 0.0015259749488905072
iteration 81, loss = 0.0017942204140126705
iteration 82, loss = 0.0015858832048252225
iteration 83, loss = 0.004031766206026077
iteration 84, loss = 0.0066213710233569145
iteration 85, loss = 0.0014122779248282313
iteration 86, loss = 0.0016077172476798296
iteration 87, loss = 0.001518099568784237
iteration 88, loss = 0.001386607880704105
iteration 89, loss = 0.0015180986374616623
iteration 90, loss = 0.002216561697423458
iteration 91, loss = 0.0013996377820149064
iteration 92, loss = 0.0014452929608523846
iteration 93, loss = 0.0016450071707367897
iteration 94, loss = 0.001898465445265174
iteration 95, loss = 0.0014612601371482015
iteration 96, loss = 0.001683101523667574
iteration 97, loss = 0.001418255502358079
iteration 98, loss = 0.0013920287601649761
iteration 99, loss = 0.0014896163484081626
iteration 100, loss = 0.0018040875438600779
iteration 101, loss = 0.0017812621081247926
iteration 102, loss = 0.0015852354699745774
iteration 103, loss = 0.0015515913255512714
iteration 104, loss = 0.0013932338915765285
iteration 105, loss = 0.0014304202049970627
iteration 106, loss = 0.0014576850226148963
iteration 107, loss = 0.0015974347479641438
iteration 108, loss = 0.0015248052077367902
iteration 109, loss = 0.0016211702022701502
iteration 110, loss = 0.003950890619307756
iteration 111, loss = 0.001672494923695922
iteration 112, loss = 0.0015191700076684356
iteration 113, loss = 0.0013258567778393626
iteration 114, loss = 0.0015499847941100597
iteration 115, loss = 0.001670419704169035
iteration 116, loss = 0.0015462434384971857
iteration 117, loss = 0.0013490747660398483
iteration 118, loss = 0.0015877094119787216
iteration 119, loss = 0.0014871498569846153
iteration 120, loss = 0.0027388697490096092
iteration 121, loss = 0.0018954016268253326
iteration 122, loss = 0.0017008380964398384
iteration 123, loss = 0.003228928195312619
iteration 124, loss = 0.0014399354113265872
iteration 125, loss = 0.0014217722928151488
iteration 126, loss = 0.0015474623069167137
iteration 127, loss = 0.0016486194217577577
iteration 128, loss = 0.0014927645679563284
iteration 129, loss = 0.001508898101747036
iteration 130, loss = 0.0013515055179595947
iteration 131, loss = 0.0014957308303564787
iteration 132, loss = 0.0020292585249990225
iteration 133, loss = 0.0020470810122787952
iteration 134, loss = 0.0016297685215249658
iteration 135, loss = 0.001588522340171039
iteration 136, loss = 0.001453437260352075
iteration 137, loss = 0.00234777107834816
iteration 138, loss = 0.0014456554781645536
iteration 139, loss = 0.0014757360331714153
iteration 140, loss = 0.0013908070977777243
iteration 141, loss = 0.0017948164604604244
iteration 142, loss = 0.0016336038243025541
iteration 143, loss = 0.0013929856941103935
iteration 144, loss = 0.0016060542548075318
iteration 145, loss = 0.001462640706449747
iteration 146, loss = 0.004200434312224388
iteration 147, loss = 0.0014660292072221637
iteration 148, loss = 0.002261132001876831
iteration 149, loss = 0.001731071388348937
iteration 150, loss = 0.0015439052367582917
iteration 151, loss = 0.0020530179608613253
iteration 152, loss = 0.004303252324461937
iteration 153, loss = 0.004134533926844597
iteration 154, loss = 0.001612495630979538
iteration 155, loss = 0.002251559868454933
iteration 156, loss = 0.001358362496830523
iteration 157, loss = 0.0015556995058432221
iteration 158, loss = 0.0014742222847416997
iteration 159, loss = 0.002176132518798113
iteration 160, loss = 0.001567136961966753
iteration 161, loss = 0.0014122474240139127
iteration 162, loss = 0.001349904341623187
iteration 163, loss = 0.0014958269894123077
iteration 164, loss = 0.0014187573688104749
iteration 165, loss = 0.0017244280315935612
iteration 166, loss = 0.001550117158330977
iteration 167, loss = 0.0015263527166098356
iteration 168, loss = 0.0015083972830325365
iteration 169, loss = 0.0014222210738807917
iteration 170, loss = 0.001467274152673781
iteration 171, loss = 0.0015597151359543204
iteration 172, loss = 0.0024247015826404095
iteration 173, loss = 0.0013726914767175913
iteration 174, loss = 0.0015852577053010464
iteration 175, loss = 0.0014440773520618677
iteration 176, loss = 0.001475078403018415
iteration 177, loss = 0.0013577251229435205
iteration 178, loss = 0.0015548844821751118
iteration 179, loss = 0.0017247272189706564
iteration 180, loss = 0.0014997611287981272
iteration 181, loss = 0.001537927659228444
iteration 182, loss = 0.0017342522041872144
iteration 183, loss = 0.0014483226696029305
iteration 184, loss = 0.00161618588026613
iteration 185, loss = 0.0013682673452422023
iteration 186, loss = 0.0013991049490869045
iteration 187, loss = 0.0013888587709516287
iteration 188, loss = 0.0020534060895442963
iteration 189, loss = 0.0013944829115644097
iteration 190, loss = 0.0013916114112362266
iteration 191, loss = 0.001443992368876934
iteration 192, loss = 0.003931635059416294
iteration 193, loss = 0.0020054641645401716
iteration 194, loss = 0.0015326113207265735
iteration 195, loss = 0.0015509354416280985
iteration 196, loss = 0.0016271163476631045
iteration 197, loss = 0.0016226830193772912
iteration 198, loss = 0.001490705064497888
iteration 199, loss = 0.0014351633144542575
iteration 200, loss = 0.0013869035756215453
iteration 201, loss = 0.0013331029331311584
iteration 202, loss = 0.0016154964687302709
iteration 203, loss = 0.002832693513482809
iteration 204, loss = 0.0013488831464201212
iteration 205, loss = 0.00140901911072433
iteration 206, loss = 0.0013636091025546193
iteration 207, loss = 0.0015748490113765001
iteration 208, loss = 0.0014556348323822021
iteration 209, loss = 0.0015867759939283133
iteration 210, loss = 0.0014467407017946243
iteration 211, loss = 0.0014127889880910516
iteration 212, loss = 0.0014179442077875137
iteration 213, loss = 0.001408528769388795
iteration 214, loss = 0.001353992847725749
iteration 215, loss = 0.0015695556066930294
iteration 216, loss = 0.001573679386638105
iteration 217, loss = 0.0023249457590281963
iteration 218, loss = 0.0014313246356323361
iteration 219, loss = 0.0014347806572914124
iteration 220, loss = 0.0015031028306111693
iteration 221, loss = 0.0013823932968080044
iteration 222, loss = 0.0013357580173760653
iteration 223, loss = 0.00390743650496006
iteration 224, loss = 0.0014096879167482257
iteration 225, loss = 0.0013606167631223798
iteration 226, loss = 0.0016360346926376224
iteration 227, loss = 0.0015744449337944388
iteration 228, loss = 0.001654912601225078
iteration 229, loss = 0.0013903541257604957
iteration 230, loss = 0.0016265924787148833
iteration 231, loss = 0.0013801377499476075
iteration 232, loss = 0.001448892056941986
iteration 233, loss = 0.0019578426145017147
iteration 234, loss = 0.0014639297733083367
iteration 235, loss = 0.0019114072201773524
iteration 236, loss = 0.001616969588212669
iteration 237, loss = 0.001520157209597528
iteration 238, loss = 0.0016303496668115258
iteration 239, loss = 0.0013712800573557615
iteration 240, loss = 0.0020526819862425327
iteration 241, loss = 0.0022182997781783342
iteration 242, loss = 0.0015116907889023423
iteration 243, loss = 0.0013862783089280128
iteration 244, loss = 0.0024451566860079765
iteration 245, loss = 0.0015858979895710945
iteration 246, loss = 0.0014535067602992058
iteration 247, loss = 0.00135591602884233
iteration 248, loss = 0.0013621979160234332
iteration 249, loss = 0.0013408912345767021
iteration 250, loss = 0.0016364181647077203
iteration 251, loss = 0.0014938366366550326
iteration 252, loss = 0.001405062503181398
iteration 253, loss = 0.0015378009993582964
iteration 254, loss = 0.0013028570683673024
iteration 255, loss = 0.0013239389518275857
iteration 256, loss = 0.0013574655167758465
iteration 257, loss = 0.002026029396802187
iteration 258, loss = 0.0014795288443565369
iteration 259, loss = 0.001331734354607761
iteration 260, loss = 0.0015400173142552376
iteration 261, loss = 0.0013577131321653724
iteration 262, loss = 0.002205966040492058
iteration 263, loss = 0.0016439617611467838
iteration 264, loss = 0.0016527933767065406
iteration 265, loss = 0.001668982789851725
iteration 266, loss = 0.00141411111690104
iteration 267, loss = 0.001333833672106266
iteration 268, loss = 0.0013761783484369516
iteration 269, loss = 0.0015974292764440179
iteration 270, loss = 0.0015830460470169783
iteration 271, loss = 0.0014476219657808542
iteration 272, loss = 0.0015325162094086409
iteration 273, loss = 0.0015669388230890036
iteration 274, loss = 0.0015782720874994993
iteration 275, loss = 0.001254497328773141
iteration 276, loss = 0.002306143520399928
iteration 277, loss = 0.0015290082665160298
iteration 278, loss = 0.0015886396868154407
iteration 279, loss = 0.001529515953734517
iteration 280, loss = 0.002337027806788683
iteration 281, loss = 0.001399310422129929
iteration 282, loss = 0.0014275007415562868
iteration 283, loss = 0.0012090462259948254
iteration 284, loss = 0.0015477624256163836
iteration 285, loss = 0.003866472514346242
iteration 286, loss = 0.001565890503115952
iteration 287, loss = 0.0013142346870154142
iteration 288, loss = 0.001454844605177641
iteration 289, loss = 0.002328684786334634
iteration 290, loss = 0.0038246922194957733
iteration 291, loss = 0.0013964739628136158
iteration 292, loss = 0.002524693962186575
iteration 293, loss = 0.0013636283110827208
iteration 294, loss = 0.0014545804588124156
iteration 295, loss = 0.0013340223813429475
iteration 296, loss = 0.00138275814242661
iteration 297, loss = 0.0014209762448444963
iteration 298, loss = 0.00147759891115129
iteration 299, loss = 0.0013734231470152736
iteration 0, loss = 0.003859645687043667
iteration 1, loss = 0.0015270585427060723
iteration 2, loss = 0.0015442266594618559
iteration 3, loss = 0.0015269153518602252
iteration 4, loss = 0.0014794424641877413
iteration 5, loss = 0.0015169616090133786
iteration 6, loss = 0.0013868095120415092
iteration 7, loss = 0.0019880635663866997
iteration 8, loss = 0.0020859939977526665
iteration 9, loss = 0.0014852697495371103
iteration 10, loss = 0.0016990620642900467
iteration 11, loss = 0.0013988105347380042
iteration 12, loss = 0.002071926835924387
iteration 13, loss = 0.0013136822963133454
iteration 14, loss = 0.001608590828254819
iteration 15, loss = 0.0013481967616826296
iteration 16, loss = 0.0013648836174979806
iteration 17, loss = 0.0014637470012530684
iteration 18, loss = 0.001465409528464079
iteration 19, loss = 0.0014907480217516422
iteration 20, loss = 0.0014169251080602407
iteration 21, loss = 0.0017852724995464087
iteration 22, loss = 0.002518542343750596
iteration 23, loss = 0.0024107913486659527
iteration 24, loss = 0.002269547199830413
iteration 25, loss = 0.0013781450688838959
iteration 26, loss = 0.001914063235744834
iteration 27, loss = 0.0037924321368336678
iteration 28, loss = 0.0013382036704570055
iteration 29, loss = 0.0014400463551282883
iteration 30, loss = 0.0014922063564881682
iteration 31, loss = 0.0014189562061801553
iteration 32, loss = 0.0014160029822960496
iteration 33, loss = 0.0015041425358504057
iteration 34, loss = 0.0015277910279110074
iteration 35, loss = 0.0014081523986533284
iteration 36, loss = 0.0013481199275702238
iteration 37, loss = 0.003813960822299123
iteration 38, loss = 0.0014195615658536553
iteration 39, loss = 0.001324369222857058
iteration 40, loss = 0.0016262660501524806
iteration 41, loss = 0.0020731298718601465
iteration 42, loss = 0.0017496871296316385
iteration 43, loss = 0.0014659990556538105
iteration 44, loss = 0.0019378851866349578
iteration 45, loss = 0.0015120516764000058
iteration 46, loss = 0.0013654286740347743
iteration 47, loss = 0.0013685685116797686
iteration 48, loss = 0.0013358155265450478
iteration 49, loss = 0.0017004896653816104
iteration 50, loss = 0.0022779330611228943
iteration 51, loss = 0.0015683651436120272
iteration 52, loss = 0.0014150692149996758
iteration 53, loss = 0.0020495709031820297
iteration 54, loss = 0.0013701413990929723
iteration 55, loss = 0.0014149388298392296
iteration 56, loss = 0.0013214797945693135
iteration 57, loss = 0.0020334485452622175
iteration 58, loss = 0.0014443515101447701
iteration 59, loss = 0.0016172613250091672
iteration 60, loss = 0.0015635443851351738
iteration 61, loss = 0.001716874074190855
iteration 62, loss = 0.0013825276400893927
iteration 63, loss = 0.004043524153530598
iteration 64, loss = 0.0013633047929033637
iteration 65, loss = 0.0015802033012732863
iteration 66, loss = 0.0014527118764817715
iteration 67, loss = 0.001499229809269309
iteration 68, loss = 0.001480972277931869
iteration 69, loss = 0.0014841810334473848
iteration 70, loss = 0.0012764518614858389
iteration 71, loss = 0.0014106667367741466
iteration 72, loss = 0.0013658296084031463
iteration 73, loss = 0.0014704515924677253
iteration 74, loss = 0.0015590357361361384
iteration 75, loss = 0.001423937501385808
iteration 76, loss = 0.001484379405155778
iteration 77, loss = 0.00148990866728127
iteration 78, loss = 0.0014788067201152444
iteration 79, loss = 0.0012552875559777021
iteration 80, loss = 0.001325925113633275
iteration 81, loss = 0.001483672414906323
iteration 82, loss = 0.0014325898373499513
iteration 83, loss = 0.0013986016856506467
iteration 84, loss = 0.001332395593635738
iteration 85, loss = 0.0014817274641245604
iteration 86, loss = 0.0017941058613359928
iteration 87, loss = 0.0014948161551728845
iteration 88, loss = 0.0014593929518014193
iteration 89, loss = 0.0013903110520914197
iteration 90, loss = 0.001412425423040986
iteration 91, loss = 0.0012911994708701968
iteration 92, loss = 0.0015097814612090588
iteration 93, loss = 0.001317370100878179
iteration 94, loss = 0.0013438659952953458
iteration 95, loss = 0.001370969577692449
iteration 96, loss = 0.0022818641737103462
iteration 97, loss = 0.0018442822620272636
iteration 98, loss = 0.001642029033973813
iteration 99, loss = 0.001368753844872117
iteration 100, loss = 0.0014925810974091291
iteration 101, loss = 0.0014803444501012564
iteration 102, loss = 0.0017120370175689459
iteration 103, loss = 0.0013593898620456457
iteration 104, loss = 0.0012898638378828764
iteration 105, loss = 0.0014823487726971507
iteration 106, loss = 0.0014444179832935333
iteration 107, loss = 0.001305098063312471
iteration 108, loss = 0.0014457122888416052
iteration 109, loss = 0.002152211731299758
iteration 110, loss = 0.001348990248516202
iteration 111, loss = 0.0012830827618017793
iteration 112, loss = 0.0020172453951090574
iteration 113, loss = 0.0037126755341887474
iteration 114, loss = 0.0024563330225646496
iteration 115, loss = 0.0016826645005494356
iteration 116, loss = 0.0015136456349864602
iteration 117, loss = 0.0014293079730123281
iteration 118, loss = 0.001350349746644497
iteration 119, loss = 0.0012570539256557822
iteration 120, loss = 0.0013380395248532295
iteration 121, loss = 0.002369857858866453
iteration 122, loss = 0.0012441108701750636
iteration 123, loss = 0.0013497285544872284
iteration 124, loss = 0.001434979378245771
iteration 125, loss = 0.0013372042449191213
iteration 126, loss = 0.0013721329160034657
iteration 127, loss = 0.0014786010142415762
iteration 128, loss = 0.001321764080785215
iteration 129, loss = 0.0013664067955687642
iteration 130, loss = 0.0019205908756703138
iteration 131, loss = 0.0015183893265202641
iteration 132, loss = 0.0013515936443582177
iteration 133, loss = 0.0013555152108892798
iteration 134, loss = 0.001361318165436387
iteration 135, loss = 0.0013028832618147135
iteration 136, loss = 0.002130759647116065
iteration 137, loss = 0.0014881257666274905
iteration 138, loss = 0.0037804078310728073
iteration 139, loss = 0.001999122556298971
iteration 140, loss = 0.002074924297630787
iteration 141, loss = 0.0013081900542601943
iteration 142, loss = 0.0013011281844228506
iteration 143, loss = 0.0012746803695335984
iteration 144, loss = 0.0014200465520843863
iteration 145, loss = 0.001322133932262659
iteration 146, loss = 0.0013253368670120835
iteration 147, loss = 0.0013555196346715093
iteration 148, loss = 0.0014989085029810667
iteration 149, loss = 0.001281066914089024
iteration 150, loss = 0.004671637900173664
iteration 151, loss = 0.0014912347542122006
iteration 152, loss = 0.0014738881727680564
iteration 153, loss = 0.001521359896287322
iteration 154, loss = 0.001380934496410191
iteration 155, loss = 0.0012485913466662169
iteration 156, loss = 0.0013127770507708192
iteration 157, loss = 0.002151083666831255
iteration 158, loss = 0.0013873061398044229
iteration 159, loss = 0.0014228045474737883
iteration 160, loss = 0.0014350292040035129
iteration 161, loss = 0.0012984249042347074
iteration 162, loss = 0.0014572779182344675
iteration 163, loss = 0.0014856396010145545
iteration 164, loss = 0.001302779302932322
iteration 165, loss = 0.0015116016147658229
iteration 166, loss = 0.0021160803735256195
iteration 167, loss = 0.0014592462684959173
iteration 168, loss = 0.0013408492086455226
iteration 169, loss = 0.0014774755109101534
iteration 170, loss = 0.0013417887967079878
iteration 171, loss = 0.001579539617523551
iteration 172, loss = 0.0015596741577610373
iteration 173, loss = 0.0014182360609993339
iteration 174, loss = 0.0038049076683819294
iteration 175, loss = 0.0016380718443542719
iteration 176, loss = 0.0013804325135424733
iteration 177, loss = 0.0017027523135766387
iteration 178, loss = 0.0014495832147076726
iteration 179, loss = 0.0016301036812365055
iteration 180, loss = 0.0012510184897109866
iteration 181, loss = 0.0013552671298384666
iteration 182, loss = 0.0013075433671474457
iteration 183, loss = 0.0013146116398274899
iteration 184, loss = 0.0012940401211380959
iteration 185, loss = 0.001421163440681994
iteration 186, loss = 0.0015535431448370218
iteration 187, loss = 0.0014702834887430072
iteration 188, loss = 0.0013524163514375687
iteration 189, loss = 0.001450987532734871
iteration 190, loss = 0.001401606947183609
iteration 191, loss = 0.0012753320625051856
iteration 192, loss = 0.0012587466044351459
iteration 193, loss = 0.0012772853951901197
iteration 194, loss = 0.0013838582672178745
iteration 195, loss = 0.0012948706280440092
iteration 196, loss = 0.0013092427980154753
iteration 197, loss = 0.0014119278639554977
iteration 198, loss = 0.001385057345032692
iteration 199, loss = 0.002147404011338949
iteration 200, loss = 0.0012511336244642735
iteration 201, loss = 0.002136846771463752
iteration 202, loss = 0.0014007415156811476
iteration 203, loss = 0.0013265935704112053
iteration 204, loss = 0.001225329120643437
iteration 205, loss = 0.0015830197371542454
iteration 206, loss = 0.001725031528621912
iteration 207, loss = 0.00136763125192374
iteration 208, loss = 0.0013493577716872096
iteration 209, loss = 0.001269445987418294
iteration 210, loss = 0.0013274179073050618
iteration 211, loss = 0.0019505550153553486
iteration 212, loss = 0.0014639804139733315
iteration 213, loss = 0.0013050276320427656
iteration 214, loss = 0.001274395501241088
iteration 215, loss = 0.0011790267890319228
iteration 216, loss = 0.0014773288276046515
iteration 217, loss = 0.0020887311547994614
iteration 218, loss = 0.003864571452140808
iteration 219, loss = 0.0016984115354716778
iteration 220, loss = 0.004248305689543486
iteration 221, loss = 0.0013065084349364042
iteration 222, loss = 0.0013053552247583866
iteration 223, loss = 0.0021190603729337454
iteration 224, loss = 0.0013020545011386275
iteration 225, loss = 0.0014383310917764902
iteration 226, loss = 0.001534857670776546
iteration 227, loss = 0.0013870539842173457
iteration 228, loss = 0.0014841669471934438
iteration 229, loss = 0.0021715937182307243
iteration 230, loss = 0.001304074889048934
iteration 231, loss = 0.001325835706666112
iteration 232, loss = 0.0021294339094311
iteration 233, loss = 0.0013081457000225782
iteration 234, loss = 0.0012862058356404305
iteration 235, loss = 0.0014820910291746259
iteration 236, loss = 0.00143530429340899
iteration 237, loss = 0.0014548294711858034
iteration 238, loss = 0.0018721387023106217
iteration 239, loss = 0.002215766813606024
iteration 240, loss = 0.0013311560032889247
iteration 241, loss = 0.0012791865738108754
iteration 242, loss = 0.001666648662649095
iteration 243, loss = 0.0015390045009553432
iteration 244, loss = 0.0013536291662603617
iteration 245, loss = 0.0011969361221417785
iteration 246, loss = 0.001667834585532546
iteration 247, loss = 0.0014094164362177253
iteration 248, loss = 0.001470494898967445
iteration 249, loss = 0.0014818107010796666
iteration 250, loss = 0.0037005478516221046
iteration 251, loss = 0.001438578707166016
iteration 252, loss = 0.0012677364284172654
iteration 253, loss = 0.0021215956658124924
iteration 254, loss = 0.0018803148996084929
iteration 255, loss = 0.001280641183257103
iteration 256, loss = 0.0012576926965266466
iteration 257, loss = 0.0015027980552986264
iteration 258, loss = 0.0015066912164911628
iteration 259, loss = 0.0013160392409190536
iteration 260, loss = 0.0013964255340397358
iteration 261, loss = 0.0011830291477963328
iteration 262, loss = 0.001559486729092896
iteration 263, loss = 0.0012388586765155196
iteration 264, loss = 0.0012551593827083707
iteration 265, loss = 0.0012978697195649147
iteration 266, loss = 0.0015263801906257868
iteration 267, loss = 0.0013185808202251792
iteration 268, loss = 0.0012845295714214444
iteration 269, loss = 0.001472701784223318
iteration 270, loss = 0.0013839658349752426
iteration 271, loss = 0.001266846084035933
iteration 272, loss = 0.0038499212823808193
iteration 273, loss = 0.0013260849518701434
iteration 274, loss = 0.0012903863098472357
iteration 275, loss = 0.001370645361021161
iteration 276, loss = 0.001337364548817277
iteration 277, loss = 0.0013264139415696263
iteration 278, loss = 0.003620896255597472
iteration 279, loss = 0.0012994027929380536
iteration 280, loss = 0.0015569470124319196
iteration 281, loss = 0.004157136660069227
iteration 282, loss = 0.0013298522680997849
iteration 283, loss = 0.0015349285677075386
iteration 284, loss = 0.0013698479160666466
iteration 285, loss = 0.0013881124323233962
iteration 286, loss = 0.0013831015676259995
iteration 287, loss = 0.0037952845450490713
iteration 288, loss = 0.0011965601006522775
iteration 289, loss = 0.0014531289925798774
iteration 290, loss = 0.0016965976683422923
iteration 291, loss = 0.0013652951456606388
iteration 292, loss = 0.0014834253815934062
iteration 293, loss = 0.0012989613460376859
iteration 294, loss = 0.0015120290918275714
iteration 295, loss = 0.0014783369842916727
iteration 296, loss = 0.0012374926591292024
iteration 297, loss = 0.0013912045396864414
iteration 298, loss = 0.0012669002171605825
iteration 299, loss = 0.0013697966933250427
iteration 0, loss = 0.001405714312568307
iteration 1, loss = 0.0012699150247499347
iteration 2, loss = 0.0013438231544569135
iteration 3, loss = 0.0014768982073292136
iteration 4, loss = 0.0017293530981987715
iteration 5, loss = 0.0012440059799700975
iteration 6, loss = 0.0016738750273361802
iteration 7, loss = 0.0012367317685857415
iteration 8, loss = 0.0014527766034007072
iteration 9, loss = 0.0011825555702671409
iteration 10, loss = 0.001479541533626616
iteration 11, loss = 0.0016052528517320752
iteration 12, loss = 0.0018786205910146236
iteration 13, loss = 0.0013748575001955032
iteration 14, loss = 0.0012481971643865108
iteration 15, loss = 0.0037621348164975643
iteration 16, loss = 0.0013807599898427725
iteration 17, loss = 0.0013927395921200514
iteration 18, loss = 0.0037517654709517956
iteration 19, loss = 0.0015047857305034995
iteration 20, loss = 0.0013869215035811067
iteration 21, loss = 0.0013431025436148047
iteration 22, loss = 0.0012961884494870901
iteration 23, loss = 0.0012990483082830906
iteration 24, loss = 0.001496815588325262
iteration 25, loss = 0.0013284040614962578
iteration 26, loss = 0.0015420455019921064
iteration 27, loss = 0.0013734305975958705
iteration 28, loss = 0.0012995459837839007
iteration 29, loss = 0.0035490095615386963
iteration 30, loss = 0.001556373666971922
iteration 31, loss = 0.0012877321569249034
iteration 32, loss = 0.0014561429852619767
iteration 33, loss = 0.0013116467744112015
iteration 34, loss = 0.001759911421686411
iteration 35, loss = 0.003497228492051363
iteration 36, loss = 0.001239567412994802
iteration 37, loss = 0.0015391589840874076
iteration 38, loss = 0.0020571944769471884
iteration 39, loss = 0.0012274014297872782
iteration 40, loss = 0.0016637068474665284
iteration 41, loss = 0.003981982823461294
iteration 42, loss = 0.0012736590579152107
iteration 43, loss = 0.0012141370680183172
iteration 44, loss = 0.0016248865285888314
iteration 45, loss = 0.001401736051775515
iteration 46, loss = 0.0036866844166070223
iteration 47, loss = 0.0012897051637992263
iteration 48, loss = 0.0014146859757602215
iteration 49, loss = 0.0019758546259254217
iteration 50, loss = 0.0016067477408796549
iteration 51, loss = 0.0014388137497007847
iteration 52, loss = 0.001440711785107851
iteration 53, loss = 0.0014909522142261267
iteration 54, loss = 0.001310947583988309
iteration 55, loss = 0.001222785096615553
iteration 56, loss = 0.0012352056801319122
iteration 57, loss = 0.0017411585431545973
iteration 58, loss = 0.0013192736078053713
iteration 59, loss = 0.0014994994271546602
iteration 60, loss = 0.0013747398043051362
iteration 61, loss = 0.0012363923015072942
iteration 62, loss = 0.0012903788592666388
iteration 63, loss = 0.0012277020141482353
iteration 64, loss = 0.0013532231096178293
iteration 65, loss = 0.0014444517437368631
iteration 66, loss = 0.0012641288340091705
iteration 67, loss = 0.0014122127322480083
iteration 68, loss = 0.0019177210051566362
iteration 69, loss = 0.0012925725895911455
iteration 70, loss = 0.0012463043676689267
iteration 71, loss = 0.00125952681992203
iteration 72, loss = 0.001563990255817771
iteration 73, loss = 0.0014890381135046482
iteration 74, loss = 0.001272361958399415
iteration 75, loss = 0.0015088716754689813
iteration 76, loss = 0.0013270870549604297
iteration 77, loss = 0.0019064098596572876
iteration 78, loss = 0.0013173390179872513
iteration 79, loss = 0.003571470268070698
iteration 80, loss = 0.0012464378960430622
iteration 81, loss = 0.0012076012790203094
iteration 82, loss = 0.0012858993140980601
iteration 83, loss = 0.0019798025023192167
iteration 84, loss = 0.001257909694686532
iteration 85, loss = 0.0014056091895326972
iteration 86, loss = 0.001370990532450378
iteration 87, loss = 0.0012158496538177133
iteration 88, loss = 0.001292741042561829
iteration 89, loss = 0.0018943710019811988
iteration 90, loss = 0.001296178437769413
iteration 91, loss = 0.001159678096882999
iteration 92, loss = 0.0012952745892107487
iteration 93, loss = 0.0014594451058655977
iteration 94, loss = 0.00114575051702559
iteration 95, loss = 0.0018045584438368678
iteration 96, loss = 0.0013118855422362685
iteration 97, loss = 0.0011569814523681998
iteration 98, loss = 0.0012600880581885576
iteration 99, loss = 0.0013701766729354858
iteration 100, loss = 0.0012577062007039785
iteration 101, loss = 0.0013375157723203301
iteration 102, loss = 0.001655585947446525
iteration 103, loss = 0.0012521914904937148
iteration 104, loss = 0.0017945404397323728
iteration 105, loss = 0.0012834762455895543
iteration 106, loss = 0.0012200756464153528
iteration 107, loss = 0.0018182166386395693
iteration 108, loss = 0.0013005118817090988
iteration 109, loss = 0.0012000094866380095
iteration 110, loss = 0.0012244043173268437
iteration 111, loss = 0.0016630108002573252
iteration 112, loss = 0.0018050260841846466
iteration 113, loss = 0.0017625675536692142
iteration 114, loss = 0.001410402823239565
iteration 115, loss = 0.0012827500468119979
iteration 116, loss = 0.0013916308525949717
iteration 117, loss = 0.0012985391076654196
iteration 118, loss = 0.001134355436079204
iteration 119, loss = 0.0012971364194527268
iteration 120, loss = 0.0014987048925831914
iteration 121, loss = 0.0013493380974978209
iteration 122, loss = 0.0012538080336526036
iteration 123, loss = 0.0014130152994766831
iteration 124, loss = 0.001485483255237341
iteration 125, loss = 0.0012863704469054937
iteration 126, loss = 0.0012680288637056947
iteration 127, loss = 0.0018792222253978252
iteration 128, loss = 0.0016698007239028811
iteration 129, loss = 0.0013463698560371995
iteration 130, loss = 0.0013103052042424679
iteration 131, loss = 0.0013466279488056898
iteration 132, loss = 0.001389733050018549
iteration 133, loss = 0.002142828656360507
iteration 134, loss = 0.0013266400201246142
iteration 135, loss = 0.0012587602250277996
iteration 136, loss = 0.0035372539423406124
iteration 137, loss = 0.0013889436377212405
iteration 138, loss = 0.001341604278422892
iteration 139, loss = 0.0011537934187799692
iteration 140, loss = 0.0013985526748001575
iteration 141, loss = 0.0021084018517285585
iteration 142, loss = 0.0021327424328774214
iteration 143, loss = 0.0034273210912942886
iteration 144, loss = 0.0013196785002946854
iteration 145, loss = 0.0017238727305084467
iteration 146, loss = 0.0012617161264643073
iteration 147, loss = 0.0012835420202463865
iteration 148, loss = 0.0012025642208755016
iteration 149, loss = 0.0011449626181274652
iteration 150, loss = 0.001332806539721787
iteration 151, loss = 0.0012086931383237243
iteration 152, loss = 0.0034848316572606564
iteration 153, loss = 0.0013345410116016865
iteration 154, loss = 0.0013420720351859927
iteration 155, loss = 0.001433861325494945
iteration 156, loss = 0.0013652846682816744
iteration 157, loss = 0.0020741564221680164
iteration 158, loss = 0.0012227412080392241
iteration 159, loss = 0.0013298183912411332
iteration 160, loss = 0.002111472189426422
iteration 161, loss = 0.0013903176877647638
iteration 162, loss = 0.001266386592760682
iteration 163, loss = 0.0012390200281515718
iteration 164, loss = 0.0011923463316634297
iteration 165, loss = 0.0012572193518280983
iteration 166, loss = 0.003731554839760065
iteration 167, loss = 0.0014791032299399376
iteration 168, loss = 0.0020391501020640135
iteration 169, loss = 0.0012693044263869524
iteration 170, loss = 0.001257228315807879
iteration 171, loss = 0.001064078533090651
iteration 172, loss = 0.0013469585683196783
iteration 173, loss = 0.001774860662408173
iteration 174, loss = 0.0014755227603018284
iteration 175, loss = 0.0016826228238642216
iteration 176, loss = 0.0013286176836118102
iteration 177, loss = 0.0013518764171749353
iteration 178, loss = 0.0019398836884647608
iteration 179, loss = 0.001929558115079999
iteration 180, loss = 0.001840753247961402
iteration 181, loss = 0.001384104834869504
iteration 182, loss = 0.0012251766165718436
iteration 183, loss = 0.0011704936623573303
iteration 184, loss = 0.0012786324368789792
iteration 185, loss = 0.0020990478806197643
iteration 186, loss = 0.0012556659057736397
iteration 187, loss = 0.0012546139769256115
iteration 188, loss = 0.0013307015178725123
iteration 189, loss = 0.0013720435090363026
iteration 190, loss = 0.001365957548841834
iteration 191, loss = 0.0012075143167749047
iteration 192, loss = 0.0013567281421273947
iteration 193, loss = 0.0012327679432928562
iteration 194, loss = 0.001162856351584196
iteration 195, loss = 0.001322511350736022
iteration 196, loss = 0.0011872065952047706
iteration 197, loss = 0.00162231782451272
iteration 198, loss = 0.0014170175418257713
iteration 199, loss = 0.001190388691611588
iteration 200, loss = 0.0013135153567418456
iteration 201, loss = 0.001506588771007955
iteration 202, loss = 0.0018930986989289522
iteration 203, loss = 0.0011983402073383331
iteration 204, loss = 0.0012575362343341112
iteration 205, loss = 0.0013844433706253767
iteration 206, loss = 0.0015544164925813675
iteration 207, loss = 0.0021506906487047672
iteration 208, loss = 0.0012485452461987734
iteration 209, loss = 0.0034959532786160707
iteration 210, loss = 0.001251798588782549
iteration 211, loss = 0.001377759501338005
iteration 212, loss = 0.0012595768785104156
iteration 213, loss = 0.0016044077929109335
iteration 214, loss = 0.0013704801676794887
iteration 215, loss = 0.0020228249486535788
iteration 216, loss = 0.0011633443646132946
iteration 217, loss = 0.001111100078560412
iteration 218, loss = 0.003547330852597952
iteration 219, loss = 0.0011532939970493317
iteration 220, loss = 0.0011719329049810767
iteration 221, loss = 0.001322843600064516
iteration 222, loss = 0.0013872901909053326
iteration 223, loss = 0.0012464199680835009
iteration 224, loss = 0.0035147350281476974
iteration 225, loss = 0.0014055129140615463
iteration 226, loss = 0.001815715921111405
iteration 227, loss = 0.0011685420759022236
iteration 228, loss = 0.0011765258386731148
iteration 229, loss = 0.0019256090745329857
iteration 230, loss = 0.0012111401883885264
iteration 231, loss = 0.0012836616951972246
iteration 232, loss = 0.0016001819167286158
iteration 233, loss = 0.0020184547174721956
iteration 234, loss = 0.0012579732574522495
iteration 235, loss = 0.0011903324630111456
iteration 236, loss = 0.0015022939769551158
iteration 237, loss = 0.0011599685531109571
iteration 238, loss = 0.001332795713096857
iteration 239, loss = 0.0014216519193723798
iteration 240, loss = 0.0013696631649509072
iteration 241, loss = 0.001228783861733973
iteration 242, loss = 0.0011629394721239805
iteration 243, loss = 0.0018156325677409768
iteration 244, loss = 0.0014793463051319122
iteration 245, loss = 0.0012920321896672249
iteration 246, loss = 0.0016562421806156635
iteration 247, loss = 0.0019743097946047783
iteration 248, loss = 0.0012349627213552594
iteration 249, loss = 0.0012897219276055694
iteration 250, loss = 0.0013552838936448097
iteration 251, loss = 0.0012301553506404161
iteration 252, loss = 0.001159569132141769
iteration 253, loss = 0.0013088188134133816
iteration 254, loss = 0.0011370149441063404
iteration 255, loss = 0.001184267457574606
iteration 256, loss = 0.001399146392941475
iteration 257, loss = 0.0015300193335860968
iteration 258, loss = 0.001292768749408424
iteration 259, loss = 0.001186935231089592
iteration 260, loss = 0.0013137145433574915
iteration 261, loss = 0.0020115915685892105
iteration 262, loss = 0.0016277206595987082
iteration 263, loss = 0.0014239404117688537
iteration 264, loss = 0.0011775135062634945
iteration 265, loss = 0.0034174721222370863
iteration 266, loss = 0.001213323324918747
iteration 267, loss = 0.001208907226100564
iteration 268, loss = 0.0014475964708253741
iteration 269, loss = 0.0013421730836853385
iteration 270, loss = 0.0013586538843810558
iteration 271, loss = 0.0014483396662399173
iteration 272, loss = 0.0012039109133183956
iteration 273, loss = 0.001157861202955246
iteration 274, loss = 0.001268386491574347
iteration 275, loss = 0.0015829656040295959
iteration 276, loss = 0.0011706592049449682
iteration 277, loss = 0.001355634187348187
iteration 278, loss = 0.0011838729260489345
iteration 279, loss = 0.0012660255888476968
iteration 280, loss = 0.0011340396013110876
iteration 281, loss = 0.0014163284795358777
iteration 282, loss = 0.0012133584823459387
iteration 283, loss = 0.00188734894618392
iteration 284, loss = 0.0013247793540358543
iteration 285, loss = 0.0016262087738141418
iteration 286, loss = 0.0013528720010071993
iteration 287, loss = 0.0013149683363735676
iteration 288, loss = 0.0011649256339296699
iteration 289, loss = 0.0014591965591534972
iteration 290, loss = 0.001997651532292366
iteration 291, loss = 0.0014391029253602028
iteration 292, loss = 0.0012354233767837286
iteration 293, loss = 0.0013156188651919365
iteration 294, loss = 0.0011920705437660217
iteration 295, loss = 0.0012503928737714887
iteration 296, loss = 0.0015400343108922243
iteration 297, loss = 0.0012202204670757055
iteration 298, loss = 0.0014503486454486847
iteration 299, loss = 0.0011725976364687085
iteration 0, loss = 0.0014586487086489797
iteration 1, loss = 0.0012923551257699728
iteration 2, loss = 0.0012625965755432844
iteration 3, loss = 0.0015213630395010114
iteration 4, loss = 0.0013472066493704915
iteration 5, loss = 0.0013219148386269808
iteration 6, loss = 0.0011402523377910256
iteration 7, loss = 0.0018732079770416021
iteration 8, loss = 0.0011616850970312953
iteration 9, loss = 0.0013067546533420682
iteration 10, loss = 0.001952227670699358
iteration 11, loss = 0.0014393938472494483
iteration 12, loss = 0.0011354342568665743
iteration 13, loss = 0.0011944309808313847
iteration 14, loss = 0.0013428917154669762
iteration 15, loss = 0.001278463751077652
iteration 16, loss = 0.0012073189718648791
iteration 17, loss = 0.0013216686202213168
iteration 18, loss = 0.0012436488177627325
iteration 19, loss = 0.0011957192327827215
iteration 20, loss = 0.0012744581326842308
iteration 21, loss = 0.0012184788938611746
iteration 22, loss = 0.0011527366004884243
iteration 23, loss = 0.0013082438381388783
iteration 24, loss = 0.0012540881289169192
iteration 25, loss = 0.0012320743408054113
iteration 26, loss = 0.001261283759959042
iteration 27, loss = 0.0016320025315508246
iteration 28, loss = 0.0013332446105778217
iteration 29, loss = 0.0011519858380779624
iteration 30, loss = 0.0012821381678804755
iteration 31, loss = 0.0011393651366233826
iteration 32, loss = 0.0013400391908362508
iteration 33, loss = 0.0017113317735493183
iteration 34, loss = 0.0012671080185100436
iteration 35, loss = 0.0015020500868558884
iteration 36, loss = 0.0012227846309542656
iteration 37, loss = 0.0012370874173939228
iteration 38, loss = 0.0013257807586342096
iteration 39, loss = 0.0017175915418192744
iteration 40, loss = 0.0012618659529834986
iteration 41, loss = 0.0013231600169092417
iteration 42, loss = 0.001286451006308198
iteration 43, loss = 0.0012272760504856706
iteration 44, loss = 0.0011950155021622777
iteration 45, loss = 0.003404782386496663
iteration 46, loss = 0.0014281811891123652
iteration 47, loss = 0.0016408294904977083
iteration 48, loss = 0.0014306750381365418
iteration 49, loss = 0.001603777171112597
iteration 50, loss = 0.0011757728643715382
iteration 51, loss = 0.0019675041548907757
iteration 52, loss = 0.0013171957107260823
iteration 53, loss = 0.0011434321058914065
iteration 54, loss = 0.0013947932748124003
iteration 55, loss = 0.0012611376587301493
iteration 56, loss = 0.0017507162410765886
iteration 57, loss = 0.0012914477847516537
iteration 58, loss = 0.0012923147296532989
iteration 59, loss = 0.0012049494544044137
iteration 60, loss = 0.001261984114535153
iteration 61, loss = 0.0012925678165629506
iteration 62, loss = 0.001357200206257403
iteration 63, loss = 0.001405116985552013
iteration 64, loss = 0.001137739047408104
iteration 65, loss = 0.0013995390618219972
iteration 66, loss = 0.0011642600875347853
iteration 67, loss = 0.0013202126137912273
iteration 68, loss = 0.001149526797235012
iteration 69, loss = 0.00113669375423342
iteration 70, loss = 0.0012757712975144386
iteration 71, loss = 0.0017771184211596847
iteration 72, loss = 0.0034633823670446873
iteration 73, loss = 0.001232554903253913
iteration 74, loss = 0.0012929679360240698
iteration 75, loss = 0.001986301736906171
iteration 76, loss = 0.0012199274497106671
iteration 77, loss = 0.0011887894943356514
iteration 78, loss = 0.0017922103870660067
iteration 79, loss = 0.0012742241378873587
iteration 80, loss = 0.0013582860119640827
iteration 81, loss = 0.0013508667470887303
iteration 82, loss = 0.0011743921786546707
iteration 83, loss = 0.00122504448518157
iteration 84, loss = 0.003278198651969433
iteration 85, loss = 0.0013819988816976547
iteration 86, loss = 0.0022013711277395487
iteration 87, loss = 0.0012749206507578492
iteration 88, loss = 0.0013330464717000723
iteration 89, loss = 0.001368614030070603
iteration 90, loss = 0.001441262662410736
iteration 91, loss = 0.0011992824729532003
iteration 92, loss = 0.0012182957725599408
iteration 93, loss = 0.0013042286736890674
iteration 94, loss = 0.0011908948654308915
iteration 95, loss = 0.0013080622302368283
iteration 96, loss = 0.0012074063997715712
iteration 97, loss = 0.0012338950764387846
iteration 98, loss = 0.0011304538929834962
iteration 99, loss = 0.0013215307844802737
iteration 100, loss = 0.0012014826061204076
iteration 101, loss = 0.0012928791111335158
iteration 102, loss = 0.0012244925601407886
iteration 103, loss = 0.0013135626213625073
iteration 104, loss = 0.0012305137934163213
iteration 105, loss = 0.001189952134154737
iteration 106, loss = 0.0017569175688549876
iteration 107, loss = 0.0016541779041290283
iteration 108, loss = 0.0012264136457815766
iteration 109, loss = 0.001288487110286951
iteration 110, loss = 0.0014200110454112291
iteration 111, loss = 0.0013255965895950794
iteration 112, loss = 0.0016340153524652123
iteration 113, loss = 0.0011964356526732445
iteration 114, loss = 0.0012055012630298734
iteration 115, loss = 0.0011927301529794931
iteration 116, loss = 0.001311302068643272
iteration 117, loss = 0.0013390680542215705
iteration 118, loss = 0.0011301139602437615
iteration 119, loss = 0.0011109561892226338
iteration 120, loss = 0.001233440823853016
iteration 121, loss = 0.0011725735384970903
iteration 122, loss = 0.0013969233259558678
iteration 123, loss = 0.0012006651377305388
iteration 124, loss = 0.0012251458829268813
iteration 125, loss = 0.0013768752105534077
iteration 126, loss = 0.0011908377055078745
iteration 127, loss = 0.001447390066459775
iteration 128, loss = 0.0013786478666588664
iteration 129, loss = 0.0011729925172403455
iteration 130, loss = 0.0013124499237164855
iteration 131, loss = 0.001265623141080141
iteration 132, loss = 0.0012340855319052935
iteration 133, loss = 0.0033958840649574995
iteration 134, loss = 0.0017551463097333908
iteration 135, loss = 0.0012294953921809793
iteration 136, loss = 0.0012150198454037309
iteration 137, loss = 0.0012552710250020027
iteration 138, loss = 0.0011635352857410908
iteration 139, loss = 0.0010759386932477355
iteration 140, loss = 0.0013401529286056757
iteration 141, loss = 0.0012429355410858989
iteration 142, loss = 0.0011897518998011947
iteration 143, loss = 0.0033601548057049513
iteration 144, loss = 0.0011610504006966949
iteration 145, loss = 0.0013240480329841375
iteration 146, loss = 0.0010530727449804544
iteration 147, loss = 0.001291088294237852
iteration 148, loss = 0.0012875586980953813
iteration 149, loss = 0.003376774722710252
iteration 150, loss = 0.0017437317874282598
iteration 151, loss = 0.001123633235692978
iteration 152, loss = 0.0013006293447688222
iteration 153, loss = 0.003343200543895364
iteration 154, loss = 0.0013448395766317844
iteration 155, loss = 0.0011837184429168701
iteration 156, loss = 0.0014080852270126343
iteration 157, loss = 0.0012217523762956262
iteration 158, loss = 0.0010783171746879816
iteration 159, loss = 0.001166273606941104
iteration 160, loss = 0.001221432932652533
iteration 161, loss = 0.0011693419655784965
iteration 162, loss = 0.0011321958154439926
iteration 163, loss = 0.0018752588657662272
iteration 164, loss = 0.0011534024961292744
iteration 165, loss = 0.0011572374496608973
iteration 166, loss = 0.0013309285277500749
iteration 167, loss = 0.001281284959986806
iteration 168, loss = 0.001165908994153142
iteration 169, loss = 0.0012259286595508456
iteration 170, loss = 0.0032719385344535112
iteration 171, loss = 0.003232052084058523
iteration 172, loss = 0.0035086809657514095
iteration 173, loss = 0.0032649023924022913
iteration 174, loss = 0.0013476124731823802
iteration 175, loss = 0.001197710749693215
iteration 176, loss = 0.0011573679512366652
iteration 177, loss = 0.0035034033935517073
iteration 178, loss = 0.0011192167876288295
iteration 179, loss = 0.001140246051363647
iteration 180, loss = 0.0019959062337875366
iteration 181, loss = 0.0012429869966581464
iteration 182, loss = 0.001587867853231728
iteration 183, loss = 0.001386889722198248
iteration 184, loss = 0.0027209895197302103
iteration 185, loss = 0.0013487219111993909
iteration 186, loss = 0.001140725682489574
iteration 187, loss = 0.001147354836575687
iteration 188, loss = 0.001795175950974226
iteration 189, loss = 0.0011801618384197354
iteration 190, loss = 0.0013451108243316412
iteration 191, loss = 0.0011594337411224842
iteration 192, loss = 0.0011135425884276628
iteration 193, loss = 0.001517156488262117
iteration 194, loss = 0.0012979187304154038
iteration 195, loss = 0.0011495890794321895
iteration 196, loss = 0.0032394814770668745
iteration 197, loss = 0.0011667428771033883
iteration 198, loss = 0.0011684518540278077
iteration 199, loss = 0.0012052261736243963
iteration 200, loss = 0.0011781585635617375
iteration 201, loss = 0.0011020917445421219
iteration 202, loss = 0.002027869690209627
iteration 203, loss = 0.0011906425934284925
iteration 204, loss = 0.003472002223134041
iteration 205, loss = 0.0012102050241082907
iteration 206, loss = 0.001525955623947084
iteration 207, loss = 0.0015244628302752972
iteration 208, loss = 0.0017347335815429688
iteration 209, loss = 0.0017188640777021646
iteration 210, loss = 0.0012695868499577045
iteration 211, loss = 0.0032827812246978283
iteration 212, loss = 0.0017387880943715572
iteration 213, loss = 0.001154047786258161
iteration 214, loss = 0.0012312059989199042
iteration 215, loss = 0.0011529140174388885
iteration 216, loss = 0.0019532477017492056
iteration 217, loss = 0.0011263430351391435
iteration 218, loss = 0.0011434698244556785
iteration 219, loss = 0.0012263520620763302
iteration 220, loss = 0.0011447345605120063
iteration 221, loss = 0.0011612169910222292
iteration 222, loss = 0.0019012023694813251
iteration 223, loss = 0.001296454225666821
iteration 224, loss = 0.0011489115422591567
iteration 225, loss = 0.0013049485860392451
iteration 226, loss = 0.0019029845716431737
iteration 227, loss = 0.0014601704897359014
iteration 228, loss = 0.0010914246086031199
iteration 229, loss = 0.0012210155837237835
iteration 230, loss = 0.0011152011575177312
iteration 231, loss = 0.0012911045923829079
iteration 232, loss = 0.001374578569084406
iteration 233, loss = 0.0013062555808573961
iteration 234, loss = 0.0018189542461186647
iteration 235, loss = 0.0010630280012264848
iteration 236, loss = 0.0011532179778441787
iteration 237, loss = 0.001278172479942441
iteration 238, loss = 0.0011997489491477609
iteration 239, loss = 0.0012247812701389194
iteration 240, loss = 0.001154453493654728
iteration 241, loss = 0.001321304589509964
iteration 242, loss = 0.001260074321180582
iteration 243, loss = 0.0011039759265258908
iteration 244, loss = 0.0011431346647441387
iteration 245, loss = 0.001162413158454001
iteration 246, loss = 0.001615869696252048
iteration 247, loss = 0.001109664561226964
iteration 248, loss = 0.0012231077998876572
iteration 249, loss = 0.0012464291648939252
iteration 250, loss = 0.0011837942292913795
iteration 251, loss = 0.0011647845385596156
iteration 252, loss = 0.0018507078057155013
iteration 253, loss = 0.001311236061155796
iteration 254, loss = 0.0012899688445031643
iteration 255, loss = 0.001258611329831183
iteration 256, loss = 0.0012479514116421342
iteration 257, loss = 0.0010866859229281545
iteration 258, loss = 0.0011597556294873357
iteration 259, loss = 0.0013252687640488148
iteration 260, loss = 0.002213652478531003
iteration 261, loss = 0.0012529492378234863
iteration 262, loss = 0.0011258780723437667
iteration 263, loss = 0.001151479547843337
iteration 264, loss = 0.0012743768747895956
iteration 265, loss = 0.0027328748255968094
iteration 266, loss = 0.0022154799662530422
iteration 267, loss = 0.001076898886822164
iteration 268, loss = 0.0011179149150848389
iteration 269, loss = 0.0013640186516568065
iteration 270, loss = 0.0014764307998120785
iteration 271, loss = 0.0011638507712632418
iteration 272, loss = 0.0013798659201711416
iteration 273, loss = 0.0014225606573745608
iteration 274, loss = 0.0013675173977389932
iteration 275, loss = 0.001174120232462883
iteration 276, loss = 0.0013071305584162474
iteration 277, loss = 0.0011630184017121792
iteration 278, loss = 0.0011917171068489552
iteration 279, loss = 0.001185258966870606
iteration 280, loss = 0.001247392501682043
iteration 281, loss = 0.0011384825920686126
iteration 282, loss = 0.001239993842318654
iteration 283, loss = 0.0016475236043334007
iteration 284, loss = 0.0015749168815091252
iteration 285, loss = 0.001227320870384574
iteration 286, loss = 0.0012137411395087838
iteration 287, loss = 0.0011311117559671402
iteration 288, loss = 0.0015704449033364654
iteration 289, loss = 0.0011182705638930202
iteration 290, loss = 0.001157799270004034
iteration 291, loss = 0.0010406817309558392
iteration 292, loss = 0.001220275997184217
iteration 293, loss = 0.0012954891426488757
iteration 294, loss = 0.0011423500254750252
iteration 295, loss = 0.001347383833490312
iteration 296, loss = 0.0018808990716934204
iteration 297, loss = 0.0013812391553074121
iteration 298, loss = 0.0011738260509446263
iteration 299, loss = 0.001236253185197711
iteration 0, loss = 0.0012865825556218624
iteration 1, loss = 0.0032407811377197504
iteration 2, loss = 0.0016705798916518688
iteration 3, loss = 0.0014543267898261547
iteration 4, loss = 0.0011756893945857882
iteration 5, loss = 0.001248235465027392
iteration 6, loss = 0.001289654290303588
iteration 7, loss = 0.0031829746440052986
iteration 8, loss = 0.00125016993843019
iteration 9, loss = 0.0011275336146354675
iteration 10, loss = 0.0013054884038865566
iteration 11, loss = 0.001149509334936738
iteration 12, loss = 0.0011511323973536491
iteration 13, loss = 0.0020995934028178453
iteration 14, loss = 0.001843367121182382
iteration 15, loss = 0.0011786302784457803
iteration 16, loss = 0.0018433037912473083
iteration 17, loss = 0.001366784330457449
iteration 18, loss = 0.0010779357980936766
iteration 19, loss = 0.0012180994963273406
iteration 20, loss = 0.001052753534168005
iteration 21, loss = 0.0012838358525186777
iteration 22, loss = 0.0011546919122338295
iteration 23, loss = 0.0011097979731857777
iteration 24, loss = 0.0010862143244594336
iteration 25, loss = 0.0011995828244835138
iteration 26, loss = 0.0012827561004087329
iteration 27, loss = 0.001475800178013742
iteration 28, loss = 0.0013845495413988829
iteration 29, loss = 0.0012384997680783272
iteration 30, loss = 0.001815396943129599
iteration 31, loss = 0.0014579147100448608
iteration 32, loss = 0.001361964037641883
iteration 33, loss = 0.0011441766982898116
iteration 34, loss = 0.0011466931318864226
iteration 35, loss = 0.0012458749115467072
iteration 36, loss = 0.0011795113096013665
iteration 37, loss = 0.0011969180777668953
iteration 38, loss = 0.0011901322286576033
iteration 39, loss = 0.0012826408492401242
iteration 40, loss = 0.0010654719080775976
iteration 41, loss = 0.0011636135168373585
iteration 42, loss = 0.0011268970556557178
iteration 43, loss = 0.001336017157882452
iteration 44, loss = 0.0011150366626679897
iteration 45, loss = 0.0010166347492486238
iteration 46, loss = 0.0011245564091950655
iteration 47, loss = 0.0013496935134753585
iteration 48, loss = 0.0011075632646679878
iteration 49, loss = 0.001362709910608828
iteration 50, loss = 0.0012264992110431194
iteration 51, loss = 0.001365297706797719
iteration 52, loss = 0.0011381444055587053
iteration 53, loss = 0.0017866319976747036
iteration 54, loss = 0.0012045465409755707
iteration 55, loss = 0.0015053190290927887
iteration 56, loss = 0.0013513818848878145
iteration 57, loss = 0.0011193715035915375
iteration 58, loss = 0.0011763166403397918
iteration 59, loss = 0.0013086491962894797
iteration 60, loss = 0.0012189140543341637
iteration 61, loss = 0.0011620140867307782
iteration 62, loss = 0.001487372675910592
iteration 63, loss = 0.0020190412178635597
iteration 64, loss = 0.0010609043529257178
iteration 65, loss = 0.0010344944894313812
iteration 66, loss = 0.0010987718123942614
iteration 67, loss = 0.0011183132883161306
iteration 68, loss = 0.001232117647305131
iteration 69, loss = 0.00134069484192878
iteration 70, loss = 0.0017940150573849678
iteration 71, loss = 0.001175121171399951
iteration 72, loss = 0.0011130801867693663
iteration 73, loss = 0.00116938806604594
iteration 74, loss = 0.0013582507381215692
iteration 75, loss = 0.0015058311400935054
iteration 76, loss = 0.0011115300003439188
iteration 77, loss = 0.0010925563983619213
iteration 78, loss = 0.0013246199814602733
iteration 79, loss = 0.0011126311728730798
iteration 80, loss = 0.0010354159167036414
iteration 81, loss = 0.0012165044900029898
iteration 82, loss = 0.0012605881784111261
iteration 83, loss = 0.0011018402874469757
iteration 84, loss = 0.001199101097881794
iteration 85, loss = 0.001214978750795126
iteration 86, loss = 0.0011537941172719002
iteration 87, loss = 0.0010600221576169133
iteration 88, loss = 0.0011223203036934137
iteration 89, loss = 0.0016352141974493861
iteration 90, loss = 0.0031239306554198265
iteration 91, loss = 0.001304839737713337
iteration 92, loss = 0.0011368963168933988
iteration 93, loss = 0.0010218808893114328
iteration 94, loss = 0.0011990885250270367
iteration 95, loss = 0.0010549394646659493
iteration 96, loss = 0.001124448492191732
iteration 97, loss = 0.0013514355523511767
iteration 98, loss = 0.0013167010620236397
iteration 99, loss = 0.001130153308622539
iteration 100, loss = 0.001206839457154274
iteration 101, loss = 0.0010814409470185637
iteration 102, loss = 0.0013650015462189913
iteration 103, loss = 0.0010287024779245257
iteration 104, loss = 0.001146475551649928
iteration 105, loss = 0.0013181047979742289
iteration 106, loss = 0.0010724060703068972
iteration 107, loss = 0.00111583957914263
iteration 108, loss = 0.001658939989283681
iteration 109, loss = 0.0011821519583463669
iteration 110, loss = 0.0011408102000132203
iteration 111, loss = 0.0032953929621726274
iteration 112, loss = 0.0015750651946291327
iteration 113, loss = 0.0012205581879243255
iteration 114, loss = 0.0011253858683630824
iteration 115, loss = 0.0012293107574805617
iteration 116, loss = 0.0010892394930124283
iteration 117, loss = 0.0017679762095212936
iteration 118, loss = 0.0012240868527442217
iteration 119, loss = 0.003252581926062703
iteration 120, loss = 0.0014538022223860025
iteration 121, loss = 0.001534690847620368
iteration 122, loss = 0.0011394764296710491
iteration 123, loss = 0.001432185061275959
iteration 124, loss = 0.0011272527044638991
iteration 125, loss = 0.0011067686136811972
iteration 126, loss = 0.0011746952077373862
iteration 127, loss = 0.0010189978638663888
iteration 128, loss = 0.001018859096802771
iteration 129, loss = 0.001078654546290636
iteration 130, loss = 0.00110243062954396
iteration 131, loss = 0.0018178018508479
iteration 132, loss = 0.0011610279325395823
iteration 133, loss = 0.001208900474011898
iteration 134, loss = 0.0018051163060590625
iteration 135, loss = 0.0011331296991556883
iteration 136, loss = 0.0011320797493681312
iteration 137, loss = 0.001226786756888032
iteration 138, loss = 0.0011283708736300468
iteration 139, loss = 0.0011995121603831649
iteration 140, loss = 0.001951138605363667
iteration 141, loss = 0.002095496281981468
iteration 142, loss = 0.0031420302111655474
iteration 143, loss = 0.0011243929620832205
iteration 144, loss = 0.0031531378626823425
iteration 145, loss = 0.00161133729852736
iteration 146, loss = 0.001106344978325069
iteration 147, loss = 0.0010743762832134962
iteration 148, loss = 0.0010982101084664464
iteration 149, loss = 0.0011355046881362796
iteration 150, loss = 0.003143768757581711
iteration 151, loss = 0.0011710845865309238
iteration 152, loss = 0.0013131190789863467
iteration 153, loss = 0.0016089221462607384
iteration 154, loss = 0.0011652230750769377
iteration 155, loss = 0.0012456991244107485
iteration 156, loss = 0.0011513724457472563
iteration 157, loss = 0.001166492118500173
iteration 158, loss = 0.0010988549329340458
iteration 159, loss = 0.00310742249712348
iteration 160, loss = 0.0013586620334535837
iteration 161, loss = 0.0013110574800521135
iteration 162, loss = 0.0011917222291231155
iteration 163, loss = 0.001248959801159799
iteration 164, loss = 0.0012027821503579617
iteration 165, loss = 0.0011387248523533344
iteration 166, loss = 0.0011915597133338451
iteration 167, loss = 0.001139617059379816
iteration 168, loss = 0.0011088333558291197
iteration 169, loss = 0.0030650210101157427
iteration 170, loss = 0.0011279858881607652
iteration 171, loss = 0.001598243834450841
iteration 172, loss = 0.0017728459788486362
iteration 173, loss = 0.0018545654602348804
iteration 174, loss = 0.0011077113449573517
iteration 175, loss = 0.0012403522850945592
iteration 176, loss = 0.0011935678776353598
iteration 177, loss = 0.0011933129280805588
iteration 178, loss = 0.0010396650759503245
iteration 179, loss = 0.0011517842067405581
iteration 180, loss = 0.001212241593748331
iteration 181, loss = 0.001079160370863974
iteration 182, loss = 0.003271381603553891
iteration 183, loss = 0.0011622257297858596
iteration 184, loss = 0.0013914467999711633
iteration 185, loss = 0.0013141477247700095
iteration 186, loss = 0.0011518337996676564
iteration 187, loss = 0.0011800124775618315
iteration 188, loss = 0.001018812065012753
iteration 189, loss = 0.0020035398192703724
iteration 190, loss = 0.0010465467348694801
iteration 191, loss = 0.0011238314909860492
iteration 192, loss = 0.0010819938033819199
iteration 193, loss = 0.0011635287664830685
iteration 194, loss = 0.0012625978561118245
iteration 195, loss = 0.0010408186353743076
iteration 196, loss = 0.001574622467160225
iteration 197, loss = 0.0011570039205253124
iteration 198, loss = 0.0011792657896876335
iteration 199, loss = 0.0015721184900030494
iteration 200, loss = 0.0012561772018671036
iteration 201, loss = 0.0019467169186100364
iteration 202, loss = 0.0013491582358255982
iteration 203, loss = 0.0010676296660676599
iteration 204, loss = 0.0012540350435301661
iteration 205, loss = 0.001283752266317606
iteration 206, loss = 0.0018347136210650206
iteration 207, loss = 0.001119141816161573
iteration 208, loss = 0.0018515321426093578
iteration 209, loss = 0.001132024684920907
iteration 210, loss = 0.0010850663529708982
iteration 211, loss = 0.0012272216845303774
iteration 212, loss = 0.0010091064032167196
iteration 213, loss = 0.0010335216065868735
iteration 214, loss = 0.0013255616649985313
iteration 215, loss = 0.0011773285223171115
iteration 216, loss = 0.0012550748651847243
iteration 217, loss = 0.0031328892800956964
iteration 218, loss = 0.0010303485905751586
iteration 219, loss = 0.001661333255469799
iteration 220, loss = 0.0013116509653627872
iteration 221, loss = 0.0011966082965955138
iteration 222, loss = 0.0010702308500185609
iteration 223, loss = 0.001479649217799306
iteration 224, loss = 0.0012576488079503179
iteration 225, loss = 0.0012918293941766024
iteration 226, loss = 0.001153802266344428
iteration 227, loss = 0.0031305730808526278
iteration 228, loss = 0.0016464198706671596
iteration 229, loss = 0.0010839232709258795
iteration 230, loss = 0.0010666740126907825
iteration 231, loss = 0.0011449449229985476
iteration 232, loss = 0.0010280613787472248
iteration 233, loss = 0.0010744441533461213
iteration 234, loss = 0.0011354309972375631
iteration 235, loss = 0.0011826811823993921
iteration 236, loss = 0.0011383105302229524
iteration 237, loss = 0.0011104715522378683
iteration 238, loss = 0.0012667722767218947
iteration 239, loss = 0.0010571550810709596
iteration 240, loss = 0.0010178888915106654
iteration 241, loss = 0.0013048877008259296
iteration 242, loss = 0.001289852662011981
iteration 243, loss = 0.0015352024929597974
iteration 244, loss = 0.0011902108089998364
iteration 245, loss = 0.0011962633579969406
iteration 246, loss = 0.0011093630455434322
iteration 247, loss = 0.0011368966661393642
iteration 248, loss = 0.0011754751903936267
iteration 249, loss = 0.0010301314759999514
iteration 250, loss = 0.0012302158866077662
iteration 251, loss = 0.002036743564531207
iteration 252, loss = 0.0020009709987789392
iteration 253, loss = 0.0012494171969592571
iteration 254, loss = 0.0011598606361076236
iteration 255, loss = 0.001088535413146019
iteration 256, loss = 0.0011555973906069994
iteration 257, loss = 0.0010847939411178231
iteration 258, loss = 0.0010151838650926948
iteration 259, loss = 0.0011812306474894285
iteration 260, loss = 0.0014616032131016254
iteration 261, loss = 0.0011757778702303767
iteration 262, loss = 0.0010726519394665956
iteration 263, loss = 0.001136065344326198
iteration 264, loss = 0.0010846047662198544
iteration 265, loss = 0.0010898546315729618
iteration 266, loss = 0.001228181179612875
iteration 267, loss = 0.0010260523995384574
iteration 268, loss = 0.001091455458663404
iteration 269, loss = 0.0010049112606793642
iteration 270, loss = 0.0013780983863398433
iteration 271, loss = 0.0015937569551169872
iteration 272, loss = 0.0011132786748930812
iteration 273, loss = 0.0011067576706409454
iteration 274, loss = 0.0015834160149097443
iteration 275, loss = 0.0011277307057753205
iteration 276, loss = 0.0011569667840376496
iteration 277, loss = 0.001187063055112958
iteration 278, loss = 0.001051784958690405
iteration 279, loss = 0.003189197275787592
iteration 280, loss = 0.0011091465130448341
iteration 281, loss = 0.001181946718133986
iteration 282, loss = 0.0011016994249075651
iteration 283, loss = 0.0011383311357349157
iteration 284, loss = 0.0010741957230493426
iteration 285, loss = 0.0010446601081639528
iteration 286, loss = 0.0011379411444067955
iteration 287, loss = 0.0011813417077064514
iteration 288, loss = 0.0015332885086536407
iteration 289, loss = 0.0011652758112177253
iteration 290, loss = 0.0014877578942105174
iteration 291, loss = 0.0010355444392189384
iteration 292, loss = 0.0031366513576358557
iteration 293, loss = 0.0011345477541908622
iteration 294, loss = 0.001014038221910596
iteration 295, loss = 0.0010669316397979856
iteration 296, loss = 0.0010755816474556923
iteration 297, loss = 0.0011233498807996511
iteration 298, loss = 0.0012110675452277064
iteration 299, loss = 0.001755023142322898
iteration 0, loss = 0.0011391876032575965
iteration 1, loss = 0.00110279128421098
iteration 2, loss = 0.0011195763945579529
iteration 3, loss = 0.0012561598559841514
iteration 4, loss = 0.0010720809223130345
iteration 5, loss = 0.001241428079083562
iteration 6, loss = 0.0012084940681234002
iteration 7, loss = 0.001219499739818275
iteration 8, loss = 0.0015601754421368241
iteration 9, loss = 0.0011068348539993167
iteration 10, loss = 0.0011696235742419958
iteration 11, loss = 0.002252207137644291
iteration 12, loss = 0.0011363525409251451
iteration 13, loss = 0.0012370040640234947
iteration 14, loss = 0.0010871035046875477
iteration 15, loss = 0.0011771993013098836
iteration 16, loss = 0.0010366798378527164
iteration 17, loss = 0.0012272996827960014
iteration 18, loss = 0.00117683841381222
iteration 19, loss = 0.0010983851971104741
iteration 20, loss = 0.0010721395956352353
iteration 21, loss = 0.0011713062413036823
iteration 22, loss = 0.0011778402840718627
iteration 23, loss = 0.0010161958634853363
iteration 24, loss = 0.001059908070601523
iteration 25, loss = 0.0017564878799021244
iteration 26, loss = 0.0011037775548174977
iteration 27, loss = 0.0010941838845610619
iteration 28, loss = 0.001364833558909595
iteration 29, loss = 0.001269883825443685
iteration 30, loss = 0.0010930603602901101
iteration 31, loss = 0.0010619608219712973
iteration 32, loss = 0.00301479478366673
iteration 33, loss = 0.0010542168747633696
iteration 34, loss = 0.001045037293806672
iteration 35, loss = 0.001273278146982193
iteration 36, loss = 0.0011017912765964866
iteration 37, loss = 0.0011598696000874043
iteration 38, loss = 0.0030152969993650913
iteration 39, loss = 0.0011297042947262526
iteration 40, loss = 0.001091520069167018
iteration 41, loss = 0.001463999506086111
iteration 42, loss = 0.0010296227410435677
iteration 43, loss = 0.001081953290849924
iteration 44, loss = 0.001708218827843666
iteration 45, loss = 0.001131902332417667
iteration 46, loss = 0.0010446058586239815
iteration 47, loss = 0.0012995421420782804
iteration 48, loss = 0.0012764830607920885
iteration 49, loss = 0.0011176960542798042
iteration 50, loss = 0.003085627220571041
iteration 51, loss = 0.0012048565549775958
iteration 52, loss = 0.0015696827322244644
iteration 53, loss = 0.0015656237956136465
iteration 54, loss = 0.0011163013987243176
iteration 55, loss = 0.0011140683200210333
iteration 56, loss = 0.0009914160473272204
iteration 57, loss = 0.0018582448828965425
iteration 58, loss = 0.0011485912837088108
iteration 59, loss = 0.0011387176346033812
iteration 60, loss = 0.0012915178667753935
iteration 61, loss = 0.001003143610432744
iteration 62, loss = 0.0010207032319158316
iteration 63, loss = 0.0012194243026897311
iteration 64, loss = 0.0010771815432235599
iteration 65, loss = 0.001712676603347063
iteration 66, loss = 0.001222417689859867
iteration 67, loss = 0.0010343141620978713
iteration 68, loss = 0.001196446013636887
iteration 69, loss = 0.001106279087252915
iteration 70, loss = 0.0010742571903392673
iteration 71, loss = 0.0010469144908711314
iteration 72, loss = 0.0011286553926765919
iteration 73, loss = 0.0012400583364069462
iteration 74, loss = 0.0010253738146275282
iteration 75, loss = 0.00101440807338804
iteration 76, loss = 0.0015447305049747229
iteration 77, loss = 0.0010644178837537766
iteration 78, loss = 0.001504997257143259
iteration 79, loss = 0.0010283271549269557
iteration 80, loss = 0.0011942358687520027
iteration 81, loss = 0.0010378843871876597
iteration 82, loss = 0.0010288518387824297
iteration 83, loss = 0.0013313998933881521
iteration 84, loss = 0.0012259704526513815
iteration 85, loss = 0.0010563356336206198
iteration 86, loss = 0.001158161903731525
iteration 87, loss = 0.0014877301873639226
iteration 88, loss = 0.001220592181198299
iteration 89, loss = 0.0010017434833571315
iteration 90, loss = 0.0030018065590411425
iteration 91, loss = 0.0012165658408775926
iteration 92, loss = 0.001148389303125441
iteration 93, loss = 0.0011797177139669657
iteration 94, loss = 0.0011087886523455381
iteration 95, loss = 0.0010095121106132865
iteration 96, loss = 0.0012296026106923819
iteration 97, loss = 0.0030532139353454113
iteration 98, loss = 0.001085403491742909
iteration 99, loss = 0.0011970708146691322
iteration 100, loss = 0.0015024292515590787
iteration 101, loss = 0.001038945629261434
iteration 102, loss = 0.003129080170765519
iteration 103, loss = 0.001068621757440269
iteration 104, loss = 0.0013343591708689928
iteration 105, loss = 0.0009798880200833082
iteration 106, loss = 0.0012913299724459648
iteration 107, loss = 0.0010649333707988262
iteration 108, loss = 0.0010811073007062078
iteration 109, loss = 0.0010664773872122169
iteration 110, loss = 0.0010699102422222495
iteration 111, loss = 0.0012663773959502578
iteration 112, loss = 0.0010268994374200702
iteration 113, loss = 0.0010957838967442513
iteration 114, loss = 0.0011512886267155409
iteration 115, loss = 0.0016957029001787305
iteration 116, loss = 0.0012919176369905472
iteration 117, loss = 0.0010676868259906769
iteration 118, loss = 0.0010281967697665095
iteration 119, loss = 0.001384641625918448
iteration 120, loss = 0.0010465238010510802
iteration 121, loss = 0.0009957358706742525
iteration 122, loss = 0.0010949859861284494
iteration 123, loss = 0.001101477537304163
iteration 124, loss = 0.0010250551858916879
iteration 125, loss = 0.0010533141903579235
iteration 126, loss = 0.0012605683878064156
iteration 127, loss = 0.001145109417848289
iteration 128, loss = 0.001008298946544528
iteration 129, loss = 0.0015025690663605928
iteration 130, loss = 0.0011193383252248168
iteration 131, loss = 0.0011080370750278234
iteration 132, loss = 0.0013930329587310553
iteration 133, loss = 0.001032539177685976
iteration 134, loss = 0.0011926211882382631
iteration 135, loss = 0.0010899391490966082
iteration 136, loss = 0.0017965902807191014
iteration 137, loss = 0.0011642996687442064
iteration 138, loss = 0.001167945796623826
iteration 139, loss = 0.0010340241715312004
iteration 140, loss = 0.0010834974236786366
iteration 141, loss = 0.0009780023247003555
iteration 142, loss = 0.0010654095094650984
iteration 143, loss = 0.0010970333823934197
iteration 144, loss = 0.0010124592809006572
iteration 145, loss = 0.0010190363973379135
iteration 146, loss = 0.0009794553043320775
iteration 147, loss = 0.0010688705369830132
iteration 148, loss = 0.0011689802631735802
iteration 149, loss = 0.0009872502414509654
iteration 150, loss = 0.00116677803453058
iteration 151, loss = 0.0011447947472333908
iteration 152, loss = 0.0015343261184170842
iteration 153, loss = 0.0012286724522709846
iteration 154, loss = 0.00106771697755903
iteration 155, loss = 0.0010158836375921965
iteration 156, loss = 0.0011519532417878509
iteration 157, loss = 0.0011438752990216017
iteration 158, loss = 0.0010152263566851616
iteration 159, loss = 0.0015324550913646817
iteration 160, loss = 0.0011162791633978486
iteration 161, loss = 0.0013915407471358776
iteration 162, loss = 0.0015994273126125336
iteration 163, loss = 0.0011359507916495204
iteration 164, loss = 0.0015433834632858634
iteration 165, loss = 0.0013541241642087698
iteration 166, loss = 0.001480425358749926
iteration 167, loss = 0.001069643534719944
iteration 168, loss = 0.0010151853784918785
iteration 169, loss = 0.0011718920432031155
iteration 170, loss = 0.0010599170345813036
iteration 171, loss = 0.001126825693063438
iteration 172, loss = 0.0013653060887008905
iteration 173, loss = 0.0010534506291151047
iteration 174, loss = 0.0014623580500483513
iteration 175, loss = 0.00114619848318398
iteration 176, loss = 0.001705891452729702
iteration 177, loss = 0.001524971448816359
iteration 178, loss = 0.0009509852970950305
iteration 179, loss = 0.0010736380936577916
iteration 180, loss = 0.0018024420132860541
iteration 181, loss = 0.002947593806311488
iteration 182, loss = 0.0010076176840811968
iteration 183, loss = 0.0011201574234291911
iteration 184, loss = 0.0014702375046908855
iteration 185, loss = 0.0010989594738930464
iteration 186, loss = 0.0010704235173761845
iteration 187, loss = 0.0010246305027976632
iteration 188, loss = 0.0010175060015171766
iteration 189, loss = 0.0016508540138602257
iteration 190, loss = 0.00113071093801409
iteration 191, loss = 0.0010291250655427575
iteration 192, loss = 0.0012783098500221968
iteration 193, loss = 0.0010311517398804426
iteration 194, loss = 0.0010083623928949237
iteration 195, loss = 0.0010102330707013607
iteration 196, loss = 0.0010168043663725257
iteration 197, loss = 0.0012590193655341864
iteration 198, loss = 0.0010456024901941419
iteration 199, loss = 0.0011991460341960192
iteration 200, loss = 0.0010973195312544703
iteration 201, loss = 0.0009882833110168576
iteration 202, loss = 0.0015871572541072965
iteration 203, loss = 0.0010529709979891777
iteration 204, loss = 0.0015884841559454799
iteration 205, loss = 0.001082599745132029
iteration 206, loss = 0.0029537356458604336
iteration 207, loss = 0.0011216449784114957
iteration 208, loss = 0.0011963644064962864
iteration 209, loss = 0.0014472340699285269
iteration 210, loss = 0.0012776028597727418
iteration 211, loss = 0.0012161786435171962
iteration 212, loss = 0.0009883613092824817
iteration 213, loss = 0.0011559758568182588
iteration 214, loss = 0.001589250285178423
iteration 215, loss = 0.0012023878516629338
iteration 216, loss = 0.003106956137344241
iteration 217, loss = 0.0012416769750416279
iteration 218, loss = 0.0010563447140157223
iteration 219, loss = 0.0011672466062009335
iteration 220, loss = 0.002969485241919756
iteration 221, loss = 0.0010482679354026914
iteration 222, loss = 0.001138091553002596
iteration 223, loss = 0.0009920469019562006
iteration 224, loss = 0.0010136107448488474
iteration 225, loss = 0.0010683531872928143
iteration 226, loss = 0.0010305237956345081
iteration 227, loss = 0.0012226466787979007
iteration 228, loss = 0.001132642268203199
iteration 229, loss = 0.001074459170922637
iteration 230, loss = 0.0009487593779340386
iteration 231, loss = 0.0010263290023431182
iteration 232, loss = 0.0011175913969054818
iteration 233, loss = 0.001424926333129406
iteration 234, loss = 0.0012230819556862116
iteration 235, loss = 0.0009867344051599503
iteration 236, loss = 0.0012640060158446431
iteration 237, loss = 0.0011344440281391144
iteration 238, loss = 0.002893167780712247
iteration 239, loss = 0.0010046791285276413
iteration 240, loss = 0.0011712221894413233
iteration 241, loss = 0.0009798085084185004
iteration 242, loss = 0.0010054847225546837
iteration 243, loss = 0.001732028555124998
iteration 244, loss = 0.0011308951070532203
iteration 245, loss = 0.002996310591697693
iteration 246, loss = 0.0010183017002418637
iteration 247, loss = 0.0011971311178058386
iteration 248, loss = 0.0010520790237933397
iteration 249, loss = 0.0012647409457713366
iteration 250, loss = 0.0012852692743763328
iteration 251, loss = 0.001067816512659192
iteration 252, loss = 0.0009900956647470593
iteration 253, loss = 0.0011301245540380478
iteration 254, loss = 0.0010891989804804325
iteration 255, loss = 0.0033000302501022816
iteration 256, loss = 0.0010770458029583097
iteration 257, loss = 0.0011340714991092682
iteration 258, loss = 0.0009950078092515469
iteration 259, loss = 0.0009971766266971827
iteration 260, loss = 0.0010998410871252418
iteration 261, loss = 0.000991648412309587
iteration 262, loss = 0.0009547054651193321
iteration 263, loss = 0.0017336878227069974
iteration 264, loss = 0.0010153795592486858
iteration 265, loss = 0.0011737624881789088
iteration 266, loss = 0.0009948461083695292
iteration 267, loss = 0.0011601608712226152
iteration 268, loss = 0.0011583655141294003
iteration 269, loss = 0.0012525904458016157
iteration 270, loss = 0.0017724825302138925
iteration 271, loss = 0.0010851285187527537
iteration 272, loss = 0.0024440030101686716
iteration 273, loss = 0.001717057661153376
iteration 274, loss = 0.001070324215106666
iteration 275, loss = 0.0010440897895023227
iteration 276, loss = 0.0029902253299951553
iteration 277, loss = 0.0010987278074026108
iteration 278, loss = 0.0011416019406169653
iteration 279, loss = 0.0010567527497187257
iteration 280, loss = 0.0011756448075175285
iteration 281, loss = 0.0010198407107964158
iteration 282, loss = 0.0010827314108610153
iteration 283, loss = 0.0008901027031242847
iteration 284, loss = 0.0010424914071336389
iteration 285, loss = 0.0009731922764331102
iteration 286, loss = 0.0009791434276849031
iteration 287, loss = 0.001042190589942038
iteration 288, loss = 0.0016075398307293653
iteration 289, loss = 0.001169856870546937
iteration 290, loss = 0.000962474150583148
iteration 291, loss = 0.0011259029852226377
iteration 292, loss = 0.001054384745657444
iteration 293, loss = 0.0010412032715976238
iteration 294, loss = 0.0009263032698072493
iteration 295, loss = 0.0018903955351561308
iteration 296, loss = 0.0010766027262434363
iteration 297, loss = 0.0016211812617257237
iteration 298, loss = 0.0029263407923281193
iteration 299, loss = 0.0010891157435253263
iteration 0, loss = 0.0009998166933655739
iteration 1, loss = 0.001025075209327042
iteration 2, loss = 0.00120677356608212
iteration 3, loss = 0.0011737325694411993
iteration 4, loss = 0.001069439691491425
iteration 5, loss = 0.0012774474453181028
iteration 6, loss = 0.001017527305521071
iteration 7, loss = 0.0012055898550897837
iteration 8, loss = 0.0008830882725305855
iteration 9, loss = 0.0010261073475703597
iteration 10, loss = 0.0009521692409180105
iteration 11, loss = 0.001018330454826355
iteration 12, loss = 0.00294724153354764
iteration 13, loss = 0.0020730686374008656
iteration 14, loss = 0.001077083870768547
iteration 15, loss = 0.001503950683400035
iteration 16, loss = 0.0010051173157989979
iteration 17, loss = 0.0010438102763146162
iteration 18, loss = 0.0010903560323640704
iteration 19, loss = 0.0010402611223980784
iteration 20, loss = 0.0012212060391902924
iteration 21, loss = 0.0014931017067283392
iteration 22, loss = 0.0017298253951594234
iteration 23, loss = 0.0013366045895963907
iteration 24, loss = 0.0012697505299001932
iteration 25, loss = 0.0010628170566633344
iteration 26, loss = 0.0016617226647213101
iteration 27, loss = 0.0012121668551117182
iteration 28, loss = 0.0016977962804958224
iteration 29, loss = 0.0009728651493787766
iteration 30, loss = 0.0012200751807540655
iteration 31, loss = 0.0015058936551213264
iteration 32, loss = 0.0011523512657731771
iteration 33, loss = 0.0009578532772138715
iteration 34, loss = 0.0010459385812282562
iteration 35, loss = 0.0016895930748432875
iteration 36, loss = 0.0010677900863811374
iteration 37, loss = 0.0009632805595174432
iteration 38, loss = 0.0012484095059335232
iteration 39, loss = 0.001165577326901257
iteration 40, loss = 0.000986271072179079
iteration 41, loss = 0.001046838820911944
iteration 42, loss = 0.0010104412212967873
iteration 43, loss = 0.0010063309455290437
iteration 44, loss = 0.000949847511947155
iteration 45, loss = 0.001740862149745226
iteration 46, loss = 0.0011803979286924005
iteration 47, loss = 0.001019626040942967
iteration 48, loss = 0.001258400036022067
iteration 49, loss = 0.0010155925992876291
iteration 50, loss = 0.0010853754356503487
iteration 51, loss = 0.001222739345394075
iteration 52, loss = 0.0009961231844499707
iteration 53, loss = 0.0010607870062813163
iteration 54, loss = 0.0010189325548708439
iteration 55, loss = 0.0030139004811644554
iteration 56, loss = 0.001162930391728878
iteration 57, loss = 0.0009530280367471278
iteration 58, loss = 0.0010515575995668769
iteration 59, loss = 0.0010524170938879251
iteration 60, loss = 0.001141699030995369
iteration 61, loss = 0.0010718435514718294
iteration 62, loss = 0.0010768447536975145
iteration 63, loss = 0.0011164280585944653
iteration 64, loss = 0.0009612921858206391
iteration 65, loss = 0.000992565997876227
iteration 66, loss = 0.0015602053608745337
iteration 67, loss = 0.001082159229554236
iteration 68, loss = 0.0014803046360611916
iteration 69, loss = 0.0011607110500335693
iteration 70, loss = 0.0010080572683364153
iteration 71, loss = 0.000975362490862608
iteration 72, loss = 0.000975203700363636
iteration 73, loss = 0.0009916083654388785
iteration 74, loss = 0.00104621984064579
iteration 75, loss = 0.001122626243159175
iteration 76, loss = 0.0009864546591416001
iteration 77, loss = 0.001049380749464035
iteration 78, loss = 0.0011336719617247581
iteration 79, loss = 0.0010109210852533579
iteration 80, loss = 0.0011590908979997039
iteration 81, loss = 0.0011032226029783487
iteration 82, loss = 0.0010987849673256278
iteration 83, loss = 0.000997300143353641
iteration 84, loss = 0.0009626817191019654
iteration 85, loss = 0.0009678604546934366
iteration 86, loss = 0.001470856019295752
iteration 87, loss = 0.0011595323449000716
iteration 88, loss = 0.0010679359547793865
iteration 89, loss = 0.0010599519591778517
iteration 90, loss = 0.0014203074388206005
iteration 91, loss = 0.000996589194983244
iteration 92, loss = 0.001086140633560717
iteration 93, loss = 0.0028697638772428036
iteration 94, loss = 0.0011141013819724321
iteration 95, loss = 0.0010322222951799631
iteration 96, loss = 0.0011700015747919679
iteration 97, loss = 0.0009616944007575512
iteration 98, loss = 0.001061773393303156
iteration 99, loss = 0.0011064120335504413
iteration 100, loss = 0.0011124461889266968
iteration 101, loss = 0.0013277179095894098
iteration 102, loss = 0.0009055934497155249
iteration 103, loss = 0.001294490648433566
iteration 104, loss = 0.001050071674399078
iteration 105, loss = 0.0009854512754827738
iteration 106, loss = 0.0010003780480474234
iteration 107, loss = 0.002805254654958844
iteration 108, loss = 0.0010425016516819596
iteration 109, loss = 0.0013153785839676857
iteration 110, loss = 0.0009442755253985524
iteration 111, loss = 0.0010309983044862747
iteration 112, loss = 0.0028983389493077993
iteration 113, loss = 0.0011180149158462882
iteration 114, loss = 0.0009692732710391283
iteration 115, loss = 0.001171402633190155
iteration 116, loss = 0.0016667376039549708
iteration 117, loss = 0.0011500375112518668
iteration 118, loss = 0.0010823755292221904
iteration 119, loss = 0.0010958458296954632
iteration 120, loss = 0.002910492941737175
iteration 121, loss = 0.001078539527952671
iteration 122, loss = 0.002774997381493449
iteration 123, loss = 0.0013246344169601798
iteration 124, loss = 0.0013079040218144655
iteration 125, loss = 0.0013075948227196932
iteration 126, loss = 0.0009575183503329754
iteration 127, loss = 0.0011069333413615823
iteration 128, loss = 0.0010835144203156233
iteration 129, loss = 0.000999689451418817
iteration 130, loss = 0.0009466311312280595
iteration 131, loss = 0.0010711709037423134
iteration 132, loss = 0.0009420298156328499
iteration 133, loss = 0.0012578866444528103
iteration 134, loss = 0.0010123818647116423
iteration 135, loss = 0.0010754390386864543
iteration 136, loss = 0.0010566107230260968
iteration 137, loss = 0.001024379744194448
iteration 138, loss = 0.0011049406602978706
iteration 139, loss = 0.0010856539011001587
iteration 140, loss = 0.0011821838561445475
iteration 141, loss = 0.0011730806436389685
iteration 142, loss = 0.0009698834037408233
iteration 143, loss = 0.003210513386875391
iteration 144, loss = 0.0014221173478290439
iteration 145, loss = 0.001415320672094822
iteration 146, loss = 0.0011033822083845735
iteration 147, loss = 0.0009808242321014404
iteration 148, loss = 0.0009513050317764282
iteration 149, loss = 0.0011362828081473708
iteration 150, loss = 0.0017143716104328632
iteration 151, loss = 0.001103588961996138
iteration 152, loss = 0.0009743933333083987
iteration 153, loss = 0.0009490054217167199
iteration 154, loss = 0.0016137895872816443
iteration 155, loss = 0.0010306951589882374
iteration 156, loss = 0.0014691720716655254
iteration 157, loss = 0.0010062383953481913
iteration 158, loss = 0.0010060891509056091
iteration 159, loss = 0.001295357826165855
iteration 160, loss = 0.001082872273400426
iteration 161, loss = 0.0009668358252383769
iteration 162, loss = 0.0010357742430642247
iteration 163, loss = 0.0010271540377289057
iteration 164, loss = 0.0009739348897710443
iteration 165, loss = 0.0009593008435331285
iteration 166, loss = 0.0009817841928452253
iteration 167, loss = 0.000996240647509694
iteration 168, loss = 0.0010028451215475798
iteration 169, loss = 0.0012773334747180343
iteration 170, loss = 0.0009949624072760344
iteration 171, loss = 0.002853268524631858
iteration 172, loss = 0.0014125230954959989
iteration 173, loss = 0.0013953936286270618
iteration 174, loss = 0.0009265675325877964
iteration 175, loss = 0.0009566037333570421
iteration 176, loss = 0.0009510840172879398
iteration 177, loss = 0.0029115795623511076
iteration 178, loss = 0.0009772181510925293
iteration 179, loss = 0.0009356473456136882
iteration 180, loss = 0.000970361870713532
iteration 181, loss = 0.0010514309396967292
iteration 182, loss = 0.002755044959485531
iteration 183, loss = 0.001070112455636263
iteration 184, loss = 0.0010030464036390185
iteration 185, loss = 0.0010362749453634024
iteration 186, loss = 0.0016200134996324778
iteration 187, loss = 0.0012451448710635304
iteration 188, loss = 0.0013509730342775583
iteration 189, loss = 0.0010143355466425419
iteration 190, loss = 0.0010241821873933077
iteration 191, loss = 0.0015835488447919488
iteration 192, loss = 0.0014981810236349702
iteration 193, loss = 0.0009760126704350114
iteration 194, loss = 0.0013381086755543947
iteration 195, loss = 0.0014916813233867288
iteration 196, loss = 0.0010713246883824468
iteration 197, loss = 0.001017415663227439
iteration 198, loss = 0.0009676921181380749
iteration 199, loss = 0.0014703229535371065
iteration 200, loss = 0.0011003816034644842
iteration 201, loss = 0.001015318208374083
iteration 202, loss = 0.0011363644152879715
iteration 203, loss = 0.0009956732392311096
iteration 204, loss = 0.0009510811069048941
iteration 205, loss = 0.0010289011988788843
iteration 206, loss = 0.0010575322667136788
iteration 207, loss = 0.001167594688013196
iteration 208, loss = 0.001491259434260428
iteration 209, loss = 0.0015624134102836251
iteration 210, loss = 0.0012163700303062797
iteration 211, loss = 0.0010807329090312123
iteration 212, loss = 0.0009466604678891599
iteration 213, loss = 0.0010541104711592197
iteration 214, loss = 0.0011278993915766478
iteration 215, loss = 0.001162656582891941
iteration 216, loss = 0.0010737850097939372
iteration 217, loss = 0.0016461543273180723
iteration 218, loss = 0.0012066508643329144
iteration 219, loss = 0.0009327728184871376
iteration 220, loss = 0.0008429619483649731
iteration 221, loss = 0.000896062352694571
iteration 222, loss = 0.0009516639402136207
iteration 223, loss = 0.0012300334637984633
iteration 224, loss = 0.0009817738318815827
iteration 225, loss = 0.0009709100122563541
iteration 226, loss = 0.001283808145672083
iteration 227, loss = 0.0009249889990314841
iteration 228, loss = 0.000994549016468227
iteration 229, loss = 0.0011107528116554022
iteration 230, loss = 0.0009486904018558562
iteration 231, loss = 0.0010324218310415745
iteration 232, loss = 0.0008969120099209249
iteration 233, loss = 0.0010572993196547031
iteration 234, loss = 0.0009637708426453173
iteration 235, loss = 0.0010092697339132428
iteration 236, loss = 0.000940480618737638
iteration 237, loss = 0.001023250981234014
iteration 238, loss = 0.002774888649582863
iteration 239, loss = 0.0013349625514820218
iteration 240, loss = 0.0012550578685477376
iteration 241, loss = 0.0009762119734659791
iteration 242, loss = 0.0009905589977279305
iteration 243, loss = 0.0010103731183335185
iteration 244, loss = 0.0009408305631950498
iteration 245, loss = 0.0009378826362080872
iteration 246, loss = 0.001001145807094872
iteration 247, loss = 0.0016063330695033073
iteration 248, loss = 0.0011127188336104155
iteration 249, loss = 0.0009605103405192494
iteration 250, loss = 0.0010925679234787822
iteration 251, loss = 0.0010219160467386246
iteration 252, loss = 0.0016249916516244411
iteration 253, loss = 0.0009798010578379035
iteration 254, loss = 0.0011540194973349571
iteration 255, loss = 0.0033248381223529577
iteration 256, loss = 0.001007324899546802
iteration 257, loss = 0.001001704134978354
iteration 258, loss = 0.001099271117709577
iteration 259, loss = 0.0010180885437875986
iteration 260, loss = 0.0009537338046357036
iteration 261, loss = 0.0010601170361042023
iteration 262, loss = 0.0009423191077075899
iteration 263, loss = 0.001142159104347229
iteration 264, loss = 0.0010379183804616332
iteration 265, loss = 0.0010128910653293133
iteration 266, loss = 0.0015376413939520717
iteration 267, loss = 0.0016270920168608427
iteration 268, loss = 0.0009815198136493564
iteration 269, loss = 0.0009541659383103251
iteration 270, loss = 0.0010012979619204998
iteration 271, loss = 0.0010852923151105642
iteration 272, loss = 0.001116831204853952
iteration 273, loss = 0.0011893755290657282
iteration 274, loss = 0.0011517598759382963
iteration 275, loss = 0.0010625768918544054
iteration 276, loss = 0.0010896670864894986
iteration 277, loss = 0.0009779691463336349
iteration 278, loss = 0.0010250350460410118
iteration 279, loss = 0.001233372138813138
iteration 280, loss = 0.0010400962783023715
iteration 281, loss = 0.0008885799325071275
iteration 282, loss = 0.0009892857633531094
iteration 283, loss = 0.000994552276097238
iteration 284, loss = 0.0013973006280139089
iteration 285, loss = 0.0009505546186119318
iteration 286, loss = 0.0009989130776375532
iteration 287, loss = 0.0009589003748260438
iteration 288, loss = 0.0010055333841592073
iteration 289, loss = 0.0010569881414994597
iteration 290, loss = 0.0009209931595250964
iteration 291, loss = 0.000980472774244845
iteration 292, loss = 0.0010014224098995328
iteration 293, loss = 0.0009756385115906596
iteration 294, loss = 0.0009055060218088329
iteration 295, loss = 0.0012078717118129134
iteration 296, loss = 0.0027701708022505045
iteration 297, loss = 0.0009526164503768086
iteration 298, loss = 0.003666186472401023
iteration 299, loss = 0.0010501970537006855
iteration 0, loss = 0.0009571042028255761
iteration 1, loss = 0.0013298954581841826
iteration 2, loss = 0.0009850547648966312
iteration 3, loss = 0.0020506568253040314
iteration 4, loss = 0.001101905945688486
iteration 5, loss = 0.0010183581616729498
iteration 6, loss = 0.0009510836680419743
iteration 7, loss = 0.0009328707819804549
iteration 8, loss = 0.0009649565909057856
iteration 9, loss = 0.0010388933587819338
iteration 10, loss = 0.0010211493354290724
iteration 11, loss = 0.0009083664044737816
iteration 12, loss = 0.0009320513345301151
iteration 13, loss = 0.0027072164230048656
iteration 14, loss = 0.001045000972226262
iteration 15, loss = 0.0011838747886940837
iteration 16, loss = 0.0009246637928299606
iteration 17, loss = 0.0010369718074798584
iteration 18, loss = 0.002781245857477188
iteration 19, loss = 0.0010753448586910963
iteration 20, loss = 0.0009831166826188564
iteration 21, loss = 0.0009831043425947428
iteration 22, loss = 0.0010628029704093933
iteration 23, loss = 0.0010436357697471976
iteration 24, loss = 0.0027410751208662987
iteration 25, loss = 0.0010689181508496404
iteration 26, loss = 0.000998782692477107
iteration 27, loss = 0.0009033764945343137
iteration 28, loss = 0.002869189251214266
iteration 29, loss = 0.0010100360959768295
iteration 30, loss = 0.001148026087321341
iteration 31, loss = 0.00157726532779634
iteration 32, loss = 0.0010335322003811598
iteration 33, loss = 0.001005435478873551
iteration 34, loss = 0.0026816988829523325
iteration 35, loss = 0.0009582996717654169
iteration 36, loss = 0.001515096751973033
iteration 37, loss = 0.0010032971622422338
iteration 38, loss = 0.0009102141484618187
iteration 39, loss = 0.0010439733741804957
iteration 40, loss = 0.0009651716100051999
iteration 41, loss = 0.0009289471199735999
iteration 42, loss = 0.0009980803588405252
iteration 43, loss = 0.0010693747317418456
iteration 44, loss = 0.0009911765810102224
iteration 45, loss = 0.0012090221280232072
iteration 46, loss = 0.0009060463053174317
iteration 47, loss = 0.0010064049856737256
iteration 48, loss = 0.0010063080117106438
iteration 49, loss = 0.0009686466073617339
iteration 50, loss = 0.0010554922046139836
iteration 51, loss = 0.001008910359814763
iteration 52, loss = 0.0013678154209628701
iteration 53, loss = 0.0011172471567988396
iteration 54, loss = 0.0009084958001039922
iteration 55, loss = 0.000961362267844379
iteration 56, loss = 0.0015386344166472554
iteration 57, loss = 0.0013242363929748535
iteration 58, loss = 0.0009673902532085776
iteration 59, loss = 0.0010742609156295657
iteration 60, loss = 0.001172670628875494
iteration 61, loss = 0.0010767882922664285
iteration 62, loss = 0.000981808640062809
iteration 63, loss = 0.0009784087305888534
iteration 64, loss = 0.0013874948490411043
iteration 65, loss = 0.0011351184220984578
iteration 66, loss = 0.0010848080273717642
iteration 67, loss = 0.0009322729893028736
iteration 68, loss = 0.0012910400982946157
iteration 69, loss = 0.0010142996907234192
iteration 70, loss = 0.0009601818164810538
iteration 71, loss = 0.000903319742064923
iteration 72, loss = 0.0016183938132598996
iteration 73, loss = 0.0008977213292382658
iteration 74, loss = 0.0012591809500008821
iteration 75, loss = 0.0010559175861999393
iteration 76, loss = 0.0009175653103739023
iteration 77, loss = 0.0011498406529426575
iteration 78, loss = 0.001011596410535276
iteration 79, loss = 0.0008752226713113487
iteration 80, loss = 0.0009325340506620705
iteration 81, loss = 0.0009235258912667632
iteration 82, loss = 0.0010418180609121919
iteration 83, loss = 0.0010235767113044858
iteration 84, loss = 0.0011136909015476704
iteration 85, loss = 0.000990213011391461
iteration 86, loss = 0.0010647588642314076
iteration 87, loss = 0.0009746706928126514
iteration 88, loss = 0.00126603152602911
iteration 89, loss = 0.0030682256910949945
iteration 90, loss = 0.0012238333001732826
iteration 91, loss = 0.0015202201902866364
iteration 92, loss = 0.0009561073966324329
iteration 93, loss = 0.0010061676148325205
iteration 94, loss = 0.0009175456361845136
iteration 95, loss = 0.0009505352936685085
iteration 96, loss = 0.003090847050771117
iteration 97, loss = 0.0009274015901610255
iteration 98, loss = 0.0012237343471497297
iteration 99, loss = 0.0010633132187649608
iteration 100, loss = 0.0010299623245373368
iteration 101, loss = 0.0009310304303653538
iteration 102, loss = 0.0013301362050697207
iteration 103, loss = 0.00096104945987463
iteration 104, loss = 0.0013532772427424788
iteration 105, loss = 0.0009073542896658182
iteration 106, loss = 0.0009930372470989823
iteration 107, loss = 0.002667909488081932
iteration 108, loss = 0.0008131427457556129
iteration 109, loss = 0.0010005190270021558
iteration 110, loss = 0.0010318778222426772
iteration 111, loss = 0.0012536649592220783
iteration 112, loss = 0.0010546701960265636
iteration 113, loss = 0.0009654852328822017
iteration 114, loss = 0.000928884488530457
iteration 115, loss = 0.0009577111341059208
iteration 116, loss = 0.001178811420686543
iteration 117, loss = 0.0018146358197554946
iteration 118, loss = 0.0032981224358081818
iteration 119, loss = 0.0014467567671090364
iteration 120, loss = 0.0010499422205612063
iteration 121, loss = 0.0009130825055763125
iteration 122, loss = 0.0010873139835894108
iteration 123, loss = 0.0011049031745642424
iteration 124, loss = 0.0015712467720732093
iteration 125, loss = 0.0009707025601528585
iteration 126, loss = 0.0009359034011140466
iteration 127, loss = 0.0013371146051213145
iteration 128, loss = 0.0009264894179068506
iteration 129, loss = 0.0009850941132754087
iteration 130, loss = 0.000906355504412204
iteration 131, loss = 0.00100799894426018
iteration 132, loss = 0.0009477603016421199
iteration 133, loss = 0.000899944338016212
iteration 134, loss = 0.001058019232004881
iteration 135, loss = 0.0009929494699463248
iteration 136, loss = 0.0011189732467755675
iteration 137, loss = 0.0009352484485134482
iteration 138, loss = 0.0009637920302338898
iteration 139, loss = 0.0014613424427807331
iteration 140, loss = 0.0009595839073881507
iteration 141, loss = 0.0029446794651448727
iteration 142, loss = 0.0010541252559050918
iteration 143, loss = 0.0014213871909305453
iteration 144, loss = 0.0011064676800742745
iteration 145, loss = 0.0011743714567273855
iteration 146, loss = 0.0009491918608546257
iteration 147, loss = 0.0014043633127585053
iteration 148, loss = 0.0009524539927951992
iteration 149, loss = 0.001586538041010499
iteration 150, loss = 0.0010015676962211728
iteration 151, loss = 0.0009498685831204057
iteration 152, loss = 0.0009519127197563648
iteration 153, loss = 0.0010496523464098573
iteration 154, loss = 0.001002901466563344
iteration 155, loss = 0.0009817990940064192
iteration 156, loss = 0.0010387274669483304
iteration 157, loss = 0.0009182812063954771
iteration 158, loss = 0.0012020914582535625
iteration 159, loss = 0.0010664360597729683
iteration 160, loss = 0.001031607505865395
iteration 161, loss = 0.0010596534702926874
iteration 162, loss = 0.0010034351143985987
iteration 163, loss = 0.0015845919260755181
iteration 164, loss = 0.0009935047710314393
iteration 165, loss = 0.001038375310599804
iteration 166, loss = 0.0010755471885204315
iteration 167, loss = 0.0009660926298238337
iteration 168, loss = 0.0013192362384870648
iteration 169, loss = 0.0008805575780570507
iteration 170, loss = 0.0009888764470815659
iteration 171, loss = 0.0009486963390372694
iteration 172, loss = 0.0010688407346606255
iteration 173, loss = 0.000948002387303859
iteration 174, loss = 0.0009456677362322807
iteration 175, loss = 0.0010790504748001695
iteration 176, loss = 0.0010980217484757304
iteration 177, loss = 0.0008944133878685534
iteration 178, loss = 0.000997380819171667
iteration 179, loss = 0.0009942392352968454
iteration 180, loss = 0.0009152287384495139
iteration 181, loss = 0.0009374503279104829
iteration 182, loss = 0.0008381901425309479
iteration 183, loss = 0.0014227404026314616
iteration 184, loss = 0.0015228248666971922
iteration 185, loss = 0.0009557442390359938
iteration 186, loss = 0.0008945794543251395
iteration 187, loss = 0.0009237466147169471
iteration 188, loss = 0.0010429223766550422
iteration 189, loss = 0.0009531257674098015
iteration 190, loss = 0.0010928300907835364
iteration 191, loss = 0.0010911369463428855
iteration 192, loss = 0.001011038781143725
iteration 193, loss = 0.0011825250694528222
iteration 194, loss = 0.000857909326441586
iteration 195, loss = 0.001550239510834217
iteration 196, loss = 0.0010541887022554874
iteration 197, loss = 0.001016252557747066
iteration 198, loss = 0.0009183164802379906
iteration 199, loss = 0.0013201762922108173
iteration 200, loss = 0.0009823446162045002
iteration 201, loss = 0.0010155490599572659
iteration 202, loss = 0.0010424336651340127
iteration 203, loss = 0.001248350366950035
iteration 204, loss = 0.0009190846467390656
iteration 205, loss = 0.0010037967003881931
iteration 206, loss = 0.000908720656298101
iteration 207, loss = 0.001032662228681147
iteration 208, loss = 0.001015983521938324
iteration 209, loss = 0.0009606573148630559
iteration 210, loss = 0.0010203180136159062
iteration 211, loss = 0.0008899443200789392
iteration 212, loss = 0.0015844233566895127
iteration 213, loss = 0.001016669673845172
iteration 214, loss = 0.0009437076514586806
iteration 215, loss = 0.0012098222505301237
iteration 216, loss = 0.0010439157485961914
iteration 217, loss = 0.0009247837006114423
iteration 218, loss = 0.0014894665218889713
iteration 219, loss = 0.0010180098470300436
iteration 220, loss = 0.0009272904717363417
iteration 221, loss = 0.0009638402261771262
iteration 222, loss = 0.0012741694226861
iteration 223, loss = 0.0009606991079635918
iteration 224, loss = 0.00095563312061131
iteration 225, loss = 0.001042233663611114
iteration 226, loss = 0.0009758685482665896
iteration 227, loss = 0.0008999632555060089
iteration 228, loss = 0.000898314465302974
iteration 229, loss = 0.0010776517447084188
iteration 230, loss = 0.0008397412020713091
iteration 231, loss = 0.0009277654462493956
iteration 232, loss = 0.0015419133706018329
iteration 233, loss = 0.0009789570467546582
iteration 234, loss = 0.0012404249282553792
iteration 235, loss = 0.0010212678462266922
iteration 236, loss = 0.0010537743801251054
iteration 237, loss = 0.003195823635905981
iteration 238, loss = 0.0011924691498279572
iteration 239, loss = 0.0010000893380492926
iteration 240, loss = 0.0008843059185892344
iteration 241, loss = 0.0016005415236577392
iteration 242, loss = 0.0009532939875498414
iteration 243, loss = 0.0009560408070683479
iteration 244, loss = 0.0009284052648581564
iteration 245, loss = 0.0010697979014366865
iteration 246, loss = 0.001014683279208839
iteration 247, loss = 0.0009744276176206768
iteration 248, loss = 0.0010017752647399902
iteration 249, loss = 0.0009103140328079462
iteration 250, loss = 0.0010700938291847706
iteration 251, loss = 0.0009523422340862453
iteration 252, loss = 0.0008940146071836352
iteration 253, loss = 0.0008913111523725092
iteration 254, loss = 0.0008912610937841237
iteration 255, loss = 0.0009167245589196682
iteration 256, loss = 0.0029062116518616676
iteration 257, loss = 0.000944924948271364
iteration 258, loss = 0.0008647635113447905
iteration 259, loss = 0.0016254145884886384
iteration 260, loss = 0.0009143444476649165
iteration 261, loss = 0.0010249954648315907
iteration 262, loss = 0.0010956989135593176
iteration 263, loss = 0.0010449299588799477
iteration 264, loss = 0.0010642308043316007
iteration 265, loss = 0.0014997835969552398
iteration 266, loss = 0.0009108027443289757
iteration 267, loss = 0.0009017677512019873
iteration 268, loss = 0.000976984272710979
iteration 269, loss = 0.0008701736805960536
iteration 270, loss = 0.0009708732832223177
iteration 271, loss = 0.000959547352977097
iteration 272, loss = 0.0008678092854097486
iteration 273, loss = 0.0025557498447597027
iteration 274, loss = 0.000945011037401855
iteration 275, loss = 0.0010727177141234279
iteration 276, loss = 0.0008949031471274793
iteration 277, loss = 0.0011170664802193642
iteration 278, loss = 0.0008639175212010741
iteration 279, loss = 0.0009252237505279481
iteration 280, loss = 0.0009952521650120616
iteration 281, loss = 0.0009364889119751751
iteration 282, loss = 0.0009127595694735646
iteration 283, loss = 0.001509690540842712
iteration 284, loss = 0.0011185167822986841
iteration 285, loss = 0.0010354539845138788
iteration 286, loss = 0.003180167870596051
iteration 287, loss = 0.0009208170231431723
iteration 288, loss = 0.0009323630365543067
iteration 289, loss = 0.0009382375865243375
iteration 290, loss = 0.0009489626390859485
iteration 291, loss = 0.000993750523775816
iteration 292, loss = 0.0010895790765061975
iteration 293, loss = 0.0008606566698290408
iteration 294, loss = 0.0010231782216578722
iteration 295, loss = 0.0010382990585640073
iteration 296, loss = 0.001053227693773806
iteration 297, loss = 0.0009676052141003311
iteration 298, loss = 0.0009818332036957145
iteration 299, loss = 0.0026601480785757303
iteration 0, loss = 0.0010407444788143039
iteration 1, loss = 0.0008630456868559122
iteration 2, loss = 0.0008905860595405102
iteration 3, loss = 0.0010125667322427034
iteration 4, loss = 0.0009611421264708042
iteration 5, loss = 0.003098962362855673
iteration 6, loss = 0.0013399990275502205
iteration 7, loss = 0.0009461797890253365
iteration 8, loss = 0.0010244152508676052
iteration 9, loss = 0.000873526674695313
iteration 10, loss = 0.0010391285177320242
iteration 11, loss = 0.0009573674760758877
iteration 12, loss = 0.0009952683467417955
iteration 13, loss = 0.001079462468624115
iteration 14, loss = 0.0009142531780526042
iteration 15, loss = 0.001059865695424378
iteration 16, loss = 0.0014699308667331934
iteration 17, loss = 0.0013009078102186322
iteration 18, loss = 0.000950950721744448
iteration 19, loss = 0.0008698283927515149
iteration 20, loss = 0.0009270576993003488
iteration 21, loss = 0.002005143789574504
iteration 22, loss = 0.0009727678843773901
iteration 23, loss = 0.0009113759151659906
iteration 24, loss = 0.0009252491872757673
iteration 25, loss = 0.0009769019670784473
iteration 26, loss = 0.0013108985731378198
iteration 27, loss = 0.0009346064762212336
iteration 28, loss = 0.0009197203908115625
iteration 29, loss = 0.0014487786684185266
iteration 30, loss = 0.0008670733077451587
iteration 31, loss = 0.000975751259829849
iteration 32, loss = 0.0009260553051717579
iteration 33, loss = 0.0009391981293447316
iteration 34, loss = 0.0010956466430798173
iteration 35, loss = 0.0010612013284116983
iteration 36, loss = 0.001151415053755045
iteration 37, loss = 0.0010006598895415664
iteration 38, loss = 0.0015034517273306847
iteration 39, loss = 0.0009073169785551727
iteration 40, loss = 0.0009687008569017053
iteration 41, loss = 0.0010773981921374798
iteration 42, loss = 0.0026078317314386368
iteration 43, loss = 0.0008990128990262747
iteration 44, loss = 0.0009853651281446218
iteration 45, loss = 0.0011513952631503344
iteration 46, loss = 0.0010116653284057975
iteration 47, loss = 0.0009342161356471479
iteration 48, loss = 0.0008328727562911808
iteration 49, loss = 0.000868584553245455
iteration 50, loss = 0.0012472845846787095
iteration 51, loss = 0.0009365997975692153
iteration 52, loss = 0.0008352171280421317
iteration 53, loss = 0.0016740488354116678
iteration 54, loss = 0.0010046081151813269
iteration 55, loss = 0.0009675076580606401
iteration 56, loss = 0.0010201893746852875
iteration 57, loss = 0.0010168887674808502
iteration 58, loss = 0.0010721611324697733
iteration 59, loss = 0.0009655522298999131
iteration 60, loss = 0.0009719005320221186
iteration 61, loss = 0.000911059498321265
iteration 62, loss = 0.0009421734139323235
iteration 63, loss = 0.0009034890681505203
iteration 64, loss = 0.0009001785074360669
iteration 65, loss = 0.0008780924254097044
iteration 66, loss = 0.0010726976906880736
iteration 67, loss = 0.0016358416760340333
iteration 68, loss = 0.000846587005071342
iteration 69, loss = 0.0008367440896108747
iteration 70, loss = 0.0017923788400366902
iteration 71, loss = 0.0010479299817234278
iteration 72, loss = 0.0009502347675152123
iteration 73, loss = 0.0010195029899477959
iteration 74, loss = 0.0008487616432830691
iteration 75, loss = 0.0009298974182456732
iteration 76, loss = 0.0010171476751565933
iteration 77, loss = 0.0011400142684578896
iteration 78, loss = 0.002571159042418003
iteration 79, loss = 0.0009333220077678561
iteration 80, loss = 0.002899999963119626
iteration 81, loss = 0.0008516882080584764
iteration 82, loss = 0.0010515048634260893
iteration 83, loss = 0.0009048715583048761
iteration 84, loss = 0.0009799611289054155
iteration 85, loss = 0.0008805033867247403
iteration 86, loss = 0.0009682062081992626
iteration 87, loss = 0.0009114478016272187
iteration 88, loss = 0.0010080900974571705
iteration 89, loss = 0.0008793865563347936
iteration 90, loss = 0.0009498095605522394
iteration 91, loss = 0.0009737549116834998
iteration 92, loss = 0.0009780076798051596
iteration 93, loss = 0.001024096505716443
iteration 94, loss = 0.0014478570083156228
iteration 95, loss = 0.0009154596482403576
iteration 96, loss = 0.0009304244886152446
iteration 97, loss = 0.0014806202379986644
iteration 98, loss = 0.0010149501031264663
iteration 99, loss = 0.0008395031909458339
iteration 100, loss = 0.0008619906730018556
iteration 101, loss = 0.0009820905979722738
iteration 102, loss = 0.000909461232367903
iteration 103, loss = 0.0008289600955322385
iteration 104, loss = 0.002562270499765873
iteration 105, loss = 0.0008696836302988231
iteration 106, loss = 0.0009615307790227234
iteration 107, loss = 0.0012630423298105597
iteration 108, loss = 0.0009123731288127601
iteration 109, loss = 0.0009500393643975258
iteration 110, loss = 0.0010082382941618562
iteration 111, loss = 0.0027416637167334557
iteration 112, loss = 0.0008611857192590833
iteration 113, loss = 0.0008753497386351228
iteration 114, loss = 0.001177329570055008
iteration 115, loss = 0.0012202798388898373
iteration 116, loss = 0.000904400774743408
iteration 117, loss = 0.002592473290860653
iteration 118, loss = 0.0012371342163532972
iteration 119, loss = 0.0010259284172207117
iteration 120, loss = 0.0009162696660496294
iteration 121, loss = 0.0015848350012674928
iteration 122, loss = 0.0009124559001065791
iteration 123, loss = 0.0012098070001229644
iteration 124, loss = 0.0009047063067555428
iteration 125, loss = 0.0017402388621121645
iteration 126, loss = 0.001251417095772922
iteration 127, loss = 0.001027796184644103
iteration 128, loss = 0.000883274304214865
iteration 129, loss = 0.0008847685530781746
iteration 130, loss = 0.000907129782717675
iteration 131, loss = 0.0008060532272793353
iteration 132, loss = 0.0009395366651006043
iteration 133, loss = 0.0008894665515981615
iteration 134, loss = 0.0009315497591160238
iteration 135, loss = 0.0009273995528928936
iteration 136, loss = 0.0010158101795241237
iteration 137, loss = 0.000932422059122473
iteration 138, loss = 0.0008749822736717761
iteration 139, loss = 0.0008890646859072149
iteration 140, loss = 0.0009959973394870758
iteration 141, loss = 0.0009590581757947803
iteration 142, loss = 0.0009974405402317643
iteration 143, loss = 0.0008619570289738476
iteration 144, loss = 0.0008431342430412769
iteration 145, loss = 0.0014576936373487115
iteration 146, loss = 0.001171814277768135
iteration 147, loss = 0.0009763197158463299
iteration 148, loss = 0.001077623339369893
iteration 149, loss = 0.0009387492318637669
iteration 150, loss = 0.0008660339517518878
iteration 151, loss = 0.0010506205726414919
iteration 152, loss = 0.0014614948304370046
iteration 153, loss = 0.001501200022175908
iteration 154, loss = 0.0010757308918982744
iteration 155, loss = 0.000923503190279007
iteration 156, loss = 0.0009071428212337196
iteration 157, loss = 0.0012489277869462967
iteration 158, loss = 0.0009001196594908834
iteration 159, loss = 0.0009008732158690691
iteration 160, loss = 0.000977667048573494
iteration 161, loss = 0.0010364343179389834
iteration 162, loss = 0.0009932775283232331
iteration 163, loss = 0.001697539584711194
iteration 164, loss = 0.0015960584860295057
iteration 165, loss = 0.0008904039277695119
iteration 166, loss = 0.0009701106464490294
iteration 167, loss = 0.000862991320900619
iteration 168, loss = 0.0008659400045871735
iteration 169, loss = 0.002600160427391529
iteration 170, loss = 0.0014698252780362964
iteration 171, loss = 0.0010158095974475145
iteration 172, loss = 0.001012240070849657
iteration 173, loss = 0.000925283005926758
iteration 174, loss = 0.0008939169347286224
iteration 175, loss = 0.0009483813773840666
iteration 176, loss = 0.002547408454120159
iteration 177, loss = 0.0013721382711082697
iteration 178, loss = 0.0008913232013583183
iteration 179, loss = 0.0009278699290007353
iteration 180, loss = 0.0009714302141219378
iteration 181, loss = 0.000983119593001902
iteration 182, loss = 0.0008747903048060834
iteration 183, loss = 0.0012848033802583814
iteration 184, loss = 0.0008744306396692991
iteration 185, loss = 0.001264636986888945
iteration 186, loss = 0.0008611592347733676
iteration 187, loss = 0.0009604652877897024
iteration 188, loss = 0.000838480715174228
iteration 189, loss = 0.0008924287976697087
iteration 190, loss = 0.0010956452460959554
iteration 191, loss = 0.0009519869927316904
iteration 192, loss = 0.0008618615684099495
iteration 193, loss = 0.0008426459971815348
iteration 194, loss = 0.0010771309025585651
iteration 195, loss = 0.0010060181375592947
iteration 196, loss = 0.0008674269774928689
iteration 197, loss = 0.0010061596985906363
iteration 198, loss = 0.0010153165785595775
iteration 199, loss = 0.0009310244349762797
iteration 200, loss = 0.0008667726651765406
iteration 201, loss = 0.0013352071400731802
iteration 202, loss = 0.0008460140088573098
iteration 203, loss = 0.0008756601018831134
iteration 204, loss = 0.0010090102441608906
iteration 205, loss = 0.000923078681807965
iteration 206, loss = 0.001009509083814919
iteration 207, loss = 0.0009194809244945645
iteration 208, loss = 0.0009825497400015593
iteration 209, loss = 0.0009014958050101995
iteration 210, loss = 0.0010279135312885046
iteration 211, loss = 0.0010037119500339031
iteration 212, loss = 0.0025912774726748466
iteration 213, loss = 0.0009373130742460489
iteration 214, loss = 0.00100415269844234
iteration 215, loss = 0.0009536606958135962
iteration 216, loss = 0.000982165802270174
iteration 217, loss = 0.0009255069307982922
iteration 218, loss = 0.0008241281029768288
iteration 219, loss = 0.0008734301663935184
iteration 220, loss = 0.0008766151149757206
iteration 221, loss = 0.0008926249574869871
iteration 222, loss = 0.0009472476085647941
iteration 223, loss = 0.0009465357870794833
iteration 224, loss = 0.0013606937136501074
iteration 225, loss = 0.0008876380743458867
iteration 226, loss = 0.001038338290527463
iteration 227, loss = 0.0010053524747490883
iteration 228, loss = 0.0009694038890302181
iteration 229, loss = 0.0009016117546707392
iteration 230, loss = 0.0008344684611074626
iteration 231, loss = 0.0010378726292401552
iteration 232, loss = 0.000955230847466737
iteration 233, loss = 0.0009418738191016018
iteration 234, loss = 0.0008432719041593373
iteration 235, loss = 0.0009532571421004832
iteration 236, loss = 0.0009120345930568874
iteration 237, loss = 0.0009626359096728265
iteration 238, loss = 0.0010074963793158531
iteration 239, loss = 0.0010393466800451279
iteration 240, loss = 0.0009118641028180718
iteration 241, loss = 0.0009602541103959084
iteration 242, loss = 0.001002292730845511
iteration 243, loss = 0.0009097490692511201
iteration 244, loss = 0.0009308287408202887
iteration 245, loss = 0.0010007059900090098
iteration 246, loss = 0.0009869055356830359
iteration 247, loss = 0.0009825819870457053
iteration 248, loss = 0.001097132102586329
iteration 249, loss = 0.0009535736171528697
iteration 250, loss = 0.0009640806238166988
iteration 251, loss = 0.0008730058907531202
iteration 252, loss = 0.0008791920845396817
iteration 253, loss = 0.0012250423897057772
iteration 254, loss = 0.002782579977065325
iteration 255, loss = 0.0016626566648483276
iteration 256, loss = 0.0024953498505055904
iteration 257, loss = 0.0009263437823392451
iteration 258, loss = 0.0009132520062848926
iteration 259, loss = 0.0008798796916380525
iteration 260, loss = 0.0009297890937887132
iteration 261, loss = 0.0010080247884616256
iteration 262, loss = 0.0009073290275409818
iteration 263, loss = 0.0008749490370973945
iteration 264, loss = 0.0009926923085004091
iteration 265, loss = 0.0009392826468683779
iteration 266, loss = 0.0008747554384171963
iteration 267, loss = 0.0008504953584633768
iteration 268, loss = 0.0008246907964348793
iteration 269, loss = 0.0008997379918582737
iteration 270, loss = 0.0010004277573898435
iteration 271, loss = 0.0009471952216699719
iteration 272, loss = 0.0010085133835673332
iteration 273, loss = 0.0009953740518540144
iteration 274, loss = 0.002626746892929077
iteration 275, loss = 0.0010154268238693476
iteration 276, loss = 0.0012118323938921094
iteration 277, loss = 0.00260867434553802
iteration 278, loss = 0.0010964132379740477
iteration 279, loss = 0.0009527988149784505
iteration 280, loss = 0.0008820149232633412
iteration 281, loss = 0.0009188956464640796
iteration 282, loss = 0.0024847635067999363
iteration 283, loss = 0.001160942716524005
iteration 284, loss = 0.0008994590025395155
iteration 285, loss = 0.0008721077465452254
iteration 286, loss = 0.0009089191444218159
iteration 287, loss = 0.0010127078276127577
iteration 288, loss = 0.0014425595290958881
iteration 289, loss = 0.0008503156714141369
iteration 290, loss = 0.0009516403661109507
iteration 291, loss = 0.000949963228777051
iteration 292, loss = 0.001581384101882577
iteration 293, loss = 0.0009532265830785036
iteration 294, loss = 0.0010444022482261062
iteration 295, loss = 0.0009414237574674189
iteration 296, loss = 0.0009389980114065111
iteration 297, loss = 0.0009832973591983318
iteration 298, loss = 0.0008276853477582335
iteration 299, loss = 0.001020123134367168
iteration 0, loss = 0.0009438510751351714
iteration 1, loss = 0.0011735326843336225
iteration 2, loss = 0.0009241963271051645
iteration 3, loss = 0.0010159756056964397
iteration 4, loss = 0.0008484020363539457
iteration 5, loss = 0.001406562514603138
iteration 6, loss = 0.0012444737367331982
iteration 7, loss = 0.0009518627193756402
iteration 8, loss = 0.001415227772668004
iteration 9, loss = 0.0009406859753653407
iteration 10, loss = 0.0008673295960761607
iteration 11, loss = 0.001095466548576951
iteration 12, loss = 0.002418442163616419
iteration 13, loss = 0.0013890156988054514
iteration 14, loss = 0.0008962262654677033
iteration 15, loss = 0.0010079725179821253
iteration 16, loss = 0.0013610458699986339
iteration 17, loss = 0.0009363989811390638
iteration 18, loss = 0.001070607453584671
iteration 19, loss = 0.0008508582250215113
iteration 20, loss = 0.000948265369515866
iteration 21, loss = 0.0009092394611798227
iteration 22, loss = 0.0012255925685167313
iteration 23, loss = 0.0009398943511769176
iteration 24, loss = 0.001324838143773377
iteration 25, loss = 0.000877758429851383
iteration 26, loss = 0.0025973946321755648
iteration 27, loss = 0.0007809181115590036
iteration 28, loss = 0.0009744248236529529
iteration 29, loss = 0.0008906555594876409
iteration 30, loss = 0.0008360978681594133
iteration 31, loss = 0.0009274426265619695
iteration 32, loss = 0.0008523660944774747
iteration 33, loss = 0.0010253885993734002
iteration 34, loss = 0.0008414271869696677
iteration 35, loss = 0.002469348721206188
iteration 36, loss = 0.0008744013030081987
iteration 37, loss = 0.0009047192288562655
iteration 38, loss = 0.000979861244559288
iteration 39, loss = 0.001478709396906197
iteration 40, loss = 0.000918775680474937
iteration 41, loss = 0.0010463384678587317
iteration 42, loss = 0.0011787444818764925
iteration 43, loss = 0.0009358818060718477
iteration 44, loss = 0.0009011364309117198
iteration 45, loss = 0.0009753843769431114
iteration 46, loss = 0.0008815346518531442
iteration 47, loss = 0.0008809253922663629
iteration 48, loss = 0.0008635036647319794
iteration 49, loss = 0.0024380844552069902
iteration 50, loss = 0.0009654078166931868
iteration 51, loss = 0.000826386793050915
iteration 52, loss = 0.0008363531669601798
iteration 53, loss = 0.00252281385473907
iteration 54, loss = 0.0008871760801412165
iteration 55, loss = 0.0008854589541442692
iteration 56, loss = 0.0009395292727276683
iteration 57, loss = 0.0009273671894334257
iteration 58, loss = 0.0008263064082711935
iteration 59, loss = 0.0008498215465806425
iteration 60, loss = 0.0009837636025622487
iteration 61, loss = 0.0008807755075395107
iteration 62, loss = 0.0011860401136800647
iteration 63, loss = 0.0008690666873008013
iteration 64, loss = 0.0008812175365164876
iteration 65, loss = 0.000982989789918065
iteration 66, loss = 0.00114024942740798
iteration 67, loss = 0.0009088041260838509
iteration 68, loss = 0.0010135603370144963
iteration 69, loss = 0.001401432673446834
iteration 70, loss = 0.0014271049294620752
iteration 71, loss = 0.0024448749609291553
iteration 72, loss = 0.0009123592753894627
iteration 73, loss = 0.0008657563012093306
iteration 74, loss = 0.0009011522633954883
iteration 75, loss = 0.0008325279341079295
iteration 76, loss = 0.00094869319582358
iteration 77, loss = 0.0008339789346791804
iteration 78, loss = 0.001011283602565527
iteration 79, loss = 0.0008391777519136667
iteration 80, loss = 0.0009396114619448781
iteration 81, loss = 0.001217429293319583
iteration 82, loss = 0.000869173847604543
iteration 83, loss = 0.0026004896499216557
iteration 84, loss = 0.0008900719112716615
iteration 85, loss = 0.0008789324201643467
iteration 86, loss = 0.0008297388558275998
iteration 87, loss = 0.001049676677212119
iteration 88, loss = 0.0009255280019715428
iteration 89, loss = 0.0008994686650112271
iteration 90, loss = 0.0013719267444685102
iteration 91, loss = 0.0009672737796790898
iteration 92, loss = 0.0008677043952047825
iteration 93, loss = 0.0009179927292279899
iteration 94, loss = 0.0009977505542337894
iteration 95, loss = 0.0008663035114295781
iteration 96, loss = 0.0008266745717264712
iteration 97, loss = 0.0009144890354946256
iteration 98, loss = 0.0008909718599170446
iteration 99, loss = 0.0008809331920929253
iteration 100, loss = 0.0008908439194783568
iteration 101, loss = 0.0008211066597141325
iteration 102, loss = 0.0010410270188003778
iteration 103, loss = 0.000830821692943573
iteration 104, loss = 0.0008980186539702117
iteration 105, loss = 0.0024500165600329638
iteration 106, loss = 0.0008713360875844955
iteration 107, loss = 0.0009547562221996486
iteration 108, loss = 0.0008862800896167755
iteration 109, loss = 0.0011501783737912774
iteration 110, loss = 0.0010205727303400636
iteration 111, loss = 0.0008596275001764297
iteration 112, loss = 0.0008997457334771752
iteration 113, loss = 0.0009817539248615503
iteration 114, loss = 0.000893435615580529
iteration 115, loss = 0.0009171917336061597
iteration 116, loss = 0.0009105493663810194
iteration 117, loss = 0.000934117822907865
iteration 118, loss = 0.0010056932223960757
iteration 119, loss = 0.0008488729363307357
iteration 120, loss = 0.0014930902980268002
iteration 121, loss = 0.0009687013225629926
iteration 122, loss = 0.0008576434338465333
iteration 123, loss = 0.0008754460723139346
iteration 124, loss = 0.0009061945602297783
iteration 125, loss = 0.0010109285358339548
iteration 126, loss = 0.0008520068950019777
iteration 127, loss = 0.0009163965587504208
iteration 128, loss = 0.001560094067826867
iteration 129, loss = 0.000947460881434381
iteration 130, loss = 0.0007787112263031304
iteration 131, loss = 0.0013660044642165303
iteration 132, loss = 0.0010097003541886806
iteration 133, loss = 0.0008328289841301739
iteration 134, loss = 0.000876025587785989
iteration 135, loss = 0.0008122979779727757
iteration 136, loss = 0.000946834625210613
iteration 137, loss = 0.001017955131828785
iteration 138, loss = 0.0009064070763997734
iteration 139, loss = 0.0008567702607251704
iteration 140, loss = 0.00243354681879282
iteration 141, loss = 0.0009282787214033306
iteration 142, loss = 0.0008521518320776522
iteration 143, loss = 0.0014742029597982764
iteration 144, loss = 0.0009032287634909153
iteration 145, loss = 0.0008315946324728429
iteration 146, loss = 0.0010176358046010137
iteration 147, loss = 0.0024124723859131336
iteration 148, loss = 0.0008797101909294724
iteration 149, loss = 0.0011405539698898792
iteration 150, loss = 0.0008495845831930637
iteration 151, loss = 0.0010497296461835504
iteration 152, loss = 0.0009859900455921888
iteration 153, loss = 0.0010434042196720839
iteration 154, loss = 0.0010227273451164365
iteration 155, loss = 0.0009950845269486308
iteration 156, loss = 0.0008673917036503553
iteration 157, loss = 0.002446010708808899
iteration 158, loss = 0.0008947389433160424
iteration 159, loss = 0.0008467160514555871
iteration 160, loss = 0.0009728150325827301
iteration 161, loss = 0.0013476978056132793
iteration 162, loss = 0.0009069793741218746
iteration 163, loss = 0.0008889429154805839
iteration 164, loss = 0.0010497929761186242
iteration 165, loss = 0.0009826021268963814
iteration 166, loss = 0.0007696313550695777
iteration 167, loss = 0.0009528357186354697
iteration 168, loss = 0.000831070588901639
iteration 169, loss = 0.0009534393320791423
iteration 170, loss = 0.0008425294072367251
iteration 171, loss = 0.002483987482264638
iteration 172, loss = 0.000925273634493351
iteration 173, loss = 0.000914654869120568
iteration 174, loss = 0.0009206957183778286
iteration 175, loss = 0.0012005273019894958
iteration 176, loss = 0.0012795053189620376
iteration 177, loss = 0.0008104996522888541
iteration 178, loss = 0.000773423002101481
iteration 179, loss = 0.0009111784747801721
iteration 180, loss = 0.0010676770471036434
iteration 181, loss = 0.0008284340146929026
iteration 182, loss = 0.0009560048347339034
iteration 183, loss = 0.0009625226375646889
iteration 184, loss = 0.0010619843378663063
iteration 185, loss = 0.0008176183910109103
iteration 186, loss = 0.00081074662739411
iteration 187, loss = 0.0008215847192332149
iteration 188, loss = 0.0008459365926682949
iteration 189, loss = 0.0009965796489268541
iteration 190, loss = 0.0013142500538378954
iteration 191, loss = 0.0009262365056201816
iteration 192, loss = 0.0009257360361516476
iteration 193, loss = 0.0008893912308849394
iteration 194, loss = 0.0008519499679096043
iteration 195, loss = 0.000835332612041384
iteration 196, loss = 0.0008277571178041399
iteration 197, loss = 0.0008522556163370609
iteration 198, loss = 0.0007942256052047014
iteration 199, loss = 0.0008204326150007546
iteration 200, loss = 0.0009311727480962873
iteration 201, loss = 0.0008585831383243203
iteration 202, loss = 0.0008243263000622392
iteration 203, loss = 0.001425560680218041
iteration 204, loss = 0.0010083854431286454
iteration 205, loss = 0.0016247354215011
iteration 206, loss = 0.0007785672205500305
iteration 207, loss = 0.0009605890954844654
iteration 208, loss = 0.0008520121918991208
iteration 209, loss = 0.0011846628040075302
iteration 210, loss = 0.0013529143761843443
iteration 211, loss = 0.0008437148062512279
iteration 212, loss = 0.000863832188770175
iteration 213, loss = 0.001269531319849193
iteration 214, loss = 0.0008607175550423563
iteration 215, loss = 0.0012687173439189792
iteration 216, loss = 0.0008648558286949992
iteration 217, loss = 0.0008323402726091444
iteration 218, loss = 0.0008339068735949695
iteration 219, loss = 0.0025318036787211895
iteration 220, loss = 0.0010192892514169216
iteration 221, loss = 0.0008787740371190012
iteration 222, loss = 0.001111722900532186
iteration 223, loss = 0.0008244253695011139
iteration 224, loss = 0.000949726440012455
iteration 225, loss = 0.0013657527742907405
iteration 226, loss = 0.000834350474178791
iteration 227, loss = 0.002398050157353282
iteration 228, loss = 0.0012236922048032284
iteration 229, loss = 0.0010025715455412865
iteration 230, loss = 0.0009226688998751342
iteration 231, loss = 0.0008994355448521674
iteration 232, loss = 0.0008722228230908513
iteration 233, loss = 0.0009505464695394039
iteration 234, loss = 0.0011613250244408846
iteration 235, loss = 0.0010563286487013102
iteration 236, loss = 0.0008850668091326952
iteration 237, loss = 0.0007909507839940488
iteration 238, loss = 0.0008646153728477657
iteration 239, loss = 0.0008185231708921492
iteration 240, loss = 0.0009621336939744651
iteration 241, loss = 0.001365751726552844
iteration 242, loss = 0.0008428083965554833
iteration 243, loss = 0.0008695563883520663
iteration 244, loss = 0.0009792052442207932
iteration 245, loss = 0.0007455942686647177
iteration 246, loss = 0.0007439189357683063
iteration 247, loss = 0.0009062991593964398
iteration 248, loss = 0.001061560120433569
iteration 249, loss = 0.0008354094461537898
iteration 250, loss = 0.0008624262409284711
iteration 251, loss = 0.0009640341741032898
iteration 252, loss = 0.0009075154666788876
iteration 253, loss = 0.0010195521172136068
iteration 254, loss = 0.0009185788803733885
iteration 255, loss = 0.0008676478755660355
iteration 256, loss = 0.0010510811116546392
iteration 257, loss = 0.0008797596674412489
iteration 258, loss = 0.0009388485923409462
iteration 259, loss = 0.0008272505365312099
iteration 260, loss = 0.0010669783223420382
iteration 261, loss = 0.0008986950851976871
iteration 262, loss = 0.0008310548728331923
iteration 263, loss = 0.0008424351690337062
iteration 264, loss = 0.0008085181470960379
iteration 265, loss = 0.0008332653669640422
iteration 266, loss = 0.001330830971710384
iteration 267, loss = 0.000834717764519155
iteration 268, loss = 0.000981508637778461
iteration 269, loss = 0.0009324534330517054
iteration 270, loss = 0.0008661391912028193
iteration 271, loss = 0.0011586090549826622
iteration 272, loss = 0.0009515504352748394
iteration 273, loss = 0.001170468982309103
iteration 274, loss = 0.0009067660430446267
iteration 275, loss = 0.0008086011512205005
iteration 276, loss = 0.0007436135783791542
iteration 277, loss = 0.0008805884281173348
iteration 278, loss = 0.000849610660225153
iteration 279, loss = 0.0011529303155839443
iteration 280, loss = 0.001446144888177514
iteration 281, loss = 0.0008273479179479182
iteration 282, loss = 0.0011815853649750352
iteration 283, loss = 0.0009661156800575554
iteration 284, loss = 0.0010218529496341944
iteration 285, loss = 0.0008242280455306172
iteration 286, loss = 0.0008950390620157123
iteration 287, loss = 0.0009205177193507552
iteration 288, loss = 0.001139596221037209
iteration 289, loss = 0.002342693042010069
iteration 290, loss = 0.0007912492146715522
iteration 291, loss = 0.001132471952587366
iteration 292, loss = 0.0013394333655014634
iteration 293, loss = 0.0011199457803741097
iteration 294, loss = 0.000913577969186008
iteration 295, loss = 0.0008658605511300266
iteration 296, loss = 0.0009245650144293904
iteration 297, loss = 0.0012400001287460327
iteration 298, loss = 0.0007793070981279016
iteration 299, loss = 0.0007998889195732772
iteration 0, loss = 0.0008285450167022645
iteration 1, loss = 0.0009976516012102365
iteration 2, loss = 0.0009138148743659258
iteration 3, loss = 0.001068459008820355
iteration 4, loss = 0.0008228499791584909
iteration 5, loss = 0.0007595177739858627
iteration 6, loss = 0.0008503944845870137
iteration 7, loss = 0.0010695586679503322
iteration 8, loss = 0.0009002629667520523
iteration 9, loss = 0.0028844585176557302
iteration 10, loss = 0.0007774317055009305
iteration 11, loss = 0.0013414215063676238
iteration 12, loss = 0.0008256057626567781
iteration 13, loss = 0.0008169462671503425
iteration 14, loss = 0.0010817511938512325
iteration 15, loss = 0.0009350497857667506
iteration 16, loss = 0.0008827607962302864
iteration 17, loss = 0.0009639693889766932
iteration 18, loss = 0.0008036862127482891
iteration 19, loss = 0.0008138345438055694
iteration 20, loss = 0.0008839575457386672
iteration 21, loss = 0.0009585536899976432
iteration 22, loss = 0.0008275911095552146
iteration 23, loss = 0.0009465571492910385
iteration 24, loss = 0.0009082816541194916
iteration 25, loss = 0.0009356298251077533
iteration 26, loss = 0.0008991873473860323
iteration 27, loss = 0.0009482809109613299
iteration 28, loss = 0.0008278811583295465
iteration 29, loss = 0.002336689969524741
iteration 30, loss = 0.0008500313851982355
iteration 31, loss = 0.0009203123627230525
iteration 32, loss = 0.002863870933651924
iteration 33, loss = 0.0008515611407347023
iteration 34, loss = 0.0008774721063673496
iteration 35, loss = 0.0010889599798247218
iteration 36, loss = 0.0013254373334348202
iteration 37, loss = 0.0008780926582403481
iteration 38, loss = 0.00084422726649791
iteration 39, loss = 0.00128517288248986
iteration 40, loss = 0.0007991580641828477
iteration 41, loss = 0.0008330404525622725
iteration 42, loss = 0.0008123335428535938
iteration 43, loss = 0.0008532466017641127
iteration 44, loss = 0.0008614147081971169
iteration 45, loss = 0.0009851037757471204
iteration 46, loss = 0.0007891128771007061
iteration 47, loss = 0.0023082727566361427
iteration 48, loss = 0.0008885326678864658
iteration 49, loss = 0.0008543512085452676
iteration 50, loss = 0.0011466109426692128
iteration 51, loss = 0.0008735285955481231
iteration 52, loss = 0.0009276354103349149
iteration 53, loss = 0.0011487506562843919
iteration 54, loss = 0.0008188669453375041
iteration 55, loss = 0.0012488648062571883
iteration 56, loss = 0.0008631350356154144
iteration 57, loss = 0.0009773519122973084
iteration 58, loss = 0.002743963385000825
iteration 59, loss = 0.0008607911295257509
iteration 60, loss = 0.0008147801854647696
iteration 61, loss = 0.0008992411312647164
iteration 62, loss = 0.0011726428056135774
iteration 63, loss = 0.0007775137783028185
iteration 64, loss = 0.0008757786708883941
iteration 65, loss = 0.0009262728272005916
iteration 66, loss = 0.0009263786487281322
iteration 67, loss = 0.002461763331666589
iteration 68, loss = 0.0008168700733222067
iteration 69, loss = 0.001032124855555594
iteration 70, loss = 0.0009110469836741686
iteration 71, loss = 0.0008518545655533671
iteration 72, loss = 0.0009672772139310837
iteration 73, loss = 0.0009342236444354057
iteration 74, loss = 0.0008308594115078449
iteration 75, loss = 0.0011612321250140667
iteration 76, loss = 0.0007837690063752234
iteration 77, loss = 0.0008065156871452928
iteration 78, loss = 0.0008234292035922408
iteration 79, loss = 0.000917843310162425
iteration 80, loss = 0.0008281900663860142
iteration 81, loss = 0.0009100472670979798
iteration 82, loss = 0.0008622008608654141
iteration 83, loss = 0.0008599687134847045
iteration 84, loss = 0.0008376551559194922
iteration 85, loss = 0.0008404612890444696
iteration 86, loss = 0.0009011237416416407
iteration 87, loss = 0.0010348246432840824
iteration 88, loss = 0.0009499576408416033
iteration 89, loss = 0.0007774720434099436
iteration 90, loss = 0.0008411332964897156
iteration 91, loss = 0.000894349068403244
iteration 92, loss = 0.00123225140850991
iteration 93, loss = 0.000808484386652708
iteration 94, loss = 0.0008459381642751396
iteration 95, loss = 0.001113462494686246
iteration 96, loss = 0.0007838935707695782
iteration 97, loss = 0.0007994170300662518
iteration 98, loss = 0.0008183778845705092
iteration 99, loss = 0.0008320539491251111
iteration 100, loss = 0.0013386121718212962
iteration 101, loss = 0.0007960655493661761
iteration 102, loss = 0.0023269650992006063
iteration 103, loss = 0.0008629020303487778
iteration 104, loss = 0.0008788781124167144
iteration 105, loss = 0.0009189240518026054
iteration 106, loss = 0.0009234474855475128
iteration 107, loss = 0.0008999563287943602
iteration 108, loss = 0.0007742075249552727
iteration 109, loss = 0.0008693941053934395
iteration 110, loss = 0.0012788387248292565
iteration 111, loss = 0.0007978994399309158
iteration 112, loss = 0.0013120704097673297
iteration 113, loss = 0.0008277404122054577
iteration 114, loss = 0.001034684362821281
iteration 115, loss = 0.0009599063196219504
iteration 116, loss = 0.0007864353246986866
iteration 117, loss = 0.0007395308930426836
iteration 118, loss = 0.0024062904994934797
iteration 119, loss = 0.001487312139943242
iteration 120, loss = 0.0012512693647295237
iteration 121, loss = 0.0008089402108453214
iteration 122, loss = 0.002410587854683399
iteration 123, loss = 0.001630657003261149
iteration 124, loss = 0.000834912178106606
iteration 125, loss = 0.0008516176603734493
iteration 126, loss = 0.001318594440817833
iteration 127, loss = 0.0009250616421923041
iteration 128, loss = 0.0008697925950400531
iteration 129, loss = 0.0009292507893405855
iteration 130, loss = 0.0007912650471553206
iteration 131, loss = 0.000808608892839402
iteration 132, loss = 0.0008226815261878073
iteration 133, loss = 0.0008423108956776559
iteration 134, loss = 0.0008638985455036163
iteration 135, loss = 0.0008749788976274431
iteration 136, loss = 0.0008429587469436228
iteration 137, loss = 0.0010988626163452864
iteration 138, loss = 0.000908798654563725
iteration 139, loss = 0.0008722672937437892
iteration 140, loss = 0.002328392816707492
iteration 141, loss = 0.0007650094339624047
iteration 142, loss = 0.0009545274078845978
iteration 143, loss = 0.0009307737345807254
iteration 144, loss = 0.0008752241265028715
iteration 145, loss = 0.0011046570725739002
iteration 146, loss = 0.0007625241996720433
iteration 147, loss = 0.0008790278807282448
iteration 148, loss = 0.0007687390316277742
iteration 149, loss = 0.001077396678738296
iteration 150, loss = 0.0009031910449266434
iteration 151, loss = 0.0008723771898075938
iteration 152, loss = 0.000828330812510103
iteration 153, loss = 0.0009501563035883009
iteration 154, loss = 0.0007557056378573179
iteration 155, loss = 0.0008003426482900977
iteration 156, loss = 0.0010215322254225612
iteration 157, loss = 0.000828105432447046
iteration 158, loss = 0.0008973386720754206
iteration 159, loss = 0.0008781832875683904
iteration 160, loss = 0.000804889015853405
iteration 161, loss = 0.0009692094754427671
iteration 162, loss = 0.0008405343978665769
iteration 163, loss = 0.0012767855077981949
iteration 164, loss = 0.0009109746897593141
iteration 165, loss = 0.0008976378012448549
iteration 166, loss = 0.0009288917644880712
iteration 167, loss = 0.0007260446436703205
iteration 168, loss = 0.0010021901689469814
iteration 169, loss = 0.0009250902221538126
iteration 170, loss = 0.000759715330787003
iteration 171, loss = 0.001225880580022931
iteration 172, loss = 0.000847715709824115
iteration 173, loss = 0.0007976542692631483
iteration 174, loss = 0.0008344209054484963
iteration 175, loss = 0.0009409671765752137
iteration 176, loss = 0.0010345220798626542
iteration 177, loss = 0.0007905653910711408
iteration 178, loss = 0.0009139521280303597
iteration 179, loss = 0.0008267828961834311
iteration 180, loss = 0.000994377420283854
iteration 181, loss = 0.003930169157683849
iteration 182, loss = 0.001347661716863513
iteration 183, loss = 0.0008873050683178008
iteration 184, loss = 0.0008212408283725381
iteration 185, loss = 0.0008422100218012929
iteration 186, loss = 0.000782053277362138
iteration 187, loss = 0.0008308160468004644
iteration 188, loss = 0.0011576891411095858
iteration 189, loss = 0.000862007902469486
iteration 190, loss = 0.0009129673126153648
iteration 191, loss = 0.0008849067962728441
iteration 192, loss = 0.0013701653806492686
iteration 193, loss = 0.0008598944987170398
iteration 194, loss = 0.0009955823188647628
iteration 195, loss = 0.0009576171287335455
iteration 196, loss = 0.0009403199073858559
iteration 197, loss = 0.0008869901648722589
iteration 198, loss = 0.0008748802356421947
iteration 199, loss = 0.0008352479781024158
iteration 200, loss = 0.0009333114721812308
iteration 201, loss = 0.0008213205146603286
iteration 202, loss = 0.0008945123408921063
iteration 203, loss = 0.0007325697224587202
iteration 204, loss = 0.0009103165939450264
iteration 205, loss = 0.0008316500461660326
iteration 206, loss = 0.0008835180778987706
iteration 207, loss = 0.0008757521281950176
iteration 208, loss = 0.0013071533758193254
iteration 209, loss = 0.0008756487513892353
iteration 210, loss = 0.0009007120970636606
iteration 211, loss = 0.0007882441277615726
iteration 212, loss = 0.000755037646740675
iteration 213, loss = 0.0008785359095782042
iteration 214, loss = 0.0009886923944577575
iteration 215, loss = 0.0007945375982671976
iteration 216, loss = 0.000855962629429996
iteration 217, loss = 0.0008110771304927766
iteration 218, loss = 0.0008210358209908009
iteration 219, loss = 0.0007943465607240796
iteration 220, loss = 0.00115721276961267
iteration 221, loss = 0.0007827190565876663
iteration 222, loss = 0.0007634710636921227
iteration 223, loss = 0.0008155623218044639
iteration 224, loss = 0.000850639829877764
iteration 225, loss = 0.0013225814327597618
iteration 226, loss = 0.0008446731371805072
iteration 227, loss = 0.001174591830931604
iteration 228, loss = 0.000758611538913101
iteration 229, loss = 0.0007569066947326064
iteration 230, loss = 0.0008161315927281976
iteration 231, loss = 0.0008410511654801667
iteration 232, loss = 0.0007640353287570179
iteration 233, loss = 0.0008390629081986845
iteration 234, loss = 0.0007735416293144226
iteration 235, loss = 0.001287694787606597
iteration 236, loss = 0.0008669024100527167
iteration 237, loss = 0.0008117512334138155
iteration 238, loss = 0.002313523320481181
iteration 239, loss = 0.0015725441044196486
iteration 240, loss = 0.0009257501224055886
iteration 241, loss = 0.000898056838195771
iteration 242, loss = 0.0008433717885054648
iteration 243, loss = 0.0014431284507736564
iteration 244, loss = 0.0008038197993300855
iteration 245, loss = 0.000857554841786623
iteration 246, loss = 0.0009953237604349852
iteration 247, loss = 0.0007956794579513371
iteration 248, loss = 0.0007880114135332406
iteration 249, loss = 0.0007842712802812457
iteration 250, loss = 0.0008381158113479614
iteration 251, loss = 0.001077443128451705
iteration 252, loss = 0.000892973446752876
iteration 253, loss = 0.0008602339657954872
iteration 254, loss = 0.0008201486780308187
iteration 255, loss = 0.0013027701061218977
iteration 256, loss = 0.0008531641215085983
iteration 257, loss = 0.000959223136305809
iteration 258, loss = 0.0008130719652399421
iteration 259, loss = 0.000843769230414182
iteration 260, loss = 0.0009419171838089824
iteration 261, loss = 0.0013485741801559925
iteration 262, loss = 0.000763698248192668
iteration 263, loss = 0.0009170136763714254
iteration 264, loss = 0.0013045368250459433
iteration 265, loss = 0.0008123842417262495
iteration 266, loss = 0.0011459803208708763
iteration 267, loss = 0.0009633420268073678
iteration 268, loss = 0.0009236883488483727
iteration 269, loss = 0.0008692429983057082
iteration 270, loss = 0.0007389856036752462
iteration 271, loss = 0.000869765761308372
iteration 272, loss = 0.000757601170334965
iteration 273, loss = 0.000976351264398545
iteration 274, loss = 0.000901174615137279
iteration 275, loss = 0.0008199636940844357
iteration 276, loss = 0.0009593616705387831
iteration 277, loss = 0.0009816179517656565
iteration 278, loss = 0.0009188138647004962
iteration 279, loss = 0.0010381805477663875
iteration 280, loss = 0.0008440390229225159
iteration 281, loss = 0.0008276647422462702
iteration 282, loss = 0.0008107977919280529
iteration 283, loss = 0.000845855160150677
iteration 284, loss = 0.0008107512257993221
iteration 285, loss = 0.0008293105056509376
iteration 286, loss = 0.0007317675626836717
iteration 287, loss = 0.0008016870706342161
iteration 288, loss = 0.0008083322900347412
iteration 289, loss = 0.0007987612625584006
iteration 290, loss = 0.0013306085020303726
iteration 291, loss = 0.00087630411144346
iteration 292, loss = 0.002353750402107835
iteration 293, loss = 0.0009841001592576504
iteration 294, loss = 0.0009121940238401294
iteration 295, loss = 0.0008713869610801339
iteration 296, loss = 0.0008888337761163712
iteration 297, loss = 0.0008095853263512254
iteration 298, loss = 0.002281223190948367
iteration 299, loss = 0.000832000863738358
iteration 0, loss = 0.0007988103898242116
iteration 1, loss = 0.0007916353642940521
iteration 2, loss = 0.000881511252373457
iteration 3, loss = 0.0008906010771170259
iteration 4, loss = 0.000796591630205512
iteration 5, loss = 0.0008477306691929698
iteration 6, loss = 0.0008398654172196984
iteration 7, loss = 0.001085013383999467
iteration 8, loss = 0.0009921196615323424
iteration 9, loss = 0.001293455483391881
iteration 10, loss = 0.0009547991794534028
iteration 11, loss = 0.001075453357771039
iteration 12, loss = 0.0008545618038624525
iteration 13, loss = 0.0008428663713857532
iteration 14, loss = 0.0008501937263645232
iteration 15, loss = 0.0008901674882508814
iteration 16, loss = 0.0012420957209542394
iteration 17, loss = 0.0007667586323805153
iteration 18, loss = 0.0007706749020144343
iteration 19, loss = 0.0007554097101092339
iteration 20, loss = 0.0007183425477705896
iteration 21, loss = 0.000727470382116735
iteration 22, loss = 0.0009873845847323537
iteration 23, loss = 0.000923472922295332
iteration 24, loss = 0.0007917559705674648
iteration 25, loss = 0.000915129785425961
iteration 26, loss = 0.0010096444748342037
iteration 27, loss = 0.0007442532805725932
iteration 28, loss = 0.0012209007982164621
iteration 29, loss = 0.0007693199440836906
iteration 30, loss = 0.0008209905354306102
iteration 31, loss = 0.0008421353413723409
iteration 32, loss = 0.0008188388892449439
iteration 33, loss = 0.0008489252650178969
iteration 34, loss = 0.0008250590180978179
iteration 35, loss = 0.0007356767309829593
iteration 36, loss = 0.0007773619727231562
iteration 37, loss = 0.0007158081280067563
iteration 38, loss = 0.0008204336045309901
iteration 39, loss = 0.0007502819644287229
iteration 40, loss = 0.0013647436862811446
iteration 41, loss = 0.0008158537675626576
iteration 42, loss = 0.0007877940079197288
iteration 43, loss = 0.0011961715063080192
iteration 44, loss = 0.0008603655151091516
iteration 45, loss = 0.0023522754199802876
iteration 46, loss = 0.0008826685370877385
iteration 47, loss = 0.0007709883502684534
iteration 48, loss = 0.0012823052238672972
iteration 49, loss = 0.0008227319922298193
iteration 50, loss = 0.0010450229747220874
iteration 51, loss = 0.0007758441497571766
iteration 52, loss = 0.0008827659767121077
iteration 53, loss = 0.0007506477413699031
iteration 54, loss = 0.00082092807861045
iteration 55, loss = 0.0008265975047834218
iteration 56, loss = 0.00123844132758677
iteration 57, loss = 0.0008467491716146469
iteration 58, loss = 0.0008206006605178118
iteration 59, loss = 0.0008513458305969834
iteration 60, loss = 0.0007978499052114785
iteration 61, loss = 0.0008180337026715279
iteration 62, loss = 0.0008153454982675612
iteration 63, loss = 0.0007762409513816237
iteration 64, loss = 0.0008059308747760952
iteration 65, loss = 0.0008606018382124603
iteration 66, loss = 0.0007767202332615852
iteration 67, loss = 0.0007701124995946884
iteration 68, loss = 0.0012852346990257502
iteration 69, loss = 0.0009149465477094054
iteration 70, loss = 0.0007512407610192895
iteration 71, loss = 0.0008286387310363352
iteration 72, loss = 0.0011702926130965352
iteration 73, loss = 0.0008204661426134408
iteration 74, loss = 0.0007995173800736666
iteration 75, loss = 0.0023557045497000217
iteration 76, loss = 0.0009306668071076274
iteration 77, loss = 0.0008623217581771314
iteration 78, loss = 0.0008709483663551509
iteration 79, loss = 0.0009012041846290231
iteration 80, loss = 0.0007855655858293176
iteration 81, loss = 0.0009142024791799486
iteration 82, loss = 0.0009786683367565274
iteration 83, loss = 0.0008156741387210786
iteration 84, loss = 0.0008687977679073811
iteration 85, loss = 0.0012517436407506466
iteration 86, loss = 0.000786091317422688
iteration 87, loss = 0.000778681191150099
iteration 88, loss = 0.0007630839827470481
iteration 89, loss = 0.0009858066914603114
iteration 90, loss = 0.0009190605487674475
iteration 91, loss = 0.0007207267917692661
iteration 92, loss = 0.0007971110171638429
iteration 93, loss = 0.0008353950106538832
iteration 94, loss = 0.0009771316545084119
iteration 95, loss = 0.0009187158429995179
iteration 96, loss = 0.0007991473539732397
iteration 97, loss = 0.0010687847388908267
iteration 98, loss = 0.0008206956554204226
iteration 99, loss = 0.0008109919726848602
iteration 100, loss = 0.0007812249241396785
iteration 101, loss = 0.000826595991384238
iteration 102, loss = 0.0008192081586457789
iteration 103, loss = 0.0012110270326957107
iteration 104, loss = 0.0008750408887863159
iteration 105, loss = 0.0012818960240110755
iteration 106, loss = 0.0007809004164300859
iteration 107, loss = 0.0008448397857137024
iteration 108, loss = 0.000802731781732291
iteration 109, loss = 0.00069295713910833
iteration 110, loss = 0.0013196126092225313
iteration 111, loss = 0.0007425591466017067
iteration 112, loss = 0.0006770336185581982
iteration 113, loss = 0.0007449154509231448
iteration 114, loss = 0.0007893862784840167
iteration 115, loss = 0.0008704150677658617
iteration 116, loss = 0.0008292217389680445
iteration 117, loss = 0.0008500713156536222
iteration 118, loss = 0.0007272748043760657
iteration 119, loss = 0.0007694469531998038
iteration 120, loss = 0.0008771223947405815
iteration 121, loss = 0.0011452423641458154
iteration 122, loss = 0.0009940649615600705
iteration 123, loss = 0.001257319119758904
iteration 124, loss = 0.00090516556520015
iteration 125, loss = 0.0008290983969345689
iteration 126, loss = 0.000768614758271724
iteration 127, loss = 0.0008467286825180054
iteration 128, loss = 0.0012422807049006224
iteration 129, loss = 0.0007995442720130086
iteration 130, loss = 0.0007523733656853437
iteration 131, loss = 0.0008981165010482073
iteration 132, loss = 0.0008564273593947291
iteration 133, loss = 0.0007653321954421699
iteration 134, loss = 0.0008258853922598064
iteration 135, loss = 0.0009107404621317983
iteration 136, loss = 0.0012477999553084373
iteration 137, loss = 0.0008236847934313118
iteration 138, loss = 0.0022489314433187246
iteration 139, loss = 0.0008685867069289088
iteration 140, loss = 0.0008825164404697716
iteration 141, loss = 0.0023262507747858763
iteration 142, loss = 0.0013162614777684212
iteration 143, loss = 0.0007829588139429688
iteration 144, loss = 0.0022330288775265217
iteration 145, loss = 0.0011280508479103446
iteration 146, loss = 0.0007827361114323139
iteration 147, loss = 0.0007724843453615904
iteration 148, loss = 0.002261984394863248
iteration 149, loss = 0.0008794390014372766
iteration 150, loss = 0.0008507595048286021
iteration 151, loss = 0.0008693193667568266
iteration 152, loss = 0.0011727490928024054
iteration 153, loss = 0.0007487728144042194
iteration 154, loss = 0.0008685014909133315
iteration 155, loss = 0.0008394143078476191
iteration 156, loss = 0.000891378556843847
iteration 157, loss = 0.0008018843946047127
iteration 158, loss = 0.0012672784505411983
iteration 159, loss = 0.0006918081780895591
iteration 160, loss = 0.0007987719727680087
iteration 161, loss = 0.0008255225257016718
iteration 162, loss = 0.0007896367460489273
iteration 163, loss = 0.0008086730958893895
iteration 164, loss = 0.0007553939940407872
iteration 165, loss = 0.0008090332266874611
iteration 166, loss = 0.0011533390497788787
iteration 167, loss = 0.0008683700580149889
iteration 168, loss = 0.0007629985921084881
iteration 169, loss = 0.0008461435791105032
iteration 170, loss = 0.0008129333727993071
iteration 171, loss = 0.0008369483985006809
iteration 172, loss = 0.001014949637465179
iteration 173, loss = 0.0008242410258390009
iteration 174, loss = 0.0011283459607511759
iteration 175, loss = 0.0007425908697769046
iteration 176, loss = 0.0008457507938146591
iteration 177, loss = 0.0008514007204212248
iteration 178, loss = 0.0008460765238851309
iteration 179, loss = 0.0011393956374377012
iteration 180, loss = 0.000815028790384531
iteration 181, loss = 0.0009292751201428473
iteration 182, loss = 0.0007314713438972831
iteration 183, loss = 0.0007636543014086783
iteration 184, loss = 0.0008595004328526556
iteration 185, loss = 0.0013398068258538842
iteration 186, loss = 0.0015871303621679544
iteration 187, loss = 0.0006963586783967912
iteration 188, loss = 0.0011651987442746758
iteration 189, loss = 0.002196881454437971
iteration 190, loss = 0.0007274553063325584
iteration 191, loss = 0.0008719385368749499
iteration 192, loss = 0.0008702506311237812
iteration 193, loss = 0.0007443797658197582
iteration 194, loss = 0.0008654742268845439
iteration 195, loss = 0.0008090843330137432
iteration 196, loss = 0.0007413893472403288
iteration 197, loss = 0.0006894557736814022
iteration 198, loss = 0.0016011870466172695
iteration 199, loss = 0.0008321483619511127
iteration 200, loss = 0.0009284912957809865
iteration 201, loss = 0.0008677405421622097
iteration 202, loss = 0.0008491702028550208
iteration 203, loss = 0.000882959458976984
iteration 204, loss = 0.000752152525819838
iteration 205, loss = 0.0007387343794107437
iteration 206, loss = 0.0008553010993637145
iteration 207, loss = 0.0007549012661911547
iteration 208, loss = 0.0008146757609210908
iteration 209, loss = 0.0008228635997511446
iteration 210, loss = 0.0008613849058747292
iteration 211, loss = 0.0007649725303053856
iteration 212, loss = 0.0007026614039205015
iteration 213, loss = 0.0007756459526717663
iteration 214, loss = 0.0008052779012359679
iteration 215, loss = 0.0007416472653858364
iteration 216, loss = 0.0008676350116729736
iteration 217, loss = 0.0007154506165534258
iteration 218, loss = 0.001044697593897581
iteration 219, loss = 0.0008426396525464952
iteration 220, loss = 0.002181941643357277
iteration 221, loss = 0.0009195843595080078
iteration 222, loss = 0.0007056237664073706
iteration 223, loss = 0.0007246605819091201
iteration 224, loss = 0.0010173919145017862
iteration 225, loss = 0.0007907062536105514
iteration 226, loss = 0.0009511929820291698
iteration 227, loss = 0.0008569096680730581
iteration 228, loss = 0.0007336610578931868
iteration 229, loss = 0.0013059451011940837
iteration 230, loss = 0.0009664205717854202
iteration 231, loss = 0.0008664646884426475
iteration 232, loss = 0.0008761967183090746
iteration 233, loss = 0.000722717319149524
iteration 234, loss = 0.0007743280148133636
iteration 235, loss = 0.0008531044004485011
iteration 236, loss = 0.002380983904004097
iteration 237, loss = 0.0008306601666845381
iteration 238, loss = 0.0009569313260726631
iteration 239, loss = 0.0008753826259635389
iteration 240, loss = 0.000772227009292692
iteration 241, loss = 0.00218399241566658
iteration 242, loss = 0.0008274617721326649
iteration 243, loss = 0.0008200249285437167
iteration 244, loss = 0.0009097644360736012
iteration 245, loss = 0.0010058593470603228
iteration 246, loss = 0.0022472916170954704
iteration 247, loss = 0.0021795460488647223
iteration 248, loss = 0.0012640791246667504
iteration 249, loss = 0.0010372904362156987
iteration 250, loss = 0.0008738681790418923
iteration 251, loss = 0.0007972307503223419
iteration 252, loss = 0.002186216413974762
iteration 253, loss = 0.0010317424312233925
iteration 254, loss = 0.0008485288126394153
iteration 255, loss = 0.0011925023281946778
iteration 256, loss = 0.0007225808803923428
iteration 257, loss = 0.0022838390432298183
iteration 258, loss = 0.0008661955362185836
iteration 259, loss = 0.0007496637990698218
iteration 260, loss = 0.0007546565029770136
iteration 261, loss = 0.0008607894415035844
iteration 262, loss = 0.0008484250865876675
iteration 263, loss = 0.0011939086252823472
iteration 264, loss = 0.0008299135370180011
iteration 265, loss = 0.0008149457862600684
iteration 266, loss = 0.0008959792321547866
iteration 267, loss = 0.0008296096930280328
iteration 268, loss = 0.0008659594459459186
iteration 269, loss = 0.0008547980687581003
iteration 270, loss = 0.000836589781101793
iteration 271, loss = 0.002731331856921315
iteration 272, loss = 0.0011611715890467167
iteration 273, loss = 0.0009504131157882512
iteration 274, loss = 0.0009074310655705631
iteration 275, loss = 0.000867491471581161
iteration 276, loss = 0.000864655536133796
iteration 277, loss = 0.000764884171076119
iteration 278, loss = 0.0007759756990708411
iteration 279, loss = 0.000818373286165297
iteration 280, loss = 0.0007273692754097283
iteration 281, loss = 0.0008575750980526209
iteration 282, loss = 0.0009571847622282803
iteration 283, loss = 0.0009023156017065048
iteration 284, loss = 0.0009941707830876112
iteration 285, loss = 0.0007245952729135752
iteration 286, loss = 0.0008036573999561369
iteration 287, loss = 0.0007140631205402315
iteration 288, loss = 0.0007521442021243274
iteration 289, loss = 0.0007441745256073773
iteration 290, loss = 0.000850408454425633
iteration 291, loss = 0.0009419035050086677
iteration 292, loss = 0.0008118766127154231
iteration 293, loss = 0.0011806588154286146
iteration 294, loss = 0.0008394060423597693
iteration 295, loss = 0.0009171577403321862
iteration 296, loss = 0.0007253881776705384
iteration 297, loss = 0.0007897296454757452
iteration 298, loss = 0.0007992738392204046
iteration 299, loss = 0.0007988217403180897
iteration 0, loss = 0.0008087638998404145
iteration 1, loss = 0.0009511332027614117
iteration 2, loss = 0.002295116661116481
iteration 3, loss = 0.0010111381998285651
iteration 4, loss = 0.0007891096756793559
iteration 5, loss = 0.0009005594765767455
iteration 6, loss = 0.001230207970365882
iteration 7, loss = 0.0008747680694796145
iteration 8, loss = 0.0007887667743489146
iteration 9, loss = 0.0008603588794358075
iteration 10, loss = 0.001081634545698762
iteration 11, loss = 0.0007694954401813447
iteration 12, loss = 0.0008275848813354969
iteration 13, loss = 0.000979444943368435
iteration 14, loss = 0.0013156433124095201
iteration 15, loss = 0.0007822664920240641
iteration 16, loss = 0.0008465473656542599
iteration 17, loss = 0.0007585182320326567
iteration 18, loss = 0.0007323893369175494
iteration 19, loss = 0.0007567658321931958
iteration 20, loss = 0.0012368354946374893
iteration 21, loss = 0.0008059369283728302
iteration 22, loss = 0.0007588381413370371
iteration 23, loss = 0.00072351103881374
iteration 24, loss = 0.0007852384587749839
iteration 25, loss = 0.0007839423487894237
iteration 26, loss = 0.000819591514300555
iteration 27, loss = 0.0008829780272208154
iteration 28, loss = 0.0009653429733589292
iteration 29, loss = 0.0007384829223155975
iteration 30, loss = 0.0007060953648760915
iteration 31, loss = 0.0008487958693876863
iteration 32, loss = 0.0008830100414343178
iteration 33, loss = 0.0021175851579755545
iteration 34, loss = 0.0007672416977584362
iteration 35, loss = 0.0008733834256418049
iteration 36, loss = 0.0008249643724411726
iteration 37, loss = 0.0007531953742727637
iteration 38, loss = 0.000963102444075048
iteration 39, loss = 0.000753464933950454
iteration 40, loss = 0.0007819417514838278
iteration 41, loss = 0.002253967337310314
iteration 42, loss = 0.000805371324531734
iteration 43, loss = 0.0008284245268441737
iteration 44, loss = 0.0008034218335524201
iteration 45, loss = 0.0007424791692756116
iteration 46, loss = 0.0007375465356744826
iteration 47, loss = 0.0008174897520802915
iteration 48, loss = 0.0008146740728989244
iteration 49, loss = 0.0007533396128565073
iteration 50, loss = 0.0007855447474867105
iteration 51, loss = 0.0008032802725210786
iteration 52, loss = 0.000717838411219418
iteration 53, loss = 0.0008432110771536827
iteration 54, loss = 0.0007321536540985107
iteration 55, loss = 0.0008014861959964037
iteration 56, loss = 0.0013381874887272716
iteration 57, loss = 0.0007867770618759096
iteration 58, loss = 0.0007840000907890499
iteration 59, loss = 0.0007509395945817232
iteration 60, loss = 0.0009175707818940282
iteration 61, loss = 0.000924434803891927
iteration 62, loss = 0.0008591649821028113
iteration 63, loss = 0.0007172152982093394
iteration 64, loss = 0.0007631882908754051
iteration 65, loss = 0.0007699233246967196
iteration 66, loss = 0.0007814263226464391
iteration 67, loss = 0.0008176630944944918
iteration 68, loss = 0.0008406866691075265
iteration 69, loss = 0.0010878467001020908
iteration 70, loss = 0.0007536247721873224
iteration 71, loss = 0.0008070567855611444
iteration 72, loss = 0.0009198114275932312
iteration 73, loss = 0.0007105565746314824
iteration 74, loss = 0.0012881821021437645
iteration 75, loss = 0.0007341859163716435
iteration 76, loss = 0.0007822296465747058
iteration 77, loss = 0.0007369732484221458
iteration 78, loss = 0.002446593251079321
iteration 79, loss = 0.0008630095981061459
iteration 80, loss = 0.0007179456879384816
iteration 81, loss = 0.0008243268239311874
iteration 82, loss = 0.0021394407376646996
iteration 83, loss = 0.000901023973710835
iteration 84, loss = 0.0010220479452982545
iteration 85, loss = 0.0007298879791051149
iteration 86, loss = 0.0008845184929668903
iteration 87, loss = 0.0008382220403291285
iteration 88, loss = 0.0008139247074723244
iteration 89, loss = 0.0007559892255812883
iteration 90, loss = 0.0008431160822510719
iteration 91, loss = 0.000754466513171792
iteration 92, loss = 0.0012337052030488849
iteration 93, loss = 0.000912914692889899
iteration 94, loss = 0.001107184449210763
iteration 95, loss = 0.0007933620363473892
iteration 96, loss = 0.00102804705966264
iteration 97, loss = 0.0007442961214110255
iteration 98, loss = 0.0021631340496242046
iteration 99, loss = 0.0009339896496385336
iteration 100, loss = 0.0007756499690003693
iteration 101, loss = 0.0008431312744505703
iteration 102, loss = 0.0007505060639232397
iteration 103, loss = 0.0008076027152128518
iteration 104, loss = 0.0008855002233758569
iteration 105, loss = 0.0007182928966358304
iteration 106, loss = 0.0008169569773599505
iteration 107, loss = 0.0007532686577178538
iteration 108, loss = 0.0008280116599053144
iteration 109, loss = 0.000687604711856693
iteration 110, loss = 0.0008168542990460992
iteration 111, loss = 0.0007598611409775913
iteration 112, loss = 0.0008805397083051503
iteration 113, loss = 0.0007473393343389034
iteration 114, loss = 0.0008103191503323615
iteration 115, loss = 0.0006854041130281985
iteration 116, loss = 0.0008638030267320573
iteration 117, loss = 0.0007823840714991093
iteration 118, loss = 0.0013021872146055102
iteration 119, loss = 0.0011299514444544911
iteration 120, loss = 0.0011905424762517214
iteration 121, loss = 0.0007540347287431359
iteration 122, loss = 0.0007012401474639773
iteration 123, loss = 0.0008879174711182714
iteration 124, loss = 0.0012538983719423413
iteration 125, loss = 0.0021289645228534937
iteration 126, loss = 0.0007194581557996571
iteration 127, loss = 0.0008672458352521062
iteration 128, loss = 0.0008723060600459576
iteration 129, loss = 0.0007164723356254399
iteration 130, loss = 0.0008642588509246707
iteration 131, loss = 0.0008274293504655361
iteration 132, loss = 0.0008217925787903368
iteration 133, loss = 0.0008616286795586348
iteration 134, loss = 0.000778365705627948
iteration 135, loss = 0.0007752679521217942
iteration 136, loss = 0.0022170383017510176
iteration 137, loss = 0.0011800978099927306
iteration 138, loss = 0.0021879998967051506
iteration 139, loss = 0.0008165339240804315
iteration 140, loss = 0.0009008962078951299
iteration 141, loss = 0.0007455175509676337
iteration 142, loss = 0.001040917937643826
iteration 143, loss = 0.0007652246858924627
iteration 144, loss = 0.0007374145206995308
iteration 145, loss = 0.0006984739447943866
iteration 146, loss = 0.0007323664613068104
iteration 147, loss = 0.0012039430439472198
iteration 148, loss = 0.0008080764673650265
iteration 149, loss = 0.0007986569544300437
iteration 150, loss = 0.0008548223413527012
iteration 151, loss = 0.0007796702557243407
iteration 152, loss = 0.0007677909452468157
iteration 153, loss = 0.0008219936862587929
iteration 154, loss = 0.0007124706171452999
iteration 155, loss = 0.0007796059944666922
iteration 156, loss = 0.0008881284738890827
iteration 157, loss = 0.0007476107566617429
iteration 158, loss = 0.0008585998439230025
iteration 159, loss = 0.0007946501136757433
iteration 160, loss = 0.0008504483848810196
iteration 161, loss = 0.001180727849714458
iteration 162, loss = 0.0006979898898862302
iteration 163, loss = 0.0013029141118749976
iteration 164, loss = 0.0007185013382695615
iteration 165, loss = 0.0007454166188836098
iteration 166, loss = 0.0008285410003736615
iteration 167, loss = 0.000925363739952445
iteration 168, loss = 0.000810400175396353
iteration 169, loss = 0.0008599210996180773
iteration 170, loss = 0.0010566117707639933
iteration 171, loss = 0.0008625854970887303
iteration 172, loss = 0.0007609055610373616
iteration 173, loss = 0.0007392102852463722
iteration 174, loss = 0.0007637076778337359
iteration 175, loss = 0.0008261450566351414
iteration 176, loss = 0.0007036976749077439
iteration 177, loss = 0.0006925813504494727
iteration 178, loss = 0.0007696877000853419
iteration 179, loss = 0.000803495233412832
iteration 180, loss = 0.0007774119731038809
iteration 181, loss = 0.0007286554318852723
iteration 182, loss = 0.0007287061307579279
iteration 183, loss = 0.000782070099376142
iteration 184, loss = 0.000917620025575161
iteration 185, loss = 0.0007228285539895296
iteration 186, loss = 0.0007461147615686059
iteration 187, loss = 0.0007543614483438432
iteration 188, loss = 0.0008917521918192506
iteration 189, loss = 0.0012244030367583036
iteration 190, loss = 0.0007269592606462538
iteration 191, loss = 0.0008357450715266168
iteration 192, loss = 0.0008482916164211929
iteration 193, loss = 0.0007768284413032234
iteration 194, loss = 0.000723593751899898
iteration 195, loss = 0.0007274934323504567
iteration 196, loss = 0.0012177727185189724
iteration 197, loss = 0.0007174073252826929
iteration 198, loss = 0.0007482303190045059
iteration 199, loss = 0.0007048374973237514
iteration 200, loss = 0.0007398831658065319
iteration 201, loss = 0.0007836545701138675
iteration 202, loss = 0.0007721971487626433
iteration 203, loss = 0.0010259030386805534
iteration 204, loss = 0.000758940470404923
iteration 205, loss = 0.0007292386726476252
iteration 206, loss = 0.0007852117996662855
iteration 207, loss = 0.0007283107261173427
iteration 208, loss = 0.0021198003087192774
iteration 209, loss = 0.0007119554793462157
iteration 210, loss = 0.0008086045272648335
iteration 211, loss = 0.0007092235027812421
iteration 212, loss = 0.0008954445365816355
iteration 213, loss = 0.0007741840672679245
iteration 214, loss = 0.0007516845944337547
iteration 215, loss = 0.0006891085067763925
iteration 216, loss = 0.0034913744311779737
iteration 217, loss = 0.0012005396420136094
iteration 218, loss = 0.0007946076802909374
iteration 219, loss = 0.0007935275207273662
iteration 220, loss = 0.0021027515176683664
iteration 221, loss = 0.000781876384280622
iteration 222, loss = 0.0012562073534354568
iteration 223, loss = 0.0007968246936798096
iteration 224, loss = 0.0007194974459707737
iteration 225, loss = 0.0008102346328087151
iteration 226, loss = 0.0010824156925082207
iteration 227, loss = 0.0009731092141009867
iteration 228, loss = 0.0007442635833285749
iteration 229, loss = 0.0007478488842025399
iteration 230, loss = 0.0008488324237987399
iteration 231, loss = 0.0007924535311758518
iteration 232, loss = 0.0007414672872982919
iteration 233, loss = 0.0007902152137830853
iteration 234, loss = 0.0011159172281622887
iteration 235, loss = 0.001241010264493525
iteration 236, loss = 0.0008496837690472603
iteration 237, loss = 0.0009536381694488227
iteration 238, loss = 0.000728955608792603
iteration 239, loss = 0.0007177654188126326
iteration 240, loss = 0.0007504004752263427
iteration 241, loss = 0.0008128127083182335
iteration 242, loss = 0.0009867780609056354
iteration 243, loss = 0.0007096872432157397
iteration 244, loss = 0.0009535793797113001
iteration 245, loss = 0.0008286446682177484
iteration 246, loss = 0.0008206692291423678
iteration 247, loss = 0.0010436100419610739
iteration 248, loss = 0.000865414971485734
iteration 249, loss = 0.0008738675387576222
iteration 250, loss = 0.0006880682776682079
iteration 251, loss = 0.0006981801125220954
iteration 252, loss = 0.0008655117126181722
iteration 253, loss = 0.002096251817420125
iteration 254, loss = 0.0007237410172820091
iteration 255, loss = 0.0008425327250733972
iteration 256, loss = 0.002262075198814273
iteration 257, loss = 0.0007202522247098386
iteration 258, loss = 0.0007528545684181154
iteration 259, loss = 0.0008004850242286921
iteration 260, loss = 0.0007852754788473248
iteration 261, loss = 0.0008030647877603769
iteration 262, loss = 0.000696769799105823
iteration 263, loss = 0.001063580042682588
iteration 264, loss = 0.0008116071112453938
iteration 265, loss = 0.000806656084023416
iteration 266, loss = 0.0007482972578145564
iteration 267, loss = 0.0006913774413987994
iteration 268, loss = 0.0007033018046058714
iteration 269, loss = 0.0010609554592519999
iteration 270, loss = 0.0009245317196473479
iteration 271, loss = 0.0006881281151436269
iteration 272, loss = 0.0007247115136124194
iteration 273, loss = 0.001189784030430019
iteration 274, loss = 0.0008353905286639929
iteration 275, loss = 0.0010141091188415885
iteration 276, loss = 0.0007318275165744126
iteration 277, loss = 0.0007534219766966999
iteration 278, loss = 0.0007149808807298541
iteration 279, loss = 0.0007042990764603019
iteration 280, loss = 0.0014858791837468743
iteration 281, loss = 0.0011546636233106256
iteration 282, loss = 0.0007077662157826126
iteration 283, loss = 0.0008091533090919256
iteration 284, loss = 0.0007059976342134178
iteration 285, loss = 0.0007470367127098143
iteration 286, loss = 0.000846702721901238
iteration 287, loss = 0.0008049869793467224
iteration 288, loss = 0.0010771118104457855
iteration 289, loss = 0.0006808402831666172
iteration 290, loss = 0.0006932615651749074
iteration 291, loss = 0.0007769740768708289
iteration 292, loss = 0.0007908859406597912
iteration 293, loss = 0.0012345223221927881
iteration 294, loss = 0.0011844151886180043
iteration 295, loss = 0.0008212235406972468
iteration 296, loss = 0.0011921533150598407
iteration 297, loss = 0.000926426553633064
iteration 298, loss = 0.0007299660937860608
iteration 299, loss = 0.000733137596398592
iteration 0, loss = 0.0007299192948266864
iteration 1, loss = 0.0007621290278621018
iteration 2, loss = 0.0008212594548240304
iteration 3, loss = 0.0008630946394987404
iteration 4, loss = 0.0011414698092266917
iteration 5, loss = 0.0007359766750596464
iteration 6, loss = 0.0007005299557931721
iteration 7, loss = 0.0008200520533137023
iteration 8, loss = 0.0007081332732923329
iteration 9, loss = 0.000785030541010201
iteration 10, loss = 0.0007363589247688651
iteration 11, loss = 0.0008173973183147609
iteration 12, loss = 0.0012059262953698635
iteration 13, loss = 0.0008214614354074001
iteration 14, loss = 0.0009841544087976217
iteration 15, loss = 0.0007424908690154552
iteration 16, loss = 0.0011637845309451222
iteration 17, loss = 0.0007455660961568356
iteration 18, loss = 0.0007079518400132656
iteration 19, loss = 0.0007590754539705813
iteration 20, loss = 0.0008179473225027323
iteration 21, loss = 0.0008943229913711548
iteration 22, loss = 0.0008666579960845411
iteration 23, loss = 0.0006866335170343518
iteration 24, loss = 0.0011098554823547602
iteration 25, loss = 0.000805101590231061
iteration 26, loss = 0.0020660609006881714
iteration 27, loss = 0.000785911048296839
iteration 28, loss = 0.0007175197824835777
iteration 29, loss = 0.0007241274579428136
iteration 30, loss = 0.0008350842981599271
iteration 31, loss = 0.0007484685629606247
iteration 32, loss = 0.0007556069176644087
iteration 33, loss = 0.0007543754763901234
iteration 34, loss = 0.0008362942025996745
iteration 35, loss = 0.0014592582592740655
iteration 36, loss = 0.0007329904474318027
iteration 37, loss = 0.0008942887652665377
iteration 38, loss = 0.0007765960763208568
iteration 39, loss = 0.0007407934754155576
iteration 40, loss = 0.0008624657057225704
iteration 41, loss = 0.0007658806280232966
iteration 42, loss = 0.0006745990831404924
iteration 43, loss = 0.0007112830644473433
iteration 44, loss = 0.0007645995356142521
iteration 45, loss = 0.0008198714931495488
iteration 46, loss = 0.0007861824706196785
iteration 47, loss = 0.0007559403893537819
iteration 48, loss = 0.0007902499637566507
iteration 49, loss = 0.0007238209946081042
iteration 50, loss = 0.0008279480971395969
iteration 51, loss = 0.0008321731584146619
iteration 52, loss = 0.0007846319349482656
iteration 53, loss = 0.000825915252789855
iteration 54, loss = 0.0007440067129209638
iteration 55, loss = 0.0010296347318217158
iteration 56, loss = 0.0007939543575048447
iteration 57, loss = 0.0008023265399970114
iteration 58, loss = 0.0006872900994494557
iteration 59, loss = 0.0007419976172968745
iteration 60, loss = 0.0007330571534112096
iteration 61, loss = 0.0007131418096832931
iteration 62, loss = 0.0007074204040691257
iteration 63, loss = 0.00081878830678761
iteration 64, loss = 0.000694574904628098
iteration 65, loss = 0.0007665629964321852
iteration 66, loss = 0.0007740826695226133
iteration 67, loss = 0.0006981517653912306
iteration 68, loss = 0.0008465386345051229
iteration 69, loss = 0.0007241801358759403
iteration 70, loss = 0.0009182615322060883
iteration 71, loss = 0.0008065950823947787
iteration 72, loss = 0.0007465847884304821
iteration 73, loss = 0.0006730229360982776
iteration 74, loss = 0.0008011666359379888
iteration 75, loss = 0.0007033769506961107
iteration 76, loss = 0.0009589983383193612
iteration 77, loss = 0.0011109207989647985
iteration 78, loss = 0.0008172354428097606
iteration 79, loss = 0.0007384140626527369
iteration 80, loss = 0.0007035803282633424
iteration 81, loss = 0.000727799953892827
iteration 82, loss = 0.0007438736502081156
iteration 83, loss = 0.0007419350440613925
iteration 84, loss = 0.0008978343685157597
iteration 85, loss = 0.0006976929726079106
iteration 86, loss = 0.000837271218188107
iteration 87, loss = 0.0012556574074551463
iteration 88, loss = 0.0007191511103883386
iteration 89, loss = 0.0007801998872309923
iteration 90, loss = 0.000707696657627821
iteration 91, loss = 0.0012819661060348153
iteration 92, loss = 0.0007108122226782143
iteration 93, loss = 0.000721292570233345
iteration 94, loss = 0.0007611409528180957
iteration 95, loss = 0.0007694521336816251
iteration 96, loss = 0.0007034692098386586
iteration 97, loss = 0.0007912798901088536
iteration 98, loss = 0.0008873780025169253
iteration 99, loss = 0.0011075292713940144
iteration 100, loss = 0.0011333476286381483
iteration 101, loss = 0.0007183396373875439
iteration 102, loss = 0.0007021044730208814
iteration 103, loss = 0.0008481167024001479
iteration 104, loss = 0.0007865962688811123
iteration 105, loss = 0.0006752996705472469
iteration 106, loss = 0.0007689556223340333
iteration 107, loss = 0.0007965893601067364
iteration 108, loss = 0.0008829733706079423
iteration 109, loss = 0.0010660762200132012
iteration 110, loss = 0.0007250795606523752
iteration 111, loss = 0.0006780611001886427
iteration 112, loss = 0.0007943669334053993
iteration 113, loss = 0.0007530719740316272
iteration 114, loss = 0.0011966652236878872
iteration 115, loss = 0.0008029418531805277
iteration 116, loss = 0.0007603046833537519
iteration 117, loss = 0.0021372423507273197
iteration 118, loss = 0.0007772907265461981
iteration 119, loss = 0.0008359382045455277
iteration 120, loss = 0.001258638221770525
iteration 121, loss = 0.0006842655711807311
iteration 122, loss = 0.0009845580207183957
iteration 123, loss = 0.000792310165707022
iteration 124, loss = 0.0011396154295653105
iteration 125, loss = 0.0009309067390859127
iteration 126, loss = 0.0008251250255852938
iteration 127, loss = 0.0008166307816281915
iteration 128, loss = 0.000743546464946121
iteration 129, loss = 0.0011193854734301567
iteration 130, loss = 0.0006758868112228811
iteration 131, loss = 0.000927611836232245
iteration 132, loss = 0.0011801099171862006
iteration 133, loss = 0.0008010610472410917
iteration 134, loss = 0.0013023472856730223
iteration 135, loss = 0.000772254541516304
iteration 136, loss = 0.0010161306709051132
iteration 137, loss = 0.0007320960867218673
iteration 138, loss = 0.0008045861613936722
iteration 139, loss = 0.002073719399049878
iteration 140, loss = 0.0007157984655350447
iteration 141, loss = 0.00083520746557042
iteration 142, loss = 0.0007439707405865192
iteration 143, loss = 0.0006468705832958221
iteration 144, loss = 0.002217045286670327
iteration 145, loss = 0.0020826326217502356
iteration 146, loss = 0.00208158977329731
iteration 147, loss = 0.0010972877498716116
iteration 148, loss = 0.0006936862482689321
iteration 149, loss = 0.0006732656038366258
iteration 150, loss = 0.0009852253133431077
iteration 151, loss = 0.0011314087314531207
iteration 152, loss = 0.0007139563676901162
iteration 153, loss = 0.0006916585261933506
iteration 154, loss = 0.0007688450277782977
iteration 155, loss = 0.00200356706045568
iteration 156, loss = 0.0007910136482678354
iteration 157, loss = 0.001200395985506475
iteration 158, loss = 0.0008249519160017371
iteration 159, loss = 0.0008022043039090931
iteration 160, loss = 0.0021235260646790266
iteration 161, loss = 0.0010500563075765967
iteration 162, loss = 0.0007962126983329654
iteration 163, loss = 0.000811258505564183
iteration 164, loss = 0.0008555274689570069
iteration 165, loss = 0.0006508678197860718
iteration 166, loss = 0.0007485352107323706
iteration 167, loss = 0.0008096626261249185
iteration 168, loss = 0.0007193480851128697
iteration 169, loss = 0.0007096436456777155
iteration 170, loss = 0.00088245898950845
iteration 171, loss = 0.001091043814085424
iteration 172, loss = 0.0019978873897343874
iteration 173, loss = 0.0007811809773556888
iteration 174, loss = 0.0007832086412236094
iteration 175, loss = 0.0008203363977372646
iteration 176, loss = 0.0008548481855541468
iteration 177, loss = 0.0007275919197127223
iteration 178, loss = 0.0006802467396482825
iteration 179, loss = 0.0006879559368826449
iteration 180, loss = 0.0011607325868681073
iteration 181, loss = 0.0007319807773455977
iteration 182, loss = 0.0007285691681317985
iteration 183, loss = 0.002060210332274437
iteration 184, loss = 0.0007947242120280862
iteration 185, loss = 0.0008409198489971459
iteration 186, loss = 0.0006989127141423523
iteration 187, loss = 0.0008292128331959248
iteration 188, loss = 0.0007807548390701413
iteration 189, loss = 0.0007420319016091526
iteration 190, loss = 0.0007409009849652648
iteration 191, loss = 0.0006281850510276854
iteration 192, loss = 0.0007336444105021656
iteration 193, loss = 0.0006963181658647954
iteration 194, loss = 0.001023978227749467
iteration 195, loss = 0.000803067465312779
iteration 196, loss = 0.0020426474511623383
iteration 197, loss = 0.0020028010476380587
iteration 198, loss = 0.000783707422669977
iteration 199, loss = 0.00076321727829054
iteration 200, loss = 0.0007249723421409726
iteration 201, loss = 0.0008093222859315574
iteration 202, loss = 0.0008486075676046312
iteration 203, loss = 0.0007358972216024995
iteration 204, loss = 0.0007200254476629198
iteration 205, loss = 0.0011383300879970193
iteration 206, loss = 0.0021092190872877836
iteration 207, loss = 0.000758390873670578
iteration 208, loss = 0.0009642334189265966
iteration 209, loss = 0.0007365581695921719
iteration 210, loss = 0.0009725535637699068
iteration 211, loss = 0.0010489934356883168
iteration 212, loss = 0.0019992373418062925
iteration 213, loss = 0.0007076658075675368
iteration 214, loss = 0.0007657872047275305
iteration 215, loss = 0.0006726615247316658
iteration 216, loss = 0.0007152080070227385
iteration 217, loss = 0.0007935562753118575
iteration 218, loss = 0.0007316711125895381
iteration 219, loss = 0.0007422197377309203
iteration 220, loss = 0.0006582083879038692
iteration 221, loss = 0.0007875965675339103
iteration 222, loss = 0.0007067352416925132
iteration 223, loss = 0.0008457297808490694
iteration 224, loss = 0.0006822138675488532
iteration 225, loss = 0.0006451441440731287
iteration 226, loss = 0.0010258803376927972
iteration 227, loss = 0.0008732438436709344
iteration 228, loss = 0.0007163698319345713
iteration 229, loss = 0.0007264462765306234
iteration 230, loss = 0.0006710081361234188
iteration 231, loss = 0.0007269100751727819
iteration 232, loss = 0.0009649599087424576
iteration 233, loss = 0.000820519111584872
iteration 234, loss = 0.0007537328056059778
iteration 235, loss = 0.0007148572476580739
iteration 236, loss = 0.000681855424772948
iteration 237, loss = 0.0007042987272143364
iteration 238, loss = 0.0007951410952955484
iteration 239, loss = 0.0011828122660517693
iteration 240, loss = 0.0008274387218989432
iteration 241, loss = 0.0007467910181730986
iteration 242, loss = 0.0007092969608493149
iteration 243, loss = 0.00072768225800246
iteration 244, loss = 0.000784361909609288
iteration 245, loss = 0.0007471216376870871
iteration 246, loss = 0.0007102195522747934
iteration 247, loss = 0.0006957434816285968
iteration 248, loss = 0.0007556203636340797
iteration 249, loss = 0.0006586149684153497
iteration 250, loss = 0.0006384088192135096
iteration 251, loss = 0.0007380431052297354
iteration 252, loss = 0.0007050609565339983
iteration 253, loss = 0.0006614692974835634
iteration 254, loss = 0.0006767665618099272
iteration 255, loss = 0.0007158219232223928
iteration 256, loss = 0.0010188361629843712
iteration 257, loss = 0.0007482306100428104
iteration 258, loss = 0.0007516822661273181
iteration 259, loss = 0.0007710159989073873
iteration 260, loss = 0.0007284384919330478
iteration 261, loss = 0.0007439734181389213
iteration 262, loss = 0.0007054481538943946
iteration 263, loss = 0.0010110537987202406
iteration 264, loss = 0.0007442286005243659
iteration 265, loss = 0.0007445453084073961
iteration 266, loss = 0.0006935373530723155
iteration 267, loss = 0.0007476445171050727
iteration 268, loss = 0.0007602730765938759
iteration 269, loss = 0.0006904113106429577
iteration 270, loss = 0.0006630164571106434
iteration 271, loss = 0.0007127693388611078
iteration 272, loss = 0.0006716006901115179
iteration 273, loss = 0.0009575789445079863
iteration 274, loss = 0.000717048067599535
iteration 275, loss = 0.0006993933347985148
iteration 276, loss = 0.0007560072699561715
iteration 277, loss = 0.0007047258550301194
iteration 278, loss = 0.0007944350363686681
iteration 279, loss = 0.0008574866224080324
iteration 280, loss = 0.0007750139338895679
iteration 281, loss = 0.0007980711525306106
iteration 282, loss = 0.0019791496451944113
iteration 283, loss = 0.0009061894961632788
iteration 284, loss = 0.0009905881015583873
iteration 285, loss = 0.0006481370073743165
iteration 286, loss = 0.001476956414990127
iteration 287, loss = 0.0007958468631841242
iteration 288, loss = 0.0008115496602840722
iteration 289, loss = 0.0007800718303769827
iteration 290, loss = 0.00099763844627887
iteration 291, loss = 0.0007071055006235838
iteration 292, loss = 0.0007520293584093451
iteration 293, loss = 0.000884304812643677
iteration 294, loss = 0.0007462661014869809
iteration 295, loss = 0.0011671216925606132
iteration 296, loss = 0.0007150069577619433
iteration 297, loss = 0.0006705854320898652
iteration 298, loss = 0.0009531878167763352
iteration 299, loss = 0.0008306954987347126
iteration 0, loss = 0.001174523145891726
iteration 1, loss = 0.0010314607061445713
iteration 2, loss = 0.0007640657131560147
iteration 3, loss = 0.001134886289946735
iteration 4, loss = 0.00080493139103055
iteration 5, loss = 0.0011923523852601647
iteration 6, loss = 0.0010791594395413995
iteration 7, loss = 0.0008035413920879364
iteration 8, loss = 0.0007465729722753167
iteration 9, loss = 0.0007131453021429479
iteration 10, loss = 0.0007839526515454054
iteration 11, loss = 0.0008836875203996897
iteration 12, loss = 0.0011649713851511478
iteration 13, loss = 0.0006752655026502907
iteration 14, loss = 0.0006936203571967781
iteration 15, loss = 0.0007049888372421265
iteration 16, loss = 0.0006889184005558491
iteration 17, loss = 0.0009959678864106536
iteration 18, loss = 0.0007753993268124759
iteration 19, loss = 0.0007682073046453297
iteration 20, loss = 0.0007440660847350955
iteration 21, loss = 0.0007819018792361021
iteration 22, loss = 0.0007910057902336121
iteration 23, loss = 0.0006943809567019343
iteration 24, loss = 0.0007189424941316247
iteration 25, loss = 0.0008718158933334053
iteration 26, loss = 0.0006257650093175471
iteration 27, loss = 0.0007213868084363639
iteration 28, loss = 0.0006729444721713662
iteration 29, loss = 0.0007534531177952886
iteration 30, loss = 0.0006776803638786077
iteration 31, loss = 0.0006950392271392047
iteration 32, loss = 0.0007592277252115309
iteration 33, loss = 0.0007079817587509751
iteration 34, loss = 0.0020378653425723314
iteration 35, loss = 0.0008234092965722084
iteration 36, loss = 0.0007335256668739021
iteration 37, loss = 0.0010320832952857018
iteration 38, loss = 0.0006631035939790308
iteration 39, loss = 0.0006471192464232445
iteration 40, loss = 0.0007323091849684715
iteration 41, loss = 0.0007973405881784856
iteration 42, loss = 0.0007640999974682927
iteration 43, loss = 0.0007526553818024695
iteration 44, loss = 0.0007569732842966914
iteration 45, loss = 0.0006767755839973688
iteration 46, loss = 0.0007779925363138318
iteration 47, loss = 0.0009454578394070268
iteration 48, loss = 0.0007469828706234694
iteration 49, loss = 0.00074450031388551
iteration 50, loss = 0.0007501162472181022
iteration 51, loss = 0.0006849238998256624
iteration 52, loss = 0.0006963908090256155
iteration 53, loss = 0.0009816756937652826
iteration 54, loss = 0.0006738938391208649
iteration 55, loss = 0.0007873502909205854
iteration 56, loss = 0.0008043948328122497
iteration 57, loss = 0.0006860376452095807
iteration 58, loss = 0.0007601649267598987
iteration 59, loss = 0.0007019707700237632
iteration 60, loss = 0.0009739215602166951
iteration 61, loss = 0.0007708128541707993
iteration 62, loss = 0.0007390696555376053
iteration 63, loss = 0.0007226296584121883
iteration 64, loss = 0.002537544583901763
iteration 65, loss = 0.0007530728471465409
iteration 66, loss = 0.0008094912045635283
iteration 67, loss = 0.0007747781928628683
iteration 68, loss = 0.0009494764963164926
iteration 69, loss = 0.0007580207311548293
iteration 70, loss = 0.0007049377309158444
iteration 71, loss = 0.0008097239769995213
iteration 72, loss = 0.0008549311896786094
iteration 73, loss = 0.0006180958007462323
iteration 74, loss = 0.0009944119956344366
iteration 75, loss = 0.0008411711314693093
iteration 76, loss = 0.0006288810982368886
iteration 77, loss = 0.0010218361858278513
iteration 78, loss = 0.0007038685726001859
iteration 79, loss = 0.0007078366470523179
iteration 80, loss = 0.0007724082679487765
iteration 81, loss = 0.0009936800925061107
iteration 82, loss = 0.0007247608155012131
iteration 83, loss = 0.0008412403985857964
iteration 84, loss = 0.0007066044490784407
iteration 85, loss = 0.0007967323763296008
iteration 86, loss = 0.0007389549864456058
iteration 87, loss = 0.0020393049344420433
iteration 88, loss = 0.0007386376382783055
iteration 89, loss = 0.0006349707255139947
iteration 90, loss = 0.0007318622665479779
iteration 91, loss = 0.0011353883892297745
iteration 92, loss = 0.0011506592854857445
iteration 93, loss = 0.0006780675612390041
iteration 94, loss = 0.0007677071262151003
iteration 95, loss = 0.0006844635354354978
iteration 96, loss = 0.0011719351168721914
iteration 97, loss = 0.0020572904031723738
iteration 98, loss = 0.0007041115895844996
iteration 99, loss = 0.0007146287825889885
iteration 100, loss = 0.0007177794468589127
iteration 101, loss = 0.0006830102065578103
iteration 102, loss = 0.0007529944414272904
iteration 103, loss = 0.0006940743769519031
iteration 104, loss = 0.0007120642112568021
iteration 105, loss = 0.0006979654426686466
iteration 106, loss = 0.0007168391603045166
iteration 107, loss = 0.0007542272214777768
iteration 108, loss = 0.0007439005421474576
iteration 109, loss = 0.0007899297634139657
iteration 110, loss = 0.0008841727394610643
iteration 111, loss = 0.0007351450040005147
iteration 112, loss = 0.0008196553098969162
iteration 113, loss = 0.0007525116670876741
iteration 114, loss = 0.0007159882225096226
iteration 115, loss = 0.0009508741786703467
iteration 116, loss = 0.0007468410767614841
iteration 117, loss = 0.0009675725596025586
iteration 118, loss = 0.0007481748471036553
iteration 119, loss = 0.0006422771257348359
iteration 120, loss = 0.001295713009312749
iteration 121, loss = 0.0007754871039651334
iteration 122, loss = 0.0008500866242684424
iteration 123, loss = 0.0007233700016513467
iteration 124, loss = 0.0007180985994637012
iteration 125, loss = 0.0011337049072608352
iteration 126, loss = 0.0007805657223798335
iteration 127, loss = 0.0010475964518263936
iteration 128, loss = 0.0006996444426476955
iteration 129, loss = 0.0007538680802099407
iteration 130, loss = 0.0007481192005798221
iteration 131, loss = 0.0006492232787422836
iteration 132, loss = 0.0007144964183680713
iteration 133, loss = 0.0006487295613624156
iteration 134, loss = 0.0007150188903324306
iteration 135, loss = 0.0020597234833985567
iteration 136, loss = 0.000883994740433991
iteration 137, loss = 0.0006989646935835481
iteration 138, loss = 0.0006945594795979559
iteration 139, loss = 0.000796620617620647
iteration 140, loss = 0.0006652417359873652
iteration 141, loss = 0.0008011457975953817
iteration 142, loss = 0.0008010452729649842
iteration 143, loss = 0.0007171939359977841
iteration 144, loss = 0.0007223365246318281
iteration 145, loss = 0.0006705946289002895
iteration 146, loss = 0.000760696770157665
iteration 147, loss = 0.0011310480767861009
iteration 148, loss = 0.0008076460799202323
iteration 149, loss = 0.0008275105501525104
iteration 150, loss = 0.0006663912208750844
iteration 151, loss = 0.0006798658287152648
iteration 152, loss = 0.0007985122501850128
iteration 153, loss = 0.002295101759955287
iteration 154, loss = 0.0007988217403180897
iteration 155, loss = 0.0007089479477144778
iteration 156, loss = 0.0006831325008533895
iteration 157, loss = 0.0006851100479252636
iteration 158, loss = 0.0007819343009032309
iteration 159, loss = 0.0006831600912846625
iteration 160, loss = 0.0007879564655013382
iteration 161, loss = 0.0006831695791333914
iteration 162, loss = 0.0008112706709653139
iteration 163, loss = 0.0007377587608061731
iteration 164, loss = 0.0006679898942820728
iteration 165, loss = 0.000700673961546272
iteration 166, loss = 0.0007850952679291368
iteration 167, loss = 0.000728929415345192
iteration 168, loss = 0.0010744939791038632
iteration 169, loss = 0.000722562603186816
iteration 170, loss = 0.000676885771099478
iteration 171, loss = 0.0009493788820691407
iteration 172, loss = 0.0006899197469465435
iteration 173, loss = 0.0006835988024249673
iteration 174, loss = 0.0006251591839827597
iteration 175, loss = 0.0007785638445056975
iteration 176, loss = 0.0007625460857525468
iteration 177, loss = 0.0007570106536149979
iteration 178, loss = 0.0007643835269846022
iteration 179, loss = 0.0019808406941592693
iteration 180, loss = 0.0006852515507489443
iteration 181, loss = 0.0009723720140755177
iteration 182, loss = 0.0006877171690575778
iteration 183, loss = 0.0007335360278375447
iteration 184, loss = 0.0019823622424155474
iteration 185, loss = 0.0007422046037390828
iteration 186, loss = 0.0006472066743299365
iteration 187, loss = 0.000600938219577074
iteration 188, loss = 0.0006689239526167512
iteration 189, loss = 0.000671552843414247
iteration 190, loss = 0.0006713142502121627
iteration 191, loss = 0.0006620913627557456
iteration 192, loss = 0.0009138616733253002
iteration 193, loss = 0.0009392913780175149
iteration 194, loss = 0.0006660926155745983
iteration 195, loss = 0.000665675092022866
iteration 196, loss = 0.0008269174722954631
iteration 197, loss = 0.0007427215459756553
iteration 198, loss = 0.0008159350836649537
iteration 199, loss = 0.0006418463308364153
iteration 200, loss = 0.0006511308019980788
iteration 201, loss = 0.0006729635642841458
iteration 202, loss = 0.0007039174670353532
iteration 203, loss = 0.00064115907298401
iteration 204, loss = 0.0006996302399784327
iteration 205, loss = 0.0008453897899016738
iteration 206, loss = 0.000764678989071399
iteration 207, loss = 0.0006424408056773245
iteration 208, loss = 0.0007124285330064595
iteration 209, loss = 0.0007869856781326234
iteration 210, loss = 0.0006568588432855904
iteration 211, loss = 0.0006906870403327048
iteration 212, loss = 0.000706937164068222
iteration 213, loss = 0.0008026104769669473
iteration 214, loss = 0.0020330913830548525
iteration 215, loss = 0.0008190683438442647
iteration 216, loss = 0.000678045442327857
iteration 217, loss = 0.0008627297938801348
iteration 218, loss = 0.0008079558610916138
iteration 219, loss = 0.0014501672703772783
iteration 220, loss = 0.0007088979473337531
iteration 221, loss = 0.0009913155809044838
iteration 222, loss = 0.0009504628251306713
iteration 223, loss = 0.0008253013365902007
iteration 224, loss = 0.0007672382052987814
iteration 225, loss = 0.0006737530347891152
iteration 226, loss = 0.0007458714535459876
iteration 227, loss = 0.0006896916893310845
iteration 228, loss = 0.0014242175966501236
iteration 229, loss = 0.0009668109705671668
iteration 230, loss = 0.0006719609955325723
iteration 231, loss = 0.0007663398864679039
iteration 232, loss = 0.000693571288138628
iteration 233, loss = 0.0019441947806626558
iteration 234, loss = 0.0006848796620033681
iteration 235, loss = 0.0007669953629374504
iteration 236, loss = 0.0006918745348230004
iteration 237, loss = 0.0007464964292012155
iteration 238, loss = 0.0007402192568406463
iteration 239, loss = 0.0007402032497338951
iteration 240, loss = 0.0023007027339190245
iteration 241, loss = 0.0007018971955403686
iteration 242, loss = 0.0006688196444883943
iteration 243, loss = 0.0006975936121307313
iteration 244, loss = 0.002013059798628092
iteration 245, loss = 0.0008030647877603769
iteration 246, loss = 0.0008382661617361009
iteration 247, loss = 0.0007951719453558326
iteration 248, loss = 0.0008724002400413156
iteration 249, loss = 0.002362289233133197
iteration 250, loss = 0.0006766973529011011
iteration 251, loss = 0.0006840553251095116
iteration 252, loss = 0.0007079143542796373
iteration 253, loss = 0.0008160060970112681
iteration 254, loss = 0.0007218081154860556
iteration 255, loss = 0.0007132448954507709
iteration 256, loss = 0.0006691417074762285
iteration 257, loss = 0.0006451443769037724
iteration 258, loss = 0.0006460389122366905
iteration 259, loss = 0.0007187993032857776
iteration 260, loss = 0.000768369878642261
iteration 261, loss = 0.001162676839157939
iteration 262, loss = 0.0007276374963112175
iteration 263, loss = 0.0009704768308438361
iteration 264, loss = 0.0008299083565361798
iteration 265, loss = 0.0006184373633004725
iteration 266, loss = 0.0006290446617640555
iteration 267, loss = 0.0006282102549448609
iteration 268, loss = 0.0006640139035880566
iteration 269, loss = 0.0006649976712651551
iteration 270, loss = 0.0006799310212954879
iteration 271, loss = 0.0006251121521927416
iteration 272, loss = 0.0006482453318312764
iteration 273, loss = 0.0009527167421765625
iteration 274, loss = 0.0020125287119299173
iteration 275, loss = 0.0006452586967498064
iteration 276, loss = 0.000695298716891557
iteration 277, loss = 0.0006822464056313038
iteration 278, loss = 0.0006900385487824678
iteration 279, loss = 0.001051915343850851
iteration 280, loss = 0.0019346359185874462
iteration 281, loss = 0.0006580821936950088
iteration 282, loss = 0.0009128231322392821
iteration 283, loss = 0.0006611351273022592
iteration 284, loss = 0.0007132017053663731
iteration 285, loss = 0.0007189452880993485
iteration 286, loss = 0.0007425700314342976
iteration 287, loss = 0.0006769694155082107
iteration 288, loss = 0.0007373688276857138
iteration 289, loss = 0.0006604976369999349
iteration 290, loss = 0.0007581347599625587
iteration 291, loss = 0.0006574640283361077
iteration 292, loss = 0.0006360516999848187
iteration 293, loss = 0.0007051952998153865
iteration 294, loss = 0.00076032750075683
iteration 295, loss = 0.0009106531506404281
iteration 296, loss = 0.000934382900595665
iteration 297, loss = 0.0008932757191359997
iteration 298, loss = 0.0006493169348686934
iteration 299, loss = 0.0007317419513128698
iteration 0, loss = 0.0010728274937719107
iteration 1, loss = 0.000737747352104634
iteration 2, loss = 0.000662208825815469
iteration 3, loss = 0.00072296621510759
iteration 4, loss = 0.0011791072320193052
iteration 5, loss = 0.000707323313690722
iteration 6, loss = 0.0007668682956136763
iteration 7, loss = 0.0007230535265989602
iteration 8, loss = 0.0010331973899155855
iteration 9, loss = 0.0007059997296892107
iteration 10, loss = 0.0007778214057907462
iteration 11, loss = 0.0006556189619004726
iteration 12, loss = 0.0008260801550932229
iteration 13, loss = 0.001098065753467381
iteration 14, loss = 0.0007182458648458123
iteration 15, loss = 0.001973465783521533
iteration 16, loss = 0.0006067812792025506
iteration 17, loss = 0.0006750152097083628
iteration 18, loss = 0.0007089633145369589
iteration 19, loss = 0.0011176617117598653
iteration 20, loss = 0.0006716942298226058
iteration 21, loss = 0.0007778154104016721
iteration 22, loss = 0.0006751424516551197
iteration 23, loss = 0.0019748706836253405
iteration 24, loss = 0.0007488015689887106
iteration 25, loss = 0.0007912718574516475
iteration 26, loss = 0.0006696793716400862
iteration 27, loss = 0.000718887837138027
iteration 28, loss = 0.0006027402123436332
iteration 29, loss = 0.0007978056673891842
iteration 30, loss = 0.0007797868456691504
iteration 31, loss = 0.0006839841371402144
iteration 32, loss = 0.0007126181153580546
iteration 33, loss = 0.0006931597599759698
iteration 34, loss = 0.0006798158283345401
iteration 35, loss = 0.0006759352399967611
iteration 36, loss = 0.0006839459529146552
iteration 37, loss = 0.0006244837422855198
iteration 38, loss = 0.0007425514049828053
iteration 39, loss = 0.0007836045697331429
iteration 40, loss = 0.000958296237513423
iteration 41, loss = 0.000637406250461936
iteration 42, loss = 0.0006578266038559377
iteration 43, loss = 0.0006822317955084145
iteration 44, loss = 0.0007259820704348385
iteration 45, loss = 0.0007114855688996613
iteration 46, loss = 0.0007629640749655664
iteration 47, loss = 0.0008034578058868647
iteration 48, loss = 0.0006384228472597897
iteration 49, loss = 0.0007173835765570402
iteration 50, loss = 0.0006515351706184447
iteration 51, loss = 0.000685853767208755
iteration 52, loss = 0.0008309585973620415
iteration 53, loss = 0.0007755144033581018
iteration 54, loss = 0.0008004388073459268
iteration 55, loss = 0.0006745427963323891
iteration 56, loss = 0.0007239240221679211
iteration 57, loss = 0.0007166298455558717
iteration 58, loss = 0.00082228216342628
iteration 59, loss = 0.001073736697435379
iteration 60, loss = 0.0006670328439213336
iteration 61, loss = 0.0007513553719036281
iteration 62, loss = 0.0019034668803215027
iteration 63, loss = 0.0010100440122187138
iteration 64, loss = 0.0008175342227332294
iteration 65, loss = 0.0006776057416573167
iteration 66, loss = 0.0006333327037282288
iteration 67, loss = 0.0007687649340368807
iteration 68, loss = 0.0006368720787577331
iteration 69, loss = 0.0006766793667338789
iteration 70, loss = 0.0007432658458128572
iteration 71, loss = 0.000657033990137279
iteration 72, loss = 0.0006078362930566072
iteration 73, loss = 0.0006766133010387421
iteration 74, loss = 0.0007341938326135278
iteration 75, loss = 0.0008048102608881891
iteration 76, loss = 0.0006875476101413369
iteration 77, loss = 0.0006582633941434324
iteration 78, loss = 0.0006770816980861127
iteration 79, loss = 0.0006923837354406714
iteration 80, loss = 0.0006765224388800561
iteration 81, loss = 0.0007904061931185424
iteration 82, loss = 0.0006549977697432041
iteration 83, loss = 0.0007372616673819721
iteration 84, loss = 0.0006147819804027677
iteration 85, loss = 0.0006700212834402919
iteration 86, loss = 0.001083172275684774
iteration 87, loss = 0.0007836443837732077
iteration 88, loss = 0.0007372269756160676
iteration 89, loss = 0.0006922271568328142
iteration 90, loss = 0.0006444565951824188
iteration 91, loss = 0.0007369235972873867
iteration 92, loss = 0.0009377857786603272
iteration 93, loss = 0.0007590232999064028
iteration 94, loss = 0.0007162722176872194
iteration 95, loss = 0.0007760386797599494
iteration 96, loss = 0.0008046090952120721
iteration 97, loss = 0.0007554522017017007
iteration 98, loss = 0.0007021667552180588
iteration 99, loss = 0.0007030717679299414
iteration 100, loss = 0.0006148687680251896
iteration 101, loss = 0.0009441565489396453
iteration 102, loss = 0.0019603283144533634
iteration 103, loss = 0.0006738760275766253
iteration 104, loss = 0.0007320172153413296
iteration 105, loss = 0.0006899242871440947
iteration 106, loss = 0.0007037486066110432
iteration 107, loss = 0.0006366310990415514
iteration 108, loss = 0.0007031896384432912
iteration 109, loss = 0.0006429717759601772
iteration 110, loss = 0.000661740021314472
iteration 111, loss = 0.0007133783074095845
iteration 112, loss = 0.0011725806398317218
iteration 113, loss = 0.0006457426934503019
iteration 114, loss = 0.0006519651506096125
iteration 115, loss = 0.0007225683657452464
iteration 116, loss = 0.001056469394825399
iteration 117, loss = 0.000665173283778131
iteration 118, loss = 0.0006505880155600607
iteration 119, loss = 0.0006783750141039491
iteration 120, loss = 0.0008764085941947997
iteration 121, loss = 0.0007988058496266603
iteration 122, loss = 0.0006565063958987594
iteration 123, loss = 0.0007384932250715792
iteration 124, loss = 0.0006885803304612637
iteration 125, loss = 0.0006628067931160331
iteration 126, loss = 0.0006446944898925722
iteration 127, loss = 0.0007090846775099635
iteration 128, loss = 0.000631457194685936
iteration 129, loss = 0.0006993953720666468
iteration 130, loss = 0.0006696518394164741
iteration 131, loss = 0.0006724115228280425
iteration 132, loss = 0.0008602432790212333
iteration 133, loss = 0.0005991710349917412
iteration 134, loss = 0.000680270662996918
iteration 135, loss = 0.0006446781917475164
iteration 136, loss = 0.0007387929945252836
iteration 137, loss = 0.0006694650510326028
iteration 138, loss = 0.0006929025403223932
iteration 139, loss = 0.0008069449104368687
iteration 140, loss = 0.000880714796949178
iteration 141, loss = 0.0006559073808602989
iteration 142, loss = 0.0006445320323109627
iteration 143, loss = 0.000749284983612597
iteration 144, loss = 0.0006505287019535899
iteration 145, loss = 0.0007286602631211281
iteration 146, loss = 0.0009006225736811757
iteration 147, loss = 0.0005953136133030057
iteration 148, loss = 0.0007249466725625098
iteration 149, loss = 0.0006233358290046453
iteration 150, loss = 0.0007002342608757317
iteration 151, loss = 0.0006653577438555658
iteration 152, loss = 0.0008845424163155258
iteration 153, loss = 0.0007164629641920328
iteration 154, loss = 0.0007365071214735508
iteration 155, loss = 0.0006974120042286813
iteration 156, loss = 0.0007044996600598097
iteration 157, loss = 0.000696590868756175
iteration 158, loss = 0.0009364798315800726
iteration 159, loss = 0.0006013534148223698
iteration 160, loss = 0.0007362585165537894
iteration 161, loss = 0.0006806001765653491
iteration 162, loss = 0.0009863075101748109
iteration 163, loss = 0.0006672936142422259
iteration 164, loss = 0.000674478942528367
iteration 165, loss = 0.0007181762484833598
iteration 166, loss = 0.0008213203400373459
iteration 167, loss = 0.0007270664209499955
iteration 168, loss = 0.0006355852819979191
iteration 169, loss = 0.0007138752844184637
iteration 170, loss = 0.0006683344836346805
iteration 171, loss = 0.0005984878516755998
iteration 172, loss = 0.0007004525396041572
iteration 173, loss = 0.001273496076464653
iteration 174, loss = 0.0010483920341357589
iteration 175, loss = 0.0006435107789002359
iteration 176, loss = 0.0019251506309956312
iteration 177, loss = 0.0009870051871985197
iteration 178, loss = 0.0006934187258593738
iteration 179, loss = 0.001136818784289062
iteration 180, loss = 0.0007938630296848714
iteration 181, loss = 0.0006314168567769229
iteration 182, loss = 0.0007223158609122038
iteration 183, loss = 0.000684332218952477
iteration 184, loss = 0.0007589323213323951
iteration 185, loss = 0.0006280930829234421
iteration 186, loss = 0.0006479707662947476
iteration 187, loss = 0.0021249211858958006
iteration 188, loss = 0.0007026463863439858
iteration 189, loss = 0.0007139783701859415
iteration 190, loss = 0.0019776313565671444
iteration 191, loss = 0.001986192772164941
iteration 192, loss = 0.0006740887765772641
iteration 193, loss = 0.0009453024249523878
iteration 194, loss = 0.000704676378518343
iteration 195, loss = 0.0007382226758636534
iteration 196, loss = 0.000663603306747973
iteration 197, loss = 0.0009913812391459942
iteration 198, loss = 0.0006417006370611489
iteration 199, loss = 0.0007483431836590171
iteration 200, loss = 0.000730032566934824
iteration 201, loss = 0.0007450993289239705
iteration 202, loss = 0.0007493067532777786
iteration 203, loss = 0.0007197388331405818
iteration 204, loss = 0.0006366943125613034
iteration 205, loss = 0.002081021200865507
iteration 206, loss = 0.000642877712380141
iteration 207, loss = 0.0007228219183161855
iteration 208, loss = 0.0022175800986588
iteration 209, loss = 0.0010394749697297812
iteration 210, loss = 0.0006940099410712719
iteration 211, loss = 0.0008149520144797862
iteration 212, loss = 0.001005191938020289
iteration 213, loss = 0.001207041321322322
iteration 214, loss = 0.000787271186709404
iteration 215, loss = 0.0008026384748518467
iteration 216, loss = 0.0010069311829283834
iteration 217, loss = 0.0006994615541771054
iteration 218, loss = 0.0008028872543945909
iteration 219, loss = 0.0007380488095805049
iteration 220, loss = 0.0007160793757066131
iteration 221, loss = 0.0007520039798691869
iteration 222, loss = 0.0007226998568512499
iteration 223, loss = 0.0006429926143027842
iteration 224, loss = 0.0006662567611783743
iteration 225, loss = 0.0007524792454205453
iteration 226, loss = 0.0008774293237365782
iteration 227, loss = 0.0006511703249998391
iteration 228, loss = 0.0007075955509208143
iteration 229, loss = 0.0006616955506615341
iteration 230, loss = 0.0022529002744704485
iteration 231, loss = 0.0007544045220129192
iteration 232, loss = 0.0006949292146600783
iteration 233, loss = 0.0006365446606650949
iteration 234, loss = 0.0006500164745375514
iteration 235, loss = 0.0007140995585359633
iteration 236, loss = 0.0006589331896975636
iteration 237, loss = 0.001070941798388958
iteration 238, loss = 0.0006798537215217948
iteration 239, loss = 0.0007175300270318985
iteration 240, loss = 0.0007587377913296223
iteration 241, loss = 0.0006553459679707885
iteration 242, loss = 0.0006694107432849705
iteration 243, loss = 0.0006151432753540576
iteration 244, loss = 0.0006383692380040884
iteration 245, loss = 0.0006698070792481303
iteration 246, loss = 0.0010534025495871902
iteration 247, loss = 0.0009721866226755083
iteration 248, loss = 0.0006870927172712982
iteration 249, loss = 0.0008103966829366982
iteration 250, loss = 0.0007393851410597563
iteration 251, loss = 0.0007194837089627981
iteration 252, loss = 0.0007375593995675445
iteration 253, loss = 0.0009686673292890191
iteration 254, loss = 0.0007129552541300654
iteration 255, loss = 0.0006356916856020689
iteration 256, loss = 0.0007206926238723099
iteration 257, loss = 0.0006717325304634869
iteration 258, loss = 0.0006790448678657413
iteration 259, loss = 0.0008825050899758935
iteration 260, loss = 0.0018991627730429173
iteration 261, loss = 0.0005955043598078191
iteration 262, loss = 0.0006444554310292006
iteration 263, loss = 0.001907728728838265
iteration 264, loss = 0.0006202169461175799
iteration 265, loss = 0.0006665679393336177
iteration 266, loss = 0.0006093396805226803
iteration 267, loss = 0.0006736403447575867
iteration 268, loss = 0.0006628122646361589
iteration 269, loss = 0.0007589546730741858
iteration 270, loss = 0.0007233757642097771
iteration 271, loss = 0.0007418149616569281
iteration 272, loss = 0.0007030121050775051
iteration 273, loss = 0.0006401886930689216
iteration 274, loss = 0.000934622366912663
iteration 275, loss = 0.0007134648622013628
iteration 276, loss = 0.0006185941747389734
iteration 277, loss = 0.0008385529508814216
iteration 278, loss = 0.0006741551333107054
iteration 279, loss = 0.0006933169206604362
iteration 280, loss = 0.0010524126701056957
iteration 281, loss = 0.0006040497100912035
iteration 282, loss = 0.0008029273594729602
iteration 283, loss = 0.0006580351036973298
iteration 284, loss = 0.0007447863463312387
iteration 285, loss = 0.000632094859611243
iteration 286, loss = 0.000625448883511126
iteration 287, loss = 0.0007211963529698551
iteration 288, loss = 0.0007114134496077895
iteration 289, loss = 0.0008847399149090052
iteration 290, loss = 0.0009615811868570745
iteration 291, loss = 0.0009580939658917487
iteration 292, loss = 0.0010558194480836391
iteration 293, loss = 0.0007032603025436401
iteration 294, loss = 0.000669302127789706
iteration 295, loss = 0.0007458199979737401
iteration 296, loss = 0.0010196462972089648
iteration 297, loss = 0.0030764229595661163
iteration 298, loss = 0.0006159835611470044
iteration 299, loss = 0.0006696238997392356
iteration 0, loss = 0.0006634583696722984
iteration 1, loss = 0.0006447397172451019
iteration 2, loss = 0.0010447221575304866
iteration 3, loss = 0.0007213900098577142
iteration 4, loss = 0.0008625583723187447
iteration 5, loss = 0.0006391285569407046
iteration 6, loss = 0.0006579802138730884
iteration 7, loss = 0.0011464739218354225
iteration 8, loss = 0.001842841855250299
iteration 9, loss = 0.0006455981056205928
iteration 10, loss = 0.000782485119998455
iteration 11, loss = 0.0006194036104716361
iteration 12, loss = 0.000649830442853272
iteration 13, loss = 0.000631113420240581
iteration 14, loss = 0.000725293648429215
iteration 15, loss = 0.0006295650382526219
iteration 16, loss = 0.0009731203899718821
iteration 17, loss = 0.0006409906200133264
iteration 18, loss = 0.0006824883166700602
iteration 19, loss = 0.0006698353099636734
iteration 20, loss = 0.0006877526175230742
iteration 21, loss = 0.0006581493653357029
iteration 22, loss = 0.0007219214458018541
iteration 23, loss = 0.0006654829485341907
iteration 24, loss = 0.0007470283308066428
iteration 25, loss = 0.0007502201478928328
iteration 26, loss = 0.0006621801294386387
iteration 27, loss = 0.0006296702194958925
iteration 28, loss = 0.0008900143438950181
iteration 29, loss = 0.0006302447873167694
iteration 30, loss = 0.0006425578612834215
iteration 31, loss = 0.0008746439125388861
iteration 32, loss = 0.0007437541498802602
iteration 33, loss = 0.0007030594279058278
iteration 34, loss = 0.001836945884861052
iteration 35, loss = 0.0006484515033662319
iteration 36, loss = 0.0006820246344432235
iteration 37, loss = 0.0011038061929866672
iteration 38, loss = 0.0006221347721293569
iteration 39, loss = 0.0006783477729186416
iteration 40, loss = 0.0006860754801891744
iteration 41, loss = 0.0006896898848935962
iteration 42, loss = 0.0006724965642206371
iteration 43, loss = 0.0006647107074968517
iteration 44, loss = 0.0006438119453378022
iteration 45, loss = 0.0006883241585455835
iteration 46, loss = 0.001093605998903513
iteration 47, loss = 0.001771327224560082
iteration 48, loss = 0.0007995963678695261
iteration 49, loss = 0.0008819649228826165
iteration 50, loss = 0.0010752242524176836
iteration 51, loss = 0.00104558898601681
iteration 52, loss = 0.0008731850539334118
iteration 53, loss = 0.0006492445827461779
iteration 54, loss = 0.0006301940302364528
iteration 55, loss = 0.0007132997852750123
iteration 56, loss = 0.001033606706187129
iteration 57, loss = 0.0006915499689057469
iteration 58, loss = 0.0006602279609069228
iteration 59, loss = 0.0018781038234010339
iteration 60, loss = 0.0007007911917753518
iteration 61, loss = 0.0006658242782577872
iteration 62, loss = 0.0005912154447287321
iteration 63, loss = 0.0006208568811416626
iteration 64, loss = 0.0008704390493221581
iteration 65, loss = 0.0006272730533964932
iteration 66, loss = 0.0006732717738486826
iteration 67, loss = 0.000637484947219491
iteration 68, loss = 0.0006114848656579852
iteration 69, loss = 0.0009526957292109728
iteration 70, loss = 0.0007611109176650643
iteration 71, loss = 0.0006839238340035081
iteration 72, loss = 0.0009909233776852489
iteration 73, loss = 0.0006325623253360391
iteration 74, loss = 0.001953943632543087
iteration 75, loss = 0.0006358193350024521
iteration 76, loss = 0.0007802025065757334
iteration 77, loss = 0.0007049095584079623
iteration 78, loss = 0.0007343285833485425
iteration 79, loss = 0.0007183926063589752
iteration 80, loss = 0.0007492320146411657
iteration 81, loss = 0.0007014299044385552
iteration 82, loss = 0.0010141681414097548
iteration 83, loss = 0.000808174314443022
iteration 84, loss = 0.0009350879699923098
iteration 85, loss = 0.0007781253661960363
iteration 86, loss = 0.0008006790303625166
iteration 87, loss = 0.0010817341972142458
iteration 88, loss = 0.0006561325863003731
iteration 89, loss = 0.0006619238993152976
iteration 90, loss = 0.0018238973570987582
iteration 91, loss = 0.0007280786521732807
iteration 92, loss = 0.0006466907216235995
iteration 93, loss = 0.00060926319565624
iteration 94, loss = 0.0007184342248365283
iteration 95, loss = 0.0006556766456924379
iteration 96, loss = 0.0010373250115662813
iteration 97, loss = 0.0007208216702565551
iteration 98, loss = 0.0018534006085246801
iteration 99, loss = 0.0007534627220593393
iteration 100, loss = 0.0006630967254750431
iteration 101, loss = 0.0006509589729830623
iteration 102, loss = 0.0005637030699290335
iteration 103, loss = 0.0007865043007768691
iteration 104, loss = 0.0006778911920264363
iteration 105, loss = 0.000753571221139282
iteration 106, loss = 0.0006471095257438719
iteration 107, loss = 0.0006761549739167094
iteration 108, loss = 0.0018237383337691426
iteration 109, loss = 0.0006381504354067147
iteration 110, loss = 0.0009665741818025708
iteration 111, loss = 0.000807999400421977
iteration 112, loss = 0.0018095527775585651
iteration 113, loss = 0.000987297622486949
iteration 114, loss = 0.0006585483788512647
iteration 115, loss = 0.0008001705282367766
iteration 116, loss = 0.0010731527581810951
iteration 117, loss = 0.0007190819596871734
iteration 118, loss = 0.0006947389920242131
iteration 119, loss = 0.0007906563696451485
iteration 120, loss = 0.0006759255775250494
iteration 121, loss = 0.000648759538307786
iteration 122, loss = 0.0006427978514693677
iteration 123, loss = 0.0009664970566518605
iteration 124, loss = 0.0006658006459474564
iteration 125, loss = 0.000724982819519937
iteration 126, loss = 0.0007944776443764567
iteration 127, loss = 0.000821191759314388
iteration 128, loss = 0.0005980737041682005
iteration 129, loss = 0.0006215309840627015
iteration 130, loss = 0.0006533719715662301
iteration 131, loss = 0.0007236158126033843
iteration 132, loss = 0.0006036472623236477
iteration 133, loss = 0.0006446939660236239
iteration 134, loss = 0.0006171897985041142
iteration 135, loss = 0.00067339395172894
iteration 136, loss = 0.0009315203642472625
iteration 137, loss = 0.0007335380651056767
iteration 138, loss = 0.0006338761304505169
iteration 139, loss = 0.0006114247371442616
iteration 140, loss = 0.0006474885158240795
iteration 141, loss = 0.000669834204018116
iteration 142, loss = 0.000739612034521997
iteration 143, loss = 0.0006202587974257767
iteration 144, loss = 0.0006749694584868848
iteration 145, loss = 0.0006140762707218528
iteration 146, loss = 0.000971702509559691
iteration 147, loss = 0.000578969600610435
iteration 148, loss = 0.0006107638473622501
iteration 149, loss = 0.0007003135397098958
iteration 150, loss = 0.0019164044642820954
iteration 151, loss = 0.0006935565616004169
iteration 152, loss = 0.0006310479366220534
iteration 153, loss = 0.0006416665855795145
iteration 154, loss = 0.0023005958646535873
iteration 155, loss = 0.0006632305448874831
iteration 156, loss = 0.0010952011216431856
iteration 157, loss = 0.0006944940541870892
iteration 158, loss = 0.0010003502247855067
iteration 159, loss = 0.0007362772012129426
iteration 160, loss = 0.0009902024175971746
iteration 161, loss = 0.0006330716423690319
iteration 162, loss = 0.0006284540286287665
iteration 163, loss = 0.000674857001286
iteration 164, loss = 0.0007707254262641072
iteration 165, loss = 0.0006555099971592426
iteration 166, loss = 0.0006789731560274959
iteration 167, loss = 0.0006592568242922425
iteration 168, loss = 0.000680021068546921
iteration 169, loss = 0.0006004824535921216
iteration 170, loss = 0.0006682414677925408
iteration 171, loss = 0.0006064028129912913
iteration 172, loss = 0.0007489864947274327
iteration 173, loss = 0.0006988872191868722
iteration 174, loss = 0.0008933731587603688
iteration 175, loss = 0.001783061190508306
iteration 176, loss = 0.0007273426745086908
iteration 177, loss = 0.0006491161184385419
iteration 178, loss = 0.0006136511801742017
iteration 179, loss = 0.0008230689563788474
iteration 180, loss = 0.0006538052693940699
iteration 181, loss = 0.0006128644454292953
iteration 182, loss = 0.000801477232016623
iteration 183, loss = 0.0006738097290508449
iteration 184, loss = 0.0006218825001269579
iteration 185, loss = 0.0006288409349508584
iteration 186, loss = 0.0007499639177694917
iteration 187, loss = 0.0007283149752765894
iteration 188, loss = 0.0007976272609084845
iteration 189, loss = 0.0009554249700158834
iteration 190, loss = 0.0005515844677574933
iteration 191, loss = 0.0006475574336946011
iteration 192, loss = 0.0006301694666035473
iteration 193, loss = 0.0007069671410135925
iteration 194, loss = 0.0006989757530391216
iteration 195, loss = 0.0006666986155323684
iteration 196, loss = 0.0006482258322648704
iteration 197, loss = 0.0021968642249703407
iteration 198, loss = 0.0006846791366115212
iteration 199, loss = 0.0010858983732759953
iteration 200, loss = 0.0008283930364996195
iteration 201, loss = 0.0007418256718665361
iteration 202, loss = 0.000728206941857934
iteration 203, loss = 0.0009331845212727785
iteration 204, loss = 0.0006852691294625401
iteration 205, loss = 0.0006634644232690334
iteration 206, loss = 0.0006256912602111697
iteration 207, loss = 0.0006765361758880317
iteration 208, loss = 0.0006983454804867506
iteration 209, loss = 0.0019274427322670817
iteration 210, loss = 0.0015112604014575481
iteration 211, loss = 0.0006665235850960016
iteration 212, loss = 0.00070969108492136
iteration 213, loss = 0.0008970738272182643
iteration 214, loss = 0.0007206020527519286
iteration 215, loss = 0.000699411379173398
iteration 216, loss = 0.0006056110141798854
iteration 217, loss = 0.0008772604051046073
iteration 218, loss = 0.0007031015120446682
iteration 219, loss = 0.0006275881314650178
iteration 220, loss = 0.0007107157143764198
iteration 221, loss = 0.0007311587105505168
iteration 222, loss = 0.0006242918316274881
iteration 223, loss = 0.000624592590611428
iteration 224, loss = 0.0006138894823379815
iteration 225, loss = 0.0018088414799422026
iteration 226, loss = 0.0007344721234403551
iteration 227, loss = 0.0007110486039891839
iteration 228, loss = 0.0005661729373969138
iteration 229, loss = 0.0006987546803429723
iteration 230, loss = 0.0006481485324911773
iteration 231, loss = 0.0006674673059023917
iteration 232, loss = 0.0008114760275930166
iteration 233, loss = 0.0005857723299413919
iteration 234, loss = 0.0006952612893655896
iteration 235, loss = 0.0006056076381355524
iteration 236, loss = 0.0009327596053481102
iteration 237, loss = 0.0006471986416727304
iteration 238, loss = 0.0006428512278944254
iteration 239, loss = 0.0006627727416343987
iteration 240, loss = 0.000635212694760412
iteration 241, loss = 0.0006947919609956443
iteration 242, loss = 0.0008759353659115732
iteration 243, loss = 0.0009307311847805977
iteration 244, loss = 0.0006242038798518479
iteration 245, loss = 0.0006090023671276867
iteration 246, loss = 0.0006192506407387555
iteration 247, loss = 0.0006431662477552891
iteration 248, loss = 0.0006953944684937596
iteration 249, loss = 0.0006785839796066284
iteration 250, loss = 0.0006402136641554534
iteration 251, loss = 0.0006743049598298967
iteration 252, loss = 0.0005979047273285687
iteration 253, loss = 0.000709933927282691
iteration 254, loss = 0.0006057217833586037
iteration 255, loss = 0.0007979634101502597
iteration 256, loss = 0.0005958873080089688
iteration 257, loss = 0.0006143641076050699
iteration 258, loss = 0.0007049979758448899
iteration 259, loss = 0.0005980859859846532
iteration 260, loss = 0.0006918511935509741
iteration 261, loss = 0.0007017844473011792
iteration 262, loss = 0.0006510215462185442
iteration 263, loss = 0.0007359752198681235
iteration 264, loss = 0.0006585348746739328
iteration 265, loss = 0.0006099149468354881
iteration 266, loss = 0.0005625299527309835
iteration 267, loss = 0.0006354709039442241
iteration 268, loss = 0.0006795901572331786
iteration 269, loss = 0.0006436155526898801
iteration 270, loss = 0.0006301896646618843
iteration 271, loss = 0.0007539907819591463
iteration 272, loss = 0.0006565882358700037
iteration 273, loss = 0.0006254445761442184
iteration 274, loss = 0.0006882759043946862
iteration 275, loss = 0.0007080593495629728
iteration 276, loss = 0.0006494918488897383
iteration 277, loss = 0.0006368207978084683
iteration 278, loss = 0.0006067896611057222
iteration 279, loss = 0.0006449238862842321
iteration 280, loss = 0.0007066960097290576
iteration 281, loss = 0.0005947900353930891
iteration 282, loss = 0.0006321857799775898
iteration 283, loss = 0.0007018268806859851
iteration 284, loss = 0.0007126849377527833
iteration 285, loss = 0.0006119571626186371
iteration 286, loss = 0.0009317082585766912
iteration 287, loss = 0.0006613287841901183
iteration 288, loss = 0.0006445547915063798
iteration 289, loss = 0.000684772792737931
iteration 290, loss = 0.0005824440158903599
iteration 291, loss = 0.000737195776309818
iteration 292, loss = 0.0007060153875499964
iteration 293, loss = 0.0006522709736600518
iteration 294, loss = 0.0006120353937149048
iteration 295, loss = 0.0006368618342094123
iteration 296, loss = 0.000837726634927094
iteration 297, loss = 0.0006826937315054238
iteration 298, loss = 0.0006700882222503424
iteration 299, loss = 0.0006729812594130635
iteration 0, loss = 0.0006310731405392289
iteration 1, loss = 0.0008940917905420065
iteration 2, loss = 0.0006585855153389275
iteration 3, loss = 0.0006592841236852109
iteration 4, loss = 0.0007462630746886134
iteration 5, loss = 0.0008112104260362685
iteration 6, loss = 0.0006541835027746856
iteration 7, loss = 0.0006569866091012955
iteration 8, loss = 0.0006284591509029269
iteration 9, loss = 0.0010955692268908024
iteration 10, loss = 0.0006582823698408902
iteration 11, loss = 0.0006438411073759198
iteration 12, loss = 0.0005810841685160995
iteration 13, loss = 0.0006282040849328041
iteration 14, loss = 0.0006661592051386833
iteration 15, loss = 0.0011062687262892723
iteration 16, loss = 0.0007245272281579673
iteration 17, loss = 0.0008600972360000014
iteration 18, loss = 0.000670134904794395
iteration 19, loss = 0.0008118237019516528
iteration 20, loss = 0.0006796012748964131
iteration 21, loss = 0.0006581511697731912
iteration 22, loss = 0.0008482743869535625
iteration 23, loss = 0.0010725227184593678
iteration 24, loss = 0.0006394514930434525
iteration 25, loss = 0.0006659715436398983
iteration 26, loss = 0.0006242614472284913
iteration 27, loss = 0.0006081589381210506
iteration 28, loss = 0.000730000261683017
iteration 29, loss = 0.0007388925296254456
iteration 30, loss = 0.0006846520118415356
iteration 31, loss = 0.001760308165103197
iteration 32, loss = 0.0006577422027476132
iteration 33, loss = 0.0006527707446366549
iteration 34, loss = 0.0006362608983181417
iteration 35, loss = 0.0006129950052127242
iteration 36, loss = 0.0007195723592303693
iteration 37, loss = 0.0005677375011146069
iteration 38, loss = 0.0006403025472536683
iteration 39, loss = 0.000677824835292995
iteration 40, loss = 0.0007713030790910125
iteration 41, loss = 0.0006842530565336347
iteration 42, loss = 0.0008618265273980796
iteration 43, loss = 0.0006431110668927431
iteration 44, loss = 0.0006146637024357915
iteration 45, loss = 0.0006425766623578966
iteration 46, loss = 0.0010243735741823912
iteration 47, loss = 0.0005842296523042023
iteration 48, loss = 0.0006153936265036464
iteration 49, loss = 0.0006384174921549857
iteration 50, loss = 0.0007303953170776367
iteration 51, loss = 0.0017637393902987242
iteration 52, loss = 0.0006483594188466668
iteration 53, loss = 0.0006528110825456679
iteration 54, loss = 0.0006232106825336814
iteration 55, loss = 0.0006927044596523046
iteration 56, loss = 0.0007357578724622726
iteration 57, loss = 0.0017385910032317042
iteration 58, loss = 0.0006969036185182631
iteration 59, loss = 0.000650791625957936
iteration 60, loss = 0.0005918512470088899
iteration 61, loss = 0.000666300009470433
iteration 62, loss = 0.000615154393017292
iteration 63, loss = 0.0008570264326408505
iteration 64, loss = 0.0006347900489345193
iteration 65, loss = 0.0005952417850494385
iteration 66, loss = 0.0006828238838352263
iteration 67, loss = 0.0006691508460789919
iteration 68, loss = 0.0005689504323527217
iteration 69, loss = 0.0006606602692045271
iteration 70, loss = 0.0005998769775032997
iteration 71, loss = 0.0007207693997770548
iteration 72, loss = 0.0007126387790776789
iteration 73, loss = 0.0006150869885459542
iteration 74, loss = 0.0010065222159028053
iteration 75, loss = 0.0006165419472381473
iteration 76, loss = 0.0006506959907710552
iteration 77, loss = 0.000565224327147007
iteration 78, loss = 0.0007575926138088107
iteration 79, loss = 0.0007809972739778459
iteration 80, loss = 0.001014445093460381
iteration 81, loss = 0.0006762953707948327
iteration 82, loss = 0.000731366453692317
iteration 83, loss = 0.0006368242902681231
iteration 84, loss = 0.0010207818122580647
iteration 85, loss = 0.0006607145187444985
iteration 86, loss = 0.0006522565963678062
iteration 87, loss = 0.0007318183779716492
iteration 88, loss = 0.0006740496610291302
iteration 89, loss = 0.0009829693008214235
iteration 90, loss = 0.0006527969962917268
iteration 91, loss = 0.0009796430822461843
iteration 92, loss = 0.0006195565219968557
iteration 93, loss = 0.0006674341857433319
iteration 94, loss = 0.001067893230356276
iteration 95, loss = 0.0005951384664513171
iteration 96, loss = 0.0010645410511642694
iteration 97, loss = 0.0006141177727840841
iteration 98, loss = 0.0006012973026372492
iteration 99, loss = 0.0006381943821907043
iteration 100, loss = 0.0006166958482936025
iteration 101, loss = 0.0006180066266097128
iteration 102, loss = 0.0006261271191760898
iteration 103, loss = 0.000783508294261992
iteration 104, loss = 0.0006495976122096181
iteration 105, loss = 0.0005523437866941094
iteration 106, loss = 0.0006913482211530209
iteration 107, loss = 0.0007127810968086123
iteration 108, loss = 0.0006937590660527349
iteration 109, loss = 0.0006274881307035685
iteration 110, loss = 0.0005941850831732154
iteration 111, loss = 0.0020779932383447886
iteration 112, loss = 0.0006110577378422022
iteration 113, loss = 0.0005857755313627422
iteration 114, loss = 0.000590464856941253
iteration 115, loss = 0.000635231495834887
iteration 116, loss = 0.0006742714322172105
iteration 117, loss = 0.0006270224112085998
iteration 118, loss = 0.0006374275544658303
iteration 119, loss = 0.0006065191701054573
iteration 120, loss = 0.0005688928649760783
iteration 121, loss = 0.0006156453164294362
iteration 122, loss = 0.0006127225933596492
iteration 123, loss = 0.0006227436242625117
iteration 124, loss = 0.0010153903858736157
iteration 125, loss = 0.000695670722052455
iteration 126, loss = 0.0005814024480059743
iteration 127, loss = 0.0006048930808901787
iteration 128, loss = 0.0006768772145733237
iteration 129, loss = 0.000984436715953052
iteration 130, loss = 0.0007175636710599065
iteration 131, loss = 0.0006100466707721353
iteration 132, loss = 0.0006864020833745599
iteration 133, loss = 0.00093084666877985
iteration 134, loss = 0.0008476704242639244
iteration 135, loss = 0.0005926804151386023
iteration 136, loss = 0.0006565089570358396
iteration 137, loss = 0.0006010286160744727
iteration 138, loss = 0.0006428909837268293
iteration 139, loss = 0.000835771148558706
iteration 140, loss = 0.00109799113124609
iteration 141, loss = 0.0006154312286525965
iteration 142, loss = 0.0006418925477191806
iteration 143, loss = 0.0007026862003840506
iteration 144, loss = 0.0006264367257244885
iteration 145, loss = 0.0006401027785614133
iteration 146, loss = 0.0007066016551107168
iteration 147, loss = 0.0007162940455600619
iteration 148, loss = 0.000597033416852355
iteration 149, loss = 0.0006878313142806292
iteration 150, loss = 0.0006733122281730175
iteration 151, loss = 0.0007740254513919353
iteration 152, loss = 0.0006896798731759191
iteration 153, loss = 0.0007259694393724203
iteration 154, loss = 0.0005837464123032987
iteration 155, loss = 0.0005937639507465065
iteration 156, loss = 0.0006080605671741068
iteration 157, loss = 0.0006164241349324584
iteration 158, loss = 0.0009909719228744507
iteration 159, loss = 0.0006209035636857152
iteration 160, loss = 0.00062629918102175
iteration 161, loss = 0.0006847340264357626
iteration 162, loss = 0.0005952445790171623
iteration 163, loss = 0.0006633980083279312
iteration 164, loss = 0.0010311224032193422
iteration 165, loss = 0.0007234716322273016
iteration 166, loss = 0.0006726040737703443
iteration 167, loss = 0.0018306655110791326
iteration 168, loss = 0.0006140687619335949
iteration 169, loss = 0.0006760385585948825
iteration 170, loss = 0.000909086549654603
iteration 171, loss = 0.0008387773996219039
iteration 172, loss = 0.0009758751257322729
iteration 173, loss = 0.0009956315625458956
iteration 174, loss = 0.000645094842184335
iteration 175, loss = 0.0005747686373069882
iteration 176, loss = 0.0006337616359815001
iteration 177, loss = 0.0006136804004199803
iteration 178, loss = 0.0006558989989571273
iteration 179, loss = 0.0006301542744040489
iteration 180, loss = 0.0005734842270612717
iteration 181, loss = 0.0005787513800896704
iteration 182, loss = 0.0007078267517499626
iteration 183, loss = 0.0007901588105596602
iteration 184, loss = 0.0006509520462714136
iteration 185, loss = 0.0008213958935812116
iteration 186, loss = 0.0007393962587229908
iteration 187, loss = 0.0005980235291644931
iteration 188, loss = 0.0006674203323200345
iteration 189, loss = 0.0007147487485781312
iteration 190, loss = 0.000660289078950882
iteration 191, loss = 0.0006373198120854795
iteration 192, loss = 0.0017429094295948744
iteration 193, loss = 0.0007064146338962018
iteration 194, loss = 0.0009203021181747317
iteration 195, loss = 0.0006578153697773814
iteration 196, loss = 0.000646517495624721
iteration 197, loss = 0.0006265044794417918
iteration 198, loss = 0.0006688575958833098
iteration 199, loss = 0.0009263090323656797
iteration 200, loss = 0.0006288228905759752
iteration 201, loss = 0.0005437196232378483
iteration 202, loss = 0.0010811381507664919
iteration 203, loss = 0.000765394710469991
iteration 204, loss = 0.0018921776209026575
iteration 205, loss = 0.0006201920914463699
iteration 206, loss = 0.0007752617239020765
iteration 207, loss = 0.0017832450103014708
iteration 208, loss = 0.0007064300589263439
iteration 209, loss = 0.0006576197920367122
iteration 210, loss = 0.0006679084617644548
iteration 211, loss = 0.0006157074822112918
iteration 212, loss = 0.0008284876239486039
iteration 213, loss = 0.000887569971382618
iteration 214, loss = 0.0006684028776362538
iteration 215, loss = 0.0005649234517477453
iteration 216, loss = 0.0006537629524245858
iteration 217, loss = 0.0006603173678740859
iteration 218, loss = 0.0008864628616720438
iteration 219, loss = 0.0006326730363070965
iteration 220, loss = 0.0005921924603171647
iteration 221, loss = 0.0007561746751889586
iteration 222, loss = 0.0006515549030154943
iteration 223, loss = 0.001772774034179747
iteration 224, loss = 0.0006317500956356525
iteration 225, loss = 0.0008624781621620059
iteration 226, loss = 0.0005841713864356279
iteration 227, loss = 0.0005981199210509658
iteration 228, loss = 0.0005951186758466065
iteration 229, loss = 0.0006278017535805702
iteration 230, loss = 0.0017228424549102783
iteration 231, loss = 0.0007899866905063391
iteration 232, loss = 0.0006803428987041116
iteration 233, loss = 0.0009260977967642248
iteration 234, loss = 0.0009265894186682999
iteration 235, loss = 0.0006858399719931185
iteration 236, loss = 0.0007037172326818109
iteration 237, loss = 0.0006128472159616649
iteration 238, loss = 0.0018244439270347357
iteration 239, loss = 0.0006104285130277276
iteration 240, loss = 0.0005454035708680749
iteration 241, loss = 0.0005941365379840136
iteration 242, loss = 0.0008457566145807505
iteration 243, loss = 0.0006770599284209311
iteration 244, loss = 0.0006985151558183134
iteration 245, loss = 0.0006306841969490051
iteration 246, loss = 0.0006173590081743896
iteration 247, loss = 0.0006293935584835708
iteration 248, loss = 0.0006417625118046999
iteration 249, loss = 0.0006924959598109126
iteration 250, loss = 0.0005731125711463392
iteration 251, loss = 0.0005999605637043715
iteration 252, loss = 0.0006910702213644981
iteration 253, loss = 0.0007428859826177359
iteration 254, loss = 0.0005932108033448458
iteration 255, loss = 0.0006235251785255969
iteration 256, loss = 0.000621864281129092
iteration 257, loss = 0.0006559332250617445
iteration 258, loss = 0.0008031497709453106
iteration 259, loss = 0.0006808260804973543
iteration 260, loss = 0.0006422887672670186
iteration 261, loss = 0.0005891351611353457
iteration 262, loss = 0.0007998336805030704
iteration 263, loss = 0.0006155729643069208
iteration 264, loss = 0.0006321667460724711
iteration 265, loss = 0.0006757621886208653
iteration 266, loss = 0.0005613343673758209
iteration 267, loss = 0.0007152057369239628
iteration 268, loss = 0.0020719305612146854
iteration 269, loss = 0.0009622176294215024
iteration 270, loss = 0.0006716601201333106
iteration 271, loss = 0.0006330226897262037
iteration 272, loss = 0.0006611632416024804
iteration 273, loss = 0.0006175526068545878
iteration 274, loss = 0.0006025676266290247
iteration 275, loss = 0.0006088772206567228
iteration 276, loss = 0.001754292519763112
iteration 277, loss = 0.0006422213627956808
iteration 278, loss = 0.0005671376129612327
iteration 279, loss = 0.000697899202350527
iteration 280, loss = 0.0007405435317195952
iteration 281, loss = 0.0006482171011157334
iteration 282, loss = 0.0006208765553310513
iteration 283, loss = 0.0006236655754037201
iteration 284, loss = 0.0005823730025440454
iteration 285, loss = 0.000590164796449244
iteration 286, loss = 0.0006042478489689529
iteration 287, loss = 0.0005858817021362484
iteration 288, loss = 0.0006263352697715163
iteration 289, loss = 0.0009843965526670218
iteration 290, loss = 0.000659427372738719
iteration 291, loss = 0.0006366913439705968
iteration 292, loss = 0.0006449694046750665
iteration 293, loss = 0.000618417514488101
iteration 294, loss = 0.0017551531782373786
iteration 295, loss = 0.001748139038681984
iteration 296, loss = 0.0005543098086491227
iteration 297, loss = 0.0007181859109550714
iteration 298, loss = 0.0005951406783424318
iteration 299, loss = 0.0005548055632971227
iteration 0, loss = 0.0005764530505985022
iteration 1, loss = 0.000573886907659471
iteration 2, loss = 0.0005247901426628232
iteration 3, loss = 0.0006935737328603864
iteration 4, loss = 0.0007358042639680207
iteration 5, loss = 0.0006364905857481062
iteration 6, loss = 0.000638981640804559
iteration 7, loss = 0.0006350446492433548
iteration 8, loss = 0.0006104390486143529
iteration 9, loss = 0.0006170002743601799
iteration 10, loss = 0.0006196564063429832
iteration 11, loss = 0.0006707931170240045
iteration 12, loss = 0.0009903652826324105
iteration 13, loss = 0.0006012215744704008
iteration 14, loss = 0.0007775364792905748
iteration 15, loss = 0.0006125044892542064
iteration 16, loss = 0.0020818135235458612
iteration 17, loss = 0.0007518852362409234
iteration 18, loss = 0.0005984673043712974
iteration 19, loss = 0.00084099848754704
iteration 20, loss = 0.0005680700414814055
iteration 21, loss = 0.000625112559646368
iteration 22, loss = 0.0009460435248911381
iteration 23, loss = 0.0006634658202528954
iteration 24, loss = 0.0006598177133128047
iteration 25, loss = 0.0005930796614848077
iteration 26, loss = 0.0006094506243243814
iteration 27, loss = 0.0006394537631422281
iteration 28, loss = 0.0006626192480325699
iteration 29, loss = 0.0008730487897992134
iteration 30, loss = 0.0006036293925717473
iteration 31, loss = 0.0005912098567932844
iteration 32, loss = 0.0005905592115595937
iteration 33, loss = 0.000589759845752269
iteration 34, loss = 0.0006996021256782115
iteration 35, loss = 0.0006073389668017626
iteration 36, loss = 0.0008277579909190536
iteration 37, loss = 0.0005912845954298973
iteration 38, loss = 0.0006516619469039142
iteration 39, loss = 0.0007169879972934723
iteration 40, loss = 0.0006070815143175423
iteration 41, loss = 0.0008295242441818118
iteration 42, loss = 0.0005969610065221786
iteration 43, loss = 0.0006018982967361808
iteration 44, loss = 0.0009119062451645732
iteration 45, loss = 0.0006366889574564993
iteration 46, loss = 0.0010342536261305213
iteration 47, loss = 0.0006113129202276468
iteration 48, loss = 0.0011971943313255906
iteration 49, loss = 0.000725237769074738
iteration 50, loss = 0.0006701440433971584
iteration 51, loss = 0.0008057667291723192
iteration 52, loss = 0.0005686871008947492
iteration 53, loss = 0.0005851703463122249
iteration 54, loss = 0.0005871967296116054
iteration 55, loss = 0.0006743547855876386
iteration 56, loss = 0.0006662496016360819
iteration 57, loss = 0.0006817023968324065
iteration 58, loss = 0.0008555304375477135
iteration 59, loss = 0.0006034796824678779
iteration 60, loss = 0.000773814448621124
iteration 61, loss = 0.0006783027783967555
iteration 62, loss = 0.0005515646189451218
iteration 63, loss = 0.0006347062881104648
iteration 64, loss = 0.0006044143810868263
iteration 65, loss = 0.0006929106311872602
iteration 66, loss = 0.0006220527575351298
iteration 67, loss = 0.0006994906580075622
iteration 68, loss = 0.000613552809227258
iteration 69, loss = 0.0006808201433159411
iteration 70, loss = 0.0006222788360901177
iteration 71, loss = 0.0008453765185549855
iteration 72, loss = 0.0006685812841169536
iteration 73, loss = 0.0007247197791002691
iteration 74, loss = 0.0005899155512452126
iteration 75, loss = 0.0008359812200069427
iteration 76, loss = 0.0006273335311561823
iteration 77, loss = 0.0008313069702126086
iteration 78, loss = 0.0005892569315619767
iteration 79, loss = 0.000591230287682265
iteration 80, loss = 0.0007106481934897602
iteration 81, loss = 0.0006394089432433248
iteration 82, loss = 0.0017414453905075788
iteration 83, loss = 0.0006132177077233791
iteration 84, loss = 0.0006597538013011217
iteration 85, loss = 0.0005896641523577273
iteration 86, loss = 0.0010092096636071801
iteration 87, loss = 0.0006517110159620643
iteration 88, loss = 0.0007279068813659251
iteration 89, loss = 0.0006328370072878897
iteration 90, loss = 0.0005969306221231818
iteration 91, loss = 0.000603555585257709
iteration 92, loss = 0.0005733667640015483
iteration 93, loss = 0.0006222878582775593
iteration 94, loss = 0.001727904542349279
iteration 95, loss = 0.0006610040436498821
iteration 96, loss = 0.0008262973860837519
iteration 97, loss = 0.0006380986887961626
iteration 98, loss = 0.0006468774517998099
iteration 99, loss = 0.0009918153518810868
iteration 100, loss = 0.0006957196746952832
iteration 101, loss = 0.0006331466720439494
iteration 102, loss = 0.0006945924833416939
iteration 103, loss = 0.0006678351201117039
iteration 104, loss = 0.0008560300339013338
iteration 105, loss = 0.0005926329758949578
iteration 106, loss = 0.0006535170250572264
iteration 107, loss = 0.0005549470079131424
iteration 108, loss = 0.0017134267836809158
iteration 109, loss = 0.0006801875424571335
iteration 110, loss = 0.0006303350673988461
iteration 111, loss = 0.0006610758136957884
iteration 112, loss = 0.0017161468276754022
iteration 113, loss = 0.0005904501886107028
iteration 114, loss = 0.000601963372901082
iteration 115, loss = 0.0006341806147247553
iteration 116, loss = 0.0005820686346851289
iteration 117, loss = 0.0007458104519173503
iteration 118, loss = 0.0006104644271545112
iteration 119, loss = 0.0006075498531572521
iteration 120, loss = 0.0005961972055956721
iteration 121, loss = 0.0006168395630083978
iteration 122, loss = 0.0009027718333527446
iteration 123, loss = 0.0006534036947414279
iteration 124, loss = 0.0006069193477742374
iteration 125, loss = 0.0005306457751430571
iteration 126, loss = 0.0009408238693140447
iteration 127, loss = 0.0006248721620067954
iteration 128, loss = 0.000639762612991035
iteration 129, loss = 0.0005249596433714032
iteration 130, loss = 0.000837417843285948
iteration 131, loss = 0.0006549545796588063
iteration 132, loss = 0.0008380949147976935
iteration 133, loss = 0.0008864483097568154
iteration 134, loss = 0.0006364621222019196
iteration 135, loss = 0.0005635015550069511
iteration 136, loss = 0.0007313517853617668
iteration 137, loss = 0.0006205626996234059
iteration 138, loss = 0.0005436181090772152
iteration 139, loss = 0.0006209475104697049
iteration 140, loss = 0.0007458177860826254
iteration 141, loss = 0.0005721776396967471
iteration 142, loss = 0.0005830544978380203
iteration 143, loss = 0.0010431661503389478
iteration 144, loss = 0.0005968438927084208
iteration 145, loss = 0.000976916984654963
iteration 146, loss = 0.0006931173265911639
iteration 147, loss = 0.0006487808423116803
iteration 148, loss = 0.0005847924039699137
iteration 149, loss = 0.0006056542624719441
iteration 150, loss = 0.0006172257126308978
iteration 151, loss = 0.0017057170625776052
iteration 152, loss = 0.0006045440677553415
iteration 153, loss = 0.0006821490824222565
iteration 154, loss = 0.0005802770610898733
iteration 155, loss = 0.0007019559852778912
iteration 156, loss = 0.000640742713585496
iteration 157, loss = 0.0016653821803629398
iteration 158, loss = 0.0017434193287044764
iteration 159, loss = 0.0006176846218295395
iteration 160, loss = 0.0005679633468389511
iteration 161, loss = 0.0005894217174500227
iteration 162, loss = 0.0009290470043197274
iteration 163, loss = 0.0005791083094663918
iteration 164, loss = 0.0006046227645128965
iteration 165, loss = 0.0005460010725073516
iteration 166, loss = 0.000616355799138546
iteration 167, loss = 0.0006171342683956027
iteration 168, loss = 0.0005825426778756082
iteration 169, loss = 0.0007191565819084644
iteration 170, loss = 0.0005952200153842568
iteration 171, loss = 0.0007536385674029589
iteration 172, loss = 0.0005250383983366191
iteration 173, loss = 0.0010058126645162702
iteration 174, loss = 0.0005793975433334708
iteration 175, loss = 0.0006026185001246631
iteration 176, loss = 0.0017469828017055988
iteration 177, loss = 0.0008707118686288595
iteration 178, loss = 0.0005847288412041962
iteration 179, loss = 0.0006646635010838509
iteration 180, loss = 0.000628129520919174
iteration 181, loss = 0.0006686626002192497
iteration 182, loss = 0.000685407780110836
iteration 183, loss = 0.0006058876751922071
iteration 184, loss = 0.0005690662073902786
iteration 185, loss = 0.0006470050429925323
iteration 186, loss = 0.0006614616140723228
iteration 187, loss = 0.0005861581303179264
iteration 188, loss = 0.001702885259874165
iteration 189, loss = 0.0005830356385558844
iteration 190, loss = 0.0010172255570068955
iteration 191, loss = 0.0008664288325235248
iteration 192, loss = 0.0007319467258639634
iteration 193, loss = 0.0006780405528843403
iteration 194, loss = 0.0007176134968176484
iteration 195, loss = 0.0006824387237429619
iteration 196, loss = 0.0006137749878689647
iteration 197, loss = 0.0006354112992994487
iteration 198, loss = 0.0006588739342987537
iteration 199, loss = 0.0006335272337310016
iteration 200, loss = 0.0006845216848887503
iteration 201, loss = 0.0017436074558645487
iteration 202, loss = 0.0006217537447810173
iteration 203, loss = 0.0006353538483381271
iteration 204, loss = 0.0006776790833100677
iteration 205, loss = 0.0005574800306931138
iteration 206, loss = 0.0008835672051645815
iteration 207, loss = 0.0005997486878186464
iteration 208, loss = 0.0016742993611842394
iteration 209, loss = 0.000711255706846714
iteration 210, loss = 0.0005518237594515085
iteration 211, loss = 0.000576104735955596
iteration 212, loss = 0.0005778510239906609
iteration 213, loss = 0.0005808294517919421
iteration 214, loss = 0.0006865474861115217
iteration 215, loss = 0.0006662805681116879
iteration 216, loss = 0.0006272211903706193
iteration 217, loss = 0.0005513575742952526
iteration 218, loss = 0.0005789082497358322
iteration 219, loss = 0.0005555342650040984
iteration 220, loss = 0.0016625913558527827
iteration 221, loss = 0.0006397154065780342
iteration 222, loss = 0.0009476288105361164
iteration 223, loss = 0.0016693072393536568
iteration 224, loss = 0.0005884338752366602
iteration 225, loss = 0.0006103167543187737
iteration 226, loss = 0.0005313449655659497
iteration 227, loss = 0.0008148809429258108
iteration 228, loss = 0.0009256895282305777
iteration 229, loss = 0.0016815223498269916
iteration 230, loss = 0.0005693765124306083
iteration 231, loss = 0.0006429526838473976
iteration 232, loss = 0.0006741519900970161
iteration 233, loss = 0.0006221256917342544
iteration 234, loss = 0.0006119524477981031
iteration 235, loss = 0.0006147698732092977
iteration 236, loss = 0.0006495268316939473
iteration 237, loss = 0.0006475989939644933
iteration 238, loss = 0.0005601166631095111
iteration 239, loss = 0.0005756877362728119
iteration 240, loss = 0.0006642655935138464
iteration 241, loss = 0.0006203342345543206
iteration 242, loss = 0.000821421854197979
iteration 243, loss = 0.0006595336017198861
iteration 244, loss = 0.0006180162308737636
iteration 245, loss = 0.000556869781576097
iteration 246, loss = 0.0006401865975931287
iteration 247, loss = 0.0005970553029328585
iteration 248, loss = 0.000997787225060165
iteration 249, loss = 0.0006597322644665837
iteration 250, loss = 0.0006559782195836306
iteration 251, loss = 0.0006117023294791579
iteration 252, loss = 0.0005191880045458674
iteration 253, loss = 0.0006398579571396112
iteration 254, loss = 0.0005911365151405334
iteration 255, loss = 0.0008038391824811697
iteration 256, loss = 0.0008949916227720678
iteration 257, loss = 0.0005884364945814013
iteration 258, loss = 0.0009315478382632136
iteration 259, loss = 0.0006286391871981323
iteration 260, loss = 0.0005604822072200477
iteration 261, loss = 0.0007054726593196392
iteration 262, loss = 0.0005789957358501852
iteration 263, loss = 0.0007508713752031326
iteration 264, loss = 0.0009451908408664167
iteration 265, loss = 0.0005997959524393082
iteration 266, loss = 0.0006253222236409783
iteration 267, loss = 0.0006323741981759667
iteration 268, loss = 0.0006317776278592646
iteration 269, loss = 0.0006677822675555944
iteration 270, loss = 0.000628112000413239
iteration 271, loss = 0.0006057744030840695
iteration 272, loss = 0.0006391301285475492
iteration 273, loss = 0.0005577461561188102
iteration 274, loss = 0.0005986589239910245
iteration 275, loss = 0.0006877975538372993
iteration 276, loss = 0.0005764932720921934
iteration 277, loss = 0.0005990879144519567
iteration 278, loss = 0.0006886562914587557
iteration 279, loss = 0.0010272267973050475
iteration 280, loss = 0.0008232010877691209
iteration 281, loss = 0.0006043800385668874
iteration 282, loss = 0.0006598735926672816
iteration 283, loss = 0.0005161987501196563
iteration 284, loss = 0.0006601922796107829
iteration 285, loss = 0.0005570509820245206
iteration 286, loss = 0.000610637478530407
iteration 287, loss = 0.0007528002606704831
iteration 288, loss = 0.0006024547619745135
iteration 289, loss = 0.0006065524648874998
iteration 290, loss = 0.0005570233333855867
iteration 291, loss = 0.0007061803480610251
iteration 292, loss = 0.0006689729634672403
iteration 293, loss = 0.0006662108935415745
iteration 294, loss = 0.0005956570967100561
iteration 295, loss = 0.0006449065404012799
iteration 296, loss = 0.0006681033410131931
iteration 297, loss = 0.0006079144077375531
iteration 298, loss = 0.0005802724044770002
iteration 299, loss = 0.0009428389021195471
iteration 0, loss = 0.0006112180999480188
iteration 1, loss = 0.0007790126255713403
iteration 2, loss = 0.0005883480771444738
iteration 3, loss = 0.0005462667904794216
iteration 4, loss = 0.0006052830722182989
iteration 5, loss = 0.0005604479811154306
iteration 6, loss = 0.0005811166483908892
iteration 7, loss = 0.0005832908791489899
iteration 8, loss = 0.0006043862085789442
iteration 9, loss = 0.0006866849726065993
iteration 10, loss = 0.0005600099684670568
iteration 11, loss = 0.0005944081931374967
iteration 12, loss = 0.000571579672396183
iteration 13, loss = 0.0005851482274010777
iteration 14, loss = 0.0006288866279646754
iteration 15, loss = 0.0005600890726782382
iteration 16, loss = 0.000518831773661077
iteration 17, loss = 0.000684073253069073
iteration 18, loss = 0.0005844990955665708
iteration 19, loss = 0.0005928610917180777
iteration 20, loss = 0.0005862737307325006
iteration 21, loss = 0.0005652934778481722
iteration 22, loss = 0.0007155629573389888
iteration 23, loss = 0.0006362305721268058
iteration 24, loss = 0.0005643222248181701
iteration 25, loss = 0.0008971977513283491
iteration 26, loss = 0.0005855069030076265
iteration 27, loss = 0.0005950451013632119
iteration 28, loss = 0.0005761023494414985
iteration 29, loss = 0.0018407813040539622
iteration 30, loss = 0.0006755569484084845
iteration 31, loss = 0.0005768774426542222
iteration 32, loss = 0.00069197709672153
iteration 33, loss = 0.000696038652677089
iteration 34, loss = 0.0005480425315909088
iteration 35, loss = 0.0005987569456920028
iteration 36, loss = 0.0006812512874603271
iteration 37, loss = 0.0006116348085924983
iteration 38, loss = 0.0005993529921397567
iteration 39, loss = 0.0005967940669506788
iteration 40, loss = 0.0017428891733288765
iteration 41, loss = 0.0009604363003745675
iteration 42, loss = 0.0006150247063487768
iteration 43, loss = 0.0005886174621991813
iteration 44, loss = 0.0005589401698671281
iteration 45, loss = 0.0005907548475079238
iteration 46, loss = 0.0012429532362148166
iteration 47, loss = 0.0017447307473048568
iteration 48, loss = 0.0005599839496426284
iteration 49, loss = 0.0006863490561954677
iteration 50, loss = 0.0006284671253524721
iteration 51, loss = 0.0005999385030008852
iteration 52, loss = 0.0006189049454405904
iteration 53, loss = 0.0006236648769117892
iteration 54, loss = 0.0005593265523202717
iteration 55, loss = 0.0006431541405618191
iteration 56, loss = 0.0005699809407815337
iteration 57, loss = 0.0009340188698843122
iteration 58, loss = 0.0006688637658953667
iteration 59, loss = 0.0006136717274785042
iteration 60, loss = 0.0005828022258356214
iteration 61, loss = 0.0006276631029322743
iteration 62, loss = 0.0005542442668229342
iteration 63, loss = 0.0006112128030508757
iteration 64, loss = 0.0006334315985441208
iteration 65, loss = 0.0007698662229813635
iteration 66, loss = 0.0006133653223514557
iteration 67, loss = 0.0005834282492287457
iteration 68, loss = 0.0016509449342265725
iteration 69, loss = 0.0005335263558663428
iteration 70, loss = 0.0005693731363862753
iteration 71, loss = 0.0009038224816322327
iteration 72, loss = 0.0005858309450559318
iteration 73, loss = 0.0005974541418254375
iteration 74, loss = 0.0016241277335211635
iteration 75, loss = 0.0016741181025281549
iteration 76, loss = 0.000557512219529599
iteration 77, loss = 0.0005467056180350482
iteration 78, loss = 0.0005176828126423061
iteration 79, loss = 0.0007536759949289262
iteration 80, loss = 0.0005944145377725363
iteration 81, loss = 0.0016464681830257177
iteration 82, loss = 0.0006048382492735982
iteration 83, loss = 0.000610311224590987
iteration 84, loss = 0.0007583905826322734
iteration 85, loss = 0.0007079487550072372
iteration 86, loss = 0.0005733860889449716
iteration 87, loss = 0.0006979300524108112
iteration 88, loss = 0.0005547511391341686
iteration 89, loss = 0.0006717761280015111
iteration 90, loss = 0.0006572643178515136
iteration 91, loss = 0.0006039543077349663
iteration 92, loss = 0.0006250118021853268
iteration 93, loss = 0.0006363596767187119
iteration 94, loss = 0.0009820432169362903
iteration 95, loss = 0.000656876596622169
iteration 96, loss = 0.0009309846209362149
iteration 97, loss = 0.0008487902814522386
iteration 98, loss = 0.0007365625351667404
iteration 99, loss = 0.0005521070561371744
iteration 100, loss = 0.0005244605126790702
iteration 101, loss = 0.001077261520549655
iteration 102, loss = 0.0009237737976945937
iteration 103, loss = 0.0006405614549294114
iteration 104, loss = 0.0005988374468870461
iteration 105, loss = 0.000617446843534708
iteration 106, loss = 0.0006127938977442682
iteration 107, loss = 0.0005417727516032755
iteration 108, loss = 0.0005760934436693788
iteration 109, loss = 0.0006074340199120343
iteration 110, loss = 0.0006430239882320166
iteration 111, loss = 0.0008567396434955299
iteration 112, loss = 0.0006645756075158715
iteration 113, loss = 0.0005915506044402719
iteration 114, loss = 0.0006004745373502374
iteration 115, loss = 0.0005976185202598572
iteration 116, loss = 0.0005640835152007639
iteration 117, loss = 0.0005300744087435305
iteration 118, loss = 0.0007350303931161761
iteration 119, loss = 0.0005470294854603708
iteration 120, loss = 0.0004974506446160376
iteration 121, loss = 0.0006075454875826836
iteration 122, loss = 0.0006128225941210985
iteration 123, loss = 0.0008142726728692651
iteration 124, loss = 0.0006335402140393853
iteration 125, loss = 0.0005802321247756481
iteration 126, loss = 0.0006292985053732991
iteration 127, loss = 0.0005901689291931689
iteration 128, loss = 0.00054842175450176
iteration 129, loss = 0.000586833106353879
iteration 130, loss = 0.000669486413244158
iteration 131, loss = 0.0005921881529502571
iteration 132, loss = 0.0016807956853881478
iteration 133, loss = 0.0006749093299731612
iteration 134, loss = 0.0007631403859704733
iteration 135, loss = 0.0009354717913083732
iteration 136, loss = 0.0005939815891906619
iteration 137, loss = 0.0005230201641097665
iteration 138, loss = 0.0007173042977228761
iteration 139, loss = 0.0007594244088977575
iteration 140, loss = 0.0007619336829520762
iteration 141, loss = 0.0005619582952931523
iteration 142, loss = 0.0008468961459584534
iteration 143, loss = 0.0005380213842727244
iteration 144, loss = 0.0005467879236675799
iteration 145, loss = 0.000578459061216563
iteration 146, loss = 0.000629634247161448
iteration 147, loss = 0.0005443713162094355
iteration 148, loss = 0.0006489461520686746
iteration 149, loss = 0.0006496466230601072
iteration 150, loss = 0.000602987187448889
iteration 151, loss = 0.0005468860035762191
iteration 152, loss = 0.0007097472553141415
iteration 153, loss = 0.0005718466709367931
iteration 154, loss = 0.0005598985007964075
iteration 155, loss = 0.0005751611315645278
iteration 156, loss = 0.0008392207673750818
iteration 157, loss = 0.0005751888966187835
iteration 158, loss = 0.0005511446506716311
iteration 159, loss = 0.000675632618367672
iteration 160, loss = 0.0005568157648667693
iteration 161, loss = 0.0005930681945756078
iteration 162, loss = 0.0005627063801512122
iteration 163, loss = 0.0005529717891477048
iteration 164, loss = 0.0005009331507608294
iteration 165, loss = 0.0006062783650122583
iteration 166, loss = 0.0005608909414149821
iteration 167, loss = 0.0007094349712133408
iteration 168, loss = 0.0006218363414518535
iteration 169, loss = 0.0006499647279269993
iteration 170, loss = 0.0005660415627062321
iteration 171, loss = 0.0006822851719334722
iteration 172, loss = 0.0005781561485491693
iteration 173, loss = 0.0005264257197268307
iteration 174, loss = 0.000564160174690187
iteration 175, loss = 0.0008088111644610763
iteration 176, loss = 0.000625493994448334
iteration 177, loss = 0.0006073162658140063
iteration 178, loss = 0.0007460369379259646
iteration 179, loss = 0.0006438104319386184
iteration 180, loss = 0.0006560080219060183
iteration 181, loss = 0.0005655406275764108
iteration 182, loss = 0.0006731087923981249
iteration 183, loss = 0.0006058301078155637
iteration 184, loss = 0.0006248086574487388
iteration 185, loss = 0.0006401597056537867
iteration 186, loss = 0.000661697587929666
iteration 187, loss = 0.0006291676545515656
iteration 188, loss = 0.000573483994230628
iteration 189, loss = 0.0005838726065121591
iteration 190, loss = 0.0005473163328133523
iteration 191, loss = 0.0019798814319074154
iteration 192, loss = 0.0005489460309036076
iteration 193, loss = 0.0007562401005998254
iteration 194, loss = 0.0005653635016642511
iteration 195, loss = 0.0009628230473026633
iteration 196, loss = 0.0005692366394214332
iteration 197, loss = 0.0006824600277468562
iteration 198, loss = 0.0016607228899374604
iteration 199, loss = 0.0005680082831531763
iteration 200, loss = 0.000528898905031383
iteration 201, loss = 0.0007194611825980246
iteration 202, loss = 0.0008768432307988405
iteration 203, loss = 0.0005955994129180908
iteration 204, loss = 0.0005703474744223058
iteration 205, loss = 0.0009350958280265331
iteration 206, loss = 0.0005450027529150248
iteration 207, loss = 0.0005871712346561253
iteration 208, loss = 0.0006262235692702234
iteration 209, loss = 0.0016357755521312356
iteration 210, loss = 0.0005707737873308361
iteration 211, loss = 0.0006508836522698402
iteration 212, loss = 0.0006550952675752342
iteration 213, loss = 0.0008673321572132409
iteration 214, loss = 0.0007539928192272782
iteration 215, loss = 0.0005598962889052927
iteration 216, loss = 0.0010279353009536862
iteration 217, loss = 0.0008547027828171849
iteration 218, loss = 0.0007255956879816949
iteration 219, loss = 0.0005963525036349893
iteration 220, loss = 0.0007153827464208007
iteration 221, loss = 0.000662644044496119
iteration 222, loss = 0.0005855917115695775
iteration 223, loss = 0.000636188022326678
iteration 224, loss = 0.0005607539787888527
iteration 225, loss = 0.0005511629278771579
iteration 226, loss = 0.0005380639340728521
iteration 227, loss = 0.0016645098803564906
iteration 228, loss = 0.0009136853623203933
iteration 229, loss = 0.0005954426596872509
iteration 230, loss = 0.0006938931182958186
iteration 231, loss = 0.0006255548214539886
iteration 232, loss = 0.0007169559830799699
iteration 233, loss = 0.0008313466096296906
iteration 234, loss = 0.0005060715484432876
iteration 235, loss = 0.0005938150570727885
iteration 236, loss = 0.0005981663125567138
iteration 237, loss = 0.0005280943587422371
iteration 238, loss = 0.0005934506189078093
iteration 239, loss = 0.0016614166088402271
iteration 240, loss = 0.0006502019241452217
iteration 241, loss = 0.0006079245358705521
iteration 242, loss = 0.0005505365552380681
iteration 243, loss = 0.0008024359703995287
iteration 244, loss = 0.0005832413444295526
iteration 245, loss = 0.0006484029581770301
iteration 246, loss = 0.00056385004427284
iteration 247, loss = 0.0006552101112902164
iteration 248, loss = 0.0006837566033937037
iteration 249, loss = 0.0010635099606588483
iteration 250, loss = 0.0008401029044762254
iteration 251, loss = 0.0005985380266793072
iteration 252, loss = 0.0005801313673146069
iteration 253, loss = 0.0005723553476855159
iteration 254, loss = 0.0009999425383284688
iteration 255, loss = 0.0006341615226119757
iteration 256, loss = 0.0005407463759183884
iteration 257, loss = 0.0006084999768063426
iteration 258, loss = 0.0006019629072397947
iteration 259, loss = 0.0006618017796427011
iteration 260, loss = 0.000556976767256856
iteration 261, loss = 0.0007008382235653698
iteration 262, loss = 0.00062188581796363
iteration 263, loss = 0.0005603366298601031
iteration 264, loss = 0.0020064355339854956
iteration 265, loss = 0.0005614038673229516
iteration 266, loss = 0.0006240628426894546
iteration 267, loss = 0.0005635591805912554
iteration 268, loss = 0.0005512832431122661
iteration 269, loss = 0.0005376472254283726
iteration 270, loss = 0.0005943365395069122
iteration 271, loss = 0.0005319933407008648
iteration 272, loss = 0.0006852676160633564
iteration 273, loss = 0.0008472923655062914
iteration 274, loss = 0.0005109734483994544
iteration 275, loss = 0.0005758074694313109
iteration 276, loss = 0.0009267301065847278
iteration 277, loss = 0.0016574531327933073
iteration 278, loss = 0.0009341646218672395
iteration 279, loss = 0.0006062002503313124
iteration 280, loss = 0.0006293671322055161
iteration 281, loss = 0.000533670128788799
iteration 282, loss = 0.0008292017737403512
iteration 283, loss = 0.0007850309484638274
iteration 284, loss = 0.0005556248361244798
iteration 285, loss = 0.0005575902760028839
iteration 286, loss = 0.0008961496059782803
iteration 287, loss = 0.0005431720637716353
iteration 288, loss = 0.000546585361007601
iteration 289, loss = 0.0006487591890618205
iteration 290, loss = 0.0006314568454399705
iteration 291, loss = 0.0005387159762904048
iteration 292, loss = 0.0006176712922751904
iteration 293, loss = 0.0005951849743723869
iteration 294, loss = 0.0006173698930069804
iteration 295, loss = 0.000830893637612462
iteration 296, loss = 0.000656551739666611
iteration 297, loss = 0.0005463392590172589
iteration 298, loss = 0.0006174187292344868
iteration 299, loss = 0.0005116488318890333
iteration 0, loss = 0.0005223745829425752
iteration 1, loss = 0.0005241227918304503
iteration 2, loss = 0.000622021034359932
iteration 3, loss = 0.0007078870548866689
iteration 4, loss = 0.0006168361287564039
iteration 5, loss = 0.0005621880991384387
iteration 6, loss = 0.0006258704815991223
iteration 7, loss = 0.0006646440597251058
iteration 8, loss = 0.000630203983746469
iteration 9, loss = 0.0005399383371695876
iteration 10, loss = 0.0005598752177320421
iteration 11, loss = 0.0005554190138354897
iteration 12, loss = 0.0007785906782373786
iteration 13, loss = 0.0005851459573023021
iteration 14, loss = 0.0005323558580130339
iteration 15, loss = 0.0005960401031188667
iteration 16, loss = 0.0016836818540468812
iteration 17, loss = 0.0008663792978040874
iteration 18, loss = 0.0008570025092922151
iteration 19, loss = 0.0005892164190299809
iteration 20, loss = 0.0006121862679719925
iteration 21, loss = 0.0006409310735762119
iteration 22, loss = 0.000519528693985194
iteration 23, loss = 0.0008834036416374147
iteration 24, loss = 0.0005896897055208683
iteration 25, loss = 0.0006520497845485806
iteration 26, loss = 0.0007482373621314764
iteration 27, loss = 0.0006104135536588728
iteration 28, loss = 0.0005096083041280508
iteration 29, loss = 0.0005918194074183702
iteration 30, loss = 0.000693632522597909
iteration 31, loss = 0.0005868510343134403
iteration 32, loss = 0.0009778309613466263
iteration 33, loss = 0.0005616978160105646
iteration 34, loss = 0.0006201306823641062
iteration 35, loss = 0.0008058554376475513
iteration 36, loss = 0.0006185540114529431
iteration 37, loss = 0.0006541056791320443
iteration 38, loss = 0.0005736059974879026
iteration 39, loss = 0.0005884032580070198
iteration 40, loss = 0.000606635061558336
iteration 41, loss = 0.0005559304263442755
iteration 42, loss = 0.0006035041296854615
iteration 43, loss = 0.000577446655370295
iteration 44, loss = 0.0005766861140727997
iteration 45, loss = 0.0006220047362148762
iteration 46, loss = 0.0016221663681790233
iteration 47, loss = 0.0006682300590910017
iteration 48, loss = 0.0015855084639042616
iteration 49, loss = 0.0005742448847740889
iteration 50, loss = 0.0009950052481144667
iteration 51, loss = 0.0006311120232567191
iteration 52, loss = 0.000536567997187376
iteration 53, loss = 0.00054762332001701
iteration 54, loss = 0.0006100407335907221
iteration 55, loss = 0.0008964596199803054
iteration 56, loss = 0.0006729955784976482
iteration 57, loss = 0.0009255752665922046
iteration 58, loss = 0.0005978542030788958
iteration 59, loss = 0.0006896793493069708
iteration 60, loss = 0.000737101654522121
iteration 61, loss = 0.0006172145367600024
iteration 62, loss = 0.0007973057799972594
iteration 63, loss = 0.0006637184997089207
iteration 64, loss = 0.0006090544047765434
iteration 65, loss = 0.0005433654296211898
iteration 66, loss = 0.0006529673701152205
iteration 67, loss = 0.0006471769884228706
iteration 68, loss = 0.0005696390871889889
iteration 69, loss = 0.0009947948856279254
iteration 70, loss = 0.0005498076789081097
iteration 71, loss = 0.0005231386749073863
iteration 72, loss = 0.0005485037108883262
iteration 73, loss = 0.0007579059456475079
iteration 74, loss = 0.0005430499440990388
iteration 75, loss = 0.0005953979562036693
iteration 76, loss = 0.0006451485678553581
iteration 77, loss = 0.0006295257480815053
iteration 78, loss = 0.0006467364728450775
iteration 79, loss = 0.000527841504663229
iteration 80, loss = 0.0009356556693091989
iteration 81, loss = 0.0006556020234711468
iteration 82, loss = 0.0006096637807786465
iteration 83, loss = 0.0010272624203935266
iteration 84, loss = 0.0005693897255696356
iteration 85, loss = 0.0005774306482635438
iteration 86, loss = 0.0005488822935149074
iteration 87, loss = 0.0016056735767051578
iteration 88, loss = 0.0005318804760463536
iteration 89, loss = 0.0005357514601200819
iteration 90, loss = 0.0005304248770698905
iteration 91, loss = 0.0016035627340897918
iteration 92, loss = 0.0006170012638904154
iteration 93, loss = 0.0005173927638679743
iteration 94, loss = 0.001604011980816722
iteration 95, loss = 0.0005962445284239948
iteration 96, loss = 0.0005431558820419014
iteration 97, loss = 0.0005424986011348665
iteration 98, loss = 0.0005633393302559853
iteration 99, loss = 0.0007794383564032614
iteration 100, loss = 0.0007327052881009877
iteration 101, loss = 0.0005446394789032638
iteration 102, loss = 0.0005361191579140723
iteration 103, loss = 0.0005535992095246911
iteration 104, loss = 0.0006009266362525523
iteration 105, loss = 0.0007081745425239205
iteration 106, loss = 0.0015909593785181642
iteration 107, loss = 0.000621634884737432
iteration 108, loss = 0.0006251871236599982
iteration 109, loss = 0.0005699751782231033
iteration 110, loss = 0.0005575775285251439
iteration 111, loss = 0.0006724191480316222
iteration 112, loss = 0.0005522697465494275
iteration 113, loss = 0.0007199050160124898
iteration 114, loss = 0.0005931755295023322
iteration 115, loss = 0.0005352157168090343
iteration 116, loss = 0.0005614957772195339
iteration 117, loss = 0.0005510016926564276
iteration 118, loss = 0.0006049119401723146
iteration 119, loss = 0.0005686549702659249
iteration 120, loss = 0.0005566403269767761
iteration 121, loss = 0.0005408310098573565
iteration 122, loss = 0.0005789207643829286
iteration 123, loss = 0.0006210156716406345
iteration 124, loss = 0.0005357541376724839
iteration 125, loss = 0.0016244972357526422
iteration 126, loss = 0.0005513582727871835
iteration 127, loss = 0.0006062702741473913
iteration 128, loss = 0.0015573159325867891
iteration 129, loss = 0.0005915415240451694
iteration 130, loss = 0.0005765074747614563
iteration 131, loss = 0.0005825700354762375
iteration 132, loss = 0.0006115681026130915
iteration 133, loss = 0.0005897122318856418
iteration 134, loss = 0.0006118567544035614
iteration 135, loss = 0.0006328194285742939
iteration 136, loss = 0.0008328197873197496
iteration 137, loss = 0.0005772477015852928
iteration 138, loss = 0.0006722576799802482
iteration 139, loss = 0.0005879673408344388
iteration 140, loss = 0.0005983107257634401
iteration 141, loss = 0.0015894831158220768
iteration 142, loss = 0.0005562285077758133
iteration 143, loss = 0.0005591344670392573
iteration 144, loss = 0.0006344304420053959
iteration 145, loss = 0.000612379633821547
iteration 146, loss = 0.0008264333009719849
iteration 147, loss = 0.0005614429828710854
iteration 148, loss = 0.0005121739814057946
iteration 149, loss = 0.0005322543438524008
iteration 150, loss = 0.0007755329133942723
iteration 151, loss = 0.0006281125242821872
iteration 152, loss = 0.0005496139056049287
iteration 153, loss = 0.0005157264531590044
iteration 154, loss = 0.0008884325507096946
iteration 155, loss = 0.0006330284522846341
iteration 156, loss = 0.0006067956564947963
iteration 157, loss = 0.000529122946318239
iteration 158, loss = 0.0005098171532154083
iteration 159, loss = 0.0006549269892275333
iteration 160, loss = 0.0008255467982962728
iteration 161, loss = 0.0005838574725203216
iteration 162, loss = 0.0005515797529369593
iteration 163, loss = 0.0007096108165569603
iteration 164, loss = 0.0005607958883047104
iteration 165, loss = 0.0005714429426006973
iteration 166, loss = 0.0005186744383536279
iteration 167, loss = 0.0008788409177213907
iteration 168, loss = 0.0005897892406210303
iteration 169, loss = 0.0005081737763248384
iteration 170, loss = 0.0005369962891563773
iteration 171, loss = 0.00067299441434443
iteration 172, loss = 0.0005056051886640489
iteration 173, loss = 0.000608663831371814
iteration 174, loss = 0.0006982843624427915
iteration 175, loss = 0.0005706505035050213
iteration 176, loss = 0.0005490218754857779
iteration 177, loss = 0.0005535499076358974
iteration 178, loss = 0.0005662605399265885
iteration 179, loss = 0.0008584477473050356
iteration 180, loss = 0.0008168305503204465
iteration 181, loss = 0.0006765212747268379
iteration 182, loss = 0.0009369596373289824
iteration 183, loss = 0.0005499512772075832
iteration 184, loss = 0.000677869888022542
iteration 185, loss = 0.0006014942773617804
iteration 186, loss = 0.0006357485544867814
iteration 187, loss = 0.0005503701977431774
iteration 188, loss = 0.0005534543888643384
iteration 189, loss = 0.0007939147762954235
iteration 190, loss = 0.0008344310335814953
iteration 191, loss = 0.0008812139858491719
iteration 192, loss = 0.0004624773282557726
iteration 193, loss = 0.0006581119378097355
iteration 194, loss = 0.0006390760536305606
iteration 195, loss = 0.0005197848659008741
iteration 196, loss = 0.0005451280740089715
iteration 197, loss = 0.0005272638518363237
iteration 198, loss = 0.0005578479031100869
iteration 199, loss = 0.0006646823021583259
iteration 200, loss = 0.000863704364746809
iteration 201, loss = 0.000761082861572504
iteration 202, loss = 0.000602878222707659
iteration 203, loss = 0.0015708990395069122
iteration 204, loss = 0.0008803166565485299
iteration 205, loss = 0.0005972187500447035
iteration 206, loss = 0.0005101539427414536
iteration 207, loss = 0.0005341671057976782
iteration 208, loss = 0.0005901796976104379
iteration 209, loss = 0.0005112184444442391
iteration 210, loss = 0.001646942226216197
iteration 211, loss = 0.0005051237531006336
iteration 212, loss = 0.0006121597834862769
iteration 213, loss = 0.0006535239517688751
iteration 214, loss = 0.0005280646146275103
iteration 215, loss = 0.000552867422811687
iteration 216, loss = 0.0005351909203454852
iteration 217, loss = 0.0005876652430742979
iteration 218, loss = 0.0005465694703161716
iteration 219, loss = 0.0006026910850778222
iteration 220, loss = 0.0006290171295404434
iteration 221, loss = 0.0005789607530459762
iteration 222, loss = 0.0006183265359140933
iteration 223, loss = 0.0005994935636408627
iteration 224, loss = 0.0005468351300805807
iteration 225, loss = 0.0005412629107013345
iteration 226, loss = 0.00055908557260409
iteration 227, loss = 0.0006215657922439277
iteration 228, loss = 0.0005535960663110018
iteration 229, loss = 0.0005334891029633582
iteration 230, loss = 0.00059453712310642
iteration 231, loss = 0.0005527873290702701
iteration 232, loss = 0.0005436096689663827
iteration 233, loss = 0.0005700477631762624
iteration 234, loss = 0.000624764827080071
iteration 235, loss = 0.0005177080747671425
iteration 236, loss = 0.0005694893188774586
iteration 237, loss = 0.0005813681054860353
iteration 238, loss = 0.0005064773722551763
iteration 239, loss = 0.00091319321654737
iteration 240, loss = 0.0005266844527795911
iteration 241, loss = 0.0005413295584730804
iteration 242, loss = 0.0009480591979809105
iteration 243, loss = 0.0005833570612594485
iteration 244, loss = 0.0005648997030220926
iteration 245, loss = 0.0006271769525483251
iteration 246, loss = 0.0005540910642594099
iteration 247, loss = 0.0005646403878927231
iteration 248, loss = 0.001575849368236959
iteration 249, loss = 0.0004888577968813479
iteration 250, loss = 0.000577625643927604
iteration 251, loss = 0.0006317115039564669
iteration 252, loss = 0.0005699764005839825
iteration 253, loss = 0.000981911551207304
iteration 254, loss = 0.000614709802903235
iteration 255, loss = 0.000538678839802742
iteration 256, loss = 0.0006024344475008547
iteration 257, loss = 0.0006178986513987184
iteration 258, loss = 0.0006787729216739535
iteration 259, loss = 0.0005816756747663021
iteration 260, loss = 0.000534824444912374
iteration 261, loss = 0.0008776458562351763
iteration 262, loss = 0.0005372316227294505
iteration 263, loss = 0.0015353591879829764
iteration 264, loss = 0.0006444731261581182
iteration 265, loss = 0.0006251785671338439
iteration 266, loss = 0.0007297745905816555
iteration 267, loss = 0.0005342827062122524
iteration 268, loss = 0.0005203705513849854
iteration 269, loss = 0.0005717042367905378
iteration 270, loss = 0.0005498407408595085
iteration 271, loss = 0.0005500991828739643
iteration 272, loss = 0.0005505841108970344
iteration 273, loss = 0.0008151943329721689
iteration 274, loss = 0.0008623599424026906
iteration 275, loss = 0.0010402218904346228
iteration 276, loss = 0.0010041502537205815
iteration 277, loss = 0.0005722979549318552
iteration 278, loss = 0.000521276262588799
iteration 279, loss = 0.0005638661095872521
iteration 280, loss = 0.0005694774445146322
iteration 281, loss = 0.0006323755369521677
iteration 282, loss = 0.0006783561548218131
iteration 283, loss = 0.0005574371316470206
iteration 284, loss = 0.0005288248066790402
iteration 285, loss = 0.0006431982037611306
iteration 286, loss = 0.001598755712620914
iteration 287, loss = 0.0006019971915520728
iteration 288, loss = 0.000592028780374676
iteration 289, loss = 0.0008945210720412433
iteration 290, loss = 0.0005151784280315042
iteration 291, loss = 0.0006028581992723048
iteration 292, loss = 0.0005458531668409705
iteration 293, loss = 0.00058832234935835
iteration 294, loss = 0.0005692778504453599
iteration 295, loss = 0.0005433833575807512
iteration 296, loss = 0.0008089514449238777
iteration 297, loss = 0.0005548657500185072
iteration 298, loss = 0.0006598911713808775
iteration 299, loss = 0.0005549191264435649
iteration 0, loss = 0.000570580072235316
iteration 1, loss = 0.0005387838464230299
iteration 2, loss = 0.0005662408075295389
iteration 3, loss = 0.0005639682058244944
iteration 4, loss = 0.0005714355502277613
iteration 5, loss = 0.0015513448743149638
iteration 6, loss = 0.0005945979501120746
iteration 7, loss = 0.0006039447616785765
iteration 8, loss = 0.0005397269269451499
iteration 9, loss = 0.0006409435300156474
iteration 10, loss = 0.0005256460863165557
iteration 11, loss = 0.000770852027926594
iteration 12, loss = 0.0006003618473187089
iteration 13, loss = 0.0019402893958613276
iteration 14, loss = 0.0005585698527283967
iteration 15, loss = 0.0006058751605451107
iteration 16, loss = 0.000559076142963022
iteration 17, loss = 0.0005442898836918175
iteration 18, loss = 0.0009271130547858775
iteration 19, loss = 0.0006297443178482354
iteration 20, loss = 0.0007483507506549358
iteration 21, loss = 0.0009075284469872713
iteration 22, loss = 0.0009224483510479331
iteration 23, loss = 0.000518174609169364
iteration 24, loss = 0.0006507402285933495
iteration 25, loss = 0.0005420758971013129
iteration 26, loss = 0.0006137520540505648
iteration 27, loss = 0.0006114835850894451
iteration 28, loss = 0.0006147415842860937
iteration 29, loss = 0.0005885027348995209
iteration 30, loss = 0.0015286073321476579
iteration 31, loss = 0.0005344563978724182
iteration 32, loss = 0.0005633599357679486
iteration 33, loss = 0.0007730114157311618
iteration 34, loss = 0.0004913327866233885
iteration 35, loss = 0.0006113435956649482
iteration 36, loss = 0.0005312616704031825
iteration 37, loss = 0.0007154319901019335
iteration 38, loss = 0.0005778653430752456
iteration 39, loss = 0.0008647674694657326
iteration 40, loss = 0.0007615797803737223
iteration 41, loss = 0.0007721391157247126
iteration 42, loss = 0.0005633739638142288
iteration 43, loss = 0.000533308950252831
iteration 44, loss = 0.0007192869088612497
iteration 45, loss = 0.0005594244576059282
iteration 46, loss = 0.0008404779364354908
iteration 47, loss = 0.0006103365449234843
iteration 48, loss = 0.000590243493206799
iteration 49, loss = 0.0009632325381971896
iteration 50, loss = 0.0004950573202222586
iteration 51, loss = 0.0005504609434865415
iteration 52, loss = 0.0005723984213545918
iteration 53, loss = 0.0005342357326298952
iteration 54, loss = 0.0005203292239457369
iteration 55, loss = 0.0005700079491361976
iteration 56, loss = 0.0005398018984124064
iteration 57, loss = 0.0005424570990726352
iteration 58, loss = 0.0005450322641991079
iteration 59, loss = 0.0006285965791903436
iteration 60, loss = 0.0006596248131245375
iteration 61, loss = 0.00048672076081857085
iteration 62, loss = 0.000729606777895242
iteration 63, loss = 0.0006246409611776471
iteration 64, loss = 0.0005586611805483699
iteration 65, loss = 0.0006711955065838993
iteration 66, loss = 0.0006605560192838311
iteration 67, loss = 0.0007606291328556836
iteration 68, loss = 0.000538840948138386
iteration 69, loss = 0.0005549821071326733
iteration 70, loss = 0.0005632307729683816
iteration 71, loss = 0.0005514334770850837
iteration 72, loss = 0.0005585660692304373
iteration 73, loss = 0.0006228066049516201
iteration 74, loss = 0.0006764432182535529
iteration 75, loss = 0.0005480989348143339
iteration 76, loss = 0.0005367533303797245
iteration 77, loss = 0.0005943654105067253
iteration 78, loss = 0.000635305535979569
iteration 79, loss = 0.0005291710840538144
iteration 80, loss = 0.0005289103137329221
iteration 81, loss = 0.0005852381582371891
iteration 82, loss = 0.0005761999054811895
iteration 83, loss = 0.0005945868906565011
iteration 84, loss = 0.0005247895023785532
iteration 85, loss = 0.0005346553516574204
iteration 86, loss = 0.0006025064503774047
iteration 87, loss = 0.0005522521096281707
iteration 88, loss = 0.0005429271841421723
iteration 89, loss = 0.0005907118902541697
iteration 90, loss = 0.0005979492561891675
iteration 91, loss = 0.0005508030881173909
iteration 92, loss = 0.0017575568053871393
iteration 93, loss = 0.0005568065098486841
iteration 94, loss = 0.0006477392162196338
iteration 95, loss = 0.0008247859659604728
iteration 96, loss = 0.000620554550550878
iteration 97, loss = 0.0005942056886851788
iteration 98, loss = 0.0008169484790414572
iteration 99, loss = 0.0005905620055273175
iteration 100, loss = 0.0005402846145443618
iteration 101, loss = 0.0006421932484954596
iteration 102, loss = 0.0005006927531212568
iteration 103, loss = 0.0005629404913634062
iteration 104, loss = 0.0005078746471554041
iteration 105, loss = 0.0009193937294185162
iteration 106, loss = 0.0008986122556962073
iteration 107, loss = 0.000549657444935292
iteration 108, loss = 0.0005372977466322482
iteration 109, loss = 0.0005424304981715977
iteration 110, loss = 0.0005460976390168071
iteration 111, loss = 0.0005616516573354602
iteration 112, loss = 0.0006026208866387606
iteration 113, loss = 0.000538450840394944
iteration 114, loss = 0.0005503518623299897
iteration 115, loss = 0.0005380415823310614
iteration 116, loss = 0.0005691452533937991
iteration 117, loss = 0.0006114515126682818
iteration 118, loss = 0.0005652664694935083
iteration 119, loss = 0.0005755695747211576
iteration 120, loss = 0.000540826702490449
iteration 121, loss = 0.0005408193101175129
iteration 122, loss = 0.0005096878157928586
iteration 123, loss = 0.001551036722958088
iteration 124, loss = 0.0005060853436589241
iteration 125, loss = 0.0005916012451052666
iteration 126, loss = 0.0005830629961565137
iteration 127, loss = 0.0009295900235883892
iteration 128, loss = 0.0005500591942109168
iteration 129, loss = 0.0006370501359924674
iteration 130, loss = 0.0005135441315360367
iteration 131, loss = 0.000591027201153338
iteration 132, loss = 0.0005322186625562608
iteration 133, loss = 0.0005291903507895768
iteration 134, loss = 0.0008661792380735278
iteration 135, loss = 0.0005694447318091989
iteration 136, loss = 0.0005564300226978958
iteration 137, loss = 0.0005512917414307594
iteration 138, loss = 0.0005747814429923892
iteration 139, loss = 0.0005402580136433244
iteration 140, loss = 0.000521601177752018
iteration 141, loss = 0.0008488150197081268
iteration 142, loss = 0.000783817027695477
iteration 143, loss = 0.0005024274578318
iteration 144, loss = 0.001592002809047699
iteration 145, loss = 0.0005964097217656672
iteration 146, loss = 0.0006316397339105606
iteration 147, loss = 0.0005237855948507786
iteration 148, loss = 0.0005240152240730822
iteration 149, loss = 0.0006143766804598272
iteration 150, loss = 0.0006116305012255907
iteration 151, loss = 0.0005111565114930272
iteration 152, loss = 0.001025188248604536
iteration 153, loss = 0.0007855826406739652
iteration 154, loss = 0.0005568831111304462
iteration 155, loss = 0.0017468344885855913
iteration 156, loss = 0.000585534842684865
iteration 157, loss = 0.0005392074235714972
iteration 158, loss = 0.0005491129122674465
iteration 159, loss = 0.0008047582814469934
iteration 160, loss = 0.000521745765581727
iteration 161, loss = 0.0005238326266407967
iteration 162, loss = 0.000575095065869391
iteration 163, loss = 0.0005238064331933856
iteration 164, loss = 0.0009237114572897553
iteration 165, loss = 0.0006729839369654655
iteration 166, loss = 0.000571546028368175
iteration 167, loss = 0.00047665758756920695
iteration 168, loss = 0.000540680019184947
iteration 169, loss = 0.0005687594530172646
iteration 170, loss = 0.0006230433937162161
iteration 171, loss = 0.0005483984714373946
iteration 172, loss = 0.0005554720992222428
iteration 173, loss = 0.0005129037308506668
iteration 174, loss = 0.000520979636348784
iteration 175, loss = 0.0005862732650712132
iteration 176, loss = 0.0008855820051394403
iteration 177, loss = 0.0015560375759378076
iteration 178, loss = 0.0015289918519556522
iteration 179, loss = 0.0007033604779280722
iteration 180, loss = 0.0005437167128548026
iteration 181, loss = 0.000514729879796505
iteration 182, loss = 0.0005125619936734438
iteration 183, loss = 0.0006402327562682331
iteration 184, loss = 0.0005396049236878753
iteration 185, loss = 0.0004946108674630523
iteration 186, loss = 0.0005238057347014546
iteration 187, loss = 0.0005589175852946937
iteration 188, loss = 0.001581358490511775
iteration 189, loss = 0.000533312268089503
iteration 190, loss = 0.0005799719365313649
iteration 191, loss = 0.000555143051315099
iteration 192, loss = 0.0005441540270112455
iteration 193, loss = 0.0005844211555086076
iteration 194, loss = 0.0004925676039420068
iteration 195, loss = 0.0006245822296477854
iteration 196, loss = 0.000596561876591295
iteration 197, loss = 0.000567807350307703
iteration 198, loss = 0.0005343835800886154
iteration 199, loss = 0.0006470147054642439
iteration 200, loss = 0.0005683330819010735
iteration 201, loss = 0.0005511589697562158
iteration 202, loss = 0.0007462676148861647
iteration 203, loss = 0.0005300732445903122
iteration 204, loss = 0.0005427904543466866
iteration 205, loss = 0.0014973231591284275
iteration 206, loss = 0.0005461716209538281
iteration 207, loss = 0.0004925219109281898
iteration 208, loss = 0.0005651560495607555
iteration 209, loss = 0.0015136429574340582
iteration 210, loss = 0.0005699529428966343
iteration 211, loss = 0.0006269373116083443
iteration 212, loss = 0.0015185070224106312
iteration 213, loss = 0.0007099990034475923
iteration 214, loss = 0.0006086842622607946
iteration 215, loss = 0.000571589102037251
iteration 216, loss = 0.0008674964192323387
iteration 217, loss = 0.00056860176846385
iteration 218, loss = 0.0005858554504811764
iteration 219, loss = 0.0005646454519592226
iteration 220, loss = 0.0015108877560123801
iteration 221, loss = 0.0005285919178277254
iteration 222, loss = 0.0005233187694102526
iteration 223, loss = 0.0006180507480166852
iteration 224, loss = 0.0004984065890312195
iteration 225, loss = 0.0005866162246093154
iteration 226, loss = 0.0008878708467818797
iteration 227, loss = 0.0005326162208802998
iteration 228, loss = 0.00054824014659971
iteration 229, loss = 0.0005158221465535462
iteration 230, loss = 0.0006384938023984432
iteration 231, loss = 0.0005599701544269919
iteration 232, loss = 0.0006082040490582585
iteration 233, loss = 0.0005655474960803986
iteration 234, loss = 0.0005072586936876178
iteration 235, loss = 0.0005095926462672651
iteration 236, loss = 0.0016324935713782907
iteration 237, loss = 0.0005268801469355822
iteration 238, loss = 0.0005428465083241463
iteration 239, loss = 0.0005306781968101859
iteration 240, loss = 0.000590207171626389
iteration 241, loss = 0.0005477075465023518
iteration 242, loss = 0.0006355958175845444
iteration 243, loss = 0.0007473786827176809
iteration 244, loss = 0.0006440524593926966
iteration 245, loss = 0.0007882299832999706
iteration 246, loss = 0.0006893365061841905
iteration 247, loss = 0.0007539668586105108
iteration 248, loss = 0.0005098986439406872
iteration 249, loss = 0.0005071377381682396
iteration 250, loss = 0.0005570138455368578
iteration 251, loss = 0.0005329310661181808
iteration 252, loss = 0.0006136630545370281
iteration 253, loss = 0.0006064458866603673
iteration 254, loss = 0.000641350750811398
iteration 255, loss = 0.0004845685325562954
iteration 256, loss = 0.0005928455502726138
iteration 257, loss = 0.0005224437336437404
iteration 258, loss = 0.0005821168306283653
iteration 259, loss = 0.0005518256803043187
iteration 260, loss = 0.0007047201506793499
iteration 261, loss = 0.0005595312104560435
iteration 262, loss = 0.0005127642070874572
iteration 263, loss = 0.0005615056143142283
iteration 264, loss = 0.0006525375647470355
iteration 265, loss = 0.0004904491361230612
iteration 266, loss = 0.0005430537858046591
iteration 267, loss = 0.0005320052150636911
iteration 268, loss = 0.0005831053131259978
iteration 269, loss = 0.0005689251702278852
iteration 270, loss = 0.000523050082847476
iteration 271, loss = 0.0008287462405860424
iteration 272, loss = 0.0007104785763658583
iteration 273, loss = 0.0005550578935071826
iteration 274, loss = 0.0004942186060361564
iteration 275, loss = 0.0007646405720151961
iteration 276, loss = 0.0005669440724886954
iteration 277, loss = 0.0009464017348363996
iteration 278, loss = 0.0005591186927631497
iteration 279, loss = 0.0006802107091061771
iteration 280, loss = 0.0004952031304128468
iteration 281, loss = 0.0005420107627287507
iteration 282, loss = 0.0005868000444024801
iteration 283, loss = 0.0004821131587959826
iteration 284, loss = 0.0005725702503696084
iteration 285, loss = 0.0006172014982439578
iteration 286, loss = 0.0004993699258193374
iteration 287, loss = 0.0008785970276221633
iteration 288, loss = 0.0008501599077135324
iteration 289, loss = 0.000530852354131639
iteration 290, loss = 0.000509062607306987
iteration 291, loss = 0.0005771904834546149
iteration 292, loss = 0.0005533714429475367
iteration 293, loss = 0.0005305506056174636
iteration 294, loss = 0.0005182435852475464
iteration 295, loss = 0.0006002552108839154
iteration 296, loss = 0.00048335178871639073
iteration 297, loss = 0.0005629158113151789
iteration 298, loss = 0.0005243537598289549
iteration 299, loss = 0.0009360340191051364
iteration 0, loss = 0.000576074467971921
iteration 1, loss = 0.0005445590941235423
iteration 2, loss = 0.0005471357144415379
iteration 3, loss = 0.0006056801648810506
iteration 4, loss = 0.0005316977621987462
iteration 5, loss = 0.0005665836506523192
iteration 6, loss = 0.0005301387282088399
iteration 7, loss = 0.001508626388385892
iteration 8, loss = 0.0006029478390701115
iteration 9, loss = 0.0005667261430062354
iteration 10, loss = 0.0005310054402798414
iteration 11, loss = 0.0006497801514342427
iteration 12, loss = 0.0005443254485726357
iteration 13, loss = 0.0006463734316639602
iteration 14, loss = 0.0005119218258187175
iteration 15, loss = 0.00048202628386206925
iteration 16, loss = 0.0006411008653230965
iteration 17, loss = 0.0008731737034395337
iteration 18, loss = 0.0004781126044690609
iteration 19, loss = 0.0005614115507341921
iteration 20, loss = 0.0004895649035461247
iteration 21, loss = 0.0005025026621297002
iteration 22, loss = 0.0005387651035562158
iteration 23, loss = 0.0005678966990672052
iteration 24, loss = 0.0005986083997413516
iteration 25, loss = 0.0005429017473943532
iteration 26, loss = 0.000827609736006707
iteration 27, loss = 0.0005568640772253275
iteration 28, loss = 0.0005215751589275897
iteration 29, loss = 0.0005299280164763331
iteration 30, loss = 0.0006028029602020979
iteration 31, loss = 0.0005266611115075648
iteration 32, loss = 0.0007256788085214794
iteration 33, loss = 0.0007262381259351969
iteration 34, loss = 0.000602438987698406
iteration 35, loss = 0.0004952215822413564
iteration 36, loss = 0.0005546987522393465
iteration 37, loss = 0.0004895174060948193
iteration 38, loss = 0.0006486098282039165
iteration 39, loss = 0.0007677337853237987
iteration 40, loss = 0.000522792455740273
iteration 41, loss = 0.0005441263783723116
iteration 42, loss = 0.0005556019605137408
iteration 43, loss = 0.0004900277708657086
iteration 44, loss = 0.001635621185414493
iteration 45, loss = 0.0006112001719884574
iteration 46, loss = 0.000533679558429867
iteration 47, loss = 0.0005567307234741747
iteration 48, loss = 0.000960519362706691
iteration 49, loss = 0.000521049601957202
iteration 50, loss = 0.0008611504454165697
iteration 51, loss = 0.0005664500058628619
iteration 52, loss = 0.0006099280435591936
iteration 53, loss = 0.0005606614868156612
iteration 54, loss = 0.0004891759599559009
iteration 55, loss = 0.0006108930101618171
iteration 56, loss = 0.0004980291123501956
iteration 57, loss = 0.0006081898463889956
iteration 58, loss = 0.00048446119762957096
iteration 59, loss = 0.0005998513079248369
iteration 60, loss = 0.0014751929556950927
iteration 61, loss = 0.0005036036018282175
iteration 62, loss = 0.0008489383617416024
iteration 63, loss = 0.0005056382506154478
iteration 64, loss = 0.00048788555432111025
iteration 65, loss = 0.0007707939948886633
iteration 66, loss = 0.0006146525265648961
iteration 67, loss = 0.0005407170974649489
iteration 68, loss = 0.0005127851036377251
iteration 69, loss = 0.0005284715443849564
iteration 70, loss = 0.0005435565835796297
iteration 71, loss = 0.0005121884751133621
iteration 72, loss = 0.0006381418206728995
iteration 73, loss = 0.0005763700464740396
iteration 74, loss = 0.00047756172716617584
iteration 75, loss = 0.001502115512266755
iteration 76, loss = 0.0006239720387384295
iteration 77, loss = 0.0005244339699856937
iteration 78, loss = 0.0005284945946186781
iteration 79, loss = 0.0005915992660447955
iteration 80, loss = 0.0008240466704592109
iteration 81, loss = 0.000556028273422271
iteration 82, loss = 0.0005328416591510177
iteration 83, loss = 0.0005257673910818994
iteration 84, loss = 0.0005067065358161926
iteration 85, loss = 0.0015010556671768427
iteration 86, loss = 0.0005646625068038702
iteration 87, loss = 0.0005239680758677423
iteration 88, loss = 0.0005720581393688917
iteration 89, loss = 0.0007712056394666433
iteration 90, loss = 0.0006454825634136796
iteration 91, loss = 0.0007142253452911973
iteration 92, loss = 0.0005138629348948598
iteration 93, loss = 0.000654083676636219
iteration 94, loss = 0.0005299100885167718
iteration 95, loss = 0.0006252945167943835
iteration 96, loss = 0.000767153047490865
iteration 97, loss = 0.0008025722927413881
iteration 98, loss = 0.0005386900738812983
iteration 99, loss = 0.000578822975512594
iteration 100, loss = 0.0005300335469655693
iteration 101, loss = 0.0006239129579626024
iteration 102, loss = 0.0006501676398329437
iteration 103, loss = 0.0005454390193335712
iteration 104, loss = 0.0009505266789346933
iteration 105, loss = 0.0004954629694111645
iteration 106, loss = 0.00146042974665761
iteration 107, loss = 0.0006104636122472584
iteration 108, loss = 0.00047703037853352726
iteration 109, loss = 0.0007523992680944502
iteration 110, loss = 0.0005245199427008629
iteration 111, loss = 0.0007418321911245584
iteration 112, loss = 0.0006077666184864938
iteration 113, loss = 0.0005690873949788511
iteration 114, loss = 0.0005102609284222126
iteration 115, loss = 0.0005383313982747495
iteration 116, loss = 0.0005409945151768625
iteration 117, loss = 0.0006245311815291643
iteration 118, loss = 0.0008581586298532784
iteration 119, loss = 0.0004892188590019941
iteration 120, loss = 0.0005141743458807468
iteration 121, loss = 0.0005269270041026175
iteration 122, loss = 0.0005339159979484975
iteration 123, loss = 0.0005573845119215548
iteration 124, loss = 0.0005807623965665698
iteration 125, loss = 0.0005616877460852265
iteration 126, loss = 0.0005482886917889118
iteration 127, loss = 0.0007694577798247337
iteration 128, loss = 0.0005326045211404562
iteration 129, loss = 0.0006122624035924673
iteration 130, loss = 0.00058444042224437
iteration 131, loss = 0.0007671174826100469
iteration 132, loss = 0.0004947506240569055
iteration 133, loss = 0.0005154505488462746
iteration 134, loss = 0.0005381465307436883
iteration 135, loss = 0.0004966330016031861
iteration 136, loss = 0.0005941446288488805
iteration 137, loss = 0.0005556048708967865
iteration 138, loss = 0.0006086176144890487
iteration 139, loss = 0.0005216848221607506
iteration 140, loss = 0.0005798180354759097
iteration 141, loss = 0.0005222471081651747
iteration 142, loss = 0.0005369067075662315
iteration 143, loss = 0.0007144424016587436
iteration 144, loss = 0.0009029780048877001
iteration 145, loss = 0.0005495785153470933
iteration 146, loss = 0.0005624326295219362
iteration 147, loss = 0.0005146964685991406
iteration 148, loss = 0.0005345181562006474
iteration 149, loss = 0.0005500087863765657
iteration 150, loss = 0.0004914984456263483
iteration 151, loss = 0.0005049174069426954
iteration 152, loss = 0.0006828790064901114
iteration 153, loss = 0.0006084267515689135
iteration 154, loss = 0.0005360263166949153
iteration 155, loss = 0.0008102915016934276
iteration 156, loss = 0.000562344619538635
iteration 157, loss = 0.0004700380959548056
iteration 158, loss = 0.00048730202252045274
iteration 159, loss = 0.0005010230233892798
iteration 160, loss = 0.0006266168784350157
iteration 161, loss = 0.0005346919060684741
iteration 162, loss = 0.0005714771687053144
iteration 163, loss = 0.0005957905086688697
iteration 164, loss = 0.000633281480986625
iteration 165, loss = 0.0005974859232082963
iteration 166, loss = 0.0004960276419296861
iteration 167, loss = 0.0015066599007695913
iteration 168, loss = 0.0005814787582494318
iteration 169, loss = 0.0005623666220344603
iteration 170, loss = 0.0005040289834141731
iteration 171, loss = 0.0005757305771112442
iteration 172, loss = 0.0004772654501721263
iteration 173, loss = 0.0004541610833257437
iteration 174, loss = 0.0005122180446051061
iteration 175, loss = 0.0005494269425980747
iteration 176, loss = 0.0006059251609258354
iteration 177, loss = 0.0008039328968152404
iteration 178, loss = 0.0005668787634931505
iteration 179, loss = 0.0005232565454207361
iteration 180, loss = 0.000567337847314775
iteration 181, loss = 0.0005046291626058519
iteration 182, loss = 0.00070289516588673
iteration 183, loss = 0.0005079132970422506
iteration 184, loss = 0.0015556571306660771
iteration 185, loss = 0.0009390488849021494
iteration 186, loss = 0.0005242929328233004
iteration 187, loss = 0.000625127402599901
iteration 188, loss = 0.0005478372913785279
iteration 189, loss = 0.000510048063006252
iteration 190, loss = 0.0006196532631292939
iteration 191, loss = 0.001510922098532319
iteration 192, loss = 0.0005279476754367352
iteration 193, loss = 0.001558947842568159
iteration 194, loss = 0.0005504246219061315
iteration 195, loss = 0.0005065390141680837
iteration 196, loss = 0.0015998829621821642
iteration 197, loss = 0.0004823996569029987
iteration 198, loss = 0.0005533240619115531
iteration 199, loss = 0.0008533687214367092
iteration 200, loss = 0.0005428682779893279
iteration 201, loss = 0.0005176762351766229
iteration 202, loss = 0.0005065884906798601
iteration 203, loss = 0.000512479105964303
iteration 204, loss = 0.000875909230671823
iteration 205, loss = 0.0005021981778554618
iteration 206, loss = 0.0008827491546981037
iteration 207, loss = 0.0005600320873782039
iteration 208, loss = 0.0005407948628999293
iteration 209, loss = 0.000498254201374948
iteration 210, loss = 0.0005942356074228883
iteration 211, loss = 0.0006079575978219509
iteration 212, loss = 0.0007323975441977382
iteration 213, loss = 0.0005204370827414095
iteration 214, loss = 0.0005425390554592013
iteration 215, loss = 0.0008287867531180382
iteration 216, loss = 0.0009278127690777183
iteration 217, loss = 0.0005823594983667135
iteration 218, loss = 0.0006379724945873022
iteration 219, loss = 0.0005616597482003272
iteration 220, loss = 0.0014839529758319259
iteration 221, loss = 0.0005027762381359935
iteration 222, loss = 0.0005160897271707654
iteration 223, loss = 0.0015075015835464
iteration 224, loss = 0.0004849881515838206
iteration 225, loss = 0.0008431771420873702
iteration 226, loss = 0.0005231027025729418
iteration 227, loss = 0.0005087583558633924
iteration 228, loss = 0.0004985568812116981
iteration 229, loss = 0.0005872963811270893
iteration 230, loss = 0.0005925437435507774
iteration 231, loss = 0.0005514719523489475
iteration 232, loss = 0.0009100913885049522
iteration 233, loss = 0.0005571793299168348
iteration 234, loss = 0.0005137156695127487
iteration 235, loss = 0.000512084225192666
iteration 236, loss = 0.0007791027310304344
iteration 237, loss = 0.0006121473270468414
iteration 238, loss = 0.0005366673576645553
iteration 239, loss = 0.0005583254969678819
iteration 240, loss = 0.0006622812943533063
iteration 241, loss = 0.00047107849968597293
iteration 242, loss = 0.0005367694539017975
iteration 243, loss = 0.0008775396272540092
iteration 244, loss = 0.0005203791661188006
iteration 245, loss = 0.0005778340273536742
iteration 246, loss = 0.0005848694127053022
iteration 247, loss = 0.0004964405088685453
iteration 248, loss = 0.0005018090596422553
iteration 249, loss = 0.000487585726659745
iteration 250, loss = 0.0005148379132151604
iteration 251, loss = 0.0004904777160845697
iteration 252, loss = 0.000501143978908658
iteration 253, loss = 0.00048769143177196383
iteration 254, loss = 0.000595108256675303
iteration 255, loss = 0.0005482002161443233
iteration 256, loss = 0.0005383489769883454
iteration 257, loss = 0.0008361367508769035
iteration 258, loss = 0.0006234407774172723
iteration 259, loss = 0.0008851283928379416
iteration 260, loss = 0.0007232591160573065
iteration 261, loss = 0.0005274960421957076
iteration 262, loss = 0.0005289972759783268
iteration 263, loss = 0.0004890032578259706
iteration 264, loss = 0.0006000157445669174
iteration 265, loss = 0.0005220804596319795
iteration 266, loss = 0.00047696824185550213
iteration 267, loss = 0.0004844972863793373
iteration 268, loss = 0.00045902395504526794
iteration 269, loss = 0.0004914748715236783
iteration 270, loss = 0.0004977360949851573
iteration 271, loss = 0.00047705951146781445
iteration 272, loss = 0.0006210978026501834
iteration 273, loss = 0.0004907292895950377
iteration 274, loss = 0.0006601763889193535
iteration 275, loss = 0.0005362748634070158
iteration 276, loss = 0.0004425306397024542
iteration 277, loss = 0.0005624198238365352
iteration 278, loss = 0.0005461309337988496
iteration 279, loss = 0.0004965504049323499
iteration 280, loss = 0.0005147982155904174
iteration 281, loss = 0.0005438479711301625
iteration 282, loss = 0.000571968499571085
iteration 283, loss = 0.0005295684677548707
iteration 284, loss = 0.0005214421544224024
iteration 285, loss = 0.0015708493301644921
iteration 286, loss = 0.0007391860126517713
iteration 287, loss = 0.0007401164621114731
iteration 288, loss = 0.0006913832039572299
iteration 289, loss = 0.0005693911807611585
iteration 290, loss = 0.000730940024368465
iteration 291, loss = 0.0015169500838965178
iteration 292, loss = 0.0005192288081161678
iteration 293, loss = 0.0005977788241580129
iteration 294, loss = 0.000475917273433879
iteration 295, loss = 0.00046874932013452053
iteration 296, loss = 0.0005075542721897364
iteration 297, loss = 0.0005913898348808289
iteration 298, loss = 0.0004759316798299551
iteration 299, loss = 0.0005364223034121096
iteration 0, loss = 0.0004918718477711082
iteration 1, loss = 0.0005913073546253145
iteration 2, loss = 0.0005939258262515068
iteration 3, loss = 0.0004917001351714134
iteration 4, loss = 0.0005297156749293208
iteration 5, loss = 0.0005080868722870946
iteration 6, loss = 0.0005530706839635968
iteration 7, loss = 0.0005239666206762195
iteration 8, loss = 0.0016230200417339802
iteration 9, loss = 0.0005448692245408893
iteration 10, loss = 0.0004916839534416795
iteration 11, loss = 0.0005133289960213006
iteration 12, loss = 0.0006034613470546901
iteration 13, loss = 0.0005418681539595127
iteration 14, loss = 0.0005291389534249902
iteration 15, loss = 0.0005402666865848005
iteration 16, loss = 0.0005432448233477771
iteration 17, loss = 0.0004595450300257653
iteration 18, loss = 0.0005037367227487266
iteration 19, loss = 0.0005361582152545452
iteration 20, loss = 0.0006971302791498601
iteration 21, loss = 0.0014434115728363395
iteration 22, loss = 0.000544572074431926
iteration 23, loss = 0.0006063573528081179
iteration 24, loss = 0.0006023977184668183
iteration 25, loss = 0.0008710766560398042
iteration 26, loss = 0.0005778008489869535
iteration 27, loss = 0.0007575082709081471
iteration 28, loss = 0.0015019227284938097
iteration 29, loss = 0.0005222682375460863
iteration 30, loss = 0.0005360662471503019
iteration 31, loss = 0.0005756293539889157
iteration 32, loss = 0.00056972581660375
iteration 33, loss = 0.0004474783199839294
iteration 34, loss = 0.00048301462084054947
iteration 35, loss = 0.00044049994903616607
iteration 36, loss = 0.0005019279778935015
iteration 37, loss = 0.0005719455075450242
iteration 38, loss = 0.0008482353296130896
iteration 39, loss = 0.000565696507692337
iteration 40, loss = 0.0005581281729973853
iteration 41, loss = 0.0005892384215258062
iteration 42, loss = 0.00073196180164814
iteration 43, loss = 0.0005177605780772865
iteration 44, loss = 0.0004946588887833059
iteration 45, loss = 0.000627475674264133
iteration 46, loss = 0.0005125781754031777
iteration 47, loss = 0.0005025603459216654
iteration 48, loss = 0.0005869760643690825
iteration 49, loss = 0.0005232947296462953
iteration 50, loss = 0.0005724805523641407
iteration 51, loss = 0.0007402180344797671
iteration 52, loss = 0.0006549259996972978
iteration 53, loss = 0.0005393986939452589
iteration 54, loss = 0.0005052933702245355
iteration 55, loss = 0.0005787497502751648
iteration 56, loss = 0.0005336427129805088
iteration 57, loss = 0.000675332616083324
iteration 58, loss = 0.0005025573773309588
iteration 59, loss = 0.0006875070976093411
iteration 60, loss = 0.0005715478328056633
iteration 61, loss = 0.0005790598806925118
iteration 62, loss = 0.0005597470444627106
iteration 63, loss = 0.0007440089248120785
iteration 64, loss = 0.0015581001061946154
iteration 65, loss = 0.0005036354996263981
iteration 66, loss = 0.0005593406385742128
iteration 67, loss = 0.0005214746342971921
iteration 68, loss = 0.0004629679024219513
iteration 69, loss = 0.0005620110314339399
iteration 70, loss = 0.0006292174803093076
iteration 71, loss = 0.000519585853908211
iteration 72, loss = 0.0005378187051974237
iteration 73, loss = 0.0009082136675715446
iteration 74, loss = 0.0014941947301849723
iteration 75, loss = 0.0004534852923825383
iteration 76, loss = 0.0004559256776701659
iteration 77, loss = 0.0005319644114933908
iteration 78, loss = 0.0004573127080220729
iteration 79, loss = 0.0005422778776846826
iteration 80, loss = 0.00047008777619339526
iteration 81, loss = 0.0004907725960947573
iteration 82, loss = 0.001429377356544137
iteration 83, loss = 0.00048038826207630336
iteration 84, loss = 0.000554082915186882
iteration 85, loss = 0.0005367059493437409
iteration 86, loss = 0.0008542680880054832
iteration 87, loss = 0.000570703879930079
iteration 88, loss = 0.0005310226697474718
iteration 89, loss = 0.0004806005163118243
iteration 90, loss = 0.0005134757375344634
iteration 91, loss = 0.0005019053933210671
iteration 92, loss = 0.0005435957573354244
iteration 93, loss = 0.0008320389897562563
iteration 94, loss = 0.0005121766007505357
iteration 95, loss = 0.0004895395832136273
iteration 96, loss = 0.0008707063389010727
iteration 97, loss = 0.00048335318570025265
iteration 98, loss = 0.000557968276552856
iteration 99, loss = 0.0005214117700234056
iteration 100, loss = 0.00047870492562651634
iteration 101, loss = 0.0005691532860510051
iteration 102, loss = 0.0004856434534303844
iteration 103, loss = 0.0004927232512272894
iteration 104, loss = 0.0005566899199038744
iteration 105, loss = 0.0006536549190059304
iteration 106, loss = 0.0004963133251294494
iteration 107, loss = 0.0008529162732884288
iteration 108, loss = 0.0005967237520962954
iteration 109, loss = 0.0006198523915372789
iteration 110, loss = 0.00050896912580356
iteration 111, loss = 0.0004984594997949898
iteration 112, loss = 0.00048331415746361017
iteration 113, loss = 0.0005133261438459158
iteration 114, loss = 0.0004860804765485227
iteration 115, loss = 0.0005906963488087058
iteration 116, loss = 0.0008554623927921057
iteration 117, loss = 0.0005816828343085945
iteration 118, loss = 0.0009820578852668405
iteration 119, loss = 0.0005294005386531353
iteration 120, loss = 0.0005386188859120011
iteration 121, loss = 0.0004932587035000324
iteration 122, loss = 0.0005835246411152184
iteration 123, loss = 0.0014426872367039323
iteration 124, loss = 0.000830387114547193
iteration 125, loss = 0.0004898578627035022
iteration 126, loss = 0.0007607376901432872
iteration 127, loss = 0.0004806077922694385
iteration 128, loss = 0.0005389099242165685
iteration 129, loss = 0.0004945612163282931
iteration 130, loss = 0.000525455514434725
iteration 131, loss = 0.0005717765307053924
iteration 132, loss = 0.0005455759237520397
iteration 133, loss = 0.0005372740561142564
iteration 134, loss = 0.0005120301502756774
iteration 135, loss = 0.0005202234024181962
iteration 136, loss = 0.00047600665129721165
iteration 137, loss = 0.0005796795594505966
iteration 138, loss = 0.002374825766310096
iteration 139, loss = 0.00047462485963478684
iteration 140, loss = 0.000688568688929081
iteration 141, loss = 0.0008449004380963743
iteration 142, loss = 0.0005972358048893511
iteration 143, loss = 0.0006515479180961847
iteration 144, loss = 0.0005030081956647336
iteration 145, loss = 0.0005335674504749477
iteration 146, loss = 0.00044666227768175304
iteration 147, loss = 0.0005645772907882929
iteration 148, loss = 0.0004848625685553998
iteration 149, loss = 0.0005923218559473753
iteration 150, loss = 0.0005874476046301425
iteration 151, loss = 0.0004475906025618315
iteration 152, loss = 0.0014581794384866953
iteration 153, loss = 0.0005122462171129882
iteration 154, loss = 0.0005726987146772444
iteration 155, loss = 0.000526000396348536
iteration 156, loss = 0.0008045231807045639
iteration 157, loss = 0.00048625044291839004
iteration 158, loss = 0.0005337308393791318
iteration 159, loss = 0.000569584546610713
iteration 160, loss = 0.0005758323823101819
iteration 161, loss = 0.000528246455360204
iteration 162, loss = 0.0006219331989996135
iteration 163, loss = 0.0004929911228828132
iteration 164, loss = 0.0005907014710828662
iteration 165, loss = 0.000519271707162261
iteration 166, loss = 0.0004605657886713743
iteration 167, loss = 0.00046783138532191515
iteration 168, loss = 0.0007497571059502661
iteration 169, loss = 0.0004707282059825957
iteration 170, loss = 0.00047123877448029816
iteration 171, loss = 0.000495667802169919
iteration 172, loss = 0.0008787020924501121
iteration 173, loss = 0.0004529886646196246
iteration 174, loss = 0.0005117475520819426
iteration 175, loss = 0.0007930878200568259
iteration 176, loss = 0.0005074961227364838
iteration 177, loss = 0.0005327171529643238
iteration 178, loss = 0.0005150699871592224
iteration 179, loss = 0.0005042994162067771
iteration 180, loss = 0.000510137586388737
iteration 181, loss = 0.0004840214387513697
iteration 182, loss = 0.000561205146368593
iteration 183, loss = 0.0005443060654215515
iteration 184, loss = 0.0004940400249324739
iteration 185, loss = 0.0014475143980234861
iteration 186, loss = 0.0004971083253622055
iteration 187, loss = 0.0005430160090327263
iteration 188, loss = 0.0005204846384003758
iteration 189, loss = 0.0006572572747245431
iteration 190, loss = 0.0005079888505861163
iteration 191, loss = 0.00046769267646595836
iteration 192, loss = 0.00048590131336823106
iteration 193, loss = 0.0005200045416131616
iteration 194, loss = 0.0005250825197435915
iteration 195, loss = 0.0005507497698999941
iteration 196, loss = 0.0005500953993760049
iteration 197, loss = 0.0005079513066448271
iteration 198, loss = 0.0005131803336553276
iteration 199, loss = 0.0005927897291257977
iteration 200, loss = 0.0007895359885878861
iteration 201, loss = 0.0004943804233334959
iteration 202, loss = 0.000524184200912714
iteration 203, loss = 0.001430389005690813
iteration 204, loss = 0.0006165911327116191
iteration 205, loss = 0.000733233115170151
iteration 206, loss = 0.0005375412874855101
iteration 207, loss = 0.0005395940388552845
iteration 208, loss = 0.0007920084754005075
iteration 209, loss = 0.00049690215382725
iteration 210, loss = 0.0005643998738378286
iteration 211, loss = 0.0006804188014939427
iteration 212, loss = 0.0004841567133553326
iteration 213, loss = 0.00046284246491268277
iteration 214, loss = 0.000607846537604928
iteration 215, loss = 0.0006960203754715621
iteration 216, loss = 0.0006715909694321454
iteration 217, loss = 0.0005487087764777243
iteration 218, loss = 0.000694963033311069
iteration 219, loss = 0.0004945152904838324
iteration 220, loss = 0.0006461003795266151
iteration 221, loss = 0.0004931184812448919
iteration 222, loss = 0.0007517332560382783
iteration 223, loss = 0.0004788023652508855
iteration 224, loss = 0.0005217707948759198
iteration 225, loss = 0.0004414338036440313
iteration 226, loss = 0.0004931952571496367
iteration 227, loss = 0.0006213735905475914
iteration 228, loss = 0.000693062087520957
iteration 229, loss = 0.0005092937499284744
iteration 230, loss = 0.0005696301232092083
iteration 231, loss = 0.0004892160068266094
iteration 232, loss = 0.0005048896418884397
iteration 233, loss = 0.0005167531780898571
iteration 234, loss = 0.0005882588448002934
iteration 235, loss = 0.0008032748010009527
iteration 236, loss = 0.0005573411472141743
iteration 237, loss = 0.0005073523498140275
iteration 238, loss = 0.0005990300560370088
iteration 239, loss = 0.0007203100831247866
iteration 240, loss = 0.0004969174624420702
iteration 241, loss = 0.0005373051390051842
iteration 242, loss = 0.0014346849638968706
iteration 243, loss = 0.0004922962398268282
iteration 244, loss = 0.0005376877961680293
iteration 245, loss = 0.0005540202837437391
iteration 246, loss = 0.00046990904957056046
iteration 247, loss = 0.0007246795576065779
iteration 248, loss = 0.0005651392857544124
iteration 249, loss = 0.0004791791725438088
iteration 250, loss = 0.0003986159572377801
iteration 251, loss = 0.0005813798634335399
iteration 252, loss = 0.0005117216496728361
iteration 253, loss = 0.0005848842556588352
iteration 254, loss = 0.0005276345764286816
iteration 255, loss = 0.0007122906390577555
iteration 256, loss = 0.0005798686179332435
iteration 257, loss = 0.0005324541707523167
iteration 258, loss = 0.0005131978541612625
iteration 259, loss = 0.0005505552398972213
iteration 260, loss = 0.00046344034490175545
iteration 261, loss = 0.0005941357812844217
iteration 262, loss = 0.0004746390332002193
iteration 263, loss = 0.0004984376719221473
iteration 264, loss = 0.0015401137061417103
iteration 265, loss = 0.00047447282122448087
iteration 266, loss = 0.0006034691468812525
iteration 267, loss = 0.0005692108534276485
iteration 268, loss = 0.00045770208816975355
iteration 269, loss = 0.0005093466024845839
iteration 270, loss = 0.0005151329096406698
iteration 271, loss = 0.00046978823957033455
iteration 272, loss = 0.0006796176312491298
iteration 273, loss = 0.0004905615351162851
iteration 274, loss = 0.0005054076900705695
iteration 275, loss = 0.0008545940509065986
iteration 276, loss = 0.000974326569121331
iteration 277, loss = 0.0004977587959729135
iteration 278, loss = 0.000534153135959059
iteration 279, loss = 0.000554150901734829
iteration 280, loss = 0.0008076621452346444
iteration 281, loss = 0.0005227221990935504
iteration 282, loss = 0.0005980774876661599
iteration 283, loss = 0.0005627435748465359
iteration 284, loss = 0.000503218499943614
iteration 285, loss = 0.0005715584848076105
iteration 286, loss = 0.0005449058953672647
iteration 287, loss = 0.0005383697571232915
iteration 288, loss = 0.0005364581011235714
iteration 289, loss = 0.0005230155074968934
iteration 290, loss = 0.0005051909829489887
iteration 291, loss = 0.0011641309829428792
iteration 292, loss = 0.00045552727533504367
iteration 293, loss = 0.000482356728753075
iteration 294, loss = 0.0004493107262533158
iteration 295, loss = 0.0005501518025994301
iteration 296, loss = 0.00048192759277299047
iteration 297, loss = 0.0015002719592303038
iteration 298, loss = 0.0004737857962027192
iteration 299, loss = 0.000585426518227905
iteration 0, loss = 0.0004975222400389612
iteration 1, loss = 0.0005856920615769923
iteration 2, loss = 0.000673288363032043
iteration 3, loss = 0.0005870976601727307
iteration 4, loss = 0.0004891889402642846
iteration 5, loss = 0.0005604910547845066
iteration 6, loss = 0.0005501671694219112
iteration 7, loss = 0.0006922558532096446
iteration 8, loss = 0.0005357004120014608
iteration 9, loss = 0.0005589832435362041
iteration 10, loss = 0.0004933879245072603
iteration 11, loss = 0.0005268043605610728
iteration 12, loss = 0.0005705147050321102
iteration 13, loss = 0.0004959345678798854
iteration 14, loss = 0.000837548344861716
iteration 15, loss = 0.0004934174357913435
iteration 16, loss = 0.0015570757677778602
iteration 17, loss = 0.0004975151969119906
iteration 18, loss = 0.0005445246351882815
iteration 19, loss = 0.000511241378262639
iteration 20, loss = 0.0007142677204683423
iteration 21, loss = 0.0014262739568948746
iteration 22, loss = 0.0005617127753794193
iteration 23, loss = 0.0005585102480836213
iteration 24, loss = 0.0005098058609291911
iteration 25, loss = 0.0004994050250388682
iteration 26, loss = 0.0008574690436944366
iteration 27, loss = 0.0005802532541565597
iteration 28, loss = 0.00048306601820513606
iteration 29, loss = 0.0004550758167169988
iteration 30, loss = 0.0006125829531811178
iteration 31, loss = 0.0006652705487795174
iteration 32, loss = 0.0009006797918118536
iteration 33, loss = 0.0005501086125150323
iteration 34, loss = 0.0005256670410744846
iteration 35, loss = 0.0004362911859061569
iteration 36, loss = 0.00048056215746328235
iteration 37, loss = 0.0006512392428703606
iteration 38, loss = 0.0004769417573697865
iteration 39, loss = 0.0005156685947440565
iteration 40, loss = 0.0006176777533255517
iteration 41, loss = 0.0005032784538343549
iteration 42, loss = 0.0014034273335710168
iteration 43, loss = 0.0004800015303771943
iteration 44, loss = 0.00047353413538075984
iteration 45, loss = 0.0004996276111342013
iteration 46, loss = 0.0005989748751744628
iteration 47, loss = 0.000809397199191153
iteration 48, loss = 0.0005617797141894698
iteration 49, loss = 0.0005392456077970564
iteration 50, loss = 0.0004741086158901453
iteration 51, loss = 0.000522233487572521
iteration 52, loss = 0.000570711970794946
iteration 53, loss = 0.0005321325152181089
iteration 54, loss = 0.0005660822498612106
iteration 55, loss = 0.0005261365440674126
iteration 56, loss = 0.0005646742647513747
iteration 57, loss = 0.0004870162229053676
iteration 58, loss = 0.0007330903317779303
iteration 59, loss = 0.0004677614779211581
iteration 60, loss = 0.0005457414663396776
iteration 61, loss = 0.0005081625422462821
iteration 62, loss = 0.000467582605779171
iteration 63, loss = 0.0005149762146174908
iteration 64, loss = 0.00045310784480534494
iteration 65, loss = 0.0005331502761691809
iteration 66, loss = 0.0004974252078682184
iteration 67, loss = 0.0005244608619250357
iteration 68, loss = 0.0005458584637381136
iteration 69, loss = 0.00048032074118964374
iteration 70, loss = 0.0008052103221416473
iteration 71, loss = 0.0005423409747891128
iteration 72, loss = 0.0005117551190778613
iteration 73, loss = 0.0005148560740053654
iteration 74, loss = 0.0004670956695917994
iteration 75, loss = 0.0005125770112499595
iteration 76, loss = 0.0005350044229999185
iteration 77, loss = 0.0006584852235391736
iteration 78, loss = 0.0008140571299009025
iteration 79, loss = 0.00046576527529396117
iteration 80, loss = 0.0004998765070922673
iteration 81, loss = 0.000513936800416559
iteration 82, loss = 0.0005370616563595831
iteration 83, loss = 0.0004744809994008392
iteration 84, loss = 0.0005114275263622403
iteration 85, loss = 0.00048733456060290337
iteration 86, loss = 0.0005319518968462944
iteration 87, loss = 0.0004890318959951401
iteration 88, loss = 0.0005505044828169048
iteration 89, loss = 0.00048758654156699777
iteration 90, loss = 0.0004890841664746404
iteration 91, loss = 0.0005334736779332161
iteration 92, loss = 0.00046425513573922217
iteration 93, loss = 0.0005169599317014217
iteration 94, loss = 0.0005385605618357658
iteration 95, loss = 0.0004903831286355853
iteration 96, loss = 0.0005248469533398747
iteration 97, loss = 0.0005230712704360485
iteration 98, loss = 0.0007491660653613508
iteration 99, loss = 0.00048801591037772596
iteration 100, loss = 0.0005427569267340004
iteration 101, loss = 0.0005033074412494898
iteration 102, loss = 0.0005268348031677306
iteration 103, loss = 0.0004905155510641634
iteration 104, loss = 0.00050414475845173
iteration 105, loss = 0.0005176052800379694
iteration 106, loss = 0.0005255807191133499
iteration 107, loss = 0.0004667281755246222
iteration 108, loss = 0.0006015733233653009
iteration 109, loss = 0.000490211124997586
iteration 110, loss = 0.0005647925427183509
iteration 111, loss = 0.0005700021283701062
iteration 112, loss = 0.0005374541506171227
iteration 113, loss = 0.0005573086673393846
iteration 114, loss = 0.0004973534378223121
iteration 115, loss = 0.000698968127835542
iteration 116, loss = 0.00048183108447119594
iteration 117, loss = 0.0005026603466831148
iteration 118, loss = 0.0006037274142727256
iteration 119, loss = 0.0005330920685082674
iteration 120, loss = 0.0005332718137651682
iteration 121, loss = 0.0007302251760847867
iteration 122, loss = 0.00044861069181934
iteration 123, loss = 0.0004991168971173465
iteration 124, loss = 0.0005470341420732439
iteration 125, loss = 0.0004544158582575619
iteration 126, loss = 0.0004915747558698058
iteration 127, loss = 0.0009085240890271962
iteration 128, loss = 0.000576758582610637
iteration 129, loss = 0.000461178133264184
iteration 130, loss = 0.0005340164643712342
iteration 131, loss = 0.0004974546027369797
iteration 132, loss = 0.0004765947815030813
iteration 133, loss = 0.000552215613424778
iteration 134, loss = 0.0006357983220368624
iteration 135, loss = 0.0005782658117823303
iteration 136, loss = 0.00046556146116927266
iteration 137, loss = 0.000513682491146028
iteration 138, loss = 0.0006057685241103172
iteration 139, loss = 0.0004644251603167504
iteration 140, loss = 0.0004782133037224412
iteration 141, loss = 0.0004823796625714749
iteration 142, loss = 0.0017075412906706333
iteration 143, loss = 0.000627354544121772
iteration 144, loss = 0.0005106237367726862
iteration 145, loss = 0.00048229508684016764
iteration 146, loss = 0.0004875529557466507
iteration 147, loss = 0.0004619499377440661
iteration 148, loss = 0.00048025473370216787
iteration 149, loss = 0.0005079305847175419
iteration 150, loss = 0.0004956647171638906
iteration 151, loss = 0.0006034003454260528
iteration 152, loss = 0.00043519027531147003
iteration 153, loss = 0.0013867368688806891
iteration 154, loss = 0.0005461095133796334
iteration 155, loss = 0.00047334321425296366
iteration 156, loss = 0.0006506160134449601
iteration 157, loss = 0.0004667235480155796
iteration 158, loss = 0.0005221277824603021
iteration 159, loss = 0.0007759422296658158
iteration 160, loss = 0.0005254408461041749
iteration 161, loss = 0.000543289294000715
iteration 162, loss = 0.0008408231660723686
iteration 163, loss = 0.00047509584692306817
iteration 164, loss = 0.0004662502324208617
iteration 165, loss = 0.000663611339405179
iteration 166, loss = 0.00045519988634623587
iteration 167, loss = 0.0008292452548630536
iteration 168, loss = 0.00047176977386698127
iteration 169, loss = 0.0005051215994171798
iteration 170, loss = 0.0005177372950129211
iteration 171, loss = 0.0004662247374653816
iteration 172, loss = 0.00046565814409404993
iteration 173, loss = 0.0004641530686058104
iteration 174, loss = 0.001506515429355204
iteration 175, loss = 0.0005660320166498423
iteration 176, loss = 0.0005538513069041073
iteration 177, loss = 0.0004850834666285664
iteration 178, loss = 0.0006775966030545533
iteration 179, loss = 0.00045306546962819993
iteration 180, loss = 0.00047014819574542344
iteration 181, loss = 0.0014370143180713058
iteration 182, loss = 0.0008014701306819916
iteration 183, loss = 0.0005376319750212133
iteration 184, loss = 0.0013805711641907692
iteration 185, loss = 0.0004389652458485216
iteration 186, loss = 0.0004891364951618016
iteration 187, loss = 0.00048750557471066713
iteration 188, loss = 0.0004624695284292102
iteration 189, loss = 0.000759248505346477
iteration 190, loss = 0.0004759421572089195
iteration 191, loss = 0.0013870738912373781
iteration 192, loss = 0.00048483459977433085
iteration 193, loss = 0.0005131529760546982
iteration 194, loss = 0.0014330504927784204
iteration 195, loss = 0.0004698926059063524
iteration 196, loss = 0.0006126257940195501
iteration 197, loss = 0.0014067664742469788
iteration 198, loss = 0.00046323324204422534
iteration 199, loss = 0.0005170673248358071
iteration 200, loss = 0.0005115561652928591
iteration 201, loss = 0.00048432007315568626
iteration 202, loss = 0.0007072268053889275
iteration 203, loss = 0.000516965170390904
iteration 204, loss = 0.0005656740977428854
iteration 205, loss = 0.0005798283964395523
iteration 206, loss = 0.0005125583265908062
iteration 207, loss = 0.0004890624550171196
iteration 208, loss = 0.0005601521115750074
iteration 209, loss = 0.00048517039977014065
iteration 210, loss = 0.00044099841034039855
iteration 211, loss = 0.0007465432281605899
iteration 212, loss = 0.0004508171696215868
iteration 213, loss = 0.0007397428853437304
iteration 214, loss = 0.0004965934786014259
iteration 215, loss = 0.00046260328963398933
iteration 216, loss = 0.0005272655398584902
iteration 217, loss = 0.0007511638686992228
iteration 218, loss = 0.0005353253800421953
iteration 219, loss = 0.0004390388203319162
iteration 220, loss = 0.0005229199305176735
iteration 221, loss = 0.0007605779683217406
iteration 222, loss = 0.00046880735317245126
iteration 223, loss = 0.0006307814037427306
iteration 224, loss = 0.00046129588736221194
iteration 225, loss = 0.0004949172725901008
iteration 226, loss = 0.0004554915358312428
iteration 227, loss = 0.0004918717313557863
iteration 228, loss = 0.0005182545864954591
iteration 229, loss = 0.0013690779451280832
iteration 230, loss = 0.0005182999884709716
iteration 231, loss = 0.0006234051543287933
iteration 232, loss = 0.0005231656832620502
iteration 233, loss = 0.0005124131566844881
iteration 234, loss = 0.00047778210137039423
iteration 235, loss = 0.0005330866551958025
iteration 236, loss = 0.0005630789091810584
iteration 237, loss = 0.0005084469448775053
iteration 238, loss = 0.0008805851684883237
iteration 239, loss = 0.0005014780326746404
iteration 240, loss = 0.0005040735122747719
iteration 241, loss = 0.0005097910179756582
iteration 242, loss = 0.0005517885438166559
iteration 243, loss = 0.0006773231434635818
iteration 244, loss = 0.0006731867324560881
iteration 245, loss = 0.0004267121839802712
iteration 246, loss = 0.0004952275776304305
iteration 247, loss = 0.00046490941895172
iteration 248, loss = 0.001396166393533349
iteration 249, loss = 0.0004831940750591457
iteration 250, loss = 0.0004916511243209243
iteration 251, loss = 0.0004735851543955505
iteration 252, loss = 0.0004929588758386672
iteration 253, loss = 0.0006360560073517263
iteration 254, loss = 0.0007897333125583827
iteration 255, loss = 0.0005278605967760086
iteration 256, loss = 0.0004772011307068169
iteration 257, loss = 0.0005213047843426466
iteration 258, loss = 0.000527200463693589
iteration 259, loss = 0.0004775715060532093
iteration 260, loss = 0.0005042037228122354
iteration 261, loss = 0.0005334976012818515
iteration 262, loss = 0.001609628088772297
iteration 263, loss = 0.0005121065187267959
iteration 264, loss = 0.0005228900699876249
iteration 265, loss = 0.0005553832743316889
iteration 266, loss = 0.00047378402086906135
iteration 267, loss = 0.0005352373118512332
iteration 268, loss = 0.0005817203200422227
iteration 269, loss = 0.0005899431416764855
iteration 270, loss = 0.00047704469761811197
iteration 271, loss = 0.0004624549765139818
iteration 272, loss = 0.0006081388564780354
iteration 273, loss = 0.0005485595320351422
iteration 274, loss = 0.0005122881848365068
iteration 275, loss = 0.0005350514547899365
iteration 276, loss = 0.000512401747982949
iteration 277, loss = 0.0005489747854880989
iteration 278, loss = 0.0004870817647315562
iteration 279, loss = 0.0005829944275319576
iteration 280, loss = 0.0007838202291168272
iteration 281, loss = 0.0009170074481517076
iteration 282, loss = 0.0004920443752780557
iteration 283, loss = 0.0006135977455414832
iteration 284, loss = 0.0006743516423739493
iteration 285, loss = 0.0005014335038140416
iteration 286, loss = 0.0005069694598205388
iteration 287, loss = 0.0006471756496466696
iteration 288, loss = 0.0007821033941581845
iteration 289, loss = 0.0004640983243007213
iteration 290, loss = 0.0004505645192693919
iteration 291, loss = 0.0006458476418629289
iteration 292, loss = 0.0017013675533235073
iteration 293, loss = 0.0005257893353700638
iteration 294, loss = 0.0004782463947776705
iteration 295, loss = 0.0004727588966488838
iteration 296, loss = 0.0005416201893240213
iteration 297, loss = 0.0005324711091816425
iteration 298, loss = 0.0007217805250547826
iteration 299, loss = 0.00046490924432873726
iteration 0, loss = 0.000516589789185673
iteration 1, loss = 0.0013469008263200521
iteration 2, loss = 0.00048164138570427895
iteration 3, loss = 0.00048533882363699377
iteration 4, loss = 0.0005240265163592994
iteration 5, loss = 0.0005247960798442364
iteration 6, loss = 0.00047644178266637027
iteration 7, loss = 0.0005810605362057686
iteration 8, loss = 0.0005495110526680946
iteration 9, loss = 0.000538360676728189
iteration 10, loss = 0.0004897487815469503
iteration 11, loss = 0.0004969460424035788
iteration 12, loss = 0.0005106554017402232
iteration 13, loss = 0.0005210188101045787
iteration 14, loss = 0.0005039728130213916
iteration 15, loss = 0.000573403958696872
iteration 16, loss = 0.00046305739670060575
iteration 17, loss = 0.0004750880762003362
iteration 18, loss = 0.0004654889926314354
iteration 19, loss = 0.0005351186846382916
iteration 20, loss = 0.0004764847399201244
iteration 21, loss = 0.00048238271847367287
iteration 22, loss = 0.0005081172566860914
iteration 23, loss = 0.0004713861853815615
iteration 24, loss = 0.0005281568155623972
iteration 25, loss = 0.0005613727844320238
iteration 26, loss = 0.00046532793203368783
iteration 27, loss = 0.0005493245553225279
iteration 28, loss = 0.00045861839316785336
iteration 29, loss = 0.001348720514215529
iteration 30, loss = 0.0005156336119398475
iteration 31, loss = 0.0006834852392785251
iteration 32, loss = 0.0006599007756449282
iteration 33, loss = 0.0004768072976730764
iteration 34, loss = 0.00048758345656096935
iteration 35, loss = 0.0004922254011034966
iteration 36, loss = 0.00046550051774829626
iteration 37, loss = 0.0005548540502786636
iteration 38, loss = 0.0005593685200437903
iteration 39, loss = 0.00046739738900214434
iteration 40, loss = 0.0004677840624935925
iteration 41, loss = 0.0004981283564120531
iteration 42, loss = 0.0004701745056081563
iteration 43, loss = 0.0005049906321801245
iteration 44, loss = 0.0004940234357491136
iteration 45, loss = 0.0005067480960860848
iteration 46, loss = 0.00047757141874171793
iteration 47, loss = 0.0005269034299999475
iteration 48, loss = 0.0005078989779576659
iteration 49, loss = 0.000492939492687583
iteration 50, loss = 0.0004985162522643805
iteration 51, loss = 0.0004616614314727485
iteration 52, loss = 0.0004610001342371106
iteration 53, loss = 0.00044900982175022364
iteration 54, loss = 0.0004787965735886246
iteration 55, loss = 0.0004588718293234706
iteration 56, loss = 0.0006367866299115121
iteration 57, loss = 0.0008451499743387103
iteration 58, loss = 0.00047014624578878284
iteration 59, loss = 0.0007121749222278595
iteration 60, loss = 0.0005383792449720204
iteration 61, loss = 0.00048824551049619913
iteration 62, loss = 0.0005203660693950951
iteration 63, loss = 0.0004308860225137323
iteration 64, loss = 0.00046811619540676475
iteration 65, loss = 0.0004784304474014789
iteration 66, loss = 0.0004971950547769666
iteration 67, loss = 0.0016996663762256503
iteration 68, loss = 0.0014078252715989947
iteration 69, loss = 0.0006676254561170936
iteration 70, loss = 0.000787408440373838
iteration 71, loss = 0.0004483535885810852
iteration 72, loss = 0.001369187026284635
iteration 73, loss = 0.00048346619587391615
iteration 74, loss = 0.0005493759526871145
iteration 75, loss = 0.0013168383156880736
iteration 76, loss = 0.0005040748510509729
iteration 77, loss = 0.0004911944852210581
iteration 78, loss = 0.000685597478877753
iteration 79, loss = 0.0008036278304643929
iteration 80, loss = 0.00046581277274526656
iteration 81, loss = 0.0004520402872003615
iteration 82, loss = 0.0004993146285414696
iteration 83, loss = 0.0005136067629791796
iteration 84, loss = 0.0009403780568391085
iteration 85, loss = 0.000716405629646033
iteration 86, loss = 0.0005106145981699228
iteration 87, loss = 0.000815711566247046
iteration 88, loss = 0.0004946988774463534
iteration 89, loss = 0.00048620058805681765
iteration 90, loss = 0.0004924284294247627
iteration 91, loss = 0.00049643061356619
iteration 92, loss = 0.0005302659119479358
iteration 93, loss = 0.0007971720187924802
iteration 94, loss = 0.00043764367001131177
iteration 95, loss = 0.0004838576423935592
iteration 96, loss = 0.00046943643246777356
iteration 97, loss = 0.00048570006038062274
iteration 98, loss = 0.00042980065336450934
iteration 99, loss = 0.0006879590218886733
iteration 100, loss = 0.0004898296901956201
iteration 101, loss = 0.0004923161468468606
iteration 102, loss = 0.0005459420499391854
iteration 103, loss = 0.0005219411686994135
iteration 104, loss = 0.0004917413461953402
iteration 105, loss = 0.0004784893535543233
iteration 106, loss = 0.000470620027044788
iteration 107, loss = 0.00045343025703914464
iteration 108, loss = 0.0005190882366150618
iteration 109, loss = 0.0004813903651665896
iteration 110, loss = 0.0005024565616622567
iteration 111, loss = 0.0005293808644637465
iteration 112, loss = 0.0005006786668673158
iteration 113, loss = 0.0005279805045574903
iteration 114, loss = 0.000495629501529038
iteration 115, loss = 0.000512931845150888
iteration 116, loss = 0.0005716611631214619
iteration 117, loss = 0.0013725168537348509
iteration 118, loss = 0.0005190658266656101
iteration 119, loss = 0.0004389725800137967
iteration 120, loss = 0.0005052072228863835
iteration 121, loss = 0.0005237936275079846
iteration 122, loss = 0.0004775298002641648
iteration 123, loss = 0.0005182755994610488
iteration 124, loss = 0.0007670577615499496
iteration 125, loss = 0.000501105678267777
iteration 126, loss = 0.0004931045696139336
iteration 127, loss = 0.0004624903667718172
iteration 128, loss = 0.001382402959279716
iteration 129, loss = 0.0013714635279029608
iteration 130, loss = 0.00043712026672437787
iteration 131, loss = 0.0005147323245182633
iteration 132, loss = 0.0004933705786243081
iteration 133, loss = 0.0004897868493571877
iteration 134, loss = 0.0005855992203578353
iteration 135, loss = 0.00042524177115410566
iteration 136, loss = 0.0004858678439632058
iteration 137, loss = 0.0005849988665431738
iteration 138, loss = 0.0005085230222903192
iteration 139, loss = 0.0004906563553959131
iteration 140, loss = 0.0007978141075000167
iteration 141, loss = 0.00047392884152941406
iteration 142, loss = 0.0007686620228923857
iteration 143, loss = 0.0014468940207734704
iteration 144, loss = 0.0005254651769064367
iteration 145, loss = 0.0004660928389057517
iteration 146, loss = 0.0004076712648384273
iteration 147, loss = 0.00046247144928202033
iteration 148, loss = 0.0005282172933220863
iteration 149, loss = 0.000523096532560885
iteration 150, loss = 0.00048134822282008827
iteration 151, loss = 0.0006864224560558796
iteration 152, loss = 0.0004418566823005676
iteration 153, loss = 0.0005029580788686872
iteration 154, loss = 0.0005452261539176106
iteration 155, loss = 0.00045981150469742715
iteration 156, loss = 0.0004913456505164504
iteration 157, loss = 0.0005013322224840522
iteration 158, loss = 0.0005926466546952724
iteration 159, loss = 0.0005324656958691776
iteration 160, loss = 0.000476810586405918
iteration 161, loss = 0.0005309676053002477
iteration 162, loss = 0.0005526589811779559
iteration 163, loss = 0.0007342722965404391
iteration 164, loss = 0.000543686212040484
iteration 165, loss = 0.0009310899185948074
iteration 166, loss = 0.0010240331757813692
iteration 167, loss = 0.0005322041688486934
iteration 168, loss = 0.0005227063084021211
iteration 169, loss = 0.00047908371197991073
iteration 170, loss = 0.0005717859021387994
iteration 171, loss = 0.0005165948532521725
iteration 172, loss = 0.0005537570104934275
iteration 173, loss = 0.0004652922216337174
iteration 174, loss = 0.0005943768192082644
iteration 175, loss = 0.0007891320274211466
iteration 176, loss = 0.0006762180710211396
iteration 177, loss = 0.000499299552757293
iteration 178, loss = 0.000694164598826319
iteration 179, loss = 0.00047213624930009246
iteration 180, loss = 0.0005675640422850847
iteration 181, loss = 0.0004970591398887336
iteration 182, loss = 0.0007592943147756159
iteration 183, loss = 0.00048138719284906983
iteration 184, loss = 0.0005532276118174195
iteration 185, loss = 0.0005387504352256656
iteration 186, loss = 0.00053231610218063
iteration 187, loss = 0.0005132980295456946
iteration 188, loss = 0.0004751606029458344
iteration 189, loss = 0.0013846891233697534
iteration 190, loss = 0.0004869122349191457
iteration 191, loss = 0.0004981412785127759
iteration 192, loss = 0.00048534543020650744
iteration 193, loss = 0.000501478323712945
iteration 194, loss = 0.0004827584489248693
iteration 195, loss = 0.0004753624089062214
iteration 196, loss = 0.00047633174108341336
iteration 197, loss = 0.00045563658932223916
iteration 198, loss = 0.0004536968481261283
iteration 199, loss = 0.0005137439584359527
iteration 200, loss = 0.0006860308931209147
iteration 201, loss = 0.0005008377484045923
iteration 202, loss = 0.0004458977491594851
iteration 203, loss = 0.0004959340440109372
iteration 204, loss = 0.0004727980704046786
iteration 205, loss = 0.0005905473954044282
iteration 206, loss = 0.0004306498449295759
iteration 207, loss = 0.0005240048049017787
iteration 208, loss = 0.0005347884143702686
iteration 209, loss = 0.00048269188846461475
iteration 210, loss = 0.0007211578777059913
iteration 211, loss = 0.0005214248667471111
iteration 212, loss = 0.0005466914153657854
iteration 213, loss = 0.0004834292340092361
iteration 214, loss = 0.0006581348134204745
iteration 215, loss = 0.0005306348321028054
iteration 216, loss = 0.00046411287621594965
iteration 217, loss = 0.000525405746884644
iteration 218, loss = 0.0005129778292030096
iteration 219, loss = 0.00045426705037243664
iteration 220, loss = 0.0004690065106842667
iteration 221, loss = 0.0005984471063129604
iteration 222, loss = 0.00043331331107765436
iteration 223, loss = 0.0006747347069904208
iteration 224, loss = 0.000448611710453406
iteration 225, loss = 0.0004999565426260233
iteration 226, loss = 0.0005170703516341746
iteration 227, loss = 0.00046936157741583884
iteration 228, loss = 0.000468853599159047
iteration 229, loss = 0.00045163053437136114
iteration 230, loss = 0.00047583715058863163
iteration 231, loss = 0.0004525463446043432
iteration 232, loss = 0.0004642193962354213
iteration 233, loss = 0.0004314537509344518
iteration 234, loss = 0.000562007597181946
iteration 235, loss = 0.0004779521550517529
iteration 236, loss = 0.0016218238743022084
iteration 237, loss = 0.00045800069347023964
iteration 238, loss = 0.0005077298847027123
iteration 239, loss = 0.0005324643570929766
iteration 240, loss = 0.0004373887786641717
iteration 241, loss = 0.00046843953896313906
iteration 242, loss = 0.00046876625856384635
iteration 243, loss = 0.0005350945866666734
iteration 244, loss = 0.0004967615241184831
iteration 245, loss = 0.0004828696546610445
iteration 246, loss = 0.0006464965990744531
iteration 247, loss = 0.0004861771303694695
iteration 248, loss = 0.0004718147683888674
iteration 249, loss = 0.0004972091992385685
iteration 250, loss = 0.0006870702491141856
iteration 251, loss = 0.0005297611933201551
iteration 252, loss = 0.00045963324373587966
iteration 253, loss = 0.0004689596826210618
iteration 254, loss = 0.00044190470362082124
iteration 255, loss = 0.00045297472388483584
iteration 256, loss = 0.00046765641309320927
iteration 257, loss = 0.0004754550172947347
iteration 258, loss = 0.0004602902045007795
iteration 259, loss = 0.0004676818789448589
iteration 260, loss = 0.0005049472092650831
iteration 261, loss = 0.000514691520947963
iteration 262, loss = 0.0005108253681100905
iteration 263, loss = 0.0007200925028882921
iteration 264, loss = 0.000467583624413237
iteration 265, loss = 0.000499197281897068
iteration 266, loss = 0.0005249407840892673
iteration 267, loss = 0.00046966460649855435
iteration 268, loss = 0.000492038787342608
iteration 269, loss = 0.0005904775462113321
iteration 270, loss = 0.0004845058429054916
iteration 271, loss = 0.0004476867616176605
iteration 272, loss = 0.0005842536338604987
iteration 273, loss = 0.0004776772693730891
iteration 274, loss = 0.0013694959925487638
iteration 275, loss = 0.0004803422780241817
iteration 276, loss = 0.0004981984384357929
iteration 277, loss = 0.0005778299528174102
iteration 278, loss = 0.0004895262536592782
iteration 279, loss = 0.00044872210128232837
iteration 280, loss = 0.0008540561539120972
iteration 281, loss = 0.000524432398378849
iteration 282, loss = 0.00046747105079703033
iteration 283, loss = 0.0004371290560811758
iteration 284, loss = 0.0008022780530154705
iteration 285, loss = 0.0014432702446356416
iteration 286, loss = 0.0007033636793494225
iteration 287, loss = 0.0004294858081266284
iteration 288, loss = 0.000718309951480478
iteration 289, loss = 0.0015930293593555689
iteration 290, loss = 0.0005371401784941554
iteration 291, loss = 0.0006899118889123201
iteration 292, loss = 0.0006188020342960954
iteration 293, loss = 0.0006390786729753017
iteration 294, loss = 0.0007573036709800363
iteration 295, loss = 0.000535228755325079
iteration 296, loss = 0.0004900000058114529
iteration 297, loss = 0.0004868475953117013
iteration 298, loss = 0.0006435055984184146
iteration 299, loss = 0.0005001597455702722
iteration 0, loss = 0.001325921039097011
iteration 1, loss = 0.000460200069937855
iteration 2, loss = 0.0004809744714293629
iteration 3, loss = 0.00041454765596427023
iteration 4, loss = 0.000439468800323084
iteration 5, loss = 0.0005285900551825762
iteration 6, loss = 0.0007880340563133359
iteration 7, loss = 0.0004511387087404728
iteration 8, loss = 0.000511526654008776
iteration 9, loss = 0.0005313714500516653
iteration 10, loss = 0.00047962897224351764
iteration 11, loss = 0.0005737844039686024
iteration 12, loss = 0.0005565588944591582
iteration 13, loss = 0.0006639484199695289
iteration 14, loss = 0.0005605546175502241
iteration 15, loss = 0.0007557034841738641
iteration 16, loss = 0.0006500574527308345
iteration 17, loss = 0.0006598110194317997
iteration 18, loss = 0.0013664503348991275
iteration 19, loss = 0.0004397264856379479
iteration 20, loss = 0.000476695568067953
iteration 21, loss = 0.0004727542109321803
iteration 22, loss = 0.0004746518679894507
iteration 23, loss = 0.0008099124534055591
iteration 24, loss = 0.0005083584692329168
iteration 25, loss = 0.00048621115274727345
iteration 26, loss = 0.00047997498768381774
iteration 27, loss = 0.00048463745042681694
iteration 28, loss = 0.0006328326999209821
iteration 29, loss = 0.0004600093816407025
iteration 30, loss = 0.0005247150547802448
iteration 31, loss = 0.000553180230781436
iteration 32, loss = 0.0007273347000591457
iteration 33, loss = 0.0004641363047994673
iteration 34, loss = 0.00044632735080085695
iteration 35, loss = 0.0013982208911329508
iteration 36, loss = 0.0006158771575428545
iteration 37, loss = 0.0004755723930429667
iteration 38, loss = 0.0006132955313660204
iteration 39, loss = 0.0005432291654869914
iteration 40, loss = 0.0004721606965176761
iteration 41, loss = 0.0005150004290044308
iteration 42, loss = 0.0005635618581436574
iteration 43, loss = 0.00044291510130278766
iteration 44, loss = 0.0006079172599129379
iteration 45, loss = 0.0005478051025420427
iteration 46, loss = 0.0004479808558244258
iteration 47, loss = 0.0004571972822304815
iteration 48, loss = 0.0009346409351564944
iteration 49, loss = 0.00048236342263408005
iteration 50, loss = 0.0005291199777275324
iteration 51, loss = 0.0005180984735488892
iteration 52, loss = 0.00048189700464718044
iteration 53, loss = 0.0005177821149118245
iteration 54, loss = 0.00048250763211399317
iteration 55, loss = 0.000528679636772722
iteration 56, loss = 0.0008170053479261696
iteration 57, loss = 0.0004412053676787764
iteration 58, loss = 0.0004557339707389474
iteration 59, loss = 0.0005300290649756789
iteration 60, loss = 0.0004751236119773239
iteration 61, loss = 0.0004386237415019423
iteration 62, loss = 0.00046137109166011214
iteration 63, loss = 0.00044694211101159453
iteration 64, loss = 0.0009191680583171546
iteration 65, loss = 0.0007716791587881744
iteration 66, loss = 0.0005528065375983715
iteration 67, loss = 0.00048458133824169636
iteration 68, loss = 0.0004504839889705181
iteration 69, loss = 0.0005153981619514525
iteration 70, loss = 0.00045428474550135434
iteration 71, loss = 0.00045345473336055875
iteration 72, loss = 0.0004522027156781405
iteration 73, loss = 0.0005103738512843847
iteration 74, loss = 0.0004924747627228498
iteration 75, loss = 0.0004601770779117942
iteration 76, loss = 0.0004360434541013092
iteration 77, loss = 0.0006546237855218351
iteration 78, loss = 0.00045485139708034694
iteration 79, loss = 0.0013200542889535427
iteration 80, loss = 0.0004587794537656009
iteration 81, loss = 0.0008437211508862674
iteration 82, loss = 0.00046546602970920503
iteration 83, loss = 0.0004259649140294641
iteration 84, loss = 0.0005054765497334301
iteration 85, loss = 0.0004518087662290782
iteration 86, loss = 0.0007662780117243528
iteration 87, loss = 0.0004900475032627583
iteration 88, loss = 0.000490652397274971
iteration 89, loss = 0.00048443046398460865
iteration 90, loss = 0.00043888131040148437
iteration 91, loss = 0.0006822770228609443
iteration 92, loss = 0.0004233463841956109
iteration 93, loss = 0.0005512862699106336
iteration 94, loss = 0.00046954010031186044
iteration 95, loss = 0.0004804942582268268
iteration 96, loss = 0.00046666848356835544
iteration 97, loss = 0.0004983779508620501
iteration 98, loss = 0.0004500036302488297
iteration 99, loss = 0.0006411127396859229
iteration 100, loss = 0.0004729906504508108
iteration 101, loss = 0.001312604988925159
iteration 102, loss = 0.0005550957284867764
iteration 103, loss = 0.00042684574145823717
iteration 104, loss = 0.00047954634646885097
iteration 105, loss = 0.0006415052339434624
iteration 106, loss = 0.0004941407823935151
iteration 107, loss = 0.0004969527944922447
iteration 108, loss = 0.0005178338615223765
iteration 109, loss = 0.0004899968625977635
iteration 110, loss = 0.00043209746945649385
iteration 111, loss = 0.0004563832189887762
iteration 112, loss = 0.0004422252532094717
iteration 113, loss = 0.00047653086949139833
iteration 114, loss = 0.0004674218944273889
iteration 115, loss = 0.0005686281947419047
iteration 116, loss = 0.00044334662379696965
iteration 117, loss = 0.0004766436468344182
iteration 118, loss = 0.0013161639217287302
iteration 119, loss = 0.0008057717932388186
iteration 120, loss = 0.0004839337198063731
iteration 121, loss = 0.0006386357708834112
iteration 122, loss = 0.0005240953178144991
iteration 123, loss = 0.0004758820869028568
iteration 124, loss = 0.0007063521770760417
iteration 125, loss = 0.0004914100281894207
iteration 126, loss = 0.00045726762618869543
iteration 127, loss = 0.0005328537663444877
iteration 128, loss = 0.0005688542732968926
iteration 129, loss = 0.0004553436010610312
iteration 130, loss = 0.0004601553955581039
iteration 131, loss = 0.0004637730016838759
iteration 132, loss = 0.0004414278082549572
iteration 133, loss = 0.00043033118708990514
iteration 134, loss = 0.0004601149121299386
iteration 135, loss = 0.0005227815127000213
iteration 136, loss = 0.0004642723361030221
iteration 137, loss = 0.000493550265673548
iteration 138, loss = 0.00039882375858724117
iteration 139, loss = 0.0004538962384685874
iteration 140, loss = 0.0008016988867893815
iteration 141, loss = 0.000796651525888592
iteration 142, loss = 0.0005040293326601386
iteration 143, loss = 0.0005133827216923237
iteration 144, loss = 0.0006682748789899051
iteration 145, loss = 0.0004418348253238946
iteration 146, loss = 0.0005449484451673925
iteration 147, loss = 0.0008223779732361436
iteration 148, loss = 0.0004982634563930333
iteration 149, loss = 0.0004489394777920097
iteration 150, loss = 0.0004301713779568672
iteration 151, loss = 0.001307773170992732
iteration 152, loss = 0.0004426257510203868
iteration 153, loss = 0.000501415051985532
iteration 154, loss = 0.0013408699305728078
iteration 155, loss = 0.0004631495685316622
iteration 156, loss = 0.00044291140511631966
iteration 157, loss = 0.0004830400284845382
iteration 158, loss = 0.0004664302396122366
iteration 159, loss = 0.0013665694277733564
iteration 160, loss = 0.00046346193994395435
iteration 161, loss = 0.0004747019847854972
iteration 162, loss = 0.00045242946362122893
iteration 163, loss = 0.0004824560892302543
iteration 164, loss = 0.00044103083200752735
iteration 165, loss = 0.0005411701858974993
iteration 166, loss = 0.00046794619993306696
iteration 167, loss = 0.0004458252224139869
iteration 168, loss = 0.0005910217296332121
iteration 169, loss = 0.00045124496682547033
iteration 170, loss = 0.0006490936502814293
iteration 171, loss = 0.0006084943888708949
iteration 172, loss = 0.00047371111577376723
iteration 173, loss = 0.00047683168668299913
iteration 174, loss = 0.00044802893535234034
iteration 175, loss = 0.00044954143231734633
iteration 176, loss = 0.0004405653162393719
iteration 177, loss = 0.0004980182857252657
iteration 178, loss = 0.00048257349408231676
iteration 179, loss = 0.0006330956821329892
iteration 180, loss = 0.00043811238720081747
iteration 181, loss = 0.000548524665646255
iteration 182, loss = 0.0005748036783188581
iteration 183, loss = 0.0004278278793208301
iteration 184, loss = 0.00046097548329271376
iteration 185, loss = 0.00041747206705622375
iteration 186, loss = 0.0005472410121001303
iteration 187, loss = 0.0013207137817516923
iteration 188, loss = 0.00042208717786706984
iteration 189, loss = 0.00046648207353428006
iteration 190, loss = 0.0004775379493366927
iteration 191, loss = 0.000573428173083812
iteration 192, loss = 0.0004585266869980842
iteration 193, loss = 0.00045719544868916273
iteration 194, loss = 0.00048292925930581987
iteration 195, loss = 0.0007945378310978413
iteration 196, loss = 0.0007645009900443256
iteration 197, loss = 0.000500078487675637
iteration 198, loss = 0.00044699202408082783
iteration 199, loss = 0.0004152600886300206
iteration 200, loss = 0.0005575408576987684
iteration 201, loss = 0.0006456106784753501
iteration 202, loss = 0.0004967016284354031
iteration 203, loss = 0.000469601945951581
iteration 204, loss = 0.0006491432432085276
iteration 205, loss = 0.0004368024819996208
iteration 206, loss = 0.0004565842973534018
iteration 207, loss = 0.0005204356275498867
iteration 208, loss = 0.00042939430568367243
iteration 209, loss = 0.0013370419619604945
iteration 210, loss = 0.0004366828943602741
iteration 211, loss = 0.0007972457678988576
iteration 212, loss = 0.0016299871494993567
iteration 213, loss = 0.0004632260533981025
iteration 214, loss = 0.0004750471853185445
iteration 215, loss = 0.0004729552601929754
iteration 216, loss = 0.00047156569780781865
iteration 217, loss = 0.0004679305711761117
iteration 218, loss = 0.00046334750368259847
iteration 219, loss = 0.0004741844313684851
iteration 220, loss = 0.0004779069568030536
iteration 221, loss = 0.0005453458288684487
iteration 222, loss = 0.0005347385886125267
iteration 223, loss = 0.0004975099000148475
iteration 224, loss = 0.0006644224631600082
iteration 225, loss = 0.0005293927970342338
iteration 226, loss = 0.0007962341187521815
iteration 227, loss = 0.0005731799174100161
iteration 228, loss = 0.000439761090092361
iteration 229, loss = 0.0004899433697573841
iteration 230, loss = 0.0004845098883379251
iteration 231, loss = 0.0005178778665140271
iteration 232, loss = 0.0008466418366879225
iteration 233, loss = 0.000555269536562264
iteration 234, loss = 0.0013086074031889439
iteration 235, loss = 0.00047073757741600275
iteration 236, loss = 0.00042559500434435904
iteration 237, loss = 0.00040595789323560894
iteration 238, loss = 0.0005377741763368249
iteration 239, loss = 0.0004249932535458356
iteration 240, loss = 0.0004889899282716215
iteration 241, loss = 0.00045231898548081517
iteration 242, loss = 0.0005099890404380858
iteration 243, loss = 0.0005508961039595306
iteration 244, loss = 0.0004750546650029719
iteration 245, loss = 0.0006422018632292747
iteration 246, loss = 0.00048133532982319593
iteration 247, loss = 0.0005068141035735607
iteration 248, loss = 0.00045755962491966784
iteration 249, loss = 0.0005112666985951364
iteration 250, loss = 0.000474288099212572
iteration 251, loss = 0.00047170522157102823
iteration 252, loss = 0.000512258498929441
iteration 253, loss = 0.00043465031194500625
iteration 254, loss = 0.0005089693586342037
iteration 255, loss = 0.0005096610984764993
iteration 256, loss = 0.0004723377642221749
iteration 257, loss = 0.00047476679901592433
iteration 258, loss = 0.0005088102770969272
iteration 259, loss = 0.0006284682895056903
iteration 260, loss = 0.0007539807120338082
iteration 261, loss = 0.0004644586006179452
iteration 262, loss = 0.0004941070219501853
iteration 263, loss = 0.0004932997399009764
iteration 264, loss = 0.0006701580132357776
iteration 265, loss = 0.0012668055715039372
iteration 266, loss = 0.0005507474998012185
iteration 267, loss = 0.0006126073421910405
iteration 268, loss = 0.00043163885129615664
iteration 269, loss = 0.0004745926707983017
iteration 270, loss = 0.0005273756687529385
iteration 271, loss = 0.00044843321666121483
iteration 272, loss = 0.00041702139424160123
iteration 273, loss = 0.00046500132884830236
iteration 274, loss = 0.0005019604577682912
iteration 275, loss = 0.0005210538511164486
iteration 276, loss = 0.0012606136733666062
iteration 277, loss = 0.0004973696195520461
iteration 278, loss = 0.0004643556894734502
iteration 279, loss = 0.0005167656927369535
iteration 280, loss = 0.00048738584155216813
iteration 281, loss = 0.0004246086173225194
iteration 282, loss = 0.0004688045009970665
iteration 283, loss = 0.00046524847857654095
iteration 284, loss = 0.00044272420927882195
iteration 285, loss = 0.00048808843712322414
iteration 286, loss = 0.0004918801714666188
iteration 287, loss = 0.0004979367949999869
iteration 288, loss = 0.0007911692373454571
iteration 289, loss = 0.0005219408194534481
iteration 290, loss = 0.0004781911848112941
iteration 291, loss = 0.0004611341573763639
iteration 292, loss = 0.0004474218003451824
iteration 293, loss = 0.0004712618247140199
iteration 294, loss = 0.00039743201341480017
iteration 295, loss = 0.00044193462235853076
iteration 296, loss = 0.0004475496243685484
iteration 297, loss = 0.0004911069408990443
iteration 298, loss = 0.00042693872819654644
iteration 299, loss = 0.00047229378833435476

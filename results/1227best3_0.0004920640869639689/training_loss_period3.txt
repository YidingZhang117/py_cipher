iteration 1, loss = 0.003102709539234638
iteration 2, loss = 0.0023664128966629505
iteration 3, loss = 0.0020687617361545563
iteration 4, loss = 0.005894900299608707
iteration 5, loss = 0.0018002833239734173
iteration 6, loss = 0.0018997025908902287
iteration 7, loss = 0.0019095714669674635
iteration 8, loss = 0.0017867002170532942
iteration 9, loss = 0.0022034586872905493
iteration 10, loss = 0.0016115433536469936
iteration 11, loss = 0.0019977958872914314
iteration 12, loss = 0.0038361407350748777
iteration 13, loss = 0.002557980129495263
iteration 14, loss = 0.0025995341129601
iteration 15, loss = 0.0021948760841041803
iteration 16, loss = 0.0019206441938877106
iteration 17, loss = 0.003824432147666812
iteration 18, loss = 0.001890905317850411
iteration 19, loss = 0.0020609551575034857
iteration 20, loss = 0.0026446422562003136
iteration 21, loss = 0.0020291234832257032
iteration 22, loss = 0.002076107542961836
iteration 23, loss = 0.0018365945434197783
iteration 24, loss = 0.0020096218213438988
iteration 25, loss = 0.0019420961616560817
iteration 26, loss = 0.0035831548739224672
iteration 27, loss = 0.0024991098325699568
iteration 28, loss = 0.0019893646240234375
iteration 29, loss = 0.0028024199418723583
iteration 30, loss = 0.0023482220713049173
iteration 31, loss = 0.0024962755851447582
iteration 32, loss = 0.002119772369042039
iteration 33, loss = 0.002541674068197608
iteration 34, loss = 0.0020135599188506603
iteration 35, loss = 0.0018768440932035446
iteration 36, loss = 0.0019998180214315653
iteration 37, loss = 0.003241607453674078
iteration 38, loss = 0.002191483974456787
iteration 39, loss = 0.002347602043300867
iteration 40, loss = 0.002766995457932353
iteration 41, loss = 0.002076962497085333
iteration 42, loss = 0.0016938545741140842
iteration 43, loss = 0.0021331431344151497
iteration 44, loss = 0.001935938373208046
iteration 45, loss = 0.0026742599438875914
iteration 46, loss = 0.0018338439986109734
iteration 47, loss = 0.0018781330436468124
iteration 48, loss = 0.0029955990612506866
iteration 49, loss = 0.004937977530062199
iteration 50, loss = 0.0020020853262394667
iteration 51, loss = 0.001589727122336626
iteration 52, loss = 0.0017656998243182898
iteration 53, loss = 0.0019726939499378204
iteration 54, loss = 0.0019467466045171022
iteration 55, loss = 0.0019759247079491615
iteration 56, loss = 0.001953266328200698
iteration 57, loss = 0.0016771842492744327
iteration 58, loss = 0.0022594365291297436
iteration 59, loss = 0.0015569725073873997
iteration 60, loss = 0.0020199683494865894
iteration 61, loss = 0.0021307049319148064
iteration 62, loss = 0.0020338522735983133
iteration 63, loss = 0.001819653669372201
iteration 64, loss = 0.002019448671489954
iteration 65, loss = 0.0018759872764348984
iteration 66, loss = 0.0025436184369027615
iteration 67, loss = 0.002412095433101058
iteration 68, loss = 0.0019860530737787485
iteration 69, loss = 0.0037303275894373655
iteration 70, loss = 0.0019014297286048532
iteration 71, loss = 0.0021086828783154488
iteration 72, loss = 0.002159344730898738
iteration 73, loss = 0.0026061213575303555
iteration 74, loss = 0.0022523284424096346
iteration 75, loss = 0.002685828600078821
iteration 76, loss = 0.001724777277559042
iteration 77, loss = 0.002399697434157133
iteration 78, loss = 0.001839812844991684
iteration 79, loss = 0.0032019929494708776
iteration 80, loss = 0.0020114153157919645
iteration 81, loss = 0.0016102358931675553
iteration 82, loss = 0.0023730823304504156
iteration 83, loss = 0.0026183004956692457
iteration 84, loss = 0.001966032199561596
iteration 85, loss = 0.002170226303860545
iteration 86, loss = 0.0021210957784205675
iteration 87, loss = 0.0024944888427853584
iteration 88, loss = 0.002136450493708253
iteration 89, loss = 0.0017389709828421474
iteration 90, loss = 0.0024147238582372665
iteration 91, loss = 0.001652992912568152
iteration 92, loss = 0.0028411871753633022
iteration 93, loss = 0.0018241680227220058
iteration 94, loss = 0.001644963282160461
iteration 95, loss = 0.0038149463944137096
iteration 96, loss = 0.0020878463983535767
iteration 97, loss = 0.0018392622005194426
iteration 98, loss = 0.001801736536435783
iteration 99, loss = 0.0049399384297430515
iteration 100, loss = 0.0020013751927763224
iteration 101, loss = 0.0033315124455839396
iteration 102, loss = 0.0020470437593758106
iteration 103, loss = 0.002513524377718568
iteration 104, loss = 0.0018453699303790927
iteration 105, loss = 0.003811500035226345
iteration 106, loss = 0.002156402450054884
iteration 107, loss = 0.002078056801110506
iteration 108, loss = 0.0021597982849925756
iteration 109, loss = 0.0019231761107221246
iteration 110, loss = 0.003079427173361182
iteration 111, loss = 0.0016720743151381612
iteration 112, loss = 0.0028576364275068045
iteration 113, loss = 0.002591905416920781
iteration 114, loss = 0.0016989458817988634
iteration 115, loss = 0.0032128123566508293
iteration 116, loss = 0.0016759292921051383
iteration 117, loss = 0.0016719273990020156
iteration 118, loss = 0.0017814603634178638
iteration 119, loss = 0.002345126820728183
iteration 120, loss = 0.002271524630486965
iteration 121, loss = 0.001892444328404963
iteration 122, loss = 0.00238398602232337
iteration 123, loss = 0.0016451467527076602
iteration 124, loss = 0.001594845554791391
iteration 125, loss = 0.0021727115381509066
iteration 126, loss = 0.002690575085580349
iteration 127, loss = 0.002215469488874078
iteration 128, loss = 0.0017193637322634459
iteration 129, loss = 0.003638605121523142
iteration 130, loss = 0.0016109865391626954
iteration 131, loss = 0.0020067503210157156
iteration 132, loss = 0.0017091032350435853
iteration 133, loss = 0.0017493959749117494
iteration 134, loss = 0.002266563707962632
iteration 135, loss = 0.0023211196530610323
iteration 136, loss = 0.0015782845439389348
iteration 137, loss = 0.0019552763551473618
iteration 138, loss = 0.0016869105165824294
iteration 139, loss = 0.0021944076288491488
iteration 140, loss = 0.0025293438229709864
iteration 141, loss = 0.0019365025218576193
iteration 142, loss = 0.0020626652985811234
iteration 143, loss = 0.0037200357764959335
iteration 144, loss = 0.0015239131171256304
iteration 145, loss = 0.0016806048806756735
iteration 146, loss = 0.004687086679041386
iteration 147, loss = 0.0020620916038751602
iteration 148, loss = 0.002163458615541458
iteration 149, loss = 0.001951478305272758
iteration 150, loss = 0.001664300449192524
iteration 151, loss = 0.0016814563423395157
iteration 152, loss = 0.0037872199900448322
iteration 153, loss = 0.0019510886631906033
iteration 154, loss = 0.0017783851362764835
iteration 155, loss = 0.0019543266389518976
iteration 156, loss = 0.0019067376852035522
iteration 157, loss = 0.00175751862116158
iteration 158, loss = 0.0019942396320402622
iteration 159, loss = 0.001751325442455709
iteration 160, loss = 0.0018923503812402487
iteration 161, loss = 0.0018486140761524439
iteration 162, loss = 0.0019070669077336788
iteration 163, loss = 0.002089818473905325
iteration 164, loss = 0.001773927011527121
iteration 165, loss = 0.0019205146236345172
iteration 166, loss = 0.0019377078860998154
iteration 167, loss = 0.00254153017885983
iteration 168, loss = 0.001981259323656559
iteration 169, loss = 0.0018250300781801343
iteration 170, loss = 0.002178254071623087
iteration 171, loss = 0.0020454018376767635
iteration 172, loss = 0.0018933193059638143
iteration 173, loss = 0.002363838255405426
iteration 174, loss = 0.002799278125166893
iteration 175, loss = 0.002815802814438939
iteration 176, loss = 0.0016247463645413518
iteration 177, loss = 0.002229205798357725
iteration 178, loss = 0.0017511406913399696
iteration 179, loss = 0.0017522420966997743
iteration 180, loss = 0.0017469890881329775
iteration 181, loss = 0.0017791971331462264
iteration 182, loss = 0.0020261609461158514
iteration 183, loss = 0.0018933872925117612
iteration 184, loss = 0.0019466386875137687
iteration 185, loss = 0.0038892815355211496
iteration 186, loss = 0.0018296320922672749
iteration 187, loss = 0.0020602254662662745
iteration 188, loss = 0.0020201550796628
iteration 189, loss = 0.0018934395629912615
iteration 190, loss = 0.0018657359760254622
iteration 191, loss = 0.0026335108559578657
iteration 192, loss = 0.0018048163037747145
iteration 193, loss = 0.0032106046564877033
iteration 194, loss = 0.0026663504540920258
iteration 195, loss = 0.0029376300517469645
iteration 196, loss = 0.00225389888510108
iteration 197, loss = 0.001664568786509335
iteration 198, loss = 0.0020986951421946287
iteration 199, loss = 0.0016677207313477993
iteration 200, loss = 0.0017130781197920442
iteration 201, loss = 0.0018954287515953183
iteration 202, loss = 0.001575260772369802
iteration 203, loss = 0.004316291771829128
iteration 204, loss = 0.001894598943181336
iteration 205, loss = 0.0020474146585911512
iteration 206, loss = 0.002014080062508583
iteration 207, loss = 0.0016715038800612092
iteration 208, loss = 0.0018376745283603668
iteration 209, loss = 0.0016349304933100939
iteration 210, loss = 0.002915422897785902
iteration 211, loss = 0.001823117258027196
iteration 212, loss = 0.0016552750021219254
iteration 213, loss = 0.002166644437238574
iteration 214, loss = 0.0024365789722651243
iteration 215, loss = 0.0027603241614997387
iteration 216, loss = 0.002641537692397833
iteration 217, loss = 0.0031860254239290953
iteration 218, loss = 0.0017507888842374086
iteration 219, loss = 0.001794191193766892
iteration 220, loss = 0.001667840639129281
iteration 221, loss = 0.0017648281063884497
iteration 222, loss = 0.0018450076458975673
iteration 223, loss = 0.001953180180862546
iteration 224, loss = 0.0016727213514968753
iteration 225, loss = 0.0027180365286767483
iteration 226, loss = 0.0018084951443597674
iteration 227, loss = 0.0017494929488748312
iteration 228, loss = 0.002004168229177594
iteration 229, loss = 0.0017621135339140892
iteration 230, loss = 0.0019832162652164698
iteration 231, loss = 0.0028491008561104536
iteration 232, loss = 0.0019984582904726267
iteration 233, loss = 0.0024687922559678555
iteration 234, loss = 0.002185251098126173
iteration 235, loss = 0.0019818611908704042
iteration 236, loss = 0.0018688065465539694
iteration 237, loss = 0.0020462768152356148
iteration 238, loss = 0.0021820792462676764
iteration 239, loss = 0.0022315243259072304
iteration 240, loss = 0.001525814994238317
iteration 241, loss = 0.0014638482825830579
iteration 242, loss = 0.0033748000860214233
iteration 243, loss = 0.002685286570340395
iteration 244, loss = 0.001727282884530723
iteration 245, loss = 0.002249306533485651
iteration 246, loss = 0.0029167067259550095
iteration 247, loss = 0.001655329018831253
iteration 248, loss = 0.001672451151534915
iteration 249, loss = 0.0016072643920779228
iteration 250, loss = 0.0014802094083279371
iteration 251, loss = 0.0018222006037831306
iteration 252, loss = 0.002115202136337757
iteration 253, loss = 0.002634649397805333
iteration 254, loss = 0.0017991388449445367
iteration 255, loss = 0.0019192848121747375
iteration 256, loss = 0.0018128169467672706
iteration 257, loss = 0.0017173975938931108
iteration 258, loss = 0.001869715517386794
iteration 259, loss = 0.0018079773290082812
iteration 260, loss = 0.0016769192880019546
iteration 261, loss = 0.0017935482319444418
iteration 262, loss = 0.001863286830484867
iteration 263, loss = 0.001605201861821115
iteration 264, loss = 0.004139884375035763
iteration 265, loss = 0.0018209114205092192
iteration 266, loss = 0.0017929394962266088
iteration 267, loss = 0.002096686279401183
iteration 268, loss = 0.002041788538917899
iteration 269, loss = 0.0017768500838428736
iteration 270, loss = 0.0017451130552217364
iteration 271, loss = 0.002177231479436159
iteration 272, loss = 0.002316598780453205
iteration 273, loss = 0.003210822818800807
iteration 274, loss = 0.0025181667879223824
iteration 275, loss = 0.0014999588020145893
iteration 276, loss = 0.0018377227243036032
iteration 277, loss = 0.001823918428272009
iteration 278, loss = 0.0026623953599482775
iteration 279, loss = 0.0018113437108695507
iteration 280, loss = 0.0017597127007320523
iteration 281, loss = 0.0015525511698797345
iteration 282, loss = 0.0018521295860409737
iteration 283, loss = 0.0023185231257230043
iteration 284, loss = 0.0023187282495200634
iteration 285, loss = 0.0021924918983131647
iteration 286, loss = 0.002233682433143258
iteration 287, loss = 0.001967773772776127
iteration 288, loss = 0.0035657130647450686
iteration 289, loss = 0.0015362349804490805
iteration 290, loss = 0.0024383170530200005
iteration 291, loss = 0.002036389894783497
iteration 292, loss = 0.002165603917092085
iteration 293, loss = 0.0015511262463405728
iteration 294, loss = 0.002635466866195202
iteration 295, loss = 0.0015749457525089383
iteration 296, loss = 0.002088353270664811
iteration 297, loss = 0.0016842703334987164
iteration 298, loss = 0.0016944165108725429
iteration 299, loss = 0.0019715060479938984
iteration 300, loss = 0.0015218020416796207
iteration 1, loss = 0.0028688106685876846
iteration 2, loss = 0.004214800428599119
iteration 3, loss = 0.002322012558579445
iteration 4, loss = 0.0021772293839603662
iteration 5, loss = 0.0016372790560126305
iteration 6, loss = 0.0016940946225076914
iteration 7, loss = 0.0019268168834969401
iteration 8, loss = 0.001928427373059094
iteration 9, loss = 0.002276959363371134
iteration 10, loss = 0.0022230506874620914
iteration 11, loss = 0.0017888827715069056
iteration 12, loss = 0.001644176198169589
iteration 13, loss = 0.002333146519958973
iteration 14, loss = 0.0017280160682275891
iteration 15, loss = 0.001736800535582006
iteration 16, loss = 0.002427853876724839
iteration 17, loss = 0.0018611556151881814
iteration 18, loss = 0.00288709276355803
iteration 19, loss = 0.0017804966773837805
iteration 20, loss = 0.0026701590977609158
iteration 21, loss = 0.0017761935014277697
iteration 22, loss = 0.001830776804126799
iteration 23, loss = 0.002252423670142889
iteration 24, loss = 0.0018873289227485657
iteration 25, loss = 0.001676080166362226
iteration 26, loss = 0.002517194487154484
iteration 27, loss = 0.0017749336548149586
iteration 28, loss = 0.0024517483543604612
iteration 29, loss = 0.001726125250570476
iteration 30, loss = 0.003364869626238942
iteration 31, loss = 0.0014972375938668847
iteration 32, loss = 0.0017593728844076395
iteration 33, loss = 0.0029816417954862118
iteration 34, loss = 0.0019615106284618378
iteration 35, loss = 0.0019323707092553377
iteration 36, loss = 0.0013972338056191802
iteration 37, loss = 0.0015032708179205656
iteration 38, loss = 0.0018759858794510365
iteration 39, loss = 0.00256445724517107
iteration 40, loss = 0.0016631655162200332
iteration 41, loss = 0.0019601485691964626
iteration 42, loss = 0.0015227248659357429
iteration 43, loss = 0.003081232775002718
iteration 44, loss = 0.002768412698060274
iteration 45, loss = 0.0023342547938227654
iteration 46, loss = 0.002100404817610979
iteration 47, loss = 0.001910289516672492
iteration 48, loss = 0.0015901218866929412
iteration 49, loss = 0.001948098884895444
iteration 50, loss = 0.0014230035012587905
iteration 51, loss = 0.0020525436848402023
iteration 52, loss = 0.002470807172358036
iteration 53, loss = 0.0017800988862290978
iteration 54, loss = 0.0017616996774449944
iteration 55, loss = 0.0014748804969713092
iteration 56, loss = 0.001894202083349228
iteration 57, loss = 0.0019390047527849674
iteration 58, loss = 0.0014091681223362684
iteration 59, loss = 0.001956735737621784
iteration 60, loss = 0.002272601705044508
iteration 61, loss = 0.0018487399211153388
iteration 62, loss = 0.0022271382622420788
iteration 63, loss = 0.0017953823553398252
iteration 64, loss = 0.0016899118199944496
iteration 65, loss = 0.0017744966316968203
iteration 66, loss = 0.0016768735367804766
iteration 67, loss = 0.002191278152167797
iteration 68, loss = 0.0027804223354905844
iteration 69, loss = 0.0016764248721301556
iteration 70, loss = 0.0016905213706195354
iteration 71, loss = 0.00154877791646868
iteration 72, loss = 0.002346386667340994
iteration 73, loss = 0.0015426648315042257
iteration 74, loss = 0.0022003932390362024
iteration 75, loss = 0.004775441717356443
iteration 76, loss = 0.0018375131767243147
iteration 77, loss = 0.0017529191682115197
iteration 78, loss = 0.0016967116389423609
iteration 79, loss = 0.0016467473469674587
iteration 80, loss = 0.0021996302530169487
iteration 81, loss = 0.003140633925795555
iteration 82, loss = 0.0015272415475919843
iteration 83, loss = 0.0013725326862186193
iteration 84, loss = 0.0016164092812687159
iteration 85, loss = 0.0019273075740784407
iteration 86, loss = 0.0018564998172223568
iteration 87, loss = 0.0017781571950763464
iteration 88, loss = 0.0014392361044883728
iteration 89, loss = 0.001728928298689425
iteration 90, loss = 0.0018713889876380563
iteration 91, loss = 0.001438214909285307
iteration 92, loss = 0.003209466114640236
iteration 93, loss = 0.0020270198583602905
iteration 94, loss = 0.0016220795223489404
iteration 95, loss = 0.0015342473052442074
iteration 96, loss = 0.00154721993021667
iteration 97, loss = 0.001546549960039556
iteration 98, loss = 0.0018190991831943393
iteration 99, loss = 0.0019626279827207327
iteration 100, loss = 0.001643375027924776
iteration 101, loss = 0.001876395894214511
iteration 102, loss = 0.0021885696332901716
iteration 103, loss = 0.0015167376259341836
iteration 104, loss = 0.0014890102902427316
iteration 105, loss = 0.0016275024972856045
iteration 106, loss = 0.0022190106101334095
iteration 107, loss = 0.0017399656353518367
iteration 108, loss = 0.0016732802614569664
iteration 109, loss = 0.0024130765814334154
iteration 110, loss = 0.0015836352249607444
iteration 111, loss = 0.0017020208761096
iteration 112, loss = 0.0034254093188792467
iteration 113, loss = 0.0019855296704918146
iteration 114, loss = 0.0017402132507413626
iteration 115, loss = 0.001967244315892458
iteration 116, loss = 0.0016491262940689921
iteration 117, loss = 0.003414347767829895
iteration 118, loss = 0.00235716812312603
iteration 119, loss = 0.0019167866557836533
iteration 120, loss = 0.0016265602316707373
iteration 121, loss = 0.0034037011209875345
iteration 122, loss = 0.001990005373954773
iteration 123, loss = 0.0016919414047151804
iteration 124, loss = 0.0019269485492259264
iteration 125, loss = 0.00215672655031085
iteration 126, loss = 0.0025039652828127146
iteration 127, loss = 0.001790184760466218
iteration 128, loss = 0.0017121380660682917
iteration 129, loss = 0.0021674882154911757
iteration 130, loss = 0.002073038602247834
iteration 131, loss = 0.002550796139985323
iteration 132, loss = 0.002000638749450445
iteration 133, loss = 0.002056481782346964
iteration 134, loss = 0.0015820885309949517
iteration 135, loss = 0.0021604353096336126
iteration 136, loss = 0.0017622241284698248
iteration 137, loss = 0.001983201364055276
iteration 138, loss = 0.001954195089638233
iteration 139, loss = 0.0028183404356241226
iteration 140, loss = 0.0014132948126643896
iteration 141, loss = 0.003259549615904689
iteration 142, loss = 0.0017292604316025972
iteration 143, loss = 0.0019630687311291695
iteration 144, loss = 0.0023956364020705223
iteration 145, loss = 0.002234996762126684
iteration 146, loss = 0.0014219805598258972
iteration 147, loss = 0.0017118678661063313
iteration 148, loss = 0.0017553158104419708
iteration 149, loss = 0.0015203237999230623
iteration 150, loss = 0.0029466086998581886
iteration 151, loss = 0.0023495524656027555
iteration 152, loss = 0.0015484633622691035
iteration 153, loss = 0.0014522221172228456
iteration 154, loss = 0.0023408581037074327
iteration 155, loss = 0.0027054823003709316
iteration 156, loss = 0.0015365484869107604
iteration 157, loss = 0.0017448484431952238
iteration 158, loss = 0.0016048182733356953
iteration 159, loss = 0.0014947874005883932
iteration 160, loss = 0.002123001031577587
iteration 161, loss = 0.0016549300635233521
iteration 162, loss = 0.0022476676385849714
iteration 163, loss = 0.0026279031299054623
iteration 164, loss = 0.002281805267557502
iteration 165, loss = 0.0022003455087542534
iteration 166, loss = 0.0018535468261688948
iteration 167, loss = 0.0016784108011052012
iteration 168, loss = 0.0015087102074176073
iteration 169, loss = 0.001593429478816688
iteration 170, loss = 0.0015803024871274829
iteration 171, loss = 0.0016633833292871714
iteration 172, loss = 0.001710897427983582
iteration 173, loss = 0.001543260645121336
iteration 174, loss = 0.001666804077103734
iteration 175, loss = 0.003239982295781374
iteration 176, loss = 0.0015167860547080636
iteration 177, loss = 0.0015929610235616565
iteration 178, loss = 0.0014427967835217714
iteration 179, loss = 0.0014086696319282055
iteration 180, loss = 0.0022027106024324894
iteration 181, loss = 0.0013974506873637438
iteration 182, loss = 0.0016646654112264514
iteration 183, loss = 0.0014406850095838308
iteration 184, loss = 0.0016465667868033051
iteration 185, loss = 0.002146038692444563
iteration 186, loss = 0.001589665305800736
iteration 187, loss = 0.00190601940266788
iteration 188, loss = 0.0016534997848793864
iteration 189, loss = 0.0018504774197936058
iteration 190, loss = 0.0017730297986418009
iteration 191, loss = 0.001635728869587183
iteration 192, loss = 0.0018035565735772252
iteration 193, loss = 0.0026557070668786764
iteration 194, loss = 0.001865696394816041
iteration 195, loss = 0.0017749161925166845
iteration 196, loss = 0.0018804791616275907
iteration 197, loss = 0.0018714586040005088
iteration 198, loss = 0.0017199621070176363
iteration 199, loss = 0.0017513784114271402
iteration 200, loss = 0.002159593626856804
iteration 201, loss = 0.0018084895564243197
iteration 202, loss = 0.001752649899572134
iteration 203, loss = 0.0018586426740512252
iteration 204, loss = 0.0021309650037437677
iteration 205, loss = 0.0020217779092490673
iteration 206, loss = 0.0017166398465633392
iteration 207, loss = 0.0018451843643561006
iteration 208, loss = 0.0015540305757895112
iteration 209, loss = 0.0023774676956236362
iteration 210, loss = 0.0015339729143306613
iteration 211, loss = 0.0017704074271023273
iteration 212, loss = 0.0018541549798101187
iteration 213, loss = 0.002753917360678315
iteration 214, loss = 0.0020724618807435036
iteration 215, loss = 0.0020571392960846424
iteration 216, loss = 0.0035192989744246006
iteration 217, loss = 0.0013860794715583324
iteration 218, loss = 0.002371968701481819
iteration 219, loss = 0.0014714383287355304
iteration 220, loss = 0.0015260374639183283
iteration 221, loss = 0.0017717455048114061
iteration 222, loss = 0.001541375881060958
iteration 223, loss = 0.00251801242120564
iteration 224, loss = 0.00186609395314008
iteration 225, loss = 0.0014958636602386832
iteration 226, loss = 0.0015610530972480774
iteration 227, loss = 0.0023419393692165613
iteration 228, loss = 0.001444809720851481
iteration 229, loss = 0.0016465883236378431
iteration 230, loss = 0.00181420031003654
iteration 231, loss = 0.0015179088804870844
iteration 232, loss = 0.003324777353554964
iteration 233, loss = 0.0017120101256296039
iteration 234, loss = 0.0033513540402054787
iteration 235, loss = 0.001834087073802948
iteration 236, loss = 0.0019411523826420307
iteration 237, loss = 0.002112825633957982
iteration 238, loss = 0.0029248434584587812
iteration 239, loss = 0.0017174393869936466
iteration 240, loss = 0.0014318479225039482
iteration 241, loss = 0.0019341569859534502
iteration 242, loss = 0.002818163251504302
iteration 243, loss = 0.00230435305275023
iteration 244, loss = 0.001494083320721984
iteration 245, loss = 0.0016735875979065895
iteration 246, loss = 0.0020618883427232504
iteration 247, loss = 0.0028232275508344173
iteration 248, loss = 0.002419060096144676
iteration 249, loss = 0.0027711489237844944
iteration 250, loss = 0.0026163074653595686
iteration 251, loss = 0.0018114703707396984
iteration 252, loss = 0.002119212644174695
iteration 253, loss = 0.0016071376157924533
iteration 254, loss = 0.00214423262514174
iteration 255, loss = 0.0015078787691891193
iteration 256, loss = 0.0017998311668634415
iteration 257, loss = 0.0020738118328154087
iteration 258, loss = 0.0016922254581004381
iteration 259, loss = 0.0013956193579360843
iteration 260, loss = 0.0014525773003697395
iteration 261, loss = 0.0014394510071724653
iteration 262, loss = 0.0016251906054094434
iteration 263, loss = 0.0016440150793641806
iteration 264, loss = 0.0017436768393963575
iteration 265, loss = 0.0016386633506044745
iteration 266, loss = 0.0019871292170137167
iteration 267, loss = 0.0018112021498382092
iteration 268, loss = 0.0013979696668684483
iteration 269, loss = 0.0022092037834227085
iteration 270, loss = 0.0014046058058738708
iteration 271, loss = 0.0022866735234856606
iteration 272, loss = 0.0019798832945525646
iteration 273, loss = 0.0022984982933849096
iteration 274, loss = 0.0016822960460558534
iteration 275, loss = 0.0016306177712976933
iteration 276, loss = 0.0020399000495672226
iteration 277, loss = 0.0014384050155058503
iteration 278, loss = 0.005015056114643812
iteration 279, loss = 0.0016492267604917288
iteration 280, loss = 0.0019214395433664322
iteration 281, loss = 0.001808440312743187
iteration 282, loss = 0.0015597023302689195
iteration 283, loss = 0.003152657300233841
iteration 284, loss = 0.0020195336546748877
iteration 285, loss = 0.0014260930474847555
iteration 286, loss = 0.003150105243548751
iteration 287, loss = 0.001493123359978199
iteration 288, loss = 0.002073082607239485
iteration 289, loss = 0.0015220590867102146
iteration 290, loss = 0.0014001581585034728
iteration 291, loss = 0.001839032513089478
iteration 292, loss = 0.002048460068181157
iteration 293, loss = 0.00197811471298337
iteration 294, loss = 0.002440493553876877
iteration 295, loss = 0.001660640467889607
iteration 296, loss = 0.0025383755564689636
iteration 297, loss = 0.001489774091169238
iteration 298, loss = 0.001627816935069859
iteration 299, loss = 0.0016719463746994734
iteration 300, loss = 0.001690551987849176
iteration 1, loss = 0.001440480351448059
iteration 2, loss = 0.0017702348995953798
iteration 3, loss = 0.0015685918042436242
iteration 4, loss = 0.0017034648917615414
iteration 5, loss = 0.0014198533026501536
iteration 6, loss = 0.0023363984655588865
iteration 7, loss = 0.0028824610635638237
iteration 8, loss = 0.0033741798251867294
iteration 9, loss = 0.0013478887267410755
iteration 10, loss = 0.001832599751651287
iteration 11, loss = 0.002055695280432701
iteration 12, loss = 0.0020549073815345764
iteration 13, loss = 0.0016884910874068737
iteration 14, loss = 0.0017080330289900303
iteration 15, loss = 0.0014164731837809086
iteration 16, loss = 0.0013042313512414694
iteration 17, loss = 0.001573818502947688
iteration 18, loss = 0.0018241710495203733
iteration 19, loss = 0.0014762741047888994
iteration 20, loss = 0.0030444746371358633
iteration 21, loss = 0.0017788566183298826
iteration 22, loss = 0.0017894415650516748
iteration 23, loss = 0.0015728946309536695
iteration 24, loss = 0.0014334296574816108
iteration 25, loss = 0.0014392939629033208
iteration 26, loss = 0.0030905946623533964
iteration 27, loss = 0.0021098454017192125
iteration 28, loss = 0.0013484990922734141
iteration 29, loss = 0.001782204257324338
iteration 30, loss = 0.0028068185783922672
iteration 31, loss = 0.0013481639325618744
iteration 32, loss = 0.001776397810317576
iteration 33, loss = 0.0015371899353340268
iteration 34, loss = 0.0020195862744003534
iteration 35, loss = 0.0014722574269399047
iteration 36, loss = 0.0015268990537151694
iteration 37, loss = 0.0013536064652726054
iteration 38, loss = 0.0020599733106791973
iteration 39, loss = 0.0019013293785974383
iteration 40, loss = 0.0016741438303142786
iteration 41, loss = 0.0017470112070441246
iteration 42, loss = 0.001578916679136455
iteration 43, loss = 0.0016701485728845
iteration 44, loss = 0.00143386609852314
iteration 45, loss = 0.0015489055076614022
iteration 46, loss = 0.0024658036418259144
iteration 47, loss = 0.00173557095695287
iteration 48, loss = 0.0018737060017883778
iteration 49, loss = 0.001930468250066042
iteration 50, loss = 0.0015792789636179805
iteration 51, loss = 0.0042755878530442715
iteration 52, loss = 0.00322435749694705
iteration 53, loss = 0.0014841887168586254
iteration 54, loss = 0.0016592710744589567
iteration 55, loss = 0.0015042641898617148
iteration 56, loss = 0.0013969214633107185
iteration 57, loss = 0.0017181294970214367
iteration 58, loss = 0.001684678252786398
iteration 59, loss = 0.0015867878682911396
iteration 60, loss = 0.00163462630007416
iteration 61, loss = 0.0018419268308207393
iteration 62, loss = 0.0016629718011245131
iteration 63, loss = 0.0014836526243016124
iteration 64, loss = 0.0017766276141628623
iteration 65, loss = 0.0024277481716126204
iteration 66, loss = 0.0013757942942902446
iteration 67, loss = 0.0014788598055019975
iteration 68, loss = 0.001608269289135933
iteration 69, loss = 0.0013964001554995775
iteration 70, loss = 0.0025754072703421116
iteration 71, loss = 0.001570057705976069
iteration 72, loss = 0.0020745075307786465
iteration 73, loss = 0.0022600600495934486
iteration 74, loss = 0.001481546089053154
iteration 75, loss = 0.0014733329880982637
iteration 76, loss = 0.0016113390447571874
iteration 77, loss = 0.001753497403115034
iteration 78, loss = 0.0015373493079096079
iteration 79, loss = 0.002703733742237091
iteration 80, loss = 0.001763052656315267
iteration 81, loss = 0.0014502053381875157
iteration 82, loss = 0.0015161065384745598
iteration 83, loss = 0.0018492514500394464
iteration 84, loss = 0.0021685564424842596
iteration 85, loss = 0.0033594679553061724
iteration 86, loss = 0.0015751492464914918
iteration 87, loss = 0.0019955134484916925
iteration 88, loss = 0.0024707617703825235
iteration 89, loss = 0.0027043113950639963
iteration 90, loss = 0.002046660054475069
iteration 91, loss = 0.001602327567525208
iteration 92, loss = 0.0022618724033236504
iteration 93, loss = 0.0016543022356927395
iteration 94, loss = 0.002055729040876031
iteration 95, loss = 0.0015782017726451159
iteration 96, loss = 0.0014488815795630217
iteration 97, loss = 0.0015500426525250077
iteration 98, loss = 0.001460751169361174
iteration 99, loss = 0.0017936197109520435
iteration 100, loss = 0.0013516959734261036
iteration 101, loss = 0.0033447567839175463
iteration 102, loss = 0.0018562027253210545
iteration 103, loss = 0.001399102620780468
iteration 104, loss = 0.0013508739648386836
iteration 105, loss = 0.0017573897494003177
iteration 106, loss = 0.0015231735305860639
iteration 107, loss = 0.002672275062650442
iteration 108, loss = 0.001583350240252912
iteration 109, loss = 0.0018060464644804597
iteration 110, loss = 0.0016468213871121407
iteration 111, loss = 0.0014711363473907113
iteration 112, loss = 0.001948890625499189
iteration 113, loss = 0.0019818278960883617
iteration 114, loss = 0.0016139271901920438
iteration 115, loss = 0.0017488304292783141
iteration 116, loss = 0.001945443102158606
iteration 117, loss = 0.0015933829126879573
iteration 118, loss = 0.0014008524594828486
iteration 119, loss = 0.0015978063456714153
iteration 120, loss = 0.001476129749789834
iteration 121, loss = 0.0016592005267739296
iteration 122, loss = 0.0013290272327139974
iteration 123, loss = 0.001632633269764483
iteration 124, loss = 0.0016312076477333903
iteration 125, loss = 0.0015324180712923408
iteration 126, loss = 0.0015466600889340043
iteration 127, loss = 0.0015195979503914714
iteration 128, loss = 0.0013951974688097835
iteration 129, loss = 0.001892361557111144
iteration 130, loss = 0.001530180568806827
iteration 131, loss = 0.0015327160945162177
iteration 132, loss = 0.002837291918694973
iteration 133, loss = 0.002317773876711726
iteration 134, loss = 0.0016089993296191096
iteration 135, loss = 0.0014277927111834288
iteration 136, loss = 0.0021708765998482704
iteration 137, loss = 0.0014065067516639829
iteration 138, loss = 0.0013886543456465006
iteration 139, loss = 0.0014071251498535275
iteration 140, loss = 0.0014832023298367858
iteration 141, loss = 0.0014556261012330651
iteration 142, loss = 0.0014167320914566517
iteration 143, loss = 0.0015649694250896573
iteration 144, loss = 0.002588442526757717
iteration 145, loss = 0.0013493922306224704
iteration 146, loss = 0.0017124863807111979
iteration 147, loss = 0.0013681603595614433
iteration 148, loss = 0.0028731797356158495
iteration 149, loss = 0.001995591213926673
iteration 150, loss = 0.0038240989670157433
iteration 151, loss = 0.0015416607493534684
iteration 152, loss = 0.002107261447235942
iteration 153, loss = 0.0014030536403879523
iteration 154, loss = 0.002038918901234865
iteration 155, loss = 0.0017994145164266229
iteration 156, loss = 0.0018306821584701538
iteration 157, loss = 0.0014989259652793407
iteration 158, loss = 0.001875533489510417
iteration 159, loss = 0.00138198456261307
iteration 160, loss = 0.0018269941210746765
iteration 161, loss = 0.0018839709227904677
iteration 162, loss = 0.0021059312857687473
iteration 163, loss = 0.002662843558937311
iteration 164, loss = 0.0015002820873633027
iteration 165, loss = 0.0014496612129732966
iteration 166, loss = 0.001948392717167735
iteration 167, loss = 0.0028071324340999126
iteration 168, loss = 0.0016360452864319086
iteration 169, loss = 0.001348955323919654
iteration 170, loss = 0.0017896023346111178
iteration 171, loss = 0.0016821448225528002
iteration 172, loss = 0.001940848072990775
iteration 173, loss = 0.001652028993703425
iteration 174, loss = 0.001995830098167062
iteration 175, loss = 0.001641686074435711
iteration 176, loss = 0.0017492573242634535
iteration 177, loss = 0.002073675161227584
iteration 178, loss = 0.001842711353674531
iteration 179, loss = 0.0014882569666951895
iteration 180, loss = 0.0015290771843865514
iteration 181, loss = 0.0032685487531125546
iteration 182, loss = 0.002797870896756649
iteration 183, loss = 0.0017889514565467834
iteration 184, loss = 0.0013903859071433544
iteration 185, loss = 0.0014611687511205673
iteration 186, loss = 0.0013125366531312466
iteration 187, loss = 0.0013493618462234735
iteration 188, loss = 0.0016238807002082467
iteration 189, loss = 0.0015487532364204526
iteration 190, loss = 0.0016683049034327269
iteration 191, loss = 0.0012746175052598119
iteration 192, loss = 0.0014004218392074108
iteration 193, loss = 0.0047121550887823105
iteration 194, loss = 0.0023607152979820967
iteration 195, loss = 0.0018294997280463576
iteration 196, loss = 0.002001790562644601
iteration 197, loss = 0.0014768884284421802
iteration 198, loss = 0.002396678552031517
iteration 199, loss = 0.003079979680478573
iteration 200, loss = 0.0012957197614014149
iteration 201, loss = 0.001736810663715005
iteration 202, loss = 0.0015084611950442195
iteration 203, loss = 0.0014847831334918737
iteration 204, loss = 0.0020013044122606516
iteration 205, loss = 0.0014322565402835608
iteration 206, loss = 0.0028231851756572723
iteration 207, loss = 0.0014594593085348606
iteration 208, loss = 0.0014287751400843263
iteration 209, loss = 0.0015388722531497478
iteration 210, loss = 0.0015381259145215154
iteration 211, loss = 0.0016461714403703809
iteration 212, loss = 0.0015271945158019662
iteration 213, loss = 0.0016416918952018023
iteration 214, loss = 0.001813880167901516
iteration 215, loss = 0.0017076883232221007
iteration 216, loss = 0.0013681737473234534
iteration 217, loss = 0.0017500031972303987
iteration 218, loss = 0.0014784762170165777
iteration 219, loss = 0.002984120510518551
iteration 220, loss = 0.0035443436354398727
iteration 221, loss = 0.002476014895364642
iteration 222, loss = 0.0017841493245214224
iteration 223, loss = 0.0025390968658030033
iteration 224, loss = 0.0013783492613583803
iteration 225, loss = 0.00182343740016222
iteration 226, loss = 0.0016848734812811017
iteration 227, loss = 0.0013987728161737323
iteration 228, loss = 0.001530506880953908
iteration 229, loss = 0.0014633540995419025
iteration 230, loss = 0.001162464264780283
iteration 231, loss = 0.002167987171560526
iteration 232, loss = 0.0016629040474072099
iteration 233, loss = 0.0019819983281195164
iteration 234, loss = 0.0015797456726431847
iteration 235, loss = 0.0014986147871240973
iteration 236, loss = 0.0016057802131399512
iteration 237, loss = 0.0011824906105175614
iteration 238, loss = 0.0035133217461407185
iteration 239, loss = 0.001780542079359293
iteration 240, loss = 0.0014033819315955043
iteration 241, loss = 0.0014169695787131786
iteration 242, loss = 0.002269516233354807
iteration 243, loss = 0.0013709969352930784
iteration 244, loss = 0.0015004249289631844
iteration 245, loss = 0.0016374839469790459
iteration 246, loss = 0.001963755115866661
iteration 247, loss = 0.0015561934560537338
iteration 248, loss = 0.0015387794701382518
iteration 249, loss = 0.0015192953869700432
iteration 250, loss = 0.0018425090238451958
iteration 251, loss = 0.001395392115227878
iteration 252, loss = 0.0019200050737708807
iteration 253, loss = 0.0016872314736247063
iteration 254, loss = 0.00150959228631109
iteration 255, loss = 0.001899366732686758
iteration 256, loss = 0.0027606228832155466
iteration 257, loss = 0.0022449379321187735
iteration 258, loss = 0.002543981885537505
iteration 259, loss = 0.001729752286337316
iteration 260, loss = 0.0016317660920321941
iteration 261, loss = 0.0014294672291725874
iteration 262, loss = 0.002669911365956068
iteration 263, loss = 0.0016031949780881405
iteration 264, loss = 0.0015713553875684738
iteration 265, loss = 0.0014303232310339808
iteration 266, loss = 0.0013009130489081144
iteration 267, loss = 0.001446429407224059
iteration 268, loss = 0.0020567169412970543
iteration 269, loss = 0.0014030361780896783
iteration 270, loss = 0.0013549347640946507
iteration 271, loss = 0.0016079612541943789
iteration 272, loss = 0.0017427909187972546
iteration 273, loss = 0.0013430776307359338
iteration 274, loss = 0.001297362963669002
iteration 275, loss = 0.001254413858987391
iteration 276, loss = 0.001789625035598874
iteration 277, loss = 0.001704280381090939
iteration 278, loss = 0.0013960542855784297
iteration 279, loss = 0.0018443064764142036
iteration 280, loss = 0.0021274196915328503
iteration 281, loss = 0.001883427263237536
iteration 282, loss = 0.0015458588022738695
iteration 283, loss = 0.001438323175534606
iteration 284, loss = 0.001472847885452211
iteration 285, loss = 0.0013952363515272737
iteration 286, loss = 0.0014970200136303902
iteration 287, loss = 0.001373330014757812
iteration 288, loss = 0.0013124666875228286
iteration 289, loss = 0.0016902796924114227
iteration 290, loss = 0.0013814258854836226
iteration 291, loss = 0.0013075107708573341
iteration 292, loss = 0.002837191801518202
iteration 293, loss = 0.002277405932545662
iteration 294, loss = 0.001719324616715312
iteration 295, loss = 0.0015950993401929736
iteration 296, loss = 0.0017122364370152354
iteration 297, loss = 0.0013293675146996975
iteration 298, loss = 0.0013113527093082666
iteration 299, loss = 0.0020996439270675182
iteration 300, loss = 0.0017710370011627674
iteration 1, loss = 0.0013955500908195972
iteration 2, loss = 0.0015740859089419246
iteration 3, loss = 0.0012723198160529137
iteration 4, loss = 0.0018469869391992688
iteration 5, loss = 0.0013055158779025078
iteration 6, loss = 0.0014898868976160884
iteration 7, loss = 0.0013324094470590353
iteration 8, loss = 0.0014897090150043368
iteration 9, loss = 0.0016508589033037424
iteration 10, loss = 0.001551009714603424
iteration 11, loss = 0.0013439544709399343
iteration 12, loss = 0.0014795756433159113
iteration 13, loss = 0.001743454486131668
iteration 14, loss = 0.001748145674355328
iteration 15, loss = 0.001985016977414489
iteration 16, loss = 0.0015422317665070295
iteration 17, loss = 0.001617533154785633
iteration 18, loss = 0.002181051531806588
iteration 19, loss = 0.0031448842491954565
iteration 20, loss = 0.002023670356720686
iteration 21, loss = 0.002852050354704261
iteration 22, loss = 0.0031394714023917913
iteration 23, loss = 0.002474804176017642
iteration 24, loss = 0.0015521117020398378
iteration 25, loss = 0.004294124897569418
iteration 26, loss = 0.0015350461471825838
iteration 27, loss = 0.0016555633628740907
iteration 28, loss = 0.0021497870329767466
iteration 29, loss = 0.0013547100825235248
iteration 30, loss = 0.0022109958808869123
iteration 31, loss = 0.001534168841317296
iteration 32, loss = 0.0014229471562430263
iteration 33, loss = 0.0014258042210713029
iteration 34, loss = 0.001404783222824335
iteration 35, loss = 0.0012967862421646714
iteration 36, loss = 0.001626694924198091
iteration 37, loss = 0.0016621631802991033
iteration 38, loss = 0.0016921836649999022
iteration 39, loss = 0.002964023733511567
iteration 40, loss = 0.001568976673297584
iteration 41, loss = 0.0025133464951068163
iteration 42, loss = 0.0016982139786705375
iteration 43, loss = 0.0013653442729264498
iteration 44, loss = 0.001887912512756884
iteration 45, loss = 0.0012802771525457501
iteration 46, loss = 0.0013918266631662846
iteration 47, loss = 0.0014207655331119895
iteration 48, loss = 0.002477794187143445
iteration 49, loss = 0.0013557015918195248
iteration 50, loss = 0.0014202204765751958
iteration 51, loss = 0.0015066228806972504
iteration 52, loss = 0.0011930648470297456
iteration 53, loss = 0.0015022484585642815
iteration 54, loss = 0.0014532150235027075
iteration 55, loss = 0.0023853422608226538
iteration 56, loss = 0.001859668642282486
iteration 57, loss = 0.0015249282587319613
iteration 58, loss = 0.0016280109994113445
iteration 59, loss = 0.0012593901483342052
iteration 60, loss = 0.001672803657129407
iteration 61, loss = 0.001959035871550441
iteration 62, loss = 0.001324088079854846
iteration 63, loss = 0.0018905909964814782
iteration 64, loss = 0.0011388593120500445
iteration 65, loss = 0.0015726909041404724
iteration 66, loss = 0.0014215079136192799
iteration 67, loss = 0.0012079075677320361
iteration 68, loss = 0.0013202906120568514
iteration 69, loss = 0.0011797308688983321
iteration 70, loss = 0.0015826445305719972
iteration 71, loss = 0.0017713293200358748
iteration 72, loss = 0.001287323422729969
iteration 73, loss = 0.002576163737103343
iteration 74, loss = 0.0013947299448773265
iteration 75, loss = 0.0014805261744186282
iteration 76, loss = 0.0017153973458334804
iteration 77, loss = 0.0014334740117192268
iteration 78, loss = 0.0012331744655966759
iteration 79, loss = 0.001883146003820002
iteration 80, loss = 0.0014721170300617814
iteration 81, loss = 0.00165200000628829
iteration 82, loss = 0.0016250188928097486
iteration 83, loss = 0.0019400245510041714
iteration 84, loss = 0.002857191488146782
iteration 85, loss = 0.0014734503347426653
iteration 86, loss = 0.0017659615259617567
iteration 87, loss = 0.0014743511565029621
iteration 88, loss = 0.001317076152190566
iteration 89, loss = 0.0013258641120046377
iteration 90, loss = 0.001505025546066463
iteration 91, loss = 0.003053567372262478
iteration 92, loss = 0.0029236942064017057
iteration 93, loss = 0.0014631920494139194
iteration 94, loss = 0.0013368746731430292
iteration 95, loss = 0.001358301960863173
iteration 96, loss = 0.0011960830306634307
iteration 97, loss = 0.0024958921130746603
iteration 98, loss = 0.0015510141383856535
iteration 99, loss = 0.0012785571161657572
iteration 100, loss = 0.0013815859565511346
iteration 101, loss = 0.0015401735436171293
iteration 102, loss = 0.0014260542811825871
iteration 103, loss = 0.0019157170318067074
iteration 104, loss = 0.0030283252708613873
iteration 105, loss = 0.001589957275427878
iteration 106, loss = 0.0014243284240365028
iteration 107, loss = 0.0015310202725231647
iteration 108, loss = 0.0020858857315033674
iteration 109, loss = 0.0018710214644670486
iteration 110, loss = 0.0016686192248016596
iteration 111, loss = 0.0014084718422964215
iteration 112, loss = 0.001866334117949009
iteration 113, loss = 0.0016146302223205566
iteration 114, loss = 0.001694181701168418
iteration 115, loss = 0.0012264371616765857
iteration 116, loss = 0.0013733560917899013
iteration 117, loss = 0.0015463769668713212
iteration 118, loss = 0.0013592003379017115
iteration 119, loss = 0.0019586256239563227
iteration 120, loss = 0.0013724841410294175
iteration 121, loss = 0.001353860949166119
iteration 122, loss = 0.00296212756074965
iteration 123, loss = 0.001452770084142685
iteration 124, loss = 0.001367904944345355
iteration 125, loss = 0.0013090671272948384
iteration 126, loss = 0.0015310539165511727
iteration 127, loss = 0.0012127961963415146
iteration 128, loss = 0.0012992124538868666
iteration 129, loss = 0.0016691024648025632
iteration 130, loss = 0.0014118643011897802
iteration 131, loss = 0.0015473035164177418
iteration 132, loss = 0.0016736665274947882
iteration 133, loss = 0.0012304793344810605
iteration 134, loss = 0.001217102981172502
iteration 135, loss = 0.0012504642363637686
iteration 136, loss = 0.0029350200202316046
iteration 137, loss = 0.0017634030664339662
iteration 138, loss = 0.002436204580590129
iteration 139, loss = 0.0013965368270874023
iteration 140, loss = 0.0010928736301138997
iteration 141, loss = 0.0014581333380192518
iteration 142, loss = 0.0013345240149646997
iteration 143, loss = 0.00133627955801785
iteration 144, loss = 0.001269705593585968
iteration 145, loss = 0.0015086871571838856
iteration 146, loss = 0.001280270516872406
iteration 147, loss = 0.002290378324687481
iteration 148, loss = 0.0015450441278517246
iteration 149, loss = 0.002036032034084201
iteration 150, loss = 0.002093495801091194
iteration 151, loss = 0.0016043710056692362
iteration 152, loss = 0.0012345046270638704
iteration 153, loss = 0.0019408573862165213
iteration 154, loss = 0.0015670977300032973
iteration 155, loss = 0.0015413223300129175
iteration 156, loss = 0.0013113119639456272
iteration 157, loss = 0.0013738947454839945
iteration 158, loss = 0.001832705456763506
iteration 159, loss = 0.001491150469519198
iteration 160, loss = 0.0018979954766109586
iteration 161, loss = 0.0013504948001354933
iteration 162, loss = 0.002384349936619401
iteration 163, loss = 0.0014819668140262365
iteration 164, loss = 0.0012999266618862748
iteration 165, loss = 0.0018074156250804663
iteration 166, loss = 0.0012696846388280392
iteration 167, loss = 0.0013956003822386265
iteration 168, loss = 0.0015985040226951241
iteration 169, loss = 0.0012578684836626053
iteration 170, loss = 0.002672037575393915
iteration 171, loss = 0.0012884316965937614
iteration 172, loss = 0.0028938506729900837
iteration 173, loss = 0.002406424144282937
iteration 174, loss = 0.0012640125351026654
iteration 175, loss = 0.0016684277215972543
iteration 176, loss = 0.0016347935888916254
iteration 177, loss = 0.0011293322313576937
iteration 178, loss = 0.0011680645402520895
iteration 179, loss = 0.0011919349199160933
iteration 180, loss = 0.0015336034120991826
iteration 181, loss = 0.0014217518037185073
iteration 182, loss = 0.0019177261274307966
iteration 183, loss = 0.0016681983834132552
iteration 184, loss = 0.0018926868215203285
iteration 185, loss = 0.002276773564517498
iteration 186, loss = 0.0012169480323791504
iteration 187, loss = 0.002089732326567173
iteration 188, loss = 0.00170711032114923
iteration 189, loss = 0.0011234288103878498
iteration 190, loss = 0.0013789933873340487
iteration 191, loss = 0.0012647398980334401
iteration 192, loss = 0.0010279383277520537
iteration 193, loss = 0.0016428011003881693
iteration 194, loss = 0.0013242618879303336
iteration 195, loss = 0.0014297101879492402
iteration 196, loss = 0.001268397900275886
iteration 197, loss = 0.0013123855460435152
iteration 198, loss = 0.0015300377272069454
iteration 199, loss = 0.0017671785317361355
iteration 200, loss = 0.0022076303139328957
iteration 201, loss = 0.0017180085415020585
iteration 202, loss = 0.0016416632570326328
iteration 203, loss = 0.0016558428760617971
iteration 204, loss = 0.0016830796375870705
iteration 205, loss = 0.001445193774998188
iteration 206, loss = 0.001731103053316474
iteration 207, loss = 0.0016826675273478031
iteration 208, loss = 0.001916104811243713
iteration 209, loss = 0.0012364472495391965
iteration 210, loss = 0.0037208879366517067
iteration 211, loss = 0.001723974710330367
iteration 212, loss = 0.0015375509392470121
iteration 213, loss = 0.0016400079475715756
iteration 214, loss = 0.001596239977516234
iteration 215, loss = 0.002096737502142787
iteration 216, loss = 0.0022957369219511747
iteration 217, loss = 0.00142183736898005
iteration 218, loss = 0.0013647868763655424
iteration 219, loss = 0.0012620039051398635
iteration 220, loss = 0.0012640177737921476
iteration 221, loss = 0.0013950052671134472
iteration 222, loss = 0.0012363881105557084
iteration 223, loss = 0.0014830556465312839
iteration 224, loss = 0.0015571154654026031
iteration 225, loss = 0.002178705995902419
iteration 226, loss = 0.0016550207510590553
iteration 227, loss = 0.001465577632188797
iteration 228, loss = 0.0014682371402159333
iteration 229, loss = 0.0014003455871716142
iteration 230, loss = 0.0013161890674382448
iteration 231, loss = 0.00227197608910501
iteration 232, loss = 0.0016927688848227262
iteration 233, loss = 0.001489589805714786
iteration 234, loss = 0.0012578513706102967
iteration 235, loss = 0.0028877214062958956
iteration 236, loss = 0.0015699256910011172
iteration 237, loss = 0.0019826029893010855
iteration 238, loss = 0.0015928549692034721
iteration 239, loss = 0.0018903928576037288
iteration 240, loss = 0.001595301553606987
iteration 241, loss = 0.001382978167384863
iteration 242, loss = 0.0018478615675121546
iteration 243, loss = 0.001766950823366642
iteration 244, loss = 0.0027038613334298134
iteration 245, loss = 0.0015556059079244733
iteration 246, loss = 0.0013267487520352006
iteration 247, loss = 0.0017189935315400362
iteration 248, loss = 0.0013091370929032564
iteration 249, loss = 0.0012592212297022343
iteration 250, loss = 0.0012558858143165708
iteration 251, loss = 0.0012995590222999454
iteration 252, loss = 0.0012191272107884288
iteration 253, loss = 0.0015849280171096325
iteration 254, loss = 0.0014405782567337155
iteration 255, loss = 0.0024261772632598877
iteration 256, loss = 0.0019091074354946613
iteration 257, loss = 0.0017089552711695433
iteration 258, loss = 0.0017319399630650878
iteration 259, loss = 0.001225390238687396
iteration 260, loss = 0.001364751486107707
iteration 261, loss = 0.0016687263268977404
iteration 262, loss = 0.0013612934853881598
iteration 263, loss = 0.0013589709997177124
iteration 264, loss = 0.0013483611401170492
iteration 265, loss = 0.0015520548913627863
iteration 266, loss = 0.0015857010148465633
iteration 267, loss = 0.0015436970861628652
iteration 268, loss = 0.0018494019750505686
iteration 269, loss = 0.0012178441975265741
iteration 270, loss = 0.002019646344706416
iteration 271, loss = 0.00139691645745188
iteration 272, loss = 0.0024095866829156876
iteration 273, loss = 0.0013239962281659245
iteration 274, loss = 0.0013150637969374657
iteration 275, loss = 0.0014363849768415093
iteration 276, loss = 0.0014747952809557319
iteration 277, loss = 0.0011187120107933879
iteration 278, loss = 0.001473346259444952
iteration 279, loss = 0.0016415651189163327
iteration 280, loss = 0.0013629008317366242
iteration 281, loss = 0.001266496954485774
iteration 282, loss = 0.0020216486882418394
iteration 283, loss = 0.0013974412577226758
iteration 284, loss = 0.0014873073669150472
iteration 285, loss = 0.0018622188363224268
iteration 286, loss = 0.0019323073793202639
iteration 287, loss = 0.0013656042283400893
iteration 288, loss = 0.001226027263328433
iteration 289, loss = 0.001315123401582241
iteration 290, loss = 0.001883610151708126
iteration 291, loss = 0.0018438301049172878
iteration 292, loss = 0.0017857991624623537
iteration 293, loss = 0.0016344421310350299
iteration 294, loss = 0.0014679625164717436
iteration 295, loss = 0.0017176744295284152
iteration 296, loss = 0.0013295506360009313
iteration 297, loss = 0.0013944258680567145
iteration 298, loss = 0.0011428918223828077
iteration 299, loss = 0.00320179620757699
iteration 300, loss = 0.0012111760443076491
iteration 1, loss = 0.0015456067631021142
iteration 2, loss = 0.001475866069085896
iteration 3, loss = 0.0014297738671302795
iteration 4, loss = 0.00249862065538764
iteration 5, loss = 0.0025262096896767616
iteration 6, loss = 0.0014481747057288885
iteration 7, loss = 0.0014771647984161973
iteration 8, loss = 0.0026132911443710327
iteration 9, loss = 0.001054844120517373
iteration 10, loss = 0.002776771318167448
iteration 11, loss = 0.0011755356099456549
iteration 12, loss = 0.0014382374938577414
iteration 13, loss = 0.001401463057845831
iteration 14, loss = 0.0026666196063160896
iteration 15, loss = 0.001216595177538693
iteration 16, loss = 0.0013174583436921239
iteration 17, loss = 0.001420946093276143
iteration 18, loss = 0.0013664064463227987
iteration 19, loss = 0.00204161973670125
iteration 20, loss = 0.0013492560246959329
iteration 21, loss = 0.0015932614915072918
iteration 22, loss = 0.00127896829508245
iteration 23, loss = 0.0013623220147565007
iteration 24, loss = 0.001499570906162262
iteration 25, loss = 0.001458259648643434
iteration 26, loss = 0.0012412378564476967
iteration 27, loss = 0.0016817771829664707
iteration 28, loss = 0.0012425730237737298
iteration 29, loss = 0.0017897600773721933
iteration 30, loss = 0.0015938320429995656
iteration 31, loss = 0.0015810336917638779
iteration 32, loss = 0.001828668755479157
iteration 33, loss = 0.0013149803271517158
iteration 34, loss = 0.0010670555057004094
iteration 35, loss = 0.0022525275126099586
iteration 36, loss = 0.0013969892170280218
iteration 37, loss = 0.0027467808686196804
iteration 38, loss = 0.0017504387069493532
iteration 39, loss = 0.0014263074845075607
iteration 40, loss = 0.0011985303135588765
iteration 41, loss = 0.0029637422412633896
iteration 42, loss = 0.0016721042338758707
iteration 43, loss = 0.001329102786257863
iteration 44, loss = 0.0012074452824890614
iteration 45, loss = 0.00137795670889318
iteration 46, loss = 0.0012438968988135457
iteration 47, loss = 0.002709413878619671
iteration 48, loss = 0.0027824295684695244
iteration 49, loss = 0.003006886225193739
iteration 50, loss = 0.001239436212927103
iteration 51, loss = 0.0014896660577505827
iteration 52, loss = 0.001357147702947259
iteration 53, loss = 0.0012115300633013248
iteration 54, loss = 0.001267151441425085
iteration 55, loss = 0.0012899142457172275
iteration 56, loss = 0.0024276264011859894
iteration 57, loss = 0.001572100562043488
iteration 58, loss = 0.0026680920273065567
iteration 59, loss = 0.0012123568449169397
iteration 60, loss = 0.0013166503049433231
iteration 61, loss = 0.0028106935787945986
iteration 62, loss = 0.0017514254432171583
iteration 63, loss = 0.0018673110753297806
iteration 64, loss = 0.001298603368923068
iteration 65, loss = 0.0018640912603586912
iteration 66, loss = 0.0016357630956918001
iteration 67, loss = 0.001021241769194603
iteration 68, loss = 0.0014066770672798157
iteration 69, loss = 0.0014106925809755921
iteration 70, loss = 0.0017377862241119146
iteration 71, loss = 0.0015028617344796658
iteration 72, loss = 0.00140498171094805
iteration 73, loss = 0.001704743248410523
iteration 74, loss = 0.0014034274499863386
iteration 75, loss = 0.0012276394991204143
iteration 76, loss = 0.001392955775372684
iteration 77, loss = 0.0014113145880401134
iteration 78, loss = 0.002603779546916485
iteration 79, loss = 0.001272800494916737
iteration 80, loss = 0.0013519038911908865
iteration 81, loss = 0.0011949508916586637
iteration 82, loss = 0.0012192738940939307
iteration 83, loss = 0.001428577583283186
iteration 84, loss = 0.0013104942627251148
iteration 85, loss = 0.0013043626677244902
iteration 86, loss = 0.0012801073025912046
iteration 87, loss = 0.0016096947947517037
iteration 88, loss = 0.001903539290651679
iteration 89, loss = 0.001242924714460969
iteration 90, loss = 0.001615815912373364
iteration 91, loss = 0.001610670704394579
iteration 92, loss = 0.0012343121925368905
iteration 93, loss = 0.0019133720779791474
iteration 94, loss = 0.001288477098569274
iteration 95, loss = 0.0012761703692376614
iteration 96, loss = 0.0029147525783628225
iteration 97, loss = 0.002147698076441884
iteration 98, loss = 0.001819066354073584
iteration 99, loss = 0.001293767592869699
iteration 100, loss = 0.001720810541883111
iteration 101, loss = 0.0014962611021474004
iteration 102, loss = 0.0010558501817286015
iteration 103, loss = 0.0014420503284782171
iteration 104, loss = 0.0011519459076225758
iteration 105, loss = 0.0012152371928095818
iteration 106, loss = 0.0016041981289163232
iteration 107, loss = 0.0013475745217874646
iteration 108, loss = 0.0015313710318878293
iteration 109, loss = 0.0012164084473624825
iteration 110, loss = 0.0012026653857901692
iteration 111, loss = 0.0013781421585008502
iteration 112, loss = 0.0012508423533290625
iteration 113, loss = 0.0011625696206465364
iteration 114, loss = 0.001602797769010067
iteration 115, loss = 0.001160221523605287
iteration 116, loss = 0.0013979858485981822
iteration 117, loss = 0.001584394951350987
iteration 118, loss = 0.0012289570877328515
iteration 119, loss = 0.0018993231933563948
iteration 120, loss = 0.0013218801468610764
iteration 121, loss = 0.0014721463667228818
iteration 122, loss = 0.0012023262679576874
iteration 123, loss = 0.001423973822966218
iteration 124, loss = 0.0011169988429173827
iteration 125, loss = 0.0012630608398467302
iteration 126, loss = 0.0017270491225644946
iteration 127, loss = 0.0012466302141547203
iteration 128, loss = 0.0013828501105308533
iteration 129, loss = 0.001530543901026249
iteration 130, loss = 0.0014944892609491944
iteration 131, loss = 0.0012527270009741187
iteration 132, loss = 0.001694943755865097
iteration 133, loss = 0.0013970922445878386
iteration 134, loss = 0.0015801028348505497
iteration 135, loss = 0.0015039007412269711
iteration 136, loss = 0.0014599580317735672
iteration 137, loss = 0.0021999399177730083
iteration 138, loss = 0.0013879458419978619
iteration 139, loss = 0.0023918687365949154
iteration 140, loss = 0.0011235744459554553
iteration 141, loss = 0.0024173373822122812
iteration 142, loss = 0.0011260631727054715
iteration 143, loss = 0.0015404662117362022
iteration 144, loss = 0.0014458622317761183
iteration 145, loss = 0.0012306449934840202
iteration 146, loss = 0.0012455051764845848
iteration 147, loss = 0.001307352795265615
iteration 148, loss = 0.001928101759403944
iteration 149, loss = 0.0014125097077339888
iteration 150, loss = 0.0011974378721788526
iteration 151, loss = 0.0014810784487053752
iteration 152, loss = 0.0012302123941481113
iteration 153, loss = 0.0017447646241635084
iteration 154, loss = 0.0015986058861017227
iteration 155, loss = 0.0014288388192653656
iteration 156, loss = 0.0011818716302514076
iteration 157, loss = 0.0010789564112201333
iteration 158, loss = 0.0012424748856574297
iteration 159, loss = 0.0013143718242645264
iteration 160, loss = 0.002128611784428358
iteration 161, loss = 0.001756949000991881
iteration 162, loss = 0.0018725721165537834
iteration 163, loss = 0.0012710965238511562
iteration 164, loss = 0.001453990931622684
iteration 165, loss = 0.0015563294291496277
iteration 166, loss = 0.0023939230013638735
iteration 167, loss = 0.001163541804999113
iteration 168, loss = 0.0012753119226545095
iteration 169, loss = 0.0011999483685940504
iteration 170, loss = 0.0011175752151757479
iteration 171, loss = 0.0013080561766400933
iteration 172, loss = 0.0012537251459434628
iteration 173, loss = 0.0012939637526869774
iteration 174, loss = 0.0015868135960772634
iteration 175, loss = 0.0014006408164277673
iteration 176, loss = 0.0010761440498754382
iteration 177, loss = 0.001311228726990521
iteration 178, loss = 0.0013455173466354609
iteration 179, loss = 0.0016857547452673316
iteration 180, loss = 0.0011470821918919683
iteration 181, loss = 0.0017199484864249825
iteration 182, loss = 0.0014720491599291563
iteration 183, loss = 0.0012195821618661284
iteration 184, loss = 0.0011195380939170718
iteration 185, loss = 0.0014140265993773937
iteration 186, loss = 0.001160545856691897
iteration 187, loss = 0.0010916204191744328
iteration 188, loss = 0.0011887416476383805
iteration 189, loss = 0.001475280849263072
iteration 190, loss = 0.001996881328523159
iteration 191, loss = 0.0022585669066756964
iteration 192, loss = 0.0017894668271765113
iteration 193, loss = 0.0011656186543405056
iteration 194, loss = 0.0013891039416193962
iteration 195, loss = 0.002325288485735655
iteration 196, loss = 0.001490314956754446
iteration 197, loss = 0.001656116684898734
iteration 198, loss = 0.0012557649752125144
iteration 199, loss = 0.001220573903992772
iteration 200, loss = 0.0025096414610743523
iteration 201, loss = 0.0014204748440533876
iteration 202, loss = 0.0017375844763591886
iteration 203, loss = 0.0013077614130452275
iteration 204, loss = 0.0019509142730385065
iteration 205, loss = 0.0017197628039866686
iteration 206, loss = 0.0018472487572580576
iteration 207, loss = 0.001269316184334457
iteration 208, loss = 0.0016096459003165364
iteration 209, loss = 0.0013535061152651906
iteration 210, loss = 0.0013821062166243792
iteration 211, loss = 0.0012284762924537063
iteration 212, loss = 0.0018599643371999264
iteration 213, loss = 0.0012992837000638247
iteration 214, loss = 0.0014434579061344266
iteration 215, loss = 0.001353733940050006
iteration 216, loss = 0.0026127120945602655
iteration 217, loss = 0.0016666499432176352
iteration 218, loss = 0.0014572514919564128
iteration 219, loss = 0.0010821587638929486
iteration 220, loss = 0.001836488489061594
iteration 221, loss = 0.0012382278218865395
iteration 222, loss = 0.0023926510475575924
iteration 223, loss = 0.001163375680334866
iteration 224, loss = 0.0013710216153413057
iteration 225, loss = 0.0011156119871884584
iteration 226, loss = 0.0013096666662022471
iteration 227, loss = 0.002692366251721978
iteration 228, loss = 0.002528409706428647
iteration 229, loss = 0.0021058523561805487
iteration 230, loss = 0.0017235161503776908
iteration 231, loss = 0.0012994182761758566
iteration 232, loss = 0.0012373893987387419
iteration 233, loss = 0.0011879345402121544
iteration 234, loss = 0.0012913328828290105
iteration 235, loss = 0.0014398747589439154
iteration 236, loss = 0.002313381526619196
iteration 237, loss = 0.0011950391344726086
iteration 238, loss = 0.002513174433261156
iteration 239, loss = 0.0016825366765260696
iteration 240, loss = 0.0014513880014419556
iteration 241, loss = 0.0013433609856292605
iteration 242, loss = 0.0012394457589834929
iteration 243, loss = 0.0016058069886639714
iteration 244, loss = 0.0011633631074801087
iteration 245, loss = 0.0012112308759242296
iteration 246, loss = 0.0013641007244586945
iteration 247, loss = 0.0011598621495068073
iteration 248, loss = 0.001318284310400486
iteration 249, loss = 0.001285930396988988
iteration 250, loss = 0.0014920233516022563
iteration 251, loss = 0.001409550430253148
iteration 252, loss = 0.0014583123847842216
iteration 253, loss = 0.0013405352365225554
iteration 254, loss = 0.0012389514595270157
iteration 255, loss = 0.0012163241626694798
iteration 256, loss = 0.0014724325155839324
iteration 257, loss = 0.0015520842280238867
iteration 258, loss = 0.0014525227015838027
iteration 259, loss = 0.0016399156302213669
iteration 260, loss = 0.0020998951513320208
iteration 261, loss = 0.0012117165606468916
iteration 262, loss = 0.0016182507388293743
iteration 263, loss = 0.0012905768817290664
iteration 264, loss = 0.0012461101869121194
iteration 265, loss = 0.001110773184336722
iteration 266, loss = 0.0016956395702436566
iteration 267, loss = 0.0012283013202250004
iteration 268, loss = 0.0014870396116748452
iteration 269, loss = 0.0010973808821290731
iteration 270, loss = 0.001642806688323617
iteration 271, loss = 0.0013342343736439943
iteration 272, loss = 0.0018749727169051766
iteration 273, loss = 0.0017095186049118638
iteration 274, loss = 0.001066000433638692
iteration 275, loss = 0.0013484936207532883
iteration 276, loss = 0.0012624699156731367
iteration 277, loss = 0.0011793924495577812
iteration 278, loss = 0.0016100720968097448
iteration 279, loss = 0.0016360890585929155
iteration 280, loss = 0.00105848943348974
iteration 281, loss = 0.0011496369261294603
iteration 282, loss = 0.0015936361160129309
iteration 283, loss = 0.0019824435003101826
iteration 284, loss = 0.0012264755787327886
iteration 285, loss = 0.0014235584530979395
iteration 286, loss = 0.00113334646448493
iteration 287, loss = 0.0011556956451386213
iteration 288, loss = 0.0012998782331123948
iteration 289, loss = 0.0012838514521718025
iteration 290, loss = 0.0019845827482640743
iteration 291, loss = 0.001297293696552515
iteration 292, loss = 0.0014849300496280193
iteration 293, loss = 0.0013653076021000743
iteration 294, loss = 0.0011380849173292518
iteration 295, loss = 0.0012576640583574772
iteration 296, loss = 0.0011636246927082539
iteration 297, loss = 0.002029057126492262
iteration 298, loss = 0.0013020047917962074
iteration 299, loss = 0.0011508864117786288
iteration 300, loss = 0.0013017179444432259
iteration 1, loss = 0.0011294964933767915
iteration 2, loss = 0.0015761659014970064
iteration 3, loss = 0.0012182981008663774
iteration 4, loss = 0.0013358399737626314
iteration 5, loss = 0.00121011002920568
iteration 6, loss = 0.0017615767428651452
iteration 7, loss = 0.0014772862195968628
iteration 8, loss = 0.0011045691790059209
iteration 9, loss = 0.0010227649472653866
iteration 10, loss = 0.001096551539376378
iteration 11, loss = 0.0011548524489626288
iteration 12, loss = 0.0024046064354479313
iteration 13, loss = 0.0012359838001430035
iteration 14, loss = 0.001110403216443956
iteration 15, loss = 0.0012386766029521823
iteration 16, loss = 0.0014823286328464746
iteration 17, loss = 0.0015944554470479488
iteration 18, loss = 0.0013327995548024774
iteration 19, loss = 0.001188767491839826
iteration 20, loss = 0.001174931530840695
iteration 21, loss = 0.0024473038502037525
iteration 22, loss = 0.0015450617065653205
iteration 23, loss = 0.0024922771845012903
iteration 24, loss = 0.0014551823260262609
iteration 25, loss = 0.0010509858839213848
iteration 26, loss = 0.0020294946152716875
iteration 27, loss = 0.001920519513078034
iteration 28, loss = 0.0010045503731817007
iteration 29, loss = 0.002112368121743202
iteration 30, loss = 0.002819397719576955
iteration 31, loss = 0.001082241185940802
iteration 32, loss = 0.0010582759277895093
iteration 33, loss = 0.0019427386578172445
iteration 34, loss = 0.0012538173468783498
iteration 35, loss = 0.0011548877228051424
iteration 36, loss = 0.001286287559196353
iteration 37, loss = 0.0011116773821413517
iteration 38, loss = 0.0017637532437220216
iteration 39, loss = 0.0012944374466314912
iteration 40, loss = 0.0012839121045544744
iteration 41, loss = 0.0015617201570421457
iteration 42, loss = 0.0010906789684668183
iteration 43, loss = 0.0011794386664405465
iteration 44, loss = 0.0013628435553982854
iteration 45, loss = 0.0011186329647898674
iteration 46, loss = 0.001044770237058401
iteration 47, loss = 0.0025508522521704435
iteration 48, loss = 0.00172909046523273
iteration 49, loss = 0.0012070408556610346
iteration 50, loss = 0.0012112155091017485
iteration 51, loss = 0.0024530666414648294
iteration 52, loss = 0.0012103496119379997
iteration 53, loss = 0.0011298768222332
iteration 54, loss = 0.0011548495385795832
iteration 55, loss = 0.0016235195798799396
iteration 56, loss = 0.0012055564438924193
iteration 57, loss = 0.001949783880263567
iteration 58, loss = 0.0011487627634778619
iteration 59, loss = 0.001158983912318945
iteration 60, loss = 0.0011956228408962488
iteration 61, loss = 0.001409825636073947
iteration 62, loss = 0.0013299197889864445
iteration 63, loss = 0.0013851707335561514
iteration 64, loss = 0.001369347795844078
iteration 65, loss = 0.00202789343893528
iteration 66, loss = 0.0015844708541408181
iteration 67, loss = 0.001239586854353547
iteration 68, loss = 0.002670364687219262
iteration 69, loss = 0.001670466037467122
iteration 70, loss = 0.0014094416983425617
iteration 71, loss = 0.0022763723973184824
iteration 72, loss = 0.0015338360099121928
iteration 73, loss = 0.0016907521057873964
iteration 74, loss = 0.0011890282621607184
iteration 75, loss = 0.0010988257126882672
iteration 76, loss = 0.0013247141614556313
iteration 77, loss = 0.001216184115037322
iteration 78, loss = 0.001225706422701478
iteration 79, loss = 0.0012211725115776062
iteration 80, loss = 0.0012419659178704023
iteration 81, loss = 0.001456686994060874
iteration 82, loss = 0.0014566207537427545
iteration 83, loss = 0.002027034293860197
iteration 84, loss = 0.0015662218211218715
iteration 85, loss = 0.001356839551590383
iteration 86, loss = 0.0015066259074956179
iteration 87, loss = 0.0014679022133350372
iteration 88, loss = 0.0014972112840041518
iteration 89, loss = 0.002857059706002474
iteration 90, loss = 0.0019482295028865337
iteration 91, loss = 0.0013437794987112284
iteration 92, loss = 0.0013498953776434064
iteration 93, loss = 0.0011584782041609287
iteration 94, loss = 0.001092239748686552
iteration 95, loss = 0.0013627847656607628
iteration 96, loss = 0.0016296524554491043
iteration 97, loss = 0.001341361436061561
iteration 98, loss = 0.0013552531599998474
iteration 99, loss = 0.0011131144128739834
iteration 100, loss = 0.0016673693899065256
iteration 101, loss = 0.0016168360598385334
iteration 102, loss = 0.0010753360111266375
iteration 103, loss = 0.001187098678201437
iteration 104, loss = 0.00118055019993335
iteration 105, loss = 0.0015521205496042967
iteration 106, loss = 0.0018464159220457077
iteration 107, loss = 0.0013140883529558778
iteration 108, loss = 0.0012967673828825355
iteration 109, loss = 0.002507006749510765
iteration 110, loss = 0.002094398019835353
iteration 111, loss = 0.0013066037790849805
iteration 112, loss = 0.001278753043152392
iteration 113, loss = 0.0011404224205762148
iteration 114, loss = 0.0015985019272193313
iteration 115, loss = 0.0022766392212361097
iteration 116, loss = 0.0012966541107743979
iteration 117, loss = 0.001501911086961627
iteration 118, loss = 0.0015646291431039572
iteration 119, loss = 0.0012024196330457926
iteration 120, loss = 0.001119106076657772
iteration 121, loss = 0.0012584005016833544
iteration 122, loss = 0.0014007214922457933
iteration 123, loss = 0.001168937305919826
iteration 124, loss = 0.0011238108854740858
iteration 125, loss = 0.0015424712328240275
iteration 126, loss = 0.0010605837451294065
iteration 127, loss = 0.0013675197260454297
iteration 128, loss = 0.00226480676792562
iteration 129, loss = 0.0014416059711948037
iteration 130, loss = 0.0010098834754899144
iteration 131, loss = 0.0023946412838995457
iteration 132, loss = 0.0011374067980796099
iteration 133, loss = 0.00167952012270689
iteration 134, loss = 0.0017898868536576629
iteration 135, loss = 0.001126171206124127
iteration 136, loss = 0.0012474830728024244
iteration 137, loss = 0.0016039088368415833
iteration 138, loss = 0.0013615475036203861
iteration 139, loss = 0.0012670919531956315
iteration 140, loss = 0.0012329849414527416
iteration 141, loss = 0.0014940394321456552
iteration 142, loss = 0.0015625919913873076
iteration 143, loss = 0.0011568751651793718
iteration 144, loss = 0.0011090743355453014
iteration 145, loss = 0.0010460155317559838
iteration 146, loss = 0.0013788612559437752
iteration 147, loss = 0.0011198961874470115
iteration 148, loss = 0.0011906655272468925
iteration 149, loss = 0.0013821367174386978
iteration 150, loss = 0.0009907090570777655
iteration 151, loss = 0.0012039222056046128
iteration 152, loss = 0.0010805183555930853
iteration 153, loss = 0.0011887920554727316
iteration 154, loss = 0.001157838385552168
iteration 155, loss = 0.0011905832216143608
iteration 156, loss = 0.0011340687051415443
iteration 157, loss = 0.001444578287191689
iteration 158, loss = 0.001238084165379405
iteration 159, loss = 0.0010608721058815718
iteration 160, loss = 0.00108344666659832
iteration 161, loss = 0.001227285247296095
iteration 162, loss = 0.0012811280321329832
iteration 163, loss = 0.0015889767091721296
iteration 164, loss = 0.0018086022464558482
iteration 165, loss = 0.0022012940607964993
iteration 166, loss = 0.00117327063344419
iteration 167, loss = 0.0010970719158649445
iteration 168, loss = 0.001446411944925785
iteration 169, loss = 0.00174189533572644
iteration 170, loss = 0.001137697952799499
iteration 171, loss = 0.0012606654781848192
iteration 172, loss = 0.0014911906328052282
iteration 173, loss = 0.0019703484140336514
iteration 174, loss = 0.002457941882312298
iteration 175, loss = 0.0010088440030813217
iteration 176, loss = 0.0011312522692605853
iteration 177, loss = 0.00107267324347049
iteration 178, loss = 0.0015208550030365586
iteration 179, loss = 0.0023588668555021286
iteration 180, loss = 0.0011292241979390383
iteration 181, loss = 0.0013976221671327949
iteration 182, loss = 0.0012980725150555372
iteration 183, loss = 0.001403537578880787
iteration 184, loss = 0.000957828015089035
iteration 185, loss = 0.002537507563829422
iteration 186, loss = 0.0013014910509809852
iteration 187, loss = 0.0014757984317839146
iteration 188, loss = 0.0014598583802580833
iteration 189, loss = 0.0017724130302667618
iteration 190, loss = 0.0013958774507045746
iteration 191, loss = 0.0014302085619419813
iteration 192, loss = 0.0010516071924939752
iteration 193, loss = 0.0011691850377246737
iteration 194, loss = 0.0019556693732738495
iteration 195, loss = 0.0015768874436616898
iteration 196, loss = 0.0012071937089785933
iteration 197, loss = 0.001034614397212863
iteration 198, loss = 0.001222826773300767
iteration 199, loss = 0.0012448669876903296
iteration 200, loss = 0.001560160773806274
iteration 201, loss = 0.0012359952088445425
iteration 202, loss = 0.0012088499497622252
iteration 203, loss = 0.0011082547716796398
iteration 204, loss = 0.0011237442959100008
iteration 205, loss = 0.0011477685766294599
iteration 206, loss = 0.0011584506137296557
iteration 207, loss = 0.00095616519683972
iteration 208, loss = 0.0015149122336879373
iteration 209, loss = 0.0011441841488704085
iteration 210, loss = 0.0021798431407660246
iteration 211, loss = 0.001220532925799489
iteration 212, loss = 0.0012829239713028073
iteration 213, loss = 0.001683525973930955
iteration 214, loss = 0.0018281781813129783
iteration 215, loss = 0.0023512193001806736
iteration 216, loss = 0.0011363218072801828
iteration 217, loss = 0.001044335076585412
iteration 218, loss = 0.0018867382314056158
iteration 219, loss = 0.002365473657846451
iteration 220, loss = 0.0012465374311432242
iteration 221, loss = 0.0012173400027677417
iteration 222, loss = 0.001179354963824153
iteration 223, loss = 0.0010508648119866848
iteration 224, loss = 0.0011287096422165632
iteration 225, loss = 0.0011294338619336486
iteration 226, loss = 0.0012373625067993999
iteration 227, loss = 0.0018398886313661933
iteration 228, loss = 0.001170828938484192
iteration 229, loss = 0.0010673762299120426
iteration 230, loss = 0.0013123437529429793
iteration 231, loss = 0.0012517966097220778
iteration 232, loss = 0.0017880784580484033
iteration 233, loss = 0.0015823387075215578
iteration 234, loss = 0.0021157015580683947
iteration 235, loss = 0.0010803417535498738
iteration 236, loss = 0.0010018228786066175
iteration 237, loss = 0.0011623477330431342
iteration 238, loss = 0.0011021302780136466
iteration 239, loss = 0.001099944580346346
iteration 240, loss = 0.000944029598031193
iteration 241, loss = 0.0010939340572804213
iteration 242, loss = 0.00153181457426399
iteration 243, loss = 0.0010294910753145814
iteration 244, loss = 0.001093833358027041
iteration 245, loss = 0.0013151164166629314
iteration 246, loss = 0.0011444478295743465
iteration 247, loss = 0.0020514684729278088
iteration 248, loss = 0.0010362514294683933
iteration 249, loss = 0.001003239187411964
iteration 250, loss = 0.0015835788799449801
iteration 251, loss = 0.0011348242405802011
iteration 252, loss = 0.0016513887094333768
iteration 253, loss = 0.002156580099835992
iteration 254, loss = 0.0010936787584796548
iteration 255, loss = 0.0020389966666698456
iteration 256, loss = 0.0011455442290753126
iteration 257, loss = 0.0010517159244045615
iteration 258, loss = 0.0010055613238364458
iteration 259, loss = 0.0013056478928774595
iteration 260, loss = 0.0016768660861998796
iteration 261, loss = 0.0011208101641386747
iteration 262, loss = 0.0010100165382027626
iteration 263, loss = 0.0010725614847615361
iteration 264, loss = 0.0012591255363076925
iteration 265, loss = 0.0021068062633275986
iteration 266, loss = 0.0011884928680956364
iteration 267, loss = 0.0010758076095953584
iteration 268, loss = 0.0010637677041813731
iteration 269, loss = 0.0010373718105256557
iteration 270, loss = 0.0010169083252549171
iteration 271, loss = 0.0016936417669057846
iteration 272, loss = 0.0012279566144570708
iteration 273, loss = 0.00098754174541682
iteration 274, loss = 0.0010736245894804597
iteration 275, loss = 0.001432437333278358
iteration 276, loss = 0.0035335300490260124
iteration 277, loss = 0.0010358060244470835
iteration 278, loss = 0.00140883878339082
iteration 279, loss = 0.001397416926920414
iteration 280, loss = 0.0011726299999281764
iteration 281, loss = 0.0010841013863682747
iteration 282, loss = 0.001271391985937953
iteration 283, loss = 0.00117195351049304
iteration 284, loss = 0.002343325410038233
iteration 285, loss = 0.001244263374246657
iteration 286, loss = 0.0015707815764471889
iteration 287, loss = 0.0011055254144594073
iteration 288, loss = 0.0012159484904259443
iteration 289, loss = 0.0017762075876817107
iteration 290, loss = 0.0010346508352085948
iteration 291, loss = 0.00126938265748322
iteration 292, loss = 0.0013158128131181002
iteration 293, loss = 0.001334525877609849
iteration 294, loss = 0.0016745391767472029
iteration 295, loss = 0.0012884285533800721
iteration 296, loss = 0.0009998355526477098
iteration 297, loss = 0.0013420062605291605
iteration 298, loss = 0.0012996528530493379
iteration 299, loss = 0.0013141147792339325
iteration 300, loss = 0.0014545671874657273
iteration 1, loss = 0.0012773164780810475
iteration 2, loss = 0.0012338242959231138
iteration 3, loss = 0.0014565803576260805
iteration 4, loss = 0.0011210195953026414
iteration 5, loss = 0.001143056433647871
iteration 6, loss = 0.0015465245814993978
iteration 7, loss = 0.0012829926563426852
iteration 8, loss = 0.002081940183416009
iteration 9, loss = 0.0023371330462396145
iteration 10, loss = 0.0012398314429447055
iteration 11, loss = 0.001123097026720643
iteration 12, loss = 0.0012115369318053126
iteration 13, loss = 0.001950195524841547
iteration 14, loss = 0.0014077353989705443
iteration 15, loss = 0.0010624380083754659
iteration 16, loss = 0.0017043681582435966
iteration 17, loss = 0.0010556691559031606
iteration 18, loss = 0.001040226430632174
iteration 19, loss = 0.0011338493786752224
iteration 20, loss = 0.001129615819081664
iteration 21, loss = 0.0013711833162233233
iteration 22, loss = 0.0012473452370613813
iteration 23, loss = 0.0009066268685273826
iteration 24, loss = 0.000993965775705874
iteration 25, loss = 0.0013145972043275833
iteration 26, loss = 0.0012361922999843955
iteration 27, loss = 0.0010209586471319199
iteration 28, loss = 0.002145161386579275
iteration 29, loss = 0.0008683000924065709
iteration 30, loss = 0.0010126888519153
iteration 31, loss = 0.0011896388605237007
iteration 32, loss = 0.0010077834594994783
iteration 33, loss = 0.0011864834232255816
iteration 34, loss = 0.0013277542311698198
iteration 35, loss = 0.0014122170396149158
iteration 36, loss = 0.0013381262542679906
iteration 37, loss = 0.0010377020807936788
iteration 38, loss = 0.0016892350977286696
iteration 39, loss = 0.0009914033580571413
iteration 40, loss = 0.0011855470947921276
iteration 41, loss = 0.001072360435500741
iteration 42, loss = 0.0011124910088256001
iteration 43, loss = 0.0013204850256443024
iteration 44, loss = 0.0017459271475672722
iteration 45, loss = 0.0013418964808806777
iteration 46, loss = 0.0011290553957223892
iteration 47, loss = 0.001041732612065971
iteration 48, loss = 0.00161936623044312
iteration 49, loss = 0.0011308379471302032
iteration 50, loss = 0.0011402566451579332
iteration 51, loss = 0.001150146359577775
iteration 52, loss = 0.0011384106473997235
iteration 53, loss = 0.0012671997537836432
iteration 54, loss = 0.0016664761351421475
iteration 55, loss = 0.0011193950194865465
iteration 56, loss = 0.0010380959138274193
iteration 57, loss = 0.0010858987225219607
iteration 58, loss = 0.0010548754362389445
iteration 59, loss = 0.0014818459749221802
iteration 60, loss = 0.0012715777847915888
iteration 61, loss = 0.001215753029100597
iteration 62, loss = 0.0014330599224194884
iteration 63, loss = 0.001174870994873345
iteration 64, loss = 0.0017285372596234083
iteration 65, loss = 0.001468817936256528
iteration 66, loss = 0.001043833908624947
iteration 67, loss = 0.0009867839980870485
iteration 68, loss = 0.0008990883361548185
iteration 69, loss = 0.0009458889253437519
iteration 70, loss = 0.001068977522663772
iteration 71, loss = 0.001589154708199203
iteration 72, loss = 0.001153216348029673
iteration 73, loss = 0.001128624309785664
iteration 74, loss = 0.0019927355460822582
iteration 75, loss = 0.0015051697846502066
iteration 76, loss = 0.0010773653630167246
iteration 77, loss = 0.0011462330585345626
iteration 78, loss = 0.0013568045105785131
iteration 79, loss = 0.0011669726809486747
iteration 80, loss = 0.0014714306453242898
iteration 81, loss = 0.0014938657404854894
iteration 82, loss = 0.0011863610707223415
iteration 83, loss = 0.0011879054363816977
iteration 84, loss = 0.000936488329898566
iteration 85, loss = 0.0014086684677749872
iteration 86, loss = 0.0012756804935634136
iteration 87, loss = 0.0009549036039970815
iteration 88, loss = 0.001436902559362352
iteration 89, loss = 0.0013730349019169807
iteration 90, loss = 0.00248717307113111
iteration 91, loss = 0.001273208181373775
iteration 92, loss = 0.0013021149206906557
iteration 93, loss = 0.0013835998252034187
iteration 94, loss = 0.0016402009641751647
iteration 95, loss = 0.0009565924992784858
iteration 96, loss = 0.0010049596894532442
iteration 97, loss = 0.001479815342463553
iteration 98, loss = 0.0011595530668273568
iteration 99, loss = 0.0010198161471635103
iteration 100, loss = 0.0012255581095814705
iteration 101, loss = 0.0011196124833077192
iteration 102, loss = 0.0019239188404753804
iteration 103, loss = 0.0012589826947078109
iteration 104, loss = 0.0015694769099354744
iteration 105, loss = 0.0009962220210582018
iteration 106, loss = 0.0012020639842376113
iteration 107, loss = 0.0011535525554791093
iteration 108, loss = 0.0014359094202518463
iteration 109, loss = 0.0013449641410261393
iteration 110, loss = 0.0009010981302708387
iteration 111, loss = 0.002167691709473729
iteration 112, loss = 0.0014262183103710413
iteration 113, loss = 0.0015955298440530896
iteration 114, loss = 0.001548489206470549
iteration 115, loss = 0.0009705687407404184
iteration 116, loss = 0.0018241156358271837
iteration 117, loss = 0.0018560917815193534
iteration 118, loss = 0.0010602821130305529
iteration 119, loss = 0.0010812485124915838
iteration 120, loss = 0.0020046245772391558
iteration 121, loss = 0.0019588025752454996
iteration 122, loss = 0.0014189835637807846
iteration 123, loss = 0.0011156115215271711
iteration 124, loss = 0.0013506849063560367
iteration 125, loss = 0.0015001640422269702
iteration 126, loss = 0.0015803150599822402
iteration 127, loss = 0.002249307930469513
iteration 128, loss = 0.001258307253010571
iteration 129, loss = 0.0010240726405754685
iteration 130, loss = 0.0010712681105360389
iteration 131, loss = 0.0010117366909980774
iteration 132, loss = 0.00146111857611686
iteration 133, loss = 0.0012853462249040604
iteration 134, loss = 0.0015650545246899128
iteration 135, loss = 0.0015358240343630314
iteration 136, loss = 0.000969697895925492
iteration 137, loss = 0.001866076374426484
iteration 138, loss = 0.0011692699044942856
iteration 139, loss = 0.001045991200953722
iteration 140, loss = 0.001312960754148662
iteration 141, loss = 0.0011628823122009635
iteration 142, loss = 0.0016153320902958512
iteration 143, loss = 0.0012195382732897997
iteration 144, loss = 0.0014540889533236623
iteration 145, loss = 0.0011643179459497333
iteration 146, loss = 0.00124307197984308
iteration 147, loss = 0.0009852976072579622
iteration 148, loss = 0.0009547414374537766
iteration 149, loss = 0.0010271542705595493
iteration 150, loss = 0.0015480670845136046
iteration 151, loss = 0.0011893972987309098
iteration 152, loss = 0.001964017516002059
iteration 153, loss = 0.0010457211174070835
iteration 154, loss = 0.0008911847253330052
iteration 155, loss = 0.0012651033466681838
iteration 156, loss = 0.0009974940912798047
iteration 157, loss = 0.0016109026037156582
iteration 158, loss = 0.0012149743270128965
iteration 159, loss = 0.0009444508468732238
iteration 160, loss = 0.0013831094838678837
iteration 161, loss = 0.0012424456654116511
iteration 162, loss = 0.0012570975814014673
iteration 163, loss = 0.0010808458318933845
iteration 164, loss = 0.0010531417792662978
iteration 165, loss = 0.001422576024197042
iteration 166, loss = 0.0009908188367262483
iteration 167, loss = 0.0009913580724969506
iteration 168, loss = 0.0011635047849267721
iteration 169, loss = 0.0019197067013010383
iteration 170, loss = 0.0014388005947694182
iteration 171, loss = 0.0010199513053521514
iteration 172, loss = 0.0010832163970917463
iteration 173, loss = 0.0010915036546066403
iteration 174, loss = 0.001931864069774747
iteration 175, loss = 0.0011613512178882957
iteration 176, loss = 0.0012428867630660534
iteration 177, loss = 0.0016878184396773577
iteration 178, loss = 0.001083813956938684
iteration 179, loss = 0.002308271126821637
iteration 180, loss = 0.0015193179715424776
iteration 181, loss = 0.001211286522448063
iteration 182, loss = 0.002709770342335105
iteration 183, loss = 0.0029101609252393246
iteration 184, loss = 0.002222111215814948
iteration 185, loss = 0.000923136540222913
iteration 186, loss = 0.0010820400202646852
iteration 187, loss = 0.0013695636298507452
iteration 188, loss = 0.0012972188415005803
iteration 189, loss = 0.0014038835652172565
iteration 190, loss = 0.001162637840025127
iteration 191, loss = 0.0018985588103532791
iteration 192, loss = 0.0028030280955135822
iteration 193, loss = 0.00117378996219486
iteration 194, loss = 0.001024489407427609
iteration 195, loss = 0.001047104480676353
iteration 196, loss = 0.001275754300877452
iteration 197, loss = 0.0010317168198525906
iteration 198, loss = 0.000970392138697207
iteration 199, loss = 0.0010703939478844404
iteration 200, loss = 0.0015528543153777719
iteration 201, loss = 0.002483754651620984
iteration 202, loss = 0.0020466302521526814
iteration 203, loss = 0.0011623764876276255
iteration 204, loss = 0.002101040678098798
iteration 205, loss = 0.002006018999963999
iteration 206, loss = 0.001857453491538763
iteration 207, loss = 0.0013245813315734267
iteration 208, loss = 0.0011068409075960517
iteration 209, loss = 0.000980685348622501
iteration 210, loss = 0.0013342259917408228
iteration 211, loss = 0.0009484518668614328
iteration 212, loss = 0.0012047283817082644
iteration 213, loss = 0.0012073613470420241
iteration 214, loss = 0.0012271192390471697
iteration 215, loss = 0.0010631551267579198
iteration 216, loss = 0.0011285151122137904
iteration 217, loss = 0.0012731949100270867
iteration 218, loss = 0.0015596803277730942
iteration 219, loss = 0.0012805924052372575
iteration 220, loss = 0.0013068894622847438
iteration 221, loss = 0.001089434139430523
iteration 222, loss = 0.0009831924689933658
iteration 223, loss = 0.0018687958363443613
iteration 224, loss = 0.0010115267941728234
iteration 225, loss = 0.00156120874453336
iteration 226, loss = 0.0015708943828940392
iteration 227, loss = 0.0010242826538160443
iteration 228, loss = 0.001250733039341867
iteration 229, loss = 0.0021258678752928972
iteration 230, loss = 0.0012577533489093184
iteration 231, loss = 0.0011961234267801046
iteration 232, loss = 0.0008528089383617043
iteration 233, loss = 0.0010204389691352844
iteration 234, loss = 0.0011795819737017155
iteration 235, loss = 0.0014543798752129078
iteration 236, loss = 0.0011106178862974048
iteration 237, loss = 0.000991765526123345
iteration 238, loss = 0.0011164864990860224
iteration 239, loss = 0.0012296432396396995
iteration 240, loss = 0.0012062309542670846
iteration 241, loss = 0.0011368764098733664
iteration 242, loss = 0.0008808481507003307
iteration 243, loss = 0.0012219022028148174
iteration 244, loss = 0.0010233963839709759
iteration 245, loss = 0.0010340575827285647
iteration 246, loss = 0.001051448518410325
iteration 247, loss = 0.0008869072771631181
iteration 248, loss = 0.0010319638531655073
iteration 249, loss = 0.0019145692931488156
iteration 250, loss = 0.0011933202622458339
iteration 251, loss = 0.0009600433986634016
iteration 252, loss = 0.0012239839415997267
iteration 253, loss = 0.0010174764320254326
iteration 254, loss = 0.001131516764871776
iteration 255, loss = 0.0011301126796752214
iteration 256, loss = 0.0010613556951284409
iteration 257, loss = 0.001366359880194068
iteration 258, loss = 0.0012627773685380816
iteration 259, loss = 0.0010749889770522714
iteration 260, loss = 0.0010537364287301898
iteration 261, loss = 0.0009197199251502752
iteration 262, loss = 0.001040074392221868
iteration 263, loss = 0.0013093872694298625
iteration 264, loss = 0.0008848186698742211
iteration 265, loss = 0.0014678060542792082
iteration 266, loss = 0.0011593237286433578
iteration 267, loss = 0.000984327052719891
iteration 268, loss = 0.0009802869753912091
iteration 269, loss = 0.001478042802773416
iteration 270, loss = 0.0013958780327811837
iteration 271, loss = 0.001922659226693213
iteration 272, loss = 0.002230442129075527
iteration 273, loss = 0.0010186880826950073
iteration 274, loss = 0.0030936263501644135
iteration 275, loss = 0.0011423181276768446
iteration 276, loss = 0.0010952448938041925
iteration 277, loss = 0.0022332873195409775
iteration 278, loss = 0.0009141277987509966
iteration 279, loss = 0.0010408993111923337
iteration 280, loss = 0.0016036389861255884
iteration 281, loss = 0.0010967965936288238
iteration 282, loss = 0.0009991918923333287
iteration 283, loss = 0.0012586755910888314
iteration 284, loss = 0.0009459112188778818
iteration 285, loss = 0.001311639090999961
iteration 286, loss = 0.0010165610583499074
iteration 287, loss = 0.0009964000200852752
iteration 288, loss = 0.0010901476489380002
iteration 289, loss = 0.0012280885130167007
iteration 290, loss = 0.0011064325226470828
iteration 291, loss = 0.0013206566218286753
iteration 292, loss = 0.0013376297429203987
iteration 293, loss = 0.0020016375929117203
iteration 294, loss = 0.001699760090559721
iteration 295, loss = 0.0011000955710187554
iteration 296, loss = 0.0012457981938496232
iteration 297, loss = 0.0011429695878177881
iteration 298, loss = 0.0009675610926933587
iteration 299, loss = 0.0010542813688516617
iteration 300, loss = 0.0012536455178633332
iteration 1, loss = 0.0010048642288893461
iteration 2, loss = 0.0011364704696461558
iteration 3, loss = 0.0010107465786859393
iteration 4, loss = 0.0014560213312506676
iteration 5, loss = 0.002242620103061199
iteration 6, loss = 0.0012498172000050545
iteration 7, loss = 0.0012641483917832375
iteration 8, loss = 0.001124477363191545
iteration 9, loss = 0.001247377134859562
iteration 10, loss = 0.0010513135930523276
iteration 11, loss = 0.0014646881027147174
iteration 12, loss = 0.0015951815294101834
iteration 13, loss = 0.001964960480108857
iteration 14, loss = 0.0010122173698619008
iteration 15, loss = 0.001954304287210107
iteration 16, loss = 0.0012609090190380812
iteration 17, loss = 0.001232518581673503
iteration 18, loss = 0.0010889762779697776
iteration 19, loss = 0.0014786027604714036
iteration 20, loss = 0.0009446772746741772
iteration 21, loss = 0.002231258200481534
iteration 22, loss = 0.0009920268785208464
iteration 23, loss = 0.001573404879309237
iteration 24, loss = 0.0010617665247991681
iteration 25, loss = 0.0019134883768856525
iteration 26, loss = 0.0011458219960331917
iteration 27, loss = 0.001208786852657795
iteration 28, loss = 0.0010045755188912153
iteration 29, loss = 0.0017886293353512883
iteration 30, loss = 0.001073498628102243
iteration 31, loss = 0.0009933633264154196
iteration 32, loss = 0.0009992977138608694
iteration 33, loss = 0.0012905540643259883
iteration 34, loss = 0.0010547252604737878
iteration 35, loss = 0.001319581875577569
iteration 36, loss = 0.0013421413023024797
iteration 37, loss = 0.0009618208860047162
iteration 38, loss = 0.0010036341845989227
iteration 39, loss = 0.00109412195160985
iteration 40, loss = 0.001852545770816505
iteration 41, loss = 0.0016078712651506066
iteration 42, loss = 0.0020529888570308685
iteration 43, loss = 0.0023338627070188522
iteration 44, loss = 0.001388543052598834
iteration 45, loss = 0.0011380910873413086
iteration 46, loss = 0.001095271436497569
iteration 47, loss = 0.001324999611824751
iteration 48, loss = 0.001110982382670045
iteration 49, loss = 0.0011863689869642258
iteration 50, loss = 0.0009850984206423163
iteration 51, loss = 0.0008593101520091295
iteration 52, loss = 0.000991669250652194
iteration 53, loss = 0.0011947420425713062
iteration 54, loss = 0.0011181546142324805
iteration 55, loss = 0.0009452117374166846
iteration 56, loss = 0.0010885557858273387
iteration 57, loss = 0.0015982097247615457
iteration 58, loss = 0.0013221993576735258
iteration 59, loss = 0.0011104259174317122
iteration 60, loss = 0.0009505259804427624
iteration 61, loss = 0.0010763694299384952
iteration 62, loss = 0.0010250136256217957
iteration 63, loss = 0.0009020238649100065
iteration 64, loss = 0.0008437064825557172
iteration 65, loss = 0.0009086008067242801
iteration 66, loss = 0.0009742911206558347
iteration 67, loss = 0.0009571067639626563
iteration 68, loss = 0.0009875755058601499
iteration 69, loss = 0.0009171140845865011
iteration 70, loss = 0.0014081469271332026
iteration 71, loss = 0.0010421262122690678
iteration 72, loss = 0.0021496531553566456
iteration 73, loss = 0.0013667169259861112
iteration 74, loss = 0.0012211913708597422
iteration 75, loss = 0.001104343100450933
iteration 76, loss = 0.0010399785824120045
iteration 77, loss = 0.0014294821303337812
iteration 78, loss = 0.001061441726051271
iteration 79, loss = 0.0009713367908261716
iteration 80, loss = 0.0013849709648638964
iteration 81, loss = 0.0015787413576617837
iteration 82, loss = 0.0011133939260616899
iteration 83, loss = 0.0010207107989117503
iteration 84, loss = 0.0022669073659926653
iteration 85, loss = 0.0013715910026803613
iteration 86, loss = 0.0010136226192116737
iteration 87, loss = 0.001249452936463058
iteration 88, loss = 0.0013149583246558905
iteration 89, loss = 0.0015747938305139542
iteration 90, loss = 0.0010591548634693027
iteration 91, loss = 0.002020725514739752
iteration 92, loss = 0.0009387016762048006
iteration 93, loss = 0.0011754861334338784
iteration 94, loss = 0.0009088084334507585
iteration 95, loss = 0.0010466336971148849
iteration 96, loss = 0.0011598834535107017
iteration 97, loss = 0.0024038099218159914
iteration 98, loss = 0.0009923819452524185
iteration 99, loss = 0.0011167076881974936
iteration 100, loss = 0.0010895157465711236
iteration 101, loss = 0.0021692197769880295
iteration 102, loss = 0.0010934937745332718
iteration 103, loss = 0.0009028219501487911
iteration 104, loss = 0.0009671862353570759
iteration 105, loss = 0.0012349230237305164
iteration 106, loss = 0.0018985316855832934
iteration 107, loss = 0.0010185661958530545
iteration 108, loss = 0.0015469255158677697
iteration 109, loss = 0.0008795182220637798
iteration 110, loss = 0.0009368359460495412
iteration 111, loss = 0.000843605725094676
iteration 112, loss = 0.001421201741322875
iteration 113, loss = 0.0020162612199783325
iteration 114, loss = 0.0013695030938833952
iteration 115, loss = 0.001061979099176824
iteration 116, loss = 0.001265298225916922
iteration 117, loss = 0.0009667602716945112
iteration 118, loss = 0.002049025846645236
iteration 119, loss = 0.0013791554374620318
iteration 120, loss = 0.001240927609615028
iteration 121, loss = 0.001068578683771193
iteration 122, loss = 0.001082287635654211
iteration 123, loss = 0.0014545457670465112
iteration 124, loss = 0.0012537771835923195
iteration 125, loss = 0.000983143923804164
iteration 126, loss = 0.0009280219674110413
iteration 127, loss = 0.0013634562492370605
iteration 128, loss = 0.0009936143178492785
iteration 129, loss = 0.0010314012179151177
iteration 130, loss = 0.0010632331250235438
iteration 131, loss = 0.0014313871506601572
iteration 132, loss = 0.0010542413219809532
iteration 133, loss = 0.001203979249112308
iteration 134, loss = 0.0015265336260199547
iteration 135, loss = 0.0014034354826435447
iteration 136, loss = 0.0009969109669327736
iteration 137, loss = 0.001249290886335075
iteration 138, loss = 0.0011368326377123594
iteration 139, loss = 0.000879803323186934
iteration 140, loss = 0.0008961050771176815
iteration 141, loss = 0.0012307558208703995
iteration 142, loss = 0.0010876673040911555
iteration 143, loss = 0.0008485607104375958
iteration 144, loss = 0.0013343507889658213
iteration 145, loss = 0.0010922516230493784
iteration 146, loss = 0.0012577219167724252
iteration 147, loss = 0.001073405728675425
iteration 148, loss = 0.0009906926425173879
iteration 149, loss = 0.0010887148091569543
iteration 150, loss = 0.001365953590720892
iteration 151, loss = 0.0010971121955662966
iteration 152, loss = 0.0011905740248039365
iteration 153, loss = 0.001072689425200224
iteration 154, loss = 0.0010713695082813501
iteration 155, loss = 0.0025706100277602673
iteration 156, loss = 0.0010951674776151776
iteration 157, loss = 0.002385345520451665
iteration 158, loss = 0.0019484248477965593
iteration 159, loss = 0.0010070590069517493
iteration 160, loss = 0.0011179136345162988
iteration 161, loss = 0.0010105164255946875
iteration 162, loss = 0.0021835006773471832
iteration 163, loss = 0.0009151392732746899
iteration 164, loss = 0.001983839552849531
iteration 165, loss = 0.0008256814908236265
iteration 166, loss = 0.001380285480991006
iteration 167, loss = 0.0010007324162870646
iteration 168, loss = 0.0008895513601601124
iteration 169, loss = 0.001278757001273334
iteration 170, loss = 0.0010166177526116371
iteration 171, loss = 0.0010727206245064735
iteration 172, loss = 0.0011918647214770317
iteration 173, loss = 0.0011024567065760493
iteration 174, loss = 0.001098273554816842
iteration 175, loss = 0.00128623575437814
iteration 176, loss = 0.0011741301277652383
iteration 177, loss = 0.0010769444052129984
iteration 178, loss = 0.0020727587398141623
iteration 179, loss = 0.0010135334450751543
iteration 180, loss = 0.0018036536639556289
iteration 181, loss = 0.0016289553605020046
iteration 182, loss = 0.0010342785390093923
iteration 183, loss = 0.002831099322065711
iteration 184, loss = 0.0009878729470074177
iteration 185, loss = 0.0012269055005162954
iteration 186, loss = 0.0009499472798779607
iteration 187, loss = 0.0012115988647565246
iteration 188, loss = 0.0008954689837992191
iteration 189, loss = 0.0011964678997173905
iteration 190, loss = 0.0008663973421789706
iteration 191, loss = 0.0010305274045094848
iteration 192, loss = 0.001040411414578557
iteration 193, loss = 0.0015634977025911212
iteration 194, loss = 0.0010679689003154635
iteration 195, loss = 0.001000170479528606
iteration 196, loss = 0.0011196108534932137
iteration 197, loss = 0.001028982107527554
iteration 198, loss = 0.0013374112313613296
iteration 199, loss = 0.0010810967069119215
iteration 200, loss = 0.0018561884062364697
iteration 201, loss = 0.0011672894470393658
iteration 202, loss = 0.0010342723689973354
iteration 203, loss = 0.000998490722849965
iteration 204, loss = 0.0010744293686002493
iteration 205, loss = 0.0011016253847628832
iteration 206, loss = 0.0008555075619369745
iteration 207, loss = 0.0015593275893479586
iteration 208, loss = 0.0013462011702358723
iteration 209, loss = 0.0009308563894592226
iteration 210, loss = 0.0022868977393954992
iteration 211, loss = 0.0010426605585962534
iteration 212, loss = 0.0022016612347215414
iteration 213, loss = 0.0011129692429676652
iteration 214, loss = 0.0012183040380477905
iteration 215, loss = 0.0008683493360877037
iteration 216, loss = 0.0012530396925285459
iteration 217, loss = 0.0013921881327405572
iteration 218, loss = 0.0010907521937042475
iteration 219, loss = 0.0020820326171815395
iteration 220, loss = 0.0013334124814718962
iteration 221, loss = 0.001460541388951242
iteration 222, loss = 0.0009629816631786525
iteration 223, loss = 0.000979302916675806
iteration 224, loss = 0.0009449486387893558
iteration 225, loss = 0.0010055118473246694
iteration 226, loss = 0.001195752527564764
iteration 227, loss = 0.0009494262048974633
iteration 228, loss = 0.001232355018146336
iteration 229, loss = 0.0010475536109879613
iteration 230, loss = 0.0011966315796598792
iteration 231, loss = 0.0013637839583680034
iteration 232, loss = 0.001049567712470889
iteration 233, loss = 0.0009921776363626122
iteration 234, loss = 0.0014763132203370333
iteration 235, loss = 0.00115688843652606
iteration 236, loss = 0.0009413445368409157
iteration 237, loss = 0.0012656728504225612
iteration 238, loss = 0.0014619422145187855
iteration 239, loss = 0.0008899827371351421
iteration 240, loss = 0.0013918920885771513
iteration 241, loss = 0.001140131615102291
iteration 242, loss = 0.0011591581860557199
iteration 243, loss = 0.0010893158614635468
iteration 244, loss = 0.0011153648374602199
iteration 245, loss = 0.0011081642005592585
iteration 246, loss = 0.0010347514180466533
iteration 247, loss = 0.0009167136158794165
iteration 248, loss = 0.0009316497016698122
iteration 249, loss = 0.0010685690212994814
iteration 250, loss = 0.0011674449779093266
iteration 251, loss = 0.0011115989182144403
iteration 252, loss = 0.001050662249326706
iteration 253, loss = 0.0012882123701274395
iteration 254, loss = 0.0009095690329559147
iteration 255, loss = 0.0019437664886936545
iteration 256, loss = 0.0008620655862614512
iteration 257, loss = 0.000997053226456046
iteration 258, loss = 0.001037787413224578
iteration 259, loss = 0.001247463864274323
iteration 260, loss = 0.0009303523693233728
iteration 261, loss = 0.001048942795023322
iteration 262, loss = 0.0010272370418533683
iteration 263, loss = 0.0008525624871253967
iteration 264, loss = 0.00095186079852283
iteration 265, loss = 0.0016622620169073343
iteration 266, loss = 0.0015929186483845115
iteration 267, loss = 0.0009888098575174809
iteration 268, loss = 0.0011637373827397823
iteration 269, loss = 0.001025734469294548
iteration 270, loss = 0.0009890106739476323
iteration 271, loss = 0.0010657894890755415
iteration 272, loss = 0.0013995268382132053
iteration 273, loss = 0.0008831937448121607
iteration 274, loss = 0.001157155493274331
iteration 275, loss = 0.0010640951804816723
iteration 276, loss = 0.001066828379407525
iteration 277, loss = 0.0009678410715423524
iteration 278, loss = 0.0012067065108567476
iteration 279, loss = 0.0009294752380810678
iteration 280, loss = 0.0011248429073020816
iteration 281, loss = 0.0010464054066687822
iteration 282, loss = 0.0011695363791659474
iteration 283, loss = 0.0008073589415289462
iteration 284, loss = 0.0012547816149890423
iteration 285, loss = 0.001254096394404769
iteration 286, loss = 0.0010947141563519835
iteration 287, loss = 0.0009170654229819775
iteration 288, loss = 0.001383465714752674
iteration 289, loss = 0.0011671391548588872
iteration 290, loss = 0.0009628531988710165
iteration 291, loss = 0.000988601241260767
iteration 292, loss = 0.001871093874797225
iteration 293, loss = 0.001086789881810546
iteration 294, loss = 0.0010120440274477005
iteration 295, loss = 0.0010996607597917318
iteration 296, loss = 0.001251049805432558
iteration 297, loss = 0.001355595188215375
iteration 298, loss = 0.0010417115408927202
iteration 299, loss = 0.0011563299922272563
iteration 300, loss = 0.0009774963837116957
iteration 1, loss = 0.0009565439540892839
iteration 2, loss = 0.00116243667434901
iteration 3, loss = 0.0008475845097564161
iteration 4, loss = 0.0011514915386214852
iteration 5, loss = 0.0019512675935402513
iteration 6, loss = 0.0012548818485811353
iteration 7, loss = 0.0010609632590785623
iteration 8, loss = 0.0012626324314624071
iteration 9, loss = 0.0011553580407053232
iteration 10, loss = 0.0009327359730377793
iteration 11, loss = 0.0010470595443621278
iteration 12, loss = 0.001154269790276885
iteration 13, loss = 0.001006264123134315
iteration 14, loss = 0.0010261768475174904
iteration 15, loss = 0.0009014311945065856
iteration 16, loss = 0.0011752850841730833
iteration 17, loss = 0.0010363183682784438
iteration 18, loss = 0.00115046720020473
iteration 19, loss = 0.0009588078246451914
iteration 20, loss = 0.0009979233145713806
iteration 21, loss = 0.0014762366190552711
iteration 22, loss = 0.0017135764937847853
iteration 23, loss = 0.0010700530838221312
iteration 24, loss = 0.001139641972258687
iteration 25, loss = 0.001020669238641858
iteration 26, loss = 0.0016242732526734471
iteration 27, loss = 0.001205882290378213
iteration 28, loss = 0.0010556046618148685
iteration 29, loss = 0.0013508175034075975
iteration 30, loss = 0.001288343919441104
iteration 31, loss = 0.0009523491025902331
iteration 32, loss = 0.0011288524838164449
iteration 33, loss = 0.001027814345434308
iteration 34, loss = 0.0012239854549989104
iteration 35, loss = 0.0009418993140570819
iteration 36, loss = 0.0009500101441517472
iteration 37, loss = 0.0008717046584933996
iteration 38, loss = 0.0010499070631340146
iteration 39, loss = 0.0010107234120368958
iteration 40, loss = 0.0012175978627055883
iteration 41, loss = 0.001307132886722684
iteration 42, loss = 0.0010163560509681702
iteration 43, loss = 0.0011857060017064214
iteration 44, loss = 0.0009667183039709926
iteration 45, loss = 0.0011187571799382567
iteration 46, loss = 0.0011399297509342432
iteration 47, loss = 0.0009741765097714961
iteration 48, loss = 0.0009179123444482684
iteration 49, loss = 0.001122250221669674
iteration 50, loss = 0.0009242558153346181
iteration 51, loss = 0.0010071606375277042
iteration 52, loss = 0.00180693413130939
iteration 53, loss = 0.0010611769976094365
iteration 54, loss = 0.0009588669054210186
iteration 55, loss = 0.0012943793553858995
iteration 56, loss = 0.001284235855564475
iteration 57, loss = 0.001149096293374896
iteration 58, loss = 0.0012870532227680087
iteration 59, loss = 0.0009555906872265041
iteration 60, loss = 0.0012548790546134114
iteration 61, loss = 0.0009687520796433091
iteration 62, loss = 0.0011555036762729287
iteration 63, loss = 0.0009673871682025492
iteration 64, loss = 0.0011249897070229053
iteration 65, loss = 0.0019021013285964727
iteration 66, loss = 0.0009401260176673532
iteration 67, loss = 0.0009260036167688668
iteration 68, loss = 0.0013924206141382456
iteration 69, loss = 0.0010606623254716396
iteration 70, loss = 0.0009742339607328176
iteration 71, loss = 0.0010055204620584846
iteration 72, loss = 0.0010431279661133885
iteration 73, loss = 0.0011009557638317347
iteration 74, loss = 0.0009270718437619507
iteration 75, loss = 0.0009213338489644229
iteration 76, loss = 0.0009753494523465633
iteration 77, loss = 0.001857605529949069
iteration 78, loss = 0.0011933241039514542
iteration 79, loss = 0.0011987851466983557
iteration 80, loss = 0.0008816646877676249
iteration 81, loss = 0.001887081773020327
iteration 82, loss = 0.0012283293763175607
iteration 83, loss = 0.0013369504595175385
iteration 84, loss = 0.0010679413098841906
iteration 85, loss = 0.0009549069800414145
iteration 86, loss = 0.00110893533565104
iteration 87, loss = 0.0009209213312715292
iteration 88, loss = 0.0010088026756420732
iteration 89, loss = 0.0011124423472210765
iteration 90, loss = 0.0009645020472817123
iteration 91, loss = 0.001047252444550395
iteration 92, loss = 0.0009638412739150226
iteration 93, loss = 0.0011306754313409328
iteration 94, loss = 0.0011326238745823503
iteration 95, loss = 0.0012181935599073768
iteration 96, loss = 0.001191715826280415
iteration 97, loss = 0.0011683322954922915
iteration 98, loss = 0.0008884700946509838
iteration 99, loss = 0.001250862842425704
iteration 100, loss = 0.0015360672259703279
iteration 101, loss = 0.0013459851033985615
iteration 102, loss = 0.0009825327433645725
iteration 103, loss = 0.0027725191321223974
iteration 104, loss = 0.0010906591778621078
iteration 105, loss = 0.0010877529857680202
iteration 106, loss = 0.0010418047895655036
iteration 107, loss = 0.0011772515717893839
iteration 108, loss = 0.0008187099010683596
iteration 109, loss = 0.0017373852897435427
iteration 110, loss = 0.0008347525727003813
iteration 111, loss = 0.0010357287246733904
iteration 112, loss = 0.0010813467670232058
iteration 113, loss = 0.0010014207800850272
iteration 114, loss = 0.0012896166881546378
iteration 115, loss = 0.0011218005092814565
iteration 116, loss = 0.0009492953540757298
iteration 117, loss = 0.0011151419021189213
iteration 118, loss = 0.0011262092739343643
iteration 119, loss = 0.0017590428469702601
iteration 120, loss = 0.0011231052922084928
iteration 121, loss = 0.0008684748318046331
iteration 122, loss = 0.0007962216041050851
iteration 123, loss = 0.001259416458196938
iteration 124, loss = 0.0010330608347430825
iteration 125, loss = 0.0023667195346206427
iteration 126, loss = 0.0007629249012097716
iteration 127, loss = 0.0011829112190753222
iteration 128, loss = 0.0011082978453487158
iteration 129, loss = 0.0009742842521518469
iteration 130, loss = 0.000913728610612452
iteration 131, loss = 0.0010335973929613829
iteration 132, loss = 0.0008294435683637857
iteration 133, loss = 0.0010562409879639745
iteration 134, loss = 0.001433469937182963
iteration 135, loss = 0.0023005022667348385
iteration 136, loss = 0.0010401124600321054
iteration 137, loss = 0.0008752127760089934
iteration 138, loss = 0.0012893002713099122
iteration 139, loss = 0.0013129257131367922
iteration 140, loss = 0.0014277031878009439
iteration 141, loss = 0.001672577578574419
iteration 142, loss = 0.0008092629141174257
iteration 143, loss = 0.0014427395071834326
iteration 144, loss = 0.0009241290972568095
iteration 145, loss = 0.0013073243899270892
iteration 146, loss = 0.0009548375383019447
iteration 147, loss = 0.0009578029857948422
iteration 148, loss = 0.0010560437804087996
iteration 149, loss = 0.0009679623763076961
iteration 150, loss = 0.0010041187051683664
iteration 151, loss = 0.0009937019785866141
iteration 152, loss = 0.002468704478815198
iteration 153, loss = 0.0009233406162820756
iteration 154, loss = 0.0009021370206028223
iteration 155, loss = 0.0009407138568349183
iteration 156, loss = 0.0009591341367922723
iteration 157, loss = 0.0010731262154877186
iteration 158, loss = 0.0008468002779409289
iteration 159, loss = 0.0008040076354518533
iteration 160, loss = 0.0008561383583582938
iteration 161, loss = 0.0011799338972195983
iteration 162, loss = 0.0008069752948358655
iteration 163, loss = 0.002065312582999468
iteration 164, loss = 0.0009605411905795336
iteration 165, loss = 0.0009445048635825515
iteration 166, loss = 0.0009369474719278514
iteration 167, loss = 0.0009705178672447801
iteration 168, loss = 0.0013947606785222888
iteration 169, loss = 0.0012003285810351372
iteration 170, loss = 0.0008245444623753428
iteration 171, loss = 0.001026895479299128
iteration 172, loss = 0.0007736691040918231
iteration 173, loss = 0.0009961825562641025
iteration 174, loss = 0.0010193580528721213
iteration 175, loss = 0.0008293736027553678
iteration 176, loss = 0.0009201351786032319
iteration 177, loss = 0.0009042645106092095
iteration 178, loss = 0.0011112865759059787
iteration 179, loss = 0.0010096104815602303
iteration 180, loss = 0.0012885801261290908
iteration 181, loss = 0.0010554587934166193
iteration 182, loss = 0.001725770067423582
iteration 183, loss = 0.002062996383756399
iteration 184, loss = 0.0008956710225902498
iteration 185, loss = 0.0011965365847572684
iteration 186, loss = 0.0008600938017480075
iteration 187, loss = 0.0012171015841886401
iteration 188, loss = 0.00130609015468508
iteration 189, loss = 0.001087331329472363
iteration 190, loss = 0.001205169828608632
iteration 191, loss = 0.0008799921488389373
iteration 192, loss = 0.0008476695511490107
iteration 193, loss = 0.0016494228038936853
iteration 194, loss = 0.0012070870725437999
iteration 195, loss = 0.001671437406912446
iteration 196, loss = 0.0010417293524369597
iteration 197, loss = 0.001082563423551619
iteration 198, loss = 0.0009573579882271588
iteration 199, loss = 0.0009346511797048151
iteration 200, loss = 0.0009430506615899503
iteration 201, loss = 0.00237800064496696
iteration 202, loss = 0.0011664375197142363
iteration 203, loss = 0.0009833029471337795
iteration 204, loss = 0.001084187999367714
iteration 205, loss = 0.0009776559891179204
iteration 206, loss = 0.002197613473981619
iteration 207, loss = 0.0009280386730097234
iteration 208, loss = 0.0009914507390931249
iteration 209, loss = 0.0009183263755403459
iteration 210, loss = 0.0011855962220579386
iteration 211, loss = 0.0012015042593702674
iteration 212, loss = 0.0024266941472887993
iteration 213, loss = 0.0008646473288536072
iteration 214, loss = 0.0010646284790709615
iteration 215, loss = 0.0008729635155759752
iteration 216, loss = 0.0010386069770902395
iteration 217, loss = 0.001141118467785418
iteration 218, loss = 0.0009381014388054609
iteration 219, loss = 0.0008851970778778195
iteration 220, loss = 0.0016070319106802344
iteration 221, loss = 0.0015706217382103205
iteration 222, loss = 0.0008678041631355882
iteration 223, loss = 0.0008236497524194419
iteration 224, loss = 0.0015112013788893819
iteration 225, loss = 0.0011965259909629822
iteration 226, loss = 0.0011436421191319823
iteration 227, loss = 0.0013163266703486443
iteration 228, loss = 0.001004936289973557
iteration 229, loss = 0.0015117082512006164
iteration 230, loss = 0.0008166630286723375
iteration 231, loss = 0.0007724971510469913
iteration 232, loss = 0.0013794548576697707
iteration 233, loss = 0.0008530179038643837
iteration 234, loss = 0.0008411157177761197
iteration 235, loss = 0.001287070568650961
iteration 236, loss = 0.0008138403645716608
iteration 237, loss = 0.0008148178458213806
iteration 238, loss = 0.0009391345665790141
iteration 239, loss = 0.0012120383325964212
iteration 240, loss = 0.0010196113726124167
iteration 241, loss = 0.0010134386830031872
iteration 242, loss = 0.0014211626257747412
iteration 243, loss = 0.0009275839547626674
iteration 244, loss = 0.0010382009204477072
iteration 245, loss = 0.0009101663599722087
iteration 246, loss = 0.00204072124324739
iteration 247, loss = 0.0012704721884801984
iteration 248, loss = 0.0008095705416053534
iteration 249, loss = 0.0009722947143018246
iteration 250, loss = 0.0010988051071763039
iteration 251, loss = 0.0008600784931331873
iteration 252, loss = 0.001020988216623664
iteration 253, loss = 0.0011014087358489633
iteration 254, loss = 0.0009990761755034328
iteration 255, loss = 0.0016034201253205538
iteration 256, loss = 0.0009341659024357796
iteration 257, loss = 0.0008893177146092057
iteration 258, loss = 0.0009952741675078869
iteration 259, loss = 0.0013152058236300945
iteration 260, loss = 0.0012749236775562167
iteration 261, loss = 0.002257175277918577
iteration 262, loss = 0.0022201375104486942
iteration 263, loss = 0.00090480234939605
iteration 264, loss = 0.0008538401452824473
iteration 265, loss = 0.0010166241554543376
iteration 266, loss = 0.0008532737265340984
iteration 267, loss = 0.0014851292362436652
iteration 268, loss = 0.0015549403615295887
iteration 269, loss = 0.0009054235415533185
iteration 270, loss = 0.0008879205561242998
iteration 271, loss = 0.0007406356744468212
iteration 272, loss = 0.0008702971390448511
iteration 273, loss = 0.0012731867609545588
iteration 274, loss = 0.0008366488036699593
iteration 275, loss = 0.0009219729108735919
iteration 276, loss = 0.0008621073211543262
iteration 277, loss = 0.0009758611558936536
iteration 278, loss = 0.0009495400590822101
iteration 279, loss = 0.00222605443559587
iteration 280, loss = 0.0026770257391035557
iteration 281, loss = 0.0008209695224650204
iteration 282, loss = 0.0012063994072377682
iteration 283, loss = 0.0007674255175516009
iteration 284, loss = 0.001041696174070239
iteration 285, loss = 0.0012426998000591993
iteration 286, loss = 0.0010033058933913708
iteration 287, loss = 0.0008790480205789208
iteration 288, loss = 0.0010796507121995091
iteration 289, loss = 0.0008407857385464013
iteration 290, loss = 0.001144086243584752
iteration 291, loss = 0.0013798220315948129
iteration 292, loss = 0.0015614302828907967
iteration 293, loss = 0.0016977491322904825
iteration 294, loss = 0.0015168626559898257
iteration 295, loss = 0.0010215110378339887
iteration 296, loss = 0.0011656911810860038
iteration 297, loss = 0.0009088636143133044
iteration 298, loss = 0.001165144145488739
iteration 299, loss = 0.0007953119347803295
iteration 300, loss = 0.0009912248933687806
iteration 1, loss = 0.0010891483398154378
iteration 2, loss = 0.001821606419980526
iteration 3, loss = 0.000988573650829494
iteration 4, loss = 0.0009110434912145138
iteration 5, loss = 0.0012517035938799381
iteration 6, loss = 0.0012392246862873435
iteration 7, loss = 0.0009748040465638041
iteration 8, loss = 0.0017108381725847721
iteration 9, loss = 0.0009060433367267251
iteration 10, loss = 0.0008930411422625184
iteration 11, loss = 0.0009368368773721159
iteration 12, loss = 0.001691322075203061
iteration 13, loss = 0.0008174518006853759
iteration 14, loss = 0.0009034058311954141
iteration 15, loss = 0.0009129248792305589
iteration 16, loss = 0.0009492542594671249
iteration 17, loss = 0.0013948156265541911
iteration 18, loss = 0.0010990018490701914
iteration 19, loss = 0.0017501004040241241
iteration 20, loss = 0.000940508849453181
iteration 21, loss = 0.0012537093134596944
iteration 22, loss = 0.0008883033879101276
iteration 23, loss = 0.0009800672996789217
iteration 24, loss = 0.0012269242433831096
iteration 25, loss = 0.0008973830263130367
iteration 26, loss = 0.0018317027715966105
iteration 27, loss = 0.0008770665735937655
iteration 28, loss = 0.0020604575984179974
iteration 29, loss = 0.001043119584210217
iteration 30, loss = 0.000922761217225343
iteration 31, loss = 0.0010882533388212323
iteration 32, loss = 0.0007176139624789357
iteration 33, loss = 0.0011528199538588524
iteration 34, loss = 0.0010206256993114948
iteration 35, loss = 0.0011562630534172058
iteration 36, loss = 0.0012869419297203422
iteration 37, loss = 0.001280822791159153
iteration 38, loss = 0.0018444417510181665
iteration 39, loss = 0.0009551683906465769
iteration 40, loss = 0.0008568205521441996
iteration 41, loss = 0.0010345260379835963
iteration 42, loss = 0.0013639909448102117
iteration 43, loss = 0.0020668422803282738
iteration 44, loss = 0.0009221523650921881
iteration 45, loss = 0.0009629724081605673
iteration 46, loss = 0.0007610758184455335
iteration 47, loss = 0.0013118453789502382
iteration 48, loss = 0.0009473045938648283
iteration 49, loss = 0.0010972488671541214
iteration 50, loss = 0.000913236232008785
iteration 51, loss = 0.0011522781569510698
iteration 52, loss = 0.0009912336245179176
iteration 53, loss = 0.0009201203938573599
iteration 54, loss = 0.0008760248310863972
iteration 55, loss = 0.0018497215351089835
iteration 56, loss = 0.0010241142008453608
iteration 57, loss = 0.0011282403720542789
iteration 58, loss = 0.0012687155976891518
iteration 59, loss = 0.0009305980638600886
iteration 60, loss = 0.0009125672513619065
iteration 61, loss = 0.0009375433437526226
iteration 62, loss = 0.0009444627212360501
iteration 63, loss = 0.0014056108193472028
iteration 64, loss = 0.0008922620909288526
iteration 65, loss = 0.0012223777594044805
iteration 66, loss = 0.001380491303279996
iteration 67, loss = 0.001218432909809053
iteration 68, loss = 0.0010591932805255055
iteration 69, loss = 0.0014952640049159527
iteration 70, loss = 0.0009233856690116227
iteration 71, loss = 0.0008767691906541586
iteration 72, loss = 0.0012841940624639392
iteration 73, loss = 0.0013150022132322192
iteration 74, loss = 0.0010214262874796987
iteration 75, loss = 0.0008778708870522678
iteration 76, loss = 0.0008631899836473167
iteration 77, loss = 0.001130852964706719
iteration 78, loss = 0.0017582651926204562
iteration 79, loss = 0.0008214759291149676
iteration 80, loss = 0.0009430077625438571
iteration 81, loss = 0.0007770348456688225
iteration 82, loss = 0.0014361048815771937
iteration 83, loss = 0.00097782036755234
iteration 84, loss = 0.0008186532068066299
iteration 85, loss = 0.0009013627422973514
iteration 86, loss = 0.0012307725846767426
iteration 87, loss = 0.0008361105574294925
iteration 88, loss = 0.0011452321195974946
iteration 89, loss = 0.0008847944554872811
iteration 90, loss = 0.0009528283844701946
iteration 91, loss = 0.0011865701526403427
iteration 92, loss = 0.0010185694554820657
iteration 93, loss = 0.001578881754539907
iteration 94, loss = 0.0011355174938216805
iteration 95, loss = 0.0007068911800161004
iteration 96, loss = 0.0009048947831615806
iteration 97, loss = 0.0007680712733417749
iteration 98, loss = 0.0009784310823306441
iteration 99, loss = 0.0016106689581647515
iteration 100, loss = 0.0008280882611870766
iteration 101, loss = 0.0007610431639477611
iteration 102, loss = 0.0011000488884747028
iteration 103, loss = 0.0009711608872748911
iteration 104, loss = 0.0010117776691913605
iteration 105, loss = 0.0014934696955606341
iteration 106, loss = 0.0009434252860955894
iteration 107, loss = 0.001113804755732417
iteration 108, loss = 0.0011528176255524158
iteration 109, loss = 0.0008880361565388739
iteration 110, loss = 0.001084476592950523
iteration 111, loss = 0.0010535219917073846
iteration 112, loss = 0.0008589554927311838
iteration 113, loss = 0.0008163518505170941
iteration 114, loss = 0.0008876005304045975
iteration 115, loss = 0.0009852275252342224
iteration 116, loss = 0.0007870140252634883
iteration 117, loss = 0.0007713463855907321
iteration 118, loss = 0.0008737302850931883
iteration 119, loss = 0.0008661926258355379
iteration 120, loss = 0.0008167976047843695
iteration 121, loss = 0.0008486368460580707
iteration 122, loss = 0.0007805633940733969
iteration 123, loss = 0.001657219254411757
iteration 124, loss = 0.0007874877192080021
iteration 125, loss = 0.0010915710590779781
iteration 126, loss = 0.0008885968709364533
iteration 127, loss = 0.0011568694608286023
iteration 128, loss = 0.0009461361914873123
iteration 129, loss = 0.000905376102309674
iteration 130, loss = 0.000834059901535511
iteration 131, loss = 0.0010570636950433254
iteration 132, loss = 0.0008303382201120257
iteration 133, loss = 0.0009140377514995635
iteration 134, loss = 0.0016925049712881446
iteration 135, loss = 0.0011624558828771114
iteration 136, loss = 0.0008364294189959764
iteration 137, loss = 0.0008661134052090347
iteration 138, loss = 0.0009611140703782439
iteration 139, loss = 0.0013427382800728083
iteration 140, loss = 0.0009245257824659348
iteration 141, loss = 0.0007908663828857243
iteration 142, loss = 0.0010430525289848447
iteration 143, loss = 0.0009147462551482022
iteration 144, loss = 0.0008963936124928296
iteration 145, loss = 0.001171068986877799
iteration 146, loss = 0.0009843296138569713
iteration 147, loss = 0.0014279772294685245
iteration 148, loss = 0.0010732973460108042
iteration 149, loss = 0.0008755750022828579
iteration 150, loss = 0.0013737572589889169
iteration 151, loss = 0.0017535060178488493
iteration 152, loss = 0.0009035750990733504
iteration 153, loss = 0.0011926142033189535
iteration 154, loss = 0.0011180544970557094
iteration 155, loss = 0.0010239430703222752
iteration 156, loss = 0.0010447953827679157
iteration 157, loss = 0.002143733436241746
iteration 158, loss = 0.0019040871411561966
iteration 159, loss = 0.0010764447506517172
iteration 160, loss = 0.0008292103884741664
iteration 161, loss = 0.0009923307225108147
iteration 162, loss = 0.001087230397388339
iteration 163, loss = 0.000922989274840802
iteration 164, loss = 0.0007256788085214794
iteration 165, loss = 0.0009597066091373563
iteration 166, loss = 0.0017442855751141906
iteration 167, loss = 0.0011133559746667743
iteration 168, loss = 0.0008361946674995124
iteration 169, loss = 0.0010556589113548398
iteration 170, loss = 0.0010085951071232557
iteration 171, loss = 0.000926076085306704
iteration 172, loss = 0.0007672083447687328
iteration 173, loss = 0.0009585964726284146
iteration 174, loss = 0.0008515441440977156
iteration 175, loss = 0.0008797190967015922
iteration 176, loss = 0.0009024211321957409
iteration 177, loss = 0.0016275531379505992
iteration 178, loss = 0.0011181398294866085
iteration 179, loss = 0.0008702447521500289
iteration 180, loss = 0.0016593174077570438
iteration 181, loss = 0.0009513712720945477
iteration 182, loss = 0.001031150808557868
iteration 183, loss = 0.0008360331994481385
iteration 184, loss = 0.0009790395852178335
iteration 185, loss = 0.0009792456403374672
iteration 186, loss = 0.0010481190402060747
iteration 187, loss = 0.0009412609506398439
iteration 188, loss = 0.0009714671759866178
iteration 189, loss = 0.0008991901413537562
iteration 190, loss = 0.0015294014010578394
iteration 191, loss = 0.0008964849403128028
iteration 192, loss = 0.0008467994630336761
iteration 193, loss = 0.001674799364991486
iteration 194, loss = 0.000844365858938545
iteration 195, loss = 0.0013174224877730012
iteration 196, loss = 0.0010244552977383137
iteration 197, loss = 0.0009307032451033592
iteration 198, loss = 0.000905594031792134
iteration 199, loss = 0.001064610667526722
iteration 200, loss = 0.0008413644973188639
iteration 201, loss = 0.001109640346840024
iteration 202, loss = 0.001095266081392765
iteration 203, loss = 0.002213397528976202
iteration 204, loss = 0.0009838143596425653
iteration 205, loss = 0.0008405943517573178
iteration 206, loss = 0.0007553178584203124
iteration 207, loss = 0.0015177858294919133
iteration 208, loss = 0.0008771102293394506
iteration 209, loss = 0.000901486724615097
iteration 210, loss = 0.0009606423554942012
iteration 211, loss = 0.001309986342675984
iteration 212, loss = 0.0008671602117829025
iteration 213, loss = 0.0008822656236588955
iteration 214, loss = 0.0018319529481232166
iteration 215, loss = 0.001112086232751608
iteration 216, loss = 0.0012694187462329865
iteration 217, loss = 0.000990995205938816
iteration 218, loss = 0.0009703110554255545
iteration 219, loss = 0.0009033107198774815
iteration 220, loss = 0.0011537171667441726
iteration 221, loss = 0.002122265752404928
iteration 222, loss = 0.0008254777640104294
iteration 223, loss = 0.0015211099525913596
iteration 224, loss = 0.000892756215762347
iteration 225, loss = 0.0011015039635822177
iteration 226, loss = 0.0009322201367467642
iteration 227, loss = 0.0009419819107279181
iteration 228, loss = 0.0013107402483001351
iteration 229, loss = 0.0008172483649104834
iteration 230, loss = 0.0009290939196944237
iteration 231, loss = 0.0008189250365830958
iteration 232, loss = 0.0008393272291868925
iteration 233, loss = 0.001218688441440463
iteration 234, loss = 0.0009422451257705688
iteration 235, loss = 0.0008418837678618729
iteration 236, loss = 0.002418669406324625
iteration 237, loss = 0.001680072396993637
iteration 238, loss = 0.0012753121554851532
iteration 239, loss = 0.0010598618537187576
iteration 240, loss = 0.0018679988570511341
iteration 241, loss = 0.0008749499684199691
iteration 242, loss = 0.0007538251229561865
iteration 243, loss = 0.00134098285343498
iteration 244, loss = 0.0008976528188213706
iteration 245, loss = 0.001501751015894115
iteration 246, loss = 0.00082935526734218
iteration 247, loss = 0.0009411482606083155
iteration 248, loss = 0.0008366876281797886
iteration 249, loss = 0.0009586435044184327
iteration 250, loss = 0.0012120612664148211
iteration 251, loss = 0.00077041209442541
iteration 252, loss = 0.0009097845759242773
iteration 253, loss = 0.0008538049878552556
iteration 254, loss = 0.0007681461283937097
iteration 255, loss = 0.0008876147912815213
iteration 256, loss = 0.0008065885631367564
iteration 257, loss = 0.000981203280389309
iteration 258, loss = 0.0010155347408726811
iteration 259, loss = 0.0007903181831352413
iteration 260, loss = 0.0015139951137825847
iteration 261, loss = 0.001962654059752822
iteration 262, loss = 0.0009390943450853229
iteration 263, loss = 0.0008806900004856288
iteration 264, loss = 0.0010897631291300058
iteration 265, loss = 0.0013882507337257266
iteration 266, loss = 0.0009294075425714254
iteration 267, loss = 0.0012452156515792012
iteration 268, loss = 0.0008357336046174169
iteration 269, loss = 0.0013863274361938238
iteration 270, loss = 0.000716475653462112
iteration 271, loss = 0.0008336999453604221
iteration 272, loss = 0.0008657759171910584
iteration 273, loss = 0.0007369256927631795
iteration 274, loss = 0.0012405324960127473
iteration 275, loss = 0.001181928557343781
iteration 276, loss = 0.001147547853179276
iteration 277, loss = 0.0008786350954324007
iteration 278, loss = 0.0008004387491382658
iteration 279, loss = 0.0008194106048904359
iteration 280, loss = 0.0008883700356818736
iteration 281, loss = 0.0009701157105155289
iteration 282, loss = 0.000771228049416095
iteration 283, loss = 0.0012357196537777781
iteration 284, loss = 0.0008075106889009476
iteration 285, loss = 0.0008605860639363527
iteration 286, loss = 0.0009003721061162651
iteration 287, loss = 0.00100255711004138
iteration 288, loss = 0.0011470967438071966
iteration 289, loss = 0.0010472562862560153
iteration 290, loss = 0.0010132850147783756
iteration 291, loss = 0.000935529125854373
iteration 292, loss = 0.0011107402388006449
iteration 293, loss = 0.00101382820867002
iteration 294, loss = 0.001255251932889223
iteration 295, loss = 0.0014940258115530014
iteration 296, loss = 0.0007346934289671481
iteration 297, loss = 0.0011725049698725343
iteration 298, loss = 0.001100029330700636
iteration 299, loss = 0.0008919067913666368
iteration 300, loss = 0.0014855412300676107
iteration 1, loss = 0.000908824906218797
iteration 2, loss = 0.0008598124259151518
iteration 3, loss = 0.001012792345136404
iteration 4, loss = 0.0009916848503053188
iteration 5, loss = 0.0008041049004532397
iteration 6, loss = 0.0009137610904872417
iteration 7, loss = 0.0009121898328885436
iteration 8, loss = 0.0009236251353286207
iteration 9, loss = 0.0016631123144179583
iteration 10, loss = 0.0009525787318125367
iteration 11, loss = 0.0008847226854413748
iteration 12, loss = 0.0007838201709091663
iteration 13, loss = 0.0009037379059009254
iteration 14, loss = 0.0011286623775959015
iteration 15, loss = 0.001774260657839477
iteration 16, loss = 0.0008992764633148909
iteration 17, loss = 0.0008284323266707361
iteration 18, loss = 0.0008037926745600998
iteration 19, loss = 0.001020591938868165
iteration 20, loss = 0.0009244874236173928
iteration 21, loss = 0.0009393900400027633
iteration 22, loss = 0.0009334284113720059
iteration 23, loss = 0.0011694736313074827
iteration 24, loss = 0.0011577419936656952
iteration 25, loss = 0.0009415316744707525
iteration 26, loss = 0.0008157981210388243
iteration 27, loss = 0.0012646865798160434
iteration 28, loss = 0.0007553543546237051
iteration 29, loss = 0.0009016375988721848
iteration 30, loss = 0.0017154414672404528
iteration 31, loss = 0.0008784302044659853
iteration 32, loss = 0.0007922276272438467
iteration 33, loss = 0.001314437948167324
iteration 34, loss = 0.001081685652025044
iteration 35, loss = 0.0007582520483992994
iteration 36, loss = 0.0011133498046547174
iteration 37, loss = 0.0008863386465236545
iteration 38, loss = 0.0008124722517095506
iteration 39, loss = 0.000904593791346997
iteration 40, loss = 0.0011739711044356227
iteration 41, loss = 0.0017799775814637542
iteration 42, loss = 0.0010460240300744772
iteration 43, loss = 0.0008996427059173584
iteration 44, loss = 0.0011398650240153074
iteration 45, loss = 0.0009865659521892667
iteration 46, loss = 0.0009814165532588959
iteration 47, loss = 0.0007439099717885256
iteration 48, loss = 0.0016120705986395478
iteration 49, loss = 0.0007820964674465358
iteration 50, loss = 0.0009443535818718374
iteration 51, loss = 0.0009676399640738964
iteration 52, loss = 0.0009182582143694162
iteration 53, loss = 0.000988662475720048
iteration 54, loss = 0.0018777557415887713
iteration 55, loss = 0.001079170498996973
iteration 56, loss = 0.0008103437139652669
iteration 57, loss = 0.0011550262570381165
iteration 58, loss = 0.0010288575431331992
iteration 59, loss = 0.0008257696754299104
iteration 60, loss = 0.0008566066389903426
iteration 61, loss = 0.0009651539148762822
iteration 62, loss = 0.0009134273859672248
iteration 63, loss = 0.000944790372159332
iteration 64, loss = 0.0007508553098887205
iteration 65, loss = 0.000896907877177
iteration 66, loss = 0.0008314843289554119
iteration 67, loss = 0.0009536865400150418
iteration 68, loss = 0.00073670269921422
iteration 69, loss = 0.0014405414694920182
iteration 70, loss = 0.0009632800938561559
iteration 71, loss = 0.0009389648912474513
iteration 72, loss = 0.0010035689920186996
iteration 73, loss = 0.0008733803988434374
iteration 74, loss = 0.0010080392239615321
iteration 75, loss = 0.0010958125349134207
iteration 76, loss = 0.0008050856413319707
iteration 77, loss = 0.0009187610703520477
iteration 78, loss = 0.0008621050510555506
iteration 79, loss = 0.001164012704975903
iteration 80, loss = 0.0009394558728672564
iteration 81, loss = 0.0009653595043346286
iteration 82, loss = 0.0008857253706082702
iteration 83, loss = 0.0008148821070790291
iteration 84, loss = 0.0008153705857694149
iteration 85, loss = 0.0009054752299562097
iteration 86, loss = 0.0010242558782920241
iteration 87, loss = 0.0010146619752049446
iteration 88, loss = 0.0018248691922053695
iteration 89, loss = 0.0012213211739435792
iteration 90, loss = 0.000956693897023797
iteration 91, loss = 0.0008194876718334854
iteration 92, loss = 0.0009575144504196942
iteration 93, loss = 0.0008280210895463824
iteration 94, loss = 0.0009297557990066707
iteration 95, loss = 0.000805049785412848
iteration 96, loss = 0.0015059042489156127
iteration 97, loss = 0.000785769079811871
iteration 98, loss = 0.0007230371702462435
iteration 99, loss = 0.0007442707428708673
iteration 100, loss = 0.0009178593754768372
iteration 101, loss = 0.0008162615122273564
iteration 102, loss = 0.0010193558409810066
iteration 103, loss = 0.0014946747105568647
iteration 104, loss = 0.001954643288627267
iteration 105, loss = 0.0008672935073263943
iteration 106, loss = 0.0008925618603825569
iteration 107, loss = 0.0007961172377690673
iteration 108, loss = 0.0009611233253963292
iteration 109, loss = 0.0008112054201774299
iteration 110, loss = 0.001146076712757349
iteration 111, loss = 0.0007466395618394017
iteration 112, loss = 0.0012306564021855593
iteration 113, loss = 0.0012084906920790672
iteration 114, loss = 0.0009073637193068862
iteration 115, loss = 0.0009660145733505487
iteration 116, loss = 0.0009573510615155101
iteration 117, loss = 0.0015851175412535667
iteration 118, loss = 0.0008444606210105121
iteration 119, loss = 0.0017114925431087613
iteration 120, loss = 0.0009122411138378084
iteration 121, loss = 0.0007409801473841071
iteration 122, loss = 0.0013949660351499915
iteration 123, loss = 0.0009546928922645748
iteration 124, loss = 0.0007933330489322543
iteration 125, loss = 0.0008842210518196225
iteration 126, loss = 0.0009848772315308452
iteration 127, loss = 0.0009149743709713221
iteration 128, loss = 0.000884506618604064
iteration 129, loss = 0.0009629980195313692
iteration 130, loss = 0.0011125693563371897
iteration 131, loss = 0.0009351146873086691
iteration 132, loss = 0.0008958496619015932
iteration 133, loss = 0.0014555284287780523
iteration 134, loss = 0.0008364162058569491
iteration 135, loss = 0.0019161971285939217
iteration 136, loss = 0.0008920720429159701
iteration 137, loss = 0.0015292855678126216
iteration 138, loss = 0.0011196357663720846
iteration 139, loss = 0.0008967893663793802
iteration 140, loss = 0.0010722081642597914
iteration 141, loss = 0.0008123097359202802
iteration 142, loss = 0.0011670446256175637
iteration 143, loss = 0.0008918237290345132
iteration 144, loss = 0.0010979912476614118
iteration 145, loss = 0.0007523278472945094
iteration 146, loss = 0.0011984907323494554
iteration 147, loss = 0.0009652686421759427
iteration 148, loss = 0.0008781144861131907
iteration 149, loss = 0.0006918896688148379
iteration 150, loss = 0.0012278520734980702
iteration 151, loss = 0.0014122622087597847
iteration 152, loss = 0.001032673753798008
iteration 153, loss = 0.001245786901563406
iteration 154, loss = 0.0007421721238642931
iteration 155, loss = 0.0009573501301929355
iteration 156, loss = 0.0010324461618438363
iteration 157, loss = 0.0008062724955379963
iteration 158, loss = 0.00185879273340106
iteration 159, loss = 0.0007826307555660605
iteration 160, loss = 0.000708575127646327
iteration 161, loss = 0.00146578811109066
iteration 162, loss = 0.001249037217348814
iteration 163, loss = 0.001022141776047647
iteration 164, loss = 0.0008740900666452944
iteration 165, loss = 0.0009960951283574104
iteration 166, loss = 0.0008349946583621204
iteration 167, loss = 0.0007904081139713526
iteration 168, loss = 0.0009392718784511089
iteration 169, loss = 0.0015583520289510489
iteration 170, loss = 0.0011608082568272948
iteration 171, loss = 0.001340476330369711
iteration 172, loss = 0.0008723733481019735
iteration 173, loss = 0.0008108112378977239
iteration 174, loss = 0.0009012476075440645
iteration 175, loss = 0.0008055894868448377
iteration 176, loss = 0.0012559425085783005
iteration 177, loss = 0.0011860739905387163
iteration 178, loss = 0.0020220368169248104
iteration 179, loss = 0.0008342103101313114
iteration 180, loss = 0.0017694532871246338
iteration 181, loss = 0.0010498390765860677
iteration 182, loss = 0.0009578467579558492
iteration 183, loss = 0.0011501949047669768
iteration 184, loss = 0.0010718381963670254
iteration 185, loss = 0.0011841092491522431
iteration 186, loss = 0.0009932068642228842
iteration 187, loss = 0.001028356608003378
iteration 188, loss = 0.0010785192716866732
iteration 189, loss = 0.0008754750597290695
iteration 190, loss = 0.000919110607355833
iteration 191, loss = 0.0008262020419351757
iteration 192, loss = 0.0017195672262459993
iteration 193, loss = 0.0012748917797580361
iteration 194, loss = 0.0007610201719217002
iteration 195, loss = 0.000803410483058542
iteration 196, loss = 0.0009729950688779354
iteration 197, loss = 0.0009365269215777516
iteration 198, loss = 0.0008947656024247408
iteration 199, loss = 0.0012221006909385324
iteration 200, loss = 0.002052562078461051
iteration 201, loss = 0.0009843974839895964
iteration 202, loss = 0.0014141290448606014
iteration 203, loss = 0.001010716543532908
iteration 204, loss = 0.0008281201007775962
iteration 205, loss = 0.0017752631101757288
iteration 206, loss = 0.0009643069934099913
iteration 207, loss = 0.001068624434992671
iteration 208, loss = 0.0012802716810256243
iteration 209, loss = 0.0010828766971826553
iteration 210, loss = 0.0008938576793298125
iteration 211, loss = 0.0012621880741789937
iteration 212, loss = 0.0007637593662366271
iteration 213, loss = 0.0009102558833546937
iteration 214, loss = 0.0008785573299974203
iteration 215, loss = 0.0009237622143700719
iteration 216, loss = 0.0008159932331182063
iteration 217, loss = 0.001037171809002757
iteration 218, loss = 0.0013808452058583498
iteration 219, loss = 0.0008493105415254831
iteration 220, loss = 0.001274077338166535
iteration 221, loss = 0.0021297470666468143
iteration 222, loss = 0.0008306553936563432
iteration 223, loss = 0.0009417149121873081
iteration 224, loss = 0.000920874357689172
iteration 225, loss = 0.0007454221486113966
iteration 226, loss = 0.0008419672958552837
iteration 227, loss = 0.0008234555134549737
iteration 228, loss = 0.0009743868140503764
iteration 229, loss = 0.0009153163409791887
iteration 230, loss = 0.0009336569928564131
iteration 231, loss = 0.0008117702673189342
iteration 232, loss = 0.0007834199932403862
iteration 233, loss = 0.001633602543734014
iteration 234, loss = 0.0010550139704719186
iteration 235, loss = 0.0010191932087764144
iteration 236, loss = 0.0010474508162587881
iteration 237, loss = 0.001211099443025887
iteration 238, loss = 0.0008418362704105675
iteration 239, loss = 0.0017604267923161387
iteration 240, loss = 0.0013859458267688751
iteration 241, loss = 0.0008479492971673608
iteration 242, loss = 0.000968526815995574
iteration 243, loss = 0.000871888012625277
iteration 244, loss = 0.0009318651864305139
iteration 245, loss = 0.0009036168339662254
iteration 246, loss = 0.0018335393397137523
iteration 247, loss = 0.0008987737819552422
iteration 248, loss = 0.0009339647367596626
iteration 249, loss = 0.0008387531852349639
iteration 250, loss = 0.0009277932695113122
iteration 251, loss = 0.0008516789530403912
iteration 252, loss = 0.0010606040013954043
iteration 253, loss = 0.0009742514230310917
iteration 254, loss = 0.0013756249099969864
iteration 255, loss = 0.0007866838714107871
iteration 256, loss = 0.00113534030970186
iteration 257, loss = 0.0008131826762109995
iteration 258, loss = 0.0008910527685657144
iteration 259, loss = 0.0012056424748152494
iteration 260, loss = 0.0011137917172163725
iteration 261, loss = 0.001266414299607277
iteration 262, loss = 0.0009819328552111983
iteration 263, loss = 0.0009697293862700462
iteration 264, loss = 0.0016930820420384407
iteration 265, loss = 0.000776766799390316
iteration 266, loss = 0.0018420217093080282
iteration 267, loss = 0.0009298798395320773
iteration 268, loss = 0.0010201657423749566
iteration 269, loss = 0.0009298844379372895
iteration 270, loss = 0.0009591805282980204
iteration 271, loss = 0.0013104670215398073
iteration 272, loss = 0.0008958993712440133
iteration 273, loss = 0.0014597187982872128
iteration 274, loss = 0.0008125494350679219
iteration 275, loss = 0.0010022456990554929
iteration 276, loss = 0.0007743440219201148
iteration 277, loss = 0.0009080426534637809
iteration 278, loss = 0.0009724999545142055
iteration 279, loss = 0.0009048251085914671
iteration 280, loss = 0.0007843620842322707
iteration 281, loss = 0.0017535503720864654
iteration 282, loss = 0.000863624329213053
iteration 283, loss = 0.0010781274177134037
iteration 284, loss = 0.0008425037376582623
iteration 285, loss = 0.0008406451670452952
iteration 286, loss = 0.0015256544575095177
iteration 287, loss = 0.0017252350226044655
iteration 288, loss = 0.0011827850248664618
iteration 289, loss = 0.0008379874052479863
iteration 290, loss = 0.000755400862544775
iteration 291, loss = 0.001151367323473096
iteration 292, loss = 0.001019370392896235
iteration 293, loss = 0.0009628955158405006
iteration 294, loss = 0.001681672059930861
iteration 295, loss = 0.0007711956859566271
iteration 296, loss = 0.0008440768579021096
iteration 297, loss = 0.0007949075661599636
iteration 298, loss = 0.001396822975948453
iteration 299, loss = 0.0011062177363783121
iteration 300, loss = 0.0008947660680860281
iteration 1, loss = 0.0010013594292104244
iteration 2, loss = 0.0009484940674155951
iteration 3, loss = 0.0010340099688619375
iteration 4, loss = 0.0007030905690044165
iteration 5, loss = 0.0010353086981922388
iteration 6, loss = 0.0011882893741130829
iteration 7, loss = 0.0008408543653786182
iteration 8, loss = 0.0008852464379742742
iteration 9, loss = 0.0008151094662025571
iteration 10, loss = 0.0008156497497111559
iteration 11, loss = 0.001809001318179071
iteration 12, loss = 0.0011940717231482267
iteration 13, loss = 0.0012493093963712454
iteration 14, loss = 0.0009610811248421669
iteration 15, loss = 0.0008808400598354638
iteration 16, loss = 0.0008707659435458481
iteration 17, loss = 0.0010862658964470029
iteration 18, loss = 0.0009920591255649924
iteration 19, loss = 0.001021590898744762
iteration 20, loss = 0.0009820983977988362
iteration 21, loss = 0.0008924392168410122
iteration 22, loss = 0.0013046679086983204
iteration 23, loss = 0.000766099663451314
iteration 24, loss = 0.0009558587335050106
iteration 25, loss = 0.0010596304200589657
iteration 26, loss = 0.000961764482781291
iteration 27, loss = 0.0013440430629998446
iteration 28, loss = 0.0009431494399905205
iteration 29, loss = 0.0010002993512898684
iteration 30, loss = 0.0009310244349762797
iteration 31, loss = 0.0009364262805320323
iteration 32, loss = 0.0010693789226934314
iteration 33, loss = 0.000976701732724905
iteration 34, loss = 0.0008532837964594364
iteration 35, loss = 0.0017988235922530293
iteration 36, loss = 0.0015304908156394958
iteration 37, loss = 0.0007937303744256496
iteration 38, loss = 0.0010290853679180145
iteration 39, loss = 0.0009678807691670954
iteration 40, loss = 0.0014316648012027144
iteration 41, loss = 0.0007107125711627305
iteration 42, loss = 0.001300813048146665
iteration 43, loss = 0.0010456382296979427
iteration 44, loss = 0.0009548250236548483
iteration 45, loss = 0.0007489662384614348
iteration 46, loss = 0.000870073214173317
iteration 47, loss = 0.0008123200386762619
iteration 48, loss = 0.00117964344099164
iteration 49, loss = 0.0009983640629798174
iteration 50, loss = 0.0014856311026960611
iteration 51, loss = 0.001041445299051702
iteration 52, loss = 0.0008404575055465102
iteration 53, loss = 0.00105164828710258
iteration 54, loss = 0.0009564874926581979
iteration 55, loss = 0.000893350807018578
iteration 56, loss = 0.0009677667403593659
iteration 57, loss = 0.001734732766635716
iteration 58, loss = 0.00211464730091393
iteration 59, loss = 0.0009560182224959135
iteration 60, loss = 0.0008456341456621885
iteration 61, loss = 0.00107731728348881
iteration 62, loss = 0.000985051621682942
iteration 63, loss = 0.0013878456084057689
iteration 64, loss = 0.0008791863801889122
iteration 65, loss = 0.0005803796811960638
iteration 66, loss = 0.0007742917514406145
iteration 67, loss = 0.0009683378739282489
iteration 68, loss = 0.000980283017270267
iteration 69, loss = 0.0006892058881931007
iteration 70, loss = 0.001209801877848804
iteration 71, loss = 0.0016948343254625797
iteration 72, loss = 0.0007283181184902787
iteration 73, loss = 0.0008519558468833566
iteration 74, loss = 0.001881326432339847
iteration 75, loss = 0.0008550284546799958
iteration 76, loss = 0.0007987473509274423
iteration 77, loss = 0.0009056012495420873
iteration 78, loss = 0.0012788336025550961
iteration 79, loss = 0.0009022989543154836
iteration 80, loss = 0.0012968057999387383
iteration 81, loss = 0.0010350745869800448
iteration 82, loss = 0.0008757354225963354
iteration 83, loss = 0.000906888977624476
iteration 84, loss = 0.0013740770518779755
iteration 85, loss = 0.0011856697965413332
iteration 86, loss = 0.0008348010596819222
iteration 87, loss = 0.000869740906637162
iteration 88, loss = 0.0009413346415385604
iteration 89, loss = 0.0013798027066513896
iteration 90, loss = 0.001019530463963747
iteration 91, loss = 0.001355246640741825
iteration 92, loss = 0.0010184429120272398
iteration 93, loss = 0.0007805322529748082
iteration 94, loss = 0.0008590377401560545
iteration 95, loss = 0.001693888334557414
iteration 96, loss = 0.0008503204444423318
iteration 97, loss = 0.0008838587091304362
iteration 98, loss = 0.0012520316522568464
iteration 99, loss = 0.001123402500525117
iteration 100, loss = 0.0012797514209523797
iteration 101, loss = 0.001027507008984685
iteration 102, loss = 0.001409879419952631
iteration 103, loss = 0.0008155539399012923
iteration 104, loss = 0.0008219217997975647
iteration 105, loss = 0.0008857090142555535
iteration 106, loss = 0.0008000101079232991
iteration 107, loss = 0.000856659491546452
iteration 108, loss = 0.0008476342773064971
iteration 109, loss = 0.0008421235252171755
iteration 110, loss = 0.0009112975094467402
iteration 111, loss = 0.0009959458839148283
iteration 112, loss = 0.0009617919567972422
iteration 113, loss = 0.0009790650801733136
iteration 114, loss = 0.0010000882903113961
iteration 115, loss = 0.0008199878502637148
iteration 116, loss = 0.0010838686721399426
iteration 117, loss = 0.000923308078199625
iteration 118, loss = 0.0009676123736426234
iteration 119, loss = 0.0009325799182988703
iteration 120, loss = 0.0008514014189131558
iteration 121, loss = 0.0007162633701227605
iteration 122, loss = 0.001297799521125853
iteration 123, loss = 0.0009582391357980669
iteration 124, loss = 0.0010241016279906034
iteration 125, loss = 0.0007876220042817295
iteration 126, loss = 0.0008049138123169541
iteration 127, loss = 0.0010068686679005623
iteration 128, loss = 0.0008748407126404345
iteration 129, loss = 0.0008108384790830314
iteration 130, loss = 0.0009082393953576684
iteration 131, loss = 0.0007782509201206267
iteration 132, loss = 0.0010581592796370387
iteration 133, loss = 0.0009383109863847494
iteration 134, loss = 0.0008772716973908246
iteration 135, loss = 0.0008549822960048914
iteration 136, loss = 0.0015154744032770395
iteration 137, loss = 0.0008614987600594759
iteration 138, loss = 0.0009840041166171432
iteration 139, loss = 0.0017384861130267382
iteration 140, loss = 0.0017742812633514404
iteration 141, loss = 0.00070319848600775
iteration 142, loss = 0.0019133341265842319
iteration 143, loss = 0.000898124766536057
iteration 144, loss = 0.0008070318144746125
iteration 145, loss = 0.0007613413035869598
iteration 146, loss = 0.0011908772867172956
iteration 147, loss = 0.0007904836093075573
iteration 148, loss = 0.001027340884320438
iteration 149, loss = 0.000880673062056303
iteration 150, loss = 0.0007210602052509785
iteration 151, loss = 0.0010696484241634607
iteration 152, loss = 0.000979711883701384
iteration 153, loss = 0.0007901217322796583
iteration 154, loss = 0.0009009005734696984
iteration 155, loss = 0.0012635421007871628
iteration 156, loss = 0.0014464185805991292
iteration 157, loss = 0.0009252175223082304
iteration 158, loss = 0.0009936325950548053
iteration 159, loss = 0.001197395846247673
iteration 160, loss = 0.0007815583958290517
iteration 161, loss = 0.000708095554728061
iteration 162, loss = 0.0009978865273296833
iteration 163, loss = 0.0008763021323829889
iteration 164, loss = 0.0008403251995332539
iteration 165, loss = 0.0007820808095857501
iteration 166, loss = 0.0008176136761903763
iteration 167, loss = 0.001037603011354804
iteration 168, loss = 0.001528799650259316
iteration 169, loss = 0.0010589968878775835
iteration 170, loss = 0.000993550755083561
iteration 171, loss = 0.0007378716836683452
iteration 172, loss = 0.0008396563935093582
iteration 173, loss = 0.0010219281539320946
iteration 174, loss = 0.002562424633651972
iteration 175, loss = 0.0009306981228291988
iteration 176, loss = 0.000984670128673315
iteration 177, loss = 0.001219522557221353
iteration 178, loss = 0.0007566784042865038
iteration 179, loss = 0.0007943544769659638
iteration 180, loss = 0.000766002107411623
iteration 181, loss = 0.0009616522584110498
iteration 182, loss = 0.0007968520512804389
iteration 183, loss = 0.0020560910925269127
iteration 184, loss = 0.0008278684690594673
iteration 185, loss = 0.0010886189993470907
iteration 186, loss = 0.000767240475397557
iteration 187, loss = 0.0008885383722372353
iteration 188, loss = 0.0015034809475764632
iteration 189, loss = 0.0010684229200705886
iteration 190, loss = 0.0016786038177087903
iteration 191, loss = 0.0013759483117610216
iteration 192, loss = 0.0009530425886623561
iteration 193, loss = 0.0012861777795478702
iteration 194, loss = 0.0011492487974464893
iteration 195, loss = 0.001765039167366922
iteration 196, loss = 0.000920023478101939
iteration 197, loss = 0.0008197477436624467
iteration 198, loss = 0.0009417624096386135
iteration 199, loss = 0.000846890383400023
iteration 200, loss = 0.0007129774894565344
iteration 201, loss = 0.0006877843989059329
iteration 202, loss = 0.0019558186177164316
iteration 203, loss = 0.0016019210452213883
iteration 204, loss = 0.0009494205587543547
iteration 205, loss = 0.0009148434619419277
iteration 206, loss = 0.0013166858116164804
iteration 207, loss = 0.0007672389037907124
iteration 208, loss = 0.0007719806744717062
iteration 209, loss = 0.0007472883444279432
iteration 210, loss = 0.0007405611104331911
iteration 211, loss = 0.0011397399939596653
iteration 212, loss = 0.0007420940673910081
iteration 213, loss = 0.0011131677310913801
iteration 214, loss = 0.0007192697958089411
iteration 215, loss = 0.0010717988479882479
iteration 216, loss = 0.0009348231251351535
iteration 217, loss = 0.0008482207194902003
iteration 218, loss = 0.000932737544644624
iteration 219, loss = 0.0007872040150687099
iteration 220, loss = 0.000878620077855885
iteration 221, loss = 0.0014075887156650424
iteration 222, loss = 0.0008190377266146243
iteration 223, loss = 0.0011490978067740798
iteration 224, loss = 0.0014532685745507479
iteration 225, loss = 0.0007988845463842154
iteration 226, loss = 0.0008331701974384487
iteration 227, loss = 0.0009482739260420203
iteration 228, loss = 0.0010816879803314805
iteration 229, loss = 0.000960729958023876
iteration 230, loss = 0.0007154837367124856
iteration 231, loss = 0.0007941477233543992
iteration 232, loss = 0.0010406723013147712
iteration 233, loss = 0.001013958710245788
iteration 234, loss = 0.0010034632869064808
iteration 235, loss = 0.0008422955870628357
iteration 236, loss = 0.0008023690897971392
iteration 237, loss = 0.0014646038180217147
iteration 238, loss = 0.0009212904842570424
iteration 239, loss = 0.0007906194077804685
iteration 240, loss = 0.0014299812028184533
iteration 241, loss = 0.000719612289685756
iteration 242, loss = 0.0008187792263925076
iteration 243, loss = 0.0008423998951911926
iteration 244, loss = 0.0008729926776140928
iteration 245, loss = 0.0008973386720754206
iteration 246, loss = 0.0009314980707131326
iteration 247, loss = 0.0012012688675895333
iteration 248, loss = 0.001182207721285522
iteration 249, loss = 0.0008704985957592726
iteration 250, loss = 0.001606294303201139
iteration 251, loss = 0.000900905579328537
iteration 252, loss = 0.0010672245407477021
iteration 253, loss = 0.0013704693410545588
iteration 254, loss = 0.001714074518531561
iteration 255, loss = 0.000976661336608231
iteration 256, loss = 0.0008181267767213285
iteration 257, loss = 0.0011979095870628953
iteration 258, loss = 0.0008509579347446561
iteration 259, loss = 0.0008979759877547622
iteration 260, loss = 0.001023583230562508
iteration 261, loss = 0.0009992461418733
iteration 262, loss = 0.0009329614695161581
iteration 263, loss = 0.000863857741933316
iteration 264, loss = 0.0008693278068676591
iteration 265, loss = 0.0008268827223218977
iteration 266, loss = 0.0008319127955473959
iteration 267, loss = 0.0014476599171757698
iteration 268, loss = 0.0010057283798232675
iteration 269, loss = 0.000887723290361464
iteration 270, loss = 0.0012490241788327694
iteration 271, loss = 0.0012765555875375867
iteration 272, loss = 0.0017064831918105483
iteration 273, loss = 0.0008129007765091956
iteration 274, loss = 0.0008618446881882846
iteration 275, loss = 0.0009594873990863562
iteration 276, loss = 0.0010373798431828618
iteration 277, loss = 0.002014129888266325
iteration 278, loss = 0.0008575964020565152
iteration 279, loss = 0.0009569814428687096
iteration 280, loss = 0.0009732822072692215
iteration 281, loss = 0.0008031809702515602
iteration 282, loss = 0.0008209741208702326
iteration 283, loss = 0.0011884575942531228
iteration 284, loss = 0.0008672468247823417
iteration 285, loss = 0.0011620874283835292
iteration 286, loss = 0.0012359799584373832
iteration 287, loss = 0.001674420665949583
iteration 288, loss = 0.000762665644288063
iteration 289, loss = 0.0012172438437119126
iteration 290, loss = 0.0009367618476971984
iteration 291, loss = 0.0010908578988164663
iteration 292, loss = 0.0008908564923331141
iteration 293, loss = 0.0008800725918263197
iteration 294, loss = 0.001084506744518876
iteration 295, loss = 0.0010304631432518363
iteration 296, loss = 0.0024780593812465668
iteration 297, loss = 0.0011919218814000487
iteration 298, loss = 0.0007437205640599132
iteration 299, loss = 0.001617827103473246
iteration 300, loss = 0.0010378429433330894
iteration 1, loss = 0.0007686519529670477
iteration 2, loss = 0.0007860746700316668
iteration 3, loss = 0.0007691751234233379
iteration 4, loss = 0.00078556191874668
iteration 5, loss = 0.000988614745438099
iteration 6, loss = 0.0017798482440412045
iteration 7, loss = 0.0009001665166579187
iteration 8, loss = 0.0010047885589301586
iteration 9, loss = 0.0007428929093293846
iteration 10, loss = 0.0007346480269916356
iteration 11, loss = 0.0008617097046226263
iteration 12, loss = 0.0008929499890655279
iteration 13, loss = 0.0010646652663126588
iteration 14, loss = 0.0010482368525117636
iteration 15, loss = 0.0007989454315975308
iteration 16, loss = 0.0008906338480301201
iteration 17, loss = 0.0007695109234191477
iteration 18, loss = 0.0009464546456001699
iteration 19, loss = 0.0008408125140704215
iteration 20, loss = 0.0008036346407607198
iteration 21, loss = 0.000978646450676024
iteration 22, loss = 0.0014643833274021745
iteration 23, loss = 0.0007708081393502653
iteration 24, loss = 0.0008612139499746263
iteration 25, loss = 0.0009301200043410063
iteration 26, loss = 0.0010819666786119342
iteration 27, loss = 0.001053515006788075
iteration 28, loss = 0.0010607207659631968
iteration 29, loss = 0.0010341268498450518
iteration 30, loss = 0.0019231172045692801
iteration 31, loss = 0.0008717714808881283
iteration 32, loss = 0.0008691296679899096
iteration 33, loss = 0.0012482579331845045
iteration 34, loss = 0.001070474972948432
iteration 35, loss = 0.0010243406286463141
iteration 36, loss = 0.0008711341652087867
iteration 37, loss = 0.0008069182513281703
iteration 38, loss = 0.0016903853975236416
iteration 39, loss = 0.0007161818211898208
iteration 40, loss = 0.0012167252134531736
iteration 41, loss = 0.0008744716178625822
iteration 42, loss = 0.0007871466805227101
iteration 43, loss = 0.0009431580547243357
iteration 44, loss = 0.0008050005999393761
iteration 45, loss = 0.0006847319891676307
iteration 46, loss = 0.00109777448233217
iteration 47, loss = 0.0010191532783210278
iteration 48, loss = 0.0007444258080795407
iteration 49, loss = 0.0013724113814532757
iteration 50, loss = 0.001159199862740934
iteration 51, loss = 0.0007228226750157773
iteration 52, loss = 0.0010222353739663959
iteration 53, loss = 0.0007744465838186443
iteration 54, loss = 0.0007676349487155676
iteration 55, loss = 0.0017744991928339005
iteration 56, loss = 0.0008040274842642248
iteration 57, loss = 0.001182720996439457
iteration 58, loss = 0.0012118580052629113
iteration 59, loss = 0.0011065483558923006
iteration 60, loss = 0.0010053686564788222
iteration 61, loss = 0.0008407498244196177
iteration 62, loss = 0.0010085580870509148
iteration 63, loss = 0.0012085366761311889
iteration 64, loss = 0.0012586826924234629
iteration 65, loss = 0.001252094516530633
iteration 66, loss = 0.0010346900671720505
iteration 67, loss = 0.000883135013282299
iteration 68, loss = 0.001237313263118267
iteration 69, loss = 0.0008226529462262988
iteration 70, loss = 0.0013277123216539621
iteration 71, loss = 0.0013331797672435641
iteration 72, loss = 0.001062090857885778
iteration 73, loss = 0.0008319069165736437
iteration 74, loss = 0.0009371554479002953
iteration 75, loss = 0.000929378904402256
iteration 76, loss = 0.0008781859069131315
iteration 77, loss = 0.0008653601398691535
iteration 78, loss = 0.0009008631459437311
iteration 79, loss = 0.0007228060276247561
iteration 80, loss = 0.000837061321362853
iteration 81, loss = 0.0008628676878288388
iteration 82, loss = 0.001697253785096109
iteration 83, loss = 0.0016423639608547091
iteration 84, loss = 0.0008147952030412853
iteration 85, loss = 0.0010586446151137352
iteration 86, loss = 0.000807868258561939
iteration 87, loss = 0.0008743127691559494
iteration 88, loss = 0.0008959082188084722
iteration 89, loss = 0.0009403733420185745
iteration 90, loss = 0.0009963697521016002
iteration 91, loss = 0.0009188010590150952
iteration 92, loss = 0.0008193360408768058
iteration 93, loss = 0.0009402792202308774
iteration 94, loss = 0.0007966664270497859
iteration 95, loss = 0.0009066289057955146
iteration 96, loss = 0.0008373238379135728
iteration 97, loss = 0.0008376619080081582
iteration 98, loss = 0.0008798357448540628
iteration 99, loss = 0.0010105969849973917
iteration 100, loss = 0.0007475591846741736
iteration 101, loss = 0.0008546815952286124
iteration 102, loss = 0.0012647585244849324
iteration 103, loss = 0.0008595586405135691
iteration 104, loss = 0.0007815679418854415
iteration 105, loss = 0.0009041457669809461
iteration 106, loss = 0.0008809511782601476
iteration 107, loss = 0.0008595489780418575
iteration 108, loss = 0.0009683260577730834
iteration 109, loss = 0.0010645696893334389
iteration 110, loss = 0.001052944571711123
iteration 111, loss = 0.0008375574252568185
iteration 112, loss = 0.0015943610342219472
iteration 113, loss = 0.0012513487599790096
iteration 114, loss = 0.0008092146017588675
iteration 115, loss = 0.0008886370342224836
iteration 116, loss = 0.0009117387817241251
iteration 117, loss = 0.0008685815264470875
iteration 118, loss = 0.0009735722560435534
iteration 119, loss = 0.0007224559667520225
iteration 120, loss = 0.0010744815226644278
iteration 121, loss = 0.0010300608118996024
iteration 122, loss = 0.001292893197387457
iteration 123, loss = 0.0012518245493993163
iteration 124, loss = 0.0007599692908115685
iteration 125, loss = 0.0009820365812629461
iteration 126, loss = 0.0012996634468436241
iteration 127, loss = 0.0008244786295108497
iteration 128, loss = 0.0015097323339432478
iteration 129, loss = 0.0007713341037742794
iteration 130, loss = 0.0009114527492783964
iteration 131, loss = 0.001019432907924056
iteration 132, loss = 0.0010130815207958221
iteration 133, loss = 0.0013807881623506546
iteration 134, loss = 0.001089273253455758
iteration 135, loss = 0.0008541220449842513
iteration 136, loss = 0.0013657724484801292
iteration 137, loss = 0.0012918910942971706
iteration 138, loss = 0.0007917461916804314
iteration 139, loss = 0.0010394779965281487
iteration 140, loss = 0.0007189234020188451
iteration 141, loss = 0.001175376819446683
iteration 142, loss = 0.0010491820285096765
iteration 143, loss = 0.0007812429103069007
iteration 144, loss = 0.0010741257574409246
iteration 145, loss = 0.0011476407526060939
iteration 146, loss = 0.000805131159722805
iteration 147, loss = 0.0008623095927760005
iteration 148, loss = 0.0009110841201618314
iteration 149, loss = 0.0009611793793737888
iteration 150, loss = 0.0010140954982489347
iteration 151, loss = 0.0007282455335371196
iteration 152, loss = 0.0007842491613700986
iteration 153, loss = 0.0009855582611635327
iteration 154, loss = 0.0008328074472956359
iteration 155, loss = 0.0010283077135682106
iteration 156, loss = 0.0009227300179190934
iteration 157, loss = 0.0009327207808382809
iteration 158, loss = 0.0008539925911463797
iteration 159, loss = 0.0007727329502813518
iteration 160, loss = 0.0008960366249084473
iteration 161, loss = 0.0011947271414101124
iteration 162, loss = 0.00078373565338552
iteration 163, loss = 0.0007474020239897072
iteration 164, loss = 0.0016208803281188011
iteration 165, loss = 0.0008280329057015479
iteration 166, loss = 0.000920370570383966
iteration 167, loss = 0.0009304658742621541
iteration 168, loss = 0.0014292546547949314
iteration 169, loss = 0.0008208220824599266
iteration 170, loss = 0.0016411643009632826
iteration 171, loss = 0.0016161489766091108
iteration 172, loss = 0.000884246313944459
iteration 173, loss = 0.0015685935504734516
iteration 174, loss = 0.000981213292106986
iteration 175, loss = 0.0009628987754695117
iteration 176, loss = 0.0008543127332814038
iteration 177, loss = 0.0008767854887992144
iteration 178, loss = 0.000754905107896775
iteration 179, loss = 0.0009202865185216069
iteration 180, loss = 0.0014394454192370176
iteration 181, loss = 0.0014276531292125583
iteration 182, loss = 0.0009398463880643249
iteration 183, loss = 0.0010902727954089642
iteration 184, loss = 0.0007895450107753277
iteration 185, loss = 0.000763382064178586
iteration 186, loss = 0.0009503219625912607
iteration 187, loss = 0.0008536718087270856
iteration 188, loss = 0.0008124006562866271
iteration 189, loss = 0.0020057340152561665
iteration 190, loss = 0.0010118660284206271
iteration 191, loss = 0.0009612792637199163
iteration 192, loss = 0.0014569867635145783
iteration 193, loss = 0.0009883542079478502
iteration 194, loss = 0.0009171469137072563
iteration 195, loss = 0.0009101107716560364
iteration 196, loss = 0.0006476409616880119
iteration 197, loss = 0.0011530084302648902
iteration 198, loss = 0.0010749660432338715
iteration 199, loss = 0.0008801913354545832
iteration 200, loss = 0.0008705540676601231
iteration 201, loss = 0.0009399872506037354
iteration 202, loss = 0.0012274913024157286
iteration 203, loss = 0.0012197213945910335
iteration 204, loss = 0.0009702297393232584
iteration 205, loss = 0.0009943486656993628
iteration 206, loss = 0.0012308149598538876
iteration 207, loss = 0.0006872164085507393
iteration 208, loss = 0.0008565094321966171
iteration 209, loss = 0.0013387061189860106
iteration 210, loss = 0.0010454482398927212
iteration 211, loss = 0.0013778730062767863
iteration 212, loss = 0.0010506135877221823
iteration 213, loss = 0.0013300959253683686
iteration 214, loss = 0.0007827786030247808
iteration 215, loss = 0.0007956522167660296
iteration 216, loss = 0.0017784744268283248
iteration 217, loss = 0.0011020739329978824
iteration 218, loss = 0.0007246370078064501
iteration 219, loss = 0.0009529567323625088
iteration 220, loss = 0.0009390527848154306
iteration 221, loss = 0.0010805160272866488
iteration 222, loss = 0.001180486287921667
iteration 223, loss = 0.0011990255443379283
iteration 224, loss = 0.0010206069564446807
iteration 225, loss = 0.0022806525230407715
iteration 226, loss = 0.0007631850894540548
iteration 227, loss = 0.0018492985982447863
iteration 228, loss = 0.0018701395019888878
iteration 229, loss = 0.0017285135108977556
iteration 230, loss = 0.0009943279437720776
iteration 231, loss = 0.0007973760366439819
iteration 232, loss = 0.0008795029716566205
iteration 233, loss = 0.0009368933388032019
iteration 234, loss = 0.0009396456298418343
iteration 235, loss = 0.001081935246475041
iteration 236, loss = 0.0007831713301129639
iteration 237, loss = 0.001722877030260861
iteration 238, loss = 0.0009823032887652516
iteration 239, loss = 0.00094980513677001
iteration 240, loss = 0.0008243466727435589
iteration 241, loss = 0.000712069682776928
iteration 242, loss = 0.0008701399783603847
iteration 243, loss = 0.0008270523976534605
iteration 244, loss = 0.0010786738712340593
iteration 245, loss = 0.00076842907583341
iteration 246, loss = 0.0013379903975874186
iteration 247, loss = 0.0014470623573288321
iteration 248, loss = 0.0016004112549126148
iteration 249, loss = 0.0009789293399080634
iteration 250, loss = 0.0009238457423634827
iteration 251, loss = 0.0015047461492940784
iteration 252, loss = 0.0007023413199931383
iteration 253, loss = 0.000748681602999568
iteration 254, loss = 0.0010208635358139873
iteration 255, loss = 0.0008654914563521743
iteration 256, loss = 0.0010883889626711607
iteration 257, loss = 0.0008830015431158245
iteration 258, loss = 0.0009301214013248682
iteration 259, loss = 0.0009712942992337048
iteration 260, loss = 0.0007939375936985016
iteration 261, loss = 0.0012535301502794027
iteration 262, loss = 0.0008294892031699419
iteration 263, loss = 0.0008079633698798716
iteration 264, loss = 0.0017709126695990562
iteration 265, loss = 0.0009499189327470958
iteration 266, loss = 0.0012221857905387878
iteration 267, loss = 0.001809921464882791
iteration 268, loss = 0.000757114146836102
iteration 269, loss = 0.0013829281087964773
iteration 270, loss = 0.0007806080393493176
iteration 271, loss = 0.0009265635744668543
iteration 272, loss = 0.001891832915134728
iteration 273, loss = 0.0006763082346878946
iteration 274, loss = 0.0017690049717202783
iteration 275, loss = 0.0016761338338255882
iteration 276, loss = 0.0008287878008559346
iteration 277, loss = 0.001160009065642953
iteration 278, loss = 0.0014604524476453662
iteration 279, loss = 0.0009562282357364893
iteration 280, loss = 0.0009643713710829616
iteration 281, loss = 0.001869194325990975
iteration 282, loss = 0.0011543375439941883
iteration 283, loss = 0.0008051300537772477
iteration 284, loss = 0.0012561404146254063
iteration 285, loss = 0.0009771290933713317
iteration 286, loss = 0.0008031611796468496
iteration 287, loss = 0.0008065197616815567
iteration 288, loss = 0.0009846301982179284
iteration 289, loss = 0.001256126444786787
iteration 290, loss = 0.0011827563866972923
iteration 291, loss = 0.001462004380300641
iteration 292, loss = 0.0010913364822044969
iteration 293, loss = 0.0008082443382591009
iteration 294, loss = 0.0009427664335817099
iteration 295, loss = 0.0007335184491239488
iteration 296, loss = 0.0007990439189597964
iteration 297, loss = 0.0007633996428921819
iteration 298, loss = 0.0012581294868141413
iteration 299, loss = 0.001000871416181326
iteration 300, loss = 0.000991725828498602
iteration 1, loss = 0.0011515294900164008
iteration 2, loss = 0.0009112483821809292
iteration 3, loss = 0.0025073387660086155
iteration 4, loss = 0.0011875402415171266
iteration 5, loss = 0.001056542037986219
iteration 6, loss = 0.0007079940987750888
iteration 7, loss = 0.001050394494086504
iteration 8, loss = 0.0010359293082728982
iteration 9, loss = 0.0012413811637088656
iteration 10, loss = 0.0015604366781190038
iteration 11, loss = 0.0007259172271005809
iteration 12, loss = 0.0009388585458509624
iteration 13, loss = 0.0018720347434282303
iteration 14, loss = 0.0008999072015285492
iteration 15, loss = 0.0009955511195585132
iteration 16, loss = 0.0007528801797889173
iteration 17, loss = 0.001140850130468607
iteration 18, loss = 0.0010323512833565474
iteration 19, loss = 0.0008485014550387859
iteration 20, loss = 0.0007744113681837916
iteration 21, loss = 0.0017954737413674593
iteration 22, loss = 0.0009761299006640911
iteration 23, loss = 0.0008530870545655489
iteration 24, loss = 0.0008537420071661472
iteration 25, loss = 0.0008826470584608614
iteration 26, loss = 0.0009616886381991208
iteration 27, loss = 0.0010188122978433967
iteration 28, loss = 0.0008566823089495301
iteration 29, loss = 0.0008439929806627333
iteration 30, loss = 0.0024120344314724207
iteration 31, loss = 0.0009940011659637094
iteration 32, loss = 0.0009366823942400515
iteration 33, loss = 0.0010117158526554704
iteration 34, loss = 0.0008973146323114634
iteration 35, loss = 0.001600210671313107
iteration 36, loss = 0.0007974167820066214
iteration 37, loss = 0.0007654307992197573
iteration 38, loss = 0.0008266223594546318
iteration 39, loss = 0.000945650041103363
iteration 40, loss = 0.0008898397209122777
iteration 41, loss = 0.0012784639839082956
iteration 42, loss = 0.0008351689903065562
iteration 43, loss = 0.0009745247079990804
iteration 44, loss = 0.0009729498415254056
iteration 45, loss = 0.0009754988132044673
iteration 46, loss = 0.00094396504573524
iteration 47, loss = 0.0009135346626862884
iteration 48, loss = 0.0008184540783986449
iteration 49, loss = 0.0008002161630429327
iteration 50, loss = 0.0010649575851857662
iteration 51, loss = 0.0007989635341800749
iteration 52, loss = 0.0009761072578839958
iteration 53, loss = 0.0010644104331731796
iteration 54, loss = 0.001307751052081585
iteration 55, loss = 0.0010463560465723276
iteration 56, loss = 0.0014713924610987306
iteration 57, loss = 0.001884825760498643
iteration 58, loss = 0.0008574143284931779
iteration 59, loss = 0.0008207164355553687
iteration 60, loss = 0.0010326207848265767
iteration 61, loss = 0.0007069583516567945
iteration 62, loss = 0.0015527354553341866
iteration 63, loss = 0.0016325877513736486
iteration 64, loss = 0.0013530305586755276
iteration 65, loss = 0.0007814861601218581
iteration 66, loss = 0.0019115530885756016
iteration 67, loss = 0.0008653022232465446
iteration 68, loss = 0.001025694771669805
iteration 69, loss = 0.0007353115943260491
iteration 70, loss = 0.0010598821099847555
iteration 71, loss = 0.0009040754521265626
iteration 72, loss = 0.0010018316097557545
iteration 73, loss = 0.0007832832634449005
iteration 74, loss = 0.0008148023625835776
iteration 75, loss = 0.0009603301296010613
iteration 76, loss = 0.0009510662639513612
iteration 77, loss = 0.0007962399395182729
iteration 78, loss = 0.0008612400270067155
iteration 79, loss = 0.001017889124341309
iteration 80, loss = 0.0008257246809080243
iteration 81, loss = 0.00095375842647627
iteration 82, loss = 0.0015718033537268639
iteration 83, loss = 0.0009682694217190146
iteration 84, loss = 0.0008652468677610159
iteration 85, loss = 0.0007408314850181341
iteration 86, loss = 0.0008563693263567984
iteration 87, loss = 0.0008304794318974018
iteration 88, loss = 0.0008189436048269272
iteration 89, loss = 0.0008669376256875694
iteration 90, loss = 0.0008775168680585921
iteration 91, loss = 0.0018235610332340002
iteration 92, loss = 0.000957392854616046
iteration 93, loss = 0.0008864231058396399
iteration 94, loss = 0.0009165047667920589
iteration 95, loss = 0.0008415657794103026
iteration 96, loss = 0.0008463033591397107
iteration 97, loss = 0.0008952913340181112
iteration 98, loss = 0.0011542344000190496
iteration 99, loss = 0.0008307726820930839
iteration 100, loss = 0.0011956252856180072
iteration 101, loss = 0.0009736516512930393
iteration 102, loss = 0.0011978966649621725
iteration 103, loss = 0.0021875042002648115
iteration 104, loss = 0.0009938734583556652
iteration 105, loss = 0.0014826562255620956
iteration 106, loss = 0.0010079627390950918
iteration 107, loss = 0.000938476761803031
iteration 108, loss = 0.000995905022136867
iteration 109, loss = 0.0007711630314588547
iteration 110, loss = 0.0010431258706375957
iteration 111, loss = 0.000772350758779794
iteration 112, loss = 0.0010347414063289762
iteration 113, loss = 0.0011642507743090391
iteration 114, loss = 0.0010462149512022734
iteration 115, loss = 0.0012646049726754427
iteration 116, loss = 0.0008954564691521227
iteration 117, loss = 0.0008854913176037371
iteration 118, loss = 0.0015801474219188094
iteration 119, loss = 0.0008469110471196473
iteration 120, loss = 0.002091808244585991
iteration 121, loss = 0.0008077698294073343
iteration 122, loss = 0.0009031479130499065
iteration 123, loss = 0.0008461381657980382
iteration 124, loss = 0.0012282244861125946
iteration 125, loss = 0.0011227354407310486
iteration 126, loss = 0.0010325480252504349
iteration 127, loss = 0.001237367745488882
iteration 128, loss = 0.0009306632564403117
iteration 129, loss = 0.0009164807852357626
iteration 130, loss = 0.0008685593493282795
iteration 131, loss = 0.000816942541860044
iteration 132, loss = 0.0011164844036102295
iteration 133, loss = 0.0008372767479158938
iteration 134, loss = 0.0008955675875768065
iteration 135, loss = 0.0010417182929813862
iteration 136, loss = 0.001086395001038909
iteration 137, loss = 0.0008331408607773483
iteration 138, loss = 0.0015202498761937022
iteration 139, loss = 0.0007720135035924613
iteration 140, loss = 0.0008909271564334631
iteration 141, loss = 0.0012373760109767318
iteration 142, loss = 0.0008182222954928875
iteration 143, loss = 0.0008698446326889098
iteration 144, loss = 0.001160702551715076
iteration 145, loss = 0.0007962799863889813
iteration 146, loss = 0.0015434148954227567
iteration 147, loss = 0.000867890368681401
iteration 148, loss = 0.0010059637716040015
iteration 149, loss = 0.001024024561047554
iteration 150, loss = 0.0008430559537373483
iteration 151, loss = 0.0007308939239010215
iteration 152, loss = 0.0009357682429254055
iteration 153, loss = 0.0007998431683517992
iteration 154, loss = 0.0015183333307504654
iteration 155, loss = 0.0011129477061331272
iteration 156, loss = 0.000860236759763211
iteration 157, loss = 0.0008863431867212057
iteration 158, loss = 0.000760028138756752
iteration 159, loss = 0.0009246134432032704
iteration 160, loss = 0.0009807678870856762
iteration 161, loss = 0.0017483409028500319
iteration 162, loss = 0.0009552800329402089
iteration 163, loss = 0.001092609018087387
iteration 164, loss = 0.001145198941230774
iteration 165, loss = 0.0011319104814901948
iteration 166, loss = 0.000919331272598356
iteration 167, loss = 0.0009145863587036729
iteration 168, loss = 0.0008334017475135624
iteration 169, loss = 0.0010123078245669603
iteration 170, loss = 0.0007704510353505611
iteration 171, loss = 0.0014359295601025224
iteration 172, loss = 0.0014764225343242288
iteration 173, loss = 0.0009993592975661159
iteration 174, loss = 0.0008421697420999408
iteration 175, loss = 0.000732819433324039
iteration 176, loss = 0.0007817918667569757
iteration 177, loss = 0.0010521906660869718
iteration 178, loss = 0.0012436945689842105
iteration 179, loss = 0.000792104983702302
iteration 180, loss = 0.0009693342144601047
iteration 181, loss = 0.0009030559449456632
iteration 182, loss = 0.001683239359408617
iteration 183, loss = 0.0008554414380341768
iteration 184, loss = 0.0009468187927268445
iteration 185, loss = 0.0009139524772763252
iteration 186, loss = 0.0008711107075214386
iteration 187, loss = 0.0008527120226062834
iteration 188, loss = 0.00112445920240134
iteration 189, loss = 0.0011159765999764204
iteration 190, loss = 0.0017754198051989079
iteration 191, loss = 0.0007165723363868892
iteration 192, loss = 0.0010875946609303355
iteration 193, loss = 0.0008320430060848594
iteration 194, loss = 0.0007496635080315173
iteration 195, loss = 0.0008386566187255085
iteration 196, loss = 0.001069652964361012
iteration 197, loss = 0.0009299278608523309
iteration 198, loss = 0.0007533380994573236
iteration 199, loss = 0.0007160300156101584
iteration 200, loss = 0.0011913314228877425
iteration 201, loss = 0.0018358398228883743
iteration 202, loss = 0.000781925511546433
iteration 203, loss = 0.0006883958703838289
iteration 204, loss = 0.0010231740307062864
iteration 205, loss = 0.0008592878002673388
iteration 206, loss = 0.001290212501771748
iteration 207, loss = 0.0011850317241623998
iteration 208, loss = 0.0008001187234185636
iteration 209, loss = 0.0011545586166903377
iteration 210, loss = 0.0010358094004914165
iteration 211, loss = 0.0008995861280709505
iteration 212, loss = 0.000925039523281157
iteration 213, loss = 0.000976241659373045
iteration 214, loss = 0.000947452150285244
iteration 215, loss = 0.0008351315627805889
iteration 216, loss = 0.0008505795849487185
iteration 217, loss = 0.0009172792779281735
iteration 218, loss = 0.0008958985563367605
iteration 219, loss = 0.0011597736738622189
iteration 220, loss = 0.0012838507536798716
iteration 221, loss = 0.0010903356596827507
iteration 222, loss = 0.0009408131008967757
iteration 223, loss = 0.0008854587795212865
iteration 224, loss = 0.0008973932126536965
iteration 225, loss = 0.0007641856791451573
iteration 226, loss = 0.001009393366985023
iteration 227, loss = 0.0008982419967651367
iteration 228, loss = 0.0008955106022767723
iteration 229, loss = 0.0014963208232074976
iteration 230, loss = 0.000948010478168726
iteration 231, loss = 0.001357794157229364
iteration 232, loss = 0.0007350996602326632
iteration 233, loss = 0.0009739777306094766
iteration 234, loss = 0.0010258532129228115
iteration 235, loss = 0.0007495015161111951
iteration 236, loss = 0.0009703687974251807
iteration 237, loss = 0.0009121993789449334
iteration 238, loss = 0.0008115646778605878
iteration 239, loss = 0.00101289723534137
iteration 240, loss = 0.0008268704987131059
iteration 241, loss = 0.001092530321329832
iteration 242, loss = 0.0008484499412588775
iteration 243, loss = 0.0018179748440161347
iteration 244, loss = 0.0010141604579985142
iteration 245, loss = 0.0009508540388196707
iteration 246, loss = 0.000885245215613395
iteration 247, loss = 0.0011600515572354198
iteration 248, loss = 0.0007512858137488365
iteration 249, loss = 0.0007885702070780098
iteration 250, loss = 0.001026781857945025
iteration 251, loss = 0.0009052192326635122
iteration 252, loss = 0.002024108311161399
iteration 253, loss = 0.0015731003368273377
iteration 254, loss = 0.001190114882774651
iteration 255, loss = 0.0011433581821620464
iteration 256, loss = 0.0007863362552598119
iteration 257, loss = 0.0010822211625054479
iteration 258, loss = 0.0011671134270727634
iteration 259, loss = 0.000769136706367135
iteration 260, loss = 0.0008667266229167581
iteration 261, loss = 0.0008069888572208583
iteration 262, loss = 0.0009433722007088363
iteration 263, loss = 0.0008122780709527433
iteration 264, loss = 0.0006950388196855783
iteration 265, loss = 0.0009242572705261409
iteration 266, loss = 0.0010249699698761106
iteration 267, loss = 0.0014159736456349492
iteration 268, loss = 0.0007950215367600322
iteration 269, loss = 0.0009045674232766032
iteration 270, loss = 0.0011323185171931982
iteration 271, loss = 0.0015736292116343975
iteration 272, loss = 0.0009182096109725535
iteration 273, loss = 0.0006779974210076034
iteration 274, loss = 0.0009599180775694549
iteration 275, loss = 0.0009174940059892833
iteration 276, loss = 0.0011589217465370893
iteration 277, loss = 0.0009300314704887569
iteration 278, loss = 0.0008959286496974528
iteration 279, loss = 0.000845692353323102
iteration 280, loss = 0.00073322030948475
iteration 281, loss = 0.0009233680320903659
iteration 282, loss = 0.0009618230978958309
iteration 283, loss = 0.001471779658459127
iteration 284, loss = 0.0009204472880810499
iteration 285, loss = 0.00082926987670362
iteration 286, loss = 0.0010075386380776763
iteration 287, loss = 0.0008991268114186823
iteration 288, loss = 0.0008566626347601414
iteration 289, loss = 0.0008128798799589276
iteration 290, loss = 0.0008187221828848124
iteration 291, loss = 0.0008763675577938557
iteration 292, loss = 0.0009585485095158219
iteration 293, loss = 0.0009973234264180064
iteration 294, loss = 0.000814075639937073
iteration 295, loss = 0.0010287739569321275
iteration 296, loss = 0.002036683028563857
iteration 297, loss = 0.0009478790452703834
iteration 298, loss = 0.0009373772772960365
iteration 299, loss = 0.0012573548592627048
iteration 300, loss = 0.001005994388833642
iteration 1, loss = 0.0008862413233146071
iteration 2, loss = 0.000868639734108001
iteration 3, loss = 0.0007477389881387353
iteration 4, loss = 0.001210428774356842
iteration 5, loss = 0.0008887836011126637
iteration 6, loss = 0.0016681092092767358
iteration 7, loss = 0.001261247554793954
iteration 8, loss = 0.0009622079087421298
iteration 9, loss = 0.0008221783209592104
iteration 10, loss = 0.0007813030388206244
iteration 11, loss = 0.0011214397381991148
iteration 12, loss = 0.000805673364084214
iteration 13, loss = 0.0019035757286474109
iteration 14, loss = 0.0008265116484835744
iteration 15, loss = 0.0010026807431131601
iteration 16, loss = 0.0008089323528110981
iteration 17, loss = 0.0011669741943478584
iteration 18, loss = 0.0009599141194485128
iteration 19, loss = 0.0009112883126363158
iteration 20, loss = 0.001781157567165792
iteration 21, loss = 0.0010679495753720403
iteration 22, loss = 0.001977948471903801
iteration 23, loss = 0.0012289459118619561
iteration 24, loss = 0.0011661839671432972
iteration 25, loss = 0.0009559263125993311
iteration 26, loss = 0.001512720831669867
iteration 27, loss = 0.000937767094001174
iteration 28, loss = 0.0015129016246646643
iteration 29, loss = 0.0008582821465097368
iteration 30, loss = 0.001179758575744927
iteration 31, loss = 0.000906245200894773
iteration 32, loss = 0.0009638454648666084
iteration 33, loss = 0.0013582998653873801
iteration 34, loss = 0.0010348368668928742
iteration 35, loss = 0.0012063157046213746
iteration 36, loss = 0.0008877974469214678
iteration 37, loss = 0.0007639306131750345
iteration 38, loss = 0.0008076082449406385
iteration 39, loss = 0.001470876974053681
iteration 40, loss = 0.0008844071999192238
iteration 41, loss = 0.0017866017296910286
iteration 42, loss = 0.0017738131573423743
iteration 43, loss = 0.0010401727631688118
iteration 44, loss = 0.0009023785823956132
iteration 45, loss = 0.0012537426082417369
iteration 46, loss = 0.0009346339502371848
iteration 47, loss = 0.0007661723648197949
iteration 48, loss = 0.0009731491445563734
iteration 49, loss = 0.0010297048138454556
iteration 50, loss = 0.0007986468845047057
iteration 51, loss = 0.0008578964043408632
iteration 52, loss = 0.0017710733227431774
iteration 53, loss = 0.0008130118949338794
iteration 54, loss = 0.0009584407671354711
iteration 55, loss = 0.000935889664106071
iteration 56, loss = 0.0016349619254469872
iteration 57, loss = 0.000892823445610702
iteration 58, loss = 0.000723204284440726
iteration 59, loss = 0.0008604878094047308
iteration 60, loss = 0.0012007460463792086
iteration 61, loss = 0.0007156892097555101
iteration 62, loss = 0.0010047295363619924
iteration 63, loss = 0.0010228462051600218
iteration 64, loss = 0.0009185497765429318
iteration 65, loss = 0.0008967083995230496
iteration 66, loss = 0.0009291456080973148
iteration 67, loss = 0.0008824224350973964
iteration 68, loss = 0.0009278539218939841
iteration 69, loss = 0.0008219824521802366
iteration 70, loss = 0.0008947937749326229
iteration 71, loss = 0.0012059463188052177
iteration 72, loss = 0.0007765000336803496
iteration 73, loss = 0.0008596258703619242
iteration 74, loss = 0.0010936910985037684
iteration 75, loss = 0.0012792374473065138
iteration 76, loss = 0.0006973047857172787
iteration 77, loss = 0.0008349533309228718
iteration 78, loss = 0.0011165046598762274
iteration 79, loss = 0.0007862666971050203
iteration 80, loss = 0.0014325582887977362
iteration 81, loss = 0.0008328563999384642
iteration 82, loss = 0.001000865362584591
iteration 83, loss = 0.0008565764874219894
iteration 84, loss = 0.0016661848640069366
iteration 85, loss = 0.0008245195494964719
iteration 86, loss = 0.0008909185416996479
iteration 87, loss = 0.0011661103926599026
iteration 88, loss = 0.0008158388081938028
iteration 89, loss = 0.0010250939521938562
iteration 90, loss = 0.0008164246683008969
iteration 91, loss = 0.0013997874921187758
iteration 92, loss = 0.000774252344854176
iteration 93, loss = 0.0010417613666504622
iteration 94, loss = 0.0011452606413513422
iteration 95, loss = 0.0008398376521654427
iteration 96, loss = 0.0008709665853530169
iteration 97, loss = 0.0009841292630881071
iteration 98, loss = 0.0006935768760740757
iteration 99, loss = 0.0008678893791511655
iteration 100, loss = 0.0008588124765083194
iteration 101, loss = 0.0008169510983861983
iteration 102, loss = 0.0008819708018563688
iteration 103, loss = 0.0008883532718755305
iteration 104, loss = 0.0015430873027071357
iteration 105, loss = 0.0017041825922206044
iteration 106, loss = 0.001711456454358995
iteration 107, loss = 0.0007273566443473101
iteration 108, loss = 0.0009994024876505136
iteration 109, loss = 0.0009577942546457052
iteration 110, loss = 0.0008836166234686971
iteration 111, loss = 0.0008480137912556529
iteration 112, loss = 0.0015525838825851679
iteration 113, loss = 0.0009426956530660391
iteration 114, loss = 0.0009397668763995171
iteration 115, loss = 0.0013262591091915965
iteration 116, loss = 0.0008916620281524956
iteration 117, loss = 0.0016982692759484053
iteration 118, loss = 0.0008649131050333381
iteration 119, loss = 0.0012402405263856053
iteration 120, loss = 0.001805585459806025
iteration 121, loss = 0.0011226539500057697
iteration 122, loss = 0.001070142723619938
iteration 123, loss = 0.0010482296347618103
iteration 124, loss = 0.0011932129273191094
iteration 125, loss = 0.0008721385383978486
iteration 126, loss = 0.0015220107743516564
iteration 127, loss = 0.0009647497208788991
iteration 128, loss = 0.0007062911172397435
iteration 129, loss = 0.0008627091883681715
iteration 130, loss = 0.0008804328390397131
iteration 131, loss = 0.0007485634996555746
iteration 132, loss = 0.0011061533587053418
iteration 133, loss = 0.0008805527468211949
iteration 134, loss = 0.0008975991513580084
iteration 135, loss = 0.000745239551179111
iteration 136, loss = 0.0009949196828529239
iteration 137, loss = 0.0010215934598818421
iteration 138, loss = 0.0008984309388324618
iteration 139, loss = 0.0011849747970700264
iteration 140, loss = 0.0010226024314761162
iteration 141, loss = 0.0011516299564391375
iteration 142, loss = 0.0011613243259489536
iteration 143, loss = 0.0007135408814065158
iteration 144, loss = 0.0011157502885907888
iteration 145, loss = 0.0015802450943738222
iteration 146, loss = 0.00104329118039459
iteration 147, loss = 0.0008516847738064826
iteration 148, loss = 0.0008076831582002342
iteration 149, loss = 0.000976156909018755
iteration 150, loss = 0.0008354237070307136
iteration 151, loss = 0.0009334281785413623
iteration 152, loss = 0.000772093771956861
iteration 153, loss = 0.0009311744361184537
iteration 154, loss = 0.0017330916598439217
iteration 155, loss = 0.0017366647953167558
iteration 156, loss = 0.001087996643036604
iteration 157, loss = 0.000890461727976799
iteration 158, loss = 0.0009915337432175875
iteration 159, loss = 0.0011054023634642363
iteration 160, loss = 0.0008285719668492675
iteration 161, loss = 0.0006944362539798021
iteration 162, loss = 0.0009029734646901488
iteration 163, loss = 0.0008393849711865187
iteration 164, loss = 0.0008189863874576986
iteration 165, loss = 0.0009657156188040972
iteration 166, loss = 0.0008359578205272555
iteration 167, loss = 0.0009062655735760927
iteration 168, loss = 0.0008364823297597468
iteration 169, loss = 0.0009455641265958548
iteration 170, loss = 0.0008816368645057082
iteration 171, loss = 0.0008535291999578476
iteration 172, loss = 0.0009736515348777175
iteration 173, loss = 0.0014117382233962417
iteration 174, loss = 0.0008671983960084617
iteration 175, loss = 0.0008026056457310915
iteration 176, loss = 0.0008074118522927165
iteration 177, loss = 0.0010825099889189005
iteration 178, loss = 0.0007943421951495111
iteration 179, loss = 0.0008214705740101635
iteration 180, loss = 0.0011476697400212288
iteration 181, loss = 0.0009807812748476863
iteration 182, loss = 0.0009487092611379921
iteration 183, loss = 0.0009045512415468693
iteration 184, loss = 0.0009590624249540269
iteration 185, loss = 0.0008449701126664877
iteration 186, loss = 0.0008479933603666723
iteration 187, loss = 0.0008610038203187287
iteration 188, loss = 0.0008139432175084949
iteration 189, loss = 0.0009022236918099225
iteration 190, loss = 0.001026601530611515
iteration 191, loss = 0.0007453003199771047
iteration 192, loss = 0.0010687483008950949
iteration 193, loss = 0.0019511427963152528
iteration 194, loss = 0.0009297828655689955
iteration 195, loss = 0.0009927002247422934
iteration 196, loss = 0.0007579631637781858
iteration 197, loss = 0.0008090243209153414
iteration 198, loss = 0.0009742288384586573
iteration 199, loss = 0.0009044875623658299
iteration 200, loss = 0.0009205578244291246
iteration 201, loss = 0.000989563879556954
iteration 202, loss = 0.0011493249330669641
iteration 203, loss = 0.001176118734292686
iteration 204, loss = 0.0010201464174315333
iteration 205, loss = 0.0017012028256431222
iteration 206, loss = 0.0009232310694642365
iteration 207, loss = 0.0007635796209797263
iteration 208, loss = 0.0008000304223969579
iteration 209, loss = 0.0008416526834480464
iteration 210, loss = 0.0008987245964817703
iteration 211, loss = 0.0011909480672329664
iteration 212, loss = 0.0008939343970268965
iteration 213, loss = 0.0008941692649386823
iteration 214, loss = 0.0007279940182343125
iteration 215, loss = 0.0009370925836265087
iteration 216, loss = 0.00112930778414011
iteration 217, loss = 0.0008706418448127806
iteration 218, loss = 0.0008845523116178811
iteration 219, loss = 0.0010962765663862228
iteration 220, loss = 0.001279022777453065
iteration 221, loss = 0.0014474916970357299
iteration 222, loss = 0.0008820703369565308
iteration 223, loss = 0.0007899676566012204
iteration 224, loss = 0.0017650056397542357
iteration 225, loss = 0.000726458034478128
iteration 226, loss = 0.000751424697227776
iteration 227, loss = 0.001384555478580296
iteration 228, loss = 0.0009497166611254215
iteration 229, loss = 0.0007239656988531351
iteration 230, loss = 0.0010582725517451763
iteration 231, loss = 0.000896860845386982
iteration 232, loss = 0.0007832025876268744
iteration 233, loss = 0.0010403646156191826
iteration 234, loss = 0.0008796978509053588
iteration 235, loss = 0.0007147104479372501
iteration 236, loss = 0.0012792847119271755
iteration 237, loss = 0.0008100244449451566
iteration 238, loss = 0.0008764783851802349
iteration 239, loss = 0.0012184344232082367
iteration 240, loss = 0.0011266602668911219
iteration 241, loss = 0.0007999608060345054
iteration 242, loss = 0.0008308335090987384
iteration 243, loss = 0.0010359378065913916
iteration 244, loss = 0.0010520881041884422
iteration 245, loss = 0.0009131163242273033
iteration 246, loss = 0.0009696732740849257
iteration 247, loss = 0.00085121562005952
iteration 248, loss = 0.0010532161686569452
iteration 249, loss = 0.0008029669988900423
iteration 250, loss = 0.000825093942694366
iteration 251, loss = 0.0008079209364950657
iteration 252, loss = 0.0010317957494407892
iteration 253, loss = 0.0008923381101340055
iteration 254, loss = 0.0007681844872422516
iteration 255, loss = 0.0008675896096974611
iteration 256, loss = 0.0008986343163996935
iteration 257, loss = 0.0012357368832454085
iteration 258, loss = 0.001621450763195753
iteration 259, loss = 0.0008139167912304401
iteration 260, loss = 0.0009263950050808489
iteration 261, loss = 0.0007180318934842944
iteration 262, loss = 0.001037023146636784
iteration 263, loss = 0.001557106152176857
iteration 264, loss = 0.0008563782903365791
iteration 265, loss = 0.0008218735456466675
iteration 266, loss = 0.0010726164327934384
iteration 267, loss = 0.0008409152505919337
iteration 268, loss = 0.0008012547041289508
iteration 269, loss = 0.0007705094176344573
iteration 270, loss = 0.0007732357480563223
iteration 271, loss = 0.0009295814670622349
iteration 272, loss = 0.001181489322334528
iteration 273, loss = 0.0008881603134796023
iteration 274, loss = 0.0008601754670962691
iteration 275, loss = 0.0012475111288949847
iteration 276, loss = 0.0008228059159591794
iteration 277, loss = 0.0009825369343161583
iteration 278, loss = 0.0016986968694254756
iteration 279, loss = 0.0008796682232059538
iteration 280, loss = 0.0015783478738740087
iteration 281, loss = 0.0009316916111856699
iteration 282, loss = 0.000729623599909246
iteration 283, loss = 0.0008063102723099291
iteration 284, loss = 0.00126961013302207
iteration 285, loss = 0.000813945778645575
iteration 286, loss = 0.0015085055492818356
iteration 287, loss = 0.0009559874306432903
iteration 288, loss = 0.0017623365856707096
iteration 289, loss = 0.000830920587759465
iteration 290, loss = 0.0009080933523364365
iteration 291, loss = 0.0009730973397381604
iteration 292, loss = 0.0011847775895148516
iteration 293, loss = 0.001650013029575348
iteration 294, loss = 0.0007943820673972368
iteration 295, loss = 0.0014636673731729388
iteration 296, loss = 0.0011055059731006622
iteration 297, loss = 0.0008466985309496522
iteration 298, loss = 0.0007379028829745948
iteration 299, loss = 0.0010795410489663482
iteration 300, loss = 0.001020593335852027
iteration 1, loss = 0.0007820383179932833
iteration 2, loss = 0.0009164597722701728
iteration 3, loss = 0.000842954614199698
iteration 4, loss = 0.0007237715180963278
iteration 5, loss = 0.0007344342302531004
iteration 6, loss = 0.0011111071798950434
iteration 7, loss = 0.0008328636176884174
iteration 8, loss = 0.0007674179505556822
iteration 9, loss = 0.0007304616156034172
iteration 10, loss = 0.0008632665849290788
iteration 11, loss = 0.0011444197734817863
iteration 12, loss = 0.0011510311160236597
iteration 13, loss = 0.001516501186415553
iteration 14, loss = 0.0008504187571816146
iteration 15, loss = 0.0012706660199910402
iteration 16, loss = 0.000861568667460233
iteration 17, loss = 0.0008370578871108592
iteration 18, loss = 0.000981969409622252
iteration 19, loss = 0.0018068166682496667
iteration 20, loss = 0.001020651194266975
iteration 21, loss = 0.0012744966661557555
iteration 22, loss = 0.0010093372547999024
iteration 23, loss = 0.0011845548870041966
iteration 24, loss = 0.0009908115025609732
iteration 25, loss = 0.0007399385212920606
iteration 26, loss = 0.0018427013419568539
iteration 27, loss = 0.0008657437865622342
iteration 28, loss = 0.0014308866811916232
iteration 29, loss = 0.001658524968661368
iteration 30, loss = 0.000819545122794807
iteration 31, loss = 0.00135613395832479
iteration 32, loss = 0.0015187878161668777
iteration 33, loss = 0.0009857306722551584
iteration 34, loss = 0.0007784743211232126
iteration 35, loss = 0.0010843172203749418
iteration 36, loss = 0.0008432854083366692
iteration 37, loss = 0.0009351759217679501
iteration 38, loss = 0.0007867581443861127
iteration 39, loss = 0.0011788700940087438
iteration 40, loss = 0.0011680284515023232
iteration 41, loss = 0.0008190911612473428
iteration 42, loss = 0.0010725845349952579
iteration 43, loss = 0.0008336238679476082
iteration 44, loss = 0.0008763251826167107
iteration 45, loss = 0.0007887851679697633
iteration 46, loss = 0.0008510953630320728
iteration 47, loss = 0.0008298540487885475
iteration 48, loss = 0.0008697938174009323
iteration 49, loss = 0.001187194255180657
iteration 50, loss = 0.0008517301757819951
iteration 51, loss = 0.0017520615365356207
iteration 52, loss = 0.0007958463975228369
iteration 53, loss = 0.0009589128894731402
iteration 54, loss = 0.0012034092796966434
iteration 55, loss = 0.0009966392535716295
iteration 56, loss = 0.0009587734239175916
iteration 57, loss = 0.0009861801518127322
iteration 58, loss = 0.000989357940852642
iteration 59, loss = 0.0007421987829729915
iteration 60, loss = 0.001830887165851891
iteration 61, loss = 0.0018578509334474802
iteration 62, loss = 0.0007864221697673202
iteration 63, loss = 0.0015761267859488726
iteration 64, loss = 0.0008917584782466292
iteration 65, loss = 0.0009228548733517528
iteration 66, loss = 0.0008876363863237202
iteration 67, loss = 0.0010308079654350877
iteration 68, loss = 0.0007878005271777511
iteration 69, loss = 0.0009260899387300014
iteration 70, loss = 0.0007808528025634587
iteration 71, loss = 0.0010599992237985134
iteration 72, loss = 0.0011151720536872745
iteration 73, loss = 0.001470943563617766
iteration 74, loss = 0.0016736448742449284
iteration 75, loss = 0.001068613608367741
iteration 76, loss = 0.0008956960518844426
iteration 77, loss = 0.0009012579685077071
iteration 78, loss = 0.0009380833944305778
iteration 79, loss = 0.0009249391150660813
iteration 80, loss = 0.0007976021734066308
iteration 81, loss = 0.0008557920809835196
iteration 82, loss = 0.0008954660734161735
iteration 83, loss = 0.0009383499855175614
iteration 84, loss = 0.0012524174526333809
iteration 85, loss = 0.0007791786920279264
iteration 86, loss = 0.0016007660888135433
iteration 87, loss = 0.0009315704228356481
iteration 88, loss = 0.0009089139057323337
iteration 89, loss = 0.0009340752149000764
iteration 90, loss = 0.0008454963681288064
iteration 91, loss = 0.001218652119860053
iteration 92, loss = 0.0008984329761005938
iteration 93, loss = 0.0020829716231673956
iteration 94, loss = 0.0011968802427873015
iteration 95, loss = 0.0009552110568620265
iteration 96, loss = 0.0009140568436123431
iteration 97, loss = 0.0009452528902329504
iteration 98, loss = 0.0009433577070012689
iteration 99, loss = 0.000737344438675791
iteration 100, loss = 0.0008695998112671077
iteration 101, loss = 0.000864097906742245
iteration 102, loss = 0.0008179815486073494
iteration 103, loss = 0.0015514125116169453
iteration 104, loss = 0.0008165816543623805
iteration 105, loss = 0.0006881109438836575
iteration 106, loss = 0.0008367932168766856
iteration 107, loss = 0.0009406394674442708
iteration 108, loss = 0.0009565936052240431
iteration 109, loss = 0.0006193132721818984
iteration 110, loss = 0.0008372844895347953
iteration 111, loss = 0.0007731947698630393
iteration 112, loss = 0.0016149948351085186
iteration 113, loss = 0.0009453665115870535
iteration 114, loss = 0.0009827491594478488
iteration 115, loss = 0.0008371245348826051
iteration 116, loss = 0.0010570741724222898
iteration 117, loss = 0.0008925714064389467
iteration 118, loss = 0.0009051875676959753
iteration 119, loss = 0.0007243365398608148
iteration 120, loss = 0.0009246342815458775
iteration 121, loss = 0.0009152347920462489
iteration 122, loss = 0.0010388265363872051
iteration 123, loss = 0.001004687394015491
iteration 124, loss = 0.0007674493244849145
iteration 125, loss = 0.001188540947623551
iteration 126, loss = 0.0008974482771009207
iteration 127, loss = 0.0011640603188425303
iteration 128, loss = 0.0011402629315853119
iteration 129, loss = 0.0008040117099881172
iteration 130, loss = 0.0007624634890817106
iteration 131, loss = 0.0007405462092719972
iteration 132, loss = 0.001185038359835744
iteration 133, loss = 0.0007756592822261155
iteration 134, loss = 0.0019859205931425095
iteration 135, loss = 0.000808357261121273
iteration 136, loss = 0.0009175667655654252
iteration 137, loss = 0.0008531438652426004
iteration 138, loss = 0.0008357822662219405
iteration 139, loss = 0.0011120943818241358
iteration 140, loss = 0.0008591040968894958
iteration 141, loss = 0.0008301068446598947
iteration 142, loss = 0.001277010072953999
iteration 143, loss = 0.0007887088577263057
iteration 144, loss = 0.0008568272460252047
iteration 145, loss = 0.0009107846999540925
iteration 146, loss = 0.0016015081200748682
iteration 147, loss = 0.0009313653572462499
iteration 148, loss = 0.0008360392530448735
iteration 149, loss = 0.0011843460379168391
iteration 150, loss = 0.0011952990898862481
iteration 151, loss = 0.000901642837561667
iteration 152, loss = 0.0009555036085657775
iteration 153, loss = 0.0008535367087461054
iteration 154, loss = 0.0009621473145671189
iteration 155, loss = 0.0010332618840038776
iteration 156, loss = 0.0007472480647265911
iteration 157, loss = 0.0009719694498926401
iteration 158, loss = 0.0008021807880140841
iteration 159, loss = 0.0017577586695551872
iteration 160, loss = 0.000704762467648834
iteration 161, loss = 0.0008273008279502392
iteration 162, loss = 0.0010516642360016704
iteration 163, loss = 0.0007964693359099329
iteration 164, loss = 0.0010202131234109402
iteration 165, loss = 0.001699651125818491
iteration 166, loss = 0.0011360557982698083
iteration 167, loss = 0.0011986432364210486
iteration 168, loss = 0.0013017114251852036
iteration 169, loss = 0.0008717513410374522
iteration 170, loss = 0.0011093534994870424
iteration 171, loss = 0.0007431854610331357
iteration 172, loss = 0.000951194204390049
iteration 173, loss = 0.0008729007095098495
iteration 174, loss = 0.0010723775485530496
iteration 175, loss = 0.0008543716976419091
iteration 176, loss = 0.0010394789278507233
iteration 177, loss = 0.001254936563782394
iteration 178, loss = 0.001109365839511156
iteration 179, loss = 0.001301456126384437
iteration 180, loss = 0.0008686788496561348
iteration 181, loss = 0.0008083627908490598
iteration 182, loss = 0.0009126636432483792
iteration 183, loss = 0.0009605888626538217
iteration 184, loss = 0.0007689045742154121
iteration 185, loss = 0.000974275404587388
iteration 186, loss = 0.0008832932217046618
iteration 187, loss = 0.0011504803551360965
iteration 188, loss = 0.000996521906927228
iteration 189, loss = 0.001012436463497579
iteration 190, loss = 0.0007932576118037105
iteration 191, loss = 0.0008933302597142756
iteration 192, loss = 0.0008852910250425339
iteration 193, loss = 0.0008575298124924302
iteration 194, loss = 0.0012440424179658294
iteration 195, loss = 0.0017713583074510098
iteration 196, loss = 0.0008906389120966196
iteration 197, loss = 0.0009599372278898954
iteration 198, loss = 0.0015552372206002474
iteration 199, loss = 0.0008357249898836017
iteration 200, loss = 0.0009328426094725728
iteration 201, loss = 0.0011651290114969015
iteration 202, loss = 0.0009165448718704283
iteration 203, loss = 0.0015988514060154557
iteration 204, loss = 0.0008017620421014726
iteration 205, loss = 0.0008370717405341566
iteration 206, loss = 0.0008295925799757242
iteration 207, loss = 0.001441159751266241
iteration 208, loss = 0.0009475068654865026
iteration 209, loss = 0.0008280954789370298
iteration 210, loss = 0.0008158370619639754
iteration 211, loss = 0.0008167617488652468
iteration 212, loss = 0.0009193040896207094
iteration 213, loss = 0.0008863277034834027
iteration 214, loss = 0.0008605388575233519
iteration 215, loss = 0.001261040335521102
iteration 216, loss = 0.0009746472351253033
iteration 217, loss = 0.0015127528458833694
iteration 218, loss = 0.0009813865181058645
iteration 219, loss = 0.0013164018746465445
iteration 220, loss = 0.0008279739995487034
iteration 221, loss = 0.0006823491421528161
iteration 222, loss = 0.0008023316040635109
iteration 223, loss = 0.0010525727411732078
iteration 224, loss = 0.0015064416220411658
iteration 225, loss = 0.0009252973250113428
iteration 226, loss = 0.0009616402676329017
iteration 227, loss = 0.0010840772883966565
iteration 228, loss = 0.0007805321365594864
iteration 229, loss = 0.0010036228923127055
iteration 230, loss = 0.000799713539890945
iteration 231, loss = 0.0010321572190150619
iteration 232, loss = 0.0010212189517915249
iteration 233, loss = 0.0010825007921084762
iteration 234, loss = 0.0011918782256543636
iteration 235, loss = 0.0009287953143939376
iteration 236, loss = 0.0009606086532585323
iteration 237, loss = 0.0008476886432617903
iteration 238, loss = 0.0008636288694106042
iteration 239, loss = 0.001058537745848298
iteration 240, loss = 0.0010237807873636484
iteration 241, loss = 0.0008786453399807215
iteration 242, loss = 0.001239644712768495
iteration 243, loss = 0.00235277833417058
iteration 244, loss = 0.0009826176101341844
iteration 245, loss = 0.002079941565170884
iteration 246, loss = 0.0009111848776228726
iteration 247, loss = 0.001010014908388257
iteration 248, loss = 0.0008293394930660725
iteration 249, loss = 0.0009651014115661383
iteration 250, loss = 0.001015185029245913
iteration 251, loss = 0.0008044529822655022
iteration 252, loss = 0.0008788565173745155
iteration 253, loss = 0.0009910070803016424
iteration 254, loss = 0.0007893191650509834
iteration 255, loss = 0.0009074893314391375
iteration 256, loss = 0.0014499675016850233
iteration 257, loss = 0.0009350568288937211
iteration 258, loss = 0.0007463011424988508
iteration 259, loss = 0.0008931023767217994
iteration 260, loss = 0.0009601236670278013
iteration 261, loss = 0.0007459133630618453
iteration 262, loss = 0.0013475411105901003
iteration 263, loss = 0.0009116336004808545
iteration 264, loss = 0.0008864579722285271
iteration 265, loss = 0.0010382586624473333
iteration 266, loss = 0.0007536110933870077
iteration 267, loss = 0.0015272207092493773
iteration 268, loss = 0.0010606960859149694
iteration 269, loss = 0.0008108416805043817
iteration 270, loss = 0.0014930925099179149
iteration 271, loss = 0.0011260759783908725
iteration 272, loss = 0.0008747612009756267
iteration 273, loss = 0.0009168889955617487
iteration 274, loss = 0.0010074637830257416
iteration 275, loss = 0.0009365241858176887
iteration 276, loss = 0.0012573908315971494
iteration 277, loss = 0.0010381275787949562
iteration 278, loss = 0.0007220807601697743
iteration 279, loss = 0.0012047733180224895
iteration 280, loss = 0.0018749741138890386
iteration 281, loss = 0.0010256358655169606
iteration 282, loss = 0.0008966578170657158
iteration 283, loss = 0.0007023491780273616
iteration 284, loss = 0.0008934172219596803
iteration 285, loss = 0.0008817232446745038
iteration 286, loss = 0.000826589239295572
iteration 287, loss = 0.0014189791399985552
iteration 288, loss = 0.0007783427136018872
iteration 289, loss = 0.0008029573364183307
iteration 290, loss = 0.0008538640686310828
iteration 291, loss = 0.001170230912975967
iteration 292, loss = 0.0007219009567052126
iteration 293, loss = 0.000687040330376476
iteration 294, loss = 0.0007265891181305051
iteration 295, loss = 0.0010577928042039275
iteration 296, loss = 0.0008548909099772573
iteration 297, loss = 0.0009227669215761125
iteration 298, loss = 0.0010924242669716477
iteration 299, loss = 0.0008004577830433846
iteration 300, loss = 0.0007673673680983484
iteration 1, loss = 0.0008446291321888566
iteration 2, loss = 0.0009298117365688086
iteration 3, loss = 0.000996135058812797
iteration 4, loss = 0.0007655578665435314
iteration 5, loss = 0.0010297682601958513
iteration 6, loss = 0.0008306005038321018
iteration 7, loss = 0.001054234104231
iteration 8, loss = 0.0010376566788181663
iteration 9, loss = 0.0010630626929923892
iteration 10, loss = 0.0008243930642493069
iteration 11, loss = 0.00090651965001598
iteration 12, loss = 0.0007416977314278483
iteration 13, loss = 0.000644147745333612
iteration 14, loss = 0.000851728837005794
iteration 15, loss = 0.0008351427386514843
iteration 16, loss = 0.0009292680770158768
iteration 17, loss = 0.0014369094278663397
iteration 18, loss = 0.0009864306775853038
iteration 19, loss = 0.0013838439481332898
iteration 20, loss = 0.0008947016322053969
iteration 21, loss = 0.0008642225875519216
iteration 22, loss = 0.0009650534484535456
iteration 23, loss = 0.0014752499992027879
iteration 24, loss = 0.0008568499470129609
iteration 25, loss = 0.0007776926504448056
iteration 26, loss = 0.000773871608544141
iteration 27, loss = 0.0011486719595268369
iteration 28, loss = 0.0010076673934236169
iteration 29, loss = 0.0009091311367228627
iteration 30, loss = 0.0010620165849104524
iteration 31, loss = 0.0011723664356395602
iteration 32, loss = 0.0016997010679915547
iteration 33, loss = 0.0011554352240636945
iteration 34, loss = 0.0008090169867500663
iteration 35, loss = 0.0008444971754215658
iteration 36, loss = 0.001275493879802525
iteration 37, loss = 0.0008554222877137363
iteration 38, loss = 0.0009321889956481755
iteration 39, loss = 0.0008333103032782674
iteration 40, loss = 0.0011783767258748412
iteration 41, loss = 0.0015190275153145194
iteration 42, loss = 0.0010026670061051846
iteration 43, loss = 0.0010388840455561876
iteration 44, loss = 0.001866768579930067
iteration 45, loss = 0.0010778246214613318
iteration 46, loss = 0.0007686152239330113
iteration 47, loss = 0.0008066810551099479
iteration 48, loss = 0.0008523899014107883
iteration 49, loss = 0.0008553414954803884
iteration 50, loss = 0.001512798946350813
iteration 51, loss = 0.0018706247210502625
iteration 52, loss = 0.0008076029480434954
iteration 53, loss = 0.0007693908410146832
iteration 54, loss = 0.0007398155867122114
iteration 55, loss = 0.0011547822505235672
iteration 56, loss = 0.0008073821081779897
iteration 57, loss = 0.0008248194353654981
iteration 58, loss = 0.0011932478519156575
iteration 59, loss = 0.0008130705682560802
iteration 60, loss = 0.0016279031988233328
iteration 61, loss = 0.0007218815735541284
iteration 62, loss = 0.001207052730023861
iteration 63, loss = 0.0008288190001621842
iteration 64, loss = 0.0008441473473794758
iteration 65, loss = 0.0007274678209796548
iteration 66, loss = 0.0008013795595616102
iteration 67, loss = 0.0009856437100097537
iteration 68, loss = 0.0015479361172765493
iteration 69, loss = 0.0009974638232961297
iteration 70, loss = 0.0015893286326900125
iteration 71, loss = 0.0008249337552115321
iteration 72, loss = 0.0008412043680436909
iteration 73, loss = 0.0011212269309908152
iteration 74, loss = 0.0008868941804394126
iteration 75, loss = 0.0017278848681598902
iteration 76, loss = 0.0009375665104016662
iteration 77, loss = 0.0007549973670393229
iteration 78, loss = 0.0008713449351489544
iteration 79, loss = 0.0009748995071277022
iteration 80, loss = 0.0011211107484996319
iteration 81, loss = 0.0009002619772218168
iteration 82, loss = 0.0007846847292967141
iteration 83, loss = 0.0008279648027382791
iteration 84, loss = 0.0012312335893511772
iteration 85, loss = 0.0007946349214762449
iteration 86, loss = 0.0008261657785624266
iteration 87, loss = 0.001102530979551375
iteration 88, loss = 0.0008363928645849228
iteration 89, loss = 0.0009003948071040213
iteration 90, loss = 0.0008766446262598038
iteration 91, loss = 0.0015437942929565907
iteration 92, loss = 0.0008251332910731435
iteration 93, loss = 0.0007916010799817741
iteration 94, loss = 0.001907901605591178
iteration 95, loss = 0.0010167104192078114
iteration 96, loss = 0.0008099567494355142
iteration 97, loss = 0.0007715030806139112
iteration 98, loss = 0.001080220565199852
iteration 99, loss = 0.0017179689602926373
iteration 100, loss = 0.001151086762547493
iteration 101, loss = 0.0007910956046544015
iteration 102, loss = 0.000909338123165071
iteration 103, loss = 0.0007469443371519446
iteration 104, loss = 0.0013835047138854861
iteration 105, loss = 0.0011880752863362432
iteration 106, loss = 0.000923014769796282
iteration 107, loss = 0.0007834031130187213
iteration 108, loss = 0.000798665510956198
iteration 109, loss = 0.0008388599380850792
iteration 110, loss = 0.000831911398563534
iteration 111, loss = 0.0007245746091939509
iteration 112, loss = 0.0010432020062580705
iteration 113, loss = 0.0007870877743698657
iteration 114, loss = 0.0010880344780161977
iteration 115, loss = 0.0016860973555594683
iteration 116, loss = 0.000906455738004297
iteration 117, loss = 0.0009142202325165272
iteration 118, loss = 0.0006992857670411468
iteration 119, loss = 0.0008765513193793595
iteration 120, loss = 0.0007654912769794464
iteration 121, loss = 0.0008071472984738648
iteration 122, loss = 0.0008082566200755537
iteration 123, loss = 0.0008041489636525512
iteration 124, loss = 0.0011365102836862206
iteration 125, loss = 0.0007845179061405361
iteration 126, loss = 0.0008193356334231794
iteration 127, loss = 0.0010085885878652334
iteration 128, loss = 0.0009167420794256032
iteration 129, loss = 0.0008648568764328957
iteration 130, loss = 0.0008632222888991237
iteration 131, loss = 0.0009075436973944306
iteration 132, loss = 0.0007603297708556056
iteration 133, loss = 0.0016163147520273924
iteration 134, loss = 0.0009190592681989074
iteration 135, loss = 0.0007624995778314769
iteration 136, loss = 0.0009897213894873857
iteration 137, loss = 0.0022838199511170387
iteration 138, loss = 0.0016504533123224974
iteration 139, loss = 0.000665142317302525
iteration 140, loss = 0.0007918499759398401
iteration 141, loss = 0.0007811625255271792
iteration 142, loss = 0.0008494234643876553
iteration 143, loss = 0.0009465042967349291
iteration 144, loss = 0.0008963640430010855
iteration 145, loss = 0.0012561875628307462
iteration 146, loss = 0.0010302939917892218
iteration 147, loss = 0.0007809695089235902
iteration 148, loss = 0.0007256437093019485
iteration 149, loss = 0.0009750578319653869
iteration 150, loss = 0.0008550031925551593
iteration 151, loss = 0.0008234090055339038
iteration 152, loss = 0.0006999175529927015
iteration 153, loss = 0.0008684999775141478
iteration 154, loss = 0.0014980442356318235
iteration 155, loss = 0.0015917583368718624
iteration 156, loss = 0.0015308884903788567
iteration 157, loss = 0.001173773198388517
iteration 158, loss = 0.0016010490944609046
iteration 159, loss = 0.0012859648559242487
iteration 160, loss = 0.0016967251431196928
iteration 161, loss = 0.0009489973890595138
iteration 162, loss = 0.0007753556128591299
iteration 163, loss = 0.000867902475874871
iteration 164, loss = 0.0009535237913951278
iteration 165, loss = 0.0007817454170435667
iteration 166, loss = 0.0010093535529449582
iteration 167, loss = 0.0010498976334929466
iteration 168, loss = 0.0007931917207315564
iteration 169, loss = 0.0007797550642862916
iteration 170, loss = 0.001109162112697959
iteration 171, loss = 0.0011369200656190515
iteration 172, loss = 0.0009631008724682033
iteration 173, loss = 0.0007631930639036
iteration 174, loss = 0.0008376440964639187
iteration 175, loss = 0.0008280908223241568
iteration 176, loss = 0.002068871632218361
iteration 177, loss = 0.0008775504538789392
iteration 178, loss = 0.0007779467850923538
iteration 179, loss = 0.0007884568767622113
iteration 180, loss = 0.001060512033291161
iteration 181, loss = 0.0009571726550348103
iteration 182, loss = 0.001223953440785408
iteration 183, loss = 0.0007339225267060101
iteration 184, loss = 0.0014501588884741068
iteration 185, loss = 0.0010601741960272193
iteration 186, loss = 0.0008092363714240491
iteration 187, loss = 0.0016021850751712918
iteration 188, loss = 0.001054484979249537
iteration 189, loss = 0.0008610403165221214
iteration 190, loss = 0.0010616867803037167
iteration 191, loss = 0.0007878899341449142
iteration 192, loss = 0.0007054730085656047
iteration 193, loss = 0.0009858658304437995
iteration 194, loss = 0.0015410890337079763
iteration 195, loss = 0.0012390301562845707
iteration 196, loss = 0.0008515513618476689
iteration 197, loss = 0.0013348613865673542
iteration 198, loss = 0.000787477008998394
iteration 199, loss = 0.0008754871087148786
iteration 200, loss = 0.0010588206350803375
iteration 201, loss = 0.0010080784559249878
iteration 202, loss = 0.000942210026551038
iteration 203, loss = 0.0008130036876536906
iteration 204, loss = 0.0011428603902459145
iteration 205, loss = 0.0007032635039649904
iteration 206, loss = 0.001203951076604426
iteration 207, loss = 0.0008264750358648598
iteration 208, loss = 0.0010402860352769494
iteration 209, loss = 0.0008739045588299632
iteration 210, loss = 0.0009570612455718219
iteration 211, loss = 0.0012444781605154276
iteration 212, loss = 0.0009260422666557133
iteration 213, loss = 0.0008563927840441465
iteration 214, loss = 0.00116296811029315
iteration 215, loss = 0.0013019329635426402
iteration 216, loss = 0.0008837670320644975
iteration 217, loss = 0.0007745437906123698
iteration 218, loss = 0.0011950680054724216
iteration 219, loss = 0.0008210670202970505
iteration 220, loss = 0.0008376904297620058
iteration 221, loss = 0.0010638425592333078
iteration 222, loss = 0.0007791374227963388
iteration 223, loss = 0.0009098927257582545
iteration 224, loss = 0.0009444865863770247
iteration 225, loss = 0.0008388683199882507
iteration 226, loss = 0.0008933835779316723
iteration 227, loss = 0.0010314536048099399
iteration 228, loss = 0.001244757091626525
iteration 229, loss = 0.001387935015372932
iteration 230, loss = 0.0012521881144493818
iteration 231, loss = 0.0009442474110983312
iteration 232, loss = 0.0008785378886386752
iteration 233, loss = 0.0007984364056028426
iteration 234, loss = 0.0009020564029924572
iteration 235, loss = 0.0009406642057001591
iteration 236, loss = 0.0008331168792210519
iteration 237, loss = 0.0010014834115281701
iteration 238, loss = 0.0009505702182650566
iteration 239, loss = 0.0007759325089864433
iteration 240, loss = 0.0009868722409009933
iteration 241, loss = 0.0008090274641290307
iteration 242, loss = 0.0008457509102299809
iteration 243, loss = 0.0007603137055411935
iteration 244, loss = 0.0008034342899918556
iteration 245, loss = 0.001436464604921639
iteration 246, loss = 0.0008587658521719277
iteration 247, loss = 0.0007692468352615833
iteration 248, loss = 0.0011222087778151035
iteration 249, loss = 0.0010567214339971542
iteration 250, loss = 0.000845567206852138
iteration 251, loss = 0.00105747499037534
iteration 252, loss = 0.0010346247581765056
iteration 253, loss = 0.0008466257713735104
iteration 254, loss = 0.0012609899276867509
iteration 255, loss = 0.00079404964344576
iteration 256, loss = 0.0008671111427247524
iteration 257, loss = 0.001209018286317587
iteration 258, loss = 0.0009459590655751526
iteration 259, loss = 0.0006805696175433695
iteration 260, loss = 0.0008348381379619241
iteration 261, loss = 0.0009846061002463102
iteration 262, loss = 0.0009055369300767779
iteration 263, loss = 0.0008830393198877573
iteration 264, loss = 0.0008740100311115384
iteration 265, loss = 0.0009181443601846695
iteration 266, loss = 0.0009337662486359477
iteration 267, loss = 0.0009544203639961779
iteration 268, loss = 0.002041612518951297
iteration 269, loss = 0.0010337454732507467
iteration 270, loss = 0.0011326505336910486
iteration 271, loss = 0.0010535918409004807
iteration 272, loss = 0.0009335196809843183
iteration 273, loss = 0.0009729097364470363
iteration 274, loss = 0.0009190666605718434
iteration 275, loss = 0.0009814670775085688
iteration 276, loss = 0.000879273284226656
iteration 277, loss = 0.0008446973515674472
iteration 278, loss = 0.0016150668961927295
iteration 279, loss = 0.0008696893928572536
iteration 280, loss = 0.0014835200272500515
iteration 281, loss = 0.001001404714770615
iteration 282, loss = 0.0012791913468390703
iteration 283, loss = 0.000936452648602426
iteration 284, loss = 0.0007494522142224014
iteration 285, loss = 0.0009561568149365485
iteration 286, loss = 0.0007562233950011432
iteration 287, loss = 0.0012096359860152006
iteration 288, loss = 0.0011382827069610357
iteration 289, loss = 0.0023717160802334547
iteration 290, loss = 0.0012017315020784736
iteration 291, loss = 0.0009277164936065674
iteration 292, loss = 0.0012793400092050433
iteration 293, loss = 0.0008484349236823618
iteration 294, loss = 0.0009256382472813129
iteration 295, loss = 0.0008567027980461717
iteration 296, loss = 0.0008972391951829195
iteration 297, loss = 0.000810164725407958
iteration 298, loss = 0.0008689220412634313
iteration 299, loss = 0.0012650338467210531
iteration 300, loss = 0.0008656494319438934
iteration 1, loss = 0.0010173596674576402
iteration 2, loss = 0.0019156031776219606
iteration 3, loss = 0.0008271160768344998
iteration 4, loss = 0.0008779017371125519
iteration 5, loss = 0.0012132860720157623
iteration 6, loss = 0.0008112950599752367
iteration 7, loss = 0.001345443190075457
iteration 8, loss = 0.0010171042522415519
iteration 9, loss = 0.0008949510520324111
iteration 10, loss = 0.0008575451793149114
iteration 11, loss = 0.0008245590724982321
iteration 12, loss = 0.0010607617441564798
iteration 13, loss = 0.0009033106034621596
iteration 14, loss = 0.001117504434660077
iteration 15, loss = 0.001056324690580368
iteration 16, loss = 0.000712669687345624
iteration 17, loss = 0.0007356718997471035
iteration 18, loss = 0.0009379919501952827
iteration 19, loss = 0.0007464590016752481
iteration 20, loss = 0.001594168716110289
iteration 21, loss = 0.0009568160166963935
iteration 22, loss = 0.0012140597682446241
iteration 23, loss = 0.0009073901455849409
iteration 24, loss = 0.0010673773940652609
iteration 25, loss = 0.0009072330431081355
iteration 26, loss = 0.0006953365518711507
iteration 27, loss = 0.0009590624249540269
iteration 28, loss = 0.0016096580075100064
iteration 29, loss = 0.0008626097114756703
iteration 30, loss = 0.0013466011732816696
iteration 31, loss = 0.0009975412394851446
iteration 32, loss = 0.0010103702079504728
iteration 33, loss = 0.0008861420210450888
iteration 34, loss = 0.0008115282980725169
iteration 35, loss = 0.0007936293259263039
iteration 36, loss = 0.001095080398954451
iteration 37, loss = 0.0008506032754667103
iteration 38, loss = 0.0009105328936129808
iteration 39, loss = 0.0007034147856757045
iteration 40, loss = 0.0009604883962310851
iteration 41, loss = 0.0009066546917892992
iteration 42, loss = 0.0007776932325214148
iteration 43, loss = 0.0008197973947972059
iteration 44, loss = 0.0009313441114500165
iteration 45, loss = 0.0016307823825627565
iteration 46, loss = 0.0008158071432262659
iteration 47, loss = 0.0011268503731116652
iteration 48, loss = 0.0007400851463899016
iteration 49, loss = 0.0008336481405422091
iteration 50, loss = 0.0018602374475449324
iteration 51, loss = 0.0006759753450751305
iteration 52, loss = 0.0008223618497140706
iteration 53, loss = 0.0010270681232213974
iteration 54, loss = 0.0011229731608182192
iteration 55, loss = 0.0014485532883554697
iteration 56, loss = 0.000891401432454586
iteration 57, loss = 0.0007812355179339647
iteration 58, loss = 0.0008350896532647312
iteration 59, loss = 0.000833966420032084
iteration 60, loss = 0.000816468964330852
iteration 61, loss = 0.0008407418499700725
iteration 62, loss = 0.0008136118995025754
iteration 63, loss = 0.0010365748312324286
iteration 64, loss = 0.0007794149569235742
iteration 65, loss = 0.0017211480299010873
iteration 66, loss = 0.0007246297318488359
iteration 67, loss = 0.0009131516562774777
iteration 68, loss = 0.0009251635055989027
iteration 69, loss = 0.0007472300203517079
iteration 70, loss = 0.0008832880994305015
iteration 71, loss = 0.0008459909586235881
iteration 72, loss = 0.0010685480665415525
iteration 73, loss = 0.0006686805281788111
iteration 74, loss = 0.000954164657741785
iteration 75, loss = 0.0011455378262326121
iteration 76, loss = 0.000912319403141737
iteration 77, loss = 0.0007932247826829553
iteration 78, loss = 0.0009081029566004872
iteration 79, loss = 0.0007963948883116245
iteration 80, loss = 0.000902452680747956
iteration 81, loss = 0.0009190433192998171
iteration 82, loss = 0.0010065218666568398
iteration 83, loss = 0.0007895654416643083
iteration 84, loss = 0.0008881381945684552
iteration 85, loss = 0.0009919940494000912
iteration 86, loss = 0.0009614680893719196
iteration 87, loss = 0.001502778846770525
iteration 88, loss = 0.0007442060741595924
iteration 89, loss = 0.000903261941857636
iteration 90, loss = 0.0008356373873539269
iteration 91, loss = 0.0007920990465208888
iteration 92, loss = 0.0008497017552144825
iteration 93, loss = 0.0011556659592315555
iteration 94, loss = 0.0010631522163748741
iteration 95, loss = 0.0009645007667131722
iteration 96, loss = 0.0007636007503606379
iteration 97, loss = 0.0010738210985437036
iteration 98, loss = 0.0011355436872690916
iteration 99, loss = 0.0008701709448359907
iteration 100, loss = 0.0013961180811747909
iteration 101, loss = 0.001043321448378265
iteration 102, loss = 0.0009483409812673926
iteration 103, loss = 0.0008231677929870784
iteration 104, loss = 0.0009880398865789175
iteration 105, loss = 0.001030248124152422
iteration 106, loss = 0.0008636389975436032
iteration 107, loss = 0.0009711876045912504
iteration 108, loss = 0.0009153086575679481
iteration 109, loss = 0.000885171233676374
iteration 110, loss = 0.001681204535998404
iteration 111, loss = 0.0012741442769765854
iteration 112, loss = 0.0010589848970994353
iteration 113, loss = 0.0007412853883579373
iteration 114, loss = 0.0006499134469777346
iteration 115, loss = 0.0008817287161946297
iteration 116, loss = 0.0009667697013355792
iteration 117, loss = 0.0016694748774170876
iteration 118, loss = 0.0010857072193175554
iteration 119, loss = 0.0008653254481032491
iteration 120, loss = 0.0008441207464784384
iteration 121, loss = 0.0008742851787246764
iteration 122, loss = 0.0009168752003461123
iteration 123, loss = 0.0010660005500540137
iteration 124, loss = 0.0008957129321061075
iteration 125, loss = 0.0008466064464300871
iteration 126, loss = 0.0019129260908812284
iteration 127, loss = 0.0018803189741447568
iteration 128, loss = 0.0008714310824871063
iteration 129, loss = 0.0009625805541872978
iteration 130, loss = 0.0017612341325730085
iteration 131, loss = 0.0012660423526540399
iteration 132, loss = 0.001426618080586195
iteration 133, loss = 0.0006913713878020644
iteration 134, loss = 0.0007208135211840272
iteration 135, loss = 0.0008218559087254107
iteration 136, loss = 0.0010019622277468443
iteration 137, loss = 0.0008844046387821436
iteration 138, loss = 0.0022156480699777603
iteration 139, loss = 0.0010847932426258922
iteration 140, loss = 0.0007530821021646261
iteration 141, loss = 0.0009992948034778237
iteration 142, loss = 0.0014268679078668356
iteration 143, loss = 0.0006680668448098004
iteration 144, loss = 0.002415323629975319
iteration 145, loss = 0.0007941919029690325
iteration 146, loss = 0.0010295682586729527
iteration 147, loss = 0.0007700756541453302
iteration 148, loss = 0.0007636969676241279
iteration 149, loss = 0.0008378748898394406
iteration 150, loss = 0.0008537338580936193
iteration 151, loss = 0.0008424448897130787
iteration 152, loss = 0.001004225923679769
iteration 153, loss = 0.0008360271458514035
iteration 154, loss = 0.0017272799741476774
iteration 155, loss = 0.001168573391623795
iteration 156, loss = 0.0010954667814075947
iteration 157, loss = 0.0007382134208455682
iteration 158, loss = 0.0008034855127334595
iteration 159, loss = 0.0008264959324151278
iteration 160, loss = 0.0009486168855801225
iteration 161, loss = 0.0008209515362977982
iteration 162, loss = 0.0016797441057860851
iteration 163, loss = 0.0007735123508609831
iteration 164, loss = 0.0007987128337845206
iteration 165, loss = 0.0008615723345428705
iteration 166, loss = 0.0007663692813366652
iteration 167, loss = 0.0009726192802190781
iteration 168, loss = 0.0008422493119724095
iteration 169, loss = 0.0017531687626615167
iteration 170, loss = 0.0017891519237309694
iteration 171, loss = 0.0006497246213257313
iteration 172, loss = 0.002040073275566101
iteration 173, loss = 0.0019170496379956603
iteration 174, loss = 0.0010697750840336084
iteration 175, loss = 0.0016326634213328362
iteration 176, loss = 0.0010669315233826637
iteration 177, loss = 0.000994257628917694
iteration 178, loss = 0.0008032135665416718
iteration 179, loss = 0.0009542771731503308
iteration 180, loss = 0.0007296622497960925
iteration 181, loss = 0.0006304666167125106
iteration 182, loss = 0.0008105301531031728
iteration 183, loss = 0.0008260719478130341
iteration 184, loss = 0.0011233289260417223
iteration 185, loss = 0.0010179108940064907
iteration 186, loss = 0.0010564452968537807
iteration 187, loss = 0.0007895129965618253
iteration 188, loss = 0.000751798739656806
iteration 189, loss = 0.0008370934519916773
iteration 190, loss = 0.000949344364926219
iteration 191, loss = 0.0009393557556904852
iteration 192, loss = 0.0008509101462550461
iteration 193, loss = 0.0008024567505344748
iteration 194, loss = 0.0015447502955794334
iteration 195, loss = 0.0010098431957885623
iteration 196, loss = 0.0006947381771169603
iteration 197, loss = 0.0010200850665569305
iteration 198, loss = 0.0009455620311200619
iteration 199, loss = 0.0007055838941596448
iteration 200, loss = 0.0007437035674229264
iteration 201, loss = 0.0009824329754337668
iteration 202, loss = 0.0013696487294510007
iteration 203, loss = 0.0008494234061799943
iteration 204, loss = 0.0008975034579634666
iteration 205, loss = 0.00201874366030097
iteration 206, loss = 0.000830144330393523
iteration 207, loss = 0.000834104313980788
iteration 208, loss = 0.0008912874036468565
iteration 209, loss = 0.000798083608970046
iteration 210, loss = 0.0009700049413368106
iteration 211, loss = 0.001116376370191574
iteration 212, loss = 0.0008046048460528255
iteration 213, loss = 0.0015157416928559542
iteration 214, loss = 0.001546350191347301
iteration 215, loss = 0.0008475911454297602
iteration 216, loss = 0.001675638253800571
iteration 217, loss = 0.0008863224065862596
iteration 218, loss = 0.0008507173624821007
iteration 219, loss = 0.000956224394030869
iteration 220, loss = 0.0012853022199124098
iteration 221, loss = 0.0009247250854969025
iteration 222, loss = 0.0009576522279530764
iteration 223, loss = 0.0009435858228243887
iteration 224, loss = 0.0012757666409015656
iteration 225, loss = 0.0011191986268386245
iteration 226, loss = 0.0007438254542648792
iteration 227, loss = 0.000999006791971624
iteration 228, loss = 0.0012549402890726924
iteration 229, loss = 0.0007805593777447939
iteration 230, loss = 0.0008101866696961224
iteration 231, loss = 0.0016036960296332836
iteration 232, loss = 0.000691644789185375
iteration 233, loss = 0.0008115358650684357
iteration 234, loss = 0.0009508427465334535
iteration 235, loss = 0.0009131773258559406
iteration 236, loss = 0.000774670101236552
iteration 237, loss = 0.0006929824594408274
iteration 238, loss = 0.0008141474099829793
iteration 239, loss = 0.0009866924956440926
iteration 240, loss = 0.0010630338219925761
iteration 241, loss = 0.0008304924704134464
iteration 242, loss = 0.0008443465339951217
iteration 243, loss = 0.0012208335101604462
iteration 244, loss = 0.0008890554308891296
iteration 245, loss = 0.0017610704526305199
iteration 246, loss = 0.0006946073262952268
iteration 247, loss = 0.0011304276995360851
iteration 248, loss = 0.0012877711560577154
iteration 249, loss = 0.001115160994231701
iteration 250, loss = 0.001545312930829823
iteration 251, loss = 0.0007266057073138654
iteration 252, loss = 0.0010993324685841799
iteration 253, loss = 0.0007865565130487084
iteration 254, loss = 0.0008060582913458347
iteration 255, loss = 0.0006913818069733679
iteration 256, loss = 0.0012439533602446318
iteration 257, loss = 0.0007869504624977708
iteration 258, loss = 0.000758076086640358
iteration 259, loss = 0.0007443182985298336
iteration 260, loss = 0.0007697416003793478
iteration 261, loss = 0.0009685910190455616
iteration 262, loss = 0.0007378853624686599
iteration 263, loss = 0.0008329174015671015
iteration 264, loss = 0.0009616526076570153
iteration 265, loss = 0.00101361027918756
iteration 266, loss = 0.0009762924164533615
iteration 267, loss = 0.0010790092637762427
iteration 268, loss = 0.0007607042789459229
iteration 269, loss = 0.0010089892894029617
iteration 270, loss = 0.0010567997815087438
iteration 271, loss = 0.001619788003154099
iteration 272, loss = 0.0008805112447589636
iteration 273, loss = 0.0008375298930332065
iteration 274, loss = 0.0009380935225635767
iteration 275, loss = 0.0010376289719715714
iteration 276, loss = 0.00112676783464849
iteration 277, loss = 0.0008649779483675957
iteration 278, loss = 0.00087499781511724
iteration 279, loss = 0.0007244217558763921
iteration 280, loss = 0.0008465172140859067
iteration 281, loss = 0.0008124441374093294
iteration 282, loss = 0.0013238127576187253
iteration 283, loss = 0.0008647471549920738
iteration 284, loss = 0.0008902219706214964
iteration 285, loss = 0.0011211329838261008
iteration 286, loss = 0.0012895074905827641
iteration 287, loss = 0.0008362074149772525
iteration 288, loss = 0.0009393384680151939
iteration 289, loss = 0.0008805195684544742
iteration 290, loss = 0.0008896259823814034
iteration 291, loss = 0.0011152392253279686
iteration 292, loss = 0.0007871180423535407
iteration 293, loss = 0.0013327489141374826
iteration 294, loss = 0.0007455105660483241
iteration 295, loss = 0.0008415405172854662
iteration 296, loss = 0.000905674125533551
iteration 297, loss = 0.0009557919111102819
iteration 298, loss = 0.0010493836598470807
iteration 299, loss = 0.0011367960833013058
iteration 300, loss = 0.0008187148487195373
iteration 1, loss = 0.0011779313208535314
iteration 2, loss = 0.0008318456239067018
iteration 3, loss = 0.0010027155512943864
iteration 4, loss = 0.0009148366516456008
iteration 5, loss = 0.0008251310209743679
iteration 6, loss = 0.002037724247202277
iteration 7, loss = 0.0007416056469082832
iteration 8, loss = 0.000940607744269073
iteration 9, loss = 0.000875590369105339
iteration 10, loss = 0.0010308120399713516
iteration 11, loss = 0.0009689817088656127
iteration 12, loss = 0.001075497712008655
iteration 13, loss = 0.0012052251258864999
iteration 14, loss = 0.0013656731462106109
iteration 15, loss = 0.0008555965032428503
iteration 16, loss = 0.0008567288168706
iteration 17, loss = 0.0008142233127728105
iteration 18, loss = 0.0009405866148881614
iteration 19, loss = 0.0007953036110848188
iteration 20, loss = 0.0009569796966388822
iteration 21, loss = 0.0009425496100448072
iteration 22, loss = 0.0007207127055153251
iteration 23, loss = 0.0008648812072351575
iteration 24, loss = 0.0008595776744186878
iteration 25, loss = 0.0009461625013500452
iteration 26, loss = 0.0016278285766020417
iteration 27, loss = 0.001798763289116323
iteration 28, loss = 0.0008291135309264064
iteration 29, loss = 0.0008946987218223512
iteration 30, loss = 0.0009522739564999938
iteration 31, loss = 0.0007937836926430464
iteration 32, loss = 0.0007197861559689045
iteration 33, loss = 0.001268901163712144
iteration 34, loss = 0.0010931261349469423
iteration 35, loss = 0.000728262064512819
iteration 36, loss = 0.0009967536898329854
iteration 37, loss = 0.0014548395993188024
iteration 38, loss = 0.000889285933226347
iteration 39, loss = 0.0012551796389743686
iteration 40, loss = 0.0009974384447559714
iteration 41, loss = 0.0011644975747913122
iteration 42, loss = 0.000951405439991504
iteration 43, loss = 0.0008426919812336564
iteration 44, loss = 0.0008470630273222923
iteration 45, loss = 0.0008069303585216403
iteration 46, loss = 0.0014807286206632853
iteration 47, loss = 0.0008816905901767313
iteration 48, loss = 0.0008536593522876501
iteration 49, loss = 0.0009259731159545481
iteration 50, loss = 0.0008362695225514472
iteration 51, loss = 0.000877123442478478
iteration 52, loss = 0.0010193056659772992
iteration 53, loss = 0.0008741338970139623
iteration 54, loss = 0.0007456695893779397
iteration 55, loss = 0.0016724902670830488
iteration 56, loss = 0.0009260759688913822
iteration 57, loss = 0.0013774832477793097
iteration 58, loss = 0.0011774030281230807
iteration 59, loss = 0.0008843691321089864
iteration 60, loss = 0.0014112842036411166
iteration 61, loss = 0.0014681464526802301
iteration 62, loss = 0.0008018050575628877
iteration 63, loss = 0.0010006572119891644
iteration 64, loss = 0.0009601363562978804
iteration 65, loss = 0.0010626098373904824
iteration 66, loss = 0.000737700960598886
iteration 67, loss = 0.0013764230534434319
iteration 68, loss = 0.0008186757913790643
iteration 69, loss = 0.0009260906372219324
iteration 70, loss = 0.0011077167000621557
iteration 71, loss = 0.0009438606211915612
iteration 72, loss = 0.001073943916708231
iteration 73, loss = 0.0007149964803829789
iteration 74, loss = 0.0010152169270440936
iteration 75, loss = 0.0014890366001054645
iteration 76, loss = 0.0011137713445350528
iteration 77, loss = 0.0007552084280177951
iteration 78, loss = 0.0010543165262788534
iteration 79, loss = 0.0008321603527292609
iteration 80, loss = 0.0009089877130463719
iteration 81, loss = 0.0009253501193597913
iteration 82, loss = 0.000840315013192594
iteration 83, loss = 0.0016687156166881323
iteration 84, loss = 0.0007949856808409095
iteration 85, loss = 0.0014652714598923922
iteration 86, loss = 0.0006896450067870319
iteration 87, loss = 0.0010398855665698647
iteration 88, loss = 0.0008816433837637305
iteration 89, loss = 0.0015982838813215494
iteration 90, loss = 0.0007901056669652462
iteration 91, loss = 0.0008370767463929951
iteration 92, loss = 0.0009282009559683502
iteration 93, loss = 0.0007139021763578057
iteration 94, loss = 0.000873349083121866
iteration 95, loss = 0.0009760621469467878
iteration 96, loss = 0.0008567962213419378
iteration 97, loss = 0.0009839445119723678
iteration 98, loss = 0.0009261806262657046
iteration 99, loss = 0.0007834322168491781
iteration 100, loss = 0.0008773490553721786
iteration 101, loss = 0.0015866437461227179
iteration 102, loss = 0.0007833713316358626
iteration 103, loss = 0.0010595943313091993
iteration 104, loss = 0.0010014724684879184
iteration 105, loss = 0.0009440348367206752
iteration 106, loss = 0.0013799865264445543
iteration 107, loss = 0.00105570862069726
iteration 108, loss = 0.0016353896353393793
iteration 109, loss = 0.0015210870187729597
iteration 110, loss = 0.0015443854499608278
iteration 111, loss = 0.0008798739872872829
iteration 112, loss = 0.0006882224115543067
iteration 113, loss = 0.0008267892408184707
iteration 114, loss = 0.0008493487257510424
iteration 115, loss = 0.0007411445840261877
iteration 116, loss = 0.0020337793976068497
iteration 117, loss = 0.0010623325360938907
iteration 118, loss = 0.0011571523500606418
iteration 119, loss = 0.000908350688405335
iteration 120, loss = 0.0008694931166246533
iteration 121, loss = 0.0007634563371539116
iteration 122, loss = 0.0007650262559764087
iteration 123, loss = 0.0010570995509624481
iteration 124, loss = 0.0015816764207556844
iteration 125, loss = 0.0008522991556674242
iteration 126, loss = 0.0009483709000051022
iteration 127, loss = 0.00076235958840698
iteration 128, loss = 0.0010487845866009593
iteration 129, loss = 0.0008251529070548713
iteration 130, loss = 0.0008487094892188907
iteration 131, loss = 0.0007380470633506775
iteration 132, loss = 0.0008564994786866009
iteration 133, loss = 0.0007362471660599113
iteration 134, loss = 0.001320606330409646
iteration 135, loss = 0.0008059482788667083
iteration 136, loss = 0.0009507253998890519
iteration 137, loss = 0.0008562291041016579
iteration 138, loss = 0.0009167770622298121
iteration 139, loss = 0.0006366863381117582
iteration 140, loss = 0.0008216329151764512
iteration 141, loss = 0.0007986187702044845
iteration 142, loss = 0.000920370570383966
iteration 143, loss = 0.0009391194907948375
iteration 144, loss = 0.0012466234620660543
iteration 145, loss = 0.0007622505072504282
iteration 146, loss = 0.0007740641594864428
iteration 147, loss = 0.0011373733868822455
iteration 148, loss = 0.001717661740258336
iteration 149, loss = 0.0008135841926559806
iteration 150, loss = 0.0008951417985372245
iteration 151, loss = 0.0007103512180037796
iteration 152, loss = 0.0009168265387415886
iteration 153, loss = 0.0010028408141806722
iteration 154, loss = 0.0007732610683888197
iteration 155, loss = 0.0007614131318405271
iteration 156, loss = 0.0011806018883362412
iteration 157, loss = 0.0014351099962368608
iteration 158, loss = 0.0017390240682289004
iteration 159, loss = 0.0024836526717990637
iteration 160, loss = 0.0008926395676098764
iteration 161, loss = 0.001259089563973248
iteration 162, loss = 0.0006634508026763797
iteration 163, loss = 0.0008524690638296306
iteration 164, loss = 0.0009202425135299563
iteration 165, loss = 0.001408738549798727
iteration 166, loss = 0.0013313204981386662
iteration 167, loss = 0.0008645980269648135
iteration 168, loss = 0.0007932985899969935
iteration 169, loss = 0.000852602010127157
iteration 170, loss = 0.0008972630021162331
iteration 171, loss = 0.0008733726572245359
iteration 172, loss = 0.0008007897413335741
iteration 173, loss = 0.0007137331995181739
iteration 174, loss = 0.000768296595197171
iteration 175, loss = 0.0007762257591821253
iteration 176, loss = 0.0011307819513604045
iteration 177, loss = 0.000845469010528177
iteration 178, loss = 0.0011290890397503972
iteration 179, loss = 0.0007956258486956358
iteration 180, loss = 0.0017914073541760445
iteration 181, loss = 0.0016441658372059464
iteration 182, loss = 0.0008430486195720732
iteration 183, loss = 0.0011861765524372458
iteration 184, loss = 0.0008049227762967348
iteration 185, loss = 0.0016003897180780768
iteration 186, loss = 0.0010348857613280416
iteration 187, loss = 0.0008206706843338907
iteration 188, loss = 0.0008753273286856711
iteration 189, loss = 0.0009978431044146419
iteration 190, loss = 0.001022245385684073
iteration 191, loss = 0.002481939736753702
iteration 192, loss = 0.0010205629514530301
iteration 193, loss = 0.0009361283155158162
iteration 194, loss = 0.0008458872907795012
iteration 195, loss = 0.0007604489801451564
iteration 196, loss = 0.0009325244463980198
iteration 197, loss = 0.0008282195776700974
iteration 198, loss = 0.0010241589043289423
iteration 199, loss = 0.0008470352622680366
iteration 200, loss = 0.0008266034419648349
iteration 201, loss = 0.0008337769540958107
iteration 202, loss = 0.0007879912736825645
iteration 203, loss = 0.0010064393281936646
iteration 204, loss = 0.001121936715207994
iteration 205, loss = 0.0007685806485824287
iteration 206, loss = 0.0012136698933318257
iteration 207, loss = 0.0009389587212353945
iteration 208, loss = 0.0007630264153704047
iteration 209, loss = 0.0010091956937685609
iteration 210, loss = 0.0008637522114440799
iteration 211, loss = 0.0011787265539169312
iteration 212, loss = 0.0007666394812986255
iteration 213, loss = 0.0007830520044080913
iteration 214, loss = 0.001038739224895835
iteration 215, loss = 0.0007553208852186799
iteration 216, loss = 0.0008403193205595016
iteration 217, loss = 0.000992259127087891
iteration 218, loss = 0.0009705859702080488
iteration 219, loss = 0.0007659269031137228
iteration 220, loss = 0.0006641633226536214
iteration 221, loss = 0.000829462194815278
iteration 222, loss = 0.0006982535705901682
iteration 223, loss = 0.0012451551156118512
iteration 224, loss = 0.0017134346999228
iteration 225, loss = 0.0009586261585354805
iteration 226, loss = 0.0008875775965861976
iteration 227, loss = 0.0007781744934618473
iteration 228, loss = 0.0007802161853760481
iteration 229, loss = 0.0010678849648684263
iteration 230, loss = 0.0008308164542540908
iteration 231, loss = 0.0009162854985333979
iteration 232, loss = 0.000699895725119859
iteration 233, loss = 0.001113232341594994
iteration 234, loss = 0.0008950292249210179
iteration 235, loss = 0.0007270355126820505
iteration 236, loss = 0.0008799543720670044
iteration 237, loss = 0.0007631931221112609
iteration 238, loss = 0.0009663731325417757
iteration 239, loss = 0.000867317954543978
iteration 240, loss = 0.0008339874329976737
iteration 241, loss = 0.000818656466435641
iteration 242, loss = 0.0017596068792045116
iteration 243, loss = 0.0007117007044143975
iteration 244, loss = 0.0007291276124306023
iteration 245, loss = 0.0010679331608116627
iteration 246, loss = 0.000740861869417131
iteration 247, loss = 0.0008496238733641803
iteration 248, loss = 0.0008218541624955833
iteration 249, loss = 0.0014324558433145285
iteration 250, loss = 0.0009255646727979183
iteration 251, loss = 0.000755635614041239
iteration 252, loss = 0.0010974581819027662
iteration 253, loss = 0.0009736936190165579
iteration 254, loss = 0.0010028763208538294
iteration 255, loss = 0.0012880918802693486
iteration 256, loss = 0.0008597333217039704
iteration 257, loss = 0.0007128581055440009
iteration 258, loss = 0.0009790060576051474
iteration 259, loss = 0.0007654782966710627
iteration 260, loss = 0.0010466162348166108
iteration 261, loss = 0.0007390238461084664
iteration 262, loss = 0.0007536562625318766
iteration 263, loss = 0.0007200592081062496
iteration 264, loss = 0.0006692010210826993
iteration 265, loss = 0.0008129066554829478
iteration 266, loss = 0.000852035591378808
iteration 267, loss = 0.0011661520693451166
iteration 268, loss = 0.0011042358819395304
iteration 269, loss = 0.0008603784954175353
iteration 270, loss = 0.001008516876026988
iteration 271, loss = 0.0010765986517071724
iteration 272, loss = 0.0009175846353173256
iteration 273, loss = 0.0009639101917855442
iteration 274, loss = 0.0007951541338115931
iteration 275, loss = 0.0015245868125930429
iteration 276, loss = 0.0009434270905330777
iteration 277, loss = 0.0007795573910698295
iteration 278, loss = 0.0009147048695012927
iteration 279, loss = 0.0011503839632496238
iteration 280, loss = 0.0023357542231678963
iteration 281, loss = 0.0010020904010161757
iteration 282, loss = 0.000820100714918226
iteration 283, loss = 0.0007658925023861229
iteration 284, loss = 0.0007400368922390044
iteration 285, loss = 0.0010806610807776451
iteration 286, loss = 0.0010079254861921072
iteration 287, loss = 0.0008441410609520972
iteration 288, loss = 0.000715619302354753
iteration 289, loss = 0.0007597086951136589
iteration 290, loss = 0.0007838894380256534
iteration 291, loss = 0.0023423945531249046
iteration 292, loss = 0.0009906275663524866
iteration 293, loss = 0.001460509723983705
iteration 294, loss = 0.0008327555260621011
iteration 295, loss = 0.0008877494838088751
iteration 296, loss = 0.001052773674018681
iteration 297, loss = 0.0009739866363815963
iteration 298, loss = 0.0008401955128647387
iteration 299, loss = 0.0009585583466105163
iteration 300, loss = 0.000855314894579351
iteration 1, loss = 0.0016274554654955864
iteration 2, loss = 0.001636536791920662
iteration 3, loss = 0.0017648995853960514
iteration 4, loss = 0.0008585662581026554
iteration 5, loss = 0.001257270690985024
iteration 6, loss = 0.0007288787164725363
iteration 7, loss = 0.0008095749071799219
iteration 8, loss = 0.0006849708734080195
iteration 9, loss = 0.0013166554272174835
iteration 10, loss = 0.0008259250316768885
iteration 11, loss = 0.0008163264719769359
iteration 12, loss = 0.0011104498989880085
iteration 13, loss = 0.0009139025351032615
iteration 14, loss = 0.0014123318251222372
iteration 15, loss = 0.0007839372265152633
iteration 16, loss = 0.000777403125539422
iteration 17, loss = 0.0008912591729313135
iteration 18, loss = 0.001144771813414991
iteration 19, loss = 0.00115837377961725
iteration 20, loss = 0.0013783142203465104
iteration 21, loss = 0.0008996390388347208
iteration 22, loss = 0.0013421718031167984
iteration 23, loss = 0.0009203084046021104
iteration 24, loss = 0.0008840822847560048
iteration 25, loss = 0.0008467879961244762
iteration 26, loss = 0.0011906600557267666
iteration 27, loss = 0.001656271400861442
iteration 28, loss = 0.0015950142405927181
iteration 29, loss = 0.0007565889391116798
iteration 30, loss = 0.000805483665317297
iteration 31, loss = 0.0010243760189041495
iteration 32, loss = 0.0008126102620735765
iteration 33, loss = 0.0011051003821194172
iteration 34, loss = 0.0007228904869407415
iteration 35, loss = 0.0008165963226929307
iteration 36, loss = 0.0009048411156982183
iteration 37, loss = 0.0008274808642454445
iteration 38, loss = 0.0017507394077256322
iteration 39, loss = 0.0011068505700677633
iteration 40, loss = 0.0007899127085693181
iteration 41, loss = 0.0011551582720130682
iteration 42, loss = 0.0008310905541293323
iteration 43, loss = 0.000892996322363615
iteration 44, loss = 0.0011920771794393659
iteration 45, loss = 0.0008848709985613823
iteration 46, loss = 0.0007786095957271755
iteration 47, loss = 0.0008536272798664868
iteration 48, loss = 0.0006949045928195119
iteration 49, loss = 0.0010711996583268046
iteration 50, loss = 0.0007839718018658459
iteration 51, loss = 0.0009495894191786647
iteration 52, loss = 0.0009922446915879846
iteration 53, loss = 0.0008409985457547009
iteration 54, loss = 0.0007689654012210667
iteration 55, loss = 0.000823006615974009
iteration 56, loss = 0.0008084808941930532
iteration 57, loss = 0.0008720661862753332
iteration 58, loss = 0.0008518086397089064
iteration 59, loss = 0.0008948937174864113
iteration 60, loss = 0.0015744424890726805
iteration 61, loss = 0.0007519769133068621
iteration 62, loss = 0.0011354491580277681
iteration 63, loss = 0.0007760010194033384
iteration 64, loss = 0.0008899722015485168
iteration 65, loss = 0.0009068829822354019
iteration 66, loss = 0.0010069572599604726
iteration 67, loss = 0.0007949639693833888
iteration 68, loss = 0.0008252926054410636
iteration 69, loss = 0.0013443371281027794
iteration 70, loss = 0.0009989645332098007
iteration 71, loss = 0.0009861716534942389
iteration 72, loss = 0.0009637291659601033
iteration 73, loss = 0.0009717840584926307
iteration 74, loss = 0.0010540990624576807
iteration 75, loss = 0.0009874696843326092
iteration 76, loss = 0.001488541136495769
iteration 77, loss = 0.000835669634398073
iteration 78, loss = 0.0010038401233032346
iteration 79, loss = 0.0008880519890226424
iteration 80, loss = 0.0008590008364990354
iteration 81, loss = 0.000948546570725739
iteration 82, loss = 0.0009002863080240786
iteration 83, loss = 0.0008889622404240072
iteration 84, loss = 0.0009852801449596882
iteration 85, loss = 0.001009758678264916
iteration 86, loss = 0.00074294104706496
iteration 87, loss = 0.0008320522028952837
iteration 88, loss = 0.0008652388351038098
iteration 89, loss = 0.0009547224617563188
iteration 90, loss = 0.0010601833928376436
iteration 91, loss = 0.002063141204416752
iteration 92, loss = 0.0008364439709112048
iteration 93, loss = 0.0007575149065814912
iteration 94, loss = 0.0008315263548865914
iteration 95, loss = 0.0008441511308774352
iteration 96, loss = 0.0008484593708999455
iteration 97, loss = 0.0014434735057875514
iteration 98, loss = 0.0009131898987106979
iteration 99, loss = 0.000824347953312099
iteration 100, loss = 0.0007428817334584892
iteration 101, loss = 0.0009887701598927379
iteration 102, loss = 0.0007847042288631201
iteration 103, loss = 0.0012093049008399248
iteration 104, loss = 0.0007755006663501263
iteration 105, loss = 0.0010857167653739452
iteration 106, loss = 0.0013636803487315774
iteration 107, loss = 0.0007913538720458746
iteration 108, loss = 0.0008429058361798525
iteration 109, loss = 0.0007927919505164027
iteration 110, loss = 0.0008013268234208226
iteration 111, loss = 0.0015244816895574331
iteration 112, loss = 0.0009380824049003422
iteration 113, loss = 0.000818055123090744
iteration 114, loss = 0.0010481897043064237
iteration 115, loss = 0.0008176569826900959
iteration 116, loss = 0.0007793051190674305
iteration 117, loss = 0.000827212177682668
iteration 118, loss = 0.0011726240627467632
iteration 119, loss = 0.0008444751147180796
iteration 120, loss = 0.0008859324734658003
iteration 121, loss = 0.0016786393243819475
iteration 122, loss = 0.00082879833644256
iteration 123, loss = 0.0009524427587166429
iteration 124, loss = 0.0010636161314323545
iteration 125, loss = 0.0008547846227884293
iteration 126, loss = 0.0012894438114017248
iteration 127, loss = 0.0006976358126848936
iteration 128, loss = 0.00105318333953619
iteration 129, loss = 0.0007491296273656189
iteration 130, loss = 0.0009365088772028685
iteration 131, loss = 0.0006688707508146763
iteration 132, loss = 0.0007095112232491374
iteration 133, loss = 0.0009443680755794048
iteration 134, loss = 0.0008599614957347512
iteration 135, loss = 0.0008768100524321198
iteration 136, loss = 0.0013317971024662256
iteration 137, loss = 0.0016103872330859303
iteration 138, loss = 0.0007409615791402757
iteration 139, loss = 0.0007897239411249757
iteration 140, loss = 0.002097167307510972
iteration 141, loss = 0.0008056375081650913
iteration 142, loss = 0.0007409724057652056
iteration 143, loss = 0.0009040060103870928
iteration 144, loss = 0.0010703816078603268
iteration 145, loss = 0.0008971333736553788
iteration 146, loss = 0.000793983053881675
iteration 147, loss = 0.0008343365043401718
iteration 148, loss = 0.0009850631467998028
iteration 149, loss = 0.0007965083350427449
iteration 150, loss = 0.0008223810000345111
iteration 151, loss = 0.0016344137256965041
iteration 152, loss = 0.0011448172153905034
iteration 153, loss = 0.0009712128667160869
iteration 154, loss = 0.0015289626317098737
iteration 155, loss = 0.0009471626253798604
iteration 156, loss = 0.0013569809962064028
iteration 157, loss = 0.0013556132325902581
iteration 158, loss = 0.0009356057271361351
iteration 159, loss = 0.0009469772921875119
iteration 160, loss = 0.0006953204865567386
iteration 161, loss = 0.0008334895828738809
iteration 162, loss = 0.0009517332655377686
iteration 163, loss = 0.0007016664603725076
iteration 164, loss = 0.0008127834880724549
iteration 165, loss = 0.0008942838758230209
iteration 166, loss = 0.0012117696460336447
iteration 167, loss = 0.0016010385006666183
iteration 168, loss = 0.001412613783031702
iteration 169, loss = 0.0008494684007018805
iteration 170, loss = 0.000822598987724632
iteration 171, loss = 0.0008011005120351911
iteration 172, loss = 0.0009309162269346416
iteration 173, loss = 0.000885278859641403
iteration 174, loss = 0.001205139560624957
iteration 175, loss = 0.0010165816638618708
iteration 176, loss = 0.0008653514087200165
iteration 177, loss = 0.0009255555924028158
iteration 178, loss = 0.0009054260444827378
iteration 179, loss = 0.0007554269395768642
iteration 180, loss = 0.000742468168027699
iteration 181, loss = 0.000924478517845273
iteration 182, loss = 0.0010184001876041293
iteration 183, loss = 0.0010968738934025168
iteration 184, loss = 0.0008221549796871841
iteration 185, loss = 0.0007972956518642604
iteration 186, loss = 0.0014946067240089178
iteration 187, loss = 0.0009016280528157949
iteration 188, loss = 0.000963599537499249
iteration 189, loss = 0.0010388048831373453
iteration 190, loss = 0.001133631682023406
iteration 191, loss = 0.0007429672405123711
iteration 192, loss = 0.001587606268003583
iteration 193, loss = 0.001036929665133357
iteration 194, loss = 0.0009736629435792565
iteration 195, loss = 0.0011728600366041064
iteration 196, loss = 0.0007982947863638401
iteration 197, loss = 0.000721084070391953
iteration 198, loss = 0.001126932678744197
iteration 199, loss = 0.0009473953978158534
iteration 200, loss = 0.0009175962768495083
iteration 201, loss = 0.0008632754907011986
iteration 202, loss = 0.0011896592332050204
iteration 203, loss = 0.001054387423209846
iteration 204, loss = 0.0011678581358864903
iteration 205, loss = 0.0023815527092665434
iteration 206, loss = 0.0009403583826497197
iteration 207, loss = 0.0016552635934203863
iteration 208, loss = 0.0011202620808035135
iteration 209, loss = 0.0006924054468981922
iteration 210, loss = 0.0009525096975266933
iteration 211, loss = 0.0007554222829639912
iteration 212, loss = 0.0008684085914865136
iteration 213, loss = 0.000746310455724597
iteration 214, loss = 0.0007483377703465521
iteration 215, loss = 0.0015699886716902256
iteration 216, loss = 0.0009253139141947031
iteration 217, loss = 0.0008642293978482485
iteration 218, loss = 0.000761062721721828
iteration 219, loss = 0.001102364039979875
iteration 220, loss = 0.0008445151033811271
iteration 221, loss = 0.0010807054350152612
iteration 222, loss = 0.0007754962425678968
iteration 223, loss = 0.0009161026100628078
iteration 224, loss = 0.0012821678537875414
iteration 225, loss = 0.0009396570967510343
iteration 226, loss = 0.000851255317684263
iteration 227, loss = 0.0008929536561481655
iteration 228, loss = 0.0008336558821611106
iteration 229, loss = 0.001212089671753347
iteration 230, loss = 0.0008127634646371007
iteration 231, loss = 0.0007190312608145177
iteration 232, loss = 0.0008235745481215417
iteration 233, loss = 0.0010019619949162006
iteration 234, loss = 0.0007208076422102749
iteration 235, loss = 0.0007895795279182494
iteration 236, loss = 0.0025784496683627367
iteration 237, loss = 0.0008570036152377725
iteration 238, loss = 0.000702233228366822
iteration 239, loss = 0.0008399126818403602
iteration 240, loss = 0.0012545851059257984
iteration 241, loss = 0.0010077825281769037
iteration 242, loss = 0.0006897132261656225
iteration 243, loss = 0.000837205967400223
iteration 244, loss = 0.0009908408392220736
iteration 245, loss = 0.0009376976522617042
iteration 246, loss = 0.0010451734997332096
iteration 247, loss = 0.0009132331470027566
iteration 248, loss = 0.0016741059953346848
iteration 249, loss = 0.0015775654464960098
iteration 250, loss = 0.0010665080044418573
iteration 251, loss = 0.000789444602560252
iteration 252, loss = 0.0017004541587084532
iteration 253, loss = 0.0008594180108048022
iteration 254, loss = 0.0010636476799845695
iteration 255, loss = 0.000788015138823539
iteration 256, loss = 0.001188953290693462
iteration 257, loss = 0.0008400039514526725
iteration 258, loss = 0.0014645999763160944
iteration 259, loss = 0.0007630388718098402
iteration 260, loss = 0.0008816845947876573
iteration 261, loss = 0.000717498071026057
iteration 262, loss = 0.0007633690838702023
iteration 263, loss = 0.000908661400899291
iteration 264, loss = 0.0007323766476474702
iteration 265, loss = 0.0007835085853002965
iteration 266, loss = 0.0008308377582579851
iteration 267, loss = 0.0006859706481918693
iteration 268, loss = 0.0006406140746548772
iteration 269, loss = 0.0007944374228827655
iteration 270, loss = 0.0009964577620849013
iteration 271, loss = 0.0017144611338153481
iteration 272, loss = 0.0010901893256232142
iteration 273, loss = 0.0010601429967209697
iteration 274, loss = 0.0010703899897634983
iteration 275, loss = 0.0007644781726412475
iteration 276, loss = 0.0008852612227201462
iteration 277, loss = 0.0008864919072948396
iteration 278, loss = 0.0009342027478851378
iteration 279, loss = 0.0009453119128011167
iteration 280, loss = 0.0008735348237678409
iteration 281, loss = 0.000800765585154295
iteration 282, loss = 0.0009342576959170401
iteration 283, loss = 0.0007904627127572894
iteration 284, loss = 0.0007643141434527934
iteration 285, loss = 0.000882343330886215
iteration 286, loss = 0.0018709476571530104
iteration 287, loss = 0.000717588234692812
iteration 288, loss = 0.0009204491507261992
iteration 289, loss = 0.0008530174964107573
iteration 290, loss = 0.0008473980124108493
iteration 291, loss = 0.0009869335917755961
iteration 292, loss = 0.0007927195401862264
iteration 293, loss = 0.0007565570413134992
iteration 294, loss = 0.0008665245259180665
iteration 295, loss = 0.0012285839766263962
iteration 296, loss = 0.0008787778206169605
iteration 297, loss = 0.001268410705961287
iteration 298, loss = 0.0010627483716234565
iteration 299, loss = 0.0007618521340191364
iteration 300, loss = 0.0007998320506885648
iteration 1, loss = 0.0008383324020542204
iteration 2, loss = 0.001112873898819089
iteration 3, loss = 0.0009400130365975201
iteration 4, loss = 0.0008448756416328251
iteration 5, loss = 0.0023855860345065594
iteration 6, loss = 0.0008919753017835319
iteration 7, loss = 0.00074689497705549
iteration 8, loss = 0.0008215011330321431
iteration 9, loss = 0.0008131000213325024
iteration 10, loss = 0.000984284793958068
iteration 11, loss = 0.0008828933932818472
iteration 12, loss = 0.0008126529864966869
iteration 13, loss = 0.0012725607957690954
iteration 14, loss = 0.000957772193942219
iteration 15, loss = 0.0008429153822362423
iteration 16, loss = 0.00120394432451576
iteration 17, loss = 0.0019963881932199
iteration 18, loss = 0.000773369858507067
iteration 19, loss = 0.0008237426518462598
iteration 20, loss = 0.000878377235494554
iteration 21, loss = 0.0007403423078358173
iteration 22, loss = 0.0007672020583413541
iteration 23, loss = 0.0008019437082111835
iteration 24, loss = 0.0008853221079334617
iteration 25, loss = 0.0008313257130794227
iteration 26, loss = 0.0011331853456795216
iteration 27, loss = 0.0008658830774948001
iteration 28, loss = 0.0007446084637194872
iteration 29, loss = 0.0008297348977066576
iteration 30, loss = 0.0009196114260703325
iteration 31, loss = 0.0012684720568358898
iteration 32, loss = 0.0009990913094952703
iteration 33, loss = 0.0009738187654875219
iteration 34, loss = 0.0012030820362269878
iteration 35, loss = 0.0007883236394263804
iteration 36, loss = 0.0010425118962302804
iteration 37, loss = 0.0008552130893804133
iteration 38, loss = 0.0012389912735670805
iteration 39, loss = 0.0015879141865298152
iteration 40, loss = 0.0009788861498236656
iteration 41, loss = 0.0007738494896329939
iteration 42, loss = 0.0007681006100028753
iteration 43, loss = 0.0020800752099603415
iteration 44, loss = 0.0008544179145246744
iteration 45, loss = 0.0006920321029610932
iteration 46, loss = 0.0010387635556980968
iteration 47, loss = 0.0012537108268588781
iteration 48, loss = 0.00071760977152735
iteration 49, loss = 0.0012180585181340575
iteration 50, loss = 0.0009640772477723658
iteration 51, loss = 0.000771236838772893
iteration 52, loss = 0.001587925129570067
iteration 53, loss = 0.0009128970559686422
iteration 54, loss = 0.0008215437992475927
iteration 55, loss = 0.0008551217615604401
iteration 56, loss = 0.0012821326963603497
iteration 57, loss = 0.0010007433593273163
iteration 58, loss = 0.0012618557084351778
iteration 59, loss = 0.000814576807897538
iteration 60, loss = 0.0019753235392272472
iteration 61, loss = 0.001129595679230988
iteration 62, loss = 0.0013965140096843243
iteration 63, loss = 0.0009657173650339246
iteration 64, loss = 0.0008168966160155833
iteration 65, loss = 0.0007974542095325887
iteration 66, loss = 0.0009661981603130698
iteration 67, loss = 0.0007740561268292367
iteration 68, loss = 0.00099560571834445
iteration 69, loss = 0.0009099700837396085
iteration 70, loss = 0.0011053967755287886
iteration 71, loss = 0.0015382930869236588
iteration 72, loss = 0.0012865220196545124
iteration 73, loss = 0.000833694648463279
iteration 74, loss = 0.0009993745479732752
iteration 75, loss = 0.0007970847655087709
iteration 76, loss = 0.0009239614591933787
iteration 77, loss = 0.0007898815674707294
iteration 78, loss = 0.000951418827753514
iteration 79, loss = 0.0017260147724300623
iteration 80, loss = 0.0009121719631366432
iteration 81, loss = 0.0009805902373045683
iteration 82, loss = 0.0011071973713114858
iteration 83, loss = 0.0007273608935065567
iteration 84, loss = 0.0007917392067611217
iteration 85, loss = 0.0011833513854071498
iteration 86, loss = 0.0011328060645610094
iteration 87, loss = 0.0009242941741831601
iteration 88, loss = 0.0009551625698804855
iteration 89, loss = 0.001444551395252347
iteration 90, loss = 0.0008136969991028309
iteration 91, loss = 0.0010178206721320748
iteration 92, loss = 0.0008347661350853741
iteration 93, loss = 0.0009242940577678382
iteration 94, loss = 0.0008989002089947462
iteration 95, loss = 0.0008173648384399712
iteration 96, loss = 0.001108060241676867
iteration 97, loss = 0.0007471737335436046
iteration 98, loss = 0.0006612365832552314
iteration 99, loss = 0.0007774964324198663
iteration 100, loss = 0.0009155733860097826
iteration 101, loss = 0.000869840441737324
iteration 102, loss = 0.0009456222178414464
iteration 103, loss = 0.0009441726142540574
iteration 104, loss = 0.0010064988164231181
iteration 105, loss = 0.0011715535074472427
iteration 106, loss = 0.0010034830775111914
iteration 107, loss = 0.0008013509796001017
iteration 108, loss = 0.0007311335066333413
iteration 109, loss = 0.0008208510698750615
iteration 110, loss = 0.0008728772518225014
iteration 111, loss = 0.0009454755345359445
iteration 112, loss = 0.0007358610746450722
iteration 113, loss = 0.000690740707796067
iteration 114, loss = 0.001228497945703566
iteration 115, loss = 0.0009031858644448221
iteration 116, loss = 0.0009448135388083756
iteration 117, loss = 0.001498472411185503
iteration 118, loss = 0.0008214573608711362
iteration 119, loss = 0.0008172708912752569
iteration 120, loss = 0.000962279736995697
iteration 121, loss = 0.0007537835044786334
iteration 122, loss = 0.0015324028208851814
iteration 123, loss = 0.001610423089005053
iteration 124, loss = 0.0007955590263009071
iteration 125, loss = 0.0011202117893844843
iteration 126, loss = 0.0008341965149156749
iteration 127, loss = 0.0008254001149907708
iteration 128, loss = 0.0009547657100483775
iteration 129, loss = 0.001227452070452273
iteration 130, loss = 0.0008579422137700021
iteration 131, loss = 0.0007606575381942093
iteration 132, loss = 0.0008783115772530437
iteration 133, loss = 0.0007291876245290041
iteration 134, loss = 0.0011370062129572034
iteration 135, loss = 0.0008873132755979896
iteration 136, loss = 0.0009678834467194974
iteration 137, loss = 0.0007121341768652201
iteration 138, loss = 0.00142866768874228
iteration 139, loss = 0.0008326801471412182
iteration 140, loss = 0.0017919526435434818
iteration 141, loss = 0.0007110444130375981
iteration 142, loss = 0.0008206066559068859
iteration 143, loss = 0.001830589259043336
iteration 144, loss = 0.0012417190009728074
iteration 145, loss = 0.0008003772818483412
iteration 146, loss = 0.0006959498859941959
iteration 147, loss = 0.0008064968278631568
iteration 148, loss = 0.0007730957586318254
iteration 149, loss = 0.0012953114928677678
iteration 150, loss = 0.0009565716027282178
iteration 151, loss = 0.001014942885376513
iteration 152, loss = 0.0008190382504835725
iteration 153, loss = 0.0008710326510481536
iteration 154, loss = 0.0007410185062326491
iteration 155, loss = 0.0008575372630730271
iteration 156, loss = 0.0007965709082782269
iteration 157, loss = 0.0007357189897447824
iteration 158, loss = 0.0007479076739400625
iteration 159, loss = 0.0007635562215000391
iteration 160, loss = 0.00097247603116557
iteration 161, loss = 0.0010445857187733054
iteration 162, loss = 0.0008308665710501373
iteration 163, loss = 0.0008238152950070798
iteration 164, loss = 0.0009053656831383705
iteration 165, loss = 0.0008384715183638036
iteration 166, loss = 0.0007362752221524715
iteration 167, loss = 0.0009586445521563292
iteration 168, loss = 0.0008764597587287426
iteration 169, loss = 0.0006947851506993175
iteration 170, loss = 0.0008114249794743955
iteration 171, loss = 0.0007342887693084776
iteration 172, loss = 0.0010768374195322394
iteration 173, loss = 0.0014366996474564075
iteration 174, loss = 0.0008560264250263572
iteration 175, loss = 0.0007497530896216631
iteration 176, loss = 0.0009431857033632696
iteration 177, loss = 0.0009337058872915804
iteration 178, loss = 0.0015419949777424335
iteration 179, loss = 0.0011192134115844965
iteration 180, loss = 0.0009594708681106567
iteration 181, loss = 0.0017580465646460652
iteration 182, loss = 0.0008521656855009496
iteration 183, loss = 0.0007913776789791882
iteration 184, loss = 0.0012382653076201677
iteration 185, loss = 0.000870509073138237
iteration 186, loss = 0.0013385626953095198
iteration 187, loss = 0.0008805455872789025
iteration 188, loss = 0.0009379772236570716
iteration 189, loss = 0.0010391994146630168
iteration 190, loss = 0.000768221914768219
iteration 191, loss = 0.0007533655734732747
iteration 192, loss = 0.0008096402161754668
iteration 193, loss = 0.0010139753576368093
iteration 194, loss = 0.001545396400615573
iteration 195, loss = 0.0018547733779996634
iteration 196, loss = 0.0008485198486596346
iteration 197, loss = 0.0008093853248283267
iteration 198, loss = 0.001244479906745255
iteration 199, loss = 0.0006703059189021587
iteration 200, loss = 0.0009238885249942541
iteration 201, loss = 0.0008897168445400894
iteration 202, loss = 0.0008186192135326564
iteration 203, loss = 0.000887853791937232
iteration 204, loss = 0.001036901492625475
iteration 205, loss = 0.0010328581556677818
iteration 206, loss = 0.0011142027797177434
iteration 207, loss = 0.0008675679564476013
iteration 208, loss = 0.0011417802888900042
iteration 209, loss = 0.0010897759348154068
iteration 210, loss = 0.0007155906641855836
iteration 211, loss = 0.0018161170883104205
iteration 212, loss = 0.0015213543083518744
iteration 213, loss = 0.000904988672118634
iteration 214, loss = 0.0007314296090044081
iteration 215, loss = 0.0008032984915189445
iteration 216, loss = 0.0007675620727241039
iteration 217, loss = 0.0007414525025524199
iteration 218, loss = 0.0008098753751255572
iteration 219, loss = 0.0008191489614546299
iteration 220, loss = 0.0011255188146606088
iteration 221, loss = 0.000726129102986306
iteration 222, loss = 0.0008264464559033513
iteration 223, loss = 0.0011042049154639244
iteration 224, loss = 0.0007859760662540793
iteration 225, loss = 0.0008688500383868814
iteration 226, loss = 0.0011732802959159017
iteration 227, loss = 0.000837563187815249
iteration 228, loss = 0.0009444719180464745
iteration 229, loss = 0.0015127952210605145
iteration 230, loss = 0.0016770971706137061
iteration 231, loss = 0.0009409446502104402
iteration 232, loss = 0.0007148638251237571
iteration 233, loss = 0.0007396354340016842
iteration 234, loss = 0.0010307850316166878
iteration 235, loss = 0.0006632605800405145
iteration 236, loss = 0.0013517702464014292
iteration 237, loss = 0.0018977329600602388
iteration 238, loss = 0.0008321938221342862
iteration 239, loss = 0.0007150460733100772
iteration 240, loss = 0.0006958864978514612
iteration 241, loss = 0.0009496239945292473
iteration 242, loss = 0.0012551173567771912
iteration 243, loss = 0.0007106544217094779
iteration 244, loss = 0.0008562326547689736
iteration 245, loss = 0.0011165417963638902
iteration 246, loss = 0.0007569330045953393
iteration 247, loss = 0.0009442730224691331
iteration 248, loss = 0.0009818185353651643
iteration 249, loss = 0.0006835322710685432
iteration 250, loss = 0.0014467501314356923
iteration 251, loss = 0.0008174615795724094
iteration 252, loss = 0.0011455356143414974
iteration 253, loss = 0.0009758686064742506
iteration 254, loss = 0.0009620990022085607
iteration 255, loss = 0.0015265516703948379
iteration 256, loss = 0.001958772772923112
iteration 257, loss = 0.0007938449853099883
iteration 258, loss = 0.0008024286944419146
iteration 259, loss = 0.0009139067260548472
iteration 260, loss = 0.000929694389924407
iteration 261, loss = 0.0008394078467972577
iteration 262, loss = 0.0014228527434170246
iteration 263, loss = 0.0009532400872558355
iteration 264, loss = 0.0007578929653391242
iteration 265, loss = 0.0007329713553190231
iteration 266, loss = 0.0008757539326325059
iteration 267, loss = 0.0010811144020408392
iteration 268, loss = 0.0008842688985168934
iteration 269, loss = 0.0007525617838837206
iteration 270, loss = 0.0007662405841983855
iteration 271, loss = 0.0007533127791248262
iteration 272, loss = 0.002155048307031393
iteration 273, loss = 0.000750567065551877
iteration 274, loss = 0.0014721241313964128
iteration 275, loss = 0.0008026622235774994
iteration 276, loss = 0.0007129402947612107
iteration 277, loss = 0.0010730224894359708
iteration 278, loss = 0.0008601709268987179
iteration 279, loss = 0.0006773812347091734
iteration 280, loss = 0.0008782183867879212
iteration 281, loss = 0.0007683329749852419
iteration 282, loss = 0.0007619273965246975
iteration 283, loss = 0.000767905090469867
iteration 284, loss = 0.000810507801361382
iteration 285, loss = 0.0012502751778811216
iteration 286, loss = 0.0007315871771425009
iteration 287, loss = 0.0010026547824963927
iteration 288, loss = 0.0007348699145950377
iteration 289, loss = 0.0008619832224212587
iteration 290, loss = 0.0007289052591659129
iteration 291, loss = 0.0007581913378089666
iteration 292, loss = 0.001196348574012518
iteration 293, loss = 0.0023676485288888216
iteration 294, loss = 0.0009042139863595366
iteration 295, loss = 0.001327364705502987
iteration 296, loss = 0.000865988084115088
iteration 297, loss = 0.000816775078419596
iteration 298, loss = 0.0009376133093610406
iteration 299, loss = 0.0007613921770825982
iteration 300, loss = 0.0008872016333043575
iteration 1, loss = 0.0006936980644240975
iteration 2, loss = 0.001455876394174993
iteration 3, loss = 0.0010503327939659357
iteration 4, loss = 0.0007737037376500666
iteration 5, loss = 0.0007472825818695128
iteration 6, loss = 0.0015148433158174157
iteration 7, loss = 0.0008427977445535362
iteration 8, loss = 0.000678700627759099
iteration 9, loss = 0.0013103036908432841
iteration 10, loss = 0.001043928088620305
iteration 11, loss = 0.0011200602166354656
iteration 12, loss = 0.0008272295817732811
iteration 13, loss = 0.0010952543234452605
iteration 14, loss = 0.0011633684625849128
iteration 15, loss = 0.0009480594890192151
iteration 16, loss = 0.0007172721088863909
iteration 17, loss = 0.0007751778466627002
iteration 18, loss = 0.0010858130408450961
iteration 19, loss = 0.0009538755402900279
iteration 20, loss = 0.000838410691358149
iteration 21, loss = 0.0008729579858481884
iteration 22, loss = 0.001917859073728323
iteration 23, loss = 0.0009758659871295094
iteration 24, loss = 0.001098169945180416
iteration 25, loss = 0.0007596787763759494
iteration 26, loss = 0.0013629646273329854
iteration 27, loss = 0.0008834766340442002
iteration 28, loss = 0.0009065513149835169
iteration 29, loss = 0.0011254145065322518
iteration 30, loss = 0.0008367372211068869
iteration 31, loss = 0.0009861282305791974
iteration 32, loss = 0.001448257127776742
iteration 33, loss = 0.0007627315353602171
iteration 34, loss = 0.001594211091287434
iteration 35, loss = 0.0009431930957362056
iteration 36, loss = 0.0008493956411257386
iteration 37, loss = 0.0008184152538888156
iteration 38, loss = 0.0008071928168646991
iteration 39, loss = 0.0010036975145339966
iteration 40, loss = 0.0008491497137583792
iteration 41, loss = 0.0006864256574772298
iteration 42, loss = 0.0008049571188166738
iteration 43, loss = 0.0008107886533252895
iteration 44, loss = 0.0008735195733606815
iteration 45, loss = 0.0008309621480293572
iteration 46, loss = 0.002473214641213417
iteration 47, loss = 0.0008585738833062351
iteration 48, loss = 0.0009198088082484901
iteration 49, loss = 0.00102849374525249
iteration 50, loss = 0.0008236904395744205
iteration 51, loss = 0.0007083174423314631
iteration 52, loss = 0.0010623885318636894
iteration 53, loss = 0.0008502883138135076
iteration 54, loss = 0.0010611905017867684
iteration 55, loss = 0.000825370429083705
iteration 56, loss = 0.0009195547318086028
iteration 57, loss = 0.0012325809802860022
iteration 58, loss = 0.0014945829752832651
iteration 59, loss = 0.0010840583126991987
iteration 60, loss = 0.0011654875706881285
iteration 61, loss = 0.0008942597778514028
iteration 62, loss = 0.0009288518340326846
iteration 63, loss = 0.0007270472124218941
iteration 64, loss = 0.0007350382511503994
iteration 65, loss = 0.0013279399136081338
iteration 66, loss = 0.000876119767781347
iteration 67, loss = 0.001008183229714632
iteration 68, loss = 0.001145359594374895
iteration 69, loss = 0.0008494807407259941
iteration 70, loss = 0.000841690634842962
iteration 71, loss = 0.0009778751991689205
iteration 72, loss = 0.0007341921445913613
iteration 73, loss = 0.0009294482879340649
iteration 74, loss = 0.001100044697523117
iteration 75, loss = 0.0009569266112521291
iteration 76, loss = 0.0007774989935569465
iteration 77, loss = 0.0008229422383010387
iteration 78, loss = 0.0007296836702153087
iteration 79, loss = 0.0009724209667183459
iteration 80, loss = 0.0016089631244540215
iteration 81, loss = 0.0008245877688750625
iteration 82, loss = 0.0009761826368048787
iteration 83, loss = 0.00172055559232831
iteration 84, loss = 0.001113607082515955
iteration 85, loss = 0.0008710506372153759
iteration 86, loss = 0.000846350914798677
iteration 87, loss = 0.0008177654235623777
iteration 88, loss = 0.0012616529129445553
iteration 89, loss = 0.0008594325045123696
iteration 90, loss = 0.0010284333257004619
iteration 91, loss = 0.0007704010349698365
iteration 92, loss = 0.000746937352232635
iteration 93, loss = 0.0008382886881008744
iteration 94, loss = 0.0007890415145084262
iteration 95, loss = 0.000825058261398226
iteration 96, loss = 0.0008835353655740619
iteration 97, loss = 0.0008429340086877346
iteration 98, loss = 0.0012172604911029339
iteration 99, loss = 0.0014669941738247871
iteration 100, loss = 0.0008927875896915793
iteration 101, loss = 0.0007231021299958229
iteration 102, loss = 0.0009612856665626168
iteration 103, loss = 0.0007820567698217928
iteration 104, loss = 0.0006817738758400083
iteration 105, loss = 0.0010112986201420426
iteration 106, loss = 0.0014095006044954062
iteration 107, loss = 0.0009411999490112066
iteration 108, loss = 0.001147374976426363
iteration 109, loss = 0.0015637378674000502
iteration 110, loss = 0.0008907982846722007
iteration 111, loss = 0.0011604593601077795
iteration 112, loss = 0.0008220298332162201
iteration 113, loss = 0.0008746525854803622
iteration 114, loss = 0.0016449801623821259
iteration 115, loss = 0.0014709950191900134
iteration 116, loss = 0.0008339633932337165
iteration 117, loss = 0.002084295963868499
iteration 118, loss = 0.0010261046700179577
iteration 119, loss = 0.0009981319308280945
iteration 120, loss = 0.000800285255536437
iteration 121, loss = 0.0007735929102636874
iteration 122, loss = 0.0009758633095771074
iteration 123, loss = 0.0007092081941664219
iteration 124, loss = 0.001035563531331718
iteration 125, loss = 0.000897551013622433
iteration 126, loss = 0.0008053269702941179
iteration 127, loss = 0.000800972105935216
iteration 128, loss = 0.0008796051843091846
iteration 129, loss = 0.0007747383788228035
iteration 130, loss = 0.0008620318258181214
iteration 131, loss = 0.0008167597698047757
iteration 132, loss = 0.0010494980961084366
iteration 133, loss = 0.0012521027820184827
iteration 134, loss = 0.0007720832945778966
iteration 135, loss = 0.0007617471273988485
iteration 136, loss = 0.000937602948397398
iteration 137, loss = 0.001013853121548891
iteration 138, loss = 0.0009490218944847584
iteration 139, loss = 0.0007863321807235479
iteration 140, loss = 0.000739461334887892
iteration 141, loss = 0.0007716316031292081
iteration 142, loss = 0.0008718047756701708
iteration 143, loss = 0.0009066119091585279
iteration 144, loss = 0.001242204918526113
iteration 145, loss = 0.0008304404909722507
iteration 146, loss = 0.0008201022283174098
iteration 147, loss = 0.00145011639688164
iteration 148, loss = 0.0012271292507648468
iteration 149, loss = 0.0013737097615376115
iteration 150, loss = 0.0008157746051438153
iteration 151, loss = 0.0009587515378370881
iteration 152, loss = 0.0007156966021284461
iteration 153, loss = 0.0010514536406844854
iteration 154, loss = 0.0007798992446623743
iteration 155, loss = 0.0008250464452430606
iteration 156, loss = 0.0010321459267288446
iteration 157, loss = 0.0009251699666492641
iteration 158, loss = 0.0006995125440880656
iteration 159, loss = 0.001203390653245151
iteration 160, loss = 0.0011821971274912357
iteration 161, loss = 0.0009548358502797782
iteration 162, loss = 0.0008271340630017221
iteration 163, loss = 0.0007763911853544414
iteration 164, loss = 0.0009086712962016463
iteration 165, loss = 0.0010686734458431602
iteration 166, loss = 0.0009438495035283267
iteration 167, loss = 0.0007569139124825597
iteration 168, loss = 0.001049756072461605
iteration 169, loss = 0.000857141858432442
iteration 170, loss = 0.0016822408651933074
iteration 171, loss = 0.0007575250929221511
iteration 172, loss = 0.0011953898938372731
iteration 173, loss = 0.0009413831867277622
iteration 174, loss = 0.0014628377975896
iteration 175, loss = 0.0009080288582481444
iteration 176, loss = 0.0011382473167032003
iteration 177, loss = 0.0008146651089191437
iteration 178, loss = 0.0008407136192545295
iteration 179, loss = 0.001626280602067709
iteration 180, loss = 0.0008931785123422742
iteration 181, loss = 0.0010661534033715725
iteration 182, loss = 0.0008780091302469373
iteration 183, loss = 0.0008568040793761611
iteration 184, loss = 0.0008479955140501261
iteration 185, loss = 0.0011170008219778538
iteration 186, loss = 0.0008871337049640715
iteration 187, loss = 0.0017156396061182022
iteration 188, loss = 0.0011326044332236052
iteration 189, loss = 0.0009100981405936182
iteration 190, loss = 0.0010298178531229496
iteration 191, loss = 0.0008464902639389038
iteration 192, loss = 0.0009532577823847532
iteration 193, loss = 0.0008145087631419301
iteration 194, loss = 0.0007539629586972296
iteration 195, loss = 0.0008458885131403804
iteration 196, loss = 0.0008293966529890895
iteration 197, loss = 0.0008325150120072067
iteration 198, loss = 0.0009460390429012477
iteration 199, loss = 0.0007590529276058078
iteration 200, loss = 0.001441130181774497
iteration 201, loss = 0.0007575456984341145
iteration 202, loss = 0.0009617359028197825
iteration 203, loss = 0.0008182149613276124
iteration 204, loss = 0.0008059008978307247
iteration 205, loss = 0.0007940697832964361
iteration 206, loss = 0.0007868732209317386
iteration 207, loss = 0.000896771380212158
iteration 208, loss = 0.0008418820216320455
iteration 209, loss = 0.0008019237429834902
iteration 210, loss = 0.0008834140608087182
iteration 211, loss = 0.000828052347060293
iteration 212, loss = 0.0008761166245676577
iteration 213, loss = 0.0007017084863036871
iteration 214, loss = 0.0015265874098986387
iteration 215, loss = 0.0007440082263201475
iteration 216, loss = 0.0011974606895819306
iteration 217, loss = 0.0009838324040174484
iteration 218, loss = 0.0008221259340643883
iteration 219, loss = 0.0009684281540103257
iteration 220, loss = 0.001347093377262354
iteration 221, loss = 0.0007203379645943642
iteration 222, loss = 0.00080667226575315
iteration 223, loss = 0.0006810011109337211
iteration 224, loss = 0.0008273277780972421
iteration 225, loss = 0.0010690094204619527
iteration 226, loss = 0.0007240159320645034
iteration 227, loss = 0.0009649847634136677
iteration 228, loss = 0.001585159683600068
iteration 229, loss = 0.0007498293416574597
iteration 230, loss = 0.0009164049406535923
iteration 231, loss = 0.0010140043450519443
iteration 232, loss = 0.0008370124851353467
iteration 233, loss = 0.0007384847267530859
iteration 234, loss = 0.0009781604167073965
iteration 235, loss = 0.0008142289589159191
iteration 236, loss = 0.0008138768607750535
iteration 237, loss = 0.0007553749019280076
iteration 238, loss = 0.0008632360841147602
iteration 239, loss = 0.0008710310212336481
iteration 240, loss = 0.0008537676185369492
iteration 241, loss = 0.0007856035372242332
iteration 242, loss = 0.001109180971980095
iteration 243, loss = 0.0008625923073850572
iteration 244, loss = 0.0009120023460127413
iteration 245, loss = 0.0010264288866892457
iteration 246, loss = 0.0007766801863908768
iteration 247, loss = 0.0007899244665168226
iteration 248, loss = 0.0007283557206392288
iteration 249, loss = 0.0009448149939998984
iteration 250, loss = 0.0007666916353628039
iteration 251, loss = 0.0009269527508877218
iteration 252, loss = 0.001577763701789081
iteration 253, loss = 0.001085889176465571
iteration 254, loss = 0.0015387425664812326
iteration 255, loss = 0.0009502473403699696
iteration 256, loss = 0.0007393101113848388
iteration 257, loss = 0.0015396139351651073
iteration 258, loss = 0.0007912686560302973
iteration 259, loss = 0.0008156739640980959
iteration 260, loss = 0.0007990710437297821
iteration 261, loss = 0.0024272645823657513
iteration 262, loss = 0.0007833772688172758
iteration 263, loss = 0.0009593084105290473
iteration 264, loss = 0.0008921323460526764
iteration 265, loss = 0.001119145774282515
iteration 266, loss = 0.0007523997919633985
iteration 267, loss = 0.0014782361686229706
iteration 268, loss = 0.0008338482584804296
iteration 269, loss = 0.0006912605604156852
iteration 270, loss = 0.0009415721870027483
iteration 271, loss = 0.001566847087815404
iteration 272, loss = 0.0008635452832095325
iteration 273, loss = 0.0008356495527550578
iteration 274, loss = 0.0008879182860255241
iteration 275, loss = 0.0011549202026799321
iteration 276, loss = 0.00195350986905396
iteration 277, loss = 0.0008073858916759491
iteration 278, loss = 0.0013956697657704353
iteration 279, loss = 0.0007300259312614799
iteration 280, loss = 0.0019634629134088755
iteration 281, loss = 0.0017324391519650817
iteration 282, loss = 0.000755221233703196
iteration 283, loss = 0.0009897208074107766
iteration 284, loss = 0.0009425675380043685
iteration 285, loss = 0.0007637154194526374
iteration 286, loss = 0.0009054969414137304
iteration 287, loss = 0.001036604167893529
iteration 288, loss = 0.0009129410609602928
iteration 289, loss = 0.0010681173298507929
iteration 290, loss = 0.0009640068165026605
iteration 291, loss = 0.0018082711612805724
iteration 292, loss = 0.0007461092318408191
iteration 293, loss = 0.0007152164471335709
iteration 294, loss = 0.0009013669332489371
iteration 295, loss = 0.0007337743300013244
iteration 296, loss = 0.0009732447215355933
iteration 297, loss = 0.0009265808621421456
iteration 298, loss = 0.0008842455572448671
iteration 299, loss = 0.0011048768647015095
iteration 300, loss = 0.001470922608859837
iteration 1, loss = 0.0009046599152497947
iteration 2, loss = 0.0009299790835939348
iteration 3, loss = 0.0008412929601036012
iteration 4, loss = 0.0014475841308012605
iteration 5, loss = 0.0018126226495951414
iteration 6, loss = 0.0007517976919189095
iteration 7, loss = 0.000994134577922523
iteration 8, loss = 0.0008313345024362206
iteration 9, loss = 0.001017382717691362
iteration 10, loss = 0.0016299462877213955
iteration 11, loss = 0.000891619361937046
iteration 12, loss = 0.001043609227053821
iteration 13, loss = 0.0008767037652432919
iteration 14, loss = 0.0007461871718987823
iteration 15, loss = 0.0007524591055698693
iteration 16, loss = 0.0007517923950217664
iteration 17, loss = 0.0013523442903533578
iteration 18, loss = 0.0007810320239514112
iteration 19, loss = 0.0010196554940193892
iteration 20, loss = 0.0008203103207051754
iteration 21, loss = 0.0008059379179030657
iteration 22, loss = 0.0008148291963152587
iteration 23, loss = 0.00099698337726295
iteration 24, loss = 0.0010026106610894203
iteration 25, loss = 0.001366776297800243
iteration 26, loss = 0.0008287499658763409
iteration 27, loss = 0.0006961206090636551
iteration 28, loss = 0.0016474495641887188
iteration 29, loss = 0.0006788332248106599
iteration 30, loss = 0.0009549924870952964
iteration 31, loss = 0.0006928705843165517
iteration 32, loss = 0.0008301390334963799
iteration 33, loss = 0.0010223145363852382
iteration 34, loss = 0.000781418289989233
iteration 35, loss = 0.0008399608195759356
iteration 36, loss = 0.000891383970156312
iteration 37, loss = 0.0012105792993679643
iteration 38, loss = 0.0007854568539187312
iteration 39, loss = 0.0009379611001349986
iteration 40, loss = 0.0008353832527063787
iteration 41, loss = 0.0007642615237273276
iteration 42, loss = 0.0018272222951054573
iteration 43, loss = 0.0011407436104491353
iteration 44, loss = 0.0007417855667881668
iteration 45, loss = 0.0008399530779570341
iteration 46, loss = 0.0012890022480860353
iteration 47, loss = 0.0007505379035137594
iteration 48, loss = 0.001151684089563787
iteration 49, loss = 0.0018103978363797069
iteration 50, loss = 0.0010948749259114265
iteration 51, loss = 0.0008407347486354411
iteration 52, loss = 0.0007996495114639401
iteration 53, loss = 0.0006666172412224114
iteration 54, loss = 0.0011500127147883177
iteration 55, loss = 0.0007876358577050269
iteration 56, loss = 0.0009639931959100068
iteration 57, loss = 0.0010887270327657461
iteration 58, loss = 0.0008891649777069688
iteration 59, loss = 0.0007830687682144344
iteration 60, loss = 0.0008610956137999892
iteration 61, loss = 0.001659194240346551
iteration 62, loss = 0.0007909723790362477
iteration 63, loss = 0.0008761715143918991
iteration 64, loss = 0.0009988542879000306
iteration 65, loss = 0.0008201636373996735
iteration 66, loss = 0.000830375705845654
iteration 67, loss = 0.0008269576937891543
iteration 68, loss = 0.0012616042513400316
iteration 69, loss = 0.001116089173592627
iteration 70, loss = 0.0010607204167172313
iteration 71, loss = 0.0008459070231765509
iteration 72, loss = 0.0009170735138468444
iteration 73, loss = 0.0015205689705908298
iteration 74, loss = 0.000757846690248698
iteration 75, loss = 0.0007911375141702592
iteration 76, loss = 0.0010602360125631094
iteration 77, loss = 0.0018908665515482426
iteration 78, loss = 0.0007855425938032568
iteration 79, loss = 0.0007288528722710907
iteration 80, loss = 0.001439928077161312
iteration 81, loss = 0.0008853264735080302
iteration 82, loss = 0.0007880039047449827
iteration 83, loss = 0.000726411584764719
iteration 84, loss = 0.002484564669430256
iteration 85, loss = 0.0012783212587237358
iteration 86, loss = 0.0008244052878580987
iteration 87, loss = 0.0008309319382533431
iteration 88, loss = 0.001590532949194312
iteration 89, loss = 0.0010401351610198617
iteration 90, loss = 0.0008553541847504675
iteration 91, loss = 0.000893854012247175
iteration 92, loss = 0.0011343542719259858
iteration 93, loss = 0.0009783700807020068
iteration 94, loss = 0.0007913862937130034
iteration 95, loss = 0.000824465649202466
iteration 96, loss = 0.0009897596901282668
iteration 97, loss = 0.0007094881148077548
iteration 98, loss = 0.0010682787979021668
iteration 99, loss = 0.0008754209848120809
iteration 100, loss = 0.0008365579415112734
iteration 101, loss = 0.0011002838145941496
iteration 102, loss = 0.0017488512676209211
iteration 103, loss = 0.0008542865398339927
iteration 104, loss = 0.0012449781643226743
iteration 105, loss = 0.0010757849086076021
iteration 106, loss = 0.0009251150768250227
iteration 107, loss = 0.0009130905382335186
iteration 108, loss = 0.0007273305673152208
iteration 109, loss = 0.000920053047593683
iteration 110, loss = 0.0010207316372543573
iteration 111, loss = 0.0007299616700038314
iteration 112, loss = 0.0016913622384890914
iteration 113, loss = 0.0008757655741646886
iteration 114, loss = 0.0008057488594204187
iteration 115, loss = 0.0007009395048953593
iteration 116, loss = 0.000834212580230087
iteration 117, loss = 0.0024092583917081356
iteration 118, loss = 0.0008898351225070655
iteration 119, loss = 0.001187603920698166
iteration 120, loss = 0.001119451248086989
iteration 121, loss = 0.0007832584669813514
iteration 122, loss = 0.0008017489453777671
iteration 123, loss = 0.0008427809225395322
iteration 124, loss = 0.0010987147688865662
iteration 125, loss = 0.000959903234615922
iteration 126, loss = 0.0008931527263484895
iteration 127, loss = 0.0011461337562650442
iteration 128, loss = 0.0008161840960383415
iteration 129, loss = 0.0007658544345758855
iteration 130, loss = 0.0009636114700697362
iteration 131, loss = 0.0014638904249295592
iteration 132, loss = 0.0016342208255082369
iteration 133, loss = 0.0007986396085470915
iteration 134, loss = 0.000820591056253761
iteration 135, loss = 0.0007953781168907881
iteration 136, loss = 0.0009497376158833504
iteration 137, loss = 0.0008050819160416722
iteration 138, loss = 0.0012343357084318995
iteration 139, loss = 0.0013598331715911627
iteration 140, loss = 0.0017312453128397465
iteration 141, loss = 0.0007514371536672115
iteration 142, loss = 0.0008792938315309584
iteration 143, loss = 0.0008761932258494198
iteration 144, loss = 0.0012530904496088624
iteration 145, loss = 0.0014157193945720792
iteration 146, loss = 0.0007886276580393314
iteration 147, loss = 0.0008809452992863953
iteration 148, loss = 0.0007820568862371147
iteration 149, loss = 0.000921733386348933
iteration 150, loss = 0.0008798440685495734
iteration 151, loss = 0.0007112377788871527
iteration 152, loss = 0.0014992390060797334
iteration 153, loss = 0.0011455054627731442
iteration 154, loss = 0.0009022622252814472
iteration 155, loss = 0.0010835513239726424
iteration 156, loss = 0.0008339135674759746
iteration 157, loss = 0.001206589862704277
iteration 158, loss = 0.0008051374461501837
iteration 159, loss = 0.0007043677032925189
iteration 160, loss = 0.0008514149230904877
iteration 161, loss = 0.0008334690937772393
iteration 162, loss = 0.0008091279305517673
iteration 163, loss = 0.0009953397093340755
iteration 164, loss = 0.0008842363022267818
iteration 165, loss = 0.0010809814557433128
iteration 166, loss = 0.001127861556597054
iteration 167, loss = 0.0007917098118923604
iteration 168, loss = 0.0007203308632597327
iteration 169, loss = 0.0008856240892782807
iteration 170, loss = 0.001850756467320025
iteration 171, loss = 0.001149869174696505
iteration 172, loss = 0.0009230829891748726
iteration 173, loss = 0.0010696032550185919
iteration 174, loss = 0.0007426799274981022
iteration 175, loss = 0.0009027504129335284
iteration 176, loss = 0.0010854662396013737
iteration 177, loss = 0.000836462655570358
iteration 178, loss = 0.0016842265613377094
iteration 179, loss = 0.0007080874056555331
iteration 180, loss = 0.0010899013141170144
iteration 181, loss = 0.00091545534087345
iteration 182, loss = 0.0008091800846159458
iteration 183, loss = 0.0009628738043829799
iteration 184, loss = 0.0012800282565876842
iteration 185, loss = 0.0012361849658191204
iteration 186, loss = 0.001115949940867722
iteration 187, loss = 0.001069297082722187
iteration 188, loss = 0.0006891426164656878
iteration 189, loss = 0.0007576813804917037
iteration 190, loss = 0.0011145769385620952
iteration 191, loss = 0.0007472982397302985
iteration 192, loss = 0.0011652323883026838
iteration 193, loss = 0.0007964078104123473
iteration 194, loss = 0.0012825418962165713
iteration 195, loss = 0.0015977303264662623
iteration 196, loss = 0.0007954745087772608
iteration 197, loss = 0.0009428390767425299
iteration 198, loss = 0.0009253706084564328
iteration 199, loss = 0.000804219045676291
iteration 200, loss = 0.0010732809314504266
iteration 201, loss = 0.000905028369743377
iteration 202, loss = 0.0009181759087368846
iteration 203, loss = 0.0007601818651892245
iteration 204, loss = 0.0007732217200100422
iteration 205, loss = 0.0014519075630232692
iteration 206, loss = 0.0007458011386916041
iteration 207, loss = 0.0008909913594834507
iteration 208, loss = 0.0010583046823740005
iteration 209, loss = 0.0007591053145006299
iteration 210, loss = 0.0009739213273860514
iteration 211, loss = 0.0009053577086888254
iteration 212, loss = 0.0009204408852383494
iteration 213, loss = 0.0009053259855136275
iteration 214, loss = 0.0011621732264757156
iteration 215, loss = 0.0007671097409911454
iteration 216, loss = 0.0007649955805391073
iteration 217, loss = 0.0007305477629415691
iteration 218, loss = 0.0008154968381859362
iteration 219, loss = 0.000786089338362217
iteration 220, loss = 0.0007026829407550395
iteration 221, loss = 0.001150050782598555
iteration 222, loss = 0.000942945305723697
iteration 223, loss = 0.0008130624773912132
iteration 224, loss = 0.0008662803447805345
iteration 225, loss = 0.000986307393759489
iteration 226, loss = 0.0009718117071315646
iteration 227, loss = 0.0009425928583368659
iteration 228, loss = 0.000972324050962925
iteration 229, loss = 0.0008923249552026391
iteration 230, loss = 0.0008041599066928029
iteration 231, loss = 0.0009588885586708784
iteration 232, loss = 0.0009078214061446488
iteration 233, loss = 0.000979941338300705
iteration 234, loss = 0.0007469675620086491
iteration 235, loss = 0.0007544617401435971
iteration 236, loss = 0.0007454098085872829
iteration 237, loss = 0.0009516155696474016
iteration 238, loss = 0.0008905804716050625
iteration 239, loss = 0.0008072524797171354
iteration 240, loss = 0.0007736630504950881
iteration 241, loss = 0.0007439410546794534
iteration 242, loss = 0.001758960192091763
iteration 243, loss = 0.0010074111633002758
iteration 244, loss = 0.000994012109003961
iteration 245, loss = 0.0008721257909201086
iteration 246, loss = 0.00078347057569772
iteration 247, loss = 0.0007756453123874962
iteration 248, loss = 0.0013136458583176136
iteration 249, loss = 0.0008187172352336347
iteration 250, loss = 0.0011213187826797366
iteration 251, loss = 0.0007258600671775639
iteration 252, loss = 0.0008537469548173249
iteration 253, loss = 0.0010707791661843657
iteration 254, loss = 0.0009556068107485771
iteration 255, loss = 0.0007142856484279037
iteration 256, loss = 0.0009110190439969301
iteration 257, loss = 0.0010705606546252966
iteration 258, loss = 0.0009264477994292974
iteration 259, loss = 0.0014861030504107475
iteration 260, loss = 0.0008149115019477904
iteration 261, loss = 0.0010058999760076404
iteration 262, loss = 0.0009128637029789388
iteration 263, loss = 0.00120066711679101
iteration 264, loss = 0.0009632882429286838
iteration 265, loss = 0.0012412487994879484
iteration 266, loss = 0.0007771821110509336
iteration 267, loss = 0.0007677937974222004
iteration 268, loss = 0.0007985940319485962
iteration 269, loss = 0.0009388758335262537
iteration 270, loss = 0.0008796682814136147
iteration 271, loss = 0.0007611628388985991
iteration 272, loss = 0.0010387676302343607
iteration 273, loss = 0.0009394946973770857
iteration 274, loss = 0.0008817469934001565
iteration 275, loss = 0.0008259487804025412
iteration 276, loss = 0.0008853714098222554
iteration 277, loss = 0.000863360648509115
iteration 278, loss = 0.001072603976354003
iteration 279, loss = 0.000802478170953691
iteration 280, loss = 0.0011773239821195602
iteration 281, loss = 0.001108485390432179
iteration 282, loss = 0.0012003687443211675
iteration 283, loss = 0.0007749135256744921
iteration 284, loss = 0.0011524028377607465
iteration 285, loss = 0.0011488315649330616
iteration 286, loss = 0.000931682821828872
iteration 287, loss = 0.0015441738069057465
iteration 288, loss = 0.0009029037319123745
iteration 289, loss = 0.0009509615483693779
iteration 290, loss = 0.0010247071040794253
iteration 291, loss = 0.0008145731408149004
iteration 292, loss = 0.001623578486032784
iteration 293, loss = 0.001403369358740747
iteration 294, loss = 0.0007237782119773328
iteration 295, loss = 0.0008783319499343634
iteration 296, loss = 0.0008137912373058498
iteration 297, loss = 0.0009479111176915467
iteration 298, loss = 0.0008207442006096244
iteration 299, loss = 0.0011317686876282096
iteration 300, loss = 0.0008692436967976391
iteration 1, loss = 0.0008716534357517958
iteration 2, loss = 0.0008996989345178008
iteration 3, loss = 0.0008896275539882481
iteration 4, loss = 0.001317175105214119
iteration 5, loss = 0.001723671332001686
iteration 6, loss = 0.0012641201028600335
iteration 7, loss = 0.0009225470712408423
iteration 8, loss = 0.0009335813811048865
iteration 9, loss = 0.0017361519858241081
iteration 10, loss = 0.0009670961298979819
iteration 11, loss = 0.0009018081473186612
iteration 12, loss = 0.0011537334648892283
iteration 13, loss = 0.0009422987932339311
iteration 14, loss = 0.0007572993054054677
iteration 15, loss = 0.0010009451070800424
iteration 16, loss = 0.0014635962434113026
iteration 17, loss = 0.0012208616826683283
iteration 18, loss = 0.0008560337591916323
iteration 19, loss = 0.0007931196014396846
iteration 20, loss = 0.0010639580432325602
iteration 21, loss = 0.0008236046996898949
iteration 22, loss = 0.0007607038132846355
iteration 23, loss = 0.0009115405264310539
iteration 24, loss = 0.000698049203492701
iteration 25, loss = 0.0007838094606995583
iteration 26, loss = 0.0008059322135522962
iteration 27, loss = 0.0009879982098937035
iteration 28, loss = 0.0014769849367439747
iteration 29, loss = 0.0008769648848101497
iteration 30, loss = 0.0015880002174526453
iteration 31, loss = 0.0007300218567252159
iteration 32, loss = 0.0011016166536137462
iteration 33, loss = 0.0007542200037278235
iteration 34, loss = 0.0008431498426944017
iteration 35, loss = 0.0008673322154209018
iteration 36, loss = 0.0007409073296003044
iteration 37, loss = 0.0008529950282536447
iteration 38, loss = 0.000703553669154644
iteration 39, loss = 0.0007878434844315052
iteration 40, loss = 0.0008580982685089111
iteration 41, loss = 0.001054408261552453
iteration 42, loss = 0.0007782644825056195
iteration 43, loss = 0.0009508614894002676
iteration 44, loss = 0.0010531669249758124
iteration 45, loss = 0.0009115394204854965
iteration 46, loss = 0.0008382606320083141
iteration 47, loss = 0.0018600922776386142
iteration 48, loss = 0.0016376826679334044
iteration 49, loss = 0.0008179086144082248
iteration 50, loss = 0.0008213045657612383
iteration 51, loss = 0.001155967591330409
iteration 52, loss = 0.0007955765468068421
iteration 53, loss = 0.0008148183114826679
iteration 54, loss = 0.0007236693636514246
iteration 55, loss = 0.0015545912319794297
iteration 56, loss = 0.0007870056433603168
iteration 57, loss = 0.0007592780748382211
iteration 58, loss = 0.0007542037637904286
iteration 59, loss = 0.0008763723890297115
iteration 60, loss = 0.0007603849517181516
iteration 61, loss = 0.000759466434828937
iteration 62, loss = 0.0008597068954259157
iteration 63, loss = 0.0008180498261936009
iteration 64, loss = 0.0010132355382665992
iteration 65, loss = 0.0007439606124535203
iteration 66, loss = 0.0007701324066147208
iteration 67, loss = 0.0006226246478036046
iteration 68, loss = 0.0011793903540819883
iteration 69, loss = 0.000992023153230548
iteration 70, loss = 0.0007660531555302441
iteration 71, loss = 0.0008699683239683509
iteration 72, loss = 0.0009037190466187894
iteration 73, loss = 0.0013955577742308378
iteration 74, loss = 0.0007233499782159925
iteration 75, loss = 0.0008175062830559909
iteration 76, loss = 0.0008171491790562868
iteration 77, loss = 0.0007848131936043501
iteration 78, loss = 0.0007887111860327423
iteration 79, loss = 0.0007585021667182446
iteration 80, loss = 0.0010237607639282942
iteration 81, loss = 0.001900597126223147
iteration 82, loss = 0.0012094047851860523
iteration 83, loss = 0.0007737035630270839
iteration 84, loss = 0.0010571344755589962
iteration 85, loss = 0.000922598410397768
iteration 86, loss = 0.001198025420308113
iteration 87, loss = 0.0007847934029996395
iteration 88, loss = 0.0007851974805817008
iteration 89, loss = 0.0007835511933080852
iteration 90, loss = 0.0007349716033786535
iteration 91, loss = 0.0018494691466912627
iteration 92, loss = 0.0010606837458908558
iteration 93, loss = 0.0008014732738956809
iteration 94, loss = 0.0013108305865898728
iteration 95, loss = 0.001178100472316146
iteration 96, loss = 0.0007399856694974005
iteration 97, loss = 0.0007973439642228186
iteration 98, loss = 0.0008968181791715324
iteration 99, loss = 0.0015430408529937267
iteration 100, loss = 0.0009633180452510715
iteration 101, loss = 0.0008307455573230982
iteration 102, loss = 0.0017292951233685017
iteration 103, loss = 0.0007653753855265677
iteration 104, loss = 0.0009716819622553885
iteration 105, loss = 0.0007640811963938177
iteration 106, loss = 0.0010648805182427168
iteration 107, loss = 0.0007010865956544876
iteration 108, loss = 0.0008943044231273234
iteration 109, loss = 0.0007549150614067912
iteration 110, loss = 0.0008507594466209412
iteration 111, loss = 0.0009183911024592817
iteration 112, loss = 0.0009539434686303139
iteration 113, loss = 0.001008913153782487
iteration 114, loss = 0.0008561671711504459
iteration 115, loss = 0.0017807406838983297
iteration 116, loss = 0.0009527544607408345
iteration 117, loss = 0.0008738150354474783
iteration 118, loss = 0.0008388764108531177
iteration 119, loss = 0.001114950980991125
iteration 120, loss = 0.0016236904775723815
iteration 121, loss = 0.0009266260894946754
iteration 122, loss = 0.0016776053234934807
iteration 123, loss = 0.001066876808181405
iteration 124, loss = 0.0009137362940236926
iteration 125, loss = 0.0009004411986097693
iteration 126, loss = 0.0007040128111839294
iteration 127, loss = 0.0008122519357129931
iteration 128, loss = 0.0007921457290649414
iteration 129, loss = 0.0008474405622109771
iteration 130, loss = 0.0008979220292530954
iteration 131, loss = 0.0008430775487795472
iteration 132, loss = 0.0007770205847918987
iteration 133, loss = 0.000799757894128561
iteration 134, loss = 0.0007107165874913335
iteration 135, loss = 0.0008897986845113337
iteration 136, loss = 0.0011744184885174036
iteration 137, loss = 0.0007457234896719456
iteration 138, loss = 0.0008433676557615399
iteration 139, loss = 0.0008464273996651173
iteration 140, loss = 0.0007473622099496424
iteration 141, loss = 0.0008978177793323994
iteration 142, loss = 0.0008076726808212698
iteration 143, loss = 0.0012537750881165266
iteration 144, loss = 0.0009676865302026272
iteration 145, loss = 0.0007669121259823442
iteration 146, loss = 0.0010133153991773725
iteration 147, loss = 0.002523043891415
iteration 148, loss = 0.0012139617465436459
iteration 149, loss = 0.0008005701238289475
iteration 150, loss = 0.0010013814317062497
iteration 151, loss = 0.0012529061641544104
iteration 152, loss = 0.0011043366976082325
iteration 153, loss = 0.0009615475428290665
iteration 154, loss = 0.0014539576368406415
iteration 155, loss = 0.000914246600586921
iteration 156, loss = 0.0008448350126855075
iteration 157, loss = 0.0008094884688034654
iteration 158, loss = 0.0008748536929488182
iteration 159, loss = 0.0007725710747763515
iteration 160, loss = 0.001102037844248116
iteration 161, loss = 0.0009273721370846033
iteration 162, loss = 0.0007975291227921844
iteration 163, loss = 0.0008461533579975367
iteration 164, loss = 0.0008148137712851167
iteration 165, loss = 0.0012809530599042773
iteration 166, loss = 0.0014087683521211147
iteration 167, loss = 0.0010304374154657125
iteration 168, loss = 0.0007820483879186213
iteration 169, loss = 0.0010292800143361092
iteration 170, loss = 0.0006879609427414834
iteration 171, loss = 0.0010902623180299997
iteration 172, loss = 0.0011485490249469876
iteration 173, loss = 0.0008073970093391836
iteration 174, loss = 0.0008592345402576029
iteration 175, loss = 0.0006209425046108663
iteration 176, loss = 0.001084977644495666
iteration 177, loss = 0.0011129049817100167
iteration 178, loss = 0.000862289103679359
iteration 179, loss = 0.0008764001540839672
iteration 180, loss = 0.001121450331993401
iteration 181, loss = 0.0009855686221271753
iteration 182, loss = 0.000999940326437354
iteration 183, loss = 0.0009464556351304054
iteration 184, loss = 0.000707734958268702
iteration 185, loss = 0.0006419128621928394
iteration 186, loss = 0.000774150132201612
iteration 187, loss = 0.001023434684611857
iteration 188, loss = 0.0009310396271757782
iteration 189, loss = 0.000995576148852706
iteration 190, loss = 0.001593819702975452
iteration 191, loss = 0.0007754224934615195
iteration 192, loss = 0.0007953443564474583
iteration 193, loss = 0.0008040494285523891
iteration 194, loss = 0.0015200066845864058
iteration 195, loss = 0.000990932690910995
iteration 196, loss = 0.0009316239738836884
iteration 197, loss = 0.0008148928172886372
iteration 198, loss = 0.001192971132695675
iteration 199, loss = 0.0007746976334601641
iteration 200, loss = 0.002349176211282611
iteration 201, loss = 0.001061363727785647
iteration 202, loss = 0.0011387445265427232
iteration 203, loss = 0.0008744252845644951
iteration 204, loss = 0.0006709741428494453
iteration 205, loss = 0.000968466978520155
iteration 206, loss = 0.0008069861214607954
iteration 207, loss = 0.0012654319871217012
iteration 208, loss = 0.00078684336040169
iteration 209, loss = 0.0007217115489766002
iteration 210, loss = 0.001238145399838686
iteration 211, loss = 0.0011092619970440865
iteration 212, loss = 0.0011640862794592977
iteration 213, loss = 0.0011033756891265512
iteration 214, loss = 0.0008395716431550682
iteration 215, loss = 0.0009927849750965834
iteration 216, loss = 0.0013980105286464095
iteration 217, loss = 0.0017200738657265902
iteration 218, loss = 0.0008247043006122112
iteration 219, loss = 0.0008302252972498536
iteration 220, loss = 0.0009343433775939047
iteration 221, loss = 0.0009800175903365016
iteration 222, loss = 0.0022378326393663883
iteration 223, loss = 0.0008065683650784194
iteration 224, loss = 0.0007226945599541068
iteration 225, loss = 0.0009151438716799021
iteration 226, loss = 0.0010868565877899528
iteration 227, loss = 0.0013992000604048371
iteration 228, loss = 0.0007891099667176604
iteration 229, loss = 0.0010590431047603488
iteration 230, loss = 0.0008681309991516173
iteration 231, loss = 0.000762073032092303
iteration 232, loss = 0.0009997261222451925
iteration 233, loss = 0.0009264337713830173
iteration 234, loss = 0.0008405997068621218
iteration 235, loss = 0.0017120124539360404
iteration 236, loss = 0.0010659554973244667
iteration 237, loss = 0.0008972830837592483
iteration 238, loss = 0.0012214083690196276
iteration 239, loss = 0.0009630328859202564
iteration 240, loss = 0.0008424347033724189
iteration 241, loss = 0.001222909428179264
iteration 242, loss = 0.000884899462107569
iteration 243, loss = 0.0015397584065794945
iteration 244, loss = 0.0008416638011112809
iteration 245, loss = 0.0008962195133790374
iteration 246, loss = 0.0009065014310181141
iteration 247, loss = 0.001087744953110814
iteration 248, loss = 0.0007259294507093728
iteration 249, loss = 0.0015234991442412138
iteration 250, loss = 0.000854947604238987
iteration 251, loss = 0.0009703789255581796
iteration 252, loss = 0.0008446255815215409
iteration 253, loss = 0.000780015136115253
iteration 254, loss = 0.0007773147663101554
iteration 255, loss = 0.0015162121271714568
iteration 256, loss = 0.0007579566445201635
iteration 257, loss = 0.001141982153058052
iteration 258, loss = 0.0008225160418078303
iteration 259, loss = 0.0009372340864501894
iteration 260, loss = 0.0008965419256128371
iteration 261, loss = 0.0009395802626386285
iteration 262, loss = 0.000844200374558568
iteration 263, loss = 0.00082664709771052
iteration 264, loss = 0.0007635674555785954
iteration 265, loss = 0.0008555749664083123
iteration 266, loss = 0.0007161237299442291
iteration 267, loss = 0.0008011459140107036
iteration 268, loss = 0.000852443277835846
iteration 269, loss = 0.0008937333477661014
iteration 270, loss = 0.0009254648466594517
iteration 271, loss = 0.0007183997659012675
iteration 272, loss = 0.0011367825791239738
iteration 273, loss = 0.001512853428721428
iteration 274, loss = 0.0008741093333810568
iteration 275, loss = 0.0008806315017864108
iteration 276, loss = 0.0016723420703783631
iteration 277, loss = 0.0011932553024962544
iteration 278, loss = 0.0008035810897126794
iteration 279, loss = 0.0007679453701712191
iteration 280, loss = 0.0011961162090301514
iteration 281, loss = 0.0008618024876341224
iteration 282, loss = 0.0008159377612173557
iteration 283, loss = 0.0022688675671815872
iteration 284, loss = 0.0008788449340499938
iteration 285, loss = 0.0006908287177793682
iteration 286, loss = 0.0008773151785135269
iteration 287, loss = 0.0007465101662091911
iteration 288, loss = 0.0009401360875926912
iteration 289, loss = 0.000897476333193481
iteration 290, loss = 0.0010021963389590383
iteration 291, loss = 0.0008381151128560305
iteration 292, loss = 0.0007342008175328374
iteration 293, loss = 0.0008106525056064129
iteration 294, loss = 0.0008084748988039792
iteration 295, loss = 0.0011402243981137872
iteration 296, loss = 0.0008857156499288976
iteration 297, loss = 0.0009368868777528405
iteration 298, loss = 0.001191514078527689
iteration 299, loss = 0.0009461555746383965
iteration 300, loss = 0.000737339083570987
iteration 1, loss = 0.0008045814465731382
iteration 2, loss = 0.0011210798984393477
iteration 3, loss = 0.0008779384079389274
iteration 4, loss = 0.0007545413100160658
iteration 5, loss = 0.0006820284761488438
iteration 6, loss = 0.00077923818025738
iteration 7, loss = 0.0008479692623950541
iteration 8, loss = 0.0007406239747069776
iteration 9, loss = 0.0008313960861414671
iteration 10, loss = 0.000773687323089689
iteration 11, loss = 0.0009647193946875632
iteration 12, loss = 0.0008729171240702271
iteration 13, loss = 0.0013645283179357648
iteration 14, loss = 0.0008124621817842126
iteration 15, loss = 0.000759686401579529
iteration 16, loss = 0.0008966528112068772
iteration 17, loss = 0.0007686780882067978
iteration 18, loss = 0.0009783819550648332
iteration 19, loss = 0.0007965210825204849
iteration 20, loss = 0.0011035032803192735
iteration 21, loss = 0.000918352568987757
iteration 22, loss = 0.0015836203237995505
iteration 23, loss = 0.0008553740917704999
iteration 24, loss = 0.0010091445874422789
iteration 25, loss = 0.0007960498915053904
iteration 26, loss = 0.0007308768108487129
iteration 27, loss = 0.00107665138784796
iteration 28, loss = 0.0006420613499358296
iteration 29, loss = 0.0008610010263510048
iteration 30, loss = 0.0009512068354524672
iteration 31, loss = 0.0015117418952286243
iteration 32, loss = 0.0008081154082901776
iteration 33, loss = 0.0007525222608819604
iteration 34, loss = 0.0006984504288993776
iteration 35, loss = 0.0007821345934644341
iteration 36, loss = 0.001425200141966343
iteration 37, loss = 0.001166852773167193
iteration 38, loss = 0.0012519056908786297
iteration 39, loss = 0.0008703127969056368
iteration 40, loss = 0.0009965921053662896
iteration 41, loss = 0.0008615045808255672
iteration 42, loss = 0.0008630012744106352
iteration 43, loss = 0.0007459461339749396
iteration 44, loss = 0.0012402229476720095
iteration 45, loss = 0.0008551454520784318
iteration 46, loss = 0.0022488352842628956
iteration 47, loss = 0.0011852513998746872
iteration 48, loss = 0.0008305342053063214
iteration 49, loss = 0.0008859460358507931
iteration 50, loss = 0.00100353779271245
iteration 51, loss = 0.001593860681168735
iteration 52, loss = 0.0010165428975597024
iteration 53, loss = 0.001382230082526803
iteration 54, loss = 0.0013333994429558516
iteration 55, loss = 0.0007892135763540864
iteration 56, loss = 0.000967369123827666
iteration 57, loss = 0.001636693486943841
iteration 58, loss = 0.0014614438405260444
iteration 59, loss = 0.000774029060266912
iteration 60, loss = 0.0008230609237216413
iteration 61, loss = 0.0009297319338656962
iteration 62, loss = 0.0008218690636567771
iteration 63, loss = 0.0008267175871878862
iteration 64, loss = 0.0008169698994606733
iteration 65, loss = 0.0008377382182516158
iteration 66, loss = 0.0008494195062667131
iteration 67, loss = 0.0008198461728170514
iteration 68, loss = 0.00158541405107826
iteration 69, loss = 0.0007392856059595942
iteration 70, loss = 0.0008656817954033613
iteration 71, loss = 0.000982007128186524
iteration 72, loss = 0.0007485910900868475
iteration 73, loss = 0.0009175328887067735
iteration 74, loss = 0.0009880192810669541
iteration 75, loss = 0.0018084945622831583
iteration 76, loss = 0.0009330037864856422
iteration 77, loss = 0.0008102909196168184
iteration 78, loss = 0.0014794376911595464
iteration 79, loss = 0.0008965155575424433
iteration 80, loss = 0.0008876481442712247
iteration 81, loss = 0.0007630505133420229
iteration 82, loss = 0.0011447332799434662
iteration 83, loss = 0.0007631966145709157
iteration 84, loss = 0.0009892672533169389
iteration 85, loss = 0.000990152359008789
iteration 86, loss = 0.0010083894012495875
iteration 87, loss = 0.0011039194650948048
iteration 88, loss = 0.0009615242597647011
iteration 89, loss = 0.0008065772708505392
iteration 90, loss = 0.002069435315206647
iteration 91, loss = 0.00179941370151937
iteration 92, loss = 0.0010960075305774808
iteration 93, loss = 0.001012261025607586
iteration 94, loss = 0.0008642174652777612
iteration 95, loss = 0.001033363281749189
iteration 96, loss = 0.0008077473030425608
iteration 97, loss = 0.0007946899859234691
iteration 98, loss = 0.001031017629429698
iteration 99, loss = 0.0007565915584564209
iteration 100, loss = 0.000730959523934871
iteration 101, loss = 0.0009182473295368254
iteration 102, loss = 0.0007662972202524543
iteration 103, loss = 0.0009323421400040388
iteration 104, loss = 0.0009338875534012914
iteration 105, loss = 0.00156625104136765
iteration 106, loss = 0.0008634297992102802
iteration 107, loss = 0.001753716031089425
iteration 108, loss = 0.001128485077060759
iteration 109, loss = 0.0010804414050653577
iteration 110, loss = 0.0008652000688016415
iteration 111, loss = 0.0008482965058647096
iteration 112, loss = 0.0007570217130705714
iteration 113, loss = 0.000862443121150136
iteration 114, loss = 0.0008776008035056293
iteration 115, loss = 0.0012367404997348785
iteration 116, loss = 0.0009535872959531844
iteration 117, loss = 0.0007598320953547955
iteration 118, loss = 0.0008604077738709748
iteration 119, loss = 0.0017148035112768412
iteration 120, loss = 0.0008027864969335496
iteration 121, loss = 0.0011523612774908543
iteration 122, loss = 0.0011609952198341489
iteration 123, loss = 0.0008410436566919088
iteration 124, loss = 0.0009167809621430933
iteration 125, loss = 0.0012609228724613786
iteration 126, loss = 0.001256240182556212
iteration 127, loss = 0.0007357735303230584
iteration 128, loss = 0.0012913707178086042
iteration 129, loss = 0.0008678148733451962
iteration 130, loss = 0.0008793392917141318
iteration 131, loss = 0.000844215857796371
iteration 132, loss = 0.0007768624927848577
iteration 133, loss = 0.0008740933262743056
iteration 134, loss = 0.0008778885239735246
iteration 135, loss = 0.0008409286383539438
iteration 136, loss = 0.0011136719258502126
iteration 137, loss = 0.000741737661883235
iteration 138, loss = 0.000877256621606648
iteration 139, loss = 0.0012483603786677122
iteration 140, loss = 0.0007919465424492955
iteration 141, loss = 0.0009702598908916116
iteration 142, loss = 0.0012750116875395179
iteration 143, loss = 0.0012755650095641613
iteration 144, loss = 0.0012056819396093488
iteration 145, loss = 0.0011537548853084445
iteration 146, loss = 0.0009166259551420808
iteration 147, loss = 0.0008367590489797294
iteration 148, loss = 0.0010428441455587745
iteration 149, loss = 0.0009383313008584082
iteration 150, loss = 0.0008084857254289091
iteration 151, loss = 0.0006786320591345429
iteration 152, loss = 0.0007088459096848965
iteration 153, loss = 0.0009235522011294961
iteration 154, loss = 0.0007990466547198594
iteration 155, loss = 0.0011237944709137082
iteration 156, loss = 0.0006787502788938582
iteration 157, loss = 0.000888694659806788
iteration 158, loss = 0.001130522578023374
iteration 159, loss = 0.0008043886628001928
iteration 160, loss = 0.0009999989997595549
iteration 161, loss = 0.0007098266505636275
iteration 162, loss = 0.000833626720122993
iteration 163, loss = 0.000857722363434732
iteration 164, loss = 0.0007269609486684203
iteration 165, loss = 0.0009133700514212251
iteration 166, loss = 0.0012472167145460844
iteration 167, loss = 0.0013919130433350801
iteration 168, loss = 0.0009276301716454327
iteration 169, loss = 0.0008519204566255212
iteration 170, loss = 0.0020126327872276306
iteration 171, loss = 0.0007788058719597757
iteration 172, loss = 0.0008534908993169665
iteration 173, loss = 0.0012876546243205667
iteration 174, loss = 0.0007977823843248188
iteration 175, loss = 0.0010860897600650787
iteration 176, loss = 0.0012381980195641518
iteration 177, loss = 0.0007459758198820055
iteration 178, loss = 0.0007300361176021397
iteration 179, loss = 0.0010190929751843214
iteration 180, loss = 0.0013656605733558536
iteration 181, loss = 0.0007316733244806528
iteration 182, loss = 0.0010695180390030146
iteration 183, loss = 0.0010646204464137554
iteration 184, loss = 0.0008106074528768659
iteration 185, loss = 0.0011335087474435568
iteration 186, loss = 0.0012091398239135742
iteration 187, loss = 0.000960390199907124
iteration 188, loss = 0.0008483483106829226
iteration 189, loss = 0.0007681961287744343
iteration 190, loss = 0.0013009378453716636
iteration 191, loss = 0.0009423802839592099
iteration 192, loss = 0.001112514641135931
iteration 193, loss = 0.000783847994171083
iteration 194, loss = 0.0017995963571593165
iteration 195, loss = 0.0010619215900078416
iteration 196, loss = 0.0011071425396949053
iteration 197, loss = 0.000762242591008544
iteration 198, loss = 0.0008118669502437115
iteration 199, loss = 0.0007788424263708293
iteration 200, loss = 0.001076537650078535
iteration 201, loss = 0.0007882193312980235
iteration 202, loss = 0.0016349791549146175
iteration 203, loss = 0.0011723166098818183
iteration 204, loss = 0.001563575817272067
iteration 205, loss = 0.0011530733900144696
iteration 206, loss = 0.0006628152332268655
iteration 207, loss = 0.0020609756466001272
iteration 208, loss = 0.0008862830582074821
iteration 209, loss = 0.0010844016214832664
iteration 210, loss = 0.0007979099173098803
iteration 211, loss = 0.0008065950823947787
iteration 212, loss = 0.0008093598880805075
iteration 213, loss = 0.0007565125706605613
iteration 214, loss = 0.00124347151722759
iteration 215, loss = 0.0007784930639900267
iteration 216, loss = 0.0013857701560482383
iteration 217, loss = 0.0009425261523574591
iteration 218, loss = 0.0008226894424296916
iteration 219, loss = 0.0011295512085780501
iteration 220, loss = 0.001335478387773037
iteration 221, loss = 0.000734051747713238
iteration 222, loss = 0.0008385003311559558
iteration 223, loss = 0.0017399531789124012
iteration 224, loss = 0.0010856686858460307
iteration 225, loss = 0.0008165595354512334
iteration 226, loss = 0.0009401560528203845
iteration 227, loss = 0.0017920028185471892
iteration 228, loss = 0.0007953933672979474
iteration 229, loss = 0.0017681134631857276
iteration 230, loss = 0.0009901428129523993
iteration 231, loss = 0.0008626377093605697
iteration 232, loss = 0.0009587366366758943
iteration 233, loss = 0.00086831278167665
iteration 234, loss = 0.0011104356963187456
iteration 235, loss = 0.0011082309065386653
iteration 236, loss = 0.0008659550221636891
iteration 237, loss = 0.0010566238779574633
iteration 238, loss = 0.0008099134429357946
iteration 239, loss = 0.0008191366796381772
iteration 240, loss = 0.0009444181923754513
iteration 241, loss = 0.0007270837668329477
iteration 242, loss = 0.000729065272025764
iteration 243, loss = 0.0013638640521094203
iteration 244, loss = 0.0009179773624055088
iteration 245, loss = 0.0007115094922482967
iteration 246, loss = 0.0008648215443827212
iteration 247, loss = 0.0007758363499306142
iteration 248, loss = 0.000842399662360549
iteration 249, loss = 0.0013541068183258176
iteration 250, loss = 0.0008570262580178678
iteration 251, loss = 0.0006514717824757099
iteration 252, loss = 0.0007530934526585042
iteration 253, loss = 0.0008589827921241522
iteration 254, loss = 0.0008440971141681075
iteration 255, loss = 0.0006917911232449114
iteration 256, loss = 0.0007597655639983714
iteration 257, loss = 0.0012327447766438127
iteration 258, loss = 0.0007670661434531212
iteration 259, loss = 0.0008760385680943727
iteration 260, loss = 0.0009292452596127987
iteration 261, loss = 0.0007171512115746737
iteration 262, loss = 0.0011543480213731527
iteration 263, loss = 0.0008137787808664143
iteration 264, loss = 0.0008491607732139528
iteration 265, loss = 0.001452347612939775
iteration 266, loss = 0.0008514998480677605
iteration 267, loss = 0.0007752776728011668
iteration 268, loss = 0.0009347493760287762
iteration 269, loss = 0.0008058202220126987
iteration 270, loss = 0.0010351680684834719
iteration 271, loss = 0.0011572977527976036
iteration 272, loss = 0.0008557677501812577
iteration 273, loss = 0.0009930450469255447
iteration 274, loss = 0.0007519120699726045
iteration 275, loss = 0.0009284390253014863
iteration 276, loss = 0.0008171313093043864
iteration 277, loss = 0.000885207555256784
iteration 278, loss = 0.000886848196387291
iteration 279, loss = 0.0010340638691559434
iteration 280, loss = 0.0008905597496777773
iteration 281, loss = 0.000994921545498073
iteration 282, loss = 0.0007742060115560889
iteration 283, loss = 0.0008462035330012441
iteration 284, loss = 0.0008431667229160666
iteration 285, loss = 0.0011885823914781213
iteration 286, loss = 0.0008402637904509902
iteration 287, loss = 0.0009233260061591864
iteration 288, loss = 0.0011748799588531256
iteration 289, loss = 0.00197072746232152
iteration 290, loss = 0.0008325226372107863
iteration 291, loss = 0.0006354132201522589
iteration 292, loss = 0.0007114622858352959
iteration 293, loss = 0.0007751008961349726
iteration 294, loss = 0.0009237586637027562
iteration 295, loss = 0.0009847813053056598
iteration 296, loss = 0.0008645751513540745
iteration 297, loss = 0.000714916386641562
iteration 298, loss = 0.0010952191660180688
iteration 299, loss = 0.0007204795838333666
iteration 300, loss = 0.0008697761222720146
iteration 1, loss = 0.0007977866334840655
iteration 2, loss = 0.0011127081234008074
iteration 3, loss = 0.0007736515253782272
iteration 4, loss = 0.0009708977304399014
iteration 5, loss = 0.0007494895253330469
iteration 6, loss = 0.0016960925422608852
iteration 7, loss = 0.0006974994321353734
iteration 8, loss = 0.0007997240172699094
iteration 9, loss = 0.0011980083072558045
iteration 10, loss = 0.000687083404045552
iteration 11, loss = 0.0013104637619107962
iteration 12, loss = 0.0017238713335245848
iteration 13, loss = 0.000988723593764007
iteration 14, loss = 0.0009134866995736957
iteration 15, loss = 0.0007512200390920043
iteration 16, loss = 0.0008878348162397742
iteration 17, loss = 0.0007822978659532964
iteration 18, loss = 0.0008356919861398637
iteration 19, loss = 0.0008567959303036332
iteration 20, loss = 0.0009042525780387223
iteration 21, loss = 0.0010147563880309463
iteration 22, loss = 0.000790444144513458
iteration 23, loss = 0.001143948407843709
iteration 24, loss = 0.0008382335654459894
iteration 25, loss = 0.000725791382137686
iteration 26, loss = 0.0008098130929283798
iteration 27, loss = 0.0011231525568291545
iteration 28, loss = 0.0010000644251704216
iteration 29, loss = 0.0007449394324794412
iteration 30, loss = 0.0007448240066878498
iteration 31, loss = 0.0009834517259150743
iteration 32, loss = 0.0009580186451785266
iteration 33, loss = 0.0008357581682503223
iteration 34, loss = 0.0008951883064582944
iteration 35, loss = 0.0010675544617697597
iteration 36, loss = 0.0008654366247355938
iteration 37, loss = 0.000653282564599067
iteration 38, loss = 0.0008596574771218002
iteration 39, loss = 0.0007042629295028746
iteration 40, loss = 0.0008121401187963784
iteration 41, loss = 0.0009757285006344318
iteration 42, loss = 0.0012726059649139643
iteration 43, loss = 0.000906606437638402
iteration 44, loss = 0.0016221278347074986
iteration 45, loss = 0.0008676316356286407
iteration 46, loss = 0.000908892834559083
iteration 47, loss = 0.0006943490589037538
iteration 48, loss = 0.0009718941291794181
iteration 49, loss = 0.0008282513008452952
iteration 50, loss = 0.0008698826422914863
iteration 51, loss = 0.0008389760041609406
iteration 52, loss = 0.0010542853269726038
iteration 53, loss = 0.0008952108910307288
iteration 54, loss = 0.0008425238193012774
iteration 55, loss = 0.0007715714164078236
iteration 56, loss = 0.0017839432694017887
iteration 57, loss = 0.0011986172758042812
iteration 58, loss = 0.0007622407865710557
iteration 59, loss = 0.0008002151735126972
iteration 60, loss = 0.0008941801497712731
iteration 61, loss = 0.0009375267545692623
iteration 62, loss = 0.0009911165107041597
iteration 63, loss = 0.0007198249222710729
iteration 64, loss = 0.000890171155333519
iteration 65, loss = 0.0009993709390982985
iteration 66, loss = 0.0009640597272664309
iteration 67, loss = 0.0008557855617254972
iteration 68, loss = 0.0007712646620348096
iteration 69, loss = 0.001356345135718584
iteration 70, loss = 0.0007437406457029283
iteration 71, loss = 0.0006905185291543603
iteration 72, loss = 0.0017684951890259981
iteration 73, loss = 0.0008766073151491582
iteration 74, loss = 0.0006321060354821384
iteration 75, loss = 0.0006475442787632346
iteration 76, loss = 0.0008588935597799718
iteration 77, loss = 0.0006712981848977506
iteration 78, loss = 0.0010031252168118954
iteration 79, loss = 0.0010775383561849594
iteration 80, loss = 0.0008430105517618358
iteration 81, loss = 0.000899529317393899
iteration 82, loss = 0.000963452854193747
iteration 83, loss = 0.0010421605547890067
iteration 84, loss = 0.0008006260031834245
iteration 85, loss = 0.0011211296077817678
iteration 86, loss = 0.0008954485529102385
iteration 87, loss = 0.0007479098276235163
iteration 88, loss = 0.0010645166039466858
iteration 89, loss = 0.0008041459368541837
iteration 90, loss = 0.0008346461690962315
iteration 91, loss = 0.0010159681551158428
iteration 92, loss = 0.0008171171066351235
iteration 93, loss = 0.0011215107515454292
iteration 94, loss = 0.0010052039287984371
iteration 95, loss = 0.0007786579080857337
iteration 96, loss = 0.0009286497952416539
iteration 97, loss = 0.0008986264001578093
iteration 98, loss = 0.0014566193567588925
iteration 99, loss = 0.000851298391353339
iteration 100, loss = 0.0011707426747307181
iteration 101, loss = 0.001036023604683578
iteration 102, loss = 0.0010320039000362158
iteration 103, loss = 0.0009802397107705474
iteration 104, loss = 0.0009026817861013114
iteration 105, loss = 0.000879175728186965
iteration 106, loss = 0.0009000412537716329
iteration 107, loss = 0.0009061297168955207
iteration 108, loss = 0.000971181143540889
iteration 109, loss = 0.0007804137421771884
iteration 110, loss = 0.0010870007099583745
iteration 111, loss = 0.000866565213073045
iteration 112, loss = 0.0010500772623345256
iteration 113, loss = 0.0007859535980969667
iteration 114, loss = 0.0008932291530072689
iteration 115, loss = 0.0011650376254692674
iteration 116, loss = 0.0010108054848387837
iteration 117, loss = 0.0007064033998176455
iteration 118, loss = 0.0007852695416659117
iteration 119, loss = 0.001293692272156477
iteration 120, loss = 0.0008795050671324134
iteration 121, loss = 0.0008266788208857179
iteration 122, loss = 0.0008903618436306715
iteration 123, loss = 0.0008251881808973849
iteration 124, loss = 0.001309162238612771
iteration 125, loss = 0.0008597962441854179
iteration 126, loss = 0.000774879998061806
iteration 127, loss = 0.0009824722073972225
iteration 128, loss = 0.0009351979824714363
iteration 129, loss = 0.0011370042338967323
iteration 130, loss = 0.0009466030169278383
iteration 131, loss = 0.0010039464104920626
iteration 132, loss = 0.0008300658082589507
iteration 133, loss = 0.0008720098994672298
iteration 134, loss = 0.0015510567463934422
iteration 135, loss = 0.0008490518084727228
iteration 136, loss = 0.0008940529078245163
iteration 137, loss = 0.0007499441271647811
iteration 138, loss = 0.0011932847555726767
iteration 139, loss = 0.0011003008112311363
iteration 140, loss = 0.0009882239392027259
iteration 141, loss = 0.0006874339887872338
iteration 142, loss = 0.001539488323032856
iteration 143, loss = 0.0008436572970822453
iteration 144, loss = 0.0010833231499418616
iteration 145, loss = 0.0006808886537328362
iteration 146, loss = 0.0008858286892063916
iteration 147, loss = 0.0008300453191623092
iteration 148, loss = 0.0011389153078198433
iteration 149, loss = 0.0017466609133407474
iteration 150, loss = 0.0009820305276662111
iteration 151, loss = 0.0010839547030627728
iteration 152, loss = 0.0011171868536621332
iteration 153, loss = 0.0008165740873664618
iteration 154, loss = 0.000908076879568398
iteration 155, loss = 0.0010055230231955647
iteration 156, loss = 0.0009682744275778532
iteration 157, loss = 0.000904891814570874
iteration 158, loss = 0.0011417020577937365
iteration 159, loss = 0.000756957451812923
iteration 160, loss = 0.000889981456566602
iteration 161, loss = 0.0007973542669788003
iteration 162, loss = 0.0014242480974644423
iteration 163, loss = 0.0007501403451897204
iteration 164, loss = 0.000773996755015105
iteration 165, loss = 0.0008463012636639178
iteration 166, loss = 0.0010007037781178951
iteration 167, loss = 0.000807200325652957
iteration 168, loss = 0.000791739032138139
iteration 169, loss = 0.001530851935967803
iteration 170, loss = 0.0012005764292553067
iteration 171, loss = 0.0009002513252198696
iteration 172, loss = 0.0008533018408343196
iteration 173, loss = 0.0008052171324379742
iteration 174, loss = 0.0011134156957268715
iteration 175, loss = 0.0007454374572262168
iteration 176, loss = 0.0009558210149407387
iteration 177, loss = 0.0008839559741318226
iteration 178, loss = 0.0008520438568666577
iteration 179, loss = 0.0016374790575355291
iteration 180, loss = 0.0015074166003614664
iteration 181, loss = 0.0012777346419170499
iteration 182, loss = 0.0008751784916967154
iteration 183, loss = 0.001057320972904563
iteration 184, loss = 0.0007918329210951924
iteration 185, loss = 0.000919794081710279
iteration 186, loss = 0.0008969401242211461
iteration 187, loss = 0.0008582431473769248
iteration 188, loss = 0.0010527370031923056
iteration 189, loss = 0.0006949116359464824
iteration 190, loss = 0.0008816691697575152
iteration 191, loss = 0.0023193275555968285
iteration 192, loss = 0.0008524668519385159
iteration 193, loss = 0.0009233818855136633
iteration 194, loss = 0.0017160801216959953
iteration 195, loss = 0.0009956607827916741
iteration 196, loss = 0.0008777055190876126
iteration 197, loss = 0.000872960954438895
iteration 198, loss = 0.0008486560545861721
iteration 199, loss = 0.001633414183743298
iteration 200, loss = 0.0008141847793012857
iteration 201, loss = 0.0007257468532770872
iteration 202, loss = 0.0009546898654662073
iteration 203, loss = 0.0010183468693867326
iteration 204, loss = 0.0024116882123053074
iteration 205, loss = 0.001479022903367877
iteration 206, loss = 0.0009035835973918438
iteration 207, loss = 0.0008500266703777015
iteration 208, loss = 0.0014831689186394215
iteration 209, loss = 0.0007333902176469564
iteration 210, loss = 0.0016453189309686422
iteration 211, loss = 0.0007316786213777959
iteration 212, loss = 0.0010084593668580055
iteration 213, loss = 0.0011138446861878037
iteration 214, loss = 0.0008520950796082616
iteration 215, loss = 0.0008583868620917201
iteration 216, loss = 0.0008259881869889796
iteration 217, loss = 0.0008301715133711696
iteration 218, loss = 0.0008090929477475584
iteration 219, loss = 0.0009201174834743142
iteration 220, loss = 0.0008275302243418992
iteration 221, loss = 0.0015992213739082217
iteration 222, loss = 0.0015959542943164706
iteration 223, loss = 0.0007475863676518202
iteration 224, loss = 0.0009711948223412037
iteration 225, loss = 0.0007302947342395782
iteration 226, loss = 0.001009553438052535
iteration 227, loss = 0.001364323077723384
iteration 228, loss = 0.00096426549134776
iteration 229, loss = 0.0006920370506122708
iteration 230, loss = 0.0009703651303425431
iteration 231, loss = 0.0007283019367605448
iteration 232, loss = 0.0008921677945181727
iteration 233, loss = 0.0009496354614384472
iteration 234, loss = 0.0006316083599813282
iteration 235, loss = 0.0007220612606033683
iteration 236, loss = 0.0007994489860720932
iteration 237, loss = 0.0010464508086442947
iteration 238, loss = 0.0007578638615086675
iteration 239, loss = 0.0008924224530346692
iteration 240, loss = 0.0007544200634583831
iteration 241, loss = 0.00116472109220922
iteration 242, loss = 0.0014696297002956271
iteration 243, loss = 0.0007099349750205874
iteration 244, loss = 0.0008320758934132755
iteration 245, loss = 0.0019753763917833567
iteration 246, loss = 0.0017960708355531096
iteration 247, loss = 0.000774193205870688
iteration 248, loss = 0.0007274200324900448
iteration 249, loss = 0.0009197107283398509
iteration 250, loss = 0.0010214190697297454
iteration 251, loss = 0.0015280288644134998
iteration 252, loss = 0.0014077271334826946
iteration 253, loss = 0.0008446422289125621
iteration 254, loss = 0.0008457585936412215
iteration 255, loss = 0.0008156273397617042
iteration 256, loss = 0.0008438103250227869
iteration 257, loss = 0.0016904574586078525
iteration 258, loss = 0.001251009525731206
iteration 259, loss = 0.0010173419723287225
iteration 260, loss = 0.0009843588341027498
iteration 261, loss = 0.0008412497118115425
iteration 262, loss = 0.0007604418788105249
iteration 263, loss = 0.0010779114672914147
iteration 264, loss = 0.0007819568272680044
iteration 265, loss = 0.0010264262091368437
iteration 266, loss = 0.0008003539405763149
iteration 267, loss = 0.0008396976045332849
iteration 268, loss = 0.0016633511986583471
iteration 269, loss = 0.0009133763960562646
iteration 270, loss = 0.001289153704419732
iteration 271, loss = 0.0012261094525456429
iteration 272, loss = 0.0009146872907876968
iteration 273, loss = 0.0008562321309000254
iteration 274, loss = 0.0007781836320646107
iteration 275, loss = 0.0007574631017632782
iteration 276, loss = 0.0009388683247379959
iteration 277, loss = 0.0008298743050545454
iteration 278, loss = 0.0009219261119142175
iteration 279, loss = 0.0010148625588044524
iteration 280, loss = 0.0008174542454071343
iteration 281, loss = 0.0007273303926922381
iteration 282, loss = 0.0007919347262941301
iteration 283, loss = 0.0010028757387772202
iteration 284, loss = 0.0008367457194253802
iteration 285, loss = 0.0009453082457184792
iteration 286, loss = 0.001851088018156588
iteration 287, loss = 0.000888907175976783
iteration 288, loss = 0.0008584917522966862
iteration 289, loss = 0.0013023392530158162
iteration 290, loss = 0.0008746539824642241
iteration 291, loss = 0.0009376327507197857
iteration 292, loss = 0.0014118555700406432
iteration 293, loss = 0.001674715313129127
iteration 294, loss = 0.0007896289462223649
iteration 295, loss = 0.0011223771143704653
iteration 296, loss = 0.0007489862618967891
iteration 297, loss = 0.0010825630743056536
iteration 298, loss = 0.0012189100962132215
iteration 299, loss = 0.0008946378948166966
iteration 300, loss = 0.0011478705564513803
iteration 1, loss = 0.0009031330700963736
iteration 2, loss = 0.0010184877319261432
iteration 3, loss = 0.0007940749637782574
iteration 4, loss = 0.0007246467284858227
iteration 5, loss = 0.001167004811577499
iteration 6, loss = 0.0010447438107803464
iteration 7, loss = 0.0008475584327243268
iteration 8, loss = 0.0010769705986604095
iteration 9, loss = 0.0012342826230451465
iteration 10, loss = 0.0009442690061405301
iteration 11, loss = 0.0010796693386510015
iteration 12, loss = 0.0009751346660777926
iteration 13, loss = 0.000739539391361177
iteration 14, loss = 0.0016489189583808184
iteration 15, loss = 0.0007948741549625993
iteration 16, loss = 0.0009064364712685347
iteration 17, loss = 0.0010320564033463597
iteration 18, loss = 0.0007612976478412747
iteration 19, loss = 0.0010130904847756028
iteration 20, loss = 0.0012704315595328808
iteration 21, loss = 0.0009626420796848834
iteration 22, loss = 0.0008202901226468384
iteration 23, loss = 0.0013898275792598724
iteration 24, loss = 0.0015237730694934726
iteration 25, loss = 0.000800964655354619
iteration 26, loss = 0.0015969902742654085
iteration 27, loss = 0.0006299943779595196
iteration 28, loss = 0.0010025386000052094
iteration 29, loss = 0.00079427968012169
iteration 30, loss = 0.00101672881282866
iteration 31, loss = 0.0008032641489990056
iteration 32, loss = 0.0009390218183398247
iteration 33, loss = 0.0007490902207791805
iteration 34, loss = 0.001785213127732277
iteration 35, loss = 0.0008066984009929001
iteration 36, loss = 0.0009864387102425098
iteration 37, loss = 0.0007174468482844532
iteration 38, loss = 0.001101313391700387
iteration 39, loss = 0.001022536656819284
iteration 40, loss = 0.0007235488155856729
iteration 41, loss = 0.0007277430850081146
iteration 42, loss = 0.0009351298213005066
iteration 43, loss = 0.0008379866485483944
iteration 44, loss = 0.0009839606937021017
iteration 45, loss = 0.0008462139521725476
iteration 46, loss = 0.0008254939457401633
iteration 47, loss = 0.0008829139405861497
iteration 48, loss = 0.0009606282692402601
iteration 49, loss = 0.0008448080043308437
iteration 50, loss = 0.0012893404345959425
iteration 51, loss = 0.0011280195321887732
iteration 52, loss = 0.0007641129777766764
iteration 53, loss = 0.0007905655656941235
iteration 54, loss = 0.0008585191681049764
iteration 55, loss = 0.0011117653921246529
iteration 56, loss = 0.0007482190849259496
iteration 57, loss = 0.0007529105641879141
iteration 58, loss = 0.0008278993191197515
iteration 59, loss = 0.001181340659968555
iteration 60, loss = 0.0007888259133324027
iteration 61, loss = 0.0010279417037963867
iteration 62, loss = 0.0008822046802379191
iteration 63, loss = 0.0006816640961915255
iteration 64, loss = 0.0008605703478679061
iteration 65, loss = 0.0010301950387656689
iteration 66, loss = 0.0012957623694092035
iteration 67, loss = 0.0008614181424491107
iteration 68, loss = 0.0008533524232916534
iteration 69, loss = 0.0009027888881973922
iteration 70, loss = 0.0011874668998643756
iteration 71, loss = 0.000956662290263921
iteration 72, loss = 0.0008172730449587107
iteration 73, loss = 0.0008609331562183797
iteration 74, loss = 0.001190851558931172
iteration 75, loss = 0.0009642535587772727
iteration 76, loss = 0.0009179804474115372
iteration 77, loss = 0.0010730901267379522
iteration 78, loss = 0.0007553256582468748
iteration 79, loss = 0.0007656767265871167
iteration 80, loss = 0.000839791027829051
iteration 81, loss = 0.001659648958593607
iteration 82, loss = 0.0007145670824684203
iteration 83, loss = 0.0010832054540514946
iteration 84, loss = 0.0012308950535953045
iteration 85, loss = 0.000784263713285327
iteration 86, loss = 0.0008906669681891799
iteration 87, loss = 0.0007602578261867166
iteration 88, loss = 0.0009209201671183109
iteration 89, loss = 0.0007274346426129341
iteration 90, loss = 0.0011998156551271677
iteration 91, loss = 0.0008804387762211263
iteration 92, loss = 0.001671215402893722
iteration 93, loss = 0.0007880771299824119
iteration 94, loss = 0.0007834836142137647
iteration 95, loss = 0.0008150144130922854
iteration 96, loss = 0.0008469198364764452
iteration 97, loss = 0.0011991059873253107
iteration 98, loss = 0.0006971531547605991
iteration 99, loss = 0.0019527983386069536
iteration 100, loss = 0.0008235161658376455
iteration 101, loss = 0.0007289872737601399
iteration 102, loss = 0.0009455217514187098
iteration 103, loss = 0.0008710450492799282
iteration 104, loss = 0.0007328057545237243
iteration 105, loss = 0.0008284699870273471
iteration 106, loss = 0.0012667933478951454
iteration 107, loss = 0.0016924269730225205
iteration 108, loss = 0.0007860582554712892
iteration 109, loss = 0.0009899201104417443
iteration 110, loss = 0.0007175271166488528
iteration 111, loss = 0.000904558168258518
iteration 112, loss = 0.0008933147764764726
iteration 113, loss = 0.0007741072331555188
iteration 114, loss = 0.0016626197611913085
iteration 115, loss = 0.0008335680468007922
iteration 116, loss = 0.0009108738740906119
iteration 117, loss = 0.001196342520415783
iteration 118, loss = 0.0007566196145489812
iteration 119, loss = 0.0010297255357727408
iteration 120, loss = 0.0008963955915533006
iteration 121, loss = 0.0007787496433593333
iteration 122, loss = 0.001223638653755188
iteration 123, loss = 0.000717533752322197
iteration 124, loss = 0.0017846482805907726
iteration 125, loss = 0.0007928802515380085
iteration 126, loss = 0.0009539701859466732
iteration 127, loss = 0.0009710610611364245
iteration 128, loss = 0.0007861497579142451
iteration 129, loss = 0.0007283855229616165
iteration 130, loss = 0.0009748749434947968
iteration 131, loss = 0.0014628652716055512
iteration 132, loss = 0.0007614202331751585
iteration 133, loss = 0.0011298267636448145
iteration 134, loss = 0.0017533453647047281
iteration 135, loss = 0.0007094688480719924
iteration 136, loss = 0.0008479647221975029
iteration 137, loss = 0.0006636172183789313
iteration 138, loss = 0.0007920429343357682
iteration 139, loss = 0.0008041980327107012
iteration 140, loss = 0.001425854628905654
iteration 141, loss = 0.0010328912176191807
iteration 142, loss = 0.0016606940189376473
iteration 143, loss = 0.0007656189845874906
iteration 144, loss = 0.0010644651483744383
iteration 145, loss = 0.0007617962546646595
iteration 146, loss = 0.0009569149697199464
iteration 147, loss = 0.0008031447068788111
iteration 148, loss = 0.0007573445327579975
iteration 149, loss = 0.0009722033282741904
iteration 150, loss = 0.0007878855103626847
iteration 151, loss = 0.0009801950072869658
iteration 152, loss = 0.0010771761881187558
iteration 153, loss = 0.0006703030085191131
iteration 154, loss = 0.0007953936001285911
iteration 155, loss = 0.0008741794154047966
iteration 156, loss = 0.001766093773767352
iteration 157, loss = 0.0008261810871772468
iteration 158, loss = 0.0009488381328992546
iteration 159, loss = 0.0013915007002651691
iteration 160, loss = 0.0011720049660652876
iteration 161, loss = 0.0009810677729547024
iteration 162, loss = 0.0011993126245215535
iteration 163, loss = 0.0014640294248238206
iteration 164, loss = 0.0008390619186684489
iteration 165, loss = 0.0009912137174978852
iteration 166, loss = 0.0011578177800402045
iteration 167, loss = 0.0008418364450335503
iteration 168, loss = 0.001612687250599265
iteration 169, loss = 0.0008071394404396415
iteration 170, loss = 0.0008547842735424638
iteration 171, loss = 0.0007945172837935388
iteration 172, loss = 0.0008277254528366029
iteration 173, loss = 0.0007691042264923453
iteration 174, loss = 0.0008138487464748323
iteration 175, loss = 0.0007014594739302993
iteration 176, loss = 0.0007508283597417176
iteration 177, loss = 0.0012965290807187557
iteration 178, loss = 0.0007733411621302366
iteration 179, loss = 0.000909183407202363
iteration 180, loss = 0.0008600980509072542
iteration 181, loss = 0.0008768301922827959
iteration 182, loss = 0.0009219310013577342
iteration 183, loss = 0.0007771719829179347
iteration 184, loss = 0.0010959869250655174
iteration 185, loss = 0.0009389037149958313
iteration 186, loss = 0.0008163746679201722
iteration 187, loss = 0.000764745578635484
iteration 188, loss = 0.0023433577734977007
iteration 189, loss = 0.0007973466999828815
iteration 190, loss = 0.0007256668759509921
iteration 191, loss = 0.0010430815163999796
iteration 192, loss = 0.0009262235835194588
iteration 193, loss = 0.0008619821164757013
iteration 194, loss = 0.0007490876014344394
iteration 195, loss = 0.0008325591334141791
iteration 196, loss = 0.0008809415157884359
iteration 197, loss = 0.0006782864802516997
iteration 198, loss = 0.0014473176561295986
iteration 199, loss = 0.0007224988657981157
iteration 200, loss = 0.0022112857550382614
iteration 201, loss = 0.0008934414363466203
iteration 202, loss = 0.0019508262630552053
iteration 203, loss = 0.0008195627015084028
iteration 204, loss = 0.0009585741790942848
iteration 205, loss = 0.0010872569400817156
iteration 206, loss = 0.000880397972650826
iteration 207, loss = 0.0007489843410439789
iteration 208, loss = 0.0010218751849606633
iteration 209, loss = 0.0016229572938755155
iteration 210, loss = 0.0009483263129368424
iteration 211, loss = 0.0008741474011912942
iteration 212, loss = 0.0011683912016451359
iteration 213, loss = 0.0010821566684171557
iteration 214, loss = 0.001181014347821474
iteration 215, loss = 0.0007927891565486789
iteration 216, loss = 0.0009576412267051637
iteration 217, loss = 0.0006847523618489504
iteration 218, loss = 0.0008966340101324022
iteration 219, loss = 0.001562546705827117
iteration 220, loss = 0.0009214075980708003
iteration 221, loss = 0.001430233009159565
iteration 222, loss = 0.0008602066081948578
iteration 223, loss = 0.000780120026320219
iteration 224, loss = 0.0008723048958927393
iteration 225, loss = 0.0009022126905620098
iteration 226, loss = 0.0009003570303320885
iteration 227, loss = 0.0007678273832425475
iteration 228, loss = 0.0017164091113954782
iteration 229, loss = 0.0007841309416107833
iteration 230, loss = 0.0007104789256118238
iteration 231, loss = 0.0009622401557862759
iteration 232, loss = 0.001286546466872096
iteration 233, loss = 0.0009717585053294897
iteration 234, loss = 0.0013888406101614237
iteration 235, loss = 0.0007509462302550673
iteration 236, loss = 0.0009493318502791226
iteration 237, loss = 0.000723079196177423
iteration 238, loss = 0.0008427351131103933
iteration 239, loss = 0.0008144667372107506
iteration 240, loss = 0.000754790788050741
iteration 241, loss = 0.001545824226923287
iteration 242, loss = 0.001747999805957079
iteration 243, loss = 0.0012383871944621205
iteration 244, loss = 0.000813425169326365
iteration 245, loss = 0.0010863714851439
iteration 246, loss = 0.0009187665418721735
iteration 247, loss = 0.0010428531095385551
iteration 248, loss = 0.0007921718643046916
iteration 249, loss = 0.000997486524283886
iteration 250, loss = 0.0014618142740800977
iteration 251, loss = 0.00081650196807459
iteration 252, loss = 0.0008963219588622451
iteration 253, loss = 0.0008663002518005669
iteration 254, loss = 0.0007410385878756642
iteration 255, loss = 0.000888755195774138
iteration 256, loss = 0.0007875230512581766
iteration 257, loss = 0.0007686862954869866
iteration 258, loss = 0.00074436713475734
iteration 259, loss = 0.0008708402747288346
iteration 260, loss = 0.0007435348816215992
iteration 261, loss = 0.0008327511604875326
iteration 262, loss = 0.0007266672910191119
iteration 263, loss = 0.000980187440291047
iteration 264, loss = 0.00108925043605268
iteration 265, loss = 0.0008066851296462119
iteration 266, loss = 0.0008469790336675942
iteration 267, loss = 0.0015629575354978442
iteration 268, loss = 0.0007160638342611492
iteration 269, loss = 0.0007567674620077014
iteration 270, loss = 0.0008678452577441931
iteration 271, loss = 0.0009067983482964337
iteration 272, loss = 0.0008041440160013735
iteration 273, loss = 0.0008660863386467099
iteration 274, loss = 0.000854382524266839
iteration 275, loss = 0.0009712892351672053
iteration 276, loss = 0.001992572331801057
iteration 277, loss = 0.0016980840591713786
iteration 278, loss = 0.00105567229911685
iteration 279, loss = 0.0008943976718001068
iteration 280, loss = 0.0008129968773573637
iteration 281, loss = 0.0006899150903336704
iteration 282, loss = 0.0008849189034663141
iteration 283, loss = 0.0007665216107852757
iteration 284, loss = 0.00166100705973804
iteration 285, loss = 0.0009633250301703811
iteration 286, loss = 0.0016863112105056643
iteration 287, loss = 0.0009411557693965733
iteration 288, loss = 0.0007779599400237203
iteration 289, loss = 0.0009588827379047871
iteration 290, loss = 0.0007451765704900026
iteration 291, loss = 0.000854825833812356
iteration 292, loss = 0.001102290814742446
iteration 293, loss = 0.0010031686397269368
iteration 294, loss = 0.0010479649063199759
iteration 295, loss = 0.001469045178964734
iteration 296, loss = 0.0007385088829323649
iteration 297, loss = 0.0008010133169591427
iteration 298, loss = 0.0010082542430609465
iteration 299, loss = 0.0009682922973297536
iteration 300, loss = 0.000802119611762464
iteration 1, loss = 0.00220464076846838
iteration 2, loss = 0.0007926577818579972
iteration 3, loss = 0.0016650689067319036
iteration 4, loss = 0.0018437743419781327
iteration 5, loss = 0.0008868714212439954
iteration 6, loss = 0.0009714110638014972
iteration 7, loss = 0.0008295811130665243
iteration 8, loss = 0.000773467356339097
iteration 9, loss = 0.0014786806423217058
iteration 10, loss = 0.001927657867781818
iteration 11, loss = 0.0010008825920522213
iteration 12, loss = 0.0007711958605796099
iteration 13, loss = 0.0011538671096786857
iteration 14, loss = 0.0008152847876772285
iteration 15, loss = 0.0008233556291088462
iteration 16, loss = 0.0007767659262754023
iteration 17, loss = 0.0007508586277253926
iteration 18, loss = 0.0008344960515387356
iteration 19, loss = 0.0011088012252002954
iteration 20, loss = 0.0008515722001902759
iteration 21, loss = 0.0011045951396226883
iteration 22, loss = 0.0011239267187193036
iteration 23, loss = 0.00164730753749609
iteration 24, loss = 0.001290742540732026
iteration 25, loss = 0.0008978659752756357
iteration 26, loss = 0.0007769678486511111
iteration 27, loss = 0.0008181667071767151
iteration 28, loss = 0.0008058791281655431
iteration 29, loss = 0.0009470743243582547
iteration 30, loss = 0.0009262184612452984
iteration 31, loss = 0.0007416228181682527
iteration 32, loss = 0.000765646283980459
iteration 33, loss = 0.0008967547910287976
iteration 34, loss = 0.0007885286468081176
iteration 35, loss = 0.0008031086763367057
iteration 36, loss = 0.0009615286253392696
iteration 37, loss = 0.000760281749535352
iteration 38, loss = 0.0008004444534890354
iteration 39, loss = 0.0016465524677187204
iteration 40, loss = 0.0010239339899271727
iteration 41, loss = 0.0007845222717151046
iteration 42, loss = 0.0012062233872711658
iteration 43, loss = 0.0014593068044632673
iteration 44, loss = 0.0007736373809166253
iteration 45, loss = 0.0010290560312569141
iteration 46, loss = 0.0008017310174182057
iteration 47, loss = 0.0007602892001159489
iteration 48, loss = 0.0009556317236274481
iteration 49, loss = 0.0008015059866011143
iteration 50, loss = 0.0009932232787832618
iteration 51, loss = 0.0008934590732678771
iteration 52, loss = 0.0010897867614403367
iteration 53, loss = 0.0011814743047580123
iteration 54, loss = 0.0009241261868737638
iteration 55, loss = 0.001403043163008988
iteration 56, loss = 0.0023477813228964806
iteration 57, loss = 0.0008394427713938057
iteration 58, loss = 0.0007720316643826663
iteration 59, loss = 0.001477657351642847
iteration 60, loss = 0.0011646394850686193
iteration 61, loss = 0.0008767322869971395
iteration 62, loss = 0.0011474555358290672
iteration 63, loss = 0.0008866353309713304
iteration 64, loss = 0.0007511163712479174
iteration 65, loss = 0.000963863916695118
iteration 66, loss = 0.0008226297795772552
iteration 67, loss = 0.00203684507869184
iteration 68, loss = 0.0007871483103372157
iteration 69, loss = 0.001640002359636128
iteration 70, loss = 0.0009016432450152934
iteration 71, loss = 0.0014320315094664693
iteration 72, loss = 0.0010493462905287743
iteration 73, loss = 0.0007895856397226453
iteration 74, loss = 0.0008369811112061143
iteration 75, loss = 0.0010835197754204273
iteration 76, loss = 0.0011842967942357063
iteration 77, loss = 0.0007253931253217161
iteration 78, loss = 0.0008671499090269208
iteration 79, loss = 0.0010386485373601317
iteration 80, loss = 0.000856841797940433
iteration 81, loss = 0.0010749029461294413
iteration 82, loss = 0.0009935657726600766
iteration 83, loss = 0.0007593047921545804
iteration 84, loss = 0.0012963953195139766
iteration 85, loss = 0.0010036169551312923
iteration 86, loss = 0.0007516638725064695
iteration 87, loss = 0.0008067853632383049
iteration 88, loss = 0.0013332886155694723
iteration 89, loss = 0.0010042381472885609
iteration 90, loss = 0.0009167025564238429
iteration 91, loss = 0.0008551093051210046
iteration 92, loss = 0.0010171987814828753
iteration 93, loss = 0.0012188307009637356
iteration 94, loss = 0.0007916924660094082
iteration 95, loss = 0.0012336734216660261
iteration 96, loss = 0.0007658782415091991
iteration 97, loss = 0.0007646420272067189
iteration 98, loss = 0.0009715005289763212
iteration 99, loss = 0.0013635087525472045
iteration 100, loss = 0.0009595954907126725
iteration 101, loss = 0.000713778252247721
iteration 102, loss = 0.0008647762006148696
iteration 103, loss = 0.0008947055321186781
iteration 104, loss = 0.0006924628978595138
iteration 105, loss = 0.001161932130344212
iteration 106, loss = 0.0015392338391393423
iteration 107, loss = 0.0007333597750402987
iteration 108, loss = 0.0012447384651750326
iteration 109, loss = 0.0009141817572526634
iteration 110, loss = 0.0007611246546730399
iteration 111, loss = 0.0008925285073928535
iteration 112, loss = 0.0008423833642154932
iteration 113, loss = 0.0007915840833447874
iteration 114, loss = 0.000822026573587209
iteration 115, loss = 0.0007979341899044812
iteration 116, loss = 0.0007027004612609744
iteration 117, loss = 0.0011407069396227598
iteration 118, loss = 0.0007504921522922814
iteration 119, loss = 0.0007348202052526176
iteration 120, loss = 0.0016362963942810893
iteration 121, loss = 0.0008863637340255082
iteration 122, loss = 0.000942509388551116
iteration 123, loss = 0.0011090118205174804
iteration 124, loss = 0.0010714128147810698
iteration 125, loss = 0.0010146951535716653
iteration 126, loss = 0.0006596552557311952
iteration 127, loss = 0.0007889775442890823
iteration 128, loss = 0.0007462181383743882
iteration 129, loss = 0.0008019939414225519
iteration 130, loss = 0.0009589062537997961
iteration 131, loss = 0.0007565682753920555
iteration 132, loss = 0.0012588651152327657
iteration 133, loss = 0.0010702792787924409
iteration 134, loss = 0.0009328598971478641
iteration 135, loss = 0.0008952907519415021
iteration 136, loss = 0.0011324930237606168
iteration 137, loss = 0.0008900225511752069
iteration 138, loss = 0.0007697134860791266
iteration 139, loss = 0.0007975545595400035
iteration 140, loss = 0.0008233264088630676
iteration 141, loss = 0.0009241051157005131
iteration 142, loss = 0.000733734923414886
iteration 143, loss = 0.0011869643349200487
iteration 144, loss = 0.0010851981351152062
iteration 145, loss = 0.0008895698701962829
iteration 146, loss = 0.0007782413740642369
iteration 147, loss = 0.0008048178860917687
iteration 148, loss = 0.0010046587558463216
iteration 149, loss = 0.001736801234073937
iteration 150, loss = 0.001257169060409069
iteration 151, loss = 0.0008641828899271786
iteration 152, loss = 0.0013048977125436068
iteration 153, loss = 0.0010778481373563409
iteration 154, loss = 0.0009942390024662018
iteration 155, loss = 0.0008565434254705906
iteration 156, loss = 0.0008617838029749691
iteration 157, loss = 0.0007993293693289161
iteration 158, loss = 0.0009716617641970515
iteration 159, loss = 0.0011930144391953945
iteration 160, loss = 0.000728958286345005
iteration 161, loss = 0.0007537806523032486
iteration 162, loss = 0.0007717867847532034
iteration 163, loss = 0.0008369318093173206
iteration 164, loss = 0.0006808678153902292
iteration 165, loss = 0.0017016398487612605
iteration 166, loss = 0.0011224555782973766
iteration 167, loss = 0.000683635356836021
iteration 168, loss = 0.00120780267752707
iteration 169, loss = 0.00113312853500247
iteration 170, loss = 0.0011331131681799889
iteration 171, loss = 0.0007713237428106368
iteration 172, loss = 0.0012671737931668758
iteration 173, loss = 0.0017604745225980878
iteration 174, loss = 0.0012446165783330798
iteration 175, loss = 0.001546681858599186
iteration 176, loss = 0.0007652967469766736
iteration 177, loss = 0.0008212279062718153
iteration 178, loss = 0.0010321018053218722
iteration 179, loss = 0.0009073223918676376
iteration 180, loss = 0.0007815920980647206
iteration 181, loss = 0.0007917139446362853
iteration 182, loss = 0.001456204685382545
iteration 183, loss = 0.0017579533159732819
iteration 184, loss = 0.0008101860876195133
iteration 185, loss = 0.0013199474196881056
iteration 186, loss = 0.0013905307278037071
iteration 187, loss = 0.0008632441167719662
iteration 188, loss = 0.0008137106779031456
iteration 189, loss = 0.0014640581794083118
iteration 190, loss = 0.0008821451920084655
iteration 191, loss = 0.0009315437637269497
iteration 192, loss = 0.0007711682701483369
iteration 193, loss = 0.0008590997895225883
iteration 194, loss = 0.0007709549972787499
iteration 195, loss = 0.0007556445780210197
iteration 196, loss = 0.0010610412573441863
iteration 197, loss = 0.00082410778850317
iteration 198, loss = 0.0008293813443742692
iteration 199, loss = 0.00073469546623528
iteration 200, loss = 0.0008287872187793255
iteration 201, loss = 0.0009887502528727055
iteration 202, loss = 0.0010719684651121497
iteration 203, loss = 0.0008428902365267277
iteration 204, loss = 0.000959905912168324
iteration 205, loss = 0.0010273901280015707
iteration 206, loss = 0.0010135285556316376
iteration 207, loss = 0.0008178396383300424
iteration 208, loss = 0.0010196110233664513
iteration 209, loss = 0.0008535622037015855
iteration 210, loss = 0.0009633010486140847
iteration 211, loss = 0.0008997568511404097
iteration 212, loss = 0.0008867827709764242
iteration 213, loss = 0.000631250673905015
iteration 214, loss = 0.0020272843539714813
iteration 215, loss = 0.0009907704079523683
iteration 216, loss = 0.0008766385726630688
iteration 217, loss = 0.0008056663791649044
iteration 218, loss = 0.001180508523248136
iteration 219, loss = 0.0007120242808014154
iteration 220, loss = 0.000806813535746187
iteration 221, loss = 0.0006822205032221973
iteration 222, loss = 0.0009815753437578678
iteration 223, loss = 0.0011247785296291113
iteration 224, loss = 0.001155827776528895
iteration 225, loss = 0.0007343943580053747
iteration 226, loss = 0.0007731706136837602
iteration 227, loss = 0.0006973002455197275
iteration 228, loss = 0.0017043175175786018
iteration 229, loss = 0.000847766175866127
iteration 230, loss = 0.0016060065245255828
iteration 231, loss = 0.0008031922043301165
iteration 232, loss = 0.0009074967820197344
iteration 233, loss = 0.000990634667687118
iteration 234, loss = 0.0013320866273716092
iteration 235, loss = 0.0012231084983795881
iteration 236, loss = 0.0007829388487152755
iteration 237, loss = 0.0009025874314829707
iteration 238, loss = 0.0009287678985856473
iteration 239, loss = 0.0008227265207096934
iteration 240, loss = 0.0010780695592984557
iteration 241, loss = 0.0008957689278759062
iteration 242, loss = 0.00091101776342839
iteration 243, loss = 0.00072017329512164
iteration 244, loss = 0.0010612336918711662
iteration 245, loss = 0.0016746849287301302
iteration 246, loss = 0.0010201958939433098
iteration 247, loss = 0.0008766924147494137
iteration 248, loss = 0.0009181535569950938
iteration 249, loss = 0.0012080767191946507
iteration 250, loss = 0.0010219815885648131
iteration 251, loss = 0.000797992805019021
iteration 252, loss = 0.000857607345096767
iteration 253, loss = 0.0008027225267142057
iteration 254, loss = 0.0007789632072672248
iteration 255, loss = 0.0008194763795472682
iteration 256, loss = 0.0016459228936582804
iteration 257, loss = 0.0009186752140522003
iteration 258, loss = 0.0008356186444871128
iteration 259, loss = 0.0011358823394402862
iteration 260, loss = 0.0008638758445158601
iteration 261, loss = 0.000828128366265446
iteration 262, loss = 0.0006555599393323064
iteration 263, loss = 0.0009361124248243868
iteration 264, loss = 0.0007557881763204932
iteration 265, loss = 0.0010601149406284094
iteration 266, loss = 0.0009474244434386492
iteration 267, loss = 0.000663331535179168
iteration 268, loss = 0.0009484837064519525
iteration 269, loss = 0.0013049421831965446
iteration 270, loss = 0.0010473434813320637
iteration 271, loss = 0.0007285827305167913
iteration 272, loss = 0.0008130695205181837
iteration 273, loss = 0.0007464982336387038
iteration 274, loss = 0.0007194457575678825
iteration 275, loss = 0.0010505556128919125
iteration 276, loss = 0.0008077616221271455
iteration 277, loss = 0.0008365774992853403
iteration 278, loss = 0.0008265345823019743
iteration 279, loss = 0.000898522324860096
iteration 280, loss = 0.000786797609180212
iteration 281, loss = 0.0009288621949963272
iteration 282, loss = 0.0011865126434713602
iteration 283, loss = 0.0008359099738299847
iteration 284, loss = 0.0010635687503963709
iteration 285, loss = 0.0009072748362086713
iteration 286, loss = 0.0011222901521250606
iteration 287, loss = 0.0007510435534641147
iteration 288, loss = 0.000851741642691195
iteration 289, loss = 0.0010697052348405123
iteration 290, loss = 0.000900637824088335
iteration 291, loss = 0.0007346633356064558
iteration 292, loss = 0.0007097264169715345
iteration 293, loss = 0.0007927456172183156
iteration 294, loss = 0.0007608374580740929
iteration 295, loss = 0.0009090648381970823
iteration 296, loss = 0.0009347675950266421
iteration 297, loss = 0.0009311033063568175
iteration 298, loss = 0.000786219781730324
iteration 299, loss = 0.0009346383158117533
iteration 300, loss = 0.0008214005501940846
iteration 1, loss = 0.0010617465013638139
iteration 2, loss = 0.0012536965077742934
iteration 3, loss = 0.0009456676780246198
iteration 4, loss = 0.0011215704726055264
iteration 5, loss = 0.0008435750496573746
iteration 6, loss = 0.0012902338057756424
iteration 7, loss = 0.0008618074934929609
iteration 8, loss = 0.0007440969930030406
iteration 9, loss = 0.0012022533919662237
iteration 10, loss = 0.0008140786667354405
iteration 11, loss = 0.0008725544321350753
iteration 12, loss = 0.0010090107098221779
iteration 13, loss = 0.0008483852725476027
iteration 14, loss = 0.0009087376529350877
iteration 15, loss = 0.0007838519522920251
iteration 16, loss = 0.0010658320970833302
iteration 17, loss = 0.000901097315363586
iteration 18, loss = 0.0009149473626166582
iteration 19, loss = 0.0013800853630527854
iteration 20, loss = 0.001213815063238144
iteration 21, loss = 0.0008262422634288669
iteration 22, loss = 0.0013596350327134132
iteration 23, loss = 0.0015152201522141695
iteration 24, loss = 0.000840330554638058
iteration 25, loss = 0.0007850622641853988
iteration 26, loss = 0.001241937279701233
iteration 27, loss = 0.000999918789602816
iteration 28, loss = 0.0008455712231807411
iteration 29, loss = 0.0007900535711087286
iteration 30, loss = 0.0007580493111163378
iteration 31, loss = 0.0009182463982142508
iteration 32, loss = 0.0008885391289368272
iteration 33, loss = 0.0010848899837583303
iteration 34, loss = 0.0010878158500418067
iteration 35, loss = 0.0009519940358586609
iteration 36, loss = 0.0016579246148467064
iteration 37, loss = 0.0008419142104685307
iteration 38, loss = 0.0007087145349942148
iteration 39, loss = 0.0008466450381092727
iteration 40, loss = 0.0008624278707429767
iteration 41, loss = 0.0009984425269067287
iteration 42, loss = 0.0007581001264043152
iteration 43, loss = 0.0015543699264526367
iteration 44, loss = 0.0012243841774761677
iteration 45, loss = 0.0007806856883689761
iteration 46, loss = 0.0007270672358572483
iteration 47, loss = 0.0015854834346100688
iteration 48, loss = 0.0014846442500129342
iteration 49, loss = 0.0006759815732948482
iteration 50, loss = 0.001318264869041741
iteration 51, loss = 0.0020610997453331947
iteration 52, loss = 0.0010840906761586666
iteration 53, loss = 0.0015288881259039044
iteration 54, loss = 0.0016506114043295383
iteration 55, loss = 0.0011292797280475497
iteration 56, loss = 0.0007953542517498136
iteration 57, loss = 0.0007920241914689541
iteration 58, loss = 0.0007294343668036163
iteration 59, loss = 0.0010827488731592894
iteration 60, loss = 0.0015110118547454476
iteration 61, loss = 0.0007497785263694823
iteration 62, loss = 0.0008052878547459841
iteration 63, loss = 0.0009017321281135082
iteration 64, loss = 0.00080555968452245
iteration 65, loss = 0.0009332349291071296
iteration 66, loss = 0.0016097917687147856
iteration 67, loss = 0.0006675331969745457
iteration 68, loss = 0.0008380155777558684
iteration 69, loss = 0.0008457993972115219
iteration 70, loss = 0.00148700678255409
iteration 71, loss = 0.0011852202005684376
iteration 72, loss = 0.0006985165528021753
iteration 73, loss = 0.001802179031074047
iteration 74, loss = 0.0009623606456443667
iteration 75, loss = 0.0008787382976152003
iteration 76, loss = 0.0007614324567839503
iteration 77, loss = 0.0008001263486221433
iteration 78, loss = 0.0008936654776334763
iteration 79, loss = 0.0008178562857210636
iteration 80, loss = 0.0010999309597536922
iteration 81, loss = 0.0010152790928259492
iteration 82, loss = 0.0008936889353208244
iteration 83, loss = 0.0014166590990498662
iteration 84, loss = 0.0009643115336075425
iteration 85, loss = 0.0008684213971719146
iteration 86, loss = 0.0009003219893202186
iteration 87, loss = 0.0007706154137849808
iteration 88, loss = 0.0008076241938397288
iteration 89, loss = 0.0009015629766508937
iteration 90, loss = 0.0008904524729587138
iteration 91, loss = 0.0008272725972346961
iteration 92, loss = 0.0013570297742262483
iteration 93, loss = 0.0008464234415441751
iteration 94, loss = 0.001727836555801332
iteration 95, loss = 0.0007316983537748456
iteration 96, loss = 0.000981889315880835
iteration 97, loss = 0.000865404203068465
iteration 98, loss = 0.000792811973951757
iteration 99, loss = 0.0010166988940909505
iteration 100, loss = 0.0009475721744820476
iteration 101, loss = 0.0008331551216542721
iteration 102, loss = 0.0007880195043981075
iteration 103, loss = 0.0017347625689581037
iteration 104, loss = 0.0008022860856726766
iteration 105, loss = 0.0019513230072334409
iteration 106, loss = 0.0008226525969803333
iteration 107, loss = 0.0008147479966282845
iteration 108, loss = 0.0008768191328272223
iteration 109, loss = 0.0007611357141286135
iteration 110, loss = 0.0008522017160430551
iteration 111, loss = 0.0017291010590270162
iteration 112, loss = 0.0007875126320868731
iteration 113, loss = 0.0007946474943310022
iteration 114, loss = 0.0008046082220971584
iteration 115, loss = 0.0008588510099798441
iteration 116, loss = 0.0008971104398369789
iteration 117, loss = 0.0011744691291823983
iteration 118, loss = 0.000875637517310679
iteration 119, loss = 0.0009352033375762403
iteration 120, loss = 0.0008718687458895147
iteration 121, loss = 0.0012864971067756414
iteration 122, loss = 0.0011409098515287042
iteration 123, loss = 0.0007599387317895889
iteration 124, loss = 0.0007689642370678484
iteration 125, loss = 0.0009428380872122943
iteration 126, loss = 0.0008097983081825078
iteration 127, loss = 0.0008215346606448293
iteration 128, loss = 0.0012721612583845854
iteration 129, loss = 0.0007456907187588513
iteration 130, loss = 0.0008103327709250152
iteration 131, loss = 0.0014260878087952733
iteration 132, loss = 0.0014985218876972795
iteration 133, loss = 0.0011675322894006968
iteration 134, loss = 0.001134249847382307
iteration 135, loss = 0.000650219910312444
iteration 136, loss = 0.0007028300315141678
iteration 137, loss = 0.0009241195512004197
iteration 138, loss = 0.0012731777969747782
iteration 139, loss = 0.0008157786214724183
iteration 140, loss = 0.0008250640821643174
iteration 141, loss = 0.0007845638319849968
iteration 142, loss = 0.0014969788026064634
iteration 143, loss = 0.0011112734209746122
iteration 144, loss = 0.000730599625967443
iteration 145, loss = 0.0008798943599686027
iteration 146, loss = 0.0008795917383395135
iteration 147, loss = 0.0009143995121121407
iteration 148, loss = 0.0006864159950055182
iteration 149, loss = 0.0008275194559246302
iteration 150, loss = 0.0010478724725544453
iteration 151, loss = 0.0008686057990416884
iteration 152, loss = 0.00084737129509449
iteration 153, loss = 0.0008296581218019128
iteration 154, loss = 0.0006527045043185353
iteration 155, loss = 0.0009060989832505584
iteration 156, loss = 0.0008160147117450833
iteration 157, loss = 0.0010272939689457417
iteration 158, loss = 0.0007766212220303714
iteration 159, loss = 0.0008659418090246618
iteration 160, loss = 0.0007492261356674135
iteration 161, loss = 0.000804621260613203
iteration 162, loss = 0.0008517821552231908
iteration 163, loss = 0.0007408641977235675
iteration 164, loss = 0.0009404139127582312
iteration 165, loss = 0.0011618349235504866
iteration 166, loss = 0.0009892683010548353
iteration 167, loss = 0.000843382382299751
iteration 168, loss = 0.0009330954635515809
iteration 169, loss = 0.000766088895034045
iteration 170, loss = 0.0008216861169785261
iteration 171, loss = 0.000887958100065589
iteration 172, loss = 0.0009841842111200094
iteration 173, loss = 0.0008036319632083178
iteration 174, loss = 0.0009756226791068912
iteration 175, loss = 0.0011215673293918371
iteration 176, loss = 0.0017044518608599901
iteration 177, loss = 0.0009641487267799675
iteration 178, loss = 0.0011937281815335155
iteration 179, loss = 0.0010178597876802087
iteration 180, loss = 0.0008405061671510339
iteration 181, loss = 0.0010348600335419178
iteration 182, loss = 0.0006568331737071276
iteration 183, loss = 0.00093584053684026
iteration 184, loss = 0.0008972202194854617
iteration 185, loss = 0.0008068765746429563
iteration 186, loss = 0.0010724407620728016
iteration 187, loss = 0.0010046056704595685
iteration 188, loss = 0.0009040436707437038
iteration 189, loss = 0.0011776022147387266
iteration 190, loss = 0.0007492529111914337
iteration 191, loss = 0.0010858413297683
iteration 192, loss = 0.0007319567957893014
iteration 193, loss = 0.0012472477974370122
iteration 194, loss = 0.0008737309835851192
iteration 195, loss = 0.0006893700920045376
iteration 196, loss = 0.0008361074142158031
iteration 197, loss = 0.0007405452197417617
iteration 198, loss = 0.0011296187294647098
iteration 199, loss = 0.0007662855787202716
iteration 200, loss = 0.00120835832785815
iteration 201, loss = 0.0007650291081517935
iteration 202, loss = 0.0013295846292749047
iteration 203, loss = 0.0008883142727427185
iteration 204, loss = 0.0008075747755356133
iteration 205, loss = 0.0009586081141605973
iteration 206, loss = 0.0008706587832421064
iteration 207, loss = 0.0008197772549465299
iteration 208, loss = 0.0012153858551755548
iteration 209, loss = 0.0009951167739927769
iteration 210, loss = 0.0008559569250792265
iteration 211, loss = 0.0012201998615637422
iteration 212, loss = 0.0009115823195315897
iteration 213, loss = 0.0008664414053782821
iteration 214, loss = 0.0009711551247164607
iteration 215, loss = 0.0007444616639986634
iteration 216, loss = 0.0007553958566859365
iteration 217, loss = 0.001036294037476182
iteration 218, loss = 0.0015705727273598313
iteration 219, loss = 0.0016272759530693293
iteration 220, loss = 0.0009744028793647885
iteration 221, loss = 0.0007085843244567513
iteration 222, loss = 0.0009079362498596311
iteration 223, loss = 0.0006524581112898886
iteration 224, loss = 0.001052111736498773
iteration 225, loss = 0.0008577185217291117
iteration 226, loss = 0.0009585487423464656
iteration 227, loss = 0.0008415365591645241
iteration 228, loss = 0.0008071396732702851
iteration 229, loss = 0.0009856298565864563
iteration 230, loss = 0.0008770889835432172
iteration 231, loss = 0.0009293209295719862
iteration 232, loss = 0.001503078849054873
iteration 233, loss = 0.0007754390826448798
iteration 234, loss = 0.000853816862218082
iteration 235, loss = 0.0006917789578437805
iteration 236, loss = 0.0008848583092913032
iteration 237, loss = 0.0009676504414528608
iteration 238, loss = 0.0008622516761533916
iteration 239, loss = 0.000870104122441262
iteration 240, loss = 0.0008790463325567544
iteration 241, loss = 0.0008508146274834871
iteration 242, loss = 0.0008283614879474044
iteration 243, loss = 0.0011132345534861088
iteration 244, loss = 0.0016925870440900326
iteration 245, loss = 0.0007522667292505503
iteration 246, loss = 0.0008713679271750152
iteration 247, loss = 0.0012265334371477365
iteration 248, loss = 0.0008316552848555148
iteration 249, loss = 0.000954576360527426
iteration 250, loss = 0.0008638989529572427
iteration 251, loss = 0.0007183370180428028
iteration 252, loss = 0.0008534759981557727
iteration 253, loss = 0.0009699077345430851
iteration 254, loss = 0.0019125298131257296
iteration 255, loss = 0.0007878600154072046
iteration 256, loss = 0.0008257320150732994
iteration 257, loss = 0.000724215351510793
iteration 258, loss = 0.0017168844351544976
iteration 259, loss = 0.0010731915244832635
iteration 260, loss = 0.0008664678316563368
iteration 261, loss = 0.0008292578277178109
iteration 262, loss = 0.0006957109435461462
iteration 263, loss = 0.0011762829963117838
iteration 264, loss = 0.0008280129404738545
iteration 265, loss = 0.000739437818992883
iteration 266, loss = 0.0012204023078083992
iteration 267, loss = 0.000677164236549288
iteration 268, loss = 0.0007742284215055406
iteration 269, loss = 0.0008749201660975814
iteration 270, loss = 0.0010099333012476563
iteration 271, loss = 0.0008910461911000311
iteration 272, loss = 0.0009146974189206958
iteration 273, loss = 0.0007897547911852598
iteration 274, loss = 0.0012479770230129361
iteration 275, loss = 0.0009231994044966996
iteration 276, loss = 0.0019764313474297523
iteration 277, loss = 0.0011220398591831326
iteration 278, loss = 0.0012065754272043705
iteration 279, loss = 0.0014069023309275508
iteration 280, loss = 0.0015651002759113908
iteration 281, loss = 0.0011676334543153644
iteration 282, loss = 0.0010848994133993983
iteration 283, loss = 0.0012497449060902
iteration 284, loss = 0.0007772071985527873
iteration 285, loss = 0.0010304164607077837
iteration 286, loss = 0.0008864359697327018
iteration 287, loss = 0.0009610811830498278
iteration 288, loss = 0.0008303075446747243
iteration 289, loss = 0.001016759779304266
iteration 290, loss = 0.000706858467310667
iteration 291, loss = 0.0008888785960152745
iteration 292, loss = 0.0007531116716563702
iteration 293, loss = 0.0010005931835621595
iteration 294, loss = 0.0007381745963357389
iteration 295, loss = 0.001455529942177236
iteration 296, loss = 0.0009985057404264808
iteration 297, loss = 0.0009538437006995082
iteration 298, loss = 0.000843917194288224
iteration 299, loss = 0.0008146565523929894
iteration 300, loss = 0.0009710245649330318
iteration 1, loss = 0.0008312258287332952
iteration 2, loss = 0.001429956522770226
iteration 3, loss = 0.0005535056698136032
iteration 4, loss = 0.0009796159574761987
iteration 5, loss = 0.0008965631714090705
iteration 6, loss = 0.0007972888415679336
iteration 7, loss = 0.0012216144241392612
iteration 8, loss = 0.0008296563173644245
iteration 9, loss = 0.0009483206085860729
iteration 10, loss = 0.000983941019512713
iteration 11, loss = 0.0008576011168770492
iteration 12, loss = 0.000734249537345022
iteration 13, loss = 0.0007267959299497306
iteration 14, loss = 0.0008999367710202932
iteration 15, loss = 0.0017334895674139261
iteration 16, loss = 0.0008019062806852162
iteration 17, loss = 0.0007787977228872478
iteration 18, loss = 0.0009219322819262743
iteration 19, loss = 0.0007838660385459661
iteration 20, loss = 0.0007609471213072538
iteration 21, loss = 0.000778731016907841
iteration 22, loss = 0.0008462310070171952
iteration 23, loss = 0.0010971433948725462
iteration 24, loss = 0.0009162223432213068
iteration 25, loss = 0.0008795714820735157
iteration 26, loss = 0.0006704775732941926
iteration 27, loss = 0.0008579829009249806
iteration 28, loss = 0.0023780425544828176
iteration 29, loss = 0.0008614243124611676
iteration 30, loss = 0.0007702435250394046
iteration 31, loss = 0.0008372195297852159
iteration 32, loss = 0.0011056518414989114
iteration 33, loss = 0.0007857818272896111
iteration 34, loss = 0.0007923380471765995
iteration 35, loss = 0.0010614244965836406
iteration 36, loss = 0.0008201166056096554
iteration 37, loss = 0.0009532351978123188
iteration 38, loss = 0.000980823184363544
iteration 39, loss = 0.0008796100155450404
iteration 40, loss = 0.0008438907680101693
iteration 41, loss = 0.000678019248880446
iteration 42, loss = 0.0008140867576003075
iteration 43, loss = 0.0006785633158870041
iteration 44, loss = 0.0008689239621162415
iteration 45, loss = 0.0008077899110503495
iteration 46, loss = 0.0009820798877626657
iteration 47, loss = 0.0008012836333364248
iteration 48, loss = 0.0007616060902364552
iteration 49, loss = 0.0009381833369843662
iteration 50, loss = 0.0010071296710520983
iteration 51, loss = 0.0008820167859084904
iteration 52, loss = 0.0010015792213380337
iteration 53, loss = 0.0016665522707626224
iteration 54, loss = 0.0008047404699027538
iteration 55, loss = 0.0010459368349984288
iteration 56, loss = 0.0007327073835767806
iteration 57, loss = 0.0009797703241929412
iteration 58, loss = 0.0008146868785843253
iteration 59, loss = 0.0007307484629563987
iteration 60, loss = 0.001308317412622273
iteration 61, loss = 0.0008444648701697588
iteration 62, loss = 0.0008417800418101251
iteration 63, loss = 0.0007802092586643994
iteration 64, loss = 0.0010365191847085953
iteration 65, loss = 0.001440875930711627
iteration 66, loss = 0.0008084886358119547
iteration 67, loss = 0.001086026313714683
iteration 68, loss = 0.0010934931924566627
iteration 69, loss = 0.0008557740948162973
iteration 70, loss = 0.0008155304240062833
iteration 71, loss = 0.0009926273487508297
iteration 72, loss = 0.0013001648476347327
iteration 73, loss = 0.001717059756629169
iteration 74, loss = 0.0009367374004796147
iteration 75, loss = 0.0009002946317195892
iteration 76, loss = 0.0011830388102680445
iteration 77, loss = 0.0008562050061300397
iteration 78, loss = 0.0007444798829965293
iteration 79, loss = 0.0007496520411223173
iteration 80, loss = 0.0008750775014050305
iteration 81, loss = 0.0009256753255613148
iteration 82, loss = 0.0008123657898977399
iteration 83, loss = 0.0008716271840967238
iteration 84, loss = 0.0006674004253000021
iteration 85, loss = 0.0009716025670059025
iteration 86, loss = 0.0008945307927206159
iteration 87, loss = 0.0015065259067341685
iteration 88, loss = 0.0016723846783861518
iteration 89, loss = 0.0007300653378479183
iteration 90, loss = 0.0009671224397607148
iteration 91, loss = 0.0010012760758399963
iteration 92, loss = 0.0007691290229558945
iteration 93, loss = 0.0008922258275561035
iteration 94, loss = 0.0018112370744347572
iteration 95, loss = 0.0009019866120070219
iteration 96, loss = 0.0009656152687966824
iteration 97, loss = 0.0007867448148317635
iteration 98, loss = 0.0009412185754626989
iteration 99, loss = 0.0007178160594776273
iteration 100, loss = 0.0009732707985676825
iteration 101, loss = 0.0018165984656661749
iteration 102, loss = 0.0008520639967173338
iteration 103, loss = 0.000870145158842206
iteration 104, loss = 0.0007569403387606144
iteration 105, loss = 0.001006535952910781
iteration 106, loss = 0.0007777242572046816
iteration 107, loss = 0.0009205406531691551
iteration 108, loss = 0.0011952186468988657
iteration 109, loss = 0.0009357787785120308
iteration 110, loss = 0.0010559067595750093
iteration 111, loss = 0.0008380193612538278
iteration 112, loss = 0.001652183011174202
iteration 113, loss = 0.0012622543144971132
iteration 114, loss = 0.0011452243197709322
iteration 115, loss = 0.0014281991170719266
iteration 116, loss = 0.001176395919173956
iteration 117, loss = 0.002079533878713846
iteration 118, loss = 0.0009761729161255062
iteration 119, loss = 0.0017438504146412015
iteration 120, loss = 0.0009835758246481419
iteration 121, loss = 0.0008023236878216267
iteration 122, loss = 0.001120212604291737
iteration 123, loss = 0.0008013667538762093
iteration 124, loss = 0.0007259616977535188
iteration 125, loss = 0.0007620779215358198
iteration 126, loss = 0.0010748914210125804
iteration 127, loss = 0.0011140437563881278
iteration 128, loss = 0.0009382135467603803
iteration 129, loss = 0.000773136445786804
iteration 130, loss = 0.0008043700945563614
iteration 131, loss = 0.0010488717816770077
iteration 132, loss = 0.0007470772252418101
iteration 133, loss = 0.0009091306710615754
iteration 134, loss = 0.001097910339012742
iteration 135, loss = 0.0009185534436255693
iteration 136, loss = 0.0010667670285329223
iteration 137, loss = 0.0009682133095338941
iteration 138, loss = 0.0007767038769088686
iteration 139, loss = 0.0007307952037081122
iteration 140, loss = 0.0008022769470699131
iteration 141, loss = 0.001571255037561059
iteration 142, loss = 0.0011245207861065865
iteration 143, loss = 0.0007516209152527153
iteration 144, loss = 0.0009152300772257149
iteration 145, loss = 0.0008172832895070314
iteration 146, loss = 0.0007299945573322475
iteration 147, loss = 0.0007753950194455683
iteration 148, loss = 0.0011140428250655532
iteration 149, loss = 0.001288435305468738
iteration 150, loss = 0.0007520520593971014
iteration 151, loss = 0.0009413608349859715
iteration 152, loss = 0.000843249203171581
iteration 153, loss = 0.001165925757959485
iteration 154, loss = 0.00092368945479393
iteration 155, loss = 0.0009791363263502717
iteration 156, loss = 0.0006978397723287344
iteration 157, loss = 0.0008048057788982987
iteration 158, loss = 0.0011182792950421572
iteration 159, loss = 0.0007634956855326891
iteration 160, loss = 0.0008585071773268282
iteration 161, loss = 0.000989374821074307
iteration 162, loss = 0.0009309985907748342
iteration 163, loss = 0.0008041170658543706
iteration 164, loss = 0.0007525342516601086
iteration 165, loss = 0.0006793034262955189
iteration 166, loss = 0.0014999810373410583
iteration 167, loss = 0.0009056264534592628
iteration 168, loss = 0.0007147492142394185
iteration 169, loss = 0.0008607829222455621
iteration 170, loss = 0.0012323991395533085
iteration 171, loss = 0.000810460012871772
iteration 172, loss = 0.000664259830955416
iteration 173, loss = 0.0010059120832011104
iteration 174, loss = 0.000824919028673321
iteration 175, loss = 0.0010537109337747097
iteration 176, loss = 0.0008007861324585974
iteration 177, loss = 0.0012670622672885656
iteration 178, loss = 0.0012033050879836082
iteration 179, loss = 0.0008025523275136948
iteration 180, loss = 0.0011370743159204721
iteration 181, loss = 0.0015065443003550172
iteration 182, loss = 0.000742959906347096
iteration 183, loss = 0.0018136288272216916
iteration 184, loss = 0.0008045912836678326
iteration 185, loss = 0.0010063016088679433
iteration 186, loss = 0.000786054355558008
iteration 187, loss = 0.0011243144981563091
iteration 188, loss = 0.000934033072553575
iteration 189, loss = 0.0016567704733461142
iteration 190, loss = 0.0008955266675911844
iteration 191, loss = 0.0008766150567680597
iteration 192, loss = 0.000842939829453826
iteration 193, loss = 0.0006949996459297836
iteration 194, loss = 0.0011407400015741587
iteration 195, loss = 0.0009613620932213962
iteration 196, loss = 0.0007992058526724577
iteration 197, loss = 0.0008427893044427037
iteration 198, loss = 0.0006764212739653885
iteration 199, loss = 0.000876914185937494
iteration 200, loss = 0.0010649606119841337
iteration 201, loss = 0.0009547435911372304
iteration 202, loss = 0.0007078839698806405
iteration 203, loss = 0.0007152564357966185
iteration 204, loss = 0.0007056532194837928
iteration 205, loss = 0.0010321353329345584
iteration 206, loss = 0.0014424531254917383
iteration 207, loss = 0.0008963205618783832
iteration 208, loss = 0.0014842133969068527
iteration 209, loss = 0.0014605313772335649
iteration 210, loss = 0.0011209469521418214
iteration 211, loss = 0.0010820513125509024
iteration 212, loss = 0.0013845991343259811
iteration 213, loss = 0.0007811258547008038
iteration 214, loss = 0.0009488661889918149
iteration 215, loss = 0.0007988014258444309
iteration 216, loss = 0.0006706214626319706
iteration 217, loss = 0.0008302959613502026
iteration 218, loss = 0.0011238767765462399
iteration 219, loss = 0.0011096870293840766
iteration 220, loss = 0.0011385934194549918
iteration 221, loss = 0.001898937625810504
iteration 222, loss = 0.0007722105365246534
iteration 223, loss = 0.0013225243892520666
iteration 224, loss = 0.0009688553400337696
iteration 225, loss = 0.0009088519727811217
iteration 226, loss = 0.0008997464319691062
iteration 227, loss = 0.0008264315547421575
iteration 228, loss = 0.0006768914172425866
iteration 229, loss = 0.0009711960447020829
iteration 230, loss = 0.0013576182536780834
iteration 231, loss = 0.001570713589899242
iteration 232, loss = 0.0011358318151906133
iteration 233, loss = 0.0008306923555210233
iteration 234, loss = 0.0007224555010907352
iteration 235, loss = 0.0008343749213963747
iteration 236, loss = 0.0009191574063152075
iteration 237, loss = 0.0008035343489609659
iteration 238, loss = 0.0008201352320611477
iteration 239, loss = 0.0017069056630134583
iteration 240, loss = 0.0007681590504944324
iteration 241, loss = 0.0007361015304923058
iteration 242, loss = 0.0010394571581855416
iteration 243, loss = 0.0008436760399490595
iteration 244, loss = 0.0007821937324479222
iteration 245, loss = 0.0015590868424624205
iteration 246, loss = 0.0008537610992789268
iteration 247, loss = 0.001358697423711419
iteration 248, loss = 0.00126441172324121
iteration 249, loss = 0.0009713781764730811
iteration 250, loss = 0.0010520024225115776
iteration 251, loss = 0.0009235772886313498
iteration 252, loss = 0.0014632128877565265
iteration 253, loss = 0.0008231367683038116
iteration 254, loss = 0.000802098133135587
iteration 255, loss = 0.0006889139185659587
iteration 256, loss = 0.0010678250109776855
iteration 257, loss = 0.0009387777536176145
iteration 258, loss = 0.001022604526951909
iteration 259, loss = 0.000856251863297075
iteration 260, loss = 0.0008984683081507683
iteration 261, loss = 0.000916296907234937
iteration 262, loss = 0.0008237408474087715
iteration 263, loss = 0.0007136713829822838
iteration 264, loss = 0.000941883772611618
iteration 265, loss = 0.0014834010507911444
iteration 266, loss = 0.0008708587847650051
iteration 267, loss = 0.0007090092985890806
iteration 268, loss = 0.0007792190299369395
iteration 269, loss = 0.001243000035174191
iteration 270, loss = 0.0009627394611015916
iteration 271, loss = 0.0010833764681592584
iteration 272, loss = 0.0009534640121273696
iteration 273, loss = 0.0010261847637593746
iteration 274, loss = 0.0008321524946950376
iteration 275, loss = 0.0008639202569611371
iteration 276, loss = 0.0011223764158785343
iteration 277, loss = 0.0010727071203291416
iteration 278, loss = 0.00100091309286654
iteration 279, loss = 0.0011540509294718504
iteration 280, loss = 0.0014386636903509498
iteration 281, loss = 0.0007181729306466877
iteration 282, loss = 0.0008977587567642331
iteration 283, loss = 0.0019235658692196012
iteration 284, loss = 0.0013300286373123527
iteration 285, loss = 0.0008618110441602767
iteration 286, loss = 0.0008426689309999347
iteration 287, loss = 0.000826587958727032
iteration 288, loss = 0.001086368807591498
iteration 289, loss = 0.0009908174397423863
iteration 290, loss = 0.0010279309935867786
iteration 291, loss = 0.0018257168121635914
iteration 292, loss = 0.0007650862680748105
iteration 293, loss = 0.0008538513211533427
iteration 294, loss = 0.0007256789831444621
iteration 295, loss = 0.0007582971011288464
iteration 296, loss = 0.0010194327915087342
iteration 297, loss = 0.001100356807000935
iteration 298, loss = 0.0009306650608778
iteration 299, loss = 0.0009301197715103626
iteration 300, loss = 0.0006942539475858212
iteration 1, loss = 0.000788939360063523
iteration 2, loss = 0.00080293562496081
iteration 3, loss = 0.0012809521285817027
iteration 4, loss = 0.0009676784393377602
iteration 5, loss = 0.0008245994104072452
iteration 6, loss = 0.000749681144952774
iteration 7, loss = 0.0010461771162226796
iteration 8, loss = 0.00088228948879987
iteration 9, loss = 0.0008396927150897682
iteration 10, loss = 0.0011582610895857215
iteration 11, loss = 0.001032456406392157
iteration 12, loss = 0.0008130126516334713
iteration 13, loss = 0.0008315268787555397
iteration 14, loss = 0.0008030541357584298
iteration 15, loss = 0.0007131044985726476
iteration 16, loss = 0.0010244823060929775
iteration 17, loss = 0.0008839439251460135
iteration 18, loss = 0.0008644689805805683
iteration 19, loss = 0.0009880800498649478
iteration 20, loss = 0.0009721934329718351
iteration 21, loss = 0.0007349177030846477
iteration 22, loss = 0.0015572920674458146
iteration 23, loss = 0.0007347461651079357
iteration 24, loss = 0.0009499741718173027
iteration 25, loss = 0.0011681488249450922
iteration 26, loss = 0.0007555722841061652
iteration 27, loss = 0.0008179594296962023
iteration 28, loss = 0.000994232832454145
iteration 29, loss = 0.0006859495188109577
iteration 30, loss = 0.0008716349839232862
iteration 31, loss = 0.0009479409200139344
iteration 32, loss = 0.0007072742446325719
iteration 33, loss = 0.0010201014811173081
iteration 34, loss = 0.0008818843052722514
iteration 35, loss = 0.0008368275593966246
iteration 36, loss = 0.000847733928821981
iteration 37, loss = 0.001584029057994485
iteration 38, loss = 0.0009089884115383029
iteration 39, loss = 0.0010398849844932556
iteration 40, loss = 0.000842124514747411
iteration 41, loss = 0.0009113868582062423
iteration 42, loss = 0.0016417847946286201
iteration 43, loss = 0.0009136631852015853
iteration 44, loss = 0.0010010518599301577
iteration 45, loss = 0.0011278368765488267
iteration 46, loss = 0.0011157640255987644
iteration 47, loss = 0.0011438975343480706
iteration 48, loss = 0.0009474814869463444
iteration 49, loss = 0.001657033571973443
iteration 50, loss = 0.001071751001290977
iteration 51, loss = 0.0008334550075232983
iteration 52, loss = 0.0008187411585822701
iteration 53, loss = 0.0008478789823129773
iteration 54, loss = 0.0008991011418402195
iteration 55, loss = 0.001060323091223836
iteration 56, loss = 0.0009104926139116287
iteration 57, loss = 0.0008437232463620603
iteration 58, loss = 0.0008094856748357415
iteration 59, loss = 0.0011790633434429765
iteration 60, loss = 0.000896100013051182
iteration 61, loss = 0.0011017691576853395
iteration 62, loss = 0.0010453960858285427
iteration 63, loss = 0.0009557016892358661
iteration 64, loss = 0.0011102647986263037
iteration 65, loss = 0.0006987997912801802
iteration 66, loss = 0.000847088813316077
iteration 67, loss = 0.0009499826701357961
iteration 68, loss = 0.0009005337487906218
iteration 69, loss = 0.0011159608839079738
iteration 70, loss = 0.0017928066663444042
iteration 71, loss = 0.000915063894353807
iteration 72, loss = 0.000683753693010658
iteration 73, loss = 0.0007739288848824799
iteration 74, loss = 0.0008939699037000537
iteration 75, loss = 0.0007944711251184344
iteration 76, loss = 0.0007652213098481297
iteration 77, loss = 0.0008996707038022578
iteration 78, loss = 0.0012922179885208607
iteration 79, loss = 0.0008730138069950044
iteration 80, loss = 0.001790241920389235
iteration 81, loss = 0.0006634597666561604
iteration 82, loss = 0.0007777471328154206
iteration 83, loss = 0.0009182858630083501
iteration 84, loss = 0.0011043268023058772
iteration 85, loss = 0.0008373298333026469
iteration 86, loss = 0.001089279307052493
iteration 87, loss = 0.0008303410140797496
iteration 88, loss = 0.0011816022451967
iteration 89, loss = 0.0010005447547882795
iteration 90, loss = 0.0008354420424439013
iteration 91, loss = 0.0007920602802187204
iteration 92, loss = 0.0010917186737060547
iteration 93, loss = 0.0008577059488743544
iteration 94, loss = 0.0009231393341906369
iteration 95, loss = 0.001613829517737031
iteration 96, loss = 0.0009910822845995426
iteration 97, loss = 0.0007332212990149856
iteration 98, loss = 0.0010285710450261831
iteration 99, loss = 0.0011097944807261229
iteration 100, loss = 0.0009171938290819526
iteration 101, loss = 0.0007839644094929099
iteration 102, loss = 0.0010040686465799809
iteration 103, loss = 0.0010796519927680492
iteration 104, loss = 0.0008065279689617455
iteration 105, loss = 0.0009146064985543489
iteration 106, loss = 0.0008027603034861386
iteration 107, loss = 0.000795782427303493
iteration 108, loss = 0.0011155691463500261
iteration 109, loss = 0.0007723317248746753
iteration 110, loss = 0.0007446454837918282
iteration 111, loss = 0.0007484579691663384
iteration 112, loss = 0.0010349592193961143
iteration 113, loss = 0.0007636038353666663
iteration 114, loss = 0.000886340334545821
iteration 115, loss = 0.0008015367202460766
iteration 116, loss = 0.0011231438256800175
iteration 117, loss = 0.0008209424559026957
iteration 118, loss = 0.0010147884022444487
iteration 119, loss = 0.0007344412733800709
iteration 120, loss = 0.0009182795765809715
iteration 121, loss = 0.0009840388083830476
iteration 122, loss = 0.000784944393672049
iteration 123, loss = 0.0008885370334610343
iteration 124, loss = 0.0008701843325980008
iteration 125, loss = 0.001724720699712634
iteration 126, loss = 0.002121291821822524
iteration 127, loss = 0.0008185866172425449
iteration 128, loss = 0.0012926800409331918
iteration 129, loss = 0.0010256222449243069
iteration 130, loss = 0.0007894205627962947
iteration 131, loss = 0.0007888622349128127
iteration 132, loss = 0.0008111170609481633
iteration 133, loss = 0.0007781672175042331
iteration 134, loss = 0.0006819409318268299
iteration 135, loss = 0.0008169185020960867
iteration 136, loss = 0.001326283672824502
iteration 137, loss = 0.0015312643954530358
iteration 138, loss = 0.0014749937690794468
iteration 139, loss = 0.0014815554022789001
iteration 140, loss = 0.0008838109206408262
iteration 141, loss = 0.0007787616341374815
iteration 142, loss = 0.0009366257581859827
iteration 143, loss = 0.0007630136678926647
iteration 144, loss = 0.001511126640252769
iteration 145, loss = 0.0009002846200019121
iteration 146, loss = 0.0008659664308652282
iteration 147, loss = 0.0008920259424485266
iteration 148, loss = 0.00087543367408216
iteration 149, loss = 0.0012517264112830162
iteration 150, loss = 0.0010873636929318309
iteration 151, loss = 0.0007227586465887725
iteration 152, loss = 0.0011487763840705156
iteration 153, loss = 0.0009545714128762484
iteration 154, loss = 0.0008911261102184653
iteration 155, loss = 0.0014404256362468004
iteration 156, loss = 0.0009210855350829661
iteration 157, loss = 0.0007279944838955998
iteration 158, loss = 0.0010351409437134862
iteration 159, loss = 0.000924057443626225
iteration 160, loss = 0.0016410432290285826
iteration 161, loss = 0.0008358233608305454
iteration 162, loss = 0.0010613767663016915
iteration 163, loss = 0.0007143106195144355
iteration 164, loss = 0.0010199679527431726
iteration 165, loss = 0.0010682030115276575
iteration 166, loss = 0.0008420487865805626
iteration 167, loss = 0.0009912484092637897
iteration 168, loss = 0.001099479733966291
iteration 169, loss = 0.0007502142107114196
iteration 170, loss = 0.0015811629127711058
iteration 171, loss = 0.0009231872390955687
iteration 172, loss = 0.0014382310910150409
iteration 173, loss = 0.0008070630137808621
iteration 174, loss = 0.001085926196537912
iteration 175, loss = 0.0007622137782163918
iteration 176, loss = 0.0010824071941897273
iteration 177, loss = 0.0008143095765262842
iteration 178, loss = 0.0008246639044955373
iteration 179, loss = 0.0009521457832306623
iteration 180, loss = 0.001252802787348628
iteration 181, loss = 0.0008900965913198888
iteration 182, loss = 0.0012342839036136866
iteration 183, loss = 0.0008802306838333607
iteration 184, loss = 0.0008670028764754534
iteration 185, loss = 0.0007102103554643691
iteration 186, loss = 0.0006560601759701967
iteration 187, loss = 0.0007832007249817252
iteration 188, loss = 0.0009849651250988245
iteration 189, loss = 0.0008985432796180248
iteration 190, loss = 0.000739568320568651
iteration 191, loss = 0.0017922609113156796
iteration 192, loss = 0.0016079344786703587
iteration 193, loss = 0.0011173427337780595
iteration 194, loss = 0.0009492114186286926
iteration 195, loss = 0.0010644239373505116
iteration 196, loss = 0.0017778457840904593
iteration 197, loss = 0.001104203169234097
iteration 198, loss = 0.0008078029495663941
iteration 199, loss = 0.0007922776276245713
iteration 200, loss = 0.0010711487848311663
iteration 201, loss = 0.0006544022471643984
iteration 202, loss = 0.001596106798388064
iteration 203, loss = 0.0018816369120031595
iteration 204, loss = 0.0009315436473116279
iteration 205, loss = 0.0009158782777376473
iteration 206, loss = 0.0007784756598994136
iteration 207, loss = 0.0007893943111412227
iteration 208, loss = 0.0007676644599996507
iteration 209, loss = 0.0007747695199213922
iteration 210, loss = 0.0016231682384386659
iteration 211, loss = 0.000882138789165765
iteration 212, loss = 0.0011794527526944876
iteration 213, loss = 0.0010822940384969115
iteration 214, loss = 0.0009575857548043132
iteration 215, loss = 0.0012778218369930983
iteration 216, loss = 0.0009129662648774683
iteration 217, loss = 0.0016499549383297563
iteration 218, loss = 0.00091418536612764
iteration 219, loss = 0.0007922920631244779
iteration 220, loss = 0.0007674203952774405
iteration 221, loss = 0.0008752192370593548
iteration 222, loss = 0.0008704521460458636
iteration 223, loss = 0.0009101390605792403
iteration 224, loss = 0.0007688879850320518
iteration 225, loss = 0.000858812069054693
iteration 226, loss = 0.0008465023711323738
iteration 227, loss = 0.0008153958478942513
iteration 228, loss = 0.0007366755744442344
iteration 229, loss = 0.0015816515078768134
iteration 230, loss = 0.0008995691896416247
iteration 231, loss = 0.0009007994085550308
iteration 232, loss = 0.0007219502585940063
iteration 233, loss = 0.0007349475054070354
iteration 234, loss = 0.0007541580125689507
iteration 235, loss = 0.0006836953689344227
iteration 236, loss = 0.0008743209182284772
iteration 237, loss = 0.0016480582999065518
iteration 238, loss = 0.0008281301707029343
iteration 239, loss = 0.0016635808860883117
iteration 240, loss = 0.0008166012703441083
iteration 241, loss = 0.000737267080694437
iteration 242, loss = 0.0008353678858838975
iteration 243, loss = 0.000856679049320519
iteration 244, loss = 0.00079615309368819
iteration 245, loss = 0.0009331015171483159
iteration 246, loss = 0.0009033707901835442
iteration 247, loss = 0.0009162972564809024
iteration 248, loss = 0.001368908560834825
iteration 249, loss = 0.0016624690033495426
iteration 250, loss = 0.0008664702181704342
iteration 251, loss = 0.0008298655739054084
iteration 252, loss = 0.0007874651928432286
iteration 253, loss = 0.0008551841601729393
iteration 254, loss = 0.0009203121881000698
iteration 255, loss = 0.000760771450586617
iteration 256, loss = 0.0008028233423829079
iteration 257, loss = 0.0008286187658086419
iteration 258, loss = 0.0011326735839247704
iteration 259, loss = 0.0007786554051563144
iteration 260, loss = 0.0007413958082906902
iteration 261, loss = 0.0007823339547030628
iteration 262, loss = 0.0007083715754561126
iteration 263, loss = 0.0010267910547554493
iteration 264, loss = 0.0007258306141011417
iteration 265, loss = 0.0011187938507646322
iteration 266, loss = 0.0009653460001572967
iteration 267, loss = 0.0016920458292588592
iteration 268, loss = 0.0010119518265128136
iteration 269, loss = 0.000724122510291636
iteration 270, loss = 0.001030604587867856
iteration 271, loss = 0.0011371191358193755
iteration 272, loss = 0.001057951943948865
iteration 273, loss = 0.0008529768092557788
iteration 274, loss = 0.0007269000634551048
iteration 275, loss = 0.000928735826164484
iteration 276, loss = 0.0007511927979066968
iteration 277, loss = 0.0015341523103415966
iteration 278, loss = 0.0008122970466502011
iteration 279, loss = 0.0007488633273169398
iteration 280, loss = 0.0009556133300065994
iteration 281, loss = 0.001244972227141261
iteration 282, loss = 0.0009306165156885982
iteration 283, loss = 0.0016602618852630258
iteration 284, loss = 0.0007645987789146602
iteration 285, loss = 0.000858076149597764
iteration 286, loss = 0.0006840461865067482
iteration 287, loss = 0.0010334267280995846
iteration 288, loss = 0.0017909369198605418
iteration 289, loss = 0.0010030148550868034
iteration 290, loss = 0.0009418188128620386
iteration 291, loss = 0.0011043554404750466
iteration 292, loss = 0.0009191652061417699
iteration 293, loss = 0.0008633665856905282
iteration 294, loss = 0.0012470141518861055
iteration 295, loss = 0.0008857061620801687
iteration 296, loss = 0.0012726042186841369
iteration 297, loss = 0.0008606310584582388
iteration 298, loss = 0.0011157308472320437
iteration 299, loss = 0.0007541385130025446
iteration 300, loss = 0.0007287571206688881
iteration 1, loss = 0.0007380335591733456
iteration 2, loss = 0.0016665428411215544
iteration 3, loss = 0.0008069986943155527
iteration 4, loss = 0.0014949495671316981
iteration 5, loss = 0.0011201391462236643
iteration 6, loss = 0.0007913723238743842
iteration 7, loss = 0.0016665654256939888
iteration 8, loss = 0.001016481313854456
iteration 9, loss = 0.0008447713917121291
iteration 10, loss = 0.0009752600453794003
iteration 11, loss = 0.0008482387638650835
iteration 12, loss = 0.0008195365662686527
iteration 13, loss = 0.000720216310583055
iteration 14, loss = 0.0017242589965462685
iteration 15, loss = 0.0008930483018048108
iteration 16, loss = 0.0007788746152073145
iteration 17, loss = 0.0011953137582167983
iteration 18, loss = 0.0009403039584867656
iteration 19, loss = 0.0007399675669148564
iteration 20, loss = 0.0008762383367866278
iteration 21, loss = 0.00101145647931844
iteration 22, loss = 0.0012430166825652122
iteration 23, loss = 0.0009200361091643572
iteration 24, loss = 0.0008301783236674964
iteration 25, loss = 0.0010702054714784026
iteration 26, loss = 0.00077721884008497
iteration 27, loss = 0.000775048800278455
iteration 28, loss = 0.0009337851079180837
iteration 29, loss = 0.0013557883212342858
iteration 30, loss = 0.0009792994242161512
iteration 31, loss = 0.0009521770407445729
iteration 32, loss = 0.0008643928449600935
iteration 33, loss = 0.0011548844631761312
iteration 34, loss = 0.0013782711466774344
iteration 35, loss = 0.0008915766957215965
iteration 36, loss = 0.0016347132623195648
iteration 37, loss = 0.0008618813008069992
iteration 38, loss = 0.0007276281830854714
iteration 39, loss = 0.0008438003133051097
iteration 40, loss = 0.0008901030523702502
iteration 41, loss = 0.0009583915816619992
iteration 42, loss = 0.0009305696003139019
iteration 43, loss = 0.0008526596357114613
iteration 44, loss = 0.001403781701810658
iteration 45, loss = 0.0007431164267472923
iteration 46, loss = 0.0014162230072543025
iteration 47, loss = 0.0014047602890059352
iteration 48, loss = 0.0016310707433149219
iteration 49, loss = 0.0009122303454205394
iteration 50, loss = 0.0008096825331449509
iteration 51, loss = 0.0009493252728134394
iteration 52, loss = 0.001242182101123035
iteration 53, loss = 0.0016900287009775639
iteration 54, loss = 0.0008930957992561162
iteration 55, loss = 0.000793841143604368
iteration 56, loss = 0.0008551180362701416
iteration 57, loss = 0.000962593243457377
iteration 58, loss = 0.0008026341674849391
iteration 59, loss = 0.0010082400403916836
iteration 60, loss = 0.0010093781165778637
iteration 61, loss = 0.0007550101727247238
iteration 62, loss = 0.0007948072161525488
iteration 63, loss = 0.00083797553088516
iteration 64, loss = 0.0020865288097411394
iteration 65, loss = 0.0009959317976608872
iteration 66, loss = 0.0007580220699310303
iteration 67, loss = 0.0007177509251050651
iteration 68, loss = 0.0007992344326339662
iteration 69, loss = 0.0009049896616488695
iteration 70, loss = 0.0020252359099686146
iteration 71, loss = 0.0009168131509795785
iteration 72, loss = 0.0017452933825552464
iteration 73, loss = 0.0008919636602513492
iteration 74, loss = 0.0006654163589701056
iteration 75, loss = 0.0007310971850529313
iteration 76, loss = 0.0007952933083288372
iteration 77, loss = 0.0007911125430837274
iteration 78, loss = 0.0010630822507664561
iteration 79, loss = 0.0007406499935314059
iteration 80, loss = 0.0011577929835766554
iteration 81, loss = 0.0007417036686092615
iteration 82, loss = 0.0008533928194083273
iteration 83, loss = 0.0008818674250505865
iteration 84, loss = 0.0009909624932333827
iteration 85, loss = 0.0015015678945928812
iteration 86, loss = 0.0010253158397972584
iteration 87, loss = 0.0019730362109839916
iteration 88, loss = 0.000715765985660255
iteration 89, loss = 0.0016741554718464613
iteration 90, loss = 0.001052026986144483
iteration 91, loss = 0.0008763649384491146
iteration 92, loss = 0.0013594088377431035
iteration 93, loss = 0.0008019979577511549
iteration 94, loss = 0.0010498632909730077
iteration 95, loss = 0.0011373517336323857
iteration 96, loss = 0.0011435014894232154
iteration 97, loss = 0.0009620212949812412
iteration 98, loss = 0.0009433674858883023
iteration 99, loss = 0.0012197907781228423
iteration 100, loss = 0.0014688351657241583
iteration 101, loss = 0.0007724366150796413
iteration 102, loss = 0.0009486340568400919
iteration 103, loss = 0.0007802775362506509
iteration 104, loss = 0.0008633775869384408
iteration 105, loss = 0.0008140210993587971
iteration 106, loss = 0.0009047905914485455
iteration 107, loss = 0.0010429183021187782
iteration 108, loss = 0.0009241341613233089
iteration 109, loss = 0.0009489366202615201
iteration 110, loss = 0.0007932711159810424
iteration 111, loss = 0.0007775459671393037
iteration 112, loss = 0.001007263083010912
iteration 113, loss = 0.0006670326692983508
iteration 114, loss = 0.0014090398326516151
iteration 115, loss = 0.0008912010816857219
iteration 116, loss = 0.0011009171139448881
iteration 117, loss = 0.0008933711796998978
iteration 118, loss = 0.0006848781486041844
iteration 119, loss = 0.0009135996806435287
iteration 120, loss = 0.0007709243218414485
iteration 121, loss = 0.0010007476666942239
iteration 122, loss = 0.0007425385992974043
iteration 123, loss = 0.000729896011762321
iteration 124, loss = 0.0008252494153566658
iteration 125, loss = 0.0007771989912725985
iteration 126, loss = 0.0011390349827706814
iteration 127, loss = 0.0010167116997763515
iteration 128, loss = 0.001519958721473813
iteration 129, loss = 0.000786078511737287
iteration 130, loss = 0.0008060277905315161
iteration 131, loss = 0.0008264252683147788
iteration 132, loss = 0.0013945348327979445
iteration 133, loss = 0.0009131996775977314
iteration 134, loss = 0.0014654949773102999
iteration 135, loss = 0.0007247267640195787
iteration 136, loss = 0.0008318900363519788
iteration 137, loss = 0.0009299142984673381
iteration 138, loss = 0.0006094523123465478
iteration 139, loss = 0.0008129150373861194
iteration 140, loss = 0.0009530220413580537
iteration 141, loss = 0.001020182971842587
iteration 142, loss = 0.0007039407500997186
iteration 143, loss = 0.0007131215534172952
iteration 144, loss = 0.001234268071129918
iteration 145, loss = 0.0008788592531345785
iteration 146, loss = 0.001034994376823306
iteration 147, loss = 0.0008578934939578176
iteration 148, loss = 0.000918727193493396
iteration 149, loss = 0.000854570884257555
iteration 150, loss = 0.0015502439346164465
iteration 151, loss = 0.0014515470247715712
iteration 152, loss = 0.0009215273312292993
iteration 153, loss = 0.0009086048230528831
iteration 154, loss = 0.0017959625693038106
iteration 155, loss = 0.0011011805618181825
iteration 156, loss = 0.0008248686790466309
iteration 157, loss = 0.0008404897525906563
iteration 158, loss = 0.0014154580421745777
iteration 159, loss = 0.001595008885487914
iteration 160, loss = 0.0007229645852930844
iteration 161, loss = 0.0007858466124162078
iteration 162, loss = 0.0008859828813001513
iteration 163, loss = 0.0010133088799193501
iteration 164, loss = 0.0010069485288113356
iteration 165, loss = 0.0011912839254364371
iteration 166, loss = 0.0009295460768043995
iteration 167, loss = 0.0009376524249091744
iteration 168, loss = 0.0007824083440937102
iteration 169, loss = 0.0008341061766259372
iteration 170, loss = 0.001149943913333118
iteration 171, loss = 0.0010279335547238588
iteration 172, loss = 0.001182299805805087
iteration 173, loss = 0.001081808703020215
iteration 174, loss = 0.0008988159243017435
iteration 175, loss = 0.0008882075781002641
iteration 176, loss = 0.001047331839799881
iteration 177, loss = 0.0008221475873142481
iteration 178, loss = 0.0013551512965932488
iteration 179, loss = 0.0008375811157748103
iteration 180, loss = 0.0007234644144773483
iteration 181, loss = 0.0008214134722948074
iteration 182, loss = 0.0016224888386204839
iteration 183, loss = 0.0008530784398317337
iteration 184, loss = 0.0008120930870063603
iteration 185, loss = 0.000831621524412185
iteration 186, loss = 0.0008597788400948048
iteration 187, loss = 0.0017340356716886163
iteration 188, loss = 0.0013027092209085822
iteration 189, loss = 0.0008964719600044191
iteration 190, loss = 0.0008840853115543723
iteration 191, loss = 0.0007547080749645829
iteration 192, loss = 0.0010574436746537685
iteration 193, loss = 0.0008663719636388123
iteration 194, loss = 0.0008614525431767106
iteration 195, loss = 0.0008421678212471306
iteration 196, loss = 0.0006923020118847489
iteration 197, loss = 0.0009937367867678404
iteration 198, loss = 0.0008372850134037435
iteration 199, loss = 0.0010263571748510003
iteration 200, loss = 0.000948319910094142
iteration 201, loss = 0.0008582085720263422
iteration 202, loss = 0.0017297040903940797
iteration 203, loss = 0.0008867445285432041
iteration 204, loss = 0.0008289499673992395
iteration 205, loss = 0.0011639920994639397
iteration 206, loss = 0.0007250187336467206
iteration 207, loss = 0.0007273011724464595
iteration 208, loss = 0.0014884256524965167
iteration 209, loss = 0.0011610398069024086
iteration 210, loss = 0.000785148935392499
iteration 211, loss = 0.0010800540912896395
iteration 212, loss = 0.0009142545750364661
iteration 213, loss = 0.0008824813994579017
iteration 214, loss = 0.0009089462691918015
iteration 215, loss = 0.0008959553670138121
iteration 216, loss = 0.0008895664941519499
iteration 217, loss = 0.0013838220620527864
iteration 218, loss = 0.0007977146888151765
iteration 219, loss = 0.001495192525908351
iteration 220, loss = 0.0009039152646437287
iteration 221, loss = 0.0013128389837220311
iteration 222, loss = 0.0008910887409001589
iteration 223, loss = 0.000967465341091156
iteration 224, loss = 0.0017627186607569456
iteration 225, loss = 0.0011056395014747977
iteration 226, loss = 0.0009123188792727888
iteration 227, loss = 0.0008326080278493464
iteration 228, loss = 0.0008892897167243063
iteration 229, loss = 0.0012325514107942581
iteration 230, loss = 0.0011712126433849335
iteration 231, loss = 0.0006930144736543298
iteration 232, loss = 0.00102433399297297
iteration 233, loss = 0.0007854767027311027
iteration 234, loss = 0.000634922063909471
iteration 235, loss = 0.0006988432724028826
iteration 236, loss = 0.0009298647637479007
iteration 237, loss = 0.0009104778873734176
iteration 238, loss = 0.0006568111712113023
iteration 239, loss = 0.000973797868937254
iteration 240, loss = 0.0007990454905666411
iteration 241, loss = 0.0008409253787249327
iteration 242, loss = 0.0009664385579526424
iteration 243, loss = 0.0007738438434898853
iteration 244, loss = 0.0011606434127315879
iteration 245, loss = 0.0008014257764443755
iteration 246, loss = 0.0009002065635286272
iteration 247, loss = 0.0011536969104781747
iteration 248, loss = 0.0008533006766811013
iteration 249, loss = 0.0008221694733947515
iteration 250, loss = 0.0012100717285647988
iteration 251, loss = 0.0012020295253023505
iteration 252, loss = 0.0006209049024619162
iteration 253, loss = 0.0007509703864343464
iteration 254, loss = 0.0008730651461519301
iteration 255, loss = 0.0007316333940252662
iteration 256, loss = 0.000790308287832886
iteration 257, loss = 0.0008376925834454596
iteration 258, loss = 0.0010598845547065139
iteration 259, loss = 0.0007834372809156775
iteration 260, loss = 0.0012206491082906723
iteration 261, loss = 0.0013159788213670254
iteration 262, loss = 0.0008424331899732351
iteration 263, loss = 0.000813402933999896
iteration 264, loss = 0.0007752288365736604
iteration 265, loss = 0.0009087557555176318
iteration 266, loss = 0.0007459844928234816
iteration 267, loss = 0.000982821686193347
iteration 268, loss = 0.0007428644457831979
iteration 269, loss = 0.0011537183308973908
iteration 270, loss = 0.0007271289941854775
iteration 271, loss = 0.0008284099167212844
iteration 272, loss = 0.0007407821249216795
iteration 273, loss = 0.0008149633649736643
iteration 274, loss = 0.0008134705713018775
iteration 275, loss = 0.0007862512138672173
iteration 276, loss = 0.0008878673543222249
iteration 277, loss = 0.0017658249707892537
iteration 278, loss = 0.0010172051843255758
iteration 279, loss = 0.0014532575150951743
iteration 280, loss = 0.0009599715122021735
iteration 281, loss = 0.0007725662435404956
iteration 282, loss = 0.0006886323681101203
iteration 283, loss = 0.0007588950102217495
iteration 284, loss = 0.0007326664053834975
iteration 285, loss = 0.0008302784990519285
iteration 286, loss = 0.0009444176102988422
iteration 287, loss = 0.0007291563670150936
iteration 288, loss = 0.0009455913677811623
iteration 289, loss = 0.0008295116713270545
iteration 290, loss = 0.0010863615898415446
iteration 291, loss = 0.0008175593102350831
iteration 292, loss = 0.0010196379153057933
iteration 293, loss = 0.0010222913697361946
iteration 294, loss = 0.0010908563854172826
iteration 295, loss = 0.0008500713738612831
iteration 296, loss = 0.000795768981333822
iteration 297, loss = 0.0006932869437150657
iteration 298, loss = 0.0008497003000229597
iteration 299, loss = 0.0008165980689227581
iteration 300, loss = 0.0008707180386409163
iteration 1, loss = 0.0011577417608350515
iteration 2, loss = 0.0018353038467466831
iteration 3, loss = 0.0008184688631445169
iteration 4, loss = 0.0007463892106898129
iteration 5, loss = 0.0008397927158512175
iteration 6, loss = 0.0016324887983500957
iteration 7, loss = 0.0007155866478569806
iteration 8, loss = 0.0008073356002569199
iteration 9, loss = 0.0011058496311306953
iteration 10, loss = 0.0008297991007566452
iteration 11, loss = 0.0008364074747078121
iteration 12, loss = 0.0008459049859084189
iteration 13, loss = 0.0006770172622054815
iteration 14, loss = 0.0009540198370814323
iteration 15, loss = 0.0008377961930818856
iteration 16, loss = 0.0007586504216305912
iteration 17, loss = 0.0008175410912372172
iteration 18, loss = 0.0008674502605572343
iteration 19, loss = 0.0007727231131866574
iteration 20, loss = 0.000852215220220387
iteration 21, loss = 0.0008840849623084068
iteration 22, loss = 0.0007443520007655025
iteration 23, loss = 0.0009392101201228797
iteration 24, loss = 0.0007815411081537604
iteration 25, loss = 0.000888751819729805
iteration 26, loss = 0.00077426916686818
iteration 27, loss = 0.0007470748969353735
iteration 28, loss = 0.0007241839193738997
iteration 29, loss = 0.0009623529040254653
iteration 30, loss = 0.000786177406553179
iteration 31, loss = 0.0010330622317269444
iteration 32, loss = 0.0007500636274926364
iteration 33, loss = 0.0019047061214223504
iteration 34, loss = 0.00075719557935372
iteration 35, loss = 0.0008644325425848365
iteration 36, loss = 0.0008099910337477922
iteration 37, loss = 0.0010823316406458616
iteration 38, loss = 0.0011329790577292442
iteration 39, loss = 0.0008027013391256332
iteration 40, loss = 0.00123656599316746
iteration 41, loss = 0.0008378734928555787
iteration 42, loss = 0.000944155384786427
iteration 43, loss = 0.0006998199387453496
iteration 44, loss = 0.0010207138257101178
iteration 45, loss = 0.0010267536854371428
iteration 46, loss = 0.0009975298307836056
iteration 47, loss = 0.0009053479298017919
iteration 48, loss = 0.0016946233808994293
iteration 49, loss = 0.0017737140879034996
iteration 50, loss = 0.0008092588395811617
iteration 51, loss = 0.0008275738800875843
iteration 52, loss = 0.0007304286118596792
iteration 53, loss = 0.0008604854810982943
iteration 54, loss = 0.0010656971717253327
iteration 55, loss = 0.0015450473874807358
iteration 56, loss = 0.001429648487828672
iteration 57, loss = 0.0011583855375647545
iteration 58, loss = 0.0008064843132160604
iteration 59, loss = 0.0008580255671404302
iteration 60, loss = 0.0008273806306533515
iteration 61, loss = 0.0009408770711161196
iteration 62, loss = 0.0008885791175998747
iteration 63, loss = 0.0009192178258672357
iteration 64, loss = 0.000824374204967171
iteration 65, loss = 0.0007918128976598382
iteration 66, loss = 0.0007873264839872718
iteration 67, loss = 0.0007665259763598442
iteration 68, loss = 0.0018658617045730352
iteration 69, loss = 0.0010505991522222757
iteration 70, loss = 0.0009134054998867214
iteration 71, loss = 0.001588334795087576
iteration 72, loss = 0.0008083634311333299
iteration 73, loss = 0.0008728737011551857
iteration 74, loss = 0.0016141739906743169
iteration 75, loss = 0.0008036749204620719
iteration 76, loss = 0.0007958188652992249
iteration 77, loss = 0.000883506378158927
iteration 78, loss = 0.0009272696916013956
iteration 79, loss = 0.000855364603921771
iteration 80, loss = 0.0013849501265212893
iteration 81, loss = 0.0009190501878038049
iteration 82, loss = 0.0010321908630430698
iteration 83, loss = 0.001069901161827147
iteration 84, loss = 0.0007233960786834359
iteration 85, loss = 0.0009174984297715127
iteration 86, loss = 0.00132999278139323
iteration 87, loss = 0.0008356454782187939
iteration 88, loss = 0.0007573261973448098
iteration 89, loss = 0.0008025396382436156
iteration 90, loss = 0.0008208861108869314
iteration 91, loss = 0.0007820843020454049
iteration 92, loss = 0.0011942709097638726
iteration 93, loss = 0.0007535789627581835
iteration 94, loss = 0.0009557081502862275
iteration 95, loss = 0.0010550488950684667
iteration 96, loss = 0.0007539956131950021
iteration 97, loss = 0.0017819750355556607
iteration 98, loss = 0.000753614236600697
iteration 99, loss = 0.0008344874950125813
iteration 100, loss = 0.0008157874690368772
iteration 101, loss = 0.001128476345911622
iteration 102, loss = 0.0008605032926425338
iteration 103, loss = 0.0009181865607388318
iteration 104, loss = 0.0010344985639676452
iteration 105, loss = 0.0007238932303152978
iteration 106, loss = 0.0007021051715128124
iteration 107, loss = 0.0014323978684842587
iteration 108, loss = 0.0007307003252208233
iteration 109, loss = 0.0007632846245542169
iteration 110, loss = 0.0007211165502667427
iteration 111, loss = 0.0008132760412991047
iteration 112, loss = 0.0011508300667628646
iteration 113, loss = 0.0007697406690567732
iteration 114, loss = 0.0009403414442203939
iteration 115, loss = 0.0008561196154914796
iteration 116, loss = 0.0013487155083566904
iteration 117, loss = 0.0016697286628186703
iteration 118, loss = 0.0008885717252269387
iteration 119, loss = 0.002366849221289158
iteration 120, loss = 0.0009449210483580828
iteration 121, loss = 0.0007966757984831929
iteration 122, loss = 0.0009814423974603415
iteration 123, loss = 0.0011521142441779375
iteration 124, loss = 0.00081241549924016
iteration 125, loss = 0.001489270944148302
iteration 126, loss = 0.0011871869210153818
iteration 127, loss = 0.0007719761924818158
iteration 128, loss = 0.0015709386207163334
iteration 129, loss = 0.0017039462691172957
iteration 130, loss = 0.0013277474790811539
iteration 131, loss = 0.0017408186104148626
iteration 132, loss = 0.00079966033808887
iteration 133, loss = 0.0009153760620392859
iteration 134, loss = 0.0017218617722392082
iteration 135, loss = 0.000782117189373821
iteration 136, loss = 0.000824462971650064
iteration 137, loss = 0.0008698748424649239
iteration 138, loss = 0.0008647871436551213
iteration 139, loss = 0.0008794482564553618
iteration 140, loss = 0.0010781806195154786
iteration 141, loss = 0.0012955596903339028
iteration 142, loss = 0.0008964148000814021
iteration 143, loss = 0.001600033836439252
iteration 144, loss = 0.001019654213450849
iteration 145, loss = 0.0012239626375958323
iteration 146, loss = 0.0010566555429250002
iteration 147, loss = 0.0009444594033993781
iteration 148, loss = 0.0012342032277956605
iteration 149, loss = 0.0011615141993388534
iteration 150, loss = 0.0018912602681666613
iteration 151, loss = 0.0017221999587491155
iteration 152, loss = 0.0007130912272259593
iteration 153, loss = 0.0006386448512785137
iteration 154, loss = 0.0012925344053655863
iteration 155, loss = 0.0009371806518174708
iteration 156, loss = 0.0008751541026867926
iteration 157, loss = 0.0007890898850746453
iteration 158, loss = 0.0009317317744717002
iteration 159, loss = 0.0011084036668762565
iteration 160, loss = 0.0007724951137788594
iteration 161, loss = 0.0008735798764973879
iteration 162, loss = 0.0009822745341807604
iteration 163, loss = 0.0009558951715007424
iteration 164, loss = 0.0008765322272665799
iteration 165, loss = 0.001690764562226832
iteration 166, loss = 0.0008914837962947786
iteration 167, loss = 0.0008837325731292367
iteration 168, loss = 0.00149068096652627
iteration 169, loss = 0.0007012328132987022
iteration 170, loss = 0.0016287392936646938
iteration 171, loss = 0.0006987996166571975
iteration 172, loss = 0.0007674696389585733
iteration 173, loss = 0.0010957826161757112
iteration 174, loss = 0.0008923645946197212
iteration 175, loss = 0.000804439652711153
iteration 176, loss = 0.0007939842762425542
iteration 177, loss = 0.0009405688033439219
iteration 178, loss = 0.000699252646882087
iteration 179, loss = 0.0011303070932626724
iteration 180, loss = 0.0011604130268096924
iteration 181, loss = 0.0008097809622995555
iteration 182, loss = 0.0012420059647411108
iteration 183, loss = 0.0007334364345297217
iteration 184, loss = 0.0008428465225733817
iteration 185, loss = 0.001632867963053286
iteration 186, loss = 0.0008070734911598265
iteration 187, loss = 0.0008301606867462397
iteration 188, loss = 0.0010063659865409136
iteration 189, loss = 0.0008847540593706071
iteration 190, loss = 0.0009755973005667329
iteration 191, loss = 0.0008143543964251876
iteration 192, loss = 0.0009315202478319407
iteration 193, loss = 0.0007957395864650607
iteration 194, loss = 0.0011906935833394527
iteration 195, loss = 0.0007255579112097621
iteration 196, loss = 0.0007206558366306126
iteration 197, loss = 0.0017311429837718606
iteration 198, loss = 0.0008547102916054428
iteration 199, loss = 0.0008602002053521574
iteration 200, loss = 0.0011940860422328115
iteration 201, loss = 0.0013934412272647023
iteration 202, loss = 0.000987100531347096
iteration 203, loss = 0.0007635124493390322
iteration 204, loss = 0.0008430354646407068
iteration 205, loss = 0.0010076292091980577
iteration 206, loss = 0.0009999991161748767
iteration 207, loss = 0.00151450140401721
iteration 208, loss = 0.0010907527757808566
iteration 209, loss = 0.0010670729679986835
iteration 210, loss = 0.0009306430583819747
iteration 211, loss = 0.000784883217420429
iteration 212, loss = 0.0007788176299072802
iteration 213, loss = 0.001031238934956491
iteration 214, loss = 0.0009426681790500879
iteration 215, loss = 0.0007203904679045081
iteration 216, loss = 0.0009164959192276001
iteration 217, loss = 0.0012044921750202775
iteration 218, loss = 0.0008352183504030108
iteration 219, loss = 0.00101568759419024
iteration 220, loss = 0.0008027507574297488
iteration 221, loss = 0.0009952823165804148
iteration 222, loss = 0.000864195404574275
iteration 223, loss = 0.0015854346565902233
iteration 224, loss = 0.0007541963714174926
iteration 225, loss = 0.0007467750110663474
iteration 226, loss = 0.0008309156983159482
iteration 227, loss = 0.0007479275809600949
iteration 228, loss = 0.0008513829670846462
iteration 229, loss = 0.0009377795504406095
iteration 230, loss = 0.0008707621018402278
iteration 231, loss = 0.0007874059956520796
iteration 232, loss = 0.0007516032783314586
iteration 233, loss = 0.0008053177152760327
iteration 234, loss = 0.0008471666369587183
iteration 235, loss = 0.0008957606041803956
iteration 236, loss = 0.0009211813448928297
iteration 237, loss = 0.0010240067495033145
iteration 238, loss = 0.0007659131661057472
iteration 239, loss = 0.0008138951379805803
iteration 240, loss = 0.0010152396280318499
iteration 241, loss = 0.0010509088169783354
iteration 242, loss = 0.0008598389104008675
iteration 243, loss = 0.0007789571536704898
iteration 244, loss = 0.0007721446454524994
iteration 245, loss = 0.000614156830124557
iteration 246, loss = 0.0007585653802379966
iteration 247, loss = 0.0008308895630761981
iteration 248, loss = 0.0011625690385699272
iteration 249, loss = 0.0010964053217321634
iteration 250, loss = 0.0010881397174671292
iteration 251, loss = 0.0011934360954910517
iteration 252, loss = 0.0008331261342391372
iteration 253, loss = 0.0014618763234466314
iteration 254, loss = 0.0007402231567539275
iteration 255, loss = 0.0007462181383743882
iteration 256, loss = 0.0021006399765610695
iteration 257, loss = 0.0012025923933833838
iteration 258, loss = 0.0009723375551402569
iteration 259, loss = 0.0011124766897410154
iteration 260, loss = 0.001126756425946951
iteration 261, loss = 0.0009505532216280699
iteration 262, loss = 0.0009705182164907455
iteration 263, loss = 0.0010790098458528519
iteration 264, loss = 0.000857474806252867
iteration 265, loss = 0.0007060951902531087
iteration 266, loss = 0.0011071664048358798
iteration 267, loss = 0.0009421324939467013
iteration 268, loss = 0.0010098802158609033
iteration 269, loss = 0.0008263624040409923
iteration 270, loss = 0.0008443210972473025
iteration 271, loss = 0.0008040184620767832
iteration 272, loss = 0.0009642927325330675
iteration 273, loss = 0.0010527883423492312
iteration 274, loss = 0.0007818981539458036
iteration 275, loss = 0.0011844020336866379
iteration 276, loss = 0.0006997262826189399
iteration 277, loss = 0.0008397144847549498
iteration 278, loss = 0.0008831450249999762
iteration 279, loss = 0.0009778294479474425
iteration 280, loss = 0.0008031610632315278
iteration 281, loss = 0.0007989900186657906
iteration 282, loss = 0.0006860470748506486
iteration 283, loss = 0.0007006014930084348
iteration 284, loss = 0.0008635497069917619
iteration 285, loss = 0.0007006142404861748
iteration 286, loss = 0.000845485832542181
iteration 287, loss = 0.0008821244118735194
iteration 288, loss = 0.0010471973801031709
iteration 289, loss = 0.0010692497016862035
iteration 290, loss = 0.0008019414963200688
iteration 291, loss = 0.0010455033043399453
iteration 292, loss = 0.0007491538999602199
iteration 293, loss = 0.0007944556418806314
iteration 294, loss = 0.0007429288816638291
iteration 295, loss = 0.001290459418669343
iteration 296, loss = 0.001004634308628738
iteration 297, loss = 0.000813160790130496
iteration 298, loss = 0.0007892862195149064
iteration 299, loss = 0.0008585481555201113
iteration 300, loss = 0.0009258365607820451
iteration 1, loss = 0.0007633650093339384
iteration 2, loss = 0.0007661286508664489
iteration 3, loss = 0.0007216986850835383
iteration 4, loss = 0.0007753993850201368
iteration 5, loss = 0.00104511808604002
iteration 6, loss = 0.001085928757674992
iteration 7, loss = 0.0009655894828028977
iteration 8, loss = 0.0008385507971979678
iteration 9, loss = 0.0009633518639020622
iteration 10, loss = 0.0007395081338472664
iteration 11, loss = 0.000956950185354799
iteration 12, loss = 0.0007178045343607664
iteration 13, loss = 0.0009507717913948
iteration 14, loss = 0.000848648080136627
iteration 15, loss = 0.001574236317537725
iteration 16, loss = 0.0009282055543735623
iteration 17, loss = 0.000858429993968457
iteration 18, loss = 0.0009174133301712573
iteration 19, loss = 0.0007409070967696607
iteration 20, loss = 0.0007878159522078931
iteration 21, loss = 0.001148383249528706
iteration 22, loss = 0.0006368993781507015
iteration 23, loss = 0.0007484937668778002
iteration 24, loss = 0.0008810721337795258
iteration 25, loss = 0.001097712549380958
iteration 26, loss = 0.0015698911156505346
iteration 27, loss = 0.0014214293332770467
iteration 28, loss = 0.0008138149278238416
iteration 29, loss = 0.0016506898682564497
iteration 30, loss = 0.0009086818899959326
iteration 31, loss = 0.0008367372793145478
iteration 32, loss = 0.0008882408146746457
iteration 33, loss = 0.0011270238319411874
iteration 34, loss = 0.0010255499510094523
iteration 35, loss = 0.0008308439282700419
iteration 36, loss = 0.0011589612113311887
iteration 37, loss = 0.0022483975626528263
iteration 38, loss = 0.0007305921171791852
iteration 39, loss = 0.0007600465323776007
iteration 40, loss = 0.0010998640209436417
iteration 41, loss = 0.0010553981410339475
iteration 42, loss = 0.0007593637565150857
iteration 43, loss = 0.0008531865896657109
iteration 44, loss = 0.0011094979709014297
iteration 45, loss = 0.0008047231822274625
iteration 46, loss = 0.0010566064156591892
iteration 47, loss = 0.0009919173317030072
iteration 48, loss = 0.0018556262366473675
iteration 49, loss = 0.0007652202621102333
iteration 50, loss = 0.000962073914706707
iteration 51, loss = 0.0007350941305048764
iteration 52, loss = 0.0008025937713682652
iteration 53, loss = 0.001059180242009461
iteration 54, loss = 0.0006772791384719312
iteration 55, loss = 0.001105854636989534
iteration 56, loss = 0.0007814504788257182
iteration 57, loss = 0.0009747991571202874
iteration 58, loss = 0.0008760859491303563
iteration 59, loss = 0.0011691247345879674
iteration 60, loss = 0.0010163704864680767
iteration 61, loss = 0.0007605760474689305
iteration 62, loss = 0.0018694389145821333
iteration 63, loss = 0.0007783101173117757
iteration 64, loss = 0.0008209377410821617
iteration 65, loss = 0.0008900840184651315
iteration 66, loss = 0.0016872845590114594
iteration 67, loss = 0.0011167627526447177
iteration 68, loss = 0.0007366968784481287
iteration 69, loss = 0.000768373254686594
iteration 70, loss = 0.0008872130420058966
iteration 71, loss = 0.0007864882354624569
iteration 72, loss = 0.0007321999873965979
iteration 73, loss = 0.000748754944652319
iteration 74, loss = 0.0008175655966624618
iteration 75, loss = 0.0007196965161710978
iteration 76, loss = 0.0010746931657195091
iteration 77, loss = 0.0009650106076151133
iteration 78, loss = 0.0008236779249273241
iteration 79, loss = 0.0014487187145277858
iteration 80, loss = 0.0009433773229829967
iteration 81, loss = 0.000684268306940794
iteration 82, loss = 0.0009269673028029501
iteration 83, loss = 0.000825610535684973
iteration 84, loss = 0.000771945808082819
iteration 85, loss = 0.0007764396141283214
iteration 86, loss = 0.0007514026947319508
iteration 87, loss = 0.0008978070691227913
iteration 88, loss = 0.0009000382851809263
iteration 89, loss = 0.0016611631726846099
iteration 90, loss = 0.0009615003946237266
iteration 91, loss = 0.0010908612748607993
iteration 92, loss = 0.0007331193191930652
iteration 93, loss = 0.0008879299857653677
iteration 94, loss = 0.0011152370134368539
iteration 95, loss = 0.0008315024897456169
iteration 96, loss = 0.0011585865868255496
iteration 97, loss = 0.0007983492105267942
iteration 98, loss = 0.0010571741731837392
iteration 99, loss = 0.0009139093453995883
iteration 100, loss = 0.000949341687373817
iteration 101, loss = 0.001233853166922927
iteration 102, loss = 0.000782592804171145
iteration 103, loss = 0.0008229154045693576
iteration 104, loss = 0.000753032392822206
iteration 105, loss = 0.001054365304298699
iteration 106, loss = 0.0007835135911591351
iteration 107, loss = 0.0009966121288016438
iteration 108, loss = 0.0008020349778234959
iteration 109, loss = 0.001049373997375369
iteration 110, loss = 0.000943956256378442
iteration 111, loss = 0.000853352714329958
iteration 112, loss = 0.0008358272025361657
iteration 113, loss = 0.0009422757430002093
iteration 114, loss = 0.0008292702259495854
iteration 115, loss = 0.0017415370093658566
iteration 116, loss = 0.0007108186255209148
iteration 117, loss = 0.0008668440277688205
iteration 118, loss = 0.0008359749335795641
iteration 119, loss = 0.0008280808106064796
iteration 120, loss = 0.0012726564891636372
iteration 121, loss = 0.0008966843015514314
iteration 122, loss = 0.0007324186153709888
iteration 123, loss = 0.001591870211996138
iteration 124, loss = 0.0007697867695242167
iteration 125, loss = 0.0007612507906742394
iteration 126, loss = 0.0008597316918894649
iteration 127, loss = 0.0009444757597520947
iteration 128, loss = 0.0007834881544113159
iteration 129, loss = 0.0008466436993330717
iteration 130, loss = 0.0008320182096213102
iteration 131, loss = 0.0011531832860782743
iteration 132, loss = 0.0008139032870531082
iteration 133, loss = 0.0009529020171612501
iteration 134, loss = 0.0008787023252807558
iteration 135, loss = 0.0007915101014077663
iteration 136, loss = 0.0008112009963952005
iteration 137, loss = 0.0014418925857171416
iteration 138, loss = 0.0007376704015769064
iteration 139, loss = 0.0009439385612495244
iteration 140, loss = 0.0007725285831838846
iteration 141, loss = 0.0007501601940020919
iteration 142, loss = 0.000739681301638484
iteration 143, loss = 0.0008630420779809356
iteration 144, loss = 0.0012257329653948545
iteration 145, loss = 0.0010982025414705276
iteration 146, loss = 0.00078490877058357
iteration 147, loss = 0.0011185145704075694
iteration 148, loss = 0.000997025752440095
iteration 149, loss = 0.0008071037591435015
iteration 150, loss = 0.000790592166595161
iteration 151, loss = 0.0011175284162163734
iteration 152, loss = 0.0008072595810517669
iteration 153, loss = 0.000824218092020601
iteration 154, loss = 0.0016004015924409032
iteration 155, loss = 0.0012401072308421135
iteration 156, loss = 0.0007921221549622715
iteration 157, loss = 0.0009717541397549212
iteration 158, loss = 0.0010029885452240705
iteration 159, loss = 0.000784934323746711
iteration 160, loss = 0.0010476252064108849
iteration 161, loss = 0.0007819123566150665
iteration 162, loss = 0.0009935642592608929
iteration 163, loss = 0.0007345570484176278
iteration 164, loss = 0.0008795906906016171
iteration 165, loss = 0.0011037671938538551
iteration 166, loss = 0.0010176061186939478
iteration 167, loss = 0.0011133666848763824
iteration 168, loss = 0.001251128502190113
iteration 169, loss = 0.0008858348010107875
iteration 170, loss = 0.0015882709994912148
iteration 171, loss = 0.0009073456167243421
iteration 172, loss = 0.0010040163761004806
iteration 173, loss = 0.0012364572612568736
iteration 174, loss = 0.0009349066531285644
iteration 175, loss = 0.0010225764708593488
iteration 176, loss = 0.000653495779260993
iteration 177, loss = 0.0007997783250175416
iteration 178, loss = 0.0006634700112044811
iteration 179, loss = 0.0007125867996364832
iteration 180, loss = 0.0007892409339547157
iteration 181, loss = 0.0007365439087152481
iteration 182, loss = 0.0009733834303915501
iteration 183, loss = 0.0015849950723350048
iteration 184, loss = 0.0007274983800016344
iteration 185, loss = 0.0008955071098171175
iteration 186, loss = 0.0008537503890693188
iteration 187, loss = 0.001711223041638732
iteration 188, loss = 0.0014877126086503267
iteration 189, loss = 0.0009652655571699142
iteration 190, loss = 0.0008492995984852314
iteration 191, loss = 0.0008419161895290017
iteration 192, loss = 0.0010233598295599222
iteration 193, loss = 0.0010050334967672825
iteration 194, loss = 0.0012752320617437363
iteration 195, loss = 0.0011307535460218787
iteration 196, loss = 0.0013964419486001134
iteration 197, loss = 0.0008862909162417054
iteration 198, loss = 0.0007946832338348031
iteration 199, loss = 0.0007679469417780638
iteration 200, loss = 0.0008319913176819682
iteration 201, loss = 0.0009493406396359205
iteration 202, loss = 0.0010314448736608028
iteration 203, loss = 0.0010369888041168451
iteration 204, loss = 0.000666004722006619
iteration 205, loss = 0.0011742862407118082
iteration 206, loss = 0.0010745247127488256
iteration 207, loss = 0.0008697261800989509
iteration 208, loss = 0.0009822326246649027
iteration 209, loss = 0.0011177122360095382
iteration 210, loss = 0.0009135347791016102
iteration 211, loss = 0.0009357861126773059
iteration 212, loss = 0.001222208491526544
iteration 213, loss = 0.000862985965795815
iteration 214, loss = 0.0012263646349310875
iteration 215, loss = 0.0007220825063996017
iteration 216, loss = 0.0009322594851255417
iteration 217, loss = 0.0008046046132221818
iteration 218, loss = 0.0010363057954236865
iteration 219, loss = 0.0008108965703286231
iteration 220, loss = 0.0015811496414244175
iteration 221, loss = 0.0007442020578309894
iteration 222, loss = 0.0007513813907280564
iteration 223, loss = 0.0009055319242179394
iteration 224, loss = 0.0010943067027255893
iteration 225, loss = 0.0007215634104795754
iteration 226, loss = 0.0007452420541085303
iteration 227, loss = 0.0017162051517516375
iteration 228, loss = 0.0016068352852016687
iteration 229, loss = 0.00075712357647717
iteration 230, loss = 0.0007950921426527202
iteration 231, loss = 0.0007982274983078241
iteration 232, loss = 0.0008123467559926212
iteration 233, loss = 0.000883718254044652
iteration 234, loss = 0.00198661582544446
iteration 235, loss = 0.0008548268815502524
iteration 236, loss = 0.0010758123826235533
iteration 237, loss = 0.0010963274398818612
iteration 238, loss = 0.000997143448330462
iteration 239, loss = 0.0009465890470892191
iteration 240, loss = 0.0008650672971270978
iteration 241, loss = 0.0010907212272286415
iteration 242, loss = 0.0008983112638816237
iteration 243, loss = 0.0008037416264414787
iteration 244, loss = 0.0007644376019015908
iteration 245, loss = 0.0018526409985497594
iteration 246, loss = 0.000706321734469384
iteration 247, loss = 0.001478190766647458
iteration 248, loss = 0.0008051377953961492
iteration 249, loss = 0.0007823026971891522
iteration 250, loss = 0.0008638387080281973
iteration 251, loss = 0.0015542072942480445
iteration 252, loss = 0.00092054542619735
iteration 253, loss = 0.0015842961147427559
iteration 254, loss = 0.0008114337688311934
iteration 255, loss = 0.000886362511664629
iteration 256, loss = 0.0007676107343286276
iteration 257, loss = 0.0008588205091655254
iteration 258, loss = 0.0010552066378295422
iteration 259, loss = 0.0009159089531749487
iteration 260, loss = 0.0009231367730535567
iteration 261, loss = 0.0006998036406002939
iteration 262, loss = 0.0008092526695691049
iteration 263, loss = 0.000741806230507791
iteration 264, loss = 0.0009712796309031546
iteration 265, loss = 0.00135520426556468
iteration 266, loss = 0.0009221537038683891
iteration 267, loss = 0.0008649688097648323
iteration 268, loss = 0.0012452438240870833
iteration 269, loss = 0.0010478427866473794
iteration 270, loss = 0.0008851991733536124
iteration 271, loss = 0.0008480210090056062
iteration 272, loss = 0.0008352055447176099
iteration 273, loss = 0.0010955434991046786
iteration 274, loss = 0.0008857749635353684
iteration 275, loss = 0.000825632712803781
iteration 276, loss = 0.000861705222632736
iteration 277, loss = 0.000979962875135243
iteration 278, loss = 0.001000203425064683
iteration 279, loss = 0.0009023690945468843
iteration 280, loss = 0.0011187100317329168
iteration 281, loss = 0.000851490069180727
iteration 282, loss = 0.001351162209175527
iteration 283, loss = 0.0015319696394726634
iteration 284, loss = 0.0010698873084038496
iteration 285, loss = 0.000834614154882729
iteration 286, loss = 0.0007759529980830848
iteration 287, loss = 0.0008219272131100297
iteration 288, loss = 0.0016263531288132071
iteration 289, loss = 0.0017128556501120329
iteration 290, loss = 0.000780425441917032
iteration 291, loss = 0.0008998307748697698
iteration 292, loss = 0.0015046312473714352
iteration 293, loss = 0.0011564292944967747
iteration 294, loss = 0.0008216825081035495
iteration 295, loss = 0.000801508838776499
iteration 296, loss = 0.0014266124926507473
iteration 297, loss = 0.001017599948681891
iteration 298, loss = 0.001970449462532997
iteration 299, loss = 0.00093379634199664
iteration 300, loss = 0.0008262327173724771
iteration 1, loss = 0.0011669122613966465
iteration 2, loss = 0.0007652630447410047
iteration 3, loss = 0.0007793360855430365
iteration 4, loss = 0.0007243401487357914
iteration 5, loss = 0.0009564197389408946
iteration 6, loss = 0.0012334706261754036
iteration 7, loss = 0.0006795770605094731
iteration 8, loss = 0.0011388056445866823
iteration 9, loss = 0.0008661330211907625
iteration 10, loss = 0.0006836181273683906
iteration 11, loss = 0.0012016505934298038
iteration 12, loss = 0.000813493796158582
iteration 13, loss = 0.0008110577473416924
iteration 14, loss = 0.0008214004919864237
iteration 15, loss = 0.0007723463932052255
iteration 16, loss = 0.0011533264769241214
iteration 17, loss = 0.0007455380982719362
iteration 18, loss = 0.0008870245655998588
iteration 19, loss = 0.0008572069928050041
iteration 20, loss = 0.0007599061354994774
iteration 21, loss = 0.0008959812694229186
iteration 22, loss = 0.0009626664686948061
iteration 23, loss = 0.000762433628551662
iteration 24, loss = 0.00172409787774086
iteration 25, loss = 0.0017633507959544659
iteration 26, loss = 0.0008908617310225964
iteration 27, loss = 0.00084983732085675
iteration 28, loss = 0.0007691445061936975
iteration 29, loss = 0.0016457907622680068
iteration 30, loss = 0.0009150013793259859
iteration 31, loss = 0.000882634602021426
iteration 32, loss = 0.0007700738497078419
iteration 33, loss = 0.0008675832068547606
iteration 34, loss = 0.0006977347657084465
iteration 35, loss = 0.0015198332257568836
iteration 36, loss = 0.0010092300362884998
iteration 37, loss = 0.00120204605627805
iteration 38, loss = 0.0014317585155367851
iteration 39, loss = 0.0008431163732893765
iteration 40, loss = 0.0011065331054851413
iteration 41, loss = 0.0007709602359682322
iteration 42, loss = 0.001010335050523281
iteration 43, loss = 0.0009701095405034721
iteration 44, loss = 0.0006992880953475833
iteration 45, loss = 0.0010682118590921164
iteration 46, loss = 0.0008043291745707393
iteration 47, loss = 0.0010673260549083352
iteration 48, loss = 0.0007790811941958964
iteration 49, loss = 0.0007907229592092335
iteration 50, loss = 0.0015529918018728495
iteration 51, loss = 0.000949828652665019
iteration 52, loss = 0.0007812487310729921
iteration 53, loss = 0.0012515382841229439
iteration 54, loss = 0.000919383717700839
iteration 55, loss = 0.0009229675633832812
iteration 56, loss = 0.001009831903502345
iteration 57, loss = 0.0014483683044090867
iteration 58, loss = 0.0006762006669305265
iteration 59, loss = 0.0010978266363963485
iteration 60, loss = 0.0007587620057165623
iteration 61, loss = 0.0008793171728029847
iteration 62, loss = 0.0007753877434879541
iteration 63, loss = 0.001177989412099123
iteration 64, loss = 0.0009053077083081007
iteration 65, loss = 0.0011518961982801557
iteration 66, loss = 0.0014445139095187187
iteration 67, loss = 0.0008933683857321739
iteration 68, loss = 0.0011988991172984242
iteration 69, loss = 0.0008223496261052787
iteration 70, loss = 0.0007907591643743217
iteration 71, loss = 0.0008606382180005312
iteration 72, loss = 0.0007255410309880972
iteration 73, loss = 0.0007689579506404698
iteration 74, loss = 0.001204344560392201
iteration 75, loss = 0.001020715688355267
iteration 76, loss = 0.0013829118106514215
iteration 77, loss = 0.0010398844024166465
iteration 78, loss = 0.0009221771615557373
iteration 79, loss = 0.0009997242596000433
iteration 80, loss = 0.000778619316406548
iteration 81, loss = 0.001545259146951139
iteration 82, loss = 0.0011436967179179192
iteration 83, loss = 0.0007556822383776307
iteration 84, loss = 0.0007771690143272281
iteration 85, loss = 0.0007776435231789947
iteration 86, loss = 0.0009015635005198419
iteration 87, loss = 0.0007988162105903029
iteration 88, loss = 0.001023565884679556
iteration 89, loss = 0.0009702321840450168
iteration 90, loss = 0.0007345617050305009
iteration 91, loss = 0.0009463259484618902
iteration 92, loss = 0.0007584842387586832
iteration 93, loss = 0.0008486138540320098
iteration 94, loss = 0.000994945177808404
iteration 95, loss = 0.0007723672897554934
iteration 96, loss = 0.0012398017570376396
iteration 97, loss = 0.0006691470043733716
iteration 98, loss = 0.0008097933605313301
iteration 99, loss = 0.0008339663618244231
iteration 100, loss = 0.0009975102730095387
iteration 101, loss = 0.0013489244738593698
iteration 102, loss = 0.0007389381644316018
iteration 103, loss = 0.0007462972425855696
iteration 104, loss = 0.0007734575774520636
iteration 105, loss = 0.0016632224433124065
iteration 106, loss = 0.0008135007228702307
iteration 107, loss = 0.0007113146129995584
iteration 108, loss = 0.000833454541862011
iteration 109, loss = 0.0008557061664760113
iteration 110, loss = 0.0010538478381931782
iteration 111, loss = 0.0008887199801392853
iteration 112, loss = 0.0008890831377357244
iteration 113, loss = 0.0016652027843520045
iteration 114, loss = 0.0008590428624302149
iteration 115, loss = 0.0009433414088562131
iteration 116, loss = 0.0008051898912526667
iteration 117, loss = 0.0007764280308037996
iteration 118, loss = 0.0008150008507072926
iteration 119, loss = 0.0008783463854342699
iteration 120, loss = 0.0008055299404077232
iteration 121, loss = 0.0009288918226957321
iteration 122, loss = 0.0011496705701574683
iteration 123, loss = 0.000953894981648773
iteration 124, loss = 0.0006941490573808551
iteration 125, loss = 0.0009235105244442821
iteration 126, loss = 0.0009392247884534299
iteration 127, loss = 0.0008331150165759027
iteration 128, loss = 0.0010143131949007511
iteration 129, loss = 0.0021607002709060907
iteration 130, loss = 0.0018628952093422413
iteration 131, loss = 0.0006681554368697107
iteration 132, loss = 0.0008332683355547488
iteration 133, loss = 0.0007362514152191579
iteration 134, loss = 0.000896305835340172
iteration 135, loss = 0.000972188136074692
iteration 136, loss = 0.00069943763082847
iteration 137, loss = 0.001065518008545041
iteration 138, loss = 0.0009625202510505915
iteration 139, loss = 0.0007438104948960245
iteration 140, loss = 0.0009742500260472298
iteration 141, loss = 0.0010792342945933342
iteration 142, loss = 0.0007614507921971381
iteration 143, loss = 0.0007117012282833457
iteration 144, loss = 0.001117246923968196
iteration 145, loss = 0.0007102788076736033
iteration 146, loss = 0.0007313789683394134
iteration 147, loss = 0.0008704204810783267
iteration 148, loss = 0.0010903418296948075
iteration 149, loss = 0.0011762978974729776
iteration 150, loss = 0.0017279285239055753
iteration 151, loss = 0.000953519600443542
iteration 152, loss = 0.0008421972743235528
iteration 153, loss = 0.0010163545375689864
iteration 154, loss = 0.0011732561979442835
iteration 155, loss = 0.00093081034719944
iteration 156, loss = 0.0008723192731849849
iteration 157, loss = 0.0008752825087867677
iteration 158, loss = 0.0011874053161591291
iteration 159, loss = 0.001137539278715849
iteration 160, loss = 0.0011751232668757439
iteration 161, loss = 0.0011718483874574304
iteration 162, loss = 0.0008445973508059978
iteration 163, loss = 0.0008509501349180937
iteration 164, loss = 0.0006874245591461658
iteration 165, loss = 0.0008050901815295219
iteration 166, loss = 0.000793134793639183
iteration 167, loss = 0.0008084390428848565
iteration 168, loss = 0.0008325863745994866
iteration 169, loss = 0.0023438269272446632
iteration 170, loss = 0.0010501946089789271
iteration 171, loss = 0.0006549631361849606
iteration 172, loss = 0.0011454102350398898
iteration 173, loss = 0.0018108203075826168
iteration 174, loss = 0.0007636089576408267
iteration 175, loss = 0.0012151373084634542
iteration 176, loss = 0.0016023952048271894
iteration 177, loss = 0.0008473021443933249
iteration 178, loss = 0.0011058761738240719
iteration 179, loss = 0.0007726387120783329
iteration 180, loss = 0.0007669060141779482
iteration 181, loss = 0.0008780010975897312
iteration 182, loss = 0.0008390276343561709
iteration 183, loss = 0.000995269394479692
iteration 184, loss = 0.0008877976797521114
iteration 185, loss = 0.0009457031264901161
iteration 186, loss = 0.0009709985461086035
iteration 187, loss = 0.0008182188612408936
iteration 188, loss = 0.0016048651887103915
iteration 189, loss = 0.0012443921295925975
iteration 190, loss = 0.0009235478355549276
iteration 191, loss = 0.0008686144719831645
iteration 192, loss = 0.0009939030278474092
iteration 193, loss = 0.0009448954369872808
iteration 194, loss = 0.0007088721031323075
iteration 195, loss = 0.0007640650728717446
iteration 196, loss = 0.0008300865301862359
iteration 197, loss = 0.0007322715246118605
iteration 198, loss = 0.0008984976448118687
iteration 199, loss = 0.0008580563589930534
iteration 200, loss = 0.001835392089560628
iteration 201, loss = 0.0008190664811991155
iteration 202, loss = 0.0007544870022684336
iteration 203, loss = 0.00085404486162588
iteration 204, loss = 0.0007954603061079979
iteration 205, loss = 0.0010374325793236494
iteration 206, loss = 0.0008668357622809708
iteration 207, loss = 0.0008012141333892941
iteration 208, loss = 0.000761093629989773
iteration 209, loss = 0.0007231993367895484
iteration 210, loss = 0.0011360724456608295
iteration 211, loss = 0.0012882148148491979
iteration 212, loss = 0.0009276840719394386
iteration 213, loss = 0.000825948198325932
iteration 214, loss = 0.0009652898879721761
iteration 215, loss = 0.0017307766247540712
iteration 216, loss = 0.0008874267223291099
iteration 217, loss = 0.0012653782032430172
iteration 218, loss = 0.0008761356002651155
iteration 219, loss = 0.0016382551984861493
iteration 220, loss = 0.0010929044801741838
iteration 221, loss = 0.000903962820302695
iteration 222, loss = 0.0009977725567296147
iteration 223, loss = 0.0008012200705707073
iteration 224, loss = 0.0008623078465461731
iteration 225, loss = 0.0008395718759857118
iteration 226, loss = 0.001231923932209611
iteration 227, loss = 0.0010676514357328415
iteration 228, loss = 0.0007759641157463193
iteration 229, loss = 0.0010011369595304132
iteration 230, loss = 0.0008152273367159069
iteration 231, loss = 0.0009650604333728552
iteration 232, loss = 0.0015303960535675287
iteration 233, loss = 0.0008126391912810504
iteration 234, loss = 0.0009162990027107298
iteration 235, loss = 0.0009254316100850701
iteration 236, loss = 0.0009368276223540306
iteration 237, loss = 0.0008655253332108259
iteration 238, loss = 0.000882396416272968
iteration 239, loss = 0.0009087160578928888
iteration 240, loss = 0.0008282230119220912
iteration 241, loss = 0.0009550765389576554
iteration 242, loss = 0.000758602749556303
iteration 243, loss = 0.0017597181722521782
iteration 244, loss = 0.001104369992390275
iteration 245, loss = 0.0009022771846503019
iteration 246, loss = 0.0012363854330033064
iteration 247, loss = 0.0009569186950102448
iteration 248, loss = 0.0012164307991042733
iteration 249, loss = 0.0008043015841394663
iteration 250, loss = 0.0008552711806260049
iteration 251, loss = 0.0007404036587104201
iteration 252, loss = 0.001214590622112155
iteration 253, loss = 0.0008339230553247035
iteration 254, loss = 0.0007213357021100819
iteration 255, loss = 0.0006722400430589914
iteration 256, loss = 0.0010961223160848022
iteration 257, loss = 0.0007843290222808719
iteration 258, loss = 0.0011198439169675112
iteration 259, loss = 0.0008579777204431593
iteration 260, loss = 0.0017145393649116158
iteration 261, loss = 0.0007411311962641776
iteration 262, loss = 0.0010991684393957257
iteration 263, loss = 0.002036590129137039
iteration 264, loss = 0.000905596069060266
iteration 265, loss = 0.0010648993775248528
iteration 266, loss = 0.000970269669778645
iteration 267, loss = 0.0007250253111124039
iteration 268, loss = 0.0009273553150705993
iteration 269, loss = 0.0006766978767700493
iteration 270, loss = 0.0008341968059539795
iteration 271, loss = 0.0008288966491818428
iteration 272, loss = 0.0010030617704614997
iteration 273, loss = 0.0010612112237140536
iteration 274, loss = 0.0008415584452450275
iteration 275, loss = 0.0010073169833049178
iteration 276, loss = 0.0007867671665735543
iteration 277, loss = 0.0008600156288594007
iteration 278, loss = 0.000862959714140743
iteration 279, loss = 0.0015990440733730793
iteration 280, loss = 0.000747077923733741
iteration 281, loss = 0.001017203088849783
iteration 282, loss = 0.000876478967256844
iteration 283, loss = 0.0008020672248676419
iteration 284, loss = 0.0008243995252996683
iteration 285, loss = 0.0008082024869509041
iteration 286, loss = 0.0008743226644583046
iteration 287, loss = 0.0010665527079254389
iteration 288, loss = 0.0010139425285160542
iteration 289, loss = 0.0008589339558966458
iteration 290, loss = 0.0009175279992632568
iteration 291, loss = 0.0008112759096547961
iteration 292, loss = 0.0008396410848945379
iteration 293, loss = 0.001430076314136386
iteration 294, loss = 0.000766333076171577
iteration 295, loss = 0.0008476011571474373
iteration 296, loss = 0.0015604704385623336
iteration 297, loss = 0.0008229861850850284
iteration 298, loss = 0.00117150554433465
iteration 299, loss = 0.0008893461781553924
iteration 300, loss = 0.0022065809462219477
iteration 1, loss = 0.0010603133123368025
iteration 2, loss = 0.0016045371303334832
iteration 3, loss = 0.000820753863081336
iteration 4, loss = 0.0008957242243923247
iteration 5, loss = 0.0007843258790671825
iteration 6, loss = 0.0010102993110194802
iteration 7, loss = 0.0010756079573184252
iteration 8, loss = 0.000868548930156976
iteration 9, loss = 0.0007831533439457417
iteration 10, loss = 0.0008399019716307521
iteration 11, loss = 0.0011689027305692434
iteration 12, loss = 0.0012202567886561155
iteration 13, loss = 0.0010267095640301704
iteration 14, loss = 0.0007813244010321796
iteration 15, loss = 0.0007314837421290576
iteration 16, loss = 0.0008127128821797669
iteration 17, loss = 0.0008370399591512978
iteration 18, loss = 0.0008715047151781619
iteration 19, loss = 0.0012190573615953326
iteration 20, loss = 0.001686173607595265
iteration 21, loss = 0.0008046076400205493
iteration 22, loss = 0.0007797210710123181
iteration 23, loss = 0.0007394708227366209
iteration 24, loss = 0.0009689267608337104
iteration 25, loss = 0.0010599467204883695
iteration 26, loss = 0.0007904980448074639
iteration 27, loss = 0.0012910000514239073
iteration 28, loss = 0.0007390840910375118
iteration 29, loss = 0.0007766797789372504
iteration 30, loss = 0.0009052050299942493
iteration 31, loss = 0.0015870369970798492
iteration 32, loss = 0.0008554724627174437
iteration 33, loss = 0.0012902747839689255
iteration 34, loss = 0.000869273382704705
iteration 35, loss = 0.0010071202414110303
iteration 36, loss = 0.0008963357540778816
iteration 37, loss = 0.001174918026663363
iteration 38, loss = 0.0013996268389746547
iteration 39, loss = 0.0008901915280148387
iteration 40, loss = 0.0007214756915345788
iteration 41, loss = 0.0010363026522099972
iteration 42, loss = 0.0009102725889533758
iteration 43, loss = 0.0007430436671711504
iteration 44, loss = 0.0009219209896400571
iteration 45, loss = 0.0009942245669662952
iteration 46, loss = 0.0016844080528244376
iteration 47, loss = 0.0007682894356548786
iteration 48, loss = 0.0008342392393387854
iteration 49, loss = 0.0008245776407420635
iteration 50, loss = 0.0011155643733218312
iteration 51, loss = 0.0008175037801265717
iteration 52, loss = 0.0007797252619639039
iteration 53, loss = 0.0007755412952974439
iteration 54, loss = 0.0007960756192915142
iteration 55, loss = 0.0010520323412492871
iteration 56, loss = 0.000946030137129128
iteration 57, loss = 0.0008602893212810159
iteration 58, loss = 0.0015722332755103707
iteration 59, loss = 0.0009940607706084847
iteration 60, loss = 0.0007726447656750679
iteration 61, loss = 0.0007876064628362656
iteration 62, loss = 0.0008061151602305472
iteration 63, loss = 0.0008702114573679864
iteration 64, loss = 0.0016768182395026088
iteration 65, loss = 0.0008001109235920012
iteration 66, loss = 0.0008766742539592087
iteration 67, loss = 0.0007860687328502536
iteration 68, loss = 0.000749999366234988
iteration 69, loss = 0.0010191206820309162
iteration 70, loss = 0.0008772304281592369
iteration 71, loss = 0.0012511650566011667
iteration 72, loss = 0.0009276003111153841
iteration 73, loss = 0.001011214917525649
iteration 74, loss = 0.0009877479169517756
iteration 75, loss = 0.001013939268887043
iteration 76, loss = 0.0010808140505105257
iteration 77, loss = 0.0007573724142275751
iteration 78, loss = 0.0008024161797948182
iteration 79, loss = 0.001063937321305275
iteration 80, loss = 0.001852283370681107
iteration 81, loss = 0.0009768817108124495
iteration 82, loss = 0.001205623266287148
iteration 83, loss = 0.001180324936285615
iteration 84, loss = 0.0015905790496617556
iteration 85, loss = 0.0009036115952767432
iteration 86, loss = 0.0007200489053502679
iteration 87, loss = 0.0007304215105250478
iteration 88, loss = 0.0008633490651845932
iteration 89, loss = 0.0010126600973308086
iteration 90, loss = 0.0008941759588196874
iteration 91, loss = 0.0010516109177842736
iteration 92, loss = 0.0008974084630608559
iteration 93, loss = 0.0008647675858810544
iteration 94, loss = 0.0009304564446210861
iteration 95, loss = 0.0008623358444310725
iteration 96, loss = 0.0013001797487959266
iteration 97, loss = 0.0007551945745944977
iteration 98, loss = 0.0006932456744834781
iteration 99, loss = 0.0007607141742482781
iteration 100, loss = 0.0008277788874693215
iteration 101, loss = 0.0008093110518530011
iteration 102, loss = 0.0008010072633624077
iteration 103, loss = 0.000807924079708755
iteration 104, loss = 0.0009047792409546673
iteration 105, loss = 0.0008997037657536566
iteration 106, loss = 0.0008229591767303646
iteration 107, loss = 0.0012833246728405356
iteration 108, loss = 0.0009925863705575466
iteration 109, loss = 0.001733731827698648
iteration 110, loss = 0.000796217005699873
iteration 111, loss = 0.0009518528822809458
iteration 112, loss = 0.0009597385651431978
iteration 113, loss = 0.0010999208316206932
iteration 114, loss = 0.0008487374288961291
iteration 115, loss = 0.0011925406288355589
iteration 116, loss = 0.0012000531423836946
iteration 117, loss = 0.0008770430576987565
iteration 118, loss = 0.0008835764019750059
iteration 119, loss = 0.0008117471588775516
iteration 120, loss = 0.0008146376931108534
iteration 121, loss = 0.001000629155896604
iteration 122, loss = 0.0008202417520806193
iteration 123, loss = 0.0009719168301671743
iteration 124, loss = 0.0009053853573277593
iteration 125, loss = 0.000788859382737428
iteration 126, loss = 0.0007642428390681744
iteration 127, loss = 0.0010888694087043405
iteration 128, loss = 0.0007630647160112858
iteration 129, loss = 0.0009070257074199617
iteration 130, loss = 0.0008084787987172604
iteration 131, loss = 0.0012280410155653954
iteration 132, loss = 0.0009826452005654573
iteration 133, loss = 0.0009770102333277464
iteration 134, loss = 0.0009542000479996204
iteration 135, loss = 0.0008484904537908733
iteration 136, loss = 0.0008654319099150598
iteration 137, loss = 0.0012662406079471111
iteration 138, loss = 0.001840457203797996
iteration 139, loss = 0.0008049419848248363
iteration 140, loss = 0.0006752416375093162
iteration 141, loss = 0.0011668774532154202
iteration 142, loss = 0.0011215918930247426
iteration 143, loss = 0.0016607558354735374
iteration 144, loss = 0.0007954173488542438
iteration 145, loss = 0.000942543672863394
iteration 146, loss = 0.0006964532076381147
iteration 147, loss = 0.0009517555590718985
iteration 148, loss = 0.0007961206138134003
iteration 149, loss = 0.0013711355859413743
iteration 150, loss = 0.0010813910048455
iteration 151, loss = 0.0008362422231584787
iteration 152, loss = 0.0016848391387611628
iteration 153, loss = 0.000890823663212359
iteration 154, loss = 0.0014837990747764707
iteration 155, loss = 0.0008956547826528549
iteration 156, loss = 0.0007619124371558428
iteration 157, loss = 0.0012372374767437577
iteration 158, loss = 0.0008592649828642607
iteration 159, loss = 0.001103737740777433
iteration 160, loss = 0.0007944642566144466
iteration 161, loss = 0.001650865189731121
iteration 162, loss = 0.0017568550538271666
iteration 163, loss = 0.0008866945863701403
iteration 164, loss = 0.0009249645518139005
iteration 165, loss = 0.0014717784943059087
iteration 166, loss = 0.0007958768401294947
iteration 167, loss = 0.0014382947701960802
iteration 168, loss = 0.0007993685430847108
iteration 169, loss = 0.000951601134147495
iteration 170, loss = 0.0015428216429427266
iteration 171, loss = 0.0006832730141468346
iteration 172, loss = 0.0014335690066218376
iteration 173, loss = 0.0009191015851683915
iteration 174, loss = 0.0007711248472332954
iteration 175, loss = 0.0010357347782701254
iteration 176, loss = 0.0006866454496048391
iteration 177, loss = 0.0007959756185300648
iteration 178, loss = 0.0009016739204525948
iteration 179, loss = 0.0006856287945993245
iteration 180, loss = 0.0008592567755840719
iteration 181, loss = 0.0007446187664754689
iteration 182, loss = 0.0015215405728667974
iteration 183, loss = 0.0008449743036180735
iteration 184, loss = 0.0007802596082910895
iteration 185, loss = 0.0009595045121386647
iteration 186, loss = 0.0008142798324115574
iteration 187, loss = 0.000805420393589884
iteration 188, loss = 0.000896331446710974
iteration 189, loss = 0.000831959187053144
iteration 190, loss = 0.0010804719058796763
iteration 191, loss = 0.0008607670315541327
iteration 192, loss = 0.0010525789111852646
iteration 193, loss = 0.0008799926145002246
iteration 194, loss = 0.001200974453240633
iteration 195, loss = 0.0008079169783741236
iteration 196, loss = 0.0015801958506926894
iteration 197, loss = 0.0007581511163152754
iteration 198, loss = 0.0011049488093703985
iteration 199, loss = 0.001108346856199205
iteration 200, loss = 0.0016727240290492773
iteration 201, loss = 0.0009854568634182215
iteration 202, loss = 0.0009008637280203402
iteration 203, loss = 0.0012746911961585283
iteration 204, loss = 0.0011173064121976495
iteration 205, loss = 0.000885692541487515
iteration 206, loss = 0.0007472678553313017
iteration 207, loss = 0.0008489873725920916
iteration 208, loss = 0.0011831946903839707
iteration 209, loss = 0.0008068359456956387
iteration 210, loss = 0.0007164205890148878
iteration 211, loss = 0.0007597054936923087
iteration 212, loss = 0.0007782833999954164
iteration 213, loss = 0.0007327937637455761
iteration 214, loss = 0.0008552856161259115
iteration 215, loss = 0.001454601646400988
iteration 216, loss = 0.0007662855205126107
iteration 217, loss = 0.0008636338170617819
iteration 218, loss = 0.0008916769875213504
iteration 219, loss = 0.000815677922219038
iteration 220, loss = 0.0015451617073267698
iteration 221, loss = 0.0010428708046674728
iteration 222, loss = 0.0008399343932978809
iteration 223, loss = 0.0009309013257734478
iteration 224, loss = 0.0008787798578850925
iteration 225, loss = 0.000838780717458576
iteration 226, loss = 0.0007172590703703463
iteration 227, loss = 0.00172200589440763
iteration 228, loss = 0.0006866460316814482
iteration 229, loss = 0.0007036696188151836
iteration 230, loss = 0.0008141940925270319
iteration 231, loss = 0.0008657447178848088
iteration 232, loss = 0.0007503728847950697
iteration 233, loss = 0.000724064651876688
iteration 234, loss = 0.0007756598643027246
iteration 235, loss = 0.0013719775015488267
iteration 236, loss = 0.0009199022315442562
iteration 237, loss = 0.00089383585145697
iteration 238, loss = 0.0007071432191878557
iteration 239, loss = 0.0006846861797384918
iteration 240, loss = 0.0017174608074128628
iteration 241, loss = 0.0009111791150644422
iteration 242, loss = 0.0009397511603310704
iteration 243, loss = 0.000831392128020525
iteration 244, loss = 0.0011284892680123448
iteration 245, loss = 0.0007837832090444863
iteration 246, loss = 0.0009616706520318985
iteration 247, loss = 0.0012238593772053719
iteration 248, loss = 0.0008016714127734303
iteration 249, loss = 0.0012755220523104072
iteration 250, loss = 0.0011985701275989413
iteration 251, loss = 0.0008394093601964414
iteration 252, loss = 0.0008852994651533663
iteration 253, loss = 0.001075536711141467
iteration 254, loss = 0.0009554253774695098
iteration 255, loss = 0.0007260735728777945
iteration 256, loss = 0.0007858557510189712
iteration 257, loss = 0.0011557473335415125
iteration 258, loss = 0.001542426529340446
iteration 259, loss = 0.0011170476209372282
iteration 260, loss = 0.000876343809068203
iteration 261, loss = 0.0007759638829156756
iteration 262, loss = 0.001783100189641118
iteration 263, loss = 0.0007457621395587921
iteration 264, loss = 0.0010561472736299038
iteration 265, loss = 0.0009680304792709649
iteration 266, loss = 0.000753690954297781
iteration 267, loss = 0.0009952186373993754
iteration 268, loss = 0.0010532679734751582
iteration 269, loss = 0.0017738956958055496
iteration 270, loss = 0.0008403776446357369
iteration 271, loss = 0.0007064072997309268
iteration 272, loss = 0.0006957524456083775
iteration 273, loss = 0.0009050932712852955
iteration 274, loss = 0.0008025605930015445
iteration 275, loss = 0.0008042397093959153
iteration 276, loss = 0.0007340963929891586
iteration 277, loss = 0.0007206756854429841
iteration 278, loss = 0.0007566802087239921
iteration 279, loss = 0.0013447112869471312
iteration 280, loss = 0.001597498543560505
iteration 281, loss = 0.0007942061638459563
iteration 282, loss = 0.0009052653331309557
iteration 283, loss = 0.0008060642867349088
iteration 284, loss = 0.0007910287822596729
iteration 285, loss = 0.0011461852118372917
iteration 286, loss = 0.000945547129958868
iteration 287, loss = 0.00193981418851763
iteration 288, loss = 0.0009624279919080436
iteration 289, loss = 0.0010724500752985477
iteration 290, loss = 0.0007834986899979413
iteration 291, loss = 0.0007727593765594065
iteration 292, loss = 0.0009116794099099934
iteration 293, loss = 0.0011711822589859366
iteration 294, loss = 0.0008004125556908548
iteration 295, loss = 0.0008399909711442888
iteration 296, loss = 0.0007193913916125894
iteration 297, loss = 0.001074888277798891
iteration 298, loss = 0.0007285328465513885
iteration 299, loss = 0.0007810076931491494
iteration 300, loss = 0.0008538974216207862
iteration 1, loss = 0.0006744489073753357
iteration 2, loss = 0.0008935863152146339
iteration 3, loss = 0.000955440104007721
iteration 4, loss = 0.0011789901182055473
iteration 5, loss = 0.0010739224962890148
iteration 6, loss = 0.0010045190574601293
iteration 7, loss = 0.0009734440827742219
iteration 8, loss = 0.0008711310802027583
iteration 9, loss = 0.0009372299537062645
iteration 10, loss = 0.0013811802491545677
iteration 11, loss = 0.00090231210924685
iteration 12, loss = 0.0014628063654527068
iteration 13, loss = 0.0009278112556785345
iteration 14, loss = 0.000790253805462271
iteration 15, loss = 0.0007287224289029837
iteration 16, loss = 0.0009367088787257671
iteration 17, loss = 0.0007600644021295011
iteration 18, loss = 0.001222758088260889
iteration 19, loss = 0.0007669273181818426
iteration 20, loss = 0.0010997912613674998
iteration 21, loss = 0.0010426355293020606
iteration 22, loss = 0.000868949166033417
iteration 23, loss = 0.0011539844563230872
iteration 24, loss = 0.0012997700832784176
iteration 25, loss = 0.0010293008526787162
iteration 26, loss = 0.0010129293659701943
iteration 27, loss = 0.0009422664879821241
iteration 28, loss = 0.000864459783770144
iteration 29, loss = 0.00118725816719234
iteration 30, loss = 0.0014129569754004478
iteration 31, loss = 0.0007918806513771415
iteration 32, loss = 0.0008114894153550267
iteration 33, loss = 0.0013843413908034563
iteration 34, loss = 0.0008139329729601741
iteration 35, loss = 0.0013708710903301835
iteration 36, loss = 0.000776806497015059
iteration 37, loss = 0.0017087898449972272
iteration 38, loss = 0.0011059533571824431
iteration 39, loss = 0.0009746892610564828
iteration 40, loss = 0.0011906916042789817
iteration 41, loss = 0.0007379481103271246
iteration 42, loss = 0.001663880655542016
iteration 43, loss = 0.0007807220099493861
iteration 44, loss = 0.0009218101040460169
iteration 45, loss = 0.0008170503424480557
iteration 46, loss = 0.0010778940049931407
iteration 47, loss = 0.0010209388565272093
iteration 48, loss = 0.0015286781126633286
iteration 49, loss = 0.0008828278514556587
iteration 50, loss = 0.0008087249589152634
iteration 51, loss = 0.001052746083587408
iteration 52, loss = 0.0009090026724152267
iteration 53, loss = 0.0008467817679047585
iteration 54, loss = 0.0008045000722631812
iteration 55, loss = 0.00080999726196751
iteration 56, loss = 0.0008430727757513523
iteration 57, loss = 0.0007146102725528181
iteration 58, loss = 0.0008856720523908734
iteration 59, loss = 0.0007715873070992529
iteration 60, loss = 0.0007911404245533049
iteration 61, loss = 0.0006819360423833132
iteration 62, loss = 0.0007406502845697105
iteration 63, loss = 0.002230254467576742
iteration 64, loss = 0.00143709615804255
iteration 65, loss = 0.0008738368633203208
iteration 66, loss = 0.0008977716788649559
iteration 67, loss = 0.0009522479958832264
iteration 68, loss = 0.0016992526361718774
iteration 69, loss = 0.0016665770672261715
iteration 70, loss = 0.0011849096044898033
iteration 71, loss = 0.0007859961478970945
iteration 72, loss = 0.0011263350024819374
iteration 73, loss = 0.001089777215383947
iteration 74, loss = 0.0008101504063233733
iteration 75, loss = 0.0017205188050866127
iteration 76, loss = 0.0011760554043576121
iteration 77, loss = 0.0011997980764135718
iteration 78, loss = 0.0007916730246506631
iteration 79, loss = 0.0007596571231260896
iteration 80, loss = 0.0009360536932945251
iteration 81, loss = 0.0007959649083204567
iteration 82, loss = 0.0007697856053709984
iteration 83, loss = 0.0008543431176804006
iteration 84, loss = 0.0017810830613598228
iteration 85, loss = 0.0011053977068513632
iteration 86, loss = 0.000867545953951776
iteration 87, loss = 0.0008692110422998667
iteration 88, loss = 0.0008651626412756741
iteration 89, loss = 0.0007421455229632556
iteration 90, loss = 0.0014104277361184359
iteration 91, loss = 0.0006480317679233849
iteration 92, loss = 0.000812278245575726
iteration 93, loss = 0.002053244272246957
iteration 94, loss = 0.0008852434111759067
iteration 95, loss = 0.0012939542066305876
iteration 96, loss = 0.0007305992185138166
iteration 97, loss = 0.0009727994911372662
iteration 98, loss = 0.0007862189086154103
iteration 99, loss = 0.0009387742029502988
iteration 100, loss = 0.0007855601143091917
iteration 101, loss = 0.0010602085385471582
iteration 102, loss = 0.0012064307229593396
iteration 103, loss = 0.0009319748496636748
iteration 104, loss = 0.0008046552538871765
iteration 105, loss = 0.0008716690936125815
iteration 106, loss = 0.0008222858305089176
iteration 107, loss = 0.001634126529097557
iteration 108, loss = 0.0006765126017853618
iteration 109, loss = 0.0015549205709248781
iteration 110, loss = 0.0008662312757223845
iteration 111, loss = 0.0009007115149870515
iteration 112, loss = 0.001023315591737628
iteration 113, loss = 0.0007598307565785944
iteration 114, loss = 0.0008652195683680475
iteration 115, loss = 0.0008424697443842888
iteration 116, loss = 0.0015246111433953047
iteration 117, loss = 0.000872019212692976
iteration 118, loss = 0.000993038760498166
iteration 119, loss = 0.0007900106138549745
iteration 120, loss = 0.0012661907821893692
iteration 121, loss = 0.0007332913810387254
iteration 122, loss = 0.0007457001483999193
iteration 123, loss = 0.0008163314778357744
iteration 124, loss = 0.0007651883643120527
iteration 125, loss = 0.0008178236894309521
iteration 126, loss = 0.0011217655846849084
iteration 127, loss = 0.0009991294937208295
iteration 128, loss = 0.0006809940678067505
iteration 129, loss = 0.0008588102646172047
iteration 130, loss = 0.0007804866181686521
iteration 131, loss = 0.0011910496978089213
iteration 132, loss = 0.001458956627175212
iteration 133, loss = 0.0009700714726932347
iteration 134, loss = 0.0008310513803735375
iteration 135, loss = 0.0009370649931952357
iteration 136, loss = 0.0007511632284149528
iteration 137, loss = 0.0011275092838332057
iteration 138, loss = 0.0018252857262268662
iteration 139, loss = 0.0009386021411046386
iteration 140, loss = 0.0007948659476824105
iteration 141, loss = 0.000934320967644453
iteration 142, loss = 0.0014462943654507399
iteration 143, loss = 0.0008922251872718334
iteration 144, loss = 0.0010308027267456055
iteration 145, loss = 0.0009858791017904878
iteration 146, loss = 0.0007684663869440556
iteration 147, loss = 0.0008106788736768067
iteration 148, loss = 0.001227637636475265
iteration 149, loss = 0.000893367687240243
iteration 150, loss = 0.0018495339900255203
iteration 151, loss = 0.0009706427226774395
iteration 152, loss = 0.0014358944026753306
iteration 153, loss = 0.0015540102031081915
iteration 154, loss = 0.0008670982206240296
iteration 155, loss = 0.0008602066081948578
iteration 156, loss = 0.0007945277029648423
iteration 157, loss = 0.0008898071828298271
iteration 158, loss = 0.000730937987100333
iteration 159, loss = 0.0006993500865064561
iteration 160, loss = 0.0007998508517630398
iteration 161, loss = 0.0009011286310851574
iteration 162, loss = 0.0007474867161363363
iteration 163, loss = 0.0010262756841257215
iteration 164, loss = 0.0011379868956282735
iteration 165, loss = 0.0012363248970359564
iteration 166, loss = 0.0010206317529082298
iteration 167, loss = 0.00090072734747082
iteration 168, loss = 0.0010460105258971453
iteration 169, loss = 0.001106400042772293
iteration 170, loss = 0.0009450376965105534
iteration 171, loss = 0.0011631293455138803
iteration 172, loss = 0.0016767848283052444
iteration 173, loss = 0.0008913475903682411
iteration 174, loss = 0.0010616591898724437
iteration 175, loss = 0.0007576430216431618
iteration 176, loss = 0.0008290869300253689
iteration 177, loss = 0.0014647560892626643
iteration 178, loss = 0.0009243416134268045
iteration 179, loss = 0.001311795087531209
iteration 180, loss = 0.000750127190258354
iteration 181, loss = 0.0009576489683240652
iteration 182, loss = 0.0007502217194996774
iteration 183, loss = 0.0010367343202233315
iteration 184, loss = 0.0007954094326123595
iteration 185, loss = 0.0007516114274039865
iteration 186, loss = 0.0011693625710904598
iteration 187, loss = 0.0008743249345570803
iteration 188, loss = 0.0010277313413098454
iteration 189, loss = 0.0007583365077152848
iteration 190, loss = 0.0010798901785165071
iteration 191, loss = 0.0010646634036675096
iteration 192, loss = 0.0010328562930226326
iteration 193, loss = 0.0019346511689946055
iteration 194, loss = 0.0015332406619563699
iteration 195, loss = 0.0009690404986031353
iteration 196, loss = 0.0009178259060718119
iteration 197, loss = 0.0009635220048949122
iteration 198, loss = 0.0014573627850040793
iteration 199, loss = 0.0007480825297534466
iteration 200, loss = 0.0011534927180036902
iteration 201, loss = 0.0010333701502531767
iteration 202, loss = 0.0006925069610588253
iteration 203, loss = 0.0008405640255659819
iteration 204, loss = 0.0010548151331022382
iteration 205, loss = 0.0006800587871111929
iteration 206, loss = 0.0007770785014145076
iteration 207, loss = 0.0009161664638668299
iteration 208, loss = 0.0009153154096566141
iteration 209, loss = 0.000763710995670408
iteration 210, loss = 0.0009512794204056263
iteration 211, loss = 0.0009932388784363866
iteration 212, loss = 0.0009827126050367951
iteration 213, loss = 0.0009753394406288862
iteration 214, loss = 0.0010655942605808377
iteration 215, loss = 0.0007733294041827321
iteration 216, loss = 0.0008910861215554178
iteration 217, loss = 0.001042746240273118
iteration 218, loss = 0.0008852171595208347
iteration 219, loss = 0.0007828606758266687
iteration 220, loss = 0.0008879489032551646
iteration 221, loss = 0.0009547912632115185
iteration 222, loss = 0.0009884388418868184
iteration 223, loss = 0.001092954771593213
iteration 224, loss = 0.0009249592549167573
iteration 225, loss = 0.0012277917703613639
iteration 226, loss = 0.0012009837664663792
iteration 227, loss = 0.0007409551762975752
iteration 228, loss = 0.0007343576871789992
iteration 229, loss = 0.0010978847276419401
iteration 230, loss = 0.0007615766953676939
iteration 231, loss = 0.0012299035442993045
iteration 232, loss = 0.000722424010746181
iteration 233, loss = 0.0008729571127332747
iteration 234, loss = 0.0008281526388600469
iteration 235, loss = 0.0008660673047415912
iteration 236, loss = 0.0008521807030774653
iteration 237, loss = 0.0009198773186653852
iteration 238, loss = 0.0008307805983349681
iteration 239, loss = 0.0007880193297751248
iteration 240, loss = 0.0008121552527882159
iteration 241, loss = 0.0009586958331055939
iteration 242, loss = 0.0010213531786575913
iteration 243, loss = 0.0011118341935798526
iteration 244, loss = 0.001193883828818798
iteration 245, loss = 0.0007903352961875498
iteration 246, loss = 0.0009847903857007623
iteration 247, loss = 0.0006974684656597674
iteration 248, loss = 0.0008526218589395285
iteration 249, loss = 0.0010887330863624811
iteration 250, loss = 0.0017736100126057863
iteration 251, loss = 0.001105436123907566
iteration 252, loss = 0.0007793576223775744
iteration 253, loss = 0.0008655780693516135
iteration 254, loss = 0.0008615152328275144
iteration 255, loss = 0.0012504651676863432
iteration 256, loss = 0.0006898110150359571
iteration 257, loss = 0.000802308670245111
iteration 258, loss = 0.0008153874659910798
iteration 259, loss = 0.0009126508375629783
iteration 260, loss = 0.001022932818159461
iteration 261, loss = 0.0008311198907904327
iteration 262, loss = 0.000957532087340951
iteration 263, loss = 0.0007939474890008569
iteration 264, loss = 0.0007170896860770881
iteration 265, loss = 0.001421179622411728
iteration 266, loss = 0.0007533647585660219
iteration 267, loss = 0.0007072193548083305
iteration 268, loss = 0.0007535326294600964
iteration 269, loss = 0.0008192602545022964
iteration 270, loss = 0.0007673969957977533
iteration 271, loss = 0.0008162592421285808
iteration 272, loss = 0.000795366067904979
iteration 273, loss = 0.0008320588385686278
iteration 274, loss = 0.0007581118261441588
iteration 275, loss = 0.0008149385685101151
iteration 276, loss = 0.0007889266707934439
iteration 277, loss = 0.0007281142170540988
iteration 278, loss = 0.0009751466568559408
iteration 279, loss = 0.000980651006102562
iteration 280, loss = 0.0007610584725625813
iteration 281, loss = 0.0007555209449492395
iteration 282, loss = 0.0014114024816080928
iteration 283, loss = 0.000678477284964174
iteration 284, loss = 0.0008954777731560171
iteration 285, loss = 0.0006521499017253518
iteration 286, loss = 0.000698251009453088
iteration 287, loss = 0.0007682872819714248
iteration 288, loss = 0.00083075778093189
iteration 289, loss = 0.0007848568493500352
iteration 290, loss = 0.0008668589289300144
iteration 291, loss = 0.0014746676897630095
iteration 292, loss = 0.000943063641898334
iteration 293, loss = 0.000792195787653327
iteration 294, loss = 0.000819060078356415
iteration 295, loss = 0.0008023110567592084
iteration 296, loss = 0.0007042550132609904
iteration 297, loss = 0.0017694277921691537
iteration 298, loss = 0.000787281256634742
iteration 299, loss = 0.0007682958384975791
iteration 300, loss = 0.0007285781321115792
iteration 1, loss = 0.001638152520172298
iteration 2, loss = 0.0008495486690662801
iteration 3, loss = 0.0010224005673080683
iteration 4, loss = 0.000784721109084785
iteration 5, loss = 0.0008002673275768757
iteration 6, loss = 0.0007378567825071514
iteration 7, loss = 0.000875632103998214
iteration 8, loss = 0.0008963520522229373
iteration 9, loss = 0.0014965954469516873
iteration 10, loss = 0.000799854751676321
iteration 11, loss = 0.0007260803249664605
iteration 12, loss = 0.001025049015879631
iteration 13, loss = 0.0007187221199274063
iteration 14, loss = 0.0009827733738347888
iteration 15, loss = 0.0009309974848292768
iteration 16, loss = 0.0007892088615335524
iteration 17, loss = 0.0009037225972861052
iteration 18, loss = 0.0009856597753241658
iteration 19, loss = 0.0013950482243672013
iteration 20, loss = 0.0007173297344706953
iteration 21, loss = 0.0011142489966005087
iteration 22, loss = 0.0007970782462507486
iteration 23, loss = 0.0010415570577606559
iteration 24, loss = 0.0007481711800210178
iteration 25, loss = 0.0008428617147728801
iteration 26, loss = 0.0009363431017845869
iteration 27, loss = 0.0011031226022168994
iteration 28, loss = 0.0012955974088981748
iteration 29, loss = 0.0007376434514299035
iteration 30, loss = 0.0013055262388661504
iteration 31, loss = 0.0008207546197809279
iteration 32, loss = 0.0008355920435860753
iteration 33, loss = 0.0009355992078781128
iteration 34, loss = 0.0011073289206251502
iteration 35, loss = 0.0008268986130133271
iteration 36, loss = 0.0012011270737275481
iteration 37, loss = 0.0011289177928119898
iteration 38, loss = 0.0007058713235892355
iteration 39, loss = 0.0007899924530647695
iteration 40, loss = 0.0009268387220799923
iteration 41, loss = 0.0011141865979880095
iteration 42, loss = 0.0007346220081672072
iteration 43, loss = 0.0008235130226239562
iteration 44, loss = 0.0009017564007081091
iteration 45, loss = 0.0007592540932819247
iteration 46, loss = 0.0013916936004534364
iteration 47, loss = 0.0008817784255370498
iteration 48, loss = 0.0009054895490407944
iteration 49, loss = 0.0008115257951430976
iteration 50, loss = 0.001860268646851182
iteration 51, loss = 0.000922049512155354
iteration 52, loss = 0.0007601584657095373
iteration 53, loss = 0.0009789945324882865
iteration 54, loss = 0.0008174233371391892
iteration 55, loss = 0.0012799325631931424
iteration 56, loss = 0.0019737256225198507
iteration 57, loss = 0.0008916961378417909
iteration 58, loss = 0.0007182335248216987
iteration 59, loss = 0.000798416614998132
iteration 60, loss = 0.0008943526772782207
iteration 61, loss = 0.0010312547674402595
iteration 62, loss = 0.0009363277931697667
iteration 63, loss = 0.0008955492521636188
iteration 64, loss = 0.0007147056167013943
iteration 65, loss = 0.0010693954536691308
iteration 66, loss = 0.0011221310123801231
iteration 67, loss = 0.0009900254663079977
iteration 68, loss = 0.0006958448211662471
iteration 69, loss = 0.0009266920387744904
iteration 70, loss = 0.0011685285717248917
iteration 71, loss = 0.0010672956705093384
iteration 72, loss = 0.0007250936469063163
iteration 73, loss = 0.0011846625711768866
iteration 74, loss = 0.001522580860182643
iteration 75, loss = 0.00078879005741328
iteration 76, loss = 0.0007052394794300199
iteration 77, loss = 0.0008701194310560822
iteration 78, loss = 0.0008584710885770619
iteration 79, loss = 0.0007172647747211158
iteration 80, loss = 0.0009062219760380685
iteration 81, loss = 0.0007306092884391546
iteration 82, loss = 0.0009887961205095053
iteration 83, loss = 0.0008262366172857583
iteration 84, loss = 0.0008109903428703547
iteration 85, loss = 0.0012070595985278487
iteration 86, loss = 0.0009685861878097057
iteration 87, loss = 0.0006438247510232031
iteration 88, loss = 0.0011094202054664493
iteration 89, loss = 0.0009176762541756034
iteration 90, loss = 0.0010254899971187115
iteration 91, loss = 0.0009168382384814322
iteration 92, loss = 0.0011775402817875147
iteration 93, loss = 0.000789114972576499
iteration 94, loss = 0.0007221967098303139
iteration 95, loss = 0.0008996028918772936
iteration 96, loss = 0.0011274866992607713
iteration 97, loss = 0.0010080263018608093
iteration 98, loss = 0.0007788485381752253
iteration 99, loss = 0.0010058018378913403
iteration 100, loss = 0.0007821868639439344
iteration 101, loss = 0.0013758656568825245
iteration 102, loss = 0.0008774352609179914
iteration 103, loss = 0.0007905811653472483
iteration 104, loss = 0.0010531819425523281
iteration 105, loss = 0.0010487940162420273
iteration 106, loss = 0.0006783654098398983
iteration 107, loss = 0.0008695267606526613
iteration 108, loss = 0.0006988238892517984
iteration 109, loss = 0.0015522864414379
iteration 110, loss = 0.001615840825252235
iteration 111, loss = 0.0011474434286355972
iteration 112, loss = 0.001241488615050912
iteration 113, loss = 0.0008199685253202915
iteration 114, loss = 0.0012638088082894683
iteration 115, loss = 0.001145795569755137
iteration 116, loss = 0.0010715400567278266
iteration 117, loss = 0.0009244218235835433
iteration 118, loss = 0.0007643700228072703
iteration 119, loss = 0.0007874280563555658
iteration 120, loss = 0.000756510067731142
iteration 121, loss = 0.0009844052838161588
iteration 122, loss = 0.0008048297022469342
iteration 123, loss = 0.0008076985832303762
iteration 124, loss = 0.001216326025314629
iteration 125, loss = 0.0009136215085163713
iteration 126, loss = 0.0006528054364025593
iteration 127, loss = 0.0008703959756530821
iteration 128, loss = 0.0008179568685591221
iteration 129, loss = 0.0008629013900645077
iteration 130, loss = 0.0007409535464830697
iteration 131, loss = 0.0010674012592062354
iteration 132, loss = 0.0006926279747858644
iteration 133, loss = 0.0010631680488586426
iteration 134, loss = 0.0008507776074111462
iteration 135, loss = 0.0013623344711959362
iteration 136, loss = 0.0010250501800328493
iteration 137, loss = 0.0008609910728409886
iteration 138, loss = 0.000766162876971066
iteration 139, loss = 0.001533767906948924
iteration 140, loss = 0.0008203366887755692
iteration 141, loss = 0.0008663471671752632
iteration 142, loss = 0.0017324064392596483
iteration 143, loss = 0.0012346016010269523
iteration 144, loss = 0.0007354645058512688
iteration 145, loss = 0.0012263577664270997
iteration 146, loss = 0.0018588333623483777
iteration 147, loss = 0.000811995763797313
iteration 148, loss = 0.0017037686193361878
iteration 149, loss = 0.0007778482395224273
iteration 150, loss = 0.000669302768073976
iteration 151, loss = 0.0012927725911140442
iteration 152, loss = 0.0008212797110900283
iteration 153, loss = 0.0010799731826409698
iteration 154, loss = 0.000782327086199075
iteration 155, loss = 0.0007759509026072919
iteration 156, loss = 0.0008970731869339943
iteration 157, loss = 0.0007655583322048187
iteration 158, loss = 0.0009083044715225697
iteration 159, loss = 0.0009336591465398669
iteration 160, loss = 0.0008525390294380486
iteration 161, loss = 0.0010937154293060303
iteration 162, loss = 0.001393085578456521
iteration 163, loss = 0.0007607340230606496
iteration 164, loss = 0.0011775849852710962
iteration 165, loss = 0.0010150822345167398
iteration 166, loss = 0.0010052494471892715
iteration 167, loss = 0.0011080773547291756
iteration 168, loss = 0.0007398035959340632
iteration 169, loss = 0.0008962795254774392
iteration 170, loss = 0.0007464706432074308
iteration 171, loss = 0.0009193071164190769
iteration 172, loss = 0.0014956491068005562
iteration 173, loss = 0.0008175846305675805
iteration 174, loss = 0.0009714915649965405
iteration 175, loss = 0.0008010212332010269
iteration 176, loss = 0.0007616930524818599
iteration 177, loss = 0.0008915687794797122
iteration 178, loss = 0.0008923210552893579
iteration 179, loss = 0.0011560306884348392
iteration 180, loss = 0.0007266800384968519
iteration 181, loss = 0.0006693871691823006
iteration 182, loss = 0.001003950135782361
iteration 183, loss = 0.0009660078212618828
iteration 184, loss = 0.0007799318991601467
iteration 185, loss = 0.0025625834241509438
iteration 186, loss = 0.001121358829550445
iteration 187, loss = 0.0010809720261022449
iteration 188, loss = 0.0010004922514781356
iteration 189, loss = 0.0009282443788833916
iteration 190, loss = 0.0006709542940370739
iteration 191, loss = 0.0009340267861261964
iteration 192, loss = 0.0018410409102216363
iteration 193, loss = 0.0008527409518137574
iteration 194, loss = 0.0012395186349749565
iteration 195, loss = 0.0014946633018553257
iteration 196, loss = 0.0010313725797459483
iteration 197, loss = 0.0007174127968028188
iteration 198, loss = 0.0008572681690566242
iteration 199, loss = 0.0007614751229993999
iteration 200, loss = 0.0009156466694548726
iteration 201, loss = 0.0008127698674798012
iteration 202, loss = 0.0007525362889282405
iteration 203, loss = 0.000704245176166296
iteration 204, loss = 0.0007482852088287473
iteration 205, loss = 0.0009447510237805545
iteration 206, loss = 0.0013910859124734998
iteration 207, loss = 0.0007971663144417107
iteration 208, loss = 0.0009884752798825502
iteration 209, loss = 0.0007634737994521856
iteration 210, loss = 0.0007563085528090596
iteration 211, loss = 0.0015288033755496144
iteration 212, loss = 0.0008579119457863271
iteration 213, loss = 0.0018433349905535579
iteration 214, loss = 0.0006832656217738986
iteration 215, loss = 0.0010013768915086985
iteration 216, loss = 0.0007401347393169999
iteration 217, loss = 0.0011641583405435085
iteration 218, loss = 0.000888630049303174
iteration 219, loss = 0.0008618090650998056
iteration 220, loss = 0.0007630327017977834
iteration 221, loss = 0.0007484856760129333
iteration 222, loss = 0.0008464599959552288
iteration 223, loss = 0.0009163579088635743
iteration 224, loss = 0.0008569968631491065
iteration 225, loss = 0.0009708746802061796
iteration 226, loss = 0.0016590645536780357
iteration 227, loss = 0.0008704490028321743
iteration 228, loss = 0.0008145682513713837
iteration 229, loss = 0.0008177902200259268
iteration 230, loss = 0.0011613555252552032
iteration 231, loss = 0.0007508823764510453
iteration 232, loss = 0.0018047906924039125
iteration 233, loss = 0.0007632929482497275
iteration 234, loss = 0.0007937705377116799
iteration 235, loss = 0.000772204075474292
iteration 236, loss = 0.001945818425156176
iteration 237, loss = 0.0007961154915392399
iteration 238, loss = 0.0013742721639573574
iteration 239, loss = 0.0010128668509423733
iteration 240, loss = 0.0008176097762770951
iteration 241, loss = 0.0009985504439100623
iteration 242, loss = 0.0009533880511298776
iteration 243, loss = 0.0009643052471801639
iteration 244, loss = 0.0009020143770612776
iteration 245, loss = 0.0010549946455284953
iteration 246, loss = 0.0008477633818984032
iteration 247, loss = 0.0008844733820296824
iteration 248, loss = 0.001016812166199088
iteration 249, loss = 0.0007716824766248465
iteration 250, loss = 0.0010926765389740467
iteration 251, loss = 0.0009782903362065554
iteration 252, loss = 0.0008134267409332097
iteration 253, loss = 0.0008051086915656924
iteration 254, loss = 0.0010501431534066796
iteration 255, loss = 0.0014599654823541641
iteration 256, loss = 0.0011500914115458727
iteration 257, loss = 0.001517767203040421
iteration 258, loss = 0.0008058325620368123
iteration 259, loss = 0.0010251738131046295
iteration 260, loss = 0.001761629362590611
iteration 261, loss = 0.0006794348591938615
iteration 262, loss = 0.000757770671043545
iteration 263, loss = 0.0009359888499602675
iteration 264, loss = 0.0007574933115392923
iteration 265, loss = 0.0010114286560565233
iteration 266, loss = 0.0017224961193278432
iteration 267, loss = 0.001001601223833859
iteration 268, loss = 0.0008220559684559703
iteration 269, loss = 0.0007737475098110735
iteration 270, loss = 0.0014130271738395095
iteration 271, loss = 0.0006651981384493411
iteration 272, loss = 0.0008389644208364189
iteration 273, loss = 0.0008133485680446029
iteration 274, loss = 0.0008401695522479713
iteration 275, loss = 0.0008165981853380799
iteration 276, loss = 0.0010681042913347483
iteration 277, loss = 0.0007977742934599519
iteration 278, loss = 0.000898336002137512
iteration 279, loss = 0.000764983706176281
iteration 280, loss = 0.0007353876135312021
iteration 281, loss = 0.0009188397089019418
iteration 282, loss = 0.0008294375147670507
iteration 283, loss = 0.0011236611753702164
iteration 284, loss = 0.0014606256736442447
iteration 285, loss = 0.0008373463642783463
iteration 286, loss = 0.0011375516187399626
iteration 287, loss = 0.0013707346515730023
iteration 288, loss = 0.0017948374152183533
iteration 289, loss = 0.0007696973043493927
iteration 290, loss = 0.0007869445253163576
iteration 291, loss = 0.0008273242274299264
iteration 292, loss = 0.0007790674571879208
iteration 293, loss = 0.0009677138878032565
iteration 294, loss = 0.0008917226805351675
iteration 295, loss = 0.0007851310074329376
iteration 296, loss = 0.0008948089089244604
iteration 297, loss = 0.0009076893329620361
iteration 298, loss = 0.000794994062744081
iteration 299, loss = 0.000835296930745244
iteration 300, loss = 0.001182821812108159
iteration 1, loss = 0.000843932619318366
iteration 2, loss = 0.0010848339879885316
iteration 3, loss = 0.0007780313608236611
iteration 4, loss = 0.0007645966252312064
iteration 5, loss = 0.0007242343272082508
iteration 6, loss = 0.0008318782201968133
iteration 7, loss = 0.0010415207361802459
iteration 8, loss = 0.0008888653828762472
iteration 9, loss = 0.0008088733302429318
iteration 10, loss = 0.0008140327990986407
iteration 11, loss = 0.0008437844226136804
iteration 12, loss = 0.0007771406089887023
iteration 13, loss = 0.0008435759227722883
iteration 14, loss = 0.0011263190535828471
iteration 15, loss = 0.0010064843809232116
iteration 16, loss = 0.0015738231595605612
iteration 17, loss = 0.0008664721390232444
iteration 18, loss = 0.0007456536404788494
iteration 19, loss = 0.0010333370883017778
iteration 20, loss = 0.000832160993013531
iteration 21, loss = 0.0008612616802565753
iteration 22, loss = 0.0008624186157248914
iteration 23, loss = 0.0007008870597928762
iteration 24, loss = 0.0007310851360671222
iteration 25, loss = 0.0008424793486483395
iteration 26, loss = 0.0007616820512339473
iteration 27, loss = 0.0007797232246957719
iteration 28, loss = 0.0008705351501703262
iteration 29, loss = 0.0008525673765689135
iteration 30, loss = 0.0007908240659162402
iteration 31, loss = 0.0007588934386149049
iteration 32, loss = 0.0011360484641045332
iteration 33, loss = 0.001168583519756794
iteration 34, loss = 0.0007591968169435859
iteration 35, loss = 0.001313188229687512
iteration 36, loss = 0.0008619026048108935
iteration 37, loss = 0.0010204154532402754
iteration 38, loss = 0.0008424242842011154
iteration 39, loss = 0.0008044319110922515
iteration 40, loss = 0.001083153416402638
iteration 41, loss = 0.0010186531580984592
iteration 42, loss = 0.0009073535329662263
iteration 43, loss = 0.0009919223375618458
iteration 44, loss = 0.0007571451715193689
iteration 45, loss = 0.0008079780964180827
iteration 46, loss = 0.0008653708500787616
iteration 47, loss = 0.0007807977381162345
iteration 48, loss = 0.0010158756049349904
iteration 49, loss = 0.0015775396022945642
iteration 50, loss = 0.0007964344113133848
iteration 51, loss = 0.000775440305005759
iteration 52, loss = 0.0007787509821355343
iteration 53, loss = 0.0007680722046643496
iteration 54, loss = 0.0024134707637131214
iteration 55, loss = 0.001001380500383675
iteration 56, loss = 0.0012354886857792735
iteration 57, loss = 0.0009411507053300738
iteration 58, loss = 0.0007410892867483199
iteration 59, loss = 0.0007401448092423379
iteration 60, loss = 0.0012412186479195952
iteration 61, loss = 0.0014618422137573361
iteration 62, loss = 0.0010980984661728144
iteration 63, loss = 0.0016917492030188441
iteration 64, loss = 0.0008575209067203104
iteration 65, loss = 0.0006980399484746158
iteration 66, loss = 0.001844220794737339
iteration 67, loss = 0.0007288006017915905
iteration 68, loss = 0.0009668547427281737
iteration 69, loss = 0.0007997326320037246
iteration 70, loss = 0.0007419802714139223
iteration 71, loss = 0.0024068753700703382
iteration 72, loss = 0.000862583052366972
iteration 73, loss = 0.0007399937021546066
iteration 74, loss = 0.0007941322983242571
iteration 75, loss = 0.0009331596083939075
iteration 76, loss = 0.0009758621454238892
iteration 77, loss = 0.0008451483445242047
iteration 78, loss = 0.0006548052770085633
iteration 79, loss = 0.0008234454435296357
iteration 80, loss = 0.0007566108833998442
iteration 81, loss = 0.001474029733799398
iteration 82, loss = 0.0009122592164203525
iteration 83, loss = 0.0009095047134906054
iteration 84, loss = 0.0010505535174161196
iteration 85, loss = 0.0006784579018130898
iteration 86, loss = 0.000901806284673512
iteration 87, loss = 0.0006488290382549167
iteration 88, loss = 0.000725490681361407
iteration 89, loss = 0.0008335958700627089
iteration 90, loss = 0.0008913793135434389
iteration 91, loss = 0.0008550116908736527
iteration 92, loss = 0.00064823281718418
iteration 93, loss = 0.0007964785909280181
iteration 94, loss = 0.0007727947668172419
iteration 95, loss = 0.0009779626270756125
iteration 96, loss = 0.0011452602921053767
iteration 97, loss = 0.0006745990249328315
iteration 98, loss = 0.0009000190766528249
iteration 99, loss = 0.0007937327609397471
iteration 100, loss = 0.0011667925864458084
iteration 101, loss = 0.0011668811785057187
iteration 102, loss = 0.0011398668866604567
iteration 103, loss = 0.0009437340777367353
iteration 104, loss = 0.0006289231241680682
iteration 105, loss = 0.0008955579251050949
iteration 106, loss = 0.0012577291345223784
iteration 107, loss = 0.0009285567793995142
iteration 108, loss = 0.0016736353281885386
iteration 109, loss = 0.0007039856282062829
iteration 110, loss = 0.0008813097956590354
iteration 111, loss = 0.001140659675002098
iteration 112, loss = 0.0008845400298014283
iteration 113, loss = 0.0013537375489249825
iteration 114, loss = 0.0010614446364343166
iteration 115, loss = 0.0008446358260698617
iteration 116, loss = 0.0007546429987996817
iteration 117, loss = 0.0012906691990792751
iteration 118, loss = 0.0008405557018704712
iteration 119, loss = 0.0007787929498590529
iteration 120, loss = 0.0008839564980007708
iteration 121, loss = 0.0011143586598336697
iteration 122, loss = 0.0007338246796280146
iteration 123, loss = 0.0011083559365943074
iteration 124, loss = 0.0007745703915134072
iteration 125, loss = 0.0007772461976855993
iteration 126, loss = 0.0008020520908758044
iteration 127, loss = 0.001546636107377708
iteration 128, loss = 0.0008170139626599848
iteration 129, loss = 0.0007216140511445701
iteration 130, loss = 0.0007088143611326814
iteration 131, loss = 0.0007844826905056834
iteration 132, loss = 0.0016989298164844513
iteration 133, loss = 0.0008270086254924536
iteration 134, loss = 0.0014755541924387217
iteration 135, loss = 0.0009410795173607767
iteration 136, loss = 0.0007684994488954544
iteration 137, loss = 0.0016339605208486319
iteration 138, loss = 0.0008300572517327964
iteration 139, loss = 0.0009024773607961833
iteration 140, loss = 0.0008899009553715587
iteration 141, loss = 0.0007724996539764106
iteration 142, loss = 0.0009959663730114698
iteration 143, loss = 0.0007035458111204207
iteration 144, loss = 0.0013793326215818524
iteration 145, loss = 0.000821409048512578
iteration 146, loss = 0.00087578414240852
iteration 147, loss = 0.0007443969952873886
iteration 148, loss = 0.0009082719916477799
iteration 149, loss = 0.0008721585036255419
iteration 150, loss = 0.0010773707181215286
iteration 151, loss = 0.0007877576863393188
iteration 152, loss = 0.0008350740536116064
iteration 153, loss = 0.0007851163973100483
iteration 154, loss = 0.0010255144443362951
iteration 155, loss = 0.0009644454694353044
iteration 156, loss = 0.0006891643279232085
iteration 157, loss = 0.0008946055313572288
iteration 158, loss = 0.0012305926065891981
iteration 159, loss = 0.0010515042813494802
iteration 160, loss = 0.0009090953972190619
iteration 161, loss = 0.0008617106359452009
iteration 162, loss = 0.0008388850837945938
iteration 163, loss = 0.0014249826781451702
iteration 164, loss = 0.0009688596474006772
iteration 165, loss = 0.0007766378112137318
iteration 166, loss = 0.0008310254197567701
iteration 167, loss = 0.000978869735263288
iteration 168, loss = 0.0010027484968304634
iteration 169, loss = 0.0008344030939042568
iteration 170, loss = 0.0008066474110819399
iteration 171, loss = 0.0010122149251401424
iteration 172, loss = 0.0009470420191064477
iteration 173, loss = 0.0009293108014389873
iteration 174, loss = 0.0013478558976203203
iteration 175, loss = 0.0016996312187984586
iteration 176, loss = 0.000823105510789901
iteration 177, loss = 0.0008478553500026464
iteration 178, loss = 0.0009842393919825554
iteration 179, loss = 0.0011564561864361167
iteration 180, loss = 0.0007898075855337083
iteration 181, loss = 0.0008906199363991618
iteration 182, loss = 0.0010082053486257792
iteration 183, loss = 0.0009563944186083972
iteration 184, loss = 0.001107832184061408
iteration 185, loss = 0.0008149696514010429
iteration 186, loss = 0.001028125872835517
iteration 187, loss = 0.0007707889308221638
iteration 188, loss = 0.0009592923452146351
iteration 189, loss = 0.0006716731004416943
iteration 190, loss = 0.0007480577332898974
iteration 191, loss = 0.000852958473842591
iteration 192, loss = 0.0007647606544196606
iteration 193, loss = 0.0009354304638691247
iteration 194, loss = 0.0009452350786887109
iteration 195, loss = 0.000774164916947484
iteration 196, loss = 0.0008032217738218606
iteration 197, loss = 0.0009021825389936566
iteration 198, loss = 0.0012084211921319366
iteration 199, loss = 0.0007956469198688865
iteration 200, loss = 0.0009335048380307853
iteration 201, loss = 0.0007377397851087153
iteration 202, loss = 0.0008855682099238038
iteration 203, loss = 0.0010401214240118861
iteration 204, loss = 0.00193022598978132
iteration 205, loss = 0.0009234861354343593
iteration 206, loss = 0.0010592740727588534
iteration 207, loss = 0.001842548605054617
iteration 208, loss = 0.000861152948345989
iteration 209, loss = 0.0008273190469481051
iteration 210, loss = 0.0008415569900535047
iteration 211, loss = 0.0010616322979331017
iteration 212, loss = 0.0008742217905819416
iteration 213, loss = 0.0008832042803987861
iteration 214, loss = 0.0008420732337981462
iteration 215, loss = 0.0008842995739541948
iteration 216, loss = 0.001632114639505744
iteration 217, loss = 0.0014535380760207772
iteration 218, loss = 0.0009612218127585948
iteration 219, loss = 0.0006926311179995537
iteration 220, loss = 0.0015400982229039073
iteration 221, loss = 0.0008874420309439301
iteration 222, loss = 0.0008215251727961004
iteration 223, loss = 0.0007810435490682721
iteration 224, loss = 0.001558590680360794
iteration 225, loss = 0.0014119426487013698
iteration 226, loss = 0.0007769886287860572
iteration 227, loss = 0.0011370644206181169
iteration 228, loss = 0.0008623966132290661
iteration 229, loss = 0.0009246720001101494
iteration 230, loss = 0.0008319559274241328
iteration 231, loss = 0.0008903704583644867
iteration 232, loss = 0.0009512905380688608
iteration 233, loss = 0.0007995928172022104
iteration 234, loss = 0.0008433241164311767
iteration 235, loss = 0.001055645290762186
iteration 236, loss = 0.0009271185845136642
iteration 237, loss = 0.0007812873227521777
iteration 238, loss = 0.0008160380530171096
iteration 239, loss = 0.0011004178086295724
iteration 240, loss = 0.0008046093280427158
iteration 241, loss = 0.0010454519651830196
iteration 242, loss = 0.0016900107730180025
iteration 243, loss = 0.0007900184136815369
iteration 244, loss = 0.0008299297769553959
iteration 245, loss = 0.0008310513221658766
iteration 246, loss = 0.0008885376737453043
iteration 247, loss = 0.0008417037897743285
iteration 248, loss = 0.0016407096991315484
iteration 249, loss = 0.0010972341988235712
iteration 250, loss = 0.0010035504819825292
iteration 251, loss = 0.0016389201628044248
iteration 252, loss = 0.0009755784412845969
iteration 253, loss = 0.0007792200776748359
iteration 254, loss = 0.0008482647826895118
iteration 255, loss = 0.0010634665377438068
iteration 256, loss = 0.0007639089017175138
iteration 257, loss = 0.000836055027320981
iteration 258, loss = 0.0008783963276073337
iteration 259, loss = 0.0007189700263552368
iteration 260, loss = 0.0011413495521992445
iteration 261, loss = 0.0008695634314790368
iteration 262, loss = 0.0013773716054856777
iteration 263, loss = 0.0011303108185529709
iteration 264, loss = 0.0016683305148035288
iteration 265, loss = 0.0010768419597297907
iteration 266, loss = 0.0012214642483741045
iteration 267, loss = 0.0010798864532262087
iteration 268, loss = 0.0011907413136214018
iteration 269, loss = 0.0015388847095891833
iteration 270, loss = 0.0007903024088591337
iteration 271, loss = 0.0009153031278401613
iteration 272, loss = 0.0008241485338658094
iteration 273, loss = 0.000910220027435571
iteration 274, loss = 0.0015547231305390596
iteration 275, loss = 0.0017866198904812336
iteration 276, loss = 0.0008193007088266313
iteration 277, loss = 0.0007477272301912308
iteration 278, loss = 0.0008184093167074025
iteration 279, loss = 0.0010422286577522755
iteration 280, loss = 0.0007685733144171536
iteration 281, loss = 0.0017463471740484238
iteration 282, loss = 0.0007135081687010825
iteration 283, loss = 0.0009451969526708126
iteration 284, loss = 0.0008927986491471529
iteration 285, loss = 0.0009988917736336589
iteration 286, loss = 0.0006944166962057352
iteration 287, loss = 0.0010962968226522207
iteration 288, loss = 0.0016479028854519129
iteration 289, loss = 0.0012345591094344854
iteration 290, loss = 0.0008253867272287607
iteration 291, loss = 0.0009702354436740279
iteration 292, loss = 0.0010509976418688893
iteration 293, loss = 0.001065963413566351
iteration 294, loss = 0.0008145971223711967
iteration 295, loss = 0.0007916693575680256
iteration 296, loss = 0.0015921000158414245
iteration 297, loss = 0.0011590005597099662
iteration 298, loss = 0.0008930577314458787
iteration 299, loss = 0.0009586015366949141
iteration 300, loss = 0.0009027188643813133
iteration 1, loss = 0.0008724794606678188
iteration 2, loss = 0.0009211198193952441
iteration 3, loss = 0.0007647283491678536
iteration 4, loss = 0.0008330455748364329
iteration 5, loss = 0.001085790223442018
iteration 6, loss = 0.0009873486123979092
iteration 7, loss = 0.0007223837310448289
iteration 8, loss = 0.0009019241551868618
iteration 9, loss = 0.0007630299660377204
iteration 10, loss = 0.0017003417015075684
iteration 11, loss = 0.0012470921501517296
iteration 12, loss = 0.0008843549876473844
iteration 13, loss = 0.0007526605622842908
iteration 14, loss = 0.0009543754858896136
iteration 15, loss = 0.000789013400208205
iteration 16, loss = 0.0008017277577891946
iteration 17, loss = 0.000821978785097599
iteration 18, loss = 0.0008722638594917953
iteration 19, loss = 0.0008985496824607253
iteration 20, loss = 0.0009803144494071603
iteration 21, loss = 0.0011620341101661325
iteration 22, loss = 0.0014852510066702962
iteration 23, loss = 0.0009188876720145345
iteration 24, loss = 0.0006735754432156682
iteration 25, loss = 0.0008750859415158629
iteration 26, loss = 0.0010012886486947536
iteration 27, loss = 0.001029809471219778
iteration 28, loss = 0.0010129534639418125
iteration 29, loss = 0.0011652386747300625
iteration 30, loss = 0.0007611004402860999
iteration 31, loss = 0.001066015101969242
iteration 32, loss = 0.0007407423108816147
iteration 33, loss = 0.0009275306947529316
iteration 34, loss = 0.0008306895033456385
iteration 35, loss = 0.0007891071727499366
iteration 36, loss = 0.0009694075561128557
iteration 37, loss = 0.0009952157270163298
iteration 38, loss = 0.0009234659373760223
iteration 39, loss = 0.0009494564728811383
iteration 40, loss = 0.0013441677438095212
iteration 41, loss = 0.0008163069142028689
iteration 42, loss = 0.0006712093600071967
iteration 43, loss = 0.0007636248483322561
iteration 44, loss = 0.0010007342789322138
iteration 45, loss = 0.0011466072173789144
iteration 46, loss = 0.0007374726701527834
iteration 47, loss = 0.0008060228428803384
iteration 48, loss = 0.0010148026049137115
iteration 49, loss = 0.0008100310806185007
iteration 50, loss = 0.0010686275782063603
iteration 51, loss = 0.0008345537935383618
iteration 52, loss = 0.0010833382839336991
iteration 53, loss = 0.0009766567964106798
iteration 54, loss = 0.0008573057129979134
iteration 55, loss = 0.0008684141794219613
iteration 56, loss = 0.0007950580911710858
iteration 57, loss = 0.0008620448061265051
iteration 58, loss = 0.0006962186307646334
iteration 59, loss = 0.000980670447461307
iteration 60, loss = 0.0008990877540782094
iteration 61, loss = 0.0010991152375936508
iteration 62, loss = 0.0006800939445383847
iteration 63, loss = 0.000948568107560277
iteration 64, loss = 0.0009815818630158901
iteration 65, loss = 0.0010689086047932506
iteration 66, loss = 0.002200876595452428
iteration 67, loss = 0.0007695148233324289
iteration 68, loss = 0.0007690270431339741
iteration 69, loss = 0.0015152627602219582
iteration 70, loss = 0.0007501334766857326
iteration 71, loss = 0.000877348764333874
iteration 72, loss = 0.0008842216921038926
iteration 73, loss = 0.0010464938823133707
iteration 74, loss = 0.002007849980145693
iteration 75, loss = 0.0010669316397979856
iteration 76, loss = 0.0010515258181840181
iteration 77, loss = 0.0008281434420496225
iteration 78, loss = 0.0008371704025194049
iteration 79, loss = 0.0007643403369002044
iteration 80, loss = 0.001022342941723764
iteration 81, loss = 0.000880495470482856
iteration 82, loss = 0.0009521364117972553
iteration 83, loss = 0.0006736710201948881
iteration 84, loss = 0.0009217389160767198
iteration 85, loss = 0.0010278963018208742
iteration 86, loss = 0.0011974454391747713
iteration 87, loss = 0.0010273241205140948
iteration 88, loss = 0.0008076904341578484
iteration 89, loss = 0.0007579929661005735
iteration 90, loss = 0.0007578918011859059
iteration 91, loss = 0.000746543169952929
iteration 92, loss = 0.0015356636140495539
iteration 93, loss = 0.0008465129067189991
iteration 94, loss = 0.001159236067906022
iteration 95, loss = 0.0007533043390139937
iteration 96, loss = 0.0008228450315073133
iteration 97, loss = 0.0006852464866824448
iteration 98, loss = 0.0008276328444480896
iteration 99, loss = 0.0012775766663253307
iteration 100, loss = 0.0012852826621383429
iteration 101, loss = 0.0008055077632889152
iteration 102, loss = 0.0008230987004935741
iteration 103, loss = 0.0016253544017672539
iteration 104, loss = 0.0007341315504163504
iteration 105, loss = 0.0007483477820642292
iteration 106, loss = 0.0008699536556378007
iteration 107, loss = 0.0010041150962933898
iteration 108, loss = 0.0007649338222108781
iteration 109, loss = 0.0010879643959924579
iteration 110, loss = 0.0015823690919205546
iteration 111, loss = 0.0008306850213557482
iteration 112, loss = 0.0009135045111179352
iteration 113, loss = 0.000868690200150013
iteration 114, loss = 0.0006911577074788511
iteration 115, loss = 0.0008892732439562678
iteration 116, loss = 0.0011621611192822456
iteration 117, loss = 0.0008797802729532123
iteration 118, loss = 0.0014627659693360329
iteration 119, loss = 0.0007240521954372525
iteration 120, loss = 0.000774701707996428
iteration 121, loss = 0.0015251600416377187
iteration 122, loss = 0.0010517738992348313
iteration 123, loss = 0.001177762751467526
iteration 124, loss = 0.0007068380364216864
iteration 125, loss = 0.0007489437703043222
iteration 126, loss = 0.000958653399720788
iteration 127, loss = 0.0010343397734686732
iteration 128, loss = 0.0008247985388152301
iteration 129, loss = 0.0008282030466943979
iteration 130, loss = 0.0007264012238010764
iteration 131, loss = 0.0008385475375689566
iteration 132, loss = 0.0010897155152633786
iteration 133, loss = 0.0007236031233333051
iteration 134, loss = 0.0008448127191513777
iteration 135, loss = 0.0009460674482397735
iteration 136, loss = 0.0008493027416989207
iteration 137, loss = 0.0008885025163181126
iteration 138, loss = 0.0015357033116742969
iteration 139, loss = 0.0008296179585158825
iteration 140, loss = 0.002474060747772455
iteration 141, loss = 0.0007554615149274468
iteration 142, loss = 0.0008124564774334431
iteration 143, loss = 0.0011239368468523026
iteration 144, loss = 0.0007524550310336053
iteration 145, loss = 0.0008113685180433095
iteration 146, loss = 0.0008048560121096671
iteration 147, loss = 0.0008176941191777587
iteration 148, loss = 0.0009461246663704515
iteration 149, loss = 0.000669224769808352
iteration 150, loss = 0.0011309939436614513
iteration 151, loss = 0.0007868452812545002
iteration 152, loss = 0.0010832981206476688
iteration 153, loss = 0.001728538190945983
iteration 154, loss = 0.000900915008969605
iteration 155, loss = 0.0007995061459951103
iteration 156, loss = 0.001671173027716577
iteration 157, loss = 0.0008987821638584137
iteration 158, loss = 0.0008243494085036218
iteration 159, loss = 0.0016147998394444585
iteration 160, loss = 0.0010617823572829366
iteration 161, loss = 0.0010208800667896867
iteration 162, loss = 0.0007097759516909719
iteration 163, loss = 0.0014606413897126913
iteration 164, loss = 0.0014044016133993864
iteration 165, loss = 0.0006656725890934467
iteration 166, loss = 0.0007695617969147861
iteration 167, loss = 0.0007904520607553422
iteration 168, loss = 0.001708630588836968
iteration 169, loss = 0.0008371714502573013
iteration 170, loss = 0.0012132161064073443
iteration 171, loss = 0.000779511930886656
iteration 172, loss = 0.0008271611877717078
iteration 173, loss = 0.0007662677089683712
iteration 174, loss = 0.0008974220836535096
iteration 175, loss = 0.0009227667469531298
iteration 176, loss = 0.0007645581499673426
iteration 177, loss = 0.0007855395087972283
iteration 178, loss = 0.0008655139827169478
iteration 179, loss = 0.0007082366500981152
iteration 180, loss = 0.0007752336096018553
iteration 181, loss = 0.0008489169995300472
iteration 182, loss = 0.0016246925806626678
iteration 183, loss = 0.0006880046566948295
iteration 184, loss = 0.0008729000110179186
iteration 185, loss = 0.0018771716859191656
iteration 186, loss = 0.0008387811249122024
iteration 187, loss = 0.0012999832397326827
iteration 188, loss = 0.000879559782333672
iteration 189, loss = 0.0007389368838630617
iteration 190, loss = 0.0008901310502551496
iteration 191, loss = 0.0008194431429728866
iteration 192, loss = 0.000778388581238687
iteration 193, loss = 0.0009353305795229971
iteration 194, loss = 0.0008936599479056895
iteration 195, loss = 0.0010400532046332955
iteration 196, loss = 0.0008588152704760432
iteration 197, loss = 0.0015615373849868774
iteration 198, loss = 0.0008063528221100569
iteration 199, loss = 0.0007633782224729657
iteration 200, loss = 0.0010720782447606325
iteration 201, loss = 0.0010876297019422054
iteration 202, loss = 0.0010239960392937064
iteration 203, loss = 0.0016447880771011114
iteration 204, loss = 0.0009332120534963906
iteration 205, loss = 0.0008522689458914101
iteration 206, loss = 0.0016309659695252776
iteration 207, loss = 0.0008972793002612889
iteration 208, loss = 0.0008990158094093204
iteration 209, loss = 0.0007951976149342954
iteration 210, loss = 0.0009159748442471027
iteration 211, loss = 0.000714618363417685
iteration 212, loss = 0.001669368939474225
iteration 213, loss = 0.0008555485401302576
iteration 214, loss = 0.001410751254297793
iteration 215, loss = 0.0008028504089452326
iteration 216, loss = 0.0007897153263911605
iteration 217, loss = 0.0010435829171910882
iteration 218, loss = 0.0008268747478723526
iteration 219, loss = 0.0007617544033564627
iteration 220, loss = 0.0007368506048806012
iteration 221, loss = 0.0010774999391287565
iteration 222, loss = 0.0006995220901444554
iteration 223, loss = 0.0007204145076684654
iteration 224, loss = 0.002041328465566039
iteration 225, loss = 0.001100738998502493
iteration 226, loss = 0.0015428478363901377
iteration 227, loss = 0.0008933076751418412
iteration 228, loss = 0.0014340273337438703
iteration 229, loss = 0.0008979176636785269
iteration 230, loss = 0.0007655013469047844
iteration 231, loss = 0.0008770419517531991
iteration 232, loss = 0.0007805086788721383
iteration 233, loss = 0.0008782257209531963
iteration 234, loss = 0.000981890014372766
iteration 235, loss = 0.0008366667898371816
iteration 236, loss = 0.0008136724354699254
iteration 237, loss = 0.0007984218536876142
iteration 238, loss = 0.0007635733927600086
iteration 239, loss = 0.0007469204138033092
iteration 240, loss = 0.0009803898865357041
iteration 241, loss = 0.000903516891412437
iteration 242, loss = 0.0008148339111357927
iteration 243, loss = 0.0015883962623775005
iteration 244, loss = 0.0008224272169172764
iteration 245, loss = 0.0009905415354296565
iteration 246, loss = 0.0008179233409464359
iteration 247, loss = 0.0010141480015590787
iteration 248, loss = 0.0016079405322670937
iteration 249, loss = 0.000820527842734009
iteration 250, loss = 0.0008566078031435609
iteration 251, loss = 0.0010058354819193482
iteration 252, loss = 0.00177413085475564
iteration 253, loss = 0.0008820840739645064
iteration 254, loss = 0.000766840938013047
iteration 255, loss = 0.0011145255994051695
iteration 256, loss = 0.0017997202230617404
iteration 257, loss = 0.001164200366474688
iteration 258, loss = 0.0008695925935171545
iteration 259, loss = 0.0007345097837969661
iteration 260, loss = 0.0009732115431688726
iteration 261, loss = 0.001333914464339614
iteration 262, loss = 0.0008423604303970933
iteration 263, loss = 0.001119412248954177
iteration 264, loss = 0.0009622785146348178
iteration 265, loss = 0.0009138945024460554
iteration 266, loss = 0.00076343584805727
iteration 267, loss = 0.0012096503050997853
iteration 268, loss = 0.0006846371688880026
iteration 269, loss = 0.0011432281462475657
iteration 270, loss = 0.0008055986254476011
iteration 271, loss = 0.000954742485191673
iteration 272, loss = 0.0014462805120274425
iteration 273, loss = 0.0008156767580658197
iteration 274, loss = 0.0006683410028927028
iteration 275, loss = 0.0008801676449365914
iteration 276, loss = 0.0008898802334442735
iteration 277, loss = 0.001769699272699654
iteration 278, loss = 0.0008762837387621403
iteration 279, loss = 0.0007549411384388804
iteration 280, loss = 0.0012762126279994845
iteration 281, loss = 0.001006405334919691
iteration 282, loss = 0.0008375030010938644
iteration 283, loss = 0.00117930443957448
iteration 284, loss = 0.0009281623060815036
iteration 285, loss = 0.0007414476131089032
iteration 286, loss = 0.0010986727429553866
iteration 287, loss = 0.0006986991502344608
iteration 288, loss = 0.0008468643063679338
iteration 289, loss = 0.0011366558028385043
iteration 290, loss = 0.0012262426316738129
iteration 291, loss = 0.0009433940867893398
iteration 292, loss = 0.000782286748290062
iteration 293, loss = 0.0010023083304986358
iteration 294, loss = 0.0008005190757103264
iteration 295, loss = 0.0009220510255545378
iteration 296, loss = 0.0008633515681140125
iteration 297, loss = 0.000779661990236491
iteration 298, loss = 0.001014958368614316
iteration 299, loss = 0.0008345152018591762
iteration 300, loss = 0.0010366912465542555
iteration 1, loss = 0.0008765291422605515
iteration 2, loss = 0.0019472604617476463
iteration 3, loss = 0.0012531521497294307
iteration 4, loss = 0.00070585822686553
iteration 5, loss = 0.0010564860422164202
iteration 6, loss = 0.0007990661542862654
iteration 7, loss = 0.0009611269924789667
iteration 8, loss = 0.0006930153467692435
iteration 9, loss = 0.0008773154695518315
iteration 10, loss = 0.0009380099945701659
iteration 11, loss = 0.0007972848834469914
iteration 12, loss = 0.001830016728490591
iteration 13, loss = 0.0007637767121195793
iteration 14, loss = 0.000728821090888232
iteration 15, loss = 0.000911831739358604
iteration 16, loss = 0.0007413716521114111
iteration 17, loss = 0.001888877246528864
iteration 18, loss = 0.0009297382785007358
iteration 19, loss = 0.0009577807504683733
iteration 20, loss = 0.000823728390969336
iteration 21, loss = 0.001390125253237784
iteration 22, loss = 0.0007494228193536401
iteration 23, loss = 0.0007234194781631231
iteration 24, loss = 0.000876408361364156
iteration 25, loss = 0.0008087584283202887
iteration 26, loss = 0.0006605027010664344
iteration 27, loss = 0.0008571971557103097
iteration 28, loss = 0.000821945141069591
iteration 29, loss = 0.0007391745457425714
iteration 30, loss = 0.001804506522603333
iteration 31, loss = 0.0007781112217344344
iteration 32, loss = 0.0015112627297639847
iteration 33, loss = 0.0013039222685620189
iteration 34, loss = 0.0008348013507202268
iteration 35, loss = 0.001446362235583365
iteration 36, loss = 0.0008400168153457344
iteration 37, loss = 0.0016574586043134332
iteration 38, loss = 0.0009510464151389897
iteration 39, loss = 0.0007636658265255392
iteration 40, loss = 0.0009390462073497474
iteration 41, loss = 0.0009686071425676346
iteration 42, loss = 0.0007008601096458733
iteration 43, loss = 0.0008718029130250216
iteration 44, loss = 0.0010374652920290828
iteration 45, loss = 0.00121811602730304
iteration 46, loss = 0.000832455581985414
iteration 47, loss = 0.000815882405731827
iteration 48, loss = 0.0018551974790170789
iteration 49, loss = 0.0007778685539960861
iteration 50, loss = 0.0009179615299217403
iteration 51, loss = 0.0007245081942528486
iteration 52, loss = 0.0008172921952791512
iteration 53, loss = 0.001499461242929101
iteration 54, loss = 0.0009811087511479855
iteration 55, loss = 0.001603191252797842
iteration 56, loss = 0.0008964113658294082
iteration 57, loss = 0.0007427679956890643
iteration 58, loss = 0.001421392080374062
iteration 59, loss = 0.0016260934062302113
iteration 60, loss = 0.0008175186812877655
iteration 61, loss = 0.0007603406556881964
iteration 62, loss = 0.0007506327820010483
iteration 63, loss = 0.0008531345520168543
iteration 64, loss = 0.0010148970177397132
iteration 65, loss = 0.0008366181282326579
iteration 66, loss = 0.0006991976406425238
iteration 67, loss = 0.0008534968364983797
iteration 68, loss = 0.0007099590729922056
iteration 69, loss = 0.0007632929482497275
iteration 70, loss = 0.0008645246271044016
iteration 71, loss = 0.0008273890125565231
iteration 72, loss = 0.000920620746910572
iteration 73, loss = 0.000801409943960607
iteration 74, loss = 0.0008022487163543701
iteration 75, loss = 0.0007336616981774569
iteration 76, loss = 0.0007780371815897524
iteration 77, loss = 0.0011789327254518867
iteration 78, loss = 0.000788557343184948
iteration 79, loss = 0.0008886709110811353
iteration 80, loss = 0.0007040224154479802
iteration 81, loss = 0.0010607681469991803
iteration 82, loss = 0.0009143314091488719
iteration 83, loss = 0.0008161857258528471
iteration 84, loss = 0.0011824488174170256
iteration 85, loss = 0.000927363580558449
iteration 86, loss = 0.0008062752312980592
iteration 87, loss = 0.0008774327579885721
iteration 88, loss = 0.000887127302121371
iteration 89, loss = 0.0009241595980711281
iteration 90, loss = 0.0008550883503630757
iteration 91, loss = 0.0007858842727728188
iteration 92, loss = 0.0012597810709849
iteration 93, loss = 0.000803203321993351
iteration 94, loss = 0.0008152808295562863
iteration 95, loss = 0.00208366010338068
iteration 96, loss = 0.0012383731082081795
iteration 97, loss = 0.0007740127621218562
iteration 98, loss = 0.0009020195575430989
iteration 99, loss = 0.0009510740637779236
iteration 100, loss = 0.0009041255107149482
iteration 101, loss = 0.000792591948993504
iteration 102, loss = 0.0007026421953924
iteration 103, loss = 0.0017469218000769615
iteration 104, loss = 0.0012394892983138561
iteration 105, loss = 0.0008942739805206656
iteration 106, loss = 0.0010670979972928762
iteration 107, loss = 0.0010436016600579023
iteration 108, loss = 0.0018536904826760292
iteration 109, loss = 0.001083696959540248
iteration 110, loss = 0.0014826639089733362
iteration 111, loss = 0.0009986109798774123
iteration 112, loss = 0.0008217617869377136
iteration 113, loss = 0.0008108736947178841
iteration 114, loss = 0.0010652656201273203
iteration 115, loss = 0.00069815554888919
iteration 116, loss = 0.001024210941977799
iteration 117, loss = 0.0006975342985242605
iteration 118, loss = 0.0008794716559350491
iteration 119, loss = 0.0008079657563939691
iteration 120, loss = 0.0010647228918969631
iteration 121, loss = 0.0008601024164818227
iteration 122, loss = 0.000822787347715348
iteration 123, loss = 0.0007007677922956645
iteration 124, loss = 0.0016377581050619483
iteration 125, loss = 0.0009148922981694341
iteration 126, loss = 0.0014646296622231603
iteration 127, loss = 0.0009706865530461073
iteration 128, loss = 0.0009293641778640449
iteration 129, loss = 0.001425506779924035
iteration 130, loss = 0.0011950071202591062
iteration 131, loss = 0.0009726269636303186
iteration 132, loss = 0.0008425975684076548
iteration 133, loss = 0.0009461138397455215
iteration 134, loss = 0.0011037577642127872
iteration 135, loss = 0.001047602971084416
iteration 136, loss = 0.0007512897136621177
iteration 137, loss = 0.0006715994677506387
iteration 138, loss = 0.0009590716217644513
iteration 139, loss = 0.000825831142719835
iteration 140, loss = 0.0008649739320389926
iteration 141, loss = 0.0009936832357198
iteration 142, loss = 0.0008480416145175695
iteration 143, loss = 0.0009269540314562619
iteration 144, loss = 0.0014170270878821611
iteration 145, loss = 0.0008488433086313307
iteration 146, loss = 0.0007721750298514962
iteration 147, loss = 0.0007826803484931588
iteration 148, loss = 0.0007966850535012782
iteration 149, loss = 0.0009207395487464964
iteration 150, loss = 0.0008144268067553639
iteration 151, loss = 0.0008685463108122349
iteration 152, loss = 0.0009300119709223509
iteration 153, loss = 0.0010971983429044485
iteration 154, loss = 0.0007763558533042669
iteration 155, loss = 0.0023775834124535322
iteration 156, loss = 0.0010105514666065574
iteration 157, loss = 0.0008627127390354872
iteration 158, loss = 0.0013885140651836991
iteration 159, loss = 0.0006592015852220356
iteration 160, loss = 0.0010033842409029603
iteration 161, loss = 0.0011060343822464347
iteration 162, loss = 0.0011807435657829046
iteration 163, loss = 0.0008155424147844315
iteration 164, loss = 0.0010909722186625004
iteration 165, loss = 0.0011022102553397417
iteration 166, loss = 0.0009993197163566947
iteration 167, loss = 0.0010365182533860207
iteration 168, loss = 0.0008637500577606261
iteration 169, loss = 0.0015092628309503198
iteration 170, loss = 0.0011791444849222898
iteration 171, loss = 0.0007542355451732874
iteration 172, loss = 0.0008394712931476533
iteration 173, loss = 0.000992752960883081
iteration 174, loss = 0.0010651720222085714
iteration 175, loss = 0.0007284296443685889
iteration 176, loss = 0.0008245068020187318
iteration 177, loss = 0.0012758265947923064
iteration 178, loss = 0.0014714030548930168
iteration 179, loss = 0.0008018674561753869
iteration 180, loss = 0.0007213098579086363
iteration 181, loss = 0.0010100227082148194
iteration 182, loss = 0.0009153819410130382
iteration 183, loss = 0.0008356086909770966
iteration 184, loss = 0.000803588074631989
iteration 185, loss = 0.0009526285575702786
iteration 186, loss = 0.0008768263505771756
iteration 187, loss = 0.0012187249958515167
iteration 188, loss = 0.0008031752659007907
iteration 189, loss = 0.0008968449546955526
iteration 190, loss = 0.0007372944382950664
iteration 191, loss = 0.0008431714959442616
iteration 192, loss = 0.0012774108909070492
iteration 193, loss = 0.000973485060967505
iteration 194, loss = 0.0007045701495371759
iteration 195, loss = 0.0009261596715077758
iteration 196, loss = 0.0010583287803456187
iteration 197, loss = 0.0007824318599887192
iteration 198, loss = 0.0008241813047789037
iteration 199, loss = 0.0008216735441237688
iteration 200, loss = 0.0009559919126331806
iteration 201, loss = 0.0014617049600929022
iteration 202, loss = 0.000765698729082942
iteration 203, loss = 0.0008717722375877202
iteration 204, loss = 0.0014927169540897012
iteration 205, loss = 0.0008575554238632321
iteration 206, loss = 0.001155730220489204
iteration 207, loss = 0.0009270769660361111
iteration 208, loss = 0.0005983813316561282
iteration 209, loss = 0.0008648026268929243
iteration 210, loss = 0.00147073098924011
iteration 211, loss = 0.000945007661357522
iteration 212, loss = 0.0008823557873256505
iteration 213, loss = 0.001575698610395193
iteration 214, loss = 0.0013854231219738722
iteration 215, loss = 0.0008999505080282688
iteration 216, loss = 0.001164618181064725
iteration 217, loss = 0.000801235088147223
iteration 218, loss = 0.0011313697323203087
iteration 219, loss = 0.0008726284140720963
iteration 220, loss = 0.0008111963979899883
iteration 221, loss = 0.0007989368168637156
iteration 222, loss = 0.0015667368425056338
iteration 223, loss = 0.0008913957863114774
iteration 224, loss = 0.0007858090102672577
iteration 225, loss = 0.000873172131832689
iteration 226, loss = 0.0007426111842505634
iteration 227, loss = 0.000866536283865571
iteration 228, loss = 0.0007614589994773269
iteration 229, loss = 0.0008402500534430146
iteration 230, loss = 0.000748923746868968
iteration 231, loss = 0.001030754647217691
iteration 232, loss = 0.0007698257686570287
iteration 233, loss = 0.0010925037786364555
iteration 234, loss = 0.0012575333239510655
iteration 235, loss = 0.001265186001546681
iteration 236, loss = 0.0008222939795814455
iteration 237, loss = 0.0010536828776821494
iteration 238, loss = 0.001250570872798562
iteration 239, loss = 0.0008122533326968551
iteration 240, loss = 0.0009796696249395609
iteration 241, loss = 0.0008461851393803954
iteration 242, loss = 0.0008582401205785573
iteration 243, loss = 0.0010141285602003336
iteration 244, loss = 0.0006711201858706772
iteration 245, loss = 0.0011283872881904244
iteration 246, loss = 0.0007458424661308527
iteration 247, loss = 0.0009283067192882299
iteration 248, loss = 0.0018132850527763367
iteration 249, loss = 0.001007289276458323
iteration 250, loss = 0.0007569894078187644
iteration 251, loss = 0.0009453918319195509
iteration 252, loss = 0.00073328020516783
iteration 253, loss = 0.0009142895578406751
iteration 254, loss = 0.0007584958802908659
iteration 255, loss = 0.0007892919820733368
iteration 256, loss = 0.0010406909277662635
iteration 257, loss = 0.0007894592708908021
iteration 258, loss = 0.0008495747460983694
iteration 259, loss = 0.0009183246293105185
iteration 260, loss = 0.0011920479591935873
iteration 261, loss = 0.001147720031440258
iteration 262, loss = 0.0007669358165003359
iteration 263, loss = 0.0007724342867732048
iteration 264, loss = 0.0010267039760947227
iteration 265, loss = 0.0006874631508253515
iteration 266, loss = 0.0007857377640902996
iteration 267, loss = 0.0007228825124911964
iteration 268, loss = 0.001128270523622632
iteration 269, loss = 0.001770970644429326
iteration 270, loss = 0.0009192670695483685
iteration 271, loss = 0.0011840161168947816
iteration 272, loss = 0.0007839183672331274
iteration 273, loss = 0.0009515956044197083
iteration 274, loss = 0.001063366187736392
iteration 275, loss = 0.00086310098413378
iteration 276, loss = 0.001033556298352778
iteration 277, loss = 0.0008446377469226718
iteration 278, loss = 0.0006559390458278358
iteration 279, loss = 0.0008524908917024732
iteration 280, loss = 0.0008117476827464998
iteration 281, loss = 0.001016176538541913
iteration 282, loss = 0.000869917101226747
iteration 283, loss = 0.0007241347921080887
iteration 284, loss = 0.0008925747824832797
iteration 285, loss = 0.001222135848365724
iteration 286, loss = 0.0008747799438424408
iteration 287, loss = 0.0010014508152380586
iteration 288, loss = 0.0010292315855622292
iteration 289, loss = 0.0014892260078340769
iteration 290, loss = 0.000971498666331172
iteration 291, loss = 0.0008507678867317736
iteration 292, loss = 0.0006754944333806634
iteration 293, loss = 0.0009212001459673047
iteration 294, loss = 0.0008083601715043187
iteration 295, loss = 0.0012085030321031809
iteration 296, loss = 0.0011556220706552267
iteration 297, loss = 0.0007386396173387766
iteration 298, loss = 0.0008126020547933877
iteration 299, loss = 0.0009650966385379434
iteration 300, loss = 0.0009070028318092227
iteration 1, loss = 0.000804042152594775
iteration 2, loss = 0.0009329812019132078
iteration 3, loss = 0.0008607875788584352
iteration 4, loss = 0.0007751128287054598
iteration 5, loss = 0.0015046070329844952
iteration 6, loss = 0.0016970210708677769
iteration 7, loss = 0.0006955207791179419
iteration 8, loss = 0.0006965064676478505
iteration 9, loss = 0.001690682373009622
iteration 10, loss = 0.0015895706601440907
iteration 11, loss = 0.0008921143598854542
iteration 12, loss = 0.0009323410922661424
iteration 13, loss = 0.0029418007470667362
iteration 14, loss = 0.0017008428694680333
iteration 15, loss = 0.0007595093920826912
iteration 16, loss = 0.0006788548198528588
iteration 17, loss = 0.0009141502669081092
iteration 18, loss = 0.0008743584039621055
iteration 19, loss = 0.0011862640967592597
iteration 20, loss = 0.0009268384310416877
iteration 21, loss = 0.0007655772496946156
iteration 22, loss = 0.0016598555957898498
iteration 23, loss = 0.0007377844303846359
iteration 24, loss = 0.00200294004753232
iteration 25, loss = 0.000999428448267281
iteration 26, loss = 0.0007971271988935769
iteration 27, loss = 0.0010873633436858654
iteration 28, loss = 0.001235191011801362
iteration 29, loss = 0.0011195247061550617
iteration 30, loss = 0.0008336658356711268
iteration 31, loss = 0.0007241155253723264
iteration 32, loss = 0.0007146346033550799
iteration 33, loss = 0.0008890879689715803
iteration 34, loss = 0.0010366018395870924
iteration 35, loss = 0.001124400645494461
iteration 36, loss = 0.0015644726809114218
iteration 37, loss = 0.0015747665893286467
iteration 38, loss = 0.0009206931572407484
iteration 39, loss = 0.0007536171469837427
iteration 40, loss = 0.0015778111992403865
iteration 41, loss = 0.0010445325169712305
iteration 42, loss = 0.0017811793368309736
iteration 43, loss = 0.0009097088477574289
iteration 44, loss = 0.0007354888366535306
iteration 45, loss = 0.0009558655438013375
iteration 46, loss = 0.0007902670186012983
iteration 47, loss = 0.0007930626161396503
iteration 48, loss = 0.0011358362389728427
iteration 49, loss = 0.0015194738516584039
iteration 50, loss = 0.000834078760817647
iteration 51, loss = 0.0009152857237495482
iteration 52, loss = 0.000786248710937798
iteration 53, loss = 0.0012136027216911316
iteration 54, loss = 0.0008757401956245303
iteration 55, loss = 0.0008383736130781472
iteration 56, loss = 0.0010135379852727056
iteration 57, loss = 0.0010141365928575397
iteration 58, loss = 0.0008790661813691258
iteration 59, loss = 0.0008064100984483957
iteration 60, loss = 0.0008401076192967594
iteration 61, loss = 0.00100469752214849
iteration 62, loss = 0.0008000453817658126
iteration 63, loss = 0.0009431934449821711
iteration 64, loss = 0.0008385739056393504
iteration 65, loss = 0.0007606088765896857
iteration 66, loss = 0.0006944271153770387
iteration 67, loss = 0.0009946533245965838
iteration 68, loss = 0.0009314781054854393
iteration 69, loss = 0.0009116181754507124
iteration 70, loss = 0.0012862058356404305
iteration 71, loss = 0.0007418494787998497
iteration 72, loss = 0.0018070738296955824
iteration 73, loss = 0.0007973454194143414
iteration 74, loss = 0.0008799532661214471
iteration 75, loss = 0.0009281003731302917
iteration 76, loss = 0.001007484970614314
iteration 77, loss = 0.0008118288242258132
iteration 78, loss = 0.0011695486027747393
iteration 79, loss = 0.0007344590267166495
iteration 80, loss = 0.0008173283422365785
iteration 81, loss = 0.0015890814829617739
iteration 82, loss = 0.001043441123329103
iteration 83, loss = 0.0012211166322231293
iteration 84, loss = 0.0008983060833998024
iteration 85, loss = 0.0007613901398144662
iteration 86, loss = 0.0010693485382944345
iteration 87, loss = 0.0007383257616311312
iteration 88, loss = 0.0008646265487186611
iteration 89, loss = 0.0008128754561766982
iteration 90, loss = 0.0016907107783481479
iteration 91, loss = 0.0007499231724068522
iteration 92, loss = 0.0007417627493850887
iteration 93, loss = 0.0007984514813870192
iteration 94, loss = 0.0008720199693925679
iteration 95, loss = 0.0011070501059293747
iteration 96, loss = 0.0010367025388404727
iteration 97, loss = 0.001431708806194365
iteration 98, loss = 0.0015041857259348035
iteration 99, loss = 0.0009867374319583178
iteration 100, loss = 0.001219038967974484
iteration 101, loss = 0.0007962046656757593
iteration 102, loss = 0.0011020265519618988
iteration 103, loss = 0.00074318319093436
iteration 104, loss = 0.0009304282139055431
iteration 105, loss = 0.0008750269189476967
iteration 106, loss = 0.0007202393026091158
iteration 107, loss = 0.0009211458964273334
iteration 108, loss = 0.001415700069628656
iteration 109, loss = 0.0010877554304897785
iteration 110, loss = 0.0008984238374978304
iteration 111, loss = 0.0007798276492394507
iteration 112, loss = 0.0009550040122121572
iteration 113, loss = 0.0010021845810115337
iteration 114, loss = 0.0007829273235984147
iteration 115, loss = 0.0010258826659992337
iteration 116, loss = 0.0008482151315547526
iteration 117, loss = 0.0013096233597025275
iteration 118, loss = 0.0014967434108257294
iteration 119, loss = 0.0009504747577011585
iteration 120, loss = 0.0007872186251915991
iteration 121, loss = 0.0009162095375359058
iteration 122, loss = 0.000819824927020818
iteration 123, loss = 0.000887401110958308
iteration 124, loss = 0.0008729147957637906
iteration 125, loss = 0.0009490660740993917
iteration 126, loss = 0.001353830681182444
iteration 127, loss = 0.0008484131540171802
iteration 128, loss = 0.0009539247839711607
iteration 129, loss = 0.0009946756763383746
iteration 130, loss = 0.0014768796972930431
iteration 131, loss = 0.0013145087286829948
iteration 132, loss = 0.0008848416619002819
iteration 133, loss = 0.0009199298801831901
iteration 134, loss = 0.0010033727157860994
iteration 135, loss = 0.0007702212897129357
iteration 136, loss = 0.0008506927988491952
iteration 137, loss = 0.0013102695811539888
iteration 138, loss = 0.0008540267590433359
iteration 139, loss = 0.0007626197766512632
iteration 140, loss = 0.000986825325526297
iteration 141, loss = 0.001151473494246602
iteration 142, loss = 0.0011281099868938327
iteration 143, loss = 0.000723320001270622
iteration 144, loss = 0.0007033683941699564
iteration 145, loss = 0.0010271914070472121
iteration 146, loss = 0.000753076106775552
iteration 147, loss = 0.0012975087156519294
iteration 148, loss = 0.0007825405918993056
iteration 149, loss = 0.001590948668308556
iteration 150, loss = 0.001174143748357892
iteration 151, loss = 0.0009073914843611419
iteration 152, loss = 0.000785533688031137
iteration 153, loss = 0.0015461960574612021
iteration 154, loss = 0.000706010905560106
iteration 155, loss = 0.0007607307634316385
iteration 156, loss = 0.0008758689509704709
iteration 157, loss = 0.0009531502146273851
iteration 158, loss = 0.0007868195534683764
iteration 159, loss = 0.000787356577347964
iteration 160, loss = 0.0010092806769534945
iteration 161, loss = 0.0008444152772426605
iteration 162, loss = 0.0017038650112226605
iteration 163, loss = 0.0007893582223914564
iteration 164, loss = 0.0009144958457909524
iteration 165, loss = 0.0009332800982519984
iteration 166, loss = 0.0007364876801148057
iteration 167, loss = 0.000775561376940459
iteration 168, loss = 0.0008882503607310355
iteration 169, loss = 0.0010457426542416215
iteration 170, loss = 0.0009514026460237801
iteration 171, loss = 0.001029133447445929
iteration 172, loss = 0.0008023030241020024
iteration 173, loss = 0.0007749483338557184
iteration 174, loss = 0.0008094731019809842
iteration 175, loss = 0.000939144054427743
iteration 176, loss = 0.0014809437561780214
iteration 177, loss = 0.0011308752000331879
iteration 178, loss = 0.0010078633204102516
iteration 179, loss = 0.0008210594533011317
iteration 180, loss = 0.0018217406468465924
iteration 181, loss = 0.0007747349445708096
iteration 182, loss = 0.0015479248249903321
iteration 183, loss = 0.0007918669143691659
iteration 184, loss = 0.0009352716151624918
iteration 185, loss = 0.0011248411610722542
iteration 186, loss = 0.0009436897234991193
iteration 187, loss = 0.0008433742332272232
iteration 188, loss = 0.0009806876769289374
iteration 189, loss = 0.0007137474603950977
iteration 190, loss = 0.0009550378890708089
iteration 191, loss = 0.0007778819999657571
iteration 192, loss = 0.0007348887738771737
iteration 193, loss = 0.0008063921704888344
iteration 194, loss = 0.0009989015525206923
iteration 195, loss = 0.0006797835812903941
iteration 196, loss = 0.0009648394770920277
iteration 197, loss = 0.0008043777197599411
iteration 198, loss = 0.0010058308253064752
iteration 199, loss = 0.0011980091221630573
iteration 200, loss = 0.0012852037325501442
iteration 201, loss = 0.0007948545971885324
iteration 202, loss = 0.0007339863805100322
iteration 203, loss = 0.0009325527353212237
iteration 204, loss = 0.0010963494423776865
iteration 205, loss = 0.0008180709555745125
iteration 206, loss = 0.0009069181978702545
iteration 207, loss = 0.0007978997891768813
iteration 208, loss = 0.0009845932945609093
iteration 209, loss = 0.0009482568129897118
iteration 210, loss = 0.0011935601942241192
iteration 211, loss = 0.0012302004033699632
iteration 212, loss = 0.0008664087508805096
iteration 213, loss = 0.000965409679338336
iteration 214, loss = 0.00127658830024302
iteration 215, loss = 0.0009014623938128352
iteration 216, loss = 0.00081957399379462
iteration 217, loss = 0.0007638543611392379
iteration 218, loss = 0.0008019512752071023
iteration 219, loss = 0.001431967131793499
iteration 220, loss = 0.0007467332179658115
iteration 221, loss = 0.0008706457447260618
iteration 222, loss = 0.0008574656094424427
iteration 223, loss = 0.0008526499732397497
iteration 224, loss = 0.0009816924575716257
iteration 225, loss = 0.0009642026852816343
iteration 226, loss = 0.0009444370516575873
iteration 227, loss = 0.0007698691333644092
iteration 228, loss = 0.0007622655830346048
iteration 229, loss = 0.0008338944753631949
iteration 230, loss = 0.0009660671348683536
iteration 231, loss = 0.0009077598806470633
iteration 232, loss = 0.0009403486037626863
iteration 233, loss = 0.0008275475120171905
iteration 234, loss = 0.0010839829919859767
iteration 235, loss = 0.001019603805616498
iteration 236, loss = 0.0009979772148653865
iteration 237, loss = 0.000849445816129446
iteration 238, loss = 0.0014169038040563464
iteration 239, loss = 0.0011009195586666465
iteration 240, loss = 0.0010212867055088282
iteration 241, loss = 0.0012466232292354107
iteration 242, loss = 0.0011666625505313277
iteration 243, loss = 0.0008011440513655543
iteration 244, loss = 0.0008145360043272376
iteration 245, loss = 0.0007067883270792663
iteration 246, loss = 0.000693481124471873
iteration 247, loss = 0.0009095717687159777
iteration 248, loss = 0.0007215309888124466
iteration 249, loss = 0.0010044934460893273
iteration 250, loss = 0.0007427788805216551
iteration 251, loss = 0.0008413346949964762
iteration 252, loss = 0.0007724233437329531
iteration 253, loss = 0.0007645578007213771
iteration 254, loss = 0.0010432075941935182
iteration 255, loss = 0.0007579470984637737
iteration 256, loss = 0.0008427165448665619
iteration 257, loss = 0.0009600247722119093
iteration 258, loss = 0.0016226570587605238
iteration 259, loss = 0.0009162133792415261
iteration 260, loss = 0.0008081590640358627
iteration 261, loss = 0.0009593809954822063
iteration 262, loss = 0.0008012642501853406
iteration 263, loss = 0.0008519452530890703
iteration 264, loss = 0.0005984380259178579
iteration 265, loss = 0.0015607152599841356
iteration 266, loss = 0.0008149828645400703
iteration 267, loss = 0.0010210213949903846
iteration 268, loss = 0.0007736670668236911
iteration 269, loss = 0.0009635884780436754
iteration 270, loss = 0.0009926481870934367
iteration 271, loss = 0.0010957838967442513
iteration 272, loss = 0.0009231888689100742
iteration 273, loss = 0.0009798036189749837
iteration 274, loss = 0.000801837828475982
iteration 275, loss = 0.0008581500733271241
iteration 276, loss = 0.0007065387908369303
iteration 277, loss = 0.001075070584192872
iteration 278, loss = 0.000719530216883868
iteration 279, loss = 0.0007618138333782554
iteration 280, loss = 0.0006667122943326831
iteration 281, loss = 0.0008211417589336634
iteration 282, loss = 0.0008727199747227132
iteration 283, loss = 0.0007887084502726793
iteration 284, loss = 0.0007614451460540295
iteration 285, loss = 0.0010428784880787134
iteration 286, loss = 0.001077358960174024
iteration 287, loss = 0.0009155272855423391
iteration 288, loss = 0.0008226991631090641
iteration 289, loss = 0.0007788781076669693
iteration 290, loss = 0.0007080453215166926
iteration 291, loss = 0.0007256317185238004
iteration 292, loss = 0.0008284909417852759
iteration 293, loss = 0.00065392762189731
iteration 294, loss = 0.0008381273946724832
iteration 295, loss = 0.0008089570328593254
iteration 296, loss = 0.0009358267416246235
iteration 297, loss = 0.0011255342978984118
iteration 298, loss = 0.001015189802274108
iteration 299, loss = 0.0006498652510344982
iteration 300, loss = 0.0015832396456971765
iteration 1, loss = 0.0007743143942207098
iteration 2, loss = 0.0007283486775122583
iteration 3, loss = 0.0008121386053971946
iteration 4, loss = 0.0008444975246675313
iteration 5, loss = 0.0008148018969222903
iteration 6, loss = 0.0007181013352237642
iteration 7, loss = 0.0019408946391195059
iteration 8, loss = 0.0007323622121475637
iteration 9, loss = 0.000957947806455195
iteration 10, loss = 0.0009374843793921173
iteration 11, loss = 0.0007854843279346824
iteration 12, loss = 0.0007624109275639057
iteration 13, loss = 0.0011871830793097615
iteration 14, loss = 0.0007793866097927094
iteration 15, loss = 0.0015516832936555147
iteration 16, loss = 0.0009559214231558144
iteration 17, loss = 0.0008043944835662842
iteration 18, loss = 0.0007909986888989806
iteration 19, loss = 0.0007956106564961374
iteration 20, loss = 0.0010848450474441051
iteration 21, loss = 0.0009885946055874228
iteration 22, loss = 0.0009968036320060492
iteration 23, loss = 0.000894519907888025
iteration 24, loss = 0.0009557330631650984
iteration 25, loss = 0.0008052251068875194
iteration 26, loss = 0.0011992662912234664
iteration 27, loss = 0.001161437132395804
iteration 28, loss = 0.0009520755847916007
iteration 29, loss = 0.0008129934431053698
iteration 30, loss = 0.0010878655593842268
iteration 31, loss = 0.0007617822266183794
iteration 32, loss = 0.0007856802549213171
iteration 33, loss = 0.0011072906199842691
iteration 34, loss = 0.0008818988571874797
iteration 35, loss = 0.0007749634678475559
iteration 36, loss = 0.0010659778490662575
iteration 37, loss = 0.0008542218711227179
iteration 38, loss = 0.0007593812770210207
iteration 39, loss = 0.0008435382042080164
iteration 40, loss = 0.000984093057923019
iteration 41, loss = 0.0008099394617602229
iteration 42, loss = 0.000744957011193037
iteration 43, loss = 0.0016428320668637753
iteration 44, loss = 0.0011338084004819393
iteration 45, loss = 0.000947210006415844
iteration 46, loss = 0.0010370418895035982
iteration 47, loss = 0.0009944892954081297
iteration 48, loss = 0.0013684462755918503
iteration 49, loss = 0.0007967513520270586
iteration 50, loss = 0.00097813387401402
iteration 51, loss = 0.0010078244376927614
iteration 52, loss = 0.0008535647066310048
iteration 53, loss = 0.0009664998506195843
iteration 54, loss = 0.0011382369557395577
iteration 55, loss = 0.0008400798542425036
iteration 56, loss = 0.000778490211814642
iteration 57, loss = 0.0009874030947685242
iteration 58, loss = 0.000888227135874331
iteration 59, loss = 0.0009070088271982968
iteration 60, loss = 0.0008189474465325475
iteration 61, loss = 0.0006871169898658991
iteration 62, loss = 0.0012757307849824429
iteration 63, loss = 0.0008013288606889546
iteration 64, loss = 0.0007098681526258588
iteration 65, loss = 0.0008981612045317888
iteration 66, loss = 0.0008455786155536771
iteration 67, loss = 0.0007307890336960554
iteration 68, loss = 0.0007719889399595559
iteration 69, loss = 0.0009152014972642064
iteration 70, loss = 0.0009935525013133883
iteration 71, loss = 0.0009846740867942572
iteration 72, loss = 0.0016477518947795033
iteration 73, loss = 0.0007768377545289695
iteration 74, loss = 0.0013749930076301098
iteration 75, loss = 0.000949686043895781
iteration 76, loss = 0.0010536402696743608
iteration 77, loss = 0.0013814904959872365
iteration 78, loss = 0.0010326448827981949
iteration 79, loss = 0.0008026454597711563
iteration 80, loss = 0.000916288117878139
iteration 81, loss = 0.0007624950958415866
iteration 82, loss = 0.0014931848272681236
iteration 83, loss = 0.001365295727737248
iteration 84, loss = 0.0011909296736121178
iteration 85, loss = 0.001248830114491284
iteration 86, loss = 0.0008797589689493179
iteration 87, loss = 0.0008237991132773459
iteration 88, loss = 0.0009598678443580866
iteration 89, loss = 0.0024855334777384996
iteration 90, loss = 0.001000097836367786
iteration 91, loss = 0.0008096233359538019
iteration 92, loss = 0.0007658760296180844
iteration 93, loss = 0.0008319899206981063
iteration 94, loss = 0.0008741965866647661
iteration 95, loss = 0.001543206162750721
iteration 96, loss = 0.0010337925050407648
iteration 97, loss = 0.0007359722512774169
iteration 98, loss = 0.0014258555602282286
iteration 99, loss = 0.0008383553358726203
iteration 100, loss = 0.0008265013457275927
iteration 101, loss = 0.001805917709134519
iteration 102, loss = 0.0008573014056310058
iteration 103, loss = 0.0008188682259060442
iteration 104, loss = 0.0013325477484613657
iteration 105, loss = 0.0008871874306350946
iteration 106, loss = 0.0007921174983493984
iteration 107, loss = 0.0018018565606325865
iteration 108, loss = 0.0010788057697936893
iteration 109, loss = 0.0008673227857798338
iteration 110, loss = 0.0007237220415845513
iteration 111, loss = 0.0008797339396551251
iteration 112, loss = 0.0007313576061278582
iteration 113, loss = 0.0008041844121180475
iteration 114, loss = 0.0013598158257082105
iteration 115, loss = 0.0010080172214657068
iteration 116, loss = 0.0010760421864688396
iteration 117, loss = 0.001784822321496904
iteration 118, loss = 0.0016051214188337326
iteration 119, loss = 0.0007594887865707278
iteration 120, loss = 0.0008676400757394731
iteration 121, loss = 0.0017256300197914243
iteration 122, loss = 0.0007821776089258492
iteration 123, loss = 0.0009124536300078034
iteration 124, loss = 0.0007775159901939332
iteration 125, loss = 0.0010160470847040415
iteration 126, loss = 0.0009069516090676188
iteration 127, loss = 0.0007540168007835746
iteration 128, loss = 0.0009609541739337146
iteration 129, loss = 0.0008560445858165622
iteration 130, loss = 0.0011734041618183255
iteration 131, loss = 0.0009459489374421537
iteration 132, loss = 0.0008846049895510077
iteration 133, loss = 0.0006765868165530264
iteration 134, loss = 0.0015758057124912739
iteration 135, loss = 0.0007923207012936473
iteration 136, loss = 0.0010268727783113718
iteration 137, loss = 0.0006300011882558465
iteration 138, loss = 0.0010190412867814302
iteration 139, loss = 0.0008064226130954921
iteration 140, loss = 0.000771612802054733
iteration 141, loss = 0.0007799502927809954
iteration 142, loss = 0.0008135768002830446
iteration 143, loss = 0.00087435805471614
iteration 144, loss = 0.0010668421164155006
iteration 145, loss = 0.0010517853079363704
iteration 146, loss = 0.0009188598487526178
iteration 147, loss = 0.000776340311858803
iteration 148, loss = 0.001000016345642507
iteration 149, loss = 0.0008497204980812967
iteration 150, loss = 0.000913323718123138
iteration 151, loss = 0.0011456782231107354
iteration 152, loss = 0.0007547055720351636
iteration 153, loss = 0.0007959326612763107
iteration 154, loss = 0.0012439630227163434
iteration 155, loss = 0.0007820630562491715
iteration 156, loss = 0.0006826483295299113
iteration 157, loss = 0.0007246335153467953
iteration 158, loss = 0.0009118958842009306
iteration 159, loss = 0.0008625076152384281
iteration 160, loss = 0.001061478047631681
iteration 161, loss = 0.0011020106030628085
iteration 162, loss = 0.0015266644768416882
iteration 163, loss = 0.0009629586129449308
iteration 164, loss = 0.0009625398670323193
iteration 165, loss = 0.0022851189132779837
iteration 166, loss = 0.0008390849106945097
iteration 167, loss = 0.0012248028069734573
iteration 168, loss = 0.0008911519544199109
iteration 169, loss = 0.0009285247651860118
iteration 170, loss = 0.000796982494648546
iteration 171, loss = 0.0007476691971533
iteration 172, loss = 0.0011425991542637348
iteration 173, loss = 0.0007988465949892998
iteration 174, loss = 0.0007440425688400865
iteration 175, loss = 0.0008396414341405034
iteration 176, loss = 0.0009614136652089655
iteration 177, loss = 0.0008110557100735605
iteration 178, loss = 0.0013196668587625027
iteration 179, loss = 0.0009057224378921092
iteration 180, loss = 0.0007724036695435643
iteration 181, loss = 0.0008259068708866835
iteration 182, loss = 0.0009781884727999568
iteration 183, loss = 0.0008163461461663246
iteration 184, loss = 0.0010143970139324665
iteration 185, loss = 0.0017032864270731807
iteration 186, loss = 0.0009780654218047857
iteration 187, loss = 0.0007369507220573723
iteration 188, loss = 0.0007516007171943784
iteration 189, loss = 0.0007734914543107152
iteration 190, loss = 0.0007272921502590179
iteration 191, loss = 0.0009360756957903504
iteration 192, loss = 0.0010789993684738874
iteration 193, loss = 0.0008469028980471194
iteration 194, loss = 0.0010005788644775748
iteration 195, loss = 0.0009798568207770586
iteration 196, loss = 0.0009317647200077772
iteration 197, loss = 0.0007648197351954877
iteration 198, loss = 0.001025251462124288
iteration 199, loss = 0.000786607270129025
iteration 200, loss = 0.0011417897185310721
iteration 201, loss = 0.00079846236621961
iteration 202, loss = 0.0007336482522077858
iteration 203, loss = 0.0008795981993898749
iteration 204, loss = 0.0007827642257325351
iteration 205, loss = 0.0009331494802609086
iteration 206, loss = 0.0009614670416340232
iteration 207, loss = 0.0008925481233745813
iteration 208, loss = 0.0007963196840137243
iteration 209, loss = 0.0011953564826399088
iteration 210, loss = 0.0010200743563473225
iteration 211, loss = 0.0007997624925337732
iteration 212, loss = 0.0008609460783191025
iteration 213, loss = 0.0008285763906314969
iteration 214, loss = 0.0009754616185091436
iteration 215, loss = 0.0009730244055390358
iteration 216, loss = 0.001123991096392274
iteration 217, loss = 0.0008254435961134732
iteration 218, loss = 0.0006859872955828905
iteration 219, loss = 0.0008096208330243826
iteration 220, loss = 0.0008220912422984838
iteration 221, loss = 0.0008211187669076025
iteration 222, loss = 0.0014925339492037892
iteration 223, loss = 0.0007820262107998133
iteration 224, loss = 0.0006942116306163371
iteration 225, loss = 0.0010452951537445188
iteration 226, loss = 0.0009629666456021369
iteration 227, loss = 0.0015024414751678705
iteration 228, loss = 0.0013154814951121807
iteration 229, loss = 0.0010836068540811539
iteration 230, loss = 0.000909533177036792
iteration 231, loss = 0.002220539841800928
iteration 232, loss = 0.0008367271511815488
iteration 233, loss = 0.0009041982120834291
iteration 234, loss = 0.0007769434596411884
iteration 235, loss = 0.000732090906240046
iteration 236, loss = 0.0008184444159269333
iteration 237, loss = 0.0009704003459773958
iteration 238, loss = 0.0007386073120869696
iteration 239, loss = 0.0010283878073096275
iteration 240, loss = 0.0008078792016021907
iteration 241, loss = 0.0010340799344703555
iteration 242, loss = 0.0008341029169969261
iteration 243, loss = 0.0007244854932650924
iteration 244, loss = 0.0009578245226293802
iteration 245, loss = 0.0008372010197490454
iteration 246, loss = 0.0007470350828953087
iteration 247, loss = 0.0010169991292059422
iteration 248, loss = 0.000829046533908695
iteration 249, loss = 0.0010667521273717284
iteration 250, loss = 0.0008881635149009526
iteration 251, loss = 0.001174494973383844
iteration 252, loss = 0.0015900498256087303
iteration 253, loss = 0.0008573753875680268
iteration 254, loss = 0.0009218266932293773
iteration 255, loss = 0.0009682919480837882
iteration 256, loss = 0.0011672547552734613
iteration 257, loss = 0.0007549991132691503
iteration 258, loss = 0.00082176155410707
iteration 259, loss = 0.0009559296304360032
iteration 260, loss = 0.000952246889937669
iteration 261, loss = 0.001043681288138032
iteration 262, loss = 0.0011932698544114828
iteration 263, loss = 0.0010935805039480329
iteration 264, loss = 0.0010878745233640075
iteration 265, loss = 0.0008169549400918186
iteration 266, loss = 0.0007623538258485496
iteration 267, loss = 0.0007666235323995352
iteration 268, loss = 0.0007771196542307734
iteration 269, loss = 0.000725866062566638
iteration 270, loss = 0.0007363976910710335
iteration 271, loss = 0.0009511371026746929
iteration 272, loss = 0.0007954558823257685
iteration 273, loss = 0.0008765239617787302
iteration 274, loss = 0.000898557947948575
iteration 275, loss = 0.0009163797367364168
iteration 276, loss = 0.0010866813827306032
iteration 277, loss = 0.0007340220035985112
iteration 278, loss = 0.000725901045370847
iteration 279, loss = 0.0009644642705097795
iteration 280, loss = 0.0014207835774868727
iteration 281, loss = 0.0007715682149864733
iteration 282, loss = 0.001639109686948359
iteration 283, loss = 0.0009306085994467139
iteration 284, loss = 0.0009200890199281275
iteration 285, loss = 0.0008909219759516418
iteration 286, loss = 0.0008761159260757267
iteration 287, loss = 0.0009135834407061338
iteration 288, loss = 0.0018266852712258697
iteration 289, loss = 0.0009181014029309154
iteration 290, loss = 0.0007986213313415647
iteration 291, loss = 0.0007991220336407423
iteration 292, loss = 0.001394382561556995
iteration 293, loss = 0.001900496892631054
iteration 294, loss = 0.000942667422350496
iteration 295, loss = 0.0017722053453326225
iteration 296, loss = 0.000776139204390347
iteration 297, loss = 0.0009936393471434712
iteration 298, loss = 0.0010926011018455029
iteration 299, loss = 0.0008562037255614996
iteration 300, loss = 0.0008285517105832696
iteration 1, loss = 0.0015228096162900329
iteration 2, loss = 0.0009255579789169133
iteration 3, loss = 0.0009663162636570632
iteration 4, loss = 0.0010266993194818497
iteration 5, loss = 0.0007586257415823638
iteration 6, loss = 0.0009185097296722233
iteration 7, loss = 0.0008995752432383597
iteration 8, loss = 0.0007426071679219604
iteration 9, loss = 0.0007878707256168127
iteration 10, loss = 0.0007897678297013044
iteration 11, loss = 0.0009170886478386819
iteration 12, loss = 0.0007716206600889564
iteration 13, loss = 0.0006958917947486043
iteration 14, loss = 0.0007933190790936351
iteration 15, loss = 0.0008488735766150057
iteration 16, loss = 0.0017191856168210506
iteration 17, loss = 0.0008763995720073581
iteration 18, loss = 0.0008505857549607754
iteration 19, loss = 0.0016683988505974412
iteration 20, loss = 0.0009780412074178457
iteration 21, loss = 0.0007314532995223999
iteration 22, loss = 0.0013482747599482536
iteration 23, loss = 0.0011503504356369376
iteration 24, loss = 0.0007872333517298102
iteration 25, loss = 0.000783641473390162
iteration 26, loss = 0.0009562969207763672
iteration 27, loss = 0.0007975960033945739
iteration 28, loss = 0.0008304676157422364
iteration 29, loss = 0.0008266831282526255
iteration 30, loss = 0.001159011502750218
iteration 31, loss = 0.0011002542451024055
iteration 32, loss = 0.0007612344925291836
iteration 33, loss = 0.0010059005580842495
iteration 34, loss = 0.0007645352743566036
iteration 35, loss = 0.0007396730361506343
iteration 36, loss = 0.0007199085084721446
iteration 37, loss = 0.0008366468828171492
iteration 38, loss = 0.0006861354922875762
iteration 39, loss = 0.0015417840331792831
iteration 40, loss = 0.0010203422280028462
iteration 41, loss = 0.0009885331382974982
iteration 42, loss = 0.0010034774895757437
iteration 43, loss = 0.0013008895330131054
iteration 44, loss = 0.0009260901133529842
iteration 45, loss = 0.000938181416131556
iteration 46, loss = 0.0008668439695611596
iteration 47, loss = 0.0007163089467212558
iteration 48, loss = 0.0008016794454306364
iteration 49, loss = 0.0007924977107904851
iteration 50, loss = 0.0016834406415000558
iteration 51, loss = 0.0008100402192212641
iteration 52, loss = 0.0008689276874065399
iteration 53, loss = 0.00096653844229877
iteration 54, loss = 0.0007354438421316445
iteration 55, loss = 0.0015423147706314921
iteration 56, loss = 0.0009465789771638811
iteration 57, loss = 0.0009676498593762517
iteration 58, loss = 0.0008996493997983634
iteration 59, loss = 0.0022654111962765455
iteration 60, loss = 0.0009437212138436735
iteration 61, loss = 0.001269614091143012
iteration 62, loss = 0.0009062988101504743
iteration 63, loss = 0.0014243214391171932
iteration 64, loss = 0.0010111816227436066
iteration 65, loss = 0.000778922694735229
iteration 66, loss = 0.0016635978827252984
iteration 67, loss = 0.0008600511937402189
iteration 68, loss = 0.0008851136662997305
iteration 69, loss = 0.0008127872133627534
iteration 70, loss = 0.0010190781904384494
iteration 71, loss = 0.0009928285144269466
iteration 72, loss = 0.0017709508538246155
iteration 73, loss = 0.0008252623956650496
iteration 74, loss = 0.0008054011268541217
iteration 75, loss = 0.0010293163359165192
iteration 76, loss = 0.0010676836827769876
iteration 77, loss = 0.0007530351867899299
iteration 78, loss = 0.0008994287345558405
iteration 79, loss = 0.0008693246054463089
iteration 80, loss = 0.0014384533278644085
iteration 81, loss = 0.0007783181499689817
iteration 82, loss = 0.0010687366593629122
iteration 83, loss = 0.0007754393154755235
iteration 84, loss = 0.0008368153939954937
iteration 85, loss = 0.0007575774798169732
iteration 86, loss = 0.0010500176576897502
iteration 87, loss = 0.0017659661825746298
iteration 88, loss = 0.0008014481863938272
iteration 89, loss = 0.0007846217486076057
iteration 90, loss = 0.0011173456441611052
iteration 91, loss = 0.0010065968381240964
iteration 92, loss = 0.0007873647846281528
iteration 93, loss = 0.0007920873467810452
iteration 94, loss = 0.0012206127867102623
iteration 95, loss = 0.0009146201191470027
iteration 96, loss = 0.0008439908269792795
iteration 97, loss = 0.0009060235461220145
iteration 98, loss = 0.0011725272051990032
iteration 99, loss = 0.0007279285346157849
iteration 100, loss = 0.0008319150074385107
iteration 101, loss = 0.0009422258008271456
iteration 102, loss = 0.0008568030316382647
iteration 103, loss = 0.0008248575613833964
iteration 104, loss = 0.0008286550873890519
iteration 105, loss = 0.0008171702502295375
iteration 106, loss = 0.0013518757186830044
iteration 107, loss = 0.0011050867615267634
iteration 108, loss = 0.0010480474447831511
iteration 109, loss = 0.0011752407299354672
iteration 110, loss = 0.0011095929658040404
iteration 111, loss = 0.0010863256175071
iteration 112, loss = 0.0007935501635074615
iteration 113, loss = 0.00070188072277233
iteration 114, loss = 0.0008835174376145005
iteration 115, loss = 0.0007391583640128374
iteration 116, loss = 0.0007868111715652049
iteration 117, loss = 0.0007593301706947386
iteration 118, loss = 0.000741603085771203
iteration 119, loss = 0.0012594747822731733
iteration 120, loss = 0.0010389818344265223
iteration 121, loss = 0.0008689638925716281
iteration 122, loss = 0.0009797890670597553
iteration 123, loss = 0.0010514333844184875
iteration 124, loss = 0.0007066780817694962
iteration 125, loss = 0.0009179507032968104
iteration 126, loss = 0.0007752205128781497
iteration 127, loss = 0.0008606801857240498
iteration 128, loss = 0.000800666690338403
iteration 129, loss = 0.0010322307934984565
iteration 130, loss = 0.0014024899573996663
iteration 131, loss = 0.000795787840615958
iteration 132, loss = 0.0007539656944572926
iteration 133, loss = 0.0007268229965120554
iteration 134, loss = 0.0008048772579059005
iteration 135, loss = 0.002074572490528226
iteration 136, loss = 0.0009632119326852262
iteration 137, loss = 0.0007527086418122053
iteration 138, loss = 0.000978463445790112
iteration 139, loss = 0.0008120665443129838
iteration 140, loss = 0.0010138731449842453
iteration 141, loss = 0.0006998673779889941
iteration 142, loss = 0.001584147335961461
iteration 143, loss = 0.0007813823176547885
iteration 144, loss = 0.0007105275290086865
iteration 145, loss = 0.0009026366169564426
iteration 146, loss = 0.0008948243339546025
iteration 147, loss = 0.0007712659426033497
iteration 148, loss = 0.0011798969935625792
iteration 149, loss = 0.0015246265102177858
iteration 150, loss = 0.001550006098113954
iteration 151, loss = 0.002020664280280471
iteration 152, loss = 0.0014137548860162497
iteration 153, loss = 0.0011751889251172543
iteration 154, loss = 0.0010347156785428524
iteration 155, loss = 0.0008336985483765602
iteration 156, loss = 0.0008421687525697052
iteration 157, loss = 0.0009357493254356086
iteration 158, loss = 0.0008216595742851496
iteration 159, loss = 0.0007351227104663849
iteration 160, loss = 0.0007571132155135274
iteration 161, loss = 0.0012596379965543747
iteration 162, loss = 0.0011101107811555266
iteration 163, loss = 0.0008653203258290887
iteration 164, loss = 0.0018433408113196492
iteration 165, loss = 0.0010030155535787344
iteration 166, loss = 0.0009007314220070839
iteration 167, loss = 0.0012372686760500073
iteration 168, loss = 0.0008596353000029922
iteration 169, loss = 0.0007655356894247234
iteration 170, loss = 0.0007603883277624846
iteration 171, loss = 0.000649818277452141
iteration 172, loss = 0.0012212260626256466
iteration 173, loss = 0.0008303179056383669
iteration 174, loss = 0.0009785835864022374
iteration 175, loss = 0.000945422041695565
iteration 176, loss = 0.0008252233965322375
iteration 177, loss = 0.0008747100364416838
iteration 178, loss = 0.0006950030801817775
iteration 179, loss = 0.00088832201436162
iteration 180, loss = 0.0010929922573268414
iteration 181, loss = 0.0008395451586693525
iteration 182, loss = 0.0009556323057040572
iteration 183, loss = 0.0009042511810548604
iteration 184, loss = 0.0008665395434945822
iteration 185, loss = 0.0007496241014450788
iteration 186, loss = 0.0011634579859673977
iteration 187, loss = 0.0013390373205766082
iteration 188, loss = 0.001272096997126937
iteration 189, loss = 0.0013728539925068617
iteration 190, loss = 0.000852611381560564
iteration 191, loss = 0.0009969193488359451
iteration 192, loss = 0.0011179097928106785
iteration 193, loss = 0.0008161365403793752
iteration 194, loss = 0.002121657133102417
iteration 195, loss = 0.0010286164470016956
iteration 196, loss = 0.0009297921787947416
iteration 197, loss = 0.0011082615237683058
iteration 198, loss = 0.0010624068090692163
iteration 199, loss = 0.0013202955015003681
iteration 200, loss = 0.0007570887100882828
iteration 201, loss = 0.0011845557019114494
iteration 202, loss = 0.0012865426251664758
iteration 203, loss = 0.0009556286968290806
iteration 204, loss = 0.0013096086913719773
iteration 205, loss = 0.0007013953290879726
iteration 206, loss = 0.0007491407450288534
iteration 207, loss = 0.0007926868856884539
iteration 208, loss = 0.000838525069411844
iteration 209, loss = 0.0008252183324657381
iteration 210, loss = 0.0015419709961861372
iteration 211, loss = 0.0007374960114248097
iteration 212, loss = 0.0009569949470460415
iteration 213, loss = 0.0009182419744320214
iteration 214, loss = 0.0015774111961945891
iteration 215, loss = 0.0006795382359996438
iteration 216, loss = 0.0008509047911502421
iteration 217, loss = 0.000983799109235406
iteration 218, loss = 0.0007587288855575025
iteration 219, loss = 0.0012272504391148686
iteration 220, loss = 0.000662316451780498
iteration 221, loss = 0.0008452190086245537
iteration 222, loss = 0.001478630700148642
iteration 223, loss = 0.0009221126674674451
iteration 224, loss = 0.0008242628537118435
iteration 225, loss = 0.0008502635173499584
iteration 226, loss = 0.0008181154844351113
iteration 227, loss = 0.0009567446541041136
iteration 228, loss = 0.0010141675593331456
iteration 229, loss = 0.0009863890009000897
iteration 230, loss = 0.0007137744687497616
iteration 231, loss = 0.0006970351678319275
iteration 232, loss = 0.0009626639075577259
iteration 233, loss = 0.0007772849639877677
iteration 234, loss = 0.0008814539760351181
iteration 235, loss = 0.0008060591062530875
iteration 236, loss = 0.0007362457108683884
iteration 237, loss = 0.0009539636084809899
iteration 238, loss = 0.000983593170531094
iteration 239, loss = 0.0007495555328205228
iteration 240, loss = 0.0009589463006705046
iteration 241, loss = 0.001112490426748991
iteration 242, loss = 0.0008272616541944444
iteration 243, loss = 0.0007816552533768117
iteration 244, loss = 0.0007581172976642847
iteration 245, loss = 0.0008980362326838076
iteration 246, loss = 0.0010310825891792774
iteration 247, loss = 0.0008087328169494867
iteration 248, loss = 0.0011327211977913976
iteration 249, loss = 0.0009054254624061286
iteration 250, loss = 0.0009309746092185378
iteration 251, loss = 0.001467641443014145
iteration 252, loss = 0.0007940279319882393
iteration 253, loss = 0.001483882311731577
iteration 254, loss = 0.00104152609128505
iteration 255, loss = 0.0011891103349626064
iteration 256, loss = 0.0010662756394594908
iteration 257, loss = 0.0014044384006410837
iteration 258, loss = 0.0007911433931440115
iteration 259, loss = 0.0007065152749419212
iteration 260, loss = 0.0008499107789248228
iteration 261, loss = 0.0007673460058867931
iteration 262, loss = 0.001097578089684248
iteration 263, loss = 0.0007473112200386822
iteration 264, loss = 0.0008377268095500767
iteration 265, loss = 0.0008728028042241931
iteration 266, loss = 0.0007723788730800152
iteration 267, loss = 0.0009435400716029108
iteration 268, loss = 0.0008847913704812527
iteration 269, loss = 0.0008919680258259177
iteration 270, loss = 0.001637592213228345
iteration 271, loss = 0.0007479228079319
iteration 272, loss = 0.0007854917203076184
iteration 273, loss = 0.001735305180773139
iteration 274, loss = 0.0007300295401364565
iteration 275, loss = 0.0007754043326713145
iteration 276, loss = 0.0009310090681537986
iteration 277, loss = 0.0012902216985821724
iteration 278, loss = 0.0012983963824808598
iteration 279, loss = 0.0010766121558845043
iteration 280, loss = 0.0007786885253153741
iteration 281, loss = 0.0008199714357033372
iteration 282, loss = 0.0009084346238523722
iteration 283, loss = 0.0007814874988980591
iteration 284, loss = 0.0013073061127215624
iteration 285, loss = 0.0007692534709349275
iteration 286, loss = 0.0007698308909311891
iteration 287, loss = 0.0008636843995191157
iteration 288, loss = 0.0008759706979617476
iteration 289, loss = 0.0016372338868677616
iteration 290, loss = 0.0007530881557613611
iteration 291, loss = 0.0009765725117176771
iteration 292, loss = 0.0007362897740676999
iteration 293, loss = 0.000705938960891217
iteration 294, loss = 0.0007298322743736207
iteration 295, loss = 0.0007653876673430204
iteration 296, loss = 0.0007851241389289498
iteration 297, loss = 0.0009568465757183731
iteration 298, loss = 0.0009254849283024669
iteration 299, loss = 0.0008572712540626526
iteration 300, loss = 0.0008099283440969884
iteration 1, loss = 0.0007838413002900779
iteration 2, loss = 0.0007900503114797175
iteration 3, loss = 0.0009059212170541286
iteration 4, loss = 0.0009746496216394007
iteration 5, loss = 0.0007076910114847124
iteration 6, loss = 0.0008997535915113986
iteration 7, loss = 0.0009691417799331248
iteration 8, loss = 0.0010222236160188913
iteration 9, loss = 0.0007890936685726047
iteration 10, loss = 0.0007445783703587949
iteration 11, loss = 0.0007806374924257398
iteration 12, loss = 0.0009343698620796204
iteration 13, loss = 0.0010811432730406523
iteration 14, loss = 0.0008979045087471604
iteration 15, loss = 0.0012556740548461676
iteration 16, loss = 0.0008269458194263279
iteration 17, loss = 0.0009011379443109035
iteration 18, loss = 0.0010650206822901964
iteration 19, loss = 0.0009375254157930613
iteration 20, loss = 0.0008040840039029717
iteration 21, loss = 0.0007264672894962132
iteration 22, loss = 0.0007189121097326279
iteration 23, loss = 0.0009465954499319196
iteration 24, loss = 0.000985743128694594
iteration 25, loss = 0.0008059887331910431
iteration 26, loss = 0.0006852259393781424
iteration 27, loss = 0.0007674481021240354
iteration 28, loss = 0.0006928072543814778
iteration 29, loss = 0.0009059818112291396
iteration 30, loss = 0.0008075600489974022
iteration 31, loss = 0.0008940660045482218
iteration 32, loss = 0.001221596379764378
iteration 33, loss = 0.000993059598840773
iteration 34, loss = 0.0009929996449500322
iteration 35, loss = 0.0008889392483979464
iteration 36, loss = 0.0007944261305965483
iteration 37, loss = 0.0007877625757828355
iteration 38, loss = 0.0007362791220657527
iteration 39, loss = 0.0008313343860208988
iteration 40, loss = 0.0008922708220779896
iteration 41, loss = 0.0008386581903323531
iteration 42, loss = 0.0008923801360651851
iteration 43, loss = 0.0007631834014318883
iteration 44, loss = 0.0010160160018131137
iteration 45, loss = 0.0006802439456805587
iteration 46, loss = 0.0007365851779468358
iteration 47, loss = 0.0008950597257353365
iteration 48, loss = 0.0012210696004331112
iteration 49, loss = 0.0010062041692435741
iteration 50, loss = 0.001264616148546338
iteration 51, loss = 0.0008459605160169303
iteration 52, loss = 0.0008277813321910799
iteration 53, loss = 0.0010616877116262913
iteration 54, loss = 0.0007314127287827432
iteration 55, loss = 0.000869401847012341
iteration 56, loss = 0.0007879051263444126
iteration 57, loss = 0.0011179782450199127
iteration 58, loss = 0.0009177245083265007
iteration 59, loss = 0.0008324920781888068
iteration 60, loss = 0.000808443408459425
iteration 61, loss = 0.0011379255447536707
iteration 62, loss = 0.0013699010014533997
iteration 63, loss = 0.0009415315580554307
iteration 64, loss = 0.0011633376125246286
iteration 65, loss = 0.0009902255842462182
iteration 66, loss = 0.0007778098806738853
iteration 67, loss = 0.0012499723816290498
iteration 68, loss = 0.0022819763980805874
iteration 69, loss = 0.0008080913103185594
iteration 70, loss = 0.0015752092003822327
iteration 71, loss = 0.0009751207544468343
iteration 72, loss = 0.0013860076433047652
iteration 73, loss = 0.0007696052780374885
iteration 74, loss = 0.0012311360333114862
iteration 75, loss = 0.0009249168215319514
iteration 76, loss = 0.0008771442226134241
iteration 77, loss = 0.000878923456184566
iteration 78, loss = 0.0007647176971659064
iteration 79, loss = 0.0008934671059250832
iteration 80, loss = 0.0008341362699866295
iteration 81, loss = 0.000876865116879344
iteration 82, loss = 0.0008871473837643862
iteration 83, loss = 0.0016873690765351057
iteration 84, loss = 0.0006258734501898289
iteration 85, loss = 0.0018586164806038141
iteration 86, loss = 0.0021126193460077047
iteration 87, loss = 0.0006877128034830093
iteration 88, loss = 0.0008449933957308531
iteration 89, loss = 0.0010443765204399824
iteration 90, loss = 0.0008793089655227959
iteration 91, loss = 0.0017566353781148791
iteration 92, loss = 0.0008994403178803623
iteration 93, loss = 0.0011281438637524843
iteration 94, loss = 0.0008845486445352435
iteration 95, loss = 0.0008433390758000314
iteration 96, loss = 0.0011681285686790943
iteration 97, loss = 0.0014620294095948339
iteration 98, loss = 0.0007620300748385489
iteration 99, loss = 0.0008537803078070283
iteration 100, loss = 0.0007024179212749004
iteration 101, loss = 0.0011669847881421447
iteration 102, loss = 0.0007579606026411057
iteration 103, loss = 0.0007948135025799274
iteration 104, loss = 0.0014636818086728454
iteration 105, loss = 0.0011796377366408706
iteration 106, loss = 0.0008287940872833133
iteration 107, loss = 0.0007668220205232501
iteration 108, loss = 0.0006524034542962909
iteration 109, loss = 0.0016277680406346917
iteration 110, loss = 0.0011962293647229671
iteration 111, loss = 0.000725453719496727
iteration 112, loss = 0.0007186421426013112
iteration 113, loss = 0.0009375083609484136
iteration 114, loss = 0.0009516900172457099
iteration 115, loss = 0.0007894288864918053
iteration 116, loss = 0.0010214683134108782
iteration 117, loss = 0.0007059254567138851
iteration 118, loss = 0.0009812758071348071
iteration 119, loss = 0.0009031178196892142
iteration 120, loss = 0.0006530052633024752
iteration 121, loss = 0.0016521608922630548
iteration 122, loss = 0.0008971641073003411
iteration 123, loss = 0.0021866243332624435
iteration 124, loss = 0.0011129416525363922
iteration 125, loss = 0.0012545508798211813
iteration 126, loss = 0.0008871860336512327
iteration 127, loss = 0.0009118278394453228
iteration 128, loss = 0.0008279975154437125
iteration 129, loss = 0.0009075823472812772
iteration 130, loss = 0.0009550908580422401
iteration 131, loss = 0.0008504993747919798
iteration 132, loss = 0.0007875888841226697
iteration 133, loss = 0.0009003016166388988
iteration 134, loss = 0.0008232371183112264
iteration 135, loss = 0.0016078304033726454
iteration 136, loss = 0.0007457634783349931
iteration 137, loss = 0.0007328407955355942
iteration 138, loss = 0.0006707227439619601
iteration 139, loss = 0.0007452474092133343
iteration 140, loss = 0.0008901086985133588
iteration 141, loss = 0.0009318400407209992
iteration 142, loss = 0.0009275863412767649
iteration 143, loss = 0.0011157802073284984
iteration 144, loss = 0.0010194345377385616
iteration 145, loss = 0.00078174052760005
iteration 146, loss = 0.0007932297885417938
iteration 147, loss = 0.0008023505215533078
iteration 148, loss = 0.0009449373465031385
iteration 149, loss = 0.0007745625334791839
iteration 150, loss = 0.0014759687473997474
iteration 151, loss = 0.0006778754177503288
iteration 152, loss = 0.0010540744988247752
iteration 153, loss = 0.0008934406796470284
iteration 154, loss = 0.0007977851782925427
iteration 155, loss = 0.0008905269205570221
iteration 156, loss = 0.0011380192590877414
iteration 157, loss = 0.0008001002133823931
iteration 158, loss = 0.0008821551455184817
iteration 159, loss = 0.0008432873291894794
iteration 160, loss = 0.0008874697959981859
iteration 161, loss = 0.0008209046209231019
iteration 162, loss = 0.0014928540913388133
iteration 163, loss = 0.0008954032091423869
iteration 164, loss = 0.0007658917456865311
iteration 165, loss = 0.0007168690208345652
iteration 166, loss = 0.0010715121170505881
iteration 167, loss = 0.0013983405660837889
iteration 168, loss = 0.0009143807692453265
iteration 169, loss = 0.0006782641867175698
iteration 170, loss = 0.0009349531028419733
iteration 171, loss = 0.000769169011618942
iteration 172, loss = 0.0008247984223999083
iteration 173, loss = 0.0008684564963914454
iteration 174, loss = 0.000923500454518944
iteration 175, loss = 0.0008036823710426688
iteration 176, loss = 0.0009099786402657628
iteration 177, loss = 0.0009430397185496986
iteration 178, loss = 0.0009230035939253867
iteration 179, loss = 0.0007600884418934584
iteration 180, loss = 0.000772211526054889
iteration 181, loss = 0.0009538749582134187
iteration 182, loss = 0.0009166568052023649
iteration 183, loss = 0.001654887804761529
iteration 184, loss = 0.0008570898789912462
iteration 185, loss = 0.001480419421568513
iteration 186, loss = 0.0008017847430892289
iteration 187, loss = 0.0009855650132521987
iteration 188, loss = 0.000700631586369127
iteration 189, loss = 0.0008388201240450144
iteration 190, loss = 0.0016918230103328824
iteration 191, loss = 0.0009206276154145598
iteration 192, loss = 0.0008417586795985699
iteration 193, loss = 0.0011415899498388171
iteration 194, loss = 0.0009038416901603341
iteration 195, loss = 0.0008640801534056664
iteration 196, loss = 0.0008813095628283918
iteration 197, loss = 0.0011830193689092994
iteration 198, loss = 0.0012713440228253603
iteration 199, loss = 0.0009008183842524886
iteration 200, loss = 0.0008286043885163963
iteration 201, loss = 0.0008550808997824788
iteration 202, loss = 0.0012726286659017205
iteration 203, loss = 0.0015983181074261665
iteration 204, loss = 0.0007368694641627371
iteration 205, loss = 0.0009841358987614512
iteration 206, loss = 0.0006299424567259848
iteration 207, loss = 0.0009652336011640728
iteration 208, loss = 0.0014361406210809946
iteration 209, loss = 0.00069663196336478
iteration 210, loss = 0.0013288763584569097
iteration 211, loss = 0.000765560835134238
iteration 212, loss = 0.000991034903563559
iteration 213, loss = 0.001095746411010623
iteration 214, loss = 0.0007266073953360319
iteration 215, loss = 0.0007831079419702291
iteration 216, loss = 0.0009590323315933347
iteration 217, loss = 0.000887918344233185
iteration 218, loss = 0.0008845949196256697
iteration 219, loss = 0.0010022458154708147
iteration 220, loss = 0.0015585909131914377
iteration 221, loss = 0.0011980572016909719
iteration 222, loss = 0.0008957429090514779
iteration 223, loss = 0.0009722850518301129
iteration 224, loss = 0.0007955498876981437
iteration 225, loss = 0.0009320800891146064
iteration 226, loss = 0.0009171943529509008
iteration 227, loss = 0.0010926320683211088
iteration 228, loss = 0.0008292411803267896
iteration 229, loss = 0.0008742277277633548
iteration 230, loss = 0.0007180685643106699
iteration 231, loss = 0.0015704190591350198
iteration 232, loss = 0.0009976023575291038
iteration 233, loss = 0.0007871389389038086
iteration 234, loss = 0.0009043289464898407
iteration 235, loss = 0.001242989324964583
iteration 236, loss = 0.0007744563044980168
iteration 237, loss = 0.0006812122883275151
iteration 238, loss = 0.0008930124458856881
iteration 239, loss = 0.0015993693377822638
iteration 240, loss = 0.0009458223939873278
iteration 241, loss = 0.0007255723467096686
iteration 242, loss = 0.0012223140802234411
iteration 243, loss = 0.000906261324416846
iteration 244, loss = 0.0007724639144726098
iteration 245, loss = 0.0016073209699243307
iteration 246, loss = 0.0008141266298480332
iteration 247, loss = 0.0008513368084095418
iteration 248, loss = 0.0011340362252667546
iteration 249, loss = 0.0008685903158038855
iteration 250, loss = 0.0007347030332311988
iteration 251, loss = 0.0017280473839491606
iteration 252, loss = 0.0013132233871147037
iteration 253, loss = 0.0008270969847217202
iteration 254, loss = 0.000765704084187746
iteration 255, loss = 0.0008411806775256991
iteration 256, loss = 0.0009198078187182546
iteration 257, loss = 0.0009849814232438803
iteration 258, loss = 0.0006726191495545208
iteration 259, loss = 0.0009102016338147223
iteration 260, loss = 0.0006724608247168362
iteration 261, loss = 0.001153738354332745
iteration 262, loss = 0.0014994449447840452
iteration 263, loss = 0.0007280490244738758
iteration 264, loss = 0.001275303540751338
iteration 265, loss = 0.0008807085105217993
iteration 266, loss = 0.0007049398263916373
iteration 267, loss = 0.0008595014805905521
iteration 268, loss = 0.0016198850935325027
iteration 269, loss = 0.0007536025950685143
iteration 270, loss = 0.0009130073012784123
iteration 271, loss = 0.0009254676406271756
iteration 272, loss = 0.0015392364002764225
iteration 273, loss = 0.0008136454853229225
iteration 274, loss = 0.0010561462258920074
iteration 275, loss = 0.0011601727455854416
iteration 276, loss = 0.0010656878584995866
iteration 277, loss = 0.0010710421483963728
iteration 278, loss = 0.0008889138698577881
iteration 279, loss = 0.0006989181856624782
iteration 280, loss = 0.0014640191802754998
iteration 281, loss = 0.0008301626658067107
iteration 282, loss = 0.0011027443688362837
iteration 283, loss = 0.0007334963884204626
iteration 284, loss = 0.0010815226705744863
iteration 285, loss = 0.0009109822567552328
iteration 286, loss = 0.0009255813201889396
iteration 287, loss = 0.0011996113462373614
iteration 288, loss = 0.0007857573800720274
iteration 289, loss = 0.0008042833651416004
iteration 290, loss = 0.0012922732857987285
iteration 291, loss = 0.0008204227196983993
iteration 292, loss = 0.0007219503168016672
iteration 293, loss = 0.0016071327263489366
iteration 294, loss = 0.0017270754324272275
iteration 295, loss = 0.0006583114736713469
iteration 296, loss = 0.001751153264194727
iteration 297, loss = 0.0007043310324661434
iteration 298, loss = 0.0008052687626332045
iteration 299, loss = 0.0010969537543132901
iteration 300, loss = 0.0007456421735696495
iteration 1, loss = 0.0013457303866744041
iteration 2, loss = 0.0010656204540282488
iteration 3, loss = 0.00087669090135023
iteration 4, loss = 0.0017636623233556747
iteration 5, loss = 0.0008010259480215609
iteration 6, loss = 0.0011165393516421318
iteration 7, loss = 0.0009233945747837424
iteration 8, loss = 0.0007828993839211762
iteration 9, loss = 0.0011761781061068177
iteration 10, loss = 0.0008272146224044263
iteration 11, loss = 0.0007499433122575283
iteration 12, loss = 0.0010005023796111345
iteration 13, loss = 0.0008530305349268019
iteration 14, loss = 0.0009597091120667756
iteration 15, loss = 0.0011502371635288
iteration 16, loss = 0.0008613754180260003
iteration 17, loss = 0.0010408159578219056
iteration 18, loss = 0.0011161966249346733
iteration 19, loss = 0.0007396161672659218
iteration 20, loss = 0.000927833141759038
iteration 21, loss = 0.0007652489002794027
iteration 22, loss = 0.0012144153006374836
iteration 23, loss = 0.0007444516522809863
iteration 24, loss = 0.0009731786558404565
iteration 25, loss = 0.0010060715721920133
iteration 26, loss = 0.0008414543699473143
iteration 27, loss = 0.0008685663342475891
iteration 28, loss = 0.0011226620990782976
iteration 29, loss = 0.0009312084293924272
iteration 30, loss = 0.0009317427175119519
iteration 31, loss = 0.000710529915522784
iteration 32, loss = 0.0008535872912034392
iteration 33, loss = 0.001124387257732451
iteration 34, loss = 0.0011721065966412425
iteration 35, loss = 0.0008336141472682357
iteration 36, loss = 0.0008478478994220495
iteration 37, loss = 0.0010551247978582978
iteration 38, loss = 0.0008516102680005133
iteration 39, loss = 0.0008054666686803102
iteration 40, loss = 0.0007995793712325394
iteration 41, loss = 0.0008539882837794721
iteration 42, loss = 0.0007024845690466464
iteration 43, loss = 0.0008242494077421725
iteration 44, loss = 0.000680596916936338
iteration 45, loss = 0.0007986921118572354
iteration 46, loss = 0.0012870003702118993
iteration 47, loss = 0.0009937569266185164
iteration 48, loss = 0.0007522186497226357
iteration 49, loss = 0.0008547583129256964
iteration 50, loss = 0.0007398689049296081
iteration 51, loss = 0.0007868189131841063
iteration 52, loss = 0.0007334236870519817
iteration 53, loss = 0.001132819801568985
iteration 54, loss = 0.0008280288893729448
iteration 55, loss = 0.0010463871294632554
iteration 56, loss = 0.0007566767744719982
iteration 57, loss = 0.0009137883898802102
iteration 58, loss = 0.0009118319721892476
iteration 59, loss = 0.001695265993475914
iteration 60, loss = 0.0008823154494166374
iteration 61, loss = 0.0016130144940689206
iteration 62, loss = 0.0009797627571970224
iteration 63, loss = 0.000894834753125906
iteration 64, loss = 0.0009030778892338276
iteration 65, loss = 0.0009480317239649594
iteration 66, loss = 0.0014692863915115595
iteration 67, loss = 0.0008982414728961885
iteration 68, loss = 0.0009711840539239347
iteration 69, loss = 0.001125030918046832
iteration 70, loss = 0.0008008499280549586
iteration 71, loss = 0.0007217841339297593
iteration 72, loss = 0.0009714237530715764
iteration 73, loss = 0.0007376738940365613
iteration 74, loss = 0.0007562806131318212
iteration 75, loss = 0.000776560278609395
iteration 76, loss = 0.0014320508344098926
iteration 77, loss = 0.0007896978640928864
iteration 78, loss = 0.0007514418684877455
iteration 79, loss = 0.0010541033698245883
iteration 80, loss = 0.0009153544087894261
iteration 81, loss = 0.0007201329572126269
iteration 82, loss = 0.001038320129737258
iteration 83, loss = 0.0011084313737228513
iteration 84, loss = 0.0008012673351913691
iteration 85, loss = 0.0009326175786554813
iteration 86, loss = 0.0009021273581311107
iteration 87, loss = 0.0010139545192942023
iteration 88, loss = 0.0008231690735556185
iteration 89, loss = 0.0006957089062780142
iteration 90, loss = 0.0011597201228141785
iteration 91, loss = 0.0007325009210035205
iteration 92, loss = 0.0008658624719828367
iteration 93, loss = 0.0012180771445855498
iteration 94, loss = 0.0007571401074528694
iteration 95, loss = 0.0011917449301108718
iteration 96, loss = 0.0008022491820156574
iteration 97, loss = 0.0008374049793928862
iteration 98, loss = 0.0008295003208331764
iteration 99, loss = 0.001652393490076065
iteration 100, loss = 0.00104912172537297
iteration 101, loss = 0.001497313380241394
iteration 102, loss = 0.000787348544690758
iteration 103, loss = 0.0009222255321219563
iteration 104, loss = 0.0021291892044246197
iteration 105, loss = 0.0008960144477896392
iteration 106, loss = 0.0007543648243881762
iteration 107, loss = 0.0008114281226880848
iteration 108, loss = 0.0008620371809229255
iteration 109, loss = 0.0008367463015019894
iteration 110, loss = 0.0008841068483889103
iteration 111, loss = 0.000880248611792922
iteration 112, loss = 0.000817279564216733
iteration 113, loss = 0.0008578945999033749
iteration 114, loss = 0.0007990527665242553
iteration 115, loss = 0.0008558510453440249
iteration 116, loss = 0.0010483745718374848
iteration 117, loss = 0.0007677116664126515
iteration 118, loss = 0.0006622883956879377
iteration 119, loss = 0.00078278046566993
iteration 120, loss = 0.0008899039821699262
iteration 121, loss = 0.0017612043302506208
iteration 122, loss = 0.000822912435978651
iteration 123, loss = 0.0015114450361579657
iteration 124, loss = 0.0012493429239839315
iteration 125, loss = 0.0010385288624092937
iteration 126, loss = 0.0008493693894706666
iteration 127, loss = 0.0010295321699231863
iteration 128, loss = 0.0008393253665417433
iteration 129, loss = 0.0012337504886090755
iteration 130, loss = 0.0011574270902201533
iteration 131, loss = 0.0012093230616301298
iteration 132, loss = 0.0008363682427443564
iteration 133, loss = 0.0012205648235976696
iteration 134, loss = 0.0016650615725666285
iteration 135, loss = 0.0007473854348063469
iteration 136, loss = 0.0008894969359971583
iteration 137, loss = 0.0011003753170371056
iteration 138, loss = 0.0009428002522327006
iteration 139, loss = 0.0008804581011645496
iteration 140, loss = 0.0006882293382659554
iteration 141, loss = 0.0008461142424494028
iteration 142, loss = 0.0009090755484066904
iteration 143, loss = 0.0014375848695635796
iteration 144, loss = 0.0008605046896263957
iteration 145, loss = 0.0008229868253692985
iteration 146, loss = 0.001051770057529211
iteration 147, loss = 0.0007539690122939646
iteration 148, loss = 0.0019607385620474815
iteration 149, loss = 0.0016763941384851933
iteration 150, loss = 0.0007302543381229043
iteration 151, loss = 0.0009438673150725663
iteration 152, loss = 0.0010658251121640205
iteration 153, loss = 0.0007622394477948546
iteration 154, loss = 0.0006469353102147579
iteration 155, loss = 0.0007971731829456985
iteration 156, loss = 0.000922264123801142
iteration 157, loss = 0.0008662132895551622
iteration 158, loss = 0.0008904464775696397
iteration 159, loss = 0.0007811775431036949
iteration 160, loss = 0.0007605492137372494
iteration 161, loss = 0.0009181826608255506
iteration 162, loss = 0.0007569122244603932
iteration 163, loss = 0.0007710467325523496
iteration 164, loss = 0.0012227598344907165
iteration 165, loss = 0.0007805904606357217
iteration 166, loss = 0.0016928249970078468
iteration 167, loss = 0.0008529232582077384
iteration 168, loss = 0.0008334081503562629
iteration 169, loss = 0.0016078101471066475
iteration 170, loss = 0.0007660791161470115
iteration 171, loss = 0.0007929791463539004
iteration 172, loss = 0.0008916517253965139
iteration 173, loss = 0.0014971939381211996
iteration 174, loss = 0.0007212316268123686
iteration 175, loss = 0.0006800029659643769
iteration 176, loss = 0.0008324271184392273
iteration 177, loss = 0.000822268077172339
iteration 178, loss = 0.0009558580350130796
iteration 179, loss = 0.0007003075443208218
iteration 180, loss = 0.0009751217439770699
iteration 181, loss = 0.0010710259666666389
iteration 182, loss = 0.0010091024450957775
iteration 183, loss = 0.0008138353587128222
iteration 184, loss = 0.0008751638233661652
iteration 185, loss = 0.0009522379259578884
iteration 186, loss = 0.0008070659823715687
iteration 187, loss = 0.0007021630299277604
iteration 188, loss = 0.0009174349252134562
iteration 189, loss = 0.0015166071243584156
iteration 190, loss = 0.0008797442424111068
iteration 191, loss = 0.0009233700111508369
iteration 192, loss = 0.0008639838779345155
iteration 193, loss = 0.0007223132997751236
iteration 194, loss = 0.0011329044355079532
iteration 195, loss = 0.001179040758870542
iteration 196, loss = 0.0012030791258439422
iteration 197, loss = 0.001175262383185327
iteration 198, loss = 0.0009054426336660981
iteration 199, loss = 0.000859151768963784
iteration 200, loss = 0.0007164821145124733
iteration 201, loss = 0.0007067654514685273
iteration 202, loss = 0.0006752714980393648
iteration 203, loss = 0.0009497132268734276
iteration 204, loss = 0.000903991109225899
iteration 205, loss = 0.0008010250166989863
iteration 206, loss = 0.000743263924960047
iteration 207, loss = 0.0007887579849921167
iteration 208, loss = 0.0009051134111359715
iteration 209, loss = 0.0008381545776501298
iteration 210, loss = 0.0012697976781055331
iteration 211, loss = 0.0008302588248625398
iteration 212, loss = 0.001211419003084302
iteration 213, loss = 0.001737197395414114
iteration 214, loss = 0.0014157783007249236
iteration 215, loss = 0.0010596723295748234
iteration 216, loss = 0.0007942786323837936
iteration 217, loss = 0.001028035650961101
iteration 218, loss = 0.0008182450546883047
iteration 219, loss = 0.0008402747334912419
iteration 220, loss = 0.000695635739248246
iteration 221, loss = 0.0016304332530125976
iteration 222, loss = 0.0007574310875497758
iteration 223, loss = 0.0007440246408805251
iteration 224, loss = 0.0008902066037990153
iteration 225, loss = 0.002516720909625292
iteration 226, loss = 0.0007975103799253702
iteration 227, loss = 0.0007752139936201274
iteration 228, loss = 0.0008806278929114342
iteration 229, loss = 0.0015569274546578526
iteration 230, loss = 0.0009565124637447298
iteration 231, loss = 0.001111950259655714
iteration 232, loss = 0.0016033713473007083
iteration 233, loss = 0.0009225148241966963
iteration 234, loss = 0.0008797974442131817
iteration 235, loss = 0.001003103912808001
iteration 236, loss = 0.0015584786888211966
iteration 237, loss = 0.0014055956853553653
iteration 238, loss = 0.0012137393932789564
iteration 239, loss = 0.0007879439508542418
iteration 240, loss = 0.0012235487811267376
iteration 241, loss = 0.00122856255620718
iteration 242, loss = 0.0008391452138312161
iteration 243, loss = 0.0010184233542531729
iteration 244, loss = 0.0011267134686931968
iteration 245, loss = 0.0009116095025092363
iteration 246, loss = 0.0007634759531356394
iteration 247, loss = 0.0008378275670111179
iteration 248, loss = 0.0011490251636132598
iteration 249, loss = 0.0007865207153372467
iteration 250, loss = 0.0007309302454814315
iteration 251, loss = 0.0016449186950922012
iteration 252, loss = 0.0008333420264534652
iteration 253, loss = 0.0009084134362637997
iteration 254, loss = 0.0009534470736980438
iteration 255, loss = 0.0007369706872850657
iteration 256, loss = 0.0016567353159189224
iteration 257, loss = 0.0007650494808331132
iteration 258, loss = 0.0014892605831846595
iteration 259, loss = 0.001269368571229279
iteration 260, loss = 0.0007218783721327782
iteration 261, loss = 0.000916972931008786
iteration 262, loss = 0.0007632010383531451
iteration 263, loss = 0.0008032814366742969
iteration 264, loss = 0.0009842382278293371
iteration 265, loss = 0.0007408047094941139
iteration 266, loss = 0.0007958037895150483
iteration 267, loss = 0.001088686753064394
iteration 268, loss = 0.0008162336307577789
iteration 269, loss = 0.0009396563982591033
iteration 270, loss = 0.0007763467147015035
iteration 271, loss = 0.0009115448920056224
iteration 272, loss = 0.0007523696403950453
iteration 273, loss = 0.0009632444707676768
iteration 274, loss = 0.0009518643491901457
iteration 275, loss = 0.0010507713304832578
iteration 276, loss = 0.0006825040327385068
iteration 277, loss = 0.0008412071038037539
iteration 278, loss = 0.0016483477083966136
iteration 279, loss = 0.0007092862506397069
iteration 280, loss = 0.0008096194360405207
iteration 281, loss = 0.0011985590681433678
iteration 282, loss = 0.0010224931174889207
iteration 283, loss = 0.0009790254989638925
iteration 284, loss = 0.000988383893854916
iteration 285, loss = 0.0008086456800810993
iteration 286, loss = 0.0008254614658653736
iteration 287, loss = 0.0010167701402679086
iteration 288, loss = 0.0009048957726918161
iteration 289, loss = 0.0010875135194510221
iteration 290, loss = 0.00072322110645473
iteration 291, loss = 0.0010356721468269825
iteration 292, loss = 0.0007578178192488849
iteration 293, loss = 0.0009009458590298891
iteration 294, loss = 0.0009125714423134923
iteration 295, loss = 0.0013970619766041636
iteration 296, loss = 0.0017786900280043483
iteration 297, loss = 0.0007327888160943985
iteration 298, loss = 0.0008900061948224902
iteration 299, loss = 0.0009066099300980568
iteration 300, loss = 0.0007920474745333195
iteration 1, loss = 0.0008365670801140368
iteration 2, loss = 0.000728832557797432
iteration 3, loss = 0.0008234301349148154
iteration 4, loss = 0.0007872851565480232
iteration 5, loss = 0.0007751446682959795
iteration 6, loss = 0.0014208829961717129
iteration 7, loss = 0.0009830605704337358
iteration 8, loss = 0.0007281743455678225
iteration 9, loss = 0.0010491976281628013
iteration 10, loss = 0.0008298309985548258
iteration 11, loss = 0.0008939579129219055
iteration 12, loss = 0.0008474153582938015
iteration 13, loss = 0.0008134834351949394
iteration 14, loss = 0.0008249114034697413
iteration 15, loss = 0.001037654117681086
iteration 16, loss = 0.0008424335392192006
iteration 17, loss = 0.00116690993309021
iteration 18, loss = 0.0007555484771728516
iteration 19, loss = 0.0008029011660255492
iteration 20, loss = 0.0006916032871231437
iteration 21, loss = 0.0007946848054416478
iteration 22, loss = 0.0009347469313070178
iteration 23, loss = 0.0007773006800562143
iteration 24, loss = 0.001246164320036769
iteration 25, loss = 0.0009476455743424594
iteration 26, loss = 0.0008411945891566575
iteration 27, loss = 0.0008246282814070582
iteration 28, loss = 0.0007786722853779793
iteration 29, loss = 0.0012154203141108155
iteration 30, loss = 0.0012392712524160743
iteration 31, loss = 0.0009892901871353388
iteration 32, loss = 0.0011134003289043903
iteration 33, loss = 0.0009674912435002625
iteration 34, loss = 0.0012207586551085114
iteration 35, loss = 0.0017383064841851592
iteration 36, loss = 0.000927108689211309
iteration 37, loss = 0.0008322220528498292
iteration 38, loss = 0.0008167927153408527
iteration 39, loss = 0.0006939146551303566
iteration 40, loss = 0.0007100435323081911
iteration 41, loss = 0.000793247134424746
iteration 42, loss = 0.0006850887439213693
iteration 43, loss = 0.0007746588089503348
iteration 44, loss = 0.0008892295300029218
iteration 45, loss = 0.0009277973440475762
iteration 46, loss = 0.0015520212473347783
iteration 47, loss = 0.0008891443139873445
iteration 48, loss = 0.0010389849776402116
iteration 49, loss = 0.0009137341403402388
iteration 50, loss = 0.0009031282970681787
iteration 51, loss = 0.0008872929611243308
iteration 52, loss = 0.0017922238912433386
iteration 53, loss = 0.0009133666171692312
iteration 54, loss = 0.0016253875801339746
iteration 55, loss = 0.000864874804392457
iteration 56, loss = 0.0008373993914574385
iteration 57, loss = 0.0010576264467090368
iteration 58, loss = 0.0010865783551707864
iteration 59, loss = 0.000924374966416508
iteration 60, loss = 0.0008444396080449224
iteration 61, loss = 0.0009552108822390437
iteration 62, loss = 0.000964647508226335
iteration 63, loss = 0.0009354200446978211
iteration 64, loss = 0.0018845212180167437
iteration 65, loss = 0.0009394753142260015
iteration 66, loss = 0.0007478983025066555
iteration 67, loss = 0.0009634753805585206
iteration 68, loss = 0.0008834159816615283
iteration 69, loss = 0.00125419395044446
iteration 70, loss = 0.0010528572602197528
iteration 71, loss = 0.0009890212677419186
iteration 72, loss = 0.0008267901139333844
iteration 73, loss = 0.0006361209671013057
iteration 74, loss = 0.0007916215108707547
iteration 75, loss = 0.0008620899170637131
iteration 76, loss = 0.0008727865060791373
iteration 77, loss = 0.0008144076564349234
iteration 78, loss = 0.0011043903650715947
iteration 79, loss = 0.0007607698207721114
iteration 80, loss = 0.0014617047272622585
iteration 81, loss = 0.0007729677599854767
iteration 82, loss = 0.0007629983592778444
iteration 83, loss = 0.0010742434533312917
iteration 84, loss = 0.0009432195220142603
iteration 85, loss = 0.0007265189196914434
iteration 86, loss = 0.0008304945658892393
iteration 87, loss = 0.0016960001084953547
iteration 88, loss = 0.0008612696547061205
iteration 89, loss = 0.0009107701480388641
iteration 90, loss = 0.0007900422788225114
iteration 91, loss = 0.0010135589400306344
iteration 92, loss = 0.0009091848041862249
iteration 93, loss = 0.0017683474579825997
iteration 94, loss = 0.0007787778740748763
iteration 95, loss = 0.0008792354492470622
iteration 96, loss = 0.0007250157650560141
iteration 97, loss = 0.0009180355118587613
iteration 98, loss = 0.0008064868743531406
iteration 99, loss = 0.0009117515874095261
iteration 100, loss = 0.0011240883031859994
iteration 101, loss = 0.0007364298799075186
iteration 102, loss = 0.0008900818647816777
iteration 103, loss = 0.001188670052215457
iteration 104, loss = 0.0015795076033100486
iteration 105, loss = 0.0011886907741427422
iteration 106, loss = 0.00088269985280931
iteration 107, loss = 0.0008150784997269511
iteration 108, loss = 0.000629539368674159
iteration 109, loss = 0.0008535463130101562
iteration 110, loss = 0.0017276839353144169
iteration 111, loss = 0.0009464347967877984
iteration 112, loss = 0.0009317367803305387
iteration 113, loss = 0.0009109207894653082
iteration 114, loss = 0.0007318772841244936
iteration 115, loss = 0.0009740130044519901
iteration 116, loss = 0.0010665652807801962
iteration 117, loss = 0.000807535310741514
iteration 118, loss = 0.0010670714545994997
iteration 119, loss = 0.0009168091346509755
iteration 120, loss = 0.0008963030413724482
iteration 121, loss = 0.0012929524527862668
iteration 122, loss = 0.0008860743837431073
iteration 123, loss = 0.0010958138154819608
iteration 124, loss = 0.001250696019269526
iteration 125, loss = 0.0007427475648000836
iteration 126, loss = 0.0006848115590400994
iteration 127, loss = 0.0011592574883252382
iteration 128, loss = 0.0006680231308564544
iteration 129, loss = 0.0007385151693597436
iteration 130, loss = 0.0009772127959877253
iteration 131, loss = 0.0009663508390076458
iteration 132, loss = 0.0008113037329167128
iteration 133, loss = 0.0008951859199441969
iteration 134, loss = 0.0008386808913201094
iteration 135, loss = 0.0008208079962059855
iteration 136, loss = 0.0009528343798592687
iteration 137, loss = 0.0016458425670862198
iteration 138, loss = 0.0007574912160634995
iteration 139, loss = 0.0007254248484969139
iteration 140, loss = 0.000911535753402859
iteration 141, loss = 0.0012761560501530766
iteration 142, loss = 0.0006954591954126954
iteration 143, loss = 0.001031396328471601
iteration 144, loss = 0.0009309991146437824
iteration 145, loss = 0.000819058099295944
iteration 146, loss = 0.0009783593704923987
iteration 147, loss = 0.0008350552525371313
iteration 148, loss = 0.0009702800307422876
iteration 149, loss = 0.0008251394610852003
iteration 150, loss = 0.0008655802812427282
iteration 151, loss = 0.0011281847255304456
iteration 152, loss = 0.0020015700720250607
iteration 153, loss = 0.0009688246063888073
iteration 154, loss = 0.0009150057449005544
iteration 155, loss = 0.00126528216060251
iteration 156, loss = 0.0007185906288214028
iteration 157, loss = 0.000948878179769963
iteration 158, loss = 0.0012045385083183646
iteration 159, loss = 0.000861422682646662
iteration 160, loss = 0.000946852786000818
iteration 161, loss = 0.0007093186723068357
iteration 162, loss = 0.0007348405197262764
iteration 163, loss = 0.0011542506981641054
iteration 164, loss = 0.0007593919872306287
iteration 165, loss = 0.0012543039629235864
iteration 166, loss = 0.0007358008879236877
iteration 167, loss = 0.0008599706343375146
iteration 168, loss = 0.0011691406834870577
iteration 169, loss = 0.0008400677470490336
iteration 170, loss = 0.0007499061757698655
iteration 171, loss = 0.0010518035851418972
iteration 172, loss = 0.0016933708684518933
iteration 173, loss = 0.001626451383344829
iteration 174, loss = 0.0018710517324507236
iteration 175, loss = 0.0007662797579541802
iteration 176, loss = 0.0008152368827722967
iteration 177, loss = 0.0016563370591029525
iteration 178, loss = 0.0009230526047758758
iteration 179, loss = 0.000886895228177309
iteration 180, loss = 0.0010470524430274963
iteration 181, loss = 0.0009168446995317936
iteration 182, loss = 0.0010368754155933857
iteration 183, loss = 0.0011808638228103518
iteration 184, loss = 0.0009786072187125683
iteration 185, loss = 0.0014583577867597342
iteration 186, loss = 0.0008835805929265916
iteration 187, loss = 0.0008326667593792081
iteration 188, loss = 0.0009517537546344101
iteration 189, loss = 0.0008500114781782031
iteration 190, loss = 0.0014319915790110826
iteration 191, loss = 0.0017011743038892746
iteration 192, loss = 0.0012863101437687874
iteration 193, loss = 0.0012076279381290078
iteration 194, loss = 0.0007258661789819598
iteration 195, loss = 0.0007824034546501935
iteration 196, loss = 0.0014563918812200427
iteration 197, loss = 0.0007811724208295345
iteration 198, loss = 0.0008231492829509079
iteration 199, loss = 0.0007708467892371118
iteration 200, loss = 0.0008419225923717022
iteration 201, loss = 0.000930599810089916
iteration 202, loss = 0.000917127646971494
iteration 203, loss = 0.0009018047130666673
iteration 204, loss = 0.0009719335939735174
iteration 205, loss = 0.0017320370534434915
iteration 206, loss = 0.0008755805902183056
iteration 207, loss = 0.0009663992677815259
iteration 208, loss = 0.0015697384951636195
iteration 209, loss = 0.0009729588055051863
iteration 210, loss = 0.0009205569513142109
iteration 211, loss = 0.0008546471362933517
iteration 212, loss = 0.0009399827104061842
iteration 213, loss = 0.0010489902924746275
iteration 214, loss = 0.0007526468252763152
iteration 215, loss = 0.0007959550712257624
iteration 216, loss = 0.0007950751460157335
iteration 217, loss = 0.0008856211206875741
iteration 218, loss = 0.0009354777866974473
iteration 219, loss = 0.0008043489651754498
iteration 220, loss = 0.0012581786140799522
iteration 221, loss = 0.0007834182470105588
iteration 222, loss = 0.0008388307178393006
iteration 223, loss = 0.0009329492459073663
iteration 224, loss = 0.0008106513414531946
iteration 225, loss = 0.0014264468336477876
iteration 226, loss = 0.0007467588875442743
iteration 227, loss = 0.0008944316650740802
iteration 228, loss = 0.0014542504213750362
iteration 229, loss = 0.0008101937128230929
iteration 230, loss = 0.000741095223929733
iteration 231, loss = 0.0007472068537026644
iteration 232, loss = 0.0007538018980994821
iteration 233, loss = 0.0007815975113771856
iteration 234, loss = 0.0013614034978672862
iteration 235, loss = 0.0007588031003251672
iteration 236, loss = 0.0007674357038922608
iteration 237, loss = 0.0007058870978653431
iteration 238, loss = 0.0008068971219472587
iteration 239, loss = 0.0011650174856185913
iteration 240, loss = 0.0011890616733580828
iteration 241, loss = 0.0006947312504053116
iteration 242, loss = 0.0009202486835420132
iteration 243, loss = 0.0008354911697097123
iteration 244, loss = 0.0012988930102437735
iteration 245, loss = 0.0010915470775216818
iteration 246, loss = 0.0007957323105074465
iteration 247, loss = 0.0008998441626317799
iteration 248, loss = 0.0006748945452272892
iteration 249, loss = 0.0010340102016925812
iteration 250, loss = 0.0006811886560171843
iteration 251, loss = 0.0007371458341367543
iteration 252, loss = 0.0009732311591506004
iteration 253, loss = 0.0009486106573604047
iteration 254, loss = 0.0007644727011211216
iteration 255, loss = 0.001457649632357061
iteration 256, loss = 0.0007961426163092256
iteration 257, loss = 0.0012814123183488846
iteration 258, loss = 0.0010484692174941301
iteration 259, loss = 0.0010629452299326658
iteration 260, loss = 0.0009264668915420771
iteration 261, loss = 0.0008231602841988206
iteration 262, loss = 0.0011198482243344188
iteration 263, loss = 0.0009586553205735981
iteration 264, loss = 0.0007373739499598742
iteration 265, loss = 0.0007484104135073721
iteration 266, loss = 0.0007313123205676675
iteration 267, loss = 0.0009137500892393291
iteration 268, loss = 0.0007319396827369928
iteration 269, loss = 0.001547797117382288
iteration 270, loss = 0.0006489304360002279
iteration 271, loss = 0.0010622552363201976
iteration 272, loss = 0.0008298977045342326
iteration 273, loss = 0.001350096776150167
iteration 274, loss = 0.0007812338881194592
iteration 275, loss = 0.0007389337988570333
iteration 276, loss = 0.0006797186797484756
iteration 277, loss = 0.0012354438658803701
iteration 278, loss = 0.001375112566165626
iteration 279, loss = 0.0009488543728366494
iteration 280, loss = 0.0007452084100805223
iteration 281, loss = 0.0007958405185490847
iteration 282, loss = 0.002335588214918971
iteration 283, loss = 0.0008374968310818076
iteration 284, loss = 0.0008800330688245595
iteration 285, loss = 0.0008128889603540301
iteration 286, loss = 0.0008580606081523001
iteration 287, loss = 0.0014131665229797363
iteration 288, loss = 0.0009171628626063466
iteration 289, loss = 0.0009654803434386849
iteration 290, loss = 0.0009009368368424475
iteration 291, loss = 0.0007294705719687045
iteration 292, loss = 0.0007933666347526014
iteration 293, loss = 0.001005070866085589
iteration 294, loss = 0.0010173618793487549
iteration 295, loss = 0.0007891485001891851
iteration 296, loss = 0.0007744976319372654
iteration 297, loss = 0.0010952914599329233
iteration 298, loss = 0.0007752801175229251
iteration 299, loss = 0.0015757795190438628
iteration 300, loss = 0.0010936340549960732
iteration 1, loss = 0.0009449151111766696
iteration 2, loss = 0.0009259578073397279
iteration 3, loss = 0.0009022854501381516
iteration 4, loss = 0.000941069214604795
iteration 5, loss = 0.0008912385092116892
iteration 6, loss = 0.0016004617791622877
iteration 7, loss = 0.0009016938274726272
iteration 8, loss = 0.0008289495017379522
iteration 9, loss = 0.0009081442840397358
iteration 10, loss = 0.0008544179145246744
iteration 11, loss = 0.0009624212398193777
iteration 12, loss = 0.0007146191201172769
iteration 13, loss = 0.0006983315106481314
iteration 14, loss = 0.0008893462363630533
iteration 15, loss = 0.0007552155293524265
iteration 16, loss = 0.0016531040892004967
iteration 17, loss = 0.0008779307827353477
iteration 18, loss = 0.0009240296203643084
iteration 19, loss = 0.0008349944837391376
iteration 20, loss = 0.0007024147198535502
iteration 21, loss = 0.000990897766314447
iteration 22, loss = 0.0012405863963067532
iteration 23, loss = 0.0009633554727770388
iteration 24, loss = 0.0007487142574973404
iteration 25, loss = 0.0008079183753579855
iteration 26, loss = 0.0007723777089267969
iteration 27, loss = 0.002014429774135351
iteration 28, loss = 0.0007524119573645294
iteration 29, loss = 0.0012028523487970233
iteration 30, loss = 0.0009496478014625609
iteration 31, loss = 0.0011974368244409561
iteration 32, loss = 0.0009803888387978077
iteration 33, loss = 0.0007972231833264232
iteration 34, loss = 0.0008839009096845984
iteration 35, loss = 0.0009018002892844379
iteration 36, loss = 0.0007610567263327539
iteration 37, loss = 0.0008195721311494708
iteration 38, loss = 0.001182044856250286
iteration 39, loss = 0.000620678358245641
iteration 40, loss = 0.0009934466797858477
iteration 41, loss = 0.0009149180259555578
iteration 42, loss = 0.0009056816343218088
iteration 43, loss = 0.0008935786318033934
iteration 44, loss = 0.0007330859662033617
iteration 45, loss = 0.0009221070213243365
iteration 46, loss = 0.0015543727204203606
iteration 47, loss = 0.0008735941955819726
iteration 48, loss = 0.000795618281699717
iteration 49, loss = 0.0010452831629663706
iteration 50, loss = 0.0010716450633481145
iteration 51, loss = 0.0007621545228175819
iteration 52, loss = 0.0008693517884239554
iteration 53, loss = 0.0006401996361091733
iteration 54, loss = 0.0018465705215930939
iteration 55, loss = 0.0008967089233919978
iteration 56, loss = 0.0008563513983972371
iteration 57, loss = 0.000874520861543715
iteration 58, loss = 0.0010945489630103111
iteration 59, loss = 0.001722484827041626
iteration 60, loss = 0.0012297406792640686
iteration 61, loss = 0.0008572979713790119
iteration 62, loss = 0.000862311280798167
iteration 63, loss = 0.0007421157788485289
iteration 64, loss = 0.000813013524748385
iteration 65, loss = 0.0009795504156500101
iteration 66, loss = 0.0011091850465163589
iteration 67, loss = 0.0006369298207573593
iteration 68, loss = 0.0007966180564835668
iteration 69, loss = 0.000747014069929719
iteration 70, loss = 0.0011016256175935268
iteration 71, loss = 0.0016623069532215595
iteration 72, loss = 0.0007742344168946147
iteration 73, loss = 0.0008054508361965418
iteration 74, loss = 0.001247471198439598
iteration 75, loss = 0.0009864711901172996
iteration 76, loss = 0.0014551014173775911
iteration 77, loss = 0.0011428443249315023
iteration 78, loss = 0.0008159240242093801
iteration 79, loss = 0.00118600286077708
iteration 80, loss = 0.0008694741409271955
iteration 81, loss = 0.000999503885395825
iteration 82, loss = 0.0010125691769644618
iteration 83, loss = 0.000639178731944412
iteration 84, loss = 0.0014911722391843796
iteration 85, loss = 0.0007825572392903268
iteration 86, loss = 0.0008923375280573964
iteration 87, loss = 0.001270269276574254
iteration 88, loss = 0.0007859606994315982
iteration 89, loss = 0.0007537872297689319
iteration 90, loss = 0.0015228362753987312
iteration 91, loss = 0.0009705671691335738
iteration 92, loss = 0.000727216131053865
iteration 93, loss = 0.0008957566460594535
iteration 94, loss = 0.000742137199267745
iteration 95, loss = 0.0007326712366193533
iteration 96, loss = 0.0007915826863609254
iteration 97, loss = 0.0010771050583571196
iteration 98, loss = 0.0008383722160942852
iteration 99, loss = 0.0008384912507608533
iteration 100, loss = 0.000739780836738646
iteration 101, loss = 0.0009479760192334652
iteration 102, loss = 0.0008160743163898587
iteration 103, loss = 0.0016009260434657335
iteration 104, loss = 0.0009615994640626013
iteration 105, loss = 0.0009150092955678701
iteration 106, loss = 0.0008099229307845235
iteration 107, loss = 0.0009350243490189314
iteration 108, loss = 0.0019798921421170235
iteration 109, loss = 0.0016187634319067001
iteration 110, loss = 0.0012586854863911867
iteration 111, loss = 0.0007816475117579103
iteration 112, loss = 0.0007050830754451454
iteration 113, loss = 0.0006949862581677735
iteration 114, loss = 0.0017193119274452329
iteration 115, loss = 0.0009384859004057944
iteration 116, loss = 0.0008055168436840177
iteration 117, loss = 0.0007792630349285901
iteration 118, loss = 0.0008320266497321427
iteration 119, loss = 0.0008712615235708654
iteration 120, loss = 0.0008240948664024472
iteration 121, loss = 0.001044033677317202
iteration 122, loss = 0.0009166162926703691
iteration 123, loss = 0.0007015019073151052
iteration 124, loss = 0.001199158257804811
iteration 125, loss = 0.0008414658950641751
iteration 126, loss = 0.0016677919775247574
iteration 127, loss = 0.0007058189949020743
iteration 128, loss = 0.0006406285101547837
iteration 129, loss = 0.0007558766519650817
iteration 130, loss = 0.0008324125083163381
iteration 131, loss = 0.0007981099188327789
iteration 132, loss = 0.002155033638700843
iteration 133, loss = 0.0008284732466563582
iteration 134, loss = 0.001251410343684256
iteration 135, loss = 0.0008558759582228959
iteration 136, loss = 0.0007775890408083797
iteration 137, loss = 0.0025431334506720304
iteration 138, loss = 0.000649629975669086
iteration 139, loss = 0.0007945458637550473
iteration 140, loss = 0.0008848377619870007
iteration 141, loss = 0.0007351839449256659
iteration 142, loss = 0.0011139289708808064
iteration 143, loss = 0.0011979540577158332
iteration 144, loss = 0.0007799286977387965
iteration 145, loss = 0.001234461204148829
iteration 146, loss = 0.0010498488554731011
iteration 147, loss = 0.0008213073015213013
iteration 148, loss = 0.0010819768067449331
iteration 149, loss = 0.0013117428170517087
iteration 150, loss = 0.001124767935834825
iteration 151, loss = 0.001062764902599156
iteration 152, loss = 0.0014426246052607894
iteration 153, loss = 0.0007527881534770131
iteration 154, loss = 0.0008554800297133625
iteration 155, loss = 0.0009353569475933909
iteration 156, loss = 0.001023422577418387
iteration 157, loss = 0.000732217391487211
iteration 158, loss = 0.0010872058337554336
iteration 159, loss = 0.0007864003418944776
iteration 160, loss = 0.0007952542509883642
iteration 161, loss = 0.000717233750037849
iteration 162, loss = 0.0007828421075828373
iteration 163, loss = 0.0008426960557699203
iteration 164, loss = 0.0007626688457094133
iteration 165, loss = 0.0009052529348991811
iteration 166, loss = 0.0009409849299117923
iteration 167, loss = 0.0011338837211951613
iteration 168, loss = 0.001120620290748775
iteration 169, loss = 0.0009036375558935106
iteration 170, loss = 0.0011647673090919852
iteration 171, loss = 0.0007693092338740826
iteration 172, loss = 0.0007343553006649017
iteration 173, loss = 0.0008381096413359046
iteration 174, loss = 0.0009203161462210119
iteration 175, loss = 0.001012842869386077
iteration 176, loss = 0.0006493504624813795
iteration 177, loss = 0.0007690614438615739
iteration 178, loss = 0.0014829185092821717
iteration 179, loss = 0.0008951943600550294
iteration 180, loss = 0.000772650062572211
iteration 181, loss = 0.00075295171700418
iteration 182, loss = 0.0008467348525300622
iteration 183, loss = 0.0011287389788776636
iteration 184, loss = 0.0007292727241292596
iteration 185, loss = 0.0007771572563797235
iteration 186, loss = 0.0007071006111800671
iteration 187, loss = 0.0015677608316764235
iteration 188, loss = 0.0007367546204477549
iteration 189, loss = 0.000649890920612961
iteration 190, loss = 0.000748064077924937
iteration 191, loss = 0.001219928148202598
iteration 192, loss = 0.0008765696547925472
iteration 193, loss = 0.0007859750767238438
iteration 194, loss = 0.0006881888257339597
iteration 195, loss = 0.0008737144526094198
iteration 196, loss = 0.0008950487244874239
iteration 197, loss = 0.0009066478814929724
iteration 198, loss = 0.000984452199190855
iteration 199, loss = 0.0008838392095640302
iteration 200, loss = 0.0008409637957811356
iteration 201, loss = 0.0012623113580048084
iteration 202, loss = 0.0010270703351125121
iteration 203, loss = 0.0010023459326475859
iteration 204, loss = 0.0008807091508060694
iteration 205, loss = 0.0007930286228656769
iteration 206, loss = 0.0010431676637381315
iteration 207, loss = 0.0007823794730938971
iteration 208, loss = 0.0008557844557799399
iteration 209, loss = 0.000927906425204128
iteration 210, loss = 0.0007803560001775622
iteration 211, loss = 0.0007869438268244267
iteration 212, loss = 0.0009021576843224466
iteration 213, loss = 0.0008315119193866849
iteration 214, loss = 0.0009240212384611368
iteration 215, loss = 0.0007796405698172748
iteration 216, loss = 0.0009833936346694827
iteration 217, loss = 0.0008445041021332145
iteration 218, loss = 0.0009356681839562953
iteration 219, loss = 0.0009555072174407542
iteration 220, loss = 0.0011672871187329292
iteration 221, loss = 0.0006948666414245963
iteration 222, loss = 0.0009096181020140648
iteration 223, loss = 0.0010115415789186954
iteration 224, loss = 0.0009039914002642035
iteration 225, loss = 0.0007142671383917332
iteration 226, loss = 0.0011024270206689835
iteration 227, loss = 0.0016360715962946415
iteration 228, loss = 0.001186737441457808
iteration 229, loss = 0.0008458666852675378
iteration 230, loss = 0.0009311176836490631
iteration 231, loss = 0.0007912884466350079
iteration 232, loss = 0.0009297982323914766
iteration 233, loss = 0.0015495856059715152
iteration 234, loss = 0.0008386566187255085
iteration 235, loss = 0.0014227398205548525
iteration 236, loss = 0.0007921434589661658
iteration 237, loss = 0.0007599066011607647
iteration 238, loss = 0.0008279630565084517
iteration 239, loss = 0.0007753249956294894
iteration 240, loss = 0.0008175722323358059
iteration 241, loss = 0.001094374223612249
iteration 242, loss = 0.0008928272873163223
iteration 243, loss = 0.0008582029840908945
iteration 244, loss = 0.0008100666454993188
iteration 245, loss = 0.0008084614528343081
iteration 246, loss = 0.0008472846238873899
iteration 247, loss = 0.0015629882691428065
iteration 248, loss = 0.0011080894619226456
iteration 249, loss = 0.0007883926737122238
iteration 250, loss = 0.001180624938569963
iteration 251, loss = 0.0007372078252956271
iteration 252, loss = 0.0015325144631788135
iteration 253, loss = 0.0010266891913488507
iteration 254, loss = 0.0014507878804579377
iteration 255, loss = 0.000910529459360987
iteration 256, loss = 0.0008871142636053264
iteration 257, loss = 0.0008501860429532826
iteration 258, loss = 0.001379546825774014
iteration 259, loss = 0.001637797337025404
iteration 260, loss = 0.0013914959272369742
iteration 261, loss = 0.0008129054331220686
iteration 262, loss = 0.000737727852538228
iteration 263, loss = 0.0008200551383197308
iteration 264, loss = 0.0009651658474467695
iteration 265, loss = 0.0007738701533526182
iteration 266, loss = 0.0010683052241802216
iteration 267, loss = 0.0015346979489549994
iteration 268, loss = 0.0009679155773483217
iteration 269, loss = 0.0010294930543750525
iteration 270, loss = 0.0013045931700617075
iteration 271, loss = 0.0007824043859727681
iteration 272, loss = 0.001568581908941269
iteration 273, loss = 0.0009868626948446035
iteration 274, loss = 0.0009315786883234978
iteration 275, loss = 0.001066843280568719
iteration 276, loss = 0.0009883400052785873
iteration 277, loss = 0.0008122051949612796
iteration 278, loss = 0.0007798232836648822
iteration 279, loss = 0.0007257118122652173
iteration 280, loss = 0.0012890290236100554
iteration 281, loss = 0.0008785518584772944
iteration 282, loss = 0.0006555730360560119
iteration 283, loss = 0.001614773995243013
iteration 284, loss = 0.0011114083463326097
iteration 285, loss = 0.0007578119402751327
iteration 286, loss = 0.001002236152999103
iteration 287, loss = 0.0009826873429119587
iteration 288, loss = 0.0007899507763795555
iteration 289, loss = 0.0012328504817560315
iteration 290, loss = 0.0006652285228483379
iteration 291, loss = 0.001207601628266275
iteration 292, loss = 0.0008168841595761478
iteration 293, loss = 0.0007899800548329949
iteration 294, loss = 0.001030646380968392
iteration 295, loss = 0.0009056457784026861
iteration 296, loss = 0.0011733698192983866
iteration 297, loss = 0.0010969684226438403
iteration 298, loss = 0.0011212698882445693
iteration 299, loss = 0.0007168067968450487
iteration 300, loss = 0.0011334414593875408
iteration 1, loss = 0.0008165623294189572
iteration 2, loss = 0.0007159598171710968
iteration 3, loss = 0.0008605530601926148
iteration 4, loss = 0.0017225199844688177
iteration 5, loss = 0.0008150888606905937
iteration 6, loss = 0.0008942171116359532
iteration 7, loss = 0.0007594295311719179
iteration 8, loss = 0.000724605459254235
iteration 9, loss = 0.0009455078397877514
iteration 10, loss = 0.0008919156971387565
iteration 11, loss = 0.0007747328490950167
iteration 12, loss = 0.0011867590947076678
iteration 13, loss = 0.0010311222868040204
iteration 14, loss = 0.0016161114908754826
iteration 15, loss = 0.0008001446258276701
iteration 16, loss = 0.001260841265320778
iteration 17, loss = 0.0008967705070972443
iteration 18, loss = 0.0007145853596739471
iteration 19, loss = 0.0008083934662863612
iteration 20, loss = 0.001597789814695716
iteration 21, loss = 0.000954129034653306
iteration 22, loss = 0.0007440299959853292
iteration 23, loss = 0.0007012784481048584
iteration 24, loss = 0.0010717445984482765
iteration 25, loss = 0.0006755020003765821
iteration 26, loss = 0.0014318955363705754
iteration 27, loss = 0.0019676771480590105
iteration 28, loss = 0.0007957842317409813
iteration 29, loss = 0.0008011107565835118
iteration 30, loss = 0.000838030013255775
iteration 31, loss = 0.0007657911628484726
iteration 32, loss = 0.0010531258303672075
iteration 33, loss = 0.001163507578894496
iteration 34, loss = 0.0020801243372261524
iteration 35, loss = 0.0007973277824930847
iteration 36, loss = 0.0007137295324355364
iteration 37, loss = 0.0009877034462988377
iteration 38, loss = 0.0015204030787572265
iteration 39, loss = 0.0008134514791890979
iteration 40, loss = 0.0008522227872163057
iteration 41, loss = 0.0016352024395018816
iteration 42, loss = 0.0007933708839118481
iteration 43, loss = 0.0008073360077105463
iteration 44, loss = 0.0008456108043901622
iteration 45, loss = 0.0009364335564896464
iteration 46, loss = 0.0012644100934267044
iteration 47, loss = 0.0009368315804749727
iteration 48, loss = 0.0017996495589613914
iteration 49, loss = 0.000850467593409121
iteration 50, loss = 0.0015175893204286695
iteration 51, loss = 0.000887143483851105
iteration 52, loss = 0.0010023353388532996
iteration 53, loss = 0.0007806297508068383
iteration 54, loss = 0.001134023885242641
iteration 55, loss = 0.0007247370667755604
iteration 56, loss = 0.0015864219749346375
iteration 57, loss = 0.0012569756945595145
iteration 58, loss = 0.001109413686208427
iteration 59, loss = 0.0008311771089211106
iteration 60, loss = 0.0007093447493389249
iteration 61, loss = 0.0012274808250367641
iteration 62, loss = 0.0015169323887676
iteration 63, loss = 0.0016443561762571335
iteration 64, loss = 0.0007904302328824997
iteration 65, loss = 0.0009541053441353142
iteration 66, loss = 0.0012017928529530764
iteration 67, loss = 0.0013157211942598224
iteration 68, loss = 0.0007188208983279765
iteration 69, loss = 0.0008348568808287382
iteration 70, loss = 0.000912714283913374
iteration 71, loss = 0.0007447196403518319
iteration 72, loss = 0.0009283222025260329
iteration 73, loss = 0.0010684351436793804
iteration 74, loss = 0.0007994291372597218
iteration 75, loss = 0.0015137293376028538
iteration 76, loss = 0.000765100063290447
iteration 77, loss = 0.0009118093294091523
iteration 78, loss = 0.001155773876234889
iteration 79, loss = 0.0008010720484890044
iteration 80, loss = 0.000817480031400919
iteration 81, loss = 0.0009571761474944651
iteration 82, loss = 0.0006912407698109746
iteration 83, loss = 0.0008089003968052566
iteration 84, loss = 0.0010603851405903697
iteration 85, loss = 0.0007707715267315507
iteration 86, loss = 0.0007524603861384094
iteration 87, loss = 0.0014664479531347752
iteration 88, loss = 0.0010014694416895509
iteration 89, loss = 0.0009444140014238656
iteration 90, loss = 0.0007424037321470678
iteration 91, loss = 0.0009196059545502067
iteration 92, loss = 0.00079228391405195
iteration 93, loss = 0.0007649785839021206
iteration 94, loss = 0.00169557670596987
iteration 95, loss = 0.0015728784492239356
iteration 96, loss = 0.0008703386411070824
iteration 97, loss = 0.0009410704951733351
iteration 98, loss = 0.0007726624025963247
iteration 99, loss = 0.0009067688370123506
iteration 100, loss = 0.0009154832805506885
iteration 101, loss = 0.001020098919980228
iteration 102, loss = 0.0010119713842868805
iteration 103, loss = 0.0008235680870711803
iteration 104, loss = 0.0008309531258419156
iteration 105, loss = 0.0007824981585144997
iteration 106, loss = 0.000829563825391233
iteration 107, loss = 0.0009083070326596498
iteration 108, loss = 0.0008943330612964928
iteration 109, loss = 0.0016793470131233335
iteration 110, loss = 0.0007825206848792732
iteration 111, loss = 0.000819153618067503
iteration 112, loss = 0.0010523492237553
iteration 113, loss = 0.0009313629707321525
iteration 114, loss = 0.0008124273736029863
iteration 115, loss = 0.0010264269076287746
iteration 116, loss = 0.0009357501403428614
iteration 117, loss = 0.0008651155512779951
iteration 118, loss = 0.000895713223144412
iteration 119, loss = 0.0007150847231969237
iteration 120, loss = 0.0008202415192499757
iteration 121, loss = 0.0008130779024213552
iteration 122, loss = 0.0009239285718649626
iteration 123, loss = 0.0008926865411922336
iteration 124, loss = 0.00195317598991096
iteration 125, loss = 0.0009730966412462294
iteration 126, loss = 0.0008208775543607771
iteration 127, loss = 0.0008503370336256921
iteration 128, loss = 0.000820140412542969
iteration 129, loss = 0.001093872357159853
iteration 130, loss = 0.0008156485855579376
iteration 131, loss = 0.0008120642160065472
iteration 132, loss = 0.0008929049945436418
iteration 133, loss = 0.0013440200127661228
iteration 134, loss = 0.0010030250996351242
iteration 135, loss = 0.0018654863815754652
iteration 136, loss = 0.0007225500885397196
iteration 137, loss = 0.0008249937673099339
iteration 138, loss = 0.0009110913379117846
iteration 139, loss = 0.000897442689165473
iteration 140, loss = 0.0007990390877239406
iteration 141, loss = 0.0012816990492865443
iteration 142, loss = 0.0009531736141070724
iteration 143, loss = 0.0009187468094751239
iteration 144, loss = 0.0009146465454250574
iteration 145, loss = 0.0006718110525980592
iteration 146, loss = 0.0009813181823119521
iteration 147, loss = 0.0008350876159965992
iteration 148, loss = 0.0009374322835355997
iteration 149, loss = 0.0014921986730769277
iteration 150, loss = 0.000675695831887424
iteration 151, loss = 0.0008080251282081008
iteration 152, loss = 0.001245825900696218
iteration 153, loss = 0.0007285380270332098
iteration 154, loss = 0.0015866875182837248
iteration 155, loss = 0.001951446640305221
iteration 156, loss = 0.0007349691004492342
iteration 157, loss = 0.0008908716263249516
iteration 158, loss = 0.0010486049577593803
iteration 159, loss = 0.0009454917744733393
iteration 160, loss = 0.0015408425824716687
iteration 161, loss = 0.00088121322914958
iteration 162, loss = 0.0010288499761372805
iteration 163, loss = 0.000904509040992707
iteration 164, loss = 0.0008910690667107701
iteration 165, loss = 0.0011703295167535543
iteration 166, loss = 0.0009695968474261463
iteration 167, loss = 0.0008450857712887228
iteration 168, loss = 0.0008641108288429677
iteration 169, loss = 0.000954447197727859
iteration 170, loss = 0.0007028787513263524
iteration 171, loss = 0.0008798984927125275
iteration 172, loss = 0.0008113714284263551
iteration 173, loss = 0.0011428804136812687
iteration 174, loss = 0.0007614372298121452
iteration 175, loss = 0.0010374661069363356
iteration 176, loss = 0.0009687864221632481
iteration 177, loss = 0.0007206873269751668
iteration 178, loss = 0.0008063531713560224
iteration 179, loss = 0.0008850828162394464
iteration 180, loss = 0.0009349054307676852
iteration 181, loss = 0.000898378377314657
iteration 182, loss = 0.0013715154491364956
iteration 183, loss = 0.0007970811566337943
iteration 184, loss = 0.0007194943027570844
iteration 185, loss = 0.0016077125910669565
iteration 186, loss = 0.000711922359187156
iteration 187, loss = 0.0008824688848108053
iteration 188, loss = 0.0008428208529949188
iteration 189, loss = 0.0007563668768852949
iteration 190, loss = 0.0007249765913002193
iteration 191, loss = 0.0007767693023197353
iteration 192, loss = 0.0008152065565809608
iteration 193, loss = 0.0016900398768484592
iteration 194, loss = 0.0008531952043995261
iteration 195, loss = 0.000834583246614784
iteration 196, loss = 0.0009124664939008653
iteration 197, loss = 0.0009757125517353415
iteration 198, loss = 0.001346895471215248
iteration 199, loss = 0.0007458791369572282
iteration 200, loss = 0.0008816453628242016
iteration 201, loss = 0.0008216763380914927
iteration 202, loss = 0.0008404032560065389
iteration 203, loss = 0.0009427139884792268
iteration 204, loss = 0.0009411915671080351
iteration 205, loss = 0.0009093573899008334
iteration 206, loss = 0.0008550427155569196
iteration 207, loss = 0.0007561914389953017
iteration 208, loss = 0.0014771254500374198
iteration 209, loss = 0.0007468884577974677
iteration 210, loss = 0.0010846118675544858
iteration 211, loss = 0.0010422103805467486
iteration 212, loss = 0.0009946655482053757
iteration 213, loss = 0.0008814485627226532
iteration 214, loss = 0.0009800354018807411
iteration 215, loss = 0.0010265535674989223
iteration 216, loss = 0.0009019189747050405
iteration 217, loss = 0.0007756314007565379
iteration 218, loss = 0.0007679184782318771
iteration 219, loss = 0.0010543538955971599
iteration 220, loss = 0.0007859717588871717
iteration 221, loss = 0.0009874477982521057
iteration 222, loss = 0.0008752617868594825
iteration 223, loss = 0.0007282624719664454
iteration 224, loss = 0.0014359699562191963
iteration 225, loss = 0.0008886200957931578
iteration 226, loss = 0.00103272614069283
iteration 227, loss = 0.001197568723000586
iteration 228, loss = 0.0008563731098547578
iteration 229, loss = 0.0010214101057499647
iteration 230, loss = 0.0011181705631315708
iteration 231, loss = 0.0007958049536682665
iteration 232, loss = 0.0008368585258722305
iteration 233, loss = 0.0007622509729117155
iteration 234, loss = 0.0010248206090182066
iteration 235, loss = 0.0009496781858615577
iteration 236, loss = 0.001141881919465959
iteration 237, loss = 0.0012761063408106565
iteration 238, loss = 0.000889197806827724
iteration 239, loss = 0.0007396856672130525
iteration 240, loss = 0.0006920075393281877
iteration 241, loss = 0.0012925652554258704
iteration 242, loss = 0.0009893957758322358
iteration 243, loss = 0.0008193390676751733
iteration 244, loss = 0.0008135052630677819
iteration 245, loss = 0.0008184420876204967
iteration 246, loss = 0.0008980290731415153
iteration 247, loss = 0.0007503862725570798
iteration 248, loss = 0.0008138545090332627
iteration 249, loss = 0.0007194368517957628
iteration 250, loss = 0.0014697546139359474
iteration 251, loss = 0.0009533428819850087
iteration 252, loss = 0.0008527456666342914
iteration 253, loss = 0.0011929136235266924
iteration 254, loss = 0.0007335992995649576
iteration 255, loss = 0.0008777305483818054
iteration 256, loss = 0.0007411761325784028
iteration 257, loss = 0.0008404097752645612
iteration 258, loss = 0.0011341115459799767
iteration 259, loss = 0.0008408841677010059
iteration 260, loss = 0.001165804686024785
iteration 261, loss = 0.0007836560253053904
iteration 262, loss = 0.0010596119100227952
iteration 263, loss = 0.0007968414574861526
iteration 264, loss = 0.0008109778864309192
iteration 265, loss = 0.000841756584122777
iteration 266, loss = 0.0009403703734278679
iteration 267, loss = 0.0010804173070937395
iteration 268, loss = 0.0010456256568431854
iteration 269, loss = 0.0011170541401952505
iteration 270, loss = 0.0014535145601257682
iteration 271, loss = 0.0009238182101398706
iteration 272, loss = 0.0008907828596420586
iteration 273, loss = 0.001452270895242691
iteration 274, loss = 0.0011370884021744132
iteration 275, loss = 0.0009361103875562549
iteration 276, loss = 0.0014030352467671037
iteration 277, loss = 0.0008136508986353874
iteration 278, loss = 0.000814542465377599
iteration 279, loss = 0.0009271663147956133
iteration 280, loss = 0.0008958191028796136
iteration 281, loss = 0.000717509537935257
iteration 282, loss = 0.0007320589502342045
iteration 283, loss = 0.0008088615140877664
iteration 284, loss = 0.0006659684586338699
iteration 285, loss = 0.0008078211103565991
iteration 286, loss = 0.0008379483479075134
iteration 287, loss = 0.0006721154786646366
iteration 288, loss = 0.0007773001561872661
iteration 289, loss = 0.0010260732378810644
iteration 290, loss = 0.0007149211596697569
iteration 291, loss = 0.0009549222304485738
iteration 292, loss = 0.0009704458061605692
iteration 293, loss = 0.0011252694530412555
iteration 294, loss = 0.0011411295272409916
iteration 295, loss = 0.0007324431790038943
iteration 296, loss = 0.001292938250117004
iteration 297, loss = 0.0011378428898751736
iteration 298, loss = 0.0008540345588698983
iteration 299, loss = 0.000985640101134777
iteration 300, loss = 0.0008312702993862331
iteration 1, loss = 0.00079535465920344
iteration 2, loss = 0.0018696744227781892
iteration 3, loss = 0.00148050207644701
iteration 4, loss = 0.0010958575876429677
iteration 5, loss = 0.0008211730746552348
iteration 6, loss = 0.0007581451791338623
iteration 7, loss = 0.0007312748348340392
iteration 8, loss = 0.0016324674943462014
iteration 9, loss = 0.0009516093414276838
iteration 10, loss = 0.0013636706862598658
iteration 11, loss = 0.0008747130050323904
iteration 12, loss = 0.0009340144461020827
iteration 13, loss = 0.001062112976796925
iteration 14, loss = 0.0011531494092196226
iteration 15, loss = 0.0009673417080193758
iteration 16, loss = 0.0007951126899570227
iteration 17, loss = 0.0008745211525820196
iteration 18, loss = 0.0007901517092250288
iteration 19, loss = 0.0008509317995049059
iteration 20, loss = 0.0008881047833710909
iteration 21, loss = 0.0016397165600210428
iteration 22, loss = 0.0008013327606022358
iteration 23, loss = 0.0008763825753703713
iteration 24, loss = 0.0012093366822227836
iteration 25, loss = 0.0009229590650647879
iteration 26, loss = 0.0007782162283547223
iteration 27, loss = 0.0009170868434011936
iteration 28, loss = 0.0007790960371494293
iteration 29, loss = 0.0010175994830206037
iteration 30, loss = 0.0011562920408323407
iteration 31, loss = 0.0009508045041002333
iteration 32, loss = 0.0011839286889880896
iteration 33, loss = 0.0016540571814402938
iteration 34, loss = 0.0008343573426827788
iteration 35, loss = 0.0011125458404421806
iteration 36, loss = 0.0007917268667370081
iteration 37, loss = 0.0008363526430912316
iteration 38, loss = 0.000883707485627383
iteration 39, loss = 0.0010932805016636848
iteration 40, loss = 0.0010639071697369218
iteration 41, loss = 0.0009077439899556339
iteration 42, loss = 0.0011624523904174566
iteration 43, loss = 0.0007313367095775902
iteration 44, loss = 0.0009192304569296539
iteration 45, loss = 0.0007920547504909337
iteration 46, loss = 0.0009763779235072434
iteration 47, loss = 0.0008770943968556821
iteration 48, loss = 0.0008850698359310627
iteration 49, loss = 0.000845705159008503
iteration 50, loss = 0.0011888830922544003
iteration 51, loss = 0.0008944402216002345
iteration 52, loss = 0.0009599669137969613
iteration 53, loss = 0.0008814395405352116
iteration 54, loss = 0.0016839017625898123
iteration 55, loss = 0.0006850480567663908
iteration 56, loss = 0.0011716943699866533
iteration 57, loss = 0.0007324559846892953
iteration 58, loss = 0.0016348689096048474
iteration 59, loss = 0.000996700138784945
iteration 60, loss = 0.0008315270533785224
iteration 61, loss = 0.0008791909785941243
iteration 62, loss = 0.0010982734384015203
iteration 63, loss = 0.0008962607244029641
iteration 64, loss = 0.0007103726384229958
iteration 65, loss = 0.0008146522450260818
iteration 66, loss = 0.0009774158243089914
iteration 67, loss = 0.0008168901549652219
iteration 68, loss = 0.000711152155417949
iteration 69, loss = 0.0009252908639609814
iteration 70, loss = 0.0007177916122600436
iteration 71, loss = 0.001732669654302299
iteration 72, loss = 0.0009787563467398286
iteration 73, loss = 0.0008391443407163024
iteration 74, loss = 0.0011142354924231768
iteration 75, loss = 0.0007828263915143907
iteration 76, loss = 0.0009461583686061203
iteration 77, loss = 0.0009543566266074777
iteration 78, loss = 0.0007662550779059529
iteration 79, loss = 0.0008240577881224453
iteration 80, loss = 0.0007132628234103322
iteration 81, loss = 0.0008527528843842447
iteration 82, loss = 0.0007002656348049641
iteration 83, loss = 0.0011304066283628345
iteration 84, loss = 0.0010072706500068307
iteration 85, loss = 0.0009074275731109083
iteration 86, loss = 0.0009363386197946966
iteration 87, loss = 0.0008797470945864916
iteration 88, loss = 0.001234665629453957
iteration 89, loss = 0.000772915140260011
iteration 90, loss = 0.0006971801049076021
iteration 91, loss = 0.0010270877974107862
iteration 92, loss = 0.0007398456800729036
iteration 93, loss = 0.0010144044645130634
iteration 94, loss = 0.0010454852599650621
iteration 95, loss = 0.0008651794632896781
iteration 96, loss = 0.0008787894039414823
iteration 97, loss = 0.0008810761501081288
iteration 98, loss = 0.0017156921094283462
iteration 99, loss = 0.0009535239660181105
iteration 100, loss = 0.000690665387082845
iteration 101, loss = 0.000717837130650878
iteration 102, loss = 0.0007699310081079602
iteration 103, loss = 0.0008889264427125454
iteration 104, loss = 0.0008617836865596473
iteration 105, loss = 0.0006874813698232174
iteration 106, loss = 0.0007645354489795864
iteration 107, loss = 0.0022374375257641077
iteration 108, loss = 0.000988326850347221
iteration 109, loss = 0.0009356210357509553
iteration 110, loss = 0.0007804522756487131
iteration 111, loss = 0.0010474484879523516
iteration 112, loss = 0.0009272293536923826
iteration 113, loss = 0.000749743718188256
iteration 114, loss = 0.0009345146245323122
iteration 115, loss = 0.0007506315014325082
iteration 116, loss = 0.0010073379380628467
iteration 117, loss = 0.0007760935113765299
iteration 118, loss = 0.0007666460587643087
iteration 119, loss = 0.0012045067269355059
iteration 120, loss = 0.0010403264313936234
iteration 121, loss = 0.0019547122064977884
iteration 122, loss = 0.0007790367235429585
iteration 123, loss = 0.001199865248054266
iteration 124, loss = 0.0008578422130085528
iteration 125, loss = 0.0007808511727489531
iteration 126, loss = 0.0007919279159978032
iteration 127, loss = 0.0010514184832572937
iteration 128, loss = 0.0007064297678880394
iteration 129, loss = 0.0008974741213023663
iteration 130, loss = 0.0007882316131144762
iteration 131, loss = 0.001105071627534926
iteration 132, loss = 0.00107478944119066
iteration 133, loss = 0.0008334701415151358
iteration 134, loss = 0.0010426371591165662
iteration 135, loss = 0.0007636055233888328
iteration 136, loss = 0.000799626053776592
iteration 137, loss = 0.0008869664743542671
iteration 138, loss = 0.0008871700847521424
iteration 139, loss = 0.0012218353804200888
iteration 140, loss = 0.0007472149445675313
iteration 141, loss = 0.0008602443267591298
iteration 142, loss = 0.0007638598326593637
iteration 143, loss = 0.0012267078272998333
iteration 144, loss = 0.0010729902423918247
iteration 145, loss = 0.0007515824981965125
iteration 146, loss = 0.0008069215691648424
iteration 147, loss = 0.001614549197256565
iteration 148, loss = 0.0008653072291053832
iteration 149, loss = 0.0007629855535924435
iteration 150, loss = 0.001441471977159381
iteration 151, loss = 0.0010201920522376895
iteration 152, loss = 0.0008529330953024328
iteration 153, loss = 0.0009544701315462589
iteration 154, loss = 0.0007511420990340412
iteration 155, loss = 0.001122422982007265
iteration 156, loss = 0.0009033074602484703
iteration 157, loss = 0.0017231612000614405
iteration 158, loss = 0.0009389977785758674
iteration 159, loss = 0.001025866949930787
iteration 160, loss = 0.00101156544405967
iteration 161, loss = 0.0009914071997627616
iteration 162, loss = 0.0008716817828826606
iteration 163, loss = 0.0009121104376390576
iteration 164, loss = 0.0015465017640963197
iteration 165, loss = 0.00175218575168401
iteration 166, loss = 0.0008110702619887888
iteration 167, loss = 0.0007240521954372525
iteration 168, loss = 0.0009906067280098796
iteration 169, loss = 0.001685709459707141
iteration 170, loss = 0.0007992228493094444
iteration 171, loss = 0.0007338736322708428
iteration 172, loss = 0.0008555816020816565
iteration 173, loss = 0.0007894343580119312
iteration 174, loss = 0.0007376935100182891
iteration 175, loss = 0.000871662050485611
iteration 176, loss = 0.000780311122070998
iteration 177, loss = 0.0009042284800671041
iteration 178, loss = 0.0011905916035175323
iteration 179, loss = 0.0006860154098831117
iteration 180, loss = 0.0008926542941480875
iteration 181, loss = 0.0006777960807085037
iteration 182, loss = 0.0007443437934853137
iteration 183, loss = 0.0013059235643595457
iteration 184, loss = 0.0009615555172786117
iteration 185, loss = 0.0017583919689059258
iteration 186, loss = 0.0009108614758588374
iteration 187, loss = 0.0007665880839340389
iteration 188, loss = 0.0007253980729728937
iteration 189, loss = 0.0007515722536481917
iteration 190, loss = 0.0014729113318026066
iteration 191, loss = 0.0013674232177436352
iteration 192, loss = 0.0008293027058243752
iteration 193, loss = 0.0008509865729138255
iteration 194, loss = 0.0008147854823619127
iteration 195, loss = 0.0009831818751990795
iteration 196, loss = 0.0007976815104484558
iteration 197, loss = 0.0007050600252114236
iteration 198, loss = 0.0008954150835052133
iteration 199, loss = 0.0008962181746028364
iteration 200, loss = 0.0009273007744923234
iteration 201, loss = 0.0006519270245917141
iteration 202, loss = 0.0010599965462461114
iteration 203, loss = 0.0007706924807280302
iteration 204, loss = 0.000927099259570241
iteration 205, loss = 0.0010534364264458418
iteration 206, loss = 0.0007972967578098178
iteration 207, loss = 0.0016215924406424165
iteration 208, loss = 0.0009490791126154363
iteration 209, loss = 0.0009641522192396224
iteration 210, loss = 0.0007908599800430238
iteration 211, loss = 0.0008156138937920332
iteration 212, loss = 0.0007454227888956666
iteration 213, loss = 0.0010351252276450396
iteration 214, loss = 0.0014341225614771247
iteration 215, loss = 0.001097576110623777
iteration 216, loss = 0.0006209974526427686
iteration 217, loss = 0.0008093268261291087
iteration 218, loss = 0.000899838691111654
iteration 219, loss = 0.0011988597689196467
iteration 220, loss = 0.0007494775345548987
iteration 221, loss = 0.0016992166638374329
iteration 222, loss = 0.0009385464363731444
iteration 223, loss = 0.0006175235030241311
iteration 224, loss = 0.0009406375465914607
iteration 225, loss = 0.0008212304092012346
iteration 226, loss = 0.0006942697800695896
iteration 227, loss = 0.0007752500241622329
iteration 228, loss = 0.000878945691511035
iteration 229, loss = 0.0019095975439995527
iteration 230, loss = 0.0007567972643300891
iteration 231, loss = 0.0009156075539067388
iteration 232, loss = 0.0013664740836247802
iteration 233, loss = 0.0007921594078652561
iteration 234, loss = 0.0008016512147150934
iteration 235, loss = 0.001437430502846837
iteration 236, loss = 0.00123527180403471
iteration 237, loss = 0.0009018381242640316
iteration 238, loss = 0.0008470678003504872
iteration 239, loss = 0.0008403511019423604
iteration 240, loss = 0.0009778173407539725
iteration 241, loss = 0.0009917987044900656
iteration 242, loss = 0.0009588460088707507
iteration 243, loss = 0.0011598272249102592
iteration 244, loss = 0.001662220573052764
iteration 245, loss = 0.0008822751697152853
iteration 246, loss = 0.0008531101630069315
iteration 247, loss = 0.0007543397950939834
iteration 248, loss = 0.000901584280654788
iteration 249, loss = 0.0008485991274937987
iteration 250, loss = 0.0007972692255862057
iteration 251, loss = 0.0017472507897764444
iteration 252, loss = 0.0015116670401766896
iteration 253, loss = 0.0010001856135204434
iteration 254, loss = 0.001173245720565319
iteration 255, loss = 0.0009412934305146337
iteration 256, loss = 0.001130926888436079
iteration 257, loss = 0.000776086759287864
iteration 258, loss = 0.0008380070212297142
iteration 259, loss = 0.0011979438131675124
iteration 260, loss = 0.0010556295746937394
iteration 261, loss = 0.0007853793213143945
iteration 262, loss = 0.0009693884057924151
iteration 263, loss = 0.0009951044339686632
iteration 264, loss = 0.0008247393416240811
iteration 265, loss = 0.0006361763225868344
iteration 266, loss = 0.0008177686249837279
iteration 267, loss = 0.0008883947157301009
iteration 268, loss = 0.0008343879599124193
iteration 269, loss = 0.0008850446902215481
iteration 270, loss = 0.0006820866838097572
iteration 271, loss = 0.0007515837787650526
iteration 272, loss = 0.0009679546346887946
iteration 273, loss = 0.0008151817601174116
iteration 274, loss = 0.0011620426084846258
iteration 275, loss = 0.000887068104930222
iteration 276, loss = 0.001015300047583878
iteration 277, loss = 0.000881772895809263
iteration 278, loss = 0.0007724369643256068
iteration 279, loss = 0.0008315975428558886
iteration 280, loss = 0.0008027178701013327
iteration 281, loss = 0.0007272841176018119
iteration 282, loss = 0.0013121002120897174
iteration 283, loss = 0.0009009065106511116
iteration 284, loss = 0.0008860677480697632
iteration 285, loss = 0.000778062385506928
iteration 286, loss = 0.0010290576610714197
iteration 287, loss = 0.000813753460533917
iteration 288, loss = 0.0008170547662302852
iteration 289, loss = 0.001685648923739791
iteration 290, loss = 0.0010910937562584877
iteration 291, loss = 0.0007211023475974798
iteration 292, loss = 0.0019413926638662815
iteration 293, loss = 0.000849813804961741
iteration 294, loss = 0.000938213721383363
iteration 295, loss = 0.0010104657849296927
iteration 296, loss = 0.0007760617299936712
iteration 297, loss = 0.0008365761605091393
iteration 298, loss = 0.0015042719896882772
iteration 299, loss = 0.0009674782049842179
iteration 300, loss = 0.000836135121062398

iteration 0, loss = 0.5003914833068848
iteration 1, loss = 0.5044916868209839
iteration 2, loss = 0.4963113069534302
iteration 3, loss = 0.5024190545082092
iteration 4, loss = 0.5035805702209473
iteration 5, loss = 0.49707671999931335
iteration 6, loss = 0.5023016333580017
iteration 7, loss = 0.5010464787483215
iteration 8, loss = 0.5013275742530823
iteration 9, loss = 0.5040796995162964
iteration 10, loss = 0.49251532554626465
iteration 11, loss = 0.5013439655303955
iteration 12, loss = 0.4982791543006897
iteration 13, loss = 0.5006093382835388
iteration 14, loss = 0.501862645149231
iteration 15, loss = 0.4969845414161682
iteration 16, loss = 0.4982926845550537
iteration 17, loss = 0.50023353099823
iteration 18, loss = 0.49963557720184326
iteration 19, loss = 0.4966583251953125
iteration 20, loss = 0.49835777282714844
iteration 21, loss = 0.4979429244995117
iteration 22, loss = 0.501025915145874
iteration 23, loss = 0.4991406798362732
iteration 24, loss = 0.49777671694755554
iteration 25, loss = 0.4990825653076172
iteration 26, loss = 0.49745845794677734
iteration 27, loss = 0.4982722997665405
iteration 28, loss = 0.497633695602417
iteration 29, loss = 0.49707406759262085
iteration 30, loss = 0.49564409255981445
iteration 31, loss = 0.5006633996963501
iteration 32, loss = 0.5028723478317261
iteration 33, loss = 0.5028051137924194
iteration 34, loss = 0.5003803372383118
iteration 35, loss = 0.4973450303077698
iteration 36, loss = 0.49639254808425903
iteration 37, loss = 0.4913034439086914
iteration 38, loss = 0.4989021122455597
iteration 39, loss = 0.49479448795318604
iteration 40, loss = 0.5001920461654663
iteration 41, loss = 0.49814102053642273
iteration 42, loss = 0.49631041288375854
iteration 43, loss = 0.49873608350753784
iteration 44, loss = 0.49707067012786865
iteration 45, loss = 0.4998646378517151
iteration 46, loss = 0.49776771664619446
iteration 47, loss = 0.4967364966869354
iteration 48, loss = 0.5005540251731873
iteration 49, loss = 0.49368733167648315
iteration 50, loss = 0.49371325969696045
iteration 51, loss = 0.49232184886932373
iteration 52, loss = 0.4967752993106842
iteration 53, loss = 0.49697065353393555
iteration 54, loss = 0.49880290031433105
iteration 55, loss = 0.496135413646698
iteration 56, loss = 0.49608370661735535
iteration 57, loss = 0.4928418695926666
iteration 58, loss = 0.49802839756011963
iteration 59, loss = 0.49179548025131226
iteration 60, loss = 0.495338499546051
iteration 61, loss = 0.49748173356056213
iteration 62, loss = 0.49674296379089355
iteration 63, loss = 0.49347877502441406
iteration 64, loss = 0.4934096932411194
iteration 65, loss = 0.4960402846336365
iteration 66, loss = 0.4937098026275635
iteration 67, loss = 0.4902360141277313
iteration 68, loss = 0.49595925211906433
iteration 69, loss = 0.4968982934951782
iteration 70, loss = 0.4933498799800873
iteration 71, loss = 0.4978843331336975
iteration 72, loss = 0.49471259117126465
iteration 73, loss = 0.49410146474838257
iteration 74, loss = 0.4976576566696167
iteration 75, loss = 0.49570780992507935
iteration 76, loss = 0.49473974108695984
iteration 77, loss = 0.4949512481689453
iteration 78, loss = 0.49163347482681274
iteration 79, loss = 0.49191173911094666
iteration 80, loss = 0.4925200343132019
iteration 81, loss = 0.4919998049736023
iteration 82, loss = 0.49860459566116333
iteration 83, loss = 0.49219098687171936
iteration 84, loss = 0.49457886815071106
iteration 85, loss = 0.4922421872615814
iteration 86, loss = 0.4934963583946228
iteration 87, loss = 0.4982801675796509
iteration 88, loss = 0.4930085241794586
iteration 89, loss = 0.4930785000324249
iteration 90, loss = 0.4904646873474121
iteration 91, loss = 0.49401944875717163
iteration 92, loss = 0.4914642572402954
iteration 93, loss = 0.4981456398963928
iteration 94, loss = 0.48791778087615967
iteration 95, loss = 0.49200960993766785
iteration 96, loss = 0.4897462725639343
iteration 97, loss = 0.49020248651504517
iteration 98, loss = 0.49256205558776855
iteration 99, loss = 0.4909774959087372
iteration 100, loss = 0.48949134349823
iteration 101, loss = 0.4927612543106079
iteration 102, loss = 0.4910306930541992
iteration 103, loss = 0.494540274143219
iteration 104, loss = 0.48876121640205383
iteration 105, loss = 0.49066320061683655
iteration 106, loss = 0.49315333366394043
iteration 107, loss = 0.49037832021713257
iteration 108, loss = 0.4899960160255432
iteration 109, loss = 0.4947105050086975
iteration 110, loss = 0.4885227680206299
iteration 111, loss = 0.490614652633667
iteration 112, loss = 0.48941659927368164
iteration 113, loss = 0.4893697202205658
iteration 114, loss = 0.4897806644439697
iteration 115, loss = 0.49126607179641724
iteration 116, loss = 0.4876897931098938
iteration 117, loss = 0.48487329483032227
iteration 118, loss = 0.49282246828079224
iteration 119, loss = 0.4912380576133728
iteration 120, loss = 0.4905361533164978
iteration 121, loss = 0.49185365438461304
iteration 122, loss = 0.491379052400589
iteration 123, loss = 0.49433135986328125
iteration 124, loss = 0.49180805683135986
iteration 125, loss = 0.4840471148490906
iteration 126, loss = 0.487789511680603
iteration 127, loss = 0.4923672080039978
iteration 128, loss = 0.48825693130493164
iteration 129, loss = 0.4951283931732178
iteration 130, loss = 0.4925914704799652
iteration 131, loss = 0.48807159066200256
iteration 132, loss = 0.48934561014175415
iteration 133, loss = 0.4874391257762909
iteration 134, loss = 0.4903802275657654
iteration 135, loss = 0.4899710714817047
iteration 136, loss = 0.48453065752983093
iteration 137, loss = 0.487957626581192
iteration 138, loss = 0.4846597909927368
iteration 139, loss = 0.49059367179870605
iteration 140, loss = 0.48735588788986206
iteration 141, loss = 0.48860499262809753
iteration 142, loss = 0.48209601640701294
iteration 143, loss = 0.4882347583770752
iteration 144, loss = 0.48239952325820923
iteration 145, loss = 0.48605722188949585
iteration 146, loss = 0.4877506494522095
iteration 147, loss = 0.4840366244316101
iteration 148, loss = 0.4870116114616394
iteration 149, loss = 0.48831814527511597
iteration 150, loss = 0.4854866862297058
iteration 151, loss = 0.4836388826370239
iteration 152, loss = 0.4871833920478821
iteration 153, loss = 0.4935404360294342
iteration 154, loss = 0.48166340589523315
iteration 155, loss = 0.48830872774124146
iteration 156, loss = 0.4874093532562256
iteration 157, loss = 0.48330867290496826
iteration 158, loss = 0.48570847511291504
iteration 159, loss = 0.48873472213745117
iteration 160, loss = 0.48432987928390503
iteration 161, loss = 0.48332056403160095
iteration 162, loss = 0.4853227138519287
iteration 163, loss = 0.4836844801902771
iteration 164, loss = 0.4857999086380005
iteration 165, loss = 0.4860289692878723
iteration 166, loss = 0.4858762323856354
iteration 167, loss = 0.4832554757595062
iteration 168, loss = 0.4846026599407196
iteration 169, loss = 0.48471760749816895
iteration 170, loss = 0.4822506904602051
iteration 171, loss = 0.484569251537323
iteration 172, loss = 0.48967602849006653
iteration 173, loss = 0.4783462882041931
iteration 174, loss = 0.4840155243873596
iteration 175, loss = 0.4886165261268616
iteration 176, loss = 0.48917460441589355
iteration 177, loss = 0.48435789346694946
iteration 178, loss = 0.48532772064208984
iteration 179, loss = 0.4862532913684845
iteration 180, loss = 0.4853453040122986
iteration 181, loss = 0.48125410079956055
iteration 182, loss = 0.48195070028305054
iteration 183, loss = 0.48447051644325256
iteration 184, loss = 0.48220932483673096
iteration 185, loss = 0.4909454584121704
iteration 186, loss = 0.4828663170337677
iteration 187, loss = 0.4796288013458252
iteration 188, loss = 0.485382080078125
iteration 189, loss = 0.48610442876815796
iteration 190, loss = 0.4875026047229767
iteration 191, loss = 0.4838871359825134
iteration 192, loss = 0.48310694098472595
iteration 193, loss = 0.48748624324798584
iteration 194, loss = 0.4799553155899048
iteration 195, loss = 0.4811762571334839
iteration 196, loss = 0.4814452528953552
iteration 197, loss = 0.48567914962768555
iteration 198, loss = 0.48232024908065796
iteration 199, loss = 0.47974756360054016
iteration 200, loss = 0.4823008179664612
iteration 201, loss = 0.479133665561676
iteration 202, loss = 0.4837476313114166
iteration 203, loss = 0.48409244418144226
iteration 204, loss = 0.48320215940475464
iteration 205, loss = 0.48763614892959595
iteration 206, loss = 0.48469430208206177
iteration 207, loss = 0.47504493594169617
iteration 208, loss = 0.4805435240268707
iteration 209, loss = 0.4791552424430847
iteration 210, loss = 0.47675350308418274
iteration 211, loss = 0.48246073722839355
iteration 212, loss = 0.4795949459075928
iteration 213, loss = 0.4839335083961487
iteration 214, loss = 0.4778434634208679
iteration 215, loss = 0.48037397861480713
iteration 216, loss = 0.48007944226264954
iteration 217, loss = 0.48130905628204346
iteration 218, loss = 0.48280614614486694
iteration 219, loss = 0.4779049754142761
iteration 220, loss = 0.48640578985214233
iteration 221, loss = 0.48073363304138184
iteration 222, loss = 0.48303449153900146
iteration 223, loss = 0.4809953272342682
iteration 224, loss = 0.47597455978393555
iteration 225, loss = 0.47644275426864624
iteration 226, loss = 0.4822973608970642
iteration 227, loss = 0.47785860300064087
iteration 228, loss = 0.47594037652015686
iteration 229, loss = 0.48189228773117065
iteration 230, loss = 0.48312872648239136
iteration 231, loss = 0.4816427230834961
iteration 232, loss = 0.4748660922050476
iteration 233, loss = 0.476917564868927
iteration 234, loss = 0.47832685708999634
iteration 235, loss = 0.4776865243911743
iteration 236, loss = 0.47914695739746094
iteration 237, loss = 0.48054856061935425
iteration 238, loss = 0.4884034991264343
iteration 239, loss = 0.48202407360076904
iteration 240, loss = 0.48196351528167725
iteration 241, loss = 0.4781953692436218
iteration 242, loss = 0.4764293432235718
iteration 243, loss = 0.477439284324646
iteration 244, loss = 0.4798531234264374
iteration 245, loss = 0.4823259711265564
iteration 246, loss = 0.4803069233894348
iteration 247, loss = 0.47779810428619385
iteration 248, loss = 0.47911542654037476
iteration 249, loss = 0.4762943387031555
iteration 250, loss = 0.47350943088531494
iteration 251, loss = 0.4765251874923706
iteration 252, loss = 0.48198068141937256
iteration 253, loss = 0.4789954721927643
iteration 254, loss = 0.4825391173362732
iteration 255, loss = 0.47362494468688965
iteration 256, loss = 0.4758165776729584
iteration 257, loss = 0.47766047716140747
iteration 258, loss = 0.4778008460998535
iteration 259, loss = 0.47970420122146606
iteration 260, loss = 0.4773339033126831
iteration 261, loss = 0.4768734574317932
iteration 262, loss = 0.4752131700515747
iteration 263, loss = 0.47501105070114136
iteration 264, loss = 0.47961124777793884
iteration 265, loss = 0.47076451778411865
iteration 266, loss = 0.4719007611274719
iteration 267, loss = 0.47561198472976685
iteration 268, loss = 0.47792351245880127
iteration 269, loss = 0.47968095541000366
iteration 270, loss = 0.4734518229961395
iteration 271, loss = 0.4814440906047821
iteration 272, loss = 0.4753230810165405
iteration 273, loss = 0.4771158695220947
iteration 274, loss = 0.4685937166213989
iteration 275, loss = 0.4754370450973511
iteration 276, loss = 0.47361788153648376
iteration 277, loss = 0.4725069999694824
iteration 278, loss = 0.4715332090854645
iteration 279, loss = 0.47276192903518677
iteration 280, loss = 0.4780622124671936
iteration 281, loss = 0.4782605767250061
iteration 282, loss = 0.4780278503894806
iteration 283, loss = 0.47638964653015137
iteration 284, loss = 0.4737584590911865
iteration 285, loss = 0.4669422507286072
iteration 286, loss = 0.4790017604827881
iteration 287, loss = 0.47499924898147583
iteration 288, loss = 0.4793153405189514
iteration 289, loss = 0.4802183508872986
iteration 290, loss = 0.4818708002567291
iteration 291, loss = 0.4735068678855896
iteration 292, loss = 0.47181418538093567
iteration 293, loss = 0.47489961981773376
iteration 294, loss = 0.46800994873046875
iteration 295, loss = 0.4773256778717041
iteration 296, loss = 0.47730931639671326
iteration 297, loss = 0.47826242446899414
iteration 298, loss = 0.4766320288181305
iteration 299, loss = 0.47369807958602905
iteration 0, loss = 0.4738773703575134
iteration 1, loss = 0.47922611236572266
iteration 2, loss = 0.4740896224975586
iteration 3, loss = 0.4736667275428772
iteration 4, loss = 0.4702126979827881
iteration 5, loss = 0.4752929210662842
iteration 6, loss = 0.47464051842689514
iteration 7, loss = 0.4740917682647705
iteration 8, loss = 0.46932676434516907
iteration 9, loss = 0.46819251775741577
iteration 10, loss = 0.47519245743751526
iteration 11, loss = 0.47440749406814575
iteration 12, loss = 0.4756183624267578
iteration 13, loss = 0.4754682779312134
iteration 14, loss = 0.47470787167549133
iteration 15, loss = 0.47018349170684814
iteration 16, loss = 0.47866564989089966
iteration 17, loss = 0.46889621019363403
iteration 18, loss = 0.47361353039741516
iteration 19, loss = 0.4745352268218994
iteration 20, loss = 0.46733465790748596
iteration 21, loss = 0.47216445207595825
iteration 22, loss = 0.46947693824768066
iteration 23, loss = 0.4735541343688965
iteration 24, loss = 0.4795687198638916
iteration 25, loss = 0.4813634157180786
iteration 26, loss = 0.46811580657958984
iteration 27, loss = 0.4631073474884033
iteration 28, loss = 0.4682657718658447
iteration 29, loss = 0.469854474067688
iteration 30, loss = 0.4701571762561798
iteration 31, loss = 0.47275498509407043
iteration 32, loss = 0.4684182405471802
iteration 33, loss = 0.46508315205574036
iteration 34, loss = 0.47485411167144775
iteration 35, loss = 0.4684794545173645
iteration 36, loss = 0.4725605845451355
iteration 37, loss = 0.46240144968032837
iteration 38, loss = 0.4745878279209137
iteration 39, loss = 0.4642743766307831
iteration 40, loss = 0.4636853337287903
iteration 41, loss = 0.470398485660553
iteration 42, loss = 0.47123903036117554
iteration 43, loss = 0.47225815057754517
iteration 44, loss = 0.4744938313961029
iteration 45, loss = 0.4702446162700653
iteration 46, loss = 0.4752807021141052
iteration 47, loss = 0.4785521626472473
iteration 48, loss = 0.4767096936702728
iteration 49, loss = 0.4690759778022766
iteration 50, loss = 0.4669000208377838
iteration 51, loss = 0.46842190623283386
iteration 52, loss = 0.4672752022743225
iteration 53, loss = 0.46574270725250244
iteration 54, loss = 0.46688729524612427
iteration 55, loss = 0.4665639102458954
iteration 56, loss = 0.4711354970932007
iteration 57, loss = 0.46995115280151367
iteration 58, loss = 0.4740297198295593
iteration 59, loss = 0.46645689010620117
iteration 60, loss = 0.46741461753845215
iteration 61, loss = 0.47284311056137085
iteration 62, loss = 0.4605705142021179
iteration 63, loss = 0.47247451543807983
iteration 64, loss = 0.4704267680644989
iteration 65, loss = 0.4735412001609802
iteration 66, loss = 0.4707959294319153
iteration 67, loss = 0.47313523292541504
iteration 68, loss = 0.4701908826828003
iteration 69, loss = 0.4630982279777527
iteration 70, loss = 0.462105393409729
iteration 71, loss = 0.4642385244369507
iteration 72, loss = 0.46266770362854004
iteration 73, loss = 0.4646129608154297
iteration 74, loss = 0.46954792737960815
iteration 75, loss = 0.4616144299507141
iteration 76, loss = 0.46411535143852234
iteration 77, loss = 0.4717930853366852
iteration 78, loss = 0.4624301791191101
iteration 79, loss = 0.4646345376968384
iteration 80, loss = 0.4660460352897644
iteration 81, loss = 0.46910935640335083
iteration 82, loss = 0.4767797589302063
iteration 83, loss = 0.4702497720718384
iteration 84, loss = 0.4654286503791809
iteration 85, loss = 0.46777111291885376
iteration 86, loss = 0.46224188804626465
iteration 87, loss = 0.4611035883426666
iteration 88, loss = 0.46534615755081177
iteration 89, loss = 0.46768951416015625
iteration 90, loss = 0.46496498584747314
iteration 91, loss = 0.4597553610801697
iteration 92, loss = 0.46192750334739685
iteration 93, loss = 0.46330422163009644
iteration 94, loss = 0.4710268974304199
iteration 95, loss = 0.46322333812713623
iteration 96, loss = 0.46417689323425293
iteration 97, loss = 0.461871862411499
iteration 98, loss = 0.4709753692150116
iteration 99, loss = 0.4660537838935852
iteration 100, loss = 0.4591112732887268
iteration 101, loss = 0.4701297879219055
iteration 102, loss = 0.4721224009990692
iteration 103, loss = 0.46424904465675354
iteration 104, loss = 0.4603811204433441
iteration 105, loss = 0.47597092390060425
iteration 106, loss = 0.46613314747810364
iteration 107, loss = 0.4639856815338135
iteration 108, loss = 0.47059881687164307
iteration 109, loss = 0.4637851119041443
iteration 110, loss = 0.46221810579299927
iteration 111, loss = 0.47044840455055237
iteration 112, loss = 0.47737860679626465
iteration 113, loss = 0.45303648710250854
iteration 114, loss = 0.4643784761428833
iteration 115, loss = 0.46010714769363403
iteration 116, loss = 0.46839439868927
iteration 117, loss = 0.4672684669494629
iteration 118, loss = 0.45554375648498535
iteration 119, loss = 0.45890912413597107
iteration 120, loss = 0.4619618058204651
iteration 121, loss = 0.46031415462493896
iteration 122, loss = 0.4612881541252136
iteration 123, loss = 0.45966923236846924
iteration 124, loss = 0.4628908038139343
iteration 125, loss = 0.46878761053085327
iteration 126, loss = 0.4553149938583374
iteration 127, loss = 0.4721071124076843
iteration 128, loss = 0.4560217261314392
iteration 129, loss = 0.4627203047275543
iteration 130, loss = 0.4635545611381531
iteration 131, loss = 0.4592519998550415
iteration 132, loss = 0.46473780274391174
iteration 133, loss = 0.4573289752006531
iteration 134, loss = 0.4605889916419983
iteration 135, loss = 0.47367432713508606
iteration 136, loss = 0.4634350538253784
iteration 137, loss = 0.46926891803741455
iteration 138, loss = 0.4551217555999756
iteration 139, loss = 0.4590698182582855
iteration 140, loss = 0.46394801139831543
iteration 141, loss = 0.4594455361366272
iteration 142, loss = 0.4601793587207794
iteration 143, loss = 0.46263471245765686
iteration 144, loss = 0.45454859733581543
iteration 145, loss = 0.46096277236938477
iteration 146, loss = 0.4670809507369995
iteration 147, loss = 0.46652236580848694
iteration 148, loss = 0.4649210572242737
iteration 149, loss = 0.45954859256744385
iteration 150, loss = 0.47106701135635376
iteration 151, loss = 0.4602079391479492
iteration 152, loss = 0.4655560851097107
iteration 153, loss = 0.46772366762161255
iteration 154, loss = 0.4598243534564972
iteration 155, loss = 0.45829230546951294
iteration 156, loss = 0.45644405484199524
iteration 157, loss = 0.4585249423980713
iteration 158, loss = 0.4588283896446228
iteration 159, loss = 0.4544951915740967
iteration 160, loss = 0.4602566957473755
iteration 161, loss = 0.4616588354110718
iteration 162, loss = 0.4622805714607239
iteration 163, loss = 0.4609861373901367
iteration 164, loss = 0.46776241064071655
iteration 165, loss = 0.4653245210647583
iteration 166, loss = 0.45477938652038574
iteration 167, loss = 0.46169036626815796
iteration 168, loss = 0.45889681577682495
iteration 169, loss = 0.4609535336494446
iteration 170, loss = 0.4612247943878174
iteration 171, loss = 0.47479307651519775
iteration 172, loss = 0.44890671968460083
iteration 173, loss = 0.4587933421134949
iteration 174, loss = 0.4584468901157379
iteration 175, loss = 0.45963209867477417
iteration 176, loss = 0.46622636914253235
iteration 177, loss = 0.45574960112571716
iteration 178, loss = 0.45588672161102295
iteration 179, loss = 0.45538198947906494
iteration 180, loss = 0.4602486491203308
iteration 181, loss = 0.4610672891139984
iteration 182, loss = 0.4575129747390747
iteration 183, loss = 0.4685479998588562
iteration 184, loss = 0.4572368860244751
iteration 185, loss = 0.4606453776359558
iteration 186, loss = 0.45923101902008057
iteration 187, loss = 0.4590342044830322
iteration 188, loss = 0.4584878087043762
iteration 189, loss = 0.45098185539245605
iteration 190, loss = 0.4437413215637207
iteration 191, loss = 0.4596371054649353
iteration 192, loss = 0.45668575167655945
iteration 193, loss = 0.4597700834274292
iteration 194, loss = 0.455160528421402
iteration 195, loss = 0.4532088041305542
iteration 196, loss = 0.45540234446525574
iteration 197, loss = 0.46189630031585693
iteration 198, loss = 0.4629131555557251
iteration 199, loss = 0.45901694893836975
iteration 200, loss = 0.46313726902008057
iteration 201, loss = 0.45288506150245667
iteration 202, loss = 0.45531877875328064
iteration 203, loss = 0.4470822811126709
iteration 204, loss = 0.4539891481399536
iteration 205, loss = 0.4573749303817749
iteration 206, loss = 0.46007537841796875
iteration 207, loss = 0.45031505823135376
iteration 208, loss = 0.4581449031829834
iteration 209, loss = 0.45604538917541504
iteration 210, loss = 0.44884198904037476
iteration 211, loss = 0.45025521516799927
iteration 212, loss = 0.4555369019508362
iteration 213, loss = 0.45660847425460815
iteration 214, loss = 0.4552919864654541
iteration 215, loss = 0.4551548659801483
iteration 216, loss = 0.4518832564353943
iteration 217, loss = 0.45401984453201294
iteration 218, loss = 0.4569237232208252
iteration 219, loss = 0.45165330171585083
iteration 220, loss = 0.44513243436813354
iteration 221, loss = 0.4554530382156372
iteration 222, loss = 0.46342259645462036
iteration 223, loss = 0.4483034610748291
iteration 224, loss = 0.45581117272377014
iteration 225, loss = 0.46474236249923706
iteration 226, loss = 0.4580439329147339
iteration 227, loss = 0.4568735659122467
iteration 228, loss = 0.4509401321411133
iteration 229, loss = 0.44907528162002563
iteration 230, loss = 0.46115031838417053
iteration 231, loss = 0.45536553859710693
iteration 232, loss = 0.45732855796813965
iteration 233, loss = 0.4562506079673767
iteration 234, loss = 0.45375144481658936
iteration 235, loss = 0.44927459955215454
iteration 236, loss = 0.45377102494239807
iteration 237, loss = 0.45977163314819336
iteration 238, loss = 0.45465847849845886
iteration 239, loss = 0.4614461660385132
iteration 240, loss = 0.45175686478614807
iteration 241, loss = 0.44969096779823303
iteration 242, loss = 0.4519454538822174
iteration 243, loss = 0.4492684006690979
iteration 244, loss = 0.44554072618484497
iteration 245, loss = 0.4428815245628357
iteration 246, loss = 0.4512271285057068
iteration 247, loss = 0.46328794956207275
iteration 248, loss = 0.46074944734573364
iteration 249, loss = 0.44955286383628845
iteration 250, loss = 0.44153422117233276
iteration 251, loss = 0.45252835750579834
iteration 252, loss = 0.4499710202217102
iteration 253, loss = 0.4642006456851959
iteration 254, loss = 0.45034492015838623
iteration 255, loss = 0.4512391686439514
iteration 256, loss = 0.4522251486778259
iteration 257, loss = 0.45691776275634766
iteration 258, loss = 0.4627855122089386
iteration 259, loss = 0.44987818598747253
iteration 260, loss = 0.4610220491886139
iteration 261, loss = 0.4428290128707886
iteration 262, loss = 0.4518928527832031
iteration 263, loss = 0.45601123571395874
iteration 264, loss = 0.45402002334594727
iteration 265, loss = 0.4539693295955658
iteration 266, loss = 0.4442178010940552
iteration 267, loss = 0.4557942748069763
iteration 268, loss = 0.44938063621520996
iteration 269, loss = 0.451626718044281
iteration 270, loss = 0.450376033782959
iteration 271, loss = 0.44778120517730713
iteration 272, loss = 0.4477248787879944
iteration 273, loss = 0.44550377130508423
iteration 274, loss = 0.4429763853549957
iteration 275, loss = 0.4449087381362915
iteration 276, loss = 0.44910022616386414
iteration 277, loss = 0.45287400484085083
iteration 278, loss = 0.4451909363269806
iteration 279, loss = 0.4552691876888275
iteration 280, loss = 0.4505113363265991
iteration 281, loss = 0.4489572048187256
iteration 282, loss = 0.4468820095062256
iteration 283, loss = 0.451198935508728
iteration 284, loss = 0.446068674325943
iteration 285, loss = 0.44273123145103455
iteration 286, loss = 0.4530700445175171
iteration 287, loss = 0.45900672674179077
iteration 288, loss = 0.4377783238887787
iteration 289, loss = 0.4375477433204651
iteration 290, loss = 0.4429203271865845
iteration 291, loss = 0.44075143337249756
iteration 292, loss = 0.45116132497787476
iteration 293, loss = 0.4437762498855591
iteration 294, loss = 0.45431023836135864
iteration 295, loss = 0.44504427909851074
iteration 296, loss = 0.4465484917163849
iteration 297, loss = 0.4501676559448242
iteration 298, loss = 0.43585705757141113
iteration 299, loss = 0.4374696612358093
iteration 0, loss = 0.4445878863334656
iteration 1, loss = 0.44201821088790894
iteration 2, loss = 0.43907395005226135
iteration 3, loss = 0.44551631808280945
iteration 4, loss = 0.4384750723838806
iteration 5, loss = 0.449021577835083
iteration 6, loss = 0.45581138134002686
iteration 7, loss = 0.4455553889274597
iteration 8, loss = 0.4354560375213623
iteration 9, loss = 0.4454174041748047
iteration 10, loss = 0.44477999210357666
iteration 11, loss = 0.4436032176017761
iteration 12, loss = 0.4440360963344574
iteration 13, loss = 0.44140613079071045
iteration 14, loss = 0.44839948415756226
iteration 15, loss = 0.4503082036972046
iteration 16, loss = 0.450836181640625
iteration 17, loss = 0.43897098302841187
iteration 18, loss = 0.4532930254936218
iteration 19, loss = 0.4425780773162842
iteration 20, loss = 0.44974297285079956
iteration 21, loss = 0.45545563101768494
iteration 22, loss = 0.44476133584976196
iteration 23, loss = 0.4345132112503052
iteration 24, loss = 0.4407675266265869
iteration 25, loss = 0.4447782635688782
iteration 26, loss = 0.4391150176525116
iteration 27, loss = 0.44392651319503784
iteration 28, loss = 0.4426644444465637
iteration 29, loss = 0.43981724977493286
iteration 30, loss = 0.4309556484222412
iteration 31, loss = 0.43583330512046814
iteration 32, loss = 0.43553537130355835
iteration 33, loss = 0.43749403953552246
iteration 34, loss = 0.4527953863143921
iteration 35, loss = 0.43255966901779175
iteration 36, loss = 0.44081443548202515
iteration 37, loss = 0.43232351541519165
iteration 38, loss = 0.4517386853694916
iteration 39, loss = 0.445900559425354
iteration 40, loss = 0.4464079737663269
iteration 41, loss = 0.45182931423187256
iteration 42, loss = 0.4393121898174286
iteration 43, loss = 0.4363114833831787
iteration 44, loss = 0.44753497838974
iteration 45, loss = 0.43606120347976685
iteration 46, loss = 0.4381096661090851
iteration 47, loss = 0.44649332761764526
iteration 48, loss = 0.43863236904144287
iteration 49, loss = 0.4444337487220764
iteration 50, loss = 0.4396028220653534
iteration 51, loss = 0.44467777013778687
iteration 52, loss = 0.4374730587005615
iteration 53, loss = 0.4486183524131775
iteration 54, loss = 0.44272565841674805
iteration 55, loss = 0.44133391976356506
iteration 56, loss = 0.4365619421005249
iteration 57, loss = 0.4468984603881836
iteration 58, loss = 0.4416605830192566
iteration 59, loss = 0.44543933868408203
iteration 60, loss = 0.4410550594329834
iteration 61, loss = 0.4507351517677307
iteration 62, loss = 0.43854936957359314
iteration 63, loss = 0.44312620162963867
iteration 64, loss = 0.4376407563686371
iteration 65, loss = 0.4416123628616333
iteration 66, loss = 0.4445783495903015
iteration 67, loss = 0.44050753116607666
iteration 68, loss = 0.4481354355812073
iteration 69, loss = 0.4462026357650757
iteration 70, loss = 0.4273056387901306
iteration 71, loss = 0.4424152970314026
iteration 72, loss = 0.4447000026702881
iteration 73, loss = 0.43993520736694336
iteration 74, loss = 0.45167815685272217
iteration 75, loss = 0.4530854821205139
iteration 76, loss = 0.439760684967041
iteration 77, loss = 0.43513715267181396
iteration 78, loss = 0.4654145836830139
iteration 79, loss = 0.4413702189922333
iteration 80, loss = 0.43414199352264404
iteration 81, loss = 0.4476248621940613
iteration 82, loss = 0.43200182914733887
iteration 83, loss = 0.43751636147499084
iteration 84, loss = 0.448699027299881
iteration 85, loss = 0.4428437352180481
iteration 86, loss = 0.4461295008659363
iteration 87, loss = 0.4351254105567932
iteration 88, loss = 0.4425761103630066
iteration 89, loss = 0.4612691104412079
iteration 90, loss = 0.4395851492881775
iteration 91, loss = 0.4462760090827942
iteration 92, loss = 0.43936869502067566
iteration 93, loss = 0.4363788962364197
iteration 94, loss = 0.4335296154022217
iteration 95, loss = 0.4468778371810913
iteration 96, loss = 0.4354904294013977
iteration 97, loss = 0.43969815969467163
iteration 98, loss = 0.4264717996120453
iteration 99, loss = 0.4409204125404358
iteration 100, loss = 0.4396236836910248
iteration 101, loss = 0.4332226514816284
iteration 102, loss = 0.44167354702949524
iteration 103, loss = 0.44350048899650574
iteration 104, loss = 0.43727248907089233
iteration 105, loss = 0.4434070587158203
iteration 106, loss = 0.4365811049938202
iteration 107, loss = 0.4267646074295044
iteration 108, loss = 0.4338289499282837
iteration 109, loss = 0.43388131260871887
iteration 110, loss = 0.4378700256347656
iteration 111, loss = 0.4356619715690613
iteration 112, loss = 0.4428088366985321
iteration 113, loss = 0.43330204486846924
iteration 114, loss = 0.4413642883300781
iteration 115, loss = 0.44505882263183594
iteration 116, loss = 0.42761972546577454
iteration 117, loss = 0.44192224740982056
iteration 118, loss = 0.43232017755508423
iteration 119, loss = 0.4202214479446411
iteration 120, loss = 0.4344644248485565
iteration 121, loss = 0.4278252422809601
iteration 122, loss = 0.42116332054138184
iteration 123, loss = 0.4302181601524353
iteration 124, loss = 0.43567895889282227
iteration 125, loss = 0.43439942598342896
iteration 126, loss = 0.4332267642021179
iteration 127, loss = 0.4232708811759949
iteration 128, loss = 0.42086511850357056
iteration 129, loss = 0.4347227215766907
iteration 130, loss = 0.4236888289451599
iteration 131, loss = 0.4403565526008606
iteration 132, loss = 0.4355052411556244
iteration 133, loss = 0.4431791305541992
iteration 134, loss = 0.4151654839515686
iteration 135, loss = 0.4316290318965912
iteration 136, loss = 0.43139398097991943
iteration 137, loss = 0.4307190179824829
iteration 138, loss = 0.43389683961868286
iteration 139, loss = 0.420588880777359
iteration 140, loss = 0.4362417459487915
iteration 141, loss = 0.43858885765075684
iteration 142, loss = 0.43753087520599365
iteration 143, loss = 0.43091896176338196
iteration 144, loss = 0.4476391077041626
iteration 145, loss = 0.4305613040924072
iteration 146, loss = 0.4294775426387787
iteration 147, loss = 0.45431989431381226
iteration 148, loss = 0.4354300796985626
iteration 149, loss = 0.4365847110748291
iteration 150, loss = 0.43893957138061523
iteration 151, loss = 0.4425984025001526
iteration 152, loss = 0.43271416425704956
iteration 153, loss = 0.420724093914032
iteration 154, loss = 0.4480798542499542
iteration 155, loss = 0.42066264152526855
iteration 156, loss = 0.42975252866744995
iteration 157, loss = 0.41933900117874146
iteration 158, loss = 0.42782658338546753
iteration 159, loss = 0.42957910895347595
iteration 160, loss = 0.4267737567424774
iteration 161, loss = 0.4352364242076874
iteration 162, loss = 0.4243139922618866
iteration 163, loss = 0.4160195589065552
iteration 164, loss = 0.4350011348724365
iteration 165, loss = 0.42278194427490234
iteration 166, loss = 0.4293478727340698
iteration 167, loss = 0.42983686923980713
iteration 168, loss = 0.4279974102973938
iteration 169, loss = 0.4220668077468872
iteration 170, loss = 0.42155036330223083
iteration 171, loss = 0.44239068031311035
iteration 172, loss = 0.41834792494773865
iteration 173, loss = 0.41851016879081726
iteration 174, loss = 0.4146808683872223
iteration 175, loss = 0.42552101612091064
iteration 176, loss = 0.4351149797439575
iteration 177, loss = 0.4163820147514343
iteration 178, loss = 0.4209296703338623
iteration 179, loss = 0.42715132236480713
iteration 180, loss = 0.4242172837257385
iteration 181, loss = 0.43186742067337036
iteration 182, loss = 0.4231334924697876
iteration 183, loss = 0.43301403522491455
iteration 184, loss = 0.4286928176879883
iteration 185, loss = 0.4204709231853485
iteration 186, loss = 0.41424137353897095
iteration 187, loss = 0.44792404770851135
iteration 188, loss = 0.4210311770439148
iteration 189, loss = 0.43873146176338196
iteration 190, loss = 0.4384555220603943
iteration 191, loss = 0.4272300601005554
iteration 192, loss = 0.42235761880874634
iteration 193, loss = 0.440545916557312
iteration 194, loss = 0.42261803150177
iteration 195, loss = 0.42634934186935425
iteration 196, loss = 0.44078803062438965
iteration 197, loss = 0.42314019799232483
iteration 198, loss = 0.43620479106903076
iteration 199, loss = 0.4136594533920288
iteration 200, loss = 0.42943745851516724
iteration 201, loss = 0.4216616153717041
iteration 202, loss = 0.43256017565727234
iteration 203, loss = 0.4155699610710144
iteration 204, loss = 0.4161226749420166
iteration 205, loss = 0.42840248346328735
iteration 206, loss = 0.42820489406585693
iteration 207, loss = 0.4304356873035431
iteration 208, loss = 0.4206083416938782
iteration 209, loss = 0.41799670457839966
iteration 210, loss = 0.4172411859035492
iteration 211, loss = 0.42368054389953613
iteration 212, loss = 0.4175151586532593
iteration 213, loss = 0.4152439534664154
iteration 214, loss = 0.4310329556465149
iteration 215, loss = 0.4325730800628662
iteration 216, loss = 0.42735594511032104
iteration 217, loss = 0.43947356939315796
iteration 218, loss = 0.43081408739089966
iteration 219, loss = 0.4299795627593994
iteration 220, loss = 0.4254065155982971
iteration 221, loss = 0.434148907661438
iteration 222, loss = 0.42413535714149475
iteration 223, loss = 0.41867923736572266
iteration 224, loss = 0.4231806993484497
iteration 225, loss = 0.43153712153434753
iteration 226, loss = 0.4310973286628723
iteration 227, loss = 0.4244447946548462
iteration 228, loss = 0.4326704740524292
iteration 229, loss = 0.42495691776275635
iteration 230, loss = 0.42574551701545715
iteration 231, loss = 0.4389444589614868
iteration 232, loss = 0.42992833256721497
iteration 233, loss = 0.40786033868789673
iteration 234, loss = 0.4345412254333496
iteration 235, loss = 0.43550780415534973
iteration 236, loss = 0.4180300831794739
iteration 237, loss = 0.425905704498291
iteration 238, loss = 0.4297860264778137
iteration 239, loss = 0.4253515601158142
iteration 240, loss = 0.42291444540023804
iteration 241, loss = 0.4037153124809265
iteration 242, loss = 0.41448625922203064
iteration 243, loss = 0.43424689769744873
iteration 244, loss = 0.40766799449920654
iteration 245, loss = 0.4249228239059448
iteration 246, loss = 0.4171592593193054
iteration 247, loss = 0.41600149869918823
iteration 248, loss = 0.4166736602783203
iteration 249, loss = 0.4236859381198883
iteration 250, loss = 0.4289324879646301
iteration 251, loss = 0.4290902018547058
iteration 252, loss = 0.433759480714798
iteration 253, loss = 0.42811957001686096
iteration 254, loss = 0.42135271430015564
iteration 255, loss = 0.4220535159111023
iteration 256, loss = 0.4094173312187195
iteration 257, loss = 0.4093770384788513
iteration 258, loss = 0.4315202832221985
iteration 259, loss = 0.4278656244277954
iteration 260, loss = 0.40848177671432495
iteration 261, loss = 0.41997188329696655
iteration 262, loss = 0.41033482551574707
iteration 263, loss = 0.40628790855407715
iteration 264, loss = 0.4168226718902588
iteration 265, loss = 0.4175507128238678
iteration 266, loss = 0.4251168668270111
iteration 267, loss = 0.4132024645805359
iteration 268, loss = 0.4264526963233948
iteration 269, loss = 0.42806723713874817
iteration 270, loss = 0.41120412945747375
iteration 271, loss = 0.42866402864456177
iteration 272, loss = 0.4217455983161926
iteration 273, loss = 0.4150717854499817
iteration 274, loss = 0.43381136655807495
iteration 275, loss = 0.4084283411502838
iteration 276, loss = 0.4267314076423645
iteration 277, loss = 0.4262794852256775
iteration 278, loss = 0.41691315174102783
iteration 279, loss = 0.41453275084495544
iteration 280, loss = 0.4103134870529175
iteration 281, loss = 0.42591965198516846
iteration 282, loss = 0.41085052490234375
iteration 283, loss = 0.4098113179206848
iteration 284, loss = 0.40166324377059937
iteration 285, loss = 0.4255560040473938
iteration 286, loss = 0.42536163330078125
iteration 287, loss = 0.4176464378833771
iteration 288, loss = 0.42820072174072266
iteration 289, loss = 0.43928056955337524
iteration 290, loss = 0.40322190523147583
iteration 291, loss = 0.4059939980506897
iteration 292, loss = 0.4131714403629303
iteration 293, loss = 0.3990367650985718
iteration 294, loss = 0.4092753827571869
iteration 295, loss = 0.40071630477905273
iteration 296, loss = 0.4074290990829468
iteration 297, loss = 0.41895145177841187
iteration 298, loss = 0.42801111936569214
iteration 299, loss = 0.4035741686820984
iteration 0, loss = 0.4172621965408325
iteration 1, loss = 0.42578643560409546
iteration 2, loss = 0.42358535528182983
iteration 3, loss = 0.4117555022239685
iteration 4, loss = 0.3979547619819641
iteration 5, loss = 0.4048524498939514
iteration 6, loss = 0.4170810580253601
iteration 7, loss = 0.40517908334732056
iteration 8, loss = 0.4372110366821289
iteration 9, loss = 0.39711302518844604
iteration 10, loss = 0.40465623140335083
iteration 11, loss = 0.416874498128891
iteration 12, loss = 0.4299740791320801
iteration 13, loss = 0.4044225811958313
iteration 14, loss = 0.41233745217323303
iteration 15, loss = 0.43112295866012573
iteration 16, loss = 0.39965498447418213
iteration 17, loss = 0.41937869787216187
iteration 18, loss = 0.4082537293434143
iteration 19, loss = 0.41623181104660034
iteration 20, loss = 0.4282705783843994
iteration 21, loss = 0.4192661643028259
iteration 22, loss = 0.4164890646934509
iteration 23, loss = 0.4086076617240906
iteration 24, loss = 0.41636359691619873
iteration 25, loss = 0.3989788889884949
iteration 26, loss = 0.43122777342796326
iteration 27, loss = 0.4057554006576538
iteration 28, loss = 0.416265606880188
iteration 29, loss = 0.4008941054344177
iteration 30, loss = 0.40650051832199097
iteration 31, loss = 0.40991121530532837
iteration 32, loss = 0.3925352096557617
iteration 33, loss = 0.40167009830474854
iteration 34, loss = 0.3976348042488098
iteration 35, loss = 0.40731701254844666
iteration 36, loss = 0.4197639226913452
iteration 37, loss = 0.40402698516845703
iteration 38, loss = 0.4224857687950134
iteration 39, loss = 0.4078078269958496
iteration 40, loss = 0.4050857722759247
iteration 41, loss = 0.41395485401153564
iteration 42, loss = 0.4356730282306671
iteration 43, loss = 0.4132496118545532
iteration 44, loss = 0.41251885890960693
iteration 45, loss = 0.40532153844833374
iteration 46, loss = 0.389636754989624
iteration 47, loss = 0.4014570415019989
iteration 48, loss = 0.39159876108169556
iteration 49, loss = 0.3986290693283081
iteration 50, loss = 0.42380204796791077
iteration 51, loss = 0.4161902964115143
iteration 52, loss = 0.403564453125
iteration 53, loss = 0.39485424757003784
iteration 54, loss = 0.40384742617607117
iteration 55, loss = 0.40203654766082764
iteration 56, loss = 0.417785108089447
iteration 57, loss = 0.3910619020462036
iteration 58, loss = 0.4044632017612457
iteration 59, loss = 0.39278674125671387
iteration 60, loss = 0.4076313376426697
iteration 61, loss = 0.40804654359817505
iteration 62, loss = 0.4182920753955841
iteration 63, loss = 0.39837032556533813
iteration 64, loss = 0.3927915096282959
iteration 65, loss = 0.3986223638057709
iteration 66, loss = 0.40918779373168945
iteration 67, loss = 0.4366943836212158
iteration 68, loss = 0.40199023485183716
iteration 69, loss = 0.39634355902671814
iteration 70, loss = 0.41761457920074463
iteration 71, loss = 0.4016726315021515
iteration 72, loss = 0.38922762870788574
iteration 73, loss = 0.3944148123264313
iteration 74, loss = 0.395469069480896
iteration 75, loss = 0.41429662704467773
iteration 76, loss = 0.4153267741203308
iteration 77, loss = 0.4050635099411011
iteration 78, loss = 0.39697879552841187
iteration 79, loss = 0.40325474739074707
iteration 80, loss = 0.40972694754600525
iteration 81, loss = 0.39080488681793213
iteration 82, loss = 0.3952500820159912
iteration 83, loss = 0.418222039937973
iteration 84, loss = 0.4002859592437744
iteration 85, loss = 0.40169262886047363
iteration 86, loss = 0.3937305510044098
iteration 87, loss = 0.38824015855789185
iteration 88, loss = 0.4068102538585663
iteration 89, loss = 0.40993446111679077
iteration 90, loss = 0.39519593119621277
iteration 91, loss = 0.42075252532958984
iteration 92, loss = 0.41756027936935425
iteration 93, loss = 0.3874703049659729
iteration 94, loss = 0.41578221321105957
iteration 95, loss = 0.38730359077453613
iteration 96, loss = 0.38786065578460693
iteration 97, loss = 0.43024250864982605
iteration 98, loss = 0.40997546911239624
iteration 99, loss = 0.4008941352367401
iteration 100, loss = 0.4212740361690521
iteration 101, loss = 0.4047093987464905
iteration 102, loss = 0.4098687171936035
iteration 103, loss = 0.41841328144073486
iteration 104, loss = 0.39803436398506165
iteration 105, loss = 0.3934534192085266
iteration 106, loss = 0.38494163751602173
iteration 107, loss = 0.41263720393180847
iteration 108, loss = 0.39485684037208557
iteration 109, loss = 0.3863464593887329
iteration 110, loss = 0.3984469473361969
iteration 111, loss = 0.38470569252967834
iteration 112, loss = 0.3891295790672302
iteration 113, loss = 0.39675089716911316
iteration 114, loss = 0.39158639311790466
iteration 115, loss = 0.4242527484893799
iteration 116, loss = 0.387190580368042
iteration 117, loss = 0.39224934577941895
iteration 118, loss = 0.39525580406188965
iteration 119, loss = 0.40181583166122437
iteration 120, loss = 0.4254997968673706
iteration 121, loss = 0.42243415117263794
iteration 122, loss = 0.39643776416778564
iteration 123, loss = 0.40672555565834045
iteration 124, loss = 0.3950968384742737
iteration 125, loss = 0.40093544125556946
iteration 126, loss = 0.39542508125305176
iteration 127, loss = 0.39374008774757385
iteration 128, loss = 0.4188857078552246
iteration 129, loss = 0.4037960171699524
iteration 130, loss = 0.38923919200897217
iteration 131, loss = 0.3855776786804199
iteration 132, loss = 0.38783952593803406
iteration 133, loss = 0.38860467076301575
iteration 134, loss = 0.40215808153152466
iteration 135, loss = 0.3738473653793335
iteration 136, loss = 0.3947731852531433
iteration 137, loss = 0.38372287154197693
iteration 138, loss = 0.3961547315120697
iteration 139, loss = 0.3972824215888977
iteration 140, loss = 0.3895540237426758
iteration 141, loss = 0.39679720997810364
iteration 142, loss = 0.4076821208000183
iteration 143, loss = 0.404805988073349
iteration 144, loss = 0.40541237592697144
iteration 145, loss = 0.40204906463623047
iteration 146, loss = 0.4018242061138153
iteration 147, loss = 0.38719063997268677
iteration 148, loss = 0.3772265315055847
iteration 149, loss = 0.4103306829929352
iteration 150, loss = 0.40351593494415283
iteration 151, loss = 0.406226247549057
iteration 152, loss = 0.40424787998199463
iteration 153, loss = 0.40330934524536133
iteration 154, loss = 0.3888673484325409
iteration 155, loss = 0.4248012602329254
iteration 156, loss = 0.40381231904029846
iteration 157, loss = 0.39050939679145813
iteration 158, loss = 0.38681817054748535
iteration 159, loss = 0.41122502088546753
iteration 160, loss = 0.4093310832977295
iteration 161, loss = 0.39196255803108215
iteration 162, loss = 0.4003191590309143
iteration 163, loss = 0.38409656286239624
iteration 164, loss = 0.39570799469947815
iteration 165, loss = 0.4051513671875
iteration 166, loss = 0.39258426427841187
iteration 167, loss = 0.4211748242378235
iteration 168, loss = 0.3734249770641327
iteration 169, loss = 0.38951176404953003
iteration 170, loss = 0.3990604877471924
iteration 171, loss = 0.38141170144081116
iteration 172, loss = 0.40318942070007324
iteration 173, loss = 0.3812081813812256
iteration 174, loss = 0.3950727581977844
iteration 175, loss = 0.4020107686519623
iteration 176, loss = 0.3993263840675354
iteration 177, loss = 0.3834472596645355
iteration 178, loss = 0.40225547552108765
iteration 179, loss = 0.3941139578819275
iteration 180, loss = 0.3998238444328308
iteration 181, loss = 0.3771069347858429
iteration 182, loss = 0.3765835165977478
iteration 183, loss = 0.39649850130081177
iteration 184, loss = 0.3756342828273773
iteration 185, loss = 0.3784107565879822
iteration 186, loss = 0.38454580307006836
iteration 187, loss = 0.3908081650733948
iteration 188, loss = 0.39457792043685913
iteration 189, loss = 0.39491868019104004
iteration 190, loss = 0.3967854380607605
iteration 191, loss = 0.3789724111557007
iteration 192, loss = 0.388821542263031
iteration 193, loss = 0.39514458179473877
iteration 194, loss = 0.4065675139427185
iteration 195, loss = 0.3939586281776428
iteration 196, loss = 0.385794997215271
iteration 197, loss = 0.39095085859298706
iteration 198, loss = 0.41992655396461487
iteration 199, loss = 0.387795627117157
iteration 200, loss = 0.39727723598480225
iteration 201, loss = 0.3874720335006714
iteration 202, loss = 0.40271827578544617
iteration 203, loss = 0.37912485003471375
iteration 204, loss = 0.3768582344055176
iteration 205, loss = 0.3917515277862549
iteration 206, loss = 0.37820619344711304
iteration 207, loss = 0.4047396779060364
iteration 208, loss = 0.39859649538993835
iteration 209, loss = 0.37587836384773254
iteration 210, loss = 0.3918629586696625
iteration 211, loss = 0.367559015750885
iteration 212, loss = 0.39697134494781494
iteration 213, loss = 0.3823743462562561
iteration 214, loss = 0.3867456912994385
iteration 215, loss = 0.3863903880119324
iteration 216, loss = 0.36971449851989746
iteration 217, loss = 0.37380683422088623
iteration 218, loss = 0.4144272208213806
iteration 219, loss = 0.3747124969959259
iteration 220, loss = 0.3754689395427704
iteration 221, loss = 0.376542866230011
iteration 222, loss = 0.38345128297805786
iteration 223, loss = 0.399122953414917
iteration 224, loss = 0.39982569217681885
iteration 225, loss = 0.38491642475128174
iteration 226, loss = 0.3871843218803406
iteration 227, loss = 0.39319801330566406
iteration 228, loss = 0.3696880042552948
iteration 229, loss = 0.3916666805744171
iteration 230, loss = 0.39022114872932434
iteration 231, loss = 0.37771135568618774
iteration 232, loss = 0.38430362939834595
iteration 233, loss = 0.38716232776641846
iteration 234, loss = 0.3882938325405121
iteration 235, loss = 0.3935558795928955
iteration 236, loss = 0.3954707384109497
iteration 237, loss = 0.3877754211425781
iteration 238, loss = 0.38703837990760803
iteration 239, loss = 0.4002808630466461
iteration 240, loss = 0.39087414741516113
iteration 241, loss = 0.4053151607513428
iteration 242, loss = 0.39204269647598267
iteration 243, loss = 0.3844529390335083
iteration 244, loss = 0.3956429064273834
iteration 245, loss = 0.3831029534339905
iteration 246, loss = 0.3980150818824768
iteration 247, loss = 0.36259549856185913
iteration 248, loss = 0.403067946434021
iteration 249, loss = 0.37862929701805115
iteration 250, loss = 0.4026831388473511
iteration 251, loss = 0.3836904764175415
iteration 252, loss = 0.3899492025375366
iteration 253, loss = 0.39506596326828003
iteration 254, loss = 0.3581887483596802
iteration 255, loss = 0.383125901222229
iteration 256, loss = 0.39558374881744385
iteration 257, loss = 0.37120652198791504
iteration 258, loss = 0.4254333972930908
iteration 259, loss = 0.37484848499298096
iteration 260, loss = 0.40529876947402954
iteration 261, loss = 0.40269604325294495
iteration 262, loss = 0.3797992467880249
iteration 263, loss = 0.383856862783432
iteration 264, loss = 0.3839936852455139
iteration 265, loss = 0.3991612195968628
iteration 266, loss = 0.38039177656173706
iteration 267, loss = 0.3746991753578186
iteration 268, loss = 0.38596436381340027
iteration 269, loss = 0.3940696120262146
iteration 270, loss = 0.390277236700058
iteration 271, loss = 0.40001994371414185
iteration 272, loss = 0.4019487500190735
iteration 273, loss = 0.3792027235031128
iteration 274, loss = 0.37366431951522827
iteration 275, loss = 0.39428800344467163
iteration 276, loss = 0.37147650122642517
iteration 277, loss = 0.363655149936676
iteration 278, loss = 0.39797264337539673
iteration 279, loss = 0.37991034984588623
iteration 280, loss = 0.3947948217391968
iteration 281, loss = 0.3607255816459656
iteration 282, loss = 0.3919129967689514
iteration 283, loss = 0.373330295085907
iteration 284, loss = 0.37951141595840454
iteration 285, loss = 0.3956073224544525
iteration 286, loss = 0.3974912166595459
iteration 287, loss = 0.38774430751800537
iteration 288, loss = 0.37468722462654114
iteration 289, loss = 0.35877829790115356
iteration 290, loss = 0.3941314220428467
iteration 291, loss = 0.37510502338409424
iteration 292, loss = 0.36402249336242676
iteration 293, loss = 0.38642948865890503
iteration 294, loss = 0.35950613021850586
iteration 295, loss = 0.38461729884147644
iteration 296, loss = 0.365314245223999
iteration 297, loss = 0.3874240517616272
iteration 298, loss = 0.37548017501831055
iteration 299, loss = 0.3718263506889343
iteration 0, loss = 0.37713831663131714
iteration 1, loss = 0.3820040225982666
iteration 2, loss = 0.36428409814834595
iteration 3, loss = 0.38421955704689026
iteration 4, loss = 0.37605714797973633
iteration 5, loss = 0.35953667759895325
iteration 6, loss = 0.36640650033950806
iteration 7, loss = 0.3715347647666931
iteration 8, loss = 0.3809642493724823
iteration 9, loss = 0.38604000210762024
iteration 10, loss = 0.35251373052597046
iteration 11, loss = 0.35122013092041016
iteration 12, loss = 0.3752218782901764
iteration 13, loss = 0.36645281314849854
iteration 14, loss = 0.39245009422302246
iteration 15, loss = 0.39128270745277405
iteration 16, loss = 0.35721632838249207
iteration 17, loss = 0.36188215017318726
iteration 18, loss = 0.37206703424453735
iteration 19, loss = 0.373932421207428
iteration 20, loss = 0.38132965564727783
iteration 21, loss = 0.4130232334136963
iteration 22, loss = 0.3655366897583008
iteration 23, loss = 0.3692576289176941
iteration 24, loss = 0.3774227201938629
iteration 25, loss = 0.37952303886413574
iteration 26, loss = 0.37661486864089966
iteration 27, loss = 0.38421306014060974
iteration 28, loss = 0.3570508658885956
iteration 29, loss = 0.3957388997077942
iteration 30, loss = 0.3595286011695862
iteration 31, loss = 0.36234918236732483
iteration 32, loss = 0.3819861114025116
iteration 33, loss = 0.3797571063041687
iteration 34, loss = 0.3765397071838379
iteration 35, loss = 0.39519959688186646
iteration 36, loss = 0.38210439682006836
iteration 37, loss = 0.38976043462753296
iteration 38, loss = 0.3757677376270294
iteration 39, loss = 0.38704419136047363
iteration 40, loss = 0.38499900698661804
iteration 41, loss = 0.3628441095352173
iteration 42, loss = 0.3525163233280182
iteration 43, loss = 0.404357373714447
iteration 44, loss = 0.3699650764465332
iteration 45, loss = 0.3662986159324646
iteration 46, loss = 0.38078606128692627
iteration 47, loss = 0.3557283878326416
iteration 48, loss = 0.374670147895813
iteration 49, loss = 0.3777875304222107
iteration 50, loss = 0.36619627475738525
iteration 51, loss = 0.343356192111969
iteration 52, loss = 0.39094263315200806
iteration 53, loss = 0.36318016052246094
iteration 54, loss = 0.3908039331436157
iteration 55, loss = 0.3568786084651947
iteration 56, loss = 0.3885261118412018
iteration 57, loss = 0.36212700605392456
iteration 58, loss = 0.37237638235092163
iteration 59, loss = 0.3863285183906555
iteration 60, loss = 0.3842690587043762
iteration 61, loss = 0.37373095750808716
iteration 62, loss = 0.362135648727417
iteration 63, loss = 0.37017887830734253
iteration 64, loss = 0.345232218503952
iteration 65, loss = 0.35603848099708557
iteration 66, loss = 0.35312578082084656
iteration 67, loss = 0.3822248876094818
iteration 68, loss = 0.3752862215042114
iteration 69, loss = 0.358866810798645
iteration 70, loss = 0.37272754311561584
iteration 71, loss = 0.36648446321487427
iteration 72, loss = 0.3505633771419525
iteration 73, loss = 0.3686889410018921
iteration 74, loss = 0.3627484440803528
iteration 75, loss = 0.3747204542160034
iteration 76, loss = 0.3619120121002197
iteration 77, loss = 0.3739793300628662
iteration 78, loss = 0.3505150377750397
iteration 79, loss = 0.3922085762023926
iteration 80, loss = 0.3545348644256592
iteration 81, loss = 0.36266738176345825
iteration 82, loss = 0.36703988909721375
iteration 83, loss = 0.34434744715690613
iteration 84, loss = 0.36629846692085266
iteration 85, loss = 0.36294490098953247
iteration 86, loss = 0.36792829632759094
iteration 87, loss = 0.36307787895202637
iteration 88, loss = 0.3649884760379791
iteration 89, loss = 0.38131770491600037
iteration 90, loss = 0.34742873907089233
iteration 91, loss = 0.3733052611351013
iteration 92, loss = 0.3642759323120117
iteration 93, loss = 0.37280523777008057
iteration 94, loss = 0.3615511655807495
iteration 95, loss = 0.39878153800964355
iteration 96, loss = 0.40425756573677063
iteration 97, loss = 0.39577555656433105
iteration 98, loss = 0.3839190900325775
iteration 99, loss = 0.3421127200126648
iteration 100, loss = 0.3743981719017029
iteration 101, loss = 0.3687485158443451
iteration 102, loss = 0.3884877562522888
iteration 103, loss = 0.34711313247680664
iteration 104, loss = 0.3384896516799927
iteration 105, loss = 0.3429962396621704
iteration 106, loss = 0.3694468140602112
iteration 107, loss = 0.35571011900901794
iteration 108, loss = 0.36765724420547485
iteration 109, loss = 0.34489989280700684
iteration 110, loss = 0.36443498730659485
iteration 111, loss = 0.3354867100715637
iteration 112, loss = 0.36691415309906006
iteration 113, loss = 0.3514440655708313
iteration 114, loss = 0.3427908420562744
iteration 115, loss = 0.3889565169811249
iteration 116, loss = 0.35835832357406616
iteration 117, loss = 0.3576653003692627
iteration 118, loss = 0.37406718730926514
iteration 119, loss = 0.3898061513900757
iteration 120, loss = 0.3559938371181488
iteration 121, loss = 0.3651338219642639
iteration 122, loss = 0.36272671818733215
iteration 123, loss = 0.3691132962703705
iteration 124, loss = 0.36814141273498535
iteration 125, loss = 0.37852540612220764
iteration 126, loss = 0.3736203908920288
iteration 127, loss = 0.34897705912590027
iteration 128, loss = 0.34025678038597107
iteration 129, loss = 0.34089505672454834
iteration 130, loss = 0.3658592700958252
iteration 131, loss = 0.3777165412902832
iteration 132, loss = 0.3484928607940674
iteration 133, loss = 0.3436296582221985
iteration 134, loss = 0.3624398708343506
iteration 135, loss = 0.3533782660961151
iteration 136, loss = 0.3499704897403717
iteration 137, loss = 0.36102768778800964
iteration 138, loss = 0.3497745990753174
iteration 139, loss = 0.3723451495170593
iteration 140, loss = 0.3724273443222046
iteration 141, loss = 0.33144819736480713
iteration 142, loss = 0.3819296956062317
iteration 143, loss = 0.35855621099472046
iteration 144, loss = 0.3528346121311188
iteration 145, loss = 0.36248648166656494
iteration 146, loss = 0.3486745357513428
iteration 147, loss = 0.3571937084197998
iteration 148, loss = 0.35250264406204224
iteration 149, loss = 0.3509279489517212
iteration 150, loss = 0.35429275035858154
iteration 151, loss = 0.3770645260810852
iteration 152, loss = 0.37117207050323486
iteration 153, loss = 0.34473860263824463
iteration 154, loss = 0.3415590226650238
iteration 155, loss = 0.33526670932769775
iteration 156, loss = 0.3449662923812866
iteration 157, loss = 0.34607046842575073
iteration 158, loss = 0.3538993000984192
iteration 159, loss = 0.3531590700149536
iteration 160, loss = 0.3535023331642151
iteration 161, loss = 0.36085379123687744
iteration 162, loss = 0.34348028898239136
iteration 163, loss = 0.33670827746391296
iteration 164, loss = 0.32947874069213867
iteration 165, loss = 0.361491322517395
iteration 166, loss = 0.3571067750453949
iteration 167, loss = 0.35040467977523804
iteration 168, loss = 0.3630358576774597
iteration 169, loss = 0.35566240549087524
iteration 170, loss = 0.3504149913787842
iteration 171, loss = 0.3717823326587677
iteration 172, loss = 0.3513261079788208
iteration 173, loss = 0.3536731004714966
iteration 174, loss = 0.36539226770401
iteration 175, loss = 0.3487761616706848
iteration 176, loss = 0.38278424739837646
iteration 177, loss = 0.3495931625366211
iteration 178, loss = 0.3494865298271179
iteration 179, loss = 0.3429851830005646
iteration 180, loss = 0.35804247856140137
iteration 181, loss = 0.3253270387649536
iteration 182, loss = 0.3119475841522217
iteration 183, loss = 0.34237414598464966
iteration 184, loss = 0.3394262194633484
iteration 185, loss = 0.3253132104873657
iteration 186, loss = 0.36324548721313477
iteration 187, loss = 0.33553367853164673
iteration 188, loss = 0.3335440158843994
iteration 189, loss = 0.3406343460083008
iteration 190, loss = 0.34159910678863525
iteration 191, loss = 0.34843990206718445
iteration 192, loss = 0.3287816047668457
iteration 193, loss = 0.3646800220012665
iteration 194, loss = 0.3393059968948364
iteration 195, loss = 0.33284544944763184
iteration 196, loss = 0.3771243095397949
iteration 197, loss = 0.3404308557510376
iteration 198, loss = 0.34744784235954285
iteration 199, loss = 0.3723035156726837
iteration 200, loss = 0.3700084686279297
iteration 201, loss = 0.32894474267959595
iteration 202, loss = 0.3455466032028198
iteration 203, loss = 0.3562668263912201
iteration 204, loss = 0.3665198087692261
iteration 205, loss = 0.33031487464904785
iteration 206, loss = 0.3395591080188751
iteration 207, loss = 0.33756476640701294
iteration 208, loss = 0.34407180547714233
iteration 209, loss = 0.3589942455291748
iteration 210, loss = 0.34367620944976807
iteration 211, loss = 0.3368817865848541
iteration 212, loss = 0.349260151386261
iteration 213, loss = 0.3210833668708801
iteration 214, loss = 0.373299777507782
iteration 215, loss = 0.35892775654792786
iteration 216, loss = 0.3368294835090637
iteration 217, loss = 0.34025782346725464
iteration 218, loss = 0.3786213994026184
iteration 219, loss = 0.3446682393550873
iteration 220, loss = 0.35782450437545776
iteration 221, loss = 0.3634844422340393
iteration 222, loss = 0.3385172486305237
iteration 223, loss = 0.33458301424980164
iteration 224, loss = 0.3638666272163391
iteration 225, loss = 0.34377580881118774
iteration 226, loss = 0.3338463008403778
iteration 227, loss = 0.3553524315357208
iteration 228, loss = 0.31864383816719055
iteration 229, loss = 0.35564279556274414
iteration 230, loss = 0.3285689651966095
iteration 231, loss = 0.3605528771877289
iteration 232, loss = 0.35499975085258484
iteration 233, loss = 0.3468671441078186
iteration 234, loss = 0.3439761996269226
iteration 235, loss = 0.32012876868247986
iteration 236, loss = 0.36004912853240967
iteration 237, loss = 0.33429691195487976
iteration 238, loss = 0.3257453441619873
iteration 239, loss = 0.3060314655303955
iteration 240, loss = 0.3210618495941162
iteration 241, loss = 0.37565889954566956
iteration 242, loss = 0.34402996301651
iteration 243, loss = 0.3582969903945923
iteration 244, loss = 0.3328101634979248
iteration 245, loss = 0.34967687726020813
iteration 246, loss = 0.32420408725738525
iteration 247, loss = 0.3334319591522217
iteration 248, loss = 0.33290061354637146
iteration 249, loss = 0.34104225039482117
iteration 250, loss = 0.3365200161933899
iteration 251, loss = 0.35045668482780457
iteration 252, loss = 0.33297842741012573
iteration 253, loss = 0.36733096837997437
iteration 254, loss = 0.3287588953971863
iteration 255, loss = 0.37032270431518555
iteration 256, loss = 0.3146938681602478
iteration 257, loss = 0.3461463451385498
iteration 258, loss = 0.3188989758491516
iteration 259, loss = 0.32790789008140564
iteration 260, loss = 0.36199843883514404
iteration 261, loss = 0.34587472677230835
iteration 262, loss = 0.3467940092086792
iteration 263, loss = 0.3254053294658661
iteration 264, loss = 0.330346018075943
iteration 265, loss = 0.3379957377910614
iteration 266, loss = 0.31779274344444275
iteration 267, loss = 0.32361388206481934
iteration 268, loss = 0.3593970537185669
iteration 269, loss = 0.3610987067222595
iteration 270, loss = 0.3741874098777771
iteration 271, loss = 0.3485632836818695
iteration 272, loss = 0.31945309042930603
iteration 273, loss = 0.3582009971141815
iteration 274, loss = 0.3352372348308563
iteration 275, loss = 0.3166719079017639
iteration 276, loss = 0.3340465724468231
iteration 277, loss = 0.3189668655395508
iteration 278, loss = 0.3248707950115204
iteration 279, loss = 0.3280787765979767
iteration 280, loss = 0.34932875633239746
iteration 281, loss = 0.3427678346633911
iteration 282, loss = 0.3269728720188141
iteration 283, loss = 0.32005569338798523
iteration 284, loss = 0.30997908115386963
iteration 285, loss = 0.3136800527572632
iteration 286, loss = 0.31957387924194336
iteration 287, loss = 0.36068597435951233
iteration 288, loss = 0.32806044816970825
iteration 289, loss = 0.3593430519104004
iteration 290, loss = 0.34939044713974
iteration 291, loss = 0.323623925447464
iteration 292, loss = 0.3124600350856781
iteration 293, loss = 0.3587148189544678
iteration 294, loss = 0.34284093976020813
iteration 295, loss = 0.3744543790817261
iteration 296, loss = 0.30908578634262085
iteration 297, loss = 0.3195035755634308
iteration 298, loss = 0.3492890000343323
iteration 299, loss = 0.32584846019744873
iteration 0, loss = 0.3398165702819824
iteration 1, loss = 0.33307963609695435
iteration 2, loss = 0.3172926604747772
iteration 3, loss = 0.31767216324806213
iteration 4, loss = 0.35829657316207886
iteration 5, loss = 0.358445942401886
iteration 6, loss = 0.33012640476226807
iteration 7, loss = 0.32495179772377014
iteration 8, loss = 0.35039979219436646
iteration 9, loss = 0.3247118890285492
iteration 10, loss = 0.33980458974838257
iteration 11, loss = 0.325432151556015
iteration 12, loss = 0.3436077833175659
iteration 13, loss = 0.36971479654312134
iteration 14, loss = 0.32020312547683716
iteration 15, loss = 0.32157444953918457
iteration 16, loss = 0.3303336501121521
iteration 17, loss = 0.3231149911880493
iteration 18, loss = 0.36439698934555054
iteration 19, loss = 0.34797194600105286
iteration 20, loss = 0.30409812927246094
iteration 21, loss = 0.3369729518890381
iteration 22, loss = 0.31124094128608704
iteration 23, loss = 0.3596217930316925
iteration 24, loss = 0.3226223587989807
iteration 25, loss = 0.35825714468955994
iteration 26, loss = 0.3504304587841034
iteration 27, loss = 0.3174588084220886
iteration 28, loss = 0.30950385332107544
iteration 29, loss = 0.31082338094711304
iteration 30, loss = 0.33212536573410034
iteration 31, loss = 0.31453144550323486
iteration 32, loss = 0.32097679376602173
iteration 33, loss = 0.3454717993736267
iteration 34, loss = 0.3185800611972809
iteration 35, loss = 0.3417893350124359
iteration 36, loss = 0.3440319895744324
iteration 37, loss = 0.35261037945747375
iteration 38, loss = 0.3363208472728729
iteration 39, loss = 0.3532123565673828
iteration 40, loss = 0.3236772418022156
iteration 41, loss = 0.34441664814949036
iteration 42, loss = 0.2897467613220215
iteration 43, loss = 0.3083670139312744
iteration 44, loss = 0.3331810235977173
iteration 45, loss = 0.31802037358283997
iteration 46, loss = 0.3006058633327484
iteration 47, loss = 0.3335825204849243
iteration 48, loss = 0.3171931207180023
iteration 49, loss = 0.33245205879211426
iteration 50, loss = 0.3134137690067291
iteration 51, loss = 0.3205735981464386
iteration 52, loss = 0.3205992579460144
iteration 53, loss = 0.3223625421524048
iteration 54, loss = 0.3658420145511627
iteration 55, loss = 0.3073175549507141
iteration 56, loss = 0.35457727313041687
iteration 57, loss = 0.33781078457832336
iteration 58, loss = 0.31647130846977234
iteration 59, loss = 0.3008306622505188
iteration 60, loss = 0.3142988085746765
iteration 61, loss = 0.34103846549987793
iteration 62, loss = 0.30270859599113464
iteration 63, loss = 0.32151612639427185
iteration 64, loss = 0.3195822834968567
iteration 65, loss = 0.28839540481567383
iteration 66, loss = 0.3260619044303894
iteration 67, loss = 0.31840428709983826
iteration 68, loss = 0.3240138292312622
iteration 69, loss = 0.33420106768608093
iteration 70, loss = 0.31732606887817383
iteration 71, loss = 0.32731813192367554
iteration 72, loss = 0.3479066491127014
iteration 73, loss = 0.3161320686340332
iteration 74, loss = 0.31731417775154114
iteration 75, loss = 0.31528204679489136
iteration 76, loss = 0.3288987874984741
iteration 77, loss = 0.3036770224571228
iteration 78, loss = 0.30857425928115845
iteration 79, loss = 0.29447516798973083
iteration 80, loss = 0.3221330940723419
iteration 81, loss = 0.3277352750301361
iteration 82, loss = 0.34567123651504517
iteration 83, loss = 0.2926618754863739
iteration 84, loss = 0.32466959953308105
iteration 85, loss = 0.3408076763153076
iteration 86, loss = 0.3117617070674896
iteration 87, loss = 0.33030158281326294
iteration 88, loss = 0.3367910087108612
iteration 89, loss = 0.30489999055862427
iteration 90, loss = 0.3508283495903015
iteration 91, loss = 0.34287822246551514
iteration 92, loss = 0.2938190698623657
iteration 93, loss = 0.3218151330947876
iteration 94, loss = 0.3028416633605957
iteration 95, loss = 0.2996289134025574
iteration 96, loss = 0.28137022256851196
iteration 97, loss = 0.30868881940841675
iteration 98, loss = 0.32353949546813965
iteration 99, loss = 0.30221086740493774
iteration 100, loss = 0.3276646137237549
iteration 101, loss = 0.31683212518692017
iteration 102, loss = 0.3395555019378662
iteration 103, loss = 0.33726316690444946
iteration 104, loss = 0.3454696834087372
iteration 105, loss = 0.3261213004589081
iteration 106, loss = 0.3301117420196533
iteration 107, loss = 0.3398606777191162
iteration 108, loss = 0.303345263004303
iteration 109, loss = 0.30835944414138794
iteration 110, loss = 0.3230295181274414
iteration 111, loss = 0.323943555355072
iteration 112, loss = 0.32479724287986755
iteration 113, loss = 0.3052780032157898
iteration 114, loss = 0.3268025815486908
iteration 115, loss = 0.3053640127182007
iteration 116, loss = 0.3351966142654419
iteration 117, loss = 0.3288019299507141
iteration 118, loss = 0.34092772006988525
iteration 119, loss = 0.2977660894393921
iteration 120, loss = 0.3044534921646118
iteration 121, loss = 0.3100614547729492
iteration 122, loss = 0.3490166664123535
iteration 123, loss = 0.2867574095726013
iteration 124, loss = 0.31862813234329224
iteration 125, loss = 0.29477187991142273
iteration 126, loss = 0.31057775020599365
iteration 127, loss = 0.30523616075515747
iteration 128, loss = 0.30628764629364014
iteration 129, loss = 0.29234278202056885
iteration 130, loss = 0.290254145860672
iteration 131, loss = 0.3079756498336792
iteration 132, loss = 0.34323757886886597
iteration 133, loss = 0.3344106674194336
iteration 134, loss = 0.3045000433921814
iteration 135, loss = 0.35259464383125305
iteration 136, loss = 0.3209984600543976
iteration 137, loss = 0.3335205018520355
iteration 138, loss = 0.2977144718170166
iteration 139, loss = 0.32117897272109985
iteration 140, loss = 0.2781546711921692
iteration 141, loss = 0.30616289377212524
iteration 142, loss = 0.3048592507839203
iteration 143, loss = 0.30639058351516724
iteration 144, loss = 0.3089223802089691
iteration 145, loss = 0.3237360417842865
iteration 146, loss = 0.2921779453754425
iteration 147, loss = 0.3260636031627655
iteration 148, loss = 0.33011046051979065
iteration 149, loss = 0.33171284198760986
iteration 150, loss = 0.29118162393569946
iteration 151, loss = 0.31080305576324463
iteration 152, loss = 0.32610777020454407
iteration 153, loss = 0.2839052379131317
iteration 154, loss = 0.29211628437042236
iteration 155, loss = 0.31048253178596497
iteration 156, loss = 0.2924540042877197
iteration 157, loss = 0.2885284423828125
iteration 158, loss = 0.2893564999103546
iteration 159, loss = 0.31641796231269836
iteration 160, loss = 0.2898593246936798
iteration 161, loss = 0.295360267162323
iteration 162, loss = 0.29873862862586975
iteration 163, loss = 0.3370249569416046
iteration 164, loss = 0.356145977973938
iteration 165, loss = 0.2975521683692932
iteration 166, loss = 0.2930486798286438
iteration 167, loss = 0.3157532215118408
iteration 168, loss = 0.29339951276779175
iteration 169, loss = 0.30646973848342896
iteration 170, loss = 0.32967621088027954
iteration 171, loss = 0.30910179018974304
iteration 172, loss = 0.2961091101169586
iteration 173, loss = 0.2911776006221771
iteration 174, loss = 0.33474570512771606
iteration 175, loss = 0.27709779143333435
iteration 176, loss = 0.29364436864852905
iteration 177, loss = 0.28473713994026184
iteration 178, loss = 0.31352558732032776
iteration 179, loss = 0.3190224766731262
iteration 180, loss = 0.3379153609275818
iteration 181, loss = 0.3320052921772003
iteration 182, loss = 0.32317882776260376
iteration 183, loss = 0.3122705817222595
iteration 184, loss = 0.29949408769607544
iteration 185, loss = 0.31651732325553894
iteration 186, loss = 0.27758288383483887
iteration 187, loss = 0.2977532148361206
iteration 188, loss = 0.2908332645893097
iteration 189, loss = 0.26721885800361633
iteration 190, loss = 0.3005407452583313
iteration 191, loss = 0.3046398162841797
iteration 192, loss = 0.3185112178325653
iteration 193, loss = 0.2923088073730469
iteration 194, loss = 0.30311718583106995
iteration 195, loss = 0.32167768478393555
iteration 196, loss = 0.29218384623527527
iteration 197, loss = 0.3046914339065552
iteration 198, loss = 0.295844167470932
iteration 199, loss = 0.2993850111961365
iteration 200, loss = 0.2872835695743561
iteration 201, loss = 0.29001104831695557
iteration 202, loss = 0.3272175192832947
iteration 203, loss = 0.3207154870033264
iteration 204, loss = 0.3002975285053253
iteration 205, loss = 0.3017650246620178
iteration 206, loss = 0.2887577414512634
iteration 207, loss = 0.2942812740802765
iteration 208, loss = 0.31014567613601685
iteration 209, loss = 0.29494306445121765
iteration 210, loss = 0.2684149742126465
iteration 211, loss = 0.33098557591438293
iteration 212, loss = 0.29118335247039795
iteration 213, loss = 0.311099112033844
iteration 214, loss = 0.2982701063156128
iteration 215, loss = 0.3068426251411438
iteration 216, loss = 0.28965863585472107
iteration 217, loss = 0.34038713574409485
iteration 218, loss = 0.2975277602672577
iteration 219, loss = 0.31899920105934143
iteration 220, loss = 0.2843671143054962
iteration 221, loss = 0.30062398314476013
iteration 222, loss = 0.28997787833213806
iteration 223, loss = 0.300164133310318
iteration 224, loss = 0.32568156719207764
iteration 225, loss = 0.29129111766815186
iteration 226, loss = 0.28949862718582153
iteration 227, loss = 0.32358086109161377
iteration 228, loss = 0.3095138669013977
iteration 229, loss = 0.29481884837150574
iteration 230, loss = 0.29752475023269653
iteration 231, loss = 0.3063412308692932
iteration 232, loss = 0.2884494960308075
iteration 233, loss = 0.3263971209526062
iteration 234, loss = 0.2947719693183899
iteration 235, loss = 0.2797504663467407
iteration 236, loss = 0.2715111970901489
iteration 237, loss = 0.31598401069641113
iteration 238, loss = 0.3046601712703705
iteration 239, loss = 0.3111703097820282
iteration 240, loss = 0.3551478385925293
iteration 241, loss = 0.26590731739997864
iteration 242, loss = 0.29827797412872314
iteration 243, loss = 0.288054883480072
iteration 244, loss = 0.27968090772628784
iteration 245, loss = 0.2949956953525543
iteration 246, loss = 0.29332005977630615
iteration 247, loss = 0.26008012890815735
iteration 248, loss = 0.29412734508514404
iteration 249, loss = 0.30076223611831665
iteration 250, loss = 0.2835548520088196
iteration 251, loss = 0.300740122795105
iteration 252, loss = 0.27851662039756775
iteration 253, loss = 0.3044969439506531
iteration 254, loss = 0.3179623782634735
iteration 255, loss = 0.29544931650161743
iteration 256, loss = 0.2995479106903076
iteration 257, loss = 0.2999306321144104
iteration 258, loss = 0.2885185480117798
iteration 259, loss = 0.3081471920013428
iteration 260, loss = 0.28263670206069946
iteration 261, loss = 0.2906646132469177
iteration 262, loss = 0.26043209433555603
iteration 263, loss = 0.28637486696243286
iteration 264, loss = 0.27269095182418823
iteration 265, loss = 0.28524646162986755
iteration 266, loss = 0.32635992765426636
iteration 267, loss = 0.3212568163871765
iteration 268, loss = 0.2898842692375183
iteration 269, loss = 0.3112625777721405
iteration 270, loss = 0.2951982915401459
iteration 271, loss = 0.296323299407959
iteration 272, loss = 0.273293673992157
iteration 273, loss = 0.2691997289657593
iteration 274, loss = 0.3274063169956207
iteration 275, loss = 0.27328041195869446
iteration 276, loss = 0.27534377574920654
iteration 277, loss = 0.259181410074234
iteration 278, loss = 0.29172176122665405
iteration 279, loss = 0.27249133586883545
iteration 280, loss = 0.29997485876083374
iteration 281, loss = 0.2967730462551117
iteration 282, loss = 0.2677076458930969
iteration 283, loss = 0.261287122964859
iteration 284, loss = 0.2676476240158081
iteration 285, loss = 0.279773086309433
iteration 286, loss = 0.2536129653453827
iteration 287, loss = 0.29600852727890015
iteration 288, loss = 0.2756056785583496
iteration 289, loss = 0.30044886469841003
iteration 290, loss = 0.25348934531211853
iteration 291, loss = 0.3139907717704773
iteration 292, loss = 0.3376148045063019
iteration 293, loss = 0.29972490668296814
iteration 294, loss = 0.2734326720237732
iteration 295, loss = 0.283995658159256
iteration 296, loss = 0.2866564393043518
iteration 297, loss = 0.31614676117897034
iteration 298, loss = 0.31107115745544434
iteration 299, loss = 0.2922477126121521
iteration 0, loss = 0.30050283670425415
iteration 1, loss = 0.2957729995250702
iteration 2, loss = 0.313925176858902
iteration 3, loss = 0.264285147190094
iteration 4, loss = 0.2915644347667694
iteration 5, loss = 0.28978145122528076
iteration 6, loss = 0.3033677637577057
iteration 7, loss = 0.2714610695838928
iteration 8, loss = 0.27955150604248047
iteration 9, loss = 0.274370402097702
iteration 10, loss = 0.2747429311275482
iteration 11, loss = 0.2810983657836914
iteration 12, loss = 0.2765442728996277
iteration 13, loss = 0.285552054643631
iteration 14, loss = 0.2837051749229431
iteration 15, loss = 0.2822817265987396
iteration 16, loss = 0.2865060567855835
iteration 17, loss = 0.28677502274513245
iteration 18, loss = 0.2866753041744232
iteration 19, loss = 0.30754220485687256
iteration 20, loss = 0.2640271782875061
iteration 21, loss = 0.2984475791454315
iteration 22, loss = 0.29850178956985474
iteration 23, loss = 0.2993760108947754
iteration 24, loss = 0.2873697578907013
iteration 25, loss = 0.2652493119239807
iteration 26, loss = 0.30391833186149597
iteration 27, loss = 0.28523367643356323
iteration 28, loss = 0.30655568838119507
iteration 29, loss = 0.3261030912399292
iteration 30, loss = 0.2642662525177002
iteration 31, loss = 0.3218340575695038
iteration 32, loss = 0.29152482748031616
iteration 33, loss = 0.28361818194389343
iteration 34, loss = 0.24526916444301605
iteration 35, loss = 0.3125333786010742
iteration 36, loss = 0.27521225810050964
iteration 37, loss = 0.2928643822669983
iteration 38, loss = 0.2555965781211853
iteration 39, loss = 0.277091383934021
iteration 40, loss = 0.28281253576278687
iteration 41, loss = 0.299773246049881
iteration 42, loss = 0.3106360137462616
iteration 43, loss = 0.2648719549179077
iteration 44, loss = 0.2713443636894226
iteration 45, loss = 0.2853044867515564
iteration 46, loss = 0.269336074590683
iteration 47, loss = 0.27890539169311523
iteration 48, loss = 0.2577131688594818
iteration 49, loss = 0.275085985660553
iteration 50, loss = 0.30015647411346436
iteration 51, loss = 0.2762371897697449
iteration 52, loss = 0.2966328561306
iteration 53, loss = 0.27468258142471313
iteration 54, loss = 0.3100283145904541
iteration 55, loss = 0.2662096917629242
iteration 56, loss = 0.2925509214401245
iteration 57, loss = 0.27272653579711914
iteration 58, loss = 0.27985161542892456
iteration 59, loss = 0.2889029383659363
iteration 60, loss = 0.2690808176994324
iteration 61, loss = 0.2841890752315521
iteration 62, loss = 0.2646385431289673
iteration 63, loss = 0.2893857955932617
iteration 64, loss = 0.2797691822052002
iteration 65, loss = 0.2705138325691223
iteration 66, loss = 0.2596893608570099
iteration 67, loss = 0.2488582879304886
iteration 68, loss = 0.2561315894126892
iteration 69, loss = 0.27443695068359375
iteration 70, loss = 0.25466978549957275
iteration 71, loss = 0.2607913017272949
iteration 72, loss = 0.2663639783859253
iteration 73, loss = 0.27497944235801697
iteration 74, loss = 0.2512950003147125
iteration 75, loss = 0.28965744376182556
iteration 76, loss = 0.2511937618255615
iteration 77, loss = 0.27508485317230225
iteration 78, loss = 0.27317261695861816
iteration 79, loss = 0.29749512672424316
iteration 80, loss = 0.261523574590683
iteration 81, loss = 0.2804719805717468
iteration 82, loss = 0.2986791133880615
iteration 83, loss = 0.2517307996749878
iteration 84, loss = 0.2800760269165039
iteration 85, loss = 0.25607916712760925
iteration 86, loss = 0.29669225215911865
iteration 87, loss = 0.26844799518585205
iteration 88, loss = 0.25088512897491455
iteration 89, loss = 0.2743852436542511
iteration 90, loss = 0.2629128694534302
iteration 91, loss = 0.22736497223377228
iteration 92, loss = 0.2936660945415497
iteration 93, loss = 0.2453947812318802
iteration 94, loss = 0.28043586015701294
iteration 95, loss = 0.2681044936180115
iteration 96, loss = 0.2547856271266937
iteration 97, loss = 0.2567436993122101
iteration 98, loss = 0.2846807539463043
iteration 99, loss = 0.26269254088401794
iteration 100, loss = 0.2645181119441986
iteration 101, loss = 0.26668742299079895
iteration 102, loss = 0.26776403188705444
iteration 103, loss = 0.28696727752685547
iteration 104, loss = 0.2526574730873108
iteration 105, loss = 0.25902366638183594
iteration 106, loss = 0.2862926125526428
iteration 107, loss = 0.2669219672679901
iteration 108, loss = 0.2582249343395233
iteration 109, loss = 0.29605498909950256
iteration 110, loss = 0.28612804412841797
iteration 111, loss = 0.2501661777496338
iteration 112, loss = 0.29666051268577576
iteration 113, loss = 0.26853030920028687
iteration 114, loss = 0.24377328157424927
iteration 115, loss = 0.2629995048046112
iteration 116, loss = 0.27196332812309265
iteration 117, loss = 0.3018815219402313
iteration 118, loss = 0.25036847591400146
iteration 119, loss = 0.2905418276786804
iteration 120, loss = 0.2863876521587372
iteration 121, loss = 0.23160244524478912
iteration 122, loss = 0.2579174339771271
iteration 123, loss = 0.22942905128002167
iteration 124, loss = 0.25276023149490356
iteration 125, loss = 0.27271613478660583
iteration 126, loss = 0.264509379863739
iteration 127, loss = 0.2755654454231262
iteration 128, loss = 0.28186723589897156
iteration 129, loss = 0.26755067706108093
iteration 130, loss = 0.2648566663265228
iteration 131, loss = 0.2534547746181488
iteration 132, loss = 0.26517385244369507
iteration 133, loss = 0.27032962441444397
iteration 134, loss = 0.27993953227996826
iteration 135, loss = 0.24384482204914093
iteration 136, loss = 0.24014437198638916
iteration 137, loss = 0.23526206612586975
iteration 138, loss = 0.23928162455558777
iteration 139, loss = 0.2573147714138031
iteration 140, loss = 0.26472359895706177
iteration 141, loss = 0.25395870208740234
iteration 142, loss = 0.26901355385780334
iteration 143, loss = 0.2824168801307678
iteration 144, loss = 0.27540358901023865
iteration 145, loss = 0.22868221998214722
iteration 146, loss = 0.2722300887107849
iteration 147, loss = 0.24038416147232056
iteration 148, loss = 0.2750040292739868
iteration 149, loss = 0.2878209948539734
iteration 150, loss = 0.2880965769290924
iteration 151, loss = 0.2717919945716858
iteration 152, loss = 0.23877741396427155
iteration 153, loss = 0.27418193221092224
iteration 154, loss = 0.2356312870979309
iteration 155, loss = 0.2722536325454712
iteration 156, loss = 0.32715803384780884
iteration 157, loss = 0.26592519879341125
iteration 158, loss = 0.2616955637931824
iteration 159, loss = 0.2527858018875122
iteration 160, loss = 0.23144790530204773
iteration 161, loss = 0.28968673944473267
iteration 162, loss = 0.23005975782871246
iteration 163, loss = 0.25379830598831177
iteration 164, loss = 0.22937172651290894
iteration 165, loss = 0.2398170530796051
iteration 166, loss = 0.26981431245803833
iteration 167, loss = 0.2530805468559265
iteration 168, loss = 0.2518582046031952
iteration 169, loss = 0.2667277455329895
iteration 170, loss = 0.2608209252357483
iteration 171, loss = 0.29874420166015625
iteration 172, loss = 0.2416994869709015
iteration 173, loss = 0.24964189529418945
iteration 174, loss = 0.24463318288326263
iteration 175, loss = 0.2465047389268875
iteration 176, loss = 0.3138290047645569
iteration 177, loss = 0.23958919942378998
iteration 178, loss = 0.24327196180820465
iteration 179, loss = 0.26825809478759766
iteration 180, loss = 0.28021240234375
iteration 181, loss = 0.2653629183769226
iteration 182, loss = 0.24715524911880493
iteration 183, loss = 0.23952984809875488
iteration 184, loss = 0.24808165431022644
iteration 185, loss = 0.23802757263183594
iteration 186, loss = 0.2526661157608032
iteration 187, loss = 0.24979251623153687
iteration 188, loss = 0.2527751922607422
iteration 189, loss = 0.2697155475616455
iteration 190, loss = 0.276056706905365
iteration 191, loss = 0.24222244322299957
iteration 192, loss = 0.29132920503616333
iteration 193, loss = 0.25720053911209106
iteration 194, loss = 0.2719169557094574
iteration 195, loss = 0.2571246027946472
iteration 196, loss = 0.2809814512729645
iteration 197, loss = 0.26934048533439636
iteration 198, loss = 0.2602267563343048
iteration 199, loss = 0.26510754227638245
iteration 200, loss = 0.26523274183273315
iteration 201, loss = 0.2441190928220749
iteration 202, loss = 0.27566662430763245
iteration 203, loss = 0.24111653864383698
iteration 204, loss = 0.2656523883342743
iteration 205, loss = 0.2721606194972992
iteration 206, loss = 0.252963662147522
iteration 207, loss = 0.23706379532814026
iteration 208, loss = 0.28318002820014954
iteration 209, loss = 0.28142428398132324
iteration 210, loss = 0.25781458616256714
iteration 211, loss = 0.2537001669406891
iteration 212, loss = 0.26340174674987793
iteration 213, loss = 0.2777872383594513
iteration 214, loss = 0.2699449956417084
iteration 215, loss = 0.2902931272983551
iteration 216, loss = 0.2822742462158203
iteration 217, loss = 0.2187929004430771
iteration 218, loss = 0.23718097805976868
iteration 219, loss = 0.25108903646469116
iteration 220, loss = 0.27149221301078796
iteration 221, loss = 0.26080891489982605
iteration 222, loss = 0.2464974969625473
iteration 223, loss = 0.2363494485616684
iteration 224, loss = 0.24061456322669983
iteration 225, loss = 0.30094635486602783
iteration 226, loss = 0.2394964098930359
iteration 227, loss = 0.2605600953102112
iteration 228, loss = 0.24447444081306458
iteration 229, loss = 0.2734760046005249
iteration 230, loss = 0.2418254166841507
iteration 231, loss = 0.27834922075271606
iteration 232, loss = 0.27755236625671387
iteration 233, loss = 0.22648455202579498
iteration 234, loss = 0.2572017312049866
iteration 235, loss = 0.28276166319847107
iteration 236, loss = 0.26223573088645935
iteration 237, loss = 0.23547540605068207
iteration 238, loss = 0.223484069108963
iteration 239, loss = 0.22346657514572144
iteration 240, loss = 0.28309062123298645
iteration 241, loss = 0.26642608642578125
iteration 242, loss = 0.23251332342624664
iteration 243, loss = 0.28523388504981995
iteration 244, loss = 0.2729494273662567
iteration 245, loss = 0.26082366704940796
iteration 246, loss = 0.26331818103790283
iteration 247, loss = 0.2399171143770218
iteration 248, loss = 0.22880250215530396
iteration 249, loss = 0.2427951693534851
iteration 250, loss = 0.2544488310813904
iteration 251, loss = 0.2435407042503357
iteration 252, loss = 0.26564574241638184
iteration 253, loss = 0.29097622632980347
iteration 254, loss = 0.24508976936340332
iteration 255, loss = 0.24247495830059052
iteration 256, loss = 0.24829202890396118
iteration 257, loss = 0.2577435076236725
iteration 258, loss = 0.25093305110931396
iteration 259, loss = 0.2804397940635681
iteration 260, loss = 0.25663459300994873
iteration 261, loss = 0.25849735736846924
iteration 262, loss = 0.24616067111492157
iteration 263, loss = 0.24400778114795685
iteration 264, loss = 0.23238690197467804
iteration 265, loss = 0.2365255355834961
iteration 266, loss = 0.22541466355323792
iteration 267, loss = 0.2507625222206116
iteration 268, loss = 0.258983850479126
iteration 269, loss = 0.21707352995872498
iteration 270, loss = 0.24567219614982605
iteration 271, loss = 0.2743343114852905
iteration 272, loss = 0.24134163558483124
iteration 273, loss = 0.24443599581718445
iteration 274, loss = 0.27033764123916626
iteration 275, loss = 0.25829020142555237
iteration 276, loss = 0.24052011966705322
iteration 277, loss = 0.2550353407859802
iteration 278, loss = 0.2430124133825302
iteration 279, loss = 0.23644135892391205
iteration 280, loss = 0.2557995915412903
iteration 281, loss = 0.24215362966060638
iteration 282, loss = 0.28937217593193054
iteration 283, loss = 0.2473975121974945
iteration 284, loss = 0.2585137188434601
iteration 285, loss = 0.23758207261562347
iteration 286, loss = 0.22653290629386902
iteration 287, loss = 0.25390592217445374
iteration 288, loss = 0.26063454151153564
iteration 289, loss = 0.2503652572631836
iteration 290, loss = 0.25161439180374146
iteration 291, loss = 0.22047200798988342
iteration 292, loss = 0.20447087287902832
iteration 293, loss = 0.25208085775375366
iteration 294, loss = 0.2434251308441162
iteration 295, loss = 0.22438283264636993
iteration 296, loss = 0.21528515219688416
iteration 297, loss = 0.2158345878124237
iteration 298, loss = 0.24891149997711182
iteration 299, loss = 0.2229318469762802
iteration 0, loss = 0.24200913310050964
iteration 1, loss = 0.23477435111999512
iteration 2, loss = 0.23968058824539185
iteration 3, loss = 0.25711899995803833
iteration 4, loss = 0.21208356320858002
iteration 5, loss = 0.23923806846141815
iteration 6, loss = 0.2981346845626831
iteration 7, loss = 0.25288864970207214
iteration 8, loss = 0.24156901240348816
iteration 9, loss = 0.2698172926902771
iteration 10, loss = 0.22189070284366608
iteration 11, loss = 0.2358296811580658
iteration 12, loss = 0.23271985352039337
iteration 13, loss = 0.2660941481590271
iteration 14, loss = 0.24446184933185577
iteration 15, loss = 0.23141255974769592
iteration 16, loss = 0.24348165094852448
iteration 17, loss = 0.26022082567214966
iteration 18, loss = 0.26806074380874634
iteration 19, loss = 0.22606948018074036
iteration 20, loss = 0.21895691752433777
iteration 21, loss = 0.2421848177909851
iteration 22, loss = 0.2161083072423935
iteration 23, loss = 0.21297989785671234
iteration 24, loss = 0.22823813557624817
iteration 25, loss = 0.25699105858802795
iteration 26, loss = 0.2287868857383728
iteration 27, loss = 0.23274074494838715
iteration 28, loss = 0.22826096415519714
iteration 29, loss = 0.22351551055908203
iteration 30, loss = 0.2740272283554077
iteration 31, loss = 0.28482285141944885
iteration 32, loss = 0.2021976113319397
iteration 33, loss = 0.2959752380847931
iteration 34, loss = 0.20977206528186798
iteration 35, loss = 0.22203375399112701
iteration 36, loss = 0.23884829878807068
iteration 37, loss = 0.2212292104959488
iteration 38, loss = 0.26443159580230713
iteration 39, loss = 0.24544385075569153
iteration 40, loss = 0.24351884424686432
iteration 41, loss = 0.23600295186042786
iteration 42, loss = 0.22157196700572968
iteration 43, loss = 0.30527010560035706
iteration 44, loss = 0.2642541825771332
iteration 45, loss = 0.2758251130580902
iteration 46, loss = 0.22414769232273102
iteration 47, loss = 0.26062870025634766
iteration 48, loss = 0.2240830659866333
iteration 49, loss = 0.22278903424739838
iteration 50, loss = 0.1987689733505249
iteration 51, loss = 0.1967836171388626
iteration 52, loss = 0.24983710050582886
iteration 53, loss = 0.2384970486164093
iteration 54, loss = 0.24040624499320984
iteration 55, loss = 0.22341665625572205
iteration 56, loss = 0.23301070928573608
iteration 57, loss = 0.27800726890563965
iteration 58, loss = 0.22269628942012787
iteration 59, loss = 0.2499905377626419
iteration 60, loss = 0.24042347073554993
iteration 61, loss = 0.24114149808883667
iteration 62, loss = 0.20136350393295288
iteration 63, loss = 0.21979933977127075
iteration 64, loss = 0.2148171216249466
iteration 65, loss = 0.23123705387115479
iteration 66, loss = 0.20815123617649078
iteration 67, loss = 0.23582999408245087
iteration 68, loss = 0.20300443470478058
iteration 69, loss = 0.22680887579917908
iteration 70, loss = 0.21590834856033325
iteration 71, loss = 0.2459321916103363
iteration 72, loss = 0.2260204553604126
iteration 73, loss = 0.2234015166759491
iteration 74, loss = 0.22096368670463562
iteration 75, loss = 0.23608015477657318
iteration 76, loss = 0.20174145698547363
iteration 77, loss = 0.2124100923538208
iteration 78, loss = 0.24034644663333893
iteration 79, loss = 0.2290538251399994
iteration 80, loss = 0.23716780543327332
iteration 81, loss = 0.22147677838802338
iteration 82, loss = 0.23462137579917908
iteration 83, loss = 0.23634913563728333
iteration 84, loss = 0.2158632129430771
iteration 85, loss = 0.20033402740955353
iteration 86, loss = 0.2144346535205841
iteration 87, loss = 0.24579696357250214
iteration 88, loss = 0.22193139791488647
iteration 89, loss = 0.2470341920852661
iteration 90, loss = 0.21853750944137573
iteration 91, loss = 0.23948726058006287
iteration 92, loss = 0.21292194724082947
iteration 93, loss = 0.196440652012825
iteration 94, loss = 0.20398278534412384
iteration 95, loss = 0.21372926235198975
iteration 96, loss = 0.23901815712451935
iteration 97, loss = 0.23339490592479706
iteration 98, loss = 0.1943964660167694
iteration 99, loss = 0.2517322301864624
iteration 100, loss = 0.24090483784675598
iteration 101, loss = 0.21269914507865906
iteration 102, loss = 0.239861860871315
iteration 103, loss = 0.2120131552219391
iteration 104, loss = 0.23061132431030273
iteration 105, loss = 0.2007245570421219
iteration 106, loss = 0.19589801132678986
iteration 107, loss = 0.22387883067131042
iteration 108, loss = 0.20932209491729736
iteration 109, loss = 0.20557226240634918
iteration 110, loss = 0.21587765216827393
iteration 111, loss = 0.21443882584571838
iteration 112, loss = 0.24982579052448273
iteration 113, loss = 0.2315143346786499
iteration 114, loss = 0.22476321458816528
iteration 115, loss = 0.2320789396762848
iteration 116, loss = 0.22816558182239532
iteration 117, loss = 0.21124254167079926
iteration 118, loss = 0.22101420164108276
iteration 119, loss = 0.20884718000888824
iteration 120, loss = 0.20996657013893127
iteration 121, loss = 0.22688151895999908
iteration 122, loss = 0.20742857456207275
iteration 123, loss = 0.21067452430725098
iteration 124, loss = 0.20833706855773926
iteration 125, loss = 0.23349180817604065
iteration 126, loss = 0.21124744415283203
iteration 127, loss = 0.24576179683208466
iteration 128, loss = 0.23978564143180847
iteration 129, loss = 0.22864565253257751
iteration 130, loss = 0.20715679228305817
iteration 131, loss = 0.20872512459754944
iteration 132, loss = 0.20557419955730438
iteration 133, loss = 0.20350556075572968
iteration 134, loss = 0.23530611395835876
iteration 135, loss = 0.2320338636636734
iteration 136, loss = 0.22764240205287933
iteration 137, loss = 0.2434253692626953
iteration 138, loss = 0.21605688333511353
iteration 139, loss = 0.24635207653045654
iteration 140, loss = 0.2431076467037201
iteration 141, loss = 0.21597938239574432
iteration 142, loss = 0.2015075534582138
iteration 143, loss = 0.1951427161693573
iteration 144, loss = 0.21372009813785553
iteration 145, loss = 0.24367812275886536
iteration 146, loss = 0.21686144173145294
iteration 147, loss = 0.21229228377342224
iteration 148, loss = 0.23084524273872375
iteration 149, loss = 0.21548618376255035
iteration 150, loss = 0.24988871812820435
iteration 151, loss = 0.2031061351299286
iteration 152, loss = 0.26348987221717834
iteration 153, loss = 0.18811550736427307
iteration 154, loss = 0.23982855677604675
iteration 155, loss = 0.2191101461648941
iteration 156, loss = 0.22620928287506104
iteration 157, loss = 0.2227892279624939
iteration 158, loss = 0.23864318430423737
iteration 159, loss = 0.21797069907188416
iteration 160, loss = 0.22826597094535828
iteration 161, loss = 0.21822834014892578
iteration 162, loss = 0.2286626398563385
iteration 163, loss = 0.2274436056613922
iteration 164, loss = 0.23748552799224854
iteration 165, loss = 0.2183818221092224
iteration 166, loss = 0.20212002098560333
iteration 167, loss = 0.26841700077056885
iteration 168, loss = 0.23613035678863525
iteration 169, loss = 0.2209668606519699
iteration 170, loss = 0.2298537939786911
iteration 171, loss = 0.20640170574188232
iteration 172, loss = 0.21812333166599274
iteration 173, loss = 0.208394855260849
iteration 174, loss = 0.2412727326154709
iteration 175, loss = 0.19861602783203125
iteration 176, loss = 0.20790132880210876
iteration 177, loss = 0.2042994648218155
iteration 178, loss = 0.2486264407634735
iteration 179, loss = 0.22768563032150269
iteration 180, loss = 0.22917440533638
iteration 181, loss = 0.19460947811603546
iteration 182, loss = 0.23797348141670227
iteration 183, loss = 0.20666444301605225
iteration 184, loss = 0.19175121188163757
iteration 185, loss = 0.2097569853067398
iteration 186, loss = 0.2418603003025055
iteration 187, loss = 0.27151450514793396
iteration 188, loss = 0.19083407521247864
iteration 189, loss = 0.199972003698349
iteration 190, loss = 0.26825258135795593
iteration 191, loss = 0.23415584862232208
iteration 192, loss = 0.18996700644493103
iteration 193, loss = 0.2175000160932541
iteration 194, loss = 0.20865224301815033
iteration 195, loss = 0.20011769235134125
iteration 196, loss = 0.20863628387451172
iteration 197, loss = 0.20957612991333008
iteration 198, loss = 0.28369542956352234
iteration 199, loss = 0.23065263032913208
iteration 200, loss = 0.1949072629213333
iteration 201, loss = 0.2091386616230011
iteration 202, loss = 0.20370817184448242
iteration 203, loss = 0.21597141027450562
iteration 204, loss = 0.23565348982810974
iteration 205, loss = 0.23782138526439667
iteration 206, loss = 0.19672656059265137
iteration 207, loss = 0.2031082659959793
iteration 208, loss = 0.2152012288570404
iteration 209, loss = 0.20805232226848602
iteration 210, loss = 0.21840596199035645
iteration 211, loss = 0.20744234323501587
iteration 212, loss = 0.19707955420017242
iteration 213, loss = 0.22966983914375305
iteration 214, loss = 0.1895032525062561
iteration 215, loss = 0.2035568803548813
iteration 216, loss = 0.20207908749580383
iteration 217, loss = 0.19682270288467407
iteration 218, loss = 0.20819811522960663
iteration 219, loss = 0.2011275738477707
iteration 220, loss = 0.20265203714370728
iteration 221, loss = 0.20177999138832092
iteration 222, loss = 0.20204073190689087
iteration 223, loss = 0.2121681272983551
iteration 224, loss = 0.2132185995578766
iteration 225, loss = 0.1837216466665268
iteration 226, loss = 0.19372479617595673
iteration 227, loss = 0.20971424877643585
iteration 228, loss = 0.17122165858745575
iteration 229, loss = 0.1730172038078308
iteration 230, loss = 0.18375539779663086
iteration 231, loss = 0.24286404252052307
iteration 232, loss = 0.20557264983654022
iteration 233, loss = 0.22024425864219666
iteration 234, loss = 0.20998185873031616
iteration 235, loss = 0.189628466963768
iteration 236, loss = 0.19468635320663452
iteration 237, loss = 0.2288936972618103
iteration 238, loss = 0.18572868406772614
iteration 239, loss = 0.2314755916595459
iteration 240, loss = 0.20343931019306183
iteration 241, loss = 0.17861895263195038
iteration 242, loss = 0.19785940647125244
iteration 243, loss = 0.1990571916103363
iteration 244, loss = 0.17851150035858154
iteration 245, loss = 0.21087738871574402
iteration 246, loss = 0.22227033972740173
iteration 247, loss = 0.2602396607398987
iteration 248, loss = 0.22091592848300934
iteration 249, loss = 0.22412371635437012
iteration 250, loss = 0.19715923070907593
iteration 251, loss = 0.23472179472446442
iteration 252, loss = 0.23069654405117035
iteration 253, loss = 0.1785343736410141
iteration 254, loss = 0.18305368721485138
iteration 255, loss = 0.20558488368988037
iteration 256, loss = 0.19228500127792358
iteration 257, loss = 0.21642553806304932
iteration 258, loss = 0.24485385417938232
iteration 259, loss = 0.21335437893867493
iteration 260, loss = 0.17554277181625366
iteration 261, loss = 0.1916625201702118
iteration 262, loss = 0.16488605737686157
iteration 263, loss = 0.19303695857524872
iteration 264, loss = 0.20027776062488556
iteration 265, loss = 0.2541545331478119
iteration 266, loss = 0.21961551904678345
iteration 267, loss = 0.17837516963481903
iteration 268, loss = 0.20864713191986084
iteration 269, loss = 0.22935065627098083
iteration 270, loss = 0.17768630385398865
iteration 271, loss = 0.19666780531406403
iteration 272, loss = 0.24484337866306305
iteration 273, loss = 0.1884804368019104
iteration 274, loss = 0.21407681703567505
iteration 275, loss = 0.21996451914310455
iteration 276, loss = 0.19685329496860504
iteration 277, loss = 0.2021128237247467
iteration 278, loss = 0.18714892864227295
iteration 279, loss = 0.2049301117658615
iteration 280, loss = 0.21213993430137634
iteration 281, loss = 0.19820335507392883
iteration 282, loss = 0.18017426133155823
iteration 283, loss = 0.19734247028827667
iteration 284, loss = 0.2336392104625702
iteration 285, loss = 0.22335919737815857
iteration 286, loss = 0.18706165254116058
iteration 287, loss = 0.2270478904247284
iteration 288, loss = 0.1732722818851471
iteration 289, loss = 0.19593718647956848
iteration 290, loss = 0.20407749712467194
iteration 291, loss = 0.22054767608642578
iteration 292, loss = 0.19018390774726868
iteration 293, loss = 0.20354841649532318
iteration 294, loss = 0.19661925733089447
iteration 295, loss = 0.1973116099834442
iteration 296, loss = 0.20924635231494904
iteration 297, loss = 0.20413510501384735
iteration 298, loss = 0.1678789108991623
iteration 299, loss = 0.20304030179977417
iteration 0, loss = 0.20695362985134125
iteration 1, loss = 0.2197570949792862
iteration 2, loss = 0.18551397323608398
iteration 3, loss = 0.18006187677383423
iteration 4, loss = 0.19019800424575806
iteration 5, loss = 0.22027213871479034
iteration 6, loss = 0.1813630908727646
iteration 7, loss = 0.1894187033176422
iteration 8, loss = 0.1941136121749878
iteration 9, loss = 0.21313410997390747
iteration 10, loss = 0.2201003134250641
iteration 11, loss = 0.17123305797576904
iteration 12, loss = 0.16259828209877014
iteration 13, loss = 0.17090122401714325
iteration 14, loss = 0.19378067553043365
iteration 15, loss = 0.1802733689546585
iteration 16, loss = 0.18872785568237305
iteration 17, loss = 0.17704744637012482
iteration 18, loss = 0.20100301504135132
iteration 19, loss = 0.21389301121234894
iteration 20, loss = 0.20168770849704742
iteration 21, loss = 0.17777343094348907
iteration 22, loss = 0.20008713006973267
iteration 23, loss = 0.19600194692611694
iteration 24, loss = 0.18629559874534607
iteration 25, loss = 0.19914452731609344
iteration 26, loss = 0.1698388159275055
iteration 27, loss = 0.20443466305732727
iteration 28, loss = 0.22252590954303741
iteration 29, loss = 0.22996988892555237
iteration 30, loss = 0.19183383882045746
iteration 31, loss = 0.21520349383354187
iteration 32, loss = 0.20864742994308472
iteration 33, loss = 0.24626490473747253
iteration 34, loss = 0.18046323955059052
iteration 35, loss = 0.1873752623796463
iteration 36, loss = 0.17715251445770264
iteration 37, loss = 0.1708334982395172
iteration 38, loss = 0.2284030318260193
iteration 39, loss = 0.17579138278961182
iteration 40, loss = 0.20144467055797577
iteration 41, loss = 0.17421451210975647
iteration 42, loss = 0.19144588708877563
iteration 43, loss = 0.21799641847610474
iteration 44, loss = 0.18499021232128143
iteration 45, loss = 0.18179546296596527
iteration 46, loss = 0.1960742026567459
iteration 47, loss = 0.19415675103664398
iteration 48, loss = 0.19460919499397278
iteration 49, loss = 0.18668325245380402
iteration 50, loss = 0.22541728615760803
iteration 51, loss = 0.17976389825344086
iteration 52, loss = 0.17980968952178955
iteration 53, loss = 0.2183699607849121
iteration 54, loss = 0.17860834300518036
iteration 55, loss = 0.1934906393289566
iteration 56, loss = 0.18155516684055328
iteration 57, loss = 0.23639683425426483
iteration 58, loss = 0.18351583182811737
iteration 59, loss = 0.21679078042507172
iteration 60, loss = 0.16255377233028412
iteration 61, loss = 0.1871422529220581
iteration 62, loss = 0.18254603445529938
iteration 63, loss = 0.1608896702528
iteration 64, loss = 0.2370256930589676
iteration 65, loss = 0.2324770987033844
iteration 66, loss = 0.16315896809101105
iteration 67, loss = 0.1684841364622116
iteration 68, loss = 0.18649248778820038
iteration 69, loss = 0.1766354739665985
iteration 70, loss = 0.1832072138786316
iteration 71, loss = 0.226679265499115
iteration 72, loss = 0.2165321409702301
iteration 73, loss = 0.1887439489364624
iteration 74, loss = 0.19760018587112427
iteration 75, loss = 0.2093672752380371
iteration 76, loss = 0.17350322008132935
iteration 77, loss = 0.1822347640991211
iteration 78, loss = 0.2047807276248932
iteration 79, loss = 0.182902991771698
iteration 80, loss = 0.2316095530986786
iteration 81, loss = 0.20664064586162567
iteration 82, loss = 0.1842072755098343
iteration 83, loss = 0.17309565842151642
iteration 84, loss = 0.1832018494606018
iteration 85, loss = 0.20403839647769928
iteration 86, loss = 0.19186559319496155
iteration 87, loss = 0.18593156337738037
iteration 88, loss = 0.175716370344162
iteration 89, loss = 0.1739460527896881
iteration 90, loss = 0.2128129005432129
iteration 91, loss = 0.20132333040237427
iteration 92, loss = 0.18199676275253296
iteration 93, loss = 0.1693309098482132
iteration 94, loss = 0.20974931120872498
iteration 95, loss = 0.17758053541183472
iteration 96, loss = 0.16772185266017914
iteration 97, loss = 0.16925141215324402
iteration 98, loss = 0.22730529308319092
iteration 99, loss = 0.18657752871513367
iteration 100, loss = 0.18942198157310486
iteration 101, loss = 0.21994459629058838
iteration 102, loss = 0.18706494569778442
iteration 103, loss = 0.17772309482097626
iteration 104, loss = 0.19033357501029968
iteration 105, loss = 0.20312419533729553
iteration 106, loss = 0.18737977743148804
iteration 107, loss = 0.1926889717578888
iteration 108, loss = 0.20222926139831543
iteration 109, loss = 0.15967005491256714
iteration 110, loss = 0.15193995833396912
iteration 111, loss = 0.18279986083507538
iteration 112, loss = 0.18418534100055695
iteration 113, loss = 0.1881142109632492
iteration 114, loss = 0.14935237169265747
iteration 115, loss = 0.21358788013458252
iteration 116, loss = 0.1844649612903595
iteration 117, loss = 0.16291318833827972
iteration 118, loss = 0.15540671348571777
iteration 119, loss = 0.22626297175884247
iteration 120, loss = 0.19514018297195435
iteration 121, loss = 0.20499269664287567
iteration 122, loss = 0.16542109847068787
iteration 123, loss = 0.14694204926490784
iteration 124, loss = 0.1821247935295105
iteration 125, loss = 0.20201407372951508
iteration 126, loss = 0.21485570073127747
iteration 127, loss = 0.14720824360847473
iteration 128, loss = 0.17463630437850952
iteration 129, loss = 0.21371355652809143
iteration 130, loss = 0.1933642029762268
iteration 131, loss = 0.1483151912689209
iteration 132, loss = 0.17223042249679565
iteration 133, loss = 0.181356281042099
iteration 134, loss = 0.19004586338996887
iteration 135, loss = 0.17005480825901031
iteration 136, loss = 0.18566033244132996
iteration 137, loss = 0.1776069849729538
iteration 138, loss = 0.16758288443088531
iteration 139, loss = 0.17177991569042206
iteration 140, loss = 0.20029842853546143
iteration 141, loss = 0.1712188869714737
iteration 142, loss = 0.18283690512180328
iteration 143, loss = 0.17185914516448975
iteration 144, loss = 0.16517165303230286
iteration 145, loss = 0.1848505288362503
iteration 146, loss = 0.20401856303215027
iteration 147, loss = 0.18349218368530273
iteration 148, loss = 0.21596306562423706
iteration 149, loss = 0.20504726469516754
iteration 150, loss = 0.21218416094779968
iteration 151, loss = 0.18240106105804443
iteration 152, loss = 0.17086315155029297
iteration 153, loss = 0.15880176424980164
iteration 154, loss = 0.16829831898212433
iteration 155, loss = 0.17376071214675903
iteration 156, loss = 0.17098018527030945
iteration 157, loss = 0.1919613629579544
iteration 158, loss = 0.17397789657115936
iteration 159, loss = 0.1497562974691391
iteration 160, loss = 0.18260566890239716
iteration 161, loss = 0.17634907364845276
iteration 162, loss = 0.18928176164627075
iteration 163, loss = 0.15200582146644592
iteration 164, loss = 0.19475406408309937
iteration 165, loss = 0.1842813640832901
iteration 166, loss = 0.2139996737241745
iteration 167, loss = 0.16208724677562714
iteration 168, loss = 0.15731185674667358
iteration 169, loss = 0.19032606482505798
iteration 170, loss = 0.1706405133008957
iteration 171, loss = 0.16165432333946228
iteration 172, loss = 0.20134767889976501
iteration 173, loss = 0.157711923122406
iteration 174, loss = 0.17480996251106262
iteration 175, loss = 0.1730552315711975
iteration 176, loss = 0.15713007748126984
iteration 177, loss = 0.17772547900676727
iteration 178, loss = 0.16064955294132233
iteration 179, loss = 0.17000392079353333
iteration 180, loss = 0.16868986189365387
iteration 181, loss = 0.1633284091949463
iteration 182, loss = 0.18270261585712433
iteration 183, loss = 0.24222391843795776
iteration 184, loss = 0.18669617176055908
iteration 185, loss = 0.19466060400009155
iteration 186, loss = 0.19466708600521088
iteration 187, loss = 0.21562018990516663
iteration 188, loss = 0.19821996986865997
iteration 189, loss = 0.1523202359676361
iteration 190, loss = 0.17522384226322174
iteration 191, loss = 0.2070659101009369
iteration 192, loss = 0.18746009469032288
iteration 193, loss = 0.1550687998533249
iteration 194, loss = 0.19097056984901428
iteration 195, loss = 0.15049344301223755
iteration 196, loss = 0.13365232944488525
iteration 197, loss = 0.17607815563678741
iteration 198, loss = 0.17627742886543274
iteration 199, loss = 0.17351411283016205
iteration 200, loss = 0.18544745445251465
iteration 201, loss = 0.16935549676418304
iteration 202, loss = 0.16021263599395752
iteration 203, loss = 0.1754010170698166
iteration 204, loss = 0.172445148229599
iteration 205, loss = 0.175806924700737
iteration 206, loss = 0.15342463552951813
iteration 207, loss = 0.14158029854297638
iteration 208, loss = 0.19540643692016602
iteration 209, loss = 0.17252880334854126
iteration 210, loss = 0.20063376426696777
iteration 211, loss = 0.16359764337539673
iteration 212, loss = 0.17537783086299896
iteration 213, loss = 0.1659248173236847
iteration 214, loss = 0.18636435270309448
iteration 215, loss = 0.13497613370418549
iteration 216, loss = 0.20298410952091217
iteration 217, loss = 0.16845248639583588
iteration 218, loss = 0.15512587130069733
iteration 219, loss = 0.14952519536018372
iteration 220, loss = 0.13390250504016876
iteration 221, loss = 0.17543639242649078
iteration 222, loss = 0.17755155265331268
iteration 223, loss = 0.17297931015491486
iteration 224, loss = 0.15796199440956116
iteration 225, loss = 0.17000240087509155
iteration 226, loss = 0.15870767831802368
iteration 227, loss = 0.17919012904167175
iteration 228, loss = 0.14999806880950928
iteration 229, loss = 0.1829945147037506
iteration 230, loss = 0.1772712767124176
iteration 231, loss = 0.16033446788787842
iteration 232, loss = 0.1491832584142685
iteration 233, loss = 0.1439921259880066
iteration 234, loss = 0.16502563655376434
iteration 235, loss = 0.16207891702651978
iteration 236, loss = 0.16512265801429749
iteration 237, loss = 0.16845697164535522
iteration 238, loss = 0.24947606027126312
iteration 239, loss = 0.1748875379562378
iteration 240, loss = 0.15885700285434723
iteration 241, loss = 0.17462825775146484
iteration 242, loss = 0.18185819685459137
iteration 243, loss = 0.18346640467643738
iteration 244, loss = 0.15082040429115295
iteration 245, loss = 0.18396484851837158
iteration 246, loss = 0.20074695348739624
iteration 247, loss = 0.13765086233615875
iteration 248, loss = 0.1679794043302536
iteration 249, loss = 0.14636461436748505
iteration 250, loss = 0.1998828649520874
iteration 251, loss = 0.19191031157970428
iteration 252, loss = 0.18447038531303406
iteration 253, loss = 0.13697564601898193
iteration 254, loss = 0.1544022262096405
iteration 255, loss = 0.13687877357006073
iteration 256, loss = 0.16393043100833893
iteration 257, loss = 0.1801224797964096
iteration 258, loss = 0.16396349668502808
iteration 259, loss = 0.15424954891204834
iteration 260, loss = 0.18335160613059998
iteration 261, loss = 0.18315629661083221
iteration 262, loss = 0.1589752435684204
iteration 263, loss = 0.14947938919067383
iteration 264, loss = 0.1631036400794983
iteration 265, loss = 0.21434934437274933
iteration 266, loss = 0.1439404934644699
iteration 267, loss = 0.15301990509033203
iteration 268, loss = 0.1607377827167511
iteration 269, loss = 0.16871778666973114
iteration 270, loss = 0.15466831624507904
iteration 271, loss = 0.16321755945682526
iteration 272, loss = 0.1889227330684662
iteration 273, loss = 0.19547632336616516
iteration 274, loss = 0.14705239236354828
iteration 275, loss = 0.16678078472614288
iteration 276, loss = 0.20297521352767944
iteration 277, loss = 0.19380688667297363
iteration 278, loss = 0.1570475697517395
iteration 279, loss = 0.18244823813438416
iteration 280, loss = 0.1334889531135559
iteration 281, loss = 0.14170332252979279
iteration 282, loss = 0.15749453008174896
iteration 283, loss = 0.1610233634710312
iteration 284, loss = 0.19283097982406616
iteration 285, loss = 0.13027971982955933
iteration 286, loss = 0.1500619649887085
iteration 287, loss = 0.1772618591785431
iteration 288, loss = 0.18244297802448273
iteration 289, loss = 0.16017739474773407
iteration 290, loss = 0.16677308082580566
iteration 291, loss = 0.1528908908367157
iteration 292, loss = 0.16071432828903198
iteration 293, loss = 0.1587950885295868
iteration 294, loss = 0.15217706561088562
iteration 295, loss = 0.14990969002246857
iteration 296, loss = 0.1376301348209381
iteration 297, loss = 0.16783902049064636
iteration 298, loss = 0.16528776288032532
iteration 299, loss = 0.14569099247455597
iteration 0, loss = 0.1686849296092987
iteration 1, loss = 0.14456170797348022
iteration 2, loss = 0.15657244622707367
iteration 3, loss = 0.17835445702075958
iteration 4, loss = 0.1941062957048416
iteration 5, loss = 0.13632360100746155
iteration 6, loss = 0.1574873924255371
iteration 7, loss = 0.18383967876434326
iteration 8, loss = 0.13444095849990845
iteration 9, loss = 0.1404859870672226
iteration 10, loss = 0.15826088190078735
iteration 11, loss = 0.15307267010211945
iteration 12, loss = 0.15958121418952942
iteration 13, loss = 0.17946884036064148
iteration 14, loss = 0.16261491179466248
iteration 15, loss = 0.14279362559318542
iteration 16, loss = 0.17629402875900269
iteration 17, loss = 0.13763190805912018
iteration 18, loss = 0.19611312448978424
iteration 19, loss = 0.1790698915719986
iteration 20, loss = 0.1266596019268036
iteration 21, loss = 0.15414196252822876
iteration 22, loss = 0.1527733951807022
iteration 23, loss = 0.12694315612316132
iteration 24, loss = 0.14961281418800354
iteration 25, loss = 0.13792984187602997
iteration 26, loss = 0.15041792392730713
iteration 27, loss = 0.1834019422531128
iteration 28, loss = 0.17751044034957886
iteration 29, loss = 0.15905781090259552
iteration 30, loss = 0.18246950209140778
iteration 31, loss = 0.16465044021606445
iteration 32, loss = 0.17653316259384155
iteration 33, loss = 0.17023006081581116
iteration 34, loss = 0.16327440738677979
iteration 35, loss = 0.18618716299533844
iteration 36, loss = 0.1554882526397705
iteration 37, loss = 0.13443949818611145
iteration 38, loss = 0.15942710638046265
iteration 39, loss = 0.16577795147895813
iteration 40, loss = 0.12327432632446289
iteration 41, loss = 0.1936739832162857
iteration 42, loss = 0.16327999532222748
iteration 43, loss = 0.13775479793548584
iteration 44, loss = 0.15456533432006836
iteration 45, loss = 0.14707395434379578
iteration 46, loss = 0.14305667579174042
iteration 47, loss = 0.15274372696876526
iteration 48, loss = 0.1520078182220459
iteration 49, loss = 0.1602182686328888
iteration 50, loss = 0.15270763635635376
iteration 51, loss = 0.17848967015743256
iteration 52, loss = 0.14919105172157288
iteration 53, loss = 0.18051087856292725
iteration 54, loss = 0.19425617158412933
iteration 55, loss = 0.1477576196193695
iteration 56, loss = 0.15752318501472473
iteration 57, loss = 0.1607358753681183
iteration 58, loss = 0.15021663904190063
iteration 59, loss = 0.18391722440719604
iteration 60, loss = 0.18762332201004028
iteration 61, loss = 0.15705116093158722
iteration 62, loss = 0.1432948261499405
iteration 63, loss = 0.17909874022006989
iteration 64, loss = 0.1710326373577118
iteration 65, loss = 0.16043241322040558
iteration 66, loss = 0.1506098210811615
iteration 67, loss = 0.16144822537899017
iteration 68, loss = 0.14387944340705872
iteration 69, loss = 0.12893040478229523
iteration 70, loss = 0.1330360621213913
iteration 71, loss = 0.16425731778144836
iteration 72, loss = 0.18917050957679749
iteration 73, loss = 0.15593798458576202
iteration 74, loss = 0.17250776290893555
iteration 75, loss = 0.15733906626701355
iteration 76, loss = 0.16419757902622223
iteration 77, loss = 0.15729568898677826
iteration 78, loss = 0.1370810568332672
iteration 79, loss = 0.14571912586688995
iteration 80, loss = 0.13020646572113037
iteration 81, loss = 0.12773028016090393
iteration 82, loss = 0.15952730178833008
iteration 83, loss = 0.17518779635429382
iteration 84, loss = 0.12869958579540253
iteration 85, loss = 0.1475115865468979
iteration 86, loss = 0.13590386509895325
iteration 87, loss = 0.13183236122131348
iteration 88, loss = 0.1449624001979828
iteration 89, loss = 0.14917758107185364
iteration 90, loss = 0.12732930481433868
iteration 91, loss = 0.13649268448352814
iteration 92, loss = 0.12659479677677155
iteration 93, loss = 0.1324089616537094
iteration 94, loss = 0.14816302061080933
iteration 95, loss = 0.18878674507141113
iteration 96, loss = 0.1901172399520874
iteration 97, loss = 0.16849374771118164
iteration 98, loss = 0.14490477740764618
iteration 99, loss = 0.13797837495803833
iteration 100, loss = 0.16110527515411377
iteration 101, loss = 0.14171360433101654
iteration 102, loss = 0.16289030015468597
iteration 103, loss = 0.12152566015720367
iteration 104, loss = 0.18657895922660828
iteration 105, loss = 0.14150993525981903
iteration 106, loss = 0.1724109947681427
iteration 107, loss = 0.12831155955791473
iteration 108, loss = 0.17440961301326752
iteration 109, loss = 0.16910944879055023
iteration 110, loss = 0.1364770382642746
iteration 111, loss = 0.1617077887058258
iteration 112, loss = 0.14452224969863892
iteration 113, loss = 0.12080177664756775
iteration 114, loss = 0.14593344926834106
iteration 115, loss = 0.1818489283323288
iteration 116, loss = 0.14335085451602936
iteration 117, loss = 0.12935377657413483
iteration 118, loss = 0.13527484238147736
iteration 119, loss = 0.14424672722816467
iteration 120, loss = 0.13800033926963806
iteration 121, loss = 0.14052754640579224
iteration 122, loss = 0.1320933848619461
iteration 123, loss = 0.14561177790164948
iteration 124, loss = 0.13919594883918762
iteration 125, loss = 0.1488358974456787
iteration 126, loss = 0.15490317344665527
iteration 127, loss = 0.13519327342510223
iteration 128, loss = 0.1705835610628128
iteration 129, loss = 0.1445029377937317
iteration 130, loss = 0.1661856472492218
iteration 131, loss = 0.15767262876033783
iteration 132, loss = 0.18623891472816467
iteration 133, loss = 0.15949851274490356
iteration 134, loss = 0.19491581618785858
iteration 135, loss = 0.10979388654232025
iteration 136, loss = 0.134175643324852
iteration 137, loss = 0.15417756140232086
iteration 138, loss = 0.14878475666046143
iteration 139, loss = 0.13324637711048126
iteration 140, loss = 0.16229601204395294
iteration 141, loss = 0.15142768621444702
iteration 142, loss = 0.15370292961597443
iteration 143, loss = 0.16492047905921936
iteration 144, loss = 0.13748379051685333
iteration 145, loss = 0.12818557024002075
iteration 146, loss = 0.14286059141159058
iteration 147, loss = 0.1485024392604828
iteration 148, loss = 0.146318256855011
iteration 149, loss = 0.14945484697818756
iteration 150, loss = 0.15223316848278046
iteration 151, loss = 0.1510581523180008
iteration 152, loss = 0.1389865279197693
iteration 153, loss = 0.16139531135559082
iteration 154, loss = 0.14233967661857605
iteration 155, loss = 0.19536900520324707
iteration 156, loss = 0.14631687104701996
iteration 157, loss = 0.15723662078380585
iteration 158, loss = 0.17482075095176697
iteration 159, loss = 0.13283997774124146
iteration 160, loss = 0.15937137603759766
iteration 161, loss = 0.11679583787918091
iteration 162, loss = 0.13321758806705475
iteration 163, loss = 0.11475533246994019
iteration 164, loss = 0.13597336411476135
iteration 165, loss = 0.16546788811683655
iteration 166, loss = 0.11101584136486053
iteration 167, loss = 0.15111060440540314
iteration 168, loss = 0.16628605127334595
iteration 169, loss = 0.13555103540420532
iteration 170, loss = 0.11924725770950317
iteration 171, loss = 0.13932335376739502
iteration 172, loss = 0.17199483513832092
iteration 173, loss = 0.11258888244628906
iteration 174, loss = 0.14470921456813812
iteration 175, loss = 0.12197688221931458
iteration 176, loss = 0.152845099568367
iteration 177, loss = 0.14107200503349304
iteration 178, loss = 0.12671032547950745
iteration 179, loss = 0.13492301106452942
iteration 180, loss = 0.16202983260154724
iteration 181, loss = 0.11603080481290817
iteration 182, loss = 0.14816327393054962
iteration 183, loss = 0.12236857414245605
iteration 184, loss = 0.1465938687324524
iteration 185, loss = 0.11197397857904434
iteration 186, loss = 0.11121612787246704
iteration 187, loss = 0.14940020442008972
iteration 188, loss = 0.16493918001651764
iteration 189, loss = 0.14120244979858398
iteration 190, loss = 0.13661567866802216
iteration 191, loss = 0.17027467489242554
iteration 192, loss = 0.15636445581912994
iteration 193, loss = 0.19167278707027435
iteration 194, loss = 0.1161990761756897
iteration 195, loss = 0.147055521607399
iteration 196, loss = 0.1734631061553955
iteration 197, loss = 0.13704803586006165
iteration 198, loss = 0.13001899421215057
iteration 199, loss = 0.15328773856163025
iteration 200, loss = 0.11569824814796448
iteration 201, loss = 0.12210407108068466
iteration 202, loss = 0.16439275443553925
iteration 203, loss = 0.16747768223285675
iteration 204, loss = 0.12048115581274033
iteration 205, loss = 0.13167986273765564
iteration 206, loss = 0.15109775960445404
iteration 207, loss = 0.12228088825941086
iteration 208, loss = 0.1541379690170288
iteration 209, loss = 0.12347457557916641
iteration 210, loss = 0.16812947392463684
iteration 211, loss = 0.13728708028793335
iteration 212, loss = 0.15056434273719788
iteration 213, loss = 0.11340514570474625
iteration 214, loss = 0.12325721979141235
iteration 215, loss = 0.1279400885105133
iteration 216, loss = 0.13829734921455383
iteration 217, loss = 0.11819352209568024
iteration 218, loss = 0.147507905960083
iteration 219, loss = 0.1386946737766266
iteration 220, loss = 0.12609261274337769
iteration 221, loss = 0.13488967716693878
iteration 222, loss = 0.1335161030292511
iteration 223, loss = 0.16036848723888397
iteration 224, loss = 0.13879741728305817
iteration 225, loss = 0.10880067944526672
iteration 226, loss = 0.17022226750850677
iteration 227, loss = 0.1515321433544159
iteration 228, loss = 0.16295704245567322
iteration 229, loss = 0.13576427102088928
iteration 230, loss = 0.14419671893119812
iteration 231, loss = 0.1356169879436493
iteration 232, loss = 0.1515132486820221
iteration 233, loss = 0.1551920473575592
iteration 234, loss = 0.13824766874313354
iteration 235, loss = 0.14014498889446259
iteration 236, loss = 0.15076646208763123
iteration 237, loss = 0.13374583423137665
iteration 238, loss = 0.12027065455913544
iteration 239, loss = 0.17098169028759003
iteration 240, loss = 0.1443217396736145
iteration 241, loss = 0.13191023468971252
iteration 242, loss = 0.12714053690433502
iteration 243, loss = 0.14229628443717957
iteration 244, loss = 0.13823837041854858
iteration 245, loss = 0.12502634525299072
iteration 246, loss = 0.11217580735683441
iteration 247, loss = 0.13417328894138336
iteration 248, loss = 0.1442418098449707
iteration 249, loss = 0.14735296368598938
iteration 250, loss = 0.12810368835926056
iteration 251, loss = 0.13243894279003143
iteration 252, loss = 0.16763727366924286
iteration 253, loss = 0.10838397592306137
iteration 254, loss = 0.1423616111278534
iteration 255, loss = 0.13455164432525635
iteration 256, loss = 0.12125231325626373
iteration 257, loss = 0.12115156650543213
iteration 258, loss = 0.14049986004829407
iteration 259, loss = 0.11236521601676941
iteration 260, loss = 0.1339760273694992
iteration 261, loss = 0.13670586049556732
iteration 262, loss = 0.13961049914360046
iteration 263, loss = 0.12512093782424927
iteration 264, loss = 0.12767094373703003
iteration 265, loss = 0.11972403526306152
iteration 266, loss = 0.1361883580684662
iteration 267, loss = 0.1465655267238617
iteration 268, loss = 0.15180382132530212
iteration 269, loss = 0.1355133354663849
iteration 270, loss = 0.15302158892154694
iteration 271, loss = 0.11559287458658218
iteration 272, loss = 0.11259588599205017
iteration 273, loss = 0.11328847706317902
iteration 274, loss = 0.1234225183725357
iteration 275, loss = 0.1596013307571411
iteration 276, loss = 0.13032986223697662
iteration 277, loss = 0.1250491738319397
iteration 278, loss = 0.12192045152187347
iteration 279, loss = 0.15694579482078552
iteration 280, loss = 0.1747211217880249
iteration 281, loss = 0.13093985617160797
iteration 282, loss = 0.11216503381729126
iteration 283, loss = 0.1236310601234436
iteration 284, loss = 0.11564701795578003
iteration 285, loss = 0.11660610884428024
iteration 286, loss = 0.15014630556106567
iteration 287, loss = 0.12796925008296967
iteration 288, loss = 0.12032193690538406
iteration 289, loss = 0.1442847102880478
iteration 290, loss = 0.1591963917016983
iteration 291, loss = 0.1407616138458252
iteration 292, loss = 0.1299269199371338
iteration 293, loss = 0.13115452229976654
iteration 294, loss = 0.14245113730430603
iteration 295, loss = 0.13943366706371307
iteration 296, loss = 0.13079488277435303
iteration 297, loss = 0.1305321753025055
iteration 298, loss = 0.10513284802436829
iteration 299, loss = 0.1439487487077713
iteration 0, loss = 0.18547400832176208
iteration 1, loss = 0.11375230550765991
iteration 2, loss = 0.11888286471366882
iteration 3, loss = 0.14373905956745148
iteration 4, loss = 0.11556068062782288
iteration 5, loss = 0.11960113048553467
iteration 6, loss = 0.1186150386929512
iteration 7, loss = 0.1414586901664734
iteration 8, loss = 0.16028955578804016
iteration 9, loss = 0.13299116492271423
iteration 10, loss = 0.13146238029003143
iteration 11, loss = 0.10862980037927628
iteration 12, loss = 0.11349522322416306
iteration 13, loss = 0.14190031588077545
iteration 14, loss = 0.131038099527359
iteration 15, loss = 0.1474333554506302
iteration 16, loss = 0.13198606669902802
iteration 17, loss = 0.15236902236938477
iteration 18, loss = 0.11291664093732834
iteration 19, loss = 0.12125277519226074
iteration 20, loss = 0.1284225583076477
iteration 21, loss = 0.1285962462425232
iteration 22, loss = 0.16736605763435364
iteration 23, loss = 0.1121971532702446
iteration 24, loss = 0.15305763483047485
iteration 25, loss = 0.12416210770606995
iteration 26, loss = 0.10722870379686356
iteration 27, loss = 0.14452843368053436
iteration 28, loss = 0.1270240843296051
iteration 29, loss = 0.09956392645835876
iteration 30, loss = 0.12681350111961365
iteration 31, loss = 0.11080352216959
iteration 32, loss = 0.09934577345848083
iteration 33, loss = 0.13477641344070435
iteration 34, loss = 0.13870811462402344
iteration 35, loss = 0.14855177700519562
iteration 36, loss = 0.11268917471170425
iteration 37, loss = 0.13759395480155945
iteration 38, loss = 0.1129351556301117
iteration 39, loss = 0.12612444162368774
iteration 40, loss = 0.1284409761428833
iteration 41, loss = 0.13213306665420532
iteration 42, loss = 0.11654801666736603
iteration 43, loss = 0.1334526538848877
iteration 44, loss = 0.1401597112417221
iteration 45, loss = 0.09866473078727722
iteration 46, loss = 0.12271571159362793
iteration 47, loss = 0.12352634221315384
iteration 48, loss = 0.12876708805561066
iteration 49, loss = 0.11062842607498169
iteration 50, loss = 0.18700429797172546
iteration 51, loss = 0.10683155059814453
iteration 52, loss = 0.1160755306482315
iteration 53, loss = 0.14566048979759216
iteration 54, loss = 0.11323787271976471
iteration 55, loss = 0.10862362384796143
iteration 56, loss = 0.1448194682598114
iteration 57, loss = 0.16621515154838562
iteration 58, loss = 0.12501929700374603
iteration 59, loss = 0.17965620756149292
iteration 60, loss = 0.11090052127838135
iteration 61, loss = 0.1302746683359146
iteration 62, loss = 0.11575844883918762
iteration 63, loss = 0.14497345685958862
iteration 64, loss = 0.10497121512889862
iteration 65, loss = 0.1280701607465744
iteration 66, loss = 0.11733345687389374
iteration 67, loss = 0.12103772163391113
iteration 68, loss = 0.11707255989313126
iteration 69, loss = 0.14950473606586456
iteration 70, loss = 0.10987305641174316
iteration 71, loss = 0.13670572638511658
iteration 72, loss = 0.14172008633613586
iteration 73, loss = 0.16287195682525635
iteration 74, loss = 0.13297462463378906
iteration 75, loss = 0.10315109044313431
iteration 76, loss = 0.10099282115697861
iteration 77, loss = 0.09287001192569733
iteration 78, loss = 0.14328545331954956
iteration 79, loss = 0.10592382401227951
iteration 80, loss = 0.100794717669487
iteration 81, loss = 0.12210971862077713
iteration 82, loss = 0.1255345344543457
iteration 83, loss = 0.10561397671699524
iteration 84, loss = 0.12063319236040115
iteration 85, loss = 0.11325041949748993
iteration 86, loss = 0.124919593334198
iteration 87, loss = 0.1016344428062439
iteration 88, loss = 0.12329105287790298
iteration 89, loss = 0.14021311700344086
iteration 90, loss = 0.1052451953291893
iteration 91, loss = 0.14844673871994019
iteration 92, loss = 0.12108607590198517
iteration 93, loss = 0.10153521597385406
iteration 94, loss = 0.12317527830600739
iteration 95, loss = 0.09412738680839539
iteration 96, loss = 0.10239775478839874
iteration 97, loss = 0.1344226896762848
iteration 98, loss = 0.10529278963804245
iteration 99, loss = 0.11768355965614319
iteration 100, loss = 0.10610555112361908
iteration 101, loss = 0.1462581306695938
iteration 102, loss = 0.1169963851571083
iteration 103, loss = 0.13271337747573853
iteration 104, loss = 0.11725073307752609
iteration 105, loss = 0.12013649940490723
iteration 106, loss = 0.12196681648492813
iteration 107, loss = 0.1327853500843048
iteration 108, loss = 0.09980978071689606
iteration 109, loss = 0.12834975123405457
iteration 110, loss = 0.12342453747987747
iteration 111, loss = 0.12392111867666245
iteration 112, loss = 0.1407437026500702
iteration 113, loss = 0.10263454169034958
iteration 114, loss = 0.12540318071842194
iteration 115, loss = 0.1460193246603012
iteration 116, loss = 0.10786772519350052
iteration 117, loss = 0.09791702032089233
iteration 118, loss = 0.1128290593624115
iteration 119, loss = 0.11834005266427994
iteration 120, loss = 0.12229915708303452
iteration 121, loss = 0.13939370214939117
iteration 122, loss = 0.15088880062103271
iteration 123, loss = 0.14068064093589783
iteration 124, loss = 0.14798098802566528
iteration 125, loss = 0.09402773529291153
iteration 126, loss = 0.09595537930727005
iteration 127, loss = 0.148243248462677
iteration 128, loss = 0.13366679847240448
iteration 129, loss = 0.10880174487829208
iteration 130, loss = 0.12029081583023071
iteration 131, loss = 0.12628160417079926
iteration 132, loss = 0.11332152783870697
iteration 133, loss = 0.13322675228118896
iteration 134, loss = 0.11646632850170135
iteration 135, loss = 0.12689419090747833
iteration 136, loss = 0.0975080281496048
iteration 137, loss = 0.1433475911617279
iteration 138, loss = 0.13893432915210724
iteration 139, loss = 0.1089511513710022
iteration 140, loss = 0.13996747136116028
iteration 141, loss = 0.11120343953371048
iteration 142, loss = 0.11229615658521652
iteration 143, loss = 0.1079159528017044
iteration 144, loss = 0.1345764696598053
iteration 145, loss = 0.10485276579856873
iteration 146, loss = 0.12491732835769653
iteration 147, loss = 0.10005289316177368
iteration 148, loss = 0.13126878440380096
iteration 149, loss = 0.1346418559551239
iteration 150, loss = 0.100841224193573
iteration 151, loss = 0.09816984087228775
iteration 152, loss = 0.10790342092514038
iteration 153, loss = 0.12417168915271759
iteration 154, loss = 0.11352136731147766
iteration 155, loss = 0.1181766465306282
iteration 156, loss = 0.10191266983747482
iteration 157, loss = 0.13942494988441467
iteration 158, loss = 0.08488494157791138
iteration 159, loss = 0.11211211234331131
iteration 160, loss = 0.10757119953632355
iteration 161, loss = 0.11469186842441559
iteration 162, loss = 0.10382174700498581
iteration 163, loss = 0.11253873258829117
iteration 164, loss = 0.11383646726608276
iteration 165, loss = 0.1124810203909874
iteration 166, loss = 0.12263529747724533
iteration 167, loss = 0.12969793379306793
iteration 168, loss = 0.1466647982597351
iteration 169, loss = 0.12740686535835266
iteration 170, loss = 0.10067982971668243
iteration 171, loss = 0.11951970309019089
iteration 172, loss = 0.10346779972314835
iteration 173, loss = 0.10532873868942261
iteration 174, loss = 0.08996565639972687
iteration 175, loss = 0.1404537707567215
iteration 176, loss = 0.09056781232357025
iteration 177, loss = 0.14742599427700043
iteration 178, loss = 0.0980173647403717
iteration 179, loss = 0.1079992800951004
iteration 180, loss = 0.12520872056484222
iteration 181, loss = 0.12879380583763123
iteration 182, loss = 0.11213415861129761
iteration 183, loss = 0.11767887324094772
iteration 184, loss = 0.09820921719074249
iteration 185, loss = 0.1322053074836731
iteration 186, loss = 0.13013574481010437
iteration 187, loss = 0.14029468595981598
iteration 188, loss = 0.09448277950286865
iteration 189, loss = 0.09836463630199432
iteration 190, loss = 0.15140758454799652
iteration 191, loss = 0.10468321293592453
iteration 192, loss = 0.10024683177471161
iteration 193, loss = 0.11420838534832001
iteration 194, loss = 0.09892953187227249
iteration 195, loss = 0.11789841949939728
iteration 196, loss = 0.1182713583111763
iteration 197, loss = 0.09899235516786575
iteration 198, loss = 0.11311656981706619
iteration 199, loss = 0.12477201968431473
iteration 200, loss = 0.11104528605937958
iteration 201, loss = 0.12623967230319977
iteration 202, loss = 0.09695519506931305
iteration 203, loss = 0.11950143426656723
iteration 204, loss = 0.11972524970769882
iteration 205, loss = 0.16086938977241516
iteration 206, loss = 0.09715784341096878
iteration 207, loss = 0.11292131245136261
iteration 208, loss = 0.10930736362934113
iteration 209, loss = 0.16441106796264648
iteration 210, loss = 0.09087181836366653
iteration 211, loss = 0.10050338506698608
iteration 212, loss = 0.13233941793441772
iteration 213, loss = 0.11108317226171494
iteration 214, loss = 0.1110270693898201
iteration 215, loss = 0.10130950808525085
iteration 216, loss = 0.16158446669578552
iteration 217, loss = 0.09781038761138916
iteration 218, loss = 0.09673546999692917
iteration 219, loss = 0.11203345656394958
iteration 220, loss = 0.1228257343173027
iteration 221, loss = 0.10981331020593643
iteration 222, loss = 0.10181938856840134
iteration 223, loss = 0.12177906930446625
iteration 224, loss = 0.14187930524349213
iteration 225, loss = 0.12410181760787964
iteration 226, loss = 0.0997510552406311
iteration 227, loss = 0.10817576199769974
iteration 228, loss = 0.102498359978199
iteration 229, loss = 0.10325442999601364
iteration 230, loss = 0.09764812141656876
iteration 231, loss = 0.0841212272644043
iteration 232, loss = 0.09210395067930222
iteration 233, loss = 0.11203920841217041
iteration 234, loss = 0.11599814891815186
iteration 235, loss = 0.1421927958726883
iteration 236, loss = 0.11085967719554901
iteration 237, loss = 0.09957867860794067
iteration 238, loss = 0.1022762656211853
iteration 239, loss = 0.1155046820640564
iteration 240, loss = 0.1187758594751358
iteration 241, loss = 0.10594875365495682
iteration 242, loss = 0.0986260250210762
iteration 243, loss = 0.12128189206123352
iteration 244, loss = 0.1127101331949234
iteration 245, loss = 0.11881831288337708
iteration 246, loss = 0.10603700578212738
iteration 247, loss = 0.12981079518795013
iteration 248, loss = 0.1221604198217392
iteration 249, loss = 0.09740239381790161
iteration 250, loss = 0.11482317745685577
iteration 251, loss = 0.11312365531921387
iteration 252, loss = 0.1001800075173378
iteration 253, loss = 0.08517739176750183
iteration 254, loss = 0.10939695686101913
iteration 255, loss = 0.10389454662799835
iteration 256, loss = 0.11317450553178787
iteration 257, loss = 0.12302441149950027
iteration 258, loss = 0.11470156162977219
iteration 259, loss = 0.13814057409763336
iteration 260, loss = 0.09572599828243256
iteration 261, loss = 0.13592684268951416
iteration 262, loss = 0.10756391286849976
iteration 263, loss = 0.0808081403374672
iteration 264, loss = 0.09598536789417267
iteration 265, loss = 0.10287937521934509
iteration 266, loss = 0.10513493418693542
iteration 267, loss = 0.1422116607427597
iteration 268, loss = 0.11844502389431
iteration 269, loss = 0.09207536280155182
iteration 270, loss = 0.13044996559619904
iteration 271, loss = 0.11258919537067413
iteration 272, loss = 0.09811166673898697
iteration 273, loss = 0.09729534387588501
iteration 274, loss = 0.0908735990524292
iteration 275, loss = 0.10101400315761566
iteration 276, loss = 0.11420504748821259
iteration 277, loss = 0.10900788009166718
iteration 278, loss = 0.09681324660778046
iteration 279, loss = 0.11069856584072113
iteration 280, loss = 0.12511758506298065
iteration 281, loss = 0.12460276484489441
iteration 282, loss = 0.13972923159599304
iteration 283, loss = 0.10317277908325195
iteration 284, loss = 0.13333432376384735
iteration 285, loss = 0.0981006920337677
iteration 286, loss = 0.08838267624378204
iteration 287, loss = 0.0947447344660759
iteration 288, loss = 0.1191006600856781
iteration 289, loss = 0.11759498715400696
iteration 290, loss = 0.09713009744882584
iteration 291, loss = 0.09596984833478928
iteration 292, loss = 0.09535671770572662
iteration 293, loss = 0.11087173968553543
iteration 294, loss = 0.09319327771663666
iteration 295, loss = 0.1404019445180893
iteration 296, loss = 0.10678074508905411
iteration 297, loss = 0.12177865952253342
iteration 298, loss = 0.1191735565662384
iteration 299, loss = 0.10483097285032272
iteration 0, loss = 0.1039334237575531
iteration 1, loss = 0.0986839309334755
iteration 2, loss = 0.12843620777130127
iteration 3, loss = 0.11752837896347046
iteration 4, loss = 0.09380767494440079
iteration 5, loss = 0.09764169901609421
iteration 6, loss = 0.11109437793493271
iteration 7, loss = 0.1214936301112175
iteration 8, loss = 0.1177545040845871
iteration 9, loss = 0.09115102142095566
iteration 10, loss = 0.11625776439905167
iteration 11, loss = 0.12190385162830353
iteration 12, loss = 0.08959894627332687
iteration 13, loss = 0.13922648131847382
iteration 14, loss = 0.10033722221851349
iteration 15, loss = 0.12260887026786804
iteration 16, loss = 0.09022456407546997
iteration 17, loss = 0.09390230476856232
iteration 18, loss = 0.10675884783267975
iteration 19, loss = 0.09288080036640167
iteration 20, loss = 0.09489674866199493
iteration 21, loss = 0.095311738550663
iteration 22, loss = 0.12180668115615845
iteration 23, loss = 0.09326942265033722
iteration 24, loss = 0.12681446969509125
iteration 25, loss = 0.11499854922294617
iteration 26, loss = 0.09438289701938629
iteration 27, loss = 0.089826300740242
iteration 28, loss = 0.09784354269504547
iteration 29, loss = 0.11279132962226868
iteration 30, loss = 0.09611894935369492
iteration 31, loss = 0.08218962699174881
iteration 32, loss = 0.10117088258266449
iteration 33, loss = 0.08884412795305252
iteration 34, loss = 0.09327631443738937
iteration 35, loss = 0.10905065387487411
iteration 36, loss = 0.10123661905527115
iteration 37, loss = 0.08493176102638245
iteration 38, loss = 0.0871441662311554
iteration 39, loss = 0.12088329344987869
iteration 40, loss = 0.09381014108657837
iteration 41, loss = 0.10228976607322693
iteration 42, loss = 0.09799927473068237
iteration 43, loss = 0.09763424843549728
iteration 44, loss = 0.09229611605405807
iteration 45, loss = 0.08907466381788254
iteration 46, loss = 0.11690127104520798
iteration 47, loss = 0.11269696801900864
iteration 48, loss = 0.13333848118782043
iteration 49, loss = 0.1222880631685257
iteration 50, loss = 0.10632430016994476
iteration 51, loss = 0.10308217257261276
iteration 52, loss = 0.11532692611217499
iteration 53, loss = 0.07759784907102585
iteration 54, loss = 0.10216954350471497
iteration 55, loss = 0.10711726546287537
iteration 56, loss = 0.08232220262289047
iteration 57, loss = 0.10506298393011093
iteration 58, loss = 0.0954117700457573
iteration 59, loss = 0.127216637134552
iteration 60, loss = 0.08550295233726501
iteration 61, loss = 0.08534432202577591
iteration 62, loss = 0.11039531230926514
iteration 63, loss = 0.1467648148536682
iteration 64, loss = 0.11977581679821014
iteration 65, loss = 0.10567042976617813
iteration 66, loss = 0.08871517330408096
iteration 67, loss = 0.08681713044643402
iteration 68, loss = 0.10588319599628448
iteration 69, loss = 0.08508403599262238
iteration 70, loss = 0.11545804888010025
iteration 71, loss = 0.08330405503511429
iteration 72, loss = 0.11264972388744354
iteration 73, loss = 0.07698189467191696
iteration 74, loss = 0.12330266088247299
iteration 75, loss = 0.09346550703048706
iteration 76, loss = 0.12216964364051819
iteration 77, loss = 0.11154375970363617
iteration 78, loss = 0.09149368852376938
iteration 79, loss = 0.0835275650024414
iteration 80, loss = 0.11151441931724548
iteration 81, loss = 0.09575609862804413
iteration 82, loss = 0.0864301472902298
iteration 83, loss = 0.08978937566280365
iteration 84, loss = 0.09091244637966156
iteration 85, loss = 0.09746383875608444
iteration 86, loss = 0.11833585798740387
iteration 87, loss = 0.0920155793428421
iteration 88, loss = 0.1227455586194992
iteration 89, loss = 0.10657452046871185
iteration 90, loss = 0.08541881293058395
iteration 91, loss = 0.10840126872062683
iteration 92, loss = 0.09767123311758041
iteration 93, loss = 0.14853037893772125
iteration 94, loss = 0.09971415996551514
iteration 95, loss = 0.09335717558860779
iteration 96, loss = 0.1307070404291153
iteration 97, loss = 0.0883953720331192
iteration 98, loss = 0.0875256359577179
iteration 99, loss = 0.09635286778211594
iteration 100, loss = 0.10500720888376236
iteration 101, loss = 0.10150455683469772
iteration 102, loss = 0.09053793549537659
iteration 103, loss = 0.0714620053768158
iteration 104, loss = 0.08247988671064377
iteration 105, loss = 0.09567214548587799
iteration 106, loss = 0.10138550400733948
iteration 107, loss = 0.09024742245674133
iteration 108, loss = 0.09870785474777222
iteration 109, loss = 0.08167977631092072
iteration 110, loss = 0.07661854475736618
iteration 111, loss = 0.10861745476722717
iteration 112, loss = 0.08982664346694946
iteration 113, loss = 0.10420578718185425
iteration 114, loss = 0.0873410701751709
iteration 115, loss = 0.09653697907924652
iteration 116, loss = 0.09017619490623474
iteration 117, loss = 0.0890621542930603
iteration 118, loss = 0.1091795563697815
iteration 119, loss = 0.082633838057518
iteration 120, loss = 0.10234031081199646
iteration 121, loss = 0.09303748607635498
iteration 122, loss = 0.11986352503299713
iteration 123, loss = 0.1054852306842804
iteration 124, loss = 0.08249508589506149
iteration 125, loss = 0.11635894328355789
iteration 126, loss = 0.10805370658636093
iteration 127, loss = 0.09780590981245041
iteration 128, loss = 0.11812590807676315
iteration 129, loss = 0.08386456221342087
iteration 130, loss = 0.09163547307252884
iteration 131, loss = 0.11638541519641876
iteration 132, loss = 0.08270646631717682
iteration 133, loss = 0.09278165549039841
iteration 134, loss = 0.10573891550302505
iteration 135, loss = 0.11390084028244019
iteration 136, loss = 0.08249520510435104
iteration 137, loss = 0.10314635932445526
iteration 138, loss = 0.11071321368217468
iteration 139, loss = 0.08123064041137695
iteration 140, loss = 0.08442603051662445
iteration 141, loss = 0.07932154834270477
iteration 142, loss = 0.09773679822683334
iteration 143, loss = 0.10935411602258682
iteration 144, loss = 0.07551132142543793
iteration 145, loss = 0.09156997501850128
iteration 146, loss = 0.12553076446056366
iteration 147, loss = 0.10395990312099457
iteration 148, loss = 0.12593026459217072
iteration 149, loss = 0.09135362505912781
iteration 150, loss = 0.10405650734901428
iteration 151, loss = 0.07121096551418304
iteration 152, loss = 0.09912006556987762
iteration 153, loss = 0.07406406849622726
iteration 154, loss = 0.07446747273206711
iteration 155, loss = 0.1172277182340622
iteration 156, loss = 0.1144670695066452
iteration 157, loss = 0.08908139169216156
iteration 158, loss = 0.1337432563304901
iteration 159, loss = 0.12273333966732025
iteration 160, loss = 0.08827480673789978
iteration 161, loss = 0.09710772335529327
iteration 162, loss = 0.10020412504673004
iteration 163, loss = 0.1015823483467102
iteration 164, loss = 0.11007599532604218
iteration 165, loss = 0.08879750967025757
iteration 166, loss = 0.07468763738870621
iteration 167, loss = 0.09896044433116913
iteration 168, loss = 0.08282734453678131
iteration 169, loss = 0.10185596346855164
iteration 170, loss = 0.12728817760944366
iteration 171, loss = 0.11396308988332748
iteration 172, loss = 0.08832486718893051
iteration 173, loss = 0.09866807609796524
iteration 174, loss = 0.07855948060750961
iteration 175, loss = 0.0966382846236229
iteration 176, loss = 0.08246710896492004
iteration 177, loss = 0.07910209894180298
iteration 178, loss = 0.0877511277794838
iteration 179, loss = 0.07417209446430206
iteration 180, loss = 0.08117830753326416
iteration 181, loss = 0.08266991376876831
iteration 182, loss = 0.10199660062789917
iteration 183, loss = 0.09128375351428986
iteration 184, loss = 0.09809306263923645
iteration 185, loss = 0.07646992057561874
iteration 186, loss = 0.10594862699508667
iteration 187, loss = 0.0859319344162941
iteration 188, loss = 0.09672927111387253
iteration 189, loss = 0.12086465954780579
iteration 190, loss = 0.10002297163009644
iteration 191, loss = 0.10398370772600174
iteration 192, loss = 0.08239142596721649
iteration 193, loss = 0.07349054515361786
iteration 194, loss = 0.09742464125156403
iteration 195, loss = 0.09032654762268066
iteration 196, loss = 0.06799696385860443
iteration 197, loss = 0.09182439744472504
iteration 198, loss = 0.09920771420001984
iteration 199, loss = 0.11032317578792572
iteration 200, loss = 0.07466166466474533
iteration 201, loss = 0.09019804000854492
iteration 202, loss = 0.0865660011768341
iteration 203, loss = 0.09046270698308945
iteration 204, loss = 0.08256459981203079
iteration 205, loss = 0.10082405805587769
iteration 206, loss = 0.08002208918333054
iteration 207, loss = 0.08939673006534576
iteration 208, loss = 0.09004576504230499
iteration 209, loss = 0.09433326125144958
iteration 210, loss = 0.11027051508426666
iteration 211, loss = 0.09632483124732971
iteration 212, loss = 0.08103658258914948
iteration 213, loss = 0.07532401382923126
iteration 214, loss = 0.12033730000257492
iteration 215, loss = 0.07557743787765503
iteration 216, loss = 0.09206052869558334
iteration 217, loss = 0.09967172145843506
iteration 218, loss = 0.14363639056682587
iteration 219, loss = 0.09472821652889252
iteration 220, loss = 0.06994156539440155
iteration 221, loss = 0.11137361079454422
iteration 222, loss = 0.08482874929904938
iteration 223, loss = 0.06465622037649155
iteration 224, loss = 0.0827498733997345
iteration 225, loss = 0.07574789971113205
iteration 226, loss = 0.0723850354552269
iteration 227, loss = 0.128615140914917
iteration 228, loss = 0.09074236452579498
iteration 229, loss = 0.109124094247818
iteration 230, loss = 0.08745589107275009
iteration 231, loss = 0.09068741649389267
iteration 232, loss = 0.09001367539167404
iteration 233, loss = 0.07403351366519928
iteration 234, loss = 0.08695682883262634
iteration 235, loss = 0.08788570761680603
iteration 236, loss = 0.06654822081327438
iteration 237, loss = 0.08823543787002563
iteration 238, loss = 0.084429070353508
iteration 239, loss = 0.09597690403461456
iteration 240, loss = 0.09771765768527985
iteration 241, loss = 0.07655003666877747
iteration 242, loss = 0.08774550259113312
iteration 243, loss = 0.10046075284481049
iteration 244, loss = 0.10205378383398056
iteration 245, loss = 0.08723598718643188
iteration 246, loss = 0.10352526605129242
iteration 247, loss = 0.09139799326658249
iteration 248, loss = 0.0673123374581337
iteration 249, loss = 0.08759075403213501
iteration 250, loss = 0.09763036668300629
iteration 251, loss = 0.07488831877708435
iteration 252, loss = 0.10143710672855377
iteration 253, loss = 0.07965822517871857
iteration 254, loss = 0.10950318723917007
iteration 255, loss = 0.08551006019115448
iteration 256, loss = 0.08325198292732239
iteration 257, loss = 0.10577891767024994
iteration 258, loss = 0.08680080622434616
iteration 259, loss = 0.095411017537117
iteration 260, loss = 0.08746951073408127
iteration 261, loss = 0.08266865462064743
iteration 262, loss = 0.07305779308080673
iteration 263, loss = 0.10879174619913101
iteration 264, loss = 0.09313597530126572
iteration 265, loss = 0.09266723692417145
iteration 266, loss = 0.0890682116150856
iteration 267, loss = 0.0843852311372757
iteration 268, loss = 0.09269031137228012
iteration 269, loss = 0.10655543208122253
iteration 270, loss = 0.08175753057003021
iteration 271, loss = 0.08512446284294128
iteration 272, loss = 0.12727589905261993
iteration 273, loss = 0.06878788024187088
iteration 274, loss = 0.08563458919525146
iteration 275, loss = 0.11309836804866791
iteration 276, loss = 0.11750023812055588
iteration 277, loss = 0.10380461812019348
iteration 278, loss = 0.09942308068275452
iteration 279, loss = 0.08077722787857056
iteration 280, loss = 0.09351614117622375
iteration 281, loss = 0.06495577096939087
iteration 282, loss = 0.07214037328958511
iteration 283, loss = 0.08691172301769257
iteration 284, loss = 0.13087108731269836
iteration 285, loss = 0.07081881165504456
iteration 286, loss = 0.08085205405950546
iteration 287, loss = 0.10235263407230377
iteration 288, loss = 0.09522224962711334
iteration 289, loss = 0.11806424707174301
iteration 290, loss = 0.08271676301956177
iteration 291, loss = 0.07388193905353546
iteration 292, loss = 0.07766200602054596
iteration 293, loss = 0.08930571377277374
iteration 294, loss = 0.08985108882188797
iteration 295, loss = 0.08973479270935059
iteration 296, loss = 0.08853546530008316
iteration 297, loss = 0.12396235764026642
iteration 298, loss = 0.06864161789417267
iteration 299, loss = 0.08969596028327942
iteration 0, loss = 0.07541757822036743
iteration 1, loss = 0.07796227931976318
iteration 2, loss = 0.07936719805002213
iteration 3, loss = 0.06681771576404572
iteration 4, loss = 0.12267525494098663
iteration 5, loss = 0.08529086410999298
iteration 6, loss = 0.08164110779762268
iteration 7, loss = 0.09693343937397003
iteration 8, loss = 0.0755629688501358
iteration 9, loss = 0.10184511542320251
iteration 10, loss = 0.06248127669095993
iteration 11, loss = 0.09985499083995819
iteration 12, loss = 0.07172150909900665
iteration 13, loss = 0.11946029961109161
iteration 14, loss = 0.06844595819711685
iteration 15, loss = 0.0948474258184433
iteration 16, loss = 0.07593026757240295
iteration 17, loss = 0.07793670147657394
iteration 18, loss = 0.07900218665599823
iteration 19, loss = 0.06685169786214828
iteration 20, loss = 0.06700120866298676
iteration 21, loss = 0.09667077660560608
iteration 22, loss = 0.10274925082921982
iteration 23, loss = 0.0871233344078064
iteration 24, loss = 0.07397854328155518
iteration 25, loss = 0.070506252348423
iteration 26, loss = 0.0886988639831543
iteration 27, loss = 0.08133585005998611
iteration 28, loss = 0.08921022713184357
iteration 29, loss = 0.10274311900138855
iteration 30, loss = 0.10089640319347382
iteration 31, loss = 0.07593078911304474
iteration 32, loss = 0.0813661441206932
iteration 33, loss = 0.07163859903812408
iteration 34, loss = 0.07302217185497284
iteration 35, loss = 0.07998064160346985
iteration 36, loss = 0.08359455317258835
iteration 37, loss = 0.07826587557792664
iteration 38, loss = 0.06830994039773941
iteration 39, loss = 0.1104729175567627
iteration 40, loss = 0.11000075191259384
iteration 41, loss = 0.06404643505811691
iteration 42, loss = 0.11596021056175232
iteration 43, loss = 0.07749631255865097
iteration 44, loss = 0.09438471496105194
iteration 45, loss = 0.07739269733428955
iteration 46, loss = 0.0928812325000763
iteration 47, loss = 0.07030642777681351
iteration 48, loss = 0.09415962547063828
iteration 49, loss = 0.07306268811225891
iteration 50, loss = 0.0686362236738205
iteration 51, loss = 0.08054624497890472
iteration 52, loss = 0.09260000288486481
iteration 53, loss = 0.10647964477539062
iteration 54, loss = 0.09113181382417679
iteration 55, loss = 0.08421441912651062
iteration 56, loss = 0.06805667281150818
iteration 57, loss = 0.12245676666498184
iteration 58, loss = 0.12172453850507736
iteration 59, loss = 0.11816075444221497
iteration 60, loss = 0.08019959926605225
iteration 61, loss = 0.07353663444519043
iteration 62, loss = 0.075829416513443
iteration 63, loss = 0.08969663083553314
iteration 64, loss = 0.10155295580625534
iteration 65, loss = 0.09183555096387863
iteration 66, loss = 0.07173893600702286
iteration 67, loss = 0.0739113911986351
iteration 68, loss = 0.06053899973630905
iteration 69, loss = 0.07243849337100983
iteration 70, loss = 0.0857696533203125
iteration 71, loss = 0.07971218228340149
iteration 72, loss = 0.08739963173866272
iteration 73, loss = 0.09406358003616333
iteration 74, loss = 0.06454259902238846
iteration 75, loss = 0.07135111093521118
iteration 76, loss = 0.07456409186124802
iteration 77, loss = 0.07767253369092941
iteration 78, loss = 0.07723433524370193
iteration 79, loss = 0.07527920603752136
iteration 80, loss = 0.07428716868162155
iteration 81, loss = 0.10162456333637238
iteration 82, loss = 0.07803256064653397
iteration 83, loss = 0.07759163528680801
iteration 84, loss = 0.0988360345363617
iteration 85, loss = 0.0893097072839737
iteration 86, loss = 0.06949923932552338
iteration 87, loss = 0.08846697211265564
iteration 88, loss = 0.07988277077674866
iteration 89, loss = 0.0657513290643692
iteration 90, loss = 0.08894217014312744
iteration 91, loss = 0.07320591807365417
iteration 92, loss = 0.08288060873746872
iteration 93, loss = 0.0638689249753952
iteration 94, loss = 0.062513068318367
iteration 95, loss = 0.09860915690660477
iteration 96, loss = 0.08454131335020065
iteration 97, loss = 0.09711925685405731
iteration 98, loss = 0.07736270874738693
iteration 99, loss = 0.07275798916816711
iteration 100, loss = 0.07555785775184631
iteration 101, loss = 0.0816943347454071
iteration 102, loss = 0.09094992280006409
iteration 103, loss = 0.08187687397003174
iteration 104, loss = 0.07619067281484604
iteration 105, loss = 0.07935824990272522
iteration 106, loss = 0.07990632206201553
iteration 107, loss = 0.06750281900167465
iteration 108, loss = 0.08750264346599579
iteration 109, loss = 0.07139348983764648
iteration 110, loss = 0.08011434972286224
iteration 111, loss = 0.07622389495372772
iteration 112, loss = 0.0839688777923584
iteration 113, loss = 0.06843838840723038
iteration 114, loss = 0.07013402879238129
iteration 115, loss = 0.07682765275239944
iteration 116, loss = 0.08412119746208191
iteration 117, loss = 0.07788126170635223
iteration 118, loss = 0.09807296097278595
iteration 119, loss = 0.08555687963962555
iteration 120, loss = 0.06967893987894058
iteration 121, loss = 0.06923345476388931
iteration 122, loss = 0.07849044352769852
iteration 123, loss = 0.07163912802934647
iteration 124, loss = 0.07571155577898026
iteration 125, loss = 0.08091725409030914
iteration 126, loss = 0.09713238477706909
iteration 127, loss = 0.09484479576349258
iteration 128, loss = 0.08618880063295364
iteration 129, loss = 0.10178712755441666
iteration 130, loss = 0.06171611696481705
iteration 131, loss = 0.10497941821813583
iteration 132, loss = 0.07990995049476624
iteration 133, loss = 0.06353405117988586
iteration 134, loss = 0.08430899679660797
iteration 135, loss = 0.08655109256505966
iteration 136, loss = 0.07934054732322693
iteration 137, loss = 0.0715559870004654
iteration 138, loss = 0.08307373523712158
iteration 139, loss = 0.0695052444934845
iteration 140, loss = 0.07088243216276169
iteration 141, loss = 0.09022745490074158
iteration 142, loss = 0.0753980204463005
iteration 143, loss = 0.0702437236905098
iteration 144, loss = 0.09935439378023148
iteration 145, loss = 0.08572518080472946
iteration 146, loss = 0.05928498134016991
iteration 147, loss = 0.09380126744508743
iteration 148, loss = 0.06865542382001877
iteration 149, loss = 0.06798925995826721
iteration 150, loss = 0.0838380679488182
iteration 151, loss = 0.10151878744363785
iteration 152, loss = 0.08747752010822296
iteration 153, loss = 0.09114527702331543
iteration 154, loss = 0.08276044577360153
iteration 155, loss = 0.07349851727485657
iteration 156, loss = 0.09214826673269272
iteration 157, loss = 0.0903400108218193
iteration 158, loss = 0.06672514230012894
iteration 159, loss = 0.11016616970300674
iteration 160, loss = 0.08852780610322952
iteration 161, loss = 0.06832810491323471
iteration 162, loss = 0.06492442637681961
iteration 163, loss = 0.07638894766569138
iteration 164, loss = 0.10336003452539444
iteration 165, loss = 0.07440725713968277
iteration 166, loss = 0.09132520109415054
iteration 167, loss = 0.07603619247674942
iteration 168, loss = 0.07629194110631943
iteration 169, loss = 0.08783680200576782
iteration 170, loss = 0.06734663248062134
iteration 171, loss = 0.06297800689935684
iteration 172, loss = 0.06465636938810349
iteration 173, loss = 0.07035006582736969
iteration 174, loss = 0.07898010313510895
iteration 175, loss = 0.06162869557738304
iteration 176, loss = 0.0714077427983284
iteration 177, loss = 0.08063924312591553
iteration 178, loss = 0.07450652867555618
iteration 179, loss = 0.07867734134197235
iteration 180, loss = 0.07390817254781723
iteration 181, loss = 0.07843223214149475
iteration 182, loss = 0.06843984872102737
iteration 183, loss = 0.08865827322006226
iteration 184, loss = 0.08793138712644577
iteration 185, loss = 0.06748490035533905
iteration 186, loss = 0.07144735008478165
iteration 187, loss = 0.09208967536687851
iteration 188, loss = 0.07030761986970901
iteration 189, loss = 0.07020297646522522
iteration 190, loss = 0.08591336011886597
iteration 191, loss = 0.06721619516611099
iteration 192, loss = 0.10178743302822113
iteration 193, loss = 0.06697793304920197
iteration 194, loss = 0.07046554237604141
iteration 195, loss = 0.06599792838096619
iteration 196, loss = 0.058309007436037064
iteration 197, loss = 0.08052092790603638
iteration 198, loss = 0.07928507030010223
iteration 199, loss = 0.06480709463357925
iteration 200, loss = 0.08366532623767853
iteration 201, loss = 0.07339131087064743
iteration 202, loss = 0.0613868273794651
iteration 203, loss = 0.0642247125506401
iteration 204, loss = 0.09237648546695709
iteration 205, loss = 0.07483559846878052
iteration 206, loss = 0.09226170182228088
iteration 207, loss = 0.056203410029411316
iteration 208, loss = 0.05696369335055351
iteration 209, loss = 0.08008776605129242
iteration 210, loss = 0.07343202829360962
iteration 211, loss = 0.07363244891166687
iteration 212, loss = 0.0817994475364685
iteration 213, loss = 0.052248962223529816
iteration 214, loss = 0.055345166474580765
iteration 215, loss = 0.06545799225568771
iteration 216, loss = 0.0663716271519661
iteration 217, loss = 0.07412642240524292
iteration 218, loss = 0.08127173781394958
iteration 219, loss = 0.0659441351890564
iteration 220, loss = 0.07147266715765
iteration 221, loss = 0.06638213247060776
iteration 222, loss = 0.07672552019357681
iteration 223, loss = 0.06555841118097305
iteration 224, loss = 0.07431063055992126
iteration 225, loss = 0.07933790236711502
iteration 226, loss = 0.07411745190620422
iteration 227, loss = 0.06327646225690842
iteration 228, loss = 0.07863285392522812
iteration 229, loss = 0.08541746437549591
iteration 230, loss = 0.07641944289207458
iteration 231, loss = 0.0758015513420105
iteration 232, loss = 0.09667368978261948
iteration 233, loss = 0.06690048426389694
iteration 234, loss = 0.07402600347995758
iteration 235, loss = 0.06580865383148193
iteration 236, loss = 0.06531054526567459
iteration 237, loss = 0.06964671611785889
iteration 238, loss = 0.09948325157165527
iteration 239, loss = 0.1075686514377594
iteration 240, loss = 0.073533795773983
iteration 241, loss = 0.07655757665634155
iteration 242, loss = 0.09196411073207855
iteration 243, loss = 0.07381519675254822
iteration 244, loss = 0.0737362876534462
iteration 245, loss = 0.061363585293293
iteration 246, loss = 0.08610226958990097
iteration 247, loss = 0.07602080702781677
iteration 248, loss = 0.06599880009889603
iteration 249, loss = 0.06673593819141388
iteration 250, loss = 0.05865601450204849
iteration 251, loss = 0.07644771039485931
iteration 252, loss = 0.07686863839626312
iteration 253, loss = 0.08317989856004715
iteration 254, loss = 0.05637481063604355
iteration 255, loss = 0.07689876109361649
iteration 256, loss = 0.08359802514314651
iteration 257, loss = 0.06771548092365265
iteration 258, loss = 0.08458270132541656
iteration 259, loss = 0.06223681569099426
iteration 260, loss = 0.078067347407341
iteration 261, loss = 0.07295297086238861
iteration 262, loss = 0.07978340983390808
iteration 263, loss = 0.0672430619597435
iteration 264, loss = 0.09440270066261292
iteration 265, loss = 0.05896821618080139
iteration 266, loss = 0.0826948806643486
iteration 267, loss = 0.06517064571380615
iteration 268, loss = 0.0952688604593277
iteration 269, loss = 0.0663912296295166
iteration 270, loss = 0.0794392079114914
iteration 271, loss = 0.06283082067966461
iteration 272, loss = 0.08600960671901703
iteration 273, loss = 0.07207465171813965
iteration 274, loss = 0.053622517734766006
iteration 275, loss = 0.06335168331861496
iteration 276, loss = 0.07710911333560944
iteration 277, loss = 0.06570546329021454
iteration 278, loss = 0.0648687556385994
iteration 279, loss = 0.1080864816904068
iteration 280, loss = 0.08404050767421722
iteration 281, loss = 0.09648682922124863
iteration 282, loss = 0.05277778580784798
iteration 283, loss = 0.06330841034650803
iteration 284, loss = 0.09141361713409424
iteration 285, loss = 0.08067866414785385
iteration 286, loss = 0.06358544528484344
iteration 287, loss = 0.053516048938035965
iteration 288, loss = 0.07695197314023972
iteration 289, loss = 0.07677073776721954
iteration 290, loss = 0.08058292418718338
iteration 291, loss = 0.08372125774621964
iteration 292, loss = 0.06441153585910797
iteration 293, loss = 0.05847834050655365
iteration 294, loss = 0.07895860821008682
iteration 295, loss = 0.08786910772323608
iteration 296, loss = 0.0755639597773552
iteration 297, loss = 0.10887353122234344
iteration 298, loss = 0.08890245109796524
iteration 299, loss = 0.10707522928714752
iteration 0, loss = 0.05731509253382683
iteration 1, loss = 0.08483511209487915
iteration 2, loss = 0.07746425271034241
iteration 3, loss = 0.07584378868341446
iteration 4, loss = 0.0708354040980339
iteration 5, loss = 0.06641580909490585
iteration 6, loss = 0.10183955729007721
iteration 7, loss = 0.07680605351924896
iteration 8, loss = 0.07011744379997253
iteration 9, loss = 0.06589241325855255
iteration 10, loss = 0.06303880363702774
iteration 11, loss = 0.0630275085568428
iteration 12, loss = 0.09060824662446976
iteration 13, loss = 0.0794096291065216
iteration 14, loss = 0.07080475240945816
iteration 15, loss = 0.0607437938451767
iteration 16, loss = 0.07716329395771027
iteration 17, loss = 0.08043371886014938
iteration 18, loss = 0.05294894799590111
iteration 19, loss = 0.0722109004855156
iteration 20, loss = 0.08347468078136444
iteration 21, loss = 0.07317724823951721
iteration 22, loss = 0.06250883638858795
iteration 23, loss = 0.08126062899827957
iteration 24, loss = 0.07222642004489899
iteration 25, loss = 0.08478685468435287
iteration 26, loss = 0.07397431135177612
iteration 27, loss = 0.054464299231767654
iteration 28, loss = 0.06176411360502243
iteration 29, loss = 0.05364074185490608
iteration 30, loss = 0.06119194254279137
iteration 31, loss = 0.06882356852293015
iteration 32, loss = 0.05781864374876022
iteration 33, loss = 0.06750662624835968
iteration 34, loss = 0.06248066946864128
iteration 35, loss = 0.07189000397920609
iteration 36, loss = 0.07087390124797821
iteration 37, loss = 0.08444565534591675
iteration 38, loss = 0.059647656977176666
iteration 39, loss = 0.06576080620288849
iteration 40, loss = 0.09504285454750061
iteration 41, loss = 0.09586110711097717
iteration 42, loss = 0.053400591015815735
iteration 43, loss = 0.08244094997644424
iteration 44, loss = 0.06718508899211884
iteration 45, loss = 0.07059870660305023
iteration 46, loss = 0.05640381947159767
iteration 47, loss = 0.07228124141693115
iteration 48, loss = 0.07857252657413483
iteration 49, loss = 0.06578754633665085
iteration 50, loss = 0.09894497692584991
iteration 51, loss = 0.05664269998669624
iteration 52, loss = 0.07214096188545227
iteration 53, loss = 0.08623796701431274
iteration 54, loss = 0.06150008365511894
iteration 55, loss = 0.05062325671315193
iteration 56, loss = 0.05879613012075424
iteration 57, loss = 0.07658390700817108
iteration 58, loss = 0.09078215807676315
iteration 59, loss = 0.07091818004846573
iteration 60, loss = 0.0714084580540657
iteration 61, loss = 0.08402401953935623
iteration 62, loss = 0.06228577345609665
iteration 63, loss = 0.07770241796970367
iteration 64, loss = 0.06915310770273209
iteration 65, loss = 0.06896573305130005
iteration 66, loss = 0.05379755049943924
iteration 67, loss = 0.09532666206359863
iteration 68, loss = 0.06705939769744873
iteration 69, loss = 0.08561652898788452
iteration 70, loss = 0.05545973777770996
iteration 71, loss = 0.07255819439888
iteration 72, loss = 0.07983655482530594
iteration 73, loss = 0.06507065147161484
iteration 74, loss = 0.05746764689683914
iteration 75, loss = 0.06975530833005905
iteration 76, loss = 0.05577128008008003
iteration 77, loss = 0.06676989048719406
iteration 78, loss = 0.08595475554466248
iteration 79, loss = 0.0700824037194252
iteration 80, loss = 0.06934452801942825
iteration 81, loss = 0.0567912720143795
iteration 82, loss = 0.05228116363286972
iteration 83, loss = 0.07770106941461563
iteration 84, loss = 0.06910422444343567
iteration 85, loss = 0.0657510980963707
iteration 86, loss = 0.0657629743218422
iteration 87, loss = 0.06902939826250076
iteration 88, loss = 0.06127607077360153
iteration 89, loss = 0.0807504951953888
iteration 90, loss = 0.06275231391191483
iteration 91, loss = 0.07064861804246902
iteration 92, loss = 0.07064981758594513
iteration 93, loss = 0.0727553591132164
iteration 94, loss = 0.06823564320802689
iteration 95, loss = 0.05862041190266609
iteration 96, loss = 0.05428828299045563
iteration 97, loss = 0.05814395844936371
iteration 98, loss = 0.07328926771879196
iteration 99, loss = 0.06145137920975685
iteration 100, loss = 0.05561145395040512
iteration 101, loss = 0.07047443836927414
iteration 102, loss = 0.05272596329450607
iteration 103, loss = 0.06774680316448212
iteration 104, loss = 0.06152127683162689
iteration 105, loss = 0.05748199671506882
iteration 106, loss = 0.06449773907661438
iteration 107, loss = 0.0820111632347107
iteration 108, loss = 0.08082473278045654
iteration 109, loss = 0.08851023763418198
iteration 110, loss = 0.07240109145641327
iteration 111, loss = 0.06847791373729706
iteration 112, loss = 0.052980709820985794
iteration 113, loss = 0.06864576041698456
iteration 114, loss = 0.055227771401405334
iteration 115, loss = 0.05701560527086258
iteration 116, loss = 0.05467154085636139
iteration 117, loss = 0.07614593952894211
iteration 118, loss = 0.059128616005182266
iteration 119, loss = 0.0879838764667511
iteration 120, loss = 0.07786239683628082
iteration 121, loss = 0.06641866266727448
iteration 122, loss = 0.05891396850347519
iteration 123, loss = 0.06402050703763962
iteration 124, loss = 0.07132520526647568
iteration 125, loss = 0.07704363018274307
iteration 126, loss = 0.08698110282421112
iteration 127, loss = 0.05865383520722389
iteration 128, loss = 0.056358709931373596
iteration 129, loss = 0.05725093558430672
iteration 130, loss = 0.05690019577741623
iteration 131, loss = 0.06855382025241852
iteration 132, loss = 0.06475932896137238
iteration 133, loss = 0.05406642705202103
iteration 134, loss = 0.05328896641731262
iteration 135, loss = 0.07568003237247467
iteration 136, loss = 0.06923659145832062
iteration 137, loss = 0.0686429813504219
iteration 138, loss = 0.05216185748577118
iteration 139, loss = 0.05457756668329239
iteration 140, loss = 0.05527712032198906
iteration 141, loss = 0.05535544827580452
iteration 142, loss = 0.04762609675526619
iteration 143, loss = 0.0678127110004425
iteration 144, loss = 0.07164563238620758
iteration 145, loss = 0.05039713531732559
iteration 146, loss = 0.07073413580656052
iteration 147, loss = 0.08125048130750656
iteration 148, loss = 0.06262095272541046
iteration 149, loss = 0.053825195878744125
iteration 150, loss = 0.09306424856185913
iteration 151, loss = 0.04939298331737518
iteration 152, loss = 0.05993916466832161
iteration 153, loss = 0.05710645765066147
iteration 154, loss = 0.07524941116571426
iteration 155, loss = 0.06686656177043915
iteration 156, loss = 0.06424043327569962
iteration 157, loss = 0.06547758728265762
iteration 158, loss = 0.07842874526977539
iteration 159, loss = 0.06326676160097122
iteration 160, loss = 0.0563427172601223
iteration 161, loss = 0.0759853646159172
iteration 162, loss = 0.06208239868283272
iteration 163, loss = 0.05515412986278534
iteration 164, loss = 0.06177752837538719
iteration 165, loss = 0.07341517508029938
iteration 166, loss = 0.07021355628967285
iteration 167, loss = 0.05646955594420433
iteration 168, loss = 0.06508444994688034
iteration 169, loss = 0.07925718277692795
iteration 170, loss = 0.07348015904426575
iteration 171, loss = 0.054296914488077164
iteration 172, loss = 0.06707354635000229
iteration 173, loss = 0.06160951778292656
iteration 174, loss = 0.05421971529722214
iteration 175, loss = 0.04900151863694191
iteration 176, loss = 0.06581177562475204
iteration 177, loss = 0.060710400342941284
iteration 178, loss = 0.07141591608524323
iteration 179, loss = 0.07636453211307526
iteration 180, loss = 0.07239685207605362
iteration 181, loss = 0.07098332792520523
iteration 182, loss = 0.04800073429942131
iteration 183, loss = 0.06268179416656494
iteration 184, loss = 0.06257130205631256
iteration 185, loss = 0.0551053062081337
iteration 186, loss = 0.07727152109146118
iteration 187, loss = 0.07119281589984894
iteration 188, loss = 0.06858064234256744
iteration 189, loss = 0.05884403735399246
iteration 190, loss = 0.07496511936187744
iteration 191, loss = 0.06512978672981262
iteration 192, loss = 0.05544693022966385
iteration 193, loss = 0.0686335563659668
iteration 194, loss = 0.06135222315788269
iteration 195, loss = 0.05437168478965759
iteration 196, loss = 0.08070999383926392
iteration 197, loss = 0.0731717050075531
iteration 198, loss = 0.058330338448286057
iteration 199, loss = 0.08785950392484665
iteration 200, loss = 0.05800090357661247
iteration 201, loss = 0.06661830842494965
iteration 202, loss = 0.06400890648365021
iteration 203, loss = 0.0577702596783638
iteration 204, loss = 0.04615249112248421
iteration 205, loss = 0.054399725049734116
iteration 206, loss = 0.0747704729437828
iteration 207, loss = 0.06107983738183975
iteration 208, loss = 0.07381114363670349
iteration 209, loss = 0.05371830612421036
iteration 210, loss = 0.04402666538953781
iteration 211, loss = 0.07182427495718002
iteration 212, loss = 0.046981148421764374
iteration 213, loss = 0.05554665997624397
iteration 214, loss = 0.06038203090429306
iteration 215, loss = 0.0579574853181839
iteration 216, loss = 0.06671331822872162
iteration 217, loss = 0.06500019878149033
iteration 218, loss = 0.08096951991319656
iteration 219, loss = 0.05237099528312683
iteration 220, loss = 0.0675586611032486
iteration 221, loss = 0.053968265652656555
iteration 222, loss = 0.04753110185265541
iteration 223, loss = 0.05506758391857147
iteration 224, loss = 0.07019484788179398
iteration 225, loss = 0.05310240760445595
iteration 226, loss = 0.05941925197839737
iteration 227, loss = 0.06307278573513031
iteration 228, loss = 0.08196071535348892
iteration 229, loss = 0.0640493780374527
iteration 230, loss = 0.08377528190612793
iteration 231, loss = 0.05387723445892334
iteration 232, loss = 0.057875894010066986
iteration 233, loss = 0.05953361093997955
iteration 234, loss = 0.07204179465770721
iteration 235, loss = 0.051387883722782135
iteration 236, loss = 0.052785031497478485
iteration 237, loss = 0.065631203353405
iteration 238, loss = 0.05161861330270767
iteration 239, loss = 0.06445015966892242
iteration 240, loss = 0.052677638828754425
iteration 241, loss = 0.056606195867061615
iteration 242, loss = 0.06650598347187042
iteration 243, loss = 0.057739801704883575
iteration 244, loss = 0.050978679209947586
iteration 245, loss = 0.06045619025826454
iteration 246, loss = 0.05561903864145279
iteration 247, loss = 0.058631252497434616
iteration 248, loss = 0.044784460216760635
iteration 249, loss = 0.06573908776044846
iteration 250, loss = 0.05613361671566963
iteration 251, loss = 0.07401350885629654
iteration 252, loss = 0.10007914155721664
iteration 253, loss = 0.07797808945178986
iteration 254, loss = 0.05697742849588394
iteration 255, loss = 0.0756475031375885
iteration 256, loss = 0.06917345523834229
iteration 257, loss = 0.05175807699561119
iteration 258, loss = 0.07073673605918884
iteration 259, loss = 0.05027933791279793
iteration 260, loss = 0.0630914494395256
iteration 261, loss = 0.0687856674194336
iteration 262, loss = 0.06076072156429291
iteration 263, loss = 0.04987673833966255
iteration 264, loss = 0.06722947955131531
iteration 265, loss = 0.048951003700494766
iteration 266, loss = 0.07937231659889221
iteration 267, loss = 0.04749099910259247
iteration 268, loss = 0.07049283385276794
iteration 269, loss = 0.05966392531991005
iteration 270, loss = 0.07128516584634781
iteration 271, loss = 0.049072135239839554
iteration 272, loss = 0.05246157571673393
iteration 273, loss = 0.04542692378163338
iteration 274, loss = 0.061879709362983704
iteration 275, loss = 0.0766678974032402
iteration 276, loss = 0.08480750024318695
iteration 277, loss = 0.06225600093603134
iteration 278, loss = 0.062202610075473785
iteration 279, loss = 0.048918746411800385
iteration 280, loss = 0.04999313876032829
iteration 281, loss = 0.06733771413564682
iteration 282, loss = 0.06811505556106567
iteration 283, loss = 0.05520087853074074
iteration 284, loss = 0.07900969684123993
iteration 285, loss = 0.049172740429639816
iteration 286, loss = 0.05979441851377487
iteration 287, loss = 0.07701831310987473
iteration 288, loss = 0.06649629026651382
iteration 289, loss = 0.058925122022628784
iteration 290, loss = 0.04630152881145477
iteration 291, loss = 0.06451141834259033
iteration 292, loss = 0.06932423263788223
iteration 293, loss = 0.0491141602396965
iteration 294, loss = 0.04466579854488373
iteration 295, loss = 0.04502958804368973
iteration 296, loss = 0.06416358053684235
iteration 297, loss = 0.05760643258690834
iteration 298, loss = 0.07367199659347534
iteration 299, loss = 0.05121825635433197
iteration 0, loss = 0.06842945516109467
iteration 1, loss = 0.04811818152666092
iteration 2, loss = 0.09414111077785492
iteration 3, loss = 0.046109430491924286
iteration 4, loss = 0.04616401717066765
iteration 5, loss = 0.047707561403512955
iteration 6, loss = 0.06812898814678192
iteration 7, loss = 0.05216566100716591
iteration 8, loss = 0.05942210555076599
iteration 9, loss = 0.05549483001232147
iteration 10, loss = 0.07997467368841171
iteration 11, loss = 0.06573615968227386
iteration 12, loss = 0.053452521562576294
iteration 13, loss = 0.047919243574142456
iteration 14, loss = 0.05053034797310829
iteration 15, loss = 0.050151921808719635
iteration 16, loss = 0.06304919719696045
iteration 17, loss = 0.04543853551149368
iteration 18, loss = 0.06934815645217896
iteration 19, loss = 0.07680489122867584
iteration 20, loss = 0.06054092198610306
iteration 21, loss = 0.06934820115566254
iteration 22, loss = 0.06434088945388794
iteration 23, loss = 0.06838253885507584
iteration 24, loss = 0.047129154205322266
iteration 25, loss = 0.05773623287677765
iteration 26, loss = 0.05876301974058151
iteration 27, loss = 0.061760544776916504
iteration 28, loss = 0.05060260370373726
iteration 29, loss = 0.057079337537288666
iteration 30, loss = 0.057127028703689575
iteration 31, loss = 0.047944575548172
iteration 32, loss = 0.0621974840760231
iteration 33, loss = 0.057605333626270294
iteration 34, loss = 0.04388478025794029
iteration 35, loss = 0.0560118742287159
iteration 36, loss = 0.05101754143834114
iteration 37, loss = 0.06867142021656036
iteration 38, loss = 0.07660078257322311
iteration 39, loss = 0.0492888018488884
iteration 40, loss = 0.07001221925020218
iteration 41, loss = 0.044032104313373566
iteration 42, loss = 0.08692869544029236
iteration 43, loss = 0.04408596083521843
iteration 44, loss = 0.07603543996810913
iteration 45, loss = 0.072905033826828
iteration 46, loss = 0.085902139544487
iteration 47, loss = 0.053075019270181656
iteration 48, loss = 0.05236952751874924
iteration 49, loss = 0.07255588471889496
iteration 50, loss = 0.044830143451690674
iteration 51, loss = 0.046165890991687775
iteration 52, loss = 0.05219237506389618
iteration 53, loss = 0.050940051674842834
iteration 54, loss = 0.048254065215587616
iteration 55, loss = 0.07701055705547333
iteration 56, loss = 0.08208422362804413
iteration 57, loss = 0.052319880574941635
iteration 58, loss = 0.05777148902416229
iteration 59, loss = 0.0534251406788826
iteration 60, loss = 0.1020520031452179
iteration 61, loss = 0.053206026554107666
iteration 62, loss = 0.07401169836521149
iteration 63, loss = 0.03941730782389641
iteration 64, loss = 0.061979990452528
iteration 65, loss = 0.06230098754167557
iteration 66, loss = 0.041331954300403595
iteration 67, loss = 0.06478535383939743
iteration 68, loss = 0.04797689616680145
iteration 69, loss = 0.06410489231348038
iteration 70, loss = 0.04820982366800308
iteration 71, loss = 0.04287831112742424
iteration 72, loss = 0.0457775853574276
iteration 73, loss = 0.08717739582061768
iteration 74, loss = 0.05831969529390335
iteration 75, loss = 0.06994731724262238
iteration 76, loss = 0.04476383328437805
iteration 77, loss = 0.05247262865304947
iteration 78, loss = 0.0633348822593689
iteration 79, loss = 0.05947992205619812
iteration 80, loss = 0.05233632028102875
iteration 81, loss = 0.048813626170158386
iteration 82, loss = 0.05418073385953903
iteration 83, loss = 0.05906089395284653
iteration 84, loss = 0.049735233187675476
iteration 85, loss = 0.06680125743150711
iteration 86, loss = 0.04885205253958702
iteration 87, loss = 0.055246636271476746
iteration 88, loss = 0.06357090175151825
iteration 89, loss = 0.04298048093914986
iteration 90, loss = 0.04576485976576805
iteration 91, loss = 0.06806988269090652
iteration 92, loss = 0.062198199331760406
iteration 93, loss = 0.04565168172121048
iteration 94, loss = 0.04992000013589859
iteration 95, loss = 0.06353474408388138
iteration 96, loss = 0.06437000632286072
iteration 97, loss = 0.05387980863451958
iteration 98, loss = 0.06154930219054222
iteration 99, loss = 0.05332062020897865
iteration 100, loss = 0.06590437889099121
iteration 101, loss = 0.06502977758646011
iteration 102, loss = 0.055585216730833054
iteration 103, loss = 0.06129688024520874
iteration 104, loss = 0.08056920021772385
iteration 105, loss = 0.05619450658559799
iteration 106, loss = 0.05539640784263611
iteration 107, loss = 0.05115870013833046
iteration 108, loss = 0.05698775127530098
iteration 109, loss = 0.044839054346084595
iteration 110, loss = 0.07451312243938446
iteration 111, loss = 0.05510107800364494
iteration 112, loss = 0.05001559853553772
iteration 113, loss = 0.049504056572914124
iteration 114, loss = 0.07522916793823242
iteration 115, loss = 0.051112622022628784
iteration 116, loss = 0.04544180631637573
iteration 117, loss = 0.04870449751615524
iteration 118, loss = 0.058839812874794006
iteration 119, loss = 0.06189338490366936
iteration 120, loss = 0.04334793984889984
iteration 121, loss = 0.049159735441207886
iteration 122, loss = 0.050265464931726456
iteration 123, loss = 0.056043557822704315
iteration 124, loss = 0.05170769244432449
iteration 125, loss = 0.05781635269522667
iteration 126, loss = 0.04795012250542641
iteration 127, loss = 0.06660633534193039
iteration 128, loss = 0.043588872998952866
iteration 129, loss = 0.050110768526792526
iteration 130, loss = 0.037635162472724915
iteration 131, loss = 0.059132423251867294
iteration 132, loss = 0.0502135306596756
iteration 133, loss = 0.07664250582456589
iteration 134, loss = 0.037547871470451355
iteration 135, loss = 0.07629740238189697
iteration 136, loss = 0.06986506283283234
iteration 137, loss = 0.06262379884719849
iteration 138, loss = 0.060151368379592896
iteration 139, loss = 0.05092430114746094
iteration 140, loss = 0.049670930951833725
iteration 141, loss = 0.046078067272901535
iteration 142, loss = 0.04981272295117378
iteration 143, loss = 0.05120237171649933
iteration 144, loss = 0.04950888827443123
iteration 145, loss = 0.044770497828722
iteration 146, loss = 0.052831653505563736
iteration 147, loss = 0.03941831737756729
iteration 148, loss = 0.05916336178779602
iteration 149, loss = 0.06874758005142212
iteration 150, loss = 0.04447231441736221
iteration 151, loss = 0.06039176881313324
iteration 152, loss = 0.053639963269233704
iteration 153, loss = 0.05907059460878372
iteration 154, loss = 0.06295858323574066
iteration 155, loss = 0.05427542328834534
iteration 156, loss = 0.0633973702788353
iteration 157, loss = 0.04496737942099571
iteration 158, loss = 0.046820223331451416
iteration 159, loss = 0.06181727349758148
iteration 160, loss = 0.06465429812669754
iteration 161, loss = 0.05118075758218765
iteration 162, loss = 0.04976367950439453
iteration 163, loss = 0.052719179540872574
iteration 164, loss = 0.04448661208152771
iteration 165, loss = 0.052709806710481644
iteration 166, loss = 0.060906801372766495
iteration 167, loss = 0.05582483485341072
iteration 168, loss = 0.0434143990278244
iteration 169, loss = 0.043969396501779556
iteration 170, loss = 0.0606287345290184
iteration 171, loss = 0.07521045953035355
iteration 172, loss = 0.0541180819272995
iteration 173, loss = 0.04882095754146576
iteration 174, loss = 0.05000557005405426
iteration 175, loss = 0.050450388342142105
iteration 176, loss = 0.04636666178703308
iteration 177, loss = 0.04906336963176727
iteration 178, loss = 0.06968078017234802
iteration 179, loss = 0.04979879409074783
iteration 180, loss = 0.05727526545524597
iteration 181, loss = 0.05013466253876686
iteration 182, loss = 0.05074339359998703
iteration 183, loss = 0.06691642105579376
iteration 184, loss = 0.04746850207448006
iteration 185, loss = 0.053892832249403
iteration 186, loss = 0.051604948937892914
iteration 187, loss = 0.04519844055175781
iteration 188, loss = 0.05592565983533859
iteration 189, loss = 0.05955607444047928
iteration 190, loss = 0.048414021730422974
iteration 191, loss = 0.09024576097726822
iteration 192, loss = 0.048351023346185684
iteration 193, loss = 0.0571998655796051
iteration 194, loss = 0.04578850790858269
iteration 195, loss = 0.07105731219053268
iteration 196, loss = 0.05031503736972809
iteration 197, loss = 0.04737572371959686
iteration 198, loss = 0.05916108563542366
iteration 199, loss = 0.06221604719758034
iteration 200, loss = 0.054516032338142395
iteration 201, loss = 0.04670839011669159
iteration 202, loss = 0.049492672085762024
iteration 203, loss = 0.0567270889878273
iteration 204, loss = 0.04090854525566101
iteration 205, loss = 0.04409351199865341
iteration 206, loss = 0.048129357397556305
iteration 207, loss = 0.045634374022483826
iteration 208, loss = 0.055588070303201675
iteration 209, loss = 0.06397613137960434
iteration 210, loss = 0.04226682707667351
iteration 211, loss = 0.0700887143611908
iteration 212, loss = 0.061208706349134445
iteration 213, loss = 0.0379156693816185
iteration 214, loss = 0.040349047631025314
iteration 215, loss = 0.06276196241378784
iteration 216, loss = 0.046452708542346954
iteration 217, loss = 0.04441314935684204
iteration 218, loss = 0.06577522307634354
iteration 219, loss = 0.05490116402506828
iteration 220, loss = 0.06424917280673981
iteration 221, loss = 0.06000824272632599
iteration 222, loss = 0.051582712680101395
iteration 223, loss = 0.04108486324548721
iteration 224, loss = 0.049138545989990234
iteration 225, loss = 0.06030888855457306
iteration 226, loss = 0.044361330568790436
iteration 227, loss = 0.04887263849377632
iteration 228, loss = 0.05340957269072533
iteration 229, loss = 0.04336312785744667
iteration 230, loss = 0.043112099170684814
iteration 231, loss = 0.0513811819255352
iteration 232, loss = 0.03791877627372742
iteration 233, loss = 0.04538062959909439
iteration 234, loss = 0.05010046809911728
iteration 235, loss = 0.04342428967356682
iteration 236, loss = 0.04881653934717178
iteration 237, loss = 0.06334903836250305
iteration 238, loss = 0.048644181340932846
iteration 239, loss = 0.05322921276092529
iteration 240, loss = 0.06295862793922424
iteration 241, loss = 0.06212104111909866
iteration 242, loss = 0.05955226719379425
iteration 243, loss = 0.05782414972782135
iteration 244, loss = 0.04821360483765602
iteration 245, loss = 0.05588846281170845
iteration 246, loss = 0.042880136519670486
iteration 247, loss = 0.04518301039934158
iteration 248, loss = 0.04679052531719208
iteration 249, loss = 0.05055477097630501
iteration 250, loss = 0.05083758756518364
iteration 251, loss = 0.052203599363565445
iteration 252, loss = 0.042078327387571335
iteration 253, loss = 0.04990355670452118
iteration 254, loss = 0.05591396614909172
iteration 255, loss = 0.05306970328092575
iteration 256, loss = 0.04155295342206955
iteration 257, loss = 0.049064479768276215
iteration 258, loss = 0.06008885055780411
iteration 259, loss = 0.07444906234741211
iteration 260, loss = 0.04080130532383919
iteration 261, loss = 0.050262611359357834
iteration 262, loss = 0.06315183639526367
iteration 263, loss = 0.04452943056821823
iteration 264, loss = 0.04864293709397316
iteration 265, loss = 0.04711880907416344
iteration 266, loss = 0.04708285257220268
iteration 267, loss = 0.03802959620952606
iteration 268, loss = 0.04088779538869858
iteration 269, loss = 0.054154329001903534
iteration 270, loss = 0.05031467229127884
iteration 271, loss = 0.0585397407412529
iteration 272, loss = 0.042163699865341187
iteration 273, loss = 0.045175954699516296
iteration 274, loss = 0.052426137030124664
iteration 275, loss = 0.050785619765520096
iteration 276, loss = 0.0485718734562397
iteration 277, loss = 0.0603095144033432
iteration 278, loss = 0.04821149632334709
iteration 279, loss = 0.04598090425133705
iteration 280, loss = 0.056643154472112656
iteration 281, loss = 0.04658442735671997
iteration 282, loss = 0.058294013142585754
iteration 283, loss = 0.04961070418357849
iteration 284, loss = 0.04125332087278366
iteration 285, loss = 0.05331353843212128
iteration 286, loss = 0.04936951398849487
iteration 287, loss = 0.053280387073755264
iteration 288, loss = 0.04962772876024246
iteration 289, loss = 0.03728416934609413
iteration 290, loss = 0.05012503266334534
iteration 291, loss = 0.03641529008746147
iteration 292, loss = 0.05848615989089012
iteration 293, loss = 0.04660606011748314
iteration 294, loss = 0.042186152189970016
iteration 295, loss = 0.045785319060087204
iteration 296, loss = 0.05257725715637207
iteration 297, loss = 0.05564055219292641
iteration 298, loss = 0.06002971529960632
iteration 299, loss = 0.0383513942360878

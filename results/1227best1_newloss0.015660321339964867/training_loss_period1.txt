iteration 1, loss = 2.6509833335876465
iteration 2, loss = 2.679619312286377
iteration 3, loss = 2.64764404296875
iteration 4, loss = 2.6359264850616455
iteration 5, loss = 2.555396556854248
iteration 6, loss = 2.5342674255371094
iteration 7, loss = 2.5158331394195557
iteration 8, loss = 2.4600512981414795
iteration 9, loss = 2.4260947704315186
iteration 10, loss = 2.3892927169799805
iteration 11, loss = 2.325028896331787
iteration 12, loss = 2.293109655380249
iteration 13, loss = 2.186612606048584
iteration 14, loss = 2.1266748905181885
iteration 15, loss = 2.0989553928375244
iteration 16, loss = 2.0994160175323486
iteration 17, loss = 1.9823411703109741
iteration 18, loss = 1.9434685707092285
iteration 19, loss = 1.8145296573638916
iteration 20, loss = 1.8748713731765747
iteration 21, loss = 1.7771450281143188
iteration 22, loss = 1.7781003713607788
iteration 23, loss = 1.6978638172149658
iteration 24, loss = 1.5649977922439575
iteration 25, loss = 1.6076682806015015
iteration 26, loss = 1.6069583892822266
iteration 27, loss = 1.332944393157959
iteration 28, loss = 1.4467947483062744
iteration 29, loss = 1.4046680927276611
iteration 30, loss = 1.3231182098388672
iteration 31, loss = 1.3461918830871582
iteration 32, loss = 1.1410918235778809
iteration 33, loss = 1.3014098405838013
iteration 34, loss = 1.2517036199569702
iteration 35, loss = 1.2617979049682617
iteration 36, loss = 1.1134520769119263
iteration 37, loss = 1.0855447053909302
iteration 38, loss = 1.004662036895752
iteration 39, loss = 1.113140344619751
iteration 40, loss = 1.1451375484466553
iteration 41, loss = 1.1042654514312744
iteration 42, loss = 1.014351487159729
iteration 43, loss = 1.0836312770843506
iteration 44, loss = 0.9906461238861084
iteration 45, loss = 0.9623383283615112
iteration 46, loss = 1.0576987266540527
iteration 47, loss = 0.9886000156402588
iteration 48, loss = 0.8903566002845764
iteration 49, loss = 0.920996367931366
iteration 50, loss = 0.8891509175300598
iteration 51, loss = 0.8831696510314941
iteration 52, loss = 0.9013025760650635
iteration 53, loss = 0.8963375091552734
iteration 54, loss = 0.9011157751083374
iteration 55, loss = 0.8842386603355408
iteration 56, loss = 0.9047605991363525
iteration 57, loss = 0.8260222673416138
iteration 58, loss = 0.8043479919433594
iteration 59, loss = 0.77071213722229
iteration 60, loss = 0.8006283044815063
iteration 61, loss = 0.8777280449867249
iteration 62, loss = 0.7346920967102051
iteration 63, loss = 0.8070919513702393
iteration 64, loss = 0.7395694851875305
iteration 65, loss = 0.802346408367157
iteration 66, loss = 0.7902556657791138
iteration 67, loss = 0.7424737215042114
iteration 68, loss = 0.7800966501235962
iteration 69, loss = 0.7709554433822632
iteration 70, loss = 0.7951052188873291
iteration 71, loss = 0.759337306022644
iteration 72, loss = 0.7978000640869141
iteration 73, loss = 0.7562533020973206
iteration 74, loss = 0.7216236591339111
iteration 75, loss = 0.7785170078277588
iteration 76, loss = 0.7155879735946655
iteration 77, loss = 0.7468901872634888
iteration 78, loss = 0.7742618918418884
iteration 79, loss = 0.7527324557304382
iteration 80, loss = 0.726130485534668
iteration 81, loss = 0.7537070512771606
iteration 82, loss = 0.7309385538101196
iteration 83, loss = 0.742027997970581
iteration 84, loss = 0.7810788750648499
iteration 85, loss = 0.7669705748558044
iteration 86, loss = 0.792568564414978
iteration 87, loss = 0.698783278465271
iteration 88, loss = 0.7329725027084351
iteration 89, loss = 0.741292417049408
iteration 90, loss = 0.7343838214874268
iteration 91, loss = 0.6710944175720215
iteration 92, loss = 0.727666437625885
iteration 93, loss = 0.7634027004241943
iteration 94, loss = 0.7239909768104553
iteration 95, loss = 0.7356489896774292
iteration 96, loss = 0.7121790051460266
iteration 97, loss = 0.743454098701477
iteration 98, loss = 0.7380877137184143
iteration 99, loss = 0.73194819688797
iteration 100, loss = 0.7204850912094116
iteration 101, loss = 0.7130784392356873
iteration 102, loss = 0.7132313251495361
iteration 103, loss = 0.6877338290214539
iteration 104, loss = 0.7029682993888855
iteration 105, loss = 0.6738654971122742
iteration 106, loss = 0.7046332359313965
iteration 107, loss = 0.6811987161636353
iteration 108, loss = 0.6838900446891785
iteration 109, loss = 0.7228823900222778
iteration 110, loss = 0.693932056427002
iteration 111, loss = 0.6832358837127686
iteration 112, loss = 0.6593618988990784
iteration 113, loss = 0.6678062677383423
iteration 114, loss = 0.7245349884033203
iteration 115, loss = 0.7050641179084778
iteration 116, loss = 0.7237843871116638
iteration 117, loss = 0.6983866691589355
iteration 118, loss = 0.6460414528846741
iteration 119, loss = 0.6860090494155884
iteration 120, loss = 0.6670176386833191
iteration 121, loss = 0.6748765707015991
iteration 122, loss = 0.7104319334030151
iteration 123, loss = 0.6657578945159912
iteration 124, loss = 0.7203291654586792
iteration 125, loss = 0.685871958732605
iteration 126, loss = 0.6775429844856262
iteration 127, loss = 0.6898690462112427
iteration 128, loss = 0.7229442596435547
iteration 129, loss = 0.6939998865127563
iteration 130, loss = 0.7024744749069214
iteration 131, loss = 0.7131205797195435
iteration 132, loss = 0.6738619208335876
iteration 133, loss = 0.7482364177703857
iteration 134, loss = 0.657407820224762
iteration 135, loss = 0.6867013573646545
iteration 136, loss = 0.6322238445281982
iteration 137, loss = 0.6626900434494019
iteration 138, loss = 0.6947832703590393
iteration 139, loss = 0.6986910104751587
iteration 140, loss = 0.690710723400116
iteration 141, loss = 0.711456835269928
iteration 142, loss = 0.6844907999038696
iteration 143, loss = 0.653537929058075
iteration 144, loss = 0.6901594400405884
iteration 145, loss = 0.6732786893844604
iteration 146, loss = 0.6806938052177429
iteration 147, loss = 0.6826181411743164
iteration 148, loss = 0.6857835054397583
iteration 149, loss = 0.6480397582054138
iteration 150, loss = 0.6781983375549316
iteration 151, loss = 0.6895536184310913
iteration 152, loss = 0.6487759947776794
iteration 153, loss = 0.6467968821525574
iteration 154, loss = 0.6607037782669067
iteration 155, loss = 0.6692185401916504
iteration 156, loss = 0.665349006652832
iteration 157, loss = 0.7086472511291504
iteration 158, loss = 0.6779930591583252
iteration 159, loss = 0.6395054459571838
iteration 160, loss = 0.6817131042480469
iteration 161, loss = 0.6713467240333557
iteration 162, loss = 0.6539482474327087
iteration 163, loss = 0.6954541206359863
iteration 164, loss = 0.6260815858840942
iteration 165, loss = 0.6397691965103149
iteration 166, loss = 0.6528992652893066
iteration 167, loss = 0.6513091325759888
iteration 168, loss = 0.6444664001464844
iteration 169, loss = 0.6394776701927185
iteration 170, loss = 0.6412556767463684
iteration 171, loss = 0.6373915672302246
iteration 172, loss = 0.6446977257728577
iteration 173, loss = 0.6545823216438293
iteration 174, loss = 0.6779336929321289
iteration 175, loss = 0.6337373852729797
iteration 176, loss = 0.6300023794174194
iteration 177, loss = 0.6580758690834045
iteration 178, loss = 0.6668745875358582
iteration 179, loss = 0.6592445373535156
iteration 180, loss = 0.6946035623550415
iteration 181, loss = 0.6199211478233337
iteration 182, loss = 0.5915441513061523
iteration 183, loss = 0.6511551141738892
iteration 184, loss = 0.6561055183410645
iteration 185, loss = 0.6246046423912048
iteration 186, loss = 0.6275747418403625
iteration 187, loss = 0.6497325897216797
iteration 188, loss = 0.6093676686286926
iteration 189, loss = 0.6117238998413086
iteration 190, loss = 0.6213154196739197
iteration 191, loss = 0.6677430868148804
iteration 192, loss = 0.5910109281539917
iteration 193, loss = 0.6138372421264648
iteration 194, loss = 0.6372807025909424
iteration 195, loss = 0.6044378280639648
iteration 196, loss = 0.6214811205863953
iteration 197, loss = 0.6295990943908691
iteration 198, loss = 0.6328203082084656
iteration 199, loss = 0.632388174533844
iteration 200, loss = 0.6087663769721985
iteration 201, loss = 0.6555541157722473
iteration 202, loss = 0.6629800796508789
iteration 203, loss = 0.6318197846412659
iteration 204, loss = 0.6172917485237122
iteration 205, loss = 0.6491245031356812
iteration 206, loss = 0.6526844501495361
iteration 207, loss = 0.6217437386512756
iteration 208, loss = 0.637841522693634
iteration 209, loss = 0.6545135974884033
iteration 210, loss = 0.6059675812721252
iteration 211, loss = 0.6418474912643433
iteration 212, loss = 0.6416252851486206
iteration 213, loss = 0.5838783979415894
iteration 214, loss = 0.6011264324188232
iteration 215, loss = 0.6361533403396606
iteration 216, loss = 0.6023152470588684
iteration 217, loss = 0.6183540225028992
iteration 218, loss = 0.6122499108314514
iteration 219, loss = 0.6573330163955688
iteration 220, loss = 0.6337392926216125
iteration 221, loss = 0.6247197389602661
iteration 222, loss = 0.6238896250724792
iteration 223, loss = 0.6198296546936035
iteration 224, loss = 0.6158750057220459
iteration 225, loss = 0.6150403618812561
iteration 226, loss = 0.6037710905075073
iteration 227, loss = 0.5666387677192688
iteration 228, loss = 0.6033669710159302
iteration 229, loss = 0.5961636304855347
iteration 230, loss = 0.606255829334259
iteration 231, loss = 0.6141877770423889
iteration 232, loss = 0.5955349802970886
iteration 233, loss = 0.6139394044876099
iteration 234, loss = 0.6155149936676025
iteration 235, loss = 0.6178971529006958
iteration 236, loss = 0.5820038914680481
iteration 237, loss = 0.6261746883392334
iteration 238, loss = 0.5980131030082703
iteration 239, loss = 0.619118869304657
iteration 240, loss = 0.6041936874389648
iteration 241, loss = 0.6012297868728638
iteration 242, loss = 0.571182131767273
iteration 243, loss = 0.6243916749954224
iteration 244, loss = 0.6359964609146118
iteration 245, loss = 0.6564317345619202
iteration 246, loss = 0.6197121143341064
iteration 247, loss = 0.5777397751808167
iteration 248, loss = 0.5477238297462463
iteration 249, loss = 0.6064738035202026
iteration 250, loss = 0.6232625246047974
iteration 251, loss = 0.6137495636940002
iteration 252, loss = 0.5842922925949097
iteration 253, loss = 0.6051923632621765
iteration 254, loss = 0.5746378302574158
iteration 255, loss = 0.6292011737823486
iteration 256, loss = 0.6162593364715576
iteration 257, loss = 0.6463000774383545
iteration 258, loss = 0.5483089685440063
iteration 259, loss = 0.5597151517868042
iteration 260, loss = 0.541040301322937
iteration 261, loss = 0.5781095623970032
iteration 262, loss = 0.571762204170227
iteration 263, loss = 0.5639666318893433
iteration 264, loss = 0.5513995289802551
iteration 265, loss = 0.5438770055770874
iteration 266, loss = 0.5545499324798584
iteration 267, loss = 0.5468477010726929
iteration 268, loss = 0.5968068242073059
iteration 269, loss = 0.5211430191993713
iteration 270, loss = 0.5531231164932251
iteration 271, loss = 0.5414121747016907
iteration 272, loss = 0.5957539081573486
iteration 273, loss = 0.5608147978782654
iteration 274, loss = 0.5307543277740479
iteration 275, loss = 0.5540946125984192
iteration 276, loss = 0.5344123244285583
iteration 277, loss = 0.551558792591095
iteration 278, loss = 0.5771285891532898
iteration 279, loss = 0.5871027708053589
iteration 280, loss = 0.5980493426322937
iteration 281, loss = 0.5815171003341675
iteration 282, loss = 0.5924050211906433
iteration 283, loss = 0.5189141035079956
iteration 284, loss = 0.5768764019012451
iteration 285, loss = 0.5496057868003845
iteration 286, loss = 0.5428130626678467
iteration 287, loss = 0.5222008228302002
iteration 288, loss = 0.5183722376823425
iteration 289, loss = 0.5385949015617371
iteration 290, loss = 0.5303224325180054
iteration 291, loss = 0.5819493532180786
iteration 292, loss = 0.5202573537826538
iteration 293, loss = 0.5532589554786682
iteration 294, loss = 0.5027170181274414
iteration 295, loss = 0.5890728831291199
iteration 296, loss = 0.5324609875679016
iteration 297, loss = 0.5349584817886353
iteration 298, loss = 0.5148964524269104
iteration 299, loss = 0.5203092694282532
iteration 300, loss = 0.5429019331932068
iteration 1, loss = 0.5443935990333557
iteration 2, loss = 0.5218766331672668
iteration 3, loss = 0.5369989275932312
iteration 4, loss = 0.5370997190475464
iteration 5, loss = 0.5014312267303467
iteration 6, loss = 0.47043830156326294
iteration 7, loss = 0.5515149235725403
iteration 8, loss = 0.5456810593605042
iteration 9, loss = 0.5282627940177917
iteration 10, loss = 0.5708218812942505
iteration 11, loss = 0.5658489465713501
iteration 12, loss = 0.5277339816093445
iteration 13, loss = 0.5318988561630249
iteration 14, loss = 0.5267649292945862
iteration 15, loss = 0.5187898874282837
iteration 16, loss = 0.5232931971549988
iteration 17, loss = 0.5376771688461304
iteration 18, loss = 0.5187705755233765
iteration 19, loss = 0.48800554871559143
iteration 20, loss = 0.5163722038269043
iteration 21, loss = 0.4822332262992859
iteration 22, loss = 0.4861890971660614
iteration 23, loss = 0.5179457068443298
iteration 24, loss = 0.5014917850494385
iteration 25, loss = 0.547746479511261
iteration 26, loss = 0.4931070804595947
iteration 27, loss = 0.4961916506290436
iteration 28, loss = 0.4504730701446533
iteration 29, loss = 0.49606311321258545
iteration 30, loss = 0.4784286618232727
iteration 31, loss = 0.47840970754623413
iteration 32, loss = 0.5118582844734192
iteration 33, loss = 0.5132006406784058
iteration 34, loss = 0.5401707291603088
iteration 35, loss = 0.5026812553405762
iteration 36, loss = 0.5117413997650146
iteration 37, loss = 0.5539186596870422
iteration 38, loss = 0.470626562833786
iteration 39, loss = 0.4528326690196991
iteration 40, loss = 0.5245445370674133
iteration 41, loss = 0.46967247128486633
iteration 42, loss = 0.5019601583480835
iteration 43, loss = 0.5016956925392151
iteration 44, loss = 0.4595353305339813
iteration 45, loss = 0.51358962059021
iteration 46, loss = 0.4513147473335266
iteration 47, loss = 0.5102910399436951
iteration 48, loss = 0.44565239548683167
iteration 49, loss = 0.49912551045417786
iteration 50, loss = 0.5162551999092102
iteration 51, loss = 0.4704819917678833
iteration 52, loss = 0.46287745237350464
iteration 53, loss = 0.4662599265575409
iteration 54, loss = 0.4635773003101349
iteration 55, loss = 0.48801830410957336
iteration 56, loss = 0.4866214394569397
iteration 57, loss = 0.4886656403541565
iteration 58, loss = 0.4364626109600067
iteration 59, loss = 0.4378717243671417
iteration 60, loss = 0.4697376489639282
iteration 61, loss = 0.4983972907066345
iteration 62, loss = 0.5012049078941345
iteration 63, loss = 0.462388277053833
iteration 64, loss = 0.4688694477081299
iteration 65, loss = 0.44263193011283875
iteration 66, loss = 0.46869707107543945
iteration 67, loss = 0.43526703119277954
iteration 68, loss = 0.41454172134399414
iteration 69, loss = 0.4693046808242798
iteration 70, loss = 0.43382465839385986
iteration 71, loss = 0.4448181986808777
iteration 72, loss = 0.47196176648139954
iteration 73, loss = 0.4604127109050751
iteration 74, loss = 0.47201499342918396
iteration 75, loss = 0.4207336902618408
iteration 76, loss = 0.4122769236564636
iteration 77, loss = 0.456525057554245
iteration 78, loss = 0.46985816955566406
iteration 79, loss = 0.4390907883644104
iteration 80, loss = 0.45391491055488586
iteration 81, loss = 0.47566163539886475
iteration 82, loss = 0.434619665145874
iteration 83, loss = 0.425645112991333
iteration 84, loss = 0.4472848176956177
iteration 85, loss = 0.44840869307518005
iteration 86, loss = 0.4988929033279419
iteration 87, loss = 0.4403190016746521
iteration 88, loss = 0.4872710704803467
iteration 89, loss = 0.45959407091140747
iteration 90, loss = 0.45211321115493774
iteration 91, loss = 0.42529165744781494
iteration 92, loss = 0.4712667465209961
iteration 93, loss = 0.4057905077934265
iteration 94, loss = 0.43139275908470154
iteration 95, loss = 0.4183868169784546
iteration 96, loss = 0.5139803886413574
iteration 97, loss = 0.40253984928131104
iteration 98, loss = 0.431130051612854
iteration 99, loss = 0.3829130232334137
iteration 100, loss = 0.4306090176105499
iteration 101, loss = 0.3988206386566162
iteration 102, loss = 0.40210774540901184
iteration 103, loss = 0.4022313952445984
iteration 104, loss = 0.416368305683136
iteration 105, loss = 0.47826722264289856
iteration 106, loss = 0.3936483860015869
iteration 107, loss = 0.4281569719314575
iteration 108, loss = 0.40755677223205566
iteration 109, loss = 0.42353978753089905
iteration 110, loss = 0.39998626708984375
iteration 111, loss = 0.4162909686565399
iteration 112, loss = 0.4357125759124756
iteration 113, loss = 0.3990887701511383
iteration 114, loss = 0.37355223298072815
iteration 115, loss = 0.43598097562789917
iteration 116, loss = 0.41136571764945984
iteration 117, loss = 0.43204087018966675
iteration 118, loss = 0.4298442602157593
iteration 119, loss = 0.4410315155982971
iteration 120, loss = 0.4302087426185608
iteration 121, loss = 0.4023565351963043
iteration 122, loss = 0.36829715967178345
iteration 123, loss = 0.4588998556137085
iteration 124, loss = 0.37797096371650696
iteration 125, loss = 0.3672272562980652
iteration 126, loss = 0.36913394927978516
iteration 127, loss = 0.39914900064468384
iteration 128, loss = 0.4227362275123596
iteration 129, loss = 0.3712344765663147
iteration 130, loss = 0.39476627111434937
iteration 131, loss = 0.4053472876548767
iteration 132, loss = 0.42217522859573364
iteration 133, loss = 0.35890859365463257
iteration 134, loss = 0.36860281229019165
iteration 135, loss = 0.4001826345920563
iteration 136, loss = 0.5012785196304321
iteration 137, loss = 0.4044771194458008
iteration 138, loss = 0.34462815523147583
iteration 139, loss = 0.41905510425567627
iteration 140, loss = 0.39096158742904663
iteration 141, loss = 0.39895111322402954
iteration 142, loss = 0.34299522638320923
iteration 143, loss = 0.36313191056251526
iteration 144, loss = 0.38128167390823364
iteration 145, loss = 0.3703981041908264
iteration 146, loss = 0.34782904386520386
iteration 147, loss = 0.3642859160900116
iteration 148, loss = 0.3934541940689087
iteration 149, loss = 0.40292537212371826
iteration 150, loss = 0.37310630083084106
iteration 151, loss = 0.3703387379646301
iteration 152, loss = 0.3353973925113678
iteration 153, loss = 0.3873193562030792
iteration 154, loss = 0.3228694498538971
iteration 155, loss = 0.34065306186676025
iteration 156, loss = 0.40877148509025574
iteration 157, loss = 0.33885714411735535
iteration 158, loss = 0.31509077548980713
iteration 159, loss = 0.35815533995628357
iteration 160, loss = 0.39905843138694763
iteration 161, loss = 0.42465317249298096
iteration 162, loss = 0.36938342452049255
iteration 163, loss = 0.3719649910926819
iteration 164, loss = 0.32983267307281494
iteration 165, loss = 0.3819315433502197
iteration 166, loss = 0.29700028896331787
iteration 167, loss = 0.34928280115127563
iteration 168, loss = 0.35443487763404846
iteration 169, loss = 0.333036869764328
iteration 170, loss = 0.3469443917274475
iteration 171, loss = 0.3446766138076782
iteration 172, loss = 0.3227265477180481
iteration 173, loss = 0.3941529095172882
iteration 174, loss = 0.3228563070297241
iteration 175, loss = 0.37092071771621704
iteration 176, loss = 0.37645062804222107
iteration 177, loss = 0.3412490487098694
iteration 178, loss = 0.34059369564056396
iteration 179, loss = 0.33718788623809814
iteration 180, loss = 0.3285544514656067
iteration 181, loss = 0.32483676075935364
iteration 182, loss = 0.39765235781669617
iteration 183, loss = 0.34083259105682373
iteration 184, loss = 0.3261759281158447
iteration 185, loss = 0.341325581073761
iteration 186, loss = 0.31526076793670654
iteration 187, loss = 0.3321945071220398
iteration 188, loss = 0.3150283992290497
iteration 189, loss = 0.3001936972141266
iteration 190, loss = 0.3499126732349396
iteration 191, loss = 0.34646183252334595
iteration 192, loss = 0.37569066882133484
iteration 193, loss = 0.31560438871383667
iteration 194, loss = 0.3514367341995239
iteration 195, loss = 0.3173949718475342
iteration 196, loss = 0.2955027222633362
iteration 197, loss = 0.34291791915893555
iteration 198, loss = 0.2967555522918701
iteration 199, loss = 0.2870877981185913
iteration 200, loss = 0.3102008104324341
iteration 201, loss = 0.2862878441810608
iteration 202, loss = 0.2869664430618286
iteration 203, loss = 0.31995394825935364
iteration 204, loss = 0.29587194323539734
iteration 205, loss = 0.31743448972702026
iteration 206, loss = 0.2853337228298187
iteration 207, loss = 0.3031306862831116
iteration 208, loss = 0.29396381974220276
iteration 209, loss = 0.2932782471179962
iteration 210, loss = 0.28620246052742004
iteration 211, loss = 0.3329148292541504
iteration 212, loss = 0.299666166305542
iteration 213, loss = 0.3486826717853546
iteration 214, loss = 0.31569603085517883
iteration 215, loss = 0.3478223383426666
iteration 216, loss = 0.29116764664649963
iteration 217, loss = 0.313558429479599
iteration 218, loss = 0.29634764790534973
iteration 219, loss = 0.2982633709907532
iteration 220, loss = 0.2774930000305176
iteration 221, loss = 0.29960736632347107
iteration 222, loss = 0.3094828426837921
iteration 223, loss = 0.2627243995666504
iteration 224, loss = 0.2574794590473175
iteration 225, loss = 0.2768571376800537
iteration 226, loss = 0.2934764623641968
iteration 227, loss = 0.28460803627967834
iteration 228, loss = 0.30978190898895264
iteration 229, loss = 0.29049059748649597
iteration 230, loss = 0.2849145829677582
iteration 231, loss = 0.2746641933917999
iteration 232, loss = 0.28015390038490295
iteration 233, loss = 0.3009788393974304
iteration 234, loss = 0.30110177397727966
iteration 235, loss = 0.2636466920375824
iteration 236, loss = 0.29991835355758667
iteration 237, loss = 0.29419460892677307
iteration 238, loss = 0.2964611053466797
iteration 239, loss = 0.27276018261909485
iteration 240, loss = 0.2761329710483551
iteration 241, loss = 0.30749303102493286
iteration 242, loss = 0.27537935972213745
iteration 243, loss = 0.321541428565979
iteration 244, loss = 0.27727529406547546
iteration 245, loss = 0.2981985807418823
iteration 246, loss = 0.27055901288986206
iteration 247, loss = 0.23778368532657623
iteration 248, loss = 0.24283500015735626
iteration 249, loss = 0.3046410381793976
iteration 250, loss = 0.250576376914978
iteration 251, loss = 0.24134176969528198
iteration 252, loss = 0.26532214879989624
iteration 253, loss = 0.267020583152771
iteration 254, loss = 0.25509482622146606
iteration 255, loss = 0.24256029725074768
iteration 256, loss = 0.24738499522209167
iteration 257, loss = 0.27912360429763794
iteration 258, loss = 0.2542666792869568
iteration 259, loss = 0.3119044899940491
iteration 260, loss = 0.266695499420166
iteration 261, loss = 0.23566429316997528
iteration 262, loss = 0.25249382853507996
iteration 263, loss = 0.2596844434738159
iteration 264, loss = 0.26761412620544434
iteration 265, loss = 0.241786390542984
iteration 266, loss = 0.28095290064811707
iteration 267, loss = 0.26555317640304565
iteration 268, loss = 0.29757723212242126
iteration 269, loss = 0.29191190004348755
iteration 270, loss = 0.24078941345214844
iteration 271, loss = 0.30519989132881165
iteration 272, loss = 0.26762524247169495
iteration 273, loss = 0.2559804320335388
iteration 274, loss = 0.2542755603790283
iteration 275, loss = 0.23774904012680054
iteration 276, loss = 0.24004347622394562
iteration 277, loss = 0.24823516607284546
iteration 278, loss = 0.2517390847206116
iteration 279, loss = 0.22726687788963318
iteration 280, loss = 0.2528529465198517
iteration 281, loss = 0.25005632638931274
iteration 282, loss = 0.26169946789741516
iteration 283, loss = 0.23219555616378784
iteration 284, loss = 0.2906871438026428
iteration 285, loss = 0.2249312400817871
iteration 286, loss = 0.28098535537719727
iteration 287, loss = 0.2265091985464096
iteration 288, loss = 0.22335049510002136
iteration 289, loss = 0.21911798417568207
iteration 290, loss = 0.23230290412902832
iteration 291, loss = 0.25821274518966675
iteration 292, loss = 0.21624813973903656
iteration 293, loss = 0.22264155745506287
iteration 294, loss = 0.25219038128852844
iteration 295, loss = 0.21768710017204285
iteration 296, loss = 0.27574700117111206
iteration 297, loss = 0.22855795919895172
iteration 298, loss = 0.22262020409107208
iteration 299, loss = 0.242132768034935
iteration 300, loss = 0.20808178186416626
iteration 1, loss = 0.232845276594162
iteration 2, loss = 0.21873533725738525
iteration 3, loss = 0.2124912589788437
iteration 4, loss = 0.21913039684295654
iteration 5, loss = 0.2411825954914093
iteration 6, loss = 0.2284824401140213
iteration 7, loss = 0.2215794324874878
iteration 8, loss = 0.22485092282295227
iteration 9, loss = 0.2132783830165863
iteration 10, loss = 0.2238045185804367
iteration 11, loss = 0.19615502655506134
iteration 12, loss = 0.2490745633840561
iteration 13, loss = 0.2233033925294876
iteration 14, loss = 0.21294599771499634
iteration 15, loss = 0.21043027937412262
iteration 16, loss = 0.20919173955917358
iteration 17, loss = 0.262235164642334
iteration 18, loss = 0.23059464991092682
iteration 19, loss = 0.1985824853181839
iteration 20, loss = 0.20352067053318024
iteration 21, loss = 0.22551916539669037
iteration 22, loss = 0.23845092952251434
iteration 23, loss = 0.19284214079380035
iteration 24, loss = 0.1888008415699005
iteration 25, loss = 0.23508960008621216
iteration 26, loss = 0.19807149469852448
iteration 27, loss = 0.18681524693965912
iteration 28, loss = 0.20850171148777008
iteration 29, loss = 0.21805411577224731
iteration 30, loss = 0.1878722459077835
iteration 31, loss = 0.24118173122406006
iteration 32, loss = 0.20655958354473114
iteration 33, loss = 0.20327700674533844
iteration 34, loss = 0.20897677540779114
iteration 35, loss = 0.22869424521923065
iteration 36, loss = 0.20923428237438202
iteration 37, loss = 0.20448511838912964
iteration 38, loss = 0.2520943880081177
iteration 39, loss = 0.216899111866951
iteration 40, loss = 0.1765918880701065
iteration 41, loss = 0.1935247778892517
iteration 42, loss = 0.19589784741401672
iteration 43, loss = 0.20498210191726685
iteration 44, loss = 0.20066417753696442
iteration 45, loss = 0.20430877804756165
iteration 46, loss = 0.18322111666202545
iteration 47, loss = 0.22735053300857544
iteration 48, loss = 0.18885797262191772
iteration 49, loss = 0.2446262240409851
iteration 50, loss = 0.20812784135341644
iteration 51, loss = 0.20816735923290253
iteration 52, loss = 0.2069777250289917
iteration 53, loss = 0.19883839786052704
iteration 54, loss = 0.19163194298744202
iteration 55, loss = 0.17921891808509827
iteration 56, loss = 0.179171621799469
iteration 57, loss = 0.23522019386291504
iteration 58, loss = 0.19517263770103455
iteration 59, loss = 0.17783020436763763
iteration 60, loss = 0.24413034319877625
iteration 61, loss = 0.21470820903778076
iteration 62, loss = 0.19008581340312958
iteration 63, loss = 0.20610859990119934
iteration 64, loss = 0.17929571866989136
iteration 65, loss = 0.19967713952064514
iteration 66, loss = 0.17134802043437958
iteration 67, loss = 0.21443217992782593
iteration 68, loss = 0.1830575168132782
iteration 69, loss = 0.19610631465911865
iteration 70, loss = 0.15750199556350708
iteration 71, loss = 0.17950117588043213
iteration 72, loss = 0.17251722514629364
iteration 73, loss = 0.20086927711963654
iteration 74, loss = 0.18276625871658325
iteration 75, loss = 0.16973930597305298
iteration 76, loss = 0.2204277664422989
iteration 77, loss = 0.1773369014263153
iteration 78, loss = 0.21113799512386322
iteration 79, loss = 0.2047654390335083
iteration 80, loss = 0.16730153560638428
iteration 81, loss = 0.15964902937412262
iteration 82, loss = 0.1718691736459732
iteration 83, loss = 0.1705847829580307
iteration 84, loss = 0.17378009855747223
iteration 85, loss = 0.1896955966949463
iteration 86, loss = 0.16997532546520233
iteration 87, loss = 0.19640454649925232
iteration 88, loss = 0.20265591144561768
iteration 89, loss = 0.1881076991558075
iteration 90, loss = 0.18735520541667938
iteration 91, loss = 0.17995944619178772
iteration 92, loss = 0.15550726652145386
iteration 93, loss = 0.19989445805549622
iteration 94, loss = 0.16053611040115356
iteration 95, loss = 0.16412459313869476
iteration 96, loss = 0.15866020321846008
iteration 97, loss = 0.1690462827682495
iteration 98, loss = 0.15392126142978668
iteration 99, loss = 0.1809052973985672
iteration 100, loss = 0.1550580859184265
iteration 101, loss = 0.21206271648406982
iteration 102, loss = 0.1472788006067276
iteration 103, loss = 0.15889212489128113
iteration 104, loss = 0.21331489086151123
iteration 105, loss = 0.18438145518302917
iteration 106, loss = 0.18095266819000244
iteration 107, loss = 0.1680152714252472
iteration 108, loss = 0.16788583993911743
iteration 109, loss = 0.19533759355545044
iteration 110, loss = 0.1607239842414856
iteration 111, loss = 0.15719054639339447
iteration 112, loss = 0.2130681276321411
iteration 113, loss = 0.1667669713497162
iteration 114, loss = 0.1426078975200653
iteration 115, loss = 0.1750047653913498
iteration 116, loss = 0.16401320695877075
iteration 117, loss = 0.14759960770606995
iteration 118, loss = 0.16221940517425537
iteration 119, loss = 0.16012027859687805
iteration 120, loss = 0.14393213391304016
iteration 121, loss = 0.1410530060529709
iteration 122, loss = 0.19302821159362793
iteration 123, loss = 0.1447732001543045
iteration 124, loss = 0.14372564852237701
iteration 125, loss = 0.18095917999744415
iteration 126, loss = 0.15798845887184143
iteration 127, loss = 0.1420040726661682
iteration 128, loss = 0.14371871948242188
iteration 129, loss = 0.14124858379364014
iteration 130, loss = 0.15111412107944489
iteration 131, loss = 0.16457130014896393
iteration 132, loss = 0.14132514595985413
iteration 133, loss = 0.15265758335590363
iteration 134, loss = 0.19115857779979706
iteration 135, loss = 0.14304664731025696
iteration 136, loss = 0.15642736852169037
iteration 137, loss = 0.15375389158725739
iteration 138, loss = 0.1273355633020401
iteration 139, loss = 0.14323270320892334
iteration 140, loss = 0.14277799427509308
iteration 141, loss = 0.14247535169124603
iteration 142, loss = 0.12747271358966827
iteration 143, loss = 0.13754606246948242
iteration 144, loss = 0.13749082386493683
iteration 145, loss = 0.1938536912202835
iteration 146, loss = 0.13565833866596222
iteration 147, loss = 0.16805577278137207
iteration 148, loss = 0.1386781930923462
iteration 149, loss = 0.1711503565311432
iteration 150, loss = 0.14095117151737213
iteration 151, loss = 0.17606835067272186
iteration 152, loss = 0.13886651396751404
iteration 153, loss = 0.14159785211086273
iteration 154, loss = 0.1343671679496765
iteration 155, loss = 0.13529154658317566
iteration 156, loss = 0.14756272733211517
iteration 157, loss = 0.13619275391101837
iteration 158, loss = 0.14226168394088745
iteration 159, loss = 0.16382068395614624
iteration 160, loss = 0.1624690294265747
iteration 161, loss = 0.13842393457889557
iteration 162, loss = 0.1442035734653473
iteration 163, loss = 0.14180055260658264
iteration 164, loss = 0.15204548835754395
iteration 165, loss = 0.12803855538368225
iteration 166, loss = 0.13541463017463684
iteration 167, loss = 0.16897441446781158
iteration 168, loss = 0.16472679376602173
iteration 169, loss = 0.151244655251503
iteration 170, loss = 0.1706983745098114
iteration 171, loss = 0.13229139149188995
iteration 172, loss = 0.1275208592414856
iteration 173, loss = 0.14641845226287842
iteration 174, loss = 0.18303564190864563
iteration 175, loss = 0.14217545092105865
iteration 176, loss = 0.12411802262067795
iteration 177, loss = 0.12219888716936111
iteration 178, loss = 0.12941572070121765
iteration 179, loss = 0.13170292973518372
iteration 180, loss = 0.13891282677650452
iteration 181, loss = 0.12779977917671204
iteration 182, loss = 0.12516848742961884
iteration 183, loss = 0.1274356245994568
iteration 184, loss = 0.14181025326251984
iteration 185, loss = 0.13660457730293274
iteration 186, loss = 0.12986086308956146
iteration 187, loss = 0.1258741021156311
iteration 188, loss = 0.13226532936096191
iteration 189, loss = 0.11035893857479095
iteration 190, loss = 0.11520075798034668
iteration 191, loss = 0.14576104283332825
iteration 192, loss = 0.119940385222435
iteration 193, loss = 0.12954193353652954
iteration 194, loss = 0.1672261506319046
iteration 195, loss = 0.13557474315166473
iteration 196, loss = 0.12103473395109177
iteration 197, loss = 0.11311037093400955
iteration 198, loss = 0.11961925029754639
iteration 199, loss = 0.1486966907978058
iteration 200, loss = 0.13380780816078186
iteration 201, loss = 0.12223295867443085
iteration 202, loss = 0.13081015646457672
iteration 203, loss = 0.14592571556568146
iteration 204, loss = 0.12197886407375336
iteration 205, loss = 0.11172442138195038
iteration 206, loss = 0.1176038384437561
iteration 207, loss = 0.1413755565881729
iteration 208, loss = 0.10899317264556885
iteration 209, loss = 0.11533188819885254
iteration 210, loss = 0.12212252616882324
iteration 211, loss = 0.11863000690937042
iteration 212, loss = 0.11688239872455597
iteration 213, loss = 0.11251784861087799
iteration 214, loss = 0.11117278039455414
iteration 215, loss = 0.12325690686702728
iteration 216, loss = 0.11228946596384048
iteration 217, loss = 0.1181865930557251
iteration 218, loss = 0.11025261878967285
iteration 219, loss = 0.12408085912466049
iteration 220, loss = 0.13677503168582916
iteration 221, loss = 0.11866471916437149
iteration 222, loss = 0.1423436403274536
iteration 223, loss = 0.10886374861001968
iteration 224, loss = 0.11165172606706619
iteration 225, loss = 0.1086045578122139
iteration 226, loss = 0.12620456516742706
iteration 227, loss = 0.11431809514760971
iteration 228, loss = 0.1412009298801422
iteration 229, loss = 0.12987639009952545
iteration 230, loss = 0.12843653559684753
iteration 231, loss = 0.10406769067049026
iteration 232, loss = 0.11381476372480392
iteration 233, loss = 0.11491146683692932
iteration 234, loss = 0.11029168963432312
iteration 235, loss = 0.11137007176876068
iteration 236, loss = 0.1141679510474205
iteration 237, loss = 0.11216410249471664
iteration 238, loss = 0.12699203193187714
iteration 239, loss = 0.10615532100200653
iteration 240, loss = 0.10983855277299881
iteration 241, loss = 0.10403397679328918
iteration 242, loss = 0.1375788301229477
iteration 243, loss = 0.1037842407822609
iteration 244, loss = 0.11287955194711685
iteration 245, loss = 0.1376219540834427
iteration 246, loss = 0.10150998830795288
iteration 247, loss = 0.10181722790002823
iteration 248, loss = 0.1322251260280609
iteration 249, loss = 0.10467637330293655
iteration 250, loss = 0.10360775887966156
iteration 251, loss = 0.12361939251422882
iteration 252, loss = 0.1280018389225006
iteration 253, loss = 0.11169491708278656
iteration 254, loss = 0.10212540626525879
iteration 255, loss = 0.10517857223749161
iteration 256, loss = 0.1120067909359932
iteration 257, loss = 0.10752105712890625
iteration 258, loss = 0.14989131689071655
iteration 259, loss = 0.10271969437599182
iteration 260, loss = 0.10206581652164459
iteration 261, loss = 0.13397669792175293
iteration 262, loss = 0.11137895286083221
iteration 263, loss = 0.10704171657562256
iteration 264, loss = 0.1308569610118866
iteration 265, loss = 0.0996493250131607
iteration 266, loss = 0.10423018038272858
iteration 267, loss = 0.12649431824684143
iteration 268, loss = 0.09900368005037308
iteration 269, loss = 0.10494881868362427
iteration 270, loss = 0.09545233845710754
iteration 271, loss = 0.09027361124753952
iteration 272, loss = 0.10263773798942566
iteration 273, loss = 0.10108711570501328
iteration 274, loss = 0.10430927574634552
iteration 275, loss = 0.09621574729681015
iteration 276, loss = 0.10976076871156693
iteration 277, loss = 0.09842941164970398
iteration 278, loss = 0.10998474061489105
iteration 279, loss = 0.10307526588439941
iteration 280, loss = 0.09659303724765778
iteration 281, loss = 0.1084723025560379
iteration 282, loss = 0.09126169979572296
iteration 283, loss = 0.10394410789012909
iteration 284, loss = 0.10043146461248398
iteration 285, loss = 0.10643787682056427
iteration 286, loss = 0.08971195667982101
iteration 287, loss = 0.0879489928483963
iteration 288, loss = 0.09698055684566498
iteration 289, loss = 0.12121933698654175
iteration 290, loss = 0.1179797351360321
iteration 291, loss = 0.08889234066009521
iteration 292, loss = 0.09136338531970978
iteration 293, loss = 0.09346882998943329
iteration 294, loss = 0.1121206283569336
iteration 295, loss = 0.10460575670003891
iteration 296, loss = 0.09989622235298157
iteration 297, loss = 0.10162114351987839
iteration 298, loss = 0.09299912303686142
iteration 299, loss = 0.10008221119642258
iteration 300, loss = 0.0985221192240715
iteration 1, loss = 0.09882662445306778
iteration 2, loss = 0.09991516172885895
iteration 3, loss = 0.10447664558887482
iteration 4, loss = 0.10182227939367294
iteration 5, loss = 0.11163174360990524
iteration 6, loss = 0.09690482169389725
iteration 7, loss = 0.09096121788024902
iteration 8, loss = 0.08999582380056381
iteration 9, loss = 0.10402139276266098
iteration 10, loss = 0.1011512354016304
iteration 11, loss = 0.09543244540691376
iteration 12, loss = 0.1092783659696579
iteration 13, loss = 0.09272366017103195
iteration 14, loss = 0.11262952536344528
iteration 15, loss = 0.09423084557056427
iteration 16, loss = 0.08892925828695297
iteration 17, loss = 0.1047959178686142
iteration 18, loss = 0.08133837580680847
iteration 19, loss = 0.09160443395376205
iteration 20, loss = 0.08903932571411133
iteration 21, loss = 0.08595357835292816
iteration 22, loss = 0.08644348382949829
iteration 23, loss = 0.09822836518287659
iteration 24, loss = 0.088422991335392
iteration 25, loss = 0.08073712140321732
iteration 26, loss = 0.08246731013059616
iteration 27, loss = 0.08725399523973465
iteration 28, loss = 0.0851229727268219
iteration 29, loss = 0.09228263050317764
iteration 30, loss = 0.08490962535142899
iteration 31, loss = 0.08783101290464401
iteration 32, loss = 0.08523738384246826
iteration 33, loss = 0.08643021434545517
iteration 34, loss = 0.08990927040576935
iteration 35, loss = 0.08247638493776321
iteration 36, loss = 0.10411233454942703
iteration 37, loss = 0.07911303639411926
iteration 38, loss = 0.09722335636615753
iteration 39, loss = 0.1090664267539978
iteration 40, loss = 0.09147997200489044
iteration 41, loss = 0.0856296643614769
iteration 42, loss = 0.08055815100669861
iteration 43, loss = 0.10515335202217102
iteration 44, loss = 0.0787365660071373
iteration 45, loss = 0.0882653072476387
iteration 46, loss = 0.08308994024991989
iteration 47, loss = 0.0910542830824852
iteration 48, loss = 0.08900664746761322
iteration 49, loss = 0.08054886758327484
iteration 50, loss = 0.08914013206958771
iteration 51, loss = 0.10117913037538528
iteration 52, loss = 0.0986921638250351
iteration 53, loss = 0.09395049512386322
iteration 54, loss = 0.08029942214488983
iteration 55, loss = 0.08717392385005951
iteration 56, loss = 0.1212385892868042
iteration 57, loss = 0.07518194615840912
iteration 58, loss = 0.10020975768566132
iteration 59, loss = 0.09263075143098831
iteration 60, loss = 0.0794576108455658
iteration 61, loss = 0.08234214782714844
iteration 62, loss = 0.08122583478689194
iteration 63, loss = 0.09883228689432144
iteration 64, loss = 0.08578133583068848
iteration 65, loss = 0.08344651758670807
iteration 66, loss = 0.0734342560172081
iteration 67, loss = 0.07830148935317993
iteration 68, loss = 0.09738347679376602
iteration 69, loss = 0.07956606149673462
iteration 70, loss = 0.07873976975679398
iteration 71, loss = 0.07142975181341171
iteration 72, loss = 0.11757335811853409
iteration 73, loss = 0.0758579820394516
iteration 74, loss = 0.07469639927148819
iteration 75, loss = 0.10078293830156326
iteration 76, loss = 0.07713369280099869
iteration 77, loss = 0.0784992203116417
iteration 78, loss = 0.09757115691900253
iteration 79, loss = 0.08118662238121033
iteration 80, loss = 0.07817742228507996
iteration 81, loss = 0.07781090587377548
iteration 82, loss = 0.07536575943231583
iteration 83, loss = 0.06968943774700165
iteration 84, loss = 0.07918811589479446
iteration 85, loss = 0.09377134591341019
iteration 86, loss = 0.08029107004404068
iteration 87, loss = 0.08225953578948975
iteration 88, loss = 0.07720940560102463
iteration 89, loss = 0.09571940451860428
iteration 90, loss = 0.0695873275399208
iteration 91, loss = 0.07049541920423508
iteration 92, loss = 0.08086702227592468
iteration 93, loss = 0.07450054585933685
iteration 94, loss = 0.07553720474243164
iteration 95, loss = 0.07472887635231018
iteration 96, loss = 0.08450936526060104
iteration 97, loss = 0.08210145682096481
iteration 98, loss = 0.07712505012750626
iteration 99, loss = 0.07587269693613052
iteration 100, loss = 0.09093750268220901
iteration 101, loss = 0.0918668657541275
iteration 102, loss = 0.08759146928787231
iteration 103, loss = 0.08742783963680267
iteration 104, loss = 0.07358931005001068
iteration 105, loss = 0.07689667493104935
iteration 106, loss = 0.07159778475761414
iteration 107, loss = 0.08996491879224777
iteration 108, loss = 0.07681826502084732
iteration 109, loss = 0.09151153266429901
iteration 110, loss = 0.07173652946949005
iteration 111, loss = 0.07374133169651031
iteration 112, loss = 0.06838805228471756
iteration 113, loss = 0.07398959249258041
iteration 114, loss = 0.07270598411560059
iteration 115, loss = 0.07157578319311142
iteration 116, loss = 0.0653776302933693
iteration 117, loss = 0.08924540132284164
iteration 118, loss = 0.06469555199146271
iteration 119, loss = 0.07047098875045776
iteration 120, loss = 0.08656460791826248
iteration 121, loss = 0.06635569781064987
iteration 122, loss = 0.068781778216362
iteration 123, loss = 0.06946501135826111
iteration 124, loss = 0.0723012164235115
iteration 125, loss = 0.06564514338970184
iteration 126, loss = 0.07015679776668549
iteration 127, loss = 0.07586830109357834
iteration 128, loss = 0.06592016667127609
iteration 129, loss = 0.06573587656021118
iteration 130, loss = 0.0647842064499855
iteration 131, loss = 0.06700718402862549
iteration 132, loss = 0.0680510550737381
iteration 133, loss = 0.0640382170677185
iteration 134, loss = 0.06511059403419495
iteration 135, loss = 0.08367986232042313
iteration 136, loss = 0.07289978116750717
iteration 137, loss = 0.07261147350072861
iteration 138, loss = 0.07451295107603073
iteration 139, loss = 0.0639846920967102
iteration 140, loss = 0.07093819230794907
iteration 141, loss = 0.07161520421504974
iteration 142, loss = 0.06377986818552017
iteration 143, loss = 0.08448364585638046
iteration 144, loss = 0.0655745416879654
iteration 145, loss = 0.07076156884431839
iteration 146, loss = 0.06305915862321854
iteration 147, loss = 0.08322788029909134
iteration 148, loss = 0.06589794158935547
iteration 149, loss = 0.06321519613265991
iteration 150, loss = 0.06283394247293472
iteration 151, loss = 0.06630374491214752
iteration 152, loss = 0.0811016708612442
iteration 153, loss = 0.06368234008550644
iteration 154, loss = 0.06275623291730881
iteration 155, loss = 0.06358157843351364
iteration 156, loss = 0.06499963253736496
iteration 157, loss = 0.0657150074839592
iteration 158, loss = 0.062250636518001556
iteration 159, loss = 0.061328988522291183
iteration 160, loss = 0.06310614198446274
iteration 161, loss = 0.08129959553480148
iteration 162, loss = 0.07634854316711426
iteration 163, loss = 0.05967653915286064
iteration 164, loss = 0.06580423563718796
iteration 165, loss = 0.06041916087269783
iteration 166, loss = 0.062220726162195206
iteration 167, loss = 0.05953604355454445
iteration 168, loss = 0.05924635007977486
iteration 169, loss = 0.06025254353880882
iteration 170, loss = 0.05987144634127617
iteration 171, loss = 0.08248182386159897
iteration 172, loss = 0.060141365975141525
iteration 173, loss = 0.06107917055487633
iteration 174, loss = 0.07871302217245102
iteration 175, loss = 0.05810571834445
iteration 176, loss = 0.06089676171541214
iteration 177, loss = 0.06063180789351463
iteration 178, loss = 0.08239541202783585
iteration 179, loss = 0.0633334144949913
iteration 180, loss = 0.0610220842063427
iteration 181, loss = 0.0742374062538147
iteration 182, loss = 0.059916332364082336
iteration 183, loss = 0.07556595653295517
iteration 184, loss = 0.06085404381155968
iteration 185, loss = 0.05789853259921074
iteration 186, loss = 0.07089443504810333
iteration 187, loss = 0.058463532477617264
iteration 188, loss = 0.06116262450814247
iteration 189, loss = 0.0585666187107563
iteration 190, loss = 0.05871521681547165
iteration 191, loss = 0.060578376054763794
iteration 192, loss = 0.06198570504784584
iteration 193, loss = 0.06255996227264404
iteration 194, loss = 0.07668592035770416
iteration 195, loss = 0.05747197940945625
iteration 196, loss = 0.06758932024240494
iteration 197, loss = 0.057412922382354736
iteration 198, loss = 0.060155048966407776
iteration 199, loss = 0.05788342282176018
iteration 200, loss = 0.0708184614777565
iteration 201, loss = 0.05440113693475723
iteration 202, loss = 0.05662795901298523
iteration 203, loss = 0.06114592403173447
iteration 204, loss = 0.0613495409488678
iteration 205, loss = 0.0725940465927124
iteration 206, loss = 0.05877191945910454
iteration 207, loss = 0.07422880828380585
iteration 208, loss = 0.05662201717495918
iteration 209, loss = 0.05833248421549797
iteration 210, loss = 0.0678882822394371
iteration 211, loss = 0.07511523365974426
iteration 212, loss = 0.05310210585594177
iteration 213, loss = 0.07021094113588333
iteration 214, loss = 0.05793933942914009
iteration 215, loss = 0.05336826667189598
iteration 216, loss = 0.05667682737112045
iteration 217, loss = 0.07214616239070892
iteration 218, loss = 0.07176561653614044
iteration 219, loss = 0.057303737848997116
iteration 220, loss = 0.05167390778660774
iteration 221, loss = 0.06300470232963562
iteration 222, loss = 0.06767856329679489
iteration 223, loss = 0.05214804783463478
iteration 224, loss = 0.06990774720907211
iteration 225, loss = 0.06000728905200958
iteration 226, loss = 0.06918671727180481
iteration 227, loss = 0.053607188165187836
iteration 228, loss = 0.05837597697973251
iteration 229, loss = 0.05311037227511406
iteration 230, loss = 0.05612825229763985
iteration 231, loss = 0.0678635984659195
iteration 232, loss = 0.05440657585859299
iteration 233, loss = 0.06185637786984444
iteration 234, loss = 0.05435384809970856
iteration 235, loss = 0.06105995923280716
iteration 236, loss = 0.05494287237524986
iteration 237, loss = 0.05597787722945213
iteration 238, loss = 0.05318089947104454
iteration 239, loss = 0.054947879165410995
iteration 240, loss = 0.05451854690909386
iteration 241, loss = 0.05334960296750069
iteration 242, loss = 0.06687664240598679
iteration 243, loss = 0.05021626874804497
iteration 244, loss = 0.06901965290307999
iteration 245, loss = 0.06439134478569031
iteration 246, loss = 0.05669432878494263
iteration 247, loss = 0.05742182955145836
iteration 248, loss = 0.05727941915392876
iteration 249, loss = 0.05497610569000244
iteration 250, loss = 0.05132481828331947
iteration 251, loss = 0.048400044441223145
iteration 252, loss = 0.05185861885547638
iteration 253, loss = 0.05245855078101158
iteration 254, loss = 0.09071630239486694
iteration 255, loss = 0.051894646137952805
iteration 256, loss = 0.04967381805181503
iteration 257, loss = 0.0648549348115921
iteration 258, loss = 0.06524839997291565
iteration 259, loss = 0.05004904419183731
iteration 260, loss = 0.049046389758586884
iteration 261, loss = 0.06275896728038788
iteration 262, loss = 0.04974525049328804
iteration 263, loss = 0.048208292573690414
iteration 264, loss = 0.05071882903575897
iteration 265, loss = 0.050593052059412
iteration 266, loss = 0.07198303192853928
iteration 267, loss = 0.05103737860918045
iteration 268, loss = 0.05242549628019333
iteration 269, loss = 0.051843881607055664
iteration 270, loss = 0.0501679964363575
iteration 271, loss = 0.06445656716823578
iteration 272, loss = 0.054103441536426544
iteration 273, loss = 0.05007809028029442
iteration 274, loss = 0.04930734261870384
iteration 275, loss = 0.04951706901192665
iteration 276, loss = 0.0476701557636261
iteration 277, loss = 0.04680537059903145
iteration 278, loss = 0.056679558008909225
iteration 279, loss = 0.0525452196598053
iteration 280, loss = 0.050472695380449295
iteration 281, loss = 0.046567320823669434
iteration 282, loss = 0.05077054351568222
iteration 283, loss = 0.04790462180972099
iteration 284, loss = 0.051009587943553925
iteration 285, loss = 0.05915938690304756
iteration 286, loss = 0.04941922426223755
iteration 287, loss = 0.05088116601109505
iteration 288, loss = 0.05055432394146919
iteration 289, loss = 0.05135395750403404
iteration 290, loss = 0.05800753831863403
iteration 291, loss = 0.047807399183511734
iteration 292, loss = 0.0634312778711319
iteration 293, loss = 0.053916893899440765
iteration 294, loss = 0.04822838306427002
iteration 295, loss = 0.047502148896455765
iteration 296, loss = 0.0469895601272583
iteration 297, loss = 0.047821030020713806
iteration 298, loss = 0.048627544194459915
iteration 299, loss = 0.07384392619132996
iteration 300, loss = 0.05373557657003403
iteration 1, loss = 0.04526512697339058
iteration 2, loss = 0.04790237918496132
iteration 3, loss = 0.04719439148902893
iteration 4, loss = 0.04680674150586128
iteration 5, loss = 0.04955805838108063
iteration 6, loss = 0.04862489551305771
iteration 7, loss = 0.04594329372048378
iteration 8, loss = 0.04731981083750725
iteration 9, loss = 0.04710565134882927
iteration 10, loss = 0.04850231856107712
iteration 11, loss = 0.04631529003381729
iteration 12, loss = 0.0530489981174469
iteration 13, loss = 0.047866735607385635
iteration 14, loss = 0.04425008222460747
iteration 15, loss = 0.04699461907148361
iteration 16, loss = 0.051148172467947006
iteration 17, loss = 0.044401902705430984
iteration 18, loss = 0.05814443156123161
iteration 19, loss = 0.043884746730327606
iteration 20, loss = 0.04702552780508995
iteration 21, loss = 0.04232807084918022
iteration 22, loss = 0.04258383437991142
iteration 23, loss = 0.044048842042684555
iteration 24, loss = 0.059896308928728104
iteration 25, loss = 0.05648993328213692
iteration 26, loss = 0.04420299455523491
iteration 27, loss = 0.04777298867702484
iteration 28, loss = 0.044780608266592026
iteration 29, loss = 0.04393031448125839
iteration 30, loss = 0.04727128893136978
iteration 31, loss = 0.043121278285980225
iteration 32, loss = 0.04694148153066635
iteration 33, loss = 0.04641678184270859
iteration 34, loss = 0.06030347943305969
iteration 35, loss = 0.04936598986387253
iteration 36, loss = 0.04427851364016533
iteration 37, loss = 0.046277906745672226
iteration 38, loss = 0.04529763385653496
iteration 39, loss = 0.04990615323185921
iteration 40, loss = 0.044340938329696655
iteration 41, loss = 0.0424904003739357
iteration 42, loss = 0.05465539172291756
iteration 43, loss = 0.06231745332479477
iteration 44, loss = 0.04565020278096199
iteration 45, loss = 0.04862856864929199
iteration 46, loss = 0.04601640999317169
iteration 47, loss = 0.05509957671165466
iteration 48, loss = 0.04604753851890564
iteration 49, loss = 0.06296002864837646
iteration 50, loss = 0.05738763511180878
iteration 51, loss = 0.051186561584472656
iteration 52, loss = 0.0432293526828289
iteration 53, loss = 0.04680255427956581
iteration 54, loss = 0.042606696486473083
iteration 55, loss = 0.046159528195858
iteration 56, loss = 0.040194086730480194
iteration 57, loss = 0.04104359820485115
iteration 58, loss = 0.045905329287052155
iteration 59, loss = 0.04390914365649223
iteration 60, loss = 0.045637428760528564
iteration 61, loss = 0.05061236768960953
iteration 62, loss = 0.039418775588274
iteration 63, loss = 0.043626025319099426
iteration 64, loss = 0.04180776700377464
iteration 65, loss = 0.05176980793476105
iteration 66, loss = 0.04067745432257652
iteration 67, loss = 0.0425061360001564
iteration 68, loss = 0.05278473347425461
iteration 69, loss = 0.04363866150379181
iteration 70, loss = 0.03971490636467934
iteration 71, loss = 0.04180017486214638
iteration 72, loss = 0.04616791009902954
iteration 73, loss = 0.0379580520093441
iteration 74, loss = 0.04275991767644882
iteration 75, loss = 0.03978942334651947
iteration 76, loss = 0.041018545627593994
iteration 77, loss = 0.041848473250865936
iteration 78, loss = 0.04551617428660393
iteration 79, loss = 0.042370669543743134
iteration 80, loss = 0.04266198351979256
iteration 81, loss = 0.04700944200158119
iteration 82, loss = 0.04900086671113968
iteration 83, loss = 0.05200435221195221
iteration 84, loss = 0.040139928460121155
iteration 85, loss = 0.05256284028291702
iteration 86, loss = 0.049919795244932175
iteration 87, loss = 0.04052463173866272
iteration 88, loss = 0.050484735518693924
iteration 89, loss = 0.03982510790228844
iteration 90, loss = 0.04183487594127655
iteration 91, loss = 0.040110498666763306
iteration 92, loss = 0.03899073600769043
iteration 93, loss = 0.03865638002753258
iteration 94, loss = 0.039368461817502975
iteration 95, loss = 0.03932638466358185
iteration 96, loss = 0.036952923983335495
iteration 97, loss = 0.0391550287604332
iteration 98, loss = 0.03812609985470772
iteration 99, loss = 0.04157600179314613
iteration 100, loss = 0.041380997747182846
iteration 101, loss = 0.03930046409368515
iteration 102, loss = 0.04905187338590622
iteration 103, loss = 0.039116788655519485
iteration 104, loss = 0.04239554703235626
iteration 105, loss = 0.039192091673612595
iteration 106, loss = 0.036849357187747955
iteration 107, loss = 0.06387978792190552
iteration 108, loss = 0.03780839219689369
iteration 109, loss = 0.0477612242102623
iteration 110, loss = 0.038433268666267395
iteration 111, loss = 0.03602254018187523
iteration 112, loss = 0.03765268251299858
iteration 113, loss = 0.037272870540618896
iteration 114, loss = 0.037763748317956924
iteration 115, loss = 0.03964082896709442
iteration 116, loss = 0.03744585067033768
iteration 117, loss = 0.03697019815444946
iteration 118, loss = 0.03731183707714081
iteration 119, loss = 0.03702378273010254
iteration 120, loss = 0.03771354258060455
iteration 121, loss = 0.037309322506189346
iteration 122, loss = 0.03489421308040619
iteration 123, loss = 0.04091167077422142
iteration 124, loss = 0.03957269713282585
iteration 125, loss = 0.03845931962132454
iteration 126, loss = 0.03773561120033264
iteration 127, loss = 0.03845704346895218
iteration 128, loss = 0.036662135273218155
iteration 129, loss = 0.03922564163804054
iteration 130, loss = 0.0486176498234272
iteration 131, loss = 0.04620441794395447
iteration 132, loss = 0.042611438781023026
iteration 133, loss = 0.0452425479888916
iteration 134, loss = 0.037658266723155975
iteration 135, loss = 0.03589256852865219
iteration 136, loss = 0.03652095049619675
iteration 137, loss = 0.04533974081277847
iteration 138, loss = 0.04864490404725075
iteration 139, loss = 0.038392119109630585
iteration 140, loss = 0.0385226272046566
iteration 141, loss = 0.03492933139204979
iteration 142, loss = 0.035792943090200424
iteration 143, loss = 0.034740835428237915
iteration 144, loss = 0.034842658787965775
iteration 145, loss = 0.03485174849629402
iteration 146, loss = 0.03973088040947914
iteration 147, loss = 0.03462979942560196
iteration 148, loss = 0.036907877773046494
iteration 149, loss = 0.03666551783680916
iteration 150, loss = 0.053393103182315826
iteration 151, loss = 0.03363568335771561
iteration 152, loss = 0.0399196594953537
iteration 153, loss = 0.03980961814522743
iteration 154, loss = 0.03465764597058296
iteration 155, loss = 0.03724970668554306
iteration 156, loss = 0.03647516667842865
iteration 157, loss = 0.03729751333594322
iteration 158, loss = 0.03482286259531975
iteration 159, loss = 0.0346376933157444
iteration 160, loss = 0.04562622308731079
iteration 161, loss = 0.04787479713559151
iteration 162, loss = 0.03712538257241249
iteration 163, loss = 0.046053726226091385
iteration 164, loss = 0.03462042659521103
iteration 165, loss = 0.033306293189525604
iteration 166, loss = 0.044737014919519424
iteration 167, loss = 0.03574754297733307
iteration 168, loss = 0.03347093239426613
iteration 169, loss = 0.033613599836826324
iteration 170, loss = 0.03360006958246231
iteration 171, loss = 0.05178172141313553
iteration 172, loss = 0.043454769998788834
iteration 173, loss = 0.03427814319729805
iteration 174, loss = 0.045884355902671814
iteration 175, loss = 0.03574959561228752
iteration 176, loss = 0.033942028880119324
iteration 177, loss = 0.03333662450313568
iteration 178, loss = 0.033337291330099106
iteration 179, loss = 0.03535880520939827
iteration 180, loss = 0.03437383845448494
iteration 181, loss = 0.043542731553316116
iteration 182, loss = 0.04642542451620102
iteration 183, loss = 0.03774991258978844
iteration 184, loss = 0.031897976994514465
iteration 185, loss = 0.03352190554141998
iteration 186, loss = 0.03115908056497574
iteration 187, loss = 0.03335929289460182
iteration 188, loss = 0.031937047839164734
iteration 189, loss = 0.032211367040872574
iteration 190, loss = 0.03134838491678238
iteration 191, loss = 0.0317058227956295
iteration 192, loss = 0.04817672073841095
iteration 193, loss = 0.04294239729642868
iteration 194, loss = 0.03229568526148796
iteration 195, loss = 0.030755355954170227
iteration 196, loss = 0.03205129876732826
iteration 197, loss = 0.041236743330955505
iteration 198, loss = 0.03517138585448265
iteration 199, loss = 0.033774882555007935
iteration 200, loss = 0.041091497987508774
iteration 201, loss = 0.03317482769489288
iteration 202, loss = 0.03417884558439255
iteration 203, loss = 0.04399607330560684
iteration 204, loss = 0.03221229836344719
iteration 205, loss = 0.043863024562597275
iteration 206, loss = 0.03396989405155182
iteration 207, loss = 0.03094499744474888
iteration 208, loss = 0.033563219010829926
iteration 209, loss = 0.03343886882066727
iteration 210, loss = 0.0319611132144928
iteration 211, loss = 0.03662295266985893
iteration 212, loss = 0.03073599748313427
iteration 213, loss = 0.03097459301352501
iteration 214, loss = 0.034123897552490234
iteration 215, loss = 0.031056558713316917
iteration 216, loss = 0.03333193436264992
iteration 217, loss = 0.03880411386489868
iteration 218, loss = 0.03044484741985798
iteration 219, loss = 0.03234149143099785
iteration 220, loss = 0.03400302305817604
iteration 221, loss = 0.03418942540884018
iteration 222, loss = 0.03567323461174965
iteration 223, loss = 0.03106098249554634
iteration 224, loss = 0.030723821371793747
iteration 225, loss = 0.0299727451056242
iteration 226, loss = 0.03271149843931198
iteration 227, loss = 0.030043121427297592
iteration 228, loss = 0.031774841248989105
iteration 229, loss = 0.03063051588833332
iteration 230, loss = 0.029033824801445007
iteration 231, loss = 0.037602078169584274
iteration 232, loss = 0.029640233144164085
iteration 233, loss = 0.030819613486528397
iteration 234, loss = 0.03884928300976753
iteration 235, loss = 0.031479720026254654
iteration 236, loss = 0.05022694543004036
iteration 237, loss = 0.03390074893832207
iteration 238, loss = 0.04126090928912163
iteration 239, loss = 0.033641692250967026
iteration 240, loss = 0.03844194859266281
iteration 241, loss = 0.029596496373414993
iteration 242, loss = 0.031392570585012436
iteration 243, loss = 0.034530527889728546
iteration 244, loss = 0.03293243423104286
iteration 245, loss = 0.03200212121009827
iteration 246, loss = 0.029640140011906624
iteration 247, loss = 0.029163572937250137
iteration 248, loss = 0.03658696264028549
iteration 249, loss = 0.031430959701538086
iteration 250, loss = 0.03169417008757591
iteration 251, loss = 0.030434660613536835
iteration 252, loss = 0.041068628430366516
iteration 253, loss = 0.02998451143503189
iteration 254, loss = 0.033749982714653015
iteration 255, loss = 0.03277287632226944
iteration 256, loss = 0.028601795434951782
iteration 257, loss = 0.03403203561902046
iteration 258, loss = 0.039697010070085526
iteration 259, loss = 0.03493177145719528
iteration 260, loss = 0.036007001996040344
iteration 261, loss = 0.03678436204791069
iteration 262, loss = 0.02878226712346077
iteration 263, loss = 0.030768170952796936
iteration 264, loss = 0.02733542025089264
iteration 265, loss = 0.02857000194489956
iteration 266, loss = 0.029627036303281784
iteration 267, loss = 0.0380370207130909
iteration 268, loss = 0.036092404276132584
iteration 269, loss = 0.027999378740787506
iteration 270, loss = 0.043138545006513596
iteration 271, loss = 0.028872499242424965
iteration 272, loss = 0.03152567893266678
iteration 273, loss = 0.03194240480661392
iteration 274, loss = 0.030132345855236053
iteration 275, loss = 0.02943354658782482
iteration 276, loss = 0.038357511162757874
iteration 277, loss = 0.030240416526794434
iteration 278, loss = 0.028816591948270798
iteration 279, loss = 0.0295631792396307
iteration 280, loss = 0.039135247468948364
iteration 281, loss = 0.029050452634692192
iteration 282, loss = 0.030123073607683182
iteration 283, loss = 0.026885652914643288
iteration 284, loss = 0.029492324218153954
iteration 285, loss = 0.026657432317733765
iteration 286, loss = 0.029728112742304802
iteration 287, loss = 0.027249569073319435
iteration 288, loss = 0.029214484617114067
iteration 289, loss = 0.02789713442325592
iteration 290, loss = 0.027253741398453712
iteration 291, loss = 0.03457879275083542
iteration 292, loss = 0.03206425905227661
iteration 293, loss = 0.0354873463511467
iteration 294, loss = 0.027349723502993584
iteration 295, loss = 0.027215486392378807
iteration 296, loss = 0.045623380690813065
iteration 297, loss = 0.028457505628466606
iteration 298, loss = 0.03109627217054367
iteration 299, loss = 0.031140318140387535
iteration 300, loss = 0.028088781982660294
iteration 1, loss = 0.03535699099302292
iteration 2, loss = 0.02942243218421936
iteration 3, loss = 0.0287123154848814
iteration 4, loss = 0.03451310470700264
iteration 5, loss = 0.028481638059020042
iteration 6, loss = 0.027068965137004852
iteration 7, loss = 0.03724643588066101
iteration 8, loss = 0.026570208370685577
iteration 9, loss = 0.04461738094687462
iteration 10, loss = 0.025527331978082657
iteration 11, loss = 0.027267921715974808
iteration 12, loss = 0.027772048488259315
iteration 13, loss = 0.025754490867257118
iteration 14, loss = 0.02661510556936264
iteration 15, loss = 0.03613413870334625
iteration 16, loss = 0.03023497946560383
iteration 17, loss = 0.028487006202340126
iteration 18, loss = 0.025998873636126518
iteration 19, loss = 0.027448223903775215
iteration 20, loss = 0.028231274336576462
iteration 21, loss = 0.0313873365521431
iteration 22, loss = 0.026057632640004158
iteration 23, loss = 0.0327930748462677
iteration 24, loss = 0.02586657740175724
iteration 25, loss = 0.026538139209151268
iteration 26, loss = 0.026991544291377068
iteration 27, loss = 0.028169622644782066
iteration 28, loss = 0.02671406976878643
iteration 29, loss = 0.03141070902347565
iteration 30, loss = 0.025060495361685753
iteration 31, loss = 0.03382095322012901
iteration 32, loss = 0.03236481174826622
iteration 33, loss = 0.04293810948729515
iteration 34, loss = 0.045688312500715256
iteration 35, loss = 0.030693218111991882
iteration 36, loss = 0.028094282373785973
iteration 37, loss = 0.025318726897239685
iteration 38, loss = 0.024606628343462944
iteration 39, loss = 0.025209985673427582
iteration 40, loss = 0.027573538944125175
iteration 41, loss = 0.03385747969150543
iteration 42, loss = 0.02672385983169079
iteration 43, loss = 0.02545771934092045
iteration 44, loss = 0.027309734374284744
iteration 45, loss = 0.025694670155644417
iteration 46, loss = 0.028869114816188812
iteration 47, loss = 0.02571256458759308
iteration 48, loss = 0.026184162124991417
iteration 49, loss = 0.025943780317902565
iteration 50, loss = 0.025101829320192337
iteration 51, loss = 0.031917180866003036
iteration 52, loss = 0.028750529512763023
iteration 53, loss = 0.028675682842731476
iteration 54, loss = 0.025148335844278336
iteration 55, loss = 0.027864694595336914
iteration 56, loss = 0.02524801529943943
iteration 57, loss = 0.03372722491621971
iteration 58, loss = 0.02664490044116974
iteration 59, loss = 0.033866316080093384
iteration 60, loss = 0.025131838396191597
iteration 61, loss = 0.030000444501638412
iteration 62, loss = 0.024405263364315033
iteration 63, loss = 0.02384600229561329
iteration 64, loss = 0.025037163868546486
iteration 65, loss = 0.02830173820257187
iteration 66, loss = 0.03163120150566101
iteration 67, loss = 0.0269585233181715
iteration 68, loss = 0.026471903547644615
iteration 69, loss = 0.03123784437775612
iteration 70, loss = 0.02641519345343113
iteration 71, loss = 0.02813275344669819
iteration 72, loss = 0.02381855808198452
iteration 73, loss = 0.023824136704206467
iteration 74, loss = 0.02434713952243328
iteration 75, loss = 0.024197004735469818
iteration 76, loss = 0.024424467235803604
iteration 77, loss = 0.02579926699399948
iteration 78, loss = 0.025974828749895096
iteration 79, loss = 0.024158183485269547
iteration 80, loss = 0.02993273362517357
iteration 81, loss = 0.022589612752199173
iteration 82, loss = 0.02814304642379284
iteration 83, loss = 0.02392657846212387
iteration 84, loss = 0.02633902244269848
iteration 85, loss = 0.023831818252801895
iteration 86, loss = 0.02565423585474491
iteration 87, loss = 0.030564632266759872
iteration 88, loss = 0.02381865493953228
iteration 89, loss = 0.023388678207993507
iteration 90, loss = 0.025103025138378143
iteration 91, loss = 0.025101235136389732
iteration 92, loss = 0.029234571382403374
iteration 93, loss = 0.025286423042416573
iteration 94, loss = 0.024351775646209717
iteration 95, loss = 0.022627275437116623
iteration 96, loss = 0.02423633076250553
iteration 97, loss = 0.02390557900071144
iteration 98, loss = 0.03155485913157463
iteration 99, loss = 0.023791952058672905
iteration 100, loss = 0.025520021095871925
iteration 101, loss = 0.022746369242668152
iteration 102, loss = 0.0244466383010149
iteration 103, loss = 0.02330971695482731
iteration 104, loss = 0.029954349622130394
iteration 105, loss = 0.02390415593981743
iteration 106, loss = 0.03300498053431511
iteration 107, loss = 0.022290896624326706
iteration 108, loss = 0.031028974801301956
iteration 109, loss = 0.024925583973526955
iteration 110, loss = 0.029760317876935005
iteration 111, loss = 0.02470727078616619
iteration 112, loss = 0.024612972512841225
iteration 113, loss = 0.02415223978459835
iteration 114, loss = 0.02426162175834179
iteration 115, loss = 0.02741643413901329
iteration 116, loss = 0.02459118887782097
iteration 117, loss = 0.02416621521115303
iteration 118, loss = 0.02289232611656189
iteration 119, loss = 0.027177421376109123
iteration 120, loss = 0.023626292124390602
iteration 121, loss = 0.02254745550453663
iteration 122, loss = 0.023547161370515823
iteration 123, loss = 0.023673977702856064
iteration 124, loss = 0.023955874145030975
iteration 125, loss = 0.021994944661855698
iteration 126, loss = 0.023422081023454666
iteration 127, loss = 0.02787942625582218
iteration 128, loss = 0.02306012064218521
iteration 129, loss = 0.022646067664027214
iteration 130, loss = 0.022335419431328773
iteration 131, loss = 0.030117785558104515
iteration 132, loss = 0.0300510972738266
iteration 133, loss = 0.027920333668589592
iteration 134, loss = 0.03302621841430664
iteration 135, loss = 0.023172983899712563
iteration 136, loss = 0.022959226742386818
iteration 137, loss = 0.024531397968530655
iteration 138, loss = 0.023129902780056
iteration 139, loss = 0.026071853935718536
iteration 140, loss = 0.022872447967529297
iteration 141, loss = 0.023061219602823257
iteration 142, loss = 0.02651256136596203
iteration 143, loss = 0.029331691563129425
iteration 144, loss = 0.021427059546113014
iteration 145, loss = 0.022362036630511284
iteration 146, loss = 0.02419585920870304
iteration 147, loss = 0.021201116964221
iteration 148, loss = 0.02438943088054657
iteration 149, loss = 0.022308414801955223
iteration 150, loss = 0.02320532687008381
iteration 151, loss = 0.021898115053772926
iteration 152, loss = 0.023358862847089767
iteration 153, loss = 0.020699338987469673
iteration 154, loss = 0.022088011726737022
iteration 155, loss = 0.02355870045721531
iteration 156, loss = 0.02445916458964348
iteration 157, loss = 0.022404426708817482
iteration 158, loss = 0.03889889642596245
iteration 159, loss = 0.024577822536230087
iteration 160, loss = 0.023692341521382332
iteration 161, loss = 0.021320948377251625
iteration 162, loss = 0.02401559241116047
iteration 163, loss = 0.021663744002580643
iteration 164, loss = 0.020985057577490807
iteration 165, loss = 0.022393079474568367
iteration 166, loss = 0.023070666939020157
iteration 167, loss = 0.027967555448412895
iteration 168, loss = 0.020890286192297935
iteration 169, loss = 0.02224585972726345
iteration 170, loss = 0.027241917327046394
iteration 171, loss = 0.0267241969704628
iteration 172, loss = 0.020704880356788635
iteration 173, loss = 0.020804833620786667
iteration 174, loss = 0.022589176893234253
iteration 175, loss = 0.024014122784137726
iteration 176, loss = 0.024577166885137558
iteration 177, loss = 0.026941314339637756
iteration 178, loss = 0.02180694043636322
iteration 179, loss = 0.025942979380488396
iteration 180, loss = 0.020186126232147217
iteration 181, loss = 0.025763623416423798
iteration 182, loss = 0.021598106250166893
iteration 183, loss = 0.02049488201737404
iteration 184, loss = 0.027863912284374237
iteration 185, loss = 0.020755182951688766
iteration 186, loss = 0.020449761301279068
iteration 187, loss = 0.02598450519144535
iteration 188, loss = 0.024462338536977768
iteration 189, loss = 0.021459383890032768
iteration 190, loss = 0.0199838038533926
iteration 191, loss = 0.020467059686779976
iteration 192, loss = 0.03019086830317974
iteration 193, loss = 0.02548231929540634
iteration 194, loss = 0.020143916830420494
iteration 195, loss = 0.023051247000694275
iteration 196, loss = 0.020389141514897346
iteration 197, loss = 0.020754797384142876
iteration 198, loss = 0.02668336033821106
iteration 199, loss = 0.024039380252361298
iteration 200, loss = 0.02155393734574318
iteration 201, loss = 0.021522872149944305
iteration 202, loss = 0.02025199495255947
iteration 203, loss = 0.02153836376965046
iteration 204, loss = 0.02028970792889595
iteration 205, loss = 0.02023365907371044
iteration 206, loss = 0.020999295637011528
iteration 207, loss = 0.02133716270327568
iteration 208, loss = 0.02024703286588192
iteration 209, loss = 0.02509189583361149
iteration 210, loss = 0.02176067791879177
iteration 211, loss = 0.02218727208673954
iteration 212, loss = 0.028242383152246475
iteration 213, loss = 0.028420915827155113
iteration 214, loss = 0.021845586597919464
iteration 215, loss = 0.022000055760145187
iteration 216, loss = 0.020546235144138336
iteration 217, loss = 0.020091889426112175
iteration 218, loss = 0.020215366035699844
iteration 219, loss = 0.02129138819873333
iteration 220, loss = 0.02080642245709896
iteration 221, loss = 0.019731275737285614
iteration 222, loss = 0.01947157084941864
iteration 223, loss = 0.01963210292160511
iteration 224, loss = 0.024984709918498993
iteration 225, loss = 0.022313512861728668
iteration 226, loss = 0.020436258986592293
iteration 227, loss = 0.02929825894534588
iteration 228, loss = 0.02591259405016899
iteration 229, loss = 0.01915563829243183
iteration 230, loss = 0.02058250829577446
iteration 231, loss = 0.019062913954257965
iteration 232, loss = 0.022609742358326912
iteration 233, loss = 0.01984243653714657
iteration 234, loss = 0.019784346222877502
iteration 235, loss = 0.019301092252135277
iteration 236, loss = 0.023638807237148285
iteration 237, loss = 0.020312190055847168
iteration 238, loss = 0.019786657765507698
iteration 239, loss = 0.023004455491900444
iteration 240, loss = 0.022380663082003593
iteration 241, loss = 0.019610673189163208
iteration 242, loss = 0.026788249611854553
iteration 243, loss = 0.024288076907396317
iteration 244, loss = 0.021183768287301064
iteration 245, loss = 0.02510000579059124
iteration 246, loss = 0.019956238567829132
iteration 247, loss = 0.01902776025235653
iteration 248, loss = 0.018784357234835625
iteration 249, loss = 0.02121071144938469
iteration 250, loss = 0.01999087445437908
iteration 251, loss = 0.020372552797198296
iteration 252, loss = 0.0217713825404644
iteration 253, loss = 0.02486174926161766
iteration 254, loss = 0.018193187192082405
iteration 255, loss = 0.01914375275373459
iteration 256, loss = 0.0233758632093668
iteration 257, loss = 0.02041221410036087
iteration 258, loss = 0.019857024773955345
iteration 259, loss = 0.022641323506832123
iteration 260, loss = 0.026199040934443474
iteration 261, loss = 0.024144411087036133
iteration 262, loss = 0.01783352717757225
iteration 263, loss = 0.022290674969553947
iteration 264, loss = 0.019721001386642456
iteration 265, loss = 0.019827375188469887
iteration 266, loss = 0.0249762162566185
iteration 267, loss = 0.019713981077075005
iteration 268, loss = 0.023117806762456894
iteration 269, loss = 0.02647237665951252
iteration 270, loss = 0.019922297447919846
iteration 271, loss = 0.01919427141547203
iteration 272, loss = 0.018128598108887672
iteration 273, loss = 0.01844365894794464
iteration 274, loss = 0.022477151826024055
iteration 275, loss = 0.019676560536026955
iteration 276, loss = 0.01769040897488594
iteration 277, loss = 0.018316632136702538
iteration 278, loss = 0.01700625941157341
iteration 279, loss = 0.018662767484784126
iteration 280, loss = 0.017669441178441048
iteration 281, loss = 0.019035253673791885
iteration 282, loss = 0.018822748214006424
iteration 283, loss = 0.018859701231122017
iteration 284, loss = 0.019412744790315628
iteration 285, loss = 0.01874919794499874
iteration 286, loss = 0.02505279891192913
iteration 287, loss = 0.01905738189816475
iteration 288, loss = 0.018825670704245567
iteration 289, loss = 0.01831725984811783
iteration 290, loss = 0.01803779788315296
iteration 291, loss = 0.01781880110502243
iteration 292, loss = 0.021197112277150154
iteration 293, loss = 0.017490189522504807
iteration 294, loss = 0.019032033160328865
iteration 295, loss = 0.01842156983911991
iteration 296, loss = 0.02539142221212387
iteration 297, loss = 0.01932380348443985
iteration 298, loss = 0.018606087192893028
iteration 299, loss = 0.017961226403713226
iteration 300, loss = 0.02246066927909851
iteration 1, loss = 0.01903308741748333
iteration 2, loss = 0.019140642136335373
iteration 3, loss = 0.018997354432940483
iteration 4, loss = 0.017635660246014595
iteration 5, loss = 0.02287680096924305
iteration 6, loss = 0.017743926495313644
iteration 7, loss = 0.018207840621471405
iteration 8, loss = 0.018777940422296524
iteration 9, loss = 0.024343110620975494
iteration 10, loss = 0.02088472805917263
iteration 11, loss = 0.019372615963220596
iteration 12, loss = 0.017343416810035706
iteration 13, loss = 0.018476281315088272
iteration 14, loss = 0.02705656737089157
iteration 15, loss = 0.018188290297985077
iteration 16, loss = 0.018373901024460793
iteration 17, loss = 0.022501084953546524
iteration 18, loss = 0.018749136477708817
iteration 19, loss = 0.018758557736873627
iteration 20, loss = 0.018640533089637756
iteration 21, loss = 0.021241940557956696
iteration 22, loss = 0.018259156495332718
iteration 23, loss = 0.017294803634285927
iteration 24, loss = 0.018683217465877533
iteration 25, loss = 0.01763840764760971
iteration 26, loss = 0.0200777780264616
iteration 27, loss = 0.01685859076678753
iteration 28, loss = 0.017611276358366013
iteration 29, loss = 0.01832243986427784
iteration 30, loss = 0.019201822578907013
iteration 31, loss = 0.01851067878305912
iteration 32, loss = 0.016903916373848915
iteration 33, loss = 0.017666781321167946
iteration 34, loss = 0.018841048702597618
iteration 35, loss = 0.017847763374447823
iteration 36, loss = 0.016248127445578575
iteration 37, loss = 0.023614075034856796
iteration 38, loss = 0.01998479664325714
iteration 39, loss = 0.019435495138168335
iteration 40, loss = 0.02649274654686451
iteration 41, loss = 0.01701272837817669
iteration 42, loss = 0.019567422568798065
iteration 43, loss = 0.023999197408556938
iteration 44, loss = 0.016860149800777435
iteration 45, loss = 0.016296755522489548
iteration 46, loss = 0.019417624920606613
iteration 47, loss = 0.016611911356449127
iteration 48, loss = 0.016384243965148926
iteration 49, loss = 0.01713849976658821
iteration 50, loss = 0.016795655712485313
iteration 51, loss = 0.01784009486436844
iteration 52, loss = 0.017443882301449776
iteration 53, loss = 0.02341657690703869
iteration 54, loss = 0.023494431748986244
iteration 55, loss = 0.02106190100312233
iteration 56, loss = 0.018861405551433563
iteration 57, loss = 0.016627291217446327
iteration 58, loss = 0.01699374057352543
iteration 59, loss = 0.01621383987367153
iteration 60, loss = 0.020614083856344223
iteration 61, loss = 0.016839345917105675
iteration 62, loss = 0.01729102060198784
iteration 63, loss = 0.023632926866412163
iteration 64, loss = 0.023460105061531067
iteration 65, loss = 0.023023545742034912
iteration 66, loss = 0.01786484569311142
iteration 67, loss = 0.01644699089229107
iteration 68, loss = 0.018271498382091522
iteration 69, loss = 0.022989924997091293
iteration 70, loss = 0.01870400458574295
iteration 71, loss = 0.025892971083521843
iteration 72, loss = 0.01991044171154499
iteration 73, loss = 0.0171565730124712
iteration 74, loss = 0.01614633947610855
iteration 75, loss = 0.02281039208173752
iteration 76, loss = 0.021692251786589622
iteration 77, loss = 0.016686925664544106
iteration 78, loss = 0.0171208493411541
iteration 79, loss = 0.02020544558763504
iteration 80, loss = 0.015273679979145527
iteration 81, loss = 0.019942548125982285
iteration 82, loss = 0.016422318294644356
iteration 83, loss = 0.015670841559767723
iteration 84, loss = 0.017307013273239136
iteration 85, loss = 0.018951116129755974
iteration 86, loss = 0.016318604350090027
iteration 87, loss = 0.016767464578151703
iteration 88, loss = 0.02940327487885952
iteration 89, loss = 0.016348157078027725
iteration 90, loss = 0.016292734071612358
iteration 91, loss = 0.01645411178469658
iteration 92, loss = 0.016209276393055916
iteration 93, loss = 0.016199609264731407
iteration 94, loss = 0.018390344455838203
iteration 95, loss = 0.01739898882806301
iteration 96, loss = 0.015655547380447388
iteration 97, loss = 0.01992466300725937
iteration 98, loss = 0.017044251784682274
iteration 99, loss = 0.01656308025121689
iteration 100, loss = 0.01592685654759407
iteration 101, loss = 0.016320709139108658
iteration 102, loss = 0.016952460631728172
iteration 103, loss = 0.016347019001841545
iteration 104, loss = 0.017410822212696075
iteration 105, loss = 0.01585501804947853
iteration 106, loss = 0.019586436450481415
iteration 107, loss = 0.01715945079922676
iteration 108, loss = 0.020477816462516785
iteration 109, loss = 0.020373420789837837
iteration 110, loss = 0.01649089902639389
iteration 111, loss = 0.016429394483566284
iteration 112, loss = 0.019591843709349632
iteration 113, loss = 0.019176065921783447
iteration 114, loss = 0.016353333368897438
iteration 115, loss = 0.018137173727154732
iteration 116, loss = 0.015032707713544369
iteration 117, loss = 0.01657157950103283
iteration 118, loss = 0.01555774174630642
iteration 119, loss = 0.015152732841670513
iteration 120, loss = 0.019239244982600212
iteration 121, loss = 0.01647711731493473
iteration 122, loss = 0.015595520846545696
iteration 123, loss = 0.015530773438513279
iteration 124, loss = 0.01511609461158514
iteration 125, loss = 0.015216653235256672
iteration 126, loss = 0.016988500952720642
iteration 127, loss = 0.01770019344985485
iteration 128, loss = 0.0201752707362175
iteration 129, loss = 0.015391235239803791
iteration 130, loss = 0.014968950301408768
iteration 131, loss = 0.014989547431468964
iteration 132, loss = 0.015530044212937355
iteration 133, loss = 0.016018826514482498
iteration 134, loss = 0.015826495364308357
iteration 135, loss = 0.016260845586657524
iteration 136, loss = 0.016063543036580086
iteration 137, loss = 0.01498427800834179
iteration 138, loss = 0.01851133070886135
iteration 139, loss = 0.01594642363488674
iteration 140, loss = 0.01688552275300026
iteration 141, loss = 0.015392537228763103
iteration 142, loss = 0.015361118130385876
iteration 143, loss = 0.01491586770862341
iteration 144, loss = 0.01651882752776146
iteration 145, loss = 0.02139294147491455
iteration 146, loss = 0.01695450395345688
iteration 147, loss = 0.01619654893875122
iteration 148, loss = 0.015103260055184364
iteration 149, loss = 0.01535702683031559
iteration 150, loss = 0.014453962445259094
iteration 151, loss = 0.015151643194258213
iteration 152, loss = 0.015537574887275696
iteration 153, loss = 0.014763277024030685
iteration 154, loss = 0.018129613250494003
iteration 155, loss = 0.015694089233875275
iteration 156, loss = 0.015859244391322136
iteration 157, loss = 0.015495629981160164
iteration 158, loss = 0.015376261435449123
iteration 159, loss = 0.019013050943613052
iteration 160, loss = 0.01525071356445551
iteration 161, loss = 0.01615745946764946
iteration 162, loss = 0.015253230929374695
iteration 163, loss = 0.014861546456813812
iteration 164, loss = 0.023824350908398628
iteration 165, loss = 0.01461138017475605
iteration 166, loss = 0.014416470192372799
iteration 167, loss = 0.014978860504925251
iteration 168, loss = 0.015193019062280655
iteration 169, loss = 0.01936141401529312
iteration 170, loss = 0.014295226894319057
iteration 171, loss = 0.018310312181711197
iteration 172, loss = 0.015355517156422138
iteration 173, loss = 0.015773097053170204
iteration 174, loss = 0.016534538939595222
iteration 175, loss = 0.017059404402971268
iteration 176, loss = 0.01479941513389349
iteration 177, loss = 0.014645440503954887
iteration 178, loss = 0.014487938955426216
iteration 179, loss = 0.019622182473540306
iteration 180, loss = 0.015972984954714775
iteration 181, loss = 0.014496422372758389
iteration 182, loss = 0.018393564969301224
iteration 183, loss = 0.019985835999250412
iteration 184, loss = 0.015713810920715332
iteration 185, loss = 0.015301027335226536
iteration 186, loss = 0.014434256590902805
iteration 187, loss = 0.019587012007832527
iteration 188, loss = 0.014732721261680126
iteration 189, loss = 0.014187662862241268
iteration 190, loss = 0.014441982842981815
iteration 191, loss = 0.02017916366457939
iteration 192, loss = 0.014615761116147041
iteration 193, loss = 0.01770603470504284
iteration 194, loss = 0.01403630618005991
iteration 195, loss = 0.014891543425619602
iteration 196, loss = 0.014771653339266777
iteration 197, loss = 0.014972011558711529
iteration 198, loss = 0.01935313642024994
iteration 199, loss = 0.01401462871581316
iteration 200, loss = 0.021207798272371292
iteration 201, loss = 0.014987165108323097
iteration 202, loss = 0.01800418086349964
iteration 203, loss = 0.01981636881828308
iteration 204, loss = 0.01454699132591486
iteration 205, loss = 0.013735641725361347
iteration 206, loss = 0.014796655625104904
iteration 207, loss = 0.017558535560965538
iteration 208, loss = 0.018400218337774277
iteration 209, loss = 0.020469192415475845
iteration 210, loss = 0.014001959934830666
iteration 211, loss = 0.013780997134745121
iteration 212, loss = 0.01878030225634575
iteration 213, loss = 0.013840571977198124
iteration 214, loss = 0.014850580133497715
iteration 215, loss = 0.016671763733029366
iteration 216, loss = 0.014299146831035614
iteration 217, loss = 0.014340483583509922
iteration 218, loss = 0.017281092703342438
iteration 219, loss = 0.013051281683146954
iteration 220, loss = 0.01677827723324299
iteration 221, loss = 0.013671927154064178
iteration 222, loss = 0.014224461279809475
iteration 223, loss = 0.013408618979156017
iteration 224, loss = 0.013453932479023933
iteration 225, loss = 0.01434828620404005
iteration 226, loss = 0.013217726722359657
iteration 227, loss = 0.014246722683310509
iteration 228, loss = 0.013644825667142868
iteration 229, loss = 0.014710568822920322
iteration 230, loss = 0.014119107276201248
iteration 231, loss = 0.016268039122223854
iteration 232, loss = 0.013595554046332836
iteration 233, loss = 0.017055898904800415
iteration 234, loss = 0.015678679570555687
iteration 235, loss = 0.013714399188756943
iteration 236, loss = 0.016560424119234085
iteration 237, loss = 0.013164370320737362
iteration 238, loss = 0.015578878112137318
iteration 239, loss = 0.013984235003590584
iteration 240, loss = 0.015665069222450256
iteration 241, loss = 0.01410657074302435
iteration 242, loss = 0.013821429572999477
iteration 243, loss = 0.013654730282723904
iteration 244, loss = 0.013591969385743141
iteration 245, loss = 0.013138906098902225
iteration 246, loss = 0.021436750888824463
iteration 247, loss = 0.014013600535690784
iteration 248, loss = 0.016716916114091873
iteration 249, loss = 0.012890364043414593
iteration 250, loss = 0.013906133361160755
iteration 251, loss = 0.015184854157269001
iteration 252, loss = 0.013740350492298603
iteration 253, loss = 0.012999032624065876
iteration 254, loss = 0.01612546108663082
iteration 255, loss = 0.013961012475192547
iteration 256, loss = 0.021750545129179955
iteration 257, loss = 0.013827674090862274
iteration 258, loss = 0.013802225701510906
iteration 259, loss = 0.015040108002722263
iteration 260, loss = 0.018376046791672707
iteration 261, loss = 0.01488029956817627
iteration 262, loss = 0.015118653886020184
iteration 263, loss = 0.013192144222557545
iteration 264, loss = 0.014273147098720074
iteration 265, loss = 0.013235270977020264
iteration 266, loss = 0.012900922447443008
iteration 267, loss = 0.012795726768672466
iteration 268, loss = 0.019326550886034966
iteration 269, loss = 0.014821135438978672
iteration 270, loss = 0.014105173759162426
iteration 271, loss = 0.013810507953166962
iteration 272, loss = 0.013088501989841461
iteration 273, loss = 0.016464512795209885
iteration 274, loss = 0.016826702281832695
iteration 275, loss = 0.013477222993969917
iteration 276, loss = 0.015116614289581776
iteration 277, loss = 0.01240444090217352
iteration 278, loss = 0.014986571855843067
iteration 279, loss = 0.013019957579672337
iteration 280, loss = 0.014259150251746178
iteration 281, loss = 0.01362425833940506
iteration 282, loss = 0.015726814046502113
iteration 283, loss = 0.013139616698026657
iteration 284, loss = 0.014265328645706177
iteration 285, loss = 0.013015558943152428
iteration 286, loss = 0.012705647386610508
iteration 287, loss = 0.012869134545326233
iteration 288, loss = 0.013272464275360107
iteration 289, loss = 0.013989565894007683
iteration 290, loss = 0.013003977946937084
iteration 291, loss = 0.012327571399509907
iteration 292, loss = 0.013055937364697456
iteration 293, loss = 0.013343511149287224
iteration 294, loss = 0.014398768544197083
iteration 295, loss = 0.012576371431350708
iteration 296, loss = 0.013343637809157372
iteration 297, loss = 0.012434126809239388
iteration 298, loss = 0.013173823244869709
iteration 299, loss = 0.016127372160553932
iteration 300, loss = 0.013665830716490746
iteration 1, loss = 0.012820121832191944
iteration 2, loss = 0.012350714765489101
iteration 3, loss = 0.012180252000689507
iteration 4, loss = 0.013036157004535198
iteration 5, loss = 0.015580997802317142
iteration 6, loss = 0.018908705562353134
iteration 7, loss = 0.014238296076655388
iteration 8, loss = 0.01559280976653099
iteration 9, loss = 0.013029356487095356
iteration 10, loss = 0.013320397585630417
iteration 11, loss = 0.01698879338800907
iteration 12, loss = 0.014435582794249058
iteration 13, loss = 0.012370123527944088
iteration 14, loss = 0.01839938759803772
iteration 15, loss = 0.013969252817332745
iteration 16, loss = 0.01277598924934864
iteration 17, loss = 0.012055954895913601
iteration 18, loss = 0.01287039928138256
iteration 19, loss = 0.01478656567633152
iteration 20, loss = 0.014953440055251122
iteration 21, loss = 0.01258747186511755
iteration 22, loss = 0.01597750000655651
iteration 23, loss = 0.012296321801841259
iteration 24, loss = 0.015811597928404808
iteration 25, loss = 0.012893629260361195
iteration 26, loss = 0.013340639881789684
iteration 27, loss = 0.014698580838739872
iteration 28, loss = 0.013013823889195919
iteration 29, loss = 0.012884770520031452
iteration 30, loss = 0.01811971515417099
iteration 31, loss = 0.012349582277238369
iteration 32, loss = 0.012574659660458565
iteration 33, loss = 0.013683132827281952
iteration 34, loss = 0.011905323714017868
iteration 35, loss = 0.013453390449285507
iteration 36, loss = 0.012307755649089813
iteration 37, loss = 0.013339780271053314
iteration 38, loss = 0.012319209054112434
iteration 39, loss = 0.01970653049647808
iteration 40, loss = 0.012075275182723999
iteration 41, loss = 0.011419598944485188
iteration 42, loss = 0.013314645737409592
iteration 43, loss = 0.013109211809933186
iteration 44, loss = 0.01201041229069233
iteration 45, loss = 0.011474689468741417
iteration 46, loss = 0.012917493470013142
iteration 47, loss = 0.012745763175189495
iteration 48, loss = 0.012510022148489952
iteration 49, loss = 0.013740317896008492
iteration 50, loss = 0.012841357849538326
iteration 51, loss = 0.012661960907280445
iteration 52, loss = 0.011783869937062263
iteration 53, loss = 0.016687016934156418
iteration 54, loss = 0.01326721627265215
iteration 55, loss = 0.011961272917687893
iteration 56, loss = 0.012650574557483196
iteration 57, loss = 0.012021642178297043
iteration 58, loss = 0.011695364490151405
iteration 59, loss = 0.017206894233822823
iteration 60, loss = 0.012267077341675758
iteration 61, loss = 0.013375877402722836
iteration 62, loss = 0.012052102014422417
iteration 63, loss = 0.011307732202112675
iteration 64, loss = 0.015018596313893795
iteration 65, loss = 0.014707658439874649
iteration 66, loss = 0.012705609202384949
iteration 67, loss = 0.01251270342618227
iteration 68, loss = 0.011454807594418526
iteration 69, loss = 0.01164750475436449
iteration 70, loss = 0.0115137854591012
iteration 71, loss = 0.012453551404178143
iteration 72, loss = 0.01608601026237011
iteration 73, loss = 0.011908980086445808
iteration 74, loss = 0.01256497111171484
iteration 75, loss = 0.014984148554503918
iteration 76, loss = 0.012812808156013489
iteration 77, loss = 0.014103948138654232
iteration 78, loss = 0.02003927156329155
iteration 79, loss = 0.01213640533387661
iteration 80, loss = 0.01752147637307644
iteration 81, loss = 0.011666616424918175
iteration 82, loss = 0.011494955979287624
iteration 83, loss = 0.013945800252258778
iteration 84, loss = 0.012322344817221165
iteration 85, loss = 0.014608403667807579
iteration 86, loss = 0.016719689592719078
iteration 87, loss = 0.012069903314113617
iteration 88, loss = 0.016759105026721954
iteration 89, loss = 0.011291038244962692
iteration 90, loss = 0.010518861003220081
iteration 91, loss = 0.012532025575637817
iteration 92, loss = 0.012553283013403416
iteration 93, loss = 0.011430265381932259
iteration 94, loss = 0.014748647809028625
iteration 95, loss = 0.012988795526325703
iteration 96, loss = 0.01327209360897541
iteration 97, loss = 0.011710137128829956
iteration 98, loss = 0.01275103259831667
iteration 99, loss = 0.015659214928746223
iteration 100, loss = 0.011484985239803791
iteration 101, loss = 0.011416425928473473
iteration 102, loss = 0.01678457111120224
iteration 103, loss = 0.013208191841840744
iteration 104, loss = 0.011252539232373238
iteration 105, loss = 0.012094034813344479
iteration 106, loss = 0.011740422807633877
iteration 107, loss = 0.012374311685562134
iteration 108, loss = 0.011477163061499596
iteration 109, loss = 0.011440139263868332
iteration 110, loss = 0.012632149271667004
iteration 111, loss = 0.011793999001383781
iteration 112, loss = 0.011786669492721558
iteration 113, loss = 0.012508363462984562
iteration 114, loss = 0.011043590493500233
iteration 115, loss = 0.013948551379144192
iteration 116, loss = 0.011235019192099571
iteration 117, loss = 0.011499942280352116
iteration 118, loss = 0.011309875175356865
iteration 119, loss = 0.011644236743450165
iteration 120, loss = 0.011931629851460457
iteration 121, loss = 0.01378935482352972
iteration 122, loss = 0.012077637016773224
iteration 123, loss = 0.011504927650094032
iteration 124, loss = 0.014064626768231392
iteration 125, loss = 0.012234327383339405
iteration 126, loss = 0.011866839602589607
iteration 127, loss = 0.011191456578671932
iteration 128, loss = 0.011590639129281044
iteration 129, loss = 0.014561287127435207
iteration 130, loss = 0.010896259918808937
iteration 131, loss = 0.011739599518477917
iteration 132, loss = 0.010252827778458595
iteration 133, loss = 0.013092105276882648
iteration 134, loss = 0.010987595655024052
iteration 135, loss = 0.012081475928425789
iteration 136, loss = 0.012250910513103008
iteration 137, loss = 0.011560995131731033
iteration 138, loss = 0.014272792264819145
iteration 139, loss = 0.010926226153969765
iteration 140, loss = 0.010734204202890396
iteration 141, loss = 0.012932178564369678
iteration 142, loss = 0.018580179661512375
iteration 143, loss = 0.011795337311923504
iteration 144, loss = 0.013133627362549305
iteration 145, loss = 0.01189118530601263
iteration 146, loss = 0.011443279683589935
iteration 147, loss = 0.01700827106833458
iteration 148, loss = 0.019892564043402672
iteration 149, loss = 0.013971715234220028
iteration 150, loss = 0.011185596697032452
iteration 151, loss = 0.01116472389549017
iteration 152, loss = 0.010634003207087517
iteration 153, loss = 0.01123861875385046
iteration 154, loss = 0.01089866366237402
iteration 155, loss = 0.011097722686827183
iteration 156, loss = 0.013341685757040977
iteration 157, loss = 0.010917918756604195
iteration 158, loss = 0.011972016654908657
iteration 159, loss = 0.011037264950573444
iteration 160, loss = 0.011095374822616577
iteration 161, loss = 0.010358630679547787
iteration 162, loss = 0.011022830381989479
iteration 163, loss = 0.013354165479540825
iteration 164, loss = 0.011660372838377953
iteration 165, loss = 0.010829468257725239
iteration 166, loss = 0.010618911124765873
iteration 167, loss = 0.010243740864098072
iteration 168, loss = 0.011062586680054665
iteration 169, loss = 0.01092834584414959
iteration 170, loss = 0.011198866181075573
iteration 171, loss = 0.01034932304173708
iteration 172, loss = 0.014729182235896587
iteration 173, loss = 0.011030282825231552
iteration 174, loss = 0.01405115146189928
iteration 175, loss = 0.013747934252023697
iteration 176, loss = 0.014613085426390171
iteration 177, loss = 0.01751582697033882
iteration 178, loss = 0.015621766448020935
iteration 179, loss = 0.011107873171567917
iteration 180, loss = 0.011001150123775005
iteration 181, loss = 0.011196492239832878
iteration 182, loss = 0.01070463564246893
iteration 183, loss = 0.012693914584815502
iteration 184, loss = 0.010937216691672802
iteration 185, loss = 0.015404187142848969
iteration 186, loss = 0.010441637597978115
iteration 187, loss = 0.011046507395803928
iteration 188, loss = 0.010651475749909878
iteration 189, loss = 0.013452719897031784
iteration 190, loss = 0.011227777227759361
iteration 191, loss = 0.010358043015003204
iteration 192, loss = 0.011100765317678452
iteration 193, loss = 0.011000950820744038
iteration 194, loss = 0.01054302416741848
iteration 195, loss = 0.01011066883802414
iteration 196, loss = 0.010272366926074028
iteration 197, loss = 0.011462641879916191
iteration 198, loss = 0.010033308528363705
iteration 199, loss = 0.01589653640985489
iteration 200, loss = 0.010997189208865166
iteration 201, loss = 0.012115499936044216
iteration 202, loss = 0.010705433785915375
iteration 203, loss = 0.010677553713321686
iteration 204, loss = 0.014191364869475365
iteration 205, loss = 0.011920678429305553
iteration 206, loss = 0.01602247729897499
iteration 207, loss = 0.010897363536059856
iteration 208, loss = 0.010637093335390091
iteration 209, loss = 0.010083927772939205
iteration 210, loss = 0.010366966016590595
iteration 211, loss = 0.011299334466457367
iteration 212, loss = 0.011781019158661366
iteration 213, loss = 0.01087532751262188
iteration 214, loss = 0.009981640614569187
iteration 215, loss = 0.01289367489516735
iteration 216, loss = 0.012051894329488277
iteration 217, loss = 0.01064732950180769
iteration 218, loss = 0.012821748852729797
iteration 219, loss = 0.010938320308923721
iteration 220, loss = 0.010620523244142532
iteration 221, loss = 0.009972606785595417
iteration 222, loss = 0.010242665186524391
iteration 223, loss = 0.013296748511493206
iteration 224, loss = 0.010418608784675598
iteration 225, loss = 0.009857880882918835
iteration 226, loss = 0.010552216321229935
iteration 227, loss = 0.011472613550722599
iteration 228, loss = 0.010573694482445717
iteration 229, loss = 0.01058744266629219
iteration 230, loss = 0.012725045904517174
iteration 231, loss = 0.012840449810028076
iteration 232, loss = 0.0152389882132411
iteration 233, loss = 0.013527393341064453
iteration 234, loss = 0.010276798158884048
iteration 235, loss = 0.012551859021186829
iteration 236, loss = 0.010258233174681664
iteration 237, loss = 0.01191193051636219
iteration 238, loss = 0.01379065215587616
iteration 239, loss = 0.009732629172503948
iteration 240, loss = 0.010129631496965885
iteration 241, loss = 0.009703013114631176
iteration 242, loss = 0.010322907008230686
iteration 243, loss = 0.01357785239815712
iteration 244, loss = 0.013174036517739296
iteration 245, loss = 0.012246949598193169
iteration 246, loss = 0.009789440780878067
iteration 247, loss = 0.010081150569021702
iteration 248, loss = 0.012551496736705303
iteration 249, loss = 0.010170049965381622
iteration 250, loss = 0.009753360413014889
iteration 251, loss = 0.009977295063436031
iteration 252, loss = 0.011731390841305256
iteration 253, loss = 0.010580032132565975
iteration 254, loss = 0.009670110419392586
iteration 255, loss = 0.012099426239728928
iteration 256, loss = 0.01007173117250204
iteration 257, loss = 0.009843876585364342
iteration 258, loss = 0.012703347019851208
iteration 259, loss = 0.012456032447516918
iteration 260, loss = 0.011055795475840569
iteration 261, loss = 0.011014629155397415
iteration 262, loss = 0.012858922593295574
iteration 263, loss = 0.010395621880888939
iteration 264, loss = 0.010618871077895164
iteration 265, loss = 0.010488010942935944
iteration 266, loss = 0.009465357288718224
iteration 267, loss = 0.009559533558785915
iteration 268, loss = 0.010711414739489555
iteration 269, loss = 0.010068167932331562
iteration 270, loss = 0.011864196509122849
iteration 271, loss = 0.010485158301889896
iteration 272, loss = 0.010981541126966476
iteration 273, loss = 0.010662805289030075
iteration 274, loss = 0.009546620771288872
iteration 275, loss = 0.010136154480278492
iteration 276, loss = 0.010314966551959515
iteration 277, loss = 0.010525478050112724
iteration 278, loss = 0.009796218946576118
iteration 279, loss = 0.009856174699962139
iteration 280, loss = 0.00945289433002472
iteration 281, loss = 0.014436024241149426
iteration 282, loss = 0.01087836641818285
iteration 283, loss = 0.009433955885469913
iteration 284, loss = 0.011001485399901867
iteration 285, loss = 0.010056473314762115
iteration 286, loss = 0.009836233220994473
iteration 287, loss = 0.00992489978671074
iteration 288, loss = 0.015599633567035198
iteration 289, loss = 0.010219065472483635
iteration 290, loss = 0.010832791216671467
iteration 291, loss = 0.010102855041623116
iteration 292, loss = 0.010591876693069935
iteration 293, loss = 0.010375919751822948
iteration 294, loss = 0.00964290089905262
iteration 295, loss = 0.009508507326245308
iteration 296, loss = 0.009888926520943642
iteration 297, loss = 0.009730007499456406
iteration 298, loss = 0.009929568506777287
iteration 299, loss = 0.00949089229106903
iteration 300, loss = 0.009459658525884151
iteration 1, loss = 0.012476918287575245
iteration 2, loss = 0.011583180166780949
iteration 3, loss = 0.009523251093924046
iteration 4, loss = 0.009094398468732834
iteration 5, loss = 0.010428749956190586
iteration 6, loss = 0.010102187283337116
iteration 7, loss = 0.012963701970875263
iteration 8, loss = 0.009969293139874935
iteration 9, loss = 0.009612411260604858
iteration 10, loss = 0.008937138132750988
iteration 11, loss = 0.009762748144567013
iteration 12, loss = 0.010600295849144459
iteration 13, loss = 0.013413747772574425
iteration 14, loss = 0.009164988994598389
iteration 15, loss = 0.00937721598893404
iteration 16, loss = 0.014326861128211021
iteration 17, loss = 0.00932964775711298
iteration 18, loss = 0.010034492239356041
iteration 19, loss = 0.01096040103584528
iteration 20, loss = 0.009579237550497055
iteration 21, loss = 0.009342922829091549
iteration 22, loss = 0.010091590695083141
iteration 23, loss = 0.00997136626392603
iteration 24, loss = 0.009622202254831791
iteration 25, loss = 0.010185923427343369
iteration 26, loss = 0.009933097288012505
iteration 27, loss = 0.008844989351928234
iteration 28, loss = 0.010177464224398136
iteration 29, loss = 0.009961024858057499
iteration 30, loss = 0.009573956951498985
iteration 31, loss = 0.009250237606465816
iteration 32, loss = 0.009712929837405682
iteration 33, loss = 0.008882954716682434
iteration 34, loss = 0.01014005858451128
iteration 35, loss = 0.010057582519948483
iteration 36, loss = 0.009329082444310188
iteration 37, loss = 0.009559279307723045
iteration 38, loss = 0.009961146861314774
iteration 39, loss = 0.009611382149159908
iteration 40, loss = 0.009495269507169724
iteration 41, loss = 0.013353890739381313
iteration 42, loss = 0.009230910800397396
iteration 43, loss = 0.010210774838924408
iteration 44, loss = 0.01168533693999052
iteration 45, loss = 0.00942655373364687
iteration 46, loss = 0.008858740329742432
iteration 47, loss = 0.012196948751807213
iteration 48, loss = 0.009220685809850693
iteration 49, loss = 0.00960424542427063
iteration 50, loss = 0.00934383925050497
iteration 51, loss = 0.008810861967504025
iteration 52, loss = 0.008682254701852798
iteration 53, loss = 0.010653306730091572
iteration 54, loss = 0.009740152396261692
iteration 55, loss = 0.010421841405332088
iteration 56, loss = 0.010493075475096703
iteration 57, loss = 0.009611979126930237
iteration 58, loss = 0.009384069591760635
iteration 59, loss = 0.010828271508216858
iteration 60, loss = 0.009482715278863907
iteration 61, loss = 0.00971134938299656
iteration 62, loss = 0.009128105826675892
iteration 63, loss = 0.009211555123329163
iteration 64, loss = 0.009367696940898895
iteration 65, loss = 0.010519555769860744
iteration 66, loss = 0.009586171247065067
iteration 67, loss = 0.009533379226922989
iteration 68, loss = 0.009399177506566048
iteration 69, loss = 0.009089706465601921
iteration 70, loss = 0.01395094022154808
iteration 71, loss = 0.013894479721784592
iteration 72, loss = 0.010701876133680344
iteration 73, loss = 0.009503103792667389
iteration 74, loss = 0.008542406372725964
iteration 75, loss = 0.00982674490660429
iteration 76, loss = 0.010309776291251183
iteration 77, loss = 0.010515155270695686
iteration 78, loss = 0.012056385166943073
iteration 79, loss = 0.008628882467746735
iteration 80, loss = 0.00903969258069992
iteration 81, loss = 0.009161843918263912
iteration 82, loss = 0.013769854791462421
iteration 83, loss = 0.009500399231910706
iteration 84, loss = 0.009479057975113392
iteration 85, loss = 0.00886930339038372
iteration 86, loss = 0.00876691099256277
iteration 87, loss = 0.009243601001799107
iteration 88, loss = 0.009878454729914665
iteration 89, loss = 0.009165467694401741
iteration 90, loss = 0.008886114694178104
iteration 91, loss = 0.008633218705654144
iteration 92, loss = 0.009171299636363983
iteration 93, loss = 0.012668882496654987
iteration 94, loss = 0.008627180010080338
iteration 95, loss = 0.00983203575015068
iteration 96, loss = 0.009724131785333157
iteration 97, loss = 0.008683309890329838
iteration 98, loss = 0.009298411197960377
iteration 99, loss = 0.00879514031112194
iteration 100, loss = 0.008479723706841469
iteration 101, loss = 0.008181764744222164
iteration 102, loss = 0.008399688638746738
iteration 103, loss = 0.00839034840464592
iteration 104, loss = 0.011422046460211277
iteration 105, loss = 0.01324409618973732
iteration 106, loss = 0.009021523408591747
iteration 107, loss = 0.009031020104885101
iteration 108, loss = 0.009046390652656555
iteration 109, loss = 0.010212576948106289
iteration 110, loss = 0.009537610225379467
iteration 111, loss = 0.010893811471760273
iteration 112, loss = 0.008547685109078884
iteration 113, loss = 0.008784598670899868
iteration 114, loss = 0.009217842482030392
iteration 115, loss = 0.00869621429592371
iteration 116, loss = 0.00903713796287775
iteration 117, loss = 0.008958621881902218
iteration 118, loss = 0.009087713435292244
iteration 119, loss = 0.008960047736763954
iteration 120, loss = 0.009284726344048977
iteration 121, loss = 0.009085552766919136
iteration 122, loss = 0.013745809905230999
iteration 123, loss = 0.009870055131614208
iteration 124, loss = 0.008895589038729668
iteration 125, loss = 0.008776683360338211
iteration 126, loss = 0.00903440173715353
iteration 127, loss = 0.009353709407150745
iteration 128, loss = 0.008344540372490883
iteration 129, loss = 0.009001918137073517
iteration 130, loss = 0.008644457906484604
iteration 131, loss = 0.00933185312896967
iteration 132, loss = 0.010277025401592255
iteration 133, loss = 0.00864391215145588
iteration 134, loss = 0.010055650025606155
iteration 135, loss = 0.009254363365471363
iteration 136, loss = 0.008804111741483212
iteration 137, loss = 0.008390652947127819
iteration 138, loss = 0.008944307453930378
iteration 139, loss = 0.008862411603331566
iteration 140, loss = 0.008661434054374695
iteration 141, loss = 0.012234698981046677
iteration 142, loss = 0.008732120506465435
iteration 143, loss = 0.009227713569998741
iteration 144, loss = 0.008416244760155678
iteration 145, loss = 0.012597167864441872
iteration 146, loss = 0.011207407340407372
iteration 147, loss = 0.010631164535880089
iteration 148, loss = 0.008297069929540157
iteration 149, loss = 0.008989674970507622
iteration 150, loss = 0.009562753140926361
iteration 151, loss = 0.009854222647845745
iteration 152, loss = 0.00867476500570774
iteration 153, loss = 0.009395146742463112
iteration 154, loss = 0.009427742101252079
iteration 155, loss = 0.01049738097935915
iteration 156, loss = 0.008373509161174297
iteration 157, loss = 0.007882321253418922
iteration 158, loss = 0.008256317116320133
iteration 159, loss = 0.008348445408046246
iteration 160, loss = 0.008399030193686485
iteration 161, loss = 0.008436155505478382
iteration 162, loss = 0.009887837804853916
iteration 163, loss = 0.008615387603640556
iteration 164, loss = 0.010189010761678219
iteration 165, loss = 0.00882001407444477
iteration 166, loss = 0.009376133792102337
iteration 167, loss = 0.011644262820482254
iteration 168, loss = 0.008233926258981228
iteration 169, loss = 0.00950425025075674
iteration 170, loss = 0.011496707797050476
iteration 171, loss = 0.00865447148680687
iteration 172, loss = 0.008570311591029167
iteration 173, loss = 0.012393653392791748
iteration 174, loss = 0.012179253622889519
iteration 175, loss = 0.008154415525496006
iteration 176, loss = 0.008638836443424225
iteration 177, loss = 0.008456172421574593
iteration 178, loss = 0.011592008173465729
iteration 179, loss = 0.010676919482648373
iteration 180, loss = 0.011146718636155128
iteration 181, loss = 0.009674041531980038
iteration 182, loss = 0.010044191032648087
iteration 183, loss = 0.009496835060417652
iteration 184, loss = 0.007972672581672668
iteration 185, loss = 0.008689340204000473
iteration 186, loss = 0.008617728017270565
iteration 187, loss = 0.007962075062096119
iteration 188, loss = 0.007792461663484573
iteration 189, loss = 0.0113616231828928
iteration 190, loss = 0.008552118204534054
iteration 191, loss = 0.008130786009132862
iteration 192, loss = 0.007991361431777477
iteration 193, loss = 0.00793427787721157
iteration 194, loss = 0.01198622863739729
iteration 195, loss = 0.0077397823333740234
iteration 196, loss = 0.00786343589425087
iteration 197, loss = 0.008321753703057766
iteration 198, loss = 0.008310790173709393
iteration 199, loss = 0.007917024195194244
iteration 200, loss = 0.008216755464673042
iteration 201, loss = 0.008450018242001534
iteration 202, loss = 0.008357655256986618
iteration 203, loss = 0.008209426887333393
iteration 204, loss = 0.009261549450457096
iteration 205, loss = 0.01005627028644085
iteration 206, loss = 0.01183741632848978
iteration 207, loss = 0.00917128473520279
iteration 208, loss = 0.00743446359410882
iteration 209, loss = 0.008087286725640297
iteration 210, loss = 0.011952484026551247
iteration 211, loss = 0.008559982292354107
iteration 212, loss = 0.0076907929033041
iteration 213, loss = 0.009574887342751026
iteration 214, loss = 0.008764605037868023
iteration 215, loss = 0.011337900534272194
iteration 216, loss = 0.0075065274722874165
iteration 217, loss = 0.007871031761169434
iteration 218, loss = 0.008098985068500042
iteration 219, loss = 0.008298943750560284
iteration 220, loss = 0.011876299045979977
iteration 221, loss = 0.008338578045368195
iteration 222, loss = 0.008711538277566433
iteration 223, loss = 0.008255131542682648
iteration 224, loss = 0.008319871500134468
iteration 225, loss = 0.007763206958770752
iteration 226, loss = 0.01000085286796093
iteration 227, loss = 0.01077285036444664
iteration 228, loss = 0.007763480301946402
iteration 229, loss = 0.007329108193516731
iteration 230, loss = 0.007626606151461601
iteration 231, loss = 0.010669897310435772
iteration 232, loss = 0.00800808984786272
iteration 233, loss = 0.00791055615991354
iteration 234, loss = 0.00787340383976698
iteration 235, loss = 0.008283515460789204
iteration 236, loss = 0.008940355852246284
iteration 237, loss = 0.009746664203703403
iteration 238, loss = 0.009395376779139042
iteration 239, loss = 0.007600883953273296
iteration 240, loss = 0.01180005632340908
iteration 241, loss = 0.010984056629240513
iteration 242, loss = 0.007200153078883886
iteration 243, loss = 0.008179058320820332
iteration 244, loss = 0.009280132129788399
iteration 245, loss = 0.00788818672299385
iteration 246, loss = 0.008159421384334564
iteration 247, loss = 0.007876850664615631
iteration 248, loss = 0.008074847981333733
iteration 249, loss = 0.007855840027332306
iteration 250, loss = 0.010101092979311943
iteration 251, loss = 0.007813775911927223
iteration 252, loss = 0.007839802652597427
iteration 253, loss = 0.009463776834309101
iteration 254, loss = 0.010858595371246338
iteration 255, loss = 0.00784714799374342
iteration 256, loss = 0.007594385650008917
iteration 257, loss = 0.010149412788450718
iteration 258, loss = 0.007758853025734425
iteration 259, loss = 0.00741264782845974
iteration 260, loss = 0.010062146931886673
iteration 261, loss = 0.011519495397806168
iteration 262, loss = 0.014146221801638603
iteration 263, loss = 0.008198695257306099
iteration 264, loss = 0.012335829436779022
iteration 265, loss = 0.00886552780866623
iteration 266, loss = 0.011977852322161198
iteration 267, loss = 0.007870315574109554
iteration 268, loss = 0.008632946759462357
iteration 269, loss = 0.007452747784554958
iteration 270, loss = 0.007101693656295538
iteration 271, loss = 0.008125639520585537
iteration 272, loss = 0.01025170087814331
iteration 273, loss = 0.007286195643246174
iteration 274, loss = 0.007473775651305914
iteration 275, loss = 0.011327506974339485
iteration 276, loss = 0.009704530239105225
iteration 277, loss = 0.011473279446363449
iteration 278, loss = 0.007871629670262337
iteration 279, loss = 0.007678506895899773
iteration 280, loss = 0.009963751770555973
iteration 281, loss = 0.008641757071018219
iteration 282, loss = 0.008391670882701874
iteration 283, loss = 0.008078656159341335
iteration 284, loss = 0.007383617106825113
iteration 285, loss = 0.008908744901418686
iteration 286, loss = 0.01135107222944498
iteration 287, loss = 0.007477472070604563
iteration 288, loss = 0.008290782570838928
iteration 289, loss = 0.007757139857858419
iteration 290, loss = 0.007439990062266588
iteration 291, loss = 0.008443981409072876
iteration 292, loss = 0.007668833248317242
iteration 293, loss = 0.008220471441745758
iteration 294, loss = 0.007139594294130802
iteration 295, loss = 0.008191750384867191
iteration 296, loss = 0.009572053328156471
iteration 297, loss = 0.00794651173055172
iteration 298, loss = 0.007726067677140236
iteration 299, loss = 0.009439331479370594
iteration 300, loss = 0.00765120517462492
iteration 1, loss = 0.006965090520679951
iteration 2, loss = 0.009754450060427189
iteration 3, loss = 0.01146275456994772
iteration 4, loss = 0.00907796248793602
iteration 5, loss = 0.007496250327676535
iteration 6, loss = 0.007939892821013927
iteration 7, loss = 0.007919705472886562
iteration 8, loss = 0.007361120544373989
iteration 9, loss = 0.007601073011755943
iteration 10, loss = 0.0075358981266617775
iteration 11, loss = 0.00798855721950531
iteration 12, loss = 0.012113920412957668
iteration 13, loss = 0.0072823818773031235
iteration 14, loss = 0.009891258552670479
iteration 15, loss = 0.007566043175756931
iteration 16, loss = 0.009287037886679173
iteration 17, loss = 0.007221401669085026
iteration 18, loss = 0.007577776908874512
iteration 19, loss = 0.007368210703134537
iteration 20, loss = 0.007681417744606733
iteration 21, loss = 0.008298292756080627
iteration 22, loss = 0.007630388252437115
iteration 23, loss = 0.00801280327141285
iteration 24, loss = 0.007200905587524176
iteration 25, loss = 0.0074160899966955185
iteration 26, loss = 0.007417026907205582
iteration 27, loss = 0.008392377756536007
iteration 28, loss = 0.0073251621797680855
iteration 29, loss = 0.009016304276883602
iteration 30, loss = 0.0075456504710018635
iteration 31, loss = 0.006786660756915808
iteration 32, loss = 0.007636124733835459
iteration 33, loss = 0.008328019641339779
iteration 34, loss = 0.009573310613632202
iteration 35, loss = 0.00810914859175682
iteration 36, loss = 0.007546412758529186
iteration 37, loss = 0.0077667501755058765
iteration 38, loss = 0.006997873540967703
iteration 39, loss = 0.007025047671049833
iteration 40, loss = 0.0073588937520980835
iteration 41, loss = 0.00803771149367094
iteration 42, loss = 0.00840573851019144
iteration 43, loss = 0.007642777636647224
iteration 44, loss = 0.0076024034060537815
iteration 45, loss = 0.007212419994175434
iteration 46, loss = 0.007317698560655117
iteration 47, loss = 0.006611051503568888
iteration 48, loss = 0.0076767513528466225
iteration 49, loss = 0.0069190156646072865
iteration 50, loss = 0.00788680836558342
iteration 51, loss = 0.007778109982609749
iteration 52, loss = 0.007434358820319176
iteration 53, loss = 0.007196745369583368
iteration 54, loss = 0.007626662030816078
iteration 55, loss = 0.007653731852769852
iteration 56, loss = 0.007252951618283987
iteration 57, loss = 0.010731317102909088
iteration 58, loss = 0.0075840032659471035
iteration 59, loss = 0.007248789072036743
iteration 60, loss = 0.007547065149992704
iteration 61, loss = 0.008129246532917023
iteration 62, loss = 0.007968643680214882
iteration 63, loss = 0.008922005072236061
iteration 64, loss = 0.009098484180867672
iteration 65, loss = 0.010820402763783932
iteration 66, loss = 0.011622018180787563
iteration 67, loss = 0.008151317946612835
iteration 68, loss = 0.008772484958171844
iteration 69, loss = 0.007025253027677536
iteration 70, loss = 0.008954904042184353
iteration 71, loss = 0.009290813468396664
iteration 72, loss = 0.00858750194311142
iteration 73, loss = 0.008610286749899387
iteration 74, loss = 0.007067931815981865
iteration 75, loss = 0.0072838375344872475
iteration 76, loss = 0.007060314994305372
iteration 77, loss = 0.007408044766634703
iteration 78, loss = 0.00697869248688221
iteration 79, loss = 0.007821870036423206
iteration 80, loss = 0.007104505784809589
iteration 81, loss = 0.00747726671397686
iteration 82, loss = 0.006997804157435894
iteration 83, loss = 0.008039940148591995
iteration 84, loss = 0.010932075791060925
iteration 85, loss = 0.006493777967989445
iteration 86, loss = 0.006880245637148619
iteration 87, loss = 0.010399026796221733
iteration 88, loss = 0.0071557993069291115
iteration 89, loss = 0.007919803261756897
iteration 90, loss = 0.007019731216132641
iteration 91, loss = 0.006668434012681246
iteration 92, loss = 0.0077042183838784695
iteration 93, loss = 0.00844863522797823
iteration 94, loss = 0.010453347116708755
iteration 95, loss = 0.007093407213687897
iteration 96, loss = 0.00929277017712593
iteration 97, loss = 0.007285243831574917
iteration 98, loss = 0.00688590295612812
iteration 99, loss = 0.007144074887037277
iteration 100, loss = 0.006834107451140881
iteration 101, loss = 0.006735415663570166
iteration 102, loss = 0.007025292608886957
iteration 103, loss = 0.0073456112295389175
iteration 104, loss = 0.0072846319526433945
iteration 105, loss = 0.007190575357526541
iteration 106, loss = 0.007289825938642025
iteration 107, loss = 0.006914434023201466
iteration 108, loss = 0.007043312303721905
iteration 109, loss = 0.007094115950167179
iteration 110, loss = 0.007764676120132208
iteration 111, loss = 0.007215308491140604
iteration 112, loss = 0.006430186331272125
iteration 113, loss = 0.0071542332880198956
iteration 114, loss = 0.010325678624212742
iteration 115, loss = 0.007334655150771141
iteration 116, loss = 0.007074528839439154
iteration 117, loss = 0.007405863609164953
iteration 118, loss = 0.006837096065282822
iteration 119, loss = 0.007924224250018597
iteration 120, loss = 0.00691488292068243
iteration 121, loss = 0.010649433359503746
iteration 122, loss = 0.006601705681532621
iteration 123, loss = 0.008136275224387646
iteration 124, loss = 0.006534680724143982
iteration 125, loss = 0.006804009433835745
iteration 126, loss = 0.00652887299656868
iteration 127, loss = 0.0067966696806252
iteration 128, loss = 0.008450140245258808
iteration 129, loss = 0.007064806763082743
iteration 130, loss = 0.006981021258980036
iteration 131, loss = 0.0069735413417220116
iteration 132, loss = 0.006616971455514431
iteration 133, loss = 0.007196508813649416
iteration 134, loss = 0.00813829991966486
iteration 135, loss = 0.006789953447878361
iteration 136, loss = 0.006848771125078201
iteration 137, loss = 0.006901562679558992
iteration 138, loss = 0.006649336311966181
iteration 139, loss = 0.006570134311914444
iteration 140, loss = 0.006487619131803513
iteration 141, loss = 0.01113506406545639
iteration 142, loss = 0.009267793968319893
iteration 143, loss = 0.0067396447993814945
iteration 144, loss = 0.007512150797992945
iteration 145, loss = 0.008063944056630135
iteration 146, loss = 0.006476700305938721
iteration 147, loss = 0.006730793509632349
iteration 148, loss = 0.007338513154536486
iteration 149, loss = 0.00807800143957138
iteration 150, loss = 0.008026228286325932
iteration 151, loss = 0.009904088452458382
iteration 152, loss = 0.0071117770858109
iteration 153, loss = 0.006783109623938799
iteration 154, loss = 0.007251254748553038
iteration 155, loss = 0.006722260732203722
iteration 156, loss = 0.00898721907287836
iteration 157, loss = 0.006825197022408247
iteration 158, loss = 0.007060432806611061
iteration 159, loss = 0.010566692799329758
iteration 160, loss = 0.006581023335456848
iteration 161, loss = 0.00650938693434
iteration 162, loss = 0.007686504162847996
iteration 163, loss = 0.006847710348665714
iteration 164, loss = 0.006650478579103947
iteration 165, loss = 0.007396574132144451
iteration 166, loss = 0.009033214300870895
iteration 167, loss = 0.01006864383816719
iteration 168, loss = 0.006446296814829111
iteration 169, loss = 0.006690911948680878
iteration 170, loss = 0.0071642748080194
iteration 171, loss = 0.006813735235482454
iteration 172, loss = 0.006696056574583054
iteration 173, loss = 0.006608723197132349
iteration 174, loss = 0.00697336345911026
iteration 175, loss = 0.011710066348314285
iteration 176, loss = 0.006683231331408024
iteration 177, loss = 0.006871144752949476
iteration 178, loss = 0.0066720303148031235
iteration 179, loss = 0.0072961412370204926
iteration 180, loss = 0.007016853895038366
iteration 181, loss = 0.006752822548151016
iteration 182, loss = 0.006877174600958824
iteration 183, loss = 0.008336431346833706
iteration 184, loss = 0.006745870690792799
iteration 185, loss = 0.007243270520120859
iteration 186, loss = 0.007658676709979773
iteration 187, loss = 0.006519778165966272
iteration 188, loss = 0.008048140443861485
iteration 189, loss = 0.007107492070645094
iteration 190, loss = 0.00645107589662075
iteration 191, loss = 0.007178823929280043
iteration 192, loss = 0.00626576691865921
iteration 193, loss = 0.006587750744074583
iteration 194, loss = 0.006683323532342911
iteration 195, loss = 0.006258650217205286
iteration 196, loss = 0.006464800331741571
iteration 197, loss = 0.008064957335591316
iteration 198, loss = 0.009328674525022507
iteration 199, loss = 0.006443684455007315
iteration 200, loss = 0.008283669129014015
iteration 201, loss = 0.006579776760190725
iteration 202, loss = 0.00640566973015666
iteration 203, loss = 0.006538106594234705
iteration 204, loss = 0.006921707186847925
iteration 205, loss = 0.00689167995005846
iteration 206, loss = 0.007583123631775379
iteration 207, loss = 0.006723947823047638
iteration 208, loss = 0.006240041460841894
iteration 209, loss = 0.009772522374987602
iteration 210, loss = 0.0058747027069330215
iteration 211, loss = 0.006180986296385527
iteration 212, loss = 0.00994489062577486
iteration 213, loss = 0.006424723193049431
iteration 214, loss = 0.008062463253736496
iteration 215, loss = 0.009303947910666466
iteration 216, loss = 0.006663883104920387
iteration 217, loss = 0.008822773583233356
iteration 218, loss = 0.006724015809595585
iteration 219, loss = 0.007305677980184555
iteration 220, loss = 0.006647522561252117
iteration 221, loss = 0.006293872371315956
iteration 222, loss = 0.005973372142761946
iteration 223, loss = 0.008725598454475403
iteration 224, loss = 0.007765773218125105
iteration 225, loss = 0.006408526096493006
iteration 226, loss = 0.006471629254519939
iteration 227, loss = 0.006049998104572296
iteration 228, loss = 0.008626498281955719
iteration 229, loss = 0.008763865567743778
iteration 230, loss = 0.006489595398306847
iteration 231, loss = 0.0075554009526968
iteration 232, loss = 0.006830708123743534
iteration 233, loss = 0.006593827158212662
iteration 234, loss = 0.006474887952208519
iteration 235, loss = 0.006505352444946766
iteration 236, loss = 0.006373349577188492
iteration 237, loss = 0.006108224391937256
iteration 238, loss = 0.00683538056910038
iteration 239, loss = 0.006129711866378784
iteration 240, loss = 0.006202710792422295
iteration 241, loss = 0.00812913104891777
iteration 242, loss = 0.006961073260754347
iteration 243, loss = 0.006237713620066643
iteration 244, loss = 0.006456097587943077
iteration 245, loss = 0.007858949713408947
iteration 246, loss = 0.006742790807038546
iteration 247, loss = 0.007596876937896013
iteration 248, loss = 0.006425763480365276
iteration 249, loss = 0.006335488520562649
iteration 250, loss = 0.006547911092638969
iteration 251, loss = 0.006430830340832472
iteration 252, loss = 0.006874180864542723
iteration 253, loss = 0.009343325160443783
iteration 254, loss = 0.006277215201407671
iteration 255, loss = 0.01019313745200634
iteration 256, loss = 0.006441251374781132
iteration 257, loss = 0.006615928839892149
iteration 258, loss = 0.00854782946407795
iteration 259, loss = 0.007369597442448139
iteration 260, loss = 0.0075916326604783535
iteration 261, loss = 0.006439511198550463
iteration 262, loss = 0.006741807796061039
iteration 263, loss = 0.006021581124514341
iteration 264, loss = 0.007246603723615408
iteration 265, loss = 0.006533580832183361
iteration 266, loss = 0.005967526230961084
iteration 267, loss = 0.006115125026553869
iteration 268, loss = 0.006821444723755121
iteration 269, loss = 0.008799050934612751
iteration 270, loss = 0.006926894653588533
iteration 271, loss = 0.006470005959272385
iteration 272, loss = 0.00582329835742712
iteration 273, loss = 0.006356930360198021
iteration 274, loss = 0.006221272516995668
iteration 275, loss = 0.00653272308409214
iteration 276, loss = 0.007251595612615347
iteration 277, loss = 0.010071407072246075
iteration 278, loss = 0.006417317781597376
iteration 279, loss = 0.00596629548817873
iteration 280, loss = 0.006895261816680431
iteration 281, loss = 0.006397259887307882
iteration 282, loss = 0.006337224505841732
iteration 283, loss = 0.007440090179443359
iteration 284, loss = 0.006378351245075464
iteration 285, loss = 0.006176288239657879
iteration 286, loss = 0.00952027179300785
iteration 287, loss = 0.00938290636986494
iteration 288, loss = 0.00623138016089797
iteration 289, loss = 0.009472806006669998
iteration 290, loss = 0.0066768997348845005
iteration 291, loss = 0.007903316989541054
iteration 292, loss = 0.005990553647279739
iteration 293, loss = 0.007413945626467466
iteration 294, loss = 0.006963738240301609
iteration 295, loss = 0.008102965541183949
iteration 296, loss = 0.010378752835094929
iteration 297, loss = 0.0062359594739973545
iteration 298, loss = 0.006476695649325848
iteration 299, loss = 0.006942428648471832
iteration 300, loss = 0.0057065170258283615
iteration 1, loss = 0.007081317249685526
iteration 2, loss = 0.0057536824606359005
iteration 3, loss = 0.006889461074024439
iteration 4, loss = 0.009522585198283195
iteration 5, loss = 0.007179318927228451
iteration 6, loss = 0.0065752663649618626
iteration 7, loss = 0.006685730069875717
iteration 8, loss = 0.008595474995672703
iteration 9, loss = 0.007291906047612429
iteration 10, loss = 0.006150900851935148
iteration 11, loss = 0.006275046616792679
iteration 12, loss = 0.007195435930043459
iteration 13, loss = 0.005830781068652868
iteration 14, loss = 0.005878864787518978
iteration 15, loss = 0.006353303790092468
iteration 16, loss = 0.006236004643142223
iteration 17, loss = 0.007152930833399296
iteration 18, loss = 0.005848916247487068
iteration 19, loss = 0.005804653745144606
iteration 20, loss = 0.005978039465844631
iteration 21, loss = 0.0061907293274998665
iteration 22, loss = 0.005945245735347271
iteration 23, loss = 0.006068720482289791
iteration 24, loss = 0.006390244700014591
iteration 25, loss = 0.006631786935031414
iteration 26, loss = 0.006464638747274876
iteration 27, loss = 0.0063586244359612465
iteration 28, loss = 0.007329306565225124
iteration 29, loss = 0.005690479185432196
iteration 30, loss = 0.007789599243551493
iteration 31, loss = 0.008560709655284882
iteration 32, loss = 0.006742332596331835
iteration 33, loss = 0.005898512899875641
iteration 34, loss = 0.0065155234187841415
iteration 35, loss = 0.007291536312550306
iteration 36, loss = 0.009716124273836613
iteration 37, loss = 0.010096522979438305
iteration 38, loss = 0.008191612549126148
iteration 39, loss = 0.00654436694458127
iteration 40, loss = 0.005931168794631958
iteration 41, loss = 0.005661059636622667
iteration 42, loss = 0.007820270024240017
iteration 43, loss = 0.005960858892649412
iteration 44, loss = 0.005777528043836355
iteration 45, loss = 0.006572601851075888
iteration 46, loss = 0.005941650364547968
iteration 47, loss = 0.006444660946726799
iteration 48, loss = 0.006996900774538517
iteration 49, loss = 0.006454684771597385
iteration 50, loss = 0.006313431076705456
iteration 51, loss = 0.0064371549524366856
iteration 52, loss = 0.00610137265175581
iteration 53, loss = 0.0063821119256317616
iteration 54, loss = 0.005773465149104595
iteration 55, loss = 0.009734464809298515
iteration 56, loss = 0.008667104877531528
iteration 57, loss = 0.005784742534160614
iteration 58, loss = 0.006886911578476429
iteration 59, loss = 0.007089646067470312
iteration 60, loss = 0.0077569447457790375
iteration 61, loss = 0.006025947630405426
iteration 62, loss = 0.007444670423865318
iteration 63, loss = 0.006463457830250263
iteration 64, loss = 0.006742896977812052
iteration 65, loss = 0.00867642555385828
iteration 66, loss = 0.006413680035620928
iteration 67, loss = 0.008795342408120632
iteration 68, loss = 0.007920329459011555
iteration 69, loss = 0.007246986497193575
iteration 70, loss = 0.006236807443201542
iteration 71, loss = 0.005945556331425905
iteration 72, loss = 0.009050383232533932
iteration 73, loss = 0.00608563469722867
iteration 74, loss = 0.007527353707700968
iteration 75, loss = 0.006338246166706085
iteration 76, loss = 0.0065047768875956535
iteration 77, loss = 0.009526548907160759
iteration 78, loss = 0.006274131126701832
iteration 79, loss = 0.006879118271172047
iteration 80, loss = 0.005998727399855852
iteration 81, loss = 0.005782427731901407
iteration 82, loss = 0.008964544162154198
iteration 83, loss = 0.005929335951805115
iteration 84, loss = 0.009458716958761215
iteration 85, loss = 0.00711754010990262
iteration 86, loss = 0.006399261299520731
iteration 87, loss = 0.0060567245818674564
iteration 88, loss = 0.006154792383313179
iteration 89, loss = 0.006009092088788748
iteration 90, loss = 0.008051156997680664
iteration 91, loss = 0.005788354203104973
iteration 92, loss = 0.005971338599920273
iteration 93, loss = 0.006102664861828089
iteration 94, loss = 0.005618685856461525
iteration 95, loss = 0.006155385635793209
iteration 96, loss = 0.006487222388386726
iteration 97, loss = 0.006160245276987553
iteration 98, loss = 0.005933945998549461
iteration 99, loss = 0.006932154297828674
iteration 100, loss = 0.009363364428281784
iteration 101, loss = 0.00602112477645278
iteration 102, loss = 0.006046787835657597
iteration 103, loss = 0.005631428211927414
iteration 104, loss = 0.006640101782977581
iteration 105, loss = 0.006335243582725525
iteration 106, loss = 0.006640966050326824
iteration 107, loss = 0.006258221808820963
iteration 108, loss = 0.00814356841146946
iteration 109, loss = 0.005927590187638998
iteration 110, loss = 0.006464806385338306
iteration 111, loss = 0.009210361167788506
iteration 112, loss = 0.005793342366814613
iteration 113, loss = 0.007095177657902241
iteration 114, loss = 0.0055772396735847
iteration 115, loss = 0.0063645788468420506
iteration 116, loss = 0.0062128594145178795
iteration 117, loss = 0.006185078527778387
iteration 118, loss = 0.00637044059112668
iteration 119, loss = 0.006123868748545647
iteration 120, loss = 0.006685695145279169
iteration 121, loss = 0.005640690680593252
iteration 122, loss = 0.006106862798333168
iteration 123, loss = 0.006665821652859449
iteration 124, loss = 0.007257434539496899
iteration 125, loss = 0.005658460780978203
iteration 126, loss = 0.006285050883889198
iteration 127, loss = 0.006108968984335661
iteration 128, loss = 0.006225094199180603
iteration 129, loss = 0.006068080198019743
iteration 130, loss = 0.005514036398380995
iteration 131, loss = 0.006029215641319752
iteration 132, loss = 0.006142597179859877
iteration 133, loss = 0.005654357839375734
iteration 134, loss = 0.006825094111263752
iteration 135, loss = 0.007148338481783867
iteration 136, loss = 0.005570721346884966
iteration 137, loss = 0.0093727121129632
iteration 138, loss = 0.005853339564055204
iteration 139, loss = 0.00588001636788249
iteration 140, loss = 0.006213394924998283
iteration 141, loss = 0.005638528615236282
iteration 142, loss = 0.005670397542417049
iteration 143, loss = 0.005876247771084309
iteration 144, loss = 0.006281907670199871
iteration 145, loss = 0.00598714267835021
iteration 146, loss = 0.005828646011650562
iteration 147, loss = 0.0056760478764772415
iteration 148, loss = 0.006360911298543215
iteration 149, loss = 0.006455256137996912
iteration 150, loss = 0.006136366631835699
iteration 151, loss = 0.005939626134932041
iteration 152, loss = 0.007922410033643246
iteration 153, loss = 0.007201564963907003
iteration 154, loss = 0.007332241628319025
iteration 155, loss = 0.0068986802361905575
iteration 156, loss = 0.006036410573869944
iteration 157, loss = 0.005824068561196327
iteration 158, loss = 0.00773845799267292
iteration 159, loss = 0.007539897691458464
iteration 160, loss = 0.007655646651983261
iteration 161, loss = 0.006078184582293034
iteration 162, loss = 0.006868122611194849
iteration 163, loss = 0.006112182512879372
iteration 164, loss = 0.006406307686120272
iteration 165, loss = 0.0061933863908052444
iteration 166, loss = 0.006453479174524546
iteration 167, loss = 0.006349053233861923
iteration 168, loss = 0.009212984703481197
iteration 169, loss = 0.0062416628934443
iteration 170, loss = 0.006299990229308605
iteration 171, loss = 0.006312374956905842
iteration 172, loss = 0.006918266881257296
iteration 173, loss = 0.005764511879533529
iteration 174, loss = 0.005981506314128637
iteration 175, loss = 0.006124107167124748
iteration 176, loss = 0.005681350361555815
iteration 177, loss = 0.005830578971654177
iteration 178, loss = 0.005879512056708336
iteration 179, loss = 0.006037846207618713
iteration 180, loss = 0.006673428229987621
iteration 181, loss = 0.0061478326097130775
iteration 182, loss = 0.005928164813667536
iteration 183, loss = 0.0061011542566120625
iteration 184, loss = 0.005952510517090559
iteration 185, loss = 0.006404312327504158
iteration 186, loss = 0.005879462696611881
iteration 187, loss = 0.006546881981194019
iteration 188, loss = 0.005932163912802935
iteration 189, loss = 0.005914011970162392
iteration 190, loss = 0.00708607817068696
iteration 191, loss = 0.009023153223097324
iteration 192, loss = 0.007513241842389107
iteration 193, loss = 0.005977402441203594
iteration 194, loss = 0.005721030756831169
iteration 195, loss = 0.005809819791465998
iteration 196, loss = 0.005810668226331472
iteration 197, loss = 0.0091491574421525
iteration 198, loss = 0.0072268275544047356
iteration 199, loss = 0.0067927953787148
iteration 200, loss = 0.005779222585260868
iteration 201, loss = 0.006082284729927778
iteration 202, loss = 0.007302520796656609
iteration 203, loss = 0.0062753548845648766
iteration 204, loss = 0.00969563890248537
iteration 205, loss = 0.007509012706577778
iteration 206, loss = 0.005710822530090809
iteration 207, loss = 0.0064183990471065044
iteration 208, loss = 0.005525680258870125
iteration 209, loss = 0.0059605189599096775
iteration 210, loss = 0.007102000992745161
iteration 211, loss = 0.006232732906937599
iteration 212, loss = 0.006568233948200941
iteration 213, loss = 0.007033764384686947
iteration 214, loss = 0.005985163152217865
iteration 215, loss = 0.006564541719853878
iteration 216, loss = 0.006006276700645685
iteration 217, loss = 0.006925680674612522
iteration 218, loss = 0.006459602154791355
iteration 219, loss = 0.006311649456620216
iteration 220, loss = 0.006139533128589392
iteration 221, loss = 0.00578022887930274
iteration 222, loss = 0.00624590739607811
iteration 223, loss = 0.005808279849588871
iteration 224, loss = 0.007884431630373001
iteration 225, loss = 0.007541976869106293
iteration 226, loss = 0.006396510638296604
iteration 227, loss = 0.006186770740896463
iteration 228, loss = 0.006143931765109301
iteration 229, loss = 0.0059201824478805065
iteration 230, loss = 0.008333196863532066
iteration 231, loss = 0.005600879900157452
iteration 232, loss = 0.009482771158218384
iteration 233, loss = 0.007097799330949783
iteration 234, loss = 0.005853478796780109
iteration 235, loss = 0.006255687214434147
iteration 236, loss = 0.005914696026593447
iteration 237, loss = 0.00598888099193573
iteration 238, loss = 0.010357783176004887
iteration 239, loss = 0.007314698304980993
iteration 240, loss = 0.00615634024143219
iteration 241, loss = 0.006542843766510487
iteration 242, loss = 0.0066583966836333275
iteration 243, loss = 0.008890197612345219
iteration 244, loss = 0.007849490270018578
iteration 245, loss = 0.005749259144067764
iteration 246, loss = 0.006320079322904348
iteration 247, loss = 0.007590779569000006
iteration 248, loss = 0.00594551395624876
iteration 249, loss = 0.005983936600387096
iteration 250, loss = 0.0066065131686627865
iteration 251, loss = 0.006302117835730314
iteration 252, loss = 0.006020987406373024
iteration 253, loss = 0.006436881143599749
iteration 254, loss = 0.005895103793591261
iteration 255, loss = 0.006618807557970285
iteration 256, loss = 0.006393989082425833
iteration 257, loss = 0.00630416302010417
iteration 258, loss = 0.0066976663656532764
iteration 259, loss = 0.005532719194889069
iteration 260, loss = 0.006309820804744959
iteration 261, loss = 0.007313581183552742
iteration 262, loss = 0.006851444952189922
iteration 263, loss = 0.006891139317303896
iteration 264, loss = 0.008927641436457634
iteration 265, loss = 0.008731462992727757
iteration 266, loss = 0.006581516470760107
iteration 267, loss = 0.008629522286355495
iteration 268, loss = 0.00849096104502678
iteration 269, loss = 0.009372523985803127
iteration 270, loss = 0.009110569953918457
iteration 271, loss = 0.006443395279347897
iteration 272, loss = 0.005936508998274803
iteration 273, loss = 0.005623609758913517
iteration 274, loss = 0.006136451847851276
iteration 275, loss = 0.006224615965038538
iteration 276, loss = 0.00801798701286316
iteration 277, loss = 0.005931847263127565
iteration 278, loss = 0.005783336237072945
iteration 279, loss = 0.006654568016529083
iteration 280, loss = 0.00595379713922739
iteration 281, loss = 0.007008182816207409
iteration 282, loss = 0.009152336046099663
iteration 283, loss = 0.005910179577767849
iteration 284, loss = 0.00725917425006628
iteration 285, loss = 0.007481677457690239
iteration 286, loss = 0.006385382264852524
iteration 287, loss = 0.0058588553220033646
iteration 288, loss = 0.005957098212093115
iteration 289, loss = 0.00672067329287529
iteration 290, loss = 0.006115890108048916
iteration 291, loss = 0.006738001015037298
iteration 292, loss = 0.005685748532414436
iteration 293, loss = 0.006466246210038662
iteration 294, loss = 0.006179376505315304
iteration 295, loss = 0.008516843430697918
iteration 296, loss = 0.005803496111184359
iteration 297, loss = 0.005890844389796257
iteration 298, loss = 0.005725146271288395
iteration 299, loss = 0.007244765292853117
iteration 300, loss = 0.005664591211825609
iteration 1, loss = 0.006260909140110016
iteration 2, loss = 0.006026723887771368
iteration 3, loss = 0.009090617299079895
iteration 4, loss = 0.006918605417013168
iteration 5, loss = 0.010015723295509815
iteration 6, loss = 0.0073477402329444885
iteration 7, loss = 0.005623735953122377
iteration 8, loss = 0.006347238086163998
iteration 9, loss = 0.007099361624568701
iteration 10, loss = 0.005849464796483517
iteration 11, loss = 0.006317301653325558
iteration 12, loss = 0.0067238593474030495
iteration 13, loss = 0.005734770558774471
iteration 14, loss = 0.006892158649861813
iteration 15, loss = 0.005606643855571747
iteration 16, loss = 0.006577969528734684
iteration 17, loss = 0.008172646164894104
iteration 18, loss = 0.005584864877164364
iteration 19, loss = 0.006068340502679348
iteration 20, loss = 0.006647017784416676
iteration 21, loss = 0.006119886878877878
iteration 22, loss = 0.006706456188112497
iteration 23, loss = 0.005831396207213402
iteration 24, loss = 0.006934926845133305
iteration 25, loss = 0.00566320912912488
iteration 26, loss = 0.00594414072111249
iteration 27, loss = 0.0057664550840854645
iteration 28, loss = 0.007205573376268148
iteration 29, loss = 0.0071416799910366535
iteration 30, loss = 0.008037078194320202
iteration 31, loss = 0.00575852720066905
iteration 32, loss = 0.005746463313698769
iteration 33, loss = 0.006093883886933327
iteration 34, loss = 0.006587713956832886
iteration 35, loss = 0.00561344251036644
iteration 36, loss = 0.0067478083074092865
iteration 37, loss = 0.005593196488916874
iteration 38, loss = 0.006242747418582439
iteration 39, loss = 0.006139053031802177
iteration 40, loss = 0.006350301206111908
iteration 41, loss = 0.006010362878441811
iteration 42, loss = 0.006372053176164627
iteration 43, loss = 0.0064715426415205
iteration 44, loss = 0.005991814192384481
iteration 45, loss = 0.0060471463948488235
iteration 46, loss = 0.008341004140675068
iteration 47, loss = 0.005855175666511059
iteration 48, loss = 0.006587350741028786
iteration 49, loss = 0.005669823847711086
iteration 50, loss = 0.006193594541400671
iteration 51, loss = 0.006337289232760668
iteration 52, loss = 0.005989500321447849
iteration 53, loss = 0.006068946328014135
iteration 54, loss = 0.008235876448452473
iteration 55, loss = 0.00586558086797595
iteration 56, loss = 0.0060084047727286816
iteration 57, loss = 0.006949988659471273
iteration 58, loss = 0.006903007626533508
iteration 59, loss = 0.006663299631327391
iteration 60, loss = 0.0068340180441737175
iteration 61, loss = 0.007459515240043402
iteration 62, loss = 0.005820487160235643
iteration 63, loss = 0.005745868198573589
iteration 64, loss = 0.006047563161700964
iteration 65, loss = 0.005836544092744589
iteration 66, loss = 0.0060061379335820675
iteration 67, loss = 0.00573355657979846
iteration 68, loss = 0.005932054482400417
iteration 69, loss = 0.005715202074497938
iteration 70, loss = 0.007577273063361645
iteration 71, loss = 0.007113233674317598
iteration 72, loss = 0.006200824864208698
iteration 73, loss = 0.005908382590860128
iteration 74, loss = 0.00526167917996645
iteration 75, loss = 0.00728332344442606
iteration 76, loss = 0.009458458982408047
iteration 77, loss = 0.00812592078000307
iteration 78, loss = 0.0064831990748643875
iteration 79, loss = 0.00796680897474289
iteration 80, loss = 0.005923795513808727
iteration 81, loss = 0.006319173611700535
iteration 82, loss = 0.006075212266296148
iteration 83, loss = 0.006921767722815275
iteration 84, loss = 0.006376364734023809
iteration 85, loss = 0.005962076131254435
iteration 86, loss = 0.006055938545614481
iteration 87, loss = 0.006766664329916239
iteration 88, loss = 0.005875678267329931
iteration 89, loss = 0.005778496153652668
iteration 90, loss = 0.005829356145113707
iteration 91, loss = 0.005627911072224379
iteration 92, loss = 0.009389743208885193
iteration 93, loss = 0.006299936678260565
iteration 94, loss = 0.005912468768656254
iteration 95, loss = 0.006316944025456905
iteration 96, loss = 0.00579175865277648
iteration 97, loss = 0.0054370807483792305
iteration 98, loss = 0.00738135538995266
iteration 99, loss = 0.007373068016022444
iteration 100, loss = 0.009172143414616585
iteration 101, loss = 0.006443369667977095
iteration 102, loss = 0.006943785585463047
iteration 103, loss = 0.007612926419824362
iteration 104, loss = 0.007266228552907705
iteration 105, loss = 0.006312252953648567
iteration 106, loss = 0.005642532370984554
iteration 107, loss = 0.006439702119678259
iteration 108, loss = 0.007207352202385664
iteration 109, loss = 0.005642284173518419
iteration 110, loss = 0.0061149634420871735
iteration 111, loss = 0.005697134416550398
iteration 112, loss = 0.005928068421781063
iteration 113, loss = 0.006855018902570009
iteration 114, loss = 0.007396195083856583
iteration 115, loss = 0.006115306168794632
iteration 116, loss = 0.007106654345989227
iteration 117, loss = 0.0062293121591210365
iteration 118, loss = 0.005790971219539642
iteration 119, loss = 0.006136246491223574
iteration 120, loss = 0.006160819437354803
iteration 121, loss = 0.00581121351569891
iteration 122, loss = 0.006737909279763699
iteration 123, loss = 0.006546210963279009
iteration 124, loss = 0.006002235226333141
iteration 125, loss = 0.009170567616820335
iteration 126, loss = 0.0058553582057356834
iteration 127, loss = 0.009511125274002552
iteration 128, loss = 0.006017986219376326
iteration 129, loss = 0.0067443507723510265
iteration 130, loss = 0.005773617420345545
iteration 131, loss = 0.008875396102666855
iteration 132, loss = 0.006067505571991205
iteration 133, loss = 0.00907671358436346
iteration 134, loss = 0.005934047978371382
iteration 135, loss = 0.007639829535037279
iteration 136, loss = 0.005857171956449747
iteration 137, loss = 0.006225520744919777
iteration 138, loss = 0.006174618378281593
iteration 139, loss = 0.005913585890084505
iteration 140, loss = 0.00596910435706377
iteration 141, loss = 0.008904344402253628
iteration 142, loss = 0.006566599477082491
iteration 143, loss = 0.007268276996910572
iteration 144, loss = 0.005778121761977673
iteration 145, loss = 0.005792384035885334
iteration 146, loss = 0.007259716279804707
iteration 147, loss = 0.007309230510145426
iteration 148, loss = 0.006074641831219196
iteration 149, loss = 0.005478523205965757
iteration 150, loss = 0.005848489701747894
iteration 151, loss = 0.00619153119623661
iteration 152, loss = 0.0060612792149186134
iteration 153, loss = 0.006290258374065161
iteration 154, loss = 0.005669014528393745
iteration 155, loss = 0.006628872826695442
iteration 156, loss = 0.009303739294409752
iteration 157, loss = 0.006387730129063129
iteration 158, loss = 0.005686516873538494
iteration 159, loss = 0.005911547690629959
iteration 160, loss = 0.007974483072757721
iteration 161, loss = 0.005868994165211916
iteration 162, loss = 0.007414520718157291
iteration 163, loss = 0.006012727040797472
iteration 164, loss = 0.008111054077744484
iteration 165, loss = 0.006088691763579845
iteration 166, loss = 0.005561964586377144
iteration 167, loss = 0.006780675612390041
iteration 168, loss = 0.005615433678030968
iteration 169, loss = 0.007401857990771532
iteration 170, loss = 0.007316050119698048
iteration 171, loss = 0.005644963122904301
iteration 172, loss = 0.005985863972455263
iteration 173, loss = 0.00558806024491787
iteration 174, loss = 0.006033408921211958
iteration 175, loss = 0.006725988816469908
iteration 176, loss = 0.005721393506973982
iteration 177, loss = 0.00565728172659874
iteration 178, loss = 0.0057591646909713745
iteration 179, loss = 0.005751058459281921
iteration 180, loss = 0.0057211932726204395
iteration 181, loss = 0.006453980691730976
iteration 182, loss = 0.005835165269672871
iteration 183, loss = 0.007414313033223152
iteration 184, loss = 0.007836230099201202
iteration 185, loss = 0.007404700852930546
iteration 186, loss = 0.007995348423719406
iteration 187, loss = 0.006623175926506519
iteration 188, loss = 0.006674901116639376
iteration 189, loss = 0.006199609488248825
iteration 190, loss = 0.006672519259154797
iteration 191, loss = 0.009190390817821026
iteration 192, loss = 0.005637454334646463
iteration 193, loss = 0.009560919366776943
iteration 194, loss = 0.006060585845261812
iteration 195, loss = 0.007131334859877825
iteration 196, loss = 0.005947538651525974
iteration 197, loss = 0.00702816154807806
iteration 198, loss = 0.007113414350897074
iteration 199, loss = 0.007897674106061459
iteration 200, loss = 0.005985827650874853
iteration 201, loss = 0.005599661264568567
iteration 202, loss = 0.007024464663118124
iteration 203, loss = 0.005697558168321848
iteration 204, loss = 0.00569606339558959
iteration 205, loss = 0.007128969766199589
iteration 206, loss = 0.007020360790193081
iteration 207, loss = 0.0064711603336036205
iteration 208, loss = 0.006475157104432583
iteration 209, loss = 0.007066860795021057
iteration 210, loss = 0.007585827261209488
iteration 211, loss = 0.006444001104682684
iteration 212, loss = 0.005925261415541172
iteration 213, loss = 0.005838313139975071
iteration 214, loss = 0.006204450502991676
iteration 215, loss = 0.009541348554193974
iteration 216, loss = 0.007175292819738388
iteration 217, loss = 0.006350300740450621
iteration 218, loss = 0.006658494938164949
iteration 219, loss = 0.005774395074695349
iteration 220, loss = 0.0056905969977378845
iteration 221, loss = 0.007583065424114466
iteration 222, loss = 0.005993629805743694
iteration 223, loss = 0.007607202045619488
iteration 224, loss = 0.006149026099592447
iteration 225, loss = 0.006149632856249809
iteration 226, loss = 0.006536727771162987
iteration 227, loss = 0.005842258688062429
iteration 228, loss = 0.005952135659754276
iteration 229, loss = 0.005908983759582043
iteration 230, loss = 0.006083397660404444
iteration 231, loss = 0.005676340311765671
iteration 232, loss = 0.005875107832252979
iteration 233, loss = 0.006966307759284973
iteration 234, loss = 0.007561761885881424
iteration 235, loss = 0.006409316323697567
iteration 236, loss = 0.006274034269154072
iteration 237, loss = 0.006207026541233063
iteration 238, loss = 0.00593072734773159
iteration 239, loss = 0.005840174853801727
iteration 240, loss = 0.005667733959853649
iteration 241, loss = 0.00714979087933898
iteration 242, loss = 0.006049201823771
iteration 243, loss = 0.005918922368437052
iteration 244, loss = 0.006322881206870079
iteration 245, loss = 0.006557267624884844
iteration 246, loss = 0.005770009011030197
iteration 247, loss = 0.008817827329039574
iteration 248, loss = 0.005941139534115791
iteration 249, loss = 0.005929143168032169
iteration 250, loss = 0.006081008352339268
iteration 251, loss = 0.00621599517762661
iteration 252, loss = 0.005585853476077318
iteration 253, loss = 0.006216217763721943
iteration 254, loss = 0.00605475390329957
iteration 255, loss = 0.006303609814494848
iteration 256, loss = 0.006100337952375412
iteration 257, loss = 0.006684538908302784
iteration 258, loss = 0.006031835451722145
iteration 259, loss = 0.009052547626197338
iteration 260, loss = 0.006081058643758297
iteration 261, loss = 0.006270250771194696
iteration 262, loss = 0.00533199543133378
iteration 263, loss = 0.0063499873504042625
iteration 264, loss = 0.01100554596632719
iteration 265, loss = 0.005594620015472174
iteration 266, loss = 0.006254082545638084
iteration 267, loss = 0.007848639041185379
iteration 268, loss = 0.006316338200122118
iteration 269, loss = 0.008239569142460823
iteration 270, loss = 0.00585084967315197
iteration 271, loss = 0.005982727278023958
iteration 272, loss = 0.006705910433083773
iteration 273, loss = 0.006142434664070606
iteration 274, loss = 0.005767046939581633
iteration 275, loss = 0.005651920568197966
iteration 276, loss = 0.0062269181944429874
iteration 277, loss = 0.006775585934519768
iteration 278, loss = 0.006032494828104973
iteration 279, loss = 0.006068887654691935
iteration 280, loss = 0.005663932766765356
iteration 281, loss = 0.006108490750193596
iteration 282, loss = 0.007592185866087675
iteration 283, loss = 0.008860110305249691
iteration 284, loss = 0.005970568396151066
iteration 285, loss = 0.006202911492437124
iteration 286, loss = 0.007847418077290058
iteration 287, loss = 0.008196503855288029
iteration 288, loss = 0.005781969055533409
iteration 289, loss = 0.005980222020298243
iteration 290, loss = 0.006460116244852543
iteration 291, loss = 0.006020800210535526
iteration 292, loss = 0.008054271340370178
iteration 293, loss = 0.005733721423894167
iteration 294, loss = 0.006157341413199902
iteration 295, loss = 0.005766028538346291
iteration 296, loss = 0.005304845981299877
iteration 297, loss = 0.005939141847193241
iteration 298, loss = 0.009082125499844551
iteration 299, loss = 0.006659082602709532
iteration 300, loss = 0.005725464783608913
iteration 1, loss = 0.007927910424768925
iteration 2, loss = 0.005915542598813772
iteration 3, loss = 0.005992562510073185
iteration 4, loss = 0.0057556480169296265
iteration 5, loss = 0.0053909472189843655
iteration 6, loss = 0.006365972105413675
iteration 7, loss = 0.0053641279228031635
iteration 8, loss = 0.007694723550230265
iteration 9, loss = 0.009079575538635254
iteration 10, loss = 0.006969067268073559
iteration 11, loss = 0.005945217330008745
iteration 12, loss = 0.00850450899451971
iteration 13, loss = 0.006047642789781094
iteration 14, loss = 0.009087101556360722
iteration 15, loss = 0.007726375944912434
iteration 16, loss = 0.00598525395616889
iteration 17, loss = 0.00843269657343626
iteration 18, loss = 0.006173461675643921
iteration 19, loss = 0.00601752195507288
iteration 20, loss = 0.007751980796456337
iteration 21, loss = 0.006569278426468372
iteration 22, loss = 0.006243117619305849
iteration 23, loss = 0.006244009826332331
iteration 24, loss = 0.006042421795427799
iteration 25, loss = 0.006148846819996834
iteration 26, loss = 0.005908890627324581
iteration 27, loss = 0.005957376211881638
iteration 28, loss = 0.005929220467805862
iteration 29, loss = 0.005390842445194721
iteration 30, loss = 0.006165645085275173
iteration 31, loss = 0.006893059238791466
iteration 32, loss = 0.0057486314326524734
iteration 33, loss = 0.006355558522045612
iteration 34, loss = 0.005862502381205559
iteration 35, loss = 0.0057493457570672035
iteration 36, loss = 0.006177552510052919
iteration 37, loss = 0.005786275491118431
iteration 38, loss = 0.006902828812599182
iteration 39, loss = 0.005709969904273748
iteration 40, loss = 0.006062433589249849
iteration 41, loss = 0.006131364032626152
iteration 42, loss = 0.005676621571183205
iteration 43, loss = 0.00776502164080739
iteration 44, loss = 0.005586129147559404
iteration 45, loss = 0.00552405184134841
iteration 46, loss = 0.0061708795838057995
iteration 47, loss = 0.00553938327357173
iteration 48, loss = 0.0072508445009589195
iteration 49, loss = 0.006040812004357576
iteration 50, loss = 0.0064892396330833435
iteration 51, loss = 0.00537208653986454
iteration 52, loss = 0.00595693988725543
iteration 53, loss = 0.005703783594071865
iteration 54, loss = 0.005915146321058273
iteration 55, loss = 0.005868383217602968
iteration 56, loss = 0.005625096615403891
iteration 57, loss = 0.005897670518606901
iteration 58, loss = 0.007120393682271242
iteration 59, loss = 0.005698270630091429
iteration 60, loss = 0.00607386976480484
iteration 61, loss = 0.006189221516251564
iteration 62, loss = 0.0062511274591088295
iteration 63, loss = 0.005587136838585138
iteration 64, loss = 0.0055443295277655125
iteration 65, loss = 0.005513729527592659
iteration 66, loss = 0.0074273101054131985
iteration 67, loss = 0.006574360188096762
iteration 68, loss = 0.007292528171092272
iteration 69, loss = 0.006509238854050636
iteration 70, loss = 0.005954463966190815
iteration 71, loss = 0.007352570537477732
iteration 72, loss = 0.006047940347343683
iteration 73, loss = 0.005503919441252947
iteration 74, loss = 0.009867233224213123
iteration 75, loss = 0.005794123746454716
iteration 76, loss = 0.005770331714302301
iteration 77, loss = 0.005947968922555447
iteration 78, loss = 0.006606020964682102
iteration 79, loss = 0.00639447383582592
iteration 80, loss = 0.0054913479834795
iteration 81, loss = 0.008597797714173794
iteration 82, loss = 0.005889509338885546
iteration 83, loss = 0.005938946269452572
iteration 84, loss = 0.005494814366102219
iteration 85, loss = 0.007622205652296543
iteration 86, loss = 0.005656909197568893
iteration 87, loss = 0.007072654087096453
iteration 88, loss = 0.007311074994504452
iteration 89, loss = 0.00851496309041977
iteration 90, loss = 0.00750028807669878
iteration 91, loss = 0.006216337904334068
iteration 92, loss = 0.006840196438133717
iteration 93, loss = 0.006069553084671497
iteration 94, loss = 0.005831063259392977
iteration 95, loss = 0.0057918839156627655
iteration 96, loss = 0.005713116377592087
iteration 97, loss = 0.007284454070031643
iteration 98, loss = 0.005854716990143061
iteration 99, loss = 0.005576201714575291
iteration 100, loss = 0.005488785449415445
iteration 101, loss = 0.006601989269256592
iteration 102, loss = 0.007386934012174606
iteration 103, loss = 0.007032581605017185
iteration 104, loss = 0.0070007494650781155
iteration 105, loss = 0.0060818735510110855
iteration 106, loss = 0.005727152805775404
iteration 107, loss = 0.005666519980877638
iteration 108, loss = 0.005706684663891792
iteration 109, loss = 0.0056605557911098
iteration 110, loss = 0.006164319813251495
iteration 111, loss = 0.006728157866746187
iteration 112, loss = 0.007300934754312038
iteration 113, loss = 0.006143084727227688
iteration 114, loss = 0.007771509699523449
iteration 115, loss = 0.00748294685035944
iteration 116, loss = 0.007019053213298321
iteration 117, loss = 0.007189188152551651
iteration 118, loss = 0.005829223897308111
iteration 119, loss = 0.00642471993342042
iteration 120, loss = 0.005751812364906073
iteration 121, loss = 0.005833163391798735
iteration 122, loss = 0.00546368770301342
iteration 123, loss = 0.005968768615275621
iteration 124, loss = 0.005512224044650793
iteration 125, loss = 0.006886548828333616
iteration 126, loss = 0.007397785782814026
iteration 127, loss = 0.005716673098504543
iteration 128, loss = 0.0077795167453587055
iteration 129, loss = 0.00787225179374218
iteration 130, loss = 0.005878334399312735
iteration 131, loss = 0.005584603175520897
iteration 132, loss = 0.007025040220469236
iteration 133, loss = 0.006735613103955984
iteration 134, loss = 0.0062043387442827225
iteration 135, loss = 0.005918576382100582
iteration 136, loss = 0.00583962956443429
iteration 137, loss = 0.005854164715856314
iteration 138, loss = 0.006042445544153452
iteration 139, loss = 0.00613721227273345
iteration 140, loss = 0.00593052851036191
iteration 141, loss = 0.005937075242400169
iteration 142, loss = 0.005492459982633591
iteration 143, loss = 0.005844146944582462
iteration 144, loss = 0.006103067193180323
iteration 145, loss = 0.006227463483810425
iteration 146, loss = 0.005369181744754314
iteration 147, loss = 0.00799216516315937
iteration 148, loss = 0.006638515740633011
iteration 149, loss = 0.006123400293290615
iteration 150, loss = 0.005965596996247768
iteration 151, loss = 0.005903779529035091
iteration 152, loss = 0.006383858621120453
iteration 153, loss = 0.005206340458244085
iteration 154, loss = 0.006268216297030449
iteration 155, loss = 0.005583925172686577
iteration 156, loss = 0.0054092626087367535
iteration 157, loss = 0.007239870727062225
iteration 158, loss = 0.007236827630549669
iteration 159, loss = 0.005691858008503914
iteration 160, loss = 0.00542606832459569
iteration 161, loss = 0.005750415846705437
iteration 162, loss = 0.005955017637461424
iteration 163, loss = 0.006078365258872509
iteration 164, loss = 0.006049410440027714
iteration 165, loss = 0.005800196900963783
iteration 166, loss = 0.00650243042036891
iteration 167, loss = 0.006839914247393608
iteration 168, loss = 0.005618100520223379
iteration 169, loss = 0.005736703984439373
iteration 170, loss = 0.005698910914361477
iteration 171, loss = 0.008659512735903263
iteration 172, loss = 0.011901337653398514
iteration 173, loss = 0.005785306449979544
iteration 174, loss = 0.006359615828841925
iteration 175, loss = 0.005316793452948332
iteration 176, loss = 0.005629478953778744
iteration 177, loss = 0.0056340075097978115
iteration 178, loss = 0.005942023359239101
iteration 179, loss = 0.006245374213904142
iteration 180, loss = 0.006833365652710199
iteration 181, loss = 0.005963247735053301
iteration 182, loss = 0.005630593281239271
iteration 183, loss = 0.005921258591115475
iteration 184, loss = 0.0064142486080527306
iteration 185, loss = 0.00777029013261199
iteration 186, loss = 0.006372176576405764
iteration 187, loss = 0.005656126420944929
iteration 188, loss = 0.005209437571465969
iteration 189, loss = 0.005764626432210207
iteration 190, loss = 0.006149563938379288
iteration 191, loss = 0.005617894232273102
iteration 192, loss = 0.005974723957479
iteration 193, loss = 0.006930410396307707
iteration 194, loss = 0.005606890190392733
iteration 195, loss = 0.006258083041757345
iteration 196, loss = 0.005810850765556097
iteration 197, loss = 0.0064782979898154736
iteration 198, loss = 0.00632259389385581
iteration 199, loss = 0.005291125737130642
iteration 200, loss = 0.005812409799546003
iteration 201, loss = 0.005656576249748468
iteration 202, loss = 0.00797218270599842
iteration 203, loss = 0.005859005730599165
iteration 204, loss = 0.0054686968214809895
iteration 205, loss = 0.00648161955177784
iteration 206, loss = 0.007621003780514002
iteration 207, loss = 0.009101992473006248
iteration 208, loss = 0.005509067792445421
iteration 209, loss = 0.0061716604977846146
iteration 210, loss = 0.00981223676353693
iteration 211, loss = 0.005923327524214983
iteration 212, loss = 0.01201770268380642
iteration 213, loss = 0.0067737530916929245
iteration 214, loss = 0.007550143636763096
iteration 215, loss = 0.007249761372804642
iteration 216, loss = 0.0060055009089410305
iteration 217, loss = 0.005661460570991039
iteration 218, loss = 0.0066460249945521355
iteration 219, loss = 0.006885059177875519
iteration 220, loss = 0.005803036969155073
iteration 221, loss = 0.006002649664878845
iteration 222, loss = 0.010184869170188904
iteration 223, loss = 0.005421385169029236
iteration 224, loss = 0.005799668841063976
iteration 225, loss = 0.005871141795068979
iteration 226, loss = 0.006714353803545237
iteration 227, loss = 0.005690034478902817
iteration 228, loss = 0.006789815612137318
iteration 229, loss = 0.006195207126438618
iteration 230, loss = 0.00583664420992136
iteration 231, loss = 0.005574771203100681
iteration 232, loss = 0.0067776646465063095
iteration 233, loss = 0.006076735910028219
iteration 234, loss = 0.005317875184118748
iteration 235, loss = 0.005388376768678427
iteration 236, loss = 0.008609659038484097
iteration 237, loss = 0.0059267086908221245
iteration 238, loss = 0.007876514457166195
iteration 239, loss = 0.006094725336879492
iteration 240, loss = 0.006621888838708401
iteration 241, loss = 0.006195541471242905
iteration 242, loss = 0.007016442716121674
iteration 243, loss = 0.009865562431514263
iteration 244, loss = 0.007629592902958393
iteration 245, loss = 0.00744494516402483
iteration 246, loss = 0.005493836477398872
iteration 247, loss = 0.007030286826193333
iteration 248, loss = 0.008114620111882687
iteration 249, loss = 0.006045590154826641
iteration 250, loss = 0.0061058043502271175
iteration 251, loss = 0.005843926686793566
iteration 252, loss = 0.005831764545291662
iteration 253, loss = 0.0060697756707668304
iteration 254, loss = 0.005904530640691519
iteration 255, loss = 0.007409713231027126
iteration 256, loss = 0.005853918381035328
iteration 257, loss = 0.006139326840639114
iteration 258, loss = 0.007215662393718958
iteration 259, loss = 0.005460607353597879
iteration 260, loss = 0.0060141971334815025
iteration 261, loss = 0.006206020247191191
iteration 262, loss = 0.008869048207998276
iteration 263, loss = 0.006315309088677168
iteration 264, loss = 0.008381394669413567
iteration 265, loss = 0.006217981688678265
iteration 266, loss = 0.00550914416089654
iteration 267, loss = 0.007301250472664833
iteration 268, loss = 0.006927270907908678
iteration 269, loss = 0.005596742499619722
iteration 270, loss = 0.006226808298379183
iteration 271, loss = 0.006611234974116087
iteration 272, loss = 0.006515214219689369
iteration 273, loss = 0.010283459909260273
iteration 274, loss = 0.00572513323277235
iteration 275, loss = 0.005362361669540405
iteration 276, loss = 0.005873420275747776
iteration 277, loss = 0.0070115611888468266
iteration 278, loss = 0.005559408571571112
iteration 279, loss = 0.006597975734621286
iteration 280, loss = 0.005882264114916325
iteration 281, loss = 0.005731538869440556
iteration 282, loss = 0.005345373880118132
iteration 283, loss = 0.006886756978929043
iteration 284, loss = 0.005943328142166138
iteration 285, loss = 0.006400046870112419
iteration 286, loss = 0.005642536096274853
iteration 287, loss = 0.006383190862834454
iteration 288, loss = 0.006676757242530584
iteration 289, loss = 0.005579742137342691
iteration 290, loss = 0.005827270448207855
iteration 291, loss = 0.0056772734969854355
iteration 292, loss = 0.005756461061537266
iteration 293, loss = 0.006555765401571989
iteration 294, loss = 0.006193521432578564
iteration 295, loss = 0.006803490221500397
iteration 296, loss = 0.00594621617347002
iteration 297, loss = 0.006581299006938934
iteration 298, loss = 0.00598634360358119
iteration 299, loss = 0.007628209423273802
iteration 300, loss = 0.009358547627925873
iteration 1, loss = 0.005913402885198593
iteration 2, loss = 0.0058121331967413425
iteration 3, loss = 0.005541771184653044
iteration 4, loss = 0.008668254129588604
iteration 5, loss = 0.005887252278625965
iteration 6, loss = 0.006624030880630016
iteration 7, loss = 0.008186738938093185
iteration 8, loss = 0.005331052001565695
iteration 9, loss = 0.006210098043084145
iteration 10, loss = 0.007455769926309586
iteration 11, loss = 0.005445647984743118
iteration 12, loss = 0.005460366606712341
iteration 13, loss = 0.005944077391177416
iteration 14, loss = 0.005639221053570509
iteration 15, loss = 0.005619185045361519
iteration 16, loss = 0.0075289602391421795
iteration 17, loss = 0.0065476116724312305
iteration 18, loss = 0.008110404014587402
iteration 19, loss = 0.006084547843784094
iteration 20, loss = 0.005807053297758102
iteration 21, loss = 0.0054129259660840034
iteration 22, loss = 0.007148070260882378
iteration 23, loss = 0.005867023952305317
iteration 24, loss = 0.00568216061219573
iteration 25, loss = 0.007360594347119331
iteration 26, loss = 0.005956106819212437
iteration 27, loss = 0.005305733997374773
iteration 28, loss = 0.006379303988069296
iteration 29, loss = 0.005541031248867512
iteration 30, loss = 0.005976716056466103
iteration 31, loss = 0.00549008883535862
iteration 32, loss = 0.0061900499276816845
iteration 33, loss = 0.006034501828253269
iteration 34, loss = 0.005874834023416042
iteration 35, loss = 0.00589958019554615
iteration 36, loss = 0.005874086637049913
iteration 37, loss = 0.005814909469336271
iteration 38, loss = 0.006188832223415375
iteration 39, loss = 0.005765451118350029
iteration 40, loss = 0.005778709892183542
iteration 41, loss = 0.006318401079624891
iteration 42, loss = 0.005810190457850695
iteration 43, loss = 0.005434805061668158
iteration 44, loss = 0.005672964733093977
iteration 45, loss = 0.005510343238711357
iteration 46, loss = 0.00582024734467268
iteration 47, loss = 0.0057304175570607185
iteration 48, loss = 0.006614651530981064
iteration 49, loss = 0.005717041902244091
iteration 50, loss = 0.009002367034554482
iteration 51, loss = 0.00554225267842412
iteration 52, loss = 0.007005768362432718
iteration 53, loss = 0.006092681549489498
iteration 54, loss = 0.010123904794454575
iteration 55, loss = 0.007336132228374481
iteration 56, loss = 0.005832256283611059
iteration 57, loss = 0.008050529286265373
iteration 58, loss = 0.006943471264094114
iteration 59, loss = 0.010403480380773544
iteration 60, loss = 0.006332527380436659
iteration 61, loss = 0.007191527169197798
iteration 62, loss = 0.006245219148695469
iteration 63, loss = 0.007781689520925283
iteration 64, loss = 0.005865762475878
iteration 65, loss = 0.005678479094058275
iteration 66, loss = 0.006143281236290932
iteration 67, loss = 0.006074694916605949
iteration 68, loss = 0.005543210078030825
iteration 69, loss = 0.008053673431277275
iteration 70, loss = 0.006101839244365692
iteration 71, loss = 0.008714544586837292
iteration 72, loss = 0.008843707852065563
iteration 73, loss = 0.008701478131115437
iteration 74, loss = 0.00575733557343483
iteration 75, loss = 0.006707143969833851
iteration 76, loss = 0.00886914599686861
iteration 77, loss = 0.006181445438414812
iteration 78, loss = 0.0073034088127315044
iteration 79, loss = 0.005827953107655048
iteration 80, loss = 0.0057945093140006065
iteration 81, loss = 0.005827176384627819
iteration 82, loss = 0.006037895567715168
iteration 83, loss = 0.007138057146221399
iteration 84, loss = 0.011713365092873573
iteration 85, loss = 0.007153067737817764
iteration 86, loss = 0.007079959847033024
iteration 87, loss = 0.0052515240386128426
iteration 88, loss = 0.005735303740948439
iteration 89, loss = 0.0057798996567726135
iteration 90, loss = 0.0060471706092357635
iteration 91, loss = 0.006461416836827993
iteration 92, loss = 0.0056763761676847935
iteration 93, loss = 0.005725850351154804
iteration 94, loss = 0.006726135034114122
iteration 95, loss = 0.006602193228900433
iteration 96, loss = 0.005349016282707453
iteration 97, loss = 0.006671318784356117
iteration 98, loss = 0.005887169390916824
iteration 99, loss = 0.006593394558876753
iteration 100, loss = 0.006790672428905964
iteration 101, loss = 0.0057327826507389545
iteration 102, loss = 0.0055750347673892975
iteration 103, loss = 0.0057174391113221645
iteration 104, loss = 0.007893165573477745
iteration 105, loss = 0.005303999409079552
iteration 106, loss = 0.0058974577113986015
iteration 107, loss = 0.005396031774580479
iteration 108, loss = 0.005898036994040012
iteration 109, loss = 0.00669769337400794
iteration 110, loss = 0.005744858179241419
iteration 111, loss = 0.006510123144835234
iteration 112, loss = 0.007490289397537708
iteration 113, loss = 0.005742083303630352
iteration 114, loss = 0.005996001418679953
iteration 115, loss = 0.006895935628563166
iteration 116, loss = 0.005421681795269251
iteration 117, loss = 0.005909851752221584
iteration 118, loss = 0.0064284163527190685
iteration 119, loss = 0.005789048969745636
iteration 120, loss = 0.0069306399673223495
iteration 121, loss = 0.005490794777870178
iteration 122, loss = 0.007713970262557268
iteration 123, loss = 0.006933835335075855
iteration 124, loss = 0.005811667535454035
iteration 125, loss = 0.005784993525594473
iteration 126, loss = 0.005899346433579922
iteration 127, loss = 0.005610412918031216
iteration 128, loss = 0.005559324752539396
iteration 129, loss = 0.005725521594285965
iteration 130, loss = 0.005726048722863197
iteration 131, loss = 0.006731870584189892
iteration 132, loss = 0.00702802836894989
iteration 133, loss = 0.010783663019537926
iteration 134, loss = 0.006060783751308918
iteration 135, loss = 0.006724773906171322
iteration 136, loss = 0.005782938562333584
iteration 137, loss = 0.005862930789589882
iteration 138, loss = 0.005603550001978874
iteration 139, loss = 0.005756682250648737
iteration 140, loss = 0.005632220767438412
iteration 141, loss = 0.005572391673922539
iteration 142, loss = 0.008778948336839676
iteration 143, loss = 0.006374994292855263
iteration 144, loss = 0.005796939600259066
iteration 145, loss = 0.006454606540501118
iteration 146, loss = 0.005620678421109915
iteration 147, loss = 0.0056027621030807495
iteration 148, loss = 0.005518046673387289
iteration 149, loss = 0.0059455037117004395
iteration 150, loss = 0.0059267315082252026
iteration 151, loss = 0.00826714001595974
iteration 152, loss = 0.006228763610124588
iteration 153, loss = 0.006993787828832865
iteration 154, loss = 0.0058296420611441135
iteration 155, loss = 0.005942334420979023
iteration 156, loss = 0.005855124909430742
iteration 157, loss = 0.006663477048277855
iteration 158, loss = 0.005744630005210638
iteration 159, loss = 0.0057006715796887875
iteration 160, loss = 0.006937078200280666
iteration 161, loss = 0.005700966343283653
iteration 162, loss = 0.007639926392585039
iteration 163, loss = 0.0066075921058654785
iteration 164, loss = 0.0056472476571798325
iteration 165, loss = 0.007830534130334854
iteration 166, loss = 0.005578265991061926
iteration 167, loss = 0.007177799940109253
iteration 168, loss = 0.006006236188113689
iteration 169, loss = 0.005707708187401295
iteration 170, loss = 0.006385410204529762
iteration 171, loss = 0.0056155226193368435
iteration 172, loss = 0.005867047235369682
iteration 173, loss = 0.005552473943680525
iteration 174, loss = 0.006932602729648352
iteration 175, loss = 0.005670062266290188
iteration 176, loss = 0.005672208499163389
iteration 177, loss = 0.0076790982857346535
iteration 178, loss = 0.006758823525160551
iteration 179, loss = 0.0068665361031889915
iteration 180, loss = 0.005936386529356241
iteration 181, loss = 0.007006129249930382
iteration 182, loss = 0.00561908446252346
iteration 183, loss = 0.007056333124637604
iteration 184, loss = 0.006355040706694126
iteration 185, loss = 0.005635696928948164
iteration 186, loss = 0.005789312534034252
iteration 187, loss = 0.005620659328997135
iteration 188, loss = 0.005571904592216015
iteration 189, loss = 0.005367986857891083
iteration 190, loss = 0.009216232225298882
iteration 191, loss = 0.005713989958167076
iteration 192, loss = 0.007352663204073906
iteration 193, loss = 0.0055185346864163876
iteration 194, loss = 0.00554551649838686
iteration 195, loss = 0.0058382130227983
iteration 196, loss = 0.006028282456099987
iteration 197, loss = 0.005575662944465876
iteration 198, loss = 0.005566414911299944
iteration 199, loss = 0.005551574751734734
iteration 200, loss = 0.006030146963894367
iteration 201, loss = 0.005260012578219175
iteration 202, loss = 0.006724322214722633
iteration 203, loss = 0.005433205049484968
iteration 204, loss = 0.007066965568810701
iteration 205, loss = 0.006200599484145641
iteration 206, loss = 0.005381670780479908
iteration 207, loss = 0.005744448397308588
iteration 208, loss = 0.005695792846381664
iteration 209, loss = 0.006636063102632761
iteration 210, loss = 0.0057435184717178345
iteration 211, loss = 0.00718905171379447
iteration 212, loss = 0.005211845505982637
iteration 213, loss = 0.008664984256029129
iteration 214, loss = 0.0059587182477116585
iteration 215, loss = 0.00571163697168231
iteration 216, loss = 0.0056161461398005486
iteration 217, loss = 0.005549333989620209
iteration 218, loss = 0.00598968006670475
iteration 219, loss = 0.006487570237368345
iteration 220, loss = 0.005354693159461021
iteration 221, loss = 0.0053634545765817165
iteration 222, loss = 0.006263200659304857
iteration 223, loss = 0.005600853823125362
iteration 224, loss = 0.0063899545930325985
iteration 225, loss = 0.005944401957094669
iteration 226, loss = 0.005987423937767744
iteration 227, loss = 0.005735895596444607
iteration 228, loss = 0.008789800107479095
iteration 229, loss = 0.005644343327730894
iteration 230, loss = 0.005762767046689987
iteration 231, loss = 0.00560607248917222
iteration 232, loss = 0.008876776322722435
iteration 233, loss = 0.006910449359565973
iteration 234, loss = 0.005779199302196503
iteration 235, loss = 0.005674304440617561
iteration 236, loss = 0.0052318754605948925
iteration 237, loss = 0.005591644439846277
iteration 238, loss = 0.006069551222026348
iteration 239, loss = 0.006247130688279867
iteration 240, loss = 0.005912354215979576
iteration 241, loss = 0.005845601204782724
iteration 242, loss = 0.008363579399883747
iteration 243, loss = 0.008797011338174343
iteration 244, loss = 0.005987193435430527
iteration 245, loss = 0.005448655225336552
iteration 246, loss = 0.007937885820865631
iteration 247, loss = 0.005510183982551098
iteration 248, loss = 0.005859716329723597
iteration 249, loss = 0.007158841006457806
iteration 250, loss = 0.005558874923735857
iteration 251, loss = 0.005533325020223856
iteration 252, loss = 0.007261866703629494
iteration 253, loss = 0.006405659019947052
iteration 254, loss = 0.005645744502544403
iteration 255, loss = 0.0049650887958705425
iteration 256, loss = 0.006163709331303835
iteration 257, loss = 0.0057786256074905396
iteration 258, loss = 0.005123703274875879
iteration 259, loss = 0.006449426524341106
iteration 260, loss = 0.006061249878257513
iteration 261, loss = 0.0060713463462889194
iteration 262, loss = 0.0055905794724822044
iteration 263, loss = 0.005667715799063444
iteration 264, loss = 0.0056307753548026085
iteration 265, loss = 0.0059692454524338245
iteration 266, loss = 0.005433041136711836
iteration 267, loss = 0.008951572701334953
iteration 268, loss = 0.00583826657384634
iteration 269, loss = 0.005587465595453978
iteration 270, loss = 0.005542397499084473
iteration 271, loss = 0.005666580982506275
iteration 272, loss = 0.008664943277835846
iteration 273, loss = 0.007716617546975613
iteration 274, loss = 0.006288063246756792
iteration 275, loss = 0.005548136308789253
iteration 276, loss = 0.0056098876520991325
iteration 277, loss = 0.006951530463993549
iteration 278, loss = 0.0054792677983641624
iteration 279, loss = 0.005397606175392866
iteration 280, loss = 0.005919266492128372
iteration 281, loss = 0.006864103022962809
iteration 282, loss = 0.0059458003379404545
iteration 283, loss = 0.005347211845219135
iteration 284, loss = 0.00618794234469533
iteration 285, loss = 0.005819542333483696
iteration 286, loss = 0.00598644046112895
iteration 287, loss = 0.00564746605232358
iteration 288, loss = 0.007323785685002804
iteration 289, loss = 0.006790845189243555
iteration 290, loss = 0.005764385219663382
iteration 291, loss = 0.0058809975162148476
iteration 292, loss = 0.005936522968113422
iteration 293, loss = 0.005597963463515043
iteration 294, loss = 0.005815187469124794
iteration 295, loss = 0.008325016126036644
iteration 296, loss = 0.007514565717428923
iteration 297, loss = 0.00621799286454916
iteration 298, loss = 0.005675096530467272
iteration 299, loss = 0.005409564357250929
iteration 300, loss = 0.005882206838577986
iteration 1, loss = 0.006562211085110903
iteration 2, loss = 0.006125268992036581
iteration 3, loss = 0.008313140831887722
iteration 4, loss = 0.005479305982589722
iteration 5, loss = 0.0069793108850717545
iteration 6, loss = 0.009065928868949413
iteration 7, loss = 0.006666669622063637
iteration 8, loss = 0.0053649479523301125
iteration 9, loss = 0.007917312905192375
iteration 10, loss = 0.005742325913161039
iteration 11, loss = 0.0058069247752428055
iteration 12, loss = 0.005888411775231361
iteration 13, loss = 0.005109937395900488
iteration 14, loss = 0.0058517116121947765
iteration 15, loss = 0.005450671538710594
iteration 16, loss = 0.005511881783604622
iteration 17, loss = 0.006282696966081858
iteration 18, loss = 0.005610078573226929
iteration 19, loss = 0.006310493219643831
iteration 20, loss = 0.00520160049200058
iteration 21, loss = 0.006679655984044075
iteration 22, loss = 0.005228887312114239
iteration 23, loss = 0.005360071547329426
iteration 24, loss = 0.0065550971776247025
iteration 25, loss = 0.006791993975639343
iteration 26, loss = 0.005607654340565205
iteration 27, loss = 0.0052392445504665375
iteration 28, loss = 0.005806339904665947
iteration 29, loss = 0.005636598914861679
iteration 30, loss = 0.005934162065386772
iteration 31, loss = 0.008908214047551155
iteration 32, loss = 0.005668385419994593
iteration 33, loss = 0.005618277937173843
iteration 34, loss = 0.005341433919966221
iteration 35, loss = 0.00664093904197216
iteration 36, loss = 0.005847637541592121
iteration 37, loss = 0.005489800125360489
iteration 38, loss = 0.006731376051902771
iteration 39, loss = 0.006745478603988886
iteration 40, loss = 0.00584927573800087
iteration 41, loss = 0.00804422702640295
iteration 42, loss = 0.006612810306251049
iteration 43, loss = 0.00581944128498435
iteration 44, loss = 0.005260523874312639
iteration 45, loss = 0.008146052248775959
iteration 46, loss = 0.005855054594576359
iteration 47, loss = 0.0057588303461670876
iteration 48, loss = 0.005722000729292631
iteration 49, loss = 0.005952934268862009
iteration 50, loss = 0.005517674144357443
iteration 51, loss = 0.005438352935016155
iteration 52, loss = 0.0053873430006206036
iteration 53, loss = 0.009068175218999386
iteration 54, loss = 0.00553397461771965
iteration 55, loss = 0.005778144579380751
iteration 56, loss = 0.0056649441830813885
iteration 57, loss = 0.006154430564492941
iteration 58, loss = 0.005576715338975191
iteration 59, loss = 0.005823816172778606
iteration 60, loss = 0.006503690965473652
iteration 61, loss = 0.006188567727804184
iteration 62, loss = 0.006349560339003801
iteration 63, loss = 0.005394254345446825
iteration 64, loss = 0.006791951600462198
iteration 65, loss = 0.00605753343552351
iteration 66, loss = 0.005807219073176384
iteration 67, loss = 0.007777546066790819
iteration 68, loss = 0.00533015001565218
iteration 69, loss = 0.00859244167804718
iteration 70, loss = 0.007510575000196695
iteration 71, loss = 0.005941458977758884
iteration 72, loss = 0.005754892714321613
iteration 73, loss = 0.0056088389828801155
iteration 74, loss = 0.005436984822154045
iteration 75, loss = 0.006134008523076773
iteration 76, loss = 0.0050891730934381485
iteration 77, loss = 0.01024280209094286
iteration 78, loss = 0.006240148562937975
iteration 79, loss = 0.0057375989854335785
iteration 80, loss = 0.00586278410628438
iteration 81, loss = 0.006054514553397894
iteration 82, loss = 0.006797220557928085
iteration 83, loss = 0.007536509074270725
iteration 84, loss = 0.005657378118485212
iteration 85, loss = 0.007688870187848806
iteration 86, loss = 0.005580553784966469
iteration 87, loss = 0.006675126496702433
iteration 88, loss = 0.006025632843375206
iteration 89, loss = 0.00567280687391758
iteration 90, loss = 0.006344809662550688
iteration 91, loss = 0.005845649167895317
iteration 92, loss = 0.006141700316220522
iteration 93, loss = 0.006026148330420256
iteration 94, loss = 0.010739266872406006
iteration 95, loss = 0.005725638475269079
iteration 96, loss = 0.005413256119936705
iteration 97, loss = 0.005290537141263485
iteration 98, loss = 0.005753897130489349
iteration 99, loss = 0.006088034249842167
iteration 100, loss = 0.005707919131964445
iteration 101, loss = 0.006571352481842041
iteration 102, loss = 0.005729828495532274
iteration 103, loss = 0.0071449582464993
iteration 104, loss = 0.006019244436174631
iteration 105, loss = 0.005394443869590759
iteration 106, loss = 0.005433829501271248
iteration 107, loss = 0.005991759710013866
iteration 108, loss = 0.005906068719923496
iteration 109, loss = 0.00572662428021431
iteration 110, loss = 0.007772158831357956
iteration 111, loss = 0.005507888272404671
iteration 112, loss = 0.005968302953988314
iteration 113, loss = 0.006896654609590769
iteration 114, loss = 0.006282513029873371
iteration 115, loss = 0.005359352566301823
iteration 116, loss = 0.005532299634069204
iteration 117, loss = 0.006929623894393444
iteration 118, loss = 0.00801127403974533
iteration 119, loss = 0.005448571406304836
iteration 120, loss = 0.00612132903188467
iteration 121, loss = 0.008779614232480526
iteration 122, loss = 0.005898669362068176
iteration 123, loss = 0.005742204375565052
iteration 124, loss = 0.005591745488345623
iteration 125, loss = 0.00580956507474184
iteration 126, loss = 0.005346314515918493
iteration 127, loss = 0.0055758277885615826
iteration 128, loss = 0.005296760704368353
iteration 129, loss = 0.005561843514442444
iteration 130, loss = 0.005785721819847822
iteration 131, loss = 0.005658866837620735
iteration 132, loss = 0.005579358898103237
iteration 133, loss = 0.005611160304397345
iteration 134, loss = 0.006466542836278677
iteration 135, loss = 0.006102387793362141
iteration 136, loss = 0.0058526406064629555
iteration 137, loss = 0.005460983142256737
iteration 138, loss = 0.006153231952339411
iteration 139, loss = 0.005589062348008156
iteration 140, loss = 0.005296924617141485
iteration 141, loss = 0.006117883138358593
iteration 142, loss = 0.005495969206094742
iteration 143, loss = 0.006622782442718744
iteration 144, loss = 0.008518049493432045
iteration 145, loss = 0.005905680358409882
iteration 146, loss = 0.0056039621122181416
iteration 147, loss = 0.0052514029666781425
iteration 148, loss = 0.007493188604712486
iteration 149, loss = 0.005764958914369345
iteration 150, loss = 0.006687804590910673
iteration 151, loss = 0.005316976923495531
iteration 152, loss = 0.007273383438587189
iteration 153, loss = 0.005971156060695648
iteration 154, loss = 0.005236406344920397
iteration 155, loss = 0.005826251115649939
iteration 156, loss = 0.006178710609674454
iteration 157, loss = 0.005344265140593052
iteration 158, loss = 0.006116179749369621
iteration 159, loss = 0.005690747871994972
iteration 160, loss = 0.006361934822052717
iteration 161, loss = 0.005473199300467968
iteration 162, loss = 0.0065362961031496525
iteration 163, loss = 0.006850908510386944
iteration 164, loss = 0.006207347847521305
iteration 165, loss = 0.006349807605147362
iteration 166, loss = 0.005327741615474224
iteration 167, loss = 0.006535196676850319
iteration 168, loss = 0.005538321100175381
iteration 169, loss = 0.00548413023352623
iteration 170, loss = 0.006769914645701647
iteration 171, loss = 0.005795439705252647
iteration 172, loss = 0.008706199005246162
iteration 173, loss = 0.006293128244578838
iteration 174, loss = 0.005736894905567169
iteration 175, loss = 0.0053956713527441025
iteration 176, loss = 0.0057516805827617645
iteration 177, loss = 0.00572231225669384
iteration 178, loss = 0.005323240999132395
iteration 179, loss = 0.005492601078003645
iteration 180, loss = 0.00639847619459033
iteration 181, loss = 0.00552019290626049
iteration 182, loss = 0.007116981316357851
iteration 183, loss = 0.005859627854079008
iteration 184, loss = 0.005577602423727512
iteration 185, loss = 0.00781114911660552
iteration 186, loss = 0.005778785794973373
iteration 187, loss = 0.0052479589357972145
iteration 188, loss = 0.005343547556549311
iteration 189, loss = 0.005519185680896044
iteration 190, loss = 0.005831782706081867
iteration 191, loss = 0.006172041408717632
iteration 192, loss = 0.005512802861630917
iteration 193, loss = 0.0051882886327803135
iteration 194, loss = 0.006879561115056276
iteration 195, loss = 0.011543622240424156
iteration 196, loss = 0.01109591219574213
iteration 197, loss = 0.006118288729339838
iteration 198, loss = 0.006368923466652632
iteration 199, loss = 0.005892069078981876
iteration 200, loss = 0.005849654786288738
iteration 201, loss = 0.006261574570089579
iteration 202, loss = 0.005937748122960329
iteration 203, loss = 0.005760186351835728
iteration 204, loss = 0.005787343252450228
iteration 205, loss = 0.005616797134280205
iteration 206, loss = 0.00642352132126689
iteration 207, loss = 0.005662665236741304
iteration 208, loss = 0.005731996614485979
iteration 209, loss = 0.005915224086493254
iteration 210, loss = 0.0056225452572107315
iteration 211, loss = 0.005930140148848295
iteration 212, loss = 0.005678028799593449
iteration 213, loss = 0.005752815864980221
iteration 214, loss = 0.005815385840833187
iteration 215, loss = 0.006348976865410805
iteration 216, loss = 0.005756791681051254
iteration 217, loss = 0.005390366073697805
iteration 218, loss = 0.0066030011512339115
iteration 219, loss = 0.00600092951208353
iteration 220, loss = 0.005946371238678694
iteration 221, loss = 0.0058729033917188644
iteration 222, loss = 0.007209452800452709
iteration 223, loss = 0.007464210502803326
iteration 224, loss = 0.005331805907189846
iteration 225, loss = 0.0051322742365300655
iteration 226, loss = 0.005477177910506725
iteration 227, loss = 0.005786442663520575
iteration 228, loss = 0.005647400394082069
iteration 229, loss = 0.007208820432424545
iteration 230, loss = 0.00800115242600441
iteration 231, loss = 0.005287154112011194
iteration 232, loss = 0.007268036250025034
iteration 233, loss = 0.005865832790732384
iteration 234, loss = 0.005716907791793346
iteration 235, loss = 0.005502169486135244
iteration 236, loss = 0.006073250435292721
iteration 237, loss = 0.005928146652877331
iteration 238, loss = 0.008654140867292881
iteration 239, loss = 0.005807856563478708
iteration 240, loss = 0.0055267903953790665
iteration 241, loss = 0.005252518225461245
iteration 242, loss = 0.005611272994428873
iteration 243, loss = 0.006845131516456604
iteration 244, loss = 0.005890647880733013
iteration 245, loss = 0.005632687825709581
iteration 246, loss = 0.010663790628314018
iteration 247, loss = 0.008452285081148148
iteration 248, loss = 0.006688157096505165
iteration 249, loss = 0.0070012821815907955
iteration 250, loss = 0.006799405440688133
iteration 251, loss = 0.005936197936534882
iteration 252, loss = 0.009007885120809078
iteration 253, loss = 0.005171097815036774
iteration 254, loss = 0.0059624724090099335
iteration 255, loss = 0.0057364096865057945
iteration 256, loss = 0.0058048199862241745
iteration 257, loss = 0.0061477250419557095
iteration 258, loss = 0.007555314339697361
iteration 259, loss = 0.005299099255353212
iteration 260, loss = 0.005906766280531883
iteration 261, loss = 0.005414305254817009
iteration 262, loss = 0.005901290103793144
iteration 263, loss = 0.006573015358299017
iteration 264, loss = 0.006295721512287855
iteration 265, loss = 0.005544920451939106
iteration 266, loss = 0.005157840438187122
iteration 267, loss = 0.006029716692864895
iteration 268, loss = 0.005571029614657164
iteration 269, loss = 0.005571709480136633
iteration 270, loss = 0.006106024142354727
iteration 271, loss = 0.006132258102297783
iteration 272, loss = 0.00812105555087328
iteration 273, loss = 0.006100062280893326
iteration 274, loss = 0.005839325953274965
iteration 275, loss = 0.007232497911900282
iteration 276, loss = 0.006287972442805767
iteration 277, loss = 0.007719976827502251
iteration 278, loss = 0.005513730924576521
iteration 279, loss = 0.005677902139723301
iteration 280, loss = 0.005165341775864363
iteration 281, loss = 0.005818852223455906
iteration 282, loss = 0.0072938986122608185
iteration 283, loss = 0.005804810207337141
iteration 284, loss = 0.005139647051692009
iteration 285, loss = 0.006251894403249025
iteration 286, loss = 0.007009468507021666
iteration 287, loss = 0.0087357759475708
iteration 288, loss = 0.005118185188621283
iteration 289, loss = 0.005636740941554308
iteration 290, loss = 0.00659719156101346
iteration 291, loss = 0.005550897680222988
iteration 292, loss = 0.005788259208202362
iteration 293, loss = 0.005509810987859964
iteration 294, loss = 0.00529105868190527
iteration 295, loss = 0.005871474742889404
iteration 296, loss = 0.00550630409270525
iteration 297, loss = 0.005501825362443924
iteration 298, loss = 0.005663069896399975
iteration 299, loss = 0.007299053017050028
iteration 300, loss = 0.005220099352300167

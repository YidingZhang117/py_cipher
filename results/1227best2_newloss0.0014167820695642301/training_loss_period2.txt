iteration 1, loss = 0.00553121417760849
iteration 2, loss = 0.018958760425448418
iteration 3, loss = 0.005296316929161549
iteration 4, loss = 0.0055609652772545815
iteration 5, loss = 0.08424481004476547
iteration 6, loss = 0.0055398340336978436
iteration 7, loss = 0.14683564007282257
iteration 8, loss = 0.06786318123340607
iteration 9, loss = 0.005600062198936939
iteration 10, loss = 0.0159298125654459
iteration 11, loss = 0.006055314559489489
iteration 12, loss = 0.037989258766174316
iteration 13, loss = 0.01719372346997261
iteration 14, loss = 0.006945556495338678
iteration 15, loss = 0.005466839298605919
iteration 16, loss = 0.013512887060642242
iteration 17, loss = 0.022631211206316948
iteration 18, loss = 0.02809712290763855
iteration 19, loss = 0.0055041019804775715
iteration 20, loss = 0.006173222791403532
iteration 21, loss = 0.012215042486786842
iteration 22, loss = 0.00868433527648449
iteration 23, loss = 0.005686736665666103
iteration 24, loss = 0.014743355102837086
iteration 25, loss = 0.011356686241924763
iteration 26, loss = 0.012987297959625721
iteration 27, loss = 0.008264969103038311
iteration 28, loss = 0.00568959629163146
iteration 29, loss = 0.005177833139896393
iteration 30, loss = 0.005926814861595631
iteration 31, loss = 0.02010400965809822
iteration 32, loss = 0.006159368436783552
iteration 33, loss = 0.01103891246020794
iteration 34, loss = 0.01508491300046444
iteration 35, loss = 0.005402687005698681
iteration 36, loss = 0.010094251483678818
iteration 37, loss = 0.004865280818194151
iteration 38, loss = 0.009530565701425076
iteration 39, loss = 0.011385933496057987
iteration 40, loss = 0.009495330974459648
iteration 41, loss = 0.013149378821253777
iteration 42, loss = 0.0094275688752532
iteration 43, loss = 0.00960710272192955
iteration 44, loss = 0.005411394871771336
iteration 45, loss = 0.010323932394385338
iteration 46, loss = 0.00845792144536972
iteration 47, loss = 0.016906296834349632
iteration 48, loss = 0.00850648619234562
iteration 49, loss = 0.00518233235925436
iteration 50, loss = 0.008272567763924599
iteration 51, loss = 0.0054946597665548325
iteration 52, loss = 0.005968007259070873
iteration 53, loss = 0.007864037528634071
iteration 54, loss = 0.00608912855386734
iteration 55, loss = 0.009525686502456665
iteration 56, loss = 0.011276783421635628
iteration 57, loss = 0.01104209665209055
iteration 58, loss = 0.006685303989797831
iteration 59, loss = 0.005363603122532368
iteration 60, loss = 0.009975272230803967
iteration 61, loss = 0.00820839125663042
iteration 62, loss = 0.00812884047627449
iteration 63, loss = 0.007381766103208065
iteration 64, loss = 0.005971149541437626
iteration 65, loss = 0.00499028479680419
iteration 66, loss = 0.008055032230913639
iteration 67, loss = 0.007849602028727531
iteration 68, loss = 0.009859851561486721
iteration 69, loss = 0.007569211535155773
iteration 70, loss = 0.009882258251309395
iteration 71, loss = 0.0070930365473032
iteration 72, loss = 0.005680140573531389
iteration 73, loss = 0.008176786825060844
iteration 74, loss = 0.007600307464599609
iteration 75, loss = 0.008877929300069809
iteration 76, loss = 0.006879306863993406
iteration 77, loss = 0.005223334766924381
iteration 78, loss = 0.010838671587407589
iteration 79, loss = 0.005326688289642334
iteration 80, loss = 0.007026116829365492
iteration 81, loss = 0.007668168283998966
iteration 82, loss = 0.007980521768331528
iteration 83, loss = 0.009761634282767773
iteration 84, loss = 0.005319755524396896
iteration 85, loss = 0.00734348502010107
iteration 86, loss = 0.0061377109959721565
iteration 87, loss = 0.0069597684778273106
iteration 88, loss = 0.005856228061020374
iteration 89, loss = 0.005570490378886461
iteration 90, loss = 0.010183964855968952
iteration 91, loss = 0.008807633072137833
iteration 92, loss = 0.008367045782506466
iteration 93, loss = 0.009304453618824482
iteration 94, loss = 0.007161758840084076
iteration 95, loss = 0.0050003910437226295
iteration 96, loss = 0.00546319130808115
iteration 97, loss = 0.005583825521171093
iteration 98, loss = 0.005102760158479214
iteration 99, loss = 0.005488927476108074
iteration 100, loss = 0.006799267139285803
iteration 101, loss = 0.00573184946551919
iteration 102, loss = 0.007819579914212227
iteration 103, loss = 0.01133884210139513
iteration 104, loss = 0.005417707841843367
iteration 105, loss = 0.007450039032846689
iteration 106, loss = 0.005011968780308962
iteration 107, loss = 0.007090846076607704
iteration 108, loss = 0.013302316889166832
iteration 109, loss = 0.005286399740725756
iteration 110, loss = 0.005828247405588627
iteration 111, loss = 0.006869018077850342
iteration 112, loss = 0.012081200256943703
iteration 113, loss = 0.006959045771509409
iteration 114, loss = 0.007108747027814388
iteration 115, loss = 0.005516487173736095
iteration 116, loss = 0.005213392432779074
iteration 117, loss = 0.006480376236140728
iteration 118, loss = 0.009356193244457245
iteration 119, loss = 0.004933399613946676
iteration 120, loss = 0.005161421839147806
iteration 121, loss = 0.012446630746126175
iteration 122, loss = 0.004942507483065128
iteration 123, loss = 0.005440275650471449
iteration 124, loss = 0.005194884724915028
iteration 125, loss = 0.006172597873955965
iteration 126, loss = 0.0069442009553313255
iteration 127, loss = 0.005467884708195925
iteration 128, loss = 0.0050787851214408875
iteration 129, loss = 0.005729848053306341
iteration 130, loss = 0.0065682451240718365
iteration 131, loss = 0.007496572565287352
iteration 132, loss = 0.00524165341630578
iteration 133, loss = 0.005724671296775341
iteration 134, loss = 0.007284265942871571
iteration 135, loss = 0.004833480808883905
iteration 136, loss = 0.009865084663033485
iteration 137, loss = 0.007251748815178871
iteration 138, loss = 0.006243055686354637
iteration 139, loss = 0.007242259103804827
iteration 140, loss = 0.00881681963801384
iteration 141, loss = 0.007094204425811768
iteration 142, loss = 0.008258928544819355
iteration 143, loss = 0.005267565604299307
iteration 144, loss = 0.006746781058609486
iteration 145, loss = 0.013048745691776276
iteration 146, loss = 0.005338592454791069
iteration 147, loss = 0.0054193222895264626
iteration 148, loss = 0.010528931394219398
iteration 149, loss = 0.006722767371684313
iteration 150, loss = 0.005067910999059677
iteration 151, loss = 0.007133558392524719
iteration 152, loss = 0.006504193879663944
iteration 153, loss = 0.006408173590898514
iteration 154, loss = 0.004922095220535994
iteration 155, loss = 0.006954754237085581
iteration 156, loss = 0.006846078671514988
iteration 157, loss = 0.006892812438309193
iteration 158, loss = 0.005282610654830933
iteration 159, loss = 0.006862543988972902
iteration 160, loss = 0.005671589635312557
iteration 161, loss = 0.006702192593365908
iteration 162, loss = 0.00625859759747982
iteration 163, loss = 0.0058818357065320015
iteration 164, loss = 0.00660086702555418
iteration 165, loss = 0.005188241135329008
iteration 166, loss = 0.004980586469173431
iteration 167, loss = 0.0065738665871322155
iteration 168, loss = 0.0048973532393574715
iteration 169, loss = 0.004810753278434277
iteration 170, loss = 0.0063038477674126625
iteration 171, loss = 0.005107042845338583
iteration 172, loss = 0.005398439709097147
iteration 173, loss = 0.006842110771685839
iteration 174, loss = 0.005333463661372662
iteration 175, loss = 0.005017917603254318
iteration 176, loss = 0.0046782661229372025
iteration 177, loss = 0.012110396288335323
iteration 178, loss = 0.005777603946626186
iteration 179, loss = 0.006557592190802097
iteration 180, loss = 0.0051813083700835705
iteration 181, loss = 0.006389693357050419
iteration 182, loss = 0.00537439901381731
iteration 183, loss = 0.004933244548738003
iteration 184, loss = 0.007972054183483124
iteration 185, loss = 0.004837799817323685
iteration 186, loss = 0.00839205365628004
iteration 187, loss = 0.012187683954834938
iteration 188, loss = 0.005075464490801096
iteration 189, loss = 0.004923700820654631
iteration 190, loss = 0.0071664899587631226
iteration 191, loss = 0.0053921230137348175
iteration 192, loss = 0.006110418122261763
iteration 193, loss = 0.00649639405310154
iteration 194, loss = 0.009477979503571987
iteration 195, loss = 0.004932029638439417
iteration 196, loss = 0.006567047908902168
iteration 197, loss = 0.008440889418125153
iteration 198, loss = 0.008616370148956776
iteration 199, loss = 0.005326171405613422
iteration 200, loss = 0.00620803888887167
iteration 201, loss = 0.009117137640714645
iteration 202, loss = 0.00956645980477333
iteration 203, loss = 0.0048039997927844524
iteration 204, loss = 0.0060083502903580666
iteration 205, loss = 0.006366604473441839
iteration 206, loss = 0.00625300919637084
iteration 207, loss = 0.0048929303884506226
iteration 208, loss = 0.008442617952823639
iteration 209, loss = 0.005248231813311577
iteration 210, loss = 0.005526036489754915
iteration 211, loss = 0.006562520749866962
iteration 212, loss = 0.004967170767486095
iteration 213, loss = 0.00469995616003871
iteration 214, loss = 0.004726729821413755
iteration 215, loss = 0.009552066214382648
iteration 216, loss = 0.00962739996612072
iteration 217, loss = 0.009474444203078747
iteration 218, loss = 0.006049857474863529
iteration 219, loss = 0.007965044118463993
iteration 220, loss = 0.006272731814533472
iteration 221, loss = 0.004853841848671436
iteration 222, loss = 0.0073594702407717705
iteration 223, loss = 0.007335502654314041
iteration 224, loss = 0.007453831844031811
iteration 225, loss = 0.009667055681347847
iteration 226, loss = 0.0047044334933161736
iteration 227, loss = 0.005944425240159035
iteration 228, loss = 0.007058375049382448
iteration 229, loss = 0.006240757182240486
iteration 230, loss = 0.009351223707199097
iteration 231, loss = 0.006847804877907038
iteration 232, loss = 0.006150445900857449
iteration 233, loss = 0.005866301245987415
iteration 234, loss = 0.013373798690736294
iteration 235, loss = 0.00699517922475934
iteration 236, loss = 0.007837559096515179
iteration 237, loss = 0.0052543263882398605
iteration 238, loss = 0.007238657213747501
iteration 239, loss = 0.005207960493862629
iteration 240, loss = 0.00496384222060442
iteration 241, loss = 0.006025617942214012
iteration 242, loss = 0.004757498390972614
iteration 243, loss = 0.006041224114596844
iteration 244, loss = 0.004664886277168989
iteration 245, loss = 0.007327104918658733
iteration 246, loss = 0.004669676534831524
iteration 247, loss = 0.007001369260251522
iteration 248, loss = 0.004657252226024866
iteration 249, loss = 0.005685793235898018
iteration 250, loss = 0.0054798549972474575
iteration 251, loss = 0.005788426846265793
iteration 252, loss = 0.005744774825870991
iteration 253, loss = 0.00644358666613698
iteration 254, loss = 0.005689999088644981
iteration 255, loss = 0.006214706227183342
iteration 256, loss = 0.011016358621418476
iteration 257, loss = 0.006086437962949276
iteration 258, loss = 0.004873144440352917
iteration 259, loss = 0.0045056273229420185
iteration 260, loss = 0.006906839553266764
iteration 261, loss = 0.005258800927549601
iteration 262, loss = 0.005863834638148546
iteration 263, loss = 0.010218840092420578
iteration 264, loss = 0.004863683599978685
iteration 265, loss = 0.004477466456592083
iteration 266, loss = 0.006237846799194813
iteration 267, loss = 0.004603224340826273
iteration 268, loss = 0.004891212563961744
iteration 269, loss = 0.00650104321539402
iteration 270, loss = 0.004850694909691811
iteration 271, loss = 0.005954058840870857
iteration 272, loss = 0.004793256986886263
iteration 273, loss = 0.005788244307041168
iteration 274, loss = 0.007666271645575762
iteration 275, loss = 0.005934393964707851
iteration 276, loss = 0.005137525498867035
iteration 277, loss = 0.008047291077673435
iteration 278, loss = 0.004344062879681587
iteration 279, loss = 0.011992375366389751
iteration 280, loss = 0.0059915101155638695
iteration 281, loss = 0.004521390423178673
iteration 282, loss = 0.006886985152959824
iteration 283, loss = 0.004401395563036203
iteration 284, loss = 0.005140341352671385
iteration 285, loss = 0.0047419434413313866
iteration 286, loss = 0.006967139430344105
iteration 287, loss = 0.005999205633997917
iteration 288, loss = 0.005427703261375427
iteration 289, loss = 0.004764803219586611
iteration 290, loss = 0.01061448734253645
iteration 291, loss = 0.006135310046374798
iteration 292, loss = 0.005621389020234346
iteration 293, loss = 0.004633165430277586
iteration 294, loss = 0.004768049344420433
iteration 295, loss = 0.007493152283132076
iteration 296, loss = 0.004661498591303825
iteration 297, loss = 0.004638595040887594
iteration 298, loss = 0.004733061883598566
iteration 299, loss = 0.00466463016346097
iteration 300, loss = 0.008841440081596375
iteration 1, loss = 0.005140178836882114
iteration 2, loss = 0.00442295940592885
iteration 3, loss = 0.004296805709600449
iteration 4, loss = 0.006650455296039581
iteration 5, loss = 0.006254576146602631
iteration 6, loss = 0.0045558493584394455
iteration 7, loss = 0.004428029991686344
iteration 8, loss = 0.004296001046895981
iteration 9, loss = 0.00549804512411356
iteration 10, loss = 0.010603819042444229
iteration 11, loss = 0.006674591451883316
iteration 12, loss = 0.005277659744024277
iteration 13, loss = 0.004326835740357637
iteration 14, loss = 0.00551105011254549
iteration 15, loss = 0.0049852728843688965
iteration 16, loss = 0.006229367572814226
iteration 17, loss = 0.005703637842088938
iteration 18, loss = 0.006945917382836342
iteration 19, loss = 0.005889061838388443
iteration 20, loss = 0.004655147902667522
iteration 21, loss = 0.005331072025001049
iteration 22, loss = 0.007270839996635914
iteration 23, loss = 0.006213871296495199
iteration 24, loss = 0.004566183313727379
iteration 25, loss = 0.004866661969572306
iteration 26, loss = 0.0064124614000320435
iteration 27, loss = 0.0066473535262048244
iteration 28, loss = 0.004663500003516674
iteration 29, loss = 0.004482618998736143
iteration 30, loss = 0.008359796367585659
iteration 31, loss = 0.007195757701992989
iteration 32, loss = 0.005138100124895573
iteration 33, loss = 0.005631404463201761
iteration 34, loss = 0.004560359288007021
iteration 35, loss = 0.0048472145572304726
iteration 36, loss = 0.009216601960361004
iteration 37, loss = 0.0046060043387115
iteration 38, loss = 0.00473039411008358
iteration 39, loss = 0.006123917642980814
iteration 40, loss = 0.006799815222620964
iteration 41, loss = 0.005475432146340609
iteration 42, loss = 0.0069360192865133286
iteration 43, loss = 0.0052985381335020065
iteration 44, loss = 0.005023404955863953
iteration 45, loss = 0.007116156164556742
iteration 46, loss = 0.00523587130010128
iteration 47, loss = 0.007740810047835112
iteration 48, loss = 0.004324131645262241
iteration 49, loss = 0.0044822730123996735
iteration 50, loss = 0.005256584845483303
iteration 51, loss = 0.006512137595564127
iteration 52, loss = 0.006294467486441135
iteration 53, loss = 0.004443297162652016
iteration 54, loss = 0.004514602012932301
iteration 55, loss = 0.008714819326996803
iteration 56, loss = 0.004823024850338697
iteration 57, loss = 0.00427809776738286
iteration 58, loss = 0.007026812061667442
iteration 59, loss = 0.0044408151879906654
iteration 60, loss = 0.005419198889285326
iteration 61, loss = 0.006052451673895121
iteration 62, loss = 0.004367379005998373
iteration 63, loss = 0.004378559533506632
iteration 64, loss = 0.006666569970548153
iteration 65, loss = 0.004978409502655268
iteration 66, loss = 0.004597451537847519
iteration 67, loss = 0.004755830392241478
iteration 68, loss = 0.009204098023474216
iteration 69, loss = 0.0044210548512637615
iteration 70, loss = 0.004390949849039316
iteration 71, loss = 0.004619147628545761
iteration 72, loss = 0.0071455808356404305
iteration 73, loss = 0.0044514392502605915
iteration 74, loss = 0.008665291592478752
iteration 75, loss = 0.0043710521422326565
iteration 76, loss = 0.0047013298608362675
iteration 77, loss = 0.006438781972974539
iteration 78, loss = 0.0055108442902565
iteration 79, loss = 0.004161843564361334
iteration 80, loss = 0.004406028427183628
iteration 81, loss = 0.007276422809809446
iteration 82, loss = 0.0064915139228105545
iteration 83, loss = 0.004197046160697937
iteration 84, loss = 0.00879652053117752
iteration 85, loss = 0.006637497805058956
iteration 86, loss = 0.004188592545688152
iteration 87, loss = 0.005871247965842485
iteration 88, loss = 0.005186610855162144
iteration 89, loss = 0.007885316386818886
iteration 90, loss = 0.006055799312889576
iteration 91, loss = 0.004176588729023933
iteration 92, loss = 0.005277983844280243
iteration 93, loss = 0.006621986627578735
iteration 94, loss = 0.006063536740839481
iteration 95, loss = 0.007942557334899902
iteration 96, loss = 0.0057378290221095085
iteration 97, loss = 0.00624148128554225
iteration 98, loss = 0.004354437347501516
iteration 99, loss = 0.006012356374412775
iteration 100, loss = 0.0069449907168745995
iteration 101, loss = 0.006077701225876808
iteration 102, loss = 0.004380110651254654
iteration 103, loss = 0.004132009577006102
iteration 104, loss = 0.0062913247384130955
iteration 105, loss = 0.0055333394557237625
iteration 106, loss = 0.004439167212694883
iteration 107, loss = 0.006946701556444168
iteration 108, loss = 0.00419947924092412
iteration 109, loss = 0.007185402326285839
iteration 110, loss = 0.004113934934139252
iteration 111, loss = 0.004543950315564871
iteration 112, loss = 0.0077580963261425495
iteration 113, loss = 0.004284568130970001
iteration 114, loss = 0.0061140358448028564
iteration 115, loss = 0.008322128094732761
iteration 116, loss = 0.005908513441681862
iteration 117, loss = 0.004888019524514675
iteration 118, loss = 0.005424465984106064
iteration 119, loss = 0.009307325817644596
iteration 120, loss = 0.008239690214395523
iteration 121, loss = 0.00630052387714386
iteration 122, loss = 0.005395570304244757
iteration 123, loss = 0.005432137753814459
iteration 124, loss = 0.004484864883124828
iteration 125, loss = 0.008254402317106724
iteration 126, loss = 0.007087624166160822
iteration 127, loss = 0.004363603424280882
iteration 128, loss = 0.004743807017803192
iteration 129, loss = 0.004192906431853771
iteration 130, loss = 0.005581796169281006
iteration 131, loss = 0.006437663454562426
iteration 132, loss = 0.00872800499200821
iteration 133, loss = 0.004167771898210049
iteration 134, loss = 0.004851759877055883
iteration 135, loss = 0.004935532808303833
iteration 136, loss = 0.0048393867909908295
iteration 137, loss = 0.0043262033723294735
iteration 138, loss = 0.004221221432089806
iteration 139, loss = 0.0062136538326740265
iteration 140, loss = 0.004356567747890949
iteration 141, loss = 0.004101167898625135
iteration 142, loss = 0.00682749692350626
iteration 143, loss = 0.0041311876848340034
iteration 144, loss = 0.005696213338524103
iteration 145, loss = 0.006702173966914415
iteration 146, loss = 0.00423036515712738
iteration 147, loss = 0.006633944809436798
iteration 148, loss = 0.00416159862652421
iteration 149, loss = 0.004667212255299091
iteration 150, loss = 0.004693849012255669
iteration 151, loss = 0.0062385606579482555
iteration 152, loss = 0.008347809314727783
iteration 153, loss = 0.004363689571619034
iteration 154, loss = 0.006371940486133099
iteration 155, loss = 0.005280775483697653
iteration 156, loss = 0.004319770261645317
iteration 157, loss = 0.0056078070774674416
iteration 158, loss = 0.005432613659650087
iteration 159, loss = 0.004282242152839899
iteration 160, loss = 0.008844737894833088
iteration 161, loss = 0.004670407623052597
iteration 162, loss = 0.003928091377019882
iteration 163, loss = 0.008054519072175026
iteration 164, loss = 0.00438475888222456
iteration 165, loss = 0.0063534812070429325
iteration 166, loss = 0.005511592607945204
iteration 167, loss = 0.00475615169852972
iteration 168, loss = 0.004807657562196255
iteration 169, loss = 0.005466123577207327
iteration 170, loss = 0.005070218350738287
iteration 171, loss = 0.004072406329214573
iteration 172, loss = 0.00557052344083786
iteration 173, loss = 0.004149315413087606
iteration 174, loss = 0.004708562977612019
iteration 175, loss = 0.004322710447013378
iteration 176, loss = 0.00615654606372118
iteration 177, loss = 0.004924445878714323
iteration 178, loss = 0.006142687518149614
iteration 179, loss = 0.0056288596242666245
iteration 180, loss = 0.004806237295269966
iteration 181, loss = 0.005088902078568935
iteration 182, loss = 0.00509197823703289
iteration 183, loss = 0.007916471920907497
iteration 184, loss = 0.004295450635254383
iteration 185, loss = 0.004785803146660328
iteration 186, loss = 0.005785050336271524
iteration 187, loss = 0.0062498231418430805
iteration 188, loss = 0.004623706918209791
iteration 189, loss = 0.0062450990080833435
iteration 190, loss = 0.004550307057797909
iteration 191, loss = 0.004027534741908312
iteration 192, loss = 0.00403884332627058
iteration 193, loss = 0.004461274947971106
iteration 194, loss = 0.00522952014580369
iteration 195, loss = 0.004044491797685623
iteration 196, loss = 0.005257919896394014
iteration 197, loss = 0.005241464823484421
iteration 198, loss = 0.005208626855164766
iteration 199, loss = 0.004831590224057436
iteration 200, loss = 0.004219271242618561
iteration 201, loss = 0.004340733401477337
iteration 202, loss = 0.003948370460420847
iteration 203, loss = 0.004134020302444696
iteration 204, loss = 0.005296100862324238
iteration 205, loss = 0.005004922393709421
iteration 206, loss = 0.004363157320767641
iteration 207, loss = 0.004840075504034758
iteration 208, loss = 0.004487946629524231
iteration 209, loss = 0.004081069491803646
iteration 210, loss = 0.005186731927096844
iteration 211, loss = 0.004645829554647207
iteration 212, loss = 0.0039734067395329475
iteration 213, loss = 0.0077875941060483456
iteration 214, loss = 0.003994426690042019
iteration 215, loss = 0.008059060201048851
iteration 216, loss = 0.0047293975949287415
iteration 217, loss = 0.003972052130848169
iteration 218, loss = 0.004167371429502964
iteration 219, loss = 0.004631977062672377
iteration 220, loss = 0.009692132472991943
iteration 221, loss = 0.00551726296544075
iteration 222, loss = 0.0052048261277377605
iteration 223, loss = 0.004731877241283655
iteration 224, loss = 0.003951325546950102
iteration 225, loss = 0.004535113461315632
iteration 226, loss = 0.004013609606772661
iteration 227, loss = 0.004247511271387339
iteration 228, loss = 0.0043647633865475655
iteration 229, loss = 0.004174961242824793
iteration 230, loss = 0.004453269764780998
iteration 231, loss = 0.004790526814758778
iteration 232, loss = 0.00420417170971632
iteration 233, loss = 0.00407516211271286
iteration 234, loss = 0.0040679750964045525
iteration 235, loss = 0.004780632443726063
iteration 236, loss = 0.003829146269708872
iteration 237, loss = 0.004396907985210419
iteration 238, loss = 0.005389643833041191
iteration 239, loss = 0.004520001355558634
iteration 240, loss = 0.004267363343387842
iteration 241, loss = 0.005306682549417019
iteration 242, loss = 0.005262929480522871
iteration 243, loss = 0.004102010745555162
iteration 244, loss = 0.004921409301459789
iteration 245, loss = 0.004334006924182177
iteration 246, loss = 0.004523120354861021
iteration 247, loss = 0.00479018734768033
iteration 248, loss = 0.003912715706974268
iteration 249, loss = 0.0039388216100633144
iteration 250, loss = 0.005997043568640947
iteration 251, loss = 0.0042417277581989765
iteration 252, loss = 0.004683938343077898
iteration 253, loss = 0.004342756699770689
iteration 254, loss = 0.006060841493308544
iteration 255, loss = 0.005227724090218544
iteration 256, loss = 0.006057165563106537
iteration 257, loss = 0.004515518434345722
iteration 258, loss = 0.005698020104318857
iteration 259, loss = 0.005197643302381039
iteration 260, loss = 0.005228268913924694
iteration 261, loss = 0.0071769896894693375
iteration 262, loss = 0.0039408570155501366
iteration 263, loss = 0.0050655570812523365
iteration 264, loss = 0.008485369384288788
iteration 265, loss = 0.0050183869898319244
iteration 266, loss = 0.006031070835888386
iteration 267, loss = 0.00414002500474453
iteration 268, loss = 0.006902333348989487
iteration 269, loss = 0.004454800859093666
iteration 270, loss = 0.005700924899429083
iteration 271, loss = 0.005813681520521641
iteration 272, loss = 0.004461037460714579
iteration 273, loss = 0.005024096462875605
iteration 274, loss = 0.005821235943585634
iteration 275, loss = 0.005010581109672785
iteration 276, loss = 0.004211122170090675
iteration 277, loss = 0.004857582040131092
iteration 278, loss = 0.0037541992496699095
iteration 279, loss = 0.005099167115986347
iteration 280, loss = 0.004309794399887323
iteration 281, loss = 0.003854258917272091
iteration 282, loss = 0.004657671321183443
iteration 283, loss = 0.0054874056950211525
iteration 284, loss = 0.0038580649998039007
iteration 285, loss = 0.0045759594067931175
iteration 286, loss = 0.004286507610231638
iteration 287, loss = 0.003951814956963062
iteration 288, loss = 0.003937387373298407
iteration 289, loss = 0.0046087587252259254
iteration 290, loss = 0.008332005701959133
iteration 291, loss = 0.005138300359249115
iteration 292, loss = 0.005344346631318331
iteration 293, loss = 0.007919829338788986
iteration 294, loss = 0.004356353543698788
iteration 295, loss = 0.008113935589790344
iteration 296, loss = 0.004797091707587242
iteration 297, loss = 0.006012841127812862
iteration 298, loss = 0.004234751686453819
iteration 299, loss = 0.007878251373767853
iteration 300, loss = 0.004702180158346891
iteration 1, loss = 0.004393965471535921
iteration 2, loss = 0.005569011904299259
iteration 3, loss = 0.004284705501049757
iteration 4, loss = 0.004941736813634634
iteration 5, loss = 0.004357648082077503
iteration 6, loss = 0.005940580274909735
iteration 7, loss = 0.00407658563926816
iteration 8, loss = 0.005565416067838669
iteration 9, loss = 0.004221262410283089
iteration 10, loss = 0.0038590035401284695
iteration 11, loss = 0.0054330239072442055
iteration 12, loss = 0.0048915920779109
iteration 13, loss = 0.007542753126472235
iteration 14, loss = 0.003921991214156151
iteration 15, loss = 0.005466466769576073
iteration 16, loss = 0.004564820788800716
iteration 17, loss = 0.004834761377424002
iteration 18, loss = 0.004274451639503241
iteration 19, loss = 0.005070536397397518
iteration 20, loss = 0.005649053491652012
iteration 21, loss = 0.00488500390201807
iteration 22, loss = 0.009222783148288727
iteration 23, loss = 0.0037471740506589413
iteration 24, loss = 0.005123726092278957
iteration 25, loss = 0.004395251162350178
iteration 26, loss = 0.0041100140661001205
iteration 27, loss = 0.005991747137159109
iteration 28, loss = 0.003591812215745449
iteration 29, loss = 0.006226751022040844
iteration 30, loss = 0.0041836826130747795
iteration 31, loss = 0.0046195778995752335
iteration 32, loss = 0.004587011411786079
iteration 33, loss = 0.0047925785183906555
iteration 34, loss = 0.0039072888903319836
iteration 35, loss = 0.009063788689672947
iteration 36, loss = 0.004617484286427498
iteration 37, loss = 0.003861302975565195
iteration 38, loss = 0.0041924710385501385
iteration 39, loss = 0.006854832172393799
iteration 40, loss = 0.00369458575733006
iteration 41, loss = 0.004757212940603495
iteration 42, loss = 0.005593604873865843
iteration 43, loss = 0.0044268169440329075
iteration 44, loss = 0.003926845733076334
iteration 45, loss = 0.004465712700039148
iteration 46, loss = 0.0045991865918040276
iteration 47, loss = 0.003794225864112377
iteration 48, loss = 0.0039565302431583405
iteration 49, loss = 0.004416333977133036
iteration 50, loss = 0.004395034164190292
iteration 51, loss = 0.0051228925585746765
iteration 52, loss = 0.0038594005163758993
iteration 53, loss = 0.005265937652438879
iteration 54, loss = 0.007450718432664871
iteration 55, loss = 0.004244880750775337
iteration 56, loss = 0.004037246108055115
iteration 57, loss = 0.0041448017582297325
iteration 58, loss = 0.003720811801031232
iteration 59, loss = 0.007547409273684025
iteration 60, loss = 0.005514668766409159
iteration 61, loss = 0.003876135218888521
iteration 62, loss = 0.004262328613549471
iteration 63, loss = 0.004859220236539841
iteration 64, loss = 0.003945583943277597
iteration 65, loss = 0.003723716828972101
iteration 66, loss = 0.0043340083211660385
iteration 67, loss = 0.004530430771410465
iteration 68, loss = 0.004895960912108421
iteration 69, loss = 0.0039000557735562325
iteration 70, loss = 0.007524116896092892
iteration 71, loss = 0.004797859117388725
iteration 72, loss = 0.0038673020899295807
iteration 73, loss = 0.0037864798214286566
iteration 74, loss = 0.0037292002234607935
iteration 75, loss = 0.004283403977751732
iteration 76, loss = 0.004078201949596405
iteration 77, loss = 0.0036379070952534676
iteration 78, loss = 0.0036897407844662666
iteration 79, loss = 0.0038361286278814077
iteration 80, loss = 0.004310391843318939
iteration 81, loss = 0.00413125054910779
iteration 82, loss = 0.004207215271890163
iteration 83, loss = 0.004965835716575384
iteration 84, loss = 0.0035177094396203756
iteration 85, loss = 0.005157193168997765
iteration 86, loss = 0.004215942230075598
iteration 87, loss = 0.004108669236302376
iteration 88, loss = 0.005040562711656094
iteration 89, loss = 0.003592082764953375
iteration 90, loss = 0.003642287803813815
iteration 91, loss = 0.004130796063691378
iteration 92, loss = 0.00527433305978775
iteration 93, loss = 0.004419744946062565
iteration 94, loss = 0.003686503041535616
iteration 95, loss = 0.0039827898144721985
iteration 96, loss = 0.004526224918663502
iteration 97, loss = 0.004938108846545219
iteration 98, loss = 0.003934937529265881
iteration 99, loss = 0.0037196956109255552
iteration 100, loss = 0.003948552068322897
iteration 101, loss = 0.004586771596223116
iteration 102, loss = 0.003530761692672968
iteration 103, loss = 0.0037828469648957253
iteration 104, loss = 0.0034804774913936853
iteration 105, loss = 0.007044763304293156
iteration 106, loss = 0.005189275369048119
iteration 107, loss = 0.004381405655294657
iteration 108, loss = 0.00427554314956069
iteration 109, loss = 0.0035220985300838947
iteration 110, loss = 0.007294969633221626
iteration 111, loss = 0.005208954215049744
iteration 112, loss = 0.003771429415792227
iteration 113, loss = 0.003750850446522236
iteration 114, loss = 0.004056158941239119
iteration 115, loss = 0.00550214434042573
iteration 116, loss = 0.0038176102098077536
iteration 117, loss = 0.0046278322115540504
iteration 118, loss = 0.005669870879501104
iteration 119, loss = 0.004258913453668356
iteration 120, loss = 0.004807496443390846
iteration 121, loss = 0.0048323217779397964
iteration 122, loss = 0.0038924221880733967
iteration 123, loss = 0.0036219682078808546
iteration 124, loss = 0.0039228773675858974
iteration 125, loss = 0.0042681326158344746
iteration 126, loss = 0.0038143687415868044
iteration 127, loss = 0.006942445877939463
iteration 128, loss = 0.004821861628443003
iteration 129, loss = 0.007686578668653965
iteration 130, loss = 0.0036269770935177803
iteration 131, loss = 0.006199815310537815
iteration 132, loss = 0.0039006390143185854
iteration 133, loss = 0.003509810194373131
iteration 134, loss = 0.003702225862070918
iteration 135, loss = 0.0034632368478924036
iteration 136, loss = 0.0055188946425914764
iteration 137, loss = 0.004239710513502359
iteration 138, loss = 0.004677047487348318
iteration 139, loss = 0.0033830958418548107
iteration 140, loss = 0.0068748644553124905
iteration 141, loss = 0.0042871092446148396
iteration 142, loss = 0.004763925913721323
iteration 143, loss = 0.0039010909385979176
iteration 144, loss = 0.003502070903778076
iteration 145, loss = 0.0037991278804838657
iteration 146, loss = 0.003580105025321245
iteration 147, loss = 0.00783380214124918
iteration 148, loss = 0.0051325480453670025
iteration 149, loss = 0.006310736760497093
iteration 150, loss = 0.0035308371298015118
iteration 151, loss = 0.0037271755281835794
iteration 152, loss = 0.005125075578689575
iteration 153, loss = 0.0035516300704330206
iteration 154, loss = 0.0034958571195602417
iteration 155, loss = 0.003545447951182723
iteration 156, loss = 0.00456138513982296
iteration 157, loss = 0.004650367423892021
iteration 158, loss = 0.004258754197508097
iteration 159, loss = 0.003662324044853449
iteration 160, loss = 0.004140798002481461
iteration 161, loss = 0.004217931535094976
iteration 162, loss = 0.005636504851281643
iteration 163, loss = 0.0036097541451454163
iteration 164, loss = 0.0044812303967773914
iteration 165, loss = 0.003484700107946992
iteration 166, loss = 0.007037399802356958
iteration 167, loss = 0.004048431757837534
iteration 168, loss = 0.004336819052696228
iteration 169, loss = 0.0038671938236802816
iteration 170, loss = 0.004636730533093214
iteration 171, loss = 0.006051861215382814
iteration 172, loss = 0.0038602687418460846
iteration 173, loss = 0.0036612600088119507
iteration 174, loss = 0.0038652780931442976
iteration 175, loss = 0.003750794567167759
iteration 176, loss = 0.0047619156539440155
iteration 177, loss = 0.003978306427598
iteration 178, loss = 0.004379650112241507
iteration 179, loss = 0.003790441667661071
iteration 180, loss = 0.007261015009135008
iteration 181, loss = 0.004830874036997557
iteration 182, loss = 0.004507340956479311
iteration 183, loss = 0.0036339021753519773
iteration 184, loss = 0.003853749018162489
iteration 185, loss = 0.0037735593505203724
iteration 186, loss = 0.004136422649025917
iteration 187, loss = 0.003603402292355895
iteration 188, loss = 0.0038121191319078207
iteration 189, loss = 0.006161122117191553
iteration 190, loss = 0.005343595519661903
iteration 191, loss = 0.003658554283902049
iteration 192, loss = 0.0034782348666340113
iteration 193, loss = 0.004396483302116394
iteration 194, loss = 0.005508605856448412
iteration 195, loss = 0.0036681732162833214
iteration 196, loss = 0.003553998190909624
iteration 197, loss = 0.0036942155566066504
iteration 198, loss = 0.00362466205842793
iteration 199, loss = 0.003587396815419197
iteration 200, loss = 0.004298392217606306
iteration 201, loss = 0.0034703409764915705
iteration 202, loss = 0.00430572684854269
iteration 203, loss = 0.0040972670540213585
iteration 204, loss = 0.0034477957524359226
iteration 205, loss = 0.003928995691239834
iteration 206, loss = 0.00393329793587327
iteration 207, loss = 0.005608618259429932
iteration 208, loss = 0.007060851436108351
iteration 209, loss = 0.0033411982003599405
iteration 210, loss = 0.004220897797495127
iteration 211, loss = 0.0033557319547981024
iteration 212, loss = 0.0039232224225997925
iteration 213, loss = 0.0067197056487202644
iteration 214, loss = 0.0033680680207908154
iteration 215, loss = 0.004304967820644379
iteration 216, loss = 0.006628639996051788
iteration 217, loss = 0.0035184635780751705
iteration 218, loss = 0.007623613812029362
iteration 219, loss = 0.0034592500887811184
iteration 220, loss = 0.005363122560083866
iteration 221, loss = 0.0041874852031469345
iteration 222, loss = 0.004054007586091757
iteration 223, loss = 0.003521471517160535
iteration 224, loss = 0.004041492007672787
iteration 225, loss = 0.0053472984582185745
iteration 226, loss = 0.005462316330522299
iteration 227, loss = 0.0045408583246171474
iteration 228, loss = 0.006652071140706539
iteration 229, loss = 0.0035886899568140507
iteration 230, loss = 0.004375453572720289
iteration 231, loss = 0.0038578161038458347
iteration 232, loss = 0.0037684570997953415
iteration 233, loss = 0.0035064222756773233
iteration 234, loss = 0.004828992299735546
iteration 235, loss = 0.0035504940897226334
iteration 236, loss = 0.0036049415357410908
iteration 237, loss = 0.004563240334391594
iteration 238, loss = 0.003600060474127531
iteration 239, loss = 0.005296909250319004
iteration 240, loss = 0.0033044260926544666
iteration 241, loss = 0.00410341564565897
iteration 242, loss = 0.004962513688951731
iteration 243, loss = 0.004001053515821695
iteration 244, loss = 0.006657438352704048
iteration 245, loss = 0.003937757574021816
iteration 246, loss = 0.0035150600597262383
iteration 247, loss = 0.0034784104209393263
iteration 248, loss = 0.0036496242973953485
iteration 249, loss = 0.003307273145765066
iteration 250, loss = 0.004438355099409819
iteration 251, loss = 0.0036468785256147385
iteration 252, loss = 0.0050971535965800285
iteration 253, loss = 0.004254315514117479
iteration 254, loss = 0.0033859428949654102
iteration 255, loss = 0.0038439554627984762
iteration 256, loss = 0.005391073878854513
iteration 257, loss = 0.0033930805511772633
iteration 258, loss = 0.006348947528749704
iteration 259, loss = 0.003495994955301285
iteration 260, loss = 0.0032081864774227142
iteration 261, loss = 0.0035692278761416674
iteration 262, loss = 0.005038758739829063
iteration 263, loss = 0.004305192735046148
iteration 264, loss = 0.004798984155058861
iteration 265, loss = 0.003142053261399269
iteration 266, loss = 0.0033768010325729847
iteration 267, loss = 0.005280766636133194
iteration 268, loss = 0.003768316702917218
iteration 269, loss = 0.004737564362585545
iteration 270, loss = 0.004273134283721447
iteration 271, loss = 0.0035584387369453907
iteration 272, loss = 0.003292119363322854
iteration 273, loss = 0.004299221560359001
iteration 274, loss = 0.004441684111952782
iteration 275, loss = 0.0034734306391328573
iteration 276, loss = 0.0037718629464507103
iteration 277, loss = 0.004427457228302956
iteration 278, loss = 0.004945211578160524
iteration 279, loss = 0.004661376588046551
iteration 280, loss = 0.004536752589046955
iteration 281, loss = 0.00451258709654212
iteration 282, loss = 0.0035819963086396456
iteration 283, loss = 0.003875535912811756
iteration 284, loss = 0.004754210356622934
iteration 285, loss = 0.004211104474961758
iteration 286, loss = 0.0035824389196932316
iteration 287, loss = 0.0033067837357521057
iteration 288, loss = 0.005402899347245693
iteration 289, loss = 0.003364108968526125
iteration 290, loss = 0.003517867997288704
iteration 291, loss = 0.004650442861020565
iteration 292, loss = 0.0034221953246742487
iteration 293, loss = 0.004050346091389656
iteration 294, loss = 0.003969885408878326
iteration 295, loss = 0.005363387521356344
iteration 296, loss = 0.0047898069024086
iteration 297, loss = 0.0033188743982464075
iteration 298, loss = 0.006375088356435299
iteration 299, loss = 0.00332815432921052
iteration 300, loss = 0.0056780800223350525
iteration 1, loss = 0.003821608377620578
iteration 2, loss = 0.004155202303081751
iteration 3, loss = 0.003833155380561948
iteration 4, loss = 0.0034770756028592587
iteration 5, loss = 0.003237913129851222
iteration 6, loss = 0.003348503727465868
iteration 7, loss = 0.003125625429674983
iteration 8, loss = 0.0036000176332890987
iteration 9, loss = 0.004830612801015377
iteration 10, loss = 0.003117401385679841
iteration 11, loss = 0.004989654757082462
iteration 12, loss = 0.003902558935806155
iteration 13, loss = 0.0032630194909870625
iteration 14, loss = 0.005038977600634098
iteration 15, loss = 0.003348704893141985
iteration 16, loss = 0.004246138967573643
iteration 17, loss = 0.0034762409050017595
iteration 18, loss = 0.003365837037563324
iteration 19, loss = 0.003476452548056841
iteration 20, loss = 0.0035600478295236826
iteration 21, loss = 0.003487024223431945
iteration 22, loss = 0.0037338663823902607
iteration 23, loss = 0.003395967883989215
iteration 24, loss = 0.003576877061277628
iteration 25, loss = 0.003443727269768715
iteration 26, loss = 0.0031527732498943806
iteration 27, loss = 0.003987463656812906
iteration 28, loss = 0.004290920682251453
iteration 29, loss = 0.004065484739840031
iteration 30, loss = 0.004371962510049343
iteration 31, loss = 0.005662568379193544
iteration 32, loss = 0.004244977608323097
iteration 33, loss = 0.003060710383579135
iteration 34, loss = 0.003729728050529957
iteration 35, loss = 0.003844073275104165
iteration 36, loss = 0.003795729950070381
iteration 37, loss = 0.0030593182891607285
iteration 38, loss = 0.005080231465399265
iteration 39, loss = 0.004342447035014629
iteration 40, loss = 0.0044705830514431
iteration 41, loss = 0.005130766425281763
iteration 42, loss = 0.0039200568571686745
iteration 43, loss = 0.003380273934453726
iteration 44, loss = 0.0035313081461936235
iteration 45, loss = 0.0034505233634263277
iteration 46, loss = 0.0032502899412065744
iteration 47, loss = 0.0037461835891008377
iteration 48, loss = 0.006453309208154678
iteration 49, loss = 0.003322113072499633
iteration 50, loss = 0.0036003096029162407
iteration 51, loss = 0.0036152061074972153
iteration 52, loss = 0.0035497737117111683
iteration 53, loss = 0.0031261153053492308
iteration 54, loss = 0.0031625807750970125
iteration 55, loss = 0.003691824385896325
iteration 56, loss = 0.006898039020597935
iteration 57, loss = 0.0041515277698636055
iteration 58, loss = 0.003862277138978243
iteration 59, loss = 0.003310280153527856
iteration 60, loss = 0.00424591451883316
iteration 61, loss = 0.0039569903165102005
iteration 62, loss = 0.003323169192299247
iteration 63, loss = 0.0033511461224406958
iteration 64, loss = 0.0031740665435791016
iteration 65, loss = 0.0056333839893341064
iteration 66, loss = 0.004870050121098757
iteration 67, loss = 0.0038862470537424088
iteration 68, loss = 0.003363784169778228
iteration 69, loss = 0.004236930515617132
iteration 70, loss = 0.0031149601563811302
iteration 71, loss = 0.00363514618948102
iteration 72, loss = 0.0032857577316462994
iteration 73, loss = 0.00491353077813983
iteration 74, loss = 0.003335379296913743
iteration 75, loss = 0.003795896889641881
iteration 76, loss = 0.0032814163714647293
iteration 77, loss = 0.0031214430928230286
iteration 78, loss = 0.003088433528319001
iteration 79, loss = 0.005840621422976255
iteration 80, loss = 0.00493191834539175
iteration 81, loss = 0.003530271351337433
iteration 82, loss = 0.005390463396906853
iteration 83, loss = 0.003617526963353157
iteration 84, loss = 0.0038166120648384094
iteration 85, loss = 0.0033473146613687277
iteration 86, loss = 0.004870276432484388
iteration 87, loss = 0.006560286972671747
iteration 88, loss = 0.0034159987699240446
iteration 89, loss = 0.0039787860587239265
iteration 90, loss = 0.004397726152092218
iteration 91, loss = 0.003363944124430418
iteration 92, loss = 0.004247360862791538
iteration 93, loss = 0.0051306504756212234
iteration 94, loss = 0.005513326730579138
iteration 95, loss = 0.004158897791057825
iteration 96, loss = 0.0035500640515238047
iteration 97, loss = 0.0034536579623818398
iteration 98, loss = 0.004316374193876982
iteration 99, loss = 0.0044205645099282265
iteration 100, loss = 0.003588989842683077
iteration 101, loss = 0.003502060892060399
iteration 102, loss = 0.0030847222078591585
iteration 103, loss = 0.003539618104696274
iteration 104, loss = 0.004770078230649233
iteration 105, loss = 0.005773297976702452
iteration 106, loss = 0.004194916691631079
iteration 107, loss = 0.003703352529555559
iteration 108, loss = 0.006107420660555363
iteration 109, loss = 0.0034795517567545176
iteration 110, loss = 0.0031813858076930046
iteration 111, loss = 0.004026750102639198
iteration 112, loss = 0.0031733796931803226
iteration 113, loss = 0.003476192709058523
iteration 114, loss = 0.004814812447875738
iteration 115, loss = 0.004568339791148901
iteration 116, loss = 0.003527668071910739
iteration 117, loss = 0.0033815724309533834
iteration 118, loss = 0.006296833045780659
iteration 119, loss = 0.003263532416895032
iteration 120, loss = 0.0036247617099434137
iteration 121, loss = 0.003472069976851344
iteration 122, loss = 0.003479704260826111
iteration 123, loss = 0.0038651684299111366
iteration 124, loss = 0.003060610732063651
iteration 125, loss = 0.003374120220541954
iteration 126, loss = 0.0038220889400690794
iteration 127, loss = 0.003347120713442564
iteration 128, loss = 0.003446451388299465
iteration 129, loss = 0.005916667636483908
iteration 130, loss = 0.0035623395815491676
iteration 131, loss = 0.003883732482790947
iteration 132, loss = 0.003005341626703739
iteration 133, loss = 0.003303188132122159
iteration 134, loss = 0.0037546709645539522
iteration 135, loss = 0.003971252124756575
iteration 136, loss = 0.003718620166182518
iteration 137, loss = 0.0036273293662816286
iteration 138, loss = 0.0035654748789966106
iteration 139, loss = 0.003833761438727379
iteration 140, loss = 0.004760986194014549
iteration 141, loss = 0.004446625243872404
iteration 142, loss = 0.0035661710426211357
iteration 143, loss = 0.0032827849499881268
iteration 144, loss = 0.003738370258361101
iteration 145, loss = 0.00596838491037488
iteration 146, loss = 0.004481377545744181
iteration 147, loss = 0.005063097923994064
iteration 148, loss = 0.0037446029018610716
iteration 149, loss = 0.003151106182485819
iteration 150, loss = 0.0037096505984663963
iteration 151, loss = 0.002976363757625222
iteration 152, loss = 0.003381290938705206
iteration 153, loss = 0.004445069469511509
iteration 154, loss = 0.0034224020782858133
iteration 155, loss = 0.003137564519420266
iteration 156, loss = 0.003400471294298768
iteration 157, loss = 0.002962205559015274
iteration 158, loss = 0.0037561003118753433
iteration 159, loss = 0.00663029495626688
iteration 160, loss = 0.00379241188056767
iteration 161, loss = 0.0034728320315480232
iteration 162, loss = 0.0042503648437559605
iteration 163, loss = 0.003947874065488577
iteration 164, loss = 0.003580625168979168
iteration 165, loss = 0.0029919592197984457
iteration 166, loss = 0.0029730377718806267
iteration 167, loss = 0.003523851279169321
iteration 168, loss = 0.003771754214540124
iteration 169, loss = 0.003250258509069681
iteration 170, loss = 0.002951061585918069
iteration 171, loss = 0.0031129121780395508
iteration 172, loss = 0.0033553813118487597
iteration 173, loss = 0.003328759456053376
iteration 174, loss = 0.0029182981234043837
iteration 175, loss = 0.00363371055573225
iteration 176, loss = 0.003087651915848255
iteration 177, loss = 0.0045315600000321865
iteration 178, loss = 0.0032751408871263266
iteration 179, loss = 0.006026994902640581
iteration 180, loss = 0.003718750551342964
iteration 181, loss = 0.0033000363036990166
iteration 182, loss = 0.004586176481097937
iteration 183, loss = 0.003977354150265455
iteration 184, loss = 0.003933754283934832
iteration 185, loss = 0.0035280862357467413
iteration 186, loss = 0.003262781770899892
iteration 187, loss = 0.005802477709949017
iteration 188, loss = 0.004920511040836573
iteration 189, loss = 0.004608881659805775
iteration 190, loss = 0.0035989778116345406
iteration 191, loss = 0.0033782096579670906
iteration 192, loss = 0.0033722613006830215
iteration 193, loss = 0.003532923525199294
iteration 194, loss = 0.003444412723183632
iteration 195, loss = 0.0031251213513314724
iteration 196, loss = 0.004342632368206978
iteration 197, loss = 0.003855921560898423
iteration 198, loss = 0.003696674481034279
iteration 199, loss = 0.0038372830022126436
iteration 200, loss = 0.00284315156750381
iteration 201, loss = 0.004422181285917759
iteration 202, loss = 0.004253254272043705
iteration 203, loss = 0.003196505829691887
iteration 204, loss = 0.0032828848343342543
iteration 205, loss = 0.0039821285754442215
iteration 206, loss = 0.0028830633964389563
iteration 207, loss = 0.0031378872226923704
iteration 208, loss = 0.0035170118790119886
iteration 209, loss = 0.0041542574763298035
iteration 210, loss = 0.0030944938771426678
iteration 211, loss = 0.003008908126503229
iteration 212, loss = 0.0030406713485717773
iteration 213, loss = 0.004108179826289415
iteration 214, loss = 0.005670547019690275
iteration 215, loss = 0.0031331146601587534
iteration 216, loss = 0.0032585500739514828
iteration 217, loss = 0.003496066899970174
iteration 218, loss = 0.00465033482760191
iteration 219, loss = 0.0031541711650788784
iteration 220, loss = 0.0033873161301016808
iteration 221, loss = 0.003211198141798377
iteration 222, loss = 0.002993440255522728
iteration 223, loss = 0.0028022783808410168
iteration 224, loss = 0.0032825542148202658
iteration 225, loss = 0.0033342866227030754
iteration 226, loss = 0.003642174880951643
iteration 227, loss = 0.0035336706787347794
iteration 228, loss = 0.0030610496178269386
iteration 229, loss = 0.0029918754007667303
iteration 230, loss = 0.0032273861579596996
iteration 231, loss = 0.0033447768073529005
iteration 232, loss = 0.00383324665017426
iteration 233, loss = 0.0028342450968921185
iteration 234, loss = 0.0029003529343754053
iteration 235, loss = 0.0031928413081914186
iteration 236, loss = 0.0029280739836394787
iteration 237, loss = 0.003536066971719265
iteration 238, loss = 0.005886981729418039
iteration 239, loss = 0.0036062439903616905
iteration 240, loss = 0.0054502468556165695
iteration 241, loss = 0.00301365926861763
iteration 242, loss = 0.0031272259075194597
iteration 243, loss = 0.003804704872891307
iteration 244, loss = 0.0038650943897664547
iteration 245, loss = 0.0034552800934761763
iteration 246, loss = 0.004366247449070215
iteration 247, loss = 0.0038563725538551807
iteration 248, loss = 0.0037700533866882324
iteration 249, loss = 0.003518869634717703
iteration 250, loss = 0.005553911905735731
iteration 251, loss = 0.00462735490873456
iteration 252, loss = 0.0032685936894267797
iteration 253, loss = 0.0033679422922432423
iteration 254, loss = 0.002972980495542288
iteration 255, loss = 0.003298701485618949
iteration 256, loss = 0.003821966703981161
iteration 257, loss = 0.002957205520942807
iteration 258, loss = 0.0032309459056705236
iteration 259, loss = 0.0037237145006656647
iteration 260, loss = 0.00333924381993711
iteration 261, loss = 0.0033411579206585884
iteration 262, loss = 0.006470248103141785
iteration 263, loss = 0.0030405751895159483
iteration 264, loss = 0.00372413732111454
iteration 265, loss = 0.003836004761978984
iteration 266, loss = 0.0030646384693682194
iteration 267, loss = 0.005024505779147148
iteration 268, loss = 0.004093250259757042
iteration 269, loss = 0.005248488392680883
iteration 270, loss = 0.004159155767410994
iteration 271, loss = 0.003395722946152091
iteration 272, loss = 0.00321371853351593
iteration 273, loss = 0.0032092540059238672
iteration 274, loss = 0.00542090367525816
iteration 275, loss = 0.003950029611587524
iteration 276, loss = 0.004436798859387636
iteration 277, loss = 0.004123344551771879
iteration 278, loss = 0.0036871610209345818
iteration 279, loss = 0.003240677062422037
iteration 280, loss = 0.0032169907353818417
iteration 281, loss = 0.0031654760241508484
iteration 282, loss = 0.003309108316898346
iteration 283, loss = 0.0029536315705627203
iteration 284, loss = 0.0032839602790772915
iteration 285, loss = 0.0035453338641673326
iteration 286, loss = 0.0034291658084839582
iteration 287, loss = 0.003997764550149441
iteration 288, loss = 0.004577258601784706
iteration 289, loss = 0.0034560186322778463
iteration 290, loss = 0.0031924843788146973
iteration 291, loss = 0.005723398178815842
iteration 292, loss = 0.00554149504750967
iteration 293, loss = 0.0028319424018263817
iteration 294, loss = 0.003955019637942314
iteration 295, loss = 0.0027563937474042177
iteration 296, loss = 0.0031582980882376432
iteration 297, loss = 0.0034101468045264482
iteration 298, loss = 0.0031316292006522417
iteration 299, loss = 0.002892759395763278
iteration 300, loss = 0.0035569926258176565
iteration 1, loss = 0.0029707713983953
iteration 2, loss = 0.003434143727645278
iteration 3, loss = 0.0032841572538018227
iteration 4, loss = 0.0027734488248825073
iteration 5, loss = 0.0028672413900494576
iteration 6, loss = 0.0038081309758126736
iteration 7, loss = 0.002745768753811717
iteration 8, loss = 0.002763582393527031
iteration 9, loss = 0.004288404248654842
iteration 10, loss = 0.002965169958770275
iteration 11, loss = 0.0030761300586163998
iteration 12, loss = 0.0030952850356698036
iteration 13, loss = 0.004040443804115057
iteration 14, loss = 0.0030479878187179565
iteration 15, loss = 0.0061551956459879875
iteration 16, loss = 0.004166224040091038
iteration 17, loss = 0.003123830072581768
iteration 18, loss = 0.0034567664843052626
iteration 19, loss = 0.004010479897260666
iteration 20, loss = 0.003408853430300951
iteration 21, loss = 0.003383476287126541
iteration 22, loss = 0.003739299951121211
iteration 23, loss = 0.004701465368270874
iteration 24, loss = 0.004051376599818468
iteration 25, loss = 0.0055357422679662704
iteration 26, loss = 0.003917118068784475
iteration 27, loss = 0.0035535243805497885
iteration 28, loss = 0.004088977817445993
iteration 29, loss = 0.0026852027513086796
iteration 30, loss = 0.003227276261895895
iteration 31, loss = 0.0030586891807615757
iteration 32, loss = 0.0036993848625570536
iteration 33, loss = 0.002919774502515793
iteration 34, loss = 0.0028869446832686663
iteration 35, loss = 0.003226855769753456
iteration 36, loss = 0.0035714933183044195
iteration 37, loss = 0.003142424626275897
iteration 38, loss = 0.0035651179496198893
iteration 39, loss = 0.0028230086900293827
iteration 40, loss = 0.0031443077605217695
iteration 41, loss = 0.0029608020558953285
iteration 42, loss = 0.002970603061839938
iteration 43, loss = 0.0031246007420122623
iteration 44, loss = 0.005539285019040108
iteration 45, loss = 0.002994310576468706
iteration 46, loss = 0.0029518338851630688
iteration 47, loss = 0.0029179395642131567
iteration 48, loss = 0.002947031520307064
iteration 49, loss = 0.003569594584405422
iteration 50, loss = 0.0035963961854577065
iteration 51, loss = 0.004049885086715221
iteration 52, loss = 0.004170288797467947
iteration 53, loss = 0.003120248205959797
iteration 54, loss = 0.0036559090949594975
iteration 55, loss = 0.0031239651143550873
iteration 56, loss = 0.003517457051202655
iteration 57, loss = 0.0029843817465007305
iteration 58, loss = 0.003311081789433956
iteration 59, loss = 0.00270790490321815
iteration 60, loss = 0.0028959782794117928
iteration 61, loss = 0.002870462602004409
iteration 62, loss = 0.0029764275532215834
iteration 63, loss = 0.0031830298248678446
iteration 64, loss = 0.00306316907517612
iteration 65, loss = 0.002900401595979929
iteration 66, loss = 0.003218050580471754
iteration 67, loss = 0.003302705939859152
iteration 68, loss = 0.0033844138961285353
iteration 69, loss = 0.0029982172418385744
iteration 70, loss = 0.0026577659882605076
iteration 71, loss = 0.0031249825842678547
iteration 72, loss = 0.0026331099215894938
iteration 73, loss = 0.0034563615918159485
iteration 74, loss = 0.0028622113168239594
iteration 75, loss = 0.0034999498166143894
iteration 76, loss = 0.0036385483108460903
iteration 77, loss = 0.0027987021021544933
iteration 78, loss = 0.003170556854456663
iteration 79, loss = 0.0037307078018784523
iteration 80, loss = 0.002781079150736332
iteration 81, loss = 0.006437033414840698
iteration 82, loss = 0.00307370419614017
iteration 83, loss = 0.003295309841632843
iteration 84, loss = 0.005074059125036001
iteration 85, loss = 0.002845410956069827
iteration 86, loss = 0.004409467801451683
iteration 87, loss = 0.003843128215521574
iteration 88, loss = 0.0036146247293800116
iteration 89, loss = 0.0031042923219501972
iteration 90, loss = 0.0027868126053363085
iteration 91, loss = 0.003531173337250948
iteration 92, loss = 0.0049422006122767925
iteration 93, loss = 0.0027833320200443268
iteration 94, loss = 0.0037373092491179705
iteration 95, loss = 0.002916488330811262
iteration 96, loss = 0.0031410930678248405
iteration 97, loss = 0.0028966276440769434
iteration 98, loss = 0.0032220559660345316
iteration 99, loss = 0.003350467886775732
iteration 100, loss = 0.0031298818066716194
iteration 101, loss = 0.004009569063782692
iteration 102, loss = 0.0034176509361714125
iteration 103, loss = 0.0034804672468453646
iteration 104, loss = 0.0031727785244584084
iteration 105, loss = 0.003965146839618683
iteration 106, loss = 0.0032598739489912987
iteration 107, loss = 0.0034699959214776754
iteration 108, loss = 0.0036947079934179783
iteration 109, loss = 0.0034293534699827433
iteration 110, loss = 0.0035245916806161404
iteration 111, loss = 0.0030168413650244474
iteration 112, loss = 0.0027212919667363167
iteration 113, loss = 0.003225794294849038
iteration 114, loss = 0.00364152854308486
iteration 115, loss = 0.003463724162429571
iteration 116, loss = 0.003569343127310276
iteration 117, loss = 0.0027759848162531853
iteration 118, loss = 0.002857604995369911
iteration 119, loss = 0.00365525227971375
iteration 120, loss = 0.005577919073402882
iteration 121, loss = 0.0030666356906294823
iteration 122, loss = 0.003615657100453973
iteration 123, loss = 0.003483476582914591
iteration 124, loss = 0.003216841723769903
iteration 125, loss = 0.0034255876671522856
iteration 126, loss = 0.0027320676017552614
iteration 127, loss = 0.003711079014465213
iteration 128, loss = 0.0037152585573494434
iteration 129, loss = 0.0036440230906009674
iteration 130, loss = 0.004329156130552292
iteration 131, loss = 0.002636674093082547
iteration 132, loss = 0.0031078755855560303
iteration 133, loss = 0.0026688214857131243
iteration 134, loss = 0.002684369683265686
iteration 135, loss = 0.0037738210521638393
iteration 136, loss = 0.0031959975603967905
iteration 137, loss = 0.003124833106994629
iteration 138, loss = 0.0032127746380865574
iteration 139, loss = 0.002806165488436818
iteration 140, loss = 0.003600551513954997
iteration 141, loss = 0.002864136593416333
iteration 142, loss = 0.002892192918807268
iteration 143, loss = 0.003226396394893527
iteration 144, loss = 0.002870590426027775
iteration 145, loss = 0.0027254833839833736
iteration 146, loss = 0.0032070132438093424
iteration 147, loss = 0.002733095781877637
iteration 148, loss = 0.003296000650152564
iteration 149, loss = 0.0035307644866406918
iteration 150, loss = 0.0029775435104966164
iteration 151, loss = 0.002932339208200574
iteration 152, loss = 0.002818217035382986
iteration 153, loss = 0.002801443450152874
iteration 154, loss = 0.0029212746303528547
iteration 155, loss = 0.003603652585297823
iteration 156, loss = 0.0029245486948639154
iteration 157, loss = 0.0031429994851350784
iteration 158, loss = 0.0028228138107806444
iteration 159, loss = 0.0032085534185171127
iteration 160, loss = 0.002840714994817972
iteration 161, loss = 0.0034308461472392082
iteration 162, loss = 0.003043490694835782
iteration 163, loss = 0.0025602581445127726
iteration 164, loss = 0.003383365459740162
iteration 165, loss = 0.005318487528711557
iteration 166, loss = 0.005404691677540541
iteration 167, loss = 0.0037105062510818243
iteration 168, loss = 0.0027983065228909254
iteration 169, loss = 0.0032440328504890203
iteration 170, loss = 0.0026938822120428085
iteration 171, loss = 0.005533195100724697
iteration 172, loss = 0.003060832154005766
iteration 173, loss = 0.003111028578132391
iteration 174, loss = 0.0027525010518729687
iteration 175, loss = 0.002933552023023367
iteration 176, loss = 0.0029205926693975925
iteration 177, loss = 0.00378526677377522
iteration 178, loss = 0.003308469196781516
iteration 179, loss = 0.0036204869393259287
iteration 180, loss = 0.002653472125530243
iteration 181, loss = 0.0026826299726963043
iteration 182, loss = 0.002713293069973588
iteration 183, loss = 0.002825576113536954
iteration 184, loss = 0.004381280392408371
iteration 185, loss = 0.0030437675304710865
iteration 186, loss = 0.0033344426192343235
iteration 187, loss = 0.0035753280390053988
iteration 188, loss = 0.002984956605359912
iteration 189, loss = 0.003395460080355406
iteration 190, loss = 0.003332029329612851
iteration 191, loss = 0.002995170885697007
iteration 192, loss = 0.002529098419472575
iteration 193, loss = 0.0029518750961869955
iteration 194, loss = 0.0041103241965174675
iteration 195, loss = 0.002693751361221075
iteration 196, loss = 0.0037537706084549427
iteration 197, loss = 0.0025980803184211254
iteration 198, loss = 0.0025806191843003035
iteration 199, loss = 0.002901500090956688
iteration 200, loss = 0.0027619460597634315
iteration 201, loss = 0.00275395717471838
iteration 202, loss = 0.004461212083697319
iteration 203, loss = 0.004263035021722317
iteration 204, loss = 0.002871209057047963
iteration 205, loss = 0.0025656339712440968
iteration 206, loss = 0.002601382788270712
iteration 207, loss = 0.002793370746076107
iteration 208, loss = 0.0032975978683680296
iteration 209, loss = 0.003949727863073349
iteration 210, loss = 0.002986572915688157
iteration 211, loss = 0.0034147650003433228
iteration 212, loss = 0.002678011544048786
iteration 213, loss = 0.0034846593625843525
iteration 214, loss = 0.0036093108355998993
iteration 215, loss = 0.0029362419154495
iteration 216, loss = 0.0025924905203282833
iteration 217, loss = 0.0030813138000667095
iteration 218, loss = 0.0028859113808721304
iteration 219, loss = 0.002965192310512066
iteration 220, loss = 0.0030822493135929108
iteration 221, loss = 0.002725708531215787
iteration 222, loss = 0.0027747699059545994
iteration 223, loss = 0.003184080822393298
iteration 224, loss = 0.003913929220288992
iteration 225, loss = 0.0027642829809337854
iteration 226, loss = 0.004031041637063026
iteration 227, loss = 0.002689132932573557
iteration 228, loss = 0.0035716905258595943
iteration 229, loss = 0.0029186452738940716
iteration 230, loss = 0.002474708715453744
iteration 231, loss = 0.004972122143954039
iteration 232, loss = 0.005251733586192131
iteration 233, loss = 0.003775295102968812
iteration 234, loss = 0.0030169927049428225
iteration 235, loss = 0.002720818156376481
iteration 236, loss = 0.002548239193856716
iteration 237, loss = 0.00373687781393528
iteration 238, loss = 0.003040468553081155
iteration 239, loss = 0.0027664173394441605
iteration 240, loss = 0.0025822946336120367
iteration 241, loss = 0.002515745349228382
iteration 242, loss = 0.0029945578426122665
iteration 243, loss = 0.0035474305041134357
iteration 244, loss = 0.00275855278596282
iteration 245, loss = 0.002974884118884802
iteration 246, loss = 0.0026663686148822308
iteration 247, loss = 0.0048536392860114574
iteration 248, loss = 0.0025077485479414463
iteration 249, loss = 0.0028886483050882816
iteration 250, loss = 0.0034360368736088276
iteration 251, loss = 0.0031807066407054663
iteration 252, loss = 0.003666311502456665
iteration 253, loss = 0.004789260216057301
iteration 254, loss = 0.0028936746530234814
iteration 255, loss = 0.005042326170951128
iteration 256, loss = 0.0025150340516120195
iteration 257, loss = 0.005886886268854141
iteration 258, loss = 0.004353174474090338
iteration 259, loss = 0.00293769221752882
iteration 260, loss = 0.003620864823460579
iteration 261, loss = 0.0027839180547744036
iteration 262, loss = 0.005477564409375191
iteration 263, loss = 0.002954933326691389
iteration 264, loss = 0.0026343241333961487
iteration 265, loss = 0.0030438301619142294
iteration 266, loss = 0.0029101166874170303
iteration 267, loss = 0.0039983526803553104
iteration 268, loss = 0.0025141597725450993
iteration 269, loss = 0.003294908907264471
iteration 270, loss = 0.002595443744212389
iteration 271, loss = 0.004801045171916485
iteration 272, loss = 0.003197641111910343
iteration 273, loss = 0.0032403976656496525
iteration 274, loss = 0.0027097458951175213
iteration 275, loss = 0.0028884790372103453
iteration 276, loss = 0.003074479289352894
iteration 277, loss = 0.002556772204115987
iteration 278, loss = 0.0035551823675632477
iteration 279, loss = 0.003304976737126708
iteration 280, loss = 0.0032889721915125847
iteration 281, loss = 0.0025543896481394768
iteration 282, loss = 0.0040800608694553375
iteration 283, loss = 0.003058717818930745
iteration 284, loss = 0.002775375498458743
iteration 285, loss = 0.0027931469958275557
iteration 286, loss = 0.0027141424361616373
iteration 287, loss = 0.002989916130900383
iteration 288, loss = 0.0026975558139383793
iteration 289, loss = 0.004691372625529766
iteration 290, loss = 0.002748367842286825
iteration 291, loss = 0.0028810000512748957
iteration 292, loss = 0.005187743343412876
iteration 293, loss = 0.0036843158304691315
iteration 294, loss = 0.002546505769714713
iteration 295, loss = 0.00244649569503963
iteration 296, loss = 0.002827353309839964
iteration 297, loss = 0.005402037408202887
iteration 298, loss = 0.003020708682015538
iteration 299, loss = 0.003548465669155121
iteration 300, loss = 0.002633071271702647
iteration 1, loss = 0.0028513718862086535
iteration 2, loss = 0.002852995414286852
iteration 3, loss = 0.002611024770885706
iteration 4, loss = 0.0025724396109580994
iteration 5, loss = 0.004715680610388517
iteration 6, loss = 0.0038469473365694284
iteration 7, loss = 0.0027615695726126432
iteration 8, loss = 0.0032358719035983086
iteration 9, loss = 0.0028421662282198668
iteration 10, loss = 0.0028030939865857363
iteration 11, loss = 0.002764363307505846
iteration 12, loss = 0.0032327710650861263
iteration 13, loss = 0.0023807641118764877
iteration 14, loss = 0.0029833465814590454
iteration 15, loss = 0.002876461483538151
iteration 16, loss = 0.0026300493627786636
iteration 17, loss = 0.0030152411200106144
iteration 18, loss = 0.0029449190478771925
iteration 19, loss = 0.0029394016601145267
iteration 20, loss = 0.0037890844978392124
iteration 21, loss = 0.003043846460059285
iteration 22, loss = 0.0032807113602757454
iteration 23, loss = 0.0023654892574995756
iteration 24, loss = 0.002548025920987129
iteration 25, loss = 0.0030252595897763968
iteration 26, loss = 0.002769632963463664
iteration 27, loss = 0.0034180537331849337
iteration 28, loss = 0.0027768516447395086
iteration 29, loss = 0.004994642920792103
iteration 30, loss = 0.003892213571816683
iteration 31, loss = 0.003068877151235938
iteration 32, loss = 0.0037623834796249866
iteration 33, loss = 0.003247840330004692
iteration 34, loss = 0.0025656884536147118
iteration 35, loss = 0.002583526773378253
iteration 36, loss = 0.0033993306569755077
iteration 37, loss = 0.003428403288125992
iteration 38, loss = 0.002434411784633994
iteration 39, loss = 0.002720507327467203
iteration 40, loss = 0.0031583395320922136
iteration 41, loss = 0.004339270293712616
iteration 42, loss = 0.0027706101536750793
iteration 43, loss = 0.004905533045530319
iteration 44, loss = 0.0029681336600333452
iteration 45, loss = 0.0025240429677069187
iteration 46, loss = 0.002579346764832735
iteration 47, loss = 0.0025692451745271683
iteration 48, loss = 0.004301678854972124
iteration 49, loss = 0.00453069107607007
iteration 50, loss = 0.0027702886145561934
iteration 51, loss = 0.0027361700776964426
iteration 52, loss = 0.0024911558721214533
iteration 53, loss = 0.002645817119628191
iteration 54, loss = 0.002719420474022627
iteration 55, loss = 0.0027491033542901278
iteration 56, loss = 0.0024413680657744408
iteration 57, loss = 0.002816453343257308
iteration 58, loss = 0.0029356577433645725
iteration 59, loss = 0.0031629500444978476
iteration 60, loss = 0.0026067167054861784
iteration 61, loss = 0.0026794536970555782
iteration 62, loss = 0.004592658020555973
iteration 63, loss = 0.0029291037935763597
iteration 64, loss = 0.002713380381464958
iteration 65, loss = 0.0028605451807379723
iteration 66, loss = 0.0027890801429748535
iteration 67, loss = 0.00313102756626904
iteration 68, loss = 0.002637556754052639
iteration 69, loss = 0.002590036718174815
iteration 70, loss = 0.002529772464185953
iteration 71, loss = 0.002661597216501832
iteration 72, loss = 0.0026446280535310507
iteration 73, loss = 0.002883310429751873
iteration 74, loss = 0.0030547704081982374
iteration 75, loss = 0.0027865872252732515
iteration 76, loss = 0.002784252166748047
iteration 77, loss = 0.0028869719244539738
iteration 78, loss = 0.0034254915080964565
iteration 79, loss = 0.002482227049767971
iteration 80, loss = 0.0027832030318677425
iteration 81, loss = 0.003961511421948671
iteration 82, loss = 0.0036218841560184956
iteration 83, loss = 0.0030533187091350555
iteration 84, loss = 0.005035783629864454
iteration 85, loss = 0.004314920399338007
iteration 86, loss = 0.005010396242141724
iteration 87, loss = 0.003888237988576293
iteration 88, loss = 0.0031887036748230457
iteration 89, loss = 0.0041820029728114605
iteration 90, loss = 0.0035290035884827375
iteration 91, loss = 0.0032042074017226696
iteration 92, loss = 0.002754735993221402
iteration 93, loss = 0.0031023805495351553
iteration 94, loss = 0.0026697705034166574
iteration 95, loss = 0.002675415016710758
iteration 96, loss = 0.002728131366893649
iteration 97, loss = 0.0032322006300091743
iteration 98, loss = 0.0034112941939383745
iteration 99, loss = 0.003190606599673629
iteration 100, loss = 0.0027881995774805546
iteration 101, loss = 0.0024385934229940176
iteration 102, loss = 0.002895538927987218
iteration 103, loss = 0.0029817514587193727
iteration 104, loss = 0.0028890352696180344
iteration 105, loss = 0.002461165189743042
iteration 106, loss = 0.0035711226519197226
iteration 107, loss = 0.0024255497846752405
iteration 108, loss = 0.002823022659868002
iteration 109, loss = 0.0025897312443703413
iteration 110, loss = 0.003361478680744767
iteration 111, loss = 0.0035637079272419214
iteration 112, loss = 0.0024875458329916
iteration 113, loss = 0.0030332594178617
iteration 114, loss = 0.002998709212988615
iteration 115, loss = 0.003123281057924032
iteration 116, loss = 0.0029696542769670486
iteration 117, loss = 0.003971978556364775
iteration 118, loss = 0.0023348527029156685
iteration 119, loss = 0.0027559278532862663
iteration 120, loss = 0.0029557719826698303
iteration 121, loss = 0.0024104181211441755
iteration 122, loss = 0.002624813700094819
iteration 123, loss = 0.002593210432678461
iteration 124, loss = 0.002404704922810197
iteration 125, loss = 0.002672312781214714
iteration 126, loss = 0.0024508077185600996
iteration 127, loss = 0.0023884554393589497
iteration 128, loss = 0.0026644477620720863
iteration 129, loss = 0.002997144591063261
iteration 130, loss = 0.002436612732708454
iteration 131, loss = 0.003056448418647051
iteration 132, loss = 0.002939431229606271
iteration 133, loss = 0.0026239610742777586
iteration 134, loss = 0.003041934221982956
iteration 135, loss = 0.0026569217443466187
iteration 136, loss = 0.002648786874487996
iteration 137, loss = 0.0022534390445798635
iteration 138, loss = 0.0030877282842993736
iteration 139, loss = 0.002930198796093464
iteration 140, loss = 0.0024027202744036913
iteration 141, loss = 0.002684876788407564
iteration 142, loss = 0.002382401144132018
iteration 143, loss = 0.002582957036793232
iteration 144, loss = 0.003073322819545865
iteration 145, loss = 0.0028841160237789154
iteration 146, loss = 0.0024230482522398233
iteration 147, loss = 0.002585302572697401
iteration 148, loss = 0.0027171364054083824
iteration 149, loss = 0.0027154902927577496
iteration 150, loss = 0.002574258716776967
iteration 151, loss = 0.003205661429092288
iteration 152, loss = 0.002954889787361026
iteration 153, loss = 0.0029583610594272614
iteration 154, loss = 0.0026102212723344564
iteration 155, loss = 0.002706432482227683
iteration 156, loss = 0.002705358900129795
iteration 157, loss = 0.002353097777813673
iteration 158, loss = 0.002400765661150217
iteration 159, loss = 0.003076249500736594
iteration 160, loss = 0.002577940234914422
iteration 161, loss = 0.0033482899889349937
iteration 162, loss = 0.003015324007719755
iteration 163, loss = 0.0025131849106401205
iteration 164, loss = 0.0025118349585682154
iteration 165, loss = 0.0026161705609411
iteration 166, loss = 0.003611087566241622
iteration 167, loss = 0.0025003787595778704
iteration 168, loss = 0.002800313988700509
iteration 169, loss = 0.003906484227627516
iteration 170, loss = 0.0029489509761333466
iteration 171, loss = 0.002517984015867114
iteration 172, loss = 0.002600838430225849
iteration 173, loss = 0.003611936466768384
iteration 174, loss = 0.005225374363362789
iteration 175, loss = 0.0032567251473665237
iteration 176, loss = 0.002555779879912734
iteration 177, loss = 0.0034442362375557423
iteration 178, loss = 0.0027674061711877584
iteration 179, loss = 0.00334957055747509
iteration 180, loss = 0.0043555027805268764
iteration 181, loss = 0.002430106047540903
iteration 182, loss = 0.0032010942231863737
iteration 183, loss = 0.002880321815609932
iteration 184, loss = 0.0024324501864612103
iteration 185, loss = 0.003278882708400488
iteration 186, loss = 0.002507002092897892
iteration 187, loss = 0.002549669938161969
iteration 188, loss = 0.0031127925030887127
iteration 189, loss = 0.002542841946706176
iteration 190, loss = 0.002320091240108013
iteration 191, loss = 0.0027132704854011536
iteration 192, loss = 0.0025042216293513775
iteration 193, loss = 0.00292292982339859
iteration 194, loss = 0.003623703494668007
iteration 195, loss = 0.00298006902448833
iteration 196, loss = 0.002694703172892332
iteration 197, loss = 0.0026223664171993732
iteration 198, loss = 0.0027854025829583406
iteration 199, loss = 0.002369469963014126
iteration 200, loss = 0.0022251023910939693
iteration 201, loss = 0.0035615076776593924
iteration 202, loss = 0.0023237469140440226
iteration 203, loss = 0.00240128580480814
iteration 204, loss = 0.004524701740592718
iteration 205, loss = 0.0023964066058397293
iteration 206, loss = 0.002296086400747299
iteration 207, loss = 0.0025124624371528625
iteration 208, loss = 0.003177737584337592
iteration 209, loss = 0.00270460220053792
iteration 210, loss = 0.00254999240860343
iteration 211, loss = 0.00239991070702672
iteration 212, loss = 0.0036081145517528057
iteration 213, loss = 0.0024283831007778645
iteration 214, loss = 0.002710771281272173
iteration 215, loss = 0.0037869710940867662
iteration 216, loss = 0.002946388442069292
iteration 217, loss = 0.003136714920401573
iteration 218, loss = 0.0028450789395719767
iteration 219, loss = 0.002703955629840493
iteration 220, loss = 0.002480082679539919
iteration 221, loss = 0.002582868793979287
iteration 222, loss = 0.002500994596630335
iteration 223, loss = 0.002581861102953553
iteration 224, loss = 0.0022428499069064856
iteration 225, loss = 0.004236533772200346
iteration 226, loss = 0.0028580231592059135
iteration 227, loss = 0.0023257483262568712
iteration 228, loss = 0.0033877629321068525
iteration 229, loss = 0.0026273182593286037
iteration 230, loss = 0.0025309801567345858
iteration 231, loss = 0.002566112205386162
iteration 232, loss = 0.003072229214012623
iteration 233, loss = 0.0029374039731919765
iteration 234, loss = 0.0023943937849253416
iteration 235, loss = 0.0027104129549115896
iteration 236, loss = 0.002432004315778613
iteration 237, loss = 0.002336266450583935
iteration 238, loss = 0.002388040069490671
iteration 239, loss = 0.0023018312640488148
iteration 240, loss = 0.00461891433224082
iteration 241, loss = 0.0025677180383354425
iteration 242, loss = 0.0023532717023044825
iteration 243, loss = 0.002425633603706956
iteration 244, loss = 0.002564114984124899
iteration 245, loss = 0.0025728971231728792
iteration 246, loss = 0.002678156131878495
iteration 247, loss = 0.0025618132203817368
iteration 248, loss = 0.003263980383053422
iteration 249, loss = 0.0023128597531467676
iteration 250, loss = 0.0025685494765639305
iteration 251, loss = 0.0027334964834153652
iteration 252, loss = 0.002931894501671195
iteration 253, loss = 0.002884962595999241
iteration 254, loss = 0.004639795050024986
iteration 255, loss = 0.002983412705361843
iteration 256, loss = 0.0030000123661011457
iteration 257, loss = 0.0023104632273316383
iteration 258, loss = 0.002650031354278326
iteration 259, loss = 0.0026629387866705656
iteration 260, loss = 0.0026502530090510845
iteration 261, loss = 0.002422627527266741
iteration 262, loss = 0.0022402380127459764
iteration 263, loss = 0.002728782594203949
iteration 264, loss = 0.002690341789275408
iteration 265, loss = 0.0023877075873315334
iteration 266, loss = 0.0025109420530498028
iteration 267, loss = 0.0023265026975423098
iteration 268, loss = 0.0037595434114336967
iteration 269, loss = 0.0024488421622663736
iteration 270, loss = 0.004787883255630732
iteration 271, loss = 0.0032510547898709774
iteration 272, loss = 0.0024679857306182384
iteration 273, loss = 0.004588040057569742
iteration 274, loss = 0.0034053511917591095
iteration 275, loss = 0.002537310589104891
iteration 276, loss = 0.002628781832754612
iteration 277, loss = 0.0025580411311239004
iteration 278, loss = 0.002989312633872032
iteration 279, loss = 0.0035558699164539576
iteration 280, loss = 0.0025018309243023396
iteration 281, loss = 0.002549431985244155
iteration 282, loss = 0.0048034354113042355
iteration 283, loss = 0.0024435895029455423
iteration 284, loss = 0.002561562927439809
iteration 285, loss = 0.002219896297901869
iteration 286, loss = 0.0023156062234193087
iteration 287, loss = 0.0025223533157259226
iteration 288, loss = 0.0022075928281992674
iteration 289, loss = 0.00448644207790494
iteration 290, loss = 0.002538513857871294
iteration 291, loss = 0.002943309023976326
iteration 292, loss = 0.0031104180961847305
iteration 293, loss = 0.0032716523855924606
iteration 294, loss = 0.0024403042625635862
iteration 295, loss = 0.002103111706674099
iteration 296, loss = 0.002166592748835683
iteration 297, loss = 0.0024970329832285643
iteration 298, loss = 0.004367879591882229
iteration 299, loss = 0.002577809151262045
iteration 300, loss = 0.003185044741258025
iteration 1, loss = 0.002883035223931074
iteration 2, loss = 0.0023109253961592913
iteration 3, loss = 0.002624132204800844
iteration 4, loss = 0.002522950991988182
iteration 5, loss = 0.0021685303654521704
iteration 6, loss = 0.0025330069474875927
iteration 7, loss = 0.0028317070100456476
iteration 8, loss = 0.0024079731665551662
iteration 9, loss = 0.002835378050804138
iteration 10, loss = 0.002437434159219265
iteration 11, loss = 0.0022453167475759983
iteration 12, loss = 0.0024219700135290623
iteration 13, loss = 0.0036812229081988335
iteration 14, loss = 0.0024525250773876905
iteration 15, loss = 0.0031896024011075497
iteration 16, loss = 0.002375266747549176
iteration 17, loss = 0.0026380526833236217
iteration 18, loss = 0.0028270799666643143
iteration 19, loss = 0.0021839095279574394
iteration 20, loss = 0.002687221160158515
iteration 21, loss = 0.0036685389932245016
iteration 22, loss = 0.004121239762753248
iteration 23, loss = 0.0025125036481767893
iteration 24, loss = 0.002667271299287677
iteration 25, loss = 0.002309529110789299
iteration 26, loss = 0.0023040161468088627
iteration 27, loss = 0.0022570686414837837
iteration 28, loss = 0.002927134046331048
iteration 29, loss = 0.0024465059395879507
iteration 30, loss = 0.002309930743649602
iteration 31, loss = 0.0027206093072891235
iteration 32, loss = 0.0023052275646477938
iteration 33, loss = 0.0022039476316422224
iteration 34, loss = 0.003591536544263363
iteration 35, loss = 0.004303283058106899
iteration 36, loss = 0.002965624211356044
iteration 37, loss = 0.00308272079564631
iteration 38, loss = 0.002629559487104416
iteration 39, loss = 0.002356400014832616
iteration 40, loss = 0.0026446047704666853
iteration 41, loss = 0.0026217401027679443
iteration 42, loss = 0.002183685777708888
iteration 43, loss = 0.0025281065609306097
iteration 44, loss = 0.0022006225772202015
iteration 45, loss = 0.0023703635670244694
iteration 46, loss = 0.0034821415320038795
iteration 47, loss = 0.0022009597159922123
iteration 48, loss = 0.002644374966621399
iteration 49, loss = 0.00219178618863225
iteration 50, loss = 0.0025577989872545004
iteration 51, loss = 0.002678247168660164
iteration 52, loss = 0.003617052687332034
iteration 53, loss = 0.003822540631517768
iteration 54, loss = 0.0036946237087249756
iteration 55, loss = 0.004009558353573084
iteration 56, loss = 0.0023710918612778187
iteration 57, loss = 0.002478789770975709
iteration 58, loss = 0.0029711073730140924
iteration 59, loss = 0.0022073073778301477
iteration 60, loss = 0.0031767261680215597
iteration 61, loss = 0.002310194307938218
iteration 62, loss = 0.002577630104497075
iteration 63, loss = 0.002962762024253607
iteration 64, loss = 0.0023783694487065077
iteration 65, loss = 0.003082635346800089
iteration 66, loss = 0.004248833283782005
iteration 67, loss = 0.0023411326110363007
iteration 68, loss = 0.0027483198791742325
iteration 69, loss = 0.0023396776523441076
iteration 70, loss = 0.004267383366823196
iteration 71, loss = 0.0032749739475548267
iteration 72, loss = 0.002826389390975237
iteration 73, loss = 0.0032223158050328493
iteration 74, loss = 0.002683207392692566
iteration 75, loss = 0.002926143351942301
iteration 76, loss = 0.00267287390306592
iteration 77, loss = 0.002754269866272807
iteration 78, loss = 0.0024243134539574385
iteration 79, loss = 0.00257190247066319
iteration 80, loss = 0.0024559074081480503
iteration 81, loss = 0.0031665160786360502
iteration 82, loss = 0.0031949826516211033
iteration 83, loss = 0.004567250143736601
iteration 84, loss = 0.0021834038197994232
iteration 85, loss = 0.0024958173744380474
iteration 86, loss = 0.0039023275021463633
iteration 87, loss = 0.002232413040474057
iteration 88, loss = 0.002757479902356863
iteration 89, loss = 0.0023318843450397253
iteration 90, loss = 0.0030433477368205786
iteration 91, loss = 0.003547846572473645
iteration 92, loss = 0.002194819040596485
iteration 93, loss = 0.0037330228369683027
iteration 94, loss = 0.0022575873881578445
iteration 95, loss = 0.0030151009559631348
iteration 96, loss = 0.002285165712237358
iteration 97, loss = 0.0025580301880836487
iteration 98, loss = 0.002554368693381548
iteration 99, loss = 0.002708444371819496
iteration 100, loss = 0.0026406848337501287
iteration 101, loss = 0.0024180065374821424
iteration 102, loss = 0.003264161990955472
iteration 103, loss = 0.0023866367992013693
iteration 104, loss = 0.002240188419818878
iteration 105, loss = 0.0031575565226376057
iteration 106, loss = 0.004475884139537811
iteration 107, loss = 0.0023625739850103855
iteration 108, loss = 0.0021426030434668064
iteration 109, loss = 0.0021588739473372698
iteration 110, loss = 0.0022255710791796446
iteration 111, loss = 0.00281606987118721
iteration 112, loss = 0.0022397160064429045
iteration 113, loss = 0.0026153891813009977
iteration 114, loss = 0.002454675268381834
iteration 115, loss = 0.0035320711322128773
iteration 116, loss = 0.0024378153029829264
iteration 117, loss = 0.0036174883134663105
iteration 118, loss = 0.002245116513222456
iteration 119, loss = 0.0028332131914794445
iteration 120, loss = 0.0020734143909066916
iteration 121, loss = 0.0027964317705482244
iteration 122, loss = 0.0025985222309827805
iteration 123, loss = 0.0043958681635558605
iteration 124, loss = 0.003985821269452572
iteration 125, loss = 0.002069501904770732
iteration 126, loss = 0.002555879997089505
iteration 127, loss = 0.0023847261909395456
iteration 128, loss = 0.0031974483281373978
iteration 129, loss = 0.0026140485424548388
iteration 130, loss = 0.0034525978844612837
iteration 131, loss = 0.0023481054231524467
iteration 132, loss = 0.0023936147335916758
iteration 133, loss = 0.003076531458646059
iteration 134, loss = 0.0022509603295475245
iteration 135, loss = 0.002590919379144907
iteration 136, loss = 0.0026397183537483215
iteration 137, loss = 0.002414789516478777
iteration 138, loss = 0.0024459140840917826
iteration 139, loss = 0.0032644937746226788
iteration 140, loss = 0.0023823047522455454
iteration 141, loss = 0.0024316676426678896
iteration 142, loss = 0.002370049711316824
iteration 143, loss = 0.0021476519759744406
iteration 144, loss = 0.0022528935223817825
iteration 145, loss = 0.0027965796180069447
iteration 146, loss = 0.0024536207783967257
iteration 147, loss = 0.003087889403104782
iteration 148, loss = 0.002466618549078703
iteration 149, loss = 0.0026846081018447876
iteration 150, loss = 0.002326309448108077
iteration 151, loss = 0.002593612764030695
iteration 152, loss = 0.0025618320796638727
iteration 153, loss = 0.0021998833399266005
iteration 154, loss = 0.0028192857280373573
iteration 155, loss = 0.0021290157455950975
iteration 156, loss = 0.002050816547125578
iteration 157, loss = 0.00415970990434289
iteration 158, loss = 0.0029821100179105997
iteration 159, loss = 0.002269863151013851
iteration 160, loss = 0.0026323229540139437
iteration 161, loss = 0.002105887746438384
iteration 162, loss = 0.0022874155547469854
iteration 163, loss = 0.0027865394949913025
iteration 164, loss = 0.0023413794115185738
iteration 165, loss = 0.002088685054332018
iteration 166, loss = 0.003033204236999154
iteration 167, loss = 0.0024823942221701145
iteration 168, loss = 0.0021969834342598915
iteration 169, loss = 0.0021414863876998425
iteration 170, loss = 0.0031019002199172974
iteration 171, loss = 0.002083780709654093
iteration 172, loss = 0.0021141187753528357
iteration 173, loss = 0.0032684041652828455
iteration 174, loss = 0.0023677574936300516
iteration 175, loss = 0.002276514656841755
iteration 176, loss = 0.0020679470617324114
iteration 177, loss = 0.002256109844893217
iteration 178, loss = 0.0021998677402734756
iteration 179, loss = 0.0022040377371013165
iteration 180, loss = 0.0029758652672171593
iteration 181, loss = 0.0022010880056768656
iteration 182, loss = 0.002235767198726535
iteration 183, loss = 0.004366017412394285
iteration 184, loss = 0.002602523425593972
iteration 185, loss = 0.002359640086069703
iteration 186, loss = 0.002315738471224904
iteration 187, loss = 0.002564419060945511
iteration 188, loss = 0.0023773566354066133
iteration 189, loss = 0.0022815244738012552
iteration 190, loss = 0.002361496677622199
iteration 191, loss = 0.0023342114873230457
iteration 192, loss = 0.0024975857231765985
iteration 193, loss = 0.004260718356817961
iteration 194, loss = 0.002200716407969594
iteration 195, loss = 0.002036781283095479
iteration 196, loss = 0.0021684588864445686
iteration 197, loss = 0.00232284446246922
iteration 198, loss = 0.002266329014673829
iteration 199, loss = 0.0021646718960255384
iteration 200, loss = 0.0038411880377680063
iteration 201, loss = 0.0023225543554872274
iteration 202, loss = 0.002313881414011121
iteration 203, loss = 0.0021525730844587088
iteration 204, loss = 0.0023175240494310856
iteration 205, loss = 0.004116048105061054
iteration 206, loss = 0.0023116690572351217
iteration 207, loss = 0.0029228695202618837
iteration 208, loss = 0.002121410332620144
iteration 209, loss = 0.0022568823769688606
iteration 210, loss = 0.0020165168680250645
iteration 211, loss = 0.002113655675202608
iteration 212, loss = 0.0021896713878959417
iteration 213, loss = 0.0025707625318318605
iteration 214, loss = 0.0020140279084444046
iteration 215, loss = 0.0021265640389174223
iteration 216, loss = 0.0031770903151482344
iteration 217, loss = 0.0027884135488420725
iteration 218, loss = 0.003169975709170103
iteration 219, loss = 0.0023494400084018707
iteration 220, loss = 0.002378674689680338
iteration 221, loss = 0.0026995905209332705
iteration 222, loss = 0.002925039501860738
iteration 223, loss = 0.00224055047146976
iteration 224, loss = 0.002239139750599861
iteration 225, loss = 0.00221620942465961
iteration 226, loss = 0.0025099527556449175
iteration 227, loss = 0.002300902735441923
iteration 228, loss = 0.0023640324361622334
iteration 229, loss = 0.002193151041865349
iteration 230, loss = 0.0028760917484760284
iteration 231, loss = 0.0021210117265582085
iteration 232, loss = 0.0023111593909561634
iteration 233, loss = 0.0022392868995666504
iteration 234, loss = 0.0026230118237435818
iteration 235, loss = 0.0022273920476436615
iteration 236, loss = 0.0021363499108701944
iteration 237, loss = 0.002793569816276431
iteration 238, loss = 0.0025865971110761166
iteration 239, loss = 0.0021123900078237057
iteration 240, loss = 0.0022036200389266014
iteration 241, loss = 0.0025370081420987844
iteration 242, loss = 0.0020724902860820293
iteration 243, loss = 0.002356630750000477
iteration 244, loss = 0.0025966069661080837
iteration 245, loss = 0.0021258860360831022
iteration 246, loss = 0.0023548826575279236
iteration 247, loss = 0.0021639615297317505
iteration 248, loss = 0.0020495851058512926
iteration 249, loss = 0.0023446818813681602
iteration 250, loss = 0.002451346954330802
iteration 251, loss = 0.0031936748418956995
iteration 252, loss = 0.0020407545380294323
iteration 253, loss = 0.0033761579543352127
iteration 254, loss = 0.002116449410095811
iteration 255, loss = 0.002714235335588455
iteration 256, loss = 0.0030017890967428684
iteration 257, loss = 0.002268480369821191
iteration 258, loss = 0.0025283549912273884
iteration 259, loss = 0.002394727896898985
iteration 260, loss = 0.0022962221410125494
iteration 261, loss = 0.0021158489398658276
iteration 262, loss = 0.0019775175023823977
iteration 263, loss = 0.0023657381534576416
iteration 264, loss = 0.002907136222347617
iteration 265, loss = 0.002922896994277835
iteration 266, loss = 0.0023637565318495035
iteration 267, loss = 0.0021224492229521275
iteration 268, loss = 0.0040027000941336155
iteration 269, loss = 0.002413839567452669
iteration 270, loss = 0.002307257615029812
iteration 271, loss = 0.002257761312648654
iteration 272, loss = 0.0023542807903140783
iteration 273, loss = 0.002833959646522999
iteration 274, loss = 0.002134895185008645
iteration 275, loss = 0.0026360880583524704
iteration 276, loss = 0.0020764314103871584
iteration 277, loss = 0.0030307467095553875
iteration 278, loss = 0.0025773702654987574
iteration 279, loss = 0.0024334974586963654
iteration 280, loss = 0.0027037402614951134
iteration 281, loss = 0.0030863434076309204
iteration 282, loss = 0.002541670110076666
iteration 283, loss = 0.002204726915806532
iteration 284, loss = 0.002315986668691039
iteration 285, loss = 0.0024161669425666332
iteration 286, loss = 0.002091987757012248
iteration 287, loss = 0.0019554297905415297
iteration 288, loss = 0.002461542608216405
iteration 289, loss = 0.0022602176759392023
iteration 290, loss = 0.0025178412906825542
iteration 291, loss = 0.0025415574200451374
iteration 292, loss = 0.0022347078192979097
iteration 293, loss = 0.00223081954754889
iteration 294, loss = 0.0025222417898476124
iteration 295, loss = 0.002288686577230692
iteration 296, loss = 0.0025866697542369366
iteration 297, loss = 0.002925676293671131
iteration 298, loss = 0.002031267387792468
iteration 299, loss = 0.0024128230288624763
iteration 300, loss = 0.003285455983132124
iteration 1, loss = 0.002509357174858451
iteration 2, loss = 0.0021953610703349113
iteration 3, loss = 0.0026558905374258757
iteration 4, loss = 0.0037573587615042925
iteration 5, loss = 0.0022684060968458652
iteration 6, loss = 0.0025176091585308313
iteration 7, loss = 0.0020421771332621574
iteration 8, loss = 0.002131797606125474
iteration 9, loss = 0.0029985285364091396
iteration 10, loss = 0.0023612859658896923
iteration 11, loss = 0.002327678259462118
iteration 12, loss = 0.0020270852837711573
iteration 13, loss = 0.0020411694422364235
iteration 14, loss = 0.0038442485965788364
iteration 15, loss = 0.0023307879455387592
iteration 16, loss = 0.002838409971445799
iteration 17, loss = 0.003314332338050008
iteration 18, loss = 0.0034723831340670586
iteration 19, loss = 0.002417079173028469
iteration 20, loss = 0.0023925574496388435
iteration 21, loss = 0.00268211355432868
iteration 22, loss = 0.0029612588696181774
iteration 23, loss = 0.002322371583431959
iteration 24, loss = 0.0028617745265364647
iteration 25, loss = 0.001995526719838381
iteration 26, loss = 0.002085223561152816
iteration 27, loss = 0.002300858963280916
iteration 28, loss = 0.0025426563806831837
iteration 29, loss = 0.0024495478719472885
iteration 30, loss = 0.0021836685482412577
iteration 31, loss = 0.002499006688594818
iteration 32, loss = 0.0022355124820023775
iteration 33, loss = 0.0021500769071280956
iteration 34, loss = 0.0021684388630092144
iteration 35, loss = 0.0020926184952259064
iteration 36, loss = 0.002404371276497841
iteration 37, loss = 0.00229292968288064
iteration 38, loss = 0.002280488144606352
iteration 39, loss = 0.003948039375245571
iteration 40, loss = 0.0029234883841127157
iteration 41, loss = 0.0025056395679712296
iteration 42, loss = 0.0039344350807368755
iteration 43, loss = 0.002325277542695403
iteration 44, loss = 0.0023068366572260857
iteration 45, loss = 0.002458603587001562
iteration 46, loss = 0.002123527927324176
iteration 47, loss = 0.002113684779033065
iteration 48, loss = 0.0040301731787621975
iteration 49, loss = 0.0023307378869503736
iteration 50, loss = 0.0024878911208361387
iteration 51, loss = 0.0023381239734590054
iteration 52, loss = 0.002340540522709489
iteration 53, loss = 0.0031302664428949356
iteration 54, loss = 0.0023238519206643105
iteration 55, loss = 0.0021199830807745457
iteration 56, loss = 0.002362134400755167
iteration 57, loss = 0.0019437532173469663
iteration 58, loss = 0.004234846215695143
iteration 59, loss = 0.0020596059039235115
iteration 60, loss = 0.0021679778583347797
iteration 61, loss = 0.002092027338221669
iteration 62, loss = 0.002354992087930441
iteration 63, loss = 0.0024498472921550274
iteration 64, loss = 0.0020811231806874275
iteration 65, loss = 0.0022910020779818296
iteration 66, loss = 0.002337375655770302
iteration 67, loss = 0.0021053599193692207
iteration 68, loss = 0.002116329735144973
iteration 69, loss = 0.002263687551021576
iteration 70, loss = 0.0023807287216186523
iteration 71, loss = 0.0021648756228387356
iteration 72, loss = 0.002942668506875634
iteration 73, loss = 0.002501447917893529
iteration 74, loss = 0.0022325797472149134
iteration 75, loss = 0.002446963917464018
iteration 76, loss = 0.002352711046114564
iteration 77, loss = 0.0020975908264517784
iteration 78, loss = 0.002908172085881233
iteration 79, loss = 0.002232813276350498
iteration 80, loss = 0.002242705086246133
iteration 81, loss = 0.0023036119528114796
iteration 82, loss = 0.0023111917544156313
iteration 83, loss = 0.0019828928634524345
iteration 84, loss = 0.0029213116504251957
iteration 85, loss = 0.0026041585952043533
iteration 86, loss = 0.0019260281696915627
iteration 87, loss = 0.0018934098770841956
iteration 88, loss = 0.002111125970259309
iteration 89, loss = 0.0023227734491229057
iteration 90, loss = 0.0030456725507974625
iteration 91, loss = 0.0021173455752432346
iteration 92, loss = 0.0026385546661913395
iteration 93, loss = 0.0022220194805413485
iteration 94, loss = 0.002416224917396903
iteration 95, loss = 0.002759477123618126
iteration 96, loss = 0.00203694892115891
iteration 97, loss = 0.00272667920216918
iteration 98, loss = 0.0021518529392778873
iteration 99, loss = 0.002335880184546113
iteration 100, loss = 0.002969814231619239
iteration 101, loss = 0.002138935262337327
iteration 102, loss = 0.0019669763278216124
iteration 103, loss = 0.003679937683045864
iteration 104, loss = 0.0024462903384119272
iteration 105, loss = 0.002359312493354082
iteration 106, loss = 0.0027528947684913874
iteration 107, loss = 0.002614405006170273
iteration 108, loss = 0.002062072278931737
iteration 109, loss = 0.002004888840019703
iteration 110, loss = 0.0024822254199534655
iteration 111, loss = 0.0020948334131389856
iteration 112, loss = 0.0030705591198056936
iteration 113, loss = 0.0020249744411557913
iteration 114, loss = 0.002132796449586749
iteration 115, loss = 0.00211533741094172
iteration 116, loss = 0.0025308935437351465
iteration 117, loss = 0.0023076024372130632
iteration 118, loss = 0.004934338387101889
iteration 119, loss = 0.001879856106825173
iteration 120, loss = 0.002117377705872059
iteration 121, loss = 0.001928818761371076
iteration 122, loss = 0.0020086602307856083
iteration 123, loss = 0.0018659010529518127
iteration 124, loss = 0.0023497645743191242
iteration 125, loss = 0.0019946773536503315
iteration 126, loss = 0.0021306064445525408
iteration 127, loss = 0.0035362890921533108
iteration 128, loss = 0.0020257257856428623
iteration 129, loss = 0.0021448221523314714
iteration 130, loss = 0.002394309500232339
iteration 131, loss = 0.003135939594358206
iteration 132, loss = 0.0018963644979521632
iteration 133, loss = 0.0039026245940476656
iteration 134, loss = 0.0022043082863092422
iteration 135, loss = 0.002135457471013069
iteration 136, loss = 0.0020859723445028067
iteration 137, loss = 0.001960261259227991
iteration 138, loss = 0.0021025200840085745
iteration 139, loss = 0.002259963657706976
iteration 140, loss = 0.002302810549736023
iteration 141, loss = 0.0021950090304017067
iteration 142, loss = 0.0019565687980502844
iteration 143, loss = 0.0022597818169742823
iteration 144, loss = 0.002002566121518612
iteration 145, loss = 0.0019511482678353786
iteration 146, loss = 0.0027098983991891146
iteration 147, loss = 0.0021198708564043045
iteration 148, loss = 0.002150708343833685
iteration 149, loss = 0.0020235348492860794
iteration 150, loss = 0.0021468489430844784
iteration 151, loss = 0.0022073988802731037
iteration 152, loss = 0.002606966532766819
iteration 153, loss = 0.0021614250726997852
iteration 154, loss = 0.001898613409139216
iteration 155, loss = 0.002743249759078026
iteration 156, loss = 0.0030105856712907553
iteration 157, loss = 0.0020943433046340942
iteration 158, loss = 0.0020609779749065638
iteration 159, loss = 0.0023112825583666563
iteration 160, loss = 0.00280342111364007
iteration 161, loss = 0.0018562717596068978
iteration 162, loss = 0.002289601368829608
iteration 163, loss = 0.0019799002911895514
iteration 164, loss = 0.002362150466069579
iteration 165, loss = 0.003124007722362876
iteration 166, loss = 0.0018256339244544506
iteration 167, loss = 0.002405105624347925
iteration 168, loss = 0.0032911174930632114
iteration 169, loss = 0.0021299661602824926
iteration 170, loss = 0.0019725977908819914
iteration 171, loss = 0.0018736384809017181
iteration 172, loss = 0.0030342580284923315
iteration 173, loss = 0.0018650388810783625
iteration 174, loss = 0.0021520263981074095
iteration 175, loss = 0.0020359510090202093
iteration 176, loss = 0.0037805938627570868
iteration 177, loss = 0.0022913075517863035
iteration 178, loss = 0.0028725077863782644
iteration 179, loss = 0.0020293279085308313
iteration 180, loss = 0.0028322096914052963
iteration 181, loss = 0.002088999142870307
iteration 182, loss = 0.0021094954572618008
iteration 183, loss = 0.001972552854567766
iteration 184, loss = 0.0022431802935898304
iteration 185, loss = 0.002066916087642312
iteration 186, loss = 0.002137102885171771
iteration 187, loss = 0.0022827894426882267
iteration 188, loss = 0.0022381464950740337
iteration 189, loss = 0.0022844106424599886
iteration 190, loss = 0.002117547206580639
iteration 191, loss = 0.0021685271058231592
iteration 192, loss = 0.002396796364337206
iteration 193, loss = 0.002011193661019206
iteration 194, loss = 0.002136936644092202
iteration 195, loss = 0.00262129632756114
iteration 196, loss = 0.0019662051927298307
iteration 197, loss = 0.0025807935744524
iteration 198, loss = 0.0025086128152906895
iteration 199, loss = 0.002015372272580862
iteration 200, loss = 0.001896513975225389
iteration 201, loss = 0.0027929136995226145
iteration 202, loss = 0.002660604426637292
iteration 203, loss = 0.0025522876530885696
iteration 204, loss = 0.002102226484566927
iteration 205, loss = 0.002195565029978752
iteration 206, loss = 0.0021033715456724167
iteration 207, loss = 0.0026665721088647842
iteration 208, loss = 0.0022414098493754864
iteration 209, loss = 0.002134329406544566
iteration 210, loss = 0.002093422459438443
iteration 211, loss = 0.002070797374472022
iteration 212, loss = 0.0022090990096330643
iteration 213, loss = 0.002937202574685216
iteration 214, loss = 0.002879651729017496
iteration 215, loss = 0.0018974242266267538
iteration 216, loss = 0.002145651960745454
iteration 217, loss = 0.0034659099765121937
iteration 218, loss = 0.0018169870600104332
iteration 219, loss = 0.001965750940144062
iteration 220, loss = 0.0028238892555236816
iteration 221, loss = 0.0017705877544358373
iteration 222, loss = 0.0021542950998991728
iteration 223, loss = 0.0029505579732358456
iteration 224, loss = 0.002185033168643713
iteration 225, loss = 0.002185214078053832
iteration 226, loss = 0.0022029285319149494
iteration 227, loss = 0.0020171459764242172
iteration 228, loss = 0.0022880330216139555
iteration 229, loss = 0.0024831921327859163
iteration 230, loss = 0.00399321224540472
iteration 231, loss = 0.0022083939984440804
iteration 232, loss = 0.004044152330607176
iteration 233, loss = 0.003046286292374134
iteration 234, loss = 0.002312404103577137
iteration 235, loss = 0.0017452314496040344
iteration 236, loss = 0.0019140064250677824
iteration 237, loss = 0.0019265079172328115
iteration 238, loss = 0.001852974179200828
iteration 239, loss = 0.002877218881621957
iteration 240, loss = 0.0021759660448879004
iteration 241, loss = 0.0018662400543689728
iteration 242, loss = 0.0027373977936804295
iteration 243, loss = 0.0027742015663534403
iteration 244, loss = 0.0021696786861866713
iteration 245, loss = 0.0025628625880926847
iteration 246, loss = 0.0019795880652964115
iteration 247, loss = 0.0019854893907904625
iteration 248, loss = 0.0022583932150155306
iteration 249, loss = 0.001968354219570756
iteration 250, loss = 0.0018032407388091087
iteration 251, loss = 0.002055505756288767
iteration 252, loss = 0.0020979363471269608
iteration 253, loss = 0.002174234017729759
iteration 254, loss = 0.0022502476349473
iteration 255, loss = 0.001983871217817068
iteration 256, loss = 0.002663824474439025
iteration 257, loss = 0.0020988204050809145
iteration 258, loss = 0.002266519470140338
iteration 259, loss = 0.002528600161895156
iteration 260, loss = 0.002369051333516836
iteration 261, loss = 0.0020051810424774885
iteration 262, loss = 0.0019345124019309878
iteration 263, loss = 0.0021151001565158367
iteration 264, loss = 0.0021822669077664614
iteration 265, loss = 0.0022954947780817747
iteration 266, loss = 0.0021727385465055704
iteration 267, loss = 0.002160225994884968
iteration 268, loss = 0.002010377822443843
iteration 269, loss = 0.001876056776382029
iteration 270, loss = 0.0020614261738955975
iteration 271, loss = 0.002188223646953702
iteration 272, loss = 0.002023013075813651
iteration 273, loss = 0.0020004662219434977
iteration 274, loss = 0.001905504846945405
iteration 275, loss = 0.0023555662482976913
iteration 276, loss = 0.002173310611397028
iteration 277, loss = 0.0021071950905025005
iteration 278, loss = 0.0020263963378965855
iteration 279, loss = 0.0025223311968147755
iteration 280, loss = 0.0021170522086322308
iteration 281, loss = 0.002366900909692049
iteration 282, loss = 0.0020763694774359465
iteration 283, loss = 0.0033519056160002947
iteration 284, loss = 0.002166859107092023
iteration 285, loss = 0.0018348345765843987
iteration 286, loss = 0.0019627436995506287
iteration 287, loss = 0.0020123182330280542
iteration 288, loss = 0.002321514766663313
iteration 289, loss = 0.0019981309305876493
iteration 290, loss = 0.0018612768035382032
iteration 291, loss = 0.0021219325717538595
iteration 292, loss = 0.0020437019411474466
iteration 293, loss = 0.001911813742481172
iteration 294, loss = 0.0022870819084346294
iteration 295, loss = 0.00344840157777071
iteration 296, loss = 0.0024705163668841124
iteration 297, loss = 0.0017608499620109797
iteration 298, loss = 0.002565842354670167
iteration 299, loss = 0.001696720370091498
iteration 300, loss = 0.0019374776165932417
iteration 1, loss = 0.0018590488471090794
iteration 2, loss = 0.0020544615108519793
iteration 3, loss = 0.002253028331324458
iteration 4, loss = 0.0018941187299787998
iteration 5, loss = 0.0020989947952330112
iteration 6, loss = 0.0018707806011661887
iteration 7, loss = 0.0018151600379496813
iteration 8, loss = 0.0024387449957430363
iteration 9, loss = 0.002124849706888199
iteration 10, loss = 0.001886729383841157
iteration 11, loss = 0.0017533203354105353
iteration 12, loss = 0.0027178446762263775
iteration 13, loss = 0.002074354561045766
iteration 14, loss = 0.0018685577670112252
iteration 15, loss = 0.002569593256339431
iteration 16, loss = 0.001860206015408039
iteration 17, loss = 0.001895464607514441
iteration 18, loss = 0.0022029357496649027
iteration 19, loss = 0.001924972515553236
iteration 20, loss = 0.0021475290413945913
iteration 21, loss = 0.002702662954106927
iteration 22, loss = 0.0022308065090328455
iteration 23, loss = 0.0022985092364251614
iteration 24, loss = 0.0019317127298563719
iteration 25, loss = 0.0022063725627958775
iteration 26, loss = 0.0018361933762207627
iteration 27, loss = 0.0019900654442608356
iteration 28, loss = 0.0018823163118213415
iteration 29, loss = 0.0022421800531446934
iteration 30, loss = 0.0021146817598491907
iteration 31, loss = 0.0017796517349779606
iteration 32, loss = 0.0026190155185759068
iteration 33, loss = 0.0021973606199026108
iteration 34, loss = 0.002128558699041605
iteration 35, loss = 0.001998084830120206
iteration 36, loss = 0.0021495833061635494
iteration 37, loss = 0.00216881325468421
iteration 38, loss = 0.0018170838011428714
iteration 39, loss = 0.002347212517634034
iteration 40, loss = 0.001850239816121757
iteration 41, loss = 0.002061339793726802
iteration 42, loss = 0.0020323218777775764
iteration 43, loss = 0.0018864035373553634
iteration 44, loss = 0.0025109881535172462
iteration 45, loss = 0.0019595460034906864
iteration 46, loss = 0.002030791714787483
iteration 47, loss = 0.0022788261994719505
iteration 48, loss = 0.0022137509658932686
iteration 49, loss = 0.002060848753899336
iteration 50, loss = 0.001806779531762004
iteration 51, loss = 0.0024681435897946358
iteration 52, loss = 0.002760922070592642
iteration 53, loss = 0.0018331566825509071
iteration 54, loss = 0.001809596549719572
iteration 55, loss = 0.00227158609777689
iteration 56, loss = 0.0027021237183362246
iteration 57, loss = 0.002058562124148011
iteration 58, loss = 0.002344649052247405
iteration 59, loss = 0.0019638214726001024
iteration 60, loss = 0.0019520012428984046
iteration 61, loss = 0.0024250210262835026
iteration 62, loss = 0.002081518294289708
iteration 63, loss = 0.0020676071289926767
iteration 64, loss = 0.002377171302214265
iteration 65, loss = 0.0021013051737099886
iteration 66, loss = 0.003285401500761509
iteration 67, loss = 0.002044845838099718
iteration 68, loss = 0.001941279391758144
iteration 69, loss = 0.0018917657434940338
iteration 70, loss = 0.0019212751649320126
iteration 71, loss = 0.0018625662196427584
iteration 72, loss = 0.002066468121483922
iteration 73, loss = 0.002090903464704752
iteration 74, loss = 0.0019398073200136423
iteration 75, loss = 0.0019491201965138316
iteration 76, loss = 0.0022076168097555637
iteration 77, loss = 0.0018961045425385237
iteration 78, loss = 0.002253885380923748
iteration 79, loss = 0.0038943348918110132
iteration 80, loss = 0.0033294770400971174
iteration 81, loss = 0.002150867134332657
iteration 82, loss = 0.0018114697886630893
iteration 83, loss = 0.0017577694961801171
iteration 84, loss = 0.0024212785065174103
iteration 85, loss = 0.0020433804020285606
iteration 86, loss = 0.0019261337583884597
iteration 87, loss = 0.002390287583693862
iteration 88, loss = 0.0018790336325764656
iteration 89, loss = 0.0019606517162173986
iteration 90, loss = 0.001868489314801991
iteration 91, loss = 0.002063027350232005
iteration 92, loss = 0.001817326177842915
iteration 93, loss = 0.0028876112774014473
iteration 94, loss = 0.0020026108250021935
iteration 95, loss = 0.0023487580474466085
iteration 96, loss = 0.0018083758186548948
iteration 97, loss = 0.0016672400524839759
iteration 98, loss = 0.001981281442567706
iteration 99, loss = 0.0021007410250604153
iteration 100, loss = 0.0027399591635912657
iteration 101, loss = 0.001967070857062936
iteration 102, loss = 0.002441094722598791
iteration 103, loss = 0.0017107450403273106
iteration 104, loss = 0.0018087137723341584
iteration 105, loss = 0.0018242397345602512
iteration 106, loss = 0.0019844339694827795
iteration 107, loss = 0.0020473268814384937
iteration 108, loss = 0.002680571284145117
iteration 109, loss = 0.0034779950510710478
iteration 110, loss = 0.0024384334683418274
iteration 111, loss = 0.00229851552285254
iteration 112, loss = 0.002163487719371915
iteration 113, loss = 0.0022500092163681984
iteration 114, loss = 0.002246944699436426
iteration 115, loss = 0.001819657627493143
iteration 116, loss = 0.002251941245049238
iteration 117, loss = 0.0018138519953936338
iteration 118, loss = 0.0020313416607677937
iteration 119, loss = 0.0031006101053208113
iteration 120, loss = 0.002452796557918191
iteration 121, loss = 0.0020028729923069477
iteration 122, loss = 0.0019794560503214598
iteration 123, loss = 0.0021028805058449507
iteration 124, loss = 0.0026016803458333015
iteration 125, loss = 0.002416596980765462
iteration 126, loss = 0.0029579924885183573
iteration 127, loss = 0.00228948425501585
iteration 128, loss = 0.0027883148286491632
iteration 129, loss = 0.0026178481057286263
iteration 130, loss = 0.0024576487485319376
iteration 131, loss = 0.002329475712031126
iteration 132, loss = 0.0027816309593617916
iteration 133, loss = 0.0021826238371431828
iteration 134, loss = 0.0018534513656049967
iteration 135, loss = 0.0018672001315280795
iteration 136, loss = 0.0031896261498332024
iteration 137, loss = 0.0018162875203415751
iteration 138, loss = 0.0026835633907467127
iteration 139, loss = 0.002015757840126753
iteration 140, loss = 0.0016448835376650095
iteration 141, loss = 0.0019364875042811036
iteration 142, loss = 0.001779017737135291
iteration 143, loss = 0.001831606961786747
iteration 144, loss = 0.00245887553319335
iteration 145, loss = 0.0018731640884652734
iteration 146, loss = 0.001817737822420895
iteration 147, loss = 0.0018205898813903332
iteration 148, loss = 0.0017816827166825533
iteration 149, loss = 0.0017593031516298652
iteration 150, loss = 0.0019768939819186926
iteration 151, loss = 0.0025762084405869246
iteration 152, loss = 0.0022961364593356848
iteration 153, loss = 0.0022603007964789867
iteration 154, loss = 0.0017781720962375402
iteration 155, loss = 0.0018973881378769875
iteration 156, loss = 0.0020324615761637688
iteration 157, loss = 0.0017801591893658042
iteration 158, loss = 0.0025245146825909615
iteration 159, loss = 0.0020359002519398928
iteration 160, loss = 0.002877356717363
iteration 161, loss = 0.0019268215401098132
iteration 162, loss = 0.0018382648704573512
iteration 163, loss = 0.0020403224043548107
iteration 164, loss = 0.003470625262707472
iteration 165, loss = 0.0022171051241457462
iteration 166, loss = 0.0018661292269825935
iteration 167, loss = 0.002074272371828556
iteration 168, loss = 0.0018516401760280132
iteration 169, loss = 0.0018635762389749289
iteration 170, loss = 0.001756975776515901
iteration 171, loss = 0.0034629283472895622
iteration 172, loss = 0.0019079431658610702
iteration 173, loss = 0.001833261689171195
iteration 174, loss = 0.0017843373352661729
iteration 175, loss = 0.002292144112288952
iteration 176, loss = 0.0016987104900181293
iteration 177, loss = 0.002086529741063714
iteration 178, loss = 0.00230722245760262
iteration 179, loss = 0.002150875050574541
iteration 180, loss = 0.001868644030764699
iteration 181, loss = 0.0035758125595748425
iteration 182, loss = 0.002261849818751216
iteration 183, loss = 0.002648270456120372
iteration 184, loss = 0.0024645973462611437
iteration 185, loss = 0.002164797857403755
iteration 186, loss = 0.0021147369407117367
iteration 187, loss = 0.0019338103011250496
iteration 188, loss = 0.0018646728713065386
iteration 189, loss = 0.0022414664272218943
iteration 190, loss = 0.0021083492320030928
iteration 191, loss = 0.0019998683128505945
iteration 192, loss = 0.001995445229113102
iteration 193, loss = 0.0016357313143089414
iteration 194, loss = 0.001963219605386257
iteration 195, loss = 0.001993515994399786
iteration 196, loss = 0.0017575360834598541
iteration 197, loss = 0.004729514010250568
iteration 198, loss = 0.0018168481765314937
iteration 199, loss = 0.001756751909852028
iteration 200, loss = 0.001956026069819927
iteration 201, loss = 0.0021434722002595663
iteration 202, loss = 0.0017848385032266378
iteration 203, loss = 0.00198039086535573
iteration 204, loss = 0.001733032171614468
iteration 205, loss = 0.002563735470175743
iteration 206, loss = 0.0032359198667109013
iteration 207, loss = 0.0029330344405025244
iteration 208, loss = 0.001902644638903439
iteration 209, loss = 0.0019232709892094135
iteration 210, loss = 0.002073819749057293
iteration 211, loss = 0.0021278285421431065
iteration 212, loss = 0.0017277440056204796
iteration 213, loss = 0.0024416064843535423
iteration 214, loss = 0.002840530825778842
iteration 215, loss = 0.0019050713162869215
iteration 216, loss = 0.002450373722240329
iteration 217, loss = 0.0018286533886566758
iteration 218, loss = 0.001853425637818873
iteration 219, loss = 0.001679080305621028
iteration 220, loss = 0.0022320272400975227
iteration 221, loss = 0.0020593085791915655
iteration 222, loss = 0.002541660564020276
iteration 223, loss = 0.0016569247236475348
iteration 224, loss = 0.0018689058488234878
iteration 225, loss = 0.0020997582469135523
iteration 226, loss = 0.002382120117545128
iteration 227, loss = 0.001737196114845574
iteration 228, loss = 0.0019278301624581218
iteration 229, loss = 0.002377438126131892
iteration 230, loss = 0.0019014828139916062
iteration 231, loss = 0.0019475752487778664
iteration 232, loss = 0.0019028077367693186
iteration 233, loss = 0.0019858016166836023
iteration 234, loss = 0.002057392615824938
iteration 235, loss = 0.0018217760371044278
iteration 236, loss = 0.0019822719041258097
iteration 237, loss = 0.0018975276034325361
iteration 238, loss = 0.003294979687780142
iteration 239, loss = 0.00180458789691329
iteration 240, loss = 0.0016715256497263908
iteration 241, loss = 0.001805387786589563
iteration 242, loss = 0.001891653286293149
iteration 243, loss = 0.0015892370138317347
iteration 244, loss = 0.0019517750479280949
iteration 245, loss = 0.0018137566512450576
iteration 246, loss = 0.0019279597327113152
iteration 247, loss = 0.0018258636118844151
iteration 248, loss = 0.0020895942579954863
iteration 249, loss = 0.0018608557293191552
iteration 250, loss = 0.0019273621728643775
iteration 251, loss = 0.0019180307863280177
iteration 252, loss = 0.00196311017498374
iteration 253, loss = 0.001867534127086401
iteration 254, loss = 0.002420918783172965
iteration 255, loss = 0.0016762734157964587
iteration 256, loss = 0.003091581165790558
iteration 257, loss = 0.0030663232319056988
iteration 258, loss = 0.0027203974314033985
iteration 259, loss = 0.0018843341385945678
iteration 260, loss = 0.0024606199003756046
iteration 261, loss = 0.0019395295530557632
iteration 262, loss = 0.0018062753370031714
iteration 263, loss = 0.0019406960345804691
iteration 264, loss = 0.0017172584775835276
iteration 265, loss = 0.0031672094482928514
iteration 266, loss = 0.0019393563270568848
iteration 267, loss = 0.0025153937749564648
iteration 268, loss = 0.001946222735568881
iteration 269, loss = 0.002009501215070486
iteration 270, loss = 0.0017517535015940666
iteration 271, loss = 0.0028755227103829384
iteration 272, loss = 0.002049498027190566
iteration 273, loss = 0.0018009126652032137
iteration 274, loss = 0.002991268178448081
iteration 275, loss = 0.001909321523271501
iteration 276, loss = 0.0018049819627776742
iteration 277, loss = 0.0023081947583705187
iteration 278, loss = 0.001984794158488512
iteration 279, loss = 0.003411020152270794
iteration 280, loss = 0.001801484264433384
iteration 281, loss = 0.0017959631513804197
iteration 282, loss = 0.0021933044772595167
iteration 283, loss = 0.0018263505771756172
iteration 284, loss = 0.0020178109407424927
iteration 285, loss = 0.0017335835145786405
iteration 286, loss = 0.0017679932061582804
iteration 287, loss = 0.002206088276579976
iteration 288, loss = 0.0020275621209293604
iteration 289, loss = 0.0016784659819677472
iteration 290, loss = 0.002007589442655444
iteration 291, loss = 0.002106095664203167
iteration 292, loss = 0.0018339098896831274
iteration 293, loss = 0.003182955551892519
iteration 294, loss = 0.0016112260054796934
iteration 295, loss = 0.001757077407091856
iteration 296, loss = 0.001965064788237214
iteration 297, loss = 0.0021379399113357067
iteration 298, loss = 0.0019347911002114415
iteration 299, loss = 0.00207581277936697
iteration 300, loss = 0.0017872643657028675
iteration 1, loss = 0.00167331681586802
iteration 2, loss = 0.0019578938372433186
iteration 3, loss = 0.0017616311088204384
iteration 4, loss = 0.0016852575354278088
iteration 5, loss = 0.0019277678802609444
iteration 6, loss = 0.0017563027795404196
iteration 7, loss = 0.0026874684263020754
iteration 8, loss = 0.0020638122223317623
iteration 9, loss = 0.0028416181448847055
iteration 10, loss = 0.0017978816758841276
iteration 11, loss = 0.0033293727319687605
iteration 12, loss = 0.0017072667833417654
iteration 13, loss = 0.0019841825123876333
iteration 14, loss = 0.002046438865363598
iteration 15, loss = 0.001891896827146411
iteration 16, loss = 0.001768528833054006
iteration 17, loss = 0.002428547479212284
iteration 18, loss = 0.002121546072885394
iteration 19, loss = 0.001913280226290226
iteration 20, loss = 0.00203148415312171
iteration 21, loss = 0.0020118835382163525
iteration 22, loss = 0.0022354775574058294
iteration 23, loss = 0.0018242287915199995
iteration 24, loss = 0.00224530347622931
iteration 25, loss = 0.0016183776315301657
iteration 26, loss = 0.002430975902825594
iteration 27, loss = 0.003327889135107398
iteration 28, loss = 0.001743204309605062
iteration 29, loss = 0.0017814007587730885
iteration 30, loss = 0.0020052136387676
iteration 31, loss = 0.0018369083991274238
iteration 32, loss = 0.0017597479745745659
iteration 33, loss = 0.002018978586420417
iteration 34, loss = 0.002463985001668334
iteration 35, loss = 0.0017924366984516382
iteration 36, loss = 0.00169289147015661
iteration 37, loss = 0.001791483024135232
iteration 38, loss = 0.00166115362662822
iteration 39, loss = 0.0021474172826856375
iteration 40, loss = 0.0018859116826206446
iteration 41, loss = 0.002238993067294359
iteration 42, loss = 0.0016543897800147533
iteration 43, loss = 0.0015891066286712885
iteration 44, loss = 0.0016134289326146245
iteration 45, loss = 0.0017586032627150416
iteration 46, loss = 0.0022128354758024216
iteration 47, loss = 0.0018152575939893723
iteration 48, loss = 0.0017182306619361043
iteration 49, loss = 0.0016958687920123339
iteration 50, loss = 0.0024130740202963352
iteration 51, loss = 0.0019422589102759957
iteration 52, loss = 0.0018072114326059818
iteration 53, loss = 0.001949512050487101
iteration 54, loss = 0.0022512718569487333
iteration 55, loss = 0.0017241057939827442
iteration 56, loss = 0.0018919563153758645
iteration 57, loss = 0.0017004379769787192
iteration 58, loss = 0.0018736824858933687
iteration 59, loss = 0.0017831994919106364
iteration 60, loss = 0.001957559958100319
iteration 61, loss = 0.001958155306056142
iteration 62, loss = 0.0018397456733509898
iteration 63, loss = 0.001878711162135005
iteration 64, loss = 0.0017075545620173216
iteration 65, loss = 0.0016805282793939114
iteration 66, loss = 0.0019607869908213615
iteration 67, loss = 0.001831764355301857
iteration 68, loss = 0.0016161680687218904
iteration 69, loss = 0.0018242322839796543
iteration 70, loss = 0.0018461485160514712
iteration 71, loss = 0.0032233018428087234
iteration 72, loss = 0.00183450011536479
iteration 73, loss = 0.0017138805706053972
iteration 74, loss = 0.0018627094104886055
iteration 75, loss = 0.0015819957479834557
iteration 76, loss = 0.00327739748172462
iteration 77, loss = 0.0017522030975669622
iteration 78, loss = 0.0018210216658189893
iteration 79, loss = 0.0017781726783141494
iteration 80, loss = 0.0017742785857990384
iteration 81, loss = 0.0018576356815174222
iteration 82, loss = 0.0017221674788743258
iteration 83, loss = 0.0025373869575560093
iteration 84, loss = 0.002282929141074419
iteration 85, loss = 0.001888230093754828
iteration 86, loss = 0.002213096246123314
iteration 87, loss = 0.001988478936254978
iteration 88, loss = 0.002518618479371071
iteration 89, loss = 0.0018329215236008167
iteration 90, loss = 0.0021025019232183695
iteration 91, loss = 0.0018757011275738478
iteration 92, loss = 0.0018744097324088216
iteration 93, loss = 0.0017884253757074475
iteration 94, loss = 0.0021025813184678555
iteration 95, loss = 0.0018690851284191012
iteration 96, loss = 0.0017276577418670058
iteration 97, loss = 0.0020367945544421673
iteration 98, loss = 0.001825783052481711
iteration 99, loss = 0.0018310572486370802
iteration 100, loss = 0.0018144483910873532
iteration 101, loss = 0.0030273839365690947
iteration 102, loss = 0.0019976033363491297
iteration 103, loss = 0.0031695347279310226
iteration 104, loss = 0.002118435688316822
iteration 105, loss = 0.0018622096395120025
iteration 106, loss = 0.0019323506858199835
iteration 107, loss = 0.002672027563676238
iteration 108, loss = 0.0016067905817180872
iteration 109, loss = 0.001854144036769867
iteration 110, loss = 0.0014717677840963006
iteration 111, loss = 0.0018131949473172426
iteration 112, loss = 0.0023359882179647684
iteration 113, loss = 0.0018596372101455927
iteration 114, loss = 0.0017015466000884771
iteration 115, loss = 0.002106997650116682
iteration 116, loss = 0.002950534224510193
iteration 117, loss = 0.0017604698659852147
iteration 118, loss = 0.0017284078057855368
iteration 119, loss = 0.00180931540671736
iteration 120, loss = 0.0018415641970932484
iteration 121, loss = 0.0017643943428993225
iteration 122, loss = 0.0022531268186867237
iteration 123, loss = 0.001797172357328236
iteration 124, loss = 0.001638026675209403
iteration 125, loss = 0.0015404195291921496
iteration 126, loss = 0.0017789013218134642
iteration 127, loss = 0.0018877428956329823
iteration 128, loss = 0.0016485548112541437
iteration 129, loss = 0.0019242207054048777
iteration 130, loss = 0.001691610785201192
iteration 131, loss = 0.002069037174805999
iteration 132, loss = 0.0017872183816507459
iteration 133, loss = 0.0017029748996719718
iteration 134, loss = 0.0017367801629006863
iteration 135, loss = 0.0017759520560503006
iteration 136, loss = 0.0018192112911492586
iteration 137, loss = 0.00233320496045053
iteration 138, loss = 0.0017366574611514807
iteration 139, loss = 0.001951864454895258
iteration 140, loss = 0.0019469817634671926
iteration 141, loss = 0.0019836104474961758
iteration 142, loss = 0.0016777992714196444
iteration 143, loss = 0.0019008852541446686
iteration 144, loss = 0.002534071682021022
iteration 145, loss = 0.002250610152259469
iteration 146, loss = 0.0019712159410119057
iteration 147, loss = 0.0020506465807557106
iteration 148, loss = 0.0017627388006076217
iteration 149, loss = 0.002615837613120675
iteration 150, loss = 0.002522335620597005
iteration 151, loss = 0.0020893625915050507
iteration 152, loss = 0.002577753970399499
iteration 153, loss = 0.001965023111552
iteration 154, loss = 0.0022100876085460186
iteration 155, loss = 0.0018079463625326753
iteration 156, loss = 0.0017905447166413069
iteration 157, loss = 0.0014837265480309725
iteration 158, loss = 0.0017107882304117084
iteration 159, loss = 0.002263283357024193
iteration 160, loss = 0.0018745207926258445
iteration 161, loss = 0.0017093756468966603
iteration 162, loss = 0.002556015970185399
iteration 163, loss = 0.0019930985290557146
iteration 164, loss = 0.0024565313942730427
iteration 165, loss = 0.002053493168205023
iteration 166, loss = 0.0018578488379716873
iteration 167, loss = 0.001971803605556488
iteration 168, loss = 0.0017192880623042583
iteration 169, loss = 0.0016676808008924127
iteration 170, loss = 0.0015779698733240366
iteration 171, loss = 0.002036809455603361
iteration 172, loss = 0.002123735612258315
iteration 173, loss = 0.0018800643738359213
iteration 174, loss = 0.0017259781016036868
iteration 175, loss = 0.0018051725346595049
iteration 176, loss = 0.001682543195784092
iteration 177, loss = 0.0018313572509214282
iteration 178, loss = 0.0021821754053235054
iteration 179, loss = 0.0023859753273427486
iteration 180, loss = 0.001560649136081338
iteration 181, loss = 0.0021618010941892862
iteration 182, loss = 0.0019027958624064922
iteration 183, loss = 0.0018690323922783136
iteration 184, loss = 0.001937982626259327
iteration 185, loss = 0.0018964254995808005
iteration 186, loss = 0.001810547080822289
iteration 187, loss = 0.001585095888003707
iteration 188, loss = 0.0016396440332755446
iteration 189, loss = 0.0018623516662046313
iteration 190, loss = 0.0016982390079647303
iteration 191, loss = 0.001627790741622448
iteration 192, loss = 0.0017837800551205873
iteration 193, loss = 0.0018258527852594852
iteration 194, loss = 0.0023677251301705837
iteration 195, loss = 0.0015278557548299432
iteration 196, loss = 0.0026405607350170612
iteration 197, loss = 0.0016037491150200367
iteration 198, loss = 0.0020317304879426956
iteration 199, loss = 0.0020516307558864355
iteration 200, loss = 0.0016668420284986496
iteration 201, loss = 0.0019712939392775297
iteration 202, loss = 0.0017583260778337717
iteration 203, loss = 0.0023168958723545074
iteration 204, loss = 0.0020657645072788
iteration 205, loss = 0.0018010219791904092
iteration 206, loss = 0.0016238470561802387
iteration 207, loss = 0.0016657095402479172
iteration 208, loss = 0.0019233684288337827
iteration 209, loss = 0.0019005388021469116
iteration 210, loss = 0.001799553050659597
iteration 211, loss = 0.0030508527997881174
iteration 212, loss = 0.001635143649764359
iteration 213, loss = 0.0033942386507987976
iteration 214, loss = 0.002073083072900772
iteration 215, loss = 0.0018175144214183092
iteration 216, loss = 0.001630362938158214
iteration 217, loss = 0.003301646327599883
iteration 218, loss = 0.001599166658706963
iteration 219, loss = 0.0016167070716619492
iteration 220, loss = 0.00176680414006114
iteration 221, loss = 0.0016407783841714263
iteration 222, loss = 0.0021962567698210478
iteration 223, loss = 0.001553672249428928
iteration 224, loss = 0.0016480854246765375
iteration 225, loss = 0.0017051029717549682
iteration 226, loss = 0.002958156866952777
iteration 227, loss = 0.0018304397817701101
iteration 228, loss = 0.0016428781673312187
iteration 229, loss = 0.0019229855388402939
iteration 230, loss = 0.001807225402444601
iteration 231, loss = 0.0016301540890708566
iteration 232, loss = 0.0019971358124166727
iteration 233, loss = 0.002363691572099924
iteration 234, loss = 0.001803671708330512
iteration 235, loss = 0.001652324222959578
iteration 236, loss = 0.0030415570363402367
iteration 237, loss = 0.001965027302503586
iteration 238, loss = 0.0018519680015742779
iteration 239, loss = 0.0017808638513088226
iteration 240, loss = 0.0021501898299902678
iteration 241, loss = 0.0015753473853692412
iteration 242, loss = 0.0034733987413346767
iteration 243, loss = 0.0015347161097452044
iteration 244, loss = 0.0017317490419372916
iteration 245, loss = 0.0015640482306480408
iteration 246, loss = 0.0018014961387962103
iteration 247, loss = 0.0016151034506037831
iteration 248, loss = 0.001973178004845977
iteration 249, loss = 0.0016458998434245586
iteration 250, loss = 0.001821097219362855
iteration 251, loss = 0.0017502910923212767
iteration 252, loss = 0.00205003609880805
iteration 253, loss = 0.0018444637535139918
iteration 254, loss = 0.0017799813067540526
iteration 255, loss = 0.0016799611039459705
iteration 256, loss = 0.0017928241286426783
iteration 257, loss = 0.0017858510836958885
iteration 258, loss = 0.0016952254809439182
iteration 259, loss = 0.0023216295521706343
iteration 260, loss = 0.001614413340575993
iteration 261, loss = 0.001717213075608015
iteration 262, loss = 0.001447921385988593
iteration 263, loss = 0.0016190304886549711
iteration 264, loss = 0.0016871675616130233
iteration 265, loss = 0.00170700007583946
iteration 266, loss = 0.0021358479280024767
iteration 267, loss = 0.001733821234665811
iteration 268, loss = 0.0024961470626294613
iteration 269, loss = 0.0016053052386268973
iteration 270, loss = 0.0015460681170225143
iteration 271, loss = 0.0016941312933340669
iteration 272, loss = 0.0016009934479370713
iteration 273, loss = 0.002125017112120986
iteration 274, loss = 0.001498798723332584
iteration 275, loss = 0.0025369487702846527
iteration 276, loss = 0.0016395259881392121
iteration 277, loss = 0.0020224342588335276
iteration 278, loss = 0.0018111413810402155
iteration 279, loss = 0.003421434899792075
iteration 280, loss = 0.0014925930881872773
iteration 281, loss = 0.0016979473875835538
iteration 282, loss = 0.0018800785765051842
iteration 283, loss = 0.0018836329691112041
iteration 284, loss = 0.003240348305553198
iteration 285, loss = 0.0018041037255898118
iteration 286, loss = 0.002577944193035364
iteration 287, loss = 0.0017490052850916982
iteration 288, loss = 0.002349320100620389
iteration 289, loss = 0.0015056831762194633
iteration 290, loss = 0.0017817110056057572
iteration 291, loss = 0.002502285409718752
iteration 292, loss = 0.0019279244588688016
iteration 293, loss = 0.0025152391754090786
iteration 294, loss = 0.0016665654256939888
iteration 295, loss = 0.0018720122752711177
iteration 296, loss = 0.002343161962926388
iteration 297, loss = 0.0017601754516363144
iteration 298, loss = 0.0016264242585748434
iteration 299, loss = 0.0017137487884610891
iteration 300, loss = 0.0017040164675563574
iteration 1, loss = 0.001829635351896286
iteration 2, loss = 0.0016726055182516575
iteration 3, loss = 0.0017526540905237198
iteration 4, loss = 0.003163886722177267
iteration 5, loss = 0.0017458491493016481
iteration 6, loss = 0.001719959662295878
iteration 7, loss = 0.0018845151644200087
iteration 8, loss = 0.0017779319314286113
iteration 9, loss = 0.0017131667118519545
iteration 10, loss = 0.0015773293562233448
iteration 11, loss = 0.003053326392546296
iteration 12, loss = 0.0015933515969663858
iteration 13, loss = 0.002131346147507429
iteration 14, loss = 0.002164228353649378
iteration 15, loss = 0.0015139427268877625
iteration 16, loss = 0.0017180894501507282
iteration 17, loss = 0.002085791202262044
iteration 18, loss = 0.0015763554256409407
iteration 19, loss = 0.0018230036366730928
iteration 20, loss = 0.0017123594880104065
iteration 21, loss = 0.0016416050493717194
iteration 22, loss = 0.0015887556364759803
iteration 23, loss = 0.0024650362320244312
iteration 24, loss = 0.0018196385353803635
iteration 25, loss = 0.00166883016936481
iteration 26, loss = 0.00188805116340518
iteration 27, loss = 0.001548085710965097
iteration 28, loss = 0.0017931235488504171
iteration 29, loss = 0.0015679041389375925
iteration 30, loss = 0.0019705307204276323
iteration 31, loss = 0.0016240550903603435
iteration 32, loss = 0.0015905472682788968
iteration 33, loss = 0.0015977147268131375
iteration 34, loss = 0.0018810853362083435
iteration 35, loss = 0.0015560627216473222
iteration 36, loss = 0.0017228899523615837
iteration 37, loss = 0.0016959825297817588
iteration 38, loss = 0.001664717448875308
iteration 39, loss = 0.001972879283130169
iteration 40, loss = 0.0018009019549936056
iteration 41, loss = 0.0016071770805865526
iteration 42, loss = 0.001698190113529563
iteration 43, loss = 0.003365653334185481
iteration 44, loss = 0.0021411695051938295
iteration 45, loss = 0.0016430882969871163
iteration 46, loss = 0.0018951831152662635
iteration 47, loss = 0.001870722509920597
iteration 48, loss = 0.0014913592021912336
iteration 49, loss = 0.0015842884313315153
iteration 50, loss = 0.0015886102337390184
iteration 51, loss = 0.0016714460216462612
iteration 52, loss = 0.001726099057123065
iteration 53, loss = 0.0018995560240000486
iteration 54, loss = 0.0014603867894038558
iteration 55, loss = 0.0016982955858111382
iteration 56, loss = 0.0016678356332704425
iteration 57, loss = 0.0016468045068904757
iteration 58, loss = 0.0015369694447144866
iteration 59, loss = 0.0015548650408163667
iteration 60, loss = 0.0028320278506726027
iteration 61, loss = 0.0015769052552059293
iteration 62, loss = 0.0018260213546454906
iteration 63, loss = 0.0024915807880461216
iteration 64, loss = 0.0015331325121223927
iteration 65, loss = 0.0016678121173754334
iteration 66, loss = 0.0022134336177259684
iteration 67, loss = 0.0015139055904000998
iteration 68, loss = 0.00182903534732759
iteration 69, loss = 0.0017333452124148607
iteration 70, loss = 0.0018444398883730173
iteration 71, loss = 0.003855418646708131
iteration 72, loss = 0.001530050067231059
iteration 73, loss = 0.0015658243792131543
iteration 74, loss = 0.0018743289401754737
iteration 75, loss = 0.0016696866368874907
iteration 76, loss = 0.0017773896688595414
iteration 77, loss = 0.0017461477546021342
iteration 78, loss = 0.001940783578902483
iteration 79, loss = 0.001980771776288748
iteration 80, loss = 0.002515226136893034
iteration 81, loss = 0.0016817637952044606
iteration 82, loss = 0.0017453264445066452
iteration 83, loss = 0.001773729338310659
iteration 84, loss = 0.0015536858700215816
iteration 85, loss = 0.0017617682460695505
iteration 86, loss = 0.0015767714940011501
iteration 87, loss = 0.0017717466689646244
iteration 88, loss = 0.0018486250191926956
iteration 89, loss = 0.002625503810122609
iteration 90, loss = 0.001543531776405871
iteration 91, loss = 0.0017321143532171845
iteration 92, loss = 0.0017707287333905697
iteration 93, loss = 0.0015655147144570947
iteration 94, loss = 0.0018849493935704231
iteration 95, loss = 0.0015134213026612997
iteration 96, loss = 0.0016417992301285267
iteration 97, loss = 0.0019537601619958878
iteration 98, loss = 0.0015482750022783875
iteration 99, loss = 0.0016300400020554662
iteration 100, loss = 0.0016127323033288121
iteration 101, loss = 0.0022595971822738647
iteration 102, loss = 0.0022257091477513313
iteration 103, loss = 0.0016905720112845302
iteration 104, loss = 0.003003532299771905
iteration 105, loss = 0.001810374204069376
iteration 106, loss = 0.0017965789884328842
iteration 107, loss = 0.0017064122948795557
iteration 108, loss = 0.002299099462106824
iteration 109, loss = 0.0017119802068918943
iteration 110, loss = 0.0024264995008707047
iteration 111, loss = 0.0016471118433400989
iteration 112, loss = 0.0018091857200488448
iteration 113, loss = 0.0017785726813599467
iteration 114, loss = 0.0023457908537238836
iteration 115, loss = 0.002023395849391818
iteration 116, loss = 0.0016471121925860643
iteration 117, loss = 0.0015106953214854002
iteration 118, loss = 0.0016882038908079267
iteration 119, loss = 0.0019804753828793764
iteration 120, loss = 0.001501896302215755
iteration 121, loss = 0.001534673385322094
iteration 122, loss = 0.0017186182085424662
iteration 123, loss = 0.0016508528497070074
iteration 124, loss = 0.0016549716237932444
iteration 125, loss = 0.0021180410403758287
iteration 126, loss = 0.002243644092231989
iteration 127, loss = 0.0018711329903453588
iteration 128, loss = 0.0018276680493727326
iteration 129, loss = 0.002464151941239834
iteration 130, loss = 0.0021087806671857834
iteration 131, loss = 0.0024196954909712076
iteration 132, loss = 0.0014203276950865984
iteration 133, loss = 0.002067727269604802
iteration 134, loss = 0.001973990350961685
iteration 135, loss = 0.0014848413411527872
iteration 136, loss = 0.001533971051685512
iteration 137, loss = 0.0016258719842880964
iteration 138, loss = 0.0019531669095158577
iteration 139, loss = 0.002547728130593896
iteration 140, loss = 0.0017191704828292131
iteration 141, loss = 0.0014406413538381457
iteration 142, loss = 0.0014772539725527167
iteration 143, loss = 0.0028153012972325087
iteration 144, loss = 0.0019155029440298676
iteration 145, loss = 0.001763498759828508
iteration 146, loss = 0.0029230834916234016
iteration 147, loss = 0.001543095102533698
iteration 148, loss = 0.0017161009600386024
iteration 149, loss = 0.001767784240655601
iteration 150, loss = 0.0019907639361917973
iteration 151, loss = 0.0014736589509993792
iteration 152, loss = 0.0018421713029965758
iteration 153, loss = 0.0028140966314822435
iteration 154, loss = 0.0015969767700880766
iteration 155, loss = 0.0021040725987404585
iteration 156, loss = 0.001765160821378231
iteration 157, loss = 0.0021609205286949873
iteration 158, loss = 0.0016187004512175918
iteration 159, loss = 0.0020929803140461445
iteration 160, loss = 0.0017844808753579855
iteration 161, loss = 0.0016302941367030144
iteration 162, loss = 0.0017267403891310096
iteration 163, loss = 0.0020468125585466623
iteration 164, loss = 0.001719872117973864
iteration 165, loss = 0.002438702154904604
iteration 166, loss = 0.001732500153593719
iteration 167, loss = 0.0017965196166187525
iteration 168, loss = 0.00232833088375628
iteration 169, loss = 0.0021627808455377817
iteration 170, loss = 0.001550163491629064
iteration 171, loss = 0.0016172360628843307
iteration 172, loss = 0.001670872326940298
iteration 173, loss = 0.001557288458570838
iteration 174, loss = 0.0016175346681848168
iteration 175, loss = 0.001988478936254978
iteration 176, loss = 0.001529195113107562
iteration 177, loss = 0.001988142728805542
iteration 178, loss = 0.0017740350449457765
iteration 179, loss = 0.002128052990883589
iteration 180, loss = 0.0029436456970870495
iteration 181, loss = 0.0021195514127612114
iteration 182, loss = 0.001575845992192626
iteration 183, loss = 0.0019539999775588512
iteration 184, loss = 0.0017300535691902041
iteration 185, loss = 0.00174433970823884
iteration 186, loss = 0.0016872738488018513
iteration 187, loss = 0.0022386705968528986
iteration 188, loss = 0.0017503988929092884
iteration 189, loss = 0.0018180906772613525
iteration 190, loss = 0.0019890074618160725
iteration 191, loss = 0.0016042221104726195
iteration 192, loss = 0.001923148985952139
iteration 193, loss = 0.0017793457955121994
iteration 194, loss = 0.0024046525359153748
iteration 195, loss = 0.00180631666444242
iteration 196, loss = 0.002151210093870759
iteration 197, loss = 0.0016802276950329542
iteration 198, loss = 0.0015175123699009418
iteration 199, loss = 0.0014074211940169334
iteration 200, loss = 0.0025104652158915997
iteration 201, loss = 0.0014553568325936794
iteration 202, loss = 0.0018020301358774304
iteration 203, loss = 0.0017683537444099784
iteration 204, loss = 0.002744065597653389
iteration 205, loss = 0.002255934989079833
iteration 206, loss = 0.0014861624222248793
iteration 207, loss = 0.001599798328243196
iteration 208, loss = 0.0016203521518036723
iteration 209, loss = 0.0020690779201686382
iteration 210, loss = 0.0018490221118554473
iteration 211, loss = 0.0018140847096219659
iteration 212, loss = 0.0018555020214989781
iteration 213, loss = 0.0017291372641921043
iteration 214, loss = 0.0014861300587654114
iteration 215, loss = 0.0025280313566327095
iteration 216, loss = 0.0015074467519298196
iteration 217, loss = 0.0015131670515984297
iteration 218, loss = 0.0017666351050138474
iteration 219, loss = 0.001641104114241898
iteration 220, loss = 0.002075795317068696
iteration 221, loss = 0.0015122625045478344
iteration 222, loss = 0.0015848276671022177
iteration 223, loss = 0.0016667703166604042
iteration 224, loss = 0.0018635126762092113
iteration 225, loss = 0.0016424884088337421
iteration 226, loss = 0.0017532347701489925
iteration 227, loss = 0.0017063391860574484
iteration 228, loss = 0.0015942093450576067
iteration 229, loss = 0.0016330969519913197
iteration 230, loss = 0.0017020476516336203
iteration 231, loss = 0.0018260591896250844
iteration 232, loss = 0.002782069379463792
iteration 233, loss = 0.0018417240353301167
iteration 234, loss = 0.00157201720867306
iteration 235, loss = 0.0016492976574227214
iteration 236, loss = 0.0015964906197041273
iteration 237, loss = 0.001672829850576818
iteration 238, loss = 0.0018112857360392809
iteration 239, loss = 0.0014487479347735643
iteration 240, loss = 0.001681544235907495
iteration 241, loss = 0.0028586878906935453
iteration 242, loss = 0.0027489489875733852
iteration 243, loss = 0.002278056228533387
iteration 244, loss = 0.0023399884812533855
iteration 245, loss = 0.0018634069710969925
iteration 246, loss = 0.0017385458340868354
iteration 247, loss = 0.0016039481852203608
iteration 248, loss = 0.0026986338198184967
iteration 249, loss = 0.0019355766708031297
iteration 250, loss = 0.0018348535522818565
iteration 251, loss = 0.001483476022258401
iteration 252, loss = 0.0017042105318978429
iteration 253, loss = 0.0016783879837021232
iteration 254, loss = 0.0018305680714547634
iteration 255, loss = 0.0016673392383381724
iteration 256, loss = 0.001642431365326047
iteration 257, loss = 0.0029231274966150522
iteration 258, loss = 0.001924309297464788
iteration 259, loss = 0.0018859525443986058
iteration 260, loss = 0.0019614901393651962
iteration 261, loss = 0.0015934465918689966
iteration 262, loss = 0.0015818435931578279
iteration 263, loss = 0.0017229726072400808
iteration 264, loss = 0.0019611697643995285
iteration 265, loss = 0.0015423162840306759
iteration 266, loss = 0.001867916202172637
iteration 267, loss = 0.0015714215114712715
iteration 268, loss = 0.0033245282247662544
iteration 269, loss = 0.00199069082736969
iteration 270, loss = 0.0017183955060318112
iteration 271, loss = 0.0018346495926380157
iteration 272, loss = 0.0021756014320999384
iteration 273, loss = 0.0017359702615067363
iteration 274, loss = 0.0019516355823725462
iteration 275, loss = 0.001860157586634159
iteration 276, loss = 0.002066711662337184
iteration 277, loss = 0.001918416703119874
iteration 278, loss = 0.0015536334831267595
iteration 279, loss = 0.0017502703703939915
iteration 280, loss = 0.002039610641077161
iteration 281, loss = 0.001823627739213407
iteration 282, loss = 0.0020479694940149784
iteration 283, loss = 0.0018072753446176648
iteration 284, loss = 0.0021744370460510254
iteration 285, loss = 0.0018352032639086246
iteration 286, loss = 0.0019307804759591818
iteration 287, loss = 0.0017155652167275548
iteration 288, loss = 0.001650236314162612
iteration 289, loss = 0.0015417768154293299
iteration 290, loss = 0.001590483239851892
iteration 291, loss = 0.0018707261187955737
iteration 292, loss = 0.0018543123733252287
iteration 293, loss = 0.001793385366909206
iteration 294, loss = 0.0016064903466030955
iteration 295, loss = 0.0016944243106991053
iteration 296, loss = 0.0017646129708737135
iteration 297, loss = 0.0029863277450203896
iteration 298, loss = 0.001563443336635828
iteration 299, loss = 0.0019788790959864855
iteration 300, loss = 0.001964002847671509
iteration 1, loss = 0.0027991465758532286
iteration 2, loss = 0.0016063239891082048
iteration 3, loss = 0.0017680940218269825
iteration 4, loss = 0.00169095816090703
iteration 5, loss = 0.002262716181576252
iteration 6, loss = 0.001446916488930583
iteration 7, loss = 0.0016093788435682654
iteration 8, loss = 0.0016732715303078294
iteration 9, loss = 0.0017634171526879072
iteration 10, loss = 0.002444111043587327
iteration 11, loss = 0.0019377726130187511
iteration 12, loss = 0.0015145707875490189
iteration 13, loss = 0.0016915003070607781
iteration 14, loss = 0.0017080773832276464
iteration 15, loss = 0.0021088127978146076
iteration 16, loss = 0.001653484534472227
iteration 17, loss = 0.001737608457915485
iteration 18, loss = 0.0017350052949041128
iteration 19, loss = 0.0014921627007424831
iteration 20, loss = 0.0016989379655569792
iteration 21, loss = 0.001766770612448454
iteration 22, loss = 0.0017432468011975288
iteration 23, loss = 0.0016175983473658562
iteration 24, loss = 0.0018825579900294542
iteration 25, loss = 0.0017889412119984627
iteration 26, loss = 0.0014659917214885354
iteration 27, loss = 0.0016258483519777656
iteration 28, loss = 0.001713909674435854
iteration 29, loss = 0.001689062686637044
iteration 30, loss = 0.0015077857533469796
iteration 31, loss = 0.0016719904961064458
iteration 32, loss = 0.0017633304232731462
iteration 33, loss = 0.0020897670183330774
iteration 34, loss = 0.0017271373653784394
iteration 35, loss = 0.0018785791471600533
iteration 36, loss = 0.0017238458385691047
iteration 37, loss = 0.0026948535814881325
iteration 38, loss = 0.0017324115615338087
iteration 39, loss = 0.0016403169138357043
iteration 40, loss = 0.0017262195469811559
iteration 41, loss = 0.001804750645533204
iteration 42, loss = 0.0017534228973090649
iteration 43, loss = 0.0016636814689263701
iteration 44, loss = 0.0016548184212297201
iteration 45, loss = 0.002026095986366272
iteration 46, loss = 0.0016571254236623645
iteration 47, loss = 0.002883263397961855
iteration 48, loss = 0.0018825494917109609
iteration 49, loss = 0.0018195339944213629
iteration 50, loss = 0.0017807476688176394
iteration 51, loss = 0.0017155951354652643
iteration 52, loss = 0.0015193402068689466
iteration 53, loss = 0.0016054105944931507
iteration 54, loss = 0.0016457292949780822
iteration 55, loss = 0.0015069094952195883
iteration 56, loss = 0.0030067404732108116
iteration 57, loss = 0.001609865459613502
iteration 58, loss = 0.001707970630377531
iteration 59, loss = 0.001623728428967297
iteration 60, loss = 0.001832420937716961
iteration 61, loss = 0.0029576527886092663
iteration 62, loss = 0.0015444994205608964
iteration 63, loss = 0.0017702990444377065
iteration 64, loss = 0.0018638203619048
iteration 65, loss = 0.001615131855942309
iteration 66, loss = 0.00202090828679502
iteration 67, loss = 0.0021301433444023132
iteration 68, loss = 0.0022101770155131817
iteration 69, loss = 0.0016517480835318565
iteration 70, loss = 0.001479396247304976
iteration 71, loss = 0.0018485547043383121
iteration 72, loss = 0.0016635166248306632
iteration 73, loss = 0.00162021373398602
iteration 74, loss = 0.0017813635058701038
iteration 75, loss = 0.0022611976601183414
iteration 76, loss = 0.0017091806512326002
iteration 77, loss = 0.001763197360560298
iteration 78, loss = 0.0015335575444623828
iteration 79, loss = 0.0015072912210598588
iteration 80, loss = 0.0024596850853413343
iteration 81, loss = 0.001972433179616928
iteration 82, loss = 0.0017240203451365232
iteration 83, loss = 0.00153645861428231
iteration 84, loss = 0.0029115229845046997
iteration 85, loss = 0.0020194631069898605
iteration 86, loss = 0.00173559773247689
iteration 87, loss = 0.0016579568618908525
iteration 88, loss = 0.001763055450282991
iteration 89, loss = 0.0016454494325444102
iteration 90, loss = 0.0015949095832183957
iteration 91, loss = 0.0019198841182515025
iteration 92, loss = 0.002256016945466399
iteration 93, loss = 0.0016557611525058746
iteration 94, loss = 0.0015971915563568473
iteration 95, loss = 0.001800477970391512
iteration 96, loss = 0.0017363482620567083
iteration 97, loss = 0.001507625333033502
iteration 98, loss = 0.0018756692297756672
iteration 99, loss = 0.0017248665681108832
iteration 100, loss = 0.0016879866598173976
iteration 101, loss = 0.0016200869577005506
iteration 102, loss = 0.002548250136896968
iteration 103, loss = 0.0018408573232591152
iteration 104, loss = 0.002790257800370455
iteration 105, loss = 0.0017154616070911288
iteration 106, loss = 0.002990891458466649
iteration 107, loss = 0.0016796750715002418
iteration 108, loss = 0.0015047156484797597
iteration 109, loss = 0.0022970489226281643
iteration 110, loss = 0.002518274588510394
iteration 111, loss = 0.002475565765053034
iteration 112, loss = 0.001966380048543215
iteration 113, loss = 0.0020335365552455187
iteration 114, loss = 0.0017327936366200447
iteration 115, loss = 0.00164307770319283
iteration 116, loss = 0.0014731052797287703
iteration 117, loss = 0.0030862095300108194
iteration 118, loss = 0.0016856128349900246
iteration 119, loss = 0.0019283292349427938
iteration 120, loss = 0.0015988221857696772
iteration 121, loss = 0.00249711936339736
iteration 122, loss = 0.0015173297142609954
iteration 123, loss = 0.0017671823734417558
iteration 124, loss = 0.0016324385069310665
iteration 125, loss = 0.002286233240738511
iteration 126, loss = 0.0016328938072547317
iteration 127, loss = 0.0016864255303516984
iteration 128, loss = 0.0019694168586283922
iteration 129, loss = 0.002316167578101158
iteration 130, loss = 0.0030870484188199043
iteration 131, loss = 0.0017016741912811995
iteration 132, loss = 0.00172615482006222
iteration 133, loss = 0.0023616652470082045
iteration 134, loss = 0.0015851458301767707
iteration 135, loss = 0.001871383050456643
iteration 136, loss = 0.001629065489396453
iteration 137, loss = 0.001696081948466599
iteration 138, loss = 0.0016265056328848004
iteration 139, loss = 0.0020332199055701494
iteration 140, loss = 0.0014749157708138227
iteration 141, loss = 0.0016012784326449037
iteration 142, loss = 0.0014914419734850526
iteration 143, loss = 0.001784477848559618
iteration 144, loss = 0.0016299131093546748
iteration 145, loss = 0.001478498918004334
iteration 146, loss = 0.002000888343900442
iteration 147, loss = 0.0015258550411090255
iteration 148, loss = 0.0020953111816197634
iteration 149, loss = 0.002274920931085944
iteration 150, loss = 0.0018408102914690971
iteration 151, loss = 0.0014972517965361476
iteration 152, loss = 0.0015999134629964828
iteration 153, loss = 0.0015954893315210938
iteration 154, loss = 0.0016598457004874945
iteration 155, loss = 0.0017044940032064915
iteration 156, loss = 0.0017974645597860217
iteration 157, loss = 0.001592888031154871
iteration 158, loss = 0.001778135891072452
iteration 159, loss = 0.0018163021886721253
iteration 160, loss = 0.002093598246574402
iteration 161, loss = 0.0016797975404188037
iteration 162, loss = 0.0017947431188076735
iteration 163, loss = 0.0019308628980070353
iteration 164, loss = 0.002010638825595379
iteration 165, loss = 0.0029258164577186108
iteration 166, loss = 0.0017240472370758653
iteration 167, loss = 0.0018752474570646882
iteration 168, loss = 0.0018875125097110868
iteration 169, loss = 0.0015982536133378744
iteration 170, loss = 0.001898081274703145
iteration 171, loss = 0.0015471006045117974
iteration 172, loss = 0.0016388854710385203
iteration 173, loss = 0.0014943123096600175
iteration 174, loss = 0.0017167659243568778
iteration 175, loss = 0.0018359150271862745
iteration 176, loss = 0.0015792074846103787
iteration 177, loss = 0.00292965373955667
iteration 178, loss = 0.0014609987847507
iteration 179, loss = 0.0015530403470620513
iteration 180, loss = 0.0016393349505960941
iteration 181, loss = 0.0015460930299013853
iteration 182, loss = 0.0013474825536832213
iteration 183, loss = 0.0014522074488922954
iteration 184, loss = 0.0017401955556124449
iteration 185, loss = 0.0030702778603881598
iteration 186, loss = 0.0015717075439170003
iteration 187, loss = 0.0019376227864995599
iteration 188, loss = 0.0016752724768593907
iteration 189, loss = 0.0015020930441096425
iteration 190, loss = 0.0020217737182974815
iteration 191, loss = 0.0032365755178034306
iteration 192, loss = 0.0018393214559182525
iteration 193, loss = 0.0019015392754226923
iteration 194, loss = 0.001676999032497406
iteration 195, loss = 0.002259774599224329
iteration 196, loss = 0.0017415332840755582
iteration 197, loss = 0.0018643287476152182
iteration 198, loss = 0.0020938238594681025
iteration 199, loss = 0.001754560973495245
iteration 200, loss = 0.0015603285282850266
iteration 201, loss = 0.0017717742593958974
iteration 202, loss = 0.0019403932383283973
iteration 203, loss = 0.0017364973900839686
iteration 204, loss = 0.0018704484682530165
iteration 205, loss = 0.0018733673496171832
iteration 206, loss = 0.0029905864503234625
iteration 207, loss = 0.0025602371897548437
iteration 208, loss = 0.0014224154874682426
iteration 209, loss = 0.00165296730119735
iteration 210, loss = 0.0016600916860625148
iteration 211, loss = 0.0014739410253241658
iteration 212, loss = 0.0015759927919134498
iteration 213, loss = 0.0015701702795922756
iteration 214, loss = 0.002776063745841384
iteration 215, loss = 0.0017268317751586437
iteration 216, loss = 0.001367424731142819
iteration 217, loss = 0.0018278195057064295
iteration 218, loss = 0.0017703461926430464
iteration 219, loss = 0.002033689757809043
iteration 220, loss = 0.001681367284618318
iteration 221, loss = 0.0014889341546222568
iteration 222, loss = 0.0015687334816902876
iteration 223, loss = 0.0017769872210919857
iteration 224, loss = 0.0016802044119685888
iteration 225, loss = 0.0018339422531425953
iteration 226, loss = 0.0021924099419265985
iteration 227, loss = 0.0017743605421856046
iteration 228, loss = 0.0021135297138243914
iteration 229, loss = 0.0015394935617223382
iteration 230, loss = 0.001795678399503231
iteration 231, loss = 0.0022343904711306095
iteration 232, loss = 0.0021264480892568827
iteration 233, loss = 0.0016180548118427396
iteration 234, loss = 0.001778693636879325
iteration 235, loss = 0.0016443046042695642
iteration 236, loss = 0.00146283523645252
iteration 237, loss = 0.0018123353365808725
iteration 238, loss = 0.0018230308778584003
iteration 239, loss = 0.0016379787120968103
iteration 240, loss = 0.0023974820505827665
iteration 241, loss = 0.001634664018638432
iteration 242, loss = 0.0017451751045882702
iteration 243, loss = 0.0017722055781632662
iteration 244, loss = 0.001895706169307232
iteration 245, loss = 0.0018135353457182646
iteration 246, loss = 0.002574481535702944
iteration 247, loss = 0.0017011368181556463
iteration 248, loss = 0.002036555204540491
iteration 249, loss = 0.0017549532931298018
iteration 250, loss = 0.002753104781731963
iteration 251, loss = 0.0017115466762334108
iteration 252, loss = 0.002911892021074891
iteration 253, loss = 0.0016019700560718775
iteration 254, loss = 0.0017563230358064175
iteration 255, loss = 0.001460702740587294
iteration 256, loss = 0.001746812486089766
iteration 257, loss = 0.002176892012357712
iteration 258, loss = 0.0016356543637812138
iteration 259, loss = 0.0015848968178033829
iteration 260, loss = 0.0016771957743912935
iteration 261, loss = 0.002624044194817543
iteration 262, loss = 0.0017143071163445711
iteration 263, loss = 0.0019253931241109967
iteration 264, loss = 0.0017546844901517034
iteration 265, loss = 0.0017878025537356734
iteration 266, loss = 0.0016625374555587769
iteration 267, loss = 0.002168636303395033
iteration 268, loss = 0.0017845765687525272
iteration 269, loss = 0.00195046397857368
iteration 270, loss = 0.0018842275021597743
iteration 271, loss = 0.0017649134388193488
iteration 272, loss = 0.0021753220353275537
iteration 273, loss = 0.0019080829806625843
iteration 274, loss = 0.002342278603464365
iteration 275, loss = 0.0016672599595040083
iteration 276, loss = 0.0020027942955493927
iteration 277, loss = 0.002546215197071433
iteration 278, loss = 0.001632699160836637
iteration 279, loss = 0.001566158840432763
iteration 280, loss = 0.0022263468708842993
iteration 281, loss = 0.0019934517331421375
iteration 282, loss = 0.001585116726346314
iteration 283, loss = 0.0016336694825440645
iteration 284, loss = 0.0016759778372943401
iteration 285, loss = 0.0017751813866198063
iteration 286, loss = 0.0021213011350482702
iteration 287, loss = 0.001912822830490768
iteration 288, loss = 0.001667012576945126
iteration 289, loss = 0.0021107359789311886
iteration 290, loss = 0.0019160691881552339
iteration 291, loss = 0.0018629408441483974
iteration 292, loss = 0.001424749381840229
iteration 293, loss = 0.001635718741454184
iteration 294, loss = 0.0015289974398911
iteration 295, loss = 0.0015095644630491734
iteration 296, loss = 0.0018040240975096822
iteration 297, loss = 0.0015376174123957753
iteration 298, loss = 0.0015821855049580336
iteration 299, loss = 0.0017165703466162086
iteration 300, loss = 0.0016240108525380492
iteration 1, loss = 0.0014936152147129178
iteration 2, loss = 0.0020450972951948643
iteration 3, loss = 0.00151577009819448
iteration 4, loss = 0.0016243604477494955
iteration 5, loss = 0.0018808868480846286
iteration 6, loss = 0.001956365304067731
iteration 7, loss = 0.0014709389070048928
iteration 8, loss = 0.0016421354375779629
iteration 9, loss = 0.0015201946953311563
iteration 10, loss = 0.0014115410158410668
iteration 11, loss = 0.0015130636747926474
iteration 12, loss = 0.0018152737757191062
iteration 13, loss = 0.0025242269039154053
iteration 14, loss = 0.00191037030890584
iteration 15, loss = 0.0020681973546743393
iteration 16, loss = 0.0018706966657191515
iteration 17, loss = 0.0016890825936570764
iteration 18, loss = 0.0024654134176671505
iteration 19, loss = 0.0016868510283529758
iteration 20, loss = 0.0020398376509547234
iteration 21, loss = 0.0015966222854331136
iteration 22, loss = 0.0016087311087176204
iteration 23, loss = 0.002311538904905319
iteration 24, loss = 0.0029693739488720894
iteration 25, loss = 0.002278477419167757
iteration 26, loss = 0.0017008441500365734
iteration 27, loss = 0.0015352338086813688
iteration 28, loss = 0.0016435844590887427
iteration 29, loss = 0.0016944179078564048
iteration 30, loss = 0.0016029030084609985
iteration 31, loss = 0.0016552114393562078
iteration 32, loss = 0.001616792636923492
iteration 33, loss = 0.0016657167579978704
iteration 34, loss = 0.0015308484435081482
iteration 35, loss = 0.0017440748633816838
iteration 36, loss = 0.0013850872637704015
iteration 37, loss = 0.0017203770112246275
iteration 38, loss = 0.0018323840340599418
iteration 39, loss = 0.0017266153590753675
iteration 40, loss = 0.0016152388416230679
iteration 41, loss = 0.0016733994707465172
iteration 42, loss = 0.0019127047853544354
iteration 43, loss = 0.002115682465955615
iteration 44, loss = 0.001768466318026185
iteration 45, loss = 0.0016711816424503922
iteration 46, loss = 0.0023826390970498323
iteration 47, loss = 0.0016346749616786838
iteration 48, loss = 0.0015441206051036716
iteration 49, loss = 0.0014037664514034986
iteration 50, loss = 0.002156409900635481
iteration 51, loss = 0.0016695817466825247
iteration 52, loss = 0.0018176351441070437
iteration 53, loss = 0.0017793849110603333
iteration 54, loss = 0.001885679317638278
iteration 55, loss = 0.0017508624587208033
iteration 56, loss = 0.0019650370813906193
iteration 57, loss = 0.0018764910055324435
iteration 58, loss = 0.0018103725742548704
iteration 59, loss = 0.0019750192295759916
iteration 60, loss = 0.0015028230845928192
iteration 61, loss = 0.002212963066995144
iteration 62, loss = 0.0017386899562552571
iteration 63, loss = 0.002479882910847664
iteration 64, loss = 0.001661737565882504
iteration 65, loss = 0.0017925141146406531
iteration 66, loss = 0.002009779214859009
iteration 67, loss = 0.0014882417162880301
iteration 68, loss = 0.0017213312676176429
iteration 69, loss = 0.0016737221740186214
iteration 70, loss = 0.0016097192419692874
iteration 71, loss = 0.0017942971317097545
iteration 72, loss = 0.0027026168536394835
iteration 73, loss = 0.00175570510327816
iteration 74, loss = 0.001933243707753718
iteration 75, loss = 0.001993112964555621
iteration 76, loss = 0.001519126002676785
iteration 77, loss = 0.0015273474855348468
iteration 78, loss = 0.0018219680059701204
iteration 79, loss = 0.0015331801259890199
iteration 80, loss = 0.0029502063989639282
iteration 81, loss = 0.0017794463783502579
iteration 82, loss = 0.0014885091222822666
iteration 83, loss = 0.0017306882655248046
iteration 84, loss = 0.0015357950469478965
iteration 85, loss = 0.0023134364746510983
iteration 86, loss = 0.0015761887189000845
iteration 87, loss = 0.0015737873036414385
iteration 88, loss = 0.0015224124072119594
iteration 89, loss = 0.0019210923928767443
iteration 90, loss = 0.0024667116813361645
iteration 91, loss = 0.0016014279099181294
iteration 92, loss = 0.001631774241104722
iteration 93, loss = 0.0023431135341525078
iteration 94, loss = 0.0016297856345772743
iteration 95, loss = 0.0017627455526962876
iteration 96, loss = 0.0015290635637938976
iteration 97, loss = 0.0016897076275199652
iteration 98, loss = 0.0017069571185857058
iteration 99, loss = 0.00189563212916255
iteration 100, loss = 0.0017686949577182531
iteration 101, loss = 0.001540654106065631
iteration 102, loss = 0.0019127780105918646
iteration 103, loss = 0.001674446975812316
iteration 104, loss = 0.0015486269257962704
iteration 105, loss = 0.0016946144169196486
iteration 106, loss = 0.001492671319283545
iteration 107, loss = 0.0016515551833435893
iteration 108, loss = 0.0018571147229522467
iteration 109, loss = 0.001686470932327211
iteration 110, loss = 0.0016400616150349379
iteration 111, loss = 0.00183344807010144
iteration 112, loss = 0.0017427371349185705
iteration 113, loss = 0.0017578393453732133
iteration 114, loss = 0.0017754295840859413
iteration 115, loss = 0.0022516597528010607
iteration 116, loss = 0.0017928396118804812
iteration 117, loss = 0.0022131798323243856
iteration 118, loss = 0.002394269220530987
iteration 119, loss = 0.0017795334570109844
iteration 120, loss = 0.0016177583020180464
iteration 121, loss = 0.0014594054082408547
iteration 122, loss = 0.002213980769738555
iteration 123, loss = 0.0016768447821959853
iteration 124, loss = 0.0015902036102488637
iteration 125, loss = 0.0015979515155777335
iteration 126, loss = 0.0028845560736954212
iteration 127, loss = 0.0018481389852240682
iteration 128, loss = 0.0018888069316744804
iteration 129, loss = 0.0015178155153989792
iteration 130, loss = 0.0017398360650986433
iteration 131, loss = 0.001802744809538126
iteration 132, loss = 0.0016371408710256219
iteration 133, loss = 0.0017877499340102077
iteration 134, loss = 0.0019955255556851625
iteration 135, loss = 0.0018855478847399354
iteration 136, loss = 0.001839571981690824
iteration 137, loss = 0.0018365830183029175
iteration 138, loss = 0.00270838662981987
iteration 139, loss = 0.001625718898139894
iteration 140, loss = 0.0015296803321689367
iteration 141, loss = 0.0016463116044178605
iteration 142, loss = 0.0016254361253231764
iteration 143, loss = 0.001692936522886157
iteration 144, loss = 0.0016271808417513967
iteration 145, loss = 0.0020401766523718834
iteration 146, loss = 0.0019147589337080717
iteration 147, loss = 0.001872083405032754
iteration 148, loss = 0.0016910741105675697
iteration 149, loss = 0.0016358657740056515
iteration 150, loss = 0.0015671293949708343
iteration 151, loss = 0.0016229529865086079
iteration 152, loss = 0.001536227180622518
iteration 153, loss = 0.001918413327075541
iteration 154, loss = 0.0019167371792718768
iteration 155, loss = 0.0020462062675505877
iteration 156, loss = 0.0017216430278494954
iteration 157, loss = 0.0018230150453746319
iteration 158, loss = 0.00328622804954648
iteration 159, loss = 0.001737248501740396
iteration 160, loss = 0.002691899659112096
iteration 161, loss = 0.0029019885696470737
iteration 162, loss = 0.0025835507549345493
iteration 163, loss = 0.0014890030724927783
iteration 164, loss = 0.0016299803974106908
iteration 165, loss = 0.0017105176812037826
iteration 166, loss = 0.0015636601019650698
iteration 167, loss = 0.0014355897437781096
iteration 168, loss = 0.002048064023256302
iteration 169, loss = 0.0018747703870758414
iteration 170, loss = 0.0017076669028028846
iteration 171, loss = 0.002103829989209771
iteration 172, loss = 0.002934311982244253
iteration 173, loss = 0.0015488654607906938
iteration 174, loss = 0.0018621209310367703
iteration 175, loss = 0.0017136407550424337
iteration 176, loss = 0.0017809621058404446
iteration 177, loss = 0.0016970265423879027
iteration 178, loss = 0.002051742048934102
iteration 179, loss = 0.002199438400566578
iteration 180, loss = 0.001548829604871571
iteration 181, loss = 0.001686294679529965
iteration 182, loss = 0.001570787513628602
iteration 183, loss = 0.0018321552779525518
iteration 184, loss = 0.0015104407211765647
iteration 185, loss = 0.00162317487411201
iteration 186, loss = 0.002036950085312128
iteration 187, loss = 0.001816116040572524
iteration 188, loss = 0.002973130438476801
iteration 189, loss = 0.002215519081801176
iteration 190, loss = 0.0023634685203433037
iteration 191, loss = 0.0017317768651992083
iteration 192, loss = 0.0017450967570766807
iteration 193, loss = 0.0016188283916562796
iteration 194, loss = 0.0018547521904110909
iteration 195, loss = 0.0015720397932454944
iteration 196, loss = 0.0017953956266865134
iteration 197, loss = 0.003240983933210373
iteration 198, loss = 0.0022726263850927353
iteration 199, loss = 0.0015714532928541303
iteration 200, loss = 0.0018177134916186333
iteration 201, loss = 0.0018861208809539676
iteration 202, loss = 0.0015238979831337929
iteration 203, loss = 0.001958186039701104
iteration 204, loss = 0.0016145702684298158
iteration 205, loss = 0.0021064942702651024
iteration 206, loss = 0.0015698149800300598
iteration 207, loss = 0.0015552471159026027
iteration 208, loss = 0.0016756985569372773
iteration 209, loss = 0.0016070904675871134
iteration 210, loss = 0.0016305631725117564
iteration 211, loss = 0.001544391387142241
iteration 212, loss = 0.001660146634094417
iteration 213, loss = 0.0015633305301889777
iteration 214, loss = 0.0015862565487623215
iteration 215, loss = 0.0015128892846405506
iteration 216, loss = 0.0016642594709992409
iteration 217, loss = 0.0021460778079926968
iteration 218, loss = 0.0018396808300167322
iteration 219, loss = 0.0028512324206531048
iteration 220, loss = 0.0016394929261878133
iteration 221, loss = 0.0016311848303303123
iteration 222, loss = 0.0015101316384971142
iteration 223, loss = 0.0016549441497772932
iteration 224, loss = 0.0021762563847005367
iteration 225, loss = 0.0015337824588641524
iteration 226, loss = 0.0017482783878222108
iteration 227, loss = 0.0017005805857479572
iteration 228, loss = 0.0018032121006399393
iteration 229, loss = 0.001626024255529046
iteration 230, loss = 0.0015373214846476912
iteration 231, loss = 0.002094019204378128
iteration 232, loss = 0.0021102153696119785
iteration 233, loss = 0.0017590704374015331
iteration 234, loss = 0.0017307704547420144
iteration 235, loss = 0.001999566098675132
iteration 236, loss = 0.0016248185420408845
iteration 237, loss = 0.0016109764110296965
iteration 238, loss = 0.0019324711756780744
iteration 239, loss = 0.0015021436847746372
iteration 240, loss = 0.0016780754085630178
iteration 241, loss = 0.0017910226015374064
iteration 242, loss = 0.0018588850507512689
iteration 243, loss = 0.0019510764395818114
iteration 244, loss = 0.0016106476541608572
iteration 245, loss = 0.0023115177173167467
iteration 246, loss = 0.0014903985429555178
iteration 247, loss = 0.001985901268199086
iteration 248, loss = 0.001606788020581007
iteration 249, loss = 0.0028979002963751554
iteration 250, loss = 0.0014289944665506482
iteration 251, loss = 0.002383640268817544
iteration 252, loss = 0.0015957916621118784
iteration 253, loss = 0.0020191643852740526
iteration 254, loss = 0.0016762190498411655
iteration 255, loss = 0.0020275062415748835
iteration 256, loss = 0.0016019924078136683
iteration 257, loss = 0.0014946605078876019
iteration 258, loss = 0.0017167795449495316
iteration 259, loss = 0.003317303955554962
iteration 260, loss = 0.0017914240015670657
iteration 261, loss = 0.0016814563423395157
iteration 262, loss = 0.0020482128020375967
iteration 263, loss = 0.0017362467478960752
iteration 264, loss = 0.0015748214209452271
iteration 265, loss = 0.001512647606432438
iteration 266, loss = 0.0018829653272405267
iteration 267, loss = 0.0016553582390770316
iteration 268, loss = 0.001564539154060185
iteration 269, loss = 0.0017095906659960747
iteration 270, loss = 0.0015074560651555657
iteration 271, loss = 0.0015488495118916035
iteration 272, loss = 0.0021262564696371555
iteration 273, loss = 0.0019112156005576253
iteration 274, loss = 0.0018418163526803255
iteration 275, loss = 0.001548607018776238
iteration 276, loss = 0.0018297769129276276
iteration 277, loss = 0.0021596443839371204
iteration 278, loss = 0.002757009817287326
iteration 279, loss = 0.0018461953150108457
iteration 280, loss = 0.0017387047410011292
iteration 281, loss = 0.001785544096492231
iteration 282, loss = 0.001835498958826065
iteration 283, loss = 0.0030566926579922438
iteration 284, loss = 0.002108797198161483
iteration 285, loss = 0.0019495552405714989
iteration 286, loss = 0.0016365357441827655
iteration 287, loss = 0.0016793925315141678
iteration 288, loss = 0.0016492251306772232
iteration 289, loss = 0.0015445816097781062
iteration 290, loss = 0.00208805943839252
iteration 291, loss = 0.0022270609624683857
iteration 292, loss = 0.0017103620339185
iteration 293, loss = 0.0022462483029812574
iteration 294, loss = 0.0016442080959677696
iteration 295, loss = 0.0017963357968255877
iteration 296, loss = 0.0015333041083067656
iteration 297, loss = 0.002415751339867711
iteration 298, loss = 0.0016642503906041384
iteration 299, loss = 0.00140699977055192
iteration 300, loss = 0.001614285632967949
iteration 1, loss = 0.001508751418441534
iteration 2, loss = 0.001812325674109161
iteration 3, loss = 0.0014974060468375683
iteration 4, loss = 0.0021602546330541372
iteration 5, loss = 0.001658427994698286
iteration 6, loss = 0.0016051467973738909
iteration 7, loss = 0.0019093803130090237
iteration 8, loss = 0.0022362000308930874
iteration 9, loss = 0.001810648012906313
iteration 10, loss = 0.002290300326421857
iteration 11, loss = 0.001480302307754755
iteration 12, loss = 0.00228534871712327
iteration 13, loss = 0.0013912146678194404
iteration 14, loss = 0.0016058763721957803
iteration 15, loss = 0.001698142383247614
iteration 16, loss = 0.001813198789022863
iteration 17, loss = 0.0020509189926087856
iteration 18, loss = 0.0019968345295637846
iteration 19, loss = 0.0015602735802531242
iteration 20, loss = 0.001977908657863736
iteration 21, loss = 0.0017069042660295963
iteration 22, loss = 0.0015894456300884485
iteration 23, loss = 0.002162669086828828
iteration 24, loss = 0.0015552062541246414
iteration 25, loss = 0.002225750358775258
iteration 26, loss = 0.002897705417126417
iteration 27, loss = 0.0016720225103199482
iteration 28, loss = 0.002218357054516673
iteration 29, loss = 0.0014467748114839196
iteration 30, loss = 0.0016051667043939233
iteration 31, loss = 0.0016024763463065028
iteration 32, loss = 0.0015327620785683393
iteration 33, loss = 0.0016752621158957481
iteration 34, loss = 0.001466140616685152
iteration 35, loss = 0.0018635685555636883
iteration 36, loss = 0.0016657630912959576
iteration 37, loss = 0.0016654536593705416
iteration 38, loss = 0.0021363620180636644
iteration 39, loss = 0.0015055644325911999
iteration 40, loss = 0.0018695758190006018
iteration 41, loss = 0.0015731097664684057
iteration 42, loss = 0.002150824060663581
iteration 43, loss = 0.0019358485005795956
iteration 44, loss = 0.0021754340268671513
iteration 45, loss = 0.0017427324783056974
iteration 46, loss = 0.0016431916737928987
iteration 47, loss = 0.002075395779684186
iteration 48, loss = 0.0016361754387617111
iteration 49, loss = 0.0016336168628185987
iteration 50, loss = 0.0028346674516797066
iteration 51, loss = 0.00172708451282233
iteration 52, loss = 0.002758082700893283
iteration 53, loss = 0.001698714098893106
iteration 54, loss = 0.0015605598455294967
iteration 55, loss = 0.0016854361165314913
iteration 56, loss = 0.0016303067095577717
iteration 57, loss = 0.0021892436780035496
iteration 58, loss = 0.002029747935011983
iteration 59, loss = 0.0019311605719849467
iteration 60, loss = 0.0016570989973843098
iteration 61, loss = 0.0016556374030187726
iteration 62, loss = 0.001747030415572226
iteration 63, loss = 0.0015000180574133992
iteration 64, loss = 0.0017029475420713425
iteration 65, loss = 0.0016606854042038321
iteration 66, loss = 0.0016101013170555234
iteration 67, loss = 0.0016769478097558022
iteration 68, loss = 0.00320493639446795
iteration 69, loss = 0.0015553829725831747
iteration 70, loss = 0.0029765544459223747
iteration 71, loss = 0.0016823611222207546
iteration 72, loss = 0.0017190619837492704
iteration 73, loss = 0.001653478480875492
iteration 74, loss = 0.0017244280315935612
iteration 75, loss = 0.002937935059890151
iteration 76, loss = 0.0015895009273663163
iteration 77, loss = 0.0015799595275893807
iteration 78, loss = 0.0015176287852227688
iteration 79, loss = 0.0015555580612272024
iteration 80, loss = 0.002924270462244749
iteration 81, loss = 0.0023006913252174854
iteration 82, loss = 0.0018018069677054882
iteration 83, loss = 0.001612666412256658
iteration 84, loss = 0.0015358712989836931
iteration 85, loss = 0.0025198126677423716
iteration 86, loss = 0.0016398815205320716
iteration 87, loss = 0.002179918810725212
iteration 88, loss = 0.0014499854296445847
iteration 89, loss = 0.0018393337959423661
iteration 90, loss = 0.0020655912812799215
iteration 91, loss = 0.0015540014719590545
iteration 92, loss = 0.0015633547445759177
iteration 93, loss = 0.0017972758505493402
iteration 94, loss = 0.0015161937335506082
iteration 95, loss = 0.0017897506477311254
iteration 96, loss = 0.0018258461495861411
iteration 97, loss = 0.0013936621835455298
iteration 98, loss = 0.001558985561132431
iteration 99, loss = 0.0016926525859162211
iteration 100, loss = 0.001990519231185317
iteration 101, loss = 0.0015217275358736515
iteration 102, loss = 0.0021330921445041895
iteration 103, loss = 0.0017815009923651814
iteration 104, loss = 0.0016731173964217305
iteration 105, loss = 0.0016164189437404275
iteration 106, loss = 0.0015561430482193828
iteration 107, loss = 0.0015314933843910694
iteration 108, loss = 0.0017272572731599212
iteration 109, loss = 0.0015080939047038555
iteration 110, loss = 0.0019034997094422579
iteration 111, loss = 0.0016691521741449833
iteration 112, loss = 0.0017933433409780264
iteration 113, loss = 0.0018487358465790749
iteration 114, loss = 0.0022607718128710985
iteration 115, loss = 0.0020932951010763645
iteration 116, loss = 0.0018953988328576088
iteration 117, loss = 0.001457209000363946
iteration 118, loss = 0.002370375907048583
iteration 119, loss = 0.0017395502654835582
iteration 120, loss = 0.0016381326131522655
iteration 121, loss = 0.0023418015334755182
iteration 122, loss = 0.0026588954497128725
iteration 123, loss = 0.0015913203824311495
iteration 124, loss = 0.001683472073636949
iteration 125, loss = 0.0015159620670601726
iteration 126, loss = 0.0016393061960116029
iteration 127, loss = 0.0016351161757484078
iteration 128, loss = 0.0014609541976824403
iteration 129, loss = 0.003870924934744835
iteration 130, loss = 0.0015905867330729961
iteration 131, loss = 0.0015402344288304448
iteration 132, loss = 0.0020160707645118237
iteration 133, loss = 0.0017434176988899708
iteration 134, loss = 0.0019291318021714687
iteration 135, loss = 0.0015624654479324818
iteration 136, loss = 0.001796564320102334
iteration 137, loss = 0.0015750395832583308
iteration 138, loss = 0.0014827435370534658
iteration 139, loss = 0.0017527379095554352
iteration 140, loss = 0.0017366937827318907
iteration 141, loss = 0.0016952829901129007
iteration 142, loss = 0.001850381027907133
iteration 143, loss = 0.001690644072368741
iteration 144, loss = 0.0017588174669072032
iteration 145, loss = 0.0018442588625475764
iteration 146, loss = 0.0015497833956032991
iteration 147, loss = 0.0016432127449661493
iteration 148, loss = 0.0019220594549551606
iteration 149, loss = 0.002304380526766181
iteration 150, loss = 0.0016607774887233973
iteration 151, loss = 0.002826920710504055
iteration 152, loss = 0.0016167950816452503
iteration 153, loss = 0.0017137592658400536
iteration 154, loss = 0.0014724410139024258
iteration 155, loss = 0.0022915625013411045
iteration 156, loss = 0.0018196904566138983
iteration 157, loss = 0.0015292660100385547
iteration 158, loss = 0.0020928417798131704
iteration 159, loss = 0.0016797756543383002
iteration 160, loss = 0.0018017824040725827
iteration 161, loss = 0.002904042135924101
iteration 162, loss = 0.0016985090915113688
iteration 163, loss = 0.0020681782625615597
iteration 164, loss = 0.0018150864634662867
iteration 165, loss = 0.0016393479891121387
iteration 166, loss = 0.001582889468409121
iteration 167, loss = 0.003007272258400917
iteration 168, loss = 0.0016362082678824663
iteration 169, loss = 0.0017063585110008717
iteration 170, loss = 0.002131492830812931
iteration 171, loss = 0.0015503803733736277
iteration 172, loss = 0.002244161209091544
iteration 173, loss = 0.001743706758134067
iteration 174, loss = 0.0016025812365114689
iteration 175, loss = 0.0015742508694529533
iteration 176, loss = 0.0014840909279882908
iteration 177, loss = 0.0014754697913303971
iteration 178, loss = 0.0018136772559955716
iteration 179, loss = 0.001656758482567966
iteration 180, loss = 0.0016644294373691082
iteration 181, loss = 0.0015820362605154514
iteration 182, loss = 0.0020642788149416447
iteration 183, loss = 0.002037570346146822
iteration 184, loss = 0.002177238930016756
iteration 185, loss = 0.0015858850674703717
iteration 186, loss = 0.0017418346833437681
iteration 187, loss = 0.0016046720556914806
iteration 188, loss = 0.0015919058350846171
iteration 189, loss = 0.0015974697889760137
iteration 190, loss = 0.0016927222022786736
iteration 191, loss = 0.0014873686013743281
iteration 192, loss = 0.0028171329759061337
iteration 193, loss = 0.001798871555365622
iteration 194, loss = 0.0019042374333366752
iteration 195, loss = 0.0015814263606444001
iteration 196, loss = 0.0020653093233704567
iteration 197, loss = 0.0016101575456559658
iteration 198, loss = 0.0016166861169040203
iteration 199, loss = 0.0015481123700737953
iteration 200, loss = 0.0017043137922883034
iteration 201, loss = 0.001523649669252336
iteration 202, loss = 0.0018618389731273055
iteration 203, loss = 0.002043342450633645
iteration 204, loss = 0.002299882937222719
iteration 205, loss = 0.0016461648046970367
iteration 206, loss = 0.001711481250822544
iteration 207, loss = 0.00184216711204499
iteration 208, loss = 0.0015547164948657155
iteration 209, loss = 0.0017357456963509321
iteration 210, loss = 0.0016933736624196172
iteration 211, loss = 0.0019357173005118966
iteration 212, loss = 0.001696106162853539
iteration 213, loss = 0.0017512432532384992
iteration 214, loss = 0.0015320273814722896
iteration 215, loss = 0.0030103945173323154
iteration 216, loss = 0.0018339985981583595
iteration 217, loss = 0.0015899977879598737
iteration 218, loss = 0.0014416100457310677
iteration 219, loss = 0.00172459427267313
iteration 220, loss = 0.002071601338684559
iteration 221, loss = 0.0015538407023996115
iteration 222, loss = 0.0018870923668146133
iteration 223, loss = 0.0014831232838332653
iteration 224, loss = 0.001549745094962418
iteration 225, loss = 0.0028674700297415257
iteration 226, loss = 0.00161705759819597
iteration 227, loss = 0.0015353818889707327
iteration 228, loss = 0.0017095636576414108
iteration 229, loss = 0.0017678162548691034
iteration 230, loss = 0.0016532960580661893
iteration 231, loss = 0.0015471774386242032
iteration 232, loss = 0.0018875949317589402
iteration 233, loss = 0.0014374948805198073
iteration 234, loss = 0.001642703078687191
iteration 235, loss = 0.0021087564527988434
iteration 236, loss = 0.0017649789806455374
iteration 237, loss = 0.001661997870542109
iteration 238, loss = 0.0018752808682620525
iteration 239, loss = 0.0015708080027252436
iteration 240, loss = 0.0027550950180739164
iteration 241, loss = 0.0016104927053675056
iteration 242, loss = 0.0019315490499138832
iteration 243, loss = 0.001533592352643609
iteration 244, loss = 0.0016252391505986452
iteration 245, loss = 0.0015207950491458178
iteration 246, loss = 0.002257983200252056
iteration 247, loss = 0.0016239308752119541
iteration 248, loss = 0.0014538525138050318
iteration 249, loss = 0.0016423044726252556
iteration 250, loss = 0.0015128037193790078
iteration 251, loss = 0.0017025501001626253
iteration 252, loss = 0.0015413278015330434
iteration 253, loss = 0.0016114090103656054
iteration 254, loss = 0.002036618534475565
iteration 255, loss = 0.0017175211105495691
iteration 256, loss = 0.0017832601442933083
iteration 257, loss = 0.002209597732871771
iteration 258, loss = 0.0017993721412494779
iteration 259, loss = 0.001782252686098218
iteration 260, loss = 0.001661893678829074
iteration 261, loss = 0.001727860770188272
iteration 262, loss = 0.0016176458448171616
iteration 263, loss = 0.0015960538294166327
iteration 264, loss = 0.0019022030755877495
iteration 265, loss = 0.0021643314976245165
iteration 266, loss = 0.0018508296925574541
iteration 267, loss = 0.0016756057739257812
iteration 268, loss = 0.0015594045398756862
iteration 269, loss = 0.0016798442229628563
iteration 270, loss = 0.001602018135599792
iteration 271, loss = 0.0021309303119778633
iteration 272, loss = 0.0017842043889686465
iteration 273, loss = 0.002398249227553606
iteration 274, loss = 0.001789796631783247
iteration 275, loss = 0.0015141174662858248
iteration 276, loss = 0.001398373511619866
iteration 277, loss = 0.002374461619183421
iteration 278, loss = 0.0019768993370234966
iteration 279, loss = 0.0014118205290287733
iteration 280, loss = 0.0016550817526876926
iteration 281, loss = 0.001810485147871077
iteration 282, loss = 0.00167629134375602
iteration 283, loss = 0.0018001929856836796
iteration 284, loss = 0.0016214556526392698
iteration 285, loss = 0.001467325841076672
iteration 286, loss = 0.0016280930722132325
iteration 287, loss = 0.001803707331418991
iteration 288, loss = 0.0032418970949947834
iteration 289, loss = 0.0014257951406762004
iteration 290, loss = 0.0019515310414135456
iteration 291, loss = 0.0018791136099025607
iteration 292, loss = 0.0015678142663091421
iteration 293, loss = 0.0018074281979352236
iteration 294, loss = 0.0018342197872698307
iteration 295, loss = 0.0020890324376523495
iteration 296, loss = 0.001611921819858253
iteration 297, loss = 0.002796978922560811
iteration 298, loss = 0.0016453784191980958
iteration 299, loss = 0.0017468291334807873
iteration 300, loss = 0.0019071296555921435
iteration 1, loss = 0.0014810377033427358
iteration 2, loss = 0.001568000065162778
iteration 3, loss = 0.0029841549694538116
iteration 4, loss = 0.002053790492936969
iteration 5, loss = 0.0016736907418817282
iteration 6, loss = 0.0027795506175607443
iteration 7, loss = 0.0017744381912052631
iteration 8, loss = 0.002095612697303295
iteration 9, loss = 0.0015635977033525705
iteration 10, loss = 0.0014392257435247302
iteration 11, loss = 0.0014084895374253392
iteration 12, loss = 0.0015611532144248486
iteration 13, loss = 0.003621620126068592
iteration 14, loss = 0.0018797931261360645
iteration 15, loss = 0.0027425172738730907
iteration 16, loss = 0.0024076574482023716
iteration 17, loss = 0.0016518791671842337
iteration 18, loss = 0.0016580463852733374
iteration 19, loss = 0.001985922921448946
iteration 20, loss = 0.0016932477010414004
iteration 21, loss = 0.0020207685884088278
iteration 22, loss = 0.001751854200847447
iteration 23, loss = 0.0017742817290127277
iteration 24, loss = 0.0015483669703826308
iteration 25, loss = 0.0016942827496677637
iteration 26, loss = 0.0027970511000603437
iteration 27, loss = 0.001623117015697062
iteration 28, loss = 0.0020495436619967222
iteration 29, loss = 0.0015476844273507595
iteration 30, loss = 0.0015944826882332563
iteration 31, loss = 0.0014745118096470833
iteration 32, loss = 0.0016555998008698225
iteration 33, loss = 0.001707258285023272
iteration 34, loss = 0.001564379082992673
iteration 35, loss = 0.0017120771808549762
iteration 36, loss = 0.0017371224239468575
iteration 37, loss = 0.0016466219676658511
iteration 38, loss = 0.0016586346318945289
iteration 39, loss = 0.0018679061904549599
iteration 40, loss = 0.0015067218337208033
iteration 41, loss = 0.0025690561160445213
iteration 42, loss = 0.0015168271493166685
iteration 43, loss = 0.0016356848645955324
iteration 44, loss = 0.001670871046371758
iteration 45, loss = 0.0016006599180400372
iteration 46, loss = 0.0017185336910188198
iteration 47, loss = 0.0016771683003753424
iteration 48, loss = 0.0017836225451901555
iteration 49, loss = 0.0016815104754641652
iteration 50, loss = 0.0016196839278563857
iteration 51, loss = 0.0015767592703923583
iteration 52, loss = 0.0017292227130383253
iteration 53, loss = 0.0015750669408589602
iteration 54, loss = 0.001659157918766141
iteration 55, loss = 0.0016346245538443327
iteration 56, loss = 0.002018410712480545
iteration 57, loss = 0.0019121934892609715
iteration 58, loss = 0.0017820862121880054
iteration 59, loss = 0.0015334024792537093
iteration 60, loss = 0.001581402961164713
iteration 61, loss = 0.002088645938783884
iteration 62, loss = 0.0018123695626854897
iteration 63, loss = 0.0016263562720268965
iteration 64, loss = 0.0020597893744707108
iteration 65, loss = 0.0017610815120860934
iteration 66, loss = 0.0021975012496113777
iteration 67, loss = 0.0014463543193414807
iteration 68, loss = 0.0018770298920571804
iteration 69, loss = 0.0021874841768294573
iteration 70, loss = 0.0015584757784381509
iteration 71, loss = 0.0016708821058273315
iteration 72, loss = 0.0017762307543307543
iteration 73, loss = 0.0017137436661869287
iteration 74, loss = 0.0015608525136485696
iteration 75, loss = 0.0015992099652066827
iteration 76, loss = 0.002818633336573839
iteration 77, loss = 0.0017095999792218208
iteration 78, loss = 0.0015594229334965348
iteration 79, loss = 0.002536162966862321
iteration 80, loss = 0.0018701276276260614
iteration 81, loss = 0.0015937649877741933
iteration 82, loss = 0.0017685862258076668
iteration 83, loss = 0.0019598084036260843
iteration 84, loss = 0.0015930271474644542
iteration 85, loss = 0.0015538145089522004
iteration 86, loss = 0.0027803510893136263
iteration 87, loss = 0.0014829388819634914
iteration 88, loss = 0.0016269709449261427
iteration 89, loss = 0.001730340183712542
iteration 90, loss = 0.0020697598811239004
iteration 91, loss = 0.0022487258538603783
iteration 92, loss = 0.0019151420565322042
iteration 93, loss = 0.0017516763182356954
iteration 94, loss = 0.0020570792257785797
iteration 95, loss = 0.0015943575417622924
iteration 96, loss = 0.0016900249756872654
iteration 97, loss = 0.0016653869533911347
iteration 98, loss = 0.0015417839167639613
iteration 99, loss = 0.0018642621580511332
iteration 100, loss = 0.0015395617811009288
iteration 101, loss = 0.0016127761919051409
iteration 102, loss = 0.0015151178231462836
iteration 103, loss = 0.0016074232989922166
iteration 104, loss = 0.00170534395147115
iteration 105, loss = 0.0019663895945996046
iteration 106, loss = 0.001963003072887659
iteration 107, loss = 0.0016467582900077105
iteration 108, loss = 0.0013897041790187359
iteration 109, loss = 0.0016057544853538275
iteration 110, loss = 0.0015418175607919693
iteration 111, loss = 0.001603826996870339
iteration 112, loss = 0.0014832833549007773
iteration 113, loss = 0.0017133178189396858
iteration 114, loss = 0.001668267184868455
iteration 115, loss = 0.001725301961414516
iteration 116, loss = 0.001443446846678853
iteration 117, loss = 0.0018007959006354213
iteration 118, loss = 0.0017175852553918958
iteration 119, loss = 0.0015180855989456177
iteration 120, loss = 0.0018493179231882095
iteration 121, loss = 0.0017328999238088727
iteration 122, loss = 0.002803281880915165
iteration 123, loss = 0.002586574526503682
iteration 124, loss = 0.0015664382372051477
iteration 125, loss = 0.0016526976833119988
iteration 126, loss = 0.0021439138799905777
iteration 127, loss = 0.0015990950632840395
iteration 128, loss = 0.001792721333913505
iteration 129, loss = 0.0017070012399926782
iteration 130, loss = 0.0016831287648528814
iteration 131, loss = 0.0014921333640813828
iteration 132, loss = 0.0019375900737941265
iteration 133, loss = 0.001486866851337254
iteration 134, loss = 0.0015664915554225445
iteration 135, loss = 0.0015350943431258202
iteration 136, loss = 0.0015277249040082097
iteration 137, loss = 0.0015459626447409391
iteration 138, loss = 0.0018430796917527914
iteration 139, loss = 0.0017863139510154724
iteration 140, loss = 0.001707758754491806
iteration 141, loss = 0.001744877197779715
iteration 142, loss = 0.0018296083435416222
iteration 143, loss = 0.0015357529046013951
iteration 144, loss = 0.002625575987622142
iteration 145, loss = 0.0022102382499724627
iteration 146, loss = 0.0024237281177192926
iteration 147, loss = 0.0015901311999186873
iteration 148, loss = 0.001615007407963276
iteration 149, loss = 0.001596319256350398
iteration 150, loss = 0.0014421145897358656
iteration 151, loss = 0.001937620691023767
iteration 152, loss = 0.0017556443344801664
iteration 153, loss = 0.002195707056671381
iteration 154, loss = 0.0015630321577191353
iteration 155, loss = 0.002095089526847005
iteration 156, loss = 0.0021996472496539354
iteration 157, loss = 0.002214600332081318
iteration 158, loss = 0.0017893447075039148
iteration 159, loss = 0.0015679014613851905
iteration 160, loss = 0.0016165971755981445
iteration 161, loss = 0.0016723874723538756
iteration 162, loss = 0.0020258184522390366
iteration 163, loss = 0.0015195842133834958
iteration 164, loss = 0.0016985598485916853
iteration 165, loss = 0.0018844911828637123
iteration 166, loss = 0.00280818622559309
iteration 167, loss = 0.0016730480128899217
iteration 168, loss = 0.001650624442845583
iteration 169, loss = 0.001684264512732625
iteration 170, loss = 0.0016421673353761435
iteration 171, loss = 0.0016250417102128267
iteration 172, loss = 0.0016819488955661654
iteration 173, loss = 0.002732774941250682
iteration 174, loss = 0.0017399638891220093
iteration 175, loss = 0.001553618349134922
iteration 176, loss = 0.002161088865250349
iteration 177, loss = 0.0016306578181684017
iteration 178, loss = 0.0015652277506887913
iteration 179, loss = 0.0020324045326560736
iteration 180, loss = 0.0017106514424085617
iteration 181, loss = 0.0019174768822267652
iteration 182, loss = 0.0017841496737673879
iteration 183, loss = 0.0016103471862152219
iteration 184, loss = 0.001646940829232335
iteration 185, loss = 0.0018966825446113944
iteration 186, loss = 0.0015864083543419838
iteration 187, loss = 0.0027727324049919844
iteration 188, loss = 0.0024798742961138487
iteration 189, loss = 0.0016698703402653337
iteration 190, loss = 0.0017752734711393714
iteration 191, loss = 0.0020882864482700825
iteration 192, loss = 0.001539911376312375
iteration 193, loss = 0.002088931854814291
iteration 194, loss = 0.0018845312297344208
iteration 195, loss = 0.002282100962474942
iteration 196, loss = 0.0021774470806121826
iteration 197, loss = 0.0018280834192410111
iteration 198, loss = 0.0013581688981503248
iteration 199, loss = 0.0014729498652741313
iteration 200, loss = 0.0014957397943362594
iteration 201, loss = 0.0020577628165483475
iteration 202, loss = 0.0015158328460529447
iteration 203, loss = 0.0015358550008386374
iteration 204, loss = 0.0018103509210050106
iteration 205, loss = 0.0015794269274920225
iteration 206, loss = 0.0014996202662587166
iteration 207, loss = 0.0016364898765459657
iteration 208, loss = 0.0015949537046253681
iteration 209, loss = 0.0015415489906445146
iteration 210, loss = 0.0014816374750807881
iteration 211, loss = 0.0016532901208847761
iteration 212, loss = 0.0017295766156166792
iteration 213, loss = 0.001738472725264728
iteration 214, loss = 0.0017036754870787263
iteration 215, loss = 0.0016073171282187104
iteration 216, loss = 0.0015592033741995692
iteration 217, loss = 0.0018071995582431555
iteration 218, loss = 0.0019340092549100518
iteration 219, loss = 0.0016176062636077404
iteration 220, loss = 0.0020221846643835306
iteration 221, loss = 0.0016075888415798545
iteration 222, loss = 0.0015430533094331622
iteration 223, loss = 0.001539963181130588
iteration 224, loss = 0.002628833055496216
iteration 225, loss = 0.0021401226986199617
iteration 226, loss = 0.0019853708799928427
iteration 227, loss = 0.0020490912720561028
iteration 228, loss = 0.0017500892281532288
iteration 229, loss = 0.0027838486712425947
iteration 230, loss = 0.001593126798979938
iteration 231, loss = 0.0018098881701007485
iteration 232, loss = 0.0016070003621280193
iteration 233, loss = 0.0017261253669857979
iteration 234, loss = 0.0014224763726815581
iteration 235, loss = 0.0014921489637345076
iteration 236, loss = 0.0014787373365834355
iteration 237, loss = 0.001724177855066955
iteration 238, loss = 0.001600339193828404
iteration 239, loss = 0.002450415166094899
iteration 240, loss = 0.0017683455953374505
iteration 241, loss = 0.0017414316534996033
iteration 242, loss = 0.0019077531760558486
iteration 243, loss = 0.0017895355122163892
iteration 244, loss = 0.0015145442448556423
iteration 245, loss = 0.0017974397633224726
iteration 246, loss = 0.002792502287775278
iteration 247, loss = 0.0019427600782364607
iteration 248, loss = 0.0016199660021811724
iteration 249, loss = 0.0019307793118059635
iteration 250, loss = 0.001334139029495418
iteration 251, loss = 0.0015781597467139363
iteration 252, loss = 0.0022852837573736906
iteration 253, loss = 0.0015468406490981579
iteration 254, loss = 0.0016165534034371376
iteration 255, loss = 0.0015187467215582728
iteration 256, loss = 0.0015178581234067678
iteration 257, loss = 0.0024657747708261013
iteration 258, loss = 0.0018486222252249718
iteration 259, loss = 0.001709276344627142
iteration 260, loss = 0.0015478958375751972
iteration 261, loss = 0.0016268540639430285
iteration 262, loss = 0.0017231388483196497
iteration 263, loss = 0.0016320248832926154
iteration 264, loss = 0.0018034552922472358
iteration 265, loss = 0.0020128805190324783
iteration 266, loss = 0.0015560495667159557
iteration 267, loss = 0.0017713936977088451
iteration 268, loss = 0.001670443220064044
iteration 269, loss = 0.0016122548840939999
iteration 270, loss = 0.001898756716400385
iteration 271, loss = 0.0014647592324763536
iteration 272, loss = 0.0015748741570860147
iteration 273, loss = 0.0016944020753726363
iteration 274, loss = 0.0016414194833487272
iteration 275, loss = 0.0017481682589277625
iteration 276, loss = 0.001651470665819943
iteration 277, loss = 0.0017908969894051552
iteration 278, loss = 0.0032645382452756166
iteration 279, loss = 0.0018737828359007835
iteration 280, loss = 0.0018840759294107556
iteration 281, loss = 0.001546139712445438
iteration 282, loss = 0.001915379660204053
iteration 283, loss = 0.0017964071594178677
iteration 284, loss = 0.0017147852340713143
iteration 285, loss = 0.001604003831744194
iteration 286, loss = 0.001537275267764926
iteration 287, loss = 0.0028613251633942127
iteration 288, loss = 0.0015786149306222796
iteration 289, loss = 0.0016781954327598214
iteration 290, loss = 0.0014817183837294579
iteration 291, loss = 0.0016499020857736468
iteration 292, loss = 0.0016405577771365643
iteration 293, loss = 0.0016637354856356978
iteration 294, loss = 0.0021814298816025257
iteration 295, loss = 0.0020382388029247522
iteration 296, loss = 0.001649396726861596
iteration 297, loss = 0.002258204622194171
iteration 298, loss = 0.001520991325378418
iteration 299, loss = 0.002207324840128422
iteration 300, loss = 0.002497995039448142

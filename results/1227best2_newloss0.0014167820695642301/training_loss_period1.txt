iteration 1, loss = 2.555168390274048
iteration 2, loss = 2.513819932937622
iteration 3, loss = 2.5167622566223145
iteration 4, loss = 2.4946048259735107
iteration 5, loss = 2.446000099182129
iteration 6, loss = 2.403174877166748
iteration 7, loss = 2.363633632659912
iteration 8, loss = 2.2996180057525635
iteration 9, loss = 2.2613117694854736
iteration 10, loss = 2.176347494125366
iteration 11, loss = 2.181119918823242
iteration 12, loss = 2.0693552494049072
iteration 13, loss = 2.012899160385132
iteration 14, loss = 2.057626485824585
iteration 15, loss = 1.9489655494689941
iteration 16, loss = 1.854560375213623
iteration 17, loss = 1.8447765111923218
iteration 18, loss = 1.78131103515625
iteration 19, loss = 1.6342706680297852
iteration 20, loss = 1.6935253143310547
iteration 21, loss = 1.6386553049087524
iteration 22, loss = 1.581967830657959
iteration 23, loss = 1.5178240537643433
iteration 24, loss = 1.3968470096588135
iteration 25, loss = 1.5113974809646606
iteration 26, loss = 1.3572611808776855
iteration 27, loss = 1.3350704908370972
iteration 28, loss = 1.267712116241455
iteration 29, loss = 1.2286064624786377
iteration 30, loss = 1.2975746393203735
iteration 31, loss = 1.3055731058120728
iteration 32, loss = 1.048011302947998
iteration 33, loss = 1.0600887537002563
iteration 34, loss = 1.1609443426132202
iteration 35, loss = 1.0085493326187134
iteration 36, loss = 1.0030517578125
iteration 37, loss = 1.0933014154434204
iteration 38, loss = 0.9035860896110535
iteration 39, loss = 1.117426872253418
iteration 40, loss = 0.9404293298721313
iteration 41, loss = 1.0319963693618774
iteration 42, loss = 1.0255193710327148
iteration 43, loss = 0.9275896549224854
iteration 44, loss = 0.8632675409317017
iteration 45, loss = 0.9590821266174316
iteration 46, loss = 0.859599769115448
iteration 47, loss = 0.8569055795669556
iteration 48, loss = 0.9256357550621033
iteration 49, loss = 0.8846639394760132
iteration 50, loss = 0.8769221901893616
iteration 51, loss = 0.884958028793335
iteration 52, loss = 0.9063025712966919
iteration 53, loss = 0.8131361603736877
iteration 54, loss = 0.8640538454055786
iteration 55, loss = 0.8207901120185852
iteration 56, loss = 0.804845929145813
iteration 57, loss = 0.786861777305603
iteration 58, loss = 0.7874875068664551
iteration 59, loss = 0.6995271444320679
iteration 60, loss = 0.7730355858802795
iteration 61, loss = 0.8280452489852905
iteration 62, loss = 0.6734600067138672
iteration 63, loss = 0.8128477931022644
iteration 64, loss = 0.7129820585250854
iteration 65, loss = 0.713110089302063
iteration 66, loss = 0.7620843052864075
iteration 67, loss = 0.7400063276290894
iteration 68, loss = 0.7866960763931274
iteration 69, loss = 0.7997461557388306
iteration 70, loss = 0.7304129600524902
iteration 71, loss = 0.7722545862197876
iteration 72, loss = 0.7427840232849121
iteration 73, loss = 0.7544440031051636
iteration 74, loss = 0.7616093158721924
iteration 75, loss = 0.7132118344306946
iteration 76, loss = 0.7671114206314087
iteration 77, loss = 0.7166211009025574
iteration 78, loss = 0.8075646162033081
iteration 79, loss = 0.7140799164772034
iteration 80, loss = 0.7723792791366577
iteration 81, loss = 0.689503014087677
iteration 82, loss = 0.737206220626831
iteration 83, loss = 0.7516340017318726
iteration 84, loss = 0.6897558569908142
iteration 85, loss = 0.6716216802597046
iteration 86, loss = 0.7059495449066162
iteration 87, loss = 0.7398995757102966
iteration 88, loss = 0.6968297958374023
iteration 89, loss = 0.731870710849762
iteration 90, loss = 0.669231653213501
iteration 91, loss = 0.7334772348403931
iteration 92, loss = 0.6459699869155884
iteration 93, loss = 0.6678048372268677
iteration 94, loss = 0.6643749475479126
iteration 95, loss = 0.6947661638259888
iteration 96, loss = 0.7399160861968994
iteration 97, loss = 0.7298855781555176
iteration 98, loss = 0.6785926222801208
iteration 99, loss = 0.7283415198326111
iteration 100, loss = 0.7894207239151001
iteration 101, loss = 0.6586258411407471
iteration 102, loss = 0.6824193000793457
iteration 103, loss = 0.7014399170875549
iteration 104, loss = 0.6826457977294922
iteration 105, loss = 0.6627976894378662
iteration 106, loss = 0.6537715196609497
iteration 107, loss = 0.6900811195373535
iteration 108, loss = 0.6717547178268433
iteration 109, loss = 0.7408211827278137
iteration 110, loss = 0.7330902814865112
iteration 111, loss = 0.6908460855484009
iteration 112, loss = 0.6906546950340271
iteration 113, loss = 0.7136232256889343
iteration 114, loss = 0.7089124321937561
iteration 115, loss = 0.6671233177185059
iteration 116, loss = 0.6891565322875977
iteration 117, loss = 0.6677975058555603
iteration 118, loss = 0.6846930384635925
iteration 119, loss = 0.6643223762512207
iteration 120, loss = 0.6878346800804138
iteration 121, loss = 0.6614700555801392
iteration 122, loss = 0.6899616718292236
iteration 123, loss = 0.7274879217147827
iteration 124, loss = 0.6686838865280151
iteration 125, loss = 0.6575775146484375
iteration 126, loss = 0.7200703620910645
iteration 127, loss = 0.705534815788269
iteration 128, loss = 0.7003710269927979
iteration 129, loss = 0.6196014881134033
iteration 130, loss = 0.6370260119438171
iteration 131, loss = 0.7177542448043823
iteration 132, loss = 0.6588565707206726
iteration 133, loss = 0.7076082825660706
iteration 134, loss = 0.6603535413742065
iteration 135, loss = 0.6743596196174622
iteration 136, loss = 0.7031272053718567
iteration 137, loss = 0.6658486127853394
iteration 138, loss = 0.7021397352218628
iteration 139, loss = 0.707663893699646
iteration 140, loss = 0.6651408076286316
iteration 141, loss = 0.698876678943634
iteration 142, loss = 0.7081699371337891
iteration 143, loss = 0.6914576292037964
iteration 144, loss = 0.675709068775177
iteration 145, loss = 0.7302759885787964
iteration 146, loss = 0.659292459487915
iteration 147, loss = 0.6751879453659058
iteration 148, loss = 0.6825296878814697
iteration 149, loss = 0.6671345233917236
iteration 150, loss = 0.6609066128730774
iteration 151, loss = 0.6798909306526184
iteration 152, loss = 0.642193078994751
iteration 153, loss = 0.6920234560966492
iteration 154, loss = 0.627040684223175
iteration 155, loss = 0.6425708532333374
iteration 156, loss = 0.6546350717544556
iteration 157, loss = 0.6436532139778137
iteration 158, loss = 0.6611905097961426
iteration 159, loss = 0.6629419326782227
iteration 160, loss = 0.6954546570777893
iteration 161, loss = 0.6702307462692261
iteration 162, loss = 0.6627874374389648
iteration 163, loss = 0.6477632522583008
iteration 164, loss = 0.6998292207717896
iteration 165, loss = 0.6801595687866211
iteration 166, loss = 0.6279149055480957
iteration 167, loss = 0.6169045567512512
iteration 168, loss = 0.6520334482192993
iteration 169, loss = 0.6283896565437317
iteration 170, loss = 0.6210585832595825
iteration 171, loss = 0.6181625723838806
iteration 172, loss = 0.6270459294319153
iteration 173, loss = 0.600604772567749
iteration 174, loss = 0.6636894941329956
iteration 175, loss = 0.6718157529830933
iteration 176, loss = 0.6222440004348755
iteration 177, loss = 0.6135532259941101
iteration 178, loss = 0.6737219095230103
iteration 179, loss = 0.6401543021202087
iteration 180, loss = 0.6830353140830994
iteration 181, loss = 0.6169022917747498
iteration 182, loss = 0.6624992489814758
iteration 183, loss = 0.6468602418899536
iteration 184, loss = 0.6227818131446838
iteration 185, loss = 0.5782055854797363
iteration 186, loss = 0.6412045955657959
iteration 187, loss = 0.6379185914993286
iteration 188, loss = 0.6753572821617126
iteration 189, loss = 0.6617909073829651
iteration 190, loss = 0.6101175546646118
iteration 191, loss = 0.5960532426834106
iteration 192, loss = 0.6267412900924683
iteration 193, loss = 0.6644331812858582
iteration 194, loss = 0.6144809722900391
iteration 195, loss = 0.6310136318206787
iteration 196, loss = 0.5865402817726135
iteration 197, loss = 0.6036558151245117
iteration 198, loss = 0.5693293809890747
iteration 199, loss = 0.6895437240600586
iteration 200, loss = 0.6356127858161926
iteration 201, loss = 0.5731813907623291
iteration 202, loss = 0.6854350566864014
iteration 203, loss = 0.6204214096069336
iteration 204, loss = 0.6436058282852173
iteration 205, loss = 0.6265484690666199
iteration 206, loss = 0.6535483598709106
iteration 207, loss = 0.5970290303230286
iteration 208, loss = 0.6089627146720886
iteration 209, loss = 0.6171621084213257
iteration 210, loss = 0.5659142136573792
iteration 211, loss = 0.6179186105728149
iteration 212, loss = 0.5960206985473633
iteration 213, loss = 0.5844948291778564
iteration 214, loss = 0.5472376942634583
iteration 215, loss = 0.5692319273948669
iteration 216, loss = 0.6429915428161621
iteration 217, loss = 0.6599222421646118
iteration 218, loss = 0.6172027587890625
iteration 219, loss = 0.6534971594810486
iteration 220, loss = 0.597869873046875
iteration 221, loss = 0.6164968013763428
iteration 222, loss = 0.5954722166061401
iteration 223, loss = 0.6178101897239685
iteration 224, loss = 0.5980587005615234
iteration 225, loss = 0.5660859942436218
iteration 226, loss = 0.6463600397109985
iteration 227, loss = 0.5796011686325073
iteration 228, loss = 0.5853620767593384
iteration 229, loss = 0.5615397095680237
iteration 230, loss = 0.5712606906890869
iteration 231, loss = 0.574693500995636
iteration 232, loss = 0.6070594787597656
iteration 233, loss = 0.5690010190010071
iteration 234, loss = 0.6517674922943115
iteration 235, loss = 0.6103795170783997
iteration 236, loss = 0.5706976056098938
iteration 237, loss = 0.583005428314209
iteration 238, loss = 0.5801788568496704
iteration 239, loss = 0.5431424975395203
iteration 240, loss = 0.562021017074585
iteration 241, loss = 0.6290934681892395
iteration 242, loss = 0.6236103773117065
iteration 243, loss = 0.572732150554657
iteration 244, loss = 0.6140076518058777
iteration 245, loss = 0.6055639386177063
iteration 246, loss = 0.6079512238502502
iteration 247, loss = 0.5913223028182983
iteration 248, loss = 0.6228988766670227
iteration 249, loss = 0.5485493540763855
iteration 250, loss = 0.5441288948059082
iteration 251, loss = 0.5904766321182251
iteration 252, loss = 0.6356301307678223
iteration 253, loss = 0.5627861022949219
iteration 254, loss = 0.5581340193748474
iteration 255, loss = 0.571288526058197
iteration 256, loss = 0.5163540840148926
iteration 257, loss = 0.5755861401557922
iteration 258, loss = 0.5394589304924011
iteration 259, loss = 0.5846580266952515
iteration 260, loss = 0.5718477368354797
iteration 261, loss = 0.5360751748085022
iteration 262, loss = 0.5356037616729736
iteration 263, loss = 0.5827231407165527
iteration 264, loss = 0.5369287133216858
iteration 265, loss = 0.5686743259429932
iteration 266, loss = 0.5558409690856934
iteration 267, loss = 0.5687146186828613
iteration 268, loss = 0.583215594291687
iteration 269, loss = 0.5716991424560547
iteration 270, loss = 0.5380504131317139
iteration 271, loss = 0.5575534105300903
iteration 272, loss = 0.5327390432357788
iteration 273, loss = 0.5569323301315308
iteration 274, loss = 0.5785505175590515
iteration 275, loss = 0.5654070377349854
iteration 276, loss = 0.5408790707588196
iteration 277, loss = 0.5330178141593933
iteration 278, loss = 0.5339969992637634
iteration 279, loss = 0.5242347121238708
iteration 280, loss = 0.5113567113876343
iteration 281, loss = 0.5592107176780701
iteration 282, loss = 0.5865612626075745
iteration 283, loss = 0.49444910883903503
iteration 284, loss = 0.5281170606613159
iteration 285, loss = 0.5342056751251221
iteration 286, loss = 0.5049517154693604
iteration 287, loss = 0.5110021829605103
iteration 288, loss = 0.5130097270011902
iteration 289, loss = 0.5696663856506348
iteration 290, loss = 0.5400298237800598
iteration 291, loss = 0.5830625891685486
iteration 292, loss = 0.5763068199157715
iteration 293, loss = 0.5600121021270752
iteration 294, loss = 0.578351616859436
iteration 295, loss = 0.5460982322692871
iteration 296, loss = 0.47602343559265137
iteration 297, loss = 0.49843767285346985
iteration 298, loss = 0.5287877321243286
iteration 299, loss = 0.48754769563674927
iteration 300, loss = 0.49487248063087463
iteration 1, loss = 0.5579007863998413
iteration 2, loss = 0.5922512412071228
iteration 3, loss = 0.480751097202301
iteration 4, loss = 0.5164303779602051
iteration 5, loss = 0.4822665750980377
iteration 6, loss = 0.516061007976532
iteration 7, loss = 0.4859667420387268
iteration 8, loss = 0.46771758794784546
iteration 9, loss = 0.5309069156646729
iteration 10, loss = 0.498667448759079
iteration 11, loss = 0.5145419239997864
iteration 12, loss = 0.5027254223823547
iteration 13, loss = 0.511695921421051
iteration 14, loss = 0.5597710013389587
iteration 15, loss = 0.5304614901542664
iteration 16, loss = 0.4733286201953888
iteration 17, loss = 0.5705276131629944
iteration 18, loss = 0.49965691566467285
iteration 19, loss = 0.5356191396713257
iteration 20, loss = 0.4903082251548767
iteration 21, loss = 0.46488937735557556
iteration 22, loss = 0.4991214871406555
iteration 23, loss = 0.5048884749412537
iteration 24, loss = 0.4622563421726227
iteration 25, loss = 0.44883155822753906
iteration 26, loss = 0.4841007590293884
iteration 27, loss = 0.5298848748207092
iteration 28, loss = 0.4846304953098297
iteration 29, loss = 0.4851997196674347
iteration 30, loss = 0.4885401427745819
iteration 31, loss = 0.47007879614830017
iteration 32, loss = 0.5254238247871399
iteration 33, loss = 0.5099349021911621
iteration 34, loss = 0.4859730005264282
iteration 35, loss = 0.49533578753471375
iteration 36, loss = 0.46055465936660767
iteration 37, loss = 0.5096691846847534
iteration 38, loss = 0.5286816954612732
iteration 39, loss = 0.4530600905418396
iteration 40, loss = 0.49503934383392334
iteration 41, loss = 0.5321478247642517
iteration 42, loss = 0.4492393136024475
iteration 43, loss = 0.46804821491241455
iteration 44, loss = 0.4468535780906677
iteration 45, loss = 0.48304665088653564
iteration 46, loss = 0.4908498227596283
iteration 47, loss = 0.4504225552082062
iteration 48, loss = 0.43860211968421936
iteration 49, loss = 0.5283176898956299
iteration 50, loss = 0.4637356102466583
iteration 51, loss = 0.4930267632007599
iteration 52, loss = 0.4457547664642334
iteration 53, loss = 0.44755417108535767
iteration 54, loss = 0.47208213806152344
iteration 55, loss = 0.44697457551956177
iteration 56, loss = 0.4572851061820984
iteration 57, loss = 0.45559224486351013
iteration 58, loss = 0.4686601161956787
iteration 59, loss = 0.5482646226882935
iteration 60, loss = 0.48531433939933777
iteration 61, loss = 0.487493634223938
iteration 62, loss = 0.42534804344177246
iteration 63, loss = 0.5040158033370972
iteration 64, loss = 0.4228578507900238
iteration 65, loss = 0.47009211778640747
iteration 66, loss = 0.4718416631221771
iteration 67, loss = 0.46466970443725586
iteration 68, loss = 0.4173106551170349
iteration 69, loss = 0.43799588084220886
iteration 70, loss = 0.4008047878742218
iteration 71, loss = 0.3944780230522156
iteration 72, loss = 0.44456636905670166
iteration 73, loss = 0.4665801227092743
iteration 74, loss = 0.46551573276519775
iteration 75, loss = 0.4738262891769409
iteration 76, loss = 0.44938573241233826
iteration 77, loss = 0.4164454936981201
iteration 78, loss = 0.41446858644485474
iteration 79, loss = 0.3948758542537689
iteration 80, loss = 0.43645957112312317
iteration 81, loss = 0.4451361298561096
iteration 82, loss = 0.42828959226608276
iteration 83, loss = 0.42880070209503174
iteration 84, loss = 0.41065260767936707
iteration 85, loss = 0.4222690463066101
iteration 86, loss = 0.45187386870384216
iteration 87, loss = 0.4183209240436554
iteration 88, loss = 0.5437332391738892
iteration 89, loss = 0.428663969039917
iteration 90, loss = 0.4443419575691223
iteration 91, loss = 0.41001689434051514
iteration 92, loss = 0.3672935366630554
iteration 93, loss = 0.47173604369163513
iteration 94, loss = 0.3942229747772217
iteration 95, loss = 0.3950781524181366
iteration 96, loss = 0.390811949968338
iteration 97, loss = 0.3885027766227722
iteration 98, loss = 0.38983669877052307
iteration 99, loss = 0.4480801820755005
iteration 100, loss = 0.40285950899124146
iteration 101, loss = 0.4195308983325958
iteration 102, loss = 0.4124482274055481
iteration 103, loss = 0.41547924280166626
iteration 104, loss = 0.394742488861084
iteration 105, loss = 0.4345712959766388
iteration 106, loss = 0.42257750034332275
iteration 107, loss = 0.3547387719154358
iteration 108, loss = 0.412031352519989
iteration 109, loss = 0.38022398948669434
iteration 110, loss = 0.4381025433540344
iteration 111, loss = 0.38605618476867676
iteration 112, loss = 0.4052279591560364
iteration 113, loss = 0.36851802468299866
iteration 114, loss = 0.4309008717536926
iteration 115, loss = 0.3719659447669983
iteration 116, loss = 0.36888474225997925
iteration 117, loss = 0.4449183940887451
iteration 118, loss = 0.39318475127220154
iteration 119, loss = 0.34655052423477173
iteration 120, loss = 0.38558337092399597
iteration 121, loss = 0.44859814643859863
iteration 122, loss = 0.41798409819602966
iteration 123, loss = 0.42588675022125244
iteration 124, loss = 0.3493194580078125
iteration 125, loss = 0.3609199523925781
iteration 126, loss = 0.42525333166122437
iteration 127, loss = 0.38782334327697754
iteration 128, loss = 0.39164474606513977
iteration 129, loss = 0.38363024592399597
iteration 130, loss = 0.42440739274024963
iteration 131, loss = 0.3650042414665222
iteration 132, loss = 0.33851802349090576
iteration 133, loss = 0.4074283242225647
iteration 134, loss = 0.37498968839645386
iteration 135, loss = 0.3259671926498413
iteration 136, loss = 0.3297673165798187
iteration 137, loss = 0.35518932342529297
iteration 138, loss = 0.3573461174964905
iteration 139, loss = 0.373492032289505
iteration 140, loss = 0.40519624948501587
iteration 141, loss = 0.34135201573371887
iteration 142, loss = 0.32159286737442017
iteration 143, loss = 0.36142197251319885
iteration 144, loss = 0.3233374357223511
iteration 145, loss = 0.32916170358657837
iteration 146, loss = 0.3588760793209076
iteration 147, loss = 0.33906275033950806
iteration 148, loss = 0.3470258116722107
iteration 149, loss = 0.3619256615638733
iteration 150, loss = 0.3450169265270233
iteration 151, loss = 0.40293288230895996
iteration 152, loss = 0.40231093764305115
iteration 153, loss = 0.3815463185310364
iteration 154, loss = 0.3488472104072571
iteration 155, loss = 0.33415454626083374
iteration 156, loss = 0.3082748353481293
iteration 157, loss = 0.3529040813446045
iteration 158, loss = 0.3788110017776489
iteration 159, loss = 0.3039979636669159
iteration 160, loss = 0.36735135316848755
iteration 161, loss = 0.3927942216396332
iteration 162, loss = 0.3141254186630249
iteration 163, loss = 0.31469592452049255
iteration 164, loss = 0.28795018792152405
iteration 165, loss = 0.34464168548583984
iteration 166, loss = 0.34477657079696655
iteration 167, loss = 0.3036612570285797
iteration 168, loss = 0.3590945601463318
iteration 169, loss = 0.2794530987739563
iteration 170, loss = 0.35247617959976196
iteration 171, loss = 0.3341820240020752
iteration 172, loss = 0.275177925825119
iteration 173, loss = 0.3412209749221802
iteration 174, loss = 0.2838945984840393
iteration 175, loss = 0.3367464542388916
iteration 176, loss = 0.2905798852443695
iteration 177, loss = 0.30829107761383057
iteration 178, loss = 0.3303285241127014
iteration 179, loss = 0.3341948688030243
iteration 180, loss = 0.3122624158859253
iteration 181, loss = 0.30051860213279724
iteration 182, loss = 0.31305763125419617
iteration 183, loss = 0.35119423270225525
iteration 184, loss = 0.2987082600593567
iteration 185, loss = 0.2941068708896637
iteration 186, loss = 0.327484667301178
iteration 187, loss = 0.31471315026283264
iteration 188, loss = 0.340770959854126
iteration 189, loss = 0.3286553919315338
iteration 190, loss = 0.3297053277492523
iteration 191, loss = 0.3100903034210205
iteration 192, loss = 0.3208028972148895
iteration 193, loss = 0.28818613290786743
iteration 194, loss = 0.30710577964782715
iteration 195, loss = 0.32408422231674194
iteration 196, loss = 0.30223870277404785
iteration 197, loss = 0.36642101407051086
iteration 198, loss = 0.31998077034950256
iteration 199, loss = 0.27380985021591187
iteration 200, loss = 0.2840387225151062
iteration 201, loss = 0.28437086939811707
iteration 202, loss = 0.2850579023361206
iteration 203, loss = 0.2737891972064972
iteration 204, loss = 0.2960599362850189
iteration 205, loss = 0.27758899331092834
iteration 206, loss = 0.25463539361953735
iteration 207, loss = 0.30468225479125977
iteration 208, loss = 0.27062922716140747
iteration 209, loss = 0.2816513180732727
iteration 210, loss = 0.3448244631290436
iteration 211, loss = 0.2888796031475067
iteration 212, loss = 0.2803729772567749
iteration 213, loss = 0.29459136724472046
iteration 214, loss = 0.3097444772720337
iteration 215, loss = 0.30481916666030884
iteration 216, loss = 0.27437081933021545
iteration 217, loss = 0.2522391974925995
iteration 218, loss = 0.2741880416870117
iteration 219, loss = 0.24538594484329224
iteration 220, loss = 0.2756901979446411
iteration 221, loss = 0.28899940848350525
iteration 222, loss = 0.2500030994415283
iteration 223, loss = 0.24469998478889465
iteration 224, loss = 0.28274476528167725
iteration 225, loss = 0.297090619802475
iteration 226, loss = 0.29567959904670715
iteration 227, loss = 0.2736656069755554
iteration 228, loss = 0.3453584909439087
iteration 229, loss = 0.23481883108615875
iteration 230, loss = 0.23295730352401733
iteration 231, loss = 0.25575995445251465
iteration 232, loss = 0.2593175768852234
iteration 233, loss = 0.28453952074050903
iteration 234, loss = 0.29414957761764526
iteration 235, loss = 0.2138087898492813
iteration 236, loss = 0.25195857882499695
iteration 237, loss = 0.261924147605896
iteration 238, loss = 0.28628450632095337
iteration 239, loss = 0.22184641659259796
iteration 240, loss = 0.27034011483192444
iteration 241, loss = 0.25379830598831177
iteration 242, loss = 0.24547319114208221
iteration 243, loss = 0.23806986212730408
iteration 244, loss = 0.24394550919532776
iteration 245, loss = 0.2211497724056244
iteration 246, loss = 0.2673708200454712
iteration 247, loss = 0.23897957801818848
iteration 248, loss = 0.24512401223182678
iteration 249, loss = 0.2691102921962738
iteration 250, loss = 0.245760977268219
iteration 251, loss = 0.22254551947116852
iteration 252, loss = 0.22252200543880463
iteration 253, loss = 0.2414190173149109
iteration 254, loss = 0.2755706012248993
iteration 255, loss = 0.25019532442092896
iteration 256, loss = 0.24315768480300903
iteration 257, loss = 0.2469244748353958
iteration 258, loss = 0.22090120613574982
iteration 259, loss = 0.21543213725090027
iteration 260, loss = 0.275588721036911
iteration 261, loss = 0.3026457726955414
iteration 262, loss = 0.1995193213224411
iteration 263, loss = 0.22747604548931122
iteration 264, loss = 0.25401365756988525
iteration 265, loss = 0.2243010550737381
iteration 266, loss = 0.21267645061016083
iteration 267, loss = 0.20338256657123566
iteration 268, loss = 0.21512626111507416
iteration 269, loss = 0.19500544667243958
iteration 270, loss = 0.2173839658498764
iteration 271, loss = 0.23147596418857574
iteration 272, loss = 0.20753256976604462
iteration 273, loss = 0.24392744898796082
iteration 274, loss = 0.2470742166042328
iteration 275, loss = 0.32287657260894775
iteration 276, loss = 0.2023617923259735
iteration 277, loss = 0.23079442977905273
iteration 278, loss = 0.24393349885940552
iteration 279, loss = 0.20131400227546692
iteration 280, loss = 0.20778582990169525
iteration 281, loss = 0.23697566986083984
iteration 282, loss = 0.25675836205482483
iteration 283, loss = 0.23233717679977417
iteration 284, loss = 0.19366267323493958
iteration 285, loss = 0.20854000747203827
iteration 286, loss = 0.19931040704250336
iteration 287, loss = 0.19003653526306152
iteration 288, loss = 0.23246173560619354
iteration 289, loss = 0.28651654720306396
iteration 290, loss = 0.2062814086675644
iteration 291, loss = 0.18728193640708923
iteration 292, loss = 0.20224307477474213
iteration 293, loss = 0.20650099217891693
iteration 294, loss = 0.20638610422611237
iteration 295, loss = 0.19484153389930725
iteration 296, loss = 0.18629534542560577
iteration 297, loss = 0.28534433245658875
iteration 298, loss = 0.22743889689445496
iteration 299, loss = 0.2072014957666397
iteration 300, loss = 0.20149800181388855
iteration 1, loss = 0.20869117975234985
iteration 2, loss = 0.18998844921588898
iteration 3, loss = 0.19506433606147766
iteration 4, loss = 0.20041489601135254
iteration 5, loss = 0.20793235301971436
iteration 6, loss = 0.19227321445941925
iteration 7, loss = 0.20036283135414124
iteration 8, loss = 0.2524969279766083
iteration 9, loss = 0.19372530281543732
iteration 10, loss = 0.21890297532081604
iteration 11, loss = 0.2668677270412445
iteration 12, loss = 0.20113423466682434
iteration 13, loss = 0.24366693198680878
iteration 14, loss = 0.2201988250017166
iteration 15, loss = 0.1834922879934311
iteration 16, loss = 0.18809834122657776
iteration 17, loss = 0.243100106716156
iteration 18, loss = 0.24851612746715546
iteration 19, loss = 0.24294380843639374
iteration 20, loss = 0.2595304548740387
iteration 21, loss = 0.18596605956554413
iteration 22, loss = 0.1909441202878952
iteration 23, loss = 0.18666386604309082
iteration 24, loss = 0.18632926046848297
iteration 25, loss = 0.21354104578495026
iteration 26, loss = 0.18121123313903809
iteration 27, loss = 0.2047463357448578
iteration 28, loss = 0.17502589523792267
iteration 29, loss = 0.1814400851726532
iteration 30, loss = 0.17042186856269836
iteration 31, loss = 0.28428003191947937
iteration 32, loss = 0.16459083557128906
iteration 33, loss = 0.17554785311222076
iteration 34, loss = 0.1984706073999405
iteration 35, loss = 0.18222081661224365
iteration 36, loss = 0.20545940101146698
iteration 37, loss = 0.19643068313598633
iteration 38, loss = 0.18647605180740356
iteration 39, loss = 0.2017216682434082
iteration 40, loss = 0.16538392007350922
iteration 41, loss = 0.15345865488052368
iteration 42, loss = 0.17490246891975403
iteration 43, loss = 0.1680319905281067
iteration 44, loss = 0.17003023624420166
iteration 45, loss = 0.17845484614372253
iteration 46, loss = 0.15740548074245453
iteration 47, loss = 0.18816523253917694
iteration 48, loss = 0.18088872730731964
iteration 49, loss = 0.15899251401424408
iteration 50, loss = 0.15857404470443726
iteration 51, loss = 0.16255353391170502
iteration 52, loss = 0.17822319269180298
iteration 53, loss = 0.15398983657360077
iteration 54, loss = 0.1565249264240265
iteration 55, loss = 0.1811574250459671
iteration 56, loss = 0.19446153938770294
iteration 57, loss = 0.16009068489074707
iteration 58, loss = 0.16027575731277466
iteration 59, loss = 0.14482462406158447
iteration 60, loss = 0.17090243101119995
iteration 61, loss = 0.1787310540676117
iteration 62, loss = 0.1822432428598404
iteration 63, loss = 0.16710105538368225
iteration 64, loss = 0.15718156099319458
iteration 65, loss = 0.1732707917690277
iteration 66, loss = 0.1744813472032547
iteration 67, loss = 0.18096373975276947
iteration 68, loss = 0.2008623629808426
iteration 69, loss = 0.14982812106609344
iteration 70, loss = 0.1716403365135193
iteration 71, loss = 0.14106707274913788
iteration 72, loss = 0.15406395494937897
iteration 73, loss = 0.15417823195457458
iteration 74, loss = 0.13500365614891052
iteration 75, loss = 0.16501595079898834
iteration 76, loss = 0.1821400225162506
iteration 77, loss = 0.14842838048934937
iteration 78, loss = 0.1525110900402069
iteration 79, loss = 0.18887123465538025
iteration 80, loss = 0.18262441456317902
iteration 81, loss = 0.18040680885314941
iteration 82, loss = 0.13757312297821045
iteration 83, loss = 0.19253748655319214
iteration 84, loss = 0.14551737904548645
iteration 85, loss = 0.1371368169784546
iteration 86, loss = 0.13400161266326904
iteration 87, loss = 0.2020941972732544
iteration 88, loss = 0.16858595609664917
iteration 89, loss = 0.17698292434215546
iteration 90, loss = 0.13257330656051636
iteration 91, loss = 0.16661518812179565
iteration 92, loss = 0.13241001963615417
iteration 93, loss = 0.14571966230869293
iteration 94, loss = 0.14822372794151306
iteration 95, loss = 0.13084949553012848
iteration 96, loss = 0.12130876630544662
iteration 97, loss = 0.15949705243110657
iteration 98, loss = 0.1601327657699585
iteration 99, loss = 0.12423951923847198
iteration 100, loss = 0.12983694672584534
iteration 101, loss = 0.1666027456521988
iteration 102, loss = 0.11943948268890381
iteration 103, loss = 0.14934475719928741
iteration 104, loss = 0.12125714123249054
iteration 105, loss = 0.12230135500431061
iteration 106, loss = 0.15525855123996735
iteration 107, loss = 0.14784368872642517
iteration 108, loss = 0.12890058755874634
iteration 109, loss = 0.16864046454429626
iteration 110, loss = 0.1240871474146843
iteration 111, loss = 0.12638436257839203
iteration 112, loss = 0.16523396968841553
iteration 113, loss = 0.12991401553153992
iteration 114, loss = 0.19699522852897644
iteration 115, loss = 0.1273597627878189
iteration 116, loss = 0.14704962074756622
iteration 117, loss = 0.1358853578567505
iteration 118, loss = 0.13036547601222992
iteration 119, loss = 0.13294723629951477
iteration 120, loss = 0.13856934010982513
iteration 121, loss = 0.12062084674835205
iteration 122, loss = 0.13018019497394562
iteration 123, loss = 0.1359519064426422
iteration 124, loss = 0.1665431708097458
iteration 125, loss = 0.1579693853855133
iteration 126, loss = 0.125477135181427
iteration 127, loss = 0.1436425894498825
iteration 128, loss = 0.13471291959285736
iteration 129, loss = 0.14365330338478088
iteration 130, loss = 0.1700262874364853
iteration 131, loss = 0.1563185751438141
iteration 132, loss = 0.12446343898773193
iteration 133, loss = 0.1223185658454895
iteration 134, loss = 0.14777632057666779
iteration 135, loss = 0.11307016015052795
iteration 136, loss = 0.12502174079418182
iteration 137, loss = 0.14482735097408295
iteration 138, loss = 0.142239049077034
iteration 139, loss = 0.12261912971735
iteration 140, loss = 0.1379598081111908
iteration 141, loss = 0.1540781557559967
iteration 142, loss = 0.11416369676589966
iteration 143, loss = 0.13200818002223969
iteration 144, loss = 0.12677305936813354
iteration 145, loss = 0.12965184450149536
iteration 146, loss = 0.14146770536899567
iteration 147, loss = 0.14169563353061676
iteration 148, loss = 0.11343946307897568
iteration 149, loss = 0.13082309067249298
iteration 150, loss = 0.10467051714658737
iteration 151, loss = 0.13884225487709045
iteration 152, loss = 0.10546792298555374
iteration 153, loss = 0.11485568434000015
iteration 154, loss = 0.11981310695409775
iteration 155, loss = 0.11228922009468079
iteration 156, loss = 0.144688680768013
iteration 157, loss = 0.14986689388751984
iteration 158, loss = 0.12321169674396515
iteration 159, loss = 0.10992969572544098
iteration 160, loss = 0.13256247341632843
iteration 161, loss = 0.09989828616380692
iteration 162, loss = 0.13777394592761993
iteration 163, loss = 0.12450014799833298
iteration 164, loss = 0.1019943356513977
iteration 165, loss = 0.14543506503105164
iteration 166, loss = 0.15482550859451294
iteration 167, loss = 0.18576768040657043
iteration 168, loss = 0.1376425176858902
iteration 169, loss = 0.1082475483417511
iteration 170, loss = 0.10881956666707993
iteration 171, loss = 0.11397842317819595
iteration 172, loss = 0.09965876489877701
iteration 173, loss = 0.1632259339094162
iteration 174, loss = 0.11185706406831741
iteration 175, loss = 0.11188015341758728
iteration 176, loss = 0.1050855815410614
iteration 177, loss = 0.12955638766288757
iteration 178, loss = 0.1010526493191719
iteration 179, loss = 0.09858978539705276
iteration 180, loss = 0.13426564633846283
iteration 181, loss = 0.10115109384059906
iteration 182, loss = 0.12379252910614014
iteration 183, loss = 0.10575021803379059
iteration 184, loss = 0.09797434508800507
iteration 185, loss = 0.14975158870220184
iteration 186, loss = 0.1412915289402008
iteration 187, loss = 0.15965521335601807
iteration 188, loss = 0.12720689177513123
iteration 189, loss = 0.110038623213768
iteration 190, loss = 0.09659339487552643
iteration 191, loss = 0.10008205473423004
iteration 192, loss = 0.1544073522090912
iteration 193, loss = 0.09471692144870758
iteration 194, loss = 0.10222039371728897
iteration 195, loss = 0.10571691393852234
iteration 196, loss = 0.08967911452054977
iteration 197, loss = 0.13455572724342346
iteration 198, loss = 0.13353487849235535
iteration 199, loss = 0.10312789678573608
iteration 200, loss = 0.11001577228307724
iteration 201, loss = 0.0916505828499794
iteration 202, loss = 0.11942553520202637
iteration 203, loss = 0.11581449210643768
iteration 204, loss = 0.10085046291351318
iteration 205, loss = 0.10846561938524246
iteration 206, loss = 0.1034749448299408
iteration 207, loss = 0.09269504249095917
iteration 208, loss = 0.12412359565496445
iteration 209, loss = 0.09291810542345047
iteration 210, loss = 0.1192418783903122
iteration 211, loss = 0.0932571068406105
iteration 212, loss = 0.10395239293575287
iteration 213, loss = 0.10724607110023499
iteration 214, loss = 0.11788778752088547
iteration 215, loss = 0.0972977876663208
iteration 216, loss = 0.09548165649175644
iteration 217, loss = 0.10913920402526855
iteration 218, loss = 0.08998715877532959
iteration 219, loss = 0.089114710688591
iteration 220, loss = 0.10933193564414978
iteration 221, loss = 0.14650912582874298
iteration 222, loss = 0.10242989659309387
iteration 223, loss = 0.08478391170501709
iteration 224, loss = 0.1254124641418457
iteration 225, loss = 0.09495055675506592
iteration 226, loss = 0.11329744756221771
iteration 227, loss = 0.12593495845794678
iteration 228, loss = 0.09057807922363281
iteration 229, loss = 0.13829484581947327
iteration 230, loss = 0.09671718627214432
iteration 231, loss = 0.0997617244720459
iteration 232, loss = 0.11223544180393219
iteration 233, loss = 0.08981528878211975
iteration 234, loss = 0.09817542135715485
iteration 235, loss = 0.11311594396829605
iteration 236, loss = 0.08006434142589569
iteration 237, loss = 0.08408631384372711
iteration 238, loss = 0.09693337976932526
iteration 239, loss = 0.09573745727539062
iteration 240, loss = 0.08940136432647705
iteration 241, loss = 0.0765952616930008
iteration 242, loss = 0.0919966772198677
iteration 243, loss = 0.08511355519294739
iteration 244, loss = 0.09780359268188477
iteration 245, loss = 0.12997929751873016
iteration 246, loss = 0.09091241657733917
iteration 247, loss = 0.10463406890630722
iteration 248, loss = 0.08069254457950592
iteration 249, loss = 0.13206911087036133
iteration 250, loss = 0.08194513618946075
iteration 251, loss = 0.07709236443042755
iteration 252, loss = 0.08097976446151733
iteration 253, loss = 0.09520003199577332
iteration 254, loss = 0.07776780426502228
iteration 255, loss = 0.09772779047489166
iteration 256, loss = 0.09954571723937988
iteration 257, loss = 0.08993939310312271
iteration 258, loss = 0.08015207946300507
iteration 259, loss = 0.08020175248384476
iteration 260, loss = 0.081149622797966
iteration 261, loss = 0.08315251022577286
iteration 262, loss = 0.07663200795650482
iteration 263, loss = 0.07501085102558136
iteration 264, loss = 0.07448413223028183
iteration 265, loss = 0.09469949454069138
iteration 266, loss = 0.08663509041070938
iteration 267, loss = 0.07295877486467361
iteration 268, loss = 0.07914216816425323
iteration 269, loss = 0.09405671060085297
iteration 270, loss = 0.07615687698125839
iteration 271, loss = 0.06947088986635208
iteration 272, loss = 0.09421634674072266
iteration 273, loss = 0.08619315177202225
iteration 274, loss = 0.08375068008899689
iteration 275, loss = 0.0970161110162735
iteration 276, loss = 0.07735271006822586
iteration 277, loss = 0.07778700441122055
iteration 278, loss = 0.08647724986076355
iteration 279, loss = 0.07654927670955658
iteration 280, loss = 0.07671329379081726
iteration 281, loss = 0.07579298317432404
iteration 282, loss = 0.1189422532916069
iteration 283, loss = 0.07878388464450836
iteration 284, loss = 0.08258941769599915
iteration 285, loss = 0.10253218561410904
iteration 286, loss = 0.09208119660615921
iteration 287, loss = 0.06818423420190811
iteration 288, loss = 0.0996541902422905
iteration 289, loss = 0.09740618616342545
iteration 290, loss = 0.11425565183162689
iteration 291, loss = 0.07964880764484406
iteration 292, loss = 0.10818459093570709
iteration 293, loss = 0.06972407549619675
iteration 294, loss = 0.09550638496875763
iteration 295, loss = 0.08960144966840744
iteration 296, loss = 0.07097268849611282
iteration 297, loss = 0.07169955968856812
iteration 298, loss = 0.07457203418016434
iteration 299, loss = 0.08947848528623581
iteration 300, loss = 0.07648846507072449
iteration 1, loss = 0.0779409185051918
iteration 2, loss = 0.0709000900387764
iteration 3, loss = 0.07007260620594025
iteration 4, loss = 0.07137484848499298
iteration 5, loss = 0.07828500121831894
iteration 6, loss = 0.07822660356760025
iteration 7, loss = 0.0901203602552414
iteration 8, loss = 0.1108049601316452
iteration 9, loss = 0.09923949837684631
iteration 10, loss = 0.07677397131919861
iteration 11, loss = 0.06973768770694733
iteration 12, loss = 0.06725219637155533
iteration 13, loss = 0.08906969428062439
iteration 14, loss = 0.06502823531627655
iteration 15, loss = 0.06899561733007431
iteration 16, loss = 0.07450242340564728
iteration 17, loss = 0.07036806643009186
iteration 18, loss = 0.06754569709300995
iteration 19, loss = 0.09969271719455719
iteration 20, loss = 0.0671805739402771
iteration 21, loss = 0.06536547094583511
iteration 22, loss = 0.07067522406578064
iteration 23, loss = 0.09558213502168655
iteration 24, loss = 0.0683738961815834
iteration 25, loss = 0.07366943359375
iteration 26, loss = 0.06115799397230148
iteration 27, loss = 0.06506859511137009
iteration 28, loss = 0.06910000741481781
iteration 29, loss = 0.10158634185791016
iteration 30, loss = 0.07099160552024841
iteration 31, loss = 0.06485756486654282
iteration 32, loss = 0.0656302273273468
iteration 33, loss = 0.09765465557575226
iteration 34, loss = 0.06343822181224823
iteration 35, loss = 0.06434524059295654
iteration 36, loss = 0.11189676821231842
iteration 37, loss = 0.07016043365001678
iteration 38, loss = 0.06291128695011139
iteration 39, loss = 0.10306011140346527
iteration 40, loss = 0.0905827134847641
iteration 41, loss = 0.08545743674039841
iteration 42, loss = 0.060736753046512604
iteration 43, loss = 0.08141031861305237
iteration 44, loss = 0.09765243530273438
iteration 45, loss = 0.07996021211147308
iteration 46, loss = 0.0797446072101593
iteration 47, loss = 0.08687542378902435
iteration 48, loss = 0.07355441153049469
iteration 49, loss = 0.10436384379863739
iteration 50, loss = 0.059599943459033966
iteration 51, loss = 0.058269284665584564
iteration 52, loss = 0.06080060824751854
iteration 53, loss = 0.07893592119216919
iteration 54, loss = 0.061389632523059845
iteration 55, loss = 0.0697048157453537
iteration 56, loss = 0.06767317652702332
iteration 57, loss = 0.06478796154260635
iteration 58, loss = 0.07788940519094467
iteration 59, loss = 0.06200845539569855
iteration 60, loss = 0.09824402630329132
iteration 61, loss = 0.09356091916561127
iteration 62, loss = 0.06117986887693405
iteration 63, loss = 0.07014324516057968
iteration 64, loss = 0.060062482953071594
iteration 65, loss = 0.05826432630419731
iteration 66, loss = 0.07361721992492676
iteration 67, loss = 0.06937864422798157
iteration 68, loss = 0.05403140187263489
iteration 69, loss = 0.08787229657173157
iteration 70, loss = 0.06619980186223984
iteration 71, loss = 0.05575355887413025
iteration 72, loss = 0.05944104865193367
iteration 73, loss = 0.056475743651390076
iteration 74, loss = 0.05850329250097275
iteration 75, loss = 0.07706856727600098
iteration 76, loss = 0.07662668824195862
iteration 77, loss = 0.05707940459251404
iteration 78, loss = 0.09574982523918152
iteration 79, loss = 0.052775781601667404
iteration 80, loss = 0.05543423444032669
iteration 81, loss = 0.05378245934844017
iteration 82, loss = 0.07707273215055466
iteration 83, loss = 0.06863739341497421
iteration 84, loss = 0.05596025288105011
iteration 85, loss = 0.06073761358857155
iteration 86, loss = 0.05499469116330147
iteration 87, loss = 0.06199415400624275
iteration 88, loss = 0.06713107228279114
iteration 89, loss = 0.060541052371263504
iteration 90, loss = 0.09370078891515732
iteration 91, loss = 0.07250969856977463
iteration 92, loss = 0.07608332484960556
iteration 93, loss = 0.06001219153404236
iteration 94, loss = 0.07104166597127914
iteration 95, loss = 0.08626407384872437
iteration 96, loss = 0.053907837718725204
iteration 97, loss = 0.06666037440299988
iteration 98, loss = 0.08242619037628174
iteration 99, loss = 0.05471448600292206
iteration 100, loss = 0.0725354552268982
iteration 101, loss = 0.06523691117763519
iteration 102, loss = 0.06891155242919922
iteration 103, loss = 0.05822470784187317
iteration 104, loss = 0.05714820325374603
iteration 105, loss = 0.057702433317899704
iteration 106, loss = 0.07408595830202103
iteration 107, loss = 0.05699937790632248
iteration 108, loss = 0.08403251320123672
iteration 109, loss = 0.06360255926847458
iteration 110, loss = 0.06910010427236557
iteration 111, loss = 0.07191255688667297
iteration 112, loss = 0.06169835850596428
iteration 113, loss = 0.07187626510858536
iteration 114, loss = 0.054161474108695984
iteration 115, loss = 0.05669954791665077
iteration 116, loss = 0.06363903731107712
iteration 117, loss = 0.050383515655994415
iteration 118, loss = 0.04749666899442673
iteration 119, loss = 0.049818310886621475
iteration 120, loss = 0.05631871894001961
iteration 121, loss = 0.09796734154224396
iteration 122, loss = 0.05082789063453674
iteration 123, loss = 0.0762539803981781
iteration 124, loss = 0.05531882122159004
iteration 125, loss = 0.05914752930402756
iteration 126, loss = 0.0513535737991333
iteration 127, loss = 0.055175162851810455
iteration 128, loss = 0.0512203685939312
iteration 129, loss = 0.052621908485889435
iteration 130, loss = 0.05564189329743385
iteration 131, loss = 0.05551460385322571
iteration 132, loss = 0.07854551076889038
iteration 133, loss = 0.056572359055280685
iteration 134, loss = 0.06241641938686371
iteration 135, loss = 0.062377236783504486
iteration 136, loss = 0.05498124286532402
iteration 137, loss = 0.060529425740242004
iteration 138, loss = 0.044858168810606
iteration 139, loss = 0.05687788128852844
iteration 140, loss = 0.05331922695040703
iteration 141, loss = 0.05321844667196274
iteration 142, loss = 0.0749267190694809
iteration 143, loss = 0.051843393594026566
iteration 144, loss = 0.04941321164369583
iteration 145, loss = 0.046264778822660446
iteration 146, loss = 0.051929354667663574
iteration 147, loss = 0.04979301616549492
iteration 148, loss = 0.051346223801374435
iteration 149, loss = 0.051427699625492096
iteration 150, loss = 0.0769190862774849
iteration 151, loss = 0.05841399356722832
iteration 152, loss = 0.05012815445661545
iteration 153, loss = 0.050762325525283813
iteration 154, loss = 0.047154784202575684
iteration 155, loss = 0.057539574801921844
iteration 156, loss = 0.04903637617826462
iteration 157, loss = 0.06890280544757843
iteration 158, loss = 0.04792740195989609
iteration 159, loss = 0.043444953858852386
iteration 160, loss = 0.044615473598241806
iteration 161, loss = 0.08226876705884933
iteration 162, loss = 0.0516844317317009
iteration 163, loss = 0.0713215321302414
iteration 164, loss = 0.0471520721912384
iteration 165, loss = 0.0432916097342968
iteration 166, loss = 0.046484410762786865
iteration 167, loss = 0.052752431482076645
iteration 168, loss = 0.042985282838344574
iteration 169, loss = 0.050157204270362854
iteration 170, loss = 0.043428629636764526
iteration 171, loss = 0.04490240663290024
iteration 172, loss = 0.050469864159822464
iteration 173, loss = 0.049597788602113724
iteration 174, loss = 0.05165799707174301
iteration 175, loss = 0.047996435314416885
iteration 176, loss = 0.05379534512758255
iteration 177, loss = 0.04607369005680084
iteration 178, loss = 0.046936485916376114
iteration 179, loss = 0.04527260735630989
iteration 180, loss = 0.05098949372768402
iteration 181, loss = 0.044828686863183975
iteration 182, loss = 0.06005606800317764
iteration 183, loss = 0.04881509393453598
iteration 184, loss = 0.07274699956178665
iteration 185, loss = 0.04498915374279022
iteration 186, loss = 0.04317275062203407
iteration 187, loss = 0.048873208463191986
iteration 188, loss = 0.06303335726261139
iteration 189, loss = 0.04135148227214813
iteration 190, loss = 0.04382218420505524
iteration 191, loss = 0.043298300355672836
iteration 192, loss = 0.044367510825395584
iteration 193, loss = 0.04384135454893112
iteration 194, loss = 0.039988912642002106
iteration 195, loss = 0.06597966700792313
iteration 196, loss = 0.05020005255937576
iteration 197, loss = 0.04222647100687027
iteration 198, loss = 0.042452163994312286
iteration 199, loss = 0.04461453855037689
iteration 200, loss = 0.04190567880868912
iteration 201, loss = 0.05671781301498413
iteration 202, loss = 0.052237313240766525
iteration 203, loss = 0.0618489533662796
iteration 204, loss = 0.04982088506221771
iteration 205, loss = 0.05668741092085838
iteration 206, loss = 0.0493423230946064
iteration 207, loss = 0.04372672364115715
iteration 208, loss = 0.05198146402835846
iteration 209, loss = 0.055442653596401215
iteration 210, loss = 0.06734472513198853
iteration 211, loss = 0.03926001116633415
iteration 212, loss = 0.048417042940855026
iteration 213, loss = 0.041831545531749725
iteration 214, loss = 0.04256066679954529
iteration 215, loss = 0.04128897562623024
iteration 216, loss = 0.041891809552907944
iteration 217, loss = 0.048502273857593536
iteration 218, loss = 0.047908276319503784
iteration 219, loss = 0.04854835569858551
iteration 220, loss = 0.042639873921871185
iteration 221, loss = 0.036673400551080704
iteration 222, loss = 0.057568956166505814
iteration 223, loss = 0.03891017287969589
iteration 224, loss = 0.04038333147764206
iteration 225, loss = 0.042968086898326874
iteration 226, loss = 0.04742608219385147
iteration 227, loss = 0.04647742584347725
iteration 228, loss = 0.041053302586078644
iteration 229, loss = 0.04188213497400284
iteration 230, loss = 0.053678303956985474
iteration 231, loss = 0.0680735632777214
iteration 232, loss = 0.04665616899728775
iteration 233, loss = 0.0786895751953125
iteration 234, loss = 0.04808320477604866
iteration 235, loss = 0.06089480221271515
iteration 236, loss = 0.03746055066585541
iteration 237, loss = 0.061506059020757675
iteration 238, loss = 0.03777701035141945
iteration 239, loss = 0.03772346302866936
iteration 240, loss = 0.041783951222896576
iteration 241, loss = 0.0422905758023262
iteration 242, loss = 0.04449190944433212
iteration 243, loss = 0.051929861307144165
iteration 244, loss = 0.06830669194459915
iteration 245, loss = 0.05077241361141205
iteration 246, loss = 0.039060674607753754
iteration 247, loss = 0.037415292114019394
iteration 248, loss = 0.03570530563592911
iteration 249, loss = 0.03719482943415642
iteration 250, loss = 0.046566009521484375
iteration 251, loss = 0.04091197997331619
iteration 252, loss = 0.03763175755739212
iteration 253, loss = 0.03736228123307228
iteration 254, loss = 0.041757989674806595
iteration 255, loss = 0.04237566143274307
iteration 256, loss = 0.046127140522003174
iteration 257, loss = 0.03536492586135864
iteration 258, loss = 0.050534073263406754
iteration 259, loss = 0.03822234645485878
iteration 260, loss = 0.051061928272247314
iteration 261, loss = 0.04272675886750221
iteration 262, loss = 0.04199032112956047
iteration 263, loss = 0.042688485234975815
iteration 264, loss = 0.05446140468120575
iteration 265, loss = 0.03795372694730759
iteration 266, loss = 0.039243221282958984
iteration 267, loss = 0.037536025047302246
iteration 268, loss = 0.05144643038511276
iteration 269, loss = 0.03594594821333885
iteration 270, loss = 0.06155695766210556
iteration 271, loss = 0.0537637434899807
iteration 272, loss = 0.038174353539943695
iteration 273, loss = 0.0382249541580677
iteration 274, loss = 0.052518080919981
iteration 275, loss = 0.058223657310009
iteration 276, loss = 0.03527455776929855
iteration 277, loss = 0.04411478340625763
iteration 278, loss = 0.03799876198172569
iteration 279, loss = 0.04796833544969559
iteration 280, loss = 0.050729259848594666
iteration 281, loss = 0.03544288128614426
iteration 282, loss = 0.03523968532681465
iteration 283, loss = 0.03367394953966141
iteration 284, loss = 0.03947332873940468
iteration 285, loss = 0.039019644260406494
iteration 286, loss = 0.04347143694758415
iteration 287, loss = 0.036022454500198364
iteration 288, loss = 0.03391478955745697
iteration 289, loss = 0.046340446919202805
iteration 290, loss = 0.038943856954574585
iteration 291, loss = 0.03528042137622833
iteration 292, loss = 0.03883832320570946
iteration 293, loss = 0.04483892768621445
iteration 294, loss = 0.03650713339447975
iteration 295, loss = 0.0421120710670948
iteration 296, loss = 0.06271068751811981
iteration 297, loss = 0.05618833005428314
iteration 298, loss = 0.036287326365709305
iteration 299, loss = 0.032958898693323135
iteration 300, loss = 0.03531218320131302
iteration 1, loss = 0.05541107803583145
iteration 2, loss = 0.0330517441034317
iteration 3, loss = 0.03381326049566269
iteration 4, loss = 0.033735185861587524
iteration 5, loss = 0.040203794836997986
iteration 6, loss = 0.05093853920698166
iteration 7, loss = 0.03825852647423744
iteration 8, loss = 0.04661032557487488
iteration 9, loss = 0.030913768336176872
iteration 10, loss = 0.03508305549621582
iteration 11, loss = 0.05096379294991493
iteration 12, loss = 0.037719473242759705
iteration 13, loss = 0.05559263378381729
iteration 14, loss = 0.03437287360429764
iteration 15, loss = 0.04038345068693161
iteration 16, loss = 0.03159685432910919
iteration 17, loss = 0.06810549646615982
iteration 18, loss = 0.03259720653295517
iteration 19, loss = 0.038776200264692307
iteration 20, loss = 0.054382629692554474
iteration 21, loss = 0.03395005688071251
iteration 22, loss = 0.045088991522789
iteration 23, loss = 0.04831550270318985
iteration 24, loss = 0.03430778160691261
iteration 25, loss = 0.034734971821308136
iteration 26, loss = 0.04194461926817894
iteration 27, loss = 0.053090646862983704
iteration 28, loss = 0.03505270928144455
iteration 29, loss = 0.03192378953099251
iteration 30, loss = 0.06979773938655853
iteration 31, loss = 0.038259267807006836
iteration 32, loss = 0.03241540864109993
iteration 33, loss = 0.028292996808886528
iteration 34, loss = 0.04319794476032257
iteration 35, loss = 0.03195716440677643
iteration 36, loss = 0.032322607934474945
iteration 37, loss = 0.04375972971320152
iteration 38, loss = 0.033099785447120667
iteration 39, loss = 0.028653688728809357
iteration 40, loss = 0.03232540935277939
iteration 41, loss = 0.035541728138923645
iteration 42, loss = 0.03431207686662674
iteration 43, loss = 0.0282773207873106
iteration 44, loss = 0.05021432042121887
iteration 45, loss = 0.03717615827918053
iteration 46, loss = 0.03927095606923103
iteration 47, loss = 0.038617946207523346
iteration 48, loss = 0.0384405255317688
iteration 49, loss = 0.03188670054078102
iteration 50, loss = 0.0352158322930336
iteration 51, loss = 0.031405720859766006
iteration 52, loss = 0.0304269902408123
iteration 53, loss = 0.031116535887122154
iteration 54, loss = 0.03291968256235123
iteration 55, loss = 0.029688289389014244
iteration 56, loss = 0.031693216413259506
iteration 57, loss = 0.055855892598629
iteration 58, loss = 0.02983280085027218
iteration 59, loss = 0.040217190980911255
iteration 60, loss = 0.02895285189151764
iteration 61, loss = 0.046277545392513275
iteration 62, loss = 0.031074706465005875
iteration 63, loss = 0.02952965348958969
iteration 64, loss = 0.045015182346105576
iteration 65, loss = 0.043698184192180634
iteration 66, loss = 0.04043717682361603
iteration 67, loss = 0.031128574162721634
iteration 68, loss = 0.0332980677485466
iteration 69, loss = 0.029986608773469925
iteration 70, loss = 0.04449816420674324
iteration 71, loss = 0.028652053326368332
iteration 72, loss = 0.04258466511964798
iteration 73, loss = 0.029885128140449524
iteration 74, loss = 0.029604699462652206
iteration 75, loss = 0.032674241811037064
iteration 76, loss = 0.02830595336854458
iteration 77, loss = 0.029206326231360435
iteration 78, loss = 0.026971785351634026
iteration 79, loss = 0.03387066349387169
iteration 80, loss = 0.030229873955249786
iteration 81, loss = 0.02757134847342968
iteration 82, loss = 0.02829645946621895
iteration 83, loss = 0.03996008634567261
iteration 84, loss = 0.029138054698705673
iteration 85, loss = 0.028294013813138008
iteration 86, loss = 0.04690808057785034
iteration 87, loss = 0.02793416939675808
iteration 88, loss = 0.0321662612259388
iteration 89, loss = 0.029862700030207634
iteration 90, loss = 0.02828134596347809
iteration 91, loss = 0.029545150697231293
iteration 92, loss = 0.026505397632718086
iteration 93, loss = 0.030863728374242783
iteration 94, loss = 0.03355010598897934
iteration 95, loss = 0.04085734859108925
iteration 96, loss = 0.02898213267326355
iteration 97, loss = 0.02737255208194256
iteration 98, loss = 0.03111635334789753
iteration 99, loss = 0.03146360442042351
iteration 100, loss = 0.025297967717051506
iteration 101, loss = 0.02694016881287098
iteration 102, loss = 0.03742332383990288
iteration 103, loss = 0.03674686700105667
iteration 104, loss = 0.03065171092748642
iteration 105, loss = 0.028656072914600372
iteration 106, loss = 0.03488041087985039
iteration 107, loss = 0.027944989502429962
iteration 108, loss = 0.032250773161649704
iteration 109, loss = 0.02655053324997425
iteration 110, loss = 0.02825981192290783
iteration 111, loss = 0.025596454739570618
iteration 112, loss = 0.03230568394064903
iteration 113, loss = 0.02661672979593277
iteration 114, loss = 0.036995671689510345
iteration 115, loss = 0.027211248874664307
iteration 116, loss = 0.04381188377737999
iteration 117, loss = 0.029357826337218285
iteration 118, loss = 0.028091441839933395
iteration 119, loss = 0.034470755606889725
iteration 120, loss = 0.03448468819260597
iteration 121, loss = 0.029848286882042885
iteration 122, loss = 0.026386084035038948
iteration 123, loss = 0.030875127762556076
iteration 124, loss = 0.027441592887043953
iteration 125, loss = 0.05479783937335014
iteration 126, loss = 0.02566075325012207
iteration 127, loss = 0.02698928862810135
iteration 128, loss = 0.035591237246990204
iteration 129, loss = 0.03497036546468735
iteration 130, loss = 0.044321343302726746
iteration 131, loss = 0.03962833434343338
iteration 132, loss = 0.02465294487774372
iteration 133, loss = 0.0287702027708292
iteration 134, loss = 0.027041010558605194
iteration 135, loss = 0.026564978063106537
iteration 136, loss = 0.03680608049035072
iteration 137, loss = 0.025879470631480217
iteration 138, loss = 0.03299666568636894
iteration 139, loss = 0.03746692091226578
iteration 140, loss = 0.029267568141222
iteration 141, loss = 0.025906888768076897
iteration 142, loss = 0.05637842044234276
iteration 143, loss = 0.02641376480460167
iteration 144, loss = 0.02934025600552559
iteration 145, loss = 0.02434539422392845
iteration 146, loss = 0.03372446075081825
iteration 147, loss = 0.02757532149553299
iteration 148, loss = 0.028541572391986847
iteration 149, loss = 0.028542932122945786
iteration 150, loss = 0.032553594559431076
iteration 151, loss = 0.0403563529253006
iteration 152, loss = 0.03631196543574333
iteration 153, loss = 0.03567827492952347
iteration 154, loss = 0.02444165199995041
iteration 155, loss = 0.026043811812996864
iteration 156, loss = 0.022206775844097137
iteration 157, loss = 0.02443331480026245
iteration 158, loss = 0.02341451682150364
iteration 159, loss = 0.02473229356110096
iteration 160, loss = 0.028087912127375603
iteration 161, loss = 0.031049299985170364
iteration 162, loss = 0.028132084757089615
iteration 163, loss = 0.04298214986920357
iteration 164, loss = 0.03750651329755783
iteration 165, loss = 0.026548171415925026
iteration 166, loss = 0.028158972039818764
iteration 167, loss = 0.02457820437848568
iteration 168, loss = 0.024968942627310753
iteration 169, loss = 0.02415778674185276
iteration 170, loss = 0.023444658145308495
iteration 171, loss = 0.028465403243899345
iteration 172, loss = 0.02637515589594841
iteration 173, loss = 0.028314407914876938
iteration 174, loss = 0.026118606328964233
iteration 175, loss = 0.025310851633548737
iteration 176, loss = 0.02376312017440796
iteration 177, loss = 0.02399282343685627
iteration 178, loss = 0.031010502949357033
iteration 179, loss = 0.03047054074704647
iteration 180, loss = 0.027377400547266006
iteration 181, loss = 0.03486853092908859
iteration 182, loss = 0.024026665836572647
iteration 183, loss = 0.04336753487586975
iteration 184, loss = 0.030504032969474792
iteration 185, loss = 0.025796271860599518
iteration 186, loss = 0.025803688913583755
iteration 187, loss = 0.022570757195353508
iteration 188, loss = 0.024638285860419273
iteration 189, loss = 0.026530012488365173
iteration 190, loss = 0.026009978726506233
iteration 191, loss = 0.025496728718280792
iteration 192, loss = 0.024950984865427017
iteration 193, loss = 0.027918798848986626
iteration 194, loss = 0.03478225693106651
iteration 195, loss = 0.02430889755487442
iteration 196, loss = 0.02583041414618492
iteration 197, loss = 0.026471365243196487
iteration 198, loss = 0.024870920926332474
iteration 199, loss = 0.028366059064865112
iteration 200, loss = 0.023655010387301445
iteration 201, loss = 0.032004665583372116
iteration 202, loss = 0.026047296822071075
iteration 203, loss = 0.025467222556471825
iteration 204, loss = 0.03901829570531845
iteration 205, loss = 0.036645300686359406
iteration 206, loss = 0.021297555416822433
iteration 207, loss = 0.035868749022483826
iteration 208, loss = 0.02450176328420639
iteration 209, loss = 0.0248048584908247
iteration 210, loss = 0.03027830272912979
iteration 211, loss = 0.022926829755306244
iteration 212, loss = 0.02891835942864418
iteration 213, loss = 0.023931946605443954
iteration 214, loss = 0.02397608943283558
iteration 215, loss = 0.02678714506328106
iteration 216, loss = 0.021284546703100204
iteration 217, loss = 0.027240334078669548
iteration 218, loss = 0.023156724870204926
iteration 219, loss = 0.030414272099733353
iteration 220, loss = 0.040561869740486145
iteration 221, loss = 0.0310608372092247
iteration 222, loss = 0.02469971962273121
iteration 223, loss = 0.02414564974606037
iteration 224, loss = 0.0297933928668499
iteration 225, loss = 0.02342279441654682
iteration 226, loss = 0.03773308917880058
iteration 227, loss = 0.02154182270169258
iteration 228, loss = 0.05059769004583359
iteration 229, loss = 0.02198270708322525
iteration 230, loss = 0.022423580288887024
iteration 231, loss = 0.02354186773300171
iteration 232, loss = 0.02882000245153904
iteration 233, loss = 0.034162815660238266
iteration 234, loss = 0.023208513855934143
iteration 235, loss = 0.022433673962950706
iteration 236, loss = 0.022457305341959
iteration 237, loss = 0.02722020260989666
iteration 238, loss = 0.024226296693086624
iteration 239, loss = 0.024492524564266205
iteration 240, loss = 0.022457236424088478
iteration 241, loss = 0.02702324464917183
iteration 242, loss = 0.02567880228161812
iteration 243, loss = 0.036436811089515686
iteration 244, loss = 0.021707095205783844
iteration 245, loss = 0.03143145143985748
iteration 246, loss = 0.022135000675916672
iteration 247, loss = 0.020548688247799873
iteration 248, loss = 0.023804761469364166
iteration 249, loss = 0.02440996840596199
iteration 250, loss = 0.023633373901247978
iteration 251, loss = 0.026675017550587654
iteration 252, loss = 0.02485460788011551
iteration 253, loss = 0.04005051031708717
iteration 254, loss = 0.024626832455396652
iteration 255, loss = 0.041575103998184204
iteration 256, loss = 0.020907049998641014
iteration 257, loss = 0.021647436544299126
iteration 258, loss = 0.027732279151678085
iteration 259, loss = 0.02550102025270462
iteration 260, loss = 0.038364753127098083
iteration 261, loss = 0.01897352933883667
iteration 262, loss = 0.023443058133125305
iteration 263, loss = 0.021144535392522812
iteration 264, loss = 0.021793795749545097
iteration 265, loss = 0.02339373156428337
iteration 266, loss = 0.03047328256070614
iteration 267, loss = 0.018832962960004807
iteration 268, loss = 0.02600564993917942
iteration 269, loss = 0.020861629396677017
iteration 270, loss = 0.032953787595033646
iteration 271, loss = 0.02149028889834881
iteration 272, loss = 0.021463697776198387
iteration 273, loss = 0.022734293714165688
iteration 274, loss = 0.02148648537695408
iteration 275, loss = 0.029248718172311783
iteration 276, loss = 0.020668599754571915
iteration 277, loss = 0.021781230345368385
iteration 278, loss = 0.02858101949095726
iteration 279, loss = 0.02568114921450615
iteration 280, loss = 0.01860140450298786
iteration 281, loss = 0.02387385256588459
iteration 282, loss = 0.025167113170027733
iteration 283, loss = 0.03177532181143761
iteration 284, loss = 0.019488250836730003
iteration 285, loss = 0.03203408420085907
iteration 286, loss = 0.020592717453837395
iteration 287, loss = 0.02353314310312271
iteration 288, loss = 0.023469451814889908
iteration 289, loss = 0.022134928032755852
iteration 290, loss = 0.02219306118786335
iteration 291, loss = 0.02163652330636978
iteration 292, loss = 0.020474690943956375
iteration 293, loss = 0.019309496507048607
iteration 294, loss = 0.019496148452162743
iteration 295, loss = 0.020730365067720413
iteration 296, loss = 0.027574537321925163
iteration 297, loss = 0.035704243928194046
iteration 298, loss = 0.020198138430714607
iteration 299, loss = 0.024324899539351463
iteration 300, loss = 0.02023606188595295
iteration 1, loss = 0.017781367525458336
iteration 2, loss = 0.04231313243508339
iteration 3, loss = 0.019226759672164917
iteration 4, loss = 0.025007739663124084
iteration 5, loss = 0.01777910627424717
iteration 6, loss = 0.01913449726998806
iteration 7, loss = 0.0206138975918293
iteration 8, loss = 0.025225074961781502
iteration 9, loss = 0.01965566724538803
iteration 10, loss = 0.01902836561203003
iteration 11, loss = 0.022083651274442673
iteration 12, loss = 0.019921906292438507
iteration 13, loss = 0.026920072734355927
iteration 14, loss = 0.02759099379181862
iteration 15, loss = 0.024860238656401634
iteration 16, loss = 0.018868938088417053
iteration 17, loss = 0.03494517505168915
iteration 18, loss = 0.024808336049318314
iteration 19, loss = 0.023609967902302742
iteration 20, loss = 0.02019328437745571
iteration 21, loss = 0.020584791898727417
iteration 22, loss = 0.019374392926692963
iteration 23, loss = 0.02032116800546646
iteration 24, loss = 0.019879724830389023
iteration 25, loss = 0.01848708651959896
iteration 26, loss = 0.01822408102452755
iteration 27, loss = 0.017743747681379318
iteration 28, loss = 0.019416511058807373
iteration 29, loss = 0.03722390532493591
iteration 30, loss = 0.0198141448199749
iteration 31, loss = 0.02657327800989151
iteration 32, loss = 0.024101387709379196
iteration 33, loss = 0.02036854438483715
iteration 34, loss = 0.018080074340105057
iteration 35, loss = 0.021658191457390785
iteration 36, loss = 0.018821043893694878
iteration 37, loss = 0.021356308832764626
iteration 38, loss = 0.025552289560437202
iteration 39, loss = 0.020366966724395752
iteration 40, loss = 0.01838107593357563
iteration 41, loss = 0.020923510193824768
iteration 42, loss = 0.0173744298517704
iteration 43, loss = 0.021309182047843933
iteration 44, loss = 0.024347057566046715
iteration 45, loss = 0.019630787894129753
iteration 46, loss = 0.027079470455646515
iteration 47, loss = 0.03168219327926636
iteration 48, loss = 0.022929172962903976
iteration 49, loss = 0.018305618315935135
iteration 50, loss = 0.020195407792925835
iteration 51, loss = 0.01815733313560486
iteration 52, loss = 0.01796315610408783
iteration 53, loss = 0.02386295236647129
iteration 54, loss = 0.028678134083747864
iteration 55, loss = 0.024716900661587715
iteration 56, loss = 0.018917538225650787
iteration 57, loss = 0.022167490795254707
iteration 58, loss = 0.017594896256923676
iteration 59, loss = 0.016278743743896484
iteration 60, loss = 0.019386276602745056
iteration 61, loss = 0.026067322120070457
iteration 62, loss = 0.02266281098127365
iteration 63, loss = 0.022552676498889923
iteration 64, loss = 0.01733212172985077
iteration 65, loss = 0.02038603276014328
iteration 66, loss = 0.020778827369213104
iteration 67, loss = 0.016534065827727318
iteration 68, loss = 0.018019000068306923
iteration 69, loss = 0.024898400530219078
iteration 70, loss = 0.018683291971683502
iteration 71, loss = 0.019053112715482712
iteration 72, loss = 0.03708192706108093
iteration 73, loss = 0.018675949424505234
iteration 74, loss = 0.016775021329522133
iteration 75, loss = 0.017327332869172096
iteration 76, loss = 0.02976992353796959
iteration 77, loss = 0.02385595068335533
iteration 78, loss = 0.02509302645921707
iteration 79, loss = 0.019450940191745758
iteration 80, loss = 0.022401023656129837
iteration 81, loss = 0.01717516966164112
iteration 82, loss = 0.016915403306484222
iteration 83, loss = 0.027522047981619835
iteration 84, loss = 0.03179227560758591
iteration 85, loss = 0.017436707392334938
iteration 86, loss = 0.01889694854617119
iteration 87, loss = 0.017375828698277473
iteration 88, loss = 0.016411565244197845
iteration 89, loss = 0.01882869377732277
iteration 90, loss = 0.02352377027273178
iteration 91, loss = 0.019311506301164627
iteration 92, loss = 0.020305410027503967
iteration 93, loss = 0.027698583900928497
iteration 94, loss = 0.01729673519730568
iteration 95, loss = 0.029287932440638542
iteration 96, loss = 0.017915010452270508
iteration 97, loss = 0.018030598759651184
iteration 98, loss = 0.016280777752399445
iteration 99, loss = 0.0173686183989048
iteration 100, loss = 0.02015400119125843
iteration 101, loss = 0.021494267508387566
iteration 102, loss = 0.02041301690042019
iteration 103, loss = 0.016161978244781494
iteration 104, loss = 0.023392923176288605
iteration 105, loss = 0.02711811289191246
iteration 106, loss = 0.02087078057229519
iteration 107, loss = 0.022030679509043694
iteration 108, loss = 0.02226092293858528
iteration 109, loss = 0.03363317251205444
iteration 110, loss = 0.02064061351120472
iteration 111, loss = 0.022603143006563187
iteration 112, loss = 0.015516742132604122
iteration 113, loss = 0.015920346602797508
iteration 114, loss = 0.017649326473474503
iteration 115, loss = 0.015003055334091187
iteration 116, loss = 0.023997731506824493
iteration 117, loss = 0.019724570214748383
iteration 118, loss = 0.018653785809874535
iteration 119, loss = 0.018304454162716866
iteration 120, loss = 0.016911031678318977
iteration 121, loss = 0.017726652324199677
iteration 122, loss = 0.02192576229572296
iteration 123, loss = 0.021168431267142296
iteration 124, loss = 0.023834414780139923
iteration 125, loss = 0.015440714545547962
iteration 126, loss = 0.022278551012277603
iteration 127, loss = 0.02992623671889305
iteration 128, loss = 0.029706567525863647
iteration 129, loss = 0.016903147101402283
iteration 130, loss = 0.025383252650499344
iteration 131, loss = 0.022302763536572456
iteration 132, loss = 0.01586579903960228
iteration 133, loss = 0.01677568256855011
iteration 134, loss = 0.01695726253092289
iteration 135, loss = 0.017531506717205048
iteration 136, loss = 0.02002035640180111
iteration 137, loss = 0.017973925918340683
iteration 138, loss = 0.029757743701338768
iteration 139, loss = 0.016558779403567314
iteration 140, loss = 0.023252688348293304
iteration 141, loss = 0.028030674904584885
iteration 142, loss = 0.02087523601949215
iteration 143, loss = 0.019476886838674545
iteration 144, loss = 0.018480375409126282
iteration 145, loss = 0.020480193197727203
iteration 146, loss = 0.016709379851818085
iteration 147, loss = 0.014673128724098206
iteration 148, loss = 0.01731761358678341
iteration 149, loss = 0.02018585056066513
iteration 150, loss = 0.017658594995737076
iteration 151, loss = 0.01763426512479782
iteration 152, loss = 0.016693972051143646
iteration 153, loss = 0.028720593079924583
iteration 154, loss = 0.01550399698317051
iteration 155, loss = 0.014325364492833614
iteration 156, loss = 0.014760304242372513
iteration 157, loss = 0.014973292127251625
iteration 158, loss = 0.0164736770093441
iteration 159, loss = 0.017158465459942818
iteration 160, loss = 0.017614999786019325
iteration 161, loss = 0.01660715416073799
iteration 162, loss = 0.01626499742269516
iteration 163, loss = 0.017051879316568375
iteration 164, loss = 0.01880229264497757
iteration 165, loss = 0.015351089648902416
iteration 166, loss = 0.017504308372735977
iteration 167, loss = 0.01734938472509384
iteration 168, loss = 0.01496727392077446
iteration 169, loss = 0.018157415091991425
iteration 170, loss = 0.014987936243414879
iteration 171, loss = 0.016061857342720032
iteration 172, loss = 0.016695551574230194
iteration 173, loss = 0.017085745930671692
iteration 174, loss = 0.015802383422851562
iteration 175, loss = 0.016219861805438995
iteration 176, loss = 0.021975671872496605
iteration 177, loss = 0.016720090061426163
iteration 178, loss = 0.02059147320687771
iteration 179, loss = 0.01472790539264679
iteration 180, loss = 0.016412176191806793
iteration 181, loss = 0.0196079108864069
iteration 182, loss = 0.016443850472569466
iteration 183, loss = 0.02066676691174507
iteration 184, loss = 0.013728727586567402
iteration 185, loss = 0.018789462745189667
iteration 186, loss = 0.014678034000098705
iteration 187, loss = 0.01462116651237011
iteration 188, loss = 0.016787424683570862
iteration 189, loss = 0.014614908024668694
iteration 190, loss = 0.016599290072917938
iteration 191, loss = 0.016740156337618828
iteration 192, loss = 0.019229812547564507
iteration 193, loss = 0.015429547056555748
iteration 194, loss = 0.015704046934843063
iteration 195, loss = 0.014355224557220936
iteration 196, loss = 0.023941226303577423
iteration 197, loss = 0.0362996906042099
iteration 198, loss = 0.018208440393209457
iteration 199, loss = 0.018715085461735725
iteration 200, loss = 0.0173180028796196
iteration 201, loss = 0.02881641685962677
iteration 202, loss = 0.014118531718850136
iteration 203, loss = 0.02502450905740261
iteration 204, loss = 0.018314942717552185
iteration 205, loss = 0.01776890456676483
iteration 206, loss = 0.015374213457107544
iteration 207, loss = 0.018609583377838135
iteration 208, loss = 0.01989729143679142
iteration 209, loss = 0.020693842321634293
iteration 210, loss = 0.016016613692045212
iteration 211, loss = 0.014822154305875301
iteration 212, loss = 0.015384407714009285
iteration 213, loss = 0.013838902115821838
iteration 214, loss = 0.029554981738328934
iteration 215, loss = 0.017932485789060593
iteration 216, loss = 0.017376791685819626
iteration 217, loss = 0.015528579242527485
iteration 218, loss = 0.01404582429677248
iteration 219, loss = 0.014429264701902866
iteration 220, loss = 0.015278731472790241
iteration 221, loss = 0.021774455904960632
iteration 222, loss = 0.016601357609033585
iteration 223, loss = 0.02926703542470932
iteration 224, loss = 0.01694365032017231
iteration 225, loss = 0.014417910017073154
iteration 226, loss = 0.015929248183965683
iteration 227, loss = 0.016314804553985596
iteration 228, loss = 0.01511702686548233
iteration 229, loss = 0.014650315046310425
iteration 230, loss = 0.014407490380108356
iteration 231, loss = 0.012479407712817192
iteration 232, loss = 0.03423542156815529
iteration 233, loss = 0.01908290758728981
iteration 234, loss = 0.014012787491083145
iteration 235, loss = 0.014929292723536491
iteration 236, loss = 0.013407021760940552
iteration 237, loss = 0.015357335098087788
iteration 238, loss = 0.013943526893854141
iteration 239, loss = 0.014328915625810623
iteration 240, loss = 0.014716963283717632
iteration 241, loss = 0.013771843165159225
iteration 242, loss = 0.014095423743128777
iteration 243, loss = 0.014917420223355293
iteration 244, loss = 0.015298731625080109
iteration 245, loss = 0.021566525101661682
iteration 246, loss = 0.017941368743777275
iteration 247, loss = 0.0179668627679348
iteration 248, loss = 0.015545401722192764
iteration 249, loss = 0.016327720135450363
iteration 250, loss = 0.014340383000671864
iteration 251, loss = 0.013278772123157978
iteration 252, loss = 0.019636675715446472
iteration 253, loss = 0.014038329012691975
iteration 254, loss = 0.01723659783601761
iteration 255, loss = 0.014154380187392235
iteration 256, loss = 0.018683573231101036
iteration 257, loss = 0.015153934247791767
iteration 258, loss = 0.014787285588681698
iteration 259, loss = 0.014281540177762508
iteration 260, loss = 0.014448744244873524
iteration 261, loss = 0.013890216127038002
iteration 262, loss = 0.01498144306242466
iteration 263, loss = 0.01953994482755661
iteration 264, loss = 0.0161091610789299
iteration 265, loss = 0.02405083365738392
iteration 266, loss = 0.015558840706944466
iteration 267, loss = 0.01581864431500435
iteration 268, loss = 0.01284795068204403
iteration 269, loss = 0.013394083827733994
iteration 270, loss = 0.014349137432873249
iteration 271, loss = 0.013424845412373543
iteration 272, loss = 0.01860230229794979
iteration 273, loss = 0.014354986138641834
iteration 274, loss = 0.020450279116630554
iteration 275, loss = 0.014336640946567059
iteration 276, loss = 0.017632272094488144
iteration 277, loss = 0.013384247198700905
iteration 278, loss = 0.024034421890974045
iteration 279, loss = 0.016220832243561745
iteration 280, loss = 0.012896607629954815
iteration 281, loss = 0.01307840645313263
iteration 282, loss = 0.014646660536527634
iteration 283, loss = 0.013198425062000751
iteration 284, loss = 0.01281089335680008
iteration 285, loss = 0.013355344533920288
iteration 286, loss = 0.014448055997490883
iteration 287, loss = 0.014947597868740559
iteration 288, loss = 0.013896968215703964
iteration 289, loss = 0.018272319808602333
iteration 290, loss = 0.014192704111337662
iteration 291, loss = 0.0129551338031888
iteration 292, loss = 0.016540592536330223
iteration 293, loss = 0.013615665957331657
iteration 294, loss = 0.014401793479919434
iteration 295, loss = 0.015224501490592957
iteration 296, loss = 0.013454770669341087
iteration 297, loss = 0.013251954689621925
iteration 298, loss = 0.019054338335990906
iteration 299, loss = 0.012400619685649872
iteration 300, loss = 0.013906808570027351
iteration 1, loss = 0.015140922740101814
iteration 2, loss = 0.015434805303812027
iteration 3, loss = 0.012057256884872913
iteration 4, loss = 0.012154190801084042
iteration 5, loss = 0.01829434186220169
iteration 6, loss = 0.016715247184038162
iteration 7, loss = 0.018986813724040985
iteration 8, loss = 0.01650342158973217
iteration 9, loss = 0.01224704273045063
iteration 10, loss = 0.016351774334907532
iteration 11, loss = 0.02085423842072487
iteration 12, loss = 0.012383513152599335
iteration 13, loss = 0.0145915811881423
iteration 14, loss = 0.014187552034854889
iteration 15, loss = 0.01361328549683094
iteration 16, loss = 0.011524015106260777
iteration 17, loss = 0.016719389706850052
iteration 18, loss = 0.01593398116528988
iteration 19, loss = 0.011889545246958733
iteration 20, loss = 0.013847620226442814
iteration 21, loss = 0.014522328041493893
iteration 22, loss = 0.014636117964982986
iteration 23, loss = 0.012226369231939316
iteration 24, loss = 0.011327045038342476
iteration 25, loss = 0.013461355119943619
iteration 26, loss = 0.018314961344003677
iteration 27, loss = 0.01354660652577877
iteration 28, loss = 0.01375349797308445
iteration 29, loss = 0.018000002950429916
iteration 30, loss = 0.015647346153855324
iteration 31, loss = 0.013140971772372723
iteration 32, loss = 0.017815956845879555
iteration 33, loss = 0.020052189007401466
iteration 34, loss = 0.01364734023809433
iteration 35, loss = 0.01209690049290657
iteration 36, loss = 0.018772728741168976
iteration 37, loss = 0.01317170262336731
iteration 38, loss = 0.011995247565209866
iteration 39, loss = 0.014554612338542938
iteration 40, loss = 0.013853229582309723
iteration 41, loss = 0.014627103693783283
iteration 42, loss = 0.012499441392719746
iteration 43, loss = 0.016778593882918358
iteration 44, loss = 0.01581769436597824
iteration 45, loss = 0.011039944365620613
iteration 46, loss = 0.013708646409213543
iteration 47, loss = 0.019277390092611313
iteration 48, loss = 0.018054349347949028
iteration 49, loss = 0.01633376255631447
iteration 50, loss = 0.012615958228707314
iteration 51, loss = 0.014353598468005657
iteration 52, loss = 0.013182214461266994
iteration 53, loss = 0.015556808561086655
iteration 54, loss = 0.01093048881739378
iteration 55, loss = 0.012317301705479622
iteration 56, loss = 0.017829101532697678
iteration 57, loss = 0.010985108092427254
iteration 58, loss = 0.01274128071963787
iteration 59, loss = 0.016050444915890694
iteration 60, loss = 0.012431609444320202
iteration 61, loss = 0.014852412976324558
iteration 62, loss = 0.011874408461153507
iteration 63, loss = 0.013436787761747837
iteration 64, loss = 0.025963323190808296
iteration 65, loss = 0.012337274849414825
iteration 66, loss = 0.012139221653342247
iteration 67, loss = 0.011792227625846863
iteration 68, loss = 0.011036798357963562
iteration 69, loss = 0.013214456848800182
iteration 70, loss = 0.014194217510521412
iteration 71, loss = 0.012480361387133598
iteration 72, loss = 0.011669839732348919
iteration 73, loss = 0.01480462308973074
iteration 74, loss = 0.015426866710186005
iteration 75, loss = 0.012798255309462547
iteration 76, loss = 0.015850035473704338
iteration 77, loss = 0.01290130615234375
iteration 78, loss = 0.021796217188239098
iteration 79, loss = 0.011281436309218407
iteration 80, loss = 0.011663737706840038
iteration 81, loss = 0.012974320910871029
iteration 82, loss = 0.012828107923269272
iteration 83, loss = 0.013801082037389278
iteration 84, loss = 0.0124730309471488
iteration 85, loss = 0.014268344268202782
iteration 86, loss = 0.011527344584465027
iteration 87, loss = 0.011994375847280025
iteration 88, loss = 0.01432048436254263
iteration 89, loss = 0.012586146593093872
iteration 90, loss = 0.011568632908165455
iteration 91, loss = 0.01175930630415678
iteration 92, loss = 0.011431838385760784
iteration 93, loss = 0.011773867532610893
iteration 94, loss = 0.0116643738001585
iteration 95, loss = 0.01225520484149456
iteration 96, loss = 0.013006428256630898
iteration 97, loss = 0.011252854019403458
iteration 98, loss = 0.011620201170444489
iteration 99, loss = 0.016537263989448547
iteration 100, loss = 0.01212435681372881
iteration 101, loss = 0.015673844143748283
iteration 102, loss = 0.011526063084602356
iteration 103, loss = 0.013957345858216286
iteration 104, loss = 0.010550959035754204
iteration 105, loss = 0.01086078304797411
iteration 106, loss = 0.02132723294198513
iteration 107, loss = 0.012025113217532635
iteration 108, loss = 0.011438599787652493
iteration 109, loss = 0.017227500677108765
iteration 110, loss = 0.02102416381239891
iteration 111, loss = 0.012249781750142574
iteration 112, loss = 0.01242708321660757
iteration 113, loss = 0.022195380181074142
iteration 114, loss = 0.011160019785165787
iteration 115, loss = 0.015145724639296532
iteration 116, loss = 0.013142303563654423
iteration 117, loss = 0.011030159890651703
iteration 118, loss = 0.012654189020395279
iteration 119, loss = 0.014272687956690788
iteration 120, loss = 0.01178552582859993
iteration 121, loss = 0.012802057899534702
iteration 122, loss = 0.011334101669490337
iteration 123, loss = 0.010912853293120861
iteration 124, loss = 0.012369666248559952
iteration 125, loss = 0.011422799900174141
iteration 126, loss = 0.01105106808245182
iteration 127, loss = 0.010647597722709179
iteration 128, loss = 0.015471121296286583
iteration 129, loss = 0.012420248240232468
iteration 130, loss = 0.01080664899200201
iteration 131, loss = 0.011107033118605614
iteration 132, loss = 0.02078571729362011
iteration 133, loss = 0.016939982771873474
iteration 134, loss = 0.011517882347106934
iteration 135, loss = 0.019993780180811882
iteration 136, loss = 0.017780765891075134
iteration 137, loss = 0.012445981614291668
iteration 138, loss = 0.015489520505070686
iteration 139, loss = 0.010865180753171444
iteration 140, loss = 0.010173710063099861
iteration 141, loss = 0.010610281489789486
iteration 142, loss = 0.01043570227921009
iteration 143, loss = 0.011717729270458221
iteration 144, loss = 0.010695808567106724
iteration 145, loss = 0.011229349300265312
iteration 146, loss = 0.013612666167318821
iteration 147, loss = 0.01161140389740467
iteration 148, loss = 0.015369775705039501
iteration 149, loss = 0.010735777206718922
iteration 150, loss = 0.011788829229772091
iteration 151, loss = 0.009622626937925816
iteration 152, loss = 0.02331334352493286
iteration 153, loss = 0.024817854166030884
iteration 154, loss = 0.01495798397809267
iteration 155, loss = 0.012695105746388435
iteration 156, loss = 0.01057395152747631
iteration 157, loss = 0.01183382049202919
iteration 158, loss = 0.01631816290318966
iteration 159, loss = 0.010083639062941074
iteration 160, loss = 0.012652480974793434
iteration 161, loss = 0.012818451970815659
iteration 162, loss = 0.01409904658794403
iteration 163, loss = 0.01067035086452961
iteration 164, loss = 0.012875008396804333
iteration 165, loss = 0.02318549156188965
iteration 166, loss = 0.010323743335902691
iteration 167, loss = 0.01220344752073288
iteration 168, loss = 0.018659524619579315
iteration 169, loss = 0.011163479648530483
iteration 170, loss = 0.010729026980698109
iteration 171, loss = 0.010669649578630924
iteration 172, loss = 0.01151877362281084
iteration 173, loss = 0.010528543964028358
iteration 174, loss = 0.011442799121141434
iteration 175, loss = 0.012813013978302479
iteration 176, loss = 0.011146534234285355
iteration 177, loss = 0.013039007782936096
iteration 178, loss = 0.01012010034173727
iteration 179, loss = 0.016040632501244545
iteration 180, loss = 0.010478540323674679
iteration 181, loss = 0.012411074712872505
iteration 182, loss = 0.011418705806136131
iteration 183, loss = 0.010890904814004898
iteration 184, loss = 0.010458545759320259
iteration 185, loss = 0.010621757246553898
iteration 186, loss = 0.012241121381521225
iteration 187, loss = 0.01028905063867569
iteration 188, loss = 0.009916256181895733
iteration 189, loss = 0.01279502920806408
iteration 190, loss = 0.012467682361602783
iteration 191, loss = 0.01375442836433649
iteration 192, loss = 0.01493491604924202
iteration 193, loss = 0.015408935025334358
iteration 194, loss = 0.009949284605681896
iteration 195, loss = 0.013247162103652954
iteration 196, loss = 0.011348745785653591
iteration 197, loss = 0.012470388785004616
iteration 198, loss = 0.009873675182461739
iteration 199, loss = 0.009990261867642403
iteration 200, loss = 0.013487974181771278
iteration 201, loss = 0.010796567425131798
iteration 202, loss = 0.012728454545140266
iteration 203, loss = 0.012246064841747284
iteration 204, loss = 0.0095201525837183
iteration 205, loss = 0.012732455506920815
iteration 206, loss = 0.009484419599175453
iteration 207, loss = 0.00985513161867857
iteration 208, loss = 0.014467692002654076
iteration 209, loss = 0.010034271515905857
iteration 210, loss = 0.011326881125569344
iteration 211, loss = 0.016921505331993103
iteration 212, loss = 0.016650455072522163
iteration 213, loss = 0.014876319095492363
iteration 214, loss = 0.009704630821943283
iteration 215, loss = 0.00972768384963274
iteration 216, loss = 0.011510301381349564
iteration 217, loss = 0.01460639014840126
iteration 218, loss = 0.012886806391179562
iteration 219, loss = 0.00992388091981411
iteration 220, loss = 0.014161638915538788
iteration 221, loss = 0.010774987749755383
iteration 222, loss = 0.01056844275444746
iteration 223, loss = 0.013070644810795784
iteration 224, loss = 0.01098608784377575
iteration 225, loss = 0.013657614588737488
iteration 226, loss = 0.010934604331851006
iteration 227, loss = 0.019173672422766685
iteration 228, loss = 0.00985465943813324
iteration 229, loss = 0.018650583922863007
iteration 230, loss = 0.020052988082170486
iteration 231, loss = 0.0103854238986969
iteration 232, loss = 0.014310155995190144
iteration 233, loss = 0.010822786018252373
iteration 234, loss = 0.017248446121811867
iteration 235, loss = 0.010851466096937656
iteration 236, loss = 0.010962490923702717
iteration 237, loss = 0.01147619541734457
iteration 238, loss = 0.013852732256054878
iteration 239, loss = 0.010982242412865162
iteration 240, loss = 0.012867048382759094
iteration 241, loss = 0.010163748636841774
iteration 242, loss = 0.012228652834892273
iteration 243, loss = 0.013915307819843292
iteration 244, loss = 0.010031295008957386
iteration 245, loss = 0.009828194044530392
iteration 246, loss = 0.010289989411830902
iteration 247, loss = 0.010530125349760056
iteration 248, loss = 0.01052065659314394
iteration 249, loss = 0.010604141280055046
iteration 250, loss = 0.011667892336845398
iteration 251, loss = 0.009440978057682514
iteration 252, loss = 0.012500274926424026
iteration 253, loss = 0.00994809903204441
iteration 254, loss = 0.011148704215884209
iteration 255, loss = 0.014772110618650913
iteration 256, loss = 0.009347878396511078
iteration 257, loss = 0.008851280435919762
iteration 258, loss = 0.01614077389240265
iteration 259, loss = 0.010029565542936325
iteration 260, loss = 0.010442491620779037
iteration 261, loss = 0.011709664016962051
iteration 262, loss = 0.014873328618705273
iteration 263, loss = 0.009638240560889244
iteration 264, loss = 0.012423893436789513
iteration 265, loss = 0.017970796674489975
iteration 266, loss = 0.013624168001115322
iteration 267, loss = 0.011241782456636429
iteration 268, loss = 0.014467155560851097
iteration 269, loss = 0.010802960023283958
iteration 270, loss = 0.009970206767320633
iteration 271, loss = 0.014049284160137177
iteration 272, loss = 0.009549897164106369
iteration 273, loss = 0.01424503419548273
iteration 274, loss = 0.009048834443092346
iteration 275, loss = 0.01088164933025837
iteration 276, loss = 0.012545408681035042
iteration 277, loss = 0.011270256713032722
iteration 278, loss = 0.011870289221405983
iteration 279, loss = 0.009757224470376968
iteration 280, loss = 0.009187466464936733
iteration 281, loss = 0.011796578764915466
iteration 282, loss = 0.012759493663907051
iteration 283, loss = 0.009317578747868538
iteration 284, loss = 0.010444091632962227
iteration 285, loss = 0.010708825662732124
iteration 286, loss = 0.008910070173442364
iteration 287, loss = 0.0167375635355711
iteration 288, loss = 0.0215787123888731
iteration 289, loss = 0.017898105084896088
iteration 290, loss = 0.010222393088042736
iteration 291, loss = 0.009442958980798721
iteration 292, loss = 0.009747296571731567
iteration 293, loss = 0.00866697821766138
iteration 294, loss = 0.010010561905801296
iteration 295, loss = 0.00923785101622343
iteration 296, loss = 0.010178099386394024
iteration 297, loss = 0.010520135052502155
iteration 298, loss = 0.010039125569164753
iteration 299, loss = 0.011656785383820534
iteration 300, loss = 0.009755455888807774
iteration 1, loss = 0.00910856667906046
iteration 2, loss = 0.008693085983395576
iteration 3, loss = 0.009712059050798416
iteration 4, loss = 0.009950485080480576
iteration 5, loss = 0.012586216442286968
iteration 6, loss = 0.009538108482956886
iteration 7, loss = 0.009742384776473045
iteration 8, loss = 0.016987673938274384
iteration 9, loss = 0.011261597275733948
iteration 10, loss = 0.0087198531255126
iteration 11, loss = 0.009334704838693142
iteration 12, loss = 0.008753461763262749
iteration 13, loss = 0.00906726811081171
iteration 14, loss = 0.009517055004835129
iteration 15, loss = 0.009815449826419353
iteration 16, loss = 0.008356759324669838
iteration 17, loss = 0.010959470644593239
iteration 18, loss = 0.009961554780602455
iteration 19, loss = 0.008625788614153862
iteration 20, loss = 0.011626343242824078
iteration 21, loss = 0.009686323814094067
iteration 22, loss = 0.01220924872905016
iteration 23, loss = 0.009135891683399677
iteration 24, loss = 0.012538366951048374
iteration 25, loss = 0.009507356211543083
iteration 26, loss = 0.008984282612800598
iteration 27, loss = 0.010230491869151592
iteration 28, loss = 0.008854713290929794
iteration 29, loss = 0.010873688384890556
iteration 30, loss = 0.008391063660383224
iteration 31, loss = 0.01794031262397766
iteration 32, loss = 0.010823143646121025
iteration 33, loss = 0.012006523087620735
iteration 34, loss = 0.01673528179526329
iteration 35, loss = 0.008574228733778
iteration 36, loss = 0.010283867828547955
iteration 37, loss = 0.00973569042980671
iteration 38, loss = 0.008938977494835854
iteration 39, loss = 0.012042609043419361
iteration 40, loss = 0.009544250555336475
iteration 41, loss = 0.01151955220848322
iteration 42, loss = 0.01780874840915203
iteration 43, loss = 0.008846510201692581
iteration 44, loss = 0.0084939394146204
iteration 45, loss = 0.00909295305609703
iteration 46, loss = 0.009991355240345001
iteration 47, loss = 0.00999713595956564
iteration 48, loss = 0.008593226782977581
iteration 49, loss = 0.008697539567947388
iteration 50, loss = 0.009055432863533497
iteration 51, loss = 0.008607779629528522
iteration 52, loss = 0.008299224078655243
iteration 53, loss = 0.015986185520887375
iteration 54, loss = 0.009332315064966679
iteration 55, loss = 0.010594446212053299
iteration 56, loss = 0.013228710740804672
iteration 57, loss = 0.011312437243759632
iteration 58, loss = 0.016688743606209755
iteration 59, loss = 0.009182951413094997
iteration 60, loss = 0.013583964668214321
iteration 61, loss = 0.010374215431511402
iteration 62, loss = 0.009395546279847622
iteration 63, loss = 0.007987059652805328
iteration 64, loss = 0.010883992537856102
iteration 65, loss = 0.011312801390886307
iteration 66, loss = 0.009845930151641369
iteration 67, loss = 0.009231344796717167
iteration 68, loss = 0.00886461604386568
iteration 69, loss = 0.008449369110167027
iteration 70, loss = 0.011368214152753353
iteration 71, loss = 0.008658068254590034
iteration 72, loss = 0.01720399223268032
iteration 73, loss = 0.012698653154075146
iteration 74, loss = 0.00800674594938755
iteration 75, loss = 0.009727424010634422
iteration 76, loss = 0.009720874950289726
iteration 77, loss = 0.013880781829357147
iteration 78, loss = 0.009156458079814911
iteration 79, loss = 0.010832710191607475
iteration 80, loss = 0.017573341727256775
iteration 81, loss = 0.008387289941310883
iteration 82, loss = 0.007908652536571026
iteration 83, loss = 0.010640916414558887
iteration 84, loss = 0.0098533621057868
iteration 85, loss = 0.009725882671773434
iteration 86, loss = 0.008375789038836956
iteration 87, loss = 0.013518717139959335
iteration 88, loss = 0.008117938414216042
iteration 89, loss = 0.013372505083680153
iteration 90, loss = 0.00974266417324543
iteration 91, loss = 0.0094184260815382
iteration 92, loss = 0.01655898056924343
iteration 93, loss = 0.00889851339161396
iteration 94, loss = 0.0093445535749197
iteration 95, loss = 0.009547710418701172
iteration 96, loss = 0.00822520162910223
iteration 97, loss = 0.010434038005769253
iteration 98, loss = 0.009141852147877216
iteration 99, loss = 0.008829798549413681
iteration 100, loss = 0.0077845933847129345
iteration 101, loss = 0.009279615245759487
iteration 102, loss = 0.00899270549416542
iteration 103, loss = 0.011379837989807129
iteration 104, loss = 0.008902465924620628
iteration 105, loss = 0.012991629540920258
iteration 106, loss = 0.010853595100343227
iteration 107, loss = 0.020726144313812256
iteration 108, loss = 0.010839001275599003
iteration 109, loss = 0.009410550817847252
iteration 110, loss = 0.01046448852866888
iteration 111, loss = 0.00748338783159852
iteration 112, loss = 0.013410409912467003
iteration 113, loss = 0.00776214525103569
iteration 114, loss = 0.008466901257634163
iteration 115, loss = 0.008852316066622734
iteration 116, loss = 0.012614002451300621
iteration 117, loss = 0.008419239893555641
iteration 118, loss = 0.008008991368114948
iteration 119, loss = 0.008667618967592716
iteration 120, loss = 0.008617470040917397
iteration 121, loss = 0.009019564837217331
iteration 122, loss = 0.008955834433436394
iteration 123, loss = 0.009045897051692009
iteration 124, loss = 0.00821754615753889
iteration 125, loss = 0.009571605361998081
iteration 126, loss = 0.00847643706947565
iteration 127, loss = 0.007360535673797131
iteration 128, loss = 0.008630524389445782
iteration 129, loss = 0.007178512867540121
iteration 130, loss = 0.009113943204283714
iteration 131, loss = 0.010679507628083229
iteration 132, loss = 0.00804990902543068
iteration 133, loss = 0.0085697490721941
iteration 134, loss = 0.008477654308080673
iteration 135, loss = 0.008390353992581367
iteration 136, loss = 0.01076772902160883
iteration 137, loss = 0.007870860397815704
iteration 138, loss = 0.007696399465203285
iteration 139, loss = 0.010002758353948593
iteration 140, loss = 0.010266133584082127
iteration 141, loss = 0.01675659790635109
iteration 142, loss = 0.008490328676998615
iteration 143, loss = 0.008274262771010399
iteration 144, loss = 0.007086333353072405
iteration 145, loss = 0.010006461292505264
iteration 146, loss = 0.011218223720788956
iteration 147, loss = 0.00907150935381651
iteration 148, loss = 0.007334190886467695
iteration 149, loss = 0.008207261562347412
iteration 150, loss = 0.009955400601029396
iteration 151, loss = 0.014982951804995537
iteration 152, loss = 0.011552256532013416
iteration 153, loss = 0.012188824824988842
iteration 154, loss = 0.007184129673987627
iteration 155, loss = 0.007352846674621105
iteration 156, loss = 0.008794478140771389
iteration 157, loss = 0.011760375462472439
iteration 158, loss = 0.00908952672034502
iteration 159, loss = 0.009548685513436794
iteration 160, loss = 0.007478890474885702
iteration 161, loss = 0.007938326336443424
iteration 162, loss = 0.008173000067472458
iteration 163, loss = 0.011610505171120167
iteration 164, loss = 0.007154167629778385
iteration 165, loss = 0.00742137199267745
iteration 166, loss = 0.011965740472078323
iteration 167, loss = 0.008398058824241161
iteration 168, loss = 0.008261938579380512
iteration 169, loss = 0.011111985892057419
iteration 170, loss = 0.007567678578197956
iteration 171, loss = 0.010312332771718502
iteration 172, loss = 0.010501330718398094
iteration 173, loss = 0.00887544546276331
iteration 174, loss = 0.014754112809896469
iteration 175, loss = 0.007557980250567198
iteration 176, loss = 0.01002809964120388
iteration 177, loss = 0.007071486208587885
iteration 178, loss = 0.012684911489486694
iteration 179, loss = 0.010239661671221256
iteration 180, loss = 0.01159108430147171
iteration 181, loss = 0.008299250155687332
iteration 182, loss = 0.007678942289203405
iteration 183, loss = 0.007487133145332336
iteration 184, loss = 0.008620880544185638
iteration 185, loss = 0.008259525522589684
iteration 186, loss = 0.007813982665538788
iteration 187, loss = 0.008914299309253693
iteration 188, loss = 0.012040708214044571
iteration 189, loss = 0.00747673399746418
iteration 190, loss = 0.01519852690398693
iteration 191, loss = 0.009972691535949707
iteration 192, loss = 0.008767952211201191
iteration 193, loss = 0.007975371554493904
iteration 194, loss = 0.009263678453862667
iteration 195, loss = 0.01741904392838478
iteration 196, loss = 0.014375831931829453
iteration 197, loss = 0.008030051365494728
iteration 198, loss = 0.008935810066759586
iteration 199, loss = 0.011132044717669487
iteration 200, loss = 0.008405652828514576
iteration 201, loss = 0.014003019779920578
iteration 202, loss = 0.010293870232999325
iteration 203, loss = 0.00696319155395031
iteration 204, loss = 0.007712045684456825
iteration 205, loss = 0.007896032184362411
iteration 206, loss = 0.009255878627300262
iteration 207, loss = 0.009889289736747742
iteration 208, loss = 0.009463380090892315
iteration 209, loss = 0.007333643268793821
iteration 210, loss = 0.0068537103943526745
iteration 211, loss = 0.007345279213041067
iteration 212, loss = 0.008038044907152653
iteration 213, loss = 0.007699952926486731
iteration 214, loss = 0.011119057424366474
iteration 215, loss = 0.007783546578139067
iteration 216, loss = 0.00798771996051073
iteration 217, loss = 0.007829781621694565
iteration 218, loss = 0.011768401600420475
iteration 219, loss = 0.008712010458111763
iteration 220, loss = 0.01001420896500349
iteration 221, loss = 0.006597389001399279
iteration 222, loss = 0.010240369476377964
iteration 223, loss = 0.008575793355703354
iteration 224, loss = 0.007643136661499739
iteration 225, loss = 0.013874131254851818
iteration 226, loss = 0.009267309680581093
iteration 227, loss = 0.00795343890786171
iteration 228, loss = 0.010607094503939152
iteration 229, loss = 0.015653157606720924
iteration 230, loss = 0.007901695556938648
iteration 231, loss = 0.007795402780175209
iteration 232, loss = 0.009487466886639595
iteration 233, loss = 0.009356594644486904
iteration 234, loss = 0.01050689909607172
iteration 235, loss = 0.007907599210739136
iteration 236, loss = 0.009720932692289352
iteration 237, loss = 0.007659178227186203
iteration 238, loss = 0.00713923666626215
iteration 239, loss = 0.009947855025529861
iteration 240, loss = 0.0070711164735257626
iteration 241, loss = 0.008812232874333858
iteration 242, loss = 0.008828368969261646
iteration 243, loss = 0.007357546593993902
iteration 244, loss = 0.009070439264178276
iteration 245, loss = 0.007249950431287289
iteration 246, loss = 0.009820432402193546
iteration 247, loss = 0.007458635605871677
iteration 248, loss = 0.008537795394659042
iteration 249, loss = 0.007390834391117096
iteration 250, loss = 0.00857889000326395
iteration 251, loss = 0.014700513333082199
iteration 252, loss = 0.012012584134936333
iteration 253, loss = 0.008405633270740509
iteration 254, loss = 0.010287603363394737
iteration 255, loss = 0.008871210739016533
iteration 256, loss = 0.008534790948033333
iteration 257, loss = 0.008547657169401646
iteration 258, loss = 0.007943706586956978
iteration 259, loss = 0.008834082633256912
iteration 260, loss = 0.008581831119954586
iteration 261, loss = 0.007278772536665201
iteration 262, loss = 0.00688602589070797
iteration 263, loss = 0.009270558133721352
iteration 264, loss = 0.0077638765797019005
iteration 265, loss = 0.0065359026193618774
iteration 266, loss = 0.00862694252282381
iteration 267, loss = 0.008199864998459816
iteration 268, loss = 0.0074503314681351185
iteration 269, loss = 0.007511687930673361
iteration 270, loss = 0.00766780087724328
iteration 271, loss = 0.006696479395031929
iteration 272, loss = 0.007463305722922087
iteration 273, loss = 0.007302256301045418
iteration 274, loss = 0.0077418009750545025
iteration 275, loss = 0.00938746239989996
iteration 276, loss = 0.007571451365947723
iteration 277, loss = 0.007033763453364372
iteration 278, loss = 0.009971785359084606
iteration 279, loss = 0.007067542988806963
iteration 280, loss = 0.00829329527914524
iteration 281, loss = 0.009663526900112629
iteration 282, loss = 0.010044754482805729
iteration 283, loss = 0.007355951704084873
iteration 284, loss = 0.00835837610065937
iteration 285, loss = 0.006711579859256744
iteration 286, loss = 0.00878919754177332
iteration 287, loss = 0.00820225477218628
iteration 288, loss = 0.008749140426516533
iteration 289, loss = 0.006261204369366169
iteration 290, loss = 0.00796473491936922
iteration 291, loss = 0.010049001313745975
iteration 292, loss = 0.006657353602349758
iteration 293, loss = 0.007253292948007584
iteration 294, loss = 0.00874808058142662
iteration 295, loss = 0.006914869416505098
iteration 296, loss = 0.008654422126710415
iteration 297, loss = 0.007451407611370087
iteration 298, loss = 0.007001033518463373
iteration 299, loss = 0.009424579329788685
iteration 300, loss = 0.007457061670720577
iteration 1, loss = 0.006372656673192978
iteration 2, loss = 0.00695719663053751
iteration 3, loss = 0.0073380544781684875
iteration 4, loss = 0.007625749334692955
iteration 5, loss = 0.007051072549074888
iteration 6, loss = 0.006032595876604319
iteration 7, loss = 0.013536343351006508
iteration 8, loss = 0.007643213961273432
iteration 9, loss = 0.012913660146296024
iteration 10, loss = 0.007180674467235804
iteration 11, loss = 0.008639946579933167
iteration 12, loss = 0.009117399342358112
iteration 13, loss = 0.011545826680958271
iteration 14, loss = 0.010484062135219574
iteration 15, loss = 0.011862183921039104
iteration 16, loss = 0.006891386583447456
iteration 17, loss = 0.007562391459941864
iteration 18, loss = 0.010860376060009003
iteration 19, loss = 0.007273302413523197
iteration 20, loss = 0.006822692230343819
iteration 21, loss = 0.008677319623529911
iteration 22, loss = 0.013615548610687256
iteration 23, loss = 0.008670066483318806
iteration 24, loss = 0.007229037582874298
iteration 25, loss = 0.010342060588300228
iteration 26, loss = 0.008510546758770943
iteration 27, loss = 0.014575105160474777
iteration 28, loss = 0.00895459484308958
iteration 29, loss = 0.006598711013793945
iteration 30, loss = 0.0072884042747318745
iteration 31, loss = 0.006876280531287193
iteration 32, loss = 0.00728860218077898
iteration 33, loss = 0.007024681195616722
iteration 34, loss = 0.0068070655688643456
iteration 35, loss = 0.006688290741294622
iteration 36, loss = 0.009222310036420822
iteration 37, loss = 0.0064771645702421665
iteration 38, loss = 0.008351129479706287
iteration 39, loss = 0.006974042393267155
iteration 40, loss = 0.007349859923124313
iteration 41, loss = 0.00802621804177761
iteration 42, loss = 0.006736575625836849
iteration 43, loss = 0.007675168104469776
iteration 44, loss = 0.005943899508565664
iteration 45, loss = 0.007504309993237257
iteration 46, loss = 0.0087840361520648
iteration 47, loss = 0.006917104125022888
iteration 48, loss = 0.007726139854639769
iteration 49, loss = 0.00654288474470377
iteration 50, loss = 0.008169922977685928
iteration 51, loss = 0.00651900190860033
iteration 52, loss = 0.006959191523492336
iteration 53, loss = 0.012219304218888283
iteration 54, loss = 0.010554785840213299
iteration 55, loss = 0.006470338441431522
iteration 56, loss = 0.019976463168859482
iteration 57, loss = 0.006796896457672119
iteration 58, loss = 0.01215383131057024
iteration 59, loss = 0.006481648422777653
iteration 60, loss = 0.006933223456144333
iteration 61, loss = 0.0066851964220404625
iteration 62, loss = 0.006978726014494896
iteration 63, loss = 0.007532123941928148
iteration 64, loss = 0.006977245677262545
iteration 65, loss = 0.00632764957845211
iteration 66, loss = 0.007483126595616341
iteration 67, loss = 0.008212616667151451
iteration 68, loss = 0.007734966464340687
iteration 69, loss = 0.007163344416767359
iteration 70, loss = 0.00958842970430851
iteration 71, loss = 0.007737650070339441
iteration 72, loss = 0.008498972281813622
iteration 73, loss = 0.00751123670488596
iteration 74, loss = 0.006841393653303385
iteration 75, loss = 0.007556768134236336
iteration 76, loss = 0.007416415959596634
iteration 77, loss = 0.008631814271211624
iteration 78, loss = 0.006774584297090769
iteration 79, loss = 0.006351753603667021
iteration 80, loss = 0.00642011733725667
iteration 81, loss = 0.006612146273255348
iteration 82, loss = 0.009397226385772228
iteration 83, loss = 0.008879875764250755
iteration 84, loss = 0.006450777407735586
iteration 85, loss = 0.009563264437019825
iteration 86, loss = 0.006526302546262741
iteration 87, loss = 0.0063710892572999
iteration 88, loss = 0.009740019217133522
iteration 89, loss = 0.009809021838009357
iteration 90, loss = 0.005716005340218544
iteration 91, loss = 0.005858807358890772
iteration 92, loss = 0.007505747489631176
iteration 93, loss = 0.006074148695915937
iteration 94, loss = 0.0060786297544837
iteration 95, loss = 0.006898903753608465
iteration 96, loss = 0.006209234241396189
iteration 97, loss = 0.006410094443708658
iteration 98, loss = 0.006998532451689243
iteration 99, loss = 0.009954375214874744
iteration 100, loss = 0.0060578603297472
iteration 101, loss = 0.006298535503447056
iteration 102, loss = 0.006824474781751633
iteration 103, loss = 0.007050863467156887
iteration 104, loss = 0.006240286864340305
iteration 105, loss = 0.012800817377865314
iteration 106, loss = 0.007710019126534462
iteration 107, loss = 0.006747717969119549
iteration 108, loss = 0.013383068144321442
iteration 109, loss = 0.006731144152581692
iteration 110, loss = 0.007205504458397627
iteration 111, loss = 0.006739895790815353
iteration 112, loss = 0.00897168181836605
iteration 113, loss = 0.006876757368445396
iteration 114, loss = 0.006808461155742407
iteration 115, loss = 0.005970936734229326
iteration 116, loss = 0.006400559097528458
iteration 117, loss = 0.008034483529627323
iteration 118, loss = 0.0067751468159258366
iteration 119, loss = 0.007888546213507652
iteration 120, loss = 0.008970552124083042
iteration 121, loss = 0.00718327471986413
iteration 122, loss = 0.007362030446529388
iteration 123, loss = 0.005949784070253372
iteration 124, loss = 0.006081118248403072
iteration 125, loss = 0.007986759766936302
iteration 126, loss = 0.0072284298948943615
iteration 127, loss = 0.006580645684152842
iteration 128, loss = 0.006269690580666065
iteration 129, loss = 0.0093209994956851
iteration 130, loss = 0.005356719717383385
iteration 131, loss = 0.006159819662570953
iteration 132, loss = 0.005966298747807741
iteration 133, loss = 0.006898003630340099
iteration 134, loss = 0.006315470673143864
iteration 135, loss = 0.006560764741152525
iteration 136, loss = 0.008016773499548435
iteration 137, loss = 0.008648144081234932
iteration 138, loss = 0.0060027106665074825
iteration 139, loss = 0.0067703453823924065
iteration 140, loss = 0.009431008249521255
iteration 141, loss = 0.006681797094643116
iteration 142, loss = 0.00653981976211071
iteration 143, loss = 0.008703630417585373
iteration 144, loss = 0.006027824245393276
iteration 145, loss = 0.006459364667534828
iteration 146, loss = 0.005845310632139444
iteration 147, loss = 0.007086697965860367
iteration 148, loss = 0.0076310476288199425
iteration 149, loss = 0.007001988124102354
iteration 150, loss = 0.006407366134226322
iteration 151, loss = 0.006434955168515444
iteration 152, loss = 0.006818362046033144
iteration 153, loss = 0.007490633055567741
iteration 154, loss = 0.005682830233126879
iteration 155, loss = 0.005881239660084248
iteration 156, loss = 0.00676986388862133
iteration 157, loss = 0.006040560081601143
iteration 158, loss = 0.009592382237315178
iteration 159, loss = 0.0072090718895196915
iteration 160, loss = 0.013340875506401062
iteration 161, loss = 0.006060732528567314
iteration 162, loss = 0.006102866027504206
iteration 163, loss = 0.006429931148886681
iteration 164, loss = 0.006643636152148247
iteration 165, loss = 0.006794699467718601
iteration 166, loss = 0.006219185888767242
iteration 167, loss = 0.005498264916241169
iteration 168, loss = 0.006472467444837093
iteration 169, loss = 0.005733426660299301
iteration 170, loss = 0.006856932304799557
iteration 171, loss = 0.007505075540393591
iteration 172, loss = 0.005761642940342426
iteration 173, loss = 0.0058250087313354015
iteration 174, loss = 0.006649391259998083
iteration 175, loss = 0.006196768954396248
iteration 176, loss = 0.006102594081312418
iteration 177, loss = 0.005845165811479092
iteration 178, loss = 0.006251797545701265
iteration 179, loss = 0.00832528155297041
iteration 180, loss = 0.007611904758960009
iteration 181, loss = 0.007206000853329897
iteration 182, loss = 0.008717983961105347
iteration 183, loss = 0.0061921896412968636
iteration 184, loss = 0.006854430306702852
iteration 185, loss = 0.006389855407178402
iteration 186, loss = 0.007918518036603928
iteration 187, loss = 0.006681624799966812
iteration 188, loss = 0.0054613384418189526
iteration 189, loss = 0.006290372461080551
iteration 190, loss = 0.0057295626029372215
iteration 191, loss = 0.005933364387601614
iteration 192, loss = 0.00717693567276001
iteration 193, loss = 0.00988567341119051
iteration 194, loss = 0.007412994746118784
iteration 195, loss = 0.0066467165015637875
iteration 196, loss = 0.006197035312652588
iteration 197, loss = 0.006929291412234306
iteration 198, loss = 0.00549744488671422
iteration 199, loss = 0.00655966904014349
iteration 200, loss = 0.011223118752241135
iteration 201, loss = 0.007449537981301546
iteration 202, loss = 0.007078288123011589
iteration 203, loss = 0.006761371623724699
iteration 204, loss = 0.007165133021771908
iteration 205, loss = 0.009237640537321568
iteration 206, loss = 0.009342173114418983
iteration 207, loss = 0.0073166838847100735
iteration 208, loss = 0.005946324672549963
iteration 209, loss = 0.00691808620467782
iteration 210, loss = 0.007711696904152632
iteration 211, loss = 0.0064821625128388405
iteration 212, loss = 0.007039965596050024
iteration 213, loss = 0.005774737801402807
iteration 214, loss = 0.00725826108828187
iteration 215, loss = 0.005540769547224045
iteration 216, loss = 0.008915393613278866
iteration 217, loss = 0.0074329255148768425
iteration 218, loss = 0.005671244114637375
iteration 219, loss = 0.012151987291872501
iteration 220, loss = 0.00607211422175169
iteration 221, loss = 0.008310760371387005
iteration 222, loss = 0.006753009743988514
iteration 223, loss = 0.007763138506561518
iteration 224, loss = 0.007846992462873459
iteration 225, loss = 0.0060706185176968575
iteration 226, loss = 0.005407243501394987
iteration 227, loss = 0.006922855041921139
iteration 228, loss = 0.006520556751638651
iteration 229, loss = 0.005864343140274286
iteration 230, loss = 0.007676875218749046
iteration 231, loss = 0.005692513193935156
iteration 232, loss = 0.007279118988662958
iteration 233, loss = 0.011016438715159893
iteration 234, loss = 0.005451511591672897
iteration 235, loss = 0.009505287744104862
iteration 236, loss = 0.010941560380160809
iteration 237, loss = 0.007275902200490236
iteration 238, loss = 0.005544555373489857
iteration 239, loss = 0.007808334194123745
iteration 240, loss = 0.006944498512893915
iteration 241, loss = 0.009446525946259499
iteration 242, loss = 0.007377678528428078
iteration 243, loss = 0.005790332797914743
iteration 244, loss = 0.006306138820946217
iteration 245, loss = 0.00570409931242466
iteration 246, loss = 0.006002271082252264
iteration 247, loss = 0.00583504606038332
iteration 248, loss = 0.005772129632532597
iteration 249, loss = 0.008000840432941914
iteration 250, loss = 0.006356727331876755
iteration 251, loss = 0.005771993659436703
iteration 252, loss = 0.00826830044388771
iteration 253, loss = 0.009879115968942642
iteration 254, loss = 0.01072990708053112
iteration 255, loss = 0.005800057668238878
iteration 256, loss = 0.0063058300875127316
iteration 257, loss = 0.005230209324508905
iteration 258, loss = 0.008198567666113377
iteration 259, loss = 0.008982318453490734
iteration 260, loss = 0.012178231030702591
iteration 261, loss = 0.0057646846398711205
iteration 262, loss = 0.007560442667454481
iteration 263, loss = 0.005490424111485481
iteration 264, loss = 0.008812012150883675
iteration 265, loss = 0.007191228214651346
iteration 266, loss = 0.006183423101902008
iteration 267, loss = 0.007204745896160603
iteration 268, loss = 0.005547959823161364
iteration 269, loss = 0.011956674046814442
iteration 270, loss = 0.006400681100785732
iteration 271, loss = 0.007265292573720217
iteration 272, loss = 0.0058720409870147705
iteration 273, loss = 0.006477874703705311
iteration 274, loss = 0.006094601936638355
iteration 275, loss = 0.007381124421954155
iteration 276, loss = 0.005700989160686731
iteration 277, loss = 0.006607534363865852
iteration 278, loss = 0.005747715476900339
iteration 279, loss = 0.0053411186672747135
iteration 280, loss = 0.00637791957706213
iteration 281, loss = 0.006294537335634232
iteration 282, loss = 0.00742869870737195
iteration 283, loss = 0.005354352295398712
iteration 284, loss = 0.0069584534503519535
iteration 285, loss = 0.00525244465097785
iteration 286, loss = 0.007370316423475742
iteration 287, loss = 0.00864424929022789
iteration 288, loss = 0.010281643830239773
iteration 289, loss = 0.007218103390187025
iteration 290, loss = 0.005352192558348179
iteration 291, loss = 0.005896702408790588
iteration 292, loss = 0.006196049507707357
iteration 293, loss = 0.006190833635628223
iteration 294, loss = 0.0055556753650307655
iteration 295, loss = 0.007680914364755154
iteration 296, loss = 0.007719771936535835
iteration 297, loss = 0.005359784234315157
iteration 298, loss = 0.006986860651522875
iteration 299, loss = 0.00809110514819622
iteration 300, loss = 0.005305279977619648
iteration 1, loss = 0.0051747034303843975
iteration 2, loss = 0.006811615079641342
iteration 3, loss = 0.005643644370138645
iteration 4, loss = 0.005010520573705435
iteration 5, loss = 0.011842862702906132
iteration 6, loss = 0.005333344917744398
iteration 7, loss = 0.0059757414273917675
iteration 8, loss = 0.005936441011726856
iteration 9, loss = 0.005723604001104832
iteration 10, loss = 0.005089967045933008
iteration 11, loss = 0.005073471460491419
iteration 12, loss = 0.005643737036734819
iteration 13, loss = 0.005957159213721752
iteration 14, loss = 0.00530029833316803
iteration 15, loss = 0.0056675272062420845
iteration 16, loss = 0.0073144095949828625
iteration 17, loss = 0.005387779325246811
iteration 18, loss = 0.0068545532412827015
iteration 19, loss = 0.009732736274600029
iteration 20, loss = 0.006342550273984671
iteration 21, loss = 0.005282186437398195
iteration 22, loss = 0.011254844255745411
iteration 23, loss = 0.006655315402895212
iteration 24, loss = 0.005544814746826887
iteration 25, loss = 0.007492756005376577
iteration 26, loss = 0.0058002532459795475
iteration 27, loss = 0.005839798133820295
iteration 28, loss = 0.005279723554849625
iteration 29, loss = 0.007379510439932346
iteration 30, loss = 0.0075638373382389545
iteration 31, loss = 0.0064529310911893845
iteration 32, loss = 0.005227387882769108
iteration 33, loss = 0.006741030141711235
iteration 34, loss = 0.010226145386695862
iteration 35, loss = 0.006775519344955683
iteration 36, loss = 0.005583351943641901
iteration 37, loss = 0.006254138890653849
iteration 38, loss = 0.006477480288594961
iteration 39, loss = 0.004892145749181509
iteration 40, loss = 0.005670502316206694
iteration 41, loss = 0.0051998174749314785
iteration 42, loss = 0.011129528284072876
iteration 43, loss = 0.0075312415137887
iteration 44, loss = 0.005966329015791416
iteration 45, loss = 0.00592610239982605
iteration 46, loss = 0.005852505099028349
iteration 47, loss = 0.005831936839967966
iteration 48, loss = 0.005664112977683544
iteration 49, loss = 0.005968076176941395
iteration 50, loss = 0.005315021611750126
iteration 51, loss = 0.005565613508224487
iteration 52, loss = 0.005242600571364164
iteration 53, loss = 0.005841180216521025
iteration 54, loss = 0.005260387901216745
iteration 55, loss = 0.0053863138891756535
iteration 56, loss = 0.005746989045292139
iteration 57, loss = 0.008310932666063309
iteration 58, loss = 0.005364684853702784
iteration 59, loss = 0.007477517239749432
iteration 60, loss = 0.005414176266640425
iteration 61, loss = 0.00808006152510643
iteration 62, loss = 0.00572572834789753
iteration 63, loss = 0.005688943900167942
iteration 64, loss = 0.006785671226680279
iteration 65, loss = 0.006077348720282316
iteration 66, loss = 0.006043612491339445
iteration 67, loss = 0.005709418561309576
iteration 68, loss = 0.005332415923476219
iteration 69, loss = 0.005683897994458675
iteration 70, loss = 0.006799638271331787
iteration 71, loss = 0.005123579408973455
iteration 72, loss = 0.007477489300072193
iteration 73, loss = 0.005486335139721632
iteration 74, loss = 0.006377531215548515
iteration 75, loss = 0.005670867394655943
iteration 76, loss = 0.005853275768458843
iteration 77, loss = 0.008465755730867386
iteration 78, loss = 0.005205674562603235
iteration 79, loss = 0.005983640905469656
iteration 80, loss = 0.005745396018028259
iteration 81, loss = 0.0058799805119633675
iteration 82, loss = 0.0068701026029884815
iteration 83, loss = 0.005300863180309534
iteration 84, loss = 0.007043043617159128
iteration 85, loss = 0.004883897490799427
iteration 86, loss = 0.005252151284366846
iteration 87, loss = 0.005199762061238289
iteration 88, loss = 0.005029184278100729
iteration 89, loss = 0.005538498051464558
iteration 90, loss = 0.005173718556761742
iteration 91, loss = 0.0053750015795230865
iteration 92, loss = 0.005279220174998045
iteration 93, loss = 0.0049708266742527485
iteration 94, loss = 0.005020165350288153
iteration 95, loss = 0.004533861763775349
iteration 96, loss = 0.005922405049204826
iteration 97, loss = 0.005907317623496056
iteration 98, loss = 0.005329377017915249
iteration 99, loss = 0.006310340017080307
iteration 100, loss = 0.005355499219149351
iteration 101, loss = 0.005044227931648493
iteration 102, loss = 0.005224261432886124
iteration 103, loss = 0.005279763601720333
iteration 104, loss = 0.004985366016626358
iteration 105, loss = 0.005028018727898598
iteration 106, loss = 0.006993701681494713
iteration 107, loss = 0.006827260367572308
iteration 108, loss = 0.006871962454169989
iteration 109, loss = 0.0048606558702886105
iteration 110, loss = 0.009014732204377651
iteration 111, loss = 0.00696191843599081
iteration 112, loss = 0.0060666343197226524
iteration 113, loss = 0.004830412101000547
iteration 114, loss = 0.007867051288485527
iteration 115, loss = 0.007023500744253397
iteration 116, loss = 0.0055459910072386265
iteration 117, loss = 0.005138348322361708
iteration 118, loss = 0.008244011551141739
iteration 119, loss = 0.004712106194347143
iteration 120, loss = 0.005215891636908054
iteration 121, loss = 0.008421093225479126
iteration 122, loss = 0.004936570301651955
iteration 123, loss = 0.006825763266533613
iteration 124, loss = 0.0055235098116099834
iteration 125, loss = 0.005199415609240532
iteration 126, loss = 0.008118908852338791
iteration 127, loss = 0.00642014667391777
iteration 128, loss = 0.005064737051725388
iteration 129, loss = 0.012018276378512383
iteration 130, loss = 0.005794594995677471
iteration 131, loss = 0.004753946792334318
iteration 132, loss = 0.009953470900654793
iteration 133, loss = 0.005518509540706873
iteration 134, loss = 0.0053662266582250595
iteration 135, loss = 0.007186094764620066
iteration 136, loss = 0.006085676606744528
iteration 137, loss = 0.004832019098103046
iteration 138, loss = 0.00605631060898304
iteration 139, loss = 0.0064729442819952965
iteration 140, loss = 0.00464777834713459
iteration 141, loss = 0.004984968341886997
iteration 142, loss = 0.004356227349489927
iteration 143, loss = 0.007528586778789759
iteration 144, loss = 0.004993453621864319
iteration 145, loss = 0.00591307645663619
iteration 146, loss = 0.008854326792061329
iteration 147, loss = 0.005057282280176878
iteration 148, loss = 0.005394890904426575
iteration 149, loss = 0.005454471800476313
iteration 150, loss = 0.0047501567751169205
iteration 151, loss = 0.005402965005487204
iteration 152, loss = 0.00574159761890769
iteration 153, loss = 0.008768723346292973
iteration 154, loss = 0.006986104883253574
iteration 155, loss = 0.009606768377125263
iteration 156, loss = 0.004655557218939066
iteration 157, loss = 0.005742351990193129
iteration 158, loss = 0.004748445935547352
iteration 159, loss = 0.006239261012524366
iteration 160, loss = 0.005462696310132742
iteration 161, loss = 0.005508071277290583
iteration 162, loss = 0.0046018450520932674
iteration 163, loss = 0.005357668735086918
iteration 164, loss = 0.008124801330268383
iteration 165, loss = 0.00463475426658988
iteration 166, loss = 0.005127638578414917
iteration 167, loss = 0.008111244067549706
iteration 168, loss = 0.005692316219210625
iteration 169, loss = 0.004827984608709812
iteration 170, loss = 0.005800503771752119
iteration 171, loss = 0.005143263377249241
iteration 172, loss = 0.010105345398187637
iteration 173, loss = 0.009874459356069565
iteration 174, loss = 0.00613004295155406
iteration 175, loss = 0.00595194473862648
iteration 176, loss = 0.006252884399145842
iteration 177, loss = 0.004379783291369677
iteration 178, loss = 0.007445528171956539
iteration 179, loss = 0.005660350900143385
iteration 180, loss = 0.006585980765521526
iteration 181, loss = 0.005826003849506378
iteration 182, loss = 0.006016443483531475
iteration 183, loss = 0.004224422853440046
iteration 184, loss = 0.006695294287055731
iteration 185, loss = 0.004454066511243582
iteration 186, loss = 0.004142026882618666
iteration 187, loss = 0.004100264050066471
iteration 188, loss = 0.008013531565666199
iteration 189, loss = 0.006777721922844648
iteration 190, loss = 0.005229652393609285
iteration 191, loss = 0.006031940691173077
iteration 192, loss = 0.00554809533059597
iteration 193, loss = 0.00955691933631897
iteration 194, loss = 0.004737289622426033
iteration 195, loss = 0.004653175361454487
iteration 196, loss = 0.004544422030448914
iteration 197, loss = 0.004536239430308342
iteration 198, loss = 0.004487320780754089
iteration 199, loss = 0.004357124678790569
iteration 200, loss = 0.004882222507148981
iteration 201, loss = 0.004563904833048582
iteration 202, loss = 0.004268261604011059
iteration 203, loss = 0.0051436349749565125
iteration 204, loss = 0.00490151671692729
iteration 205, loss = 0.005135234445333481
iteration 206, loss = 0.005203735549002886
iteration 207, loss = 0.00440826965495944
iteration 208, loss = 0.0051901210099458694
iteration 209, loss = 0.005643235985189676
iteration 210, loss = 0.004558552987873554
iteration 211, loss = 0.0054726689122617245
iteration 212, loss = 0.005759740713983774
iteration 213, loss = 0.0063497330993413925
iteration 214, loss = 0.006736154668033123
iteration 215, loss = 0.005545094143599272
iteration 216, loss = 0.006197938695549965
iteration 217, loss = 0.009287807159125805
iteration 218, loss = 0.0049565620720386505
iteration 219, loss = 0.0071670617908239365
iteration 220, loss = 0.004738617688417435
iteration 221, loss = 0.005121890921145678
iteration 222, loss = 0.005122270435094833
iteration 223, loss = 0.005653319880366325
iteration 224, loss = 0.004960017278790474
iteration 225, loss = 0.005591640714555979
iteration 226, loss = 0.006745556369423866
iteration 227, loss = 0.0060887946747243404
iteration 228, loss = 0.004714662674814463
iteration 229, loss = 0.004948888905346394
iteration 230, loss = 0.005166522692888975
iteration 231, loss = 0.00483207730576396
iteration 232, loss = 0.00430587911978364
iteration 233, loss = 0.0058299521915614605
iteration 234, loss = 0.005771061405539513
iteration 235, loss = 0.004965181462466717
iteration 236, loss = 0.004859135486185551
iteration 237, loss = 0.005222024396061897
iteration 238, loss = 0.0052039301954209805
iteration 239, loss = 0.005385349504649639
iteration 240, loss = 0.008949199691414833
iteration 241, loss = 0.004226628690958023
iteration 242, loss = 0.004512098617851734
iteration 243, loss = 0.005028769839555025
iteration 244, loss = 0.005381183233112097
iteration 245, loss = 0.005282004363834858
iteration 246, loss = 0.007625230588018894
iteration 247, loss = 0.007083165924996138
iteration 248, loss = 0.0049261716194450855
iteration 249, loss = 0.007778142113238573
iteration 250, loss = 0.007884189486503601
iteration 251, loss = 0.004776955582201481
iteration 252, loss = 0.005193331744521856
iteration 253, loss = 0.004919626750051975
iteration 254, loss = 0.006626056041568518
iteration 255, loss = 0.00491191865876317
iteration 256, loss = 0.005588998086750507
iteration 257, loss = 0.004676141310483217
iteration 258, loss = 0.0045725321397185326
iteration 259, loss = 0.005080496426671743
iteration 260, loss = 0.006339121609926224
iteration 261, loss = 0.005331500433385372
iteration 262, loss = 0.007123059593141079
iteration 263, loss = 0.006114686839282513
iteration 264, loss = 0.00945319700986147
iteration 265, loss = 0.009052610024809837
iteration 266, loss = 0.004955655429512262
iteration 267, loss = 0.0062397741712629795
iteration 268, loss = 0.010071036405861378
iteration 269, loss = 0.004305943381041288
iteration 270, loss = 0.0046878233551979065
iteration 271, loss = 0.007860874757170677
iteration 272, loss = 0.004403105936944485
iteration 273, loss = 0.004276324063539505
iteration 274, loss = 0.005380162503570318
iteration 275, loss = 0.004380559083074331
iteration 276, loss = 0.00573248416185379
iteration 277, loss = 0.009082639589905739
iteration 278, loss = 0.004785321652889252
iteration 279, loss = 0.006313548423349857
iteration 280, loss = 0.005435662344098091
iteration 281, loss = 0.005674881394952536
iteration 282, loss = 0.004253779072314501
iteration 283, loss = 0.005539970006793737
iteration 284, loss = 0.004772830288857222
iteration 285, loss = 0.005882238037884235
iteration 286, loss = 0.004771774634718895
iteration 287, loss = 0.005321155302226543
iteration 288, loss = 0.006097794510424137
iteration 289, loss = 0.005608435720205307
iteration 290, loss = 0.005113403312861919
iteration 291, loss = 0.005566106177866459
iteration 292, loss = 0.00715302862226963
iteration 293, loss = 0.004611851181834936
iteration 294, loss = 0.004863063804805279
iteration 295, loss = 0.004255003295838833
iteration 296, loss = 0.004794763401150703
iteration 297, loss = 0.005982400383800268
iteration 298, loss = 0.00461643747985363
iteration 299, loss = 0.005541516002267599
iteration 300, loss = 0.004824573639780283
iteration 1, loss = 0.004971623886376619
iteration 2, loss = 0.005919536575675011
iteration 3, loss = 0.004888447932898998
iteration 4, loss = 0.004144058097153902
iteration 5, loss = 0.004366278648376465
iteration 6, loss = 0.004593033343553543
iteration 7, loss = 0.004016379360109568
iteration 8, loss = 0.008887247182428837
iteration 9, loss = 0.008830605074763298
iteration 10, loss = 0.0059433006681501865
iteration 11, loss = 0.004391304217278957
iteration 12, loss = 0.009909272193908691
iteration 13, loss = 0.0052866446785628796
iteration 14, loss = 0.005725763272494078
iteration 15, loss = 0.0049894931726157665
iteration 16, loss = 0.005321570672094822
iteration 17, loss = 0.004775931593030691
iteration 18, loss = 0.004548228345811367
iteration 19, loss = 0.0048393369652330875
iteration 20, loss = 0.008935228921473026
iteration 21, loss = 0.005858268588781357
iteration 22, loss = 0.004508079960942268
iteration 23, loss = 0.004867876414209604
iteration 24, loss = 0.0045934258960187435
iteration 25, loss = 0.004740585573017597
iteration 26, loss = 0.0050374665297567844
iteration 27, loss = 0.004154230933636427
iteration 28, loss = 0.0047896141186356544
iteration 29, loss = 0.004839392378926277
iteration 30, loss = 0.006512550171464682
iteration 31, loss = 0.00410081772133708
iteration 32, loss = 0.0050230976194143295
iteration 33, loss = 0.005815089680254459
iteration 34, loss = 0.004027025308459997
iteration 35, loss = 0.0048319874331355095
iteration 36, loss = 0.0074426038190722466
iteration 37, loss = 0.0041578360833227634
iteration 38, loss = 0.004800987895578146
iteration 39, loss = 0.007777975872159004
iteration 40, loss = 0.004882418550550938
iteration 41, loss = 0.004809921141713858
iteration 42, loss = 0.005025855731219053
iteration 43, loss = 0.00814791303128004
iteration 44, loss = 0.006327691953629255
iteration 45, loss = 0.004620066843926907
iteration 46, loss = 0.007283712737262249
iteration 47, loss = 0.004176886286586523
iteration 48, loss = 0.005342467688024044
iteration 49, loss = 0.004735372960567474
iteration 50, loss = 0.004686108324676752
iteration 51, loss = 0.004963358398526907
iteration 52, loss = 0.0037240078672766685
iteration 53, loss = 0.0042911432683467865
iteration 54, loss = 0.00518925441429019
iteration 55, loss = 0.005067921709269285
iteration 56, loss = 0.0048286025412380695
iteration 57, loss = 0.005317361559718847
iteration 58, loss = 0.005563223734498024
iteration 59, loss = 0.004508453421294689
iteration 60, loss = 0.004568897187709808
iteration 61, loss = 0.004007573239505291
iteration 62, loss = 0.004816096741706133
iteration 63, loss = 0.004398268647491932
iteration 64, loss = 0.003732605604454875
iteration 65, loss = 0.004583381582051516
iteration 66, loss = 0.004675719887018204
iteration 67, loss = 0.004406820051372051
iteration 68, loss = 0.005544587504118681
iteration 69, loss = 0.0088805528357625
iteration 70, loss = 0.0041067348793148994
iteration 71, loss = 0.0053820060566067696
iteration 72, loss = 0.004075600765645504
iteration 73, loss = 0.005681972485035658
iteration 74, loss = 0.005561212543398142
iteration 75, loss = 0.007874076254665852
iteration 76, loss = 0.004283279180526733
iteration 77, loss = 0.00746837118640542
iteration 78, loss = 0.005049212370067835
iteration 79, loss = 0.005799379665404558
iteration 80, loss = 0.004895779304206371
iteration 81, loss = 0.00567674869671464
iteration 82, loss = 0.004343464504927397
iteration 83, loss = 0.005413015838712454
iteration 84, loss = 0.004655989818274975
iteration 85, loss = 0.004780653398483992
iteration 86, loss = 0.004583392292261124
iteration 87, loss = 0.004866059869527817
iteration 88, loss = 0.005074396263808012
iteration 89, loss = 0.005003240890800953
iteration 90, loss = 0.007173935417085886
iteration 91, loss = 0.004170706961303949
iteration 92, loss = 0.004398853052407503
iteration 93, loss = 0.0047034285962581635
iteration 94, loss = 0.00486617861315608
iteration 95, loss = 0.005104433745145798
iteration 96, loss = 0.0048925913870334625
iteration 97, loss = 0.005453632678836584
iteration 98, loss = 0.005777417682111263
iteration 99, loss = 0.004629839211702347
iteration 100, loss = 0.00464263092726469
iteration 101, loss = 0.005186370108276606
iteration 102, loss = 0.006069731432944536
iteration 103, loss = 0.004873012192547321
iteration 104, loss = 0.0039668818935751915
iteration 105, loss = 0.004440066870301962
iteration 106, loss = 0.006256812252104282
iteration 107, loss = 0.00405462272465229
iteration 108, loss = 0.006196013651788235
iteration 109, loss = 0.004170558415353298
iteration 110, loss = 0.004549130797386169
iteration 111, loss = 0.004254740662872791
iteration 112, loss = 0.00574060995131731
iteration 113, loss = 0.005817123223096132
iteration 114, loss = 0.0053853197023272514
iteration 115, loss = 0.005289985798299313
iteration 116, loss = 0.0047120894305408
iteration 117, loss = 0.006972755305469036
iteration 118, loss = 0.004988612607121468
iteration 119, loss = 0.004180043004453182
iteration 120, loss = 0.005062454845756292
iteration 121, loss = 0.004514714237302542
iteration 122, loss = 0.005860690493136644
iteration 123, loss = 0.004954692907631397
iteration 124, loss = 0.005120663437992334
iteration 125, loss = 0.004255184903740883
iteration 126, loss = 0.00584071408957243
iteration 127, loss = 0.0051614693365991116
iteration 128, loss = 0.0041063157841563225
iteration 129, loss = 0.005430308636277914
iteration 130, loss = 0.00844388548284769
iteration 131, loss = 0.006002397276461124
iteration 132, loss = 0.005076908506453037
iteration 133, loss = 0.0058376542292535305
iteration 134, loss = 0.005097114481031895
iteration 135, loss = 0.005979406181722879
iteration 136, loss = 0.006253719329833984
iteration 137, loss = 0.0054747071117162704
iteration 138, loss = 0.009007934480905533
iteration 139, loss = 0.004550648387521505
iteration 140, loss = 0.003917512018233538
iteration 141, loss = 0.004589778371155262
iteration 142, loss = 0.004998242482542992
iteration 143, loss = 0.003981338813900948
iteration 144, loss = 0.004899824969470501
iteration 145, loss = 0.0063651809468865395
iteration 146, loss = 0.0050122905522584915
iteration 147, loss = 0.004101751837879419
iteration 148, loss = 0.0054125674068927765
iteration 149, loss = 0.004170508123934269
iteration 150, loss = 0.004250099416822195
iteration 151, loss = 0.004176402930170298
iteration 152, loss = 0.009872673079371452
iteration 153, loss = 0.006161373574286699
iteration 154, loss = 0.004921515937894583
iteration 155, loss = 0.005098030436784029
iteration 156, loss = 0.004752920474857092
iteration 157, loss = 0.005376516841351986
iteration 158, loss = 0.003909895662218332
iteration 159, loss = 0.004916128236800432
iteration 160, loss = 0.004151725675910711
iteration 161, loss = 0.00458144024014473
iteration 162, loss = 0.004491741303354502
iteration 163, loss = 0.0048670959658920765
iteration 164, loss = 0.00468837283551693
iteration 165, loss = 0.0057809012942016125
iteration 166, loss = 0.007472342811524868
iteration 167, loss = 0.0064250933937728405
iteration 168, loss = 0.006896729581058025
iteration 169, loss = 0.004692160524427891
iteration 170, loss = 0.004541984759271145
iteration 171, loss = 0.005157263018190861
iteration 172, loss = 0.0046808719635009766
iteration 173, loss = 0.004479117225855589
iteration 174, loss = 0.004581066779792309
iteration 175, loss = 0.004105817526578903
iteration 176, loss = 0.004097661003470421
iteration 177, loss = 0.004108259920030832
iteration 178, loss = 0.008557209745049477
iteration 179, loss = 0.004236280918121338
iteration 180, loss = 0.006598606705665588
iteration 181, loss = 0.003987051080912352
iteration 182, loss = 0.00610819086432457
iteration 183, loss = 0.005758409388363361
iteration 184, loss = 0.005139427725225687
iteration 185, loss = 0.004237419459968805
iteration 186, loss = 0.005482096690684557
iteration 187, loss = 0.00433962931856513
iteration 188, loss = 0.009344620630145073
iteration 189, loss = 0.004683389328420162
iteration 190, loss = 0.004877789877355099
iteration 191, loss = 0.008757920935750008
iteration 192, loss = 0.004798894748091698
iteration 193, loss = 0.007485124282538891
iteration 194, loss = 0.00731266662478447
iteration 195, loss = 0.0042547183111310005
iteration 196, loss = 0.004032043740153313
iteration 197, loss = 0.004755967762321234
iteration 198, loss = 0.004777461290359497
iteration 199, loss = 0.004184501711279154
iteration 200, loss = 0.006034476682543755
iteration 201, loss = 0.004568576347082853
iteration 202, loss = 0.004897604696452618
iteration 203, loss = 0.004138381220400333
iteration 204, loss = 0.004711946938186884
iteration 205, loss = 0.005649578757584095
iteration 206, loss = 0.005110676400363445
iteration 207, loss = 0.004589378833770752
iteration 208, loss = 0.0064835865050554276
iteration 209, loss = 0.004614740144461393
iteration 210, loss = 0.00389809999614954
iteration 211, loss = 0.004152833484113216
iteration 212, loss = 0.0063401516526937485
iteration 213, loss = 0.003876341274008155
iteration 214, loss = 0.005936713889241219
iteration 215, loss = 0.006159347947686911
iteration 216, loss = 0.004274680744856596
iteration 217, loss = 0.0044823805801570415
iteration 218, loss = 0.003809079062193632
iteration 219, loss = 0.004409979097545147
iteration 220, loss = 0.006995451636612415
iteration 221, loss = 0.0048189531080424786
iteration 222, loss = 0.004496901761740446
iteration 223, loss = 0.00663366774097085
iteration 224, loss = 0.009547963738441467
iteration 225, loss = 0.005111703183501959
iteration 226, loss = 0.004587131552398205
iteration 227, loss = 0.004785528406500816
iteration 228, loss = 0.004240838345140219
iteration 229, loss = 0.0075416420586407185
iteration 230, loss = 0.005458883009850979
iteration 231, loss = 0.004383903928101063
iteration 232, loss = 0.004603207111358643
iteration 233, loss = 0.004338221624493599
iteration 234, loss = 0.004287780728191137
iteration 235, loss = 0.010907771065831184
iteration 236, loss = 0.0044090948067605495
iteration 237, loss = 0.005215632729232311
iteration 238, loss = 0.0058153774589300156
iteration 239, loss = 0.006120479665696621
iteration 240, loss = 0.0046556429006159306
iteration 241, loss = 0.004604246001690626
iteration 242, loss = 0.006148494780063629
iteration 243, loss = 0.004541756119579077
iteration 244, loss = 0.007513340562582016
iteration 245, loss = 0.004350750707089901
iteration 246, loss = 0.004787446931004524
iteration 247, loss = 0.0067528304643929005
iteration 248, loss = 0.005430141929537058
iteration 249, loss = 0.004163002595305443
iteration 250, loss = 0.004580747336149216
iteration 251, loss = 0.004162773489952087
iteration 252, loss = 0.007402794901281595
iteration 253, loss = 0.006259465590119362
iteration 254, loss = 0.005896294489502907
iteration 255, loss = 0.008512892760336399
iteration 256, loss = 0.006099823862314224
iteration 257, loss = 0.00670737586915493
iteration 258, loss = 0.005041344556957483
iteration 259, loss = 0.005013505928218365
iteration 260, loss = 0.0062435083091259
iteration 261, loss = 0.004094563890248537
iteration 262, loss = 0.008952775038778782
iteration 263, loss = 0.004313989542424679
iteration 264, loss = 0.004308542236685753
iteration 265, loss = 0.005240296479314566
iteration 266, loss = 0.004413708113133907
iteration 267, loss = 0.004378148354589939
iteration 268, loss = 0.007368831895291805
iteration 269, loss = 0.005662139039486647
iteration 270, loss = 0.004996195435523987
iteration 271, loss = 0.008895514532923698
iteration 272, loss = 0.004560657776892185
iteration 273, loss = 0.004534027073532343
iteration 274, loss = 0.0038247136399149895
iteration 275, loss = 0.00630522146821022
iteration 276, loss = 0.004212104715406895
iteration 277, loss = 0.004290788434445858
iteration 278, loss = 0.004948307294398546
iteration 279, loss = 0.004162485711276531
iteration 280, loss = 0.004175598733127117
iteration 281, loss = 0.004338484723120928
iteration 282, loss = 0.006269083824008703
iteration 283, loss = 0.004571358673274517
iteration 284, loss = 0.004778040573000908
iteration 285, loss = 0.006935190875083208
iteration 286, loss = 0.0045585623010993
iteration 287, loss = 0.006813691928982735
iteration 288, loss = 0.006519824266433716
iteration 289, loss = 0.004248506389558315
iteration 290, loss = 0.008676905184984207
iteration 291, loss = 0.004607697948813438
iteration 292, loss = 0.004447293002158403
iteration 293, loss = 0.004046006128191948
iteration 294, loss = 0.009925222024321556
iteration 295, loss = 0.0065430887043476105
iteration 296, loss = 0.004316591192036867
iteration 297, loss = 0.004370576236397028
iteration 298, loss = 0.00451919948682189
iteration 299, loss = 0.004689674358814955
iteration 300, loss = 0.005166134797036648
iteration 1, loss = 0.009859615936875343
iteration 2, loss = 0.004540651571005583
iteration 3, loss = 0.004470499232411385
iteration 4, loss = 0.003901655087247491
iteration 5, loss = 0.0046101585030555725
iteration 6, loss = 0.004358778707683086
iteration 7, loss = 0.004744146019220352
iteration 8, loss = 0.006736744195222855
iteration 9, loss = 0.004714862909168005
iteration 10, loss = 0.005648871883749962
iteration 11, loss = 0.0047133429907262325
iteration 12, loss = 0.00661786925047636
iteration 13, loss = 0.004254858009517193
iteration 14, loss = 0.0039049158804118633
iteration 15, loss = 0.0056253354996442795
iteration 16, loss = 0.011099104769527912
iteration 17, loss = 0.006227076519280672
iteration 18, loss = 0.005563098471611738
iteration 19, loss = 0.004841904155910015
iteration 20, loss = 0.004538585897535086
iteration 21, loss = 0.005019009578973055
iteration 22, loss = 0.004659360274672508
iteration 23, loss = 0.005351637955754995
iteration 24, loss = 0.004571770783513784
iteration 25, loss = 0.005946342833340168
iteration 26, loss = 0.005740434862673283
iteration 27, loss = 0.005531849339604378
iteration 28, loss = 0.003993705380707979
iteration 29, loss = 0.004003175068646669
iteration 30, loss = 0.004628012888133526
iteration 31, loss = 0.004556822124868631
iteration 32, loss = 0.004216593690216541
iteration 33, loss = 0.004914559423923492
iteration 34, loss = 0.006819732487201691
iteration 35, loss = 0.004499533679336309
iteration 36, loss = 0.004290726501494646
iteration 37, loss = 0.009090854786336422
iteration 38, loss = 0.004792546853423119
iteration 39, loss = 0.004381515551358461
iteration 40, loss = 0.00790679082274437
iteration 41, loss = 0.00447443313896656
iteration 42, loss = 0.005954985041171312
iteration 43, loss = 0.0048007783479988575
iteration 44, loss = 0.006152505055069923
iteration 45, loss = 0.004848328419029713
iteration 46, loss = 0.004778459668159485
iteration 47, loss = 0.005673220846801996
iteration 48, loss = 0.005723374895751476
iteration 49, loss = 0.004505793564021587
iteration 50, loss = 0.009073272347450256
iteration 51, loss = 0.004545895382761955
iteration 52, loss = 0.004399836529046297
iteration 53, loss = 0.004488812759518623
iteration 54, loss = 0.004608790390193462
iteration 55, loss = 0.004648234229534864
iteration 56, loss = 0.004140337463468313
iteration 57, loss = 0.004247634205967188
iteration 58, loss = 0.004083111882209778
iteration 59, loss = 0.006098522339016199
iteration 60, loss = 0.004702668637037277
iteration 61, loss = 0.005168895237147808
iteration 62, loss = 0.0043865772895514965
iteration 63, loss = 0.0049894992262125015
iteration 64, loss = 0.004433339927345514
iteration 65, loss = 0.004174065310508013
iteration 66, loss = 0.004910020157694817
iteration 67, loss = 0.009609631262719631
iteration 68, loss = 0.004354766570031643
iteration 69, loss = 0.0066235242411494255
iteration 70, loss = 0.008013907819986343
iteration 71, loss = 0.004493136424571276
iteration 72, loss = 0.004747843369841576
iteration 73, loss = 0.005095322150737047
iteration 74, loss = 0.0050012897700071335
iteration 75, loss = 0.004718868061900139
iteration 76, loss = 0.004310730379074812
iteration 77, loss = 0.005290079861879349
iteration 78, loss = 0.00437066750600934
iteration 79, loss = 0.005230254493653774
iteration 80, loss = 0.005149610806256533
iteration 81, loss = 0.007070222403854132
iteration 82, loss = 0.004622950218617916
iteration 83, loss = 0.004743451252579689
iteration 84, loss = 0.004340972285717726
iteration 85, loss = 0.008723732084035873
iteration 86, loss = 0.00609977962449193
iteration 87, loss = 0.004256313666701317
iteration 88, loss = 0.004296290222555399
iteration 89, loss = 0.00433137733489275
iteration 90, loss = 0.00511925620958209
iteration 91, loss = 0.004441393539309502
iteration 92, loss = 0.004659832455217838
iteration 93, loss = 0.00555548258125782
iteration 94, loss = 0.004762954078614712
iteration 95, loss = 0.00403199577704072
iteration 96, loss = 0.004649433307349682
iteration 97, loss = 0.004025260452181101
iteration 98, loss = 0.005066277924925089
iteration 99, loss = 0.004093395080417395
iteration 100, loss = 0.004128867294639349
iteration 101, loss = 0.00450395792722702
iteration 102, loss = 0.00506047485396266
iteration 103, loss = 0.004788482096046209
iteration 104, loss = 0.004582677502185106
iteration 105, loss = 0.0044012549333274364
iteration 106, loss = 0.0054404702968895435
iteration 107, loss = 0.00475276680663228
iteration 108, loss = 0.00664709834381938
iteration 109, loss = 0.0045985565520823
iteration 110, loss = 0.007189528085291386
iteration 111, loss = 0.006048475857824087
iteration 112, loss = 0.004434430506080389
iteration 113, loss = 0.004814273677766323
iteration 114, loss = 0.006826435215771198
iteration 115, loss = 0.004983778111636639
iteration 116, loss = 0.004344030283391476
iteration 117, loss = 0.005543175619095564
iteration 118, loss = 0.004274013917893171
iteration 119, loss = 0.004867814015597105
iteration 120, loss = 0.004681724589318037
iteration 121, loss = 0.004592392593622208
iteration 122, loss = 0.004761229269206524
iteration 123, loss = 0.004606467206031084
iteration 124, loss = 0.010348831303417683
iteration 125, loss = 0.006415023468434811
iteration 126, loss = 0.004288868512958288
iteration 127, loss = 0.004514223895967007
iteration 128, loss = 0.0054007964208722115
iteration 129, loss = 0.006702076643705368
iteration 130, loss = 0.005818330682814121
iteration 131, loss = 0.005104964133352041
iteration 132, loss = 0.004315078724175692
iteration 133, loss = 0.006677189841866493
iteration 134, loss = 0.0049153901636600494
iteration 135, loss = 0.0042793527245521545
iteration 136, loss = 0.005522623658180237
iteration 137, loss = 0.004530064295977354
iteration 138, loss = 0.008566214703023434
iteration 139, loss = 0.004123027436435223
iteration 140, loss = 0.004296424333006144
iteration 141, loss = 0.004197321832180023
iteration 142, loss = 0.005614905618131161
iteration 143, loss = 0.004433913622051477
iteration 144, loss = 0.004560786299407482
iteration 145, loss = 0.004584214650094509
iteration 146, loss = 0.008119399659335613
iteration 147, loss = 0.005475813057273626
iteration 148, loss = 0.005060459487140179
iteration 149, loss = 0.005480545572936535
iteration 150, loss = 0.00978857558220625
iteration 151, loss = 0.005212515126913786
iteration 152, loss = 0.00433496804907918
iteration 153, loss = 0.005322461947798729
iteration 154, loss = 0.005318398587405682
iteration 155, loss = 0.004802885465323925
iteration 156, loss = 0.0046024806797504425
iteration 157, loss = 0.005847959313541651
iteration 158, loss = 0.00458805076777935
iteration 159, loss = 0.004896355792880058
iteration 160, loss = 0.004114865325391293
iteration 161, loss = 0.006040533073246479
iteration 162, loss = 0.008546327240765095
iteration 163, loss = 0.00809240248054266
iteration 164, loss = 0.004639557562768459
iteration 165, loss = 0.0037497016601264477
iteration 166, loss = 0.005175851751118898
iteration 167, loss = 0.004015836399048567
iteration 168, loss = 0.008621261455118656
iteration 169, loss = 0.004508067388087511
iteration 170, loss = 0.0037927855737507343
iteration 171, loss = 0.00484441639855504
iteration 172, loss = 0.005356305278837681
iteration 173, loss = 0.00582869304344058
iteration 174, loss = 0.006239074282348156
iteration 175, loss = 0.004827745724469423
iteration 176, loss = 0.004125924315303564
iteration 177, loss = 0.005104642361402512
iteration 178, loss = 0.004850533325225115
iteration 179, loss = 0.0043260157108306885
iteration 180, loss = 0.004161239601671696
iteration 181, loss = 0.0044714175164699554
iteration 182, loss = 0.004038617480546236
iteration 183, loss = 0.009883281774818897
iteration 184, loss = 0.0037348263431340456
iteration 185, loss = 0.004590434953570366
iteration 186, loss = 0.004994614515453577
iteration 187, loss = 0.004931081086397171
iteration 188, loss = 0.00593763031065464
iteration 189, loss = 0.005241634324193001
iteration 190, loss = 0.009148132987320423
iteration 191, loss = 0.00407140189781785
iteration 192, loss = 0.004085144959390163
iteration 193, loss = 0.004912938456982374
iteration 194, loss = 0.0049479748122394085
iteration 195, loss = 0.00721324235200882
iteration 196, loss = 0.0037935511209070683
iteration 197, loss = 0.004807640332728624
iteration 198, loss = 0.005465028807520866
iteration 199, loss = 0.004707964602857828
iteration 200, loss = 0.004882661625742912
iteration 201, loss = 0.0040410165674984455
iteration 202, loss = 0.004097893834114075
iteration 203, loss = 0.004807275719940662
iteration 204, loss = 0.006127795670181513
iteration 205, loss = 0.007073109038174152
iteration 206, loss = 0.005687212571501732
iteration 207, loss = 0.004570333752781153
iteration 208, loss = 0.00530468812212348
iteration 209, loss = 0.0037809915374964476
iteration 210, loss = 0.0050577931106090546
iteration 211, loss = 0.004568107426166534
iteration 212, loss = 0.006823220290243626
iteration 213, loss = 0.004640880506485701
iteration 214, loss = 0.004769155289977789
iteration 215, loss = 0.0057540410198271275
iteration 216, loss = 0.005784702952951193
iteration 217, loss = 0.004224990028887987
iteration 218, loss = 0.006706841289997101
iteration 219, loss = 0.004348380025476217
iteration 220, loss = 0.006208200938999653
iteration 221, loss = 0.009346384555101395
iteration 222, loss = 0.0047513884492218494
iteration 223, loss = 0.0041812025010585785
iteration 224, loss = 0.00471338490024209
iteration 225, loss = 0.0038892184384167194
iteration 226, loss = 0.005763148423284292
iteration 227, loss = 0.004072339739650488
iteration 228, loss = 0.006021804641932249
iteration 229, loss = 0.0040651001036167145
iteration 230, loss = 0.005353771615773439
iteration 231, loss = 0.006370349321514368
iteration 232, loss = 0.004594030790030956
iteration 233, loss = 0.007051446475088596
iteration 234, loss = 0.004577335435897112
iteration 235, loss = 0.004603386856615543
iteration 236, loss = 0.004104577470570803
iteration 237, loss = 0.005532409530133009
iteration 238, loss = 0.0043417480774223804
iteration 239, loss = 0.0037266449071466923
iteration 240, loss = 0.004106724634766579
iteration 241, loss = 0.005549481604248285
iteration 242, loss = 0.004299679771065712
iteration 243, loss = 0.004931231494992971
iteration 244, loss = 0.005803082138299942
iteration 245, loss = 0.0048327879048883915
iteration 246, loss = 0.004912400618195534
iteration 247, loss = 0.004352459218353033
iteration 248, loss = 0.003960868809372187
iteration 249, loss = 0.005448589101433754
iteration 250, loss = 0.004073554649949074
iteration 251, loss = 0.005789594259113073
iteration 252, loss = 0.005379894282668829
iteration 253, loss = 0.004654590971767902
iteration 254, loss = 0.004149031825363636
iteration 255, loss = 0.004469740204513073
iteration 256, loss = 0.004150854889303446
iteration 257, loss = 0.0066133104264736176
iteration 258, loss = 0.0056981490924954414
iteration 259, loss = 0.004514271393418312
iteration 260, loss = 0.00438310531899333
iteration 261, loss = 0.004072106443345547
iteration 262, loss = 0.0063865529373288155
iteration 263, loss = 0.005186697002500296
iteration 264, loss = 0.004766426049172878
iteration 265, loss = 0.004414491355419159
iteration 266, loss = 0.004861976020038128
iteration 267, loss = 0.00405907304957509
iteration 268, loss = 0.004525729455053806
iteration 269, loss = 0.004416915122419596
iteration 270, loss = 0.004711727611720562
iteration 271, loss = 0.0050243837758898735
iteration 272, loss = 0.007235100492835045
iteration 273, loss = 0.008770649321377277
iteration 274, loss = 0.005719760432839394
iteration 275, loss = 0.00624103145673871
iteration 276, loss = 0.005985687952488661
iteration 277, loss = 0.004417894408106804
iteration 278, loss = 0.004690601024776697
iteration 279, loss = 0.005023064091801643
iteration 280, loss = 0.004259523935616016
iteration 281, loss = 0.006568565033376217
iteration 282, loss = 0.006087686400860548
iteration 283, loss = 0.006448265165090561
iteration 284, loss = 0.005752266384661198
iteration 285, loss = 0.0043448577634990215
iteration 286, loss = 0.004341155756264925
iteration 287, loss = 0.0067986492067575455
iteration 288, loss = 0.003871947294101119
iteration 289, loss = 0.005900258664041758
iteration 290, loss = 0.004420435056090355
iteration 291, loss = 0.004288127180188894
iteration 292, loss = 0.004982719197869301
iteration 293, loss = 0.00470840185880661
iteration 294, loss = 0.004062141291797161
iteration 295, loss = 0.004867232870310545
iteration 296, loss = 0.0059400200843811035
iteration 297, loss = 0.007629195228219032
iteration 298, loss = 0.00445725629106164
iteration 299, loss = 0.004386686719954014
iteration 300, loss = 0.004056584555655718
iteration 1, loss = 0.004470363259315491
iteration 2, loss = 0.00922956969588995
iteration 3, loss = 0.003869112581014633
iteration 4, loss = 0.008568373508751392
iteration 5, loss = 0.00400901772081852
iteration 6, loss = 0.00480185030028224
iteration 7, loss = 0.004696895834058523
iteration 8, loss = 0.005580345634371042
iteration 9, loss = 0.004434834234416485
iteration 10, loss = 0.004162481054663658
iteration 11, loss = 0.004411398433148861
iteration 12, loss = 0.0047387308441102505
iteration 13, loss = 0.0046283346600830555
iteration 14, loss = 0.004419002681970596
iteration 15, loss = 0.005399878602474928
iteration 16, loss = 0.004132730420678854
iteration 17, loss = 0.004323089495301247
iteration 18, loss = 0.008238609880208969
iteration 19, loss = 0.004044033121317625
iteration 20, loss = 0.005431918427348137
iteration 21, loss = 0.004859706852585077
iteration 22, loss = 0.005849008448421955
iteration 23, loss = 0.003566483501344919
iteration 24, loss = 0.004374737851321697
iteration 25, loss = 0.005474474281072617
iteration 26, loss = 0.003921014256775379
iteration 27, loss = 0.007582430727779865
iteration 28, loss = 0.006122084800153971
iteration 29, loss = 0.0045265983790159225
iteration 30, loss = 0.0044033643789589405
iteration 31, loss = 0.008476270362734795
iteration 32, loss = 0.007166262250393629
iteration 33, loss = 0.005396382883191109
iteration 34, loss = 0.00543650146573782
iteration 35, loss = 0.004425359889864922
iteration 36, loss = 0.0058219535276293755
iteration 37, loss = 0.004390821326524019
iteration 38, loss = 0.00656891381368041
iteration 39, loss = 0.00437550013884902
iteration 40, loss = 0.00566926458850503
iteration 41, loss = 0.0048874919302761555
iteration 42, loss = 0.004574816673994064
iteration 43, loss = 0.004039509687572718
iteration 44, loss = 0.004001008812338114
iteration 45, loss = 0.005870298948138952
iteration 46, loss = 0.005087503232061863
iteration 47, loss = 0.004500320181250572
iteration 48, loss = 0.0045462604612112045
iteration 49, loss = 0.004214185755699873
iteration 50, loss = 0.004526187665760517
iteration 51, loss = 0.00526360422372818
iteration 52, loss = 0.004578462336212397
iteration 53, loss = 0.006914079189300537
iteration 54, loss = 0.00513182207942009
iteration 55, loss = 0.004511518869549036
iteration 56, loss = 0.0068924305960536
iteration 57, loss = 0.0045858994126319885
iteration 58, loss = 0.009437010623514652
iteration 59, loss = 0.004451578017324209
iteration 60, loss = 0.005666639655828476
iteration 61, loss = 0.004140662029385567
iteration 62, loss = 0.004868705756962299
iteration 63, loss = 0.004467769525945187
iteration 64, loss = 0.007674563210457563
iteration 65, loss = 0.004072400275617838
iteration 66, loss = 0.00460367975756526
iteration 67, loss = 0.0038289884105324745
iteration 68, loss = 0.008171540684998035
iteration 69, loss = 0.004153726156800985
iteration 70, loss = 0.005748243536800146
iteration 71, loss = 0.0066073243506252766
iteration 72, loss = 0.004057123325765133
iteration 73, loss = 0.004241902381181717
iteration 74, loss = 0.004935426637530327
iteration 75, loss = 0.008034034632146358
iteration 76, loss = 0.004734168294817209
iteration 77, loss = 0.008802839554846287
iteration 78, loss = 0.004503069445490837
iteration 79, loss = 0.004235909320414066
iteration 80, loss = 0.005500808358192444
iteration 81, loss = 0.004120771307498217
iteration 82, loss = 0.004540379159152508
iteration 83, loss = 0.004889911040663719
iteration 84, loss = 0.004159286618232727
iteration 85, loss = 0.004407062195241451
iteration 86, loss = 0.004237629473209381
iteration 87, loss = 0.00820672046393156
iteration 88, loss = 0.005662149749696255
iteration 89, loss = 0.003950142301619053
iteration 90, loss = 0.003994840197265148
iteration 91, loss = 0.004617859143763781
iteration 92, loss = 0.004245897755026817
iteration 93, loss = 0.004603592678904533
iteration 94, loss = 0.008253263309597969
iteration 95, loss = 0.006060238927602768
iteration 96, loss = 0.00796600617468357
iteration 97, loss = 0.0037774022202938795
iteration 98, loss = 0.005380227696150541
iteration 99, loss = 0.004104603547602892
iteration 100, loss = 0.00626519788056612
iteration 101, loss = 0.004645140841603279
iteration 102, loss = 0.004331805743277073
iteration 103, loss = 0.004207248333841562
iteration 104, loss = 0.005514709744602442
iteration 105, loss = 0.004024420399218798
iteration 106, loss = 0.004392798990011215
iteration 107, loss = 0.004281090572476387
iteration 108, loss = 0.004076653625816107
iteration 109, loss = 0.007324776146560907
iteration 110, loss = 0.008313384838402271
iteration 111, loss = 0.004889805801212788
iteration 112, loss = 0.004295920021831989
iteration 113, loss = 0.005802481900900602
iteration 114, loss = 0.004331036005169153
iteration 115, loss = 0.0046485550701618195
iteration 116, loss = 0.005051560699939728
iteration 117, loss = 0.00912924949079752
iteration 118, loss = 0.003951418213546276
iteration 119, loss = 0.005898765288293362
iteration 120, loss = 0.004164113197475672
iteration 121, loss = 0.004188866354525089
iteration 122, loss = 0.0043039764277637005
iteration 123, loss = 0.00471854442730546
iteration 124, loss = 0.003835445735603571
iteration 125, loss = 0.010388728231191635
iteration 126, loss = 0.004675041884183884
iteration 127, loss = 0.004277224652469158
iteration 128, loss = 0.005973778199404478
iteration 129, loss = 0.004174728877842426
iteration 130, loss = 0.004286136012524366
iteration 131, loss = 0.004383856430649757
iteration 132, loss = 0.00432581314817071
iteration 133, loss = 0.004475675988942385
iteration 134, loss = 0.0042417412623763084
iteration 135, loss = 0.0049123698845505714
iteration 136, loss = 0.007316959090530872
iteration 137, loss = 0.004392615053802729
iteration 138, loss = 0.005443744361400604
iteration 139, loss = 0.006646665278822184
iteration 140, loss = 0.005838642828166485
iteration 141, loss = 0.00531249912455678
iteration 142, loss = 0.005475150886923075
iteration 143, loss = 0.0039581432938575745
iteration 144, loss = 0.005894939415156841
iteration 145, loss = 0.006437825970351696
iteration 146, loss = 0.004710317589342594
iteration 147, loss = 0.006865441333502531
iteration 148, loss = 0.004982165992259979
iteration 149, loss = 0.005342170130461454
iteration 150, loss = 0.004670138470828533
iteration 151, loss = 0.00447076465934515
iteration 152, loss = 0.004654766991734505
iteration 153, loss = 0.005279572680592537
iteration 154, loss = 0.004394222982227802
iteration 155, loss = 0.004345634486526251
iteration 156, loss = 0.008170421235263348
iteration 157, loss = 0.008635078556835651
iteration 158, loss = 0.007211495190858841
iteration 159, loss = 0.0054324837401509285
iteration 160, loss = 0.004603246692568064
iteration 161, loss = 0.006058197468519211
iteration 162, loss = 0.006342774722725153
iteration 163, loss = 0.00467488169670105
iteration 164, loss = 0.005476749036461115
iteration 165, loss = 0.004928727634251118
iteration 166, loss = 0.0038962354883551598
iteration 167, loss = 0.004205870442092419
iteration 168, loss = 0.00473678857088089
iteration 169, loss = 0.0066304258070886135
iteration 170, loss = 0.004613812081515789
iteration 171, loss = 0.004195725079625845
iteration 172, loss = 0.004330287221819162
iteration 173, loss = 0.004403214901685715
iteration 174, loss = 0.004786759614944458
iteration 175, loss = 0.0043968320824205875
iteration 176, loss = 0.004874570295214653
iteration 177, loss = 0.004731510765850544
iteration 178, loss = 0.004949390422552824
iteration 179, loss = 0.00503271771594882
iteration 180, loss = 0.008268152363598347
iteration 181, loss = 0.004685467574745417
iteration 182, loss = 0.0036736626643687487
iteration 183, loss = 0.004793766885995865
iteration 184, loss = 0.003692160826176405
iteration 185, loss = 0.004026668146252632
iteration 186, loss = 0.0058811078779399395
iteration 187, loss = 0.004376628901809454
iteration 188, loss = 0.009775280021131039
iteration 189, loss = 0.005476098973304033
iteration 190, loss = 0.004491312429308891
iteration 191, loss = 0.004102534614503384
iteration 192, loss = 0.004456311464309692
iteration 193, loss = 0.004653181415051222
iteration 194, loss = 0.005267166066914797
iteration 195, loss = 0.004632491152733564
iteration 196, loss = 0.006139207165688276
iteration 197, loss = 0.003991417121142149
iteration 198, loss = 0.0047759548760950565
iteration 199, loss = 0.00623080600053072
iteration 200, loss = 0.005169717129319906
iteration 201, loss = 0.004816713277250528
iteration 202, loss = 0.004191727377474308
iteration 203, loss = 0.004998910240828991
iteration 204, loss = 0.004081549122929573
iteration 205, loss = 0.004597130231559277
iteration 206, loss = 0.0042809960432350636
iteration 207, loss = 0.004270805045962334
iteration 208, loss = 0.00484369695186615
iteration 209, loss = 0.003974068909883499
iteration 210, loss = 0.005118271801620722
iteration 211, loss = 0.007805668283253908
iteration 212, loss = 0.01024376880377531
iteration 213, loss = 0.0045784274116158485
iteration 214, loss = 0.005519216880202293
iteration 215, loss = 0.004461316391825676
iteration 216, loss = 0.004795266315340996
iteration 217, loss = 0.005105416756123304
iteration 218, loss = 0.004534599836915731
iteration 219, loss = 0.004560943227261305
iteration 220, loss = 0.0044080172665417194
iteration 221, loss = 0.004588407929986715
iteration 222, loss = 0.004518074914813042
iteration 223, loss = 0.004481329582631588
iteration 224, loss = 0.006698458921164274
iteration 225, loss = 0.003994029015302658
iteration 226, loss = 0.004243734758347273
iteration 227, loss = 0.004521666094660759
iteration 228, loss = 0.003942616283893585
iteration 229, loss = 0.005765396635979414
iteration 230, loss = 0.004035059362649918
iteration 231, loss = 0.004242228344082832
iteration 232, loss = 0.0036185462959110737
iteration 233, loss = 0.004054151941090822
iteration 234, loss = 0.004882217850536108
iteration 235, loss = 0.004771308042109013
iteration 236, loss = 0.006579898297786713
iteration 237, loss = 0.004541237372905016
iteration 238, loss = 0.0051751346327364445
iteration 239, loss = 0.004934061784297228
iteration 240, loss = 0.003962020389735699
iteration 241, loss = 0.0060736932791769505
iteration 242, loss = 0.005018472205847502
iteration 243, loss = 0.004858420230448246
iteration 244, loss = 0.004282641224563122
iteration 245, loss = 0.004615154583007097
iteration 246, loss = 0.004419006872922182
iteration 247, loss = 0.0041865878738462925
iteration 248, loss = 0.005020692478865385
iteration 249, loss = 0.005134403705596924
iteration 250, loss = 0.007505801040679216
iteration 251, loss = 0.006631521973758936
iteration 252, loss = 0.0043097292073071
iteration 253, loss = 0.004174139350652695
iteration 254, loss = 0.003774520941078663
iteration 255, loss = 0.005497146863490343
iteration 256, loss = 0.004154781810939312
iteration 257, loss = 0.005501205567270517
iteration 258, loss = 0.003862372599542141
iteration 259, loss = 0.007883370853960514
iteration 260, loss = 0.0040438612923026085
iteration 261, loss = 0.005235543008893728
iteration 262, loss = 0.0037668554577976465
iteration 263, loss = 0.004152737557888031
iteration 264, loss = 0.004392644856125116
iteration 265, loss = 0.004477607551962137
iteration 266, loss = 0.004358927719295025
iteration 267, loss = 0.005806721281260252
iteration 268, loss = 0.004951963201165199
iteration 269, loss = 0.004559127613902092
iteration 270, loss = 0.005615808069705963
iteration 271, loss = 0.0077832285314798355
iteration 272, loss = 0.004874344915151596
iteration 273, loss = 0.00459704827517271
iteration 274, loss = 0.004604515619575977
iteration 275, loss = 0.0064573101699352264
iteration 276, loss = 0.003890494117513299
iteration 277, loss = 0.0038921814411878586
iteration 278, loss = 0.004788462072610855
iteration 279, loss = 0.005347627215087414
iteration 280, loss = 0.007682299241423607
iteration 281, loss = 0.0040050530806183815
iteration 282, loss = 0.004085804335772991
iteration 283, loss = 0.003925117664039135
iteration 284, loss = 0.0055910805240273476
iteration 285, loss = 0.006402464583516121
iteration 286, loss = 0.0034200167283415794
iteration 287, loss = 0.004465267527848482
iteration 288, loss = 0.004459889605641365
iteration 289, loss = 0.00469133211299777
iteration 290, loss = 0.004642828367650509
iteration 291, loss = 0.0042748646810650826
iteration 292, loss = 0.004382342100143433
iteration 293, loss = 0.00597783550620079
iteration 294, loss = 0.006007838994264603
iteration 295, loss = 0.004594366066157818
iteration 296, loss = 0.005011825822293758
iteration 297, loss = 0.005266139283776283
iteration 298, loss = 0.004846860654652119
iteration 299, loss = 0.004908655770123005
iteration 300, loss = 0.0038075700867921114
iteration 1, loss = 0.004037259612232447
iteration 2, loss = 0.009624558500945568
iteration 3, loss = 0.006248269695788622
iteration 4, loss = 0.00433070445433259
iteration 5, loss = 0.004224122501909733
iteration 6, loss = 0.003993447870016098
iteration 7, loss = 0.004512612707912922
iteration 8, loss = 0.007705620490014553
iteration 9, loss = 0.005004143808037043
iteration 10, loss = 0.004649435635656118
iteration 11, loss = 0.004626935347914696
iteration 12, loss = 0.0037872560787945986
iteration 13, loss = 0.0046534244902431965
iteration 14, loss = 0.004315641243010759
iteration 15, loss = 0.004002794157713652
iteration 16, loss = 0.005546955391764641
iteration 17, loss = 0.008661377243697643
iteration 18, loss = 0.008124953135848045
iteration 19, loss = 0.005798967555165291
iteration 20, loss = 0.004552287980914116
iteration 21, loss = 0.003915091510862112
iteration 22, loss = 0.004099472425878048
iteration 23, loss = 0.004606312140822411
iteration 24, loss = 0.0042967526242136955
iteration 25, loss = 0.005413149483501911
iteration 26, loss = 0.004235131666064262
iteration 27, loss = 0.0038403579965233803
iteration 28, loss = 0.003932812251150608
iteration 29, loss = 0.004276757128536701
iteration 30, loss = 0.0059197042137384415
iteration 31, loss = 0.004549957811832428
iteration 32, loss = 0.004424415994435549
iteration 33, loss = 0.003924243152141571
iteration 34, loss = 0.0037250074092298746
iteration 35, loss = 0.004217866808176041
iteration 36, loss = 0.007509326562285423
iteration 37, loss = 0.0042463322170078754
iteration 38, loss = 0.0051661147736012936
iteration 39, loss = 0.005779293831437826
iteration 40, loss = 0.004454918205738068
iteration 41, loss = 0.006515101529657841
iteration 42, loss = 0.004822596907615662
iteration 43, loss = 0.007854885421693325
iteration 44, loss = 0.004296738654375076
iteration 45, loss = 0.0049521708860993385
iteration 46, loss = 0.005633864551782608
iteration 47, loss = 0.004682873375713825
iteration 48, loss = 0.004162571392953396
iteration 49, loss = 0.004706165287643671
iteration 50, loss = 0.00483764847740531
iteration 51, loss = 0.0038011178839951754
iteration 52, loss = 0.004184001591056585
iteration 53, loss = 0.004827151075005531
iteration 54, loss = 0.004418880678713322
iteration 55, loss = 0.0042843385599553585
iteration 56, loss = 0.00411659199744463
iteration 57, loss = 0.0048368857242167
iteration 58, loss = 0.004909065552055836
iteration 59, loss = 0.0038862950168550014
iteration 60, loss = 0.004213422536849976
iteration 61, loss = 0.005137431435286999
iteration 62, loss = 0.004738478921353817
iteration 63, loss = 0.005456465296447277
iteration 64, loss = 0.005506558809429407
iteration 65, loss = 0.005827497690916061
iteration 66, loss = 0.004837700631469488
iteration 67, loss = 0.006471103988587856
iteration 68, loss = 0.007278230506926775
iteration 69, loss = 0.004086996894329786
iteration 70, loss = 0.004682643339037895
iteration 71, loss = 0.005534454248845577
iteration 72, loss = 0.006181105971336365
iteration 73, loss = 0.004661286249756813
iteration 74, loss = 0.004519549664109945
iteration 75, loss = 0.006489201914519072
iteration 76, loss = 0.005176667124032974
iteration 77, loss = 0.005343516357243061
iteration 78, loss = 0.003944295924156904
iteration 79, loss = 0.008002118207514286
iteration 80, loss = 0.005928699858486652
iteration 81, loss = 0.005079584661871195
iteration 82, loss = 0.004177991766482592
iteration 83, loss = 0.004760041367262602
iteration 84, loss = 0.004112881142646074
iteration 85, loss = 0.0038715496193617582
iteration 86, loss = 0.0039742570370435715
iteration 87, loss = 0.003852707101032138
iteration 88, loss = 0.003839670680463314
iteration 89, loss = 0.005004434380680323
iteration 90, loss = 0.0047087278217077255
iteration 91, loss = 0.0049630324356257915
iteration 92, loss = 0.008091423660516739
iteration 93, loss = 0.005541343707591295
iteration 94, loss = 0.008267417550086975
iteration 95, loss = 0.004730019718408585
iteration 96, loss = 0.008232667110860348
iteration 97, loss = 0.004458046052604914
iteration 98, loss = 0.007083101198077202
iteration 99, loss = 0.00910256989300251
iteration 100, loss = 0.004634653218090534
iteration 101, loss = 0.004376308526843786
iteration 102, loss = 0.004423770122230053
iteration 103, loss = 0.005347938276827335
iteration 104, loss = 0.00523741589859128
iteration 105, loss = 0.003901924006640911
iteration 106, loss = 0.004680571146309376
iteration 107, loss = 0.004340966232120991
iteration 108, loss = 0.0038043258246034384
iteration 109, loss = 0.005632869899272919
iteration 110, loss = 0.005599660333245993
iteration 111, loss = 0.004675718490034342
iteration 112, loss = 0.005069878417998552
iteration 113, loss = 0.0038804286159574986
iteration 114, loss = 0.004360251594334841
iteration 115, loss = 0.006027371622622013
iteration 116, loss = 0.00620512617751956
iteration 117, loss = 0.004417425952851772
iteration 118, loss = 0.0041618854738771915
iteration 119, loss = 0.006074638105928898
iteration 120, loss = 0.004057975020259619
iteration 121, loss = 0.0042306287214159966
iteration 122, loss = 0.004757859744131565
iteration 123, loss = 0.004032406955957413
iteration 124, loss = 0.005074455868452787
iteration 125, loss = 0.003911370877176523
iteration 126, loss = 0.009363683871924877
iteration 127, loss = 0.005726735107600689
iteration 128, loss = 0.005119781009852886
iteration 129, loss = 0.005046049132943153
iteration 130, loss = 0.004289260134100914
iteration 131, loss = 0.0063787298277020454
iteration 132, loss = 0.0044649941846728325
iteration 133, loss = 0.004249246791005135
iteration 134, loss = 0.005081582814455032
iteration 135, loss = 0.004601302556693554
iteration 136, loss = 0.004694520961493254
iteration 137, loss = 0.004276383202522993
iteration 138, loss = 0.005594015121459961
iteration 139, loss = 0.0069716814905405045
iteration 140, loss = 0.0054381852969527245
iteration 141, loss = 0.004249716177582741
iteration 142, loss = 0.004223325289785862
iteration 143, loss = 0.005551954731345177
iteration 144, loss = 0.004964398220181465
iteration 145, loss = 0.004605589900165796
iteration 146, loss = 0.006468361243605614
iteration 147, loss = 0.008631214499473572
iteration 148, loss = 0.004654937889426947
iteration 149, loss = 0.004671474918723106
iteration 150, loss = 0.008686348795890808
iteration 151, loss = 0.0047502415254712105
iteration 152, loss = 0.0041465796530246735
iteration 153, loss = 0.004261222667992115
iteration 154, loss = 0.0037861017044633627
iteration 155, loss = 0.0049928841181099415
iteration 156, loss = 0.005243527702987194
iteration 157, loss = 0.004907476250082254
iteration 158, loss = 0.004711192101240158
iteration 159, loss = 0.004448466002941132
iteration 160, loss = 0.004364728461951017
iteration 161, loss = 0.003910778556019068
iteration 162, loss = 0.004402635619044304
iteration 163, loss = 0.004140495788305998
iteration 164, loss = 0.004205480683594942
iteration 165, loss = 0.003950553014874458
iteration 166, loss = 0.007404215168207884
iteration 167, loss = 0.004876266699284315
iteration 168, loss = 0.004326810594648123
iteration 169, loss = 0.004191805608570576
iteration 170, loss = 0.0051004174165427685
iteration 171, loss = 0.004249940626323223
iteration 172, loss = 0.004481312353163958
iteration 173, loss = 0.006849952042102814
iteration 174, loss = 0.005482652224600315
iteration 175, loss = 0.007837587036192417
iteration 176, loss = 0.004719747230410576
iteration 177, loss = 0.004432085435837507
iteration 178, loss = 0.004711071029305458
iteration 179, loss = 0.004740356933325529
iteration 180, loss = 0.005362501367926598
iteration 181, loss = 0.0051041473634541035
iteration 182, loss = 0.004913505166769028
iteration 183, loss = 0.003900212701410055
iteration 184, loss = 0.0039163995534181595
iteration 185, loss = 0.0048683094792068005
iteration 186, loss = 0.004573920741677284
iteration 187, loss = 0.004755774512887001
iteration 188, loss = 0.005036751739680767
iteration 189, loss = 0.004335037898272276
iteration 190, loss = 0.005024368409067392
iteration 191, loss = 0.005990072153508663
iteration 192, loss = 0.0047084130346775055
iteration 193, loss = 0.004254521802067757
iteration 194, loss = 0.0058015151880681515
iteration 195, loss = 0.00461954902857542
iteration 196, loss = 0.0035413906443864107
iteration 197, loss = 0.005247253924608231
iteration 198, loss = 0.005639844108372927
iteration 199, loss = 0.0051261056214571
iteration 200, loss = 0.0062086754478514194
iteration 201, loss = 0.004142257384955883
iteration 202, loss = 0.00437111547216773
iteration 203, loss = 0.004813168663531542
iteration 204, loss = 0.004085680935531855
iteration 205, loss = 0.009324777871370316
iteration 206, loss = 0.003971970174461603
iteration 207, loss = 0.005890396423637867
iteration 208, loss = 0.0071863895282149315
iteration 209, loss = 0.0038283627945929766
iteration 210, loss = 0.004269620403647423
iteration 211, loss = 0.004271991550922394
iteration 212, loss = 0.005007049068808556
iteration 213, loss = 0.007705128286033869
iteration 214, loss = 0.005474681966006756
iteration 215, loss = 0.004167234059423208
iteration 216, loss = 0.004502567928284407
iteration 217, loss = 0.0042866249568760395
iteration 218, loss = 0.004162695724517107
iteration 219, loss = 0.007875381968915462
iteration 220, loss = 0.004587279632687569
iteration 221, loss = 0.0046731214970350266
iteration 222, loss = 0.005858760792762041
iteration 223, loss = 0.008463447913527489
iteration 224, loss = 0.008776338770985603
iteration 225, loss = 0.004378014709800482
iteration 226, loss = 0.006088379770517349
iteration 227, loss = 0.005532458424568176
iteration 228, loss = 0.005021261051297188
iteration 229, loss = 0.00421537971124053
iteration 230, loss = 0.004331485368311405
iteration 231, loss = 0.0054488517343997955
iteration 232, loss = 0.004586609546095133
iteration 233, loss = 0.004191645421087742
iteration 234, loss = 0.004445133730769157
iteration 235, loss = 0.004498917143791914
iteration 236, loss = 0.004001281224191189
iteration 237, loss = 0.006396561861038208
iteration 238, loss = 0.006432209629565477
iteration 239, loss = 0.004556639585644007
iteration 240, loss = 0.003820690792053938
iteration 241, loss = 0.005662102717906237
iteration 242, loss = 0.003610026091337204
iteration 243, loss = 0.004427771084010601
iteration 244, loss = 0.004262201953679323
iteration 245, loss = 0.008608877658843994
iteration 246, loss = 0.0052456334233284
iteration 247, loss = 0.004044966772198677
iteration 248, loss = 0.0040700375102460384
iteration 249, loss = 0.005243105348199606
iteration 250, loss = 0.004473779816180468
iteration 251, loss = 0.004945648834109306
iteration 252, loss = 0.0056626019068062305
iteration 253, loss = 0.004450774751603603
iteration 254, loss = 0.005737947300076485
iteration 255, loss = 0.003758253064006567
iteration 256, loss = 0.004294597543776035
iteration 257, loss = 0.006377972662448883
iteration 258, loss = 0.004688727203756571
iteration 259, loss = 0.0052184597589075565
iteration 260, loss = 0.00461113965138793
iteration 261, loss = 0.004776763264089823
iteration 262, loss = 0.009945385158061981
iteration 263, loss = 0.00406551081687212
iteration 264, loss = 0.005346324294805527
iteration 265, loss = 0.0051441313698887825
iteration 266, loss = 0.003985793329775333
iteration 267, loss = 0.004762195982038975
iteration 268, loss = 0.00421172147616744
iteration 269, loss = 0.004097445867955685
iteration 270, loss = 0.003984168637543917
iteration 271, loss = 0.003987820819020271
iteration 272, loss = 0.004281829111278057
iteration 273, loss = 0.004385572392493486
iteration 274, loss = 0.004308963194489479
iteration 275, loss = 0.003995613660663366
iteration 276, loss = 0.004721249919384718
iteration 277, loss = 0.005218660458922386
iteration 278, loss = 0.004104847554117441
iteration 279, loss = 0.005031121429055929
iteration 280, loss = 0.004267855081707239
iteration 281, loss = 0.0043055154383182526
iteration 282, loss = 0.005111591890454292
iteration 283, loss = 0.004689090419560671
iteration 284, loss = 0.004881431348621845
iteration 285, loss = 0.0045206607319414616
iteration 286, loss = 0.006693826988339424
iteration 287, loss = 0.0042595406994223595
iteration 288, loss = 0.004582089837640524
iteration 289, loss = 0.004381523933261633
iteration 290, loss = 0.00392704363912344
iteration 291, loss = 0.003982631489634514
iteration 292, loss = 0.004136532079428434
iteration 293, loss = 0.003911116160452366
iteration 294, loss = 0.004831952508538961
iteration 295, loss = 0.004245001822710037
iteration 296, loss = 0.005779442377388477
iteration 297, loss = 0.004595652222633362
iteration 298, loss = 0.0040156040340662
iteration 299, loss = 0.004308644216507673
iteration 300, loss = 0.004176972433924675
iteration 1, loss = 0.004358015023171902
iteration 2, loss = 0.006513584405183792
iteration 3, loss = 0.006574536208063364
iteration 4, loss = 0.004052098840475082
iteration 5, loss = 0.004198211710900068
iteration 6, loss = 0.004028890747576952
iteration 7, loss = 0.004389388021081686
iteration 8, loss = 0.006016974337399006
iteration 9, loss = 0.005547955632209778
iteration 10, loss = 0.0038370611146092415
iteration 11, loss = 0.004138885997235775
iteration 12, loss = 0.004082770552486181
iteration 13, loss = 0.003805080894380808
iteration 14, loss = 0.005964485928416252
iteration 15, loss = 0.005679023452103138
iteration 16, loss = 0.0053502521477639675
iteration 17, loss = 0.004624054301530123
iteration 18, loss = 0.004819025751203299
iteration 19, loss = 0.004347589798271656
iteration 20, loss = 0.005172242876142263
iteration 21, loss = 0.004357887897640467
iteration 22, loss = 0.005756692960858345
iteration 23, loss = 0.004629487171769142
iteration 24, loss = 0.003865667153149843
iteration 25, loss = 0.005487408023327589
iteration 26, loss = 0.008362323977053165
iteration 27, loss = 0.004139766097068787
iteration 28, loss = 0.004967059474438429
iteration 29, loss = 0.003985904622823
iteration 30, loss = 0.005449187476187944
iteration 31, loss = 0.004900029394775629
iteration 32, loss = 0.0059096477925777435
iteration 33, loss = 0.006499571725726128
iteration 34, loss = 0.007589814718812704
iteration 35, loss = 0.004027056507766247
iteration 36, loss = 0.006282744463533163
iteration 37, loss = 0.01027357205748558
iteration 38, loss = 0.00852748192846775
iteration 39, loss = 0.005420868285000324
iteration 40, loss = 0.003833186347037554
iteration 41, loss = 0.00859383586794138
iteration 42, loss = 0.004362285137176514
iteration 43, loss = 0.006401377730071545
iteration 44, loss = 0.0036555661354213953
iteration 45, loss = 0.005208788439631462
iteration 46, loss = 0.005881550256162882
iteration 47, loss = 0.004923946224153042
iteration 48, loss = 0.004489478655159473
iteration 49, loss = 0.008407563902437687
iteration 50, loss = 0.004198096692562103
iteration 51, loss = 0.005377568304538727
iteration 52, loss = 0.0037474939599633217
iteration 53, loss = 0.00356978434138
iteration 54, loss = 0.004871171899139881
iteration 55, loss = 0.004102231469005346
iteration 56, loss = 0.004747935105115175
iteration 57, loss = 0.0038878729101270437
iteration 58, loss = 0.00473350565880537
iteration 59, loss = 0.0042044417932629585
iteration 60, loss = 0.004338858183473349
iteration 61, loss = 0.004292555619031191
iteration 62, loss = 0.004572647158056498
iteration 63, loss = 0.005997699219733477
iteration 64, loss = 0.004915670491755009
iteration 65, loss = 0.005239265039563179
iteration 66, loss = 0.003971462603658438
iteration 67, loss = 0.005017089657485485
iteration 68, loss = 0.009067349135875702
iteration 69, loss = 0.005604925099760294
iteration 70, loss = 0.004111545626074076
iteration 71, loss = 0.004014093428850174
iteration 72, loss = 0.003982644993811846
iteration 73, loss = 0.0038761566393077374
iteration 74, loss = 0.006665530614554882
iteration 75, loss = 0.0042098392732441425
iteration 76, loss = 0.005068342667073011
iteration 77, loss = 0.0035089829470962286
iteration 78, loss = 0.005235899705439806
iteration 79, loss = 0.004256228916347027
iteration 80, loss = 0.003998264670372009
iteration 81, loss = 0.004053124226629734
iteration 82, loss = 0.007572461385279894
iteration 83, loss = 0.005861785262823105
iteration 84, loss = 0.004675247706472874
iteration 85, loss = 0.004303381312638521
iteration 86, loss = 0.004866842180490494
iteration 87, loss = 0.00396417872980237
iteration 88, loss = 0.005129327531903982
iteration 89, loss = 0.004582620225846767
iteration 90, loss = 0.0041529154404997826
iteration 91, loss = 0.0050580198876559734
iteration 92, loss = 0.004711917135864496
iteration 93, loss = 0.008145341649651527
iteration 94, loss = 0.008271605707705021
iteration 95, loss = 0.0038273928221315145
iteration 96, loss = 0.004607950337231159
iteration 97, loss = 0.004499467555433512
iteration 98, loss = 0.005128888878971338
iteration 99, loss = 0.0037421747110784054
iteration 100, loss = 0.004557793959975243
iteration 101, loss = 0.00450940104201436
iteration 102, loss = 0.004165059886872768
iteration 103, loss = 0.004572981037199497
iteration 104, loss = 0.0055124107748270035
iteration 105, loss = 0.004819490946829319
iteration 106, loss = 0.0044173295609653
iteration 107, loss = 0.003888043574988842
iteration 108, loss = 0.005805483087897301
iteration 109, loss = 0.008401518687605858
iteration 110, loss = 0.004026065114885569
iteration 111, loss = 0.0077651264145970345
iteration 112, loss = 0.004467563703656197
iteration 113, loss = 0.0052514271810650826
iteration 114, loss = 0.004583017434924841
iteration 115, loss = 0.004257401917129755
iteration 116, loss = 0.004376598168164492
iteration 117, loss = 0.004313573706895113
iteration 118, loss = 0.004283676855266094
iteration 119, loss = 0.0050104279071092606
iteration 120, loss = 0.004825307056307793
iteration 121, loss = 0.005438079126179218
iteration 122, loss = 0.003929321188479662
iteration 123, loss = 0.005360441282391548
iteration 124, loss = 0.004502059891819954
iteration 125, loss = 0.004201680887490511
iteration 126, loss = 0.0036370260640978813
iteration 127, loss = 0.004455256275832653
iteration 128, loss = 0.007851928472518921
iteration 129, loss = 0.005495951510965824
iteration 130, loss = 0.0043524159118533134
iteration 131, loss = 0.009077481925487518
iteration 132, loss = 0.0037161174695938826
iteration 133, loss = 0.003807087428867817
iteration 134, loss = 0.004751219879835844
iteration 135, loss = 0.0051321107894182205
iteration 136, loss = 0.0048728203400969505
iteration 137, loss = 0.004346827045083046
iteration 138, loss = 0.005683436989784241
iteration 139, loss = 0.004454180598258972
iteration 140, loss = 0.003979391418397427
iteration 141, loss = 0.0038953886833041906
iteration 142, loss = 0.003946114331483841
iteration 143, loss = 0.004158192779868841
iteration 144, loss = 0.0059208995662629604
iteration 145, loss = 0.00410689041018486
iteration 146, loss = 0.008359842002391815
iteration 147, loss = 0.005144891794770956
iteration 148, loss = 0.004839387722313404
iteration 149, loss = 0.005549808032810688
iteration 150, loss = 0.003916020505130291
iteration 151, loss = 0.003907868638634682
iteration 152, loss = 0.004230170045047998
iteration 153, loss = 0.004270636476576328
iteration 154, loss = 0.004569832235574722
iteration 155, loss = 0.003869112813845277
iteration 156, loss = 0.0042409831658005714
iteration 157, loss = 0.005603089928627014
iteration 158, loss = 0.008364609442651272
iteration 159, loss = 0.0042862845584750175
iteration 160, loss = 0.008237081579864025
iteration 161, loss = 0.009275668300688267
iteration 162, loss = 0.00477093830704689
iteration 163, loss = 0.004259767942130566
iteration 164, loss = 0.008152147755026817
iteration 165, loss = 0.00389327434822917
iteration 166, loss = 0.004135902971029282
iteration 167, loss = 0.004390190355479717
iteration 168, loss = 0.004197839181870222
iteration 169, loss = 0.006037087179720402
iteration 170, loss = 0.004224527161568403
iteration 171, loss = 0.004152804613113403
iteration 172, loss = 0.00719661358743906
iteration 173, loss = 0.005186631344258785
iteration 174, loss = 0.003484836546704173
iteration 175, loss = 0.004809454549103975
iteration 176, loss = 0.005462735891342163
iteration 177, loss = 0.004624808207154274
iteration 178, loss = 0.004222156945616007
iteration 179, loss = 0.0038100476376712322
iteration 180, loss = 0.00438669603317976
iteration 181, loss = 0.004526487551629543
iteration 182, loss = 0.00406258087605238
iteration 183, loss = 0.006599338259547949
iteration 184, loss = 0.004792611114680767
iteration 185, loss = 0.00476772291585803
iteration 186, loss = 0.006065215915441513
iteration 187, loss = 0.0046366192400455475
iteration 188, loss = 0.00492109777405858
iteration 189, loss = 0.004766818601638079
iteration 190, loss = 0.004374903626739979
iteration 191, loss = 0.004066714085638523
iteration 192, loss = 0.0036894383374601603
iteration 193, loss = 0.011179321445524693
iteration 194, loss = 0.004568152129650116
iteration 195, loss = 0.004410811699926853
iteration 196, loss = 0.003995837643742561
iteration 197, loss = 0.00453443918377161
iteration 198, loss = 0.0039869979955255985
iteration 199, loss = 0.00424518808722496
iteration 200, loss = 0.005061290226876736
iteration 201, loss = 0.004450715146958828
iteration 202, loss = 0.005428197793662548
iteration 203, loss = 0.004180800169706345
iteration 204, loss = 0.006807247176766396
iteration 205, loss = 0.004572183825075626
iteration 206, loss = 0.0037035332061350346
iteration 207, loss = 0.004144957289099693
iteration 208, loss = 0.004588455427438021
iteration 209, loss = 0.004592840559780598
iteration 210, loss = 0.004611206706613302
iteration 211, loss = 0.004184408579021692
iteration 212, loss = 0.004230656195431948
iteration 213, loss = 0.0037958237808197737
iteration 214, loss = 0.005816091317683458
iteration 215, loss = 0.004047745373100042
iteration 216, loss = 0.004122435115277767
iteration 217, loss = 0.004873686004430056
iteration 218, loss = 0.006394028663635254
iteration 219, loss = 0.004184111021459103
iteration 220, loss = 0.004000352695584297
iteration 221, loss = 0.005639682989567518
iteration 222, loss = 0.004933223128318787
iteration 223, loss = 0.004001200199127197
iteration 224, loss = 0.0041231922805309296
iteration 225, loss = 0.004167950712144375
iteration 226, loss = 0.0037765426095575094
iteration 227, loss = 0.005009680986404419
iteration 228, loss = 0.004814917221665382
iteration 229, loss = 0.005876757204532623
iteration 230, loss = 0.004200794734060764
iteration 231, loss = 0.005401216447353363
iteration 232, loss = 0.00640351977199316
iteration 233, loss = 0.0041424608789384365
iteration 234, loss = 0.00434935512021184
iteration 235, loss = 0.004259876906871796
iteration 236, loss = 0.005610501393675804
iteration 237, loss = 0.004351796582341194
iteration 238, loss = 0.004105600528419018
iteration 239, loss = 0.003951443824917078
iteration 240, loss = 0.004989880137145519
iteration 241, loss = 0.004144814796745777
iteration 242, loss = 0.006840355694293976
iteration 243, loss = 0.0037960605695843697
iteration 244, loss = 0.004035823047161102
iteration 245, loss = 0.00404950650408864
iteration 246, loss = 0.004154672380536795
iteration 247, loss = 0.0042839981615543365
iteration 248, loss = 0.003677428001537919
iteration 249, loss = 0.003994576167315245
iteration 250, loss = 0.005396263208240271
iteration 251, loss = 0.004420586861670017
iteration 252, loss = 0.004296781960874796
iteration 253, loss = 0.0067918384447693825
iteration 254, loss = 0.006697230972349644
iteration 255, loss = 0.004333188757300377
iteration 256, loss = 0.004309064242988825
iteration 257, loss = 0.004415507428348064
iteration 258, loss = 0.004214826039969921
iteration 259, loss = 0.006264369003474712
iteration 260, loss = 0.004935943055897951
iteration 261, loss = 0.003744270885363221
iteration 262, loss = 0.005344799719750881
iteration 263, loss = 0.006137161981314421
iteration 264, loss = 0.0045698280446231365
iteration 265, loss = 0.005536737851798534
iteration 266, loss = 0.005034519359469414
iteration 267, loss = 0.003809964982792735
iteration 268, loss = 0.004073066636919975
iteration 269, loss = 0.004080733750015497
iteration 270, loss = 0.004712160676717758
iteration 271, loss = 0.003951007965952158
iteration 272, loss = 0.004460760857909918
iteration 273, loss = 0.004068024922162294
iteration 274, loss = 0.004206500481814146
iteration 275, loss = 0.004599187057465315
iteration 276, loss = 0.003974998369812965
iteration 277, loss = 0.006724299397319555
iteration 278, loss = 0.005551583133637905
iteration 279, loss = 0.003996022045612335
iteration 280, loss = 0.004706269595772028
iteration 281, loss = 0.004543179180473089
iteration 282, loss = 0.005627906881272793
iteration 283, loss = 0.0050319829024374485
iteration 284, loss = 0.0042379251681268215
iteration 285, loss = 0.0038926086854189634
iteration 286, loss = 0.004043527413159609
iteration 287, loss = 0.004634929355233908
iteration 288, loss = 0.00398024870082736
iteration 289, loss = 0.003939855843782425
iteration 290, loss = 0.005592770874500275
iteration 291, loss = 0.00576838618144393
iteration 292, loss = 0.004526161588728428
iteration 293, loss = 0.004558497574180365
iteration 294, loss = 0.00539938872680068
iteration 295, loss = 0.0043339841067790985
iteration 296, loss = 0.00375452172011137
iteration 297, loss = 0.0076809353195130825
iteration 298, loss = 0.004128959961235523
iteration 299, loss = 0.005125912372022867
iteration 300, loss = 0.005517186131328344
